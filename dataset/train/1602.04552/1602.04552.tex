\secput{sb-sched}{Space-Bounded Schedulers for the ND Model}

We show that {\it reasonably regular} programs in the ND model,
including all the algorithms in~\secref{num}, can be effectively
mapped to Parallel Memory Hierarchies by adapting the design of
space-bounded (SB) schedulers for NP programs. Regularity is a
quantifiable property of the algorithm (or spawn tree) that measures
how difficult it is to schedule; we will quantify this and argue show
that the algorithm in~\secref{num} are highly regular.  Space-bounded
schedulers for programs in the NP model were first proposed for
completely regular programs \cite{ChowdhurySiBl13}, improved upon and
rigorously analyzed in \cite{BlellochFiGi11}, and empirically
demonstrated to outperform work-stealing based schedulers for many
algorithms in \cite{SimhadriBlFi14}, but not for TRSM and Cholesky
algorithms due to their limited parallelism in the NP
model~\cite{SimhadriBlFi15}. The key idea in SB schedulers is that
each task is annotated with the size of its memory footprint to guide
the mapping of tasks to processors and caches in the hierarchy.  The
main result of this section is~\thmref{rt}, which says that the SB
scheduler is able to exploit the extra parallelism exposed in the ND
model.

\para{Machine Model: Parallel Memory Hierarchy.}
SB schedulers are well suited for the Parallel Memory Hierarchy (PMH)
machine model \cite{ACF93} (see~\figref{PMH}), which models the
multi-level cache hierarchies and cache sharing common in shared
memory multi-core architectures.  The PMH is represented by a
symmetric tree rooted at a main memory of infinite size. The internal
nodes are caches and the leaves are processors. We refer to subtrees
rooted at some cache as \defn{subclusters}. Each cache at level  is
assumed to be of the same size , and has the same the number of
level- caches attached to it. We call this the \defn{fan-out}
of level- and denote it by the constant , so that the number
of processors in a -level tree if . We
let  denote a constant indicative of the number of registers on a
processor. We let  denote the cost parameter representing the
cost of servicing a cache miss at level  from level . A
cache miss that must be serviced from level  requires  time steps.  For simplicity, we let the cache
block be one word long.  This limitation can be relaxed and analyzed
as in \cite{BlellochFiGi11}.


\para{Terminology.} 
A task is \defn{done} when all the leaf nodes (strands) associated with
its subtree have been executed.  A dataflow arrow originating at a leaf
node in the spawn tree is \defn{satisfied} when its source node is done.
A dataflow arrow originating at an internal node of the spawn tree is
\defn{satisfied} when all its descendants (rewritings) according to the
fire rules have been satisfied.  
A task is \defn{fully ready} or just \defn{ready} when all the
incoming dependencies (dataflow arrows) originating outside the
subtree are satisfied.  The \defn{size}, , of a task or a
strand is the number of distinct memory locations accessed by it.  We
assume that programs are statically allocated, that is all necessary
heap space is allocated up front and freed at program termination, so
that the size function is well defined. The size annotation can be
supplied by the programmer or can be obtained from a profiling
tool. If the size of a task in the spawn tree is not specified, we
inherit the annotation from its lowest annotated ancestor in the spawn
tree. We call a task \defn{-maximal} if its size is at most ,
but its parent in the spawn tree has size . A task is level-
maximal in a PMH if it is -maximal,  being the size of a
level- cache. Note that even though an -maximal task is not
ready, a -maximal subtask inside it (where ) can be ready.

\vspace{1ex}
\para{SB Schedulers.}
We define a space-bounded scheduler to be any scheduler that has the
anchoring and boundedness properties \cite{SimhadriBlFi15}:
\begin{itemize}
\item[\textbf{Anchor:}]
As the spawn tree unfolds dynamically, we assign and \defn{anchor}
ready tasks to caches in the hierarchy with respect to which they are
maximal.  Tasks are \textit{allocated} a part of the subcluster rooted
at the assigned cache. The anchoring property requires that all the
leaves of the spawn tree of a task be executed by processors in the
part of the subcluster allocated.
\item[\textbf{Boundedness:}]
 Tasks anchored to a cache of size  have a total size ,
where  is a scheduler chosen \defn{dilation parameter}.
\end{itemize}

There are several ways to maintain these properties and operate within
its constraints. The approach taken in \cite{BlellochFiGi11} is to
have a task queue with each anchored task that contains its subtasks
than can be potentially unrolled and anchored to the caches below it.
We adopt the same approach here (outlined below for convenience) for
the ND model with the \textbf{difference} being that we only anchor
and run ready subtasks. In the course of execution, ready tasks are
anchored to a suitable cache level (provided there is sufficient space
left), and each anchored task is \defn{allocated} subclusters beneath
the cache, based on the size of the task. Just as
in \cite{BlellochFiGi11}, a task of size  anchored at level-
cache is allocated
 
level- subclusters \footnote{The factor 3 in the allocation
function is a detail necessary to prove Thm.\ref{thm:rt}.}
where  is the parallelizability
of the task, a term we will define shortly. All processors in the
subclusters are required to work exclusively on this task.  Initially, 
the root node of the spawn tree is anchored to the root of the PMH.

To find work, a processor traverses the path from the leaf it
represents in the tree towards the root of the PMH until it reaches
the lowest anchor it is part of. Here it checks for ready tasks in the
queue associated with this anchor, and if empty, re-attempts to find
work after a short wait.  Otherwise, it pulls out a task from the
queue.  If the task is from an anchor at the cache immediately above
the processor, i.e. {\bf at an  cache}, it executes the subtask
by traversing the corresponding spawn tree in depth-first order.  If
the processor pulled this task out of an allocation at a {\bf cache at
level }, it does not try to execute its strands (leaves)
immediately. Instead, it unrolls the spawn tree corresponding to the
task using the DRS and enqueues those subtasks that are either of size
, or not ready, in the queue corresponding to the
anchor. Those subtasks that cannot be immediately worked on due to
lack of space in the caches are also enqueued. However, if the
processor encounters a ready task that has size less than that of a
level- cache (), and is able to find sufficient space for it
in the subcluster allocated to the anchor, the task is anchored at the
level- cache, and allocated a suitable number of subclusters below
the level- cache.  The processor starts unraveling the spawn tree
and finding work repeatedly.  When an anchored task is done, the
anchor, allocation and the associated resources are released for
future tasks. We also borrow other details in the design of the
space-bounded schedulers (e.g. how many subclusters are provisioned
for making progress on ``worst case allocations''? what fraction of
cache is reserved for tasks that ``skip cache levels''?)  from prior
work \cite{BlellochFiGi11}.

Roughly speaking, this scheduler uses all the partial parallelism
between level- maximal subtasks within a level- maximal
task. However, it does not use all the partial parallelism across
level- subtasks, especially those dataflow arrows between
level- subtasks in two different level- subtasks
(see~\figref{succ-levels}).

\begin{figure}[!th]
\includegraphics[width=\linewidth]{sb-succ-levels.eps}
\vspace{1ex}
\caption{Use of partial parallelism in the SB scheduler.  In this
  diagram, white represents tasks that are yet to start, gray
  represents running tasks, and black represents complete tasks.
  Green arrows represent dataflow arrows that may be used to start new
  tasks by the SB scheduler while orange datalow arrows are never
  immediately used.  Task  is level- maximal; tasks 
  and  are level- maximal; tasks  and
   are level- maximal.  Although subtask  has
  completed and has two outgoing dataflow edges, only , which
  is in the same level- maximal subtask () can be
  started;  can not immediately started until subtask 
  completes.
\label{fig:succ-levels}}
\end{figure}




\para{Metrics.} We now analyze the running time of the SB scheduler,
accounting for the cost of executing the work and load imbalance, but
not the overhead of the data structures need to keep track of anchors,
allocations, and the readiness of subtasks. We leave the optimization
of this overhead for a future empirical study.  The anchoring and
boundedness properties make it easy to preserve locality while trading
off some parallelism. Inspired by the analysis in
\cite{BlellochFiGi11,Simhadri13}, we develop a new analysis for the
ND model to argue that the impact of the loss of parallelism caused by
the anchoring property on load balance is not significant.

A critical consequence of the anchoring property of the SB scheduler
is that once a task is anchored to a cache, all the memory locations
needed for the task are loaded only once and are not forced to be
evicted until the completion of the task. This motivates the following
quantification of locality.  Given a task , decompose the spawn
tree into -maximal subtasks, and ``glue nodes'' that hold these
trees together (this decomposition is unique).  Define the
\defn{parallel cache complexity (PCC)}, , of task  
to be the sum of sizes of the maximal subtrees, plus a constant
overhead from each glue node. This is motivated by the expectation
that a good scheduler (such as SB) should be able to preserve locality
within -sized tasks given an cache of appropriate size, while it
might be too cumbersome to preserve locality across maximal
subtasks. \footnote{This definition is a generalization
of \cite[Defn.2]{BlellochFiGi11} for the ND model.  The full metric
measures cache complexity in terms of cache lines to model latency and
is also parameterized by a second parameter : size of a cache
line. We set  here for simplicity.  This simplification can be
reversed.}  The PCC metric differs from the another common metric for
locality of NP programs: the cache complexity  of the depth-first
traversal in the ideal cache model~\cite{AcarBlBl00}. Unlike ,
 does not depend on the order of traversal, but does not capture
data reuse across -maximal subtasks, which is a smaller order term
in our algorithms.

Note that  is a free parameter in this analysis. When the context
is clear, we often replace the task  in the  expression with
a size parameter corresponding to the task, so that cache complexity
is denoted . With this notation we have the following bound
on the cache complexity of the algorithms in ~\secref{num}.

\begin{claim}
For dense matrices of size , the divide and conquer
classical matrix multiplication, Triangular System Solve, Cholesky
and LU factorizations, and the 2D analog of the Floyd-Warshall algorithm
in~\secref{num} have parallel cache complexity 

when , with the glue nodes contributing an asymptotically
smaller term. The LCS algorithm has  for input
of size . This is true even if the algorithms are
expressed in the NP model by replacing fire constructs with ``''.
\label{clm:pcc-algos}
\end{claim}

As a direct consequence of the anchoring and boundedness properties,
which conservatively provision cache space, the following restatement
of \cite[Theorem 3] {BlellochFiGi11} applies to the ND model with the
same proof.
\begin{theorem}
\vspace{-3pt}
Suppose  is a task in ND program that is anchored at a level-
cache of a PMH by a SB scheduler with dilation parameter 
(i.e., a SB scheduler that anchors tasks of size at most 
at level ). Then for all cache levels , the sum of
cache misses incurred by all caches at level  is at most
.
\end{theorem}

In conjunction with~\clmref{pcc-algos}, this gives a bound on
the communication cost of the schedulers for ND algorithms. One can
verify from results on lower bounds on communication
complexity~\cite{BallardDeHoSc11} that these bounds are asymptotically
optimal.  If the scheduler is able to perfectly load balance a program
at every cache level on an  level PMH with  processors, we would
expect a task  to take

time steps to complete, where  is the dilation parameter.

\begin{figure}[!t]
\includegraphics[width=\linewidth]{PCC-ECC.eps}
\vspace{1ex}
\caption{-maximal subtasks (in gray) and glue nodes in the spawn tree 
of a task . The PCC, , is the sum of sizes of -maximal
subtasks plus one miss for each glue node. The red and blue sets of arrows
represent two chains of dependencies in . The ECC,  is
determined by the maximum, among all such chains, of the sum of effective
depth of -maximal subtasks in the chain, and the ratio 
 for a parameter .
\label{fig:CC}}
\vspace{1ex}
\end{figure}

However, if the algorithm does not have sufficient parallelism for the
PMH or is too irregular to load balance, we would expect it take
longer. Furthermore, since the number of processors assigned to a task
by a SB scheduler depends on its size, unlike in the case of
work-stealing style schedulers, a work-span analysis of programs is
not an accurate indicative of their running time.  A more
sophisticated cost model that takes account of locality,
parallelism-space imbalances, and lack of parallelism at different
levels of recursion is necessary.

In the NP model~\cite[Defn. 3]{BlellochFiGi11}, this was quantified by
the {\bf effective cache complexity metric (ECC, )}. We provide
a new definition of this metric for the ND model.  ECC attempts to
capture the cost of load balancing the program on hypothetical machine
with \defn{machine with parallelism at most}  --- a PMH which
has at most  level- caches
beneath each level- cache for all .

The metric assigns to each subtree of a spawn tree an estimate of its
complexity, measured in cache miss cost equivalents, when mapped to a
PMH by a SB scheduler. The estimate is based on its position in the
spawn tree and its cache complexity in the PCC metric. The metric has
two free parameters:  which represents the parallelism
available of a hypothetical machine, and  which represents the size
of one of the caches in the hierarchy with respect to which the spawn
tree is being analyzed.

\begin{definition}[Effective Cache Complexity (ECC)]
\label{def:qhat-fire}

Let  be a task in the ND model.  Unroll the spawn tree of ,
applying the DAG rewriting rules until all the leaves of the tree are
-maximal. Regard all dataflow arrows (solid or dashed) between the
leaves to be dependencies (see~\figref{CC}).

\noindent The ECC of a -maximal task  is
 .\\

\noindent The ECC of  is  , where
  


where  represents the set of chains of dependence edges
between -maximal tasks, , in the spawn tree of .
\end{definition}

The work dominated term has the same denominator as the left hand side
and thus captures the total amount of cache complexity in the spawn
tree (summation over leaves).  The depth dominated term captures the
critical path for the SB scheduler.  The term
 is the proxy for span
in our analysis and we call it the \defn{effective depth} of the task
. The depth dominated ensures that the effective depth defined by
ECC for a task is at least the sum of the effective depths of all
-maximal tasks along any chain between -maximal tasks induced
by DAG rewriting with respect to the fire rules.
The definition of ECC is such that:
\begin{enumerate}
\item
 In the range , for some
algorithm-specific constant , 
for all , for some positive universal constants .
\item
 On a machine with parallelism  for
some arbitrarily small positive constant , the running time
of the SB scheduler is within a constant factor of the perfectly load
balanced scenario in equation~\ref{eq:lb} (see~\thmref{rt}).
\item For NP programs, it coincides with the definition in \cite{BlellochFiGi11}.
\end{enumerate}

\para{Parallelizability of an Algorithm.} 
For the above reasons, we refer to the  of an algorithm
as its \defn{parallelizability} just as in \cite{Simhadri13}.  The
greater the parallelizability of the algorithm, the more efficient it
is to schedule on larger machines. When the parallelizability of the
algorithm asymptotically approaches the difference between the work
and the span exponents of the algorithm, we call it \defn{reasonably
regular}.  For an input of size , TRS, Cholesky and 2D
Floyd-Warshall have work exponent  and span exponent , and
the difference between them is . In many divide-and-conquer
algorithms, such as in \cite{BlellochGiSi10}, where the NP model does
not induce too many artificial dependencies, the parallelizability
exceeds that of largest shared memory machines available today. In
such algorithms SB schedulers have been empirically shown to be
effective at managing locality without compromising load balance, and
as a consequence, capable of outperforming work-stealing
schedulers \cite{SimhadriBlFi14}. However, this is not the case for
algorithms in~\secref{num}, which lose some parallelism when expressed
in the NP model.

For example, in the NP model, the parallelizability (w.r.t. cache size
) of the cache-oblivious matrix multiplication is  for some small constant 
(see~\clmref{CC-MM} in Appendix \ref{app-calc}), which is as high as
it can be. We expect the parallelizability of nested parallel TRS
algorithm to be less than . In fact, for an  upper triangular  and a right hand side  of size , the parallelizability the nested parallel TRS algorithm
in~\eqref{TRS-NP} which is  (see
~\clmref{CC-TRS} in Appendix \ref{app-calc}). This is smaller than the
parallelizability of matrix multiplication when . Since L3
caches are of the order of 10MB, the reduced parallelism adversely
affects load balance even in large instances that are of the order of
gigabytes (also empirically observed in~\cite{SimhadriBlFi15}).  When
expressed in the ND model, the parallelizability of TRS improves.
This can be seen from the geometric picture
in~\figref{trs-lattice-dag} where the depth dominated term
corresponding to the longest chain has effective depth
, which is less than the work
dominated term when .  This is the
parallelizability of TRS in the ND model.  This is also the case for
other linear algebra algorithms including Cholesky and LU
factorizations.

\vspace{3pt}
\para{Running time analysis.}
The main result of this section is~\thmref{rt} which shows
that SB schedulers can make use of the extra parallelizability of
programs expressed in the ND model.
\begin{theorem}
\vspace{-3pt}
Consider an -level PMH with  processors where a level- cache
has size , fanout  and cache miss cost . Let  be a
task such that  (the scheduler allocates the
entire hierarchy to such a task) with parallelizability 
in the ND model. Suppose that  exceeds the parallelism
of the machine by a constant. The running time of  is no more than:

for some constant , where .
\label{thm:rt}
\end{theorem}

The theorem says that when the machine parallelism is no greater than
the parallelizability of the algorithm in the ND model, the algorithm
runs within a constant factor () of the perfectly load balanced
scenario in~\eqref{lb}.  Relating this theorem to the definition of
machine parallelism, we infer that for highly regular algorithms
considered in this paper, the SB scheduler can effectively use up to
 level- subclusters for some arbitrarily
small constant .

We prove this theorem using the notion of effective work, the
separation lemma (lemma~\ref{lemma:lvl-decomp}) and a work-span
argument based on effective depth as in~\cite{BlellochFiGi11}.
The \defn{latency added effective work} is similar to the effective
cache complexity, but instead of counting just cache misses at one
cache level, we add the cost of cache misses at each instruction.
The cost  of an instruction  accessing location  is
, where  is the work, and  is the cost of a cache miss if the scheduler causes
the instruction  to fetch  from a level- cache in the
PMH. The instruction would need to incur a cache miss at level- if
it is the first instruction within the unique maximal level- task
that accesses a particular memory location.  Using this
per-instruction cost, we define effective work  of a task
using structural induction in a manner that is deliberately similar to
that of .

\begin{definition}[Latency added cost]
With cost  assigned to instructions, the \defn{latency added
effective work} of a task , or a strand  nested inside a
task  (from which it inherits space declaration) is:
\noindent\rulestrand

\noindent\ruletask  For task  of size between  and ,
 the l.a.e.w. is , where
  

where  represents the set of chains of dependence edges
between -maximal tasks, , in the spawn tree of .
\end{definition}

Because of the large number of machine parameters involved
(, etc.), it is undesirable to compute the latency
added work directly for an algorithm.  Instead, we will show that the
latency added effective work can be upper bounded by a sum of per
(cache) level machine costs  that can, in turn be
bounded by machine parameters and ECC of the algorithm.  For
,  of a task  is computed exactly like
 using a different base case: for each instruction  in
, if the memory access at  costs at least , assign a
cost of  to that node.  Else, assign a cost of
.  Further, we set , and define
 in terms of . It also follows from
these definitions that  for all instructions .

\begin{lemma}
\textbf{Separation Lemma}: On an -level PMH, and for a parameter
, for a task  with size at least , we have:

\label{lemma:lvl-decomp}
\end{lemma}
\begin{proof}
  The proof is based on induction on the structure of the task
in terms of decomposition into strands and maximal tasks.
  For the base case of the induction, consider the strand  at the
  lowest level in the spawn tree. If  denotes the space of the
  strand or the task immediately enclosing  from which it
  inherits space declaration, then by
  definition 

For a task  corresponding to a spawn tree , the \emph{latency
added effective depth}
 is either
defined by the work or the depth dominated term which is one of the
chains in , the set of chains of level- maximal
tasks in the spawn tree .  Index the work dominated term as the
-th term and index the chains in  in some order
starting from .  Suppose that of these terms, the term that
determines 
is the -th term. Denote this by , and the -th summand in
the term by .  Similarly, consider the terms for evaluating
each of  -- which are numbered the same way as in
 -- and suppose that the -th term (denoted by
) on the right hand side determines the value of
.  Then,


where the inequality is an application of the inductive hypothesis.
Further, by the definition of   and , we have


  which completes the proof.
\end{proof}
With the separation lemma in place for the ND model, the proof of
\thmref{rt} follows from the two lemmas which we adapt from
\cite{BlellochFiGi11}. The first is a bound on the per level latency
added effective work term in terms of the effective cache complexity.
The second is a bound on the runtime in terms of the latency added
effective work using a modified work-span analysis akin to
 Brent's theorem.

\begin{lemma}
  Consider an -level PMH and a task (or a strand) . 
  If  is scheduled on this PMH using a space-bounded
  scheduler with dilation , then
.
  \label{lemma:leveldecompapp}
\end{lemma}
\begin{lemma}
  Consider an -level PMH and a task with parallelizability with
   that exceeds the parallelism of the PMH by a small
  constant. Let .
  Let  be a task or strand which has been
  assigned a set  of 
  level- subclusters by the scheduler. Letting  (by definition, ), the running time of  is at most:

for some constant .
  \label{lem:runtime}
\end{lemma}
The proofs of these two lemmas follow the same arguments as
in~\cite{BlellochFiGi11} with minor, but straightforward, modifications
that account for the new definition of the ECC in the ND model.

\punt{
\begin{lemma}
  Consider an -level PMH and a task (or a strand) . 
  If  is scheduled on this PMH using a space-bounded
  scheduler with dilation , then
.
  \label{lemma:leveldecompapp}
\end{lemma}
\begin{lemma}
  Consider an -level PMH and a task with parallelizability with
   that exceeds the parallelism of the PMH by a small
  constant. Let .
  Let  be a task or strand which has been
  assigned a set  of 
  level- subclusters by the scheduler. Letting  (by definition, ), the running time of  is at most:

for some constant .
  \label{lem:runtime}
\end{lemma}
The proofs of these two lemmas follow the same arguments as
in~\cite{BlellochFiGi11} with minor, but straightforward, modifications
that account for the new definition of the ECC in the ND model.

 Note that we did not use the fact that
  some of the components were work or cache complexities. The proof
  only depended on the fact that  and the structure of the composition rules.
   could have been  replaced with any other kind of work and
   with its decomposition.

Consider the {\bf rule for a leaf node}  nested immediately
within a task  with size : a SB scheduler might assign
it up to .  Since the leaf node has no parallelism
it uses only one processor for 
``steps''. However, we make it ``pay'' the cost of keeping
 processors idle for 
``steps''.  Consider the {\bf rule for the parallel block} , which is shorthand for a subtree rooted
at node  that is a disjoint union of a few  operators
and subtrees representing tasks .  We take the space of a
parallel block to be the size of the task in which it is immediately
nested, say . A lower bound on the complexity of  is the sum
of the complexities of the tasks , which gives rise to the
``work dominated'' term. Another possibility is that one of the tasks
in the parallel block takes much longer than the rest, and therefore
defines the complexity of .  To account for this, we use the term
 as a measure of
the \defn{effective depth} of , i.e., how long it takes when
assigned  processors. The ``depth dominated'' term
proposes that the effective depth of  is at least as much as the
effective depth of any of .  The {\bf rule for tasks} is
similarly explained as the composition of effective depths.  Thus this
metric is at once a generalization of work, span, and metrics of
locality, all of which can be recovered by setting parameters in
 appropriately (e.g., work is .

\begin{definition}\label{def:qhat}
\vspace{-8pt}
For cache parameter  and parallelism parameter ,
the \defn{effective cache complexity} of a leaf , parallel block
, or task  starting at cache state  is defined as:

\noindent\comprule{leaf} Let  be the nearest containing task of
leaf 
\vspace{-4pt}

\vspace{-4pt}
\noindent\ruleparblock For  in task , 

\vspace{-4pt}
\noindent\ruletask For , 

where  if  is  and if
, , where
 is the set of locations touched by .
\vspace{-4pt}
\end{definition}
}
