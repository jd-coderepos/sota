

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{./figures/prcurve.pdf}
\end{center}
\vspace*{-2mm}
\caption{Evaluation results of PIXOR and MV3D \cite{mv3d} on KITTI BEV Object Detection validation set. For each approach, we plot 5 Precision-Recall curves corresponding to 5 different IoU thresholds between 0.5 and 0.9, and report the averaged AP (\%). We compare in three different ranges.}
\label{fig:fine_kitti}
\vspace{-0.3cm}
\end{figure*}


\section{Experiments}

We conduct three types of experiments here. First, we compare PIXOR with other state-of-the-art 3D object detectors on the public KITTI bird's eye view object detection benchmark \cite{kitti}. We show that PIXOR achieves best performance both in accuracy and speed compared with all previously published methods. Second, we conduct an ablation study of PIXOR in three aspects: optimization, network architecture, and speed. Third, we verify the generalization ability of PIXOR by applying it to a new large-scale vehicle detection dataset for autonomous driving.

\subsection{BEV Object Detection on KITTI}

\subsubsection{Implementation Details} 
We set the region of interest for the point cloud to  meters and do bird's eye view projection with a discretization resolution of  meter. We set the height range to  meters in LIDAR coordinates and divide all points into  slices with bin size of  meter. One reflectance channel is also computed. As a result, our input representation has the dimension of . We use data augmentation of rotation between  degrees along the Z axis and a random flip along  axis during training. Unlike other detectors \cite{mv3d} that initialize the network weights from a pre-trained model, we train our network from scratch without resorting to any pre-trained model.

\vspace{-0.2cm}
\subsubsection{Evaluation Metric}
We use Average Precision (AP) computed at  Intersection-Over-Union (IoU) as our evaluation metric in all experiments unless mentioned otherwise. We compute the AP as Area Under Precision-Recall Curve (AUC) \cite{pascal}. We evaluate on `Car' category and ignore `Van', `Truck', `Tram' and `DontCare' categories in KITTI during evaluation, meaning that we don't count True Positive (TP) or False Negative (FN) on them. Note that the metric we use is different from what KITTI reports in the following two aspects: (1) KITTI computes AP by sampling at  linearly sampled recall rates (from  to ), which is a rough approximation to real AUC. (2) KITTI divides labels into three subsets with image-based definition (e.g, object height in pixels, visibility in image), and reports AP on each subset, which doesn't suit pure LIDAR based object detection. In contrast, we evaluate on {\it all} labels within the region of interest, and do fine-grained evaluation with respect to ranges (object distance to the ego-car).

\vspace{-0.2cm}
\subsubsection{Evaluation Result}
We compare with 3D object detectors that use LIDAR on KITTI benchmark: VeloFCN \cite{velofcn}, 3D FCN \cite{3dfcn} and MV3D \cite{mv3d}. We show the evaluation results in Table \ref{tab:object_kitti}. From the table we see that PIXOR largely outperforms other approaches in AP at 0.7 IoU within 70 meters range, leading the second best by over . We also show evaluation results with respect to ranges, and show that PIXOR outperforms more in the long range. On KITTI's test set, PIXOR outperforms MV3D in {\it moderate} and {\it hard} settings.

Since MV3D is the best approach among all state-of-the-art methods, we'd like to make a more detailed comparison using the AUC based AP metric. We show fine-grained Precision-Recall (PR) curves of both PIXOR and MV3D in Figure \ref{fig:fine_kitti}. From the figure, we get the following observations: (1) PIXOR outperforms MV3D in all IoU thresholds, especially at very high IoU like 0.8 and 0.9, showing that even without using proposal, PIXOR can still get super-accurate object localization, compared to the two-stage proposal based detector MV3D. (2) PIXOR has similar precision with MV3D at low recall rates. However, when it comes to higher recall rates, PIXOR shows huge advantage. At the same precision rate of the end point of MV3D's curve, PIXOR generally has over  higher recall rate in all ranges. This shows that dense detector like PIXOR does have an advantage of higher recall rate, compared with two-stage detectors. (3) In the more difficult long range part, PIXOR still shows superiority over MV3D, which justifies our input representation design that reserves the 3D information well and our network architecture design that captures both fine details and regional context.


\begin{table}[t]
\begin{center}
\begin{small}
\begin{tabular}{|c|c|cc|}
\hline
Classification & Regression &  &  \\
\hline
cross-entropy & smooth\_L1 & 73.46\% & 55.25\%\\
focal & smooth\_L1 & 74.93\% & 55.89\% \\
focal & decoding & 71.05\% & 53.05\% \\
focal & smooth\_L1 + decode (f.t.) & {\bf 77.16\%} & {\bf 58.31\%} \\
\hline
\end{tabular}
\caption{Ablation study of different loss functions. {\bf smooth\_L1 + decode (f.t.)} means that the network is trained with smooth L1 loss first, and then fine-tuned by replacing the smooth L1 loss with decoding loss.}
\label{tab:loss}
\end{small}
\end{center}
\vspace{-0.3cm}

\end{table}
\begin{table}[t]
\begin{center}
\begin{small}
\begin{tabular}{|c|c|cc|}
\hline
Training Samples & Data Aug. &  &  \\
\hline
all pixels & none & 71.10\% & 53.99\% \\
ignore boundary pixels & none & 74.54\% & 55.79\% \\
ignore boundary pixels & rotate + flip & {\bf 74.93\%} & {\bf 55.89\%} \\
\hline
\end{tabular}
\caption{Ablation study of different data sampling strategies.}
\label{tab:sample}
\end{small}
\end{center}
\vspace{-0.5cm}
\end{table}


\subsection{Ablation Study}

We show an extensive ablation study of the proposed detector in terms of optimization, network architecture, speed and failure mode.

\vspace{-0.2cm}
\subsubsection{Experimental Setting}

Since we also compare with other state-of-the-art methods on the {\it val} set, it would be inappropriate to do the ablation study on the same set. Therefore we resort to KITTI Raw dataset \cite{kitti-raw} and randomly pick 3000 frames that are not overlapped with both {\it train} and {\it val} sets in KITTI object detection dataset, which we call {\it val-dev} set. We report ablation study results on this set. We use AP at 0.7 IoU as well as AP averaged from 0.5 to 0.95 IoUs (with a stride of 0.05) as the evaluation metrics.

\begin{table}[t]
\begin{center}
\begin{small}
\begin{tabular}{|l|c|}
\hline
Backbone Network &  \\
\hline
pvanet & 51.28\% \\
resnet-50 & 53.03\% \\
vgg16-half & 54.46\% \\
ours & {\bf 55.07\%} \\
\hline
\end{tabular}
\caption{Ablation study of different backbone networks.}
\label{tab:backbone_exp}
\end{small}
\end{center}
\vspace{-0.5cm}
\end{table}
\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{./figures/header_net.pdf}
\caption{Three versions of header network architectures.}
\label{fig:subnet}
\end{center}
\vspace{-0.5cm}
\end{figure}


\vspace{-0.2cm}
\subsubsection{Optimization}

We investigate into four topics here: the classification loss, the regression loss, the sampling strategy, and data augmentation.

\vspace{-0.2cm}
\paragraph{Classification loss} RetinaNet \cite{focal} proposes the focal loss to re-weight samples for dense detector training. For simplicity, we use their hyper-parameter setting. We show results in Table \ref{tab:loss}, and find that focal loss improves the AP\_0.7 by more than 1\%.

\vspace{-0.2cm}
\paragraph{Regression loss} For box regression, our default choice is smooth L1 loss \cite{frcn} on every dimension of the regression targets. We also adopt a {\it decoding loss}, where the output targets are first decoded into oriented boxes and then smooth L1 loss is computed on the (x, y) coordinates of four box corners directly with regard to ground-truth. Since the decoding the oriented box from regression targets is just a combination of some normal mathematic operations, this decoding process is differentiable and gradients can be back-propagated through this decoding process. We believe that this decoding loss is more end-to-end and implicitly balances different dimensions of the regression targets. In the results shown in Table \ref{tab:loss}, we show that directly training with the decoding loss doesn't work very well. However, training with conventional loss first and then fine-tuning with the proposed decoding loss helps improve the performance a lot.

\vspace{-0.2cm}
\paragraph{Data sampling and augmentation} 
When training dense detectors, one issue is how to define positive and negative samples. In proposal based approaches, this is defined by the IoU between proposal and ground-truth. Since PIXOR is a proposal-free method, we go for a more straightforward sampling strategy: all pixels inside the ground-truth are positive samples while outside pixels are negative samples. This simple definition already gives decent performance. However, one issue with this definition is that the variance of regression targets could be large for pixels near the object boundary. Therefore we propose to sub-sample the pixels, i.e, to ignore pixels near object boundary during training. Specifically, we zoom the ground-truth object box twice by  and  respectively, and ignore all pixels between these two. From the results shown in Table \ref{tab:sample}, we find that this sub-sampling strategy is beneficial to stabilize training. We also find that our data augmentation for KITTI helps a bit since PIXOR is trained from scratch instead of from a pre-trained model.

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.24\linewidth]{./figures/dets/00.png}\includegraphics[width=0.24\linewidth]{./figures/dets/01.png}\includegraphics[width=0.24\linewidth]{./figures/dets/02.png}\includegraphics[width=0.24\linewidth]{./figures/dets/03.png}
   \includegraphics[width=0.24\linewidth]{./figures/dets/04.png}\includegraphics[width=0.24\linewidth]{./figures/dets/05.png}\includegraphics[width=0.24\linewidth]{./figures/dets/06.png}\includegraphics[width=0.24\linewidth]{./figures/dets/07.png}
\end{center}
\vspace*{-3mm}
   \caption{Example detection results of PIXOR on KITTI BEV Object Detection validation set. The detection is in red color, while the ground-truth is in blue color. Gray area is out of the scope of the camera view and therefore has no labels.}
\label{fig:det_demo}
\vspace{-0.3cm}
\end{figure*}
\begin{table}[t]
\begin{center}
\begin{small}
\begin{tabular}{|l|cc|}
\hline
Header Network &  &  \\
\hline
non-sharing & 74.93\% & 55.89\% \\
partially-shared & 74.66\% & 55.75\% \\
fully-shared & {\bf 75.13\%} & {\bf 56.04\%} \\
\hline
\end{tabular}
\caption{Ablation study of different header network architectures.}
\label{tab:subnet}
\end{small}
\end{center}
\vspace{-0.5cm}
\end{table}
\begin{table}[t]
\begin{center}
\begin{small}
\begin{tabular}{|l|ccc|c|}
\hline
 & digitization & network & NMS & total \\
\hline
time (ms) & 1 & 31 & 3 & 35 \\
\hline
\end{tabular}
\caption{The detailed timing analysis of PIXOR on KITTI dataset.}
\label{tab:object_time}
\end{small}
\end{center}
\vspace{-0.5cm}
\end{table}



\vspace{-0.2cm}
\subsubsection{Network Architecture}

\vspace{-0.2cm}
\paragraph{Backbone network} We first compare different backbone networks: vgg16 with half channel number \cite{vgg}, pvanet \cite{pvanet}, resnet-50 \cite{resnet}, and the proposed architecture as presented in Figure \ref{fig:network_architect}. All of these backbone networks run below 100 milliseconds. All backbone networks except for {\it vgg16-half} uses residual unit as building blocks. We find that {\it vgg16-half} converges faster in {\it train} set and gets lower training loss than all other residual variants, but the performance drops quite a lot when evaluated on {\it val} set. This doesn't happen to the other three residual-based networks. We conjecture that this is because {\it vgg16-half} is more prone to overfitting without implicit regularization imposed by residual connections. 

\vspace{-0.2cm}
\paragraph{Header network} We also compare different structures for the header network. We investigate into how much we should share the parameters for the multi-task outputs. Three versions of header network are proposed with different extent of weight sharing in Figure \ref{fig:subnet} and compared in Table \ref{tab:subnet}. All these three versions have very close number of parameters. We find that fully-shared structure works best as it utilizes the parameters most efficiently.

\vspace{-0.2cm}
\subsubsection{Speed} We show detailed timing analysis of PIXOR in Table \ref{tab:object_time} for one single frame. All computations are performed on GPU. The network time is measured on a NVIDIA Titan Xp GPU and averaged over 100 non-sequential frames in KITTI.

\vspace{-0.2cm}
\subsubsection{Failure Mode} We show some detection results of PIXOR in Figure \ref{fig:det_demo}, and discover some failure modes. In general PIXOR will fail when there's no observed LIDAR points. In longer range we have very few evidence of the object, and therefore object localization becomes inaccurate, leading to false positives at higher IoU thresholds.

\subsection{BEV Object Detection on Large-scale Dataset}

\subsubsection{TOR4D Dataset}
We also collect a large-scale 3D vehicle detection dataset called TOR4D which has a different sensor configuration from KITTI and is collected in North-American cities. There are in total 6500 sequences collected, which are divided into 5000/500/1000 as train/val/test splits. The training sequences are sampled at 10 Hz into frames, while validation and testing sequences are sampled at 0.5Hz. As a result, there are over 1.2 million frames in training set, 5969 and 11969 frames in the val and test sets. All vehicles are annotated with bird's eye view bounding boxes.

\vspace{-0.2cm}
\subsubsection{Evaluation Result}
We make the following modifications to PIXOR on TOR4D dataset: we use ``vgg-half'' backbone network \cite{mv3d}, the detection region is 100m forward and backward and 40m to the left and right of the ego car, and the voxelization resolution is 0.2m. The network inference time of this model is 24 ms on a NVIDIA 1080Ti GPU. In comparison, we build a YOLO-like \cite{yolo} baseline detector with a customized backbone network, and add object anchors and multi-scale feature fusion to further improve the performance. The evaluation results on TOR4D test set are listed in Table \ref{tab:our-data}, where we show that PIXOR outperforms the baseline by 3.9\% in , proving that PIXOR is simple and easy to generalize.

\begin{table}[t]
\begin{center}
\begin{small}
\begin{tabular}{|l|c|}
\hline
Method &  \\
\hline
Baseline \cite{yolo} & 69.4\% \\
PIXOR & \textbf{73.3\%} \\
\hline
\end{tabular}
\caption{Evaluation of PIXOR on TOR4D test set.}
\label{tab:our-data}
\end{small}
\end{center}
\vspace{-0.7cm}
\end{table}
