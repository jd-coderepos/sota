\documentclass{article} \usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage[ruled,vlined,noresetcount]{algorithm2e}
\usepackage{color,xcolor}
\usepackage{epsfig}
\usepackage{graphicx}

\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{float,wrapfig}
\usepackage{hhline}
\usepackage{multirow}


\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{changepage}
\usepackage{extramarks}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage{soul}
\usepackage{xspace}

\usepackage{url}

\usepackage{enumerate}
\usepackage{todonotes} 

\usepackage{titlesec}

\usepackage{makecell}

\usepackage{pifont} \usepackage{subcaption}




\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\xpar}[1]{\noindent\textbf{#1}\ \ }
\newcommand{\vpar}[1]{\vspace{3mm}\noindent\textbf{#1}\ \ }

\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\sects}[1]{Sections~\ref{#1}}
\newcommand{\eqn}[1]{Equation~\ref{#1}}
\newcommand{\eqns}[1]{Equations~\ref{#1}}
\newcommand{\fig}[1]{Figure~\ref{#1}}
\newcommand{\figs}[1]{Figures~\ref{#1}}
\newcommand{\tbl}[1]{Table~\ref{#1}}
\newcommand{\tbls}[1]{Tables~\ref{#1}}

\newcommand{\ignorethis}[1]{}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\fcseven}{}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\def\naive{na\"{\i}ve\xspace}
\def\Naive{Na\"{\i}ve\xspace}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\iid{\emph{i.i.d}\onedot}
\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{i.e\onedot} \def\Ie{I.e\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{etc\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{et al\onedot}
\makeatother

\definecolor{MyDarkBlue}{rgb}{0,0.08,1}
\definecolor{MyDarkGreen}{rgb}{0.02,0.6,0.02}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{MyDarkOrange}{rgb}{0.40,0.2,0.02}
\definecolor{MyPurple}{RGB}{111,0,255}
\definecolor{MyRed}{rgb}{1.0,0.0,0.0}
\definecolor{MyGold}{rgb}{0.75,0.6,0.12}
\definecolor{MyDarkgray}{rgb}{0.66, 0.66, 0.66}

\newcommand{\jw}[1]{\textcolor{MyDarkGreen}{[Jiajun: #1]}}
\newcommand{\jiayuan}[1]{\textcolor{MyDarkBlue}{[Jiayuan: #1]}}
\newcommand{\gc}[1]{\textcolor{MyDarkRed}{[Chuang: #1]}}
\newcommand{\kyi}[1]{\textcolor{MyDarkOrange}{[Kexin: #1]}}

\newcommand{\modelfull}{neuro-symbolic concept learner\xspace}
\newcommand{\model}{NS-CL\xspace}
\newcommand{\myparagraph}[1]{\vspace{-3pt}\paragraph{#1}}

\newcommand{\revisioncolor}{}



 
\title{The Neuro-Symbolic Concept Learner:\\Interpreting Scenes, Words, and Sentences\\from Natural Supervision}



\iclrfinalcopy



\author{Jiayuan Mao \\
MIT CSAIL and IIIS, Tsinghua University\\
\texttt{mjy14@mails.tsinghua.edu.cn}
\And
Chuang Gan\\
MIT-IBM Watson AI Lab\\
\texttt{ganchuang@csail.mit.edu}
\And
Pushmeet Kohli\\
Deepmind\\
\texttt{pushmeet@google.com}
\And
Joshua B. Tenenbaum\\
MIT BCS, CBMM, CSAIL\\
\texttt{jbt@mit.edu}
\And
Jiajun Wu\\
MIT CSAIL\\
\texttt{jiajunwu@mit.edu}
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval. \footnotetext{Project page: \url{http://nscl.csail.mit.edu}}
\end{abstract}

\section{Introduction}



Humans are capable of learning visual concepts by jointly understanding vision and language \citep{fazly2010probabilistic,chrupala2015learning,gauthier2018word}.
Consider the example shown in \fig{fig:teaser}-I. Imagine that someone with no prior knowledge of colors is presented with the images of the red and green cubes, paired with the questions and answers. They can easily identify the difference in objects' visual appearance (in this case, color), and align it to the corresponding words in the questions and answers (\texttt{Red} and \texttt{Green}). Other object attributes (\eg, shape) can be learned in a similar fashion. Starting from there, humans are able to inductively learn the correspondence between visual concepts and word semantics (\eg, spatial relations and referential expressions, \fig{fig:teaser}-II), and unravel compositional logic from complex questions assisted by the learned visual concepts (\fig{fig:teaser}-III, also see \cite{abend2017bootstrapping}). 







Motivated by this, we propose the \modelfull (\model), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs. \model has three modules: a neural-based perception module that extracts object-level representations from the scene, a visually-grounded semantic parser for translating questions into executable programs, and a symbolic program executor that reads out the perceptual representation of objects, classifies their attributes/relations, and executes the program to obtain an answer.


\model learns from natural supervision (\ie, images and QA pairs), requiring no annotations on images or semantic programs for sentences. Instead, analogical to human concept learning, it learns via curriculum learning. \model starts by learning representations/concepts of individual objects from short questions (\eg, What's the color of the cylinder?) on simple scenes (3 objects). By doing so, it learns object-based concepts such as colors and shapes. \model then learns relational concepts by leveraging these object-based concepts to interpret object referrals (\eg, Is there a box right of a cylinder?). The model iteratively adapts to more complex scenes and highly compositional questions. 





\model's modularized design enables interpretable, robust, and accurate visual reasoning: it achieves state-of-the-art performance on the CLEVR dataset~\citep{Johnson2017CLEVR}. More importantly, it naturally learns disentangled visual and language concepts, enabling combinatorial generalization \wrt both visual scenes and semantic programs. In particular, we demonstrate four forms of generalization. First, \model generalizes to scenes with more objects and longer semantic programs than those in the training set. Second, it generalizes to new visual attribute compositions, as demonstrated on the CLEVR-CoGenT~\citep{Johnson2017CLEVR} dataset. Third, it enables fast adaptation to novel visual concepts, such as learning a new color. Finally, the learned visual concepts transfer to new tasks, such as image-caption retrieval, without any extra fine-tuning.



\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/Teaser.pdf}
\caption{Humans learn visual concepts, words, and semantic parsing jointly and incrementally. {\bf I.} Learning visual concepts (red \vs green) starts from looking at simple scenes, reading simple questions, and reasoning over contrastive examples~\citep{fazly2010probabilistic}. {\bf II.} Afterwards, we can interpret referential expressions based on the learned object-based concepts, and learn relational concepts (\eg, on the right of, the same material as). {\bf III} Finally, we can interpret complex questions from visual cues by exploiting the compositional structure.}
\label{fig:teaser}
\vspace{-5pt}
\end{figure}
\section{Related Work}

Our model is related to research on joint learning of vision and natural language. In particular, there are many papers that learn visual concepts from descriptive languages, such as image-captioning or visually-grounded question-answer pairs \citep{kiros2014unifying,shi2018learning,mao2016training,vendrov2015order,ganju2017what}, dense language descriptions for scenes \citep{densecap}, video-captioning \citep{Donahue2015Long} and video-text alignment \citep{zhu2015aligning}.

Visual question answering (VQA) stands out as it requires understanding both visual content and language. The state-of-the-art approaches usually use neural attentions~\citep{malinowski2014multi,chen2015abc,Yang2016Stacked,xu2016ask}.  Beyond question answering, \cite{Johnson2017CLEVR} proposed the CLEVR (VQA) dataset to diagnose reasoning models. CLEVR contains synthetic visual scenes and questions generated from latent programs. \tbl{tab:related-vqa} compares our model with state-of-the-art visual reasoning models~\citep{Andreas2016Learning,Suarez2018DDRprog,Santoro2017simple} along four directions: visual features, semantics, inference, and the requirement of extra labels.

For visual representations, \cite{Johnson2017Inferring} encoded visual scenes into a convolutional feature map for program operators. \cite{Mascharka2018Transparency,Hudson2018Compositional} used attention as intermediate representations for transparent program execution. Recently, \cite{kexin} explored an interpretable, object-based visual representation for visual reasoning. It performs well, but requires fully-annotated scenes during training. Our model also adopts an object-based visual representation, but the representation is learned only based on natural supervision (questions and answers).

{\revisioncolor \cite{Anderson2017BottomUp} also proposed to represent the image as a collection of convolutional object features and gained substantial improvements on VQA. Their model encodes questions with neural networks and answers the questions by question-conditioned attention over the object features. In contrast, \model parses question inputs into programs and executes them on object features to get the answer. This makes the reasoning process interpretable and supports combinatorial generalization over quantities (\eg, counting objects). Our model also learns general visual concepts and their association with symbolic representations of language. These learned concepts can then be explicitly interpreted and deployed in other vision-language applications such as image caption retrieval.}


There are two types of approaches in semantic sentence parsing for visual reasoning: implicit programs as conditioned neural operations (\eg, conditioned convolution and dual attention)~\citep{Perez2017Film,Hudson2018Compositional} and explicit programs as sequences of symbolic tokens~\citep{Andreas2016Learning,Johnson2017Inferring,Mascharka2018Transparency}. As a representative, \cite{Andreas2016Learning} build modular and structured neural architectures based on programs for answering questions.
Explicit programs gain better interpretability, but usually require extra supervision such as ground-truth program annotations for training. This restricts their application. We propose to use visual grounding as distant supervision to parse questions in natural languages into explicit programs, with {\it zero} program annotations. Given the semantic parsing of questions into programs, \cite{kexin} proposed a purely symbolic executor for the inference of the answer in the logic space. Compared with theirs, we propose a quasi-symbolic executor for VQA.


\begin{table}[t]
\centering\small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Models} & \multirow{2}{*}{Visual Features} & \multirow{2}{*}{Semantics} & \multicolumn{2}{c}{Extra Labels} & \multirow{2}{*}{Inference} \\ 
        \cmidrule{4-5}
        & & & \# Prog. & Attr. & \\
        \midrule
        FiLM \citep{Perez2017Film} & Convolutional & Implicit & 0 & No & Feature Manipulation\\
        IEP \citep{Johnson2017Inferring} & Convolutional & Explicit & 700K & No &  Feature Manipulation\\
        \midrule
        MAC \citep{Hudson2018Compositional} & Attentional & Implicit & 0 & No & Feature Manipulation\\
        Stack-NMN \citep{Hu2018Explainable} & Attentional & Implicit & 0 & No &  Attention Manipulation\\
        TbD \citep{Mascharka2018Transparency} & Attentional & Explicit & 700K & No &  Attention Manipulation\\
        \midrule
        NS-VQA \citep{kexin} & Object-Based & Explicit & 0.2K & Yes & Symbolic Execution\\ 
        \model & Object-Based & Explicit & 0 & No & Symbolic Execution \\
        \bottomrule
    \end{tabular}
    \caption{Comparison with other frameworks on the CLEVR VQA dataset, \wrt visual features, implicit or explicit semantics and supervisions.}
\label{tab:related-vqa}
\end{table} 
Our work is also related to learning interpretable and disentangled representations for visual scenes using neural networks. \citet{kulkarni2015deep} proposed convolutional inverse graphics networks for learning and inferring pose of faces, while \citet{Yang2015Weakly} learned disentangled representation of pose of chairs from images. \citet{Wu2017Neural} proposed the neural scene de-rendering framework as an inverse process of any rendering process. \citet{Siddharth2017Learning,Higgins2018SCAN} learned disentangled representations using deep generative models. In contrast, we propose an alternative representation learning approach through joint reasoning with language.

\section{Neuro-Symbolic concept learner}
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/Framework.pdf}
\caption{We propose to use neural symbolic reasoning as a bridge to jointly learn visual concepts, words, and semantic parsing of sentences.}
\vspace{-5pt}
\label{fig:framework}
\end{figure}
 
We present our \modelfull, which uses a symbolic reasoning process to bridge the learning of visual concepts, words, and semantic parsing of sentences without explicit annotations for any of them.
We first use a visual perception module to construct an object-based representation for a scene, and run a semantic parsing module to translate a question into an executable program. We then apply a quasi-symbolic program executor to infer the answer based on the scene representation. We use paired images, questions, and answers to jointly train the visual and language modules.

Shown in \fig{fig:framework}, given an input image, the visual perception module detects objects in the scene and extracts a deep, latent representation for each of them. The semantic parsing module translates an input question in natural language into an executable program given a domain specific language (DSL). The generated programs have a hierarchical structure of symbolic, functional modules, each fulfilling a specific operation over the scene representation. The explicit program semantics enjoys compositionality, interpretability, and generalizability.



The program executor executes the program upon the derived scene representation and answers the question. Our program executor works in a symbolic and deterministic manner.
This feature ensures a transparent execution trace of the program.
Our program executor has a fully differentiable design \wrt the visual representations and the concept representations, which supports gradient-based optimization during training.




\begin{figure}[t]
\centering \includegraphics[width=\textwidth]{raw/VSE.pdf}
\caption{We treat attributes such as \texttt{Shape} and \texttt{Color} as neural operators. The operators map object representations into a visual-semantic space. We use similarity-based metric to classify objects.}
\vspace{-5pt}
\label{fig:vse}
\end{figure}
 
\subsection{Model details}

\paragraph{Visual perception.}
Shown in \fig{fig:framework}, given the input image, we use a pretrained Mask R-CNN \citep{He2017Mask} to generate object proposals for all objects. The bounding box for each single object paired with the original image is then sent to a ResNet-34~\citep{He2015Deep} to extract the region-based (by RoI Align) and image-based features respectively. We concatenate them to represent each object. Here, the inclusion of the representation of the full scene adds the contextual information, which is essential for the inference of relative attributes such as size or spatial position.

\myparagraph{Concept quantization.}
Visual reasoning requires determining an object's attributes (\eg, its color or shape). We assume each visual attribute (\eg, shape) contains a set of visual concept (\eg, \texttt{Cube}). In \model, visual attributes are implemented as neural operators
, mapping the object representation into an attribute-specific embedding space. \fig{fig:vse} shows an inference an object's shape. Visual concepts that belong to the shape attribute, including \texttt{Cube}, \texttt{Sphere} and \texttt{Cylinder}, are represented as vectors in the shape embedding space. These concept vectors are also learned along the process. We measure the cosine distances  between these vectors to determine the shape of the object. Specifically, we compute the probability that an object  is a cube by
,
where ShapeOf denotes the neural operator,  the concept embedding of \texttt{Cube} and  the Sigmoid function.  and  are scalar constants for scaling and shifting the values of similarities. We classify relational concepts (\eg, \texttt{Left}) between a pair of objects similarly, except that we concatenate the visual representations for both objects to form the representation of their relation.

\myparagraph{DSL and semantic parsing.}

The semantic parsing module translates a natural language question into an executable program with a hierarchy of primitive operations, represented in a domain-specific language (DSL) designed for VQA. The DSL covers a set of fundamental operations for visual reasoning, such as filtering out objects with certain concepts or query the attribute of an object. The operations share the same input and output interface, and thus can be compositionally combined to form programs of any complexity. We include a complete specification of the DSL used by our framework in the Appendix~\ref{sec:app:dsl}.

Our semantic parser generates the hierarchies of latent programs in a sequence to tree manner \citep{Dong2016Language}. We use a bidirectional GRU \citep{Cho2014Learning} to encode an input question, which outputs a fixed-length embedding of the question. A decoder based on GRU cells is applied to the embedding, and recovers the hierarchy of operations as the latent program. Some operations takes concepts their parameters, such as \texttt{Filter}(~\underline{\texttt{Red}}~) and \texttt{Query}(~\underline{\texttt{Shape}}~). These concepts are chosen from all concepts appeared in the input question. \fig{fig:nscl}(B) shows an example, while more details can be found in Appendix~\ref{sec:app:semantic}.





\myparagraph{Quasi-symbolic program execution.}

Given the latent program recovered from the question in natural language, a symbolic program executor executes the program and derives the answer based on the object-based visual representation. Our program executor is a collection of deterministic functional modules designed to realize all logic operations specified in the DSL. \fig{fig:nscl}(B) shows an illustrative execution trace of a program.

To make the execution differentiable \wrt visual representations, we represent the intermediate results in a probabilistic manner: a set of objects is represented by a vector, as the attention mask over all objects in the scene. Each element,  denotes the probability that the -th object of the scene belongs to the set.
For example, shown in \fig{fig:nscl}(B), the first \texttt{Filter} operation outputs a mask of length  (there are in total 4 objects in the scene), with each element representing the probability that the corresponding object is selected out (\ie, the probability that each object is a green cube). The output ``mask'' on the objects will be fed into the next module (\texttt{Relate} in this case) as input and the execution of programs continues. The last module outputs the final answer to the question. We refer interested readers to Appendix~\ref{sec:app:execution} for the implementation of all operators.






\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/NSCL.pdf}
\vspace{-15pt}
\caption{{\bf A.} Demonstration of the curriculum learning of visual concepts, words, and semantic parsing of sentences by watching images and reading paired questions and answers. Scenes and questions of different complexities are illustrated to the learner in an incremental manner. {\bf B.} Illustration of our neuro-symbolic inference model for VQA. The perception module begins with parsing visual scenes into object-based deep representations, while the semantic parser parse sentences into executable programs. A symbolic execution process bridges two modules.}
\vspace{-5pt}
\label{fig:nscl}
\end{figure}
 
\subsection{Training paradigm}

\paragraph{Optimization objective.}

The optimization objective of \model is composed of two parts: concept learning and language understanding. Our goal is to find the optimal parameters  of the visual perception module  (including the ResNet-34 for extracting object features, attribute operators. and concept embeddings) and  of the semantic parsing module , to maximize the likelihood of answering the question  correctly:

where  denotes the program,  the answer,  the scene, and  the quasi-symbolic executor. The expectation is taken over .

Recall the program executor is fully differentiable \wrt the visual representation. We compute the gradient \wrt  as
. We use REINFORCE~\citep{Williams1992Simple} to optimize  the semantic parser :

where the reward  if the answer is correct and 0 otherwise. We also use off-policy search to reduce the variance of REINFORCE, the detail of which can be found in Appendix~\ref{sec:app:reinforce}.

\myparagraph{Curriculum visual concept learning.}
\label{subsec:curriculum}
Motivated by human concept learning as in \fig{fig:teaser}, we employ a curriculum learning approach to help joint optimization. We heuristically split the training samples into four stages (\fig{fig:nscl}(A)): first, learning object-level visual concepts; second, learning relational questions; third, learning more complex questions with perception modules fixed; fourth, joint fine-tuning of all modules. We found that this is essential to the learning of our \modelfull.
We include more technical details in Appendix~\ref{sec:app:curriculum}.
\section{Experiments}

We demonstrate the following advantages of our \model. First, it learns visual concepts with remarkable accuracy; second, it allows data-efficient visual reasoning on the CLEVR dataset \citep{Johnson2017CLEVR}; third, it generalizes well to new attributes, visual composition, and language domains.

We train \model on 5K images (10\% of CLEVR's 70K training images). We generate 20 questions for each image for the entire curriculum learning process. The Mask R-CNN module is pretrained on 4K generated CLEVR images with bounding box annotations, following \citet{kexin}.

\subsection{Visual concept learning}

\paragraph{Classification-based concept evaluation.}

Our model treats attributes as neural operators that map latent object representations into an attribute-specific embedding space (\fig{fig:vse}). We evaluate the concept quantization of objects in the CLEVR validation split. Our model can achieve near perfect classification accuracy (99\%) for all object properties, suggesting it effectively learns generic concept representations. The result for spatial relations is relatively lower, because CLEVR does not have direct queries on the spatial relation between objects. Thus, spatial relation concepts can only be learned indirectly.



\myparagraph{Count-based concept evaluation.}
The SOTA methods do not provide interpretable representation on individual objects~\citep{Johnson2017CLEVR,Hudson2018Compositional,Mascharka2018Transparency} . To evaluate the visual concepts learned by such models, we generate a synthetic question set. The diagnostic question set contains simple questions as the following form: ``How many \texttt{red} objects are there?''. We evaluate the performance on all concepts appeared in the CLEVR dataset.

\tbl{tab:expr:concept-count} summarizes the results compared with strong baselines, including methods based on convolutional features~\citep{Johnson2017Inferring} and those based on neural attentions~\citep{Mascharka2018Transparency,Hudson2018Compositional}.
Our approach outperforms IEP by a significant margin (8\%) and attention-based baselines by 2\%, suggesting object-based visual representations and symbolic reasoning helps to interpret visual concepts.

\begin{figure*}[t]
\vspace{-15pt}
\begin{minipage}{0.48\textwidth}
    \centering\small
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lccccccc}
        \toprule
        & \thead{Visual} & \thead{Mean} & \thead{Color} & \thead{Mat.} & \thead{Shape} & \thead{Size} \\ \midrule
        IEP & Conv. & 90.6 & 91.0 & 90.0 & 89.9 & 90.6 \\ \midrule
        MAC & Attn. & 95.9 & 98.0 & 91.4 & 94.4 & 94.2 \\
        TbD (hres.) & Attn. & 96.5 & 96.6 & 92.2 & 95.4 & 92.6 \\ \midrule
        \model & Obj. & \textbf{98.7} & \textbf{99.0} & \textbf{98.7} & \textbf{98.1} & \textbf{99.1} \\ \bottomrule
    \end{tabular}
    \captionof{table}{We also evaluate the learned visual concepts using a diagnostic question set containing simple questions such as ``How many \texttt{red} objects are there?''. \model outperforms both convolutional and attentional baselines. The suggested object-based visual representation and symbolic reasoning approach perceives better interpretation of visual concepts.}
    \label{tab:expr:concept-count}
\end{minipage}
 \hfill
\begin{minipage}{0.48\textwidth}
    \vspace{0pt}
    \centering
    \small
    \setlength{\tabcolsep}{2pt}
\begin{tabular}{lcccc}
    \toprule
        \thead{Model} & \thead{Visual} & \thead{Accuracy\10\%>99.9\%93.9\%4.6\%6.1\%>64\%<><><>\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\longrightarrow\{\tikz\draw[red,fill=red] (0,0) circle (.5ex);, \tikz\draw[blue,fill=blue] (0,0) rectangle (1ex, 1ex);\}0, 1, 2, \cdots\{\texttt{OEncoder}_i\}f\{c_i\}program \leftarrow \texttt{EmptyProgram}()program.op \leftarrow \texttt{OpDecoder}(f)program.opprogram.concept \leftarrow \texttt{ConceptDecoder}(f, \{c_i\})i = 0, 1, \cdots program.opprogram.input[i] \leftarrow_ifprogram.op\{c_i\}programparsef\{c_i\}\texttt{IEncoder}f_0\{c_i\}parse(f_0, \{c_i\})\{c_i\}Q\{c_i\}Q\{c_i\}parse(f, \{c_i\})op{\tt OpDecoder}(f)op\{c_i\}op\texttt{OEncoder}_0\texttt{OEncoder}_1ff_1f_2parsef_1f_2\texttt{IEncoder}256128256 * 2parse\texttt{ConceptDecoder}\{c_i\}\texttt{OEncoder}_0\texttt{OEncoder}_1<><><><><><><><>f_0\{c_i\} = \{\textnormal{Attribute 1, ObjConcept 1, RelConcept 1, ObjConcept 2}\}parsef_0\texttt{OpDecoder}(f_0) \rightarrow \texttt{Query}\texttt{ConceptDecoder}(f_0) \rightarrow <\textnormal{Attribute 1}>\texttt{OEncoder}_0(f_0, \texttt{Query}) \rightarrow f_1parse(f_1)f_1\texttt{OpDecoder}(f_1) \rightarrow \texttt{Filter}\texttt{ConceptDecoder}(f_1) \rightarrow <\textnormal{ObjConcept 1}>\texttt{OEncoder}_0(f_1, \texttt{Filter}) \rightarrow f_2parse(f_2)f_2\texttt{OpDecoder}(f_2) \rightarrow \texttt{Relate}\texttt{ConceptDecoder}(f_2) \rightarrow <\textnormal{RelConcept 1}>\texttt{OEncoder}_0(f_2, \texttt{Relate}) \rightarrow f_3parse(f_3)f_3\texttt{OpDecoder}(f_3) \rightarrow \texttt{Filter}\texttt{ConceptDecoder}(f_3) \rightarrow <\textnormal{ObjConcept 2}>\texttt{OEncoder}_0(f_3, \texttt{Filter}) \rightarrow f_4parse(f_4)f_4\texttt{OpDecoder}(f_3) \rightarrow \texttt{Scene}\{c_i\}n\mathrm{Object}n\mathrm{Object}_{i} \in [0, 1]\sum_i \mathrm{Object}_i = 1\mathrm{Object}_ii\mathrm{ObjectSet}n\mathrm{ObjectSet}_{i} \in [0, 1]\mathrm{ObjectSet}_ii\mathrm{ObjectSet}\mathrm{Object}\mathrm{Object} = \mathrm{softmax}(\sigma^{-1}(\mathrm{ObjectSet}))\sigma^{-1}(x) = \log(x / (1-x))o_ii\mathnormal{OC}\mathnormal{A}ocv^{oc}b^{oc}|\mathnormal{A}|b^{oc}a \in Au^au^{\texttt{Color}}i\sigmoid\langle \cdot, \cdot \rangle\gamma\taun\mathrm{ObjClassify}(\texttt{Red})\texttt{Left}n\times n\mathrm{RelClassify}(\texttt{Left})\mathrm{RelClassify}(\texttt{Left})_{j, i}ijij\mathrm{AEClassify}(\texttt{Color})\mathrm{AEClassifier}(\texttt{Color})_{j, i}ij\rightarrowoutout_i := 1inoc\rightarrowoutout_i := \min(in_i, \mathrm{ObjClassify}(oc)_i)inrc\rightarrowoutout_i := \sum_{j} (in_j \cdot \mathrm{RelClassify}(rc)_{j, i}))ina\rightarrowoutout_i := \sum_{j} (in_j \cdot \mathrm{AEClassify}(a)_{j, i}))in^{(1)}in^{(2)}\rightarrowoutout_i := \min(in^{(1)}_i, in^{(2)}_i)in^{(1)}in^{(2)}\rightarrowoutout_i := \max(in^{(1)}_i, in^{(2)}_i)ina\rightarrowout\Pr[out=oc] := \sum_{i} in_i \cdot \displaystyle\frac{\mathrm{ObjClassify}(oc)_i \cdot b^{oc}_{a}}{\sum_{oc'} \mathrm{ObjClassify}(oc')_i \cdot b^{oc'}_{a}}in^{(1)}in^{(2)}a\rightarrowbb := \sum_{i} \sum_j (in^{(1)}_i \cdot in^{(2)}_j \cdot \mathrm{AEClassify}(a)_{j, i}))in\rightarrowbb := \max_i in_iin\rightarrowii := \sum_i in_iin^{(1)}in^{(2)}\rightarrowbb := \sigma\big(( \sum_i in^{(2)}_i - \sum_i in^{(1)}_i - 1 + \gamma_c)/\tau_c\big)in^{(1)}in^{(2)}\rightarrowbb := \sigma\big(( \sum_i in^{(1)}_i - \sum_i in^{(2)}_i - 1 + \gamma_c)/\tau_c\big)in^{(1)}in^{(2)}\rightarrowbb := \sigma\big(( -|\sum_i in^{(1)}_i - \sum_i in^{(2)}_i| + \gamma_c)/(\gamma_c \cdot \tau_c) \big)\gamma_c=0.5\tau_c=0.25\mathbb P(s)s\Theta_sP \sim \mathrm{SemanticParse}(s; {\Theta_s})r\mathbb Q(s)\mathbb Q\mathbb P(s)\mathbb Q\nabla \Theta_s\mathbb Q(s)\mathbb Q(s)\ell = \sum_{p \in \mathbb Q(S)} -\log \Pr(p)\nabla_{\Theta_s}\Pr[p]9~\cdot~~\cdot~> 99.9\%k \times d_{obj}kd_{obj}k = 12MMxxk \times d_{obj}\rightarrow\rightarrow$ Bool: Query if the input object belongs to a set.
\end{enumerate}

\paragraph{Results. } Table~\ref{tab:expr:mc} summarizes the results and Figure~\ref{fig:vis:mc} shows sample execution traces. We compare our method against the NS-VQA baseline~\citep{kexin}, which uses strong supervision for both scene representation (\eg, object categories and positions) and program traces. In contrast, our method learns both by looking at images and reading question-answering pairs. \model outperforms NS-VQA by 5\% in overall accuracy. We attribute the inferior results of NS-VQA to its derendering module. Because objects in the Minecraft world usually occlude with each other, the detected object bounding boxes are inevitably noisy. During the training of the derendering module, each detected bounding box is matched with one of the ground-truth bounding boxes and uses its class and pose as supervision. Poorly localized bounding boxes lead to noisy labels and hurt the accuracy of the derendering module. This further influences the overall performance of NS-VQA.
 
\begin{table}[th]
\centering
    \begin{tabular}{l ccccc}
    \toprule
       \thead{Model} & \thead{Overall} & \thead{Count} & \thead{Exist} & \thead{Belong} & \thead{Query} \\ \midrule
NS-VQA & 87.7 & 83.3 & 91.5 & 91.1 & 86.4 \\
\model & \textbf{93.3} & \textbf{91.3} & \textbf{95.6} & \textbf{93.9} & \textbf{94.3} \\ \bottomrule
    \end{tabular}
    \caption{Our model achieves comparable results on the Minecraft dataset with baselines trained by full program annotations.}
    \label{tab:expr:mc}
\end{table}



\subsection{VQS Dataset}

\label{sec:app:vqs}
We conduct experiments on the VQS dataset~\citep{Gan2017Vqs}. VQS is a subset of the VQA 1.0 dataset~\citep{Antol2015Vqa}. It contains questions that can be visually grounded: each question is associated with multiple image regions, annotated by humans as necessary for answering the question.

\begin{figure}[thbp]
    \centering
    \includegraphics[width=0.7\textwidth]{raw/VQAExample.pdf}
    \captionof{figure}{An example image from the VQS dataset. The orange bounding boxes are object proposals. On the right, we show the original question and answer in natural language, as well as the latent program recovered by our parser. To answer this question, models are expected to attend to the man and his pen in the pocket.}
    \label{fig:vqs:example}
\end{figure}

\paragraph{Setup. } All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation. We use the multiple-choice setup for VQA: the models choose their most confident answer from 18 candidate answers for each question.

To obtain the latent programs from natural languages, we use a pre-trained syntactic dependency parser~\citep{Andreas2016Learning,Schuster2015Generating} for extracting programs and concepts that need to be learned. A sample question and the program obtained by our parser is shown in Figure~\ref{fig:vqs:example}. The concept embeddings are initialized by the bag of words (BoW) over the GloVe word embeddings~\citep{pennington2014glove}.

\paragraph{Baselines. } We compare our model against two representative baselines: MLP~\citep{Jabri2016Revisiting} and MAC~\citep{Hudson2018Compositional}.

MLP is a standard baseline for visual-question answering, which treats the multiple-choice task as a ranking problem. For a specific candidate answer, a multi-layer perceptron (MLP) model is used to encode a tuple of the image, the question, and the candidate answer. The MLP outputs a score for each tuple, and the answer to the question is the candidate with the highest score. We encode the image with a ResNet-34 pre-trained on ImageNet and use BoW over the GloVe word embeddings for the question and option encoding.

We slightly modify the MAC network for the VQS dataset. For each candidate answer, we concatenate the question and the answer as the input to the model. The MAC model outputs a score from 0 to 1 and the answer to the question is the candidate with the highest score. The image features are extracted from the same ResNet-34 model.

\paragraph{Results. }
Table~\ref{fig:vqs:execution} summarizes the results. \model achieves comparable results with the MLP baseline and the MAC network designed for visual reasoning. Our model also brings transparent reasoning over natural images and language. Example execution traces generated by \model are shown in Figure~\ref{fig:vis:vqs:1}. Besides, the symbolic reasoning process helps us to inspect the model and diagnose the error sources. See the caption for details. 

\section{Visualization of execution traces and visual concepts}
\label{sec:app:concept}

Another appealing benefit is that our reasoning model enjoys full interpretability. Figure~\ref{fig:visualization}, Figure~\ref{fig:vis:mc}, and Figure~\ref{fig:vis:vqs:1} show \model's execution traces on CLEVR, Minecraft, and VQS, respectively. As a side product, our system detects ambiguous and invalid programs and throws out exceptions. As an example (Figure~\ref{fig:visualization}), the question ``What's the color of the cylinder?'' can be ambiguous if there are multiple cylinders or even invalid if there are no cylinders.

Figure~\ref{fig:vis:clevr:concept} and Figure~\ref{fig:vis:mc:concept} include qualitative visualizations of the concepts learned from the CLEVR and Minecraft datasets, including object categories, attributes, and relations. We choose samples from the validation or test split of each dataset by generating queries of the corresponding concepts. We set a threshold to filter the returned images and objects. For quantitative evaluations of the learned concepts on the CLEVR dataset, please refer to Table~\ref{tab:expr:concept-count} and Table~\ref{tab:expr:retrieval}.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{raw/CLEVRInference.pdf}
\caption{Visualization of the execution trace generated by our Neuro-Symbolic Concept Learner on the CLEVR dataset. Example A and B are successful executions that generate correct answers. In example C, the execution aborts at the first operator. To inspect the reason why the execution engine fails to find the corresponding object, we can read out the visual representation of the object, and locate the error source as the misclassification of the object material. Example D shows how our symbolic execution engine can detect invalid or ambiguous programs during the execution by performing sanity checks.}
\label{fig:visualization}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/MCInference.pdf}
    \caption{Exemplar execution trace generated by our Neuro-Symbolic Concept Learner on the Minecraft reasoning dataset. Example A, B and C are successful execution. Example C demonstrates the semantics of the FilterMost operation. Example D shows a failure case: the detection model fails to detect a pig hiding behind the big tree.}
    \label{fig:vis:mc}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/VQAInference.pdf}
    \caption{Illustrative execution trace generated by our Neuro-Symbolic Concept Learner on the VQS dataset. Execution traces A and B shown in the figure leads to the correct answer to the question. Our model effectively learns visual concepts from data. The symbolic reasoning process brings transparent execution trace and can easily handle quantities (\eg, object counting in Example A). 
In Example C, although \model answers the question correctly, it locates the wrong object during reasoning: a dish instead of the cake. In Example D, our model misclassifies the sport as frisbee.
    }
    \label{fig:vis:vqs:1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/CLEVRConcept.pdf}
    \caption{Concepts learned on the CLEVR dataset.}
    \label{fig:vis:clevr:concept}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/MCConcept.pdf}
    \caption{Concepts learned on the Minecraft dataset.}
    \label{fig:vis:mc:concept}
\end{figure}
} 
\end{document}
