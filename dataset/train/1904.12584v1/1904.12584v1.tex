\documentclass{article} \usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage[ruled,vlined,noresetcount]{algorithm2e}
\usepackage{color,xcolor}
\usepackage{epsfig}
\usepackage{graphicx}

\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{float,wrapfig}
\usepackage{hhline}
\usepackage{multirow}


\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{changepage}
\usepackage{extramarks}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage{soul}
\usepackage{xspace}

\usepackage{url}

\usepackage{enumerate}
\usepackage{todonotes} 

\usepackage{titlesec}

\usepackage{makecell}

\usepackage{pifont} \usepackage{subcaption}




\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\xpar}[1]{\noindent\textbf{#1}\ \ }
\newcommand{\vpar}[1]{\vspace{3mm}\noindent\textbf{#1}\ \ }

\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\sects}[1]{Sections~\ref{#1}}
\newcommand{\eqn}[1]{Equation~\ref{#1}}
\newcommand{\eqns}[1]{Equations~\ref{#1}}
\newcommand{\fig}[1]{Figure~\ref{#1}}
\newcommand{\figs}[1]{Figures~\ref{#1}}
\newcommand{\tbl}[1]{Table~\ref{#1}}
\newcommand{\tbls}[1]{Tables~\ref{#1}}

\newcommand{\ignorethis}[1]{}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\fcseven}{$\mbox{fc}_7$}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\def\naive{na\"{\i}ve\xspace}
\def\Naive{Na\"{\i}ve\xspace}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\iid{\emph{i.i.d}\onedot}
\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{i.e\onedot} \def\Ie{I.e\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{etc\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{et al\onedot}
\makeatother

\definecolor{MyDarkBlue}{rgb}{0,0.08,1}
\definecolor{MyDarkGreen}{rgb}{0.02,0.6,0.02}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{MyDarkOrange}{rgb}{0.40,0.2,0.02}
\definecolor{MyPurple}{RGB}{111,0,255}
\definecolor{MyRed}{rgb}{1.0,0.0,0.0}
\definecolor{MyGold}{rgb}{0.75,0.6,0.12}
\definecolor{MyDarkgray}{rgb}{0.66, 0.66, 0.66}

\newcommand{\jw}[1]{\textcolor{MyDarkGreen}{[Jiajun: #1]}}
\newcommand{\jiayuan}[1]{\textcolor{MyDarkBlue}{[Jiayuan: #1]}}
\newcommand{\gc}[1]{\textcolor{MyDarkRed}{[Chuang: #1]}}
\newcommand{\kyi}[1]{\textcolor{MyDarkOrange}{[Kexin: #1]}}

\newcommand{\modelfull}{neuro-symbolic concept learner\xspace}
\newcommand{\model}{NS-CL\xspace}
\newcommand{\myparagraph}[1]{\vspace{-3pt}\paragraph{#1}}

\newcommand{\revisioncolor}{}



 
\title{The Neuro-Symbolic Concept Learner:\\Interpreting Scenes, Words, and Sentences\\from Natural Supervision}



\iclrfinalcopy



\author{Jiayuan Mao \\
MIT CSAIL and IIIS, Tsinghua University\\
\texttt{mjy14@mails.tsinghua.edu.cn}
\And
Chuang Gan\\
MIT-IBM Watson AI Lab\\
\texttt{ganchuang@csail.mit.edu}
\And
Pushmeet Kohli\\
Deepmind\\
\texttt{pushmeet@google.com}
\And
Joshua B. Tenenbaum\\
MIT BCS, CBMM, CSAIL\\
\texttt{jbt@mit.edu}
\And
Jiajun Wu\\
MIT CSAIL\\
\texttt{jiajunwu@mit.edu}
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval. \footnotetext{Project page: \url{http://nscl.csail.mit.edu}}
\end{abstract}

\section{Introduction}



Humans are capable of learning visual concepts by jointly understanding vision and language \citep{fazly2010probabilistic,chrupala2015learning,gauthier2018word}.
Consider the example shown in \fig{fig:teaser}-I. Imagine that someone with no prior knowledge of colors is presented with the images of the red and green cubes, paired with the questions and answers. They can easily identify the difference in objects' visual appearance (in this case, color), and align it to the corresponding words in the questions and answers (\texttt{Red} and \texttt{Green}). Other object attributes (\eg, shape) can be learned in a similar fashion. Starting from there, humans are able to inductively learn the correspondence between visual concepts and word semantics (\eg, spatial relations and referential expressions, \fig{fig:teaser}-II), and unravel compositional logic from complex questions assisted by the learned visual concepts (\fig{fig:teaser}-III, also see \cite{abend2017bootstrapping}). 







Motivated by this, we propose the \modelfull (\model), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs. \model has three modules: a neural-based perception module that extracts object-level representations from the scene, a visually-grounded semantic parser for translating questions into executable programs, and a symbolic program executor that reads out the perceptual representation of objects, classifies their attributes/relations, and executes the program to obtain an answer.


\model learns from natural supervision (\ie, images and QA pairs), requiring no annotations on images or semantic programs for sentences. Instead, analogical to human concept learning, it learns via curriculum learning. \model starts by learning representations/concepts of individual objects from short questions (\eg, What's the color of the cylinder?) on simple scenes ($\leq$3 objects). By doing so, it learns object-based concepts such as colors and shapes. \model then learns relational concepts by leveraging these object-based concepts to interpret object referrals (\eg, Is there a box right of a cylinder?). The model iteratively adapts to more complex scenes and highly compositional questions. 





\model's modularized design enables interpretable, robust, and accurate visual reasoning: it achieves state-of-the-art performance on the CLEVR dataset~\citep{Johnson2017CLEVR}. More importantly, it naturally learns disentangled visual and language concepts, enabling combinatorial generalization \wrt both visual scenes and semantic programs. In particular, we demonstrate four forms of generalization. First, \model generalizes to scenes with more objects and longer semantic programs than those in the training set. Second, it generalizes to new visual attribute compositions, as demonstrated on the CLEVR-CoGenT~\citep{Johnson2017CLEVR} dataset. Third, it enables fast adaptation to novel visual concepts, such as learning a new color. Finally, the learned visual concepts transfer to new tasks, such as image-caption retrieval, without any extra fine-tuning.



\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/Teaser.pdf}
\caption{Humans learn visual concepts, words, and semantic parsing jointly and incrementally. {\bf I.} Learning visual concepts (red \vs green) starts from looking at simple scenes, reading simple questions, and reasoning over contrastive examples~\citep{fazly2010probabilistic}. {\bf II.} Afterwards, we can interpret referential expressions based on the learned object-based concepts, and learn relational concepts (\eg, on the right of, the same material as). {\bf III} Finally, we can interpret complex questions from visual cues by exploiting the compositional structure.}
\label{fig:teaser}
\vspace{-5pt}
\end{figure}
\section{Related Work}

Our model is related to research on joint learning of vision and natural language. In particular, there are many papers that learn visual concepts from descriptive languages, such as image-captioning or visually-grounded question-answer pairs \citep{kiros2014unifying,shi2018learning,mao2016training,vendrov2015order,ganju2017what}, dense language descriptions for scenes \citep{densecap}, video-captioning \citep{Donahue2015Long} and video-text alignment \citep{zhu2015aligning}.

Visual question answering (VQA) stands out as it requires understanding both visual content and language. The state-of-the-art approaches usually use neural attentions~\citep{malinowski2014multi,chen2015abc,Yang2016Stacked,xu2016ask}.  Beyond question answering, \cite{Johnson2017CLEVR} proposed the CLEVR (VQA) dataset to diagnose reasoning models. CLEVR contains synthetic visual scenes and questions generated from latent programs. \tbl{tab:related-vqa} compares our model with state-of-the-art visual reasoning models~\citep{Andreas2016Learning,Suarez2018DDRprog,Santoro2017simple} along four directions: visual features, semantics, inference, and the requirement of extra labels.

For visual representations, \cite{Johnson2017Inferring} encoded visual scenes into a convolutional feature map for program operators. \cite{Mascharka2018Transparency,Hudson2018Compositional} used attention as intermediate representations for transparent program execution. Recently, \cite{kexin} explored an interpretable, object-based visual representation for visual reasoning. It performs well, but requires fully-annotated scenes during training. Our model also adopts an object-based visual representation, but the representation is learned only based on natural supervision (questions and answers).

{\revisioncolor \cite{Anderson2017BottomUp} also proposed to represent the image as a collection of convolutional object features and gained substantial improvements on VQA. Their model encodes questions with neural networks and answers the questions by question-conditioned attention over the object features. In contrast, \model parses question inputs into programs and executes them on object features to get the answer. This makes the reasoning process interpretable and supports combinatorial generalization over quantities (\eg, counting objects). Our model also learns general visual concepts and their association with symbolic representations of language. These learned concepts can then be explicitly interpreted and deployed in other vision-language applications such as image caption retrieval.}


There are two types of approaches in semantic sentence parsing for visual reasoning: implicit programs as conditioned neural operations (\eg, conditioned convolution and dual attention)~\citep{Perez2017Film,Hudson2018Compositional} and explicit programs as sequences of symbolic tokens~\citep{Andreas2016Learning,Johnson2017Inferring,Mascharka2018Transparency}. As a representative, \cite{Andreas2016Learning} build modular and structured neural architectures based on programs for answering questions.
Explicit programs gain better interpretability, but usually require extra supervision such as ground-truth program annotations for training. This restricts their application. We propose to use visual grounding as distant supervision to parse questions in natural languages into explicit programs, with {\it zero} program annotations. Given the semantic parsing of questions into programs, \cite{kexin} proposed a purely symbolic executor for the inference of the answer in the logic space. Compared with theirs, we propose a quasi-symbolic executor for VQA.


\begin{table}[t]
\centering\small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Models} & \multirow{2}{*}{Visual Features} & \multirow{2}{*}{Semantics} & \multicolumn{2}{c}{Extra Labels} & \multirow{2}{*}{Inference} \\ 
        \cmidrule{4-5}
        & & & \# Prog. & Attr. & \\
        \midrule
        FiLM \citep{Perez2017Film} & Convolutional & Implicit & 0 & No & Feature Manipulation\\
        IEP \citep{Johnson2017Inferring} & Convolutional & Explicit & 700K & No &  Feature Manipulation\\
        \midrule
        MAC \citep{Hudson2018Compositional} & Attentional & Implicit & 0 & No & Feature Manipulation\\
        Stack-NMN \citep{Hu2018Explainable} & Attentional & Implicit & 0 & No &  Attention Manipulation\\
        TbD \citep{Mascharka2018Transparency} & Attentional & Explicit & 700K & No &  Attention Manipulation\\
        \midrule
        NS-VQA \citep{kexin} & Object-Based & Explicit & 0.2K & Yes & Symbolic Execution\\ 
        \model & Object-Based & Explicit & 0 & No & Symbolic Execution \\
        \bottomrule
    \end{tabular}
    \caption{Comparison with other frameworks on the CLEVR VQA dataset, \wrt visual features, implicit or explicit semantics and supervisions.}
\label{tab:related-vqa}
\end{table} 
Our work is also related to learning interpretable and disentangled representations for visual scenes using neural networks. \citet{kulkarni2015deep} proposed convolutional inverse graphics networks for learning and inferring pose of faces, while \citet{Yang2015Weakly} learned disentangled representation of pose of chairs from images. \citet{Wu2017Neural} proposed the neural scene de-rendering framework as an inverse process of any rendering process. \citet{Siddharth2017Learning,Higgins2018SCAN} learned disentangled representations using deep generative models. In contrast, we propose an alternative representation learning approach through joint reasoning with language.

\section{Neuro-Symbolic concept learner}
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/Framework.pdf}
\caption{We propose to use neural symbolic reasoning as a bridge to jointly learn visual concepts, words, and semantic parsing of sentences.}
\vspace{-5pt}
\label{fig:framework}
\end{figure}
 
We present our \modelfull, which uses a symbolic reasoning process to bridge the learning of visual concepts, words, and semantic parsing of sentences without explicit annotations for any of them.
We first use a visual perception module to construct an object-based representation for a scene, and run a semantic parsing module to translate a question into an executable program. We then apply a quasi-symbolic program executor to infer the answer based on the scene representation. We use paired images, questions, and answers to jointly train the visual and language modules.

Shown in \fig{fig:framework}, given an input image, the visual perception module detects objects in the scene and extracts a deep, latent representation for each of them. The semantic parsing module translates an input question in natural language into an executable program given a domain specific language (DSL). The generated programs have a hierarchical structure of symbolic, functional modules, each fulfilling a specific operation over the scene representation. The explicit program semantics enjoys compositionality, interpretability, and generalizability.



The program executor executes the program upon the derived scene representation and answers the question. Our program executor works in a symbolic and deterministic manner.
This feature ensures a transparent execution trace of the program.
Our program executor has a fully differentiable design \wrt the visual representations and the concept representations, which supports gradient-based optimization during training.




\begin{figure}[t]
\centering \includegraphics[width=\textwidth]{raw/VSE.pdf}
\caption{We treat attributes such as \texttt{Shape} and \texttt{Color} as neural operators. The operators map object representations into a visual-semantic space. We use similarity-based metric to classify objects.}
\vspace{-5pt}
\label{fig:vse}
\end{figure}
 
\subsection{Model details}

\paragraph{Visual perception.}
Shown in \fig{fig:framework}, given the input image, we use a pretrained Mask R-CNN \citep{He2017Mask} to generate object proposals for all objects. The bounding box for each single object paired with the original image is then sent to a ResNet-34~\citep{He2015Deep} to extract the region-based (by RoI Align) and image-based features respectively. We concatenate them to represent each object. Here, the inclusion of the representation of the full scene adds the contextual information, which is essential for the inference of relative attributes such as size or spatial position.

\myparagraph{Concept quantization.}
Visual reasoning requires determining an object's attributes (\eg, its color or shape). We assume each visual attribute (\eg, shape) contains a set of visual concept (\eg, \texttt{Cube}). In \model, visual attributes are implemented as neural operators
, mapping the object representation into an attribute-specific embedding space. \fig{fig:vse} shows an inference an object's shape. Visual concepts that belong to the shape attribute, including \texttt{Cube}, \texttt{Sphere} and \texttt{Cylinder}, are represented as vectors in the shape embedding space. These concept vectors are also learned along the process. We measure the cosine distances $\langle \cdot , \cdot \rangle$ between these vectors to determine the shape of the object. Specifically, we compute the probability that an object $o_i$ is a cube by
$\sigmoid \left({~\langle \mathrm{\small ShapeOf}(o_i), v^{\texttt{Cube}} \rangle - \gamma}~\right) \big/ \tau$,
where ShapeOf$(\cdot)$ denotes the neural operator, $v^{\texttt{Cube}}$ the concept embedding of \texttt{Cube} and $\sigmoid$ the Sigmoid function. $\gamma$ and $\tau$ are scalar constants for scaling and shifting the values of similarities. We classify relational concepts (\eg, \texttt{Left}) between a pair of objects similarly, except that we concatenate the visual representations for both objects to form the representation of their relation.

\myparagraph{DSL and semantic parsing.}

The semantic parsing module translates a natural language question into an executable program with a hierarchy of primitive operations, represented in a domain-specific language (DSL) designed for VQA. The DSL covers a set of fundamental operations for visual reasoning, such as filtering out objects with certain concepts or query the attribute of an object. The operations share the same input and output interface, and thus can be compositionally combined to form programs of any complexity. We include a complete specification of the DSL used by our framework in the Appendix~\ref{sec:app:dsl}.

Our semantic parser generates the hierarchies of latent programs in a sequence to tree manner \citep{Dong2016Language}. We use a bidirectional GRU \citep{Cho2014Learning} to encode an input question, which outputs a fixed-length embedding of the question. A decoder based on GRU cells is applied to the embedding, and recovers the hierarchy of operations as the latent program. Some operations takes concepts their parameters, such as \texttt{Filter}(~\underline{\texttt{Red}}~) and \texttt{Query}(~\underline{\texttt{Shape}}~). These concepts are chosen from all concepts appeared in the input question. \fig{fig:nscl}(B) shows an example, while more details can be found in Appendix~\ref{sec:app:semantic}.





\myparagraph{Quasi-symbolic program execution.}

Given the latent program recovered from the question in natural language, a symbolic program executor executes the program and derives the answer based on the object-based visual representation. Our program executor is a collection of deterministic functional modules designed to realize all logic operations specified in the DSL. \fig{fig:nscl}(B) shows an illustrative execution trace of a program.

To make the execution differentiable \wrt visual representations, we represent the intermediate results in a probabilistic manner: a set of objects is represented by a vector, as the attention mask over all objects in the scene. Each element, $\mathrm{Mask}_i \in [0, 1]$ denotes the probability that the $i$-th object of the scene belongs to the set.
For example, shown in \fig{fig:nscl}(B), the first \texttt{Filter} operation outputs a mask of length $4$ (there are in total 4 objects in the scene), with each element representing the probability that the corresponding object is selected out (\ie, the probability that each object is a green cube). The output ``mask'' on the objects will be fed into the next module (\texttt{Relate} in this case) as input and the execution of programs continues. The last module outputs the final answer to the question. We refer interested readers to Appendix~\ref{sec:app:execution} for the implementation of all operators.






\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/NSCL.pdf}
\vspace{-15pt}
\caption{{\bf A.} Demonstration of the curriculum learning of visual concepts, words, and semantic parsing of sentences by watching images and reading paired questions and answers. Scenes and questions of different complexities are illustrated to the learner in an incremental manner. {\bf B.} Illustration of our neuro-symbolic inference model for VQA. The perception module begins with parsing visual scenes into object-based deep representations, while the semantic parser parse sentences into executable programs. A symbolic execution process bridges two modules.}
\vspace{-5pt}
\label{fig:nscl}
\end{figure}
 
\subsection{Training paradigm}

\paragraph{Optimization objective.}

The optimization objective of \model is composed of two parts: concept learning and language understanding. Our goal is to find the optimal parameters $\Theta_{v}$ of the visual perception module $\mathrm{Perception}$ (including the ResNet-34 for extracting object features, attribute operators. and concept embeddings) and $\Theta_{s}$ of the semantic parsing module $\mathrm{SemanticParse}$, to maximize the likelihood of answering the question $Q$ correctly:
\begin{equation}
     \Theta_v, \Theta_s \leftarrow \arg \max_{\Theta_v, \Theta_s} \E_{P}[ \Pr[ A= \mathrm{Executor}(\mathrm{Perception}(S; {\Theta_v}), P)] ], \label{eq:reward}
\end{equation}
where $P$ denotes the program, $A$ the answer, $S$ the scene, and $\mathrm{Executor}$ the quasi-symbolic executor. The expectation is taken over $P \sim \mathrm{SemanticParse}(Q; {\Theta_s})$.

Recall the program executor is fully differentiable \wrt the visual representation. We compute the gradient \wrt $\Theta_v$ as
$ \nabla_{\Theta_v} \E_{P}[\KL(\mathrm{Executor}(\mathrm{Perception}({S}; \Theta_v), P) \| A)]$. We use REINFORCE~\citep{Williams1992Simple} to optimize  the semantic parser $\Theta_s$:
$\nabla_{\Theta_s} = \E_{P}[ r \cdot \log \Pr[P = \mathrm{SemanticParse}(Q; {\Theta_s})]], $
where the reward $r=1$ if the answer is correct and 0 otherwise. We also use off-policy search to reduce the variance of REINFORCE, the detail of which can be found in Appendix~\ref{sec:app:reinforce}.

\myparagraph{Curriculum visual concept learning.}
\label{subsec:curriculum}
Motivated by human concept learning as in \fig{fig:teaser}, we employ a curriculum learning approach to help joint optimization. We heuristically split the training samples into four stages (\fig{fig:nscl}(A)): first, learning object-level visual concepts; second, learning relational questions; third, learning more complex questions with perception modules fixed; fourth, joint fine-tuning of all modules. We found that this is essential to the learning of our \modelfull.
We include more technical details in Appendix~\ref{sec:app:curriculum}.
\section{Experiments}

We demonstrate the following advantages of our \model. First, it learns visual concepts with remarkable accuracy; second, it allows data-efficient visual reasoning on the CLEVR dataset \citep{Johnson2017CLEVR}; third, it generalizes well to new attributes, visual composition, and language domains.

We train \model on 5K images ($<$10\% of CLEVR's 70K training images). We generate 20 questions for each image for the entire curriculum learning process. The Mask R-CNN module is pretrained on 4K generated CLEVR images with bounding box annotations, following \citet{kexin}.

\subsection{Visual concept learning}

\paragraph{Classification-based concept evaluation.}

Our model treats attributes as neural operators that map latent object representations into an attribute-specific embedding space (\fig{fig:vse}). We evaluate the concept quantization of objects in the CLEVR validation split. Our model can achieve near perfect classification accuracy ($\sim$99\%) for all object properties, suggesting it effectively learns generic concept representations. The result for spatial relations is relatively lower, because CLEVR does not have direct queries on the spatial relation between objects. Thus, spatial relation concepts can only be learned indirectly.



\myparagraph{Count-based concept evaluation.}
The SOTA methods do not provide interpretable representation on individual objects~\citep{Johnson2017CLEVR,Hudson2018Compositional,Mascharka2018Transparency} . To evaluate the visual concepts learned by such models, we generate a synthetic question set. The diagnostic question set contains simple questions as the following form: ``How many \texttt{red} objects are there?''. We evaluate the performance on all concepts appeared in the CLEVR dataset.

\tbl{tab:expr:concept-count} summarizes the results compared with strong baselines, including methods based on convolutional features~\citep{Johnson2017Inferring} and those based on neural attentions~\citep{Mascharka2018Transparency,Hudson2018Compositional}.
Our approach outperforms IEP by a significant margin (8\%) and attention-based baselines by $>$2\%, suggesting object-based visual representations and symbolic reasoning helps to interpret visual concepts.

\begin{figure*}[t]
\vspace{-15pt}
\begin{minipage}{0.48\textwidth}
    \centering\small
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lccccccc}
        \toprule
        & \thead{Visual} & \thead{Mean} & \thead{Color} & \thead{Mat.} & \thead{Shape} & \thead{Size} \\ \midrule
        IEP & Conv. & 90.6 & 91.0 & 90.0 & 89.9 & 90.6 \\ \midrule
        MAC & Attn. & 95.9 & 98.0 & 91.4 & 94.4 & 94.2 \\
        TbD (hres.) & Attn. & 96.5 & 96.6 & 92.2 & 95.4 & 92.6 \\ \midrule
        \model & Obj. & \textbf{98.7} & \textbf{99.0} & \textbf{98.7} & \textbf{98.1} & \textbf{99.1} \\ \bottomrule
    \end{tabular}
    \captionof{table}{We also evaluate the learned visual concepts using a diagnostic question set containing simple questions such as ``How many \texttt{red} objects are there?''. \model outperforms both convolutional and attentional baselines. The suggested object-based visual representation and symbolic reasoning approach perceives better interpretation of visual concepts.}
    \label{tab:expr:concept-count}
\end{minipage}
 \hfill
\begin{minipage}{0.48\textwidth}
    \vspace{0pt}
    \centering
    \small
    \setlength{\tabcolsep}{2pt}
\begin{tabular}{lcccc}
    \toprule
        \thead{Model} & \thead{Visual} & \thead{Accuracy\\($100\%$ Data)} & \thead{Accuracy\\($10\%$ Data)}\\
    \midrule
        TbD        & Attn. & 99.1 & 54.2\\
        TbD-Object & Obj.  & 84.1 & 52.6\\
        TbD-Mask   & Attn. & 99.0 & 55.0 \\
        MAC        & Attn. & 98.9 & 67.3 \\
        MAC-Object & Obj.  & 79.5 & 51.2 \\
        MAC-Mask   & Attn. & 98.7 & 68.4 \\
        NS-CL      & Obj.  & {\bf 99.2} & {\bf 98.9} \\
    \bottomrule
    \end{tabular}
    \captionof{table}{We compare different variants of baselines for a systematic study on visual features and data efficiency. Using only 10\% of the training images, our model is able to achieve a comparable results with the baselines trained on the full dataset. See the text for details.}
    \label{tab:expr:systematic}
\end{minipage} \end{figure*}

\subsection{Data-efficient and interpretable visual reasoning}

\model jointly learns visual concepts, words and semantic parsing by watching images and reading paired questions and answers. It can be directly applied to VQA.

\tbl{tab:expr:vqa} summarizes results on the CLEVR validation split. Our model achieves the state-of-the-art performance among all baselines using zero program annotations, including MAC \citep{Hudson2018Compositional} and FiLM \citep{Perez2017Film}. Our model achieves comparable performance with the strong baseline TbD-Nets \citep{Mascharka2018Transparency}, whose semantic parser is trained using 700K programs in CLEVR (ours need 0). The recent NS-VQA model from \cite{kexin} achieves better performance on CLEVR; however, their system requires annotated visual attributes and program traces during training, while our \model needs no extra labels.

Here, the visual perception module is pre-trained on ImageNet~\citep{Deng2009Imagenet}. Without pre-training, the concept learning accuracies drop by 0.2\% on average and the QA accuracy drops by 0.5\%. Meanwhile, \model recovers the underlying programs of questions accurately ($>99.9\%$ accuracy). \model can also detect ambiguous or invalid programs and indicate exceptions. Please see Appendix~\ref{sec:app:visualization} for more details. \model can also be applied to other visual reasoning testbeds. Please refer to Appendix~\ref{sec:app:mc} for our results on the Minecraft dataset \citep{kexin}.

For a systematic study on visual features and data efficiency, we implement two variants of the baseline models: TbD-Object and MAC-Object. Inspired by \citep{Anderson2017BottomUp}, instead of the input image, TbD-Object and MAC-Object take a stack of object features as input. TbD-Mask and MAC-Mask integrate the masks of objects by using them to guide the attention over the images.

Table~\ref{tab:expr:systematic} summarizes the results. Our model outperforms all baselines on data efficiency. This comes from the full disentanglement of visual concept learning and symbolic reasoning: how to execute program instructions based on the learned concepts is programmed. TbD-Object and MAC-Object demonstrate inferior results in our experiments. We attribute this to the design of model architectures and have a detailed analysis in Appendix~\ref{sec:app:efficiency}. Although TbD-Mask and MAC-Mask do not perform better than the originals, we find that using masks to guide attentions speeds up the training. 

Besides achieving a competitive performance on the visual reasoning testbeds, by leveraging both object-based representation and symbolic reasoning, out model learns fully interpretable visual concepts: see Appendix~\ref{sec:app:concept} for qualitative results on various datasets.




\begin{figure*}[t]
\begin{minipage}{0.54\textwidth}
\centering\small
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l cc cccccc}
    \toprule
       \thead{Model} & \thead{Prog.\\ Anno.} & \thead{Overall} & \thead{Count} & \thead{Cmp. \\ Num.} & \thead{Exist} & \thead{Query\\ Attr.} & \thead{Cmp.\\ Attr.} \\ \midrule
Human & N/A & 92.6 & 86.7 & 86.4 & 96.6 & 95.0 & 96.0 \\ \midrule
NMN & 700K & 72.1 & 52.5 & 72.7 & 79.3 & 79.0 & 78.0 \\
N2NMN & 700K & 88.8 & 68.5 & 84.9 & 85.7 & 90.0 & 88.8 \\
IEP & 700K & 96.9 & 92.7 & 98.7 & 97.1 & 98.1 & 98.9 \\
DDRprog & 700K & 98.3 & 96.5 & 98.4 & 98.8 & 99.1 & 99.0 \\
TbD & 700K & \textbf{99.1} & \textbf{97.6} & \textbf{99.4} & \textbf{99.2} & \textbf{99.5} & \textbf{99.6} \\ \midrule
RN & 0 & 95.5 & 90.1 & 93.6 & 97.8 & 97.1 & 97.9 \\
FiLM & 0 & 97.6 & 94.5 & 93.8 & 99.2 & 99.2 & 99.0 \\
MAC & 0 & 98.9 & 97.2 & \textbf{99.4} & \textbf{99.5} & 99.3 & \textbf{99.5} \\ \midrule
\model & 0 & \textbf{98.9} & \textbf{98.2} & 99.0 & 98.8 & \textbf{99.3} & 99.1 \\ \bottomrule
    \end{tabular}
    \captionof{table}{Our model outperforms all baselines using no program annotations. It achieves comparable results with models trained by full program annotations such as TbD.}
\label{tab:expr:vqa}
\end{minipage}
 \hfill
\begin{minipage}{0.4\textwidth}
\vspace{0pt}
\centering\small
\setlength{\tabcolsep}{2pt}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c}{Test}\\
\cmidrule{2-5}
 & Split A & Split B & Split C & Split D \\ \midrule
MAC  & 97.3    & N/A  & 92.9 & N/A     \\
IEP  & 96.1    & 92.1 & 91.5 &   90.9    \\
TbD  & 98.8  & 94.5 & 94.3 & 91.9    \\ \midrule
\model      & \textbf{98.9}    & \textbf{98.9} & \textbf{98.7} & \textbf{98.8}    \\ \bottomrule
\end{tabular}
\caption{We test the combinatorial generalization \wrt the number of objects in scenes and the complexity of questions (\ie the depth of the program trees). We makes four split of the data containing various complexities of scenes and questions. Our object-based visual representation and explicit program semantics enjoys the best (and almost-perfect) combinatorial generalization compared with strong baselines.}
\label{tab:expr:combinatorial}
\end{minipage}

\end{figure*}

\subsection{Generalization to new attributes and compositions}
\label{sec:expr:comb}

\paragraph{Generalizing to new visual compositions.} The CLEVR-CoGenT dataset is designed to evaluate models' ability to generalize to new visual compositions. It has two splits: Split A only contains gray, blue, brown and yellow cubes, but red, green, purple, and cyan cylinders; split B imposes the opposite color constraints on cubes and cylinders. If we directly learn visual concepts on split A, it overfits to classify shapes based on the color, leading to a poor generalization to split B.

Our solution is based on the idea of seeing attributes as operators. Specifically, we jointly train the concept embeddings (\eg, \texttt{Red}, \texttt{Cube}, \etc) as well as the semantic parser on split A, keeping pre-trained, frozen attribute operators. As we learn distinct representation spaces for different attributes, our model achieves an accuracy of 98.8\% on split A and 98.9\% on split B.

\myparagraph{Generalizing to new visual concepts.} We expect the process of concept learning can take place in an incremental manner: having learned 7 different colors, humans can learn the 8-th color incrementally and efficiently. To this end, we build a synthetic split of the CLEVR dataset to replicate the setting of incremental concept learning. Split A contains only images without any purple objects, while split B contains images with at least one purple object. We train all the models on split A first, and finetune them on 100 images from split B. We report the final QA performance on split B's validation set. All models use a pre-trained semantic parser on the full CLEVR dataset. 

Our model performs a $93.9\%$ accuracy on the QA test in Split B, outperforming the convolutional baseline IEP \citep{Johnson2017Inferring} and the attentional baseline TbD \citep{Mascharka2018Transparency} by $4.6\%$ and $6.1\%$ respectively. The acquisition of \texttt{Color} operator brings more efficient learning of new visual concepts.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{raw/ExampleCombinatorial.pdf}
\vspace{-20pt}
\caption{Samples collected from four splits in \sect{sec:expr:comb} for illustration. Models are trained on split A but evaluated on all splits for testing the combinatorial generalization.}
\vspace{-5pt}
\label{fig:example-comb}
\end{figure}
 
\subsection{Combinatorial generalization to new scenes and questions}

Having learned visual concepts on small-scale scenes (containing only few objects) and simple questions (only single-hop questions), we humans can easily generalize the knowledge to larger-scale scenes and to answer complex questions. To evaluate this, we split the CLEVR dataset into four parts: \textbf{Split A} contains only scenes with less than 6 objects, and questions whose latent programs having a depth less than 5; \textbf{Split B} contains scenes with less than 6 objects, but arbitrary questions; \textbf{Split C} contains arbitrary scenes, but restricts the program depth being less than 5; \textbf{Split D} contains arbitrary scenes and questions. Figure~\ref{fig:example-comb} shows some illustrative samples.

As VQA baselines are unable to count a set of objects of arbitrary size, for a fair comparison, all programs containing the ``count'' operation over $>6$ objects are removed from the set. For methods using explicit program semantics, the semantic parser is pre-trained on the full dataset and fixed. Methods with implicit program semantics~\citep{Hudson2018Compositional} learn an entangled representation for perception and reasoning, and cannot trivially generalize to more complex programs. We only use the training data from the Split A and then quantify the generalization ability on other three splits. Shown in \tbl{tab:expr:combinatorial}, our \model leads to almost-perfect generalization to larger scenes and more complex questions, outperforming all baselines by at least $4\%$ in QA accuracy. 


\subsection{Extending to other program domain}

\begin{table}[t]
\centering\vspace{2pt}
    \begin{subfigure}[*]{0.25\textwidth}
    \includegraphics[width=\textwidth]{raw/ExampleRetrieval.pdf}
    \caption{An illustrative pair of image and caption in our synthetic dataset.}
    \end{subfigure}
    ~
    \begin{subfigure}[*]{0.33\textwidth}
        \centering\small
        \begin{tabular}{lc}
        \toprule
        Model & Retrieval Accuracy \\ \midrule
        IEP & 95.5 \\
        TbD & \textbf{97.0} \\
        \model & 96.9 \\ \bottomrule
        \end{tabular}
        \caption{Image-caption retrieval accuracy on a subset of data.  Our model archives comparable results with VQA baselines.}
        \label{tab:expr:retrieval:a}
    \end{subfigure}~ 
    \begin{subfigure}[*]{0.33\textwidth}
        \centering\small
        \begin{tabular}{lc}
        \toprule
        Model & Retrieval Accuracy \\ \midrule
        CNN-LSTM & 68.9 \\
        \model & \textbf{97.0} \\ \bottomrule
        \end{tabular}
        \caption{Image-caption retrieval accuracy on the full dataset. Our model outperforms  baselines and requires no extra training or fine-tuning of the visual perception module.}
        \label{tab:expr:retrieval:b}
    \end{subfigure}
    \caption{We introduce a new simple DSL for image-caption retrieval to evaluate how well the learned visual concepts transfer. Due to the difference between VQA and caption retrieval, VQA baselines are only able to infer the result on a partial set of data. The learned object-based visual concepts can be directly transferred into the new domain for free.}
    \label{tab:expr:retrieval}
    \vspace{-5pt}
\end{table}
 The learned visual concepts can also be used in other domains such as image retrieval. With the visual scenes fixed, the learned visual concepts can be directly transferred into the new domain. We only need to learn the semantic parsing of natural language into the new DSL.

We build a synthetic dataset for image retrieval and adopt a DSL from scene graph--based image retrieval~\citep{Johnson2015Image}. The dataset contains only simple captions: ``There is an $<$object A$>$ $<$relation$>$ $<$object B$>$.'' (\eg, There is a box right of a cylinder). The semantic parser learns to extract corresponding visual concepts (\eg, \texttt{box}, \texttt{right}, and \texttt{cylinder}) from the sentence. The program can then be executed on the visual representation to determine if the visual scene contains such relational triples.

For simplicity, we treat retrieval as classifying whether a relational triple exists in the image. This functionality cannot be directly implemented on the CLEVR VQA program domain, because questions such as ``Is there a box right of a cylinder'' can be ambiguous if there exist multiple cylinders in the scene. Due to the entanglement of the visual representation with the specific DSL, baselines trained on CLEVR QA can not be directly applied to this task. For a fair comparison with them, we show the result in \tbl{tab:expr:retrieval:a} on a subset of the generated image-caption pairs where the underlying programs have no ambiguity regarding the reference of object B. A separate semantic parser is trained for the VQA baselines, which translates captions into a CLEVR QA-compatible program (\eg, \texttt{Exist(Filter(Box, Relate(Right, Filter(Cylinder)))}.

\tbl{tab:expr:retrieval:b} compares our \model against typical image-text retrieval baselines on the full image-caption dataset. Without any annotations of the sentence semantics, our model learns to parse the captions into the programs in the new DSL. It outperforms the CNN-LSTM baseline by 30\%. 


\subsection{Extending to natural images and language}

\begin{figure}[t]
\begin{minipage}[m]{0.65\textwidth}
    \vspace{0pt}
    \centering
    \includegraphics[width=\textwidth]{raw/VQAExample2.pdf}
\end{minipage}
\hfill
\begin{minipage}[m]{0.32\textwidth}
\centering
    \begin{tabular}{l c}
    \toprule
      Model & Accuracy \\ \midrule
MLP & 43.9 \\
MAC & \textbf{46.2} \\
NS-CL & 44.3 \\ \bottomrule
    \end{tabular}
\label{tab:expr:vqs}
\end{minipage}
    \captionof{figure}{{\bf Left}: An example image-question pair from the VQS dataset and the corresponding execution trace of \model. {\bf Right}: Results on the VQS test set. Our model achieves a comparable results with the baselines.}
    \label{fig:vqs:execution}
\end{figure} \begin{figure}[t]
\centering
    \includegraphics[width=\textwidth]{raw/VQAConcept.pdf}
    \caption{Concepts learned from VQS, including object categories, attributes, and relations.}
    \label{fig:vis:vqs:concept}
\vspace{-5pt}
\end{figure}
 
{\revisioncolor

We further conduct experiments on MS-COCO~\citep{lin2014microsoft} images. Results are presented on the VQS dataset~\citep{Gan2017Vqs}. VQS contains a subset of images and questions from the original VQA 1.0 dataset~\citep{Antol2015Vqa}. All questions in the VQS dataset can be visually grounded: each question is associated with multiple image regions, annotated by humans as essential for answering the question. \fig{fig:vqs:execution} illustrates an execution trace of \model on VQS.

We use a syntactic dependency parser to extract programs and concepts from language~\citep{Andreas2016Learning,Schuster2015Generating}. The object proposals and features are extracted from models pre-trained on the MS-COCO dataset and the ImageNet dataset, respectively. Illustrated in \fig{fig:vqs:execution}, our model shows competitive performance on QA accuracy, comparable with the MLP baseline~\citep{Jabri2016Revisiting} and the MAC network~\citep{Hudson2018Compositional}. Additional illustrative execution traces of \model are in Appendix~\ref{sec:app:concept}. Beyond answering questions, \model effectively learns visual concepts from data. \fig{fig:vis:vqs:concept} shows examples of the learned visual concepts, including object categories, attributes, and relations. Experiment setup and implementation details are in Appendix~\ref{sec:app:vqs}. 

In this paper, we focus on a neuro-symbolic framework that learns visual concepts about object properties and relations. Indeed, visual question answering requires AI systems to reason about more general concepts such as events or activities \citep{Levin1993English}. We leave the extension of \model along this direction and its application to general VQA datasets \citep{Antol2015Vqa} as future work.
}
\section{Discussion and Future Work}
We presented a method that jointly learns visual concepts, words, and semantic parsing of sentences from natural supervision. The proposed framework, \model, learns by looking at images and reading paired questions and answers, without any explicit supervision such as class labels for objects. Our model learns visual concepts with remarkable accuracy. Based upon the learned concepts, our model achieves good results on question answering, and more importantly, generalizes well to new visual compositions, new visual concepts, and new domain specific languages.

{\revisioncolor
The design of \model suggests multiple research directions. First, constructing 3D object-based representations for realistic scenes needs further exploration~\citep{Anderson2017BottomUp,Baradel2018Object}. Second, our model assumes a domain-specific language for describing formal semantics. The integration of formal semantics into the processing of complex natural language would be meaningful future work~\citep{Artzi2013Weakly,Oh2017Zero}. We hope our paper could motivate future research in visual concept learning, language learning, and compositionality. 

Our framework can also be extended to other domains such as video understanding and robotic manipulation. Here, we would need to discover semantic representations for actions and interactions (\eg, push) beyond static spatial relations. Along this direction, researchers have studied building symbolic representations for skills~\citep{Konidaris2018Skills} and learning instruction semantics from interaction~\citep{Oh2017Zero} in constrained setups. Applying neuro-symbolic learning frameworks for concepts and skills would be meaningful future work toward robotic learning in complex interactive environments. }
\myparagraph{Acknowledgements.}
We thank Kexin Yi, Haoyue Shi, and Jon Gauthier for helpful discussions and suggestions. This work was supported in part by the Center for Brains, Minds and Machines (NSF STC award CCF-1231216), ONR MURI N00014-16-1-2007, MIT-IBM Watson AI Lab, and Facebook. 
\bibliography{reference,concept}
\bibliographystyle{iclr2019_conference}

\newpage

\appendix

\section{CLEVR domain-specific language and implementations}
\label{sec:app:dsl}
We first introduce the domain-specific language (DSL) designed for the CLEVR VQA dataset~\citep{Johnson2017CLEVR}. \tbl{tab:clevr-dsl} shows the available operations in the DSL, while \tbl{tab:clevr-typesystem} explains the type system.

\begin{table}[ht]
    \centering
    \begin{tabular}{llp{0.35\columnwidth}} \toprule
        Operation & Signature & Semantics\\ \midrule
        {\tt Scene} & () $\longrightarrow$ ObjectSet & Return all objects in the scene.\\ \midrule
        {\tt Filter} & (ObjectSet, ObjConcept) $\longrightarrow$ ObjectSet & Filter out a set of objects having the object-level concept (\eg, red) from the input object set. \\ \midrule
        {\tt Relate} & (Object, RelConcept) $\longrightarrow$ ObjectSet & Filter out a set of objects that have the relational concept (\eg, left) with the input object. \\ \midrule
        {\tt AERelate} & (Object, Attribute) $\longrightarrow$ ObjectSet & (Attribute-Equality Relate) Filter out a set of objects that have the same attribute value (\eg, same color) as the input object. \\ \midrule
        {\tt Intersection} & (ObjectSet, ObjectSet) $\longrightarrow$ ObjectSet & Return the intersection of two object sets. \\ \midrule
        {\tt Union} & (ObjectSet, ObjectSet) $\longrightarrow$ ObjectSet & Return the union of two object sets.\\ \midrule
        {\tt Query} & (Object, Attribute) $\longrightarrow$ ObjConcept & Query the attribute (\eg, color) of the input object.\\ \midrule
        {\tt AEQuery} & (Object, Object, Attribute) $\longrightarrow$ Bool & (Attribute-Equality Query) Query if two input objects have the same attribute value (\eg, same color).\\ \midrule
        {\tt Exist} & (ObjectSet) $\longrightarrow$ Bool & Query if the set is empty.\\ \midrule
        {\tt Count} & (ObjectSet) $\longrightarrow$ Integer & Query the number of objects in the input set.\\ \midrule
        {\tt CLessThan} & (ObjectSet, ObjectSet) $\longrightarrow$ Bool & (Counting LessThan) Query if the number of objects in the first input set is less than the one of the second set.\\ \midrule
        {\tt CGreaterThan} & (ObjectSet, ObjectSet) $\longrightarrow$ Bool & (Counting GreaterThan) Query if the number of objects in the first input set is greater than the one of the second set.\\ \midrule
        {\tt CEqual} & (ObjectSet, ObjectSet) $\longrightarrow$ Bool & (Counting Equal) Query if the number of objects in the first input set is the same as the one of the second set.\\ \bottomrule
    \end{tabular}
    \caption{All operations in the domain-specific language for CLEVR VQA.}
    \label{tab:clevr-dsl}
\end{table}

\begin{table}[ht]
\vspace{-5pt}
    \centering
    \begin{tabular}{lp{0.25\columnwidth}p{0.35\columnwidth}} \toprule
        Type & Example & Semantics\\ \midrule
        ObjConcept & {\tt Red}, {\tt Cube}, \etc & Object-level concepts. \\ \midrule
        Attribute & {\tt Color}, {\tt Shape}, \etc & Object-level attributes. \\ \midrule
        RelConcept & {\tt Left}, {\tt Front}, \etc & Relational concepts. \\ \midrule
        Object & \tikz\draw[red,fill=red] (0,0) circle (.5ex); & A single object in the scene. \\ \midrule
        ObjectSet & $\{\tikz\draw[red,fill=red] (0,0) circle (.5ex);, \tikz\draw[blue,fill=blue] (0,0) rectangle (1ex, 1ex);\}$ & A set of objects in the scene. \\ \midrule
        Integer & $0, 1, 2, \cdots$ & A single integer. \\ \midrule
        Bool & \texttt{True}, \texttt{False} & A single boolean value. \\ \bottomrule
    \end{tabular}
    \caption{The type system of the domain-specific language for CLEVR VQA.}
    \label{tab:clevr-typesystem}
\end{table}

We note that some function takes Object as its input instead of ObjectSet. These functions require the uniqueness of the referral object. For example, to answer the question ``What's the color of the red object?'', there should be one and only one red object in the scene. During the program execution, the input object set will be implicitly cast to the single object (if the set is non-empty and there is only one object in the set). Such casting is named \texttt{Unique} in related works \citep{Johnson2017Inferring}.

\section{Semantic parsing}
\label{sec:app:semantic}

As shown in Appendix~\ref{sec:app:dsl}, a program can be viewed as a hierarchy of operations which take concepts as their parameters. Thus, \model generates the hierarchies of latent programs in a sequence to tree manner \citep{Dong2016Language}. The semantic parser adopts an encoder-decoder architecture, which contains four neural modules: (1) a bidirectional GRU encoder {\tt IEncoder} \citep{Cho2014Learning} to encode an input question into a fixed-length embedding, (2) an operation decoder {\tt OpDecoder} that determines the operation tokens, such as {\tt Filter}, in the program based on the sentence embedding, (3) a concept decoder {\tt ConceptDecoder} that selects concepts appeared in the input question as the parameters for certain operations (\eg, {\tt Filter} takes an object-level concept parameter while {\tt Query} takes an attribute), and (4) a set of output encoders $\{\texttt{OEncoder}_i\}$ which encode the decoded operations by {\tt OpDecoder} and output the latent embedding for decoding the next operation. The operation decoder, the concept decoder, and the output encoders work jointly and recursively to generate the hierarchical program layout. Algorithm~\ref{alg:semantic-parser} illustrates the algorithmic outline of the semantic parser.

\begin{algorithm}[h]
  \SetKwProg{Fn}{Function}{:}{}
  \SetKwFunction{FParse}{parse}
  \Fn{\FParse{$f$, $\{c_i\}$}}{
    $program \leftarrow \texttt{EmptyProgram}()$\;
    $program.op \leftarrow \texttt{OpDecoder}(f)$\;
    
    \If{$program.op$ \textnormal{requires a concept parameter}}{
      $program.concept \leftarrow \texttt{ConceptDecoder}(f, \{c_i\})$\;
    }
    \For{$i = 0, 1, \cdots $\textnormal{number of non-concept inputs of} $program.op$}{
      $program.input[i] \leftarrow$
      \FParse( \texttt{OEncoder}$_i$($f$, $program.op$) ,$\{c_i\}$ )\;
    }
    
    \Return{$program$}
  }
\caption{The String-to-Tree Semantic Parser.}
\label{alg:semantic-parser}
\end{algorithm}

The function $parse$ takes two inputs: the current decoding state $f$ and all concepts appeared in the question, as a set $\{c_i\}$. The parsing procedure begins with encoding the input question by $\texttt{IEncoder}$ as $f_0$, extracting the concept set $\{c_i\}$ from the input question, and invoking $parse(f_0, \{c_i\})$.

The concept set $\{c_i\}$ is extracted using hand-coded rules. We assume that each concept (including object-level concepts, relational concepts, and attributes) is associated with a single word in the question. For example, the word ``red'' is associated with the object-level concept {\tt Red}, while the word ``shape'' is associated with the attribute {\tt Shape}. Informally, we call these words {\it concept words}. For a given question $Q$, the corresponding concept set $\{c_i\}$ is composed of all occurrences of the {\it concept words} in $Q$.
The set of {\it concept words} is known for the CLEVR dataset. For natural language questions, one could run POS tagging to find all {\it concept words} \citep{Andreas2016Learning,Schuster2015Generating}. We leave the automatic discovery of concept words as a future work \citep{gauthier2018word}. We use the word embedding of the {\it concept words} as the representation for the concepts $\{c_i\}$. Note that, these ``concept embeddings'' are only for the program parsing. The visual module has separate concept embeddings for aligning object features with concepts in the visual-semantic space.

We now delve into the main function $parse(f, \{c_i\})$: we first decode the root operation $op$ of the hierarchy by ${\tt OpDecoder}(f)$. If $op$ requires a concept parameter (an object-level concept, a relational concept, or an attribute), {\tt ConceptDecoder} will be invoked to choose a concept from all concepts $\{c_i\}$. Assuming $op$ takes two non-concept inputs  (\eg, the operation {\tt Intersection} takes two object sets as its input), there will be two branches for this root node. Thus, two output encoders $\texttt{OEncoder}_0$ and $\texttt{OEncoder}_1$ will be applied to transform the current state $f$ into two sub-states $f_1$ and $f_2$. $parse$ will be recursively invoked based on $f_1$ and $f_2$ to generate the two branches respectively. In the DSL, the number of non-concept inputs for any operation is at most 2.

In our implementation, the input encoder $\texttt{IEncoder}$ first maps each word in the question into an embedding space. The word embeddings are composed of two parts: a randomly initialized word embedding of dimension $256$ and a positional embedding of dimension $128$ \citep{Gehring2017Convolutional}. For a {\it concept word}, its word embedding only depends on which type it belongs to (\ie object-level, relational or attribute). Thus, after being trained on a fixed dataset, the semantic parser can parse questions with novel (unseen) {\it concept words}. The sequence of word embeddings is then encoded by a two-layer GRU with a hidden dimension of $256 * 2$ (bidirectional). The function $parse$ starts from the last hidden state of the GRU, and works recursively to generate the hierarchical program layout. Both {\tt OpDecoder} and {\tt ConceptDecoder} are feed-forward networks. $\texttt{ConceptDecoder}$ performs attentions over the representations of all concepts $\{c_i\}$ to select the concepts. Output encoders $\texttt{OEncoder}_0$ and $\texttt{OEncoder}_1$ are implemented as GRU cells. 

Another pre-processing of the sentence is to group consecutive object-level {\it concept words} into a group and treat them together as a single concept, inspired by the notion of ``noun phrases'' in natural languages. The computational intuition behind this grouping is that, the latent programs of CLEVR questions usually contain multiple consecutive \texttt{Filter} tokens. During the program parsing and execution, we aim to fuse all such \texttt{Filter}s into a single \texttt{Filter} operation that takes multiple concepts as its parameter.

\paragraph{A Running Example}
As a running example, consider again the question ``What is the color of the cube right of the red matte object?''. We first process the sentence (by rules) as:
``What is the $<$Attribute 1 (color)$>$ of the $<$(ObjConcept 1 (cube)$>$ $<$RelConcept 1 (right)$>$ of the  $<$ObjConcept 2 (red matte object)$>$?''. The expected parsing result of this sentence is:

\begin{adjustwidth}{4em}{0em}
\texttt{Query}($<$\text{Attribute 1}$>$,\\
\hspace*{1cm} \texttt{Filter}($<$\text{ObjConcept 1}$>$,\\
\hspace*{2cm} \texttt{Relate}($<$\text{RelConcept 1}$>$,\\
\hspace*{3cm} \texttt{Filter}($<$\text{ObjConcept 2}$>$, \texttt{Scene})\\
\hspace*{2cm})\\
\hspace*{1cm})\\
).
\end{adjustwidth}

The semantic parser encode the word embeddings with {\tt IEncoder}. The last hidden state of the GRU will be used as $f_0$. The word embeddings of the {\it concept words} form the set $\{c_i\} = \{\textnormal{Attribute 1, ObjConcept 1, RelConcept 1, ObjConcept 2}\}$. The function $parse$ is then invoked recursively to generate the hierarchical program layout. \tbl{tab:parse-example} illustrates the decoding process step-by-step.

\begin{table}[th]
    \centering
    \begin{tabular}{llp{0.5\columnwidth}l}
    \toprule
        Step & Inputs & Outputs & Recursive Invocation \\ \midrule
        1 & $f_0$ &
        $\texttt{OpDecoder}(f_0) \rightarrow \texttt{Query}$;\newline
        $\texttt{ConceptDecoder}(f_0) \rightarrow <\textnormal{Attribute 1}>$;\newline
        $\texttt{OEncoder}_0(f_0, \texttt{Query}) \rightarrow f_1$
        &$parse(f_1)$\\ \midrule
        2 & $f_1$ &
        $\texttt{OpDecoder}(f_1) \rightarrow \texttt{Filter}$;\newline
        $\texttt{ConceptDecoder}(f_1) \rightarrow <\textnormal{ObjConcept 1}>$;\newline
        $\texttt{OEncoder}_0(f_1, \texttt{Filter}) \rightarrow f_2$
        &$parse(f_2)$\\ \midrule
        3 & $f_2$ &
        $\texttt{OpDecoder}(f_2) \rightarrow \texttt{Relate}$;\newline
        $\texttt{ConceptDecoder}(f_2) \rightarrow <\textnormal{RelConcept 1}>$;\newline
        $\texttt{OEncoder}_0(f_2, \texttt{Relate}) \rightarrow f_3$
        &$parse(f_3)$\\ \midrule
        4 & $f_3$ &
        $\texttt{OpDecoder}(f_3) \rightarrow \texttt{Filter}$;\newline
        $\texttt{ConceptDecoder}(f_3) \rightarrow <\textnormal{ObjConcept 2}>$;\newline
        $\texttt{OEncoder}_0(f_3, \texttt{Filter}) \rightarrow f_4$
        &$parse(f_4)$\\ \midrule
        5 & $f_4$ &
        $\texttt{OpDecoder}(f_3) \rightarrow \texttt{Scene}$;
        & (End of branch.)\\ \bottomrule
    \end{tabular}
    \caption{A step-by-step running example of the recursive parsing procedure. The parameter $\{c_i\}$ is omitted for better visualization.}
    \label{tab:parse-example}
\end{table}

\section{Program Execution}
\label{sec:app:execution}

In this section, we present the implementation of all operations listed in \tbl{tab:clevr-dsl}. We start from the implementation of Object-typed and ObjectSet-typed variables. Next, we discuss how to classify objects by object-level concepts or relational concept, followed by the implementation details of all operations.

\paragraph{Object-typed and ObjectSet-typed variables.} We consider a scene with $n$ objects. An Object-typed variable can be represented as a vector $\mathrm{Object}$ of length $n$, where $\mathrm{Object}_{i} \in [0, 1]$ and $\sum_i \mathrm{Object}_i = 1$. $\mathrm{Object}_i$ can be interpreted as the probability that the $i$-th object of the scene is being referred to. Similarly, an ObjectSet-typed variable can be represented as a vector $\mathrm{ObjectSet}$ of length $n$, where $\mathrm{ObjectSet}_{i} \in [0, 1]$. $\mathrm{ObjectSet}_i$ can be interpreted as the probability that the $i$-the object is in the set. To cast an ObjectSet-typed variable $\mathrm{ObjectSet}$ as an Object-typed variable $\mathrm{Object}$ (\ie, the \texttt{Unique} operation), we compute: $\mathrm{Object} = \mathrm{softmax}(\sigma^{-1}(\mathrm{ObjectSet}))$,
where $\sigma^{-1}(x) = \log(x / (1-x))$ is the logit function.

\paragraph{Concept quantization.} Denote $o_i$ as the visual representation of the $i$-th object, $\mathnormal{OC}$ the set of all object-level concepts, and $\mathnormal{A}$ the set of all object-level attributes. Each object-level concept $oc$ (\eg, \texttt{Red}) is associated with a vector embedding $v^{oc}$ and a L1-normalized vector $b^{oc}$ of length $|\mathnormal{A}|$. $b^{oc}$ represents which attribute does this object-level concept belong to (\eg, the concept \texttt{Red} belongs to the attribute \texttt{Color}). All attributes $a \in A$ are implemented as neural operators, denoted as $u^a$ (\eg, $u^{\texttt{Color}}$). To classify the objects as being \texttt{Red} or not, we compute:
\[\Pr[\text{object $i$ is \texttt{Red}}] = 
\sigmoid \left( ~\sum_{a \in A} \left( b^{\texttt{Red}}_a \cdot \frac{{~\langle u^a(o_i), v_{\texttt{Red}} \rangle - \gamma}~}{\tau} \right) \right), \]
where $\sigmoid$ denotes the Sigmoid function, $\langle \cdot, \cdot \rangle$ the cosine distance between two vectors. $\gamma$ and $\tau$ are scalar constants for scaling and shifting the values of similarities. By applying this classifier on all objects we will obtain a vector of length $n$, denoted as $\mathrm{ObjClassify}(\texttt{Red})$. Similarly, such classification can be done for relational concepts such as $\texttt{Left}$. This will result in an $n\times n$ matrix $\mathrm{RelClassify}(\texttt{Left})$, where $\mathrm{RelClassify}(\texttt{Left})_{j, i}$ is the probability that the object $i$ is left of the object $j$.

To classify whether two objects have the same attribute (\eg, have the same \texttt{Color}), we compute:
\[ \Pr[\text{object $i$ has the same \texttt{Color} as object $j$}] = 
\sigmoid \left( \frac{{~\langle u^{\texttt{Color}}(o_i), u^{\texttt{Color}}(o_j) \rangle - \gamma}~}{\tau} \right),
\]
We can obtain a matrix $\mathrm{AEClassify}(\texttt{Color})$ by applying this classifier on all pairs of objects, where $\mathrm{AEClassifier}(\texttt{Color})_{j, i}$ is the probability that the object $i$ and $j$ have the same \texttt{Color}.

\paragraph{Quasi-symbolic program execution.} Finally, \tbl{tab:program-executor} summarizes the implementation of all operators. In practice, all probabilities are stored in the log space for better numeric stability.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{p{0.42\columnwidth}l} \toprule
        Signature & Implementation\\ \midrule
        {\tt Scene}() $\rightarrow$ $out$: ObjectSet & $out_i := 1$\\ \midrule
        {\tt Filter}($in$: ObjectSet, $oc$: ObjConcept) $\rightarrow$\newline \hspace*{0.8cm}$out$: ObjectSet & $out_i := \min(in_i, \mathrm{ObjClassify}(oc)_i)$\\ \midrule
        {\tt Relate}($in$: Object, $rc$: RelConcept) $\rightarrow$\newline
        \hspace*{0.8cm}$out$: ObjectSet & $out_i := \sum_{j} (in_j \cdot \mathrm{RelClassify}(rc)_{j, i}))$\\ \midrule
        {\tt AERelate}($in$: Object, $a$: Attribute) $\rightarrow$\newline
        \hspace*{0.8cm}$out$: ObjectSet & $out_i := \sum_{j} (in_j \cdot \mathrm{AEClassify}(a)_{j, i}))$\\ \midrule
        {\tt Intersection}($in^{(1)}$: ObjectSet, \newline
        \hspace*{0.8cm}$in^{(2)}$: ObjectSet) $\rightarrow$ $out$: ObjectSet & $out_i := \min(in^{(1)}_i, in^{(2)}_i)$ \\ \midrule
        {\tt Union}($in^{(1)}$: ObjectSet, $in^{(2)}$: ObjectSet) $\rightarrow$\newline
        \hspace*{0.8cm}$out$: ObjectSet & $out_i := \max(in^{(1)}_i, in^{(2)}_i)$ \\ \midrule
        \begin{tabular}{@{}l@{}}{\tt Query}($in$: Object, $a$: Attribute) $\rightarrow$\\
        \hspace*{0.8cm}$out$: ObjConcept\end{tabular} &
        \begin{tabular}{@{}l@{}}
        $\Pr[out=oc] := \sum_{i} in_i \cdot \displaystyle\frac{\mathrm{ObjClassify}(oc)_i \cdot b^{oc}_{a}}{\sum_{oc'} \mathrm{ObjClassify}(oc')_i \cdot b^{oc'}_{a}}$ \end{tabular}
        \\ \midrule
        {\tt AEQuery}($in^{(1)}$: Object, $in^{(2)}$: Object,\newline
        \hspace*{0.8cm} $a$: Attribute) $\rightarrow$ $b$: Bool & $b := \sum_{i} \sum_j (in^{(1)}_i \cdot in^{(2)}_j \cdot \mathrm{AEClassify}(a)_{j, i}))$\\ \midrule
        {\tt Exist}($in$: ObjectSet) $\rightarrow$ $b$: Bool & $b := \max_i in_i$\\ \midrule
        {\tt Count}($in$: ObjectSet) $\rightarrow$ $i$: Integer & $i := \sum_i in_i$\\ \midrule
        {\tt CLessThan}($in^{(1)}$: ObjectSet,\newline
        \hspace*{0.8cm}  $in^{(2)}$: ObjectSet) $\rightarrow$ $b$: Bool & $b := \sigma\big(( \sum_i in^{(2)}_i - \sum_i in^{(1)}_i - 1 + \gamma_c)/\tau_c\big)$\\ \midrule
        {\tt CGreaterThan}($in^{(1)}$: ObjectSet,\newline
        \hspace*{0.8cm}  $in^{(2)}$: ObjectSet) $\rightarrow$ $b$: Bool & $b := \sigma\big(( \sum_i in^{(1)}_i - \sum_i in^{(2)}_i - 1 + \gamma_c)/\tau_c\big)$\\ \midrule
        {\tt CEqual}($in^{(1)}$: ObjectSet,\newline
        \hspace*{0.8cm}  $in^{(2)}$: ObjectSet) $\rightarrow$ $b$: Bool & $b := \sigma\big(( -|\sum_i in^{(1)}_i - \sum_i in^{(2)}_i| + \gamma_c)/(\gamma_c \cdot \tau_c) \big)$\\ \midrule
    \end{tabular}
    \caption{All operations in the domain-specific language for CLEVR VQA. $\gamma_c=0.5$ and $\tau_c=0.25$ are constants for scaling and shift the probability. During inference, one can quantify all operations as \citet{kexin}.}
    \label{tab:program-executor}
\end{table}

\section{Optimization of the Semantic Parser}
\label{sec:app:reinforce}
To tackle the optimization in a non-smooth program space, we apply an off-policy program search process~\citep{Sutton2000Policy} to facilitate the learning of the semantic parser. Denote $\mathbb P(s)$ as the set of all valid programs in the CLEVR DSL for the input question $s$. We want to compute the gradient \wrt $\Theta_s$, the parameters of the semantic parser:
\[\nabla_{\Theta_s} = \nabla_{\Theta_s} \E_{P}[ r \cdot \log \Pr[P]],\]
where $P \sim \mathrm{SemanticParse}(s; {\Theta_s})$. In REINFORCE, we approximate this gradient via Monte Carlo sampling.

An alternative solution is to exactly compute the gradient. Note that in the definition of the reward $r$, only the set of programs $\mathbb Q(s)$ leading to the correct answer will contribute to the gradient term. With the perception module fixed, the set $\mathbb Q$ can be efficiently determined by an off-policy exhaustive search of all possible programs $\mathbb P(s)$. In the third stage of the curriculum learning, we search for the set $\mathbb Q$ offline based on the quantified results of concept classification and compute the exact gradient $\nabla \Theta_s$. An intuitive explanation of the off-policy search is that, we enumerate all possible programs, execute them on the visual representation, and find the ones leading to the correct answer. We use $\mathbb Q(s)$ as the ``groundtruth'' program annotation for the question, to supervise the learning, instead of running the Monte Carlo sampling-based REINFORCE.

\paragraph{Spurious program suppression.}
However, directly using $\mathbb Q(s)$ as the supervision by computing $\ell = \sum_{p \in \mathbb Q(S)} -\log \Pr(p)$ can be problematic, due to the spuriousness or the ambiguity of the programs. This comes from two aspects: \newline
1) intrinsic ambiguity: two programs are different but equivalent. For example
\begin{align*}
    \text{P1: }&\texttt{AEQuery}(\texttt{Color}, \texttt{Filter}(\texttt{Cube}), \texttt{Filter}(\texttt{Sphere})) \text{~and}\\
    \text{P2: }&\texttt{Exist}(\texttt{Filter}(\texttt{Sphere}, \texttt{AERelate}(\texttt{Color}, \texttt{Filter}(\texttt{Cube}))))
\end{align*}
are equivalent.\newline
2) extrinsic spuriousness: one of the program is incorrect, but also leads to the correct answer in a specific scene. For example,
\begin{align*}
    \text{P1: }& \texttt{Filter}(\texttt{Red}, \texttt{Relate}(\texttt{Left}, \texttt{Filter}(\texttt{Sphere}))) \text{~and}\\
    \text{P2: }&\texttt{Filter}(\texttt{Red}, \texttt{Relate}(\texttt{Left}, \texttt{Filter}(\texttt{Cube})))
\end{align*}
may refer to the same red object in a specific scene. Motivated by the REINFORCE process, to suppress such spurious programs, we use the loss function: 
\[ \ell = \sum_{p \in \mathbb{Q}} \text{stop\_gradient}(\Pr[p]) \cdot (-\log \Pr[p]). \]
The corresponding gradient $\nabla_{\Theta_s}$ is,
\[ \nabla_{\Theta_s} = \sum_{p \in \mathbb{Q}} \Pr[p] \cdot \nabla_{\Theta_s}\left(r \cdot \log \Pr[P]\right) = \nabla_{\Theta_s} \left( \sum_{p \in \mathbb{Q}} r \cdot \Pr[p] \right). \]
The key observation is that, given a sufficiently large set of scenes, a program can be identified as spurious if there exists at least one scene where the program leads to a wrong answer. As the training goes, spurious programs will get less update due to the sampling importance term $\Pr[p]$ which weights the likelihood maximization term.

\section{Curriculum Learning Setup}
\label{sec:app:curriculum}

During the whole training process, we gradually add more visual concepts and more complex question examples into the model. 
Summarized in \fig{fig:nscl}(A), in general, the whole training process is split into 3 stages. 
First, we only use questions from lesson 1 to let the model learn object-level visual concepts. Second, we train the model to parse simple questions and to learn relational concepts. In this step, we freeze the neural operators and concept embeddings of object-level concepts. Third, the model gets trained on the full question set (lesson 3), learning to understand questions of different complexities and various format. For the first several iterations in this step, we freeze the parameters in the perception modules. In addition, during the training of all stages, we gradually increase the number of objects in the scene: from 3 to 10.

We select questions for each lesson in the curriculum learning by their depth of the latent program layout. For eaxmple, the program ``\texttt{Query}(\texttt{Shape}, \texttt{Filter}(\texttt{Red}, \texttt{Scene}))'' has the depth of 3, while the program  ``\texttt{Query}(\texttt{Shape},
\texttt{Filter}(\texttt{Cube}, \texttt{Relate}(\texttt{Left}, \texttt{Filter}(\texttt{Red}, \texttt{Scene}))))'' has the depth of 5. Since we have fused  consecutive \texttt{Filter} operations into a single one, the maximum depth of all programs is $9$ on the CLEVR dataset. We now present the detailed split of our curriculum learning lessons:

For lesson 1, we use only programs of depth 3. It contains three types of questions: querying an attribute of the object, querying the existence of a certain type of objects, count a certain type of objects, and querying if two objects have the same attribute (\eg, of the same color). These questions are almost about fundamental object-based visual concepts. For each image, we generate 5 questions of lesson 1.

For lesson 2, we use programs of depth less than 5, containing a number of questions regarding relations, such as querying the attribute of an object that is left of another object. We found that in the original CLEVR dataset, all \texttt{Relate} operations are followed by a \texttt{Filter} operation. This setup degenerates the performance of the learning of relational concepts such as \texttt{Left}. Thus, we add a new question template into the original template set: \texttt{Count}(\texttt{Relate}($~\cdot~$, \texttt{Filter($~\cdot~$, \texttt{Scene})})) (\eg, ``What's the number of objects that are left of the cube?''). For each image, we generate 5 questions of lesson 2.

For lesson 3, we use the full CLEVR question set.

Curriculum learning is crucial for the learning of our \modelfull. We found that by removing the curriculum setup \wrt the number of object in the scenes, the visual perception module will get stuck at an accuracy that is similar to a random-guess model, even if we only use stage-1 questions. If we remove the curriculum setup \wrt the complexity of the programs, the joint training of the visual perception module and the semantic parser can not converge.

\section{Ablation study}
\label{sec:app:visualization}
We conduct ablation studies on the accuracy of semantic parsing, the impacts of the ImageNet pre-training of visual perception modules, the data efficiency of our model, and the usage of object-based representations.

\subsection{Semantic parsing accuracy.}
We evaluate how well our model recovers the underlying programs of questions. Due to the intrinsic equivalence of different programs, we evaluate the accuracy of programs by executing them on the ground-truth annotations of objects. Invalid or ambiguous programs are also considered as incorrect. Our semantic parser archives $> 99.9\%$ QA accuracy on the validation split.

\subsection{Impacts of the ImageNet pre-training.} The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet \citep{Deng2009Imagenet}.  To quantify the influence of this pre-training, we conduct ablation experiments where we randomly initialize the perception module following \citet{He2015Deep}. The classification accuracies of the learned concepts almost remain the same except for \texttt{Shape}. The classification accuracy of \texttt{Shape} drops from 98.7 to 97.5 on the validation set while the overall QA accuracy on the CLEVR dataset drops to 98.2 from 98.9. We speculate that large-scale image recognition dataset can provide prior knowledge of shape.




















{
\revisioncolor
\subsection{Data Efficiency and Object-based Representations}
\label{sec:app:efficiency}
In this section, we study whether and how the number of training samples and feature representations affect the overall performance of various models on the CLEVR dataset. Specifically, we compare the proposed NS-CL against two strong baselines: TbD~\citep{Mascharka2018Transparency} and MAC~\citep{Hudson2018Compositional}.

\paragraph{Baselines. } For comparison, we implement two variants of the baseline models: TbD-Object and MAC-Object. Inspired by \cite{Anderson2017BottomUp}, instead of using a 2D convolutional feature map, TbD-Object and MAC-Object take a stack of object features as inputs, whose shape is $k \times d_{obj}$. $k$ is the number of objects in the scene, and $d_{obj}$ is the feature dimension for a single object. In our experiments, we fix $k = 12$ as a constant value. If there are fewer than 12 objects in the scene, we add ``null'' objects whose features are all-zero vectors.

We extract object features in the same way as \model. Features are extracted from a pre-trained ResNet-34 network before the last residual block for a feature map with high resolution. For each object, its feature is composed of two parts: region-based (by RoI Align) and image-based features. We concatenate them to represent each object. As discussed, the inclusion of the representation of the full scene is essential for the inference of relative attributes such as size or spatial position on the CLEVR domain.

TbD and MAC networks are originally designed to use image-level attention for reasoning. Thus, we implement two more baselines: TbD-Mask and MAC-Mask. Specifically, we replace the original attention module on images with a mask-guided attention. Denotes the union of all object masks as $M$. Before the model applies the attention on the input image, we multiply the original attention map computed by the model with this mask $M$. The multiplication silences the attention on pixels that are not part of any objects.

\paragraph{Results. } Table~\ref{tab:expr:systematic} summarizes the results. We found that TbD-Object and MAC-Object approach show inferior results compared with the original model. We attribute this to the design of the network architectures. Take the {\tt Relate} operation (\eg, finds all objects left of a specific object $x$) as an example. TbD uses a stack of dilated convolutional layers to propagate the attention from object $x$ to others. In TbD-Object, we replace the stack of 2D convolutions by several 1D convolution layers, operating over the $k \times d_{obj}$ object features. This ignores the equivalence of objects (the order of objects should not affect the results). In contrast, MAC networks always use the attention mechanism to extract information from the image representation. This operation is invariant to the order of objects, but is not suitable for handling quantities (\eg, counting objects).

As for TbD-Mask and MAC-Mask, although the mask-guided attention does not improve the overall performance, we have observed noticeably faster convergence during model training. TbD-Mask and MAC-Mask leverage the prior knowledge of object masks to facilitate the attention. Such prior has also been verified to be effective in the original TbD model: TbD employs an attention regularization during training, which encourages the model to attend to smaller regions.

In general, \model is more data-efficient than MAC networks and TbD. Recall that \model answers questions by executing symbolic programs on the learned visual concepts. Only visual concepts (such as \texttt{Red} and \texttt{Left}) and the interpretation of questions (how to translate questions into executable programs) need to be learned from data. In contrast, both TbD and MAC networks need to additionally learn to execute (implicit or explicit) programs such as counting.

For the experiments on the full CLEVR training set, we split 3,500 images (5\% of the training data) as the hold-out validation set to tune the hyperparameters and select the best model. We then apply this model to the CLEVR validation split and report the testing performance. Our model reaches an accuracy of 99.2\% using the CLEVR training set.





\section{Extending to Other Scene and Language Domains}
\label{sec:app:new}

\subsection{Minecraft Dataset}
\label{sec:app:mc}
We also extend the experiments to a new reasoning testbed: Minecraft worlds~\citep{kexin}. The Minecraft reasoning dataset differs from CLEVR in both visual appearance and question types. Figure~\ref{fig:mc:example} gives an example instance from the dataset.

\begin{figure}[thbp]
    \centering
    \includegraphics[width=0.7\textwidth]{raw/MCExample.pdf}
    \caption{An example image and a related question-answering pair from the Minecraft dataset.}
    \label{fig:mc:example}
\end{figure}

\paragraph{Setup. } Following \cite{kexin}, we generate 10,000 Minecraft scenes using the officially open-sourced tools by \cite{Wu2017Neural}. Each image contains 3 to 6 objects. The objects are chosen from 12 categories, with 4 different facing directions (front, back, left and right). They stand on a 2D plane.

Besides different 3D visual appearance and image contexts, the Minecraft reasoning dataset introduces two new types of reasoning operations. We add them to our domain-specific language:

\begin{enumerate}
    \item {\tt FilterMost}(ObjectSet, Concept) $\rightarrow$ ObjectSet: Given a set of objects, finds the ``most'' one. For example, {\tt FilterMost}({\tt Closest}, set) locates the object in the input set that is cloest to the camera (\eg, what is the direction of the closest animal?)
    \item {\tt BelongTo}(Object, ObjectSet) $\rightarrow$ Bool: Query if the input object belongs to a set.
\end{enumerate}

\paragraph{Results. } Table~\ref{tab:expr:mc} summarizes the results and Figure~\ref{fig:vis:mc} shows sample execution traces. We compare our method against the NS-VQA baseline~\citep{kexin}, which uses strong supervision for both scene representation (\eg, object categories and positions) and program traces. In contrast, our method learns both by looking at images and reading question-answering pairs. \model outperforms NS-VQA by 5\% in overall accuracy. We attribute the inferior results of NS-VQA to its derendering module. Because objects in the Minecraft world usually occlude with each other, the detected object bounding boxes are inevitably noisy. During the training of the derendering module, each detected bounding box is matched with one of the ground-truth bounding boxes and uses its class and pose as supervision. Poorly localized bounding boxes lead to noisy labels and hurt the accuracy of the derendering module. This further influences the overall performance of NS-VQA.
 
\begin{table}[th]
\centering
    \begin{tabular}{l ccccc}
    \toprule
       \thead{Model} & \thead{Overall} & \thead{Count} & \thead{Exist} & \thead{Belong} & \thead{Query} \\ \midrule
NS-VQA & 87.7 & 83.3 & 91.5 & 91.1 & 86.4 \\
\model & \textbf{93.3} & \textbf{91.3} & \textbf{95.6} & \textbf{93.9} & \textbf{94.3} \\ \bottomrule
    \end{tabular}
    \caption{Our model achieves comparable results on the Minecraft dataset with baselines trained by full program annotations.}
    \label{tab:expr:mc}
\end{table}



\subsection{VQS Dataset}

\label{sec:app:vqs}
We conduct experiments on the VQS dataset~\citep{Gan2017Vqs}. VQS is a subset of the VQA 1.0 dataset~\citep{Antol2015Vqa}. It contains questions that can be visually grounded: each question is associated with multiple image regions, annotated by humans as necessary for answering the question.

\begin{figure}[thbp]
    \centering
    \includegraphics[width=0.7\textwidth]{raw/VQAExample.pdf}
    \captionof{figure}{An example image from the VQS dataset. The orange bounding boxes are object proposals. On the right, we show the original question and answer in natural language, as well as the latent program recovered by our parser. To answer this question, models are expected to attend to the man and his pen in the pocket.}
    \label{fig:vqs:example}
\end{figure}

\paragraph{Setup. } All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation. We use the multiple-choice setup for VQA: the models choose their most confident answer from 18 candidate answers for each question.

To obtain the latent programs from natural languages, we use a pre-trained syntactic dependency parser~\citep{Andreas2016Learning,Schuster2015Generating} for extracting programs and concepts that need to be learned. A sample question and the program obtained by our parser is shown in Figure~\ref{fig:vqs:example}. The concept embeddings are initialized by the bag of words (BoW) over the GloVe word embeddings~\citep{pennington2014glove}.

\paragraph{Baselines. } We compare our model against two representative baselines: MLP~\citep{Jabri2016Revisiting} and MAC~\citep{Hudson2018Compositional}.

MLP is a standard baseline for visual-question answering, which treats the multiple-choice task as a ranking problem. For a specific candidate answer, a multi-layer perceptron (MLP) model is used to encode a tuple of the image, the question, and the candidate answer. The MLP outputs a score for each tuple, and the answer to the question is the candidate with the highest score. We encode the image with a ResNet-34 pre-trained on ImageNet and use BoW over the GloVe word embeddings for the question and option encoding.

We slightly modify the MAC network for the VQS dataset. For each candidate answer, we concatenate the question and the answer as the input to the model. The MAC model outputs a score from 0 to 1 and the answer to the question is the candidate with the highest score. The image features are extracted from the same ResNet-34 model.

\paragraph{Results. }
Table~\ref{fig:vqs:execution} summarizes the results. \model achieves comparable results with the MLP baseline and the MAC network designed for visual reasoning. Our model also brings transparent reasoning over natural images and language. Example execution traces generated by \model are shown in Figure~\ref{fig:vis:vqs:1}. Besides, the symbolic reasoning process helps us to inspect the model and diagnose the error sources. See the caption for details. 

\section{Visualization of execution traces and visual concepts}
\label{sec:app:concept}

Another appealing benefit is that our reasoning model enjoys full interpretability. Figure~\ref{fig:visualization}, Figure~\ref{fig:vis:mc}, and Figure~\ref{fig:vis:vqs:1} show \model's execution traces on CLEVR, Minecraft, and VQS, respectively. As a side product, our system detects ambiguous and invalid programs and throws out exceptions. As an example (Figure~\ref{fig:visualization}), the question ``What's the color of the cylinder?'' can be ambiguous if there are multiple cylinders or even invalid if there are no cylinders.

Figure~\ref{fig:vis:clevr:concept} and Figure~\ref{fig:vis:mc:concept} include qualitative visualizations of the concepts learned from the CLEVR and Minecraft datasets, including object categories, attributes, and relations. We choose samples from the validation or test split of each dataset by generating queries of the corresponding concepts. We set a threshold to filter the returned images and objects. For quantitative evaluations of the learned concepts on the CLEVR dataset, please refer to Table~\ref{tab:expr:concept-count} and Table~\ref{tab:expr:retrieval}.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{raw/CLEVRInference.pdf}
\caption{Visualization of the execution trace generated by our Neuro-Symbolic Concept Learner on the CLEVR dataset. Example A and B are successful executions that generate correct answers. In example C, the execution aborts at the first operator. To inspect the reason why the execution engine fails to find the corresponding object, we can read out the visual representation of the object, and locate the error source as the misclassification of the object material. Example D shows how our symbolic execution engine can detect invalid or ambiguous programs during the execution by performing sanity checks.}
\label{fig:visualization}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/MCInference.pdf}
    \caption{Exemplar execution trace generated by our Neuro-Symbolic Concept Learner on the Minecraft reasoning dataset. Example A, B and C are successful execution. Example C demonstrates the semantics of the FilterMost operation. Example D shows a failure case: the detection model fails to detect a pig hiding behind the big tree.}
    \label{fig:vis:mc}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/VQAInference.pdf}
    \caption{Illustrative execution trace generated by our Neuro-Symbolic Concept Learner on the VQS dataset. Execution traces A and B shown in the figure leads to the correct answer to the question. Our model effectively learns visual concepts from data. The symbolic reasoning process brings transparent execution trace and can easily handle quantities (\eg, object counting in Example A). 
In Example C, although \model answers the question correctly, it locates the wrong object during reasoning: a dish instead of the cake. In Example D, our model misclassifies the sport as frisbee.
    }
    \label{fig:vis:vqs:1}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/CLEVRConcept.pdf}
    \caption{Concepts learned on the CLEVR dataset.}
    \label{fig:vis:clevr:concept}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{raw/MCConcept.pdf}
    \caption{Concepts learned on the Minecraft dataset.}
    \label{fig:vis:mc:concept}
\end{figure}
} 
\end{document}
