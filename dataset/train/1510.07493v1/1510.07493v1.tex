\section{Experimental comparison}

\begin{table*}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Method & Holidays & Oxford5K (full) & Oxford105K (full) & UKB\\
\hline
Fisher vector, k=16 & 0.704 & 0.490 & --- & ---\\
\hline
Fisher vector, k=256 & 0.672 & 0.466 & --- & ---\\
\hline
Triangulation embedding, k=1 & 0.775 & 0.539 & --- & ---\\
\hline
Triangulation embedding, k=16 & 0.732 & 0.486 & --- & ---\\
\hline
Max pooling  & 0.711 & 0.524 & 0.522 & 3.57\\
\hline
Sum pooling (SPoC w/o center prior) & 0.802 & 0.589 & 0.578 & 3.65\\
SPoC (with center prior) & 0.784 & 0.657 & 0.642 & 3.66\\
\hline
\end{tabular}
\vspace{1mm}

\caption{ Detailed comparison of feature aggregation methods for deep convolutional features (followed by PCA compression to  dimensions and whitening/normalization).
Sum pooling (SPoC) consistently outperforms other aggregation methods. Full (uncropped) query images are used for Oxford datasets.
\textit{See text for more discussions.}}
\label{tab:mainComparison}
\end{table*}

\begin{figure*}
\centering
\begin{tabular}{c}
\includegraphics[width=16.5cm]{out1.pdf}\\
\includegraphics[width=16.5cm]{out2.pdf}\\
\includegraphics[width=16.5cm]{out3.pdf}\\
\includegraphics[width=16.5cm]{out4.pdf}
\end{tabular}
\caption{Retrieval examples (queries and top-ten matches) using SPoC descriptor on the Oxford Buildings dataset (Oxford5K). Red color marks false positives, green color marks true positives and blue color marks images from ''junk'' lists. Two top examples demonstrate that SPoC is robust to changes in viewpoint, cropping and scale. Two bottom rows are the cases where SPoC fails. In these cases SPoC ''is distracted'' by irrelevant objects such as the pavement or the tree.}
\label{fig:examples}
\vspace{-3mm}
\end{figure*}

\begin{figure*}
\centering
\begin{tabular}{c}
\includegraphics[width=16.5cm]{1111_cropped.pdf}
\end{tabular}
\caption{The examples of similarity maps between the local features of a query image and the SPoC descriptors of top-ten matches. The local features are compressed by the same PCA+whitening matrices as were used for SPoC descriptors and the cosine similarity between each local feature of a query and the SPoC descriptor of a dataset image is computed. The similarity maps allow to localize the regions of a query which are ``responsible'' for the fact that the particular image is considered similar to a query. For instance, for the query above, the spires of the two towers are ``responsible'' for most of the top matches.}
\label{fig:localSimilarities}
\end{figure*}

{\bf Datasets.} We evaluate the performance of SPoC and other aggregation algorithms on four standard datasets.

INRIA Holidays dataset~\cite{Holidays} {\em (Holidays)} contains 1491 vacation snapshots corresponding to 500 groups each having the same scene or object. One image from each group serves as a query. The performance is reported as mean average precision over 500 queries. Similarly to e.g.~\cite{Babenko14}, we manually fix images in the wrong orientation by rotating them by  degrees.

Oxford Buildings dataset~\cite{Philbin07} {\em (Oxford5K)} contains 5062 photographs from Flickr associated with Oxford landmarks. 55 queries corresponding to 11 buildings/landmarks are fixed, and the ground truth relevance of the remaining dataset w.r.t.\ these 11 classes is provided. The performance is measured using mean average precision (mAP) over the 55 queries.

Oxford Buildings dataset+100K~\cite{Philbin07} {\em (Oxford105K)} contains the Oxford Building dataset and additionally 100K distractor images from Flickr.

University of Kentucky Benchmark dataset~\cite{Nister06} {\em (UKB)} contains  indoor photographs of  objects (four photos per object). Each image is used to query the rest of the dataset. The performance is reported as the average number of same-object images within the top four results.

{\bf Experimental details.} We extract deep convolutional features using the very deep CNN trained by Simonyan and Zisserman~\cite{Simonyan14}. {\tt Caffe}~\cite{Caffe} package for CNNs is used. For this architecture, the number of maps in the last convolutional layer is . All images are resized to the size  prior to passing through the network. As a result the spatial size of the last layer is . The final dimensionality for SPoC and, where possible, for other methods is fixed at . 


{\bf Aggregation methods.} The emphasis of the experiments is on comparing different aggregation schemes for deep convolutional features.

We consider simple sum pooling and max pooling aggregation. In addition, we consider the two more sophisticated aggregation methods, namely Fisher Vectors~\cite{Perronnin10} (Yael \cite{Yael} implementation) and Triangulation embedding~\cite{Jegou14} (authors implementation). We have carefully tweaked the design choices of these methods in order to adapt them to new kind of features. 

Thus, for Fisher vectors it was found beneficial to PCA-compress the features to 32 dimensions before embedding. For the triangulation embedding several tweaks that had strong impact for SIFTs had relatively small impact in the case of deep features (this includes square rooting of initial features and removing highly-energetic components). We have not used democratic kernel \cite{Jegou14} in the systematic comparisons as it can be applied to all embedding methods, while its computationally complexity can be prohibitive in some scenarios. We observed that for Holidays it consistently improved the performance of triangulation embedding by  percent (measured prior to PCA).

All embedding methods were followed by PCA reduction to  dimensions. For sum pooling (SPoC) this was followed by whitening, while for Fisher vectors and Triangulation embedding we used power normalization in order to avoid overfitting (as suggested in \cite{Jegou14}). While \cite{Azizpour14} recommends to use whitening with max pooling aggregation, we observed that it reduces retrieval performance and we do not use whitening for max pooling. In the end, all representations were -normalized and the scalar product similarity (equivalent to Euclidean distance) was used during retrieval. The parameters of PCA (and whitening) were learned on hold-out datasets (Paris buildings for Oxford Buildings, 5000 Flickr images for Holidays) unless noted otherwise.

\begin{table}
\setlength{\tabcolsep}{2pt}
\centering
\begin{tabular}{|c|c|c|}
\hline
Method & Holidays & Oxford5K\\
\hline
Fisher vector, k=16 & 0.704 & 0.490 \\
Fisher vector, PCA on test, k=16 & 0.747 & 0.540\\
\hline
Fisher vector, k=256 & 0.672 & 0.466 \\
Fisher vector, PCA on test, k=256 & 0.761 & 0.581\\
\hline
Triang. embedding, k=1 & 0.775 & 0.539\\
Triang. embedding, PCA on test, k=1 & 0.789 & 0.551\\
\hline
Triang. embedding, k=16 & 0.732 & 0.486\\
Triang. embedding, PCA on test, k=16 & 0.785 & 0.576\\
\hline
Max pooling & 0.711 & 0.524 \\
Max pooling, PCA on test & 0.728 & 0.531 \\
\hline
SPoC w/o center prior & 0.802 & 0.589 \\
SPoC w/o center prior, PCA on test & 0.818 & 0.593 \\
\hline
SPoC (with center prior) & 0.784 & 0.657 \\
SPoC (with center prior), PCA on test & 0.797 & 0.651 \\
\hline
\end{tabular}
\vspace{1mm}

\caption{Comparison of overfitting effect arose from PCA matrix learning for SPoC and other methods. Dimensionalities of all descriptors were reduced to 256 by PCA. Overfitting is much smaller for SPoC and max pooling than for the state-of-the-art high-dimensional aggregation methods.}
\label{tab:overfitting}
\vspace{-2mm}
\end{table}

\begin{table*}
\setlength{\tabcolsep}{4pt}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Method & D & Holidays & \begin{tabular}{@{}c@{}}Oxford5K \\ (full query)\end{tabular} & \begin{tabular}{@{}c@{}}Oxford5K \\ (crop query)\end{tabular} & \begin{tabular}{@{}c@{}}Oxford105K \\ (full query)\end{tabular} & \begin{tabular}{@{}c@{}}Oxford105K \\ (crop query)\end{tabular} &  UKB\\
\hline
SIFT + Triang. + Democr. aggr.\cite{Jegou14} & 1024 & 0.720 & -- & 0.560 & -- & 0.502 & 3.51 \\
SIFT + Triang. + Democr. aggr.\cite{Jegou14} & 128 &  0.617 & -- & 0.433 & -- & 0.353 & 3.40 \\
\hline
Deep fully connected \cite{Babenko14} & 256 & 0.749 & 0.435 & -- & 0.386 & -- & 3.42 \\
Deep fully connected + fine-tuning \cite{Babenko14} & 256 & 0.789 & 0.557 & -- & 0.524 & -- & 3.56 \\
Deep convolutional + Max pooling \cite{Razavian15} & 256 & 0.716 & 0.533 & -- & 0.489 & -- & -- \\
Deep fully connected + VLAD \cite{Gong14} & 512 & 0.783 & -- & -- & -- & -- & -- \\
\hline
Sum pooling (SPoC w/o center prior) & 256 & 0.802 & 0.589 & 0.531 & 0.578 & 0.501 & 3.65\\
\hline
\end{tabular}
\vspace{1mm}

\caption{ Comparison with state-of-the-art for compact global descriptors. For the recent works we report results for dimensionality  or for the closest dimensionalities reported in those papers. Despite their simplicity, SPoC features considerably improve state-of-the-art on all four datasets.}
\vspace{-2mm}
\label{tab:soa}
\end{table*}

{\bf Results.} The comparison of different aggregation methods as well as different variants of SPoC are shown in \tab{mainComparison} and \tab{overfitting}. Several things are worth noting:

\begin{itemize}
\item For deep convolutional features sum pooling emerges as the best aggregation strategy by a margin. It is better than equally simple max pooling, but also better than Fisher vectors and Triangulation embedding even with handicaps discussed below, which is in sharp contrast with SIFT features.

\item We demonstrate the amenability to the overfitting for different methods in \tab{overfitting}. One can see that despite replacing whitening with power normalization, Fisher vectors and Triangulation embedding suffer from the overfitting of the final PCA. When learning PCA on the test dataset their performance improves very considerably. Because of this overfitting effect, it is actually beneficial to use simpler aggregation models:  vs  mixture components for Fisher vectors,  vs  cluster centers in the triangulation embedding. For SPoC and max-pooling overfitting is very small.

\item For triangulation embedding, degenerate configuration with one centroid performs best (more exhaustive search was performed than reported in the table). Even without PCA compression of the final descriptor to 256 dimensions, we observed that the performance of uncompressed descriptor benefitted very little from using more than one centroid, which is consistent with our observations about the statistics of deep convolutional features.



\item Center prior helps for Oxford (a lot), Oxford105K (a lot) and UKB (very little) datasets and hurts (a little) for the Holidays dataset.

\item Whitening is much more beneficial for sum pooling than for max pooling (e.g. max pooling with whitening achieves 0.48 mAP on Oxford while 0.52 without whitening). Apparently some “popular” features that are both common across images and bursty and their contribution to SPoC are suppressed by whitening. For max-pooling burstiness of popular features are less of an issue.

\item PCA compression benefits deep descriptors, as was observed in \cite{Babenko14}. The uncompressed (but still whitened) SPoC features achieve mAP 0.55 on Oxford (0.59 with compression) and 0.796 on Holidays (0.802 with compression).

\end{itemize}

Some qualitative examples of good and bad retrieval examples using SPoC descriptors are shown in \fig{examples}. We also demonstrate some examples of similarity maps between local features of a query image and a global SPoC descriptors of dataset images. To produce these maps we compress the local features by the same PCA+whitening transformation as was used for SPoC construction. Then cosine similarities between local features of the query image and the SPoC descriptor of the dataset image are calculated and visualized as a heatmap. Such heatmaps allow to localize the regions of a query image which are similar to a particular image in the search results. 

{\bf Comparison with state-of-the-art} for compact global descriptors is given in \tab{soa}. Existing works use different evaluation protocols for Oxford datasets, e.g.~\cite{Jegou14,Tolias13} crop query images before retrieval, while recent works \cite{Razavian15,Babenko14,Azizpour14,Razavian14} use uncropped query images. Here, we evaluate our SPoC descriptor in both protocols. In the crop case, for a query image we aggregate only features which have the centers of their receptive fields inside a query bounding box (as it usually done in SIFT-based approaches). As some information about context is discarded by cropping, the results with croped queries are lower.

It turns out that the gap between Oxford5K and Oxford105K performance is quite small for all evaluated settings (especially when queries are not cropped). It seems that the 100K Flickr distractor images while ``distracting enough'' for hand-crafted features, do not really ``distract'' deep convolutional features as they are too different from the Oxford Buildings images.

SPoC features provide considerable improvement over previous state-of-the-art for compact descriptors including deep descriptors in \cite{Babenko14, Gong14, Razavian15}. There are several ways how the results can be further improved. First, a mild boost can be obtained by pooling together features extracted from multiple scales of the same image (about  percent mAP in our preliminary experiments). Similar amount of improvement can be obtained by fine-tuning the original CNN on a specially collected dataset (in the same vein to \cite{Babenko14}). 



