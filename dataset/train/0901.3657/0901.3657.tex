\documentclass[amsthm]{elsart}

\usepackage{ifpdf}
\usepackage{yjsco}
\usepackage{natbib}
\usepackage{bbm,mathrsfs}
\usepackage{amsmath}
\usepackage{alltt, amssymb}
\usepackage{graphicx}

\def\d {\ensuremath{\mathbf{d}}}
\def\e {\ensuremath{\mathbf{e}}}
\def\C {\ensuremath{\mathsf{C}}}
\def\L {\ensuremath{\mathsf{L}}}
\def\Q {\ensuremath{\mathbb{Q}}}
\def\N {\ensuremath{\mathbb{N}}}
\def\R {\ensuremath{\mathbb{R}}}
\def\red {\ensuremath{\mathsf{R}}}
\def\S {\ensuremath{\mathbf{S}}}
\def\Z {\ensuremath{\mathbb{Z}}}
\def\F {\ensuremath{\mathbb{F}}}
\def\H {\ensuremath{\mathbb{H}}}
\def\K {\ensuremath{\mathbb{K}}}
\def\A {\ensuremath{\mathbb{A}}}
\def\x {\ensuremath{\mathbf{x}}}
\def\X {\ensuremath{\mathbf{X}}}
\def\T {\ensuremath{\mathbf{T}}}
\def\M {\ensuremath{\mathsf{M}}}
\def\MM {\ensuremath{\mathsf{MM}}}
\def\U {\ensuremath{\mathbf{U}}}
\def\V {\ensuremath{\mathbf{V}}}
\def\W {\ensuremath{\mathbf{W}}}
\def\kost {\ensuremath{\mathsf{k}}}
\def\rng {\ensuremath{\mathsf{R}}}
\def\sng {\ensuremath{\mathsf{S}}}
\def\ms {\ensuremath{\mathsf{MS}}}
\def\Span {\ensuremath{\mathsf{Span}}}
\def\Eval {\ensuremath{\mathsf{Eval}}}
\def\Lift {\ensuremath{\mathsf{Lift}}}
\def\Interp {\ensuremath{\mathsf{Interp}}}
\def\vanish {\ensuremath{\mathsf{Vanish}}}

\theoremstyle{plain}

\newtheorem{Corollary}{Corollary}
\newtheorem{Theo}{Theorem}
\newtheorem{Proposition}{Proposition}
\newtheorem{Lemma}{Lemma}
\newtheorem{Remark}{Remark}
\newcommand{\ead}[1]{(\texttt{#1})}
\newcommand{\newblock}{ }



\begin{document}
\begin{frontmatter}
\title{Homotopy techniques for multiplication modulo triangular sets}
\thanks{This research was supported by ANR Gecko, the joint
  Inria-Microsoft Research Lab, MITACS, NSERC and the Canada Research
  Chair program.}

\author{Alin Bostan} 
\ead{alin.bostan@inria.fr}
\address{Algorithms Project, INRIA Rocquencourt, 78153 Le Chesnay Cedex, France}
\author{Muhammad Chowdhury} 
\ead{mchowdh3@csd.uwo.ca}
\address{Computer Science Department, The University of Western Ontario, London, Ontario, Canada}
\author{Joris van der Hoeven} 
\ead{Joris.Vanderhoeven@math.u-psud.fr}
\address{CNRS, D\'epartement de Math\'ematiques, Universit\'e Paris-Sud, 91405 Orsay Cedex, France}
\author{{\'E}ric Schost} 
\ead{eschost@uwo.ca}
\address{Computer Science Department, The University of Western Ontario, London, Ontario, Canada}

\begin{abstract}
  We study the cost of multiplication modulo triangular families of
  polynomials. Following previous work by Li, Moreno Maza and Schost,
  we propose an algorithm that relies on homotopy and fast
  evaluation-interpolation techniques. We obtain a quasi-linear time
  complexity for substantial families of examples, for which no such
  result was known before. Applications are given to notably addition
  of algebraic numbers in small characteristic.
\end{abstract}

\begin{keyword}
Triangular sets, multiplication, complexity
\end{keyword}
\maketitle
\end{frontmatter}




\section{Introduction}\label{sec:intro}

Triangular families of polynomials are a versatile data structure,
well adapted to encode geometric problems with some form of
symmetry~\cite{AuVa00,KoMo02,FoMo02,GaSc04}. However, in spite of
this, many complexity questions are still not answered in a satisfying
manner.

A high-level question is to provide sharp estimates on the cost of
solving polynomial systems by means of triangular
representations. This problem has a geometric nature; it itself relies
on several difficult lower-level questions, such as the cost of basic
operations with triangular sets. In this paper, we address one such
question: the arithmetic cost of multiplication of polynomials modulo
a triangular set. This justifiably stands as a central question,
since many higher-level routines are built on top of it, such as
inversion~\cite{Langemyr91,HoMo02,LiMoSc07}, lifting
techniques~\cite{DaMaScWuXi05} for modular algorithms, solving systems
of equations~\cite{LiMoRaSc08b}, etc.

\paragraph*{Problem statement, overview of our results.}
We work in , where  is a ring, and we are
given a set of relations of the form

The polynomials  form a {\em triangular set}: for all , 
is in , is {\em monic} in  and {\em reduced}
modulo , in the sense that
 for . As an aside, note that
the case where  is not monic but with a leading coefficient
invertible modulo  reduces in
principle to the monic case; however, inversion modulo  remains a difficult
question~\cite{DaMoScXi06}, of complexity higher than that of
multiplication.

As input, we consider two polynomials  reduced modulo . The direct approach to multiply them modulo  is to perform a polynomial multiplication, followed by the
reduction modulo , by a generalization of Euclidean
division. As far as complexity is concerned, when the number of
variables grows, this kind of approach cannot give linear time
algorithms. Consider for instance the case where all  have degree
2 in their main variables . Then,  and  both have 
monomials, but their product before reduction has  monomials;
after reduction, the number of monomials is  again.  If we let
 be a measure of the input and output size, the cost of
such an algorithm is at least .

In this paper, we show that a different approach can lead to a
quasi-linear time algorithm, in cases where the monomial support of
 is sparse, or when the polynomials in  have a low total
degree. 
This will for example be the case for systems of the form

whose applications are described later on. Our result also applies to
the following construction: start from , say
, and define the so-called ``Cauchy
modules''~\cite{ReVa99}, which are in effective Galois
theory~\cite{ReVa99,AbOrReVa04,ReYo06}:
1mm]
F_2(X_1,X_2)&=&\frac{F_1(X_1)-F_1(X_2)}{X_1-X_2} &=& X_2^2 + X_2X_1 - X_2 + X_1^2 - X_1 + 1\
For examples~\eqref{eq:T1} and~\eqref{eq:T2}, our algorithms give the
following results:

\smallskip

\begin{itemize}
\item for  as in~\eqref{eq:T1}, multiplication modulo  can be performed in quasi-linear time ,
  where  is the input and output size,
  and where  stands for .

\smallskip

\item for  as in~\eqref{eq:T2}, with , multiplication
  modulo  can be performed in quasi-linear time
  , where  is the input and output
  size.
\end{itemize}

\smallskip\noindent
No previous algorithm was known featuring such complexity estimates.

\paragraph*{Our approach.}
To obtain this quasi-linear cost, we have to avoid multiplying  and
 as polynomials. Our solution is to use evaluation and
interpolation techniques, just as FFT multiplication of univariate
polynomials is multiplication modulo .

Fast evaluation and interpolation may not be possible directly, if
 does not have roots in  (as in the previous
examples). However, they become possible using deformation techniques:
we construct a new triangular set  with all roots in , and
multiply  and  modulo , where 
is a new variable. The triangular set  has roots in
, by Hensel's lemma, so one can use evaluation-interpolation
techniques over .

This idea was introduced in~\cite{LiMoSc07}, but was limited to the
case where all polynomials in  are univariate:  was
restricted to depend on  only, so this did not apply to the
examples above. Here, we extend this idea to cover such examples; our
main technical contribution is a study of precision-related issues
involved in the power series computations, and how they relate to the
monomial support of .

\paragraph*{Previous work.} It is only recently that fast algorithms
for triangular representations have been throughly investigated; thus,
previous results on efficient multiplication algorithms are scarce.
All natural approaches introduce in their cost estimate an overhead of
the form , for some constant . 

The main challenge (still open) is to get rid of this exponential
factor unconditionnally: we want algorithms of cost
, where  is the number of
monomials in ,  and their product modulo .
For instance, with , the first complexity result of the form
, for any , was in~\cite{Schost05}.

The previous work~\cite{LiMoSc07} gives a general algorithm of cost
. That algorithm uses fast Euclidean division;
for polynomials of low degree (e.g., for ), the naive approach
for Euclidean division can actually give better results.  Previous
mentions of such complexity estimates (with a constant higher than
) are in~\cite{Langemyr91}.

As said above, in~\cite{LiMoSc07}, one also finds the precursor of the
algorithm presented here; the algorithm of~\cite{LiMoSc07} applies to
families of polynomials having , and achieves the
cost  for any . In that
case, the analysis of the precision in power series computation was
immediate. Our main contribution here is to perform this study in the
general case, and to show that we can still achieve similar costs for
much larger families of examples.

\paragraph*{Basic notation.}
Let  be a vector of positive integers. In what
follows, these will represent the main degrees of the polynomials in
our triangular sets; without loss of generality, we will thus always
suppose  for all . 

Recall that  is our base ring and that  are
interminates over . We let  be the set of monomials
 We
denote by  the free -submodule generated by 
in :

This is thus the set of polynomials  in , 
such that  holds for all .
Finally, we let  be the product ;
this is the cardinality of . Remark that since all  are at
least 2, we have the bounds
 The former plainly follows from the inequality ; the latter comes from observing that ; this yields , from which the claim follows by summation.

The {\em multi-degree} of a triangular set  is the
-uple , with . In this case,  is a
free -module isomorphic to .  We say that a
polynomial  is {\em reduced} with respect
to  if  holds for all . For any , there exists a unique , reduced with respect to , and such that
 in is the ideal . We call it the {\em normal
  form} of  and write .


\paragraph*{Outlook of the paper.} In Section~\ref{sec:prel}, we
introduce some basic complexity notation. The next section presents
basic evaluation-interpolation algorithms for so-called {\em
  equiprojectable} sets, which are extensions of algorithms known for
univariate polynomials. We deduce our multiplication algorithm in
Section~\ref{sec:mul}; examples, applications and experimental results
are in Sections~\ref{sec:appli} and~\ref{sec:algnum}.




\section{Preliminaries}\label{sec:prel}

Big-O notation is delicate to use in our situation, since our
estimates may depend on several (possibly an unbounded number of)
parameters (typically, the multi-degree of our triangular
sets). Hence, whenever we use a big-O inequality such as ,
it is implied that there exists a universal constant  such
that  holds for {\em
  all} possible values of the arguments. When needed, we use explicit
inequalities. Finally, the notation  means that
there exists a constant  such that , where the big-O is to be understood as above.

Our complexity estimates count additions, multiplications, and
inversions, when they are possible. We denote by  a
function such that over any ring, polynomials of degree less than 
can be multiplied in  operations, and which
satisfies the super-linearity conditions of~\cite[Chapter
8]{GaGe99}. Using the algorithm of Cantor-Kaltofen~\cite{CaKa91}, one
can take , with .

We next let  be a function such that 
holds for all  and such that we have, over any ring :
\begin{enumerate}
\item for any  in  and any polynomial  of degree less than , one can compute all values
   in  additions and multiplications in ;

\smallskip

\item for any  in , one can compute the
  coefficients of the polynomial  in 
  additions and multiplications in ;

\smallskip

\item for any  in , with  a unit for
  , and any values  in , one can compute
  the unique polynomial  of degree less than  such
  that  holds for all  in  operations in
  .
\end{enumerate}

\smallskip\noindent
By the results of~\cite[Chapter 10]{GaGe99}, one can take . We continue with the well-known fact that the function
 also enables us to estimate the cost of lifting power series
roots of a bivariate polynomial by Newton iteration. In the following
lemma,  is a new variable over .

\begin{Lemma}\label{lemma:lift}
  For any polynomial  in , monic in  and with
  , if the roots of  are known and have
  multiplicity 1, one can compute the roots of  in
   in 
  operations in .
\end{Lemma}

\begin{pf}
  The algorithm consists in lifting all roots of  in parallel using
  Newton iteration, using fast evaluation to compute the needed values
  of  and ; it is given in
  Figure~\ref{Fig:1}, where we use a subroutine called  to do the evaluation.  Each pass through the loop
  at line 2 takes two evaluations in degree  and  inversions,
  with coefficients that are power series of precision
  . Using~\cite[Chapter~9]{GaGe99}, the cost is thus , for some constant . Using
  the super-linearity of the function , the conclusion follows.
\end{pf}

\begin{Remark}\textup{
  When performing all multiplications in
   using Kronecker's method,
  a more precise cost analysis yields the bound 
  instead of
  
  If , then we usually have ,
  which makes the new bound slightly better.
  However, this improvement only has a minor impact on what follows,
  so it will be more convenient to use the technically simpler bound
  from the lemma.}
\end{Remark}

\begin{figure}[!!!h]
\begin{center}
\fbox{
\begin{minipage}{5 cm}
\begin{tabbing}
\qquad \= \qquad \= \quad \= \quad \kill
\
  \label{eq:L}
\L(\d) =\sum_{i \le n} \frac {\C(d_i)}{d_i} \le n  \frac{\C(d)}{d},  
\begin{array}{cccc}
\pi_{i,j}:& \rng^i & \to & \rng^{j} \\
& (x_1,\dots,x_i) & \mapsto & (x_1,\dots,x_j);
\end{array}V_\beta = [ (\beta_1,\dots,\beta_{n-1},a_{\beta,i}) \ | \ a_{\beta,i} \in v_\beta ]V\ =\ 
[ (\beta_1,\dots,\beta_{n-1},a_{\beta,i}) \ | \
\beta=(\beta_1,\dots,\beta_{n-1}) \in \pi(V),\ a_{\beta,i} \in
v_\beta,\ 1 \le i \le d_n ].\begin{array}{cccc}
\Eval_V:&\Span(M_\d) & \to & \rng^{\delta_\d} \\
& F & \mapsto &[F(\alpha) \ | \ \alpha \in V].
\end{array}A = \sum_{i < d_n}
A_i(X_1,\dots,X_{n-1}) X_n^i,A_\beta = \sum_{i < d_n}
A_i(\beta) X_n^i\ \in\ \rng[X_n].1mm]
1 \> if  return \\
2 \> \\
3 \> for  do\\
3.1 \> \>  \\
3.2 \> \>  \\
\> \>  has the form \\
4 \> for  in  do \\
4.1 \> \> \\
5 \> return 
\end{tabbing}
\end{minipage}
}
\caption{Evaluation algorithm.}
\label{Fig:2_1}
\end{center}
\end{figure}
The algorithm is given in Figure~\ref{Fig:2_1}. From this, we deduce
that we can take  satisfying the recurrence

This implies 

which proves the proposition. 
\end{pf}




\subsection{Interpolation}

Using the same notation as above, the inverse of the evaluation map is
interpolation at :

For this map to be well-defined, we impose a natural condition on the
points of . Let . We say that  {\em supports interpolation} if

\smallskip

\begin{itemize}
\item if ,  supports interpolation, and

\smallskip

\item for all  in  and all  in , 
   is a unit;
\end{itemize}

\smallskip
\noindent
if the base ring is a field, this condition is vacuous.  We will see
in the following proposition that if  supports interpolation,
then the map  is well-defined. Moreover, we let 
be such that, for  equiprojectable of multi-degree ,
if  supports interpolation, then the map 
can be evaluated in  operations (including inversions).
\begin{Proposition}\label{prop:interp}
  If  supports interpolation, the map  is well-defined.
  Besides, one can take 
\end{Proposition}
\begin{pf}
  If , we do nothing; otherwise, we let . The set of
  values to interpolate at  has the shape ; we can thus rewrite it as , where each  is in~.

Since  supports interpolation, for  in , there exists a
unique polynomial  of degree less than ,
such that . Applying the
algorithm recursively on the coefficients of the polynomials ,
we can find a polynomial  such that 
holds for all .  Then, the polynomial  satisfies our
constraints. This provides a right-inverse, and thus a two-sided
inverse for the map .
\begin{figure}[!!!h]
\begin{center}
\fbox{
\begin{minipage}{5 cm}
\begin{tabbing}
\qquad \= \qquad \= \quad \= \quad \kill
\\
\C_\Interp(d_1,\dots,d_n) \leq 
     \C_\Interp(d_1,\dots,d_{n-1})\, d_n+ 
       \, d_1\cdots d_{n-1} \C(d_n),
1mm]
1 \> if  return \\
2 \> \\
3 \>  \\  
4 \> for  in  do\\
4.1 \> \> \\
5 \> for  do \\
5.1 \> \> \\
6 \> return 
\end{tabbing}
\end{minipage}
}
\caption{Associated triangular set of .}
\label{Fig:VI}
\end{center}
\end{figure}

For a given  in , the function {\sf PolyFromRoots} computes
 in  base ring operations; this implies that given
, one can construct  using  operations. The total
cost for constructing all  is thus at most
 Using the trivial bound  for the left-hand term, and the bound given
in Proposition~\ref{prop:interp} for the right-hand one, we get the
upper bounds

Using the upper bound ,
we finally obtain the estimate .
\end{pf}



\subsection{Multiplication}\label{ssec:mult}

Using our evaluation and interpolation algorithms, it becomes
immediate to perform multiplication modulo a triangular set 
associated to an equiprojectable set. 
\begin{Proposition}\label{prop:mul}
  Let  be an equiprojectable set of multi-degree
   that supports interpolation, and let  be
  the associated triangular set. Then one can perform multiplication
  modulo  in time .
\end{Proposition}
\begin{pf}
  The algorithm is the same as in~\cite[Section~2.2]{LiMoSc07}, except
  that we now use the more general evaluation and interpolation
  algorithms presented here. Let  and  be reduced modulo
  , and let . Then
  for all  in , . Since 
  is reduced modulo , it suffices to interpolate
  the values  to obtain .
The cost is thus that of two evaluations, one interpolation, and of
all pairwise pairwise products; the bounds of Propositions~\ref{prop:eval}
and ~\ref{prop:interp} conclude the proof.
\end{pf}

\begin{figure}[!!!h]
\begin{center}
\fbox{
\begin{minipage}{5 cm}
\begin{tabbing}
\qquad \= \qquad \= \quad \= \quad \kill
\S_i = \eta T_i + (1-\eta) U_i, \quad 1 \le i \le n.H_0(e_1,\dots,e_n) = \deg(X_1^{e_1}\cdots X_n^{e_n} \bmod \langle \S \rangle, \eta)H(e_1,\dots,e_n) = \max_{e'_1 \le e_1,\ \dots,\ e'_n \le e_n}\ 
H_0(e'_1,\dots,e'_n).\begin{array}{rccc}
  \pi: & \rng[[\eta]]^n & \to & \rng[[\eta]]^{n-1} \\
  & (\alpha^\star_1,\dots,\alpha^\star_n) & \mapsto & (\alpha^\star_1,\dots,\alpha^\star_{n-1}).
\end{array}\frac{\partial S_i}{\partial X_i}(\eta=0,\alpha_1,\dots,\alpha_n) = 
\frac{\partial U_i}{\partial X_i}(\alpha_1,\dots,\alpha_n) = 
\frac{\partial U_i}{\partial X_i}(\alpha_1,\dots,\alpha_i)U_i(\alpha_1,\dots,\alpha_{i-1},X_i) = \prod_{a \in v_{\alpha'}} (X_i-a), \frac{\partial U_i}{\partial X_i}(\alpha_1,\dots,\alpha_{i-1},\alpha_i) = 
\prod_{a \in v_{\alpha'}, a\ne \alpha_i} (\alpha_i-a).1mm]
1 \> \\
2 \> if  return \\
3 \>  \\
4 \> for  do \\
4.1 \> \> \\
\> \> \eta^\ell\\
5 \> for  in  do\\
5.1 \> \> \\
5.2 \> \> \\
6 \> return 
\end{tabbing}
\end{minipage}
}
\caption{Lifting the roots of .}
\label{Fig:2_4}
\end{center}
\end{figure}

Lemma~\ref{lemma:lift} shows that we can lift the power series roots
of a bivariate polynomial of degree  at precision  in time
. As a consequence, the overall cost  of the
lifting process satisfies

the middle term gives the cost of evaluating the coefficients of 
at  (so we apply our evaluation algorithm
with power series coefficients); and the right-hand term gives the
cost of lifting the roots of .  This gives
 As in the proof of
Proposition~\ref{prop:van}, one deduces that the overall sum is bounded by

which concludes the proof.
\end{pf}



\subsection{Multiplication by homotopy}\label{ssec:prodEta}

We continue with the same notation as before. To multiply two
polynomials  modulo ,
we may multiply them modulo  over  and
let  in the result. Now the results of the multiplication modulo
 over  and over  are the same.
When working over , we may use the evaluation-interpolation
techniques from Subsection~\ref{ssec:mult}. Indeed,
by Proposition~\ref{Prop:lift},  is associated to a subset
 of  that supports interpolation.

Of course, when multiplying  and  modulo 
over , we cannot compute with (infinite) power series,
but rather with their truncations at a suitable order.  On the one
hand, this order should be larger than the largest degree of a
coefficient of the multiplication of  and  modulo  over . On the other hand, this order will
determine the cost of the multiplication algorithm, so it should be
kept to a minimum.  For  in , we define

and
 
The following proposition relates the cost of our algorithm to
the function ; the behavior of this function is
studied in the next subsection.

\begin{Proposition}\label{coro:mul}
  Given ,  and , one can compute  in time , with .
\end{Proposition}
\begin{pf}
  The algorithm is simple: we compute  and use it to obtain
   at a high enough precision. In , the
  product  satisfies  for all ;
  since the multiplication algorithm does not perform any division by
  , it suffices to apply it with coefficients in
  , with
  . The resulting algorithm is given in
  Figure~\ref{Fig:2_5}; as before, we write  for simplicity,
  whereas we should write .

\begin{figure}[!!!h]
\begin{center}
\fbox{
\begin{minipage}{5 cm}
\begin{tabbing}
\qquad \= \qquad \= \quad \= \quad \kill
\\label{eq:Si}
S_i = X_i^{d_i} + \sum_{\nu \in E_i} s_\nu \X_i^\nu,
\label{eq:hi}
h_i = \max_{\nu \in E_i}  \frac{h_1 \nu_{1} + \cdots + h_{i-1} \nu_{i-1} + 1}{d_i-\nu_{i}}.
H(e_1,\dots,e_n) \le h_1 e_1 + \cdots + h_n e_nE'_i = \{ \nu - (0,\dots,0,d_i) \in \Z^i \ | \ \nu \in E_i\},
h_i = \max_{\nu \in E'_i}  \frac{h_1 \nu_{1} + \cdots + h_{i-1} \nu_{i-1} + 1}{-\nu_{i}}.
 H_0(\e) = H_0(\e') \quad\text{if}\quad e_i < d_i,\qquad
H_0(\e) \le 1 + \max_{\nu \in E'_i}\  H_0(\e + \nu)\quad\text{otherwise.}X_1^{e_1}\cdots X_i^{e_i} \bmod \langle \S \rangle= (X_1^{e_1}\cdots
X_{i-1}^{e_{i-1}} \bmod \langle \S \rangle) X_i^{e_i},X_i^{f_i} S_i =
X_i^{f_i+d_i} + \sum_{\nu \in E_i} s_\nu X_i^{f_i} \X_i^\nu,X_i^{f_i} S_i = X_i^{e_i} + \sum_{\nu \in E'_i} s_\nu X_i^{e_i} \X_i^\nu,\X_i^\e - X_1^{e_1}\cdots X_{i-1}^{e_{i-1}} X_i^{f_i} S_i 
= -\sum_{\nu \in E'_i} s_\nu \X_i^{\e+\nu}.\deg(\X_i^\e \bmod \langle \S \rangle, \eta)
\ \le\ \max_{\nu \in E'_i} \deg(s_\nu \X_i^{\e+\nu} \bmod \langle \S \rangle, \eta).\deg(s_\nu \X_i^{\e+\nu} \bmod \langle \S \rangle, \eta)
=\deg(s_\nu,\eta) + \deg(\X_i^{\e+\nu} \bmod \langle \S \rangle, \eta)
=1+H_0(\e+\nu),H_0(\e) \le \max_{\small
\begin{array}{c}
(f_\nu)_{\nu \in E'_i} \text{~non-negative integers}\\
\text{ such~that~}  0 \le e_i + \sum_{\nu \in E'_i} f_\nu \nu_i \le d_i-1
\end{array}
}\ 
 H_0(\e + \sum_{\nu \in E'_i} f_\nu \nu)+ \sum_{\nu \in E'_i} f_\nu.\label{eq:fnu}H_0(\e) \le 1 + \max_{\nu \in E'_i}\  H_0(\e + \nu).
H_0(\e+\nu) \le \max_{\small
\begin{array}{c}
(f_{\nu'})_{\nu' \in E'_i} \text{~non-negative integers}\\
\text{such~that~}  0 \le e_i+\nu_i + \sum_{\nu' \in E'_i} f_{\nu'} \nu'_i \le d_i-1
\end{array}
}\ H_0(\e + \nu + \sum_{\nu' \in E'_i} f_{\nu'} \nu')+ \sum_{\nu' \in
  E'_i} f_{\nu'}.0 \le e_i+\nu_i + \sum_{\nu' \in
  E'_i} f_{\nu'} \nu'_i \le d_i-10 \le e_i + \sum_{\nu' \in  E'_i} f'_{\nu'} \nu'_i \le d_i-1
H_0(\e + \nu + \sum_{\nu' \in E'_i} f_{\nu'} \nu')+ \sum_{\nu' \in
 E'_i} f_{\nu'}=
H_0(\e + \sum_{\nu' \in E'_i} f'_{\nu'} \nu')+ \sum_{\nu' \in
  E'_i} f'_{\nu'}-1.
H_0(\e+\nu) \le  \max_{\small \begin{array}{c}
(f'_{\nu'})_{\nu' \in E'_i} \text{~non-negative integers}\\
\text{such~that~}  0 \le e_i + \sum_{\nu' \in E'_i} f'_{\nu'} \nu'_i \le d_i-1
\end{array}}
\ H_0(\e + \sum_{\nu' \in E'_i} f'_{\nu'} \nu')+ \sum_{\nu' \in
  E'_i} f'_{\nu'}-1.H_0(\e) \le \max_{\small
\begin{array}{c}
(f_\nu)_{\nu \in E'_i} \text{~non-negative integers}\\
\text{such~that~}  0 \le e_i + \sum_{\nu \in E'_i} f_\nu \nu_i \le d_i-1
\end{array}
}\ H_0(\e + \sum_{\nu \in E'_i} f_\nu \nu)+ \sum_{\nu \in E'_i}
f_\nu.H_0(\e + \sum_{\nu \in E'_i} f_\nu \nu) \ = \ 
H_0(\varphi(\e + \sum_{\nu \in E'_i} f_\nu \nu));L_{i-1}(\varphi(\e + \sum_{\nu \in E'_i} f_\nu \nu)).\max_{\small
\begin{array}{c}
(f_\nu)_{\nu \in E'_i} \text{~non-negative integers}\\
\text{such~that~}  0 \le e_i + \sum_{\nu \in E'_i} f_\nu \nu_i \le d_i-1
\end{array}
}\ L_{i-1}(\varphi(\e + \sum_{\nu \in E'_i} f_\nu \nu))+\sum_{\nu \in
  E'_i} f_\nu.H_0(\e) \ \le \ \max_{\small
\begin{array}{c}
(f_\nu)_{\nu \in E'_i} \text{~non-negative real numbers} \\ 
\text{such~that~} 0 \le e_i + \sum_{\nu \in E'_i} f_\nu \nu_i \le d_i-1
\end{array}
}\ L_{i-1}(\varphi(\e + \sum_{\nu \in E'_i} f_\nu \nu))+\sum_{\nu \in
  E'_i} f_\nu.f_\nu \ge 0 \text{~for all~} \nu \in E'_i, \quad 0 \le  e_i + \sum_{\nu \in E'_i}  f_\nu \nu_i \le d_i-1,f_\nu \ge 0 \text{~for all~} \nu \in E'_i, \quad 0 \le e_i + \sum_{\nu \in E'_i}  f_\nu \nu_i.E_\nu = \{f_{\nu'} = 0 \text{~~for~~} \nu'\ne \nu,\qquad f_\nu = -\frac{e_i}{\nu_i}\},
\quad\text{for}\quad \nu\in E'_i. L_{i-1}(\varphi(\e - \frac{e_i}{\nu_i} \nu))  -\frac{e_i}{\nu_i}.  L_{i-1}(\varphi(\e)) - L_{i-1}(\varphi(\frac{e_i}{\nu_i} \nu))-\frac{e_i}{\nu_i}\ =\ 
L_{i-1}(\varphi(\e)) - \frac{L_{i-1}(\varphi(\nu))+1}{\nu_i} e_i.H_0(\e) \ \le L_{i-1}(\varphi(\e)) + \max_{\nu \in E'_i}  \frac{L_{i-1}(\varphi(\nu))+1}{-\nu_i} e_i.H_0(\e) \ \le\ h_1 e_1 + \cdots + h_{i-1} e_{i-1} \ + \
\max_{\nu \in E'_i}  \frac{h_1 \nu_1 + \cdots + h_{i-1} \nu_{i-1}+1}{-\nu_i} e_i,
H_0(\e) \ \le\ h_1 e_1 + \cdots + h_{i-1} e_{i-1} + h_i e_i,\label{eq:ex2}
  T_i = X_i^{d_i} + \sum_{\nu \in D_i} t_\nu \X_i^\nu, \quad t_\nu \in \rng,

\begin{array}{rcll}
O\big(\,n\, \delta_\d\, \frac{\C(d)}d\, \M(nd)\,\big )\ & \subset&\ O\tilde{~}(d \delta_\d) & \text{~if  for all },\1mm]
O\big(\,n\, \delta_\d\, \frac{\C(d)}d\,\M(2^nd)\,\big ) \ &\subset&\ O\tilde{~}(2^n d \delta_\d) & \text{~if  for all }.
\end{array}
\label{eq:V}
V = [x_1,\dots,x_{d_1}] \times \cdots \times [x_1,\dots,x_{d_n}].
D'_i = D_i \cup \{(0,\dots,0,\nu_i) \ | \ 0 \le \nu_i < d_i\}.h_i\ \le \
 \max_{\nu \in D'_i} \frac{ h_1 \nu_1+\cdots + h_{i-1} \nu_{i-1} + 1} {d_i-\nu_i}
\ \le \
 \max\, \Big(\max_{\nu \in D_i} \frac{ h_1 \nu_1+\cdots + h_{i-1} \nu_{i-1} + 1} {d_i-\nu_i}, \ 1\Big).
  \label{eq:hprime}
h_i\ \le\ 
 \max\, \Big( \max_{\nu \in D_i} \frac{  h'_{i-1}(\nu_1+\cdots + \nu_{i-1}) + 1} {d_i-\nu_i}, 1 \Big)
\ \le\
 \max\, \Big( \max_{\nu \in D_i} \frac{  h'_{i-1}(\lambda_i  - \nu_i) + 1} {d_i-\nu_i}, 1 \Big).
 F_{i+1}(X_1,\dots,X_{i+1})
=
\frac{F_{i}(X_1,\dots,X_{i-1},X_i)-F_{i}(X_1,\dots,X_{i-1},X_{i+1})}{X_i-X_{i+1}}
\qquad 1 \le i < d.\left | 
\begin{array}{l}
X_1-X_n^{2^{n-1}}\\
~~~\vdots\\
X_{n-1}-X_n^2\\
X_n^{2^n},
\end{array}\right .
\left | 
\begin{array}{l}X_n^2-X_{n-1}\\
~~~\vdots\\
X_2^2-X_1\\
X_1^2.
\end{array}\right .
\M(2^n)\ \le\ {\sf k}  2^n n \M(n) \quad\text{and thus}\quad
\M(d)\ \le\ {\sf k}'  d \log(d) \M(\log(d))
\M(d) \in O(d\log(d)^3) \quad\text{or} \quad \M(d) \in O(d\log(d)^2\log(\log(d))^3), \quad \dots
  \label{eq:power0}
c_k =\sum_{i+j=k} {k \choose i} a_i b_{j},

  \label{eq:power}
 \sum_{i \le d} \frac{c_i}{i!}X^i\ =\ \sum_{i\le d} \frac{a_i}{i!}X^i\, \sum_{i \le d} \frac{b_i}{i!}X^i \mod X^{d+1},
\left | 
\begin{array}{l}
X_n^p-pX_{n-1}\\
~~~\vdots\\
X_2^p-pX_1\\
X_1^p.
\end{array}\right .
A=\sum_{i \le d} \frac{a_i}{f(i)} X_n^{i_0} \cdots X_1^{i_{n-1}},\quad
B=\sum_{i \le d} \frac{b_i}{f(i)} X_n^{i_0} \cdots X_1^{i_{n-1}},\quad
C=\sum_{i \le d} \frac{c_i}{f(i)} X_n^{i_0} \cdots X_1^{i_{n-1}}.
    \label{eq:val}
 {k \choose i} = \frac{f(k)}{f(i)f(j)}\, p^{v({k \choose i})}    
  \frac{a_i b_j}{f(i)f(j)} p^{c(i,j)} X_n^{k_0} \cdots X_1^{k_{n-1}},{k \choose i} \frac{a_i b_j}{f(k)}X_n^{k_0} \cdots X_1^{k_{n-1}}. f\oplus g = \prod _{\alpha, \beta}
(T-\alpha-\beta),a_i=\sum_{f(\alpha)=0} \alpha^i,\quad b_i=\sum_{g(\beta)=0} \beta^iO\big( (\M(d) \, + \,  d\, \log(d)\, \M(p)\, \M(p\log_d(p))) \ {\sf N}(p,d)\big)O\big(\,(\M(d)\,+\, d \log(d)\M(\log(d)))\,\M_\Z(\log(d))\,\big),1mm]
1. \> \\
2. \> \\
3. \> \\
4. \> \\
5. \> \\
6. \> \\
7. \> \\
8. \> return 
\end{tabbing}
\end{minipage}
}
\caption{Composed sum in small characteristic.}
\label{Fig:composed_sum}
\end{center}
\end{figure}



\subsection{Experimental results}

We implemented the composed sum algorithm over  ({\it i.e.},
 here). We used the NTL C++ package as a basis~\cite{NTL}. Since
NTL does not implement bivariate resultants, we also used
Magma~\cite{magma} for comparison with the resultant method. All
timings are obtained on an AMD Athlon 64 with 5GB of RAM.

Figure~\ref{fig:detailed} gives detailed timings for our algorithm;
each colored area gives the time of one of the main tasks. The less
costly step is the first, the conversion from the original polynomials
to their Newton sums. Then, we give the time needed to compute all the
power series roots needed for our multiplication algorithm, followed
by the evaluation-interpolation process itself; finally, we give the
time necessary to recover  from its power sums. Altogether, the
practical behavior of our algorithm matches the quasi-linear
complexity estimates. The steps we observe correspond to the increase
in the number of variables in our multivariate polynomials, and are
the analogues of the steps observed in classical FFT.

\begin{figure}[!!!h]
\begin{center}
\ifpdf
\includegraphics[width=15cm]{ntl_split_time.pdf}  
\else
\includegraphics[width=15cm]{ntl_split_time.eps}  
\fi
\end{center}
\caption{Detailed timings for our algorithm}
\label{fig:detailed}
\end{figure}

Figure~\ref{fig:magma} gives timings obtained in Magma, using the
built-in resultant function, on the same set of problems as above. As 
predicted by the complexity analysis, the results are significantly
slower (about two orders of magnitude for the larger problems).

\begin{figure}[!!!h]
\begin{center}
\ifpdf
\includegraphics[width=9cm]{resultant_magma.pdf}
\else
\includegraphics[width=9cm]{resultant_magma.eps}
\fi
\end{center}
\caption{Timings in magma}
\label{fig:magma}
\end{figure}



\section{Conclusion}

Several questions remain open after this work. Of course, the most
challenging one remains how to unconditionally get rid of all
exponential factors in multiplication algorithms for triangular sets.
More immediate questions may be the following: at the fine tuning
level, adapting the idea of the Truncated Fourier
Transform~\cite{VdH04} should enable us to reduce the step effect in
the timings of the previous section. Besides, it will be worthwhile to
investigate what other applications can be dealt with using the
``homotopy multiplication'' model, such as the product of matrices with 
entries defined modulo a triangular set, or further tasks such as 
modular  inversion or modular composition.


\bibliographystyle{plain}
\bibliography{mul} 

\end{document}
