









\documentclass[journal]{IEEEtran}






\usepackage{cite}
\usepackage{url}
\usepackage{ragged2e}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphics}
\usepackage{threeparttable}
\usepackage{color}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{diagbox}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{overpic}
\usepackage{textcomp}
\usepackage{contour}
\usepackage{enumitem}
\usepackage[pagebackref=false,breaklinks=false,colorlinks=false]{hyperref}

\renewcommand{\fontsubfuzz}{\maxdimen}

\RequirePackage{silence}
\hbadness=10000 \vbadness=10000 \vfuzz=30pt \hfuzz=30pt

\definecolor{mygray}{gray}{.92}

\newcommand{\trb}[1]{\textbf{\textcolor{black}{#1}}}
\newcommand{\tgb}[1]{\textcolor{green}{#1}}
\newcommand{\tbb}[1]{\textcolor{blue}{#1}}

\newcommand{\red}[1]{{\textcolor{red}{\textbf{#1}}}}
\newcommand{\blue}[1]{{\textcolor{black}{#1}}}
\newcommand{\cyan}[1]{{\textcolor{black}{#1}}}

\newcommand{\tabref}[1]{Tab. \ref{#1}}
\newcommand{\tickYes}{\bullet}
\newcommand{\cmark}{\ding{51}}
\newcommand{\tickNo}{\hspace{1pt}\ding{55}}
\newcommand{\figref}[1]{Fig. \ref{#1}}
\newcommand{\fdp}[1]{{\textcolor{blue}{#1}}}
\newcommand{\supp}[1]{\textcolor{magenta}{#1}}
\newcommand{\notsure}[1]{{\textcolor{red}{#1}}}
\newcommand{\secref}[1]{\ref{#1}}
\newcommand{\sArt}{state-of-the-art }
\newcommand{\myPara}[1]{\vspace{10pt}\noindent~\textbf{#1} \quad}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\AddText}[3]{\put(#1,#2){\contour{black}{\textbf{\textcolor{white}{#3}}}}}
\newcommand{\AddBlack}[3]{\put(#1,#2){\contour{white}{\textbf{\textcolor{black}{#3}}}}}

\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}
\def\etc{\emph{etc}}
\def\etal{{\em et al.~}}

\def\ourmodel{\emph{BBS-Net}}

\definecolor{mygray}{gray}{.92}

\graphicspath{{./Imgs/}}
\DeclareGraphicsExtensions{.jpg,.pdf,.png}















\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Bifurcated Backbone Strategy \\for RGB-D Salient Object Detection }



\author{Yingjie Zhai*, Deng-Ping Fan*, Jufeng Yang, Ali Borji, Ling Shao, \IEEEmembership{Fellow, IEEE}, \\Junwei Han, \IEEEmembership{Senior Member, IEEE}, and Liang Wang, \IEEEmembership{Fellow, IEEE}\IEEEcompsocitemizethanks{
		\IEEEcompsocthanksitem *Equal contribution. Listing order is random.
Yingjie Zhai, Deng-Ping Fan and Jufeng Yang are with College of Computer Science, Nankai University. Ali Borji is with Primer.AI, SF, USA. Ling Shao is with the Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE, and also with the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. Junwei Han is with School of Automation, Northwestern Polytechnical University, China. 
Liang Wang is with the National Laboratory of Pattern Recognition, CAS Center for Excellence in Brain Science and Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. A preliminary version of this work has appeared in ECCV 2020~\cite{fan2020bbs}.
Corresponding author: Jufeng Yang (yangjufeng@nankai.edu.cn).
	}}





\markboth{IEEE TRANSACTIONS ON IMAGE PROCESSING}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}














\maketitle

\begin{abstract}
\justifying
Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. 
When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato.
In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. 
In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS).
Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. 
Then, RGB and depth modalities are fused in a complementary way.
Our architecture, named \textbf{B}ifurcated \textbf{B}ackbone \textbf{S}trategy \textbf{Net}work (\textbf{\emph{\ourmodel}}), is simple, efficient, and backbone-independent.
Extensive experiments show that \ourmodel~significantly outperforms 18 SOTA models on 8 challenging datasets
under 5 evaluation measures, demonstrating the superiority of our approach
(4\% improvement in S-measure  the top-ranked model: DMRA-iccv2019).
In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.
\end{abstract}

\begin{IEEEkeywords}
RGB-D salient object detection, bifurcated backbone strategy, multi-level features, cascaded refinement.
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}


\IEEEPARstart{T}{he} goal of salient object detection (SOD) is to find and segment the most visually prominent object(s) in an image~\cite{borji2015salient,wang2019salient}.
Over the last decade, SOD has attracted significant attention due to its widespread applications in object recognition~\cite{ChengLLZRT19bing}, content-based image retrieval~\cite{cheng2017retrival}, image segmentation~\cite{Wang2015sal_seg}, image editing~\cite{Cheng2010RFA}, video analysis~\cite{Fan2019VSOD,Yan_2019_video}, and visual tracking~\cite{borji2012cvpr, Hong2015tracking}.
Traditional SOD algorithms~\cite{cheng2015GC,zhang2016co} are typically based on handcrafted features and fall short in capturing high-level semantic information (see also~\cite{borji2012state,borji2019saliency}).
Recently, convolutional neural networks (CNNs) have been used for RGB SOD~\cite{Liu2019SPBD,wang2018salient}, achieving better performance compared to the traditional methods.\par
However, the performance of RGB SOD models tends to drastically decrease in certain complex scenarios (\eg, cluttered backgrounds, multiple objects, varying illuminations, transparent objects, \etc)~\cite{chen2019TANet}.
One of the most important reasons behind these failure cases may be the lack of depth information, which is critical for saliency prediction.
For example, an object with less texture but closer to the camera is usually salient than an object with more texture but farther away.
Depth maps contain abundant spatial structure and layout information~\cite{piao2019DMRA}, providing geometrical cues for improving the performance of SOD.
Besides, depth information can be easily obtained using popular devices, \eg, stereo cameras, Kinect and smartphones, which are becoming increasingly more ubiquitous.
Therefore, various algorithms (\eg,~\cite{li2016saliency,zhao2019CPFP}) have been proposed to solve the SOD problem by combining RGB and depth information (\ie, RGB-D SOD).
\par
To efficiently integrate RGB and depth cues for SOD, researchers have explored different but complementary multi-modal and multi-level strategies~\cite{chen2018PCF,chen2019MMCI,li2020crossmodal} and have achieved encouraging results.
However, existing RGB-D SOD methods still have to solve the following challenges: 
\par
\begin{figure}[t!]
\centering
\begin{overpic}[width=1.0\columnwidth]{figure1-min}
\end{overpic}
\caption{\small
	Saliency maps of state-of-the-art (SOTA) CNN-based methods (\ie, DMRA~\cite{piao2019DMRA}, CPFP~\cite{zhao2019CPFP}, TANet~\cite{chen2019TANet}, PCF~\cite{chen2018PCF} and Ours) and methods based on handcrafted features (\ie, SE~\cite{guo2016SE} and LBE~\cite{feng2016LBE}). Our method generates higher-quality saliency maps and suppresses background distractors in challenging scenarios (top: complex background; bottom: depth with noise).}
	\label{fig:figure1}
\end{figure}
(1) \textbf{Effectively aggregating \emph{multi-level} features.}
As discussed in \cite{Liu2019SPBD}, teacher features contain rich semantic macro information and can serve as strong guidance for locating salient objects,
while student features provide affluent micro details that are beneficial for refining object edges.
Therefore, current RGB-D SOD methods use either a dedicated aggregation strategy~\cite{piao2019DMRA,zhao2019CPFP} or a progressive merging process~\cite{LIU2019SSRC,zhu2019PDNet} to leverage multi-level features.
However, because they directly fuse multi-level features without considering level-specific characteristics, these operations suffer from the inherent problem of noisy low-level features~\cite{chen2019TANet,Wu2019CPD}.
As a result, several methods are easily confused by the background (\eg, first and second rows in \figref{fig:figure1}).\par


\begin{figure*}[t!]
	\centering
	\begin{overpic}[width=.88\linewidth]{FrameworkCompare}
	\end{overpic}
	\vspace{-10pt}
	\caption{
		(a) Existing multi-level feature aggregation methods for RGB-D
		SOD~\cite{chen2018PCF,zhao2019CPFP,piao2019DMRA,chen2019TANet,zhu2019PDNet,wang2019AFNet,LIU2019SSRC}.
		(b) In this paper, we adopt a bifurcated backbone strategy (BBS)
		to split the multi-level features into student and teacher features.
		The initial saliency map  is utilized to refine the student features to effectively suppress distractors. Then, the refined features are 
		passed to another cascaded decoder to generate the final saliency map .
	}
	\label{fig:FrameworkCompare}
\end{figure*}

(2) \textbf{Excavating informative cues from the \emph{depth modality}.}
Previous algorithms usually regard the depth map as a fourth-channel input~\cite{cong2019HSCS,peng2014LHM} of the original three-channel RGB image, or fuse RGB and depth features by simple summation~\cite{fan2014DSP,fang2014TIP} and multiplication~\cite{cheng2014DESM,zhu2017CDCP}.
However, these methods treat depth and RGB information from the same perspective and ignore the fact that RGB images capture color and texture, whereas depth maps capture the spatial relations among objects.
Due to this modality difference, the above-mentioned simple combination methods are not very efficient.
Further, depth maps often have low quality, which introduces randomly distributed errors and redundancy into the network~\cite{fan2019D3Net}.
For example, the depth map in the last row of \figref{fig:figure1} is blurry and noisy. As a result, many methods (\eg, the top-ranked model DMRA\cite{piao2019DMRA}) fail to detect the full extent of the salient object.
\par

To address the above issues, we propose a novel \textbf{B}ifurcated \textbf{B}ackbone \textbf{S}trategy \textbf{Net}work (\ourmodel) for RGB-D SOD.
The proposed method exploits multi-level features in a cascaded refinement way to suppress distractors in the lower layers.
This strategy is based on the observation that teacher features provide discriminative semantic information without redundant details~\cite{Liu2019SPBD,Wu2019CPD}, which may contribute significantly to eliminating the lower-layer distractors.
As shown in \figref{fig:FrameworkCompare} (b), \ourmodel~contains two cascaded decoder stages:
(1) Cross-modal teacher features are integrated by the first cascaded decoder  to predict an initial saliency map .
(2) Student features are refined by an
element-wise multiplication with the initial saliency map  and are then aggregated
by another cascaded decoder  to produce the final saliency map .
To fully capture the informative cues in the depth map and improve the compatibility of RGB and depth features, we further introduce a depth-enhanced module (DEM). This module exploits the inter-channel and spatial relations of the depth features and discovers informative depth cues.


Additionally, to obtain reasonable performance in real-world scenarios, not only an efficient model is needed but also a dataset with great generalization ability is required to train such model.
There are several large-scale RGB-D datasets, \eg, NJU2K~\cite{ju2014ACSD}, NLPR~\cite{peng2014LHM}, STERE~\cite{niu2012STERE},
SIP~\cite{fan2019D3Net} and DUT~\cite{piao2019DMRA} with more than  image pairs.
However, researchers have often trained RGB-D models on the fixed training set (\ie,  images from NJU2K and  images from NLPR). This limits the model's generation ability in various scenarios.
Further, they have not studied the generalization ability of different datasets and have not proposed powerful training sets.
In this paper, one of our goals is to study this problem in detail.

Our main contributions are summarized as follows:

\begin{itemize}
	\item
	\textbf{We propose a powerful Bifurcated Backbone Strategy Network (\ourmodel)} to deal with multiple complicated real-world scenarios in RGB-D SOD.
To address the long-overlooked problem of noise in low-level features decreasing the performance of saliency models, 
	we carefully explore the characteristics of multi-level features in a bifurcated backbone strategy (BBS), \ie, features are split into two groups, as shown in \figref{fig:FrameworkCompare} (b).
In this way, noise in student features can be eliminated effectively by the saliency map generated from teacher features.
	
\item	
	\textbf{We further introduce a depth-enhanced module (DEM)} in \ourmodel~to enhance the depth features before merging them with the RGB features.
The DEM module concentrates on the most informative parts of depth maps by two sequential attention operations. We leverage the attention mechanism to excavate important cues from the depth features of multiple side-out layers. This module is simple but has proven effective for fusing RGB and depth modalities in a complementary way.
	
\item
	\textbf{We conduct a comprehensive comparison with  SOTA methods} using various metrics (\eg, max F-measure, MAE, S-measure, max E-measure, and PR curves).
Experimental results show that \ourmodel~outperforms all of these methods on eight public datasets, by a large margin.
In terms of the predicted saliency maps, \ourmodel~generates maps with sharper edges and fewer background distractors compared to existing models.


\item
	\textbf{We conduct a number of cross-dataset experiments} to evaluate the quality of current popular RGB-D datasets and introduce a training set with high generalization ability for fair comparison and future research.
Current RGB-D methods train their networks using the fixed training-test splits of different datasets, without exploring the difficulties of those datasets.
To the best of our knowledge, we are the first to investigate this important but overlooked problem in the area of RGB-D SOD.
\end{itemize}

This work is based on our previous conference paper~\cite{fan2020bbs} and extends it significantly in five ways:
1) We further extend the approach by designing a depth adapter module, which makes the model contain around  percent parameters of the previous version but with similar performance.
2) We provide more details and experiments regarding our \ourmodel~model, including motivation, feature visualizations, experimental settings, \etc.
3) We investigate several previously unexplored issues, including cross-dataset generalization ability, post-processing methods, failure cases analysis, \etc.
4) To further demonstrate our model performance, we conduct several comprehensive experiments over the recently released dataset, DUT~\cite{piao2019DMRA}. 
5) We perform in-depth analyses and draw several novel conclusions which are critical in developing more powerful models in the future. 
We are hopeful that our study will provide deep insights into the underlying design mechanisms of RGB-D SOD, and will spark novel ideas.
The complete algorithm, benchmark results, and post-processing toolbox are publicly available at \supp{\href{https://github.com/zyjwuyan/BBS-Net}{https://github.com/zyjwuyan/BBS-Net}.}

\vspace{-5pt}
\section{Related Works}\label{sec:related_works}

\vspace{-5pt}
\subsection{Salient Object Detection}\label{sec:sod}
\vspace{-5pt}
Over the past several decades, SOD~\cite{liu2010learning,achanta2009frequency,fan2018foreground} has garnered significant research interest due to its diverse applications~\cite{LiY16,ZhangWLWY17,zhao2020suppress}.
In early years, SOD methods were primarily based on intrinsic prior knowledge such as center-surround color contrast~\cite{itti1998model}, global region contrast~\cite{cheng2015GC}, background prior~\cite{li2015visual} and appearance similarity~\cite{cheng2013efficient}.
However, these methods heavily rely on heuristic saliency cues and low-level handcrafted features, thus lacking the guidance of high-level semantic information.\par 


Recently, to solve this problem, deep learning based methods~\cite{Chen_2018_reverse,Zhang2018PAGR,Su_2019_ICCV,zhang2020multi,Li_2019_video} have been explored, exceeding handcrafted feature-based methods in complex scenarios.
These deep methods~\cite{ZhangWLWR17} usually leverage CNNs to extract multi-level multi-scale features from RGB images and then aggregate them to predict the final saliency map.
Such multi-level multi-scale features~\cite{wang2019progressive,wei2019f3net} can help the model better understand the contextual and semantic information to generate high-quality saliency maps.
Besides, since image-based SOD may be limited in some real-world applications such as video captioning~\cite{PanYLM17caption}, autonomous driving~\cite{ZhangFU16drive} and robotic interaction~\cite{XuPCYH16interactive}, SOD algorithms~\cite{Fan2019VSOD,Yan_2019_video} have also been explored for video analysis. 


To further overcome the limits of deep models, researchers have also proposed to excavate edge information~\cite{XieT17edge} to guide prediction.
These methods use an auxiliary boundary loss to improve the training and representative ability of segmentation tasks~\cite{ZhugeYZL18edge,zhao2019egnet,wu2019stacked}.
With the auxiliary guidance from the edge information, deep models can predict maps with finer and sharper edges.
In addition to edge guidance, another useful type of auxiliary information are depth maps, which capture the spatial distance information. These are the main focus of this paper. 

\subsection{RGB-D Salient Object Detection}\label{sec:rgbd}


\myPara{\textbf{Traditional Models.}} Previous algorithms for RGB-D SOD mainly rely on extracting handcrafted features~\cite{cheng2014DESM, zhu2017CDCP} from RGB and depth images.
Contrast-based cues, including edge, color, texture and region, are largely utilized by these methods to compute the saliency of a local region.
For example, Desingh~\etal\cite{desingh2013BMCV} adopted the region-based contrast to calculate contrast strengths for the segmented regions.
Ciptadi~\etal\cite{ciptadi2013BMCV} used surface normals and color contrast to compute saliency. However, the local contrast methods are easily disturbed by high-frequency content~\cite{qu2017DF}, since they mainly rely on the boundaries of salient objects.
Therefore, some algorithms, such as spatial prior~\cite{cheng2014DESM}, global contrast~\cite{cong2019DGTM}, and background prior~\cite{Shigematsu2017BED}, proposed to compute saliency by combining both local and global information.\par
\par
To combine saliency cues from RGB and depth modalities more effectively, researchers have explored multiple fusion strategies.
Some methods~\cite{peng2014LHM,cong2019HSCS} process RGB and depth images together by regarding depth maps as fourth-channel inputs (early fusion).
This operation is simple but does not achieve reliable results, since it disregards the differences between the RGB and depth modalities.
Therefore, some algorithms~\cite{fan2014DSP,zhu2017CDCP} extract the saliency information from the two modalities separately by first leveraging two backbones to predict saliency maps and then fusing the saliency results (late fusion).
Besides, to enable the RGB and depth modalities to share benefits, other methods~\cite{feng2016LBE,ju2014ACSD} fuse RGB and depth features in a middle stage and then produce the corresponding saliency maps (middle fusion).
Deep models also use the above three fusion strategies, and our method falls under the middle fusion category.


\begin{figure*}[t!]
	\small
	\centering
\begin{overpic}[width=0.98\linewidth]{pipe_main}
	\end{overpic}
	\vspace{-5pt}
	\caption{ \textbf{Architecture of our \ourmodel.}
		Feature Extraction: `\textit{Conv1}'`\textit{Conv5}' denote different layers from ResNet-50~\cite{He2016resnet}.
Multi-level features () from the depth branch are enhanced by the DEM and are
		then fused with features (\ie, ) from the RGB branch.
Stage 1: cross-modal teacher features () are first aggregated by the cascaded decoder (a)
		to produce the initial saliency map .
Stage 2: Then, student features () are refined by the initial saliency map 
		and are integrated by another cascaded decoder to predict the final saliency map .
See \secref{sec:proposedMethod} for details. }
	\label{fig:pipe}
\end{figure*}

\myPara{\textbf{Deep Models.}}
Early deep methods~\cite{qu2017DF,Shigematsu2017BED} compute saliency confidence scores by first extracting handcrafted features, and then feeding them to CNNs.
However, these algorithms need the low-level handcrafted features to be manually designed as input, and thus cannot be trained in an end-to-end manner.
More recently, researchers have begun to extract deep RGB and depth features using CNNs in a bottom-up fashion~\cite{han2018CTMF}.
Unlike handcrafted features, deep features contain a lot of contextual and semantic information, and can thus better capture representations of the RGB and depth modalities.
These methods have achieved encouraging results, which can be attributed to two important aspects of feature fusion.
One is their extraction and fusion of multi-level and multi-scale features from different layers, while the other is the mechanism by which the two different modalities (RGB and depth) are combined.\par


Various architectures have been designed to effectively integrate the multi-scale features.
For example, Liu \etal\cite{LIU2019SSRC} obtained saliency map outputs from each side-out features by feeding a four-channel RGB-D image into a single backbone (single stream).
Chen \etal\cite{chen2018PCF} leveraged two independent networks to extract RGB and depth features respectively, and then combined them in a progressive merging way (double stream).
Furthermore, to learn supplementary features,~\cite{chen2019TANet} designed a three-stream network consisting of two modality-specific streams and a parallel cross-modal distillation stream to exploit complementary cross-modal information in the bottom-up feature extraction process (three streams).
Depth maps are sometimes low-quality and may thus contain significant noise or misleading information, which greatly decreases the performance of SOD models.
To address this issue, Zhao~\etal\cite{zhao2019CPFP} proposed a contrast-enhanced network to improve the quality of depth maps using the contrast prior.
Fan \etal\cite{fan2019D3Net} designed a depth depurator unit to evaluate the quality of depth maps and filter out the low-quality ones automatically.
Three recent works have explored uncertainty~\cite{Zhang2020UCNet}, depth prediction~\cite{wang2020synergistic} and a joint learning strategy~\cite{Fu2020JLDCF} for saliency detection and achieved reasonable performance.
There were also some concurrent works published in recent top conferences (\eg, ECCV~\cite{Luo_2020_ECCV,zhao2020single,li2020rgbd}).
Discussing these works in detail is beyond the scope of this article. Please refer to the online benchmark (\supp{\href{http://dpfan.net/d3netbenchmark/}{http://dpfan.net/d3netbenchmark/}}) and the latest survey~\cite{zhou2020rgbd} for more details.



\section{Proposed Method}\label{sec:proposedMethod}
\subsection{Overview}\label{sec:overview}
Current popular RGB-D SOD models directly integrate multi-level features using a single decoder (\figref{fig:FrameworkCompare} (a)).
In contrast, the network flow of the proposed \ourmodel~(\figref{fig:pipe}) explores a bifurcated backbone strategy.
In \secref{sec:BifucatedStrategy}, we first detail the proposed bifurcated backbone strategy with the cascaded refinement mechanism. Then, to fully excavate informative cues from the depth map, we introduce a new depth-enhanced module in \secref{sec:DEM}.
Additionally, we design a depth adapter module to further improve the efficiency of the model in \secref{sec:efficiency}.

\subsection{Bifurcated Backbone Strategy (BBS)}\label{sec:BifucatedStrategy}


Our cascaded refinement mechanism leverages the rich semantic information in high-level cross-modal features to suppress background distractors.
To support such a feat, we devise a bifurcated backbone strategy (BBS). It divides the multi-level cross-modal features into two groups, \ie,  = \{\textit{Conv1, Conv2, Conv3}\} and \{\textit{Conv3, Conv4, Conv5}\}, where \textit{Conv3} is the split point.
The original multi-scale information is well preserved by each group.

\myPara{\textbf{Cascaded Refinement Mechanism.}}
To effectively leverage the characteristics of the features in the two groups' features, we train the network using a cascaded refinement mechanism.
This mechanism first generates an initial saliency map with three cross-modal teacher features (\ie, ) and then enhances the details of the initial saliency map  with three cross-modal student features (\ie, ), which are refined by the
initial saliency map. 
This is based on the observation that high-level features contain rich semantic information that helps locate salient objects, while low-level features provide micro-level details that are beneficial for refining the boundaries.
In other words, by exploring the characteristics of the multi-level features, this strategy can efficiently suppress noise in low-level cross-modal features, and can produce the final saliency map through a progressive refinement. \par
Specifically, we first merge RGB and depth features processed by the DEM to obtain the cross-modal features .
In stage one, the three cross-modality teacher features (\ie, ) are aggregated by the first cascaded decoder, which is denoted as:

where  is the first cascaded decoder,  is the initial saliency map, and  represents two simple convolutional layers that transform the channel number from  to .
In stage two, we leverage the initial saliency map  to refine the three cross-modal student features, which is defined as:

where  () represents the refined features and  denotes the element-wise multiplication.
After that, the three refined student features are aggregated by another decoder followed by a progressively transposed module (PTM), which is formulated as:

where  is the second cascaded decoder,  denotes the final saliency map, and  represents the PTM module. 


\myPara{\textbf{Cascaded Decoder.}}
After computing the two groups of multi-level cross-modal features (), which are a fusion of the RGB and depth features from multiple layers, we need to efficiently leverage the multi-scale multi-level information in each group to carry out the cascaded refinement.
Therefore, we introduce a light-weight cascaded decoder~\cite{Wu2019CPD} to integrate the two groups of multi-level cross-modal features.
As shown in \figref{fig:pipe} (a), the cascaded decoder consists of three global context modules (GCM) and a simple feature aggregation strategy.
The GCM is refined from the RFB module~\cite{liu2018rfb}.
Specifically, it contains an additional branch to enlarge the receptive field and a residual connection~\cite{He2016resnet} to preserve the information.
The GCM module thus includes four parallel branches.
For all of these branches, a  convolution is first applied to reduce the channel size to .
Then, for the  () branch, a convolution operation with a kernel size of  and dilation rate of 1 is applied.
This is followed by another  convolution operation with the dilation rate of .
We aim to excavate the global contextual information from the cross-modal features.
Next, the outputs of the four branches are concatenated together and a 33 convolution operation is then applied to reduce the channel number to 32.
Finally, the concatenated features form a residual connection with the input features.
The GCM module operation in the two cascaded decoders is denoted by:
 
To further improve the representations of cross-modal features,
we leverage a pyramid multiplication and concatenation feature aggregation strategy
to aggregate the cross-modal features ().
As illustrated in \figref{fig:pipe} (a), first, each refined feature  is updated by multiplying it with all higher-level features:

in which ,  or , .  represents the upsampling operation if the features are not of the same scale.  represents the element-wise multiplication, and  represents the standard 33 convolution operation. Then, the updated features are integrated by a progressive concatenation strategy to produce the output:

where  is the predicted saliency map,  denotes the concatenation operation of  and , and . In the first stage,  denotes two sequential convolutional layers (\ie, ), while, for the second stage, it represents the PTM module (\ie, ). 
The scale of the output of the second decoder is 8888, which is  of the ground-truth (352352), so directly upsampling the output to the size of the ground-truth will lose some details.
To address this issue, we propose a simple yet effective progressively transposed module (PTM, \figref{fig:pipe} (b)) to generate the final predicted map () in a progressive upsampling way.
It consists of two residual-based transposed blocks~\cite{2019huAcnet} and three sequential  convolutions.
Each residual-based transposed block contains a  convolution and a residual-based transposed convolution.

Note that the proposed cascaded refinement mechanism is different from the recent refinement strategies CRN~\cite{chen2017photographic}, SRM~\cite{WangBZZL17}, R3Net~\cite{deng2018r3net}, and RFCN~\cite{wang2018salient} in its usage of the initial map and multi-level features.
The obvious difference and advantage of the proposed design is that our model only requires one round of saliency refinement to produce a good saliency map, while CRN, SRM, R3Net, and RFCN all need more iterations, which increases both the training time and computational resources.
Besides, the proposed cascaded mechanism is also different from CPD~\cite{Wu2019CPD} in that it exploits both the details in student features and the semantic information in teacher features, while suppressing the noise in the student features at the same time. 
\subsection{Depth-Enhanced Module (DEM)}\label{sec:DEM}
To effectively fuse the RGB and depth features, two main problems need to be solved: a) the compatibility of RGB and depth features needs to be improved due to the intrinsic modality difference, and b) the redundancy and noise in low-quality depth maps must be reduced.
Inspired by~\cite{Woo2018CBAM}, we design a depth-enhanced module (DEM) to address the issues by improving the compatibility of multi-modal features and excavating informative cues from the depth features. 

\par
Specifically, let ,  represent the feature maps of the  () side-out layer from the RGB and depth branches, respectively.
As shown in \figref{fig:pipe}, each DEM is added before each side-out feature map from the depth branch to enhance the compatibility of the depth features.
This side-out process improves the saliency representation of depth features and, at the same time, preserves the multi-level multi-scale information.
The fusion process of the two modalities is depicted as:

where  denotes the cross-modal features of the  layer.
The DEM module contains a sequential channel attention operation and a spatial attention operation, which are formulated as:

in which  and  represent the spatial and channel attention operations, respectively. More specifically, the channel attention is implemented as:

where  denotes the global max pooling operation for each feature map,  represents a multi-layer (two-layer) perceptron,  denotes the input feature map, and  is the multiplication by the dimension broadcast.
The spatial attention is denoted as:

where  is the global max pooling operation for each point in the feature map along the channel axis.
The proposed depth enhanced module is different from previous RGB-D algorithms, which fuse the multi-level cross-modal features
by direct concatenation~\cite{chen2018PCF,chen2019TANet,zhu2019PDNet}, enhance the multi-level depth features by a simple convolutional layer~\cite{piao2019DMRA} or improve the depth map by contrast prior~\cite{zhao2019CPFP}.
To the best of our knowledge, we are the first to introduce the attention mechanism to excavate informative cues from depth features in multiple side-out layers.
Our experiments (see \tabref{tab:ablation} and \figref{fig:visual_ablation}) demonstrate the effectiveness of our approach in
improving the compatibility of multi-modal features.

Besides, the spatial and channel attention mechanisms are different from the operation proposed in~\cite{Woo2018CBAM}.
Based on the fact that SOD aims at finding the
most prominent objects in an image, we only leverage a single global max pooling~\cite{oquab2015GMP} to excavate the most critical cues in depth
features, which reduces the complexity of the module.
\begin{figure}[t!]
	\small
	\centering
\begin{overpic}[width=0.8\linewidth]{DAM}
	\end{overpic}
	\vspace{-5pt}
	\caption{ \textbf{Architecture of the depth adapter module (DAM).}
	 }
	\label{fig:dam}
\end{figure}
\subsection{Improve the efficiency of BBS-Net.}\label{sec:efficiency}
Note that the above proposed~\ourmodel~leverages two backbones, without sharing weights, to extract RGB features and depth features.
Such a design can make the model extract discriminative RGB features and depth features, respectively, but also introduces more parameters, leading to a suboptimal solution for lightweight applications.
However, making the two branches share weights can cause a big degradation of the performance.
It may be because the RGB image and the depth image are two different modalities, \ie, RGB image contains color, structure, and semantic information while the depth image includes the spatial distance information.
Thus a naive sharing-weight mechanism of the two-branch backbones cannot be suitable to extract the multi-modal features.
To solve this problem, we design a depth adapter module (DAM) to consider the modality difference of the RGB image and depth image. The same backbone can be suitable to extract two-modality features without decreasing much performance.
\par
The whole architecture of the DAM is shown in \figref{fig:dam}.
Let  and  denote the input RGB and depth image pair, respectively.
We first calculate the modality difference  by,

where  is broadcast to the same dimension as . 
Such an operation can make the model understand the explicit difference between the depth image and the RGB image.
Then the adapted depth output is computed by:

In the efficient version of BBS-Net, the backbones of the two branches share parameters.
When calculating the depth features, the depth image is first fed to the DAM module to obtain the adapted depth information and is then fed to the backbone to extract features.
To further reduce model parameters, we also remove the last progressively transposed module (which makes negligible performance degradation) in the efficient version of \ourmodel.
\begin{figure*}[t!]
	\small
	\centering
	\begin{overpic}[width=.98\linewidth]{PRcurve}
		
	\end{overpic}
	\vspace{-10pt}
	\caption{ \textbf{PR Curves of the proposed model and 18 SOTA algorithms over six datasets.} Dots on the curves represent the value of precision and recall at the maximum F-measure.}
	\label{fig:prcurve}
\end{figure*}

\subsection{Implementation Details}\label{sec:implement}
\myPara{\textbf{Training Loss.}} Let  and  denote the height and width of the input images. Given the input RGB image  and its corresponding depth map , our model predicts an initial saliency map  and a final saliency map .
Let  denote the binary ground-truth saliency map. We jointly optimize the two cascaded stages by defining the total loss:

in which  represents the binary cross entropy loss~\cite{zhao2019CPFP} and  controls the trade-off between the two parts of the losses. The  is computed as:

where  is the predicted saliency map.\par
\myPara{\textbf{Training and Test Protocol.}}
We use PyTorch~\cite{steiner2019pytorch} to implement our model on a single 1080Ti GPU. Parameters of the backbone network (ResNet-50~\cite{He2016resnet}) are initialized from the model pre-trained on ImageNet~\cite{KrizhevskySH2012Imagenet}.
Other parameters are initialized using the default PyTorch settings.
We discard the last pooling and fully connected layers of ResNet-50 and
leverage each middle output of the five convolutional blocks as the side-out feature maps.
The two branches do not share weights and the only difference between them is that the
depth branch has the input channel number set to one.
Note for the efficient version \ourmodel, the two branches share weights, and the depth images are first fed to the depth adapter module to reduce the modality difference.
The Adam algorithm~\cite{KingmaB2014adam} is used to optimize our model, the betas are set to  and , and the weight decay is set to .
We set the initial learning rate to 1e-4 and divide it by  every 60 epochs.
The gradients are clipped into  to make the training stable.
The input RGB and depth images are resized to  for both the training and test phases.
We augment all the training images using multiple strategies (\ie, random flipping, rotating, and border clipping).
It takes about ten hours to train the model with a mini-batch size of 10 for  epochs.
Our experiments show that the model is robust to the hyper-parameter .
Thus, we set  to  (\ie, same importance for the two losses).
In the test phase, the predicted maps are upsampled to the same dimension of ground truth by the bilinear interpolation and are then normalized to [0,1].
\section{Experiments and Results}


\subsection{Experimental Settings}\label{sec:experimental_settings}


\myPara{\textbf{Datasets.}}\label{sec:dataset}
We conduct our experiments on eight challenging RGB-D SOD benchmark datasets:
NJU2K~\cite{ju2014ACSD}, NLPR~\cite{peng2014LHM}, STERE~\cite{niu2012STERE}, DES~\cite{cheng2014DESM}, LFSD~\cite{li2014LFSD}, SSD~\cite{zhu2017SSD},
SIP~\cite{fan2019D3Net} and DUT~\cite{piao2019DMRA}.
\textbf{NJU2K}~\cite{ju2014ACSD} is the largest RGB-D dataset containing  image pairs. \textbf{NLPR}~\cite{peng2014LHM} consists of  image pairs captured by a standard \textit{Microsoft Kinect} with a resolution of . \textbf{STERE}~\cite{niu2012STERE} is the first stereoscopic photo collection, containing  images downloaded from the Internet. \textbf{DES}~\cite{cheng2014DESM} is a small-scale RGB-D dataset that includes  indoor image pairs. \textbf{LFSD}~\cite{li2014LFSD} contains  image pairs from indoor scenes and  image pairs from outdoor scenes. \textbf{SSD}~\cite{zhu2017SSD} includes  images picked from three stereo
movies with both indoor and outdoor scenes. The collected images have a high resolution of .
\textbf{SIP}~\cite{fan2019D3Net} consists of  image pairs captured by a \textit{smart phone} with a resolution of , using a dual camera. \textbf{DUT}~\cite{piao2019DMRA} 
includes  images from multiple challenging scenes (\eg, transparent objects, multiple objects, complex backgrounds and low-intensity environments). \begin{table}[t!]
	\caption{ Performance of different models on the DUT~\cite{piao2019DMRA} dataset. Models are trained and tested on the DUT using the proposed training and test sets split from~\cite{piao2019DMRA}.}
	\vspace{-5pt}
	\label{tab:dut_result}
	\centering
\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\renewcommand{\arraystretch}{0.5}
	\setlength{\tabcolsep}{2.0mm}
	\footnotesize
	\begin{tabular}{c|r|ccc}
		\toprule
		\multicolumn{1}{c|}{\multirow{2}{*}{\#}}&\multicolumn{1}{c|}{\multirow{2}{*}{\diagbox{Method}{Dataset}}} & \multicolumn{3}{c}{DUT~\cite{piao2019DMRA}}\\\cline{3-5}
		&&&   &    \\\hline
		\multirow{5}{*}{Handcrafted}&
		MB~\cite{ZhuLGWW17MB}&.607&.577&.691\\
		&LHM~\cite{peng2014LHM}&.568&.659&.767\\
		&DESM~\cite{cheng2014DESM}&.659&.668&.733\\
		&DCMC~\cite{cong2016DCMC}&.499&.406&.712\\
		&CDCP~\cite{zhu2017CDCP}&.687&.633&.794\\\hline
		\multirow{4}{*}{Deep-based}&
DMRA~\cite{piao2019DMRA}&.888&.883&.927\\
		&A2dele~\cite{piao2020a2dele}&.886&.892&.929\\
		&SSF~\cite{zhang2020select}&.916&.924&.951\\
		&BBS-Net (ours)&\textbf{.920}&\textbf{.927}&\textbf{.955}\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[t!]
\footnotesize
	\caption{\small Multiple comparisons of BBS-Net and BBS-Net. The efficient version BBS-Net has only around  percent parameters of BBS-Net.}
\renewcommand\arraystretch{0.5}
	\setlength{\tabcolsep}{5.0mm}
	\begin{center}		
		\begin{tabular}{c|c|c|c}\toprule
			\#&Parameters (M)&FLOPs (G)&fps \\\hline
			BBS-Net&49.77&31.40&24.32\\
			BBS-Net&25.96&25.26&25.54\\
			\bottomrule
		\end{tabular}
	\end{center}
\label{tab:speed}
\end{table}


\begin{table*}[t!]
\centering
	\caption{ Quantitative comparison of models using S-measure (), max F-measure (), max E-measure () and MAE () scores on seven public datasets.
 () denotes that the higher (lower) the score, the better.
 denotes the efficient version of \ourmodel.
}
		\vspace{-5pt}
	\label{tab:methodcompare}
\footnotesize
	\renewcommand{\arraystretch}{0.5}
	\setlength\tabcolsep{2.05pt}
\begin{tabular}{cr|cccccccccc|cccccccc|cc}
		\toprule
		\multicolumn{1}{c}{\multirow{2}{*}{\rotatebox{90}{Data}}} & \multicolumn{1}{c|}{\multirow{2}{*}{Metric}} & \multicolumn{10}{c|}{Hand-crafted-features-Based Models} & \multicolumn{8}{c|}{CNNs-Based Models}&\multicolumn{2}{c}{BBS-Net} \\
		&&LHM &CDB &DESM &GP &CDCP &ACSD &LBE & DCMC &MDSF &SE &DF &AFNet &CTMF &MMCI &PCF &TANet &CPFP &DMRA&Ours&Ours \\
		&&~\cite{peng2014LHM}&~\cite{liang2018CDB}&\cite{cheng2014DESM}&~\cite{ren2015GP}&~\cite{zhu2017CDCP}&~\cite{ju2014ACSD}&~\cite{feng2016LBE}&~\cite{cong2016DCMC}&~\cite{song2017MDSF}&~\cite{guo2016SE}&~\cite{qu2017DF}&~\cite{wang2019AFNet}&~\cite{han2018CTMF}&~\cite{chen2019MMCI}&~\cite{chen2018PCF}&~\cite{chen2019TANet}&~\cite{zhao2019CPFP}&~\cite{piao2019DMRA}&&\\
\hline\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{NJU2K}}}
		&  &.514&.624&.665&.527&.669&.699&.695&.686&.748&.664&.763&.772&.849&.858&.877&.878&.879&{.886}&.916&\textbf{{.921}}\\
& 
		&.632&.648&.717&.647&.621&.711&.748&.715&.775&.748&.650&.775&.845&.852&.872&.874&.877&{.886}&.918&\textbf{{.920}}\\
& 
		&.724&.742&.791&.703&.741&.803&.803&.799&.838&.813&.696&.853&.913&.915&.924&.925&.926&{.927}&.948&\textbf{{.949}}\\
		& 
		&.205&.203&.283&.211&.180&.202&.153&.172&.157&.169&.141&.100&.085&.079&.059&.060&.053&{.051}&.038&\textbf{.035}\\
		\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{NLPR}}}
		& 
		&.630&.629&.572&.654&.727&.673&.762&.724&.805&.756&.802&.799&.860&.856&.874&.886&.888&{.899}&.925&\textbf{{.930}}\\
& 
		&.622&.618&.640&.611&.645&.607&.745&.648&.793&.713&.778&.771&.825&.815&.841&.863&.867&{.879}&.909&\textbf{{.918}}\\
& 
		&.766&.791&.805&.723&.820&.780&.855&.793&.885&.847&.880&.879&.929&.913&.925&.941&.932&{.947}&.959&\textbf{{.961}}\\
		& 
		&.108&.114&.312&.146&.112&.179&.081&.117&.095&.091&.085&.058&.056&.059&.044&.041&.036&{.031}&.026&\textbf{{.023}}\\
		\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{STERE}}}
		& 
		&.562&.615&.642&.588&.713&.692&.660&.731&.728&.708&.757&.825&.848&.873&.875&.871&{.879}&.835&.905&\textbf{{.908}}\\
& 
		&.683&.717&.700&.671&.664&.669&.633&.740&.719&.755&.757&.823&.831&.863&.860&.861&{.874}&.847&.898&\textbf{{.903}}\\
& 
		&.771&.823&.811&.743&.786&.806&.787&.819&.809&.846&.847&.887&.912&{.927} &.925&.923&.925&.911&.940&\textbf{{.942}}\\
		& 
		&.172&.166&.295&.182&.149&.200&.250&.148&.176&.143&.141&.075&.086&.068&.064&.060&{.051}&.066&.043&\textbf{{.041}}\\
		\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{DES}}}
		& 
		&.578&.645&.622&.636&.709&.728&.703&.707&.741&.741&.752&.770&.863&.848&.842&.858&.872&{.900}&.930&\textbf{{.933}}\\
& 
		&.511&.723&.765&.597&.631&.756&.788&.666&.746&.741&.766&.728&.844&.822&.804&.827&.846&{.888}&.921&\textbf{{.927}}\\
& 
		&.653&.830&.868&.670&.811&.850&.890&.773&.851&.856&.870&.881&.932&.928&.893&.910&.923&{.943}&.965&\textbf{{.966}}\\
		& 
		&.114&.100&.299&.168&.115&.169&.208&.111&.122&.090&.093&.068&.055&.065&.049&.046&.038&{.030}&.022&\textbf{{.021}}\\
		\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{LFSD}}}
		& 
		&.553&.515&.716&.635&.712&.727&.729&.753&.694&.692&.783&.738&.788&.787&.786&.801&.828&{.839}&.859&\textbf{{.864}}\\
& 
		&.708&.677&.762&.783&.702&.763&.722&.817&.779&.786&.813&.744&.787&.771&.775&.796&.826&{.852}&.855&\textbf{{.858}}\\
& 
		&.763&.871&.811&.824&.780&.829&.797&.856&.819&.832&.857&.815&.857&.839&.827&.847&.863&{.893}&.896&\textbf{{.901}}\\
		& 
		&.218&.225&.253&.190&.172&.195&.214&.155&.197&.174&.145&.133&.127&.132&.119&.111&.088&{.083}&.076&\textbf{{.072}}\\
		\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{SSD}}}
		& 
		&.566&.562&.602&.615&.603&.675&.621&.704&.673&.675&.747&.714&.776&.813&.841&.839&.807&{.857}&.858&\textbf{{.882}}\\
& 
		&.568&.592&.680&.740&.535&.682&.619&.711&.703&.710&.735&.687&.729&.781&.807&.810&.766&{.844}&.827&\textbf{{.859}}\\
& 
		&.717&.698&.769&.782&.700&.785&.736&.786&.779&.800&.828&.807&.865&.882&.894&.897&.852&{.906}&{.894}&{\textbf{.919}}\\
		& 
		&.195&.196&.308&.180&.214&.203&.278&.169&.192&.165&.142&.118&.099&.082&.062&.063&.082&{.058}&.058&\textbf{{.044}}\\
		\hline
		\multicolumn{1}{c}{\multirow{4}{*}{\rotatebox{90}{SIP}}}
		& 
		&.511&.557&.616&.588&.595&.732&.727&.683&.717&.628&.653&.720&.716&.833&.842&.835&{.850}&.806&.876&\textbf{{.879}}\\
& 
		&.574&.620&.669&.687&.505&.763&.751&.618&.698&.661&.657&.712&.694&.818&.838&.830&{.851}&.821&.880&\textbf{{.883}}\\
& 
		&.716&.737&.770&.768&.721&.838&.853&.743&.798&.771&.759&.819&.829&.897&.901&.895&{.903}&.875&.919&\textbf{{.922}}\\
		& 
		&.184&.192&.298&.173&.224&.172&.200&.186&.167&.164&.185&.118&.139&.086&.071&.075&{.064}&.085&.056&\textbf{{.055}}\\	
		\bottomrule	
	\end{tabular}
\end{table*}
\myPara{\textbf{Training/Testing.}} We follow the same settings as~\cite{chen2018PCF,piao2019DMRA} for fair comparison. In particular, the training set contains  samples from the NJU2K dataset and  samples from the NLPR dataset.
The test set consists of the remaining images from NJU2K () and NLPR (), and the whole of STERE (), DES, LFSD, SSD and SIP.
As for the recent proposde DUT~\cite{piao2019DMRA} dataset, following~\cite{piao2019DMRA}, 
we adopt the same training data of DUT, NJU2K, and NLPR to train the compared deep models (\ie, DMRA~\cite{piao2019DMRA}, A2dele~\cite{piao2020a2dele}, SSF~\cite{zhang2020select}, and our \ourmodel) and test the performance on the test set of DUT. Please refer to \tabref{tab:dut_result} for more details.

\myPara{\textbf{Evaluation Metrics.}}
We employ five widely used metrics, including S-measure ()~\cite{Fan2017Smeasure},
E-measure ()~\cite{Fan2018Emeasure},
F-measure ()~\cite{Ach2009Fmeasure},
mean absolute error (MAE), and precision-recall (PR) curves to evaluate various methods. 
Evaluation code: \href{http://dpfan.net/d3netbenchmark/}{http://dpfan.net/d3netbenchmark/}.



\subsection{Comparison with SOTAs}\label{sec:compare_sota}
\myPara{\textbf{Contenders.}}\label{sec:contenders}
We compare the proposed~\ourmodel~with ten algorithms based on handcrafted features~\cite{cheng2014DESM,cong2016DCMC,feng2016LBE,guo2016SE,ju2014ACSD,liang2018CDB,peng2014LHM,ren2015GP,zhu2017CDCP,song2017MDSF}
and eight methods~\cite{chen2018PCF,chen2019TANet,chen2019MMCI,han2018CTMF,piao2019DMRA,qu2017DF,wang2019AFNet,zhao2019CPFP} that use deep learning.
We train and test these methods using their default settings.
For the methods without released source codes, we compare with their reported results.\par




\myPara{\textbf{Quantitative Results.}} As shown in \tabref{tab:dut_result}, \tabref{tab:methodcompare}, our method outperforms all algorithms based on handcrafted features as well as SOTA CNN-based methods by a large margin, in terms of all four evaluation metrics (\ie, S-measure (), F-measure (), E-measure () and MAE ()).
Performance gains over the best compared algorithms (ICCV'19 DMRA~\cite{piao2019DMRA} and CVPR'19 CPFP~\cite{zhao2019CPFP}) are (, , , ) for the metrics (, , , ) on seven challenging datasets.
The PR curves of different methods on various datasets are shown in \figref{fig:prcurve}.
It can be easily deduced from the PR curves that our method (\ie, solid red lines) outperforms all the SOTA algorithms.
\par
In terms of speed, \ourmodel~achieves  fps on a single GTX 1080Ti GPU (batch size of one), as shown in \tabref{tab:speed}, which is suitable for real-time applications.
In terms of parameters, \ourmodel contains only around  percent parameters of the \ourmodel~(\ie, M vs. M), but its performance is similar to the \ourmodel~and also superior to other compared methods (as shown in the last two columns in \tabref{tab:methodcompare}).
It means that \ourmodel can process more images in the same time (with a larger batch size) for real-world applications.
\par
There are three popular backbone models used in deep RGB-D models (\ie, VGG-16~\cite{simonyan2014vgg}, VGG-19~\cite{simonyan2014vgg} and ResNet-50~\cite{He2016resnet}).
To further validate the effectiveness of the proposed method, we provide performance comparisons using different backbones in \tabref{tab:backbone}.
We find that ResNet-50 performs best among the three backbones, and VGG-19 and VGG-16 have similar performances.
Besides, the proposed method exceeds the SOTA methods (\eg, TANet~\cite{chen2019TANet}, CPFP~\cite{zhao2019CPFP}, and DMRA~\cite{piao2019DMRA}) with any of the backbones.


\begin{figure*}[t!]
	\centering
	\begin{overpic}[width=.98\linewidth]{resultmap-min}
	\end{overpic}
	\vspace{-10pt}
	\caption{ \textbf{Qualitative visual comparison of our model versus eight SOTA models.} }
	\label{fig:visual_result}
\end{figure*}
\begin{table*}[t!]
	\caption{ Performance comparison using different backbone models. We experiment with multiple popular backbone models used in RGB-D SOD, including VGG-16~\cite{simonyan2014vgg}, VGG-19~\cite{simonyan2014vgg} and ResNet-50~\cite{He2016resnet}.}
		\vspace{-5pt}
	\label{tab:backbone}
	\centering
	\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\setlength{\tabcolsep}{2.40mm}{
		\footnotesize
		\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc}
			\toprule
			\multicolumn{1}{c|}{\multirow{2}{*}{{Models}}} & \multicolumn{2}{c|}{NJU2K~\cite{ju2014ACSD}} & \multicolumn{2}{c|}{NLPR~\cite{peng2014LHM}} & \multicolumn{2}{c|}{STERE~\cite{niu2012STERE}}& \multicolumn{2}{c|}{DES~\cite{cheng2014DESM}}& \multicolumn{2}{c|}{LFSD~\cite{li2014LFSD}}& \multicolumn{2}{c|}{SSD~\cite{zhu2017SSD}}&
			\multicolumn{2}{c}{SIP~\cite{fan2019D3Net}} \\
			&&& && && && && && &\\
			\midrule
			TANet (VGG-16)~\cite{chen2019TANet}&.878&.060 &.886&.041 &.871&.060 &.858&.046 &.801&.111 &.839&.063 &.835&.075\\
			CPFP (VGG-16)~\cite{zhao2019CPFP}&.879&.053 &.888&.036 &.879&.051 &.872&.038 &.828&.088 &.807&.082 &.850&.064\\
			Ours (VGG-16)&.916&.039 &.923&.026 &.896&.046 &.908&.028 &.845&.080 &.858&.055 &.874&.056\\
			\hline
			DMRA (VGG-19)~\cite{piao2019DMRA}&.886&.051 &.899&.031 &.835&.066 &.900&.030 &.839&.083 &.857&.058 &.806&.085\\
			Ours (VGG-19)&.918&.037 &.925&.025 &.901&.043 &.915&.026 &.852&.074 &.855&.056 &.878&\textbf{.054}\\
			\hline
			D3Net (ResNet-50)~\cite{fan2019D3Net}&.900&.041 &.912&.030 &.899&.046 &.898&.031 &.825&.095 &.857&.058 &.860&.063\\
			\rowcolor{mygray}
			Ours (ResNet-50)&\textbf{.921}&\textbf{.035} &\textbf{.930}&\textbf{.023} &\textbf{.908}&\textbf{.041} &\textbf{.933}&\textbf{.021} &\textbf{.864}&\textbf{.072} &\textbf{.882}&\textbf{.044}  &\textbf{.879}&.055\\
			
			\bottomrule
	\end{tabular}}
\end{table*}



\myPara{\textbf{Visual Comparison.}}\label{sec:visual_comparison}
\figref{fig:visual_result} provides examples of maps predicted by our method and several SOTA algorithms.
Visualizations cover simple scenes (a) and various challenging scenarios, including small objects (b), multiple objects (c), complex backgrounds (d), and low contrast scenes (e).\par
First, the first row of (a) shows an easy example. The flower in the foreground is evident in the original RGB image, but the depth map is of low quality and contains some misleading information.
The SOTA algorithms, such as DMRA and CPFP, fail to predict the whole extent of the salient object due to the interference from the depth map. Our method can eliminate the side-effects of the depth map by utilizing the complementary depth information more effectively.
Second, two examples of small objects are shown in (b). Despite the handle of the teapot in the first row being tiny, our method can accurately detect it.
Third, we show two examples with multiple objects in an image in (c).
Our method locates all salient objects in the image. It segments the objects more accurately and generates sharper edges compared to other algorithms.
Even though the depth map in the first row of (c) lacks clear information, our algorithm predicts the salient objects correctly.
Fourth, (d) shows two examples with complex backgrounds.
Here, our method produces reliable results, while other algorithms confuse the background as a salient object.
Finally, (e) presents two examples in which the contrast between the object and the background is low.
Many algorithms fail to detect and segment the entire extent of the salient object.
Our method produces satisfactory results by suppressing background distractors and exploring the informative cues from the depth map.


\subsection{Ablation Study}\label{sec:ablation_study}
\vspace{-5pt}
\myPara{\textbf{Analysis of Different Aggregation Strategies.}}
To validate the effectiveness of our cascaded refinement mechanism,
we conduct several experiments to explore different aggregation strategies. Results are shown in \tabref{tab:agg} and \figref{fig:visual_agg}.
`Low3' means that we only integrate the low-level features (\textit{Conv1}\textit{3}) using the decoder without the refinement from the initial map.
Low-level features contain abundant details that are beneficial for refining the object edges, but at the same time introduce a lot of background distraction.
\begin{table*}[t!]
	\caption{Comparison of feature aggregation strategies. 
	1: Only aggregating the low-level features (\textit{Conv1}\textit{3}), 2: Only aggregating the high-level features (\textit{Conv3}\textit{5}), 3: Directly integrating all five-level features (\textit{Conv1}\textit{5}) by a single decoder, 4: Our model without the refinement flow, 5: High-level features (\textit{Conv3}\textit{5}) are first refined by the initial map aggregated by low-level features (\textit{Conv1}\textit{3}) and are then integrated to generate the final saliency map, and 6: Our cascaded refinement mechanism.}
	\vspace{-5pt}
	\label{tab:agg}
	\centering
\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\setlength{\tabcolsep}{2.52mm}{
		\footnotesize
		\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc}
			\toprule
			\multicolumn{1}{c|}{\multirow{2}{*}{{\#}}}&
			\multicolumn{1}{c|}{\multirow{2}{*}{Settings}} & \multicolumn{2}{c|}{NJU2K~\cite{ju2014ACSD}} & \multicolumn{2}{c|}{NLPR~\cite{peng2014LHM}} & \multicolumn{2}{c|}{STERE~\cite{niu2012STERE}}& \multicolumn{2}{c|}{DES~\cite{cheng2014DESM}}& \multicolumn{2}{c|}{LFSD~\cite{li2014LFSD}}& \multicolumn{2}{c|}{SSD~\cite{zhu2017SSD}}&
			\multicolumn{2}{c}{SIP~\cite{fan2019D3Net}} \\
			&& && && && && && && &\\
			\midrule
			1&Low 3 levels&.881&.051 &.882&.038  &.832&.070  &.853&.044  &.779&.110  &.805&.080&.760&.108   \\
			2&High 3 levels&.902&.042
			&.911&.029 &.886&.048 &.912&.026  &.845&.080  &.850&.058 &.833&.073\\
			3&All 5 levels&.905&.042   &.915&.027  &.891&.045 &.901&.028  &.845&.082  &.848&.060&.839&.071   \\
			4&BBS-NoRF&.893&.050 &.904&.035 &.843&.072 &.886&.039 &.804&.105 &.839&.069 &.843&.076\\
			5&BBS-RH &.913&.040  &.922&.028  &.881&.054  &.919&.027  &.833&.085 &.872&.053&.866&.063 \\\hline
			\rowcolor{mygray}
			6&BBS-RL (ours)&\textbf{.921}&\textbf{.035}  &\textbf{.930}&\textbf{.023}  &\textbf{.908}&\textbf{.041} &\textbf{.933}&\textbf{.021}  &\textbf{.864}&\textbf{.072}  &\textbf{.882}&\textbf{.044}&\textbf{.879}&\textbf{.055}  \\
			\bottomrule
	\end{tabular}}
\end{table*}
\begin{table*}[t!]
\caption{ Ablation analysis of our \ourmodel. `BM' = base model. `CA' = channel attentio. `SA' = spatial attention.  `PTM' = progressively transposed module.}
	\vspace{-5pt}
\label{tab:ablation}
	\centering
	\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\footnotesize
	\setlength{\tabcolsep}{2.12mm}{
		\begin{tabular}{c|cccc|cc|cc|cc|cc|cc|cc|cc}
			\toprule
			\multicolumn{1}{c|}{\multirow{2}{*}{{\#}}}&
			\multicolumn{4}{c|}{Settings} &
			\multicolumn{2}{c|}{NJU2K~\cite{ju2014ACSD}} & \multicolumn{2}{c|}{NLPR~\cite{peng2014LHM}} & \multicolumn{2}{c|}{STERE~\cite{niu2012STERE}}& \multicolumn{2}{c|}{DES~\cite{cheng2014DESM}}& \multicolumn{2}{c|}{LFSD~\cite{li2014LFSD}}& \multicolumn{2}{c|}{SSD~\cite{zhu2017SSD}}&
			\multicolumn{2}{c}{SIP~\cite{fan2019D3Net}} \\	&BM&CA&SA&PTM& && && && && && &&& \\
			\midrule
			1& \checkmark &          &          &          &.908&.045
			&.918&.029  &.882&.055  &.917&.027  &.842&.083 &.862&.057&.864&.066 \\
			2& \checkmark &\checkmark&          &          &.913&.042
			&.922&.027 &.896&.048 &.923&.025  &.840&.086  &.855&.057&.868&.063\\
			3& \checkmark &          &\checkmark&
			&.912&.045 &.918&.029 &.891&.054 &.914&.029  &.855&.083  &.872&.054&.869&.063 \\
			4&\checkmark&\checkmark&\checkmark&            &.919&.037
			&.928&.026 &.900&.045 &.924&.024  &.861&.074  &.873&.052&.869&.061 \\
			\rowcolor{mygray}\hline
			5& \checkmark &\checkmark&\checkmark&\checkmark&\textbf{.921}&\textbf{.035} &\textbf{.930}&\textbf{.023} &\textbf{.908}&\textbf{.041} &\textbf{.933}&\textbf{.021}  &\textbf{.864}&\textbf{.072}  &\textbf{.882}&\textbf{.044}&\textbf{.879}&\textbf{.055} \\
			\bottomrule
	\end{tabular}}
\end{table*}
Integrating only low-level features produces inadequate results
and generates many distractors
(\eg, the example in \figref{fig:visual_agg}). `High3' only integrates the high-level features (\textit{Conv3}\textit{5}) to predict the saliency map.
Compared with low-level features, high-level features contain more semantic information.
As a result, they help locate the salient objects and preserve edge information.
Thus, integrating high-level features leads to better results.
`All5' aggregates features from all five levels (\textit{Conv1}\textit{5}) directly, using a single decoder for training and testing.
It achieves comparable results with the 'High3' but may include background noise introduced by the low-level features (see column `All5' in \figref{fig:visual_agg}).
`BBS-NoRF' indicates that we directly remove the refinement flow of our model. This leads to poor performance.
`BBS-RH' is a reverse refinement strategy to our cascaded refinement mechanism, where teacher features (\textit{Conv3}\textit{5}) are first refined by the initial map aggregated by low-level features (\textit{Conv1}\textit{3}) and are then integrated to generate the final saliency map.
It performs worse than the proposed mechanism (BBS-RL), because noise in low-level features cannot be effectively suppressed in this reverse refinement strategy.
Besides, compared to `All5', our method fully utilizes the features at different levels, and thus achieves significant performance improvement (\ie, the last row in \tabref{tab:agg}) with fewer background distractors and sharper edges. \par
\begin{figure}[t!]
	\centering
	\begin{overpic}[width=1.0\linewidth]{result_agg}
	\end{overpic}
	\vspace{-20pt}
	\caption{ \textbf{Visual comparison of aggregation strategies.} `Low3' only integrates low-level features (\textit{Conv1}\textit{3}), while `High3' aggregates high-level features (\textit{Conv3}\textit{5}) for predicting the saliency map. `All5' combines all five-level features directly for prediction. `BBS-RH/BBS-RL' denotes that high-level/low-level features are first refined by the initial map aggregated by the low-level/high-level features and are then integrated to predict the final map.}
\label{fig:visual_agg}
\end{figure}
\begin{figure}[t!]
	\centering
	\begin{overpic}[width=1.0\linewidth]{result_ablation_revise}
	\end{overpic}
	\vspace{-20pt}
	\caption{\textbf{Analysis of gradually adding various modules.} The first two columns are the RGB and ground-truth images, respectively. `\#' denotes the corresponding row of \tabref{tab:ablation}. }
	\label{fig:visual_ablation}
\end{figure}
\myPara{\textbf{Impact of Different Modules.}} To validate the effectiveness of the different modules in the proposed \ourmodel, we conduct various experiments, as shown in \tabref{tab:ablation} and \figref{fig:visual_ablation}.
The base model (BM) is our \ourmodel ~without additional modules (\ie, CA, SA, and PTM).
Note that the BM alone performs better than the SOTA methods over almost all datasets, as shown in \tabref{tab:methodcompare} and \tabref{tab:ablation}.
Adding the channel attention (CA) and spatial attention (SA) modules enhances the performance on most of the datasets (see the results shown in the second and third rows of \tabref{tab:ablation}).
When we combine the two modules (the fourth row in \tabref{tab:ablation}), the performance is greatly improved on all datasets, compared to the BM.
We can easily conclude from the `\#2', `\#3' and `\#4' columns in 
\figref{fig:visual_ablation} that the spatial attention and
channel attention mechanisms in DEM allow the model to focus on the informative parts of the depth features, which results in better suppression of background clutter.
Finally, we add a progressively transposed block before the second decoder to gradually upsample the feature map to the same resolution as the ground truth.
The results in the fifth row of \tabref{tab:ablation} and the '\#5' column of \figref{fig:visual_ablation} show that the `PTM' achieves impressive performance gains on all datasets and generates sharper edges with finer details. 
\par
To further analyze the effectiveness of the cascaded decoder, we experiment with changing it to an element-wise summation mechanism.
That is to say, we first change the features from different layers to the same dimension using  convolution and upsampling operation and then fuse them by element-wise summation.
Experimental results in \tabref{tab:cascaded_decoder} show that the cascaded decoder achieves comparable results on SIP, and outperforms the element-wise sum on the other six datasets, which demonstrates its effectiveness.
\begin{table}[t!]
	\vspace{-5pt}
	\caption{  Effectiveness analysis of the cascaded decoder in terms of the S-measure () on seven datasets.}
	\vspace{-5pt}
	\label{tab:cascaded_decoder}
	\centering
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{1.15mm}{
		\footnotesize
		\begin{tabular}{r|ccccccccc}
			\toprule
			{\multirow{2}{*}{Methods}}&NJU2K&NLPR&STERE&DES&SSD&LFSD&SIP \\
			&~\cite{ju2014ACSD}&~\cite{peng2014LHM}&~\cite{niu2012STERE}&~\cite{cheng2014DESM}&~\cite{zhu2017SSD}&~\cite{li2014LFSD}&~\cite{fan2019D3Net}\\\midrule
			Element-wise sum&.915 &.925&.897&.925&.868&.856&\textbf{.880}\\
			\hline
			\rowcolor{mygray}
			Cascaded decoder&\textbf{.921} &\textbf{.930}&\textbf{.908}&\textbf{.933}&\textbf{.882}&\textbf{.864}&.879\\
			\bottomrule
	\end{tabular}}\end{table}
\begin{table}[t!]
\caption{\small Hyper-parameter  analysis on the NJU2K dataset. We do not report the result for , because its loss of the final predicted map is .}
	\label{tab:hyper}
	\centering
\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\setlength{\tabcolsep}{0.98mm}{
		\footnotesize
		\begin{tabular}{c|ccccccccccc}
			\toprule
			&0&0.1&0.2&0.3&0.4&0.5&0.6&0.7&0.8&0.9\\ \hline
			NJU2K ()&.918&.925&.923&.919&.924&.923&.920&.923&.922&.924\\
			NJU2K (MAE)&.037&.034&.034&.036&.033&.034&.035&.034&.035&.033\\
			\bottomrule
	\end{tabular}}\end{table}
\par
\myPara{Hyper-parameter Analysis.} We conduct an experiment to discuss the settings of .
As shown in \tabref{tab:hyper}, the performance ( and MAE) is about the same for different values of , thus we simply set it to  to balance the weight between the losses of the initial map and the final map.
\begin{table}[t!]
	\vspace{-5pt}
	\caption{Effectiveness analysis of the depth adapter module in terms of the S-measure () on seven datasets.  represents the efficient version of \ourmodel, where the two backbones share parameters.}
	\vspace{-5pt}
	\label{tab:dam}
	\centering
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{0.8mm}{
		\footnotesize
		\begin{tabular}{r|ccccccccc}
			\toprule
			{\multirow{2}{*}{Settings}}&NJU2K&NLPR&STERE&DES&SSD&LFSD&SIP \\
			&~\cite{ju2014ACSD}&~\cite{peng2014LHM}&~\cite{niu2012STERE}&~\cite{cheng2014DESM}&~\cite{zhu2017SSD}&~\cite{li2014LFSD}&~\cite{fan2019D3Net}\\\midrule
			\ourmodel (w/o DAM)&.905&.922&.899&.928&.856&.841&.849\\
			\hline
			\rowcolor{mygray}
			\ourmodel (w/ DAM)&\textbf{.916} &\textbf{.925}&\textbf{.905}&\textbf{.930}&\textbf{.858}&\textbf{.859}&\textbf{.876}\\
			\bottomrule
	\end{tabular}}\end{table}

\par
\myPara{Effectiveness Analysis of the Depth Adapter Module.} To demonstrate the effectiveness of the proposed depth adapter module (DAM), we conduct an experiment in \tabref{tab:dam}.
As shown in the table, \ourmodel (w/ DAM) performs better than \ourmodel (w/o DAM) on seven datasets, especially on the dataset of NJU2K, LFSD, and SIP.
The DAM can model the modality difference between the RGB image and depth image, reduces the gap between them. Thus the same backbone is more suitable to extract two different modality features.
\section{Discussion}\label{sec:discussion}


\begin{table}[t!]
	\vspace{-5pt}
	\caption{ S-measure () comparison with SOTA RGB SOD methods. `w/o depth' and `w/ depth' represent training and testing the proposed method without/with the depth information (\ie, the inputs of the depth branch are or are not set to zeros).}
	\vspace{-5pt}
	\label{tab:depth_benefits}
	\centering
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{0.85mm}{
		\footnotesize
		\begin{tabular}{r|ccccccccc}
			\toprule
			\multirow{2}{*}{Methods}&NJU2K&NLPR&STERE&DES&LFSD&SSD&SIP \\
			&~\cite{ju2014ACSD}&~\cite{peng2014LHM}&~\cite{niu2012STERE}&~\cite{cheng2014DESM}&~\cite{li2014LFSD}&~\cite{zhu2017SSD}&~\cite{fan2019D3Net}\\
			\midrule
			PiCANet~\cite{Liu_2018_PiCAN}&.847&.834&.868&.854&.761&.832&-\\
			PAGRN~\cite{Zhang2018PAGR}&.829&.844&.851&.858&.779&.793&-\\
			R3Net~\cite{deng2018r3net}&.837&.798&.855&.847&.797&.815&-\\
			CPD~\cite{Wu2019CPD}&.894&.915&.902&.897&.815&.839&.859\\
			PoolNet~\cite{Liu2019SPBD}&.887&.900&.880&.873&.787&.773&.861\\
			\hline
			BBS-Net (w/o depth)&.914&.925&.915&.912&.836&.855&.875\\
			\rowcolor{mygray}
			BBS-Net (w/ depth) &\textbf{.921}&\textbf{.930}&\textbf{.908}&\textbf{.933}&\textbf{.864}&\textbf{.882}&\textbf{.879}\\
			\bottomrule
	\end{tabular}}\end{table}


\subsection{Utility of Depth Information}\label{sec:depth_benefits}
To explore whether depth information can really contribute to the performance of SOD, we conduct two experiments, results of which are shown in \tabref{tab:depth_benefits}.
On the one hand, we compare the proposed method with five SOTA RGB SOD methods (\ie, PiCANet~\cite{Liu_2018_PiCAN}, PAGRN~\cite{Zhang2018PAGR}, R3Net~\cite{deng2018r3net}, CPD~\cite{Wu2019CPD}, and PoolNet~\cite{Liu2019SPBD}) by neglecting the depth information.
We train and test CPD and PoolNet using the same training and test sets as our model.
For other methods, we use the published results from \cite{piao2019DMRA}.
It is clear that the proposed methods (\ie, BBS-Net (w/ depth)) can significantly exceed SOTA RGB SOD methods thanks to depth information.
On the other hand, we train and test the proposed method without using the depth information by setting the inputs of the depth branch to zero (\ie, BBSNet (w/o depth)).
Comparing the results of the last two rows in the table, we find that depth information effectively improves the performance of the proposed model (especially over the small datasets, \ie, DES, LFSD, and SSD).\par
The two experiments together demonstrate the benefits of the depth information for SOD.
Depth map serves as prior knowledge and provides spatial distance information and contour guidance to detect salient objects.
For example, in \figref{fig:feature}, depth feature (b) has high activation on the object border.
Thus, cross-modal feature (c) has clearer borders compared with the original RGB feature (a).
\begin{figure}[t!]
	\centering	
	\begin{overpic}[width=1.0\linewidth]{feature}
	\end{overpic}
	\vspace{-20pt}
	\caption{ Feature visualization. Here, (a), (b), and (c) are the average RGB feature, depth feature and cross-modal feature of the \textit{Conv3} layer. To visualize them, we average the
		feature maps along their channel axis to obtain the visualization map. `Ours' refers to the BBS-Net (w/ depth).}
	\label{fig:feature}
\end{figure}


\subsection{Analysis of Post-processing\label{post} Methods}\label{sec:post_process}
According to~\cite{yang2016top,wang2019pyramid,zeng2019joint}, the predicted saliency maps can be further refined by post-processing methods.
This may be useful to sharpen the salient edges and suppress the background response.
We conduct several experiments to study the effects of various post-processing methods, including the adaptive threshold cut (\ie, the threshold is defined as the double of the mean value of the saliency map), Ostu's method~\cite{otsu1979threshold}, and conditional random field (CRF)~\cite{KrahenbuhlK11crf}.
The performance comparisons of the post-processing methods in terms of MAE are shown in \tabref{tab:postpro}, while a visual comparison is provided in \figref{fig:post_pro}.\par



From the results, we draw the following conclusions.
First, the three post-processing methods all make the salient edges sharper, as shown in the fourth to sixth columns in \figref{fig:post_pro}.
\begin{table}[t!]
\caption{Performance comparison (MAE) of different post-processing strategies on seven datasets. The last column is the time for the post-processing methods to optimize each image. See \secref{post} for details.
}
	\vspace{-5pt}
\label{tab:postpro}
	\centering
\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\setlength{\tabcolsep}{0.85mm}{
		\footnotesize
		\begin{tabular}{l|cccccccccc}
			\toprule
			\multirow{2}{*}{Strategy}&NJU2K&NLPR&STERE&DES&LFSD&SSD&SIP&time \\
			&~\cite{ju2014ACSD}&~\cite{peng2014LHM}&~\cite{niu2012STERE}&~\cite{cheng2014DESM}&~\cite{li2014LFSD}&~\cite{zhu2017SSD}&~\cite{fan2019D3Net}&ms\\
			\midrule
			BBS-Net&.035&.023&.041&.021&.072&.044&.055&-\\
			BBS-Net+ADP&.050&.024&.049&\textbf{.018}&.072&.053&.055&1.46\\
			BBS-Net+Ostu&\textbf{.030}&\textbf{.020}&.036&\textbf{.018}&.066&.039&\textbf{.051}&0.99\\\rowcolor{mygray}
			\hline
			BBS-Net+CRF&\textbf{.030}&\textbf{.020}&\textbf{.035}&.019&\textbf{.065}&\textbf{.038}&\textbf{.051}&450.8\\
			\bottomrule
			
	\end{tabular}}\vspace{5pt}
\end{table}
\begin{figure}[t!]
	\centering
	\begin{overpic}[width=1.0\linewidth]{postprocess}
	\end{overpic}
	\vspace{-20pt}
	\caption{ Visual effects of different post-processing methods. We explore three methods, including the adaptive threshold cut (`ADP' in the paper), Ostu's method and the popular algorithm of conditional random fields (CRF). }
\label{fig:post_pro}
\end{figure}
Second, both Ostu and CRF help reduce the MAE effectively, as shown in \tabref{tab:postpro}.
This is possibly because they can suppress the background noise.
As shown in \figref{fig:post_pro}, Ostu and CRF can significantly reduce the background noise, while the adaptive threshold operation further expands the background blur from the original results of~\ourmodel.
Further, in terms of overall results, CRF performs the best, while the adaptive threshold algorithm is the worst.
Ostu performs worse than CRF, because it cannot always fully eliminate the background noise (\eg, the fifth and sixth columns in \figref{fig:post_pro}).
\begin{figure}[t!]
	\centering	
	\begin{overpic}[width=1.0\linewidth]{failure_case}
	\end{overpic}
\caption{\small Some representative failure cases of the model.} \label{fig:failure_case}
\end{figure}
\subsection{Failure Case Analysis}\label{sec:failure_cases}
We illustrate six representative failure cases in \figref{fig:failure_case}.
The failure examples are divided into four categories.
In the first category, the model either misses the salient object or detects it imperfectly.
For example, in column (a), our model fails to detect the salient object even when the depth map has clear boundaries.
This is because the salient object has the same texture and content layout as the background in the RGB image. Thus, the model cannot find the salient object based only on the borders.
In column (b), our method cannot fully segment the transparent salient objects, since the background has low contrast, and the depth map lacks useful information.
The second situation is that the model identifies the background as the salient part.
For example, the lanterns in column (c) have a similar color to the background wallpaper, which confuses the model into thinking that the wallpaper is the salient object. 
Besides, the background of the RGB image in column (d) is complex and thus our model does not detect the complete salient objects.
The third type of failure case is when an image contains several separate salient objects. In this case, our model may not detect them all.
As shown in column (e), with five salient objects in the RGB images, the model fails to detect the two objects that are far from the camera.
This may be because the model tends to consider the objects that are closer to the camera more salient.
The final case is when salient objects are occluded by non-salient ones.
Note that in column (f), the car is occluded by two ropes in front of the camera. Here our model predicts the ropes as salient objects.\par
Most of these failure cases can be attributed to interference information from the background (\eg, color, contrast, and content).
We propose some ideas that may be useful for solving these failure cases.
The first is to introduce some human-designed prior knowledge, such as providing a boundary that can approximately distinguish the foreground from the background.
Leveraging such prior knowledge, the model may better capture the characteristics of the background and salient objects.
This strategy may contribute significantly to solving the failure cases especially for columns (a) and (b).
Besides, the depth map can also be seen as a type of prior knowledge for this task.
Thus, some failure cases (\ie, (b), (c), and (e)) may be solved when a high-quality depth map is available.
Second, we find that in the current RGB-D datasets, the image pairs for challenging scenarios (\eg, complex backgrounds, low-contrast backgrounds, transparent objects, multiple objects, shielded objects, and small objects) constitute a small fraction of the whole dataset.
Therefore, adding more difficult examples to the training data could help mitigate the failure cases.
Finally, depth maps may sometimes introduce misleading information, such as in column (d).
Considering how to exploit salient cues from the RGB image to suppress the noise in the depth map could be a promising solution.

\subsection{Cross-Dataset Generalization Analysis}\label{sec:cross_datasets}
\begin{table*}[t!]
	\caption{ Performance comparison when training with different datasets. The number in parentheses denotes the number of the corresponding training and test images. See \secref{sec:cross_datasets} for details.
}
	\vspace{-5pt}
	\label{tab:single_dataset}
	\centering
\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\setlength{\tabcolsep}{1.80mm}{
		\footnotesize
		\begin{tabular}{r|cc|cc|cc|cc|cc||cc|cc|cc}
			\toprule
			\multicolumn{1}{c|}{\multirow{2}{*}{{\diagbox{Train}{Test}}}} & \multicolumn{2}{c|}{NJU2K~(1285)} & \multicolumn{2}{c|}{NLPR~(300)} & \multicolumn{2}{c|}{STERE~(300)}&
			\multicolumn{2}{c|}{SIP~(229)}&
			\multicolumn{2}{c||}{DUT~(500)}& \multicolumn{2}{c|}{Self}& \multicolumn{2}{c|}{Mean Others}& \multicolumn{2}{c}{Drop~} \\
			&& &&  &&  &&  & & &&  &&  &&\\\midrule
			NJU2K~(700)&\textbf{.902}&\textbf{.894} &.834&.795 &.864&.846 &.802&.782 &.741&.691 &.902&.894 &.810&.779 &10.2\%&12.9\%\\
			NLPR~(700)&.712&.689 &\textbf{.919}&\textbf{.903} &.876&.882 &.883&.881 &.795&.779 &.919&.903 &.817&.808 &11.2\%&10.5\% \\
			STERE~(700)&.779&.741 &.897&.868 &\textbf{.915}&\textbf{.913} &.900&.900 &.724&.731 &.915&.913 &.825&.810 &9.8\%&11.3\%  \\
			SIP~(700)&.436&.325 &.618&.528 &.534&.479 &\textbf{.963}&\textbf{.972} &.423&.303 &.963&.972 &.503&.409 &47.8\%&57.9\%  \\
			DUT~(700)&.751&.777 &.808&.761 &.736&.764 &.801&.802 &\textbf{.887}&\textbf{.877} &.887&.877 &.774&.776 &12.7\%&11.5\%\\\hline
			Mean Others&.670&.633 &.789&.738 &.753&.743 &.847&.841 &.671&.626 &-&- &-&- &-&-\\
			
			\bottomrule
	\end{tabular}}
\end{table*}

\begin{table*}[t!]
	\caption{Performance comparison when training with different combinations of multiple datasets. `NJ', `NL', `ST', `SI' and `DU' represent NJU2K, NLPR, STERE, SIP and DUT, respectively. The number in parentheses denotes the number of corresponding training and test images. The number of training images for each dataset is . 
The training and test sets will be available at: \supp{\href{ https://drive.google.com/drive/folders/1UYGyg50-0y7i21tWREVMcBpatQPFpSr0?usp=sharing} https://drive.google.com/drive/folders/1UYGyg50-0y7i21tWREVMcBpatQPFpSr0?usp=sharing.}}
	\vspace{-5pt}
	\label{tab:multi_dataset}
	\centering
\renewcommand{\arraystretch}{0.5}
	\setlength\aboverulesep{0.5pt}\setlength\belowrulesep{1pt}
\setlength{\tabcolsep}{0.18mm}{
		\footnotesize
		\begin{tabular}{r|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
			\toprule
			\multicolumn{1}{c|}{\multirow{2}{*}{{\diagbox{Train}{Test}}}} & \multicolumn{3}{c|}{NJ~(1285)} & \multicolumn{3}{c|}{NL~(300)} & \multicolumn{3}{c|}{ST~(300)}& \multicolumn{3}{c|}{DES~(135)}& \multicolumn{3}{c|}{LFSD~(80)}& \multicolumn{3}{c|}{SSD~(80)}&
			\multicolumn{3}{c|}{SI~(229)}&\multicolumn{3}{c}{DU~(500)} \\
			&& &&&  &&&  &&&  && & &&&  &&&  &&&  &\\\midrule
			NJ+NL~(1,400)&.911&.905&.039 &{.926}&{.916}&{.025} &.899&.898&.044 &.934&.932&.019 &.865&.862&.070 &.861&{.836}&.054 &.890&.893&.048&.799&.748&.095\\
			NJ+ST~(1,400)&.913&.909&{.038} &.885&.859&.040 &.916&.912&{.035} &.927&.910&.022 &.853&.836&.078 &{.869}&.848&{.054} &.885&.882&.052&.729&.719&.118\\
			NJ+DU~(1,400)&.906&.893&.043 &.852&.802&.050 &.875&.854&.053 &.884&.859&.037 &.861&.854&.070 &.862&.839&.053 &.834&.825&.075 &.905&.903&.041 \\
			NL+ST~(1,400)&.781&.748&.097 &.930&.919&.024 &.919&.920&.032 &{.942}&.938&.019 &.672&.645&.161 &.774&.722&.090 &.895&.894&.047 &.836&.819&.070\\
			NL+DU~(1,400)&.777&.771&.104 &.923&.908&.023 &.878&.882&.052 &.940&.936&.019 &.717&.728&.135 &.801&.774&.091 &.886&.888&.054 &.905&.903&.040\\
			ST+DU~(1,400)&.821&.794&.082 &.893&.863&.036 &.917&.914&.034 &.940&.935&.020 &.762&.734&.125 &.777&.736&.092 &{.907}&{.910}&{.039} &.913&.914&.037\\
			NJ+NL+ST~(2,100)&.913&.910&.038 &.923&.904&.027 &{.922}&{.924}&{.033} &.943&.939&.018 &.865&.858&.072 &.853&.818&.056 &.902&.905&.043 &.816&.780&.088\\
			NJ+NL+DU~(2,100)&.911&.905&.041 &.924&.909&.027 &.902&.901&.043 &.942&.939&.018 &{.865}&.856&{.067} &.866&.838&.051 &.894&.897&.048 &.916&.915&.036\\
			NJ+ST+DU~(2,100)&{.910}&.903&.041 &.890&.867&.039 &.923&.923&.031 &.932&.918&.021 &.859&.851&.073 &.863&.838&.055 &.896&.899&.046 &{.917}&{.916}&{.035}\\
			NL+ST+DU~(2,100)&.825&.808&.079 &.924&.911&{.026} &.919&.920&.033 &.946&.944&.017 &.751&.732&.125 &.797&.758&.082 &{.901}&.905&{.043} &.916&.911&.036\\
\hline
			NJ+NL+ST+DU~(2,800)&{.912}&{.905}&{.039} &.932&.917&.024 &.921&.920&{.033} &.946&{.942}&{.018} &{.864}&{.856}&{.070} &.858&.829&{.054} &.903&.905&{.042} &.917&.913&.037\\
			
			\bottomrule
	\end{tabular}}
\end{table*}

For a deep model to obtain reasonable performance in real-world scenarios, it not only requires an efficient design but must also be trained on a high-quality dataset with a great generalization power.
A good dataset usually contains sufficient images, with all types of variations that occur in reality, so that deep models trained on it can generalize well to the real world. 
In the area of RGB-D SOD, there are several large-scale datasets (\ie, NJU2K, NLPR, STERE, SIP, and DUT), with around  training images. \par


\myPara{\textbf{Single Dataset Generalization Analysis.}} Here, we conduct cross-dataset generalization experiments on the above-mentioned five datasets to measure their generalization ability.
To make fair comparisons among multiple datasets, we balance the datasets with equal number of training samples.
Specifically, we randomly choose  image pairs in each dataset for training, and the remaining images are used for testing.
We then retrain the proposed model on a single training set, and test it on all four test sets.
The results are summarized in \tabref{tab:single_dataset}.
`Self' represents the results of training and testing on the same dataset.
`Mean Others' indicates the average performance on all test sets except self.
`Drop' means the (percent) drop from `Self' to `Mean Others'.
First, it can be seen from the table that NJU2K and DUT are the hardest datasets since their `Mean Others' of column `NJU2K' and `DUT' are significantly lower than the other three datasets.
This may be because the two datasets include multiple challenging scenes (e.g., transparent objects, multiple objects, complex backgrounds, \etc).
Second, STERE has the best generalization ability, because the average drop of  and  is lowest among all five datasets.
Besides, SIP generalizes worst (\ie, the drop is the largest among all five datasets), since it mainly focuses on a single person or multiple persons in the wild.
We also notice that the score of the SIP column (`Mean Others') is the highest.
This is likely because the quality of the depth maps captured by the Huawei Mate10 is higher than that produced by traditional devices.
Finally, none of the models trained with a single dataset perform best over all test sets.
Thus, we further explore training on different combinations of datasets with the aim of building a dataset with a strong generalization ability for future research.\par

\myPara{\textbf{Dataset Combination for Generalization Improvement.}} 
According to the results in \tabref{tab:single_dataset}, the model trained on the SIP dataset does not generalize well to other datasets, so we discard it. 
We thus select four relatively large-scale datasets, \ie, NJU2K, NLPR, STERE, and DUT, to conduct our multi-dataset training experiments.
As shown in \tabref{tab:multi_dataset}, we consider all possible training combinations of these four datasets and test the models on all available test sets.
From the results in the table, we draw the following conclusions.
First, more training examples do not necessarily lead to better performance on some test sets.
For example, although `NJ+NL+ST', `NJ+NL+DU' and `NJ+NL+ST+DU' contain external training sets, unlike `NJ+NL', they perform similarly with `NJ+NL' on the test set of `NL'.
Second, including the NJU2K dataset is important for the model to generalize well to small datasets (\ie, LFSD, SSD).
The model trained using the combinations without NJU2K (\ie, `NL+ST' `NL+DU', `ST+DU' and `NL+ST+DU') all obtain low F-measure values (less than ) on the LFSD and SSD test sets.
In contrast, including `NJ' in the training sets increases the F-measures on the LFSD and SSD datasets by over .
Finally, including more examples in the training sets can improve the stability of the model, as it allows diverse scenarios to be taken into consideration.
Thus, the model trained on `NJ+NL+ST+DU', which has the most examples, obtains the best, or are very close to the best, performance.
Due to the limited size of current RGB-D datasets, it is hard for a model trained using a single dataset to perform well under various scenarios.
Thus, we recommend training a model using a combination of datasets with diverse examples to avoid model over-fitting issues.
To promote the development of RGB-D SOD, we hope more challenging RGB-D datasets with diverse examples and high-quality depth maps can be proposed in the future.


\section{Conclusion}
In this paper, we present a Bifurcated Backbone Strategy Network (\ourmodel)~for the RGB-D SOD.
To effectively suppress the intrinsic distractors in low-level cross-modal features, we propose to leverage the characteristics of multi-level cross-modal features in a cascaded refinement way: \emph{low-level features are refined by the initial saliency map that is produced by the high-level cross-modal features.}
Besides, we introduce a depth-enhanced module to excavate the informative cues from the depth features in the channel and spatial views, in order to improve the cross-modal compatibility when merging RGB and depth features.
Experiments on eight challenging datasets demonstrate that \ourmodel~outperforms  SOTA models, by a large margin, under multiple evaluation metrics.
Finally, we conduct a comprehensive analysis of the existing RGB-D datasets and introduce a powerful training set with a strong generalization ability for future research. 


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi







\bibliographystyle{IEEEtran}
\bibliography{BBS-Net}












\end{document}
