\section{Results and Observations}
We apply our method for ``hey Siri'' detection. The remainder if this section explains the dataset, training, and comparison of \prjname with existing methods.



\subsection{Datasets}
Our training data consist of 500k utterances each containing the wake-word along with a user query that follows the wake-word. These audio samples are augmented using room-impulse responses (RIRs) and echo residuals. We also use ambient noise samples from various acoustic environments to augment the training data. We also apply training time gain augmentation of 10dB to -40dB during training. The positive test data contain both near and far-field utterances (captured at 3ft and 6ft distance) with ``hey Siri'' in them. The negative test data contain around 2000 hours of dense speech. These datasets have been collected through internal user studies with informed consent approvals.
\subsection{Training and Evaluation}
Given an input segment, the model is trained for two tasks:
\begin{itemize}
    \item Classification: Probability that the segment contains the wake-word at the end.
    \item Regression: Distance of start of wake-word from the segment end.
\end{itemize}
For the classification task, we use focal loss~\cite{lin2017focal} especially because of the skewed distribution of positive and negative samples. For the regression task we use mean squared error loss. The overall loss function is defined in Equation \ref{eq:loss}

where  is the ground-truth label,  is the wake-word probability predicted by the network,  is the sigmoid function,  is a focal-loss hyper-parameter (4 in our case), BCE denotes cross-entropy, MSE denotes mean-squared error, and  is a binary filter that is  where the label is positive. We optimize the proposed loss in Eq.~(\ref{eq:loss}) using Adam optimizer \cite{kingma2014adam} with learning rate of 0.01 with a cosine-annealing scheduler for 100 epochs. We use data distributed training 16 GPUs (2nodes x 8 GPU configuration) with a per GPU batch size of 64 utterances. Using our data sampling technique we select 1 positive  segment and 20 negative segments from each utterance making the effective batch size per GPU as 1344. Our training being non sequential takes  less time than \cite{shrivastava2021optimize}. The input audio samples are augmented using gain and ambient noise augmentation on the fly in time domain. These samples are then transformed into 16 MFCC components for every 250 ms of audio at a 100 ms frame rate.

During evaluation, we consider a detected keyword as a true positive (TP) if it overlaps with the ground-truth window, otherwise we count it as a false accept (FA). All positive windows where the detector does not trigger are counted as false rejects (FRs). The detection threshold is varied to compute the detection error trade-off (DET) curve (FRR vs FA/hr). Figure~\ref{fig:detcurve} compares \prjname to End-to-end trained DNN-HMM \cite{shrivastava2021optimize}. As seen, our method outperforms the prior work by a large margin at different operating points. We also report the FRRs of all the proposed models at 12FA/hr operating point in Table \ref{tab:results} as compared to S1-DCNN \cite{higuchi2020stacked} and End-to-end trained DNN-HMM \cite{shrivastava2021optimize}. Compared to the best of the two prior works, our method reduces the FRR from  to , which yields about  relative FRR improvement.

\begin{table}[!t]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         Method & \#params & FRR\%  \\
         \hline
         S1DCNN \cite{higuchi2020stacked} & 13993 & 1.99\\
         End-to-end DNN-HMM \cite{shrivastava2021optimize} & 13979 & 1.7\\
         \prjname (\textbf{Ours}) & 13832 & 0.45\\
         \hline
    \end{tabular}
    \caption{Comparison of FRRs at 12 FA/hr on test data. Our model is 70\% better than the previous best model.}
    \label{tab:results}
\end{table}

\begin{figure} [!t]
\vspace{-0.25in}
    \centering
    \includegraphics[width=9.3cm]{det_curve_ndapi.png}
    \caption{Comparison of DET-curves obtained by End to End trained DNN-HMM model and \prjname.}
    \label{fig:detcurve}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=9.3cm]{localization.png}
    \caption{Comparing IOU vs TPR of End to End trained DNN-HMM model and \prjname.}
    \label{fig:localization}
\end{figure}


We also introduce a localization metric by plotting Intersection Over Union (IOU) vs True Positive Rate (TPR) for a baseline End to End trained DNN-HMM model and our \prjname model. The trigger end is identified by the point where the score from our models exceed the threshold at 12 FA/hr operating point for them. \prjname uses the offset predicted at that point to find the start of the the wake-word whereas the DNN-HMM model uses the trace of the HMM to find the start. Figure \ref{fig:localization} demonstrates the localization accuracy of both the methods where the Area Under Curve (AUC) for End to End DNN-HMM and \prjname is around 0.8.
