\documentclass{article}







\usepackage[square,numbers]{natbib}

\usepackage[preprint]{neurips_2020}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\usepackage{wrapfig}

\newcommand\BigBox{\vcenter{\hbox{\scalebox{2}{}}}}
\newcommand\bigsquare{\mathop{\BigBox}\limits}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage[group-separator={,}]{siunitx}
\renewcommand{\vec}[1]{\mathbf{#1}}
\usepackage{dutchcal}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\title{SIGN: Scalable Inception Graph Neural Networks}



\author{Fabrizio Frasca\thanks{Equal contribution} \\
   Twitter / Imperial College London \\
   United Kingdom\\
\And
  Emanuele Rossi\samethanks \\
   Twitter / Imperial College London \\
   United Kingdom\\   
\And
   Davide Eynard \\
   Twitter \\
   United Kingdom\\   
\AND
   Ben Chamberlain \\
   Twitter \\
   United Kingdom\\   
\And
   Michael M. Bronstein \\
   Twitter / Imperial College London\\
   United Kingdom\\   
\And
   Federico Monti \\
   Twitter \\
   United Kingdom\\   
}



\begin{document}
\hspace{-5cm}

\maketitle

\begin{abstract}
Graph representation learning has recently been applied to a broad spectrum of problems ranging from computer graphics and chemistry to high energy physics and social media. 
The popularity of graph neural networks has sparked interest, both in academia and in industry, in developing methods that scale to very large graphs such as Facebook or Twitter social networks. In most of these approaches, the computational cost is alleviated by a sampling strategy retaining a subset of node neighbors or subgraphs at training time.
In this paper we propose a new, efficient and scalable graph deep learning architecture which sidesteps the need for graph sampling by using graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and inference. 
Our architecture allows using different local graph operators (e.g. motif-induced adjacency matrices or  Personalized Page Rank diffusion matrix) to best suit the task at hand.
We conduct extensive experimental evaluation on various open benchmarks and show that our approach is competitive with other state-of-the-art architectures, while requiring a fraction of the training and inference time. Moreover, we obtain state-of-the-art results on \texttt{ogbn-papers100M}, the largest public graph dataset, with over 110 million nodes and 1.5 billion edges.
\end{abstract}







\maketitle

\section{Introduction}



Deep learning on graphs, also known as {\em geometric deep learning} (GDL) \cite{7974879} or {\em graph representation learning} (GRL) \cite{hamilton2017representation,battaglia2018relational,zhang2018deep}, has emerged in a matter of just a few years from a niche topic to one of the most prominent fields in machine learning. 
Graph deep learning models have recently scored successes in various applications relying on modeling relational data, see e.g. \cite{zhang2018link,qi2018learning,Monti2016GeometricDL,choma2018graph,NIPS2015_5954,pmlr-v70-gilmer17a,parisot2018disease,zitnik2018modeling,veselkov2019hyperfoods,gainza2019deciphering,rossi2019ncrna,monti2019fake}. Graph Neural Networks (GNNs) seek to generalize classical convolutional architectures (CNNs) to graph-structured data, 
with a wide variety of convolution-like operations available in the literature \cite{scarselli2008graph,defferrard2016convolutional,DBLP:conf/nips/AtwoodT16,DBLP:conf/icml/NiepertAK16,DBLP:conf/cvpr/SimonovskyK17,Monti2016GeometricDL,kipf2016semi,pmlr-v97-wu19e,DBLP:conf/iclr/VelickovicCCRLB18,GraphSAGE}. 



Until recently, most of the research in the field has focused on small-scale datasets,  and relatively little effort has  been devoted to scaling these methods to web-scale graphs such as the Facebook or Twitter social networks.
Scaling is a major challenge precluding the wide application of graph deep learning methods in industrial settings. Compared to Euclidean neural networks where the training loss can be decomposed into individual samples and computed independently, graph convolutional networks diffuse information between nodes along the edges of the graph, making the loss computation interdependent for different nodes. 
Furthermore, in typical graphs the number of nodes grows exponentially with the increase of the filter receptive field, incurring significant computational and memory complexity. 
So far, various 
{\em graph sampling} approaches  \cite{GraphSAGE, pinsage, fastgcn, adaptive-sampling, stochastic-training,Chiang:2019:CEA:3292500.3330925,DBLP:journals/corr/abs-1907-04931,zou2019layer} have been proposed as a way to alleviate the cost of training graph neural networks by selecting a small number of neighbors that reduce the computational and memory complexity. 



In this paper, we take a different approach for scalable deep learning on graphs. We propose \textit{SIGN}, a simple scalable Graph Neural Network architecture inspired by the inception module  \cite{szegedy2015going, anees2019inception}. \textit{SIGN} combines graph convolutional filters of different types and sizes that are amenable to efficient precomputation, allowing extremely fast training and inference with complexity independent of the graph structure. Our architecture is able to scale to web-scale graphs without resorting to any sample technique and retaining sufficient expressiveness for effective learning: while being faster in training and, especially, inference (even one order of magnitude speedup), by employing \textit{SIGN} with only one graph convolutional layer we are able to achieve results on par with state-of-the-art on several large-scale graph learning datasets. In particular, \textit{SIGN} obtains state-of-the-art results on \texttt{ogbn-papers100M}, the largest public graph learning benchmark, with over 110 million nodes and 1.5 billion edges.


These results raise the important question on when deep graph neural network architectures are useful, especially when scalability is an important requirement, as in large-scale industrial systems. 
Significant effort has recently been devoted to methods allowing to design deep Graph Neural Networks with many graph convolutional layers \cite{jk,gong2020geometrically,li2019deepgcns,zhao2019pairnorm,rong2019dropedge}, which otherwise appear difficult to train \cite{li2018adaptive,klicpera2018predict,wu2020comprehensive}. 
While there is strong evidence in favor of depth on geometric graphs \cite{he2016deep,li2019deepgcns,gong2020geometrically}, there has been almost no gain from depth on general irregular graphs like `small-world' networks \cite{shchur2018pitfalls,rong2019dropedge}. 
Given the abundance of such graphs e.g. in social network applications, it is important to take a step back and deliberate if deep architectures are the right approach.
We conjecture that deep graph learning architectures are not useful for general irregular graphs and argue that future research in the field should focus on designing local more expressive operators \cite{barbarossa2019topological,monti2018motifnet,flam2020neural} rather than going deeper. 





\section{Background and Related Work} \label{sec:background}

\subsection{Deep learning on graphs}

The goal of graph representation learning is to construct an embedding representing the structure of the graph and the data thereon.
In node-wise prediction problems, we distinguish between {\bf Transductive} setting, which assumes that the entire graph is known, and thus the same graph is used during training and testing (albeit different nodes are used for training and testing), and {\bf Inductive} setting, in which training and testing are performed on different graphs.
A typical graph neural network architecture consists of graph {\bf Convolution-like operators} (discussed in Section~\ref{sec:convo}) performing local aggregation of features by means of message passing with the neighbor nodes, and possibly {\bf Pooling} amounting to fixed \cite{dhillon2007weighted} or learnable \cite{ying2018hierarchical,bianchi2019mincut} graph coarsening. 
Additionally, graph {\bf Sampling} schemes (detailed in Section~\ref{sec:sampling}) can be employed on large-scale graphs to reduce the computational complexity. 

\subsection{Basic notions}

Let  be an undirected weighted graph, represented by the symmetric  {\em adjacency matrix} , where  if  and zero otherwise. 
The diagonal {\em degree matrix}  represents the number of neighbors of each node. 
We further assume that each node is endowed with a -dimensional feature vector and arrange all the node features as rows of the -dimensional matrix . 
We denote by  the normalized adjacency matrix. The normalized {\em graph Laplacian} is an  positive semi-definite matrix . 

\subsection{Convolution-like operators on graphs} \label{sec:convo}

\paragraph{Spectral methods}
Bruna et al. \cite{bruna2013spectral} used the analogy between the eigenvectors of the graph Laplacian and the Fourier transform to generalize convolutional neural networks (CNN) \citep{lecun1989backpropagation} to graphs. 
Among the key drawbacks of this approach is a high (at least  complexity), 
large () number of filter parameters,  
no spatial localization, and no generalization of filters across graphs. Furthermore, the method explicitly assumes the underlying graph to be undirected, in order for the Laplacian to be a symmetric matrix with orthogonal eigenvectors. 

\paragraph{ChebNet}
A way to address the shortcomings of \cite{bruna2013spectral} is to model the filter as a transfer function , applied to the Laplacian as . Filters computed in this manner are stable under graph perturbations \cite{levie2019transferability}. 
In the case when  is expressed as simple matrix-vector operations (e.g. a polynomial \cite{defferrard2016convolutional} or rational function \cite{levie2018cayleynets}), the eigendecomposition of the Laplacian  can be avoided altogether. 
A particularly simple choice is a polynomial spectral filter  of degree , allowing the convolution to be computed entirely in the spatial domain as 
\noindent with   parameters , does not require explicit multiplication by , and has a compact support of  hops in the node domain. Though originating from a spectral construction, the resulting filter is an operation in the node domain amounting to a successive aggregation of features in the neighbor nodes,  
which can be performed with complexity  . The polynomial filters can be combined with non-linearities, concatenated in multiple layers, and interleaved with pooling layers based on graph coarsening \cite{defferrard2016convolutional}. 
The Laplacian in~(\ref{eqn:cheby}) can be replaced with other operators that diffuse information across neighbor nodes, e.g. the simple or normalized adjacency matrix, without affecting performance. 

\paragraph{GCN}
In the case , equation~(\ref{eqn:cheby}) reduces to computing , which can be interpreted as a combination of the node features and the neighbors filtered features. 
Kipf and Welling \cite{kipf2016semi} proposed a model of graph convolutional networks (\textit{GCN}) combining node-wise and graph diffusion operations: 

Here  is the adjacency matrix with self-loops,  is the respective degree matrix, and  is a matrix of learnable parameters. 

\paragraph{S-GCN}
Stacking  \textit{GCN} layers with element-wise non-linearity  and a final layer for node classification with activation  (e.g. softmax or sigmoid), it is possible to obtain filters with larger receptive fields on the graph nodes, 

\noindent Wu et al. \cite{pmlr-v97-wu19e} argued that graph convolutions with large filters is practically equivalent to multiple convolutional layers with small filters and showed that all but the last non-linearities can be removed without practically harming the performance, resulting in the \emph{simplified GCN} (\textit{S-GCN}) model, 


\paragraph{MotifNet}
Monti et al. \cite{monti2018motifnet} used adjacency matrices with weights proportional to the count of simple subgraphs (motifs) on edges in order to account for higher order structures. Related ideas have been explored using higher-order Laplacians on simplicial complexes \cite{barbarossa2019topological}.

\subsection{Graph sampling} \label{sec:sampling}

For Web-scale graphs such as Facebook or Twitter that typically have  nodes and  edges, the diffusion matrix cannot be stored in memory for training, making the straightforward application of graph neural networks impossible. 
Graph sampling has been shown to be a successful technique to scale GNNs to large graphs by approximating local connectivity with subsampled versions which are amenable for computation. 

\paragraph{Node-wise sampling} These strategies perform graph convolutions on \emph{partial} node neighborhoods to reduce computational and memory complexity, and are coupled with minibatch training, where each training step is performed only on a batch of nodes rather than on  the whole graph. A training batch is assembled by first choosing  `optimization' nodes and partially expanding their corresponding neighborhoods. In a single training step, the loss is computed and optimized only for optimization nodes. 
Node-wise sampling coupled with minibatch training was first introduced in \textit{GraphSAGE} \cite{GraphSAGE} to address the challenges of scaling GNNs. PinSAGE \cite{pinsage} extended \textit{GraphSAGE} by exploiting a neighbor selection method using scores from approximations of Personalized PageRank \cite{haveliwala2003topic} via random walks. \textit{VR-GCN} \cite{stochastic-training} uses control variates to reduce the variance of  stochastic training and increase the speed of convergence with a small number of neighbors.

\paragraph{Layer-wise sampling} A characteristic of many graphs, in particular `small-world'  social networks, is the exponential growth of the neighborhood size with number of hops . \cite{fastgcn, adaptive-sampling} avoid over-expansion of neighborhoods to overcome the redundancy of node-wise sampling. Nodes in each layer only have directed edges towards nodes of the next layer, thus bounding the maximum amount of computation to  per layer. Moreover, sharing common neighbors prevents feature replication across the batch, drastically reducing the memory complexity during training.

\paragraph{Graph-wise sampling} In \cite{Chiang:2019:CEA:3292500.3330925, DBLP:journals/corr/abs-1907-04931}, feature sharing is further advanced: each batch consists of a connected subgraph and at each training iteration the GNN model is optimized over all nodes in the subgraph.
In \textit{ClusterGCN} \cite{Chiang:2019:CEA:3292500.3330925}, non-overlapping clusters are computed as a pre-processing step and then sampled during training as input minibatches. \textit{GraphSAINT} \cite{DBLP:journals/corr/abs-1907-04931} adopts a similar approach, while also correcting for the bias and variance of the minibatch estimators when sampling subgraphs for training. It also explores different schemes to sample the subgraphs such as a random walk-based sampler, which is able to co-sample nodes having high influence on each other and guarantees each edge has a non-negligible probability of being sampled.

\begin{table}
\caption{Theoretical time complexity where  is the number of graph convolition and MLP layers,  is the filter size,  the number of nodes (in training or inference,  respectively),  the number of edges, and  the feature dimensionality (assumed fixed for all layers). For \textit{GraphSAGE},  is the number of sampled neighbors per node. Forward pass complexity corresponds to an entire epoch where all nodes are seen.}
    \label{tab:complexity}
    \centering
    \begin{tabular}{| l | cc |}
    \hline
     & \textit{Preproc.} & \textit{Forward Pass} \\ \hline
    \textit{GraphSAGE}  &  &   \\ 
    \textit{ClusterGCN}  &  &  \\ 
    \textit{GraphSAINT} &  &  \\ 
    \hline
    \textit{SIGN-} &  &  \\  
    \hline
    \end{tabular} 
\end{table}


\section{Scalable Inception Graph Neural Networks}\label{s:method}

In this work we propose \textit{SIGN}, an alternative method to scale Graph Neural Networks to very large graphs.
The key building block of our architecture is a set of linear diffusion operators represented as  matrices , whose application to the node-wise features can be pre-computed.  
For node-wise classification tasks, our architecture has the form (Figure~\ref{fig:sign-architecture}):

\noindent where  and   are learnable matrices respectively of dimensions  and  for  classes, and ,  are non-linearities, the second one computing class probabilities, e.g. via softmax or sigmoid function, depending on the task at hand. We denote a model with  operators by {\em SIGN-}. 

\begin{figure}\centering
    \includegraphics[width=0.5\linewidth]{images/sign_diagram_neur.png}
    \caption{The \textit{SIGN} architecture for  generic graph filtering operators.  represents the -th dense layer transforming node-wise features downstream the application of operator ,  is the concatenation operation and  refers to the dense layer used to compute final predictions.}
    \label{fig:sign-architecture}
\end{figure}

A key observation is that matrix products  in equation~(\ref{eqn:sign}) {\em do not depend} on the learnable model parameters and can be easily precomputed. For large graphs, distributed computing infrastructures such as Apache Spark can speed up computation. This effectively reduces the computational complexity of the overall model to that of a Multi-Layer Perceptron (MLP), i.e. , where  is the number of features,  the number of nodes in the training/testing graph and  is the overall number of feed-forward layers in the model. 

Table~\ref{tab:complexity} compares the complexity of our \textit{SIGN} model to the other scalable architectures \textit{GraphSAGE}, \textit{ClusterGCN}, and \textit{GraphSAINT}. While all models scale linearly w.r.t. the number of nodes , we show experimentally that our model is significantly faster than all others due to the fact the forward and backward pass complexity of our model does not depend on the graph structure.
Unlike the aforementioned scalable methods, \textit{SIGN} is not based on sampling nodes or subgraphs, operations potentially introducing bias into the optimization procedure. 


\paragraph{Inception-like module}
Within the \textit{SIGN} framework, it is possible to choose one specific operator  and to define  for . In this setting the proposed model is analogous to the popular {\em Inception module} \cite{szegedy2015going} for classic CNN architectures: it consists of convolutional filters of different sizes determined by the parameter , where  corresponds to  convolutions in the inception module (amounting to linear transformations of the features in each node without diffusion across nodes).
Owing to this analogy, we refer to our model as the \emph{Scalable Inception Graph Network} (\textit{SIGN}).
It is also easy to observe  that various graph convolutional layers can be obtained as particular settings of~(\ref{eqn:sign}). In particular, by setting the  non-linearity to PReLU \cite{prelu}, ChebNet, GCN, and \textit{S-GCN} can be automatically learnt if suitable diffusion operator  and activation  are used (see Table \ref{tab:reduction}).
 
\paragraph{Choice of the operators}
Generally speaking, the choice of the diffusion operators jointly depends on the task, graph structure, and the features. 
In complex networks such as social graphs, operators induced by triangles or cliques might help distinguishing edges representing weak or strong ties \cite{Granovetter82thestrength}. In graphs with noisy connectivity, it was shown that diffusion operators based on Personalized PageRank (PPR) or Heat Kernel can boost performance \citep{klicpera_diffusion_2019}. In our experiments, we choose three specific types of operators: simple (normalized) adjacency, Personalized PageRank-based adjacency, and triangle-based adjacency matrices, as well as their powers. 
We denote by {\em SIGN(,,)} with  the configuration using up to the -th, -th, and -th power of simple GCN-normalized, PPR-based, and triangle-based adjacency matrices, respectively.
Lastly, when working on directed graphs, \textit{SIGN} can be equipped with powers of the (properly normalized) directed adjacency matrix  and its transpose , in addition to the standard operators built on top of its undirected counterpart .

\begin{table}[t!]
    \caption{By appropriate configuration, \textit{SIGN} inception layer is able to replicate some popular graph convolutional layers.  represents the learnable parameter of a PReLU activation.}
    \label{tab:reduction}
    \centering
    \begin{tabular}{| l | c c c c |}
        \hline
            & 
            & 
            & 
            &  \\
        \hline
        \textit{ChebNet} \cite{defferrard2016convolutional}
            & 
            & 
            & 
            &  \\
        
        \textit{GCN} \cite{kipf2016semi}
            & 
            & 
            & 
            & \\
        
        \textit{S-GCN} \cite{pmlr-v97-wu19e}
            & 
            & 
            & 
            & \\
        
        \hline
    \end{tabular}\end{table}

\paragraph{SIGN and S-GCN}
\textit{SIGN} and \textit{S-GCN} are the only graph neural models which are inherently `shallow': contrary to standard GNN architectures, graph convolutional layers are not sequentially stacked, but either collapsed into a single linear filtering operation (S-GCN) or applied in parallel to obtain multi-scale node representations capturing diverse connectivity patterns depending on the chosen operators (\textit{SIGN}). This is the crucial feature that allows these models to naturally scale their training and inference to graphs of any size and family given that all graph operations can be conveniently pre-computed. While we notice that \textit{S-GCN} can be considered as a specific configuration of \textit{SIGN}- (it is sufficient to choose  and to constrain  to , see Table~\ref{tab:reduction}), we remark the fact that the more general \textit{SIGN} architecture easily allows to incorporate more expressivity via parallel application of several, possibly, domain-specific, operators. We experimentally show this in the following section where we demonstrate that the \textit{S-GCN} paradigm is indeed too limiting and that a more expressive \textit{SIGN} model is not only able to perform on par with `deeper' sampling-based models, but also to achieve state-of-the-art results on the largest publicly available graph learning benchmark.


\section{Experiments} \label{sec:experiments}

\paragraph{Datasets}
We evaluated the proposed method on node-wise classification tasks, both in transductive and inductive settings. 
Inductive experiments are performed using four datasets: \texttt{Reddit} \cite{GraphSAGE}, \texttt{Flickr}, \texttt{Yelp} \cite{DBLP:journals/corr/abs-1907-04931}, and \texttt{PPI} \cite{Zitnik_2017}. To date, these are the largest graph learning inductive node classification benchmarks available in the public domain. Related tasks are multiclass node-wise classification for \texttt{Reddit} and \texttt{Flickr} and multilabel classification for \texttt{Yelp} and \texttt{PPI}. Transductive experiments were performed on the new \texttt{ogbn-products} and \texttt{ogbn-papers100M} datasets \cite{ogb2020}. The former represents an Amazon product co-purchasing network \cite{Bhatia16} where the task is to predict the category of a product in a multi-class classification setup. The latter represents a directed citation network of  million academic papers, where the task is to leverage information from the entire citation network to infer the labels (subject areas) of a smaller subset of ArXiv papers. Overall, this dataset is orders-of-magnitude larger than any existing node classification dataset and is therefore the most important testbed for the scalability of \textit{SIGN} and related methods.

Furthermore, we also test the scalability of our method on \texttt{Wikipedia} links \cite{Konect2017wikipedia}, a large-scale network of links between articles in the English version of Wikipedia. 

Statistics for all the datasets are reported in Table~\ref{tab:dataset-statistics-ind}.

\paragraph{Setup}
We tested several {\em SIGN} configurations, with  the maximum power of the GCN-normalized adjacency matrix,  that of a random-walk normalized PPR diffusion operator \cite{klicpera_diffusion_2019}, and  that of a row-normalized triangle-induced adjacency matrix \cite{monti2018motifnet}, with weights proportional to edge occurrences in closed triads. PPR-based operators are computed from a symmetrically normalized adjacency transition matrix in an approximated form, with a restart probability of  for inductive datasets and  in the transductive case.
To allow for larger model capacity in the inception modules and in computing final model predictions, we replace the single-layer projections performed by  and  modules with multiple feedforward layers. Model parameters are found by minimizing the cross-entropy loss via minibatch gradient descent with the Adam optimizer \cite{adam}. Early stopping is applied with a patience of . In order to limit overfitting, we apply the standard regularization techniques of weight decay and dropout~\cite{10.5555/2627435.2670313}. Additionally, batch-normalization~\cite{pmlr-v37-ioffe15} was used in every layer to stabilize training and increase convergence speed. 
Architectural and optimization hyperparameters were estimated using Bayesian optimization with a tree Parzen estimator surrogate function~\cite{NIPS2011_4443} over all inductive datasets. As for the the transductive setting, we employ standard exhaustive search on a predefined hyperparameter grid on \texttt{ogbn-products}, while on \texttt{ogbn-papers100M} we only test a basic configuration that can be found in Supplemetary Materials, along with further details on the hyperparameter search spaces. Given that this dataset represents a directed network, we experimented with operators built via asymmetric normalization of the original directed adjacency matrix and its transpose, as well as their powers.
The \texttt{Wikipedia} dataset, due to the lack of node attributes and labels, is only used to assess scalability: to this end, we randomly generate -dimensional node feature vectors and scalar targets and consider the whole network for both training and inference. No hyperparameter tuning is required in this case. 


\begin{table*}[t]
    \centering
    \caption{Summary of (s)ingle and (m)ulti-label dataset statistics. \texttt{Wikipedia} is used, with random features, for timing purposes only.}\label{tab:dataset-statistics-ind}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{| l | cccccc |}
    \hline
     &  &  & \hspace{-2mm}\textit{Avg. Deg.}\hspace{-2mm} &  & Classes & Train / Val / Test \\
    \hline
    \texttt{Wikipedia}
        & 12,150,976
        & 378,142,420
        & 62
        & 100
        & 2(s)
        & 100\% / --- / 100\% \\
    \texttt{ogbn-papers100M}
        & 111,059,956
        & 1,615,685,872
        & 30
        & 128
        & 172(s)
        & 78\% / 8\% / 14\% \\
    \texttt{ogbn-products}
        & 2,449,029
        & 61,859,140
        & 51
        & 100
        & 47(s)
        & 10\% / 2\% / 88\% \\
    \texttt{Reddit}
        & 232,965
        & 11,606,919
        & 50
        & 602
        & 41(s)
        & 66\% / 10\% / 24\% \\  
    \texttt{Yelp}
        & 716,847
        & 6,977,410
        & 10
        & 300
        & 100(m)
        & 75\% / 10\% / 15\% \\ 
    \texttt{Flickr}
        & 89,250
        & 899,756
        & 10
        & 500
        & 7(s)
        & 50\% / 25\% / 25\% \\  
    \texttt{PPI}
        & 14,755
        & 225,270
        & 15
        & 50
        & 121(m)
        & 66\% / 12\% / 22\% \\
    \hline
    \end{tabular}}
\end{table*}

\begin{table*}[t]
\caption{Mean and standard deviation of preprocessing, training (one epoch) and inference times, in seconds, on \texttt{ogbn-products} and \texttt{Wikipedia} datasets, computed over 10 runs. {\em SIGN-} denotes architecture with  precomputed operators. Preprocessing and training times for \textit{ClusterGCN} on \texttt{Wikipedia} are not reported due to the clustering algorithm failing to complete.}
    \label{tab:time_stats}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{| l | ccc | ccc |}
     \hline
     & \multicolumn{3}{c|}{\texttt{ogbn-products}} &
     \multicolumn{3}{c|}{\texttt{Wikipedia}}  \\
    \hline
     & \textit{Preprocessing} & \textit{Training} & \textit{Inference} & \textit{Preprocessing} & \textit{Training} & \textit{Inference} \\ 
    \hline
    \textit{ClusterGCN} &  36.93   0.52  
                        &   13.34  0.16
                        &   93.00  0.68
                        & ---
                        & ---
                        &  183.76  3.01\\
    \textit{GraphSAINT} &   52.06  0.54 
                        &    2.89  0.05
                        &   94.76  0.81  
                        &  123.60  1.60
                        &  135.73  0.06
                        &  209.86  4.73\\  
    \textit{SIGN-2}     &   88.21  1.33
                        &    1.04  0.10
                        &    2.86  0.10 
                        &  192.88  0.12
                        &   62.37  0.17
                        &   13.40  0.15\\  
    \textit{SIGN-4}     &  160.16  1.20
                        &    1.54  0.04
                        &    3.79  0.08
                        &  326.21  1.14
                        &   93.84  0.08
                        &   18.15  0.05\\  
    \textit{SIGN-6}     &  226.48  1.43 
                        &    2.05  0.00
                        &    4.84  0.08
                        &  459.24  0.14
                        &  125.24  0.03
                        &   22.94  0.02\\  
    \textit{SIGN-8}     &  297.92  2.92
                        &    2.53  0.04
                        &    5.88  0.09
                        &  598.67  0.82
                        &  154.73  0.12
                        &   27.69  0.11\\  
    \hline
    \end{tabular}}
\end{table*}

\paragraph{Baselines}
On the inductive datasets, we compare our method with \textit{GCN}~\cite{kipf2016semi}, \textit{FastGCN}~\cite{fastgcn}, \textit{Stochastic-GCN}~\cite{stochastic-training}, \textit{AS-GCN}~\cite{adaptive-sampling}, \textit{GraphSAGE}~\cite{GraphSAGE}, \textit{ClusterGCN}~\cite{Chiang:2019:CEA:3292500.3330925}, and \textit{GraphSAINT}~\cite{DBLP:journals/corr/abs-1907-04931}, which constitute the current state-of-the-art. On \texttt{ogbn-products} we compare against scalable sampling-free baselines, i.e. a feed-forward network trained over node features only (\textit{MLP}) and on their concatenation with structural \textit{Node2Vec} embeddings~\cite{grover2016node2vec}, and the sampling-based approaches  \textit{ClusterGCN}~\cite{Chiang:2019:CEA:3292500.3330925} and \textit{GraphSAINT}~\cite{DBLP:journals/corr/abs-1907-04931}. 
As for \texttt{ogbn-papers100M}, \textit{SIGN} is compared with sampling-free baselines: an \textit{MLP} trained on node features and an \textit{S-GCN} model. Sampling-based methods have not been scaled yet to this benchmark. All results for OGB datasets are directly taken from the latest version of the arXiv paper~\cite{ogb2020} (\emph{v4}, at the time of writing). Lastly, being \textit{S-GCN} an important baseline for our model, we additionally report its performance on all the other datasets as well. In this case we choose power  of its (only) operator  as the value  of the best corresponding \textit{SIGN(,,)} configuration and we tune its hyperparameters in the same space searched for \textit{SIGN}.

\paragraph{Implementation}
\textit{SIGN} is implemented using Pytorch~\cite{NEURIPS2019_9015}.
All experiments, including timings, were run on an AWS p2.8xlarge instance, with 8 NVIDIA K80 GPUs, 32 vCPUs, a processor Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz and 488GiB of RAM. 

\subsection{Results}

\begin{table*}[t]
\caption{Micro-averaged F1 scores. 
For \textit{SIGN}, we show the best performing configurations. 
    The top three performance scores are highlighted as: {\bf \bf \color{red} First}, {\bf \bf \color{violet} Second}, {\bf Third}. 
    }
    \label{tab:results_inductive}
    \centering
        \begin{tabular}{| l | cccc |}
        \hline
         & \texttt{Reddit} & \texttt{Flickr} & \texttt{PPI} & \texttt{Yelp} \\ \hline
        \textit{GCN}~\cite{kipf2016semi} & 0.9330.000 & 0.4920.003 & 0.5150.006 & 0.3780.001\\
        \textit{FastGCN}~\cite{fastgcn} & 0.9240.001 &  {\bf 0.5040.001} & 0.5130.032 & 0.2650.053\\
        \textit{Stochastic-GCN}~\cite{stochastic-training} & {\bf 0.9640.001} &  0.4820.003 & {\bf 0.9630.010} & {\bf \color{violet} 0.6400.002}\\
        \textit{AS-GCN}~\cite{adaptive-sampling} & 0.9580.001 &  {\bf 0.5040.002} & 0.6870.012 & ---\\
        \textit{GraphSAGE}~\cite{GraphSAGE} & 0.9530.001 &  0.5010.013 & 0.6370.006 & \textbf{0.6340.006}\\ 
        \textit{ClusterGCN}~\cite{Chiang:2019:CEA:3292500.3330925} & 0.9540.001 &  0.4810.005 & 0.8750.004 & 0.6090.005 \\ 
        \textit{GraphSAINT}~\cite{DBLP:journals/corr/abs-1907-04931} & {\bf \color{violet} 0.9660.001} & {\bf \color{violet} 0.5110.001} & {\bf \color{red} 0.9810.004} & {\bf \color{red} 0.6530.003} \\
\textit{S-GCN}~\cite{pmlr-v97-wu19e} & 0.9490.000 & 0.5020.001 & 0.8920.015 & 0.3580.006\\\hline
        \textit{SIGN} & {\bf \color{red} 0.9680.000} & {\bf \color{red} 0.5140.001} & {\bf \color{violet} 0.9700.003} & 0.6310.003\\ &  &  &  &  \\


        \hline
    \end{tabular} 
\end{table*}


\paragraph{Inductive}
Table \ref{tab:results_inductive} presents the results on the inductive dataset. In line with \cite{DBLP:journals/corr/abs-1907-04931}, we report the micro-averaged F1 score means and standard deviations computed over 10 runs. For each dataset we report the best performing \textit{SIGN} configuration, specifying the maximum power for each of the three employed operators.
\textit{SIGN} outperforms other methods on \texttt{Reddit} and \texttt{Flickr}, and performs competitively to state-of-the-art on \texttt{PPI}. Our performance on \texttt{Yelp} is worse than in the other datasets; we hypothesize that a more tailored choice of operators is required to better suit the characteristics of this dataset.
Interestingly, \textit{SIGN} significantly outperforms \textit{S-GCN} in all datasets, suggesting that the additional expressivity introduced by the different operators in our model is required for effective learning.

\paragraph{Transductive}
\textit{SIGN} obtains state-of-the-art results on the \newline \texttt{ogbn-papers100M} dataset (Table \ref{tab:ogbn-100m}), outperforming other sampling-free methods by at least . This shows that \textit{SIGN} can scale to massive graphs while retaining ample expressivity. Sampling based methods have not been scaled yet to this benchmark. In \texttt{ogbn-papers100M} only  of nodes are labeled; at \emph{each} training and inference iteration these methods still need to perform computation on subgraphs where the majority of nodes are unlabeled and thus do not contribute to the computation of loss and evaluation metrics. On the contrary, at training and inference \textit{SIGN} only processes the required labeled nodes given that the graph has already been employed during the one-time pre-computation phase, thus avoiding this redundant computation and memory usage.

\texttt{ogbn-products} results are reported in Table \ref{tab:results_ogb}.
\textit{SIGN} outperforms all other sampling-free methods by at least . However, contrary to the inductive benchmarks, sampling methods outperform \textit{SIGN} and appear to generally be more suitable to this dataset. We hypothesise that, on this particular task, sampling may implicitly act as a regularizer, making these methods generalize better to the held-out test set, which in this dataset is sampled from a different distribution w.r.t. training and validation nodes \cite{ogb2020}. This phenomenon, as well as its connection to the DropEdge method~\cite{rong2019dropedge} and the bottleneck problem~\cite{alon2020bottleneck}, would be object of further investigation. 

\paragraph{Runtime}
While performing on par or better than state-of-the-art methods on most benchmarks in terms of accuracy, our method has the advantage of being significantly faster than other methods for large graphs. 
We perform comprehensive timing comparisons on \texttt{ogbn-products} and \texttt{Wikipedia} datasets and report average training, inference, and preprocessing times in Table~\ref{tab:time_stats}. For these experiments, we run the implementations of \textit{ClusterGCN} and \textit{GraphSAINT} provided in the OGB code repository\footnote{https://github.com/snap-stanford/ogb/tree/master/examples/nodeproppred/products}. 

We use these datasets rather than \texttt{ogbn-papers100M} so we can compare to \textit{ClusterGCN} and \textit{GraphSAINT}. For completeness we report, however, that on \texttt{ogbn-papers100M} the best performing \textit{SIGN(3,3,3)} model completes one evaluation pass on the validation set in  seconds and on the test set in  seconds (statistics are estimated over  runs and include the time required by device data transfers and by the computation of evalution metric).

Our model is faster than \textit{ClusterGCN} and of comparable speed w.r.t. \textit{GraphSAINT} in training\footnote{Traning time is measured as forward-backward time to complete one epoch.}, while being by far the fastest approach in inference: all \textit{SIGN} architectures are always at least one order of magnitude faster than other methods, with the largest one ( operators) requiring no more than  seconds to perform inference on over M nodes.
\textit{SIGN}'s preprocessing is slightly longer than other methods, but we notice that most of the calculations can be cast as sparse matrix multiplications and easily parallelized with frameworks for distributed computing. We envision to engineer faster and even more scalable \textit{SIGN} preprocessing implementations in future developments of this work.
Finally, in order to also study the convergence behavior of our proposed model, in Figure~\ref{fig:ogb_timings} we plot the validation performance on \texttt{ogbn-products} from the start of the training as a function of run time for \textit{ClusterGCN}, \textit{GraphSAINT} and several \textit{SIGN} configurations. We observe that \textit{SIGN} does not only converge to a better validation accuracy than other methods, but also exhibits much faster convergence than \textit{ClusterGCN} and comparable speed than to \textit{GraphSAINT}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/convergence_ogbn_products-crop}
    \caption{Convergence of different methods on \texttt{ogbn-products}.}
    \label{fig:ogb_timings}
\end{figure}

\begin{table}[t]
    \centering
    \caption{Performance on \texttt{ogbn-products}. {\em SIGN(,,)} refers to a configuration using , , and  powers of simple, PPR-based, and triangle-based adjacency matrices. The top three performance scores are highlighted as: {\bf \color{red} First}, {\bf \color{violet} Second}, {\bf Third}.}
    \label{tab:results_ogb}
    \begin{tabular}{| l | ccc |}
        \hline
        & \textit{Training} & \textit{Validation} & \textit{Test} \\
        \hline
        \textit{MLP} & 84.030.93 & 75.540.14 & 61.060.08 \\
        \textit{Node2Vec} & 93.390.10 & 90.320.06 & 72.490.10 \\
        \textit{S-GCN}(=5) & 92.540.09 & 91.380.07 & 74.870.25 \\
        \hline
\textit{ClusterGCN} & 93.750.13 & 92.120.09 & {\bf \color{violet} 78.970.33} \\ 
        \textit{GraphSAINT} & 92.710.14 & 91.620.08 & {\bf \color{red} 79.080.24} \\ 
        \hline
        \textit{SIGN}(3,0,0) & 96.210.31 & {\bf 92.990.05} & 76.520.14 \\
        \textit{SIGN}(3,0,1) & {\bf 96.460.29} & 92.930.04 & 75.730.20 \\
        \textit{SIGN}(3,3,0) & {\bf \color{violet} 96.870.23} & {\bf \color{violet} 93.020.04} & 77.130.10 \\
        \textit{SIGN}(5,0,0) & 95.990.69 & 92.980.18 & 76.830.39 \\
        \textit{SIGN}(5,3,0) & {\bf \color{red} 96.920.46} & {\bf \color{red} 93.100.08} & {\bf  77.600.13}\\
        \hline
    \end{tabular}
\end{table}

\paragraph{Ablation study}
How do different operator combinations affect \textit{SIGN} performance? Results obtained with different choices of operators and their powers are reported in Tables \ref{tab:results_ogb}, \ref{tab:ogbn-100m} and~\ref{tab:inductive_ablation} for, respectively, the the transductive \texttt{ogbn-products} and \texttt{ogbn-papers100M} and inductive datasets. We notice that best performance is obtained on each benchmark by a specific combination of operators, remarking the fact that each dataset features particular topological and content characteristics requiring suitable filters. Interestingly, we also observe that while PPR operators do not bring significant improvements in the inductive setting (being even harmful in certain cases), they are beneficial on the transductive \texttt{ogbn-products}. This finding is in accordance with \cite{klicpera_diffusion_2019}, where the effectiveness of PPR diffusion operators in transductive settings has been extensively studied. Finally, we notice promising results attained in \texttt{Flickr} and \texttt{PPI} inductive settings by pairing standard adjacency matrices with a triangle-induced one.
Studying the effect of operators induced by more complex network motifs is left for future research.

\begin{table*}[t]
    \centering
    \caption{Impact of various operator combinations on inductive datasets. Best results are in bold.}
    \label{tab:inductive_ablation}
    \begin{tabular}{| c | cccc |}
        \hline
        & \texttt{Reddit} & \texttt{Flickr} & \texttt{PPI} & \texttt{Yelp} \\
        \hline
        \textit{SIGN}(2,0,0) & 0.9660.003 & 0.5030.003 & 0.9650.002 & 0.6230.005 \\
        \textit{SIGN}(2,0,1) & 0.9660.000 & 0.5100.001 & \textbf{0.9700.003} & \textbf{0.6310.003} \\\textit{SIGN}(2,2,0) & 0.9670.000 & 0.4950.002 & 0.9640.003 & 0.6170.005 \\
        \textit{SIGN}(4,0,0) & 0.9670.000 & 0.5080.001 & 0.9590.002 & 0.6230.004 \\
        \textit{SIGN}(4,0,1) & 0.9670.000 & \textbf{0.5140.001} & 0.9650.003 & 0.6230.004 \\\textit{SIGN}(4,2,0) & \textbf{0.9680.000} & 0.5000.001 & 0.9300.010 & 0.6180.004 \\
        \textit{SIGN}(4,2,1) & 0.9670.000 & 0.5080.002 & 0.9690.001 & 0.6200.004 \\\hline
    \end{tabular} 
\end{table*}

\begin{table}[t]
    \centering
    \caption{Results on \texttt{ogbn-papers100M}, the largest public graph dataset with over 110 million nodes. {\em SIGN(,,)} refers to a configuration using , , and  powers of simple undirected, directed and directed-transposed adjacency matrices. The top three performance scores are highlighted as: {\bf \color{red} First}, {\bf \color{violet} Second}, {\bf Third}.}
    \label{tab:ogbn-100m}
    \begin{tabular}{| l | ccc |}
        \hline
        & \textit{Training} & \textit{Validation} & \textit{Test} \\
        \hline
        \textit{MLP}
            & 54.840.43
            & 49.600.29
            & 47.240.31 \\
        \textit{Node2Vec}
            & ---
            & 55.600.23
            & 58.070.28 \\
        \textit{S-GCN}(=3)
            & 67.540.43
            & 66.480.20
            & 63.290.19 \\
        \hline
        \textit{SIGN}(3,0,0)
            & \textbf{70.180.37}
            & \textbf{67.570.14}
            & \textbf{64.280.14}\\
        \textit{SIGN}(3,1,1)
            & \textbf{\color{violet} 72.240.32}
            & \textbf{\color{violet} 67.760.09}
            & \textbf{\color{violet} 64.390.18 }\\
        \textit{SIGN}(3,3,3)
            & \textbf{\color{red} 73.940.72}
            & \textbf{\color{red} 68.60.04}
            & \textbf{\color{red} 65.110.14} \\
        \hline
    \end{tabular} 
\end{table}


\section{Conclusion and Future Work} \label{sec:conclusion}
 
In this paper we presented \textit{SIGN}, a sampling-free Graph Neural Network model that is able to easily scale to gigantic graphs while retaining enough expressive power to attain competitive results in all large-scale graph learning benchmarks. \textit{SIGN} attains state-of-the-art results on many of them, including the massive \texttt{ogbn-papers100M}, currently the largest  publicly available node-classification benchmark  with M nodes and B edges.
Our experiments have further demonstrated that the ability of our model to flexibly incorporate diverse, possibly domain-specific, operators is crucial to overcome the expressivity limitations of other sampling-free scalable models such as \textit{S-GCN}, which \textit{SIGN} has constantly outperformed over all datasets.
Overall, our architecture achieves an optimal trade-off between simplicity and expressiveness; as it has shown to attain competitive results with fast training and inference, it represents the most suitable architecture for scalable applications to web-scale graphs.

\paragraph{Depth vs. width for Graph Neural Networks}
Our results have shown that it is possible to obtain competitive -- and often state-of-the-art -- results with one single graph convolutional layer and hence a shallow architecture. An important question is, therefore, when one should apply deep architectures to graphs, where by `depth' we refer to the number of stacked graph convolutional layers.
Deep Graph Neural Networks are notoriously hard to train due to vanishing gradients and feature smoothing \cite{li2018adaptive,klicpera2018predict,wu2020comprehensive}, and, although recent works have shown that these issues can be addressed to some extent \cite{jk,gong2020geometrically, li2019deepgcns, zhao2019pairnorm, rong2019dropedge}, yet extensive experiments conducted in \cite{rong2019dropedge} showed that depth often does not bring any significant gain in performance w.r.t. to shallow baselines.
A promising direction for future investigation is, rather than `going deep', to `go wide', in the sense of exploring more expressive local operators. We believe this to be especially crucial in all those settings where scalability is a concern of paramount importance, such as in industrial large-scale systems.

\paragraph{Extensions}
In our experiments, triangle-based operators showed promising results. Possible extensions can employ operators that account for higher-order structures such as simplicial complexes \cite{barbarossa2019topological}, paths \cite{flam2020neural}, or motifs \cite{monti2018motifnet} that can be tailored to the specific problem. 
Furthermore, temporal information can be integrated e.g. in the form of temporal motifs \cite{paranjape2017motifs}. 

\paragraph{Limitations}
While our method relies on linear graph aggregation operations of the form  for efficient precomputation, it is possible to make the diffusion operator dependent on the node features (and edge features, if available). 
In particular, graph attention \cite{DBLP:conf/iclr/VelickovicCCRLB18} and similar mechanisms \cite{Monti2016GeometricDL} use , where  are learnable parameters. The limitation is that such operators preclude efficient precomputation, which is key to the efficiency of our approach. 
Attention can be implemented in our scheme by training on a small subset of the graph to first determine the attention parameters, then fixing them to precompute the diffusion operator that is used during training and inference.  


\bibliographystyle{ACM-Reference-Format}
\bibliography{neurips_2020}



\newpage
\appendix

\section{Datasets}

\subsection{Inductive datasets}

\texttt{Reddit} \cite{GraphSAGE} and \texttt{Flickr} \cite{DBLP:journals/corr/abs-1907-04931} are multiclass classification problems, \texttt{Yelp} \cite{DBLP:journals/corr/abs-1907-04931} and \texttt{PPI} \cite{Zitnik_2017} are multilabel classification instances. In \texttt{Reddit}, the task is to predict communities of online posts based on user comments. In \texttt{Flickr} the task is image categorization based on the description and common properties of online images. In \texttt{Yelp} the objective is to predict business attributes based on customer reviews; the task of \texttt{PPI} consists in predicting protein functions from the interactions of human tissue proteins. Further details on the generation of the \texttt{Yelp} and \texttt{Flickr} datasets can be found in \cite{DBLP:journals/corr/abs-1907-04931}.

\subsection{Transductive dataset}

\texttt{ogbn-products} \cite{ogb2020} represents an Amazon product co-purchasing network \cite{Bhatia16} where the task is to predict the category of a product in a multi-class classification setup. Dataset splitting is not random, sales ranking (popularity) is instead used to split nodes into training/validation/test. Top 10\% products in the ranking are assigned to the training set, next top 2\% to validation and the remaining 88\% of products are for testing.

\texttt{ogbn-papers100M} \cite{ogb2020} represents a directed citation network of  111 million academic papers, where the task is to leverage information from the entire citation network to infer the labels (subject areas) of a smaller subset of ArXiv papers. The splitting strategy is time-based. Specifically, the training nodes (with labels) are all ArXiv papers published until 2017, validation nodes are ArXiv papers published in 2018 and test nodes are ArXiv papers published since 2019.

\subsection{Wikipedia}

\texttt{Wikipedia} links is a large-scale directed network of links between articles in the English version of Wikipedia. For the sake of our timing experiments the network has been turned into undirected. Node features have been randomly generated with a dimensionality of  as in \texttt{ogbn-products}.

\section{Model Selection and Hyperparameter Tuning}

Tuning involved the following architectural and optimization hyperparameters: weight decay, dropout rate, batch size, learning rate, number of feedforward layers and units both in inception and classification modules. For each inductive experiment we chose the set of hyperparameters matching the best average validation loss calculated over  runs. For the the transductive setting we kept, instead, the set of hyperparameters with minimum validation loss over a single run. The hyperparameter search space for the inductive setting and grid for the transductive one are described in Table \ref{tab:hyperparameter_search}. The estimated hyperparameters for each best \textit{SIGN} configuration are reported in Table \ref{tab:hyperparameter_tuned} for inductive datasets and Table \ref{tab:hyperparameter_tuned_ogb} for the transductive ones.

\begin{table*}[h!]
    \caption{Hyperparameter search space/grid. Ranges in the form [\textit{low}, \textit{high}] and sampling distributions. \textit{Inception Layers} and \textit{Classification Layers} are the number of feedforward layers in the representation part of the model (replacing ) and the classification part of the model (replacing ) respectively. The only exception is represented by \textit{Yelp}, for which the  module was kept shallow (no hidden layers) to allow for lighter training and the left bounds on the dropout, learning rate and batch size intervals were lowered to, respectively, ,  and .}\label{tab:hyperparameter_search}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{| r | c | cc |}
    \hline
    & \multicolumn{1}{c|}{Transductive} & \multicolumn{2}{c|}{Inductive} \\
    \hline
    Hyperparameter & Values & Space & Distribution \\
    \hline
    \textit{Learning Rate}
        & 0.0001, 0.001 
        & [0.0001, 0.0025]
        & Uniform \\
    \textit{Batch Size}
        & 4096, 8192, 16384
        & [128, 2048]
        & Quantized Uniform \\
    \textit{Dropout}
        & 0.5
        & [0.2, 0.8]
        & Uniform \\ 
    \textit{Weight Decay}
        & 0.0, 0.00001
        & [0, 0.0001]
        & Uniform \\
    \textit{Inception Layers}
        & 1
        & 1, 2
        & ---\\ 
    \textit{Inception Units}
        & 256, 512
        & [128, 512]
        & Quantized Uniform\\ 
    \textit{Classification Layers}
        & 1
        & 1, 2
        & --- \\
    \textit{Classification Units}
        & 256, 512
        & [512, 1024]
        & Quantized Uniform\\ 
    \textit{Activation}
        & PReLU
        & ReLU, PReLU
        & --- \\
    \hline
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}


\begin{table*}[h!]
    \caption{Hyperparameters chosen for the best configuration of SIGN on inductive datasets.}
    \label{tab:hyperparameter_tuned}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{| r | cccc |}
    \hline
    Hyperparameter & Reddit & Flickr & PPI & Yelp\\\hline
    \textit{Learning Rate}
        & 0.00012278578238312588
        & 0.0017230142114465549
        & 0.0014386686616183625
        & 0.00005\\
    \textit{Dropout}
        & 0.707328910934901
        & 0.7608352140584778
        & 0.3085607444207686
        & 0.05\\
    \textit{Weight Decay}
        & 9.176773905054599e-05
        & 9.419820474221673e-05
        & 3.2571631135664696e-06
        & 4.452466189193362e-07\\
    \textit{Batch Size}
        & 830
        & 330
        & 210
        & 90\\
    \textit{Inception Layers}
        & 1
        & 2
        & 2
        & 2\\
    \textit{Inception Units}
        & 460
        & 465
        & 315
        & 320\\
    \textit{Classification Layers}
        & 1
        & 1
        & 2
        & 0\\
    \textit{Classification Units}
        & 675
        & 925
        & 870
        & ---\\
    \textit{Activation}
        & ReLU
        & PReLU
        & ReLU
        & ReLU\\
    \hline
    \end{tabular}}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

\begin{table*}[h!]
    \caption{Hyperparameters chosen for the best configuration of SIGN on \texttt{ogbn-product} and those used on the \texttt{ogbn-product} dataset.}
    \label{tab:hyperparameter_tuned_ogb}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{| r | c | c |}
    \hline
    Hyperparameter & \texttt{ogbn-products} & \texttt{ogbn-papers100M} \\
    \hline
    \textit{Learning Rate}
        & 0.0001
        & 0.001 \\
    \textit{Dropout}
        & 0.5
        & 0.1 \\
    \textit{Weight Decay}
        & 0.0001
        & 0.0 \\
    \textit{Batch Size}
        & 4096
        & 256 \\
    \textit{Inception Layers}
        & 1
        & 1 \\
    \textit{Inception Units}
        & 512
        & 256 \\
    \textit{Classification Layers}
        & 1
        & 3 \\
    \textit{Classification Units}
        & 512 
        & 256 \\
    \textit{Activation}
        & PReLU
        & ReLU \\
    \hline
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

\section{Triangle-based Operators}

The triangle operator encodes the concept of \emph{homophily} with a stronger acceptation with respect to the adjacency matrix: two nodes are connected by an edge only if they are both part of the same closed triad, i.e. if they are connected together and are both connected to the same node. Edge weights are proportional to the amount of triangles an edge belongs to, and they are normalised row-wise so to represent, for each node in a neighbourhood, its relative importance with respect to all the other neighbors.

This brings us to two considerations: first of all, the triangle operator is not carrying information related to nodes which were not already in the neighborhood. Secondly, it emphasizes the connections with those neighbors which are more related to our source node in virtue of the relationship described above. We can thus envision this operator being more useful in those graphs where this kind of relationship can be more discriminative within a neigborhood.

\begin{figure}
    \begin{subfigure}[t]{0.465\linewidth}
    \centering
    \includegraphics[width=\textwidth]{images/reddit.png}
    \end{subfigure}\hfill \begin{subfigure}[t]{0.465\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/flickr.png}
    \end{subfigure}\hfill \begin{subfigure}[t]{0.465\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/ppi.png}
    \end{subfigure}\hfill \begin{subfigure}[t]{0.465\linewidth}
    \centering
    \includegraphics[width=\textwidth]{images/yelp.png}
    \end{subfigure}
    \caption{Normalized frequency distributions for row-wise variations on the diffusion weights of triangle operators over inductive datasets. Variations are measured as the standard deviation on the weight value over original neighborhoods from the test graph.}
    \label{fig:triangles}
\end{figure}

To verify this, in Figure~\ref{fig:triangles} we plot the normalized frequency distribution of intra-neighborhood standard deviation for the weights of triangle operators. It is interesting to notice the significantly different trends characterizing \texttt{Flickr} and \texttt{Reddit}, two datasets where triangle operators have experimentally brought, respectively, relative large and small performance improvement. \texttt{Flickr} tends to exhibit larger weight variations than other datasets, while, on the contrary, \texttt{Reddit} is the dataset where the smallest intra-neighborhood variation is observed. This suggests how, in \texttt{Flickr}, the triangle operator is able to restrict feature aggregation to a subset of the original neighbors --those co-occurring in the larger number of triangles-- while in \texttt{Reddit} it mostly boils down to uniform averaging, making this operator not much more expressive than a simple adjacency matrix.

For replicability we report that, in the computation of triangle operators for \texttt{PPI}, we retained the self-loops already present in the original dataset. Investigations on how the presence of these edges affects the expressiveness of the triangle operator are left for future work. 
\end{document}
