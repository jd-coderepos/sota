\documentclass{article}





\usepackage[preprint, nonatbib]{neurips_2023}











\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks, linkcolor=green]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}


\title{Late-Constraint Diffusion Guidance for Controllable Image Synthesis}





\author{Chang Liu\textsuperscript{1}, Dong Liu\textsuperscript{1}\thanks{Corresponding author.}\\
  \textsuperscript{1}University of Science and Technology of China\\
  \texttt{lc980413@mail.ustc.edu.cn, dongeliu@ustc.edu.cn}
}


\begin{document}


\maketitle


\begin{abstract}


Diffusion models, either with or without text condition, have demonstrated impressive capability in synthesizing photorealistic images given a few or even no words. These models may not fully satisfy user need, as normal users or artists intend to control the synthesized images with specific guidance, like overall layout, color, structure, object shape, and so on. To adapt diffusion models for controllable image synthesis, several methods have been proposed to incorporate the required conditions as regularization upon the intermediate features of the diffusion denoising network. These methods, known as \emph{early-constraint} ones in this paper, have difficulties in handling multiple conditions with a single solution. They intend to train separate models for each specific condition, which require much training cost and result in non-generalizable solutions. To address these difficulties, we propose a new approach namely \emph{late-constraint}: we leave the diffusion networks unchanged, but constrain its output to be aligned with the required conditions. Specifically, we train a lightweight condition adapter to establish the correlation between external conditions and internal representations of diffusion models. During the iterative denoising process, the conditional guidance is sent into corresponding condition adapter to manipulate the sampling process with the established correlation. We further equip the introduced late-constraint strategy with a timestep resampling method and an early stopping technique, which boost the quality of synthesized image meanwhile complying with the guidance. Using the proposed method, we manage to perform image synthesis upon multiple conditions at the training cost of several hours on a single 3090 GPU. Our method outperforms the existing early-constraint methods and demonstrates multiple applications with its plausible generalization ability and flexible controllability. Our code would be open-sourced at \href{https://github.com/AlonzoLeeeooo/LCDG}{https://github.com/AlonzoLeeeooo/LCDG}.







\end{abstract}  

\section{Introduction}







Recently, diffusion model has come into prominence in the field of image synthesis. With the enhancement of multimodal learning~\cite{radford2021learning, cherti2022reproducible, raffel2020exploring}, text-to-image diffusion model~\cite{rombach2022high, huang2023composer, saharia2022photorealistic, nichol2021glide, gu2022vector, avrahami2022blended, kim2022diffusionclip, brooks2022instructpix2pix, gafni2022make, kawar2022imagic, liu2023more} has revealed plausible generation ability in numerous tasks. Such model is capable of producing photorealistic results given a few words of description or even none, which has revealed great potentials for inspiring and creative applications.

Nevertheless, text prompts could still be limited during user interfaces. It could be ambiguous if used to describe structural information as guidance, \textit{e.g.}, layout, structure, object position, and so on. Such limitations motivate several methods~\cite{zhang2023adding,mou2023t2i}, to introduce more precise control upon diffusion models. As is shown Fig.~\ref{figure:tissor}, these methods, known as \textit{early-contraint} ones in this paper, implement condition-specific encoders to regularize the intermediate layers of diffusion denoising networks. However, learning the process of such intra-manipulation requires much resources to implement. Furthermore, such techniques are confined as one-for-one solutions, which sacrifices possible generalization ability to other unseen conditions during user interfaces.



\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{tissor.jpg} \vspace{-1.5em}
  \caption{Compared to existing \textit{early-constraint} solutions, we present \textit{Late-Constraint Diffusion Guidance (LCDG)}, to provide guidance for diffusion models externally meanwhile keeping the diffusion U-Net unchanged.}
  \label{figure:tissor}
  \vspace{-1.5em}
  \end{figure}



To tackle the aforementioned limitations, we present a novel diffusion guidance named \textit{Late-Constraint Diffusion Guiance (LCDG)}, which keeps the diffusion network unchanged and regularize its output to align with the external guidance. Fig.~\ref{figure:tissor} illustrates the comparison between our \textit{late-constraint} solution and \textit{early-constraint} ones. Specifically, we train a lightweight condition adapter to capture the correlation between internal representations of diffusion models and external guidance. During sampling, we reformulate the estimated score of diffusion networks with the external control using corresponding trained adapter. Note that LCDG could serve as a plug-and-play tool for diffusion sampling of different backbones, and is compatible with their original sampling processes. Furthermore, we propose a timestep resampling strategy to ensure the stability of the proposed method, and introduce an early stopping technique to boost the sample quality meanwhile preserving the conditional guidance. Notably, the training of adapter is compute-efficient compared to early-constraint ones, which requires solely hours of training with small-scale self-collected data on a single 3090 GPU. Evaluation on COCO validation set~\cite{lin2014microsoft} indicates that the proposed late-constraint solution obtains outperforming FID~\cite{NIPS2017_8a1d6947} and comparable CLIP score~\cite{radford2021learning}, compared to existing early-constraint ones~\cite{mou2023t2i, zhang2023adding, rombach2022high}. Furthermore, experiments upon multiple applications with different diffusion backbones demonstrate the superiority of the proposed techniques. Our contribution could be summarized as:
\begin{enumerate}
\setlength{\itemsep}{0pt}
  \setlength{\parsep}{0pt}
  \setlength{\parskip}{0pt}
 \item We observe the limitations of existing \textit{early-constraint} solutions, and present \textit{Late-Constraint Diffusion Guidance}, which manages to align the output of the diffusion U-Net with the external guidance keeping the diffusion model unchanged.
  \item We propose a lightweight and compute-efficient adapter to learn the correlation between the internal representations of the diffusion model and multiple external conditions.
  \item Furthermore, we propose a timestep resampling strategy to stabilize the proposed method, and introduce a early stopping technique to boost the sample quality meanwhile preserving the effect of conditional guidance.
  \item Evaluation upon public dataset demonstrates the promising performance of the proposed technique compared to existing early-constraint methods. Also, applications upon multiple conditions with both unconditional and text-to-image diffusion models illustrate the effectiveness and universal applicability of our method.
\end{enumerate}





\section{Related Work}



\textbf{Image Synthesis and Translation.} Learning the high-dimensional data manifold of natural images is a great challenge for the topic of image synthesis, which have driven numerous efforts using different generative models, \textit{e.g.}, GANs~\cite{karras2019style, brock2018large, karras2020analyzing, miyato2018spectral, karras2017progressive, kang2023scaling}, VAEs~\cite{van2017neural, razavi2019generating}, autoaggressive models~\cite{esser2021taming, chang2022maskgit, yu2022scaling, lee2022autoregressive, chang2023muse, li2022mage}, and diffusion models~\cite{ho2020denoising, ho2022cascaded, song2020score, nichol2021improved, song2020denoising, blattmann2022retrieval, saharia2022palette, choi2021ilvr}. Furthermore, conditional generative methods~\cite{isola2017image, wang2018high, park2019semantic, esser2021taming, yu2022scaling, lee2022autoregressive, saharia2022palette, rombach2022high} are also investigated, which utilize information in other domains to guide the synthesis process, \textit{e.g.}, sketch, semantic map. Recently, text-to-image generative models~\cite{chang2023muse, gafni2022make, brooks2022instructpix2pix, rombach2022high, kang2023scaling} have come into prominence and emerged numerous inspiring applications, especially text-to-image diffusion models~\cite{rombach2022high, huang2023composer, saharia2022photorealistic, nichol2021glide, gu2022vector, avrahami2022blended, kim2022diffusionclip, brooks2022instructpix2pix, gafni2022make, kawar2022imagic, liu2023more}. Such models have achieved significant performance for the community of conditional image synthesis. 

\textbf{Controllable Image Synthesis for Diffusion Models.} Compared to Generative Adversarial Networks (GAN)~\cite{karras2019style, brock2018large, karras2020analyzing, miyato2018spectral, karras2017progressive, kang2023scaling, liang2020cpgan, zhang2021cross, tao2020df, zhang2021dtgan, cheng2020rifegan, ruan2021dae}, diffusion models~\cite{ho2020denoising, ho2022cascaded, song2020score, nichol2021improved, song2020denoising, blattmann2022retrieval, gu2022vector, dhariwal2021diffusion, rombach2022high} have shown guaranteed training stability and superior generation ability across various benchmarks. Besides, text-to-image diffusion models~\cite{rombach2022high, gu2022vector} reveal more controllability with guidance of text prompts. To provide more precise controls for diffusion models, several efforts~\cite{zhang2023adding, mou2023t2i} have been further motivated. Known as \textit{early-constraint} methods in this paper, they manage to provide intra-manipulations upon diffusion U-Net before output. However, such early-constraint solutions still reveal limitations, which may sacrifice much flexibility during user interfaces.



\textbf{Diffusion Guidance.} With solely modifications upon the sampling process, diffusion guidance reveal great flexibility and efficiency to manipulate diffusion sampling without retraining. Previous researches intend to enhance the sample quality of diffusion models using either noisy classifier~\cite{dhariwal2021diffusion} or the diffusion U-Net~\cite{ho2022classifier}. One related research with ours is SDG~\cite{liu2023more} attempting to introduce high-level guidance, \textit{e.g.}, language and image, based on pre-trained CLIP~\cite{radford2021learning} and DDPM~\cite{nichol2021improved}. But the potentials of discovering structural guidance from such aspect still remain to be investigated.

\section{Approach}
\label{section:approach}

\subsection{Preliminaries}

\textbf{Latent Diffusion Models.} We implement our method on both unconditional and text-to-image models of Stable Diffusion (SD)~\cite{rombach2022high}. SD is implemented as a two-stage architecture. It first encodes the RGB image  into its corresponding latent representation  using a pre-trained VQ-GAN~\cite{esser2021taming}. Then, it learns the reverse denoising process of a fixed Markov Chain of length  in the latent space. The optimization objective could be formulated as:

where , representing the process of adding a -step noise to .  denotes the estimated score. In the case of text-to-image generation,  is conditioned on , which denotes the text embeddings extracted by external text encoders~\cite{radford2021learning, cherti2022reproducible}.

\textbf{Sampling.} During sampling of diffusion models, also known as the iterative denoising process, we start from a random Gaussian noise . In text-to-image generation case, the sampling process is conditioned on the  extracted from the user-provided text prompts. Given , the diffusion U-Net estimates the sampling score  and iteratively subtracts noises from the intermediate sample , where we eventually obtains the clean sample .

\subsection{Correlating by Reconstructing}
\label{section:condition adapter}



Our goal is to manipulate diffusion sampling with external condition in a late-constraint manner. We first manage to discover the correlation between the output of diffusion U-Net and the external condition. Recently, diffusion models are proven to obtain well-differentiated internal representations, which highly correlate with semantic knowledge of the source images~\cite{baranchuk2021label, xu2023open}. We draw motivation from such solutions, and manage to learn the correlation towards more unexplored conditions.

\begin{figure*}[t!]
  \label{figure:training pipeline}
  \centering
  \includegraphics[width=1.0\textwidth]{training_pipeline.jpg} \vspace{-1em}
  \caption{Illustration of the training pipeline of LCDG. Keeping the diffusion networks unchanged, our proposed LCDG requires to train a lightweight Condition Adapter (CA), to capture the correlation between the internal representations of diffusion models with external conditions.}
  \vspace{-1em}
  \end{figure*}

\textbf{Condition Adapter.} Therefore, we propose Condition Adapter (CA), a lightweight CNN adapter to capture the correlation by learning to reconstruct the external conditions. Given an RGB image , we first encode it into the latent representation  using~\cite{esser2021taming}. We add -step noises to  according to , and obtains the noisy representation  with . Then, we extract a series of internal representations  from different blocks  in the diffusion U-Net passing  forward. Note that in the case of text-to-image generation~\cite{rombach2022high}, the forward process is conditioned on the text embeddings  extracted by text encoders~\cite{radford2021learning, cherti2022reproducible}.  are then aligned and integrated as  by upsampling and concatenating them. Eventually, we train CA to reconstruct the external condition from the given .

In practice, it is natural to implement CA as CNN-based architecture to capture local structural information from the internal representations. Notably with a few stacks of convolution blocks, CA is able to learn the correlation. Each block is a two-branch design, which processes the input representations  and timestep , respectively. We follow the implementation in the U-Net of SD~\cite{nichol2021improved} using a sigmoid linear unit (SiLU) and a linear layer to formulate the timestep branch. The other branch is simply implemented with architecture of \texttt{Conv-ReLU-BN}.  is encoded into time embeddings and merged to the intermediate features in each block. CA learns to reconstruct the structural conditions supervised by . The overall optimization objective could be formulated as:

where  denotes CA with  as its model weights. Note that the training of CA is compute-efficient and fast to implement, which only requires hours of training on a single 3090 GPU per condition. Also, the training of CA is dataset-free, which could be well implemented with small-scale self-collected data and automatically synthetic labels (Sec.~\ref{section:experiments}).


\textbf{Timestep Resampling.} Timestep is an essential variable for diffusion models, which is highly associated with the magnitude of noise. During the training process of SD~\cite{rombach2022high}, the timestep  is randomly sampled from a uniform distribution . During sampling, the diffusion U-Net iteratively subtracts noise in the intermediate sample  with  starting from  to . As is known that the overall contents of diffusion models are determined at the beginning stage during sampling~\cite{mou2023t2i}, which indicates that CA is expected to handle highly dynamic noisy samples  with different contents and levels of noises. To this end, we propose to resample the timestep  instead of sampling it from a uniform distribution . Given a uniform-sampled timestep as , the resampled timestep  becomes:

where .  is a hyperparameter that determines the magnitude of resampling. Compared to the uniform-sampled timestep , the curve of  with resampling is a convex function, where for each input , the resampled timestep obtains a greater value than the original one.



\subsection{Structure-Aware Diffusion Sampling}
During inference, we manipulate the diffusion sampling with the external guidance and corresponding adapter. We demonstrate some applications of practical usage with a series of structural guidance, including edge, sketch, color stroke, palette, and mask, denoting the manipulated sampling process as \textit{Structure-Aware Diffusion Sampling}. Fig.~\ref{figure:visualizations of Intermediates} shows the visualizations of text-to-image diffusion sampling and the guided one. One could see that LCDG aligns the intermediate samples with the external guidance and stabilizes corresponding conditions with the learned correlation. For one manipulated sampling step , we obtain the internal representations of  from the diffusion U-Net, and reconstruct its corresponding condition . Then, we obtain the difference map between  and the external condition  by calculating the latent distance between them, written as . The condition score  correlated to the structural guidance is then computed in forms of gradients with respect to . Furthermore, we normalize  with a controlling scale , eventually reformulating the estimated score  of diffusion U-Net as:

where the condition score could be obtained by corresponding CA. For the text-to-image generation scenario, the sampling process is conditioned on the text embeddings . In brief, we summarize the sampling score using LCDG as . 



\begin{figure}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{intermediates.jpg} \vspace{-1em}
  \caption{Visualizations of the intermediate samples and corresponding conditions under text-to-image diffusion sampling (left) and structure-aware diffusion sampling (right).}
\label{figure:visualizations of Intermediates}
\vspace{-1em}
\end{figure}

\textbf{Truncation Conditioned Sampling.} Motivated by the effectiveness of early stopping strategy~\cite{lyu2022accelerating} upon diffusion models, we manage to further explore its possible benefits for controllable image synthesis. Therefore, we propose Truncation Conditioned Sampling (TCS) strategy to boost the sample quality meanwhile preserving the guidance of external conditions. We discover that manipulating parts of the sampling process instead of the whole one obtains 
synthesized image in better quality, meanwhile accelerating the sampling process (Sec.~\ref{subsection:ablation study}). Specifically, we assign a truncation threshold , and solely manipulate the steps in  during sampling. In a nutshell, the overall process of LCDG could be summarized as follows:
\vspace{-0.5em}
\begin{algorithm}[H]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \caption{Structure-Aware Diffusion Sampling}
    \begin{algorithmic}
    \label{algorithm: LCDG sampling}
        \Require Target condition , frozen diffusion U-net , controlling scale , truncation threshold of TCS 
        \Ensure result 
        \State 
        \State 
        \For {} \textcolor{blue}{\Comment{Sample from the conditional distribution}}
        \State \qquad 
        \EndFor
        \For {} \textcolor{blue}{\Comment{Sample from the original distribution}}
        \State \qquad 
        \EndFor\\
        \Return 
    \end{algorithmic}
\end{algorithm}
\vspace{-1em}
Here,  represents the shifted mean correlated to external conditions, written as . Note that the process is conditioned on extracted text embeddings  in the case of text-to-image generation. It is proven that the conditional distribution could be approximated by a Gaussian distribution with a shifted mean in the case of class-conditional image synthesis~\cite{dhariwal2021diffusion}. LCDG embodies as a broader case of structural conditions that using LCDG essentially samples from the conditional distribution associated with the structural guidance. The aforementioned algorithm also 
illustrates the compatibility of LCDG with the original sampling process, which provides more flexibility and controllability for users to customize their manipulations. 

\section{Experiments}
\label{section:experiments}

\subsection{Experimental Setup}

\begin{table}[t!]
  \centering
  \caption{Quantitative comparison on COCO 2017 validation set~\cite{lin2014microsoft}, compared to SD*~\cite{rombach2022high}, ControlNet~\cite{zhang2023adding}, and T2I-Adapter~\cite{mou2023t2i}. The \textbf{best} results under each setting are highlighted. Note that we use HED edge~\cite{xie2015holistically} for evaluation of edge condition. SD*: Stable Diffusion~\cite{rombach2022high} using SDEdit~\cite{meng2021sdedit}. T2I-Adapter*: Reproduced results.}
\begin{tabular}{lccc}
   \toprule
   Method & Condition & FID & CLIP Score\\
   \midrule
   SD~\cite{rombach2022high} & text & 27.99 & 0.2673 \\
   \midrule
ControlNet~\cite{zhang2023adding} & text + edge & 28.09 & 0.2525 \\
   T2I-Adapter*~\cite{mou2023t2i} & text + edge & 21.72 & \textbf{0.2597} \\
   \textbf{Ours} & text + edge & \textbf{21.02} & 0.2590 \\
   \midrule
   SD*~\cite{rombach2022high} & text + stroke & 32.93 & 0.2257 \\
   T2I-Adapter~\cite{mou2023t2i} & text + stroke & 30.84 & 0.2587 \\
   \textbf{Ours} & text + stroke & \textbf{20.27} & \textbf{0.2589} \\
   \midrule
   SD*~\cite{rombach2022high} & text + palette & 71.16 & 0.2138 \\
   T2I-Adapter~\cite{mou2023t2i} & text + palette & 26.54 & \textbf{0.2613} \\
   \textbf{Ours} & text + palette & \textbf{20.61} & 0.2580 \\
   \midrule
   \textbf{Ours} & text + mask & \textbf{20.94} & \textbf{0.2617} \\
   \bottomrule
\end{tabular}
\vspace{-1.5em}
\label{table:quantitative comparison}
\end{table}

\textbf{Implementation Details.} LCDG is compute-efficient and fast to implement. We notice that current early-constraint methods~\cite{mou2023t2i, zhang2023adding} still rely on annotated datasets in million scales for training, \textit{e.g.}, COCO. In reverse, LCDG works as a dataset-free solution, which could perform well with training of small-scale self-collected data and synthetic labels. For training, we randomly collect 10,000 images from the internet and automatically annotate their corresponding captions using a pre-trained image captioning model~\cite{hu2022expansionnet}. We summarize the implementations of LCDG on three main domains of structural conditions: \textit{edge}, \textit{color}, and \textit{mask}. Note that each domain of the implemented conditions shares the same model during inference. \textit{Edge condition} includes Canny edge~\cite{canny1986computational}, HED-like edge~\cite{xie2015holistically}, and user sketch. We use a HED-like edge detector~\cite{he2020bdcn} together with some data augmentation strategies to simulate the supervision signal, which includes random thresholds, random warping, random dilation and corrosion. \textit{Color condition} consists of color stroke and image palette. We refer to the human-stroke simulation algorithm in~\cite{meng2021sdedit} with augmentation of varying the k-means center, and simulate the image palette by resizing source images using the nearest interpolation. \textit{Mask condition} contains binary masks in different grained levels. We demonstrate some instances in Fig.~\ref{figure:singe-conditioned results} that coarse-grained user scribbles intend to guide the abstract position in the generated contents, and fine-grained segmentation masks manage to locate the precise shape and position of the salient objects. To simulate the supervision labels, we use a pre-trained saliency detection model~\cite{qin2020u2} to detect masks from source images.

\textbf{Evaluation Setting.} For evaluation, we compare LCDG to state-of-the-art conditional text-to-image synthesis methods as well as early-constraint solutions, including SD*~\cite{rombach2022high} (SD using SDEdit~\cite{meng2021sdedit}, for comparison of stroke condition), ControlNet~\cite{zhang2023adding}, and T2I-Adapter~\cite{mou2023t2i}. Text-to-image SD~\cite{rombach2022high} is also evaluated for reference. Besides, we implement cross evaluations to measure the performance of early-constraint solutions upon other unseen conditions. Evaluations are implemented on COCO 2017 validation set~\cite{lin2014microsoft} using the official annotated captions. We use FID score~\cite{NIPS2017_8a1d6947} and CLIP score~\cite{radford2021learning} (using ViT/L-14 model) as evaluation metrics, and SD~\cite{rombach2022high} with version of  as our base model during evaluation. Note that we could not reproduce the result of \cite{mou2023t2i} as good as their reported one, and provide our reproduced one in Tab.~\ref{table:quantitative comparison}. We would open-source our evaluation code and provide evaluation details in our supplementary materials.

\textbf{Demonstrated Applications.} LCDG provides a paradigm to introduce external conditions into diffusion sampling, which should be compatible with different kinds of diffusion backbones. Out of demonstration, we show some applications of LCDG upon multiple structural conditions, and both unconditional and text-to-image diffusion models~\cite{rombach2022high}. For unconditional generation setting, we demonstrate some conditional generation tasks in the face domain with SD~\cite{rombach2022high} trained on CelebA-HQ~\cite{karras2017progressive}. For text-to-image generation setting, we use text-to-image SD~\cite{rombach2022high} with version of  for applications. Note that we implement \textit{edge} and \textit{color} conditions on unconditional applications, and all three domains on text-to-image ones.

\subsection{Evaluation}



Tab.~\ref{table:quantitative comparison} reports the quantitative results of LCDG compared to state-of-the-art competitors~\cite{wang2022pretraining, zhang2023adding, mou2023t2i, rombach2022high} under different conditions. Our method outperforms others in FID \cite{NIPS2017_8a1d6947} under all settings, indicating the promising sample quality of LCDG. Note that all compared methods obtain lower CLIP scores than text-to-image SD~\cite{rombach2022high} does. But the proposed LCDG obtains the least deterioration in most cases and comparable performance. For the cross evaluation for early-constraint solutions~\cite{rombach2022high,mou2023t2i}, one could see that such methods fail the generalization process, thereby obtaining obvious performance drop in unseen conditions compared to their implemented ones.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{unconditional_results.jpg} \vspace{-1em}
  \caption{Conditional image synthesis results of LCDG, implemented with unconditional SD~\cite{rombach2022high} trained CelebA-HQ~\cite{karras2017progressive}. Each row of the results share the same model during inference. For hand-drawn sketches, we choose the proposed sketch set in~\cite{yang2020deep} for evaluation.}
  \label{figure:unconditional results}
  \vspace{-0.5em}  
\end{figure*}
  
  \begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{text_results.jpg} \vspace{-1em}
  \caption{Conditional text-to-image synthesis results of LCDG, implemented with text-to-image SD~\cite{rombach2022high}. Note that each row of the cases share the same model during inference.}
  \label{figure:singe-conditioned results}
  \vspace{-1em}
  \end{figure*}

\subsection{Applications}

Fig.~\ref{figure:unconditional results} reports some applications using LCDG for conditional generation tasks in the face domain. And Fig.~\ref{figure:singe-conditioned results} shows some conditional text-to-image applications using LCDG. As is shown in Fig.~\ref{figure:unconditional results} and Fig.~\ref{figure:singe-conditioned results}, LCDG is capable of supporting both synthesized (left) and user-provided conditions (right). One can see that using LCDG allows users to confirm their desired overall structure of the generated results in the first place, then let diffusion models sample from the original distribution to produce realistic details in the remaining steps. By using different text prompts, users could further enrich the semantic information of the generated contents. Notably, each separated row of showcases shares the same model weights, \textit{e.g.}, color stroke and image palette, indicating the plausible generalization ability of LCDG to similar conditions in near domains without retraining. In Fig~\ref{figure:controlling scale ablation} and Fig.~\ref{figure:truncation threshold ablation}, applications for controllable image synthesis demonstrate the flexibility of LCDG, and the computational advantage without retraining over early-constraint solutions~\cite{zhang2023adding,mou2023t2i,wang2018high}.


\subsection{Ablation Study}
\label{subsection:ablation study}



\textbf{Controlling Scale.} The controlling scale is a vital component of LCDG, which determines the weight of the condition score at each guided sampling step. Fig.~\ref{figure:ablation curves} (a) reports the curve of FID~\cite{NIPS2017_8a1d6947} for the ablation study and Fig.~\ref{figure:controlling scale ablation} shows corresponding qualitative results. By varying the controlling scale, LCDG provides plausible controllability for users to focus on different aspects of the guidance. One could see that the sampling process is manipulated leniently and focus more on semantic fidelity. As the controlling scale increases, samples are more strictly controlled concentrating more on the condition fidelity. Notably, the samples would be severely deteriorated once over-controlled.



\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{ablation_controlling_scale.jpg} \vspace{-1em}
  \caption{Qualitative results for ablation studies of controlling scale.}
  \label{figure:controlling scale ablation}
  \vspace{-0.5em}
  \end{figure*}
  
  \begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{ablation_truncation_step.jpg} \vspace{-1em}
  \caption{Qualitative results for ablation studies of truncation threshold of TCS.}
  \label{figure:truncation threshold ablation}
  \vspace{-1em}  
\end{figure*}

\textbf{Truncation Threshold of TCS.} The truncation threshold of the TCS strategy determines the number of manipulated steps during diffusion sampling. Also, it demonstrates the controllability of LCDG from another perspective. Fig.~\ref{figure:ablation curves} (b) reports the FID scores~\cite{NIPS2017_8a1d6947} and Fig.~\ref{figure:truncation threshold ablation} shows corresponding qualitative results. The smaller the truncation threshold is, the more steps are manipulated. LCDG is able to provide effective regularization with solely a few steps manipulated. The samples further follow the exact color and boundaries as the threshold increases. Without TCS, the details of samples would be severely deteriorated or reveal checkerboard artifacts. Also, the sampling speed (22.01s) would obviously slower than the one using TCS (11.94s, both tested on a single 3090 GPU).

\textbf{Magnitude of Timestep Resampling.} The timestep resampling strategy ensures the stability of LCDG while processing the highly dynamic noisy inputs during sampling. Fig.~\ref{figure:ablation curves} (c) and Fig.~\ref{figure:timestep resampling magnitude} report the ablation study both quantitatively and qualitatively. On one hand, we expect such augmentation strategy could benefit the training of CA sufficiently, where the FID scores and details of the samples gradually improve until the magnitude reaches the optimal one (). As  keeps increasing greater, the training data becomes over-noised. LCDG fails the manipulation upon diffusion sampling, and produces inaccurately controlled results with less details.

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\textwidth]{ablation_curves.jpg} \vspace{-1em}
\caption{Curves of FID scores for ablation studies of (a) controlling scale, (b) truncation threshold, (c) timestep resampling magnitude, and (d) model parameters of CA.}
\label{figure:ablation curves}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\textwidth]{ablation_timestep_resampling.jpg} \vspace{-1em}
\caption{Qualitative results for the ablation studies of timestep resampling magnitude.}
\label{figure:timestep resampling magnitude}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=1.0\textwidth]{ablation_model_parameters.jpg} \vspace{-1em}
\caption{Qualitative results for the ablation studies of model parameters of CA.}
\label{figure:model parameters}
\vspace{-1em}
\end{figure*}

\textbf{Model Parameters.} In addition to a series of hyperparameters of LCDG, we also ablate the minimum model parameters of CA and the performance lower bound of LCDG. Fig.~\ref{figure:ablation curves} (d) and Fig.~\ref{figure:model parameters} report the ablated results. We decrease the model parameters by removing several layers in CA and compress the network channels. It is worth noting that LCDG is still capable of  constraining the sampling process even with model parameter of \textbf{8M} (\textit{tiny} model), which proves the effectiveness of the design of LCDG once again. However, since CA could solely learn simple relationship between the internal representations and structural characteristics, the manipulated diffusion sampling tend to produce controlled results with less photorealistic details.

\section{Conclusions and Discussions}
We present an innovative solution for controllable image synthesis, called \textit{Late-Constrained Diffusion Guidance (LCDG)}, to manipulate diffusion sampling with external guidance meanwhile preserving the diffusion networks unchanged. Boosted by strategies like timestep resampling and truncation conditioned sampling, LCDG reveals promising performance for controllable image synthesis, and great generalization ability and flexibility over existing early-constraint methods. However, LCDG still suffers from some limitations that gradient-based methods~\cite{dhariwal2021diffusion, ho2022classifier} share, which would increase the sampling speed with additional forwarding processes of the diffusion U-Net. In our supplementary materials, we would illustrate more details and present more applications of the proposed LCDG.

{\small
\bibliographystyle{plain}
\bibliography{main_paper}
}

\clearpage
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\section{Overview}
We formulate our supplementary materials as follows. Sec.~\ref{section:mathematical derivation} provides a detailed mathematical Illustration of our proposed Late-Constraint Diffusion Guidance (LCDG). Sec.~\ref{section:implementation details} provide the details of network architecture of the proposed Condition Adapter (CA), pipelines for data preparation, training implementations, and evaluation settings. We discuss some limitations of LCDG in Sec.~\ref{limitations}, and propose possible solutions to address them. In Sec.~\ref{section:extended ablation studies}, we further ablate the domains of training data and minimum training iteration of CA. In Sec.~\ref{section:more qualitative results}, we report more qualitative results with both unconditional and text-to-image diffusion models, and comparison to state-of-the-art conditional image synthesis methods~\cite{rombach2022high,mou2023t2i,zhang2023adding}.

\section{Mathematical Illustration}
\label{section:mathematical derivation}

We start with the text-to-image conditional denoising process , where  represents the extracted text embeddings using a pre-trained CLIP~\cite{radford2021learning}. Given the external control signal as , it suffices to sample each noise subtracting process according to:

where  is a normalization weight to maintain the conditional score to have the same magnitude as other terms in the sampling score, written as .  is the diffusion model and  represents the proposed Condition Adapter (CA). Given an RGB image , we denote its corresponding latent representation as  in the following derivation. We refer to the derivation of \cite{dhariwal2021diffusion} and review the one of ours. Starting from that the pre-trained diffusion model \cite{rombach2022high} which estimates the previous timestep  from  using a Gaussian distribution:

Similarly, given the limit of the diffusion steps is infinite where , we could assume that  is low-curvatured compared to . One different thing of LCDG from \cite{dhariwal2021diffusion} is that we need to consider introducing the external condition  into this derivation process. To correlate the intermediate samples  with , we observe the difference map , where  denotes L2 norm. Therefore, we could approximate  using a Taylor expansion around  as:

where , corresponding to the  in Alg. 1. By submitting Eqn.~\ref{eqn2} and Eqn.~\ref{eqn3} into Eqn.~\ref{eqn1}, we have:

where , corresponding to the reported conclusion in Alg. 1. Note that all constants during the derivation could be safely ignored after computing the gradients. The process of LCDG is essentially sampling from a conditional Gaussian distribution, which could approximated by a mean-shifted Gaussian distribution correlated with the structural condition. 

Also, for DDIM sampling~\cite{ho2020denoising}, the process could be formulated as:

Note that we add the conditional term instead of the original subtraction, since we re-write the estimated score during DDIM sampling in the form of negative gradients of . Specifically, we implement LCDG together with classifier-free guidance~\cite{ho2022classifier}. Therefore, we write the sampling score at each step with~\cite{ho2022classifier} as:

where  is the sampling timestep where .  denotes the truncation threshold of TCS.  represents the text embeddings extracted by text encoders~\cite{radford2021learning,cherti2022reproducible}.











\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{network_architecture.jpg} \caption{Graphical illustration of the network architecture of CA. The composed blocks of CA are denoted as \texttt{CABlock}.}
  \label{figure:network architecture}
  \end{figure*}

\section{Implementation Details}
\label{section:implementation details}
\subsection{Network architecture}


Fig.~\ref{figure:network architecture} shows the detailed architecture design of CA. It is implemented with a series of \texttt{CABlock}, and a convolution layer at its end with kernel size of  for decreasing the channel size. Each \texttt{CABlock} has two-branch, which process timestep and the internal representations, respectively. For the timestep branch, the integer timestep is first implemented with a Positional Encoding (PE) same as those from the transformers~\cite{NIPS2017_3f5ee243}, and extracted into timestep embeddings. The timestep embeddings are then sent into a linear layer and a Sigmoid Linear Unit (SiLU) sequentially, following the implementations of the one in diffusion U-Net~\cite{rombach2022high}. On the other branch, the integrated representations pass through a simple architecture of \texttt{Conv-ReLU-BN}. The processed timestep embeddings are projected to the same shape as the intermediate features on the other branch. Then we finish the process of forwarding one block in CA by adding both of them. We implement the last layer of CA with solely a simple  convolution instead of \texttt{CABlock} to downscale the channel size.

\subsection{Data Preparation}
\label{subsection:data preparation}
As is mentioned in Sec. 4 in our main paper, we does not require annotated datasets to train the proposed LCDG, which indicates the flexibility of LCDG and its convenience to implement. We further illustrate the data preparation pipeline in details in this section. The collected dataset consists of 10,000 images. We use a pre-trained image captioning model~\cite{hu2022expansionnet} to automatically annotate the description for each image. We list details of the data preparation pipeline for each implemented condition as follows:
\begin{itemize}
\item \textit{Edge conditions}. We use a HED-like edge detector~\cite{he2020bdcn} to detect the edge maps from the source images. To simulate Canny edge~\cite{canny1986computational} during inference, we implement erosion operation upon the detected edge map in random degrees. To simulate user sketch, we implement dilation operation, random binarized threshold, random warping upon the detected edges. To detect Canny edge during inference, the binarize threholds are set as  and .
  \item \textit{Color stroke.} During training of CA, we select simulated color stroke as the supervision signal. We build our stroke simulation algorithm based on the one proposed in~\cite{meng2021sdedit}. In practice, we first process the source image using a median filter with kernel size of . Then, we implement a k-means clustering upon the grayscale values of the filtered image, \textit{i.e.}, color. For the centers of k-means clustering, we randomly sample  from  during each simulation, to simulate different types of used color during interfaces. Note that we do not implement specific design for image palette. But it would be reasonable for LCDG to be compatible with image palette condition, which works as stroke of square shape and guides the overall color distribution. For simulation of image palette during inference, we first downsample the source image to , then upsample it back to the original resolution using the same nearest interpolation.
  \item \textit{Mask conditions.} The binary mask condition is less investigated for the community of conditional image synthesis. Motivated by~\cite{huang2023composer}, we demonstrate some applications of practical usage with mask conditions in this paper. The mask condition provides a guidance of foreground position for diffusion sampling. Once precisely annotated, it also guides the contours of generated contents. The supervision signal of CA is simply the detected saliency mask using a pre-trained saliency detection model~\cite{qin2020u2}.
\end{itemize}

\subsection{Training Details}
We provide the training details of the implementation of the proposed Condition Adapter (CA). To extract the internal representations of the diffusion models, we choose outputs of different layers in the diffusion U-Net, which include the 2th, 4th, 8th layers of the encoder, the last outputs of the middle layers, and outputs of the 2th, 4th, 8th, 12th layer of the decoder. We use L2 norm between the reconstructed condition and external condition for optimization, which corresponds to Eqn. 2 in our main paper. Besides, we use Adam~\cite{DBLP:journals/corr/KingmaB14} optimizer with learning rate of . We train CA with 10,000 iterations, which requires approximately four hours of training on a single 3090 GPU. The batch size is set as .

\begin{table}[t]
    \centering
\caption{Hyperparameter details of the proposed LCDG under different conditions for the evaluation upon COCO 2017 validation set~\cite{lin2014microsoft}.}
\begin{tabular}{lcccc}
     \toprule
     &Ours (edge) &Ours (stroke)&Ours (palette)&Ours (mask)\\
     \midrule
     SD version    & 1.4 & 1.4 & 1.4 & 1.4 \\ 
     DDIM steps    & 50 & 50 & 50 & 50 \\
     DDIM eta    & 0.0 & 0.0 & 0.0 & 0.0 \\
     Unconditional guidance scale    & 6 & 6 & 6 & 6 \\
     Controlling scale    & 2 & 2.5 & 2.5 & 2 \\
     Truncation threshold    & 500 & 650 & 750 & 600 \\
\bottomrule
\end{tabular}
    \label{table:hyperparameter details of LCDG}
\end{table}

\begin{table}[t]
  \centering
  \small
  \caption{Hyperparameter details of other compared methods for the evaluation upon COCO 2017 validation set~\cite{lin2014microsoft}. We try to ensure all compared methods using the same diffusion backbone (SD v) for fair comparison. Note that ControlNet~\cite{zhang2023adding} only implements their methods based upon SD~\cite{rombach2022high} with version of .}
  \setlength{\tabcolsep}{0.65mm}
  \scalebox{0.72}{
  \begin{tabular}{lccccccc}
   \toprule
   &SD (text)~\cite{rombach2022high} &ControlNet (edge)&SD* (stroke)&SD* (palette)&T2I (edge)~\cite{mou2023t2i}&T2I (stroke)~\cite{mou2023t2i}&T2I (palette)~\cite{mou2023t2i}\\
   \midrule
   SD version     & 1.4 & 1.5 & 1.4 & 1.4 & 1.4 & 1.4 & 1.4 \\
   DDIM steps     & 50 & 50 & 50 & 50 & 50 & 50 & 50 \\
   DDIM eta     & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
   Unconditional guidance scale     & 7.5 & 9.0 & 7.5 & 7.5 & 7.5 & 9.0 & 9.0 \\
   Controlling scale (weight, strength)     & - & 1.0 & 0.8 & 0.8 & 1.0 & 1.0 & 1.0\\
      & - & - & - & - & 0.5 & 1.0 & 1.0 \\
\bottomrule
\end{tabular}
  }
  \label{table:hyperparameter details of others}
\end{table}

\subsection{Evaluation Details}
Tab.~\ref{table:hyperparameter details of LCDG} and Tab.~\ref{table:hyperparameter details of others} provide the hyperparameter details of LCDG and other compared methods during the evaluation on COCO 2017 validation set~\cite{lin2014microsoft}. We use the first caption of the official annotated captions as corresponding text prompt following~\cite{mou2023t2i}. For fair comparison, we set the DDIM sampling steps as  for all compared methods. For simulation of HED edge~\cite{xie2015holistically}, we use the HED detector~\cite{xie2015holistically} to detect edge maps from the source images. For stroke, palette, and mask conditions, we simulate these conditions the same as our introduction in Sec.~\ref{subsection:data preparation}. For hyperparameter settings of ControlNet~\cite{zhang2023adding} and T2I-Adapter~\cite{mou2023t2i}, we use the \textit{recommended} settings (unconditional guidance scale for~\cite{ho2022classifier}, controlling scale, and  for~\cite{mou2023t2i}) following their official instructions according to different conditions. For the implementations of SD* (SD using SDEdit~\cite{meng2021sdedit}), we also follow the official 
\textit{recommended} instructions of SD~\cite{rombach2022high} with a controlling strength of .

\section{Limitations}
\label{limitations}

We illustrate and discuss some of the limitations and working boundaries of LCDG in this section. One aspect is that LCDG would increase the sampling time during inference. Such limitation is shared by gradient-based methods~\cite{dhariwal2021diffusion,ho2022classifier} with additional forwarding processes of the diffusion U-Net. For LCDG during sampling, we require to pass the intermediate sample forward additionally to obtain the internal representations of the diffusion model. 



Tab.~\ref{table:sampling time} reports the sampling time compared with other methods. One could see that either early-constraint methods or the proposed late-constraint one would lead to certain degree of additional sampling time. This could be reasonable because introducing additional guidance into diffusion models require passing conditions through additional network components. Thus, all methods obtain slower sampling speed than text-to-image sampling of SD~\cite{rombach2022high}. ControlNet~\cite{zhang2023adding} obtains the slowest sampling time, where the model parameters of ControlNet is approximately half of the diffusion U-Net of SD~\cite{rombach2022high}. That is to say, at each sampling time, it requires to pass either the intermediate samples or the conditions through approximately  diffusion U-Net in total. T2I-Adapter~\cite{mou2023t2i} is the fastest of all, which requires passing through a additional condition-specific encoder at each sampling step. For LCDG, we need to pass  forward the diffusion U-Net and also the lightweight condition adapter at the manipulated steps using TCS.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{sampling_speed.jpg} \caption{Qualitative results of LCDG using different acceleration strategies, including ToMe~\cite{bolya2022token} and the proposed SSC strategy.}
  \label{figure:sampling speed}
  \end{figure*}

\begin{table}[h]
  \centering
\caption{Sampling time compared to other methods, including SD~\cite{rombach2022high}, ControlNet~\cite{zhang2023adding}, T2I-Adapter~\cite{mou2023t2i}. For LCDG, we assign the truncation threshold as  (maximum truncation magnitude of TCS). Note that the comparison is implemented with HED edge~\cite{xie2015holistically} as guidance on a single NVIDIA 3090 GPU.}
\begin{tabular}{lccccc}
   \toprule
   &SD (text)&ControlNet&T2I-Adapter&Ours\\
   \midrule 
   DDIM steps    & 50 & 50 & 50 & 50 \\
   Sampling time    & 5.31 & 14.69 & 6.44 & 14.11 \\
\bottomrule
\end{tabular}
  \label{table:sampling time}
\end{table}

\begin{table}[h]
  \centering
\caption{Sampling time with different truncation thresholds, and with different acceleration strategies using ToMe~\cite{bolya2022token} and SSC. DDIM steps is set as  and SD~\cite{rombach2022high} with version of  is used as the base model. The sampling time is tested under the HED edge~\cite{xie2015holistically} as guidance on a NVIDIA 3090 GPU.}
\begin{tabular}{lc}
   \toprule
   &Sampling Time\\
   \midrule 
       & 14.11 \\
       & 11.53 \\
       & 10.04 \\
       & 8.55 \\
       & 7.02 \\
   \midrule
   , using SSC    & 10.35 \\
   , using ToMe (ratio=)    & 11.21 \\
   , using ToMe (ratio=) and SSC    & 8.02 \\
\bottomrule
\end{tabular}
  \label{table:sampling time with acceleration}
\end{table}


We have also explored some possible solutions to alleviate such limitations. For LCDG, the main factor that determines the sampling time would be the truncation threshold in TCS, where the greater its value is, the less time we require to sample an image. To improve the sampling time, we implement ToMe~\cite{bolya2022token} and a Skip-Step Conditioning (SSC) strategy to speed-up the original sampling process of LCDG. As for SSC strategy, we only manipulate even-numbered DDIM sampling steps within the TCS interval. Tab.~\ref{table:sampling time with acceleration} reports the sampling time with different truncation thresholds and the improved ones using the proposed solution. We notice that the accelerating effect of ToMe~\cite{bolya2022token} would be more obvious while sampling images with higher resolution. Notably, we report the slowest sampling time in Tab.~\ref{table:sampling time} with the most truncation magnitude. For other conditions which requires less manipulated steps to achieve the best performance, \textit{e.g.}, image palette, it would be obviously faster than the reported one. Further, with the acceleration using ToMe~\cite{bolya2022token} and SSC, we could obtain comparable sampling speed (6.29s) with the fastest one~\cite{mou2023t2i} (6.71s). Fig.~\ref{figure:sampling speed} shows the corresponding qualitative results under the image palette condition, indicating that such acceleration strategy could produce satisfying results with acceptable details loss.


\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{training_data_domain_ablation.jpg} \caption{Qualitative results for the ablation studies of training data domains, implemented under HED edge~\cite{xie2015holistically} condition. We denote our reported experiment trained on self-collected data with synthetic labels from the internet as \texttt{Random Set} in this figure.}
  \label{figure:training data domains}
  \end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{training_iterations.jpg} \caption{Qualitative results for the ablation studies of training iterations, implemented under color stroke condition. We highlight the model trained with 10k iterations as the \textbf{optimal} result.}
    \label{figure:training iterations}
\end{figure*}

\section{Extended Ablation Studies}
\label{section:extended ablation studies}

\subsection{Training Data Domains}
We ablate the effects of training CA with different specific domains of data, instead of randomly collected ones. Besides, we also ablate the effects of training CA with only simple class label or even no text prompt. We randomly collect a series of cats and dogs images from ImageNet~\cite{deng2009imagenet}, which consists of 10,400 images of 8 categories in total (class labels are: \textit{``bernese mountain dog, french bull dog, old English sheep dog, maltese dog, siamese cat, tiger cat, egyptian cat, persian cat''}). We set the text prompts as ``dog'' for images with dogs, and ``cat'' for ones with cats. We also randomly collect 10,000 images from CelebA-HQ~\cite{karras2017progressive} as demonstration of the face domain. For the face domain, we simply use an empty string as text prompts.

Fig.~\ref{figure:training data domains} shows the qualitative results for the ablation studies. An interesting phenomenon is that training with solely face images even without text prompts could introduce the guidance of edge condition. However, the result is only approximately controlled with details loss, meanwhile revealing artifacts in the background. We regard this might be caused by the absence of text condition during the training of CA. The learned correlation fails to align with the text embeddings while sampling the text-to-image diffusion model, thus causing a inconsistency in the generated result. With simple class labels as text conditions, such phenomenon has be obviously alleviated with a better consistency with the text condition. The generated result embodies more detailed controls, \textit{e.g.}, the pocket on the bag, but still obtaining artifacts. The experiments indicate that training with text conditions is still vital to ensure the performance of LCDG upon text-to-image diffusion models, but also address the potentials of implementing LCDG with more easy-collected data without salient text labels.



\subsection{Training Iterations}
We further ablate the minimum training iterations of CA. Fig.~\ref{figure:training iterations} shows corresponding qualitative results. We demonstrate the ablation study under stroke condition for its complexity with three channels to learn than the binary conditions with only one. One could see that training with less than 10,000 iterations  would be insufficient, where the guided results reveal unstable textures and artifacts. This indicates that CA fails to learn a promising correlation between the internal representations of diffusion and color condition. From results trained with more than 10,000 iterations, we could see that the contents are stable, but the overall color is darker and monotonous. An representative phenomenon is that the color of the castle is getting black, and the river reveals similar color as the forest. In such circumstances, CA is over-fitting the limited color distribution of the small-scale training data, and fails to generalize to more vivid color during inference. We recommend training with \textit{10,000} iterations to obtain the best performance of LCDG.

\section{More Qualitative Results}
\label{section:more qualitative results}


We demonstrate more qualitative results in this section. Sec.~\ref{subsection:more results} demonstrates qualitative comparison and corresponding analysis with state-of-the-art conditional image synthesis solutions, also denoted as \textit{early-constraint} methods in this paper.

\subsection{More Conditional Text-to-Image Results with Unconditional Model}
Fig.~\ref{figure:edge-to-face} and Fig.~\ref{figure:color-to-face} report more results implemented under edge condition and color condition. Note that in each figure, we use the same model for evaluation.

\subsection{More Conditional Results with Text-to-Image Model}
\label{subsection:more results}
We provide more conditional text-to-image results compared with state-of-the-art competitors~\cite{rombach2022high,zhang2023adding,mou2023t2i,meng2021sdedit} under each implemented condition separately, including Canny edge~\cite{canny1986computational} (Fig.~\ref{figure:canny edge}), HED edge~\cite{xie2015holistically} (Fig.~\ref{figure:hed edge}), user sketch (Fig.~\ref{figure:user sketch}), color stroke (Fig.~\ref{figure:color stroke}), image palette (Fig.~\ref{figure:image palette}), segmentation mask (Fig.~\ref{figure:mask} left), and user scribble (Fig.~\ref{figure:mask} right). Notably, results in Fig.~\ref{figure:canny edge},~\ref{figure:hed edge},~\ref{figure:user sketch} share the same model. Results in Fig.~\ref{figure:color stroke}, ~\ref{figure:image palette} use the same model. And all results in Fig.~\ref{figure:mask} are inferenced with the same model, indicating the plausible generation ability of our method.

We analyze the inferenced results from the following aspects:
\begin{itemize}
  \item[1.] \textbf{Qualitative Comparison.} One could see that the proposed LCDG could obtain outperforming results over other competitors~\cite{rombach2022high,zhang2023adding,mou2023t2i,meng2021sdedit}. The generated results using LCDG are more consistent with the text condition, meanwhile following the guidance of external conditions. \textit{Early-constraint} methods, however, could sometimes be not consistent with the text condition due to the provided internal regularization, \textit{e.g.,} the generated result in Fig.~\ref{figure:canny edge} using ~\cite{mou2023t2i} fails to generate a church under the blue sky. In reverse, the \textit{late-constraint} manner of LCDG is compatible with the original text-to-image sampling, thus producing more consistent results.
  \item[2.] \textbf{Cross Evaluation of Early-Constraint Methods.} In the reported results, we also prove our observation that \textit{early-constraint} methods obtain poor generalization ability to other conditions. Since T2I-Adapter~\cite{mou2023t2i} only implement their method under sketch condition, the evaluated performance upon HED edge is less satisfying. For instance, the generated results could sometimes fail to produce consistent results with the text condition, \textit{e.g.}, the generated forest behind the blimp instead of the prompted ``sky''. For color condition, SD* is implemented for the stroke condition, and T2I-Adapter~\cite{mou2023t2i} is designed for color palette. For a cross evaluation, we also evaluate their performance on image palette and color stroke, respectively. For color stroke, results generated by~\cite{mou2023t2i} reveal severe color discrepancy. Also, SD*~\cite{rombach2022high} tends to produce blur and less detailed results due to the failure of generalizing to the palette guidance.
  \item[3.] \textbf{Structural Guidance Benefits Sample Quality.} We observe that most of the guided results using LCDG are more qualitatively satisfying than the text-driven ones generated by SD~\cite{rombach2022high}. Taking mask-guided results as examples, one could see that the manipulated results are in better quality in most cases. This corresponds to the analysis of Fig. 3 and Tab. 1 in our main paper, where the visualized structures of intermediate samples during text-to-image diffusion sampling are unstable and in disorderliness. Even with user scribble of a few strokes, the guided results are more plausible than the text-to-image ones, demonstrating the effectiveness of the proposed diffusion guidance in the \textit{late-constraint} manner.
\end{itemize}



\clearpage

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.9\textwidth]{edge_to_face.jpg} \caption{More results under edge condition working with SD~\cite{rombach2022high} trained on CelebA-HQ~\cite{karras2017progressive}, including Canny edge~\cite{canny1986computational}, HED edge~\cite{xie2015holistically}, and user sketch (from left to right). Note that the sketches are collected from the released free-hand sketches of ~\cite{yang2020deep}.}
\label{figure:edge-to-face}
\end{figure*}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.7\textwidth]{color_to_face.jpg} \caption{More results under color condition working with SD~\cite{rombach2022high} trained on CelebA-HQ~\cite{karras2017progressive}, including color stroke (left) and image palette (right).}
\label{figure:color-to-face}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{canny_edge.jpg} \caption{More results and comparison of Canny edge~\cite{canny1986computational} condition working with text-to-image SD~\cite{rombach2022high}, compared to ControlNet~\cite{zhang2023adding}, and T2I-Adapter~\cite{mou2023t2i}. We also evaluate the result of text-to-image SD~\cite{rombach2022high} for reference in this figure.}
\label{figure:canny edge}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{hed_edge.jpg} \caption{More results and comparison of HED edge~\cite{xie2015holistically} condition working with text-to-image SD~\cite{rombach2022high}, compared to ControlNet~\cite{zhang2023adding}, and T2I-Adapter~\cite{mou2023t2i}. We also evaluate the result of text-to-image SD~\cite{rombach2022high} for reference in this figure. Since T2I-Adapter~\cite{mou2023t2i} does not implement an individual model for HED edge condition, we use their released \texttt{sketch} model for evaluation.}
    \label{figure:hed edge}
  \end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\textwidth]{user_sketch.jpg} \caption{More results and comparison of user sketch condition working with text-to-image SD~\cite{rombach2022high}, compared to ControlNet~\cite{zhang2023adding}, and T2I-Adapter~\cite{mou2023t2i}. We also evaluate the result of text-to-image SD~\cite{rombach2022high} for reference in this figure. Note that the sketches are collected from Sketchy~\cite{sangkloy2016sketchy} dataset.}
    \label{figure:user sketch}
  \end{figure*}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{color_stroke.jpg} \caption{More results and comparison of color stroke condition working with text-to-image SD~\cite{rombach2022high}, compared to SD* (SD using SDEdit~\cite{meng2021sdedit}) and T2I-Adapter~\cite{mou2023t2i}. We also evaluate the result of text-to-image SD~\cite{rombach2022high} for reference in this figure. Since T2I-Adapter~\cite{mou2023t2i} does not implement an individual model for color stroke condition, we use their released \texttt{color} model for evaluation.}
  \label{figure:color stroke}
\end{figure*}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{image_palette.jpg} \caption{More results and comparison of image palette condition working with text-to-image SD~\cite{rombach2022high}, compared to SD* (SD using SDEdit~\cite{meng2021sdedit}) and T2I-Adapter~\cite{mou2023t2i}. We also evaluate the result of text-to-image SD~\cite{rombach2022high} for reference in this figure, and further evaluate SD*~\cite{rombach2022high} in this figure for a cross evaluation of its performance upon unseen condition, \textit{i.e.}, image palette.}
  \label{figure:image palette}
\end{figure*}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0\textwidth]{mask.jpg} \caption{More results of segmentation masks (left) and user scribble (right) working with text-to-image SD~\cite{rombach2022high}.}
  \label{figure:mask}
\end{figure*}


\clearpage





\end{document}
