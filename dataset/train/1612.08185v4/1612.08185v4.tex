\section{Experiments}\label{sec:experiments}

In this section we experimentally study the 
proposed \GRAY and \PYRAMID models on natural image modeling task
and report quantitative and qualitative evaluation results. 

\subsection{\GRAY} 

\textbf{Experimental setup.} We evaluate the modeling performance of 
a \GRAY on the CIFAR-10 dataset~\cite{krizhevsky2009learning}.
It consists of 60,000 natural images of size $32 \times 32$ belonging 
to 10 categories.
The dataset is split into two parts: a training set with 50,000 images 
and a test set with 10,000 images.
We augment the training data by random horizontal image flipping.

For setting up the architectures for modeling the distributions 
$p_{\hat\theta}(\widehat{X})$, $p_{\theta}(X|\widehat{X})$
and embedding $f_w(X)$ we use the same hyperparameters as in~\cite{salimans2016pixel}.
The only exception is that for parameterizing $p_{\theta}(X|\widehat{X})$ and $f_w(\widehat X)$
we use 24 residual blocks instead of 36.

In the Adam optimizer we use an initial learning rate of 0.001, a batch size of 
64 images and an exponential learning rate decay of 0.99999 that is applied after each iteration.
We train the grayscale model $p_{\hat\theta}(\widehat{X})$ for 30 epochs
and the conditional model $p_{\theta}(X|\widehat{X})$ for 200 epochs.

\textbf{Modeling performance.} The \GRAY achieves an upper bound 
on the negative log-likelihood score of \textbf{2.98} bits-per-dimension.
This is on par with current state-of-the art models, see Table~\ref{table:nll}.
Note, that since we measure an upper bound, the actual model performance might 
be slightly better. However, in light of our and other experiments, we believe 
small differences in this score to be of minor importance, as the log-likelihood 
does not seem to correlate well with visual quality in this regime. 

In Figure~\ref{fig:samples} we present random samples produced 
by the \GRAY model, demonstrating grayscale samples from 
$p_{\hat\theta}(\widehat{X})$ and resulting colored samples from $p_{\theta}(X|\widehat{X})$.
We observe that the produced samples are highly diverse, and, unlike samples 
from previously proposed probabilistic autoregressive models, often exhibit a 
strongly coherent global structure, resembling highly complex objects, such as 
cars, dogs, horses, \etc.

Given the high quality of the samples, one might be worried if possibly 
the grayscale model, $p_{\hat\theta}(\widehat X)$, had overfit the training data. 
We observe that training and test loss of $p_{\hat\theta}(\widehat{X})$ are 
very close to each other, namely 0.442 and 0.459 of bits-per-dimension, 
which speaks against significant overfitting. 
Note also, that it is not clear if an overfitted model would automatically produce good
samples.
For instance, as reported in~\cite{salimans2016pixel}, severe overfitting of the PixelCNN++
model does not lead to a high perceptual quality of sampled images.


\textbf{Discussion.} By explicitly emphasizing the modeling of high-level 
image structures in the \GRAY, we achieve significantly better visual 
quality of the produced samples.
Additionally, the \GRAY offers interesting insights into the image modeling task. 
As the objective we minimize for training is a sum of two scores, we can individually examine 
the performance of auxiliary variable model $p(\widehat{X})$ and the conditional model $p(X|\widehat{X})$.
The trained conditional model achieves $p(X|\widehat{X})$ a negative log-likelihood score of \textbf{2.52} 
bits-per-dimension, while $p(\widehat{X})$ achieves a score of \textbf{0.459} bits-per-dimension on the 
CIFAR-10 test set. In other words: high-level image properties, despite being harder to model for the 
network, contribute only a small fraction of the uncertainty score to the the overall log-likelihood. 
We take this as indication that if low-level and high-level image details are modeled by one 
model, then at training time low-level image uncertainty will dominate the training objective.
We believe that this is this the key reason why previously proposed PixelCNN models failed
to produce globally coherent samples of natural images.
Importantly, this problem does not appear in the \GRAY, because global and local image models
do not share parameters and, thus, do not interfere with each other at training phase.

An alternative explanation for the differences in log-likelihood scores would be 
that PixelCNN-type models are actually not very good at modeling low-level image input. 
Additional experiments that we performed show that this is not the case: 
we applied the learned conditional model $p(X|\widehat{X})$ to 4-bit grayscale images
obtained by quantizing real images of the CIFAR-10 test set. Figure~\ref{fig:coloring} 
compares the resulting colorized samples with the corresponding original images,
showing that the samples produced by our conditional model are of visual quality comparable
to the original images.
This suggests that in order to produce even better image samples, 
mainly improved models for $p_{\hat\theta}(\widehat{X})$ are required. 

\begin{figure*}
    \center
	\includegraphics[width=1.0\textwidth,trim={0 1.4cm 4.04cm 0},clip]{images/sample-gray.jpg}

  \vspace{1mm}

  \includegraphics[width=1.0\textwidth,trim={0 1.4cm 4.04cm 0},clip]{images/sample-my.jpg}
  \caption{Random quantized grayscale samples from $p(\widehat{X})$ \textit{(top)} and
           corresponding image samples from $p(X|\widehat{X})$ \textit{(bottom)}. The 
           grayscale samples show several recognizable objects, which are subsequently
           also present in the color version.}
    \label{fig:samples}
\end{figure*}

\begin{figure*}
    \center
	\includegraphics[width=0.25\textwidth]{images/sample-color-cifar.jpg}
    \quad
	\includegraphics[width=0.25\textwidth]{images/sample-gray-cifar.jpg}
    \quad
	\includegraphics[width=0.25\textwidth]{images/sample-colorized.jpg}
    \caption{CIFAR-10 images in original color \emph{(left)} and quantized to 4-bit grayscale \emph{(center)}.
     Images sampled from our conditional model $p(X|\widehat{X})$, using the grayscale CIFAR images as auxiliary 
     variables \emph{(right)}. The images produced by our model are visually as plausible as the original ones.}
    \label{fig:coloring}
\end{figure*}



\begin{table}[t]
        \center
	\resizebox{0.5\textwidth}{!}{
        \begin{tabular}{|lc|}
        \hline
	Model & Bits per dim. \\
        \hline
      Deep Diffusion \cite{sohl2015deep} & $\le$ 5.40 \\
                NICE \cite{dinh2014nice} & 4.48 \\
                DRAW \cite{gregor2015draw} & $\le$ 4.13 \\
           Deep GMMs \cite{van2014factoring} & 4.00 \\
           Conv Draw \cite{gregor2016towards} & $\le$ 3.58 \\
            Real NVP \cite{dinh2016density} & 3.49 \\
  Matnet + AR \cite{bachman2016architecture} & $\le$ 3.24 \\
            PixelCNN \cite{oord2016pixel} & 3.14 \\
        VAE with IAF \cite{kingma2016improving} & $\le$ 3.11 \\
      Gated PixelCNN \cite{van2016conditional} & 3.03 \\
            PixelRNN \cite{oord2016pixel} & 3.00 \\
  \textbf{\GRAY} (this paper) & $\le$ 2.98 \\
 DenseNet VLAE \cite{chen2016variational} & $\le$ 2.95 \\
	PixelCNN++ \cite{salimans2016pixel} & 2.92 \\
        \hline
        \end{tabular}
	}
    \caption{The negative log-likelihood of the different models for the CIFAR-10 \textbf{test set}
             measured as bits-per-dimension.}
	\label{table:nll}
\end{table}

\subsection{\PYRAMID.} 

\textbf{Experimental setup.} We evaluate the \PYRAMID on 
the task of modeling face images. We rely on the \emph{aligned$\&$cropped CelebA} 
dataset~\cite{liu2015faceattributes} that contains approximately 
200,000 images of size 218x178.
In order to focus on human faces and not background, we preprocess all images in the dataset by applying a 
fixed 128x128 crop (left margin: 25 pixels, right margin: 
25 pixel, top margin: 50, bottom margin: 40 pixels).
We use a random 95\% subset of all images as training set 
and the remaining images as a test set.


For the \PYRAMID we apply the auxiliary variable decomposition 4 times.
This results in a sequences of probabilistic models, where the first model generates 
faces in 8$\times$8 resolution. We use a PixelCNN++ architecture without down or up-sampling layers with 
only 3 residual blocks to model the distributions $p_{\theta}(\widehat{X})$ and $p_{\hat{\theta}}(X|\widehat{X})$
for all scales.
For the embedding $f_w(\widehat{X})$ we use a PixelCNN++ architecture with
15 residual blocks with downsampling layer after the residual block 
number 3 and upsampling layers after the residual blocks number 9 and 12.
For all convolutional layers we set the number of filters to 100.

In the Adam optimizer we use an initial learning rate 0.001, a batch 
size of 16 and a learning rate decay of 0.999995.
We train the model for 60 epochs.

\begin{table}[t]
	\center
        \begin{tabular}{|l||c|c|c|c|c|}
        \hline
		Res. & 8$\times$8 & 16$\times$16 & 32$\times$32  & 64$\times$64 & 128$\times$128 \\
		Bpd  & 4.58 & 4.30 & 3.33 & 2.61 & 1.52 \\
	\hline
	\end{tabular}
	\caption{Bits-per-dimension (\emph{Bpd}) achieved by \PYRAMID on the test images
	         from the \emph{CelebA} dataset for various resolutions (\emph{Res.}).}
	\label{table:CelebA}
\end{table}

\textbf{Modeling performance.}
We present a quantitative evaluation of \PYRAMID in Table~\ref{table:CelebA}.
We evaluate the performance of our model on different output resolutions,
observing that bits-per-dimension score is smaller for higher resolutions,
as pixel values are more correlated and, thus, easier to predict.
As an additional check, we also train and evaluate \PYRAMID model on
the CIFAR dataset, achieving a competitive score of 3.32 bits-per-dimension.

Before demonstrating and discussing face samples produced by our model 
we make an observation regarding the PixelCNN's sampling procedure.
Recall that the output of the \PYRAMID is a mixture of logistic 
distributions.
We observe an intriguing effect related to the mixture representation 
of the predicted pixel distributions for face images:
the perceptual quality of sampled faces substantially increases if 
we artificially reduce the predicted variance of the mixture components.
We illustrate this effect in Figure~\ref{fig:face-variance}, where 
we alter the variance by subtracting constants from a fixed set 
of $\{0.0, 0.1, \dots, 1.0\}$ from the predicted log-variance of 
the mixture components.
Inspired by this observation we propose an alternative sampling procedure: 
for each pixel, we randomly sample one of the logistic components based 
on their weight in the predicted mixture. Then, we use the mode of this 
component as sampled pixel value, instead of performing a second random 
sampling step.
This sampling procedure can be seen as a hybrid of 
probabilistic sampling and maximum a posteriori (MAP) prediction. 

Figure~\ref{fig:face-samples} shows further samples obtained by such 
\emph{\SAMPLING}. 
The produced images have very high perceptual quality, with some generated 
faces appearing almost photo-realistic.
The complete multi-scale sampling mechanism of the \PYRAMID, from 
8$\times$8 to 128$\times$128 images, is demonstrated in Figure~\ref{fig:face-upsampling}. 

\textbf{Discussion.} First, we observe that, despite the very high 
resolution of modeled images, the produced samples capture global human 
face characteristics, such as arrangement of face elements and global 
symmetries. At the same time, the set of samples is diverse, containing 
male as well as female faces, different hair and skin colors as well 
as facial expressions and head poses. 
Second, we emphasize that by properly decomposing the model we are 
able to scale the \PYRAMID to produce samples with very high 
resolution of 128x128.
As discussed previously in Section~\ref{sec:method}, this results from 
the fact that our decomposition allows to parametrize autoregressive 
parts of the image model by a light-weight architecture.
Concretely, on an NVidia TitanX GPU, our \PYRAMID without 
caching optimizations requires approximately \textbf{0.004} seconds 
on average to generate one image pixel, while a PixelCNN++ even with 
recently suggested caching optimizations requires roughly \textbf{0.05} 
seconds for the same task.
If we add caching optimizations to our model, we expect its speed to
improve even further.

\begin{figure*}
    \center
	\includegraphics[width=.98\textwidth]{images/sample-face-variance.jpg}
	\caption{Effect of the variance reduction. Numbers on top of each column indicates the amount of reduction
             in the predicted log-variance of the mixture components. The last column corresponds to \SAMPLING.}
    \label{fig:face-variance}
\end{figure*}

\begin{figure*}
    \center
	\includegraphics[width=1.0\textwidth]{images/sample-faces.jpg}
	\caption{Images sampled from the \PYRAMID by \SAMPLING. The generated faces are of very high quality,
    many being close to photorealistic. At the same time, the set of sample is diverse in terms of the depicted 
    gender, skin color and head pose.}
    \label{fig:face-samples}
\end{figure*}

\begin{figure*}
    \center
	\includegraphics[width=1.0\textwidth]{images/sample-upsample-faces.jpg}
	\caption{Visualization of the \PYRAMID sampling process. Faces are first generated on a small, 8x8, resolution
             and then are gradually upsampled until reaching the desired 128x128 resolution.}
    \label{fig:face-upsampling}
\end{figure*}
