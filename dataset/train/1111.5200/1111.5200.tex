\documentclass[11pt]{amsart}
\usepackage{fullpage}
\usepackage{mydefs}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{amsmath}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{epstopdf}
\usepackage{mydefs}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{url}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\def\calS{{\mathcal S}}   
\def\calL{{\mathcal L}}   
\def\calP{{\mathcal P}}
\def\calQ{{\mathcal Q}}      

\title{Wireless Capacity and Admission Control in Cognitive Radio}


\author[M. M. Halld\'orsson]{Magn\'us M. Halld\'orsson}
\address[M. M. Halld\'orsson]{School of Computer Science\\
 Reykjavik University\\
 101 Reykjavik, Iceland\\}
\email{mmh@ru.is}

\author[P. Mitra]{Pradipta Mitra}
\address[P. Mitra]{School of Computer Science\\
Reykjavik University\\
Reykjavik 101, Iceland}
\email{ppmitra@gmail.com}



\begin{document}


\begin{abstract}
We give algorithms with constant-factor performance guarantees for
several capacity and throughput problems in the SINR model.  The
algorithms are all based on a novel LP formulation for capacity
problems.  First, we give a new constant-factor approximation
algorithm for selecting the maximum subset of links that can be
scheduled simultaneously, under any non-decreasing and sublinear power assignment.
For the case of uniform power, we extend this to the case of variable QoS requirements and link-dependent
noise terms. 
Second, we approximate a problem related to cognitive radio: find a maximum set of links that can be simultaneously scheduled without affecting a given set of previously assigned links.
Finally, we obtain constant-factor approximation of weighted capacity under linear power assignment.

\iffalse
We give a constant-factor approximation algorithm for 
We formulate the wireless capacity problem in the SINR model as a Linear program. We show how to use it
to get an alternative algorithm to achieve a constant approximation to wireless capacity for all non-decreasing,
sub-linear power assignments.

We show the effectiveness of our approach by finding improved algorithms for a number of variations
of the capacity problem, including: For uniform power, capacity with variable QoS and noise requirements,
for uniform and bi-directional mean power admission control and cognitive radio, and for linear power
the weighted capacity problem.
\fi
\end{abstract}



\maketitle 
\section{Introduction}
How much communication can be active simultaneously in a given wireless network?
This is a topic of major research effort. We address this question in
a more generalized setting than previously considered, and give
efficient algorithms that achieve good performance guarantees based on
a novel mathematical programming formulation. 

In the \emph{capacity} problem in wireless networks, we are given a
set of communication links in a metric space, each consisting of
a sender-receiver pair, and we seek to find the largest subset of links
that can transmit simultaneously within the model of interference.  We
adopt the SINR model of interference where transmission over a link
succeeds if the received signal at the receiver is sufficiently large,
compared to ambient noise and interference from other transmissions.
This model has emerged as a superior model for wireless interference
patterns, as it is both analytically manageable, and reasonably
realistic, especially in comparison to graph based
models~\cite{GronkMibiHoc01,MaheshwariJD08,Moscibroda2006Protocol}.
We assume that the powers have been pre-assigned to the links, based
only on the length of the links. Having such simple assignments can be of great benefit
in a distributed context.

The basic capacity problem has been addressed in numerous recent
works. Constant-factor approximation algorithms have been given for
uniform power \cite{GHWW09} and more generally for any non-decreasing
sub-linear power assignment (see Sect.~2 for definitions) \cite{SODA11}, and for arbitrary power 
\cite{KesselheimSoda11}. 
These results assume a uniformity of the links, both in their signal
characteristics as well as their value. They also assume that no other
wireless activity is affecting these transmissions. 
We aim to handle more general scenarios, allowing for heterogeneity in
link characteristics and environment.
In particular, we address three extensions:
\begin{enumerate}
 \item (QoS) Each link has its own signal requirements and its own
   ambient noise term.
 \item (Weights) Each link has an associated weight, and the
   objective is to maximize the total weight of the satisfied links.
 \item (Admission control) Certain communication is already taking
   place, which cannot be interfered with (possibly for regulatory reasons).
\end{enumerate}
We discuss each of these extensions further.

\iffalse
We consider the fundamental \emph{capacity} problem in wireless networks.
Given is a set of communication links , where a link  is a directed pair of a ``sender''  and a ``receiver'' . Senders and receivers are points in a metric space. Each link represents a intended wireless communication from the sender to the receiver.
The \emph{capacity} of this network is the size of the largest subset of  that
can successfully transmit simultaneously, given some model of interference among simultaneously transmitting links.
\fi

\subsection*{Quality-of-Service requirements}

The SINR achieved at a particular link determines the data-rate achieved at this link, or the quality of service (QoS). Different links may have a minimum acceptable QoS requirements, for example if one link is used for video transmission and another for data transmission. In addition, the noise level at receivers may not be the same across the network. This practically motivated version of the capacity problem has not been handled in much of the previous research \cite{GHWW09,HW09,SODA11}.
We tackle this problem, both as an interesting problem in its own right, and as a stepping stone for the following problem.


\subsection*{Cognitive radio and admission control}
Given are two sets of links  and . The goal is to find   such that  can transmit simultaneously and the size of  is maximized.  We refer to this as the \emph{admission control} problem.

This problem naturally arises in at least two application areas. The first is the so-called ``cognitive radio'', which has been the object of intense study of late (see \cite{Bahl:2009:WSN:1592568.1592573,SuZhangCognitive,Levorato:2009:CIM:1793974.1793991} and their \emph{many} references). This area has gained great salience due to recent 
regulatory changes in wireless bandwidth management. Though the exact technological scenario for cognitive radios is still being figured out, the essential point is as follows: a wireless channel is allocated to a ``primary user''; however, one would like to accommodate more users in the channel, as long as the primary user remains feasible. 
This clearly is an instance of the above mentioned problem, with   being typically small (in fact, perhaps, just ).

However, there is a more ``classical'' source of the same problem, referred to as admission control or \emph{access control} \cite{GoldsmithSurvey,WuBertsekas}, sometimes referred to as ``active link protection'' \cite{ChiangSurvey}. The capacity problem, is its basic form, captures a scenario where each slot is independent of previous slots. In practice however, links can require sustained communication (and different links for different periods of time). Thus, in certain applications, a more realistic model is to maximize capacity under the constraint that older links still communicating not be disturbed. This again, is exactly the problem defined above (but perhaps with a typically larger ). Though heuristic approaches to this problem abound, we are unaware of rigorous algorithmic
results in the SINR model.

\subsection*{Weighted capacity}
In this problem each link  is associated with a non-negative weight  and the goal is to find a feasible
set  so as to maximize .

This \emph{weighted capacity} problem is a natural extension of the capacity problem, and a case can
be made for theoretical investigation for this reason alone.
As it happens, though, the problem is further motivated by questions about \emph{stability} in queuing theory. 
In this setting, packets arrive at network nodes according to some stochastic process, and the problem is
to characterize the set of arrival rates under which the network can be stabilized, i.e., the network queues
remain bounded. In the case of 
wireless networking stability, the seminal work of Tassiulas and Ephremides \cite{TE92} 
established
the existence policy that
stabilizes the system under all arrival rates for which stability is potentially possible. 
This policy can be seen to be equivalent to solving the maximum
weighted capacity problem in the SINR model. 

\subsection*{Solution method}

It is easy to verify whether a given set of links is feasible. 
In fact, an appropriate power assignment that makes it feasible can be found efficiently. Namely, Eqn. \ref{gen_sinr} can be cast as a linear program with 's as variables, 
which can thus be solved optimally. Indeed, there is a large body of work where one starts with a feasible set and then tries to optimize over some other criteria, say to minimize the power consumed \cite{ChiangSurvey}. 

Naturally, one doesn't expect this approach to work directly for the capacity problem introduced before, which is  ``combinatorial'' and in fact happens to be NP-hard \cite{Goussevskaia2008Complexity}.
What is perhaps more surprising is that the capacity problem does not appear to easily admit a linear
programming relaxation either, even for simple cases. Most algorithms developed for the capacity problem have thus been very simple greedy algorithms \cite{GHWW09,SODA11,KesselheimSoda11}, with some exceptions \cite{hoeferspaa,CKMPS08}.

In this work, starting from a simple observation, we develop an integer program that approximates the capacity problem for a large class of oblivious power assignments.
We then show how to round the corresponding linear programming relaxation to get a constant factor approximation.
Thus we recover the main result of \cite{SODA11} but via linear programming as opposed to a greedy algorithm. 
We also show that the LP formulation can be easily modified to tackle a class of important problems where
greedy algorithms do not appear to work very well, including the
problems discussed above.

\section{Preliminaries and results}
\label{sec:model}

The capacity problem in the SINR model is defined as follows.
We are given a set  of  links, 
each consisting of a sender and receiver pair , 
which are points in a metric space with a distance metric .
The asymmetric distance from link  to link  is the distance from
's sender to 's receiver, denoted .
Each link  has been assigned transmission power . 
A link  succeeds if

where  is the ambient \emph{noise},  is the required SINR level,
 is the \emph{path loss} constant, and 
  is the set of concurrent transmissions. A set  is \emph{feasible} if the above constraint holds for all . Thus the capacity problem is equivalent to finding
the feasible subset  of maximum size.

Let  denote the length of link .
Let  denote the ratio between the maximum and minimum length of a link.
A power assignment  is \emph{non-decreasing} if  whenever
 and \emph{sub-linear} if  whenever . We will restrict our attention to this class or particular assignments belonging to this class. 
Note that this class essentially contains all ``natural'' length based assignments, and specifically all
 well studied length based power assignments. These include the \emph{uniform} power assignment, where all links use the same power, \emph{linear} power assignment where  (which is thought to be energy efficient), and \emph{mean power} assignment where  which is known to be 
essentially the ``best'' length-based assignment as far as capacity is concerned \cite{us:esa09full,SODA11}.

\emph{Affectance. }
We will use the notion of \emph{affectance}, introduced in
\cite{GHWW09,HW09} and refined in \cite{KV10} to the thresholded form
used here, which has a number of
technical advantages.  
The affectance  \emph{on} link  \emph{from} another link ,
with a given power assignment ,
is the interference of  on  relative to the power
received, or

where  is a constant
depending only on the parameters of the link . 

We will drop  when clear from context. 
Let .
For a set  of links and a link , 
let  and .
For sets  and , .
Using such notation, Eqn.~\ref{gen_sinr} can be rewritten as follows, which we will adopt:

In the variable QoS version of the capacity problem,  and  are no longer constants, but can be different for different links.
Note that the definition of affectance stays the same apart from a changed definition of  where  and  are respectively the signal requirement and noise level for .

For all problems that we consider, we will  to mean the optimal solution, which will apply to the problem being discussed at that point.

\subsection*{Our results}
We prove the following results.
\begin{theorem}
For length monotone, sub-linear power assignments, there is constant-approximation algorithm for the wireless capacity problem. For uniform power, there is a constant-approximation algorithm in the QoS generalization.
\label{mainth1}
\end{theorem}
The first part (not involving QoS) is the same as the main result proven in \cite{SODA11}, but via a linear programming relaxation.

\begin{theorem}
For the admission control problem with uniform power,
\begin{itemize}
\item[a)] There is a  approximation algorithm. 
\item[b)] If the optimum solution  (for some constant ), there is a constant-approximation algorithm.
\end{itemize}
\label{mainth2}
\end{theorem}
Specifically, for the ``cognitive radio'' case of the problem where  (or small, at any rate) we get a constant factor
approximation for uniform power. There is no straight-forward greedy algorithm to tackle this problem.
We believe that a greedy algorithm for the variable QoS problem is possible, but even if this is true,
the resultant version for admission control would result in approximation factor worse than our results by a  factor. Additionally, we see no way of utilizing the  condition in the greedy algorithm.

Finally,
\begin{theorem}
For linear power, there is  constant-approximation algorithm for weighted capacity problem.
\label{mainth3}
\end{theorem}
For this problem, greedy algorithms combined with some basic observations can yield  a -approximation algorithm (we describe this algorithm in detail in Section \ref{sec:simul} when we experimentally compare it with our LP based algorithm). 

We remark that our results holds in arbitrary metric space, independent of the path loss constant , and faithfully treat the ambient noise term.

\subsection*{Related Work.}

The first work to study capacity of randomly deployed networks was the work of Gupta and Kumar~\cite{kumar00}. 
Rigorous worst case algorithmic analysis started with
the  work of Moscibroda and Wattenhofer \cite{MoWa06}, who
studied of the \emph{scheduling complexity} of arbitrary set
of wireless links. 
Early work on approximation algorithms
 produced approximation
factors that grew with structural properties of the network \cite{moscibroda06b,MoscibrodaOW07,chafekar07}.


The first constant factor approximation algorithm was obtained for
capacity problem for uniform power in \cite{GHWW09} (see also
\cite{HW09}) in  with .
Fangh\"anel, Kesselheim and V\"ocking \cite{FKV09} gave an algorithm
that uses at most  slots for the scheduling problem
with linear power assignment ,
that holds in general distance metrics.

Recently, Kesselheim obtained 
a constant-approximation algorithm
for the capacity problem with power control for doubling metrics
and  for general metrics \cite{KesselheimSoda11}. In another work \cite{SODA11}, constant
factor approximation was achieved for all non-decreasing, sub-linear power assignments.
The greedy algorithms of \cite{GHWW09,HW09,SODA11} can be modified to handle the problems
we address here, and these algorithms essentially constitute the previous best results on these 
problems.

As far as we can ascertain, the algorithmic situation for the admission control and weighted capacity
is somewhat similar to the situation the ``basic'' capacity problem was in before the array of results
mentioned above. Thus, we have a large body of works and results in different settings motivating
the study of these questions, but no worst case algorithmic results.

The  works on the emergent field of cognitive radio are too numerous to adequately cover. We refer the reader to  \cite{Bahl:2009:WSN:1592568.1592573} for a  thorough discussion. For results on capacity of networks
in a cognitive radio context see \cite{jafarjournal,shiicccn} etc. (``capacity'' not necessarily meaning the exact same thing we do). For the stability problem in a queuing theory setting
that gives rise to the weighted capacity problem there are many works in graph based models \cite{DBLP:conf/infocom/SharmaMS06,bestInfocom08,DBLP:conf/mobihoc/LiBX09} as well as recent
ones on the SINR model \cite{lqfmobihoc}.  The weighted capacity problem was recently studied in \cite{wanwireless}, where the authors propose a version of the greedy-based  approximation. 

In terms of using a LP approach in the SINR setting, there is recent work of Hoefer \emph{et. al.} \cite{hoeferspaa}, using 
related insights in their formulation. In the context of throughput maximization, \cite{CKMPS08} employed a linear programming solution. Being based on unit disc graphs, that approach  does not lead to performance bounds we seek here.

\section{The basic capacity problem}

Let us first consider how one would attempt to write an Integer
program (and a subsequent Linear programming relaxation) for the
capacity problem. If the variable  denotes that
link  was selected in the solution, we see that for selected
links, the condition  would
have to hold. This is quite nice and linear, except for the fact that
we would have to somehow indicate that this condition need only hold
for , and that no condition need hold for
links in . There appears no way to do this
in a linear program. 


For clarity, we will first present our linear program (and the whole algorithm) below, and then in proving its correctness, we will
describe how out algorithm evades the problem elucidated above.

\subsection{Algorithm}
Our algorithm has three main steps:

\begin{itemize}
\item {\bf Linear Program}
The first step of our algorithm is to solve the following linear program, with variables , one corresponding to 
every link .
Let  be a large enough constant.


\item {\bf Rounding}
We then ``round" the fractional solution to this linear program in two steps.

Let  be the value of the (fractional) solution to .

First, we select a set , defined by binary variables , which are generated independently at random such that  (and thus ). 

Next, we choose a subset of  named  defined by ,
where  is a binary random variable
corresponding to this second round of selection.
The variable  is defined as follows:
 iff  and the following two conditions hold:


\item {\bf Final Selection}
Finally, a feasible set is extracted from  using a simple \emph{signal-strengthening} technique which we will detail later.
\end{itemize}

\subsection{Analysis}
We need the following definitions.
\begin{defn}
A link set  is \emph{-feasible} (resp., \emph{-anti-feasible}), if  for all  (resp. if  for all ). A link set is \emph{-bi-feasible} if it is both -feasible and -anti-feasible.
\end{defn}
We will simply write ``feasible'', ``anti-feasible'' and ``bi-feasible" when .



Our first step is to show that the solution to the linear program is an approximation to the 
capacity problem, or more formally:
\begin{lemma}
Let  be the value of the optimal solution of . Then, .
\label{lpopt}
\end{lemma}
\begin{proof}
To prove this, it suffices to construct a solution  (for all ) to the  such that
, and that satisfies all the constraints in the linear program.

Since  is feasible, there is a 2-bi-feasible subset  such that  (See \cite{icalp11} for a simple proof of this
fact).

Now construct the solution by setting
 if  and  otherwise. Thus .  Lemma \ref{lpopt}
can be proven then if we can show that Conditions  \ref{lpcond1} and \ref{lpcond2} hold for this solution, and thus form a valid
solution to . These follow directly from two Lemmas noted below (Lemmas \ref{cl2} and \ref{cl3}), by setting
 to be larger than the implicit constants in those two Lemmas.
\end{proof}


The following Lemma was proven in \cite{KV10}. For completeness, we give a proof in the
appendix that holds for arbitrary ambient noise.
\begin{lemma}
Assume  is -feasible using a non-decreasing, sub-linear power assignment. Let  be any link such that  for all . Then .
\label{cl2}
\end{lemma}

The next Lemma, something of a dual of the previous one, was proven recently in \cite{icalp11}:
\begin{lemma}
Assume  is -anti-feasible using a non-decreasing, sub-linear power assignment. Let  be any link such that  for all . Then .
\label{cl3}
\end{lemma}

{\bf Remarks:} Lemmas \ref{cl2} and \ref{cl3} hold the crucial insight that allow us to circumvent the problem mentioned at the beginning
of this section. Note how these lemmas bound the affectance to and from a link  without the condition that  
be a part of the feasible (or anti-feasible) set . This allows us to evade the issue of having to express conditions
that only apply for links in the solution set. Instead we can write constraints (Equations
\ref{lpcond1} and \ref{lpcond2}) which apply to \emph{all} links.


The next step is to analyze the {\bf Rounding} phase. In particular, we claim that 
\begin{lemma}
.
\end{lemma}
\begin{proof}
Recall that . Then
by linearity of expectation, 


where we use .

Let  denote the indicator random variable of the event that both Cond.\ 
\ref{rndcond1} and \ref{rndcond2} are fulfilled for link . Then  .
The point to note here is that the events  and  are independent since
the random variable  is not involved in the former (because ). 

We will prove below that  (Lemma \ref{lem:rhobound}).

Thus, . Now continuing with Eqn. \ref{eqExS1},  .

\end{proof}


As promised, we lower bound :
\begin{lemma}

\label{lem:rhobound}
\end{lemma}
\begin{proof}
By  Eqn.~\ref{lpcond1}, it holds that
.
Thus by Markov inequality, the probability that Cond.\ \ref{rndcond1} 
fails is at most .
The same applies to Cond.\ \ref{rndcond2}, using Eqn.~\ref{lpcond2}.
The Lemma then follows by the union bound. 
\end{proof}


Finally, we need to show that we can extract a large feasible subset from  in the {\bf Final Selection} phase. 
The following \emph{signal-strengthening} lemma from \cite{HW09}
will be frequently useful.
\begin{lemma}{[Thm. 1 of \cite{HW09}, slightly restated]}
If  is an -feasible set, 
then  can partitioned in to  -feasible sets,
for any .
\label{lem:signal}
\end{lemma}

\begin{lemma}
There is an efficient algorithm to find a feasible set  such that .
\end{lemma}
\begin{proof}
By conditions \ref{rndcond1} and \ref{rndcond2},  the average
affectance in the selected set  is

Define . From the above bound on average affectance, it is easy to see that .
Finally, by the signal strengthening lemma, we can find a feasible set  such that
.
 \end{proof}
The first part of Thm.~\ref{mainth1} now clearly follows. The part of Thm.~\ref{mainth1} about uniform power in the variable QoS case will be handled in the next section.

The algorithms in the following two sections will follow the same tri-partite design of LP, Rounding and Final Selection. Due to space
constraints, we will mostly focus on the changes in the LP formulation, and when appropriate, the changes in the Rounding phase, without
proving everything from scratch.


\section{Cognitive radio/Admission control}

\subsection*{Variable noise and signal requirements (QoS)}

Recall that in this variation of the problem each link  has a separate QoS  and noise level  and definition of affectance
changes accordingly.
If a link set is such that  for all  for some unspecified constant , we call the 
power assignment \emph{nearly uniform}. The following holds.
\begin{lemma}
Assume  is anti-feasible and  is some link. Assume that all
links use a nearly uniform power assignment. Then  .
\label{cl2uni}
\end{lemma}
The proof is a standard modification of the same result for uniform
power with constant  and  (see, for example, Lemma 11 of
\cite{infocom11}).  Our proof of Lemma \ref{cl2} provided in the appendix gives a general idea
of this type of proof, and we mention after that proof the main changes needed to achieve Lemma \ref{cl2uni}.

The following modified LP can be used for uniform power capacity in this setting:


The additional steps after solving the LP and the analysis follows the same lines as Thm.~\ref{mainth1} (which we omit due to space constraints).


\subsection*{Admission control}

Now we can focus on the admission control problem for which we will use some ideas
from the variable QoS case.

We will prove the following more general result first.
\begin{theorem}
Assume links in  use a nearly uniform power assignment. Assume that links in  use some
arbitrary power assignment. Then we can approximate the admission control problem up to a  factor of .
\label{admingen1}
\end{theorem}

\begin{proof}
Recall that the goal is to find  of maximum size
such that  is feasible. Thus in choosing , we
have to be careful about the affectance of  on  and
vice-versa. Our approach is to handle the affectance from  as
noise. In this regime, the new ``noise" present at each link is the
original noise , plus the interference received from all links in
. Specifically, for ,
we define a variable noise level .  Define 
to be the affectance taking this variable noise into account.

Now consider the following LP relaxation:

We show that the solution of LP2 is close to .
\begin{lemma}
Let  be the value of the solution to . Then .
\end{lemma}
\begin{proof}
Consider a -anti-feasible subset of , call this .
Consider the following solution to the : set  if  and  otherwise.
Cond.\ \ref{cog1} is satisfied since the incoming affectance on 
each link in  from  (and thus from ) is at most . 
The case of Cond.\ \ref{cog2} follows from Lemma \ref{cl2uni}. Thus .
\end{proof}

The next step is to round the fractional solution achieved from solving the LP. As before, we first set  with independent probability .
Let us define the event  as the condition  holding. Let us define the event , for each link , as the condition  holding.

We derive another round of selections by setting   iff  and both  and  occur. Thus,
 



Now . As we have seen before,  is independent of , thus  (via  Cond.\ \ref{cog2} and Markov's inequality). 

On the other hand,  is not independent of . However,  occurring given  is the same as   being true. But , by the definition of affectance. Thus . Thus finally, . Therefore, .



After the last round of selection, we thus get a set  such that
\begin{itemize}
\item , in expectation
\item 
\item   for all 
\end{itemize}
Using averaging arguments and signal strengthening as before, we can extract  which is feasible.
To complete the solution, we need to extract a subset of  such that
 

From the condition , it is not hard to see that the set of
selected links from  can be partitioned into  sets such that Eqn. \ref{cogfeas1} holds. This
gives us the sought after -approximation.
\end{proof}

Thm.~\ref{admingen1} implies part a) of Thm.~\ref{mainth2} directly.
We note that this implies a -approximation 
algorithm that holds under any other
non-decreasing sublinear power assignment, by partitioning the
linkset into at most  sets of nearly-uniform power.


We prove the last part of Thm.~\ref{mainth2} below.
\begin{theorem}
Let .
If , for a large enough constant , then there is a constant-factor approximation algorithm for the admission control problem for uniform power.
\end{theorem}

First we show that if  is large, we can assume that the affectances from  to  are small.

\begin{lemma}
Assume , for a large enough constant . 
Define  and .
Then .
\label{lem:saffect}
\end{lemma}
\begin{proof}
To see this, note that , since  must be feasible in presence of . Now, defining , .
Thus , or  and finally  if  is large enough.
\end{proof}

We can also claim a strengthening property.
\begin{lemma}
Assume  is a set such that for all , , and  for all . Then there is a subset 
with ,
such that  for all  and such a subset can be found in polynomial time with high probability.
\label{lem:sparsify1}
\end{lemma}
\begin{proof}
Simply select each link in  with probability . Let the set of selected links be . Then
 for all . Consider a fixed . Now
 where  is the iid random variable indicating selection in to .
We can use Hoeffding's inequality to get a large deviation bound.

\begin{theorem}{[Hoeffding, \cite{hoeffding1963}]}
Let the independent random variables  be bounded, i.e., , and let . Then,

\end{theorem}

Set  to be . We can verify that given our assumptions, setting  and  suffices. Setting ,

This implies, by the union bound, that with probability at least ,  for all  simultaneously. We now have proof of not only the existential statement, but the algorithmic one, since we can repeat the random experiments multiple times to get the high-probability result. 
\end{proof}
Note that the above holds equally for any affectance function (specifically, the case ).

Now we can describe the linear programming relaxation. First note that by virtue of Lemma \ref{lem:saffect} it suffices to  assume the input instance is  and the optimum is  (links not in  can be thrown out by simple pre-processing). 
Let us reuse notation  and  to refer to this new instance after pre-processing.

Consider the following linear program ()

We claim that this is a relaxation up to constant factors.
\begin{lemma}
Let  be the optimal value of . The .
\end{lemma}
\begin{proof}{(Sketch)}
By Lemma \ref{lem:sparsify1}, there exists   such that  for all . Now consider
a 2-anti-feasible subset  of . Consider as the solution of the
,  if  and 
otherwise. That Cond.\ \ref{cog1l} is satisfied follows from the claim
that  for all . Cond.\ \ref{cog2l} follows from anti-feasibility arguments along the lines made before.
\end{proof}


We can round this solution in the same way as before, with a two stage selection process. The proof varies only in that we need to claim that after the second
selection (characterized by Bernoulli variable ),  with high probability, simultaneously for all .
This  follows from an argument similar to Lemma \ref{lem:sparsify1} using the fact that affectances are bounded by  and using the Hoeffding's inequality.


\section{Weighted capacity}
\label{sec:weighted}
Recall that the result for weighted capacity applies only to linear power. For linear power the following stronger version of Lemma \ref{cl2} holds.

\begin{lemma}
Assume  is feasible using linear power and  is any link (also using linear power). Then, .
\label{cl4}
\end{lemma}
The proof is nearly identical to that of Lemma \ref{cl2}, as elaborated
in the appendix.


We can now write the following LP relaxation for the weighted capacity problem.




\begin{proof}{of Thm.~\ref{mainth3} (sketch)}
The proof is rather like that of Thm.~\ref{mainth1}. Once again, we select each link into a set  with probability  (characterized by Bernoulli variable  for each link ) and then do a further selection by setting  where  iff  and . As in the proof of Thm.~\ref{mainth1}, one can show that  and thus the expected weighted output is  which is within a constant factor of the optimum. Finally, the set  can be partitioned into a constant number of
feasible subsets using signal strengthening (Lemma \ref{lem:signal}), completing the proof.
\end{proof}

For other power assignments, such as uniform power, we seem to be within striking distance of a -approximation.
This is unfortunately not the case, we can only claim a poly-logarithmic approximation, worse than the greedy case. However, as
we show in Section \ref{sec:simul}, in practice the LP approach might be applicable to these other power assignments as well.



\section{Simulations}
\label{sec:simul}
In this section, we present results from simulation experiments. We focus on the weighted capacity problem for our experiments. It is difficult to conduct a comparative experiment for the admission control problem, there being no obvious previous algorithm to compare it with.

In contrast, the weighted capacity problem admits straightforward modifications of to the greedy algorithm, and thus a better comparative benchmark for our algorithm. Two natural greedy algorithms can be proposed:

\begin{itemize}
\item {\bf Using weight classes}: Let  (by scaling). Now we can assume that all . This is because links with smaller weights can be discarded without losing more than a factor of  in the approximation quality. Now divide the links into
 weight classes, the weight class  is defined by  for  to . Now if we consider links belonging to a single , the weights do not matter (up to a factor of 2). We simply run the greedy algorithm of \cite{SODA11} for each , and output the solution for the best weight class. This gives a straightforward  approximation factor.
\item {\bf Using length classes}: Let  by scaling and let . Divide the links
into length classes  for  to . Within  we can choose to run the greedy algorithm on the links in any order since the lengths are essentially the same, thus we go through links according to descending order of weights, achieving a constant factor approximation on . We choose the solution for the best , thus getting a  approximation.
\end{itemize}
Thus, comparing the two greedy algorithms, we achieve a  approximation. In what follows, we shall refer to
this joint algorithm as 
``greedy algorithm''.

\subsection*{Experimental setup}
We randomly generated the instances. Some important parameters of the experiments are as follows:
\begin{enumerate}
\item : The maximum length of a link (the implicit minimum being )
\item : A number  indicating that the sender of a link is chosen
  from a  square
\item : number of links
\end{enumerate}
We also use , , and .
The instances were generated as follows. For each link, the sender was chosen randomly from a  square. The length of the link was chosen randomly from . The receiver was thus placed at this distance from the sender and at a random direction. The weight was chosen independently from . We will mention different weight distributions later, and mention this initial choice of weight distribution as the \emph{ordinary} distribution.

One crucial aspect of both greedy algorithms as well as the LP algorithm is the constants used. For the LP algorithm, this is the constant  in Eqn. \ref{inaffectancebound1}. The greedy algorithm of \cite{SODA11} also depends on a constant.
Though theoretical bounds for these constants are available, it has been observed before that these theoretical
bounds do not perform the best in practice \cite{infocom11}. We run all algorithms with different values of the constant in question, running over reasonable values in small increments, and choosing the best solution for each algorithm separately.
We ran our experiments in MATLAB, and used the convex optimization package  \texttt{CVX} \cite{cvx} to solve the LP.

The overall message from the experiments is that using the linear
programming formulation gives a substantial improvement in the
solution quality in many cases. 
On the other hand, the greedy algorithm is also not without merit, and can outperform the LP in certain other situations.
It appears that the smaller the maximum feasible set is, the better
greedy does, while as the solution size/quality improves, LP
outperforms greedy. This is not surprising. When the set is really
dense and the link lengths are large, the quality of the solution is
bad and the cost incurred by greedy due to length-class or
weight-class partitions is minimal. 

\begin{figure}
\begin{center}
\includegraphics[height=2in]{linstraight400b.eps}
\caption{Simulation with linear power and  (ordinary weight assignment). Individual lines refer to different values of . The ratio of the solution from the LP algorithm to the greedy algorithm is plotted on the Y-axis against the density.} 
\label{fig:linstraight400}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in]{linstraight600.eps}
\caption{Simulation with linear power and  (ordinary weight assignment). } 
\label{fig:linstraight600}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in]{linreverse600.eps}
\caption{Simulation with linear power and  and {\bf Reversed} weight assignment. } 
\label{fig:linreverse600}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[height=2in]{unistraight400.eps}
\caption{Simulation with uniform power and  and the {\bf ordinary} weight assignment. } 
\label{fig:unistraight400}
\end{center}
\end{figure}


In Fig. \ref{fig:linstraight400}, we see the results for linear power with  links. On Y-axis is plotted , where
 and  are, respectively, the quality of the solution found from the linear programming algorithm and the greedy algorithm.
As alluded before, the greedy algorithm does better when  and density are both large (these are the points for which the Y-axis value is lesser than ), with the trend reversing when these change. 
Running the same experiment run for an increased number of links  confirms these trends (Fig. \ref{fig:linstraight600}).

We experimented with different distributions on the weights, to see if changes here change the solution trend significantly. We tried the following weight distributions.

\begin{itemize}
\item {\bf Reversed}: Set the weight of link  to be  where  is chosen according to the ordinary distribution.
\item {\bf Length determined}: Set weight of the link to be equal to its length.
\item {\bf Weight class}: Choose a parameter  randomly from  and set weight to .
\end{itemize}
The overall trend is similar. For reversed and  length determined distributions, LP did extremely well, whereas for the case
of weight class distribution, greedy did much better, with LP only barely outperforming it in a few cases. This further points to
the benefit of combining these algorithms in practice. The results for the reversed case are shown in Fig. \ref{fig:linreverse600}.

Next we experimented with uniform power. As we discussed in Section \ref{sec:weighted}, for uniform power we can only claim a poly-logarithmic approximation factor. However, the bounds are only so bad on rather pathological instances and one needs to do some work to come up with them. Thus in practice, it is reasonable to assume that an LP approach will be not without benefit.
  This is indeed borne out by our experiments, as seen in Fig. \ref{fig:unistraight400}.

\subsection*{Acknowledgements}
Research partially funded by grant 90032021 and grant-of-excellence 120032011 from the Icelandic Research Fund.
Authors thank Neal Young for helpful discussions.

\vspace*{20pt}


\bibliographystyle{plain}
\bibliography{references}		


\appendix






We give a proof of Lemma \ref{cl2}, originally due \cite{KV10}, that holds
also in the presence of arbitrary noise.

\emph{Lemma \ref{cl2}:}
If  is -feasible using a non-decreasing sublinear power
assignment and   is a link such that  for all , then .
\smallskip

\begin{proof}
Assume that  is a -feasible set. By the signal
strengthening (Lemma \ref{lem:signal}), this affects only the constant factor.

Consider the link  such that  is minimum.
Also consider the link  with  minimum.
Let . We claim that for all links  in , ,

To prove this, assume, for contradiction, that . 
Then,  , by definition of . 
Now, again by the definition of ,  
and . 
Thus 
and similarly . 
On the other hand .
Now, , contradicting the following:
  
  \begin{lemma}[\cite{us:esa09full}]
Let  be links in a -feasible set.
Then, . 
\label{lem:ind-separation}
\end{lemma}


Consider now any link  in , .
By the triangle inquality and Eqn.~\ref{eqn:dist2}, 
. 
Now . 
Since , it holds that 
 and by sub-linearity it holds that
.
Thus, 

where the final equality follows from the feasibility of .
Finally, summing over all links in 

since  by assumption.
\end{proof}

\medskip

\emph{Proofs of other lemmas:} When using linear power, it holds for all links  and  that
 and the signal received ,
satisfying Eqn.~\ref{eq:affs1} holds without the condition .
This yields Lemma \ref{cl4}.

By inverting the role of senders and receivers, we can obtain similar
bounds on out-affectance () as above on in-affectance ().
Modulo this change, the proof of Lemma \ref{cl3} is nearly identical
to the above proof of Lemma \ref{cl2}, and the argument for Lemma
\ref{cl2uni} mirrors that of Lemma \ref{cl4}.

\end{document}  
