\newif\ifcccg
\ifcccg
  \documentclass{cccg13}
  \usepackage{amsfonts}
  \usepackage{balance} \raggedbottom
\else
  \documentclass[12pt]{article}
  \usepackage{amsthm, fullpage}
  \newtheorem{theorem}{Theorem}
  \newtheorem{lemma}{Lemma}
\parskip=0pt plus 3pt\fi
\usepackage[pdfborderstyle={/S/U/W 1}]{hyperref}
\usepackage{graphicx,amsmath,enumitem,cite,microtype,caption}
\ifcccg\renewcommand{\theoremautorefname}{Lemma}
\else\newcommand{\lemmaname}{Lemma}\fi
\renewcommand{\itemautorefname}{step}
\setlist{nolistsep}
\newcommand\arr{\mathcal A}
\let\eps\varepsilon

\begin{document}
\title{How To Place a Point to Maximize Angles\thanks{This is an extended version of \cite{c3g}.}}
\author{
	Boris Aronov\thanks{
		Research supported by NSF grants CCF-11-17336 and CCF-12-18791.}\\
	aronov@poly.edu
	\and Mark Yagnatinsky\thanks{
		Research supported by GAANN Grant P200A090157 from the US Department of Education and by NSF grant CCF-11-17336.}\\
  myag@cis.poly.edu
}
\date{\ifcccg\normalsize\fi Polytechnic Institute of NYU, Brooklyn, New York}
\maketitle
\begin{abstract}
  We describe a randomized algorithm that, given a set  of points in the plane, computes the best location to insert a new point , such that the Delaunay triangulation of  has the largest possible minimum angle.  The expected running time of our algorithm is at most cubic, improving the roughly quartic time of the best previously known algorithm.  It slows down to slightly super-cubic if we also specify a set of non-crossing segments with endpoints in  and insist that the triangulation respect these segments, i.e., is the constrained Delaunay triangulation of the points and segments.
\end{abstract}

\section{Introduction}
The subject of meshing and specifically constructing ``well behaved'' triangulations has been researched extensively \cite{mesh}.  One of the problems addressed in the literature is that of refining or improving an existing mesh by incremental means.  Motivated by this, Aronov \emph{et~al.} \cite{orig} considered the following problem: ``Given a set of points in the plane, where would you place one additional point, so as to maximize the smallest angle in a good triangulation of the point set?''  Since Delaunay triangulations are known to maximize the smallest angle over all possible triangulations with a given vertex set \cite{Delaunay-good}, the question can be rephrased as: ``Given a point set, where do we place an additional point, so as to maximize the minimum angle in the Delaunay triangulation of the resulting set?''  In the rest of the paper we always picture the new point as lying within the convex hull of the existing points, but the algorithm is essentially the same without this assumption.  (Another variant of the problem mentioned in \cite{orig} involved incrementally improving an existing triangulation by ``tweaking'' the position of an existing interior vertex, one at a time, so that, again, the smallest angle is maximized.)  In \cite{orig}, they also discuss the more challenging question of how to position several points in the best possible coordinated way; we do not address this variant of the problem here.

The previous algorithm \cite{orig} for placing an additional point runs in worst-case  time, for any~, with the constant of proportionality depending on~.  We propose a randomized algorithm whose expected running time is roughly a factor of  lower.  Somewhat surprisingly, Aronov \emph{et al.} considered and rejected the approach we use in this paper \cite[page~96]{orig}.

The algorithm from \cite{orig} actually handles constrained Delaunay
triangulations, where a set of edges that must be present in the
triangulation is provided as part of the input in addition to a point
set.  This allows one to handle, for example, triangulating a simple
polygon.  We can modify our procedure to deal with constraints in
near-cubic time, slightly slower than the unconstrained case.

We present our algorithm, and show that it runs in cubic time, in the following section.  The analyses of this and the precursor algorithm \cite{orig} are misleading in that they reflect situations unlikely to happen for ``reasonable'' inputs.  We discuss how to measure how realistic an input is, and the resulting behavior of both algorithms on realistic inputs in \autoref{sec:realistic}.  In \autoref{sec:cdt}, we extend our algorithm to handle constrained triangulations in  time, and we conclude in \autoref{sec:concl}.

\section{The Algorithm}
Our algorithm takes a set  of  points in the plane and computes the best location for a new point~, such that the Delaunay triangulation of  has the largest possible minimum angle; for ease of presentation we will assume that the points of  are in \emph{general position}, that is no three points of  lie on a line and no four on a circle.  We start by recalling an argument detailed in \cite{orig} which duplicates the insertion step of the standard incremental Delaunay triangulation algorithm \cite{incremental}.  Let  be the Delaunay triangulation of~.  We begin by computing the arrangement  induced by the Delaunay circles of , i.e., of the circumcircles of the triangles of .  (If we are to allow placing  outside the convex hull of , we must deal with  points: the input points and the point at infinity.  Thus,  really has  triangles: the normal ones and the infinite ones, which are really the lines supporting the convex hull of .  This does not materially affect the algorithm, and for simplicity we ignore this possibility throughout the presentation.)  Although there are only a linear number of Delaunay circles, in the worst case every pair of them intersect, so that  has quadratic complexity.  We examine how the Delaunay triangulation~ of~ differs from .  Let  be the face of~ containing~.  Recall that a triangle is present in a Delaunay triangulation if and only if its Delaunay disk is empty of vertices.  Point  \emph{invalidates} some triangles of~ by appearing in the interior of the corresponding disks. After we have inserted~, we no longer have a triangulation; instead we have a star-shaped polygonal hole~ in~ containing~; see \autoref{hole} (left).
\begin{figure*}
\centering
\includegraphics[page=3]{points-for-holes}\hfil
\includegraphics[page=5]{points-for-holes}
\caption{\label{hole}The new point  is in the kernel of the shaded star-shaped polygonal hole~.  Removed edges of  are shown dashed (left) and added edges are dotted (right).}
\end{figure*}
Since the insertion of~ only invalidates previously valid triangles, but cannot make an invalid triangle valid (since insertion of  can't turn nonempty disks into empty ones), new edges of~ must have~ as an endpoint.  So, connecting  to all vertices of  (\autoref{hole}, right) is the way to complete .  This suggests the following algorithm outline:
\begin{enumerate}
\item Compute the Delaunay triangulation .
\item Build the arrangement  of Delaunay circles of~.
\item For each of the  cells :
  \begin{enumerate}
  \item Identify the set of  triangles invalidated by placing  in , the union of which
  	forms the hole~.
  \item Optimize the placement of  in . \label{step}
  \end{enumerate}
\item Return the best placement of  found.
\end{enumerate}
This outline was in fact used in \cite{orig}.  The main contribution of this paper is to use a different approach for \autoref{step}.  Specifically, in \cite{lp}, it was shown that the following is an LP-type problem.\footnote{In \cite{quasi-convex}, this and related problems are presented in a unified framework.}
\begin{quote}
Given a star-shaped polygon , find the point~ in its kernel that maximizes the smallest angle in the triangulation that results by connecting~ to all vertices of~.
\end{quote}
Being an LP-type problem, it can be solved in expected time linear in the number of vertices of , while the approach from \cite{orig}, based on explicitly computing lower envelopes of bivariate functions, takes time roughly quadratic in their number.  However, this LP-type problem is not quite the problem we actually wish to solve, as we need the optimal placement of  \emph{within the current cell~}, which is why this idea was rejected in \cite{orig}.  Fortunately, there is a conceptually simple fix.  In the \emph{region search} stage of our procedure, for each cell~, we run the algorithm from \cite{lp} discarding the result if the returned optimum lies outside~.  A simple argument (see \autoref{one} below) shows that if the solution to the unconstrained problem results in a point not in~, then the optimum within~ must lie on its boundary.  So in a separate \emph{boundary search} step detailed below, we find the best placement of  on any cell boundary.  Combining the results from the two steps we obtain the globally optimal placement for .

\begin{lemma}\label{one}  If the optimal solution to the unconstrained LP-type problem corresponding to cell~ is not in , then the optimal solution for  lies on its boundary.\end{lemma}
\begin{proof} Consider the locus  of points~ such that every angle in the new triangulation of  is at least .  It was shown in \cite{lp} that  is convex; it is easy to see that it varies continuously with~, when non-empty.  Clearly,  for .  As  decreases from its optimum unconstrained value,  will gradually grow from a single point outside~ and eventually intersect~; as it is connected and changes continuously with~, the first intersection must occur along the boundary of~.\end{proof}

It remains to find the best placement for  on each cell boundary.  A cell boundary has two sides, and we process each separately.  First consider an edge of .  For a fixed side of a fixed edge , we know which cell of  we are in, and thus the hole~.  If  has  vertices, the triangulation has  angles.  The measure of each of these angles is a univariate function of the position of~ along the edge.  To maximize the smallest of these functions, we find the maximum of their lower envelope by computing the envelope explicitly.  We will show that the graphs of any pair of these functions intersect at most 16 times.  A well-known result from the theory of Davenport-Schinzel sequences immediately implies that the maximum complexity  of the lower envelope is , where  is the maximum length of a DS sequence \cite[section 1.2]{ds}.  The maximum length of a DS sequence grows slowly as a function of  when  is constant: it is  for any constant .  (It was recently shown in \cite{tight} that a more precise bound is , where .)

\begin{lemma} The complexity of the lower envelope of  angle functions is
. \end{lemma}
\begin{proof} There are two kinds of angles to consider: angles at the boundary of , and angles at the new point .  We consider first angles at .  Let , and let  and  be two consecutive vertices of ; the coordinates of  and  are fixed.  We are interested in the angle  at which  sees the segment~; see \autoref{fig:curve} (left). 
\begin{figure}
  \centering
  \includegraphics{stinefig}
  \caption{``Artist's impression'' of the curves defined by the three types of angle equality constraints.}
  \label{fig:curve}
\end{figure}
Let  be another pair of consecutive vertices.  The angle that  makes with the segment  is .  Consider the locus of points~ specified by the equation
; a point~ satisfying this equation will see  and  at the same angle; refer to \autoref{fig:curve} (left).  An intersection between this curve and an edge of  corresponds precisely to an intersection of the graphs of two angle functions.  Once we prove that there are at most 16 such intersections, we are done.  For convenience, we will equate the cosines of the angles instead of the angles themselves.  Using  to denote segment length, the law of cosines gives
.  Solving for  gives

Setting  equal to  produces

After squaring both sides and reshuffling, we obtain

Now each side of the equation is a polynomial in  and  of total degree eight, which means that the locus of points  with  is a curve of degree eight.  How many times can such a curve intersect an edge of ?  An edge is an arc of a circle, which is the zero set of a polynomial of degree two.  According to B\'ezout's  theorem \cite{bez}, the number of proper intersection points is at most the product of the degrees, so there can be at most 16 intersection points.  A similar argument is needed for  and also  (refer to \autoref{fig:curve} (center and right)), but they also result in polynomial equations of degree at most eight; we omit the entirely analogous calculation.  (In some cases, the degree is only two, but since we are concerned with the worst case, this is little comfort.)  So, the complexity of the envelope is , and we are done.
\end{proof}

If the worst-case complexity of the lower envelope of ~functions from some class is , then we can compute the lower envelope of ~functions from that class in  time using a simple divide-and-conquer algorithm \cite[Theorem 6.1]{ds}.  This gives us a running time of  per arc, which would then make our total running super-cubic if there are a quadratic number of arcs.  However, we are duplicating much work: if we follow a Delaunay circle as it crosses another circle, very little changes when we cross: either one triangle of  ceases to be valid, or else one triangle becomes valid.  (This assumes that we only cross one circle at at time.  At a point of , we may cross many circles at once, so the total change is large, but it is still true that each circle we cross does only one triangle's worth of damage.)  Suppose that a triangle becomes valid when we cross (the other case is symmetric).  Then  loses a boundary vertex, and our triangulation of  loses two old triangles and gains one new one, which means our set of angle functions gains 3 new angles and loses 6 old ones.  The other angle functions remain unchanged.  Instead of restricting the domain of the angle functions to a single arrangement edge, we allow them to be defined wherever the corresponding angle itself exists (so the domain becomes an arc of a Delaunay circle).  On a given arc, there are at most  functions.  If there are  Delaunay circles, then the boundary of a fixed circle can only have  intersections with other circles, and for each of those intersections, at most 6 new functions appear.  The number of circles equals the number of triangles, which is less than .  Thus for the entire circle, there are at most a linear number of functions ().  It is still the case that any pair of function graphs intersect at most 16 times, but because each is not defined over the entire circle, but only a contiguous arc on it, the complexity of the lower envelope can increase slightly, up to  \cite{ds}.  This is the complexity of an envelope associated with a single circle, and there are  circles.  We explicitly compute the  envelopes and find the  associated maxima, .  We also do the region search from the beginning of the section.  The algorithm's final answer is the either the best value that the region search found, or the biggest , whichever is larger.  Thus, the running time of the boundary search stage is  and the total (expected) running time of our algorithm is dominated by the  region search time.  This concludes our description and analysis of the algorithm.

\section{Realistic inputs}\label{sec:realistic}
In the long tradition in computational geometry, exemplified by \cite{realistic}, we would like to be able to analyze our problem in non-worst-case situations.  To this end, we introduce several parameters, besides  that measures the number of input points, that quantify the ``badness'' of the input point set and express the running time of the algorithms in terms of them. 

Consider the arrangement  of Delaunay disks of  and let  be its complexity, that is the total number of vertices, edges, and faces; let  be the maximum \emph{depth} of the arrangement, that is the maximum, over all points in the plane, of the number of disks covering the point. In the worst case  is  and  is .  In well-behaved point sets, such as those corresponding to uniformly distributed points,  is ; one would also
expect  to be near-constant, however, somewhat surprisingly, an unfortunate, but arbitrarily small perturbation of the  grid can cause~ to be , even if we only measure depth within the hull of .  In particular, take the top of the grid, and move the points down slightly so they sit on an upward facing circle.  After a slight perturbation, there are  almost identical Delaunay disks which all intersect within the hull of ; see \autoref{pt}.
\begin{figure}
  \centering
  \includegraphics{perturb}
  \caption{\label{pt}A grid with high depth.}
\end{figure}

We now express the running times in terms of , , and~.  Our algorithm starts by computing the Delaunay triangulation, which can be done in  time.  We then compute the arrangement of circles in  time using a standard sweepline algorithm (better running times are possible using more involved techniques).  Our algorithm and that of \cite{orig} share the first two steps of the outline.  Their analog of the region search runs in time , for any positive , since for every cell , it performs an independent bivariate lower envelope calculation on~ functions, for a total time of .  We analyze the region search and the boundary search stages of our proposed algorithm separately.  The region search runs in expected time , as its bottleneck is solving  LP-type problems of size at most  each.  (Note that this requires that we quickly determine the set of constraints that correspond to a cell.  This is easy to arrange if we traverse the arrangement going from a cell to its immediate neighbor.)

We now turn our attention to the boundary search.  Our analysis here needs stronger general position assumptions than the algorithm itself does.  In particular, we require that if two Delaunay circles intersect in some point not in , no third circle passes through that point.

The running time for one Delaunay circle is affected by how many functions appear on the lower envelope corresponding to that circle.  We earlier derived a bound of  for the number of functions on a given circle.  We now make this more precise.  Let  denote the number of functions along circle .  If  intersects  other circles, and the deepest cell adjacent to  has depth , then by refining our previous analysis we obtain
.  (If the new point is at depth , then the star-shaped hole is composed of  triangles and has  vertices, and the new triangulation will therefore have  new triangles, and three times as many new angles.  Each time a circle is crossed, six new angles may appear, and each circle is crossed twice.)  Note that since these are Delaunay circles, no disk fully contains another.  Hence, any circle adjacent to a cell of large depth must intersect many other circles.  In particular, .  Thus, we have
, which is .

We now show that the sum of  over all circles is at most proportional to the arrangement complexity .  Note first that this sum is simply twice the number of pairs of intersecting circles.  Our approach will thus be to show that most pairs of intersecting circles contribute a vertex of degree four to the arrangement , that is, a vertex that no third circle passes through.  Indeed, consider a pair of intersecting circles such that both intersection points, call them  and , have degree at least six (in a circle arrangement, all vertices have even degree).  By our stronger general position assumption, both  and  are from the original point set~.  We now have a pair of points with two Delaunay circles passing through it: hence  must be a Delaunay edge!  But there are only a linear number of such edges, so we are done: all but  pairs of intersecting circles contribute a vertex of degree four to the arrangement, and each such pair can be charged to the vertex.

Finally, let  be the number of circles,  be the number of pairs of intersecting circles,  be the number of vertices of degree four, and  be the number of edges of the Delaunay triangulation.  We now bound the sum of~ over all circles:
,
which is .

Lastly, the total running time of the boundary search stage is at most proportional to

which is .  We thus have the following:
\begin{theorem} Let  be a set of  points in general position, let  be complexity of the arrangement of Delaunay disks induced by , and let  be the maximum depth of this arrangement.  Then the algorithm from the previous section can be implemented to run in time .
\end{theorem}
Therefore, our algorithm outperforms (in expectation) that of \cite{orig} for most values of  and .  (If  is a constant, their algorithm would take  time if implemented carefully, while our boundary search could take time , which is slightly worse.)

We can slightly refine the above analysis in another direction: recall that we defined  to be the \emph{maximum} depth of the arrangement .  If we let  be the \emph{average} depth, over all the cells, the expected running time of the region search can then be bounded by
, while the running time of the analogous part of algorithm of \cite{orig} is , where  is the depth of cell~; the latter quantity is, roughly,  times the average \emph{squared} depth.  The running time of the boundary search is not easily expressed in terms of , but it is less likely to dominate the running time of our algorithm.

\section{Constrained Delaunay triangulations}\label{sec:cdt}
The algorithm from \cite{orig} is actually designed to process
\emph{constrained Delaunay triangulations}: in addition to a point
set, a set of edges is given, which the resulting triangulation must respect.
We now describe how to modify our algorithm to handle this as well.

The first obvious change is that instead of starting with the Delaunay triangulation, we start with the constrained Delaunay triangulation, which can be computed in  time \cite{cdt}.

The second change to the algorithm is that the arrangement  must include not only the constrained Delaunay circles, but also the constrained edges.  With this change, knowing the cell containing a new point gives enough information to determine the star-shaped polygonal hole formed by the invalidated triangles \cite{orig}.  However, it is important to actually compute this information quickly.  (Before it sufficed to do  in-circle tests, which tell you which triangles to eliminate.)  However, if we are willing to spend  time, we can simply insert an artificial point in the cell, compute the constrained Delaunay triangulation from scratch, and then compare the resulting triangulation to the triangulation~ to determine which triangles were invalidated.

It remains to handle the boundary search.  We argued above that, as we trace along a circle, crossing another circle does only one triangle's worth of damage.  Unfortunately, when crossing a constrained edge, it is possible that due to changed visibility, many triangles disappear or reappear.  A simple-minded analysis is as follows: at each constrained edge crossing, at most  triangles appear or disappear.  A circle can only cross  constrained edges.  Thus, there are at most  triangle ``state changes'' along the whole circle boundary.  This analysis is only a constant factor away from being tight: start with  nearly co-circular points, and then add  constrained segments that each clip the circles; see \autoref{board}.  Thus there is no longer any sense in doing ``whole-circle-at-once'' processing, and the running time of the boundary search is  by computing envelopes on each arc of the arrangement of Delaunay circles.  This dominates the total running time of the whole algorithm.  (To figure out which angle functions to take the envelope of, we again simply insert an artificial point on the arc in question and re-triangulate from scratch.)  The boundary search can be sped up slightly to  by using the algorithm of Hershberger \cite{Hersh}.  So, we can compute the best place to insert a new point, so as to maximize the smallest angle in a constrained Delaunay triangulation in  time.
\begin{figure}
  \centering
  \includegraphics{whiteboard}
  \caption{\label{board}Many visibility changes along a circle, for many circles.}
\end{figure}

\section{Conclusions and open problems}\label{sec:concl}
Is there a way to compute which triangles are invalidated in each cell of the arrangement in a way proportional to the depth of that cell in the presence of constraints?  (Our realistic input analysis does not extend to the constrained version of the problem.)

It would be interesting to see if our algorithm can be derandomized using the results of Chazelle and Matou\v sek \cite{derand}; the LP-type problem needs to meet some technical requirements the discussion of which is omitted here.

Can the unconstrained algorithm be sped up by roughly another order of magnitude by observing that there is generally very little difference between LP-type problems corresponding to adjacent cells of ?  If we were dealing with actual linear programs instead of LP-type problems, we could use dynamic linear programming; see \cite{dynamic} and subsequent improvements.

Is there any hope of generalizing our approach to multiple Steiner points as in \cite{orig}?

\begin{thebibliography}{}
\bibitem[ABE99]{quasi-convex} N. Amenta, M. Bern, and D. Eppstein.
	Optimal point placement for mesh smoothing.
	\emph{J. Algorithms} 30(2), 302--322, 1999.
\bibitem[AAF10]{orig} B. Aronov, T. Asano, and S. Funke.	
	Optimal triangulations of points and segments with Steiner points.
	\emph{Int. J. Comput. Geom. Appl.},
	20(1) 89--104, 2010.
\bibitem[AY13]{c3g}
	B. Aronov and M. Yagnatinsky.
	How to place a point to maximize angles.
	\emph{CCCG} 2013, pages 259--263.
\bibitem[BKSV02]{realistic}
	M.~de~Berg, M.~J. Katz, A.~F. van~der Stappen, and J.~Vleugels.
	Realistic input models for geometric algorithms.
	\emph{Algorithmica}, 34:81--97, 2002.
\bibitem[B04]{mesh} M. Bern.  Triangulations and mesh generation.
	In \emph{Handbook of Discrete and Computational Geometry, 2nd Ed.},
	J. E. Goodman and J. O'Rourke, Eds., 563--582.
	CRC Press LLC, Boca Raton, FL, April 2004.

{\raggedright \bibitem[B1779]{bez}
	B\'ezout theorem.  Wikipedia.  From
	\url{http://en.wikipedia.org/wiki/B\%C3\%A9zout\%27s\_theorem}; retrieved 11 May 2013.

}\bibitem[C89]{cdt} L. P. Chew. Constrained Delaunay triangulations.
	\emph{Algorithmica}, 4(1--4) 97--108, 1989.
\bibitem[CM93]{derand} B. Chazelle and J. Matou\v{s}ek.
	On linear-time deterministic algorithms for optimization problems in fixed dimension.
	\emph{Proc. Fourth Annu. ACM-SIAM Symp. Discr. Algorithms}, pages 281--290, 1993.
\bibitem[E91]{dynamic} D. Eppstein.  Dynamic three-dimensional linear programming.
        \emph{ORSA J. Computing},
        4(4) 360--368, 1992.
\bibitem[GS85]{incremental} L. Guibas and J. Stolfi.
  Primitives for the manipulation of general subdivisions and the computation of Voronoi diagrams.
  \emph{ACM Transactions on Graphics}, 4(2) 74--123, 1985.
\bibitem[H89]{Hersh} J. Hershberger.
	Finding the upper envelope of n line segments in  time.
	\emph{Inf. Proc. Letters}, 33(4) 169--174, 1989.
\bibitem[MSW96]{lp} J. Matou\v sek, M. Sharir, and E. Welzl.
  A subexponential bound for linear programming.
  \emph{Algorithmica}, 16(4--5) 498--516, 1996.
\bibitem[P13]{tight}  Seth Pettie. Sharp Bounds on Davenport-Schinzel Sequences of Every Order,
	\emph{SoCG} 2013, pages 319--328.  Full version at \url{http://arxiv.org/abs/1204.1086}.
\bibitem[SA95]{ds} M. Sharir and P. K. Agarwal.
	\emph{Davenport-Schinzel Sequences and Their Geometric Applications}. 1995.
\bibitem[S78]{Delaunay-good} R. Sibson.  Locally equiangular triangulations.
	\emph{The Computer Journal}, 21(3) 243--245, 1978.
\end{thebibliography}
\ifcccg\balance\fi
\end{document}
