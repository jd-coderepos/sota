

{Our method for utilizing the co-occurrence information of HOI labels consists of three key components:} 
(1) establishing action co-occurrence priors (\Sref{sec.prior}), (2) hierarchical learning including anchor action selection (\Sref{sec.anchor}) and devising the hierarchical architecture (\Sref{sec.architecture}), followed by an extension of the architecture via self-attention (\Sref{sec.attention}), and 
(3) {ACP projection for knowledge distillation} (\Sref{sec.knowledge_distillation}), followed by an extension of the loss function via a language prior (\Sref{sec.language}).







\subsection{Action Co-occurrence Priors}
\label{sec.prior}











{Here, we formalize the action co-occurrence priors.} 
The priors for the actions are modeled by a co-occurrence matrix $C \in \mathbb{R}^{N\times N}$ where an entry $c_{ij}$ in $C$ represents the conditional probability that action $j$ occurs when action $i$ is happening: 
\begin{equation}
    c_{ij} = p(j|i), i,j \in [0, N),
\label{eq:acp}
\end{equation}
where $N$ 
(117 for the HICO-Det~\cite{chao2018learning} dataset) 
denotes the total number of action classes and $i,j$ are indices of two actions. 
By definition, the diagonal entries are one.
$C$ is constructed from the target HOI detection dataset by counting the {image-level} statistics of its training labels.
Examples of co-occurrence matrices constructed for a single object are visualized in \Fref{fig:co-occurrence}.




Meanwhile, we also consider the complementary event of action $i$ (\ie~where the $i$-th action does not occur) and denote it as $i^{'}$, such that $p(i^{'}) + p(i) = 1$.
The complementary action co-occurrence matrix $C^{'} \in \mathbb{R}^{N\times N}$ can thus be defined by entries $c_{ij}^{'}$ in $C^{'}$ that represent the conditional probability that an action $j$ occurs when another action $i$ does not occur: 
\begin{equation}
    c^{'}_{ij} = p(j|i^{'}), i,j \in [0, N).
\end{equation}
In this matrix, the diagonal entries are zero.



It can be seen from \Fref{fig:co-occurrence} that different types of relationships can exist between actions. 
They can be divided into three forms.
The first is the \textbf{prerequisite} relationship, where the given action is highly likely to co-occur with the conditional action. 
For example, the HOI `sit on-bicycle' is a prerequisite of the HOI `ride-bicycle'. 
In this case, 
$p(\text{sit on-bicycle}|\text{ride-bicycle})$ is close to $1$.
Next is \textbf{exclusion}, where the given action is highly unlikely to co-occur with the conditional action. An example is that the HOI `wash-bicycle' and HOI `ride-bicycle' are unlikely to happen together. 
As a result, $p(\text{wash-bicycle}|\text{ride bicycle})$ is close to $0$.
Finally, we have \textbf{overlapping}, where the given action and conditional action may possibly co-occur, for example HOI `hold-bicycle' and HOI `inspect-bicycle', such that $p(\text{hold-bicycle}|\text{inspect-bicycle})$ is in between $0$ and $1$.
We introduce two 
ways to exploit the co-occurrence matrix by regarding exclusion (action space partitioning) and prerequisite actions (knowledge distillation). 
Detailed descriptions of the proposed methods are presented in the following sections.





The strong relationships that may exist between action labels can provide strong priors on the presence or absence of an HOI in an image. 
{In contrast to previous works where models may implicitly learn label co-occurrence via relational architectures~\cite{baradel2018object,zhang2019co}, we explicitly exploit these relationships between action labels as priors, to effectively train our model especially for rare HOIs.}













\subsection{Anchor Action Selection via Non-Exclusive Suppression}
\label{sec.anchor}
{{From a co-occurrence matrix for an object, one can see that some actions are close in semantics or commonly co-occur while others are not. }}
Intuitively, 
visually similar actions (\eg~`ride-horse' and `hop on-horse')
tend to be harder to distinguish from each other, but these actions should not occur at the same time.
{If the positive labels for these actions are rare, then they become even more difficult to distinguish.}
Such cases require fine-grained recognition~\cite{fu2017look} and demand more dedicated classifiers.
This motivates us to learn HOI classes in a coarse-to-fine manner. 
In particular, as a pre-processing step, we first collect a set of mutually exclusive action {classes}, called \emph{anchor actions}, which tend to be distinguishable from one another. 
The anchor actions are used to partition the entire action {label} space into fine-grained sub-spaces. The other action {classes} are attributed to one or more sub-spaces and recognized in the context of a specific anchor action.
{In summary, unlike previous HOI detection works which predict action probabilities independently of one another, we divide the whole action label set into two sets, one for anchor actions and one for regular actions, which are modeled in different ways as explained in detail in \Sref{sec.architecture}}.








Before 
training, in selecting anchor actions, we seek a set of action {classes} that are exclusive of one another. To this end, we define the exclusiveness of an action {class} as counting the number of actions that never occur if action $i$ is happening: 
\begin{equation}
e_i = \sum_j ({1} \text{ if } (c_{ij} =0), \text{ else } {0}).
\label{eq:exclusiveness}
\end{equation}
$e_i$ will have a high value if few other actions can occur when $i$ does.
Based on exclusiveness values, the anchor action {label} set $\mathcal{D}$ is generated through \emph{\textbf{non-exclusive suppression (NES)}} as described in \Aref{alg:correlation}. It iteratively finds the most exclusive action {class} as an anchor action and removes remaining action {classes} that are not exclusive to the selected anchor actions. 
The anchors in the list are action {classes} that never occur together in the training labels. For example, if an action such as `toast' is on the anchor list, then actions like `stand' and `sit' cannot be on the list because they may co-occur with `toast', while actions such as `hunt' or `hop on' can potentially be on the list.
While there may exist other ways the anchor action selection could be done, we empirically found this approach to be simple, effective in terms of detection accuracy, and computationally efficient (less than 0.01 second).


\begin{algorithm}[ht]
\KwIn{
$\mathcal{E}=\{e_{i}, i \in [0, N)\}, \mathcal{C}=\{c_{ij}, i,j \in [0, N)\}$\;
}
\KwOut{
$\mathcal{D}$
} 
\Begin
{
  $\mathcal{D} \gets \{\}$  \;
  \While{$\mathcal{E}$ is not empty}
{
    \# Find the most exclusive action\;
    $m \gets argmax \mathcal{E}$\;
$\mathcal{D} \gets \mathcal{D} \cup \{m\}$  \;
\For{$e_k \in \mathcal{E}$}{
            \# Remove the actions correlated (not exclusive) to $m$\;
            \If{$c_{mk} > 0$}
            {
                $\mathcal{E} \gets \mathcal{E} - e_k; \mathcal{C} \gets C-\{c_{ij}, i \text{ or } j = k\}$
            }
    }
  }
}
\caption{
{Non-Exclusive Suppression (NES) algorithm for mutually exclusive anchor action selection.
}
}
\label{alg:correlation}
\end{algorithm}


The anchor action {label} set acts as a finite partition of the action {label} space (a set of pairwise disjoint events whose union is the entire action {label} space). To form a complete action {label} space, we add an \emph{\textbf{`other'}} anchor action, denoted as $\mathcal{O}$, for when an action {class} does not belong to $\mathcal{D}$. 
Finally, we have $|\mathcal{D}| + 1$ anchor actions including $\mathcal{D}$ and the `other' action {class} $\mathcal{O}$. 







There are several benefits to having this anchor action {label} set.
First, only one anchor action can happen at one time between a given human and object. Thus, we can use the relative (one-hot) probability representation with \emph{softmax} activation instead of a sigmoid, where softmax features were shown to compare well against distance metric learning-based features~\cite{horiguchi2019significance}.
Second, anchor actions tend to be easier to distinguish from one another since they generally have prominent differences in an image.
Third, it decomposes the action {label} space into several sub-spaces, which facilitates a coarse-to-fine solution. Each sub-task will have a much smaller solution space, which can improve learning.
Finally, each sub-task will use a standalone sub-network which focuses on image features specific to the sub-task, which is an effective strategy for fine-grained recognition~\cite{fu2017look}.







After selecting anchor actions, the entire action {label} set $\mathcal{A}$ is divided into the anchor action {label} set $\mathcal{D}$ and the remaining set of `regular' action {classes} $\mathcal{R}$, so that $\mathcal{A} = \{\mathcal{D}, \mathcal{R}\}$.
Each of the regular action {classes} is then associated with one or more anchor actions to form $|\mathcal{D}|+1$ action groups $\mathbf{G} = \{\mathcal{G}_i; i \in \mathcal{D}\cup\mathcal{O}\}$, one for each anchor action. 
A regular action {class} $j \in \mathcal{R}$ will be assigned to the group associated with anchor action $i$ ($\mathcal{G}_i$) if action $j$ is able to co-occur with the anchor action $i$,
\begin{equation}
    j \in \mathcal{G}_i,  \text{if } c_{ij} > 0 \;\; (i \in \mathcal{D}\cup\mathcal{O}, j \in \mathcal{R}).
\label{eq:action_groups2}
\end{equation}
Note that the anchor actions themselves are not included in the action groups and a regular action $j$ can be assigned to multiple action groups since it may co-occur with multiple anchor actions.









  

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth,keepaspectratio]{Figures/architecture_extended.pdf}
    \vspace{-2mm}
    \caption{Illustration of our overall network architecture.
	Our work differs from the baseline \cite{gupta2019no} by the addition of a hierarchical \emph{action prediction module}.
	For our hierarchical architecture, anchor action probabilities are directly generated by a \emph{softmax} sub-network. Regular action probabilities are generated by a matrix multiplication of the anchor probability and the output from a few \emph{sigmoid} based conditional sub-networks.}
	\vspace{-2mm}
    \label{fig:model_merged}
\end{figure*}


\subsection{Hierarchical Architecture}
\label{sec.architecture}
We implement our hierarchical approach upon several existing HOI detection architectures~\cite{gao2020drg,gupta2019no,liao2020ppdm}.
In this section, we introduce our main baseline architecture, built on the `No-Frills' (NFs) baseline presented in~\cite{gupta2019no} on account of its simplicity, effectiveness, and code availability~\cite{nofrillsgit}.
Here, we give a brief review of the NFs architecture. 

\noindent\textbf{Baseline Network}
NFs follows the common scheme of CNN feature extraction followed by multi-information fusion. The feature extraction uses an off-the-shelf Faster R-CNN~\cite{ren2015faster} object detector with ResNet152~\cite{he2016deep} backbone network to detect human and object bounding boxes. From the final fully connected (FC) layer of the detector, the features $\hat{x}=\{\hat{x}_h,\hat{x}_o\}$ are extracted, where $\hat{x}_h$ and $\hat{x}_o$ denote human and object appearance, respectively. These CNN features, together with the human pose ($\hat{k}$), object category ($\hat{o}$) and bounding boxes ($\hat{b}$ including both object and human bounding boxes) compose the multiple information as $X=\{\hat{x},\hat{k},\hat{o},\hat{b}\}$. As illustrated in \Fref{fig:model_merged}, they are fed to our target model via corresponding network streams: human appearance ($f_h$), object appearance($f_o$), bounding box and object category ($f_b$), and human pose and object category ($f_k$). Each individual type of information is first fed through a separate network of two FC layers to generate a fixed dimension (number of actions $N$) feature. Then, all the features are added together and sent through a \emph{sigmoid} activation to obtain the probability prediction for the action $a$:
\begin{equation}
    \hat{A}= sigmoid(F(X)) \in \mathbb{R}^N,
\label{eq:base_action_pred}
\end{equation}
where $\hat{A}(a) = p(a|X)$ represents the probability prediction for action class $a$.
The multi-information fusion procedure $F(X)$ is expressed as 
 \begin{equation}
     F(X) = f_h(\hat{x}_h) + f_o(\hat{x}_o) + f_k(\hat{k}||\hat{o})+ f_b(\hat{b}||\hat{o}),
 \label{eq:base_action_pred_simple}
 \end{equation}
 where 
$||$ denotes concatenation. The baselines built on other models~\cite{gao2020drg,liao2020ppdm} are constructed in a similar manner.


To eliminate training-inference mismatch, NFs directly optimizes the HOI class probabilities instead of separating the detection and interaction losses as done in~\cite{gao2018ican,gkioxari2018detecting}. The final HOI prediction is a joint probability distribution over $M$ number of HOI classes (600 for HICO-Det dataset)  {computed from the probabilities $\hat{H}$, $\hat{O}$, and $\hat{A}$ for human, object, and action, respectively:}
\begin{equation}
    \hat{Y} = joint(\hat{H}, \hat{O}, \hat{A}) \in \mathbb{R}^{M}.
\end{equation}
Specifically, for an HOI class $(h,o,a)$,
\begin{equation}
\begin{split}
\hat{Y} (h,o,a) & = \hat{H}(h) * \hat{O}(o) * \hat{A}(a) = p(h|I) * p(o|I) * p(a|X),\\
\end{split}
    \label{eq:base_hoi_pred}
\end{equation}
where $\hat{H}(h)=p(h|I)$ and $\hat{O}(o)=p(o|I)$ are the probability of a candidate box pair being a human $h$ and object $o$, provided by the object detector~\cite{ren2015faster}. Finally, the binary cross-entropy loss $\mathcal{L}(\hat{Y}, Y^{gt})$ is directly computed from the HOI prediction.
This ‘No-Frills’ baseline network is referred to as \emph{\textbf{Baseline}}.



\noindent\textbf{Modified Baseline Network} 
{For a stronger baseline comparison,}
we make two {simple} but very effective modifications on the baseline network.
(1) Replace the one-hot representation with the Glove word2vec~\cite{pennington2014glove} representation for the object category ($\hat{o}$ ).
(2) Instead of directly adding up the multiple information, we average them and forward this through another \emph{action prediction module}
to obtain the final action probability prediction. As a naive implementation of the \emph{\textbf{Modified  Baseline}}, we simply use a sub-network $f_{sub}$ of a few FC layers as the \emph{action prediction module}.
Then \Eref{eq:base_action_pred}-\ref{eq:base_action_pred_simple} are modified to
\begin{equation}
\hat{A} = 
sigmoid(f_{sub}(F(X))),
 \label{eq:modified}
\end{equation}
 \begin{equation}
 F(X) = \dfrac{f_h(\hat{x_h}) + f_o(\hat{x_o}) + f_k(\hat{k}||\hat{o})+ f_b(\hat{b}||\hat{o})}{n_{stream}}.
 \end{equation}
 In this case, $n_{stream}=4$. 

Our hierarchical architecture further modifies the \emph{action prediction module} by explicitly exploiting ACP information as described in the next paragraph.












\noindent\textbf{Proposed Hierarchical Architecture}
Now we introduce the \emph{action prediction module} for our hierarchical architecture (illustrated in \Fref{fig:model_merged}) that better exploits the inherent co-occurrence among actions. While the baseline network predicts all the action probabilities directly from $F(\cdot)$ with a single feed-forward sub-network $f_{sub}$, we instead use $|\mathcal{D}|+2$ sub-networks where one ($f_{anchor}(\cdot)$) is first applied to predict the anchor action set and then one of the $|\mathcal{D}|+1$ other sub-networks ($f_{\mathcal{G}_i}(\cdot)$) which corresponds to the predicted anchor action is used to estimate the specific action within the action group. Because of the mutually exclusive property of anchor actions, we use the \emph{softmax} activation for anchor action predictions, while employing the \emph{sigmoid} activation for regular action prediction conditioned on the action group:


\begin{equation}
    \hat{A}_{anchor} = softmax(f_{anchor}(F(X))) \in \mathbb{R}^{|\mathcal{D}|+1}
\label{eq:hie_anchor_pred}
\end{equation}
\begin{equation}
    \hat{A}_{\mathcal{G}_i} = sigmoid(f_{\mathcal{G}_i}(F(X))) \in \mathbb{R}^{N-|\mathcal{D}|}, \text{where } i \in \mathcal{D}\cup\mathcal{O},\label{eq:hie_group_pred}
\end{equation}
where $\hat{A}_{anchor}(i)$ is directly used as the final probability predictions for the anchor actions:\begin{equation}
     p(i|X) = \hat{A}_{anchor}(i), i \in \mathcal{D}.
\end{equation}
We let $\hat{A}_{\mathcal{G}_i}(j)$ represent the learned conditional probability that action $j$ occurs when action $i$ is happening, namely, 
\begin{equation}
 \hat{A}_{\mathcal{G}_i}(j) = p(j|i,X).
\end{equation}
Since the anchor action set is a finite partition of the entire action {label} space, the probability of a regular action $j$ is predicted according to the law of total probability:
\begin{equation}
    \begin{split}
    \hat{A}_{regular}(j) = p(j|X) & 
    =\sum_{ i \in \mathcal{D}\cup\mathcal{O}}^{} p(i|X) * p(j|i,X) \\
         & = \sum_{ i \in \mathcal{D}\cup\mathcal{O}}^{} \hat{A}_{anchor}(i) * \hat{A}_{\mathcal{G}_i}(j),\end{split}
\label{eq:hie_regular_pred}
\end{equation}
where $j \in \mathcal{R}$.
Thus, 
instead of \Eref{eq:modified},
we obtain the final action probability predictions for our hierarchical architecture $\hat{A}(a) = p(a|X)$ as 
\begin{equation}
    \hat{A}(a) = \begin{cases}
    \hat{A}_{anchor}(a),
& \text{if } a\in \mathcal{D}\\
    \sum_{ i \in \mathcal{D}\cup\mathcal{O}}^{} \hat{A}_{anchor}(i) * \hat{A}_{\mathcal{G}_i}(a),
& \text{otherwise.}
\end{cases}
\label{eq:hie_action_pred}
\end{equation}
We use the same method as in \Eref{eq:base_hoi_pred} and a cross-entropy loss $\mathcal{L}$ to compute the final HOI probability prediction $\hat{Y}$.

To demonstrate the effectiveness of the hierarchical learning, we introduce two other baselines,  \emph{\textbf{MultiTask}} and \emph{\textbf{TwoStream}}, that lie between the \emph{\textbf{Modified Baseline}} and our hierarchical learning. \emph{\textbf{MultiTask}} only uses the anchor action classification as an additional multi-task element to the \emph{\textbf{Modified Baseline}}. \emph{\textbf{TwoStream}} separately predicts the anchor and the regular actions
but without using the hierarchical modeling between anchor and regular actions. 







\subsection{Self-Attention Module Extension}
\label{sec.attention}
In order to enhance the capability for holistic relational understanding across objects or humans, we additionally leverage a self-attention module that applies the non-local layer~\cite{wang2018non} to each human-object pair. In particular, let $Z \in \R^{B\times 512}$ denote a stack of $B$ merged human-object features from our $F(\cdot)$ ($Z=F(X)$).
Then, we compute the relational association matrix by:
\begin{equation}
        R = \mathrm{Softmax}({\sigma}(Z W_a){\sigma}(Z W_b)^\top) \in \R^{B\times B},
		\label{eq:REM1}
\end{equation}
where {$\sigma(\cdot)$ denotes ReLU and} $W_a, W_b \in \R^{512\times 128}$ are learnable {weights} that map the human-object features $Z$ to their own roles, (\eg~key and query) and 
the softmax operation is applied row-wise.
Then, the relational feature matrix is computed by:
\begin{equation}
A = R\sigma(ZW_x)W_z^\top \in \R^{B\times 512},
\label{eq:REM2}
\end{equation}
where $W_x\in \R^{512\times 128}$ and $W_z\in \R^{512\times 128}$ are again learnable {weights}.
The matrix $A$ encodes aggregated features across all the objects according to the degree of relational association given in $R$. This relational feature matrix is combined with the original feature $Z$ by $\tilde{Z} = Z + A$, so that $Z$ is enhanced by holistic relational information.








\subsection{{ACP Projection for Knowledge Distillation}}
\label{sec.knowledge_distillation}

Knowledge distillation~\cite{hinton2015distilling} was originally proposed to transfer knowledge from a large network to a smaller one. 
Recently, knowledge distillation has been utilized for various purposes such as lifelong learning~\cite{li2016learning}, multi-task learning~\cite{kim2018disjoint} or modality transfer~\cite{cho2021dealing,hoffman2016learning}.
Hu~\etal~\cite{hu2016harnessing} extended this concept to distill prior knowledge in the form of logic rules into a deep neural network. Specifically, they propose a teacher-student framework to project the network prediction (student) to a rule-regularized subspace (teacher), where the process is termed distillation. 
The network is then updated to balance between emulating the teacher's output and predicting the true labels.


Our work fits this setting as
the ACPs can act as a prior to distill. 
We first introduce \emph{ACP Projection} to map the action distribution to the ACP constraints. Then, we use the teacher-student framework~\cite{hu2016harnessing} to distill knowledge from ACPs.

\noindent\textbf{ACP Projection} In ACP Projection, an arbitrary action distribution 
$A = \{p(i), i \in [0, N)\} \in \mathbb{R}^{ N}$
is projected into the ACP-constrained probability space:
\begin{equation}
A^{*} = project(A, C, C^{'}) \in \mathbb{R}^{N},
\end{equation}
where $A^{*}$ is the projected action prediction. The projected probability for the $j$-th action $A^{*}(j) = p(j^{*})$ is generated using the law of total probability:
\begin{equation}
    \begin{split}
    p(j^{*}) & =\dfrac{1}{N}\sum_{i=1}^{N} (p(i) * p(j|i) + p(i^{'}) * p(j|i^{'})) \\
& = \dfrac{1}{N} (\sum_{i=1}^{N} p(i) * c_{ij} + \sum_{i=1}^{N} (1-p(i)) * c_{ij}^{'}). \\
    \end{split}
\end{equation}


In matrix form, the ACP projection is expressed as
\begin{equation}
project(A, C, C^{'}) = \dfrac{AC + (1-A)C^{'}}{N}.
\end{equation}



In practice, we use the object-based action co-occurrence matrices $C_{o}\in\mathbb{R}^{N\times N}$ and $C_{o}^{'}\in\mathbb{R}^{N\times N}$ which only count actions related to a specific object $o$. \Fref{fig:co-occurrence} shows examples of $C_{o}$ with respect to object classes.
Also, we give different weights $\alpha$ and $\beta$ {as hyper-parameters} to the action co-occurrence matrix $C_{o}$ and its complementary matrix $C_{o}^{'}$, with the weights subject to $\alpha + \beta = 2,\alpha > \beta$.
The projection function is then modified to
\begin{equation}
project(A, C_{o}, C^{'}_{o}) = \dfrac{\alpha AC_{o} + \beta (1-A)C^{'}_{o}}{N}.
\end{equation}
This is done because we empirically found the co-occurrence relationships in $C_{o}$ to generally be much stronger than the complementary actions in $C^{'}_{o}$. 


\noindent\textbf{Teacher-Student Framework} Now we can distill knowledge from the ACPs using ACP Projection in both the training and inference phases. There are three ways ACP Projection can be used:
(1) Directly project the action prediction $\hat{A}$ into the ACP-constrained probability space at the testing phase to obtain the final action output (denoted as \emph{\textbf{PostProcess}}). 
(2) Project the action prediction $\hat{A}$ in the training phase and use the projected action as an additional learning target~\cite{hu2016harnessing,yu2017visual}.
(3) Project the ground truth label $H^{gt}$, $O^{gt}$, and $A^{gt}$\footnote{The triplet ground truth labels $H^{gt}$, $O^{gt}$, and $A^{gt}$ are straightforward to determine from the HOI ground truth label $Y^{gt}$.} to the ACP space in the training phase and use the projected action $project(A^{gt}, C_{{O}^{gt}}, C_{{O}^{gt}}^{'})$ as an additional learning target.
The second and third items are incorporated into the teacher-student framework as terms in a new objective function {(denoted as \emph{\textbf{Distillation}})}:
\begin{equation}
\mathcal{L}_{distill} = \lambda_1 \mathcal{L}(\hat{Y}, Y^{gt}) + \lambda_2 \mathcal{L}(\hat{Y}, \hat{Y}_{projO}) + \lambda_3 \mathcal{L}(\hat{Y}, Y^{gt}_{projO}),
\label{eqn:distill_loss}
\end{equation}
where
\begin{equation}
\hat{Y}_{projO} = joint(\hat{H}, \hat{O}, project(\hat{A}, C_{\hat{O}}, C_{\hat{O}}^{'}))  \in \mathbb{R}^{M},
\label{eqn:teacher2_self}
\end{equation}
\begin{equation}
Y^{gt}_{projO} = joint({H}^{gt},{O}^{gt},project(A^{gt}, C_{{O}^{gt}}, C_{{O}^{gt}}^{'}))  \in \mathbb{R}^{M}.
\label{eqn:teacher2_gt}
\end{equation}
$\lambda_1, \lambda_2, \lambda_3$ are balancing weights among the ground truth HOI term and the teacher objectives.
The object type can be easily determined from the object probability predictions $\hat{O}$ or the ground truth label ${O}^{gt}$.
With the new objective function, rare classes receive different supervisions, so confusion between them can be alleviated.






\subsection{Word Embedding Loss Extension}
\label{sec.language}
As an extension to enhance performance, we add another loss term, namely a word embedding regression loss.
Given a word2vec representation for the object category ${\hat{o}}$, we regress our model to the same object representation in the output by computing the regressed word embedding $v$. 
Similar to Peyre~\etal~\cite{peyre2019detecting}, we design the word embedding regularization loss as the sigmoid loss function:
\begin{equation}
    \mathcal{L}_{emb}=\sum^{|O|}_{i=1}{1}_{(y^o=i)}\log(\frac{1}{1+e^{-\hat{o}\cdot v}}),
    \label{eqn:emb_loss}
\end{equation}
where $|O|$ is the number of objects (80 for HICO-Det datasets) and $y^o\in\mathbb{R}^{|O|}$ is the class for the object.
Multi-task learning with the object prediction works as prior knowledge for action prediction, which leads to performance improvements.
Therefore, the total loss function can be described as a weighted sum of the knowledge distillation loss and the embedding loss:\begin{equation}
    \mathcal{L}_{total}=\mathcal{L}_{distill} + \lambda_0 \mathcal{L}_{emb},
    \label{eqn:total_loss}
\end{equation}
where $\lambda_0$ is the additional balancing weight between \Eref{eqn:distill_loss} and \Eref{eqn:emb_loss}.
Note that, as we have an object word2vec vector as input, this word embedding regression loss can be seen as an auto-encoding loss which is commonly used as a regularizer.



















