\section{Experiments}
\label{sec:experiments}

We experimentally show that our proposed algorithm \ours (Algorithm~\ref{alg:final}) improves the test performance of the noisier and rarer groups of examples (by stronger regularization) without negatively affecting the training and test performance of the other groups. We evaluate our algorithms on three vision datasets and one NLP dataset: CIFAR-10 and  CIFAR-100~\citep{krizhevsky2009learning}, IMDB-review~\citep{maas2011learning} (see Appendix~\ref{sec:imdb}), and WebVision~\citep{li2017webvision}, a real-world heteroskedastic and imbalanced dataset. Please refer to Appendix~\ref{sec:implementation} for low-level implementation details.

\textbf{Baselines.} We compare our proposed \ours{} with the following baselines. The simplest one is (1) Empirical risk minimization (ERM): the vanilla cross-entropy loss with all examples having the same weights of losses. We select two representatives from the {\bf noise-cleaning} line of approach. (2) Co-teaching~\citep{han2018co}: two deep networks are trained simultaneously. Each network aims to identify clean data points that have small losses and use them to guide the training of the other network. (3) INCV~\citep{chen2019understanding}: it extends Co-teacing to an interative version to estimate the noise ratio and select data. We consider three representatives from the \textbf{reweighting-based methods}, including two that learn the weighting using \textbf{meta-learning}. (4) MentorNet~\citep{jiang2018mentornet}: it pretrains a teacher network that outputs weights for examples that are used to train the student network with reweighting. (5) L2RW~\citep{ren2018learning}: it directly optimizes weights of each example in the training set by minimizing its corresponding loss on a small meta validation set. (6) MW-Net~\citep{shu2019meta}: it extends L2RW by explicitly defining a weighting function which depends only on the loss of the example. We also compare against two representatives from the \textbf{robust loss function}. (7) GCE~\citep{zhang2018generalized}: it generalizes mean average error and cross-entropy loss to obtain a new loss function. (8) DMI~\citep{xu2019l_dmi}: it designs a new loss function based on generalized mutual information. In addition, as an essential ablation study, we consider vanilla uniform regularization.
(9) Unif-reg: we apply the Jacobian regularizer on all examples with equal strength, and tune the strength to get the best possible validation accuracy.

\subsection{Simulating heteroskedastic and imbalanced datasets on CIFAR}
\label{sec:imbalance}

\textbf{Setting.}
Unlike previous works that test on uniform random or asymmetric noise, which is often not the case in reality, in this paper we test our method on more realistic noisy settings, as suggested by \citet{patrini2017making,zhang2018generalized}. In order to simulate heteroskedasticity, we only corrupt semantically-similar classes. For CIFAR-10, we exchange 40\% of the labels between classes `cat' and `dog', and between `truck' and `automobile'. CIFAR-100 has 100 classes grouped into 20 super classes. For each class of the 5 classes under the super class `vehicles 1' and `vehicles 2', we corrupt the labels with 40\% probability uniformly randomly to the rest of four classes under the same super class. As a result, the 10 classes under super class `vehicle 1' and `vehicle 2' have high label noise level and the corruption are only within the same super class. Heteroskedasticity of the labels and imbalance of the inputs  commonly coexist in the real world settings. {\ours} can take both of them into account. To understand the challenge imposed by the entanglements of heteroskedasticity and imbalance, and compare {\ours} with the aforementioned baselines, we inject data imbalance concurrently with the heteroskedastic noise. 
We remove samples from the corrupted classes to simulate the most difficult scenario --- the rare and noisy groups overfit significantly. (A more benign interaction between the noises and imbalance is that the rare classes have lower noise level, we defer it to Appendix~\ref{sec:cifar-benign}.) We use the imbalance ratio to denote the frequency ratio between the frequent (and clean) classes to the rare (and corrupted) classes. We consider imbalance ratio to be 10 and 100. 

\textbf{Result.} Table~\ref{tab:CIFAR_imb} summarizes the results. Since examples from rare classes tend to have larger training and validation loss regardless of whether the labels are correct or not, noise-cleaning based methods might drop excessive examples with correct labels. We examined the noise ratio of dropped samples for INCV under the setting of imbalance ratio equals 10. Among all dropped examples, there is only 19.2\% of true noise examples. In addition, the rare class examples selected still have 29.8\% of label noise. This explains that the significant decrease of accuracies of Co-teaching and INCV on corrupted and rare classes. Reweighting-based methods tend to suffer from the loss of accuracy in other more frequent classes, which is aligned with the findings in \citet{cao2019learning}. While the aforementioned baselines struggle to deal with heteroskedasticity and imbalance together, \ours{} is able to put them under the same regularization framework and achieve significant improvements. Notably, \ours{} also shows improvement over uniform regularization with optimally tuned strength. This clearly demonstrates the importance of introducing adaptive regularization among all examples for a better trade-off. A more detailed ablation study on the trade-off between training accuracy and validation accuracy can be found in Section~\ref{sec:webvision}.

\begin{table}[t]
\centering
\caption{Top-1 validation accuracy (averaged over 3 runs) of ResNet-32 on heteroskedastic and imbalanced CIFAR-10. \ours{} significantly improves noisy and rare classes, while keeping the accuracy on other classes almost unaffected.}

\label{tab:CIFAR_imb}
{\small
\begin{tabular}{lcc|cc}
\toprule
Imbalance ratio             & \multicolumn{2}{c|}{10}        & \multicolumn{2}{c}{100}                \\
Method            & Noisy\&Rare Cls.  &  Clean Cls.    &  Noisy\&Rare Cls.    & Clean Cls. \\ \midrule
ERM & $52.9 \pm 1.2$ & $94.4 \pm 0.1$ & $18.9 \pm 1.0$ & $94.2 \pm 0.1$ \\ 
Co-teaching & $30.2 \pm 2.3$ & $88.9 \pm 0.3$ & $15.4 \pm 2.8$ & $86.4 \pm 0.7$ \\ 
INCV & $48.9 \pm 1.7$ & $94.0 \pm 0.2$ & $25.8 \pm 1.8$ & $93.8 \pm 0.2$ \\ 
MentorNet & $54.1 \pm 1.0$ & $90.3 \pm 0.5$ & $28.3 \pm 1.5$ & $90.2 \pm 0.4$ \\ 
L2RW & $44.3 \pm 2.0$ & $90.1 \pm 0.5$ & $31.2 \pm 1.9$ & $89.7 \pm 0.7$ \\ 
MW-Net & $55.4 \pm 1.1$ & $91.7 \pm 0.5$ & $35.6 \pm 1.6$ & $92.3 \pm 0.5$ \\
GCE & $48.2 \pm 0.6$ & $91.6 \pm 0.3$ & $14.1 \pm 2.0$ & $91.7 \pm 0.4$ \\
DMI & $44.7 \pm 2.3$ & $90.7 \pm 0.8$ & $14.0 \pm 2.1$ & $91.8 \pm 0.6$ \\
\midrule
Unif-reg (optimal) & $53.9 \pm 0.9$ & $92.1 \pm 0.2$ & $36.7 \pm 1.0$ & $92.4 \pm 0.3$ \\ 
\textbf{Ours (\ours{})} & $\mathbf{63.5 \pm 0.8}$ & $\mathbf{94.3 \pm 0.2}$ & $\mathbf{42.4 \pm 0.7}$ & $\mathbf{94.0 \pm 0.2}$ \\ 
\bottomrule
\end{tabular}}
\end{table}

\subsection{Ablation study on CIFAR} We disentangle the problem setting to show the effectiveness of our uniÔ¨Åed framework. 

\textbf{Simulating heteroskedastic noise on CIFAR.} We study the uncertainty part of \ours{} by testing under the setting with only heteroskedastic noise. The type of noise injection is the same as Section~\ref{sec:imbalance}. 

We report the top-1 validation accuracy of various methods in Table~\ref{tab:CIFAR_full}. 
Aligned with our analysis in Section~\ref{sec:related}, we observe that both noise-cleaning and reweighting based methods don't get a comparable accuracy on noisy classes with applying strong regularization $(\lambda=0.1)$ under this heteroskedastic setting.
We observe the behavior that too strong regularization impede the model from fitting informative samples, thus it could lead to a decrease on clean classes' accuracy. On the contrary, too weak regularization leads to overfitting the noisy examples thus the accuracy on noisy classes do not reach the optimal.

Interestingly, we find that even the well-studied CIFAR-100 dataset has intrinsic heteroskedasticity and \ours{} can improve over uniform regularization to some extent. Please refer to Appendix~\ref{sec:cifar100} for the results on CIFAR-100 and Appendix~\ref{sec:imdb} for results on IMDB-review. 

\begin{table}[htpb]
\centering
\caption{Top-1 validation accuracy (averaged over 3 runs) of ResNet-32 on heteroskedastic CIFAR-10 and CIFAR-100 for the noisy classes and the clean classes.}
\label{tab:CIFAR_full}
{\small
\begin{tabular}{lcc|cc}
\toprule
Dataset             & \multicolumn{2}{c|}{CIFAR-10}        & \multicolumn{2}{c}{CIFAR-100}                \\
Method           &  Avg. Noisy Cls. &  Avg. Clean Cls.   & Avg. Noisy Cls. &  Avg. Clean Cls.   \\ \midrule
ERM & $68.6 \pm 0.2$ & $93.6 \pm 0.2$ & $65.3 \pm 0.3$ & $67.8\pm 0.2$ \\ 
Co-teaching & $64.7 \pm 0.4$ & $89.1 \pm 0.3$ & $59.8 \pm 0.4$ & $65.3 \pm 0.3$ \\ 
INCV & $76.7 \pm 0.6$ & $93.0 \pm 0.2$ & $66.2 \pm 0.3$ & $68.6 \pm 0.2$ \\ 
MentorNet & $71.1 \pm 0.4$ & $93.7 \pm 0.2$ & $65.9 \pm 0.3$ & $67.5 \pm 0.3$ \\ 
L2RW & $70.1 \pm 0.3$ & $92.5 \pm 0.3$ & $65.1 \pm 0.5$ & $67.0 \pm 0.3$ \\ 
MW-Net & $75.0 \pm 0.3$ & $94.4 \pm 0.2$ & $65.7 \pm 0.3$ & $69.1 \pm 0.2$ \\
GCE & $62.6 \pm 1.1$ & $90.2 \pm 0.2$ & $61.2 \pm 0.6$ & $66.9 \pm 0.2$ \\
DMI & $73.2 \pm 0.7$ & $90.8 \pm 0.2$ & $64.8 \pm 0.5$ & $67.1 \pm 0.2$ \\
\midrule
Unif-reg ($\lambda$ = 0.1) & $77.5 \pm 0.6$ & $92.3 \pm 0.2$ & $69.3 \pm 0.5$ & $66.6 \pm 0.3$ \\ 
Unif-reg (optimal) & $75.3 \pm 0.3$ & $94.1 \pm 0.2$ & $68.5 \pm 0.3$ & $68.6 \pm 0.2$ \\ 
\textbf{Ours (\ours{})} & $\mathbf{80.7 \pm 0.3}$ & $\mathbf{94.5 \pm 0.2}$ & $\mathbf{74.2 \pm 0.3}$ & $\mathbf{69.3 \pm 0.2}$ \\ 
\bottomrule
\end{tabular}}
\end{table}

\textbf{Simulating data imbalance on CIFAR.} We study the density part of \ours{} by testing under the setting with only data imbalance. We follow the same setting as ~\citet{cao2019learning} to create imbalanced CIFAR. Long-tailed imbalance follows an exponential decay in sample sizes across different classes. For step imbalance setting, all rare classes have the same sample size, as do all frequent classes. Our approach achieves better results than LDAM-DRW and is comparable to recent state-of-the-art methods under the imbalanced setting.


\begin{table}[htpb]
	\caption{Top-1 validation errors of ResNet-32 on imbalanced CIFAR-10 and CIFAR-100.}
	\label{tab:cifar-imb-table}
	\centering
	{\small
	\begin{tabular}{c|cc|cc|cc|cc}
		\toprule
		Dataset           & \multicolumn{4}{c|}{Imbalanced CIFAR-10}                               & \multicolumn{4}{c}{Imbalanced CIFAR-100}                              \\ \midrule
		Imbalance Type         & \multicolumn{2}{c|}{long-tailed} & \multicolumn{2}{c|}{step} & \multicolumn{2}{c|}{long-tailed} & \multicolumn{2}{c}{step} \\ \midrule
		Imbalance Ratio        & \multicolumn{1}{c|}{100}  & 10 & \multicolumn{1}{c|}{100}   & 10   & \multicolumn{1}{c|}{100}  & 10 & \multicolumn{1}{c|}{100}   & 10   \\ \midrule
		ERM & 29.64& 13.61& 36.70 & 17.50 & 61.68& 44.30 & 61.45& 45.37 \\
		Focal & 29.62 & 13.34 & 36.09 & 16.36 & 61.59 & 44.22 &  61.43                 & 46.54 \\
        CB Focal & 25.43 & 12.90 & 39.73 & 16.54 & 63.98 & 42.01 & 80.24 & 49.98 \\
	LDAM-DRW& 22.97 & 11.84 & 23.08 & 12.19& 57.96& 41.29& 54.64& 40.54 \\
	BBN~\citep{zhou2020bbn} & \textbf{20.18} & 11.68 & 21.64 & 11.99 & 57.44 & 40.88 & 57.44 & 40.36 \\
	\textbf{HAR-DRW} & 20.46 & \textbf{10.62} & \textbf{20.27} & \textbf{11.58} & \textbf{55.35}& \textbf{38.98} & \textbf{51.73} & \textbf{37.54} \\
		\bottomrule
	\end{tabular}}
\end{table}

\subsection{Evaluation on WebVision with real-world heterogeneity}
\label{sec:webvision}

WebVision~\citep{li2017webvision} contains 2.4 million images crawled from Google and Flickr using 1,000 labels shared with the ImageNet dataset. Its training set is both heteroskedastic and imbalanced (detailed statistics can be found in \citep{li2017webvision}), and it is considered as a popular benchmark for noise robust learning. As the full dataset is very large, we follow~\citep{jiang2018mentornet} to use a mini version, which contains the first 50 classes of the Google subset of the data. Following the standard protocol~\citep{jiang2018mentornet}, we test the trained model on the WebVision
validation set and the ImageNet validation set. We use ResNet-50 for ablation study and InceptionResNet-v2 for a fair comparison with the baselines. We report results comparing against other state-of-the-art approaches in Table~\ref{tab:WebVision_full}. Strikingly, \ours{} achieves significant improvement.

{\bf Ablation study.} We demonstrate the trade-off between training accuracy and validation accuracy on mini WebVision with various uniform regularization strength and \ours{} in Table~\ref{tab:WebVision_ablation}. It's evident that when we gradually increase the overall uniform regularization strength, the training accuracy continues to decrease, and the validation accuracy reaches its peak at 5e-2. While a strong regularization could improve generalization, it reduces deep networks' capacity to fit the training data. However, with our proposed \ours{}, we only enforce strong regularization on a subset so that we improve the generalization on noisier groups while maintaining the overall training accuracy not affected.


\begin{table}[t]
\centering
\caption{Validation accuracy of ResNet-50 when tuning the regularization strength on mini WebVision. \ours{} stands out of the trade-off constraint of fitting and generalization.}

\label{tab:WebVision_ablation}
{\small
\begin{tabular}{lcc|cc}
\toprule
\textbf{}            & \multicolumn{2}{c|}{Train Acc}   &\multicolumn{2}{c}{Val Acc}  \\ 
\textbf{Reg Strength}            & Top 1 & Top 5 & Top 1 & Top 5 \\
\midrule
0 & 69.01 &88.64 & 59.40 &80.84 \\ 
Unif-reg ($\lambda$ = 0.01) & 68.96 &88.54 & 64.32 &86.11\\ 
Unif-reg ($\lambda$ = 0.02) & 67.02 & 87.51& 64.40 &85.92 \\ 
Unif-reg ($\lambda $ = 0.05) & 65.11 & 86.33 & 65.80 & 86.84 \\ 
Unif-reg ($\lambda $ = 0.1) & 63.35 & 84.98 & 65.04 & 86.56\\ 
\textbf{Adaptive ({\ours})} & 69.12 & 88.41 & \textbf{69.20} & \textbf{88.96} \\ 
\bottomrule
\end{tabular}}
\end{table}



\begin{table}[t]
\centering
\caption{Validation accuracy of InceptionResNet-v2 on WebVision and ImageNet validation sets. \ours{} demonstrates significant improvements over the previous state-of-the-arts.}

\label{tab:WebVision_full}
{\small
\begin{tabular}{lcccc|cccc}
\toprule
Train             & \multicolumn{4}{c|}{mini WebVision}        & \multicolumn{4}{c}{full WebVision}                \\
Test            & \multicolumn{2}{c}{WebVision}    & \multicolumn{2}{c|}{ImageNet}    & \multicolumn{2}{c}{WebVision}    & \multicolumn{2}{c}{ImageNet}  \\ 
Method & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 & Top 1 & Top 5 \\ 
\midrule
ERM & 62.5 &80.8 & 58.5 &81.8 & 69.7 &87.0  & 62.9 &83.6 \\ 
Co-teaching & 63.6 & 85.2 & 61.5 &84.7 & - & - & - & - \\ 
INCV & 65.2 &85.3 & 61.6 &85.0 & - & - & - & - \\ 
MentorNet & 63.0 &81.4 & 57.8 &79.9 & 70.8 & 88.0 & 62.5 &83.0 \\ 
\textbf{Ours (\ours{})} & \textbf{75.5} & \textbf{90.7} & \textbf{70.3} & \textbf{90.0} & \textbf{75.0} & \textbf{90.6} & \textbf{67.1} & \textbf{86.7} \\
\bottomrule
\end{tabular}}
\end{table}



