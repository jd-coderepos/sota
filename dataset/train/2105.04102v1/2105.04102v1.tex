\documentclass{article}
\usepackage{amsmath,epsfig}
\usepackage[preprint]{spconfa4}
\usepackage{multirow, subfigure, amssymb}

\copyrightnotice{\textbf{978-1-6654-3864-3/21/\^{\ast}\textbf{I}_{\text{R}} \in \mathbb{R}^{H \times W \times 3}\textbf{I}_{\text{H}} \in \mathbb{R}^{\emph{H} \times \emph{W} \times 3}\emph{j}\textbf{F}_{\text{rgb}}^{j} \in \mathbb{R}^{\emph{H} \times \emph{W} \times \emph{C}}\emph{j}\textbf{F}_{\text{hha}}^{j} \in \mathbb{R}^{\emph{H} \times \emph{W} \times \emph{C}}\emph{H}\emph{W}\emph{C}\textbf{S}_{\text{rgb}}^j\textbf{S}_{\text{hha}}^j\emph{j}\emph{f}_{{\text{s}_1}}^{}( \cdot )\emph{f}_{{\text{s}_2}}^{}( \cdot )1 \times 1\emph{j}\textbf{F}_{\text{fuse}}^{j-1} \in \mathbb{R}^{\emph{H} \times \emph{W} \times \emph{C}}\textbf{F}_{\text{fuse}}^\emph{j}{\emph{f}_{\text{down}}}( \cdot ){\emph{f}_{\text{conv}}}( \cdot )\emph{j}=1\emph{f}_{{r_1}}^{}( \cdot )\emph{f}_{r_2}^{}( \cdot )\emph{i}\textbf{F}_{\text{encoder}}^\emph{i} \in \mathbb{R}^{\emph{H} \times \emph{W} \times \emph{C}}\textbf{F}_{\text{decoder}}^\emph{m-i} \in \mathbb{R}^{\emph{H} \times \emph{W} \times \emph{C}}\tilde{\textbf{F}}_{\text{decoder}}^\emph{m-i}\emph{i} \in\{2,3\}{\emph{f}_{\text{select}}}( \cdot ){\emph{f}_{\text{fuse}}}( \cdot )\emph{l}_{\emph{i}}\lambda_{\emph{i}}\emph{i}{\rm{480}} \times {\rm{480}}2.9\%1.7\%\uparrow\uparrow\uparrow$)} &  \\ \cline{1-2}
\end{tabular}
\end{center}
\vspace{-0.4cm}
\end{table}

In order to display the results of the model more intuitively, we show a part of the visualization results in Fig.~\ref{result}. It can be observed that compared to the baseline, our model has a better segmentation performance on different classes of objects, such as ceiling, table, \emph{etc.}, which can prove that our model combines the characteristics of RGB and depth information well. In addition, our model can distinguish small-scale objects and can perform more accurate contour segmentation, which can prove the effectiveness of DFP.

\section{conclusions}

In this work, we propose a feature selection-and-fusion network to address two main challenges in RGB-D semantic segmentation. Firstly, for the effective fusion of multi-modality information, we put forward the cascaded SCRF module to obtain unified multi-modality representations. Secondly, aimed at the loss of detailed information in the down-sampling stage, we design the DFP module to make the important details helpful in predicting the results. Experimental results demonstrate our model achieves competitive performance on NYUDv2 and SUN RGB-D datasets.


\footnotesize
\bibliographystyle{IEEEbib}
\bibliography{icme2021template}

\end{document}
