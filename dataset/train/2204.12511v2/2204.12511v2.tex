\vspace{-5pt}
\section{Experimental Results}
\vspace{-5pt}

In this section, we compare our PolyLoss against the commonly used cross-entropy loss and focal loss on various tasks, models, and datasets. For the following experiments, we adopt the default training hyperparameters in the public repositories without any tuning. Nevertheless, Poly-1 formulation leads to consistent advantage over default loss functions at the cost of a simple grid search.

\vspace{-5pt}
\subsection{\texorpdfstring{$L_{\text{Poly-1}}$}{} improves 2D image classification on ImageNet}
\vspace{-5pt}

Image classification is a fundamental problem in computer vision, and progress on image classification has led to progress on many related computer vision tasks. 
In terms of the network architecture, in addition to the ResNet-50 already used in \autoref{sec:understanding}, we also experiment with the state-of-the-art EfficientNetV2~\citep{tan2021efficientnetv2}. 
We use the ImageNet settings in~\citep{tan2021efficientnetv2} except for replacing the original cross-entropy loss with our PolyLoss $L_{Poly-1}$ with different values of $\epsilon_1$.
In terms of the dataset, in addition to the ImageNet-1K dataset already used in \autoref{sec:understanding}, we also consider ImageNet-21K, which has about 13M training images with 21,841 classes. We will study both the ImageNet-21K pretraining results and the ImageNet-1K finetuning results.





Pretraining EfficientNetV2-L on ImageNet-21K, then finetuning it on ImageNet-1K can improve classification accuracy \citep{tan2021efficientnetv2}. Here, we follow the same pretraining and finetuning schedule as reported in \citet{tan2021efficientnetv2} without modification\footnote{\label{note:enet} Code at \url{https://github.com/google/automl/tree/master/efficientnetv2}} but replace the cross-entropy loss with $L_{\text{Poly-1}}= -\log(P_t) + \epsilon_1 (1-P_t)$. We reserve 25,000 images from the training set as \textit{minival} to search the optimal $\epsilon_1$.

\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-20pt}
  \centering
  \includegraphics[width=0.9\linewidth]{enet_pretrain.pdf}
    \vspace{-10pt}
  \caption{\textbf{PolyLoss improves EfficientNetV2 family on the speed-accuracy Pareto curve.} Validation accuracy of EfficientNetV2 models pretrained on ImageNet-21K are plotted. PolyLoss outperforms cross-entropy loss with about $\times$2 speed-up.}
  \label{fig:Enet-pretrain}
    \vspace{-15pt}
\end{wrapfigure}
\vspace{-10pt}
\paragraph{Pretraining on ImageNet-21K}\autoref{fig:Enet-pretrain} highlights the importance of using tailored loss function when pretraining model on ImageNet-21K dataset. A simple grid search over $\epsilon_1 \in \{0, 1, 2, \hdots, 7\}$ in $L_{\text{Poly-1}}$ without changing other default hyperparameters leads to around 1\% accuracy gain for all SOTA EfficientNetV2 models with different sizes.  The accuracy improvement of using a better loss function nearly matches the improvement of scaling up the model architecture (S to M and M to L). 

Surprisingly, see \autoref{fig:EV2tunefirst}, increasing the weight of the leading polynomial coefficient improves the accuracy of pretraining on ImageNet-21K (+0.6), whereas reducing it lowers the accuracy (-0.9). Setting $\epsilon_1 = -1$ truncates the leading polynomial term in the cross-entropy loss (\autoref{eq:ce-loss}), which is similar to having a focal loss with $\gamma=1$ (\autoref{eq:fl-loss}).
However, the opposite change, where $\epsilon_1 >0$, improves the accuracy on the imbalanced ImageNet-21K. 

We hypothesize the prediction of the imbalanced ImageNet-21K is not confident enough ($P_t$ is small), and using positive $\epsilon_1$ PolyLoss leads to more confident predictions. To validate our hypothesis, we plot $P_t$ as a function of training steps in \autoref{fig:EV2Pt}. We observe that $\epsilon_1$ directly controls the mean $P_t$ over all classes. Using positive $\epsilon_1$ PolyLoss leads to more confident prediction (higher $P_t$). On the other hand, negative $\epsilon_1$ PolyLoss lowers the confidence. 

\begin{figure}[!h]
\vspace{-8pt}
  \centering
  \begin{subfigure}{0.45\textwidth}
  \includegraphics[width=\textwidth]{EV2tunefirst.pdf}
  \caption{Validation accuracy of EfficientNetV2-L on ImageNet-21K. PolyLoss with positive $\epsilon_1$ outperforms baseline cross-entropy loss (red dash line).}
  \label{fig:EV2tunefirst}
  \end{subfigure}
  \quad
  \begin{subfigure}{0.45\textwidth}
  \includegraphics[width=\textwidth]{EV2Pt.pdf}
  \caption{Positive $\epsilon_1=1$ (dark) increases the prediction confidence, while negative $\epsilon_1=-1$ (light) decreases the prediction confidence.}
  \label{fig:EV2Pt}
  \end{subfigure}
  \vspace{-5pt}
  \caption{\textbf{PolyLoss improves EfficientNetV2-L by increasing prediction confidence $P_t$.}}
  \vspace{-15pt}
\end{figure}
\begin{wraptable}{r}{0.4\textwidth}
\vspace{-10pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c
>{\columncolor[HTML]{EFEFEF}}c c}
\toprule
EfficientNetV2-L  & $L_{\text{CE}}$& \cellcolor[HTML]{EFEFEF}$L_{\text{Poly-1}}$ & Improv. \\ \midrule
ImageNet-21K & 45.8& \textbf{46.4} & {\color[HTML]{3166FF} \textbf{+0.6}} \\
ImageNet-1K & 86.8 & \textbf{87.2}& {\color[HTML]{3166FF} \textbf{+0.4}} \\ \bottomrule
\end{tabular}
}
\vspace{-5pt}
\caption{\textbf{PolyLoss improves classification accuracy on ImageNet \textit{validation set}}. We set $\epsilon_1=2$ for both.}
\label{table:enet-l}
\vspace{-10pt}
\end{wraptable}
\paragraph{Fine tuning on ImageNet-1K} After pretraining on ImageNet-21K, we take the EfficientNetV2-L checkpoint and finetune it on ImageNet-1K, using the same procedure as~\citet{tan2021efficientnetv2} except for replacing the original cross-entropy loss with the Poly-1 formulation. PolyLoss improves the finetuning accuracy by 0.4\%, advancing the ImageNet-1K top-1 accuracy from 86.8\% to 87.2\%.

\vspace{-8pt}
\subsection{\texorpdfstring{$L_{\text{Poly-1}}$}{} improves 2D instance segmentation and object detection on COCO}
\vspace{-5pt}

Instance segmentation and object detection require localizing objects in an image in addition to recognizing them: the former in the form of arbitrary shapes and the latter in the form of bounding boxes. 
For both instance segmentation and object detection, we use the popular COCO \citep{lin2014microsoft} dataset, which contains 80 object classes.
We choose Mask R-CNN \citep{he2017mask} as the representative model for instance segmentation and object detection.
These models optimize multiple losses, e.g. $L_{\text{MaskRCNN}} = L_{\text{cls}} + L_{\text{box}} + L_{\text{mask}}$. For the following experiments, we only replace the $L_{\text{cls}}$ with PolyLoss and leave other losses intact. Results are summarized in \autoref{table:coco}.

\begin{table}[!t]\centering
\vspace{-20pt}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{c|c|cccc}
\toprule
\multicolumn{1}{l|}{} & Loss & \multicolumn{2}{c|}{Box} & \multicolumn{2}{c}{Mask} \\
\multicolumn{1}{l|}{} &  & AP & \multicolumn{1}{c|}{AR} & AP & AR \\ \midrule
Mask R-CNN $L_{\text{CE}}$ & \small$-\log(P_t)$\par & 35.0$\pm$ 0.09 & 47.2$\pm$ 0.16 & 31.3 $\pm$ 0.09 & 42.3 $\pm$ 0.02 \\
\rowcolor[HTML]{EFEFEF} 
\cellcolor[HTML]{EFEFEF}Mask R-CNN $L_{\text{Poly-1}}$ & \small\textbf{$-\log(P_t) - (1-P_t)$}\par & \textbf{35.3 $\pm$ 0.12} & \textbf{49.7$\pm$ 0.07} & \textbf{31.6 $\pm$ 0.11} & \textbf{44.4 $\pm$ 0.07} \\
Improvement & - & {\color[HTML]{3166FF} \textbf{+0.3}} & {\color[HTML]{3166FF} \textbf{+2.5}} & {\color[HTML]{3166FF} \textbf{+0.3}} & {\color[HTML]{3166FF} \textbf{+2.1}} \\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\caption{\textbf{PolyLoss improves detection results on COCO \textit{validation set}.} Bounding box and instance segmentation mask average-precision (AP) and average-recall (AR) are reported for Mask R-CNN model with a ResNet-50 backbone. Mean and stdev of three runs are reported. }
\label{table:coco}
\vspace{-18pt}
\end{table}

\textbf{Reducing the leading polynomial coefficient improves Mask R-CNN AP and AR.}  In training Mask R-CNN, we use the training schedule optimized for cross-entropy loss,\footnote{\label{detection}Code at \url{https://github.com/tensorflow/tpu/tree/master/models/official}} and replace the cross-entropy loss with $L_{Poly-1}=-\log(P_t) + \epsilon_1 (1-P_t)$ for the classification loss $L_{cls}$, where $\epsilon_1 \in \{-1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.5, 1.0 \}$. We ensure the leading coefficient is positive, i.e. $\epsilon_1\geq-1$. Our results in \autoref{fig:MaskRCNN_APAR} show systematic improvements of box AP, box AR, mask AP, and mask AR as we reduce the weight of the first polynomial by using negative $\epsilon_1$ values. Note that Poly-1 ($\epsilon=-1$) not only improves AP but also significantly increases AR, shown in \autoref{table:coco}. 
\begin{figure}[!h]
\vspace{-5pt}
  \centering
  \begin{subfigure}{0.6\textwidth}
  \centering
  \includegraphics[width=0.49\textwidth]{MaskRCNN_boxAP.pdf}
  \includegraphics[width=0.49\textwidth]{MaskRCNN_maskAP.pdf}
  \includegraphics[width=0.49\textwidth]{MaskRCNN_boxAR.pdf}
  \includegraphics[width=0.49\textwidth]{MaskRCNN_maskAR.pdf}
  \caption{Bound box AP, AR and Mask AP, AR increase as $\epsilon_1$ decreases. Negative $\epsilon_1$ outperforms cross-entropy loss (red dash line).}
  \label{fig:MaskRCNN_APAR}
  \end{subfigure}
  \quad
  \begin{subfigure}{0.35\textwidth}
  \includegraphics[width=1.0\textwidth]{MaskRCNN_Pt.pdf}
  \caption{Negative $\epsilon_1=-1$ (light) reduces the overconfident prediction $P_t$.}
  \label{fig:MaskRCNN_Pt}
  \end{subfigure}
  \vspace{-5pt}
  \caption{\textbf{PolyLoss improves Mask R-CNN by lowering overconfident predictions.} Mean and stdev of three runs are plotted.}
  \vspace{-18pt}
\end{figure}

\textbf{Tailoring loss function to datasets and tasks is important.}  ImageNet-21K and COCO are both imbalanced but the optimal $\epsilon$ for PolyLoss are opposite in sign, i.e. $\epsilon =2$ for ImageNet-21K classification and $\epsilon =-1$ for Mask R-CNN detection. We plot the $P_t$ of the Mask R-CNN classification head and found the original prediction is overly confident ($P_t$ is close to $1$) on the imbalanced COCO dataset, thus using a negative $\epsilon$ lowers the prediction confidence, as shown in \autoref{fig:MaskRCNN_Pt}. This effect is similar to label smoothing \citep{szegedy2016rethinking} and confidence penalty \citep{pereyra2017regularizing}, but unlike those methods, as long as $0>\epsilon>-1$, PolyLoss lowers the gradients of overconfident predictions but will not encourage incorrect predictions or directly penalize prediction confidence. 
\vspace{-10pt}
\subsection{\texorpdfstring{$L_{\text{Poly-1}}$}{} improves 3D object detection on Waymo Open Dataset}
\vspace{-8pt}
\begin{table}[!h]
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|l|l}
\toprule
 & Polynomial expansion in the basis of $(1-P_t)$ & Loss \\[0.8ex] \midrule
Focal loss & $(1-P_t)^{\gamma+1} + 1/2(1-P_t)^{\gamma+2} + 1/3(1-P_t)^{\gamma+3} + ...$ & $L_{\text{FL}} = -(1-P_t)^{\gamma}\log(P_t)$ \\ [0.8ex] 
Poly-1 (PointPillars)& $(\mathcolorbox{red!10}{\epsilon_1}+1) (1-P_t)^{\gamma+1}  +1/2(1-P_t)^{\gamma+2} + 1/3(1-P_t)^{\gamma+3}+... $ &$L^{\text{FL}}_{\text{Poly-1}} =L_{\text{FL}} + \epsilon_1 (1-P_t)^{\gamma+1}$\\ [0.8ex]
Poly-1$^*$ (RSN)& $\mathcolorbox{red!10}{\text{(drop first) }} (1/2+\mathcolorbox{red!10}{\epsilon_2})(1-P_t)^{\gamma+2} +1/3(1-P_t)^{\gamma+3} + ...$ & $L^{\text{FL}}_{\text{Poly-1}^*} = L_{\text{FL}} -(1-P_t)^{\gamma+1}+ \epsilon_2 (1-P_t)^{\gamma+2}$ \\ \bottomrule
\end{tabular}
}
\caption{\textbf{PolyLoss vs. focal loss for 3D detection models.} Differences are highlighted in red. We found the best Poly-1 for PointPillars is $\epsilon_1 =-1$, which is equivalent to dropping the first term. Therefore, for RSN, we drop the first term and tune the new leading polynomial $(1-P_t)^{\gamma+2}$.  }
\label{tab:focal_poly}
\vspace{-5pt}
\end{table}

Detecting 3D objects from LiDAR point clouds is an important topic and can directly benefit autonomous driving applications. 
We conduct these experiments on the Waymo Open Dataset~\citep{sun2020scalability}.
Similar to 2D detectors, 3D detection models are commonly based on single-stage and two-stage architectures. Here, we evaluate our PolyLoss on two models: a popular single-stage PointPillars model \citep{lang2019pointpillars}; and a state-of-the-art two-stage Range Sparse Net (RSN) model \citep{rsn}. Both models rely on multi-task loss functions during training. Here, we focus on improving the classification focal loss by replacing it with PolyLoss. Similar to the 2D perception cases, we adopt the Poly-1 formulation to improve upon focal loss, shown in \autoref{tab:focal_poly}.

\begin{table}[t]
\vspace{-20pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{1}{l|}{} & \multicolumn{1}{c|}{Loss} & \multicolumn{2}{c|}{BEV} & \multicolumn{2}{c}{3D} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{c|}{} & AP/APH L1 & \multicolumn{1}{c|}{AP/APH L2} & AP/APH L1 & AP/APH L2 \\ \midrule
\multicolumn{6}{c}{Vehicle (IoU=0.7)} \\ \midrule
\multicolumn{1}{c|}{PointPillars $L_{\text{FL}}$} & \multicolumn{1}{c|}{\small$-(1-P_t)^2\log(P_t)$\par} & 82.5/81.5 & 73.9/72.9 & 63.3/62.7 & 55.2/54.7 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}PointPillars $L^{FL}_{\text{Poly-1}}$} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\small$-(1-P_t)^2\log(P_t)- (1-P_t)^3$\par} & \textbf{83.6/82.5} & \textbf{74.8/73.7} & \textbf{63.7/63.1} & \textbf{55.5/55.0} \\
\multicolumn{1}{c|}{Improvement} & \multicolumn{1}{c|}{-} & {\color[HTML]{3166FF} \textbf{+1.1/+1.0}} & {\color[HTML]{3166FF} \textbf{+0.9/+0.8}} & {\color[HTML]{3166FF} \textbf{+0.4/+0.7}} & {\color[HTML]{3166FF} \textbf{+0.3/+0.3}} \\
\multicolumn{1}{c|}{RSN $L_{\text{FL}}$} & \multicolumn{1}{c|}{\small$-(1-P_t)^2\log(P_t)$\par} & 91.3/90.8 & 82.6/\textbf{82.2} & 78.4/78.1 & 69.5/69.1 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}RSN $L^{FL}_{\text{Poly-1}^*}$} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF} \small $-(1-P_t)^2\log(P_t)- (1-P_t)^3-0.4(1-P_t)^4$ \par} & \textbf{91.5/90.9} & \textbf{82.7}/82.1 & \textbf{78.9/78.4} & \textbf{69.9/69.5} \\
\multicolumn{1}{c|}{Improvement} & \multicolumn{1}{c|}{-} & {\color[HTML]{3166FF} \textbf{+0.2/+0.1}} & {\color[HTML]{3166FF} \textbf{+0.1}}/-0.1 & {\color[HTML]{3166FF} \textbf{+0.5/+0.3}} & {\color[HTML]{3166FF} \textbf{+0.4/+0.4}} \\ \midrule
\multicolumn{6}{c}{Pedestrian (IoU=0.5)} \\ \midrule
\multicolumn{1}{c|}{PointPillars $L_{\text{FL}}$} & \multicolumn{1}{c|}{\small$-(1-P_t)^2\log(P_t)$\par} & 76.0/62.0 & 67.2/54.6 & 68.9/56.6 & 60.0/49.1 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}PointPillars $L^{FL}_{\text{Poly-1}}$} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\small$-(1-P_t)^2\log(P_t)- (1-P_t)^3$\par} & \textbf{77.1/62.9} & \textbf{67.7/55.1} & \textbf{69.6/57.1} & \textbf{60.2/49.3} \\
\multicolumn{1}{c|}{Improvement} & \multicolumn{1}{c|}{-} & {\color[HTML]{3166FF} \textbf{+1.1/+0.9}} & {\color[HTML]{3166FF} \textbf{+0.5/+0.5}} & {\color[HTML]{3166FF} \textbf{+0.7/+0.5}} & {\color[HTML]{3166FF} \textbf{+0.2+0.2}} \\
\multicolumn{1}{c|}{RSN $L_{\text{FL}}$} & \multicolumn{1}{c|}{\small$-(1-P_t)^2\log(P_t)$\par} & 85.0/81.4 & 75.5/72.2 & 79.4/76.2 & 69.9/67.0 \\
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}RSN $L^{FL}_{\text{Poly-1}^*}$} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF} {\small $-(1-P_t)^2\log(P_t)- (1-P_t)^3+0.2(1-P_t)^4$ \par}} & \textbf{85.4/81.8} & \textbf{75.8/72.5} & \textbf{80.2/77.0} & \textbf{70.6/67.7} \\
\multicolumn{1}{c|}{Improvement} & \multicolumn{1}{c|}{-} & {\color[HTML]{3166FF} \textbf{+0.4/+0.4}} & {\color[HTML]{3166FF} \textbf{+0.3/+0.3}} & {\color[HTML]{3166FF} \textbf{+0.8/+0.8}} & {\color[HTML]{3166FF} \textbf{+0.7/+0.7}} \\ \bottomrule
\end{tabular}
}
 \caption{\textbf{PolyLoss improves detection results on Waymo Open Dataset \textit{validation set}}. Two detection models: single-stage PointPillars \citep{lang2019pointpillars} and two-stage SOTA RSN \citep{rsn} are evaluated. Bird's eye view (BEV) and 3D detection average precision (AP) and average precision with heading (APH) at Level 1 (L1) and Level 2 (L2) difficulties are reported. The IoU threshold is set to 0.7 for vehicle detection and 0.5 for pedestrian detection.}
 \label{table:3d}
 \vspace{-15pt}
\end{table}

\textbf{PolyLoss improves single-stage PointPillars model.} The PointPillars model converts the raw 3D point cloud to a 2D top-down pseudo image, and then detect 3D bounding boxes from the 2D image in a similar way to RetinaNet \citep{lin2017focal}. Here, we replace the classification focal loss ($\gamma=2$) with $L^{\text{FL}}_{\text{Poly-1}} = -(1 - P_t)^2 \log P_t + \epsilon_1(1 - P_t)^3$ and adopt the same training schedule optimized for focal loss without any modification\footnote{Code at \url{https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car}}. \autoref{table:3d} shows that $L^{\text{FL}}_{\text{Poly-1}}$ with $\epsilon=-1$  leads to significant improvement on all the metrics for both vehicle and pedestrian models. 
\begin{wrapfigure}{r}{0.42\textwidth}
  \centering
  \vspace{-10pt}
  \includegraphics[width=1\linewidth]{polycoeff_pillar.pdf}
  \includegraphics[width=1\linewidth]{polycoeff_RSN.pdf}
  \caption{\textbf{Visualizing $L^{FL}_{\text{Poly-1}}$ and  $L^{FL}_{\text{Poly-1}^*}$ in the PolyLoss framework}. }
  \vspace{-10pt}
  \label{fig:poly_wod}
\end{wrapfigure}

\vspace{-5pt}
\textbf{Advancing the state-of-the-art with RSN.}
RSN segments foreground points from the 3D point cloud in the first stage, and then applies sparse convolution to predict 3D bounding boxes from the selected foreground points. RSN uses the same focal loss as the PointPillars model, i.e., $L_{\text{FL}}=-(1 - P_t)^2 \log P_t$. Since the optimal $L^{\text{FL}}_{\text{Poly-1}}$ for PointPillars ($\epsilon_1=-1$) is equivalent to dropping the first polynomial, we adapt the same loss formulation for RSN and tune the new leading polynomial $(1-P_t)^4$ by defining $L^{\text{FL}}_{\text{Poly-1}^*}=-(1-P_t)^2\log(P_t) - (1-P_t)^3 + \epsilon_2(1-P_t)^4$, shown in \autoref{fig:poly_wod}. We follow the same training schedule optimized for focal loss described in \citet{rsn} without adjustment. Our results, in \autoref{table:3d}, show that tuning the new leading polynomial improves all metrics (except vehicle detection BEV APH L2) for the SOTA 3D detector.