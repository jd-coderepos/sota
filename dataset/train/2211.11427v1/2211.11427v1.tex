\documentclass{article}




\usepackage[final]{neurips_2022}
\newcommand{\jf}[1]{{\color{blue}{#1}}}










\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{bm}
\usepackage{amsmath}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green
            ]{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{enumitem} 
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{enumitem} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{makecell}



\makeatletter
\newcommand{\ssymbol}[1]{}
\makeatother

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\tabfootnotesize}{\fontsize{8}{9}\selectfont}

\newcommand{\myparagraph}[1]{\textbf{#1}\hspace{1.8ex}}
\newcommand{\mysubparagraph}[1]{\textit{#1}\hspace{1.8ex}}

\title{Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations}






\author{Peng Jin\thanks{Equal contribution.} \quad
    Jinfa Huang \quad
    Fenglin Liu \quad
    Xian Wu \quad
    Shen Ge \quad
    Guoli Song
    \And
    David A. Clifton \quad
    Jie Chen\thanks{Corresponding author: Guoli Song, Jie Chen.}\5pt]
    \{jp21, jinfahuang\}@stu.pku.edu.cn \quad \{songgl, chenj\}@pcl.ac.cn\\
    \{shenge, kevinxwu\}@tencent.com \quad \{fenglin.liu, david.clifton\}@eng.ox.ac.uk
}


\begin{document}
\newcommand*{\dif}{\,\mathrm{d}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\setcounter{footnote}{0}
\maketitle

\begin{abstract}
Most video-and-language representation learning approaches employ contrastive learning, e.g., CLIP \cite{radford2021learning}, to project the video and text features into a common latent space according to the semantic similarities of text-video pairs. However, such learned shared latent spaces are not often optimal, and the modality gap between visual and textual representation can not be fully eliminated. In this paper, we propose Expectation-Maximization Contrastive Learning (EMCL) to learn compact video-and-language representations. Specifically, we use the Expectation-Maximization algorithm to find a compact set of bases for the latent space, where the features could be concisely represented as the linear combinations of these bases. Such feature decomposition of video-and-language representations reduces the rank of the latent space, resulting in increased representing power for the semantics. Extensive experiments on three benchmark text-video retrieval datasets prove that our EMCL can learn more discriminative video-and-language representations than previous methods, and significantly outperform previous state-of-the-art methods across all metrics. More encouragingly, the proposed method can be applied to boost the performance of existing approaches either as a jointly training layer or an out-of-the-box inference module with no extra training, making it easy to be incorporated into any existing methods\footnote[3]{Code : \href{https://github.com/jpthu17/EMCL}{https://github.com/jpthu17/EMCL}}.
\end{abstract}


\section{Introduction}
Text-video retrieval \cite{Yu2017Retrieval}, which aims to fetch relevant videos using textual queries or vice versa, is an important yet challenging cross-modal task. The dominating paradigm of text-video retrieval is contrastive learning \cite{wu2018unsupervised,hjelm2018learning,chen2020a,he2020momentum,chen2021an,chen2020improved}, which is a commonly adopted framework for video-and-language representation learning. The core idea of contrastive learning is to pull the textual and visual representations of matched text-video pairs together and push the representations of unmatched text-video pairs apart. In this manner, contrastive learning enables neural networks to learn discriminative video-and-language representations.

However, standard contrastive learning has intrinsic limitations for text-video retrieval tasks, since the success of contrastive learning largely depends on the volume and variety of negative samples \cite{chen2020a}. Without adequate negative samples, it would be hard to guide the direction of sample learning, and the features could not be contrasted well. As shown in Figure~\ref{risk}a, we randomly select three videos (each video has 20 text captions) from the benchmark dataset \cite{xu2016msr} and analyze the entire feature space of a recent standard contrastive learning method \cite{gabeur2020multi}. Here we visualize the feature space with t-SNE \cite{maaten2014accelerating}. In an ideal feature space, the instances of the same semantic classes should be close to each other. However, as shown in Figure~\ref{risk}a, we notice that the feature space learned by standard contrastive learning fails to preserve inter-modal semantic relatedness, where videos and texts with the same class semantics are still far away. In other words, the semantic-relevant but modal-different instances can not be grouped together in this feature space. To improve the performance of contrastive learning, recent image-text representation learning methods either increase batch sizes or maintain large data memory banks \cite{chen2020a,chen2021an,misra2020self,wu2018unsupervised,he2020momentum}. These works target to collect sufficient negative samples for contrastive learning. However, such text-video retrieval approaches are relatively expensive due to the incurred extra computational costs, especially on large-scale datasets \cite{sun2019videobert}. So an urgent challenge here is to find an efficient method to learn a semantically relevant feature space. 
 
 \begin{figure}[tbp]
\centering
\includegraphics[width=0.98\textwidth]{motivation.pdf}
\caption{
Visualization of video and language representations learned by (a) a recent standard contrastive learning method \cite{gabeur2020multi} and (b) our expectation-maximization contrastive learning method. 
(a) shows that although the representations belonging to the same class are well-clustered, there is still a clear dividing line (see Hyperplane) between video representations and language representations. Besides, the representations with different semantics (i.e., from different semantic classes) overlap with each other. On the contrary, (b) shows that our method can learn more discriminative video-and-language representations. In detail, the video-and-language representations with the same semantics (i.e., belonging to the same semantic class) are well-grouped; and there are clear dividing spaces between different semantic classes.
}
\vspace{-1.0em}
\label{risk}
\end{figure}

To efficiently bridge the modality gap and group visual and textual representation according to semantics, we propose to learn a low-rank and compact latent feature space. For this purpose, we propose a novel method named Expectation-Maximization Contrastive Learning (EMCL), which uses the same parametric model to abstract and reconstruct both textual and visual representations. In detail, we find a set of reconstruction bases for subspace representation by estimating the parameters in Expectation-Maximization (EM) algorithm \cite{dempster1977maximum}. In the learned subspace, both video and text features are represented with the distributions over the same sets of hidden variables, which can preserve strong semantic relations across modalities. As shown in Figure~\ref{risk}b, our EMCL effectively learns a semantically related subspace which has smaller intra-class variance and larger inter-class variance compared to the previous contrastive learning method (Figure~\ref{risk}a). We further propose EMCL-Net based on the EMCL and apply EMCL-Net to the task of text-video retrieval. In particular, for better adapting the proposed EMCL into the downstream task, our EMCL-Net introduces a parameter initialization strategy of the EM algorithm. Experimental results on three text-video retrieval benchmark datasets, i.e., MSR-VTT \cite{xu2016msr}, ActivityNet \cite{krishna2017dense}, and LSMDC \cite{rohrbach2015a}, show the advantages of the proposed EMCL. The main contributions are as follows:
\vspace{-5pt}
\begin{itemize}
    \item We identify the intrinsic limitation of contrastive learning for text-video retrieval, i.e., the cross-modal representation bias could not be fully eliminated via standard contrastive learning approaches.

    \item To alleviate such limitation, we reformulate the contrastive learning for video-and-language representations into an expectation-maximization iteration manner and propose a plug-and-play feature projection module named Expectation-Maximization Contrastive Learning (EMCL), which learns the subspace that aims to become semantic-relevant representation.

    \item Based on our EMCL, we further propose the EMCL-Net, which introduces a parameter initialization strategy for the EM algorithm. Experiments show that our approach achieves state-of-the-art results on three text-video retrieval datasets. More encouragingly, our method can be easily applied to boost the performances of existing approaches either as a jointly training layer or an out-of-the-box inference module with no extra training.
\end{itemize}

\vspace{-5pt}
\section{Related Work}
\vspace{-5pt}
Cross-modal learning is widely studied in many areas, including cross-modal retrieval \cite{Yu2017Retrieval,wu2018unsupervised,hjelm2018learning}, transfer learning \cite{phung2021learning,neyshabur2020being}, domain adaptation \cite{stojanov2021domain,liang2021pareto}, and captioning \cite{liu2020prophet} in which different modalities/domains come from different distributions. The main challenge of cross-modal learning is to use given vision-and-language pairs to learn common representations shared between modalities \cite{liu2019MIA}.
Most existing works of text-video retrieval \cite{chen2020a,he2020momentum,gorti2022x} map text and video to the same latent space, where the similarity between them can be directly calculated \cite{gabeur2022masking,cao2022visual,wang2022many,dong2021dual,wei2021universal,wray2021semantic,chen2021learning,croitoruteachtext,yang2021taco,qi2021semantics}. In detail, CE \cite{liu2019use} introduces a mixture-of-experts method that mixes the features of many different pre-training experts; MMT \cite{gabeur2020multi} shows a multi-modal transformer with multiple self-attended layers for video embedding. Recently, contrastive learning methods, e.g., CLIP \cite{radford2021learning}, show great success in advancing the state-of-the-art performances of cross-modal tasks \cite{luo2021clip4clip}.
Contrastive learning methods \cite{dosovitskiy2014discriminative,cao2022locvtp,he2020momentum,misra2020self,chen2020improved,liu2021CA} try to learn data representations from positive and negative pairs, making the representations of positive pairs (usually data augmentations that retain semantic information) have high similarity, and negative pairs (different semantic examples) have low similarity. Inspired by the great success of contrastive learning, we employ CLIP \cite{radford2021learning} for learning video-and-language representations. In this paper, a positive pair is a matching text-video pair, and a negative pair contains non-matching text and video. However, due to the multi-modal nature and spatial-temporal evolution of video \cite{chen2019weakly,liu2017hierarchical}, it is essential to capture the most important semantic concept for the video. Meanwhile, the visualization in Figure~\ref{risk} also shows that the feature space of video-and-language representations has semantically irrelevant redundancy which leads to non-ideal contrast in the feature space. \cite{liang2022mind} analyzes the non-optimal feature space with modality gap impacts model's performance and fairness.
Therefore, we propose to conduct contrastive learning in an expectation-maximization iteration manner to eliminate the non-optimal semantically redundant dimensions, acquiring compact representations.

\vspace{-5pt}
\section{Approach}
\vspace{-5pt}
In this section, we first introduce how to reformulate the Contrastive Learning into an expectation-maximization iteration manner, i.e., Expectation-Maximization Contrastive Learning (EMCL). Then, we introduce how to incorporate the proposed EMCL into the neural network for video-and-language representations, which are used to perform the text-video retrieval task.

\subsection{Preliminaries}\label{assumptions0}
\myparagraph{Expectation-Maximization Algorithm}
The Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} is an iterative optimization strategy, which was originally designed to solve the problem when data are missing in the process of parameter estimation. Briefly, given unobserved hidden variables  and observed data sets  with  samples, the goal of EM algorithm is to estimate the maximum likelihood solution of model parameters .
In step E, the EM algorithm calculates the conditional probability expectation . In the M step, the likelihood function is maximized to get the revised parameter .

\myparagraph{Gaussian Mixture Model} The Gaussian Mixture Model (GMM) \cite{richardson1997on} combines multiple single Gaussian models. Assuming that GMM consists of  Gaussians, given the input  with hidden variables , the probability of GMM is as follows:


\begin{minipage}{0.57\textwidth}
\begin{algorithm}[H]\small
\small
\caption{\small{The proposed Expectation-Maximization Contrastive Learning, with  iterations of routing. Typically, .}}
\label{alg:EMCL}
\begin{algorithmic}[1]
\REQUIRE  (Video features and text features),  (Initial Value Maintenance)
\STATE Initialize  with a splice of  copied  times
\FOR {routing iteration }
\STATE   
\FOR{}
\STATE    \# Eq.~\eqref{_E}
\ENDFOR
\STATE   
\FOR{ subspace }
\STATE   \quad \quad \quad \quad \quad \quad \quad \quad \# Eq.~\eqref{_lambda}
\ENDFOR
\ENDFOR 
\STATE Reconstruct  with   \quad \quad  \# Eq.~\eqref{Reconstruction}
\STATE Update  with  \quad \quad \ \ \# Eq.~\eqref{Initial Value Maintenance}
\RETURN 
\end{algorithmic}
\end{algorithm}
\vspace{0.em}
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{module.pdf}
\vspace{-1.5em}
\caption{\textbf{Module overview.} For clarity, we show the case when the number of subspaces . The input features come from  samples, which are colored differently. EMCL projects input features into  subspaces to filter out the redundancy in features.}
\label{fig00}
\end{figure}
\vspace{0.em}
\end{minipage}

where  is the probability function of the  Gaussian and  is the prior probability.

Through the E step of the EM algorithm, the estimated value of  can be obtained by:


In the M step of the EM algorithm, the parameters of the Gaussian mixture model are updated as:



\subsection{Expectation-Maximization Contrastive Learning (EMCL)} \label{assumptions1}

In this section, we introduce our EMCL in detail.
Representing the features in low-dimensional space is a fundamental method to eliminate the inherent redundancies in features, which is also the goal of our method. We leverage these compact features for contrastive learning to efficiently bridge the gap and improve their performance. The overview of our EMCL is shown in Figure~\ref{fig00} and Algorithm~\ref{alg:EMCL}.

Mathematically, the subspace can be defined as follows: let  be the features, where  is the sample size,  is the dimension of the feature. Suppose that these features are distributed in many unknown linear subspaces, where the intrinsic dimensions of the linear subspaces are less than . We define the union of  semantically related low-dimensional linear subspaces as the semantic subspace.
To this end, our method includes two parts: Firstly, we use the EM algorithm \cite{dempster1977maximum} to find the bases of the  optimal subspaces, which is called ``\textbf{Maximum Probability Projection}''; Secondly, we need to re-represent the features in these subspaces, which is called ``\textbf{Feature Reconstruction}''; Finally, in the ``\textbf{Contrastive Learning}'', the compact features we re-represent are used in contrastive learning to bridge the gap and improve their performance. 



\myparagraph{Maximum Probability Projection}
Given original video features  and original text features , we combine both video features and text features to be the input data , where  is the  coding bit of the  sample. In our method, each coding bit  is assigned to a specific subspace, and meanwhile we introduce the hidden variable , where  represents whether the  subspace is selected by . We take the base  of the subspace as the parameter of EM algorithm \cite{dempster1977maximum}, where  represents the distribution of samples in  semantically related low-dimensional linear subspaces.
Given input data  and hidden variable , the complete likelihood of the input feature  can be expressed as:

where  is the probability function and  is a kernel function. Here we assume that  can be represented by  shared Gaussian distributions like GMM \cite{richardson1997on}. For simplicity, we freeze  and , and only consider the mean  of the Gaussian model.

There are many choices for the kernel function , such as linear kernel , polynomial kernel , Gaussian kernel  and so on. We find that different kernel functions have slightly different impacts on the final results.
For easy implementation, we use Gaussian kernel and rewrite it in a form similar to the attention model \cite{bahdanau2015neural,vaswani2017attention}. The formula is:

where  is a hyper-parameter to adjust the distribution, similar to the mean and covariance in the Gaussian distribution. In this paper, we define  to be the  column of , and  to be the  column of .

Reviewing the EM algorithm \cite{dempster1977maximum}, it estimates the parameters of the model by continuously executing E steps and M steps. In step E, it calculates the conditional probability expectation with current parameters. In step M, it maximizes the likelihood function to update the parameters.

In step E, referring to the Eq. (\ref{Y}), the estimated value of  can be obtained by:


In the M step, we update base  according to Eq. (\ref{lambda}), which is formulated as:


By repeatedly iterating step E and step M, our algorithm forces this set of optimal subspaces to represent the original video features and original text features at the same time. After several iterations, we could keep the information that appears in both the video and the text to remove the redundancy.

\myparagraph{Feature Reconstruction}
When reconstructing features, we use  and  to linearly reconstruct the original features. Learning from the meaning of  and  mentioned above,  indicates whether the  subspace is selected by  and  represents the base of the  subspace. The formula for feature reconstruction is:


As shown in Eq. (\ref{Reconstruction}), we only use the corresponding base  (for all ) to reconstruct the  sample . Therefore, we estimate different  from  subspaces. 

\myparagraph{Contrastive Learning}
The above feature reconstruction method generates the low-rank and compact features stripped of redundancy. Therefore, we can leverage these compact features for contrastive learning to efficiently bridge the modality gap and improve task performance.


\vspace{-5pt}
\subsection{EMCL for Cross-Modal Learning}
\vspace{-5pt}
In this section, we discuss in detail how to incorporate the EMCL into the neural network for cross-modal downstream tasks.
Targeting on the cross-modal tasks, we propose the ECML-Net, which introduces a parameter initialization strategy for the EM algorithm that can establish the connection between batches.
At last, we take an important yet challenging cross-modal task, i.e., the text-video retrieval, as an example to illustrate how to employ our EMCL-Net to perform downstream tasks. 

\myparagraph{Initial Value Maintenance}
Considering that a corpus of video-based cross-modal tasks usually contains thousands of text-video pairs, it is often not enough to only establish connections between samples within single batches. Therefore, our module builds an inter-batch information transfer mechanism by maintaining a separate initial value .
In implementations, each time a new batch of features is fed into the model, the EM algorithm needs to initialize the . Rather than randomly initializing  for each batch, we use  as the initial value of the . For all , we initialize  with . We update  using an average moving method similar to the Batch Normalization (BN) layer \cite{ioffe2015batch}. The moving method is formulated as:

where  is the momentum. Similar to the average moving method in the BN layer, we don't update  in the inference stage. Because the initial value is crucial to the stability of the EM algorithm \cite{abdolali2021beyond,li2019expectation}, we need to limit the value range of the initial value. In each iteration, we use the L2 normalization operation to limit the value range of .

\myparagraph{EMCL-Net}
For fair comparisons, we follow common practice \cite{luo2021clip4clip,cheng2021improving,wang2022disentangled} to extract the video representations of input videos and the language representations of input texts. In detail, for video representations, we first extract the frames from the video clip as the input sequence of video; Then we use ViT \cite{dosovitskiy2021an} to encode the frame sequence, by exploiting the transformer architecture to model the interactions between image patches; Followed by the CLIP \cite{radford2021learning}, the output from the [class] token is used as the frame embedding; Finally, we aggregate the embedding of all frames and obtain the video representation . For text representation, we directly use the text encoder of CLIP to acquire the text representation .

As a plug-and-play module, our proposed EMCL is inserted at the end of the video-text encoders. 
We concatenate video features  and text features , generating the input data of our method . Following \cite{chun2021probabilistic}, we add reconstructed features  by the EMCL and original features  to obtain final text-video representations  
where  is the scale factor. 
Here,  is the scale factor which is used to balance the original and new embeddings. By adjusting , we can add some flexibility in the reconstructed video-and-language representations for the robustness of the model.

\myparagraph{Training Objective}
During the training stage, the EMCL module is trained with the neural network. We use  to initialize the . Then the components  and  are updated in unsupervised way by iteration, as shown in Algorithm 1. Moreover, we update the initial value  using an average moving method.
The goal of the EMCL-Net is to map text and video into a joint representation space to measure the text-video similarity. 
Following common practice, we use cosine similarity  as the similarity measure between text  and video , where  represents the reconstructed text representation of text ; and  represents the reconstructed video representation of video . 
We train our model with  InfoNCE loss \cite{van2018representation}:

where  is the batch size and  is the temperature hyper-parameter. This loss function maximizes the similarity of positive pairs  and minimizes the similarity of negative pairs.


During the inference stage, given a set of queries (text/video) and a set of candidates (videos/texts), we use the trained  to initialize the . Then  and  can be updated in an unsupervised way by iteration.
The goal of cross-modal retrieval task is to map the query and candidate into a joint video-and-language representation space to measure the query-candidate similarity .
In this way, we can output the similarity scores for all the input candidates, and take the candidates with the Top-1/5/10/50 similarity scores with input query as the final prediction.

\section{Experiments}
\myparagraph{Datasets, Metrics and Implementation Details}\label{Datasets_Metrics_and_Implementation_Details}
\mysubparagraph{Datasets.}
We conduct the experiments on three popular text-video  retrieval datasets, i.e., {MSR-VTT} \cite{xu2016msr}, {ActivityNet Captions} \cite{krishna2017dense}, {LSMDC} \cite{rohrbach2015a}, and follow common practice \cite{luo2021clip4clip,cheng2021improving,wang2022disentangled} to pre-process the datasets for fair comparison. In detail, {MSR-VTT} \cite{xu2016msr} contains 10,000 videos, each with 20 text descriptions; We follow the 1k-A split \cite{liu2019use} with 9,000 videos for training and 1,000 for testing. {ActivityNet Captions} \cite{krishna2017dense} contains 20,000 videos with multiple sentence descriptions; We report results on the "vall" split (10,009 training, 4,917 testing) as in \cite{gabeur2020multi}. {LSMDC} \cite{rohrbach2015a} contains 118,081 video clips from 202 movies; We follow the split of \cite{gabeur2020multi} with 1,000 videos for testing.

\mysubparagraph{Metrics.}
We choose standard retrieval metrics: Recall at K (R@K) and Median Rank (MdR) to evaluate the text-to-video and video-to-text retrieval performance.

\mysubparagraph{Implementation Details.}
We utilize the CLIP (ViT-B/32) \cite{radford2021learning} equipped with Temporal Transformer \cite{luo2021clip4clip} as pre-trained Bi-Encoder (Base Model). Following previous works \cite{luo2021clip4clip}, the frame length and caption length are 12 and 32 for MSR-VTT and LSMDC. For ActivityNet, a long video retrieval dataset, we set the frame length to 64 and caption length to 64. We follow training schedules from previous works \cite{luo2021clip4clip,cheng2021improving,wang2022disentangled}. Concretely, we use the Adam optimizer \cite{kingma2014adam} with a linear warmup. The initial learning rate is 1e-7 for text encoder and video encoder and 1e-4 for other modules. 
We set the temperature , , the momentum , the number of iterations is set to 9 and the parameter  is set to 32. The network is optimized with the batch size of 128 in 5 epochs. 
During the training stage, the EMCL module is trained with the neural network. Each time a new batch of features is fed into the model, we use  to initialize the . Then the components  and  are updated by iteration. Moreover, we update the initial value M using an average moving method.
During the inference stage, given a set of queries (text/video) and a set of candidates (videos/texts), we use the trained  to initialize the . Then the components  and  can be updated by iteration.
When the EMCL module is incorporated into trained baselines as an out-of-the-box inference module with no extra training,  is randomly initialized. Then the components  and  can be updated in an unsupervised way by iteration.
\begin{table}[t]
\centering
\caption{Comparisons to current state-of-the-art methods on the MSR-VTT \cite{xu2016msr}, ActivityNet \cite{krishna2017dense} and LSMDC \cite{rohrbach2015a} datasets. ``'' denotes higher is better. ``'' denotes lower is better. \ssymbol{8} denotes employing inverted softmax~\cite{cheng2021improving,bogolin2022cross}.}
\label{Comparisons_to_State-of-the-arts}
\tabfootnotesize
\subfloat[Retrieval performance on the \textbf{Text-\textgreater{}Video} task.]
{
\setlength{\tabcolsep}{0.5pt}
\begin{tabular}{l|c|cccc|cccc|cccc}
\specialrule{.08em}{0pt}{0pt} \multirow{2}{*}{Methods} & Pre-trained
&  \multicolumn{4}{c|}{MSR-VTT \cite{xu2016msr}}        & \multicolumn{4}{c|}{ActivityNet \cite{krishna2017dense}} & \multicolumn{4}{c}{LSMDC \cite{rohrbach2015a}}      \\  
&weights & R@1 & R@5 & R@10 & MdR 
& R@1 & R@5 & R@50 & MdR 
& R@1 & R@5 & R@10 & MdR \\ \specialrule{.05em}{0pt}{0pt}
JSFusion \cite{yu2018a} &- & 10.2 & 31.2 & 43.2 & 13.0 & - & - & - & - &9.1 &21.2 &34.1 &36.0 \\
CE \cite{liu2019use} &GPT-1 & 20.9 & 48.8 & 62.4 & 6.0 &18.2 &47.7 &91.4 &6.0  &11.2 &26.9 &34.8 &25.3\\
MMT \cite{gabeur2020multi} &BERT-Base & 24.6 & 54.0 & 67.1 & 4.0 &22.7 &54.2 &93.2 &5.0 &13.2 &29.2 &38.8 &21.0 \\
Support-Set \cite{patrick2021support} &T5-Base & 27.4 & 56.3 & 67.7 & 3.0 &26.8 &58.1 &93.5 &3.0 & - & - & - & -\\
T2VLAD \cite{wang2021t2vlad}  &BERT-Base & 29.5 & 59.0 & 70.1 &4.0 &23.7 &55.5 &93.5 &4.0 &14.3 &32.4 &42.2 &16.0 \\
CLIP4Clip \cite{luo2021clip4clip} &CLIP (ViT-B/32) & 44.5 & 71.4 & 81.6 &2.0  & 40.5 & 72.4 & \bf 98.1 & 2.0 & 22.6 & 41.0 & 49.1 &11.0  \\
\specialrule{.05em}{0pt}{0pt}
\rowcolor{gray!10} EMCL-Net (Ours) &CLIP (ViT-B/32) & 46.8 & 73.1 & 83.1 & 2.0 & 41.2 & 72.7 & \bf 98.1 & 2.0 & 23.9 & 42.4 & 50.9 & 10.0 \\
\rowcolor{gray!10} EMCL-Net (Ours)\ssymbol{8} &CLIP (ViT-B/32) & \textbf{51.6} & \textbf{78.1} & \textbf{85.3} & \textbf{1.0}& \textbf{50.6} & \textbf{78.7} & \textbf{98.1} &\textbf{1.0} &\textbf{25.9}  &\textbf{46.4} &\textbf{53.7} &\textbf{8.0} \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
} 
\\\vspace{-.5em}
\subfloat[Retrieval performance on the \textbf{Video-\textgreater{}Text} task.]
{
\setlength{\tabcolsep}{0.5pt}
\begin{tabular}{l|c|cccc|cccc|cccc}
\specialrule{.08em}{0pt}{0pt} \multirow{2}{*}{Methods} &Pre-trained
&  \multicolumn{4}{c|}{MSR-VTT \cite{xu2016msr}}        & \multicolumn{4}{c|}{ActivityNet \cite{krishna2017dense}} & \multicolumn{4}{c}{LSMDC \cite{rohrbach2015a}}      \\  
& weights & R@1 & R@5 & R@10 & MdR 
& R@1 & R@5 & R@50 & MdR 
& R@1 & R@5 & R@10 & MdR \\ \specialrule{.05em}{0pt}{0pt}
CE \cite{liu2019use} &GPT-1  & 20.6 & 50.3 & 64.0 & 5.3 &17.7 &46.6 &90.9 &6.0 & - & - & - & - \\
MMT \cite{gabeur2020multi} &BERT-Base  & 24.4 & 56.0 & 67.8 & 4.0 &22.9 &54.8 &93.1 &4.3 &12.1 &29.3 &37.9 &22.5 \\
Support-Set\cite{patrick2021support} &T5-Base &26.6 &55.1 &67.5 &3.0  &25.5 &57.3 &93.5 &3.0 & - & - & - & - \\
T2VLAD \cite{wang2021t2vlad} &BERT-Base & 31.8 & 60.0 & 71.1 & 3.0 &24.1 &56.6 &94.1 &4.0  &14.2 &33.5 &41.7 &17.0 \\
CLIP4Clip \cite{luo2021clip4clip} & CLIP (ViT-B/32) & 42.7 & 70.9& 80.6 & 2.0 & 42.5 & 74.1 & 98.1 & 2.0 & 20.8 & 39.0 & 48.6 & 12.0\\
\specialrule{.05em}{0pt}{0pt}
\rowcolor{gray!10} EMCL-Net (Ours) & CLIP (ViT-B/32) & 46.5 & 73.5 & 83.5 & 2.0 & 42.7 & 74.0 & 98.3 & 2.0 & 22.2  & 40.6 & 49.2 & 12.0 \\
\rowcolor{gray!10} EMCL-Net (Ours)\ssymbol{8} & CLIP (ViT-B/32) & \textbf{51.8} & \textbf{80.2} & \textbf{88.0} & \textbf{1.0} & \textbf{50.6} & \textbf{78.9} & \textbf{98.4} & \textbf{1.0} &\textbf{26.7}  &\textbf{44.7} &\textbf{54.4} &\textbf{8.0} \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
}
\vspace{.5em}
\centering
\caption{Generalization analysis of our EMCL on the MSR-VTT dataset \cite{xu2016msr}. We equip our EMCL with three strong contrastive learning baselines.
\ssymbol{2} denotes our own re-implementation of baselines;
\ssymbol{3} denotes the EMCL is trained jointly with the baselines from scratch;
\ssymbol{4} denotes the EMCL is incorporated into trained baselines as an out-of-the-box inference module with no extra training.
We conducted 5 runs with different seeds for all experiments, the t-tests indicate that   0.01. The (+Number) denotes the absolute improvements.}
\label{tab:ECML}
\tabfootnotesize
\begin{tabular}{l|lll|lll}
\specialrule{.08em}{0pt}{0pt} 
\multirow{2}{*}{Methods} &  \multicolumn{3}{c|}{\textbf{Text-\textgreater{}Video}}        & \multicolumn{3}{c}{\textbf{Video-\textgreater{}Text}}  \\  
& R@1 & R@5 & R@10  
& R@1 & R@5 & R@10      \\ \specialrule{.05em}{0pt}{0pt}
MMT \cite{gabeur2020multi}\ssymbol{2}  & 25.9 & 54.8 & 68.5  & 26.0 & 58.2 & 69.3  \\
\rowcolor{gray!10}  \ + EMCL (Ours)\ssymbol{4} & 26.2 (+0.3) & 57.2 (+2.4) & \bf 70.8 (+2.3)   & 27.2 (+1.2)  & \bf 59.8 (+1.6)  & \bf 70.4 (+1.1)  \\
\rowcolor{gray!10}  \ + EMCL (Ours)\ssymbol{3} & \bf 27.1 (+1.2)  & \bf 57.6 (+2.8)  &70.5 (+2.0)    & \bf 27.8 (+1.8)  & 59.3 (+1.1)  & 69.8 (+0.5)    \\
\specialrule{.05em}{0pt}{0pt}
CLIP4Clip \cite{luo2021clip4clip}\ssymbol{2} & 43.4 & 70.4 & 78.5   & 42.4 & 68.6& 79.2   \\
\rowcolor{gray!10} \ + EMCL (Ours)\ssymbol{4} & 44.6 (+1.2)  & 71.4 (+1.0)  & 79.5 (+1.0) & 45.0 (+2.6)  & 71.2 (+2.6)  & 79.2 (+0.0)          \\
\rowcolor{gray!10}  \ + EMCL (Ours)\ssymbol{3} & \bf 46.9 (+3.5)  & \bf 72.5 (+2.1)  & \bf 81.8 (+3.3)    & \bf 46.6 (+4.2)  & \bf 73.3 (+4.7)  & \bf 82.3 (+3.1)        \\
\specialrule{.05em}{0pt}{0pt}
DCR \cite{wang2022disentangled}\ssymbol{2} & 46.8 & 72.6 & 82.6   & 45.8 & 72.1& 82.1   \\
\rowcolor{gray!10} \ + EMCL (Ours)\ssymbol{4} & 47.5 (+0.7)  & \bf 74.9 (+2.3)  & \bf 83.8 (+1.2)   & \bf 45.9 (+0.1)  & \bf 73.6 (+1.5)  & \bf 83.5 (+1.4)    \\
\rowcolor{gray!10}  \ + EMCL (Ours)\ssymbol{3} & \bf 48.0 (+1.2)  & 73.7 (+1.1)  & 83.0 (+0.4)  & \bf 45.9 (+0.1)  & 73.5 (+1.4)  & \bf 83.5 (+1.4)  \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
\vspace{-.5em}
\end{table}


\myparagraph{Comparisons to State-of-the-art}
Table~\ref{Comparisons_to_State-of-the-arts} shows the results of our method on three text-video retrieval datasets. As we can see, our ECML-Net method  consistently outperforms the recently proposed state-of-the-art methods on both text-to-video retrieval and video-to-text retrieval tasks across all datasets and metrics. 


\myparagraph{Generalization Analysis}
Since the input and output dimensions of the EMCL module are the same, it is model-agnostic and can be applied to features extracted from any language and video encoders. 
Therefore, we further equip our EMCL with three strong baseline models, i.e., MMT \cite{gabeur2020multi}, CLIP4Clip \cite{luo2021clip4clip}, DCR \cite{wang2022disentangled}, and evaluate the performance of these models on the MSR-VTT datasets.
Specifically, we insert the EMCL module at the end of the video-text encoders.
Table~\ref{tab:ECML} shows that our EMCL can be applied to successfully boost all baselines either as a jointly training layer or an out-of-the-box inference module with no extra training. Overall, our approach can boost the baselines with the most significant improvement up to 3.5\% and 4.2\% for text-to-video task and video-to-text task in terms of R@1 metric, respectively. The significant improvements demonstrate the generalization ability of EMCL. 


\begin{table}[t]
\tabfootnotesize
\begin{minipage}[c]{0.48\textwidth}
\caption{Comparisons to other baseline methods on MSR-VTT dataset \cite{xu2016msr}. We perform the analysis on the \textbf{Text-\textgreater{}Video} task.  is the sample size.  is the dimension of the original feature.  is the number of subspaces. \ssymbol{8} denotes employing inverted softmax~\cite{cheng2021improving,bogolin2022cross}.}
\vspace{.5em}
\tabfootnotesize
\centering
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{l|ll|c}
\specialrule{.08em}{0pt}{0pt}
\multirow{2}{*}{{Methods}}
& \multicolumn{2}{c|}{{Complexity}} & \multirow{2}{*}{{R@1}}\\ 
 &\multicolumn{1}{c}{Time}  &\multicolumn{1}{c|}{Space}   \\ \specialrule{.05em}{0pt}{0pt}
Base Model & - & - & 42.4 \\ \specialrule{.05em}{0pt}{0pt}
\ + PCA &  & &36.0 \\
\ + Transformer & &  &41.3 \\
\ + Fully Connected Layers & &  &42.1 \\
\ + Sparse Autoencoders & &  &43.8 \\
\rowcolor{gray!10} \ + EMCL (Ours)  &\textbf{} &\textbf{}  & 46.8 \\
\rowcolor{gray!10} \ + EMCL (Ours)\ssymbol{8}  &\textbf{} &\textbf{}  &\textbf{51.6} \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
\label{Comparisons}
\end{minipage}
\hfill
\begin{minipage}[c]{0.48\textwidth}
\centering
\caption{Effect of the Parameter Initialization Strategy in our EMCL. \ssymbol{8} denotes employing inverted softmax~\cite{cheng2021improving,bogolin2022cross}.}
\label{tab:initialization}
\vspace{.5em}
\tabfootnotesize
\setlength{\tabcolsep}{1.5pt}
{
\begin{tabular}{l|c|cccc}
\specialrule{.08em}{0pt}{0pt} \multirow{2}{*}{{Methods}} & \multirow{2}{*}{{Initialization}} & \multicolumn{4}{c}{\textbf{Text-\textgreater{}Video}}          \\  
& & R@1 & R@5 & R@10 & MdR 
     \\ \specialrule{.05em}{0pt}{0pt}
Base Model & - & 42.4 & 70.8 & 80.6 & 2.0     \\
\specialrule{.05em}{0pt}{0pt}

\rowcolor{gray!10} \ + EMCL & -  & 43.3 & 71.8 & 81.6 & 2.0    \\
\rowcolor{gray!10} \ + EMCL &  & 46.8 & 73.1 & 83.1 & 2.0\\
\rowcolor{gray!10} \ + EMCL\ssymbol{8} & &\textbf{51.6} & \textbf{78.1} & \textbf{85.3} & \textbf{1.0}\\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
}
\\
\vspace{3pt}
{
\begin{tabular}{l|c|cccc}
\specialrule{.08em}{0pt}{0pt} \multirow{2}{*}{{Methods}} & \multirow{2}{*}{{Initialization}} & \multicolumn{4}{c}{\textbf{Video-\textgreater{}Text}}          \\  
& & R@1 & R@5 & R@10 & MdR 
     \\ \specialrule{.05em}{0pt}{0pt}
Base Model & - & 43.2 & 70.0 & 81.1 & 2.0     \\
\specialrule{.05em}{0pt}{0pt}

\rowcolor{gray!10} \ + EMCL & - & 45.1 & 72.8 & 82.3 & 2.0 \\
\rowcolor{gray!10} \ + EMCL &  & 46.5 & 73.5 & 83.5 & 2.0 \\
\rowcolor{gray!10} \ + EMCL\ssymbol{8} &  &\textbf{51.8} & \textbf{80.2} & \textbf{88.0} & \textbf{1.0} \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
}
\end{minipage}
\vspace{-1.5em}
\end{table}
\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth]{ablation.pdf}
\vspace{-1.5em}
\caption{Effect of (a) the number of subspaces ; (b) the iteration number ; (c) the Hyper-parameter  for our proposed EMCL module with inverted softmax~\cite{cheng2021improving,bogolin2022cross}.}
\label{effect_of_T}
\vspace{-.5em}
\end{figure}
\myparagraph{Comparisons to Other Baseline Methods}
We further compare our method with other baseline methods in Table~\ref{Comparisons}. 
PCA \cite{tipping1999probabilistic} is a popular method for finding salient features shared by the two modalities so that the modality gap can be reduced to a certain extent. ``Transformer'' represents using a shared transformer between two modalities, which can explicitly project the data from two modalities into a shared space. ``Fully Connected Layers'' represents using two shared fully connected layers between two modalities. ``Sparse Autoencoders'' is a common sparse autoencoder \cite{ng2011sparse}, which reduces the average response of the encoding layer and learns the compact representation. Different from these representative dimensionality reduction methods, the motivation of our method is to use contrastive learning to preserve video-text semantic relatedness. By maximizing the joint posterior probability of video and text, we find a semantically related subspace for compact video-text representation.
In contrast, PCA and its variants are feature dimension reduction methods, which aim to maximize the variance of projected data and cannot guarantee semantic relationships. Sparse autoencoder reduces the average response of the encoding layer for sparsity, which may potentially discard semantic information. In addition,
our method has linear complexity  and does not require additional training. In contrast, the complexity of PCA is  ( and ), and the autoencoder requires additional training.

\begin{table}[t]
\tabfootnotesize
\begin{minipage}[c]{0.60\textwidth}
\centering
\caption{Comparisons with state-of-the-art methods for video
captioning on MSR-VTT dataset~\cite{xu2016msr}. We report BLEU-4~\cite{papineni2002bleu},
METEOR~\cite{banerjee2005meteor}, ROUGE-L~\cite{rouge2004package}, and CIDEr~\cite{vedantam2015cider} metrics. All methods only use image modality as input.
}
\label{tab:caption}
\vspace{.5em}
\tabfootnotesize
\setlength{\tabcolsep}{2.5pt}
{
\begin{tabular}{l|cccc}
\specialrule{.08em}{0pt}{0pt} {Methods} & BLEU-4 & METEOR & ROUGE-L  &CIDEr \\ \specialrule{.05em}{0pt}{0pt}
STG-KD~\cite{pan2020spatio} & 40.5 & 28.3 & 60.9 & 47.1\\
ORG-TRL~\cite{zhang2020object} & 43.6 & 28.8 & 62.1 & 50.9    \\
MGCMP~\cite{Chen_2021_ICCV} & 41.7 & 28.9 & 62.1 & 51.4     \\
OpenBook~\cite{zhang2021open} & 42.8 & 29.3 & 61.7 & 52.9    \\
ARB-ACL~\cite{li2022adaptive} & 42.6 & 28.9 & 61.5 & 51.3     \\
DCD~\cite{yang2021clip} & 43.4 & 29.6 & 61.8 & 52.8     \\
\specialrule{.05em}{0pt}{0pt}
\rowcolor{gray!10} Base model & 45.2 & 29.8 & 63.0 & \bf 54.6     \\
\rowcolor{gray!10} \ + EMCL (Ours)  & \bf 45.3 & \bf 30.2 & \bf 63.2 & \bf 54.6 \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[c]{0.36\textwidth}
\caption{Comparisons with state-of-the-art methods for video question answering on MSRVTT-QA dataset~\cite{xu2017video}.}
\vspace{.5em}
\tabfootnotesize
\centering
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{l|c}
\specialrule{.08em}{0pt}{0pt}
{Methods} & Accuracy \\ 
 \specialrule{.05em}{0pt}{0pt}
ClipBERT~\cite{lei2021less} &37.4\\
VGT~\cite{xiao2022video} &39.7\\
VQA-T~\cite{yang2021just}  &41.5\\
SiaSamRea~\cite{yu2021learning} &41.6\\
MERLOT~\cite{zellers2021merlot} &43.1\\
Co-Tokenization~\cite{piergiovanni2022video} & 45.7\\
 \specialrule{.05em}{0pt}{0pt}
\rowcolor{gray!10} Base model & 45.0 \\ 
\rowcolor{gray!10} \ + EMCL (Ours)  &\textbf{45.8}  \\
\specialrule{.08em}{0pt}{0pt}
\end{tabular}
\label{tab:videoqa}
\end{minipage}
\vspace{-.5em}
\end{table}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{em.pdf} 
    \vspace{-1.5em}
    \caption{Visualization of EMCL iterative process.  
(a) shows the influence of reducing the intra-class variance and increasing the inter-class variance on the feature space.  
(b) shows the relationship between the intra-class variance and the iteration number on the MSR-VTT dataset.  
(c) shows the relationship between the inter-class variance and the iteration number on the MSR-VTT dataset.
}
    \label{4.0}
    \vspace{-.5em}
\end{figure}
\begin{figure}[t]
\includegraphics[width=1.0\textwidth]{visualization.pdf}
\centering
 \vspace{-1.0em}
    \caption{ Visualization of our EMCL. We randomly generate a set of 3D data containing 3 types of samples corresponding to 3 different text-video semantic centers. The figure on the top is the visualization of the original data, and the figure on the bottom is the visualization of EMCL's outputs.}
    \label{analysis_of_filter0}
    \vspace{-.5em}
    \vspace{-3pt}
\end{figure}


\myparagraph{Ablative Analysis}
\mysubparagraph{Effect of the parameter initialization strategy.} \label{limit}
As shown in Table~\ref{tab:initialization}, with or without parameter initialization strategy, our method can successfully promote the base model.
It is worth noting that the EM algorithm \cite{dempster1977maximum} is sensitive to initial values \cite{abdolali2021beyond,li2019expectation}. In other words, the convergence of EM algorithm depends mainly on initial parameters. Random initialization leads to large fluctuations in convergence results. This fatal flaw limits the performance of our module. Fortunately, with the proposed parameter initialization strategy, our EMCL method surpasses the base model by a large margin with 3.5\% R@1 and 1.4\% R@1 on the text-to-video and video-to-text tasks, respectively, proving the effectiveness of our parameter initialization strategy used for EMCL-Net. 

\mysubparagraph{Effect of the number of subspaces.} In Figure~\ref{effect_of_T}a, we show the effect of the number of subspaces . On the one hand, we find that fewer subspaces mean fewer semantic centers, which limits our moduleâ€™s ability to reconstruct the features. On the other hand, a larger number of subspaces requires more training data, which increases the cost of our model learning. We set the center size  
to achieve the best performance in practice.

\mysubparagraph{Effect of the iteration number.} In Figure~\ref{effect_of_T}b, we show the influence of EM iteration number . Overall performance improves slightly before leveling off. We find that the algorithm has converged when the number of iterations is 9, so we set the number of iterations to 9 as default.

\mysubparagraph{Hyper-parameter selection.} The parameter  is hyper-parameter to adjust the distribution (Eq.~\ref{sigma}), similar to the mean and covariance in the Gaussian distribution. We evaluate the scale range setting  as shown in Figure~\ref{effect_of_T}c. We find that R@1 is improved from 43.8\% to 51.6\% when  and saturated with . As a result, we adopt  to achieve the best performance.

\myparagraph{Generalize to other tasks}
\mysubparagraph{Video captioning.}
The purpose of video captioning is to describe the content of the video in fluent sentences. ``Base model'' uses CLIP~\cite{radford2021learning} to extract video features and is trained with cross-entropy loss. To generate higher-quality sentences, we apply EMCL between video features and ground-truth text features.
As shown in Table~\ref{tab:caption}, using EMCL brings significant improvements on caption quality, e.g., gaining a relative improvement of 0.4\% at METEOR.

\mysubparagraph{Video question answering.}
Visual question answering requires the model to predict an answer using visual information~\cite{li2022joint,li2022toward}. We use the target vocabulary for MSRVTT-QA dataset~\cite{xu2017video}, and train a fully connected layer on top of the final language features to classify the answer. ``Base model'' uses CLIP \cite{radford2021learning}, a transformer-based~\cite{dosovitskiy2021an,li2022locality} visual-language pre-training model, to extract video-and-language features and is trained with cross-entropy loss. To learn compact video-and-language representations, we apply EMCL between video features and question features. Table~\ref{tab:videoqa} shows that EMCL can be applied to boost video question answering successfully and boost the baseline with an improvement up to 0.8\%.

\myparagraph{Qualitative Analysis}
\mysubparagraph{Analysis of EMCL iterative process.} As shown in Figure \ref{4.0}a, reducing the intra-class variance makes videos and texts belonging to the same semantic class gather, and increasing the inter-class variance makes those belonging to different semantic classes separate from each other. The two conclusions we get from Figure \ref{4.0}b and Figure \ref{4.0}c are as follows.
(1) EMCL module reduces the intra-class variance of the same semantic classes and increases the inter-class variance of different semantic classes. 
(2) The number  of subspaces has a great influence on the EMCL module. The effect of the EMCL module is limited if  is too small, while the intra-class variance increases if  is too large. 

\mysubparagraph{Visualization.} 
To better understand EMCL, we provide the visualization form of both the original representations and the filtered representations. 
We notice that the EMCL module eliminates the redundant dimensions, and the features reconstructed by our module  are very compact in the feature space. 
As shown in Figure \ref{analysis_of_filter0}, even though features have intense noise in the redundancy dimension, the EMCL module still learns semantic information. The EMCL module forces the differences between classes in subspace to be more obvious than original, which is helpful for the contrastive learning to learn the semantic centers shared by videos and texts.

\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}
In this paper, we studied the intrinsic limitation of classic contrastive learning for text-video retrieval.  We found that the contrastive method in the entire representation space fails to preserve inter-modal semantic relatedness, which makes the features gather or separate in the subspace which is irrelevant to semantics. To mitigate this effect, we propose to directly learn the subspace that is related to shared semantics and do contrastive learning in it. By learning the subspaces related to semantics, we are able to learn the common semantic center of video and text in the semantic subspace. Further, it is worth noting that our method could be applied for other contrastive learning tasks, which include similar samples containing redundant dimensions or with a limited number of negative samples.

\myparagraph{Acknowledgements} This work is supported by the Nature Science Foundation of China (No. 61972217, 62081360152, 62006133, 32071459), Guangdong Basic and Applied Basic Research Foundation (No.2019B1515120049) and Guangdong Science and Technology Department (No. 2020B1111340056).
Also, this work is supported in part by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre; an InnoHK Project at the Hong Kong Centre for Cerebro-cardiovascular Health Engineering; and the Pandemic Sciences Institute, University of Oxford, Oxford, UK.

\bibliographystyle{plain}
\bibliography{main}
\end{document}