\section{Methodology}

Our approach comprises two algorithmic steps: computation of the individual image shock graphs across a dataset and subsequently training/evaluating a graph neural network on this collection. Given an input image we initially compute an edge map using the Structured Forest approach of~\cite{Dollar:Zitnick:PAMI15}.  The orientation of the edges are corrected using the third-order approach of~\cite{Tamrakar:Kimia:ICCV07}. These edges are subsequently grouped into contour fragments (ordered set of edges) utilizing~\cite{Guo:etal:ECCV14}. The resulting set of contour fragments, typically on the order of eighty to hundred, are fed into the computational algorithm of~\cite{Tamrakar:Kimia:Shock} to produce the shock graph output.


The choice of Graph Neural Network architecture \ie type of graph convolution, number of layers, activation function, \etc is determined by experimentation on STL-10~\cite{Coates:etal:AISTATS11}. We picked this dataset as it is outside the domain generalization community and will not bias our results in any way. We utilized the Deep Graph Library~\cite{wang2019dgl} to evaluate three common graph architectures: Topological Adaptive Networks~\cite{Du:etal:CorR17} (TAG), Graph attention networks~\cite{Velickovic:etal:ICLR18} (GAT),  and graph convolutional networks~\cite{Kipf:Welling:ICLR17} (GCN)) and chose the architecture that performed best on our benchmark dataset. TAG achieved 58\%, while GAT  and GCN achieved 56.2\% and 54.3\%, respectively. Our final network depth is four layers, composed of three TAG convolutional layers and an ultimate linear layer coupled with SoftMax activation to produce class scores. The input layer transforms the initial 58-dimensional feature vector, described in Section~\ref{sec:sg}, for each node in the graph to a hidden dimension of 192 with subsequent TAG layers also having a hidden dimension of 192. Note that since we are doing graph classification, the node embeddings learned across multiple layers need to be merged into a single representation for the entire graph - we used simple mean pooling as an input into the final layer. Finally, following best practices in Image CNN architectures, we introduce batch norm and drop out between convolutional layers. 



\section{Experiments}



We evaluated our framework on three popular image classification benchmarks: Colored MNIST~\cite{Arjovsky:etal:ARXIV20, Gulrajani:etal:ARXIV20}, PACS~\cite{Li:etal:ICCV17}, and VLCS~\cite{Fang:etal:ICCV13}. We follow the literature and perform ``leave-one-domain-out'' testing. In this paradigm we designate  domains, where  represents the number of domains, as ``source'' domains, and train our shock graph approach to completion on this collection. We then test this model on the ``leave-one'' ``target'' domain, with the expectation that a model with domain generalization capabilities will achieve similar performance on the source and the target datasets. We repeat this process across all source/target splits and report the image accuracy per domain and the average accuracy across all  target domains. We compare our approach on these three datasets against a wide variety of state-of-the-art methods, Table~\ref{tbl:sota}. In each column of Table~\ref{tbl:sota} we have highlighted the best and second best methods in terms of classification performance in \textcolor{green}{green} and \textcolor{red}{red}, respectively. 


There are two major challenges in fairly comparing our approach with the existing state-of-the-art methods: \emph{1)} the model complexity, and \emph{2)} the utility of pretrained ImageNet weights. For the first challenge, note that all existing methods are highly dependent on the backbone architecture being used - for example, the performance may greatly differ if a ResNet18 model is used compared to a ResNet50. While we are able to use a consistent backbone architecture for all of the Image CNN methods, our approach uses a GNN and thus cannot make use of the same architecture.  In particular, we note that most literature uses AlexNet (61M parameters), ResNet18 (11M parameters), or ResNet50 (25M parameters) backbones for their DG methods. Our method, in contrast, uses only 257k parameters, which has significant benefits not explicitly shown in the experimental metrics. These small neural networks coupled with the sparsity of the graphs allows us to achieve extremely fast inference times and process extremely large batch sizes on modern GPUs. So while we do incur some extra processing converting an image to graph, that is mostly offset by the major speed increase achieved during graph inference. Furthermore, when considering the results in Table~\ref{tbl:sota} if we were to enforce Image CNN architectures to have similar parameter complexity to our approach, its performance would be drastically poorer. 

The second major challenge in comparing our approach to the state-of-the-art domain generalization results is the use of pretrained ImageNet weights. It is very common for domain generalization methods to find powerful invariant deep features that are originally generated by training on the very large and diverse ImageNet dataset. While recent work such as~\cite{he2019rethinking} claim that CNNâ€™s can achieve the same performance without ImageNet pretraining, the initial feature extraction from ImageNet provides a strong baseline for the invariant features that domain generalization methods discover, and in some cases are used directly for transfer learning. However, since our approach has not been trained on ImageNet, comparing it to domain generalization methods that utilize pretrained CNNs is not a fair comparison! This is because one method has benefited significantly from pretraining and the other has not. To mitigate this, we compare our results to the performance of state-of-the-art domain generalization methods with backbones starting from scratch (random weights). Training from scratch is NOT the best that these methods can do and the paper does not pretend it is, but it is necessary to have a fair comparison between the various approaches. 








\begin{table*}[ht]
\begin{center}
    \begin{tabular}{ccccccc}
  \toprule



         &          &   \multicolumn{4}{c}{\bf Colored MNIST~\cite{Arjovsky:etal:ARXIV20, Gulrajani:etal:ARXIV20}} & \\
  \multicolumn{2}{c}{Method} & BackBone &   &  &  & Avg\\
  \hline

   \multicolumn{2}{c}{GroupDRO~\cite{Sagawa:etal:ICLR20}} & ResNet-18 (Scratch)        &     \textcolor{green}{72.7}   &       \textcolor{red}{73.1} &          10.2 &          52.0 \\
   \multicolumn{2}{c}{DeepALL~\cite{Li:etal:ICCV17}}    & ResNet-18 (Scratch)       &     71.7   &       73.0 &          10.3 &           \textcolor{red}{51.7} \\

   \multicolumn{2}{c}{MLDG~\cite{Li:mini:etal:AAAI18}}    & ResNet-18 (Scratch)        &     71.6   &       \textcolor{green}{73.2} &          10.1 &          51.6 \\
   \multicolumn{2}{c}{Self Challenge~\cite{Huang:etal:ECCV20}}  & ResNet-18 (Scratch)        &     66.6   &       65.7 &          \textcolor{red}{10.3} &           47.5 \\


  \hline
  \multicolumn{2}{c}{Our Approach}   & GNN-Tag &  \textcolor{red}{71.99} & 72.23 & \textcolor{green}{71.60} & \textcolor{green}{71.94} \\ 
    
  \hline
     
         &          & \multicolumn{4}{c}{\bf PACS Dataset~\cite{Li:etal:ICCV17}} & \\
  Method & BackBone &  Photo(P) & Art(A) & Cartoon(C) & Sketch(S) & Avg\\
  \hline

  Self Challenge~\cite{Huang:etal:ECCV20}& ResNet-18 (scratch)  & \textcolor{green}{55.02} & \textcolor{green}{42.38} & \textcolor{green}{53.28} & 37.15 & \textcolor{red}{46.95} \\
  GroupDRO~\cite{Sagawa:etal:ICLR20}      & ResNet-18 (scratch)  & 51.20 & 32.20 & 37.30 & 35.70 & 39.10 \\
  Episodic-DG~\cite{Li:mini:etal:ICCV19} & ResNet-18 (scratch)  & 41.13 & 29.83 & 42.15 & \textcolor{red}{37.69} & 37.70\\
  Jigsaw~\cite{Carlucci:mini:etal:CVPR19} & ResNet-18 (scratch)  & 42.34 & 30.37 & 45.65 & 29.14 & 36.66 \\
  MLDG~\cite{Li:mini:etal:AAAI18}         & ResNet-18 (scratch)  & 47.30 & 29.30 & 40.30 & 28.80 & 36.40 \\
  DeepALL~\cite{Li:etal:ICCV17}      & ResNet-50 (scratch)  & 50.80 & 29.10 & 39.20 & 24.00 & 35.80 \\

  DeepALL~\cite{Li:etal:ICCV17} & AlexNet (scratch)  & 21.53 & 22.14 & 18.11 &  8.940  & 17.68\\
  DeepALL~\cite{Li:etal:ICCV17} & ResNet-18 (scratch)  &  14.07 & 11.31 & 15.72 & 20.69 & 15.45 \\ 

  \hline
  Our Approach                       & GNN-Tag & \textcolor{red}{53.23} & \textcolor{red}{33.26} & \textcolor{red}{49.16} & \textcolor{green}{54.15} & \textcolor{green}{47.45}\\
  
  \hline
&          & \multicolumn{4}{c}{\bf VLCS Dataset~\cite{Fang:etal:ICCV13}} & \\
  Method & BackBone &   Pascal(V) & LabelMe(L) & SUN(S) & Caltech(C) & Avg\\
  \hline
  MLDG~\cite{Li:mini:etal:AAAI18}         & ResNet-18 (scratch)  & 44.20 & 54.00 & 47.80 & \textcolor{red}{68.10}  & \textcolor{red}{53.50} \\
    Self Challenge~\cite{Huang:etal:ECCV20} & ResNet-18 (scratch) & 43.10 & \textcolor{green}{57.20}  & 45.20 &  66.00 & 52.90 \\
  GroupDRO~\cite{Sagawa:etal:ICLR20}      & ResNet-18 (scratch)  & \textcolor{red}{48.30} & 53.50 & \textcolor{red}{49.00} & 58.00 & 52.20 \\

  DeepALL~\cite{Li:etal:ICCV17} & ResNet-50 (scratch) & 45.30 & 49.20 & 43.60 & 64.40 & 50.60 \\
  DeepALL~\cite{Li:etal:ICCV17} & ResNet-18 (scratch)  & 45.35 & 46.42 & 39.85 & 63.67 & 48.82 \\
  DeepALL~\cite{Li:etal:ICCV17} & AlexNet (scratch)    & 43.25 & 47.61 & 38.51 & 62.51 & 47.97 \\

  \hline
  Our Approach                       & GNN-Tag & \textcolor{green}{49.15} & \textcolor{red}{55.49} & \textcolor{green}{52.13} &  \textcolor{green}{71.87} & \textcolor{green}{57.16} \\
  \bottomrule
\end{tabular}
\end{center}
\caption{Comparison of our approach (bottom row) to Domain Generalization methods. Each dataset column represents the target domain on which we test. The best classification accuracy is highlighted in \textcolor{green}{green} and the second best in \textcolor{red}{red}. The authors of Jigsaw~\cite{Carlucci:mini:etal:CVPR19} and  Episodic-DG~\cite{Li:mini:etal:ICCV19} only provided code to run on PACS.  Observe our approach is the best performing method across all datasets in average domain accuracy. }
\label{tbl:sota}
\end{table*}



















































\subsection{Colored MNIST}

We evaluated our approach on Colored MNIST~\cite{Arjovsky:etal:ARXIV20, Gulrajani:etal:ARXIV20} to empirically confirm that our approach utilizes the shape-based features when available, and also that existing methods fail to use shape-based features in favor of texture-based features. This dataset is a binary classification variant of the MNIST dataset and has three domains, Figure~\ref{fig:cmnist}, denoted , , and , each with varying degrees of how correlated the colors of the digits are with the true labels. For all domains, the shape of the digit has a  correlation with the true label. While we leave the exact details of the construction of the dataset to the supplemental material, intuitively, the difference between these domains is that for , red is predictive of the label 0, while for , green is predictive of the label 0. Note that by construction, each individual domain has color more highly correlated with the label, which means that effective domain generalization requires that the shape is used (since the same color is not predictive of the same label across domains).






\begin{figure}[!ht]


  \begin{center}
    \includegraphics[width=0.45\textwidth]{figs/cmnist_example.pdf}
    
\end{center}
\caption{a) Samples images from the Colored MNIST dataset. Note that the color of specific numbers significantly varies across domains. }
\label{fig:cmnist}
\end{figure}









Observe that Image CNN based methods, Table~\ref{tbl:sota},  perform poorly on this dataset - in particular, they all perform reasonably
well when testing on the  and the  domain, but perform
significantly worse when testing on the  domain. This makes sense, as
training with a mixture of the  domain and either of the other two
domains necessitates the Image CNN based methods to focus on shape in order to achieve high performance  because color
is no longer highly correlated with the label. As expected, our graph
method focuses more on the shape-based cues and thus has consistent performance
across the domains. We also note that, for the Image CNN methods, the results were the same when pretrained ImageNet weights were used.  




This experiment verifies that Image CNN based domain generalization methods do
neglect shape-based cues in favor of texture based cues when more predictive,
even when the shape-based cues may be more generalizable. Further, it validates
that our GNN domain generalization method works as intended and can focus on the
shape-based features which are more generalizable. 



\subsection{PACS and VLCS} 

For a more traditional comparison, we use the heavily benchmarked VLCS and PACS datasets to evaluate our approach. VLCS~\cite{Fang:etal:ICCV13} includes images from four datasets (domains): PASCAL VOC2007 (V)~\cite{Everingham:etal:IJCV15}, LabelMe (L)~\cite{Russell:etal:IJCV08}, Caltech (C)~\cite{Fei-Fei:Fergus:Perona:CVPRW04} and SUN09 (S)~\cite{Choi:etal:CVPR10}. PACS~\cite{Li:etal:ICCV17} exhibits a larger cross domain shift than VLCS~\cite{Fang:etal:ICCV13}, and contains four domains covering Photo (P), Art Painting (A), Cartoon (C) and Sketch (S) images. Our results in Table~\ref{tbl:sota} show our method is consistently in the top two in performance across individual domains,  and our method is the top performing method when considering the mean classification performance across domains for both datasets. 

Observe that in PACS our performance on ``Art'' and ``Cartoon'' is worse than ``Sketch'' and ``Photo''. We attribute this to the fact that in the ``Sketch'' and ``Photos'' domains the role of shape and texture appear distinctly or not at all while in the ``Art'' and ``Cartoon'' domains texture is often itself represented as shape (internal contour patterns), thus confounding the role of shape. Further, observe that our performance in ``Sketch'' is vastly superior to all methods. This is a clear demonstration of the role of shape, as the black/white nature of sketches nullifies the role of appearance and methods generalizing from color-centric domains will suffer. We also notice in VLCS,  our performance on the SUN domain is the best performing method. This is somewhat surprising as SUN is a scene dataset, but shows that even in scenes the shock graph and GNN are able to recover an amorphous notion of shape that is predictive of the underlying class. 


When assessing our results against other methods, we first look at how well we compare against DeepAll~\cite{Li:etal:ICCV17}, sometimes known as empirical risk minimization (ERM). DeepAll~\cite{Li:etal:ICCV17} trains an Image CNN with standard backbones (ResNet-18, ResNet-50, \emph{etc.}) on the source domains and evaluates its performance on the target domains. Since DeepAll is simple and is  one of the earliest approaches, it generally serves as a baseline to measure relative performance. Further, DeepALL is the closest analogue of our own work as we also do not do any complex training or evaluation procedures, but simply train our approach normally on the source domain and then evaluate on the target domain. Observe that our results exceed DeepALL (ResNet-50) by  on PACS and  on VLCS in terms of average domain accuracy regardless of backbone. Furthermore if we look at the individual target domains across PACS and VLCS we see our approach is better across the board. When comparing our approach against the rest of the methods, we notice that our approach is consistently the best or second best, where as the relative ranking of the other best performing methods differ according to the dataset (including Colored MNIST). This demonstrates that using the shape cue is more generalizable across different datasets than exclusively relying on learned appearance or texture cues. Finally, we note that our approach, despite not having the benefit of more advanced techniques like episodic training and meta-learning commonly employed by state-of-the-art approaches, is quite competitive across all domains and methods.











































































