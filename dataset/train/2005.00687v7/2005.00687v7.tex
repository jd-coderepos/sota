
We currently provide 6 datasets, adopted from diverse application domains for predicting the properties of links (pairs of nodes).
Specifically,  is a protein-protein association network \citep{szklarczyk2019string} (\cf~Section~\ref{sec:ogbl-ppa}), 
 is an author collaboration network \citep{wang2020mag} (\cf~Section~\ref{sec:ogbl-collab}),
 is a drug-drug interaction network \citep{wishart2018drugbank} (\cf~Section~\ref{sec:ogbl-ddi}),
 is a paper citation network \citep{wang2020mag} (\cf~Section~\ref{sec:ogbl-citation2}),
 is a heterogeneous knowledge graph compiled from a large number of biomedical repositories  (\cf~Section~\ref{sec:ogbl-biokg}),
and  is a Wikidata knowledge graph \citep{vrandevcic2014wikidata} (\cf~Section~\ref{sec:ogbl-wikikg2}).


The different datasets are highly diverse in their graph structure, as shown in Table \ref{tab:datasets_stats}.
For example, the biological networks (\texttt{ogbl-ppa} and ) are much denser than the academic networks (\texttt{ogbl-collab} and \texttt{ogbl-citation2}) and the knowledge graphs (\texttt{ogbl-wikikg2} and \texttt{ogbl-biokg}), as can be seen from the large average node degree, small number of nodes, and the small graph diameter.
On the other hand, the collaboration network, , has more clustered graph structure than the other datasets, as can be seen from its high average clustering coefficient.
Comparing the two knowledge graph datasets, \texttt{ogbl-wikikg2} and \texttt{ogbl-biokg}, we see that the former is much more sparse and less clustered than the latter.


\xhdr{Baselines}
We implement different sets of baselines for link prediction datasets that only have a single edge type, and KG completion datasets that have multiple edge/relation types.

\xhdr{Baselines for link prediction datasets} 
We consider the following representative models as our baselines for the link prediction datasets unless otherwise specified. For all models, edge features are obtained by using the Hadamard operator  between pair-wise node embeddings, and are then inputted to an \textsc{MLP} for the final prediction. During training, we randomly sample edges and use them as negative examples. We use the same number of negative edges as there are positive edges.
Below, we describe how each model obtains node embeddings:
\begin{itemize}
  \setlength{\parskip}{0cm}
  \setlength{\itemsep}{0cm}
    \item : Input node features are directly used as node embeddings. 
    \item : The node embeddings are obtained by concatenating input features and  embeddings \citep{grover2016node2vec,perozzi2014deepwalk}.
    \item : The node embeddings are obtained by full-batch Graph Convolutional Networks (GCN) \citep{kipf2016semi}. 
    \item : The node embeddings are obtained by full-batch GraphSAGE \citep{hamilton2017inductive}, where we adopt its mean pooling variant and a simple skip connection to preserve central node features.
    \item : The distinct embeddings are assigned to different nodes and are learned in an end-to-end manner together with the MLP predictor.
        \item  (optional): A mini-batch training technique of GNNs \citep{hamilton2017inductive} that samples neighborhood nodes when performing aggregation.
     \item  (optional): A mini-batch training technique of GNNs \citep{chiang2019cluster} that partitions the graphs into a fixed number of subgraphs and draws mini-batches from them.
    \item  (optional): A mini-batch training technique of GNNs \citep{zeng2019graphsaint} that samples subgraphs via a random walk sampler.
\end{itemize}
Similar to the node property prediction baselines, the mini-batch GNN training, , , and , are experimented only for graph datasets where full-batch  and  did not fit into the common GPU memory size of 11GB.
To choose the GNN architecture for the mini-batch GNNs, we first run full-batch  and  on a NVIDIA Quadro RTX 8000 with 48GB of memory, and then adopt the best performing full-batch GNN architecture for the mini-batch GNNs.
All models are trained with a fixed hidden dimensionality of 256, a fixed number of three layers, and a tuned dropout ratio .

\xhdr{Baselines for KG completion datasets}
We consider the following representative KG embedding models as our baselines for the KG datasets unless otherwise specified.
\begin{itemize}
  \setlength{\parskip}{0cm}
  \setlength{\itemsep}{0cm}
    \item : Translation-based KG embedding model by \citet{bordes2013translating}. 
    \item : Multiplication-based KG embedding model by \citet{yang2014embedding}.
    \item : Complex-valued multiplication-based KG embedding model by \citet{trouillon2016complex}.
    \item : Rotation-based KG embedding model by \citet{sun2019rotate}.
\end{itemize}
For KGs with many entities and relations, the embedding dimensionality can be limited by the available GPU memory, as the embeddings need to be loaded into GPU all at once. We therefore choose the dimensionality such that training can be performed on a fixed-budget of GPU memory.
Our training procedure follows \citet{sun2019rotate}, where we perform negative sampling and use margin-based logistic loss for the loss function.

\dataset
{ogbl-ppa}
{Protein-Protein Association Network} 
{The  dataset is an undirected, unweighted graph. Nodes represent proteins from 58 different species, and edges indicate biologically meaningful associations between proteins, \eg, physical interactions, co-expression, homology or genomic neighborhood \citep{szklarczyk2019string}. We provide a graph object constructed from training edges (\ie, no validation and test edges are contained). Each node contains a 58-dimensional one-hot feature vector that indicates the species that the corresponding protein comes from.}
{The task is to predict new association edges given the training edges.
The evaluation is based on how well a model ranks positive test edges over negative test edges.
Specifically, we rank each positive edge in the validation/test set against 3,000,000 randomly-sampled negative edges, and count the ratio of positive edges that are ranked at the -th place or above (Hits@). We found  to be a good threshold to rate a model's performance in our initial experiments.
Overall, this metric is much more challenging than ROC-AUC because the model needs to consistently rank the positive edges higher than \emph{nearly all} the negative edges. }
{We provide a biological throughput split of the edges into training/validation/test edges.
Training edges are protein associations that are measured experimentally by a high-throughput technology (\eg, cost-effective, automated experiments that make large scale repetition feasible~\citep{macarron2011impact,bajorath2002integration,younger2017high}) or are obtained computationally (\eg, via text-mining). In contrast, validation and test edges contain protein associations that can only be measured by low-throughput, resource-intensive experiments performed in laboratories.
In particular, the goal is to predict a particular type of protein association, \eg, physical protein-protein interaction, from other types of protein associations (\eg, co-expression, homology, or genomic neighborhood) that can be more easily measured and are known to correlate with associations that we are interested in. 
}
\begin{table}[t]
\centering
    \caption{\textbf{Results for .}}
    \label{tab:ogbl-ppa-baseline}
    \renewcommand{\arraystretch}{1}
\begin{tabular}{lccc}
      \toprule
        \mr{2}{\textbf{Method}} & \mc{3}{c}{\textbf{Hits@100 (\%)}} \\
        & Training & Validation & \textbf{Test} \\
      \midrule
        \textsc{MLP} & 0.46\std{0.00} & 0.46\std{0.00} & 0.46\std{0.00} \\
        \textsc{Node2Vec} & 24.43\std{0.92} & 22.53\std{0.88} & 22.26\std{0.83} \\
        \textsc{GCN} & 19.89\std{1.51} & 18.45\std{1.40} & 18.67\std{1.32} \\
        \textsc{GraphSAGE} & 18.53\std{2.85} & 17.24\std{2.64} & 16.55\std{2.40} \\
        \textsc{MatrixFactorization} & 81.65\std{9.15} & \textbf{32.28}\std{4.28} & \textbf{32.29}\std{0.94} \\
      \bottomrule
    \end{tabular}
\end{table}
{Our initial benchmarking results are shown in Table~\ref{tab:ogbl-ppa-baseline}.
First, the MLP baseline\footnote{Here we obtain node embeddings by applying a linear layer to the raw one-hot node features.} performs extremely poorly, which is to be expected since the node features are not rich in this dataset.
Surprisingly, both GNN baselines (, ) and  fail to overfit on the training data and show similar performances across training/validation/test splits.
The poor training performance of GNNs suggests that \emph{positional} information, which cannot be captured by GNNs alone \citep{you2019position}, might be crucial to fit training edges and obtain meaningful node embeddings.
On the other hand, we see that , which learns a distinct embedding for each node (thus, it can express any positional information of nodes), is indeed able to overfit on the training data, while also reaching better validation and test performance.
However, the poor generalization performance still encourages the development of new research ideas to close this gap, \eg, by injecting positional information into GNNs or by developing more sophisticated negative sampling techniques.}

\dataset
{ogbl-collab}
{Author Collaboration Network}
{The \texttt{ogbl-collab} dataset is an undirected graph, representing a subset of the collaboration network between authors indexed by ~\citep{wang2020mag}.
Each node represents an author and edges indicate the collaboration between authors. All nodes come with 128-dimensional features, obtained by averaging the word embeddings of papers that are published by the authors. All edges are associated with two types of meta-information: the year and the edge weight, representing the number of co-authored papers published in that year. 
The graph can be viewed as a dynamic multi-graph since there can be multiple edges between two nodes if they collaborate in more than one year. 
}
{The task is to predict the author collaboration relationships in a particular year given the past collaborations. 
As the task is a time-series problem, it is natural for models to incorporate the most recent edge information to make prediction, \eg, use validation edges when predicting test edges.
The evaluation metric is similar to \texttt{ogbl-ppa} in Appendix \ref{sec:ogbl-ppa}, where we would like the model to rank true collaborations higher than false collaborations. Specifically, we rank each true collaboration among a set of 100,000 randomly-sampled negative collaborations, and count the ratio of positive edges that are ranked at -place or above (Hits@). We found  to be a good threshold in our preliminary experiments.
}
{We split the data according to time, in order to simulate a realistic application in collaboration recommendation. Specifically, we use the collaborations until 2017 as training edges, those in 2018 as validation edges, and those in 2019 as test edges.}
\begin{table}[t]
    \centering
    \caption{{\bf Results for .}
    }
    \label{tab:ogbl-collab-baseline}
    \renewcommand{\arraystretch}{1.1}
\begin{tabular}{lcccc}
      \toprule
        \mr{2}{\textbf{Method}} &  \textbf{Use most}  & \mc{3}{c}{\textbf{Hits@50 (\%)}} \\
         & \textbf{recent edges} & Training & Validation & \textbf{Test} \\
      \midrule
        \textsc{MLP} & \textcolor{red}{\XSolidBrush}  & 45.70\std{1.66} & 24.02\std{1.45} & 19.27\std{1.29} \\
        \textsc{Node2Vec} & \textcolor{red}{\XSolidBrush} & 99.73\std{0.36} & \textbf{57.03}\std{0.52} & \textbf{48.88}\std{0.54} \\
        \textsc{GCN}    & \textcolor{red}{\XSolidBrush} & 84.28\std{1.78} & 52.63\std{1.15} & 44.75\std{1.07} \\
        \textsc{GraphSAGE} & \textcolor{red}{\XSolidBrush}  & 93.58\std{0.59} & 56.88\std{0.77} & 48.10\std{0.81} \\
        \textsc{MatrixFactorization} & \textcolor{red}{\XSolidBrush} & 100.00\std{0.00} & 48.96\std{0.29} & 38.86\std{0.29} \\
      \midrule
      \textsc{GCN} & \textcolor{dkgreen}{\CheckmarkBold} & 84.28\std{1.78} & 52.63\std{1.15} & 47.14\std{1.45} \\
      \textsc{GraphsAGE} & \textcolor{dkgreen}{\CheckmarkBold} & 93.58\std{0.59} & 56.88\std{0.77}& \textbf{54.63}\std{1.12}         \\
      \bottomrule
    \end{tabular}
\end{table}
{Our initial benchmarking results are shown in Table \ref{tab:ogbl-collab-baseline}.
First, we consider the conventional setting where validation edges are used only for model selection.
From the upper half of Table \ref{tab:ogbl-collab-baseline}, we see that  achieves the best results, followed by the two GNN models and .
This can be explained by the fact that positional information, \ie, past collaborations, is a much more indicative feature for predicting future collaboration than solely relying on the average paper representations of authors, \ie, the same research interest.
Notably,  achieves nearly perfect training results, but cannot transfer the good results to the validation and test splits, even when heavy regularization is applied. 
Overall, it is fruitful to explore injecting positional information into GNNs, and develop better regularization methods.
This dataset further provides a unique research opportunity for dynamic multi-graphs.
To demonstrate the potential benefit of time-series modeling, we use the same \textsc{GCN} and \textsc{GraphSAGE} models as before but at test time, we additionally incorporate the most recent edges (\ie, validation edges) as input to the models. From the lower half of Table \ref{tab:ogbl-collab-baseline}, we see that the test performances of both GNN models increase significantly by using validation edges at the inference time.
One promising direction to further increase the performance is to treat edges at different timestamps differently, as recent collaborations may be more indicative about the future collaborations than the past ones.
}


\dataset
{ogbl-ddi}
{Drug-Drug Interaction Network}
{The \texttt{ogbl-ddi} dataset is a homogeneous, unweighted, undirected graph, representing the drug-drug interaction network~\citep{wishart2018drugbank}. Each node represents an FDA-approved or experimental drug. Edges represent interactions between drugs and can be interpreted as a phenomenon where the joint effect of taking the two drugs together is considerably different from the expected effect in which drugs act independently of each other. }
{The task is to predict drug-drug interactions given information on already known drug-drug interactions.  The evaluation metric is similar to \texttt{ogbl-collab} discussed in Section \ref{sec:ogbl-collab}, where we would like the model to rank true drug interactions higher than non-interacting drug pairs. Specifically, we rank each true drug interaction among a set of approximately 100,000 randomly-sampled negative drug interactions, and count the ratio of positive edges that are ranked at -place or above (Hits@). We found  to be a good threshold in our preliminary experiments.}
{We develop a \emph{protein-target split}, meaning that we split drug edges according to what proteins those drugs target in the body. As a result, the test set consists of drugs that predominantly bind to different proteins from drugs in the train and validation sets. This means that drugs in the test set work differently in the body, and have a rather different biological mechanism of action than drugs in the train and validation sets. The protein-target split thus enables us to evaluate to what extent the models can generate practically useful predictions~\citep{guney2017reproducible}, \ie, non-trivial predictions that are not hindered by the assumption that there exist already known and very similar medications available for training.}
{Our initial benchmarking results are shown in Table~\ref{tab:ogbl-ddi-baseline}.
Since \texttt{ogbl-ddi} does not contain any node features, we omit the graph-agnostic \textsc{MLP} baseline for this experiment.
Furthermore, for \textsc{GCN} and \textsc{GraphSAGE}, node features are also represented as distinct embeddings and learned in an end-to-end manner together with the GNN parameters.}

Interestingly, both the GNN models and the \textsc{MatrixFactorization} approach achieve significantly higher training results than \textsc{Node2Vec}. 
However, only the GNN models are able to transfer this performance to the test set to some extent, suggesting that relational information is crucial to allow the model to generalize to unseen interactions.
Notably, most of the models show high performance variance, which can be partly attributed to the dense nature of the graph and the challenging data split.
We further perform the conventional random split of edges, where we find \textsc{GraphSAGE} is able to achieve 80.88\std{2.42}\% test Hits@20. This indicates that the protein-target split is indeed more challenging than the conventional random split.
Overall, \texttt{ogbl-ddi} presents a unique challenge of predicting out-of-distribution links in dense graphs.

\begin{table}[t]
    \centering
    \captionsetup{justification=centering}
    \caption{{\bf Results for .}}
    \label{tab:ogbl-ddi-baseline}
    \renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccc}
      \toprule
        \mr{2}{\textbf{Method}} & \mc{3}{c}{\textbf{Hits@20 (\%)}} \\
         & Training & Validation & \textbf{Test} \\
      \midrule
        \textsc{Node2Vec}            & 37.82\std{1.35} & 32.92\std{1.21} & 23.26\std{2.09} \\
        \textsc{GCN}                 & 63.95\std{2.17} & 55.50\std{2.08} & 37.07\std{5.07} \\
        \textsc{GraphSAGE}           & 72.24\std{0.45} & \textbf{62.62}\std{0.37} & \textbf{53.90}\std{4.74} \\
        \textsc{MatrixFactorization} & 56.56\std{13.88} & 33.70\std{2.64} & 13.68\std{4.75} \\

      \bottomrule
    \end{tabular}
\end{table}


\dataset
{ogbl-citation2}
{Paper Citation Network}
{The \texttt{ogbl-citation2} dataset\footnote{The older version  has been deprecated due to a bug in negative samples of validation and test sets.} is a directed graph, representing the citation network between a subset of papers extracted from MAG~\citep{wang2020mag}. 
Similar to \texttt{ogbn-arxiv} in Section \ref{sec:ogbn-arxiv}, each node is a paper with 128-dimensional \textsc{word2vec} features that summarizes its title and abstract, and each directed edge indicates that one paper cites another. All nodes also come with meta-information indicating the year the corresponding paper was published. 
}
{The task is to predict missing citations given existing citations. Specifically, 
for each source paper, 
two of its references are randomly dropped, and we would like the model to rank the missing two references higher than 1,000 negative reference candidates.
The negative references are randomly-sampled from all the previous papers that are not referenced by the source paper. 
The evaluation metric is Mean Reciprocal Rank (MRR), where the reciprocal rank of the true reference among the negative candidates is calculated for each source paper, and then the average is taken over 
all source papers. 
}
{We split the edges according to time, in order to simulate a realistic application in citation recommendation (\eg, a user is writing a new paper and has already cited several existing papers, but wants to be recommended additional references). To this end, we use the most recent papers (those published in 2019) as the source papers for which we want to recommend the references. For each source paper, we drop \emph{two} papers from its references---the resulting two dropped edges (pointing from the source paper to the dropped papers) are used respectively for validation and testing. 
All the rest of the edges are used for training.
}
{Our initial benchmarking results are shown in Table \ref{tab:ogbl-citation2-baseline}, where the directed graph is converted to an undirected one for simplicity. 
Here, the GNN models achieve the best results, followed by \textsc{MatrixFactorization} and \textsc{Node2Vec}.
Among the GNNs, \textsc{GCN} performs better than \textsc{GraphSAGE}.
However, these GNNs use full-batch training; thus, they are not scalable and require more than 40GB of GPU memory to train, which is intractable on most of the GPUs available today.
Hence, we also experiment with the scalable mini-batch training techniques of GNNs, \textsc{NeighborSampling}, \textsc{ClusterGCN}, and \textsc{GraphSAINT}.
Interestingly, we see from Table \ref{tab:ogbl-citation2-baseline} that these techniques give worse performance than their full-batch counterpart, which is in contrast to the node classification datasets (\eg, \texttt{ogbn-products} and \texttt{ogbn-mag}), where the mini-batch-based models give stronger generalization performances.
This limitation presents a unique challenge for applying the mini-batch techniques to link prediction, differently from those pertaining to node prediction.
Overall, \texttt{ogbl-citation2} provides a research opportunity to further improve GNN models and their scalable mini-batch training techniques in the context of link prediction.
}
\begin{table}[t]
    \centering
    \captionsetup{justification=centering}
    \caption{{\bf Results for .} \\
    Requires a GPU with 40GB of memory}
    \label{tab:ogbl-citation2-baseline}
    \renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccc}
      \toprule
        \mr{2}{\textbf{Method}} & \mc{3}{c}{\textbf{MRR}} \\
         & Training & Validation & \textbf{Test} \\
      \midrule
        \textsc{MLP}  & 0.2884\std{0.0014} & 0.2891\std{0.0012} & 0.2895\std{0.0014} \\
        \textsc{Node2Vec} & 0.7004\std{0.0012} & 0.6124\std{0.0011} & 0.6141\std{0.0011} \\
        \textsc{GCN}  & 0.9092\std{0.0019} & \textbf{0.8479}\std{0.0023} & \textbf{0.8474}\std{0.0021} \\
        \textsc{GraphSAGE}  & 0.8970\std{0.0056} & 0.8263\std{0.0033} & 0.8260\std{0.0036} \\
\textsc{MatrixFactorization}  & 0.9185\std{0.0170} & 0.5181\std{0.0436} & 0.5186\std{0.0443} \\
      \midrule
        \textsc{NeighborSampling} & 0.8645\std{0.0015} & 0.8054\std{0.0009} & 0.8044\std{0.0010} \\
        \textsc{ClusterGCN} & 0.8749\std{0.0035} & 0.7994\std{0.0025} & 0.8004\std{0.0025} \\
        \textsc{GraphSAINT} & 0.8682\std{0.0026} & 0.7975\std{0.0039} & 0.7985\std{0.0040} \\
      \bottomrule
    \end{tabular}
\end{table}



\dataset
{ogbl-wikikg2}
{Wikidata Knowledge Graph}
{The  dataset\footnote{The older version  has been deprecated due to a bug in negative samples of validation and test sets.} is a Knowledge Graph (KG) extracted from the Wikidata knowledge base \citep{vrandevcic2014wikidata}. It contains a set of triplet edges (\textmd{head}, \textmd{relation}, \textmd{tail}), capturing the different types of relations between entities in the world, \eg, {\it Canada  Hinton}. We retrieve all the relational statements in Wikidata and filter out rare entities. Our KG contains 2,500,604 entities and 535 relation types.}
{The task is to predict new triplet edges given the training edges. The evaluation metric follows the standard filtered metric widely used in KGs~\citep{bordes2013translating,yang2014embedding,trouillon2016complex,sun2019rotate}. Specifically, we corrupt each test triplet edges by replacing its \textmd{head} or \textmd{tail} with randomly-sampled 1,000 negative entities (500 for head and 500 for tail), while ensuring the resulting triplets do not appear in KG. The goal is to rank the true \textmd{head} (or \textmd{tail}) entities higher than the negative entities, which is measured by Mean Reciprocal Rank (MRR).
}
{We split the triplets according to time, simulating a realistic KG completion scenario that aims to fill in missing triplets that are not present at a certain timestamp.
Specifically, we downloaded Wikidata at three different time stamps\footnote{Available at \url{https://archive.org/search.php?query=creator\%3A\%22Wikidata+editors\%22}} (May, August, and November of 2015), and constructed three KGs where we only retain entities and relation types that appear in the earliest May KG.
We use the triplets in the May KG for training, and use the additional triplets in the August and November KGs for validation and test, respectively.
Note that our dataset split is different from the existing Wikidata KG dataset that adopts a conventional random split~\citep{wang2019kepler}, which does not reflect the practical usage.
}
{Our benchmark results are provided in Table \ref{tab:ogbl-wikikg2-baseline}, where the upper-half baselines are implemented on a single commodity GPU with 11GB memory, while the bottom-half baselines are implemented on a high-end GPU with 45GB memory.\footnote{Given a fixed 11GB GPU memory budget, we adopt 100-dimension embeddings for \textsc{DistMult} and \textsc{TransE}. Since \textsc{RotatE} and \textsc{ComplEx} require the entity embeddings with the real and imaginary parts, we train these two models with the dimensionality of 50 for each part. On the other hand, on the high-end GPU with 45GB memory, we are able to train all the models with  larger embedding dimensionality.}
Training MRR in Table \ref{tab:ogbl-wikikg2-baseline} is an \emph{unfiltered} metric,\footnote{This means that the training MRR is computed by ranking against randomly-selected negative entities without filtering out triplets that appear in KG.
The unfiltered metric has the systematic bias of being smaller than the filtered counterpart (computed by ranking against ``true'' negative entities, \ie, the resulting triplets do not appear in the KG)~\citep{bordes2013translating}.} as filtering is computationally expensive for the large number of training triplets.

First, we see from the upper-half of Table \ref{tab:ogbl-wikikg2-baseline} that when the limited embedding dimensionality is used, \textsc{ComplEx} performs the best among the four baselines.
With the increased dimensionality, all four models are able to achieve higher MRR on training, validation and test sets, as seen from the bottom-half of Table \ref{tab:ogbl-wikikg2-baseline}. This suggests the importance of using a sufficient large embedding dimensionality to achieve good performance in this dataset.
Interestingly, although \textsc{TransE} and \textsc{RotatE} underperform with the limited dimensionality, they obtain the best performances with the increased dimensionality.
Nevertheless, the extremely low test MRR\footnote{Note that our test MRR on \texttt{ogbl-wikikg2} is computed using only 500 negative entities per triplet, which is much less than the number of negative entities used to compute MRR in the existing KG datasets, such as \textsc{FB15k} and \textsc{FB15k-237} (around 15,000 negative entitiesf).
Nevertheless, \textsc{RotatE} gives either lower or comparable test MRR on \texttt{ogbl-wikikg2} compared to \textsc{FB15k} and \textsc{FB15k-237}~\citep{sun2019rotate}.} suggests that our realistic KG completion dataset is highly non-trivial. It presents a realistic generalization challenge of \emph{discovering} new triplets based on existing ones, which necessitates the development of KG models with more robust and generalizable reasoning capability.
Furthermore, this dataset presents an important challenge of effectively scaling embedding models to large KGs---na\"ively training KG embedding models with reasonable dimensionality would require a high-end GPU, which is extremely costly and not scalable to even larger KGs. 
A promising approach to improve scalability is to distribute training across multiple commodity GPUs~\citep{zheng2020dgl,zhu2019graphvite,lerer2019pytorch}. A different approach is to share parameters across entities and relations, so that a smaller number of embedding parameters need to be put onto the GPU memory at once.
}

\begin{table}[t]
    \centering
    \captionsetup{justification=centering}
    \caption{{\bf Results for .} \\
    Requires a GPU with 45GB of memory.}
    \label{tab:ogbl-wikikg2-baseline}
    \renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccc}
      \toprule
        \mr{2}{\textbf{Method}} & \mc{3}{c}{\textbf{MRR}} \\
         & Training (Unfiltered) & Validation (Filtered) & \textbf{Test} (Filtered) \\
      \midrule
        \textsc{TransE}     & 0.3408\std{0.0044} & 0.2465\std{0.0020} & 0.2622\std{0.0045} \\
        \textsc{DistMult}   & 0.4115\std{0.0077} & 0.3150\std{0.0088} & 0.3447\std{0.0082} \\
        \textsc{ComplEx} & 0.4573\std{0.0035} & 0.3534\std{0.0052} & 0.3804\std{0.0022} \\
        \textsc{RotatE}   & 0.3464\std{0.0015} & 0.2250\std{0.0035} & 0.2530\std{0.0034} \\
      \midrule
        \textsc{TransE} (5dim)     & 0.6174\std{0.0026} & 0.4272\std{0.0030} & 0.4256\std{0.0030} \\
        \textsc{DistMult} (5dim)   & 0.4350\std{0.0038}	& 0.3506\std{0.0042} & 0.3729\std{0.0045} \\
        \textsc{ComplEx} (5dim) & 0.4760\std{0.0030} & 0.3759\std{0.0016} & 0.4027\std{0.0027} \\
        \textsc{RotatE} (5dim)   & 0.6111\std{0.0032} &	\textbf{0.4353}\std{0.0028}	& \textbf{0.4332}\std{0.0025} \\
      \bottomrule
    \end{tabular}
\end{table}





\dataset
{ogbl-biokg}
{Biomedical Knowledge Graph}
{
The  dataset is a Knowledge Graph (KG), which we created using data from a large number of biomedical data repositories. 
It contains 5 types of entities: diseases (10,687 nodes), proteins (17,499), drugs (10,533 nodes), side effects (9,969 nodes), and protein functions (45,085 nodes). There are 51 types of directed relations connecting two types of entities, including 39 kinds of drug-drug interactions, 8 kinds of protein-protein interaction, as well as drug-protein, drug-side effect, drug-protein, function-function relations. All relations are modeled as directed edges, among which the relations connecting the same entity types (\eg, protein-protein, drug-drug, function-function) are always symmetric, \ie, the edges are bi-directional. 

This dataset is relevant to both biomedical and fundamental ML research.
On the biomedical side, the dataset allows us to get better insights into human biology and generate predictions that can guide downstream biomedical research. On the fundamental ML side, the dataset presents challenges in handling a noisy, incomplete KG with possible contradictory observations.
This is because the  dataset involves heterogeneous interactions that span from the molecular scale (\eg, protein-protein interactions within a cell) to whole populations (\eg, reports of unwanted side effects experienced by patients in a particular country). Further, triplets in the KG come from sources with a variety of confidence levels, including experimental readouts, human-curated annotations, and automatically extracted metadata. 
}
{The task is to predict new triplets given the training triplets. The evaluation protocol is exactly the same as \texttt{ogbl-wikikg2} in Section \ref{sec:ogbl-wikikg2}, except that here we only consider ranking against entities \emph{of the same type}. For instance, when corrupting head entities of the protein type, we only consider negative protein entities.
}
{For this dataset, we adopt a random split. While splitting the triplets according to time is an attractive alternative, we note that it is incredibly challenging to obtain accurate information as to when individual experiments and observations underlying the triplets were made. We strive to provide additional dataset splits in future versions of the \nameshort{}. 
}
{Our benchmark results are provided in Table~\ref{tab:ogbl-biokg-baseline}, where we adopt 2000-dimensional embeddings for \textsc{DistMult} and \textsc{TransE}, and 1000-dimensional embeddings for the real and imaginary parts of \textsc{RotatE} and \textsc{ComplEx}.
Negative sampling is performed only over entities of the same types.
Similar to Table \ref{tab:ogbl-wikikg2-baseline} in Section \ref{sec:ogbl-wikikg2}, training MRR in Table \ref{tab:ogbl-biokg-baseline} is an \emph{unfiltered} metric.\footnote{In Table~\ref{tab:ogbl-biokg-baseline}, training MRR is lower than validation and test MRR because it is an \emph{unfiltered} metric (computed by ranking against randomly-selected negative entities), and is expected to give systematically lower MRR than the filtered metric (computed by ranking against ``true'' negative entities, \ie, the resulting triplets do not appear in the KG).}

Among the four models, \textsc{ComplEx} achieves the best test MRR, while \textsc{TransE} gives significantly worse performance compared to the other models. The worse performance of \textsc{TransE} can be explained by the fact that \textsc{TransE} cannot model symmetric relations \citep{trouillon2016complex} that are prevalent in this dataset, \eg, protein-protein and drug-drug relations are all symmetric. Overall, it is of great practical interest to further improve the model performance. A promising direction is to develop a more specialized method for the \emph{heterogeneous} knowledge graph, where multiple node types exist and the entire graph follows the pre-defined schema.}

\begin{table}[t]
    \centering
    \captionsetup{justification=centering}
    \caption{{\bf Results for .} }
    \label{tab:ogbl-biokg-baseline}
    \renewcommand{\arraystretch}{1.1}
\begin{tabular}{lccc}
      \toprule
        \mr{2}{\textbf{Method}} & \mc{3}{c}{\textbf{MRR}} \\
         & Training (Unfiltered) & Validation (Filtered) & \textbf{Test} (Filtered) \\
      \midrule
        \textsc{TransE}     & 0.5145\std{0.0005} & 0.7456\std{0.0003} & 0.7452\std{0.0004} \\
        \textsc{DistMult}   & 0.5250\std{0.0006} & 0.8055\std{0.0003}& 0.8043\std{0.0003} \\
        \textsc{ComplEx} & 0.5315\std{0.0006} & \textbf{0.8105\std{0.0001}} & \textbf{0.8095\std{0.0007}} \\
        \textsc{RotatE}   & 0.5363\std{0.0007} & 0.7997\std{0.0002} & 0.7989\std{0.0004} \\
      \bottomrule
    \end{tabular}
\end{table}
