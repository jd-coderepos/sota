\begin{figure*}[t]
\setlength\tabcolsep{1pt} 
\centering
\begin{tabular}{@{} r M{0.31\linewidth} M{0.31\linewidth} M{0.31\linewidth} @{}}
    \begin{subfigure}{0.03\linewidth} \caption{}\label{subfig:bdd100k_vid1_c_main-paper} \end{subfigure} 
      & \includegraphics[width=\hsize]{17911.png} 
      & \includegraphics[width=\hsize]{17915.png}
      & \includegraphics[width=\hsize]{17920.png}\\ \addlinespace
    \begin{subfigure}{0.03\linewidth} \caption{}\label{subfig:bdd100k_vid1_b_main-paper} \end{subfigure} 
      & \includegraphics[width=\hsize]{16861.png} 
      & \includegraphics[width=\hsize]{16865.png}
      & \includegraphics[width=\hsize]{16869.png}\\ 
\end{tabular}
   \caption{Predictions our model on the validation set of BDD100K, each color represents a different predicted ID. The method is robust to occlusions and in the nighttime.}
   \label{fig:bdd100k_qualitative}
   \vspace{-1.5em}
\end{figure*}

\section{Experiments}

In this section, we show the results of our method on the MOT17 \cite{MOT16} and BDD100K \cite{bdd100k} benchmarks. We then conduct an ablation study to obtain more detailed insights about our framework.

\subsection{Experimental setup}
\label{sec:experimental-setup}

\paragrax{Datasets.} MOT17 \cite{MOT16} is a widely used benchmark to evaluate \ac{mot} methods. It consists of training and test sets, each including seven sequences. Only pedestrians are evaluated in this benchmark.
\noindent BDD100K \cite{bdd100k} is a large-scale driving video dataset comprising 100K diverse video clips in different environmental and weather conditions. The dataset provides several types of annotations, such as object bounding boxes, semantic and instance segmentation, tracking IDs, etc. 
The object detection set consists of one annotated frame per video and the MOT set is a subset of 1600 videos, annotated at a lowered frame rate of 5 Hz.

\paragrax{Evaluation metrics.} The most widely used metric to evaluate \ac{mot} methods is \ac{mota} \cite{Bernardin2008EvaluatingMO}. However, the more recently introduced \ac{hota} \cite{Luiten2020IJCV} metric better balances detection and association. Since its introduction, many \ac{mot} benchmarks have adopted it, including MOT17 and BDD100K. \ac{idf1} \cite{10.1007/978-3-319-48881-3_2} is also widely used to measure \ac{mot} systems and it focuses on object's identities association. While on MOT17 the main metrics are computed over all the classes together, BDD100K computes these metrics independently for each class and then computes an average. As a result, the main metrics are defined as \ac{mhota}, \ac{mmota} and \ac{midf1}. 

\begin{table*}[t]
    \centering \footnotesize
    \begin{tabular}{llllllll}
         \toprule
        Method             & Backbone   & Pre-train & Train  & HOTA$\uparrow$ & MOTA$\uparrow$ & IDF1$\uparrow$  & IDS$\downarrow$ \\ 
        \midrule
        \textit{CNN-based}: &&&&&&& \\
        CenterTrack \cite{zhou2020tracking}    &  DLA-34 & CH      & MOT17      & 52.2   & 67.8      & 64.7      & 3039  \\
        QDTrack \cite{cvpr_qdtrack}    & ResNet-50 & COCO      & MOT17      & ---   & 68.7      & 66.3      & 3378  \\
        MTrack \cite{yu2022towards}    & DLA-34    & CH        & MOT17      & ---   & 72.1      & 73.5      & 2028  \\
        FairMOT \cite{zhang2021fairmot}& DLA-34    & COCO      & MOT17+CH+CP+ETHZ+CS+CT+PRW & 59.3  & 73.7      & 72.3      & 3303  \\ 
        Swin-JDE \cite{tsai2023swinJDE}& ResNet-50 & CH        & MOT17+CP+ETHZ+CS+CT+PRW & 57.2   & 71.7  & 71.3 & 2784 \\
        ByteTrack \cite{ByteTrack}     & YOLOX-X   & COCO      & MOT17+CH+CP+ETHZ  & 63.1 & 80.3   & 77.3      & 2196 \\
        SUSHI \cite{Cetintas_2023_CVPR}     & YOLOX-X   & COCO      & MOT17+CH+CP+ETHZ  & 66.5 & 81.1   & 83.1      & 1149 \\
         \midrule
        \textit{Transformer-based}: &&&&&&& \\
        MO3TR PIQ-SST \cite{zhu2022looking} & DarkNet & COCO      & MOT17+CH  & 60.5      & 78.6     & 72.4 & 2808 \\
        TransCenter \cite{9964258} &  PVTv2 & COCO & MOT17+CH & --- & 76.2 & 65.5 & 5394 \\
        MOTRv2  \cite{Zhang_2023_CVPR}     & YOLOX-X+ResNet-50 & COCO     & MOT17+CH*+CP+ETHZ   & 62.0      & 78.6      & 75.0      & 2619  \\
        \cdashlinelr{1-8}
        MO3TR \cite{zhu2022looking} & ResNet-50 & COCO      & MOT17+CH+ETHZ+CS  & 49.9      & 63.9     & 60.5          & 2847 \\
        MeMOT \cite{cai2022memot}          & ResNet-50 & COCO      & MOT17+CH*   & 56.9      & 72.5      & 69.0      & 2724  \\
        TrackFormer \cite{meinhardt2022trackformer} & ResNet-50 & CH        & MOT17+CH   & 57.3      & 74.1      & 68.0      & 2829  \\
        MOTR  \cite{zeng2022motr}          & ResNet-50 & COCO      & MOT17+CH*   & 57.8      & 73.4      & 68.6      & \textbf{2439}  \\
        TransTrack \cite{sun2020transtrack}& ResNet-50 & CH        & MOT17+CH   & --- & \textbf{74.5}      & 63.9      & 3663  \\
        TransTrack \cite{sun2020transtrack}& ResNet-50 & CH        & MOT17      & --- & 68.4      &  ---         & 3942  \\ 
        \textbf{\ourmodelname{}} (ours)  & ResNet-50 & CH        & MOT17      & \textbf{58.9}     & 73.7      & \textbf{71.8}     & 2619  \\
         \bottomrule
    \end{tabular}
    \caption{Results on the MOT17 test set using private detections. The second group shows DETR-like models, all based on Deformable-DETR except MO3TR, which is based on DETR. The best results among DETR-like models using the ResNet-50 backbone are highlighted in \textbf{bold}. Datasets abbreviations refer to the following works  ETHZ \cite{4587581}, CityPersons \cite{8099957}, CS \cite{8099843}, CT \cite{dollar2009caltech_dataset}, PRW \cite{zheng2017PRW_dataset}. 
    }
    \label{tab:metrics-on-MOT17}
\end{table*}

\vspace{-0.5em}

\subsection{Implementation details} 

Our method is built on top of Deformable-DETR \cite{zhu2020deformable} with a ResNet-50 \cite{7780459} backbone. We also experiment with a more powerful Swin-L \cite{liu2021swin} backbone to demonstrate the scalability of our method.
The architecture includes \textit{mixed selection} and \textit{look forward twice} refinements introduced in DINO \cite{zhang2022dino}. Unless stated otherwise, we train the models with their default hyper-parameter sets. More details are provided in Appendix \ref{app:hyper_params}. 
Our additional loss term is computed over every transformer decoder layer output, and the temperature value $\tau$ is set to $0.1$. 
The \textit{objectness threshold} is set to 0.5 during inference.

Our model is pre-trained on the object detection task with the procedure described in Section \ref{sec:learnmotfromdet} with the default Deformable-DETR hyperparameters and augmentations. The contrastive loss coefficient is set to $\lambda_{\mathrm{contr}} = 2$. We did not use the contrastive pre-training when pre-trained on COCO \cite{lin2014microsoft}.

\paragrax{MOT17.} We follow a similar procedure described in \cite{zhou2020tracking} by pre-training the model on CrowdHuman (CH)\cite{shao2018crowdhuman}. However, we do not simulate tracking frames. For simplicity of the training procedure, we skip the joint MOT17+CH dataset fine-tuning stage and only use MOT17. 
We pre-train the model for 50 epochs with a batch size of $8$ and augment each image twice for the contrastive loss, leading to a total batch size of $16$. In the training stage, the batches are built by sampling $N_v=2$ videos and $N_f=8$ frames, and $\lambda_{\mathrm{contr}}$ is set to 2. In addition, we allow past detected objects to be kept in memory for a maximum of $T=20$ frames, and we evaluate our method under the private detection protocol.

\paragrax{BDD100K.} Our model is pre-trained on the BDD100K detection dataset for 36 epochs. We use a batch size of 24 and again augment each image twice, leading to a total batch size of 48. 
Afterwards, we train it on the tracking set for 10 epochs and decrease the learning rate by a factor of 10 at epoch 8. Each training batch samples $N_v=4$ videos and $N_f=10$ frames. The contrastive loss coefficient is set to $\lambda_{\mathrm{contr}} = 1$. In addition, we allow past detected objects to be kept in memory for a maximum of $T=9$ frames.

\vspace{-0.5em}

\begin{table*}[t]
\centering \footnotesize
\begin{tabular}{lllllll}
\toprule
Method                              & Backbone  & Pre-train & mHOTA$\uparrow$ & mMOTA$\uparrow$ & mIDF1$\uparrow$ & IDS$\downarrow$ \\ \midrule
\textit{Yu et al.} \cite{bdd100k}   & ResNet-101 & ---        & ---    & 26.3     & 44.7      & 14674 \\ 
QDTrack \cite{cvpr_qdtrack}              & ResNet-50 & BDD100K   & 41.8   & 35.6     & 52.4      & \textbf{10790} \\ 
TETer \cite{li2022TETer_tracking}   & ResNet-50 & BDD100K   & ---    & 37.4     & 53.3      & --- \\
ByteTrack \cite{ByteTrack}          & YOLOX-X   & COCO      & ---    & 40.1     & 55.8      & 15466 \\
SUSHI \cite{Cetintas_2023_CVPR}     & YOLOX-X   & COCO      & \textbf{48.2}    & 40.2     & \textbf{60.0}      & 13626 \\
\textbf{\ourmodelname{}}  (ours)              & Swin-L    & BDD100K   & 46.1 & \textbf{42.8} & 56.5     & 10793   \\
\bottomrule
\end{tabular}
\caption{Results on BDD100K test split, with an objectness threshold of 0.4. The best results are shown in \textbf{bold}.}
\label{tab:metrics-on-BDD100K-test}
\vspace{-2.0em}
\end{table*}


\begin{table*}[t]
\centering \footnotesize
\begin{tabular}{lllllll}
\toprule
Method                                       & Backbone  & Pre-train & mHOTA$\uparrow$ & mMOTA$\uparrow$ & mIDF1$\uparrow$ & IDS$\downarrow$ \\ \midrule               
MOTR \cite{zeng2022motr}        & ResNet-50 & COCO      & ---             & 32.0            & 43.5            & \textbf{3493}   \\
MOTRv2 \cite{Zhang_2023_CVPR}   & ResNet-50 & COCO      & ---             & 35.5            & 48.2            & ---  \\
\textbf{\ourmodelname{}} w/o CPT (ours)  & ResNet-50 & COCO      & 40.2            & 36.4            & 48.1            & 6067  \\ 
\textbf{\ourmodelname{}} (ours)          & ResNet-50 & BDD100K & \textbf{40.8} & \textbf{36.7} & \textbf{49.2} &  6695 \\ \midrule 
MOTRv2 \cite{Zhang_2023_CVPR}   & YOLOX-X+ResNet-50 & BDD100K+COCO & ---       & \textbf{43.6}     & \textbf{56.5}            & ---  \\ 
\textbf{\ourmodelname{}} (ours)          & Swin-L & BDD100K   & 44.4   & 41.7   & 52.9   & 6363            \\ 
\bottomrule
\end{tabular}
\caption{Results on the BDD100K validation set, with an objectness threshold of 0.4. The best results are highlighted in \textbf{bold}. } 
\label{tab:metrics-on-BDD100K-val}
\vspace{-1.4em}
\end{table*}

\subsection{Results}

\paragrax{MOT17.} We report quantitative results on the test set of MOT17 in Table \ref{tab:metrics-on-MOT17}. Our method outperforms comparable DETR-like models (bottom section) on \ac{hota}, suggesting that our model has the best balance between detection and association. It also outperforms comparable DETR-like models on \ac{idf1} which shows the effectiveness of the object representations learned through the instance-level contrastive loss. 

\paragrax{BDD100K.} For BDD100K, we report results on the test set in Table \ref{tab:metrics-on-BDD100K-test}. 
Our method outperforms all methods by a significant margin on the mMOTA metric, including unpublished methods\footnote{The full leaderboard, including unpublished methods, is available on \textit{eval.ai} \cite{evalAI:Online}.}.
We further report results on the validation set (Table \ref{tab:metrics-on-BDD100K-val}) to compare with MOTR \cite{zeng2022motr} and MOTRv2 \cite{Zhang_2023_CVPR}, since the methods were not evaluated on the test set.
MOTRv2 outperforms our method when using an additional YOLOX \cite{ge2021yolox} detector for object proposals, which highly reduces the number of misses.
Nevertheless, our model with a ResNet-50 backbone outperforms MOTR \cite{zeng2022motr} and MORTv2 \cite{Zhang_2023_CVPR}. 
To make a fair comparison, we also pre-train our model on COCO. Although we don't use contrastive pre-training in that setting, we still exceed MOTRv2's performance by +0.3 \ac{mmota} and +1.1 \ac{midf1}. 

\paragrax{Limitations.} Yet, our method does not achieve the same level of performance as the state-of-the-art tracking methods on MOT17. As shown in the table, differences can be attributed to extra training data, more powerful backbones, extra post-processing and more elaborate association strategies. 

\vspace{-0.2em}

\subsection{Ablation study}

We conduct the ablation study on BDD100K \cite{bdd100k} as the dataset is large and offers adequate variation. We pre-train our models on the detection set of BDD100K without the contrastive loss unless specified, and then we train on $1/8$ of the tracking training data by randomly subsampling $25\%$ of the videos and $50\%$ of the frames. The entire validation set is used for evaluation. We use a batch size of 40, $T=5$ previous frames and an \textit{objectness threshold} of 0.5. Due to limited computational resources, the ablations have not been run with the optimal parameters.

\paragrax{Sampling strategy.} In this ablation, we investigate different variations of the number of sampled videos $N_v$ and sampled frames $N_f$ to show the effect of increasing one variable and simultaneously decreasing the other. For the experiment, we used a batch size of $N_vN_f=40$ and a contrastive loss weighting of $\lambda_{\mathrm{contr}} = 2$. Results are given in Table \ref{tab:samples-variation}. As the number of sampled videos $N_v$ decreases and the number of sampled frames per video $N_f$ increases, the probability of sampling larger sets of positive examples rises. This explains the increasing tendency of tracking metrics that emphasize association: the contrastive loss can leverage a greater variety of positive and negative examples. 
However, when sampling all 40 frames within one video, the scenery variability decreases and so the variation in the set of negative examples dramatically drops, harming performance.

\begin{table}[h!]
\centering \footnotesize
\begin{tabular}{lllllll}
\toprule
$N_v$ & $N_f$ & mHOTA & mMOTA & mIDF1 & mAP & IDSw \\
\midrule
20 & 2  & 35.0     & 33.3     & 40.8     & 33.8     & 10116     \\
8  & 5  & 35.6     & 33.3     & 41.6     & 33.8     & 8724     \\
4  & 10 & 35.9     & \textbf{33.7}     & 42.3     & 33.7     & 7869     \\
2  & 20 & \textbf{36.2}     & 33.6     & \textbf{42.9}     & 33.7     & 7483     \\
1  & 40 & 35.4     & 32.1     & 41.3     & \textbf{34.0}     & \textbf{7191}     \\
\bottomrule
\end{tabular}
\caption{Influence of the number of frames sampled per video on BDD100K validation set. The batch size $N_v N_f$ is fixed at 40.}
\label{tab:samples-variation}
\vspace{-1em}
\end{table}

\paragrax{Contrastive loss weight.}
The contrastive loss parameter $\lambda_{\mathrm{contr}}$ also impacts performance. We experimented with different values and found values close to 0.5 to offer the best results. When the coefficient is too high, detection performance suffers, so overall tracking performance decreases.
In practice, we found a slight shift in the optimal value when using contrastive pre-training and the whole training data.

\vspace{-0.5em}
\begin{table}[h!]
\centering \footnotesize
\begin{tabular}{llllll}
\toprule
$\lambda_{\mathrm{contr}}$ & mHOTA & mMOTA & mIDF1 & mAP & IDSw \\ \midrule
0.25 & 36.4         & 34.9          & 42.9          & \textbf{34.8}      & 10105      \\
0.5 & \textbf{36.6}      & \textbf{35.0}      & \textbf{43.1}     & 34.7     & 9216 \\
1   & 36.5      & 34.5      & \textbf{43.1}     & 34.4     & 8502 \\
2   & 35.9      & 33.7      & 42.3     & 33.7     & 7869    \\
3   & 35.0      & 32.2      & 40.9     & 33.2     & 7693 \\
4   & 34.3      & 31.0      & 39.8     & 32.7     & \textbf{7550} \\
\bottomrule
\end{tabular}
\caption{Influence of the contrastive loss weighting on object detection and MOT metrics on BDD100K validation set. Classification and localization coefficients are kept fixed. A higher coefficient reduces the number of identity switches but comes at the cost of a lower mAP. All models have been trained with $N_f=10$, $N_v=4$.} 
\label{tab:contr-loss-weight-variation}
\vspace{-1em}
\end{table}

\paragrax{Contrastive learning, pre-training and projection head.}
We ablate the effect of our contributions on the tracking metrics in Table \ref{tab:contr-proj-cpt}. All models are trained with $N_f=10$, $N_v=4$ and $\lambda_{\mathrm{contr}}=1$.
Without the contrastive loss, the model is not trained for tracking and produces many ID switches (fourth row).
When the model is trained without the FFN projection head, the contrastive loss is applied directly over the embeddings produced by the transformer decoder. In this setup, the embeddings produced by the transformer decoder are used to generate the bounding boxes and class predictions through the respective heads and as tracking embeddings to re-identify objects over time. The performance gap (between the second and third row) shows that this additional head provides more flexibility to the network to generate the tracking embeddings and preserves detection performance.
Pre-training the model with the contrastive loss on object detection using the methodology outlined in Section \ref{sec:learnmotfromdet} improves tracking metrics significantly (first row).
Finally, the performance difference between Deformable-DETR (last row) and ContrasTR (first row) is small in terms of mMOTA. This is because mMOTA overemphasizes the effect of detection performance \cite{Luiten2020IJCV}.

\vspace{-0.5em}

\begin{table}[h!]
\centering \footnotesize
\begin{tabular}{cccllll}
\toprule
Contr. & Proj. & CPT & mHOTA & mMOTA & mIDF1 & mAP \\ \midrule
\cmark  & \cmark  & \cmark    & \textbf{38.0}     & \textbf{35.1} & \textbf{45.1} & 34.3 \\ 
\cmark  & \cmark  & \xmark    & 36.5     & 34.5   & 43.1    & \textbf{34.4} \\
\cmark  & \xmark  & \xmark    & 35.4     & 33.8   & 41.4    & 33.6 \\
\xmark  & \xmark  & \xmark    & 33.4     & 32.9   & 38.3    & 34.3 \\
\bottomrule
\end{tabular}
\caption{Ablation study of different method components on BDD100K validation set: the contrastive loss and the sampling strategy (Contr.); the tracking projection head (Proj.); the contrastive pre-training on object detection (CPT).}
\label{tab:contr-proj-cpt}
\vspace{-1.5em}
\end{table}