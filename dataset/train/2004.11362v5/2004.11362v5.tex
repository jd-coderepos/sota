\documentclass[10pt,letterpaper]{article}


\usepackage[nonatbib]{neurips_2020}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{wrapfig}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\definecolor{blue}{rgb}{1,0,0}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\newcommand{\bff}[1]{\bf{#1}\normalfont}
\newcommand{\etal}{~et al.~}
\newcommand{\bt}[1]{\bf{#1}\normalfont}
\newcommand {\tr}{{\hspace{-0.025in}\vspace{-0.04in} \scriptstyle \top \normalfont}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\rhs}{\mathbf{b}}
\newcommand{\crhs}{b}
\newcommand{\er}{\mathbf{e}}
\newcommand{\vc}{\mathbf{v}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\Ls}{\tilde{L}}
\newcommand{\Ts}{\tilde{T}}
\newcommand{\spr}{\Phi}
\newcommand{\inds}{\mathcal{I}}
\newcommand{\wsp}{\mathcal{W}}
\newcommand{\vsp}{\mathcal{V}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\eq}{\!=\!}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{}}}}}
\makeatother



\newcommand{\mcrs}{\mathcal{C}}
\newcommand{\mfin}{\mathcal{F}}
\newcommand{\crs}{~}
\newcommand{\fin}{~}

\newcommand{\dilip}[1]{{\color{red}\bf{DILIP: }\normalfont \emph{#1}}}
\newcommand{\prannay}[1]{{\color{red}\bf{PRANNAY: }\normalfont \emph{#1}}}
\newcommand{\piotr}[1]{{\color{blue}\bf{PIOTR: }\normalfont \emph{#1}}}
\newcommand{\aj}[1]{{\color{red}\bf{AJ: }\normalfont \emph{#1}}}
\newcommand{\aaron}[1]{{\color{red}\bf{AARON: }\normalfont \emph{#1}}}
\newcommand{\yonglong}[1]{{\color{red}\bf{YONGLONG: }\normalfont \emph{#1}}}
\newcommand{\chen}[1]{{\color{red}\bf{CHEN: }\normalfont \emph{#1}}}
\newcommand{\ce}[1]{{\color{red}\bf{CE: }\normalfont \emph{#1}}}
\newcommand{\phillip}[1]{{\color{red}\bf{PHILLIP: }\normalfont \emph{#1}}}





\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}

\newcommand{\eqn}[1]{Eq.~\ref{eqn:#1}}
\newcommand{\eqs}[2]{Eqs.~\ref{eqn:#1}--\ref{eqn:#2}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\secc}[1]{Section~\ref{sec:#1}}
\newcommand{\app}[1]{Appendix~\ref{app:#1}}
\newcommand{\alg}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\later}[1]{}
\newcommand{\shorter}[1]{#1}	\newcommand{\longer}[1]{}	\newcommand{\lemm}[1]{Lemma~\ref{lem:#1}}


\def\BE{}
\def\BEA{}
\def\BEAS{}
 \begin{document}

\title{Connections between supervised contrastive loss, cross entropy loss, label smoothing and noise contrastive loss.}

\maketitle

The connection between contrastive loss, cross entropy loss, label smoothing and noise contrastive estimation: let  be the normalized or unnormalized vectors under consideration; in case of the contrastive loss, these might be -dimensional normalized vectors; in the case of cross-entropy the -dimensional ( is number of classes). If labels are given represent the label vector with the letter ; thus  is a 1-hot vector for sample  with a 1 in the right position for sample . Now the multi-positive, multi-negative contrastive loss that performs well is given by:



where  corresponds to the index of the positive sample associated with ; and  is the 'th negative sample for . In contrastive learning, we always assume that  and  are also ``variable" that are associated with the training process. 

Now suppose we consider the following case: single positive;  negatives; where the positive and negatives are specific 1-hot vectors: the positive has a single 1 in the position of the class as sample ; and the negatives are in the other  positions. Then, we can write Eq. \ref{eqn:conloss} as:



and we see Eq. \ref{eqn:conxent} can be simplified to the standard cross-entropy loss, by re-writing the above as:



where  is given by:



where  refers to the class for sample ; and  refers to the -th element of vector . Thus, cross-entropy with 1-hot vectors can be treated as a specific case of the contrastive loss with specific fixed (1-hot) vectors being used as positives and negatives. Therefore, we can reinterpret MI-based analysis by using this equivalence: the first view of the data is the sample itself; the second view of the data is given by the labels; and we try to maximize MI between the labels and the data. Note that as compared to Eq. \ref{eqn:conloss}, Eq. \ref{eqn:conxent} does not have  normalized  vectors, but \emph{does} have normalized  vectors (although the 1-hot vector is normalized in any  norm; see below). Noise contrastive estimation can be seen as a special case of Eq. \ref{eqn:conxent}, when the number of classes , and half the samples are from the data distribution and the other half are ``noise", with data being assigned to class  and noise to class .

Now, let's start from Eq. \ref{eqn:celoss} (label smoothed cross-entropy loss) and  pull the  back into the , then we get:



where . Hence label smoothing can be seen upper bounded by a contrastive loss with a special form: per sample/class temperatures in the contrastive loss for the positive and negative samples in the loss; and multiple () positives for each sample  instead of a single positive. The definition of  in Eq. \ref{eqn:celsloss} can be set to any vector that one may desire; for the case of label smoothing:



E.g. often  and , so that  is still an \emph{} normalized vector. 

\begin{enumerate}
\item What is the ``optimal" form of the contrastive loss given the many choices that come up above: choice of positive and negative vectors; per sample temperature scaling; normalize or unnormalize? How can we analyze these choices? 

\item If one incorporates noise as additional negatives, so that we have noise contrastive and data contrastive terms, then what is the mutual information interpretation?

\item Techniques developed to analyze cross-entropy may be useful for analyzing contrastive, and vice-versa. 

\end{enumerate}




\end{document}