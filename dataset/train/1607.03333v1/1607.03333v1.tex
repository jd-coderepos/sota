









\documentclass[journal]{IEEEtran}


\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{bm}
\newcommand{\etal}{\textit{et al.~}}
\def\hspacefigure{\hspace{-0.8mm}}
\def\widtheight{0.13\linewidth}
\def\widtheighttwo{0.23\linewidth}
\def\widthnine{0.092\linewidth}
\def\widthtwelve{0.086\linewidth}
\def\widthNne{0.086\linewidth}
\usepackage{array}
\newcolumntype{I}{!{\vrule width 1.12pt}}



\ifCLASSINFOpdf
\else
\fi


\usepackage[bookmarks=false]{hyperref}


\usepackage{amsmath}

\usepackage{algorithmic}


\begin{document}
\title{RGBD Salient Object Detection via Deep Fusion}


\author{Liangqiong~Qu,
        Shengfeng~He,
        Jiawei~Zhang,
        Jiandong~Tian,
        Yandong~Tang,
        and~Qingxiong~Yang
        \\{\url{http://www.cs.cityu.edu.hk/~jiawzhang8/saliency/TIP_saliency.htm}} \thanks{L. Qu, S. He, J. Zhang, and Y. Qing are with the Department of Computer Science, City University of Hong Kong, Hong Kong. L. Qu
is also with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, and the University of Chinese Academy of Sciences, Beijing, China, 100049. (E-mail: quliangqiong@sia.cn; shengfeng\_he@yahoo.com; jiawzhang8-c@my.cityu.edu.hk; qiyang@cityu.edu.hk).}
\thanks{J. Tian and Y. Tang are with the State Key Laboratory of Robotics,
Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016 (E-mail:
tianjd@sia.cn; ytang@sia.cn).}}





\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}














\maketitle

\begin{abstract}
Numerous efforts have been made to design different low level saliency cues for the RGBD saliency detection, such as color or depth contrast features, background and color compactness priors. However, how these saliency cues interact with each other and how to incorporate these low level saliency cues effectively to generate a master saliency map remain a challenging problem.
In this paper, we design a new convolutional neural network (CNN) to fuse different low level saliency cues into hierarchical features for automatically detecting salient objects in RGBD images. In contrast to the existing works that directly feed raw image pixels to the CNN, the proposed method takes advantage of the knowledge in traditional saliency detection by adopting various meaningful and well-designed saliency feature vectors as input. This can guide the training of CNN towards detecting salient object more effectively due to the reduced learning ambiguity. We then integrate a Laplacian propagation framework with the learned CNN to extract a spatially consistent saliency map by exploiting the intrinsic structure of the input image. Extensive quantitative and qualitative experimental evaluations on three datasets demonstrate that the proposed method consistently outperforms state-of-the-art methods.
\end{abstract}

\begin{IEEEkeywords}
RGBD saliency detection, Convolutional neural network, Laplacian propagation.
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{S}ALIENCY detection, which is to predict where human looks in the image,
has attracted a lot of research interests in recent years. It serves as an important pre-processing step
in many problems such as image classification, image retargeting and
object recognition \cite{rutishauser2004bottom,mahadevan2009saliency,sharma2012discriminative,luo2014switchable}.
Unlike RGB saliency detection which receives much research attention, there are not many exploration on RGBD cases. The recently emerged sensing technologies, such as Time-of-flight sensor and Microsoft Kinect, provides excellent ability and flexibility to capture RGBD image \cite{zhang2012microsoft,gokturk2004time}.
Detecting RGBD saliency becomes essential for many applications such as 3D content surveillance, retrieval, and image recognition \cite{mishra2012segmenting,fu2015object,banica2015second}. In this paper, we focus on how to integrate RGB and the additional depth information for RGBD saliency detection.


\begin{figure*}
\centering
\captionsetup[subfigure]{labelformat=empty}
\subfloat[(a)]{ \label{fig:Intro:a}\includegraphics[width=\widthnine]{10_01-00-31.jpg}} \hspacefigure
\subfloat[(b)]{ \label{fig:Intro:b}\includegraphics[width=\widthnine]{10_01-00-31_Depth.png}}\hspacefigure
\subfloat[(c)]{ \label{fig:Intro:c}\includegraphics[width=\widthnine]{10_01-00-31_GT.jpg}}\hspacefigure
\subfloat[(d)]{ \label{fig:Intro:d}\includegraphics[width=\widthnine]{10_01-00-31_houwen.jpg}} \hspacefigure
\subfloat[(e)]{ \label{fig:Intro:e}\includegraphics[width=\widthnine]{10_01-00-31_ACSD.jpg}} \hspacefigure
\subfloat[(f)]{ \label{fig:Intro:f}\includegraphics[width=\widthnine]{10_01-00-31_GP.jpg}} \hspacefigure
\subfloat[(g)]{ \label{fig:Intro:g}\includegraphics[width=\widthnine]{10_01-00-31_linear.jpg}} \hspacefigure
\subfloat[(h)]{ \label{fig:Intro:h}\includegraphics[width=\widthnine]{10_01-00-31_MCA.jpg}}\hspacefigure
\subfloat[(i)]{ \label{fig:Intro:i}\includegraphics[width=\widthnine]{10_01-00-31_Qumerg.jpg}}\hspacefigure
\subfloat[(j)]{ \label{fig:Intro:j}\includegraphics[width=\widthnine]{10_01-00-31_QU_RGBD.jpg}} \hspacefigure\\
\vspace{-1.5mm}
  \caption{Example to show the problem of saliency map merging methods.
(a) Original RGB image. (b) Original depth image. (c) Ground truth saliency map.
  (d) Saliency map by LMH \cite{peng2014rgbd}. (e) Saliency map by ACSD \cite{ju2014depth}. (f) Saliency map by GP \cite{ren2015exploiting}. (g) to (i) are the saliency map integration results of (d), (e), and (f). (g) Linear combination (i.e., averaging). (h) MCA integration \cite{qin2015saliency}. (i) CNN based fusion. (j) Saliency map by the proposed hyper-feature fusion.}\vspace{-1.5mm}
\label{fig:Intro}
\vspace{-2mm}
\end{figure*}

According to how saliency is defined, saliency detection methods can be classified into two categories: top-down approach and bottom-up approach \cite{itti1998model,ma2003contrast}. Top-down saliency detection is task-dependent that incorporates high level features to locate the salient object. On the other hand, bottom-up approach is task-free, and it utilizes low level features that are biologically motivated to estimate salient regions. Most of the existing bottom-up saliency detection methods focus on designing different low-level cues to represent salient objects. The saliency maps of these low-level features are then fused to become a master saliency map. As human attention are preferentially attracted by the high contrast regions with their surrounding, contrast-based features (like the color, edge orientation or texture contrasts) make a crucial role to derive the salient objects. Background \cite{wei2012geodesic} and color compactness priors~\cite{keyang2013} consider salient object in different perspectives. The first one leverages the fact that most of the salient objects are far from image boundaries, the latter one utilizes the color compactness of the salient object. In addition to RGB information, depth has been shown to be one of the practical cue to extract saliency \cite{maki1996computational,lang2012depth,desingh2013depth,zhang2010stereoscopic}. Most existing approaches for 3D saliency detection either treat the depth map as an indicator to weight the RGB saliency map \cite{maki1996computational,zhang2010stereoscopic} or consider depth cues as an independent image channel \cite{lang2012depth,desingh2013depth}.

Notwithstanding the demonstrated success of these features, whether these features complement to each other remains a question. The interaction mechanism of different saliency features is not well explored, and it is not clear how to integrate 2D saliency features with depth-induced saliency feature in a better way. Linearly combining the saliency maps produced by these features cannot guarantee better result (as shown in Figure~\ref{fig:Intro:g}). Some other more complex combination algorithms have been proposed in \cite{liu2011learning,yan2013hierarchical,qin2015saliency,zhou2015salient,peng2014rgbd,ren2015exploiting}.
Qin \emph{et al.} \cite{qin2015saliency} propose a Multi-layer Cellular Automata (MCA, a Bayesian framework) to merge different saliency maps by taking advantage of the superiority of each saliency detection methods. Recently, several heuristic algorithms are designed to combine the 2D related saliency maps and depth-induced saliency map \cite{peng2014rgbd,ren2015exploiting}.
However, as restricted by the computed saliency values, these saliency map combination methods are not able to correct wrongly estimated salient regions. For example in Figure \ref{fig:Intro}, heuristic based algorithms (Figure \ref{fig:Intro:d} to \ref{fig:Intro:f}) cannot detect the salient object correctly. Adopting these saliency maps for further fusion, neither simple linear fusion (Figure \ref{fig:Intro:g}) nor MCA integration (Figure \ref{fig:Intro:h}) are able to recover the salient object. We wonder whether a good integration can address this problem by further adopting Convolutional Neural Network technique to train a saliency map integration model. The resulted image shown in Figure \ref{fig:Intro:i} indicates that saliency map integration is hugely influenced by the quality of the input saliency maps. Based on the these observations, we take one step back to handle more raw and flexible saliency features.

In this paper, we propose a deep fusion framework for RGBD saliency detection. The proposed method takes advantage of the representation learning power of CNN to extract the hyper-feature by fusing different hand-designed saliency features to detect salient object (as shown in Figure~\ref{fig:Intro:j}). We first compute several feature vectors from original RGBD image, which include local and global contrast, background prior, and color compactness. We then propose a CNN architecture to incorporate these regional feature vectors into a more representative and unified features. Compared with feeding raw image pixels, these extracted saliency features are well-designed and they can guide the learning of CNN towards saliency-optimized more effectively. As the resulted saliency map may suffer from local inconsistency and noisy false positive, we further integrate a Laplacian propagation framework with the proposed CNN. This approach propagates high confidence saliency to the other regions by taking account of the color and depth consistency and the intrinsic structure of the input image \cite{zhou2004learning}, which is able to remove noisy values and produce smooth saliency map. The Laplacian propagation is solved with fast convergence by the adoption of Conjugate gradient and preconditioner. Experimental evaluations demonstrate that, once our deep fusion framework are properly trained, it generalizes well to different datasets without any additional training and outperforms the state-of-the-art approaches.

The main contributions of this paper are summarized as follows.

1. We propose a simple yet effective deep learning model to explore the interaction mechanism of RGB and depth-induced saliency features for RGBD saliency detection. This deep model is able to generate representative and discriminative hyper-features automatically rather than hand-designing heuristical features for saliency. 

2. We adopt Laplacian Propagation to refine the resulted saliency map and solve it with fast convergence.
Different from CRF model, our Laplacian Propagation not only considers the spatial consistency but also exploits the intrinsic structure of the input image \cite{zhou2004learning}. Extensive experiments further demonstrate that this proposed Laplacian Propagation is able to refine the saliency maps of existing approaches, which can be widely adopted as a post processing step.

3. We investigate the limitations of saliency map integration, and demonstrate that simple features fusion are able to obtain superior performance.
\section{Related work}
\begin{figure*}
\centering
\includegraphics[width=0.99\linewidth]{framework3.pdf}
\vspace{-1.0mm}
   \caption{The pipeline of the proposed method. Our method composes of three modules. First, it generates different RGB and depth based saliency feature vectors from the RGBD input image. These generated saliency feature vectors are then fed to the CNN. The CNN takes an input of size  and generates the saliency confidence value (the probability of this patch belonging to salient). Finally, a Laplacian propagation is performed on the resulted probabilities to extract the final spatially consistent saliency map.
  }
\label{fig:Convnet}
\vspace{-1.5mm}
\end{figure*}

In this section, we give a brief survey and review of RGB and RGBD saliency detection methods, respectively. Comprehensive literature reviews on these saliency detection
methods can be found in ~\cite{borji2014salient,peng2014rgbd}.

\textbf{RGB saliency detection:} As suggested by the studies of cognitive science~\cite{itti1998model}, bottom-up saliency is driven by low-level stimulus features. This concept is also adopted in computer vision to model saliency. Contrast-based cues, especially color contrast, are the most widely adopted features in previous works. These contrast-based methods can be roughly classified into two categories: local and global approaches. Local method calculates color, edge orientation or texture contrast of a pixel/region with respect to a local window to measure saliency \cite{itti1998model,bruce2005saliency}. In \cite{itti1998model}, they develop an early local based visual saliency detection method by computing center surrounding differences across multi-scale image features to estimate saliency. Bruce \emph{et al.} \cite{bruce2005saliency} propose to apply sparse representation on local image patches. However, based only on local contrast, these methods may highlight the boundaries of salient object \cite{keyang2013} and be sensitive to high frequency content \cite{shenfeng2014}. In contrast to local approach, the global approach measures salient region by estimating the contrast over the entire image. Achanta \etal \cite{achanta2009frequency} model saliency by computing color difference to the mean image color. Cheng \etal \cite{ChengPAMI} propose a histogram-based global contrast saliency method by considering the spatial weighted coherence. Although these global methods achieve superior performances, they may suffer from distractions when background shares similar color to the salient object. Background and color compactness priors are proposed as a complement to contrast-based methods \cite{wei2012geodesic,shen2012unified,keyang2013}. These methods are built on strong assumptions, which may invalid in some scenarios.

As each feature has different strengths, some works focus on designing the integration mechanism for different saliency features \cite{liu2011learning,shen2012unified,yan2013hierarchical,jiang2013salient}. Liu \etal \cite{liu2011learning} use CRF to integrate three different features from both local and global point of views. Yan \etal \cite{yan2013hierarchical} propose a hierarchical framework to integrate saliency maps in different scales, which can handle small high contrast regions well. Unlike these methods that directly combine the saliency maps obtained from different saliency cues, the proposed method records low-level saliency feature in vector forms and jointly learns the interaction mechanism to become a hyper-feature with CNN.

Similar to the proposed method, CNN has been adopted in some other works to extract hierarchical feature representations for detecting salient regions \cite{vig2014large,zhao2015saliency,li2015visual,he2015supercnn,wang2015deep,LiYu16}. In contrast to most of these deep networks that take raw image pixels as input, the proposed method aims at designing a unified CNN framework to learn the interaction mechanism of different saliency cues.

\textbf{RGBD saliency detection:} Unlike RGB saliency detection, RGBD saliency receives less research attention \cite{maki1996computational,zhang2010stereoscopic,desingh2013depth,lang2012depth,wang2013computational}. Maki \etal \cite{maki1996computational} propose an early computational model on depth-based attention by measuring disparity, flow and motion. Similar to color contrast, Zhang \etal design a stereoscopic visual attention algorithm based on depth and motion contrast for 3D video \cite{zhang2010stereoscopic}. Desingh \etal \cite{desingh2013depth} estimate saliency regions by fusing the saliency maps produced by appearance and depth cues independently. These methods either treat the depth map as an indicator to weight the RGB saliency map \cite{maki1996computational,zhang2010stereoscopic} or consider depth map as an independent
image channel for saliency detection \cite{lang2012depth,desingh2013depth}. On the other hand, Peng \etal \cite{peng2014rgbd} propose a multi-stage RGBD model to combine both depth and appearance cues to detect saliency. Ren \etal \cite{ren2015exploiting} integrate the normalized depth prior and the surface orientation prior with RGB saliency cues directly for the RGBD saliency detection. These methods combine the depth-induced saliency map with RGB saliency map either directly  \cite{ju2014depth,ren2015exploiting} or in a hierarchy way to calculate the final RGBD saliency map \cite{peng2014rgbd}. However, these saliency map level integration is not optimal as it is restricted by the determined saliency values. On the contrary, we incorporate different saliency cues and fuse them with CNN in feature level.



\section{Proposed method}
As shown in Figure \ref{fig:Convnet}, the proposed deep fusion framework for RGBD salient object detection composes of three modules. The first module generates various saliency feature vectors for each superpixel region. The second module is to extract hyper-feature representation from the obtained saliency feature vectors. The third module is the Laplacian propagation framework which helps to detect a spatially consistent saliency map.

\subsection{Saliency feature vectors extraction}\label{title_3_1}
Given an image, we aim to represent saliency by some demonstrated effective saliency features.
Figure \ref{fig:saliency_cue} gives an illustration on the proposed saliency feature extraction. We first segment the image into \emph{N} superpixels using SLIC method \cite{achanta2012slic}. Given a RGB image , we denote the segmented \emph{N} regions as . For each superpixel , we denote the calculated saliency features as a vector .
In the following, we will take region  (the region that marked in orange in Figure \ref{fig:saliency_cue}) as an example to show how we calculate different saliency feature vectors.
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\linewidth]{feature_different.pdf}

   \caption{Saliency feature extraction.}
\label{fig:saliency_cue}
\vspace{-1.0mm}
\end{figure*}

Different from the classical saliency detection methods that directly calculate the saliency values for each superpixel, we record the saliency features for each image region and no further operation is performed to make saliency features as raw as possible. For region , there are seven types of feature vectors: , where  and  represent color and depth information respectively,  indicates that saliency is determined in the local scope and  indicates the global scope,  and  represent the background and color compactness priors respectively. More specifically, the color based feature vectors are recorded in the following formula,

and the depth based feature vectors are defined similarly. We compute the color-based features in  color space.

The local color contrast  is calculated as:

where  is the total number of pixels in region , and a larger superpixel contributes more to the saliency.  and  are the mean color values of the region  and .  is used to control the spatial influential distance. This weight is defined as , and  and  are the centers of corresponding regions. In our experiment, the parameter  = 0.15 is set to make the neighbors have higher influence on the calculated contrast values, while the influence of other regions are negligible. Similar to the local color contrast vector, the global color contrast vector is defined as,

The difference between the global contrast and local contrast lies in the spatial weight , where in the global contrast the parameter  is set to 0.45 to cover the entire image.

Likewise, the depth contrast between region  and region  can be calculated as in Eq. \ref{Depth_local} and Eq. \ref{Depth_global}.


where  and  are the mean depth values of the region  and  respectively.

Generally speaking, the colors of an object are compacted together whereas the colors belong to the background are widely distributed in the entire image. The element  in the color compactness based feature vector is calculated as following.

where the function  is used to calculate the similarity of two colors  and , and is defined as .  defines the weighted mean position of color . The parameter  is set to 20 in our implementation. We omit the depth compactness prior in our method since the depth map contains only dozens of depth levels and their spatial distributions can be very random. The experiment results also show that whether adding the depth compactness or not does not affect the final results too much.

Beside color compactness prior, we further introduce the background prior, which leverages the fact that salient object is less possible to be arranged to close to the image boundaries. We first extract  regions along the image boundary as pseudo-background regions. Then the color or depth contrast to the pseudo-background regions will be calculated similar to Eq. \ref{color_contrast_global} and Eq. \ref{Depth_global}. In our experiment, the number of superpixels  is set to 1024 and  is set to 160.

\subsection{Hyper-feature extraction with CNN}\label{title_3_2}

Given the obtained saliency feature vectors, we then propose a CNN architecture to automatically incorporate them into unified and representative features. We formulate saliency detection as a binary logistic regression problem, which takes a patch as input and output the probabilities of two classes. Our CNN takes an input of size , and generates a prediction as saliency output. For each superpixel , all the seven saliency feature vectors are integrated into a multiple channel image as follows:

(1) Reshape the  length vector (, , ,  and ) to size  to form the first five channels, respectively;

(2) Perform zero padding to the  length vector  and  to length  and then concatenate and reshape them into size  to form the sixth channel.

As shown in Figure \ref{fig:Convnet}, our network consists of three convolutional layers followed by a fully connected layer and a logistic regression output layer with sigmoid nonlinear function. Following the first and second convolutional layers, we add an average pooling layer for translation invariance. We adopt the sigmoid function as the nonlinear mapping function for the three convolutional layers, while Rectified Linear Unites (ReLUs) is applied in the last two layers. Dropout procedure is applied after the first fully connected layers to avoid overfitting.

For simplification, we use  and  to indicate the convolutional layer and the fully connected layer with  output and kernel size .  indicates the pooling layer with type  and kernel size .  and  represent the sigmoid function and ReLUs. Then the architecture of our CNN can be described as . This proposed CNN was trained with back-propagation using stochastic gradient descent (SGD).

\subsection{Laplacian propagation}
As saliency values are estimated for each superpixel individually, the proposed CNN in Section \ref{title_3_2} may fail to retain the spatial consistency and lead to noisy output. Figure \ref{fig:init_refine:c} shows two examples of the saliency maps produced by our CNN for RGBD image. It indicates that our CNN omits some salient regions and wrongly detects some background regions as salient. Despite these misdetected regions, most of the regions with high probability to be salient are correct, robust, and reliable. The same situation also occurs for non-salient probability in the background (Figure \ref{fig:init_refine:d}). As a consequence, these high confident regions are used as guidance, and they are employed in a Laplacian propagation framework \cite{zhou2004learning} to obtain a more spatially consistent saliency map. The key of the Laplacian propagation lies in propagating the saliency from the regions with high probability to those ambiguous regions by considering two criteria:
(1) neighboring regions are more likely to have similar saliency values; and (2) regions within the same manifold are more likely to have similar saliency values.
\begin{figure*}[t]
\centering
\captionsetup[subfigure]{labelformat=empty}
\subfloat{ \label{fig:init_refine:a}\includegraphics[width=\widtheight,height=0.12\linewidth]{10_03-25-34.jpg}} \hspacefigure
\subfloat{ \label{fig:init_refine:b}\includegraphics[width=\widtheight,height=0.12\linewidth]{10_03-25-34_depth.png}}\hspacefigure
\subfloat{ \label{fig:init_refine:c}\includegraphics[width=\widtheight,height=0.12\linewidth]{10_03-25-34_sal_probabilty.png}}\hspacefigure
\subfloat{ \label{fig:init_refine:d}\includegraphics[width=\widtheight,height=0.12\linewidth]{10_03-25-34_non_sal_probabilty.png}} \hspacefigure
\subfloat{ \label{fig:init_refine:e}\includegraphics[width=\widtheight,height=0.12\linewidth]{10_03-25-34_final_sal.png}} \hspacefigure
\subfloat{ \label{fig:init_refine:f}\includegraphics[width=\widtheight,height=0.12\linewidth]{10_03-25-34_GT.jpg}} \hspacefigure\\
\vspace{-1.5mm} \subfloat[(a)]{ \includegraphics[width=\widtheight]{6_07-00-18_o.jpg}}\hspacefigure
 \subfloat[(b)]{ \includegraphics[width=\widtheight]{6_07-00-18_depth.png}}\hspacefigure
 \subfloat[(c)]{ \includegraphics[width=\widtheight]{6_07-00-18_sal_probabilty.png}}\hspacefigure
 \subfloat[(d)]{ \includegraphics[width=\widtheight]{6_07-00-18_non_sal_probabilty.png}}\hspacefigure
\subfloat[(e)]{ \includegraphics[width=\widtheight]{6_07-00-18_final_sal.png}}\hspacefigure
 \subfloat[(f)]{ \includegraphics[width=\widtheight]{6_07-00-18_GT.jpg}}\hspacefigure\\
  \caption{Examples of the proposed Laplacian propagation. (a) RGB image. (b) Depth image. (c) Saliency probability produced by the proposed CNN. (d) Background (non-salient) probability produced by the proposed CNN. (e) Refined saliency map using (c) and (d) as guidance. (f) The ground truth saliency map.}
  \vspace{-4mm}
\label{fig:init_refine}
\end{figure*}

Given a set of superpixels  of an input image  and a label set , we denote the salient and non-salient probability generated by the proposed CNN as  and . The superpixels in  are labeled as 1 if ,  or as 2 if . The goal of Laplacian propagation is to predict the labels of the remaining regions.

Let  denotes a  non-negative matrix which corresponds to the binary classification results of ,
and each region  is assigned with a label , where . An indicator matrix is defined as  with  if region  is labeled as , otherwise .
We further adopt the color and depth information to form the affinity matrix :

where the first term defines the color distance of superpixel region  and , and the second term defines the relative depth distance. Most of the elements of the affinity matrix  are zero except for those neighbouring  and  pairs. In order to better leverage the local smoothness, we use a two-hierarchy neighboring connection model, i.e., each region is not only connected to its neighboring regions but also connected to the regions that share the same boundaries with its neighboring regions. We set  to avoid self-reinforcement.
Then the Laplacian propagation can be formulated to solve the following optimization functions:


where parameter  controls the balance between the smoothness constraint (the first term) and the fitting constraint (the second term).  is the element of the degree matrix  derived from affinity matrix , and .
This designed smoothness constraint not only considers local smoothness but also confines the regions within the same manifold to have the same label by
constructing a smooth classifying function. This classifying function can change sufficiently slow along the coherent structure revealed by the original image \cite{zhou2004learning}.

This optimization function Eq. \ref{sal_optimization} can be solved using an iteration algorithm as shown in \cite{zhou2004learning}, or it can be reformulated into a linear system. For efficiency, we set the derivative of the  to zero and the optimal solution of Eq. \ref{sal_optimization} can be obtained by solving the following linear equation:

where  is an identity matrix and . We further adopt Conjugate Gradient and  preconditioner to solve this linear equation for fast convergence.

After propagating from the high probability salient and non-salient regions, the final saliency map is normalized to [0,1] and it is denoted as . Two examples of the proposed propagation are shown in Figure \ref{fig:init_refine}. Those wrongly estimated regions in Figure \ref{fig:init_refine:b} and Figure \ref{fig:init_refine:c} are corrected in the final saliency maps produced by the Laplacian propagation. In our implementation, parameters  and  are adaptively determined by Otsu method \cite{otsu1975threshold}.
\begin{figure*}[t]
\centering
\captionsetup[subfigure]{labelformat=empty}
\subfloat{ \label{fig:saliency2:a}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:b}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_depth.png}}\hspacefigure
\subfloat{ \label{fig:saliency2:c}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_GT.pdf}}\hspacefigure
\subfloat{ \label{fig:saliency2:e}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_SuperCNN.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:f}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_BSCA.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:g}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_MB+.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:g}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_LEGS.png}} \hspacefigure
\subfloat{ \label{fig:saliency2:i}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_houwen.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:j}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_ACSD.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:k}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_GP.pdf}} \hspacefigure
\subfloat{ \label{fig:saliency2:l}\includegraphics[width=\widthtwelve,height=0.091\linewidth]{000301_left_QU_RGBD.pdf}} \hspacefigure\\
\vspace{-1.5mm} \subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_SuperCNN.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.075\linewidth]{9_03-46-43_QU_RGBD.pdf}} \hspacefigure\\
 \iffalse \vspace{-1.5mm}
\subfloat{  \includegraphics[width=\widthtwelve]{3_10-23-35.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_SuperCNN.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{3_10-23-35_QU_RGBD.pdf}} \hspacefigure\\
\fi
 \vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_sf.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-17-08_QU_RGBD.pdf}} \hspacefigure\\
 \vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_SuperCNN.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000136_left_QU_RGBD.pdf}} \hspacefigure\\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve]{62.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_SuperCNN.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{62_QU_RGBD.pdf}} \hspacefigure\\
\vspace{-1.5mm}
\subfloat{  \includegraphics[width=\widthtwelve]{10_01-05-11.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_sf.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{10_01-05-11_QU_RGBD.pdf}} \hspacefigure\\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_depth.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_GT.pdf}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_SuperCNN.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_BSCA.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_MB+.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_LEGS.png}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_houwen.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_ACSD.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_GP.pdf}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.07\linewidth]{000759_left_QU_RGBD.pdf}} \hspacefigure\\
\vspace{-1.5mm}
\subfloat[{RGB}]{  \includegraphics[width=\widthtwelve]{10_12-54-12.pdf}} \hspacefigure
\subfloat[{Depth}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_depth.pdf}}\hspacefigure
\subfloat[{GT}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_GT.pdf}}\hspacefigure
\subfloat[{S-CNN}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_SuperCNN.pdf}} \hspacefigure
\subfloat[{BSCA} ]{ \includegraphics[width=\widthtwelve]{10_12-54-12_BSCA.pdf}} \hspacefigure
\subfloat[{MB+} ]{ \includegraphics[width=\widthtwelve]{10_12-54-12_MB+.pdf}} \hspacefigure
\subfloat[{LEGS}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_LEGS.png}} \hspacefigure
\subfloat[{LMH}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_houwen.pdf}} \hspacefigure
\subfloat[{ACSD}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_ACSD.pdf}} \hspacefigure
\subfloat[{GP}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_GP.pdf}} \hspacefigure
\subfloat[{Ours}]{ \includegraphics[width=\widthtwelve]{10_12-54-12_QU_RGBD.pdf}} \hspacefigure\\
  \caption{Visual comparisons of the proposed deep fusion framework with four RGB saliency methods and three RGBD saliency methods. The saliency maps of S-CNN \cite{he2015supercnn}, BSCA \cite{qin2015saliency}, MB+ \cite{zhang2015MBD}, and LEGS \cite{wang2015deep} are obtained from RGB image while the saliency maps of LMH \cite{peng2014rgbd}, ACSD \cite{ju2014depth}, GP \cite{ren2015exploiting} are from RGBD image.}
\label{fig:saliency2}
\end{figure*}

\section{Experimental evaluations}
In this section, we evaluate the proposed method on three datasets, NLPR RGBD salient dataset \cite{peng2014rgbd}, NJUDS2000 stereo datast \cite{ju2014depth}, and LFSD dataset \cite{Li_2014_CVPR}.

\textbf{NLPR dataset \cite{peng2014rgbd}.}
The NLPR RGBD salient dataset \cite{peng2014rgbd} contains 1000 images captured by Microsoft Kinect in different indoor and outdoor scenarios. We split this dataset into two part randomly: 750 for training and 250 for testing.

\textbf{NJUDS2000 dataset \cite{ju2014depth}.}
The NJUDS2000 dataset contains 2000 stereo images, as well as the corresponding depth maps and manually labeled groundtruth. The depth maps are generated using an optical flow method.
We also split this dataset into two part randomly: 1000 for training and 1000 for testing.

\textbf{LFSD dataset \cite{Li_2014_CVPR}.}
The LFSD dataset \cite{Li_2014_CVPR} contains 100 images with depth information and manually labeled groundtruth. The depth information are captured with Lytro light field camera. All the images in this dataset are for testing.

\textbf{Evaluation metrics.} We compute the precision-recall (PR) curve, mean of average precision and recall, and F-measure score to evaluate the performance of different saliency detection methods. The PR curve indicates the mean precision and recall of the saliency map at different thresholds. The F-measure is defined as , where  is set to 0.3.

\subsection{Implementation details}
We use the randomly sampled 750 training images of NLPR dataset \cite{peng2014rgbd} and the randomly sampled 1000 training images of NJUDS2000 dataset \cite{ju2014depth} to train our deep learning framework. These randomly selected training dateset covers more than 1000 kinds of common objects under different circumstances. The remaining NLPR, NJUDS2000, and LFSD datesets are used to verify the generalization of the proposed method.

The proposed method is implemented using Matlab. We set the momentum in our network to 0.9 and the weight decay to be 0.0005. The learning rate of our network is gradually decreased from 1 to 0.001. Due to the ``data-hungry'' nature of CNN, the existing training data is insufficient for training, in addition to the dropout procedure, we also employed data augmentation to enrich our training dataset. Similar to \cite{krizhevsky2012imagenet}, we adopted two different image augmentation operations, the first one consists of image translations and horizontal flipping and the other is to alter the intensities of the RGB channels. These data augmentations greatly enlarge our training dataset and make it possible for us to train the proposed CNN without overfitting. It took around  days for our training to converge.

\begin{table*}
\centering
\caption{The F-measure scores of different approaches on three datasets.}
\label{table:belta}
\begin{tabular}{|c||c|c|c|cIc|c|c|c|}
\hline
{Dataset} & S-CNN  & BSCA  &  MB+ & LEGS & LMH & ACSD & GP & Ours \\
\hline\hline
NLPR test set & 0.5141 & 0.5634 & 0.6049 & 0.6335 & 0.6519 & 0.5448 & 0.7184 & \textbf{0.7823}\\
NJUD test set & 0.6096 & 0.6133 & 0.6156 & 0.6791 & 0.6381 & 0.6952 &0.7246 & \textbf{0.7874} \\
LFSD dataset & 0.6982 & 	0.7311&	0.7029 & 0.7384&	0.7041&	0.7567&	0.7877&	\textbf{0.8439}\\
\hline
\end{tabular}
\end{table*}
\begin{figure*}
\vspace{-3mm}
\centering
\includegraphics[width=0.31\linewidth]{NLPR.pdf}
\includegraphics[width=0.31\linewidth]{NJUD.pdf}
\includegraphics[width=0.31\linewidth]{LFSD.pdf}
\includegraphics[width=0.31\linewidth]{NLPR_column.pdf}
\includegraphics[width=0.31\linewidth]{NJUD_column.pdf}
\includegraphics[width=0.31\linewidth]{LFSD_column.pdf}
\caption{PR curves and F-measure curves of different methods on three datasets. Left: quantitative comparisons on the 250 test images of NLPR dataset \cite{peng2014rgbd}. Middle: quantitative comparisons on the 1000 test images of NJUDS2000 dataset \cite{ju2014depth}. Right: quantitative comparisons on the LFSD dataset \cite{Li_2014_CVPR}.}
\label{fig:saliency_qut}
\end{figure*}

\subsection{Performance Comparison}
In this section, we compare our method with four state-of-the-art methods designed for RGB image (S-CNN \cite{he2015supercnn},  BSCA \cite{qin2015saliency}, MB+ \cite{zhang2015MBD}, and LEGS \cite{wang2015deep}), and three RGBD saliency methods designed specially for RGBD image (LMH \cite{peng2014rgbd}, ACSD \cite{ju2014depth}, and GP \cite{ren2015exploiting}).

The results of these different methods are either provided by authors or achieved using the publicly available source codes. The qualitative comparisons of different methods on different scenes are shown in Figure \ref{fig:saliency2}. As can be seen in the first and fifth rows of Figure \ref{fig:saliency2}, the salient object has a high color contrast with the background, as thus RGB saliency methods are able to detect salient object correctly. However, when the salient object shares similar color with the background, e.g., sixth, seventh, and eighth rows in Figure \ref{fig:saliency2}, it is difficult for existing RGB models to extract saliency. With the help of depth information, salient object can be easily detected by the proposed RGBD method. Figure \ref{fig:saliency2} also shows that the proposed method consistently outperforms all the other RGBD saliency methods (LMH \cite{peng2014rgbd}, ACSD \cite{ju2014depth}, and GP \cite{ren2015exploiting}).

The quantitative comparisons on NLPR, NJUDS2000, and LFSD dataset are shown in Figure \ref{fig:saliency_qut} and Table \ref{table:belta}.
Figure \ref{fig:saliency_qut} and Table \ref{table:belta} show that the proposed method performs favorably against the existing algorithms with higher
precision, recall values and F-measure scores on all the three datasets. For the NLPR dataset, it is challenging as most of the salient object share similar color to the background. As a consequence, RGB saliency methods perform relative worse than RGBD saliency methods in terms of precision. By providing accurate depth map (NLPR dataset), LMH \cite{peng2014rgbd} and GP \cite{ren2015exploiting} methods perform well in both precision and recall. However, they performs not well when tested on the NJUDS2000 dataset and LFSD dataset. This is because these two datasets provide only the rough depth information (calculated from stereo images or using Light field camera), LMH \cite{peng2014rgbd} and GP \cite{ren2015exploiting} can only detect a small fraction of the salient objects (high precision but with low recall). ACSD \cite{ju2014depth} works worse when the salient object lies in the same plane with the background, e.g., the third row in the Figure \ref{fig:saliency2}, and the bad quantitative results on the NLPR dataset. Both qualitative and quantitative results show that the proposed method performs better in terms of accuracy and robustness than the compared methods with RGBD input images.

\begin{table*}
\centering
\caption{The comparisons of F-measure scores for different saliency map merging approaches with or without LP on NLPR test dataset \cite{peng2014rgbd}.}
\label{table:anlysis_NLPR}
\begin{tabular}{|c|c|c|c|cIc|c|c|cIc|cIc|}
\hline
\multirow{2}{*}{LP} & \multicolumn{4}{cI}{\bf{Fundamental fusion}} & \multicolumn{4}{cI}{\bf{Sophisticated fusion}} & \multicolumn{2}{cI}{{Heuristic fusion}}& \multirow{2}{*}{Ours}\\
\cline{2-11}
& LF & CRF & MCA & CNN-F & LF & CRF & MCA & CNN-F &LMH &GP& \\
\hline\hline
{No} &  0.393 & 0.2991 & 0.3713 & 0.4667 & 0.7020 & 0.698 & 0.7017 & 0.6921 &0.6519 & \bf{0.718} &{\color{blue}{0.7315}} \\
{Yes} & \bf{0.536} & \bf{0.398} & \bf{0.486} & \bf{0.597} & \bf{0.711} & \bf{0.739} &\bf{0.7623} &\bf{0.737} &\bf{0.665}&0.7111&{\color{red}\bf{0.7823}}\\
\hline
\end{tabular}
\end{table*}

\begin{table*}
\centering
\caption{The comparisons of F-measure scores for different saliency map merging approaches with or without Laplacian propagation (LP) on NJUD test dataset \cite{ju2014depth}.}
\label{table:anlysis}
\begin{tabular}{|c|c|c|c|cIc|c|c|cIc|cIc|}
\hline
\multirow{2}{*}{LP} & \multicolumn{4}{cI}{\bf{Fundamental fusion}} & \multicolumn{4}{cI}{\bf{Sophisticated fusion}} & \multicolumn{2}{cI}{{Heuristic fusion}}& \multirow{2}{*}{Ours}\\
\cline{2-11}
& LF & CRF & MCA & CNN-F & LF & CRF & MCA & CNN-F &LMH &GP& \\
\hline\hline
{No} &  0.437 & 0.450 & 0.458 & 0.644 & 0.675 & 0.671 &0.7376 & 0.7319& {0.6381} &\bf{0.7246} &{\color{blue}{0.7447}} \\
{Yes} & \bf{0.605} & \bf{0.609} & \bf{0.632} & \bf{0.731} & \bf{0.698} & \bf{0.741} &\bf{0.742} &\bf{0.7423} &\bf{0.6810}&0.7179&{\color{red}\bf{0.7874}}\\
\hline
\end{tabular}
\end{table*}

\begin{table*}
\centering
\caption{The comparisons of F-measure scores for different saliency map merging approaches with or without LP on LFSD dataset \cite{Li_2014_CVPR}.}
\label{table:anlysis_LFSD}
\begin{tabular}{|c|c|c|c|cIc|c|c|cIc|cIc|}
\hline
\multirow{2}{*}{LP} & \multicolumn{4}{cI}{\bf{Fundamental fusion}} & \multicolumn{4}{cI}{\bf{Sophisticated fusion}} & \multicolumn{2}{cI}{{Heuristic fusion}}& \multirow{2}{*}{Ours}\\
\cline{2-11}
& LF & CRF & MCA & CNN-F & LF & CRF & MCA & CNN-F &LMH &GP& \\
\hline\hline
{No} &  0.461 & 0.436 & 0.558 & 0.672 & 0.723 & 0.771 & 0.8071 & 0.706 &0.704 & \bf{0.7877} &{\color{blue}{0.8157}} \\
{Yes} & \bf{0.616} & \bf{0.693} & \bf{0.654} & \bf{0.757} & \bf{0.762} & \bf{0.792} & \bf{0.802} &\bf{0.800} &\bf{0.718} &{0.7830}&{\color{red}\bf{0.8439}}\\
\hline
\end{tabular}
\end{table*}

\noindent{\textbf{Saliency maps vs. features.}}
In here we conduct a series of experiments to analyze the flexibility of the proposed framework and the effectiveness of Laplacian propagation.


Apart from previous heuristic saliency map merging algorithm \cite{peng2014rgbd,ren2015exploiting}, we further compare our method with four other
saliency map integration methods on three test dataset \cite{peng2014rgbd,ju2014depth,Li_2014_CVPR} to show the flexibility of fusing different cues in feature level.  These four integration methods are directly linear fusion (LF), fusing in CRF \cite{liu2011learning}, the latest Multi-layer Cellular Automata (MCA) integration \cite{qin2015saliency}, and a CNN based fusion (denoted as CNN-F).
To investigate the importance of saliency map quality, we test these saliency map merging methods on two set of inputs. The first set is from seven saliency maps computed by widely used features (similar to those seven saliency feature vectors computed in section \ref{title_3_1}), and the second set is from more representative sophisticated saliency maps (obtained using three state-of-the-art RGBD saliency detection methods \cite{peng2014rgbd,ju2014depth,ren2015exploiting}).

The original CRF fusion framework in \cite{liu2011learning} is utilized for merging three color based saliency maps. In our implementation, we retrain this CRF framework for merging the seven adopted fundamental saliency maps and three sophisticated saliency maps respectively \footnote{ We adopt the implementation in \url{http://www.cs.unc.edu/~vicente/code.html} for training and testing. This CRF is trained on the NLPR training dataset \cite{peng2014rgbd} and the NJUDS2000 training dataset \cite{ju2014depth}}.

For CNN-F, we utilize the same CNN architecture as shown in Fig. \ref{fig:saliency_cue} to perform the CNN based saliency map fusion, i.e., the same convolutional layers and fully connected layers except the input layer.
More specifically, we formulate the saliency map merging as a binary logistic regression problem, which takes several saliency map patches as input (size  for fundamental saliency map merging and  for sophisticated saliency map merging), and output the probabilities of the pixel being salient and non-salient. CNN-F is trained in patch-wise manner. We collect training samples by cropping patches of size  from each saliency map using sliding window. We label a patch as salient if the central pixel is salient or 75\% pixels in this patch are salient, otherwise it is labeled as non-salient. This CNN-F is trained on the cropped patches of the NLPR training set \cite{peng2014rgbd} and NJUDS2000 training set \cite{ju2014depth}.


The relevant comparison results of our proposed methods with these saliency map merging methods are shown in Figure \ref{fig:sal_map},
Table \ref{table:anlysis_NLPR}, Table \ref{table:anlysis}, and Table \ref{table:anlysis_LFSD}.  ``Fundamental fusion'' represents the results of four merging methods performed on seven fundamental saliency maps. ``Heuristic fusion'' gives the results of two state-of-the-art heuristic saliency map merging methods \cite{peng2014rgbd,ren2015exploiting}, while ``Sophisticated fusion'' gives the results of four merging methods performed on three sophisticated saliency maps (calculated from the existing state-of-the-art RGBD saliency detection methods LMH
\cite{peng2014rgbd}, ACSD \cite{ju2014depth}, and GP \cite{ren2015exploiting}).

For ``Fundamental fusion'' in Table \ref{table:anlysis_LFSD}, all the existing saliency map merging methods (including deep learning framework) cannot achieve satisfactory  performance.
Even though feeding with the state-of-the-art sophisticated saliency maps, these saliency merging methods still perform worse than our saliency feature fusion without LP
framework (0.8071 vs 0.8157), which further validates the flexibility of our feature level fusion. Note that 0.8157 are obtained from our initial saliency feature fusion network, which performs only on the pixel level and without considering spatial consistency. Our model achieves superior performance even though the input features are very simple (similar to the features used in ``Fundamental fusion''). Compared to those methods using similar features (in ``Fundamental fusion''), we can observe that fusing features is much more flexible than fusing saliency map.

\begin{figure*}
\centering
\captionsetup[subfigure]{labelformat=empty}

\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_GT.png}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_MCA.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_CNN_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_CNN_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve]{000252_left_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_GT.png}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_MCA.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_CNN_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_CNN_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.091\linewidth]{001078_left_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_MCA.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_CNN_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_CNN_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.095\linewidth]{1_03-33-28_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_MCA.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_CNN_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_CNN_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.09\linewidth]{8_11-11-20_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_MCA.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_CNN_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_CNN_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthtwelve,height=0.08\linewidth]{10_01-02-50_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat[{RGB}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_o.jpg}} \hspacefigure
\subfloat[{Depth}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_depth.jpg}}\hspacefigure
\subfloat[{GT}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_GT.jpg}}\hspacefigure
\subfloat[{LMH}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_LMH.jpg}} \hspacefigure
\subfloat[{ACSD}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_ACSD.jpg}} \hspacefigure
\subfloat[{GP}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_GP.jpg}} \hspacefigure
\subfloat[{MCA}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_MCA.jpg}} \hspacefigure
\subfloat[{CNN\_F Init}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_CNN_init.jpg}} \hspacefigure
\subfloat[{CNN\_F+LP}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_CNN_LP.jpg}} \hspacefigure
\subfloat[{{Our Init}}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_init.jpg}} \hspacefigure
\subfloat[{{Ours+LP}}]{ \includegraphics[width=\widthtwelve]{10_03-30-53_DF.jpg}} \hspacefigure \\
\caption{ More examples to show the problem of saliency map merging methods. MCA and CNN\_F are the results of sophisticated fusion (fusing LMH \cite{peng2014rgbd}, ACSD \cite{ju2014depth}, and GP \cite{ren2015exploiting}). ``CNN\_F init'' and ``ours init'' are the initial results of CNN\_F and proposed hyper-feature without Lapalacian propagation respectively.}
\label{fig:sal_map}
\end{figure*}

\noindent{\textbf{Analysis of Laplacian Propagation.}}
We then evaluate the effective of the proposed Laplacian propagation, and the optimized results of the existing methods using Laplacian propagation. The F-measure scores of our RGBD method without Laplacian propagation on three test dataset \cite{peng2014rgbd,ju2014depth,Li_2014_CVPR} are shown in blue in Table \ref{table:anlysis_NLPR}, Table \ref{table:anlysis}, and Table \ref{table:anlysis_LFSD}. These learned hyper-features still outperform the state-of-the-art approaches, while with LP we achieve almost 0.79, 0.79, and 0.84 F-measures. Figure \ref{fig:sal_LP} shows some examples of the optimized results of the existing methods (LMH \cite{peng2014rgbd}, ACSD \cite{ju2014depth}, and GP \cite{ren2015exploiting}) using Laplacian propagation.
These quantitative and qualitative experimental
evaluations further demonstrate that the proposed Laplacian propagation is able to refine the saliency maps of existing methods, which can be widely adopted as a post processing step.

\begin{figure*}
\centering
\captionsetup[subfigure]{labelformat=empty}
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_LMH_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_ACSD_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_GP_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{8_08-42-48_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_LMH_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_ACSD_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_GP_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{9_07-43-30_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_LMH_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_ACSD_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_GP_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{11_04-07-01_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthNne]{000168_left_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_GT.png}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_LMH_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_ACSD_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_GP_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000168_left_DF.jpg}} \hspacefigure\\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthNne]{000706_left_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_GT.png}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_LMH_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_ACSD_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_GP_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne]{000706_left_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_o.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_depth.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_GT.jpg}}\hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_LMH.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_LMH_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_ACSD.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_ACSD_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_GP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_GP_LP.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_init.jpg}} \hspacefigure
\subfloat{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{11_03-48-39_DF.jpg}} \hspacefigure \\
\vspace{-1.5mm}
\subfloat[RGB]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_o.jpg}} \hspacefigure
\subfloat[Depth]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_depth.jpg}}\hspacefigure
\subfloat[GT]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_GT.jpg}}\hspacefigure
\subfloat[LMH]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_LMH.jpg}} \hspacefigure
\subfloat[LMH+LP]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_LMH_LP.jpg}} \hspacefigure
\subfloat[ACSD]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_ACSD.jpg}} \hspacefigure
\subfloat[ACSD+LP]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_ACSD_LP.jpg}} \hspacefigure
\subfloat[GP]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_GP.jpg}} \hspacefigure
\subfloat[GP+LP]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_GP_LP.jpg}} \hspacefigure
\subfloat[Our init]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_init.jpg}} \hspacefigure
\subfloat[Ours+LP]{ \includegraphics[width=\widthNne,height=0.0859\linewidth]{10_03-27-04_DF.jpg}} \hspacefigure\\
\caption{ Examples to show the effectiveness of Laplacian propagation.}
\label{fig:sal_LP}
\end{figure*}

\noindent{\textbf{Failure cases.}} Figure \ref{fig:saliencyRGBvsRGBD} gives more visual results and some failure cases of our proposed method on RGBD images.
Compared with the these two pictures, we can find that depth information is more helpful when the salient objects have high depth contrast with background or lie closer to the camera.
Our method may fail when the salient object shares a very similar color and depth information with the background.
\begin{figure}
\centering
\captionsetup[subfigure]{labelformat=empty}
\subfloat{ \label{fig:saliencyRGBvsRGBD:a}\includegraphics[width=\widtheighttwo,height=0.189\linewidth]{000905_left.pdf}} \hspacefigure
\subfloat{ \label{fig:saliencyRGBvsRGBD:b}\includegraphics[width=\widtheighttwo,height=0.189\linewidth]{000905_left_Depth.pdf}} \hspacefigure
\subfloat{ \label{fig:saliencyRGBvsRGBD:c}\includegraphics[width=\widtheighttwo,height=0.189\linewidth]{000905_left.pdf}} \hspacefigure
\subfloat{ \label{fig:saliencyRGBvsRGBD:d}\includegraphics[width=\widtheighttwo,height=0.189\linewidth]{000905_left_MR_refine24.pdf}}\hspacefigure\\
\vspace{-1.5mm}
\subfloat[RGB]{ \label{fig:saliencyRGBvsRGBD:a}\includegraphics[width=\widtheighttwo,height=0.1859\linewidth]{000880_left.pdf}}  \hspace{-1mm}\subfloat[Depth]{ \label{fig:saliencyRGBvsRGBD:b}\includegraphics[width=\widtheighttwo,height=0.1859\linewidth]{000880_left_Depth.pdf}}\hspacefigure
\subfloat[GT]{ \label{fig:saliencyRGBvsRGBD:c}\includegraphics[width=\widtheighttwo,height=0.1859\linewidth]{000880_left_GT.pdf}}\hspacefigure
\subfloat[Our result]{ \label{fig:saliencyRGBvsRGBD:d}\includegraphics[width=\widtheighttwo,height=0.1859\linewidth]{000880_left_MR_refine24.pdf}}\hspacefigure
   \caption{More visual results and some failure cases.}
    \label{fig:saliencyRGBvsRGBD} \end{figure}
\section{Conclusion}
In this paper, we propose a novel RGBD saliency detection method. Our framework consists of three different modules. The first module generates various low level saliency feature vectors
from the input image. The second module learns the interaction mechanism of RGB saliency features and depth-induced features and produces hyper-feature using CNN. Feeding with these hand-designed features can guide the learning process of CNN towards saliency-optimized. In the third module, we integrate a Laplacian propagation framework with CNN to obtain a spatially consistent saliency map. Both quantitative and qualitative experiment results show that the fused RGBD hyper-feature outperforms all the state-of-the-art methods.

We demonstrated that an optimized fusion leads to superior performance, and this flexible hyper-feature extraction framework can be further extended by including more saliency cues (e.g., flash cue \cite{shenfeng2014}). We aim to explore a deeper and more effective fusion network and extend it to other applications in our future work.
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{rutishauser2004bottom}
U.~Rutishauser, D.~Walther, C.~Koch, and P.~Perona, ``Is bottom-up attention
  useful for object recognition?'' in \emph{CVPR}, vol.~2, 2004, pp. II--37.

\bibitem{mahadevan2009saliency}
V.~Mahadevan and N.~Vasconcelos, ``Saliency-based discriminant tracking,'' in
  \emph{CVPR}, 2009, pp. 1007--1013.

\bibitem{sharma2012discriminative}
G.~Sharma, F.~Jurie, and C.~Schmid, ``Discriminative spatial saliency for image
  classification,'' in \emph{CVPR}, 2012, pp. 3506--3513.

\bibitem{luo2014switchable}
P.~Luo, Y.~Tian, X.~Wang, and X.~Tang, ``Switchable deep network for pedestrian
  detection,'' in \emph{CVPR}, 2014, pp. 899--906.

\bibitem{zhang2012microsoft}
Z.~Zhang, ``Microsoft kinect sensor and its effect,'' \emph{IEEE MultiMedia},
  vol.~19, no.~2, pp. 4--10, 2012.

\bibitem{gokturk2004time}
S.~B. Gokturk, H.~Yalcin, and C.~Bamji, ``A time-of-flight depth sensor-system
  description, issues and solutions,'' in \emph{CVPR}, 2004, pp. 35--35.

\bibitem{mishra2012segmenting}
A.~K. Mishra, A.~Shrivastava, and Y.~Aloimonos, ``Segmenting imple?
  objects using rgb-d,'' in \emph{ICRA}, 2012, pp. 4406--4413.

\bibitem{fu2015object}
H.~Fu, D.~Xu, S.~Lin, and J.~Liu, ``Object-based rgbd image co-segmentation
  with mutex constraint,'' in \emph{CVPR}, 2015, pp. 4428--4436.

\bibitem{banica2015second}
D.~Banica and C.~Sminchisescu, ``Second-order constrained parametric proposals
  and sequential search-based structured prediction for semantic segmentation
  in rgb-d images,'' in \emph{CVPR}, 2015, pp. 3517--3526.

\bibitem{peng2014rgbd}
H.~Peng, B.~Li, W.~Xiong, W.~Hu, and R.~Ji, ``Rgbd salient object detection: a
  benchmark and algorithms,'' in \emph{ECCV}, 2014, pp. 92--109.

\bibitem{ju2014depth}
R.~Ju, L.~Ge, W.~Geng, T.~Ren, and G.~Wu, ``Depth saliency based on anisotropic
  center-surround difference,'' in \emph{ICIP}, 2014, pp. 1115--1119.

\bibitem{ren2015exploiting}
J.~Ren, X.~Gong, L.~Yu, W.~Zhou, and M.~Yang, ``Exploiting global priors for
  rgb-d saliency detection,'' in \emph{CVPRW}, 2015, pp. 25--32.

\bibitem{qin2015saliency}
Y.~Qin, H.~Lu, Y.~Xu, and H.~Wang, ``Saliency detection via cellular
  automata,'' in \emph{CVPR}, 2015, pp. 110--119.

\bibitem{itti1998model}
L.~Itti, C.~Koch, and E.~Niebur, ``A model of saliency-based visual attention
  for rapid scene analysis,'' \emph{IEEE TPAMI}, vol.~20, no.~11, pp.
  1254--1259, 1998.

\bibitem{ma2003contrast}
Y.-F. Ma and H.-J. Zhang, ``Contrast-based image attention analysis by using
  fuzzy growing,'' in \emph{ACM Multimedia}, 2003, pp. 374--381.

\bibitem{wei2012geodesic}
Y.~Wei, F.~Wen, W.~Zhu, and J.~Sun, ``Geodesic saliency using background
  priors,'' in \emph{ECCV}, 2012, pp. 29--42.

\bibitem{keyang2013}
K.~Shi, K.~Wang, J.~Lu, and L.~Lin, ``Pisa: Pixelwise image saliency by
  aggregating complementary appearance contrast measures with spatial priors,''
  in \emph{CVPR}, June 2013, pp. 2115--2122.

\bibitem{maki1996computational}
A.~Maki, P.~Nordlund, and J.-O. Eklundh, ``A computational model of depth-based
  attention,'' in \emph{Pattern Recognition}, vol.~4, 1996, pp. 734--739.

\bibitem{lang2012depth}
C.~Lang, T.~V. Nguyen, H.~Katti, K.~Yadati, M.~Kankanhalli, and S.~Yan, ``Depth
  matters: Influence of depth cues on visual saliency,'' in \emph{ECCV}, 2012,
  pp. 101--115.

\bibitem{desingh2013depth}
K.~Desingh, K.~M. Krishna, D.~Rajan, and C.~Jawahar, ``Depth really matters:
  Improving visual salient region detection with depth,'' in \emph{BMVC}, 2013.

\bibitem{zhang2010stereoscopic}
Y.~Zhang, G.~Jiang, M.~Yu, and K.~Chen, ``Stereoscopic visual attention model
  for 3d video,'' in \emph{Advances in Multimedia Modeling}, 2010, pp.
  314--324.

\bibitem{liu2011learning}
T.~Liu, Z.~Yuan, J.~Sun, J.~Wang, N.~Zheng, X.~Tang, and H.-Y. Shum, ``Learning
  to detect a salient object,'' \emph{IEEE TPAMI}, vol.~33, no.~2, pp.
  353--367, 2011.

\bibitem{yan2013hierarchical}
Q.~Yan, L.~Xu, J.~Shi, and J.~Jia, ``Hierarchical saliency detection,'' in
  \emph{CVPR}, 2013, pp. 1155--1162.

\bibitem{zhou2015salient}
L.~Zhou, Z.~Yang, Q.~Yuan, Z.~Zhou, and D.~Hu, ``Salient region detection via
  integrating diffusion-based compactness and local contrast,'' \emph{IEEE
  TIP}, vol.~24, no.~11, pp. 3308--3320, 2015.

\bibitem{zhou2004learning}
D.~Zhou, O.~Bousquet, T.~N. Lal, J.~Weston, and B.~Sch{\"o}lkopf, ``Learning
  with local and global consistency,'' \emph{NIPS}, vol.~16, no.~16, pp.
  321--328, 2004.

\bibitem{borji2014salient}
A.~Borji, M.-M. Cheng, H.~Jiang, and J.~Li, ``Salient object detection: A
  survey,'' \emph{arXiv preprint arXiv:1411.5878}, 2014.

\bibitem{bruce2005saliency}
N.~Bruce and J.~Tsotsos, ``Saliency based on information maximization,'' in
  \emph{NIPS}, 2005, pp. 155--162.

\bibitem{shenfeng2014}
S.~He and R.~W.~H. Lau, ``Saliency detection with flash and no-flash image
  pairs,'' in \emph{ECCV}, 2014, pp. 110--124.

\bibitem{achanta2009frequency}
R.~Achanta, S.~Hemami, F.~Estrada, and S.~Susstrunk, ``Frequency-tuned salient
  region detection,'' in \emph{CVPR}, 2009, pp. 1597--1604.

\bibitem{ChengPAMI}
M.-M. Cheng, N.~J. Mitra, X.~Huang, P.~H.~S. Torr, and S.-M. Hu, ``Global
  contrast based salient region detection,'' \emph{IEEE TPAMI}, 2014.

\bibitem{shen2012unified}
X.~Shen and Y.~Wu, ``A unified approach to salient object detection via low
  rank matrix recovery,'' in \emph{CVPR}, 2012, pp. 853--860.

\bibitem{jiang2013salient}
H.~Jiang, J.~Wang, Z.~Yuan, Y.~Wu, N.~Zheng, and S.~Li, ``Salient object
  detection: A discriminative regional feature integration approach,'' in
  \emph{CVPR}, 2013, pp. 2083--2090.

\bibitem{vig2014large}
E.~Vig, M.~Dorr, and D.~Cox, ``Large-scale optimization of hierarchical
  features for saliency prediction in natural images,'' in \emph{CVPR}, 2014,
  pp. 2798--2805.

\bibitem{zhao2015saliency}
R.~Zhao, W.~Ouyang, H.~Li, and X.~Wang, ``Saliency detection by multi-context
  deep learning,'' in \emph{CVPR}, 2015, pp. 1265--1274.

\bibitem{li2015visual}
G.~Li and Y.~Yu, ``Visual saliency based on multiscale deep features,'' in
  \emph{CVPR}, 2015.

\bibitem{he2015supercnn}
S.~He, R.~W. Lau, W.~Liu, Z.~Huang, and Q.~Yang, ``Supercnn: A superpixelwise
  convolutional neural network for salient object detection,'' \emph{IJCV}, pp.
  1--15, 2015.

\bibitem{wang2015deep}
L.~Wang, H.~Lu, X.~Ruan, and M.-H. Yang, ``Deep networks for saliency detection
  via local estimation and global search,'' in \emph{CVPR}, 2015, pp.
  3183--3192.

\bibitem{LiYu16}
G.~Li and Y.~Yu, ``Deep contrast learning for salient object detection,'' in
  \emph{CVPR}, June 2016.

\bibitem{wang2013computational}
J.~Wang, M.~P. DaSilva, P.~LeCallet, and V.~Ricordel, ``Computational model of
  stereoscopic 3d visual saliency,'' \emph{IEEE TIP}, vol.~22, no.~6, pp.
  2151--2165, 2013.

\bibitem{achanta2012slic}
R.~Achanta, A.~Shaji, K.~Smith, A.~Lucchi, P.~Fua, and S.~Susstrunk, ``Slic
  superpixels compared to state-of-the-art superpixel methods,'' \emph{IEEE
  TPAMI}, vol.~34, no.~11, pp. 2274--2282, 2012.

\bibitem{otsu1975threshold}
N.~Otsu, ``A threshold selection method from gray-level histograms,''
  \emph{Automatica}, vol.~11, no. 285-296, pp. 23--27, 1975.

\bibitem{zhang2015MBD}
J.~Zhang, S.~Sclaroff, Z.~Lin, X.~Shen, B.~Price, and R.~M\u{e}ch, ``Minimum
  barrier salient object detection at 80 fps,'' in \emph{ICCV}, 2015.

\bibitem{Li_2014_CVPR}
N.~Li, J.~Ye, Y.~Ji, H.~Ling, and J.~Yu, ``Saliency detection on light field,''
  in \emph{CVPR}, 2014.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{NIPS}, 2012, pp. 1097--1105.

\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{qu.pdf}}]{Liangqiong Qu}
received the B.S. degree in automation from Central South University, China, in 2011. She is currently a joint Ph.D. student
of University of Chinese Academy of Sciences and City University of Hong Kong. Her research interests include illumination modeling, image processing, saliency detection and deep learning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{he.png}}]{Shengfeng He} obtained his B.Sc. degree and M.Sc. degree from Macau University of Science and Technology, and the Ph.D degree from City University of Hong Kong. He is currently a Research Fellow at City University of Hong Kong. His research interests include computer vision, image processing, computer graphics, and deep learning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{zjw.png}}]{Jiawei Zhang} received his BEng degree in Electronic Information Engineering from the University of
Science and Technology of China in 2011 and master degree in Institute of Acoustics, Chinese Academy of Sciences in 2014. He is currently a Computer Science PhD student in City university of Hong Kong.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{tian.jpg}}]{Jiandong Tian}
received his B.S. Tech. degree in automation at Heilongjiang University, China, in 2005.
He received his Ph.D. degree in Pattern Recognition and Intelligent System at Chinese Academy of Sciences, China, in 2011.
He is currently an asassociate professor in computer vision at State Key Laboratory of Robotic, Shenyang Institute of Automation,
Chinese Academy of Sciences. His research interests include pattern recognition and robot vision.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{tang.pdf}}]{Yandong Tang}
received B.S. and M.S. degrees in the department of mathematics, Shandong University in 1984 and 1987.
In 2002 he received the doctor's degree in applied mathematics from the University of Bremen, Germany. Currently he is a professor in Shenyang
Institute of Automation, Chinese Academy of Sciences. His research interests include robot vision, pattern recognition and numerical computation.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{qx.png}}]{Qingxiong Yang}
received the B.E. degree in
electronic engineering and information science from
the University of Science and Technology of China,
Hefei, China, in 2004, and the Ph.D. degree in electrical and computer engineering from the University
of Illinois at Urbana-Champaign, Champaign, IL,
USA, in 2010. He is currently an Assistant Professor
with the Department of Computer Science, City
University of Hong Kong, Hong Kong. His research
interests reside in computer vision and computer
graphics. He was a recipient of the Best Student
Paper Award at the 2010 International Workshop on Multimedia Signal
Processing and the Best Demo Award at the 2007 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition.
\end{IEEEbiography}


\end{document}
