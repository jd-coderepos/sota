\section{Experiments}
\label{sec:experiments}

In this section, we seek to investigate the following two questions: (1) Can PEMIRL learn a policy with competitive few-shot generalization abilities compared to one-shot imitation learning methods using only unstructured demonstrations? (2) Can PEMIRL efficiently infer robust reward functions of new continuous control tasks where one-shot imitation learning fails to generalize, enabling an agent to continue to improve with more trials?

We evaluate our method on four simulated domains using the Mujoco physics engine~\cite{mujoco}. To our knowledge, there's no prior work on designing meta-IRL or one-shot imitation learning methods for complex domains with high-dimensional continuous state-action spaces with unstructured demonstrations. Hence, we also designed the following variants of existing state-of-the-art (one-shot) imitation learning and IRL methods so that they can be used as fair comparisons to our method:
\begin{itemize}[leftmargin=.3in]
\item \textbf{AIRL}: The original AIRL algorithm without incorporating latent context variables, trained across all demonstrations.
\item \textbf{Meta-Imitation Learning with Latent Context Variables (Meta-IL)}: As in~\citep{pearl}, we use the inference model  to infer the context of a new task from a single demonstrated trajectory, denoted as , and then train the conditional imitaiton policy  using the same demonstration. This approach also resembles~\cite{duan2017}.
\item \textbf{Meta-InfoGAIL}: Similar to the method above, except that an additional discriminator  is introduced to distinguish between expert and sample trajectories, and trained along with the conditional policy using InfoGAIL~\cite{li2017infogail} objective.
\end{itemize}
We use trust region policy optimization (TRPO)~\cite{trpo} as our policy optimization algorithm across all methods. We collect demonstrations by training experts with TRPO using ground truth reward. However, the ground truth reward is not available to imitation learning and IRL algorithms. We provide full hyperparameters, architecture information, data efficiency, and experimental setup details in Appendix~\ref{appendix:experiment}. We also include ablation studies on sensitivity of the latent dimensions, importance of the mutual information objective and the performance on stochastic environments in Appendix~\ref{app:ablation}. Full video results are on the anonymous supplementary website\footnote{Video results can be found at: \url{https://sites.google.com/view/pemirl}} and our code is open-sourced on GitHub\footnote{Our implementation of PEMIRL can be found at: \url{https://github.com/ermongroup/MetaIRL}}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/setup.png}
\caption{\textbf{Experimental domains} (left to right):
Point-Maze, Ant, Sweeper, and Sawyer Pusher. 
    }
\label{fig:setup}
\end{figure*}

\subsection{Policy Performance on Test Tasks}
\label{sec:imitation_results}

\begin{table}[h]
    \begin{center}
\begin{small}
    \begin{tabular}{l|c|c|c|c}
    \toprule
        & Point Maze & Ant & Sweeper & Sawyer Pusher  \\
      \midrule
      Expert &  &  &  & \\
Random &  &  &  & \\
      \midrule
      AIRL~\cite{fu2017learning} &  &  &  & \\
Meta-IL &  &  &  & \\
Meta-InfoGAIL &  &  &  & \\
PEMIRL (ours) &  &  &  &  \\
      \bottomrule
    \end{tabular}
    \end{small}
    \end{center}
    \vspace{0.2cm}
    \caption{One-shot policy generalization to test tasks on four experimental domains.
    Average return and standard deviations are reported over  runs.}
\label{tbl:imitation_results}
\end{table}

We first answer our first question by showing that our method is able to learn a policy that can adapt to test tasks from a single demonstration, on four continuous control domains: \textbf{Point Maze Navigation}: In this domain, a pointmass needs to navigate around a barrier to reach the goal. Different tasks correspond to different goal positions and the reward function measures the distance between the pointmass and the goal position; \textbf{Ant}: Similar to~\cite{finn2017maml}, this locomotion task requires fast adaptation to walking directions of the ant where the ant needs to learn to move backward or forward depending on the demonstration;
\textbf{Sweeper}: A robot arm needs to sweep an object to a particular goal position. Fast adaptation of this domain corresponds to different goal locations in the plane;
\textbf{Sawyer Pusher}: A simulated Sawyer robot is required to push a mug to a variety of goal positions and generalize to unseen goals.
We illustrate the set-up for these experimental domains in Figure~\ref{fig:setup}. 

We summarize the results in Table~\ref{tbl:imitation_results}. PEMIRL achieves comparable imitation performance compared to Meta-IL and Meta-InfoGAIL, while AIRL is incapable of handling multi-task scenarios without incorporating the latent context variables.

\subsection{Reward Adaptation to Challenging Situations}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.2088\columnwidth]{figures/maze_reward_1.png}
    \includegraphics[width=0.2088\columnwidth]{figures/maze_reward_2.png}
    \includegraphics[width=0.2088\columnwidth]{figures/maze_reward_3.png}
    \includegraphics[width=0.252\columnwidth]{figures/maze_reward_4.png}
\caption{Visualizations of learned reward functions for point-maze navigation. The red star represents the target position and the white circle represents the initial position of the agent (both are different across different iterations). The black horizontal line represents the barrier that cannot be crossed. To show the generalization ability, the expert demonstration used to infer the target position are sampled from new target positions that have not been seen in the meta-training set.
    }
\label{fig:maze_reward_fig}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/ant_film_strips.png}
\caption{From top to bottom, we show the disabled ant running forward and backward respectively.
    }
\label{fig:ant_film_strips}
\end{figure*}

After demonstrating that the policy learned by our method is able to achieve competitive ``one-shot'' generalization ability, we now answer the second question by showing PEMIRL learns a robust reward that can adapt to new and more challenging settings where the imitation learning methods and the original AIRL fail. Specifically, after providing the demonstration of an unseen task to the agent, we change the underlying environment dynamics but keep the same task goal. In order to succeed in the task with new dynamics, the agent must correctly infer the underlying goal of the task instead of simply mimicking the demonstration. We show the effectiveness of our reward generalization by training a new policy with TRPO using the learned reward functions on the new task.

\prg{Point-Maze Navigation with a Shifted Barrier}. Following the setup of ~\citet{fu2017learning}, at meta-test time, after showing a demonstration moving towards a new target position, we change the position of the barrier from left to right. 
As the agent must adapt by reaching the target with a different path from what was demonstrated during meta-training, it cannot succeed without correctly inferring the true goal (the target position in the maze) and learning from trial-and-error. As a result, all direct policy generalization approaches fail as all the policies are still directing the pointmass to the right side of the maze. As shown in Figure~\ref{fig:maze_reward_fig}, PEMIRL learns disentangled reward functions that successfully infer the underlying goal of the new task without much reward shaping. Such reward functions enable the RL agent to bypass the right barrier and reach the true goal position. The RL agent trained with the reward learned by AIRL also fail to bypass the barrier and
navigate to the target position, as without incorporating the latent context variables and treating the demonstration as multi-modal, AIRL learns an ``average'' reward and policy among different tasks. We also use the output of the discriminator of Meta-InfoGAIL as reward signals and evaluate its adaptation performance. The agent trained by this reward fails to complete the task since Meta-InfoGAIL does not explicitly optimize for reward learning and the discriminator output converges to uninformative uniform distribution at convergence.


\begin{table}[t]
    \begin{center}
    \setlength\aboverulesep{1.5pt}\setlength\belowrulesep{1.5pt}
\begin{tabular}{c|c|c|c}
    \toprule
      & Method & Point-Maze-Shift & Disabled-Ant  \\
      \midrule
      \multirow{3}{*}{\shortstack{
      Policy\\ Generalization}} & Meta-IL  &  & \\
& Meta-InfoGAIL &  & \\
& PEMIRL &  &  \\
      \midrule
      \multirow{3}{*}{\shortstack{Reward\\ Adaptation}} & AIRL &  & \\
& Meta-InfoGAIL &  &  \\
      & PEMIRL (ours) &  & \\
    \midrule
      & Expert &  & \\
      \bottomrule
    \end{tabular}
\end{center}
\caption{Results on direct policy generalization and reward adaptation to challenging situations. Policy generalization examines if the policy learned by Meta-IL is able to generalize to new tasks with new dynamics, while reward adaptation tests if the learned RL can lead to efficient RL training in the same setting. The RL agent learned by PEMIRL rewards outperforms other methods in such challenging settings.
    }
\label{tbl:reward_results}
    \vspace{-0.5cm}
\end{table}


\prg{Disabled Ant Walking}. As in ~\citet{fu2017learning}, we disable and shorten two front legs of the ant such that it cannot walk without changing its gait to a large extent. Similar to Point-Maze-Shift, all imitaiton policies fail to maneuver the disabled ant to the right direction. As shown in Figure~\ref{fig:ant_film_strips}, reward functions learned by PEMIRL encourage the RL policy to orient the ant towards the demonstrated direction and move along that direction using two healthy legs, which is only possible when the inferred reward corresponds to the true underlying goal and is disentangled with the dynamics. In contrast, the learned reward of original AIRL as well as the discriminator output of Meta-InfoGAIL cannot infer the underlying goal of the task and provide precise supervision signal, which leads to the unsatisfactory performance of the induced RL policies. Quantitative results are presented in Table~\ref{tbl:reward_results}.















%