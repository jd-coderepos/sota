\vspace{-5pt}
\section{Experiments}\label{sect_exp}
\vspace{-5pt}
\subsection{Experimental Setup}\label{sec_exp_setup}
\vspace{-5pt}
\textbf{Datasets.}
For semi-supervised node classification, we first adopt three well-known citation network datasets: Cora, Citeseer and Pubmed~\citep{sen2008collective}, with the standard train/val/test split setting~\citep{kipf_2017_iclr, yang2016revisiting}.
Experiments on these citation networks are transductive, i.e., all nodes are accessible during training.
For inductive learning, we use a much larger social graph Reddit~\citep{hamilton2017inductive}.
In this graph, testing nodes are unseen during the model learning process.

\begin{figure*}[!ht]
  \begin{minipage}{.65\textwidth}
  \centering
    \scriptsize
    \renewcommand{\arraystretch}{0.9}
    \begin{tabular}{lll|ccc}
    \toprule
    \multicolumn{2}{c}{\textbf{Method}} &\textbf{\#Layer} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} \\
    \midrule
    \multirow{16}{*}{\begin{turn}{90}\textbf{supervised}\end{turn}}
    & LP & $-$ &  71.5  & 48.9 & 65.8 \\
    & ManiReg & $-$ & 59.5 & 60.1  & 70.7 \\
    & GCN &  $2$ & 81.5   & 70.3  & 79.0\\
    & APPNP & $2$ & 83.3  & 71.8 & 80.1 \\
    & Eigen-GCN &  $2$ &78.9$\pm$ 0.7  & 66.5$\pm$0.3 & 78.6$\pm$0.1 \\
    & GNN-LF/HF & $2$  & 84.0$\pm$0.2  & 72.3$\pm$0.3  & 80.5$\pm$0.3 \\
    & C\&S & $3$  & 84.6$\pm$0.5 & 75.0$\pm$0.3 & 81.2$\pm$0.4 \\
    & NDLS      &  $2$ &84.6$\pm$0.5 &73.7$\pm$0.6 &81.4$\pm$0.4 \\
    & ChebNetII & $2$ &  83.7$\pm$0.3 &72.8$\pm$0.2 &80.5$\pm$0.2  \\
    & OAGS & $2$ &  83.9$\pm$0.5 &73.7$\pm$0.7 &81.9$\pm$0.9  \\
    & JKNet & $\{4, 16, 32\}$ &  82.7 $\pm$ 0.4 &  73.0 $\pm$ 0.5 &  77.9 $\pm$ 0.4  \\
    & Incep & $\{64, 4, 4\}$  & 82.8  & 72.3 & 79.5 \\
    & GCNII & $\{64, 32, 16\}$  & 85.5$\pm$0.5  & 73.4 $\pm$ 0.6 & 80.2 $\pm$ 0.4 \\
    & GRAND & $\{8, 2, 5\}$  & 85.4$\pm$0.4 & 75.4$\pm$0.4 & 82.7$\pm$0.6 \\
    & ACMP & $\{8, 4, 32\}$ & 84.9$\pm$0.6 & 75.5 $\pm$ 1.0 & 79.4$\pm$0.4 \\
    & \textbf{\myoptalg} (ours) & $>2$\tnote{*} & \textbf{86.9 $\pm$ 0.0} & \textbf{77.5 $\pm$ 0.0} & \textbf{83.4 $\pm$ 0.0} \\ \midrule
\multirow{13}{*}{\begin{turn}{90}\textbf{unsupervised}\end{turn}}
    & DeepWalk & $-$ & 67.2 & 43.2 & 65.3  \\
    & node2vec & $-$  &71.5 &45.8 &71.3\\
    & VERSE & $-$  & 72.5 $\pm$ 0.3 & 55.5  $\pm$ 0.4  & $-$ \\
    & GAE &  $4$  & 71.5 $\pm$ 0.4 & 65.8  $\pm$ 0.4 & 72.1 $\pm$ 0.5 \\
    & DGI &  $2$  & 82.3 $\pm$ 0.6 & 71.8 $\pm$ 0.7  & 76.8 $\pm$ 0.6 \\
    & DGI Random-Init  & $2$  & 69.3 $\pm$ 1.4 & 61.9 $\pm$ 1.6  & 69.6 $\pm$ 1.9 \\
    & BGRL & $2$  & 81.1$\pm$0.2 &71.6$\pm$0.4 &80.0$\pm$0.4 \\
    & N2N  & $2$  & 83.1$\pm$0.4 &73.1$\pm$0.6 &80.1$\pm$0.7 \\
    & SGC &$2$  & 81.0 $\pm$0.0	&71.9 $\pm$0.1	&78.9 $\pm$0.0 \\
    & S$^2$GC  &$16$    & 83.0 $\pm$0.2	&73.6 $\pm$0.1	&80.2 $\pm$0.2 \\ & G$^2$CN &$10$     &82.7 &73.8 &80.4 \\
    & \textbf{\mygspalg} (ours)   & $>2$\tnote{*}  & 81.8 $\pm$ 0.3 & 73.8 $\pm$ 0.2 & 79.6 $\pm$ 0.1 \\
    & \textbf{\mygspalg M} (ours)  & $>2$\tnote{*} & \textbf{83.6 $\pm$ 0.2} & \textbf{74.2 $\pm$ 0.1} & \textbf{80.8 $\pm$ 0.1} \\ \bottomrule
    \end{tabular}
    \captionof{table}{\footnotesize{Classification accuracy ($\%$). The ``\#Layers'' column highlights the best layer number.}}
    \label{tab_node_classification}
  \end{minipage}
  \quad
  \begin{minipage}{.32\textwidth}
    \centering
    \includegraphics[width=0.78\linewidth]{images/diff_k/cora_k.eps}
    \includegraphics[width=0.78\linewidth]{images/diff_k/citeseer_k.eps}
    \includegraphics[width=0.78\linewidth]{images/diff_k/pubmed_k.eps}
	\captionof{figure}{\footnotesize{Classification performance w.r.t. layer number ($x$-axis: layer number).}}
	\label{fig_acc_diff_k}
  \end{minipage}
\end{figure*}

\textbf{Baselines.}
We compare our methods with some unsupervised methods including the skip-gram based methods (DeepWalk~\citep{perozzi_2014_kdd} and node2vec~\citep{grover2016node2vec}), auto-encoder based methods (GAE~\citep{kipf_2016_arxiv} and  VERSE~\citep{tsitsulin_2018_www}), graph contrastive methods (DGI~\citep{velickovic_2019_iclr}, BGRL~\citep{thakoor2022large}, and N2N~\citep{dong2022node} ) and some GCN methods (SGC~\citep{wu2019simplifying}, S$^2$GC~\citep{zhusimple2021} and G$^2$CN~\citep{li2022g}).
In addition, we further compare some supervised methods which can be mainly categorized into three types.
The first type are shallow cluster-assumption based GSSL methods: label propagation (LP) ~\citep{zhu_2003_icml} and manifold regularization (ManiReg)~\citep{belkin2006manifold}.
The second type are some shallow GCN methods, including GCN~\citep{kipf_2017_iclr}, APPNP~\citep{klicpera2018predict}, Eigen-GCN~\citep{zhang2021eigen}, GNN-LF/HF~\citep{zhu2021interpreting}, C\&S~\citep{huang2020combining}, NDLS~\citep{zhang2021node}, ChebNetII~\citep{he2022convolutional} and OAGS~\citep{song2022towards}.
Finally, we also compare with some deep GCN methods: JKNet~\citep{xu2018representation}, Incep~\citep{rong2019dropedge}, GCNII~\citep{chen2020simple}, GRAND~\citep{feng2020graph} and ACMP~\citep{wang2023acmp}.


\textbf{Implementation Details.}
In our methods, we always set the maximum iteration number as 64.
As OGC is actually a shallow method, it does not need the validation set to avoid overfitting.
Therefore, like LP~\citep{zhu_2003_icml} and C\&S~\citep{huang2020combining}, we use both train and validation sets as the supervised knowledge.
In addition, to further mitigate the risk of overfitting within the learned node embeddings, when updating $U$ in Eq.~\ref{eq_update_X_Qsup_Q_reg_lazy}, we only use the train set to build the label indicator diagonal matrix $S$.
We call this trick ``\emph{\textbf{L}ess \textbf{I}s \textbf{M}ore (\textbf{LIM})}''.
In our two unsupervised methods GGC and GGCM, after obtaining the node embeddings, we train a linear logistic regression classifier for label prediction on the training set, and conduct a grid search to tune all hyper-parameters on the validation set.
More experimental details can be found in Appendix~\ref{app_datasets_imps}.


\vspace{-5pt}
\subsection{Transductive Node Classification}\label{sub_sect_node_clf}
\vspace{-5pt}
Table~\ref{tab_node_classification} summarizes the classification results.
Firstly, we can clearly see that our supervised method OGC consistently outperforms all baselines by a large margin.
This indicates that the ability to include validation labels is an advantage of our approach.
Secondly, our two unsupervised methods GGC and GGCM are also very powerful; especially, GGCM achieves very competitive performance on all three datasets.
It's worth noting that very deep models, such as GCNII and GRAND, typically involve huge amounts of parameters to learn, contain lots of hyper-parameters to tune, and take a long time to train.
Moreover, they all have to retrain the model totally, once the layer number is fixed.
On the contrary, our three methods, which all run in an iterative way with very few parameters, do not have these issues.
Finally, we also highlight that: \mygspalg\ and GGCM greatly outperform Eigen-GCN whose experimental results show that preserving graph structure information in GCN-type models may hurt the performance.
In contrast, our work evidently demonstrates this value.



\textbf{The Effect of Depths/Iterations.}
As shown in Fig.~\ref{fig_acc_diff_k}, in most cases, the performance of our methods increases as the iteration goes on.
Specifically, they all tend to get the best performance at around 16-th or 32-th iteration, and could maintain similar performance as the iteration number increases to 64.
In addition, we give the detailed results of depth/iteration effect experiment in Appendix~\ref{app_sect_exp_depth}, an efficiency test in Appendix~\ref{app_exp_eff_test}, supervised label setting test in Appendix~\ref{app_sect_train_val}, and various setting test of our methods in Appendix~\ref{app_sect_various_set_ours}.


\vspace{-5pt}
\subsection{Inductive Node Classification}
\vspace{-5pt}
For baselines, besides the above-mentioned methods GCN, DGI, SGC and S2GC, we further adopt GaAN~\citep{zhang2018gaan}, NDLS~\citep{zhang2021node}, supervised and unsupervised variants of GraphSAGE~\citep{hamilton2017inductive} and FastGCN~\citep{chen2018fastgcn}.
We also reuse the metrics already reported in ~\citet{wu2019simplifying} and ~\citet{chen2018fastgcn}.
Table~\ref{tab_reddit} shows the averaged test results. As expected, our supervised method OGC could still get the best performance.
In addition, our unsupervised methods GGC and GGCM still outperform SGC, S2GC, all other unsupervised methods, and even some supervised GNNs.


\begin{figure*}[!ht]
  \begin{minipage}{.45\textwidth}
  \centering
  \scriptsize
    \renewcommand{\arraystretch}{0.8}
    \begin{tabular}{l|l|l}
        \toprule
        \textbf{Setting} & \textbf{Method} & \textbf{Test F1} \\
         \midrule
        \multirow{6}{*}{\shortstack[c]{\textbf{Supervised}}}
        & GaAN  & $96.4$ \\
        & SAGE-mean & $95.0$\\
        & SAGE-LSTM & $95.4$\\
        & SAGE-GCN & $93.0$\\
        & FastGCN & $93.7$\\
        & GCN & OOM \\
        & NDLS & $96.8$ \\
        & \textbf{OGC} (ours) & \textbf{97.9}\\
        \midrule
        \multirow{9}{*}{\shortstack[c]{\textbf{Unsupervised}}}
        & SAGE-mean & $89.7$ \\
        & SAGE-LSTM & $90.7$\\
        & SAGE-GCN  & $90.8$\\
        & DGI & $94.0$\\
        & SGC & $94.9 $ \\
        & S$^2$GC & $95.3 $ \\
        & \textbf{\mygspalg} (ours) & $95.0$ \\
        & \textbf{\mygspalg M} (ours) & \textbf{95.8}\\
        \bottomrule
        \end{tabular}
    \captionof{table}{\footnotesize{Micro-averaged F1 (\%) on Reddit. }}
    \label{tab_reddit}
  \end{minipage}
  \quad
  \begin{minipage}{.55\textwidth}
  \scriptsize
  \setlength{\tabcolsep}{1.5mm}
  \renewcommand{\arraystretch}{0.95}
    \begin{tabular}{l|l|cccccc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{Layers/Iterations}} \\
             &  & 2  & 4  & 8 & 16 & 32 & 64 \\
        \midrule
        \multirow{4}{*}{\shortstack[c]{\textbf{Cora}}}
        & SGC &68.15 &60.47 &50.38 &41.76 &31.93 &20.51\\
        & S$^2$GC & 84.84 &80.49 &76.74 &73.87 &72.55 &72.05 \\
        & \textbf{\mygspalg} & 81.19 &81.22 &81.22 &81.22 &81.22 &81.22 \\
        & \textbf{\mygspalg M} & \textbf{88.97} &86.92 &84.98 &83.93 &82.96 &82.94\\
        \midrule
        \multirow{4}{*}{\shortstack[c]{\textbf{Citeseer}}}
        & SGC &74.89 &58.26 &50.84 &44.56 &37.60 &31.16\\
        & S$^2$GC & 90.66 &88.67 &84.99 &78.04 &71.10 &68.17 \\
        & \textbf{\mygspalg} &88.66 &88.87 &88.90 &88.90 &88.90 &88.90 \\
        & \textbf{\mygspalg M} &\textbf{91.94} &91.62 &91.11 &90.64 &90.10 &89.98\\
        \midrule
        \multirow{4}{*}{\shortstack[c]{\textbf{Pubmed}}}
        & SGC &39.57 &27.81 &17.12 &13.06 &8.72 &4.341\\
        & S$^2$GC &56.00 &54.63 &50.65 &48.78 &48.14 &47.07 \\
        & \textbf{\mygspalg} &46.88 &49.48 &49.96 &50.00 &50.00 &50.00 \\
        & \textbf{\mygspalg M} &56.95 &\textbf{57.90} &57.63 &56.82 &56.47 &55.90\\
        \bottomrule
        \end{tabular}
    \captionof{table}{\footnotesize{Graph reconstruction accuracy. }}
    \label{tab_graph_recons_eculid}
  \end{minipage}

\end{figure*}


\vspace{-5pt}
\subsection{Graph Reconstruction}
\vspace{-5pt}


Following~\citet{tsitsulin_2018_www}, to avoid interference, we only use graph structure information by setting $X=I_{n}$.
Then, for each node, we rank all other nodes according to their distance to this one in the embedding space based on Euclidean distance.
Then, we take the same number of nodes equal to its actual degree as its predicted neighbors.
Next, we count the rate of correct predictions.
Finally, the graph reconstruction accuracy is computed as the average over all nodes.
For comparison, we adopt two no-learning baselines: SGC and S$^2$GC.
As shown in Table~\ref{tab_graph_recons_eculid}, the performance of SGC and S$^2$GC declines quickly as the iteration number increases.
In contrast, our two methods \mygspalg\ and GGCM perform much better.
Especially, by gathering the multi-scale information, GGCM always obtains the best performance.
