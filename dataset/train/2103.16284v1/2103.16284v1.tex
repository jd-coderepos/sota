In this section, we first introduce the experimental setup including dataset, evaluation metrics, and implementation details. Then, we analyze the quantitative results of our method and a set of baseline variants. Finally, we visualize several segmentation masks.

\subsection{Experimental Setup}
\textbf{Datasets and Metrics}
We evaluate the proposed method on three benchmark datasets, i.e., RefCOCO \cite{kazemzadeh2014referitgame}, RefCOCO+ \cite{kazemzadeh2014referitgame} and RefCOCOg \cite{mao2016generation}.
We adopt intersection-over-union (IoU) and prec@X as the evaluation metrics \cite{luo2020multi,ye2019cross}. The IoU calculates intersection regions over union regions of the predicted segmentation mask and the ground truth. The prec@X measures the percentage of test images with an IoU score higher than the threshold  , where .

The RefCOCO dataset contains 19,994 images with 142,210 referring expressions for 50,000 objects. The images and expressions are collected from the MSCOCO \cite{lin2014microsoft} with a two-player game \cite{kazemzadeh2014referitgame}. It is split into train, validation, test A and test B with a number of 120,624, 10,834, 5,657 and 5,095 samples, respectively. In general, each image contains two or more objects with the same object class and each expression has an average length of 3.5 words.

The RefCOCO+ dataset contains
19,992 images with 141,564 referring expressions for 49,856 objects. The images and expressions are also collected from the MSCOCO. It is also split into train, validation, test A and test B with a number of 120,191, 10,758, 5,726 and 4,889 samples, respectively. Different from RefCOCO, the expressions in RefCOCO+ include more appearances than absolute locations.

The RefCOCOg dataset is also collected from MSCOCO and contains 26,711 images with 104,560 referring expressions for 54,822 objects. Different from RefCOCO and RefCOCO+, expressions in RefCOCOg are collected from Amazon Mechanical Turk instead of a two-player game and have a longer length of 8.4 words includes both appearances and locations of the referent.
RefCOCOg \cite{mao2016generation,nagaraja2016modeling} have two types of data partitions, i.e., google partition \cite{mao2016generation} and UNC partition \cite{nagaraja2016modeling}. We adopt UNC partition in this paper.


\textbf{Implementation Details}
Following previous work \cite{luo2020multi}, we adopt the Darknet53 \cite{redmon2018yolov3} as the visual backbone, which is pre-trained on MSCOCO while removing the images appeared in the validation and test sets of three datasets. The input images are resized to 416416 and the input sentences are set with a maximum sentence length of 15 for RefCOCO and RefCOCO+, and 20 for RefCOCOg. A 1024 dimensional bi-GRU is used to extract the textual feature. The filtering dimension  is set to 1024. The decoder has 1 layer network, 4 heads and 1024 hidden units. 
Adam \cite{Kingma2014Adam}
is used as the optimizer to train our model. The initial learning rate is 0.001, which is decreased by a factor of 0.1 at 30-th epoch. The batch size and training epochs are set to 18 and 45, respectively.

\subsection{Main Results}

\begin{table*}[htbp]
\begin{center}
\footnotesize
\caption{Comparison with the state-of-the-art methods on three benchmarks datasets using IoU as metric. ``-'' represents that the result is not provided. DCRF and ASNLS means DenseCRF \cite{krahenbuhl2011efficient} and ASNLS \cite{luo2020multi} post-processings, respectively.}
\label{table:sota}
\begin{tabular}{|l|c|c||c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Methods}&\multirow{2}{*}{Backbone}&\multirow{2}{*}{DCRF}&\multicolumn{3}{c|}{RefCOCO}&\multicolumn{3}{c|}{RefCOCO+} &\multicolumn{2}{c|}{RefCOCOg} \\
\cline{4-11}
{}&{}&{}&{val}&{testA}&{testB}&{val}&{testA}&{testB}&{val}&{test} \\
\hline
\hline
{RMI \cite{liu2017recurrent}}&{ResNet101}&{\cmark}&{45.18}&{45.69}&{45.57}&{29.86}&{30.48}&{29.50}&{-}&{-} \\
\hline
{DMN \cite{margffoy2018dynamic}}&{ResNet101}&{\xmark}&{49.78}&{54.83}&{45.13}&{38.88}&{44.22}&{32.29}&{-}&{-} \\
\hline
{RRN \cite{li2018referring}}&{ResNet101}&{\cmark}&{55.33}&{57.26}&{53.95}&{39.75}&{42.15}&{36.11}&{-}&{-} \\
\hline
{MAttNet \cite{yu2018mattnet}}&{MRCN-ResNet101}&{\xmark}&{56.51}&{62.37}&{51.70}&{46.67}&{52.39}&{40.08}&{47.64}&{48.61} \\
\hline
{NMTree \cite{liu2019learning}}&{MRCN-ResNet101}&{\xmark}&{56.59}&{63.02}&{52.06}&{47.40}&{53.01}&{41.56}&{46.59}&{47.88} \\
\hline
{CMSA \cite{ye2019cross}}&{ResNet101}&{\cmark}&{58.32}&{60.61}&{55.09}&{43.76}&{47.60}&{37.89}&{-}&{-} \\
\hline 
{Lang2seg \cite{chen2019referring}}&{ResNet101}&{\xmark}&{58.90}&{61.77}&{53.81}&{-}&{-}&{-}&{46.37}&{46.95} \\
\hline
{BCAM \cite{hu2020bi}}&{ResNet101}&{\cmark}&{61.35}&{63.37}&{59.57}&{48.57}&{52.87}&{42.13}&{-}&{-} \\
\hline
{CMPC \cite{huang2020referring}}&{ResNet101}&{\cmark}&{61.36}&{64.53}&{59.64}&{49.56}&{53.44}&{43.23}&{-}&{-} \\
\hline
{MCN+ASNLS \cite{luo2020multi}}&{DarkNet53}&{\xmark}&{62.44}&{64.20}&{59.71}&{50.62}&{54.99}&{44.69}&{49.22}&{49.40} \\
\hline
{LSCM \cite{huilinguistic}}&{ResNet101}&{\cmark}&{61.47}&{64.99}&{59.55}&{49.34}&{53.12}&{43.50}&{-}&{-} \\
\hline 
{CGAN \cite{luo2020cascade}}&{DarkNet53}&{\xmark}&{64.86}&{\textbf{68.04}}&{62.07}&{51.03}&{55.51}&{44.06}&{51.01}&{51.69} \\
\hline
\hline
{\method (Ours)}&{DarkNet53}&{\xmark}&{\textbf{65.43}}&{67.76}&{\textbf{63.08}}&{\textbf{54.21}}&{\textbf{58.32}}&{\textbf{48.02}}&{\textbf{54.40}}&{\textbf{54.25}} \\
\hline
\end{tabular}
\end{center}
\end{table*}


To demonstrate the effectiveness of our model, we compare our segmentation results with the state-of-the-arts (SOTAs) methods on three referring segmentation benchmarks utilizing relevance filtering as the localization module, as shown in Tab.~\ref{table:sota}. It can be seen that our model achieves the best performances under IoU metric across different datasets even though we do not utilize the time-consuming post-processing, e.g., DenseCRF \cite{krahenbuhl2011efficient} and ASNLS \cite{luo2020multi}. Note that our model can further improve the performance when adopting relevance filtering for two times as shown in Tab.~\ref{table:filter}. Specifically, compared with the best competitor CGAN \cite{luo2020cascade} which proposes a cascade grouped attention network to perform step-wise reasoning, our model significantly outperforms it by about 3\% absolute IoU point on two challenging datasets (RefCOCO+ and RefCOCOg) performing only the simple relevance filtering once. The improved performances over the best competitor indicate that our model is very effective for this task.

\begin{figure*}[t]
\centering
\includegraphics[width=0.86\linewidth]{fig/filter.pdf}
\caption{Visualization of correlation heatmaps  (generated by relevance filtering) and final results  (pred\_mask) predicted by our model. gt means the ground truth segmentation mask of input image. Best viewed in color.}
\label{fig:filter}
\end{figure*}

Compared with the methods CMPC \cite{huang2020referring} and LSCM \cite{huilinguistic} which either perceive all the entities that are referred by the expression or utilize the linguistic structure as guidance to segment the referred object, our model achieves much better performances by explicitly
modeling the object position prior followed with a segmentation module, demonstrating the effectiveness of our pipeline. Here DMN \cite{margffoy2018dynamic} and Lang2seg \cite{chen2019referring} also model the multi-modal context by filtering.
But DMN utilizes every word to generate the kernel and performs convolution. Therefore, it needs to regress all the generated maps. Lang2seg utilizes the spatial-aware dynamic filters and needs to perform filtering in 7 times for different local regions. Different from them, our \method utilizes the sentence feature to generate one kernel and only needs filtering once. And the heatmap is utilized as the location to guide the generation of segmentation mask. 
The improved performance (16\% and 6\% IoU point) over them suggests that our segmentation module can learn a more accurate segmentation mask after obtaining the object location by language expression. 










\subsection{Qualitative Results}

To verify whether the proposed localization module can obtain the correct location of the referent and how the segmentation module works, we visualize the response heatmap  (generated by relevance filtering) and segmentation results of several samples shown in Fig.~\ref{fig:filter}. We can see that our model is able to localize the referent by the language-dependent filtering. 
We also evaluate the localization quality (the probability that the maximum value of heatmap lies in the ground truth mask). The result is 81.74\%, which further demonstrates our localization capacity of the relevance filtering. However, this heatmap is far from an accurate segmentation mask. After refined by the segmentation module which captures objects and image context at multiple scales, we can obtain the final precise prediction mask.

\begin{figure*}[t]
\centering
\includegraphics[width=0.75\linewidth]{fig/fig4.pdf}
\caption{Qualitative examples of referring image segmentation by different models. (b) (c) (d) show the proposed model w/o segmentation, fusion, and filter, respectively. MCN is the method proposed in \cite{luo2020multi}. Best viewed in color.}
\label{fig:visual}
\end{figure*}


\begin{table*}[t]
\begin{center}
\footnotesize
\caption{Ablation studies on RefCOCO dataset. Seg means the segmentation module.}
\label{table:abla}
\begin{tabular}{|c||cccc||c|c|c|c|c|c|}
\hline
{}&{Fusion}&{Filter}&{Seg}&{}&{IoU}&{prec@0.5}&{prec@0.6}&{prec@0.7}&{prec@0.8}&{prec@0.9} \\
\hline
\hline
{}&{\cmark}&{}&{}&{}&{55.08}&{60.58}&{51.26}&{41.55}&{27.15}&{6.76} \\
\cline{2-11}
{}&{}&{\cmark}&{}&{}&{54.53}&{60.43}&{50.00}&{39.03}&{24.95}&{5.85} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{}&{}&{56.94}&{63.49}&{54.20}&{44.13}&{29.43}&{8.36} \\
\cline{2-11}
{val}&{\cmark}&{}&{\cmark}&{}&{63.50}&{72.84}&{65.85}&{57.61}&{42.33}&{13.41} \\
\cline{2-11}
{}&{}&{\cmark}&{\cmark}&{}&{63.64}&{72.94}&{66.52}&{58.01}&{42.84}&{13.20} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{\cmark}&{}&{65.05}&{75.01}&{68.48}&{60.69}&{44.93}&{14.03} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{{\cmark}}&{{\cmark}}&{\textbf{65.43}}&{\textbf{75.16}}&{\textbf{69.51}}&{\textbf{60.74}}&{\textbf{45.17}}&{\textbf{14.41}} \\
\hline
\hline
{}&{\cmark}&{}&{}&{}&{56.82}&{62.49}&{53.60}&{42.87}&{28.27}&{6.65} \\
\cline{2-11}
{}&{}&{\cmark}&{}&{}&{56.05}&{61.66}&{51.76}&{40.32}&{25.61}&{5.67} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{}&{}&{58.53}&{64.52}&{55.65}&{45.59}&{30.51}&{7.94} \\
\cline{2-11}
{testA}&{\cmark}&{}&{\cmark}&{}&{65.31}&{75.32}&{69.45}&{60.46}&{44.76}&{11.38} \\
\cline{2-11}
{}&{}&{\cmark}&{\cmark}&{}&{66.41}&{77.00}&{71.27}&{62.63}&{46.65}&{12.82} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{\cmark}&{}&{67.49}&{78.10}&{72.87}&{64.47}&{\textbf{48.31}}&{\textbf{13.24}} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{{\cmark}}&{{\cmark}}&{\textbf{67.76}}&{\textbf{78.47}}&{\textbf{73.13}}&{\textbf{64.56}}&{47.98}&{12.92} \\
\hline
\hline
{}&{\cmark}&{}&{}&{}&{53.52}&{59.20}&{49.64}&{39.20}&{26.22}&{8.58} \\
\cline{2-11}
{}&{}&{\cmark}&{}&{}&{52.55}&{56.45}&{46.91}&{37.45}&{24.10}&{8.03} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{}&{}&{55.12}&{60.61}&{51.21}&{41.32}&{29.07}&{10.64} \\
\cline{2-11}
{testB}&{\cmark}&{}&{\cmark}&{}&{60.97}&{69.13}&{62.24}&{53.39}&{39.65}&{15.68} \\
\cline{2-11}
{}&{}&{\cmark}&{\cmark}&{}&{60.72}&{68.03}&{61.14}&{52.60}&{39.59}&{16.68} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{\cmark}&{}&{62.43}&{70.99}&{\textbf{64.65}}&{55.27}&{42.41}&{\textbf{17.48}} \\
\cline{2-11}
{}&{\cmark}&{\cmark}&{{\cmark}}&{{\cmark}}&{\textbf{63.08}}&{\textbf{71.82}}&{64.59}&{\textbf{55.74}}&{\textbf{42.79}}&{17.35} \\
\hline
\end{tabular}
\end{center}
\end{table*}



Fig.~\ref{fig:visual} shows the segmentation masks obtained by different models, which demonstrates the benefits of each module in our proposed method.
\begin{itemize}
    \item The predicted mask from model w/o segmentation can only obtain the location of the referent but not the fine mask of object, as shown in column (b);
    \item The model w/o fusion or filter also generates lower quality masks compared with our proposed full model as they fail to make clear judgement of the referred object, as shown in column (c) and (d).
\end{itemize}


Here we also compare our results with MCN~\cite{luo2020multi} in column (e) and (f).
Our generated segmentation masks have more obvious object shapes and finer outlines.


\subsection{Ablation Experiments}
Our proposed model is mainly composed of three modules, cross-modal fusion, localization (relevance filtering or transformer) and segmentation.
To investigate these three components and the proposed locating loss in our model, we perform a set of ablation studies on the Refcoco dataset. Tab.~\ref{table:abla} shows the result.


\begin{table}[!ht]
\begin{center}
\caption{Results of utilizing segmentation module on CMPC \cite{huang2020referring} on the RefCOCO dataset using IoU as metric.}
\label{table:refine}
\begin{tabular}{|c||c|c|c|}
\hline
{Model}&{val}&{testA}&{testB} \\
\hline
\hline
{CMPC}&{61.36}&{64.53}&{59.64}\\
\hline
{CMPC+Seg}&{\textbf{62.75}}&{\textbf{65.34}}&{\textbf{61.08}}\\

\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[!ht]
\begin{center}
\caption{Results of utilizing transformer instead of filtering on the val sets of three datasets using IoU as metric.}
\label{table:trans}
\begin{tabular}{|c||c|c|c|}
\hline
{Model}&{RefCOCO}&{RefCOCO+}&{RefCOCOg} \\
\hline
\hline
{LTS}&{65.43}&{54.21}&{54.40}\\
\hline
{LTS-Trans}&{\textbf{66.15}}&{\textbf{54.52}}&{\textbf{54.51}}\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[!ht]
\begin{center}
\caption{Effects of filtering on RefCOCO dataset using IoU as metric.  denotes number of filtering times. WordFilter means utilizing every word feature to generate kernel as DMN \cite{margffoy2018dynamic}.}
\label{table:filter}
\begin{tabular}{|c|l||c|c|c|}
\hline
{Model}&{n}&{val}&{testA}&{testB}\\
\hline
\hline
{}&{1}&{65.43}&{67.76}&{63.08}\\
\cline{2-5}
{\method}&{2}&{\textbf{66.04}}&{\textbf{68.68}}&{\textbf{63.27}}\\
\cline{2-5}
{}&{3}&{65.54}&{67.82}&{62.97}\\
\hline
{\method-WordFilter}&{1}&{64.92}&{67.31}&{62.50}\\
\hline
\end{tabular}
\end{center}
\end{table}


We first investigate the importance of fusing textual feature with visual feature to build multi-modal representations. It can be seen that the IoU accuracy on val dataset
drops 2.4\% (Fusion+Filter vs Filter) and 1.4\% (Fusion+Filter+Seg vs Filter+Seg). The fusion module proves the effectiveness of multi-modal representation in learning the semantic alignment between visual and linguistic modalities. Then we investigate the importance of relevance filtering by removing it from Fusion+Filter and Fusion+Filter+Seg. The IoU accuracy on val dataset drops 1.9\% and 1.5\%, respectively, which demonstrates that obtaining the location of the referent by the language description is beneficial to enhance the segmentation results. Comparing the result between Fusion (Filter / Fusion+Filter) and Fusion+Seg (Filter+Seg / Fusion+Filter+Seg), we can find that segmentation module can effectively improve the performances by obtaining a refined segmentation mask. In addition, we add this proposed segmentation module on CMPC \cite{huang2020referring} as shown in Tab.~\ref{table:refine}. The improved performances demonstrate that our module can generate more precise prediction mask. Finally, we can see that adding the locating loss also obtains better performance by supervising the alignments between image and text.



Tab.~\ref{table:trans} shows the results when utilizing transformer instead of filtering as the localization module. Using more complex attention model can further improve the performance by locating the referent better. 

\begin{table}[!ht]
\begin{center}
\caption{Results of \method with different input resolutions on the RefCOCO dataset using IoU as metric.}
\label{table:size}
\begin{tabular}{|c||c|c|c|c|}
\hline
{Model}&{resolution}&{val}&{testA}&{testB} \\
\hline
\hline
{}&{320320}&{63.01}&{65.40}&{60.78}\\
\cline{2-5}
{}&{352352}&{64.04}&{66.24}&{61.76} \\
\cline{2-5}
{\method}&{384384}&{64.45}&{67.02}&{62.47} \\
\cline{2-5}
{(n=1)}&{416416}&{65.43}&{67.76}&{63.08}\\
\cline{2-5}
{}&{448448}&{65.71}&{67.90}&{63.23} \\
\cline{2-5}
{}&{480480}&{\textbf{65.90}}&{\textbf{68.16}}&{\textbf{63.45}} \\
\hline
\end{tabular}
\end{center}
\end{table}



\begin{table}[!ht]
\begin{center}
\caption{Results of utilizing transformer, more filters and larger input resolution on the val sets of three datasets.}
\label{table:best}
\begin{tabular}{|c||c|c|c|}
\hline
{Model}&{RecCOCO}&{RecCOCO+}&{RecCOCOg} \\
\hline
\hline
{LTS}&{65.43}&{54.21}&{54.40}\\
\hline
{LTS*}&{\textbf{66.75}}&{\textbf{54.94}}&{\textbf{54.51}}\\
\hline
\end{tabular}
\end{center}
\end{table}


Tab.~\ref{table:filter} shows the experimental results when adopting multiple relevance filters, where  means we utilize relevance filtering twice in our model. When , our method gets better performance (+0.61 IoU). Such score is much better than previous published best result. For simplicity, all other experiments are performed with . Besides, we conduct an experiment by utilizing every word to generate kernel as DMN \cite{margffoy2018dynamic}. The results are shown in Tab.~\ref{table:filter}, where they obtain comparative performances. Considering the simplicity, we adopt sentence-based filtering in this paper.  


In addition, we find that larger input resolution will improve the performance by providing richer information as shown in Tab.~\ref{table:size}. In this paper, we set the image to  for fair comparison with previous methods.

Furthermore, we perform the experiments with the setting of using transformer, more filtering times, and larger input resolution on RefCOCO, RefCOCO+, and RefCOCOg datasets. The results are shown in Tab.~\ref{table:best}. We can see that our model obtains better performances with this setting. 


