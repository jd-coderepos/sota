\documentclass{sig-alternate}

\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{alltt, amssymb}
\usepackage{comment}  
\usepackage{color}
\usepackage{url}            
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true,citecolor=blue,hypert
exnames=false]{hyperref}


\def\bell{\mbox{\boldmath}}
\def\sbell{\mbox{\scriptsize\boldmath}}

\def\bsigma{\mbox{\boldmath}}
\def\sbsigma{\mbox{\scriptsize\boldmath}}

\def\bpi{\mbox{\boldmath}}
\def\sbpi{\mbox{\scriptsize\boldmath}}

\def\balpha{\mbox{\boldmath}}
\def\sbalpha{\mbox{\scriptsize\boldmath}}

\def\bdelta{\mbox{\boldmath}}
\def\sbdelta{\mbox{\scriptsize\boldmath}}

\def\blambda{\mbox{\boldmath}}
\def\sblambda{\mbox{\scriptsize\boldmath}}

\def\bgamma{\mbox{\boldmath}}
\def\sbgamma{\mbox{\scriptsize\boldmath}}

\newcommand{\bigOsoft}{\tilde{O}}


\def\Q {\ensuremath{\mathbb{Q}}}
\def\N {\mathbb{N}}
\def\R {\ensuremath{\mathbb{R}}}
\def\Z {\ensuremath{\mathbb{Z}}}
\def\F {\mathbb{F}}
\def\H {\ensuremath{\mathbb{H}}}
\def\K {\ensuremath{\mathbb{K}}}
\def\Kbar {\ensuremath{\overline{\mathbb{K}}}}
\def\A {\ensuremath{\mathsf{A}}}

\def\M{\ensuremath{\mathsf{M}}}
\def\mA {\ensuremath{{\bf A}}}  
\def\mmu {\ensuremath{{\bf u}}}
\def\mv {\ensuremath{{\bf v}}}
\def\mw {\ensuremath{{\bf w}}}
\def\mB {\ensuremath{{\bf B}}}
\def\mS {\ensuremath{{\bf S}}}
\def\mI {\ensuremath{{\bf I}}}   
\def\mZ {\ensuremath{{\bf 0}}}
\def\mC {\ensuremath{{\bf C}}} 
\def\mD {\ensuremath{{\bf D}}}
\def\mE {\ensuremath{{\bf E}}}
\def\mM {\ensuremath{{\bf M}}}

\def\ZZ {\ensuremath{\mathbb{Z}}}
\def\J {\ensuremath{\mathbb{J}}}
\def\CC {\ensuremath{\mathbb{M}}}
\def\C {\ensuremath{\mathbb{C}}}
\def\V {\ensuremath{\mathbb{V}}}
\def\W {\ensuremath{\mathbb{W}}}
\def\B {\ensuremath{\mathbb{B}}}
\def\D {\ensuremath{\mathbb{D}}}
\def\L {\ensuremath{\mathbb{L}}}
\def\U {\ensuremath{\mathbb{U}}}
\def\I {\ensuremath{\mathbb{I}}}

\def\myproof{\noindent{\sc Proof.}~}
\def\foorp{\hfill}

\def\gathen#1{{#1}}
\def\hoeven#1{{#1}}

\newtheorem{Def}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{Coro}{Corollary}
\newtheorem{Prop}{Proposition}
\newtheorem{PropDef}{Proposition - Definition}
\newtheorem{Lemma}{Lemma}
\newtheorem{Hyp}{Assumption}

\title{Fast algorithms for differential equations \\ in positive characteristic}

\numberofauthors{2}
\author{
\alignauthor Alin Bostan\\
\affaddr{Algorithms Project}\\
\affaddr{INRIA Rocquencourt}\\ 
\affaddr{France}\\
\affaddr{78153 Le Chesnay Cedex France}\\
\alignauthor \'Eric Schost\\
\affaddr{ORCCA and Computer Science Department}\\
\affaddr{The University of Western Ontario}\\
\affaddr{London, ON, Canada}\\
\affaddr{eschost@uwo.ca} 
}

\begin{document}

\maketitle
\begin{abstract} 
We address complexity issues for linear differential equations in characteristic~: resolution and computation of the -curvature. For these tasks, our main focus is on algorithms whose complexity behaves well with respect to~. We prove bounds linear in  on the degree of polynomial solutions and propose algorithms for testing the existence of polynomial solutions in sublinear time , and for determining a whole basis of the solution space in quasi-linear time ; the  notation indicates that we hide logarithmic factors. We show that for equations of arbitrary order, the -curvature can be computed in subquadratic time , and that this can be improved to  for first order equations and to  for classes of second order equations.  
\end{abstract}



\vspace{1mm}
 \noindent
 {\bf Categories and Subject Descriptors:} \\
\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
  Manipulation -- \emph{Algebraic Algorithms}
 
 \vspace{1mm}
 \noindent
 {\bf General Terms:} Algorithms, Theory
 
 \vspace{1mm}
 \noindent
 {\bf Keywords:} Algorithms, complexity, differential equations, polynomial solutions, -curvature.



\medskip

\section{Introduction}\label{sec:intro}

\noindent We study several algorithmic questions related to linear
differential equations in characteristic~, where  is a prime
number: resolution of such equations and computation of their
-curvature. Our emphasis is on the complexity viewpoint.

Let thus  be the finite field with  elements, and let  be the algebra of differential operators with
coefficients in , with the commutation relation . One of the important objects associated to a
differential operator  of order  in  is its -curvature, hereafter denoted . By
definition, this is the  matrix with coefficients in
, whose -entry is the coefficient of  in
the remainder of the Euclidean (right) division of  by
, for .

The concept of -curvature originates in Grothendieck's work in the
late 1960s, in connection to one of his famous (still unsolved)
conjectures. In its simplest form, this conjecture is an arithmetic
criterion of algebraicity, which states that a linear differential
equation with coefficients in  has a basis of algebraic
solutions over  if and only if its reductions
modulo~ have zero -curvature, for almost all primes~. The
search of a proof of this criterion motivated the development of a
theory of differential equations in characteristic~ by
Katz~\cite{Katz70}, Dwork~\cite{Dwork82}, Honda~\cite{Honda81}, etc.

There are two basic differences between differential equations in
characteristic zero and~: one concerns the dimension of the
solution space, the other, the form of the solutions. While in
characteristic zero, a linear differential equation of order 
admits exactly  linearly independent solutions, this is no longer
true in positive characteristic: for , the dimension of the solution space of the equation 
over the field of constants  is generally less than the
order~. Moreover, by a theorem of Cartier and Katz (see
Lemma~\ref{lemma:pcurv-vs-ratsols} below), the dimension is exactly
 if and only if the -curvature matrix  is zero. Thus,
roughly speaking, the -curvature measures to what extent the
solution space of a differential equation modulo~ has dimension
close to its order. 

On the other hand, the form of the solutions is simpler in
characteristic~ than in characteristic zero. Precisely, the
existence of polynomial solutions is equivalent to the existence of
solutions which are either algebraic over , or power series
in , or rational functions in ~\cite{Honda81}.
Therefore, in what follows, by solving  we simply understand
finding its polynomial solutions.

In computer algebra, the -curvature was publicised by van der
Put~\cite{vanDerPut95,vanDerPut96}, who used it as a central tool in
designing algorithms for factoring differential operators in
. Recently, his algorithms were
analyzed from the complexity perspective and implemented by
Cluzeau~\cite{Cluzeau03}, who extended them to the case of
systems. Cluzeau also took in~\cite{Cluzeau04} a first step towards a
systematic modular approach to the algorithmic treatment of
differential equations.

Improving the complexity of the -curvature computation is an
interesting problem in its own right. Our main motivation for studying
this question comes, however, from concrete applications. First, in a
combinatorial context, the use of the -curvature served in the
automatic classification of restricted lattice walks~\cite{BoKa08b}
and notably provided crucial help in the treatment of the notoriously
difficult case of Gessel's walks~\cite{BoKa08a}.  Also, intensive
-curvature computations were needed in~\cite{BBHMWZ08}, where the
question is to decide whether various differential operators arising
in statistical physics have nilpotent, or zero, -curvature. 

In the latter questions, the prime  was ``large'', typically of the
order of . This remark motivates our choice of considering 
as the most important parameter: our primary objective is to obtain
complexity estimates featuring a low exponent in .

\smallskip\noindent{\bf Previous work.}  The non-commutativity of
 prevents one from straightforwardly
using binary powering techniques for the computation of  via
that of . Thus, the complexity of all currently
known algorithms for computing the -curvature is quadratic in~.

Katz~\cite{Katz82} gave the first algorithm, based on the following
matrix recurrence: define

where  is the companion matrix
associated to ; then,  is the -curvature matrix (hence
our notation). 

It was observed in~\cite[\S13.2.2]{PuSi03} that it is slightly more
efficient to replace~\eqref{eq:pcurv} by the recurrence  which computes the first column  of
, by taking for  the first column of . Then
 are the columns of~. This
alternative requires only matrix-vector products, and thus saves a
factor of~, but still remains quadratic in~.  Cluzeau proposed
in~\cite[Prop.~3.2]{Cluzeau03} a fraction-free version
of~\eqref{eq:pcurv} having essentially the same complexity, but
incorrectly stated that the method in~\cite{PuSi03} works in linear
time in~.

Concerning polynomial and rational solutions of differential equations
modulo~, very few algorithms can be found in the literature. Cluzeau
proposes in~\cite[\S2]{Cluzeau03} an algorithm of cubic complexity
in~ and, in the special case when , a different algorithm
of quadratic complexity in~, based on a formula due to Katz which is
the nub of Lemma~\ref{lemma:pcurv-vs-ratsols} below.

\smallskip\noindent{\bf Our contribution.}  We prove in
Section~\ref{sec:solpol} a linear bound in  on the degree for a
basis of the solution space of polynomial solutions of an equation
. Then, we adapt the algorithm in~\cite{AbBrPe95} and its
improvements~\cite{BoClSa05} to the case of positive characteristic;
we show how to test the existence of polynomial solutions in time
nearly proportional to~, and how to determine a full basis of the solution
space in time quasi-linear in~.

Regarding the -curvature, we first focus on two particular cases:
first order operators, where the cost is polynomial in 
(Section~\ref{sec:one}), and second order ones, for which we obtain a
cost quasi-linear in~ in some cases 
(Section~\ref{sec:two}).

In general, a useful way to see~\eqref{eq:pcurv} is to note that the
-curvature is obtained by applying the operator
 to~. In Section~\ref{sec:high} we exploit
this observation. As a side result, we give a baby steps~/~giant steps
algorithm for computing the image  of an operator  applied to a
polynomial ; this is inspired by Brent-Kung's algorithm for power
series composition~\cite{BrKu78}.

\smallskip\noindent{\bf Complexity measures.} Time complexities are
measured in terms of arithmetic operations in~. 

We let  be such that polynomials of degree at
most~ in~ can be multiplied in time . Furthermore,
we assume that  satisfies the usual assumptions
of~\cite[\S8.3]{GaGe99}; using Fast Fourier Transform,  can be
taken in ~\cite{ScSt71,CaKa91}.  We suppose
that  is a constant such that two matrices in
 can be multiplied in time . The
current tightest upper bound is ~\cite{CoWi90}.

The precise complexity estimates of our algorithms are sometimes
quite complex; to highly their main features, we rather give
simplified estimates. Thus, we use the notation 
for  if  is in  for some
. For instance,  is in .



\section{Preliminaries}

\noindent {\bf Basic properties of the -curvature.} 
We first give degree bounds on the -curvature of an operator.
Consider 

with all  in  of degrees at most  and . As in~\eqref{eq:pcurv}, we define  and  for .
\begin{Lemma}\label{lemma:1}
  For , let . Then  is in
  , with entries of degree at most .
\end{Lemma}
\myproof
Explicitly, we have
 From this, we see that the sequence 
satisfies the equation

where  is the  identity matrix. The claim
follows.  \foorp

\smallskip\noindent In particular, the -curvature  has
the form , with  a polynomial matrix of degree
at most .



A second useful result is the following lemma, attributed to Katz. It
relates the solution space of  to the -curvature and
generalizes a theorem of Cartier.  A proof can be found
in~\cite[Th.~3.8]{Cluzeau03}.



\begin{Lemma}\label{lemma:pcurv-vs-ratsols}
  The dimension over  of the vector space of rational
  solutions of  is equal to the dimension over  of the
  kernel of .  In particular,  has a basis of polynomial
  solutions if and only if its -curvature is zero.
\end{Lemma}

\smallskip\noindent{\bf Operator algebras.} In what follows, we mainly
consider operators with coefficients in , but also sometimes
more generally in the  matrix algebra
;  as has been done up to now, we will write 
matrices in bold face. If  is in
 of the form

with coefficient matrices  in  of
maximal degree , we say that  has {\em bidegree} . 

\smallskip\noindent{\bf Regularization.} For most of our algorithms,
we must assume that the origin  does not cancel the leading term
 of the operator . 

If we can find  such that , we can
ensure this property by translating the origin to . To ensure that
we can find , we must make the following hypothesis, written :  does not vanish identically on .
\begin{Lemma}
  Given  of bidegree , testing whether  holds
  can be done in time . If so, one can find 
  such that  and translate the coordinates' origin
  to  in time .
\end{Lemma}
\myproof Testing  amounts to verify whether  divides
. If ,  obviously holds. Else, we
have ; then, it is enough to reduce  modulo , which
takes time .

If  holds, we know that we can find  such that ; so it is enough to evaluate
 at this set of points, which by~\cite[\S 10.1]{GaGe99} takes time
. Once  is known, we shift all coefficients of
 by . Using fast algorithms for polynomial shift~\cite{GaGe97},
the time is  per coefficient; the conclusion follows.
\foorp

\smallskip\noindent As a consequence, in all the following algorithms, we
will assume that  holds. If not, one could actually work in a
low-degree extension of  to find ; we do not consider this 
generalization here.




\section{Polynomial solutions}\label{sec:solpol}

\noindent We start with the study of the polynomial solutions of a
linear differential equation; aside from its own interest, this
question will arise in our algorithm for order two operators in
Section~\ref{sec:two}. 
\begin{theorem}\label{coro:solpol}
  Let  be as in~\eqref{eq:L}, with  and , and such
  that  holds. Then, one can test whether the equation 
  has non-zero solutions in  in time
   
  If so, one can determine a basis of the solution set consisting of
  polynomials of degree at most  in extra time

\end{theorem}
\noindent The main point here is that for fixed  and , {\em
  testing} the existence of solutions takes time
, whereas finding a basis of the solution space
takes time .

In all this section,  is fixed, and the assumptions of
Theorem~\ref{coro:solpol} are satisfied. The assumptions on the
relative order of magnitude of  help us obtain simple cost
estimates and rule out some possible overlaps in indices modulo
. The assumption  is here mostly for convenience; the
assumption  is necessary.



\subsection{Degree bounds}

\noindent 
Let  be the -vector space of rational
solutions of the equation .  The following proposition proves a
bound linear in  on the degree of a basis of . To our
knowledge, such linear bounds were previously available only in two
particular cases: (a) when the equation has a basis of polynomial
solutions and under the additional hypotheses  and ~\cite[Th.~7]{Honda81}; (b) when  and
the equation has exactly one nonzero polynomial
solution~\cite[Lemma~10.1]{Dwork82}. These bounds are respectively
 for (a) and  for (b). In the
general case, the analysis in~\cite{Cluzeau03,Cluzeau04} suggests a
bound quadratic in  of type . Our result refines this
approach.

\begin{Prop}\label{theo:theorysolpol}
  If  has at least one nonzero solution in , then
   admits a basis consisting of polynomial solutions of
  degree at most  each.
\end{Prop}
\myproof The map  defined by  is -linear. Let  be the matrix of this map with respect to the basis
.  Write  for some  in . Then,  is in
 if and only if ,
with  in  such that .

Since  is a sum of 
polynomials of pairwise distinct degrees , we deduce
that for all , .

Since  has a non-zero solution in , it has also a
non-zero solution in , by clearing denominators. Let thus 
be in  such that , or equivalently . Since all terms in the
right-hand side have degree at most , we deduce that
. This implies that  has degree
at most .

To summarize, for all , we obtain the inequality
. This implies that for 
any permutation  of , 

since the sum of the terms  is zero. This implies
that all minors of  have degree at most , since
any term appearing in the expansion of such minors can be completed
to form one of the form .

The nullspace of  admits a basis , all of whose 
entries are minors of . By what was said above, they all have
degree at most . A basis of  is easily deduced:
to  corresponds the polynomial
. We deduce that
, as claimed. \foorp



\subsection{Solutions of bounded degree}
 
\noindent Let  be the -vector space
of polynomial solutions of  of degree at most . We are
interested in computing either the dimension of , or an
-basis of it. In view of the former proposition, this will be
sufficient to prove
Theorem~\ref{coro:solpol}. Proposition~\ref{theo:solpol} gives cost
estimates for these tasks, adapting the algorithm in~\cite{AbBrPe95}
and its improvements~\cite{BoClSa05}.
\begin{Prop}\label{theo:solpol}
  Under the assumptions of Theorem~\ref{coro:solpol}, one can compute
   in time
  
One can deduce a basis of  in extra time
.
\end{Prop}
For  and  fixed, the main feature of this result is that the
cost of computing the dimension of  is the sublinear
, whereas the cost of computing a basis of it is
.

\smallskip \myproof Let  be unknowns and let
 be the polynomial
; for  or , we let
. There exist  in , of degree at
most , such that for , the coefficient of degree  of

is 

note for further use that

The polynomial  is in  if and only if  for . Shifting indices, we obtain the system of linear
equations , with , in the unknowns
.

The matrix of this system is band-diagonal, with a band of width
. In characteristic zero or large enough, one can eliminate each
unknown , with , using . Here, some equations
 become deficient, in the sense that the coefficient of
 vanishes; this induces a few complications.

\smallskip\noindent{\bf Outline of the computation.}
Since  is not zero,  is the non-zero
polynomial , and
.  Let then 
be the set of roots of the latter polynomial. For ,
if  is not in , then  is the highest-index
unknown appearing with a non-zero coefficient in ; we can
then eliminate it, by expressing it in terms of the previous 's.

The unknowns we cannot eliminate this way are , with  in 
 
the residual equations are ,
for  in , with

and 

To determine the dimension of , and later on find a basis
of it, we rewrite the residual equations using the residual unknowns.

For  in , the unknowns present in  are
. Of those, only
 need to be rewritten in terms of ; the others already belong to this set. Thus, it is
enough to express all  in terms of , for .

For  in , the unknowns in  are
 (the higher index ones are zero). So, it is
enough to compute  in terms of . This is thus the same problem as above, for index .

\smallskip\noindent{\bf Expressing all needed unknowns using .}  Let
. For  in , one can rewrite the equation
 as the first order recurrence
-1mm]
\vdots \\
u_{n}
\end{matrix} \right ] = 
\mA(n) \left [ \begin{matrix}
u_{n-d-r}\
with
 note that for ,
. Let next  be the matrix factorial
. Then we have the equalities, for :
-1mm]
\vdots \\
u_{ip-1}
\end{matrix} \right ] = 
\mB \left [ \begin{matrix}
u_{(i-1)p-d}\
Note that ; we let  be the 
column-vector consisting of all , for  in . Let further
 be the  zero matrix. For ,
suppose that we have determined  matrices 
 such that, for , we have
-1mm]
\vdots \\
u_{jp-1}
\end{matrix} \right ] = 
\mC_j \mmu
\quad\text{and}\quad
\left [ \begin{matrix}
u_{jp-r-d}\
with
  
Letting  be the matrix made of the last  rows of
, we define
 then,~\eqref{eq:Cj} is satisfied at index
 as well. 

\smallskip\noindent{\bf Rewriting all residual equations using .}
Combining all previous information, we obtain a matrix equality of the
form , where  is the column vector with entries
, for , and where  is
the matrix obtained by stacking up .

We have seen that all indeterminates appearing in the residual
equations , with  in , are actually in .  By
evaluating the coefficients  at , for  in
, we obtain the matrix  of the residual equations, expressed
in terms of the unknowns in . Hence, the matrix 
expresses the residual equations in terms of , for  in .

By construction, the dimension of  equals the dimension
of the nullspace of . Knowing a basis of the nullspace of ,
one deduces a basis of  using~\eqref{eq:rec}, to compute
all  for  in .

\smallskip\noindent{\bf Cost analysis.} By~\cite[Lemma~7]{BoClSa05},
one can compute~ in time ,
which is .
Computing a matrix  requires one matrix multiplication of size
. In view of the inequality ,
using block matrix multiplication, this can be done in time
. Thus, computing all needed matrices  takes time
.

The matrix  has size ; no more computations
are needed to fill its entries.
The matrix  has size . Its entries are
obtained by evaluating  at all  in . Since
 and , this takes time 
per polynomial. Since , the total time is
.



The matrix  has size ; using block
matrix multiplication with blocks of size , it can be computed in
time . A basis of its nullspace can
be computed in time .

Given a vector  in the nullspace of , one can
reconstruct  using~\eqref{eq:rec}. This
first requires evaluating all coefficients of all equations ,
for  in , which takes time , with . 

Then, for a given  in the nullspace of ,
deducing  requires  matrix-vector
products in size . The dimension of the nullspace is ; we
process all vectors in the nullspace basis simultaneously, so that we
are left to do  matrix products in size  by
. The cost of each product is , so the
total cost is .

Summing  proves the first part of the
proposition. Adding to this  and  gives the second claim.



\subsection{Proof of Theorem~\ref{coro:solpol}}
 
\noindent Let  and  be as above. By
Proposition~\ref{theo:theorysolpol},  if and
only if . Hence, the first estimate
of Proposition~\ref{theo:solpol} proves our first claim.

Suppose that , and let 
be an -basis of .
Proposition~\ref{theo:theorysolpol} implies that 
generates  over .  We deduce an -basis
 of  in a naive way: starting from
, we successively try to add  to
. Independence tests are performed at each step, using
the following lemma.

\begin{Lemma}
  Given  in  of degree less than , one
  can determine whether they are linearly independent over 
  in time .
\end{Lemma}
\myproof It suffices to compute their Wronskian determinant. The
determinant of a matrix of size  can be computed using
 sums and products~\cite{Berkowitz84}; since here
all products can be truncated in degree , the cost is
. \foorp

\smallskip\noindent
At all times, there are at most  elements in , so we
always have . Since we also have , the overall
time is , as claimed.





\section{p-curvature: first order}\label{sec:one}

\noindent For first order operators, there is a closed form formula
for the -curvature. Let , with  in ;
then, by~\cite[Lemma 1.4.2]{vanDerPut95}, the -curvature of  is the
 matrix with entry , where the first term is
the derivative of order  of . In this case, we do not
distinguish between the -curvature and its unique entry.

The case of first order operators stands out as the only one where a
cost polynomial in  can be reached; this is possible since in
this case, we only compute  non-zero coefficients. As per our
convention, in the following statement, we take  not necessarily
monic, but with polynomial coefficients.

\begin{theorem}\label{theo:ordre1}
  Given  in  of
  bidegree  that satisfies , one can compute its
  -curvature in time .
\end{theorem}
\myproof Since the -curvature belongs to , it suffices
to compute its th root. Computing the -curvature itself requires
no extra arithmetic operation, since taking -powers is free 
over , as far as arithmetic operations are concerned.
Hence, we claim that the rational function

can be computed in time .  Of course, the only
non-trivial point is to compute , with .

Observe that  is a polynomial of degree less than ,
so  is a polynomial of degree less than . Hence, it is enough
to compute the power series expansion . From this, we 
deduce the polynomial  by a power series multiplication in degree
, and finally  by division by .

Let us write the power series expansion . Then, the series  equals , so it
is enough to compute the coefficients .

We start by computing the first coefficients  by
power series division, in time . From these initial
conditions, the coefficients  can be deduced for
 operations using binary powering techniques,
see~\cite{Fiduccia85} or~\cite[Sect.~3.3.3]{Bostan03}.  Iterating this
process  times, we obtain the values , for
, in time .\foorp

\smallskip\noindent As an aside, note that by
Lemma~\ref{lemma:pcurv-vs-ratsols}, a rational function  is a
logarithmic derivative in  if and only if .  This point also forms the basis of Niederreiter's algorithm
for polynomial factoring~\cite{Niederreiter93}.



\section{p-curvature: second order}\label{sec:two}

\noindent For second order operators, it is possible to exploit a
certain linear differential system satisfied by the entries of the
-curvature matrix: already in~\cite{Dwork90,vanDerPut96}, one finds
a third order linear differential equation satisfied by an
anti-diagonal entry of the -curvature, for the case of operators of
the form , or more generally ,
when .

In this section, we let  have the form ,
with  in  of degree at most . We assume that  and , and that  holds (we do not repeat these
assumptions in the theorems); we let  be the companion matrix of
 and let  be its -curvature.

We give partial results regarding the computation of : we give
algorithms of cost  or  to test
properties of , or compute it in some cases, up maybe to some
indeterminacy.  Though these algorithms do not solve all questions,
they are still substantially faster than the ones for the general case
in the next section.

\smallskip\noindent{\bf The trace of the -curvature.} We start by
an easy but useful consequence of the result of the previous
section: the trace of  can be computed fast.
\begin{theorem}\label{theo:trace}
  One can compute the trace  of  in time
  .
\end{theorem}
\myproof The -curvature of a determinant connection is the trace of the
-curvature of the original connection~\cite{Katz82,Voloch00}.  
Concretely, this
means that the trace of  is equal to the -curvature of .
By Theorem~\ref{theo:ordre1}, it can be computed in time
.  \foorp

\smallskip\noindent{\bf Testing nilpotence.} As a consequence of the
previous theorems, we obtain a decision procedure for nilpotence.
\begin{Coro}\label{coro:nilpotence}
  One can decide whether  is nilpotent in time
  
\end{Coro} 
\myproof The -curvature  is nilpotent if and only if its
trace and determinant are both zero. By Theorem~\ref{theo:trace}, the
condition on the trace can be checked in time logarithmic in~. By
Lemma~\ref{lemma:pcurv-vs-ratsols}, the second condition
 is equivalent to the fact that  has a non-zero
solution, which can be tested in the requested time by
Theorem~\ref{coro:solpol}.  \foorp

\smallskip\noindent{\bf The eigenring.} To state our further results,
we need an extra object: the {\em eigenring}  of
. This is the set of matrices  in 
that satisfy the matrix differential equation

(our definition differs slightly from the usual one in the sign
convention). By construction, the eigenring  is a
-vector space of dimension at most 4, which contains the
-curvature . Then, we let  be its dimension over
; we will prove later on that  is in .

Let further  be the set of solutions of  in
 and let  be its dimension over .  Then,
our main results are the following.
\begin{theorem}\label{theo:main2}
  One can compute in time 
  \begin{enumerate}
  \item[1.] the dimensions  of 
    and ~of~;
  \item[2.] , if  or .
  \item[3.] , up to a multiplicative constant in  of
    degree at most , if  and the trace .
  \item[4.] a list of two candidates for , if  and
    .
\end{enumerate}        
\end{theorem}
\noindent The rest of this section is devoted to prove this theorem.

\smallskip\noindent{\bf The dimension of the eigenring.} The following
lemmas restrict the possible dimension  of .

\begin{Lemma}\label{lemma:4.a}
  If  has the form , then .
\end{Lemma}
\myproof In this case, the commutator of  in
 is  itself, so it has
dimension 4 over . Then,~\cite[Prop.~3.5]{Cluzeau04} implies
that  has dimension~4 over . \foorp
 
\begin{Lemma}\label{lemma:4}
  Either , or . In the second case,  is
  equal to , where  is the trace of
  .
\end{Lemma}
\myproof Corollary~1 of~\cite{Cluzeau04} shows that if the minimal and
characteristic polynomials of  coincide, then 
equals . In this case,  has
dimension 2 over .  Else, the minimal polynomial of 
must have degree 1, so  is necessarily equal to
, and we are under the assumptions of the previous
lemma. \foorp

\smallskip\noindent{\bf Computing  and .}
The equality~\eqref{eq:eigenring} gives a system of four linear
differential equations of order one for the entries
 of . An easy computation shows
that~\eqref{eq:eigenring} is equivalent to the system

where  belong to , and are given by 

 and

Since Equation~\eqref{eq:6} is equivalent to , we readily deduce that the dimension  
of  equals , where 
is the dimension of the solution-set of~\eqref{eq:3}.

Computing both  and  can be done using
Theorem~\ref{coro:solpol}, with respectively  or , and in
degree respectively at most  or . This proves point~1 of
Theorem~\ref{theo:main2}.

If , we are in the second case of Lemma~\ref{lemma:4}. Since
the trace can be computed in time  by
Theorem~\ref{theo:trace}, point~2 of Theorem~\ref{theo:main2} is
established in this case. If , then  is zero by
Lemma~\ref{lemma:pcurv-vs-ratsols}, so point~2 of
Theorem~\ref{theo:main2} is established as well.

\smallskip\noindent{\bf Eigenrings of dimension .}  The rest of
this section is devoted to analyze what happens if 
has dimension  over , so that the dimension
 of the solution-space of~\eqref{eq:3} is 1. In this case,
the information provided by the eigenring is not sufficient to
completely determine the -curvature. However, it is still possible
to recover some useful partial information. To fix notation,
we write the -curvature as 



\begin{Lemma}\label{lemma:dim1}
  If ,  is a nonzero polynomial solution of
  degree at most~ of Equation~\eqref{eq:3}.
\end{Lemma}
\myproof Since the -curvature  belongs to the eigenring, its
entries  satisfy~\eqref{eq:3}
to~\eqref{eq:6}. Lemma~\ref{lemma:1} shows that  is a
polynomial solution of degree at most~ of Equation~\eqref{eq:3}.
Moreover,  cannot be , since otherwise Equations~\eqref{eq:4}
to~\eqref{eq:6} would imply that  has the form 
for some  in . By Lemma~\ref{lemma:4.a}, this
would contradict the assumption  .  \foorp

\begin{Lemma}\label{lemma:dim2}
  Suppose that  and let  be the nontrivial
  polynomial solution of minimal degree of
  Equation~\eqref{eq:3}. There exists a nonzero polynomial  in
   of degree at most~, such that the entries of 
  are given by

\end{Lemma}
\myproof By Lemma~\ref{lemma:dim1}, the polynomials  and  both
satisfy Equation~\eqref{eq:3}; thus, they differ by an element  in
. Moreover, the minimality of the degree of  implies
that  actually belongs to  and has degree at most
.  The rest of the assertion follows from the
relations ,  and the
equalities~\eqref{eq:4} and~\eqref{eq:5}. \foorp

\smallskip\noindent{\bf Concluding the proof of Theorem~\ref{theo:main2}.}
To conclude, we consider two special cases. If , as
in~\cite{vanDerPut96}, the previous lemma shows that  is known
up to a multiplicative constant in , as soon as the
polynomial  has been computed. In this case,
Corollary~\ref{coro:solpol} shows that one can compute a non-zero
solution  of~\eqref{eq:3} in the required time. The minimal
degree solution  by clearing out the factor in  in 
using~\cite[Ex.~14.27]{GaGe99}, in negligible time , and the substitution in the former formulas
takes time  as well.

If ,  has a non-trivial polynomial solution, so by
Lemma~\ref{lemma:pcurv-vs-ratsols} the determinant of  is zero;
the additional equation , in
conjunction with the formulas in Proposition~\ref{lemma:dim1},
uniquely determines the polynomial  and thus leaves us with only
two possible candidates for .






\section{p-curvature: higher order}\label{sec:high}

\noindent In this final section, we study operators of higher order,
and we prove that the -curvature can be computed in time
subquadratic in . 
\begin{theorem}
  Given  in  of bidegree ,
  one can compute its -curvature in time

\end{theorem}
Hence, the exponent in  is ; in the best
possible case , we would obtain an exponent  in ,
unfortunately still not optimal.

As a result of independent interest, we also give an algorithm for
computing the image of a matrix of rational functions by an
differential operator similar in spirit to Brent and Kung's algorithm
for modular composition~\cite{BrKu78}; to our knowledge, no prior
non-trivial algorithm existed for this task.



\subsection{Preliminaries}

\smallskip\noindent{\bf Euler's operator.}
Besides operators in the usual variables , it will also be
convenient to consider operators in  or
, where  is
Euler's operator~, which satisfies the commutation rule
. To avoid confusion, we may say that  has
bidegree  {\em in } or {\em in }, if  is
written respectively on the bases  or .

\smallskip\noindent{\bf Conversion.}
Given an operator  in  of bidegree ,  can be rewritten as an
operator in  with polynomial coefficients. The operator 
has bidegree  in . By~\cite[Section~3.3]{BoChLe08},
computing the coefficients of  takes time . Since representing all coefficients of 
requires  elements, this is quasi-linear, up to
logarithmic factors.

\smallskip\noindent{\bf Multiplication.} Next, we give an algorithm
for the multiplication of operators with rational coefficients of a
special type, inspired by that of~\cite{BoChLe08} (which handles
polynomial coefficients). The algorithm relies on an evaluation /
interpolation idea originally due to~\cite{vanDerHoeven02}, and 
introduces fast matrix multiplication to solve the problem.

\begin{Lemma}\label{lemma:mul}
  Let  be of degree at most , with  and
  let  be in , with
   where 
  and  have degrees at most . Then if , one can compute  in time .
\end{Lemma}
\myproof Define 
and . A quick verification shows that these
operators are in , of respective
bidegrees bounded by ,  and .

We first compute  and  for . This is done by computing the corresponding values of
 and , and dividing the results by . The
former computation takes time  using algorithm {\sf Eval}
of~\cite{BoChLe08}; the latter  by Newton iteration for
power series division. Our assumption  ensures that
divisions performed in the evaluation algorithm (and in the
interpolation below) are well-defined.

From the values of  and , the values  are obtained as in~\cite[Th. 3]{BoChLe08}; the cost is
. We can then compute the values of  in
time  by fast polynomial multiplication.  Knowing its
values, we recover  using algorithm {\sf Interpol}
of~\cite{BoChLe08}; this takes time . Finally, we deduce
 by division by ; this takes time ,
using fast gcd computation.  \foorp




\smallskip\noindent{\bf Left and right forms.} Let  have the form

with  of degrees at most . It
can be rewritten

with  in  of degrees at most 
as well. The former expression will be called the {\em right-form} of
; the latter is its {\em left-form}. 

\begin{Lemma}\label{lemma:leftright}
  Let  have bidegree  in , given in its right-form (resp. in its left-form). Then
  \sloppy one can compute its left-form (resp. right-form) in time
  .
\end{Lemma}
\myproof We prove one direction only; the other is similar. Given the
right-form of , we can (without performing any operation) rewrite
 where  has
constant coefficients and order at most .  Since , the result follows by using algorithms for
polynomial shift by ~\cite{GaGe97}.  \foorp

\smallskip\noindent The number of elements needed to represent  in
either left- or right-form is , so the previous algorithm is
quasi-linear, up to logarithmic factors.




\subsection{Evaluation}

\noindent For  in  or  and 
in ,  denotes the matrix in
 obtained by applying  to . In this
subsection, we give cost estimates on the computation of .

\smallskip\noindent{\bf The polynomial case.} We start with the case
of an operator with polynomial coefficients, which we apply to a
matrix with polynomial entries. We use an operator in , since
this makes operations slightly more convenient than in . As
in Section~\ref{sec:solpol}, we make assumptions on the relative sizes
of the input parameters (here ), for
simplicity's sake. 
\begin{Lemma}\label{Prop:2}
  Given  of
  bidegree  and  of
  degree , one can compute  in time
  ,
  assuming  and .
\end{Lemma}
The cost can be rewritten as . Since  and , this is always better than : the cost ranges from 
for a hypothetical  to  for . As a matter of comparison, let us write
Computing 
naively amounts to computing all  for ,
multiplying them by the respective coefficients , and summing
the results; the cost is in  so
our estimate is better. 

\smallskip\noindent\myproof Our result is achieved using a baby
steps~/~giant steps strategy inspired by Brent-Kung's algorithm for
power series composition~\cite{BrKu78}. Let  and . First, we rewrite 
in left-form, as
 
by Lemma~\ref{lemma:leftright}, the cost is .
Next,  is cut into  slices of the form

Each  has order less than  and can be written as
 
where for ,   is
zero. Finally, we rewrite each  in right-form:

where all  have degree at most . By
Lemma~\ref{lemma:leftright}, the cost is , which is in  as before.  
To apply
 to , we first compute the baby steps

then, we deduce all , for ; finally, we do the giant
steps

All  can be computed in time , by successive applications of . The cost  of deducing
the polynomials  is detailed below. Finally, one recovers  by first computing all , for , and then
summing them. Since ,  can be
applied to  in time , so the
total cost of this final step is .

It remains to compute all , given all ; we compute
them all at once. In view of Equation~\eqref{eq:L''}, we have

where the  are known. We cut  into slices of length :
 
where  has degree less than  and . This gives

We will compute all inner sums

at once, for  and ; from this, one can recover all
 in time .

The computation of these sums amounts to perform a  matrix multiplication, with entries that are polynomial
matrices of size  and degree at most .  Since , we have . Hence, we
divide the previous matrices into blocks of size  and we are left
to do a  product of such blocks, where
 is lower-bounded by a constant.  Multiplying a single
block takes time , so the total time
 is , which is 
.

The conclusion of Lemma~\ref{Prop:2} comes after a few
simplifications, which shows that the dominant cost is , for the
final linear algebra step. \foorp

\smallskip\noindent{\bf The rational function case.} Next, we study
the application of an operator to a matrix of rational functions~
(we make some simplifying assumptions on the denominators in~,
which will be satisfied in the cases in \S 6.3 where we apply this
result). Besides, our operator is now in
 rather than in
.

Because of the larger number of parameters appearing in the
construction, the cost estimate unfortunately becomes more complex than
in the polynomial case.
\begin{Lemma}\label{prop:eval}
  Let  be of
  bidegree . Let  be of
  the form , with  of degree at most 
  and  of degree at most . Define
   If  and , one can compute  in time 
  .
\end{Lemma}
\myproof Let . Given  as an operator in , we
saw that we can write  as an operator in , of bidegree
; the coefficients of  in  can be computed
in time . To conclude, it is enough to compute , since then  is deduced by a division by , which is free.

For any ,  has the form ,
with  in  of degree at most
. Thus,  has the form
, with  of degree less than , with
.

Knowing , one can recover the numerator
matrix  through multiplication by ; a gcd
computation finally gives  in normal form. These latter steps
take time .

Since , the matrix  is
well-defined; it can be computed in time  by power series
division. Lemma~\ref{Prop:2} gives complexity estimates for computing
. Since this matrix coincides with 
modulo , this concludes the proof of the lemma, as all
previous costs are negligible compared to the one of
Lemma~\ref{Prop:2}. \foorp



\subsection{Computing the p-curvature}
\noindent Let  be in  of bidegree
 and let  be its companion matrix. We define the operator
 as
 thus, as pointed out in the introduction,
the -curvature of  is obtained by applying  to
.

To obtain a cost better than , we first compute a high enough
power  of ; then, we apply  to
  times, with . Since  may not factor
exactly as , a few iterations of this process are needed.

\smallskip\noindent{\bf Computing .} Let  be the leading coefficient of . Then,  has the
form , with  in
 and  of degree at most 
and . More generally, for , we can write
 as

and  in  of degree at most .  

\begin{Lemma}\label{lemma:powerK}
  If , one can compute  in time .
\end{Lemma}
\myproof We use a divide-and-conquer scheme. Let ; we assume for simplicity that ; if  is odd, an
extra (cheaper) multiplication by  is needed. We assume that
 is known, and we see it as an  matrix with
entries that are scalar operators; hence, to compute , we
do  products of such scalar operators. All these products
have the form  of the form seen in
Lemma~\ref{lemma:mul}, so each of their costs is .
\foorp

\smallskip\noindent{\bf Computing .}  We fix ,
and we compute the operators  and . Writing ,
we compute the sequence
 so that . Thus, we have
, where the latter matrix is defined in
Equation~\eqref{eq:pcurv}. Using the subroutines seen before, a quick
analysis not reproduced here shows that the optimal choice is
. Then, computing  takes time
 by
Lemma~\ref{lemma:powerK}.

By Lemma~\ref{lemma:1}, each matrix  has the form
, with  of
degree at most . Given , we compute  by
first applying  to  and dividing the result by
. 

The first step, applying , is the more costly. We obtain its
cost by applying Lemma~\ref{prop:eval}, with 
and . Then, we have  and . For all , we are under the assumptions of that
lemma; after a few simplifications, the cost becomes
. Summing over all ,
we obtain an overall cost of .  Taking into account that  and , this finally gives a cost of  for computing .

\smallskip\noindent{\bf Computing the -curvature.} The definitions
of  imply that . To obtain the
-curvature , we iterate the previous process,
replacing the required number of steps  by , until the
required number of steps is . Since , it
takes  iterations; hence, the overall time is still in
.

\smallskip\noindent{\bf Acknowledgments.} We wish to acknowledge financial
support from the French National Agency for Research (ANR Project ``Gecko"),
the joint Inria-Microsoft Research Centre, NSERC and the Canada Research Chair
program.

\smallskip
\bibliographystyle{abbrv}
\bibliography{curvature}

\end{document}
