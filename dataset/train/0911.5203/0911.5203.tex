\chapter{An Abstract Machine and Processing Model}\label{chp:compile}
We are interested in this thesis in a compilation-based model for
realizing $\lp$. One possible target for a compiler that emerges from
our considerations could be the instruction set for standard
hardware. This is, in fact, the usual choice for conventional
languages. However, the distance between typical machine architectures
and the computational model for $\lp$ is too significant to bridge in
one step. Moreover, these differences make it difficult to visualize
and to state precisely the optimizations that can be performed on
particular instruction sequences that a compiler might generate. For
this reason, we introduce an intermediate level ``abstract machine''
for $\lp$. We describe the structure of this abstract machine in this
chapter, also interleaving with this description a presentation of the
process of compiling $\lambda$Prolog programs into instructions for
this machine. Our abstract machine will inherit its basic structure
from the developments related to compiling Prolog programs that have
resulted in the abstract machine designed by Warren for that language
\cite{Warren83WAM}. We will also make use of a previous machine
designed by Nadathur and colleagues for $\lp$ \cite{KNW94cl,
  N03treatment, NJK95lp} that underlies the {\em Version 1} of the {\em Teyjus}
implementation of this language \cite{NM99cade}. However, unlike this
earlier implementation that tackled full higher-order unification
using Huet's procedure, we will exploit the possibility of using the
higher-order pattern unification algorithm described in
Chapter~\ref{chp:interpreter}.  This
choice simplifies the structure of the abstract machine considerably,
leads to optimizations in the treatment of types as we discuss in the
next chapter and also has the potential for impacting the overall
runtime performance on $\lp$ programs.

This chapter is organized as follows. We introduce the basic
processing model underlying the new abstract machine in
Section~\ref{sec:basic_model}; as mentioned already, this model is
based on the Warren Abstract Machine (WAM) for Prolog
\cite{Warren83WAM}, with which we shall assume that the reader to have
some familiarity.
Section~\ref{sec:ho_control} and Section~\ref{sec:ho_unif}
then discuss the details of the enhancements to this model that are
needed for handling the higher-order features of $\lp$.
Specifically, Section~\ref{sec:ho_control} addresses the treatment
of generic and augment goals, and Section~\ref{sec:ho_unif}
discusses how the higher-order pattern unification is embedded
into the overall processing.
A complete example of a compiled $\lp$ program is presented in
Section~\ref{sec:inst_exp}.
Section~\ref{sec:misc} sketches the treatments to flexible and disjunctive
goals. Significant aspects of the treatment of generic and augment
goals and almost all of the treatment of flexible and disjunctive
goals are inherited from the earlier abstract machine for $\lp$ but
their presentation is, nevertheless, needed here for the sake of
completeness.

Our focus in this chapter will be on
the {\em conceptual structure} of the new abstract machine and the
processing model embodied by it.
This design has been realized in an actual implementation of
$\lp$---{\em Version 2} of the {\it Teyjus} system---a presentation
  of which appears in Chapter~\ref{chp:system}.

\section{The Processing Model}\label{sec:basic_model}
The WAM provides a basic framework for compiling the aspects of
control and unification that are part of the computation in {\em
  Prolog}-like languages. These aspects appear also in $\lp$ and so we
can use this structure in our abstract machine as well.
In this context, we note that the compilation of control refers to the
translation of the dynamic analysis of the structure of complex goals
carried out by the abstract interpreter described in
Chapter~\ref{chp:interpreter} into low-level abstract machine
instructions. The compilation of unification, on the other hand,
corresponds to using knowledge of one half of the disagreement
pairs to reduce the amount of work that needs to be done at
runtime. Specifically, this translates into generating instructions for
analyzing the structure of terms that arrive in argument registers
when attempting to match with the head of a clause and for
correspondingly setting up the argument registers when calling
predicates.


The basic WAM model is enhanced in our implementation in order to
support the richer set of features present in $\lp$.
First, our compilation treatment of control computations should include that
of generic and augment goals in addition to the set of goal structures
contained in the Horn clauses that underlie {\em Prolog}.
Second, in comparison to first-order setting, the unification
operation of interest to us deals with a richer term structure and involves a
more complicated notion of equality. To accommodate this, we make the
following additions. To treat the richer equality notion, we utilize
invocations to a head normalization procedure at relevant points in
the computation. Then we partition the unification computation into
first-order and higher-order parts, so that the former can be handled by
(compiled) WAM style instructions, and the latter by an auxiliary
interpretive procedure that is based on the higher-order pattern
unification algorithm. Notice that this partitioning is something that
must happen dynamically because whether the unification problem is a
first-order or a higher-order one depends also on a term whose
structure is known only at runtime. To deal with this, we build
appropriate machinery into the abstract machine instruction
set that is responsible for recognizing and delaying the higher-order
parts of unification, and for invoking the interpretive phase of
unification at chosen computation points. Further, we provide
devices for delaying the unification problems that are recognized to
be beyond the $\Ll$ subset during the interpretive phase and for
carrying them across goal invocations, to be re-examined when variable
bindings may have altered their status.

The details of the additional support that is summarized above are
presented in the next two sections.
The rest of the discussion in this section provides a sketch of the memory
structure of our abstract machine and the underlying processing model
that will be needed in order to explain these details.

The basic data areas in our abstract machine consist of a code area,
a heap, a stack, a collection
of registers, a push down list (PDL) and a trail.
The first four categories of data areas are familiar from conventional
machine architectures although some of them have different actual
purposes in our setting.
The code area contains the compiled forms of clauses that constitute
the definitions of predicates.
The heap is a global memory space for holding data
that is accessible at any point of computation; specifically, this is
where complex terms that survive after the successful completion of a
goal must be placed. One of the uses of the stack, that is similar to
the use made of it in conventional languages, is to record environment
frames for calls to particular clauses that constitute the definition
of a predicate; such frames will store register images and other
relevant data that need to be maintained between calls to goals that
are part of the body of the clause in question.
The stack is also used to store information for handling
nondeterminism, a feature that is peculiar
to logic programming languages.
In particular, when alternatives are available during clause selection,
the contents of relevant registers should be saved,
so that the execution context can be recovered when it is necessary to
attempt a different clause choice, \ie, when backtracking
occurs. Such information is maintained
in structures called  choice points, which are interleaved with
environment frames on the stack.
In our abstract machine, the
stack is also used to maintain information that is needed to support
augment goals. We defer discussion of this usage till the next
section.
Registers are of two kinds: those that store data and those that
are needed for execution control.
Examples of the former include a set of data registers, $A_1$, ..., $A_n$ that
are used for passing arguments across calls to clause definitions, and
the $S$ register that points to the next argument of a complex
first-order term (which is an application with a rigid head).
The set of registers relevant to execution control consist of the program
pointer, {\it P}, the continuation pointer, {\it CP}, the top of the
heap register,
{\it H}, the most recent environment frame register,
{\it E}, the most recent choice point register, {\it B} and the top of
the trail register, {\it TR}.
Both sets of registers will be enriched to support higher-order
features, as we discuss in the next two sections.
The push down list, PDL, is used within the interpretive
unification process for recording the subproblems that are created by
the process of term simplification discussed in
Section~\ref{chp:interpreter}.
The trail area is also used to
assist the branching behavior, which records images of the fragments
of the heap and stack that need to be recovered upon backtracking.

\begin{figure}\footnotesize
\begin{tabbing}
\quad\= c1\ \ : \=\{\ \=Set up a choice point on the top of stack and record that the next candidate clause \ \ \ \=\}\kill
\>{\it /*  copy a a. */}                                                                                     \\
\> {\it C1}:  \>\{  \>Set up a choice point on the top of stack and record that the next candidate clause       \\
\>            \>    \>is available at {\it C2}.                                                            \>\} \\
\> {\it L1}:  \>\{  \>Unify arguments of the incoming goal with those of the clause head.                  \>\} \\
\>            \>\{  \>Return by continuing from the continuation point.                                    \>\} \\
\>{\it /*  copy (app T1 T2) (app T3 T4)} $\pif$ {\it copy T1 T3, copy T2 T4. */}                             \\
\> {\it C2}:  \>\{ \>Recover relevant registers from the information in the latest choice point on the stack,      \\
\>            \>   \>update the choice point and record that the next candidate clasue is available at {\it C3}. \>\} \\
\> {\it L2}:  \>\{ \>Set up an environment frame on the top of the stack.                                        \>\} \\
\>            \>\{ \>Unify arguments of the incoming goal with those of the clause head.                         \>\} \\
\>            \>\{ \>Set up arguments for {\it copy T1 T3}.                                                      \>\} \\
\>            \>\{ \>Shrink the environment frame, update the continuation point to the next instruction              \\
\>            \>   \>and call {\it copy}.                                                                        \>\} \\
\>            \>\{ \>Set up arguments for {\it copy T2 T4}.                                                      \>\} \\
\>            \>\{ \>Remove the latest environment frame from the stack.                                            \>\} \\
\>            \>\{ \>Call {\it copy}.                                                                            \>\} \\
\> {\it /* copy (abs T1) (abs T2)} $\pif$ $Pi\ c\plam \ (copy\ c\ c\ \pimp \ copy\ (T1\ c)\ (T2\ c))$. {\it */} \\
\> {\it C3}:  \>\{ \>Recover relevant registers from the information in the latest choice point on the stack,        \\
\>            \>  \>and remove the choice point.                                                             \>\} \\
\> {\it L3}:  \>\{ \>Set up an environment frame on the top of the stack.                                    \>\} \\
\>            \>\{ \>Unify arguments of the incoming goal with those of the clause head.                     \>\} \\
\> {\it H1}:  \>\{ \>Carry out control actions for entering a generic goal and then an augment goal.         \>\} \\
\>            \>\{ \>Set up arguments for {\it copy (T1 c) (T2 c)}.                                          \>\} \\
\>            \>\{ \>Shrink the environment frame, update the continuation point to the next instruction          \\
\>            \>   \>and call {\it copy}.                                                                    \>\} \\
\> {\it H2}:  \>\{ \>Carry out control actions for leaving an augment goal and then a generic goal.          \>\} \\
\>            \>\{ \>Remove the latest environment frame from the stack.                                        \>\} \\
\>            \>\{ \>Return by continuing from the continuation point.                                       \>\}
\end{tabbing}
\caption{Compiled computations underlying the program {\it
copy}.}\label{fig:copy_proc_model}
\end{figure}


Computations occur within our abstract machine from executing a sequence
of instructions that are generated from compiling goals, which
correspond to the user query or the bodies of clauses, or from
compiling the selection of a clause for a predicate and the subsequent
unification with the head of the clause.
The compilation of a goal
is organized as follows. First, instructions are generated to realize
the processing of the logical symbols that appear in a complex
goal. Eventually, an atomic goal is reached. At this point,
instructions are produced to set up the arguments of the
goal in the argument registers; if these arguments are complex
terms or variables, they will reside in either the heap or in the
environment frame and the relevant registers will contain references
to these structures. The last instruction for the atomic goal will be
a call to the code for the predicate in question.
Apart from transferring control to the (next) relevant clause for a
predicate, the code for clause selection has the responsibility of
setting up a choice point in the stack to represent the remaining
alternatives.
The first action that the code for unification with the head of a
selected clause must do is set up an environment frame if one is
needed. The remaining instructions are responsible for carrying out
the needed unification between the arguments appearing in the clause
head and the ones passed in the argument registers from the invocation
of the atomic goal. If this unification is successful, computation
passes to the instructions arising from the compilation of the goal
constituting the clause body, whose treatment we have already
described.
If this goal is solved successfully, then computation must return to
the caller and the last instruction for the clause body will have the
effect of realizing this. Notice that the environment frame that was
created for this clause can be released at this point provided it is
not needed for backtracking, in which case it will be protected by a
choice point that appears above it in the stack.
Of course, failure can occur in the course of unification with the
head of a clause. This triggers a backtracking procedure whose first
task is to carry out a resetting of the heap and stack state to what
it was at the current most recent choice point. The information for
such a resetting is stored in the trail and, hence, this process
is referred to as the ``unwinding of the trail.''  Once this is done,
the relevant registers are restored from the information available
from the most recent choice point and computation proceeds
to the next clause definition (also recorded with the choice point),
after updating or discarding the choice point itself depending on
whether or not further alternatives are available.


The control computations are optimized in a manner similar to that
in the WAM within our abstract machine as well.
First, upon making a call, the environment frame of the caller is dynamically
shrunk by discarding permanent variables whose binding information are no
longer needed for solving the goals in the clause body remained to be
processed; this process is referred to commonly as ``environment trimming.''
Second, when a clause body is constructed from a sequence of conjunctions and
the last conjunct is atomic, last call optimization~\cite{Warren80} is
performed.
Essentially, the caller's environment frame is deallocated from the stack
before computation actually proceeds to the callee, and the call is carried
out after setting the continuation point register to the continuation
point passed to the caller, so that the callee can directly
returns to its grand parent in the call graph. This optimization
subsumes the traditional tail recursion optimization in the logic
programming setting.

\begin{figure}\footnotesize
\begin{tabbing}
\quad\= {\it copy}\ : \=\{\ \=\dquad\=variable:\dquad\dquad\= continue with the instruction at xxxxxxxxxxxxxxxxxxxxxxxx \=\}\kill
\>      {\it copy}\ : \>\{  \>Switch on the head of the (head normal form of) the first actual argument of {\it copy}: \\
\>                    \>    \>      \>variable:        \> continue with the instruction at {\it C1}.                   \\
\>                    \>    \>      \>de Bruijn index: \> continue with the instruction at {\it C1}.                   \\
\>                    \>    \>      \>constant:        \> continue with the instruction at {\it S}.       \>\}         \\
\>      {\it S}\ :    \>\{  \>Switch on the given constant:                                                            \\
\>                    \>    \>      \>{\it a}\ :       \> continue with the instruction at {\it L1}.                   \\
\>                    \>    \>      \>{\it app}\ :     \> continue with the instruction at {\it L2}.                   \\
\>                    \>    \>      \>{\it abs}\ :     \> continue with the instruction at {\it L3}.      \>\}         \\
\>      {\it C1}\ :   \>...
\end{tabbing}
\caption{Indexing on {\it copy}.}\label{fig:copy_index}
\end{figure}

We illustrate the compilation model and the associated processing
scheme that we have described relative to the simple $\lp$ program
appearing in Figure~\ref{fig:copy} that defines the {\it copy}
predicate. A high-level pseudo code description of the compiled
program in our implementation is contained in
Figure~\ref{fig:copy_proc_model}.

A final aspect to be mentioned with regard to the compilation model is
the optimization corresponding to the detection of determinism.
The runtime treatment of nondeterminism involves the manipulation of choice
points that is known to be costly and can often be eliminated by
utilizing the structure of actual arguments of atomic goals to prune choices
early during execution. For this purpose, a special set of instructions are
included that allow clause choices to be indexed by the head arguments.
Taking the {\it copy} example, instructions in Figure~\ref{fig:copy_index} can
be added to those in Figure~\ref{fig:copy_proc_model} for the purpose of
indexing.

\section{Compiling the New Search Primitives in $\lambda$Prolog}\label{sec:ho_control}
We now consider the extensions to the basic processing model to deal
with generic and augment goals. Our discussion only sketches these
extensions to the extent needed for a complete description of our
abstract machine. A more thorough treatment may be found in
~\cite{NJK95lp}.

As described in the previous chapters, the presence of generic goals
requires a more careful treatment of unification. More specifically,
to deal with the scoping effect of such goals on names, universe
levels are associated with constants and logic variables and are
examined and adjusted by the unification process. The determination of
the appropriate universe level in our abstract machine is based on a
global universe counter, which starts from $0$ on the top-level query,
and is increased or  decreased upon entering or leaving each generic
goal. This global universe counter is maintained in a new
register called {\it UC}. This register is incremented and decremented
by two new instructions, {\it incr\_universe} and {\it decr\_universe},
respectively.
Some of the actions in the WAM based model are also modified to
facilitate the proper manipulation of the universe counter.
The contents of the {\it UC} register is stored
in choice points so that this register can be restored upon
backtracking. These contents are also recorded in environment frames;
the instructions that create terms corresponding to the arguments of
atomic goals appearing in the body of a clause and possibly embedded
within generic goals may need the old value in this register for
tagging variables that are bound by the implicit quantifiers at the
clause level.

It is necessary also to deal with the direct effects of a generic
goal: such a goal must give rise to a new constant that is tagged with
the (incremented) value of the UC register and that must then be
substituted in the body of the goal for the quantifier variable. In
our abstract machine, we deal with these requirements by assigning a
slot in the environment frame to the quantified variable---thereby
treating it as a {\em permanent} variable in WAM terminology---and by
storing the appropriate constant in this slot. These actions are
carried out by a new instruction called {\it set\_univ\_tag}: as
expected, this instruction takes as operands a displacement in the
environment frame and a constant.
As a concrete example of the design above, the pseudo instructions
from label {\it H1} to {\it H2} in Figure~\ref{fig:copy_proc_model} that
corresponds to the generic goal
$Pi\ c\plam \ (copy\ c\ c\ \pimp \ copy\ (T1\ c)\ (T2\ c))$
can take the following structure.
\begin{tabbing}
\quad\= c1\ \ : \=\{\ \=Set up a choice point on the top of stack and record ssssss\ \ \ \=\}\kill
\> {\it H1}\ :  \>\{ \>{\it incr\_universe}                                                                          \>\} \\
\>              \>\{ \>{\it set\_univ\_tag} $<$offset to the environment frame$>$, $c$                               \>\} \\
\> {\it H3}\ :  \>\{ \>Carry out control actions for entering an augment goal.                                       \>\} \\
\>              \>\{ \>Instructions for $copy\ (T1\ c)\ (T2\ c)$.                                                    \>\} \\
\> {\it H4}\ :  \>\{ \>Carry out control actions for leaving an augment goal.                                        \>\} \\
\> {\it H2}\ :  \>\{ \>{\it decr\_universe}                                                                          \>\}
\end{tabbing}

Goals in $\lambda$Prolog could also have the form
$(Sigma\ x\plam\ G)$, \ie, they could be explicitly existentially
quantified. Such goals may be permitted in Prolog too, but, because of
the simple syntactic structure of goals in that setting, in
particular, the absence of generic goals, such goals can be treated
statically by moving the existential quantifiers out into universal
ones over the entire clause and can then be treated via standard
techniques. In our case, we can almost use the same scheme. There is,
however, one exception: the particular location of the existential
quantifier may have an impact on what universe index is to be
stored with the variable. To accommodate this, we add a further
instruction that is called {\em tag\_exists} to our abstract
machine. This instruction takes a variable, which is eventually a stack
or heap location, as an argument and sets its universe index to the
value currently in the {\em UC} register.

The semantics of an augment goal $D\pimp G$ require the addition of $D$
to the existing set of program clauses before the processing of $G$,
and the retraction of these added clauses upon the successful
solution of $G$.
The searching mechanism used for clause selection has to therefore
support dynamic modifications to the available predicate definitions.
To realize this, a memory component called an {\it implication point}
is introduced. These implication points are stored on the stack and a
new register, the $I$ register, is introduced to record the most
recent implication point. Each implication point also records the most
recent implication point at the time of its creation; in other words,
the sequence of implication points themselves form a stack. Suppose
that $D$ provides (additional) clauses for the predicates
$\{p_1,\ldots,p_n\}$. Then one of the components contained in the
implication point corresponding to the addition of $D$ is a search
table that will ultimately yield a pointer to the compiled form of the
code for each of these predicates. If no entry is found for a
particular predicate when searching from this implication point, the
search continues from the implication point that this one points to;
thus, the overall program context existing at any stage of computation
is completely defined by the contents of the $I$ register.  The
implication point also contains a {\em next clause} table of size $n$
that provides pointers to the definition (or code) for each of the
predicates $p_1,\ldots,p_n$ that existed at the time of its creation
paired with the implication point that corresponds to this
definition. This table complements a special instruction called {\it
  trust\_ext} to complete the compiled form of the code for the
predicates $p_1,\ldots,p_n$ as we describe later. Notice that the
right next clause table to use is determined by the implication point
that added the code currently being tried for the relevant
predicate. To isolate this implication point, we add to the abstract
machine yet another register called {\it CI}.

Two new instructions are introduced to support the compilation of an
augment goal. The {\it push\_impl\_point} instruction is used upon
entering an augment goal for the creation of an implication
point. This instruction is also responsible for setting up the next
clause table for the implication point, something that is done by
searching the program context given by the current contents of the $I$
register for definitions for each of the relevant predicate. The {\it
  push\_impl\_point} instruction takes as argument a pointer to a
compile-time prepared table that contains information about the
predicates for which code is being added and also pointers to the
specific code that needs to be included.
Symmetrically, the instruction {\it pop\_impl\_point} serves to remove
the latest implication point from the stack upon leaving an augment
goal. This action is carried out simply by setting the {\it I}
register to the implication point reference stored in the one that
this register currently points to.
Considering the {\it copy} example, now the pseudo instructions labeled from
{\it H3} to {\it H4} that correspond to the augment goal
$(copy\ c\ c\ \pimp \ copy\ (T1\ c)\ (T2\ c))$
can take the following form.
\begin{tabbing}
\quad\= c1\ \ : \=\{\ \=Set up a choice point on the top of stack and record ssssss\ \ \ \=\}\kill
\> {\it H3}\ : \>\{ \>{\it push\_impl\_point t}                                                                    \>\} \\
\>             \>\{ \>Instructions for $copy\ (T1\ c)\ (T2\ c)$.                                                 \>\} \\
\> {\it H4}\ : \>\{ \>{\it pop\_impl\_point}                                                                     \>\}
\end{tabbing}
We assume above that {\it t} is a pointer to a table prepared for the
addition of the clause $copy\ c\ c$ to the existing collection of
predicate definitions.

Code that is added dynamically for a predicate must allow for the
possibility that it is extending an already existing definition. To
support this situation, the code that is normally generated from the
clauses for the predicate is enclosed within a
{\it try\_me\_else} and a {\it trust\_ext} instruction. The leading
{\it try\_me\_else} sets up a choice point with the indication that
the alternative definition starts from the {\it trust\_ext}
instruction at the end of this segment of code.
The {\it trust\_ext} instruction takes as argument an index into a
next clause table.
The {\it trust\_ext} instruction first retrieves a pointer to the next
clause to try for the predicate from the next clause table stored in
the implication point referenced by the {\it CI} register and it
resets this register to the associated implication point also obtained
from this table. It then transforms the rest of the computational
context as needed for backtracking by using the contents of the
current choice point, which it then discards.

A subtle but important point to be noticed about the clauses that
appear in augment goals is that these may contain free variables in
them. For example, consider the following generic goal that appears in
one of the clauses for the {\it copy} predicate:
\begin{tabbing}
\qquad\=\kill
\>$Pi\ c\plam \ (copy\ c\ c\ \pimp \ copy\ (T1\ c)\ (T2\ c))$
\end{tabbing}
Recall that the quantified variable $c$ is treated as a variable for
which space is allocated in the environment record for the parent copy
clause. Further, the processing of the universal quantifier results in
a constant (with appropriate universe index) being bound to this
variable. When interpreting the embedded clause $copy\ c\ c$,
therefore, it is important to have available the environment record of
the parent clause in order to interpret the ``variable'' corresponding
to the occurrences of $c$. In short, we treat clauses as closures, to
be interpreted relative to an environment that is pointed to by a
special register called {\it CE}. Use is made of a new instruction
called {\it init\_variable} whenever it is necessary to get the
binding for a variable from the ``parent'' environment. This
instruction takes two arguments: a register or an environment slot
designating the location of the variable local to the clause being
considered and the environment slot for the parent clause from which
the binding must be obtained. The instruction uses its two arguments
to tie these two variables together.

As an illustration of the discussion of the compilation of embedded
clauses, the clause {\it copy c c} that occurs within the generic goal
just considered would be compiled into the following sequence of
(pseudo-)instructions:
\begin{tabbing}
\quad\= c1\ \ : \=\{\ \=Set up a choice point on the top of stack and record ssssss\ \ \ \=\}\kill
\> {\it D1}\ :  \>\{ \>{\it try\_me\_else} {\it D2}                                            \>\} \\
\>              \>\{ \>{\it init\_variable} $\langle${\it local location of
c}$\rangle$, {\it Yi} \>\} \\
\>              \>\{ \>Code for unifying first two argument registers \\
\>              \>   \> with variable denoting $c$ local to this environment.                                      \>\} \\
\>              \>\{ \>Return control to the continuation point.  \>\} \\
\> {\it D2}\ :  \>\{ \>{\it trust\_ext} {\it 1}                                             \>\}

\end{tabbing}
Here {\it Yi} denotes the location of the slot assigned to the
universally quantified variable corresponding to $c$ in the
environment record pointed to by the {\it CE} register. It is, of
course, necessary to set this register appropriately for each clause
that is being tried. To facilitate this, a pointer to the relevant
environment record is stored in the implication point at the time that
it is set up. Notice also that the index for the {\it trust\_ext}
instruction here is {\it 1} because there is code for exactly one
predicate that is added by the associated augment goal.


A final point concerns the instructions for invoking the code
for predicates. As we have noted in this section, the entry
point into such code can change during execution. For this reason, we
need a special set of calling instructions that will
initiate the search for appropriate code from the implication point
referenced by the {\it I} register. These instructions will,
for instance, have to be used for any calls to the {\it copy}
predicate whose compilation we have just considered.
Note, however, that the old WAM style calling instructions are also
retained in our abstract machine. These can be used for predicates
whose code cannot be altered dynamically. Moreover, it is preferable
to use them wherever possible because the address to which control
needs to be transferred then does not need to be calculated at
runtime.

\section{Compilation of Higher-Order Pattern Unification}\label{sec:ho_unif}
We now turn our attention to providing support for higher-order
pattern unification. We first consider extensions for this purpose to
the data areas present in the original structure of the WAM.
These extensions are of two kinds: the introduction of new devices and
enhancements and modification to the ones already present in the WAM.
The specifics of these changes are as follows. First, we add new
registers called {\it Head}, {\it
  ArgVector}, {\it NumArgs} and {\it NumAbs} that provide access to
the head, the arguments, the number of arguments and the binder length
of a head-normal form right after it has been computed.
Second, in addition to the role it plays in realizing the interpretive
unification process,
the PDL is also used to temporarily maintain higher-order unification
problems that are delayed when executing the compiled form of
unification arising from matching with the clause head.
Third, unification problems that lie outside the $\Ll$ subset need to
be carried as constraints across goal invocations and the heap is used
to maintain such problems in the form of a list of disagreement
pairs. The beginning of this list is recorded in a new register called
{\it LL}.
The heap is further used to store the terms that are created in the
course of head-normalization and in the binding phase of pattern
unification.
In the intended scheme, $\beta$-contractions are carried out
destructively during head-normalization so as to share
the effects of such rewriting steps. Since it may be necessary to undo
these mutations on backtracking, we also change the trail so that it
additionally maintains a record of any such mutations that arise
during processing.

As mentioned in Section~\ref{sec:basic_model}, the unification
on the arguments of a clause essentially consists of a
first-order and a higher-order part,
whereas WAM style instructions
for unification are only sufficient in handling the former.
Our abstract machine still uses the WAM
style instructions to solve the first-order subproblems, and delays the
higher-order ones by pushing them onto the PDL. The problems left on
the PDL in this way are examined by an interpretive pattern
unification procedure that is invoked as the culminating instruction
in the sequence that realizes unification with the clause head.
The structure of the unification part of the processing model can thus
be described schematically as follows:
\begin{tabbing}
\quad\= \{\ \ \=  For \=each argument in the clause head       \\
\>       \>         \>\{ \ \=Instructions for carrying out the first-order part unification and  \\
\>       \>         \>     \>postponing the higher-order part onto the PDL.     \\
\>       \>         \>\} \\
\>       \> Invoke the interpretive pattern unification procedure on the PDL. \\
\> \}
\end{tabbing}



\begin{figure}\footnotesize
\begin{tabbing}
\dquad {\it unify}\=\ $(t,\ ${\boldmath $s$}$)$  \\
                  \> switch on the structure of $t$\ : \\
                  \> case \=$\lambdadb(n, t')$\ : \\
                  \>      \>{\bf create {\boldmath $t$} on the heap}  \\
                  \>      \>{\bf interp\_unify({\boldmath $t,\ s$})} \\
                  \> case \>$(F\ a_1\ ...\ a_n)$, where $F$ is a variable and $n > 0$\ : \\
                  \>      \> let \= $t'$ be a term of form   \\
                  \>      \>     \> $(F\ a_1\ ...\ a_n)$, where $F$ is a new logic variable, \\
                  \>      \>     \> if this is the first occurrence of the variable in the clause;\\
                  \>      \>     \> $(f\ a_1\ ...\ a_n)$, where $f$ is the term to which the variable $F$ is bound, \\
                  \>      \>     \> if this is the subsequent occurrence of the variable in the clause. \\
                  \>      \>{\bf create {\boldmath $t'$} on the heap} \\
                  \>      \>{\bf interp\_unify({\boldmath $t,\ s$})} \\
                  \> case \>$X$, where $X$ is a variable\ : \\
                  \>      \>if this is the first occurrence of $X$ in the clause, then {\bf bind({\boldmath $X,\ s$})}. \\
                  \>      \>else {\bf interp\_unify({\boldmath $t',\ s$})}, where $t'$ is the term to which $X$ is bound. \\
                  \> case \>$(c\ a_1\ ...\ a_n)$, where $c$ is a constant, and $n \geq 0$\ : \\
                  \>      \>{\bf head\_norm(\boldmath $s$)} \\
                  \>      \>{\bf if \boldmath $s$ is \boldmath $(r'\ b_1\ ...\ b_m)$, where \boldmath $r'$ is rigid and \boldmath $m\geq 0$} \\
                  \>      \>{\bf then}\ \={\bf if \boldmath $r' \neq c$ or \boldmath $n \neq m$ then backtrack} \\
                  \>      \>            \> {\bf else } for $1\leq i \leq n$: {\it unify}\ $(a_i,\ ${\boldmath $b_i$}$)$  \\
                  \>      \>{\bf else}  \> \\
                  \>      \>            \>{\bf create \boldmath $t'$ as \boldmath $(c\ X_1\ ...\ X_n)$ on heap, where \boldmath $X_i$ are new variables} \\
                  \>      \>            \>{\bf if \boldmath $s$ is a logic variable \boldmath $X$ }\\
                  \>      \>            \>{\bf then}\ \={\bf if \boldmath $uc(X) \leq uc(c)$ then backtrack} \\
                  \>      \>            \>            \>{\bf else \boldmath bind$(X,\ t')$} \\
                  \>      \>            \>{\bf else} /* $s$ must be a higher-order term */ \\
                  \>      \>            \>            \>{\bf push the pair \boldmath $(t', \ s)$ onto PDL} \\
                  \>      \>            \>for $1\leq i \leq n$: {\it unify}\ $(a_i, X_i)$
\end{tabbing}
\caption{The unification model in our compilation
implementation.}\label{fig:unify}
\end{figure}

Now we consider the compilation
of the unification on each pair of arguments. Compared with what has
to be dealt with by the WAM, the following new issues arise in our
setting.
First, a richer collection of term structures participate in the
computation. Second, a head normalization procedure has to be invoked
to bring terms into comparable forms at the necessary
points. Finally, relevant instructions have to be enhanced with
the ability to properly separate higher-order subproblems from
first-order ones, taking the necessary steps to solve the latter while
pushing the former onto the PDL.
Taking these issues into account, the processing in our implementation can be
described by the {\it unify} procedure in Figure~\ref{fig:unify}.
The first argument to this procedure is the argument from a clause head,
\ie, whose structure is statically known, and is assumed to be normalized
at compilation time. The second argument is the one dynamically appearing
at runtime. It should also be noted that the actions carried out in
compilation and at runtime are both present in this procedure, and we use
bold letters to distinguish the latter.

The auxiliary functions {\it interp\_unify}
and {\it head\_norm} in {\it unify} denote the interpretive
pattern unification and head normalization procedures respectively.
A call to the procedure {\it bind} in a form {\it bind\ $(X,\ t)$}
essentially carries out the action of binding a logic variable $X$ to the
term $t$.
In the situation when $X$ is from a static argument of the clause head,
the logic variable is not explicitly created, but, rather, given by a
data register or a slot in the environment frame.
Binding in this case is carried out by placing a reference to the term
$t$ in the relevant place.
Finally, in the case when the static term $t$ input to the {\it unify}
procedure is a first-order application and the dynamic term $s$ is a logic
variable or higher-order term, the recursive calls to {\it unify} simply
serve to construct the arguments of $t$ on the heap. For this reason, it is
not necessary to actually create the new variables $X_i$'s that are used
in the presentation of the pseudo code. Instead, space is allocated on
the heap for an argument vector of size $n$ and the recursive calls to
{\it unify} enter a term creation mode---known as the WRITE mode in
contrast to the READ mode that is used when term structure needs to be
analyzed---during which the arguments of $t$ are
created and references to them are placed into the relevant slots in the
argument vector.

The conventional WAM style term creation and unification instructions
are categorized into the {\it put}, {\it set}, {\it get}
and {\it unify} classes. Roughly mapping to the {\it unify} procedure in
Figure~\ref{fig:unify}, the {\it get} class of instructions can be used to
carry out the actions required by the cases where the static term is a
first-order application and where it is a constant or variable that appears
directly as an argument of the clause head.
When the {\it unify} procedure is invoked recursively over the
arguments of the (static) applications, the unifications over the embedded
variables and constants can be handled by the set of {\it unify} instructions.
The {\it put} and {\it set} instructions are used in the WAM solely for
setting up the the actual arguments of atomic goals and do not get used
in head unification. In our context, when the static
term has a higher-order structure, it has to be first
created and then handed to the interpretive unification process.
The term creation actions are carried out by
the {\it put} and {\it set} classes of instructions, \ie, these
instructions may be interleaved with {\it get} and {\it unify}
instructions in the compilation of head unification.

Within this picture, now we start to examine the enhancements to each
category of instructions for supporting the higher-order aspects
of unification.
Since the {\it set} category of instructions are in
fact a light-weight form of those in the {\it unify} class, \ie,
their actions are the same as those carried out
by the {\it unify} instructions in the WRITE mode, we do not discuss
these separately in what follows.

In contrast to the first-order setting, term creation in our context
has to deal with a
richer collection of structures. First, the head of (a head normal form of)
an application can be a de Bruijn index or a logic variable in
addition to being a constant.
For this reason, the {\it put\_structure} instruction in the WAM is
generalized into {\it put\_app}. This instruction gets three
arguments: a data (argument) register $A_i$, a data register or an
offset into an environment frame $X_j$ and a positive number $n$.
This instruction first creates an application term on the
heap with its head being the term referred to by $X_j$ and an empty argument
vector of size $n$. Then $A_i$ is set to refer to the new application term
and the $S$ register
is prepared to refer to the beginning of the argument vector for the subsequent
instructions to actually fill in the arguments.
The second source of higher-order structures is the appearances of de Bruijn
indexes and abstractions. For the creation of the former, new instructions
\begin{tabbing}
\dquad {\it put\_index\ $A_i$, $n$} \dquad and\dquad {\it unify\_index $n$}
\end{tabbing}
are introduced. The first one is used for a de Bruijn index
that is not directly an argument of an application. Its execution
constructs a term corresponding to the de Bruijn index $n$ on the top
of the heap and sets the data register $A_i$ to refer to it.
The {\it unify\_index} instruction corresponds to an application argument.
It can be only invoked in the WRITE mode and its effect is to create
a term corresponding to the de Bruijn index $n$ in the
heap location given by the register $S$ and to increment $S$ to point
to the next argument vector slot.
Similarly, the creation of an abstraction $\lambdadb(n, t)$ is realized by
the pair of new instructions
\begin{tabbing}
\dquad{\it put\_lambda\ $A_i$, $X_j$, $n$} \dquad and\dquad {\it unify\_lambda\ $X_j$, $n$},
\end{tabbing}
depending on whether the abstraction appears directly as an argument of an
application. A reference to the term $t$ is assumed to be contained by the data
register or environment offset $X_j$.

The instructions constructing compound terms assume that the head of an
application and the body of an abstraction are given by data registers.
However, these components can in particular situations correspond to
permanent variables which reside in environment frames
on the stack. In these situations, the relevant permanent variables have to be
globalized prior to use. To facilitate this,
our abstract machine include the instructions
\begin{tabbing}
\dquad{\it globalize\ $Y_i$, $A_j$} \dquad and \dquad {\it globalize\ $A_i$}.
\end{tabbing}
The first one dereferences the permanent variable $Y_i$ given by an
offset to an environment frame. If the resulting term still resides on the
stack, it is copied to the top of the heap and then sets both that
stack cell and the data register $A_j$ to refer to the newly created
heap cell. Otherwise $A_j$ is made to be a reference to the
dereferenced result.
The second instruction simply dereferences the given
$A_i$, carries out the globalizing actions described before if necessary and
leaves a reference to the appropriate heap term in $A_i$.

The {\it get} and {\it unify} instructions are used for carrying out
compiled unification. These instructions are enhanced to handle terms
whose structures may be revealed to be higher-order at runtime.
Changes are made for the instructions
\begin{tabbing}
\dquad{\it get\_structure\ $A_i$, $f$, $n$},\quad {\it
  get\_constant\ $A_i$, $c$ }\quad and\ {\it unify\_constant\ $c$},
\end{tabbing}
in which $A_i$ is required to be a data register referring to the
incoming term, $f$ and $c$ are required to be constants and $n$ is a
number denoting the arity of the application.
Executing these instructions (in the READ mode for the last
instruction) first invokes the interpretive
head normalization procedure on the term referred to by $A_i$ for the
first two instructions and the one referred to by the $S$ register for
the last. Let the resulting term be $s$; as already explained, its
decomposition will be given by the contents of the registers  {\it Head},
{\it ArgVector}, {\it NumArgs} and {\it NumAbs} at the end of head
normalization. If $s$ has a higher-order
structure, \ie, if it is an abstraction or a flexible application, a
disagreement  pair with the first term being (a reference to) $s$ and
the second referring to the current top
of heap or to the location given by $S$ is created on the PDL.
In the situation when {\it get\_constant} or {\it unify\_constant}
is executed, the constant $c$ is then created as the second term of the
disagreement pair. When the executed instruction is {\it get\_structure},
the term pushed onto the top of heap is then an application with an empty
argument vector of size $n$ and with its head referring to a new constant term
corresponding to $f$. Further, the $S$ register is set to the first entry of
the argument vector, and execution proceeds to the following
{\it unify} instructions in WRITE mode. The {\it unify\_value\ $X_i$}
instruction is also changed so that when it is executed in the READ
mode, it causes the pattern unification procedure, rather than the
first-order unification procedure, to be invoked in interpretive mode
on the pair of terms given by the register or environment offset $X_i$
and the $S$ register.
In addition, a new instruction
\begin{tabbing}
\dquad{\it pattern\_unify $X_j$,\ $A_i$}
\end{tabbing}
is introduced as a variant of {\it unify\_value} in the READ mode.
This instruction appears at the end of a sequence of {\it put} and
{\it unify} (in the WRITE mode) instructions that serves to create a
higher-order term appearing in a clause head. This instruction also
invokes the higher-order pattern unification procedure in interpretive
mode to unify the created term that is referenced by $X_j$ and the
incoming term that is given by the argument register $A_i$.

For a concrete example of the usage of our unification and term creation
instructions, we can consider the compilation of the term
$(app\ X\ (abs\ (y\plam \ X)))$ as an argument within a clause head,
assuming that $app$ and $abs$ are the constants that we encountered in
the {\it copy} program.
The instructions resulting from a compilation of this term are shown
in Figure~\ref{fig:unify_exp}.
\begin{figure}
\begin{tabbing}
\dquad\dquad\= {\it get-structure-}\dquad\={\it A22,}\ \={\it A22,}\ \={it A22} \= \kill
\>{\it get\_structure}  \> {\it A1,}\> {\it app,}\>{\it 2} \> \%\ {\it A1 = (app\ }\= \\
\>{\it unify\_variable} \> {\it A2}\>          \>        \> \%                   \>{\it X\ }\=         \\
\>{\it unify\_variable} \> {\it A3}\>          \>        \> \%                   \>         \> {\it A3)} \\
\>{\it get\_structure}  \> {\it A3,}\> {\it abs,}\>{\it 1} \> \%\ {\it A3 = (abs\ } \\
\>{\it unify\_variable} \> {\it A4}\>          \>        \> \%                   \> {\it A4)} \\
\>{\it put\_lambda}     \> {\it A5,}\> {\it A2,} \>{\it 1} \> \%\ {\it A5 = $\lambdadb(1, X)$} \\
\>{\it pattern\_unify}  \> {\it A4,}\> {\it A5} \>        \> \%\ {\it A4 = A5}
\end{tabbing}
\caption{Compiled unification over a head argument $(app\ X\ (abs\
(y\plam \ X)))$.}\label{fig:unify_exp}
\end{figure}

The instruction set for our abstract machine includes a new
instruction called {\it finish\_unify} that is used at the end
of the processing of the entire clause head. This instruction invokes
the interpretive
pattern unification procedure over the disagreement pairs that have been
pushed onto the PDL during the head processing. Further, if bindings to logic
variables have actually occurred during head unification,
the global disagreement set recording non-$\Ll$ problems generated
from computation steps prior to the processing of the current clause is also
examined at this stage with the expectation that some of them could actually
become $\Ll$ after the bindings.
It is interesting to note that this way of examining the global disagreement
set could in theory lead to bad performance: if a large number of non-$\Ll$
pairs are carried along across the solutions of atomic queries and only a
relatively small portion of it actually becomes $\Ll$ after the processing
of each clause head, then the repeated examination on the contained
disagreement pairs will be mostly redundant. This conceptual problem can be
solved by using a sophisticated {\it freeze-wake} mechanism proposed
by~\cite{MP93ppcp}. Within this scheme, a unsolvable disagreement pair
is directly associated with the logic variables contributing to it, and
the re-examination is triggered only when the binding of the logic
variable actually occurs. However, the ``extreme'' case described
above in fact rarely occurs in the context that we are interested in: in most
practical $\lp$ programs, it is either the case that
all the disagreement pairs are $\Ll$ the first time they are looked
at, usually because the program itself has been written to adhere to
the $\Ll$ style, or the case that a non-$\Ll$ pair is
transformed into an $\Ll$ one at the end of the processing of the
clause head in which the pair was encountered.
Based on this observation, the simple processing scheme that we have
chosen for delayed disagreement pairs seems justified.

A final new instruction for our abstract machine is
{\it head\_normalize\ $X_i$}, which carries out the head normalization of a
term referred to by the data register or
environment offset given by $X_i$. This instruction is used in the term
creation process
needed for setting up the arguments of atomic goals when it is obvious that a
higher-order structure
has been created. The purpose of enforcing head normalization over such
structures at an early stage is to reduce the overhead of backtracking.
The actual arguments have to be in head normalized form
during the unification operations carried out during the clause
selection. If this normalization is done before a choice point
corresponding to clause selection is created, then the process of
undoing and then redoing it because of a backtracking internal to this
selection process can be avoided.

A comparison between the processing model we have described here and
the one underlying the implementation of {\it Version 1} of the {\it
  Teyjus} system is in order. We focus here only on the issues that
have been discussed so far; more differences will arise when we
consider the treatment of types in the next chapter.
In the earlier abstract machine, the higher-order part of the
unification problems are separated from the first-order ones in a way similar
to our scheme and are also handed to an interpretive unification
procedure for their solution.
However, due to the branching nature of the unification procedure
dealt with in that abstract machine, a more sophisticated (and more
costly) control mechanism has to be considered. In particular, in
addition to the choice point, a
structure known as {\it branch point} had to be introduced for the
purpose of recording choices in the incremental steps taken to solve
rigid-flexible pairs \cite{N03treatment}. Further, these branch points
have to be
examined during backtracking for attempting the next alternative. This
also introduces further complexity in treating choice points at least
in that they have to be differentiated from branch points so that it
is clear what action needs to be taken in the relevant cases.
To avoid the storage of redundant control information for affecting
backtracking caused by the
branching of unification, special attention was paid in the design of
that abstract machine to the precise structure of a branch point.
The creation and the maintenance of branch points is carried out in
that machine by an instruction that is also called {\it finish\_unify}.
The necessity of branch points is entirely eliminated
in our context because we simply delay unification on any pairs that
could cause branching. This has lead to a considerable simplification
of the processing model and is also expected to lead to improvements
in the execution behavior over practical $\lp$ programs.

\section{An Complete Example of Compilation}\label{sec:inst_exp}
We are now in a position to show the complete sequence of instructions
that would be generated for the {\it copy} clauses shown in
Figure~\ref{fig:copy}. The code that we expect a compiler to generate
corresponding to the first two clauses is shown in
Figure~\ref{fig:copy_1}, the code for the last clause is appears in
Figure~\ref{fig:copy_2} respectively, and Figure~\ref{fig:copy_3}
contains the instructions for the embedded clause in the body of the
last clause for the predicate.

\begin{figure}\footnotesize
\begin{tabbing}
\dquad\=copy\ :\dquad\=switch-on-constant\dquad\=L2,\ L1,\ fail,\ fail,\ \dquad\dquad a\= \kill
\> {\it copy}\ :     \>{\it switch\_on\_term}  \>{\it L2,\ \ L1,\ \ fail,\ \ fail}                \\
\> {\it L1}\ :       \>{\it switch\_on\_constant}\>{\it 3,\ \ ht}                               \\
\> {\it L2}\ :       \>{\it try\_me\_else}     \>{\it 2,\ \ L4}  \> \%\ {\it copy}\  \=         \\
\> {\it L3}\ :       \>{\it get\_constant}     \>{\it A1,\ \ a}    \> \%\              \>{\it a}  \\
\>                   \>{\it get\_constant}     \>{\it A2,\ \ a}    \> \%\              \>{\it a}  \\
\>                   \>{\it finish\_unify}                                           \\
\>                   \>{\it proceed}                                                 \\
\> {\it L4}\ :       \>{\it retry\_me\_else}   \>{\it 2,\ \ L6}  \> \%\ {\it copy}\  \=                                       \\
\> {\it L5}\ :       \>{\it allocate}          \>{\it 3}                                                                     \\
\>                   \>{\it get\_structure}    \>{\it A1,\ \ app,\ \ 2}\> \%              \>{\it (app\ }\=                     \\
\>                   \>{\it unify\_variable}   \>{\it A1}            \> \%              \>           \>{\it T1}\ \=          \\
\>                   \>{\it unify\_variable}   \>{\it Y1}            \> \%              \>           \>          \>{\it T2)} \\
\>                   \>{\it get\_structure}    \>{\it A2,\ \ app,\ \ 2}\> \%              \>{\it (app\ }\=                     \\
\>                   \>{\it unify\_variable}   \>{\it A2}            \> \%              \>           \>{\it T3}\ \=          \\
\>                   \>{\it unify\_variable}   \>{\it Y2}            \> \%              \>           \>          \>{\it T4)} \\
\>                   \>{\it finish\_unify}     \>                    \> \%  $\pif$        \\
\>                   \>{\it head\_normalize}   \>{\it A1}            \> \%  {\it A1 = T1} \\
\>                   \>{\it head\_normalize}   \>{\it A2}            \> \%  {\it A2 = T3} \\
\>                   \>{\it call\_name}        \>{\it 2,\ \  copy}    \> \%  {\it copy A1 A2, } \\
\>                   \>{\it put\_value}        \>{\it Y1,\ \ A1}      \> \%  {\it A1 = T2} \\
\>                   \>{\it head\_normalize}   \>{\it A1}            \\
\>                   \>{\it put\_value}        \>{\it Y2,\ \ A2}      \> \%  {\it A2 = T4} \\
\>                   \>{\it head\_normalize}   \>{\it A2}            \\
\>                   \>{\it deallocate}        \\
\>                   \>{\it execute\_name}     \>{\it copy}          \> \%  {\it copy A1 A2.}
\end{tabbing}
\caption{Instructions for the first two clauses of {\it copy}.}\label{fig:copy_1}
\end{figure}

\begin{figure}\footnotesize
\begin{tabbing}
\dquad\=copy\ :\dquad\=switch-on-constant\dquad\=L3,\ L1,\ fail,\ fail\ \dquad\dquad a\= \kill
\> {\it L6}\ :      \>{\it trust\_me}          \>{\it 2}                 \> \% {\it copy\ }\= \\
\> {\it L8}\ :      \>{\it allocate}           \>{\it 2}                 \\
\>                  \>{\it get\_structure}     \>{\it A1,\ \ abs,\ \ 1}    \> \%             \> {\it (abs\ }\= \\
\>                  \>{\it unify\_variable}    \>{\it A3}                \> \%             \>             \> {\it T1)} \\
\>                  \>{\it get\_structure}     \>{\it A2,\ \ abs,\ \ 1}    \> \%             \> {\it (abs\ }\= \\
\>                  \>{\it unify\_variable}    \>{\it A4}                \> \%             \>             \> {\it T2)} \\
\>                  \>{\it finish\_unify}      \>                        \> \% $\pif$          \\
\>                  \>{\it incr\_universe}     \>                        \> \% {\it (}\={\it Pi\ }\=    \\
\>                  \>{\it set\_univ\_tag}     \>{\it Y1,\ \ c}           \> \%        \>     \> {\it c$\plam$\ }\= \\
\>                  \>{\it push\_impl\_point}  \>{\it 1,\ \ t}            \> \%        \>     \>                 \> {\it (}\={\it (copy c c) $\pimp$} \\
\>                  \>{\it put\_app}           \>{\it A1,\ \ A3,\ \ 1}     \> \%        \>     \>                 \>        \>{\it A1 = (T1\ }\= \\
\>                  \>{\it globalize}          \>{\it Y1,\ \ A255}        \\
\>                  \>{\it set\_value}         \>{\it A255}              \> \%        \>     \>                 \>        \>                \>{\it c)} \\
\>                  \>{\it head\_normalize}    \>{\it A1}                \\
\>                  \>{\it put\_app}           \>{\it A2,\ \ A4,\ \ 1}     \> \%        \>     \>                 \>        \>{\it A2 = (T2\ }\= \\
\>                  \>{\it set\_value}         \>{\it Y1}                \> \%        \>     \>                 \>        \>                \>{\it c)} \\
\>                  \>{\it head\_normalize}    \>{\it A2}                \\
\>                  \>{\it call\_name}         \>{\it 1,\ \ copy}         \> \%        \>     \>                 \>        \> {\it copy A1 A2}  \\
\>                  \>{\it pop\_impl\_point}   \>                        \> \%        \>     \>                 \> {\it )}\\
\>                  \>{\it decr\_universe}     \>                        \> \% {\it ).}\\
\>                  \>{\it deallocate}         \\
\>                  \>{\it proceed}
\end{tabbing}
\caption{Instructions for the last clause of {\it copy}.}\label{fig:copy_2}
\end{figure}

\begin{figure}\footnotesize
\begin{tabbing}
\dquad\=copy\ :\dquad\=switch-on-constant\dquad\=L3,\ L1,\ fail,\ fail\ \dquad\dquad a\= \kill
\>{\it copy}\ : \> {\it try\_me\_else}    \> {\it 0,\ \ L9}   \> \% {\it copy\ }\= \\
\>              \> {\it init\_variable}   \> {\it A3,\ \ Y1}  \\
\>              \> {\it pattern\_unify}   \> {\it A3,\ \ A1}  \> \%             \> {\it c} \\
\>              \> {\it pattern\_unify}   \> {\it A3,\ \ A2}  \> \%             \> {\it c.} \\
\>              \> {\it finish\_unify}    \\
\>              \> {\it proceed}          \\
\>{\it L9}\ :   \> {\it trust\_ext}       \> {\it 2, \ \ 1}
\end{tabbing}
\caption{Instructions for the dynamic clause of {\it copy} in the augment goal in its last definition.}\label{fig:copy_3}
\end{figure}

The instructions {\it switch\_on\_term} and {\it switch\_on\_constant}
in Figure~\ref{fig:copy_1} are used for indexing clause choices in a way
described in Section~\ref{sec:basic_model}.
Specifically, the former
takes the form
\begin{tabbing}
\dquad{\it switch\_on\_term\ \ V,\ \ C,\ \ L,\ \ BV}
\end{tabbing}
where $V$, $C$, $L$ and $BV$ are instruction addresses to which control must
be transferred to when head normal form of the term referred to by $A1$
is a flexible term, a rigid term with a constant head other than $::$,
a nonempty list and a bound variable head respectively. The label {\it fail}
is assumed to be the location of code that causes backtracking.
The other instruction {\it switch\_on\_constant} carries out the second-level
indexing among different constant heads. The first argument of it is a positive
number indicating the number of constants under consideration and the second
argument refers to a hash table in which the mapping from the constants to
the addresses of the corresponding clause definitions are stored.

Among the control instructions appearing in the figures,  {\it try\_me\_else},
{\it retry\_me\_else} and {\it trust\_me} are used for the manipulation
of choice points, and the former two have their second argument being
the address of the clause definition that should be attempted upon
backtracking.
Their first numeric argument is used to indicate the number of argument
registers that are to be saved or retrieved as relevant.
The instructions {\it allocate} and {\it deallocate} are used for the
creation and deletion of environment frames on the stack. The argument
of the former contains a positive number corresponding to the number of
permanent variables that are to be allocated on the frame.
The calls to clause definitions that need to be dynamically determined
are handled by the instructions
{\it call\_name} and {\it execute\_name}, whereas the return from a
clause definition is effected by the instruction
{\it proceed}. The instruction
{\it execute\_name} is specially intended for the last call optimization
mentioned
in Section~\ref{sec:basic_model}. The numeric argument of the call instructions
is used to indicate the number of variables that remain on the
caller's environment frame at the time of the call.
The instruction {\it trust\_ext\ n, i} in Figure~\ref{fig:copy_3} is used
to search for dynamically extended clause definitions in a way described in
Section~\ref{sec:ho_control}.
The first argument
{\it n} is the number of argument registers that should be recovered
before the control is transferred to the found clause definition.
Figure~\ref{fig:copy_2} also illustrates the usages of the higher-order
control instructions {\it push\_impl\_point}, {\it pop\_impl\_point},
{\it incr\_universe} and {\it decr\_universe},
the computations underlying which are described in
Section~\ref{sec:ho_control}.

Following the WAM convention, in the instructions shown in the
figures, we have used the name {\it Yi} to depict the $i$th variable
that is allocated in the environment frame. Also, the instructions
{\it unify\_variable} and {\it put\_value} are identical to the ones
with the same name in the WAM and the instruction {\it set\_value}
is used as a special case of {\it unify\_value} in the WRITE mode.

\section{Treatment of Flexible and Disjunctive Goals}\label{sec:misc}
Up to this point, we have provided a conceptual picture of our
abstract machine and compilation model insofar as these related to
the treatment of higher-order pattern unification. There are two
issues that are remained to be explained. First, the implementation
discussed so far assumes a monomorphic type system for our language,
within which no runtime processing of types is necessary. This
restriction has to be removed in the presence of the first-order
polymorphic types, on which our language is actually based. A
treatment of this aspect is deferred to the next chapter. Second, it
is not clear yet on how the flexible and disjunctive goals are
handled. We discuss these aspects in this section.

The appearance of flexible goals, \ie, of goals of form
$(P\ t_1\ \ldots\ t_n)$, where
$P$ is a variable, embodies the ability to mix in our language
meta and object level usages of predicate expressions.
A predicate definition that exploits this ability is shown below:
\begin{tabbing}
\dquad\dquad\={\it kind}\dquad\dquad\={\it i}\dquad\dquad\={\it type}.\\
            \>{\it type}\>{\it mappred}\>{\it (list i) $\ra$ (i $\ra$ i $\ra$ o) $\ra$ (list i) $\ra$ o. } \\[5pt]
            \>{\it mappred}\>{\it nil P nil.} \\
            \>{\it mappred}\>{\it (X :: L1) P (Y :: L2)} $\pif$ {\it P X Y, mappred L1 P L2.}
\end{tabbing}
Let {\it bob, john, mary, sue, dick} and {\it kate} be constants declared
with type {\it i}, and let {\it parent} be a constant of type
{\it i $\ra$ i $\ra$ o}. Then the following additional clauses
define a ``parent'' relationship between different individuals.
\begin{tabbing}
\dquad\dquad\={\it parent}\dquad\dquad\={\it bob}\dquad\={\it john}.\\
            \>{\it parent}\>{\it john}\> {\it mary}.\\
            \>{\it parent}\>{\it sue}\> {\it dick}.\\
            \>{\it parent}\>{\it dick}\> {\it kate}.
\end{tabbing}
In this context, a query of form
\begin{tabbing}
\dquad\dquad{\it ?-}\quad{\it mappred}\dquad{\it (bob :: sue :: nil)}\dquad {\it parent}\dquad {\it L}
\end{tabbing}
can be asked, and can be solved with the answer
substitution $\{\dg{L}{john::dick::nil}\}$.
Following the operational semantics of our language specified in
Section~\ref{sec:interpreter}, it can be observed that in the course of
solving this query, two new goals
\begin{tabbing}
\dquad\dquad{\it parent  bob  Y1}\dquad and\dquad {\it parent  sue  Y2}
\end{tabbing}
will be dynamically formed and solved. Another example of a query is
\begin{tabbing}
\dquad{\it ?- mappred (bob :: sue :: nil) (x$\plam$ y$\plam$ (Sigma z$\plam$ (parent x z, parent z y))) L}.
\end{tabbing}
This goal asks for the grandparents of {\it bob} and {\it sue} and has as
its solution the substitution $\{\dg{L}{mary::kate::nil}\}$. Finding this
answer requires two new goals of complex structures---each with an
embedded conjunction and existential quantifier---to be constructed
dynamically and then solved.

As illustrated by the {\it mappred} example, flexible goals may be
instantiated by terms containing predicate constants and with complex
logical structures, thereby dynamically
reflecting object-level occurrences of quantifiers and connectives into
positions where they function as search directives.

The problem faced in supporting flexible goals is that instantiations of
their heads can change their structure  dynamically, and so it is
impossible to know at compile time the specific control action that
they would give rise to during computation.
However, we can provide a partial compilation in that we can use the
top level structure of these goals at runtime to pick between
different compiled treatments of control structure. In particular,
flexible goals
can be compiled into calls to a special procedure named {\it solve} to which
(the instantiated version of) the goal is provided as an argument. In the case
that the incoming goal has a complex structure, the behavior of {\it solve}
can be envisaged as of it were based on a compilation of the following
clauses:
\begin{tabbing}
\dquad\dquad\={\it solve (G1 , G2)}\dquad\=$\pif$\quad\={\it solve G1, solve G2}.\\
\>{\it solve (G1 ; G2)}\>$\pif$\>{\it solve G1; solve G2}.\\
\>{\it solve (Sigma G)}\>$\pif$\>{\it solve (G X)}.\\
\>{\it solve (Pi G)}\>$\pif$\>{\it Pi x$\plam$ (solve (G x))}.
\end{tabbing}
When the argument given to {\it solve} is an atomic goal with a rigid head,
then its arguments are loaded into appropriate data registers and the head is
used to determine the code to be invoked subsequently. The only other situation
that could possibly arise is that the actual argument passed to {\it
  solve} remains a flexible atomic goal; the syntactic restriction on
the appearance of logical symbols in terms makes it impossible for
any other case to arise. In this last case---when the argument of {\it
  solve} is a flexible goal---we follow the suggestion in
\cite{NM98Handbook} and solve the goal immediately with a substitution
of the form $\lambda x_1\ldots\lambda x_n\top$ for the variable that
appears as the head of this goal.

In our implementation, the {\it solve} predicate is
treated as a builtin one whose realization is ``hard-wired'' into
the abstract machine.

Our treatment of disjunctive goals is based on a compile-time
pre-processing of clauses to eliminate such disjunctions. Upon seeing
a goal of the form {\it (G1 ; G2)}, the compiler creates a new
predicate definition consisting of the following clauses:
\begin{tabbing}
\dquad\dquad\=
{\it new\_pred  X1 ... Xn}\quad $\pif$\quad {\it G1.}\\
\>
{\it new\_pred  X1 ... Xn}\quad $\pif$\quad {\it G2.}
\end{tabbing}
Here, {\it new\_pred} is a name chosen such that it is distinct from
any other name used in the program and $\{X_1, ..., X_n\}$ is the
set of variables occurring free in {\it (G1 ; G2)}. After generating
and adding these clauses to the program, the compiler replaces the
disjunctive goal with the atomic goal {\it (new\_pred X1 ... Xn)}.
As a concrete example, a clause presented in the form
\begin{tabbing}
\dquad\dquad{\it foo X}\quad $\pif$\quad {\it bar1 U V , (bar2 (f X) U ;  bar3 (f X) V)}.
\end{tabbing}
will be transformed into the sequence of clauses
\begin{tabbing}
\dquad\dquad\={\it foo X}\dquad\dquad\dquad\= $\pif$\quad\= {\it bar1 U V , new\_pred X U V}.\\
\>{\it new\_pred X U V}\>$\pif$\>{\it bar2 (f X) U}. \\
\>{\it new\_pred X U V}\>$\pif$\>{\it bar3 (f X) V}.
\end{tabbing}
by the pre-processing pass just described.

An alternative treatment to disjunctive goals is possible: we could
build in mechanisms for creating choice points in the bodies of
clauses. Thus, in the example just considered, we could use the
following structure to compile the body of the clause for {\it foo}:
\begin{tabbing}
\dquad\=\dquad\= \{\ \ \= Instructions for {\it (bar1 U V)}\ \ \}\\
      \>       \>\>{\it try\_me\_else\_disj L} \\
      \>       \>\{\ \ Instructions for {\it (bar2 (f X) U)}\ \ \} \\
      \>{\it L:}\>\>{\it trust\_me\_disj} \\
      \>       \>\{\ \ Instructions for {\it (bar3 (f X) V)}\ \ \}
\end{tabbing}
Here, the instructions {\it try\_me\_else\_disj} and {\it
  trust\_me\_disj} are like the WAM instructions {\it try\_me\_else}
and {\it trust\_me} except that it is the free variables occurring on
the disjunctive goal that are recorded and used by these instructions
rather than the argument registers. In the above example, instead of
the contents of registers
{\it A1} and {\it A2}, the actual information
recorded in the choice point should be the bindings of the variables
{\it X} and {\it V}.
Notice that we do not need to keep the information about {\it U} in
this example. In general, the compilation process would have to carry
out a ``usefulness'' analysis on the free variables that appear in
disjunctive goals to determine the ones that really have to be
remembered.

Compared with the approach of creating new predicates, this
alternative direct compilation of disjunctive goals has some
advantages. First, it obviates the call to the additional predicate
{\it new\_pred} and consequently avoids the runtime overhead for such
calls. Second, it provides a framework for analyzing which variables
really need to be stored and hence for avoiding redundant
book-keeping.
For these reasons, the direct compilation of disjunctive goals is
something that might be explored further as an improvement to our
implementation ideas.
