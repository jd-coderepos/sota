

\begin{theorem}\label{thm:pc}
  For mixed/packing covering, there are $(1+\eps)$-approximation algorithms
  running
  
  \noindent 
  (i) in time $O(N\log(m)/\eps^2)$,

  \noindent 
  (ii) in parallel time $O(\log N\,\log^2 m\, \log(n\log(m)/\eps)/\eps^4)$,
  doing work $O(N \log(n\log(m)/\eps)\log(m)/\eps^2)$.
\end{theorem}











\begin{algorithm}[t]
  \caption{Generic approximation algorithm for mixed packing/covering \LPs\label{alg:pc}}
  \begin{algorithmic}[1]
    \Function{Packing-Covering}{matrices $C$, $P$; initial solution $x=x^0\in\R_+^n$, $\eps\in(0,1/10)$}
    \State\label{pc:defn}
    {\renewcommand{\arraystretch}{1.3}
      \begin{tabular}[t]{@{}lr@{\,~}l@{~~~~~}l@{~\,}r@{\,~}l}
        Define:& $U$ &$=~ \lefteqn{\textstyle(\max_i P_{i} x^0 + \ln m)/\eps^2}$ \\
        & $p_i(x)$ & $=\, {(1+\eps)}^{P_i x}$
        &  &$c_i(x)$ & $=\, {(1-\eps)}^{C_i x}$~~if~$C_i \,x\, \le\, U$, else $c_i(x)=0$\\ 
        & $|p(x)|$ & $=\, |p(x)|_1$ 
        &  &$|c(x)|$ & $=\, |c(x)|_1 \,=\, \sum_{i} c_i(x)$\\
        & $\lambda(x, j)$ & $=\, P\tran_j\, p(x) / C\tran_j\, c(x)$
        &  &$\lambda^*(x)$ & $=\, \min_{j\in [n]} \lambda(x,j)$.
        \\[3pt]
      \end{tabular}
    }
    \State Initialize $\lambda_0 \gets |p(x)|/|c(x)|$.
    \Block Repeatedly do {\bf either}
    of the following two operations whose precondition is met:
    \Block {\bf operation (a): increment $x$}\label{op:a}
    \Comment{\emph{precondition:}
      {$\lambda^*(x) \,\le\, (1+4\eps)\lambda_0$}}
    \State Choose $\delta\in\R_+^n$ such that
    \State~~~(i) $\forall j \in [n]$, if $\delta_j > 0$ then $\lambda(x,j) \le (1+4\eps)\lambda_0$, and
    \State~~(ii) $\max\{ \max_i P_i\,\delta, \max_{i: C_i x\le U} C_i\,\delta\} $ 
    is in $[1/2,1]$
    \State~~~~~~~(the maximum increase in any $P_i x$ or active $C_i x$ 
    is between $1/2$ and $1$). 
    \State Let $x \gets x + \delta$\label{pc:update}.
    \State If $\min_{i} C_i x \ge U$ then return $x/U$.
    \EndBlock\label{op:a:end}
    \Block {\bf operation (b): scale $\lambda_0$}
    \Comment{
      \emph{precondition:}\label{op:b}
      {$\lambda^*(x) \,\ge\, (1+\eps)\lambda_0$}
      \State Let $\lambda_0 \gets (1+\eps)\lambda_0$.\label{pc:lambda}
    }
    \EndBlock\label{op:b:end}
    \EndBlock
    \EndFunction
  \end{algorithmic}
\end{algorithm}

The rest of this section proves Thm.~\ref{thm:pc}. 
The starting point is Alg.~\ref{alg:pc} (above),
which is essentially a convenient reformulation of the generic ``algorithm with phases''
in~\cite[Fig.~2]{Young01Sequential}.
For part (i), we'll describe how to implement it to run faster
by only estimating the intermediate quantities of interest
and updating the estimates periodically.
We'll need the following properties (essentially from~\cite{Young01Sequential}):

\begin{lemma}\label{lemma:pc} 
  Given any feasible mixed packing/covering instance $(P,C)$, Alg.~\ref{alg:pc}
  \\(i) returns a $(1+O(\eps))$-approximate solution
  (i.e., $x$ such that $Cx \ge 1$ and $Px \le 1+O(\eps)$),
  \\(ii) scales $\lambda_0$ (lines~\ref{op:b}--\ref{op:b:end}) at most $O(U)$ times, 
  where $U = O(\log(m)/\eps^2 + \max_i P_i x^0/\eps)$, and
  \\(iii) increments $x$ (lines~\ref{op:a}--\ref{op:a:end}) at most $O(m\, U)$ times.
\end{lemma}
See the appendix for a proof,
which follows the proofs of Lemmas 1--5 of~\cite{Young01Sequential}. 
After initialization, Alg.~\ref{alg:pc} simply 
repeats one of two operations: {\bf (a)}
incrementing the current solution $x$ by some vector $\delta$,
or {\bf (b)} scaling $\lambda_0$ by $1+\eps$.
In each iteration, it can do either operation whose precondition is met,
and when incrementing $x$ there are many valid ways to choose $\delta$.

\subsection{Proof of part (i), sequential algorithm}
Alg.~\ref{alg:pc:imp}, which we use to prove part (i) of Thm.~\ref{thm:pc},
repeats these two operations in a particular way.
To reduce the run time to nearly linear,
instead of computing $Px$, $Cx$, $p(x)$ and $c(x)$ exactly as it proceeds,
Alg.~\ref{alg:pc:imp} maintains estimates:
$\widehat P$, $\widehat C$, $\widehat p$, and $\widehat c$.
To prove correctness,
we show that the estimates suffice to ensure
a correct implementation of Alg.~\ref{alg:pc},
which is correct by Lemma~\ref{lemma:pc} (i).

\begin{algorithm}[t]
  \caption{Sequential implementation 
    of Alg.~\ref{alg:pc}\label{alg:pc:imp}
    for mixed packing/covering \LPs}
  \begin{algorithmic}[1]
    \Function{Sequential-Packing-Covering}{$P, C, \eps$}
    \State Initialize $x_{j} \gets 0$ for $j\in [n]$,
    $\lambda_0 \gets |p(x)|/|c(x)| = m_p/m_c$,
    and $U = \ln(m)/\eps^2$.
    \State\label{pc:imp:estimate1} 
    Maintain vectors $\est P$, $\est C$, $\est p$, and $\est c$, to satisfy invariant
    \vspace*{-1.5ex}
    \begin{equation}\label{pc:imp:invariant}
      \begin{array}{ll}
        \text{For all } i:
        &
        \begin{array}{@{~~~}l@{~}l@{~}l@{~~~~~}l@{~}l@{~}l}
          \est P_i  &\in& (P_i\, x - 1, P_i\,x]
          &
          \est p_i &=& {(1+\eps)}^{\est P_i},
          \\[0pt]
          \est C_i &\in& (C_i \,x - 1, C_i\, x]
          &
          \est c_i &=& {(1-\eps)}^{\est C_i} \text{ if } \est C_i \le U, \text{ else } \est c_i=0.
        \end{array}
      \end{array}
      \vspace*{-0.39in}
    \end{equation}

    \State
    \Repeat
    \For{each $j\in [n]$}
    \Comment{do a \emph{run} for $x_j$}
    \State\label{pc:imp:lambda}\label{pc:imp:b}Compute values
    of $P\tran_j\, \est p$ 
    and $C\tran_j\, \est c\,$ 
    from $\est p$ and $\est c$.
    Define $\est \lambda_j\,=\, P\tran_j\, \est p \,/\, C\tran_j\, \est c$.
    \While{$\est \lambda_j \le {(1+\eps)}^2\lambda_0/{(1-\eps)}$}
    \Block {\bf operation (a): increment $x_j$}\label{op:aj}
    \Comment{\emph{assertion:}
      {$\lambda^*(x) \,\le\, (1+4\eps)\lambda_0$}}
    \State\label{pc:imp:z} Let $x_j \gets x_j + z$, 
    choosing $z$ 
    so $\max\{ \max_i P_{ij}\,z, \max_{i: C_i x\le U} C_{ij}\,z\} = 1/2$.
    \Block\label{pc:imp:estimate3} \textbf{As described in text}, 
    to maintain Invariant~\eqref{pc:imp:invariant}:
    \State\label{pc:imp:estimate4} For selected $i$ with $P_{ij}\ne 0$,
    update $\est P_i$ and
    $\est p_i$.  Update $P\tran_j \est p$ accordingly.
    \State\label{pc:imp:estimate5} For selected $i$ with $C_{ij}\ne 0$,
    update $\est C_i$ and $\est c_i$.
    Update $ C\tran_j \est c$ accordingly.
    \EndBlock
    \State If $\min_{i} \est C_i \ge U$ then {\bf return} $x/U$ 
    \Comment{finished}\label{pc:imp:return}
    \EndBlock\label{pc:imp:b:end}
    \EndWhile 
    \EndFor
    \Block {\bf operation (b): scale $\lambda_0$}
    \Comment{\emph{assertion:}\label{op:b2}
      {$\lambda^*(x) \,\ge\, (1+\eps)\lambda_0$}}
    \State Let $\lambda_0 \gets (1+\eps)\lambda_0$.\label{pc:imp:a}
\EndBlock
    \EndRepeat
    \EndFunction 
  \end{algorithmic}
\end{algorithm}

\begin{lemma}\label{lemma:pc:imp:correct}
  Given any feasible packing/covering instance $(P,C)$, 
  provided the updates in lines~\ref{pc:imp:estimate4}--\ref{pc:imp:estimate5} 
  maintain Invariant~\eqref{pc:imp:invariant}:

  \noindent (i)~~Each operation (a) or (b) done by Alg.~\ref{alg:pc:imp}
  is a valid operation (a) or (b) of Alg.~\ref{alg:pc}, so

  \noindent (ii) Alg.~\ref{alg:pc:imp} returns a $(1+O(\eps))$-approximate solution.
\end{lemma}
The proof is in the appendix.
The proof follows~\cite{Young01Sequential},
but adds the idea
from~\cite{koufogiannakis2007beating,Koufogiannakis13Nearly}
of maintaining estimates by sampling.
(Here we maintain the estimates differently, though,
using deterministic, periodic sampling
as detailed in the next proof.)

\begin{lemma}\label{lemma:pc:imp:sampling}
  Alg.~\ref{alg:pc:imp} can do the updates 
  in lines~\ref{pc:imp:estimate3}--\ref{pc:imp:estimate4} 
  so as to maintain Invariant~\eqref{pc:imp:invariant}
  and take total time $O(N \log(m)/\eps^2)$.
\end{lemma}
\begin{proof}
  The algorithm maintains the following global data:
  \begin{itemize}\setlength{\itemsep}{0in}
  \item the current solution $x$, 
    and vectors $\widehat P$, $\widehat C$, $\widehat p$ and $\widehat c$ 
    satisfying Invariant~\eqref{pc:imp:invariant};
  \item for each $j$, \emph{column maxima}: 
    $\max_i P_{ij}$ and $\max \{C_{ij} : C_i x \le U\}$.
  \end{itemize}
  
  Initializing these items takes $O(N)$ (linear) time, 
  with the exception of the column maxima.
  To initialize and maintain the maxima,
  the algorithm presorts the entries within each column of $P$ and $C$, 
  in total time $O(N\log m)$,
  then, every time some covering constraint $C_i x \ge U$ becomes satisfied,
  updates the maxima of the columns $j$ with $C_{ij} \ne 0$.
  (The total time for these updates is linear, as each can be charged to a non-zero $C_{ij}$.)

  To maintain Invariant~\eqref{pc:imp:invariant},
  the algorithm will actually guarantee something stronger:
  outside the {\bf while} loop, each estimate $\est P_i$ and $\est C_i$ 
  will be exactly accurate (that is, $\est P_i = P_i x$ and $\est C_i = C_i x$ for all $i$).
  Inside the {\bf while} loop, during a run of increments for a particular $x_j$,
  for each $i$, only the contributions of $P_{ij} x_j$ to $P_i x$ (for the current $j$)
  will be underestimated in $\widehat P_i$. 
  Likewise, only the contributions of $C_{ij} x_j$ to $C_i x$
  will be underestimated in $\widehat C_i$.  
  In line~\ref{pc:imp:estimate4},
  the algorithm will update those $\est P_i$ and $\est C_i$ 
  for which the under-estimation is in danger of exceeding 1, as follows.

  \paragraph{Maintaining the estimates using periodic sampling.}
  Define the \emph{top} of any number $y> 0$ 
  to be the smallest power of 2 greater than or equal to $y$.
  In a preprocessing step,
  within each column $C\tran_j$ and $P\tran_j$ separately, 
  partition the non-zero entries $C_{ij}$ and $P_{ij}$ into equivalence classes 
  according to their tops,
  and order the groups by decreasing top.
  (Use the presorted entries within each column to do this in $O(N$) total time.)

  Call a consecutive sequence of increments to $x_j$
  (done within a single iteration of the {\bf for} loop for $j$) a \emph{run} for $x_j$.
  During a run for $x_j$,
  say that a group $G$ in $P\tran_j$ with top $2^t$ is \emph{eligible for update}
  if the increase $\delta_G$ in $x_j$ 
  since the last update of group $G$ during the run
  (or, if none, the start of the run) is at least $1/2^{t+1}$.

  Implement line~\ref{pc:imp:estimate4} as follows. 
  Starting with the group $G$ in $P\tran_j$ with largest top,
  Check the group to see if it's eligible for update ($\delta_G \ge 1/2^{t+1}$).
  If it is, then, for each $i$ in the group $G$, increase $\est P_i$ to $P_i x$
  in constant time by adding $P_{ij} \delta_G$ to $\est P_i$.
  Update each scalar dependent of $\est P_i$ 
  ($\est p_i$, $P\tran_j \est p$, $\est\lambda_j$).
  Then, continue with the next group in $P\tran_j$ (the one with next smaller top).
  Stop processing the groups in $P\tran_j$ 
  with the first group that is not eligible for update.
  --- don't process any subsequent groups with smaller tops, regardless of eligibility.

  Implement line~\ref{pc:imp:estimate5} for $\est C$ and its dependents likewise.
  (When updating some $\est C_i$, check
  whether $\est C_i \ge U$, and if so, delete row $i$ from $C$
  and associated data structures.\footnote
  {The condition ``$\est C_i \ge U$'' 
  differs from $C_i x \ge U$ in Alg.~\ref{alg:pc}.
  We note without proof that this doesn't affect correctness.}
  When the last row of $C$ is deleted, stop and return $x/U$
  (line~\ref{pc:imp:return}).)

  \smallskip

  Finally, at the end of the run for $x_j$,
  to maintain the invariant that all estimates are exact outside of the while loop,
  do the following.
  For each group $G$ in $P\tran_j$,
  for each $i$ in $G$,
  update $\est P_i$ to the exact value of $P_i x$ by increasing
  $\est P_i$ by $P_{ij}\delta_G$ (for $\delta_G$ defined above),
  and update $\est p_i$ accordingly.
  Likewise, update $\est C_i$ (and its dependent $\est c_i$) 
  for every $i$ with $C_{ij} \ne 0$ to its exact value.

  \paragraph{Correctness of periodic sampling.}
  To show Invariant~\eqref{pc:imp:invariant} holds during a run,
  we prove that, \emph{if a given group $G$ with top $2^t$ is not updated
  after a given increment of $x_j$, then $\delta_G \le 1/2^t$}.
  (Invariant~\eqref{pc:imp:invariant} follows,
  because, for $i\in G$, the increase $P_{ij}\delta_G$ in $P_i x$ 
  since the last update of $\est P_i$ is less than $2^t / 2^t = 1$;
  similarly, the increase $C_{ij}\delta_G$ in $C_i x$
  since the last update of $\est C_i$ is less than 1.)

  Suppose for contradiction that the claim fails.
  Consider the first increment of $x_j$ for which it fails,
  and the group $G$ with largest top $2^t$ for which $\delta_G > 1/2^t$ 
  after that increment.
  Group $G$ cannot be the group with maximum top in its column,
  because the algorithm considered that group after the increment.
  Let $G'$ be the group with next larger top $2^{t'} > 2^t$.
  $G'$ was not updated after the increment,
  because if it had been $G$ would have been considered and updated.
  Let $x_j$ denote the current value of $x_j$,
  and let $x'_j < x_j$ denote the value at the most recent update of $G'$.

  When group $G'$ was last updated, group $G$ was considered but not updated
  (for, if $G$ had been updated then,
  we would now have $\delta_G = \delta_{G'} \le 1/2^{t'} < 1/2^t$).
  Thus, letting $x''_j$ be the value of $x_j$ at the most recent update of $G$,
  we have $x'_j - x''_j < 1/2^{t+1}$.
  Since group $G'$ was not updated after the current increment,
  we have (by the choice of $G$) that $x_j - x'_j  = \delta_{G'} \le 1/2^{t'} \le 1/2^{t+1}$.
  Summing gives $x_j - x''_j < 2/2^{t+1} = 1/2^{t}$,
  violating the supposition $\delta_G > 1/2^t$.

  \paragraph{Time.}
  At the start of each run for a given $x_j$,
  the time in line~\ref{pc:imp:lambda}
  is proportional to the number of non-zeroes in the $j$th columns of $P$ and $C$,
  as is the time it spends at the end of the run updating 
  all $\est P_i$ (for $P_{ij} \ne 0$) and $\est C_i$ (for $C_{ij} \ne 0$).
  Thus, the cumulative time spent on these actions during any single
  iteration of the {\bf repeat} loop is $O(N)$.
  By Lemma~\ref{lemma:pc} (ii), Alg.~\ref{alg:pc:imp}
  does $O(U)$ iterations of its {\bf repeat} loop,
  so the total time for the actions outside of increments is $O(N U)$, as desired.

  Each increment to some $x_j$
  takes time proportional to the number
  of updates made to $\est P_i$'s and $\est C_i$'s.
  An update to $\est P_i$ in group $G$ with top $2^t$
  increases $\est P_i$ by $P_{ij} \delta_G \ge P_{ij}/2^{t+1} > 2^{t-1}/2^{t+1} = 1/4$.
  Throughout,
  $\est  P_i$ does not exceed $(1+O(\eps)) U$,
  so $\est P_i$ is updated $O(U)$ times during increments.
  Likewise (using that $\est C_i$ is updated only while $\est C_i \le U$),
  each $\est C_i$ is updated $O(U)$ times during increments.
  There are $m$ $\est P_i$'s and $\est C_i$'s,
  so there are $O(mU) = O(NU)$ such updates.
\end{proof}

\begin{algorithm}[t]
  \caption{Parallel implementation 
    of Alg.~\ref{alg:pc} 
    for mixed packing /covering \LPs\label{alg:pc:par}}
  \begin{algorithmic}[1]
    \Function{Parallel-Packing-Covering}{$P, C, \eps$}
    \State Initialize $x_{j} \gets n^{-1}/\max_i P_{ij}$ for $j\in [n]$,
    $\lambda_0 \gets |p(x)|/|c(x)| = m_p/m_c$.
    \State Define $U$, $P_i x$, $C_i x$, $p_i(x)$, $c_i(x)$, $\lambda(x,j)$, 
    etc.~per Alg.~\ref{alg:pc}.
    \Repeat
    \While{$\lambda^*(x) \le (1+\eps)\lambda_0$}
    \Block {\bf operation (a): increment $x$}\label{op:ap}
    \Comment{\emph{assertion:}
      {$\lambda^*(x) \,\le\, (1+4\eps)\lambda_0$}}
    \State \begin{tabular}[t]{@{}r@{}r@{\,~}ll@{~~}r@{\,~}l}
      & Define $J$ & 
      \rlap{$\,=\, \{j\in [n] : \lambda(x,j) \le (1+\eps)\lambda_0\}$,
        and, for $j\in J$,}
      \\
&$I^p_j$ & $\,=\, \{i :P_{ij} \ne 0\}$
      &and& $I^c_j$ & $\,=\, \{i : C_{ij} \ne 0 \text{ and } C_i x \le U\}$. \\[3pt]
    \end{tabular}
    \State For $j\in J$, 
    let $\delta_j = z\, x_j$ (and, implicitly, $\delta_j = 0$ for $j\not\in J$),
    \State~~choosing $z$ 
    such that $\max\{ \max_i P_{i}\,\delta, \max_{i: C_i x\le U} C_{i}\,\delta\} = 1$.
    \State For $j\in J$, let $x_j \leftarrow x_j + \delta_j$.
    \State For $i\in \bigcup_{j\in J} I^p_j$, update $P_i x$ and $p_i(x)$.
    For $i\in \bigcup_{j\in J} I^c_j$, update $C_i x$ and $c_i(x)$.
    \State\label{pc:par:update}
    For $j\in J$, update $C\tran_j c(x)$,  $P\tran_j p(x)$, and $\lambda(x,j)$.
    \State If $\min_{i} C_i x \ge U$ then {\bf return} $x/U$. \Comment{finished}
    \EndBlock
    \EndWhile 
    \Block {\bf operation (b): scale $\lambda_0$}\label{op:bp}
    \Comment{\emph{assertion:}
      {$\lambda^*(x) \,\ge\, (1+\eps)\lambda_0$}}
    \State Let $\lambda_0 \gets (1+\eps)\lambda_0$.
    \EndBlock
    \EndRepeat
    \EndFunction 
  \end{algorithmic}
\end{algorithm}

\subsection{Proof of part (ii), parallel algorithm}
Next we prove part (ii) of Thm.~\ref{thm:pc}, using Alg.~\ref{alg:pc:par}.
By careful inspection, Alg.~\ref{alg:pc:par}
just repeats the two operations of Alg.~\ref{alg:pc}
(increment $x$ or scale $\lambda_0$),
so is correct by Lemma~\ref{lemma:pc} (i).
To finish, we detail how to implement the steps
so a careful accounting yields the desired time and work bounds.

Call each iteration of the repeat loop a {\em phase}.
Each phase scales $\lambda_0$, so by Lemma~\ref{lemma:pc} (ii), there are $O(U)$ phases.
By inspection, $\max_i P_i x^0 \le 1$, so $U=O(\log(m)/\eps^2)$.  
Within any given phase, 
for the \emph{first} increment,
compute all quantities directly
in $O(\log N)$ time and $O(N)$ total work.
In each subsequent increment within the phase,
update all quantities incrementally,
in time $O(\log N)$ and doing total work
linear in the sizes of the sets
$E_p = \{(i,j) : j \in J, i \in I^p_j\}$
and $E_c = \{(i,j) : j\in J, i \in I^c_j\}$
of \emph{active edges}.

(For example:
update each $P_i x$ by noting that the increment
increases $P_i x$ by $\Delta^p_i = \sum_{j: i\in I^p_j} P_{ij} \delta_j$;
update $P\tran_j p(x)$ by noting that the increment increases 
it by $\sum_{i \in I^p_j} P_{ij} \Delta^p_i$.
Update $J$ by noting that $\lambda(x,j)$ only increases within the phase,
so $J$ only shrinks, so it suffices to delete a given $j$ from $J$
in the first increment when $\lambda(x,j)$ exceeds $(1+\eps)\lambda_0$.)

\paragraph{Bounding the work and time.}
In each increment, if a given $j$ is in $J$, then the increment increases $x_j$.
When that happens the parameter $z$ is at least $\Theta(1/U)$
(using $P_i x = O(U)$ and $C_i x = O(U)$)
so $x_j$ increases by at least a factor of $1+\Theta(1/U)$.
The value of $x_j$ is initially at least $n^{-1}/\max_i P_{ij}$
and finally $O(U/\max_i P_{ij})$.
It follows that $j$ is in the set $J$ during at most $O(U\log(nU))$ increments.
Thus, for any given non-zero $P_{ij}$, the pair $(i,j)$ is in $E_p$
in at most $O(U\log(nU))$ increments.
Likewise, for any given non-zero $C_{ij}$, the pair $(i,j)$ is in $E_c$
in at most $O(U\log(nU))$ increments.
Hence, the total work for Alg.~\ref{alg:pc:par}
is $O(\nz U\log(n U))$, as desired.

To bound the total time, note that, within each of the $O(U)$ phases,
some $j$ remains in $J$ throughout the phase.
As noted above, no $j$ is in $J$ for more than $O(U\log(n U))$ increments.
Hence, each phase has $O(U\log(n U))$ increments.
To finish, recall that each increment takes $O(\log N)$ time.
This concludes the proof of Thm.~\ref{thm:pc}. \hfill \qed

