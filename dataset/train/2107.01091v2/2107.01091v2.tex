\pdfoutput=1

\documentclass{article}

\usepackage[final]{neurips_data_2021}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{enumitem}

\newcommand{\libri}{\textsc{LibriSpeech}}
\newcommand{\speech}{\textsc{CrowdSpeech}}
\newcommand{\vox}{\textsc{VoxDIY}}
\newcommand{\news}{\textsc{RusNews}}
\newcommand{\crowd}{\textsc{CrowdWSA2019}}
\newcommand{\common}{\textsc{CommonVoice}}
\newcommand{\giga}{\textsc{GigaSpeech}}

\newcommand{\commentivan}[1]{\textcolor{blue}{\textbf{IS:#1}}}
\newcommand{\commentnikita}[1]{\textcolor{red}{\textbf{NP:#1}}}
\newcommand{\commentdmitry}[1]{\textcolor{violet}{\textbf{DU:#1}}}

\newcommand{\rebuttal}[1]{{\color{blue} #1}}

\makeatletter\let\NAT@force@numbers\relax\makeatother

\title{CrowdSpeech and VoxDIY: Benchmark Datasets \\ for Crowdsourced Audio Transcription}

\author{Nikita Pavlichenko \\
  Yandex \\
  Moscow, Russia \\
  \texttt{pavlichenko@yandex-team.ru} \\
  \And
  Ivan Stelmakh \\
  Carnegie Mellon University \\
  Pittsburgh, PA, USA \\
  \texttt{stiv@cs.cmu.edu} \\
  \And
  Dmitry Ustalov \\
  Yandex \\
  Saint Petersburg, Russia \\
  \texttt{dustalov@yandex-team.ru} \\
}

\begin{document}

\maketitle

\begin{abstract}
Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks  (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, \emph{we focus on bridging this gap in speech recognition}. For this, we collect and release \speech{} --- the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, \emph{we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing}. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing \vox{} --- a counterpart of \speech{} for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.\footnote{The code and data are released at \url{https://github.com/Toloka/CrowdSpeech} and \url{https://doi.org/10.5281/zenodo.5574585}. Our code is available under the Apache license 2.0, and datasets are available under the CC~BY 4.0 license.}
\end{abstract}

\section{Introduction}

Speech recognition is an important research problem that has found its applications in various areas from voice assistants such as Siri or Alexa~\citep{hoy18alexa} to call centers~\citep{neustein2010advances} and accessibility tools~\citep{bain05accessibility}. The research community has been actively developing tools for automated speech recognition~\citep[and many other works]{amodei15deep,hannun2014deep,Lamere03thecmu,lee09recent,liptchinsky17convnets,park19specaugment,Povey11thekaldi,synnaeve19endtoend,xu18neural, Young2006}. As a result, the state-of-the-art methods achieve near-perfect performance~\citep{zhang2020pushing} on \libri{}~\citep{panayotov15librispeech} --- a famous benchmark to compare speech recognition systems.

While the technical performance on curated benchmarks is almost perfect, it does not necessarily result in reliable practical performance~\citep{szymanski20werwer}. Indeed, in real applications, people may use some specific vocabulary or dialects underrepresented in the conventional training data. Thus, blind application of methods trained on the standard benchmarks may result in low accuracy or, perhaps more concerning, discrimination of some subpopulations. For example, a recent study of YouTube's Automatic Captions reveals a difference in accuracy across gender and dialect of the speaker~\citep{tatman17gender}.

One approach towards improving the practical performance of speech-recognition systems is to fine-tune the models in these systems on domain-specific ground truth data. Fine-tuning is very important and efficient for the speech-recognition task~\citep{chan21speechstew,yu2010roles}, but the main problem with this approach is the lack of data. Indeed, even datasets that are considered to be very small in the area (e.g., CHiME-6~\citep{watanabe20chime6}) contain hours of annotated audios. While getting such an amount of unlabeled data in a speech-focused application may be feasible, annotating this data with the help of expert annotators may be prohibitively expensive or slow.

Recently, crowdsourcing has become an appealing alternative to the conventional way of labeling data by a small number of experts. Platforms like Mechanical Turk (\url{https://www.mturk.com/}) and Toloka (\url{https://toloka.ai/}) significantly reduce the time and cost of data labeling by providing on-demand access to a large crowd of workers. Of course, this flexibility comes at some expense, and the main challenge with the crowdsourcing paradigm is that individual workers are noisy and may produce low-quality results. A long line of work \citep[and others]{Dawid1979MaximumLE,karger11iterative,shah16permutation,sheng08get,whitehill09count,zhang2014spectral} has designed various methods to estimate true answers from noisy workers' responses to address this issue in multiclass classification. As a result, crowdsourcing has become an industry standard for image labeling with small and large technology companies  using it to improve their services~\citep{drutsa19practice}.

In speech recognition, however, the annotations obtained from crowd workers are sentences and not discrete labels, which makes the aforementioned classification methods impractical. Unfortunately, the problem of learning from noisy textual responses and other non-conventional modalities is much less studied in Machine Learning and Computer Science communities. One of the obstacles towards solving this problem in a principled manner is the lack of training data: in contrast to the classification setup, worker answers in the speech recognition tasks are high-dimensional, and researchers need a large amount of data to build and evaluate new methods. Therefore, \emph{we focus our work  on bridging this gap by constructing and analyzing a large-scale dataset of crowdsourced audio transcriptions}.

At a higher level, \emph{this work also considers the more general challenge of developing the methodology for reliable data collection via crowdsourcing}. In many areas, data is the key resource for research and development. When the costs of getting data are high, it becomes available to only a privileged population of researchers and practitioners, contributing to the overall inequity in the community. Crowdsourcing offers an appealing opportunity to make data collection affordable. However, to take the full benefits of crowdsourcing, the research community needs to develop procedures and practices to take reliable control over the quality of collected data. In this work, we build on our long experience of industrial data collection at Yandex~\citep{drutsa20crowd, drutsa20crowd2, drutsa21crowd} and share resources as well as insights that may benefit researchers and engineers who want to collect reliable data on crowdsourcing platforms.

\textbf{Our Contributions} Overall, in this work we make several contributions:

First, we collect and release \speech{} --- the first publicly available large-scale dataset of crowdsourced audio annotations. In that, we obtain annotations for more than   hours of English speech from 3,994 crowd workers. 

Second, we propose a fully automated pipeline to construct semi-synthetic datasets of crowdsourced audio annotations in under-resourced domains. Using this procedure, we construct \vox{} --- a counterpart of \speech{} for Russian language.

Third, we evaluate the performance of several existing and novel methods for aggregation of noisy transcriptions on collected datasets. Our comparisons indicate room for improvement, suggesting that our data may entail progress in designing better algorithms for crowdsourcing speech annotation.

Fourth and finally, we release the code to \emph{fully replicate} the data preparation and data collection processes we execute in this work. Additionally, we share various actionable insights that researchers and practitioners can use to fulfill their data collection needs. 

The remainder of this paper is organized as follows. We begin with a survey of related work in Section~\ref{section:related}. We then construct a pool of speech recordings for annotation in Section~\ref{section:acquisition} and describe the annotation pipeline in Section~\ref{section:annotation}. We provide an exploratory analysis of our datasets in Section~\ref{section:analysis} and evaluate existing and novel methods in Section~\ref{section:evaluation}. A short discussion of the results is provided in Section~\ref{section:conclusion}. Finally, we note that while the discussion in the present paper is centered around speech recognition, this work extends to other applications where textual sequences may be crowdsourced (e.g., optical character recognition~\citep{chrons11making, clematide16ocr, vonahn08re}).

\section{Related Work}
\label{section:related}

Several past works investigated the use of crowdsourcing platforms to obtain training data for speech recognition systems. We now discuss the most relevant studies. 

\textbf{Aggregation Methods} Despite the problem of aggregation of textual responses has been receiving much less attention than aggregation in the context of classification, there are several works in this direction.  The first study dates back to 1997 when Fiscus~\citep{fiscus97rover} proposed a method called ROVER to combine outputs of multiple speech-recognition systems. Several subsequent works~\citep{audhkhasi11accurate,evanini10nonnative,lee2011transcription,marge10turk} demonstrated the usefulness of this method in the crowdsourcing setup. More recently, two novel methods, RASA and HRRASA, were proposed to aggregate multiple translations of a sentence from one language to another~\citep{li20hrrasa,li19crowdwsa}. Despite the fact that these methods were designed in the context of machine translation, they are positioned as general methods for text aggregation and hence apply to our problem. Overall, the three methods mentioned above constitute a pool of available baselines for our problem. In Section~\ref{section:evaluation}, we return to these methods for additional discussion and evaluation.

\textbf{Data Collection} Novotney and Callison-Burch~\citep{novotney10cheap} annotated about thirty hours of audio recordings on MTurk to evaluate the quality of crowdsourced transcriptions by measuring the accuracy of a speech recognition model trained on this data. They concluded that quantity outweighs quality, that is, a larger number of recordings annotated once each  led to a higher accuracy than a smaller number of recordings annotated multiple times with subsequent aggregation. We note, however, that this conclusion may itself be influenced by an absence of a principled aggregation algorithm. Additionally, it is not always the case that one can obtain more data for annotation. Thus, in our work, we complement that study by investigating an orthogonal problem of developing better aggregation algorithms. For this, in our data-collection procedure, we obtain more annotations for each recording (seven vs. three) to give algorithm designers more freedom in using our data. Finally, to the best of our knowledge, the dataset collected by Novotney and Callison-Burch is not publicly available.

Another relevant contribution is a small dataset of translations from Japanese to English constructed by Li~\citep{li19crowdwsa}. Each sentence in this dataset is associated with ten crowdsourced translations and a ground truth translation. We treat this data as a \emph{baseline dataset} and return to it in Sections~\ref{section:analysis} and~\ref{section:evaluation}.

Several other works construct benchmark datasets for automated speech recognition without relying on crowdsourced annotation of audios. Specifically, \libri{}~\citep{panayotov15librispeech} (discussed in detail below) and \giga{}~\citep{chen2021gigaspeech} build on audios with known transcriptions (e.g., audio books or videos with human-generated captions). Starting from annotated long audios, they split the recordings into smaller segments and carefully align these segments with the ground truth texts. While this approach may result in high fidelity datasets, its applicability is limited to domains with pre-existing annotated recordings. Another clever approach is used in the \common{} dataset~\citep{commonvoice2020}. \common{} is constructed by starting from short ground truth texts and then crowdsourcing speech recordings of these texts. We note that this approach is complementary to ours (start from audios and then crowdsource transcriptions) and the choice between the two approaches may be application-dependent.

\textbf{Other Approaches} Several papers propose procedures to crowdsource high-quality annotations while avoiding automated aggregation of texts. One approach~\citep{lee2011transcription, McGraw2012AutomatingCL} it to develop a multi-stage process in which initial transcriptions are improved in several rounds of
post-processing. While this pipeline offers an appealing alternative to automated aggregation, it is much more complicated and may provide unstable quality due to variations in workers' accuracy. Another approach~\citep{bernstein10soylent} is to reduce the aggregation of texts to the conventional classification problem. Specifically, given noisy annotations obtained in the first stage, a requester can hire an additional pool of workers to listen to original recordings and vote for the best annotation from the given set, thereby avoiding the challenging step of learning from noisy texts. However, this approach is associated with increased costs, and its accuracy is fundamentally limited by the accuracy of the best annotation produced in the first stage. In contrast, aggregation-based approach that we focus on in this work can in principle result in transcriptions that are better than all initial transcriptions of the corresponding recordings.


\section{Data Source}
\label{section:acquisition}

In this section, we begin the description of our data collection procedure by introducing the pool of speech recordings that we annotate in Section~\ref{section:annotation}. Table~\ref{table:librisummary} gives an overview of our data sources.

\textbf{LibriSpeech Benchmark (cf. Contribution 1)} \libri{} is a famous benchmark for comparing speech recognition systems that consists of approximately 1,000 hours of read English speech derived from audiobooks and split into small segments (\url{https://www.openslr.org/12}). Specifically, \libri{} consists of two subsets --- ``\emph{clean}'' and ``\emph{other}''. The \emph{clean} subset contains recordings of higher quality with accents of the speaker being closer to the US English, while the \emph{other} subset contains recordings that are more challenging for recognition. To achieve a better diversity of our data, we use both the \emph{other} and \emph{clean} parts of \libri{}: our initial pool of recordings comprises full dev and test sets as well as a part of the \emph{clean} train set (11,000 recordings selected uniformaly at random from the \emph{train-clean-100} subset of \libri{}). An important feature of the \libri{} dataset is its gender balance -- approximately half of the recordings are made by female speakers. Thus, the recordings we use in this work are also gender-balanced.
\begin{table}[tb]
  \caption{\label{table:librisummary}
  Summary statistics for the source data used in this work. ``Spkrs'' stands for ``speakers'', letters M and F stand for male and female, respectively.}
  \centering
  \begin{footnotesize}
   \resizebox{\textwidth}{!}{\begin{tabular}{lcccrrrr}
  \toprule
  \textbf{Source Dataset} & \textbf{Version} & \textbf{Language} & \textbf{Nature} & \textbf{Total Length, hrs} & \textbf{\# Recordings} & \textbf{\# F Spkrs} & \textbf{\# M Spkrs} \\\midrule
  \multirow{4}{*}{\libri} & train-clean   & \multirow{5}{*}{English} & \multirow{5}{*}{Real} &  & 11,000 & 52    & 46   \\
  & dev-clean   & &  &  &  &     &    \\
  & dev-other   &  & &  &  &     &    \\ 
   &test-clean  &  & &  &  &     &    \\
  & test-other  &  & &   &  &     &    \\
  \midrule
  \news & Ru     & Russian & Synthetic &  &  &  &  \\
  \bottomrule
  \end{tabular}}
  \end{footnotesize}
\end{table}

\paragraph{Under-Resourced Domains (cf. Contribution 2)} \libri{} is a rich source of recordings, but it is focused on a specific domain of audiobooks and contains recordings of English speech only. Thus, the aggregation algorithms trained on the annotations of \libri{} we collect in Section~\ref{section:annotation} may not generalize to other domains and languages due to potential differences in workers' behaviour. To alleviate this issue, we propose the following pipeline to obtain domain-specific datasets for fine-tuning or evaluating aggregation methods:
\begin{enumerate}[itemsep=2pt, leftmargin=15pt, topsep=0pt]
  \item \label{step:synthetic} Obtain texts from the target under-resourced domain.  
  \item Use speech synthesis tools to construct recordings of texts collected in the previous step.
  \item Obtain annotations of these recordings using crowdsourcing.\footnote{Note that this data captures the behavior of real workers in the target domain modulo potential differences induced by the use of a synthetic speech generator.}
\end{enumerate}

The dataset constructed in this procedure can be used to fine-tune (evaluate) aggregation methods on data from novel domains. In this work, we demonstrate this pipeline by collecting a dataset of crowdsourced annotations of Russian speech recordings. Let us now describe the first two steps of the pipeline and introduce the second pool of synthetic recordings that we will annotate in Section~\ref{section:annotation}. 

\underline{\emph{Texts from a Target Domain}} For the sake of the example, we use the domain of news as our target domain. For this, we take sentences in Russian from the test set of the machine translation shared task executed as a part of the Eights and Ninth Workshops on Statistical Machine Translation~\citep{secondsmtworkshop, smtworkshop}. To support reliable evaluation, we additionally filter these texts to align their formatting with that used in the \libri{} dataset. A detailed description of this stage is given in Appendix~\ref{section:filtering}.

\underline{\emph{Recording}} Synthetic recording is the crux of our approach as it allows us to obtain recordings with known ground truth transcriptions without involving costly human speakers. In this example, we rely on Yandex SpeechKit~\footnote{\url{https://cloud.yandex.com/en-ru/services/speechkit}} --- an industry-level tool for speech synthesis --- to obtain recordings of the ground truth texts. Importantly, Yandex SpeechKit gives access to both ``male'' and ``female'' voices, as well as to different intonations (neutral and evil). Thus, in the recording stage, we choose the ``gender'' and intonation for each recording uniformly at random, ensuring the diversity of our synthetic dataset.

Following the procedure outlined above, we obtain 3,091 recordings of Russian speech that we title \news{}. Table~\ref{table:librisummary} gives summary statistics for two pools of recordings used in this work. In the next section, we will use audios from \libri{} and \news{} to construct datasets \speech{} (based on \libri{}) and \vox{} (based on \news{}) of crowdsourced audio annotations.

\section{Data Annotation}
\label{section:annotation}

With datasets of speech recordings prepared in Section~\ref{section:acquisition}, we now proceed to the annotation stage in which we build our \speech{} and \vox{} datasets. In that, we introduce the pipeline (Figure~\ref{fig:pipeline}) that we used to gather reliable transcriptions on the Toloka crowdsourcing platform.~\footnote{Code to fully reproduce the pipeline is available at \url{https://github.com/Toloka/CrowdSpeech}.} Additionally, throughout this section, we reflect on our data collection experience and give practical advice that may be useful for researchers and practitioners. 
\begin{figure}[t]
  \centering
  \includegraphics[width=11cm]{pipeline.pdf}
  \caption{\label{fig:pipeline}Schematic representation of the data annotation pipeline.}
\end{figure}

\subsection{Task Design}

We begin the exposition of our pipeline with a discussion of instructions, interface, and compensations.

\textbf{Instructions and Interface} 
Despite the task of audio transcription may sound natural to workers, there are important nuances that should be captured in the instructions and the interface. First, as our ground truth annotations have some specific formatting, we put a strong emphasis on conveying the transcription rules to the workers in the instructions. Next, throughout the task, workers may experience technical difficulties with some recordings, and we design the interface with that in mind. Specifically, at the beginning of the task, workers are asked whether the given audio plays well in their browser. The positive answer to this question  triggers the text field, and the negative answer allows workers to report the technical issue without contaminating our data with an arbitrary transcription. The full version of instructions and a screenshot of the interface are given in Appendices~\ref{section:task_instructions} and~\ref{section:screenshot}.


\textbf{Compensation} The recordings we annotate in this work can roughly be grouped by the level of difficulty: \news{} and the \emph{clean} subset of \libri{} are relatively easy while the \emph{other} subset of \libri{} is harder to annotate. Thus, we issue a compensation of one cent (respectively, three cents) per annotation for recordings in the first (respectively, second) group. This amount of compensation was selected for the following reasons: (i) a typical amount of compensation for similar tasks on Toloka is one cent per recording; (ii) several past speech recognition studies that employed crowdsourcing~\citep{audhkhasi11accurate,marge10turk,novotney10cheap} were issuing a compensation ranging from 0.5 cents to 5 cents for annotation of a recording of comparable length; (iii) a large fraction of workers on Toloka and other crowdsourcing platforms are residents of countries with a low minimum hourly wage.



\textbf{Workers Well-Being} Throughout the experiment, we were monitoring various quantities related to workers well-being. Specifically, the hourly compensation for active workers was close to or even exceeded the minimum hourly wage in Russia -- the country of residence for primary investigators of this study. Additionally, our projects received mean quality ratings of 4.5 and above (out of 5) in anonymous surveys of workers, suggesting that workers deemed the work conditions reasonable.

\emph{Practical Comment.} In preliminary trials, we experimented with issuing compensations to workers even when they were unable to play the audio due to self-reported technical difficulties. Unfortunately, this setup resulted in workers reporting technical difficulties for a huge share of the tasks. Once we switched to the compensation for annotated recordings only, the amount of self-reported technical problems reduced drastically without affecting the quality of annotations. This observation suggests a spamming behavior in the original setup.

\subsection{Worker Selection and Quality Control}

Another key aspect of crowdsourcing data collection is to recruit the right population of workers for the task. For this, we make our task available to only those workers who self-report the knowledge of the language of the task: English for \speech{} and Russian for \vox{}. Additionally, we implement an \emph{Entrance Exam}. For this, we ask all incoming eligible workers to annotate ten audio recordings. We then compute our target metric --- \emph{Word Error Rate (WER)} --- on these recordings and accept to the main task all workers who achieve WER of 40\% or less (the smaller the value of the metric, the higher the quality of annotation).
In total, the acceptance rate of our exam was 64\%.

Importantly, to achieve a consistent level of annotation quality, it is crucial to control the ability of workers not only at the beginning but also throughout the task. For this, we implement the following rules to detect spammers and workers who consistently provide erroneous annotations:

\begin{itemize}[itemsep=2pt, leftmargin=15pt, topsep=0pt]
  \item \emph{Spam-detection rules.} Spammers often try to complete as many tasks as possible before getting detected and removed from the platform. To mitigate this behavior, we use a rule that automatically blocks workers from our projects if they complete two or more tasks in less than ten seconds.
  
  \item \emph{Golden set questions.} We use golden set questions to continuously monitor the quality of annotations supplied by workers. If the mean value of the WER metric over the last five completed golden set questions was reaching , we were blocking the worker from taking more tasks.\footnote{For the sake of quality control, in this work we treat all recordings as golden set questions. In practice, one could annotate a handful of examples manually and use them as golden set questions.}
\end{itemize}

\emph{Practical Comment.} Workers may be hesitant to participate in the tasks if there is a risk that all their work is rejected without compensation. To avoid the additional burden on workers, we follow best practices of (i) compensating the exam for all workers who attempted it, irrespective of whether they passed the bar for the main task or not; (ii) issuing compensations to the workers for the tasks they have completed before being flagged by our quality control rules (these tasks are also included in the final datasets).

\subsection{Running the Task}

Having the pipeline prepared, we annotate each of the six sets of recordings described in Table~\ref{table:librisummary} to construct our \speech{} and \vox{} datasets. In that, we annotate each set of recordings in a separate pool (five pools for \speech{} with a common entrance exam and one pool for \vox{} with a separate exam), keeping the task setup identical modulo the use of instructions in Russian for the \vox{} dataset.  Each individual recording was annotated by seven workers. If a worker reported technical issues on any recording, that recording was reassigned to another worker. 

As a result of this procedure, we obtained six pools of annotated recordings --- first five of these pools comprise the \speech{} dataset and the last pool of Russian recordings comprises the \vox{} dataset. We release annotated data as well as the Python code to replicate our pipeline in the GitHub repository referenced on the first page of this manuscript.

\emph{Privacy Remark.} The Toloka crowdsourcing platform associates workers with unique identifiers and returns these identifiers to the requester. To further protect the data, we additionally encode each identifier with an integer that is eventually reported in our released datasets. 

\section{\label{section:analysis}Exploratory Analysis of Collected Datasets}

Having constructed the \speech{} and \vox{} datasets, we now proceed to the analysis of collected data on various dimensions, specifically focusing on the reliability of annotators. Before we delve into details, let us make some important remarks.

First, for the sake of analysis, we slightly post-process the annotations obtained in our datasets by removing punctuation marks and making all sentences lowercased. This post-processing step is only needed to ensure consistency with the ground truth data but does not conceptually affect the quality of collected data. Second, when possible, we compare our datasets with \crowd{} --- a dataset of crowdsourced translations (supplied with ground truth translations) constructed by Li et al.~\citep{li19crowdwsa}. While this dataset is constructed in a different application, it is the largest publicly available dataset for the problem of noisy text aggregation. Hence, it is interesting to juxtapose it to our data. With these preliminaries, we are now ready to present the exploratory analysis of collected datasets.

\subsection{Overview of Annotated Datasets}

A general overview of the collected datasets is presented in Table~\ref{tab:length}. First, observe that in total, we have collected 176,519 annotations of 25,217 recordings made by 4,386 unique workers. Thus, our datasets are several orders of magnitude larger than \crowd{}. To the best of our knowledge, our data is also the largest publicly available data of crowdsourced texts. 

Second, it appears that in all datasets, the mean length of the crowdsourced annotations (translations) is slightly smaller than the mean length of the ground truth texts. This observation suggests that workers tend to skip some words in both the annotation and translation tasks. 

Finally, Figure~\ref{fig:tasks_per_worker} shows the distribution of the number of tasks completed by a worker for data collected in this study. Observe that these distributions differ significantly between projects, likely being dependent on the task difficulty. It would be interesting to see if the aggregation algorithms can adapt for the changing distribution to provide a consistent improvement on different kinds of projects. 


\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{img/tasks_per_worker_alt.pdf}
  \caption{\label{fig:tasks_per_worker} Distribution of the number of tasks completed by a worker. For brevity, we use only dev and test subsets of \speech{} and combine dev-clean and test-clean subsets (similarly, dev-other and test-other subsets) together.}
\end{figure}


\begin{table}[b]
  \caption{Overview of datasets collected in this work and comparison with \crowd{}.}
  \label{tab:length}
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{lrrrrrr}\toprule
  & & \multicolumn{2}{c}{\textbf{Mean Sentence Length, words}} \\\cmidrule(lr){3-4}
  \textbf{Dataset} & \textbf{Version} & \textbf{Ground Truth} & \textbf{Crowdsourced} & \textbf{\# Recordings} & \textbf{\# Workers} & \textbf{\# Answers} \\\midrule
  \multirow{5}{*}{\speech} & train-clean &  &  &  &  &  \\
   & dev-clean &  &  &  &  &  \\
   & dev-other &  &  &  &  &  \\
   & test-clean &  &  &  &  &  \\
   & test-other &  &  &  &  &  \\
    \midrule
  {\vox} & \textsc{ru} &  &  &  &  &  \\\midrule
  \multirow{3}{*}{\crowd} & J1 &  &  &  &  &  \\
   & T1 &  &  &  &  &  \\
   & T2 &  &  &  &  &  \\\bottomrule
  \end{tabular}}
\end{table}

\subsection{Inter-Rater Agreement}

To evaluate whether the workers understood our task correctly, we compute Krippendorff's  \citep{Krippendorff:18}, a chance-corrected inter-rater agreement measure that handles missing values and allows an arbitrary distance function. In this work, we compute the value of Krippendorff's  using the Levenshtein distance (i.e., edit distance). Since the computation of  is time-consuming as it iterates over all the possible co-occurring item pairs, we obtain and report the sampling estimate of this value as follows. For each sample in the set of 10,000 samples, we randomly select 100 different audio recordings with replacement and compute  for all the transcriptions obtained for these recordings. We then take the mean of these values across all iterations and report it in Table~\ref{tab:alpha}.

\begin{table}[t]
  \caption{Inter-rater agreement according to the Krippendorff's  with Levenshtein distance. Higher values indicate higher reliability.}
  \centering
  \begin{tabular}{lrrr}\toprule
  \textbf{Dataset} & \textbf{Version} & \textbf{Overlap} & \textbf{Krippendorff's}   \\\midrule
  \multirow{5}{*}{\speech} & train-clean &  &   \\
   & dev-clean &  &   \\
   & dev-other &  &   \\
   & test-clean &  &   \\
   & test-other &  &  \\
  \midrule
  {\vox} & \textsc{ru} &  &   \\\midrule
  \multirow{3}{*}{\crowd} & J1 &  &  \\
   & T1 &  &   \\
   & T2 &  &   \\\bottomrule
  \end{tabular}
  \label{tab:alpha}
\end{table}

Following recommendations of Krippendorff~\citep{Krippendorff:18}, we note that values of  suggest that annotations obtained for our dataset are reliable. Thus, we conclude that workers on Toloka successfully understood and performed our audio transcription task. Interestingly, the \crowd{} dataset demonstrates a much lower agreement between raters. We hypothesize that this discrepancy is due to the different natures of the tasks. Indeed, in the case of translations, there may be multiple equally good translations, and even ideal translators may have some disagreement. In contrast, the audio transcription task has unique underlying ground truth (ideal annotators can be in perfect agreement).

\section{Evaluation}
\label{section:evaluation}

In this section, we evaluate the existing and novel methods for aggregation of noisy texts on our data. Specifically, in Sections~\ref{section:baseline_intro} and~\ref{section:baseline_result} we analyze several existing methods. Next, in Section~\ref{section:st} we introduce a novel method developed on our data and compare it against the best baseline.

\subsection{Baseline Methods}
\label{section:baseline_intro}

In our evaluations, we consider the following baseline methods, using implementations from the Crowd-Kit library (\url{https://github.com/Toloka/crowd-kit}) when available.

\begin{itemize}[itemsep=2pt, leftmargin=15pt, topsep=0pt]
    \item \textbf{Random} A naive baseline that uniformly at random picks one of the  annotations to be the answer. 
    
    \item \textbf{ROVER} \emph{Recognizer Output Voting Error Reduction}~\cite{fiscus97rover} was originally designed to combine the output of several different automatic speech recognition systems but was also demonstrated to work well on crowdsourced sequences~\citep{audhkhasi11accurate,evanini10nonnative,lee2011transcription,marge10turk}. Under the hood, it aligns given sequences using dynamic programming and then computes the majority vote on each token.

    \item \textbf{RASA} \emph{Reliability Aware Sequence Aggregation}~\cite{li19crowdwsa} employs large-scale language models for aggregating texts. It encodes all worker responses using RoBERTa~\cite{Liu19:roberta} (RuBERT\footnote{\url{https://huggingface.co/DeepPavlov/rubert-base-cased}} for the Russian language) and iteratively updates the mean weighted embedding of workers' answers together with estimates of workers' reliabilities. Finally, the method defines the final answer to be the response closest to the aggregated embedding based on the notion of cosine distance.

    \item \textbf{HRRASA} We also use a modification of RASA called HRRASA~\citep{li20hrrasa} that, besides the global reliabilities, uses local reliabilities represented by the distance from a particular response to other responses for the task. In the original paper~\citep{li20hrrasa}, GLEU~\cite{napoles-EtAl:2015:ACL-IJCNLP,napoles2016gleu} metric was suggested to calculate the distance between sequences, and we resort to this choice in our experiments as well.

\end{itemize}

In addition to comparing the baselines, we want to make a rough conclusion on whether any of them demonstrate the optimal performance on our data. Note that it may be infeasible to uncover all transcriptions with absolute accuracy as for some recordings, the noise in annotations could vanish out all the signal. To obtain a more reasonable estimate of \emph{achievable} performance, we introduce the \textbf{Oracle} aggregation algorithm to the comparison. For each recording, it enjoys the knowledge of the ground truth and selects \emph{the best} transcription provided by the workers as its answer. 

The Oracle method achieves the maximum accuracy that can be reached by an aggregation algorithm restricted to the set of transcriptions provided by the workers. Thus, Oracle gives a \emph{weak} estimate of the \emph{achievable} quality as its accuracy could be improved by an algorithm that is allowed to modify transcriptions provided by the workers. Nevertheless, in the analysis below, we focus on the \emph{gap} between the baselines and the Oracle to estimate if there is some room for improvement on our data.


\subsection{Performance of Baseline Methods}
\label{section:baseline_result}


To evaluate baseline methods, we run them on each of the datasets under consideration excluding the train set of \speech{} as its main purpose is model training. We then compute the mean value of WER (Word Error Rate) over all recordings in each dataset and report it in Table~\ref{tab:comparison}. First, we note that when the quality of recordings is good, as in the case of the semi-synthetic \vox{} dataset, non-trivial baseline methods achieve a near-perfect performance. This observation suggests that when the level of noise in collected annotations is relatively low, existing baselines satisfy the needs of practitioners.


However, observe that on the more challenging \speech{} dataset, there is a consistent gap between all baselines and Oracle, with the gap being larger for more difficult subsets (dev-other and test-other). This observation indicates that there is room for the development of better aggregation methods that keep up with, or even exceed, the performance of Oracle on more difficult tasks.

Finally, we note that the performance of all aggregation methods, including Oracle, is much weaker on the \crowd{} dataset. This effect is likely an artifact of the subjective nature of the machine translation task, which, in contrast to the speech recognition task, does not have a unique ground truth answer. Thus, 
\crowd{} may not be the best choice to design aggregation methods for objective tasks such as speech recognition. The same observation applies to the methods developed for that dataset (RASA and HRRASA): Table~\ref{tab:comparison} indicates that on our data a simple ROVER baseline is always superior to these more advanced algorithms. Of course, a symmetric observation applies to our \speech{} and \vox{} which may also be suboptimal for the machine translation task. 

\begin{table}[t]
\centering
\caption{\label{tab:comparison}Comparison of the baselines and the oracle performance. Evaluation criterion is the average word error rate (WER) and lower values are better.}
\begin{tabular}{lrrrrrr}\toprule
\textbf{Dataset} & \textbf{Version} & \textbf{Oracle} & \textbf{Random} &  \textbf{ROVER} & \textbf{RASA} & \textbf{HRRASA} \\\midrule
\multirow{4}{*}{\speech}
& dev-clean &  &  &   &  &  \\
& dev-other &  &  &  &  &  \\
& test-clean &  &  &   &  &  \\
& test-other &   &  & &  &  \\\midrule
{\vox} & \textsc{ru} &  &  &   &  &  \\\midrule
\multirow{3}{*}{\crowd} & J1 &  &  &   &  &  \\
& T1 &  &  &   &  & \\
& T2 &  &  &   &  &  \\\bottomrule
\end{tabular}
\end{table}


\subsection{Novel Methods Developed on Our Data}
\label{section:st}

In parallel with preparing this paper, we have designed and executed a shared task on developing aggregation methods for crowdsourced audio transcriptions \citep{VLDB2021Challenge}. The competitive nature of the task does not allow us to use \libri{} audios as it would result in the test data leakage. To avoid this problem, in the shared task we relied on the pipeline used to construct the \vox{} dataset in the present paper. Specifically, we crowdsourced annotations of synthetic recordings of passages from Wikipedia and books~\citep{soskkobayashi2018bookcorpus}.

One of the best results~\citep{pletenev21noisy} in this shared task was demonstrated by a carefully fine-tuned  T5 model \citep{JMLR:v21:20-074}. With permission of the author, we evaluate their approach on the test sets of the \speech{} dataset collected in this paper. For this, we introduce three additional models (see details in Appendix~\ref{section:t5}):

\begin{itemize}[itemsep=0pt, leftmargin=15pt, topsep=0pt]
    \item  First, we consider T5 model trained on the shared task data [\textbf{T5 (ST)}]
    \item  Second, we fine-tune T5 (ST) on the development sets of \speech{} [\textbf{T5 (ST+FT)}]
    \item  Third, we fine-tune the original T5~\citep{JMLR:v21:20-074} on the train-clean subset of \speech{} [\textbf{T5 (FT)}]
\end{itemize}

Table~\ref{table:new_results} juxtaposes these models to the best of the available baselines (ROVER) on the test sets of \speech{}. First, we note that all T5-based models significantly outperform ROVER --- the baseline that remained unchallenged for more than twenty years --- setting new state-of-the-art results. Second, we observe that fine-tuning on the domain-specific data is crucial for our task as T5 (ST+FT) model is superior to T5 (ST) on both test sets. Similarly, T5 (FT) that got fine-tuned on the train-clean subset of \speech{} (which is much larger than dev-clean)  outperforms other models on the test-clean set. Not surprisingly, however, it demonstrates lower accuracy on the test-other set because, in contrast to T5 (ST + FT), it did not get to see data from the \emph{other} subset of \speech{}.

With these observation, we make the following conclusions:
\begin{itemize}[itemsep=0pt, leftmargin=15pt, topsep=0pt]
    \item First, there is initial evidence that the data we release in this paper is instrumental in developing novel principled methods for aggregation of crowdsourced annotations. 
    
    \item Second, observe that T5-based models do not use the fact that each worker provides multiple annotations in the dataset and do not estimate workers' expertise. In contrast, this information is known to be crucial for aggregation of categorical data~\citep{Dawid1979MaximumLE,whitehill09count}. Thus, we believe that it is possible to further improve the results by coupling T5 with some way to estimate worker skills.
    
    \item Third, note that the T5-based method is designed on the data collected through the semi-synthetic procedure used to construct \vox{}. Given that the strong performance of thes method carried over to the realistic \speech{} dataset, we conclude that the semi-synthetic data may be useful to quickly explore new domains in which no annotated recordings of human voice exist.
\end{itemize}

Finally, we refer the reader to Appendix~\ref{section:error} where we provide additional comparison of models in terms of types of errors they make.

\begin{table}[t]
\centering
\caption{\label{table:new_results}Comparison of the T5-based method developed on our shared task with ROVER --- the strongest of the existing baselines. Models are compared on WER and lower values are better.}
\begin{tabular}{lrrrrrr}\toprule
\textbf{Dataset} & \textbf{Version}  & \textbf{Oracle} & \textbf{ROVER} & \textbf{T5 (ST)} & \textbf{T5 (ST+FT)} & \textbf{T5 (FT)} \\\midrule
\multirow{2}{*}{\speech} & test-clean &   &  &  &  &  \\
& test-other &  &  &  &  &  \\
\midrule
\end{tabular}
\end{table}


\section{Conclusion}
\label{section:conclusion}

In this work, we collected and released \speech{} --- the first publicly available large-scale dataset of crowdsourced audio transcriptions. Based on evaluations of existing and novel methods on our data, we believe that our work will enable researchers to develop principled algorithms for learning from noisy texts in the crowdsourcing setting. Additionally, we proposed an automated pipeline for collecting semi-synthetic datasets of crowdsourced audio transcriptions in under-resourced domains. We demonstrated this pipeline by constructing \vox{} --- a Russian counterpart of \speech{}.

In the end, we should mention some limitations of our work. First, we admit that the use of speech synthesis techniques could affect the distribution of errors people make when annotating audios, thereby affecting the generalization ability of aggregation tools trained on \vox{}. Second, in this work, we annotated our datasets in an industry-level pipeline, which resulted in annotations of high quality. It would be interesting to additionally collect datasets under less stringent quality control rules or for more challenging data to make our data even more diverse in terms of complexity. 

Better understanding and addressing these limitations is an interesting direction for future research. With these caveats, we encourage researchers and practitioners to use our data judiciously and to carefully evaluate all the risks and benefits in their specific application.

\section*{Acknowledgements \& Author Contributions}

I.S. and N.P. designed the setup of the problem and the data-collection pipeline. N.P. collected data on Toloka. N.P. and D.U. conducted data analysis. All authors contributed to the writeup.

The work of I.S. was supported in part by NSF grants CIF 1763734 and CAREER: CIF 1942124.

\bibliographystyle{plain}
\bibliography{voxdiy}

\newpage

\appendix

\textbf{\LARGE Appendix}

We now provide supplementary materials. In Appendix~\ref{section:task_instructions}, we provide the text of instructions given to workers on the Toloka crowdsourcing platform. Appendix~\ref{section:screenshot} provides a screenshot of the interface; details of the filtering procedure mentioned in Section~\ref{section:acquisition} are given in Appendix~\ref{section:filtering}. In Appendix~\ref{section:t5}, we provide additional details on the T5-based models. Finally, Appendix~\ref{section:error} reports the analysis of errors made by humans and algorithms on our data. 

\section{Task Instruction}
\label{section:task_instructions}

Figure~\ref{fig:instruction} displays the task instruction presented to the workers. HTML template of the instruction is also available on our GitHub data release.

\begin{figure}[htbp]
  \centering
  \fbox{\parbox{\textwidth}{\sffamily\scriptsize\textbf{Transcription Rules}

\begin{itemize}
  \item All recordings have a clearly identifiable speaker -- you need to  transcribe their speech only. If there is some background speech, ignore it.
  \item Your transcription must use only letters and apostrophes ('). Do not use digits and any punctuation marks (including the question mark ``?'') except the apostrophe.
\end{itemize}

\textsl{Important Details:}

\begin{enumerate}
  \item If you hear a number, spell it out in words (e.g., 19 {-\textgreater} nineteen).
  \item Even when the grammar rules and speaker's intonation suggest the punctuation mark, omit it (except the apostrophe). We need to obtain texts without punctuation marks.
  \item Use the apostrophe according to the grammar rules: \begin{itemize}
    \item To show omissions of letters (e.g., constructions like I'm, don't,
shouldn't).

    \textit{Importantly:} listen carefully to what the speaker says. If they use a full form (e.g., do not), you must also write the full form. If they use a shortform (e.g., I'm), you should also write the short form.
    \item To show possession (e.g., Ivan's pen)
    \item To form plurals of letters/abbreviations etc. (e.g., She got two A's in the biology exam)
  \end{itemize}
\end{enumerate}

\textbf{Pipeline}\smallskip

Follow a simple pipeline to perform the task:
\begin{enumerate}
\item Play the audio and carefully listen to the speech.

\textit{Important:} You must listen to the complete audio record to submit your response.

\textit{Technical Difficulties:} If the audio does not play, or there is no voice in the recording, or any other technical difficulty arises, answer ``No'' to the ``Does the audio play properly?'' question and proceed to the next task (skip all steps below).

\textit{Hint:} It is best to use headphones to perform this task -- you will hear the speech better.
\item Transcribe the audio and type the transcription into the text field

\textit{Important:} You must follow the transcription rules outlined above.

\textit{Hint:} You can play the audio multiple times and pause it at any point.
Please do your best to produce a high-quality transcription.
\item Carefully check your transcription for typos and mistakes

\textit{Important:} Speech in the recordings should be grammatically correct. But if you are sure that the speaker makes a mistake, do not fix it. Your goal is to provide accurate transcriptions.

\textit{Hint:} Listen to the audio in full one more time to double-check your transcription.
\end{enumerate}

\textbf{Exam}\smallskip

To start working on the main task, you have to pass a qualification exam. In the exam, you need to transcribe 10 audios and you will be accepted to the main task if the quality of your transcriptions is high enough.

\medskip\textbf{FAQ}\smallskip

Q: Do I get paid for the tasks I had technical difficulties with?\\
A: Unfortunately, no. We will reject these tasks, but it won't hurt your status or payment for other tasks.

\smallskip

Q: I have technical difficulties with all audios; what to do?\\
A: Try to use another browser (we recommend Google Chrome).

\smallskip

Q: I do not understand some words in the audio; what to do?\\
A: Some audios are hard to transcribe, and it is ok. Listen to it once more and write your best guess if you do not know for sure.

\smallskip

Q: When will I get paid?\\
A: We try to review the tasks within several hours, but occasionally it may take up to a day.

\smallskip

Q: All my tasks are rejected. Why?\\
A: We have some spam detection rules in place and reject users who spam. Additionally, we have some ground truth transcriptions and may block a worker if they consistently supply clearly inadequate transcriptions. If you believe that we made a mistake, shoot us a note.
 }}
  \caption{Task instruction presented to the workers.}
  \label{fig:instruction}
\end{figure}

\newpage

\section{Task Interface}
\label{section:screenshot}

Figure~\ref{fig:task_interface} demonstrates the interface of the task available to workers. In order to activate the text field to enter the 
annotation, a worker needs to positively answer the first question. The negative answer to the first question indicates a technical issue and enables the worker to skip the question.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{img/task_interface.png}
    \caption{Task interface. There was a single audio-annotation task on each page, and a worker could complete as many tasks as they like (subject to availability of tasks).}
    \label{fig:task_interface}
\end{figure}

\section{Additional Filtering of the \news{} Dataset}
\label{section:filtering}

In order to align the format of sentences in the \news{} dataset to the format of the \libri{} ground truth, we perform the following filtering steps on the \news{} dataset.

\begin{itemize}
  \item \emph{Remove sentences that contain digits.} From the workers' standpoint, numerals and spelled-out numbers in the original texts do not make a difference because they are pronounced in the same way. However, to compute the accuracy of annotations (both crowdsourced and aggregated), we need to compare a target text against the ground truth text. Thus, it is crucial to ensure that numbers in the ground-truth texts are written in a consistent  manner. To ensure this consistency, we remove sentences that contain digits.
  
  \item \emph{Remove sentences that contain specific abbreviations.} Some texts in the source contain abbreviations that are special for the Russian written language (e.g., a Russian equivalent of the word ``doctor'' is sometimes shortened to ``d-r''). Such abbreviations are not used in the spoken language so we remove sentences that contains them from the pool.
  
  \item \emph{Remove sentences that contain letters from non-Russian alphabets.} Finally, some sentences in the initial pool contain words from other languages (e.g., names of companies). We also remove such sentences from the pool because these cannot be properly annotated using Russian alphabet.
  
\end{itemize}

In practice, instead of removing sentences that fall in the aforementioned categories, one could alternatively pre-process sentences to convert numerals to the spelled-out form and adjust instructions to account for abbreviations and non-Cyrillic letters. However, careful implementation of such changes requires a non-trivial amount of work and we do not take this route in this paper as it is orthogonal to our main contributions.


\section{T5, a Transformer-based Model}
\label{section:t5}

In this section, we give a high-level overview of the T5 model --- a workhorse for comparisons we described in Section~\ref{section:st}. Additionally, we provide details on the fine-tuning procedure.

\paragraph{T5 Model.} Our evaluations in Section~\ref{section:st} rely on a pre-trained T5-large model~\citep{JMLR:v21:20-074} --- a Transformer-based model that is designed to solve various text-to-text tasks. By using T5, we reduce the problem of aggregation of crowdsourced transcriptions to a more studied text summarization problem. In our case, the input to the model consists of concatenated transcriptions provided by crowd workers for a particular recording, and the T5 model outputs a single final transcription. 

\paragraph{Details of Fine-Tuning} The main power of the T5 model is its ability to quickly adjust to new tasks: starting from initial weights provided by Google (\url{https://huggingface.co/t5-large}), one can utilize the knowledge of pre-trained T5 while letting the model to pick up the new task. In this work, both T5 (ST + FT) and T5 (FT) models are fine-tuned on the corresponding data for 8 epochs with 10\% of data set aside for the validation purposes. Specifically, to train the T5 (FT) model, we use the original weights provided by Google as the initialization. For T5 (FT + ST), we started with the weights reported by Pletenev~\citep{pletenev21noisy}. The fine-tuning procedure directly follows the summarization training approach by HuggingFace.\footnote{\url{https://github.com/huggingface/transformers/tree/master/examples/pytorch/summarization}} 




\section{Error Analysis}
\label{section:error}

In order to further understand the difference between the performance of aggregation algorithms, we now conduct an analysis of errors made by human annotators and aggregation algorithms discussed in Section~\ref{section:evaluation}. Specifically, we rely on automated and manual analysis and evaluate (i) the impact of errors in human annotations on the performance of the algorithms and (ii) the causes of errors made by crowd workers. 


\paragraph{Automated Analysis} We begin from the automated analysis. For each example in the \speech{} test-clean dataset, we compute the WER scores of each human annotation and the output of each algorithm under comparison. We then treat transcriptions with zero WER as correct () and those with non-zero WER as incorrect (). To study the effect of the quality of human transcriptions on the accuracy of aggregation algorithms, we now compare the performance of the algorithms with breakdown by three different settings: 
\begin{itemize}[itemsep=0pt, leftmargin=15pt, topsep=0pt]
    \item \textbf{All Correct} All workers provided the correct transcription
    
    \item \textbf{Has Correct} At least one worker provided the correct transcription
    
    \item \textbf{All Incorrect} All the workers provided an incorrect transcription
\end{itemize}

Similar to the experiments conducted in Section~\ref{section:evaluation}, we compare the performance of the baseline and novel methods with Oracle that always produces the best transcription submitted by the workers.

Results of the comparisons are summarized in Table~\ref{tab:errors}. Let us now make several important observations:
\begin{itemize}[itemsep=0pt, leftmargin=15pt, topsep=0pt]
    \item  \textbf{All Correct} First, we observe that none of the methods introduced errors if the crowd unanimously agreed in their correct response. 
    
    \item  \textbf{Has Correct} Next, as we increase the difficulty of the task, methods start to behave differently with T5 demonstrating better performance than other methods.
    
    \item \textbf{All Incorrect} Finally, in the most challenging category, RASA, HRRASA, and Oracle always produce an incorrect result with non-zero WER. In contrast, ROVER and T5 perform better and sometimes are able to recover correct transcriptions even when all transcriptions contain mistakes.
\end{itemize}

These observations corroborate the results presented in Section~\ref{section:evaluation} (Table~\ref{tab:comparison} and Table~\ref{table:new_results}) and demonstrate promise of methods that are not limited to crowd-generated transcriptions (ROVER and especially T5). 

As a side remark, we note that RASA and HRRASA showed identical performance in this experiment, while the former outperforms the latter in terms of the WER metric (Table~\ref{tab:comparison}). Inspection of the data reveals that HRRASA tends to choose transcriptions with a higher WER than RASA when they both make errors, thereby being scored worse on the WER metric.


\begin{table}[t]
\centering
\caption{\label{tab:errors} Comparison of the automated algorithms with breakdown by the human performance. Specifically, for each of the three regimes (All Correct, Has Correct, All Incorrect), we compare how often aggregation methods are able to come up with perfect transcriptions. Comparison is executed on the test-clean subset of \speech{}.}
\begin{tabular}{l*{10}{r}}\toprule
\multirow{2}{*}{\textbf{Crowd}} & \multicolumn{2}{c}{\textbf{ROVER}} & \multicolumn{2}{c}{\textbf{RASA}} & \multicolumn{2}{c}{\textbf{HRRASA}} & \multicolumn{2}{c}{\textbf{T5 (ST)}} & \multicolumn{2}{c}{\textbf{Oracle}} \\
&  &  &  &  &  &  &  &  &  &  \\\cmidrule{2-11}
All Correct &  &  &  &  &  &  &  &  &  &  \\
Has Correct &  &  &  &  &  &  &  &  &  &  \\
All Incorrect &  &  &  &  &  &  &  &  &  &  \\\bottomrule 
\end{tabular}
\end{table}

\paragraph{Manual Analysis} Next, we proceed to the manual analysis with a goal of understanding the causes of human errors. In that, we condition on a subset of hard recordings: those, that caused non-zero WER in outputs of all algorithms under consideration and sample 100 transcriptions from this subset for manual analysis. In the analysis, the authors of this paper independently classified human errors into three predefined categories:
\begin{itemize}[itemsep=0pt, leftmargin=15pt, topsep=0pt]
    \item \textbf{Task Difficulty} Some recordings were lengthy or contained unexpectedly difficult lexemes, such as rare words or proper nouns. Thus, the first cause of mistakes is the task difficulty.
    
    \item \textbf{Violation of Instructions} The second category corresponds to various violations of instructions: incorrect punctuation marks, wrong formatting of numbers, incomplete transcriptions -- all these mistakes were classified in this category.

    \item \textbf{Homophones} Finally, a transcription could be grammatically correct and meaningful, but contain homophones or verbs in wrong tenses that impact the WER score. We classified such cases in a separate category.
\end{itemize}


In the sample of 100 transcriptions that we annotated, homophones caused 55 errors, difficult or lengthy recordings caused 28 errors, and instruction breaks caused the remaining 17 errors. We evaluated the inter-rater agreement of our analysis using Krippendorff's alpha for nominal scale and found that our annotation is reliable in classifying error causes ().

\clearpage

\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments (e.g. for benchmarks)...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerNA{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNA{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerNA{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerYes{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerYes{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerYes{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerYes{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerYes{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerYes{}
\end{enumerate}

\end{enumerate}

\clearpage


\end{document}
