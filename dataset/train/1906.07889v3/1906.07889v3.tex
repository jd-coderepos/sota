
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\theequation}{S\arabic{equation}}
\setcounter{equation}{0}
\setcounter{figure}{0}

\maketitle

\section{Model implementation details}\label{sup_sec:implementation_details}
\subsection{Architecture}

\subsubsection{Keypoint detector}

Our goal is to encode the input image in terms of the locations of objects in the image \cite{jakab_conditional_2018, zhang_unsupervised_2018}. To this end, we use a \textbf{keypoint detector network} (Figure~\ref{fig:architecture_schematic}, bottom) that consists of  keypoint detectors. Ideally, each detector learns to detect a distinct object or object part. The detector network is implemented as a series of convolutional layers with stride 2 that reduce the input image  into a stack of keypoint detection score maps with  channels, , where . We use the softplus function  on the activations of the final layer to ensure the maps are positive.

The raw maps  are normalized to obtain detection weight maps ,

where  is the value of the -th channel of  at pixel . We then reduce each  to a single -coordinate by computing the weighted mean over pixel coordinates:

To model keypoint presence or absence, we compute the mean value of the raw detection score maps,

In summary, each keypoint is represented by a -triplet encoding its location in the image and its scale.

For \textbf{image reconstruction}, each keypoint is converted back into a pixel representation by creating a map  containing a Gaussian blob with standard deviation  at the location of the keypoint, scaled by :

The map  contains the same information as the keypoint tuple , but in a pixel representation that is suitable as input to the convolutional reconstructor network . The image is reconstructed as follows:

where  applies alternating convolutional layers and twofold bilinear upsampling until the  maps are expanded to the original image resolution,  denotes concatenation (here, channel-wise), and  is a network with the same architecture as  (except for the final softmax nonlinearity) that extracts image features from the first frame  to capture appearance information of the scene. 

The internal layers of the convolutional encoder and decoder are connected through leaky rectified linear units . L2 weight decay of  is applied to all convolutional kernels. To increase model capacity, we add one (for Basketball and DMCS) or two (for Human3.6M) additional size-preserving (stride 1) convolutional layers at each resolution scale of the detector and reconstructor. The image resolution is  pixels.

\subsubsection{Dynamics model}

The dynamics model (Figure~\ref{fig:architecture_schematic}, top) has the following components:

The \textbf{prior} network consists of a dense layer with ReLU activation functions (for number of units, see \emph{Prior net size} in Table~\ref{tab:hyperparameters}), followed by a dense layer that projects the activations to the mean and standard deviation that parameterize the prior latent distribution ,

The \textbf{encoder} network consists of a dense layer with 128 units and ReLU activation functions, followed by a dense layer that projects the activations to the mean and standard deviation that parameterize the posterior latent distribution ,

The \textbf{decoder} network consists of a dense layer with 128 units and ReLU activation functions, followed by a dense layer that projects the activations to the the linearized keypoint vector  of length  (containing ,  and  components),

where  for observed steps and  for predicted steps.

The \textbf{recurrent} network consists of a GRU layer with 512 units:

For the action-conditional model used for reward prediction (Figure~\ref{fig:reward_prediction}), the input to  is , where  is the vector of random actions used to generate frame  of the DeepMind Control Suite dataset.

The size of  and the latent representation  were optimized as hyperparameters (see Table~\ref{tab:hyperparameters}).

\subsection{Optimization}

We used the ADAM optimizer \cite{kingma_adam_2014} with  and . We trained on batches of size 32 for  steps. The learning rate was set to  at the start of training and reduced by half every  steps. We used an L2 weight decay of  on the weights of the convolutional layers in the image encoder and decoder. Weights were initialized using the "He uniform" method as implemented by Keras. Models were trained on a single Nvidia P100 GPU. Training took approximately 12 hours.

During training, we linearly annealed the KL loss scale from  to  over the first  steps, as in \cite{bowman_generating_2016}.

\subsection{Scheduled sampling}

When training an RNN for many timesteps, the initially large errors compound over time, leading to slow learning. Therefore, during training, we initially supplied the observed keypoint coordinates as  to the RNN, instead of the RNN's own predictions. This is similar to teacher forcing, although we note that we used the output of the unsupervised keypoint detector, rather than the ground truth. 

We find that teacher forcing causes the model to make more dynamic predictions which are qualitatively realistic, but may have poor error metrics because of the mismatch between the training and test distributions. We therefore gradually switched to using samples from the model over the course of training (scheduled sampling, \cite{bengio_scheduled_2015}). We linearly increased the probability of choosing samples from the model from  to a final value over the course of training. We chose the final probability to be  for the observed timesteps and  for the predicted timesteps. 

\subsection{Hyperparameter optimization}

We used a black-box optimization tool based on Gaussian process bandits \cite{vizier_2017} to tune several of the hyperparameters of our model. The target for optimization was the mean coordinate trajectory error as computed for Figure~\ref{fig:trajectory_error}. See Table~\ref{tab:hyperparameters} for parameters and their tuning ranges.

\begin{table}
  \caption{Hyperparameters}
  \label{tab:hyperparameters}
  \centering
  \begin{tabular}{lcllll}
    \toprule
    Parameter name          & Symbol        & Tuning range      & Basketball    & Human3.6M & DMCS \\
    \midrule
    Batch size              &               & -                 & 32            & 32        & 32 \\
    Init. learning rate     &               & -                 &      &  &  \\
    Input steps             &            & -                 & 8             & 8         & 8 \\
    Predicted steps         &     &         & 8             & 8         & 8 \\
    Num. keypoints          &            & varied            & 12            & 48        & 64 \\
    Keypoint sparsity scale &  &  &   &  & 5 \\
    Separation loss scale   &  &  &   &  &  \\
    Separation loss width   &           &         &  &  &  \\
    Keypoint blob width     &    &  (pix) & 1.5          & 1.5       & 1.5 \\
    Latent code size        &               &         & 16            & 16        & 128 \\
    KL loss scale           &        &  &      &  &  \\
    Prior net size          &               &         & 16            & 4         & 16 \\
    Posterior net size      &               & -                 & 128           & 128      & 128 \\     
    Num. RNN units          &               &        & 512           & 512       & 512 \\
    Num. samp. for BoM loss &             &         & 50            & 50        & 50 \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Experimental details}

\subsection{Comparison to SVG and SAVP}

For both SVG and SAVP, models were trained on the same datasets as our models, using the code made available by the authors. We trained all models using 8 input steps and 8 predicted steps. For SAVP, the other hyperparameters were set to those recommended by the authors for the BAIR robot pushing dataset.

\subsection{Human3.6M action recognition}\label{sup_sec:action_recognition}

To understand how much semantically useful information the representations of our models contain, we predicted the actions performed in the Human3.6M dataset from the model representations (Figure~\ref{fig:action_recognition}). We first used trained models to extract keypoints (or unstructured image representations) for sequences of 8 observed steps from the Human3.6M test set. These keypoint sequences represented the dataset used for action recognition. The action recognition training set comprised 881 sequences, the test set 279 sequences. We ensured that no test sequences came from the same original videos as those used in the training set.

We then trained a separate recurrent neural network to classify each sequence into one of the 15 action categories (Walking, Sitting, Eating, Discussions, ...) in the Human3.6M dataset. No categories were excluded. The network consisted of two GRU layers (128 units), followed by a dense layer (15 units) and a softmax layer. We used 25\% dropout after each GRU layer. The model was trained for 100 epochs using the ADAM optimizer with a starting learning rate of 0.01 that was successively reduced to 0.0001. We report the mean action recognition accuracy (fraction correct) on the 279 test sequences.

\subsection{DeepMind Control Suite reward prediction}\label{sup_sec:reward_prediction}

To explore if the structured representation learned by our model may be useful for planning, we used our model to predict rewards in DeepMind Control Suite \cite{tassa_deepmind_2018} continuous control tasks (Figure~\ref{fig:reward_prediction}). We chose tasks that have dense rewards and thus provide a strong signal for evaluation (Acrobot Swingup, Cartpole Balance, Cheetah Run, Reacher Easy, Walker Stand, Walker Walk).

We generated a dataset based on DeepMind Control Suite (DMCS) continuous control tasks by performing random actions and recording 64 by 64 pixel observations, the actions, and the rewards. We then trained our model variants on this dataset. Importantly, we trained a single model on data from all domains, to test the generality of our approach. We modified our models to be action-conditional by passing the vector of actions as an additional input to the RNN at each timestep.

To predict rewards, we used the RNN hidden state of our models as a representation of the dynamics learned by the model. We first collected the hidden states of the trained models for 10,000 length-20 sequences from the test split for each of the six domains in our DMCS dataset. We then trained a separate, smaller reward prediction model to predict rewards for each of the six domains. The reward prediction models took the sequence of RNN hidden states as input and returned a sequence of scalar reward values as output. The model consisted of a fully connected layer (128 units), two GRU layers (128 units) and a dense layer (1 unit), all connected through rectified linear units. The reward prediction model was trained on 80\% of the data with the ADAM optimizer with a starting learning rate of 0.001 that was successively reduced to 0.0001. We report the mean squared error of the predicted reward on the remaining 20\% of the data.

\begin{figure*}[h]
    \centering
\includegraphics[width=1.0\textwidth]{supplemental_figures/video_metrics_human/20190526_human_5985036_17_video_metrics_best_and_worst.pdf}
    \caption{Additional video metrics on Human3.6M: structural similarity (SSIM) and peak signal-to-noise ratio (PSNR). The models were conditioned on 8 frames and trained to predict 8 future frames. Higher is better (closer to ground truth). Top row shows the mean across all test-set examples of the closest-to-GT of 100 stochastic samples, bottom shows the furthest. Lines show the mean across 5 random model initializations, with the 95\% confidence interval shaded. } 
    \label{fig:sup_video_metrics_human}
\end{figure*}

 
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{supplemental_figures/video_metrics_basketball/20191015_video_qual_sequence_basketball_5985740.pdf}\\
    \vspace{0.1in}
    \includegraphics[width=1.0\textwidth]{supplemental_figures/video_metrics_basketball/20190618_basketball_5985740_17_video_metrics_best_and_worst.pdf}
    \caption{Video generation quality on Basketball. The models were conditioned on 8 frames and trained to predict 8 future frames. Our stochastic structured model (Struct-VRNN) outperforms our deterministic baseline (Struct-VRNN), our unstructured baseline (CNN-VRNN), and the SVG model \cite{denton_stochastic_2018} in the FVD metric and qualitatively. \textbf{Top:} Example input (green borders) and predicted (red borders) frames. \textbf{Bottom left:} Fr\'{e}chet Video Distance (FVD) \cite{unterthiner_towards_2018}. Lower is better. Each dot represents a separate model initialization. For SVG, the FVD for several runs was greater than 1700. The example at the top comes from the best run. \textbf{Bottom right:} VGG feature cosine similarity, structural similarity (SSIM), and peak signal-to-noise ratio (PSNR). Higher is better. Lines show the mean across 5 random model initializations, with the 95\% confidence interval shaded. The SVG model fails to represent objects stably at later timepoints. This is captured by the FVD metric, causing a large difference to our models. However, it is not captured by the other metrics, suggesting that they are not informative on this synthetic dataset. Also see videos in supplemental material or at \supplementalUrl.} 
    \label{fig:video_metrics_basketball}
\end{figure*}

 
\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.6\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/basketball_trajectory_diversity/20190522_basketball_trajectory_diversity_examples_5985740_21_struct_vrnn.pdf}
        \caption{Struct-VRNN}
    \end{subfigure}\\
    \vspace{0.2in}
    \begin{subfigure}[t]{0.6\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/basketball_trajectory_diversity/20190522_basketball_trajectory_diversity_examples_5985740_23_no_bms.pdf}
        \caption{Struct-VRNN without best-of-many-samples objective}
    \end{subfigure}\\
    \vspace{0.2in}
    \begin{subfigure}[t]{0.6\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/basketball_trajectory_diversity/20190522_basketball_trajectory_diversity_examples_5985740_37_deterministic.pdf}
        \caption{Struct-RNN (deterministic dynamics)}
    \end{subfigure}
    \caption{Effect of stochastic belief and best-of-many-samples objective on sample diversity. Each row shows one example Basketball play, with the trajectories for one player in each column. The black line indicates the true trajectory, the colored lines indicate 20 stochastic predictions, all conditioned on the same observed steps. Trajectory endpoints are marked with dots. The model trained with the best-of-many-samples objective (a) produces more diverse samples than the model without (b). As expected, the deterministic model (c) lacks diversity completely. Players were matched to detected keypoints by finding, for each player, the keypoint which was closest to that player on average.}
    \label{fig:basketball_diversity}
\end{figure*} 
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{supplemental_figures/human_sample_diversity/20190522_human_sample_diversity_ex_0_5985036.pdf}\\
    \vspace{0.1in}
    \includegraphics[width=1.0\textwidth]{supplemental_figures/human_sample_diversity/20190522_human_sample_diversity_ex_1_5985036.pdf}\\
    \vspace{0.1in}
    \includegraphics[width=1.0\textwidth]{supplemental_figures/human_sample_diversity/20190522_human_sample_diversity_ex_2_5985036.pdf}\\
    \vspace{0.1in}
    \includegraphics[width=1.0\textwidth]{supplemental_figures/human_sample_diversity/20190522_human_sample_diversity_ex_3_5985036.pdf}
    \caption{The Struct-VRNN model generates plausible and diverse predictions. Each block shows the true sequence in the top row, followed by three samples conditioned on the same initial frames (green outlines). Also see videos in supplemental material or at \supplementalUrl.}
    \label{fig:human_sample_diversity}
\end{figure*}

 
\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/dmcs_predictions/20190522_dmcs_prediction_ex_Acrobot_6016471.pdf}
        \caption{Acrobot}
    \end{subfigure}\\
    \vspace{0.1in}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/dmcs_predictions/20190522_dmcs_prediction_ex_Cartpole_6016471.pdf}
        \caption{Cartpole}
    \end{subfigure}\\
    \vspace{0.1in}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/dmcs_predictions/20190522_dmcs_prediction_ex_Cheetah_6016471.pdf}
        \caption{Cheetah}
    \end{subfigure}\\
    \vspace{0.1in}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/dmcs_predictions/20190522_dmcs_prediction_ex_Reacher_6016471.pdf}
        \caption{Reacher}
    \end{subfigure}\\
    \vspace{0.1in}
    \begin{subfigure}[t]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/dmcs_predictions/20190522_dmcs_prediction_ex_Walker_6016471.pdf}
        \caption{Walker}
    \end{subfigure}
    \caption{Action-conditional predictions for the DeepMind Control Suite domains. Even though the CNN-VRNN has enough capacity to encode the observed frames (green outlines) well, it struggles to make future predictions (red outlines), in contrast to the Struct-VRNN. Also see videos in supplemental material or at \supplementalUrl.}
    \label{fig:dmcs_predictions}
\end{figure*} 
\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \includegraphics[width=\textwidth]{supplemental_figures/failure_mode_analysis/20191015_basketball_detection_failure_modes.pdf}
        \caption{Coordinate error over time for individual objects. In the left plot, lines indicate the mean across 10 model initializations, with the 95\% confidence interval shaded. In the middle and right plot, lines show individual model initializations. For these runs,  and .}
    \end{subfigure}\\
    \vspace{0.2in}
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{supplemental_figures/failure_mode_analysis/20191017_basketball_detection_failure_mode_grids.pdf}
        \caption{Mean coordinate error of the ball (left) and Player 3 (right) across different settings of  and , relative to the best settings. Each entry in the heatmaps corresponds to the mean trajectory error across time and 10 model initializations.}
    \end{subfigure}\\
    
    \caption{Analysis of object tracking failure modes (Basketball dataset). (a) shows the coordinate error for individual objects. We identify two different failure modes, corresponding to failures of the dynamics model and failures of the keypoint detector: Some objects, e.g. the ball (yellow; middle plot), have relatively large tracking errors across all 10 model initializations, presumably because their dynamics are hard to learn. Other objects, e.g. Player 3 (pink; right plot) are tracked well in some and poorly in other model initializations, presumably because the keypoint detector completely fails to detect these objects in some runs. The sweep over  and  in (b) shows that  and  primarily reduce the keypoint detector failure mode (exemplified by the Player 3 error; right), while the tracking error of the ball (left) is insensitive to these losses. In other words,  and  improve the reliability of the keypoint detector.}
    \label{fig:failure_mode_analysis}
\end{figure*} 