\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{tikz}
\usepackage{physics}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{booktabs, tabularx, wrapfig}
\usepackage{comment}
\usepackage{caption}
\usepackage{float}
\usepackage{url}
\usepackage[breaklinks=true]{hyperref}
\usepackage{breakcites}
\def\UrlBreaks{\do\/\do-}

\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
\title{Deep Generative Views to Mitigate Gender Classification Bias Across Gender-Race Groups\thanks{Supported by National Science Foundation (NSF)}}
\author{Sreeraj Ramachandran\and
Ajita Rattani\thanks{Corresponding author}}
\authorrunning{S. Ramachandran \and A. Rattani}
\titlerunning{Deep Generative Views to Mitigate Gender
Classification Bias}
\institute{School of Computing\\
Wichita State University, USA\\
\email{sxramachandran2@shockers.wichita.edu}, 
\email{ajita.rattani@wichita.edu}}
\maketitle              \begin{abstract}
Published studies have suggested the bias of automated face-based gender classification algorithms across gender-race groups. Specifically, unequal accuracy rates were obtained for women and dark-skinned people. To mitigate the bias of gender classifiers, the vision community has developed several strategies. However, the efficacy of these mitigation strategies is demonstrated for a limited number of races mostly, Caucasian and African-American. Further, these strategies often offer a trade-off between bias and classification accuracy. To further advance the state-of-the-art, we leverage the power of generative views, structured learning, and evidential learning towards mitigating gender classification bias. We demonstrate the superiority of our bias mitigation strategy in improving classification accuracy and reducing bias across gender-racial groups through extensive experimental validation, resulting in state-of-the-art performance in intra- and cross dataset evaluations.

\keywords{Fairness and Bias in AI \and Deep Generative Views \and Generative Adversarial Networks}
\end{abstract}


\section{Introduction}

Gender is one of the important demographic attributes. Automated gender classification\footnote{Studies in~\cite{bowker,keyes} have shown the inherent problems with gender and race classification. While the datasets used in this study use an almost balanced dataset for the training, it still lacks representation of entire large demographic groups, effectively erasing such categories. We use the gender and race categories defined in these datasets to make comparisons to prior work, not to reinforce or endorse the use of such reductive categories.} refers to automatically assigning gender labels to biometric samples. It has drawn significant interest in numerous applications such as demographic research, surveillance, human-computer interaction, anonymous customized advertisement system, and image retrieval system~\cite{nist_gender,wayman1997large,article1,DBLP:conf/fgr/JainH04}. Companies such as Amazon, Microsoft~\cite{gender_shades}, and many others have released commercial software containing an automated gender classification system from biometric facial images. 

Over the last few years, published research has questioned the fairness of these face-based automated gender classification systems across gender and race~\cite{gender_shades,commercial_bias,bias_color,bias_fairface}.  A classifier is said to satisfy group fairness if subjects in both the protected and unprotected groups have an equal chance of being assigned to the positively predicted class~\cite{10.1145/3194770.3194776}. Existing gender classification systems produce unequal rates for women and people with dark skin, such as African-Americans. Studies in~\cite{BiasDeep,Siddiqui_2022_CVPR} have also evaluated the gender bias of facial analysis based deepfake detection and BMI prediction models. Further, studies in~\cite{10.1007/978-3-030-68793-9_16,9717383} have evaluated the bias of ocular-based attribute analysis models across gender and age-groups.

To mitigate the bias of the gender classification system, several solutions have been developed by the vision community. These solutions are based on regularization strategies~\cite{readme}, attention mechanism~\cite{att_aware_debias} and adversarial debiasing~~\cite{tradeoff1,tradeoff2}, data augmentation techniques~\cite{fair_mixup}, subgroup-specific classifiers~\cite{multitask}, and over-sampling the minority classes using Generative Adversarial Networks(GANs)~\cite{gan_debias}. 

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=1.7]{figures/teaser_2.pdf}
    \caption{Input samples are projected into a latent space and augmentations are generated. The loss function minimizes the distance between the embedding along with the classification loss.}
    \label{fig:teaser}
    \vspace{-1.5em}
\end{figure}

Often the aforementioned mitigation strategies offer a trade-off~\cite{tradeoff1,tradeoff2} between the fairness and classification task. Further, the efficacy of these strategies are demonstrated on limited number of races; mostly African-American and Caucasian~\cite{gender_shades,bias_color}. A bias mitigation strategy that offer fairness across several inter-sectional subgroups without compromising the gender classification accuracy is still an \textbf{open challenge}~\cite{chlng}.

\begin{comment}
They offer a trade-off between fairness and classification task
In this context, two lines of research are being pursued (a) examining the bias of existing gender classification algorithms across gender-race groups and (2) developing solutions in the form of fairness-aware loss functions, and unlearning the dependency of the model on sensitive
attributes and disentangling the features related to sensitive attributes from those for the main classification task.

Overall, these strategies for bias mitigation of gender classification algorithms could be categorized as (a) data level, and (b) algorithmic level. At the data level, bias is mitigated by using gender and race-balanced datasets, and using generative approaches to synthesize images of the under-represented class for bias mitigation. At the algorithmic level, fairness-aware loss functions, and techniques for disentangling race-related features from the classification layers of the model are introduced.
\end{comment}

To further advance the state-of-the-art, this paper proposes a solution that combines the power of structured learning, deep generative views, and evidential learning theory for classifier's uncertainty quantification, for bias mitigation of the gender classification algorithms. We observe that the locally smooth property of the latent space of a Generative Adversarial Network (GAN) model facilitates the generation of the perceptually similar samples. The augmentation strategies that exploit the local geometry of the data-manifold are more powerful in general when used in a consistency regularization style setting. We chose the latent space of the GAN model to be a surrogate space for the data-manifold and therefore sampled augmentations from the latent space called deep generative views. 

Specifically, the facial images of the training set are inverted to the GAN latent space to generate a latent code by training an encoder of the trained StyleGAN. The generated latent code is perturbed to produce variations called \emph{neighboring views} (deep generative views) of the training images. The original training samples along with the neighboring views are used for the gender classifier's training. The regularization term is added to the loss function that enforces the model to learn similar embedding between the original images and the neighboring views. Lastly, a reject option based on uncertainty quantification using evidential deep learning is introduced. The reject option is used to discard samples during the test time based on the quantification of the uncertainty of the classifier's prediction. Figure~\ref{fig:teaser} shows the schema of the proposed approach based on obtaining generative views by projecting the original samples into the latent space.

The proposed bias mitigation strategy has the \textbf{dual advantage} of enhancing the classifier's representational ability as well as the fairness, as demonstrated through extensive experimental evaluations. Also, its generality allows it to be applied to any classification problem, not just face-based gender classification.





















\begin{comment}
Motivated by structured learning~\cite{neural_graph_learning, ssl_gcn, goodfellow_adverarial, miyato} and generative views literature~\cite{ensembling_views}, we propose a solution based on deep generative views of the real facial images along with the neighbour-loss based regularizer for bias mitigation of the gender classification system. Our proposed method is based on training a StyleGAN based generative model that can produce synthetic data of the same distribution. Subsequently, we trained an encoder for the trained StyleGAN for inverting our training set images to the GAN latent space such that we can perturb the latent vectors to produce variations of the input image called \emph{neighbouring views}. We introduce a regularizing term (Neighbour Loss) to the loss function, which calculate the distance between the source and neighbour embedding. The proposed method improves the classification performance from the baseline as well as improves the fairness of the model significantly, contrary to previous attempts where it is a trade-off between the two.
\end{comment}









\begin{comment}
We also introduce a reject-option method based on uncertainty prediction introduced by evidential deep learning. By replacing the classification head with an evidential layer that outputs a Dirichlet distribution and a corresponding loss, the model acquires the ability to not only predict the class but also to predict the uncertainty of its prediction. We use this model's own prediction of uncertainty as a means of rejection threshold and observe that  ... \end{comment}

In summary, the main \textbf{contributions} of this paper are as follows:

\begin{itemize}
\item A bias mitigation strategy for deep learning-based gender classifiers that leverages and combines the power of GAN to produce deep generative views, structured learning, and evidential learning theory for uncertainty quantification. 

\item We demonstrate the merit of our proposed bias mitigation strategy through experimental analysis on state-of-the-art facial attribute datasets, namely, FairFace~\cite{fairface}, UTKFace~\cite{utkface}, DiveFace~\cite{diveface} and Morph~\cite{morph}, and ocular datasets namely, VISOB~\cite{visob} and UFPR~\cite{ufpr}.

\item Extensive experiments demonstrate the dual advantage of our approach in improving the representational capability as well as the fairness of the gender classification task, thus obtaining state-of-the-art performance over existing baselines most of the time.

\end{itemize}

\begin{comment}
is to explore leveraging the power of a GAN to produce deep generative views of a given real image. Using these generative views such as altering of pose, shape or colour based on the variation a GAN learns, metric learning is used improve the representational ability as well as fairness of the model. Specifically, in this paper we investigate the following techniques to improve the fairness of a model:
\begin{itemize}
    \item \textbf{Metric Learning based optimization}, where a GAN model is used to create generative views of the input image and uses a metric loss for optimization
    \item \textbf{An Evidential deep learning based rejection method}, where an evidential loss term and final layer is added to the model, which can be used to estimate uncertainty and therefore as a threshold for reject option.
    \end{itemize}
    
\end{comment}  
    

\begin{figure*}[!hbtp]
    \centering
\clearpage{}\scalebox{0.7}{
\begin{tikzpicture}
\tikzstyle{data_space} = [draw=cyan!70!black,circle, fill=cyan!20!white]
\tikzstyle{latent_space} = [draw=green!70!black,circle, fill=green!20!white, inner sep=15]
\tikzstyle{rep_space} = [draw=magenta!70!black,circle, fill=magenta!20!white, inner sep=15]
\tikzstyle{gen} = [draw=red!70!black,circle, fill=red!20!white]
\tikzstyle{desc} = [draw=magenta!70!black,circle, fill=magenta!20!white]
\tikzstyle{enc} = [draw=magenta!70!black,circle, fill=magenta!20!white]
\tikzstyle{cls} = [draw=magenta!70!black,circle, fill=magenta!20!white]

\node [align=left] at (0.1,-0.4) {\scriptsize Given:  \\ \scriptsize Learn: 
};
\node [latent_space] (v2) at (0.6,-2.1) {};
\node at (0.5,-1.7) {};


\node at (0.5,-3.1) {\scriptsize Latent space};
\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {G}};}] (1.9,-1.8) node (v1) {} -- (1.9,-2.4) -- (2.6,-2.7) -- (2.6,-1.5) -- cycle;
        
        
\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {D}};}] (5.4,-1.8) node (v1) {} -- (5.4,-2.4) -- (4.7,-2.7) -- (4.7,-1.5) -- cycle;
\draw [-latex] (0.5634,-1.6918) .. controls (1,-2) and (1.5,-2.1) .. (1.9,-2.1);
\draw [data_space]  plot[smooth cycle, tension=.7] coordinates {(3,-2.2) (3.1,-2.7) (3.6,-2.5) (4.1,-2.7) (4.4,-2.3) (3.9,-2) (3.8,-1.5) (3.3,-1.7) (3.3,-2.1)};
\node at (3.5,-2.3) {};
\node at (3.6,-1.7) {};
\node at (3.6,-3) {\scriptsize Data space};
\node at (2.3,-1.3) {\scriptsize Generator};
\node at (5.2,-1.3) {\scriptsize Discriminator};
\draw [-latex] (2.6,-2.1) .. controls (2.9,-2.3) and (3.2,-2.1) .. (3.3753,-2.3399);
\draw [-latex] (3.3694,-2.3373) .. controls (3.7,-2) and (4.2,-2) .. (4.7,-2.4);
\draw [-latex] (3.5212,-1.6938) node (v5) {} .. controls (3.8,-1.3) and (4.3,-1.5) .. (4.7,-1.9);
\node [draw,rectangle] (v3) at (6.5,-2.1) {};
\draw [-latex] (5.4,-2.1) -- (v3);
\node (v4) at (3.5,-0.9) {\scriptsize };
\draw [-latex] (v4) -- (v5.center);


\node [align=left] at (9.2277,-0.1171) {\scriptsize Given:  \\
\scriptsize Learn: 
};
\node (v6) at (8.2,-2.1) {};

\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {E}};}] (9.965,-1.805) node (v1) {} -- (9.965,-2.405) -- (9.265,-2.705) -- (9.265,-1.505) -- cycle;
\draw [-latex] (v6) -- (9.265,-2.105);
\node[latent_space] at (11.065,-2.105) {};
\node at (11.065,-2.405) {};
\draw [-latex] (9.965,-2.105) .. controls (10.415,-2.005) and (10.765,-2.105) .. (10.979,-2.4001);
\node at (9.565,-1.305) {\scriptsize Encoder};
\node at (11.015,-3.005) {\scriptsize Latent space};

\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {G}};}] (12.265,-1.805) node (v1) {} -- (12.265,-2.405) -- (12.965,-2.705) -- (12.965,-1.505) -- cycle;
        

\draw [-latex] (10.9912,-2.3957) .. controls (11.265,-2.105) and (11.665,-2.055) .. (12.269,-2.1239);
\node at (12.815,-1.355) {\scriptsize Generator};
\draw [data_space]  plot[smooth cycle, tension=.7] coordinates {(13.47,-2.205) (13.57,-2.705) (14.07,-2.505) (14.57,-2.705) (14.87,-2.305) (14.37,-2.005) (14.27,-1.505) (13.77,-1.705) (13.77,-2.105)};

\node at (14.115,-1.905) {};
\draw [-latex] (12.965,-2.105) .. controls (13.215,-1.805) and (13.465,-1.755) .. (13.9777,-1.9471);
\node at (14.115,-3.005) {\scriptsize Data space};
\node [draw, rectangle] (v8) at (15.715,-2.105) {};
\draw [-latex] (13.9768,-1.9461) .. controls (14.0158,-1.5359) and (14.7391,-1.9778) .. (15.2596,-2.1347);


\node [align=left] at (0.8,-6.9) {\scriptsize Given:  \\
\scriptsize Generate: 
};
\draw [data_space]  plot[smooth cycle, tension=.7] coordinates {(-0.6,-5) (-0.5,-5.5) (0,-5.3) (0.5,-5.5) (0.8,-5.1) (0.3,-4.8) (0.2,-4.3) (-0.3,-4.5) (-0.3,-4.9)};
\node at (-0.1,-4.6) {};

\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {E}};}] (2,-4.6) node (v1) {} -- (2,-5.2) -- (1.3,-5.5) -- (1.3,-4.3) -- cycle;
\node [latent_space] (v2) at (3.2,-4.9) {};
\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {G}};}] (4.4,-4.6) node (v1) {} -- (4.4,-5.2) -- (5.1,-5.5) -- (5.1,-4.3) -- cycle;

\draw [data_space]  plot[smooth cycle, tension=.7] coordinates {(5.6,-4.9) (5.7,-5.4) (6.2,-5.2) (6.7,-5.4) (7,-5) (6.5,-4.7) (6.4,-4.2) (5.9,-4.4) (5.9,-4.8)};
\node at (3,-5.3) {};
\node at (3.3,-5) {};
\draw [-stealth, densely dotted, thick, draw=orange] (2.9419,-5.2854) .. controls (2.9205,-5.1095) and (3.032,-5.0108) .. (3.1951,-5.028);
\node at (2.8518,-5.0195) {};

\draw [-latex] (0.0019,-4.5875) .. controls (0.4633,-4.4135) and (0.7658,-4.9354) .. (1.3029,-4.9657);
\draw [-latex] (2.014,-4.9127) .. controls (2.3241,-4.8598) and (2.4678,-5.3288) .. (2.9104,-5.3006);
\draw [-latex] (3.1997,-5.0267) .. controls (3.3858,-4.634) and (3.9179,-4.9234) .. (4.4139,-4.9182);
\node at (6.124,-5.0164) {};

\draw [-latex] (5.1062,-4.9182) .. controls (5.4109,-4.9699) and (5.4988,-4.7219) .. (5.7934,-5.0215);
\node at (0.0732,-5.9148) {\scriptsize Data space};
\node at (3.1793,-5.9935) {\scriptsize Latent space};
\node at (6.2503,-5.9585) {\scriptsize Data space};
\node at (1.5868,-4.0074) {\scriptsize Encoder};
\node at (4.6754,-4.0074) {\scriptsize Generator};




\node [align=left] at (9.5389,-7.0384) {\scriptsize Given:  \\
\scriptsize Learn: 
};

\draw [data_space]  plot[smooth cycle, tension=.7] coordinates {(8.4823,-5.0714) (8.5823,-5.5714) (9.0823,-5.3714) (9.5823,-5.5714) (9.8823,-5.1714) (9.3823,-4.8714) (9.2823,-4.3714) (8.7823,-4.5714) (8.7823,-4.9714)};

\draw [gen,path picture={
      \node at (path picture bounding box.center) {
        {F}};}] (10.8823,-4.7714) node (v1) {} -- (10.8823,-5.3714) -- (10.1823,-5.6714) -- (10.1823,-4.4714) -- cycle;
        
        
\node at (8.9834,-4.7815) {};
\node at (9.352,-5.2059) {};
\node [rep_space] at (12.1526,-5.0281) {};
\node at (12.0727,-5.4529) {};
\node at (12.2491,-4.823) {};
\node [draw, rectangle] at (13.5898,-4.9714) {};
\node at (14.1753,-4.9714) {};
\node [draw, rectangle] at (14.7684,-4.9714) {};
\draw [-latex] (8.9012,-4.7717) .. controls (9.2786,-4.5478) and (9.6435,-4.5403) .. (10.1758,-5.1106);
\draw [-latex] (9.0201,-5.1942) .. controls (9.1721,-4.9737) and (9.6512,-4.9661) .. (10.1834,-5.1258);
\draw [-latex] (10.8677,-5.0725) .. controls (11.0502,-4.8064) and (11.324,-4.6696) .. (11.6965,-4.8216);
\draw [-latex] (10.8677,-5.0725) .. controls (11.1339,-5.3007) and (11.4684,-5.4679) .. (11.765,-5.4451);
\draw [-latex] (11.765,-5.4451) .. controls (12.1452,-5.0118) and (12.6393,-4.9736) .. (13.1488,-5.0193);
\draw [-latex, draw=orange] (11.6889,-4.814) .. controls (12.1527,-3.9776) and (13.9776,-3.9472) .. (14.7,-4.7);
\draw [-latex, draw=orange] (11.7574,-5.4451) .. controls (12.2972,-5.9925) and (14.3654,-6.1599) .. (14.7684,-5.2246);
\node at (9.135,-6.1838) {\scriptsize Data space};
\node [align=center] at (12.1378,-6.2819) {\scriptsize Representation \\ \scriptsize space};
\node at (10.3126,-4.0838) {\scriptsize Classifier};

\draw [dashed, draw=gray] (-1,-3.4) -- (16,-3.4);
\draw [dashed, draw=gray] (7.4,-0.1) -- (7.4,-6.9);
\node at (-0.9,-0.3) {(A)};
\node at (8,0) {(B)};
\node at (-1,-4) {(C)};
\node at (8,-4) {(D)};

\end{tikzpicture}}
\clearpage{}
\caption{Overview of the proposed method. (A) Training a GAN to learn the input image data distribution, . (B) Training an Encoder model,  to learn to project input image,  to the latent space. (C) Generating Neighbour Views by first projecting input image into latent space using , applying perturbation in latent space, then using  to generate. (D) Training the Classifier,  using both the input image and its neighbors.} 
    \label{fig:nsl_teaser}
\end{figure*}


\section{Related Work}

\noindent\textbf{Fairness in Gender Classification}:
In~\cite{gender_shades}, authors evaluated the fairness using commercial SDKs and observed least accuracy rates for dark-skinned females. Studies in~\cite{gender_age_transfer,Ryu18} proposed data augmentation and two-fold transfer learning for measuring and mitigation bias in deep representation. Attribute aware filter drop was proposed in~\cite{att_aware_filter} and regularization strategy in~\cite{tartaglione} are used to unlearn the dependency of the model on sensitive attributes. 
In ~\cite{multitask}, a multi-task Convolution Neural Network (MTCNN) is proposed, that use a joint dynamic loss to jointly classify race, gender and age to minimize bias. A data augmentation strategy called fair mixup is proposed in~\cite{fair_mixup}, that regularize the model on interpolated distributions between different sub-groups of a demographic group. An auto-encoder that disentangle data representation into latent sub-spaces is proposed by~\cite{readme}. 
Using GANs to generate balanced training set images w.r.t protected attributes was proposed in~\cite{gan_debias}. Recently OpenAI released its large scale language-image pretraining model called CLIP~\cite{clip} which was trained contrastively on 400M image-text pairs and demonstrated its exceptional performance as well as bias on both zero-shot and fine-tuning setting on gender classification tasks.\\


\noindent \textbf{GANs for real image editing}: 
Advancements in GANs~\cite{gan,gan1,progan,bigan,stylegan,stylegan2,stylegan2-ada} enable us to produce increasingly realistic images that can replicate variations in training data. Specifically, the learnt intermediate latent space of the StyleGAN~\cite{stylegan,stylegan2,stylegan2-ada} represents the distribution of the training data more efficiently than the traditional Gaussian latent space. By leveraging the disentangling properties of the latent space as shown in~\cite{editing4,walking1,editing2,editing1,e4e,psp}, extensive image manipulations could be performed. Once we invert the input image into the learned latent space, we can modify real images in a similar way. High-quality inversion methods (i) can properly reconstruct the given image, and (ii) produce realistic and semantically meaningful edits. Optimization methods~\cite{invert3,invert2,invert1} optimize the latent code by minimizing error for an individual image, whereas faster encoder-based methods~\cite{invert4,invert5,e4e} train an encoder to map images to the latent space. 

Once inverted, manipulations can be done on the given image by finding “walking” directions that control the attribute of interest. Studies in ~\cite{walking1,ganalyze,walking_bias} use supervised techniques to find these latent directions specific to attributes. Whereas studies in~\cite{editing2,walking_un1,walking_un2} find directions in an unsupervised manner, requiring manual identification of the determined directions later and~\cite{sefa} using a closed-form factorization algorithm for identifying top semantic latent directions by directly decomposing the pre-trained weights. \\

\noindent \textbf{Structured Learning, Ensembling of Deep Generative Views}: 
Studies in~\cite{neural_graph_learning,ssl_gcn,goodfellow_adverarial,miyato} have shown that combining feature inputs with a structured signal improves the overall performance of a model in various scenarios such as Graph Learning and Adversarial learning. These structured signals either implicit (adversarial) or explicit (graph) are used to regularize the training while the model learns to accurately predict (by minimizing supervised loss)~\cite{nsl} by maintaining the input structural similarity through minimizing a neighbor loss. Recently ~\cite{ensembling_views} proposed a test-time ensembling with GAN-based augmentations to improve classification results. 



\section{Proposed Approach}
Figure~\ref{fig:nsl_teaser} illustrates the overview of the proposed approach. The process involves training a generative model to produce different views of a given input image. The training images are projected to the latent space of the generator. The variations to the input images are produced using augmentation in the latent space. These generative views act as a structured signal and are used along with the original images to train the downstream gender classifier. Therefore, by injecting prior knowledge into the learning process through the learned latent space and by enforcing local invariance properties of the manifold when used as a consistency regularizer, the classifier's performance is significantly improved. This helps propagate the information contained in the labeled samples to the unlabeled neighboring samples. The proposed strategy of leveraging neighbor views is used during classifier's training, the test images are used as it is during the test-time. 

\subsection{GAN Preliminaries}
A GAN~\cite{gan} consists of a generator network~() and a discriminator network~() involved in a min-max game, where  tries to fool  by producing realistic images from a latent vector  and D gets better at distinguishing between real and generated data. We employ the StyleGAN2~\cite{stylegan2} generator in this study. Previous works have shown that the intermediate  space, designed to control the "style" of the image is better able to represent images than the original latent code  with fine-grained control and better disentanglement. In a StyleGAN2 generator, a mapping network  maps  to  and  generates the image , given . i.e .

\subsection{Projecting Images into GAN Latent Space}
To alter an image  with the generator, we must first determine the appropriate latent code that generates . GAN inversion refers to the process of generating the latent code for a given image . For GAN inversion, there are optimization-based methods as well as encoder-based methods that offer various trade-offs between edit-ability, reconstruction distortion, and speed. We specifically use the \emph{encoder4editing} method (e4e)\cite{e4e}. e4e use adversarial training to map a given real image to a style code composed of a series of low variance style vectors, each close to the distribution of . We can generate the reconstruction, , which is closer to the original  using the given style vectors and the generator, G. 

\subsection{Generating Views using Pretrained GAN}
For generating views, we use existing latent editing methods. We specifically employ SeFA~\cite{sefa}, a closed-form factorization technique for latent semantic discovery that decomposes the pre-trained weights directly. Unsupervised, SeFa determines the top  semantic latent directions. To produce alternate views, we randomly select an arbitrary set of semantic latent directions and sample distances from a Gaussian distribution for latent space traversal for each image . The generator then uses these updated style vectors to generate the views. In the case of latent vectors occurring in poorly defined i.e., warped region of latent space which may produce non-face images, a simple MTCNN-based~\cite{mtcnn} face detector is utilized to screen out the non-faces.

\subsection{Structured Learning on Deep Generative Views}
Let  be the input sample. We obtain its corresponding latent style vector  using e4e. We may apply SeFA on  to obtain , where  is a hyperparameter representing the number of neighbours, the neighbouring latent vectors. Using the pretrained StyleGAN generator we may produce the neighbouring views .

During training each batch will contain samples with pairs of original sample  and the generated neighbours . Both  and  are used in the forward pass but only  is backpropagated and used for calculating batch statistics in the normalization layers. This is because of the distributional difference between the real and generated images. The total loss,  is given by

where  is the classification loss,  is a hyperparameter,  is any distance function that can be used to calculate the distance between the sample embedding and the neighbour embedding and  is the sample embedding from the neural network. For our experiments, we use Jensen-Shannon Divergence~\cite{jsdiv} to calculate the distance between the sample embedding and the neighbour embedding.

\subsection{Evidential Deep Learning for Quantifying Uncertainty}
In standard classifier training where prediction loss is minimized, the resultant model is ignorant of the confidence of its prediction. A study in~\cite{edl} proposed explicit modeling of uncertainty using subjective logic theory by applying a Dirichlet distribution to class probabilities. Thus, treating model predictions as subjective opinions, and learning the function that collects the evidence leading to these opinions from data using a deterministic neural net. The resulting predictor for a multi-class classification problem is another Dirichlet distribution, the parameters of which are determined by a neural net's continuous output. 

Let us assume that  is the parameters of a Dirichlet distribution for the classification of a sample , then  is the total evidence,  estimated by the network for the assignment of the sample  to the  class. The epistemic uncertainty can be calculated from the evidences  using 

where  is the number of classes and 
\cite{edl} proposes the following loss function in this scenario: 


where  is the one-hot vector encoding of ground-truth and  is the class-assignment probabilities given by .

We use the estimated uncertainty as the basis for rejection, similar to reject-option-based classification~\cite{reject_option}, where a test sample in low confidence region is rejected based on the model's confidence values. The provided model of uncertainty is more detailed than the point estimate of the standard softmax-output networks and can handle out-of-distribution queries, adversarial samples as well as samples belonging to the critical region. Therefore a high uncertainty value of a test sample suggests the model's under-confidence. And we use this uncertainty estimate as the basis for rejecting samples during test time rather than using the softmax probabilities.

\section{Experiments and Results}
\subsection{Datasets used}
For all our experiments we used the FairFace~\cite{fairface} as our training dataset. Testing was done on the test set of the FairFace as well as DiveFace~\cite{diveface}, UTKFace~\cite{utkface} and Morph~\cite{morph} datasets.

Table~\ref{tab:datasets} shows the characteristics of these datasets used in our study.

\begin{table}[!ht]
    \centering
    \caption{Datasets}
    \label{tab:datasets}
    \begin{tabular}{llp{8cm}}
    \toprule
    \multicolumn{1}{c}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{Images}} & \multicolumn{1}{c}{\textbf{Races}} \\ \midrule
    FairFace &  & White, Black, Indian, East Asian, Southeast Asian,~Middle Eastern, Latino Hispanic \\
    DiveFace &  & East Asian, Sub-Saharan,South Indian, Caucasian \\
    UTKFace &  & White, Black, Indian, Asian \\
    Morph &  & White, Black\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Gender Classification}
For our experiments on gender classification, we used FairFace as our training dataset. The FairFace dataset was also used for training the StyleGAN2 generator as well as the e4e encoder that was subsequently used to generate the neighbouring views. 




\subsubsection{\textbf{Generating Neighbouring Views}}: For pretrained StyleGAN2, we use the official StyleGAN2-ADA implementation pretrained on high quality FFHQ-256 face dataset~\cite{stylegan} and used transfer learning on the FairFace training set with images resized to . We obtained a Fréchet inception distance (FID)~\cite{fid} of , similar perceptual quality as of FFHQ on StyleGAN~\cite{stylegan}. The uncurated images generated by the trained generator are shown in Figure~\ref{fig:imgs_recons_vars}.

\begin{figure*}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/figures.pdf}
    \caption{(a) Uncurated images generated by StyleGAN2 trained on FairFace. (b) Images reconstructed using the trained e4e encoder and pretrained StyleGAN2 generator. (c) Generative views created by selecting top semantic latent directions with SeFA and randomly sampling along those directions.}
    \label{fig:imgs_recons_vars}
    \vspace{-0.2em}
\end{figure*}


For pretraining e4e, we use the official e4e implementation with the pretrained StyleGAN2 generator from above and the FairFace training set. Figure \ref{fig:imgs_recons_vars} shows uncurated reconstructions by the encoder. Finally, for generating the views, the training set images are first inverted using e4e and then we use SeFA to choose the top  semantic latent directions. We pick distances in both positive and negative directions randomly. For our experiments we initially generate 56 neighbors per image, to have a wide variation of views. We then use a pretrained MTCNN face detector to detect faces on the generated images and remove non-faces to ensure a clean dataset sampled from the well-defined region of latent space. Figure~\ref{fig:imgs_recons_vars} shows examples of generated variations. 

\subsubsection{\textbf{Training the classifier}}: For our experiments, we use a baseline \emph{EfficientNetV2-L} pretrained on ImageNet as a gender classification model. Our proposed method based on deep generative views and structured learning, denoted by \emph{Neighbour Learning~(NL)}, is applied to this model. We also compare our results with a sensitive-attribute aware multi-task classifier~(MT)~\cite{multitask}, with race as the secondary attribute. 

Further, we combine both NL and MT and evaluate their performance (third configuration). And finally, our last configuration which also has the ability to predict uncertainty, combines NL, and MT and replaces the final classification head of the gender classifier with an evidential layer (EDL) and replaced cross-entropy loss with an evidential loss. 

For all our experiments, we use an RMSProp optimizer with a cosine annealing schedule with an initial learning rate of  and weight decay of . We use a batch size of  across 2 RTX8000 GPUs, with label smoothing of , and \emph{autoaugment} policy for data augmentation. For NL,  is set to , and the number of neighbors,  to  for the final configuration. We also apply \emph{lazy regularization} to speed up the training, inspired by StyleGAN, where we apply the costly NL regularization every  batch. For our final model, we set this hyperparameter value to . For evaluation, we used the \emph{Degree of Bias~(DoB)}~\cite{dob}, the standard deviation of classification accuracy across different subgroups, as well as \emph{Selection Rate (SeR)}~\cite{ser}, the ratio of worst to best accuracy rate, as a metric for evaluation of the fairness.

Table \ref{tab:results} show the results on the FairFace validation set. The empirical results suggest that the proposed method NL improves the fairness of the model as well as the overall accuracy, outperforming both baseline and multi-task aware setup~(MT). Combining NL with MT and evidential deep learning~(EDL) further improves the performance by reducing the DoB to  and with an accuracy of . To compare with the state of the art as well as to evaluate the generality of the method, we applied the same to the vision tower of the CLIP~\cite{clip} model. We used the ViT-L/14 version of the model with pretrained weights and added a final linear classification head for the configuration. Applying our method to the CLIP model~\cite{clip}, an already SOTA method, improved the DoB while maintaining the accuracy. As most studies constraint the bias evaluation to a simple white vs non-white subjects, we restrict our comparisons to SOTA methods~\cite{lp_insta,clip} that evaluate fairness across multiple-race groups on FairFace and observed our method either outperformed or enhanced the models that were pretrained on huge datasets such as WebImageText()~\cite{clip} and Instagram ()~\cite{lp_insta}.

\begin{table*}[hbtp]
    \centering
    \caption{Gender classification results of the proposed method across gender-race groups when trained and tested on FairFace.} \label{tab:results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccccccc} 
    \toprule
    \multicolumn{1}{c}{\textbf{Configuration}} & \textbf{Gender} & \textbf{Black} & \begin{tabular}[c]{@{}c@{}}\textbf{East}\\\textbf{Asian}\end{tabular} & \textbf{Indian} & \begin{tabular}[c]{@{}c@{}}\textbf{Latino}\\\textbf{Hispanic}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Middle}\\\textbf{Eastern}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Southeast}\\\textbf{Asian}\end{tabular} & \textbf{White} & \textbf{Average } & \textbf{DoB } & \textbf{SeR } \\ \midrule
    
    \multirow{2}{*}{A Baseline} & Male & 91.24 & 94.08 & 95.88 & 93.06 & 97.54 & 92.93 & 94.30 & \multirow{2}{*}{94.27} & \multirow{2}{*}{2.01} & \multirow{2}{*}{91.96} \\
     & Female & 89.70 & 94.95 & 94.50 & 96.02 & 95.96 & 96.18 & 94.18 &  &  &  \\\cmidrule[0.01pt](lr){2-12}
     
    \multirow{2}{*}{B   + NL (LR=2)} & Male & 91.24 & 95.24 & 96.15 & 94.83 & 97.79 & 94.01 & 95.63 & \multirow{2}{*}{94.67} & \multirow{2}{*}{1.67} & \multirow{2}{*}{\textbf{93.78}} \\
     & Female & 91.81 & 94.70 & 94.89 & 95.18 & 96.21 & 95.29 & 93.04 &  &  &  \\\cmidrule[0.01pt](lr){2-12}

     \multirow{2}{*}{C   + NL (No LR)} & Male & 91.74 & 95.62 & 96.41 & 95.08 & 97.91 & 94.29 & 95.45 & \multirow{2}{*}{\textbf{95.06}} & \multirow{2}{*}{1.67} & \multirow{2}{*}{93.68} \\
     & Female & 91.68 & 95.08 & 95.94 & 96.63 & 95.96 & 95.59 & 93.87 &  &  &  \\\cmidrule[0.01pt](lr){2-12}
     
    \multirow{2}{*}{D   + MT} & Male & 91.61 & 94.85 & 94.82 & 94.45 & 97.91 & 94.56 & 95.99 & \multirow{2}{*}{94.58} & \multirow{2}{*}{1.73} & \multirow{2}{*}{92.83} \\
     & Female & 90.89 & 94.05 & 95.28 & 95.90 & 96.21 & 93.82 & 93.77 &  &  &  \\\cmidrule[0.01pt](lr){2-12}
     
    \multirow{2}{*}{E   + MT + NL} & Male & 91.86 & 94.98 & 96.81 & 93.69 & 97.79 & 93.20 & 94.92 & \multirow{2}{*}{94.59} & \multirow{2}{*}{1.66} & \multirow{2}{*}{93.48} \\
     & Female & 91.41 & 94.95 & 95.41 & 95.66 & 95.45 & 94.12 & 94.08 &  &  &  \\\cmidrule[0.01pt](lr){2-12}
     
    \multirow{2}{*}{F    + MT + NL + EDL} & Male & 91.86 & 94.85 & 95.35 & 94.58 & 97.79 & 92.65 & 94.83 & \multirow{2}{*}{94.70} & \multirow{2}{*}{\textbf{1.62}} & \multirow{2}{*}{93.62} \\
     & Female & 91.55 & 95.73 & 95.41 & 96.02 & 95.71 & 94.41 & 95.02 &  &  &  \\\midrule
     
    \multirow{2}{*}{G CLIP + Linear} & Male & 94.99 & 96.78 & 97.21 & 96.72 & 98.77 & 96.05 & 97.78 & \multirow{2}{*}{\textbf{96.76}} & \multirow{2}{*}{1.10} & \multirow{2}{*}{95.36} \\
     & Female & 94.19 & 96.63 & 97.25 & 97.83 & 96.71 & 96.62 & 96.47 &  &  &  \\\cmidrule[0.01pt](lr){2-12}
     
    \multirow{2}{*}{H   + NL} & Male & 95.12 & 96.78 & 97.21 & 96.72 & 98.52 & 95.92 & 97.15 & \multirow{2}{*}{96.70} & \multirow{2}{*}{\textbf{0.99}} & \multirow{2}{*}{\textbf{95.87}} \\
     & Female & 94.45 & 96.90 & 96.59 & 97.71 & 97.22 & 96.18 & 96.99 &  &  &  \\
      \cmidrule[0.01pt](lr){2-12}
     
    \multirow{2}{*}{I  + MT + NL + EDL} & Male & 95.37 & 96.78 & 97.08 & 97.10 & 98.89 & 95.92 & 97.15 & \multirow{2}{*}{96.70} & \multirow{2}{*}{1.00} & \multirow{2}{*}{95.38} \\
     & Female & 94.32 & 96.63 & 96.46 & 97.11 & 97.22 & 96.75 & 96.78 &  &  &  \\
     
     \midrule
     \midrule
     \multirow{2}{*}{Instagram + Linear~\cite{lp_insta}} & Male & 92.50 & 93.40 & 96.20 & 93.10 & 96.00 & 92.70 & 94.80 & \multirow{2}{*}{93.77} & \multirow{2}{*}{1.73} & \multirow{2}{*}{93.66} \\
     & Female & 90.10 & 94.30 & 95.00 & 94.80 & 95.00 & 94.10 & 91.40 &  &  &  \\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\textbf{Cross dataset} evaluations were done on UTKFace, DiveFace, and Morph Datasets. Table \ref{tab:cross_dataset} shows cross-dataset results. In the calculation of DoB for datasets where the test set is not balanced across demographic groups, a weighted standard deviation was used instead of a standard deviation.  For DiveFace and Morph, various configurations improved the overall fairness over the baseline whereas it was not the case for UTKFace. It must also be noted that across all datasets the proposed method improved the overall classification accuracy regardless.





\begin{table}
\centering
\caption{Cross-dataset evaluation of the proposed model.}
\label{tab:cross_dataset}
\resizebox{\textwidth}{!}{

\begin{tabular}{lccccccccc} 
\toprule
                     & \multicolumn{3}{c}{\textbf{UTKFace}}                                               & \multicolumn{3}{c}{\textbf{DiveFace}}                                              & \multicolumn{3}{c}{\textbf{Morph}}                                                  \\ 
\cmidrule(l){2-10}
                     & \textbf{DoB } & \textbf{Avg. Acc } & \textbf{SeR } & \textbf{DoB } & \textbf{Avg. Acc } & \textbf{SeR } & \textbf{DoB } & \textbf{Avg. Acc } & \textbf{SeR }  \\ 
\midrule
Baseline             & \textbf{1.96}                      & 94.67                        & \textbf{92.60}                   & 0.74                      & 98.45                        & 97.71                   & 7.67                      & 96.26                        & 74.89                    \\
MT                   & 3.01                      & 94.25                        & 88.89                   & 0.78                      & 98.34                        & 97.67                   & 10.01                     & 95.02                        & 69.97                    \\
NL (LR=2)            & 2.55                      & 94.54                        & 90.11                   & \textbf{0.49}             & 98.49                        & \textbf{98.48}          & 8.99                      & 95.95                        & 70.68                    \\
NL (No LR)           & 2.26                      & \textbf{94.76}                        & 91.55                   & 0.51                      & \textbf{98.60}               & 98.39                   & 7.72                      & \textbf{96.41}               & 74.84                    \\
MT + NL              & 2.31                      & 94.47                        & 90.91                   & 0.77                      & 98.40                        & 97.52                   & 6.69                      & 96.21                        & 78.06                    \\
MT + NL + EDL        & 2.27                      & 94.50                        & 91.53                   & 0.83                      & 98.37                        & 97.43                   & \textbf{6.26}             & 96.32                        & \textbf{78.98}           \\
\midrule
CLIP + LP            & 1.86                      & \textbf{96.58}               & 92.91                   & 0.68                      & \textbf{99.08}                        & 97.90                   & \textbf{0.89}                      & \textbf{99.46}                        & \textbf{97.04}                    \\
CLIP + NL            & \textbf{1.60}             & 96.47                        & \textbf{94.59}          & \textbf{0.62}                      & 99.02                        & \textbf{98.09}                   & 1.39                      & 99.19                        & 95.43                    \\
CLIP + MT + NL + EDL & 2.04                      & 96.52                        & 92.85                   & 0.69                      & 99.04                        & 97.81                   & 1.10                      & 99.26                        & 96.33                    \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{\textbf{Rejecting Samples based on Uncertainty as Threshold}} For evidential learning, we replace the classification head such that, it outputs parameters of a Dirichlet Distribution. We also replace the classification loss  with the evidential loss term defined by Equation~\ref{eq:loss}. For our experiments, removing the annealing coefficient  worked better. We may calculate the uncertainty of the prediction using Equation~\ref{eq:unc}. This uncertainty is used as a threshold to reject/accept the prediction of the model.
\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.8]{figures/plots.pdf}
    \caption{Model behaviour at various uncertainty thresholds}
    \label{fig:reject_option}
\end{figure}

Figure~\ref{fig:reject_option} shows the behavior of the model at different uncertainty thresholds. The choice of uncertainty threshold is application dependent. Although it is to be noted, at a threshold of  the model rejects only  of all images and at the same time improving the overall accuracy  and reducing the DoB to  from .

\subsubsection{\textbf{Effectiveness on Other Biometric Modalities}} In order to analyze the generalizability of our method across different biometric modalities, we also conducted similar experiments on an ocular and a periocular dataset. For ocular analysis across gender, we used the VISOB~\cite{visob} dataset. Table ~\ref{tab:results_visob} shows the condensed results on VISOB dataset. The proposed method improved the accuracy as well as the DoB. For periocular analysis, we used the UFPR Periocular~\cite{ufpr} dataset. For this dataset, since race annotations are not available, we present the results across gender only. Table ~\ref{tab:results_ufpr} shows the results on UFPR dataset. As can be seen, the proposed method improved accuracy across gender even for ocular and periocular biometrics. 

\begin{table}[H]
    \centering
    \caption{Gender classification results of the proposed method across gender when trained and tested on VISOB dataset.}
    \label{tab:results_visob}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Config} & \textbf{Avg. Acc } & \textbf{DoB } & \textbf{SeR } \\ \midrule
    Baseline &  87.95 & 15.27 & 48.37 \\
    NL & \textbf{89.17} & \textbf{14.18} & \textbf{52.51} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{Gender classification results of the proposed method across gender when trained and tested on UFPR dataset.}
    \label{tab:results_ufpr}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Config} & \textbf{Male } & \textbf{Female } & \textbf{Overall } \\ \midrule
    Baseline & 96.13 & 95.13 & 95.60 \\
    NL & \textbf{97.13} & \textbf{95.88} & \textbf{96.47} \\
    \bottomrule
    \end{tabular}
\end{table}




\subsection{Ablation Study}
\subsubsection{Neighbour Size}
We observe that increasing the neighbor size of a sample improves the overall accuracy as well as the fairness of the model. For the ablation study, we conducted our experiments on the base NL configuration.

\begin{table}[H]
    \centering
    \caption{Ablation Study: Neighbour Size}
    \label{tab:ablate_neighbour}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Neighbour Size} & \textbf{Avg. Acc } & \textbf{DoB } & \textbf{SeR } \\ \midrule
    1 & 94.35 & 1.95 & 92.20 \\
    3 & 94.58 & 1.69 & 92.69 \\
    5 & 94.50 & 1.73 & 93.39 \\
    7 & \textbf{94.67} & \textbf{1.67} & \textbf{93.78}\\
    \bottomrule
    \end{tabular}
    
\end{table}

\subsubsection{Lazy Regularization}
The lazy regularization value is chosen as a trade-off between training speed and the overall performance. The model works best when  is , i.e. no lazy regularization. Although this particular configuration is the best of all, we decided to stick with the  configuration considering the speed trade-off.
\begin{table}[H]
    \centering
    \caption{Ablation Study : Lazy Regularization}
    \label{tab:ablate_lazy}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{n} & \textbf{Avg. Acc } & \textbf{DoB } & \textbf{SeR } \\ \midrule
    1 & \textbf{95.06} & \textbf{1.67} & 93.68 \\
    2 & 94.67 & \textbf{1.67} & \textbf{93.78} \\
    4 & 94.94 & 1.79 & 93.21 \\
    8 & 94.92 & 1.72 & 92.96 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Distance Metrics}
The distance function  for neighbor regularization can be any function that can compute the distances between embedding. We evaluated L2 and Jensen-Shannon (JS) Divergence. Though cosine similarity and KL divergence are also possibilities. Experiments proved JS Divergence to be a better distance function for the model.
\begin{table}[H]
    \centering
    \caption{Ablation Study : Distance Metrics}
    \label{tab:ablate_dist}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Distance Metric} & \textbf{Avg. Acc } & \textbf{DoB } & \textbf{SeR } \\ \midrule
    L2 & \textbf{94.76} & 1.71 & 92.91 \\
    JS Div & 94.67 & \textbf{1.67} & \textbf{93.78}\\
    \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Effect of Generated views on Backpropagation}
The generated views are solely used for calculating the regularization term from the embeddings in the proposed approach, not for backpropagation or batch statistics calculation. We test this by using backpropagation to train a model using generated views, and we discover that the bias is substantially higher when generated views dominate the data distribution, despite the total accuracy improving marginally.
\begin{table}[H]
    \centering
    \caption{Ablation Study : With BackProp vs Without BackProp}
    \label{tab:ablate_backprop}
    \begin{tabular}{cccc} 
    \toprule
    \textbf{Config} & \textbf{Avg. Acc } & \textbf{DoB } & \textbf{SeR } \\ \midrule
    With BackProp & \textbf{94.79} & 2.11 & 91.14 \\
    Without BackProp & 94.67 & \textbf{1.67} & \textbf{93.78}\\
    \bottomrule
    \end{tabular}
\end{table}

\section{Conclusion and Future Work}
Several studies have demonstrated that deep learning models can discriminate based on demographic attributes for biometric modalities such as face and ocular. Existing bias mitigation strategies often offer the trade-off between accuracy and fairness. In this study, we proposed a bias mitigation strategy that leverages the power of generative views and structured learning to learn invariant features that can improve the fairness and classification accuracy of a model simultaneously. We also propose a rejection mechanism based on uncertainty quantification that efficiently rejects uncertain samples at test time. Extensive evaluation across datasets and two biometric modalities demonstrate the generalizability of our proposed bias mitigation strategy to any biometric modality and classification task.  While the training process we currently use uses pregenerated neighbour samples, future research would explore ways to generate neighbor samples during the training phase to take into account the needs of the task at hand. Further, we will evaluate the efficacy of our proposed approach in mitigating bias for other computer vision tasks such as deepfake and biometric spoof attack detection.



\section{Acknowledgement}
This work is supported from National Science Foundation (NSF) award
no. 2129173. The research infrastructure used in this study is supported in part from a grant no. 13106715 from the Defense University Research Instrumentation Program (DURIP) from Air Force Office of Scientific Research.

\bibliographystyle{splncs04}
\bibliography{ref}
\end{document}
