

\renewcommand\thelstlisting{1}
\begin{figure}[t]
\centering
\begin{minipage}[t]{0.9\linewidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily, mathescape, language=Python, caption={PyTorch-style pseudocode for MobileVOS}, captionpos=t,
numberbychapter=false,label={alg:training_pseudo_code}]
# f_s, f_t: Student and teacher network
# y: Ground-truth one-hot encoded labels
# y_s, y_t: Student and teacher logits
# z_s, z_t: Student and teacher representations
for x, y in loader:
  # Forward pass
  z_s, y_s = f_s(x)
  z_t, y_t = f_t(x)
  z_s = embed_s(z_s)
  
  # Cross entropy loss
  loss = bootstrap_poly_cross_entropy(y_s, y)
  
  # Normalise representations
  z_s_norm = F.normalize(z_s, dim=1)
  z_t_norm = F.normalize(z_t, dim=1)
  
  # Compute correlation-matrices
  c_ss = matmul(z_s_norm, z_s_norm.T)
  c_tt = matmul(z_t_norm, z_t_norm.T)
  
  # Interpolate between KD and SCL
  y_d = downsample(y)
  yy = matmul(y_d, y_d.T)
  
  r =  * c_tt + (1 - ) * yy
  loss += log2(c_ss.pow(2).sum()) / len(z_s) 
  loss -= log2((c_ss * r).pow(2).sum()) / len(z_s)
  
  # Logit distillation
  prob_s = softmax(y_s / , dim=1)
  prob_t = softmax(y_t / , dim=1)
  loss += kl(prob_t, prob_s)
  
  # Optimisation step
  loss.backward()
  optimizer.step()
\end{lstlisting}
\end{minipage}
\end{figure}
%
