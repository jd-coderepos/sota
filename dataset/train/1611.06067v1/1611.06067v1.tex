\def\year{2017}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{comment} \usepackage{graphicx} \usepackage{epstopdf} \usepackage{amsmath} \usepackage{mathrsfs} \usepackage{scrextend} \usepackage{url} \usepackage{subcaption} \usepackage{epstopdf}\usepackage{bm} \usepackage{amssymb} \usepackage{color}
\usepackage{flushend} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{multirow} \newcommand{\mathsfsl}{}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} \renewcommand{\algorithmicensure}{\textbf{Output:}}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}

\setlength{\textfloatsep}{0pt}

 \begin{document}
\title{An End-to-End Spatio-Temporal Attention Model for  Human Action Recognition from Skeleton Data}

\author{
	{Sijie Song{\small }, Cuiling Lan{\small }\thanks{Corresponding author. This work was done at Microsoft Research Asia. This work was supported by National Natural Science Foundation of China under contract No. 61472011 and No. 61303178.}, Junliang Xing{\small }, Wenjun Zeng{\small }}, Jiaying Liu{\small \large} \\
	\	Institute of Computer Science and Technology, Peking University, Beijing, China \\
    \,Microsoft Research Asia, Beijing, China \\
	\,Institute of Automation, Chinese Academy of Sciences, Beijing, China \\	
	\{ssj940920, liujiaying\}@pku.edu.cn,
	\{culan,wezeng\}@microsoft.com,
	jlxing@nlpr.ia.ac.cn
}


\maketitle
\begin{abstract}
Human action recognition is an important task in computer vision. Extracting discriminative spatial and temporal features to model the spatial and temporal evolutions of different actions plays a key role in accomplishing this task. In this work, we propose an end-to-end spatial and temporal attention model for human action recognition from skeleton data. We build our model on top of the Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which learns to selectively focus on discriminative joints of skeleton within each frame of the inputs and pays different levels of attention to the outputs of different frames. Furthermore, to ensure effective training of the network, we propose a regularized cross-entropy loss to drive the model learning process and develop a joint training strategy accordingly. Experimental results demonstrate the effectiveness of the proposed model, both on the small human action recognition dataset of SBU and the currently largest NTU dataset.


\end{abstract}


\section{1~~Introduction}
\label{sec:introduction}
Recognition of human action is a fundamental yet challenging task in computer vision. It facilitates many applications such as intelligent video surveillance, human-computer interaction, video summary and understanding \cite{IVC10SurveyAction,CVIU11SurveyAction}. The key to the success of this task is how to extract discriminative spatial temporal features to effectively model the spatial and temporal evolutions of different actions.


One general approach  focuses on the recognition from RGB videos \cite{CVIU11SurveyAction}. Since each frame is a capture of the highly articulated human in a two-dimensional space, it loses some information of the three-dimensional (3D) space and then loses the flexibility of achieving human location and scale invariance. The other general approach leverages the high level information of skeleton data, which represents a person by the 3D coordinate positions of key joints (i.e., head, neck,, foot). Such representation is robust to variations of locations and viewpoints. Without combining RGB information, there is a lack of appearance information. Fortunately, biological observations from the early seminal work of Johansson suggest that the positions of a small number of joints can effectively represent human behavior even without appearance information \cite{PP73Perception}. Skeleton-based human representation has attracted increasing attention for recognizing human actions thanks to its high level representation and robustness to variations of locations and appearances \cite{han2016space}. The prevalence of cost-effective depth cameras such as Microsoft Kinect \cite{zhang2012microsoft} and the advance of a powerful human pose estimation technique from depth \cite{Shotton2011} make 3D skeleton data easily accessible. This boosts  research on skeleton-based human action recognition. In this work, we focus on recognition from skeleton data.
\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{figs/action_seq_green.pdf}
\vspace{-5mm}
	\caption{Illustration of the procedure for an action ``punching". An action may experience different stages, and involve different discriminative subsets of joints (as the red circles). }
\label{fig:skeleton}
\end{figure}


Fig. 1 shows an example of a series of skeleton frames (and RGB images) for the action ``punching". Each human body is represented by key joints in terms of coordinate positions in the 3D space. The articulated configurations of joints constitute various postures and a series of postures in a certain time order identifies an action. With the skeleton as an explicit high level representation of human pose, many works design algorithms taking the positions of joints as inputs. There are two basic components in these works. One is the design and mining of discriminative features from the skeleton, such as histograms of 3D joint locations (HOJ3D) \cite{CVPR12HO3DJ}, pairwise relative position features \cite{CVPR12Actionlet}, relative 3D geometry features \cite{vemulapalli2016r3dg}. The other is the modeling of temporal dynamics, such as Hidden Markov Model \cite{CVPR12HO3DJ}, Conditional Random Fields \cite{ICCV05CRF}, and Recurrent Neural Networks \cite{CVPR15HRNN}. In this work, we present a spatio-temporal attention model to incorporate the two components into an end-to-end deep learning architecture.


For spatial joints of skeleton, we propose a spatial attention module which conducts automatic mining of discriminative joints. A certain type of action is usually only associated with and characterized by the combinations of a subset of kinematic joints \cite{CVPR12Actionlet}. As the action proceeds, the associated joints may also change accordingly. For example, the joints ``hand", ``elbow", and ``head" are discriminative for the action ``drinking" while the joints from legs can be considered as noise. For an action ``approaching and shaking hands", at the beginning, the legs may be paid attention to; at the middle stage, the arms attract more attention. In contrast to actionlet \cite{CVPR12Actionlet}, the attentions to joints are allowed to vary over time, being content-dependent.


Furthermore, for a sequence of frames, we propose a temporal attention module which explicitly learns and allocates the content-dependent attentions to the output of each frame to boost recognition performance. For a sequence of some action, the flow of the action may experience different stages, e.g., the preparation, climax, and the end (Fig. \ref{fig:skeleton}). Taking the action ``punching" as an example, the two persons approach each other, stretch out the hands, and kick out the legs. The frames for identifying stretching out the hands and kicking out the legs are a part of the key sub-stage. Different sub-stages have different degrees of importance and robustness to variations. In this paper, in contrast to the ideas of extracting key frames \cite{carlsson2001action,BMVC08Information}, our proposed scheme pays different attentions to different frames instead of simply skipping frames.




In summary, we have made the following four main contributions in this work.
\begin{itemize}
\setlength{\itemsep}{0pt}\item An end-to-end framework with two types of attention modules is designed based on the LSTM networks for skeleton based human action recognition.

\setlength{\parsep}{0pt}\item A spatial attention module with joint-selection gates is designed to adaptively allocate different attentions to different joints of the input skeleton within each frame. A temporal attention module with frame-selection gate is designed to allocate different attentions to different frames.

\setlength{\parskip}{0pt}\item Spatio-temporal regularizations are proposed to enable the better learning of the networks.


\setlength{\parskip}{0pt}\item A joint training strategy is designed to efficiently train the entire end-to-end network.
\end{itemize}



\section{2~~Related Work}
\label{sec:relatedwork}

\subsection{2.1~~Spatial Co-Occurrence Exploration}
\label{subsec:Cooccurrence}
An action is usually associated with and characterized by the interactions and combinations of a subset of skeleton joints. An actionlet ensemble model is proposed to mine such discriminative joints \cite{CVPR12Actionlet}, where an actionlet is a particular conjunction of the features for a subset of the joints and an action is represented as a linear combination of the actionlets. For example, for the action ``drinking", the subset of joints including ``hand", ``elbow", and ``head" composes a actionlet. Orderlet makes an extension of actionlet by including the feature of pairwise joint distance and allowing various sizes of a subset \cite{yu2014discriminative}. Actionlets or orderlets are mined from training samples for robust performance. In a recurrent neural network, a group sparsity constraint is introduced to the connection matrix to encourage the network to explore the co-occurrence of joints \cite{zhu2015co}.


In the above methods, once the mining is done, the degrees of importance of joints/features are fixed and there will be no change for different temporal frames and sequences. In contrast, our spatial attention module determines the degrees of importance of joints on the fly based on the contents.

\subsection{2.2~~Temporal Key Frame Exploration}
For identifying an action, not all frames in a sequence have the same importance. Some frames capture less meaningful information, or even carry misleading information associated with other types of actions, while some other frames carry more discriminative information \cite{liu2013boosted}. A number of approaches have proposed  using key frames as a representation for action recognition. One is to utilize the conditional entropy of visual words to measure the discriminative power of a given frame and the classification results from the top 25\% most discriminative frames are employed to make a majority vote for recognition \cite{BMVC08Information}. Another one employs the AdaBoost algorithm to select the most discriminative key frames for human action recognition \cite{liu2013boosted}. The learning of key frames can also be cast in a max-margin discriminative framework by treating them as latent variables \cite{raptis2013poselet}.


Leveraging key frames can help exclude noise frames, e.g., frames which are less relevant to the underlying actions. However, in comparisons to the holistic based approaches \cite {simonyan2014two,wu2015modeling,zhu2015co} which use all the frames, it loses some information. In this paper, our temporal attention module determines the degree of importance for each frame. Instead of skipping frames, it allocates different attention weights to different frames to automatically exploit their respective discriminative power and focus more on the important frames.


\subsection{2.3~~Attention-Based Models}
When observing the real-world, a human usually focuses on some fixation points at the first glance of the scene, i.e., paying different attentions to different regions \cite{goferman2012context}. Many applications leverage predicted saliency maps for performance enhancement \cite{yu2010object,jiang2014saliency,bazzani2016recurrent}, which explicitly learn the saliency maps guided by human labeled groundtruths.


The human labeled groundtruths for the explicit attention, however, are generally unavailable and might not be consistent with real attention related to the specific tasks.  Recently, the exploitation of an attention model which implicitly learns attention has attracted increasing interest in various fields, such as machine translation \cite{bahdanau2014neural}, image caption generation \cite{xu2015show}, and image recognition \cite{ba2014multiple}. Selective focus on different spatial regions is proposed for action recognition on RGB videos \cite{sharma2015action}. Ramanathan et al. propose an attention model which learns to detect events in RGB videos while attending to the people responsible for the event \cite{Ramanathan2015action}. The fusion of neighboring frames within a sliding window with learned attention weights is proposed to enhance the performance of dense labeling of actions in RGB videos \cite{yeung2015every}. However, all the attention models mentioned above for action recognition are based on RGB videos. There is a lack of investigation of skeleton sequences, which exhibit different characteristics from RGB videos.


\section{3~~Overview of RNN and LSTM}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.25\linewidth}
		\centering\includegraphics[width=\textwidth]{figs/RNN.pdf}
		\caption{}
		\label{subfig:RNN}
	\end{subfigure}	
	\begin{subfigure}[t]{0.65\linewidth}
		\centering\includegraphics[width=\textwidth]{figs/LSTM.pdf}
		\caption{}			
		\label{subfig:LSTM_unit}
	\end{subfigure}
	\vspace{-3mm}
	\caption[]{Structures of the neurons. (a) RNN,
		(b) LSTM.}\label{fig:RNNLSTM}
	\vspace{1.5mm}
\end{figure}
In this section, we briefly review the Recurrent Neural Network (RNN), and Long Short-Term Memory (LSTM) to make the paper self-contained.

RNN is a popular model for sequential data modeling and feature extraction \cite{Graves2012}. Fig. \ref{fig:RNNLSTM}(a) shows an RNN neuron. The output response  at time step  is determined by the input  and the hidden outputs from RNN themselves at the last time step 

\begin{comment}

\end{comment}
where  represents a non-linear activation function,  and  denote the learnable connection vectors, and  is the bias value. The recurrent structure and the internal memory of RNN facilitate its modeling of the long-term temporal dynamics of the sequential data.


LSTM is an advanced RNN architecture which mitigates the vanishing gradient effect of RNN \cite{LSTM1997,vanish2001,Graves2012}. As illustrated in Fig. \ref{fig:RNNLSTM}(b), an LSTM neuron contains a memory cell  which has a self-connected recurrent edge of weight 1. At each time step , the neuron can choose to write, reset, and read the memory cell governed by the input gate , forget gate  and output gate .
\begin{comment}
Their activations of LSTM neurons can be summarized as follows:

where  is the element-wise product,  is the sigmoid activation function,  denotes the weighting matrix between  and  , , .
\end{comment}



\section{4~~Deep LSTM with Spatio-Temporal Attention Model}
\label{sec:algorithm}
\begin{figure}[t] \begin{center}
		\includegraphics[width=0.92\linewidth]{figs/subnets.pdf}
	\end{center}
\vspace{-4mm}
	\caption{Overall architecture of our proposed network, which consists of the main LSTM network, the spatial attention subnetwork, and the temporal attention subnetwork.}
	\label{fig:Subnets}
\vspace{1.8mm}
\end{figure}
We propose an end-to-end multi-layered LSTM network with spatial and temporal attention mechanisms for action recognition. The network is designed to automatically select dominant joints within each frame through the spatial attention module, and assign different degrees of importance to different frames through the temporal attention module. Fig. \ref{fig:Subnets} shows its overall architecture, which consists of a main LSTM network, a spatial attention subnetwork, and a temporal attention subnetwork. Because of the inter-play among the three subnetworks, it is challenging to train the network.


In the following, we discuss the proposed spatial attention module and temporal attention module respectively, which are both built based on the LSTM networks. We then introduce a regularized learning objective of our model and a joint training strategy, which help overcome the difficulty of model learning for the highly coupled network.





\subsection{4.1 Spatial Attention with Joint-Selection Gates}

The action of persons can be described by the evolution of a series of human poses represented by the 3D coordinates of joints. In general, different actions involve different subsets of joints as discussed in Section 2.1. 

We propose a spatial attention model to automatically explore and exploit the different degrees of importance of joints. With a soft attention mechanism, each joint within a frame is assigned a spatial attention weight based on the joint-selection gates. This enables our model to adaptively focus more on those discriminative joints.


At each time step , given the full set of  joints , with , the scores  for indicating the importance of the  joints are jointly obtained as

where , ,  are the learnable parameter matrixes, ,  are the bias vectors.  denotes the hidden variable from an LSTM layer as illustrated in Fig. \ref{fig:Subnets}. For the  joint, the activation as the joint-selection gate is computed as

which is a normalization of the scores. The set of gates controls the amount of information of each joint to flow to the main LSTM network. Among the joints, the larger the activation, the more important this joint is for determining the type of action. We also refer to the activation values as attention weights. Instead of assigning equal degrees of importance to all the joints , as illustrated in Fig. \ref{fig:network}, the input to the main LSTM network is modulated to , with .
\begin{comment}

\end{comment}

Note that the proposed spatial attention model determines the importance of joints based on all the joints of the current time step and the hidden variables from an LSTM layer. On one hand, the hidden variables  contain information of past frames, benefiting from the merit of LSTM which is capable of exploring temporal long range dynamics. In this paper, the spatial attention subnetwork composes of an LSTM layer, two fully connected layers and a normalization unit as illustrated in Fig. \ref{fig:Subnets}. On the other hand, leveraging all joints within the current frame provides necessary ingredient for determining their importance.


Bridged by the joint-selection gate, the main LSTM network and the spatial attention subnetwork can be jointly trained to implicitly learn the spatial attention model.



\begin{comment}
In this paper, we propose spatial attention model to automatically explore and exploit the importances of joints for each skeleton frame. With a soft attention mechanism, each joint within a frame is allocated to a spatial attention weight based on the soft joint-selection gates, which allows our model to focus more on those discriminative joints at each frame corresponding to that action. Thus, at each time step , given the full set of  joints within one frame , the weighted input  is computed:

where  is the sequence of spatial attention weight,  is element-wise multiplication. Considering the current skeleton context  and the memorized information from the hidden state  of the last time step, the weights can be computed as:



where  is jointly trained with all the other parameters of the neural network and  is obtained through feed-forward layers. The spatial attention  represents the importance of  joint , which can be thought of how active our model believes the joint is in the current frame at time .
\end{comment}

\begin{figure}[t]
\begin{center}
		\includegraphics[width=1\linewidth]{figs/framework-objectfunction.pdf} \end{center}
	\vspace{-4mm}
	\caption{Illustration of how spatial attention output  and temporal attention output  influence the LSTM network.}
\label{fig:network}
	\vspace{1mm}
\end{figure}

\subsection{4.2 Temporal Attention with Frame-Selection Gate}

For a sequence, the amount of valuable information provided by different frames is in general not equal. Only some of the frames (key frames) contain the most discriminative information while the other frames provide context information. For example, for the action ``shaking hands", the sub-stage ``approaching" should have lower importance than the sub-stage of ``hands together". Based on such insight, we design a temporal attention module to automatically pay different levels of attention  to different frames.

For the sequence level classification, based on the output  of the main LSTM network and the temporal attention value  at each time step , the scores for  classes are the weighted summation of the scores at all time steps
\vspace{-1mm}

where ,  denotes the length of the sequence. Fig. \ref{fig:network} illustrates how the temporal attention output  is incorporated to the main LSTM network. The predicted probability being the  class given a sequence  is



As illustrated in Fig. \ref{fig:Subnets}, the attention module is composed of an LSTM layer, a fully connected layer, and a ReLU non-linear unit, being connected in series. It plays the role of soft frame selection. The activation as the frame-selection gate can be computed as

which depends on the current input , and the hidden variables  of time step  from an LSTM layer. We use the non-linear function of ReLU due to its good convergence performance. The gate controls the amount of information of each frame to be used for making the final classification decision. The works \cite{CVPR15HRNN,zhu2015co} are our special cases where the attention weights on each frame are equal.

Bridged by the frame-selection gate, the main LSTM network and the temporal attention subnetwork can be jointly trained to implicitly learn the temporal attention model.
\begin{comment}

where  is a scalar whose value reflects the temporal attention weights, depending on the current input and hidden state from the last time step,  represents the parameters of PReLU \cite {he2015delving} jointly trained in the whole network.

For a video sequence, with the temporal attention assigned on each frame, we fuse all the hidden states across temporal domain \cite{CVPR15HRNN,zhu2015co} to summarize the relevant information and predict the final class label. Concretely, the hidden layer is then feed into a fully connected layer, whose outputs are accumulated with different temporal attention weights to get the probability that a sequence  belongs to the class :



where , and the labeling indicating different class types is denoted by ,  represents the length of the video sequence,  indicates the hidden state of the top layer of the LSTM network.
\end{comment}

\subsection{4.3 Joint Spatial and Temporal Attention}

The purpose of the attention models is to enable the network to pay different levels of attention to different joints and assign different degrees of importance to different frames as an action proceeds. We integrate spatial and temporal attention in the same network as illustrated in Fig. \ref{fig:Subnets}. How the spatial attention model acts on the input and how the temporal attention model acts on the output of the main LSTM network are illustrated in Fig. \ref{fig:network}.

\subsubsection{Regularized Objective Function}

We formulate the final objective function of the spatio-temporal attention network with a regularized cross-entropy loss for a sequence as,

where   denotes the groundtruth label. If it belongs to the  class, then  and  for .  indicates the probability that the sequence is predicted as the  class, where . The scalars , , and  balance the contribution of the three regularization terms. We discuss the regularization designs in the following.  

The first regularization item is designed to encourage the spatial attention model to dynamically focus on more spatial joints  in a sequence. We found the spatial attention model is prone to consistently ignoring many joints along time even though these joints are also valuable for determining the type of action, i.e., trapped to a local optimum. We introduce this regularization item to avoid such ill-posed solutions. For clarity, we re-describe it as , with . This encourages paying equal attentions to different joints.
\begin{comment}

\end{comment}

The second regularization item is to regularize the learned temporal attention values under control with  norm rather than to increase them unboundedly. This alleviates  gradient vanishing in the back propagation, where the back-propagated gradient is proportional to .

The third regularization item with  norm is to reduce overfitting of the networks.  denotes the connection matrix (merged to one matrix here) in the networks.




\subsubsection{Joint Training of the Networks} Due to the mutual influence of the three networks, the optimization is rather difficult. We propose a joint training strategy to efficiently train the spatial and temporal attention modules, as well as the main LSTM network. The separate pre-training of the attention modules ensures the convergence of the networks. The training procedure is described in Algorithm \ref{alg:Framwork}.
\vspace{-3mm}
\begin{algorithm}[htb]


	\caption{Joint Training of the LSTM Network with Spatio-Temporal Attention Model.}
	\label{alg:Framwork}
	\begin{algorithmic}[1]
	    \REQUIRE model training parameters ,  (e.g., , ).


	    \STATE Initialize the network parameters using Gaussian. 

	    \textbf{// Pre-train Temporal Attention Model.}
	     	
		\STATE With spatial attention weights being fixed as ones, jointly train the main LSTM network with only one LSTM layer and the temporal attention subnetwork to obtain the temporal attention model.
		\STATE Fix this learned temporal attention subnetwork. Train the main LSTM network after increasing its number of LSTM layers to three by  iterations.
		\STATE Fine-tune this temporal attention subnetwork and the main LSTM network by  iteration.
		
		\textbf{// Pre-train Spatial Attention Model.}
		
		\STATE With temporal attention weights being fixed as ones, jointly train the main LSTM network with only one LSTM layer and the spatial attention subnetwork to obtain the spatial attention model.
		\STATE Fix this learned spatial attention subnetwork. Train the main LSTM network after increasing its number of LSTM layers to three by  iterations.
		\STATE Fine-tune this spatial attention subnetwork and the main LSTM network for  iterations.
		
		\textbf{// Train the Main LSTM Network.}
		
		\STATE Fix both the temporal and spatial attention subnetworks learned in Step-4 and Step-7. Fine-tune the main LSTM network by  iterations.
		
		\textbf{// Jointly Train the Whole Network.}
		\STATE Jointly fine-tune the whole network (main LSTM network, the spatial attention subnetwork, and the temporal attention subnetwork) by  iterations.
		\ENSURE  the final converged whole model.
	\end{algorithmic}
\end{algorithm}
\vspace{-3mm}

\begin{comment}
\begin{algorithm}[htb] \caption{ Combined training of spatial and temporal attention based network.} \label{alg:Framwork} \begin{algorithmic}[1] \STATE Set the spatial attention weight to each joint to 1. With such fixed spatial attention weights, jointly train the main subnetwork with 1-layer LSTM and the temporal attention subnetwork .
		\STATE Fix the learned temporal attention subnetwork. Train the main subnetwork with 3-layer LSTM for  iterations, e.g. .
		\STATE Jointly train temporal attention subnetwork and main subnetwork for  iterations, e.g. .
		\STATE Set the temporal attention weight to each joint to 1. Repeat step  to train spatial attention network.
		\STATE Fix both temporal and spatial attention network. Train the main subnetwork for  iterations.
		\STATE Jointly train the main subnetwork and the temporal attention subnetwork for  iterations.
	\end{algorithmic}
\end{algorithm}
\end{comment}
\begin{comment}
\begin{algorithm}[htb] \caption{ Combined training of spatial and temporal attention based network.} \label{alg:Framwork} \begin{algorithmic}[1] \STATE Set the spatial attention weight to each joint to 1. With such fixed spatial attention weights, train the main subnetwork and the temporal attention subnetwork jointly for  iterations, e.g., . Set the round id . \label{code:fram:1}
		\STATE Fix the learned temporal attention subnetwork. Jointly train the main subnetwork and the spatial attention subnetwork for  iterations. \label{code:fram:2}
		\STATE Fix the learned spatial attention model. Jointly train the main subnetwork and the temporal attention subnetwork for  iterations. set round id . \label{code:fram:3}	
		\STATE When , go to step 2; otherwise, stop. \label{code:fram:4}
    \end{algorithmic}
\end{algorithm}
\begin{algorithm}[htb]
\caption{ Framework of ensemble learning for our system.}
\label{alg:Framwork}
\begin{algorithmic}[1] \REQUIRE ~~\\ The set of positive samples for current batch, ;\\
The set of unlabelled samples for current batch, ;\\
Ensemble of classifiers on former batches, ;
\ENSURE ~~\\ Ensemble of classifiers on the current batch, ;
\STATE Extracting the set of reliable negative and/or positive samples  from  with help of ;
\label{ code:fram:extract }\STATE Training ensemble of classifiers  on , with help of data in former batches;
\label{code:fram:trainbase}
\STATE ;
\label{code:fram:add}
\STATE Classifying samples in  by ;
\label{code:fram:classify}
\STATE Deleting some weak classifiers in  so as to keep the capacity of ;
\label{code:fram:select}
\RETURN ; \end{algorithmic}
\end{algorithm}
\end{comment}





\begin{comment}
We integrate spatial and temporal attention in the same network, which is able to pay different attention to each joint and assign different importance levels to each frame as the action proceeds. Due to the normalized spatial attention through softmax function, it is possible for the network to ignore some joints though they also contain dominant features. Taking the various length of the video sequence into account, to encourage the network pay equal attention to every joint \cite{xu2015show,Ramanathan2015action}, we introduce an additional constraint which can be given by:

This attention penalty can force the model to attend every joint at some point during as the time flows. Finally, we formulate the objective function of the spatial-temporal attention network with a regularised cross-entropy loss as follows:

where  is the one hot label vector,  is the probability that the whole video is labelled as the  class,  is a scalar to balance the contribution of the regularization term, and  is the weight decay coefficient. Besides,  denotes connection matrix in the model.

The LSTM network in Fig.\ref{fig:network} is hard to train because of the huge parameter space. Thus, we train the network iteratively to simplify the training process. Spatial attention weights are first set to 1 to every joint and only temporal attention weights are learned. Then we fix the temporal
attention weights and optimize the parameters in the model of spatial attention. Our LSTM network can be optimized to its best performance after several iterations.
\end{comment}


\section{5.~~Experimental Results}
\subsection{5.1~~Datasets and Settings}
We perform our experiments on the following two datasets: the SBU Kinect interaction dataset \cite{yun2012two}, and the largest RGB+D dataset of NTU (Shahroudy et al. 2016).

\textbf{SBU Kinect Interaction Dataset (SBU).} The SBU dataset is an interaction dataset with two subjects. It contains 230 sequences of 8 classes (6614 frames) with subject independent 5-fold cross validation. Each person has 15 joints and the dimension of the input vector is . Note that we smooth each joint's position of the skeleton in the temporal domain to reduce the influence of noise \cite{CVPR15HRNN,zhu2015co}.


\textbf{NTU RGB+D Dataset (NTU).} The NTU dataset is currently the largest action recognition dataset with high quality skeleton \cite{Shahroudy_2016_CVPR}. It contains 56880 sequences (with 4 million frames) of 60 classes, including Cross-Subject (CS) and Cross-View (CV) settings. Each person has 25 joints. We apply the similar normalization preprocessing step to have position and view invariance \cite{Shahroudy_2016_CVPR}. To avoid destroying the continuity of a sequence, no temporal down-sampling is performed.


\textbf{Implementation Details.} For the network and parameter settings, we use three LSTM layers for the main LSTM network, and one LSTM layer for each attention network. Each LSTM layer composes of 100 LSTM neurons. We set , , and  to , , and  for the SBU dataset, and ,  and  for the NTU dataset experimentally. Adam \cite{kingma2014adam} is adopted to automatically adjust the learning rate during optimization. The batch sizes for the SBU dataset and the NTU dataset are  and  respectively. Dropout is utilized to mitigate overfitting \cite{ICLR15DropoutLSTM}.





\subsection{5.2~~Visualization of the Learned Attentions}
We analyze where the learned spatial and temporal attention attend to by visualizing the attention weights in the test.
\begin{figure}[http]
	\vspace{-3mm}
	\centering
	\begin{subfigure}[t]{0.36\textwidth}
		\centering\includegraphics[scale=0.28]{figs/vis_tem_1.pdf}
		\vspace{-2mm}
		\caption{}
		\label{fig:vis_tem_skeleton}
	\end{subfigure}
	
	\begin{subfigure}[t]{0.22\textwidth}
		\centering\includegraphics[scale=0.27]{figs/tem.pdf}
		\vspace{-2.2mm}
		\caption{}
		\label{fig:skeleton2}
	\end{subfigure}
	\begin{subfigure}[t]{0.22\textwidth}
		\centering\includegraphics[scale=0.28]{figs/diff_tem.pdf}
		\vspace{-2.2mm}
		\caption{}
		\label{fig:skeleton2}
	\end{subfigure}
	\vspace{-4mm}
	\caption[]{Visualization of the spatial and temporal attention weights from our model for the action ``kicking". (a) Spatial attention weights. The larger of the red circle, the higher of the attention on that joint. We only mark on the 8 joints with the largest attentions. (b) Temporal attention weights  on each frames. (c) Differentiated temporal attention weights (i.e., ). Best viewed in color.}\label{fig:discuss}
\end{figure}
\vspace{-2mm}

\begin{comment}
\begin{figure}[th]
	\vspace{-3mm}
	\begin{center}
		\includegraphics[scale = 0.17]{figs/vis_tem.png}
	\end{center}
	\vspace{-4mm}
	\caption{Visualization of spatial and temporal attention weights of ``Kicking" (top) and ``Pushing" (bottom). (a) to (f) show the spatial attention weights (marked by green circles, the larger of the circle, the higher of the attention intensity) on those frames. The right figure shows the temporal attention weights as the action proceeds. Best viewed in color.}
	\label{fig:discuss}
	\vspace{0mm}	
\end{figure}
\end{comment}




\textbf{Spatial Attention.} For a sequence of action ``kicking", Fig. \ref{fig:discuss}(a) shows the amplitude of the spatial attention weights on joints by the sizes of the red circles. We also present concrete attention values in Fig. \ref{fig:spa_att_vis}. The attention weights on the left foot, right elbow and left hand of the right person are large. Meanwhile, the weights on the torso and right foot of the left person are large. Being content-dependent, the attentions vary across frames. The learned important types of joints are consistent with what human perceives.


\textbf{Temporal Attention.} Fig. \ref{fig:discuss}(b) shows the temporal attention weights . Fig. \ref{fig:discuss}(c) shows the differentiated attention weights (i.e., ) for ``Kicking". Since the LSTM network usually accumulates more information as time goes, the attention weight usually increases correspondingly. The increased amplitude of the attention weight, i.e., , can indicate the importance of the frame . We can see the differentiated attention weight goes up to a climax as the person on the right lifts his foot to the highest point, which human also considers as more discriminative.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.85\linewidth]{figs/kick_spa.pdf}
\vspace{-4mm}
	\end{center}
\caption{Visualization of spatial attention on the two actors of the action ``kicking" for a sequence. Vertical axis denotes the joint indexes. Horizontal axis denotes the frame indexes (time). Color values indicate the spatial attention weights.}
\label{fig:spa_att_vis}
\vspace{1mm}
\end{figure}


\subsection{5.3~~Effectiveness of the Proposed Attention Models}
\begin{figure}[th]
	\vspace{-3mm}
	\centering
	\begin{subfigure}[t]{0.15\textwidth}
		\centering\includegraphics[width=2.7cm,height=3.5cm]{figs/gain_sbu.pdf}
		\vspace{-4mm}
		\caption{SBU}
		\label{fig:skeleton1}
		
	\end{subfigure}
	\begin{subfigure}[t]{0.15\textwidth}
		\centering\includegraphics[width=2.7cm,height=3.5cm]{figs/gain_cs.pdf}
\vspace{-4mm}
		\caption{NTU-CS}
		\label{fig:skeleton2}
		
	\end{subfigure}
	\begin{subfigure}[t]{0.15\textwidth}
		\centering\includegraphics[width=2.7cm,height=3.5cm]{figs/gain_cv.pdf}
		\vspace{-4mm}
		\caption{NTU-CV}
		\label{fig:skeleton2}
	\end{subfigure}
	\vspace{-3mm}
	\caption[]{Performance evaluation of our attention models, and the regularization items on two datasets in accuracy (\%).}\label{fig:in-comp}
\vspace{-3.5mm}	
\end{figure}
\vspace{1.8mm}
To validate the effectiveness of our designs, we conduct experiments with different configurations as follows.
\vspace{-1mm}
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
	\item \textbf{LSTM}: main LSTM network without attention designs.
	\item \textbf{SA-LSTM(w/o reg.)}: LSTM  + spatial attention without regularization (only includes  and  items in (\ref{equ:lossfuntion})).
	\item \textbf{SA-LSTM}: LSTM  + spatial attention network.
	\item \textbf{TA-LSTM(w/o reg.)}: LSTM + temporal attention without regularization(only includes  and  items in (\ref{equ:lossfuntion})).
	\item \textbf{TA-LSTM}: LSTM + temporal attention network.
	\item \textbf{STA-LSTM}: LSTM+spatio-temporal attention network.
\end{itemize}

Fig. \ref{fig:in-comp} shows the performance comparisons on the SBU, NTU (Cross-Subject), NTU (Cross-View) datasets respectively. We can see in comparison with the baseline scheme LSTM, the introduction of the spatial attention module (SA-LSTM) and the temporal attention module (TA-LSTM) brings up to 5.1\% and 6.4\% accuracy improvement, respectively. The best performance is achieved by combining both modules (STA-LSTM).

In the objective function as defined in (\ref{equ:lossfuntion}), the second and the third items for regularizations are designed for the spatial attention and temporal attention model, respectively. We can see they improve the performance of both spatial attention model and temporal attention model.




\begin{comment}
\begin{table}[htbp]
\fontsize{8pt}{9pt}\selectfont\centering
\begin{center}
\caption{Accuracy(\%) comparisons for our spatial and temporal attention models on NTU dataset.} \label{table:In-comp}
\begin{tabular}{c|c|c|c}
\hline
Methods & SBU & Cross Subject) & Cross View \\
\hline
Baseline & 86.72 & 66.80 & 77.53\\
\hline
SA-LSTM & 88.00 & 71.89 & 80.35\\
\hline
TA-LSTM & 88.79 & 73.16 & 80.51\\
\hline
STA-LSTM & 91.51 & 73.42 & 81.23\\
\hline
\end{tabular}
\end{center}
\end{table}
\end{comment}



\subsection{5.5~~Comparisons to Other State-of-the-Art}


We show performance comparisons of our final scheme with the other state-of-the-art methods in Table \ref{table:SBU} and Table \ref{table:NTU} for the SBU and NTU datasets, respectively. Thanks to the introduction of the spatio-temporal attention models with efficient regularizations and the training strategy, our model is capable of extracting discriminative spatio-temporal features. We can see that our scheme achieves about 10\% accuracy gain on the NTU dataset for the Cross-Subject and Cross-View settings, respectively.
\begin{table}[htbp]
\vspace{-3mm}
	\fontsize{8pt}{9pt}\selectfont\centering
	\begin{center}
		\caption{Comparisons on the SBU dataset in accuracy (\%).} \label{table:SBU}
		\vspace{-2mm}
		\begin{tabular}{c|c}
			\hline
			Methods & Acc. (\%) \\
			\hline
			Raw skeleton \cite{yun2012two} & 49.7 \\
			\hline
			Joint feature \cite{yun2012two} & 80.3 \\
			\hline
			Raw skeleton \cite{ji2014interactive} & 79.4 \\
			\hline
			Joint feature \cite{ji2014interactive} & 86.9 \\
			\hline
			Hierarchical RNN \cite{CVPR15HRNN} & 80.35 \\
			\hline
			Co-occurrence RNN \cite{zhu2015co} & 90.41 \\
			\hline
\hline
			STA-LSTM & \textbf{91.51} \\
			\hline
		\end{tabular}
	\end{center}
\vspace{-8mm}
\end{table}

\begin{table}[htbp]
	\fontsize{8pt}{9pt}\selectfont\centering
	\begin{center}
		\caption{Comparisons on the NTU dataset with Cross-Subject and Cross-View settings in accuracy (\%).} \label{table:NTU}
		\vspace{-2mm}
		\begin{tabular}{c|c|c}
			\hline
			Methods & CS  & CV \\
			\hline
			Lie Group (Vemulapalli et al. 2014) & 50.1  & 52.8\\ \hline
			Skeleton Quads (Evangelidis et al. 2014)  & 38.6 & 41.4\\ \hline
			Dynamic Skeletons  \cite{hu2015jointly} & 60.2  & 65.2\\
			\hline
			HBRNN  \cite{CVPR15HRNN} & 59.1 &  64.0\\
			\hline
Deep LSTM \cite{Shahroudy_2016_CVPR} & 60.7  & 67.3\\
			\hline
			Part-aware LSTM   \cite{Shahroudy_2016_CVPR} & 62.9 & 70.3  \\
			\hline
\hline
STA-LSTM & \textbf{73.4} &  \textbf{81.2} \\ \hline
		\end{tabular}
	\end{center}
\vspace{-6mm}	
\end{table}





\section{6.~~Conclusion}
We present an end-to-end spatio-temporal attention model for human action recognition from skeleton data. To select discriminative joints automatically and adaptively, we propose a spatial attention module with joint-selection gates to assign different importance to each joint. To automatically exploit the different levels of importance of different frames, we propose a temporal attention module to allocate different attention weights to each frame of the whole sequence. Finally, we design a joint training procedure to efficiently combine spatial and temporal attention with a regularized cross-entropy loss. Experimental results demonstrate the effectiveness of our proposed model which achieves remarkable performance improvement in comparison with other state-of-the-art methods.




\bibliographystyle{aaai}
\small
\bibliography{egbib}
\begin{comment}
\begin{table}[htbp]
	\fontsize{8pt}{9pt}\selectfont\centering
	\begin{center}
		\caption{Performance evluation of our spatial and temporal attention models on two datasets in accuracy (\%).} \label{table:In-comp}
\begin{tabular}{c|c|c|c}
			\hline
			\multirow{2}{*} {Methods} & \multirow{2}{*}{SBU} & \multicolumn{2}{c}{NTU} \\
			\cline{3-4} & & Cross Subject & Cross View \\
			\hline
			LSTM & 86.7 & 66.8 & 77.5\\
			\hline
			SA-LSTM & 88.0 & 71.9 & 80.4\\
			\hline
			TA-LSTM & 88.8 & 73.2 & 80.5\\
			\hline
			STA-LSTM & \textbf{91.5} & \textbf{73.4} & \textbf{81.2}\\
			\hline
			\hline
			SA-LSTM (w/o regularization) & - & - & -\\
			\hline
			TA-LSTM (w/o regularization) & - & - & -\\
			\hline
		\end{tabular}
	\end{center}
\end{table}
\end{comment}
\end{document}
