

\documentclass{sig-alternate-05-2015}

\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{url}
\usepackage{breakurl}


\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{listings}

\lstset{language=C,
        captionpos=b,
        breaklines=true,
        basicstyle=\ttfamily\footnotesize}
        
\lstdefinelanguage{barriers}{keywords={[1]for,do,while,if,else,endif,break,continue,return,
    typedef,struct,_Atomic},
  keywords={[2]const,volatile,static,signed,unsigned,__attribute__,void,bool,char,short,long,int,float,double,boolean,size_t,sr_Bar_t, tp_Data_t},
  keywords={[4]thread_fence,load_explicit,store_explicit,compare_exchange_strong_explicit,compare_exchange_weak_explicit,relaxed,acquire,release,seq_cst,R,W,sync,ctrl_isync,atomic,mic_store,mic_pause,add_and_fetch,_mm512_load_epi32,_mm512_div_epi32,_mm512_rem_epi32,_mm512_prefetch_i32gather_ps,
_mm512_i32gather_epi32,_mm512_sllv_epi32,_mm512_knot,_mm512_kor,_mm512_test_epi32_mask,
_mm512_mask_prefetch_i32scatter_ps,_mm512_mask_i32scatter_epi32,_mm512_mask_or_epi32, _mm512_set1_epi32},
  emph={main,sr_wait},
string=[b]",
comment=[l]//,
  morecomment=[s]{/*}{*/},
flexiblecolumns=true,
  tabsize=2,
captionpos=b,
frame=single,
  framerule=0pt,
  aboveskip=1em,
  belowskip=1pt,
  framesep=0pt,
basicstyle=\ttfamily\fontsize{7}{7}\selectfont,
  keywordstyle={[1]\bfseries},
  keywordstyle={[2]\ttfamily},
  keywordstyle={[3]\bfseries},
  keywordstyle={[4]\bfseries},
emphstyle=\itshape,
  identifierstyle=\color{black},
  commentstyle=\color{darkgray}\itshape,
  stringstyle=\color{darkgray}
}


\lstset{language=barriers}


\usepackage{algorithm}


\begin{document}

\setcopyright{acmcopyright}



\doi{10.475/123_4}

\isbn{123-4567-24-567/08/06}

\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\GsEGV|V||E|s\Theta(1)O(V+E)kL_{k}L_{k+1}\infty\inftyuvuvuG, sin.init()out.init()vis.init()u\inV(G) - sP[u] \leftarrow  \inftyin.add(s)vis.Set(s)P[s] = s{in} \neq 0u \in inv \in Adj[u]vis.Test(v)vis.Set(v)out.add(v)P[v] = u swap(in, out)out \leftarrow 0G, sin.init()out.init()vis.init()u\inV(G) - sP[u] = \inftyin.add(s)vis.Set(s)P[s] = s{in} \neq 0u \in {in}v \in Adj[u]vis.Test(v)vis.Set(v)out.add(v)P[v] = u swap(in, out)out \leftarrow 0vInitBitmap()SetBit(n)GetBit(n)TestBit(n)bit2vertex(n)nin.InitBitmap()out.InitBitmap()vis.InitBitmap()u\inV(G) - sP[u] = \inftyin.SetBit(s)vis.SetBit(s)P[s] = s{in} \neq 0u \in {in}v \in Adj[u]v \notin (vis.TestBit(v)out.TestBit(v) )out.SetBit(v)P[v] = u - nodesw \in {out}w \neq 0 b \in wP[vertex] < 0out.SetBit(vertex)vis.SetBit(vertex)P[vertex] = P[vertex] + nodes swap(in, out)out \leftarrow 02^{SCALE}2^{SCALE} *
edgefactor2^{SCALE}2^{SCALE} * edgefactorSCALE=18edgefactor=16SCALE=19edgefactor=16SCALE=20edgefactor=16SCALEedgefactor=16$. In Figure (c) the dashed line represents the best MTEPS reported in \cite{MICgraphs}.}
    \label{fig:exp}
\end{figure}

\subsection{Thread affinity}
\label{sub:affinity}
Table \ref{tab:affinity} shows the result of running with 48 threads but varying the number of threads pinned per physical core manually.  
\begin{table}
\begin{center}
\caption{Performance \textit{SIMD} version by setting thread affinity for a graph size of SCALE=20 and edgefactor=16.}
\label{tab:affinity}
 \begin{tabular}{||c c c c||} 
 \hline
 \textbf{\#Threads} & \textbf{Thread Affinity} & \textbf{Cores} & \textbf{TEPS} \\ [0.5ex] 
 \hline
48&	1T/C & 48	& \textbf{4.69E+08} \\
 \hline
&2T/C& 24 & 2.67E+08 \\
\hline
 &3T/C&16&	1.89E+08 \\
 \hline
 &4T/C&12&	1.42E+08 \\
 \hline
 \hline
\end{tabular}
\end{center}
\vspace{-8mm}
\end{table}
These results show the detrimental effects of over-populating the
cores for the BFS. The TEPS obtained with one and two threads per core
are significantly higher than the TEPS with three and four threads per
core. This reduction in the TEPS rate as the number of threads per
core increase is the key driver to the changes in slope observed in
Figure~\ref{fig:simd} occuring around 60, 120 and 180 cores when the
number of threads per core has to increase as more threads are
used. At these points, each thread's exclusive access to cache space
decreases as does its share of memory bandwidth.Despite these changes in slope, the performance of the both the simd
and non-simd BFS top-down algorithm continues to scale. So, by using
fully populated cores (59), each one with the maximum number of
threads, the number of TEPS for 236 logical threads is the
fastest. Beyond 236 thread, threads are placed on the final core,
which is reserved for the operating system on the Phi, resulting in a
dramatic fall in performance.

In the future, it might be possible to exploit this behaviour by
under-populating cores with threads performing BFS and to make use of
so-called {\em helper} threads running on a core to assist with, for
example, prefetching to help hide memory latency
\cite{Kamruzzaman:2011}.

\section{Related Work}
\label{relwork}
This section briefly presents related work targeting mainly the
top-down BFS on the Xeon Phi. The Intel MIC Xeon Phi is a relative
recent parallel architecture released in 2012. An early work related
with graph algorithms on the Xeon Phi was reported
\cite{SauleC12}. However, they port a BFS parallel algorithm without
actually using the vector unit of the Xeon Phi. Other, and most
recent, work which is more related with ours is \cite{Gao2013} and
\cite{MICgraphs}. In \cite{Gao2013} they present the vectorization of
the top-down BFS algorithm, whereas in \cite{MICgraphs}, a complete
hybrid (top-down and bottom-up) heterogeneous BFS algorithm is
presented. However, few details about their use of vectorization are
presented. \cite{stanic2014} also explored the top-down BFS on the Xeon Phi but with a traditional, queue-based, algorithm that uses atomic updates. Despite their exploration related with the use of the vector unit and prefetching, their results are much lower than ours. We implemented our BFS top-down algorithm using SIMD
instructions by extending their work, which lead us to obtain a faster
vectorized implementation. Another approach to exploiting the Xeon
Phi is presented in a recently published system for heterogeneous
graph processing \cite{ChenHRJA15} allows the utilization of the Xeon
Phi in combination with a multi-core CPU through the use of a simple
programming interface. Although it is a good start towards
heterogeneous systems working on graphs, it is not made clear in this
paper how fast the resulting implementation is in comparison with
previous BFS implementations on the Xeon Phi.










\section{Conclusions}
\label{sec:conc}
In this paper, we revisited the vectorization and performance issues
of the top-down BFS algorithm on the Xeon Phi. In particular, we
studied the BFS top-down algorithm without (bit-wise) race conditions
to improve vectorization. The contributions of the paper are, first,
the development of an improved OpenMP parallel, highly vectorized SIMD
version of the BFS using vector intrinsics and successfully exploiting
data alignment and prefetching. This new implementation achieves a
higher number of TEPS than previous published results for the same
type of Xeon Phi. The second contribution is an investigation into the
impact of the thread affinity mapping and hyperthreading on
performance on an underpopulated system, a topic under researched in
the literature. This work suggests future possibilities to take
advantage of under-populating the cores and using the spare capacity
to improve latency hiding through the use of helper threads, for
example, to complement prefetching.

In the future, we plan to explore the benefit of our vectorization techniques beyond their use in native mode on the Xeon Phi, including targeting offload mode, GPGPUs and in the context of SSE and AVX SIMD technologies on multicore CPU-based systems. In addition, we are working on a version of the state-of-the-art hybrid BFS algorithm.







\section*{Acknowledgment}
This research was conducted with support from the UK Engineering and Physical Sciences Research Council (EPSRC) PAMELA EP/K008730/1, and AnyScale Apps EP/L000725/1. M. Paredes is funded by a National Council for Science and
Technology of Mexico PhD Scholarship. Mikel Luj{\'a}n is supported by a Royal Society University Research Fellowship.  

\bibliographystyle{abbrv}

\bibliography{refs}


\end{document}
