
\documentclass{article} \usepackage{iclr2021_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{booktabs}      \usepackage{wrapfig}       

\newcommand{\lrschedule}{\textit{Knee schedule}}
\newcommand{\lrscheduleshort}{\textit{Knee}}
\newcommand{\lrschedulenonitalic}{Knee schedule} 

\title{Wide-minima Density Hypothesis and the \\Explore-Exploit Learning Rate Schedule}


\author{
Nikhil Iyer \\
Microsoft Research India \\
\And

V Thejas \thanks{work done during internship at Microsoft Research India} \\
Atlassian India \\
\And

Nipun Kwatra \\
Microsoft Research India \\
\And

Ramachandran Ramjee \\
Microsoft Research India \\
\And

Muthian Sivathanu \\
Microsoft Research India
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}

Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also
provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima.
Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84\% higher absolute accuracy using the original training budget or up to 57\% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) and WMT'14 (DE-EN) datasets by just modifying the learning rate schedule of a high performing model.


    
\end{abstract} \section{Introduction}
\label{sec:introduction}

One of the fascinating properties of deep neural networks (DNNs) is their ability to generalize well, i.e., deliver high accuracy on the unseen test dataset. It is well-known that the learning rate (LR) schedules  play an important role in the generalization performance~\citep{keskar2016large,wu2018sgd,goyal-imagenet-in-an-hour-2017}. In this paper, we study the question, {\it what are the key properties of a learning rate schedule that help DNNs generalize well during training?} 





We start with a series of experiments training Resnet18 on Cifar-10 over 200 epochs. We vary the number of epochs trained at a high LR of , called the {\it explore} epochs, from 0 to 100  and divide up the remaining epochs equally for training with LRs of  and . Note that the training loss typically stagnates around 50 epochs with  LR. Despite that, we find that as the number of explore epochs increase to 100, the average test accuracy also increases. We also find that the minima found in higher test accuracy runs are wider than the minima from lower test accuracy runs, corroborating past work on wide-minima and generalization~\citep{keskar2016large,hochreiter1997flat,jastrzkebski2017three,wang2018identifying}. Moreover, what was particularly surprising was that, even when using fewer explore epochs, a few runs out of many trials still resulted in high test accuracies!

Thus, we not only find that an initial exploration phase with a high learning rate is essential to the good generalization of DNNs, but that {\it this exploration phase needs to be run for sufficient time, even if the training loss stagnates much earlier. Further, we find that, even when the exploration phase is not given sufficient time, a few runs still see high test accuracy values.}




To explain these observations, we hypothesize that, {\it in the DNN loss landscape, the density of narrow minima is significantly higher than that of wide minima.} A large learning rate can escape narrow minima easily (as the optimizer can jump out of them with large steps). However, once it reaches a wide minima, it is likely to get stuck in it (if the "width" of the wide minima is large compared to the step size). With fewer explore epochs, a large learning rate might still get lucky occasionally in finding a wide minima but invariably finds only a narrower minima due to their higher density. As the explore duration increase, the probability of eventually landing in a wide minima also increase. Thus, {\it a minimum duration of explore} is necessary to land in a wide minimum with high probability. 

Heuristic-based LR decay schemes such as cosine decay~\citep{loshchilov2016sgdr} {\it implicitly} maintain a higher LR for longer than schemes like linear decay. Thus, the hypothesis also explains cosine decay's better generalization compared to linear decay. Moreover, the hypothesis enables a principled learning rate schedule design that {\it explicitly} accounts for the requisite explore duration. 

Motivated by the hypothesis, we design a novel {\it Explore-Exploit} learning rate schedule, where the initial \textit{explore} phase optimizes at a high learning rate in order to arrive in the vicinity of a wide minimum. This is followed by an \textit{exploit} phase which descends to the bottom of this wide minimum. We give {\it explore} phase enough time so that the probability of landing in a wide minima is high. For the \textit{exploit} phase, we experimented with multiple schemes, and found a simple, parameter-less, linear decay to zero to be effective. \textit{Thus, our proposed learning rate schedule optimizes at a constant high learning rate for some minimum time, followed by a linear decay to zero. We call this learning rate schedule the \lrschedule{}.} 



We extensively evaluate the \lrschedule{} across a wide range of models and datasets, ranging from NLP (BERT pre-training, Transformer on WMT'14(EN-DE) and IWSLT'14(DE-EN)) to CNNs (ImageNet on ResNet-50, Cifar-10 on ResNet18), and spanning multiple optimizers: SGD Momentum, Adam, RAdam,  and LAMB.  In all cases, \lrschedule{} improves the test accuracy of state-of-the-art hand-tuned learning rate schedules, when trained using the original training budget. The explore duration is a hyper-parameter in \lrschedule{} but even if we set the explore duration to a fixed 50\% fraction of total training budget, we find that it still outperforms prior schemes.

We also experimented with reducing the training budget, and found that \lrschedule{} can achieve the same accuracy as the baseline under significantly reduced training budgets. For the BERT\textsubscript{LARGE} pretraining, WMT'14(EN-DE) and ImageNet experiments, we are able to train in 33\%, 57\% and 44\% less training budget, respectively, for the same test accuracy. This corresponds to significant savings in GPU compute, e.g. \textit{savings of over 1000 V100 GPU-hours} for BERT\textsubscript{LARGE} pretraining.


The main contributions of our work are:
\begin{enumerate}[nosep]
 \item A hypothesis of lower density of wide minima in the DNN loss landscape, backed by extensive experiments, that explains why a high learning rate needs to be {\it maintained for sufficient duration} to achieve good generalization.
  \item The hypothesis also  {\it explains} the good performance of heuristic-based schemes such as cosine decay,  and promotes a {\it principled design} of learning rate decay schemes.
 \item Motivated by the hypothesis, we design an {\it Explore-Exploit} learning rate schedule called \lrschedule{} that outperforms prior heuristic-based learning rate schedules, including achieving state-of-the-art results in 
 IWSLT’14 (DE-EN) and WMT’14 (DE-EN) datasets.
\end{enumerate}
\vspace{-4pt} \section{Wide-Minima Density Hypothesis}
\label{sec:wide_minima_hypothesis}

Many popular learning rate (LR) schedules, such as the step decay schedules for image datasets, start the training with high LR, and then reduce the LR periodically. For example, consider the case of Cifar-10 on Resnet-18, trained using a typical step LR schedule of  for 100, 50, 50 epochs each. In many such schedules, even though training loss stagnates after several epochs of high LR, one still needs to continue training at high LR in order to get good generalization. 

For example, Figure~\ref{fig:cifar-trloss-warmup50vs100} shows the training loss for Cifar-10 on Resnet-18, trained with a fixed LR of 0.1 (orange curve), compared to a model trained via a step schedule with LR reduced at epoch 50 (blue curve). As can be seen from the figure, the training loss stagnates after  50 epochs for the orange curve, and locally it makes sense to reduce the learning rate to decrease the loss. However, as shown in Table~\ref{tab:warmup_accuracy_baseline}, generalization is directly correlated with duration of training at high LR, with the highest test accuracy achieved when the high LR is used for 100 epochs, well past the point where training loss stagnates.

To understand the above phenomena, we perform another experiment. We train Cifar-10 on Resnet-18 for 200 epochs, using a high LR of  for only 30 epochs and then use LR of  and  for 85 epochs each. We repeat this training 50 times with different random weight initializations. On an average, as expected, this training yields a low test accuracy of . However, {\it in 1 of the 50 runs, we find that the test accuracy reaches , even higher than the average accuracy of  obtained while training at high LR for 100 epochs!}

\begin{figure*}[t]
\begin{minipage}{0.4\textwidth}
  \centering
  \includegraphics[width=0.8\columnwidth]{figures/tr_loss_momentum_50_vs_100_0_01.pdf}
  \caption{Training loss for Cifar-10 on Resnet-18. Orange plot uses a fixed LR of 0.1, while in blue plot, the LR is reduced from 0.1 to 0.01 at epoch 50.
  }
  \label{fig:cifar-trloss-warmup50vs100}
\end{minipage}
\hfil
\begin{minipage}{0.5\textwidth}
  \centering
  \small
  \captionsetup{type=table} \vspace{-12pt}
  \caption{Cifar-10 on Resnet-18 trained for 200 epochs with Momentum. An LR of 0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 and the other half at 0.001. Reported results are average over 4 runs.}
\begin{tabular}{ccc}
\toprule
Epochs at & Test Accuracy & Train Loss \\ 
0.1 LR &  Avg. (Std. Dev) &  Avg. (Std. Dev.)\\
\midrule
    0 & 94.34 (0.13) & 0.0017 (8e-5) \\
    30 & 94.81 (0.15) & 0.0017 (8e-5) \\
    40 & 94.91 (0.14) & 0.0018 (9e-5) \\
    60 & 95.01 (0.14) & 0.0018 (1e-4) \\ 
    80 & 95.05 (0.15)  & 0.0019 (1e-4) \\
    100 & 95.10 (0.14)  & 0.0021 (1e-4) \\
\bottomrule
\end{tabular}
  \label{tab:warmup_accuracy_baseline}
\end{minipage}
\vspace*{-0.3in}
\end{figure*}




{\bf Hypothesis.}
To explain the above observations, i.e., using a high learning rate for {\it short duration} results in low average test accuracy with rare occurrences of high test accuracy, while using the same high learning rate for {\it long duration} achieves high average test accuracy, we introduce a new hypothesis. We hypothesize that, \textit{in the DNN loss landscape, the density of narrow minima is significantly  higher than that of wide minima}. 

An intuitive explanation of why high LRs are necessary to locate wide minima then follows: a large LR can escape narrow minima ``valleys'' easily (as the optimizer can jump out of them with large steps). However, once it reaches a wide minima ``valley'', it is likely to get stuck in it (if the ``width'' of the wide valley is large compared to the step size). For example, see \cite{wu2018sgd} for a result showing that large LRs are unstable at narrow minima and thus don't converge to them. Thus the optimizer, when running at a high LR, jumps from one narrow minimum region to another, until it lands in a wide minimum region where it then gets stuck. Now, the probability of an optimization step landing in a wide minima is a direct function of the proportion of wide minima compared to that of narrow minima. Thus, if our hypothesis is true, i.e., wide minima are much fewer than narrow minima, this probability is very low, and the optimizer needs to take a lot of steps to have a high probability of eventually landing in a wide minimum. This explains the observation in Table~\ref{tab:warmup_accuracy_baseline}, where the average accuracy continues to improve as we increase the number of high LR training steps. The hypothesis also explains why very few (just 1) of the 50 runs trained at  LR for 30-epochs also managed to attain high accuracy -- they just got lucky probabilistically and landed in a wide minimum even with a shorter duration.


To validate this hypothesis further, we run experiments similar to the one in Table~\ref{tab:warmup_accuracy_baseline}. Specifically, we train Cifar-10 on Resnet-18 model for 200 epochs using a standard step schedule with LR of . We vary the number of epochs trained using the high LR of 0.1, called the {\it explore epochs}, from 30 to 100 epochs, and divide up the rest of the training equally between 0.01 and 0.001. For each experimental setting, we conduct 50 random trials and plot the distributions of final test accuracy and the minima sharpness as defined by the metric in \cite{keskar2016large}. If our hypothesis is true, then the more you explore, the higher the probability of landing (and getting stuck) in a wide minima region, which should cause the distribution to tighten and move towards wider minima (lower sharpness), as the number of explore steps increase. This is exactly what is observed in Figure~\ref{fig:keskar_hist_explores}. Also since wide minima correlate with higher test accuracy, we should see the test accuracy distribution move towards higher accuracy and sharpen, as the number of explore steps increase. This is confirmed as well in Figure~\ref{fig:accuracy_hist_explores}.

Finally, to verify whether explore at high LR is essential, we train Cifar-10 for 10,000 epochs at a fixed lower LR of 0.001. The training converged but the final test accuracy was only \textbf{93.9}. Thus, even training 50x longer at low LR is not sufficient, adding more evidence to the hypothesis.


\begin{figure*}[t]
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{cccc}
    \includegraphics[width=.25\textwidth]{figures/cifar/0-30-60-100/0explore_sharpness.pdf}&   
    \includegraphics[width=.25\textwidth]{figures/cifar/0-30-60-100/30explore_sharpness.pdf}&
    \includegraphics[width=.25\textwidth]{figures/cifar/0-30-60-100/60explore_sharpness.pdf}&
    \includegraphics[width=.25\textwidth]{figures/cifar/0-30-60-100/100explore_sharpness.pdf}\\
    {\footnotesize (a) Explore 0} & {\footnotesize (b) Explore 30} & {\footnotesize (c) Explore 60} & {\footnotesize (d) Explore 100}
  \end{tabular}
  }
\caption{Histogram of minima sharpness~\citep{keskar2016large} for 50 random trials of Cifar-10 on Resnet-18. Each figure shows histograms for runs with different number of explore epochs. The distribution moves toward lower sharpness and tightens as the number of explore epochs increase.}
  \label{fig:keskar_hist_explores}

\end{figure*}

\begin{figure*}[t]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{cccc}
    \includegraphics[width=0.25\textwidth]{figures/cifar/0-30-60-100/0explore_acc.pdf}&   
    \includegraphics[width=0.25\textwidth]{figures/cifar/0-30-60-100/30explore_acc.pdf}&
    \includegraphics[width=0.25\textwidth]{figures/cifar/0-30-60-100/60explore_acc.pdf}&
    \includegraphics[width=0.25\textwidth]{figures/cifar/0-30-60-100/100explore_acc.pdf}\\
    {\footnotesize (a) Explore 0} & {\footnotesize (b) Explore 30} & {\footnotesize (c) Explore 60} & {\footnotesize (d) Explore 100}
  \end{tabular}
  }
\caption{Histogram of test accuracy for 50 random trials of Cifar-10 on Resnet-18. Each figure shows histograms for runs with different number of explore epochs. The distribution moves toward higher test accuracy and sharpens as the number of explore epochs increase.}
  \label{fig:accuracy_hist_explores}
 \vspace*{-0.2in}
\end{figure*}










{\bf Multi-scale.} Given the importance of explore at high LR, a natural question that may arise is whether explore is necessary at smaller LR as well. To answer this, we train the same network for a total of 200 epochs with an initial high LR of  for 100 epochs, but now we vary the number of epochs trained with the LR of  (we call this finer-scale explore), and train with LR of  for the remaining epochs. As can be seen from Table~\ref{tab:finewarmup_accuracy_baseline}, although the final training loss remains similar, we find that finer-scale explore also plays a role similar to the initial explore in determining the final test accuracy. {\it This indicates that our hypothesis about density of wide/narrow regions indeed holds at multiple scales}.

\begin{table}[b]
\small
\centering
\caption{Cifar-10 on Resnet-18 trained for 200 epochs. An LR of 0.1 is used for the first 100 epochs. We then vary the number of epochs trained with LR of  (called finer-scale explore), and train the remaining epochs with an LR of . We report averages values over 3 runs.}
\label{tab:finewarmup_accuracy_baseline}
\begin{tabular}{cccc}
\toprule
Explore Epochs (Finer-scale) & Test Accuracy & Training Loss & Sharpness \\
\midrule
  10 & 94.78 & 0.0031 & 5.48\\ 
  20 & 94.91 & 0.0026 & 4.47\\ 
  30 & 95.00 & 0.0023 & 4.02\\ 
  40 & 95.02 & 0.0021 & 3.91\\ 
  50 & 95.10 & 0.0021 & 3.54\\ 
\bottomrule
\end{tabular}
\vspace{-0.2in}
\end{table}





\section{Explore-Exploit Learning Rate Schedule}

Given that we need to explore at multiple scales for good generalization, how do we go about designing a good learning rate schedule? The search space of the varying learning rate steps and their respective explore duration is enormous. 

Fortunately, since the explore at the initial scale is searching over the entire loss surface while explore at finer-scales is confined to exploring only the wide-minima region identified by the initial explore, the former is more crucial. In our experiments as well, we found that the initial portion of training is much more sensitive to exploration and needs a substantial number of \textit{explore} steps, while after this initial phase, several decay schemes worked equally well. This is similar to the observations in~\citep{golatkar2019time} where the authors found that regularization such as weight-decay and data augmentation mattered significantly only during the initial phase of training.

The above observations motivate our {\it Explore-Exploit} learning rate schedule, where the \textit{explore} phase first optimizes at a high learning rate for some minimum time in order to land in the vicinity of a wide minima. We should give the \textit{explore} phase enough time (a hyper-parameter), so that the probability of landing in a wide minima is high.
After the \textit{explore} phase, we know with a high probability, that the optimizer is in the vicinity of a wide region. We now start the {\it exploit} phase to descend to the bottom of this wide region while progressively decreasing the learning rate. Any smoothly decaying learning rate schedule can be thought of as doing micro \textit{explore-exploit} at progressively reduced scales. A steady descent would allow more \textit{explore} duration at all scales, while a fast descent would explore less at higher learning rates. We experimented with multiple schedules for the exploit phase, and found a simple linear decay to zero, that does not require any hyper-parameter, to be effective in all the models/datasets we tried. We call our proposed learning rate schedule which starts at a constant high learning rate for some minimum time, followed by a linear decay to zero, the \lrschedule{}.

Note that any learning rate decay scheme incorporates an implicit explore during the initial part, where the learning rate stays high enough. To evaluate the benefit of an explicit explore phase, we compare \lrschedule{} against several decay schemes such as linear and cosine. Interestingly, the results depend on the length of training. For long budget experiments, simple decay schemes perform comparable to \lrschedule{} in some experiments, since the implicit explore duration is also large, helping these schemes achieve good generalization. However for short budget experiments, these schemes perform significantly worse than \lrschedule{}, since the implicit explore duration is much shorter. See Table~\ref{tab:all_results_combined} and~\ref{tab:all_results_combined_short} for the comparison.

{\bf Warmup.} Some optimizers such as Adam use an initial warmup phase to slowly increase the learning rate. However, as shown in~\citet{liu2019variance_radam}, learning rate warmup is needed mainly to reduce variance during initial training stages and can be eliminated with an optimizer such as RAdam. Learning rate warmup is also used for large-batch training~\citep{goyal-imagenet-in-an-hour-2017}. Here, warmup is necessary since the learning rate is scaled to a very large value to compensate for the large batch size. This warmup is complementary and can be incorporated into \lrschedule{}. For example, we do this for BERT\textsubscript{LARGE} pretraining experiment where a large 16k batch size was used.





 \vspace{-6pt}
\section{Experiments}
\label{sec:experiments}

We have done multiple experiments in section~\ref{sec:wide_minima_hypothesis} to validate our hypothesis. We do further hypothesis validation on a text dataset. This is discussed in Section~\ref{sec:hypothesis_validation} of supplementary material.

We now extensively evaluate the effectiveness of \lrschedule{} on multiple models and datasets, across various optimizers including SGD Momentum, Adam~\citep{adam_kingma2014adam}, RAdam~\citep{liu2019variance_radam} and LAMB~\citep{bert76lamb}. For all experiments, we used an out of the box policy, where we only change the learning rate schedule, without modifying anything else. We evaluate on image datasets -- Imagenet on Resnet-50, Cifar-10 on Resnet-18; as well as NLP datasets -- pretraining BERT\textsubscript{LARGE} on Wikipidea+BooksCorpus and fine-tuning it on SQuADv1.1; and WMT'14 (EN-DE), IWSLT'14 (DE-EN) on Transformers.

For all settings we compare \lrschedule{} against the original hand-tuned learning rate baseline for the corresponding model and dataset, showing an improvement in test accuracy in all cases. We also show that \lrschedule{} can achieve the same accuracy as the baseline with a much reduced training budget (e.g.  less for ImageNet). Further, we also run our experiments with other common LR schedules such as linear decay, cosine decay, one-cycle~\citep{smith2018disciplined_onecycle}. See Table~\ref{tab:all_results_combined} and Table~\ref{tab:all_results_combined_short} for a comparison of all LR schedules on the original budget and shorter budgets, respectively.

\subsection{ImageNet Image Classification on Resnet-50}

We train ImageNet dataset \citep{imagenet-dataset} on Resnet-50 network \citep{resnet_he_2016} with the SGD Momentum optimizer. For baseline runs, we used the standard hand-tuned step learning rate schedule of  for 30 epochs each. For \lrschedule{} we used a seed LR of 0.1 (same as baseline). We trained with the original budget of 90 epochs as well as with a reduced budget of 50 epochs. We used 30 and 20 explore epochs for the two runs, respectively. Table~\ref{tab:imagenet_test_training_loss} shows the training loss and test accuracies for our experiments. \lrschedule{} comfortably beats the test accuracy of baseline in the full budget run
(with absolute gains of 0.8\% and 0.4\% in top-1 and top-5 accuracy, respectively), while meeting the baseline accuracy even with a much shorter budget. The fact that the baseline schedule takes almost  more training time than \lrschedule{} for the same test accuracy, shows the effectiveness of our \textit{Explore-Exploit} scheme.
See Figure~\ref{fig:imagenet_momentum_result} in Appendix for training curves.

\begin{table}[h]
\small
\centering
\caption{ImageNet on Resnet-50 results. We report mean (stddev) over 3 runs.}
\label{tab:imagenet_test_training_loss}
\begin{tabular}{ccccc}
\toprule
\multirow{1}{*}{LR Schedule}  & Test Top 1 Acc. & Test Top 5 Acc. & Training Loss & Training Epochs\\
\midrule
  Baseline     & \multirow{1}{*}{75.87 (0.035)} & \multirow{1}{*}{92.90 (0.015)} & \multirow{1}{*}{0.74 (1e-3)} & 90\\
  \lrscheduleshort{}     & \multirow{1}{*}{76.71 (0.097)} & \multirow{1}{*}{93.32 (0.031)} & \multirow{1}{*}{0.79 (1e-3)} & 90\\
  \lrscheduleshort{} (short budget)    & \multirow{1}{*}{75.92 (0.11)} & \multirow{1}{*}{92.90 (0.085)} & \multirow{1}{*}{0.90 (3e-3)} & 50\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cifar-10 Image Classification on Resnet-18}

We train Cifar-10 dataset~\citep{cifar-10-dataset} on Resnet-18 network~\citep{resnet_he_2016} with the SGD Momentum optimizer. For baseline, we used the hand-tuned step learning rate schedule of  for 100, 50, 50 epochs, respectively. With \lrschedule{}, we train the network with the original budget of 200 epochs, as well as a reduced budget of 150 epochs. We used 100 explore epochs for both runs, and a seed learning rate of 0.1 (same as baseline). Table~\ref{tab:all_results_combined} and Table~\ref{tab:all_results_combined_short} shows the test accuracy for the full and reduced budget runs. \lrschedule{} beats the test accuracy of baseline in the full budget run, while meeting the baseline test accuracy in  less budget. See Table~\ref{tab:cifar_results_train_loss_test_acc}, figure~\ref{fig:cifar_momentum_result} in Appendix for detailed comparisons of training loss, test accuracy, and LR.

\subsection{BERT\textsubscript{LARGE} Pre-training}
We now evaluate \lrschedule{} on a few NLP tasks. In the first task, we pre-train the BERT\textsubscript{LARGE} model~\citep{devlin2018bert} on the Wikipedia+BooksCorpus dataset with LAMB~\citep{bert76lamb} optimizer using a batch-size of 16k.

Since large batch training requires learning rate warmup (see \citet{goyal-imagenet-in-an-hour-2017}), we incorporate it into the \lrschedule{} by first doing a warmup followed by the explore-exploit phases. We used an explore of 50\% steps for both phases of BERT training. For baseline, we use the warmup + linear decay schedule~\citep{bert76lamb,devlin2018bert}. The pre-trained models are evaluated on the SQuAD v1.1~\citep{rajpurkar2016squad} dataset by fine-tuning on the dataset for 2 epochs. We pretrain the model on both the full budget, as well as with 33\% less budget. See Table~\ref{tab:bert_pretraining} for the results. For the full budget run, \lrschedule{} improves the baseline by ~0.2\%, while for the reduced budget we achieved similar fine-tuning accuracy as baseline. The baseline schedule achieves a much lower accuracy with shorter budget training, showing the efficacy of \lrschedule{}. BERT pre-training is extremely compute expensive and takes around 47 hours on 64 V100 GPUs (3008 V100 GPU-hrs) on cloud VMs. The reduced budget amounts to a saving of approximately 1002 V100 GPU-hours!

\vspace{-4pt}
\begin{table}[h]
\small
\centering
\caption{BERT\textsubscript{LARGE} results. We report the pre-training train loss, and the test F1 accuracy on SQuAD v1.1 after fine-tuning. See figure~\ref{fig:bert_plots} in Appendix for training curves.}
\label{tab:bert_pretraining}
\begin{tabular}{cccc}
\toprule
LR Schedule  & F1 score on SQuAD v1.1 & Training loss & Total Training Steps \\ 
\midrule
Knee & 91.51  & 1.248 & 31250\\
Baseline \citep{bert76lamb} &  91.34 & - & 31250 \\
Baseline (short budget)   & 90.64  & 1.336 & 20854\\
\lrscheduleshort{} (short budget)  & 91.29 & 1.275 & 20854\\
\bottomrule
\end{tabular}
\end{table}


\subsection{Machine Translation on Transformer Network with WMT'14 and IWSLT}

In the second NLP task, we train the Transformer (base model)~\citep{vaswani2017attention} on the IWSLT'14 (De-En)~\citep{iwslt_dataset_cettolo2014} and  WMT'14 (En-De)~\citep{wmt14translation}   datasets with the RAdam \citep{liu2019variance_radam} optimizer.

\paragraph{WMT'14 (EN-DE):} The baseline schedule uses a linear decay for 70 epochs \citep{liu2019variance_radam}. With \lrschedule{}, we trained with the original budget of 70 epochs, as well as a reduced budget of 30 epochs. We used 50 and 25 explore epochs for the two runs, respectively and a seed learning rate of  for both \lrschedule{} and baseline. In all cases we use the model checkpoint with least loss on the validation set for computing BLEU scores on the test set. Table~\ref{tab:wmt_results} shows the training loss and test accuracy averaged over 3 runs. \lrschedule{} improves the test BLEU score of baseline in the full budget run by 0.24 points. In the shorter budget run, \lrschedule{} matches the test accuracy of the baseline while taking  less training time (a saving of 80 V100 GPU-hours!)

\vspace{-4pt}
\begin{table}[h]
\captionsetup{type=table} \small
\centering
\caption{Results for WMT'14 (EN-DE) on Transformer networks. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report mean (stdev) over 3 runs.}
\label{tab:wmt_results}
\begin{tabular}{ccccc}
\toprule
  \multirow{1}{*}{LR Schedule} & Test BLEU Score & Train Perplexity & Validation Perplexity & Training Epochs \\
 \midrule
  Baseline   &  27.29 (0.06) & 3.87 (0.017) & 4.89 (0.02) & 70 \\
  \lrscheduleshort{}  & 27.53 (0.12) & 3.89 (0.017) & 4.87 (0.006) & 70 \\ 
  \lrscheduleshort{} (short budget) & 27.28 (0.17) & 4.31 (0.02) & 4.92 (0.007)  & 30\\
\bottomrule
\end{tabular}
\vspace{-6pt}
\end{table}

\paragraph{IWSLT'14 (DE-EN):} The baseline schedule uses a linear decay for 50 epochs \citep{liu2019variance_radam}. With \lrschedule{}, we trained with the original budget of 50 epochs, as well as a reduced budget of 35 epochs. We used 40 and 30 explore epochs for the two runs, respectively and a seed LR of  for both \lrschedule{} and baseline. In all cases we use the model checkpoint with least loss on the validation set for computing BLEU scores on the test set. Table~\ref{tab:all_results_combined} and ~\ref{tab:all_results_combined_short} show the average test accuracy for full and reduced budget runs. \lrschedule{} improves the baseline test BLEU score by 0.56 points in the full budget run. In the shorter budget run, \lrschedule{} matches the test accuracy of the baseline schedule while taking  less training time. Also see Table~\ref{tab:iwslt_results} in Appendix. See Figure~\ref{fig:wmt_radam_lrl} and Figure~\ref{fig:iwslt_adam_result} in Appendix for training curves.

\subsection{State of the Art Results}
To further demonstrate the effectiveness of \lrschedule{}, we took a recent high performing model, MAT~\citep{mat_fan2020}, which had shown very high accuracy on the IWSLT'14 (DE-EN) dataset with a BLEU score of 36.22 trained with inverse square root LR schedule. We simply retrained the model with \lrschedule{}, and achieved a new SOTA BLEU score of 36.6 (a 0.38 point increase). We also trained the model with WMT'14 (DE-EN) dataset and again achieved a SOTA BLEU score of 31.9 with \lrschedule{}, compared to the baseline score of 31.34 which uses the inverse square root LR schedule. See Section~\ref{sec:sota_mat}, Table~\ref{tab:iwslt_mat_results_train_loss_test_acc}, Table~\ref{tab:wmt_mat_results_train_loss_test_acc} and Figure~\ref{fig:MAT_adam_result} in Appendix for more details.

\subsection{Comparison with other learning schedules}

We also ran all our experiments with multiple other learning rate schedules -- one-cycle \citep{smith2018disciplined_onecycle}, cosine decay~\citep{loshchilov2016sgdr} and linear decay. See section~\ref{sec:extra_baselines} in supplementary material for details of these learning rate schedules, and a detailed performance comparison.
Note that in \lrschedule{}, the explore duration is a hyperparameter. To avoid tuning this hyperparameter, we also experimented with a fixed 50\% explore duration for the full budget runs.
Table~\ref{tab:all_results_combined} shows the test accuracies of the various experiments on all schedules, when trained with the original budget; while Table~\ref{tab:all_results_combined_short} shows the results when trained with a reduced budget. As shown, for the full budget runs, \lrschedule{} improves on the test accuracies on all experiments. Even the fixed 50\% explore \lrschedule{} outperforms all the other baselines. Also noteworthy is that \lrschedule{} is able to achieve the same test accuracies as the baseline's full budget runs with a much lower training budget, saving precious GPU cycles. 
We also demonstrate that the delta in accuracy between \lrschedule{} and other decay schemes is non-trivial by computing the number of epochs needed by each scheme to reach the target BLEU scores for WMT`14 and IWSLT'14. As shown in Table~\ref{tab:target_accuracy_epochs}, \lrschedule{} is highly efficient compared to say Cosine Decay which takes 100\% more training time to achieve the same accuracy for WMT`14.




\begin{table*}[ht]
\small
\centering
{\setlength{\extrarowheight}{1pt}

\caption{We report the top-1 accuracy for ImageNet and Cifar-10, BLEU score for IWSLT'14 and WMT'14 and F1 score for BERT on SQuAD. All values are averaged over multiple runs.}

\label{tab:all_results_combined}

\begin{tabular}{ccc@{\hspace{.95\tabcolsep}}c@{\hspace{.75\tabcolsep}}cccc}
\toprule
\multirow{3}{*}{Experiment} & Training & \multirow{3}{*}{\shortstack[l]{\textit{Knee} \\ \textit{Schedule}}} & \textit{Knee} & \multirow{3}{*}{Baseline} & \multirow{3}{*}{One-Cycle} & \multirow{3}{*}{\shortstack[l]{Cosine \\ Decay}} & \multirow{3}{*}{\shortstack[l]{Linear \\ Decay}} \\
& Budget   &  & \textit{Schedule} & & & &  \\
& (epochs) &                   & (Fixed 50\% explore) & & &  &  \\ 
\midrule
ImageNet    & 90 & \textbf{76.71}  & 76.58  & 75.87 & 75.39 & 76.41 & 76.54 \\
Cifar-10    & 200 & \textbf{95.26} & 95.26 & 95.10 & 94.09 & 95.23 & 95.18 \\
IWSLT       & 50 & \textbf{35.53}  & 35.23 & 34.97 & 34.77 & 35.21  & 34.97  \\
WMT'14       & 70 & \textbf{27.53} & 27.41 & 27.29 & 27.19 & 27.35  & 27.29  \\ 
BERT\textsubscript{LARGE}    & 31250 (iters) & \textbf{91.51} & 91.51 & 91.34 & - & -  & 91.34  \\ 
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[ht]
\small
\centering
{\setlength{\extrarowheight}{1pt}

\caption{Shorter budget training: Test accuracy on all learning rate schedules tried in this paper, but trained with a shortened budget. We report same metrics as Table~\ref{tab:all_results_combined}. \lrschedule{} achieves the same accuracy as baseline schedules in much lower budget, saving precious GPU-hours.}


\label{tab:all_results_combined_short}

\begin{tabular}{ccccccc}
\toprule
Experiment  & Shortened Training & \textit{Knee} & One-Cycle & Cosine  & Linear  & Saving  \\
& Budget (epochs) & \textit{Schedule} & &  Decay & Decay & ( V100 GPU-hours) \\
\midrule
ImageNet     &  50 & \textbf{75.92} & 75.36 & 75.71 & 75.82 & 27\\
Cifar-10     & 150 & \textbf{95.14} & 93.84 & 95.06 & 95.02 & 0.25\\
IWSLT        & 35 & \textbf{35.08} & 34.43 & 34.46  & 34.16 & 0.75\\
WMT'14       & 30 & \textbf{27.28} & 26.80 & 26.95  & 26.77 & 80\\
BERT\textsubscript{LARGE} & 20854 (iterations) & \textbf{91.29}  & - & -  & 90.64  & 1002 \\
\bottomrule
\end{tabular}}
\end{table*}
\vspace{-12pt}

\begin{table*}[ht]
\small
\centering
\caption{Epochs required by different LR schedules to reach the target accuracy.}
{\setlength{\extrarowheight}{1pt}\begin{tabular}{ccccc}
\toprule
Experiment & Target BLEU Score & \lrschedule{} & Cosine Decay & Linear Decay \\ 
\midrule
IWSLT       & 35.08 & 35 & 45 & 60 \\
WMT'14      & 27.28 & 30 & 60 & 70  \\ 
\bottomrule
\end{tabular}}

\label{tab:target_accuracy_epochs}
\vspace{-10pt}
\end{table*} \section{Related Work}
\label{sec:related_work}

\noindent
{\bf Generalization.}
There has been a lot of work on understanding the generalization characteristics of DNNs. \citet{kawaguchi2016deep} found that DNNs have many local minima, but all local minima were also the global minima. It has been observed by several authors that wide minima generalize better than narrow minima~\citep{arora2018stronger,hochreiter1997flat,keskar2016large,jastrzkebski2017three,wang2018identifying} but there have been other works questioning this hypothesis as well~\citep{dinh2017sharp,golatkar2019time,guiroy2019towards,jastrzebski_iclr_2019,yoshida2017spectral}.

\citet{keskar2016large} found that small batch SGD generalizes better and lands in wider minima than large batch SGD. However, recent work has been able to generalize quite well even with very large batch sizes~\citep{goyal-imagenet-in-an-hour-2017,large-batch-training-openai-2018,large-batch-training-google-2018}, by scaling the learning rate linearly as a function of the batch size.~\citet{jastrzebski_iclr_2019}
analyze how batch size and learning rate influence the curvature of not only the SGD endpoint but also the whole trajectory. They found that small batch or large step SGD have similar characteristics, and yield smaller and earlier peak of spectral norm as well as smaller largest eigenvalue.~\citet{chaudhari2019entropy,shapinglandscape2019baldassi} propose methods to drive the optimizer to wide minima. ~\citet{wang2018identifying} analytically show that generalization of a model is related to the Hessian and propose a new metric for the generalization capability of a model that is unaffected by model reparameterization of~\citet{dinh2017sharp}. ~\citet{yoshida2017spectral} argue that regularizing the spectral norm of the weights of the neural network help them generalize better. On the other hand,~\citet{arora2018stronger} derive generalization bounds by showing that networks with low stable rank (high spectral norm) generalize better.~\citet{guiroy2019towards} looks at generalization in gradient-based meta-learning and they show experimentally that generalization and wide minima are not always correlated. 



\noindent
{\bf Lower density of wide minima.} \citet{wu2018sgd} compares the sharpness of minima obtained by full-batch gradient descent (GD) with different learning rates for small neural networks on FashionMNIST and Cifar10 datasets. They find that GD with a given learning rate finds the theoretically sharpest feasible minima for that learning rate. Thus, in the presence of several flatter minimas, GD with lower learning rates does not find them, leading to the conjecture that density of sharper minima is perhaps larger than density of wider minima. \section{Conclusions} \label{sec:conclusions}


In this paper, we make an observation that an initial \textit{explore} phase with a high learning rate is essential for good generalization of DNNs. Further, we find that a minimum \textit{explore} duration is required even if the training loss stops improving much earlier. We explain this observation via our hypothesis that in the DNN loss landscape, the density of wide minima is significantly lower than that of narrow minima. 
Motivated by this hypothesis, we present an \textit{Explore-Exploit} based learning rate schedule, called the \lrschedule{}. We do extensive evaluation of \lrschedule{} on multiple models and datasets. In all experiments, the \lrschedule{} outperforms prior hand-tuned baselines, including achieving SOTA test accuracies, when trained with the original training budget, and achieves the same test accuracy as the baseline when trained with a much shorter budget.

 \section{Acknowledgement}
\label{sec:acknowledgement}

We would like to thank Sanjith Athlur for his help in setting up VM cluster for large training runs and Harshay Shah for helpful discussions.
 \bibliography{references}
\bibliographystyle{iclr2021_conference}

\clearpage

\appendix

\section{Hypothesis Validation}
\label{sec:hypothesis_validation}

For validating our hypothesis on the density of wide minima vs narrow minima, we did multiple experiments, most of which were discussed in section~\ref{sec:wide_minima_hypothesis} of the main paper.
To summarize, in figures~\ref{fig:keskar_hist_explores}, \ref{fig:accuracy_hist_explores} of the main paper, we showed that for Cifar-10 on Resnet-18, as the number of explore steps increase, the distribution of minima width and test accuracy sharpens and shifts towards wider minima and better accuracy, respectively. This behaviour is predictable from our hypothesis as increasing explore steps increases the probability of landing in a wide region. From the same argument, the average accuracy should increase as the number of explore steps increase, which is confirmed in Table~\ref{tab:warmup_accuracy_baseline} of the main paper. Our hypothesis also predicts that even at low explore epochs, although the probability of landing in a wide region is low, it is non zero. Thus, out of many trials with low number of explore epochs, a few runs could still yield high test accuracy. This is what we observe in Figure~\ref{fig:accuracy_hist_explores}(b) of the main paper, where 1 out of 50 trials for the 30 explore runs obtains an accuracy more than  even though the average accuracy for 30 explore is !.

\begin{table}[h]
\small
\centering
\caption{IWSLT'14 (DE-EN) on the Transformer network trained with the \lrschedule{}. The \textit{explore} duration is varied, while keeping the total training budget fixed at 50 epochs. We report averages over 3 runs.}

\begin{tabular}{ccc}
  \toprule
  Explore Epochs & Test BLEU score & Training Perplexity \\
  \midrule
  5  & 34.93 & 3.29 \\ 
  10 & 35.02 & 3.22 \\ 
  15 & 35.08 & 3.11 \\ 
  20 & 35.10  & 3.08 \\ 
  25 & 35.23 & 3.02 \\ 
  30 & 35.28 & 2.99 \\
  40 & 35.53 & 3.00 \\
  \bottomrule
\end{tabular}
\label{tab:warmup_accuracy_iwslt}
\end{table}

We do a similar experiments on the IWSLT'14 German to English dataset \citep{iwslt_dataset_cettolo2014} trained on Transformer networks \citep{vaswani2017attention} to demonstrate that our hypothesis holds even on a completely different NLP dataset and network architecture. We train with the \lrschedule{} for a total budget of 50 epochs, but keep varying the number of explore epochs. As shown in Table~\ref{tab:warmup_accuracy_iwslt}, the test BLEU score increases as we increase the number of explore epochs. Further, we found that among multiple trials, a 20 epoch explore run had a high BLEU score of 35.29, suggesting that the run got lucky. Thus, these results on the IWSLT'14 (DE-EN) dataset add more evidence to the wide-minima density hypothesis. 

Also, see section~\ref{sec:seed_sensitivity} and Table~\ref{tab:seed_senstivity_cifar} for a sensitivity analysis of the \lrschedule{} on the starting learning rate. Interestingly, we found that the optimal explore duration varies inversely with the starting LR. Since a bigger learning rate has higher probability of escaping narrow minima compared to a lower learning rate, it would, on an average, require fewer steps to land in a wide minima. Thus, larger learning rates can explore faster. This observation is thus consistent with our hypothesis and further corroborates it.

\vspace{20pt} \section{Experiment Details}
\label{sec:experiment_details}

In this section we describe the implementation and other details of each experiment. For the smaller datasets -- Cifar-10 and IWSLT'14 (DE-EN), we also provide the detailed training loss, test accuracy table which we skipped in the main paper. We also discuss an additional experiment, where we fine-tune a pretrained BERT\textsubscript{BASE} network on the SQuAD-v1.1 dataset.


\subsection{ImageNet Image Classification on Resnet-50}

We train ImageNet \cite{imagenet-dataset} on Resnet-50 \cite{resnet_he_2016}, which has 25 million parameters, with a batch size of 256 and a seed learning rate of 0.1. Random cropping and random horizontal flipping augmentations were applied to the training dataset. We use SGD optimizer with momentum of 0.9 and weight decay of 1e-4 \footnote{We used the opensource implementation at: https://github.com/cybertronai/imagenet18\_old}. For knee schedule, we choose explore as 30 epochs for the full budget and 20 epochs for the short budget. Figure~\ref{fig:imagenet_momentum_result} shows the training loss, test accuracy and learning rate curves during training.

\subsection{Cifar-10 Image Classification on Resnet-18}
For this experiment, we use a batch size of 128 and a seed learning rate of 0.1 with ResNet-18\cite{resnet_he_2016} which has around 11 million parameters. SGD optimizer is used with momentum of 0.9 and weight decay of 5e-4 \footnote{We used the open-source implementation at: https://github.com/kuangliu/pytorch-cifar}. Random cropping and random horizontal flipping augmentations were applied to the training dataset. For knee schedule, we choose 100 epochs as explore for short and full budget runs. Table~\ref{tab:cifar_results_train_loss_test_acc} shows the training loss and test accuracies for the various runs. Figure~\ref{fig:cifar_momentum_result} shows the training loss, test accuracy and learning rate curves during training.  

\begin{table}[h]
    \small
\centering
\caption{Training loss and Test accuracy for Cifar-10 on Resnet-18. We report mean (stddev) over 7 runs.} 
\label{tab:cifar_results_train_loss_test_acc}
\begin{tabular}{cccc}
\toprule
\multirow{1}{*}{LR Schedule} & Test Accuracy & Training Loss & Training Epochs \\
\midrule
  Baseline   & \multirow{1}{*}{95.10 (0.14)}  & \multirow{1}{*}{0.002 (1e-4)}  & 200 epochs \\ 

  \lrscheduleshort{}     & \multirow{1}{*}{95.26 (0.11)} &  \multirow{1}{*}{0.002 (1e-4)} & 200 epochs \\
   
  
  \lrscheduleshort{} (short budget)     & \multirow{1}{*}{95.14 (0.18)} &  \multirow{1}{*}{0.004 (3e-4)}  & 150 epochs \\  
\bottomrule
\end{tabular}
\end{table}

\subsection{BERT\textsubscript{LARGE} Pre-training}
We pre-train BERT using the LAMB optimizer
\footnote{We used the open-source implementation at: \\https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT}. BERT\textsubscript{LARGE} has around 330 million parameters and the pre-training is divided into two phases with different sequence lengths. The first phase consists of 90\% steps with sequence length of 128 and the second phase consists of the remaining 10\% steps with sequence length of 512 \cite{devlin2018bert}. We used a batch size of 16384 in both phases of training. We train the model on a shortened training budget of 2/3rd the original steps (20854 instead of 31250 steps) and on the full budget (31250 steps). For \lrschedule{}, we explore for 50\% of the phase budget after performing warmup for 10\% of the phase steps. Please refer to Figure~\ref{fig:bert_plots} for the training loss and learning rate curves of the short budget runs.

\subsection{Machine Translation on Transformer Network with WMT’14(EN-DE) and IWSLT'14(DE-EN)}

We use the default implementation provided by the fairseq package \cite{ott2019fairseq} \footnote{https://github.com/pytorch/fairseq}. We train WMT'14 (EN-DE) dataset on the Transformer\textsubscript{BASE}~\cite{vaswani2017attention} model which has around 86 million parameters and use the RAdam~\cite{liu2019variance_radam} optimizer with  of 0.9 and  of 0.999. Label smoothed cross entropy was used as the objective function with an uncertainty of 0.1. A dropout of 0.1, clipping norm of 25 and weight decay of 1e-4 is used. Each training batch contains approximately 30000 tokens. For \lrschedule{}, we choose explore of 25 epochs for short budget runs and 50 epochs for full budget runs. Please see Figure~\ref{fig:wmt_radam_lrl} for training perplexity, validation perplexity and learning rate curves.

\begin{table}[h!]
  \centering
  \small
\caption{Training, validation perplexity and test BLEU scores for IWSLT on Transformer networks. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.} 
\label{tab:iwslt_results}
\begin{tabular}{ccccc}
\toprule
  \multirow{2}{*}{LR Schedule} & Test BLEU  & Train & Validation  & Training \\
  &  Score & Perplexity  & Perplexity & Epochs \\ 
 \midrule
  Baseline   &  34.97 (0.035) & 3.36 (0.001) & 4.91 (0.035) & 50 \\
  \lrscheduleshort{}  & 35.53 (0.06) & 3.00 (0.044) & 4.86 (0.02) & 50 \\ 
  \lrscheduleshort{} (short budget) & 35.08 (0.12) & 3.58 (0.049) & 4.90 (0.063)  & 35\\

\bottomrule
\end{tabular}
\end{table}

For IWSLT'14 we use the same configuration as WMT'14, except for a dropout of 0.3 following Fairseq's out-of-box implementation. Each training batch contains approximately 4000 tokens. For \lrschedule{}, we choose explore as 30 epochs for short budget runs and 40 epochs for full budget runs. Table~\ref{tab:iwslt_results} shows the training loss and test accuracies for the various runs. Figure~\ref{fig:iwslt_adam_result} shows the training perplexity, validation perplexity and learning rate curves.


\subsection{State of the Art Results using the MAT model with IWSLT'14 (DE-EN) and WMT'14 (DE-EN) datasets}
\label{sec:sota_mat}
We use the default implementation provided by the authors of \cite{mat_fan2020} \footnote{https://github.com/HA-Transformer/HA-Transformer}. The MAT model is trained in two phases. A smaller model is trained in the first phase, which is used to initialize a larger model of the second phase. Adam optimizer with  of 0.9 and  of 0.98 is used for both phases.

\textbf{IWSLT'14 (DE-EN):} For the baseline run with IWSLT'14 (DE-EN) dataset, we follow the exact instructions provided by the authors in their code repository -- each phase is trained for 200 epochs, using an inverse square root LR schedule with warmup of 4000 steps and a peak LR of . For the run with \lrschedule{} we simply replace the learning rate schedule, keeping everything else unchanged. We used the same warmup of 4000 steps, and explored at  LR till 100 epochs followed by the linear decay exploit phase. Table~\ref{tab:iwslt_mat_results_train_loss_test_acc} shows the train loss and test accuracies for the various runs. Please see Figure~\ref{fig:MAT_adam_result} for detailed plots.

\begin{table}[h]
\small
\centering
\caption{Training, validation perplexity and test BLEU scores for IWSLT'14 DE-EN on MAT. The test BLEU scores are computed on the checkpoint with the best validation perplexity. Results are averaged over 3 runs.}
\label{tab:iwslt_mat_results_train_loss_test_acc}
{\setlength{\extrarowheight}{1pt}\begin{tabular}{cccccc}
\toprule
  \multirow{2}{*}{LR Schedule} & Test BLEU  & Train & Validation  & Training \\
  &  Score & Perplexity  & Perplexity & Epochs \\ 
 \midrule
  Baseline (inv. sqrt)  &  36.20 (0.02)   & 3.87 ( 0.009)  & 4.58 (0.009)  & 200 + 200 (each phase) \\
  \lrschedule{}  & 36.59 (0.017)  & 3.74 (0.012)  & 4.45 ( 0.009)  & 200 + 200 (each phase) \\ 
\bottomrule 
\end{tabular}}

\end{table}

\textbf{WMT'14 (DE-EN):} We follow the instructions provided in \cite{mat_fan2020} to preprocess the train and test datasets. For baseline, we train each phase for 70 epochs, using an inverse square root LR schedule with warmup of 4000 steps and a peak LR of . For the run with \lrschedule{} we simply replace the learning rate schedule, keeping everything else unchanged. We used the same warmup of 4000 steps, and explored at  LR for 35 epochs followed by the linear decay exploit phase. Table~\ref{tab:wmt_mat_results_train_loss_test_acc} shows the train loss and test accuracies for the various runs.
\begin{table}[h]
\small
\centering
\caption{Training, validation perplexity and test BLEU scores for WMT'14 DE-EN on MAT. The test BLEU scores are computed on the checkpoint with the best validation perplexity.}
\label{tab:wmt_mat_results_train_loss_test_acc}
{\setlength{\extrarowheight}{1pt}\begin{tabular}{cccccc}
\toprule
  \multirow{2}{*}{LR Schedule} & Test BLEU  & Train & Validation  & Training \\
  &  Score & Perplexity  & Perplexity & Epochs \\ 
 \midrule
  Baseline (inv. sqrt)  &  31.34  & 4.78  & 4.75  & 70 + 70 (each phase) \\
  \lrschedule{}         & 31.90  & 4.67  & 4.68   & 70 + 70 (each phase) \\ 
\bottomrule 
\end{tabular}}

\end{table}

\subsection{SQuAD-v1.1 fine-tuning on BERT\textsubscript{BASE}}

We also evaluate \lrschedule{} on the task of fine-tuning BERT\textsubscript{BASE} model~\cite{devlin2018bert} on SQuAD v1.1~\cite{rajpurkar2016squad} with the Adam~\cite{kingma2014adam} optimizer \footnote{We used the implementation at: https://github.com/huggingface/pytorch-transformers}. BERT fine-tuning is prone to overfitting because of the huge model size compared to the small fine-tuning dataset, and is typically run for only a few epochs. For baseline we use the linear decay schedule mentioned in~\cite{devlin2018bert}. We use a seed learning rate of  and train for 2 epochs. For \lrschedule{}, we train the network with 1 explore epoch with the same seed learning rate of . Table~\ref{tab:squad_results} shows our results over 3 runs. We achieve a mean EM score of 81.4, compared to baseline's 80.9, a 0.5\% absolute improvement. We don't do a short budget run for this example, as the full budget is just 2 epochs. Please refer to Figure~\ref{fig:squad_bert_adam_result} for the training loss, test accuracy and learning rate curves.

\begin{table}[h]
\small
\centering
\caption{SQuAD fine-tuning on BERT\textsubscript{BASE}. We report the average training loss, and average test EM, F1 scores over 3 runs.}
\label{tab:squad_results}
{\setlength{\extrarowheight}{1pt}\begin{tabular}{cccccc}
  \toprule
  LR Schedule & EM  & F1 & Train Loss & Training Epochs \\
  \midrule
  Baseline   & 80.89 (0.15) & 88.38 (0.032) & 1.0003 (0.004) & 2\\
  \lrschedule{}{}   & 81.38 (0.02) & 88.66 (0.045) & 1.003 (0.002) & 2 \\
  \bottomrule
\end{tabular}}

\end{table}
 \section{Minima Sharpness}
\label{sec:metrics_for_wide_minima}
Our hypothesis predicts that a higher explore should help the optimizer in finding a wider minimum, which in turn helps generalization. For quantitative evaluation, we used two different metrics for measuring the minima width, and evaluate the effect of explore on the width of the converged minima.

\subsection{Keskar's Sharpness Metric~\cite{keskar2016large}}

In our first evaluation we used the sharpness metric proposed in~\cite{keskar2016large} (see Metric 2.1 in section 2.2.2 of the paper). This metric was used to compute the histograms in Figure~\ref{fig:keskar_hist_explores} of the main paper as well. We use an  of 1e-4, and  is chosen to be the identity matrix. The maximization problem is solved by applying 1000 iterations of projected gradient ascent. We compute this metric for the Cifar-10 on Resnet-18 experiments for different number of explore epochs. As shown in Table~\ref{tab:keskar_avg}, the average sharpness of the minima decreases as we increase the number of explore epochs, as predicted by our hypothesis.





\begin{table}[h]
\small
\centering
\caption{Keskar's sharpness metric for Cifar-10 on Resnet-18 trained for 200
epochs with Momentum. An LR of 0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 and the other half at 0.001. We report the average sharpness over 50 different trials.}
\begin{tabular}{cc}
  \toprule
  Explore Epochs       &  Sharpness \\
\midrule
  0 & 10.56\\
  30 & 5.43\\
  60 & 3.86\\
  100 & 3.54\\
 \bottomrule
\end{tabular}

\label{tab:keskar_avg}
\end{table}

\subsection{Fisher Score}
The maximum Eigen value of the Fisher Information Matrix (FIM) is another metric used to measure the maximum curvature of the loss landscape (see~\cite{fim2018information}). We used an unbiased estimate of the true Fisher matrix (see~\cite{empiricalfisher2019limitations}) using 10 unbiased samples per training data. Table~\ref{tab:fim_scores} shows the average FIM scores for the Cifar-10 experiments with different explores. As shown, the FIM score decreases as the number of explore epochs increases, indicating that higher explore leads to wider minimum with a lower curvature. This is in line with the prediction from our hypothesis.

\begin{table}[h]
\small
\centering
\caption{FIM Score for Cifar-10 on Resnet-18 trained for 200
epochs with Momentum. A LR of 0.1 is used for the explore epochs. Half the remaining epochs are trained at 0.01 and the other half at 0.001. We report the average FIM score over 10 different trials.}
\begin{tabular}{cc}
  \toprule
  Explore Epochs      &  FIM score   \\
\midrule
  0 & 0.051\\
  30 & 0.046\\
  60 & 0.043\\
  100 & 0.042\\
 \bottomrule
\end{tabular}
\label{tab:fim_scores}

\end{table} \section{Learning Rate Sensitivity}
\label{sec:seed_sensitivity}

We performed sensitivity analysis of the starting learning rate, referred to as the seed learning rate, for \lrschedule{}. We trained the Cifar-10 dataset on Resnet-18 with the \lrschedule{} for a shortened budget of 150 epochs, starting at different seed LRs. For each experiment, we do a simple linear search to find the best explore duration. The test accuracies and optimal explore duration for the different seed LR choices is shown in Table~\ref{tab:seed_senstivity_cifar}. As shown, the seed learning rate can impact the final accuracy, but \lrschedule{} is not highly sensitive to it. In fact, we can achieve the target accuracy of 95.1 with multiple seed learning rates of 0.05, 0.075, 0.0875 and 0.115, as compared to the original seed learning rate of 0.1, by tuning the number of explore epochs.

Another interesting observation is that the optimal explore duration varies inversely with the seed LR. Since a bigger learning rate has higher probability of escaping narrow minima compared to a lower learning rate, it would, on an average, require fewer steps to land in a wide minima. Thus, larger learning rates can \textit{explore} faster, and spend more time in the \textit{exploit} phase to go deeper in the wide minimum. This observation is thus consistent with our hypothesis and further corroborates it.

We also note that by tuning both seed LR and explore duration, we can achieve the twin objectives of achieving a higher accuracy, as well as a shorter training time -- e.g. here we are able to achieve an accuracy of 95.34 in 150 epochs (seed LR 0.075), compared to 95.1 achieved by the baseline schedule in 200 epochs.

\begin{table}[h]
\small
\centering
\caption{Seed LR sensitivity analysis. Cifar-10 on Resnet-18 trained for 150 epochs with \lrschedule{}. We vary the seed LR and explore epochs to get the best test accuracy for the particular setting. We report averages over 3 runs.}
\label{tab:seed_senstivity_cifar}

\begin{tabular}{ccc}
  \toprule
  Seed LR        &  Test Accuracy & Optimal Explore Epochs   \\
\midrule
  0.03   & 95.07 & 120\\ 
  0.05   & 95.12 & 120 \\ 
  0.0625    & 95.15 & 120\\ 
  0.075   & 95.34 & 100 \\ 
  0.0875   & 95.22 & 100\\ 
  0.1 & 95.14 & 100 \\
  0.115   & 95.20 & 60\\ 
  0.125   & 95.06 & 60 \\ 
  0.15   & 95.04 & 30 \\ 
 \bottomrule
\end{tabular}

\end{table}
 \section{Comparisons with More Baseline Learning Rate Schedules}
\label{sec:extra_baselines}

In this section we compare \lrschedule{} against more learning rate schedules -- one-cycle, linear decay and cosine decay.

\textbf{One-Cycle}: The one-cycle learning rate schedule was proposed in \cite{smith2018disciplined_onecycle} (also see \cite{smith2017cyclical}). This schedule first chooses a maximum learning rate based on an LR Range test. The LR range test starts from a small learning rate and keeps increasing the learning rate until the loss starts exploding (see figure~\ref{fig:lr_range_tests}). \cite{smith2018disciplined_onecycle} suggests that the maximum learning rate should be chosen to be bit before the minima, in a region where the loss is still decreasing. There is some subjectivity in making this choice, although some blogs and libraries\footnote{See e.g. \url{https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6} and \url{https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html}. Also see \url{https://docs.fast.ai/callbacks.lr_finder.html} and \url{https://docs.fast.ai/callbacks.one_cycle.html}} suggest using a learning rate one order lower than the one at minima. We go with this choice for all our runs.

Once the maximum learning rate is chosen, the one-cycle schedule proceeds as follows. The learning rate starts at a specified fraction\footnote{See div\_factor in \url{https://docs.fast.ai/callbacks.one_cycle.html}. We chose the fraction to be 0.1 in our experiments.} of the maximum learning rate and is increased linearly to the maximum learning rate for 45 percent of the training budget and then decreased linearly for the remaining 45. For the final 10 percent, the learning rate is reduced by a large factor (we chose a factor of 10).  We used an opensource implementation \footnote{\url{https://github.com/nachiket273/One_Cycle_Policy} } for our experiments.

\textbf{Linear Decay}: The linear decay learning rate schedule simply decays the learning rate linearly to zero starting from a seed LR.

\textbf{Cosine Decay}: The cosine decay learning rate schedule decays the learning rate to zero following a cosine curve, starting from a seed LR.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/one_cycle/lr_cifar.pdf}
        \caption{LR Range test for CIFAR-10}
        \label{fig:lr_range_test_cifar}
    \end{subfigure} \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/one_cycle/lr_fairseq.pdf}
        \caption{LR Range test for IWSLT}
        \label{fig:lr_range_test_iwslt}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/one_cycle/lr_vs_loss_wmt_radam.pdf}
        \caption{LR Range test for WMT}
        \label{fig:lr_range_test_WMT}
    \end{subfigure} \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/one_cycle/lr_imagenet.pdf}
        \caption{LR Range test for ImageNet}
        \label{fig:lr_range_test_imagenet}
    \end{subfigure}
\caption{LR Range test for selecting the maximum learning rate. A good choice is the learning rate is a bit before the minima in a region where the loss is still decreasing.}
\label{fig:lr_range_tests}
\end{figure}

\subsection{Cifar-10}
Figure~\ref{fig:lr_range_test_cifar}  shows the LR range test for Cifar-10 with the Resnet-18 network. The minima occurs around learning rate of 0.09, and we choose  as the maximum learning rate for the One-Cycle runs. For linear, cosine decay schedules we start with a seed learning rate of 0.1 as used in the standard baselines. The training loss and test accuracy for the various schedules are shown in Table~\ref{tab:cifar_results_extra_baselines_full_budget} for the full budget runs (200 epochs), and in Table~\ref{tab:cifar_results_extra_baselines_short_budget} for the short budget runs (150 epochs).

\begin{table}[h!]
\small
\centering
\caption{Cifar-10 on Resnet-18 full budget training (200 epochs): Training loss and Test accuracy for more learning rate schedules. We report the mean and standard deviation over 7 runs.}
\label{tab:cifar_results_extra_baselines_full_budget}

\begin{tabular}{ccc}
  \toprule
  LR Schedule     & Test Accuracy  & Train Loss \\ 
  \midrule
  One-Cycle       & 94.08 (0.07)  & 0.0041 (6e-5) \\
  Cosine Decay    & 95.23 (0.11) & 0.0023 (9e-5)  \\
  Linear Decay    & 95.18 (0.15) & 0.0018 (7e-5)  \\
  \lrschedule{}   & \textbf{95.26} (0.11) & 0.0023 (1e-4)  \\ 
\bottomrule
\end{tabular}

\end{table}

\begin{table}[h!]
\small
\centering
\caption{Cifar-10 on Resnet-18 short budget training (150 epochs): Training loss and Test accuracy for more learning rate schedules. We report the mean and standard deviation over 7 runs.}
\label{tab:cifar_results_extra_baselines_short_budget}

\begin{tabular}{ccc}
  \toprule
  LR Schedule   & Test Accuracy   & Train Loss  \\ 
  \midrule
  One-Cycle     & 93.84 (0.082)    & 0.0052 (7e-5)   \\
  Cosine Decay   & 95.06 (0.16)    & 0.0030 (2e-4)\\
  Linear Decay    & 95.02 (0.10)  & 0.0021 (1e-4) \\
  \lrschedule{}   & \textbf{95.14} (0.18) & 0.0044 (3e-4)  \\ 
  \bottomrule
\end{tabular}

\end{table}

\subsection{ImageNet}
Figure~\ref{fig:lr_range_test_imagenet} shows the LR range test for ImageNet with the Resnet-50 network. The minima occurs around learning rate of 2.16, and we choose  as the maximum learning rate for One-Cycle runs. For linear, cosine decay schedules we start with a seed learning rate of 0.1 as used in the standard baselines. The training loss and test accuracy for the various schedules are shown in Table~\ref{tab:ImageNet_full_budget_runs} for the full budget runs (90 epochs), and in Table~\ref{tab:ImageNet_short_budget_runs} for the short budget runs (50 epochs).

\begin{table}[h]
\small
\centering
\caption{ImageNet with ResNet-50 full budget training (90 epochs): Training loss, Test Top-1 and Test Top-5 for more learning rate schedules. We report the mean and standard deviation over 3 runs.}
\label{tab:ImageNet_full_budget_runs}
\begin{tabular}{cccc}
  \toprule
  LR Schedule    & Test Top-1  & Test Top-5  & Train Loss (av) \\ 
  \midrule
  One Cycle        & 75.39 (0.137) & 92.56 (0.040) & 0.96 (0.003) \\
  Cosine Decay      & 76.41 (0.212) & 93.28 (0.066) & 0.80 (0.002) \\
  Linear decay     &  76.54 (0.155) & 93.21 (0.051)  & 0.75 (0.001)\\
  \lrschedule{}     & \textbf{76.71} (0.097)  & \textbf{93.32} (0.031) & 0.79 (0.001) \\ \bottomrule
\end{tabular}

\end{table}
\begin{table}[h!]
\small
\centering
\caption{ImageNet with ResNet-50 short budget training (50 epochs): Training loss, Test Top-1 and Test Top-5 for more learning rate schedules. We report the mean and standard deviation over 3 runs.}
\label{tab:ImageNet_short_budget_runs}
\begin{tabular}{cccc}
  \toprule
  LR Schedule    & Test Top-1  & Test Top-5  & Train Loss (av) \\ 
  \midrule
  One Cycle        & 75.36 (0.096) & 92.53 (0.079) & 1.033 (0.004)\\
  Cosine Decay     & 75.71 (0.116) & 92.81 (0.033) & 0.96 (0.002) \\
  Linear decay     & 75.82 (0.080) & 92.84 (0.036) & 0.91 (0.002) \\
  \lrschedule{}    & \textbf{75.92} (0.11) & \textbf{92.90} (0.085) & 0.90 (0.003) \\
  \bottomrule
\end{tabular}

\end{table}



\subsection{WMT'14 EN-DE}
Figure~\ref{fig:lr_range_test_WMT} shows the LR range test for WMT'14 EN-DE on the transformer networks. The minima occurs near 1.25e-3. For the maximum learning rate, we choose 2.5e-4 for the default one-cycle policy. For linear, cosine decay schedules we start with a seed learning rate of 3e-4 as used in the standard baselines The training, validation perplexity and BLEU scores for the various schedules are shown in Table~\ref{tab:wmt_results_extra_baselines_full_budget} for the full budget runs (70 epochs), and in Table~\ref{tab:wmt_results_extra_baselines_short_budget} for the short budget runs (30 epochs).

\begin{table}[h]
\small
\centering
\caption{WMT'14 (EN-DE) on Transformer networks full budget training (70 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.}
\label{tab:wmt_results_extra_baselines_full_budget}
\begin{tabular}{cccc}
  \toprule
  LR Schedule   & Test BLEU Score  & Train ppl & Validation ppl   \\ 
  \midrule 
  One-Cycle     &  27.19 (0.081)    & 3.96 (0.014)  & 4.95 (0.013)     \\
  Cosine Decay  &  27.35 (0.09)     & 3.87 (0.011)   & 4.91 (0.008)   \\
  Linear Decay  &  27.29 (0.06)     & 3.87 (0.017)   & 4.89 (0.02)   \\
  \lrschedule{} & \textbf{27.53} (0.12)      & 3.89 (0.017)   & 4.87 (0.006)    \\ 
  \bottomrule
\end{tabular}

\end{table}

\begin{table}[h]
\small
\centering
\caption{WMT'14 (EN-DE) on Transformer networks short budget training (30 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.}
\label{tab:wmt_results_extra_baselines_short_budget}
\begin{tabular}{cccc}
  \toprule
  LR Schedule    & Test BLEU Score  & Train ppl & Validation ppl  \\ 
  \midrule
  One-Cycle      &  26.80 (0.2) & 4.38 (0.017)  & 5.02 (0.007)    \\
  Cosine Decay  &  26.95 (0.23) & 4.32 (0.013)  & 4.99 (0.011)   \\
  Linear Decay  & 26.77  (0.12) & 4.36 (0.092)  & 5.02 (0.01)  \\
  \lrschedule{} & \textbf{27.28} (0.17)  & 4.31 (0.02)   & 4.92 (0.007)    \\ 
  \bottomrule
\end{tabular}

\end{table}





\subsection{IWSLT'14 DE-EN}
Figure~\ref{fig:lr_range_test_iwslt} shows the LR range test for IWSLT on the transformer networks. The minima occurs near 2.5e-3. For the maximum learning rate, we choose 2.5e-4 for the default one-cycle policy. For linear, cosine decay schedules we start with a seed learning rate of 3e-4 as used in the standard baselines The training, validation perplexity and BLEU scores for the various schedules are shown in Table~\ref{tab:iwslt_results_extra_baselines_full_budget} for the full budget runs (50 epochs), and in Table~\ref{tab:iwslt_results_extra_baselines_short_budget} for the short budget runs (35 epochs).

\begin{table}[h]
\small
\centering
\caption{IWSLT'14 (DE-EN) on Transformer networks full budget training (50 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.}
\label{tab:iwslt_results_extra_baselines_full_budget}
\begin{tabular}{cccc}
  \toprule
  LR Schedule   & Test BLEU Score  & Train ppl & Validation ppl  \\ 
  \midrule
  One-Cycle     &  34.77 (0.064) & 3.68 (0.009)  & 4.97 (0.010)   \\
  Cosine Decay  &  35.21 (0.063) & 3.08 (0.004)   & 4.88 (0.014)  \\
  Linear Decay  &  34.97 (0.035) & 3.36 (0.001)   & 4.92 (0.035)   \\
  \lrschedule{} &  \textbf{35.53} (0.06)  & 3.00 (0.044)   & 4.86 (0.02)    \\ 
  \bottomrule
\end{tabular}

\end{table}

\begin{table}[h]
\small
\centering
\caption{IWSLT'14 (DE-EN) on Transformer networks short budget training (35 epochs): Training, validation perplexity and test BLEU scores for more learning rate schedules. The test BLEU scores are computed on the checkpoint with the best validation perplexity. We report the mean and standard deviation over 3 runs.}
\label{tab:iwslt_results_extra_baselines_short_budget}
\begin{tabular}{cccc}
  \toprule
  LR Schedule   & Test BLEU Score & Train ppl & Validation ppl    \\
  \midrule
  One-Cycle     &  34.43 (0.26) & 3.98 (0.028)  & 5.09 (0.017)     \\
  Cosine Decay  &  34.46 (0.33) & 3.86 (0.131)   & 5.06 (0.106)   \\
  Linear Decay  & 34.16  (0.28) & 4.11 (0.092)   & 5.14 (0.066)   \\
  \lrschedule{} & \textbf{35.08} (0.12) & 3.58 (0.063)   & 4.90 (0.049)    \\ 
  \bottomrule
\end{tabular}

\end{table}


\subsection{SQuAD-v1.1 finetuning with BERT\textsubscript{BASE}}
We choose 1e-5 as the maximum learning rate for One-Cycle runs as the minima occurs close to 1e-4 . For linear, cosine decays we start with a seed learning rate of 3e-5 as used in standard baselines.  Table~\ref{tab:squad_results_extra_baselines} show the average training loss, average test EM and F1 scores for the various schedules. We did not do a short budget training for this dataset, as the full budget is just 2 epochs.

\begin{table}[h!]
\small
\centering
\caption{SQuAD-v1.1 fine-tuning on BERT\textsubscript{BASE} for more learning rate schedules. We report the average training loss, average test EM, F1 scores over 3 runs.}
\label{tab:squad_results_extra_baselines}
\begin{tabular}{cccc}
  \toprule
  LR Schedule    & EM (av)  & F1 (av) & Train Loss (av)  \\ 
  \midrule
  One Cycle        & 79.9 (0.17) & 87.8 (0.091) & 1.062 (0.003) \\
  Cosine Decay      & 81.31 (0.07) & 88.61 (0.040) & 0.999 (0.003) \\
  Linear decay      & 80.89 (0.15) & 88.38 (0.042) & 1.0003 (0.004)\\
  \lrschedule{}     & \textbf{81.38} (0.02)  & \textbf{88.66} (0.045) & 1.003 (0.002)\\
  \bottomrule
\end{tabular}

\end{table} \clearpage
\section{Detailed Plots}


\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.88\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/imagenet/imagenet_momentum_tr_loss.pdf}
        \label{fig:imagenet_momentum_tr_loss}
    \end{subfigure}
    \begin{subfigure}[t]{0.88\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/imagenet/imagenet_momentum_test_top1.pdf}
        \label{fig:imagenet_momentum_test_top1_acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.88\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/imagenet/imagenet_momentum_test_top5.pdf}
        \label{fig:imagenet_momentum_test_top5_acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.88\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/imagenet/imagenet_momentum_lr_fixed_rb.pdf}
        \label{fig:imagenet_momentum_lr}
    \end{subfigure}
\caption{ImageNet on Resnet-50 trained with Momentum. Shown are the training loss, top-1/top-5 test accuracy and learning rate as a function of epochs, for the baseline scheme (orange) vs the \lrschedule{} scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.}
\label{fig:imagenet_momentum_result}
\end{figure}


\begin{figure}[h]
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/cifar/cifar_momentum_tr_loss.pdf}
        \label{fig:cifar_momentum_tr_loss}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/cifar/cifar_momentum_test_acc.pdf}
        \label{fig:cifar_momentum_test_acc}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/cifar/cifar_momentum_lr_fixed_rb.pdf}
        \label{fig:cifar_momentum_lr}
    \end{subfigure}
\caption{Cifar-10 on Resnet-18 trained with Momentum. Shown are the training loss, test accuracy and learning rate as a function of epochs, for the baseline scheme (orange) vs the \lrschedule{} scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.}
\label{fig:cifar_momentum_result}
\end{figure}


\begin{figure}[ht]
\begin{minipage}{\textwidth}
  \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/pretraining_bert/loss.pdf}
        \label{fig:bert_tr_loss}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/pretraining_bert/lr.pdf}
        \label{fig:bert_lr}
    \end{subfigure}
\caption{BERT\textsubscript{LARGE} pretraining for batch size of 16k with LAMB optimizer for the short budget runs. Shown are the training loss and learning rate as a function of steps, for the baseline scheme short budget (orange) vs the \lrschedule{} scheme short budget (blue). The plot is split into 2 parts to give a clear picture of the two phases of training~\cite{devlin2018bert}. Note that even though the training loss curves look similar for the two runs, we see a significant gap in F1 score obtained when we fine-tune the model checkpoints on SQuAD-v1.1 \cite{rajpurkar2016squad}. See Table~\ref{tab:bert-large_finetuning} for details.
}
\label{fig:bert_plots}
\end{minipage}
\begin{minipage}{\textwidth}
\small
\centering
\vspace{25pt}
\begin{tabular}{cccccc}
  \toprule
  LR Schedule & F1 - Trial 1   & F1 - Trial 2 & F1 - Trial 3 & F1 avg. & F1 max \\
  \midrule
    Baseline (short budget)  & 90.39 & 90.64 & 90.53 & 90.52 & 90.64\\
  \lrschedule{}{} ( short budget ) & 91.22  & 91.29 & 91.18 & 91.23 & 91.29\\
  \lrschedule{}{} ( full budget ) & 91.45  & 91.41 & 91.51 & 91.46 & 91.51\\
  
  \bottomrule
\end{tabular}
\captionsetup{type=table}
\caption{SQuAD fine-tuning on BERT\textsubscript{LARGE}. We report F1 scores for 3 different trials as well as the maximum and average values.}
\label{tab:bert-large_finetuning}
\end{minipage}
\end{figure}

\begin{figure}[h]
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/wmt/tr_ppl.pdf}
        \label{fig:wmt_tr_loss}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/wmt/val_ppl.pdf}
        \label{fig:wmt_test_acc}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/wmt/lr.pdf}
        \label{fig:wmt_lr}
    \end{subfigure}
\caption{WMT'14 (EN-DE) on Transformer\textsubscript{BASE} network trained with RAdam. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the \lrschedule{} scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.}
\label{fig:wmt_radam_lrl}
\end{figure}


\begin{figure}[h]
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/iwslt_transformer/tr_ppl.pdf}
        \label{fig:iwslt_adam_tr_loss}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/iwslt_transformer/val_ppl.pdf}
        \label{fig:iwslt_adam_test_acc}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/iwslt_transformer/lr.pdf}
        \label{fig:iwslt_adam_lr}
    \end{subfigure}
\caption{IWSLT'14 (DE-EN) on Transformer\textsubscript{BASE} network trained with RAdam. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the \lrschedule{} scheme (blue). The plot is split into 3 parts to permit higher fidelity in the y-axis range.}
\label{fig:iwslt_adam_result}
\end{figure}



\begin{figure}[h]
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/MAT_IWSLT/tr_ppl.pdf}
        \label{fig:MAT_adam_tr_ppl}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/MAT_IWSLT/val_ppl.pdf}
        \label{fig:MAT_adam_val_ppl}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/MAT_IWSLT/lr.pdf}
        \label{fig:MAT_adam_lr}
    \end{subfigure}
\caption{IWSLT'14 (DE-EN) on MAT network trained with Adam. Shown are the training perplexity, validation perplexity and learning rate as a function of epochs, for the baseline scheme (orange) vs the \lrschedule{} scheme (blue). MAT training involves two training phases with 200 epochs each, shown in separate columns above.}
\label{fig:MAT_adam_result}
\end{figure}


\begin{figure}[h]
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/squad_bert/squad_bert_adam_tr_loss.pdf}
        \label{fig:squad_bert_adam_tr_loss}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/squad_bert/squad_bert_adam_val_em_score.pdf}
        \label{fig:squad_bert_adam_test_acc}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/squad_bert/squad_bert_adam_lr_fixed_rb.pdf}
        \label{fig:squad_bert_adam_lr}
    \end{subfigure}
\caption{SQuAD-v1.1 fine-tuning on BERT\textsubscript{BASE} trained with Adam. Shown are the training loss, test EM score, and learning rate as a function of epochs, for the baseline scheme (orange) vs the \lrschedule{} scheme (blue). The plot is split into 2 parts to permit higher fidelity in the y-axis range. It is clear that with \lrschedule{} the network starts to overfit after the 2nd epoch, where the testing loss continues to go down, but generalization suffers. We saw similar behavior with different seeds, and thus need to train with \lrschedule{} for only 2 epochs.}

\label{fig:squad_bert_adam_result}
\end{figure}
 \clearpage
\clearpage


\end{document}
