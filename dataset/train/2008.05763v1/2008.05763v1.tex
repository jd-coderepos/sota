

\documentclass[a4paper,10pt]{extarticle}
\usepackage{a4wide}
\usepackage{times}
\usepackage[margin=0.65in]{geometry}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}       
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}       
\usepackage{microtype}     
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{color}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{soul}
\usepackage{cleveref}

\makeatother


\def\Ac{\mathcal A}
\def\Bc{\mathcal B}
\def\Ec{\mathcal E}
\newcommand{\etal}{\emph{et al.}}
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{0em}{-0.5em}{\normalfont\normalsize\bfseries}}


\title{Powers of layers for image-to-image translation}

\author{Hugo Touvron  
\hspace{0.32cm} 
Matthijs Douze  
\hspace{0.32cm} 
Matthieu Cord 
\hspace{0.32cm} 
Herv\'e J\'egou 
\\
~\\
\scalebox{1.}{ Facebook AI Research \hspace{0.6cm}  Sorbonne University}
\date{}
}


\begin{document}

\maketitle

\begin{abstract}
We propose a simple architecture to address unpaired image-to-image  translation tasks: style or class transfer, denoising, deblurring, deblocking, etc. 
We start from an image autoencoder architecture with fixed weights. 
For each task we learn a residual block operating in the latent space, which is iteratively called until the target domain is reached. 
A specific training schedule is required to alleviate the exponentiation effect of the iterations. 
At test time, it offers several advantages: the number of weight parameters is limited and the compositional design allows one to modulate the strength of the transformation with the number of iterations. 
This is useful, for instance, when the type or amount of noise to suppress is not known in advance.  
Experimentally, we provide proofs of concepts showing the interest of our method for many transformations. 
The performance of our model is comparable or better than CycleGAN with significantly fewer parameters.
\end{abstract}

\section{Introduction}\label{sec:introduction}


Neural networks define arbitrarily complex functions involved in discriminative or generative tasks by stacking layers, as supported by the universal approximation theorem~\cite{hornik1989multilayer,montufar2014universal,goodfellow2016deep}.
More precisely, the theorem states that stacking a number of basic blocks can approximate any function with arbitrary precision, provided it has enough hidden units, with mild conditions on the non-linear basic blocks.

Studies on non-linear complex holomorphic functions involved in escape-time fractals showed that iterating simple non-linear functions can also construct arbitrarily complex landscapes~\cite{barnsley1988science}.
These functions are complex in the sense that their iso-surfaces are made arbitrarily large by increasing the number of iterations. 
Yet there is no control on the actual shape of the resulting function. This is why generative fractals remain mathematical curiosities or at best
tools to construct intriguing landscapes. 

Our objective is to combine the expressive power of both  constructions, and study 
the optimization of a function that iterates a single building block in the latent space of an auto-encoder. 
We focus on image translation tasks, that can be trained from either \emph{paired} or \emph{unpaired} data.
In the paired case, pairs of corresponding input and output images are provided during training. It offers a direct supervision, so the best results are usually obtained with these methods~\cite{chen2017fast,wang2018pix2pixHD,park2019SPADE}.

In this paper we focus on the unpaired case: only two corpora of images are provided, one for the input domain  and the other for output domain . Therefore we do not have access to any parallel data~\cite{conneau2017word}, which is a  realistic scenario in many applications, e.g., image restoration. 
We train a function , such that the output  for  is indiscernible from images of . 


Our transformation is performed by a single residual block that is composed a variable number of times. 
We obtain this compositional property thanks to a progressive learning scheme that ensures that the output is valid for a large range of iterations.
As a result, we can modulate the strength of the transformation by varying the number of times the transformation is composed. This is of particular interest in image translation tasks such as denoising, where the noise level is unknown at training time, and style transfer, where the user may want to select the best rendering. 
This ``Powers of layers'' (PoL) mechanism is illustrated in Figure~\ref{fig:method} in the category transfer context (horse to zebra).

Our architecture is very simple and only the weights of the residual block differ depending on the task, which makes it suitable to address a large number of tasks with a limited number of parameters. 
This proposal is in sharp contrast with the trend of current state-of-the-art works to specialize the architecture and to increase its complexity and number of parameters~\cite{Fu2019GeometryConsistentGA,viazovetskyi2020stylegan2,choi2019starganv2}. 
Despite its simplicity, our proof of concept exhibits similar or better performance than a vanilla CycleGAN architecture, all things being equal otherwise, for the original set of image-to-image translation tasks proposed in their papers, as well as for denoising, deblurring and deblocking. With significantly fewer parameters and a versatile architecture, we report competitive results confirmed by objective and psycho-visual metrics,  illustrated by visualizations.  
 \begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{images/Fig_continuous/method_schem3.pdf}
\vspace{-5pt}
\caption{\label{fig:method} 
Illustration of Powers of layers for a category transfer task. The encoder and decoder are directly borrowed from a vanilla auto-encoder and are not learnable. 
At inference time, we apply a variable number of compositions, producing different images depending on how many times we compose the residual block in the embedding space. 
Depending on the task, we either modulate the transformation and choose the result, or let a discriminator determine when to stop iterating. 
}
\end{figure*}

 \section{Related work}\label{sec:related}

\paragraph{Generative adversarial networks (GANs)}~\cite{Goodfellow2014GenerativeAN} is a framework where two networks, a generator and a discriminator, are learned together in a zero-sum game fashion. 
The generator learns to produce more and more realistic images wrt. the training dataset with real images.
The discriminator learns to discriminate better and better between real data and generated images. 
GANs are used in many tasks such as domain adaptation, style transfer, inpainting and talking head generation~\cite{Bousmalis2016DomainSN,Karras2019AnalyzingAI,Pathak2016ContextEF,Zakharov2019FewShotAL}. 

\paragraph{Unpaired image-to-image translation} considers the tasks of  transforming an image from a domain  into an image in a domain . 
The training set comprises a sample of images from domains  and , but no pairs of corresponding images. 
A classical approach is to train two generators ( and ) and two discriminators, one for each domain. 
When there is a shared latent space between the domains, a possible choice is to use a variational auto encoder like in CoGAN~\cite{Liu2016CoupledGA}. 
CycleGAN~\cite{Zhu2017CycleGAN}, DualGAN~\cite{Yi2017DualGAN} and subsequent works~\cite{huang2018munit,liu2019FUNIT,Liu2017UnsupervisedIT,Fu2019GeometryConsistentGA,choi2019starganv2} augment the adversarial loss induced by the discriminators with a cycle consistency constraint to preserve semantic information throughout the domain changes. 
All these variants have architectures roughly similar to CycleGAN: an encoder, a decoder and residual blocks operating on the latent space. 
They also incorporate elements of other networks such as StyleGAN~\cite{Karras2018ASG}.
In our work, we build upon a simplified form of the CycleGAN architecture that generalizes over tasks easily.

\paragraph{High resolution images with GANs. } 
Generating high-resolution images is challenging. 
Until recently, GANs architectures were designed to produce low-resolution images.
Indeed, the memory usage at training time depends heavily on the size of the images. 
The general approach is to produce the target image in a scale pyramid, which outputs results of much finer quality~\cite{karras2017progressive,Karras2018ASG,Brock2018LargeSG,choi2019starganv2,Lin2019COCOGANGB,Karnewar2019MSGGANMG}.
Generating high resolution images makes it possible to get closer to the format of real pictures used in CGI~\cite{Karras2019AnalyzingAI,choi2019starganv2} or medical imaging~\cite{Zbontar2018fastMRIAO}. 

\paragraph{Transformation modulation}
is an interpolation between two image domains.
It is a byproduct of some approaches~\cite{lample2017fader,Brock2018LargeSG,Radford2015UnsupervisedRL}.
For instance, a linear interpolation in latent space~\cite{Brock2018LargeSG,Radford2015UnsupervisedRL} morphs between two images.
Nevertheless, one important limitation is that the starting and ending points must both be known, which is not the case in unpaired learning. 
Other approaches such as the Fader networks~\cite{lample2017fader} or StyleGan2~\cite{viazovetskyi2020stylegan2} act on scalar or boolean attributes that are disentangled in the latent space ({\it eg.,} age for face images, wear glasses or not, etc). 
Nevertheless, this results in complex models, 
for which dataset size and the variability of images strongly impacts the performance: they fail to modulate the transform with small datasets or with large variabilities. 
A comparison of PoL with the Fader network is provided in Appendix~\ref{app:cyclegan} and shows that our approach is more effective. 

\paragraph{Progressive learning and inference time modulation}
are performed in multi-scale methods such as SinGAN~\cite{rottshaham2019singan} and ProgressiveGAN~\cite{karras2017progressive}. 
Progressive learning obtains excellent results for high resolution images where it is more difficult to use classical approaches. 
The training is performed in several steps during which the size of both the images and the network are increased.

The inference time of some architectures can be modulated by stopping the forward pass at some layer~\cite{Huang2016DeepNW,Wu2017BlockDropDI,Veit2017ConvolutionalNW,Figurnov2016SpatiallyAC}.
This differs from our approach, where the number of residual block compositions (``powers'') can be chosen, to shorten the inference. 
A by-product is a reduction of the number of network parameters.

\paragraph{Weight sharing}
is a way of looking at our method, because the same layer is applied several times within the same network.
Recurrent Neural Networks (RNN) are the most classical example weight sharing in a recursive architecture.
Besides RNNs, weight sharing is mainly used for model compression, sequential data and ordinary differential equations (ODE)~\cite{Gao2019CrossDM,Polino2018ModelCV,Han2015DeepCC,Chen2018NeuralOD}. 
A few works~\cite{Jastrzebski2017ResidualCE,Zhang2018RecurrentCA} apply weight sharing to unfold a ResNet and evaluate its performance in classification tasks.
The optimization is inherently difficult, so they use independent batch normalization for each shared layer.
With PoL we observe the same optimization issues, that we solve by a progressive training strategy, see Section~\ref{sec:progressive}.
Recent work~\cite{Jeon2020DifferentiableFI} are interested in the composition of the same block by considering the parallel with the fixed point theorem, nevertheless their application remains to rather simple problems compared to our unpaired image-to-image translation tasks. 
 \section{Power of layers}\label{sec:method}

\newcommand\norm[1]{\left\lVert#1\right\rVert}
We adopt the same context as CycleGAN~\cite{Zhu2017CycleGAN} and focus on unpaired image to image translation: the objective is to transform an image from domain  into an image from domain .
In our case the domains can be noise levels, painting styles, blur, JPEG artifacts, or simply object classes that appear in the image.
The training is unpaired: we do not have pairs of corresponding images at training time. 
CycleGAN is simple, adaptable to different tasks and allows a direct comparison in Sections~\ref{sec:analysis} and~\ref{sec:experiments}. 

We learn two generators and two discriminators.
The generator  transforms an element of  into an element of , and  goes the other way round,  being the fixed-resolution image space.
The discriminators  (resp. ) predicts whether an element belongs to domain  (resp ).
We use the same losses as commonly used in unpaired image-to-image translation:





The Adversarial loss  verifies that the generated images are in the correct domain. The Cycle Consistency loss  ensures a round-trip through the two generators reconstructs the initial image, and the {identity loss}  penalizes the generators transforming images that are already in their target domain. We keep the same linear combination coefficients as in CycleGAN~\cite{Zhu2017CycleGAN}:
, , .

\newcommand{\Enc}{\mathrm{Enc}}
\newcommand{\Dec}{\mathrm{Dec}}
\newcommand{\res}{\mathrm{res}}

\subsection{Network architecture}
\label{sec:arch}

We start from the CycleGAN architecture~\cite{Zhu2017CycleGAN}. 
The encoder and decoder consist of 2 layers and a residual block. 
The embedding space  of our model is : its spatial resolution is 1/4 the input image resolution of  and it has 256 channels.
All translation operations take place in the fixed embedding space .
The encoder  produces the embedding and consists of two convolutions. 
The decoder  turns the embedding back to image space and consists of two transposed convolutions.

Note that we will provide the implementation for the sake of reproducibility.

\paragraph{Pre-training of the auto-encoder.}
We train the encoder and decoder of our model 
on a reconstruction task with 6M images randomly drawn from the YFCC100M dataset~\cite{Thomee2016YFCC100MTN} during one epoch, using an  reconstruction loss in pixel space. 
We use the Adam optimizer with a learning rate of .
Our data-augmentation consists of an image resizing, a random crop and a random horizontal flip. 
Both the encoder and decoder weights are fixed for all the other tasks, only the residual block is adapted (and the discriminator in case we use it for the stopping criterion). 

\paragraph{The embedding transformer -- single block.}

The transformation between domains is applied is based on a residual block , similar to the feed-forward network used in transformers~\cite{Vaswani2017AttentionIA}.
It writes: 


\newcommand{\expfactor}{K}

There is a dimensionality expansion factor  between the two convolutions in the residual block (see Figure~\ref{fig:method}). 
Adjusting  changes the model's capacity. We adopt the now standard choice of the original transformer paper (). 
The full generator writes

The other direction, with  and , is defined accordingly. 


\paragraph{Powers of layers. }

We start from the architecture above and augment its representation capacity. 
There are two standard ways of doing this: (1) augmenting the capacity of the  block by increasing ; (2) increasing the depth of the network by chaining several instances of , since the intermediate representations are compatible. 

In contrast to these fixed architectures, PoL \emph{iterates} the  block  times, which amounts to sharing the weights of a deeper network:



\subsection{Optimization in a residuals blocks weight sharing context}

In the following, we drop the  suffix from , since powers of layers operates in the same way on  and . Thus, 
 is . 
The parameters of  are collected in a vector .
The embedding  is 3D activation map, but for the sake of the mathematical derivation we linearize it to a vector.
The partial derivatives of  are  and .
We compose the  function  times as





The stability of the SGD optimization depends on the magnitude and conditioning of the matrix  defined as:



which is sensitive to initialization during the first optimization epochs.
Indeed, the length of the SGD steps on  depends on the eigenvalues of .
When simplifying the basic residual block to a linear transformation  (i.e., ignoring the normalization and the ReLU non-linearity), we have .
The eigenvalues of  are , where  are the eigenvalues of . 
At initialization, the components of  are sampled from a random uniform distribution. 
To reduce the magnitude of , one option is to make the entries of  small. 
However, to decrease   sufficiently,  must be so small that it introduces floating-point cancellations when the residual block is added back to the shortcut connection. 
This is why we prefer to adjust , as detailed next.

\subsection{Progressive training}
\label{sec:progressive}

We adopt a progressive learning schedule in a ``warm up'' phase: we start the optimization with a single block and add one iteration at every epoch until we reach the required  blocks.
This is possible because the blocks operate in the same embedding space  and because their weights are shared, so all blocks are still in the same learning schedule.
In addition, this approach allows the discriminator to improve progressively during the training. 
For example, in the case of the transformation horse  zebra, a slightly whitened horse fools the discriminator at the beginning of the training, but a stronger stripes texture is required later on.

\paragraph{Training for modulation.}
If the network is trained with a fixed number of compositions, the intermediate states do not correspond to modulations of the transformation that ``look right'' (see Appendix~\ref{app:training}).
Therefore, in addition to this scheduled number of iterations during the first  epochs of warm up, we also randomize the number of iterations in subsequent epochs. 
This forces the generator to also produce acceptable intermediate states, and enables modulating the transform.

\paragraph{Stopping criterion at inference time.} 
Each image of domain  is more or less close to domain .
For example, when denoising, the noise level can vary so the denoising strength should adapt to the input. 
Similarly for horsezebra: a white horse is closer to a zebra than a brown horse.
Therefore, at inference time, we can adjust the number of compositions as well. 
In particular, for each test image, we select  that best deceives the discriminator, thus effectively adapting the processing to the relative distance to the target domain.  
 \section{Analysis}\label{sec:analysis}

\newcommand{\stdminus}[1]{ \scalebox{0.8}{}}

In this section we study the impact of the main training and design choices and on the performance of powers of layers. 
Appendices~\ref{app:training} and~\ref{app:inference} provide complementary analysis for training- and inference-time choices, respectively. 

For this preliminary analysis, we focus on denoising tasks for which the performance is easily measurable. We add three types of noise to images: 
Gaussian noise, Gaussian blur and JPEG artifacts. 
The noise intensity is quantified by the noise standard deviation, the blur radius and the JPEG quality factor, respectively. 
We generate transformed images and measure how well our method recovers the initial image. 


Note that, in the literature, these tasks are best addressed by providing  (original, noisy) pairs if images. 
Our objective is to remain in a completely unpaired setting during the training phase. 
It corresponds to the case where parallel data is not available (like for the restoration of ancient movies), and also better reflects the situation where the noise strength is not known in advance. 
Therefore, the original image is solely employed to measure the performance.
This provides a more reliable signal than more classical unpaired image-to-image translation evaluations. 
\newcommand{\maxcomp}{n_\mathrm{tr}}
\newcommand{\maxcomptest}{n_\mathrm{te}}


\paragraph{Experimental protocol.}
To train, we sample 800 domain  images from the high-resolution Div2K dataset~\cite{Agustsson_2017_CVPR_Workshops}. 
In the baseline training procedure, the warm up phase starts from a single block and increases the number of compositions at every epoch, until we reach epoch . Then we keep the number of compositions fixed. 

We test on the Urban-100~\cite{Huang2015Urban100} dataset.
Unless specified otherwise, we set the number of compositions to  and
measure the Peak Signal to Noise Ratio (PSNR) of our model on the dataset images, degraded with the same intensity as at training time.
For the JPEG case we use the  Naturalness Image Quality Evaluator metric~\cite{Mittal2013NIQE} (NIQE, lower=better) instead, because it is more sensitive to JPEG artifacts.
NIQE is a perceptual metric that does not take the original image into account. 





\begin{table*}
\centering
\scalebox{0.8}
{
\small
\begin{tabular}{|c|cc|cc|}
  \toprule

      & \multicolumn{2}{c}{Gaussian noise (std=30)} & \multicolumn{2}{|c|}{Gaussian blur (=4)}\\
       & POL &  ind & POL &  ind  \\
    \midrule
    1 &  23.3 & \textbf{23.3}&  18.6 & 18.6 \\
    2 &  23.3 & 23.2&  18.5 & 18.6 \\
    4 &  24.4 &  23.2&  19.2 & 19.2 \\
    8 &  23.9 & 22.3 &  19.0 & \textbf{19.3} \\
    12 &  23.9 & 22.5 &  \textbf{19.7} & 18.8 \\
    16 &  23.9 & 22.5&  19.0 & 18.1 \\
    18 &  \textbf{24.2} & \_ &  19.0 & \_  \\
    24  &  23.8 &\_ &  18.6 & \_  \\
    30 &   23.5 & \_ & 19.0&\_  \\
    \bottomrule
\end{tabular}
}
\hfill
\begin{minipage}{0.55\linewidth}
\caption{\label{tab:psnr_urban100_sharing_max_rep}
Denoising: PSNR on Urban-100~\cite{Huang2015Urban100}.
Comparison between Power of layers (POL) and independent (ind) blocks for different maximum number of composition / residual block. Best value for each column  are in \textbf{Bold}.
We could not fit more than 16 independent blocks in memory in our experiments.
 We provide standard deviation and additional results in Appendix~\ref{app:training}.
} \vfill
\end{minipage}
\end{table*}
 
\paragraph{Block composition or independent successive blocks?}

Table~\ref{tab:psnr_urban100_sharing_max_rep} compares PoL's composition of the same block versus using independent blocks with distinct weights. 
In spite of the much larger capacity offered by independent blocks, the noise reduction operated by Power of layers is stronger. 
Our interpretation is that the model is easier to train. 

\begin{figure*}
\scalebox{0.76}{
\small
\begin{tabular}{c@{}cc@{}}
     & Discriminator of Natural images & Discriminator of Noisy images \\
     \midrule
     \rotatebox{90}{\quad \quad Fixed } &
      \includegraphics[width=0.40\linewidth]{images/Fig_randomised/curve_no_randomised_Disc_Natural_6.pdf}&
    \includegraphics[width=0.40\linewidth]{images/Fig_randomised/curve_no_randomised_Disc_Noise_6.pdf}\\
     \midrule
   \rotatebox{90}{Random } &
      \includegraphics[width=0.40\linewidth]{images/Fig_randomised/curve_randomised_Disc_Natural_6.pdf}&
      \includegraphics[width=0.40\linewidth]{images/Fig_randomised/curve_randomised_Disc_Noise_6.pdf}\\
\end{tabular}
}
\hfill
\begin{minipage}{0.3\linewidth}
\caption{\label{tab:ada_comp}
 Response of the discriminators as a function of the number of compositions for the transformation Gaussian noisenatural image.
 We plot the average response and the standard deviation over examples (gray). 
 Higher=the discriminator classifies the image into its target domain.
\emph{Top:} training with a fixed number of compositions, =30.
\emph{Bottom:} training with randomised . 
}
\end{minipage}
\vspace{-0.6em}
\end{figure*}
 

\paragraph{Analysis of the progressive training strategy.}

Table~\ref{tab:psnr_urban100_sharing_max_rep} also evaluates the impact of the maximum number of compositions .
Having several compositions clearly helps.
Since we choose the number of compositions  at inference time (see next paragraph), it may be relevant to vary  at training time to minimize the discrepancy between the train- and test-time settings.

For this, we tried different intervals to randomly draw the maximum number of compositions for each epoch, after the warm-up phase.
If  is fixed, the optimal choice is . However, if we use an adaptive , the best range is , and the adaptive case with randomised training gives the best performance for denoising and debluring. 
Appendix~\ref{app:training} reports results obtained with different  ranges. 

\paragraph{Stopping criterion.} 

We consider two cases: either we use a fixed , or we 
use the discriminator to evaluate the  transformation quality: it selects the value  maximizing the target discriminator error for a given image.
Figure~\ref{tab:ada_comp} shows that setting a fixed  causes the discriminator to select  as the best iteration at inference time.
By selecting the best  for each image we obtain on average 
a PSNR improvement of +1.36dB   
for a Gaussian noise of standard deviation , compared to fixing .
In Appendix~\ref{app:inference}, we compare it with the best possible stopping criterion: an Oracle that selects  directly on the PSNR. 
Our adaptive strategy significantly tightens the gap to this  upper bound. 

\paragraph{Comparison with CycleGAN.}

We use CycleGAN as a baseline. 
The differences between CycleGAN and powers of layers are 
(1) we use a single encoder and decoder trained in advance and common to all tasks;
(2) CycleGAN has 9 residual blocks, PoL iterates a single residual block an arbitrary number of times.
The inference time of PoL depends on the number of compositions  but the number of parameters does not:

\begin{center}
{
\vspace{-10pt}
\scalebox{0.8}{
\begin{tabular}{c|cc|c|c}
    \toprule
    & encoder + decoder & residual block & discriminators & total \\
    PoL&  1.7M & M & 2.7M & 15.9M\\
    CycleGAN & \multicolumn{2}{c|}{ 11.4M}& 2.7M & 28.2M \\
    \bottomrule
\end{tabular}
}
}
\end{center}

Figure~\ref{tab:psnr_urban100_comp} compares the performance obtained by the two methods on denoising tasks with varying noise intensities. 
PoL gives better results than CycleGAN in terms of objective metrics, and overall the  images produced by our method look as realistic and/or accurate. 



\begin{figure*}
\centering
\scalebox{0.625}{
\small
\def \mysp {\hspace{5pt}}
\begin{tabular}{|c|ccc|c|ccc|c|ccc||c|cc|cc|cc|}
  \toprule
    \multicolumn{12}{|c||}{Impact of the noise intensity applied to Urban-100 images} & \multicolumn{7}{|c|}{Impact of the amount of training data } \\
    \midrule
    \multicolumn{8}{|c|}{PSNR (higher=better)} & \multicolumn{4}{|c||}{NIQE (lower=better)} & &\multicolumn{4}{|c|}{PSNR} & \multicolumn{2}{|c|}{NIQE}\\
    \midrule
     \multicolumn{1}{|c}{Noise} &  Noisy & \multicolumn{2}{c|}{Denoising}  &
     \multicolumn{1}{|c}{Sigma} & Blur & \multicolumn{2 }{c|}{Debluring}& 
     \multicolumn{1}{|c}{JPEG} & JPEG &\multicolumn{2}{c||}{Deblocking} &
     \# train  & \multicolumn{2}{|c|}{Denoise (std=30)} &
     \multicolumn{2}{c}{Deblur (=4)}& 
     \multicolumn{2}{|c|}{JPEG (qual 25)}\\
     
     
    \multicolumn{1}{|c}{(std)} &   images & CLG & PoL &
    \multicolumn{1}{|c}{Blur} &  images & CLG & PoL& 
    \multicolumn{1}{|c}{quality} &  images & CLG & PoL &
    images & CLG  & PoL & CLG  & PoL & CLG  & PoL \\
    \midrule	
    15 & 24.9 & 22.4  &\textbf{27.4}  &
    2 & 21.6 & 20.4 & \textbf{22.1}&
    15 & 9.0  & \textbf{7.9}  &\textbf{7.9} &
    1 & 14.5& \textbf{22.3}& 14.4& \textbf{18.7} & 15.6& \textbf{7.9}\\
    
    
    30 & 19.2   & 21.9 & \textbf{23.7}  & 
    4 & 19.2  & 18.5 & \textbf{19.2} &
    25 &  8.9 & 7.5  & \textbf{7.1} &
    5 & 16.6 & \textbf{23.1} & 16.7 & \textbf{18.8} & 12.6 & \textbf{7.6} \\
    
    
    50 & 15.2  & 21.6 & \textbf{ 22.5}  &
    8 &  17.5  & 16.0 & \textbf{17.5} &
    30 & 8.9  & 7.5 & \textbf{6.8}&
    10 & 20.9 & \textbf{23.2} & 17.0 & \textbf{18.8} & 7.8 & \textbf{7.3}\\
    
    
    70 & 12.7  & \textbf{21.0}& 20.9 &
    16 & 16.1  & 16.1 & \textbf{16.2}  &
    50 & 9.0  & 7.0  &   \textbf{6.6} &
    100 & 21.0 & \textbf{23.4} & 18.2 & \textbf{19.0} & 7.5& \textbf{7.0} \\
    
    
    100 & 10.4  & \textbf{20.0} & 19.4 & 
    24 & 15.5  & 12.9 & \textbf{15.5}&
    70 & 8.9 & 7.1  &   \textbf{6.8} & 
    400 & 21.8 & \textbf{23.6} & 18.2 & \textbf{19.0}& 7.5 & \textbf{6.9}\\
\bottomrule
\end{tabular}
}
\scalebox{0.65}{
    \small
    \begin{tabular}{@{}c@{\hspace{5pt}}c|c@{\hspace{5pt}}c|c@{\hspace{5pt}}c@{}}
      \multicolumn{2}{c}{Deblocking} & \multicolumn{2}{c}{Denoising} & \multicolumn{2}{c}{Debluring}    \\
     Original Image  & Image JPEG (quality=25)  & Image with Noise(std=30)  & CycleGAN & Image with Blur (=4) & CycleGAN\\ 
    \includegraphics[width=0.24\linewidth]{images/analyse_blurr/im_org2_crop_2.png} &
    \includegraphics[width=0.24\linewidth]{images/JPEG/im_JPEG_2.png} &
    
     \includegraphics[width=0.24\linewidth]{images/analyse_noise/im_With_noise_org_crop_2.png} & 
     \includegraphics[width=0.24\linewidth]{images/analyse_noise/im_bsl_denoise_crop_2.png}&


     \includegraphics[width=0.24\linewidth]{images/analyse_blurr/im_blurr2_crop_org_2_s4.png}&
      \includegraphics[width=0.24\linewidth]{images/analyse_blurr/im_bsl_deblurr2_crop_2_s4.png}\\


      CycleGAN & PoL  & PoL (manual) & PoL (discriminator) &  PoL (manual)  & PoL (discriminator)\\
      \includegraphics[width=0.24\linewidth]{images/JPEG/im_bsl_dejpeg_190.png}&
     \includegraphics[width=0.24\linewidth]{images/JPEG/im_POL_DEJPEG_15.png} &
     
     \includegraphics[width=0.24\linewidth]{images/analyse_noise/im_POL_denoise_rz10.png}&
    \includegraphics[width=0.24\linewidth]{images/analyse_noise/im_POL_denoise_rz16.png} &
    
     \includegraphics[width=0.24\linewidth]{images/analyse_blurr/im_deblurr2_crop_2_s4.png}&
        \includegraphics[width=0.24\linewidth]{images/analyse_blurr/im_deblurr4_crop_5.png} \\
    \end{tabular}

 }
\caption{\label{tab:psnr_urban100_comp}
{\it Top:} Comparison between Powers of layers (PoL) and CycleGAN (CLG) to denoise images of  Urban-100~\cite{Huang2015Urban100}.
We provide standard deviation and additional results in Appendix~\ref{app:cyclegan}.
{\it Bottom:} Visual comparison between our method (manual and discriminator choice) and CycleGAN for deblocking, denoising and debluring.  
}
\end{figure*}
 

\paragraph{Training with few images.}
 
Figure~\ref{tab:psnr_urban100_comp} also compares our method with CycleGAN  when training in a data-starving scenario. 
Whatever the number of  training images, PoL outperforms CycleGAN, but the gap is particularly important with very few images. 
This is expected for two reasons. Firstly, our approach has fewer parameters to learn than CycleGAN. Secondly, it only requires to learn the transformation because the encoder and decoder are pre-learned as a vanilla auto-encoder, while CycleGAN needs to learn how to encode and decode images.

\paragraph{Parametrization: remarks.}

Beyond the settings inherited from CycleGAN, the main training parameters of Powers of layers are the maximum number of compositions  and the range from which they are randomly sampled.
The number of compositions at inference time  is also important but the discriminator criterion can be used to set it automatically.
 \section{Experiments}\label{sec:experiments}

We now run experiments on two image generation applications. We refer to Section~\ref{sec:arch} for the architecture and training protocol. 

In Appendix~\ref{app:cyclegan} we also give a comparison with the Fader network for the capacity to modulate a transformation, and more visual examples in Appendix~\ref{app:visu}. 

\paragraph{Unpaired image-to-image translation.}
We report results for 6 of the 8 unpaired image-to-image translation tasks introduced in the CycleGAN~\cite{Zhu2017CycleGAN} paper (the two remaining ones lead to the same conclusions) and we used the datasets from the website~\cite{cycleGANtasks}. 
We compare the Frechet Inception Distance (FID) of these two approaches in Figure~\ref{tab:cycleganfid}.
The FID measures the similarity between two datasets of images, we use it to compare the target dataset with the transformed dataset. 
It is a noisy measure for which only large deviations are significant. 
Yet the results and visualization show that our method has results comparable to those of CycleGAN, achieved with much fewer parameters.



 
    \begin{figure*}

    \centering
    \scalebox{0.8}{
    \small
    \begin{tabular}{lcc}
      \toprule
      Domain &  CycleGAN  &   POL\\
    \midrule	
    Summer  Winter & 48.8 & \textbf{46.1} \\
    Summer  Winter & 48.4  & \textbf{44.4} \\
    \midrule	
    Horse  Zebra & 89.7  & \textbf{53.0} \\
    Horse  Zebra & \textbf{110.5} & 112.3 \\
    \midrule	
    Van-Gogh  Picture  & 163.4  & \textbf{134.4} \\
    Van-Gogh  Picture  & \textbf{151.4} & 152.7 \\
    \midrule	
    Cezanne  Picture &\textbf{127.4}  & 138.8 \\
    Cezanne  Picture & \textbf{145.5}  & 147.6 \\
    \midrule	
    Monet  Picture & \textbf{60.3}  & 70.3 \\
    Monet  Picture & \textbf{61.8}  & 82.1 \\
    \midrule	
    Apple  Orange & 88.9  & \textbf{83.2} \\
    Apple  Orange & 116.7  & \textbf{113.2} \\
    \bottomrule
    \end{tabular}\hspace{1cm}  \scalebox{0.96}{
  \small
  \newcommand{\cycleganim}[1]{\includegraphics[width=0.15\linewidth]{images/CyCleGAN_Visual/#1}}
  \begin{tabular}{ccccc}
  \toprule
&  horsezebra  & 
 summerwinter &
 Monetphoto &
 orangeapple\\ 
    \rotatebox{90}{Original image} &
    \cycleganim{Comparison/im_zebra_org_0.png} &
    \cycleganim{Comparison/im_winter_org_0.png} &
    \cycleganim{Comparison/im_monet_org_0.png} &
    \cycleganim{Comparison/im_apple_org_0.png}\\

    \midrule

    \rotatebox{90}{CycleGAN} &
    \cycleganim{Comparison/im_zebra_clg_change_crop_0.png} &
    \cycleganim{Comparison/im_winter_clg_change_crop_0.png}&
    \cycleganim{Comparison/im_monet_clg_change_crop_0.png}&
    \cycleganim{Comparison/im_apple_clg_change_crop_0.png}\\

    \midrule
    \rotatebox{90}{Powers of layers} &
    \cycleganim{Comparison/im_zebra_change_Human_crop_0.png} &
    \cycleganim{Comparison/im_winter_change_Human_crop_0.png}&
    \cycleganim{Comparison/im_monet_change_Human_crop_0.png}&
    \cycleganim{Comparison/im_apple_change_Human_crop_0.png}\\
  \end{tabular}
  }
     }
    \caption{\label{tab:cycleganfid}
     Left: Frechet Inception Distance (FID) obtained on image-to-image translation tasks (lower is better).
     We compare CycleGAN and our Powers of layers method.
     Right: example results.
    }
    \end{figure*}
 \begin{figure*}
\centering

\newcommand{\imHR}[1]{\includegraphics[width=0.25\linewidth]{images/High_Resolution/#1}}
\newcommand{\drawat}[3]{\makebox[0pt][l]{\raisebox{#2}{\hspace*{#1}#3}}}
\newcommand{\imHRsub}[1]{\drawat{-35mm}{0mm}{\fcolorbox{white}{white}{\includegraphics[width=0.1\linewidth]{images/High_Resolution/#1}}
}}

\scalebox{0.86}{
\small
\begin{tabular}{@{}ccc@{\hspace{0.7cm}}|}
 Photo  Van Gogh & Photo  Monet & Horse  Zebra \\
 \imHR{VanGogh/im_apple_change_Human_crop_12.jpg}\imHRsub{VanGogh/im_apple_change_Human_crop_12_closeup.png} &
 \imHR{Monet/im_monet_change_Human_crop_20.jpg}
   &\imHR{Horse/im_Horse_HD_4.jpg}\\
  \imHR{VanGogh/im_apple_change_Human_crop_16.jpg}
  &\imHR{Monet/im_monet_change_Human_crop_18.jpg} 
  &\imHR{Horse/im_Horse_HD_2.jpg}\\
\end{tabular}
\hspace{0.4cm}\centering
\small
\scalebox{0.8}{
\begin{tabular}{c}
Trained at   \\
 \includegraphics[width=0.3\linewidth]{images/High_Resolution/im_0.png}\\
  Trained at \\
 \includegraphics[width=0.3\linewidth]{images/High_Resolution/im_notHR_0.png}\\
\end{tabular}
}
 }
\caption{\label{fig:highres}
{\em Left:} Different visual results with high resolution images.
See the original images in Appendix \ref{app:visu}.
{\em Right:} Comparisons of the generations obtained by models trained either with high-resolution or low-resolution images, applied to a high-resolution test image.
}
\end{figure*}
 
\paragraph{High resolution experiments.}

PoL is fully convolutional architecture, therefore it is technically possible to apply models trained in a low resolution to high resolution images.
However, the results are not always convincing, as shown in  Figure~\ref{fig:highres} (right) where the model trained on low resolution images does not create stripes at the ``right'' scale on zebras.
To circumvent this problem, CycleGAN trains on patches taken from high resolution images.
This works for transformations affecting the whole image (paintingphoto), but this is not applicable in the case where only a part of the image is affected (horsezebra).
In contrast, our proposal can adapt the memory used by changing its number of compositions, so we can apply it to very large images without running out of memory.
Figure~\ref{fig:highres} (left) depicts results obtained with our method trained on high resolution images.

\paragraph{Combining transformation.} 

The different blocks associated with different transformations operate in the same embedding space for different tasks. Hence we can compose transformations, each being realized by one residual block.
We train Transform~\#1 in the usual way, then freeze its residual block.
Transform~\#2 is trained on the output of \#1. 
Visual results are in Figure~\ref{fig:composition_embedding_space}.
The composition in the embedding space gives better results than decoding/encoding to image space mid-way. 

\begin{figure*}
\vspace{-0.2em}
\centering
\scalebox{1}{
\small
\begin{tabular}{@{}c@{\hspace{20pt}}|@{\hspace{20pt}}c@{\hspace{10pt}}c@{\hspace{20pt}}|@{\hspace{20pt}}c@{\hspace{10pt}}c@{}}
  \multicolumn{3}{c}{\hspace{50pt} Composition in embedding space}  &\multicolumn{2}{c}{Composition in image space} \\
 Original & +denoising & +to zebra & +denoising & +to zebra \\
\includegraphics[width=0.125\linewidth]{images/Composition_transformations/orig.png}&
\includegraphics[width=0.125\linewidth]{images/Composition_transformations/denoise.png}
 &\includegraphics[width=0.125\linewidth]{images/Composition_transformations/denoise_zebra.png}&
 
 \includegraphics[width=0.125\linewidth]{images/Composition_transformations/image_denoise_29.png}
 &\includegraphics[width=0.125\linewidth]{images/Composition_transformations/image_denoise_zebra_29.png}\\
 
    \includegraphics[width=0.125\linewidth]{images/Composition_transformations/orig13.png}&
 \includegraphics[width=0.125\linewidth]{images/Composition_transformations/denoise13.png}
   &\includegraphics[width=0.125\linewidth]{images/Composition_transformations/denoise_zebra13.png}&
   
    \includegraphics[width=0.125\linewidth]{images/Composition_transformations/image_denoise_13.png}
   &\includegraphics[width=0.125\linewidth]{images/Composition_transformations/image_denoise_zebra_13.png}\\
\end{tabular}
}
\vspace{-5pt}
\caption{\label{fig:composition_embedding_space}
Composition of transformations in the embedding/image space.
}
\vspace{-0.6em}
\end{figure*}
 


 
\section{Conclusion}

Powers of layers consists in iterating a residual block to learns a complex transformation with no direct supervision. 
On various tasks, power of layers gives similar performance to CycleGAN with fewer parameters. 
The flexibility offered by the common embedding space can be used to modulate the strength of a transformation or to compose several transformations.
While in most examples the discriminator is only used for training, 
Powers of layers can also exploit it to adjust the transformation to the input image at inference time. 


\clearpage
\bibliographystyle{plain}
\bibliography{egbib}
\clearpage \newpage \appendix\newpage

\begin{center}
{\LARGE
Supplementary material for \\
``Powers of layers for image-to-image translation'' 
\vspace{1cm} \newline 
}

\textbf{Hugo Touvron, Matthijs Douze, Matthieu Cord, Herv\'e J\'egou \vspace{1em}}
\end{center}
\vspace*{1cm}

In this supplementary material we report additional analyses, results and examples that complement our paper. 
In Appendix~\ref{app:training} we consider the training phase, which supports the importance of our progressive training strategy compared to one with a fixed number of iterations. 
Appendix~\ref{app:inference} considers the inference-time choices, in particular possible strategies to select the number of iterations. 
Appendix~\ref{app:cyclegan} provides additional comparison to CycleGAN and makes a comparison with the Fader network. 
Finally we present additional visual results for high resolution images and illustrate the progressive evaluation of results along iterations in the appendix~\ref{app:visu}. 

\section{Analysis of our progressive training strategy}
\label{app:training}

\begin{figure}[b]
\centering
\includegraphics[width=.8\linewidth]{images/analyse_noise/illustration_progressive_Trasining.pdf}

\caption{\label{fig:illustration_training}
  Number of compositions at training time for different training strategies.
  The number of compositions is adjusted per batch. 
}
\end{figure} 

Figure~\ref{fig:illustration_training} shows the different training strategies we explore. 
The degree of freedom that we can adjust is the number of compositions. 
It can be set independently per training mini-batch.

\paragraph{Progressive training versus fixed training.}

Figure~\ref{fig:schem_continuous_not_continuous} 
illustrates the modulation of the horsezebra transformation.
This is effective only with our progressive learning, which forces the network to produce acceptable intermediate states. 

If the network is trained with a fixed number of compositions, the intermediate states do not correspond to modulations of the transformation.
The output is satisfactory only when the . In all other cases we observe artifacts in images, which therefore do not qualify as natural images. 
The generated images do not look right, and the source and target discriminators would not accept them as real images.

In contrast, with our progressive training, each number of iterations  produces a satisfactory output. Iterating the residual block gradually transforms the horse into a zebra. 

\paragraph{Warm-up phase.}
Figure~\ref{fig:PSNR_training_fix} compares the performance obtained during the first three epochs of learning with and without progressive training, with different maximum number of compositions . 
We compare this way of stabilizing the training with another classical approach: reducing the ranges of initialisation of the residual blocks. 
Figure~\ref{fig:PSNR_training_fix} shows that changing the initialization improves the performance during the first epoch, but that a warm-up phase with progressive training is more effective to improve the optimization stability. 
Figure~\ref{fig:illustration_training} shows the evolution of the number of compositions for different training strategies.

\begin{figure}[b]
\centering
\scalebox{0.85}
{
\small
\newcommand{\contHZ}[1]{\includegraphics[width=0.117\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_#1.png}}
\newcommand{\noncontHZ}[1]{\includegraphics[width=0.117\linewidth]{images/not_continuous/Horse/im_not_prog_Horse_#1.png}}

\fboxrule=1pt
\fboxsep=0.1mm

\def \mysp {\hspace{2pt}}
\hspace{-20pt}
\begin{tabular}{lc@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}}
\multirow{2}{*}{
\rotatebox{90}{
\begin{minipage}[c]{1.5cm}
\centering \small Progressive \newline training
\end{minipage}
}}
& 
0& 5 & 10 &15 & 20 & 25 & 30 & 35 &40\\
& 
\contHZ{0} & 
\contHZ{5} & 
\contHZ{10} & 
\contHZ{15} & 
\contHZ{20} & 
\contHZ{25} & 
\fcolorbox{red}{red}{\contHZ{30}} & 
\contHZ{35} & 
\contHZ{40} \\
\multirow{2}{*}{
\rotatebox{90}{
\begin{minipage}[c]{1.5cm}
\centering \small fixed \newline training
\end{minipage}
}}
& 0& 5 & 10 &15 & 20 & 25 & 30 & 35 &40\\
 & 
\noncontHZ{0} & 
\noncontHZ{5} & 
\noncontHZ{10} & 
\noncontHZ{15} & 
\noncontHZ{20} & 
\noncontHZ{25} & 
\fcolorbox{red}{red}{\noncontHZ{30}}& 
\noncontHZ{35} & 
\noncontHZ{40} \\
\end{tabular}
}
\caption{\label{fig:schem_continuous_not_continuous}
Comparison between our progressive training approach and a non progressive approach. 
We represent the images obtained by varying the number of iterations at inference time  in the network that transforms from domain A (horse) into domain B (zebra).
The first image (=0) is the original image. 
Since our method was learned with =30 compositions the last two images are extrapolations. 
Our progressive training is key to ensure that all outputs look like natural images and therefore that we can modulate transformation strength at inference time.}
\end{figure}
 \definecolor{amber}{rgb}{1.0, 0.75, 0.0}
\definecolor{darkgreen}{rgb}{0, 0.5, 0.0}

\begin{figure*}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=\linewidth]{images/analyse_noise/curve_progressive_Training_4.pdf}
\end{minipage}
\hfill
\begin{minipage}{0.4\linewidth}
\caption{\label{fig:PSNR_training_fix}
    Difference in PSNR between fixed learning and progressive learning during the first training epochs, evaluated on a denoising task. \newline
    \textcolor{darkgreen}{Green}: =30 \newline
    \textcolor{amber}{orange}: =16 \newline
    \textcolor{red}{red}: =4
}
\end{minipage}
\end{figure*}
 
\paragraph{Block composition or independent successive blocks?}
Table~\ref{tab:psnr_urban100_sharing} provides results that complement Table~\ref{tab:psnr_urban100_sharing_max_rep} in the main paper. It compares the performance with (1) the composition of the same block or (2) using independent residual blocks. In particular, we report standard deviations that assess the statistical significance of our improvement. 

\begin{table}
\caption{\label{tab:psnr_urban100_sharing}
PSNR on Urban-100. 
Comparison between our choice (PoL) and independent blocks. 
}
\vspace{0.5em}
\centering
{
\small
\begin{tabular}{|c|cc|cc|}
  \toprule
   Number & \multicolumn{2}{c|}{Gaussian noise (std 30)}& \multicolumn{2}{c|}{Gaussian blur (sigma 4)}\\
     of blocks & POL &  independent & POL &  independent   \\
    \midrule
    1 &  23.26~\stdminus{0.15} & 23.26~\stdminus{0.15} &  18.61~\stdminus{0.13} & 18.61~\stdminus{0.13} \\
    2 &  23.28~\stdminus{0.07} & 23.21~\stdminus{0.05}&  18.48~\stdminus{0.25} & 18.64~\stdminus{0.21}\\
    4 &  24.41~\stdminus{0.27} &  23.16~\stdminus{0.09}&  19.19~\stdminus{0.04} & 19.17~\stdminus{0.51}\\
    8 &  23.91~\stdminus{0.23} & 22.27~\stdminus{0.02} &  18.95~\stdminus{0.14} & 19.33~\stdminus{0.29}\\
    12 &  23.91~\stdminus{0.10} & 22.48~\stdminus{0.32} &  19.70~\stdminus{0.21} & 18.76~\stdminus{0.09}\\
    16 &  23.88~\stdminus{0.14} & 22.54~\stdminus{0.48} &  19.00~\stdminus{0.30} & 18.11~\stdminus{0.41}\\
    \bottomrule
\end{tabular}
}
\end{table} 
\paragraph{Choice of the maximum number of compositions.}

Table~\ref{tab:fusion_niqe_psnr_urban100_maxrep} compares the PSNR obtained for denoising and deblurring and NIQE for deblocking task on Urban-100~\cite{Huang2015Urban100}. 
We report more results and standard deviations compared to the main paper. 
Note that these tasks work well with a relatively low maximum of iterations, in contrast to style transfer image-to-image translations, which require more complex functions. 

\begin{table*}
\caption{\label{tab:fusion_niqe_psnr_urban100_maxrep}
Comparison between different maximum number of compositions. 
We report the most adapted metric on Urban-100: PSNR for the Gaussian noise and blur, NIQE for  deblocking. 
}
\centering
{
\small
\begin{tabular}{|c|c|c|c|}
  \toprule
    &\multicolumn{2}{c|}{PSNR (higher=better)} & \multicolumn{1}{c|}{NIQE (lower=better)}\\
      & Gaussian noise (std 30) & Gaussian blur (=4) &  JPEG (quality=25)  \\
    \midrule
    1 &23.26 \stdminus{0.15} &  18.61 \stdminus{0.13}  &  10.17\stdminus{0.55} \\
    2  & 23.28 \stdminus{0.07} &  18.48 \stdminus{0.25} &  10.78 \stdminus{0.39}  \\
    3 &  23.89 \stdminus{0.07} &  19.13 \stdminus{0.08} &  10.57 \stdminus{0.21}  \\
    4 &  \textbf{24.41 \stdminus{0.27}} &  19.19 \stdminus{0.04}   &  10.43 \stdminus{0.25}  \\
    5 &  23.79 \stdminus{0.59} &  19.06 \stdminus{0.21} &  10.65 \stdminus{0.52}  \\
    6 &  23.80  \stdminus{0.31} &  19.09 \stdminus{0.08} & 10.42 \stdminus{0.19}  \\
    7 &  23.64 \stdminus{0.27} &  19.12 \stdminus{0.13}  &  10.93 \stdminus{0.61}  \\
    8 &  23.91 \stdminus{0.23} &  18.95 \stdminus{0.14}   &  10.34 \stdminus{0.61}  \\
    12 &  23.91 \stdminus{0.10} &  \textbf{19.70~\stdminus{0.21}}  &  9.74 \stdminus{0.41}  \\
    16 &  23.88 \stdminus{0.14} &  19.00~\stdminus{0.30}  &  8.51 \stdminus{0.19}  \\
    17 &  23.97 \stdminus{0.17} &  18.83 \stdminus{0.21}   &  8.36 \stdminus{0.32}   \\
    18 &  24.17 \stdminus{0.18} &  18.97 \stdminus{0.13}  &  7.49 \stdminus{0.63}   \\
    24  &  23.83 \stdminus{0.20} &  18.56 \stdminus{0.16}  &  \textbf{7.22 \stdminus{0.45}}  \\
    27 &  23.62 \stdminus{0.02} &  18.48 \stdminus{0.36}  & 7.25 \stdminus{0.36}  \\
    30 &   23.45 \stdminus{0.10} & 19.09 \stdminus{0.31}  &  8.15 \stdminus{0.64} \\
    \bottomrule
\end{tabular}
}
\end{table*}
 
\paragraph{Composition step.} 
Table~\ref{tab:psn_urban100_step} compares different choices for the number of steps of augmentation associated with the number of compositions . 
As we can see, taking too large steps tends to affect performance,
it is better to ramp up the number of compositions quickly during the warm-up phase.
\begin{table*}
\caption{\label{tab:psn_urban100_step}
PSNR on Urban-100~\cite{Huang2015Urban100} -- Gaussian noise (std=30).
Comparison between augmentation steps during the warm-up phase.
The augmentation step is to the number of epochs performed with the same number of compositions (1 epoch corresponds to 800 backward passes). 
}
\smallskip 
\centering
\scalebox{1.0}{
\small
\begin{tabular}{|c|c|c|c|c|c|}
    \toprule
      & \multicolumn{5}{c|}{augmentation step}\\
       &1 & 2 & 4 & 8 & 16 \\
    \midrule
       4 & 24.41 \stdminus{0.27}     & 24.44 \stdminus{0.11}   &  23.62  \stdminus{0.16}   &  23.93\stdminus{0.10}   &   23.22 \stdminus{0.07}\\
     \midrule
       16 & 23.88 \stdminus{0.14} & 23.97  \stdminus{0.22} &  23.57 \stdminus{0.06} &  23.52 \stdminus{0.1}     & 23.19 \stdminus{0.02}  \\
    \midrule
       30  &  23.45 \stdminus{0.10}   &  23.77 \stdminus{0.14} & 23.95 \stdminus{0.17} & 23.14 \stdminus{0.04}  &23.17 \stdminus{0.04}   \\
    \bottomrule
\end{tabular}
}
\vspace{-1em}
\end{table*} 

\paragraph{Comparison between randomised and fixed number of compositions~.} 
Figure~\ref{fig:ada_psnr} compares the trajectories of PSNRs as a function of the number of Powers-of-layers composition.
We get a better average performance if we randomly draw the maximum number of composition.

The different positions of the maxima in the adaptive case also suggests that it is necessary to adjust the amount of transformation to each image, as discussed below. 


\begin{figure}

\centering
\small
\vspace{1em}
\begin{tabular}{c@{\hspace{0.75em}}c}
  \toprule
     fixed  & Random  \\
     \midrule
     \includegraphics[width=0.48\linewidth]{images/Fig_randomised/curve_denoising_no_random_psnr.pdf}&
     \includegraphics[width=0.48\linewidth]{images/Fig_randomised/curve_denoising_no_random_psnr_more_Noise_30_rdm.pdf} \\
    \bottomrule
\end{tabular}
\caption{\label{fig:ada_psnr}
 Evolution of the average PSNR as well as for different individual images according to the number of compositions .
 The maximum number of compositions used for training is  and the Gaussian Noise standard deviation is 30.
}
\end{figure} 
\section{Analysis of choices at inference time}
\label{app:inference}

\paragraph{Adjusting  at inference time.}
Each image is more or less distant from the target domain, so we explore adapting the transformation to each image rather than applying a fixed transformation. 
For example, depending on the amount of noise, we may want to adjust the strength of the denoising. 
By modulating  we can adapt the transformation to each image.
Figure~\ref{fig:noise_complexity} shows that the more noisy the input image is, the more we should compose to best denoise with Powers-of-Layers. 

\begin{figure*}
    \centering
    \includegraphics[width=.7\linewidth]{images/Noise_complexity/curve_complexity_noise_2.pdf}
    \caption{\label{fig:noise_complexity}
    Evolution of the optimal number of composition according to the noise. 
    The maximum number of compositions used for training is 30 and the Gaussian Noise standard deviation is 30. 
    }
\end{figure*}
 

\paragraph{Stopping criterion: fixed, discriminator, versus an Oracle.} 
At inference time the number of composition  applied to each image can be set using several strategies.
We can choose to apply a constant number of composition or use the discriminator to choose the  for which it gets the best response.

Table~\ref{tab:rand_comp} compares the performance of two strategies at test time for different random ranges at training time, and compare it to the upper bound achieved by an Oracle (i.e,. the performance attained when the optimal number of iteration is known for each image). 

With 30, we have chosen different ranges of the form  for .
The optimal range for debluring and denoising is with 


\begin{table*}
\caption{\label{tab:rand_comp}
Effect of setting the number of compositions  randomly on the PSNR on Urban-100 with two types of noise. 
We compare  
(Constant) a fixed 
and (Adaptive) value  maximizing the target discriminator error for each image. 
Oracle:  minimizing PSNR for each image.}
\smallskip
\centering
{
\small
\begin{tabular}{|c|ccc|ccc|}
  \toprule
     & \multicolumn{3}{c}{Gaussian Noise (std=30)} & \multicolumn{3}{c|}{Gaussian Blur (=4)} \\
     Random range  & Constant & Adaptive & Oracle & Constant & Adaptive & Oracle \\
    \midrule	
     & 21.41\stdminus{3.33} & 22.11 \stdminus{0.56} & 23.86 \stdminus{0.27} &16.95\stdminus{1.29} & 16.16 \stdminus{1.33} & 19.43 \stdminus{0.09}\\
     & 22.56\stdminus{1.94}  & 22.84\stdminus{0.67} &  23.73 \stdminus{0.98} & 16.39\stdminus{2.00}  & 15.42\stdminus{2.26} &  19.58 \stdminus{0.14}\\
     & 21.99\stdminus{1.20} & 23.14\stdminus{0.21} & 23.42\stdminus{0.32} &17.48\stdminus{1.56} & 18.10\stdminus{0.70} & \textbf{19.65}\stdminus{0.10}\\
     & 21.41\stdminus{1.71}  & 23.47\stdminus{0.69} &  23.77 \stdminus{0.61} &18.14\stdminus{0.54}  & 18.81\stdminus{0.61} &  19.42\stdminus{0.07}\\
     & 21.53\stdminus{1.90}  & \textbf{23.68\stdminus{0.17}} &   \textbf{23.99 \stdminus{0.06}} &18.58\stdminus{0.10}  &  \textbf{19.22} \stdminus{0.37} &   19.56 \stdminus{0.09} \\
     & 21.16\stdminus{0.99}  & 23.08\stdminus{0.37} &  23.42 \stdminus{0.28} &17.55\stdminus{1.32}  & 19.06\stdminus{0.39} &  19.52 \stdminus{0.05} \\
     & \textbf{23.45\stdminus{0.10}}  & 23.46\stdminus{0.40} &  23.81 \stdminus{0.48} &\textbf{19.09} \stdminus{0.31}  & 18.87\stdminus{0.41} &  19.49 \stdminus{0.08} \\
\bottomrule
\end{tabular}
}
\end{table*}
  
\section{Additional comparisons with CycleGAN and the Fader Network} 
\label{app:cyclegan}

Table~\ref{tab:psnr_urban100_comp_1} compares the results obtained with PoL and CycleGAN for different noise levels.
Table~\ref{tab:data_comp} compares the results obtained with PoL and CycleGAN for different amounts of data.
These numbers are the same as Figure~\ref{tab:psnr_urban100_comp}, with standard deviations.
In most cases, whether with different amounts of data or different noise, our method is better than CycleGAN.
This is mainly due to its smaller number of parameters and the flexibility brought by the adaptive criterion.

\begin{table*}
\caption{\label{tab:psnr_urban100_comp_1}
Comparison between our approach (PoL) and CycleGAN. 
We three tasks, all computed with the Urban-100 dataset: 
PSNR (higher is better) with different amount of Gaussian noise and Gaussian blur, and 
NIQE (lower is better) measured for different JPEG compression quality.
}
\smallskip
\centering
\scalebox{0.73}{
\small
\begin{tabular}{@{}|c|ccc|c|ccc|c|ccc|}
  \toprule
     Noise &  Noisy & \multicolumn{2}{c}{Denoising}  & Sigma & Blur & \multicolumn{2 }{c|}{Debluring}& JPEG & JPEG &\multicolumn{2}{c|}{Deblocking}\\
        (std) &   images & CycleGAN & PoL &   Blur &  images & CycleGAN & PoL&   quality &  images & CycleGAN & PoL\\
    \midrule	
    15 & 24.9& 22.37\stdminus{0.19}  &\textbf{27.37} \stdminus{0.26} &
    2 & 21.58 & 20.37 \stdminus{0.26} & \textbf{22.14}  \stdminus{0.21}&
    15& 9.01  & 7.89\stdminus{1.14}  &7.90\stdminus{0.32}\\
    30 & 19.2   & 21.93\stdminus{0.04} & \textbf{23.68} \stdminus{0.17} & 
    4 & 19.20  & 18.55 \stdminus{0.37} & \textbf{19.22} \stdminus{0.37}&
    25 &  8.94 & 7.45\stdminus{0.63}  & \textbf{7.10}\stdminus{0.58} \\
    50 & 15.2  & 21.57\stdminus{0.04} & \textbf{ 22.52} \stdminus{0.37} &
    8 &  17.48  & 16.09 \stdminus{0.14} & \textbf{17.53} \stdminus{0.34}&
    30 & 8.90  & 7.46\stdminus{0.32}  & \textbf{6.76}\stdminus{0.83}\\
    70 & 12.7  & \textbf{21.02}\stdminus{0.17}& 20.94 \stdminus{0.24} &
    16 & 16.13  & 16.11 \stdminus{0.31} & \textbf{16.16} \stdminus{0.14} &
    50 & 8.99  & 6.96\stdminus{0.55}  &   \textbf{6.56}\stdminus{0.48}\\
    100 & 10.4  & \textbf{20.00}\stdminus{0.12} & 19.42\stdminus{0.22} & 
    24 & 15.50  & 12.88 \stdminus{0.24} & \textbf{15.48} \stdminus{0.07}&
    70 & 8.94 & 7.11\stdminus{0.21}  &   \textbf{6.81}\stdminus{0.39}\\
\bottomrule
\end{tabular}
}
\vspace{-1em}
\end{table*}
 
\begin{table*}
\caption{\label{tab:data_comp}
Comparison between CycleGAN and Power of layer on Urban-100~\cite{Huang2015Urban100} with different amount of training data. We use PSNR to compare methods for Gaussian noise and Gaussian blur.
}
\smallskip
\centering
\small
\begin{tabular}{|c|cc|cc|}
  \toprule
   Number of    data & \multicolumn{2}{c|}{Denoising (std=30)} & \multicolumn{2}{c|}{Debluring (sigma=4)}\\
      training images & CycleGAN  & PoL & CycleGAN  & PoL  \\
    \midrule	
    1 & 
    14.50\stdminus{0.24} & 
    \textbf{22.27\stdminus{0.16}} & 
    14.36\stdminus{0.12} & 
     \textbf{18.68 \stdminus{0.27}}\\
    5 &
    16.57\stdminus{0.04} & 
     \textbf{23.13\stdminus{0.18}} &
    16.72\stdminus{0.15} & 
     \textbf{18.76 \stdminus{0.23}} \\
    10 & 
    20.88\stdminus{0.36}& 
     \textbf{23.19\stdminus{0.58}} & 
    17.03 \stdminus{0.07} & 
     \textbf{18.82 \stdminus{0.56}}\\
    100 & 
    21.03\stdminus{0.76}  & 
     \textbf{23.39\stdminus{0.33}}&
    18.17 \stdminus{0.15} & 
     \textbf{18.97 \stdminus{0.23}}\\
    400 & 
    21.78\stdminus{0.12} & 
     \textbf{23.60 \stdminus{0.32}} & 
    18.21\stdminus{0.11} & 
     \textbf{19.01 \stdminus{0.31}} \\ 
    800 & 
    21.93\stdminus{0.04} &
     \textbf{23.88 \stdminus{0.14}} & 
    18.55 \stdminus{0.37} & 
     \textbf{19.22 \stdminus{0.37}} \\
\bottomrule
\end{tabular}
\end{table*}

 

\paragraph{Experiments on transformation  adjustment}

As baseline  we use the Fader network~\cite{lample2017fader} for transformation adjustment.
The Fader Network is a neural network composed of an encoder and a decoder, 
for which it it is possible to modulate a transformation. This is done by removing the factors of variations related to this transformation in the latent space resulting from the encoder, and in turn by choosing the factors to be added to the embedding going into the decoder.


To interpolate between domain  and domain , the Fader network has a latent representation where the attributes relative to each domain have been disentangled. 
The  Fader network has been applied to faces, for instance to add glasses on a face, to age a person, etc.
We observe experimentally with smaller datasets, where the variability from one image to another is larger than with faces, that the Fader's results are not as good. 

In contrast, our approach, like CycleGAN, does not have limitations incurred by a latent space disentanglement because it exploits a cyclic loss.

Figure~\ref{fig:fader_horse} shows the results obtained with the Fader network and with our method on the HorseZebra transformation adjustment.
The Fader network is unable to significantly transform the source when the network is too shallow (3 layers) and destroys the image when it is deep (6 layers). 
In contrast, Powers-of-layers convincingly hybridizes a horse and a zebra.
In terms of FID for the Horse to Zebra task, the Fader network is significantly worse: it obtains a FID greater than 163.0 in the both case against 53.0 for our method (lower is better).

\begin{figure*}
\centering
\newcommand{\fadercmp}[1]{\includegraphics[width=0.13\linewidth]{images/Fader/#1}}
\centering
\small
\begin{tabular}{c|c|ccccc}
 \multicolumn{2}{c}{} &\multicolumn{5}{c}{\large Transformation rate} \\
 Original & & 0\%  & 25\%  & 50\% & 75\% & 100\%  \\
 \midrule
 \fadercmp{small/org_images_0.png} &
 \rotatebox{90}{Fader,  3 layers} &
  \fadercmp{small/fader24089005_images_0_0.png} & \fadercmp{small/fader24089005_images_0_2.png} & \fadercmp{small/fader24089005_images_0_5.png} & \fadercmp{small/fader24089005_images_0_7.png} & \fadercmp{small/fader24089005_images_0_10.png}\\
 
 
 \fadercmp{small/org_images_0.png} &
 \rotatebox{90}{Fader, 6 layers} &
  \fadercmp{Deeper/fader24010842_images_0_0.png} & \fadercmp{Deeper/fader24010842_images_0_2.png} & \fadercmp{Deeper/fader24010842_images_0_5.png} & \fadercmp{Deeper/fader24010842_images_0_7.png} & \fadercmp{Deeper/fader24010842_images_0_10.png}\\
 
 \midrule

  \fadercmp{small/org_images_0.png} &
   \rotatebox{90}{PoL} &
 \fadercmp{Our_Method/our_Method_True_images_0_0.png} & \fadercmp{Our_Method/our_Method_True_images_0_7.png} & \fadercmp{Our_Method/our_Method_True_images_0_15.png} & \fadercmp{Our_Method/our_Method_True_images_0_22.png} & \fadercmp{Our_Method/our_Method_True_images_0_30.png}\\
 
\end{tabular}
\caption{\label{fig:fader_horse}
Visual comparison between Fader networks~\cite{lample2017fader} and our power-of-layers on the task Horse to Zebra. 
}
\vspace{-1em}
\end{figure*} 
\section{Additional results: visualizations of transform modulation and high resolution}
\label{app:visu}

\paragraph{Progressive results}
Figure~\ref{tab:continuousappendix} shows the progressive transformations obtained on different tasks. 
It shows that progressive transformations are realistic for most tasks.

\def \mysp {\hspace{5.4pt}}

\begin{figure*}
\centering
\scalebox{0.75}{
\small
\begin{tabular}{c|@{}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{\mysp}c@{}}
Domain / Composition  & 0 & 5 & 10 & 15 & 20 & 25 & 30 & 35 & 40\\
 Horse  Zebra & \includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_0.png} & \includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_5.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_10.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_15.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_20.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_25.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_30.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_35.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Horse_Zebra2/im_Zebra_Horse_40.png} \\

Zebra  Horse &\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_0.png} & \includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_5.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_10.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_15.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_20.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_25.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_30.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_35.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Zebra_Horse2/im_Zebra_Horse_40.png} \\

Summer  Winter &\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_0.png} & \includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_5.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_10.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_15.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_20.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_25.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_30.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_35.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Summer_Winter2/im_summer_winter_40.png} \\

Winter  Summer &\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_0.png} & \includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_5.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_10.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_15.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_20.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_25.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_30.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_35.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Winter_Summer2/im_winter_summer_40.png} \\


Photo  Monet &\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_0.png} & \includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_5.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_10.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_15.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_20.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_25.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_30.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_35.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Photo_Monet2/im_photo_monet_40.png} \\

Monet  photo & \includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_0.png} & \includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_5.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_10.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_15.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_20.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_25.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_30.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_35.png} &
\includegraphics[width=0.11\linewidth]{images/continuous/Monet_Photo/im_monet_photo_40.png} \\
\end{tabular}
}
\caption{\label{tab:continuousappendix}
Illustration of the different results obtained along the iterations of the recurrent block, during the transformation of a horse into a zebra.
The first image is the original image, then each image corresponds to 5 additional compositions of our method. Since our method was learned with a maximum of 30 compositions the last two images are extrapolations.
}
\vspace{-1em}
\end{figure*}
 
\paragraph{Results in high resolution.}
Figure~\ref{fig:highres_app} shows the high-resolution results of the section~\ref{sec:experiments}, along with the original images. 


\begin{figure*}
\centering
\small
\scalebox{0.9}{
\begin{tabular}{ccc}
\multicolumn{3}{c}{Powers-of-Layers Transformations}\\
 Photo  Van Gogh & Photo  Monet  & Horse  Zebra \\
 \includegraphics[width=0.35\linewidth]{images/High_Resolution/VanGogh/im_apple_change_Human_crop_12.jpg}&
 \includegraphics[width=0.35\linewidth]{images/High_Resolution/Monet/im_monet_change_Human_crop_20.jpg} &
   \includegraphics[width=0.35\linewidth]{images/High_Resolution/Horse/im_Horse_HD_4.jpg}\\
  \includegraphics[width=0.35\linewidth]{images/High_Resolution/VanGogh/im_apple_change_Human_crop_16.jpg}
  &\includegraphics[width=0.35\linewidth]{images/High_Resolution/Monet/im_monet_change_Human_crop_18.jpg} &
  \includegraphics[width=0.35\linewidth]{images/High_Resolution/Horse/im_Horse_HD_2.jpg}\0.1cm]
  \multicolumn{3}{c}{Original images}\\
   \includegraphics[width=0.35\linewidth]{images/High_Resolution/VanGogh/im_apple_org_12.jpg}&
 \includegraphics[width=0.35\linewidth]{images/High_Resolution/Monet/im_monet_org_20.jpg}&
  
   \includegraphics[width=0.35\linewidth]{images/High_Resolution/Horse/im_Horse_org_HD_4.jpg}\\
  \includegraphics[width=0.35\linewidth]{images/High_Resolution/VanGogh/im_apple_org_16.jpg}
  &\includegraphics[width=0.35\linewidth]{images/High_Resolution/Monet/im_monet_org_18.jpg}& 
  \includegraphics[width=0.35\linewidth]{images/High_Resolution/Horse/im_Horse_org_HD_2.jpg}\\
\end{tabular}
}
\caption{\label{fig:highres_app}
{\it Top:} Differents visuals results with high resolution image. {\it Bottom:} Original images}
\end{figure*}
 
 
\end{document}