

\documentclass[final]{cvpr}

\renewcommand{\rmdefault}{ptm}
\renewcommand{\sfdefault}{phv}



\usepackage{cite}




\usepackage[margin=6pt,font=small,labelfont=bf,labelsep=endash,tableposition=top]{caption}


\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm,nicefrac}
\usepackage{makecell, verbatim, sidecap}
\usepackage[position=bottom]{subfig}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\Real{{\mathbb R}}
\def\real{{\mathbb R}}
\def\R{{\mathbb R}}
\def\max{{\rm max}}
\def\Proj{{\rm Proj}}
\def\Loss{{\rm Loss}}
\def\BCE{{\rm BCE}}
\def\Sim{{\rm Sim}}

\usepackage{multirow, xspace}


\renewcommand{\vec}[1]{\ensuremath{\pmb{#1}}}
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\set}[1]{\ensuremath{\mathscr{#1}}}

\makeatletter
\@tfor\next:=abcdefghijklmnopqrstuvwxyxz\do
{\begingroup\edef\x{\endgroup
        \noexpand\@namedef{v\next}{\noexpand\vec{\next}}}\x}
\@tfor\next:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\do
{\begingroup\edef\x{\endgroup
        \noexpand\@namedef{m\next}{\noexpand\mat{\next}}}\x}
\@tfor\next:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\do
{\begingroup\edef\x{\endgroup
        \noexpand\@namedef{s\next}{\noexpand\set{\next}}}\x}
\makeatother


\def\sG{\mathcal{G}}                               

\newcommand{\Transpose}{\ensuremath{{\!\top}} }    \let\T\Transpose


\newcommand{\opt}{\ensuremath{\textsc{Opt}}}

\newcommand{\norm}[1]{\ensuremath{\left\|#1 \right\|}}

\newcommand{\eyes}{\ensuremath{{\mathbf{I}}}}

\newcommand{\1}{{\mathbbm{1}}}                     





\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\def\Ours{{BoxInst}\xspace}



\begin{document}


\title{\Ours:
High-Performance Instance Segmentation with Box Annotations
}

\author{Zhi Tian, ~ ~ ~ Chunhua Shen\thanks{Corresponding author.},
~  ~ ~ Xinlong Wang, ~  ~ ~ Hao Chen
\
\begin{aligned}
    \Proj_x(\vb) = \vl_x,~~\Proj_y(\vb) = \vl_y,
\end{aligned}

\begin{aligned}
    \Proj_x(\vb) = \max_y(\vb) = \vl_x, \\
    \Proj_y(\vb) = \max_x(\vb) = \vl_y,
\end{aligned}
\label{eq:proj}

\begin{aligned}
    L_{proj} &= L(\Proj_x(\tilde{\vm}), \Proj_x(\vb)) + L(\Proj_y(\tilde{\vm}), \Proj_y(\vb)) \\
    &= L(\max_y(\tilde{\vm}), \max_y(\vb)) + L(\max_x(\tilde{\vm}), \max_x(\vb)) \\
    &= L(\tilde\vl_x, \vl_x) + L(\tilde\vl_y, \vl_y),
\end{aligned}

\begin{aligned}
     P(y_e = 1) = \tilde{\vm}_{i, j}
     \cdot
     \tilde{\vm}_{k, l} + (1 - \tilde{\vm}_{i, j})
\cdot
     (1 - \tilde{\vm}_{k, l}),
\end{aligned}
\label{eq:pairwise_loss}
\begin{aligned}
L_{pairwise} = -\frac{1}{N}\sum_{e \in E_{in}}y_{e}\log P(y_e = 1) \\
 +
 (1 - y_{e})\log P(y_e = 0),
\end{aligned}
\label{eq:loss_mask}
\begin{aligned}
L_{mask} = L_{proj} + L_{pairwise}.
\end{aligned}
\label{eq:color_sim}
\begin{aligned}
S_e = S(\vc_{i, j}, \vc_{l, k}) = \exp\left(-\frac{||\vc_{i, j} - \vc_{l, k}||}{\theta}\right),
\end{aligned}
\label{eq:partial_pairwise_mask_loss}
\begin{aligned}
L_{pairwise} = -\frac{1}{N}\sum_{e \in E_{in}}\mathbbm{1}_{\{S_e \geq \tau\}}\log P(y_e = 1),
\end{aligned}

where  is the indicator function, being 1 if  and 0 otherwise. Eq.~(\ref{eq:partial_pairwise_mask_loss}) only involves the term in Eq.~(\ref{eq:pairwise_loss}) for positive edges because we can only infer that the label of  is 1 if ; but the label is agnostic if .

We compute the proportion of the positive edges in the edges with  with mask annotations on the COCO \texttt{val2017} split. Fig.~\ref{fig:color_sim_and_pos_edges} (blue curve) shows that the change of the proportion of the positive edges in the edges with  as the threshold  increases. As shown in figure, if the threshold is 0.1, more than  of the edges are positive. The proportion can be further improved if we continue to increase , but an overlarge threshold would reduce the number of the supervised edges (red curve in Fig.~\ref{fig:color_sim_and_pos_edges}). We need to trade off between them.

As we only have positive labels in Eq.~(\ref{eq:partial_pairwise_mask_loss}), one may note this would result in two possible trivial solutions, \ie, the masks of all the pixels being 0 or 1. However, the masks with all pixels being 0 do not meet the projection term; and the masks of all pixels being 1 almost never appear since the pairwise term encourages the pixels near the box boundaries to be negative if their colors are similar to that of the negative pixels outside the box.




\section{Experiments}
\begin{table*}
\setlength{\tabcolsep}{5pt}
\centering
\small
\subfloat[Varying the color similarity threshold . \label{table:color_sim_threshold}]{
    \begin{tabular}{ l|c|c|c|c c|c c c }
     & prop. & AP & AP & AP & AP & AP & AP \\
    \Xhline{2\arrayrulewidth}
fully-sup. & - & 35.4 & 55.9 & 37.6 & 17.0 & 38.8 & 50.7 \\
    \hline
     & 94.1\% & 9.4 & 30.3 & 3.3 & 7.6 & 10.3 & 11.4 \\
     & 98.3\% & \textbf{30.7} & 52.2 & \textbf{31.1} & 13.8 & \textbf{33.1} & \textbf{45.7} \\
     & \textbf{98.4\%} & 30.6 & \textbf{52.6} & 30.9 & \textbf{13.9} & 32.8 & 45.5 \\
\end{tabular}
}
\hspace{0.5em}
\subfloat[Varying the size and dilation of the local patches (with ). \label{table:varying_kernel}]{
    \begin{tabular}{ c|c|c|c c|c c c }
    size & dilation & AP & AP & AP & AP & AP & AP \\
    \Xhline{2\arrayrulewidth}
    3 & 1 & 29.7 & 52.0 & 29.6 & 13.4 & 32.3 & 44.4 \\
    3 & 2 & \textbf{30.7} & 52.2 & \textbf{31.1} & \textbf{13.8} & \textbf{33.1} & \textbf{45.7} \\
    5 & 1 & 30.5 & \textbf{52.3} & 30.7 & 13.7 & 33.0 & \textbf{45.7} \\
    5 & 2 & 29.9 & 51.9 & 30.0 & 13.8 & 32.1 & 45.0 \\
    \end{tabular}
}
\caption{\textbf{Ablation study on
the hyper-parameters} in the proposed mask loss on the COCO  \texttt{val2017} split. ``prop." is the proportion of the positive edges in the edges with . ``fully-sup.": fully-supervised results. As shown in Table~\ref{table:color_sim_threshold}, by using , \Ours\ can achieve 30.7 mask AP with only box annotations, which is close the fully-supervised mask AP (35.4\%) and significantly better localization precision than boxes (10.6\% mask AP as shown in Table~\ref{table:loss_terms}). Table~\ref{table:varying_kernel} shows the experiment results by varying the neighbours of each pixel.}

\end{table*}

\begin{table}
\centering
\small
\begin{tabular}{ l|c|c c|c c c }
mask loss & AP & AP & AP & AP & AP & AP \\
\Xhline{2\arrayrulewidth}
Dice loss & 35.6 & 56.3 & 37.8 & 16.9 & 38.9 & 51.0 \\
proposed & 35.4 & 55.9 & 37.6 & 17.0 & 38.8 & 50.7 \\
\end{tabular}
\caption{\textbf{The projection and pairwise affinity mask loss vs.\  the original pixelwise one} in the fully-supervised settings. As
we can see here,
they attain very
similar mask AP on the COCO split \texttt{val2017}.}
\label{table:pixel_vs_pair}
\end{table}

We conduct experiments on COCO~\cite{lin2014microsoft} and Pascal VOC~\cite{everingham2010Pascal}. For COCO, the models are trained with \texttt{train2017} (115K images) and the ablation experiments are evaluated on \texttt{val2017} (5K images). Unless specified, \textit{only box annotations} are used during training. Our main results are reported on \texttt{test}-\texttt{dev}. For Pascal VOC, following previous works~\cite{hsu2019weakly, khoreva2017simple}, we train the models on the augmented Pascal VOC 2012 dataset~\cite{hariharan2011semantic} with 10, 582 training images, and evaluate them on Pascal VOC 2012 \texttt{val} split with 1, 449 images.

\subsection{Implementation Details}
Our proposed method only requires to change the mask loss in CondInst. Other training and testing details are kept as similar as possible to the original CondInst. On COCO, unless specified, we use the following training details. The models are trained for 90K iterations with batch size 16 on 8 V100 GPUs (2 images per GPU). The initial learning rate is set to 0.01 and reduced by a factor of 10 at step 60K and 80K, respectively. ResNet-50~\cite{he2016deep} is used as the backbone and is initialized with the ImageNet~\cite{deng2009imagenet} pre-training weights. The newly added layers are initialized as in ~\cite{tian2020conditional}. We use exactly the same data augmentation as in CondInst (\eg, left-right flipping and multi-scale training). For the dynamic mask heads, we use 3 conv.\ layers as in CondInst, but we increase the channels of the mask heads from 8 to 16, which can result in better performance with negligible extra computational overheads, and the compared baselines are adjusted accordingly. Following CondInst, the output mask is up-sampled to  resolution of the input image. For the pairwise loss term, we compute the pairwise relationship within  patches with the dilation rate being 2. On Pascal VOC, following \cite{hsu2019weakly}, we use batch size 8 and the number of iterations is 20K. The learning rate is reduced by a factor of 10 at step 15K. Only left-right flipping is used as the data augmentation during training. Other settings are the same as on COCO. The inference is the same as the original CondInst on both benchmarks. The performance is evaluated with the COCO-style mask AP.

\begin{table}[t!]
\setlength{\tabcolsep}{4pt}
\centering
\small
\begin{tabular}{ c|c|c|c c|c c c }
 &  & AP & AP & AP & AP & AP & AP \\
\Xhline{2\arrayrulewidth}
\multicolumn{2}{c|}{box mask} & 10.6 & 32.2 & 4.6 & 5.7 & 11.3 & 15.6 \\
\hline
\checkmark & & 21.2 & 45.2 & 17.7 & 10.0 & 21.4 & 32.5 \\
\checkmark & \checkmark & \textbf{30.7} & \textbf{52.2} & \textbf{31.1} & \textbf{13.8} & \textbf{33.1} & \textbf{45.7} \\
\end{tabular}
\caption{\textbf{The mask AP on COCO \texttt{val2017}} by applying the different loss terms. ``box mask": using the masks generated by boxes. If both terms are not used, the model can only provide the box-level localization precision (10.6\% mask AP).}
\label{table:loss_terms}

\end{table}

\begin{table*}[t!]
	\centering
    \small
	\begin{tabular}{l| l |c|c|c|cc|ccc}
		method & backbone & aug. & sched. & AP & AP & AP & AP & AP & AP \\
		\Xhline{2\arrayrulewidth}
	\hspace{-.3025cm}	\textit{fully supervised methods}: & & & & & & & \\
Mask R-CNN~\cite{he2017mask} & ResNet-50-FPN & \checkmark &  & 37.5 & 59.3 & 40.2 & 21.1 & 39.6 & 48.3 \\
		CondInst \cite{tian2020conditional} & ResNet-50-FPN & \checkmark &  & 37.8 & 59.1 & 40.5 & 21.0 & 40.3 & 48.7 \\
Mask R-CNN & ResNet-101-FPN & \checkmark &  & 38.8 & 60.9 & 41.9 & 21.8 & 41.4 & 50.5 \\
		YOLACT-700~\cite{yolact} & ResNet-101-FPN & \checkmark &  & 31.2 & 50.6 & 32.8 & 12.1 & 33.3 & 47.1 \\
		PolarMask \cite{xie2020polarmask} & ResNet-101-FPN & \checkmark &  & 32.1 & 53.7 & 33.1 & 14.7 & 33.8 & 45.3 \\
		CondInst & ResNet-101-FPN & \checkmark &  & 39.1 & 60.9 & 42.0 & 21.5 & 41.7 & 50.9 \\
		\hline
\hspace{-.3025cm} \textit{box-supervised methods}: & & & & & & & \\
		BBTP~\cite{hsu2019weakly} (prev.\ best) & ResNet-101-FPN & &  & 21.1 & 45.5 & 17.2 & 11.2 & 22.0 & 29.8 \\
		\textbf{\Ours\!} & ResNet-101-FPN & &  & 31.6 & 54.0 & 31.9 & 13.9 & 34.2 & 48.2 \\
		\textbf{\Ours} & ResNet-50-FPN & \checkmark &  & 32.1 & 55.1 & 32.4 & 15.6 & 34.3 & 43.5 \\
		\textbf{\Ours} & ResNet-101-FPN & \checkmark &  & 32.5 & 55.3 & 33.0 & 15.6 & 35.1 & 44.1 \\
		\textbf{\Ours} & ResNet-101-FPN & \checkmark &  & 33.2 & 56.5 & 33.6 & 16.2 & 35.3 & 45.1 \\
		\textbf{\Ours} & ResNet-101-BiFPN~\cite{EfficientDetTanPL20} & \checkmark &  & 33.9 & 57.7 & 34.5 & 16.5 & 36.1 & 46.6 \\
		\textbf{\Ours} & ResNet-DCN-101-BiFPN~\cite{zhu2019deformable} & \checkmark &  & \textbf{35.0} & \textbf{59.3} & \textbf{35.6} & \textbf{17.1} & \textbf{37.2} & \textbf{48.9} \\
	\end{tabular}
	\caption{\textbf{Comparisons with state-of-the-art methods} on the COCO \texttt{test}-\texttt{dev} split. ``"
means that
	the results are on the COCO \texttt{val2017} split. BBTP only reported the results on the \texttt{val2017} split.
	Our \Ours\ outperforms the previous best reported mask AP by over absolute 10\% mask AP.
	Ours even outperforms two recent fully supervised methods, YOLACT and PolarMask, and is close to state-of-the-art fully-supervised results. `DCN': deformable convolutions~~\cite{zhu2019deformable}. `1' means 90K iterations.
	}
	\label{table:comparisons_state_of_the_art_methods}

\end{table*}

\subsection{Projection and Pairwise Affinity Loss for Mask Learning}
We first demonstrate that the redesigned mask loss can have similar performance to the original pixelwise mask loss in the fully-supervised settings. The experiments are conducted on COCO. We replace the original Dice loss for mask training in CondInst with the proposed one, and keep other settings exactly the same. As shown in Table~\ref{table:pixel_vs_pair}, the proposed mask loss can have similar performance (35.4\% vs.\  35.6\% mask AP), which suggests that using the proposed loss for mask learning is feasible.

\subsection{Box-supervised Instance Segmentation}
The key advantage of the proposed mask loss is that it can still supervise the predicted masks with only box annotations. We confirm this here and conduct experiments to investigate the hyper-parameters in the proposed mask loss.


\noindent\textbf{Varying the threshold of color similarity.} As mentioned before, we use a color similarity threshold  to determine the edges that will be used to compute the pairwise loss. Here, we conduct experiments by varying . When , all of the edges defined by the size of neighborhood are considered positive and used in the loss computation, as shown in Eq.~(\ref{eq:partial_pairwise_mask_loss}).
As shown in Table~\ref{table:color_sim_threshold}, in this case, 94.1\% of the edges are truly positive, and 6\% of the edges are actually negative. Thus, the loss computation would introduce 6\% noisy labels and all of the truly negative edges are wrongly labelled positive. Unsurprisingly, this experiment yields a trivial solution with poor performance (9.4\% mask AP) that almost all pixels in the box are predicted as foreground. If we increase  to 0.1, the proportion of the truly positive edges are improved to 98.3\%, and only less than 2\% of the edges are wrongly labelled. As a result, the model can yield high-quality instance masks, achieving 30.7\% mask AP (vs.\  fully-supervised counterpart 35.4\%). This result is even better than that of some fully-supervised methods such as YOLACT and PolarMask. Some qualitative results are shown in Fig.~\ref{fig:vis_r101_3x}. If we further increase the threshold  to 0.2, the performance slightly drops to 30.6\% mask AP. This might be because the number of the supervised positive edges decreases as we increase the threshold, as shown in Fig.~\ref{fig:color_sim_and_pos_edges}.

\noindent\textbf{Varying the neighborhood  of the pixels.} We conduct experiments with the different neighbours for each pixel. The size (\ie, ) defines how many surrounding pixels of each pixel are used to compute the pairwise loss with the pixel. Additionally, we may use the dilation trick to enlarge the scope (as in dilated convolutions). As shown in Table~\ref{table:varying_kernel}, by increasing the size from  to , the performance is boosted from 29.7\% to 30.5\%. This suggests that a relatively long-distance pairwise relationship is important to the final performance. However, using  requires more computational overheads in the training. Thus, we apply the dilation rate 2 to the  patches. This can capture the long-distance relationship without increasing the computational overheads and achieves similar performance (30.7\%). The performance cannot be further improved by applying the dilation trick to the  patches because the assumption, two pixels with similar colors probably have the same label, might not hold if the two pixels are far from each other.

\noindent\textbf{The contribution of each loss term.} Table~\ref{table:loss_terms} shows the contribution of each loss term. Even if only the first projection term is used, we can also achieve decent performance (21.2\% mask AP), which can already provide much higher localization precision than boxes (10.6\% mask AP). By further using the proposed pairwise term, high-quality instance masks can be obtained and the performance is largely improved to 30.7\%.


\subsection{Comparisons with State-of-the-art}
We compare \Ours\ with state-of-the-art fully/box supervised instance segmentation methods on the COCO dataset. As shown in Table~\ref{table:comparisons_state_of_the_art_methods}, with the same backbone and training settings, \Ours\ significantly surpasses the previous best reported result~\cite{hsu2019weakly} by absolute 10.5\% mask AP (\eg, from 21.1\% to 31.6\%).
\textit{\Ours, without using any mask annotations, performs even better than some recent fully-supervised methods such as PolarMask~\cite{xie2020polarmask} and YOLACT~\cite{yolact}}, with the same backbones as well as similar training and testing settings (32.5\% with R-101  vs.\ PolarMask 32.1\% R-101  and YOLACT-700 31.2\% R-101 ). \Ours\ also demonstrates
competitive performance with top-performing
fully-supervised instance segmentation methods. For example, with the same backbone ResNet-50-FPN and  training schedule, \Ours\ achieves 32.1\% mask AP (vs. 37.8\% of the fully-supervised CondInst~\cite{tian2020conditional}). With BiFPN~\cite{EfficientDetTanPL20} and DCN~\cite{zhu2019deformable}, the performance can be further boosted to 35.0\% mask AP. Some qualitative results are shown in Fig.~\ref{fig:vis_r101_3x}. The excellent performance shows that \Ours\ dramatically
narrows the performance
gap between the fully supervised and box-supervised instance segmentation, and for the first time, the great potential of box-supervised instance segmentation is revealed.


\subsection{Experiments on Pascal VOC}
\begin{table}
\centering
\small
\begin{tabular}{ l|l|c|c c}
method & backbone & AP & AP & AP \\
\Xhline{2\arrayrulewidth}
GrabCut~\cite{rother2004grabcut} & ResNet-101 & 19.0 & 38.8 & 17.0 \\
SDI~\cite{khoreva2017simple} & VGG-16 & - & 44.8 & 16.3 \\
BBTP~\cite{hsu2019weakly} & ResNet-101 & 23.1 & 54.1 & 17.1 \\
BBTP w/ CRF & ResNet-101 & 27.5 & 59.1 & 21.9 \\
\hline
\textbf{\Ours} & ResNet-50 & 34.3 & 59.1 & 34.2 \\
\textbf{\Ours} & ResNet-101 & \textbf{36.5} & \textbf{61.4} & \textbf{37.0} \\
\end{tabular}
\caption{
\textbf{Results on Pascal VOC \texttt{val2012}}. \Ours\ surpasses previous methods by a large margin. Here, the GrabCut obtains the instance masks by taking as input the boxes generated by \Ours. Thus, the only difference between the GrabCut and \Ours\ is the way to obtain the masks.
Clearly,
\Ours\ achieves
significantly
improved mask AP, outperforming previous best by about 10\%.
}
\label{table:experiments_on_voc}

\end{table}
We also conduct experiments on Pascal VOC. As shown in Table~\ref{table:experiments_on_voc}, \Ours\ achieves state-of-the-art instance segmentation with only box annotations. With the same backbone and training settings, \Ours\ outperforms BBTP both in AP and AP by a large margin. Notably, the AP is improved by more than relative 200\% (17.1\% vs.\ 37.0\% mask AP), which suggests \Ours\ can produce the masks of much higher quality. \Ours\ is even much better than the BBTP with CRF. Additionally, \Ours\ also performs much better than SDI~\cite{khoreva2017simple}. We also compare \Ours\ with the traditional unsupervised segmentation method GrabCut~\cite{rother2004grabcut}. In the experiment, GrabCut takes as input the bounding-boxes predicted by the ResNet-101 based FCOS in \Ours. Thus the only difference between \Ours\ and GrabCut is the way of obtaining instance masks. As shown in Table~\ref{table:experiments_on_voc}, \Ours\ is far better than GrabCut (19.0\% vs.\ 36.5\% mask AP). Moreover, \Ours\ is fully convolutional and can benefit from the highly-efficient GPUs, thus inferring tens of times faster than GrabCut.

\subsection{Extensions: Semi-supervised Instance Segmentation}\label{SEC:SSIS}
\begin{table}
\setlength{\tabcolsep}{4pt}
\centering
\small
\begin{tabular}{ c|c|c c c|c c c}
\multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{3}{c|}{all 80 classes} & \multicolumn{3}{c}{60 unseen classes} \\
 & & AP & AP & AP & AP & AP & AP \\
\Xhline{2\arrayrulewidth}
& & 24.7 & 44.6 & 24.2 & 19.9 & 38.3 & 18.5 \\
\checkmark & & 31.8 & 52.5 & 33.2 & 29.7 & 49.3 & 31.0 \\
\checkmark & \checkmark & \textbf{32.5} & \textbf{53.0} & \textbf{34.0} & \textbf{30.9} & \textbf{50.1} & \textbf{32.4} \\
\hline
\multicolumn{2}{c|}{box supervised} & 30.7 & 52.2 & 31.1 & 29.6 & 49.7 & 30.4 \\
\end{tabular}
\caption{
\textbf{\Ours\ for semi-supervised instance segmentation}.
These models are trained with the 20 classes mask annotations and the other 60 classes (\ie, unseen classes) are only with box annotations.}
\label{table:semi_sup_voc_to_nonvoc}

\end{table}

\begin{table}[t!]
\setlength{\tabcolsep}{4pt}
\centering
\small
\begin{tabular}{ c|c|c c c|c c c}
\multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{3}{c|}{all 80 classes} & \multicolumn{3}{c}{20 unseen classes} \\
 & & AP & AP & AP & AP & AP & AP \\
\Xhline{2\arrayrulewidth}
& & 32.1 & 51.6 & 33.9 & 25.5 & 45.5 & 25.1 \\
\checkmark & & 33.1 & 53.8 & 34.3 & 31.6 & 57.4 & 30.0 \\
\checkmark & \checkmark & \textbf{33.8} & \textbf{54.3} & \textbf{35.7} & \textbf{35.9} & \textbf{60.9} & \textbf{36.3} \\
\hline
\multicolumn{2}{c|}{box supervised} & 30.7 & 52.2 & 31.1 & 29.6 & 49.7 & 30.4 \\
\end{tabular}
\caption{\textbf{\Ours\ for semi-supervised instance segmentation}. The models are trained with the 60 classes mask annotations and other 20 classes (\ie, unseen classes) are only with box annotations.}
\label{table:semi_sup_non_voc_to_voc}

\end{table}

In this section, we show that our method can also help the model generalize to unseen categories in the semi-supervised setting where only partial classes have the mask annotations. Following previous works~\cite{Zhou2020ShapeProp, hu2018learning, kuo2019shapemask} in this setting, we conduct the experiments on the COCO dataset and split the  classes in COCO into two groups -- 20 classes present in Pascal VOC and 60 classes not in Pascal VOC. Then the models are trained with the mask annotations of the classes in one group, and another group of classes only have the box annotations. The generalization ability is evaluated with the mask AP averaged over the group of classes without mask annotations (\ie, unseen classes).

We first train the model with the 20 classes mask annotations. As shown in Table~\ref{table:semi_sup_voc_to_nonvoc} (1st row), if our proposed loss terms are not used, where the mask loss is only computed for the instances with mask annotations and other instances are discarded during the mask learning, the model can only achieve 19.9\% mask AP on the unseen categories. This low performance suggests that the model is difficult to generalize to unseen classes. If we use the  term for the 60 classes without the mask annotations during training, as shown in the table (2nd row), the performance can be dramatically improved to 29.7\%. If we further apply the pairwise term , the performance can be boosted to 30.9\%. Moreover, compared to the setting only using the box annotations (last row in Table~\ref{table:semi_sup_voc_to_nonvoc}), the performance on the unseen classes is also improved from 29.6\% to 30.9\%, which suggests that the box-supervised model can benefit from the partial mask annotations. Additionally, the experimental results with the 60 classes masks are shown in Table~\ref{table:semi_sup_non_voc_to_voc}, and the same conclusions can be drawn.

\subsection{Extensions: Box-supervised Character Segmentation} \label{SEC:CHAR}


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/vis_rects.pdf}
\caption{\textbf{Character masks predicted
   by \Ours.} No
   mask annotations are used for
   training.}
\label{fig:char_segmentation}

\end{figure}

 In order to
 demonstrate the generality of \Ours, we conduct experiments to obtain the character masks with character box annotations. Our experiments are conducted on the ICDAR 2019 ReCTS dataset ~\cite{zhang2019icdar}, which contains
20K training images and 5K testing images. These images are annotated with text-line and character-level boxes. We train our model with the character boxes. All the training settings are the same as that of COCO. Since we do not have mask annotations for the testing set, it is impossible to
report the mask AP. We instead show some qualitative results in Fig.~\ref{fig:char_segmentation},
demonstrating that
\Ours\ can obtain high-quality character masks.
The text masks might provide useful cues for
detecting and recognising text of arbitrary shapes.
We believe that the ability of \Ours generating
character masks automatically may inspire new applications on this task.


\section{Conclusions}

In this work, we have proposed \Ours\ that can achieve high-quality instance segmentation with only box annotations. The core idea of \Ours\ is to replace the original pixelwise mask loss with the proposed projection and pairwise affinity mask loss. With the proposed mask loss, we show excellent instance segmentation performance without using any mask annotations on COCO and Pascal VOC, significantly improving the state-of-the-art.




{\small
\bibliographystyle{ieee_fullname}
\bibliography{draft}
}

\end{document}
