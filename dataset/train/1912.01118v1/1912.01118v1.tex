\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{cvpr}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\rohan}[1]{\textcolor{blue}{#1}}
\newcommand{\bigO}[1]{\mathcal{O}(#1)}
\newcommand{\simname}{simname}
\newcommand{\sota}{state-of-the-art}
\newcommand{\algoname}{GraphRQI}
\newcommand{\trisha}[1]{\textcolor{red}{#1}}
\newcommand{\srujan}[1]{\textcolor{green}{#1}}
\newcommand{\utt}[1]{\textcolor{blue}{#1}}
\newcommand{\tian}[1]{\textcolor{orange}{#1}}
\newcommand{\dataname}{Sub-$k$ matrix}
\newcommand{\datanames}{Sub-$k$ matrices}
\newcommand{\G}{$G$}
\newcommand{\V}{$V$}
\newcommand{\E}{$E$}
\newcommand{\brr}[1]{\left( #1 \right)}
\newcommand{\bcc}[1]{ \left{ #1 \right} }
\newcommand{\bss}[1]{\left[ #1 \right]}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\sg}{\mathcal{L}}
\newcommand{\li}{\sg}
\newcommand{\vts}[1]{\lvert #1 \rvert}
\newcommand{\Vts}[1]{\lVert #1 \rVert}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         \newcommand\Bstrut{\rule[-1.3ex]{0pt}{0pt}}   \newcommand\mathdash{\text{\normalfont --}}
\newcommand{\cost}{\bigO{ \vts{\li^{\scriptscriptstyle -1}_t}k }}
\newcommand{\costk}{\bigO{ \vts{\li^{\scriptscriptstyle -1}_t} }}
\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother
\newcommand{\size}{\bigO{d}}
\newcommand{\shorteq}{\settowidth{\@tempdima}{-}\resizebox{\@tempdima}{\height}{=}}
\newcommand*\midpoint[1]{\overline{#1}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}



\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage[linewidth=1pt]{mdframed}
\usepackage[T1,OT1]{fontenc}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{algorithmicx}
\usepackage[Algorithm,ruled]{algorithm}
\usepackage[font=small,belowskip=-1.5pt]{caption} \usepackage[noend]{algpseudocode}
\usepackage{array}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{soul}
\usepackage{hhline}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{booktabs}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{color}
\usepackage{transparent}
\usepackage{url}
\usepackage[symbol]{footmisc}
\usepackage{setspace}

\theoremstyle{plain}
\linespread{0.88}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma} 

\usepackage[colorlinks]{hyperref}
\cvprfinalcopy 

\def\cvprPaperID{9832} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs }

\author{Rohan Chandra \qquad Tianrui Guan \qquad Srujan Panuganti\\
Trisha Mittal \qquad Uttaran Bhattacharya \qquad Aniket Bera \qquad Dinesh Manocha\\
{\small{ (University of Maryland, College Park)}}\\
{Code, Video, Datasets at \url{https://gamma.umd.edu/spectralcows/}}\vspace{-15pt}
}




\maketitle
\begin{abstract}
\vspace{-10pt}
We present a novel approach for traffic forecasting in urban traffic scenarios using a combination of spectral graph analysis and deep learning. We predict both the low-level information (future trajectories) as well as the high-level information (road-agent behavior) from the extracted trajectory of each road-agent. Our formulation represents the proximity between the road agents using a dynamic weighted traffic-graph. We use a two-stream graph convolutional LSTM network to perform traffic forecasting using these weighted traffic-graphs.
The first stream predicts the spatial coordinates of road-agents, while the second stream predicts whether a road-agent is going to exhibit aggressive, conservative, or normal behavior. We introduce spectral cluster regularization to reduce the error margin in long-term prediction (3-5 seconds) and improve the accuracy of the predicted trajectories.
We evaluate our approach on the Argoverse, Lyft, and Apolloscape datasets and highlight the benefits over prior trajectory prediction methods. In practice, our approach reduces the average prediction error by more than 54\% over prior algorithms and achieves a weighted average accuracy of 91.2\% for behavior prediction. 
\end{abstract}

\vspace{-15pt}
 \section{Introduction}

Autonomous driving is an active area of research~\cite{ad1,ad2} and includes many issues related to navigation and trajectory prediction~\cite{ad2}. In order to perform efficient and collision-free navigation, we need accurate trajectory prediction capabilities.
Trajectory prediction is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and even animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles~\cite{humanfactor1,humanfactor2,humanfactor3}. 
\begin{figure}[t]
    \centering
    \includegraphics[width = \columnwidth]{cover.png}
    \caption{\textbf{Trajectory and Behavior Prediction:} We predict the long-term (3-5 seconds) trajectories of road-agents, as well as their behavior (e.g. overspeeding, braking, etc.), in urban traffic scenes. Our approach represents the spatial coordinates of road-agents (colored points in the image) as vertices of a traffic-graph to improve long-term prediction using a new regularization method. We report a reduction in average RMSE of at least 54\% with respect to prior methods on the Lyft, Argoverse, and Apolloscape datasets and improve long-term prediction by up to 83\%. We also report a weighted average accuracy of 91.2\% for behavior prediction. }
    \vspace{-15pt}
    \label{fig:cover}
\end{figure}


Trajectory prediction~\cite{traphic,nachiket,chandra2019robusttp} is strongly correlated with road-agent behavior classification~\cite{chandra2019graphrqi,ernest,sarkar2019behavior}. For instance, knowing if a neighboring road-agent is going to overtake another agent or if a road-agent in front is going to brake suddenly, is useful for navigation. However, the problems of trajectory~\cite{traphic,nachiket,chandra2019robusttp}, and behavior prediction~\cite{chandra2019graphrqi,ernest} have been mostly studied in isolation, and there is not much work towards a unified approach to perform both these predictions simultaneously. 

Road-agents in close proximity interact with each other to avoid collisions. Most prior trajectory prediction methods~\cite{traphic,nachiket,chandra2019robusttp} use spatial representations of the traffic to model the dynamic road-agent interactions using only the coordinate values of road-agents. 
However, many frequent complex interactions between road-agents, such as overtaking or tailgating, cannot be captured using spatial coordinates. 


Finally, a major challenge in traffic forecasting is ensuring accurate long-term prediction (3-5 seconds). As the prediction horizon increases, the temporal correlations in the data between current and previous time-steps grow weaker, which increases the error-margin of long-term prediction (\cite{anima}, cf. Figure 4 in~\cite{nachiket,traphic}, Figure 3 in~\cite{li2019grip}). 
Some approaches have been developed to reduce the long-term error-margin for trajectory forecasting~\cite{anima}, but they assume knowledge of high-order, non-linear traffic dynamics.















\paragraph{Main Contributions:} We present a unified algorithm for traffic forecasting that combines trajectory prediction and road-agent behavior prediction. We represent the inter-road-agent interactions in the traffic using a dynamic weighted traffic-graph, in which the nodes represent the road-agents, and the weighted edges correspond to the proximity between the road-agents. Our approach makes no assumptions about the size and shape of the road-agents or traffic density. Our main contributions include: 


\begin{enumerate}[noitemsep]
    \item A novel two-stream graph-LSTM network for traffic forecasting in urban traffic.
The first stream predicts the spatial coordinates of the future trajectory, while the second stream predicts the eigenvectors of future traffic-graphs, which are further used to predict the behavior of road-agents.   


    \item To reduce the error of long-term predictions, we propose a new regularization algorithm for sequence prediction models called spectral cluster regularization. 

    \item We present a rule-based behavior prediction algorithm to forecast whether a road-agent is overspeeding (aggressive), slowing down (conservative), or neutral, based on the traffic behavior classification in psychology literature~\cite{ernestref2, rohanref5}. \end{enumerate}
We evaluate our approach on three recent large-scale urban driving datasets -- Argoverse, Lyft, and Apolloscape. Further details of these datasets are given in Section~\ref{subsec: datasets}. We also perform an exhaustive comparison with the SOTA trajectory prediction methods and report an average RMSE (root mean square error) reduction of at least 54\% with respect to the next best method. We also achieved a weighted average accuracy of 91.2\% for behavior prediction. Our regularization algorithm improves long-term prediction by up to 83\%. 
 \section{Related Work}

In this section, we discuss related work in trajectory prediction, road-agent behavior prediction, and traffic forecasting.

\subsection{Trajectory Prediction}

Trajectory prediction is a well-known problem in statistics~\cite{box2015time}, signal processing~\cite{early4-kalman,particle}, and controls and systems engineering~\cite{ljung2001system}. These approaches, however, rely on the knowledge of certain parameters that may not be readily available in traffic videos. In such instances, data-driven methods such as deep learning have become the SOTA for designing trajectory prediction algorithms.




Trajectory prediction of pedestrians in dense crowds using LSTM and GANs has been studied extensively~\cite{pedestrian_cvpr17,pedestrian_cvpr18,pedestrian2_cvpr18,pedestrian_wacv18,pedestrian2_wacv18,pedestrian_cvpr19,pedestrian2_cvpr19,pedestrian3_cvpr19,pedestrian_eccv18}.
On the contrary, trajectory prediction of road-agents has been explored in lesser detail, as the inter-agent dynamics that govern the motion of pedestrians in a crowd are much different from the dynamics between vehicles in traffic. Despite this challenge, Deo et al.~\cite{nachiket} combined LSTMs with Convolutional Neural Networks (CNNs) to predict the trajectories of vehicles on sparse U.S. highways. Chandra et al.~\cite{traphic, chandra2019robusttp} proposed algorithms to predict trajectories in urban traffic with high density and heterogeneity. For traffic scenarios with moderate density and heterogeneity, Ma et al.~\cite{ma2018trafficpredict} proposed a method based on reciprocal velocity obstacles. Some additional deep learning-based trajectory prediction methods include~\cite{cnnpredict1,cnnpredict2}. However, these methods only capture road-agent interactions inside a local grid, whereas graph-based approaches such as~\cite{guo2019attention,Yu2018SpatioTemporalGC} for trajectory prediction of road-agents and~\cite{cui2018traffic,guo2019attention,Yu2018SpatioTemporalGC,diao2019dynamic} for traffic density prediction consider all interactions independent of local neighborhood restrictions. However, these deep learning methods do not predict the behavior of road-agents, nor do they resolve long-term prediction error.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{network.png}
    \caption{\textbf{Network Architecture}: We show the trajectory and behavior prediction for the $i^{\textrm{th}}$ road-agent (red circle in the traffic-graphs). The input consists of the spatial coordinates over the past $T$ seconds as well as the eigenvectors (green rectangles, each shade of green represents the index of the eigenvectors) of the traffic-graphs corresponding to the first $T$ traffic-graphs. We perform spectral clustering on the predicted eigenvectors from the second stream (orange block) to regularize the original loss function and perform backpropagation on the new loss function to improve long-term prediction.}
    \label{fig:network}
    \vspace{-10pt}
\end{figure*}

\subsection{Road-Agent Behavior Prediction}

Many studies have been performed that provide insights into factors that contribute to different driver behaviors classes such as aggressive, conservative, or moderate driving. These factors can be broadly categorized into four categories. The first category of factors that indicate road-agent behavior is driver-related. These include characteristics of drivers such as age, gender, blood pressure, personality, occupation, hearing, and so on~\cite{ernestref2, rohanref3, rohanref5, big5, ernestref9, ernestref10}. 
The second category corresponds to environmental factors such as weather or traffic conditions~\cite{behaviorref-category2-1,behaviorref-category2-2}.
The third category refers to psychological aspects that affect driving styles. These could include drunk driving, driving under the influence, state of fatigue, and so on~\cite{behaviorref-category3-2,behaviorref-category3-1}.
The final category of factors contributing to driving behavior corresponds to vehicular factors such as positions, acceleration, speed, throttle responses, steering wheel measurements, lane changes, and brake pressure~\cite{ernestref8,ernestref12, ernestref13,ernestref14,ernestref15, ernest, chandra2019graphrqi}. Our behavior prediction algorithm is based on factors from the final category.




\subsection{Traffic Flow and Forecasting}

Traffic forecasting has been studied in different contexts in prior literature. From a deep-learning perspective, traffic forecasting is synonymous with trajectory prediction and does not take into account road-agent behavior~\cite{Argoverse}. However, in a broader sense, traffic forecasting refers to predicting traffic flow~\cite{flow2,flow3,flow4} or traffic density~\cite{forecast1,forecast2,forecast3,forecast4} on a macroscopic scale. Predicting traffic flow is important for applications such as congestion management and vehicle routing. In this paper, we mainly limit ourselves to forecasting low-level trajectories and high-level behaviors of each road-agent.









 \section{Background and Overview}
In this section, we define the problem statement and give a brief overview of spectral traffic-graphs in the context of road-agents. 

\subsection{Problem Statement}
The problem statement is defined as follows: In a traffic video with $n$ road agents, given the trajectory for each road agent for the previous $T$ seconds, our goal is to predict for the next $\tau$ seconds, the following:

\begin{itemize}[noitemsep]
    \item Future trajectory for all road-agents, denoted as $[(x_{T+1},y_{T+1}),\ldots,(x_{T+\tau},y_{T+\tau})]^\top$. $(x,y)$ denote the spatial coordinates of a road-agent in meters according to the world coordinate frame.
    \item Driving behavior for each road-agent, for example, whether a road-agent will act aggressively, conservatively, or drive normally. We assign a label from the following set, $\{ Overspeeding, Neutral, Braking\}$, based on road-agent behavior classification literature~\cite{ernestref15,ernestref16}.
\end{itemize}

\subsection{Spectral Traffic-Graphs}
\label{subsec: spectral graph theory}

We represent traffic at each time instance with $n$ road-agents using a traffic-graph $\mc{G}$, with the spatial coordinates of the road-agent representing the set of vertices $\mc{V} = \{ v_1, v_2, \dots, v_n \}$ and a set of undirected, weighted edges, $\mc{E}$. Two road-agents are said to be connected through an edge if $d(v_i,v_j) < \mu$, where $d(v_i,v_j)$ represents the Euclidean distance between the road-agents and $\mu$ is a heuristically chosen threshold parameter. In our experiments, we choose $\mu=10$ meters, taking into account the typical size of road-agents and the width of the road. 

We define the symmetrical adjacency matrix, $A \in \mathbb{R}^{n \times n}$ of a traffic-graph $\mc{G}$ as,

\begin{equation}
\resizebox{0.7\columnwidth}{!}{
$A(i,j)
     \begin{cases}
       e^{-d(v_i,v_j)}  & \text{if $d(v_i,v_j) < \mu,i \neq j$ },\\
0 &\text{otherwise.}
     \end{cases}$
     }
     \label{eq: similarity_function}
\end{equation}

\noindent The function $f(v_i,v_j) = e^{-d(v_i,v_j)}$~\cite{belkin2003laplacian} denotes the interactions between any two road-agents, $v_i$ and $v_j$. This implies that road-agents at a greater distance are assigned a lower weight, while road-agents in close proximity are assigned a higher weight. This follows the intuition that each road-agent needs to pay more attention to other nearby agents compared to agents farther away to avoid collisions.



For the adjacency matrix $A$ at each time instance, the corresponding degree matrix $D \in \mathbb{R}^{n \times n}$ of the graph, $\mc{G}$, is a diagonal matrix with main diagonal $D(i, i) = \sum_{j=1}^n A(i, j)$ and 0 otherwise. The unnormalized Laplacian matrix $L  = D  - A$ of the graph is defined as the symmetric matrix,

\begin{equation}
\resizebox{0.6\columnwidth}{!}{
$L(i,j) =
     \begin{cases}
       D(i,i)  &\text{if $i=j$},\\
      -e^{-d(v_i,v_j)} &\text{if $d(v_i,v_j) < \mu$}, \\
      0 &\text{otherwise.}
     \end{cases}$
     }
\end{equation}

The Laplacian matrix for each time-step is correlated with the Laplacian matrices for all previous time-steps. Let the Laplacian matrix at a time instance $t$ be denoted as $L_t$. Then, the laplacian matrix for the next time-step, $L_{t+1}$ is given by the following update,

\begin{equation}
\resizebox{0.4\columnwidth}{!}{
$L_{t+1} =
\left[
\begin{array}{c|c}
L_{t} \Bstrut & 0 \Bstrut\\
\hline
0 \Tstrut & 1
\end{array}
\right] + \delta\delta^\top$,
}
\label{eq: A_update}
\end{equation}

\noindent where $\delta\delta^\top$ is an outer product of rank 2 with $\delta \in \bb{R}^{d \times 2} $ is a sparse matrix $\Vts{\delta}_0 \ll n$. The presence of a non-zero entry in the $j^\textrm{th}$ row of $\delta$ implies that the $j^\textrm{th}$ road-agent has observed a new neighbor, that has now been added to the current traffic-graph. During training time, the maximum size of a traffic-graph and its corresponding Laplacian matrix is the total number of distinct road-agents in the traffic scenario. During test time, our approach does not require traffic-graphs for trajectory prediction.

The matrix $U \in \mathbb{R}^{n \times k} := \{ u_j \in \mathbb{R}^{n} | j = 1 \dots k\}$ of eigenvectors of $L$ is called the \emph{spectrum} of $L$, and can be efficiently computed using eigenvalue algorithms.





 \section{Traffic Forecasting}
The overall flow of the approach can be described as follows:

\begin{enumerate}[noitemsep]
    \item Our input consists of computing the spatial coordinates over the past $T$ seconds as well as the eigenvectors of the traffic-graphs corresponding to the first $T$ traffic-graphs.
    
    \item The first stream (blue block in Figure~\ref{fig:network}) accepts the extracted spatial coordinates and uses an LSTM-based sequence model~\cite{graves2013generating} to predict the trajectory of a road-agent for the next $\tau$ seconds.
    
    \item The second stream (orange block in Figure~\ref{fig:network}) accepts the eigenvectors of the traffic-graphs and predicts the eigenvectors corresponding to the traffic-graphs for the next $\tau$ seconds. The predicted eigenvectors are used within the behavior prediction algorithm to assign a behavior label to the road-agent (Section~\ref{subsec: behavior_protocol}). We provide theoretical analysis for the second stream in~\ref{subsec: analysis}.
    
    \item To improve long-term prediction, we propose a new regularization algorithm in Section~\ref{sec: spectral_clustering}.
\end{enumerate}


\subsection{Network Overview}

We present an overview of our approach in Figure~\ref{fig:network} and defer technical implementation details of our network in Section~\ref{subsec: implementation}. Our approach consists of two parallel LSTM networks (or streams). 


\textbf{Stream 1:} The first stream is an LSTM-based encoder-decoder network (blue layer in Figure~\ref{fig:network}). The input consists of the trajectory history, $ [(x_{1},y_{1}),\ldots,(x_{T},y_{T})]^\top$ for each road-agent for the past $T$ seconds. The stream predicts the future spatial coordinates, $[(x_{T+1},y_{T+1}),\ldots,(x_{T+\tau},y_{T+\tau})]^\top$, for the next $\tau$ seconds.

\textbf{Stream 2:} The second stream is also an LSTM-based encoder-decoder network (orange layer in Figure~\ref{fig:network}). To prepare the input to this stream, we first form a sequence of traffic-graphs, $\{\mc{G}_1, \mc{G}_2, \ldots, \mc{G}_T\}$ for each time instance of traffic until time $T$. For each traffic-graph, $\mc{G}_i$, we first compute its corresponding Laplacian matrix, $L_i$ and use SOTA eigenvalue algorithms to obtain the spectrum, $U_i$ consisting of the top $k$ eigenvectors of length $n$. We form $k$ different sequences, $\{\mc{S}_1, \mc{S}_2, \ldots, \mc{S}_k\}$, where each $\mc{S}_j = \{u_j\}$ is the set containing the $j^{\textrm{th}}$ eigenvector from each $U_i$ corresponding to the $i^{\textrm{th}}$ time-step, with $\vts{\mc{S}_j} = T$. 

The second stream then accepts a sequence, $\mc{S}_j$, as input to predict the $j^{\textrm{th}}$ eigenvectors for the next $\tau$ seconds. This is repeated for each $\mc{S}_j$. The resulting sequence of spectrums, $\{U_{t+1}, \ldots, U_{T+\tau}\}$ are used to reconstruct the sequence, $\{L_{t+1}, \ldots, L_{T+\tau}\}$, which is then used to assign a behavior label to a road-agent, as explained below.

\subsection{Behavior Prediction Algorithm}
\label{subsec: behavior_protocol}
We define a rule-based behavior algorithm (red block in Figure~\ref{fig:network}). This is largely due to the fact that most data-driven behavior prediction approaches require large, well-annotated datasets that contain behavior labels. Our algorithm is based on information obtained from the predicted eigenvectors of the traffic-graphs of the next $\tau$ seconds.



The degree of $i^{\textrm{th}}$ road-agent, ($\theta_i \leq n$), can be computed from the diagonal elements of the Laplacian matrix $L_t$. $\theta_i$ measures the total number of distinct neighbors with which road-agent $v_i$ has shared an edge connection until time $t$. As $L_t$ is formed by simply adding a row and column to $L_{t-1}$, the degree of each road-agent monotonically increases. Let the rate of increase of $\theta_i$ be denoted as $\theta_i^{'}$. Intuitively, an aggressively overspeeding vehicle will observe new neighbors at a faster rate as compared to a road-agent driving at a uniform speed. Conversely, a conservative road-agent that is often braking at unconventional spots such as green light intersections (Figure~\ref{fig:cover}) will observe new neighbors very slowly. This intuition is formalized by noting the change in $\theta_i$ across time-steps. To predict the behavior of the $i^{\textrm{th}}$ road-agent, we follow the following steps:

\begin{enumerate}[noitemsep]
    \item Form the set of predicted spectrums from stream 2, $\mc{U}= \{ U_{T+1}, U_{T+2}, \dots, U_{t+\tau} \}$.
    \item For each $U_t \in \mc{U}$, compute $L_t = U_t\Lambda U_t^\top$.
    \item $\theta_i = i^{\textrm{th}}$ element of diag($L_t$).
    \item $\theta_i^{'} = \frac{\Delta \theta_i}{\Delta t}$.
\end{enumerate}

\noindent where $\Lambda$ is the eigenvalue matrix of $L_t$. Based on heuristically pre-determined threshold parameters $\lambda_1$ and $\lambda_2$, we apply the rules summarized in Table~\ref{tab: behavior_protocol} to assign the final behavior label.
\begin{table}[t]
  \caption{\textbf{Behavior Definitions:} Definitions of the different types of road-agent behaviors. $\lambda_1, \lambda_2$ are constant threshold parameters.}
  \label{tab: behavior_protocol}
  \centering
  \resizebox{.5\columnwidth}{!}{
  \begin{tabular}{lc}
  \toprule
    Behavior  & Definition \\
    \midrule
    Overspeeding &  $\theta^{'} > \lambda_1$.\\
    Neutral & $\lambda_2 \leq \theta^{'} \leq \lambda_1$.\\
    Braking & $\theta^{'} < \lambda_2$. \\
\bottomrule
  \end{tabular}
  }
\vspace{-5pt}
\end{table}
 


\subsection{Analysis of Stream 2}
\label{subsec: analysis}


LSTMs make accurate sequence predictions if elements of the sequence are correlated across time, as opposed to being generated randomly. In the general case, eigenvectors may not be correlated across time. Consequently, it is difficult for LSTM networks to predict the sequence of eigenvectors, $\mc{U}$ accurately. This may adversely affect the behavior prediction algorithm described in the previous section. Our goal in this section is to show that by using the correlation between Laplacian matrices across time-steps (See Section~\ref{subsec: spectral graph theory}), there exists sufficient correlation in a sequence of eigenvectors for accurate prediction.

At time instance $t$, the Laplacian matrix, $L_t$, its block form,$
\left[ 
\begin{array}{c|c}
L_{t} \Bstrut & 0 \Bstrut\\
\hline
0 \Tstrut & 1
\end{array}
\right ]$, denoted as block($L_t$), and the laplacian matrix for the next time-step, $L_{t+1}$ are described by Equation~\ref{eq: A_update}. In order to demonstrate sufficient correlation, it is equivalent to show minimal noise, or error distance, between the $j^{\textrm{th}}$ eigenvectors of $L_t$ and $L_{t+1}$. We denote this error distance through the angle $\phi_j$. Then, using Theorem 5.4 of~\cite{demmel1997applied}, the following corollary can be shown,

\begin{corollary}
    $\phi_j \leq \frac{  \Vts{\delta_t\delta_t^\top}_2  }{  min(\lambda_j, \Lambda)  }$, where $min(\lambda_j, \Lambda)$ denotes the minimum distance between $\lambda_j$ and $\lambda_k$ with $k \neq j$.
    \label{eq: theoretical_bound}
\end{corollary}

\begin{proof}
From Theorem 5.4 of~\cite{demmel1997applied}, the numerator of bound corresponds to the frobenius norm of the error between $L_t$ and $L_{t+1}$. In our case, the update to the Laplacian matrix is given by Equation~\ref{eq: A_update} where the error matrix is $\delta \delta^\top$. 
\end{proof}


\noindent Here, $\phi_j \ll 1$ and $\delta$ is defined in equation~\ref{eq: A_update}. $\lambda_j$ represents the $j^{\textrm{th}}$ eigenvalue and $\Lambda$ represents all the eigenvalues of $L_t$. If the maximum component of $\delta_t$ is $\delta_{max}$, then $\phi_j = \bigO{\sqrt{n} \delta_{max}}$.

Corollary~\ref{eq: theoretical_bound} shows that in a sequence of $j^{\textrm{th}}$ eigenvectors, the maximum angular difference between successive eigenvectors is bounded by $\bigO{\sqrt{n} \delta_{max}}$. In our experiments, we have $n=300$, and $\delta_{max}$ value corresponds to the distance between the two closest road-agents. A smaller value of $\phi_j$ indicates a greater similarity between successive eigenvectors, thereby implying a greater correlation in the sequence of eigenvectors. This allows sequence prediction models to learn future eigenvectors efficiently.

 \section{Spectral Clustering Regularization}
\label{sec: spectral_clustering}



The original loss function of stream 1 for the $i^{\textrm{th}}$ road-agent in an LSTM network is given by,
\begin{equation}
\resizebox{0.6\columnwidth}{!}{
    $F_i = -\sum_{t=1}^T \log Pr(x_{t+1} | \mu_t, \sigma_t, \rho_t)$
    }
    \label{eq: original_loss}
\end{equation}



\noindent Our goal is to optimize the parameters, $\mu^{*}_t, \sigma^{*}_t$, that minimize equation~\ref{eq: original_loss}. Then, the next spatial coordinate is sampled from a search space defined by $\mathcal{N}(\mu^{*}_t, \sigma^{*}_t)$. The resulting optimization forces $\mu_t, \sigma_t$ to stay close to the next spatial coordinate. However, in general trajectory prediction models, the predicted trajectory diverges gradually from the ground-truth, causing the error-margin to monotonically increase as the length of the prediction horizon increases (\cite{anima}, cf. Figure 4 in~\cite{nachiket,traphic}, Figure 3 in~\cite{li2019grip}). The reason for this may be that while equation~\ref{eq: original_loss} ensures that $\mu_t, \sigma_t$ stays close to the next spatial coordinate, it does not, however, guarantee the same for $\hat x_{t+1} \sim \mathcal{N}(\mu_t, \sigma_t)$. Our solution to this problem involves regularizing equation~\ref{eq: original_loss} by adding appropriate constraints on the parameters, $\mu_t, \sigma_t$, such that sampled coordinates from $\mathcal{N}(\mu^{*}_t, \sigma^{*}_t)$ are as close to the ground-truth trajectory as possible.
\begin{table*}[h!]
\centering
\resizebox{.9\linewidth}{!}{\begin{tabular}{lcccccccccccccc}
\toprule[1.1pt]
\multirow{2}{*}{Dataset}                       &
\multicolumn{2}{c}{Enc-Dec }         &
\multicolumn{2}{c}{CS-LSTM~\cite{nachiket}  }         &
\multicolumn{2}{c}{TraPHic~\cite{traphic}  }         &
\multicolumn{2}{c}{Social-GAN~\cite{social-gan}  }         &
\multicolumn{2}{c}{GRIP~\cite{li2019grip}  }         &
\multicolumn{2}{c}{Ours (Stream 1 only)  }  &
\multicolumn{2}{c}{Ours (Both Streams)  }            \\
&
ADE  & FDE  &
ADE & FDE &
ADE & FDE &
ADE & FDE &
ADE & FDE &
ADE & FDE &
ADE & FDE \\
\midrule
Lyft            &  1.163  &  1.242  &  4.423  &  8.640  & 5.031  &  9.882   &  6.660$^*$  &  11.270$^*$  & 0.262  &  0.27  &  0.056  &  0.052  &  \textbf{0.008} &  \textbf{0.009}\\ 
Argoverse       &  0.871  &  0.884  &  1.050  &  3.085  & 1.039  &  3.079   &  0.930$^*$   &  1.850$^*$  & 0.244  & 0.243  &  0.031 &  0.032  &  \textbf{0.005} &  \textbf{0.006} \\ 
Apolloscape     &  0.013  &  0.017  &  2.144  &  11.699  & 1.283  &  11.674  &  0.110$^*$  &  0.150$^*$  & 0.011  &  0.007  &  0.021  &  0.023  &  \textbf{0.005} &  \textbf{0.004}\\ 
\bottomrule[1.1pt]
\end{tabular}
}
\caption{\textbf{Main Results:} We report the Average Displacement Error (ADE) and Final Displacement Error (FDE) for prior road-agent trajectory prediction methods. Lower scores are better and \textbf{bold} indicates the SOTA. In addition to achieving the lowest ADE/FDE, we also reduce long term prediction error by at least 5 times (difference between values in the last two columns). All results are reported in meters. ($*$) indicates a particular method did not converge for the entire prediction length. }
\label{tab: accuracy}
\vspace{-5pt}
\end{table*} 
We assume the ground-truth trajectory of a road-agent to be equivalent to their ``preferred" trajectory, which is defined as the trajectory a road-agent would have taken in the absence of other dynamic road-agents.  Preferred trajectories can be obtained by minimizing the Dirichlet energy of the traffic-graph, which in turn can be achieved through spectral clustering on the road-agents~\cite{dirichlet}. Our regularization algorithm (shown in the yellow arrow in Figure~\ref{fig:network}) is summarized below. For each road-agent, $v_i$: 

\begin{enumerate}[noitemsep]
    \item The second stream computes the spectrum sequence, $\{U_{T+1}, \ldots, U_{T+\tau}\}$.
    \item For each $U$, perform spectral clustering~\cite{von2007tutorial} on the eigenvector corresponding to the second smallest eigenvalue.
    \item Compute cluster centers from the clusters obtained in the previous step.
    \item Identify the cluster to which $v_i$ belongs and retrieve the cluster center, $\mu_c$ and deviation, $\sigma_c$. 
\end{enumerate}
Then for each road-agent, $v_i$, the regularized loss function, $F^{\textrm{reg}}_i$, for stream 1 is given by,

\begin{equation}
\resizebox{.8\columnwidth}{!}{
    $\sum_{t=1}^T \Big (- \log Pr(\hat y_{t+1} | \mu_t, \sigma_t, \rho_t \Big ) + b_1\Vts{\mu_t - \mu_c}_2 + b_2\Vts{\sigma_t - \sigma_c}_2$
    }
    \label{eq: oregularized_loss}
\end{equation}

\noindent where $b_1, b_2$ are regularization constants. The regularized loss function is used to backpropagate the weights corresponding to $\mu_t$ in stream 1.


 
\section{Experiments and Results}

\begin{figure*}[h]
    \centering
    \includegraphics[width = .9\linewidth]{traj_clusters.png}
    \caption{\textbf{Qualitative Analysis:} We compare the predicted trajectory with the ground truth trajectory (green line with cyan coordinates). The prediction time is 5 seconds. Each red blob in the figure represents a predicted bi-variate Gaussian distribution, $\mathcal{N}(\mu^{*}, \sigma^{*}, \rho^{*})$. The prediction is more accurate when the cyan points are closer to the center of the red blobs. }
    \label{fig: qual}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}
  \begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{Lyft1.png}
    \caption{RMSE values for Lyft Dataset}
    \label{fig: rmse_lyft}
  \end{subfigure}
\begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{Argo1.png}
    \caption{RMSE values for Argoverse Dataset}
    \label{fig: rmse_argo}
  \end{subfigure}
\begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{Apol1.png}
    \caption{RMSE values for Apolloscape Dataset}
    \label{fig: rmse_apol}
  \end{subfigure}
  \caption{\textbf{RMSE Curves:} We plot the logarithm of the RMSE values for visualization purposes. Lower values indicate the direction of better performance. The prediction window is 5 seconds for the Lyft and Apolloscape datasets, and 3 seconds for the Argoverse dataset, which corresponds to a frame length of 30, 10, and 30, respectively.}
  \label{fig: rmse}
  \vspace{-5pt}
\end{figure*}


We begin by describing the datasets used in our approach in Section~\ref{subsec: datasets}. We list the evaluation metrics used and methods compared within Section~\ref{subsec: eval_metrics_methods}. We analyze the main results and discuss the results of ablation studies of our approach in Section~\ref{subsec: analysis_and_discussion}. We conclude by presenting the implementation and training details of our approach in Section~\ref{subsec: implementation}. We will make all the data processing and implementation code available.


\subsection{Datasets}
\label{subsec: datasets}

We use the Lyft Level 5~\cite{lyft2019}, Argoverse Motion Forecasting~\cite{Argoverse}, and the Apolloscape Trajectory~\cite{ma2018trafficpredict} datasets for evaluation. These are large-scale urban trajectory prediction datasets for autonomous driving. We give a brief description of each below.

\noindent \textbf{Level 5 Lyft: } The LYFT Level 5 dataset contains 180 videos of traffic in Palo Alto, California, U.S. Each video consists of 126 frames covering a duration of 20 seconds. A single traffic video consists of around 300 distinct road-agents. The data format is similar to the nuScenes format~\cite{nuscenes2019}.


\noindent \textbf{Argoverse Motion Forecasting: } Argoverse motion forecasting data consists of 324,557 video segments of 5 seconds each. The total video length is 320 hours. The dataset contains traffic videos recorded in Miami (204 kilometers) and Pittsburgh (6 kilometers). The format of the data includes the timestamp, road-agent I.D., road-agent type, the spatial coordinates, and the location.

\noindent \textbf{ApolloScape Trajectory:} The ApolloScape trajectory dataset consists of 53 minutes of training sequences and 50 minutes of testing sequences captured at two fps. The dataset has been collected in Beijing, China. The format of the data includes the frame I.D., road-agent I.D., road-agent type, 3D spatial coordinates, heading angle and height, length, and width of the object.

It is worth mentioning that there are several other datasets related to autonomous driving, for instance, the TRAF~\cite{traphic} and the Honda Driving Dataset~\cite{honda}. However, TRAF is not relevant for our current work as the number of traffic videos in TRAF is less than 50, and the annotations consist of pixel coordinates instead of meters in the world coordinate system. 
\subsection{Evaluation Metrics and Methods}
\label{subsec: eval_metrics_methods}
\subsubsection{Metrics}
For trajectory prediction, we use the standard metrics used in the trajectory prediction literature~\cite{social-lstm,social-gan,traphic,nachiket,chandra2019robusttp}.
\begin{enumerate}[noitemsep]
    \item Average Displacement Error (ADE): The root mean square error (RMSE) of all the predicted positions and real positions during the prediction window.
    \item Final Displacement Error (FDE): The RMSE distance between the final predicted positions at the end of the predicted trajectory and the corresponding true location.
\end{enumerate}

For behavior prediction, we report a weighted classification accuracy (W.A.) over the 3 class labels: $\{ overspeeding, neutral, braking\}$. 
\vspace{-5pt}

\subsubsection{Methods} 

We compare our approach with SOTA trajectory prediction approaches for road-agents. Our definition of SOTA is not limited to ADE/FDE values. We consider SOTA additionally with respect to the deep learning architecture used in a different approach.
Combined, our basis for selecting SOTA methods not only evaluates the ADE/FDE scores of our approach but also evaluates the benefits of using the two-stream network versus other deep learning-based architectures.

\begin{itemize}[noitemsep]
    \item Enc-Dec: This method is based on a standard encoder-decoder architecture similar to the Seq2Seq model~\cite{rnn1}.
    \item Deo et al.~\cite{nachiket} (CS-LSTM): This method combines CNNs with LSTMs to perform trajectory prediction on U.S. highways.
    \item Chandra et al.~\cite{traphic} (TraPHic): This approach also uses a CNN + LSTM approach along with spatial attention-based pooling to perform trajectory prediction of road-agents in dense and heterogeneous traffic.
    \item Gupta et al.~\cite{social-gan} (Social-GAN): This GAN-based trajectory prediction approach is originally trained on pedestrian crowd datasets. The method uses the encoder-decoder architecture to act as the generator and trains an additional encoder as the discriminator.   
    \item Li et al.~\cite{li2019grip} (GRIP): This is a graph-based trajectory prediction approach that replaces standard CNNs with graph convolutions and combines GCNs with an encoder-decoder framework. 
\end{itemize}

We use the publicly available implementations for CS-LSTM, TraPHic, and Social-GAN, and train the entire model on all three datasets. We performed hyper-parameter tuning on all three methods and reported the best results. We implement GRIP and Enc-Dec from scratch due to the absence of standard implementations.
\begin{figure*}
  \begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{LYFT_orig.png}
    \caption{Lyft Ground-Truth with $\lambda$=$0.00015$.}
    \label{fig:1}
  \end{subfigure}
\begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{ARGO_orig.png}
    \caption{Argoverse Ground-Truth with $\lambda$=$0.00025$.}
    \label{fig:1}
  \end{subfigure}
\begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{APOL_orig.png}
    \caption{Apolloscape Ground-Truth with $\lambda$=$0.0005$.}
    \label{fig:2}
  \end{subfigure}

  \begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{LYFT_pred.png}
    \caption{Lyft Behavior Predictions.}
    \label{fig:1}
  \end{subfigure}
\begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{ARGO_pred.png}
    \caption{Argoverse Behavior Predictions.}
    \label{fig:1}
  \end{subfigure}
\begin{subfigure}[h]{0.34\textwidth}
    \includegraphics[width=\textwidth]{APOL_pred.png}
    \caption{Apolloscape Behavior Predictions.}
    \label{fig:2}
  \end{subfigure}
  \caption{\textbf{Behavior Prediction Results:} We classify the three behaviors-- overspeeding(blue), neutral(green), and braking(red), for all road-agents in one traffic video from the Lyft, Argoverse, and Apolloscape datasets, respectively. The y-axis shows $\theta^{'}$ and the x-axis denotes the road-agents. We follow the behavior prediction protocol described in Section~\ref{subsec: behavior_protocol}. Each figure in the top row represents the ground-truth labels for a dataset, and the corresponding bottom figure shows the predicted labels on that dataset. In our experiments, we set $\lambda = \lambda_1 = \lambda_2$.}
  \label{fig: behavior}
  \vspace{-15pt}
\end{figure*}
 
\subsection{Analysis and Discussion}
\label{subsec: analysis_and_discussion}


We first analyse the results in Table~\ref{tab: accuracy} followed by a discussion on the spectral cluster regularization method. We conclude by presenting the results of behavior prediction and discussing some odd artifacts.

\subsubsection{Trajectory Prediction Evaluation}
We compare the ADE and FDE scores of our predicted trajectories with prior methods in Table~\ref{tab: accuracy} and show qualitative results in Figure~\ref{fig: qual}. We compare with several SOTA trajectory prediction methods and reduce the average RMSE by at least 54\% with respect to the next best method. There are several interesting observations:

\textbf{RMSE of TraPHic and CS-LSTM:} The RMSE of both TraPHic and CS-LSTM increases sharply throughout the prediction window (Figure~\ref{fig: rmse}). While not conclusive, the reason for this poor performance may be attributed to the fact that the success of both approaches requires convolutions in a heuristic local neighborhood. The size of the neighborhood is specifically adjusted to the dataset that each method is trained on. We use the default neighborhood parameters provided in the publicly available implementations, and apply them to the Lyft, Argoverse, and Apolloscape datasets.


\textbf{Effect of Loss Functions:} The encoder-decoder network in both Enc-Dec and GRIP is nearly identical to stream 1 of our approach except that they are trained with the mean square error (MSE) loss function instead of the negative log-likelihood (NLL) loss function in Equation~\ref{eq: original_loss}. The results (Table~\ref{tab: accuracy}, Figures~\ref{fig: rmse_lyft} and~\ref{fig: rmse_argo}) show that minimizing the NLL loss is on average better than minimizing the MSE loss. 


The difference in results for GRIP may also be attributed to the fact that Li et al.~\cite{li2019grip} do not use the Laplacian matrix for graph operations, rather they use a normalized version of the adjacency matrix, which does not contain spectral information.

\textbf{Traffic Density Bias in TraPHic:} Chandra et al.~\cite{traphic} show that while TraPHic is SOTA on dense and heterogeneous traffic datasets, it does not outperform prior methods including CS-LSTM and Social-GAN on low or moderately dense datasets. This is supported through our results as well, where TraPHic performs poorly on all three datasets. This leads us to believe TraPHic is biased towards datasets with a specific traffic density.

\textbf{Failure of Social-GAN:} We observe that Social-GAN does not converge beyond the prediction of the first few time-steps. While at this point we do not have a conclusive reason for this failure, we hypothesize that it may be due to the fact that the scale of pedestrian trajectories differs significantly from the scale of road-agent trajectories. 
\subsubsection{Improving Long-Term Prediction by Regularization}

To highlight the benefit of the spectral cluster regularization on long-term prediction, we remove the second stream and only train the first stream with the original loss function ( equation~\ref{eq: original_loss}). Our results (Table~\ref{tab: accuracy}, last four columns) show that regularizing stream 1 reduces the FDE by up to 83\%. 

Figure~\ref{fig: rmse} shows that, in the presence of regularization, the RMSE reduces across the entire prediction window in all three datasets. Despite this positive result, the effect of regularization is not so strong on the Lyft and Argoverse datasets. On the Apolloscape dataset, however, the RMSE decreases significantly (76\% ADE and 82\% FDE) with every time-step in the prediction window. We conclude that spectral cluster regularization reduces long-term prediction error in general, but further experiments are required to confirm the range of its benefits.
\subsubsection{Behavior Prediction Results} 
We observe a weighted accuracy of 92.96\% on the Lyft dataset, 84.11\% on the Argoverse dataset, and 96.72\% on the Apolloscape dataset. Figure~\ref{fig: behavior} shows the results of our behavior prediction algorithm for one video, each from the Lyft, Argoverse, and Apolloscape datasets, respectively. We plot the value of $\theta^{'}$ on the vertical axis and the road-agent I.D.s on the horizontal axis. Road-agents towards the end of the x-axis appear late in the traffic video while road-agents at the beginning of the x-axis appear early in the video. We follow the behavior prediction algorithm described in Section~\ref{tab: behavior_protocol}. The values for $\lambda_1$ and $\lambda_2$ are based on the ground truth labels and are hidden from the test set. 

Interestingly, the variation in behavior class labels decreases towards the end of the x-axis. This intuitively makes sense as $\theta^{'}$ for a road-agent depends on the number of distinct neighbors that it observes. This is difficult for road-agents towards the end of the traffic video.
\subsubsection{Odd Artifacts}
Although we achieve SOTA RMSE in road-agent trajectory prediction, we also note some inconsistent results in our experiments. For example, in Figure~\ref{fig: rmse_lyft} and Figure~\ref{fig: rmse_argo}, the RMSE values of Enc-Dec oscillates at the beginning of the prediction window. And in Figure~\ref{fig: rmse_argo}, the RMSE of GRIP mysteriously drops to zero at the end of the prediction window. These artifacts may be explained once the original standard implementations of GRIP and Enc-Dec are made available.



\subsection{Training Details}
\label{subsec: implementation}
We use 2 Nvidia GeForce RTX 2080 Ti GPUs with 12GB memory each, for all experiments. Initially, we trained both streams together for 20 epochs. However, we found that by training stream one first for 20 epochs, and then training both streams for another five epochs generated the best results, reported in Table~\ref{tab: accuracy}.

Due to the computations involved in obtaining the Laplacian matrices and their corresponding eigenvectors, data processing for stream two is both time-consuming and expensive in terms of computational resources. Consequently, we choose $6310$, $5126$, and $5073$ valid trajectories to form the training set, and $769$, $1678$ and $1012$ valid trajectories to form the testing set for the Lyft, Argoverse and Apolloscape datasets, respectively. We consider the trajectory for a road-agent to be valid if that road-agent is present for at least 8 seconds (3 observation + 5 prediction ) in Lyft and Apolloscape. In Argoverse, we only use the first 2 seconds as an observed trajectory to predict the next 3 seconds since each video in Argoverse is limited to 5 seconds. 





\textbf{Stream 1: }The input to stream one consists of trajectory tensors of size $B\times T \times 2$, with $B=128$ representing the batch size, $T=3$ seconds denoting the length of the observed trajectory, and 2-dimensional spatial coordinates. In stream 1, our training converges in 20 epochs in approximately 20 minutes for each data set. The best results from stream one are obtained by using the RMSprop optimizer~\cite{rmsprop} with a learning rate of $0.001$.

\textbf{Stream 2: }The input to stream 2 consists of a sequence of eigenvectors represented as a tensor of size $k \times B \times T \times N$, where $k$ denotes the number of eigenvectors for each Laplacian matrix, $N$ is the number of road-agents in the traffic-video. It takes approximately 42 hours per epoch to process the data for stream 2. Therefore, we pre-compute the input data for stream two offline, which reduces the training time to 2.2, 1.5, and 1.2 hours per epoch for Lyft, Argoverse, and Apolloscape, respectively. The hyper-parameters for stream 2 are identical to those used in stream 1.
\vspace{-5pt}
 \section{Conclusion, Limitations, and Future Work}

We present a unified algorithm for trajectory prediction and behavior prediction of road-agents. Our approach represents traffic through weighted traffic-graphs. We use a two-stream LSTM network in which the first stream predicts the trajectories, while the second stream predicts the behavior of road-agents. We also present a regularization algorithm to reduce long-term prediction errors.

Our method has some limitations. Currently, we use only one feature to design our behavior prediction model, which may not be able to generalize to new traffic scenarios. In addition, our training is slow and takes several hours due to the number of computations required for computing the traffic-graphs and corresponding Laplacian matrices. As part of our future work, we plan to make our behavior prediction model data-driven, rather than rule-based. We will also optimize our implementation using GPU parallelization to improve the runtime. 



{\small
\bibliographystyle{ieee_fullname}
\bibliography{refs}
}
\clearpage
\section{Data Preprocessing}
\label{sec: AppendixA}
We include all data and code used for data preprocessing with the supplementary material. Our data structure format (See Section~\ref{sec: dat_structures}) includes the time-stamp, road-agent ID, and the road-agent's spatial coordinates in the world coordinate frame. The process of obtaining these attributes, and utilizing them, to construct the data structures differs for all three datasets -- Lyft, Apolloscape, and Argoverse. We converted the three datasets to one unique representation that includes frame$\textunderscore$ID, road-agent$\textunderscore$ID, X, Y, dataset$\textunderscore$ID. 








\subsection{Metadata}
To the best of our knowledge, there is very little known prior work using these datasets as they are relatively recent. As such, the raw datasets are not trivial to work with, due to their large size. In particular, to understand the performance, and gain interpretability, of an approach on a dataset, it is essential to study the underlying meta-features of each dataset. In Table~\ref{tab: meta}, we list some descriptive meta-features of the datasets. Additionally in this work, we also release code for efficient implementations for several key operations such as storage, extraction, querying, addition, and deletion on these datasets to facilitate future research using these datasets.

\begin{table}[h]
\centering
\begin{tabular}{lcc} 
\toprule
Dataset & Batch & Avg. Density  \\
\hline

\multirow{3}{*}{Lyft} & Train & \Tstrut 0.80 \\
           & Val & 0.83 \\
           & Test & 0.84 \\                    
\midrule
\multirow{3}{*}{Argoverse} & Train & \Tstrut  0.75 \\
                           & Val & 0.67 \\
                           & Test & 1.67 \\


\midrule                           
\multirow{3}{*}{Apolloscape} & Train & \Tstrut  3.49 \\
                            & Val & 3.50 \\
                            & Test & 2.56 \\                         
            








\bottomrule
\end{tabular}
\caption{\textbf{Meta-Feature information} for the Lyft Level 5, Apolloscape, and the Argoverse datasets. The average density is reported by measuring the average number of road-agents per frame.}
\label{tab: meta}
\end{table}


%
 

\begin{figure}[t]
\includegraphics[width=\columnwidth]{adjacency.png}
    \caption{\textbf{Schematic:} Data structure for Adjacency matrices.}
\label{fig:adjacency}
\end{figure}
\section{Data Structures}
\label{sec: dat_structures}
This section describes the different data structures that are used in our approach. The implementations of these data structures are included with the code. 

\subsection{Adjacency Matrices}
Figure~\ref{fig:adjacency} shows a schematic for the data structure used to create the adjacency matrices for the whole dataset. The adjacency matrix corresponding to a traffic graph captures the interactions between all agents within that frame. A python list is used to store adjacency data of each traffic video, where each video is again a list of all frames in that dataset. Each frame is a python dictionary containing the `dataset$\textunderscore$ID', `frame$\textunderscore$ID' and `adj$\textunderscore$matrix'. Each adjacency matrix is an array of size $ N \times N$, where $N$ is the total number of agents in the dataset. The adjacency matrices are used to form the Laplacian matrices that are updated at every time-step according to equation $3$.

\subsection{Input for Stream 1}
Figure \ref{fig:stream1} shows the schematic for the data structure used to prepare the input for stream 1. The implementation consists of a python list of dictionaries. Each dictionary denoted as item$\textunderscore$1, item$\textunderscore$2....item$\textunderscore$n in Figure~\ref{fig:stream1}, has three keys- 'dataset$\textunderscore$ID', 'agent$\textunderscore$ID', and 'sequence'. The value of the 'sequence' consists of an array of size $n \times 2$, where $n$ is the length of either the observation sequence or prediction sequence. Each row of this sequence array consists of the global $X,Y$ coordinates, respectively, of the road-agent at that observation or prediction time step.  

\begin{figure}[h]
    \includegraphics[width=\columnwidth]{stream1.png}
    \caption{\textbf{Schematic:} Data structure for the input to Stream1.}
\label{fig:stream1}
\end{figure}

\subsection{Input for Stream 2}
Figure~\ref{fig:stream2} shows the schematic for the data structure used to prepare the input for stream 2. This data structure is similar to the stream1 data structure in that it also consists of  a list of dictionaries. Each dictionary has the keys `dataset$\textunderscore$ID', `agent$\textunderscore$ID', `mean$\textunderscore$theta$\textunderscore$hat', `mean$\textunderscore$theta', and $F_1,F_{2}, \ldots, F_{n}$, where $n$ is the length of the observation sequence or prediction sequence, respectively. Each $F_i$ represents the $i^{\textrm{th}}$ frame which is an array of size $2 \times N$, where $N$ denotes the total number of road-agents in that traffic video. The columns of this array stores the global $X,Y$ coordinates of all the road-agents at $i^{\textrm{th}}$ frame. The keys, `mean$\textunderscore$theta$\textunderscore$hat' and `mean$\textunderscore$theta' store information corresponding to the ground-truth behavior labels for that sequence of frames.

\begin{figure}[h]
\includegraphics[width=\columnwidth]{stream2.png}
    \caption{\textbf{Schematic:} Data structure for the input to Stream2.}
\label{fig:stream2}
\end{figure}



 \section{Motivation for Equation 3}

Our intuition is that a road-agent should remember the interactions with not just its current neighbors, but all neighbors it has observed up until current time $t$. Therefore, the first term on the RHS stores the information of road-agents uptil time $t$, while the second term on the RHS contains the information of the newly observed road-agents at time $t+1$.






\section{Analysis of Stream 2 Continued}

In Section 4.2, we presented a behavior prediction algorithm that begins by forming the set of predicted spectrums from the second stream, $\mc{U}= \{ U_{T+1}, U_{T+2}, \dots, U_{t+\tau} \}$. The success of the algorithm depends on the accuracy of these predictions, that further depends on the amount of correlation existing between the sequence of eigenvectors. In Section 4.3, we proved an upper bound for the error distance between the $j^{\textrm{th}}$ eigenvectors of $L_t$ and $L_{t+1}$, denoted as $\phi_j$. We showed that $\phi_j = \bigO{\sqrt{n} \delta_{max}}$, where $\delta_{max}$ is the maximum component of $\delta_t$.

An alternative approach to computing the spectrums $\{U_{T+1}, \ldots, U_{T+\tau}\}$ is to first form traffic-graphs from the predicted trajectory given as the output from the stream 1. Next, obtain the corresponding Laplacian matrices for these traffic-graphs. Finally, use standard eigenvalue algorithms to compute the spectrum sequence. This is, however, a relatively sub-optimal approach as in this case, $\phi = \bigO{n L_{max}}$, with $L_{max} \gg \delta_{max}$.

\section{Additional Related Work}

In Section 6, we compared our approach against several prominent SOTA deep learning-based trajectory prediction algorithms. However, it is worth noting that there are other additional methods in the trajectory prediction literature that we have not considered in this present work. Social-LSTM~\cite{social-lstm} is a popular approach for trajectory prediction of pedestrians in crowds. However, in the interest of clarity, we provide here an explanation regarding its exclusion from the experimental setup:

\begin{itemize}
    \item The official implementation of Social-LSTM has been recently withdrawn from public access. Therefore, any results obtained via custom implementations would not offer an unbiased view of the method compared to prior works, that have used the official implementation before its removal.
    
    \item Instead of presenting a somewhat biased comparisons with a well-known pedestrian trajectory prediction method, we compare with CS-LSTM~\cite{nachiket}, which is a slight modification of the Social-LSTM method for road-agent trajectory prediction.
\end{itemize}


\section{Training Details of Comparison Methods}

\begin{itemize}
    \item \textbf{TraPHic \& CS-LSTM}: Implementations of both methods can be found in \cite{chandra2019robusttp}. On the Lyft, Argoverse, and Apolloscape datasets, we use NLL loss and MSE loss for pretraining and training. We perform a hyperparameter grid search with 10-40 epochs of batch size $64$ and $128$. We use the adam, adamax, Rprop, and RMSprop optimizers, with learning rates of $0.01$, $0.001$, and $0.005$, respectively, and dropouts between $0.4-0.6$. In TraPHic, we get the best result on Lyft and Argoverse with the adamax optimizer with a dropout of $0.4$ and $0.5$, respectively, and on Apolloscape with the adam optimizer with a dropout of $0.5$. In CS-LSTM, our best result on Lyft is obtained with the adamax optimizer with a dropout of $0.4$, and on Argoverse and Apolloscape with the adam optimizer with a dropout of $0.5$. However, compared to other methods, these two methods perform poorly as time increases.
    
    \item \textbf{Social-GAN}: We use the standard implementation in~\cite{social-gan}. On each of the three datasets, our hyperparameter grid search ranges from $1500-3000$ epochs of batch size $32$, using adam, adamax, Rprop, and RMSprop optimizers, with a learning rate of $0.001$, $0.0001$ and $0.0005$, respectively, on both generator and discriminator. We achieve the best result with the adamax optimizer on Lyft and Argoverse, and adam optimizer on Apolloscape, all with a learning rate of $0.0005$. Due to limited GPU memory, we were not able to experiment on larger batch sizes. Since Social-GAN is a pedestrian trajectory prediction approach, we scaled down the trajectories in all three datasets by a factor of $20$ to resemble the trajectories of pedestrians. Even so, we achieve unstable results during testing.


    \item \textbf{Enc-Dec}: We implement this method from scratch, similar to the implementation of stream 1 in our two-stream network. The key difference is that we train Enc-Dec with the MSE loss whereas we train our two stream network with the NLL loss function. We obtain optimum results for Enc-Dec after $50$ epochs with batch size of $40$ and learning rate of $0.001$.

    \item \textbf{GRIP}: We also implement this method following the approach in~\cite{li2019grip}. We obtain the best results with batch size of $128$ and learning rate of $0.001$. 


    
    
    

\end{itemize}








 
\end{document}
