\documentclass[onecolumn, draft, 12pt]{IEEEtran}
\usepackage{amsmath,cases}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage[final]{graphicx}
\usepackage{multirow}
\allowdisplaybreaks

\newcommand{\sign}{\ensuremath{\sigma_n^2}}
\newcommand{\sigv}{\ensuremath{\sigma_v^2}}
\newcommand{\sigx}{\ensuremath{\sigma_X^2}}
\newcommand{\w}{\ensuremath{\omega}}
\newcommand{\vrho}{\ensuremath{\varrho}}
\newcommand{\gm}{\ensuremath{\gamma}}

\newcommand{\E}{{\rm E}}
\newcommand{\Cs}{\mathbb{B}}
\newcommand{\Ni}{\mathbb{N}_{i}}
\newcommand{\Ns}{\mathbb{N}}
\newcommand{\Bs}{\Cs}
\newcommand{\Us}{\mathbb{U}}
\newcommand{\Vs}{\mathbb{V}}
\newcommand{\Cprime}{\Cs^{'}}
\newcommand{\Ss}{\Cs}
\newcommand{\G}{\mathbb{G}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rs}{\mathbb{R}^{M}}
\newcommand{\Rn}{\mathbb{R}^{N}}
\newcommand{\Rnm}{\mathbb{R}^{(N-1)}}
\newcommand{\Rmn}{\mathbb{R}^{N \times (N-1) }}
\newcommand{\Rnmm}{\mathbb{R}^{(N-1) \times (N-1)}}
\newcommand{\Rxc}{\mathbb{R}^{N} \setminus \Cs}
\newcommand{\RXC}{\mathbb{R}^{N} \setminus \Cs}
\newcommand{\mse}{\xi_{_{N}}}

\newcommand{\nld}{\rm NLC}
\newcommand{\rnld}{\rm RC}
\newcommand{\anld}{\mathcal{A - ND}}
\newcommand{\Lg}{\mathcal{L}}
\newcommand{\Lone}{\mathcal{L}_{1}}
\newcommand{\Ltwo}{\mathcal{L}_{2}}
\newcommand{\dmp}{\mathcal{X}}

\newcommand{\onevect}{\mathbf{1}}
\newcommand{\onevectT}{\mathbf{1}^{\mathrm{T}}}

\newcommand{\La}{\mathbf{L}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\Gm}{\mathbf{G}}
\newcommand{\Mm}{\mathbf{M}}
\newcommand{\Pm}{\mathbf{M}}
\newcommand{\Rm}{\mathbf{R}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\Qm}{\mathbf{Q}}
\newcommand{\Jm}{\frac{\onevect \onevectT }{N}}

\newcommand{\e}{\mathbf{e}}
\newcommand{\heta}{\boldsymbol\eta}
\newcommand{\Lt}{\mathbf{L}(t)}
\newcommand{\Ht}{\mathbf{H}_{t}}
\newcommand{\HXt}{\mathbf{H}_{t}(\mathbf{X})}
\newcommand{\HX}{\mathbf{H}}
\newcommand{\Hheta}{\mathbf{\HX}(\heta(t))}

\newcommand{\Xc}{\mathbf{x}_{\Cs}}
\newcommand{\Xcp}{\mathbf{x}_{\Cs \perp}}
\newcommand{\hXc}{\mathbf{h}_{\Cs}(\mathbf{x})}
\newcommand{\hXcp}{\mathbf{h}_{\Cs \perp}(\mathbf{x})}

\newcommand{\mXc}{\mathbf{\boldsymbol\mu}_{\Cs}(\mathbf{x})}
\newcommand{\mXcp}{\mathbf{\boldsymbol\mu}_{\Cs \perp}(\mathbf{x})}

\newcommand{\U}{\mathbf{U}}
\newcommand{\Sgma}{\mathbf{\Sigma}}
\newcommand{\Lmda}{\mathbf{\Lambda}}
\newcommand{\UT}{\mathbf{U}^{\mathrm{T}}}
\newcommand{\XPX}{{\mathbf{x}}^{\mathrm{T}} \mathbf{M} \mathbf{x}}
\newcommand{\XTMXt}{{\mathbf{X}(t+1)}^{\mathrm{T}} \mathbf{M} \mathbf{X}(t+1)}
\newcommand{\YMY}{{\mathbf{Y}}^{\mathrm{T}} \mathbf{M} \mathbf{Y}}
\newcommand{\Czero}{\mathbf{C}}
\newcommand{\CS}{\Czero_{\rm nlc}}
\newcommand{\CSR}{\Czero_{\rm RC}}
\newcommand{\CSL}{\Czero_{\rm lin}}

\newcommand{\So}{\mathbf{S}}
\newcommand{\Ctwotheta}{\Ctwo^{\theta_0}}
\newcommand{\Stheta}{\So^{\theta_0}}
\newcommand{\fiN}{\mathbf{\Phi}}
\newcommand{\fiNT}{\mathbf{\Phi}^{\mathrm{T}}}
\newcommand{\thetanotone}{\theta_0 \mathbf{1}}
\newcommand{\thetaone}{\cval \mathbf{1}}
\newcommand{\xbarone}{\bar x \mathbf{1}}

\newcommand{\xbar}{\bar{x}}
\newcommand{\onexbar}{\bar{x} \onevect}
\newcommand{\Xbar}{\onexbar}
\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\xX}{\mathbf{x}}
\newcommand{\Y}{\mathbf{y}}
\newcommand{\Z}{\mathbf{z}}
\newcommand{\Lbar}{\bar{\mathbf{L}}}

\newcommand{\cval}{\ensuremath{\theta^{*}}}
\newcommand{\Xstar}{\ensuremath{\X^{*}}}

\newcommand{\mx}{\mathbf{\boldsymbol\mu}(\mathbf{x})}
\newcommand{\mX}{\mathbf{\boldsymbol\mu}(\mathbf{X})}
\newcommand{\mXt}{\mathbf{\boldsymbol\mu}(\mathbf{X}(t))}


\newcommand{\hatx}{\hat{x}}
\newcommand{\hatX}{\hat{\mathbf{x}}}
\newcommand{\hX}{\mathbf{h}(\mathbf{x})}
\newcommand{\hXt}{\mathbf{h}(\mathbf{X}(t))}
\newcommand{\nt}{\mathbf{n}(t)}
\newcommand{\nv}{\mathbf{n}}
\newcommand{\gf}{\tilde{\mathbf{f}}}
\newcommand{\vX}{ V(\mathbf{x})}
\newcommand{\vXt}{ V(t,\mathbf{x})}
\newcommand{\vtXt}{ V(\mathbf{X}(t))}
\newcommand{\vecXtone}{ V(\mathbf{X}(t+1))}
\newcommand{\fitX}{ \varphi(\mathbf{x})}
\newcommand{\vecxt}{\mathbf{X}(t)}
\newcommand{\vecxini}{\mathbf{X}(0)}
\newcommand{\vecyt}{\mathbf{Y}(t)}

\newcommand{\Ltilde}{\tilde{\mathbf{L}}(t)}
\newcommand{\LtldX}{\tilde{\mathbf{L}}(t,\X)}
\newcommand{\ntilde}{\tilde{\mathbf{n}}}
\newcommand{\ntildet}{\tilde{\mathbf{n}}(t)}
\newcommand{\xtilde}{\tilde{x}}
\newcommand{\xtildet}{\tilde{x}(t)}
\newcommand{\Xtilde}{\tilde{\mathbf{X}}}
\newcommand{\Xtildet}{\tilde{\mathbf{X}}(t)}
\newcommand{\delX}{\delta{(\mathbf{X}})}
\newcommand{\delXt}{\delta{(\mathbf{X}(t)})}
\newcommand{\delXtilde}{\tilde{\delta}(\mathbf{X}) }
\newcommand{\delXtildet}{\tilde{\delta}(\mathbf{X}(t)) }
\newcommand{\lmN}{\lambda_{N}}
\newcommand{\itN}{1 \leq i \leq N}

\newtheorem{thm}{Theorem}
\newtheorem{prp}{Property}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem*{rem}{Remark}

\begin{document}
\title{Robust Consensus in the Presence of Impulsive Channel Noise}
\author{Sivaraman Dasarathan, Cihan Tepedelenlio\u{g}lu, \emph{Member, IEEE}, Mahesh Banavar, \emph{Member, IEEE} and Andreas Spanias, \emph{Fellow, IEEE}
\thanks{The authors are with the School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ 85287, USA. (Email: \{sdasarat, cihan, mbanavar, spanias\}@asu.edu). This work was supported in part by the National Science Foundation under Grants NSF FRP 1231034 and NSF CCSS 1307982.}
} \maketitle
 
\begin{abstract}
A distributed average consensus algorithm robust to a wide range of impulsive channel noise distributions is proposed. This work is the first of its kind in the literature to propose a consensus algorithm which relaxes the requirement of finite moments on the communication noise. It is shown that the nodes reach consensus asymptotically to a finite random variable whose expectation is the desired sample average of the initial observations with a variance that depends on the step size of the algorithm and the receiver nonlinear function. The asymptotic performance is characterized by deriving the asymptotic covariance matrix using results from stochastic approximation theory. Simulations corroborate our analytical findings and highlight the robustness of the proposed algorithm.
\end{abstract}
\begin{IEEEkeywords}
Distributed Consensus, Sensor Networks, Bounded Transmissions, Impulsive Noise, Asymptotic Covariance, Stochastic Approximation, Markov Processes.
\end{IEEEkeywords}

\section{Introduction} \label{sec:intro_btx_consensus_robust}
Wireless sensor networks (WSNs) without a fusion center have the advantages of robustness to node failures and being able to function autonomously without a central node controlling the entire network \cite{Sankarasubramaniam2002}. In such fully distributed networks, sensors collaborate with their neighbours by repeatedly exchanging information which they combine locally to achieve a desired global objective. For example, the sensors could come to an agreement on the sample average (or on a global function) of initial measurements. This is called distributed consensus. Distributed consensus algorithms have attracted significant interest in the recent past and have found several applications in areas such as healthcare, environmental monitoring, military and home appliances \cite{Boyd2003,Boyd2004,OlfatiSaber2003,OlfatiSaber2007,MinyiHuang2008,Oreshkin2008,KarMoura2009}. 

In existing literature on consensus in the presence of communication noise, the additive noise is always assumed to have finite moments \cite{Boyd2007,Touri2009,MinyiHuang2007,Pescosolido2008,Barbarossa2008,MinyiHuang2008,AysalBarner2010,Nedic2011cvx, KarMoura2009,KarMoura2007}. Sensor networks which operate in adverse conditions can be susceptible to impulsive noise distributions. For example, the aggregated interference at a desired node from its neighbouring nodes of a Poisson network is characterized by alpha-stable distribution which may not have finite mean or variance \cite{Sousa1992,Ilow1998,Yang2003,Hughes2000,Haenggi2009,Win2009,JunghoonLee2011,RajanTep2010}. Therefore there is a need to develop consensus algorithms which are robust to impulsive channel noise. Consensus with nonlinear combining at the receiver has been considered in \cite{KhanKar,OlfatiSaber2003,Ulrich2008,HuiWassim2008,WenwuChen2011,Ajorlou2011} only in the absence of inter-sensor communication noise. Therefore, it is of interest to solve the problem of distributed consensus with receiver nonlinearities that soft-limit the impulsive additive noise.

In this paper, we propose a robust consensus () algorithm which is robust to impulsive communication noise by soft-limiting at receiver sensor nodes before combining. We do not require the channel noise to have finite moments as is assumed in all the previous work on distributed average consensus algorithms \cite{Boyd2007,Touri2009,MinyiHuang2007,Pescosolido2008,Barbarossa2008,MinyiHuang2008,AysalBarner2010,Nedic2011cvx, KarMoura2009,KarMoura2007}. In addition, like in \cite{dastep2013}, we assume that every sensor maps its state value through a bounded function before transmission to respect a peak power constraint at every iteration making it ideal for resource-constrained WSNs. We prove that all the sensors employing the  algorithm reach consensus to a finite random variable whose mean is the desired sample average. We characterize the asymptotic performance by deriving the asymptotic covariance matrix using results from stochastic approximation theory. Finally, we explore the performance of the proposed algorithm employing various functions for the transmit and receiver non-linearities. Different from \cite{MinyiHuang2008,KarMoura2009} and \cite{KarMoura2007} which also considered consensus in the presence of noisy transmissions, herein we analyse nonlinear processing both at the transmit and receiver nodes and study the asymptotic covariance matrix and its dependence on both the power-constraining transmit nonlinearity, and the soft-limiting receive nonlinearity. It is shown that the norm of the asymptotic covariance matrix is limited by the Fisher information of the noise distribution with respect to a location parameter.

The rest of this paper is organized as follows. We begin by reviewing network graph theory in Section \ref{sec:review_spectral_robust}. In Section \ref{sec:consensus_no_noise_robust}, we describe the sensing and channel models and introduce the consensus problem. We consider the  algorithm in the presence of noise in Section \ref{sec:consensus_with_noise_robust}, and prove that the sensors reach consensus to a random variable. In Section  \ref{sec:simulations_nld_robust}, we present several simulation examples to study the performance of the proposed algorithm. Concluding remarks are presented in Section \ref{Sec:Conclusions:consensus_robust}. 

\subsection*{Notations and Conventions}\label{subsec:nld_notations_robust}
Vectors are denoted by boldface upper-case or lower-case letters and matrices are denoted by boldface upper-case letters.  denotes the maximum of  and .  denotes an  diagonal matrix whose diagonal elements are given by .  denotes the expectation operator. The symbol  denotes the  norm for vectors and spectral norm for symmetric matrices. For a symmetric matrix , , , denotes the  smallest eigenvalue, , and  denotes the identity matrix. 

\section{Review of Network Graph Theory} \label{sec:review_spectral_robust}
In this section, we provide a brief background on network graph theory. Consider an undirected graph  containing a set of nodes  and a set of edges . Nodes that communicate with each other have an edge between them. We denote the set of neighbours of node  by ,  where  indicates an edge between the nodes  and  \cite{chung}. A graph is connected if there exists at least one path between every pair of nodes. We denote the number of neighbours of a node  by  and . The graph structure is described by an  symmetric matrix called the adjacency matrix , whose  element  if . The diagonal matrix  captures the degrees of all the nodes in the network. The Laplacian matrix of the graph is defined as . The graph Laplacian characterises a number of useful properties of the graph. The eigenvalues of  are non-negative and the number of zero eigenvalues denotes the number of distinct components of the graph. When the graph is connected, , and ,  so that the rank of  for a connected graph is . The vector  is the eigenvector of  associated with the eigenvalue , i.e, . The eigenvalue  characterizes how densely the graph is connected and the performance of consensus algorithms depend on this eigenvalue \cite{OlfatiSaber2004}.

\section{Sensing and Channel Model} \label{sec:consensus_no_noise_robust}
\subsection{Sensing Model}\label{subsec:nld_sys_model_robust}
Consider a WSN with  sensor nodes each with an initial measurement , . Measurements made at the sensor nodes are modeled as

where  is an unknown real-valued parameter and  is the sensing noise at the  sensor. For many distributions on , the sample mean of these initial measurements is the maximum likelihood estimate of :

We would like to design an iterative distributed algorithm, in which each sensor communicates only with its neighbours and each sensor has a state that converges to . If the states of all the sensor nodes converge to  , then the network is said to have reached \emph{consensus} on the sample average.  

\subsection{Channel Model}\label{subsec:nld_chn_model_robust}

Each sensor can transmit or receive information to or from its neighbours. When a sensor transmits its state information, it can send a function of its state instead of the state itself. In this link there is additive noise at the receiver node which can be modeled as


where , is the state value of the  node at time ;   is the power-constraining transmission function used at every node,  is the noise associated with the reception of , and  is the received signal at node  from node  at time . The existing linear consensus algorithms in \cite{Boyd2007,Touri2009,MinyiHuang2007,Pescosolido2008,Barbarossa2008,MinyiHuang2008,AysalBarner2010,Nedic2011cvx, KarMoura2009,KarMoura2007} require  to have finite moments. Instead, we assume that the noise samples  are mutually independent identically distributed (i.i.d.), symmetric real-valued with zero median (e.g., its PDF, when it exists, is symmetric about zero).

\section{Robust Consensus with Impulsive Communication Noise} \label{sec:consensus_with_noise_robust}
In this section, we propose a robust consensus algorithm in which every node performs a nonlinear operation by soft-limiting the noisy state information at the receiver node. The receiver non-linearity makes the algorithm robust to a wide range of heavy-tailed channel noise distributions. Also, at the transmitter side every sensor maps its state value through a bounded function before transmission to constrain the transmit power making it ideal for resource-constrained WSNs. 

\subsection{The  Algorithm with Communication Noise}\label{subsec:nld_with_noise_robust}
As discussed in \eqref{eq:nld_ch_noise_recvd_info}, each sensor maps its state value at time  through the function  before transmission, and combines the received state values through a nonlinear function  according to the following recursion: 
where , and , is the time index, and  is a positive step size which will be assumed to satisfy assumption \textbf{(A5)} in the sequel. The node  transmits its information  by mapping it through the function , node  receives a noisy signal . The function  is applied at the receiver side to combat the effect of impulsive channel noise  and will be further assumed to satisfy \textbf{(A2)} in the sequel.

We now compare the existing work on nonlinear consensus in \cite{KhanKar,OlfatiSaber2003,Ulrich2008,HuiWassim2008,WenwuChen2011,Ajorlou2011} against the proposed algorithm in \eqref{eq:nld_ch_noise_robust}. The algorithm in \cite{KhanKar} becomes a special case of \eqref{eq:nld_ch_noise_robust} with  and  in a setting with no channel noise (). The algorithm in \cite{OlfatiSaber2003} becomes a special case of \eqref{eq:nld_ch_noise_robust} with  and  being an increasing odd function. There is no communication noise assumed in all the existing work on consensus with nonlinear  \cite{KhanKar,OlfatiSaber2003,Ulrich2008,HuiWassim2008,WenwuChen2011,Ajorlou2011} whereas we consider herein the communication noise in the presence of both the transmit and receive non-linearities. Moreover, with the transmit non-linearity , the transmit power from all the sensors are always bounded which is a desirable feature for power constrained WSNs. The  algorithm considered in \cite{dastep2013} is a special case of \eqref{eq:nld_ch_noise_robust} with  but assumes noise samples have finite moments, and fails in the presence of impulsive channel noise.

We make the following assumptions on , , ,  and the graph: 
\\
\textbf{Assumptions \;}\\
\textbf{(A1) Graph:\;} The graph  is undirected and connected so that  \cite{chung}.\\
\textbf{(A2) Receive Nonlinearity:\;} The function  is strictly increasing, odd and bounded.\\   
\textbf{(A3) Transmit Nonlinearity:\;} The function  is strictly increasing.\\ 
\textbf{(A4) Independent Noise Sequence:\;} The noise samples  are mutually i.i.d., symmetric real-valued with zero median (e.g., its PDF, when it exists, is symmetric about zero). \\  
\textbf{(A5) Decreasing Weight Sequence:\;} In order to control the variance growth rate of the cumulative noise we need the following standard conditions on the sequence :
 

Let  be such that  where  denotes the expectation with respect to any of the i.i.d.  so that . Here  is a noise process which depends on  and its randomness is due to the noise process , and satisfies . Let . Since  is bounded due to \textbf{(A2)},  is finite. Hence we have . Using the fact that  is a strictly increasing odd function and that  has the same distribution as  due to symmetry, it can be easily proved that  is a strictly increasing odd function satisfying . Using , the recursion in \eqref{eq:nld_ch_noise_robust} can be written as


The recursion in \eqref{eq:nld_ch_noise_robust_g_x} can be written in vector form as

where  is the state vector at time  given by , and  is a function with  element is given by

and . Due to the fact that  is odd and that the graph is connected, we have . The vector  in \eqref{eq:nld_vector_ch_noise_robust} captures the additive noise at  nodes contributed by their neighbours and their state values and its  component is given by

Clearly, conditioned on , the noise  is an independent sequence across time , and sensors  due to assumption \textbf{(A4)}. It also satisfies

Note that the inequality in \eqref{eq:assump_A44_robust} is because of \textbf{(A2)} and the fact that the number of neighbours of a given node is upper bounded by .

We will prove convergence of the  algorithm in Section \ref{subsec:conv_res_dmp_robust} and asymptotic normality in Section \ref{subsec:asym_norm_nld_robust}. We now present a result on the convergence of a discrete time Markov process which will be used in establishing convergence of the  algorithm. 

\subsection{A Result on the Convergence of Discrete time Markov Processes}\label{subsec:conv_res_dmp_robust}
Let  be a discrete time vector Markov process on . The generating operator  of  is defined as

for functions , for which the conditional expectation exists. Let  and its complement be . We now state the desired result as a simplification of Theorem 2.7.1 in \cite{Nevelson1973} (see also Theorem 1 in \cite{KarMoura2009}). In general  may depend on . 

\begin{thm} \label{nld_conv_dmp_res_thm_robust}
Let  be a discrete time vector Markov process with the generator operator  as in \eqref{eq:dmp_generator_robust}. If there exists a potential function , and  with the following properties


where ,  is such that

and

then, the discrete time vector Markov process  with arbitrary initial distribution converges almost surely (a.s.) to the set  as . That is,


\end{thm}

Intuitively, Theorem \ref{nld_conv_dmp_res_thm_robust} indicates that if the one-step prediction error of the Markov process evaluated at the potential function in \eqref{eq:dmp_generator_robust} is bounded as in \eqref{eq:nld_conv_dmp_res_thm4_robust} then it is possible to establish convergence of .

To prove the a.s. convergence of the consensus algorithm in \eqref{eq:nld_vector_ch_noise_robust} using Theorem \ref{nld_conv_dmp_res_thm_robust}, we choose the consensus subspace , the set of all vectors whose entries are of equal value as,


We are now ready to state the main result of Section \ref{sec:consensus_with_noise_robust}. But first, we start out with a preparatory lemma.

\begin{lem} \label{nld_lem_XMmx_positive} 
Define a positive semi-definite matrix  as the Laplacian of a fully connected graph: . Let , then .
\end{lem}
\begin{IEEEproof}
Consider

where we have used the fact that  in \eqref{eq:nld_thm_fitX_greater_than_zero3a_robust} to get \eqref{eq:nld_thm_fitX_greater_than_zero4_robust}. Expanding  using \eqref{eq:nld_mu_comp_robust}, we get 

Note that the  summation in \eqref{eq:nld_thm_fitX_greater_than_zero6_robust} corresponds to the  node. Now suppose that node  is connected to node . Then there exists a term  in the summation corresponding to the  node in \eqref{eq:nld_thm_fitX_greater_than_zero6_robust}, and a term  in the summation corresponding to the  node in \eqref{eq:nld_thm_fitX_greater_than_zero6_robust}. Both of these terms can be combined as  and this corresponds to the edge . Thus equation \eqref{eq:nld_thm_fitX_greater_than_zero6_robust} can be written as pairwise products enumerated over all the edges in the graph as follows

Since ,  in \eqref{eq:nld_thm_fitX_greater_than_zero7_robust} is positive due to the facts that  is strictly increasing and  is a strictly increasing odd function so that there is at least one term in the sum which is greater than zero and this completes the proof. 
\end{IEEEproof}

\begin{thm} \label{nld_thm_as_conv_fixed_graph_robust} Let the assumptions \textbf{(A1)}-\textbf{(A5)} hold. Consider the  algorithm in \eqref{eq:nld_vector_ch_noise_robust} with the initial state vector . Then, the state vector  in \eqref{eq:nld_vector_ch_noise_robust} approaches the consensus subspace  a.s., i.e.,


\end{thm}

\begin{IEEEproof}
We will make use of Theorem \ref{nld_conv_dmp_res_thm_robust} to prove \eqref{eq:nld_thm_as_conv_fixed_graph_robust}. We will choose an appropriate potential function  that is non-negative and satisfies equation \eqref{eq:nld_conv_dmp_res_thm1_robust}. We will then prove that the generating operator  applied on  as in \eqref{eq:dmp_generator_robust} can be upper bounded as in \eqref{eq:nld_conv_dmp_res_thm4_robust} with , , and a  will be chosen to satisfy \eqref{eq:nld_conv_dmp_res_thm4a_robust}. 

First we see that under the assumptions the discrete time vector process  in \eqref{eq:nld_vector_ch_noise_robust} is Markov. Let  be a positive semi-definite matrix as defined in Lemma \ref{nld_lem_XMmx_positive}. Let , then the function  is non-negative since  is a positive semi-definite matrix. Note that any  is an eigenvector of  associated with the zero eigenvalue, therefore we have
  
We have now verified that  satisfies the second condition in \eqref{eq:nld_conv_dmp_res_thm1_robust}. We now proceed to show the first condition. Let  where  is the orthogonal projection of  on . When , we have . Therefore, for any ,

where the last inequality is due to . The equations \eqref{eq:nld_thm_as_conv_fixed_graph2_robust} and \eqref{eq:nld_thm_as_conv_fixed_graph3_robust} establish that the conditions in \eqref{eq:nld_conv_dmp_res_thm1_robust} in Theorem \ref{nld_conv_dmp_res_thm_robust} are satisfied.

Let  and  be as defined in \eqref{eq:nld_mu_comp_robust}, and  be the orthogonal projection of  on . Then, , where  is non-zero, i.e.,  which is proved now. First we recall that  when  due to Lemma \ref{nld_lem_XMmx_positive}. This means  =  for . If  were zero, then  which contradicts with the fact that . Therefore,  is non-zero. Define , then , where the finiteness of  can be seen from the fact that  is bounded for all  because  is bounded due to , and by expressing  around  using Taylor's series and observing that the ratio  is finite as . 

Now we will prove that \eqref{eq:nld_conv_dmp_res_thm4_robust} is satisfied as well. Towards this end, consider  defined in \eqref{eq:dmp_generator_robust},

We get \eqref{eq:nld_conv_dmp_res_thm9_robust} by expanding \eqref{eq:nld_conv_dmp_res_thm8a_robust} and taking the expectations and using the fact that . We have 

where the second inequality follows from \eqref{eq:assump_A44_robust}. Using \eqref{eq:nld_thm_as_conv_fixed_graph11_robust} in \eqref{eq:nld_conv_dmp_res_thm9_robust}, we get the following bound

where we have used the fact  and  in \eqref{eq:nld_thm_as_conv_fixed_graph12_robust} to get \eqref{eq:nld_thm_as_conv_fixed_graph13_robust}. In \eqref{eq:nld_thm_as_conv_fixed_graph13_robust}, we have used the fact that  due to  \eqref{eq:nld_thm_as_conv_fixed_graph3_robust} and  to get \eqref{eq:nld_thm_as_conv_fixed_graph14_robust}. In \eqref{eq:nld_thm_as_conv_fixed_graph14_robust}, we defined  , and  to get \eqref{eq:nld_thm_as_conv_fixed_graph15_robust} and it is easy to see that . From  \eqref{eq:nld_thm_as_conv_fixed_graph15_robust}, due to the fact that  and letting , we get \eqref{eq:nld_thm_as_conv_fixed_graph16_robust}.

We will now prove that  in \eqref{eq:nld_thm_as_conv_fixed_graph16_robust} satisfies equation \eqref{eq:nld_conv_dmp_res_thm4a_robust} of Theorem \ref{nld_conv_dmp_res_thm_robust}.

Whenever , i.e., , then , which means , and hence . This implies that . From Lemma \ref{nld_lem_XMmx_positive}, it is immediate that  whenever . 

Letting  and by assumption \textbf{(A5)}, we see that the sequence  in \eqref{eq:nld_thm_as_conv_fixed_graph16_robust} satisfies \eqref{eq:nld_conv_dmp_res_thm6_robust}. Thus all the conditions of Theorem \ref{nld_conv_dmp_res_thm_robust} are satisfied to yield \eqref{eq:nld_thm_as_conv_fixed_graph_robust}.
\end{IEEEproof}

Theorem \ref{nld_thm_as_conv_fixed_graph_robust} states that the sample paths of  approach the consensus subspace almost surely. Now, like in \cite{KarMoura2009}, we will prove the convergence of  to a finite point in  in Theorem \ref{nld_conv_as_limiting_rv_robust}. 

\begin{thm} \label{nld_conv_as_limiting_rv_robust}
Let the assumptions of Theorem \ref{nld_thm_as_conv_fixed_graph_robust} hold. Consider the  algorithm in \eqref{eq:nld_vector_ch_noise_robust} with the initial state . Then, there exists a finite real random variable  such that



\end{thm}

\begin{IEEEproof}
Let the average of  be . It suffices to show that  is an  bounded martingale. A sequence of random variables  is called as a martingale if for all ,  and . The sequence  is an  bounded martingale if  (see \cite[pp. 110]{David1991}). Since , Theorem \ref{nld_thm_as_conv_fixed_graph_robust} implies,


where \eqref{eq:nld_conv_as_limiting_rv2_robust} follows from \eqref{eq:nld_thm_as_conv_fixed_graph_robust} since the infimum in \eqref{eq:nld_thm_as_conv_fixed_graph_robust} is achieved by . Pre-multiplying \eqref{eq:nld_vector_ch_noise_robust} by  on both sides and noting that  due to the symmetric structure of the graph we get, 


where . From \eqref{eq:assump_A44_robust} it follows that

which implies

Equation \eqref{eq:nld_conv_as_limiting_rv7_robust} together with \eqref{eq:nld_conv_as_limiting_rv3_robust} implies that the sequence  is an  bounded martingale and hence converges a.s. to a finite random variable  (see \cite[Theorem 2.6.1]{Nevelson1973}). Therefore the theorem follows from \eqref{eq:nld_conv_as_limiting_rv2_robust}.
\end{IEEEproof}

In what follows, we present the properties of the limiting random variable .

\subsection{Mean Square Error of  Algorithm}\label{subsec:mse_nld_robust}
Theorems \ref{nld_thm_as_conv_fixed_graph_robust} and \ref{nld_conv_as_limiting_rv_robust} establish that the sensors reach consensus asymptotically and converge a.s. to a finite random variable  . We can view  as an estimate of  . In the following theorem we characterize the bias and mean squared error (MSE) properties of .  We define the MSE of  as 

\begin{thm} \label{nld_limiting_rv_mse_robust}
Let  be the limiting random variable as in Theorem  \ref{nld_conv_as_limiting_rv_robust}. Then  is unbiased, , and its MSE is bounded, .
\end{thm}
The proof is obtained by following the same steps of the Lemma 5 in \cite{KarMoura2009}. 

We point out that with non-linear processing at both the transmitter and receiver nodes, we have obtained a similar bound on the MSE  as that of the linear consensus algorithm in \cite{KarMoura2009} but in our case the bound depends on the function  (see assumption \textbf{(A2)}) through  but does not depend on . Recall that  from \eqref{eq:assump_A44_robust} which implies that . Therefore, if  is finite for a large connected network, we have  and this means that  converges to  for large . If the graph is densely connected, then  is relatively high which increases the worst-case MSE. On the other hand, when the graph is densely connected,  is larger which aids in the speed of convergence to , as quantified through the covariance matrix in Section \ref{subsec:asym_norm_nld_robust}. 

\subsection{Asymptotic Normality of  Algorithm}\label{subsec:asym_norm_nld_robust} 

In this section, we establish the asymptotic normality of the  algorithm in \eqref{eq:nld_vector_ch_noise_robust}. Our approach here is similar to the one in \cite{MinyiHuang2008} and \cite{dastep2013}. Basically, we decompose the  algorithm in  into a scalar recursion and a recursion in . We now formally state and prove the result as a theorem. 

\begin{thm} \label{asym_norm_nld_lemma_robust}
Let , then the  algorithm in \eqref{eq:nld_vector_ch_noise_robust} becomes

Suppose that the assumptions \textbf{(A1)}-\textbf{(A5)} hold, and that the functions  and  are differentiable with , for some . Let the eigenvalue decomposition of  be given by , where  is a unitary matrix whose columns are the eigenvectors of  such that 
 where  is a stable diagonal matrix containing the  negative eigenvalues of  along its diagonal. In addition, let  be a realization of the random variable  and  is chosen such that  so that the matrix  is stable. Define  so that  and . Let  and , . Then, as ,


where 
\end{thm}
\begin{IEEEproof}
Define . From Theorem \ref{nld_conv_as_limiting_rv_robust}, we have   a.s. as  which implies that  a.s. as , and therefore  a.s. as . For a given , the error  can be written as the sum of two error components (see also Section VI in \cite{MinyiHuang2008}) as given below

Define  and  as the first and second terms in \eqref{eq:asym_norm_nld_error1_robust}. By calculating the covariance matrix between  and , it can be proved that they are asymptotically uncorrelated as , and that asymptotically  (see Theorem 12 in \cite{MinyiHuang2008}) where  is the variance of  as  which is calculated to be . To show that  is asymptotically normal, it suffices to show that   is asymptotically normal. To this end, we linearize  in \eqref{eq:asym_norm_nld_lemma1_robust} around  using Taylor's series expansion,


where the Jacobian matrix of  has  element given by 
.

Using \eqref{eq:asym_norm_nld_proof1a_robust} in \eqref{eq:asym_norm_nld_lemma1_robust} we get

Pre-multiplying \eqref{eq:asym_norm_nld_proof3_robust} on both sides by  and using \eqref{eq:asymp_norm_linear2_robust} we get the following recursions 

In \cite{Nevelson1973}, asymptotic normality  of a recursion similar to \eqref{eq:asym_norm_nld_proof5_robust} has been proved  under certain conditions. With the assumption that  is a stable matrix for , it can be verified that all the conditions of Theorem 6.6.1 in \cite[p. 147]{Nevelson1973} are satisfied for the process  in \eqref{eq:asym_norm_nld_proof5_robust}. Therefore, for a given , the process  is asymptotically normal with zero mean and covariance matrix given by \eqref{eq:asym_norm_nld_lemma7_robust}. Since  and using \eqref{eq:asym_norm_nld_lemma7_robust} together with the fact that  and  are asymptotically independent as , we get \eqref{eq:asym_norm_nld_lemma4_robust} which completes the proof.
\end{IEEEproof}

Equation \eqref{eq:asym_norm_nld_lemma4_robust} indicates how fast the process  will converge to  for a given . The convergence speed clearly depends on  and  which captures the effect of receiver and transmit non-linearities respectively. 

Let the asymptotic covariance in \eqref{eq:asym_norm_nld_lemma4_robust} be denoted by . Since  are asymptotically i.i.d. across space and time,  in \eqref{eq:asym_norm_nld_lemma7_robust} becomes  with  and thus we have  where  is a diagonal matrix whose diagonal elements are given by . A reasonable quantitative measure of largeness \cite{Polyak1981} of the asymptotic covariance matrix is   which is the maximum eigenvalue of the symmetric matrix . 

Further,  can be minimized with respect to the parameter  and this can be formulated as the following optimization problem,
 which can be solved analytically. The value of  that optimizes \eqref{eq:optimize_asymp_covariance_robust} is found to be  and the corresponding optimal value of the is given by

The best speed of convergence characterized by the asymptotic covariance depends on the point of convergence through . To select the optimal  that would result in the best speed of convergence for a given  and , knowledge of  is required. Since  unknown apriori, the performance characterized in \eqref{eq:asymp_covariance_value_robust} could serve as the benchmark for a given . In practice it may be possible for sensors to adapt the value of  as they converge towards the limiting value  to speed up the convergence, and approach this benchmark.  An optimized value for , also provides a simpler final expression for the asymptotic covariance in terms of its dependence on the receive nonlinearity  and transmit nonlinearity .

The size of the asymptotic covariance matrix in \eqref{eq:asymp_covariance_value_robust} is inversely proportional to the square of the smallest non-zero eigenvalue  which quantifies how densely a graph is connected. Even though the asymptotic covariance  has been derived in the literature \cite{MinyiHuang2008}, its optimization has not been considered. The optimization considered in \eqref{eq:optimize_asymp_covariance_robust} enables us to infer some interesting conclusions. In Table \ref{table1}, we have summarized the behavior of  for several graphs for large  \cite{deAbreu2007,ZhangDong2011,Olfati-Saber2007,Spielman2012}. For the fully connected graph,  goes to zero faster than the star graph and thus the former will converge faster than the latter. For the ring and line graphs, with large , the convergence will become slower since  increases with . For other graphs in Table \ref{table1}, the convergence speed is better compared to the line and ring graphs since  decreases with  for those graphs. 
\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|l|l|} 
\hline 
\multirow{1}{*} Type of Graph &  & Behavior of  \\ \hline
\multirow{1}{*} Fully Connected  &  &   \\ \hline
\multirow{1}{*} Star &  1 &  \\ \hline
\multirow{1}{*} Ring  & 4  &  \\ \hline
\multirow{1}{*} Line  & 4  &  \\ \hline
\multirow{1}{*} Tree (excluding star graphs) &  &  \\ \hline
\multirow{1}{*} Cubic Graph  & 2 &  \\ \hline
\multirow{1}{*} Planar  &  &  \\ \hline
\multirow{1}{*} Bipartite complete graph with  and  vertices  &  &  \\ \hline
\multirow{1}{*} k-regular (includes Ramanujan graphs)  &  &  \\ \hline
\multirow{1}{*} k-regular Lattice  &  &  \\ \hline
\end{tabular}
\caption{Behavior of  for some common graphs}
\label{table1}
\vspace{-0.4 in}
\end{center}
\end{table}
It is also interesting to note that the minimization of \eqref{eq:asymp_covariance_value_robust} with respect to the transmit and receive nonlinearities can be done separately and thus asymptotic covariance is an easier and helpful metric in optimizing the performance. The nonlinear receiver function  for which the ratio  is smaller will be better in terms of speed of convergence. For example, if  is Laplacian distributed with variance of 2 and if , then  whereas if we choose , we have  indicating  will perform better than the linear case. This is due to the fact that Laplacian is a heavy tailed distribution and therefore a bounded function such as  curtails the effect of outliers which does not happen when  is linear. Equation \eqref{eq:asymp_covariance_value_robust} also indicates when  is fixed, scaling  does not change the speed of convergence. We will illustrate these findings using simulations in Section \ref{sec:simulations_nld_robust}. 

When  is a bounded function, from equation (8) in \cite{chinesephysics} we have

where  is the Fisher information of  with respect to a location parameter \cite[(8)]{zamir} and thus we see an interesting relationship between the maximum eigenvalue of the asymptotic covariance and the Fisher information. For any , the best choice of  is the one that achieves equality in \eqref{eq:chinesephysics_robust}. For instance, when  is Gaussian,  achieves equality in \eqref{eq:chinesephysics_robust} in which case we have   equals the inverse of Fisher information. In addition, when  has finite moments, our  algorithm in \eqref{eq:nld_ch_noise_robust} subsumes the non-linear consensus algorithm discussed in \cite{dastep2013} with , and we get the same result as in \eqref{eq:asymp_covariance_value_robust} except  is replaced by the noise variance  defined in \cite{dastep2013}. Further, our model subsumes the linear case studied in \cite{MinyiHuang2008} with  and . 

\section{Simulations} \label{sec:simulations_nld_robust}
In this section, we corroborate our analytical findings through various simulations. In all the simulations presented, the initial samples  were generated randomly using Gaussian distribution with a standard deviation equal to 10. The desired global average value is indicated in each of the simulations. We focus here on bounded functions for both the transmit and receiver non-linearities to study their performance. 
 
\subsection{Performance of  algorithm with Channel Noise}\label{subsec:perf_nld_withs_noise_robust}
First, we highlight that the linear consensus algorithms in \cite{Boyd2007,Touri2009,MinyiHuang2007,Pescosolido2008,Barbarossa2008,MinyiHuang2008,AysalBarner2010,Nedic2011cvx, KarMoura2009,KarMoura2007} fail to achieve consensus when the channel noise does not have finite variance. An example plot is shown in Figure \ref{fig:Linear_Fails_Cauchy} for the case when the channel noise is Cauchy distributed with the scale parameter . Clearly, the sensors do not reach consensus. Whereas the proposed  algorithm works when we choose  as a nonlinear function as shown next. 

Figures \ref{fig:TxRx_Cauchy_TanxArcTanx_SmallWSN_Noisy} - \ref{fig:Difference_Between_VarTheta_Cov} illustrate the performance of  algorithm in the presence of communication noise. As explained in the assumption \textbf{(A5)} in Section \ref{subsec:nld_with_noise_robust}, we chose the decreasing step sequence to be , in all simulations. Here we assumed that  is the maximum power available at each sensor to transmit its state value. The receiver nonlinear function  is indicated in each case. Figure \ref{fig:TxRx_Cauchy_TanxArcTanx_SmallWSN_Noisy} shows that the nodes employing the  algorithm reach consensus for a small network with  in about  iterations and Figure \ref{fig:TxRx_Cauchy_TanxArcTanx_LargeWSN_Noisy} shows convergence for a large network with  in about  iterations.  

In Figures \ref{fig:Diff_WSN_Same_Fx_Hx_Cauchy}, \ref{fig:Rx_Tx_Scaling_Speed_SignxAbsx2} and \ref{fig:Diff_Noise_Distributions_Robustness3} we show the convergence speed performance of the proposed  algorithm by plotting the maximum eigenvalue of the covariance matrix of the vector process  versus iterations . These plots indicate how fast the process  converges towards the limiting value . 

The speed of convergence for two graphs with different algebraic connectivity is illustrated in Figure \ref{fig:Diff_WSN_Same_Fx_Hx_Cauchy}. We see that the graph with smaller connectivity (smaller ) converges slower than the one with large connectivity as dictated by  \eqref{eq:asymp_covariance_value_robust}. In Theorem \ref{asym_norm_nld_lemma_robust}, we also saw that scaling  does not change the asymptotic convergence speed. This is shown in Figure \ref{fig:Rx_Tx_Scaling_Speed_SignxAbsx2} where we see that when the iterations are large (), the speed of convergence of all the three functions are nearly the same. We depict the robustness of the  algorithm for various channel noise distributions in Figure \ref{fig:Diff_Noise_Distributions_Robustness3}. We observe that the performance is nearly the same for Gaussian and Laplacian distributions, whereas there is a significant gap between Cauchy and alpha-stable distributions considered in this simulation. The latter effect is due to the fact that, for a given , the ratio  is significantly different for those two cases which justifies the performance gap. Finally, we illustrate the difference between the variance of  and the asymptotic variance in Figure \ref{fig:Difference_Between_VarTheta_Cov}. Here we consider the evolution of the state value  of the first node for several consensus runs for the same initial conditions. Recall that in every consensus run the state value  converges to an instance of the limiting random variable  and the variation among these several realizations is characterized by the variance of . In contrast, how fast the state value  converges to the limiting value  is characterized by the asymptotic variance of  as .

\section{Conclusions} \label{Sec:Conclusions:consensus_robust}
A distributed average consensus algorithm that converges in the presence of impulsive noise is considered. Every sensor also maps its state value through a bounded function before transmission to constrain the transmit power. It is shown that non-linearity at the receiver nodes makes the algorithm robust to a wide range of channel noise distributions including heavy-tailed channel noise. The proposed algorithm relaxes the requirement of finite moments on the communication noise and thus it is proved to be not only more general than the existing consensus algorithms but is practically viable for WSNs deployed in adverse conditions. It is proved using the theory of Markov processes that the sensors reach consensus asymptotically on a finite random variable whose expectation contains the desired sample average of the initial sensor measurements, and whose mean-squared error is bounded. The asymptotic convergence speed of the proposed algorithm is characterized by deriving the asymptotic covariance matrix using results from stochastic approximation theory. It is shown that the norm of the asymptotic covariance matrix is limited by the Fisher information of the noise distribution with respect to a location parameter.

\bibliographystyle{IEEEtran}
\bibliography{consensus}


\newpage

\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.2cm,width=12cm]{./figs/Linear_Fails_Cauchy.eps} \caption{Linear consensus fails with impulsive channel noise, Entries of  versus Iterations : Cauchy noise, , , , .}\label{fig:Linear_Fails_Cauchy}
\end{center}
\end{minipage}
\end{figure}

\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.2cm,width=12cm]{./figs/TxRx_Cauchy_TanxArcTanx_SmallWSN_Noisy.eps} \caption{Convergence of  algorithm for a small Graph, Entries of  versus Iterations : Cauchy noise, , , ,  dB, .}\label{fig:TxRx_Cauchy_TanxArcTanx_SmallWSN_Noisy}
\end{center}
\end{minipage}
\end{figure}

\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.15cm,width=12cm]{./figs/TxRx_Cauchy_TanxArcTanx_LargeWSN_Noisy.eps} \caption{Convergence of  algorithm for a large Graph, Entries of  versus Iterations : Cauchy noise, , , ,  dB, .}\label{fig:TxRx_Cauchy_TanxArcTanx_LargeWSN_Noisy}
\end{center}
\end{minipage}
\end{figure}

\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.15cm,width=12cm]{./figs/Diff_WSN_Same_Fx_Hx_Cauchy.eps} \caption{Difference in speed of convergence: sparsely versus densely connected Graphs,  versus Iterations : Cauchy noise, , , , , , .}\label{fig:Diff_WSN_Same_Fx_Hx_Cauchy}
\end{center}
\end{minipage}
\end{figure}

\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.2cm,width=12cm]{./figs/Rx_Tx_Scaling_Speed_SignxAbsx2.eps} \caption{Scaling  does not change speed of convergence,  versus Iterations : , , , , .}\label{fig:Rx_Tx_Scaling_Speed_SignxAbsx2}
\end{center}
\end{minipage}
\end{figure}

\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.2cm,width=12cm]{./figs/Diff_Noise_Distributions_Robustness3.eps} 
\caption{Robustness to various noise distributions,  versus Iterations : , , , , }\label{fig:Diff_Noise_Distributions_Robustness3}
\end{center}
\end{minipage}
\end{figure}


\begin{figure}[tb]
\begin{minipage}{1\textwidth}
\centering
\begin{center}
\includegraphics[height=9.2cm,width=12cm]{./figs/Difference_Between_VarTheta_Cov.eps}
\caption{Difference between Variance of  and Asymptotic Variance, Entries of  versus Iterations : , , , , }\label{fig:Difference_Between_VarTheta_Cov}
\end{center}
\end{minipage}
\end{figure}


\end{document}
