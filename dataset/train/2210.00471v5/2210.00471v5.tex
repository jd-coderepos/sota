

\documentclass{article}



\usepackage{amsmath,amsfonts,bm}


\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmiccomment}[1]{#1}
\usepackage{xcolor,amsthm}
\newtheorem{lemma}{Lemma}
\usepackage{epigraph,booktabs}
\setlength\epigraphwidth{.685\textwidth}
\colorlet{shadecolor}{gray!90}


\renewcommand{\topfraction}{0.9}\renewcommand{\bottomfraction}{0.8}\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.99}

\renewcommand{\topfraction}{0.9}\renewcommand{\bottomfraction}{0.8}\renewcommand{\textfraction}{0.05}
\renewcommand{\floatpagefraction}{0.99}
\usepackage{hyperref}






\usepackage[accepted]{icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{mathtools}

\theoremstyle{plain}
\makeatletter
\newcommand{\@giventhat}[2]{\left(#1\;\middle|\;#2\right)}
\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{OCD: Learning to Overfit with Conditional Diffusion Models}




\begin{document}

\twocolumn[
\icmltitle{OCD: Learning to Overfit with Conditional Diffusion Models}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shahar Lutati}{comp}
\icmlauthor{Lior Wolf}{comp}
\end{icmlauthorlist}
\icmlaffiliation{comp}{Blavatnik School of Computer Science, Tel Aviv University}



\icmlcorrespondingauthor{Shahar Lutati}{shahar761@gmail.com}
\icmlcorrespondingauthor{Lior Wolf}{wolf@cs.tau.ac.il}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{} 

\begin{abstract}

We present a dynamic model in which the weights are conditioned on an input sample  and are learned to match those that would be obtained by finetuning a base model on  and its label . This mapping between an input sample and network weights is approximated by a denoising diffusion model. The diffusion model we employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Since the diffusion model is stochastic in nature, multiple initializations generate different networks, forming an ensemble, which leads to further improvements. Our experiments demonstrate the wide applicability of the method for image classification, 3D reconstruction, tabular data, speech separation, and natural language processing. Our code is attached as supplementary material.\end{abstract}
\section{Introduction}

\epigraph{\normalsize Here is a simple local algorithm: For each testing pattern, (1) select the few training examples located in the vicinity of the testing pattern, (2) train a neural network with only these few examples, and (3) apply the resulting network to the testing pattern.}{\normalsize\citet{6797141}}
 
Thirty years after the local learning method in the epigraph was introduced, it can be modernized in a few ways. First, instead of training a neural network from scratch on a handful of samples, the method can finetune, with the same samples, a base model that is pre-trained on the entire training set. The empirical success of transfer learning methods~\citep{finetune2} suggests that this would lead to an improvement. 

Second, instead of retraining a neural network each time, we can learn to predict the weights of the locally-trained neural network for each set of input samples. This idea utilizes a dynamic, input-dependent architecture, also known as a hypernetwork~\citep{ha2016hypernetworks}.

Third, we can take the approach to an extreme and consider local regions that contain a single sample. During training, we finetune the base model for each training sample separately. In this process, which we call ``overfitting'', we train on each specific sample  from the training set, starting with the weights of the base model and obtaining a model . We then learn a model  that maps between  (without the label) and the shift in the weights of  from those of the base model. Given a test sample , we apply the learned mapping  to it, obtain model weights, and apply the resulting model to .

The overfitted models are expected to be similar to the base model, since the samples we overfit are part of the training set of the base model. As a result, it is likely that a diffusion process would be able to generate the weights of the fine-tuned networks. Recently, diffusion models, such as DDPM \citep{ho2020denoising} and DDIM \citep{ddim} were shown to be highly successful in generating perceptual samples \citep{diffusion_beats_gan,kong2021diffwave}. Here, we employ such models as hypernetworks, i.e., as means for conditionally generating network weights. 

In order to make the diffusion models suitable for predicting network weights, we make three adjustments. First, we automatically select a specific layer of the neural model and modify only this layer. This considerably reduces the size of the generated data and, in our experience, is sufficient for supporting the overfitting effect. Second, we condition the diffusion process on the input of the selected layer, its activations, and its output. Third, since the diffusion process assumes unit variance scale~\citep{ho2020denoising}, we learn the scale of the weight modification separately.


Similarly to other diffusion processes, our hypernetwork is initialized with normal noise and different initializations lead to slightly different results. Using this feature of the diffusion model, we  generate multiple models from the same instance and use the resulting ensemble technique to further improve the prediction accuracy. Our method is widely applicable, and  we evaluate it across five very different domains: image classification, image synthesis, regression in tabular data, speech separation, and few-shot NLP. In all cases, the results obtained by our method improve upon the baseline model to which our method is applied. Whenever the baseline model is close to the state of the art, the leap in  performance sets new state-of-the-art results.

\section{Related Work}

{\bf Local learning} approaches perform inference with models that are focused on training samples in the vicinity of each test sample. This way, the predictions are based on what are believed to be the most relevant data points. K-nearest neighbors, for example, is a local learning method. \citet{6797141} have presented a simple algorithm for adjusting the capacity of the learned model locally, and discuss the advantages of such models for learning with uneven data distributions. \citet{alpaydin1996local} combine multiple local perceptrons in either a cooperative or a discriminative manner, and \citet{zhang2006svm} combine multiple local support vector machines. These and other similar contributions rely on local neighborhoods containing multiple samples. The one-shot similarity kernel of \citet{wolf2009one} contrasts a single test sample with many training samples, but it does not finetune a model based on a single sample, as we do.



More recently, \citet{wang2021tent} employ local learning to perform single-sample domain adaptation (including robustness to corruption). The adaptation is performed through an optimization process that maximizes the entropy of the prediction provided for each test sample. Our method does not require any test-time optimization and focuses (on the training samples) on improving the accuracy of the ground truth label rather than label-agnostic confidence. 

\citet{alet2021tailoring} propose a method called Tailoring that employs, like our method, meta-learning to local learning. The approach is based on applying unsupervised learning on a dataset that is created by augmenting the test sample, in a way that is related to the adaptive instance normalization of \citet{huang2017arbitrary}. Our method does not employ any such augmentation and is based on supervised finetuning on a single sample.

Tailoring was tested on synthetic datasets with very specific structures, in a very specific unsupervised setting of CIFAR-10. Additionally, it was tested as a defense against adversarial samples, with results that fell short of the state of the art in this field. Since the empirical success obtained by Tailoring so far is limited and since there is no published code, it is not used as a baseline in our experiments.

As far as we can ascertain, all existing local learning contributions are very different from our work. No other contribution overfits samples of the training set, trains a hypernetwork for local learning, nor builds a hypernetwork based on diffusion models. 

{\bf Hypernetworks~\citep{ha2016hypernetworks}} are neural models that generate the weights of a second {\em primary} network, which performs the actual prediction task. Since the inferred weights are multiplied by the activations of the primary network, hypernetworks are a form of multiplicative interactions~\citep{jayakumar2020multiplicative}, and extend layer-specific dynamic networks, which have been used to adapt neural models to the properties of the input sample~\citep{klein2015dynamic,riegler2015conditioned}.  

Hypernetworks benefit from the knowledge-sharing ability of the weight-generating network and are therefore suited for meta-learning tasks, including few-shot learning~\citep{bertinetto2016learning}, continual learning~\citep{Oswald2020Continual}, and model personalization~\citep{shamsian2021personalized}. When there is a need to repeatedly train similar networks, predicting the weights can be more efficient than backpropagation.  Hypernetworks have, therefore, been used for neural architecture search~\citep{brock2018smash,zhang2018graph}, and hyperparameter selection~\citep{lorraine2018stochastic}. 

MEND by~\citet{mitchell2021fast} explores the problem of model editing for large language models, in which the model's parameters are updated after training to incorporate new data. In our work, the goal is to predict the label of the new sample and not to update the model. Unlike MEND, our method does not employ the label of the new sample.

{\bf Diffusion models\quad}
Many of the recent generative models for images \citep{ho2022cascaded,chen2020wavegrad, dhariwal2021diffusion} and speech \citep{kong2021diffwave,chen2020wavegrad} are based on a degenerate form of the Focker-Planck equation. \citet{sohl2015deep} showed that complicated distributions could be learned using a simple diffusion process. The Denoising Diffusion Probabilistic Model (DDPM) of \citet{ho2020denoising} extends the framework and presents high-quality image synthesis. \citet{ddim} sped up the inference time by an order of magnitude using implicit sampling with their DDIM method. \citet{watson2021learning} propose a dynamic programming algorithm to find an efficient denoising schedule and \citet{noise_scaling_nachamani} apply a learned scaling adjustment to noise scheduling. \citet{luhman2021knowledge} combined knowledge distillation with DDPMs.



The iterative nature of the denoising generation scheme creates an opportunity to steer the process, by considering the gradients of additional loss terms. The Iterative Latent Variable Refinement (ILVR) method~\citet{choi2021ilvr} does so for images by directing the generated image toward a low-resolution template. A similar technique was subsequently employed for voice modification~\citet{levkovitch2022zero}. Direct conditioning is also possible: \citet{imagen} generate photo-realistic text-to-image scenes by conditioning a diffusion model on text embedding;  \citet{amit2021segdiff} repeatedly condition on the input image to obtain image segmentation. In voice generation, the mel-spectrogram can be used as additional input to the denoising network~\citet{chen2020wavegrad,kong2021diffwave,liu2021diffsinger}, as can the input text for a text-to-speech diffusion model \citet{popov2021grad}. The conditioning we employ is of the direct type.





























\section{Method}
Our method is based on a modified diffusion process. 
Denote the training dataset as ,
where  are the data points in the dataset , and  are the associated labels. First, a base model,  is trained over the entire dataset, , where  are the learned weights of the model when trained over the entire dataset. 

Next, for every training sample  we run fine-tuning based on that single sample to obtain the overfitted parameters (function) as  (). 

where  is the loss function that is minimized in the training of the base model,  are the data point and label of sample , and  is the weight difference obtained when finetuning the model. Finetuning is performed with three gradient descent iterations, and,  as shown in our runtime analysis, is typically much less computationally demanding than the training of the base network.

The meta-learning problem we consider is the one of learning a model , which maps  (the input domain of sample ), and potentially multiple latent representations of  in the context of , collectively denoted as , to a vector of weight differences, such that

 where  are the base model's parameters trained over , and  is a mapping function that maps the input, i.e., the  part of , and multiple latent representations of it, , to the desired shift in the model parameters.

{\bf Layer selection\quad}  Current deep neural networks can have millions or even billions of parameters. Thus, learning to modify all network parameters can be a prohibitive task. Therefore, we opt to modify, via function , a single layer of . 

To select this layer, we follow \citet{lutati2021hyperhypernetwork} and choose the layer that presents the maximal entropy of the loss, when fixing the samples , and perturbing the layer's parameters.

The layer that yields the highest entropy score when perturbed is a natural candidate for a fine-tuning algorithm, since a large entropy reflects that, near the layer's base parameters, the surface imposed by the loss function has a large variance. Thus, a small perturbation could result in a dramatic change in loss when the perturbation is in the right direction.

Denote the perturbed weights, in which only layer  is perturbed, as . The score used for selection is

where  is the loss objective on which the function of  is trained, and the entropy is computed over multiple draws of . Since sampling does not involve a backpropagation computation, the process is not computationally demanding, and  samples per each training sample  are used.

The entropy, per each sample , is computed by fitting a Gaussian Kernel Density Estimation (GKDE)~\citep{Silverman86} to the obtained empirical distribution of the loss function. The layer that obtains the highest mean entropy is selected.


{\bf The conditioning signal\quad} The latent representation, , has three components. Given a selected layer, , we denote the input to this layer, when passing a sample  to , as  and the activation of this layer as . We also use the output of the base function .  is given by the tuple



\subsection{Diffusion Process}

The goal of the diffusion process is to reconstruct , the difference between the fine-tuned weights , and the base model weights, .
The process iteratively starts a random , with the same dimensions as . 

Next, it iterates with , where  is decreasing and is the diffusion step, and returns . After appropriate diffusion steps,  should be as close as possible to  for a given point .

The diffusion error estimation network,  is a function of the current estimation, , the latent representation tuple, , and the diffusion timestep, . The last is encoded through a positional encoding network \citep{attention_is_all_u_need}, . All inputs, except for ,  are combined into one vector: , where , ,  are the encodings of the layer input, layer activations, and network output. Note that most of the components of  do not change during the diffusion process, and can be computed only once. This way, the conditioning overhead is reduced to a minimum. The conditional diffusion process is depicted in Fig~\ref{fig:array_arc}.




{\bf Training Phase\quad} The complete training procedure of  is depicted in Alg.~\ref{alg:diffalg}. The first phase is overfitting, using vanilla gradient descent over a single input-output pair, see line~1-5. The overfitting phase is not demanding, since the backpropagation is conducted only over the selected layer and a single sample. 


As stated in Sec.~\ref{subsec:Scale}, while regular diffusion assumes that the input has unit variance, when estimating network weights, scaling has a crucial impact. The normalization in line 6 ensures that the diffusion is trained over unit-variant input. We denote by  the normalized difference between  and the parameters  of the base model.

Following \citet{ddim}, linear scheduling is used for the diffusion process, and , , ,  are set in line~8. A training example is then sampled:
 where  is normal noise. Since our goal is to recover the noiseless , the objective is
 where  is the diffusion error estimating model defined above. A gradient step is taken in order to optimize this objective, see line.~10.

{\bf Inference Phase\quad} Given an unseen input ,  is computed using the base network  and is used for all calls to the diffusion network . The diffusion steps are listed in Appendix \ref{app:alg2}.



\begin{figure}
    \centering
    \includegraphics[width=0.9980\linewidth]{arch.PNG}
    \caption{The diffusion process.      is the input of the base network, .  is a tuple of latent representations of . 
    ,, and  are the input, activation, and output encoders, respectively, of the selected layer that is being modified.  is the diffusion step, and  is the current diffusion estimation.}
    \label{fig:array_arc}
\end{figure}

\begin{algorithm}[t]
    \caption{Training Algorithm. \\ \textbf{Input:}  training set,  base network parameters,  the loss of the primary task,   diffusion steps\\
\textbf{Output:}  diffusion network (incl. ,, ).}
\label{alg:diffalg}
\begin{algorithmic}[1]
\REPEAT
\STATE \text{sample } and set 
\REPEAT
    \STATE Grad step on  to update   \label{lst:line:overfitting_alg_part}
\UNTIL{ converges}\label{lst:loopends}
\STATE  \label{lst:line:normalizing}
\STATE , 
\STATE ,,
 \label{scheduling}
\STATE   \label{sampling}
\STATE \text{Grad step on} , updating ,  and the components of \label{lst:line:objective}
\UNTIL{ converges}
\end{algorithmic}
\end{algorithm}




\subsection{Scale Estimation}
\label{subsec:Scale}

The Evidence Lower Bound (ELBO) used in \citet{ho2020denoising} assumes that the generated data has unit variance. In our case, where the generated data reflects a difference in the layer's weights, the scale of data presents considerable variation. Naturally, shifting the weights of a network by some vector  or by some scale times  can create a significant difference. 

We, therefore, use an MLP network  to estimate the appropriate scale factor, based on the same conditioning signal that is used for the network  that implements  as a diffusion process. 
When learning network , the following objective function is used

where .
The use of the log scale is due to the large variance of . When a data point  is misclassified by the base model , the change in the model's weights, which results from finetuning, will be of a larger magnitude. In contrast, in the case of a sample   for which the output of  matches the label, finetuning would result in a minor update.

\subsection{Architecture}
\label{sec:architecture}
Following  \citet{ho2020denoising}, the network  is a U-Net~\citep{unet}. Each resolution level has residual blocks and an attention layer. The bottleneck contains two attention layers. 
The weights  are arranged as a matrix, with zeros added at the edges to create a square input matrix. The output of  is cropped to return the original dimensions of .

The positional encoder is composed of stacked sine and cosine encodings, following \citet{attention_is_all_u_need}. The encoders of  are both single fully-connected layers, with dimensions to match the positional embedding.  The encoder of the base network's output  depends on the output type. In the case of a classification network, where the output is a vector in , where  is the number of classes, the encoder  is a single fully-connected layer. This is also the case for our NLP experiments, which are multiclass classification experiments. In the case of image generation, the output image is first encoded using a VGG11 encoder \citep{vgg}, and then the latent representation is passed through a single fully-connected layer, again matching the dimension of the positional encoder. For speech separation, the estimated speech is first transformed to a spectogram with 1024 bins of FFT, and then encoded using the same VGG11.

\section{Experiments}

 In all experiments, the UNet  has 128 channels and five downsampling layers. The positional encoding,  has dimensions of 128x128. The Adam optimizer \citep{kingma2014adam} is used, with a learning rate of . A linear noise schedule is used based on \citet{ddim}, and the number of diffusion steps is 10. All experiments are repeated three times to report the standard deviation (SD) of the success metrics. 
 
In addition to the full method, we also show results for the network that overfits the test data, which serves as an upper bound that cannot be materialized without violating the train/test protocol. On some datasets we check to what extent selecting a single layer limits our results, by performing the overfitting process on all of the model weights. On all datasets, we ablate the scale component of our ``Overfit with Conditional Diffusion models'' (OCD) method, by estimating a fixed global scale factor  as the mean value of the scale factor  over the training set. An additional ablation selects the model  of the training sample  with the closest  to the test sample. This ``nearest neighbor'' ablation can be seen as the simplest way to implement the concept of OCD. Finally, we present an ablation that selects the layer with the second-highest layer selection score, to evaluate the significance of the selection criterion.

As an additional baseline, we employ Tent's official implementation published by \citet{wang2021tent}. This baseline is employed in the vanilla classification experiments, which are similar in nature to the type of experiments Tent was previously evaluated for. 



{\bf Image Classification\quad} Results for the MNIST dataset \citep{mnist}  are obtained with the LeNet5 architecture \citep{lent5}. The selected layer is the one next to the last fully connected layer, which, as can be seen in Appendix \ref{app:criterion} has the maximal entropy among LeNet5's layers. CIFAR10 images~\citep{cifar} are classified using GoogleNet~\citep{googlenet}. The selected layer was the last fully-connected layer, see Appendix \ref{app:criterion}. 
For both architectures, the three encoders  are simple fully-connected layers, with dimensions to match the number of channels in the UNet (128). 

For classification experiments, we measure both the cross entropy (evaluated on the test set) and the test accuracy. As can be seen in Tab.~\ref{tab:results_image_classification_mnist}, our method reduces the CE loss by a factor of 8 in comparison to the base network and there is an improvement of 0.5\% in accuracy.  Ablating the scale prediction, the results considerably degrade in comparison to the full method. The Nearest-Neighbor ablation yields slightly better results than the base network. 

The ablation that selects an alternative layer results in performance that is similar to or slightly better than the base network. This is congruent with the small difference between fitting the selected layer and fitting all layers, which may suggest that much of the benefit of overfitting occurs in the selected layer.

On CIFAR10, our method improves classification accuracy from  92.9\% to 93.7\%. As in MNIST, much of the improvement is lost when the three ablations are run. 
In both MNIST and CIFAR, when using the ground truth to overfit a specific example, the accuracy becomes, as expected, 100\%. Considering the CE loss, overfitting the entire model instead of the selected layers yields only mild improvement (below the standard deviation for MNIST). This indicates that the added improvement gained by applying our method to all layers (and not just to the selected one) may not justify the additional resources required. 

Experiments were also conducted on the TinyImageNet dataset~\citep{le2015tiny}. The baseline network used is the (distilled) Data efficient transformer (DeiT-B/16-D) of \citet{touvron2022deit}. The selected layer is the weight of the head module. Our method is able to improve on the baseline network, achieving  accuracy on the test set, and falls short of the current state of the art - which requires twice as many parameters - by less than 0.5\%. 

The weights in the OCD hypernetwork are obtained by a diffusion process, and multiple networks can be drawn for each sample . When an ensemble of five classifiers is employed, using different initializations of noise in the diffusion process, the results surpass the current state of the art, yielding  ( better than the baseline model, and  better than the current state of the art).

Tent is not competitive with OCD on any of the image classification datasets. In the results tables, in addition to the default of 10 Tent iterations, we provide results for 1 iteration, as provided by \citet{wang2021tent}, and for 30 iterations, for good measure. For all benchmarks and configurations, OCD is markedly better than Tent. While Tent improves over the baseline in both MNIST and TinyImageNet, in the latter case by almost a full percent, OCD, even without an ensemble, achieves more than 2.5 percent higher accuracy.

\begin{table*}[t]
\caption{Performance on classification tasks. CE=Cross Entropy}
\label{tab:results_image_classification_mnist}
\smallskip
\centering \begin{tabular}{@{}l@{~~}c@{~~}c@{~~}c@{~~}c@{}} 
 \toprule
  & \multicolumn{2}{c}{MNIST (LeNet5)} & \multicolumn{2}{c}{CIFAR10 (GoogleNet)}\\
 \cmidrule(lr){2-3}
  \cmidrule(lr){4-5}

Method &  Test-CE {} & Accuracy \%{} & Test-CE {} & Accuracy \%{} \\
\midrule
Base network & &  &  & \\
{\color{shadecolor}Overfitting on test} &{\color{shadecolor} } &  {\color{shadecolor}} & {\color{shadecolor}} & {\color{shadecolor}}\\
{\color{shadecolor}Overfitting on test (All Layers)}  &{\color{shadecolor}} & {\color{shadecolor} } & {\color{shadecolor}} &{\color{shadecolor} }\\
OCD nearest neighbor ablation & &  &  & \\
OCD no scaling ablation & &  &  & \\
OCD alternative layer ablation &  &  &  & \\
OCD (ours) & &  &   & \\
\midrule
Base network +  Tent (\citet{wang2021tent}, 1 iter) &  &  &  & \\
Base network +  Tent (\citet{wang2021tent}, 10 iters) &  & 	 &  &  \\
Base network +  Tent (\citet{wang2021tent}, 30 iters) &  & 	 &  &\\

 \bottomrule
\end{tabular}


\caption{Classification accuracy for TinyImageNet dataset.}
\label{tab:results_tinyImageNet}
\smallskip
\centering \begin{tabular}{lcc} 
 \toprule
 Method & \multicolumn{1}{c}{Test-CE {}} & \multicolumn{1}{c}{Accuracy \%{}}\\
    \midrule
Swin L/4 ~\citep{liu2021swin} & NA & \\
DeiT-B/16-D~\citep{touvron2022deit} & &  \\
DeiT-B/16-D + OCD &  &  \\
DeiT-B/16-D + OCD, ensemble of five &   & \\
\midrule
DeiT-B/16-D + Tent (\citet{wang2021tent}, 1 iter) &  &  \\
DeiT-B/16-D + Tent (\citet{wang2021tent}, 10 iters) &   & \\ 
DeiT-B/16-D + Tent (\citet{wang2021tent}, 30 iters) &  &   \\
 \bottomrule
\end{tabular}
\end{table*}



\begin{table}[t]
\caption{MSESD, lower is better, for the TinyNeRF network.}
\label{tab:results_nerf}
\smallskip
\centering \begin{tabular}{@{}l@{~}c@{~}c@{~}c@{}} 
 \toprule
 Method & \multicolumn{1}{c}{Lego} & \multicolumn{1}{c}{Hotdog} &\multicolumn{1}{c}{Drums}\\
    \midrule
Base model & & &\\
{\color{shadecolor}Overfitting on test} & {\color{shadecolor} }& {\color{shadecolor}}& {\color{shadecolor}}\\
OCD no scaling  & & & \\
OCD (ours) & & & \\
 \bottomrule
\end{tabular}
\vspace{-.43cm}
\end{table}



\begin{figure*}[t]
\centering
  \begin{tabular}{@{}c@{~}c@{~}c@{}}
     \includegraphics[width=0.254\linewidth]{lego_tf.png} & 
          \includegraphics[width=0.254\linewidth]{lego_tf_finetuned.png} &
     \includegraphics[width=0.254\linewidth]{lego_tf_diff.png}\\
     (a)&      (b)&     (c)\\
  \end{tabular}
    \caption{Sample TinyNeRF results (Lego model). More results can be found in Appendix~\ref{app:tinynerfresults}. (a) Base model on a test view. (b) Same test view, overfitted using the ground truth  (c) OCD (ours).}
    \label{fig:array_sample}
    \vspace{-.3cm}
\end{figure*}

{\bf Image Synthesis\quad} We further tested our method on the image generation task of novel view synthesis, using a NeRF architecture \citep{nerf} and the ``Synthetic-Blender'' dataset. The Tiny-NeRF architecture \citep{tinynerf} employs an MLP network consisting of three fully-connected layers. The input is a 3D ray as a 5D coordinate (spatial location and viewing direction). The output is the corresponding emitted radiance. For each view, a batch of 4096 rays is computed, from which the interpolated image is synthesized. 

We experimented with three objects from the dataset: Lego, Hotdog, and Drums. For each object, a different TinyNeRF base model is trained over the corresponding training set. A single overfitting example is produced by considering a batch of 4096 rays from the same viewpoint.

Based on the data in Appendix \ref{app:criterion}, the first layer is selected. We, therefore, changed the layer-input encoder, , such that the input image is first encoded by the VGG-11 encoder of \citet{vgg} (pretrained over ImageNet-1k), followed by a fully-connected layer, to match the dimensions of UNet channels. The encoders  are simple fully-connected layers, with dimensions to match the number of channels in the UNet (128).

As can be seen in Tab.~\ref{tab:results_nerf}, our method improves the MSE by 31\% for the Lego model, by 25\% for Hotdog, and 16\% for Drums. Without input-dependent scaling, the performance is much closer to the base network than to that of our complete method. Sample test views are shown in Fig.~\ref{fig:array_sample} and in Appendix~\ref{app:tinynerfresults}. Evidently, our method improves both image sharpness and color palette, bringing the synthesized image closer to the one obtained by overfitting the test image. 


{\bf Tabular Data\quad}
\citet{tabdata} have extensively benchmarked various architectures and tabular datasets. We use their simple MLP architecture as a base network (3 Layers). We were unable to reproduce the reported transformer, since the hyperparameters are not provided, and our resources did not allow us to run a neural architecture search, as \citet{tabdata} did. We run on two of the benchmarks listed: California Housing \citet{KELLEYPACE1997291} (CA), which is the first listed and has the least number of samples, and Microsoft LETOR4.0(MI) \citep{microsoft_dataset}, which is the last listed and has the largest number of samples. 

Based on the layer selection criterion, as depicted in Appendix \ref{app:criterion}, the first layer chosen for both datasets. As can be seen in Tab.~\ref{tab:results_tabular}, for CA the base MLP model falls behind ResNet. Applying our method, the simple architecture achieves better results. 

For MI when applying our method, the simple baseline achieves a record MSE of , surpassing the current best record on this dataset, which is 0.745 \citep{Popov2020Neural}. 
The ablation that removes input-dependent scaling degrades the performance of the base network, emphasizing the importance of accurate scaling per sample.


\begin{table}[t]
    \centering
        \caption{Tabular benchmarks. , lower is better.}
    \label{tab:results_tabular}
    \smallskip
    \begin{tabular}{@{}l@{~}c@{~}c@{}}
         \toprule
         Method &\multicolumn{1}{c} {CA} & \multicolumn{1}{c} {MI}\\
        \midrule
        MLP  & & \\
        ResNet  & &\\
        {\color{shadecolor} Overfit MLP on test} & {\color{shadecolor}}& {\color{shadecolor}}\\
        OCD + MLP no scale & & \\
        OCD + MLP (ours) & &\\
         \bottomrule
    \end{tabular}
\end{table}





\begin{table*}[t]
\begin{minipage}[c]{0.29\linewidth}
      \caption{Audio separation. DM: Dynamic Mixing}
    \label{tab:results_speech}
    \smallskip
       \centering
    \begin{tabular}{@{}l@{}c@{}} 
 \toprule
 Method & SI-SDRi[dB]() \\\midrule
SepIt \citep{sepit_paper} &\\
\midrule
Baseline \citep{many_speak} &\\
{\color{shadecolor} Overfit baseline on test} & {\color{shadecolor}}\\
Baseline + OCD no scale & \\
Baseline + OCD (ours) & \\
Baseline + OCD + DM (ours) & \\
 \bottomrule
\end{tabular}
 \end{minipage}\hfill
\begin{minipage}[c]{0.60\linewidth}
        \caption{Accuracy for few-shot NLP classification with 8 samples per class.}
    \label{tab:results_nlp}
    \smallskip
    \begin{tabular}{@{}l@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c@{}} 
        \toprule
         Method & {SST-5} & {AmazonCF}&{CR}&{Emotion}&{Enron}&{Mean}\\
            \midrule
        T-few 3B~\citep{liu2022few} & &&&&&\\
        \midrule
        SetFit~\citep{tunstall2022efficient} &&&&&&\\
        SetFit + OCD & &&&&&\\
        Mean of 5 OCD instances& &&&&&\\
        Ensemble of SetFit + OCD & &&&&&\\        
         \bottomrule
        \end{tabular}
\end{minipage}
\end{table*}



{\bf Speech Separation\quad}
To tackle the task of single microphone speech separation of multiple speakers, \citet{VSUNS} introduce the Gated-LSTM architecture with MulCat block and \citet{many_speak} introduced a permutation-invariant loss based on the Hungarian matching algorithm, using the same architecture. \citet{sepit_paper}  further improved results for this architecture, by employing an iterative method based on a theoretical upper bound, achieving state-of-the-art results. 





The same backbone and Hungarian-method loss are used in our experiments, which run on the Libri5Mix dataset without augmentations, measuring the SI-SDRi score. The selected layer was the projection layer of the last MulCat block  (Appendix \ref{app:criterion}). The output of the Gated-LSTM is the separated sounds, and to encode it, we apply the audio encoding described in  Sec.~\ref{sec:architecture} to each output channel separately and concatenate before applying the linear projection to  .

As can be seen in Tab.~\ref{tab:results_speech}, applying our diffusion model over the Gated-LSTM model, we achieve , surpassing current state-of-the-art results and approaching the results obtained by overfitting on the test data. The ablation that removes input-dependent scaling is much closer in performance to the base network than to our complete method.

{\bf Prompt-free Few-Shot NLP Classification\quad}
Recent few-shot methods have achieved impressive results in label-scarce settings. However, they are difficult to employ, since they require language models with billions of parameters and hand-crafted prompts. Very recently, \citet{tunstall2022efficient} introduced a lean hypernetwork architecture, named SetFit, for finetuning a pretrained sentence transformer over a small dataset (8 examples per class), and then employing logistic regression over the corresponding embedding. In our experiments, we apply OCD to the last linear layer of the SetFIT sentence transformer. The U-Net has 64 channels, with 5 downsample layers. The size of the last linear layer is .  is the embedding of the Sentence Transformer.

As can be seen in Tab.~\ref{tab:results_nlp} using OCD and this lean architecture (with 110M parameters) outperforms, on average, the state-of-the-art model of \citet{liu2022few}, which has 3B parameters. The mean performance across the datasets is improved by 1.0\%  over the state-of-the-art and by 2.1\% over the SetFIT model we build upon. Recall that since the weights in the OCD hypernetworks are obtained by a diffusion process, one can draw multiple networks for each sample . The variability that arises from the random initialization of the diffusion process allows us to use an ensemble. As can be seen in Tab.~\ref{tab:results_nlp}, when applying an ensemble of five inferences, the results of OCD further improve by 0.4\% (on average) to a gap of 1.5\% over the state-of-the-art. 

Interestingly, when testing the average network weights of the 5 networks, the accuracy is virtually identical to that obtained with a single OCD network. However, there is some reduction in Cross-Entropy for the mean network, see Appendix \ref{app:results_nlp_ce}).



{\bf Runtime\quad}
Tab.~\ref{tab:running_time} lists the measured runtime with and without OCD for both training and inference, on the low-end Nvidia RTX2060 GPU on which all experiments were run. Since overfitting each sample includes only 3 gradient descent steps over a single layer, most of the data collection results are shorter than the training time of the base model, with the exception of LeNet5 training that used a batch size of 16 versus batch size of 1 in the overfitting stage. Training the diffusion model is slower in most experiments than training the base model, except for the speech model, in which training the base model is lengthy. 

The inference overhead is almost constant across the six models, since it consists of running the UNet  ten times. The inference overhead due to a U-Net forward pass is 27[ms]  (there are ten such passes, one per diffusion step). In absolute terms,  the overhead incurred by OCD, even on a very modest GPU, is only a quarter of a second, while the results improve substantially. We note that the same  is used for all experiments, padding the actual weight matrix, which induces an unnecessary computational cost in the case of small networks.



\begin{table}[t]
    \caption{Runtime on a low-end Nvidia RTX2060GPU. Training includes the base model training, the collection of data for OCD by overfitting each sample in the training set, and the training of the diffusion model. The main overhead between the base model and the OCD one during inference is a result of running the UNet of the diffusion process for ten iterations.}
    \label{tab:running_time}
    \smallskip
    \centering
    \begin{tabular}{@{}l@{~}c@{~}c@{~}c@{~~}c@{~}c@{}}
    \toprule
    & \multicolumn{3}{c}{Training[hrs]}& \multicolumn{2}{c}{Inference[ms]}\\
    \cmidrule(lr){2-4}
    \cmidrule(lr){5-6}
         Architecture& Base & Overfit & Diffusion&Base & OCD\\
    \midrule
         LeNet5&0.5&1.0&6.0&15&288 \\
         GoogleNet&2.5&1.0&5.0&20&298 \\
         TinyNeRF&1.1&0.2&5.2&230&510 \\
         MLP&1.2&0.1&6.2&75&355 \\
         Gated-LSTM&30&2.5&6.8&450&730\\
         SetFit&0.2&0.1&5.1&125&401 \\
    \bottomrule
    \end{tabular}
\end{table}

\section{Conclusions}

We present what is, as far as we can ascertain, the first diffusion-based hypernetwork, and show that independently learning the scale is required for obtaining the best performance. The hypernetwork is studied in the specific context of local learning, in which a dynamic model is conditioned on a single input sample. 

The training samples for the hypernetwork are collected by finetuning a model on each specific sample from the training set used by the base model. By construction, this is only slightly more demanding than fitting the base model on the training set. More limiting is the size of the output of the hypernetwork, which for modern networks can be larger than the output dimensions of other diffusion processes. We, therefore, focus on a single layer, which is selected as the one most affected by weight perturbations. 

We tested our method extensively, tackling a very diverse set of tasks with the same set of hyperparameters. We are yet to find a single dataset or architecture on which our OCD method does not significantly improve the results of the baseline architecture.  

\section*{Acknowledgements}
The contribution of Shahar Lutati is part of a Ph.D. thesis
research conducted at Tel Aviv University.
This work was supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD).

\bibliography{hypernet}
\bibliographystyle{icml2023}



\appendix
\newpage


\section{The inference time algorithm}
\label{app:alg2}
The steps of the inference algorithm are listed on Alg.~\ref{alg:inference}.

\begin{algorithm}[ht]
\caption{Inference Algorithm. \\ \textbf{Input:}  input sample,   the parameters of the base network,  diffusion network, T  diffusion steps.\\
\textbf{Output:}  estimated normalized () for  associates with .}
\label{alg:inference}
\begin{algorithmic}[1]
\STATE 
\STATE 
\WHILE{} \label{lst:line:overfitting}
\STATE , , , 
    \STATE 
\STATE 
\ENDWHILE\label{overfitting_while}
\STATE \textbf{return} \end{algorithmic}
\end{algorithm}


\section{Layer selection plots}
\label{app:criterion}
Fig.~\ref{fig:criterion} depicts the layer selection criterion for various experiments.

\begin{figure*}[t]
  \begin{tabular}{ccc}
     \includegraphics[clip,width=0.32\linewidth,height=0.275\linewidth]{lenet5_criterion.png} & 
          \includegraphics[clip,width=0.32\linewidth]{googlenet_criterion.png} &
          \includegraphics[clip,width=0.32\linewidth,height=0.275\linewidth]{TinyNeRF_criterion.png}\\
          (a)&
     (b) &
     (c)\\
     \includegraphics[clip,width=0.32\linewidth,height=0.275\linewidth]{MLP_criterion.png} & 
     \includegraphics[clip,width=0.32\linewidth,height=0.275\linewidth]{MulCat_criterion.png} \\ 
     (d)&(e)
  \end{tabular}
    \caption{ Layer Selection Criterion for different experiments. (a) For LeNet5 on MNIST, the next to last Fully-Connected layer is selected since it has the maximal entropy. (b) For GoogleNet on CIFAR10, the last Fully-Connected layer is selected. (c) For TinyNeRF (three datasets), the first Fully-Connected layer is selected. 
    (d) For Tabular MLP the first layer is selected. (e) For MulCat the last projection layer is selected.}
    \label{fig:criterion}
\end{figure*}

\section{TinyNeRF sample results}

\label{app:tinynerfresults}
Fig.~\ref{fig:array_sample_1} presents sample results for the reconstruction of the Lego/Hotdog/Drums Model. The quality of the OCD results approaches that of the model that overfits on the test view.

\begin{figure*}[t]
  \begin{tabular}{@{}ccc@{}}
     \includegraphics[width=0.32\linewidth]{drum_tf.png} & 
          \includegraphics[width=0.32\linewidth]{drum_tf_finetuned.png} &
     \includegraphics[width=0.32\linewidth]{drum_tf_diff.png}\\
     \includegraphics[width=0.32\linewidth]{lego_tf.png} & 
          \includegraphics[width=0.32\linewidth]{lego_tf_finetuned.png} &
     \includegraphics[width=0.32\linewidth]{lego_tf_diff.png}\\
          \includegraphics[width=0.32\linewidth]{hotdog_tf.png} & 
          \includegraphics[width=0.32\linewidth]{hotdog_tf_finetune.png} &
     \includegraphics[width=0.32\linewidth]{hotdog_tf_diff.png}\\
     (a)&      (b)&     (c)\\
  \end{tabular}
    \caption{Sample TinyNeRF results on the Drums/Lego/Hotdog model. 
    (a) Base model on a test view. (b) Same test view, overfitted using the ground truth  (c) OCD (ours).}
    \label{fig:array_sample_1}
\end{figure*}

\section{Cross entropy statistics for the NLP experiments}
\label{app:results_nlp_ce}
Table \ref{tab:results_nlp_ce} depicts the cross entropy loss for the NLP task of few-shot classification using SetFIT+OCD, when using one random initialization of the diffusion process or when averaging the networks obtained by five random initializations.

\begin{table*}[t]
    \centering
    \begin{tabular}{lccccc} 
         \toprule
          Method & \multicolumn{1}{c}{SST-5} & \multicolumn{1}{c}{AmazonCF}&\multicolumn{1}{c}{CR}&\multicolumn{1}{c}{Emotion}&\multicolumn{1}{c}{EnronSpam}\\
            \midrule
        One instance &&&&&\\
        Mean network of 5 instances & &&&&\\
         \bottomrule
        \end{tabular}
    \caption{The Cross-Entropy loss for the NLP task of few-shot classification with 8 samples per class using SetFIT + OCD, comparing one instance to the mean network obtained with 5 instances.}
    \label{tab:results_nlp_ce}
\end{table*}
\end{document}
