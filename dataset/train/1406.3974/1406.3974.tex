
\documentclass[runningheads,envcountsect,envcountsame]{llncs}

\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphics}
\usepackage{srcltx}
\usepackage{amssymb,amsmath,amsfonts}
\usepackage{textcomp}
\usepackage{epsf}
\usepackage[dvips]{epsfig}
\iffalse
\usepackage{latexsym}
\usepackage{psfrag}
\usepackage{graphicx}
\usepackage{amsopn}
\fi
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\nats}{{\mathbb N}}

\tolerance=2000

\begin{document}

\author{Julien Cassaigne\inst{1} \and Anna E. Frid\inst{1,3} \and Svetlana Puzynina\inst{2,3} \and Luca Q. Zamboni \inst{2,4}}
\institute{Aix-Marseille Universit\'{e}, France
\email{cassaigne@iml.univ-mrs.fr, anna.e.frid@gmail.com}
 \\
\and Department of Mathematics and Statistics, University of Turku, Finland
\email{svepuz@utu.fi}\\
\and Sobolev Institute of Mathematics, Russia \\
\and Universit\'e de Lyon 1, France
\email{zamboni@math.univ-lyon1.fr}}
\authorrunning{J. Cassaigne, A. E. Frid, S. Puzynina, L. Q. Zamboni}
\title{Subword complexity and decomposition of the set of factors}


\maketitle

\begin{abstract}
In this paper we explore a new hierarchy of classes of languages
and infinite words and its connection with complexity
classes. Namely, we say that a language belongs to the class  if it is a
subset of the catenation of  languages , where the number of words of
length  in each of  is bounded by a constant.
The class of infinite words whose set of factors is in  is denoted by
. In this paper we focus on the relations between the classes  and the subword complexity
of infinite words, which is as usual defined as the number of factors of the word of length . In particular, we prove that the class  coincides with the class of
 infinite words of linear complexity. On the
other hand, although the class 
is included in the class of words of complexity , this
inclusion is strict for .
\end{abstract}

\section{Preliminaries}

The complexities of infinite words and languages is a widely
studied area in formal languages theory. We follow the general approach where the complexity is measured as the number of fragments of a given size. Applied to words, it means that the complexity of a language  (or an infinite word ) is the function  (resp., ) counting the number of elements of  (resp., factors of ) of length . This function was introduced
by  Morse and Hedlund in 1938 \cite{MoHe1} under the name
\emph{block growth} as a tool to study symbolic dynamical
systems. The name \emph{subword complexity} was given by Ehrenfeucht,
Lee, and Rozenberg \cite{elr}; as the term ``factor'' replaces ``subword'', the term ``factor complexity'' is more and more popular \cite{cas_livre}.

An infinite word is ultimately periodic if and only if its complexity is ultimately constant, and it is a classical result that the smallest complexity of aperiodic words is  \cite{MoHe1}. The words of this complexity are called Sturmian and form a very interesting and well-explored family (see, e.g., Chapter 2 in \cite{Lo}). Results on the complexity usually belong to one of the two families: they give either conditions or formulas on the complexity of words from given families (see, e.g., \cite{pansiot}), or conditions on words with given restrictions on the complexity. As an example of a complicated problem of that kind, we mention the -adic conjecture on words of linear complexity (see \cite{leroy} and references therein). For a recent survey and deep results on subword complexity, see \cite{cas_livre}.

In the paper we relate the subword complexity to local conditions of factorization type.
Namely, we are interested in the following question: What is the relation between the complexity of the word and the condition that each its factor can be decomposed into a product of a finite number  of words belonging to a language of a bounded complexity? In a related paper \cite{fpz} instead of languages of bounded complexity we considered the language of palindromes. 
Note that in both cases we need the language of factors to be a subset of the concatenation of these languages and not the concatenation itself. For another family of problems where the equality to the concatenation is needed, see e.g. \cite{af,hsw}.

\section{Classes and basic hierarchy}

We consider finite and infinite words over a finite alphabet
, i.e., finite or infinite sequences of elements from the set . A \emph{factor} or a \emph{subword} of an infinite word
is any sequence of its consecutive letters. The factor
 of an infinite word ,
with , is denoted by . As usual, the set
of factors of a finite or infinite word  is denoted by
Fac. A factor  of a right infinite word  is called
\emph{right} (resp., \emph{left}) \emph{special} if 
Fac (resp.,  Fac) for distinct letters .  The length of a finite word  is denoted by ,
and the number of occurrences of a letter  in  is denoted by
. The empty word is denoted  and we define
. An infinite word  for some non-empty word  is called ultimately (-)periodic. In the paper we mostly follow the terminology and notation from \cite{Lo}.



Denote by  the set of infinite words of complexity .

Let us introduce the classes  of languages and  of infinite words as follows: a language  (infinite word ) belongs to the class  (resp., )
if

(resp., Fac) for some
languages  with .
In other words,  if and only if Fac, and the condition  means exactly
that for some constant  we have  for all .
We also have . 
By a simple cardinality argument, we have the following inclusion:

\begin{lemma}\label{l1}
 For each integer , we have .
\end{lemma}
\noindent {\sc Proof.} Suppose a word  is in 
and consider the factors of length  of . There is  ways to decompose a positive integer  to
 non-negative summands in a given order:
. If the summand  is the length of the th factor in a decomposition of a
word of length  to  factors, and there are at most 
words of length  in the set , it means that in total,
there are not more than  decompositions of words
corresponding to a given decomposition of . Taking all the
factors of  of length  together, we see that they are not
more than , which means exactly that . \hfill 


\begin{example} Now we are going to show that the Thue-Morse word , defined as the fixed point starting with 0 of the morphism , belongs to  . For each  the Thue-Morse word consists of words  and , both of them of length : . Defining  to be the set of suffixes of all  and , and  to be the set of their prefixes, we see that  and  contain exactly two words of length  each. To cut each factor  of , we just choose any of its occurrences and a position  in it divided by the maximal power  of : . By the definition of ,  is a suffix of  or , and  is a prefix of one of them, and thus, . So, . This construction can be generalized to any fixed point of a 
primitive morphism but obviously not to fixed points whose complexity is higher than linear (see \cite{pansiot} for examples).
\end{example}


\begin{example} Sturmian words, which can be defined as infinite words with complexity  for each , also belong to  . These words have exactly one right and one left special factor of each length. One of the ways to construct the sets  and  for a Sturmian word  is the following:

Remark that in fact the set  is the set of reversals of factors from , and  for each .
The fact that every factor of  belongs to  follows from the properties of Sturmian words: it can be proved that every factor  of  has an occurrence  with  in the biinfinite characteristic Sturmian word  of , where either  or  , with  the right infinite characteristic word (i.e., the infinite left special word). \end{example}

Now let us introduce the {\it accumulative complexity} function  (resp., ) of a language  (resp., a word )
as

As above, we introduce the classes  of languages
and  of infinite words as follows: a language 
(resp., infinite word ) belongs to the class 
(resp., ) if

(resp., Fac)
for some languages  with .

As above,  if and only if Fac. The condition  means exactly that for all  we have  for some constant .

Clearly, , since  for all  implies . As for an opposite inclusion, we can only can prove the following theorem and its corollary.

\begin{theorem}
 .
\end{theorem}
\noindent {\sc Proof.} Consider a language ,
by definition this means  that  for some . We shall construct inductively the sets  and  of complexity
 such that .

Let us order the elements of  according to their length:
 with . The
sets  and  are constructed inductively: we choose any
 and  so that  and then do
as follows. Suppose that we constructed the sets  and
 of cardinality less than or equal to  each so that
 and the number of
words of each length  in each of   is bounded
by .

 Consider the word  and denote its length by . It admits 
 factorizations . If for a given factorization we have  and ,
 we do not need to add anything to these sets and can take , . If for example , we can construct 
 by adding  to :  if the words of length  in  are at most  (and symmetrically for ).
 But the number  of lengths  such that  (resp., )
 and thus no more of words of length  can be added to  (resp., ) is bounded by ,
 since the total number of words in  (resp., ) is at most .

So, to assure that at least one of  factorizations is
admitted and we (if necessary) can add new words  and :
,  such that
, we should check that . But since  is the length of the word number 
in , we have  and thus , which was to be proved. \hfill 

\begin{corollary}
For each , we have .
\end{corollary}
\noindent {\sc Proof.} Take a language : by the definition,  with  for all . Due to the theorem above, all , that is,  where the complexities of ,  are bounded. Clearly, we have
, which proves the corollary. \hfill 

\medskip
So, for all  we have  and thus .


\section{Linear complexity and }
In this section, we prove the main result of this paper, namely,
\begin{theorem}\label{t:w2} An infinite word is of linear complexity if and only if its language of factors is a subset of the catenation of two languages of bounded complexity:
 .
\end{theorem}
The  inclusion has been proven in Lemma \ref{l1}.
Since for periodic words the statement is obvious, it remains
to find the languages  of bounded complexity for a given infinite word  of
linear complexity  such that the set of factors of  is a subset of .

The construction of the sets   and  is based on so-called {\it markers} which we define below.

\subsection{Markers and classification of occurrences}
Let  be an infinite word. Given a length , we say that a
subset  of the set of factors of  of length  is a set of
{\it markers}, or, more precisely, of {\it -markers} for a
constant , if each factor of  of length  contains at
least one word  as a factor. 
Recall that a factor  of  is called {\it right special} if  Fac for at least two different symbols .
\begin{lemma}
The set of right special factors of  of length  is a set of -markers, where .
\end{lemma}
\noindent {\sc Proof.} Consider a factor  of  of length  and suppose that none of its factors of length  is right special. It means that each factor of  of length , whenever it occurs in , uniquely determines the next factor of length , shifted by one letter. But there are  occurrences of factors of length  in . So, at least two of them correspond to the same factor, and what happens after its second occurrence repeats what happens after the first one. So, the word  is ultimately periodic, a contradiction.  \hfill 

\medskip
The number of right special factors of  of length  is
uniformly bounded by a constant  which is a polynomial of ,
where , due to a result of Cassaigne
\cite{cas_lin,cas_livre}. Thus, we have the following 
\begin{corollary}\label{c:mark}
 For each length , there exists a set of cardinality  of -markers of length  in .
\end{corollary}

Remark that the set of right special factors is just one the
possible ways to build the set of markers. For the proof below it
does not matter how the set of markers was constructed, the only
thing we use is that the set of markers of each length is bounded. 

Consider a factor  of  and denote by  its minimal period, that is,
the minimal positive integer such that  for all  and .
The word , also called the minimal period of , is denoted by ; each time it will be clear from the context whether the period means the word or the number.

An occurrence  of  in  is called {\it internal} if two conditions hold. First,  for all  such that  and ; second, symmetrically,  for all  such that . In other words, due to the definition of , for an internal occurrence of  in the infinite word  we have  and, provided that , .

An occurrence which is not internal is called {\it extreme}. More precisely, if  for some  such that , it is called {\it initial},
and if  for some  such that , it is called {\it final}. Clearly, an occurrence of a word in  can be initial and final at the same time.

Since  is not ultimately periodic, each its factor  admits a final occurrence, otherwise  would be ultimately -periodic.

\subsection{Construction and proof}
For each , consider the set of -markers of length  whose cardinality is bounded by . Due to Corollary \ref{c:mark}, such a set exists and we shall call its elements markers of order .

Consider a factor  of length  of . Our goal is to construct two words  and  such that . By the definition of markers,  contains a marker of order one; now consider the largest  such that it contains a marker  of order . Choose an occurrence of  in : . If all occurrences of  in  are internal, take one of them (say, the first one). If not, choose an extreme occurrence of  in  (again, the first of them if they are several). In both cases, we denote the chosen occurrence ; here  and .

Now we define  and . Clearly, . Note that the marker  is cut exactly in the middle of an occurrence:  with . Here  ends by  and 
starts with .

At last, let us define

where  is the empty word,  and .

It follows immediately from the definitions that Fac. It remains to prove that the cardinalities of  and  are uniformly bounded.

Consider a length . Let us count the words from .

What can be the length of a marker  used to construct a word ? It is equal to , where the word  of length  is a prefix of  and thus . On the other hand, since  was chosen to be maximal and by the definition of , we have . These two inequalities can be rewritten as

which means that  can take at most  values for a given .

Since we use a construction with at most  markers of each order , in total there are at most  markers which are used to construct the words from . Exactly the same counting works for the words from . They can be a bit shorter with respect to  in average, since we choose the first occurrence of a longest marker whenever we have a choice, and since the factor which we decompose can be close to the beginning of . However, the same bounds hold, and the same  (or less) markers can be used to construct the words from .

Now let us consider separately the cases when the occurrence of a marker used for a decomposition is internal, initial or final.

\begin{lemma}
Consider an occurrence of a factor  of length  in  and a longest marker  in it. If all the occurrences of  to the chosen occurrence of  are internal, then  is -periodic.
\end{lemma}
\noindent {\sc Proof.} Follows from the definition of an internal occurrence. \hfill 

Let us fix a length . Clearly, for a given marker  of a suitable length , there is exactly one possible word in  which can belong to  because of internal occurrences of : It is -periodic and obtained from the prefix of length  of 
by deleting the first  symbols. Symmetrically, there is exactly one possible word in  which can belong to  because of internal occurrences of .


It follows that for each , each of at most  possible markers for this length, its internal occurrences can give at most one word of length  in  and at most one word in . Now let us consider words arising from extreme occurrences.

For the sake of convenience, define a new symbol  and fix  for . So, instead of , we can now consider a bi-infinite word .

Let us fix a marker  of length  and a length  satisfying \eqref{e:kl} and consider the set  of words from  of length  arising from final occurrences of  to . For any word  consider a place in  which gives rise to it, that is, fix a position  such that  and . Now for each  such that  define the word  of length  as
 (see Fig.~\ref{f:1}). Note that if , the word  for sufficiently large -s starts with one or several (but not more than ) symbols .
\begin{center}
\begin{figure}
\centering \includegraphics[width=0.8\textwidth]{ewvi}
\caption{Construction of }\label{f:1}
\end{figure}
\end{center}

\begin{lemma}
 If  with , then  and .
\end{lemma}
\noindent {\sc Proof.} Denote . Note also that  can be uniquely reconstructed from .

Suppose that ; then .

Suppose that . Then the word  has  as a prefix and a suffix and thus is -periodic. In particular,  is -periodic. Since  is the minimal period of , we have . So, for each  both symbols  and  belong to either the prefix copy of  or to the suffix copy of  (or to both). So,  for all  from 1 to , and in particular for all  such that . This contradicts to the fact that  is a final occurrence of  to .\hfill 


\medskip
So, the number of possible words  for a given marker  and a given length  of  is minorized by the number of pairs ; here  is a word from  arising from a final occurrence of a marker , and for each ,  and , the parameter  takes exactly  values. On the other hand, all  are words of length , which are either factors of  or its prefixes preceded by at most  new symbols : the number of factors of  of length  is , the number of words with  is at most , and the number of words  is majorized by . So, we have

where  is the contribution to  of all the final occurrences of a marker  of length .

Since , the latter inequality can be rewritten as

In other words,


Exactly the same upper bound can be symmetrically proved for the contribution to  of initial occurrences of a marker : . So, each of  possible markers for the length  can contribute at most for the following number of words to : one word arising from its internal occurrences, plus  words arising from final occurrences, plus  words arising from initial occurrences. This gives the desired upper bound: the total number of words in the set  is bounded by the constant

The proof for  is similar and gives the same constant as the upper bound. \hfill 

\medskip
Note that the analogous fact for general languages is not true: there exists a language of linear complexity not belonging to any . However, this language (which we do not describe here because of the lack of space) is not closed under taking a factor.
\section{Word of quadratic complexity}

Lemma \ref{l1} and Theorem  \ref{t:w2} imply that , and in general  for all . So, the following natural question
arises: is it true that  for all
?


The answer is negative, and, since , to show it we just point an example of a word of quadratic complexity which does not belong to .

Consider the word . Its complexity : this can be either proved directly or derived from the famous paper by Pansiot \cite{pansiot}, since  is obtained by erasing the first letter  from the fixed point starting with  of the morphism .

\begin{lemma}\label{babaab}
        The word  does not belong to .
\end{lemma}
\noindent {\sc Proof.}
Suppose the opposite: Fac with . Now for each word Fac of length at most  fix some its decomposition  with , , . We shall estimate the number of words  which can be decomposed like that.

Now for each  define the word . Clearly,  is a factor of  of length .

\begin{claim}
Let   be the set of pairs   such that ,   and . Then .
\end{claim}
\noindent {\sc Proof.} Note that the condition  implies the inequality . So,


Observe that this set is empty for : indeed, if , then . So,


Here
 
and
 
The claim follows. \hfill 

\medskip
Let us say that a factor  of  is {\it of type } if  for some  and . Clearly, each factor of  either is of some type , or contains at most one letter .

Denote by  the set of pairs  with  and  such that there exists a factor  of  of length at most  and of type  whose decomposition is  with , . There were  letters  in , and at least  of them stay in the word . The type of  is thus one of the four following: ,
, , . But the total number of words in  of length at most  is , and each word  can give rise to at most four types from . So, , and due to the previous claim, there are still  pairs  with  and  such that each word  of type  and of length at most  is decomposed so that its middle part  contains at most one letter . Since there are  letters  in , we see that either  or  contains at least two letters .

We denote this set of pairs by . The number of all factors  of  whose types are in  is denoted by .

Consider a factor  of  of length at most  whose type is in . Suppose first that the word  contains more than one letter . Then the word  is uniquely determined by  and the length . So, the number of words  of length  admitting such a decomposition is bounded by .

Symmetrically, the number of words  such that  contains more than one letter  is bounded by .

So, the number  of words whose types are in  is . But on the other hand, the number of types in  is , and for each type , the number of words of this type is : indeed, such a word is of the form , where  can take  values from 0 to  and  can take  values from 0 to . Since we restricted ourselves to the case of , the number of words of each type is . In total, we have that
, that is,

A contradiction to the previous condition . \hfill 

\medskip
Since , we get also the following
\begin{corollary}
        There exists a word of quadratic complexity which does not belong to .
\end{corollary}

\section{Belonging to some }

The word  of quadratic complexity considered in the previous section does not belong to , but it can be proved that it belongs to . We omit this proof here since it does not add much to the theory. However, this result suggests the following question: given a word of complexity majorated by a polynomial, is it true that it belongs to  for some ?

As we show in the next proposition, the answer to this question is
negative.


\begin{proposition}\label{n2fn} For any growing integer function  such that ,  and , there exists an infinite
word  of complexity  which does not belong to
 for any .\end{proposition}

\noindent {\sc Proof.}
First we describe the construction of the word , then we prove
that  does not belong to  for any , and after that we prove
that the word has complexity .

\medskip

Define the infinite word  as follows:


where  is a growing function:  and  for all  and . 


Let us prove that  for any . Suppose by contrary
that : Fac with  for all . Define ; then  for an appropriate constant . Consequently,  for all .

\begin{claim}\label{c:pq}
For every pair of integers , such that ,  and , there
exists a word , , such that  contains 
as a factor, and all those words  are distinct.
\end{claim}

\noindent {\sc Proof.} Consider the word .
Since  and , it is a factor of ,
and since , its length is at most .
However we cut the word  into at most 
pieces, at least one piece will contain  as a factor.
The claim follows. \hfill 

\medskip

Let us estimate the number of words  for ,
 and . Since the function  is growing, there exists a constant  such that  for all  and all . Since  for all , we have , and thus the number of pairs  is bounded from below by the sum . Since  with , and since  is bounded from below by the number of pairs  due to Claim \ref{c:pq}, we have

for some sufficiently large . A contradiction to the fact that .

\medskip
Now let us check that the complexity of the word  is . The
word  contains factors of the following types:

\begin{enumerate}

\item Factors of a block  for some ,  and .

\item Factors of a concatenation of blocks .

\item Factors of a concatenation of blocks .

\item Factors containing some complete block   as a factor.
\end{enumerate}
Remark that some of these families intersect, but this is not a
problem since we only need a bound. So, let us estimate the number
of words of length  in each family.

In the family 1, we have  words of the form  or , plus  words of the form  (uniquely determined by ) or  (uniquely determined by ), plus words containing a factor  or . The latter words are uniquely determined by ,  and the position of the first occurrence of , which takes values from 0 to . So, the number of such words (and thus of all the words in family 1) is .

Treating the other three families analogously, we see that the complexity of each of them is at most  too.
\iffalse
In the family 2, a word not belonging to family 1 contains factors  and . So, such a word is uniquely determined by ,  and the position of the first occurrence of , which takes values from 0 to . So, the number of such words is also .

An analogous counting works for the family 3: the new words
appearing there are determined by  and the first position of
, so, their number is .

At last, for the family 4, let us choose the first complete block  in a given word. Then this word is determined by ,  and the position of its beginning, so, the number of such words is again .
\fi
So, the complexity , which completes the proof. \hfill 


\section{Conclusion}

We finalize this paper by suggesting the following open problem: What is the minimal possible complexity of a word which does not
belong to any ?

Remark that Theorem \ref{t:w2} and Proposition \ref{n2fn} imply
that this complexity is strictly bigger than linear and is at most
quadratic.

\medskip
Supported in part by RFBR grants 12-01-00089 and  12-01-00448, as well as by the Academy of Finland grant 251371.


\begin{thebibliography}{10}
\bibitem{af}
S. V. Avgustinovich, A. E. Frid. A unique decomposition theorem for factorial languages. Int. J. Alg. Comput. 15 (2005) 149--160.
 \bibitem{cas_lin}
J. Cassaigne.
Special factors of sequences with linear subword complexity.
DLT 1995, 25--34, World Sci. Publishing, Singapore, 1996.

\bibitem{cas_livre}
J. Cassaigne, F. Nicolas.
Factor complexity.
Combinatorics, automata and number theory, 163--247, Encyclopedia Math. Appl., 135, Cambridge Univ. Press, 2010.

\bibitem{elr} A. Ehrenfeucht, K.P. Lee, G. Rozenberg.
Subword complexities of various deterministic developmental languages without interactions.
Theoret. Comput. Sci. 1 (1975) 59-76.

\bibitem{fpz} A. Frid, S. Puzynina, L. Q. Zamboni. On palindromic factorization of words. Advances in Applied Mathematics 50 (2013) 737--748.

\bibitem{hsw}
Y.-S. Han, K. Salomaa, D. Wood. Prime Decompositions of Regular Languages. Proc. DLT 2006, LNCS 4036 (2006) 145--155.

\bibitem{leroy}  Leroy, J. Some improvements of the S -adic conjecture. Adv. in Appl. Math. 48 (2012), no. 1, 79--98.

\bibitem{Lo}
Lothaire, M.: Algebraic combinatorics on words. Cambridge
University Press, 2002.

\bibitem{MoHe1} M. Morse and G. Hedlund, Symbolic dynamics,
Amer. J. Math. 60 (1938),  815--866.

\bibitem{pansiot}
Pansiot, J.J.: Complexit\'{e} des facteurs des mots infinis engendr\'{e}s
par morphismes it\'{e}r\'{e}s. In: Paredaens, J. (ed.) ICALP 1984. LNCS,
vol. 172, pp. 380--389. Springer, Heidelberg (1984)
\end{thebibliography}


\end{document}
