\newcommand{\minif}{\mathtt{miniF2F}}
\newcommand{\proverbot}{\textsc{Proverbot9001}\xspace}

Our findings about \copra are that: (i) the approach can find proofs significantly quicker than the state-of-the-art finetuning-based baselines, both in terms of number of LLM queries and wall-clock time;  (ii) in problems where all current methods fail, \copra fails faster; (iii) the use of GPT-4, as opposed to GPT-3.5, within the agent is essential for success; and (iv) backtracking significantly improves the system's performance on harder problems. 
Now we elaborate on our experimental methodology and these results. 


\begin{wrapfigure}{l}{2.2in}
\vspace{-0.1in}
  \centering
    \includegraphics[scale=0.3]{main_minif2f_pass_k_inference.png}
  \caption{\copra vs. \reprover on the  benchmark}
  \label{fig:pass_k_inferences_minif2f}
  \vspace{-0.2in}
\end{wrapfigure}

\parag{Implementing \copra.} Our implementation of \copra has GPT-4 as the underlying LLM and can interact with both the Lean and the Coq proof environments. Because of the substantial cost of GPT-4 queries, 
we cap the number of LLM queries that \copra can make by 60. 
To further reduce costs, \system first tries to prove its theorems via a single LLM query (one-shot prompting).
It only invokes its agent behavior when the one-shot prompting fails to find a proof. 

The ``system prompt" in the one-shot approach is slightly different than that for \copra, containing instructions to generate a proof in one go rather than step by step. For both \copra and the one-shot baselines, the prompt contains a single proof example that clarifies how proofs need to be formatted. 
This proof example remains the same for all test cases.


\begin{wrapfigure}{l}{2.2in}
\vspace{-0.1in}
  \centering
  \includegraphics[scale=0.3]{main_compcert_pass_60_inferences.png}
  \caption{\copra vs. \proverbot on the Compcert benchmark}
  \label{fig:pass_k_inferences_compcert}
  \vspace{-0.2in}
\end{wrapfigure}

\parag{Benchmarks.} 
We evaluate our approach on two domains: (i)  \citep{zheng2021minif2f}, a collection of 244 Lean formalizations of mathematics competition problems, solved using a range of techniques such as induction, algebraic manipulation, and contradiction; and (ii) a set of Coq problems from the CompCert compiler verification project \citep{leroy2009formal} that was previously used to evaluate the \proverbot system \citet{sanchez2020generating}. 





\parag{Baselines.} 
We compare with one-shot invocations of GPT-3.5 and GPT-4 in both the  and the Compcert domains. We also consider an ablation of \copra that uses GPT-3.5 as its LLM and another that does not use backtracking. 
As for fine-tuned baselines, a challenge is that all existing open-source theorem-proving systems only target a single proof environment. As a result, we had to choose different baselines for the Lean () and Coq (Compcert) domains.



Our fine-tuned baseline for the  domain is \reprover, a state-of-the-art open-source prover that is part of the Leandojo project \citep{yang2023leandojo}. A challenge with this baseline is that like \copra, it uses a retrieval mechanism. However, building a comparable retriever for \copra would require an indexed training corpus on problems relevant to . However,  is only an evaluation set and does not come with a training corpus.
As a result, for an apples-to-apples comparison, our evaluation on  turns off \copra's and \reprover's retrievers.  

In the Compcert domain, we compare with \proverbot \citep{sanchez2020generating}, which, while not LLM-based, is the best publicly available model for Coq. Unlike , this benchmark comes with a large training set as well as a test set, and we use the training set for retrieving relevant lemmas and definitions. Our retrieval mechanism, in this case, is a simple BM25 search. 



For cost reasons, our evaluation for Compcert uses 118 out the 501 theorems used in the original evaluation of \proverbot \cite{sanchez2020generating}. For fairness, we include all the 98 theorems proved by \proverbot in our subset. The remaining theorems are randomly sampled. 

\parag{Metric: pass@-inferences.} 
The standard metric for evaluating theorem-provers is \emph{pass@k} \citep{lample2022hypertree,yang2023leandojo}. In this metric, a prover is given a budget of  
\emph{proof attempts}; the method is considered successful if one of these attempts leads to success.
However, a key objective of our research is to discover proofs \emph{quickly}, with fewer LLM queries and lower wall-clock time. The pass@k metric does not evaluate this characteristic as it does not quantify the number of LLM queries or amount of time needed by a proof attempt.   


\renewcommand\theadfont{}
\begin{table}[t]
    \centering
    {\footnotesize
    \begin{tabular}{|l | l | l | l | l | l|l|} 
 \hline
 Approach & \thead{\# Theorems\\proved\\/\# Theorems}  & \thead{\% \\proved} & \thead{Avg.\\Inferences \\ in Total} & \thead{Avg. \\Inferences \\on Failure} & \thead{Avg. \\Inferences \\on Pass} & \thead{Max. \\Inferences\\Allowed}\\ [0.5ex] 
 \hline
 \hline
 \multicolumn{7}{|c|}{miniF2F Test Dataset} \\
 \hline
 \thead{GPT 3.5 \\One-Shot} & 7/244 & 2.8\% & 1 & 1 & 1 & 1\\ 
 \thead{GPT 4 \\One-Shot} & 26/244 & 10.6\% & 1 & 1 & 1 & 1\\
 \thead{\system \GPT-3.5)} & 29/244 & 11.89\% & 12.83 & 14.23 & 2.45 & 60\\
 ReProver & 54/244 & 22.13\% & 350.7 & 427.24 & 81.6 & 1076\\
 \textbf{\thead{\system\\ (GPT-4)}} & \textbf{57/244} & \textbf{23.36}\% & \textbf{20.94} & \textbf{26.79} & \textbf{1.75} & 60\\
 \hline
 \hline
\multicolumn{7}{|c|}{CompCert Test Dataset} \\
 \hline
  \thead{GPT 3.5 \\One-Shot} & 10/118 & 8.47\% & 1 & 1 & 1 & 1\\ 
 \thead{GPT 4 \\One-Shot} & 36/118 & 30.51\% & 1 & 1 & 1 & 1\\
 Proverbot & \textbf{98/118} & \textbf{83.05}\% & 184.7 & 256.8 & 170.0 & 2344\\
 \textbf{\system} & 76/118 & 64.41\% & \textbf{12.9} & \textbf{10.9} & \textbf{16.57} & 60\\
 \hline
\end{tabular}    
}
\caption{Aggregate statistics for \copra and the baselines on  and Compcert}
\label{tab:gpt-baseline-inferences}
\vspace{-0.1in}
\end{table}



\begin{table}[t]
    \centering
    {\footnotesize 
    \begin{tabular}{|l |l | l| l | l | l | l |} 
 \hline
 Approach & \multicolumn{6}{|c|}{Avg. Time In Seconds}  \\ [0.5ex] 
 \hline
  & \multicolumn{3}{|c|}{Per Proof} & \multicolumn{3}{|c|}{Per Inference}\\
 \hline
  & {On Pass} & {On Fail} & {All} & {On Pass} & {On Fail} & {All}\\
 \hline
 ReProver (on CPU) & 279.19 & 618.97 & 543.78 & 3.42 & 1.45 & 1.55\\
 ReProver (on GPU) & 267.94 & 601.35 & 520.74 & 2.06 & 0.44 & 0.48\\ \system (GPT-3.5) & 39.13 & \textbf{134.26} & \textbf{122.21} & 15.97 & 9.43 & 9.53\\
 \textbf{\system (GPT-4)} & \textbf{30.21} & 191.73 & 140.86 & 17.26 & 7.16 & 6.73\\
 \hline
\end{tabular}
}

\caption{Average time taken by our approach (\system) and ReProver on miniF2F dataset.}
\label{tab:wall-time-miniF2F}
\vspace{-0.1in}
\end{table}

\begin{wraptable}{l}{3in}
  \vspace{-0.2in}
\centering
      {\footnotesize
      \begin{tabular}{|l | l | l |} 
   \hline
   Approach & \thead{\# Theorems\\proved\\/\# Theorems}  & \thead{\% \\proved}\\ [0.5ex] 
   \hline
   \hline
   \multicolumn{3}{|c|}{miniF2F Test Dataset} \\
   \hline
    \begin{tabular}{l}\system (GPT-4) \\ w/o backtracking\end{tabular}  & 56/244 & 22.95\% \\
    \textbf{~~~\system (GPT-4)} & \textbf{57/244} & \textbf{23.36}\% \\
    \hline
   \hline
  \multicolumn{3}{|c|}{CompCert Test Dataset} \\
   \hline
   \begin{tabular}{l} \system (GPT-4) \\ w/o backtracking \end{tabular} & 52/118 & 44.06\%  \\
   \textbf{~~~\system (GPT-4)} & \textbf{76/118} & \textbf{64.41}\% \\
   \hline
   \end{tabular}
   }
  \vspace{-0.1in}
  \caption{The effectiveness of backtracking}
  \label{tab:copra-ablation}
  \vspace{-0.1in}
\end{wraptable}



\Cref{fig:pass_k_inferences_compcert} shows a comparison between \copra and \proverbot.


To address this concern, we introduce a new metric, \emph{pass@k-inferences}, and 
evaluate \copra and its competitors using this metric. Here, we measure the number of correct proofs that a prover can generate with a budget of \emph{ or fewer LLM inference queries}. One challenge here is that we want this metric to be correlated number of correct proofs that the prover produces within a wall-clock time budget; however, the cost of an inference query is proportional to the number of responses generated per query. To maintain the correlation between 
the number of inference queries and wall-clock time, we restrict each inference on LLM to a single response. 


\parag{Results.}
\Cref{fig:pass_k_inferences_minif2f} shows our comparison results for the  domain.
As we see, \copra outperforms \reprover, completing, within just 60 inferences, problems that \reprover could not solve even after a thousand inferences. This is remarkable given that \copra is based on a black-box foundation model and \reprover was fine-tuned for at least a week on a dataset derived from Lean's Mathlib library. For fairness, we ran \reprover multiple times with 16, 32, and 64 (default) as the maximum number of inferences per proof step. We obtained success rates of 15.9\%, 20.1\%, and  22.13\% in the respective cases and took the best for comparison.


We find that \copra is significantly faster than \proverbot. Since we put a cap of 60 inferences on \copra, it cannot prove all the theorems that \proverbot eventually proves. However, as shown in the figure, \copra proves many more theorems than \proverbot if only 60 inferences as allowed. Specifically, we prove 77.5\% of all proofs found by \proverbot in less than 60 steps.

Aggregate statistics for the two approaches, as well as a comparison with the one-shot GPT-3.5 and GPT-4 baselines, appear in Table \ref{tab:gpt-baseline-inferences}. It is clear from this data that the language-agent approach offers a significant advantage over the one-shot approach. For example, \copra solves more than twice as many problems as the one-shot GPT-4 baseline, which indicates that it does not just rely on GPT-4 recalling the proof from its memory. Also, the use of GPT-4 as opposed to GPT-3.5 seems essential.


We establish the correlation between the number of inferences needed for a proof and wall-clock time in Table \ref{tab:wall-time-miniF2F}. 
Although the average time per inference is higher for \system, \copra still finds proofs almost 9x faster than \reprover. This can explained by the fact that our search is more effective as it uses 46x fewer inferences than \reprover. These inference steps not only contain the average time spent on generating responses from LLM but at times have some contribution corresponding to the execution of the tactic on the Lean environment itself. 



\begin{wrapfigure}{l}{3in}
\vspace{-0.1in}
\begin{mdframed}[roundcorner=10pt]
\begin{lstlisting}
 theorem algebra_sqineq_at2malt1
(a : ℝ) :
a * (2 - a) ≤ 1 :=
begin
    have h : ∀ (x : ℝ), 0 ≤ (1 - x) ^ 2,
    from λ x, pow_two_nonneg (1 - x),
    calc a * (2 - a)
            = 1 - (1 - a) ^ 2 : by ring
        ... ≤ 1 : sub_le_self _ (h a),
end
\end{lstlisting}
\end{mdframed}
\caption{A theorem in the ``algebra" category that \system could prove but \reprover could not.}
\label{fig:appendix-miniF2F-example}
\end{wrapfigure}



Table \ref{tab:wall-time-miniF2F} also offers data on when the different approaches report failures. 
Since \reprover uses a timeout for all theorems, we also use a timeout of 11 minutes while considering failures in Table \ref{tab:wall-time-miniF2F}. The data indicates that \copra is comparatively better at giving up when the problem is too hard to solve. We also note that less time is spent per inference in case of failure for all approaches. 

We show the impact of ablating the backtracking feature of \copra in Table \ref{tab:copra-ablation}. We note that backtracking has a greater positive impact in the Compcert domain. We believe this is because the Compcert problems are more complex and 
backtracking helps more when the proofs are longer.


Finally, we offer an analysis of the different categories of  problems solved by \copra and \reprover in \Cref{fig:problem-category-miniF2F}.
We see that certain kinds of problems, for example, International Mathematics Olympiad (IMO) problems and theorems that require induction, 
are difficult for all approaches.
However, \Cref{fig:steps-per-category-miniF2F} shows that \system takes fewer steps consistently across various categories of problems in .

From our qualitative analysis, there are certain kinds of problems where the language-agent approach seems especially helpful.
For instance, \Cref{fig:appendix-miniF2F-example} shows a problem

in the `algebra' category that \reprover could not solve. More examples of interesting Coq and Lean proofs that \copra found appear in the appendix. 

\begin{figure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{problems_per_category.png}
  \caption{Problems solved in different categories}
  \label{fig:problems-per-category-miniF2F}
\end{subfigure}\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{steps_per_category.png}
  \caption{Number of inferences in different categories}
  \label{fig:steps-per-category-miniF2F}
\end{subfigure}
\caption{Breakdown of theorems proved in various categories}
\label{fig:problem-category-miniF2F}
\vspace{-0.2in}
\end{figure}
