\section{Experiments}\label{sec:experiments}
In this section, we evaluate \ModelName~on two popular video-and-language tasks, text-to-video retrieval and video question answering, across six different datasets.
We also provide extensive ablation studies to analyze the key factors that contribute to \ModelName's success.


\subsection{Downstream Tasks}
\noindent\textbf{Text-to-Video Retrieval.} 
$(i)$ \textbf{MSRVTT}~\cite{xu2016msr} contains 10K YouTube videos with 200K descriptions. 
We follow~\cite{yu2018joint,miech2019howto100m}, using 7k train+val videos for training and report results on the 1K test set~\cite{yu2018joint}. 
We also create a local val set by sampling 1K video-caption pairs from unused test videos for our ablation study.
$(ii)$ \textbf{DiDeMo}~\cite{anne2017localizing} contains 10K Flickr videos annotated with 40K sentences.
$(iii)$ \textbf{ActivityNet Captions}~\cite{krishna2017dense} contains 20K YouTube videos annotated with 100K sentences.
The training set contains 10K videos, and we use val1 set with 4.9K videos to report results. 
For MSRVTT, we evaluate standard sentence-to-video retrieval. For DiDeMo and ActivityNet Captions, we follow~\cite{zhang2018cross, liu2019use} to evaluate paragraph-to-video retrieval, where all sentence descriptions for a video are concatenated to form a paragraph for retrieval. We use average recall at K (R@K) and median rank (MdR) to measure performance.


\noindent\textbf{Video Question Answering.} 
$(i)$ \textbf{TGIF-QA}~\cite{jang2017tgif} contains 165K QA pairs on 72K GIF videos. 
We experiment with 3 TGIF-QA tasks: \textit{Repeating Action} and \textit{State Transition} for multiple-choice QA, and \textit{Frame QA} for open-ended QA. 
We leave the \textit{Count} task as future work as it requires directly modeling full-length videos.
$(ii)$ \textbf{MSRVTT-QA}~\cite{xu2017video} is created based on videos and captions in MSRVTT, containing 10K videos and 243K open-ended questions. 
$(iii)$ \textbf{MSRVTT multiple-choice test}~\cite{yu2018joint} is a multiple-choice task with videos as questions, and captions as answers.
Each video contains 5 captions, with only one positive match.
For all the QA tasks, we use standard train/val/test splits and report accuracy to measure performance.



\begin{figure}[!t]
  \centering
  \vspace{-5pt}
  \includegraphics[width=\linewidth]{figs/video_duration.pdf}
  \vspace{-15pt}
  \caption{
Average video length in different datasets. 
}
  \label{fig:video_duration}
\end{figure}

Figure~\ref{fig:video_duration} shows a comparison of average video length of evaluated datasets. 
Videos across datasets demonstrate considerable difference in domains and lengths, ranging from 3-second generic-domain GIF videos in TGIF-QA to 180-second human activity videos in ActivityNet Captions.



\begin{table}[!t]
\tablestyle{5pt}{1.1} 
\def\w{20pt} 
\scalebox{1.0}{
\begin{tabular}{c|R{14pt}R{\w}R{\w}R{\w}|c}
\multirow{2}{*}{ $L$ } & \multicolumn{4}{c|}{ MSRVTT Retrieval } & MSRVTT- \\  \cline{2-5}
& R1 & R5 & R10 & MdR & QA Acc. \\
\shline
224 & 6.8 & 24.4 & 35.8 & 20.0 & \bf 35.78 \\
448 & \bf 10.2 & \bf 28.6 & \bf 40.5 & \bf 17.0 & \bf  35.73 \\
768 & \bf 11.0 & 27.8 & \bf 40.9 & \bf 16.0 & \bf 35.73 \\
1000 & 10.0 & \bf 28.4 & 39.4 & 18.0 & 35.19 \\
\end{tabular}
}
\caption{
Impact of \textbf{input image size $L$}. 
}
\label{tab:image_size}
\vspace{-3mm}
\end{table}



\subsection{Analysis of Sparse Sampling}\label{subsec:analysis_sparse_sampling}
We conduct comprehensive ablation studies concerning various aspects of \ModelName's design in this section and Section~\ref{subsec:analysis_pretraining_and_e2e_training}.
If not otherwise stated, we randomly sample a single frame ($N_{train}{=}1$ and $T{=}1$) from full-length videos for training, and use the middle frame ($N_{test}{=}1$) for inference, with input image size $L{=}448$. 
All ablation results are on MSRVTT retrieval local val and MSRVTT-QA val sets.

\paragraph{Do we need larger input image size?} 
We compare models with different input image sizes $L \in \{224, 448, 768, 1000\}$, results shown in Table~\ref{tab:image_size}.
Compared to the model with $L{=}224$, larger input resolution improves performance on the retrieval task, while maintaining a similar performance on the QA task.
The best performance is achieved at around $L{=}448$. Further increasing the resolution does not provide significant performance boost.
\cite{jiang2020defense} shows that increasing input image size from 448 to 1333 always improves image VQA~\cite{antol2015vqa} performance with no sign of saturation, while we observe the performance converges at around 448 for MSRVTT retrieval and QA.
This is potentially because VQA images are typically of higher raw resolution than MSRVTT videos (we are only able to obtain videos at a maximum height of 240 pixels).
We expect higher resolution videos could further improve model performance.

\paragraph{Do we need densely sampled frames?} 
A common practice for video understanding and video+language understanding is to model densely sampled frames from the original video 
(\eg,~\cite{carreira2017quo,Xie_2018_ECCV} sample frames at 25 frames per second).
To understand the impact of using densely sampled frames, we conduct a set of controlled experiments. 
Specifically, we randomly sample a fixed-length 1-second clip from the video, then evenly sample $T {=} \{1, 2, 4, 8, 16\}$ frames within this clip for training. 
For inference, we use the middle clip of the video.
When multiple frames are used (\ie, $T$\textgreater1), we use mean pooling for temporal fusion. 

We also experiment with variants using additional 3D convolutions before mean pooling: ($i$) Conv3D: a standard 3D convolution layer with kernel size 3, stride 1; ($ii$) Conv(2+1)D: a spatial and temporal separable 3D convolution~\cite{tran2018closer,Xie_2018_ECCV}.
Adding 3D convolutions to 2D convolutions essentially leads to a design similar to Top-Heavy S3D architecture~\cite{Xie_2018_ECCV}, which shows better performance than full 3D convolutions on video action recognition and runs faster.

Results are shown in Table~\ref{tab:num_frames_in_training}.
Overall, models that use 3D convolutions perform worse than models that adopt a simple mean pooling. 
For mean pooling, we observe that using two frames provides a notable improvement over using a single frame.
However, models that use more than two frames perform similarly compared to the one using two frames, suggesting that two frames already represent enough local temporal information for the tasks.


\begin{table}[!t]
\tablestyle{5pt}{1.1}
\def\w{20pt} 
\scalebox{1.0}{
\begin{tabular}{c|c|R{12pt}R{\w}R{\w}R{\w}|c}
\multirow{2}{*}{$\mathcal{M}$} & \multirow{2}{*}{$T$} & \multicolumn{4}{c|}{ MSRVTT Retrieval } & MSRVTT- \\ \cline{3-6}
 &  & R1 & R5 & R10 & MdR & QA Acc. \\
\shline
- & 1 & 10.2 & 28.6 & 40.5 & 17.0 & 35.73 \\ 
\hline
\multirow{4}{*}{ Mean Pooling } & 2 & \bf 11.3 & 31.7 & 44.9 & 14.0 & \bf 36.02 \\
& 4 & 10.8 & 30.0 & 43.6 & 14.0  & 35.83 \\
& 8 & 10.6 & \bf 32.5 & \bf 45.0 & \bf 13.0 & 35.69 \\
& 16 & \bf 11.6 & \bf 33.9 & \bf 45.8 & \bf 13.0 & \bf 36.05 \\
\hline
\multirow{2}{*}{ Conv3D } & 2 & 8.7 & 27.3 & 40.2 & 17.0 & 34.85 \\
& 16 & 10.1 & 28.9 & 41.7 & 16.0 & 35.03 \\
\hline
\multirow{2}{*}{ Conv(2+1)D } & 2 & 7.3 & 24.1 & 35.6 & 22.0 & 34.13 \\
& 16 & 9.9 & 27.3 & 39.6 & 17.0 & 33.92 \\
\end{tabular}
}
\caption{
Impact of \textbf{\#frames ($T$) and temporal fusion function ($\mathcal{M}$).}
We use a 1-second clip for all experiments.
}
\label{tab:num_frames_in_training}

\end{table}


\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figs/num_infer_clips.pdf}
  \caption{
Impact of \textbf{\#inference clips ($N_{test}$).}
}
  \label{fig:num_clips_at_inferece}
  \vspace{-3mm}
\end{figure}



\paragraph{Do more clips at inference help?} 
At inference, we aggregate prediction scores from multiple densely sampled clips as the final score. 
To show how this strategy affects performance, we evenly sample $N_{test} \in \{1, 2, 4, 8, 16\}$ clips from a video and average their individual predictions at inference. 
For this experiment, we provide two models trained with different numbers of training frames per clip: one with a single frame and the other with two frames.
Both models use a single clip for training.
Results are shown in Figure~\ref{fig:num_clips_at_inferece}.
Adding more clips generally improves performance, especially the first few additions, but after a certain point performance saturates.
For example, in Figure~\ref{fig:num_clips_at_inferece} (\textit{left}), MSRVTT retrieval performance increases substantially as we use two and four clips, compared to using a single clip; then the performance gain gradually becomes marginal.





\begin{table}[!t]
\tablestyle{4pt}{1.1}
\def\w{19pt} 
\scalebox{1.0}{
\begin{tabular}{c|c|R{12pt}R{\w}R{\w}R{\w}|c}
\multirow{2}{*}{$\mathcal{G}$} & \multirow{2}{*}{$N_{train}$} & \multicolumn{4}{c|}{ MSRVTT Retrieval } & MSRVTT- \\ \cline{3-6}
 &  & R1 & R5 & R10 & MdR & QA Acc. \\
\shline
-  & 1 & 12.7 & 34.5 & 48.8 & 11.0 & 36.24 \\
\hline
\multirow{4}{*}{ Mean Pooling } & 2 & 13.3 & 37.1 & 50.6 & 10.0 & 35.94 \\
& 4 & 14.0 & 38.6 & 51.6 & 10.0 &  35.40 \\
& 8 & 13.4 & 36.4 & 49.7 & 11.0 &  35.76 \\
& 16 & 15.2 & \bf 39.4 & \bf 53.1 & \bf 9.0 & 35.33 \\
\hline
\multirow{2}{*}{ Max Pooling } & 2 & 8.5 & 28.7 & 42.2 & 14.0 & \bf 36.41 \\
& 16 & 12.5 & 33.1 & 46.8 & 12.0 & 36.25 \\
\hline
\multirow{2}{*}{ LogSumExp } & 2 & \bf 15.5 & 38.4 & 52.6 & \bf 9.0 & \bf 36.59 \\
& 16 & \bf 17.4 & \bf 41.5 & \bf 55.5 & \bf 8.0 & 36.16 \\
\end{tabular}
}
\caption{
Impact of \textbf{\#training clips ($N_{train}$) and score aggregation function ($\mathcal{G}$).}
All models use $N_{test}{=}$16 clips for inference.
}
\label{tab:num_clips_for_training}
\vspace{-3mm}
\end{table}


\begin{table}[!t]
\tablestyle{2pt}{1.1}
\def\w{19pt}
\scalebox{1.0}{
\begin{tabular}{c|c|R{12pt}R{\w}R{\w}R{\w}|c}
\multirow{2}{*}{Sampling Method} & \multirow{2}{*}{$N_{train}$} & \multicolumn{4}{c|}{ MSRVTT Retrieval } & MSRVTT- \\ \cline{3-6}
 &  & R1 & R5 & R10 & MdR & QA Acc. \\
\shline
\multirow{1}{*}{ Dense Uniform } & 16 & 15.5 & 39.6 & 55.0 & 9.0 & 35.88 \\
\hline
\multirow{3}{*}{ Sparse Random } & 1 & 12.7 & 34.5 & 48.8 & 11.0 & 36.24 \\
 & 2 & 15.5 & 38.4 & 52.6 & 9.0 & 36.59 \\
& 4 & \bf 15.7 & \bf 41.9 & \bf 55.3 & \bf 8.0 & \bf 36.67 \\
\end{tabular}
}
\caption{
\textbf{Sparse random sampling \textit{vs.} dense uniform sampling.}
All models use $N_{test}{=}$16 clips for inference. 
}
\label{tab:dense_uniform_vs_sparse_random}
\vspace{-3mm}
\end{table}



\paragraph{Do more clips in training help?} 
We randomly sample $N_{train}$ clips and aggregate scores from the clips with aggregation function $\mathcal{G}$ as the final score to calculate the training loss.
When multiple clips are used, information from these samples is aggregated through multiple instance learning to maximize the utility of these clips. 
To understand how this strategy affects model performance, we evaluate model variants that use $N_{train} \in \{1, 2, 4, 8, 16\}$ at training.
We also consider 3 different commonly used score aggregation functions for $\mathcal{G}$: mean pooling, max pooling, and LogSumExp~\cite{miech2020end}. In mean pooling and max pooling, the cross-clip pooling is performed over logits, followed by a softmax operator. In LogSumExp, logits from each clip are first fed through an element-wise exponential operator, followed by a cross-clip mean pooling. The aggregated output is further normalized by its own sum to make it eligible as a probability distribution. For simplicity, we always use the same aggregation function for training and inference. 
For a fair comparison, all models use a single frame per clip for training and 16 clips for inference, \ie, $T{=}1$ and $N_{test}{=}16$.


Results are shown in Table~\ref{tab:num_clips_for_training}.
In general, adding more clips helps, and the second added clip gives the most performance gain. 
For example, for models with LogSumExp, $N_{train}{=}2$ improves retrieval R1 score of $N_{train}{=}1$ by 2.8\%, while $N_{train}{=}16$ improves only 1.9\% upon $N_{train}{=}2$, even though it adds much more clips.
As for score aggregation function $\mathcal{G}$, LogSumExp works the best. 



\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{figs/memory_computation_cost.pdf}
  \vspace{-3mm}
  \caption{
\textbf{Memory and computation cost comparison} w.r.t. different numbers of clips ($N_{train}$) or frames ($T$) at training. 
\textbf{(a)}: Maximum allowed batch size with fixed $T{=}1$. 
\textbf{(b)}: Time cost for a single forward and backward pass with fixed $T{=}1$, batch size 8. 
\textbf{(c)}: Maximum allowed batch size with fixed $N_{train}{=}1$. 
\textbf{(d)}: Time cost for a single forward and backward pass with fixed $N_{train}{=}1$, batch size 8.
All experiments are conducted on a single NVIDIA V100 GPU with 32GB memory. 
MSRVTT retrieval performance is also added in (b, d) for reference. Best viewed in color.
}
  \label{fig:memory_computation_cost}
  \vspace{-3mm}
\end{figure}



\paragraph{Sparse Random Sampling \textit{vs.} Dense Uniform Sampling.} 
At each training step, \ModelName~randomly samples only a single or a few short clips from a full-length video.
Intuitively, this sparse random sampling strategy can be interpreted as a type of data augmentation where different subsets of clips for a video are used to calculate the loss at different training steps.
To show the effectiveness of this approach, we compare \ModelName~with a variant that uses uniformly sampled dense clips.
Specifically, we use the same \ModelName~architecture as before but always uses 16 uniformly sampled clips for training.
Table~\ref{tab:dense_uniform_vs_sparse_random} shows the comparison. 
Sparse random sampling with only 4 clips outperforms dense uniformly sampling with 16 clips across all metrics in both retrieval and QA tasks.
Meanwhile, using 4 clips is much more memory and computation efficient than using 16 clips, as we show in the next paragraph.
In addition to these two sampling approaches, it is also possible to choose clips using content-based methods such as~\cite{wu2019adaframe}. 
However, this requires an extra non-trivial selection step, and may also remove some of the data augmentation effect brought by random sampling.



\paragraph{Memory and Computation Cost.} 
Figure~\ref{fig:memory_computation_cost} shows a comparison of memory and computation cost \emph{w.r.t.} different numbers of clips ($N_{train}$) or frames ($T$) at training. 
We observe that using more clips or more frames at training considerably increases memory demand and computational cost.
For example, in Figure~\ref{fig:memory_computation_cost}~(\textit{a}), we see that the maximum allowed batch size for a single NVIDIA V100 GPU is 190 when $N_{train}{=}2$, compared to that of 16 when $N_{train}{=}16$.
Meanwhile, in Figure~\ref{fig:memory_computation_cost}~(\textit{b}), we see that the time cost increases almost linearly with $N_{train}$, while the performance improvement on MSRVTT retrieval is less significant.
These comparisons demonstrate the efficiency and effectiveness of the proposed sparse training strategy.


\subsection{Analysis of Pre-training/End-to-end Training}\label{subsec:analysis_pretraining_and_e2e_training}



\begin{table}[!t]
\tablestyle{5pt}{1.1}
\scalebox{1.0}{
\begin{tabular}{l|l|R{12pt}R{11pt}R{11pt}R{20pt}|c}
\multicolumn{2}{c|}{ Weight Initialization } & \multicolumn{4}{c|}{ MSRVTT Retrieval } & MSRVTT- \\  \cline{1-6}
CNN & transformer & R1 & R5 & R10 & MdR & QA Acc. \\
\shline
random & random & 0.3 & 0.4 & 0.9 & 506.0 & 28.05 \\
random & $\text{BERT}_{\textsc{base}}$ & 0.0 & 0.2 & 0.7 & 505.0 & 31.72 \\
TSN, K700 & $\text{BERT}_{\textsc{base}}$ & 5.7 & 22.1 & 33.1 & 23.0 & 35.40 \\
ImageNet & $\text{BERT}_{\textsc{base}}$  & 7.2 & 23.3 & 35.6 & 21.0 & 35.01 \\
grid-feat & $\text{BERT}_{\textsc{base}}$  & 7.4 & 21.0 & 30.7 & 26.0 & 35.27 \\
\hline
\multicolumn{2}{c|}{ image-text pre-training } & \bf 10.2 & \bf 28.6 & \bf 40.5 & \bf 17.0 & \bf 35.73 \\
\end{tabular}
}
\caption{
Impact of \textbf{weight initialization strategy}.
}
\label{tab:weight_init}
\vspace{-3mm}
\end{table}



\paragraph{Impact of Image-text Pre-training.} 
Our model is initialized with image-text pre-training on COCO and Visual Genome Captions, to obtain better-aligned visual and textual representations.
To validate the effectiveness of using image-text pre-training for weight initialization, we also evaluate variants that use other weight initialization strategies. 
Specifically, for CNN, we use weights from random initialization, image classification model pre-trained on ImageNet~\cite{deng2009imagenet}, frame-wise action recognition model TSN~\cite{wang2016temporal,2020mmaction2} pre-trained on Kinetics-700~\cite{smaira2020short,carreira2017quo}, or detection model grid-feat~\cite{jiang2020defense} pre-trained on Visual Genome~\cite{krishna2017visual}. 
For transformer and word embedding layers, we use weights from random initialization or pre-trained $\text{BERT}_{\textsc{base}}$ model~\cite{devlin2018bert}.
For random initialization, we use the default setup in PyTorch~\cite{paszke2019pytorch} and Transformer~\cite{Wolf2019HuggingFacesTS} libraries for CNN and transformer layers, respectively.
Results are summarized in Table~\ref{tab:weight_init}.
We notice that randomly initializing CNN leads to massive performance degradation or even training failure, we hypothesize that it is mostly because of the difficulty of training large models on relatively small datasets (\eg, MSRVTT retrieval train split: 7K videos).
The best performance is achieved using image-text pre-trained weights, clearly indicating the benefit of utilizing image-text pre-training for video-text tasks.



\paragraph{Impact of End-to-End Training.} 
The standard paradigm for video-and-language understanding is to train models with offline extracted features, in which the task objective does not affect the video and text encoding process. 
In this work, we train our model in an end-to-end manner, allowing the model to finetune feature representations by leveraging task supervision. 
In Table~\ref{tab:e2e}, we compare our model with variants that freeze portions of the parameters.
Overall, the best performance is achieved by our model, showing the importance of end-to-end training.
Note that all the models in Table~\ref{tab:e2e} are finetuned from our end-to-end image-text  pre-trained model, which partly resolves the multimodal feature disconnection issue in Section~\ref{sec:intro}. 
Thus, we expect smaller improvement from further end-to-end finetuning.



\paragraph{Main Conclusions} from the analyses in Section~\ref{subsec:analysis_sparse_sampling} and Section~\ref{subsec:analysis_pretraining_and_e2e_training} are summarized as follows:
($i$) Larger input image size helps improve model performance, but the gain diminishes when image size is larger than 448;
($ii$) Sparsely sampling 2 frames from each clip performs on par with dense sampling 16 frames, showing that just one or two frames are sufficient for effective video-and-language training; 
mean-pooling is more effective than 3D Conv when fusing information from different frames;
($iii$) More clips at inference helps improve model performance; aggregation strategy of predictions across clips affects final performance;
($iv$) Sparse (random) sampling is more effective than dense uniform sampling while being more memory and computation efficient;
($v$) Image-text pre-training benefits video-text tasks; 
and ($vi$) End-to-end training improves model performance.



\begin{table}[!t]
\tablestyle{2pt}{1.1}
\def\w{20pt}  
\scalebox{1.0}{
\begin{tabular}{c|c|R{12pt}R{16pt}R{18pt}R{\w}|c}
\multicolumn{2}{c|}{ Parameters Trainable? } & \multicolumn{4}{c|}{ MSRVTT Retrieval } & MSRVTT- \\ \cline{1-6}
\quad\;$\mathcal{F}_{v}$\quad\; & $\mathcal{F}_{l}$ & R1 & R5 & R10 & MdR & QA Acc. \\
\shline
\xmark & \xmark & 8.0 & 27.2 & 38.9 & 17.0 & \bf 35.78 \\
\xmark & \cmark & 9.0 & 27.5 & 39.4 & 18.0 & 35.50 \\
\hline
\cmark & \cmark & \bf 10.2 & \bf 28.6 & \bf 40.5 & \bf 17.0 & 35.73 \\
\end{tabular}
}
\caption{
Impact of \textbf{end-to-end training}.
}
\label{tab:e2e}
\vspace{-3mm}
\end{table}





\begin{table*}[t!]\centering
\subfloat[MSRVTT 1K test set.
\label{tab:main_msrvtt_retrieval}]{\tablestyle{2pt}{1.1}
\def\w{15pt} 
\begin{tabular}{l|R{14pt}R{\w}R{\w}r}
\multicolumn{1}{c|}{Method} & R1 & R5 & R10 & MdR \\
\shline
\demph{HERO~\cite{li2020hero} ASR, PT} & \demph{20.5} & \demph{47.6} & \demph{60.9} & \demph{-} \\
\hline
JSFusion~\cite{yu2018joint} & 10.2 & 31.2 & 43.2 & 13.0 \\
HT~\cite{miech2019howto100m} PT & 14.9 & 40.2 & 52.8 & 9.0 \\
ActBERT~\cite{zhu2020actbert} PT & 16.3 & 42.8 & 56.9 & 10.0 \\
HERO~\cite{li2020hero} PT & 16.8 & 43.4 & \bf 57.7 & - \\
\hline
\ModelName~4\x1 & \bf 19.8 & \bf 45.1 & 57.5 & \bf 7.0 \\
\ModelName~8\x2 & \bf 22.0 & \bf 46.8 & \bf 59.9 & \bf 6.0 \\
\multicolumn{5}{c}{} \\
\end{tabular}
}
\hspace{1mm}
\subfloat[DiDeMo test set.  \label{tab:main_didemo_retrieval}]{\tablestyle{2pt}{1.1}
\def\w{15pt}  
\begin{tabular}{l|R{14pt}R{\w}R{\w}r}
\multicolumn{1}{c|}{Method} & R1 & R5 & R10 & MdR  \\
\shline
\demph{CE~\cite{liu2019use}} & \demph{16.1}	& \demph{41.1} & \demph{-} & \demph{8.3}  \\
\hline
S2VT~\cite{venugopalan2014translating} & 11.9 & 33.6 & - & 13.0 \\
FSE~\cite{zhang2018cross}	& 13.9	& 36.0 & - & 11.0 \\
\hline
\ModelName~4\x1 & \bf 19.9 & \bf 44.5 & \bf 56.7 & \bf 7.0  \\
\ModelName~8\x2 &\bf 20.4	& \bf 48.0	& \bf 60.8	& \bf 6.0\\
\multicolumn{5}{c}{} \\
\multicolumn{5}{c}{} \\
\multicolumn{5}{c}{} \\
\end{tabular}
}
\hspace{1mm}
\subfloat[ActivityNet Captions val1 set. 
\label{tab:main_anet_cap_retrieval}]{\tablestyle{2pt}{1.1}
\def\w{15pt} 
\begin{tabular}{l|R{14pt}R{\w}R{\w}r}
\multicolumn{1}{c|}{Method} & R1 & R5 & R10 & MdR  \\
\shline
\demph{CE~\cite{liu2019use}} & \demph{18.2}	& \demph{47.7} & \demph{-} & \demph{6.0}  \\
\demph{MMT~\cite{gabeur2020multi}} & \demph{22.7} & \demph{54.2} & \demph{93.2} & \demph{5.0}  \\
\demph{MMT~\cite{gabeur2020multi} PT} & \demph{28.7} & \demph{61.4} & \demph{94.5} & \demph{3.3}  \\
\hline
Dense~\cite{krishna2017dense} & 14.0 & 32.0 & - & 34.0 \\
FSE~\cite{zhang2018cross}	& 18.2	& 44.8 & - & 7.0 \\
HSE~\cite{zhang2018cross}	& 20.5	& \bf 49.3 & - & - \\
\hline
\ModelName~4\x2$^*$ & \bf 20.9 & 48.6 & \bf 62.8 & \bf 6.0 \\
\ModelName~4\x2$^*$ ($N_{test}{=}$20)  & \bf 21.3 & \bf 49.0 & \bf 63.5 & \bf 6.0 \\
\end{tabular}
}
\caption{
\textbf{Comparison with state-of-the-art methods on text-to-video retrieval}.
\ModelName~models with different training input sampling methods are denoted by $N_{train}$\x$T$. 
We use $N_{test}{=}16$ if not otherwise stated. 
We gray out models that used features other than appearance and motion for a fair comparison, \eg, CE used appearance, scene, motion, face, audio, OCR, ASR features from 11 different models.
\textit{PT} indicates the model is pre-trained on HowTo100M. * denotes models use 2-second clips instead of the default 1-second clips.
\label{tab:main_retrieval}
}
\vspace{-3mm}
\end{table*}


\begin{table*}[!t]
\subfloat[TGIF-QA test set.
\label{tab:main_tgif_qa}]{\tablestyle{3pt}{1.1}
\def\w{30pt} 
\begin{tabular}{l|C{24pt}C{\w}C{\w}}
\multicolumn{1}{c|}{Method} & Action & Transition & FrameQA \\
\shline
ST-VQA~\cite{jang2017tgif} & 60.8 & 67.1 & 49.3 \\
Co-Memory~\cite{gao2018motion} & 68.2 & 74.3 & 51.5 \\
PSAC~\cite{li2019beyond} & 70.4 & 76.9 & 55.7 \\
Heterogeneous Memory~\cite{fan2019heterogeneous} & 73.9 & 77.8 & 53.8 \\
HCRN~\cite{le2020hierarchical} & 75.0 & 81.4 & 55.9 \\
QueST~\cite{jiang2020divide} & 75.9 & 81.0 & \bf 59.7 \\
\hline
\ModelName~1\x1 ($N_{test}{=}$1) & \bf 82.9 & \bf 87.5 & 59.4 \\
\ModelName~1\x1 & \bf 82.8 & \bf 87.8 & \bf 60.3 \\
\end{tabular}
}
\hspace{2mm}
\subfloat[MRSVTT-QA test set.
\label{tab:main_msrvtt_qa}]{\tablestyle{3pt}{1.1}
\def\w{40pt} 
\begin{tabular}{l|C{30pt}}
\multicolumn{1}{c|}{Method} & Accuracy \\
\shline
ST-VQA~\cite{jang2017tgif} (by \cite{fan2019heterogeneous}) & 30.9  \\
Co-Memory~\cite{gao2018motion} (by \cite{fan2019heterogeneous}) & 32.0 \\
AMU~\cite{xu2017video} & 32.5 \\
Heterogeneous Memory~\cite{fan2019heterogeneous} & 33.0 \\
HCRN~\cite{le2020hierarchical} & 35.6 \\
\hline
\ModelName~4\x1 & \bf 37.0 \\
\ModelName~8\x2 & \bf 37.4 \\
\multicolumn{2}{c}{} \\
\end{tabular}
}
\hspace{2mm}
\subfloat[MRSVTT multiple-choice test. \label{tab:main_msrvtt_multiple_choice_test}]{\tablestyle{3pt}{1.1}
\def\w{40pt} 
\begin{tabular}{l|C{30pt}}
\multicolumn{1}{c|}{Method} & Accuracy \\
\shline
SNUVL~\cite{yu2016video} (by \cite{yu2018joint})  & 65.4 \\
ST-VQA~\cite{jang2017tgif} (by \cite{yu2018joint}) & 66.1 \\
CT-SAN~\cite{yu2017end} (by \cite{yu2018joint}) & 66.4 \\
MLB~\cite{kim2016hadamard} (by \cite{yu2018joint}) & 76.1 \\
JSFusion~\cite{yu2018joint} & 83.4 \\
ActBERT~\cite{zhu2020actbert} PT & 85.7 \\
\hline
\ModelName~4\x1 & \bf 87.9 \\
\ModelName~8\x2 & \bf 88.2 \\
\end{tabular}
}\caption{
\textbf{Comparison with state-of-the-art methods on video question answering}.
}
\label{tab:main_video_qa}
\end{table*}



\subsection{Comparison to State-of-the-art}
For evaluation, we compare \ModelName~with state-of-the-art methods on text-to-video retrieval and video question answering tasks. 
We denote models using different sampling methods at training as \textit{\ModelName~$N_{train}$\x$T$}, (randomly sample $N_{train}$ 1-second clips for training, each contains $T$ uniformly sampled frames of size $L{=}448$).
We use LogSumExp to aggregate scores from multiple clips.
At inference time, if not otherwise stated, we aggregate scores from $N_{test}{=}$16 uniformly sampled clips.


\paragraph{Text-to-Video Retrieval.}
Table~\ref{tab:main_retrieval} summarizes results on text-to-video retrieval. 
In Table~\ref{tab:main_msrvtt_retrieval}, \ModelName~achieves significant performance gain over existing methods on MSRVTT retrieval, including HT~\cite{miech2019howto100m}, ActBERT~\cite{zhu2020actbert}, and HERO~\cite{li2020hero}, which are pre-trained on 136M clip-caption pairs from HowTo100M~\cite{miech2019howto100m}. 
Under a fair comparison, \ModelName~4\x1 outperforms HERO~\cite{li2020hero} by 3.0\% on R@1.
Note that HERO uses SlowFast~\cite{feichtenhofer2019slowfast} features extracted from full-length videos at a very dense frame rate of 21 FPS (\ie, on average 310 frames per video for MSRVTT), while \ModelName~4\x1 uses only 4 randomly sampled frames.
When more frames are used, \ModelName~8\x2 achieves even higher performance, surpassing HERO by 5.2\%.
Compared to the HERO ASR model that uses extra input from Automatic Speech Recognition (ASR), \ModelName~still obtains 1.5\% higher R1 score.


Similarly, on DiDeMo and ActivityNet Captions paragraph-to-video retrieval tasks (Table~\ref{tab:main_didemo_retrieval} and Table~\ref{tab:main_anet_cap_retrieval}), we notice our best \ModelName~models outputform CE~\cite{liu2019use} by 4.3\% and 3.1\% on R1, respectively, despite CE's use of appearance, scene, motion, face, audio, OCR, ASR features densely extracted from 11 different models.
ActivityNet Caption videos are on average 180-second long. 
In Table~\ref{tab:main_anet_cap_retrieval} we show \ModelName~performs competitively with existing methods that model long-range relations in this dataset. 
Especially, \ModelName~obtains 0.8\% higher R1 than HSE~\cite{zhang2018cross} and is competitive compared to MMT~\cite{gabeur2020multi} that uses extra audio features\footnote{\cite{gabeur2020multi} shows that adding audio features greatly improves performance.}, even though \ModelName~4\x2$^*$ ($N_{test}{=}20$) samples only 8-second clips from 180-second videos at each training step, and uses only 40-second content for inference.
We expect \ModelName's performance to be further improved by sampling more clips during training and inference.
Meanwhile, we also encourage future work to explore combining extra input signals, such as audio, into the \ModelName~framework for better performance.

\paragraph{Video Question Answering.}
Table~\ref{tab:main_video_qa} shows evaluation results on video question answering. 
Across all three tasks, \ModelName~achieves significant performance gain.
In Table~\ref{tab:main_tgif_qa}, \ModelName~1\x1 outperforms prior state-of-the-art QueST~\cite{jiang2020divide} by 6.9\%, 6.8\%, and 0.6\% on TGIF-QA Action, Transition, and FrameQA tasks, respectively.
This is especially surprising considering \ModelName~1\x1 uses only a single randomly sampled frame from the videos at each training step, while QueST uses 10 uniformly sampled frames.
Moreover, when using only a single frame (the middle frames of the videos) for inference, \ModelName~1\x1 ($N_{test}{=}1$) already far outperforms QueST on Action and Transition tasks, and is on par with QueST on FrameQA.
In Table~\ref{tab:main_msrvtt_qa}, \ModelName~4\x1 outperforms HCRN~\cite{le2020hierarchical} by 1.4\% on MSRVTT-QA.
Note that HCRN adopts a sophisticated hierarchical relation modeling network over the entire video sequence of 24 clips at training time, while we use only four randomly sampled frames. 
Using more frames further increases this performance gap to 1.8\%.
Table~\ref{tab:main_msrvtt_multiple_choice_test} shows \ModelName~8\x2 improves ActBERT~\cite{zhu2020actbert} model pre-trained on HowTo100M by 2.5\%, on MSRVTT multiple choice test task.
