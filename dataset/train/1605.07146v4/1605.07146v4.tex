\documentclass{bmvc2k}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{soul}

\def\titlespace{\hspace{0.4cm}}

\soulregister\cite7
\soulregister\ref7
\soulregister\pageref7

\title{\titlespace Wide Residual Networks}

\addauthor{Sergey Zagoruyko}{sergey.zagoruyko@enpc.fr}{1}
\addauthor{Nikos Komodakis}{nikos.komodakis@enpc.fr}{1}

\addinstitution{
 Universit\'e Paris-Est, \'Ecole des Ponts ParisTech\\
 Paris, France
}

\runninghead{Sergey Zagoruyko and Nikos Komodakis}{Wide residual networks}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}
\newcommand{\ve}[1]{\mathbf{#1}} 

\begin{document}

\maketitle

\begin{abstract}
  Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at \url{https://github.com/szagoruyko/wide-residual-networks}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Convolutional neural networks have seen a gradual increase of the number of layers in the last few years, starting from AlexNet \cite{AlexNet}, VGG \cite{Simonyan15}, Inception \cite{GoogLeNet} to Residual \cite{he2015deep} networks, corresponding to improvements in many image recognition tasks. The superiority of deep networks has been spotted in several works in the recent years \cite{Bianchini,MontufarPCB14}. However, training deep neural networks has several difficulties, including exploding/vanishing gradients and degradation. Various techniques were suggested to enable training of deeper neural networks, such as well-designed initialization strategies \cite{GlorotAISTATS2010,journals/corr/HeZR015}, better optimizers \cite{SutskeverMartensDahlHinton_icml2013}, skip connections \cite{dsn,AISTATS2012_RaikoVL12}, knowledge transfer \cite{Romero-et-al-TR2014,Chen:ICLR16} and layer-wise training \cite{Schmidhuber:92ncchunker}.

The latest residual networks \cite{he2015deep}  had a large success winning ImageNet and COCO 2015 competition and achieving state-of-the-art in several benchmarks, including object classification on ImageNet and CIFAR, object detection and segmentation on PASCAL VOC and MS COCO. Compared to Inception architectures they show better generalization, meaning the features can be utilized in transfer learning with better efficiency. Also, follow-up work showed that residual links speed up convergence of deep networks \cite{inception-v4}. Recent follow-up work explored the order of activations in residual networks, presenting identity mappings in residual blocks \cite{basicblock2} and improving training of very deep networks. Successful training of very deep networks was also shown to be possible through the use of highway networks  \cite{highway}, which is an architecture  that had been proposed prior to residual networks. The essential difference between residual and highway networks is that in the latter residual links are gated and weights of these gates are learned.

Therefore, up to this point, the  study of residual networks has focused mainly  on the order of activations inside a ResNet block and the depth of residual networks.  In this work we attempt to conduct an experimental study that goes beyond the above points. By doing so, our goal is  to explore a much richer set of  network architectures  of  ResNet blocks and thoroughly examine how several other different aspects besides the order of activations affect performance. As we explain below, such an exploration of architectures has led to   new interesting findings  with great practical importance concerning residual networks.

\textbf{Width vs depth in residual networks}.
The problem of shallow vs deep networks has been in discussion for a long time in machine learning \cite{LarochelleH2007,Bengio+chapter2007} with pointers to the circuit complexity theory literature showing that shallow circuits can require exponentially more components than deeper circuits. The authors of residual networks tried to make them as thin as possible in favor of increasing their depth and having less parameters, and even introduced a <<bottleneck>> block which makes ResNet blocks even thinner.
\begin{figure}
  \centering
  \subfigure[basic]{\label{fig:blocks_basic}\includegraphics[scale=0.4]{images/basic_a.pdf}}
  \subfigure[bottleneck]{\label{fig:blocks_bottleneck}\includegraphics[scale=0.4]{images/basic_b.pdf}}
  \subfigure[basic-wide]{\label{fig:blocks_wide}\includegraphics[scale=0.4]{images/basic_c.pdf}}
  \subfigure[wide-dropout]{\label{fig:blocks_dropout}\includegraphics[scale=0.4]{images/basic_d.pdf}}
  \caption{Various residual blocks used in the paper. Batch normalization and ReLU precede each convolution (omitted for clarity)}
  \vspace{-0.4cm}
  \label{fig:blocks}
\end{figure}

 We note, however, that the residual block with identity mapping that allows to train very deep networks is at the same time a weakness of residual networks. As gradient flows through the network there is nothing to force it to go through residual block weights and it can avoid learning anything during training, so it is possible that there is either only a few blocks that learn useful representations, or many blocks share very little information with small contribution to the final goal. This problem was formulated as diminishing feature reuse in \cite{highway}.
The authors of \cite{stochastic_depth} tried to address this problem with the idea of randomly disabling residual blocks during training. This method can be viewed as a special case of dropout \cite{srivastava14a}, where each residual block has an identity scalar weight on which dropout is applied. The effectiveness of this approach proves the hypothesis above.

Motivated by the above observation, our work builds on top of \cite{basicblock2} and tries to answer the question of how wide deep residual networks should be and address the problem of training. In this context, we  show  that the  widening of  ResNet blocks (if done properly) provides a much more effective way of improving performance of residual networks compared to increasing their depth.
 In particular, we   present wider deep residual networks that significantly improve over \cite{basicblock2},  having \textit{50 times} less layers and being more than 2 times faster.
 We call the resulting network architectures \emph{wide residual networks}. For instance, our wide 16-layer deep network has the same accuracy as a 1000-layer thin deep network and a comparable number of parameters, although being several times faster to train. This type of experiments thus seem to indicate that the main power of deep residual networks is in residual blocks, and that the effect of depth is supplementary.
 We  note that one can train even better wide residual networks  that have twice as many parameters (and more), which suggests that to further improve performance by increasing depth of thin networks   one needs to add thousands of layers in this case.

\textbf{Use of dropout in ResNet blocks.}
Dropout was first introduced in \cite{srivastava14a} and then was adopted by many successful architectures as \cite{AlexNet,Simonyan15} etc. It was mostly applied on top layers that had a large number of parameters to prevent feature coadaptation and overfitting. It was then mainly substituted by batch normalization  \cite{icml2015_ioffe15} which was introduced as a technique to reduce internal covariate shift in neural network activations by normalizing them to have specific distribution. It also works as a regularizer and the authors experimentally showed that a network with batch normalization achieves better accuracy than a network with dropout. In our case, as widening of residual blocks results in an increase of the number of parameters, we studied the effect of dropout to regularize training and prevent overfitting. Previously, dropout in residual networks was studied in \cite{basicblock2} with dropout being inserted in the identity part of the block, and the authors showed negative effects of that. Instead, we argue here that dropout should be inserted between convolutional layers.
Experimental results on wide residual networks show that this leads to consistent  gains, yielding even new state-of-the-art results (\eg, 16-layer-deep wide residual network with dropout  achieves 1.64\% error on SVHN).

In summary, the contributions of this work are as follows:
\begin{itemize}
    \setlength\itemsep{0.1em}
  \item{We present a detailed  experimental study of residual network architectures that thoroughly examines several important aspects of ResNet block structure.}
  \item{We propose a novel \textit{widened} architecture for ResNet blocks that allows for residual networks with significantly improved  performance.}
  \item{We propose a new way of utilizing  dropout within deep residual networks so as to properly regularize them and prevent overfitting during training. }
  \item{Last, we show that our proposed ResNet architectures achieve state-of-the-art results on several datasets dramatically improving accuracy and speed of residual networks.}
\end{itemize}

\section{Wide residual networks}

Residual block with identity mapping can be represented by the following formula:


where  and  are input and output of the -th unit in the network,  is a residual function and  are parameters of the block. Residual network consists of sequentially stacked residual blocks.

In \cite{basicblock2} residual networks consisted of two type of blocks:

\begin{itemize}
  \item{\textit{basic} - with two consecutive  convolutions with batch normalization and ReLU preceding convolution: \texttt{conv-conv} Fig.\ref{fig:blocks_basic}}
  \item{\textit{bottleneck} - with one  convolution surrounded by dimensionality reducing and expanding  convolution layers: \texttt{conv-conv-conv} Fig.\ref{fig:blocks_bottleneck}}
\end{itemize}

Compared to the original architecture \cite{he2015deep} in \cite{basicblock2} the order of batch normalization, activation and convolution in residual block was changed from \texttt{conv}-\texttt{BN}-\texttt{ReLU} to \texttt{BN}-\texttt{ReLU}-\texttt{conv}. As the latter was shown to train faster and achieve better results we don't consider the original version.
Furthermore,
so-called  <<bottleneck>> blocks were initially used to make blocks less computationally expensive to increase the number of layers. As we want to study the effect of widening and <<bottleneck>> is used to make networks thinner we don't consider it too,  focusing instead on  <<basic>> residual architecture.

There are essentially three simple ways to increase representational power of residual blocks:
\vspace{-0.1cm}
\begin{itemize}
    \setlength\itemsep{-0.2em}
  \item{to add more convolutional layers per block}
  \item{to widen the convolutional layers by adding more feature planes}
  \item{to increase filter sizes in convolutional layers}
\end{itemize}

As small filters were shown to be very effective in several works including \cite{Simonyan15,inception-v4} we do not  consider using filters larger than .
Let us also introduce two factors, deepening factor  and widening factor , where  is the number of convolutions in a block and  multiplies the number of features in convolutional layers, thus the baseline <<basic>> block corresponds to  , . Figures \ref{fig:blocks_basic} and \ref{fig:blocks_wide} show schematic examples of <<basic>> and <<basic-wide>> blocks respectively.


\newcommand{\blocka}[2]{
  -.1em]
        \text{33, #1}
      \end{array}
    \right]\times\timesB(3,3)32\times32\times\times\times\times1\times18\times8kk=1NB(3,3)Nkk = 1B(M)MB(3,1)3\times31\times13\times31\times11\times13\times3B(1,3)B(1,3)B(3,1,1)B(3,3)B(3,1,3)1\times1B(1,3,1)B(1,3)1\times13\times3B(3,1)B(3,1,1)lldddlkldkdkk=1k>1nknkk=2B(3,3)32\times32k=2B(1,3,1)B(3,1)B(1,3)B(3,1,1)B(3,3)B(3,1,3)k=2llBB(1,3,1)B(3,1)B(1,3)B(3,1,1)3\times3B(3,3)B(3,1,3)B(3,3)B(3,1)B(3,1,3)B(3,3)B(3,1,3)3\times3l3\times3l \in [1,2,3,4]\times10^6B(3,3)B(3,3,3)B(3,3,3,3)B(3)B(3,3)B(3,3)kkk=8k=10\times10^6\times10^6k3.65kk56\times10^6k3\times32\times$ faster than the best-performing pre-activation ResNet-200, althouth having slightly more parameters (table~\ref{table:imagenet_bottleneck}). In general, we find that, unlike CIFAR, ImageNet networks need more width at the same depth to achieve the same accuracy. It is however clear that it is unnecessary to have residual networks with more than 50 layers due to computational reasons.

We didn't try to train bigger bottleneck networks as 8-GPU machines are needed for that.

\begin{table}[ht]
  \centering\small
  \begin{tabular}{c|c|c|c|c|c}
    \hline
\multicolumn{2}{c|}{width} & 1.0 & 1.5 & 2.0 & 3.0 \\ \hline
    \multirow{2}{*}{WRN-18} & top1,top5 & 30.4, 10.93 & 27.06, 9.0 & 25.58, 8.06 & 24.06, 7.33 \\
    & \#parameters & 11.7M & 25.9M & 45.6M & 101.8M \\
    \hline
    \multirow{2}{*}{WRN-34} & top1,top5 & 26.77, 8.67 & 24.5, 7.58 & 23.39, 7.00 &  \\
    & \#parameters & 21.8M & 48.6M & 86.0M \\
    \hline
  \end{tabular}
  \caption{ILSVRC-2012 validation error (single crop) of non-bottleneck ResNets for various widening factors. Networks with a comparable number of parameters achieve similar accuracy, despite having 2 times less layers.}
  \label{table:imagenet_non_bottleneck}
\end{table}

\begin{table}[ht]
  \centering\small
  \begin{tabular}{l|c|c|c|c}
    \hline
    Model & top-1 err, \% & top-5 err, \% & \#params & time/batch 16 \\ \hline
    ResNet-50 & 24.01 & 7.02 & 25.6M & 49 \\
    ResNet-101 & 22.44 & 6.21 & 44.5M & 82 \\
    ResNet-152 & 22.16 & 6.16 & 60.2M & 115 \\
    \bf{WRN-50-2-bottleneck} & 21.9 & 6.03 & 68.9M & 93 \\
    pre-ResNet-200 & 21.66 & 5.79 & 64.7M & 154 \\
    \hline
  \end{tabular}
  \caption{ILSVRC-2012 validation error (single crop) of bottleneck ResNets. Faster WRN-50-2-bottleneck outperforms ResNet-152 having 3 times less layers, and stands close to pre-ResNet-200.}
  \label{table:imagenet_bottleneck}
\end{table}

We also used WRN-34-2 to participate in COCO 2016 object detection challenge, using a combination of MultiPathNet \cite{Zagoruyko2016Multipath} and LocNet \cite{gidaris2016locnet}. Despite having only 34 layers, this model achieves state-of-the-art single model performance, outperforming even ResNet-152 and Inception-v4-based models. 

Finally, in table~\ref{table:overall} we summarize our best WRN results over various commonly used datasets.

\begin{table}[ht]
  \centering\small
  \begin{tabular}{l|l|c|c}
    \hline
    Dataset & model & dropout & test perf. \\
    \hline
    CIFAR-10  & WRN-40-10 & \chk & 3.8\% \\
    CIFAR-100 & WRN-40-10 & \chk & 18.3\% \\
    SVHN      & WRN-16-8 & \chk  & 1.54\% \\
    ImageNet (single crop) & WRN-50-2-bottleneck & & 21.9\% top-1, 5.79\% top-5 \\
    COCO test-std & WRN-34-2 & & 35.2 mAP \\
    \hline
  \end{tabular}
  \caption{Best WRN performance over various datasets, single run results. COCO model is based on WRN-34-2 (wider basicblock), uses VGG-16-based AttractioNet proposals, and has a LocNet-style localization part. To our knowledge, these are the best published  results for CIFAR-10, CIFAR-100, SVHN, and COCO (using non-ensemble models).}
  \label{table:overall}
\end{table}

\subsubsection*{Computational efficiency} Thin and deep residual networks with small kernels are against the nature of GPU computations because of their sequential structure. Increasing width helps effectively balance computations in much more optimal way, so that wide networks are many times more efficient than thin ones as our benchmarks show. We use cudnn v5 and Titan X to measure forward+backward update times with minibatch size 32 for several networks, the results are in the figure \ref{fig:benchmark}. We show that our best CIFAR wide WRN-28-10 is 1.6 times faster than thin ResNet-1001. Furthermore, wide WRN-40-4, which has  approximately the same accuracy as ResNet-1001, is 8 times faster.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{./images/benchmark-edited.pdf}
  \caption{Time of forward+backward update per minibatch of size 32 for wide and thin networks(x-axis denotes network depth and widening factor). Numbers beside bars indicate test error on CIFAR-10, on top - time (ms). Test time is a proportional fraction of these benchmarks. Note, for instance, that wide WRN-40-4 is 8 times faster than thin ResNet-1001 while having approximately the same accuracy.
}
  \vspace{-0.2cm}
  \label{fig:benchmark}
\end{figure}


\subsubsection*{Implementation details} In all our experiments we use SGD with Nesterov momentum and cross-entropy loss. The initial learning rate is set to 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9 and minibatch size to 128. On CIFAR learning rate dropped by 0.2 at 60, 120 and 160 epochs and we train for total 200 epochs. On SVHN initial learning rate is set to 0.01 and we drop it at 80 and 120 epochs by 0.1, training for total 160 epochs. Our implementation is based on Torch \cite{torch}. We use \cite{optimize-net} to reduce memory footprints of all our networks. For ImageNet experiments we used \texttt{fb.resnet.torch} implementation \cite{fb.resnet.torch}. Our code and models are available at \url{https://github.com/szagoruyko/wide-residual-networks}.

\section{Conclusions}

We presented a study on the width of residual networks as well as   on the use of dropout in residual architectures. Based on this study, we  proposed a   wide residual network architecture that provides state-of-the-art results on several commonly used benchmark datasets (including CIFAR-10, CIFAR-100, SVHN and COCO), as well as significant improvements on ImageNet. We demonstrate that wide networks with only 16 layers can significantly outperform 1000-layer deep networks on CIFAR, as well as that 50-layer outperform 152-layer on ImageNet, thus showing that the main power of residual networks is in residual blocks, and not in extreme depth as claimed earlier. Also, wide residual networks are several times faster to train. We think that these intriguing findings will help further advances in research in deep neural networks.

\section{Acknowledgements}

We thank startup company VisionLabs and Eugenio Culurciello for giving us access to their clusters, without them ImageNet experiments wouldn't be possible. We also thank Adam Lerer and Sam Gross for helpful discussions. Work supported by EC project FP7-ICT-611145 ROBOSPECT.

\bibliography{bibliography}
\end{document}
