In this section, we first detail the experimental settings and the compared methods; then the main results and ablation study are described; finally, we report the human evaluation results and additional analysis based on some cases. Our experiments are conducted on \textit{Restoration-200K} \cite{pan2019improving} \textcolor{red}{and \textit{CANARD} \cite{elgohary-etal-2019-unpack}}. 




\subsection{Experiment Settings}\label{setting}
We initialize  SARG with RoBERTa-wwm-ext \cite{chinese-bert-wwm}, the hidden size is set to 768, the number of attention heads to 12, the number of attention layers to 12, and the size of chinese vocabulary to 21128. Adam optimizer is utilized, the loss of tagger weighted to , the initial learning rate is 5e-5 and the batch size is 64. For the model with coverage mechanism, we first optimize the model 14000 steps with no coverage loss and then train it until convergence with coverage loss weighted to . The above hyperparameters are all tuned on the standard validation data.

The according automatic evaluation metrics are utilized as in previus works \cite{pan2019improving, elgohary-etal-2019-unpack}, which contain BLEU, ROUGE, and restoration score. Specifically, the n-gram restoration precision, recall, and F-score are calculated as:
\begin{linenomath}

\end{linenomath}


\subsection{Compared Methods}
We compare the performance of our proposed SARG with the following methods:
\begin{itemize}
	\item \textbf{Copy}: in this baseline, the rewrite results are set to be the same as the input original utterance.
	
	\item \textbf{CopyNet}: in this baseline, a LSTM-based Seq2Seq model with attention and a copy mechanism is employed.
	
	\item \textbf{PAC}\ \cite{pan2019improving}:\ this model restores the incomplete utterance in a cascade way: firstly, select the remained words by fintuning BERT, and then roughly concatenate the selected words, history, original utterance and feed them into a standard pointer-generator network.
	
	\item \textbf{T-Ptr-} \footnote{We re-implement the transformer-based method and evaluate on the same blind test set for the fair comparison.}\ \cite{su2019improving}:\ this model solves such restoration task in an end-to-end way. It employs six layers of transformer blocks as encoder and another six layers of transformer blocks as pointer decoder. Moreover, to emphasize the difference between history and utterance, it takes two individual channels in the encoder-decoder attention.
	
	\item \textbf{Seq2Seq-Uni}:\ traditional Seq2Seq has the isolated transformer encoder and decoder, which is not convenient for loading the weights from a pretrained model like BERT. Thus, we employ the unified transformer blocks \cite{dong2019unified}, which supports both bi-directional encoding and uni-directional decoding flexibly via specific attention masks, as the backbone of Seq2Seq. 
\end{itemize}

\subsection{Main Results}

\begin{table*}[!htb]
	\centering
	
	\begin{tabular}{|l|ccc|cccc|c|}
		\hline
		Model &  &  &  & BLEU-1 & BLEU-2 & ROUGE-1 & ROUGE-2   & Time \\ \hline
		PAC\ (greedy) & 61.1 & 46.9 & 37.7 & 89.5 & 85.7 & 91.2 & 82.2 &-\\
		PAC\ (n\_beam=5) & \textbf{63.7} & 49.7 & 40.4 & 89.9 & 86.3 & 91.6 & 82.8   & - \\ \hline
		T-Ptr-\ (greedy) & 47.1 & 37.5 & 31.3  & 88.3 & 85.7 & 90.5 & 83.8 & 522\ s \\ 
		T-Ptr-\ (n\_beam=5) & 51.0 & 40.4 & 33.3  & 90.3 & 87.4 & 90.1 & 83.0 & 602\ s \\ \hline
		Seq2Seq-Uni\ (greedy)  & 55.2 & 44.8 & 38.3 & 90.1 & 87.5 & 91.4 & 84.9 & 321\ s \\ 
		Seq2Seq-Uni\ (n\_beam=5)  & 56.8 & 46.4 & 39.8  & 90.8 & 88.3 & 91.4 & 85.0 & 467\ s \\ \hline
		SARG\ (greedy) & 62.4 & 52.5 & 46.3  & \textbf{92.2} & \textbf{89.6} & \textbf{92.1} & \textbf{86.0} & 50\ s \\ 
		SARG\ (n\_beam=5) & 62.3 & \textbf{52.5} & \textbf{46.4} & 91.4 & 88.9 & 91.9 & 85.7 & 70\ s \\ \hline
	\end{tabular}
	\caption{Main results of our method and other SOTA methods. Inference time is evaluated on the same blind test set (5104 examples) with one Nvidia Tesla P40.}\label{mainrlt}
\end{table*}

\begin{table*}[!htb]
	\centering
	\begin{tabular}{|l|ccc|cccc|}
		\hline
		&  &  &   & BLEU-1 & BLEU-2 & ROUGE-1 & ROUGE-2\\ \hline
		SARG  & 62.4 & 52.5 & 46.3  & 92.2 & 89.6 & 92.1 & 86.0 \\ \hline
		\ w/o WEIGHT  & 52.8 & 41.1 & 33.8 & 89.2 & 86.7 & 89.9 & 83.6 \\ \hline
		\ w/o COPY  & 55.6 & 38.9 & 32.8 & 89.4 & 85.6 & 89.9 & 81.7 \\ \hline
		\ w/o GEN & 56.2 & 48.0 & 42.9 & 90.4 & 88.2 & 91.4 & 85.6 \\ \hline
	\end{tabular}
	\caption{Ablation study of proposed model on the test set. The beam size is fixed to 1.}\label{ablation}
\end{table*}

The main results on the automatic metric are shown in Table \ref{mainrlt}, and we also evaluate the time cost of inference in test set. Focusing on the automatic metrics, we can make the following observations:
\begin{itemize}
	\item SARG achieves the best results on 6 of 7 automatic metrics. And PAC is 1.2 higher than SARG on restoration  score but 3.0 and 6.1 lower on  and  separately. In fact,  pays more attention to those tokens restored from history than others from the original utterance. In other words, though PAC can recall appropriate restored tokens from history, it may not place these restored tokens in their right positions well. We also exemplify such problem in the case study.


	\item T-Ptr- performs far away from other methods, and the reason is two-fold: one is that T-Ptr- only copies words from history or original utterance, however, some target words can only be generated from the overall vocabulary; the other is that T-Ptr- does not enjoy the benefit of the pretrained transformers. By contrast,  Seq2Seq-Uni is promising and provides a convenient way for loading pretrained weights in Seq2Seq model.
	
	\item Beam-search brings pretty significant improvements on these complete autoregressive models, but less obvious improvements on our model. PAC gains 2.6 points, T-Ptr- gains 3.9 points, and Seq2Seq-Uni gains 1.6 points on restoration  score through the beam-search; by contrast, SARG gains only 0.1 points on the restoration  score. Moreover, in SARG, we find that beam-search can recall more restored tokens (including some undesired ones) and harms the BLEU and ROUGE to some extent.
\end{itemize} 

From the aspect of inference time \footnote{we do not consider the inference speed of PAC, because the cascade way takes lower efficiency than other end-to-end methods}, we can make the following observations:
\begin{itemize}
	\item Compared to those complete autoregressive methods, our semi autoregressive model takes less time for inference. SARG is near 10x times as fast as T-Ptr- and 6x times as fast as Seq2Seq-Uni.
	\item Seq2Seq-Uni is consistently faster than T-Ptr-. Though such two methods are both complete autoregressive, the T-Ptr- takes more multi-head attention calculations for decoding.
	\item Beam-search increases the burden on inference. It needs more time and more memory for maintaining the candidate beams. Generally, the incomplete utterance restoration is required to be time-efficient as the intermediate subtask of multi-turn dialogue task, and it is unpractical to maintain plenty of beams in decoding. Therefore, our SARG may be a suitable choice with the less dependent on the beam-search.
\end{itemize}

Through the above analysis, SARG has advantages in automatic evaluation and computation complexity. 

\subsection{Ablation Study}
\begin{CJK}{UTF8}{gbsn}
	\newcommand{\hanzi}{\fontsize{8.5pt}{\baselineskip}\selectfont}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table*}[!htb]
		\centering
		\scalebox{0.8}{
			\begin{tabular}{|c|ccc|}
				\hline
				& \small Example 1 & \small Example 2 & \small Example 3 \\ \hline
				\small & \tabincell{c}{\hanzi{男185} \\ \small Male, 185} & \tabincell{c}{\hanzi{天蝎座有喜欢的吗}\\ \small Does anyone like Scorpio} & \tabincell{c}{\hanzi{比尔吉沃特有互送皮肤的吗} \\ \small Who want to exchange skin in Bilgewater} \\
				\small & \tabincell{c}{\hanzi{老乡你帖子要沉了}\\ \small Bro, your post is totally ignored} & \tabincell{c}{\hanzi{天蝎座的小伙子你喜欢么} \\ \small Do you like Scorpio boy} & \tabincell{c}{\hanzi{直接买个蘑菇的不就行了} \\ \small Just buy the Timo's directly} \\ 
				\small & \tabincell{c}{\hanzi{这不还有你么哈哈} \\ \small You're still here haha} & \tabincell{c}{\hanzi{喜欢啊} \\ \small Yes} & \tabincell{c}{\hanzi{抽奖比较欢乐} \\ \small Lucky draw is interesting} \\ 
				\small & \tabincell{c}{\hanzi{你雇我给你吆喝啊} \\ \small You can hire me to cry out for you} & \tabincell{c}{\hanzi{可是我不信星座诶} \\ \small I don't believe in constellations} & \tabincell{c}{\hanzi{我一般都是从我同学的号挨个送我} \\ \small I usually send myself from classmate's account} \\ 
				\small & \tabincell{c}{\hanzi{哈哈好} \\ \small OK} & \tabincell{c}{\hanzi{这东西只是娱乐罢} \\ \small It is just entertainment} & \tabincell{c}{\hanzi{看来我只能拿朋友的号送自己了} \\ \small I'll have to send myself from friends' account} \\ \hline
				\small Reference& \tabincell{c}{\hanzi{哈哈\textcolor{red}{给我吆喝}好} \\ \small OK, \textcolor{red}{cry out for me}} & \tabincell{c}{\hanzi{\textcolor{red}{星座}这东西只是娱乐罢} \\ \small \textcolor{red}{Constellation} is just entertainment}  & \tabincell{c}{\hanzi{看来我只能拿朋友的号送\textcolor{red}{皮肤}自己了} \\ \small I'll have to send myself \textcolor{red}{skin} from friends' account} \\ \hline
				\small SARG& \tabincell{c}{\hanzi{哈哈好\textcolor{blue}{雇你给我吆喝}} \\ \small OK, \textcolor{blue}{hire you to cry out for me}} & \tabincell{c}{\hanzi{\textcolor{blue}{星座}这东西只是娱乐罢} \\ \small \textcolor{blue}{Constellation} is just entertainment} & \tabincell{c}{\hanzi{看来我只能拿朋友的号送自己\textcolor{blue}{皮肤}了} \\ \small I'll have to send myself \textcolor{blue}{skin} from friends' account} \\ 
				\small PAC & \tabincell{c}{\hanzi{哈哈\textcolor{blue}{吆喝}好} \\ \small OK, \textcolor{blue}{cry out}} & \tabincell{c}{\hanzi{这东西只是娱乐\textcolor{blue}{星座}罢} \\ \small It is just entertainment \textcolor{blue}{constellation}} & \tabincell{c}{\hanzi{看来我只能拿朋友的号送自己了} \\ \small I'll have to send myself from friends' account} \\ 
				\small T-Ptr-& \tabincell{c}{\hanzi{哈哈好\textcolor{blue}{吆喝}} \\ \small OK, \textcolor{blue}{cry out}} & \tabincell{c}{\hanzi{\textcolor{blue}{不信星座}这东西只是娱乐罢} \\ \small \textcolor{blue}{Not believing constellation} is just entertainment} & \tabincell{c}{\hanzi{看来我只能拿朋友的号送自己了} \\ \small I'll have to send myself from friends' account} \\ 
				\small Seq2Seq-Uni& \tabincell{c}{\hanzi{哈哈好} \\ \small OK} & \tabincell{c}{\hanzi{\textcolor{blue}{不信星座}这东西只是娱乐罢} \\ \small \textcolor{blue}{Not believing constellation} is just entertainment} & \tabincell{c}{\hanzi{看来我只能拿朋友的号送自己了} \\ \small I'll have to send myself from friends' account} \\ \hline
		\end{tabular}}
		\caption{Examples for incomplete utterance restoration.  to  is the history of conversation,  is the original utterance.} \label{caseexample}
	\end{table*}
\end{CJK}

In this subsection, we conduct a series of ablation studies to evaluate the effectiveness of different components in our proposed SARG, which includes pretrained weights (WEIGHT), copy mechanism (COPY), and generation from vocabulary (GEN), and the results are shown in Table \ref{ablation}. 

As can be seen, GEN plays the least important role in our model. By contrast, the absence of COPY or WEIGHT may raise a substantial lack of performance. Following our previous experimental setting, the above two variant models both can not converge well. In fact, the model without COPY only selects words from the pre-defined overall vocabulary, and the decoder is more difficult to be trained well. \textcolor{red}{Furthermore, without the WEIGHT, the model needs to update the overall weights from scratch, which incorporate the 768-dimensional embedding table and 12 transformer layers. Therefore, it is a considerable burden for the optimization, where the limited corpus is provided.} 

And we also compare the output of tagger among the above listed models. An observation is that the tagger without WEIGHT is conservative on predicting the \texttt{CHANGE} operations; by contrast, the decoder without WEIGHT is less affected and has normal-appearing. Therefore, in some cases, even though the decoder produces the right restored words, the model still can not output the correct answers because the tagger does not produce the corresponding \texttt{CHANGE} operations.


\subsection{Human Evaluation}


\begin{table}[!htb]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		& Quality  & Fluency \\ \hline
		SARG & \textbf{2.70} & 2.85 \\ \hline
		PAC & 2.67 & 2.83 \\ \hline
		T-Ptr-& 2.58 & 2.80 \\ \hline
		Seq2Seq-Uni& 2.65 & \textbf{2.87} \\ \hline
	\end{tabular}
	\caption{Human evaluation of the restoration quality and language fluency. Both quality and fluency score adopt a 3-point scale.}\label{human}
\end{table}

In the phase of human evaluation, we employ three experienced workers to score the restoration quality and sentence fluency separately on 200 randomly selected samples. \textcolor{red}{Specifically, each sample is scored by the three workers in turn, and the final quality or fluency scores are calculated by averaging the annotated results}.

As shown in Table \ref{human}, SARG obtains the highest score in restoration quality among the compared methods, which is consistent with the results of automatic evaluation. However, in the aspect of fluency score, Seq2Seq-Uni achieves the best performance. Seq2Seq-Uni takes a way of complete autoregression and benefits from the pretrained weights, which can complete the causal language modeling well. 
\textcolor{red}{Naturally, thinking about how to introduce the constraints from language modeling in SARG is the direction for the further improvement.}

\subsection{Case Study}\label{casestudy}

In this subsection, we observe the prediction results among different models, and then select several representative examples to illustrate the superiority of our proposed model as Table \ref{caseexample} shows.

As can be seen in Example 1, the first three models can restore the action ``\textit{cry out}", and only SARG can restore the predicate ``\textit{hire you}", which is important to understand the direction of the action. 

In Example 2, all four models restore the keyword ``\textit{constellation}" correctly. However, for T-Ptr- and Seq2Seq-Uni, undesired words ``\textit{not believing}" are also restored, which changes the intention of utterance. In PAC, we can find the keyword ``\textit{constellation}" is placed in a wrong position, which leads to the difficulty in understanding. Moreover, for the restoration scores, the wrong position problem has no effect on  but is negative for  and . That is a possible reason, compared with SARG, PAC has higher  but lower  and  in the automatic evaluation.

Finally Example 3 demonstrates the ability of SARG to restore utterance from distant context. Specifically, the keyword ``\textit{skin}" appears in , and the model is required to restore it after three utterances.

