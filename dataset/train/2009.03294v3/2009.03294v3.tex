
\documentclass{article} \usepackage{icml2021}
\usepackage{xcolor}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{bm}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{enumitem}
\usepackage{footnote}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\makesavenoteenv{tabular}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xspace}
\usepackage{amsthm}
\newcommand{\C}{\mathbb{C}} \newcommand{\F}{\mathbb{F}} \newcommand{\N}{\mathbb{N}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\R}{\mathbb{R}} \newcommand{\Z}{\mathbb{Z}} 

\newcommand{\tr}{\mathop{\mathrm{tr}}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\rank}{\mathop{\mathsf{rank}}}
\newcommand{\vect}{\mathop{\mathrm{vec}}}
\newcommand{\nnz}{\mathop{\mathrm{nnz}}}

\newcommand*{\mini}{\mathop{\mathrm{minimize}}}
\newcommand*{\maxi}{\mathop{\mathrm{maximize}}}
\newcommand*{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand*{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand*{\arginf}{\mathop{\mathrm{arginf}}}
\newcommand*{\argsup}{\mathop{\mathrm{argsup}}}
\newcommand{\st}{\text{subject to: }}
\newcommand{\dom}{\mathop{\mathrm{dom}}}
\newcommand{\grad}{{\nabla}}
\newcommand{\hess}{\grad^{2}}

\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\tv}{\mathsf{TV}}
\newcommand{\EE}{\mathsf{E}} \newcommand{\HH}{\mathsf{H}} \newcommand{\id}{\mathsf{Id}} \newcommand{\KK}{\mathds{K}} \newcommand{\MM}{\mathsf{M}} 

\newcommand{\RRbar}{\overline{\RR}} \newcommand{\RRi}{\RR_{\infty}} \newcommand{\pp}{\mathsf{p}} \newcommand{\II}{\mathsf{I}} 




\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\newcommand{\traj}{\mathop{\mathrm{Traj}}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\cone}{\mathrm{cone}}
\newcommand{\const}{\mathrm{constant}}
\newcommand{\ri}{\mathop{\mathrm{ri}}}
\newcommand{\sri}{\mathop{\mathrm{sri}}}
\newcommand{\cl}{\mathop{\mathrm{cl}}}
\newcommand{\intr}{\mathop{\mathrm{int}}}
\newcommand{\bd}{\mathop{\mathrm{bd}}}
\newcommand{\emp}{\mathop{\mathrm{emp}}}
\newcommand{\core}{\mathop{\mathrm{core}}}
\newcommand{\epi}{\mathop{\mathrm{epi}}}
\newcommand{\co}{\mathop{\mathrm{co}}}
\newcommand{\med}{\mathop{\mathrm{med}}}
\newcommand{\eproof}{}






\newcommand{\atil}{\tilde{a}}
\newcommand{\btil}{\tilde{b}}
\newcommand{\ctil}{\tilde{c}}
\newcommand{\dtil}{\tilde{d}}
\newcommand{\etil}{\tilde{e}}
\newcommand{\ftil}{\tilde{f}}
\newcommand{\gtil}{\tilde{g}}
\newcommand{\htil}{\tilde{h}}
\newcommand{\itil}{\tilde{i}}
\newcommand{\jtil}{\tilde{j}}
\newcommand{\ktil}{\tilde{k}}
\newcommand{\ltil}{\tilde{l}}
\newcommand{\mtil}{\tilde{m}}
\newcommand{\ntil}{\tilde{n}}
\newcommand{\otil}{\tilde{o}}
\newcommand{\ptil}{\tilde{p}}
\newcommand{\qtil}{\tilde{q}}
\newcommand{\rtil}{\tilde{r}}
\newcommand{\stil}{\tilde{s}}
\newcommand{\ttil}{\tilde{t}}
\newcommand{\util}{\tilde{u}}
\newcommand{\vtil}{\tilde{v}}
\newcommand{\wtil}{\tilde{w}}
\newcommand{\xtil}{\tilde{x}}
\newcommand{\ytil}{\tilde{y}}
\newcommand{\ztil}{\tilde{z}}

\newcommand{\ahat}{\hat{a}}
\newcommand{\bhat}{\hat{b}}
\newcommand{\chat}{\hat{c}}
\newcommand{\dhat}{\hat{d}}
\newcommand{\ehat}{\hat{e}}
\newcommand{\fhat}{\hat{f}}
\newcommand{\ghat}{\hat{g}}
\newcommand{\hhat}{\hat{h}}
\newcommand{\ihat}{\hat{i}}
\newcommand{\jhat}{\hat{j}}
\newcommand{\khat}{\hat{k}}
\newcommand{\lhat}{\hat{l}}
\newcommand{\mhat}{\hat{m}}
\newcommand{\nhat}{\hat{n}}
\newcommand{\ohat}{\hat{o}}
\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\shat}{\hat{s}}
\newcommand{\that}{\hat{t}}
\newcommand{\uhat}{\hat{u}}
\newcommand{\vhat}{\hat{v}}
\newcommand{\what}{\hat{w}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\zhat}{\hat{z}}

\newcommand{\abar}{\bar{a}}
\newcommand{\bbar}{\bar{b}}
\newcommand{\cbar}{\bar{c}}
\newcommand{\dbar}{\bar{d}}
\newcommand{\ebar}{\bar{e}}
\newcommand{\fbar}{\bar{f}}
\newcommand{\gbar}{\bar{g}}
\newcommand{\hbr}{\bar{h}}
\newcommand{\ibar}{\bar{i}}
\newcommand{\jbar}{\bar{j}}
\newcommand{\kbar}{\bar{k}}
\newcommand{\lbar}{\bar{l}}
\newcommand{\mbar}{\bar{m}}
\newcommand{\nbar}{\bar{n}}
\newcommand{\obar}{\bar{o}}
\newcommand{\pbar}{\bar{p}}
\newcommand{\qbar}{\bar{q}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\sbar}{\bar{s}}
\newcommand{\tbar}{\bar{t}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\vbar}{\bar{v}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\abf}{\mathbf{a}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\cbf}{\mathbf{c}}
\newcommand{\dbf}{\mathbf{d}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\fbf}{\mathbf{f}}
\newcommand{\gbf}{\mathbf{g}}
\newcommand{\hbf}{\mathbf{h}}
\newcommand{\ibf}{\mathbf{i}}
\newcommand{\jbf}{\mathbf{j}}
\newcommand{\kbf}{\mathbf{k}}
\newcommand{\lbf}{\mathbf{l}}
\newcommand{\mbf}{\mathbf{m}}
\newcommand{\nbf}{\mathbf{n}}
\newcommand{\obf}{\mathbf{o}}
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\qbf}{\mathbf{q}}
\newcommand{\rbf}{\mathbf{r}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\tbf}{\mathbf{t}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\vbf}{\mathbf{v}}
\newcommand{\wbf}{\mathbf{w}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}

\newcommand{\abftil}{\tilde{\abf}}
\newcommand{\bbftil}{\tilde{\bbf}}
\newcommand{\cbftil}{\tilde{\cbf}}
\newcommand{\dbftil}{\tilde{\dbf}}
\newcommand{\ebftil}{\tilde{\ebf}}
\newcommand{\fbftil}{\tilde{\fbf}}
\newcommand{\gbftil}{\tilde{\gbf}}
\newcommand{\hbftil}{\tilde{\hbf}}
\newcommand{\ibftil}{\tilde{\ibf}}
\newcommand{\jbftil}{\tilde{\jbf}}
\newcommand{\kbftil}{\tilde{\kbf}}
\newcommand{\lbftil}{\tilde{\lbf}}
\newcommand{\mbftil}{\tilde{\mbf}}
\newcommand{\nbftil}{\tilde{\nbf}}
\newcommand{\obftil}{\tilde{\obf}}
\newcommand{\pbftil}{\tilde{\pbf}}
\newcommand{\qbftil}{\tilde{\qbf}}
\newcommand{\rbftil}{\tilde{\rbf}}
\newcommand{\sbftil}{\tilde{\sbf}}
\newcommand{\tbftil}{\tilde{\tbf}}
\newcommand{\ubftil}{\tilde{\ubf}}
\newcommand{\vbftil}{\tilde{\vbf}}
\newcommand{\wbftil}{\tilde{\wbf}}
\newcommand{\xbftil}{\tilde{\xbf}}
\newcommand{\ybftil}{\tilde{\ybf}}
\newcommand{\zbftil}{\tilde{\zbf}}

\newcommand{\abfhat}{\hat{\abf}}
\newcommand{\bbfhat}{\hat{\bbf}}
\newcommand{\cbfhat}{\hat{\cbf}}
\newcommand{\dbfhat}{\hat{\dbf}}
\newcommand{\ebfhat}{\hat{\ebf}}
\newcommand{\fbfhat}{\hat{\fbf}}
\newcommand{\gbfhat}{\hat{\gbf}}
\newcommand{\hbfhat}{\hat{\hbf}}
\newcommand{\ibfhat}{\hat{\ibf}}
\newcommand{\jbfhat}{\hat{\jbf}}
\newcommand{\kbfhat}{\hat{\kbf}}
\newcommand{\lbfhat}{\hat{\lbf}}
\newcommand{\mbfhat}{\hat{\mbf}}
\newcommand{\nbfhat}{\hat{\nbf}}
\newcommand{\obfhat}{\hat{\obf}}
\newcommand{\pbfhat}{\hat{\pbf}}
\newcommand{\qbfhat}{\hat{\qbf}}
\newcommand{\rbfhat}{\hat{\rbf}}
\newcommand{\sbfhat}{\hat{\sbf}}
\newcommand{\tbfhat}{\hat{\tbf}}
\newcommand{\ubfhat}{\hat{\ubf}}
\newcommand{\vbfhat}{\hat{\vbf}}
\newcommand{\wbfhat}{\hat{\wbf}}
\newcommand{\xbfhat}{\hat{\xbf}}
\newcommand{\ybfhat}{\hat{\ybf}}
\newcommand{\zbfhat}{\hat{\zbf}}

\newcommand{\abfbar}{\bar{\abf}}
\newcommand{\bbfbar}{\bar{\bbf}}
\newcommand{\cbfbar}{\bar{\cbf}}
\newcommand{\dbfbar}{\bar{\dbf}}
\newcommand{\ebfbar}{\bar{\ebf}}
\newcommand{\fbfbar}{\bar{\fbf}}
\newcommand{\gbfbar}{\bar{\gbf}}
\newcommand{\hbfbar}{\bar{\hbf}}
\newcommand{\ibfbar}{\bar{\ibf}}
\newcommand{\jbfbar}{\bar{\jbf}}
\newcommand{\kbfbar}{\bar{\kbf}}
\newcommand{\lbfbar}{\bar{\lbf}}
\newcommand{\mbfbar}{\bar{\mbf}}
\newcommand{\nbfbar}{\bar{\nbf}}
\newcommand{\obfbar}{\bar{\obf}}
\newcommand{\pbfbar}{\bar{\pbf}}
\newcommand{\qbfbar}{\bar{\qbf}}
\newcommand{\rbfbar}{\bar{\rbf}}
\newcommand{\sbfbar}{\bar{\sbf}}
\newcommand{\tbfbar}{\bar{\tbf}}
\newcommand{\ubfbar}{\bar{\ubf}}
\newcommand{\vbfbar}{\bar{\vbf}}
\newcommand{\wbfbar}{\bar{\wbf}}
\newcommand{\xbfbar}{\bar{\xbf}}
\newcommand{\ybfbar}{\bar{\ybf}}
\newcommand{\zbfbar}{\bar{\zbf}}

\newcommand{\asf}{\mathsf{a}}
\newcommand{\bsf}{\mathsf{b}}
\newcommand{\csf}{\mathsf{c}}
\newcommand{\dsf}{\mathsf{d}}
\newcommand{\esf}{\mathsf{e}}
\newcommand{\fsf}{\mathsf{f}}
\newcommand{\gsf}{\mathsf{g}}
\newcommand{\hsf}{\mathsf{h}}
\newcommand{\isf}{\mathsf{i}}
\newcommand{\jsf}{\mathsf{j}}
\newcommand{\ksf}{\mathsf{k}}
\newcommand{\lsf}{\mathsf{l}}
\newcommand{\msf}{\mathsf{m}}
\newcommand{\nsf}{\mathsf{n}}
\newcommand{\osf}{\mathsf{o}}
\newcommand{\psf}{\mathsf{p}}
\newcommand{\qsf}{\mathsf{q}}
\newcommand{\rsf}{\mathsf{r}}
\newcommand{\ssf}{\mathsf{s}}
\newcommand{\tsf}{\mathsf{t}}
\newcommand{\usf}{\mathsf{u}}
\newcommand{\vsf}{\mathsf{v}}
\newcommand{\wsf}{\mathsf{w}}
\newcommand{\xsf}{\mathsf{x}}
\newcommand{\ysf}{\mathsf{y}}
\newcommand{\zsf}{\mathsf{z}}

\newcommand{\att}{\mathtt{a}}
\newcommand{\btt}{\mathtt{b}}
\newcommand{\ctt}{\mathtt{c}}
\newcommand{\dtt}{\mathtt{d}}
\newcommand{\ett}{\mathtt{e}}
\newcommand{\ftt}{\mathtt{f}}
\newcommand{\gtt}{\mathtt{g}}
\newcommand{\htt}{\mathtt{h}}
\newcommand{\itt}{\mathtt{i}}
\newcommand{\jtt}{\mathtt{j}}
\newcommand{\ktt}{\mathtt{k}}
\newcommand{\ltt}{\mathtt{l}}
\newcommand{\mtt}{\mathtt{m}}
\newcommand{\ntt}{\mathtt{n}}
\newcommand{\ott}{\mathtt{o}}
\newcommand{\ptt}{\mathtt{p}}
\newcommand{\qtt}{\mathtt{q}}
\newcommand{\rtt}{\mathtt{r}}
\newcommand{\stt}{\mathtt{s}}
\newcommand{\ttt}{\mathtt{t}}
\newcommand{\utt}{\mathtt{u}}
\newcommand{\vtt}{\mathtt{v}}
\newcommand{\wtt}{\mathtt{w}}
\newcommand{\xtt}{\mathtt{x}}
\newcommand{\ytt}{\mathtt{y}}
\newcommand{\ztt}{\mathtt{z}}

\newcommand{\abld}{\boldsymbol{a}}
\newcommand{\bbld}{\boldsymbol{b}}
\newcommand{\cbld}{\boldsymbol{c}}
\newcommand{\dbld}{\boldsymbol{d}}
\newcommand{\ebld}{\boldsymbol{e}}
\newcommand{\fbld}{\boldsymbol{f}}
\newcommand{\gbld}{\boldsymbol{g}}
\newcommand{\hbld}{\boldsymbol{h}}
\newcommand{\ibld}{\boldsymbol{i}}
\newcommand{\jbld}{\boldsymbol{j}}
\newcommand{\kbld}{\boldsymbol{k}}
\newcommand{\lbld}{\boldsymbol{l}}
\newcommand{\mbld}{\boldsymbol{m}}
\newcommand{\nbld}{\boldsymbol{n}}
\newcommand{\obld}{\boldsymbol{o}}
\newcommand{\pbld}{\boldsymbol{p}}
\newcommand{\qbld}{\boldsymbol{q}}
\newcommand{\rbld}{\boldsymbol{r}}
\newcommand{\sbld}{\boldsymbol{s}}
\newcommand{\tbld}{\boldsymbol{t}}
\newcommand{\ubld}{\boldsymbol{u}}
\newcommand{\vbld}{\boldsymbol{v}}
\newcommand{\wbld}{\boldsymbol{w}}
\newcommand{\xbld}{\boldsymbol{x}}
\newcommand{\ybld}{\boldsymbol{y}}
\newcommand{\zbld}{\boldsymbol{z}}

\newcommand{\Atil}{\tilde{A}}
\newcommand{\Btil}{\tilde{B}}
\newcommand{\Ctil}{\tilde{C}}
\newcommand{\Dtil}{\tilde{D}}
\newcommand{\Etil}{\tilde{E}}
\newcommand{\Ftil}{\tilde{F}}
\newcommand{\Gtil}{\tilde{G}}
\newcommand{\Htil}{\tilde{H}}
\newcommand{\Itil}{\tilde{I}}
\newcommand{\Jtil}{\tilde{J}}
\newcommand{\Ktil}{\tilde{K}}
\newcommand{\Ltil}{\tilde{L}}
\newcommand{\Mtil}{\tilde{M}}
\newcommand{\Ntil}{\tilde{N}}
\newcommand{\Otil}{\tilde{O}}
\newcommand{\Ptil}{\tilde{P}}
\newcommand{\Qtil}{\tilde{Q}}
\newcommand{\Rtil}{\tilde{R}}
\newcommand{\Stil}{\tilde{S}}
\newcommand{\Ttil}{\tilde{T}}
\newcommand{\Util}{\tilde{U}}
\newcommand{\Vtil}{\tilde{V}}
\newcommand{\Wtil}{\tilde{W}}
\newcommand{\Xtil}{\tilde{X}}
\newcommand{\Ytil}{\tilde{Y}}
\newcommand{\Ztil}{\tilde{Z}}

\newcommand{\Ahat}{\hat{A}}
\newcommand{\Bhat}{\hat{B}}
\newcommand{\Chat}{\hat{C}}
\newcommand{\Dhat}{\hat{D}}
\newcommand{\Ehat}{\hat{E}}
\newcommand{\Fhat}{\hat{F}}
\newcommand{\Ghat}{\hat{G}}
\newcommand{\Hhat}{\hat{H}}
\newcommand{\Ihat}{\hat{I}}
\newcommand{\Jhat}{\hat{J}}
\newcommand{\Khat}{\hat{K}}
\newcommand{\Lhat}{\hat{L}}
\newcommand{\Mhat}{\hat{M}}
\newcommand{\Nhat}{\hat{N}}
\newcommand{\Ohat}{\hat{O}}
\newcommand{\Phat}{\hat{P}}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Rhat}{\hat{R}}
\newcommand{\Shat}{\hat{S}}
\newcommand{\That}{\hat{T}}
\newcommand{\Uhat}{\hat{U}}
\newcommand{\Vhat}{\hat{V}}
\newcommand{\What}{\hat{W}}
\newcommand{\Xhat}{\hat{X}}
\newcommand{\Yhat}{\hat{Y}}
\newcommand{\Zhat}{\hat{Z}}

\newcommand{\Abar}{\bar{A}}
\newcommand{\Bbar}{\bar{B}}
\newcommand{\Cbar}{\bar{C}}
\newcommand{\Dbar}{\bar{D}}
\newcommand{\Ebar}{\bar{E}}
\newcommand{\Fbar}{\bar{F}}
\newcommand{\Gbar}{\bar{G}}
\newcommand{\Hbar}{\bar{H}}
\newcommand{\Ibar}{\bar{I}}
\newcommand{\Jbar}{\bar{J}}
\newcommand{\Kbar}{\bar{K}}
\newcommand{\Lbar}{\bar{L}}
\newcommand{\Mbar}{\bar{M}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\Obar}{\bar{O}}
\newcommand{\Pbar}{\bar{P}}
\newcommand{\Qbar}{\bar{Q}}
\newcommand{\Rbar}{\bar{R}}
\newcommand{\Sbar}{\bar{S}}
\newcommand{\Tbar}{\bar{T}}
\newcommand{\Ubar}{\bar{U}}
\newcommand{\Vbar}{\bar{V}}
\newcommand{\Wbar}{\bar{W}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}

\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Cbf}{\mathbf{C}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Ebf}{\mathbf{E}}
\newcommand{\Fbf}{\mathbf{F}}
\newcommand{\Gbf}{\mathbf{G}}
\newcommand{\Hbf}{\mathbf{H}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Jbf}{\mathbf{J}}
\newcommand{\Kbf}{\mathbf{K}}
\newcommand{\Lbf}{\mathbf{L}}
\newcommand{\Mbf}{\mathbf{M}}
\newcommand{\Nbf}{\mathbf{N}}
\newcommand{\Obf}{\mathbf{O}}
\newcommand{\Pbf}{\mathbf{P}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Rbf}{\mathbf{R}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Tbf}{\mathbf{T}}
\newcommand{\Ubf}{\mathbf{U}}
\newcommand{\Vbf}{\mathbf{V}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Ybf}{\mathbf{Y}}
\newcommand{\Zbf}{\mathbf{Z}}

\newcommand{\Abftil}{\tilde{\Abf}}
\newcommand{\Bbftil}{\tilde{\Bbf}}
\newcommand{\Cbftil}{\tilde{\Cbf}}
\newcommand{\Dbftil}{\tilde{\Dbf}}
\newcommand{\Ebftil}{\tilde{\Ebf}}
\newcommand{\Fbftil}{\tilde{\Fbf}}
\newcommand{\Gbftil}{\tilde{\Gbf}}
\newcommand{\Hbftil}{\tilde{\Hbf}}
\newcommand{\Ibftil}{\tilde{\Ibf}}
\newcommand{\Jbftil}{\tilde{\Jbf}}
\newcommand{\Kbftil}{\tilde{\Kbf}}
\newcommand{\Lbftil}{\tilde{\Lbf}}
\newcommand{\Mbftil}{\tilde{\Mbf}}
\newcommand{\Nbftil}{\tilde{\Nbf}}
\newcommand{\Obftil}{\tilde{\Obf}}
\newcommand{\Pbftil}{\tilde{\Pbf}}
\newcommand{\Qbftil}{\tilde{\Qbf}}
\newcommand{\Rbftil}{\tilde{\Rbf}}
\newcommand{\Sbftil}{\tilde{\Sbf}}
\newcommand{\Tbftil}{\tilde{\Tbf}}
\newcommand{\Ubftil}{\tilde{\Ubf}}
\newcommand{\Vbftil}{\tilde{\Vbf}}
\newcommand{\Wbftil}{\tilde{\Wbf}}
\newcommand{\Xbftil}{\tilde{\Xbf}}
\newcommand{\Ybftil}{\tilde{\Ybf}}
\newcommand{\Zbftil}{\tilde{\Zbf}}

\newcommand{\Abfhat}{\hat{\Abf}}
\newcommand{\Bbfhat}{\hat{\Bbf}}
\newcommand{\Cbfhat}{\hat{\Cbf}}
\newcommand{\Dbfhat}{\hat{\Dbf}}
\newcommand{\Ebfhat}{\hat{\Ebf}}
\newcommand{\Fbfhat}{\hat{\Fbf}}
\newcommand{\Gbfhat}{\hat{\Gbf}}
\newcommand{\Hbfhat}{\hat{\Hbf}}
\newcommand{\Ibfhat}{\hat{\Ibf}}
\newcommand{\Jbfhat}{\hat{\Jbf}}
\newcommand{\Kbfhat}{\hat{\Kbf}}
\newcommand{\Lbfhat}{\hat{\Lbf}}
\newcommand{\Mbfhat}{\hat{\Mbf}}
\newcommand{\Nbfhat}{\hat{\Nbf}}
\newcommand{\Obfhat}{\hat{\Obf}}
\newcommand{\Pbfhat}{\hat{\Pbf}}
\newcommand{\Qbfhat}{\hat{\Qbf}}
\newcommand{\Rbfhat}{\hat{\Rbf}}
\newcommand{\Sbfhat}{\hat{\Sbf}}
\newcommand{\Tbfhat}{\hat{\Tbf}}
\newcommand{\Ubfhat}{\hat{\Ubf}}
\newcommand{\Vbfhat}{\hat{\Vbf}}
\newcommand{\Wbfhat}{\hat{\Wbf}}
\newcommand{\Xbfhat}{\hat{\Xbf}}
\newcommand{\Ybfhat}{\hat{\Ybf}}
\newcommand{\Zbfhat}{\hat{\Zbf}}

\newcommand{\Abfbar}{\bar{\Abf}}
\newcommand{\Bbfbar}{\bar{\Bbf}}
\newcommand{\Cbfbar}{\bar{\Cbf}}
\newcommand{\Dbfbar}{\bar{\Dbf}}
\newcommand{\Ebfbar}{\bar{\Ebf}}
\newcommand{\Fbfbar}{\bar{\Fbf}}
\newcommand{\Gbfbar}{\bar{\Gbf}}
\newcommand{\Hbfbar}{\bar{\Hbf}}
\newcommand{\Ibfbar}{\bar{\Ibf}}
\newcommand{\Jbfbar}{\bar{\Jbf}}
\newcommand{\Kbfbar}{\bar{\Kbf}}
\newcommand{\Lbfbar}{\bar{\Lbf}}
\newcommand{\Mbfbar}{\bar{\Mbf}}
\newcommand{\Nbfbar}{\bar{\Nbf}}
\newcommand{\Obfbar}{\bar{\Obf}}
\newcommand{\Pbfbar}{\bar{\Pbf}}
\newcommand{\Qbfbar}{\bar{\Qbf}}
\newcommand{\Rbfbar}{\bar{\Rbf}}
\newcommand{\Sbfbar}{\bar{\Sbf}}
\newcommand{\Tbfbar}{\bar{\Tbf}}
\newcommand{\Ubfbar}{\bar{\Ubf}}
\newcommand{\Vbfbar}{\bar{\Vbf}}
\newcommand{\Wbfbar}{\bar{\Wbf}}
\newcommand{\Xbfbar}{\bar{\Xbf}}
\newcommand{\Ybfbar}{\bar{\Ybf}}
\newcommand{\Zbfbar}{\bar{\Zbf}}

\newcommand{\Asf}{\mathsf{A}}
\newcommand{\Bsf}{\mathsf{B}}
\newcommand{\Csf}{\mathsf{C}}
\newcommand{\Dsf}{\mathsf{D}}
\newcommand{\Esf}{\mathsf{E}}
\newcommand{\Fsf}{\mathsf{F}}
\newcommand{\Gsf}{\mathsf{G}}
\newcommand{\Hsf}{\mathsf{H}}
\newcommand{\Isf}{\mathsf{I}}
\newcommand{\Jsf}{\mathsf{J}}
\newcommand{\Ksf}{\mathsf{K}}
\newcommand{\Lsf}{\mathsf{L}}
\newcommand{\Msf}{\mathsf{M}}
\newcommand{\Nsf}{\mathsf{N}}
\newcommand{\Osf}{\mathsf{O}}
\newcommand{\Psf}{\mathsf{P}}
\newcommand{\Qsf}{\mathsf{Q}}
\newcommand{\Rsf}{\mathsf{R}}
\newcommand{\Ssf}{\mathsf{S}}
\newcommand{\Tsf}{\mathsf{T}}
\newcommand{\Usf}{\mathsf{U}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\Wsf}{\mathsf{W}}
\newcommand{\Xsf}{\mathsf{X}}
\newcommand{\Ysf}{\mathsf{Y}}
\newcommand{\Zsf}{\mathsf{Z}}

\newcommand{\Att}{\mathtt{A}}
\newcommand{\Btt}{\mathtt{B}}
\newcommand{\Ctt}{\mathtt{C}}
\newcommand{\Dtt}{\mathtt{D}}
\newcommand{\Ett}{\mathtt{E}}
\newcommand{\Ftt}{\mathtt{F}}
\newcommand{\Gtt}{\mathtt{G}}
\newcommand{\Htt}{\mathtt{H}}
\newcommand{\Itt}{\mathtt{I}}
\newcommand{\Jtt}{\mathtt{J}}
\newcommand{\Ktt}{\mathtt{K}}
\newcommand{\Ltt}{\mathtt{L}}
\newcommand{\Mtt}{\mathtt{M}}
\newcommand{\Ntt}{\mathtt{N}}
\newcommand{\Ott}{\mathtt{O}}
\newcommand{\Ptt}{\mathtt{P}}
\newcommand{\Qtt}{\mathtt{Q}}
\newcommand{\Rtt}{\mathtt{R}}
\newcommand{\Stt}{\mathtt{S}}
\newcommand{\Ttt}{\mathtt{T}}
\newcommand{\Utt}{\mathtt{U}}
\newcommand{\Vtt}{\mathtt{V}}
\newcommand{\Wtt}{\mathtt{W}}
\newcommand{\Xtt}{\mathtt{X}}
\newcommand{\Ytt}{\mathtt{Y}}
\newcommand{\Ztt}{\mathtt{Z}}

\newcommand{\Abld}{\boldsymbol{A}}
\newcommand{\Bbld}{\boldsymbol{B}}
\newcommand{\Cbld}{\boldsymbol{C}}
\newcommand{\Dbld}{\boldsymbol{D}}
\newcommand{\Ebld}{\boldsymbol{E}}
\newcommand{\Fbld}{\boldsymbol{F}}
\newcommand{\Gbld}{\boldsymbol{G}}
\newcommand{\Hbld}{\boldsymbol{H}}
\newcommand{\Ibld}{\boldsymbol{I}}
\newcommand{\Jbld}{\boldsymbol{J}}
\newcommand{\Kbld}{\boldsymbol{K}}
\newcommand{\Lbld}{\boldsymbol{L}}
\newcommand{\Mbld}{\boldsymbol{M}}
\newcommand{\Nbld}{\boldsymbol{N}}
\newcommand{\Obld}{\boldsymbol{O}}
\newcommand{\Pbld}{\boldsymbol{P}}
\newcommand{\Qbld}{\boldsymbol{Q}}
\newcommand{\Rbld}{\boldsymbol{R}}
\newcommand{\Sbld}{\boldsymbol{S}}
\newcommand{\Tbld}{\boldsymbol{T}}
\newcommand{\Ubld}{\boldsymbol{U}}
\newcommand{\Vbld}{\boldsymbol{V}}
\newcommand{\Wbld}{\boldsymbol{W}}
\newcommand{\Xbld}{\boldsymbol{X}}
\newcommand{\Ybld}{\boldsymbol{Y}}
\newcommand{\Zbld}{\boldsymbol{Z}}

\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}

\newcommand{\Abb}{\mathbb{A}}
\renewcommand{\Bbb}{\mathbb{B}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Dbb}{\mathbb{D}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Fbb}{\mathbb{F}}
\newcommand{\Gbb}{\mathbb{G}}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Jbb}{\mathbb{J}}
\newcommand{\Kbb}{\mathbb{K}}
\newcommand{\Lbb}{\mathbb{L}}
\newcommand{\Mbb}{\mathbb{M}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Obb}{\mathbb{O}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Sbb}{\mathbb{S}}
\newcommand{\Tbb}{\mathbb{T}}
\newcommand{\Ubb}{\mathbb{U}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Wbb}{\mathbb{W}}
\newcommand{\Xbb}{\mathbb{X}}
\newcommand{\Ybb}{\mathbb{Y}}
\newcommand{\Zbb}{\mathbb{Z}}

\renewcommand{\vec}[1]{\mathbf{\boldsymbol{#1}}}

\newcommand{\avec}{\vec{a}}
\newcommand{\bvec}{\vec{b}}
\newcommand{\cvec}{\vec{c}}
\newcommand{\dvec}{\vec{d}}
\newcommand{\evec}{\vec{e}}
\newcommand{\fvec}{\vec{f}}
\newcommand{\gvec}{\vec{g}}
\newcommand{\hvec}{\vec{h}}
\newcommand{\ivec}{\vec{i}}
\newcommand{\jvec}{\vec{j}}
\newcommand{\kvec}{\vec{k}}
\newcommand{\lvec}{\vec{l}}
\newcommand{\mvec}{\vec{m}}
\newcommand{\nvec}{\vec{n}}
\newcommand{\ovec}{\vec{o}}
\newcommand{\pvec}{\vec{p}}
\newcommand{\qvec}{\vec{q}}
\newcommand{\rvec}{\vec{r}}
\newcommand{\svec}{\vec{s}}
\newcommand{\tvec}{\vec{t}}
\newcommand{\uvec}{\vec{u}}
\newcommand{\vvec}{\vec{v}}
\newcommand{\wvec}{\vec{w}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\yvec}{\vec{y}}
\newcommand{\zvec}{\vec{z}}

\newcommand{\Avec}{\vec{A}}
\newcommand{\Bvec}{\vec{B}}
\newcommand{\Cvec}{\vec{C}}
\newcommand{\Dvec}{\vec{D}}
\newcommand{\Evec}{\vec{E}}
\newcommand{\Fvec}{\vec{F}}
\newcommand{\Gvec}{\vec{G}}
\newcommand{\Hvec}{\vec{H}}
\newcommand{\Ivec}{\vec{I}}
\newcommand{\Jvec}{\vec{J}}
\newcommand{\Kvec}{\vec{K}}
\newcommand{\Lvec}{\vec{L}}
\newcommand{\Mvec}{\vec{M}}
\newcommand{\Nvec}{\vec{N}}
\newcommand{\Ovec}{\vec{O}}
\newcommand{\Pvec}{\vec{P}}
\newcommand{\Qvec}{\vec{Q}}
\newcommand{\Rvec}{\vec{R}}
\newcommand{\Svec}{\vec{S}}
\newcommand{\Tvec}{\vec{T}}
\newcommand{\Uvec}{\vec{U}}
\newcommand{\Vvec}{\vec{V}}
\newcommand{\Wvec}{\vec{W}}
\newcommand{\Xvec}{\vec{X}}
\newcommand{\Yvec}{\vec{Y}}
\newcommand{\Zvec}{\vec{Z}}

\newcommand{\yvecbar}{\bar{\vec{y}}}
\newcommand{\wvecbar}{\bar{\vec{w}}}
\newcommand{\xvecbar}{\bar{\vec{x}}}
\newcommand{\yvectil}{\tilde{\vec{y}}}
\newcommand{\yvechat}{\hat{\vec{y}}}






\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi

\ifx\QED\undefined
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\fi

\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\em Proof:\ }}{\hfill\BlackBox\
    \label{eq:combine}
    h_i^{(k)}   &= \text{AGGREGATE}^{(k)} \big( h_i^{(k-1)}, \big\lbrace h_j^{(k-1)}  : v_j \in \mathcal{N}(v_i) \big\rbrace \big), 
\label{equ:gcn-agg-matrix}
    H^{(k)} = \relu\rbr{W^{(k)} H^{(k-1)} Q_{\text{GCN}}},

    \label{eq:gin-agg-matrix}
    H^{(k)} = {\rm MLP}^{(k)}\rbr{W^{(k)} H^{(k-1)}Q_{\mathrm{GIN}}},

    \label{eq:structure_with_norm}
    H^{(k)} = F^{(k)}\rbr{\mathrm{Norm}\rbr{W^{(k)}H^{(k-1)}Q}},
\label{eq:general_form_norm}
    \mathrm{Norm}\rbr{\hhat_{i,j,g}} = \gamma \cdot \frac{\hhat_{i,j,g} - \mu}{\sigma} + \beta,
 \label{eq:structure_with_norm_exp}
    \mathrm{Norm}\rbr{W^{(k)}H^{(k-1)}Q} = S\rbr{W^{(k)}H^{(k-1)}Q}N,
 \label{eq:mean_sub_eigen}
        \lambda_1\le\mu_1\le\lambda_2\le\cdots\le\lambda_{n-1}\le\mu_{n-1}\le\lambda_n,
    
    \norm{\wbf_t^{\mathrm{Shift}} - \wbf_*^{\mathrm{Shift}}}_2 = O\rbr{\rho_1^t},

    \norm{\wbf_t^{\mathrm{Vanilla}} - \wbf_*^{\mathrm{Vanilla}}}_2 = O\rbr{\rho_2^t}\ \mathrm{and}\ \rho_1 < \rho_2,

    \label{eq:graph_norm_formula}
    \mathrm{GraphNorm}\rbr{\hhat_{i,j}} = \gamma_j \cdot \frac{\hhat_{i,j} - \alpha_j\cdot\mu_j}{\hat{\sigma}_j} + \beta_j,

        \lambda_1\le\mu_1\le\lambda_2\le\cdots\le\lambda_{n-1}\le\mu_{n-1}\le\lambda_n,
    
        U = \begin{pmatrix}
            U_1 & \frac{1}{\sqrt{n}}\one
        \end{pmatrix},
    
        D &= \diag\rbr{1,\cdots,1,0}=
        \begin{pmatrix}
            I_{n-1} & \zero\\
            \zero^\top & 0
        \end{pmatrix},\\
        B &=
        \begin{pmatrix}
            I_{n-1}\\
            \zero^\top
        \end{pmatrix},\\
        \Cbar &= {Q}^\top{Q},
    
        N{Q}^\top{Q} N&\sim DU^\top\Cbar UD\\
        &= 
        D
        \begin{pmatrix}
            U_1^\top\\
            \frac{1}{\sqrt{n}}\one^\top
        \end{pmatrix}
        \Cbar
        \begin{pmatrix}
            U_1 & \frac{1}{\sqrt{n}}\one
        \end{pmatrix}
        D\\
        &=D
        \begin{pmatrix}
            U_1^\top\Cbar U_1 & \frac{1}{\sqrt{n}}U_1^\top\Cbar\one\\
            \frac{1}{\sqrt{n}}\one^\top\Cbar U_1 & \frac{1}{n}\one^\top\Cbar\one
        \end{pmatrix}
        D\\
        &=\begin{pmatrix}
            B^\top\\
            \begin{matrix}
                \zero^\top& 0
            \end{matrix}
        \end{pmatrix}
        \begin{pmatrix}
            U_1^\top\Cbar U_1 & \frac{1}{\sqrt{n}}U_1^\top\Cbar\one\\
            \frac{1}{\sqrt{n}}\one^\top\Cbar U_1 & \frac{1}{n}\one^\top\Cbar\one
        \end{pmatrix}
        \begin{pmatrix}
            B &\begin{matrix}
            \zero\\
            0
            \end{matrix}
        \end{pmatrix}\\
        &= \begin{pmatrix}
            U_1^\top\Cbar U_1 &\zero\\
            \zero^\top &0
        \end{pmatrix}\label{eq:block_decomp}.
    
        \lambda_1^2\le\mu_1^2\le\lambda_2^2\le\cdots\le\lambda_{n-1}^2\le\mu_{n-1}^2\le\lambda_n^2.
    
        U_1^\top\Cbar U_1 z = \mu z,\label{eq:equal_eigen}\\
        \one^\top\Cbar U_1 z = 0\label{eq:equal_otho},
    
        \Cbar U_1 z = U_1 y.
    
        U_1^\top U_1 y = \mu z.
    
        y = \mu z,
    
        \Cbar U_1 z = U_1 y = \mu U_1 z,
    
\norm{X_i}\norm{Q_i}\norm{\pbf_i}\le\sqrt{b}.

    \label{eq:linear_model}
    f^{\mathrm{Vanilla}}_\wbf(X, Q, \pbf) = \wbf^\top X Q \pbf.

    \label{eq:linear_model_shift}
    f^{\mathrm{Shift}}_\wbf(X, Q, \pbf) = \wbf^\top X Q N \pbf,

    \label{eq:linear_obj}
L(f) = \sum_{i = 1}^m\frac{1}{2}\rbr{f(X_i, Q_i, \pbf_i) - y_i}^2.

    \wbf_{t+1} = \wbf_t - \eta \nabla_\wbf L(f_{\wbf_t}),

    \norm{\wbf_t^{\mathrm{Vanilla}} - \wbf_*^{\mathrm{Vanilla}}}_2 \le O\rbr{\rho_1^t},

    \norm{\wbf_t^{\mathrm{Shift}} - \wbf_*^{\mathrm{Shfit}}}_2 \le O\rbr{\rho_2^t},

    1 > \rho_1 > \rho_2,

\label{eq:linear_obj_mat}
L(f_\wbf) = \frac{1}{2}\norm{Z^\top \wbf - \ybf}^2_2,

\label{eq:linear_mat_update}
\wbf_{t+1} &= \wbf_t - \eta \rbr{ZZ^\top \wbf_t - Z\ybf}\\
&= (I_d - \eta ZZ^\top) \wbf_t + \eta Z\ybf,

    \wbf_{t+1} - \wbf_* = \rbr{I_d - \eta ZZ^\top} \rbr{\wbf_t - \wbf_*}.

    \norm{\wbf_t - \wbf_*} &= \norm{\rbr{I_d - \eta ZZ^\top}^t \wbf_*}\\
    &\le \norm{I_d - \eta ZZ^\top}^t\norm{\wbf_*}.\label{eq:linear_converge}

    \norm{\wbf_t - \wbf_*} &\le \norm{I_d - \eta ZZ^\top}^t\norm{\wbf_*}\\
    &\le \rbr{1 - \frac{\sigma_{\min}\rbr{ZZ^\top}}{\sigma_{\max}\rbr{ZZ^\top}}}^t\norm{\wbf_*}.\label{eq:linear_rate}

    \frac{1}{m} ZZ^\top &= \frac{1}{m}\sum_{i=1}^m \zbf_i\zbf_i^\top,

    \Ebb_{\zbf} \zbf\zbf^\top &= \Ebb_{X,Q,\pbf}{XQ\pbf\rbr{XQ\pbf}^\top}\\
    &=\Ebb_{X,Q} XQ\rbr{\Ebb\sbr{\pbf\pbf^\top}}(XQ)^\top\\
    &=\Ebb_{X, Q} XQ(XQ)^\top\hspace{0.5cm}{\text{(By Assumption~\ref{assumpt:indepency})}}\\
    &=YY^\top + \Ebb_{X,Q} (XQ-Y)(XQ-Y)^\top.

    &\Ebb_{\zbf^{\mathrm{Shift}}} \zbf^{\mathrm{Shift}}\zbf^{\mathrm{Shift}\top} \\
    =& \Ebb_{X, Q} (XQ)N^2(XQ)^\top\\
    =&\Ebb_{X, Q} (XQ)N(XQ)^\top \hspace{0.5cm}{(N^2 = N)}\\
    =&YNY^\top + \Ebb_{X, Q} (XQ-Y)N(XQ-Y)^\top.

    0=&\one^\top Y^{-1}\Ebb_{\zbf^{\mathrm{Shift}}} \zbf^{\mathrm{Shift}}\zbf^{\mathrm{Shift}\top}\\
    =&\one^\top Y^{-1}\rbr{YNY^\top + \Ebb_{X, Q} (XQ-Y)N(XQ-Y)^\top}\\
    =&\one^\top Y^{-1}\Ebb_{X, Q} (XQ-Y)N(XQ-Y)^\top,

\Pr\sbr{\norm{\hat{\Sigma} - \Sigma}_2\ge\delta}\le 2d\exp\rbr{-\frac{\delta^2}{2b\rbr{\norm{\Sigma}_2+\delta}}}.

\norm{\hat{\Sigma} - \Sigma}_2\le O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}},

&\sigma_{\max}\rbr{YY^\top}-O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
\le&\sigma_{\max}\rbr{\frac{1}{m}ZZ^\top}\\
\le&\sigma_{\max}\rbr{YY^\top}+\delta_1+O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}};\\
&\sigma_{\min}\rbr{YY^\top}-O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
\le&\sigma_{\min}\rbr{\frac{1}{m}ZZ^\top}\\
\le&\sigma_{\min}\rbr{YY^\top}+\delta_1+O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}};\\
&\sigma_{\max}\rbr{YNY^\top}-O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
\le&\sigma_{\max}\rbr{\frac{1}{m}Z^{\mathrm{Shift}}Z^{\mathrm{Shift}\top}}\\
\le&\sigma_{\max}\rbr{YNY^\top}+\delta_1+O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
&\sigma_{\min}\rbr{YNY^\top}-O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
\le&\sigma_{\min}\rbr{\frac{1}{m}Z^{\mathrm{Shift}}Z^{\mathrm{Shift}\top}}\\
\le&\sigma_{\min}\rbr{YNY^\top}+\delta_1+O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}.

&\sigma_{\min}\rbr{\frac{1}{m}ZZ^\top}\\
\le&\sigma_{\min}\rbr{YY^\top}+\delta_1+O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
<&\sigma_{\min}\rbr{YNY^\top}-O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
\le&\sigma_{\min}\rbr{\frac{1}{m}Z^{\mathrm{Shift}}Z^{\mathrm{Shift}\top}}\\
\le&\sigma_{\max}\rbr{\frac{1}{m}Z^{\mathrm{Shift}}Z^{\mathrm{Shift}\top}}\\
\le&\sigma_{\max}\rbr{YNY^\top}+\delta_1+O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
<&\sigma_{\max}\rbr{YY^\top}-O\rbr{\sqrt{\frac{\log (1/\epsilon)}{m}}}\\
\le&\sigma_{\max}\rbr{\frac{1}{m}ZZ^\top}.

&\rho_2 = 1 - \frac{\sigma_{\min}\rbr{Z^{\mathrm{Shift}}Z^{\mathrm{Shift}\top}}}{\sigma_{\max}\rbr{Z^{\mathrm{Shift}}Z^{\mathrm{Shift}\top}}}\\
<&\rho_1 = 1 - \frac{\sigma_{\min}\rbr{ZZ^\top}}{\sigma_{\max}\rbr{ZZ^\top}},

        c\cdot\one^\top {Q_{\mathrm{GIN}}} N &= c\cdot\one^\top (r+1+\xi^{(1)})I_n \rbr{I_n - \frac{1}{n}\one\one^\top}\\
        &=c\rbr{r+1+\xi^{(1)}}\rbr{\one^\top -\one^\top\cdot\frac{1}{n}\one\one^\top} = 0.
    
    {Q_{\mathrm{GIN}}} N &= (A+ I_n+\xi^{(k)}I_n)N \\&= (\one\one^\top +\xi^{(k)I_n})N \\&= \xi^{(k)}N,

    \frac{\partial\Lcal}{\partial W^{(k)}}=
\rbr{\rbr{H^{(k-1)}QN}^\top\otimes S}\frac{\partial\Lcal}{\partial Z^{(k)}},

where  represents the Kronecker product, and thus  is an operator on matrices.

Analogously, the gradient of  without normalization consists a  term. As suggested by Theorem~\ref{thm:precondition},  has a smoother distribution of spectrum than , so that the gradient of  with normalization enjoys better optimization curvature than that without normalizaiton.

\section{Datasets}\label{appsec:datasets}

Detailed of the datasets used in our experiments are presented in this section. Brief statistics of the datasets are summarized in Table \ref{tab:appendix-dataset-statistics}. Those information can be also found in \citet{xu2018how} and \citet{hu2020open}.

\paragraph{Bioinformatics datasets.}
PROTEINS is a dataset where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn.
NCI1 is a dataset made publicly available by the National Cancer Institute (NCI) and is a subset of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines, having 37 discrete labels.
MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels. 
PTC is a dataset of 344 chemical compounds that reports the carcinogenicity for male and female rats and it has 19 discrete labels.

\paragraph{Social networks datasets.}
IMDB-BINARY is a movie collaboration dataset. Each graph corresponds to an ego-network for each actor/actress, where nodes correspond to actors/actresses and an edge is drawn betwen two actors/actresses if they appear in the same movie. Each graph is derived from a pre-specified genre of movies, and the task is to classify the genre graph it is derived from.
REDDIT-BINARY is a balanced dataset where each graph corresponds to an online discussion thread and nodes correspond to users. An edge was drawn between two nodes if at least one of them responded to another's comment.
The task is to classify each graph to a community or a subreddit it belongs to.
COLLAB is a scientific collaboration dataset, derived from 3 public collaboration datasets, namely, High Energy Physics, Condensed Matter
Physics and Astro Physics. Each graph corresponds to an ego-network of different researchers from
each field. The task is to classify each graph to a field the corresponding researcher belongs to.

\paragraph{Large-scale Open Graph Benchmark: ogbg-molhiv.}
Ogbg-molhiv is a molecular property prediction dataset, which is adopted from the the MOLECULENET \citep{DBLP:journals/corr/WuRFGGPLP17}. Each graph represents a molecule, where nodes are atoms and edges are chemical bonds. Both nodes and edges have associated diverse features. Node features are 9-dimensional, containing atomic number and chirality, as well as other additional atom features. Edge features are 3-dimensional, containing bond type, stereochemistry as well as an additional bond feature indicating whether the bond is conjugated. 

\begin{table*}[t]
\caption{{\bf Summary of statistics of benchmark datasets.}}
\resizebox{\textwidth}{!}{ \renewcommand{\arraystretch}{1.25}
\begin{tabular}{@{}clcccccccc@{}}
\cmidrule[\heavyrulewidth]{2-10}
& Datasets &  {\textsc{MUTAG}} & {\textsc{PTC}}  & {\textsc{PROTEINS}} & {\textsc{NCI1}} & {\textsc{IMDB-B}} & {\textsc{RDT-B}} & {\textsc{COLLAB}} & {\textsc{ogbg-molhiv}}  \\
\cmidrule[\heavyrulewidth]{2-10}
& \text{\# graphs }  & 188  & 344  & 1113  &  4110 & 1000 & 2000 & 5000 & 41127   \\
& \text{\# classes }   &  2  & 2  & 2  &  2 &  2 &  2 & 2 & 2 \\
& \text{Avg \# nodes }  &  17.9  & 25.5  & 39.1  &  29.8 &  19.8 &  429.6 & 74.5 & 25.5 \\
& \text{Avg \# edges }  &  57.5  & 72.5  & 184.7  &  94.5 &  212.8 &  1425.1 & 4989.5 & 27.5 \\
& \text{Avg \# degrees } & 3.2 & 3.0 & 4.7 & 3.1 & 10.7 & 3.3 & 66.9 & 2.1 \\
\cmidrule[\heavyrulewidth]{2-10}
\end{tabular}}
  \label{tab:appendix-dataset-statistics}
\end{table*}

\section{The Experimental Setup}\label{appsec:experiments}

\paragraph{Network architecture.}
For the medium-scale bioinformatics and social network datasets, we use 5-layer GIN/GCN with a linear output head for prediction followed \citet{xu2018how} with residual connection. The hidden dimension of GIN/GCN is set to be 64. For the large-scale ogbg-molhiv dataset, we also use 5-layer GIN/GCN\citep{xu2018how} architecture with residual connection. Following \citet{hu2020open}, we set the hidden dimension as 300.  

\paragraph{Baselines.} For the medium-scale bioinformatics and social network datasets, we compare several competitive baselines as in \citet{xu2018how}, including the WL subtree kernel model \citep{shervashidze2011weisfeiler}, diffusion-convolutional neural networks (DCNN)~\citep{atwood2016diffusion}, Deep Graph CNN (DGCNN) \citep{zhang2018end} and Anonymous Walk Embeddings (AWL) \citep{ivanov2018anonymous}. We report the accuracies reported in the original paper \citep{xu2018how}. For the large-scale ogbg-molhiv dataset, we use the baselines in \citet{hu2020open}, including the Graph-agnostic MLP model,  GCN \citep{kipf2016semi} and GIN \citep{xu2018how}. We also report the roc-auc values reported in the original paper \citep{hu2020open}.

\paragraph{Hyper-parameter configurations.}
We use Adam \citep{kingma2014adam} optimizer with a linear learning rate decay schedule. 
We follow previous work \citet{xu2018how} and \citet{hu2020open} to use hyper-parameter search (grid search) to select the best hyper-parameter based on validation performance. In particular, we select the batch size , the dropout ratio , weight decay ,  the learning rate . For the drawing of the training curves in Figure \ref{fig:gin-dataset-training-curve}, for simplicity, we set batch size to be 128, dropout ratio to be 0.5, weight decay to be 0.0, learning rate to be 1e-2, and train the models for 400 epochs for all settings. 

\paragraph{Evaluation.} Using the chosen hyper-parameter, we report the averaged test performance over different random seeds (or cross-validation). In detail, for the medium-scale datasets, following \citet{xu2018how}, we perform a 10-fold cross-validation as these datasets do not have a clear train-validate-test splitting format. The mean and standard deviation of the validation accuracies across the 10 folds are reported. For the ogbg-molhiv dataset, we follow the official setting~\citep{hu2020open}. We repeat the training process with 10 different random seeds. 

For all experiments, we select the best model checkpoint with the best validation accuracy and record the corresponding test performance. 

\section{Additional Experimental Results}\label{appsec:additional_exps}

\subsection{Visualization of the singular value distributions}\label{appsec:vis-singular}
As stated in Theorem \ref{thm:precondition}, the shift operation  serves as a preconditioner of  which makes the singular value distribution of  smoother. To check the improvements, we sample graphs from 6 median-scale datasets (PROTEINS, NCI1, MUTAG, PTC, IMDB-BINARY, COLLAB) for visualization, as in Figure \ref{fig:appendix-singular-value}.

\subsection{Visualization of noise in the batch statistics }\label{appsec:vis-noise}
We show the noise of the batch statistics on the PROTEINS task in the main body. Here we provide more experiment details and results. 

For graph tasks (PROTEINS, PTC, NCI1, MUTAG, IMDB-BINARY datasets), we train a 5-layer GIN with BatchNorm as in \citet{xu2018how} and the number of sub-layers in MLP is set to 2. For image task (CIFAR10 dataset), we train a ResNet18 \citep{he2016deep}. Note that for a 5-layer GIN model, it has four graph convolution layers (indexed from 0 to 3) and each graph convolution layer has two BatchNorm layers; for a ResNet18 model, except for the first 33 convolution layer and the final linear prediction layer, it has four basic layers (indexed from 0 to 3) and each layer consists of two basic blocks (each block has two BatchNorm layers). For image task, we set the batch size as 128, epoch as 100, learning rate as 0.1 with momentum 0.9 and weight decay as 5e-4. For graph tasks, we follow the setting of Figure \ref{fig:gin-dataset-training-curve} (described in Appendix \ref{appsec:experiments}). 

The visualization of the noise in the batch statistics is obtained as follows. We first train the models and dump the model checkpoints at the end of each epoch; Then we randomly sample one feature dimension and fix it. For each model checkpoint, we feed different batches to the model and record the maximum/minimum batch-level statistics (mean and standard deviation) of the feature dimension across different batches. We also calculate dataset-level statistics. 

As Figure \ref{fig:noise-comparison} in the main body, pink line denotes the dataset-level statistics, and green/blue line denotes the maximum/minimum value of the batch-level statistics respectively. First, we provide more results on PTC, NCI1, MUTAG, IMDB-BINARY tasks, as in Figure \ref{fig:appendix-noise-in-graphs-other-dts}. We visualize the statistics from the first (layer-0) and the last (layer-3) BatchNorm layers in GIN for comparison. Second, we further visualize the statistics from different BatchNorm layers (layer 0 to layer 3) in GIN on PROTEINS and ResNet18 in CIFAR10, as in Figure \ref{fig:appendix-noise-in-graphs-layers}. Third, we conduct experiments to investigate the influence of the batch size. We visualize the statistics from BatchNorm layers under different settings of batch sizes [8, 16, 32, 64], as in Figure \ref{fig:appendix-noise-in-graphs-batch-sizes}. We can see that the observations are consistent and the batch statistics on graph data are noisy, as in Figure \ref{fig:noise-comparison} in the main body. 

\subsection{Training Curves on GCN}\label{appsec:training_gcn}
As Figure 2 in the main body, we train GCNs with different normalization methods (GraphNorm, InstanceNorm, BatchNorm and LayerNorm) and GCN without normalization in graph classification tasks and plot the training curves in Figure 6. It is obvious that the GraphNorm also enjoys the fastest convergence on all tasks. Remarkably, GCN with InstanceNorm even underperforms GCNs with other normalizations, while our GraphNorm with learnable shift significantly boosts the training upon InstanceNorm and achieves the fastest convergence.

\subsection{Further Results of Ablation Study}\label{appsec:ablation_study}
\paragraph{BatchNorm with learnable shift.} We conduct experiments on BatchNorm to investigate whether simply introducing a learnable shift can already improve the existing normalization methods without concrete motivation of overcoming expressiveness degradation. Specifically, we equip BatchNorm with a similar learnable shift (-BatchNorm for short) as GraphNorm and evaluate its performance. As shown in Figure 12, the -BatchNorm cannot outperform the BatchNorm on the three datasets. Moreover, as shown in Figure 5 in the main body, the learnable shift significantly improve upon GraphNorm on IMDB-BINARY dataset, while it cannot further improve upon BatchNorm, which suggests the introduction of learnable shift in GraphNorm is critical.

\paragraph{BatchNorm with running statistics.} We study the variant of BatchNorm which uses running statistics (MS-BatchNorm for short) to replace the batch-level mean and standard deviation (similar idea is also proposed in \citet{yan2019towards}). At first glance, this method may seem to be able to mitigate the problem of large batch noise. However, the running statistics change a lot during training, and using running statistics disables the model to back-propagate the gradients through mean and standard deviation. Thus, we also train GIN with BatchNorm which stops the back-propagation of the graidients through mean and standard deviation (DT-BatchNorm for short). As shown in Figure 12, both the MS-BatchNorm and DT-BatchNorm underperform the BatchNorm by a large margin, which shows that the problem of the heavy batch noise cannot be mitigated by simply using the running statistics.

\paragraph{The effect of batch size.} We further compare the GraphNorm and BatchNorm with different batch sizes (8, 16, 32, 64). As shown in Figure 11, our GraphNorm consistently outperforms the BatchNorm on all the settings.

\section{Other Related Works}
\label{appsec:related}
Due to space limitations, we add some more related works on normalization and graph neural networks here. \citet{zou2019layer} used normalization to stabilize the training process of GNNs. \citet{Zhao2020PairNorm:} introduced PAIRNORM to prevent node embeddings from over-smoothing on the node classification task. Our GraphNorm focuses on accelerating the training and has faster convergence speed on graph classification tasks. \citet{yang2020revisiting} interpreted the effect of mean subtraction on GCN as approximating the Fiedler vector. We analyze more general aggregation schemes, e.g., those in GIN, and understand the effect of the shift through the distribution of spectrum. Some concurrent and independent works \citep{li2020deepergcn,chen2020learning,zhou2020towards, zhou2020effective} also seek to incorporate normalization schemes in GNNs, which show the urgency of developing normalization schemes for GNNs. In this paper, we provide several insights on how to design a proper normalization for GNNs. Before the surge of deep learning, there are also many classic architectures of GNNs such as \citet{scarselli2008graph,bruna2013spectral,defferrard2016convolutional} that are not mentioned in the main body of the paper. We refer the readers to \citet{zhou2018graph,wu2020comprehensive,zhang2020deep} for surveys of graph representation learning.

\begin{figure*}[ht]
    \centering
\includegraphics[width=\textwidth]{ICML/Appendix-Figure/ICML-GCN-gn-in-bn-ln-no-scale.pdf}
    \caption{{\bf Training performance} of GCN with different normalization methods and GCN without normalization in graph classification tasks. }
\label{fig:appendix-norm-GCN-ablation}
\end{figure*}

\begin{figure*}[ht]
    \centering        \includegraphics[width=0.8\textwidth]{Appendix-Figure/appendix-singular.pdf}
    \caption{{\bf Singular value distribution of  and }. Graph samples from PROTEINS, NCI1, MUTAG, PTC, IMDB-BINARY, COLLAB are presented. }
\label{fig:appendix-singular-value}
\end{figure*}

\begin{figure*}[ht]
    \centering
        \includegraphics[width=\textwidth]{Appendix-Figure/appendix-plot-1-other-dt.pdf}
    \caption{\textbf{Batch-level statistics are noisy for GNNs} (Examples from PTC, NCI1, MUTAG, IMDB-BINARY datasets). We plot the batch-level mean/standard deviation and dataset-level mean/standard deviation of the first (layer 0) and the last (layer 3) BatchNorm layers in different checkpoints. GIN with 5 layers is employed.}
\label{fig:appendix-noise-in-graphs-other-dts}
\end{figure*}

\begin{figure*}[ht]
    \centering
        \includegraphics[width=\textwidth]{Appendix-Figure/appendix-plot-2-noise-layers.pdf}
    \caption{\textbf{Batch-level statistics are noisy for GNNs of different depth.} We plot the batch-level mean/standard deviation and dataset-level mean/standard deviation of different BatchNorm layers (from layer 0 to layer 3) in different checkpoints. We use a five-layer GIN on PROTEINS and ResNet18 on CIFAR10 for comparison.}
\label{fig:appendix-noise-in-graphs-layers}
\end{figure*}

\begin{figure*}[ht]
    \centering
        \includegraphics[width=\textwidth]{Appendix-Figure/ICLR-Noise-BS-ablation-proteins.pdf}
    \caption{\textbf{Batch-level statistics are noisy for GNNs of different batch sizes.} We plot the batch-level mean/standard deviation and dataset-level mean/standard deviation of different BatchNorm layers (layer 0 and layer 3) in different checkpoints. Specifically, different batch sizes (8, 16, 32, 64) are chosed for comparison. GIN with 5 layers is employed.}
\label{fig:appendix-noise-in-graphs-batch-sizes}
\end{figure*}



\begin{figure*}[ht]
    \centering
\includegraphics[width=\textwidth]{ICML/Appendix-Figure/ICML-Ablation-BS-PROTEINS-RDTB.pdf}
    \caption{{\bf Training performance} of GIN/GCN with GraphNorm and BatchNorm with batch sizes of (8, 16, 32, 64) on PROTEINS and REDDITBINARY datasets.}
\label{fig:appendix-norm-bs-ablation-gin-gcn}
\end{figure*}

\begin{figure*}[ht]
    \centering
\includegraphics[width=\textwidth]{ICML/Appendix-Figure/ICML-Ablation-BN-PROTEINS-PTC-IMDBB.pdf}
    \caption{{\bf Training performance} of GIN with GraphNorm and variant BatchNorms (-BatchNorm, MS-BatchNorm and DT-BatchNorm) on PROTEINS, PTC and IMDB-BINARY datasets.}
\label{fig:appendix-norm-variant-ablation-gin-gcn}
\end{figure*}

\end{document}
