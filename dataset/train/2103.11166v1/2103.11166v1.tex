\documentclass[10pt, twocolumn]{article}

\usepackage{times}


\usepackage{adjustbox}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{float}
\usepackage{microtype}      \usepackage[subrefformat=parens,labelformat=parens]{subfig}

\usepackage{breakcites}
\usepackage[toc,page]{appendix}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{adjustbox} \usepackage{framed,multirow} 


\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}  

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}



\title{Efficient Subsampling for Generating \\ High-Quality Images from \\ Conditional Generative Adversarial Networks}

 \author{
 	Xin Ding\thanks{equal contribution}, \hspace*{0.05cm} Yongwei Wang\footnotemark[1], \hspace*{0.05cm} Z. Jane Wang, and William J. Welch \\
 	The University of British Columbia, Canada\\
 	\texttt{\small \{xin.ding@stat, yongweiw@ece, will@stat, zjanew@ece\}.ubc.ca}\
 }


\begin{document}

\maketitle
\begin{abstract}
  Subsampling unconditional generative adversarial networks (GANs) to improve the overall image quality has been studied recently. However, these methods often require high training costs (e.g., storage space, parameter tuning) and may be inefficient or even inapplicable for subsampling conditional GANs, such as class-conditional GANs and continuous conditional GANs (CcGANs), when the condition has many distinct values.     In this paper, we propose an efficient method called conditional density ratio estimation in feature space with conditional Softplus loss (cDRE-F-cSP). With cDRE-F-cSP, we estimate an image's conditional density ratio based on a novel conditional Softplus (cSP) loss in the feature space learned by a specially designed ResNet-34 or sparse autoencoder. We then derive the error bound of a conditional density ratio model trained with the proposed cSP loss. Finally, we propose a rejection sampling scheme, termed cDRE-F-cSP+RS, which can subsample both class-conditional GANs and CcGANs efficiently. An extra filtering scheme is also developed for CcGANs to increase the label consistency. Experiments on CIFAR-10 and Tiny-ImageNet datasets show that cDRE-F-cSP+RS can substantially improve the Intra-FID and FID scores of BigGAN. Experiments on RC-49 and UTKFace datasets demonstrate that cDRE-F-cSP+RS also improves Intra-FID, Diversity, and Label Score of CcGANs. Moreover, to show the high efficiency of cDRE-F-cSP+RS, we compare it with the state-of-the-art unconditional subsampling method (i.e., DRE-F-SP+RS). With comparable or even better performance, cDRE-F-cSP+RS only requires about \textbf{10}\% and \textbf{1.7}\% of the training costs spent respectively on CIFAR-10 and UTKFace by DRE-F-SP+RS.
\end{abstract}



\section{Introduction}\label{sec:intro}



\textit{Generative adversarial networks} (GANs) \cite{goodfellow2014generative} are popular generative models for image synthesis. Mathematically, they aim to estimate the marginal distribution of images. As an extension and an essential family of GANs, \textit{conditional GANs} (cGANs) \cite{mirza2014conditional} are able to estimate the image distribution given some conditions. These conditions are usually categorical variables such as class labels. cGANs with class labels are also known as \textit{class-conditional GANs} \cite{odena2017conditional, miyato2018cgans, brock2018large, zhang2019self}. Recently, \cite{ding2021ccgan, ding2020continuous} propose a new conditional GAN framework, \textit{continuous conditional GANs} (CcGANs), which takes continuous, scalar variables (termed as regression labels) such as steering angles as conditions. Recent advances in unconditional GANs \cite{karras2019style, karras2020analyzing} and conditional GANs \cite{brock2018large, ding2021ccgan, ding2020continuous} generally enable these models to generate high-quality images. Nevertheless, low-quality images still appear frequently even with such advanced GAN models during image generation. We would like to clarify that image quality discussed in this paper refers to not only the visual quality but also (1) image diversity for unconditional GANs; (2) intra-label diversity and label consistency (the consistency of generated images with respect to the conditioning label) \cite{ding2021ccgan, devries2019evaluation} for conditional GANs. 


Improving the sampling strategy of a \textbf{trained} unconditional GAN to filter out low-quality samples attracted increasing attention recently \cite{azadi2018discriminator, turner2018metropolis, ding2020subsampling, liu2020collaborative, che2020your}. These \textbf{post hoc}, \textbf{subsampling} methods for GANs can improve the sample quality without changing the network architecture or training algorithms. To accomplish such a subsampling, \textit{discriminator rejection sampling} (DRS) \cite{azadi2018discriminator} accepts or rejects a fake image by \textit{rejection sampling} (RS). \textit{Metropolis-Hastings GAN} (MH-GAN) \cite{turner2018metropolis} applies the \textit{Metropolis-Hastings} (MH) algorithm to sample from a trained GAN. Denote the true marginal image distribution and the fake marginal image distribution by  and  respectively. The success of DRS and MH-GAN requires an accurate \textit{density ratio estimation} (DRE) of . However, since the DRE step in both DRS and MH-GAN relies on the assumption of optimality of the discriminator, these two methods may not perform well if the discriminator is far from optimal. \cite{ding2020subsampling} improves DRS and MH-GAN by proposing a new DRE method, termed \textit{density ratio estimation in the feature space with Softplus loss} (DRE-F-SP), which does not require an optimal discriminator. By incorporating DRE-F-SP into RS, MH, and \textit{sampling importance resampling} (SIR), \cite{ding2020subsampling} introduces three density ratio based subsampling methods for GANs (i.e., DRE-F-SP+RS, DRE-F-SP+MH, and DRE-F-SP+SIR respectively). In addition to these density ratio based subsampling methods \cite{azadi2018discriminator, turner2018metropolis, ding2020subsampling}, \cite{liu2020collaborative} refines generated images by using information from a trained discriminator. \cite{che2020your} proposes to generate images by sampling from an energy-based model defined in the latent space of the generator in a GAN. The above methods have been demonstrated effective in subsampling different unconditional GANs (e.g., DCGAN \cite{radford2015unsupervised}, Wasserstein GAN \cite{arjovsky2017wasserstein, gulrajani2017improved}, and MMD-GAN \cite{li2017mmd}) trained on multiple datasets. These methods, however, are not designed for the conditional image synthesis setting.

A rejection sampling scheme is proposed by \cite{mo2019mining} to subsample the \textit{auxiliary classifier GAN} (ACGAN) \cite{odena2017conditional} (a class-conditional GAN) based on the gap in log-densities that measures the discrepancy between the true image distribution and the fake image distribution on given samples. Empirical studies show that this scheme can improve the performance of ACGAN in the class-conditional image synthesis. However, this method is not applicable to some class-conditional GANs (e.g., BigGAN \cite{brock2018large}) and it is not designed for CcGANs. Moreover, \cite{mo2019mining} focuses on a semi-supervised setting (most of the training data are unlabeled) while the supervised setting has not been well-studied yet. 

Another promising approach to subsample class-conditional GANs is applying DRE-F-SP+RS, DRE-F-SP+MH, or DRE-F-SP+SIR \cite{ding2020subsampling} within each image class. This approach is practical only if there are a small number of image classes. Conversely, if there exist many classes, it may be infeasible in practice. For example, if we apply this approach to subsample a BigGAN trained on Tiny-ImageNet \cite{Le2015TinyIV} with 200 classes, we need to fit 200 density ratio models separately, which is often time-consuming and computationally expensive. Moreover, since these 200 density ratio models may not share the same optimal training setups and it is generally impractical to tune the hyper-parameters for each model carefully, it is likely that some density ratio models are not well-trained. Besides the high training costs, this approach may be invalid in subsampling CcGANs because the feature extraction in DRE-F-SP is inapplicable to images with regression labels only. Even if we find a suitable feature extraction mechanism, this approach is still impractical unless there are very few distinct regression labels.

To address such challenges and fill the research gap in subsampling conditional GANs, we propose a conditional density ratio based subsampling method. The proposed method is applicable to both class-conditional GANs and CcGANs. Moreover, we only need to fit one density ratio model in subsampling such conditional GANs. Our contributions can be summarized as follows:



\begin{itemize}


	\item In Section \ref{sec:cDRE-F-cSP}, we propose cDRE-F-cSP, which is able to estimate an image's density ratio conditional on a class/regression label. We first introduce a new feature extraction method for images with regression labels, since the feature extraction mechanism in DRE-F-SP \cite{ding2020subsampling} is not applicable to CcGANs \cite{ding2021ccgan}. Then, we propose a novel \textit{conditional Softplus} (cSP) loss, which enables us to estimate density ratios conditional on different labels by fitting only one density ratio model. This density ratio model takes as input both the high-level features and the class/regression label of an image and outputs the density ratio conditional on the given label. 
	
	\item We derive in Section \ref{sec:cDRE_error_bound} the error bound of the proposed conditional density ratio model trained with the novel cSP loss.
	
	\item In Section \ref{sec:cDRE-F-cSP+RS}, we propose a rejection sampling scheme based on the estimated conditional density ratios, termed cDRE-F-cSP+RS, to subsample cGANs. An extra filtering scheme is proposed in Section \ref{sec:filtering_CcGAN} for CcGANs to increase the label consistency. 
	
	\item In Section \ref{sec:experiment}, experiments on CIFAR-10, Tiny-ImageNet, RC-49 and UTKFace demonstrate the effectiveness of the proposed cDRE-F-cSP+RS in subsampling BigGAN and CcGANs in terms of multiple metrics. Even with much less training cost, the proposed method performs comparable to (or even better than) the state-of-the-art subsampling method. 
	
	
	


\end{itemize}

\section{Related Works}\label{sec:related_work}

\subsection{Generative adversarial networks}\label{sec:related_GANs}


A GAN model \cite{goodfellow2014generative} includes two components: a generator  and a discriminator . The generator  takes as the input random noise drawn from  and outputs a fake image , which follows the fake marginal image distribution . The discriminator takes as the input a fake image  or a real image  and outputs the probability that this image comes from the true marginal image distribution . The discriminator is trained to distinguish between real and fake images while the generator is trained to fool the discriminator. This adversarial training of  and  aims to make  as close as possible to . 

cGANs \cite{mirza2014conditional} extend GANs \cite{goodfellow2014generative} into the conditional image synthesis setting, where a condition  is fed into both  and . Similar to unconditional GANs, cGANs approximate the true conditional image distribution  by the fake conditional image distribution . The conditional  is often a categorical variable such as a class label and cGANs with class labels as conditions are also known as \textit{class-conditional GANs}. Class-conditional GANs have been widely studied \cite{odena2017conditional, miyato2018cgans, brock2018large, zhang2019self}. State-of-the-art class conditional GANs such as BigGAN \cite{brock2018large} can generate photo-realistic images for a given class. However, GANs conditional on regression labels have been rarely studied due to two problems. First, very few (even zero) real images exist for some regression labels. Second, since regression labels are continuous and infinitely many, they cannot be embedded by one-hot encoding like class labels. To solve these two problems, \cite{ding2020continuous,ding2021ccgan} propose the CcGAN framework, which introduces novel empirical cGAN losses and label input mechanisms. The novel empirical cGAN losses, consisting of the \textit{hard vicinal discriminator loss} (HVDL), the \textit{soft vicinal discriminator loss} (SVDL), and a new generator loss, are developed to solve the first problem. The second problem is solved by a \textit{naive label input} (NLI) mechanism and an \textit{improved label input} (ILI) mechanism. The effectiveness of CcGAN has been demonstrated on diverse datasets.  

\subsection{Subsampling GANs by DRE-F-SP+RS, DRE-F-SP+MH, or DRE-F-SP+SIR}\label{sec:related_DRE-F-SP}


\cite{ding2020subsampling} proposes a subsampling framework for unconditional GANs to replace DRS \cite{azadi2018discriminator} and MH-GAN \cite{turner2018metropolis}. This framework consists of two components: a density ratio estimation method termed DRE-F-SP and a density ratio based sampler. DRE-F-SP aims to estimate the density ratio function  based on  real images  and  fake images . Based on the estimated density ratios, to push  to , a density ratio based sampler such as RS, MH, or SIR is used to sample from the trained GAN model. The framework with three different samplers results in three subsampling methods which are denoted respectively by DRE-F-SP+RS, DRE-F-SP+MH, and DRE-F-SP+SIR.

As the key component of the subsampling framework, DRE-F-SP \cite{ding2020subsampling} first trains a specially designed ResNet-34 \cite{he2016deep} on a set of real images with class labels under the cross-entropy loss. The network architecture of this ResNet-34 is adjusted to ensure that the dimension of one hidden map  equals that of the input image . Thus, this ResNet34 defines a mapping of an image  to a high-level feature , i.e., , 
where  is assumed invertible and the absolute value of the Jacobian determinant  is assumed positive. Denote  and  respectively as the marginal distributions of real and fake features, \cite{ding2020subsampling} shows that 

which implies that the density ratio of an image  equals to the density ratio of the corresponding high-level feature . Then, instead of estimating  directly, DRE-F-SP models the true density ratio function  in the feature space by a 5-layer multilayer perceptron (MLP) denoted by . Theoretically analysis in \cite{ding2020subsampling} shows that DRE-F-SP enables us to model the true density ratio function by a small neural network (e.g., 5-layer MLP), which is able to increase the generalization performance of the density ratio model. DRE-F-SP also proposes to train this MLP by the Softplus (SP) loss, i.e.,

where  and . The empirical approximation of Eq.\ \eqref{eq:SP_uncond} is 

where  and  are high-level features extracted by  from  and  respectively. In practice, DRE-F-SP minimizes the penalized Softplus loss:

where  controls the penalty strength and

The penalty term  encourages the average density ratios of fake images to be close to 1 to prevent  from overfitting the training data. Empirical study in \cite{ding2020subsampling} shows that DRE-F-SP often performs well if . Thus, we fix  when implementing DRE-F-SP in our experiments.







\section{Method}\label{sec:method}

As discussed in Section \ref{sec:intro}, due to high training costs and the incomplete feature extraction mechanism (only suitable for images with class labels), the unconditional subsampling approach \cite{ding2020subsampling} (i.e., DRE-F-SP+RS, DRE-F-SP+MH, DRE-F-SP+SIR) may be impractical for subsampling cGANs. Moreover, the only existing conditional subsampling method \cite{mo2019mining} is designed for ACGAN \cite{odena2017conditional} and cannot be applied to other cGANs. Motivated by these issues, in this section, we propose a general and efficient subsampling method, which is suitable for both class-conditional GANs and CcGANs no matter the number of distinct class/regression labels. 







\subsection{Conditional density ratio estimation in feature space with conditional Softplus loss} \label{sec:cDRE-F-cSP}
In this section, we introduce cDRE-F-cSP, a novel \textit{conditional density ratio estimation} (cDRE) method. Assume we have  real image-label pairs  and  fake image-label pairs . Based on these samples, cDRE-F-cSP aims to estimate .
Similar to DRE-F-SP, we conduct cDRE in a feature space learned by a pre-trained neural network . For class-conditional GANs, we use the specially designed ResNet-34 (also used by DRE-F-SP) to extract high-level features. For CcGANs, since regression datasets (e.g., Cell-200 \cite{ding2020continuous}) may not have class labels, we train a specially designed sparse autoencoder (AE) to extract features whose architecture is visualized in Fig.\ \ref{fig:sparseAE}. The encoder with ReLU \cite{glorot2011deep} as the final layer is treated as  to extract sparse high-level features from images. The bottleneck dimension of the sparse autoencoder equals the dimension of the flattened input image. The decoding process is trained to not only reconstruct the input image but also predict the regression label of the input image. The training loss of this sparse AE is the summation of three components: (1) the \textit{mean square error} (MSE) between the input image and the reconstructed image; (2) the MSE between the true regression label and the predicted regression label; (3) the product of a positive constant  and the mean of all elements in , where  controls the sparsity and is set as  in our experiment. 
The sparsity regularizer and the extra branch to predict regression labels are both used to avoid overfitting. 
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{./sparseAE.png}
	\caption{The architecture of the sparse autoencoder for feature extraction in subsampling CcGANs. The bottleneck dimension equals the size of the flattened input image . Since the encoder has ReLU as the last layer, the feature vector  is sparse. This autoencoder also has an extra branch to predict the regression label of .}
	\label{fig:sparseAE}
\end{figure}





Next, we propose a formulation of the \textbf{true} conditional density ratio function in the feature space as follows
  where  and  are respectively the real and fake condition distributions of high-level features. Based on Eq.\ \eqref{eq:cdre_equivalence} and the pre-trained neural network , we model the conditional density ratio function  in the feature space by a 5-layer MLP denoted by  with both  and its label  as input. To train , we propose the \textit{conditional Softplus} (cSP) loss as follows: 

where , , and  is the distribution of labels. The empirical approximation to Eq.\ \eqref{eq:SP_loss_cond} is 

Similar to DRE-F-SP, to prevent  from overfitting the training data, a natural constraint applied to  is 

If Eq.\ \eqref{eq:cond_constraint} holds, then

An empirical approximation to Eq.\ \eqref{eq:joint_constraint} is

Therefore, in practice, we minimize the penalized version of Eq.\ \eqref{eq:emp_SP_loss_cond} as follows:

where 

All experiments in this paper use . An algorithm shown in Alg.\ \ref{alg:cDRE-F-cSP} is used to implement cDRE-F-cSP in practice.

\begin{algorithm}[!htbp]
	\footnotesize
	\SetAlgoLined
	\KwData{ real image-label pairs , a generator , a pre-trained ResNet-34 or encoder  for feature extraction, a 5-layer MLP  and a preset hyperparameter .}
	\KwResult{a trained conditional density ratio model . }
	Initialize \;
	\For{ \KwTo }{
		Sample a mini-batch of  \textbf{real} image-label pairs  from \;
		Sample a mini-batch of  \textbf{fake} image-label pairs  from \;
		Update  via SGD or a variant with the gradient of Eq.\eqref{eq:penalized_SP_loss_cond}, i.e., .
}
	\caption{The optimization algorithm for the density ratio model training in cDRE-F-cSP.}
	\label{alg:cDRE-F-cSP}
\end{algorithm}



\subsection{Error bound}\label{sec:cDRE_error_bound}
In this section, we derive the error bound of a density ratio model  trained with the Softplus loss . For simplicity, we ignore the penalty term in this analysis. 

Firstly, we introduce some notations. Let  denote the hypothesis space of the density ratio model . We also define  and  as follows:
 and 
Please note that the hypothesis space  may not cover the true density ratio function . Therefore, . Denote by  all learnable parameters of  and assume  is in a parameter space . Denote  by . Let  denote the empirical Rademacher complexity of , which is defined based on independent feature-label pairs  from . Then,
we derive the error bound of the conditional density ratio estimate  under the theoretical loss as follows:
\begin{theorem}
	\label{thm:error_bound}
	If (i)  is large enough, (ii)  is compact, (iii)  is continuous at , (iv) ,  a function  that does not depend on , s.t. , and (iv) , then  and  with probability at least ,

\end{theorem}

\begin{proof}
	The proof is in Supp.\ \ref{supp:proofs}.
\end{proof}

\begin{remark}
	 on the right of Eq.\ \eqref{eq:error_bound} implies that we should not use an overly complicated density ratio model. It supports our proposed cDRE-F-cSP because we just need a small neural network (e.g., a shallow MLP) to model the density ratio function in the feature space. 
\end{remark}




\subsection{cDRE-F-cSP+RS: A cDRE-based rejection sampling scheme for conditional GANs}\label{sec:cDRE-F-cSP+RS}

Based on the cDRE method proposed in Section \ref{sec:cDRE-F-cSP}, we develop a rejection sampling scheme, termed cDRE-F-cSP+RS, to subsample conditional GANs. The workflow can be summarized in Fig.\ \ref{fig:workflow_cDRE-F-cSP+RS} and Alg.\ \ref{alg:cDRE-F-cSP+RS}. This rejection sampling scheme is conducted for each distinct label  of interest. For example, on CIFAR-10 \cite{krizhevsky2009learning}, we train only one density ratio model , based on which we repeat the rejection sampling scheme 10 times for 10 classes respectively. 



\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth, height=4cm]{./workflow_cDRE-F-cSP+RS.png}  \caption{The workflow of cDRE-F-cSP+RS has two modules: cDRE-F-cSP and rejection sampling.  in the acceptance probability  equals to , which can be estimated by evaluating  on some burn-in samples before subsampling.}
	\label{fig:workflow_cDRE-F-cSP+RS}
\end{figure}

\begin{algorithm}[!htbp]
	\footnotesize
	\SetAlgoLined
	\KwData{a generator , a trained ResNet-34 or encoder   for feature extraction, a trained conditional density ratio model .}
	\KwResult{y}
	Generate  burn-in fake images from  conditional on label .\;
	Estimate the density ratios of these  fake images conditional on  by evaluating \;
	\;
	\;
	\While{}{
		y\;
		\;
		\;
 (i.e., the acceptance probability in RS)\;
		\;
		\If{}{
			\;
		}
	}
	\caption{Subsampling fake images with label  by cDRE-F-cSP+RS.}
	\label{alg:cDRE-F-cSP+RS}
\end{algorithm}


\subsection{A filtering scheme for subsampling CcGANs}\label{sec:filtering_CcGAN}

To solve the problem of insufficient data, CcGANs use images with labels in the vicinity of  to estimate . When the CcGAN training is complete, the generator can generate infinite images given a regression label  (termed the assigned label). However, the true labels of these generated images may be inconsistent with the assigned label, and such label inconsistency is very likely to happen in the CcGAN sampling because of the vicinal training technique. Consequently, evaluating the density ratio model  on images generated from CcGANs with the assigned label  may be problematic because the true density ratio function  may be ill-defined on these images. We propose a filtering scheme in both the density ratio model training and the subsampling process to deal with this issue. To be specific, we first train a regression-oriented CNN (we use VGG-11 \cite{simonyan2014very} in our experiments) on the same dataset used for the CcGAN training. This CNN is used to predict labels of fake images generated from CcGANs, and these predicted labels are treated as the true labels of the generated images. Then, before evaluating  in the density ratio model training (Alg.\ \ref{alg:cDRE-F-cSP}), we predict the labels of the  fake images and filter out those images if the mean absolute error (MAE) between their assigned labels and predicted labels is larger than a threshold . Finally, before conducting rejection sampling in the subsampling process (Fig.\ \ref{fig:workflow_cDRE-F-cSP+RS} and Alg.\ \ref{alg:cDRE-F-cSP+RS}), we filter out fake images with MAE between assigned labels and true labels larger than the threshold . A rule of thumb to select the filtering threshold  is shown in Alg.\ \ref{alg:rule_of_thumb_threshold}. Besides making valid the evaluation of  on images generated from CcGANs, empirical study in Section \ref{sec:exp_rc49} and \ref{sec:exp_utkface} shows that this filtering scheme can effectively increase the label consistency. 

\begin{algorithm}[!htbp]
	\footnotesize
	\SetAlgoLined
	Sample  images per label from the trained CcGAN without subsampling\;
	Predict the labels of these fake images by the pre-trained VGG-11\;
	Compute MAE between the predicted labels and assigned labels of the generated images\;
	Sort these MAEs from smallest to largest and the \textbf{0.8} quantile of these MAEs is set as the filtering threshold .
	\caption{A rule of thumb to select the filtering threshold .}
	\label{alg:rule_of_thumb_threshold}
\end{algorithm}







\vspace{-4mm}
\section{Experiments}\label{sec:experiment}



In this section, the objective is to experimentally demonstrate the effectiveness of the proposed subsampling scheme for both class-conditional GANs and CcGANs. We conduct experiments on four image datasets. In subsampling class-conditional GANs, we utilize the CIFAR-10 \cite{krizhevsky2009learning} and Tiny-ImageNet \cite{Le2015TinyIV} datasets. In subsampling CcGANs, we experiment on  RC-49 \cite{ding2021ccgan, ding2020continuous} and UTKFace \cite{utkface} datasets. 

In the class-conditional GAN setting, the quality of fake images is evaluated by two metrics: \textit{Fr\'echet inception distance} (FID) \cite{heusel2017gans} and Intra-FID \cite{miyato2018cgans}. Intra-FID is an overall image quality metric, which computes the FID separately for each class and reports the average FID score. A lower Intra-FID/FID index indicates a better quality of sampled fake images, or vice versa. In the CcGAN setting, we follow CcGAN \cite{ding2021ccgan} and adopt four image quality assessment metrics: (i) Intra-FID \cite{miyato2018cgans} is an overall image quality metric; (ii) \textit{Naturalness Image Quality Evaluator} (NIQE) \cite{mittal2012making} evaluates the visual quality of fake images. Please note that visual quality is only one aspect of image quality; (iii) \textit{Diversity} measures the diversity of fake images; and (iv) \textit{Label Score} (LS) evaluates label consistency. \textbf{We prefer small Intra-FID, FID, NIQE, and Label Score but large Diversity.} Please refer to Supp.\ \ref{supp:test_setup_of_cifar10_exp} and \ref{supp:test_setup_of_rc49} for the details of these metrics. \textbf{Many example fake images generated in each experiment are also shown in the supplementary material.}



\subsection{CIFAR-10}\label{sec:exp_cifar10}
We first evaluate the effectiveness of the proposed method in subsampling BigGAN \cite{brock2018large} in the conditional image synthesis setting on the CIFAR-10 dataset \cite{krizhevsky2009learning}.  



{\setlength{\parindent}{0cm}\textbf{Experimental setup:}} CIFAR-10 consists of 60,000 () RGB images uniformly from 10 classes. The overall number of training samples is 50,000 (5000 for each class), and the remaining 10,000 samples (1000 for each class) are for test. The class-conditional GAN models employ the BigGAN architecture \cite{brock2018large}. To investigate the effectiveness of the proposed method on different GAN models, we train the BigGAN under three different settings by varying the number of training samples. Thus we obtain three BigGAN models, denoted respectively as BC-50k, BC-20k, and BC-10k. Specifically, the BC-50k model is obtained by training BigGAN using all 50,000 samples from the CIFAR-10 training set. The other two models are obtained using randomly chosen subsets of the training set with 20,000 (BC-20k) and 10,000 (BC-10k) samples, respectively.

Next, we implement the proposed cDRE-F-cSP+RS for each setting. Firstly, we train a customized ResNet-34 classifier on the training set for feature extraction. Then, by employing the proposed loss in Eq. \eqref{eq:penalized_SP_loss_cond}, a 5-layer MLP is trained as the conditional density ratio model in the feature space learned by the ResNet-34. As a reference, we also implement DRE-F-SP+RS \cite{ding2020subsampling} in each setting (denoted by 10-DRE-F-SP+RS), where ten unconditional density ratio models (ten 5-layer MLPs) are carefully trained for the ten image classes separately. Thus, the proposed method needs only 10\% of the training costs (e.g., storage space and parameter tuning) spent by 10-DRE-F-SP+RS. Please refer to Section \ref{supp:details_of_rc49} for the detailed setups.


{\setlength{\parindent}{0cm}\textbf{Quantitative results:}} We quantitatively compare the image quality of fake images subsampled using no subsampling, 10-DRE-F-SP+RS, and cDRE-F-cSP+RS. With each sampling method, we draw 50,000 fake images (5000 per class) from each of the three GAN models. Table~\ref{tab:cifar10_compare} compares the results and shows that samples from either 10-DRE-F-SP+RS or cDRE-F-cSP+RS subsampling methods significantly outperform no subsampling. Moreover, the performance gain tends to increase when GAN models become less well-trained after applying either of the two subsampling schemes. Finally, compared with 10-DRE-F-SP+RS, the proposed method achieves comparable performances, however, it requires only 10\% of the computational costs to train the density ratio models. Please note that it is impractical to adopt DRE-F-SP+RS \cite{ding2020subsampling} and other unconditional subsampling methods \cite{azadi2018discriminator, turner2018metropolis, liu2020collaborative, che2020your} if the number of classes is large (e.g., 200 classes on Tiny-ImageNet as shown in Section~\ref{supp:details_of_cifar10}). 

We also show in Fig.~\ref{fig:FID_vs_class_CIFAR10} the FID comparison for each class. For each class, cDRE-F-cSP+RS shows clear superiority over the no-subsampling scheme, and it achieves comparable performance with 10-DRE-F-SP+RS. Please refer to Supp.\ \ref{supp:details_of_cifar10_visual_results} for visual comparison examples. 



\begin{table}[htb]
	\caption{CIFAR-10 dataset: Quality of 50,000 fake images (5000 per class) sampled from different sampling methods on three trained GAN models. cDRE-F-cSP+RS only requires \textbf{10\%} training costs of 10-DRE-F-SP+RS.  }
	\centering
	\begin{adjustbox}{width=0.48\textwidth}
		\begin{tabular}{c|c|c|c|c|c|c}
			\toprule
			\multirow{2}{*}{Sampling methods} & \multicolumn{2}{c|}{BC-50k} & \multicolumn{2}{c|}{BC-20k} & \multicolumn{2}{c}{BC-10k} \\ \cline{2-7} 
			& Intra-FID      & FID        & Intra-FID      & FID        & Intra-FID      & FID       \\ \hline
			no-subsampling                    & 0.915          & 0.415      & 1.636          & 0.973      & 3.271          & 2.601     \\ \hline
			10-DRE-F-SP+RS                    & \textbf{0.542}          & \textbf{0.226}      & \textbf{1.158}          & \textbf{0.675}      & \textbf{2.511}          & \textbf{1.948}     \\ \hline
			cDRE-F-cSP+RS                      & 0.598          & 0.240      & 1.267          & 0.700      & 2.621          & 1.987     \\ 
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\label{tab:cifar10_compare}
\end{table}


\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{./CIFAR10_10000_line_graphs_fid_vs_class.png} \caption{CIFAR-10: FID versus class plots in sampling the BC-10k class-conditional GAN model. FID scores of cDRE-F-cSP+RS are smaller than those of no-subsampling and comparable to those of 10-DRE-F-SP+RS over all classes. }
	\label{fig:FID_vs_class_CIFAR10}
\end{figure}


\subsection{Tiny-ImageNet}\label{sec:exp_tiny_imagenet}
This experiment further demonstrates the effectiveness of the proposed method in subsampling class-conditional GANs on the Tiny-ImageNet dataset \cite{Le2015TinyIV}.  

{\setlength{\parindent}{0cm}\textbf{Experimental setup:}} The Tiny-ImageNet dataset \cite{Le2015TinyIV} contains 200 image classes for training. Each class has 500 images for training and 50 images for test. All images are RGB images of size . In the Tiny-ImageNet dataset, we adopt the BigGAN architecture and train the BigGAN model on all training images. We also adopt respectively a customized ResNet-34 and a 5-layer MLP for feature extraction and conditional density ratio estimation. 

Since it is impractical to train 200 unconditional density ratio models \cite{ding2020subsampling} on this dataset, we compare the proposed subsampling scheme with the no-subsampling method only. From each method, we sample one million (5000 per class) fake images for image quality evaluation. Please refer to Supp.\ \ref{supp:details_of_tinyImageNet} for details and Supp.\ \ref{supp:details_of_tinyImageNet_visual_results} for visual comparison examples. 


{\setlength{\parindent}{0cm}\textbf{Quantitative results:}} Table~\ref{tab:TinyImageNet_compare} shows image quality comparisons of fake images sampled using no-subsampling and the proposed methods on Tiny-ImageNet. The proposed method enhances image quality for both metrics, particularly the FID score.         

\begin{table}[!htbp]
	\caption{Tiny-ImageNet dataset: Quality of one million fake images (5000 per class) sampled from two sampling methods on a trained BigGAN model.}
	\centering
	\begin{adjustbox}{width=0.27\textwidth}
		\begin{tabular}{c|c|c}
			\toprule
			Sampling methods & Intra-FID & FID   \\ \hline
			no-subsampling   & 30.226    & 5.012 \\ \hline
			cDRE-F-cSP+RS     & \textbf{29.361}    & \textbf{2.005} \\ 
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\label{tab:TinyImageNet_compare}
\end{table}

\subsection{RC-49}\label{sec:exp_rc49}
This experiment is conducted on RC-49 to show that the proposed subsampling scheme also works in CcGANs.   

{\setlength{\parindent}{0cm}\textbf{Experimental setup:}} RC-49 is a benchmark dataset for CcGANs \cite{ding2021ccgan,ding2020continuous}. It is made by rendering 49 3-D chair models individually. Each chair model is rendered at 899 yaw angles from  to  with a stepsize of . This dataset contains 44,051 RGB images of size  with corresponding yaw angles as labels. We follow the official implementation of CcGAN in \cite{ding2020continuous} which adopts the SNGAN architecture \cite{miyato2018spectral}. On RC-49, we train three CcGAN models with SVDL+ILI by varying the number of training samples (i.e., 25, 15, 5) for each label. Thus we obtain three CcGAN models as SR-25, SR-15, and SR-5, respectively. To perform subsampling, we firstly train on RC-49 a sparse autoencoder with an extra branch for label prediction (see Fig.~\ref{fig:sparseAE}). The trained autoencoder is utilized as the feature extractor. We then train a 5-layer MLP to estimate conditional density ratios. In subsampling CcGANs, we also adopt the proposed filtering scheme in Alg.~\ref{alg:rule_of_thumb_threshold}. Detailed experimental setups can be found in Supp\ \ref{supp:details_of_rc49}.          


{\setlength{\parindent}{0cm}\textbf{Quantitative results:}} Table~\ref{tab:rc49_compare} reports quantitative evaluations of subsampled fake images from the no-subsampling and cDRE-F-cSP+RS on RC-49. Clearly, cDRE-F-cSP+RS consistently outperforms no-subsampling in all three settings in terms of the overall image quality measured by Intra-FID. More specifically, both subsampling methods produce fake images with comparable visual quality as indicated by NIQE. The proposed subsampling method, however, generates fake images with improved diversity and label consistency on RC-49. Moreover, as shown in Fig.~\ref{fig:line_graphs_of_diversity_labelScore}, we evaluate the diversity/label consistency of both methods at each angle. The proposed method consistently improves over no-subsampling, indicating its effectiveness in generating more diverse and label consistent fake images uniformly over all angles in RC-49. Please refer to Supp.\ \ref{supp:details_of_rc49_visual_results} for visual comparison examples. 

The influence of the MAE quantile in Alg.\ \ref{alg:rule_of_thumb_threshold} on the performance of cDRE-F-cSP+RS is shown in Table \ref{tab:rc49_tau_selection}. We can see a smaller MAE quantile often leads to a larger Intra-FID, a higher Diversity, a smaller Label Score, but an unchanged NIQE, which implies there is a trade-off between the image diversity and label consistency and Intra-FID is more sensitive to Diversity. This table confirms 0.8 is a good choice for the MAE quantile, trading slightly less image diversity for better label consistency. 


\begin{table}[!htbp]
	\caption{RC-49 dataset: Quality of 179,800 fake images (200 per angle) sampled from two sampling methods on three trained GAN models. }
	\centering
	\begin{adjustbox}{width=0.47\textwidth}
		\begin{tabular}{c|c|c|c|c|c}
			\toprule
			\begin{tabular}[c]{@{}c@{}}CcGAN\\ models\end{tabular}                   & \begin{tabular}[c]{@{}c@{}}Sampling \\ methods\end{tabular} & Intra-FID & NIQE  & Diversity & \begin{tabular}[c]{@{}c@{}}Label\\ Score\end{tabular} \\ \hline
			\multirow{2}{*}{SR-25} & no-subsampling                                              & 0.386     & 1.759 & 2.952     & 1.930                                                 \\ \cline{2-6} 
			& cDRE-F-cSP+RS                                                & \textbf{0.307}     & \textbf{1.755} & \textbf{3.081}     & \textbf{1.545}                                                 \\ \hline 
			\multirow{2}{*}{SR-15} & no-subsampling                                              & 0.395     & \textbf{1.779} & 2.939     & 2.020                                                 \\ \cline{2-6} 
			& cDRE-F-cSP+RS                                                & \textbf{0.309}     & 1.794 & \textbf{3.052}     & \textbf{1.559}                                                 \\ \hline 
			\multirow{2}{*}{SR-5}  & no-subsampling                                              & 0.465     & 1.860 & 2.778     & 2.515                                                 \\ \cline{2-6} 
			& cDRE-F-cSP+RS                                                & \textbf{0.357}     & \textbf{1.836} & \textbf{2.971}     & \textbf{1.930}                                                 \\ 
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\label{tab:rc49_compare}
\end{table}


\begin{table}[!htbp]
	\caption{RC-49 dataset: Influence of the MAE quantile in the filtering algorithm (Alg.\ \ref{alg:rule_of_thumb_threshold}) on the performance of cDRE-F-cSP+RS in subsampling SR-25. }
	\centering
	\begin{adjustbox}{width=0.4\textwidth}
		\begin{tabular}{c|c|c|c|c}
			\toprule
			\begin{tabular}[c]{@{}c@{}}MAE\\ quantile\end{tabular}  & Intra-FID & NIQE & Diversity & Label Score \\
			\hline
			1.0                   & 0.292      & 1.763 & 3.110       & 1.996        \\\hline
			0.9                   & 0.308     & 1.755 & 3.089       & 1.724        \\\hline
			0.8                   & 0.307      & 1.755 & 3.081       & 1.545        \\\hline
			0.7                   & 0.353      & 1.756 & 3.035      & 1.371   \\
			\bottomrule    
		\end{tabular}
	\end{adjustbox}
	\label{tab:rc49_tau_selection}
\end{table}





\begin{figure}[!htbp]
	\captionsetup[subfloat]{farskip=2pt,captionskip=1pt}
	\centering
	\subfloat[RC-49: Diversity vs Angle]{\includegraphics[width=4cm, height=3cm]{./RC49_5_line_graphs_diversity_vs_angle.png}\label{fig:rc49_diversity_angle}}  
	\subfloat[RC-49: Label Score vs Angle]{\includegraphics[width=4cm, height=3cm]{./RC49_5_line_graphs_LS_vs_angle.png}\label{fig:rc49_labelScore_vs_angle}}   
	\caption{Line graphs of Diversity/Label Score versus regression labels in subsampling the SR-5 CcGAN model. The proposed subsampling method consistently outperforms the no-subsampling method across all regression labels.}
	\label{fig:line_graphs_of_diversity_labelScore}
\end{figure}





\subsection{UTKFace}\label{sec:exp_utkface}
This experiment evaluates the performance of subsampling methods on UTKFace \cite{utkface}, another regression dataset used in CcGANs \cite{ding2020continuous,ding2021ccgan}.       

{\setlength{\parindent}{0cm}\textbf{Experimental setup:}} UTKFace is an RGB human face image dataset with ages as regression labels. This experiment utilizes the preprocessed UTKFace dataset \cite{ding2020continuous,ding2021ccgan}, which consists of 14,760 RGB images with ages in [1, 60]. The number of images ranges from 50 to 1051 for different ages. All images are of size . Similar to the RC-49 experiments in Section~\ref{sec:exp_rc49}, we adopt the SNGAN architecture \cite{miyato2018spectral} for CcGAN. In training CcGAN on UTKFace, the maximum number of images per age equals 200. Thus we obtain a trained CcGAN model as SU-200.  Next, we train a sparse autoencoder on UTKFace as the feature extractor. We then train a 5-layer MLP for conditional density estimation with training images from UTKFace and images generated by SU-200. Besides the proposed method, we also implement 60-DRE-F-SP+RS, i.e., training 60 unconditional density ratio models \cite{ding2020subsampling} for 60 ages separately. By contrast, cDRE-F-cSP+RS only needs to train one density ratio model. Thus, cDRE-F-cSP+RS consumes about 1.7\% of the training costs spent by 60-DRE-F-SP+RS. A detailed experimental setup is in Supp.\ \ref{supp:details_of_cifar10}.


{\setlength{\parindent}{0cm}\textbf{Quantitative results:}} In Table~\ref{tab:utkface_compare}, we show performance comparisons of the no-subsampling method, 60-DRE-F-SP+RS, and cDRE-F-cSP+RS. In terms of the overall image quality comparison, both the proposed method and 60-DRE-F-SP+RS outperform the no-subsampling method measured by Intra-FID. Compared with 60-DRE-F-SP+RS, the proposed method shows a comparable overall performance. The three methods generate fake images with comparable visual quality.  While 60-DRE-F-SP+RS has slightly worse diversity than no-subsampling, the proposed cDRE-F-cSP+RS shows substantial improvement in image diversity and label score over both no-subsampling and 60-DRE-F-SP+RS.  Some example images for this experiments are also shown in Supp.\ \ref{supp:details_of_utkface_visual_results}.

\begin{table}[!htbp]
	\caption{UTKFace dataset: Quality of 60,000 fake images (1000 per age) sampled from different sampling methods on the SU-200 CcGAN model. cDRE-F-cSP+RS only requires \textbf{1.7}\% training costs of 60-DRE-F-SP+RS.}
	\centering
	\begin{adjustbox}{width=0.45\textwidth}
		\begin{tabular}{c|c|c|c|c}
			\toprule
			\begin{tabular}[c]{@{}c@{}}Subsampling\\ methods\end{tabular} & Intra-FID      & NIQE           & Diversity      & \begin{tabular}[c]{@{}c@{}}Label \\ Score\end{tabular} \\ \hline
			no-subsampling                                                & 0.432          & 1.725          & 1.304          & 7.444                                                  \\ \hline
			60-DRE-F-SP+RS                                                & \textbf{0.423} & \textbf{1.722} & 1.298          & 7.300                                                  \\ \hline
			cDRE-F-cSP+RS                                                  & 0.424          & 1.737          & \textbf{1.335} & \textbf{6.763}                                         \\ 
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\label{tab:utkface_compare}
\end{table}




\section{Conclusion} \label{sec:conclusion}
This work presents a novel conditional subsampling scheme to improve the image quality of fake images from conditional GANs. First, we propose novel conditional extensions of density ratio estimation (cDRE) in the feature space and the Softplus loss function (cSP). Then, we learn the conditional density ratio model through an MLP network. Also, we derive the error bound of a conditional density ratio model trained with the proposed cSP loss. A novel filtering scheme is also proposed in subsampling CcGANs to improve the label consistency. Finally, we validate the effectiveness of the proposed subsampling scheme with extensive experiments in sampling multiple conditional GAN models on four image datasets with diverse evaluation metrics. 


\clearpage
\newpage
\bibliographystyle{plain}
\bibliography{cdre_ref}


\clearpage
\newpage
\appendix

\section*{Supplementary Material}
\addcontentsline{toc}{section}{Supplementary Material}


\renewcommand{\thesection}{S.\arabic{section}} 
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thetheorem}{S.\arabic{theorem}} 
\renewcommand{\thedefinition}{S.\arabic{definition}} 
\renewcommand{\thelemma}{S.\arabic{lemma}} 
\renewcommand{\theremark}{S.\arabic{remark}}


\section{GitHub repository}\label{supp:codes}
Please find the codes for this paper at Github:
\begin{center}
	\url{https://github.com/UBCDingXin/cDRE-based_Subsampling_cGANS}
\end{center}



\section{The Proof of Theorems \ref{thm:error_bound}}\label{supp:proofs}
This proof follows the proof of Theorem 3 in \cite{ding2020subsampling}. 

\begin{proof}
	Following \cite{ding2020subsampling}, we decompose  as follows:
	
	The second term in Eq.\ \eqref{eq:first_decompose} is a constant which implies an inevitable error. The first term can be bounded as follows:
	
	
	Because of assumptions (i)-(iv), based on the uniform law of large number \cite{noteULLN}, for ,
	
	
	Based on this limit, we can derive an upper bound of the first term of Eq.\ \eqref{eq:second_decompose} as follows. Since we can generate infinite fake images from a trained cGAN,  is large enough. Let ,  with probability at least , 
	
	
	The second term of Eq.\ \eqref{eq:second_decompose} can be bounded based on Lemma 1 and Theorem 2 (the Rademacher bound \cite{lafferty2010}) in \cite{ding2020subsampling} as follows:  with probability at least ,
	
	Let  and , based on Eq.\ \eqref{eq:bound_first_term} and \eqref{eq:bound_second_term}, we can derive Eq.\ \eqref{eq:error_bound}.
\end{proof}


\section{More Details of Experiments on CIFAR-10}\label{supp:details_of_cifar10}

In this section, we introduce the training and test setups for our experiments on CIFAR-10. We also show some example fake images in Section \ref{supp:details_of_cifar10_visual_results}.

\subsection{Network architectures} \label{supp:net_arch_of_cifar10_exp}

For feature extraction, we use the specially designed ResNet-34 in \cite{ding2020subsampling}. Similar to \cite{ding2020subsampling}, we use an MLP-5 with structure shown in Table \ref{tab:cifar10_MLP5} as the unconditional density ratio model to implement DRE-F-SP. Another MLP-5 with structure shown in Table \ref{tab:cifar10_cMLP5} is used as the conditional density ratio model to implement cDRE-F-cSP. 


\begin{table}[!htbp]
	\centering
	\caption{The 5-layer MLP for DRE in feature space for CIFAR-10.}
	\begin{tabular}{c}
		\toprule
		Extracted feature  \\
		\hline
		fc, GN (8 groups), ReLU, Dropout() \\\hline
		fc, GN (8 groups), ReLU, Dropout() \\\hline
		fc, GN (8 groups), ReLU, Dropout() \\\hline
		fc, GN (8 groups), ReLU, Dropout() \\\hline
		fc, GN (8 groups), ReLU, Dropout() \\\hline
		fc, ReLU \\
		\bottomrule
	\end{tabular}\label{tab:cifar10_MLP5}\end{table}

\begin{table}[!htbp]
	\centering
	\caption{The 5-layer MLP for cDRE in feature space for CIFAR-10. The embedded class label is appended to the extracted feature .}
	\begin{tabular}{c}
		\toprule
		Input: extracted feature  \\
		and embedded class label  \\
		\hline
		Concatenate \\
		\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, ReLU \\
		\bottomrule
	\end{tabular}\label{tab:cifar10_cMLP5}\end{table}

\subsection{Training setups} \label{supp:train_setup_of_cifar10_exp}

For each experimental setting described in Section \ref{sec:exp_cifar10}, we train ResNet-34, MLPs, and BigGAN with setups as follows. The specially designed ResNet-34 for feature extraction is trained on the corresponding training set for 200 epochs with the SGD optimizer, initial learning rate 0.1 (decay at epoch 100 and 150 with factor 0.1), weight decay , and batch size 128. Two MLP-5 models are trained on the training set with the Adam optimizer \cite{kingma2014adam}, initial learning rate  (decay at epoch 150 and 250), batch size 128, 350 epochs, and . The implementation of BigGAN is mainly based on \url{https://github.com/ajbrock/BigGAN-PyTorch}. The BigGAN model in each setting is trained for 2000 epochs with batch size 512. Following \cite{ding2020subsampling}, to evaluate fake images, we pre-train an Inception V3 on CIFAR-10 using all 50000 training images.
z

\subsection{Testing setups} \label{supp:test_setup_of_cifar10_exp}

At the testing stage, we generate 50,000 images (5000 per class) from each subsampling method. We adopt two popular metrics---\textit{Fr\'echet inception distance} (FID) \cite{heusel2017gans} and Intra-FID \cite{miyato2018cgans}---to evaluate the image quality of the fake images. Intra-FID is an overall metric for the evaluation of conditional GANs, which evaluate cGANs from three perspectives: (1) visual quality, (2) intra-label diversity, and (3) label consistency. On the other hand, FID can only evaluate the visual quality and the marginal diversity (since it is not computed within each class). Both Intra-FID and FID are computed based on the feature map with dimension 2048 of the last average pooling layer of the pre-trained Inception V3. 



\subsection{Visual results} \label{supp:details_of_cifar10_visual_results}



In this section, we show the example fake images generated from each subsampling method for some image classes (e.g., car and horse). E.g., in Figs.\ \ref{fig:cifar10_10K_visual_results_car} and \ref{fig:cifar10_10K_visual_results_horse}, we show 100 examples images for each method for the ``car'' class and the ``horse'' class in the BC-10k setting. We also show 100 real images as reference in the visualized image samples. 

In Figs.\ \ref{fig:cifar10_10K_visual_results_car}, we observe numerous images (marked in red boxes) from the no-subsampling method, which we can hardly recognize as cars. By contrast, there appear much less such low-quality examples after subsampling by 10-DRE-F-SP+RS and the proposed cDRE-F-cSP+RS methods. We have similar observations in Fig.~\ref{fig:cifar10_10K_visual_results_horse}.   

\begin{figure*}[!htbp]
	\centering
	\subfloat[][Real Images]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_real_images_class_2_10x10.png}\label{fig:cifar10_10K_real_car}}\quad
	\subfloat[][no-subsampling]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_ntrain_10000_no_subsampling_class_2_10x10.png}\label{fig:cifar10_10K_no_subsampling_car}}
	\\
	\subfloat[][10-DRE-F-SP+RS]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_ntrain_10000_unconditional_class_2_10x10.png}\label{fig:cifar10_10K_unconditional_car}}\quad
	\subfloat[][cDRE-F-cSP+RS]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_ntrain_10000_conditional_class_2_10x10.png}\label{fig:cifar10_10K_conditional_car}}
	\caption{Example CIFAR-10 images for the ``car'' class in the BC-10k setting.}
	\label{fig:cifar10_10K_visual_results_car}
\end{figure*}

\begin{figure*}[!htbp]
	\centering
	\subfloat[][Real Images]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_real_images_class_8_10x10.png}\label{fig:cifar10_10K_real_horse}}\quad
	\subfloat[][no-subsampling]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_ntrain_10000_no_subsampling_class_8_10x10.png}\label{fig:cifar10_10K_no_subsampling_class_horse}}
	\\
	\subfloat[][10-DRE-F-SP+RS]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_ntrain_10000_unconditional_class_8_10x10.png}\label{fig:cifar10_10K_unconditional_class_horse}}\quad
	\subfloat[][cDRE-F-cSP+RS]{\includegraphics[width=0.45\textwidth, height=7cm]{./cifar10_ntrain_10000_conditional_class_8_10x10.png}\label{fig:cifar10_10K_conditional_horse}}
	\caption{Example CIFAR-10 images for the ``horse'' class in the BC-10k setting.}
	\label{fig:cifar10_10K_visual_results_horse}
\end{figure*}



\section{More Details of Experiments on Tiny-ImageNet}\label{supp:details_of_tinyImageNet}

In this section, we introduce the detailed training and testing setups of our experiments on Tiny-ImageNet. Some example fake images are shown in Section \ref{supp:details_of_tinyImageNet_visual_results}.

\subsection{Network architectures} \label{supp:net_arch_of_tinyImageNet_exp}

Similar to the CIFAR-10 experiment, a specially designed ResNet-34 is trained to extract high-level features from images. This ResNet-34 is almost identical to the one used in the CIFAR-10 experiment. The MLP-5 with structure shown in Table~\ref{tab:tinyimagenet_cMLP5} is used as the conditional density ratio model to implement cDRE-F-cSP on the Tiny-ImageNet dataset.    

\begin{table}[!htbp]
	\centering
	\caption{5-layer MLP for cDRE in feature space for Tiny-ImageNet. The embedded class label is appended to the extracted feature .}
	\begin{tabular}{c}
		\toprule
		Input: extracted feature  \\
		and embedded class label  \\
		\hline
		Concatenate \\
		\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, GN (4 groups), ReLU, Dropout() \\\hline
		fc, ReLU \\
		\bottomrule
	\end{tabular}\label{tab:tinyimagenet_cMLP5}\end{table}



\subsection{Training setups} \label{supp:train_setup_of_tinyImageNet_exp}

The modified ResNet-34 for feature extraction is trained for 200 epochs with the SGD optimizer, initial learning rate 0.1 (decay at epoch 100 and 150 with factor 0.1), weight decay , and batch size 128. A 5-layer MLP is trained as the conditional density ratio model in cDRE-F-cSP+RS with Adam optimizer \cite{kingma2014adam}, initial learning rate  (decay at epoch 150 and 250), batch size 128, 350 epochs, and . The BigGAN model is trained with 2000 epochs and batch size 128. We also adopt DiffAugment \cite{zhao2020differentiable} (a data augmentation method for the GAN training with limited data) to improve the performance of BigGAN. Similar to the CIFAR-10 experiment, we train an Inception V3 net on Tiny-ImageNet from scratch for evaluation.


\subsection{Testing setups} \label{supp:test_setup_of_tinyImageNet_exp}
In the testing phase, we generate one million images (5000 per class) from the no-sbusampling and the proposed methods, respectively. Similar to CIFAR-10 experiments, we adopt FID and Intra-FID as two evaluation metrics. Both metrics are also computed based on the feature map with dimension 2048 of the last average pooling layer of a pretrained Inception V3. 



\subsection{Visual results} \label{supp:details_of_tinyImageNet_visual_results}
In this section, we visualize some example fake images generated from two sampling methods for some image classes (e.g., goldfish and tower) in Figs.\ \ref{fig:tiny-imagenet_visual_results_goldfish} and \ref{fig:tiny-imagenet_visual_results_tower}. 

In Fig. \ref{fig:tiny-imagenet_visual_results_goldfish}, as marked by red boxes, such ``goldfish'' images from the no-subsampling method are of very low quality. Also, we can observe some replicated samples (a.k.a mode collapse) for samples from the no-subsampling method. Even for images that look like ``goldfish'', such images appear to look ``darker'' and less colorful than real images. Compared to the no-subsampling method, the proposed method generates significantly improved image quality, i.e., our image samples are both more diverse and look much more natural those from the baseline method. In Fig. \ref{fig:tiny-imagenet_visual_results_tower}, we can also observe fake image samples from the no-subsampling method generally look less appealing than those from the proposed method. Moreover, for some images (marked in red boxes) from the no-subsampling method, the tower is actually missing in those ``tower'' images. However, we can clearly see the tower in images that are sampled from our method.  



\begin{figure*}[!htbp]
	\centering
	\subfloat[][Real Images]{\includegraphics[width=0.48\textwidth]{./tiny-imagenet_real_images_class_1_10x10.png}\label{fig:tiny-imagenet_real_goldfish}}
	\\
	\subfloat[][no-subsampling]{\includegraphics[width=0.48\textwidth]{./tiny-imagenet_no_subsampling_class_1_10x10.png}\label{fig:tiny-imagenet_no_subsampling_goldfish}}
	\quad
	\subfloat[][cDRE-F-cSP+RS]{\includegraphics[width=0.48\textwidth]{./tiny-imagenet_conditional_class_1_10x10.png}\label{fig:tiny-imagenet_conditional_goldfish}}\quad
	\caption{Example Tiny-ImageNet images for the ``goldfish'' class.}
	\label{fig:tiny-imagenet_visual_results_goldfish}
\end{figure*}

\begin{figure*}[!htbp]
	\centering
	\subfloat[][Real Images]{\includegraphics[width=0.48\textwidth]{./tiny-imagenet_real_images_class_125_10x10.png}\label{fig:tiny-imagenet_real_tower}}
	\\
	\subfloat[][no-subsampling]{\includegraphics[width=0.48\textwidth]{./tiny-imagenet_no_subsampling_class_125_10x10.png}\label{fig:tiny-imagenet_no_subsampling_tower}}
	\quad
	\subfloat[][cDRE-F-cSP+RS]{\includegraphics[width=0.48\textwidth]{./tiny-imagenet_conditional_class_125_10x10.png}\label{fig:tiny-imagenet_conditional_tower}}\quad
	\caption{Example Tiny-ImageNet images for the ``tower'' class.}
	\label{fig:tiny-imagenet_visual_results_tower}
\end{figure*}






\section{More Details of Experiments on RC-49} \label{supp:details_of_rc49}
In this section, we introduce the detailed training and testing setups of our experiments on RC-49. We also show some example fake images in Section~\ref{supp:details_of_rc49_visual_results}. 


\subsection{Network architectures}  \label{supp:net_arch_of_rc49}
For the feature extraction model, we adopt a specially designed sparse antoencoder with architecture shown in Table~\ref{tab:RC49_SparseAE_encoder}. We also employ an MLP-5 as the conditional density ratio model, which uses the same architecture as shown in Table~\ref{tab:tinyimagenet_cMLP5} (i.e., encoder branch), Table~\ref{tab:RC49_SparseAE_decoder} (i.e., decoder branch), and Table~\ref{tab:RC49_SparseAE_labelPred} (i.e., label prediction branch). We adopt the SNGAN architecture for CcGAN which is also used by \cite{ding2020continuous}.

The models for the computation of Intra-FID, NIQE, Diversity, and Label Score are consistent with those used by \cite{ding2021ccgan, ding2020continuous}. Please refer to the official implementation of CcGANs at \url{https://github.com/UBCDingXin/improved_CcGAN} for more details.




\begin{table}[!htbp]
	\centering
	\caption{The architecture of the encoder in the sparse autoencoder for extracting features from  RGB images. In convolutional (Conv) operations,  denotes the number of channels,  denote kernel size, stride and number of padding, respectively. }
	\begin{tabular}{c}
		\toprule
		Input: an RGB image . \\ 
		\hline
		Conv (, ), BN, ReLU \\\hline
		Conv (, ), BN, ReLU \\\hline
		Conv (, ), BN, ReLU \\\hline
		Conv (, ), BN, ReLU \\\hline
		Conv (, ), BN, ReLU \\\hline
		Conv (, ), BN, ReLU \\\hline
		Conv (, ), BN, ReLU
		\\\hline
		Conv (, ), ReLU
		\\ \hline
		Output: extracted sparse features 
		\\ \bottomrule
	\end{tabular}\label{tab:RC49_SparseAE_encoder}\end{table}

\begin{table}[!htbp]
	\centering
	\caption{The architecture of the decoder in the sparse autoencoder for reconstructing  input images from extracted features. In transposed-convolutional (ConvT) operations,  denotes the number of channels,  denote kernel size, stride and number of padding, respectively. }
	\begin{tabular}{c}
		\toprule
		Input: extracted sparse features  
		\\ from Table~\ref{tab:RC49_SparseAE_encoder}. \\
		\hline
		ConvT (, ), BN, ReLU \\\hline
		ConvT (, ), BN, ReLU \\\hline
		ConvT (, ), BN, ReLU \\\hline
		ConvT (, ), BN, ReLU \\\hline
		ConvT (, ), BN, ReLU \\\hline
		ConvT (, ), BN, ReLU \\\hline
		ConvT (, ), BN, ReLU
		\\\hline
		ConvT (, ), Tanh
		\\ \hline
		Output: a reconstructed image 
		\\ \bottomrule
	\end{tabular}\label{tab:RC49_SparseAE_decoder}\end{table}


\begin{table}[!htbp]
	\centering
	\caption{The architecture of the label prediction branch in the sparse autoencoder for  images.}
	\begin{tabular}{c}
		\toprule
		Input: extracted sparse features  \\
		from Table~\ref{tab:RC49_SparseAE_encoder}. \\
		\hline
		fc, BN, ReLU \\\hline
		fc, BN, ReLU \\\hline
		fc, BN, ReLU \\\hline
		fc, BN, ReLU \\\hline
		fc, ReLU \\
		\hline
		Output: the predicted label 
		\\ \bottomrule
	\end{tabular}\label{tab:RC49_SparseAE_labelPred}\end{table}






\subsection{Training setups} \label{supp:train_setup_of_rc49}
The sparse autoencoder for feature extraction is trained for 200 epochs with the SGD optimizer, initial learning rate 0.01 (decay every 50 epochs with a factor 0.1), weight decay , and batch size 128. Similarly, the VGG-11 network in the filtering scheme is trained for 200 epochs with the SGD optimizer, initial learning rate 0.01 (decay at epoch 50 and 120 with factor 0.1), weight decay , and batch size 128. The CcGAN (SVDL+ILI) training setup is consistent with work \cite{ding2020continuous}.


\subsection{Testing setups} \label{supp:test_setup_of_rc49}
In the testing phase, we generate 179,800 fake images (200 per angle) from the no-subsampling and the proposed cDRE-F-cSP+RS methods, respectively. This experiment adopts four popular evaluation metrics---(i) Intra-FID \cite{miyato2018cgans} is an overall image quality metric; (ii) \textit{Naturalness Image Quality Evaluator} (NIQE) \cite{mittal2012making} evaluates the visual quality of fake images. Please note again that the visual quality is only one aspect of image quality; (iii) \textit{Diversity} measures the diversity of fake images; and (iv) \textit{Label Score} (LS) evaluates label consistency. Quantitatively, we prefer smaller Intra-FID, FID, NIQE  and Label Score indices but larger Diversity values. 

Specifically, the four metrics are computed as follows. (i) For the Intra-FID index, at each of the 899 angles (), we compute the FID \cite{heusel2017gans} value between 49 real images and 200 fake images in terms of the bottleneck feature of the pre-trained autoencoder. The Intra-FID score is the average FID over all 899 evaluation angles. (ii) For the NIQE index, firstly we fit an NIQE model with the 49 real rendered chair images at each of the 899 angles which gives 899 NIQE models. We then compute an average NIQE score for each evaluated angle using the NIQE model at that angle. Finally, we report the average of the 899 average NIQE scores over the 899 yaw angles. The block size and the sharpness threshold are set to 8 and 0.1 respectively in this experiments. We employ the built-in NIQE library in \texttt{MATLAB}. (iii) For the Diversity index, at each evaluation angle, firstly we use a pretrained classification-oriented ResNet-34 to predict the chair types (49 types in total) of these 200 fake images. Then, an entropy value can be computed based on the chair type predictions at this angle. Finally, the Diversity index is defined as the average of the entropies at all 899 angles. (iv) For the Label Score index, at each evaluation angle, firstly we ask a pretrained regression-oriented ResNet-34 to predict the yaw angles of all fake image samples and the predicted angles are then compared with the assigned angles. The Label Score value is defined as the average absolute distance between the predicted angles and assigned angles over all fake images, which is equivalent to the Mean Absolute Error (MAP).   




\subsection{Visual results}  \label{supp:details_of_rc49_visual_results}
In this section, we show the example fake images generated from each subsampling method in Figs.\ \ref{fig:rc49_5_visual_results}. From such image examples, we observe that the no-subsampling method generates many image samples that suffer from the mode collapse issue. By contrast, we have largely alleviated this issue in the proposed cDRE-F-cSP+RS subsampling method. Therefore, we can safely conclude that our method generates more diverse fake images than the baseline sampling method.    




\begin{figure*}[!htbp]
	\centering
	\subfloat[][Real Images]{\includegraphics[width=0.48\textwidth]{./RC49_real_images_grid_10x10.png}\label{fig:rc49_5_real}}
	\\
	\subfloat[][no-subsampling]{\includegraphics[width=0.48\textwidth]{./RC49_5_no_subsampling.png}\label{fig:rc49_5_no_subsampling}}
	\quad
	\subfloat[][cDRE-F-cSP+RS]{\includegraphics[width=0.48\textwidth]{./RC49_5_conditional.png}\label{fig:rc49_5_conditional}}\quad
	\caption{Example RC-49 images for angles from 4.5 degree to 85.5 degree (from top to bottom) in the SR-5 setting.}
	\label{fig:rc49_5_visual_results}
\end{figure*}



\section{More Details of Experiments on UTKFace} \label{supp:details_of_utkface}
This section describes the details of training and testing setups of our experiments on the UTKFace dataset. 

\subsection{Network architectures} \label{supp:net_arch_of_utkface_exp}
The network architectures used in this experiment is similar to those in the RC-49 experiment. Please refer to Section~\ref{supp:details_of_rc49} for details. 


\subsection{Training setups} \label{supp:train_setup_of_utkface}
Similar to the RC-49 experiment, we evaluate the quality of fake images by Intra-FID, NIQE, Diversity, and Label Score. We also train a sparse autoencoder (bottleneck dimension is 512), a classification-oriented ResNet-34, and a regression-oriented ResNet-34 on the UTKFace dataset. Please note that, the UTKFace dataset consists of face images from 5 races based on which we train the classification-oriented ResNet-34. The autoencoder and both two ResNets are trained for 200 epochs with a batch size 256.



\subsection{Testing setups} \label{supp:test_setup_of_utkface}
In the testing stage, we generate 60,000 fake images (1000 per age) from each subsampling method. Similar to experiments on RC-49, we adopt Intra-FID, NIQE, Diversity and Label Score as quantitative measures in Section~\ref{supp:test_setup_of_rc49}. 
Analogue to experiments on RC-49, four metrics can be computed accordingly. Please note that the metric values are average over 60 ages. 




\subsection{Visual results}  \label{supp:details_of_utkface_visual_results}

\begin{figure*}[!htbp]
	\centering
	\subfloat[][Real Images]{\includegraphics[width=0.48\textwidth]{./UTKFace_real_images_grid_10x10.png}\label{fig:utkface_real}}
	\quad
	\subfloat[][no-subsampling]{\includegraphics[width=0.48\textwidth]{./UTKFace_no_subsampling.png}\label{fig:utkface_no_subsampling}}
	\\
	\subfloat[][60-DRE-F-SP+RS]{\includegraphics[width=0.48\textwidth]{./UTKFace_unconditional.png}\label{fig:utkface_unconditional}}
	\quad
	\subfloat[][cDRE-F-cSP+RS]{\includegraphics[width=0.48\textwidth]{./UTKFace_conditional.png}\label{fig:utkface_conditional}}\quad
	\caption{Example UTKFace images for ages from 3 to 57 (from top to bottom).}
	\label{fig:utkface_visual_results}
\end{figure*}


We show the example fake images generated from each subsampling method in Figs.\ \ref{fig:utkface_visual_results}. As marked in red boxes in images from the no-subsampling method, we observe that, occasionally some face images indeed have clearly wrong ages. For example, in the second row, two marked face images at an age 9 look clearly more mature than they should be. Such image samples give rise to the label inconsistency issue in CcGANs. By contrast, we can hardly observe such label inconsistency issue for images from the proposed method. 

\end{document}
