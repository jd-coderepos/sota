\documentclass[twoside,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[boxruled,vlined,nokwfunc]{algorithm2e}

\usepackage{graphics,graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{url}

\newcommand{\TODO}{\textcolor{red}{TODO}}
\newcommand{\REDUCED}{\mbox{\sc Reduced-Cost}}
\newcommand{\PIVOT}{\mbox{\sc Pivot}}
\newcommand{\RANDOM}{\mbox{\sc Random}}
\newcommand{\IMPROVE}{\mbox{\sc Improve}}

\usepackage{ifpdf}
\ifpdf
\DeclareGraphicsRule{*}{mps}{*}{}
\else
\DeclareGraphicsRule{x*}{eps}{*}{}
\fi

\clearpage{}\setlength{\textwidth}{6.8in}
\setlength{\evensidemargin}{-0.15in}
\setlength{\oddsidemargin}{-0.15in}
\setlength{\topmargin}{-0.7in}
\setlength{\textheight}{9.4in}
\setlength{\textfloatsep}{10pt}

\renewcommand{\baselinestretch}{1.1}

\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt plus 2pt}

\renewcommand{\textfraction}{.01}
\renewcommand{\topfraction}{.99}
\renewcommand{\bottomfraction}{.99}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}

\newenvironment{proof}{{\bf Proof:\ }}{\hfill\medskip}
\newcommand{\qed}{\hfill\medskip}

\newcommand{\simpleTODOcomment}[2]{\stepcounter{TODOcounter#1}{\bf
    \scriptsize({\arabic{TODOcounter#1}~{#1}})
    {\bfseries{TODO:} #2}
  }
}

\newcounter{TODOcounter}
\newcommand{\TODOX}[1]{\simpleTODOcomment{}{#1}}

\newcommand{\RR}{{\mathbb R}}
\def\EE{\mathbb E}
\def\NN{\mathbb N}

\def\PP{\mathcal{P}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\dom}{dom}

\newcommand{\etal}{{\em et al.\ }}
\newcommand{\mathsc}[1]{\mbox{\textsc #1}}
\newcommand{\id}{1\hspace{-1,5ex}1}

\newcommand{\setofpermutationfunctions}{\mathcal{P}}
\newcommand{\setofgoodindexfunctions}{\mathcal{I}^\oplus}
\newcommand{\setofchoicefunctions}{\mathcal{C}}
\newcommand{\setofbijections}{\mathcal{S}}
\newcommand{\setofresetedges}{R}

\newcommand{\resetbudget}{\mathtt{res}}
\newcommand{\completecycles}{\mathtt{Com}}
\newcommand{\completeabstractcycles}{\mathtt{Abs}}
\newcommand{\indexfunction}{\mathtt{ind}}
\newcommand{\permutationfunction}{\mathtt{per}}

\newcommand{\ind}{\indexfunction}
\newcommand{\bit}{\mathtt{bit}}
\newcommand{\first}{\mathtt{first}}
\newcommand{\last}{\mathtt{last}}
\newcommand{\choicefunction}{\mathtt{cho}}
\newcommand{\abstractbits}{\mathtt{Bit}}
\newcommand{\abstractset}{\mathtt{Sta}}
\newcommand{\lowestsetbit}{\mathtt{low}}

\newcommand{\dis}{\mathtt{dis}}

\newcommand{\valuation}{val}
\newcommand{\VAL}{\mbox{\sc val}}
\newcommand{\POT}{\mbox{\sc pot}}

\newcommand{\appendixproof}[1]{{\bf {#1.}}}


\newcommand{\abstractstates}{\mathcal{B}}
\newcommand{\abstractaccessstates}{\mathcal{A}}
\newcommand{\abstractcountingbit}{\beta}
\newcommand{\abstractaccessbit}{\alpha}

\def\pp{p}
\newcommand{\goodindexprobability}{\pp}



\newcommand{\aaa}{{\bf a}}
\newcommand{\bbb}{{\bf b}}
\newcommand{\uuu}{{\bf u}}
\newcommand{\www}{{\bf w}}
\newcommand{\eee}{{\bf e}}

\newcommand{\A}{{\mathsc{a}}}
\newcommand{\B}{{\mathsc{b}}}
\newcommand{\BB}{{\mathcal{B}}}

\def\AC{\mathsc{a}}
\def\BC{\mathsc{b}}
\def\DC{\mathsc{d}}
\newcommand{\cyclecenter}{\BC}
\newcommand{\accesscenter}{\AC}
\newcommand{\resetlaneentry}{\DC}
\newcommand{\TT}{{\mathsc t}}
\newcommand{\sinknode}{\mathsc t}
\newcommand{\accessentry}{\textsc g}
\newcommand{\cycleentry}{\mathsc h}

\def\ll{\ell}
\def\rr{r}
\def\ss{s}
\newcommand{\numberofbits}{n}
\newcommand{\numberofresetcopies}{\rr}
\newcommand{\numberofaccesscopies}{\ll}
\newcommand{\lengthofaccessbits}{\rr}
\newcommand{\lengthofcounterbits}{\ll\rr}
\newcommand{\lowerboundsignature}{\zeta}

\newcommand{\RandomFacet}{\mbox{\sc Random-}\allowbreak\mbox{\sc Facet}}
\newcommand{\RandomBland}{\mbox{\sc Random-}\allowbreak\mbox{\sc Bland}}
\newcommand{\Bland}{\mbox{\sc Bland}}
\newcommand{\RandomEdge}{\mbox{\sc Random-}\allowbreak\mbox{\sc Edge}}
\newcommand{\RandomPerm}{{\mbox{\sc Random-}\allowbreak\mbox{}}}

\newcommand{\RF}{\mbox{\sc Random-Facet}}
\newcommand{\RP}{\mbox{}}
\newcommand{\RB}{\mbox{\sc Ran-Bla}}

\newcommand{\RandCountP}{\mbox{}}

\newcommand{\RandCount}{\mbox{\sc RandCount}}
\newcommand{\NonRecursiveRandCount}{\mbox{\sc NonRecursiveRandCount}}
\newcommand{\AugmentedRandCount}{\mbox{}}
\newcommand{\BlandsRule}{\mbox{\sc Bland's rule}}
\newcommand{\RandomizedBlandsRule}{\mbox{\sc Randomized Bland's rule}}
\newcommand{\OnePassVariant}{\mbox{\sc One-Pass RandomFacet}}

\newcommand{\dotcup}{\,\ensuremath{\mathaccent\cdot\cup}\,}

\newcommand{\sab}{{\sigma_{\alpha,\beta}}}
\newcommand{\vst}{{val_{\sigma,\tau}}}
\newcommand{\vvv}{v}
\newcommand{\val}{\mbox{y}}

\newcommand{\E}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand{\PPr}[1]{\mathbb{P}\left[{#1}\right]}

\newcommand{\ignore}[1]{} 
\clearpage{}

\begin{document}

\title{\textbf{Errata for:} A subexponential lower bound for the \\Random Facet algorithm for Parity Games}

\author{Oliver Friedmann\thanks{Department of Computer Science,
University of Munich, Germany. E-mail: {\tt
  Oliver.Friedmann@gmail.com}.}\\
 \and
Thomas Dueholm Hansen\thanks{Department of Computer Science,
Aarhus University, Denmark. E-mail:
{\tt tdh@cs.au.dk}.}\\
\and
Uri Zwick\thanks{School of Computer Science, Tel Aviv University,
  Israel. E-mail: {\tt zwick@tau.ac.il}.} }
\date{}

\maketitle

\begin{abstract}\noindent In Friedmann, Hansen, and Zwick (2011) we claimed that the expected number of pivoting steps performed by the  algorithm of Kalai and of Matou{\v{s}}ek, Sharir, and Welzl is equal to the expected number of pivoting steps performed by , a variant of  that bases its random decisions on one random permutation. We then obtained a lower bound on the expected number of pivoting steps performed by  and claimed that the same lower bound holds also for .
Unfortunately, the claim that the expected numbers of steps performed by  and  are the same is false. We provide here simple examples that show that the expected numbers of steps performed by the two algorithms are not the same.
\end{abstract}

\section{Introduction}

The  algorithm was introduced by Kalai \cite{Kalai92,Kalai97} and by Matou{\v{s}}ek, Sharir, and Welzl \cite{MaShWe96}. It is a randomized pivoting rule for the simplex algorithm that solves linear programs, and more general \emph{LP-type problems}, in expected time , where  is the number of inequalities\footnote{It is customary to denote the number of inequalities by , but we use  to be consistent with the notation for graphs and shortest paths.} and  is the dimension. Kalai's formulation of the algorithm works recursively as follows: Select a uniformly random facet containing the current vertex. Recursively find the optimal solution within the selected facet. If possible, perform an improving pivot from the resulting vertex and repeat. Otherwise return the vertex as the solution.

In Friedmann, Hansen, and Zwick \cite{FriedmannHansenZwick/SODA11} we proved a  lower bound for the expected number of pivots performed by a modified variant, , of the  algorithm. The lower bound was proved for \emph{parity games}, which are of LP-type (see \cite{Halman07}).  takes as input a fixed permutation of the facets and always selects the first available facet according to this permutation. We claimed that  performs the same expected number of pivots as  when the permutation is chosen uniformly at random. This would imply that our lower bound also holds for the original  algorithm. The claim is incorrect, however, and in this errata we explain the error. The main result of \cite{FriedmannHansenZwick/SODA11} is thus flawed.

In Friedmann, Hansen, and Zwick \cite{FriedmannHansenZwick/STOC11} we transferred the lower bound construction from the setting of parity games to the setting of \emph{Markov decision processes}. This proves a lower bound for the  algorithm for Markov decision processes, and consequently for linear programming. Again, due to our mistake, the lower bound does not hold for the original  algorithm.

It should be stressed that the error is unrelated to the lower bound construction itself; it only involves the relationship between the  algorithm and the modified version, . In particular, the lower bound for  remains correct. Also, the main focus of \cite{FriedmannHansenZwick/STOC11} was to prove a lower bound for the  pivoting rule, and this work remains unaffected.

The error was pointed out briefly in \cite{Hansen12}, and an alternative proof of a  lower bound for the  algorithm was given. Note that this lower bound is weaker than the  lower bound we originally claimed in \cite{FriedmannHansenZwick/SODA11,FriedmannHansenZwick/STOC11}. The proof in \cite{Hansen12} was presented for Markov decision processes. In \cite{FriedmannHansenZwick/2014} we transfer the lower bound construction to the setting of shortest paths where we again prove a  lower bound for , and a  lower bound for .

In this errata we explain our error in detail and give examples where the expected running times of  and  differ. We give a short introduction of the algorithms in Section~\ref{sec:algorithms}. To simplify the presentation we restrict our attention to shortest paths problems. In particular we adopt the notation from \cite{FriedmannHansenZwick/2014}. In Section \ref{sec:slower} we give an example where  requires more pivots in expectation than , and in Section \ref{sec:faster} we give an example where the opposite is the case. It is not difficult to obtain similar examples for parity games.


\section{The  and  algorithms}\label{sec:algorithms}

Let  be a weighted directed graph, where  is a \emph{cost} function defined on its edges. Let  be a designated \emph{target} vertex. We let  and  be the number of vertices, not counting the target, and edges in~, respectively. We are interested in finding a tree of shortest paths from all vertices to~.\footnote{To maintain consistency with \cite{FriedmannHansenZwick/SODA11,FriedmannHansenZwick/STOC11} it is more convenient for us to work with the \emph{single-target} version of the shortest paths problem, rather than the more standard \emph{single-source} version.}

Let  be a tree containing directed paths from all vertices to~. For some vertex , let  be the distance from  to  in the tree . The  algorithm takes two arguments: a set of edges  and a tree . It computes the tree of shortest paths for the subgraph defined by  as follows. It picks a uniformly random edge , removes  from , and finds the optimal tree  for the resulting subgraph defined by . It then checks whether including  in  improves the solution, i.e., whether the path from  to  that starts with the edge , which is currently not in the tree, is shorter than the path from  to  in the tree. If the path is shorter, then  is exchanged with the edge  emanating from  in , and a second recursive call is made with  and . Note that updating the tree in this way corresponds to an improving pivot of the simplex algorithm. We assume for simiplicity that  does not contain negative cycles. If the path is not shorter, then  is a tree of shortest paths for .

Pseudo-code for \RandomFacet\ is given on the left of Figure~\ref{F-RandomFacet}. The first argument~
is the set of available edges. Initially . The second argument~ is the current tree.
The call  checks whether  can be used to improve .
The call  returns the tree obtained from pivoting with , i.e., it exchanges  with some .

\begin{figure}[t]
\begin{center}
\parbox{3.2in}{
\SetAlgoFuncName{Algorithm}{anautorefname}
\begin{function}[H]
\DontPrintSemicolon
\SetAlgoRefName{}
\eIf{}
{\Return{}\;}
{
     \;
     \;
    \eIf{}
    {
         \;
        \Return{}\;
    }
    {
      \Return{}\;
    }
}
\caption{\RandomFacet()}
\end{function}
}
\hspace*{10pt}
\parbox{3.2in}{
\SetAlgoFuncName{Algorithm}{anautorefname}
\begin{function}[H]
\DontPrintSemicolon
\SetAlgoRefName{}
\eIf{}
{\Return{}\;}
{
     \;
     \;
    \eIf{}
    {
         \;
        \Return{}\;
    }
    {
      \Return{}\;
    }
}
\caption{()}
\end{function}
}
\end{center}
\caption{The  and  algorithms.}\label{F-RandomFacet}
\end{figure}

The  algorithm is identical to the  algorithm, except that it takes as an additional argument a permutation  of the edges and always removes the first available edge according to this permutation. Pseudo-code for  is shown on the right of Figure~\ref{F-RandomFacet}. Note that  is a deterministic algorithm. We are however interested in the case when the permutation  is chosen uniformly at random.

Let  be the expected number of pivots performed by , and let  be the expected number of pivots performed by  when  is uniformly random.
In Lemma 4.1 in \cite{FriedmannHansenZwick/SODA11} we claim that  for all  and all trees . This claim is false, and we next show where the mistake was made.

Let  and  be given where .
Suppose  is a permutation of  such that the elements of  are ordered uniformly at random. Let  be the first available edge from  according to . At this stage, selecting  according to  is equivalent to selecting  uniformly at random. Let  be the optimal tree for , and assume that  improves  such that  and  perform a second recursive call with . Note that we may assume that  is uniquely determined by  and . In \cite{FriedmannHansenZwick/SODA11} we incorrectly assume that if  is uniformly random then the elements of  are also ordered uniformly at random, and it is this assumption that is the source of our error.

In Sections \ref{sec:slower} and \ref{sec:faster} we give two concrete examples in which  and , respectively. The examples are for shortest paths. In the remainder of this section we consider an abstract example that illustrates why the permutation is not uniformly random for the second recursive call.

Suppose  and . For some permutation , we write  as shorthand notation for . There are six permutations of :

As there is an equal number of permutations in which  and , the first random choice made by  is uniform. However, if, for example, , then only three equally likely permutations remain:

Now, if , then the distribution of the elements from  is no longer uniform for the second recursive call of . We now have  with probability , and
 with probability . Thus, elements that leave  are more likely to be picked sooner in the second recursive call.

\section{Example:  can be slower than }\label{sec:slower}

Consider the simple example of a shortest paths problem given in Figure \ref{fig:example}. There are three non-terminal vertices , each with two out-going edges. We refer to the edges leaving  as  and , where the cost of  is 0 and the cost of  is positive. We refer to the other edges in a similar fashion such that the set of edges is . Note that a tree  can be viewed as a binary string  of length 3. In particular the set of trees can be viewed as vertices of a cube, and in fact the corresponding linear program is combinatorially equivalent to a cube.

\begin{figure}[t]
\begin{center}
\includegraphics[scale=1]{errata.1}
\end{center}
\caption{A shortest paths problem in which the expected number of improving switches performed by \RandomFacet\ and  differ.}\label{fig:example}
\end{figure}

Consider two neighboring vertices of the cube. Moving from one vertex to the other corresponds to performing a pivot step, and we orient the edge in the improving direction. When all edges are oriented, the resulting orientation is known as an \emph{acyclic unique sink orientation} (see, e.g., \cite{SzWe01}). Note that a set of edges  defines a corresponding \emph{face} that contains the trees in . In every such face there is a unique optimal solution, which means that in the subcube there is a unique vertex where all the edges are incoming. Such a vertex is called a \emph{sink}.

The orientation corresponding to the graph in Figure \ref{fig:example} is shown in Figure \ref{fig:slower}. For the example we start with the tree , which corresponds to the vertex 001. The optimal solution is the vertex 000, and there are three paths (sequences of pivots) leading from 001 to 000. These paths are shown at the top of Figure \ref{fig:slower}. The vertex 000 is in the lower left corner of the cube, and the cube is oriented according to the axes shown on the left.

\begin{figure}[t]
\begin{center}
\parbox{1.2in}{
\center
\includegraphics[scale=2.3]{errata.2}
}
\hspace*{5pt}
\parbox{1.5in}{
\center
\includegraphics[scale=2.3]{errata.3}
}
\hspace*{5pt}
\parbox{1.5in}{
\center
\includegraphics[scale=2.3]{errata.4}
}
\hspace*{5pt}
\parbox{1.5in}{
\center
\includegraphics[scale=2.3]{errata.5}
}

\vspace*{5pt}

\parbox{6in}{
\center
\includegraphics{errata.9}
\caption{Paths generated by the  and  algorithms when starting from 001.}\label{fig:slower}
}
\end{center}
\end{figure}

The bottom of Figure \ref{fig:slower} shows the computations performed by \RandomFacet\ and . The set  is the set of edges given as an argument to the calls  and . The elements in  are underlined, and  is assumed to be uniformly random. The labelled squares indicate the edges that are chosen by the algorithms. The probabilities of picking the edges are shown as well. In most cases the probabilities are the same, however, the label  means that  picks the edge with probability  and  picks the edge with probability . The lines below the squares indicate recursive calls with the chosen edge. The first recursive call is shown on the left, and if a second recursive call is made, it is shown on the right. Lines corresponding to second recursive calls are labelled by the pivot that is made. Recall that the vertex returned by the first recursive call is uniquely determined. When only a single sequence of pivots can be generated in a subtree this sequence is shown instead of the subtree itself. It is also shown which of the paths the sequence corresponds to. For a selection of edges, the sequence of pivots for the generated path can be read in a pre-order traversal of the tree.

The line to  is dashed because removing this edge does not affect the behavior of the algorithms. More precisely there is no path from 010 that leads back to the  facet. (In fact, the edge that leaves a tree during a pivot can in general never appear in the corresponding second recursive call, and it may be viewed as if it has been removed.) We instead combine the probabilities that should have appeared after  with those at the same recursive call as .

Consider the path in the computation tree that first picks  and then picks .
In order for the call

to realize this path, the permutation  must satisfy , , and . There are 150 permutations that satisfy these restrictions, which means that the path is generated with probability , and the edge is picked with probability , given that the call is made. The permutations can be obtained by adding  and  to the following five permutations:

The same choice is made with probability  by . Note that  was part of the original tree , and as illustrated in Section \ref{sec:algorithms} this means that it is more likely to be picked during a second recursive call, which is exactly the behavior we observe here. The resulting path, \textbf{Path 3}, is longer than the path, \textbf{Path 2}, generated when  is picked. Thus the expected running time is worse for  than for . To be exact, the expected number of pivots made by  is

and
the expected number of pivots made by  is



\section{Example:  can be faster than }\label{sec:faster}

We next give an example for which  is faster than . In fact the example is again for the graph shown in Figure~\ref{fig:example}, and we therefore use the same notation as in Section~\ref{sec:slower}. The only difference is that we now start from the vertex 111. The resulting paths and computation tree are shown in Figure \ref{fig:faster}. There are again only three paths from 111 to 000.

\begin{figure}[t]
\begin{center}
\parbox{1.2in}{
\center
\includegraphics[scale=2.3]{errata.2}
}
\hspace*{5pt}
\parbox{1.5in}{
\center
\includegraphics[scale=2.3]{errata.6}
}
\hspace*{5pt}
\parbox{1.5in}{
\center
\includegraphics[scale=2.3]{errata.7}
}
\hspace*{5pt}
\parbox{1.5in}{
\center
\includegraphics[scale=2.3]{errata.8}
}

\vspace*{5pt}

\parbox{6in}{
\center
\includegraphics{errata.10}
\caption{Paths generated by the  and  algorithms when starting from 111.}\label{fig:faster}
}
\end{center}
\end{figure}

Consider the path in the computation tree that first picks  and then picks . Note that  is part of the original tree , and we would therefore expect this choice to be made with higher probability for  than for , which is exactly the case.
In order for the call

to realize the path, the permutation  must satisfy , , and . There are 150 permutations that satisfy these restrictions, which means that the path is generated with probability , and the edge is picked with probability , given that the call is made. The permutations can be obtained by adding  and  to the following five permutations:

The same choice is again made with probability  by . This time, the resulting path, \textbf{Path 2}, is shorter than the path, \textbf{Path 3}, generated when  is picked. Thus the expected running time is better for  than for . To be exact, the expected number of pivots made by  is

and
the expected number of pivots made by  is





\section*{Acknowledgement}

We would like to thank Bernd G{\"a}rtner, G\"{u}nter Rote and Tibor Szab\'{o} for various discussions on the \RandomFacet\ algorithm and its variants that helped us realize that the expected number of pivoting steps performed by \RandomFacet\ and  are \emph{not} the same.

\bibliographystyle{abbrv}
\bibliography{../../random_facet_shortest_paths}

\end{document}
