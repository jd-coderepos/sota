\section{Experiments} \label{sec:experiments}

    In this section, we thoroughly evaluate the most recent few-shot transductive methods using our imbalanced setting. Except for SIB \cite{hu2020empirical} and LR-ICI \cite{wang2020instance} all the methods have been reproduced in our common framework. All the experiments have been executed on a single GTX 1080 Ti GPU. 

    \subparagraph{Datasets}

        We use three standard benchmarks for few-shot classification: \textit{mini}-Imagenet \cite{ILSVRC15}, \textit{tiered}-Imagenet \cite{ren18fewshotssl} and \textit{Caltech-UCSD Birds 200} (\textit{CUB}) \cite{wah2011caltech}. The \textit{mini}-Imagenet benchmark is a subset of the ILSVRC-12 dataset \cite{ILSVRC15}, composed of 60,000 color images of size 84 x 84 pixels \cite{Vinyals2016MatchingNF}. It includes 100 classes, each having 600 images. In all experiments, we used the standard split of 64 base-training, 16 validation and 20 test classes \cite{Ravi2017OptimizationAA,wang2019simpleshot}. The \textit{tiered}-Imagenet benchmark is a larger subset of ILSVRC-12, with 608 classes and 779,165 color images of size  pixels. We used a standard split of 351 base-training, 97 validation and 160 test classes. The \textit{Caltech-UCSD Birds 200} (\textit{CUB}) benchmark also contains images of size  pixels, with 200 classes. For CUB, we used a standard split of 100 base-training, 50 validation and 50 test classes, as in \cite{chen2018a}. It is important to note that for all the splits and data-sets, the base-training, validation and test classes are all different.


    \subparagraph{Task sampling}
    \label{par:task_sampling}

        We evaluate all the methods in the 1-shot 5-way, 5-shot 5-way, 10-shot 5-way and 20-shot 5-way scenarios, with the classes of the query sets randomly distributed following Dirichlet's distribution, as described in section \ref{sec:imbalance_query_set}. Note that the total amount of query samples  remains fixed to 75. All the methods are evaluated by the average accuracy over 10,000 tasks, following \cite{wang2019simpleshot}. We used different Dirichlet's concentration parameter  for validation and testing. The validation-task generation is based on a random sampling within the simplex (i.e Dirichlet with ). Testing-task generation uses  to reflect the fact that extremely imbalanced tasks (i.e., only one class is present in the task) are unlikely to happen in practical scenarios; see \autoref{fig:dirichlet_density} for visualization.

    \subparagraph{Hyper-parameters}

        Unless identified as directly linked to a class-balance bias, all the hyper-parameters are kept similar to the ones prescribed in the original papers of the reproduced methods. For instance, the marginal entropy in TIM \cite{malik2020Tim} was identified in \autoref{sec:methods_biases} as a penalty that encourages class balance. Therefore, the weight controlling the relative importance of this term is tuned. For all methods, hyper-parameter tuning is performed on the validation set of each dataset, using the validation sampling described in the previous paragraph.


    \subparagraph{Base-training procedure}

        All non-episodic methods use the same feature extractors, which are trained using the same procedure as in \cite{malik2020Tim,Laplacian}, via a standard cross-entropy minimization on the base classes with label smoothing. 
The feature extractors are trained for 90 epochs, using a learning rate initialized to 0.1 and divided by 10 at epochs 45 and 66. We use a batch size of 256 for ResNet-18 and of 128 for WRN28-10. 
During training, color jitter, random croping and random horizontal flipping augmentations are applied. For episodic/meta-learning methods, given that each requires a specific training, we use the pre-trained models provided with the GitHub repository of each method.

    \begin{table}
\caption{Comparisons of state-of-the-art methods in our realistic setting on \textit{mini}-ImageNet, \textit{tiered}-ImageNet and CUB. Query sets are sampled following a Dirichlet distribution with . Accuracy is averaged over 10,000 tasks. A red arrow ({\color{red}}) indicates a performance drop between the artificially-balanced setting and our testing procedure, and a blue arrow ({\color{blue}}) an improvement. Arrows are not displayed for the inductive methods as, for these, there is no significant change
        in performance between both settings (expected). `--' signifies the result was computationally intractable to obtain.}
        \label{tab:mini}
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tabular}{llcccccc}
                \toprule
                 & & & \multicolumn{4}{c}{\textbf{\textit{mini}-ImageNet}} \\
                \cmidrule(lr){4-7}
                & \textbf{Method} & \textbf{Network}&\textbf{1-shot}&\textbf{5-shot}&\textbf{10-shot}&\textbf{20-shot}\\


\midrule
                \multirow{4}{*}{\rotatebox{90}{Induct.}} & Protonet (\textsc{\scriptsize NeurIPS'17 \cite{snell2017prototypical}}) & \multirow{4}{*}{RN-18} & 53.4 & 74.2  & 79.2 & 82.4 \\
                & Baseline (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) &   & 56.0 & 78.9 & 83.2 & 85.9 \\
                & Baseline++ (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) &   & 60.4 & 79.7 & 83.8 & 86.3 \\
                & Simpleshot (\textsc{\scriptsize arXiv \cite{wang2019simpleshot}}) &   & 63.0 & 80.1 & 84.0 & 86.1 \\
                \hline
\multirow{9}{*}{\rotatebox{90}{Transduct.}}& MAML (\textsc{\scriptsize ICML'17 \cite{Finn2017ModelAgnosticMF}}) & \multirow{9}{*}{RN-18} & 47.6 ({\color{red}  3.8}) & 64.5 ({\color{red}  5.0}) & 66.2 ({\color{red}  5.7}) & 67.2 ({\color{red}  3.6}) \\
                 & Versa (\textsc{\scriptsize ICLR'19 \cite{gordon2018metalearning}}) & & 47.8 ({\color{red}  2.2}) & 61.9 ({\color{red}  3.7}) & 65.6 ({\color{red}  3.6}) & 67.3 ({\color{red}  4.0}) \\
                 & Entropy-min (\textsc{\scriptsize ICLR'20 \cite{Dhillon2020A}}) &  & 58.5 ({\color{red}  5.1}) & 74.8 ({\color{red} 7.3}) & 77.2 ({\color{red} 8.0}) &  79.3 ({\color{red} 7.9}) \\
                & LR+ICI (\textsc{\scriptsize CVPR'2020 \cite{wang2020instance}}) &  & 58.7 ({\color{red} 8.1}) & 73.5 ({\color{red} 5.7}) & 78.4 ({\color{red} 2.7}) & 82.1 ({\color{red} 1.7})\\
                & PT-MAP (\textsc{\scriptsize arXiv \cite{pt_map}}) &  & 60.1 ({\color{red} 16.8}) & 67.1 ({\color{red} 18.2}) & 68.8 ({\color{red} 18.0}) & 70.4 ({\color{red} 17.4}) \\
                
                & LaplacianShot (\textsc{\scriptsize ICML'20 \cite{Laplacian}})  &  & 65.4 ({\color{red} 4.7}) & 81.6 ({\color{red} 0.5}) & 84.1 ({\color{red} 0.2}) & 86.0 ({\color{blue} 0.5})\\
                & BD-CSPN (\textsc{\scriptsize ECCV'20 \cite{liu2019prototype}})&  & 67.0 ({\color{red} 2.4}) & 80.2({\color{red} 1.8}) & 82.9 ({\color{red} 1.4}) & 84.6 ({\color{red} 1.1})\\
& TIM (\textsc{\scriptsize NeurIPS'20 \cite{malik2020Tim}})  &  & 67.3 ({\color{red} 4.5}) & 79.8 ({\color{red} 4.1}) & 82.3 ({\color{red} 3.8}) & 84.2 ({\color{red} 3.7}) \\
                \rowcolor{Gray} \cellcolor{white} & -TIM (ours) &  & \textbf{67.4} & \textbf{82.5} & \textbf{85.9} & \textbf{87.9} \\
                \midrule
                \multirow{3}{*}{\rotatebox{90}{Induct.}} & Baseline (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) & \multirow{3}{*}{WRN} & 62.2 & 81.9 & 85.5 & 87.9 \\
                & Baseline++ (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) &  & 64.5 & 82.1 & 85.7 & 87.9 \\
                & Simpleshot (\textsc{\scriptsize arXiv \cite{wang2019simpleshot}}) &  & 66.2 & 82.4 & 85.6 & 87.4 \\
                \hline
               
               \multirow{7}{*}{\rotatebox{90}{Transduct.}} & Entropy-min (\textsc{\scriptsize ICLR'20 \cite{Dhillon2020A}}) & \multirow{7}{*}{WRN} &  60.4 ({\color{red} 5.7}) & 76.2 ({\color{red} 8.0}) & -- & --  \\
                & PT-MAP (\textsc{\scriptsize arXiv \cite{pt_map}}) & & 60.6 ({\color{red} 18.3}) & 66.8 ({\color{red} 19.8}) & 68.5 ({\color{red} 19.3}) & 69.9 ({\color{red} 19.0}) \\
                
                & SIB (\textsc{\scriptsize ICLR'20 \cite{hu2020empirical}})&  & 64.7 ({\color{red} 5.3}) & 72.5 ({\color{red} 6.7}) & 73.6 ({\color{red} 8.4}) & 74.2 ({\color{red} 8.7})\\
                
                & LaplacianShot (\textsc{\scriptsize ICML'20 \cite{Laplacian}}) &  & 68.1 ({\color{red} 4.8}) & 83.2 ({\color{red} 0.6}) & 85.9 ({\color{blue} 0.4}) & 87.2 ({\color{blue} 0.6})\\
& TIM (\textsc{\scriptsize NeurIPS'20 \cite{malik2020Tim}}) &  & 69.8 ({\color{red} 4.8}) &  81.6 ({\color{red} 4.3}) & 84.2 ({\color{red} 3.9}) & 85.9 ({\color{red} 3.7})\\
                & BD-CSPN (\textsc{\scriptsize ECCV'20 \cite{liu2019prototype}}) &  & \textbf{70.4} ({\color{red} 2.1}) & 82.3({\color{red} 1.4}) & 84.5 ({\color{red} 1.4})& 85.7 ({\color{red} 1.1}) \\
\rowcolor{Gray} \cellcolor{white} & -TIM (ours)&  & 69.8 & \textbf{84.8} & \textbf{87.9} & \textbf{89.7}  \\
                \midrule



                 & & & \multicolumn{4}{c}{\textbf{\textit{tiered}-ImageNet}} \\
                 \cmidrule(lr){4-7}
                 & & &\textbf{1-shot}&\textbf{5-shot}&\textbf{10-shot}&\textbf{20-shot}\\
                \midrule
                \multirow{3}{*}{\rotatebox{90}{Induct.}} & Baseline (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) & \multirow{3}{*}{RN-18} & 63.5 & 83.8 & 87.3 & 89.0 \\
                & Baseline++ (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) &  & 68.0 & 84.2 & 87.4 & 89.2 \\
& Simpleshot (\textsc{\scriptsize arXiv \cite{wang2019simpleshot}}) &  & 69.6 & 84.7 & 87.5 & 89.1 \\
                \hline
\multirow{7}{*}{\rotatebox{90}{Transduct.}}& Entropy-min (\textsc{\scriptsize ICLR'20 \cite{Dhillon2020A}}) & \multirow{7}{*}{RN-18} & 61.2 ({\color{red} 5.8}) & 75.5 ({\color{red} 7.6}) & 78.0 ({\color{red} 7.9}) &  79.8 ({\color{red} 7.9}) \\
                & PT-MAP (\textsc{\scriptsize arXiv \cite{pt_map}}) &  & 64.1 ({\color{red} 18.8}) & 70.0 ({\color{red} 18.8}) & 71.9 ({\color{red} 17.8}) & 73.4 ({\color{red} 17.1}) \\
& LaplacianShot (\textsc{\scriptsize ICML'20 \cite{Laplacian}})  &  & 72.3 ({\color{red} 4.8}) & 85.7 ({\color{red} 0.5}) & 87.9 ({\color{red} 0.1}) & 89.0 ({\color{blue} 0.3}) \\
                & BD-CSPN (\textsc{\scriptsize ECCV'20 \cite{liu2019prototype}})&  & 74.1 ({\color{red} 2.2}) & 84.8 ({\color{red} 1.4}) & 86.7 ({\color{red} 1.1}) & 87.9 ({\color{red} 0.8})\\


& TIM (\textsc{\scriptsize NeurIPS'20 \cite{malik2020Tim}})  &  & 74.1 ({\color{red} 4.5}) & 84.1 ({\color{red} 3.6}) & 86.0 ({\color{red} 3.3}) & 87.4 ({\color{red} 3.1})\\
                & LR+ICI (\textsc{\scriptsize CVPR'20 \cite{wang2020instance}}) &  & \textbf{74.6} ({\color{red} 6.2}) & 85.1 ({\color{red} 2.8}) & 88.0 ({\color{red} 2.1}) & 90.2 ({\color{red} 1.2})\\
                \rowcolor{Gray} \cellcolor{white} & -TIM (ours) &  & 74.4 & \textbf{86.6} & \textbf{89.3} & \textbf{90.9} \\
                \midrule
                \multirow{3}{*}{\rotatebox{90}{Induct.}} & Baseline (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) & \multirow{3}{*}{WRN} & 64.6 & 84.9 & 88.2 & 89.9 \\
                & Baseline++ (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) &  & 68.7 & 85.4 & 88.4 & 90.1 \\
                & Simpleshot (\textsc{\scriptsize arXiv \cite{wang2019simpleshot}}) &  & 70.7 & 85.9 & 88.7 & 90.1 \\
                \hline
                \multirow{6}{*}{\rotatebox{90}{Transduct.}} & Entropy-min (\textsc{\scriptsize ICLR'20 \cite{Dhillon2020A}}) & \multirow{7}{*}{WRN} & 62.9 ({\color{red} 6.0}) & 77.3 ({\color{red} 7.5}) & -- & -- \\
                & PT-MAP (\textsc{\scriptsize arXiv \cite{pt_map}}) &  & 65.1 ({\color{red} 19.5}) & 71.0 ({\color{red} 19.0}) & 72.5 ({\color{red} 18.3}) & 74.0 ({\color{red} 17.7}) \\
                & LaplacianShot (\textsc{\scriptsize ICML'20 \cite{Laplacian}}) &  & 73.5 ({\color{red} 5.3}) & 86.8 ({\color{red} 0.5})& 88.6 ({\color{red} 0.4}) & 89.6 ({\color{red} 0.2})\\
                & BD-CSPN (\textsc{\scriptsize ECCV'20 \cite{liu2019prototype}}) &  & 75.4 ({\color{red} 2.3})& 85.9 ({\color{red} 1.5})& 87.8 ({\color{red} 1.0})& 89.1 ({\color{red} 0.6})\\
& TIM (\textsc{\scriptsize NeurIPS'20 \cite{malik2020Tim}}) &  & 75.8 ({\color{red} 4.5}) & 85.4 ({\color{red} 3.5}) & 87.3 ({\color{red} 3.2}) & 88.7 ({\color{red} 2.9})\\
                \rowcolor{Gray} \cellcolor{white} & -TIM (ours) &  & \textbf{76.0} & \textbf{87.8} & \textbf{90.4} & \textbf{91.9} \\

                \bottomrule
            \end{tabular}
            }
    \end{table}

\begin{table}
\caption{Comparaisons of state-of-the-art methods in our realistic setting on CUB. Query sets are sampled following a Dirichlet distribution with . Accuracy is averaged over 10,000 tasks. A red arrow ({\color{red}}) indicates a performance drop between the artificially-balanced setting and our testing procedure, and a blue arrow ({\color{blue}}) an improvement. Arrows are not displayed for the inductive methods as, for these, there is no significant change
        in performance between both settings (expected). `--' signifies the result was computationally intractable to obtain.}
        \label{tab:cub}
        \centering
        \resizebox{\textwidth}{!}{
            \begin{tabular}{llcccccc}
                \toprule
& & & \multicolumn{4}{c}{\textbf{CUB}} \\
                 \cmidrule(lr){4-7}
                 & & &\textbf{1-shot}&\textbf{5-shot}&\textbf{10-shot}&\textbf{20-shot}\\

                \midrule
                \multirow{3}{*}{\rotatebox{90}{Induct.}} & Baseline (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) & \multirow{3}{*}{RN-18} & 64.6 & 86.9 & 90.6 & 92.7 \\
                & Baseline++ (\textsc{\scriptsize ICLR'19 \cite{chen2018a}}) &  & 69.4 & 87.5 & 91.0 & 93.2 \\
                & Simpleshot (\textsc{\scriptsize arXiv \cite{wang2019simpleshot}}) &  & 70.6 & 87.5 & 90.6 & 92.2 \\
                \hline
                \multirow{6}{*}{\rotatebox{90}{Transduct.}} & PT-MAP (\textsc{\scriptsize arXiv  \cite{pt_map}}) & \multirow{6}{*}{RN-18} & 65.1 ({\color{red} 20.4}) & 71.3 ({\color{red} 20.0}) &  73.0 ({\color{red} 19.2}) & 72.2 ({\color{red} 18.9}) \\
                & Entropy-min (\textsc{\scriptsize ICLR'20 \cite{Dhillon2020A}}) & & 67.5 ({\color{red} 5.3}) & 82.9 ({\color{red} 6.0}) & 85.5 ({\color{red} 5.6}) & 86.8 ({\color{red} 5.7}) \\
                & LaplacianShot (\textsc{\scriptsize ICML'20 \cite{Laplacian}})  &  & 73.7 ({\color{red} 5.2}) & 87.7 ({\color{red} 1.1}) & 89.8 ({\color{red} 0.7}) & 90.6 ({\color{red} 0.5})\\
                & BD-CSPN (\textsc{\scriptsize ECCV'20 \cite{liu2019prototype}})&  & 74.5 ({\color{red} 3.4}) & 87.1 ({\color{red} 1.8}) & 89.3 ({\color{red} 1.3}) & 90.3 ({\color{red} 1.1})\\
& TIM (\textsc{\scriptsize NeurIPS'20 \cite{malik2020Tim}})  &  & 74.8 ({\color{red} 5.5}) & 86.9 ({\color{red} 3.6}) & 89.5 ({\color{red} 2.9}) & 91.7 ({\color{red} 2.8}) \\
                \rowcolor{Gray} \cellcolor{white} & -TIM (ours) &   & \textbf{75.7}  & \textbf{89.8}  & \textbf{92.3} & \textbf{94.6} \\

                \bottomrule
            \end{tabular}
            }
    \end{table}



    \subsection{Main results}
        
        The main results are reported in \autoref{tab:mini}. As baselines, we also report the performances of state-of-the-art inductive methods that do not use the statistics of the 
        query set at adaptation and are, therefore, unaffected by class imbalance. In the 1-shot scenario, all the transductive methods, without exception, undergo a significant drop in performances as compared to the balanced setting. Even though the best-performing transductive methods still outperforms the inductive ones, we observe that 
        \ul{more than half of the transductive methods evaluated perform overall worse than inductive baselines in our realistic setting}. 
        Such a surprising finding highlights that the standard benchmarks, initially developed for the inductive setting, are not well suited to evaluate transductive methods. 
        In particular, when evaluated with our protocol, the current state-of-the-art holder PT-MAP averages more than  performance drop across datasets and backbones, Entropy-Min 
        around , and TIM around . Our proposed -TIM method outperforms transductive methods across almost all task formats, datasets and backbones, and is the only method that consistently inductive baselines in fair setting. While stronger inductive baselines have been proposed in the literature \cite{zhang2020deepemd}, we show in the supplementary material that -TIM keeps a consistent relative improvement when evaluated under the same setting.

    \subsection{Ablation studies}

        \subparagraph{In-depth comparison of TIM and -TIM }
While not included in the main \autoref{tab:mini}, keeping the same hyper-parameters for TIM as prescribed in the original paper \cite{malik2020Tim} would result in a 
        drastic drop of about  across the benchmarks. As briefly mentioned in \autoref{sec:methods_biases} and implemented for tuning \cite{malik2020Tim} in \autoref{tab:mini}, adjusting marginal-entropy weight  in Eq. \eqref{eq:tim_objective} strongly helps in imbalanced scenarios, reducing the drop from  to only . However, we argue that such a strategy is sub-optimal in comparison to using -divergences, where the only hyper-parameter controlling the flexibility of the marginal-distribution term becomes . 
        First, as seen from \autoref{tab:mini}, our -TIM achieves consistently better performances with the same budget of hyper-parameter optimization as the standard TIM.
        In fact, in higher-shots scenarios (5 or higher), the performances of -TIM are substantially better that the standard mutual information (\textit{i.e.} TIM). Even more crucially, we
        show in \autoref{fig:param_tuning} that -TIM does not fail drastically when  is chosen sub-optimally, as opposed to the case of weighting parameter  for the TIM formulation. We argue that such a robustness makes of -divergences a particularly interesting choice for more practical applications, where such a tuning might
        be intractable. Our results points to the high potential of -divergences as loss functions for leveraging unlabelled data, beyond the few-shot scenario, e.g., in semi-supervised or unsupervised domain adaptation problems.
        
        \begin{figure}[h]
            \centering
                \includegraphics[width=\textwidth]{figures/mini_tuning.pdf} \\
                \includegraphics[width=\textwidth]{figures/cub_tuning.pdf} \\
            \caption{Validation and Test accuracy versus  for TIM \cite{malik2020Tim} and  for our proposed -TIM, using our protocol. 
            Results are obtained with a RN-18. 
Best viewed in color.}
            \label{fig:param_tuning}
        \end{figure}

        \subparagraph{Varying imbalance severity}

            While our main experiments used a fixed value , we wonder whether our conclusions generalize to different levels of imbalance. Controlling for Dirichlet's parameter  naturally allows us to vary the imbalance severity. In \autoref{fig:acc_vs_a}, we display the results obtained by varying , while keeping the same hyper-parameters obtained through our validation protocol. Generally, most methods follow the expected trend: as  decreases and tasks become more severely imbalanced, performances decrease, with sharpest losses for TIM \cite{malik2020Tim} and PT-MAP \cite{pt_map}. In fact, past a certain imbalance severity, the inductive baseline in  \cite{wang2019simpleshot} becomes more competitive than most transductive methods. Interestingly, both LaplacianShot \cite{Laplacian} and our proposed -TIM are able to cope with extreme imbalance, while still conserving good performances on balanced tasks.
         \begin{figure}[h]
                \centering
                \includegraphics[width=0.8\textwidth]{figures/acc_vs_a.pdf} \\
                \caption{5-shot test accuracy of transductive methods versus imbalance level (lower  corresponds to more severe imbalance, as depicted in \autoref{fig:dirichlet_density}). }
                \label{fig:acc_vs_a}
            \end{figure}
        \subparagraph{On the use of transductive BN} In the case of imbalanced query sets, we note that transductive batch normalization (e.g as done in the popular MAML \cite{finn2018probabilistic}) hurts the performances. This aligns with recent observations from the concurrent work in \cite{post_hoc}, where a shift in the marginal label distribution is clearly identified as a failure case of statistic alignment via  batch normalization. 
        
%
