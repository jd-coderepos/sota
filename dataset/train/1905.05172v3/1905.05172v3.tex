\documentclass[10pt,twocolumn, letterpaper]{article} 

\makeatletter
\makeatother

\newcommand{\final}{0}
\newcommand{\forReview}{1}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{xifthen}
\usepackage{caption}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\hyphenpenalty=5000 \usepackage{color}
\usepackage{ifthen}
\usepackage{float}
\usepackage{alltt}
\usepackage{newlfont} \usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{comment}
\usepackage{gensymb}
\usepackage{pifont}
\usepackage{xcolor}


	
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}






\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{microtype}
\usepackage{blindtext} 
\newcommand{\pcline }{\rule{1in}{0.0in}    }  \newcommand{\pctab  }{\hspace{0.10in}      }  \newcommand{\pcbigtab  }{\hspace{1.75in}    }  \newcommand{\pcasgn }{\mbox{}  }  \newcommand{\pcomment}[1]{\mbox{// {\it #1}}}  \newcommand{\pckeyword}[1]{\mbox{{\bf #1}}}  \newcommand{\pcspace}{\hspace{0.10in}}

\newcommand{\cmark}{\textcolor{green!80!black}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}

\newcommand{\pcode}[1]{
    \footnotesize
    \vspace{-0.1in}
    \begin{minipage}{100in} \begin{tabbing} \hspace{0.10in} \= \pctab \= \pctab \= \pctab \= \pcbigtab \= \\
        #1
    \end{tabbing}
    \end{minipage}
}


\newcommand{\tabtextsize}{\small}
\newcommand{\figtext}[1]{{\footnotesize #1}}
\newcommand{\Caption}[2]{\caption[#1]{{\footnotesize #1} {\footnotesize #2}}}


\newcommand{\invisible}[1]{{\color{white} #1}}
\newcommand{\emacsquote}[1]{{``#1''}}

\newcommand{\revision}[1]{{\color{blue} #1}}

\definecolor{DeltaColor}{rgb}{0.039,0.73,0.71}
\definecolor{SetaColor}{rgb}{0.867, 0.0235, 0.376}
\definecolor{SigmaColor}{rgb}{0.98,0.45,0.0}
\definecolor{HaoColor}{rgb}{0.8,0,0}
\definecolor{AlphaColor}{rgb}{0,0,0.8}
\definecolor{BetaColor}{rgb}{0.8,0,0.8}
\definecolor{GammaColor}{rgb}{0.5,0,0.7}
\definecolor{EpsilonColor}{rgb}{0.353,0.725,0.906}
\definecolor{TauColor}{rgb}{0.423,0.235,0.192}
\newcommand{\shunsuke}[1]{{\color{AlphaColor} Shunsuke: #1 }}
\newcommand{\ryota}[1]{{\color{SetaColor} Ryota: #1 }}
\newcommand{\zeng}[1]{{\color{SigmaColor} Zeng: #1 }}
\newcommand{\hao}[1]{{\color{GammaColor} Hao: #1 }}
\newcommand{\angjoo}[1]{{\color{DeltaColor} Angjoo: #1}}

\newcommand{\warning}[1]{{\it\color{red} #1}}
\newcommand{\note}[1]{{\it\color{blue} #1}}
\newcommand{\nothing}[1]{}
\newcommand{\passthrough}[1]{#1}
\definecolor{AudioColor}{rgb}{0.56,0.34,0.62}
\newcommand{\audio}[1]{{\color{AudioColor} Audio: #1}}

\definecolor{DeadlineColor}{rgb}{0.9,0.4,0} \newcommand{\deadline}[1]{{\bf\color{DeadlineColor} ETA: #1}}

\newcommand{\psdraftboxDefault}{\psnodraftbox}

\definecolor{figred}{rgb}{1,0,0}
\definecolor{figgreen}{rgb}{0,0.6,0}
\definecolor{figblue}{rgb}{0,0,1}
\definecolor{figpink}{rgb}{1,0.63,0.63}



\ifthenelse{\equal{\final}{1}}
{
\renewcommand{\l}[1]{}
\renewcommand{\shunsuke}[1]{}
\renewcommand{\zeng}[1]{}
\renewcommand{\ryota}[1]{}
\renewcommand{\hao}[1]{}
\renewcommand{\angjoo}[1]{}
\renewcommand{\warning}[1]{}
\renewcommand{\note}[1]{}
\renewcommand{\deadline}[1]{}
}
{}

\newcommand{\pseudocode}{Pseudocode}
\newcounter{pccount}
\setcounter{pccount}{1}
\newcommand{\pccaption}[1]{\textbf{Pseudocode \thepccount \hspace{0.5em}} #1 \addtocounter{pccount}{ 1}}

\newcommand{\algoname}[1]{\textnormal{\textsc{#1}}}

\floatstyle{plain}


\newcommand{\filename}[1]{\url{#1}}
\newcommand{\foldername}[1]{\url{#1}}
\newcommand{\bloginput}[1]{\input{blog/#1}}
\newcommand{\oldinput}[1]{\input{old/#1}}

\DeclareMathOperator*{\argmin}{argmin}         

\hyphenpenalty=1000 

\newcommand{\inputsection}[1]{\input{sections/#1}}
\newcommand{\inputsectionsupp}[1]{\input{sections_supp/#1}}
\newcommand{\inputfigure}[1]{\input{figs/#1}}
\newcommand{\inputfiguresupp}[1]{\input{figs_supp/#1}}
\newcommand{\inputtable}[1]{\input{tables/#1}}
 \newcommand{\loss}{\mathcal{L}}
\newcommand{\bceloss}{\loss_{BCE}}
\newcommand{\adversarialloss}{\loss_{adv}}
\newcommand{\silhouette}{\mathbf{S}}
\newcommand{\silnet}{\mathcal{G}_{s}}
\newcommand{\pose}{\mathbf{P}}
\newcommand{\sourcepose}{\pose_{s}}
\newcommand{\targetpose}{\pose_{t}}
\newcommand{\sourcesil}{\silhouette_{s}}
\newcommand{\targetsil}{\silhouette_{t}}

\newcommand{\vh}{\mathcal{H}}
\newcommand{\view}{\mathcal{V}}
\newcommand{\bin}{\mathcal{B}}
\newcommand{\prob}{P}

\newcommand{\dvhull}{\mathcal{G}_{v}}

\newcommand{\fronttoback}{\mathcal{G}_{t}}
\newcommand{\featurematchloss}{\loss_{FM}}
\newcommand{\vggloss}{\loss_{VGG}}
\newcommand{\inputimage}{\mathbf{I}}
\newcommand{\frontimage}{\inputimage_{f}}
\newcommand{\realbackimage}{\inputimage_{b}}
\newcommand{\frontsil}{\silhouette_{f}}
\newcommand{\fakebackimage}{\hat{\inputimage}_{b}}

\newcommand{\normal}{\mathbf{n}}
\newcommand{\camera}{\mathbf{c}}

\newcommand{\volloss}{\loss_{vol}}
\newcommand{\pfloss}{\loss_{pf}}
\newcommand{\psloss}{\loss_{ps}} \usepackage{iccv_rebuttal}

\def\iccvPaperID{3540} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\graphicspath{
{figures}
{images}
}

\begin{document}
\setlength{\parindent}{0cm}



\title{\large{Pixel-Aligned Implicit Function for\\ High-Resolution Clothed Human Digitization -- Authors' Rebuttal}}
\maketitle


\newcommand{\QandA}[2] {

\textit{Q:~#1}

{\textit{A:}~#2}

}



\newcommand{\cShun}[1]{{\color{green}{#1}}}
\newcommand{\cRyo}[1]{{\color{blue}{#1}}}
\newcommand\todo[1]{\textcolor{red}{#1}}

We thank the reviewers for their comments and for appreciating the quality of our results. To summarize, we present a fully-convolutional implicit functions that can digitize high-resolution clothed humans including textures in 3D from a single input photograph. Our method also supports multiple images as input, and can faithfully predict complete shapes of dresses and skirts without any template.






\noindent\textbf{R2: Technical novelty.}

Despite its simplicity, 1) our representation has never been explored before; 2) we believe this is the strength and an elegant aspect of our approach; and 3) there are crucial algorithmic and implementation details/insights in order to achieve the presented results.
In particular, the regression from global feature [10, 34, 38] and fully convolutional network makes a significant quantitative and qualitative difference in terms of spatial alignment and preserving details as shown in Fig.~5 and Table.~1, where we compare against the global implicit function approach (IM-GAN). As R3 notes, the "combination is novel", and even if this combination may seem simple in hindsight, it may not be obvious. We also demonstrate that the impact is significant as shown in our results. We emphasize, that the novelty aspect of our technique is somewhat analogous to the concept of fully convolutional networks, which has a clear impact on the problem of semantic segmentation [44].
We believe that the concurrent work of learning implicit fields using global features does not impair our findings when using our pixel-aligned representation. Furthermore, we show how texture can be predicted using the same framework, as well as how to incorporate multi-view information when available, which has not been explored in prior works.

\noindent\textbf{R2, R3: Clarification and framing w.r.t. other works.}
In our final paper, we will make the differences between the related approaches clear. As suggested by R2, we will add more discussions with template based methods [1] and place the paper in context in the introduction. Here we expand on key differences with recent works:

The proposed approach addresses a fundamental limitation of [24]. Since our learned features are aware of its 3D spatial position, we can infer an entire 3D model from a single input image. This is not possible using the approach of [24]. Furthermore, we show that our method not only outperforms existing single/multi-view methods, but even for [24], we achieve better results, both, qualitatively and quantitatively (Fig.~5, Fig.~7, Table.~1 and Table.~2).

Moreover, we want to emphasize that texture inference on 3D surface with arbitrary topology has not been addressed previously. We will incorporate the clarification of the technical novelty in the introduction and result sections. 

\noindent\textbf{R2, R3: Generality and expressiveness of PIFu.}
Since our primary objective was to model the texture and shape variations of clothed humans, we leave the handling of complex poses to future work. Note that in previous works [1-3] that recover clothing with a template model also demonstrate this on simple poses. Regarding general objects, note that they are preliminary results that we included for completeness and to provide a more comprehensive evaluation of our method. We do believe that these insights will encourage further explorations in follow-up works.

\noindent\textbf{R2, R3: Implementation detail.}
We provide all the technical details including network architectures and training procedure in the supplemental materials, and will also release our code to the public. As suggested, we will shorten the main text and incorporate these in the paper.

\noindent\textbf{R2: Occluded limbs and profile views.}

\begin{wrapfigure}{r}[0mm]{0.2\textwidth}
	\vspace*{-7mm}
	\begin{center}
        \hspace*{-6.2mm}
		\includegraphics[width=0.25\textwidth]{figures/rebuttal_view.pdf}
	\end{center}
	\vspace*{-6.8mm}
\end{wrapfigure}
Since the fully convolutional pixel-aligned features incorporate global context as well as local feature, PIFu can recover globally plausible as well as detailed clothed humans, even if the limbs are partially occluded or images are from profile views as shown on the right (3D reconstruction visualized from a different view).



\noindent\textbf{R2: Comparison with a template based method}

\begin{wraptable}[3]{r}[10mm]{52mm}
\vspace*{-4mm}
\hspace*{-8.2mm}
\resizebox{50mm}{!}{
\begin{tabular}{l|lll}
\hline
           &  \multicolumn{3}{c}{Buff} \\
Methods  & Normal    & P2S           & Chamfer \\ \hline
Template, video [1] &  0.127      & 0.820        & 0.795  \\
Ours (3 views)  & \textbf{0.107}      & \textbf{0.665}        & \textbf{0.641} \\
\hline
\end{tabular}
}
\label{tab:multiview}
\vspace*{-6.8mm}
\end{wraptable}
We compare our approach with a template based method [1] that takes a dense 360 degrees view video as an input on Buff. Just from 3 views we outperform the template based method that takes video. We will add more discussion in the next revision.





 
\noindent\textbf{R2, R3: Other suggestions.}
Thank you for the valuable suggestions. We will clarify the requirements in the introduction, discuss how to handle scale (we use a fixed scale for all evaluations incl. baseline) and add units to the results. 



\noindent\textbf{R3: Similarity of training and test subjects.}
We ensure that shape and texture of the clothing in training and test subjects have little overlap. Since we demonstrate a wide range of successful reconstruction from Internet photos, it is evident that our model is not overfitting to particular garment types in the training data. We also note that the previous methods for comparison are trained with the same training dataset for fair comparison. Only SiCloPe and BodyNet are used as off-the-shelf solutions for comparison.





























\end{document}
