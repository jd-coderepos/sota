\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{biblatex}
\usepackage{csvsimple}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{hhline,booktabs}
\usepackage{subfig}
\usepackage{microtype}
\usepackage[font=small]{caption}

\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\TODO}[1]{{\color{red}\textbf{TODO #1}}}
\newcommand{\e}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}., }
\newcommand{\vs}{\textit{vs}.\ }
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\frenchspacing \usepackage{pdfpages}

\usepackage{times}
\usepackage{microtype}

\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}




\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\addbibresource{bibliography.bib}

\newcommand{\ASComment}[1]{{\color{red}AS: #1}}
\newcommand{\MEComment}[1]{{\color{blue}ME: #1}}

\begin{document}

\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{570}  

\title{Quantization Guided JPEG Artifact Correction}

\author{Max Ehrlich \\ \texttt{maxehr@umiacs.umd.edu} \and
    Ser-Nam Lim \\ \texttt{sernamlim@fb.com} \and
    Larry Davis \\ \texttt{lsd@umiacs.umd.edu} \and
    Abhinav Shrivastava \\ \texttt{abhinav@cs.umd.edu} 
}


\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\author{Max Ehrlich\inst{1,2} \and
Larry Davis\inst{1} \and
Ser-Nam Lim\inst{2} \and
Abhinav Shrivastava\inst{1}}
\authorrunning{M. Ehrlich et al.}
\institute{University of Maryland, College Park, MD 20742, USA \\
\email{\{maxehr,lsd\}@umiacs.umd.edu}, \email{abhinav@cs.umd.edu} \and
Facebook AI, New York, NY 10003, USA \\
\email{sernamlim@fb.com}}

\maketitle

\begin{abstract}   
The JPEG image compression algorithm is the most popular method of image compression because of it’s ability for large compression ratios. However, to achieve such high compression, information is lost. For aggressive quantization settings, this leads to a noticeable reduction in image quality. Artifact correction has been studied in the context of deep neural networks for some time, but the current methods delivering state-of-the-art results require a different model to be trained for each quality setting, greatly limiting their practical application. We solve this problem by creating a novel architecture which is parameterized by the JPEG file’s quantization matrix. This allows our single model to achieve state-of-the-art performance over models trained for specific quality settings.
\dots
\keywords{JPEG, Discrete Cosine Transform, Artifact Correction, Quantization}
\end{abstract} 
\section{Introduction}

The JPEG image compression algorithm~\cite{wallace1992jpeg} is ubiquitous in modern computing. Thanks to 
its high compression ratios, it is extremely popular in bandwidth constrained applications. The JPEG algorithm is a lossy compression algorithm, so by 
using it, some information is lost for a corresponding gain in saved space. This is most noticable for low quality settings 

For highly space-constrained scenarios, it may be desirable to use aggressive compression. Therefore, 
algorithmic restoration of the lost information, referred to as artifact correction, has been well studied both in classical literature and in the context of deep neural networks.

While these methods have enjoyed academic success, their practical application is limited by a single 
architectural defect:  
they train a single model per JPEG quality level. The JPEG quality level is an 
integer between 0 and 100, where 100 indicates very little loss of information and 0 
indicates the maximum loss of information. Not only is this expensive to train and deploy, but the quality setting is not known at inference time (it 
is not stored with the JPEG image~\cite{wallace1992jpeg}) making it impossible to use these models in 
practical applications. Only recently have methods begun considering the ``blind'' restoration scenario \cite{kim2019pseudo, kim2020agarnet} with a single network, with mixed results compared to non-blind methods.

We solve this problem by creating a single model that uses quantization data, which is stored in the JPEG file. Our CNN model processes the image entirely in the DCT~\cite{ahmed1974discrete} domain. While 
previous works have recognized that the DCT domain is less likely to spread quantization errors 
\cite{wang2016d3, zhang2018dmcnn}, DCT domain-based models alone have historically not been successful unless 
combined with pixel domain models (so-called ``dual domain'' models). Inspired by recent methods 
\cite{ehrlich2019deep, deguerre2019fast,dong2015compression,gueguen2018faster}, we formulate fully DCT domain
regression. This allows our model to be parameterized by the quantization matrix, 
an  matrix that directly determines the quantization applied to each DCT coefficient. We develop
a novel method for parameterizing our network called Convolution Filter Manifolds, an 
extension of the Filter Manifold technique~\cite{kang2016crowd}. By adapting our network weights to the input 
quantization matrix, our single network is able to handle a wide range of quality settings. Finally, since JPEG images are stored in
the YCbCr color space, with the Y channel containing more information than the subsampled color channels, we
use the reconstructed Y channel to guide the color channel reconstructions. As in~\cite{zini2019deep}, we observe that using the Y channel in this way achieves good color correction
results. Finally, since regression results for artifact correction are often blurry, as a result of lost
texture information, we fine-tune our model using a GAN loss specifically designed to restore texture. This allows 
us to generate highly realistic reconstructions. See Figure \ref{fig:header} for an overview of the correction flow.
 
To summarize, our contributions are:
\begin{enumerate}
    \item A single model for artifact correction of JPEG images at any quality, parameterized by the 
    quantization matrix, which is state-of-the-art in color JPEG restoration.
    \item A formulation for fully DCT domain image-to-image regression.
    \item Convolutional Filter Manifolds for parameterizing CNNs with spatial side-channel information.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/header_2.pdf}
    \caption{\textbf{Correction process.} Excerpt from ICB RGB 8bit dataset ``hdr.ppm''. Input was compressed at quality 10.}
    \label{fig:header}
\end{figure}
 

\section{Prior Work}

Pointwise 
Shape-Adaptive DCT~\cite{foi2006pointwise} is a standard classical technique which uses thresholded
DCT coefficients reconstruct local estimates of the input signal. Yang~\etal~\cite{yang2000blocking} use a 
lapped transform to approximate the inverse DCT on the quantized coefficients.

More recent techniques use convolutional neural networks~\cite{lecun1990handwritten, sutskever2012imagenet}.
ARCNN~\cite{dong2015compression} is a regression model inspired 
by superresolution techniques; L4/L8~\cite{svoboda2016compression} continues this work. CAS-CNN~\cite{cavigelli2017cas} adds hierarchical skip connections and a 
multi-scale loss function. Liu~\etal~\cite{liu2018multi} use a wavelet-based network 
for general denoising and artifact correction, which is extended by Chen~\etal~\cite{chen2018dpw}. Galteri~\etal~\cite{galteri2017deep} use a GAN formulation to
achieve more visually appealing results. S-Net~\cite{zheng2018s} introduces a scalable architecture that can produce
different quality outputs based on the desired computation complexity. Zhang~\etal~\cite{zhang2020residual} use a dense residual formulation for image enhancement. Tai~\etal 
\cite{tai2017memnet} use persistent memory in their restoration network.

Liu~\etal~\cite{liu2015data} introduce the dual domain idea in the sparse coding setting. Guo and Chao~\cite{guo2016building}
use convolutional autoencoders for both domains. DMCNN~\cite{zhang2018dmcnn} extends this with DCT rectifier to
constrain errors. Zheng~\etal~\cite{zheng2019implicit} target color images and use an implicit DCT layer
to compute DCT domain loss using pixel information. D3~\cite{wang2016d3} extends Liu~\etal~\cite{liu2015data}
by using a feed-forward formulation for parameters which were assumed in~\cite{liu2015data}. Jin \etal \cite{jin2020dual} extend the dual domain concept to separate streams processing low and high frequencies, allowing them to achieve competitive results with a fraction of the parameters. 

The latest works examine the "blind" scenario that we consider here. Zhang~\etal~\cite{zhang2017beyond} formulate general image denoising and apply it to JPEG artifact correction with a single network. DCSC uses convolution features in their sparse coding scheme~\cite{fu2019jpeg} with a single network. Galteri~\etal~\cite{galteri2019deep} extend their GAN work with an ensemble of GANs where each GAN in the ensemble is trained to correct artifacts of a specific quality level. They train an auxiliary network to classify the image into the quality level that it was compressed with. The resulting quality level is used to pick a GAN from the ensemble to use for the final artifact correction. Kim \etal \cite{kim2019pseudo} also use an ensemble method based on quality factor estimation. AGARNET \cite{kim2020agarnet} uses a single network by learning a per-pixel quality factor extending the concept \cite{galteri2019deep} from a single quality factor to a per-pixl map. This allows them to avoid the ensemble method and using a single network with two inputs. 
 
\section{Our Approach}

Our goal is to design a single model capable of JPEG artifact correction at any quality. Towards this, we formulate an architecture that is parameterized by the quantization matrix.

Recall that a JPEG quantization matrix captures the amount of rounding applied to DCT coefficients and is indicative of information lost during compression. A key contribution of our approach is utilizing this quantization matrix directly to guide the restoration process using a fully DCT domain image-to-image regression network. JPEG stores color data in the YCbCr colorspace. The compressed Y channel is much higher quality compared to CbCr channels since human perception is less sensitive to fine color details than to brightness details. Therefore, we follow a staged approach: first restoring artifacts in the Y channel and then using the restored Y channel as guidance to restore the CbCr channels. 

An illustrative overview of our approach is presented in Figure~\ref{fig:overview}. 
Next, we present building blocks utilized in our architecture in \ref{sec:app:bb}, that allow us to parameterize our model using the quantization matrix and operate entirely in the DCT domain.  Our Y channel and color artifact correction networks are described in \ref{sec:app:y} and \ref{sec:app:c} respectively, and finally the training details in \ref{sec:app:train}.


\subsection{Building Blocks}
\label{sec:app:bb}
By creating a single model capable of JPEG artifact correction at any quality, our model solves a significantly harder problem than previous works. To solve it, we parameterize our network using the  quantization matrix available with every JPEG file. We first describe Convolutional Filter Manifolds (CFM), our solution for adaptable convolutional kernels parameterized by the quantization matrix. Since the quantization matrix encodes the amount of rounding per each DCT coefficient, this parameterization is most effective in the DCT domain, a domain where CNNs have previously struggled. Therefore, we also formulate artifact correction as fully DCT domain image-to-image regression and describe critical frequency-relationships-preserving operations.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/overview.pdf}
  \caption{\textbf{Overview.} We first restore the Y channel of the input image, then use the restored Y channel to correct
  the color channels which have much worse input quality.}
  \label{fig:overview}
\end{figure}

\noindent\textbf{Convolutional Filter Manifold (CFM).}
Filter Manifolds~\cite{kang2016crowd} were introduced as a way to parameterize a deep CNN using side-channel scalar data. The method learns a manifold of convolutional kernels, which is a function of a scalar input. The manifold is modeled as a three-layer multilayer perceptron. The input to this network is the scalar side-channel data, and the output vector is reshaped to the shape of the desired convolutional kernel and then convolved with the input feature map for that layer.

Recall that in the JPEG compression algorithm, a quantization matrix is derived from a scalar quality setting to determine the amount of rounding to apply, and therefore the amount of information removed from the original image. This quantization matrix is then stored in the JPEG file to allow for correct scaling of the DCT coefficients at decompression time. This quantization matrix is then a strong signal for the amount of information lost. However, the quantization matrix is an  matrix with spatial structure, applying the Filter Manifold technique to it has the same drawbacks as processing images with multilayer perceptrons,
\eg a large number of parameters and a lack of spatial relationships.

To solve this, we propose an extension to create Convolutional Filter Manifolds (CFM), replacing the multilayer perceptron by a lightweight three-layer CNN. The input to the CNN is our quantization matrix, and the output is reshaped to the desired convolutional kernel shape and convolved with the input feature map as in the Filter Manifold method. For our problem, we follow the network structure in Figure~\ref{fig:cmf} for each CFM layer. However, this is a general technique and can be used with a different architecture when spatially arranged side-channel data is available. 

\begin{figure}[t]
    \begin{minipage}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cmf.pdf}
        \caption{\textbf{Convolutional Filter Manifold}, as used in our network. Note that the convolution with the input
        feature map is done with stride-8.}
        \label{fig:cmf}
    \end{minipage}
    \hspace{0.03\linewidth}
    \begin{minipage}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/coef.pdf}
        \caption{\textbf{Coefficient Rearrangement}. Frequencies are arranged channelwise giving an image with 64 times the number of channels
        at th the size. This can then be convolved with 64 groups per convolution to learn per-frequency filters.}
        \label{fig:coef}
    \end{minipage}
\end{figure}

\noindent\textbf{Coherent Frequency Operations.}
In prior works, DCT information has been used in dual-domain models~\cite{wang2016d3,zhang2018dmcnn}. These models used standard  convolutional kernels with U-Net~\cite{ronneberger2015u} structures to process the coefficients. Although the DCT is a linear map on image pixels~\cite{smith1994fast, ehrlich2019deep}, ablation studies in prior work show that the DCT network alone is not able to surpass even classical artifact correction techniques.

Although the DCT coefficients are arranged in a grid structure of the same shape as the input image, that spatial structure does not have the same meaning as pixels. Image pixels are samples of a continuous signal in two dimensions. DCT coefficients, however, are samples from different, orthogonal functions and the two-dimensional arrangement indexes them. This means that a  convolutional kernel is trying to learn a relationship not between spatially related samples of the same function as it was designed to do, but rather between samples from completely unrelated functions. Moreover, it must maintain
this structure throughout the network to produce a valid DCT as output. This is the root cause of CNN's poor performance on DCT coefficients for image-to-image regression, semantic segmentation, and object detection (Note that this should not affect whole image classification performance as in~\cite{gueguen2018faster,ghosh2016deep}).

A class of recent techniques~\cite{deguerre2019fast,lo2019exploring}, which we call Coherent Frequency Operations for their preservation of frequency relationships, are used as the building block for our regression network. The first layer is an  stride-8 layer~\cite{deguerre2019fast}, which computes a representation for each block (recall that JPEG blocks are non-overlapping  DCT coefficients). This block representation, which is one eighth the size of the input, can then be processed with a standard CNN. 

The next layer is designed to process each frequency in isolation. Since each of the 64 coefficients in an  JPEG block corresponds to a different frequency, the input DCT coefficients are first rearranged so that the coefficients corresponding to different frequencies are stored channelwise (see Figure~\ref{fig:coef}). This gives an input, which is again one eighth the size of the original image, but this time with 64 channels (one for each frequency). This was referred to as Frequency Component Rearrangement in~\cite{lo2019exploring}. We then use convolutions with 64 groups to learn per-frequency convolutional weights. 

Combining these two operations (block representation using  8-stride and frequency component rearrangement) allows us to match state-of-the-art pixel and dual-domain results using only DCT coefficients as input and output.

\subsection{Y Channel Correction Network}
\label{sec:app:y}
Our primary goal is artifact correction of full color images, and we again leverage the JPEG algorithm to do this. JPEG stores color data in the YCbCr colorspace. The color channels, which contribute less to the human visual response, are both subsampled and more heavily quantized. Therefore, we employ a larger network to correct only the Y channel, and a smaller network which uses the restored Y channel to more effectively correct the Cb and Cr color channels. 


\begin{figure}[t]
    \begin{minipage}[t]{0.32\linewidth}
        \centering
        \includegraphics[height=0.9\linewidth]{figures/blocknet_abhinav.pdf}
        \caption{\textbf{BlockNet.} Both the block generator and decoder
        are parameterized by the quanitzation matrix.}
        \label{fig:blocknet}
    \end{minipage}
\hfill
    \begin{minipage}[t]{0.3\linewidth}
        \centering
        \includegraphics[height=\linewidth]{figures/frequencynet_abhinav.pdf}
        \caption{\textbf{FrequencyNet.} Note that the 256 channels in the
        RRDB layer actually compute 4 channels per frequency.}
        \label{fig:frequencynet}
    \end{minipage}
        \hfill
\begin{minipage}[t]{0.32\linewidth}
        \centering
        \includegraphics[height=\linewidth]{figures/fusion_abhinav.pdf}
        \caption{\textbf{Fusion subnetwork.} Outputs from all three subnetworks are fused to produce the final residual.}
        \label{fig:fusion}
    \end{minipage}   
\end{figure}

\noindent\textbf{Subnetworks.}
Utilizing the building blocks developed earlier, our network design proceeds in two phases: block enhancement, which learns a quantization invariant representations for each JPEG block, and frequency enhancement, which tries to match each frequency reconstruction to the regression target. These phases are fused to produce the final residual for restoring the Y channel. We employ two purpose-built subnetworks: the block network (BlockNet) and the frequency network (FrequencyNet). Both of these networks can be thought of as separate image-to-image regression models with a structure inspired by ESRGAN~\cite{wang2018esrgan}, which allows sufficient low-level information to be preserved as well as allowing sufficient gradient flow to train these very deep networks. Following recent techniques \cite{wang2018esrgan}, we remove batch normalization layers. While recent works have largely replaced PReLU~\cite{he2015delving} with LeakyReLU~\cite{maas2013rectifier, wang2018esrgan,galteri2017deep,galteri2019deep}, we find that PReLU activations give much higher accuracy.


\noindent\textbf{BlockNet.} This network processes JPEG blocks to restore the Y channel (refer to Figure~\ref{fig:blocknet}). We use
the  stride-8 coherent frequency operations to create a block representation. Since this layer is computing a block representation from all the
input DCT coefficients, we use a Convolutional Filter Manifold (CFM) for this layer
so that it has access to quantization information. This allows the layer to learn the quantization table entry to DCT coefficient correspondence with the goal to output a quantization-invariant block representation. Since there is a one to one correspondence between the quantization table entry and rounding applied to a DCT coefficient, this motivates our choice to operate entirely in the DCT domain. We then process these quantization-invariant block representations with Residual-in-Residual Dense Blocks (RRDB) from~\cite{wang2018esrgan}. RRDB layers are an extension of the commonly used residual block~\cite{he2016deep} and define several recursive and highly residual layers. Each RRDB has 15 convolution layers, and we use a single RRDB for the block network with 256 channels. The network terminates with another 
 stride-8 CFM, this time transposed, to reverse the block representation back to its original form so that it can be used for later tasks. 


\noindent\textbf{FrequencyNet.} This network, shown in Figure \ref{fig:frequencynet}, processes the individual frequency coefficients using the Frequency Component Rearrangement technique (Figure~\ref{fig:coef}). The architecture of this network is similar to BlockNet. We use a single  convolution to change the number of channels from the 64 input channels to the 256 channels used by the RRDB layer. The single RRDB layers processes feature maps with 256 channels and 64 groups yielding 4 channels per frequency. An output  convolution transforms the 4 channel output to the 64 output channels, and the coefficients are rearranged back into blocks for later tasks.

\noindent\textbf{Final Network.}
The final Y channel artifact correction network is shown in Figure~\ref{fig:ychannel}. We observe that since the FrequencyNet processes frequency coefficients in isolation, if those coefficients were zeroed out by the compression 
process, then it can make no attempt at restoring them (since they are zero valued they would be set to the layer bias). This is common with high frequencies by design, since they have larger quanitzation table entries and they contribute less to the human visual response. We, therefore, lead with the BlockNet to restore high frequencies. We then pass the result to the FrequencyNet, and its result is then processed by a second block network to restore more information. Finally, a three-layer fusion network (see Figure~\ref{fig:fusion} and~\ref{fig:ychannel}) fuses the output of all three subnetworks into a final result. Having all three subnetworks contribute to the final result in this way allows for better gradient flow. The effect of fusion, as well as the three subnetworks, is tested in our ablation study. The fusion output is treated as a residual and added to the input to produce the final corrected coefficients for the Y channel.


\begin{figure}[t]
    \begin{minipage}[t]{0.43\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/ynetwork_abhinav.pdf}
        \caption{\textbf{Y Channel Network.} We include two copies of the BlockNet, one to perform early restoration of
        high frequency coefficients, and one to work on the restored frequencies. All three
        subnetworks contribute to the final result using the fusion subnetwork.}
        \label{fig:ychannel}
    \end{minipage}
\hfill
    \begin{minipage}[t]{0.54\linewidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{figures/colornet_abhinav.pdf}
        \caption{\textbf{Color Channel Network.} Color channels are downsampled, so the block representation is upsampled using a learned upsampling. 
        The Y and color channel block representations are then concatenated to guide the color channel restoration. Cb and Cr channels are processed independently with the
        same network.}
        \label{fig:color}
    \end{minipage}
\end{figure}

\subsection{Color Correction Network}
\label{sec:app:c}
The color channel network (Figure~\ref{fig:color}) processes the Cb and Cr DCT coefficients. Since the color channels are subsampled with respect to the Y channel by half, they incur a much higher loss of information and lose the structural information which is preserved in the Y channel. We first compute the block representation of the downsampled color channel coefficients using a CFM layer, then process them with a single RRDB layer. The block representation is then upsampled using a  stride-2  convolutional layer. We compute the block representation of the restored Y channel, again using a CFM layer. The block representations are concatenated channel-wise and processed using a single RRDB layer before being transformed back into coefficient space using a transposed  stride-8 CFM. By concatenating the Y channel restoration, we give the network structural information that may be completely missing in the color channels. The result of this network is the color channel residual. This process is repeated individually for each color channel with a single network learned on Cb and Cr. The output residual is added to nearest-neighbor upsampled input coefficients to give the final restoration.

\subsection{Training}
\label{sec:app:train}
\noindent\textbf{Objective.} We use two separate objective functions to train, an error loss and a GAN loss. Our error loss is based on prior works which minimize the  error of the result and the target image. We additionally
maximize the Structural Similarity (SSIM)~\cite{wang2004image} of the result since SSIM is generally regarded as a closer metric to human perception than PSNR. This gives our final objective function as 

where  is the network output,  is the target image, and  is a balancing hyperparameter.

A common phenomenon in JPEG artifact correction and superresolution is the production of a blurry or textureless result. 
To correct for this, we fine tune our fully trained regression network with a GAN loss. For this objective, we use the relativistic 
average GAN loss ~\cite{jolicoeur2018relativistic}, we use  error to prevent the image from moving too far away from the regression result, and we use preactivation network-based loss~\cite{wang2018esrgan}. Instead of a perceptual loss that tries to keep the outputs close in ImageNet-trained VGG feature space used in prior works, we use a network trained on the MINC dataset~\cite{bell2015material},
for material classification. This texture loss provided only marginal benefit in ESRGAN \cite{wang2018esrgan} for super-resolution. We find it to be critical in our task for restoring texture to blurred regions, since JPEG compression destroys these fine details. The texture loss is defined as 

where MINC indicates that the output is from layer 5 convolution 3.
The final GAN loss is 
 
with  and  balancing hyperparameters. We note that the texture restored using the GAN model is, in general,
not reflective of the regression target at inference time and actually produces worse numerical results than the regression model despite the images looking more realistic.

\noindent\textbf{Staged Training.} Analogous to our staged restoration, Y channel followed by color channels, we follow a staged training approach. We first train the Y channel correction network using . We then train the color correction network using  keeping the Y channel network weights frozen. Finally, we train the entire network (Y and color correction) with . 

\section{Experiments}

We validate the theoretical discussion in the previous sections with experimental results. We first describe the datasets
we used along with the training procedure we followed. We then show artifact correction results and compare them with previous state-of-the-art methods. Finally, we perform an ablation study.  Please see our supplementary material for further
results and details.

\begin{table}[t]
    \centering
    \caption{\textbf{Color Artifact Correction Results.} PSNR / PSNR-B / SSIM format. Best result in bold, second best underlined. JPEG column gives input error. For ICB, we used the RGB 8bit dataset.}
    \footnotesize    
    \renewcommand{\arraystretch}{1.2}
	\renewcommand{\tabcolsep}{1.2mm}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{@{}l *{7}{c}@{}}
            \toprule
            Dataset & Quality & JPEG & ARCNN\cite{dong2015compression} &  MWCNN \cite{liu2018multi} & IDCN \cite{zheng2019implicit} & DMCNN \cite{zhang2018dmcnn} & Ours \\
            \midrule
            \multirow{3}{*}{Live-1} & \csvreader[late after line=\{data/comparisons/BSDS500_color.csv}{}{\csviffirstrow{}{&}\csvlinetotablerow}
            \midrule
            \multirow{3}{*}{ICB} & \csvreader[late after line=\{data/comparisons/live1.csv}{}{\csviffirstrow{}{&}\csvlinetotablerow}
            \midrule
            \multirow{3}{*}{Classic-5} & \csvreader[late after line=\{data/comparisons//BSDS500.csv}{}{\csviffirstrow{}{&}\csvlinetotablerow}
            \midrule
            \multirow{3}{*}{ICB} & \csvreader[late after line=\{data/fid/fid_live1.csv}{}{\csviffirstrow{}{&}\csvlinetotablerow}
            \midrule
            \multirow{3}{*}{BSDS500} & \csvreader[late after line=\{data/fid/fid_icb.csv}{}{\csviffirstrow{}{&}\csvlinetotablerow}
            \bottomrule
        \end{tabular} }
        \label{tab:fid}
    \end{minipage}
    \hfill
    \begin{minipage}[t][][b]{0.51\linewidth}
        \centering
        \caption{\textbf{Ablation Results.} (refer to Section~\ref{sec:res:ablation} for details).}
        \resizebox{0.75\columnwidth}{!}{
        \begin{tabular}{@{}L{1.8cm} L{1.8cm} *{7}{c}@{}}
            \toprule
            Experiment & Model & PSNR & PSNR-B & SSIM \\
            \midrule
            \multirow{3}{*}{CFM} & \csvreader[late after line=\{data/ablation/bvf.csv}{}{\csviffirstrow{}{&}\csvlinetotablerow}
            \midrule
            \multirow{2}{*}{Fusion} & \csvreader[late after line=\
    \Theta_I = (1 - \alpha)\Theta_R + \alpha\Theta_G

    Y = 2.99R + 0.587B + 0.114G \\
    Cb = 128 - 0.168736R - 0.331264B + 0.5G \nonumber \\
    Cr = 128 + 0.5R - 0.418688B - 0.081312G \nonumber

    D_{i, j} = \frac{1}{4}C(i)C(j)\sum_{x=0}^7\sum_{y=0}^7P_{x,y}\cos\left[\frac{(2x+1)i\pi}{16}\right]\cos\left[\frac{(2y+1)j\pi}{16}\right] \\
    C(u) = \left\{\begin{array}{lr}
            \frac{1}{\sqrt{2}} & u = 0 \\
            1 & \text{otherwise}
        \end{array}\right. \nonumber

    Y'_{i, j} = \text{truncate}\left[\frac{Y_{i, j}}{Q_{Y_{i,j}}}\right] \\
    Cb'_{i, j} = \text{truncate}\left[\frac{Cb_{i, j}}{Q_{C_{i,j}}}\right] \nonumber \\ 
    Cr'_{i, j} = \text{truncate}\left[\frac{Cr_{i, j}}{Q_{C_{i,j}}}\right] \nonumber
    \label{eq:quant}

    Y_{i, j} = Y'_{i, j}Q_{Y_{i,j}} \\
    Cb_{i, j} = Cb'_{i, j}Q_{C_{i,j}} \nonumber \\ 
    Cr_{i, j} = Cr'_{i, j}Q_{C_{i,j}} \nonumber

    P_{x, y} = \frac{1}{4}\sum_{i=0}^7\sum_{j=0}^7C(i)C(j)D_{i,j}\cos\left[\frac{(2x+1)i\pi}{16}\right]\cos\left[\frac{(2y+1)j\pi}{16}\right] \\
    C(u) = \left\{\begin{array}{lr}
            \frac{1}{\sqrt{2}} & u = 0 \\
            1 & \text{otherwise}
        \end{array}\right. \nonumber

R = Y + 1.402(Cr - 128) \\ 
G = Y - 0.344136(Cb - 128) -0.714136(Cr - 128) \nonumber \\
B = Y + 1.772(Cb - 128) \nonumber

and cropped to remove any block padding that was added during compression. The image is now ready for display. 
\end{document}
 
\end{document}
