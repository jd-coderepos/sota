[{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiQ', 'Metric': 'Accuracy', 'Score': '91.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiQ', 'Metric': 'Accuracy', 'Score': '00'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'CheGeKa', 'Metric': 'Accuracy', 'Score': '64.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'CheGeKa', 'Metric': 'Accuracy', 'Score': '00'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'RuOpenBookQA', 'Metric': 'Accuracy', 'Score': '86.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'RuOpenBookQA', 'Metric': 'Accuracy', 'Score': '57.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'RuOpenBookQA', 'Metric': 'Accuracy', 'Score': '57.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'RuOpenBookQA', 'Metric': 'Accuracy', 'Score': '55.5'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics', 'Metric': 'Accuracy', 'Score': '68.6'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics', 'Metric': 'Accuracy', 'Score': '68.3'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics', 'Metric': 'Accuracy', 'Score': '55.5'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics', 'Metric': 'Accuracy', 'Score': '52.9'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics 2', 'Metric': 'Accuracy', 'Score': '67.6'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics 2', 'Metric': 'Accuracy', 'Score': '60.9'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics 2', 'Metric': 'Accuracy', 'Score': '44.9'}}, {'LEADERBOARD': {'Task': 'Ethics', 'Dataset': 'Ethics 2', 'Metric': 'Accuracy', 'Score': '44.1'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'Winograd Automatic', 'Metric': 'Accuracy', 'Score': '87.0'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'Winograd Automatic', 'Metric': 'Accuracy', 'Score': '57.9'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'Winograd Automatic', 'Metric': 'Accuracy', 'Score': '57.2'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'Winograd Automatic', 'Metric': 'Accuracy', 'Score': '55.5'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'RuWorldTree', 'Metric': 'Accuracy', 'Score': '83.7'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'RuWorldTree', 'Metric': 'Accuracy', 'Score': '40.7'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'RuWorldTree', 'Metric': 'Accuracy', 'Score': '38.0'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'RuWorldTree', 'Metric': 'Accuracy', 'Score': '34.0'}}]
