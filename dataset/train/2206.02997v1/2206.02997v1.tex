

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{array}
\setlist[enumerate]{listparindent=\parindent}
\usepackage[pagebackref,breaklinks,colorlinks,linkcolor=red]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\confYear{2022}


\begin{document}

\title{TadML: A fast temporal action detection with Mechanics-MLP}



\author{
{
Bowen Deng\thanks{\* indicates equal contributions. This work was done when Bowen Deng was an intern at Institute of Automation, Chinese Academy of Sciences.Dongchang Liu is the corresponding author.}
\qquad
{Dongchang Liu}
}\\
Institute of Automation, Chinese Academy of Sciences\\
{\tt\small a18608202465@gmail.com}\\
{\tt\small dongchang.liu@ia.ac.cn}\\
}


\maketitle
\begin{abstract}
Temporal action detection is a crucial but challenging task in video understanding.This task is aimed at detecting both the action category and start and end frames for each action instance in a long, untrimmed video.
rather than using RGB datasets,most current models adopt both RGB and flow datasets for TAD.Such methods needed to be converted into optical flow manually, it is difficult to achieve real-time with RGB and optical flow data. At the same time, many models adopt two-tage, which makes the reasoning speed of the model very slow and heavy tuning of locations and sizes corresponding to different anchors.
By comparison,RGB only anchor-free methods is faster, getting rid of
redundant hyper-parameters,gaining few attention.
In this paper, we propose the RGB only anchor-free
temporal localization method, which is both efficient
and effective. Our model includes (i) The model proves that optical flow data is not indispensable in TAD task, and the use of RGB dataset can also achieve very good results (ii) The ML-like model based on Newtonian mechanics can not only achieve high accuracy when using RGB datasets, but also achieve end-to-end rapid reasoning. In order to prove the superiority of our model, our model achieves higher accuracy when using RGB and optical flow.a remarkable margin on THUMOS14 and comparable ones on ActivityNet v1.3. Code is available at \url{https://github.com/TencentYoutuResearch/ActionDetection-AFSD}
\end{abstract}

\section{Introduction}
\label{sec:intro}
\begin{figure}[!]
 \centering
\includegraphics[width=0.8\linewidth]{figer1.jpg}
 \caption{(above) Comparing the performance (average chart) and speed of the latest time action detection model on THUMOS 14. Our model shows advanced performance and very fast speed when using RGB stram.(below) followed by the two-stage method, using the one-stage method of optical flow and only the one-stage method of RGB.}
   \label{fig1}
\end{figure}
Recent developments in mobile communication and the Internet have made videos more accessible and integral parts of people's lives, including in areas such as Human-Computer Interaction, video retrieval, video monitoring and motion analysis, among others,etc.The use of video understanding has thus gained a great deal of attention in academia and industry as a method of effectively handling video data.A crucial aspect of video comprehension is temporal motion localization.In the case of untrimmed videos with multiple action instances and background videos, all actions in the unedited video category,the start and end times must be determined.
Previously, many TAD methods used complicated pipelines.Some early methods Use hand-designed features or local spatio-temporal description operators as video representations for video action classification and detection.Given the rising popularity of deep learning methods in image processing, at present data processing primarily the combination of video RGB data and optical flow data with the deep learning network.The RGB image of the video enables three color channels (red R, green G and blue b) to store pixel information, which contains the shape information of the video.Because the motion recognition of video is closely related to some objects in video, and the shape information is an important information of motion recognition, RGB image can extract the spatial features of video.Optical flow field refers to a two-dimensional instantaneous velocity field composed of all pixels in the image, in which the two-dimensional velocity vector is the projection of the three-dimensional velocity vector of the visible points in the scene on the imaging surface. Therefore, optical flow not only contains the motion information of the observed object, but also contains rich information about the three-dimensional structure of the scene.most of the TAD work uses RGB data and optical flow data fusion. However, in the actual application scenario,it is difficult to convert video into optical flow from end to end, and the conversion requires time, computing and other resources.

In the structure of the network.It should be noted that this task contains two parts,the first being to predict the target's category based on the available information,and the second being to predict its location in video. The difference is that in object detection,the position here refers to the box,and the position in the task refers to the starting point of the action. Therefore, many method this task also learn from the idea of some methods of target detection.It is also divided into single-stage and two-stage methods according to the he network structure.in dual-stage methods methods, suggestions with high recall are first generated.Then they are classified to predict their labels.The reasoning speed is relatively slow, generally can not be done in real time, and the computing power of application equipment is very high.while in single-stage methods, the start and end times of each action and their labels are generated in one step, which improves the efficiency of the model.

According to the structure of anchor, it can be divided into three categories: the action guided method represented by BSN\cite{lin2018bsn} and the anchor base method represented by BMN\cite{lin2019bmn}. They not only have high time complexity (T2) and (c * t) respectively, but also have a lot of super parameters to be tuned, such as the scale and quantity of anchor. Determination of IOU threshold when dividing positive and negative samples, etc. These parameters need to be tuned.Inspired by the great impact of anchor free's model on target detection. Anchor free model can also play a great potential in TAD. This method generates the start and end distance for each time point

In this paper, we adopt a minimalist design and develop a RGB only TAD model based on anchor free,single stage and Mlp-like. The TAD architecture based on Newtonian mechanics mlp-like unit is inspired by the recent success of mlp-like on the backbone of vision.The results of  Our network achieves 53.11\% map with only RGB data nat tIoU=0.4 on THUMOS14. When RGB and optical flow are used, achievements 59.95\% map
at tIoU=0.4 on THUMOS14

In summary, our paper has the following contributions:
\begin{itemize}
    \item[$\bullet$]The model proves that optical flow data is not indispensable in TAD task, and the use of RGB datasets can also achieve very good results.
    \item[$\bullet$]The MLP-Like model based on Newtonian mechanics can not only achieve high accuracy when using RGB datasets, but also achieve end-to-end rapid reasoning.
    \item[$\bullet$]In order to prove the superiority of our model, our model achieves higher accuracy when using RGB and optical flow.
\end{itemize}

\section{Related Works}
In this section, we review the prior work relevant to action recognition temporal action detection and mlp-like.

\textbf{Action recognition.}It refers to the recognition of people's actions in the video clips (2D frame sequence),which seemed to be an extension of the image classification task to multiple frames.In traditional action recognition models, manual features (HOG, HOF, Dense Trajectories, etc.) are usually extracted first and then predicted using a classifier(SVM, RF, etc.).In the present,deep learning methods have dominated vision.
Lrcns\cite{donahue2015long} uses CNN to extract spatial features and RNN (such as LSTM) to extract temporal features for behavior recognition.R(2+1)\cite{tran2018closer} directly extends 2D convolution to 3D (adds time dimension), and directly extracts features including time and space.two stream I3D\cite{carreira2017quo} designs two separate networks, one for space flow and one for time flow.Spatial network can capture the spatial dependence in video, while temporal network can capture the spatial position of periodic motion in video.Long distance time modeling through better remote loss.One part of our model is based on input from a kinetics dataset pre-trained in I3D.

\textbf{Temporal action detection}
As mentioned earlier, the goal of sequential action detection task is to detect action instances with time boundaries and action categories in untrimmed video.\textbf{Two stages}: A given video is first segmented into multiple class-independent Proposals, then each Proposal is categorized into an action category.Proposal generation is the focus of most methods.For example,BSN\cite{lin2018bsn} generates recommendations through local positioning time boundary and global evaluation confidence. BMN\cite{lin2019bmn} proposes a boundary matching mechanism to evaluate the reliability of proposals, which greatly simplifies the process of BSN and brings significant improvement in efficiency and effectiveness.By properly expanding the receptive field alignment, TAL-Net\cite{chao2018rethinking} makes better use of the temporal context of action to generate proposals and action classification.PGCN\cite{zeng2019graph} uses graph convolutional networks and the concept of edges to share context and background information between  nodes (proposals).
By enriching the local-snippet and global video-level contexts, ContextLoc\cite{zhu2021enriching} enhances the features of each proposal.
context

Contrary to this,
\textbf{One stage}.it focuses on the localization and classification of video actions for direct detection, so this method is more efficient.For example,Inspired by yolo\cite{DBLP:conf/cvpr/RedmonF17}, SSAD\cite{DBLP:conf/mm/LinZS17} does not need to predict the candidate time series interval first,but carries out category prediction, time series offset correction and IOU prediction at the same time. SS-TAD\cite{BMVC2017_93} enforces semantic constraints on intermediate modules that are gradually relaxed as learning progresses. Gtan\cite{DBLP:conf/cvpr/LongYQTLM19} considers the timing structure between actions by learning a set of Gaussian kernels. Each Gaussian check should have an action interval, which can be predicted flexibly through the combination of Gaussian kernels.some methods attempt to achor-free,which got rid of redundant hyper-parameters.AFSD\cite{DBLP:conf/cvpr/Lin0LWTWLHF21} Makes full use of the characteristics of boundary and extract significant boundary features with boundary pooling.In order to be faster, our model also uses the method of one stage anchor free.

\textbf{Mlp-like.} In recent years, new architectures in the field of computer vision emerge one after another. For example, transformer and MLP have achieved performance beyond CNN in many tasks. Among them, visual MLP has an extremely simple architecture, which is only stacked by multi-layer perceptrons (MLP)For example,MLP-Mixe\cite{DBLP:conf/cvpr/Lin0LWTWLHF21}r contains two types of layers: one with MLPs applied independently to image patches and one with MLPs applied across patches.The MorphMLP\cite{DBLP:journals/corr/abs-2111-12527} architecture focuses on local information in the low-level layer while gradually shifting to long-term modle at the high-level layer.Above two achieved Competitive performance on image classification.Inspired by this, we try to Develop MLP-like potential in the downstream task of tad.
\section{Model}
\begin{figure*}
  \centering
\includegraphics[width=0.9\linewidth]{figer2.jpg}
    \caption{The architecture is mainly composed of four parts are as follows: a basic backbone module with feature extraction and a time down sampling module, and then a time fusion pyramid network(TFPN) as the neck,at last the module includes action and time prediction branches as the head.}
    \label{fig2}

\end{figure*}
\subsection{Overview}
Denote untrimmed video dataset as $D = \{D_{train}, D_{test} \} $.each data as $ V\in\mathbb{R}^{T\times C\times H\times W}$,where \emph{T,C,H,W} denote the time step, channel, height and width individually.In most tasks, \emph{V} will convert into ${(\emph{V}_{rgb},\emph{V}_{opt})}$,where ${\textit{V}_{rgb}}$ 
contains RGB information and ${\textit{V}_{opt}}$ contains optical flow information which takes a lot of time and calculation cost to get it.Our model can only use RGB as dataset.Then take the above as input and get the output through the model,where output Y =  ${(d_{i,s}, d_{i,e}, c_i)}$.There are $d_{i,s}, d_{i,e}$ distances between this time step and the star and end of this action and A moment is either part of the action category or part of the background category according to $c_i$.

The architecture is mainly composed of four parts. The components are as follows: a basic backbone module with feature extraction and a time down sampling module, and then a time fusion pyramid network(TFPN) as the neck,at last the module includes action and time prediction branches as the head.



\textbf{Mechanics token mixing block.}
To better aggregate tokens by dynamically modulating the relationship between tokens and fixed weights in MLP.We regard each marker as a force and aggregate the relationship between its force and force through Newtonian mechanics.For example,given $D^0 \in \{X_1,X_2...,X_n\}$with n time steps.$z^0$ is projected using $F_a and F_B$ with FC layer.The angle between $F_a and F_B$ is $\theta$.According to the laws of mechanics, their resultant force is $A_f$.
\begin{figure}
  \center
\includegraphics[width=0.8\linewidth]{figer3.jpg}
   \caption{The architecture is mainly composed of four parts are as follows: a basic backbone module with feature extraction and a time down sampling module, and then a time fusion pyramid network(TFPN) as the neck,at last the module includes action and time prediction branches as the head.}
    \label{fig3}
\end{figure}
\begin{equation}
\begin{aligned}
    &Token_{mechanics} = \{X_1,X_2...,X_n\}\\
    &F_a=FC(Token_{mechanics},W^i)\\
    &F_b=FC(Token_{mechanics},W^j)\\
    &A_f = \sqrt{F_a+F_b+2F_aF_b\cos\theta}
\end{aligned}
\end{equation}
In a basic mechanics unit, the inputs first were fed into mechanics token mixing and then channel mixing block, and a layernorm before each mixing (Figure 2). The mechanics token mixing MLP is stack by one MTM modules, which aggregating different tokens by considering both $F_1$ and $F_2$  and relu activation functions.The channel mixing MLP,which extracts features for each token. 

\begin{equation}
\begin{aligned}
    &X = Norm(X)\\
    &Z = A_f(X)\\
    &Z = Norm(Z)\\
    &Z = Channe-FC(Z)
\end{aligned}
\end{equation}
\textbf{Backbone.} Given an input video, we first generate video clips with constant time size, and reshape each clip to $(\emph{T},\emph{C},\emph{H},\emph{W})$.The feature extraction  transforms a video into a sequence of feature vectors corresponding to each visual modality RGB.At present, most of all methods also needs to convert video into optical flow, which not only consumes computing resources,but also takes up time.This is the reason why most current TAD methods can not fully realize rapid detection.Our model can ignore the cumbersome process in this step.Then we use the backbone network of extracting 3D feature map, which  chunked the videos into short overlapping segments of
8 framesis and pre trained on the Kinetics I3D model.Finally we got $Y_{I3D}=(Y_{rgb},Y_{opt})$,both of them is 1024-D feature.Different from others,Our method only use  $Y_{rgb}$.We also tried $Y_{I3D}$ as our input, which also played a good effect, indicating the superiority of our method.The feature is then flattened along the last four dimensions to a two-dimensional feature sequence. Such a sequence can contain the temporal and spatial information of the whole video. Then we use a multilayer semantic module(MSM),There are six downsample layers in total. Each layer is composed of mechanics unit and outputs (512, t/2), (512, t/4), (512, t/8), (512, t/16), (512, t/32), (512, t/64) respectively.Details are shown in Fig2. The features of this module will generate multi-coarse texture basic feature.

\textbf{Temporal Feature Pyramid Network.} 
Neck contributes to the architecture of TAD as does target detection.The neck is designed to take advantage of the backbone's coarse texture to its full potential.Moreover, the model must also be simple in order to be faster.
Therefore, affpns, which have adaptive attention modules, bifpns, which use repeated superposition method, and so on can not meet our needs.The high-resolution features by backbone, which are up sampled in six layers in turn.The up sampling method is bilinear interpolation, which is combined with the high-resolution features at the same time in the process of up sampling.The modified method can simplify the model and learn the characteristics of more detailed examples at the same time.

\textbf{Temporal Action Detection Heads} 
Our proposed Temporal Action Detection Heads (TADH) includes two branches: the classification branch that predicts the probability of each class $c_i$ and The regression branch that predicts the starting and ending time distance $(t_{i,s}, t_{i,e})$. Each time step t further decodes an action instance
$(T_s = T-T^e$ and $T_e = T+T^s)$ are the start and end of the action, and $c_i$ is an action confidence score.Both two branches include three FC layers and two layernorm layers.The classification branch needs to predict the number of categories, so added to the branch to activate RELU.

\subsection{Training and Inference.}
\textbf{Loss.}
Classification loss we use focal loss\cite{lin2017focal} for the category label, which can alleviate the problem of class imbalance.It control the weight of positive and negative samples that are easy to classify and difficult to classify, so as to improve the detection accuracy
We've set up for classified loss $\lambda_{cls}$ Super parameter.
Regression loss is used to exchange instance time boundary regression, Unlike giou\cite{rezatofighi2019generalized} our tad task, it is not very sensitive to detect shapes We set the super parameter in the error term of the predicted value and position of giou loss $\beta$.We've set up for classified loss $\lambda_{reg}$ Super parameter.$l_{ct}$is an indicator function that denotes if a time step t is within an action.$T_{+}$ is the total number of positive samples.

\begin{equation}
\begin{aligned}
    &\mathcal{L}_{\beta-\text { GloU }}=1-IoU+\left(\frac{\left|C \backslash\left(A \cup B^{g t}\right)\right|}{|C|}\right)^{\beta}\\
    &\ell_{cls}\left(\left\{\hat{y}_{i}\right\}\right)=\frac{1}{N} \sum_{i} \ell_{\text {focal }}\left(\hat{y}_{i}, y_{i}\right)\\
&\mathcal{L} = \sum_{k=1}^N (\frac{\lambda_{cls}}{T}\ell_{cls}+ \frac{\lambda_{reg}}{T_+}\mathbb{1}_{c_{t}}\ell_{reg})
\end{aligned}
\end{equation}

\textbf{Train.}We use pytoch to implement the model. It is in an NVIDIA geforce GTX 2080ti GPU.The models are trained for 80 epoches using Adam with warm-up for training. The warm-up stage turned out to be rate of $10^{-5}$.Batch size is set to 4.The weight of loss $\lambda_{cls}$ and $\lambda_{reg}$ is set 1.Input sequences were cropped or padded according to the maximum sequence length of 2304.

\textbf{inference.}During inference, we ignore the action predictions from all but the last Lightweight mlp layer.we feed the full sequences
into the model. Our model takes the input video X, and outputs 
$(t_{i,s}, t_{i,e}, c_i)$
for every time step T across all pyramid
levels.The result action candidates are further processed
using Non maximum suppression(Soft-NMS)\cite{bodla2017soft} to remove highly overlapping instances,
leading to the final outputs of actions.

\section{Experiments and Results.}
Our proposed approach was tested in several experiments to demonstrate its effectiveness. Our main results include benchmarks on THUMOS14\cite{THUMOS14}, ActivityNet1.3\cite{caba2015activitynet}. Moreover, we provide extensive ablation studies of our model.

\textbf{Evaluation indicators.} 
For all dataset, we report the standard
mean average precision (mAP) at different temporal intersection over union (tIoU) thresholds, widely used to evaluate.On thumos14, the tiou threshold is selected from {0.3,0.4,0.5,0.6,0.7}. In activitynet v1. 3. The tiou threshold is {0.5,0.75,0.95}. We also report that the average graph of the tiou threshold is between 0.5 and 0.95 in steps of 0.05. 
\subsection{Results on THUMOS14}
\textbf{Dataset.} The thumos14 dataset contains 413 untrimmed data sets
Video containing 20 kinds of actions. Data sets are partitioned
It is divided into two subsets: verification set and test set. verification
The test set contains 200 videos and the test set contains 213 videos.
By convention, we use validation sets for training and report results on test sets.
This dataset has 20 action categories. The videos
contain an average of 15 action instances per video with an
average of 8\% overlapping with other instances.

\begin{table*}[!t]
 \centering
\setlength{}{
  \begin{tabular}{c@{}|l|c|c|c|c|c|c|c@{}}
\toprule
Type& Method& RGB stream&\quad0.3&0.4&0.5&0.6&0.7&Avg\\
\midrule 
  \multirow{2}*{Two-stage} 
 &CDC\cite{cdc} &\XSolid &\quad40.10& 29.40& 23.30& 13.10& 7.90& 20.76\\
 &TCN\cite{tcn}&\XSolid&\quad-& 33.30& 25.60& 15.90& 9.00&-\\
 &TURN-TAP\cite{turn}&\XSolid&\quad44.10& 34.90& 25.60& -& -&-\\
 &R-C3D\cite{rc3d}&\XSolid&\quad44.80& 35.60& 28.90& -& -&-\\
 &MGG\cite{liu2019multi} &\XSolid &\quad53.9&46.8&37.4&29.5&21.3& 37.78\\
 &BMN\cite{lin2019bmn} &\XSolid&\quad56&47.4&38.8&29.7&20.5& 38.48\\
 &DBG\cite{dbg}&\XSolid&\quad57.8& 49.4& 39.8& 30.2& 21.7&39.78\\
 &BSN++\cite{su2020bsn++}&\XSolid&\quad59.90& 45.90& 41.30& 31.90& 22.80&40.36\\
 &GCN\cite{zeng2019graph}&\XSolid&\quad63.6& 57.8& 49.1& -& -&-\\
 &TAL-Net\cite{chao2018rethinking}&\XSolid&\quad53.2&48.5&42.8&33.8&20.8&39.8 \\
 &BMN\cite{lin2019bmn}&\XSolid&\quad56.4& 47.9& 39.2& 30.2& 21.2& 39.0\\
 &G-TAD\cite{G-tad}&\XSolid&\quad58.7& 52.7& 44.9& 33.6& 23.8& 42.7\\
 &MR\cite{MR}&\XSolid&\quad53.9& 50.7& 45.4& 38.0& 28.5& 43.3\\
 &ContextLoc\cite{contextloc}&\XSolid&\quad68.3& 63.8& 54.3& 41.8& 26.2&50.88\\
\midrule
\multirow{2}*{One-stage} & PBRNet\cite{pbrnet} &\XSolid &\quad58.5& 54.6& 51.3& 41.8& 29.5& -\\
&A2Net\cite{a2net} &\XSolid &\quad58.6& 54.1& 45.5& 32.5& 17.2& 41.6\\
&G-TAD\cite{DBLP:conf/cvpr/LongYQTLM19}&\Checkmark&\quad57.8&47.2&38.8&-&-&-\\
&TadTR\cite{liu2021end} &\XSolid &\quad62.4&57.4&49.2&37.8&26.3&46.6 \\
&TadML & \Checkmark&\quad68.78&64.66&56.61&45.40&31.88&53.46\\
&TadML & \XSolid&\quad73.29&69.73&62.53&53.36&39.60&59.70\\
\bottomrule
\end{tabular}}
\caption{Performance comparison with  methods on THUMOS14, measured by mAP at different IoU thresholds, and average mAP in [0.3 : 0.1 : 0.7] on THUMOS14.}
\end{table*}
\textbf{Results}
Table 1 Summarizing the results,without the need to manually convert optical flow data,our method achieves an average mAP of 53.46\% ([0.3 : 0.1 : 0.7]), with
an mAP of 56.61\% at tIoU=0.5 and an mAP of 31.88\% at
tIoU=0.7,which is not only comparable to and exceeds most models requiring optical flow in accuracy, but also faster in the whole method while our model is fast because there is no need for manual conversion. In order to prove the superiority of our model, we also carried out experiments using optical flow.It achieves an average mAP of 59.7\% ([0.3 : 0.1 : 0.7]), with
an mAP of 62.53\% at tIoU=0.5 and an mAP of 39.6\% at
tIoU=0.7.
\subsection{ Results on ActivityNet-1.3}
\textbf{ActivityNet1.3} ActivityNet-1.3 is a large-scale action dataset
which contains 200 activity classes and around 20,000 videos
with more than 600 hours. The dataset is divided into three
subsets: 10,024 videos for training, 4,926 for validation, and
5,044 for testing. Follow the common practice in [36, 38,
76], we train our model on the training set and report the
performance on the validation set.

\textbf{Results}
Table 2 shows the results. With I3D
features, our method reaches an average mAP of 34.94([0.5 : 0.05 : 0.95]), outperforming all previous methods
using the same features by at least 0.6\%. This boost is significant as the result is averaged across many tIoU thresholds,
including those tight ones e.g. 0.95. Using the pre-training
method from TSP slightly improves our results (36.0average mAP). Our model thus outperforms the best method
with the same features by a small margin. Again, our
method largely outperforms TadTR. Our results are
only worse than TCANet —a latest two-stage method
using stronger SlowFast features . We conjecture that
our method will also benefit from better features. None the less, our simple model clearly demonstrates state-of-the-art
results on this challenging dataset.

\begin{table}
\centering
\resizebox{.45\textwidth}{!}{
\begin{tabular}{l|c|c c c c}
\hline
 Method&Single-stage&\quad0.5&0.75&0.95& Avg\\
\midrule 
R-C3D\cite{rc3d}&\XSolid &\quad26.80&\quad$-$&\quad$-$&\quad$-$\\
TAL-Net\cite{chao2018rethinking}&\XSolid&\quad38.23&\quad18.30&\quad1.30&\quad20.22\\
BSN\cite{lin2018bsn}&\XSolid&\quad56.45&\quad29.96&\quad8.02&\quad30.03\\
BMN \cite{lin2019bmn}&\XSolid&\quad50.07&\quad34.78&\quad8.29&\quad33.85\\
P-GCN\cite{zeng2019graph}&\XSolid&\quad42.90&\quad28.14&\quad2.47&\quad26.99\\
Contextloc\cite{contextloc}&\XSolid&\quad51.24&\quad 31.40&\quad 2.83&\quad 30.59 \\
TadTR+BMN\cite{liu2021end}&\XSolid&\quad50.51&\quad 35.35&\quad 8.18&\quad 34.55\\
A2Net\cite{a2net}&\Checkmark&\quad43.55&\quad 28.69&\quad 3.70&\quad 27.75\\
SSN\cite{ssn}&\Checkmark&\quad43.26&\quad28.70&\quad5.63&\quad28.28\\
TadTR\cite{liu2021end}&\Checkmark&\quad49.08&\quad 32.58&\quad 8.49&\quad 32.27\\
G-TAD\cite{DBLP:conf/cvpr/LongYQTLM19}&\Checkmark&\quad50.36&\quad 34.60&\quad 9.02&\quad 34.09\\
AFSD\cite{DBLP:conf/cvpr/Lin0LWTWLHF21}& \Checkmark&\quad52.4&\quad 35.3&\quad 6.5&\quad 34.4\\
Ours  &\Checkmark&\quad53.15&\quad35.75&\quad7.47&\quad34.94\\
\bottomrule
\end{tabular}}
\caption{Performance comparison with methods on ActivityNetv1.3, measured by mAP at different IoU thresholds, and average mAP in [0.5 : 0.75 : 0.95] on ActivityNetv1.3.}
\label{tab2}
\end{table}

\subsection{Ablation Study}
In this section,to further verify the efficacy of our contributions, we conduct several ablation studies on THUMOS14 for the model.
Thumos14 further studies the efficacy of keys
Components and super parameters are set in our recommended
.For all experiments, we only change the corresponding parts and used the same evaluation settings.

\begin{table}[!h]
\center
\resizebox{.4\textwidth}{!}{
\begin{tabular}{c|c|c c c c c}
\toprule
Neck Stages&RGB&\quad0.3&0.4&0.5&0.6&0.7\\
\hline
1&\XSolid&\quad56.09&49.11&38.35&24.92&12.03 \\
2&\XSolid&\quad61.74&55.6&44.81&29.39&14.29  \\
3&\XSolid&\quad65.74&60.16&49.91&36.09&19.78  \\
4&\XSolid&\quad66.82&62.32&53.82&43.26&28.96  \\
5&\XSolid&\quad66.98&62.77&55.42&44.81&31.82  \\
6&\XSolid&\quad68.7&64.66&56.61&45.40&31.88 \\
7&\XSolid&\quad68.7&64.66&56.61&45.40&- \\
1&\Checkmark& \quad62.7&57.06&46.64&30.73&14.52\\
2&\Checkmark& \quad68.04&62.6&52.37&35.65&18.52\\
3&\Checkmark& \quad70.78&66.1&57.6&44.36&27.41\\
4&\Checkmark& \quad68.7&64.66&56.61&45.40&-\\
5&\Checkmark& \quad73.32&68.91&62.28&52.81&39.04\\
6&\Checkmark& \quad72.79&69.49&62.72&52.29&38.94\\
7&\Checkmark& \quad73.59&69.69&62.79&53.13&40.22\\
\hline
\end{tabular}}
\caption{Study of different number of frozen stages of backbone
on THUMOS14 in terms of mAP(\%)@tIoU}
\label{tab4}
\end{table}
We compare different neck layers and RGB stream ,the results are presented in Table 3. It is obvious that neck layers are crucial for temporal action detection. The average mAP degrades seriously from 36.03\% to 53.46\%,while neck layer is 6.At this point, when the neck layer increases again, the performance of the model begins to decrease.In order to verify the superiority of the model, when layers are set to 7 layers, the average mAP is 59.7\% with two stream.


\begin{table}[!h]
\center
\resizebox{.45\textwidth}{!}{
\begin{tabular}{l|c|c c c c c c}
\hline
Neck Stages&RGB&\quad0.3&0.4&0.5&0.6&0.7&Avg\\
\hline
WaveMLP\cite{wave}&\XSolid&\quad66.87&62.46&54.33&44.00&30.53&51.64 \\
WaveMLP\cite{wave}&\Checkmark&\quad72.01&68.02&61.51&52.01&38.28&58.36\\
MorphMLP\cite{DBLP:journals/corr/abs-2111-12527}&\XSolid&\quad66.91&62.83&54.93&44.57&31.20&52.09 \\
MorphMLP\cite{DBLP:journals/corr/abs-2111-12527}&\Checkmark&\quad72.21&69.12&62.87&52.55&38.47&59.04\\
Ours  & \XSolid&\quad68.78&64.66&56.61&45.40&31.88&53.46\\
Ours  &\Checkmark&\quad73.29&69.73&62.53&53.36&39.60&59.70\\
\hline
\end{tabular}}
\caption{Study of different number of frozen stages of backbone
on THUMOS14 in terms of mAP(\%)@tIoU}
\label{tab5}
\end{table}

\begin{table}[!h]
\center
\resizebox{.4\textwidth}{!}{
\begin{tabular}{c|c c c c c c}
\hline
$\beta$&\quad0.3&0.4&0.5&0.6&0.7&Avg\\
\hline
1&\quad66.68&63.60&56.55&43.77&31.24&52.57 \\
2&\quad67.95&63.55&56.68&45.12&31.97&53.05\\
3&\quad68.78&64.66&56.61&45.40&31.88&53.46\\
4&\quad67.75&64.03&56.47&43.74&31.34&52.67\\
5&\quad67.64&63.82&56.43&44.06&31.17&52.63\\
10&\quad67.44&63.80&56.42&44.15&30.81&52.52\\
\hline
\end{tabular}}
\caption{Study of different $\beta$($\beta-Giou$)
on THUMOS14 in terms of mAP(\%)@tIoU}
\label{tab3}
\end{table}
We also compared with different MLP-Like blocks. Other parameter settings remained unchanged. Results in table 4,when RGB stream was used, the average map increased from 51.64\% of Waveblock to 53.46\%(TadML).We also added optical flow for comparison. The average map of the model is as high as 59.70\%.The result in Table 5, When beta is set to 3, the average map increased from 52.57\% to 53.46\%.As shown in Table 5, when the weight of classification and regression loss is kept at 1, $\beta$ achieves the best performance at 3.
\section{Conclusion}
In the work,we have presented fast but effective schemes for using RGB only.To the best of our knowledge,our Mlp-like effort is the first of its kind for fully supervised TAD.mechanics were proposed as our token mixing.the model shows that MLPs also have great potential for downstream tasks.Additionally, it can greatly impact both RGB and optical flow data sets, as well as having a high effect when only RGB data sets are used. This makes it possible for TAD tasks to be completely end-to-end.

{\small
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{bodla2017soft}
Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry~S Davis.
\newblock Soft-nms--improving object detection with one line of code.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 5561--5569, 2017.

\bibitem{caba2015activitynet}
Fabian Caba~Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos~Niebles.
\newblock Activitynet: A large-scale video benchmark for human activity
  understanding.
\newblock In {\em Proceedings of the ieee conference on computer vision and
  pattern recognition}, pages 961--970, 2015.

\bibitem{carreira2017quo}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In {\em proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6299--6308, 2017.

\bibitem{chao2018rethinking}
Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David~A Ross, Jia
  Deng, and Rahul Sukthankar.
\newblock Rethinking the faster r-cnn architecture for temporal action
  localization.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1130--1139, 2018.

\bibitem{tcn}
Xiyang Dai, Bharat Singh, Guyue Zhang, Larry~S Davis, and Yan Qiu~Chen.
\newblock Temporal context network for activity localization in videos.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 5793--5802, 2017.

\bibitem{donahue2015long}
Jeffrey Donahue, Lisa Anne~Hendricks, Sergio Guadarrama, Marcus Rohrbach,
  Subhashini Venugopalan, Kate Saenko, and Trevor Darrell.
\newblock Long-term recurrent convolutional networks for visual recognition and
  description.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2625--2634, 2015.

\bibitem{turn}
Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram Nevatia.
\newblock Turn tap: Temporal unit regression network for temporal action
  proposals.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 3628--3636, 2017.

\bibitem{THUMOS14}
Y.-G. Jiang, J. Liu, A. Roshan~Zamir, G. Toderici, I. Laptev, M. Shah, and R.
  Sukthankar.
\newblock {THUMOS} challenge: Action recognition with a large number of
  classes, 2014.

\bibitem{dbg}
Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie
  Wang, Jilin Li, Feiyue Huang, and Rongrong Ji.
\newblock Fast learning of temporal action proposal via dense boundary
  generator.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 11499--11506, 2020.

\bibitem{DBLP:conf/cvpr/Lin0LWTWLHF21}
Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang, Ying Tai, Chengjie Wang,
  Jilin Li, Feiyue Huang, and Yanwei Fu.
\newblock Learning salient boundary feature for anchor-free temporal action
  localization.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition,
  {CVPR} 2021, virtual, June 19-25, 2021}, pages 3320--3329. Computer Vision
  Foundation / {IEEE}, 2021.

\bibitem{lin2019bmn}
Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.
\newblock Bmn: Boundary-matching network for temporal action proposal
  generation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3889--3898, 2019.

\bibitem{DBLP:conf/mm/LinZS17}
Tianwei Lin, Xu Zhao, and Zheng Shou.
\newblock Single shot temporal action detection.
\newblock In Qiong Liu, Rainer Lienhart, Haohong Wang, Sheng{-}Wei~"Kuan{-}Ta"
  Chen, Susanne Boll, Yi{-}Ping~Phoebe Chen, Gerald Friedland, Jia Li, and
  Shuicheng Yan, editors, {\em Proceedings of the 2017 {ACM} on Multimedia
  Conference, {MM} 2017, Mountain View, CA, USA, October 23-27, 2017}, pages
  988--996. {ACM}, 2017.

\bibitem{lin2018bsn}
Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang.
\newblock Bsn: Boundary sensitive network for temporal action proposal
  generation.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{pbrnet}
Qinying Liu and Zilei Wang.
\newblock Progressive boundary refinement network for temporal action
  detection.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 11612--11619, 2020.

\bibitem{liu2021end}
Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Song Bai, and Xiang Bai.
\newblock End-to-end temporal action detection with transformer.
\newblock {\em arXiv preprint arXiv:2106.10271}, 2021.

\bibitem{liu2019multi}
Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, and Shih-Fu Chang.
\newblock Multi-granularity generator for temporal action proposal.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3604--3613, 2019.

\bibitem{DBLP:conf/cvpr/LongYQTLM19}
Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo Luo, and Tao Mei.
\newblock Gaussian temporal awareness networks for action localization.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition,
  {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, pages 344--353. Computer
  Vision Foundation / {IEEE}, 2019.

\bibitem{DBLP:conf/cvpr/RedmonF17}
Joseph Redmon and Ali Farhadi.
\newblock {YOLO9000:} better, faster, stronger.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pages
  6517--6525. {IEEE} Computer Society, 2017.

\bibitem{rezatofighi2019generalized}
Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and
  Silvio Savarese.
\newblock Generalized intersection over union: A metric and a loss for bounding
  box regression.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 658--666, 2019.

\bibitem{BMVC2017_93}
Bernard~Ghanem Shyamal~Buch, Victor~Escorcia and Juan~Carlos Niebles.
\newblock End-to-end, single-stream temporal action detection in untrimmed
  videos.
\newblock In Gabriel~Brostow Tae-Kyun~Kim, Stefanos~Zafeiriou and Krystian
  Mikolajczyk, editors, {\em Proceedings of the British Machine Vision
  Conference (BMVC)}, pages 93.1--93.12. BMVA Press, September 2017.

\bibitem{su2020bsn++}
Haisheng Su, Weihao Gan, Wei Wu, Yu Qiao, and Junjie Yan.
\newblock Bsn++: Complementary boundary regressor with scale-balanced relation
  modeling for temporal action proposal generation.
\newblock {\em arXiv preprint arXiv:2009.07641}, 2020.

\bibitem{wave}
Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.
\newblock An image patch is a wave: Phase-aware vision mlp.
\newblock {\em arXiv preprint arXiv:2111.12294}, 2021.

\bibitem{tran2018closer}
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
  Paluri.
\newblock A closer look at spatiotemporal convolutions for action recognition.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 6450--6459, 2018.

\bibitem{rc3d}
Huijuan Xu, Abir Das, and Kate Saenko.
\newblock R-c3d: Region convolutional 3d network for temporal activity
  detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 5783--5792, 2017.

\bibitem{G-tad}
Mengmeng Xu, Chen Zhao, David~S Rojas, Ali Thabet, and Bernard Ghanem.
\newblock G-tad: Sub-graph localization for temporal action detection.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10156--10165, 2020.

\bibitem{cdc}
Ke Yang, Peng Qiao, Dongsheng Li, Shaohe Lv, and Yong Dou.
\newblock Exploring temporal preservation networks for precise temporal action
  localization.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{a2net}
Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, and Junwei Han.
\newblock Revisiting anchor mechanisms for temporal action localization.
\newblock {\em IEEE Transactions on Image Processing}, 29:8535--8548, 2020.

\bibitem{zeng2019graph}
Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang,
  and Chuang Gan.
\newblock Graph convolutional networks for temporal action localization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7094--7103, 2019.

\bibitem{DBLP:journals/corr/abs-2111-12527}
David~Junhao Zhang, Kunchang Li, Yunpeng Chen, Yali Wang, Shashwat Chandra, Yu
  Qiao, Luoqi Liu, and Mike~Zheng Shou.
\newblock Morphmlp: {A} self-attention free, mlp-like backbone for image and
  video.
\newblock {\em CoRR}, 2021.

\bibitem{MR}
Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, and Qi Tian.
\newblock Bottom-up temporal action localization with mutual regularization.
\newblock In {\em European Conference on Computer Vision}, pages 539--555.
  Springer, 2020.

\bibitem{ssn}
Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin.
\newblock Temporal action detection with structured segment networks.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2914--2923, 2017.

\bibitem{zhu2021enriching}
Zixin Zhu, Wei Tang, Le Wang, Nanning Zheng, and Gang Hua.
\newblock Enriching local and global contexts for temporal action localization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 13516--13525, 2021.

\bibitem{contextloc}
Zixin Zhu, Wei Tang, Le Wang, Nanning Zheng, and Gang Hua.
\newblock Enriching local and global contexts for temporal action localization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 13516--13525, 2021.

\end{thebibliography}
 \bibliographystyle{ieee_fullname}

}

\end{document}
