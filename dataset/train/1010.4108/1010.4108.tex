
\documentclass[twoside,leqno,twocolumn]{article}  
\usepackage{ltexpprt} 
\usepackage{nicefrac}
\usepackage{xspace}
\usepackage{amsmath,amstext,amssymb,amsfonts}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{graphicx}


\newcommand{\flatfrac}[2]{#1/#2}
\newcommand{\ffrac}{\flatfrac}
\newcommand{\nfrac}{\nicefrac}



\newcommand{\Erdos}{Erd\H{o}s\xspace}
\newcommand{\Renyi}{R\'enyi\xspace}
\newcommand{\Lovasz}{Lov\'asz\xspace}
\newcommand{\Juhasz}{Juh\'asz\xspace}
\newcommand{\Bollobas}{Bollob\'as\xspace}
\newcommand{\Furedi}{F\"uredi\xspace}
\newcommand{\Komlos}{Koml\'os\xspace}
\newcommand{\Luczak}{\L uczak\xspace}
\newcommand{\Kucera}{Ku\v{c}era\xspace}
\newcommand{\Szemeredi}{Szemer\'edi\xspace}


\usepackage[varg]{txfonts}


\renewcommand{\mathbb}{\varmathbb}

\usepackage{fullpage}


\renewcommand{\leq}{\leqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\ge}{\geqslant}



\usepackage{bm}

\usepackage{xspace}



 \usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}



\newcommand{\mper}{\,.}
\newcommand{\mcom}{\,,}

\newcommand{\cA}{\mathcal A}
\newcommand{\cB}{\mathcal B}
\newcommand{\cC}{\mathcal C}
\newcommand{\cD}{\mathcal D}
\newcommand{\cE}{\mathcal E}
\newcommand{\cF}{\mathcal F}
\newcommand{\cG}{\mathcal G}
\newcommand{\cJ}{\mathcal J}
\newcommand{\cH}{\mathcal H}
\newcommand{\cK}{\mathcal K}
\newcommand{\cI}{\mathcal I}
\newcommand{\cL}{\mathcal L}
\newcommand{\cN}{\mathcal N}
\newcommand{\cM}{\mathcal M}
\newcommand{\cO}{\mathcal O}
\newcommand{\cP}{\mathcal P}
\newcommand{\cQ}{\mathcal Q}
\newcommand{\cR}{\mathcal R}
\newcommand{\cS}{\mathcal S}
\newcommand{\cT}{\mathcal T}
\newcommand{\cU}{\mathcal U}
\newcommand{\cV}{\mathcal V}
\newcommand{\cW}{\mathcal W}
\newcommand{\cX}{\mathcal X}
\newcommand{\cY}{\mathcal Y}
\newcommand{\cZ}{\mathcal Z}

\renewcommand{\vec}[1]{{\bm{#1}}}

\newcommand{\pvec}[1]{\vec{#1}'}
\newcommand{\ppvec}[1]{\vec{#1}''}
\newcommand{\tvec}[1]{{\tilde{\vec{#1}}}}

\newcommand{\paren}[1]{(#1 )}
\newcommand{\Paren}[1]{\left(#1 \right )}

\newcommand{\brac}[1]{[#1 ]}
\newcommand{\Brac}[1]{\left[#1 \right]}

\newcommand{\set}[1]{\{#1\}}
\newcommand{\Set}[1]{\left\{#1\right\}}

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\Abs}[1]{\left\lvert#1\right\rvert}

\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\from}{\colon}

\newcommand{\supp}{\mathrm{supp}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\dist}{\mathrm{dist}}

\newcommand{\defeq}{\stackrel{\textup{def}}{=}}

\newcommand{\vbig}{\vphantom{\bigoplus}}

\newcommand{\iprod}[1]{\langle #1\rangle}

\newcommand{\snorm}[1]{\norm{#1}^2}

\newcommand{\normt}[1]{\norm{#1}_{\scriptstyle 2}}
\newcommand{\snormt}[1]{\norm{#1}^2_2}

\newcommand{\normo}[1]{\norm{#1}_{\scriptstyle 1}}
\newcommand{\normi}[1]{\norm{#1}_{\scriptstyle \infty}}

\newcommand{\Z}{\mathbb Z}
\newcommand{\N}{\mathbb N}
\newcommand{\R}{\mathbb R}
\newcommand{\Rnn}{\R_+}

\newcommand{\sdp}{{\sf sdp}}
\newcommand{\sgn}{{\rm sgn}}
\newcommand{\qp}{\mathrm{qp}}
\newcommand{\opt}{{\sf OPT}}
\newcommand{\val}{{\sf VAL}}


\newcommand{\seteq}{\mathrel{\mathop:}=}
\newcommand{\minimize}{\text{minimize}}
\newcommand{\maximize}{\text{maximize}}
\newcommand{\subjectto}{\text{subject to}}
\newcommand{\card}{\abs}
\newcommand{\Card}{\Abs}
\newcommand{\thalf}{\tfrac12}
\renewcommand{\opt}{\mathsf{OPT}}
\newcommand{\lp}{\mathsf{LP}}



\newcommand{\Esymb}{\mathbb{E}}
\newcommand{\Psymb}{\mathbb{P}}
\newcommand{\Vsymb}{\mathbb{V}}
\DeclareMathOperator*{\E}{\Esymb}
\DeclareMathOperator*{\Var}{\Vsymb}
\DeclareMathOperator*{\ProbOp}{\Psymb}
\renewcommand{\Pr}{\ProbOp}
\newcommand{\Tr}{{\rm Tr}}

\newcommand{\prob}[1]{\Pr\set{ #1 }}
\newcommand{\Prob}[1]{\Pr\Set{ #1 }}

\newcommand{\ex}[1]{\E\brac{#1}}
\newcommand{\Ex}[1]{\E\Brac{#1}}
\newcommand{\varex}[1]{\E\paren{#1}}
\newcommand{\varEx}[1]{\E\Paren{#1}}
\newcommand{\eset}{\emptyset}
\newcommand{\e}{\epsilon}

\newcommand{\super}[2]{#1^{\paren{#2}}}

\newcommand{\Ind}{\mathbb I}

\newcommand{\bits}{\{0,1\}}

\newcommand{\sbits}{\{1,-1\}}

\renewcommand{\labelitemi}{--}

\newcommand{\conv}{\mathrm{conv}}







\let\e\varepsilon




\newcommand\bdot\bullet

\newcommand{\sge}{\succeq}
\newcommand{\sle}{\preceq}

\renewcommand{\ij}{{ij}}

\newcommand{\lmin}{\lambda_{\min}}
\newcommand{\lmax}{\lambda_{\max}}

\newcommand{\sse}{\subseteq}


\newcommand{\multicut}{\textsc{MultiCut}\xspace}
\newcommand{\MC}{{\sc Maximum Cut}}
\newcommand{\MUC}{{\sc Minimum Uncut}}
\newcommand{\SC}{{\sc Sparsest Cut}}
\newcommand{\BS}{{\sc Balanced Separator}\xspace}
\newcommand{\UG}{{\sc Unique Games}}
\newcommand{\GML}{\Gamma{\sf MAX2LIN}}
\newcommand{\instance}{{\cal{U}}=(G(V,E),[k], \{\pi_{uv}\}_{(u,v) \in E})}
\newcommand{\Zk}{\mathbb{Z}_k}
\newcommand{\SDP}{{\sf SDP}\xspace}
\newcommand{\U}{\ensuremath{\mathcal{U}}}
\newcommand{\LM}{\Lambda}
\newcommand{\IS}{{\sc Independent Set}}

\newcommand{\Primal}{{\sf Primal}\xspace}
\newcommand{\Dual}{{\sf Dual}\xspace}

\newcommand{\primal}{{\sf primal}\xspace}
\newcommand{\dual}{{\sf dual}\xspace}


\newcommand{\x}{{\mathbf{x}}}
\newcommand{\y}{{\mathbf{y}}}
\renewcommand{\v}{{\mathbf{v}}}
\renewcommand{\u}{{\mathbf{u}}}
\newcommand{\V}{{\mathbf{V}}}
\newcommand{\W}{{\mathbf{W}}}



\newcommand{\cutratio}[1]{\Phi(#1)}
\newcommand{\pagerank}[2]{\rho_{#1,#2}}
\newcommand{\edgenorm}[1]{\norm{#1}_D}
\newcommand{\edgesnorm}[1]{\snorm{#1}_D}
\newcommand{\normed}[2]{\frac{#1}{\edgenorm{#2}}}
\newcommand{\prog}{\mathsf{PROG}}
\newcommand{\Vol}[1]{\text{vol}(#1)}
\newcommand{\Supp}[1]{\text{Supp}(#1)}
\newcommand{\SP}{{\sc SpImprove}}

\newcommand{\cuts}[1]{(S_{#1},\overline{S}_{#1})}
\newcommand{\cut}{(S,\overline{S})}

\newcommand{\Deg}{D^{\nfrac{1}{2}}}
\newcommand{\Degin}{D^{-\nfrac{1}{2}}}

\newcommand{\prnibble}{{\tt PageRankNibble}}
\newcommand{\prpartition}{{\tt PageRankPartition}}
\newcommand{\justnibble}{{\tt Nibble}}
\newcommand{\nibble}{{\tt RandomNibble}}
\newcommand{\partition}{{\tt Partition}}
\newcommand{\evocut}{{\tt EvoCut}}
\newcommand{\evonibble}{{\tt EvoNibble}}
\newcommand{\evopartition}{{\tt EvoPartition}}
\newcommand{\polylog}[1]{ {(#1) \cdot O(\operatorname{polylog}(n))}}
\newcommand{\polylognoparen}[1]{ {O(#1 \operatorname{polylog}(n))}}
\numberwithin{equation}{section}




\newcommand{\psdp}{{\ensuremath{{\mathsf{psdp}}}\xspace}}
\newcommand{\dsdp}{{\ensuremath{\mathsf{dsdp}}\xspace}}

\newcommand{\alg}{{\sc BalCut}\xspace}
\newcommand{\avg}{{\ensuremath{\mathsf{avg}}\xspace}}

\begin{document}


\title{
  \textsc{Towards an SDP-based Approach to Spectral Methods}\\ {\Large A Nearly-Linear-Time Algorithm for Graph Partitioning and Decomposition}
}

\author{Lorenzo Orecchia  \thanks{UC Berkeley. Supported by NSF grants CCF-0830797 and CCF-0635401. This work was initiated while this author was visiting Microsoft Research India, Bangalore.}\\
\and 
Nisheeth K. Vishnoi \thanks{Microsoft Research India, Bangalore.}}
\date{}




\maketitle	

\begin{abstract}
In this paper, we consider the following graph partitioning problem: The input is an undirected graph  a balance parameter   and a target conductance value  
The  output is a cut which, if non-empty, is of conductance at most  for some function  and which  is either balanced or well correlated with all cuts of conductance at most  
In a seminal paper, Spielman and Teng \cite{ST1} gave an -time algorithm for  and used it to decompose graphs into a collection of near-expanders \cite{ST2}.

We present a new spectral algorithm for this problem which runs in time  for  Our result yields the first nearly-linear time algorithm for the classic \BS problem that achieves the asymptotically optimal approximation guarantee for spectral methods.

Our method has the advantage of being conceptually simple and relies on a primal-dual semidefinite-programming (\SDP) approach. 
We first consider a natural SDP relaxation for the \BS problem.  While it is easy to obtain from this SDP a certificate of the fact that the graph has no balanced cut of conductance less than  somewhat  surprisingly,  we can obtain a certificate for the stronger correlation condition. 
This is achieved via a novel separation oracle for our SDP and by appealing to Arora and Kale's \cite{AK}  framework to bound the running time.
Our result contains technical ingredients that may be  of independent interest.
\end{abstract}

\section{Introduction}


\subsection{Graph Partitioning.}
Given a graph   the conductance of
a cut  is   where  is the sum of the degrees of the vertices in the set . 
A cut  is -balanced if  
A graph partitioning problem of widespread interest is the  \BS  problem: given  a constant\footnote{
We will use  and  in our asymptotic notation when we want to emphasize the dependence of the hidden coefficent on }
balance parameter  and a conductance value  does  have a -balanced cut  such that ?

\BS is an intensely studied problem in both theory and practice.  
It has far-reaching connections to spectral graph theory, the study of random walks and metric embeddings. Besides being a theoretically rich problem, \BS is of great practical importance, as it plays a central role in the design of recursive algorithms, image segmentation and clustering. 


Since {\BS}  is an  NP-hard problem~\cite{GareyJ79}, we seek approximation algorithms that either output a cut of conductance at-most  and balance  or a certificate that  has no -balanced cut of conductance at most  
In their seminal series of papers~\cite{ST1, ST2, ST3}, Spielman and Teng use an approximation algorithm for {\BS} as a fundamental primitive to decompose the instance graph into a collection of near-expanders. This decomposition is then used to construct spectral sparsifiers and solve systems of linear equations in nearly linear time. Their algorithm has two crucial features: first, it runs in nearly linear time; second, in the case that no balanced cut exists in the graph, it outputs a certificate of a special form. This certificate consists of an unbalanced cut of small conductance which is well-correlated with all low-conductance cuts in the graph. We prove in Section \ref{app:cut} in the Appendix that such a cut is indeed a negative certificate for the \BS problem.
Formally, they prove the following:
\begin{theorem}\cite{ST1} \label{thm:st}
Given a graph  , a balance parameter  and a conductance value  {\sc Partition} runs in time  and outputs a cut  such that    or   and with high probability, either
\begin{enumerate}

	\item  is -balanced, or 
	\item for all  such that  and   

\end{enumerate}
\end{theorem}


\noindent
Originally, Spielman and Teng showed Theorem \ref{thm:st} with  and  This was subsequently improved by Andersen, Chung and Lang \cite{ACL} and then by Andersen and Peres \cite{AP} to the current best of  and  All these results made use of bounds on the convergence of random walk processes on the instance graph, such as the Lovasz-Simonovits bounds \cite{LS}. These bounds yield the  factor in the approximation guarantee, which appears hard to remove while following this approach.

\subsection{Our Contribution}


In this paper,  we use a semidefinite programming approach to design a new spectral algorithm, called {\sc BalCut}, that improves on the result of Theorem \ref{thm:st}. The following is our main result. 


\begin{theorem}[Main Theorem] \label{thm:main}
Given a graph  , a balance parameter  and a conductance value  {\sc BalCut} runs in time  and outputs a cut   such that   if  then   and with high probability, either
\begin{enumerate}
\item  is -balanced, or 
\item for all  such that  and   

\end{enumerate} 
\end{theorem}


\noindent
Note that our result improves the parameters  of previous algorithms by eliminating the  factor in the quality of the cut output, making the approximation comparable to the best that can be hoped for using spectral methods~\cite{GM}. Our result is also conceptually simple: we use the primal-dual framework of Arora and Kale \cite{AK} to solve {\SDP}s combinatorially, and we give a new separation oracle that yields  Theorem \ref{thm:main}.
Finally, our result implies an approximation algorithm for {\BS}, as the guarantee of Theorem \ref{thm:main} on the cut  output by \alg also implies a lower bound on the conductance of balanced cuts of  The proof can be found in Section \ref{app:cut} in the Appendix.
\begin{corollary}\label{cor:cut}
Given an instance graph  a balance parameter  and a target conductance  \alg either outputs an -balanced cut of conductance at most  or a certificate that all -balanced cuts have conductance at least  The running time of the algorithm is 
\end{corollary}
This is the first nearly-linear-time spectral algorithm for \BS that achieves the asymptotically optimal approximation guarantee for spectral methods.

\subsection{Graph Decomposition.} The main application of Theorem \ref{thm:st} is the construction of a particular kind of graph decomposition. In this decomposition, we wish to partition the vertex set of the instance graph  into components  such that the graph induced by  on each  has conductance as large as possible, while at most a constant fraction of the edges have endpoints in different components. These decompositions are a useful algorithmic tool in several areas \cite{Trevisan05, KM, ST2}.

Kannan, Vempala and Vetta \cite{Kannan} construct such decompositions achieving a conductance value of  However, their algorithm runs in time  on some instances. 

Spielman and Teng~\cite{ST2} relax this notion of decomposition by only requiring that each  be contained in a superset  in , where  has large induced conductance in  In the same work, they show that this relaxed notion of decomposition suffices for the purposes of sparsification by random sampling. The advantage of this relaxation is that it is now possible to compute this decomposition in nearly-linear time by recursively applying the algorithm of Theorem \ref{thm:st}.
\begin{theorem} \label{thm:dec} \cite{ST2} 
Assume the existence of an algorithm achieving parameters  and  in Theorem \ref{thm:st}. Given  in time  it is possible to construct a decompositions of the instance graph  into components  such that:
\begin{enumerate}
\item for each  there exists  such that the conductance of the graph induced by  on  is  
\item the fraction of edges with endpoints in different components is 
\end{enumerate}
\end{theorem}
Using Theorem~\ref{thm:dec}, Spielman and Teng showed the existence of a decomposition achieving conductance 
Our improved results in Theorem \ref{thm:main} imply that we can obtain decompositions of the same kind with conductance  bound  Our improvement also implies speed-ups in the sparsification procedure described by Spielman and Teng~\cite{ST2}. However, this result has since been superceded by work of Koutis, Miller and Peng~\cite{KMP} that gives a very fast linear equation solver that can be used to compute sampling probabilities for each edge, yielding a spectral sparsifier with high probability~\cite{SS}.

Our work leaves open the important question posed by Spielman~\cite{SpielmanICM} of whether stronger decompositions, of the kind proposed by Kannan, Vempala and Vetta~\cite{Kannan}, can be produced in nearly-linear time.




\subsection{Overview of Techniques}
\paragraph{Spectral Approach.}
The simplest algorithm for \BS, also used by Kannan et al.~\cite{Kannan}, is the recursive spectral algorithm. 
This algorithm finds the minimum-conductance sweep cut of the second eigenvector of ,  removes the cut and all adjacent edges from  and reiterates on the remaining graph. The algorithm stops when the union of the cuts removed becomes -balanced or when the residual graph is found to have spectral gap at least  certifying that no more progress can be made. As every cut may only remove  volume  and the eigenvector computation takes  time, this algorithm may have quadratic running time. It can be shown using Cheeger's Inequality \cite{FAN} that the cut this procedure outputs is of conductance at most  
\paragraph{Spielman-Teng Approach.}
The algorithm of Spielman and Teng which proves Theorem~\ref{thm:st} is also spectral in nature and  uses, as the main subroutine, {\it local random walks} that run in time proportional to the volume of the output cut to find sparse cuts around vertices of the graphs. These local methods are based on  non-trivial random walks on the input graph and aggregation of the information obtained from these walks, all performed while maintaining  nearly-linear running time.
\paragraph{Our Approach.} 
We depart from the random-walk paradigm and first consider  a natural {{\SDP}} relaxation for the {\BS} problem, which \alg solves approximately using a primal-dual method.
Intuitively, \alg manages to maintain the approximation guarantee of the recursive spectral algorithm while running in nearly-linear time by considering a distribution over eigenvectors,  represented as a vector embedding of the vertices, rather than a single eigenvector, at each iteration. The sweep cut over the eigenvector is replaced by a sweep cut over the radius of the vectors in the embedding (see Figure~\ref{fig:hedging}).
Moreover, at any iteration, rather than removing the unbalanced cut found, \alg penalizes it by modifying the graph so that it is unlikely but still possible for it to turn up in future iterations. Hence, in both its cut-finding and cut-eliminating procedures, \alg tends to ``hedge its bets" more than the greedy recursive spectral method. This hedging, which ultimately allows \alg to achieve its faster running time, is implicit in the primal-dual framework of Arora and Kale~\cite{AK}.

\begin{figure}[!h]
\begin{center}
\includegraphics[ clip=true, scale=.45]{hedging2.pdf}
\caption{Schematic representation of the speed-up introduced by \alg when the instance graph contains many unbalanced cuts of low conductance. Let  and  be the two slowest-mixing eigenvectors of  Assume that their minimum-conductance sweep cuts  and  are unbalanced cuts of conductance less than  If we use the recursive algorithm of Kannan et al.~\cite{Kannan}, two iterations could be required to remove  and . However, \alg considers a multidimensional embedding containing contributions from multiple eigenvectors and performs a radial sweep cut. This allows  and  to be removed in a single iteration.}
\label{fig:hedging}
\end{center}
\vspace{-8mm}
\end{figure}



The {{\SDP}} relaxation appears in Figure \ref{fig:sdp-intro}.
We denote by  the distribution defined as  and by  the degree of the -th vertex. Also, 
Even though our algorithm uses the {{\SDP}}, at the core, it is spectral in nature, as it relies on the matrix-vector multiplication primitive. Hence, if one delves deeper, a random walk interpretation can be derived for our algorithm.
\begin{figure}[htb]


\label{fig:sdp-intro}
\caption{{\SDP} for -\BS}
\end{figure}





\paragraph{The Primal-Dual Framework.}
For our {\SDP}, the method of Arora and Kale can be understood as a game between two players: an embedding player and an  oracle player. The embedding player, in every round of this game, gives a candidate vector embedding of the vertices of the instance graph to the oracle player. We show that, if we are lucky and  the embedding is feasible for the {\SDP} and, in addition, also has the property that for a large set  for every    (we call such an embedding roundable), then a projection of the vectors along a random direction followed by a  sweep cut  gives an -balanced cut of conductance at most  The  difficult case is when the embedding given to the oracle player is not roundable. In this case, the oracle outputs a candidate dual solution along with a cut. The oracle obtains this  cut  by performing a radial sweep cut of the vectors given by the embedding player.  If at any point in this game the union of cuts output by the oracle becomes balanced, we output this union and stop. We show that such a cut is of conductance at most   If this union of cuts is not balanced, then the embedding player uses the dual solution output by the oracle to update the embedding.
Finally, the matrix-exponential update rule ensures that this game cannot keep on going  for more that  rounds. Hence, if a balanced cut is not found after this many rounds, we certify that the graph does not contain any -balanced cut of conductance less than   
To achieve a nearly-linear running time, we  maintain only  a -dimensional sketch of the embedding.  The guarantee on the running time then follows by noticing that,  in each iteration,  the most expensive computational step for each player is a logarithmic number of matrix-vector multiplications, which takes at most  time. 

The reason why our approach yields the desired correlation condition in Theorem \ref{thm:main} is that, if no balanced cut is found, every unbalanced cut of conductance lower than  will, at some iteration, have a lot of its vertices mapped to vectors of large radius. At that iteration, the cut output by the oracle player will have a large correlation with the target cut, which implies that the union of cuts output by the oracle player will also display such large correlation. This intuition is formalized in the proof of Theorem \ref{thm:main}.





\paragraph{Our Contribution.}
The implementation of the oracle player, specifically dealing with the case when the embedding is not roundable,   is the main technical novelty of the paper. Studying the problem in the SDP-framework is the main conceptual novelty. 
The main advantage of using {\SDP}s to design a spectral algorithm seems to be that {\SDP} solutions provide a  simple representation for possibly complex random-walk objects. 
Furthermore, the benefits of using a carefully designed {\SDP} formulation can often  be reaped with little or no burden on the running time of the algorithm, thanks to the primal-dual framework of Arora and Kale~\cite{AK}.  


\subsection{Rest of the Paper}
In Section~\ref{sec:notation}, we set the notation for the paper. In Section~\ref{sec:sdp},  we present our {\SDP} and its dual, and also define the notion of a roundable embedding. In Section~\ref{sec:pd},  we present the algorithm {\sc BalCut} and the separation oracle {\sc Oracle}, and reduce the task of  proving  Theorem \ref{thm:main} to proving statements about the {\sc Oracle}. Section~\ref{sec:mp} contains the proof of the main theorem about the {\sc Oracle} used in Section~\ref{sec:pd}.  For clarity of presentation, several proofs are omitted from the above sections and appear in the appendix. 


\section{Algorithm Statement and Main Theorems}



\subsection{Notation} \label{sec:notation}


\paragraph{Instance graph and edge volume.} We denote by  the unweighted instance graph, where  and  We let  be the degree vector of  i.e.  is the degree of vertex  We mostly work with the edge measure  over  defined as  For a subset  we also define  as the edge measure over , i.e. 



\paragraph{Special graphs}
For a subset  we denote by  the complete graph over  such that edge  has weight  for  and  otherwise.  is the complete graph with weight  between every pair 




\paragraph{Graph matrices.} For an undirected graph , let
 denote the adjacency matrix of  and  the diagonal matrix of
degrees of .
The (combinatorial) Laplacian of  is defined as .
Note that for all , .
By  and , we denote  and  respectively.




\paragraph*{Vector and matrix notation.}

For a symmetric matrix  we will use  to denote that it is positive semi-definite and  to denote that it is positive definite.
The expression  is equivalent to . For two matrices  of equal dimensions, denote  
For a matrix , we indicate by  the time necessary to compute the matrix-vector multiplications  for any vector . 



\paragraph{Embedding notation.} We will deal with vector embeddings of , where each vertex  is mapped to a vector  For such an embedding  we denote by  the mean vector, i.e. 
Given a vector embedding of  recall that  is the Gram matrix of the embedding if  For any  we call  the {\it embedding corresponding to } if  is the Gram matrix of  
For  we denote by  the matrix such that 




\paragraph{Basic facts.} We will alternatively use vector and matrix notation to reason about the graph embeddings. 
The following are some simple conversions between vectors and matrix forms and some basic geometric facts which follow immediately from definitions.
\begin{fact}\label{fct:mean}

\end{fact}
\begin{fact} \label{fct:star} For a subset  

\end{fact}
\begin{fact} \label{fct:subset}
For a subset  
\end{fact}



\paragraph{Modified matrix exponential update.}
Let  e the subspace of  orthogonal to  and let  be the identity over  i.e. 
 For a positive  and a symmetric matrix  we define 

The following fact about  will also be needed:
\begin{fact}\label{fct:identity}

\end{fact}

\subsection{\SDP Formulation} \label{sec:sdp}

We consider an  relaxation to the decision problem of determining whether the instance graph  has a -balanced cut of conductance at most 
The  feasibility program  appears in Figure  where we also rewrite the program in matrix notation, using Fact \ref{fct:mean} and the definition of 
\begin{figure}[htb]

\begin{minipage}[b]{0.5\linewidth}

\end{minipage}
\begin{minipage}[b]{0.5\linewidth}

\end{minipage}
\label{fig:sdp}
\caption{ for -\BS}
\end{figure}
 can be seen as a scaled version of the balanced-cut  of \cite{ARV}, modified by replacing  for the origin and removing the triangle-inequality constraints. The first change makes our  invariant under translation of the embeddings and makes the connection to spectral methods more explicit. Indeed, the first two constraints of \psdp now exactly correspond to the standard eigenvector problem, with the addition of the  constraint ideally forcing all entries in the eigenvector not to be too far from the mean, just as it would be the case if the eigenvector exactly corresponded to a balanced cut. 
The removal of the triangle-inequality constraints causes  to only deal with the spectral structure of  and not to have a flow component.
For the rest of the paper, denote by  the set 

\noindent
The following simple lemma establishes that  is indeed a relaxation for the integral decision question  and is proved in Section \ref{app:basic}. 
\begin{lemma}[SDP is a Relaxation]\label{lem:relax} If there exists a -balanced cut  with  then  has a feasible solution.
\end{lemma}


\noindent
\alg will use the primal-dual approach of \cite{AK} to determine the feasibility of  When \psdp is infeasible, \alg will output a solution to  the dual  shown in Figure \ref{fig:dspd}. 

\begin{figure}[htb]
 \caption{ feasibility problem}
\label{fig:dspd}
\end{figure} 
\noindent
In the rest of the paper, we are going to use the following shorthands for the dual constraints

Notice that  is a scalar, while  is a matrix in 
Given  a choice of  such that  and  corresponds to a hyperplane separating  from the feasible region of  and constitutes a certificate that  is not feasible. 

\noindent
Ideally, \alg would produce a feasible solution to \psdp and then round it to a balanced cut. However, as discussed in \cite{AK}, it often suffices to find a solution ``close'' to feasible for the rounding procedure to apply. In the case of \psdp, the concept of ``closeness'' is captured by the notion of {\it roundable} solution.
\begin{Definition}[Roundable Embedding]\label{def:roundable}
 Given an embedding  let  We say that  is a {\it roundable} solution to  if:
\begin{itemize}
\item ,
\item ,
\item 
\end{itemize} 
\end{Definition}

\noindent
A roundable embedding can be converted into a balanced cut of the conductance required by Theorem \ref{thm:main} by using a standard projection rounding, which is a simple extension of an argument already appearing in \cite{ARV} and \cite{AK}. The rounding procedure {\sc ProjRound} is described precisely in Section \ref{app:round}, where the following theorem is proved.
\begin{theorem}[Rounding Roundable Embeddings]\label{thm:stdround}
If  is a roundable solution to , then {\sc ProjRound} produces a - balanced cut of conductance  with high probability in time 
\end{theorem}

\subsection{Primal-Dual Framework} \label{sec:pd}


\begin{figure*}[htb]
  	\begin{tabularx}{\textwidth}{|X|}
    \hline
	 \vspace{1mm}

  {\bf \textsc{Input:}} An instance graph  a balance value  such that  a conductance value 
	\vspace{1mm}

Let  
	For 
	\begin{itemize}
	\item Compute the embedding  corresponding to  If  
   \item Execute {\sc Oracle}
	
	\item If {\sc Oracle}\xspace finds that  is roundable, run {\sc ProjRound} output the resulting cut and terminate. 
	\item Otherwise, {\sc Oracle}\xspace outputs coefficients  and cut  
	
	\item Let  If  is -balanced, output  and terminate.

	\item Otherwise, let  and proceed to the next iteration.
  \end{itemize}

Output  Also output   and  
\\
\\
\hline 
\end{tabularx}
  \caption{The \alg Algorithm}
  \label{fig:algorithm}
\end{figure*}



\paragraph{Separation Oracle.} The problem of checking the feasibility of a {\SDP} can be reduced to that of, given a candidate solution  to check whether it is close to feasible and, if not, provide a certificate of infeasibility in the form of a hyperplane separating  from the feasible set. The algorithm performing this computation is known as a separation oracle. 
Arora and Kale show that the original feasiblity problem can be solved very efficiently if there exists a separation oracle obeying a number of conditions. We introduce the concept of {\it good} separation oracle to capture these conditions for the program 

\begin{Definition}[Good Separation Oracle] An algorithm is a {\it good} separation oracle if, on input some representation of  the algorithm either finds  to be a  roundable solution to  or outputs coefficents  such that    and 
\end{Definition}

\begin{comment}
More formally \Authormarginnote{Lorenzo}{and specifically to our case, must emphasize otherwise confusing, must say slight modification, these are our defintions}, let  be the set of feasible solutions to   A -separation oracle is an algorithm that, given  as input, checks wether  is "close" to  In our case,  is considered "close" to  if it can be rounded w.h.p. to produce a -balanced cut of conductance 
If  is not "close" to  a -separation oracle exhibits a separating hyperplane by outputing  such that  and 
A separation oracle has  width if, in the case that  is not "close" to  the output coefficents  obey 
\end{comment}

\paragraph{Algorithmic Scheme.} 
We adapt the techniques of \cite{AK} to our setting, where we require feasible solutions to be in  rather than having trace equal to 1.  The argument is a simple modification of the anaylsis of \cite{AK} and in \cite{Steurer}.
The algorithmic strategy of \cite{AK} is to produce a sequence of candidate primal solutions  iteratively, such that  for all  

Our starting point  will be the solution  
At every iteration, a {\it good} separation oracle {\sc Oracle}\xspace will take  and either guarantee that  is roundable or output coefficents  certifying the infeasiblity of  The algorithm makes use of the information contained in  by updating the next candidate solution as follows:

where  is a parameter of the algorithm. The following is immediate.
\begin{lemma}\label{lem:delta} For all  
\end{lemma}

\noindent
Following \cite{AK}, we prove that, after a small number of iterations this algorithm either yields a roundable embedding or a feasible solution to \dsdp
We present the proof in Section \ref{app:ak} for completeness.
\begin{theorem}[Iterations of Oracle, \cite{AK}]\label{thm:ak}
Let  Assume that the procedure  is a {\it good} separation oracle . Then, after  iterations of the update of Equation \ref{eqn:update},  we either find a roundable solution to  or the coefficents   and  are a feasible solution to  
\end{theorem}


\paragraph{Approximate Computation.} 
Notice that, while we are seeking to construct a nearly-linear-time algorithm, we cannot hope to compute  exactly and explicitly, as just maintaining the full  matrix requires quadratic time in   
Instead, we settle for a approximation   to  which we define as

The function  is a randomized approximation to  obtained by applying the Johnson-Linderstrauss dimension reduction to the embedding corresponding to   is described in full in Section \ref{app:exp}, where we also prove the following lemma about the accuracy and sparsity of the approximation. It is essentially the same argument appearing in \cite{Kthesis} applied to our context.

\begin{lemma}[Approximate Computation]\label{lem:approx}
Let 
For a matrix   let  and 
\begin{enumerate}
\item  and 
\item The embedding  corresponding to  can be represented in  dimensions.
\item  can be computed in time 
\item for any graph , with high probability
 and, for any vertex 


\end{enumerate}
where 
\end{lemma}
This lemma shows that   is a close approximation to  
We will use this lemma to show that {\sc Oracle}\xspace can receive   as input, rather than   and still meet the conditions of Theorem \ref{thm:ak}.
In the rest of the paper, we assume that  is represented by its corresponding embedding 


\paragraph{The Oracle.} {\sc Oracle}\xspace is described in Figure \ref{fig:oracle}.
 We show that {\sc Oracle}\xspace on input  meets the condition of Theorem \ref{thm:ak}. Moreover, we show that {\sc Oracle}\xspace obeys an additional condition, which, combined with the dual guarantee of Theorem \ref{thm:ak} will yield the correlation property of {\sc BalCut}.
\begin{theorem}[Main Theorem on {\sc Oracle}] \label{thm:oracle}
On input  {\sc Oracle} runs in time  and is a {\it good} separation oracle for  Moreover, the cut  in Step \ref{stp:cut} is guaranteed to exist.

\end{theorem}

\begin{figure*}[htb]
  	\begin{tabularx}{\textwidth}{|X|}
    \hline
  	\begin{enumerate}
\item {\bf \textsc{Input:}} The embedding  corresponding to  Let  for all 
Denote 
\item {\sc Case 1}:   
	Output   and 
	\item {\sc Case 2}: not {\sc Case 1} and  Then  is roundable, as  implies 
\item \label{stp:cut} {\sc Case 3}: not {\sc Case 1} or {\sc 2}. Relabel the vertices of  such that  and let  be the -th sweep cut of  
Let  the smallest index such that  	Let  the most balanced sweep cut among  such that   Output   for  and  for  Also output the cut  
 \end{enumerate}\\
   \hline
    \end{tabularx}
  \caption{{\sc Oracle}}
  \label{fig:oracle}
\end{figure*}


\paragraph{Proof of Main Theorem.}  We are now ready to prove Theorem \ref{thm:main}. To show the overlap condition,  we consider the dual condition implied by Theorem \ref{thm:ak} together with the cut  and the values of the coefficents output by the {\sc Oracle}.


\begin{proof}[Proof of Theorem \ref{thm:main}]
If at any iteration  the embedding  corresponding to   is roundable, the standard projection rounding {\sc ProjRound}\xspace produces a cut of balance  and conductance  by Theorem \ref{thm:stdround}. Similarly, if for any   is -balanced, \alg satisfies the balance condition  in Theorem \ref{thm:main}, as  because  is the union of cuts of conductance at most 


Otherwise, after  iterations, by Theorem \ref{thm:ak}, we have that   and  constitute a feasible solution   This implies that
 i.e.


For any cut  such that  and  let the embedding  be defined as  for  and  for 
Then  and  Moreover,   
Let  be the Gram matrix of  

We apply the lower bound of Equation \ref{eqn:dual} to  By Facts \ref{fct:mean} and \ref{fct:star}.


Recall that, by the definition of {\sc Oracle}, for all   and  for  and  for  Hence,


Dividing by  and using the fact that  and  we obtain

Now,

so that we have

Moreover, being the union of cuts of conductance   also has 
As   This finally implies that

Finally,  both {\sc ProjRound} and {\sc Oracle} run in time  as the embedding is  dimensional. By Lemma \ref{lem:approx}, the update at time  can be performed in time  where  This is a matrix of the form  The first two terms can be multiplied by a  vectors in time  while the third term can be decomposed as  by Fact \ref{fct:mean} and can therefore be also multiplied in time 
Hence, each iteration runs in time  which shows that the total running time is  as required.







\end{proof}



\section{Proof of Main Theorem on {\sc Oracle}} \label{sec:mp}


\subsection{Preliminaries}

The following is a variant of the sweep cut argument of Cheeger's Inequality \cite{FAN}, tailored to ensure that a constant fraction of the variance of the embedding is contained inside the output cut.
For a vector  let  be the set of vertices where  is not zero. 
\begin{lemma} \label{lem:cheeger}
Let  such that   and  Relabel the vertices so that  and  For  denote by  the sweep cut  
Further, assume that  and, for some fixed  
Then, there is a sweep cut  of  such that  and 
\end{lemma}

\noindent
We will also need the following simple fact.
\begin{fact} \label{fct:triangle}
Given  
\end{fact}





\subsection{Proof of Theorem \ref{thm:oracle}}

\begin{proof}
Notice that, by Markov's Inequality,   Recall that 
\begin{itemize}
\item {\sc Case 1}:  We have  and, by Lemma \ref{lem:approx}, 

\item {\sc Case 2}:  Then  is {\it roundable} by Definition \ref{def:roundable}.

\item {\sc Case 3}:   This means that, by Fact \ref{fct:subset},  Hence, by Fact \ref{fct:star},


We then have  for some  where we also  denote by    the  largest coordinates dictated by the sweep cut  
Let  be the the vertex in  such that  and   
By the definition of  we have  and  Hence, we have  for all 
Define the vector  as  for  and  for  
Notice that:

Also,  and  by the definition of  Moreover,
 and

Hence, by Lemma \ref{lem:cheeger}, there exists a sweep cut  with  such that  This shows that  as defined in Figure \ref{fig:oracle} exists. Moreover, it must be the case that  As  we have

Recall also that, by the construction of  
Hence, we have 


\end{itemize}
This completes all the three cases.
Notice that in every case we have:

\noindent
Hence,


\noindent
Finally,  using the fact that  is embedded in  dimensions, we can compute  in time   can also be computed in time  by using the decomposition   where  is the mean of vectors representing vertices in  
The sweep cut over  takes time  Hence, the total running time is  
\end{proof}



\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{balanced}

\appendix
\section{Appendix}

\subsection{Proof of Corollary \ref{cor:cut}}\label{app:cut}
\begin{lemma}
If only the second condition in Theorem \ref{thm:st} holds, then  has no -balanced cut of conductance 
\end{lemma}
\begin{proof}
We may assume that the cut  output by \alg is not -balanced. Then, by the second condition, any cut  with  and  must have 
Hence,  This implies that there are no -balanced cuts of conductance less than 
\end{proof}

\subsection{Proof of Basic Lemmata} \label{app:basic}

\begin{proof}[Proof of Lemma \ref{lem:relax}]
For a -balanced cut  with  Without loss of generality, assume  Consider the one-dimensional solution assigning  to  and  to  Notice that  and that  for 
We then have:
\begin{itemize}
 \item 
\item
 
\item for all  


\noindent
where the last inequality follows as  is -balanced.
\end{itemize}
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:cheeger}]
For all  let 
By Cauchy-Schwarz,

Hence,

Then, let  be the conductance of the least conductance cut among 

Hence, 



\end{proof}

\subsection{Primal-Dual Framework}\label{app:ak}

\subsubsection{Preliminaries}
Recall that  is the subspace of  orthogonal to  and that  be the identity over 

Define 


The following simple facts will be useful.
\begin{fact}\label{fct:exp}
Let  be a symmetric matrix in  such that  Then, 
\end{fact}

\begin{theorem}\label{thm:expo}
Let  and let  be a sequence of symmetric matrices in  such that, for all   and 
Then:

where 
\end{theorem}

\begin{comment}
\begin{proof}
Let 
Notice:

By the Golden-Thompson Inequality we have:

Moreover, by Fact \ref{fct:exp}

Hence,

Iterating this argument over  we obtain:

\end{proof}
By taking logs this implies:

Finally, using the fact that  yields the required result.
\end{comment}
\subsubsection{Proofs}

 In the following, let  Also, let

Then,  we have 


\begin{proof}[Proof of Lemma \ref{lem:delta}]
By Fact \ref{fct:identity},

\end{proof}
              
We are now ready to complete the proof of Theorem \ref{thm:ak}.                                                                                                                                                                                                                                                                                                                                                                                               \begin{proof}[Proof of Theorem \ref{thm:ak}]
Suppose that {\sc Oracle}\xspace outputs coefficents  for  iterations.
Then for all  by the definition of {\it good} oracle 

Notice that, as  are the output of a {\it good} oracle, we have
 which implies 
Hence, we can apply Theorem \ref{thm:expo} to obtain:

This implies

and by the definition of 

Hence,

and

By picking  and  we obtain 
As  this also implies

Finally, by the definition of {\it good} oracle,  Hence,  is a solution to 
\end{proof}

\subsection{Projection Rounding}\label{app:round}

\begin{figure*}[h]
  \begin{tabularx}{\textwidth}{|X|}
    \hline
  \begin{enumerate}
  \item {\bf \textsc{Input:}} An embedding  
  \item Let  be a constant to be fixed in the proof. 
\item For :
\begin{enumerate}
\item Pick a unit vector  uniformly at random from  and let  with .
\item Sort the vector  Assume w.l.og. that . Define .
   
  \item  Let  which minimizes
   among sweep-cuts for which  
\end{enumerate}
\item {\bf \textsc{Output:}} The cut  of least conductance over all choices of  
  \end{enumerate}\\
   \hline
    \end{tabularx}

  \caption{{\sc ProjRound}}
  \label{fig:rounding}
\end{figure*}

The description of the rounding algorithm {\sc ProjRound} is given in Figure \ref{fig:rounding}. We remark that during the execution of \alg the embedding  will be represented by a projection over  random directions, so that it will suffice to take a balanced sweep cut of each coordinate vector.

We now present  the proof of Theorem \ref{thm:stdround}. 
The constants in this argument were not optimized to preserve the simplicity of the proof.

\subsubsection{Preliminaries.} We will make use of the following simple facts. Recall that for   if  and  otherwise.



\begin{fact}\label{fct:ab}
For all   \end{fact}
\begin{proof}
 
\end{proof}

\begin{fact}\label{fct:abs}
For all   \end{fact}
\begin{proof} 

\begin{enumerate}
\item If  then   as 
\item If  then since   
Hence, 
\end{enumerate}
\end{proof}

\begin{fact}\label{fct:ab2}
For all   \end{fact}
\begin{proof} 

\begin{enumerate}
\item If    as  Since   
\item If   Here, we have used Fact \ref{fct:ab}. 
\end{enumerate}
\end{proof}



We also need the following standard facts.
\begin{fact}\label{fct:prob}
Let  be a vector of length  and  a unit vector chosen uniformly at random in . Then,
\begin{enumerate}
\item  and 
\item  for ,

\end{enumerate}
\end{fact}




\begin{fact}\label{fct:antimarkov}
Let  be a non-negative random variable such that   and  Then,   
  
\end{fact}





The following lemma about projections will be crucial in the proof of Theorem \ref{thm:stdround}. It is a simple adaptation of an argument appearing in \cite{ARV}.
\begin{lemma}[Projection]\label{lem:projection}
Given a roundable embedding   consider the embedding  such that  where  and assume without loss of generality that     Then, there exists   such that with  probability  over the choice of , the following conditions hold simultaneously:
\begin{enumerate}
\item ,
\item  and 
\item there exists  with  and, there exists  such that  such that .
\end{enumerate}
\end{lemma}

\begin{proof}
We are going to lower bound the probability, over ,  of each of (1), (2) and (3) in the lemma and then apply the union bound. 

\paragraph{Part (1).} 
By applying Fact \ref{fct:prob} to  and noticing  , we have

Hence, by Markov's Inequality, for some  to be fixed later


\paragraph{Part (2).}
 Hence, for some  be fixed later 

 
\paragraph{Part (3).}
Let  
Let 
By Markov's Inequality, 
As  is roundable, for all   Hence,    for such  
This, together with the roundability of , implies that

For any , we can apply the triangle inequality for the Euclidean norm as follows

Hence, for all 

Let  be the set . 
Since , applying Fact \ref{fct:antimarkov} yields that, for all , 

For all vertices , by Fact \ref{fct:prob}

Let  Consider the event 
Then,


Hence, from Fact \ref{fct:antimarkov}, with probability at least  over directions  for a fraction  of pairs    
Let  be the median value of .
Let  and . Any pair  with  has at least one vertex in . 
Hence,

Assume  otherwise, apply the same argument to .
Let  be the largest index in  
For all  and  such that , we have . (Similarly, let  be the smallest index in .)
This implies that, 

with probability at least , satisfying the required condition.
Let  be the probability that this event does not take place. Then,


\noindent
To conclude the proof, notice that the probability that all three conditions do not hold simultaneously is, by a union bound, at most . Setting , we satisfy the first and third conditions and obtain

Hence, all conditions are satisfied at the same time with probability at least .
\end{proof}
\noindent
From this proof, it is possible to see that the parameter  in our rounding scheme should be set to 


We are now ready to give a proof of Theorem \ref{thm:stdround}. It is essentially a variation of the proof of Cheeger's Inequality, tailored to produce balanced cuts.





\begin{proof}[Proof of Theorem \ref{thm:stdround}]
For this proof, assume that  has been translated so that  Notice that the guarantees of  still apply.
Let  and  be as promised by Lemma \ref{lem:projection}. 
For  let  be  if  and  otherwise.
Let 

 Hence,

Now we lower bound  Notice that if  then  and vice-versa. Hence, 

Let   and let  be the minimum conductance of  over all    

Hence,  with constant probability over the choice of projection vectors  Repeating the projection  times and picking the best balanced cut found yields a high probability statement. 
Finally, as the embedding is in  dimensions, it takes  time to compute the projection. After that, the one-dimensional embedding can be sorted in time  and the conductance of the relevant sweep cuts can be computed in time  so that the total running time is 
\end{proof}

\subsection{Proof of Lemma \ref{lem:approx}}\label{app:exp}


\subsubsection{Preliminaries}
For the rest of this section the norm notation will mean the norm in the subspace  Hence 
We will need the following lemmata.

\begin{lemma}[Johnson-Lindenstrauss]\label{lem:jl}
Given an embedding ,  let , be vectors sampled independently uniformly from the -dimensional sphere of radius  Let  be the  matrix having the vector  as -th row and let . Then, for  for all  

and

\end{lemma}

\begin{lemma}[\cite{Kthesis}]\label{lem:expv} 
There exists an algorithm {\sf EXPV} which, on input of a matrix , a vector  and a parameter , computes a vector , such that  in time 
\end{lemma}
\noindent
The algorithm {\sf EXPV} is described in \cite{Kthesis} and \cite{YPS} .

\subsubsection{Proof}

We define the  algorithm in Figure \ref{fig:approx} and proceed to prove Lemma \ref{lem:approx}.
\begin{figure*}[h]
  \begin{tabularx}{\textwidth}{|X|}
    \hline
  \begin{itemize}
\item {\bf \textsc{Input:}} A matrix 
\item Let  Let  and 
\item For  as in Lemma \ref{lem:jl}, sample  vectors  as in Lemma \ref{lem:jl}.
\item Let 
\item For  compute vectors 

 \item Let  be the matrix having  as -th row,  and let  be the -th column of . Compute 
\item  Return , by giving its correspoding embedding, i.e.,  

  
  \end{itemize}\\
   \hline
    \end{tabularx}

  \caption{The  algorithm}
  \label{fig:approx}
\end{figure*}


\begin{proof}
We verify that the conditions required hold.
\begin{itemize}
\item By construction, , as  and 
\item  and  is a  matrix, with  by Lemma \ref{lem:jl}.
\item We perform  calls to the algorithm {\sf EXPV}, each of which takes time  Sampling the vectors  and computing  also requires  time. Hence, the total running time is 

\item Let  be the  matrix having the sampled vectors  as rows. 
Let 
be the embedding corresponding to matrix  i.e.,  is the -th column of  Notice that 
Define  for all  and let  be the Gram matrix corresponding to this embedding, i.e., 
Also, let  be the Gram matrix corresponding to the embedding  i.e.,  and 
We will relate  to  and  to  to complete the proof.

First, by Lemma \ref{lem:jl}, applied to , with high probability, for all 

and for all 

In particular, this implies that 
Hence,

and for all 


Now we relate  and . Let  
By Lemma \ref{lem:expv} 

This also implies

As  we have

and


Finally, combining these bounds we have

by taking  sufficiently small in 

Hence, as  and 
 
and

This, together with the fact that  and  completes the proof.
\end{itemize}


\end{proof}





\end{document}
