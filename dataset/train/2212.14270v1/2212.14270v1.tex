\section{Experiments}

\subsection{Setting}

\noindent\textbf{Dataset and Metrics.} We evaluate \textbf{KLG} on TACRED \cite{DBLP:conf/emnlp/ZhangZCAM17}, TACRED-Revisit \cite{DBLP:conf/acl/AltGH20a}, and SemEval2010 \cite{DBLP:conf/semeval/HendrickxKKNSPP10}, three commonly used relation extraction datasets. Due to the space limitation, we present detailed information about these datasets in the Appendix \ref{dataset}. Following previous work, we use the Micro-F1 score excluding 'No Relation' as the metric. Besides, we also report the precision, recall, and Macro-F1 score in our detailed analysis.



\noindent\textbf{Model Details.} We use Pytorch \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19} and RoBERTa-large as the text encoder for both parts of \textbf{KLG}. The checkpoint can be downloaded here. \footnote{https://huggingface.co/roberta-large} The batch size is 16, and the optimizer is AdamW \cite{DBLP:conf/iclr/LoshchilovH19} with a 1e-5 learning rate and a warm-up strategy. The maximum training epoch is 10, and the maximum input length is 256. The scalar temperature parameter $\tau$ is 0.05, and the loss weighting factor $\alpha$ is 0.9. We set $k$ as the \textit{Top-$k$} recall achieves 0.99 on the dev set. The checkpoint that achieves the best result on the dev set is used for testing. \footnote{Please refer to Appendix \ref{param} for the detailed information of parameter searching.} We conduct each experiment three times and report the average result to reduce the randomness. Besides, we also report the performances using different backbone networks. Please refer Appendix \ref{othermodel} for more details.

\subsection{Comparison Methods}\label{comparison}

We compare \textbf{KLG} with state-of-the-art RE systems that represent a diverse array of approaches. 

\textbf{Classification-based Methods.} These methods fine-tune PLMs on RE datasets with a standard classification loss. \textbf{LUKE} \cite{DBLP:conf/emnlp/YamadaASTM20} is a popular model for various information extraction tasks, it is pre-trained with large number of external weakly supervised data. \textbf{IRE} \cite{DBLP:journals/corr/abs-2102-01373} is a strong model which uses entity type marker and achieves impressive performances. \textbf{RECENT-SpanBERT} \cite{DBLP:conf/acl/LyuC21} designs multi-task learning for better usage of entity type information.
    
\textbf{Sequence-to-sequence Methods.} These methods use text generation models such as BART \cite{lewis-etal-2020-bart} and T5 \cite{DBLP:journals/jmlr/RaffelSRLNMZLL20} for RE. 
\textbf{REBEL} \cite{DBLP:conf/emnlp/CabotN21} leverages a BART-large model and is pre-trained on a huge external corpus for RE. \textbf{TANL}\cite{DBLP:conf/iclr/PaoliniAKMAASXS21} reformulates RE as a translation task by utilizing the powerful T5 model. Since the above two methods do not leverage entity type information in their original settings. To have a fair comparison, we re-implement these methods with official code and add the entity type information. We also directly treat relation names as generation objectives and fine-tune a \textbf{BART} model on RE datasets.

\textbf{Prompt-based Methods.} This line of research uses prompt and treats RE as a cloze-style task. \textbf{PTR} \cite{DBLP:journals/corr/abs-2105-11259} and \textbf{KnowPrompt} \cite{DBLP:conf/www/ChenZXDYTHSC22} use different prompt and answer word engineering for RE. While \textbf{NLI-DeBERTa} \cite{DBLP:conf/emnlp/SainzLLBA21} reformulates relation extraction as an entailment task.

Besides, we also design several other models for comparison. \textbf{Base Model} is the basic network that used for generating the \textit{Top-k} prediction set in \S \ref{2.1}. Based on the \textbf{Base Model}, we design two improved methods that utilize the \textit{Top-k} prediction set in different ways. \textbf{Base Model(P)} adds a \textbf{P}rompt $P$ after the input text, where $P$ is: \uline{Choose a relation from \textit{s(Top-k)} for $e_1$ and $e_2$}. \textbf{Base Model(LS)} uses \textbf{L}abel \textbf{S}moothing \cite{DBLP:conf/icml/IoffeS15} and modifies the one-hot label as the soft label. 


\subsection{Main Results}\label{main_result}
Table \ref{main} shows the performances on three RE datasets, and we can have the following observations.

\textbf{KLG} outperforms all previous state-of-the-art methods on three RE datasets, including pushing the TACRED F1-score to 75.6\%, TACRED-Revisit F1-score to 84.1\%, and SemEval2010 F1-score to 90.5\%. Compared with the most popular classification-based methods, \textbf{KLG} shows favorable results without any external dataset usage or additional pre-training stages. Although the newly emerging methods offer desirable performances with larger PLMs, such as T5 and DeBERTa, \textbf{KLG} still suppresses them by a large margin. The above results verify the effectiveness of \textbf{KLG}. By utilizing \textit{s(Top-k)} via a label graph and the dynamic $k$-selection mechanism, \textbf{KLG} could fully use the available information existing in the \textit{Top-k} prediction set and achieve better performance.
    
Although we apply different technologies to utilize the \textit{Top-k} prediction set, the performances of Base Model(P) and Base Model(LS) are undesirable. Base Model(P) shows that directly injecting the \textit{Top-k} prediction set into the input text can not achieve better performance as we expected, even with the help of a prompt. We think there are two possible reasons for the above phenomenon: 1) the backbone network can not understand the semantic meaning of relation names, and appending the \textit{Top-k} prediction set after the input text may confuse the model's encoder, making the model pays less attention to the original input text. 2) With sufficient training samples, models may learn to select a relation name from the \textit{Top-k} prediction set directly without considering the original input text. As for Base Model(LS), it obtains slight improvements compared with its corresponding baseline model. Meanwhile, \textbf{KLG} achieves much better performances than the above methods. 

We also report the ablation study on two crucial components of \textbf{KLG}: 1) the usage of label graph with \textit{Top-k} prediction set, and 2) the dynamic $k$-selection mechanism. The results show that both parts bring notable improvements over the Base Model. With the help of the label graph, our model could review candidate labels existing in the \textit{top-k} prediction set and learn useful information from them. In addition, after being equipped with the dynamic $k$-selection mechanism, \textbf{KLG} learns more powerful relation representations and further improves its performance.

\begin{table}[]
\centering
\setlength{\tabcolsep}{1.3mm}
\begin{tabular}{cccc}
\toprule[1.5pt]
\multicolumn{1}{l|}{}                         & \multicolumn{1}{l|}{\textbf{TACRED}}     & \multicolumn{1}{l|}{\textbf{TACRED-Rev}} & \multicolumn{1}{l}{\textbf{SemEval}} \\ \hline
\multicolumn{4}{l}{\textit{\textbf{Classification-based Methods}}}                                                                                                                 \\ \hline
\multicolumn{1}{c|}{\textbf{LUKE}}            & \multicolumn{1}{c|}{72.7}                & \multicolumn{1}{c|}{80.8}                    & 90.1                                    \\
\multicolumn{1}{c|}{\textbf{IRE}}             & \multicolumn{1}{c|}{74.6}                & \multicolumn{1}{c|}{83.2}                    & 89.8                                     \\
\multicolumn{1}{c|}{\textbf{RECENT}} & \multicolumn{1}{c|}{\underline{75.2}}          & \multicolumn{1}{c|}{\underline{83.4}}              & -                                        \\ \hline
\multicolumn{4}{l}{\textit{\textbf{Sequence-to-Sequence Methods}}}                                                                                                                 \\ \hline
\multicolumn{1}{c|}{\textbf{BART}}           & \multicolumn{1}{c|}{72.7}                & \multicolumn{1}{c|}{81.5}                       & \multicolumn{1}{c}{89.5}                                        \\
\multicolumn{1}{c|}{\textbf{REBEL}}           & \multicolumn{1}{c|}{73.7}                & \multicolumn{1}{c|}{-}                       & -                                        \\
\multicolumn{1}{c|}{\textbf{TANL}}            & \multicolumn{1}{c|}{74.8}                & \multicolumn{1}{c|}{-}                       & -                                        \\ \hline
\multicolumn{4}{l}{\textit{\textbf{Prompt-based Methods}}}                                                                                                                         \\ \hline
\multicolumn{1}{c|}{\textbf{PTR}}             & \multicolumn{1}{c|}{72.4}                & \multicolumn{1}{c|}{81.4}                    & \multicolumn{1}{c}{89.9}                                       \\
\multicolumn{1}{c|}{\textbf{KnowPrompt}}      & \multicolumn{1}{c|}{72.4}                & \multicolumn{1}{c|}{82.4}                    & \multicolumn{1}{c}{\underline{90.2}}                                        \\
\multicolumn{1}{c|}{\textbf{NLI-DeBERTa}}     & \multicolumn{1}{c|}{73.9}                & \multicolumn{1}{c|}{-}                       & -                                        \\ \hline
\multicolumn{4}{l}{\textit{\textbf{Ours}}}                                                                                                                                         \\ \hline
\multicolumn{1}{c|}{\textbf{Base Model}}             & \multicolumn{1}{c|}{74.3} & \multicolumn{1}{c|}{83.1}     & 89.6                      \\
\multicolumn{1}{c|}{\textbf{Base Model(P)}}             & \multicolumn{1}{c|}{74.1} & \multicolumn{1}{c|}{82.7}     & 89.1                      \\
\multicolumn{1}{c|}{\textbf{Base Model(LS)}}             & \multicolumn{1}{c|}{74.6} & \multicolumn{1}{c|}{83.5}     & 89.9                      \\
\multicolumn{1}{c|}{\textbf{KLG}}             & \multicolumn{1}{c|}{\textbf{75.6(+0.4)}} & \multicolumn{1}{c|}{\textbf{84.1(+0.7)}}     & \textbf{90.5(+0.3)}                      \\ 
\multicolumn{1}{c|}{\textbf{KLG w/o DS}}             & \multicolumn{1}{c|}{75.0} & \multicolumn{1}{c|}{83.6}     & 90.1                      \\
\bottomrule[1.5pt]
\end{tabular}
\caption{Micro-F1 score of test sets on three relation extraction datasets. Results are all cited from published papers or re-implemented using official code. For each dataset, \underline{underlined} indicates previous state-of-the-art, \textbf{bold} indicates the best model performance. Results of our methods are averaged over three random seeds, and the results are statistically significant with $p < 0.05$.}
\label{main}
\end{table}




\begin{figure}[h]
	\centering
	\includegraphics[width=0.98\linewidth]{k}
	\caption{The effectiveness of $k$. We find that the fixed $k$ results in unstable performances. With the help of the dynamic $k$-selection mechanism, \textbf{KLG} could achieve desirable results across a wide range of $k$.}
	\label{k_number}
\end{figure}

\subsection{The Sensitivity of $k$}
In this section, we explore the sensitivity of $k$. These results also verify the robustness of our dynamic $k$-selection mechanism (DS). We present the performance on TACRED test set with DS and without DS in Figure \ref{k_number}. We find that without DS, the performances are very sensitive to the $k$-selection, especially when $k$ is too small or too large. With the help of DS, \textbf{KLG} could achieve more stable and much better performances across a wide range of $k$.


%
