\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{caption}


\usepackage{booktabs}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}





\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Location-Free Scene Graph Generation}

\author{Ege Ã–zsoy\thanks{Both authors share first authorship.}\\
\and
Felix Holm\textsuperscript{\textasteriskcentered}\\
\and
Tobias Czempiel\\
\and
Nassir Navab\\
\and
Benjamin Busam\\
\and
Technical University of Munich\\
{\tt\small ege.oezsoy@tum.de  felix.holm@tum.de}
}



\ificcvfinal\thispagestyle{empty}\fi


\twocolumn[{\renewcommand\twocolumn[1][]{#1}\maketitle
\begin{center}
    \centering
\includegraphics[width=.76\textwidth]{gfx/graphical_abstract.pdf}
    \captionof{figure}{We introduce the new task of location-free Scene Graph Generation, completely removing the requirement for expensive bounding box annotations from Scene Graph Datasets. We further introduce (1) Pix2SG, a method leveraging autoregressive language modeling for congruent scene graph predictions, and (2) a Heuristic Tree Search algorithm for Scene Graph Matching for evaluation.}
\end{center}}]
 \begin{abstract}
   Scene Graph Generation (SGG) is a challenging visual understanding task. It combines the detection of entities and relationships between them in a scene. Both previous works and existing evaluation metrics rely on bounding box labels, even though many downstream scene graph applications do not need location information. The need for localization labels significantly increases the annotation cost and hampers the creation of more and larger scene graph datasets. We suggest breaking the dependency of scene graphs on bounding box labels by proposing location-free scene graph generation (LF-SGG). This new task aims at predicting instances of entities, as well as their relationships, without spatial localization. To objectively evaluate the task, the predicted and ground truth scene graphs need to be compared. We solve this NP-hard problem through an efficient algorithm using branching. Additionally, we design the first LF-SGG method, Pix2SG, using autoregressive sequence modeling. Our proposed method is evaluated on Visual Genome~\cite{krishna_visual_2016} and 4D-OR~\cite{ozsoy_4d-or_2022}. Although using significantly fewer labels during training, we achieve 74.12\% of the location-supervised SOTA performance on Visual Genome and even outperform the best method on 4D-OR.\looseness=-1
   
   \renewcommand{\thefootnote}{\fnsymbol{footnote}}
   \footnotetext{* Both authors share first authorship.}
\end{abstract}



 \section{Introduction} Humans quickly abstract information from sight. The visual apparatus is our broadband interface to the world. As such, it heavily interconnects the brain to facilitate fast interpretation of sensory impressions reflected by our experience.
The task of computer-aided scene understanding reflects this process in silico with the aim of automatic visual scene interpretation from camera data. This task can be partitioned into many subtasks, including object and instance detection, localization, or segmentation. Much like the hierarchical abstraction in humans, more complex tasks aim to not only describe the spatial context of objects in a scene but also try to identify the relationships between them.
Scene graphs are one potent tool for describing relationships in a scene. They provide a structured representation using nodes, describing the objects (object instances) and edges connecting two nodes to encode the relationship between them.
Such semantic representation of a scene can be used as the foundation for multiple downstream tasks, such as Visual Question Answering (VQA)~\cite{hudson_learning_2019}, image captioning~\cite{zhong_learning_2021}, image retrieval~\cite{johnson_image_2015}, or image generation and manipulation~\cite{dhamo_graph--3d_2021, dhamo_semantic_2020}.
While computer vision has long surpassed human performance for standard tasks such as classification~\cite{he_delving_2015}, reliable hierarchical abstraction in the form of Scene Graph Generation remains a challenging open research question.\looseness=-1

Most existing Scene Graph Generation approaches~\cite{xu_scene_2017,zhang_graphical_2019, lin_gps-net_2020, tang_unbiased_2020, zellers_neural_2018, tang_learning_2019, wu_scenegraphfusion_2021, wald_learning_2022} divide the task into two sub-tasks: 1) object detection and 2) pairwise relationship prediction. Recent works also propose to use an end-to-end pipeline, where object detection and relationship prediction are performed and optimized jointly~\cite{teng_structured_2022, cong_reltr_2022, shit_relationformer_2022}. 

Nonetheless, to our knowledge, all previous methods require object locations in some part of their pipeline. However, object locations, such as bounding boxes, are not needed for many downstream applications~\cite{hudson_learning_2019, zhong_learning_2021, johnson_image_2015, dhamo_graph--3d_2021, dhamo_semantic_2020} but are mainly used to simplify the SGG pipeline. Their location information provides a straightforward way for instance-level matching in case multiple objects of the same type are present in the scene. Bounding boxes and semantic segmentation labels add significant overhead in creating datasets and can further include annotation errors.

In our work, we want to challenge the requirement of object localizations for Scene Graph Generation and evaluation and overcome the burden of location labels. Not requiring bounding box object annotations could not only significantly simplify the creation of scene graph datasets but it would also allow disentangling of the research fields of object detection and scene graph generation.
While there are many benefits of predicting scene graphs without object location annotations, existing approaches face two major pitfalls. Firstly, the detection task is deeply rooted in the design of most methods, and its removal makes the scene graph generation task impossible. Secondly, current scene graph evaluation metrics are unable to evaluate the performance without bounding box annotations and predictions. The metrics measure the intersection over union threshold between prediction and ground truth based on their locations and the subsequent evaluation of the pairwise predicates~\cite{xu_scene_2017}. Without localization, it is not possible to evaluate pairwise predicates for multiple different instances.

In this paper, we introduce the new task of location-free scene graph generation (LF-SGG) and make its objective evaluation feasible. We design the first baseline method, which overcomes the challenge of correctly identifying entities with their corresponding instances using a novel transformer-based sequence generation design, which does not rely on bounding box annotations during training or validation. While conceptually simple, our method can represent the complexities in an image or scene and produce corresponding scene graphs. Even though we neither train nor evaluate our model on the object location, our method intrinsically learns location awareness which can be retrieved after the fact using the transformer's attention maps.\looseness=-1

We further introduce the first location-free evaluation paradigm for SGG. The predicted scene graph is matched with the target graph using a heuristic tree search formulation. This provides a task-adjusted approximate solution to the NP-hard~\cite{gold_graduated_1996} matching problem of two scene graphs and thereby enables objective evaluation of the new task.

Extensive experiments on the visual genome~\cite{krishna_visual_2016} and 4D-OR~\cite{ozsoy_4d-or_2022} datasets help to understand the performance of the proposed model on this new task and justify design choices. On Visual Genome, our model performs only slightly worse than the current state-of-the-art methods that rely on significantly more labels during training~\cite{xu_scene_2017, tang_learning_2019, teng_structured_2022, tang_unbiased_2020, zellers_neural_2018}. On 4D-OR, we even significantly surpass the location-based state-of-the-art method~\cite{ozsoy_4d-or_2022}.

\noindent In summary, we make the following major contributions:

\begin{itemize}
\item We propose a new task of location-free scene graph generation (LF-SGG). The challenge is to predict scene graphs without access to bounding boxes at training or evaluation time.
\item A necessity for the objective evaluation of LF-SGG is scene graph matching. To this end, we design a task-specific efficient heuristic tree search algorithm.
\item We introduce Pix2SG, the first method to solve LF-SGG, and evaluate its performance on the Visual Genome and 4D-OR datasets.

\end{itemize}

 \section{Related Work} 

\subsection{SGG with Location Supervision}
Previous methods for SGG have typically relied on two-stage architectures. The first stage consists of an object detector, often a pretrained Faster-R-CNN model~\cite{ren_faster_2016}.
The detected localized objects are used as proposals for the scene graph. The proposed objects and visual relationships between them are then classified in the second stage of the architecture, which can take the form of Iterative Message Passing~\cite{xu_scene_2017, zareian_weakly_2020}, Graph Convolutional Neural Networks~\cite{zhang_graphical_2019, lin_gps-net_2020, yang_graph_2018, wald_learning_2020, wu_scenegraphfusion_2021}, and Recurrent Neural Networks~\cite{zellers_neural_2018, tang_learning_2019}.
Lately, some works have tried to move away from this paradigm towards end-to-end approaches~\cite{teng_structured_2022, cong_reltr_2022, shit_relationformer_2022, yang_panoptic_2022}, closely integrating the generation of localized objects and their relationships. These methods are sometimes described as detection-free because they omit an explicit object detector, but they still localize the entities in the output scene graph. All of the methods mentioned above rely on location supervision in the form of bounding box labels, regardless of architectural choice.

\subsection{Weakly-Supervised Scene Graph Generation}
Recently, some works have attempted Weakly-Supervised Scene Graph Generation (WS-SGG), omitting the costly location labels in their training. Zareian et al. (VSPNet~\cite{zareian_weakly_2020}) take object proposals from a pretrained object detector, build them into a semantic bipartite graph as a different formulation of a scene graph and classify and refine the entities through message passing. Shi et al.~\cite{shi_simple_2021} use a weakly-supervised graph matching method to match object proposals from a pretrained object detector to the ungrounded scene graph labels. This matching generates a localized scene graph as a pseudo-label to train conventional fully-supervised SGG methods. Other methods utilize natural language from image captions as a weak supervision source, matching linguistic structures to object proposals~\cite{zhong_learning_2021, ye_linguistic_2021, li_integrating_2022}.

We argue that the reliance of the WS-SGG methods mentioned above on pretrained object detectors is still a major limitation. While this approach does not require location labels in the scene graph datasets itself, a dataset from the same domain that contains location labels for all relevant objects is still required to pretrain the object detector. Shi et al.~\cite{shi_simple_2021} notice a considerable domain gap for the detector pre-training. This manifests in subpar performance if Faster-R-CNN is pretrained on Open Images~\cite{kuznetsova_open_2020} and applied to Visual Genome data, although both contain natural images. This issue becomes even more apparent when SGG is attempted in domains where large-scale image datasets are not readily available, such as the medical domain~\cite{ozsoy_4d-or_2022}. We therefore aim to enable Scene Graph Generation without the use of pretrained object detectors to eliminate the need for location supervision completely.

The methods mentioned in this section output localized scene graphs without training on location labels and are therefore considered weakly supervised. While our suggested LF-SGG task also does not include location labels, it further works without an intermediate generation of localized scene graphs. We, therefore, do not consider LF-SGG weakly supervised.

\subsection{Scene Graph Matching}

Fully Supervised location-based Scene Graph Generation methods rely on the location to match scene graph nodes of prediction and ground truth for evaluation by simply thresholding bounding box IoU~\cite{xu_scene_2017}. When location-free (ungrounded) scene graphs are used, this simple solution for graph matching is no longer available.
The WS-SGG methods shown in the previous section also address some graph-matching problems. VSPNet~\cite{zareian_weakly_2020} aligns their predicted semantic bipartite graph with the ground truth with an iterative optimization algorithm. Shi et al.~\cite{shi_simple_2021} leverage the simple and efficient Hungarian matching algorithm~\cite{kuhn_hungarian_1955} to find a first-order graph matching of node to node without considering graph structure. Both of these approaches, however, can rely on visual features extracted from the used object detector to find a strong node-to-node similarity. In ungrounded scene graphs, such as in LF-SGG, visual features are no longer available. We, therefore, introduce a new scene graph matching algorithm that is capable of matching scene graphs purely based on node labels, edge labels, and graph structure by heuristic tree search.
\subsection{Autoregressive Decoding}
Previous SGG methods generally leverage object proposals to initialize a scene graph. Direct prediction of ungrounded scene graphs at once from an image without this intermediary step requires an exceedingly complex output vector. This could impede training and lack the potential to model the interdependencies between the entities and relations of the scene graph. 

We, therefore, look at autoregressive decoding, which allows a model to make multiple predictions from one data object sequentially. Given the similarity of scene graphs to texts regarding grammatical and semantic structure, we take inspiration from natural language processing (NLP).
Transformers and Autoregressive Decoding have enabled great progress in this domain lately~\cite{devlin_bert_2019, alayrac_flamingo_2022, brown_language_2020}. Chen et al. (Pix2Seq~\cite{chen_pix2seq_2022}) showed that autoregressive decoding could also be used successfully with images for object detection. We argue that we can leverage the advantages of Autoregressive Decoding even more so in the field of SGG, as there are interdependencies in the semantic structure of the scene graph, which can profit from more congruent sequential predictions. We introduce Pix2SG, a model featuring autoregressive sequence decoding for LF-SGG.\looseness=-1

 \section{Methods} 

In this section, we first introduce the new location-free scene graph generation task. To our knowledge, all existing SGG methods require bounding boxes in some parts of their pipeline, making them unsuitable for the LF-SGG task. Therefore, in~\cref{sec:prop_solution}, we present our novel architecture, specifically designed for LF-SGG, visualized in~\cref{fig:autoreg_sg}. Lastly, we design a heuristic tree search-based matching algorithm to enable objective evaluation on our new task.

\subsection{Problem Formulation}
\noindent
In this section, we introduce the location-free scene graph generation task. In this task, the input is an image $I$, and the goal is to produce a scene graph prediction $G = (V,E)$, where $V$ corresponds to the entity nodes, and $E$ to the pairwise relationships. For an objective evaluation the matching
\begin{equation} \label{graph_matching}
M\left( G,G' \right) = G_{m}
\end{equation}
 fits the prediction $G$ to the ground truth $G'$, to produce the mapped graph prediction $G_{m}$.
Recall@K $\mathcal{R}_K \left( G_{m},G' \right)$ is computed between $G_{m}$ and $G'$ to evaluate the task. With this formulation, unlike the conventional SGG task, the LF-SGG task refrains from using location information for both subject and object entities. The task of LF-SGG has increased difficulty compared to SGG, where the bounding box labels provide a strong supervision signal.

\subsection{Proposed Solution}
\label{sec:prop_solution}

\begin{figure}[t]
  \centering
  
   \includegraphics[width=0.95\linewidth]{gfx/sg_no_loc_2.pdf}

   \caption{Conversion of existing location-based Scene Graph Annotations to location-free Scene Graphs with instance identification and mapping to the graph representation.}
   \label{fig:sg_no_loc}
\end{figure}

\begin{figure*}[t]
  \centering
  
   \includegraphics[width=0.95\linewidth]{gfx/autoreg_sg_4.pdf}

   \caption{Pix2SG Architecture: An image encoder encodes the image as a feature map that is flattened and used as the input sequence to the Autoregressive Transformer module. The Autoregressive Transformer predicts the components of the scene graph, token by token, considering all its previous predictions until the output SG-sequence is completed.}
   \label{fig:autoreg_sg}
\end{figure*}

In this section, we introduce our proposed architecture, Pix2SG, the first location-free Scene Graph Generation method.
Inspired by Pix2Seq~\cite{chen_pix2seq_2022}, Pix2SG follows the autoregressive pattern, which is a well-known paradigm in natural language processing but so far has not been used for the task of Scene Graph Generation. We simultaneously predict objects as entity labels, entity instance indices, as well as pairwise relationships directly from a given image. Our model produces the entire output in a straight-forward, autoregressive fashion, producing one token after another, visualized in \cref{fig:autoreg_sg}. One advantage of this autoregressive formulation is that it allows our neural network to exploit the dependencies and relations between different tokens and triplets to improve its prediction. 

\noindent\textbf{Vocabulary.} To formulate scene graph generation as autoregressive sequence generation, we first define a vocabulary of tokens uniquely identifying all entities and predicates in the scene graph dataset. Each entity is represented by two tokens, where the first token represents the entity class and the second token the entity instance, identifying entities of the same class in one image. Additionally, we represent each predicate with a single token $pred_{cls}$. Therefore, a relation between two entities can be encoded using our vocabulary as a quintuple with a combination of five tokens, 
\begin{equation} \label{SceneGraphQuintupel} (sub_{cls},sub_{idx},obj_{cls},obj_{idx},pred_{cls}) \end{equation}
where $sub_{cls}$ and $obj_{cls}$ represent the entity class and $sub_{idx}$ and $obj_{idx}$ the instance ids of the subject and object. 

\noindent\textbf{Ground Truth Sequence Generation.} 
To train our proposed sequence generation model, we convert the scene graph labels $G'$ into a sequence of tokens from our vocabulary. This SG-sequence will be used in both the training and inference of our method.
We first convert the scene graph into individual quintuples as visualized in \cref{fig:sg_no_loc}, then randomly concatenate them to convert all SG-quintuples into a sequence to generate the SG-sequence. We assign ascending entity IDs to multiple instances of the same class, where the first appearance of a class in the sequence will have the instance id 0, the next one 1, and so on. We design our evaluation method to be invariant to the order of the triplets as well as to the order of the instance ids to eliminate any ambiguities during validation.

\noindent\textbf{Network.} Our approach, visualized in~\cref{fig:autoreg_sg}, first employs an image encoding step to generate a flattened representation of the image features encoding the visual information of the frame. In accordance with previous works~\cite{chen_pix2seq_2022}, we augment the flattened image information with a positional encoding which allows the model to identify the spatial location of each token in the feature map.

In the decoding step of the method, the flattened image feature and the start token are used as the first input to the decoder network to generate the first output token. Each predicted output token is subsequently appended to the token sequence that accumulates the predicted tokens. This token sequence is then again used as input to generate the next output token in an autoregressive manner. By the repetition of this autoregressive step, our method is generating an output sequence. This output sequence of tokens can then be effortlessly translated into the desired scene graph representation~(\cref{fig:sg_no_loc}).

\noindent\textbf{Inference.} 
During inference, the next predicted token of the SG-sequence should intuitively be selected using the confidence values of the prediction. However, we noticed that this can lead to unwanted repetitions of the same token in the SG-sequence. To address this problem, we propose the use of nucleus sampling~\cite{holtzman_curious_2020}, which introduces a limited amount of randomness for the selection of the next token. We noticed that we can generate the highest recall results for a p-value of 0.95. We always generate a fixed number of tokens and convert the results into an output of $N$ SG-quintuples (\cref{SceneGraphQuintupel}) with $N$ defining the number of total relations.

\subsection{Objective Evaluation Process}

\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{gfx/sg_matching.pdf}
   \caption{Illustration of the Scene Graph Matching problem. Ground Truth Scene Graph and Prediction have to be correctly matched for the evaluation. A suboptimal matching can obscure the actual model performance.}
   \label{fig:sg_matching_problem}
\end{figure*}

\begin{algorithm}
\caption{Heuristic Tree Search (HTS)} 
\textbf{Input:} gt graph $y$, predicted graph $\hat{y}$, branching $B$\\
\textbf{Output:} best mapping $m_{best}$ \\
\textbf{function} HTS($y, \hat{y}, B, m$)
\begin{algorithmic}
\State $y\_inst \gets$ instance from $y$ with highest node degree
\State $\hat{y}\_insts \gets $ instances from $\hat{y}$ with same class as $y\_inst$
\State $y\_nbhd \gets$ connected nodes and edges of $y\_inst$
\For{$\hat{y}\_inst$ in $\hat{y}\_insts$}
\State $\hat{y}\_nbhd \gets$ connected nodes and edges of $\hat{y}\_inst$
\State $overlaps \gets $ append $|y\_nbhd \cup \hat{y}\_nbhd| / |y\_nbhd|$
        \EndFor
    \State $\hat{y}\_insts \gets \hat{y}\_insts$ sorted by $overlaps$
        \State $M \gets \varnothing $  set for branched mappings \For{$i = 0$ : $B$}{~$m_i \gets m \cup (y\_inst\mapsto$ $\hat{y}\_insts[i]) $}
        \If{$y\backslash y\_inst == \varnothing$} $M \gets M \cup m_i$
        \Else{~$M \gets M \cup $ $\small\text{HTS}(y\backslash y\_inst, \hat{y}\backslash \hat{y}\_insts[i], B, m_i)$}    
        \EndIf
   \EndFor\\
\textbf{return} $M$

\end{algorithmic}
$m_{best} \gets$ select highest recall from $\small\text{HTS}(y, \hat{y}, B, \varnothing$)
\label{tree_based_matching}
\end{algorithm}

To facilitate an objective evaluation process for LF-SGG, scene graph matching has to be performed to calculate $M\left( G,G' \right) = G_{m}$ (\cref{graph_matching}). The evaluation of LF-SGGen has to be performed by only relying on the graph information. However, as shown in \cref{fig:sg_matching_problem}, matching graphs with each other is not straightforward, especially when multiple class instances are present in one scene. In this case, node identification has multiple possible solutions, and identifying the correct instance can only be done by considering the entire graph. Finding the optimal correspondence between prediction and ground truth involves calculating all possible combinations for the matching, maximizing the overlap of graph nodes and relationships. On the one hand, an exhaustive search over all possible matching combinations is too computationally expensive. On the other hand, the Hungarian matching algorithm~\cite{kuhn_hungarian_1955} cannot address the task appropriately, as the cost of an assignment can not necessarily be derived from the direct vicinity of a node.

Therefore, we propose a new algorithm shown in \cref{tree_based_matching} to approximate an exhaustive search with an emphasis on 1) matching quality, 2) computational efficiency, and 3) flexibility and configurability. Our matching algorithm $M^{\ast}\left( G,G',B \right) = G_{m}^{B}$ retrieves an approximate solution $G_{m}^{B}$ to the matching problem $M$ (eqn.\ref{graph_matching}). It is inspired by tree search-based algorithms. We define a branching factor $B$ to control the depth of our algorithmic tree search. By setting $B$ to 1, our algorithm performs greedy matching where iteratively, each predicted entity instance is mapped to only the most overlapping ground truth entity instance. To compute the overlap, we first get the respective local one-hop neighborhoods, represented as a list of (predicate, entity class) tuples. We count how many of these tuples are the same for both of them, where both the predicate and the entity classes have to match. This score is normalized by the node degree. For $B > 1$, the $B$ highest overlaps will be used to create new branches, representing multiple alternative mappings. These branches will be explored recursively until all the instances are mapped. For $B \geq N$, where $N$ is the number of instances of the most common entity class in a scene, our algorithm performs an exhaustive search to find the exact but computationally expensive solution. It holds $G_{m}^{B} \xrightarrow{B \to N} G_{m}$. To further optimize the performance, we start our tree search from the ground truth entity node with the highest node degree, arguing that this node will likely be a key component of our scene.

The result of our heuristic tree search is a set of graph matches between prediction and ground truth, and we subsequently select the match producing the highest evaluation metric as visualized in \cref{fig:sg_matching_problem}. We then use this matching to convert $G$ to $G_{m}$. We provide an efficient implementation of our algorithm in C++, with a sub-second runtime for most samples using three as Branching-factor $B$. We empirically motivate the choice of $B$ in \cref{sec:ablationstudies} and \cref{fig:convergence_20}. 

 

\section{Experiments} \subsection{Datasets}
\noindent\textbf{Visual Genome (VG).} VG is the most frequently used scene graph dataset and is considered as one of the main benchmarking datasets for SGG. As most previous works~\cite{teng_structured_2022}, we use a split of VG with 150 most frequent objects and 50 predicates. While conventionally PredCls, SGCls, and SGGen are used as metrics, none are applicable to the case of LF-SGG. Instead, we evaluate all approaches with our proposed metric, LF-SGGen, where the recall is calculated directly by matching and comparing the predicted scene graph to the ground truth scene graph.

\noindent\textbf{4D-OR.} 4D-OR is a recently published surgical scene graph dataset. Unlike VG, which is sparse in its annotations, 4D-OR includes dense annotations, enabling the calculation of precision in addition to recall. As it includes images from multiple views per scene, it allows us to demonstrate the extension of our method for multiple image inputs per scene. Finally, as the dataset size is an order of magnitude smaller than VG, it allows us to evaluate the performance of our method in lower data regimes. 

\subsection{Implementation details}
We use EfficientNet-B7~\cite{tan_efficientnet_2020} as the image encoder backbone for VG and EfficientNet-B5 in the case of 4D-OR, both implemented in \cite{wightman_pytorch_2019} and pretrained on Imagenet~\cite{russakovsky_imagenet_2015}. We resize the input images to 600x600 pixels for VG, and 456x456 for 4D-OR. In 4D-OR, the four multiview images per scene are processed individually by the feature extraction backbone, then the feature maps are flattened and concatenated to build the input sequence. We use pix2seq~\cite{chen_pix2seq_2022} as the starting point of autoregressive sequence modeling implementation.
We use the categorical cross-entropy loss, with the entire vocabulary as target classes, and optimize our model with AdamW~\cite{loshchilov_decoupled_2019}, with a constant learning rate of $4 \times 10^{-5}$ and a weight decay of $1 \times 10^{-4}$. The Batch size is set to 16 in all our experiments and we train our methods for 200 epochs, employing early stopping. We use a Transformer~\cite{vaswani_attention_2017}, with a hidden size of 256, eight attention heads, six encoding, and two decoding layers. Unless otherwise specified, we predict 300 relations using nucleus sampling~\cite{holtzman_curious_2020} with a p-value of 0.95 and pick the top K unique predictions for Recall@K. We set the branching factor $B$ of our proposed heuristic three-matching algorithm to 3 for all the validation experiments except when indicated otherwise. For the baseline methods on  visual genome, we use the implementations provided by~\cite{tang_scene_2020}. 



\subsection{Results}
\label{sec:results}
\begin{figure*}[t]
  \centering
   \includegraphics[width=0.9\linewidth]{gfx/qual_vg.pdf}
   \caption{Qualitative Results of Pix2SG of the Visual Genome dataset. Images and corresponding Ground Truth Scene Graphs are shown. Nodes and edges correctly predicted by our model are highlighted in green. Many correctly classified predicates are of geometrical nature even though our method does not include the localization task.}
   \label{fig:qual_vg}
\end{figure*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{gfx/qual_4dor_2.pdf}
   \caption{Qualitative Result of Pix2SG on 4D-OR dataset. Images from two of the six viewing angles and the corresponding ground truth scene graphs are displayed. Nodes and edges correctly predicted by our model are highlighted in green.}
   \label{fig:qual_4dor}
\end{figure*}

\begin{table}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l | c  c | c c c}
  \toprule
Model & SG & BBox & R@20 & R@50 & R@100 \\
    \midrule
    IMP~\cite{xu_scene_2017} & $\checkmark$  & $\checkmark$ & 21.66 & 30.78 & 37.07 \\
    MOTIFS~\cite{zellers_neural_2018} &  $\checkmark$  & $\checkmark$ & 29.02 & 38.08 & 43.64 \\
    Transformer~\cite{tang_unbiased_2020} & $\checkmark$  & $\checkmark$ & 28.79 & 37.81 & 43.69 \\
    VCTree~\cite{tang_learning_2019} & $\checkmark$  & $\checkmark$ & 27.06 & 35.59 & 41.21 \\
    SS-R-CNN~\cite{teng_structured_2022} & $\checkmark$  & $\checkmark$ & 22.09 & 26.43 & 28.57 \\
    RelTR~\cite{cong_reltr_2022} & $\checkmark$  & $\checkmark$ & 25.86 & 30.99 & 33.31 \\
    SGTR~\cite{li_sgtr_2022} & $\checkmark$  & $\checkmark$ & 23.62 & 30.38 & 34.85 \\
    \bottomrule
    Pix2SG(Ours) & $\checkmark$  &  & 21.51 & 24.81 & 26.66\\
    \bottomrule
  \end{tabular}
  }
  \caption{LF-SGGen results of different SG models at R@k on Visual Genome dataset. Checkmarks indicating the use of the supervision signal during model training.}
  \label{tab:vg_results}
\end{table}


\begin{table}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l | c  c | c c c}
  \toprule
Model & SG & BBox & Prec. & Rec & F1 \\
    \midrule
    4D-OR SOTA~\cite{ozsoy_4d-or_2022} & $\checkmark$  & $\checkmark$ & 0.68 & 0.87 & 0.75 \\
    \bottomrule
    Pix2SG(Ours) &  $\checkmark$  & & 0.88 & 0.92 & 0.90 \\
    \bottomrule
  \end{tabular}
  }
  \caption{LF-SGGen results of 4D-OR method and Pix2SG on 4D-OR dataset. Checkmarks indicating the use of the supervision signal during model training.}
  \label{tab:4d_or_results}
\end{table}

\begin{figure}[t]
  \centering
  
   \includegraphics[width=0.75\linewidth]{gfx/attention_2.pdf}

   \caption{Visualization of the attention maps of Pix2SG on three images. While the subject attention seems to focus on a few entities, the object and predicate attentions tend to focus on the surroundings as well.}
   \label{fig:attention}
\end{figure}

\noindent\textbf{Visual Genome (VG).} As we introduce a new task and a new metric, we first reevaluate existing methods trained on the task of SGG with bounding boxes, with our LF-SGG evaluation method. While their reliance on bounding boxes makes them not directly comparable to the task of LF-SGG, we still provide these results as a rough but valuable guideline. We then evaluate our approach, Pix2SG, which is trained and evaluated without any bounding boxes. We present these results in \cref{tab:vg_results}. 
As we are the first and only method to not require location information at any stage, our method and results serve as the first baseline for the new task of LF-SGG. In addition to the quantitative results, we also provide qualitative results in \cref{fig:qual_vg}, and visualize the attention maps for three quintuple predictions \cref{fig:attention}. It can be seen that while some attention maps correspond highly with the entity locations such as for "bottle" and "man", for the "sidewalk" we see a more scattered attention. 

\noindent\textbf{4D-OR.} As the evaluation method proposed in 4D-OR~\cite{ozsoy_4d-or_2022} is applicable to our method, we can directly compare our method to the existing SOTA in \cref{tab:4d_or_results}. We significantly improve upon the SOTA, from 75\% F1 to 90\% F1. Importantly, we achieve this without using bounding boxes, depth, or 3D point clouds, which are all used by the existing SOTA. We provide qualitative results for 4D-OR in \cref{fig:qual_4dor}.

\subsection{Ablation Studies}
\label{sec:ablationstudies}
We perform ablation studies to validate the performance of our evaluation method, as well as to demonstrate the importance of nucleus sampling.

In \cref{fig:convergence_20}, we evaluate five different branching factors for our tree search-based matching on the four baseline methods, IMP, VCTree, Motifs, and Transformers. The results show that our algorithm converges at $B=3$, providing a good balance between speed and matching performance. We, therefore, set $B$ to 3 in all other experiments. 

In \cref{tab:sampling_results}, we show the importance of nucleus sampling by comparing it against always choosing the token with the highest probability. By reducing repetition and increasing variance, it leads to significantly higher Recall in all thresholds.

\begin{figure}[t]
  \centering

   \includegraphics[width=0.9\linewidth]{gfx/convergence_of_branching_algorithm.png}

   \caption{Ablation of Branching factor with $B=3$ providing a good trade-off between speed and matching performance.}
   \label{fig:convergence_20}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{l c c c}
  \toprule
Sampling & R@20 & R@50 & R@100 \\
    \midrule
    Maximum Likelihood & 19.05 & 21.18 & 23.19 \\
    Nucleus \cite{holtzman_curious_2020} & 21.51 & 24.81 & 26.66\\
    \bottomrule
  \end{tabular}
  \caption{Effect of nucleus sampling with a p-value of 0.95 on VG compared to conventional maximum likelihood selection for LF-SGGen.}
  \label{tab:sampling_results}
\end{table}


\subsection{Discussion \& Limitations}
The conventional scene graph annotation consists of subtasks, scene graph annotation and bounding box annotation. For the creation of the bounding box labels, the necessary time investment~\cite{su_crowdsourcing_2012} has been reported as 42 seconds for a single bounding box, which breaks down into drawing (25.5~sec), quality verification (9~sec), and coverage verification (7.8~sec).
This allows us to approximate the labeling effort for the dataset used in our experiments which are summarized in \cref{tab:annotation_effort}.
The approximated additional workload to create the Bbox labels is significant for both datasets. While it would take a single person 276 days to create the bounding boxes for the 4D-OR dataset, for the Visual Genome dataset, the time required is 1993 days. Even though there exist many methods that can support annotators in improving their efficiency by a factor of 6-9x~\cite{su_crowdsourcing_2012}, this result strongly motivates the design of algorithms that can avoid the use of these costly additional annotations.

\begin{table}[ht]
  \centering
  \begin{tabular}{l r r}
  \toprule
    & Visual Genome & 4D-OR \\
    \midrule
\# of BBox & 1.3M & 189K \\
hours total & 15,945 & 2,205 \\
person days & 1,993 & 276 \\
\bottomrule
  \end{tabular}
  \caption{Approximate annotation effort for bounding box labels with median annotation time per bounding box of 42~sec and 8h working days.}
  \label{tab:annotation_effort}
\end{table}

Finally, while we reach 74.12\% of the location-supervised SOTA~\cite{zellers_neural_2018,tang_unbiased_2020} performance on R@20 on the VG dataset (\cref{tab:vg_results}), our results also indicate that the location cues provide orthogonal information to scene graph labels beneficial for the SGG task. Therefore, this work does not advocate excluding location information when it is available. Instead, it presents a novel approach that enables the use of scene graphs even in settings where location information is not present.
 \section{Conclusion}
This work introduces the task of location-free scene graph generation (LF-SGG), where we break the dependency of scene graph generation on object location information. Along with this new task, we introduce the first scene graph generation method, Pix2SG, which does not use object location information at any stage of the pipeline. Pix2SG uses autoregressive sequence modeling to directly generate the scene graph from the input image. Finally, we provide an efficient heuristic tree search-based algorithm for the complex problem of scene graph matching. This allows the evaluation of LF-SGG without object localization also for future pipelines. Our new task and its objective evaluation can pave the way to more efficient scene interpretation from visual content in the form of scene graphs. 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{FelixZoteroLibrary}
}

\end{document}