\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}
\usepackage{hyperref}
\usepackage{algorithm}      \usepackage{amssymb}
\usepackage[noend]{algpseudocode} 

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\algrule}[1][.2pt]{\par\vskip.1\baselineskip\hrule height #1\par\vskip.1\baselineskip}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\title{SepTr: Separable Transformer for Audio Spectrogram Processing}
\name{Nicolae-C\u{a}t\u{a}lin Ristea, Radu Tudor Ionescu\thanks{corresponding author}, Fahad Shahbaz Khan}

\address{
  University Politehnica of Bucharest, Romania\\
  University of Bucharest, Romania, Link\"{o}ping University, Sweden\\
  Mohamed bin Zayed University of Artificial Intelligence, UAE
  }
\email{r.catalin196@yahoo.ro, raducu.ionescu@gmail.com, fahad.khan@liu.se}

\begin{document}

\maketitle


\begin{abstract}
Following the successful application of vision transformers in multiple computer vision tasks, these models have drawn the attention of the signal processing community. This is because signals are often represented as spectrograms (e.g.~through Discrete Fourier Transform) which can be directly provided as input to vision transformers. However, naively applying transformers to spectrograms is suboptimal. Since the axes represent distinct dimensions, i.e.~frequency and time, we argue that a better approach is to separate the attention dedicated to each axis. To this end, we propose the \textbf{Sep}arable \textbf{Tr}ansformer (SepTr), an architecture that employs two transformer blocks in a sequential manner, the first attending to tokens within the same time interval, and the second attending to tokens within the same frequency bin. We conduct experiments on three benchmark data sets, showing that our separable architecture outperforms conventional vision transformers and other state-of-the-art methods. Unlike standard transformers, SepTr linearly scales the number of trainable parameters with the input size, thus having a lower memory footprint. Our code is available as open source at \url{https://github.com/ristea/septr}.
\end{abstract}
\noindent\textbf{Index Terms}: separable transformer, multi-head attention, audio spectrogram processing, sound recognition.

\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}

\vspace{-0.1cm}
\section{Introduction}

Vision transformers \cite{Carion-ECCV-2020,Chen-arXiv-2021,Dosovitskiy-ICLR-2020,Gao-MICCAI-2021,Khan-ACS-2021,Parmar-ICML-2018,Ristea-ArXiv-2021,Touvron-ICML-2021,Wu-ICCV-2021,Zheng-BMVC-2021,Zhu-ICLR-2020} have rapidly become the hottest topic in computer vision to date, showing promising results across a broad range of tasks, from object recognition \cite{Dosovitskiy-ICLR-2020,Touvron-ICML-2021,Wu-ICCV-2021} and detection \cite{Carion-ECCV-2020,Zheng-BMVC-2021,Zhu-ICLR-2020} to medical image segmentation \cite{Chen-arXiv-2021,Gao-MICCAI-2021,Hatamizadeh-WACV-2022} and generation \cite{Ristea-ArXiv-2021}. Due to the unquestionable success of vision transformers in solving many computer vision tasks, these models have also drawn the attention of the signal processing community \cite{Gong-INTERSPEECH-2021,Illium-INTERSPEECH-2021}. For example, ViT \cite{Dosovitskiy-ICLR-2020} was transferred to the processing of signals by Gong et al.~\cite{Gong-INTERSPEECH-2021}, by essentially providing visual representations of signals, i.e.~spectrograms, as input to the model. Since the axes of a spectrogram represent distinct dimensions, e.g.~frequency and time, we argue that naively applying transformers to spectrograms is suboptimal. Indeed, the attention spans across both directions, leading to a quadratic complexity with respect to the number of tokens. While operating over both horizontal and vertical axes seems reasonable for natural or medical images, we conjecture that a better approach for processing spectrograms is to separate the attention for each axis. To this end, we propose an architecture that employs two transformer blocks in a sequential manner. The first block (vertical transformer) attends to tokens within the same time interval, individually processing each time interval. Similarly, the second block (horizontal transformer) attends to tokens within the same frequency bin, independently operating over each frequency bin. To easily implement our transformer, we convert the input spectrogram into a batch of data samples along each axis, where a data sample is alternatively formed of tokens within the same time interval or within the same frequency bin, as illustrated in Figure~\ref{fig_septr}. Our approach leads to a quadratic reduction of the number of learnable parameters, which further translates into a more efficient attention mechanism. In a nutshell, our transformer separates the attention for the horizontal and vertical axes of spectrograms, thus bearing the name \textbf{Sep}arable \textbf{Tr}ansformer (SepTr).

We conduct experiments on three benchmark data sets, namely Speech Commands V2 \cite{Warden-ArXiv-2018}, ESC-50 \cite{Piczak-ACMMM-2015} and CREMA-D \cite{Cao-TAC-2014}, showing that our separable architecture attains significantly better results than models inspired by vision transformers \cite{Gong-INTERSPEECH-2021}, as well as other state-of-the-art methods \cite{georgescu2020non,He-CVPRW-2020,Kim-DCASE-2020,Majumdar-INTERSPEECH-2020,Ristea-INTERSPEECH-2021,Sailor-INTERSPEECH-2017,Shukla-ICASSP-2020}, on all benchmarks. Moreover, we demonstrate that SepTr significantly reduces the number of learnable parameters (weights) when compared to ViT.

In summary, our contribution is twofold:
\begin{itemize}
    \item \vspace{-0.1cm} We propose a novel architecture based on separable transformer blocks, which is particularly suitable for the efficient processing of spectrograms.  
    \item \vspace{-0.1cm} We provide empirical evidence to support our claims regarding the high level of effectiveness and the low number of trainable parameters of our approach in relation to competing methods.
\end{itemize}

\section{Related Work}

\begin{figure*}[!ht]
\begin{center}
\centerline{\includegraphics[width=0.82\linewidth]{septr_figure_v2.pdf}}
\vspace{-0.25cm}
\caption{Our SepTr architecture. The spectrogram is tokenized, projected and further processed by two sequential transformers (vertical and horizontal). Our separable transformer block (comprising both vertical and horizontal transformers) is repeated  times, leading to a SepTr model of  blocks in depth. The final class token is taken as input by an MLP head which decides the final output. }
\label{fig_septr}
\end{center}
\vspace{-1.0cm}
\end{figure*}

Due to the recent progress of attention mechanisms \cite{Vaswani-NIPS-2017}, transformers have become attractive and powerful choices for audio related tasks. On the one hand, some studies took self-attention mechanisms used in natural language processing to process text sequences and adapted them to process audio sequences and solve various audio tasks, e.g.~automatic speech recognition \cite{Kanda-INTERSPEECH-2021, Lohrenz-INTERSPEECH-2021, Leong-INTERSPEECH-2021}, speech synthesis \cite{Wu-INTERSPEECH-2021}, and speech representation \cite{Luo-INTERSPEECH-2021}. On the other hand, a few approaches adopted architectures from computer vision \cite{Gong-INTERSPEECH-2021,Illium-INTERSPEECH-2021}, mainly due to the similarity between images and time-frequency representations.

In the preliminary studies adopting transformers for audio tasks, the attention architecture was typically used in conjunction with convolutional neural networks (CNNs) \cite{Miyazaki-DCASE-2020, Kong-TASLP-2020, Gulati-INTERSPEECH-2020}. In \cite{Miyazaki-DCASE-2020, Kong-TASLP-2020}, the authors stacked a transformer on top of a CNN, while in \cite{Gulati-INTERSPEECH-2020}, the authors combined the transformer module with a CNN in each block. Our method differs from these studies in that it is convolution-free, being purely based on multi-head attention modules. 

The self-attention mechanism was widely adopted in the audio field, providing remarkable results \cite{Miyazaki-ICASSP-2020, Koizumi-ICASSP-2020}. For sound event detection, Miyazaki et al.~\cite{Miyazaki-ICASSP-2020} utilized a transformer encoder based on BERT, which consists of multiple self-attention modules, allowing the capture of both local and global contextual information of the input sequence. Koizumi et al.~\cite{Koizumi-ICASSP-2020} proposed a similar method based on multi-head self-attention modules, which is used in a multi-task learning setting for the speech enhancement task. They observed that the attention mechanism is able to handle long-term dependencies in speech and noise, obtaining improved results. Instead of introducing attention modules into existing architectures as \cite{Miyazaki-ICASSP-2020, Koizumi-ICASSP-2020}, we design a stand-alone transformer architecture based on separable attention, which is both effective and efficient.

Kanda et al.~\cite{Kanda-INTERSPEECH-2021} proposed an end-to-end automatic speech recognition method, which jointly performs speaker counting, speech recognition and speaker identification for monaural multi-talker audio. They replaced a memory-based approach with custom transformers, attaining better word error rates. Similarly, Wang et al.~\cite{Wang-ICASSP-2020} proposed a hybrid architecture based on transformers for speech recognition. In contrast, we design an efficient transformer based on separable attention, which exploits the data structure of time-frequency representations.

Other works \cite{Gong-INTERSPEECH-2021,Illium-INTERSPEECH-2021} relied on the similarity between digital images and time-frequency representations by applying solutions originally proposed for computer vision tasks. Illium et al.~\cite{Illium-INTERSPEECH-2021} applied the vision transformer (ViT) \cite{Dosovitskiy-ICLR-2020} to primates classification and COVID detection. Even if they naively employ the solution proposed in vision, their results surpass the baseline methods. The authors also tried to built more natural tokens for time-frequency representations, considering all the frequency bins from a specific time slot as one token. However, this approach led to inferior results compared with ViT. A more comprehensive study about the utilization of ViT in the audio domain is conducted by Gong et al.~\cite{Gong-INTERSPEECH-2021}. The authors discussed the utility of vision pre-training in the audio domain, reporting state-of-the-art results on multiple data sets. Unlike other approaches inspired by vision transformers, we propose an efficient transformer with separable attention along the axes, which is more natural for time-frequency representations, e.g.~audio spectrograms. Notably, our approach is capable of attaining superior results with a lower number of parameters. 







\section{Method}

\noindent{\bf Data representation.}
We transform each audio sample into a 2D time-frequency matrix, obtaining an image-like representation. To this end, we compute the discrete Short Time Fourier Transform (STFT), as follows:

where  is the discrete input signal,  is the window function (in our approach, Hamming),  is the STFT length and  is the hop size. Next, we compute the spectrogram as the squared magnitude of the STFT and map the frequency bins onto the Mel scale.

\noindent{\bf Overview of our architecture.}
We propose SepTr, a \textbf{Sep}arable \textbf{Tr}ansformer architecture composed of two sequential transformer blocks, each attending to tokens within separate dimensions. The architecture does not impose a certain axis (time or frequency) for the first transformer block, being flexible in this regard. Without loss of generality, in Figure~\ref{fig_septr}, we illustrate a model that separates the tokens along the time axis first. Our separable transformer block can be repeated  times to increase the depth of the architecture. The final mean class token (given by the last separable block) is processed by a multi-layer perceptron (MLP), which outputs the final prediction of our model. We next present the building blocks of SepTr.

\noindent{\bf Tokenization and linear projection.}
Considering a spectrogram with  frequency bins and  time slots as input, in the first stage, we divide the input spectrogram into  square patches (tokens) of size . 
In the experiments, we take patches of size  as individual tokens, since we consider that it is more natural to compute the self-attention at the finest possible level.
However, for a better visualization of the architecture shown in Figure \ref{fig_septr}, the tokens are depicted as larger patches.
Regardless of their size, the tokens are further fed into a linear projection block, which projects the tokens into -dimensional vectors. Let  represent the output tensor of the linear projection layer, where  is a projected token,  and , where  is the latent token dimension. 

\noindent{\bf Vertical transformer.}
We separate the projected tokens in the time domain into data sub-samples denoted as , thus obtaining a batch of  data samples (one sample for each time slot ), where each data sample is composed of  tokens. Moreover, we replicate the class token   times and add one copy to each data sample . We also add a learnable positional embedding to each token. The resulting batch of  individual data samples is further processed by the vertical transformer. 

\noindent{\bf Horizontal transformer.}
Let  denote a data sample given as output by the vertical transformer. Next, we concatenate the processed data samples into a tensor , while decoupling the class tokens and applying average pooling to obtain a mean class token denoted as . We proceed by separating the tensor  in the frequency domain into data sub-samples denoted as , generating a batch of  data samples (one sample for each frequency bin ), where each sample is formed of  tokens. As for the vertical transformer, we replicate the class token   times and add one copy to each data sample . We further append a learnable positional embedding to each token. The resulting batch of  data samples is given as input to the horizontal transformer.



\noindent{\bf Transformer block.}
The operations performed inside the vertical and horizontal transformers are identical, the only difference being the format of the input data samples. We thus describe the inner operations for the general case. Let  denote a sequence of  tokens (either  or ), where  and  is the embedding dimension of each token. Let  be a multi-head attention layer,  a multi-layer perceptron,  a normalization layer, and  some auxiliary tensors. The transformer block is formally described as follows:



The goal of the transformer block is to capture the interaction among all  entities by encoding each entity in terms of the global contextual information. This is achieved via the multi-head attention layer . This layer comprises three learnable weight matrices (, , , where ) which are used to derive the queries , keys  and values  from the input sequence . Indeed, the input sequence  is
first projected onto these weight matrices to get , , and , respectively. The output  of the self-attention is given by:

where  is the transpose of . 










\section{Experiments}

\subsection{Data sets}

\noindent{\bf ESC-50.}
The ESC-50 \cite{Piczak-ACMMM-2015} data set is a collection of 2,000 samples of 5 seconds each, comprising 50 classes of various common sound events. Samples are recorded at a 44.1 kHz sampling frequency, with a single channel. For evaluation, we followed the 5-fold cross-validation procedure described in \cite{Gong-INTERSPEECH-2021}.

\noindent{\bf Speech Commands V2.}
The Speech Commands V2 (SCV2) \cite{Warden-ArXiv-2018} data set is composed of spoken words, being designed to train and evaluate keyword spotting systems. It consists of 105,829 1-second recordings of 35 common speech commands. We used the official training (84,843 samples), validation (9,981 samples) and test (11,005 samples) splits proposed in \cite{Warden-ArXiv-2018}.

\noindent{\bf CREMA-D.}
The CREMA-D multi-modal database \cite{Cao-TAC-2014} is formed of 7,442 videos of 91 actors (48 male and 43 female) of different ethnic groups. The actors perform various emotions while uttering 12 particular sentences that evoke one of the 6 emotion categories. 
Following \cite{Ristea-INTERSPEECH-2021}, we conduct experiments only on the audio modality, dividing the audio samples into  for training,  for validation and  for testing.

\subsection{Evaluation setup}

\noindent{\bf Performance metrics.}
In all our experiments, we employ the classification accuracy as evaluation measure. We also conduct significance tests to compare SepTr with the top competitor, using a paired McNemar's test \cite{Dietterich-NC-1998} at a significance level of 0.01.

\noindent{\bf Data preprocessing.} 
For CREMA-D, we first standardize all audio clips to a fixed dimension of 4 seconds by padding or clipping the samples. For both CREMA-D and SCV2, we apply the STFT with ,  and a window size of 512. We use the same  value and window length for ESC-50, but we increase the hop size to . Next, for each STFT, we compute the square root of the magnitude and map the values to 128 Mel bins. The result is converted to a logarithmic scale (decibels) and normalized to the interval , generating a single-channel output matrix. In all our experiments, we use the following data augmentation methods: noise perturbation, time shifting, speed perturbation, mix-up and SpecAugment \cite{Park-INTERSPEECH-2019}.

\noindent{\bf Hyperparameter tuning.} 
While using the validation sets to tune the hyperparameters of SepTr and ViT, we found common optimal hyperparameters for all data sets. Thus, all models are optimized with Adam using the cross-entropy loss function. We start with an initial learning rate of  and use a decay factor of  after every 10 epochs. We train each model for 50 epochs on mini-batches of 4 samples. For SepTr, we set the number of blocks to  and the token size to , while for ViT we use a depth of  and the same token size (). Since our transformer block comprises two (vertical and horizontal) attention modules, the depths of SepTr and ViT are equivalent. Both SepTr and ViT are designed with  attention heads. For the ViT architecture, we divide the spectrograms into  patches at an overlap of  pixels in both directions, following \cite{Gong-INTERSPEECH-2021}. We hereby underline that we were not able to train the ViT architecture with  patches due to out-of-memory issues, while SepTR does not suffer from this problem (see Figure~\ref{fig_noparams}).

\begin{table}[!t]
\caption{Results of SepTr and its ablated versions in comparison with various state-of-the-art methods on CREMA-D. Significantly better results compared to \cite{Ristea-INTERSPEECH-2021} are marked with , using a paired McNemar's test \cite{Dietterich-NC-1998} at a significance level of . Top scores are highlighted in bold.}
\label{tab_results_cremad}
\vspace{-0.2cm}
\centering
\begin{tabular}{l c}
\toprule
{\textbf{Method}} & {\textbf{Accuracy}}\\
\midrule
GRU (Shukla et al.~\cite{Shukla-ICASSP-2020}) & 55.01\% \\
GAN (He et al.~\cite{He-CVPRW-2020}) & 58.71\% \\
ResNet-18 (Georgescu et al.~\cite{georgescu2020non}) & 65.15\% \\
ResNet-18 ensemble (Ristea et al.~\cite{Ristea-INTERSPEECH-2021}) & 68.12\% \\
ViT (Gong et al.~\cite{Gong-INTERSPEECH-2021})  & 67.81\% \\
\midrule
SepTr-V         & 65.29\% \\
SepTr-H         & 65.11\% \\
\midrule
SepTr-HV         & 70.31\% \\
SepTr-VH (proposed)        & \textbf{70.47\%} \\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\end{table}


\subsection{Ablation study}

We first analyze the impact of the vertical and horizontal transformers, when these modules are used alone or jointly. We present ablation results on the CREMA-D data set in Table \ref{tab_results_cremad}. We include ablated versions of SepTr performing attention either inside individual time slots (SepTr-V uses only vertical transformer blocks) or frequency bins (SepTr-H uses only horizontal transformer blocks). We observe that computing the attention on a single axis with SepTr-V or SepTr-H leads to suboptimal results, even below the ViT baseline. This shows that performing attention on one axis only is not sufficient.

Further, we show results while alternating the order of the vertical and horizontal transformers, i.e.~SepTr-HV starts with a horizontal transformer and SepTr-VH starts with a vertical transformer. Regardless of how the separable attention is applied via SepTr-HV or SepTr-VH, we observe important performance boosts (higher than ) over the models attending to one of the axes (SepTr-V and SepTr-H). Switching the order of the vertical and horizontal transformers has a marginal influence on the accuracy level, both SepTr-HV or SepTr-VH being able to outperform the state-of-the-art methods by significant margins. As we obtained slightly better results with SepTr-VH, we continued the rest of the experiments with this architecture, which we further simply refer to as \emph{SepTr}.

\begin{table}[!t]
\caption{Results of SepTr versus various state-of-the-art methods on Speech Commands V2 (SCV2) and ESC-50. Significantly better results compared to ViT \cite{Gong-INTERSPEECH-2021} are marked with , using a paired McNemar's test \cite{Dietterich-NC-1998} at a significance level of . Top scores are highlighted in bold.}
\label{tab_results_2}
\setlength\tabcolsep{4.0pt}  
\vspace{-0.2cm}
\centering
\begin{tabular}{l c c}
\toprule
{\textbf{Method}} & \textbf{SCV2} & {\textbf{ESC-50}}\\
\midrule
RBM (Sailor et al.~\cite{Sailor-INTERSPEECH-2017})  & - &  86.50\% \\
EfficientNet (Kim et al.~\cite{Kim-DCASE-2020}) & - & 89.50\% \\
MatchboxNet (Majumdar et al.~\cite{Majumdar-INTERSPEECH-2020}) & 97.40\% & - \\
ViT (Gong et al.~\cite{Gong-INTERSPEECH-2021}) & 98.11\% & 88.70\% \\
\midrule
SepTr (proposed)         & \textbf{98.51\%} & \textbf{91.13\%}\\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\end{table}

\subsection{Results}

\noindent{\bf Effectiveness.}
On the CREMA-D data set (see Table~\ref{tab_results_cremad}), we obtain state-of-the-art results, reaching an accuracy of  and surpassing the previous best method \cite{Ristea-INTERSPEECH-2021}, composed of an ensemble of ResNet models, by . Moreover, we attain a performance boost of  compared to ViT \cite{Gong-INTERSPEECH-2021}.
On the SCV2 and ESC-50 data sets (see Table~\ref{tab_results_2}), we consider state-of-the-art methods that are not pre-trained on external data, ensuring a fair comparison with SepTr, which is trained from scratch. We observe that SepTr attains superior results on both data sets. On SCV2, SepTr attains an accuracy of , which is  higher than the previous state-of-the-art accuracy obtained by ViT \cite{Gong-INTERSPEECH-2021}. On ESC-50, SepTr yields an accuracy of , surpassing ViT \cite{Gong-INTERSPEECH-2021} by  and EfficientNet \cite{Kim-DCASE-2020} by , respectively. In summary, SepTr surpasses all the state-of-the-art methods on each and every data set by significant margins.

\begin{figure}
\begin{center}
\centerline{\includegraphics[width=0.94\linewidth]{fig_params.pdf}}
\vspace{-0.2cm}
\caption{The number of trainable parameters for SepTr and ViT \cite{Gong-INTERSPEECH-2021} with respect to the input dimension.}
\label{fig_noparams}
\vspace{-1.0cm}
\end{center}
\end{figure}

\noindent{\bf Efficiency.}
We compare the performance of SepTr and ViT in terms of the memory footprint (number of learnable parameters), inference time and number of multiply-accumulate operations (MACs), on an Nvidia GeForce GTX 3090 GPU with 24 GB of VRAM. We compare equivalent models having 6 attention layers each, which are fed with  patches. In terms of GMACs ( for SepTr versus  for ViT) and inference time ( ms for SepTr versus  ms for ViT), the differences are negligible, but when we refer to the memory footprint, we observe a large difference between the two models. In Figure \ref{fig_noparams}, we illustrate the number of learnable parameters as a function of the input spectrogram size. We observe that SepTr has an almost constant number of parameters with respect to the input dimension, while the number of weights in ViT exhibits a quadratic growth, leading to comparatively larger models. For instance, for an input size of , SepTr has 9.4M parameters, while ViT has 75.7M. Unlike ViT, our model is able to handle high resolution spectrograms, an essential advantage leading to high accuracy levels in the audio domain.

\section{Conclusion}
In this work, we proposed a novel Separable Transformer (SepTr) architecture composed of two sequential transformers, each computing the attention on a different axis of the input spectrogram, either time or frequency. We obtained state-of-the-art results on three data sets, surpassing all competing methods by statistically significant margins. Moreover, we showed that SepTr reduces the memory footprint when compared with ViT, without impacting the inference time or MACs. In future work, we aim to employ SepTr for other signal processing tasks.

\vspace{0.2cm}
\noindent
{\bf Acknowledgements.} Work supported by a grant of the Romanian Ministry of Education and Research, CNCS - UEFISCDI, project no. PN-III-P1-1.1-TE-2019-0235, within PNCDI III.

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}
