[{'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (High)', 'Score': '84.0'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (Middle)', 'Score': '88.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1 dev', 'Metric': 'EM', 'Score': '89.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1 dev', 'Metric': 'F1', 'Score': '95.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Quora Question Pairs', 'Metric': 'Accuracy', 'Score': '92.3%'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'EM', 'Score': '89.898'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'F1', 'Score': '95.080'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'Hardware Burden', 'Score': '46449G'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'EM', 'Score': '87.926'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'F1', 'Score': '90.689'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'F1', 'Score': '90.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'EM', 'Score': '87.9'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'QNLI', 'Metric': 'Accuracy', 'Score': '94.9%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A1', 'Score': '70.3'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A2', 'Score': '50.9'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'ANLI test', 'Metric': 'A3', 'Score': '49.4'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'WNLI', 'Metric': 'Accuracy', 'Score': '92.5%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '85.9%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Matched', 'Score': '90.8'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'MRPC', 'Score': '93.0/90.7'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'SICK-R', 'Score': '-'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'SICK-E', 'Score': '-'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'STS', 'Score': '91.6/91.1*'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'Accuracy', 'Score': '90.8%'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'STS Benchmark', 'Metric': 'Pearson Correlation', 'Score': '0.925'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'IMDb', 'Metric': 'Accuracy', 'Score': '96.21'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'Yelp Fine-grained classification', 'Metric': 'Error', 'Score': '27.05'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'Yelp Binary classification', 'Metric': 'Error', 'Score': '1.37'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'SST-2 Binary classification', 'Metric': 'Accuracy', 'Score': '97'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'SST-2 Binary classification', 'Metric': 'Accuracy', 'Score': '96.8'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'Yelp-2', 'Metric': 'Accuracy', 'Score': '98.63%'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'AG News', 'Metric': 'Error', 'Score': '4.45'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'Yelp-5', 'Metric': 'Accuracy', 'Score': '72.95%'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'Amazon-5', 'Metric': 'Error', 'Score': '31.67'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'Amazon-2', 'Metric': 'Error', 'Score': '2.11'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'DBpedia', 'Metric': 'Error', 'Score': '0.62'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'IMDb', 'Metric': 'Accuracy (2 classes)', 'Score': '96.8'}}, {'LEADERBOARD': {'Task': 'Text Classification', 'Dataset': 'IMDb', 'Metric': 'Accuracy (10 classes)', 'Score': '-'}}, {'LEADERBOARD': {'Task': 'Humor Detection', 'Dataset': '200k Short Texts for Humor Detection', 'Metric': 'F1-score', 'Score': '0.920'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA', 'Metric': 'Accuracy', 'Score': '69%'}}]
