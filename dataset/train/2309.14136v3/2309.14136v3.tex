\documentclass{article}





\usepackage[final]{neurips_2023} 










    \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
    \makeatother

   
\usepackage[utf8]{inputenc}
\usepackage{booktabs}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{nicematrix}

\usepackage{listings}
\usepackage{color,xcolor}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{multirow}
\usepackage{dcolumn}
\usepackage{xspace}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{url}
\usepackage{color, colortbl}
\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,linkcolor=linkcolor]{hyperref}
\usepackage{macros/mysymbols}
\usepackage{macros/widetext}
\usepackage{macros/space_saver}
\usepackage{floatrow}
\floatsetup{heightadjust=all, floatrowsep=columnsep}
\newfloatcommand{figurebox}{figure}[\nocapbeside][\dimexpr(\textwidth-\columnsep)/2\relax]
\newfloatcommand{tablebox}{table}[\nocapbeside][\dimexpr(\textwidth-\columnsep)/2\relax]

\floatstyle{plaintop}
\restylefloat{table}

\usepackage{natbib}
\setcitestyle{square,sort,comma,numbers}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{colortbl}
\usepackage{bm}
\usepackage{times}
\usepackage{epsfig}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
 \usepackage{transparent}
\usepackage{mathrsfs}
\usepackage{dashrule}



\usepackage{multirow}
\usepackage{bbding}
\usepackage[flushleft]{threeparttable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{booktabs}


\usepackage{floatrow}
\usepackage{epsfig}
\usepackage{stfloats}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\usepackage[ruled,linesnumbered,boxed]{algorithm2e}
\definecolor{MyGray}{gray}{0.9}
\makeatletter
\newcommand{\thickhline}{\noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}

\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}



\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}



\title{Masked Image Residual Learning for Scaling Deeper Vision Transformers}






\author{Guoxi Huang \\
  \small Baidu Inc.\\
  \small{huangguoxi@baidu.com} \\
  \And
  Hongtao Fu \\
  \small Huazhong University of Science and Technology \\
  \small{m202173233@hust.edu.cn} \\
  \And
  Adrian G. Bors\thanks{Corresponding author. Work done when H. Fu was an intern at Baidu.}\\
  \small University of York \\
  \small{adrian.bors@york.ac.uk} \\
}



\begin{document}


\maketitle


\begin{abstract}
Deeper Vision Transformers (ViTs) are more challenging to train. We expose a degradation problem in deeper layers of ViT when using masked image modeling (MIM) for pre-training.
To ease the training of deeper ViTs, we introduce a self-supervised learning framework called \textbf{M}asked \textbf{I}mage \textbf{R}esidual \textbf{L}earning (\textbf{MIRL}), which significantly alleviates the degradation problem, making scaling ViT along depth a promising direction for performance upgrade. We reformulate the pre-training objective for deeper layers of ViT as learning to recover the residual of the masked image.
We provide extensive empirical evidence showing that deeper ViTs can be effectively optimized using MIRL and easily gain accuracy from increased depth. 
With the same level of computational complexity as ViT-Base and ViT-Large, we instantiate 4.5{} and 2{} deeper ViTs, dubbed ViT-S-54 and ViT-B-48.
The deeper ViT-S-54, costing 3{} less than ViT-Large, achieves performance on par with ViT-Large.
ViT-B-48 achieves 86.2\% top-1 accuracy on ImageNet. 
On one hand, deeper ViTs pre-trained with MIRL exhibit excellent generalization capabilities on downstream tasks, such as object detection and semantic segmentation. On the other hand, MIRL demonstrates high pre-training efficiency. With less pre-training time, MIRL yields competitive performance compared to other approaches. Code and pretrained models are
available at: \href{https://github.com/russellllaputa/MIRL}{\textit{https://github.com/russellllaputa/MIRL}}.


\end{abstract}

    

\section{Introduction}


Transformer architecture~\cite{vaswani2017attention} has become the de-facto standard in natural language processing (NLP). 
A major driving force behind the success of Transformers in NLP is the self-supervised learning method called masked language modeling (MLM)~\cite{devlin2019bert}. 
MLM significantly expends the generalization capabilities of Transformers, with the underlying principle being very intuitive - removing portions of a sentence and learning to predict the removed content.
Recent advancements in computer vision have been profoundly inspired by the scaling successes of Transformers in conjunction with MLM in NLP, successively introducing the Vision Transformer (ViT)~\cite{dosovitskiy2020image} and masked image modeling (MIM) for training generalizable vision models.
The concept of MIM is as straightforward as MLM; its pre-training objective is to predict masked image patches based on the unmasked image patches, thereby capturing rich contextual information. 

This paper first reveals that MIM can induce negative optimization in deeper layers of ViT, which not only constrains the generalization performance of ViT but also hinders its scaling along the depth dimension.
Previous work~\cite{xie2022revealing,ren2023deepmim} suggests that deeper layers of ViT are more properly pre-trained by using MIM, the conclusions of which contradict our observation.  
Another branch of work~\cite{chen2022sdae,wang2022closer,chen2022context,jiang2023layer,ren2023deepmim} tentatively suggests that, due to the lack of semantic information in MIM, the shallower layers of ViTs are more effectively pre-trained than the deeper layers.
In our preliminary experiments in Sec.~\ref{sec: A Deep Dive into AutoEncoders for MIM}, we demonstrate that replacing the deeper Transformer blocks pre-trained by using MIM with randomly initialized blocks does not degrade performance, which supports our statement.
We hypothesize the negative pre-training effect enforced on the deeper layers is due to depth restriction, which can be regarded as a degradation problem occurring in deeper ViTs.
\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.98\textwidth]{imgs/manners.pdf}
\end{center}
\caption{Three MIM pre-training schemes. 
For simplicity, the diagram omits the random masking process.
A complete ViT consists of shallow and deep parts. The dashed box indicates the part of the model that is not involved in the pre-training process. 
}
\label{fig: manners}
\end{figure}

\begin{figure}
\flushleft 
  \begin{floatrow}
    \ffigbox{\includegraphics[width=0.46\textwidth]{imgs/taevsmae.pdf}}
    {\vspace{0pt}\caption{\label{fig: taevsmae}Truncated MAE \vs MAE.
The -axis represents the number of blocks replaced with randomly initialized blocks from the encoder's end after pre-training. ViT-S is used as the encoder to better observe the differences.
}}
    \ffigbox{\includegraphics[width=0.46\textwidth]{imgs/ae_variants.pdf}}
    {\vspace{0pt}\caption{\label{fig: ae_variants}Comparison of MIRL, truncated MAE, and MAE. To maintain the same decoding computational cost, each decoder in the MIRL model contains 2 blocks, while other models have 4 decoder blocks.}}
    
  \end{floatrow}
\end{figure}

We address the pre-training degradation problem in deeper layers of ViT by introducing a \textbf{M}asked \textbf{I}mage \textbf{R}esidual \textbf{L}earning (\textbf{MIRL}) framework. We establish a multi-decoding process by segmenting the encoding blocks according to their depth levels. 
Instead of letting the entire autoencoder learn to reconstruct the masked content, MIRL
encourages the deep layers to learn latent features that are beneficial for recovering the image residual, distinct from the main image component. 
The diagram of the MIRL with 2 segments is illustrated in Figure~\ref{fig: manners}c, where we divide the encoder into shallow and deep segments, and append a separate decoder to each. 
The shallow segment learns to reconstruct the main component of the masked content, while the deep segment is explicitly reformulated as learning the image residual.
The MIRL framework is essentially equivalent to shifting the pre-training objective of deeper layers of ViT from image reconstruction to image residual reconstruction. 
This simple yet effective concept of image residual reconstruction significantly alleviates the degradation problem in deeper layers of ViT, making scaling ViTs along depth a promising direction for improving performance.
By extending the MIRL framework and dividing the encoder into more segments, as illustrated in Figure~\ref{fig: framework}, we can train deeper ViTs and readily achieve accuracy gains from substantially increased depth. 
Consequently, we instantiate deeper encoders: ViT-B-24, ViT-B-48, and ViT-S-54, comprising 24, 48, and 54 Transformer blocks, respectively. Notably, with similar computational complexity, our deeper ViT variants deliver considerably better generalization performance than the wider ViT encoders (\eg ViT-S-54 \vs ViT-B, ViT-B-48 \vs ViT-L), thanks to the increased depth.
Meanwhile, our experiments in Sec.~\ref{sec: Ablation Studies} demonstrate that employing additional feature-level objectives~\cite{dong2022bootstrapped,chen2022sdae} or VGG loss~\cite{johnson2016perceptual,dong2021peco} can further improve performance, suggesting that the improvement directions of feature-level loss and MIRL are orthogonal and can complement each other.



\vspace{5pt}
\section{Preliminaries}
\vspace{5pt}

\subsection{Revisit masked image modeling}
Following the paradigm of Masked AutoEncoder(MAE)~\cite{he2022masked}, the input image is split into a set of non-overlapping patches , where  denotes the image patch in the -th position with  resolution and  channels. The image patches are further tokenized into visual tokens via a linear projection, denoted by 
\footnote{We still perform the position embedding addition and class token concatenation processes as in ViT, but these steps are omitted for notational simplicity.}.
Subsequently, a random sampling strategy divides the indexes of patches into two subsets,  and , where .
The image patches () and visual tokens () with indexes in  are considered to be visible to the encoder.
Thereafter, the encoder with  blocks only takes as input visible tokens , and maps them to embedding features . The objective of MIM is to predict the unseen content from  by employing a decoder. A learnable mask token  is introduced after the encoder, which is placed in  masked positions. Then the full set of encoded patches and mask tokens is processed by a small decoder to reconstruct the original image in pixels. 
The architecture of MAE can be described as:

where  refers to the -th Transformer block in the encoder,  denotes a shallow decoder.
The objective loss of MIM is given by

where the reconstruction loss is only calculated in the masked positions.   


\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.99\textwidth]{imgs/pipeline.pdf}
\end{center}
\caption{Example of the MIRL framework. The Transformer blocks in the ViT encoder are split into four segments,  and . The output of each segment is then sent to its corresponding decoder. Shortcut connections are established between the shallower decoders  and deeper decoders , enabling deeper decoders to predict the masked image residual.
} 
\label{fig: framework}
\end{figure}

\vspace{5pt}
\subsection{A deep dive into autoencoders for MIM}
\label{sec: A Deep Dive into AutoEncoders for MIM}
We present three distinct autoencoder architectures in Figure~\ref{fig: manners} to clarify our motivation. Figure~\ref{fig: manners}a shows the MAE framework~\cite{he2022masked}. Figure~\ref{fig: manners}b depicts a truncated MAE with only the shallow ViT blocks in the encoder. Figure~\ref{fig: manners}c presents a simplified MIRL with two decoders connected to the shallow and deep encoder segments. Models are pre-trained for 300 epochs with the same hyperparameters. Full setup details are in Appendix~A of the supplementary materials.



\textbf{Observation \uppercase\expandafter{\romannumeral1}:} \textit{MIM pre-training can induce negative optimization in deeper layers of ViT.}
However, due to the overwhelmingly positive pre-training effect that MIM bestows upon the earlier blocks, its adverse influence on the latter blocks remains undiscovered.
Given a ViT encoder pre-trained with MIM (Figure~\ref{fig: manners}a), we substitute the pre-trained weights in the latter blocks of the encoder with the random parameters and subsequently fine-tune the encoder on ImageNet-1K.
The curve plot of vanilla MAE, depicted in Figure~\ref{fig: taevsmae}, illustrates that applying random re-initialization to deeper blocks from the end of the encoder can improve performance. 
The initial point in Figure~\ref{fig: taevsmae} indicates the result of an MAE model that has been pre-trained and subsequently fine-tuned without any random initialization. As more shallow blocks are randomly re-initialized, accuracy declines.
Intriguingly, random initialization generalizes better than MIM pre-training in deeper layers of ViT, defying intuition.


\textbf{Observation \uppercase\expandafter{\romannumeral2}:} 
\textit{Performing MIM pre-training on fewer layers can lead to better efficiency and effectiveness.} A truncated MAE illustrated in Figure~\ref{fig: manners}b requires less pre-training time than the vanilla MAE while still achieving better or comparable performance. 
For fine-tuning, we initialize a complete ViT model with pre-trained weights from the truncated MAE. Regarding the blocks that are not included in the truncated MAE, we apply random initialization.
As shown in Figure~\ref{fig: taevsmae}, when truncating 3 blocks from the end of the encoder, the fine-tuned model has better accuracy than the rest ones.  
By using the truncated MAE, we only pre-train 4 blocks and achieve similar fine-tuning accuracy as the vanilla MAE, reducing pre-training cost by 66\%.


\textbf{Observation \uppercase\expandafter{\romannumeral3}:} \textit{Learning to recover image residual is a more productive pre-training objective.}
A simplified version of MIRL, shown in Figure~\ref{fig: manners}c, formulates the pre-training objective for deeper layers as learning image residual, promoting more vivid image detail recovery and imposing a positive pre-training effect on deeper layers. Figure~\ref{fig: ae_variants} demonstrates that MIRL achieves the highest fine-tuning accuracy among the three MIM pre-training schemes.

In summary, Observations \uppercase\expandafter{\romannumeral1} and \uppercase\expandafter{\romannumeral2}  expose a pre-training degradation problem in ViT's deeper layers, leading to sub-optimal solutions. The same issue is also observed in the BEiT~\cite{bao2021beit} paradigm, potentially attributed to depth limitations in MIM. In Observation \uppercase\expandafter{\romannumeral3}, we introduce MIRL to alleviate the degradation problem in deeper layers. The rest of the paper demonstrates how we employ MIRL to tackle the challenges of training deeper Vision Transformers.
 



\vspace{5pt}
\section{Method}
\label{sec: Method}
\vspace{5pt}
\subsection{Masked Image Residual Learning (MIRL)}


Upon observing that deeper layers pre-trained by MIM underperform against those with random initialization, we infer that the weight parameters of these deeper layers have indeed been updated during MIM pre-training, but in an unfavorable direction. In contrast, the shallower layers demonstrate improved performance after MIM pre-training. This leads us to speculate that the depth of the layers could be the root cause of the degradation problem.


To alleviate degradation in deeper ViTs during pre-training, we propose letting the deeper Transformer blocks learn to predict the residual of the masked image, rather than directly predicting the masked image itself.
An overview of the MIRL framework is illustrated in Figure~\ref{fig: framework}. Specifically, the encoder is partitioned into multiple segments, with each segment being followed by a separate small decoder. Subsequently, we establish shortcut connections between the shallower and deeper decoders.
We underscore that these shortcut connections constitute the core of our method. This configuration fosters a seamless collaboration between very shallow and deep Transformer blocks in corrupted image reconstruction: the shallower segment learns to reconstruct the main component of the masked image, while the deeper segment learns the image residual. During pre-training, the established shortcut connections enable back-propagation to affect both the deeper and shallower layers simultaneously. This intertwined relationship between the deeper and shallower layers implies that the pre-training should either guide both towards a beneficial direction or lead both astray. With the introduction of the MIRL framework, our experimental results indicate that the shallower layers have, in essence, steered the deeper layers towards a more favorable direction.


Formally, we reformulate the encoder in Eq.~\eqref{eq: mae_1} by evenly grouping the encoding Transformer blocks into  segments:

where  denotes a stack of encoding blocks in the -th segment. In the output of each encoding segment , the masked positions are filled with a shared masked token , denoted as .
Subsequently, for the -th segment and the ()-th segment selected from bottom-up and top-down directions, two separate decoders  and  are appended for feature decoding. Let us consider that the -th shallower segment learns a mapping function  producing the main component of the reconstructed image . Thereafter, we let  from the ()-th deeper segment asymptotically approximate the residual . The objective loss  for the -th segment is defined as:

Different from the residual learning in~\cite{he2016deep}, our image residual learning would not fit an identity mapping, considering that the inputs to the two segments are different. See Appendix~B.2 for further discussions on an alternative form of .
The final loss function is formed by accumulating all  reconstruction loss terms: 

where  is the scaling coefficient, which is set to  by default.
Additional pre-training objective losses, such as the feature-level loss used in~\cite{chen2022sdae,wei2022FD} and the VGG loss~\cite{johnson2016perceptual}, can be employed to enhance performance. We provide the definitions of other loss terms in Appendix~B.1.
However, as indicated in the ablation study in Sec.\ref{sec: Ablation Studies}, incorporating additional loss terms introduces non-negligible overhead during pre-training. By default, we solely utilize the per-pixel loss defined in Eq.\eqref{eq: loss_g}.




\textbf{Densely Interconnected Decoding (DID).}
We design a densely interconnected decoding (DID) module, inserted into the decoders across different segments, enabling access to the features produced by previous segments. DID allows subsequent segments to avoid relearning features already acquired in earlier segments, thereby enhancing representation diversity. Immediately following the self-attention module in the first Transformer block in decoder , we insert a DID module, which is ingeniously implemented using the Multi-Head Attention mechanism, :

where the query  is , while the key and value  are . The dense interconnection property of the DID module is rendered by the concatenation form in MHA's key   and value  where we concatenate the output from previous encoding segments, thereby enabling feature integration at different depth levels.




\vspace{5pt}
\subsection{Scaling to deeper ViT}
\label{sec: Scaling to Deeper ViT}
\vspace{5pt}
Without modifying the Transformer block design, we instantiate deeper ViT variants by simply stacking more blocks. We consider the embedding hidden sizes in the Transformer block of 384 and 768. The details of the deeper ViTs are provided in Table~\ref{tab: details_vitd}.
Choosing an appropriate hidden size is non-trivial for scaling up Vision Transformers, considering that a larger hidden size (\eg, 1024 adopted in ViT-Large and 1280 adopted in ViT-Huge) can cause instability due to very large values in attention logits, leading to (almost one-hot) attention weights with near-zero entropy. 
The instability of very wide ViTs is also reported in~\cite{he2022masked,dehghani2023scaling}.

\begin{table}[htbp]
    \caption{Details of Vision Transformer scaling along the depth dimension.}
\centering
    \setlength{\tabcolsep}{30pt}
    \renewcommand{\arraystretch}{1.1}
    \small
    \begin{tabular}{@{}l c c c c@{}}
    \\
    \toprule
     Model & Depth  & Hidden size & MLP size & Heads   \\
    \midrule
     ViT-S-54 & 54 & 384 & 1536 & 12  \\
     ViT-B-24 & 24 & 768 & 3072 & 12 \\
     ViT-B-48 & 48 & 768 & 3072 & 12  \\
    \bottomrule
    \end{tabular}

    \label{tab: details_vitd}
\end{table}

By utilizing MIRL, we demonstrate that deeper ViTs exhibit stronger or comparative generalization capabilities when compared to their shallower and wider counterparts. 
With similar computational complexity, ViT-S-54 generalizes better than ViT-B. With only 31\% of the computational cost of ViT-L, ViT-S-54 delivers performance on par with ViT-L.
Notably, ViT-B-48 not only that it achieves higher performance but also provides a more stable training than ViT-L. This suggests that deepening ViTs could be a promising direction for enhancing vision model performance. Furthermore, MIRL helps to alleviate the training difficulties typically encountered in deeper ViTs, unlocking their potential and making them more effective for a variety of downstream tasks.


\vspace{5pt}
\section{Experiment}
\vspace{5pt}
The proposed MIRL method is evaluated on image classification, object detection and semantic segmentation tasks. All models are pre-trained on ImageNet-1K and then fine-tuned in downstream tasks. 
The input size is , which is split into 196 patches with a size of .



\textbf{Pre-training setup.} We pre-train all models on the training set of ImageNet-1K with 32 GPUs.
By default, ViT-B-24 is divided into 4 segments, while ViT-S-54 and ViT-B-48 are split into 6 segments, and others into 2. Each appended decoder has 2 Transformer blocks with an injected DID module. We follow the setup in~\cite{he2022masked}, masking 75\% of visual tokens and applying basic data augmentation, including random horizontal flipping and random resized cropping. Full implementation details are in Appendix~A.





\subsection{Instantiations of deeper ViT}
\label{sec: Instantiations of deeper ViT}
We compare the performance of the deeper ViTs detailed in Sec.\ref{sec: Scaling to Deeper ViT} with the ViT instantiations presented in~\cite{dosovitskiy2020image}. As illustrated in Figure~\ref{fig: vit_instances}, we can easily gain accuracy from increased depth by leveraging MIRL for pre-training. In particular, 
ViT-S-54, which has the same level of computational complexity as ViT-B but is  deeper than ViT-B, significantly outperforms ViT-B and even achieves performance on par with ViT-L pre-trained with MAE. Likewise, ViT-B-48 surpasses ViT-L while maintaining the same level of computational cost. Furthermore, the encoder pre-trained with MIRL consistently delivers higher performance than the one pre-trained with MAE. 

\vspace{5pt}
\subsection{Ablation studies}
\label{sec: Ablation Studies}
\vspace{5pt}


\textbf{Various objective functions.} 
Our approach is a general masking modeling architecture, seamlessly complementing prior methods that propose various objective functions.
We study the compatibility of the recent feature-level loss and perceptual loss (\ie VGG loss) with our method. The results listed in Table~\ref{tab: various_objs} show that incorporating these objective functions can further improve the model, which demonstrates the generality of MIRL.
Nonetheless, the additional loss terms will introduce heavy computational overhead. To accelerate our experiments, we have not used them by default. 


\begin{figure}[!t]
\flushleft 
  \begin{floatrow}
    \ffigbox{\includegraphics[width=0.49\textwidth]{imgs/vit_instances.pdf}}
    {\vspace{0pt}\caption{\label{fig: vit_instances} Fine-tuning evaluation on ImageNet versus model size. With a similar complexity, deeper ViTs outperform shallower ViTs. The models are pre-trained for 300 epochs.
}}
    \ffigbox{
    \includegraphics[width=0.49\textwidth]{imgs/train_schedule.pdf}}
    {\vspace{0pt}\caption{\label{fig: train_time} Training schedules. The performance of deeper ViTs has not yet plateaued even after 1000 pre-training epochs. We employ a step decay learning rate scheduler.}}
  \end{floatrow}
\end{figure}


\begin{table*}[!t]
\caption{MIRL ablation experiments on ImageNet-1K: We report the fine-tuning (ft) accuracy(\%) for all models, which are pre-trained for 300 epochs. Unless specified otherwise, the encoder is ViT-B-24. 
}
\flushleft
\captionsetup[subfloat]{labelformat=parens, labelsep=space, skip=7pt, position=top}

\subfloat[Various objective functions. The encoder is ViT-B.  \label{tab: various_objs}]{
\small
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{3.9pt}
\begin{tabularx}{0.34\textwidth}{@{}l c c c@{}}
\toprule
 objectives & MIRL  & ft & time \\
\midrule
pixel & \ding{51} & 83.5 & 1.0{} \\
pixel{}feature & \ding{55} & 83.2 & 1.6{}\\
pixel{}feature  & \ding{51} & 83.6 & 1.8{}\\
pixel{}vgg~\cite{johnson2016perceptual} & \ding{51} & 83.8 & 2.5{}\\
\bottomrule
\end{tabularx}}
 \hspace{3mm}
\subfloat[Numbers of segments (seg.) in the autoencoder of MIRL.  \label{tab: num_seg}]{
\small
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{6.5pt}
\begin{tabularx}{0.29\textwidth}{@{}c c c@{}}
\toprule
 \#seg. & blocks per seg.  & ft \\
\midrule
1 & 24 & 83.5 \\
2 & 12 & 84.3 \\
4 & 6& 84.7 \\
6 & 4 & 84.6 \\
\bottomrule
\end{tabularx}}
 \hspace{3mm}
\subfloat[Effect of Densely Interconnected Decoding. \label{tab: abl_dc}]{
\small
\setlength{\tabcolsep}{9.3pt}
\begin{tabularx}{0.26\textwidth}{@{}l c c@{}}
\toprule
model & DID & ft  \\
\midrule
ViT-B-24 & \ding{55} & 84.5 \\
ViT-B-24 & \ding{51} & 84.7 \\
ViT-B-48 & \ding{55} & 85.0 \\
ViT-B-48 & \ding{51} & 85.3 \\
\bottomrule
\end{tabularx}}

\vspace{10pt}
\subfloat[MIRL \vs simple multi-decoders. Column  reports the performance gap between MIRL and multi-decoders. 
\label{tab: mirl_vs_multiencoders}]{
\small
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{7.2pt}
\begin{tabularx}{0.5\textwidth}{@{}l c c c c@{}}
\toprule
 model  & depth & MIRL &  & multi-decoders \\
\midrule
ViT-B & 12 & 83.5 & 0.3 & 83.2 \\
ViT-B-24 & 24 & 84.7 & 0.6 & 84.1 \\
ViT-B-48 &48 & \textbf{85.3} & 0.8 & 84.5 \\
\bottomrule
\end{tabularx}}
 \hspace{4mm}
\subfloat[MIRL \vs coarse and fine separation.
\label{tab: mirl_vs_finecoarse}]{
\small
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{16pt}
\begin{tabularx}{0.43\textwidth}{@{}l c  c@{}}
\toprule
 method   & ViT-B  & ViT-B-24  \\
 \midrule
 MIRL & \textbf{83.5}  & \textbf{84.7} \\
 coarse-to-fine & 83.1  & 84.2 \\
 fine-to-coarse & 82.9  & 84.2 \\
\bottomrule
\end{tabularx}}
\vspace{10pt}
\end{table*}

\textbf{Number of segments.} 
The purpose of dividing an encoder into more segments is to construct auxiliary reconstruction losses to facilitate the training of the intermediate layers. 
We observe that these auxiliary reconstruction branches can amplify the gradient magnitude, potentially improving the optimization of deeper Transformer blocks.
Table~\ref{tab: num_seg} reports the performance of MIRL with respect to various numbers segments. 
For ViT-B-24, the configuration with 2 segments shows lower accuracy than the one with 4 segments. 
However, further splitting the encoder into more segments brings no more performance gain. 

\begin{table*}[t]
\small
\centering
\renewcommand{\arraystretch}{1.1}
\addtolength{\tabcolsep}{3.2pt}
\begin{tabular}{@{}l c c l c c c@{}}
    \toprule
    Encoder  & \#params & FLOPs  & Method & Training Data   & Epochs& FT (\%) \\
     \midrule
     \multirow{9}{*}{ViT-B} & \multirow{9}{*}{86M} & \multirow{9}{*}{16.8G} & Supervised & IN1K &  - & 82.3 \\
      & & & MoCov3 \cite{chen2021empirical}& IN1K  & 300 & 83.2   \\
& & &\textcolor{gray}{BEiT} \cite{bao2021beit} & \textcolor{gray}{DALLE250M+IN1K}    & \textcolor{gray}{800}  & \textcolor{gray}{83.2}   \\
      & & &SimMIM \cite{xie2021simmim} & IN1K  &  800 & 83.8  \\
& & & \textcolor{gray}{CIM} \cite{fang2022corrupted} & \textcolor{gray}{DALLE250M+IN1K}  &  \textcolor{gray}{300} & \textcolor{gray}{83.3}    \\
      & & & \textcolor{gray}{LocalMIM} \cite{wang2023masked} & \textcolor{gray}{IN1K}  &  \textcolor{gray}{1600} & \textcolor{gray}{84.0}    \\
      & && MAE \cite{he2022masked} & IN1K & 300/800 & 82.6/83.1  \\
      & && \textcolor{gray}{MAE} \cite{he2022masked} & \textcolor{gray}{IN1K}   &\textcolor{gray}{1600} & \textcolor{gray}{83.6}   \\
     & && MIRL  & IN1K & 300/800 & 83.5/84.1  \\
    \midrule
    ViT-S-54 &  96M & 18.8G & MIRL & IN1K  &  300/800 & 84.4/84.8  \\
    ViT-B-24  & 171M & 33.5G  & MIRL  & IN1K &  300 & 84.7  \\
    \midrule
     \multirow{6}{*}{ViT-L} &  \multirow{6}{*}{307M} & \multirow{6}{*}{59.7G} & Supervised & IN1K  & - & 82.6 \\
      & & &  MaskFeat \cite{wei2022masked} & IN1K & 1600 &  85.7   \\
      & & &\textcolor{gray}{ConMIM} \cite{yi2023masked} & \textcolor{gray}{IN1K}  & \textcolor{gray}{1600} & \textcolor{gray}{85.5}   \\
      & &&HPM  \cite{wang2023hard} & IN1K & 800 & 85.8   \\
      & &&\textcolor{gray}{MAE} \cite{he2022masked} & \textcolor{gray}{IN1K}  & \textcolor{gray}{1600} & \textcolor{gray}{\underline{85.9}}   \\
      & &&MAE  \cite{he2022masked} & IN1K & 300/800 & 84.5/85.4   \\
     

    \midrule
    ViT-B-48 &  341M & 67.0G & MIRL & IN1K   &  300/800 & 85.3/\textbf{86.2} \\
   
     
     \bottomrule
\end{tabular}
\caption{Image classification results on ImageNet-1K.
All models are pre-trained and fine-tuned with 224224 input resolution.
``IN'' refer to ImageNet, while ``FT'' is the fine-tuning accuracy. ``Epochs'' refer to the number of pre-training epochs. The models pre-trained with extra data or very long schedules are marked in \textcolor{gray}{gray}. We report the best result in \textbf{bold} and the second best result(s) \underline{underlined}.} 
\label{tab: classification_finetune}
\end{table*}


\textbf{Effect of densely interconnected decoding.} 
Considering that the auxiliary reconstruction branches from different segments adopt the same objective metric, the proposed DID establishes a feature reuse mechanism, preventing layers at various depth levels from learning similar feature representations. Table~\ref{tab: abl_dc} demonstrates that the MIRL models embodying the DID module yield higher fine-tuning accuracy than those without the DID. 
Deeper models gain more advantages from DID.


\textbf{MIRL \vs simple multi-decoders.} 
In order to demonstrate that image residual learning, powered by the shortcut connections, is the key to effective training deeper ViTs, we construct a segmented autoencoder with multiple decoders. Unlike MIRL, each decoder in the  multi-decoder model independently learns to reconstruct the masked content.
As shown in Table~\ref{tab: mirl_vs_multiencoders}, MIRL achieves substantially higher accuracy than the simple multi-decoder approach. 
Notably, the performance gap between MIRL and the multi-decoders widens as more Transformer blocks are stacked. 
When pre-training with multi-decoders, the deeper ViT seems to gain accuracy from increased depth. However,  this does not imply that the multi-decoder approach addresses the degradation problem. Since replacing the weights of its deeper layers with random weights does not lead to a performance drop, the trivial improvement is attributed to the increased number of shallower layers. 


\textbf{MIRL \vs coarse and fine separation.}
As the reconstructed image residual shows some fine-grained details images, it is intriguing to know what pre-training results can be produced by replacing the reconstruction targets with the coarse and fine image components separated by using a Laplacian of Gaussian operator.
We construct a segmented autoencoder with multiple decoders, referred to as ``coarse-to-fine'', in which the reconstruction targets of the shallower and deeper segments correspond to the coarse and fine image components, respectively. ``fine-to-coarse'' denotes the reversed targets compared to the "coarse-to-fine" configuration.
Table~\ref{tab: mirl_vs_finecoarse} indicates that the segmented autoencoder with fine and coarse reconstruction targets achieves lower accuracy than MIRL, demonstrating that the main and residual components are not equivalent to the fine and coarse components.

\textbf{Training schedules.} 
\label{sec: Training schedules}
So far, we have only trained our models using a relatively short pre-training schedule of 300 epochs. 
Note that deeper ViTs gain more advantages from longer pre-training schedules, compared to shallower ViTs. We extend pre-training to 1000 epochs and record fine-tuning performance for various pre-training lengths. To resume pre-training from previous checkpoints, we use a step decay learning rate scheduler, decaying the learning rate by a factor of 10 at 90\% and 95\% of the specified pre-training length.
Figure~\ref{fig: train_time} shows that ViT-B tends to plateau after 800 pre-training epochs, while ViT-S-54 keeps improving even after 1000 epochs. This implies that deeper ViTs' potential can be further unleashed by adopting a very long pre-training schedule, such as 1600 epochs.

\vspace{5pt}
\subsection{Image classification on ImageNet-1K}

We compare our models with previous results on ImageNet-1K. Hyperparameters are provided in Appendix~A.
For ViT-B, MIRL pre-trained for 300 epochs achieves 83.5\% top-1 fine-tuning accuracy, comparable to MAE (83.6\%) pre-trained for 1600 epochs. Our pre-training is  shorter, demonstrating the high efficiency of MIRL. MIRL alleviates degradation in deeper ViTs, showing impressive generalization.
In an 800-epoch pre-training scheme, the deeper encoder ViT-S-54 produces 84.8\% accuracy, which is 1.7\% higher than ViT-B (83.1\%) pre-trained with MAE and only 0.6\% lower than ViT-L (85.4\%).
ViT-B-48, with computational complexity similar to ViT-L but  deeper, achieves 85.3\% and 86.2\% accuracy with 300 and 800-epoch pre-training schemes, outperforming the ViT-L models pre-trained by other listed methods.
Furthermore, the deeper encoders can further benefit from very long pre-training schedules, as discussed in Sec.~\ref{sec: Training schedules}.

\vspace{5pt}
\subsection{Object detection and segmentation on COCO}
To evaluate the generalization capabilities of our approach, we transfer our pre-trained models to the object detection task. The experiment is conducted on MS COCO~\cite{coco} on account of its wide use.
Following~\cite{he2022masked}, we choose Mask R-CNN~\cite{he2017mask} as the detection framework and trained with the {} schedule. 
For fair comparisons, we adopt the identical training configurations from \texttt{mmdetection}~\cite{chen2019mmdetection}, and Average Precision (AP) is used as the evaluation metric. As summarized in Table~\ref{tab:object_detection}, MIRL outperforms all the listed methods.


\begin{table*}[!t]
    \centering
\small
    \renewcommand{\arraystretch}{1.1}
    \addtolength{\tabcolsep}{4.3pt}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method & Backbone & Pre-training Data & Epochs & Detection  & Segmentation   \\
\midrule 
        DeiT \cite{touvron2021training}& ViT-B & - & - &   &  \\
\textcolor{gray}{BEiT} \cite{bao2021beit}& \textcolor{gray}{ViT-B} & \textcolor{gray}{IN1K+DALLE} &\textcolor{gray}{800} &  \textcolor{gray}{} & \textcolor{gray}{} \\
        {MAE} \cite{he2022masked} & {ViT-B} & IN1K & {800} &  {} & {} \\
        
        \textcolor{gray}{MAE} \cite{he2022masked} & \textcolor{gray}{ViT-B} & \textcolor{gray}{IN1K} & \textcolor{gray}{1600} &  {48.4}  & \textcolor{gray}{}  \\
        MIRL &ViT-B & {IN1K} & 800 &  49.3  & 43.7 \\
        \midrule
        \textcolor{gray}{MAE} \cite{he2022masked} & \textcolor{gray}{ViT-L} & \textcolor{gray}{IN1K} & \textcolor{gray}{1600} &  \textbf{\textcolor{gray}{}} & \textbf{\textcolor{gray}{47.2}} \\
        MIRL &ViT-B-48 & {IN1K} & 800 &  \textbf{53.4}  & {46.5} \\
    \bottomrule
    \end{tabular}
\caption{Object detection results with Mask R-CNN on MS-COCO.
The models pre-trained with extra data or very long schedules are marked in \textcolor{gray}{gray}.}
\label{tab:object_detection}
\vspace{10pt}
\end{table*}

\vspace{5pt}
\subsection{Semantic segmentation on ADE20K}
We compare our method with previous results on the ADE20K~\cite{zhou2017scene} dataset, utilizing the UperNet framework for our experiments, based on the implementation provided by~\cite{bao2021beit} (see Appendix A for training details). The evaluation metric is the mean Intersection over Union (mIoU) averaged across all semantic categories. We employ pre-trained ViT-B-48 as the backbone, which has a computational cost similar to ViT-L. The results are summarized in Table~\ref{tab: semantic}. The segmentation model using ViT-B-48 achieves competitive results compared to ViT-L pre-trained with BEiT~\cite{bao2021beit} and MAE~\cite{he2022masked}. This indicates that the instantiated deeper ViTs exhibit strong transferability to downstream vision tasks.

\vspace{5pt}
\subsection{Limitation and discussion}
\vspace{5pt}
While MIRL significantly alleviates training challenges for deeper ViTs, a comprehensive theoretical explanation for the effectiveness of image residual reconstruction in training deeper ViTs remains elusive. We provide some insights into why MIRL might work well for deeper ViTs:
1) By reformulating the pre-training objective to recover the masked image's residual, MIRL implicitly encourages the model to focus on learning high-level contextual information and fine-grained details that are otherwise difficult to capture.
2) MIRL could stabilize gradient flow and enhance learning dynamics for deeper layers in ViTs, as evidenced by a larger gradient norm of the encoder in MIRL compared to vanilla MIM (see gradient visualization in Appendix~C).
Despite these insights, further theoretical analysis and investigation are required to fully understand MIRL's effectiveness in training deeper ViTs. The deepest ViT presented in this research comprises only 54 blocks. We anticipate that a depth of 54 is far from the upper limit for scaling ViT along the depth dimension. These areas are left for future work.
\begin{table}[!t]
    \centering
\small
    \renewcommand{\arraystretch}{1.1}
    \addtolength{\tabcolsep}{19.05pt}
    \begin{tabular}{@{}llccc@{}}
    \toprule
        Method & Pre-training Data & Backbone & Epochs & mIoU \\
        \midrule
        MoCo v3~\cite{chen2021empirical}& IN1K & ViT-L & 300 & 49.1 \\
        \textcolor{gray}{BEiT}~\cite{bao2021beit} & \textcolor{gray}{IN1K+DALLE} & \textcolor{gray}{ViT-L} & \textcolor{gray}{800} & \underline{\textcolor{gray}{53.3}} \\
        \textcolor{gray}{MAE}~\cite{he2022masked} & \textcolor{gray}{IN1K} & \textcolor{gray}{ViT-L} & \textcolor{gray}{1600} & \textbf{\textcolor{gray}{53.6}} \\
        MIRL & IN1K &  ViT-B-48 & 800 & 53.2 \\
    \bottomrule \end{tabular}
    \caption{Semantic segmentation results on ADE20K. The models pre-trained with extra data or very long schedules are marked in \textcolor{gray}{gray}.}
    \label{tab: semantic}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=.965\textwidth]{imgs/visual.pdf}
{\caption{\label{fig: visual_examples} Visualization of MIRL. Example images are generated from the validation set on ImageNet. 
}}
\end{figure}



\vspace{5pt}
\section{Related Work}
\vspace{5pt}
\textbf{Self-supervised learning.}
After the triumph of BERT~\cite{devlin2019bert} and GPT~\cite{radford2018improving} models in NLP, self-supervised learning (SSL) has undergone a paradigm shift, replacing the conventional supervised learning approach~\cite{he2016deep,tan2019efficientnet}, and has made remarkable advancements in numerous domains~\cite{pan2009survey,zhai2019large,kolesnikov2020big}. 
Larger datasets, new training methods and scalable architectures~\cite{mahajan2018exploring,dosovitskiy2020image,clip,zhai2022scaling} have accelerated this growth.
In computer vision, inspired by the success of BEiT~\cite{bao2021beit}, recent research~\cite{ermolov2021whitening,li2020prototypical,gidaris2020learning,henaff2020data,gidaris2020online,PriyaGoyal2021SelfsupervisedPO,zbontar2021barlow,bardes2021vicreg,ren2023deepmim,XiangyuPeng2022CraftingBC,wang2023masked} has explored adapting the transformer architecture to the task of image self-supervised domain. After that, the emergence of MAE~\cite{he2022masked} has further led to a resurgence of interest in reconstruction-based masked image methods, such as~\cite{he2022masked,el2021large,wei2021masked,xie2021simmim,dong2021peco}. We are particularly intrigued by these masked methods, as they have shown state-of-the-art performance on numerous transfer tasks and are computationally efficient. This has motivated us to introduce MIRL, a novel approach that builds on these methods.
 

 


 \textbf{Relation to scaling models.}
Scaling deeper ConvNets~\cite{he2016deep,tan2019efficientnet, simonyan2014very,szegedy2015going, srivastava2015highway, szegedy2017inception} is an effective way to attain improved performance, but the same cannot be easily achieved with ViTs~\cite{riquelme2021scaling}. 
While the Transformer architecture has succeeded in building large-scale language models~\cite{chowdhery2022palm, wei2022emergent,t5paper,tay2022unifying,chung2022scaling,fedus2021switch}, 
the implementation of scalable Transformers for visual models still significantly lags behind. 
Recent work~\cite{zhou2021deepvit,lepikhin2020gshard,wang2022deepnet,dehghani2023scaling, zhai2022scaling} has endeavored to explore deep Transformer-like models. These studies introduce necessary modifications to the original Transformer architecture, such as parallel layers, altered layer norm positions, composite attention mechanisms, larger embedding dimensions, unique optimization strategies, and exhaustive hyperparameter searches.
Although they have demonstrated commendable performance, they lack a guiding principle about how to deepen or enlarge the Transformer-like models.
Contrary to previous methods, our approach is rooted in an in-depth analysis, dissecting the standard ViT architecture. This allows us to identify the challenges in fully realizing the potential deeper ViTs and develop effective solutions accordingly. Building upon the principles we proposed, we efficiently construct deep-scale ViT models.







\vspace{5pt}
\section{Conclusion}
\vspace{5pt}
In this paper, we first reveal a performance degradation problem in Vison Transformers (ViTs) when pre-training with masked image modeling (MIM). Through an in-depth experimental analysis, we determine that the degradation is caused by the negative optimization effect of MIM enforced on deeper layers of ViT.
We then introduce a novel concept of masked image residual learning (MIRL) to establish a self-supervised learning framework, aimed at alleviating the performance degradation problem. Leveraging MIRL, we unleash the potential of deeper ViTs and instantiate deeper encoders, including ViT-S-54, ViT-B-24 and ViT-B-48. These deeper ViTs variants exhibit superior generalization performance on downstream tasks. 

\textbf{Broader impacts.} 
The proposed approach, which predicts content from training data statistics, may reflect biases with adverse societal impacts and generate non-existent content, underlining the need for further research in this area.



{
\small
\bibliographystyle{ieee_fullname}
\bibliography{mirl.bbl}
}


\appendix

\section{Implementation details}
\subsection{Details for ImageNet experiments}
\label{sec: details_imagenet}

\textbf{Pre-training.}
We mostly adopt the pre-training setting in~\cite{he2022masked}, except for that we adopt shorter fewer training epochs.
The default pre-training setting is provided in Table~\ref{tab: impl_pre_train}. The learning rate \textit{lr}\textit{base\_lr}batchsize / 256.
\begin{table}[htbp]
\small
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{20pt}
\begin{tabular}{@{}l  l@{}}
\toprule
Pre-training Config. & Value \\
\midrule
optimizer & AdamW \\
base learning rate & 1.5e-4 \\
weight decay & 0.05 \\
optimizer momentum &  \\
batch size & 4096 \\
learning rate schedule & cosine decay \\
warmup epochs & 20/40 \\
training epochs & 300/800 \\
augmentation & RandomResizedCrop \\
\bottomrule
\end{tabular}
\caption{Pre-training setting.}
\label{tab: impl_pre_train}
\end{table}

\textbf{Fine-tuning.} The layer-wise learning rate decay~\cite{clark2020pre} factors of the deeper ViTs are set to larger than those of the shallower ViTs to maintain similar learning rates in the lowest layers for all models. The fine-tuning setting is provided in~Table~\ref{tab: impl_ft}. In our experiment, models are insensitive to the droppath~\cite{huang2016deep} configuration, and setting it to 0 does not lead to any noticeable differences.  
We employ EMA to enhance tuning performance on small datasets consisting of only a few hundred training samples, such as private industrial datasets. When tuning on these limited-scale datasets, we gravitate towards loading weights from the ImageNet fine-tuned model instead of the MIM pre-trained model because the MIM lacks semantic features. In this context, using the EMA-fine-tuned models yields better tuning accuracy on tiny datasets, especially when picking a non-final checkpoint, compared to counterparts without EMA.
However, EMA does not significantly impact the fine-tuning accuracy on ImageNet. The reason could be attributed to the sufficiently long training period (e.g., spanning 800 pre-training epochs + 200/100 fine-tuning epochs). This allows models with or without EMA to likely converge to the same optimum. Nonetheless, EMA remains crucial for training models from scratch, as indicated in [1] (e.g., ViT-B achieves 82.3\% accuracy with EMA and 82.1\% without EMA).
\begin{table}[htb]
\small
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{@{}l  l@{}}
\toprule
Fine-tuning Config. & Value \\
\midrule
optimizer & AdamW \\
base learning rate & 7.5e-4 \\
weight decay & 0.05 \\
optimizer momentum &  \\
layer-wise lr decay~\cite{clark2020pre} &  0.65(S), 0.88(S-54), 0.65(B), 0.65(B-24), 0.88(B-48) \\ 
batch size & 2048 \\
learning rate schedule & cosine decay \\
warmup epochs & 20 \\
training epochs &  200(S), 100(S-54), 100(B), 100(B-24), 50(B-48) \\
augmentation & \texttt{RandAug} (9, 0.5) \cite{cubuk2020randaugment} \\
label smoothing \cite{szegedy2016rethinking} & 0.1 \\
mixup \cite{zhang2017mixup} & 0.8 \\
cutmix \cite{yun2019cutmix} & 1.0 \\
drop path \cite{huang2016deep} & 0.1(S), 0.1(S-54) 0.1(B), 0.1(B-24) 0.2 (B-48) \\
exp. moving average  & 0.9998 \\
\bottomrule
\end{tabular}
\caption{Fine-tuning setting.}
\label{tab: impl_ft}
\end{table}

\subsection{Details for transfer learning experiments}



\textbf{Object detection on COCO.} For a fair comparison, we conduct experiments using the Mask R-CNN framework. We utilize multi-scale training and resize the
image with the size of the short side
between  and  and the long side no larger than . 
we initialize the backbone with the pre-trained ViT model.
During fine-tuning, the batch size is  and the learning rate is e-. 
For ViT-B, the layer decay rate is , and the drop path rate is . 
For ViT-B-48, the layer decay rate is , and the drop path rate is . Other training configurations are adopted from \texttt{mmdetection}~\cite{chen2019mmdetection}. We do not use multi-scale testing.

\textbf{Semantic segmentation on ADE20K.} We adopt the UperNet~\cite{xiao2018unified} framework for semantic segmentation, following the implementation of~\cite{bao2021beit}. We initialize the backbone with the pre-trained weights and fine-tune the entire model for 160k iterations with a batch size of 16. The learn rate is set to 0.0002. Different from the implementation of~\cite{bao2021beit}, we do not use relative position bias in our models.



\section{Other pre-training objectives}
\subsection{Feature-level and VGG losses}
\textbf{Feature-level loss.}
Regarding the feature-level loss, we employ the InfoNCE loss used in contrastive learning:

where  is the prediction,  denotes a temperature parameter.
In a batch with  images,  represent a positive pair in which positive sample  is a momentum encoder's output on the same view of the image as . The momentum encoder's parameters are the moving average of the encoder.  represents a negative pair where negative sample  is generated with an image different from that of  in the image batch. Previous work in~\cite{chen2022sdae,dong2022bootstrapped} eliminates negative sample comparisons in their feature-level loss, which emphasizes the importance of positive samples, resembling a BYOL style~\cite{grill2020bootstrap}, but we find that involving in negative samples can slightly improve the accuracy. The feature-level loss is only calculated at the end of the encoder, by appending two decoding blocks to predict the masked features.


\textbf{VGG loss.}
VGG loss is previously used in generative models~\cite{johnson2016perceptual,esser2021taming}, eliminating the influence of pixel shifting for high-quality image synthesis. Specifically, for reconstruction from -th prediction head, we replace the reconstructed patches in visible positions with the ground-truth image patches to ease the optimization difficulty, given . 
The mixed fake image  and growth-truth image  are forwarded to a fixed, lightweight VGG model, and the VGG loss is calculated by measuring the difference between their VGG activations from multiple layers, which is formulated as:

where  denotes the activations of the -th layer of the VGG network by inputting ;  represents the dimensions of the activation feature map,  denotes a set of layers from which the VGG features are extracted. Concurrent work~\cite{dong2021peco} also experiments with VGG loss.

\subsection{An alternative definition of loss } One of our early attempts regarding the form of reconstruction loss is defined as:

where  is reconstruction loss defined in~Eq.\eqref{eq: loss_g} from the main paper,  refers to the regularization weight to loss term .
This variant  minimize the similarity between the original image  and the reconstructed image  with reference to the first loss term, . By minimizing the second loss term, , the deeper segment explicitly learn the image residual. In Table~\ref{tab: comp_loss_g}, we compare the results generated by using loss  and loss . When setting  from Eq.~\eqref{eq: variant_loss_g} to 1, we observe that the two different loss definitions generate similar results in a shorter pre-training period (\eg, 100 epochs). However, when we pre-train the model for a longer period (\eg, 300 epochs), training with loss  defined in Eq.\eqref{eq: loss_g} from the main paper can provide better results than loss . We give the reason that  determinedly minimize the distance between  and , which could result in a very small residual. As the residual is the optimization target for those deeper segments, such a small residual due to the  term could corrupt the training in the deeper segments. Alternatively, by setting  to a smaller value (\eg, 0.1), we achieve a smoother optimization experience; nonetheless, the results are similar to those obtained when optimizing  alone. 
\begin{table}
    \centering
    \small
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{@{}l c c@{}}
    \toprule
    loss definition  & pre-training epochs & fine-tuning  \\
    \midrule
     & 100 &   83.5 \\
     & 100 &  83.5 \\
    \midrule
     & 300 &   84.2 \\
     & 300 &  84.0  \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between loss   and loss . The encoder is ViT-S-54.  in  is set to 1. We adopt a step-wise decay learning rate scheduler.  }
    \label{tab: comp_loss_g}
\end{table}



\begin{figure}[!t]
\flushleft 
\includegraphics[width=1.0\textwidth]{imgs/visual2.png}
{\caption{\label{fig: visual_examples2} Example results on ImageNet validation images. For each quintuplet, we show the ground-truth, masked image, reconstruction, residual and the main component.
}}
\end{figure}


\begin{figure}[!t]
\flushleft 

\includegraphics[width=0.24\textwidth]{grads/19_qkv.png}
\includegraphics[width=0.24\textwidth]{grads/19_mlp.png}
\includegraphics[width=0.24\textwidth]{grads/19_ln.png}
\includegraphics[width=0.24\textwidth]{grads/19_fc.png}

\includegraphics[width=0.24\textwidth]{grads/20_qkv.png}
\includegraphics[width=0.24\textwidth]{grads/20_mlp.png}
\includegraphics[width=0.24\textwidth]{grads/20_ln.png}
\includegraphics[width=0.24\textwidth]{grads/20_fc.png}

\includegraphics[width=0.24\textwidth]{grads/21_qkv.png}
\includegraphics[width=0.24\textwidth]{grads/21_mlp.png}
\includegraphics[width=0.24\textwidth]{grads/21_ln.png}
\includegraphics[width=0.24\textwidth]{grads/21_fc.png}

\includegraphics[width=0.24\textwidth]{grads/22_qkv.png}
\includegraphics[width=0.24\textwidth]{grads/22_mlp.png}
\includegraphics[width=0.24\textwidth]{grads/22_ln.png}
\includegraphics[width=0.24\textwidth]{grads/22_fc.png}
{\caption{\label{fig: gard_examples} Example visualization of gradient norm.
\textcolor{orange}{Original} is MIRL, while \textcolor{gray}{gray} is MAE. 
From top to bottom, we visualize the gradient norms in 19-22 Transformer blocks in ViT-B-24. Each row contains the gradients of attention qkv, mlp, layer norm and fc weights. 
}}
\end{figure}


\section{More Visualization}
In Figure~\ref{fig: gard_examples}, we provide gradient norm visualization for MIRL and MAE. We observe that when employing MIRL for pre-training, the gradient magnitudes of Transformer blocks are larger than those when using MAE. This suggests that MIRL provides a more stable gradient flow that benefits the model optimization.

In Figure~\ref{fig: visual_examples2}, we provide more visualization about image reconstruction.

\section{Inference Speed}
Although we have shown deeper ViTs can gain accuracy from stacking more Transformer blocks, we also notice that deeper ViTs provide lower inference speed due to the series connections between blocks. The inference speed measurement is provided in Table~\ref{tab: infer_speed}. 

\begin{table}[!t]
    \centering
    \small
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{14.1pt}
    \begin{tabular}{@{}l c c c c@{}}
    \\
    \toprule
     model & depth  & \#params & FLOPs & throughput (imgs/s)  \\
    \midrule
    ViT-S & 12 & 22M & 4.2G & 751  \\
    ViT-S-54 & 54 & 96M & 18.8G & 257  \\
    ViT-B & 12 & 86M & 16.8G & 488  \\
    ViT-B-24 & 24 & 171M & 33.5G & 285  \\
    ViT-B-48 & 48 & 341M & 67.0G & 160 \\
    \bottomrule
    \end{tabular}
    \caption{Inference speed, measuring the throughput (images/sec) on a single V100 GPU, where the batch size is set to 256.}
    \label{tab: infer_speed}
\end{table}

\section{Whether the phenomenon in observation II still exists in MIRL?}
We devise an additional model named "truncated MIRL". The concept is akin to the truncated MAE depicted in Figure~\ref{fig: manners}(b) from the main paper. It involves pre-training the early encoding blocks using MIRL, while the subsequent blocks are randomly initialized. As detailed in Table~\ref{tab: truncatedMIRL}, MIRL outperforms truncated MIRL by 0.3\%. This demonstrates that MIRL effectively pre-trains the deeper layers, outperforming random initialization. This also suggests that the phenomenon observed in Observation II does not exist in the MIRL method. 
\begin{table}[!t]
    \centering
    \small
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{14.1pt}
    \begin{tabular}{@{}l c c@{}}
    \\
    \toprule
     encoder & method  & ft accuracy (\%) \\
    \midrule
    ViT-S & MIRL & 82.3  \\
    ViT-S & truncated MIRL & 82.0  \\
    ViT-S & MAE & 81.0  \\
    ViT-S & truncated MAE & 81.7  \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between MIRL and truncated MIRL. For both truncated MIRL and truncated MAE, 3 blocks are not involved in pre-training, and the 5th block solely focuses on recovering the masked content.}
    \label{tab: truncatedMIRL}
\end{table}

\end{document}