
\documentclass[preprint,1p,11pt]{IR-Template/ISAS_IR}

\usepackage{bm} \usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont} \usepackage{graphicx}
\usepackage{subfigure}
\usepackage[hidelinks]{hyperref} \usepackage[absolute,overlay]{textpos}
\usepackage{theorem}
\usepackage{color}
\usepackage{booktabs}
\usepackage{enumitem}

\usepackage{multirow}

\usepackage{bbm} 

\usepackage[protrusion=false]{microtype}

\usepackage[ruled,vlined]{algorithm2e}

\clubpenalty = 100000
\widowpenalty = 100000
\brokenpenalty = 100000
\finalhyphendemerits = 100000
\displaywidowpenalty = 100000

\newcommand{\rv}[1]{\ensuremath{{\boldsymbol{#1}}}}
\newcommand{\nvec}[1]{\ensuremath{{{\underline{#1}}}}}
\newcommand{\rvec}[1]{\ensuremath{{\boldsymbol{\underline{#1}}}}}
\newcommand{\mat}[1]{{\ensuremath{{\mathbf{#1}}}}}
\newcommand{\eig}[1]{{\rm eig}\left( {#1} \right)}
\newcommand{\Ex}{{\rm E}}
\newcommand{\mytrace}{{\rm tr}}
\newcommand{\pinv}{^\dagger}
\newcommand{\tr}{^{\top}}
\newcommand{\inv}{^{-1}}
\DeclareMathOperator{\ttrace}{trace}
\newcommand{\trace}[1]{\ttrace\bklammer{#1}}
\newcommand{\expect}[1]{\mathrm{E}\left\lbrace #1 \right\rbrace}
\newcommand{\ssep}[1]{\left. #1 \right|}
\newcommand{\expectcond}[2]{\expect{\hspace{-.1cm}\ssep{#1} #2}}
\newcommand{\ls}{\left\{}
\newcommand{\rs}{\right\}}
\newcommand{\IN}{\mathbb{N}}  \newcommand{\IR}{\mathbb{R}}  \newcommand{\IC}{\mathbb{C}}  

\DeclareMathOperator{\vvec}{vec}
\renewcommand{\vec}[1]{\vvec\klammer{#1}}
\DeclareMathOperator{\ddevec}{devec}
\newcommand{\devec}[1]{\ddevec\klammer{#1}}
\DeclareMathOperator{\st}{subject\ to}
\DeclareMathOperator{\drand}{rand}
\newcommand{\rand}[1]{\drand\klammer{#1}}
\DeclareMathOperator{\Inf}{Inf}

\newcommand{\zeromatrix}{\mat{0}}
\newcommand{\identitymatrix}{\mat{I}}

\newcommand{\todo}[1]{{\color{red} #1}}
\newcommand{\klammer}[1]{\left( #1 \right)}
\newcommand{\bklammer}[1]{\left[ #1 \right]}

\DeclareMathOperator{\pp}{P}
\renewcommand{\Pr}{\pp}



\newcommand{\dirac}[1]{\delta_{#1}}
\newcommand{\indicator}[1]{\mathbbm{1}_{#1}}
\newcommand{\kron}[2]{{#1 \otimes #2}}
\DeclareMathOperator{\diag}{diag}


\newcommand{\Asys}[1]{\mat{A}_{#1}}
	\newcommand{\Bsys}[1]{\mat{B}_{#1}}
	\newcommand{\Qsys}[1]{\mat{Q}_{#1}}
	\newcommand{\Rsys}[1]{\mat{R}_{#1}}
	\newcommand{\Hsys}[1]{\mat{H}_{#1}}
	\newcommand{\Csys}[1]{\mat{C}_{#1}}
	\newcommand{\Jsys}[1]{\mat{J}_{#1}}
	
	\newcommand{\xsys}[1]{\rvec{x}_{#1}}
	\newcommand{\usys}[1]{\nvec{u}_{#1}}
	\newcommand{\ysys}[1]{\rvec{y}_{#1}}
	\newcommand{\wsys}[1]{\rvec{w}_{#1}}
	\newcommand{\vsys}[1]{\rvec{v}_{#1}}
	
	\newcommand{\wCov}{\mat{W}}
	\newcommand{\vCov}{\mat{V}}
	\newcommand{\Xest}[2]{\widehat{\mat{X}}^{(#1)}_{#2}}
	
	\newcommand{\xest}[1]{\widehat{\nvec{x}}_{#1}}
	
	\newcommand{\xaug}[1]{\widetilde{\rvec{x}}_{#1}}
	\newcommand{\waug}[1]{\widetilde{\rvec{w}}_{#1}}
	\newcommand{\Aaug}[1]{\widetilde{\mat{A}}_{#1}}
	\newcommand{\Baug}[1]{\widetilde{\mat{B}}_{#1}}
	\newcommand{\Qaug}[1]{\widetilde{\mat{Q}}_{#1}}
	\newcommand{\wAug}{\widetilde{\mat{W}}}
	
	\newcommand{\vectorX}[2]{\nvec{\chi}^{(#1)}_{#2}}
	\newcommand{\stackedVectorX}[1]{\widetilde{\nvec{\chi}}_{#1}}
	\newcommand{\bigN}[1]{\nvec{\rho}_{#1}}
	\newcommand{\bigW}{\nvec{\psi}}
	
	\newcommand{\mode}[1]{\rv{\theta}_{#1}}
	\newcommand{\modeis}[1]{\theta_{#1}}
	\newcommand{\modeest}[2]{\widehat{\theta}^{(#1)}_{#2}}
	\newcommand{\modeinf}{\widehat{\theta}_{\infty}}
	
	\newcommand{\spectralradius}[1]{\rho\klammer{#1}}
	\newcommand{\spectralradiusopt}[1]{\widetilde{\rho}\klammer{#1}}
	
	\newcommand{\bigM}{\mat{M}}

	\newcommand{\TransitionMatrix}[1]{\mat{T}_{#1}}
	\newcommand{\transitionprob}[1]{p_{#1}}
	
	\newcommand{\Xsysest}[2]{\mat{X}^{(#1)}_{#2}}	
	\newcommand{\Xpart}[2]{\mat{X}^{(#1)}_{#2}}	
	\newcommand{\ControlLaw}{\mat{L}}
	\newcommand{\EstimationLaw}{\mat{G}}
	\newcommand{\Acon}{\mat{F}}
	
	\newcommand{\GradMat}[2]{\mat{M}^{(#1)}_{#2}}
	
	\newcommand{\NumModes}{M}		
	\newcommand{\terT}{K}
	\newcommand{\CostFunc}{\mathcal{J}}
	
	\renewcommand{\P}[1]{\mat{P}_{#1}}
	\newcommand{\Pest}[2]{\mat{P}^{(#1)}_{#2}}
	\newcommand{\Lagrange}[2]{\mat{\Lambda}^{(#1)}_{#2}}
	\newcommand{\Hamiltonian}{\mathcal{H}}
	
	\newcommand{\vectorizedState}[1]{\nvec{\psi}_{#1}}
	\newcommand{\RiccatiP}[1]{\mat{V}_{#1}}
	\newcommand{\OpEpsilon}[2]{\mathcal{E}_{#1}(#2)}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{proof}{Proof}
\newtheorem{proposition}{Proposition}


\begin{document}
    \begin{frontmatter}
        \title{Infinite-horizon Linear Optimal Control of\\Markov Jump Systems without Mode Observation\\via State Feedback}
        
        \author{Maxim~Dolgov}
        \ead{maxim.dolgov@kit.edu}
        
        \author{Uwe~D.~Hanebeck}
        \ead{uwe.hanebeck@ieee.org}
        
        \address{Intelligent Sensor-Actuator-Systems Laboratory (ISAS)\\
                 Institute for Anthropomatics and Robotics\\
                 Karlsruhe Institute of Technology (KIT), Germany\vspace{3mm}}
        
        \begin{abstract}
        In this paper, we consider stochastic optimal control of Markov Jump Linear Systems with state feedback but without observation of the jumping parameter. The proposed control law is assumed to be linear with constant gains that can be obtained from the necessary optimality conditions using an iterative algorithm. The proposed approach is demonstrated in a numerical example.
        \end{abstract}
    \end{frontmatter}
    
   \section{Introduction}
   	\label{sec:Introduction}
   	Since their introduction by Krasovskii and Lidskii in 1969 \cite{Krasovskii_1961_1,Krasovskii_1961_2,Krasovskii_1961_3}, Markov Jump Linear Systems (MJLS) have received a considerable amount of interest. This is due to their ability to capture systems whose dynamics are subject to abrupt changes that are not independently distributed. MJLS modeling approach is used, e.g., in networked control~\cite{Hespanha_2007,ACC13_Fischer}, economics~\cite{doVal_1999,Elliott_2007}, or control of systems with component failures~\cite{Vargas_2013}.\\

Most works that consider control of MJLS assume availability of the jumping parameter or \emph{mode} that models the abrupt model switching. This assumption allows to derive optimal control laws in continuous~\cite{Sworder_1969} and discrete time~\cite{Chizeck_1986,Fragoso_1989} for systems with state feedback. For measurement-feedback case, mode availability guarantees that the separation between control and estimation holds. Thus, the optimal control law consists of an optimal linear regulator and an optimal Kalman filter~\cite{Chizeck_1988}.\\

However, if the mode is not available, the control law becomes nonlinear because of the dual effect~\cite{Griffiths_1985,Casiello_1989}. In this case, the optimal solution is computationally intractable due to the curse of dimensionality. Thus, research concentrates on approximate control laws. We distinguish between two classes of approaches: (i) approaches based on assumed separation and (ii) approaches based on structural assumptions. Approaches that belong to the first class approximate the involved conditional densities. By doing so, it is possible to establish separation. Then, the optimal control law consists of an estimator and a regulator whose gains are linear. The estimator is either based on an Interacting Multiple Model (IMM) algorithm~\cite{Campo_1992} or on a Viterbi-like algorithm~\cite{Gupta_2003}. Approaches that belong to the second class make an assumption considering the control law, usually that the control law is linear such as in~\cite{doVal_1999,Costa_2000,Vargas_2013,Vargas_2014}.\\

Between full mode observation and no mode observation is the clustered mode observation. The term clustered can refer to (1) temporal interchange between full mode observation and no mode observation, and (2) observation of subsets of modes, i.e., observation whether one of the modes in a subset is active or not. We will not review this field in our paper. We refer an interested reader to, e.g.,~\cite{doVal_1999,doVal_2002} and the references therein.\\

In this paper, we take the approach (ii) and assume the controller to be linear and to possess constant regulator gain. Our approach differs from the works \cite{doVal_1999,Costa_2000,Vargas_2013,Vargas_2014} in the following way: \cite{doVal_1999,Costa_2000,Vargas_2013} assume time-variant controller gains and \cite{Vargas_2014} considers finite-horizon control with constant gains. And the works \cite{doVal_1999,Costa_2000,Vargas_2013,Vargas_2014} have to be implemented in a receding-horizon framework to be applicable for long operation times. The approach presented in \cite{Vargas_2014} can be used to compute constant gains. However, in this case, the optimization horizon becomes a parameter that must be chosen sufficiently large in order to obtain an infinite-horizon control law. To obtain the controller gain for the approach presented in this paper, we minimize an infinite-horizon cost function. By doing so, there is neither a need for choosing an optimization horizon, nor for implementing the control law in a receding-horizon framework. However, the latter can be done in order to, e.g., adapt the control law to changes in the system dynamics (both continuous- and discrete-valued), if desired. As we will see in the numerical example, the performance of the proposed controller, although it is time-invariant, can be almost identical to the performance of the receding-horizon time-variant controller from~\cite{Vargas_2013}.\\

The dynamics of the discrete-time MJLS considered in this paper are given by

where  denotes the system state,  the control input, and  the independent and identically distributed (i.i.d.) zero-mean second-order noise with covariance . Here  is the expectation operator and  denotes the transpose of . The matrices , , and  are selected from time-invariant sets of matrices , , etc. according to the jumping parameter  which is the state of a regular homogeneous Markov chain. We will refer to  as the \emph{mode}. The regularity assumption guarantees that the limit distribution  of  exists~\cite{Grinstead_2003}.\\

The performance of the controlled system is measured by an infinite-horizon cost function

where for  the mode-dependent cost matrices  are positive semidefinite and  are positive definite, respectively.\\

The task is to find a control law that minimizes (\ref{eq:CostFunc}). As mentioned above, the optimal nonlinear control law that solves this task is computationally intractable. Thus, we make a structural assumption and choose the control law to be linear, mode-independent, and constant, i.e.,

With this control law assumption, the considered problem can be formulated as


\emph{Outline.} The remainder of the paper is organized as follows. Before we present the main result in Sec.~\ref{sec:MainResult}, we introduce necessary definitions in the next section. A numerical example is given in Sec.~\ref{sec:NumericalExample} and Sec.~\ref{sec:Conclusion} concludes the paper.
    
   \section{Prerequisites}
   	\label{sec:Definitions}
   	Consider the MJLS 

with  being the system state,  being the state of a regular, homogeneous Markov chain with transition matrix , and .
\begin{definition}[Mean Square Stability]\hfill\\
System (\ref{eq:StabSysDynamics}) is mean square (MS) stable for any initial  and , if it holds

\label{def:MSS}
\end{definition}

\begin{remark}
If system~(\ref{eq:StabSysDynamics}) is affected by zero-mean second-order noise  such that

then the second moment  converges to a fixed point that is not , i.e.,

where  are positive semidefinite. This claim can be shown using Banach's fixed point theorem (see~\ref{app:ProofConvergence}).
\end{remark}

The following theorem provides necessary and sufficient conditions for MS stability.
\begin{theorem}
For system~(\ref{eq:StabSysDynamics}), the two following conditions for mean square stability exist.
\begin{enumerate}[label = \alph*)]
\item 
	System~(\ref{eq:StabSysDynamics}) is MS stable, if for any positive matrices  there exist positive definite matrices  such that

where  denotes the transition probabilities from mode  to mode .
\item
	System~(\ref{eq:StabSysDynamics}) is MS stable, if for the spectral radius  of the matrix

where  is the Kronecker product and  the block diagonalization operator, it holds

\end{enumerate}
\label{theor:MSS}
\end{theorem}
\begin{proof}
The proofs are given in~\cite{Morozan_1983,Fang_2002,Ling_2012} for systems with real-valued state and in~\cite{Costa_1993} for systems with complex-valued state.
\end{proof}

Next, we define MS stabilizability.
\begin{definition}[Mean Square Stabilizability]\hfill\\
System

with  being the system state,  being the state of a regular, homogeneous Markov chain, and , , is linearly mean square (MS) stabilizable without mode observation, if there exists a matrix  such that

is mean square stable.
\label{def:MSStab}
\end{definition}
    	
   \section{Main Result}
   	\label{sec:MainResult}
   	Before we present the necessary optimality conditions for~(\ref{eq:OptimizationProblem}), we define the second-moment system state

where  if  and  otherwise. The dynamics of the second-moment system state are

where  is the probability of being in mode  at time step .
\begin{theorem}[Necessary Optimality Conditions]\hfill\\
The necessary optimality conditions for the optimization problem~(\ref{eq:OptimizationProblem}) are given by

where ,  are positive definite, and .
\label{theor:Main}
\end{theorem}
\begin{proof}
The proof is given in~\ref{app:ProofMainTheorem}.
\end{proof}

Please observe that equations~(\ref{eq:CoupledEquationsP}) constitute a set of coupled Riccati-like equations that reduce to the uncoupled Riccati equations if system~(\ref{eq:SystemDynamics}) has only one mode.\\

Finding a solution of~(\ref{eq:CoupledEquationsP})-(\ref{eq:CoupledEquationsL}) is not trivial. We propose to use a scheme similar to that presented in~\cite{DeKoning_1992} or~\cite{Bernstein_1987}. To this end, we first rewrite~(\ref{eq:CoupledEquationsL}) using the vectorization operator as

Solving for  yields

where  denotes the Moore-Penrose pseudoinverse of .\\

The numerical algorithm is the following.\\

\begin{enumerate}[leftmargin = 1.4cm,rightmargin = 1cm,label= \hspace{1cm}\emph{Step \arabic*}:]
\item Initialize  and  with random values and compute .

\item Compute

and reverse the vectorization operator in order to obtain , i.e.,

with .

\item Compute

Stop if  converged. Otherwise, return to \emph{Step 2}.
\end{enumerate}

\begin{remark}
As in the case with i.i.d. system parameters considered in~\cite{DeKoning_1992}, convergence of the given algorithm does not always guarantee stability of the MJLS. Thus, it is always necessary to check if the computed control law stabilizes~(\ref{eq:SystemDynamics}) using Theorem~\ref{theor:MSS} with

\end{remark}

\begin{remark}
In order to check whether a MJLS is MS-stabilizable, it is possible to use the procedure described in Appendix~\ref{app:StabilizabilityTest}.
\end{remark}



%
    	
   \section{Numerical Example}
   	\label{sec:NumericalExample}
   	In order to demonstrate the performance of the proposed control approach, we performed Monte Carlo simulation runs with  time steps each for different system and noise parameters. For each run, we computed the control law using different random initial guesses. The evolution of the mode and the noise were also randomly generated for each run. For comparison, we used the optimal controller published by Chizeck et al.~\cite{Chizeck_1986} that needs a mode feedback, and the finite-horizon controller without mode availability presented by Vargas et al.~\cite{Vargas_2013}.\\

The constant parameters of the simulated MJLS were chosen to


We considered two different noise scenarios with

three different Markov chains with

and two different initial states

The spectral radii of the corresponding matrices constructed according to~(\ref{eq:AugmentedMatrix}) are 

which shows that the MJLS is unstable for each of the transition matrices , , and .\\

Fig.~\ref{fig:ExampleRun} depicts the state trajectory, the applied control inputs, and the modes of the MJLS of an example run with , , and . Although the controller from~\cite{Vargas_2013} has time-variant gains while the proposed controller is time-invariant, the trajectories of both controllers are very similar.\\

The results of the Monte Carlo simulation with  are depicted in Fig.~\ref{fig:MC_x0_1} and the corresponding mean values of the costs are given in Table~\ref{tab:Costs_1}. In this scenario, the performance of the proposed controller and the controller from~\cite{Vargas_2013} is only slightly worse than the performance of the optimal controller with mode observation. And the performance of the proposed controller and the controller from~\cite{Vargas_2013} is almost equal. For , the simulation results are depicted in Fig.~\ref{fig:MC_x0_2} and the mean costs are given in Table~\ref{tab:Costs_2}. In this second scenario, the proposed control law performs well compared to the two other controllers if the noise covariance is large. However, the performance is worse if the noise covariance is low and the transition matrix is either  or . It is important to note that in contrast to the controller from~\cite{Vargas_2013}, the proposed controller is precomputed offline and does not depend on the initial state  and the initial mode . Thus, the computational footprint during operation is low.\\

\begin{figure}[h]
\centering
\includegraphics[width = .8\textwidth]{Graphics/ExampleRun.pdf}
\caption{Example run of the three compared controllers with , , and .}
\label{fig:ExampleRun}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width = .9\textwidth]{Graphics/BoxPlot_x0_1.pdf}
\caption{Results of the Monte Carlo simulation. Depicted are the costs of the three compared controllers for different transition matrices and noise covariances, and .}
\label{fig:MC_x0_1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width = .9\textwidth]{Graphics/BoxPlot_x0_2.pdf}
\caption{Results of the Monte Carlo simulation. Depicted are the costs of the three compared controllers for different transition matrices and noise covariances, and .}
\label{fig:MC_x0_2}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{ccccccc}
\toprule
& \multicolumn{3}{c}{} & \multicolumn{3}{c}{}\\ \cmidrule{2-7}
& Chizeck et al. & proposed & Vargas et al.& Chizeck et al. & proposed & Vargas et al.\\\cmidrule{1-7}
\multicolumn{1}{c}{}&  &  &  &  &  &  \\
\cmidrule{1-7}
\multicolumn{1}{c}{}&  &  &  &  &  &  \\
\cmidrule{1-7}
\multicolumn{1}{c}{}&  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\caption{Mean costs of the three compared controllers for .}
\label{tab:Costs_1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ccccccc}
\toprule
& \multicolumn{3}{c}{} & \multicolumn{3}{c}{}\\ \cmidrule{2-7}
& Chizeck et al. & proposed & Vargas et al.& Chizeck et al. & proposed & Vargas et al.\\\cmidrule{1-7}
\multicolumn{1}{c}{}&  &  &  &  &  &  \\
\cmidrule{1-7}
\multicolumn{1}{c}{}&  &  &  &  &  &  \\
\cmidrule{1-7}
\multicolumn{1}{c}{}&  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\caption{Mean costs of the three compared controllers for .}
\label{tab:Costs_2}
\end{table}

An implementation of the presented control law is available at the CloudRunner homepage~\cite{Cloudrunner}.    	
   \section{Conclusion}
   	\label{sec:Conclusion}
   	In this paper, we presented a method to compute a constant linear policy for infinite-horizon optimal control of stochastic MJLS with state feedback but without mode observation. To this end, we have rewritten the MJLS dynamics in terms of the second moment, constructed the Hamiltonian, and proposed an iterative algorithm that minimizes the cost function.\\

In the provided numerical example, the proposed control law has only slightly worse performance than the control laws from~\cite{Vargas_2013} and~\cite{Chizeck_1986} although it is mode-independent, time-invariant, and can be precomputed offline.\\

Future work will be concerned with derivation of convergence guarantees for the iterative algorithm and an extension of the proposed approach to measurement-feedback control. Furthermore, an assumption of a more complicated policy structures such as polynomials constitutes another possible research direction.    	
   \section{ACKNOWLEDGMENTS}
This work was supported by the German Science Foundation
   (DFG) within the Research Training Group RTG 1194 ``Self-organizing Sensor-Actuator-Networks''.
   	
   \appendix
   \section{Proof of Theorem~\ref{theor:Main}}
   	\label{app:ProofMainTheorem}
   	If the dynamics~(\ref{eq:SystemDynamics}) is mean square stabilizable then the second-moment state converges to a fixed point  that is the unique solution of

This claim is proven in~\ref{app:ProofConvergence}.\\

Thus, the costs~(\ref{eq:CostFunc}) are finite and can be rewritten as~

and the optimization problem~(\ref{eq:OptimizationProblem}) becomes

Defining the positive semidefinite Lagrange multiplier , we obtain the Hamiltonian~ of~(\ref{eq:OptimizationProblemRestated}) with

Differentiation with respect to , , and  yields~(\ref{eq:CoupledEquationsP})-(\ref{eq:CoupledEquationsL}).


    	
   \section{Proof of Second Moment Convergence}
   	\label{app:ProofConvergence}
   	According to Banach's fixed point theorem,  converges to the unique solution  for  if (\ref{eq:SecondMomentDynamics}) is a contraction mapping. To show that (\ref{eq:SecondMomentDynamics}) is indeed a contraction mapping if (\ref{eq:SystemDynamics}) is MS-stabilizable, we define the vectorized second moment state vector

where  denotes the vectorization operator. The dynamics of  can be written as

with  as in Theorem~\ref{theor:MSS} and

We need to show that

where  and

for any positive semidefinite . Using Lemma~5 from~\cite{Wang_86}, it holds

where  is the largest eigenvalue of . Because for  (\ref{eq:ContractionMapping}) is a contraction mapping, it has a unique fixed point.\\

Please note that the obtained result corresponds to the stability condition in Theorem~\ref{theor:MSS}.b because  holds.    	
   \section{Stabilizability Test}
   	\label{app:StabilizabilityTest}
   	In order to determine whether the MJLS~(\ref{eq:SystemDynamics}) is MS-stabilizable, we can solve the following optimization problem

where

with

If the solution  of~(\ref{eq:SpectralRadiusMinimization}) it holds  then system~(\ref{eq:SystemDynamics}) is MS-stabilizable and we can compute the optimal linear control law according to the numerical algorithm provided in Sec.~\ref{sec:MainResult}. Fig.~\ref{fig:SpectralRadii} illustrates the spectral radii for the system from Sec.~\ref{sec:NumericalExample}. It can be seen that the value function in~(\ref{eq:SpectralRadiusMinimization}) is convex in this scenario.\\

\begin{figure}
\centering
\includegraphics[width=.9\textwidth]{Graphics/SpectralRadii.pdf}
\caption{Spectral radii for the MJLS from Sec.~\ref{sec:NumericalExample}.}
\label{fig:SpectralRadii}
\end{figure}

However, the value function  in~(\ref{eq:SpectralRadiusMinimization}) is non-smooth. Thus, we propose to use the smooth convex approximation presented in~\cite{Chen_2004}. The approximation replaces the spectral radius operator  by

where  and  are the  eigenvalues of .\\

Using this approximation, we let  go from  to  and solve a sequence of optimization problems

Because for the approximation (\ref{eq:Approximation}) it holds

we recover the initial optimization problem (\ref{eq:SpectralRadiusMinimization}) as  goes to zero. Additionally, we can use the gradient and the Hessian given in~\cite{Chen_2004}.     
    \bibliographystyle{IEEEtran}
    \bibliography{Sections/00_Literature}
\end{document}
