\section{Experiment}
\label{sec:exp}

\begin{table}
\centering
\resizebox{\columnwidth}{!}
{
\begin{tabular}{c|c|c|c}
\toprule
Task                     & Dataset Name     & \# Train & \# Test \\ \hline
\multirow{2}{*}{\shortstack{Forgery \\Detection}} & CAISA~\cite{dong2013casia}    & 5,123  & 921   \\ \cline{2-4} 
& IMD20~\cite{novozamsky2020imd2020}    & -  & 2,010  \\ \hline
\multirow{2}{*}{\shortstack{Shadow \\Detection}} 
& ISTD~\cite{wang2018stacked}     & 1,330  & 540  \\ \cline{2-4} 
                         & SBU~\cite{sbu}      & 4,089  & 638  \\ \hline
\multirow{2}{*}{\shortstack{Defocus Blur \\Detection}} & CUHK~\cite{shi2014discriminative}     & 604   & 100  \\ \cline{2-4} 
                         &  DUT~\cite{zhao2018defocus}      & -     & 500  \\ \hline
\multirow{3}{*}{ \shortstack{Camouflaged \\ Object Detection}} & COD10K~\cite{fan2020camouflaged}     & 3,040   & 2,026  \\ \cline{2-4} 
& CAMO~\cite{le2019anabranch}    & 1,000     & 250  \\ \cline{2-4}
&  CHAMELEON ~\cite{skurowski2018animal}      & -     & 76  \\ \bottomrule
\end{tabular}
}
\caption{Summary of datasets considered in this work. We show the number of images in training (\textit{\# Train}) and testing set (\textit{\# Test}) for different datasets.}
\label{tab:dataset}
\end{table} 
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\begin{table*}[t]
	\centering
	\begin{minipage}{0.3\linewidth}
		\centering
        \tablestyle{1pt}{1.05}
        \begin{tabular}{l||cc|cc}
            \toprule
             \multirow{2}{*}{Method} &  \multicolumn{2}{c|}{DUT~\cite{zhao2018defocus}}   & \multicolumn{2}{c}{CUHK~\cite{shi2014discriminative}}  \\  \cline{2-5}
            &  & MAE   &  & MAE    \\ \hline            DeFusionNet~\cite{tang2019defusionnet}  &  .823    & .118  & .818 & .117 \\
            BTBNet~\cite{zhao2019btbnet}  &  .827    & .138  & .889 & .082 \\
            CENet~\cite{zhao2019cenet}    & .817    & .135  & .906 & .059 \\
            DAD~\cite{zhao2021self}     & .794    & .153  & .884    & .079  \\
            EFENet~\cite{zhao2021defocus} & .854    & .094  & .914   & .053  \\ \hline
            Ours & \textbf{.890} & \textbf{.068} & \textbf{.928} & \textbf{.045}  \\ \bottomrule
        \end{tabular} 
\caption{Comparison with state-of-the-art approaches on defocus blur detection.}
    \label{tab:sota_defocus}
	\end{minipage}\quad
    \vspace{.1em}
\begin{minipage}{0.3\linewidth}
		\centering
        \tablestyle{1pt}{1.05}
        \begin{tabular}{l||c|c}
            \toprule
             \multirow{2}{*}{Method}  & ISTD~\cite{wang2018stacked} &  SBU~\cite{sbu}  \\   \cline{2-3}
            & BER & BER    \\ \hline
            BDRAR~\cite{zhu2018bidirectional}  & 2.69   & 3.89   \\
            DSC~\cite{hu2018direction}          & 3.42      & 5.59   \\
            DSD~\cite{zheng2019distraction}                 & 2.17 & 3.45   \\
            MTMT~\cite{mtmt}   & 1.72 & 3.15 \\
            FDRNet~\cite{zhu2021mitigating} & 1.55 & \textbf{3.04} \\ \hline
            Ours & \textbf{1.35} & 4.31 \\ \bottomrule
        \end{tabular}
\caption{Comparison with state-of-the-art approaches on shadow detection.}
    \label{tab:sota_shadow}
	\end{minipage}\quad
    \vspace{.1em}
\begin{minipage}{0.3\linewidth}
		\centering
        \tablestyle{1pt}{1.05}
        \begin{tabular}{l||cc|cc}
            \toprule
             \multirow{2}{*}{Method} & \multicolumn{2}{c|}{IMD20~\cite{novozamsky2020imd2020}}   & \multicolumn{2}{c}{CAISA~\cite{dong2013casia}}  \\ \cline{2-5}
            & F1 & AUC   & F1 & AUC      \\ \hline
            ManTra~\cite{wu2019mantra}    & -         & .748    &  -   & .817       \\
            SPAN~\cite{hu2020span}        &    -    & .750         & .382       & .838       \\
            PSCCNet~\cite{liu2022pscc}        &   -  & .806         & .554       & .875       \\
            TransForensics~\cite{hao2021transforensics} & -   & \textbf{.848}    & .627    & .837       \\ 
            ObjectFormer~\cite{wang2022objectformer}    & -   & .821     & .579     & \textbf{.882}       \\\hline
            Ours  & \textbf{.443} & .807 & \textbf{.636} & .862  \\ \bottomrule
        \end{tabular}
\caption{Comparison with state-of-the-art approaches on forgery detection.}
    \label{tab:sota_forgery}
	\end{minipage}
    \vspace{.1em}
\vspace{1em}
    \begin{minipage}{1.0\linewidth}
		\centering
\begin{tabular}{l||cccc|cccc|cccc}
            \toprule
            \multirow{2}{*}{Method}&  \multicolumn{4}{c|}{CHAMELEON~\cite{skurowski2018animal}} & \multicolumn{4}{c|}{CAMO~\cite{le2019anabranch}} & \multicolumn{4}{c}{COD10K~\cite{fan2020camouflaged}}    \\ \cline{2-13}
            &  &   &   & MAE   &  &   &   & MAE  &  &   &   & MAE \\ \hline
            SINet~\cite{fan2020camouflaged} & .869 & .891 & .740 & .044 & .751 & .771 & .606 & .100 & .771 & .806 & .551 & .051 \\
            RankNet~\cite{lv2021simultaneously}  & .846 & .913 & .767 & .045 & .712 & .791 & .583 & .104 & .767 & .861 & .611 & .045 \\
            JCOD~\cite{li2021uncertainty}  & .870 & .924 & - & .039 & .792 & .839 & - & .082 & .800 & .872 & - & .041 \\
            PFNet~\cite{mei2021camouflaged} & .882 & \textbf{.942} & .810 & .033 & .782 & .852 & .695 & .085 & .800 & .868 & .660 & .040  \\
            FBNet~\cite{jiaying2022frequency} & \textbf{.888} & .939 & \textbf{.828} & \textbf{.032} & .783 & .839 & .702 & .081 & .809 & .889 & .684 & .035 \\
            \hline
            Ours & .871 & .917 & .795 & .036 & \textbf{.846} & \textbf{.895} & \textbf{.777} & \textbf{.059} & \textbf{.843} & \textbf{.907} & \textbf{.742} & \textbf{.029} \\
            \bottomrule
        \end{tabular}
\caption{Comparison with state-of-the-art approaches on camouflaged object detection.}
    \label{tab:sota_cod}
	\end{minipage}\quad
\end{table*}




\begin{table*}[t]
\centering
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{Method}& Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) & & MAE       & BER     &  & AUC   &  &    &   & MAE    \\ \hline
Full-tuning & 64.00 & \textbf{.935} & \textbf{.039} & 2.42  & .465 & .754  & .837 & .887 & \textbf{.778} & .060 \\  
Only Decoder & 3.15 & .891 & .080 & 4.36  & .396 & .722  & .783 & .827 & .671 & .088 \\
VPT-Deep~\cite{vpt} & 3.27 & .913   & .058    & \color{orange}{1.73} & \color{orange}{.588}   & \color{orange}{.847} & .833 & .884 & .751 & .068 \\
AdaptFormer~\cite{chen2022adaptformer} & 3.21 & .912   & .057    & \color{orange}{1.85} & \color{orange}{.602}   & \color{orange}{.855}  & .830 & .877 & .750 & .068 \\
Ours~(r=16) & 3.22 & .924 & .051 & \color{orange}{1.67}  & \color{orange}{.602} & \color{orange}{.857}  & \color{orange}{.838} & \color{orange}{.888} & .761 & .065 \\ 
Ours~(r=4) & 3.70 & .928  & .045  & \color{orange}{\textbf{1.35}} & \color{orange}{\textbf{.636}}   & \color{orange}{\textbf{.862}} & \color{orange}{\textbf{.846}} & \color{orange}{\textbf{.895}} & .777 & \color{orange}{\textbf{.059}} \\ \bottomrule
\end{tabular}}
\caption{
Comparison with state-of-the-art efficient tuning approaches. We conduct evaluations on four datasets for four different tasks.
The efficient tuning method which achieves better performance than full-tuning is marked as {\color{orange}{orange}}.
The best performance among all methods is shown as \textbf{blod}.
}
\label{tab:sota_finetune}
\end{table*}


 

\subsection{Datasets}
We evaluate our model on a variety of datasets for four tasks: forgery detection, shadow detection, defocus blur detection, and camouflaged object detection. A summary of the basic information of these datasets is illustrated in Table~\ref{tab:dataset}.


\paragraph{Forgery Detection.} CASIA~\cite{dong2013casia} is a large dataset for forgery detection, which is composed of 5,123 training and 921 testing spliced and copy-moved images. 
IMD20~\cite{novozamsky2020imd2020} is a real-life forgery image dataset that consists of 2, 010 samples for testing.
We follow the protocol of previous works~\cite{liu2022pscc, hao2021transforensics,wang2022objectformer} to conduct the training and evaluation at the resolution of . We use pixel-level Area Under the Receiver Operating Characteristic Curve~(AUC) and  score to evaluate the performance. 

\paragraph{Shadow Detection.} SBU~\cite{sbu} is the largest annotated shadow dataset which contains 4,089 training and 638 testing samples, respectively.
ISTD~\cite{wang2018stacked} contains triple samples for shadow detection and removal, we only use the shadowed image and shadow mask to train our method.
Following \cite{mtmt,zhu2018bidirectional,zhu2021mitigating}, we train and test both datasets with the size of . 
As for the evaluation metrics, We report the balance error rate~(BER).

\paragraph{Defocus Blur Detection.} 
Following previous work~\cite{zhao2018defocus, cun2020defocus}, we train the defocus blur detection model in the CUHK dataset~\cite{shi2014discriminative}, which contains a total of 704 partial defocus samples. We train the network on the 604 images split from the CUHK dataset and test in DUT~\cite{zhao2018defocus} and the rest of the CUHK dataset. 
The images are resized into , following~\cite{cun2020defocus}. We report performances with commonly used metrics: F-measure~() and mean absolute error~(MAE). 



\paragraph{Camouflaged Object Detection.} 
COD10K~\cite{fan2020camouflaged} is the largest dataset for camouflaged object detection, which contains 3,040 training and 2,026 testing samples. CHAMELEON~\cite{skurowski2018animal} includes 76 images collected from the Internet for testing. CAMO~\cite{le2019anabranch} provides diverse images with naturally camouflaged objects and artificially camouflaged objects. Following~\cite{fan2020camouflaged,mei2021camouflaged}, we train on the combined dataset and test on the three datasets. We employ commonly used metrics: S-measure~(), mean E-measure~(), weighted F-measure~(), and MAE for evaluation. 




\subsection{Implementation Details}
All the experiments are performed on a single NVIDIA Titan V GPU with 12G memory. AdamW~\cite{adam} optimizer is used for all the experiments. The initial learning rate is set to  for defocus blur detection and camouflaged object detection, and  for others. Cosine decay is applied to the learning rate. The models are trained for 20 epochs for the SBU~\cite{sbu} dataset and camouflaged combined dataset~\cite{fan2020camouflaged,skurowski2018animal}, and 50 epochs for others. Random horizontal flipping is applied during training for data augmentation. The mini-batch is equal to 4. Binary cross-entropy (BCE) loss is used for defocus blur detection and forgery detection, balanced BCE loss is used for shadow detection, and BCE loss and IOU loss are used for camouflaged object detection. All the experiments are conducted with SegFormer-B4~\cite{xie2021segformer} pre-trained on the ImageNet-1k~\cite{imagenet} dataset.
 

\begin{figure*}[tp]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/images/sota_result.pdf}
    \caption{Comparisons with other task-specific methods. We show the results  of: 
    SINet~\cite{fan2020camouflaged} and PFNet~\cite{mei2021camouflaged} on CAMO~\cite{le2019anabranch} dataset for camouflaged object detection (Top-left),
    ManTra~\cite{wu2019mantra} and SPAN~\cite{hu2020span} on CAISA~\cite{dong2013casia} dataset for forgery detection (Top-right),
    MTMT~\cite{mtmt} and FDRNet~\cite{zhu2021mitigating} on ISTD~\cite{wang2018stacked} dataset for shadow detection (Bottom-left), CENet~\cite{zhao2019cenet} and EFENet~\cite{zhao2021defocus} on CUHK~\cite{shi2014discriminative} dataset for defocus blur detection (Bottom-right).}
    \label{fig:sota_result}
\end{figure*} \begin{table*}[!t]
\centering
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{Method}& Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) & & MAE       & BER     &  & AUC   &  &    &   & MAE    \\ \hline
Decoder~(No prompting) & 3.15 & .891 & .080 & 4.36  & .396 & .722  & .783 & .827 & .671 & .088 \\ 
Ours w/o  & 3.61 & .924 & .049 & 1.68  & .540 & .833  & .840 & .887 & .759 & .065 \\ 
Ours w/o  & 3.58 & .926  & .046  & 1.61 & .619   & .846 & .844 & .893 & .773 & .063 \\ 
Ours w/ Shared  & 3.49 & .928   & .048    & 1.77 & .619   & .860 & .837 & .889 & .763 & .064 \\
Ours w/ Unshared  & 4.54 & .927   & \textbf{.045}    & \textbf{1.33} & \textbf{.647}   & \textbf{.875}  & .844 & .893 & .774 & .060 \\ 
Ours & 3.70 & \textbf{.928}  & \textbf{.045}  & 1.35 & .636   & .862 & \textbf{.846} & \textbf{.895} & \textbf{.777} & \textbf{.059} \\ \bottomrule
\end{tabular}}
\caption{Ablation on the architecture designs described in Figure~\ref{fig:arch}. We conduct evaluations on four datasets for four different tasks. The proposed prompting strategy (Decoder +  +  + Adaptor) performs more effectively.}
\label{tab:arch}
\end{table*}



\begin{table*}[!t]
\centering
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
Tuning & Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      Stage & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) & & MAE       & BER     &  & AUC   &  &    &   & MAE    \\ \hline
Stage & 3.16 & .895 & .072 & 3.64  & .408 & .725  & .793 & .834 & .681 & .088 \\ 
Stage & 3.18 & .917 & .058 & 2.45  & .457 & .765  & .806 & .853 & .706 & .081 \\ 
Stage & 3.43 & .927   & .047  & 1.46 & .627 & .858 & .841 & .888 & .768 & .062 \\
Stage & 3.70 & \bf.928  & \bf.045  & \bf1.35 & \bf.636   & \bf.862 & \bf.846 & \bf.895 & \bf.777 & \bf.059 \\ 
\bottomrule
\end{tabular}}
\caption{Ablation on the tuning stages in SegFormer. We conduct evaluations on four datasets for four different tasks. The performance of EVP becomes better as the tuning stages increase.}
\label{tab:tuning_stage}
\end{table*}



\begin{table*}[!t]
\centering
{
\begin{tabular}{c||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{}& Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) & & MAE       & BER     &  & AUC   &  &    &   & MAE    \\ \hline
64 & 3.17 & .910 & .055 & 2.09  & .547 & .830  & .829 & .875 & .743 & .070 \\ 
32 & 3.18 & .919 & .054 & 1.84  & .574 & .844  & .832 & .877 & .749 & .067 \\ 
16  & 3.22 & .924  & .051  & 1.67 & .602 & .857 & .838 & .888 & .761 & .065 \\
8  & 3.34 & .923  & .049  & 1.46 & .619 & .856 & .841 & .890 & .767 & .062 \\
4 & 3.70 & .928  & .045   & 1.35 & .636   & \bf.862 & \bf.846 & .895 & .777 & \bf.059 \\
2 & 4.95 & .929  & .042   & \bf1.31 & \bf.642   & .859 & .842 & \bf.896 & .776 & .059 \\ 
1 & 9.56 & \bf.931  & \bf.040   & 1.48 & .621   & .847 & .843 & .894 & \bf.778 & .059 \\ \bottomrule
\end{tabular}}
\caption{Ablation on the parameter scale factor . We conduct evaluations on four datasets for four different tasks. EVP gets the balance between the number of tunable parameters and performances when .}
\label{tab:model_size}
\end{table*}


\begin{table*}[t]
\centering
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{Method} & Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) & & MAE       & BER     &  & AUC   &  &    &   & MAE    \\ \hline
Full-tuning & 98.98 & \bf.862 & \bf.077 & 4.39  & .290 & .650  & .593 & \bf.677 & .382 & .157 \\ 
Only Decoder & 13.00 & .836 & .097 & 4.77  & \color{orange}{.318} & \color{orange}{.662}  & \color{orange}{.615} & .659 & \color{orange}{.385} & .162 \\ 
VPT~\cite{vpt}  & 13.09 & .843  & .092  & 4.56 & \color{orange}{.315} & \color{orange}{.666} & \color{orange}{.615} & .660 & \color{orange}{.387} & .161 \\
AdaptFormer~\cite{chen2022adaptformer} & 13.08 & .845   & .092  & 4.60 & \color{orange}{.319} & \color{orange}{.662} & \color{orange}{.614} & .662 & \color{orange}{.387} & .161 \\
EVP & 13.06 & .850  & 087   & \color{orange}{\bf4.36} & \color{orange}{\bf.324}   & \color{orange}{\bf.675} & \color{orange}{\bf.622} & .674 & \color{orange}{\bf.402} & \color{orange}{\bf.156} \\ \bottomrule
\end{tabular}}
\caption{Comparison with other tuning methods with SETR~\cite{zheng2021rethinking} on four different tasks. We conduct an evaluation on four datasets for four different tasks.
The best performance is shown as \textbf{bold}. 
The prompt-tuning method which achieves better performance than full-tuning is marked as \color{orange}{orange}.}
\label{tab:setr}
\end{table*}
 
\subsection{Main Results}
\label{sec:main_results}


\paragraph{Comparison with the task-specific methods.}
EVP performs well when compared with task-specific methods. We report the comparison of our methods and other task-specific methods in Table~\ref{tab:sota_defocus}, Table~\ref{tab:sota_shadow}, Table~\ref{tab:sota_forgery}, and Table~\ref{tab:sota_cod}. Thanks to our stronger backbone and prompting strategy, EVP achieves the best performance in 5 datasets across 4 different tasks. However, compared with other well-designed domain-specific methods, EVP only introduces a small number of tunable parameters with the frozen backbone and obtains non-trivial performance. We also show some visual comparisons with other methods for each task individually in Figure~\ref{fig:sota_result}. We can see the proposed method predicts more accurate masks compared to other approaches.



\paragraph{Comparison with the efficient tuning methods.}
We evaluate our method with full finetuning and only tuning the decoder, which are the widely-used strategies for down-streaming task adaption. And similar methods from image classification, \ie, VPT~\cite{vpt} and AdaptFormer~\cite{chen2022adaptformer}. 
The number of prompt tokens is set to 10 for VPT and the middle dimension of AdaptMLP is set to 2 for a fair comparison in terms of the tunable parameters.
It can be seen from Table~\ref{tab:sota_finetune} that when only tuning the decoder, the performance drops largely. Compared with similar methods, introducing extra learnable tokens~\cite{vpt} or MLPs in Transformer block~\cite{chen2022adaptformer} also benefits the performance. We introduce a hyper-parameter~() which is used to control the number of parameters of the Adaptor as described in equation~\ref{eqn:fpe}. We first compare EVP~(=16) with similar parameters as other methods. From the table, our method achieves much better performance. We also report EVP~(=4), with more parameters, the performance can be further improved and outperforms full-tuning on 3 of 4 datasets. 


\begin{figure*}[!t]
  \captionsetup[subfigure]{position=b}
  \centering
  \setlength{\tabcolsep}{0pt}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/input/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/input/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/gt/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/gt/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/fulltune/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/fulltune/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/decoder/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/decoder/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_without_embedding_tune/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_without_embedding_tune/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_without_hfc_tune/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_without_hfc_tune/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_with_shared_tunemlp/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_with_shared_tunemlp/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_with_unshared_upmlp/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred_with_unshared_upmlp/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\subcaptionbox{}{\begin{tabular}{c}
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred/91-14.png} \0.1em]
      \includegraphics[width=0.1\textwidth, height=0.1\textwidth]{images/arch/pred/126-3.png} \\
    \end{tabular}}\hspace{0.0em}\caption{Quantitative comparison using full-tuning and different prompting designs on ISTD~\cite{wang2018stacked} dataset for shadow detection. From the left to right is: (a)~Input, (b)~GT, (c)~Full-tuning, (d)~Decoder~(No prompting), (e)~Ours w/o , (f)~Ours w/o , (g)~Ours w/ Shared , (h)~Ours w/ Unshared , (i)~Ours Full.}
\label{fig:ablation_arch}
\end{figure*}

 \subsection{Ablation Study}
\label{sec:ablation_study}
We conduct the ablation to show the effectiveness of each component. The experiments are performed with the scaling factor  except specified.

\paragraph{Architecture Design.}
To verify the effectiveness of the proposed visual prompting architecture, we modify it into different variants. As shown in Table~\ref{tab:arch} and Figure~\ref{fig:ablation_arch}, sharing  in different Adaptors only saves a small number of parameters~(0.55M \textit{v.s.} 0.34M) but leads to a significant performance drop. It cannot obtain consistent performance improvement when using different  in different Adaptors, moreover introducing a large number of parameters~(0.55M \textit{v.s.} 1.39M). On the other hand, the performance will drop when we remove  or , which  means that they are both effective visual prompts.


\paragraph{Tuning Stage.}
We try to answer the question: which stage contributes mostly to prompting tuning? Thus, we show the variants of our tuning method by changing the tunable stages in the SegFormer backbone. SegFormer contains 4 stages for multi-scale feature extraction. We mark the Stage where the tunable prompting is added in Stage . Table~\ref{tab:tuning_stage} shows that better performance can be obtained via the tunable stages increasing. Besides, the maximum improvement occurs in Stage to Stage. Note that the number of transformer blocks of each stage in SegFormer-B4 is 3, 8, 27, and 3, respectively. Thus, the effect of EVP is positively correlated to the number of the prompted transformer blocks.


\paragraph{Scale Factor ~(equation \ref{eqn:fpe}).}
We introduce  in Sec~\ref{sec:explicit_visual_prompting} of the main paper to control the number of learnable parameters. A larger  will use fewer parameters for tuning. As shown in Table~\ref{tab:model_size}, the performance improves on several tasks when  decreases from 64 to 4; when  continues to decrease to 2 or 1, it can not gain better performance consistently even if the model becomes larger. It indicates that  is a reasonable choice to make a trade-off between the performance and model size. 



\paragraph{EVP in Plain ViT.}
We experiment on SETR~\cite{zheng2021rethinking} to confirm the generalizability of EVP. SETR employs plain ViT as the backbone and a progressive upsampling ConvNet as the decoder, while SegFormer has a hierarchical backbone with 4 stages. Therefore, the only distinction between the experiments using SegFormer is that all modifications are limited to the single stage in plain ViT. The experiments are conducted with ViT-Base~\cite{dosovitskiy2020image} pretrained on the ImageNet-21k~\cite{imagenet} dataset. The number of prompt tokens is set to 10 for VPT, the middle dimension of AdaptMLP is set to 4 for AdaptFormer, and  is set to 32 for our EVP. As shown in Table~\ref{tab:setr}, EVP also outperforms other tuning methods when using plain ViT as the backbone.

