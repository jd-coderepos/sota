Since the \basic\ model is too weak for the task of position discovery (when $n$ is even),
we considered the \lazy\ model. Although one can solve position discovery in this model, the
overhead cost for this problem is $\Omega(n\log(N/n)/\log n)$.
In \cite{FriedetzkyGGM12}, it is shown that position discovery
can be solved in the {\perceptive} model (i.e., when the position
of the first collision in a round can be detected while each
agent has to start the round moving to the {\rright} or {\lleft}).
In this section, we inspect efficiency of coordination problems as well as position discovery in this model.
First, we show that the {\perceptive} model gives an opportunity to exchange information between neighbors
on a ring (Section~\ref{sub:sec:comm}). Then, we use this feature to build algorithms for the nontrivial move problem 
which brake the lower bounds working in the {\basic} model and the {\lazy} model (Section~\ref{sub:sec:move:perc}).
Finally, using these solutions as tools, we provide a solution for the positions discovery problem
in time $n/2+o(n)$ provided $\log N=o(\sqrt{n})$ which is optimal up to the $o(n)$ term (Section~\ref{sub:sec:disc:perc}).

\subsection{Communication on a ring}\labell{sub:sec:comm}
First, we discuss the following {\em neighbors discovery} task in which
each agent $a$ should:
\begin{itemize}
\item
\vspace{-5pt}
learn (relative) location of its left neighbor $\Lleft(a)$ and its right neighbor $\Rright(a)$;
\item
\vspace{-5pt}
determine whether $\Lleft(a)$ and $\Rright(a)$ have the same sense of direction as $a$ has.
\end{itemize}
\vspace{-5pt}
Algorithm~\ref{alg:neighbor} solves this problem based on the fact that each two
IDs differ on at least one bit. (Some calculations performed by agents are not
explicitly described in the algorithm, they are discussed later.)
In Algorithm~\ref{alg:neighbor}, each execution of {\SingleRound} is followed by {\ReversedRound} in which
each agent starts a round with the direction opposite to its local direction {\dir}. We omit this detail in the pseudocode. However, let us stress here that this gives a guarantee that each agent starts each application of {\SingleRound} at exactly the same position as its position before the execution of the
algorithm (so, its distances to neighbours are the same as well). 

\begin{algorithm}[]
	\caption{NeighborDiscovery($a$)}
	\label{alg:neighbor}
	\begin{algorithmic}[1]
    \State $D_{\lleft}\gets\emptyset$; $D_{\rright}\gets\emptyset$ \Comment{distances to collisions}
    \For{$i=1,2,\ldots,\log N$}
        \For{$j\in[0,1]$}
            \For{$k\in\{\lleft,\rright\}$}
                \If{$\ID_a[i]=j$} $\dir\gets k$
                \Else   \ $\dir\gets$ direction opposite to $k$ \EndIf
                \State \SingleRound
                \State After a round: $D_k\gets D_k\cup\{\coll()\}$ \EndFor
        \EndFor
    \EndFor
    \State $\dir_a\gets \rright$ \Comment{All agents choose direction {\rright}}
    \State \SingleRound; $D_\rright\gets D_\rright\cup\{\coll()\}$ \State $\dir_a\gets \lleft$
    \State \SingleRound; $D_\lleft\gets D_\lleft\cup\{\coll()\}$ \State Location of $\Rright(a)\gets 2 \min (D_{\rright})$
    \State Location of $\Lleft(a)\gets 2 \min (D_{\lleft})$
    \end{algorithmic}
\end{algorithm}

\begin{proposition}\label{p:alg1}
Algorithm~\ref{alg:neighbor} gives solution to neighbors discovery in $O(\log N)$ rounds.
\end{proposition}
\iffull
\begin{proof}
Consider $a$ and $a'=\Rright(a)$. First, we show that, in some round,
they start moving towards each other (which gives the distance to collision
equal to halve of their distance, the smallest possible). If they have {\em opposite} sense of direction,
then this happens in line 10 of the algorithm. Otherwise (i.e., they have the
same sense of direction), they start moving towards every other for such $i,j,k$
that $\ID_a[i]\neq \ID_{a'}[i]$, $j=\ID_a[i]$ and $k=\rright$.

When $a$ already knows its distance to $\Rright(a)$, the distance to collision
in line 10 gives information whether $a$ and $\Rright(a)$ have the same sense of direction.
(Namely, their senses of directions are opposite iff the result of $\coll()$ is equal to halve of
the distance between them.)
\end{proof}
\else
\fi

\comment{
\begin{proposition}\label{p:alg1}
One can modify Algorithm~\ref{alg1} such that each agent learns ID of its left neighbor
and ID of its right neighbor in $O(\log N)$ rounds.
\end{proposition}
\begin{proof}
Recall that, after Algorithm~\ref{alg1}, each agent knows sense of direction of its neighbors
and distances $d_l,d_r$ to them.
Let $a_1$ be an agent, and $a_2=\Rright(a_1)$.
Observe that, in Algorithm~\ref{alg1}, $a_1$ and $a_2$ start moving towards
each other in line 7 for particular $i$ iff:
\begin{itemize}
\item
$a_1$ and $a_2$ have the same sense of direction, $\ID_{a_1}[i]\neq \ID_{a_2}[i]$, $j=\ID_{a_1}[i]$, $k=\rright$; OR
\item
$a_1$ and $a_2$ have opposite sense of direction, $\ID_{a_1}[i]= \ID_{a_2}[i]$, $j=\ID_{a_1}[i]$, $k=\rright$.
\end{itemize}
\end{proof}
}


\begin{proposition}\label{prop:communication}
If each agent knows:
\begin{itemize}
\item
\vspace{-5pt}
locations of its neighbors (relative to its initial location); AND
\item
\vspace{-5pt}
sense of direction of its neighbors (with respect to its own sense of direction);
\end{itemize}
then each agent can transmit one bit of information
to its neighbors in time $O(1)$.
\end{proposition}
\iffull
\begin{proof}
Assume that each agent $a$ is going to transmit bit $z_a$. Consider a {\em phase}
which consists of four rounds:
\begin{itemize}
\item Round $1,2$: if $z_a=1$ then $\dir_a\gets \rright$ else $\dir_a\gets \lleft$;

\SingleRound, \ReversedRound

\item Round $3,4$: if $z_a=0$ then $\dir_a\gets \rright$ else $\dir_a\gets \lleft$;

\SingleRound, \ReversedRound
\end{itemize}
Consider an agent $a$ and its right neighbor $b=\rright(a)$. We show that $a$ is able
to determine $z_b$ (the bit transmitted by $b$) based on results of Rounds $1$ and $3$ (i.e., on moments of first
collisions). We say that a round is {\em clear} if $a$ and $b$ collide in the middle point
between their original locations, which means that they start a round moving towards
each other.
We have two cases:

\noindent Case 1: the sense of direction of $a$ and $b$ agree:
 \begin{enumerate}
 \item[(a)]
 $z_a=1$: if Round 1 is clear, then $z_b=0$, otherwise $z_b=1$.
 \item[(b)]
 $z_a=0$: if Round 3 is clear, then $z_b=1$, otherwise $z_b=0$.
 \end{enumerate}

\noindent Case 2: the sense of direction of $a$ and $b$ do {\em not} agree:
 \begin{enumerate}
 \item[(a)]
 $z_a=1$: if Round 1 is clear, then $z_b=1$, otherwise $z_b=0$.
 \item[(b)]
 $z_a=0$: if Round 3 is clear, then $z_b=0$, otherwise $z_b=1$.
 \end{enumerate}

Communication with $\lleft(a)$ can be performed in an analogous way.

\end{proof}
\else
\tj{\vspace{-5pt}
The statement of Prop.~\ref{prop:communication} can be obtained such that each
agent starts  round 1 (2, resp.) moving left/right depending on the transferred  bit. Then, the distances to the first collision in both rounds give information about the bits of  neighbors.}
\fi
Since agents can learn location of their neighbors and their sense(s) of direction
in $O(log N)$ rounds (see Proposition~\ref{p:alg1}), Proposition~\ref{prop:communication}
leads to the following corollary.

\begin{corollary}\labell{c:sim}
There exists a possibility to exchange one bit of information
between each two neighbors in the {\perceptive} model  in time $O(1)$, after a $O(\log N)$ preprocessing.
\end{corollary}

The above corollary gives opportunity to simulate any distributed algorithm on a ring
in message passing model (i.e., when each pair of neighbors can exchange a message in
one round of computation).
However, the time efficiency of such simulations is limited by the fact that only one bit
of information is exchanged between neighbors in a round.


Let {\em information dissemination task} with parameters $d$ and $p$ be to disseminate
a message $m_a$ with $p$ bits by each agent $a$ to all agents in ring distance $\leq d$
from $a$.
\begin{corollary}\labell{cor:disseminate}
Information dissemination task in which agents are supposed
to transmit messages of length $p$ on the ring distance $d$ can be accomplished in time $O(p\cdot d)$.
\end{corollary}
A solution claimed in the above corollary might we designed such that
first all agents transmit own messages, then messages arriving from their left neighbors and finally messages arriving from their right
neighbors.

Assume that $A'\subset A$ is a set of {\em marked} agents such that each agent
knows whether it is marked or not and the ring distance between any different $a,a'\in A'$
is at least $d$. Moreover, each $a\in A'$ has a message $M_a$ of size $\leq m$.
The {\em sparsed information dissemination task} with parameters $A',d$ and $m$
is to deliver the message of each $a\in A'$ to all agents in the ring distance
$\leq d$ from $a$. 
For an agent in $A'$, we denote this task by Diss($M_a,d$).
Using the procedure exchanging a bit of information between
each pair of neighbors in time $O(1)$, we obtain the following result.
\begin{corollary}\labell{cor:sdisseminate}
Sparsed information dissemination task in which agents in distances $\geq d$ are supposed
to transmit messages of length $p$ on the ring distance $d$ can be accomplished in time $O(p+d)$.
\end{corollary}
In a solution to the sparsed information dissemination we have to tackle
the fact that an agent has no direct way to convey a message of the type ``I have nothing to transmit (yet)''. One can solve this issue by a simple encoding, e.g., $00/11$ encodes $0/1$, while $01$ encodes ``no bit to transmit''.

\subsection{Nontrivial Move}\labell{sub:sec:move:perc}
As we know, the nontrivial move problem is intuitively to break balance between the number
of agents moving clockwise and anticlockwise.
In our solution we use $(N,k)$-selective families from \cite{ClementiMS03}.
\begin{definition}
Let $n<N$. A family $\m{F}$ of subsets of $[N]$ is 
$(N, n)$-selective if, for every non empty subset $Z$ of $[N]$ such that $|Z|\leq n$, there is a
set $F$ in $\m{F}$ such that $|Z \cap F|=1$.
\end{definition}
Clementi et al.\ \cite{ClementiMS03} showed that 
for any $N>2$ and $n\leq N$, there exists an $(N,n)$-selective family of size
$O(n \log(N/n))$.

Let a {\em local leader} for some fixed number $d$ be an agent $a$ with the largest ID among agents
in the ring distance $d$ from $a$.
In Algorithm~\ref{alg:move:perceptive} we present a solution to the nontrivial move
problem by establishing local leaders for exponentially growing distances $d=2^k$ and trying
to execute $(N,2^k)$-selective family on those leaders. As the number of local leaders is
$\leq n/2^k$, it becomes smaller than $2^k$ for $k>\frac12\log n$ and gives a nontrivial
move after $O(2^{\frac12\log n}\log N)=O(\sqrt{n}\log N)$ rounds.

\begin{algorithm}[]
	\caption{NMoveS($a$)}
	\label{alg:move:perceptive}
	\begin{algorithmic}[1]
    \State $\dir_a\gets \rright$; set the status of $a$ as a local leader;
		\State \SingleRound
    \State If the current directions give a nontrivial move: return
\State Establish $1$-bit communication\Comment{Cor.~\ref{c:sim}}
    \For{$k=0,1,2,\ldots,\log N$}
        \State Sparsed dissemination of ID$_a$ of local leaders on distance $2^k$\Comment{Corollary~\ref{cor:sdisseminate}}
        \If{$\ID_a=\max(N_a(2^k))$}
            \State set the status of $a$ as the {\em local leader}
\Else
            \State set the status of $a$ as {\em not leader}
\EndIf
\State Execute a $(N,2^k)$-selective family $\m{F}$ on the local leaders
        \State \textbf{if} a nontrivial move appears during execution of $\m{F}$ \textbf{then} return
    \EndFor
\end{algorithmic}
\end{algorithm}



\begin{lemma}\labell{lem:ntmp}
The algorithm NMoveS solves the nontrivial move problem in $O(\sqrt{n}\log N/\log n)$ rounds
in the {\perceptive} model.
\end{lemma}
\iffull
\begin{proof}
As for correctness, the number of local leaders in the $i$th iteration of the
for-loop is $O(n/2^i)$. Thus, for $k\geq \log\sqrt{n}$, the number of local leaders
is not larger than $2^k$ and therefore the $(N,2^k)$-selective family breaks symmetry
between them which gives a nontrivial move. 



Below, we present the above intuition in more detail.
Let us fix some ``objective correct'' sense of direction.
Let $A_C,A_I$ be the subsets of agents with correct and 
incorrect sense of direction, respectively.
If the algorithm does not finish its execution in line 3,
the rotation index is in $\{0,n/2\}$ for a round with all $\dir_a$ equal to
{\lleft}. If exactly one agent changes its initial direction in
a round, the rotation index will increase by $2$ or $-2$.
That is, it will not be in $\{0,n/2\}$ since $n>4$,
and a nontrivial move will be obtained.
Let $L$ be the set of local leaders in the $k$th iteration of the for-loop.
If $k\geq \log \sqrt{n}$, the selective family in line 11 will select in some
round exactly one element from the set of local leaders. This follows
from the fact that the set of local leaders is not larger than $2^k$
in such case thus the $(N,2^k)$-selective family is sufficient to select
exactly one element from $L$.

Regarding time complexity, the $k$th iteration of the for-loop requires
$O(2^k\log N)$ rounds, which gives
$$O\left(\sum_{k=1}^{(\log n)/2} 2^k\log N\right)=O(\sqrt{n}\log N)$$
rounds. \comment{
Regarding time complexity, the $k$th iteration of the for-loop requires
$O(2^k\log(N/2^k)/k)$ rounds, which gives
$$O\left(\sum_{k=1}^{(\log n)/2} 2^k\log(N/2^k)/k\right)=O(\sqrt{n}\log N)$$
rounds. }
\end{proof}
\else
\fi

\comment{
\begin{corollary}
The leader election problem can be solved in time $O(\sqrt{n}\log(N))$.
\end{corollary}
\begin{proof}
\tj{BEDZIE WYNIKAC Z REDUKCJI I TU NIEPOTRZEBNE}
First, we solve the nontrivial move problem (Lemma~\ref{lem:ntmp}). Using a nontrivial round, the common
sense of direction can be established in $O(1)$ rounds. Finally, the leader can
be elected in $O(\log N)$ rounds by Lemma~\ref{lem:emptiness:leader}.
\end{proof}
}

\subsection{Position Discovery in the {\perceptive} model}\labell{sub:sec:disc:perc}

In this section we design an efficient solution for the position
discovery in the {\perceptive} model. Using results from the previous
section and Theorem~\ref{ABC},
we can assume that the leader is elected and the common sense of direction is established
in $O(\sqrt{n}\log N)$ rounds.
Throughout this section, we use a labeling of agents such that $a_1$ is the label of the leader and $a_i$ is the
label of the $i$th agent on the ring in the clockwise direction from the leader.


We solve the position discovery problem in two stages. First, each agent determines
its right ring distance to the leader (i.e., its {\em label}; note that a label denotes
the distance to the leader, not ID).  
In order to achieve this goal in the standard message passing model on a ring, linear
time is necessary. In order to perform this task faster, we use arithmetic relationships
between distances to collisions (\coll()) and  distances traversed in consecutive rounds (\pos()).
For appropriately designed protocol, an agent in ring distance $\leq d^2$ from the leader will
be able to learn its ring distance in $O(d\log N)$ rounds.
Then, using the knowledge about ring distances of agents to the leader, the position discovery
will be finally solved in the following way. Let $x_1,\ldots,x_n$ be the original distances between agents.
Here, we plan movements of agents in such a way that, for each agent and each round,
the distance to collision in the round and the distance traversed in the round gives a linear equation
over $x_1,\dots,x_n$ which is linearly independent from equations
derived before. In this way each round provides two new equations and $n/2$ rounds are sufficient
to determine the actual values of $x_1,\dots,x_n$,
since they give a system of $n$ independent linear equations over
$n$ variables.




\subsubsection{Ring distances}
Now, we design the RingDist protocol in which each agent learns
its right ring distance to the leader. Throughout this section, ring distance denotes the right ring distance from
the leader. We call it a {\em label} of an agent and denote the agent in ring
distance $i$ by $a_i$.


Let Shift($l$) for $l\in\NAT$ be a round in which
$\dir_{a_{i}}=\rright$ for each $i\in[l]$ and
$\dir_{a_{i}}=\lleft$ for $i>l$.
Moreover, Shift($-l$) is a round with directions of agents
opposite to their direction in Shift($l$).
Observe that the rotation index of Shift($l$) is equal to 
$$(l-(n-l))\ \modd n\equiv 2l\modd n.$$

RingDist works under assumption that (exactly) one distinguished
agent has the status leader (it is denoted $a_1$).
Each agent but the leader starts an execution of a protocol with unspecified
ring distance. The idea of Algorithm~\ref{alg:move:perceptive} is that the
agents gradually learn their ring distances in the following way:
\begin{itemize}
\item
The agents in ring distance $\leq 4$ learn their distances in step 1
(the same applies to the agents $\geq n-4$, although they learn merely their
relative values, without knowing $n$).
\item
In the $i$th iteration of the for-loop, the agents $a_k,a_{k+k},\ldots,a_{k+k^2}$ for $k=2^i$
learn their ring distances in the following way (see Fig.~\ref{fig:ring:dist}).
\begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.45]{ring-dist.pdf}
  \caption{An illustration for Algorithm~\ref{alg:move:perceptive}. The agent's
	label is not $a_k+jk$ for any $j\leq k$ iff $2z$ is not equal to any of the sums
	$\sum_{i=1}^j y_i$.}
\label{fig:ring:dist}
\end{center}
\end{figure}
For each $l>k$, the value of $\coll()$ in Shift($k/2$) is equal to $z=(x_{l-k}+\cdots+x_{l-1})/2$
(see Prop.~\ref{prop:bounce} for $b_0=a_l$, $\dir=\lleft$ and thus
$b_i=a_{(l-i)\modd n}$).
On the other hand, if one applies Shift($-k/2$) several times,
the values of $\pos()$ in the $j$th executions of Shift($-k/2$) is equal to
$y_j=x_{l-jk}+\cdots+x_{l-(j-1)k+1}$, since the rotation index of Shift($-k/2$) is equal to $-k$.
Using these relationships, we see that there exists $j$ such that $2z=y_1+\cdots+y_j$
iff $l=k+k\cdot j$.
This observation is exploited in RingDist in order to determine ring distances of $a_1,\ldots a_{k+k^2}$
in the $i$th iteration of the main for-loop for $k=2^i$.

\item
The remaining agents $a_j$ for $j\leq k+k^2$ learn their distances
in the execution of line 8, as each agent knowing
its ring distance propagates it in the distance $2k$.
\end{itemize}
Then, it remains to guarantee that the for-loop is finished when all agents
know their ring distances and $2^i=O(\sqrt{n})$.
To this aim, we execute CheckCompleteness.
Note that the agent $a_n$ knows
that it is the last one already at the beginning (without knowing $n$), as it is
the left neighbour of the leader.
CheckCompleteness is a round in which all agents different from $a_n$ move
{\lleft}, while $a_n$ moves {\rright} iff it already knows its own
right ring distance (which in turn implies that every other agent knows its ring distance
as well).
Thus, the rotation index of this round is not zero iff each agent knows its ring distance.
\begin{algorithm}[]
	\caption{RingDist($a$)}
	\label{alg:move:perceptive}
	\begin{algorithmic}[1]
\State \textbf{if} $a=a_1$: Diss(''leader'',4)\iffull\Comment{The leader $a_1$ broadcasts its message on ring distance $4$}\else\tj{\Comment{$a_1$ broadcasts on dist.\ $4$}}\fi
    \For{$i=1,2,\ldots,\log N$}
        \State $k\gets 2^i$
        \State For $j=1,\ldots,k$: Shift($-k/2$); $y_j\gets\pos()$
        \State Repeat $k$ times: Shift($k/2$) \iffull\Comment{Reverse the result of $k\times$ Shift($-k/2$)}\else\tj{\Comment{Reverse res. of l.\ 4}}\fi
        \State Shift($k$); $z\gets \coll()$; Shift($-k$)
        \State \textbf{if} $2z=y_1+\cdots+y_j$ for some $j$
				and $a\not\in\{a_1,\ldots,a_k\}$: 
				\State\phantom{abc}Set the ring distance of $a$ to $k+jk$; mark $a$\iffull\Comment{i.e., $a\gets a_{k+jk}$}\else\tj{\State\phantom{abc} (i.e., $a$ is $a_{k+jk}$ and $a$ is marked)}\fi
        \If{$a=a_{k+jk}$ for $j\leq k$ and $a$ marked}
					\State Diss($k+jk,k$)
					\iffull\Comment{Marked agents broadcast their ring dist.\ on distance $k$}\else\tj{ (i.e., marked agents \State broadcast their ring dist.\ on distance $k$)}\fi
				\EndIf
\State If CheckCompleteness: return\iffull\Comment{See description of the alg. for details}\fi
    \EndFor
\end{algorithmic}
\end{algorithm}
In the following, we show more formally that the above described idea works. First, we make
an observation following from the definition of Shift (the rotation index of Shift($l$) is $2l$)
and Proposition~\ref{prop:bounce}.
\begin{proposition}\label{prop:shift}
Let $k=2^i$ for $i\leq \log N$.
Assume that agents $a_1,\ldots,a_k$ know their labels before the $i$th
iteration of the for-loop (and other agents know that they do not belong
to $\{a_1,\ldots,a_k\}$).
Then, for $l>k$ the values of $z$, $y_1,\ldots,y_k$ recorded by the agent $a_l$ satisfy the following
conditions in the iteration $i$ of the for-loop:
\begin{itemize}
\item $y_j=x_{l-kj}+x_{l-kj+1}+\cdots+x_{l-k(j-1)-1}$;
\item $z=(x_k+\cdots+x_{l-1})/2$.
\end{itemize}
\end{proposition}


\iffull
The following corollary is an immediate consequence of Proposition~\ref{prop:shift}.
\fi
\begin{corollary}\labell{cor:shift}
The condition $2z=y_1+\cdots+y_j$ is satisfied for an agent $a\not\in\{a_1,\ldots,a_k\}$ iff $a$ is in the right ring distance
$k+jk$ from the leader (i.e., $a=a_{k+jk}$).
\end{corollary}

\begin{lemma}\labell{lem:ring}
Assume that the leader is elected and all agents share common sense of direction.
Then, each agent $a$ determines its ring distance during the algorithm RingDist and the
algorithm lasts $O(\sqrt{n}\log N)$ rounds.
\end{lemma}
\iffull
\begin{proof}
Before the for-loop, the agents in ring distance $\leq 4$ are aware of their ring distance,
while the agent $a_n$ knows that it is ``the last one''. 
We show by induction that, after the $i$th iteration of the main for-loop, the agents
$a_1,\ldots,a_{k^2}$ know their labels for $k=2^i$.
As the base step is obvious, assume inductively that before the $i$th iteration,
the agents $a_1,\ldots, a_{(2^{i-1})^2}$ know their labels. Thus, in particular
$a_1,\ldots,a_{k}$ know their labels for $k=2^i$ since $2^i<(2^{i-1})^2$.
This assures that agents are able to perform all steps in the $i$ iteration
of the main for-loop.
Then, Proposition~\ref{prop:shift}
and Corollary~\ref{cor:shift} imply that each agent
$a_{k+jk}$ for $j\in[k]$ and $k=2^i$ becomes aware of its ring distance
before dissemination of distances in line 10.
In line 10 agents $a_k,a_{2k},\ldots,a_{(k-1)k},a_{k^2}$
\tj{broadcast} information about their ring distances to agents in their ring distance
$\leq k$.
Thus, the remaining agents $a_l$ for $l\leq k+k^2+k$ learn their ring distances
from the agents $a_{k+k},\ldots,a_{k+2k},\ldots,a_{k+k^2}$.
Finally, for the smallest $i$ such that $2^i+(2\cdot 2^i)^2+2^i\geq n$, the for-loop
is finished and all agents know their ring distances.
In order to execute the $i$th iteration, $O(2^i\log N)$ rounds are sufficient
(by Corollary~\ref{cor:sdisseminate}, dissemination of distances by marked agents
can be done in $O(2^i+\log N)=O(2^i\log N)$ rounds).
Time complexity of the whole procedure is
$$O(\sum_{i=1}^{\frac12\log n} 2^i\log N)=O(\sqrt{n}\log N).$$
\end{proof}
\else
\fi

\subsubsection{Position discovery}
In this section we describe a solution for the position discovery
problem based on protocols presented before.
Recall that, given  the common sense of direction and the leader, one can obtain a round
with rotation index $2$ by assigning the direction $\lleft$ to all agents but the leader.
If $n$ is odd, this gives a solution to the position discovery problem in $n$ rounds.
The goal of this section is to get advantage of information provided by positions of 
the first collision in a round, in order to decrease time from $n$ to $n/2$ and manage
the case that $n$ is even.

Here, we assume that the leader $a_1$ is elected, the agent(s) in the right ring distance
$i$ from the leader is $a_{i+1}$ and each agent $a_i$ knows $i$ (see Corollary~\ref{cor:shift}). Moreover, we assume that
$n$ is even (one can easily build a similar solution for odd $n$).
Let $x_i$ denote the distance on the ring between the agent $a_i$ and the agent $a_{i+1}$
(or $a_1$ if $i=n$). (Note that this is the geometric distance on the ring, not the ring distance!)


Let Convolution($j$) be a round in which the agents' directions are as 
\iffull
follows (see Figure~\ref{fig:red:strong}):
\else
follows:
\fi
\begin{itemize}
\item[]
$\dir_{a_{2i-1}}=\rright$, $\dir_{a_{2i}}=\lleft$ for each $i\in[n/2]$, with an
exception: $\dir_{a_{2j}}=\rright$.
\end{itemize}
\iffull
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{conv.pdf}
  \caption{An illustration of Convolution$(j)$ and Pivot$(n)$. In Convolution($j$), the blue agents are the only one which do not collide with a neighbor in the middle of the distance between them. In Pivot$(j)$, blue agents are the only one which collide with a neighbor in the middle of distance between them.}
\label{fig:red:strong}
\end{center}
\end{figure}
\else
\fi

Let Pivot($j$) be a round in which the agents' directions are as follows:
$\dir_{a_{j+1}}=a_{j+2}=\cdots=a_{j+n/2}=\lleft$ and
$\dir_{a_{j}}=\dir_{a_{j-1}}=\cdots=\dir_{a_{j-n/2+1}}=\lleft$.
(Here, the subscript indices are calculated modulo $n$ and $a_0$ is identified
with $a_n$.)
Observe that the rotation index of Convolution($i$) is equal to $2$ and the rotation
index of Pivot($i$) is equal to $0$ for each $i$.
\begin{algorithm}[]
	\caption{Distances($a$)}
	\label{alg:disc:perc}
	\begin{algorithmic}[1]
    \For{$i=1,2,\ldots,n/2$}
        \State Convolution$(\frac{n-2(i-1)}{2})$
    \EndFor
    \State Pivot($n$); Pivot($n-1$); Pivot($n-2$);
    \end{algorithmic}
\end{algorithm}
In the following, we show that information collected during an
execution of Algorithm~\ref{alg:disc:perc} can determine original
positions of all other agents. 
\begin{proposition}\label{prop:linear}
After the for-loop of Algorithm~\ref{alg:disc:perc}, the following
conditions hold:
\begin{enumerate}
\item[(a)]
the agent $a_{2i-1}$ can
determine the values of
$x_{1}, x_2, \ldots,x_{n-2}$ and $x_{n-1}+x_n$.
\item[(b)]
the agent $a_{2i}$ can determine the values of
$x_1,x_2,\ldots,x_{n-3}$, $x_n$, and $x_{n-2}+x_{n-1}$
\end{enumerate}
for each $i\in[n/2]$.
\end{proposition}
\iffull
\begin{proof}
First, observe that
the agent $a_k$ stays at the original position
of $a_{k+2(i-1)}$ before the $i$th execution of line 2, since the rotation index
of Convolution is equal to $2$. Therefore, the only agent with ``swapped'' direction
stays in each round of the for loop at the original position of $a_{j}$
for $j=\frac{n-2(i-1)}{2}+2(i-1)=\frac{n}2$.
As a result, the agent $a_k$ learns in the $(i+1)$st execution of line 2 exactly the same values as $a_{k+2i}$
in the first execution of line 2.
Consider this first round. As $a_{2j-1}$ and $a_{2j}$ start
the round moving towards each other for each $j\in[n/2-1]$ (i.e., $j< n/2$),
they collide after traversing the distance $x_{2j-1}/2$ and therefore learn $\coll()=x_{2j-1}/2$.
On the other hand, the rotation index of Convolution with any
parameter is $2$ and thus each agent $a_k$ learns $\pos()=x_k+x_{k+1}$.
All these observations lead to the following conclusion:
\begin{itemize}
\item
Each agent with odd label learns the values of
$x_{2j-1}+x_{2j}$ for each $j\in[n/2]$ (registered as $\pos()$ in consecutive rounds)
and $x_{2k-1}$ (corresponding to $\coll()$) for each $k\in[1,n/2-1]$.
\item
Each even agent learns the values of
$x_{2(j-1)}+x_{2j-1}$ for each $j\in[n/2]$
and $x_{2k-1}$ for each $k\in[1,n/2-1]$.
\end{itemize}
Given, the above, each agent can deduce the values enumerated
in the proposition.
E.g., each agent with add label solves the 
system of equations $x_{2j-1}+x_{2j}=d_j$ for $j\leq n/2$
and $x_{2j-1}=c_j$ for $j<n/2$, where $d_j$ and $c_j$ are
the values observed as $\pos()$ and $\coll()$ in various rounds.
\end{proof}
\else
\tj{The above proposition uses the fact that each execution of 
Convolution gives information about the sum of two consecutive
$x_i$'s (as the distance between an agent's position at the beginning and the end of a round) and about a particular $x_j$ (the distance to the first collision is equal to halve of $x_j$ for some $j$).}
\fi
Now, observe that
\begin{itemize}
\item
The agent $a_{2i-1}$ has the first collision during Pivot($n$) in distance
$x_n/2+(x_1+\cdots+x_{2i-2})/2$ for each $i\in[n/4]$.
As $a_{2i-1}$ knows $x_1,\ldots,x_{2i-2}$ and $x_{n-1}+x_n$ by Proposition~\ref{prop:linear}(a), it can determine $x_n/2$ from Pivot$(n)$ and 
therefore also $x_{n-1}$.
\item
The agent $a_{2i-1}$ has the first collision during Pivot($n-1$) in distance
$x_{n-1}/2+(x_{2i-1}+\cdots+x_{n-2})/2$ for each $i\in[n/4+1,n/2-1]$.
As it knows $x_{2i-1},\ldots,x_{n-2}$ and $x_{n-1}+x_n$, it can
determine $x_{n-1}$ from Pivot$(n-1)$ and then $x_n$ as well.


A similar reasoning works for $a_{n-1}$ as well.
\end{itemize}
By combining the above with Prop.~\ref{prop:linear}(a), one can conclude that each 
agent $a_i$ with odd label $i$ knows original positions of all agents. A similar argument
applies for even agents and executions of Pivot($n-1$) and Pivot($n-2$),
since Prop.~\ref{prop:linear}(b) can be seen as Prop.~\ref{prop:linear}(a) ``shifted'' by $-1$.

\iffull
Thus, we obtain the following conclusion.
\else
\fi
\begin{lemma}\labell{lem:distances}
The protocol Distances (Alg.~\ref{alg:disc:perc}) solves the position discovery problem, provided
the leader ($a_1$) is elected, agents share common sense of direction
and each agent knows its ring distance.
\end{lemma}

\iffull
By combining the subroutines described before, we get the following result.
\fi
\begin{theorem}
The position discovery problem can be solved in the perceptive model
in $n/2+O(\sqrt{n}\log N)$ rounds.
\end{theorem}

