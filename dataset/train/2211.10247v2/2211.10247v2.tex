

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{bm}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{subcaption}
\usepackage{setspace}
\usepackage{stfloats}
\usepackage{array}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\setlength\titlebox{8cm}

\title{GoSum: Extractive Summarization of Long Documents by Reinforcement \\ Learning and Graph Organized discourse state}

\author{Junyi Bian \textsuperscript{\rm 1 \rm 8} , Xiaodi Huang \textsuperscript{\rm 2} , Hong Zhou \textsuperscript{\rm 3}, Shanfeng Zhu \textsuperscript{\rm 4 \rm 5 \rm 6 \rm 7 \rm 8}   \\
  \textsuperscript{\rm 1} School of Computer Science, Fudan University, Shanghai 200433, China\\
  \textsuperscript{\rm 2} School of Computing and Mathematics, Charles Sturt University \\ Albury, NSW 2640, Australia\\
  \textsuperscript{\rm 3} Atypon Systems, LLC, UK\\
  \textsuperscript{\rm 4} Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, China\\
  \textsuperscript{\rm 5} Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence \\ (Fudan University), Ministry of Education, Shanghai 200433, China\\
  \textsuperscript{\rm 6} MOE Frontiers Center for Brain Science, Fudan University, Shanghai 200433, China\\
  \textsuperscript{\rm 7} Zhangjiang Fudan International Innovation Center, Shanghai 200433, China\\
  \textsuperscript{\rm 8} Shanghai Key Lab of Intelligent Information Processing, \\ Fudan University, Shanghai 200433, China\\
  \texttt{\{zhusf, 20110240003\}@fudan.edu.cn, hzhou@atypon.com} \\
}


\date{}

\begin{document}
\maketitle

\begin{abstract}
Extracting summaries from long documents can be regarded as sentence classification  using the structural information of the documents.
How to use such structural information to summarize a document is challenging.
In this paper, we propose GoSum, a novel graph and reinforcement learning based extractive model for long-paper summarization.
In particular, GoSum encodes sentence states in reinforcement learning by building a heterogeneous graph for each input document at different discourse levels. An edge in the graph reflects the discourse hierarchy of a document for restraining the semantic drifts across section boundaries.
We evaluate GoSum on two datasets of scientific articles summarization: PubMed and arXiv. 
The experimental results have demonstrated that GoSum achieve state-of-the-art results compared with strong baselines of both extractive and abstractive models.
The ablation studies further validate that the performance of our GoSum benefits from the use of discourse information.


























\end{abstract}
 \section{Introduction}
\label{sec:intro}



Document summarization refers to generating short and conductive summaries over given texts, which can help readers rapidly acquire essential knowledge from documents.
There are two main categories of approaches to summarization: the extractive approach and the abstractive approach.
The extractive approaches score and filter out the sentences of a given document to ensure the semantic and grammatical correctness of the selected sentences in the summary.
Abstractive approaches mostly read an input text, comprehend it, and output its summary within the seq2seq framework.
This procedure is similar to humans'  summarising articles.
The theoretical upper bound on the performance of the seq2seq model is higher than what extractive approaches can achieve.
However, abstractive approaches  have the drawback of producing some meaningless and unfaithful summaries~\cite{fact_2020}. The generated summaries read smoothly with a high ROUGE score, but there is a significant gap in semantic information between them and the gold summaries.

In this paper, we focus on the use of extractive models for summarizing scientific literature.
Extractive summarization~\cite{matchsum_2020,zhou2018neural} has been extensively studied in short summarization datasets such as CNN/DailyMail \cite{cnndm}.
However, studies on long texts have lagged relatively behind because long document summarization is more challenging due to the following two reasons:
1) An increase in the input length expands the memory cost of the model; and
2) The complex discourse structural information about long-form documents should be taken into account.
Reading a long text, especially scientific literature, one usually glances at the discourse structure of the whole text. Once reading a section title, one roughly should know on which this section focuses. Using this structural information  of a text, one can better understand the meanings of its sentences. From the perspective of extractive summarization, it would be better to use this information for encoding sentences.
The previous method encodes sentences and sections separately, making it difficult to capture the hierarchical structure of the document.
In this paper, we thereby propose to use a graph neural network (GNN) to well represent the structure information of documents. The  additional benefit is that the computational complexity of GNNs is linear for long inputs.


Unlike abstractive approaches that are trained by  using available gold summaries directly, the training labels of an extractive model need to be obtained by using a search algorithm (typically greedy search) based on the gold summary provided.
This kind of label is not optimal and deterministic, i.e., the algorithm yields a single extracted label for each pair of document-abstract.
In fact, there may be many valid labels that are very similar to these suboptimal labels.
Insufficient such positive pairs may cause under-fitting~\cite{rl_2018}. 
These problems can be alleviated by increasing the number of samples and giving each training sample a reward from reinforcement learning (RL).




To address the above problems, we propose a novel model called GoSum that is trained by using reinforcement learning.
Based on a given input and previously extracted sentences, GoSum generates the sentences of a summary sequentially.  
The process of scoring and selecting a sentence is regarded as an action in reinforcement learning.
This action is taken after the agent (the GoSum model) takes the  sentence state as input.
For encoding sentence states, we  leverage the  structure of a document.
Specifically,  we  use  a graph neural network to encode  the hierarchical structure of a document.
In more detail, we treat each sentence and section as a node of a heterogeneous graph.
 A state contains 1) a local representation of a sentence with discourse awareness, 2) the global context of a sentence within the document, and 3) information about  the extraction history.
As such, we seamlessly integrate RL with GNN in GoSum. To summarize, our main contributions of this paper are:
1) We propose an  approach called GoSum \footnote{Source code is available  on Supplementary Files} as a novel graph-based discourse-aware extractive summarization model. GoSum can  generate a concise and informative summary operating on a subsentential discourse unit level. 2) We effectively integrate reinforcement learning with GNN under GoSum. With obtaining sufficient samples in reinforcement learning,  GoSum relies on GNN to capture discourse information about documents, particularly for the discourse hierarchy, to extract  compact summaries. 3) We have conducted comprehensive experiments to validate the performance of GoSum. GoSum has achieved state-of-the-art performance compared with strong baselines on two benchmark datasets: PubMed and arXiv.


 \section{Related work}
\label{sec:relate}




\subsection{Long Document Summarization}
Unlike the short-input summarization that BERT-based models~\cite{2019_bertsum} have been successfully used, studies on long document summarization struggle with long-input sequences. 
Research on abstractive models~\cite{bigbrid_2020,hepo_2021} mainly exploring different architectures of Transformer to cope with excessively long inputs.
However, the study of extractive models focus on other perspectives.
For example, long documents follow a standard discourse structure, i.e. scientific papers are written section by section to describe the background, methodology, experiment etc.
Several methods~\cite {localglobal_2019,collins2017supervised,discourse_2021} leverage such section information to guide the generation of summaries.
Reinforcement learning has also successfully been applied to long document extractive summarization.
LG+RdLoss~\cite{rdloss_2020} is an improved version of LG~\cite{localglobal_2019} that constrains sentence redundancy with reinforcement learning.
Differing from LG-RdLoss, MemSum ~\cite{memsum_2022} uses extraction history~\cite{neusumm_history_2018}, and treat extractive summarization as a multi-step episodic Markov decision process.

\subsection{Graph-based Extractive Summarization}
Early summarization solutions are graph-based unsupervised methods~\cite{graph0_2004}, relying on explicit surface features. They construct a similarity graph between sentences and formulate extractive summarization as a task of ranking nodes.
Recently, researchers use graph neural network on supervised summarization.
HSG~\cite{hsg_2020} was the first  to construct a heterogeneous graph neural network for extractive document summarization.
HahSum~\cite{hahsum_2020} considers inter-sentence redundancy in graph construction.
HEROS~\cite{discourse_2021} applies graph-based to the long text field and uses the information about input article discourse.
All these methods treat sentences and words as nodes in a graph.
Based on the RST tree, DiscoSum\cite{disco_2020} uses a graph  to capture the long-range dependencies among discourse units,  with  Elementary Discourse Units as the nodes in a graph.
 To some extent, the graph-based approach solves the quadratic computational and memory complexities encoded using Transformer and works well with the structural information of the input. Therefore, we choose to use GNNs for GoSum.
 \begin{figure*}[htp]
    \centering
    \includegraphics[width=2.0\columnwidth]{./miscs/framework3.pdf}
    \caption{The overall framework of GoSum. MHP: multi-head pooling, and MLP: multi-layer perceptrons }
    \label{fig_framework}
\end{figure*}
 
\section{GoSum }
\label{sec:method}

Figure 1 shows the architecture of GoSum. With the input of a structural text,  GoSum starts with constructing a  graph of the text
and then generates  an embedding for the current state by using three sub-encoders: 1) The Graph-based Discourse Awareness Encoder, 2) The Global Context Encoder, and 3) The Extraction History Encoder. After this, the extractor decides whether to stop or continue the extraction based on the current embedding. 


\subsection{Task Definition}
Extractive summarization is regarded as a sequence labeling task.
Denote $D=\{s_1,s_2,...,s_n\}$ as a document that consists of $n$ sentences. Extractive summarizer produces a sequence of indexes $\hat{Y} = \{\hat{y}_1,\hat{y}_2,...,\hat{y}_T\}$ to determine which sentences should be included in the summaries.
$\hat{y}_i$ denotes the index of the sentence.
Since the datasets only contain document-abstract pairs, we use beam search and automatic metric ROUGE to sample a set of oracle labels $\{Y^1,Y^2,...\}$.
Then, we keep the ROUGE score of each oracle label's corresponding summary against the abstract as a reward for reinforcement learning.

\subsection{GoSum via Policy Gradient}
From the perspective of RL for extractive summarization, we can view our GoSum model  as an agent, parameters of the network as a policy $\pi_{\theta}$, and  extracting at each step  as an action.
Given an oracle label $Y = \{y_1,y_2,...,y_T\}$, $R=(r_1,r_2,...,r_T)$ is a reward list, $r_t$ is the reward of an action to select sentence $y_t$ after the set of $\{y_1,y_2,...,y_{t-1}\}$ are already selected.
The goal of policy gradient in GoSum is to maximize objective function $\mathcal{L}(\theta)=E_{\pi_{\theta}} (R)$.
The reward value $r_t$ is the same as the ROUGE \cite{rouge_2004} score $r$ between the oracle summary and gold abstract.
\begin{align}
    r = \frac{1}{3} \left(  \text{ROUGE-1}_f + \text{ROUGE-2}_f + \text{ROUGE-L}_f  \right)
\end{align}

In reinforcement learning~\cite{rl_1992}, the policy gradient is defined as:

\begin{align}
    \nabla \mathcal{L} (\theta) = -E_{\pi_\theta} \left[  r \sum\limits_{t=1}\limits^T \nabla_{\theta} \log \pi_{\theta} (A_t|S_t,\theta)  \right]
\end{align}

where $\pi_{\theta} (A_t|S_t,\theta)$ represents the likelihood of action $A_t$ from policy net $\pi_{\theta}$ when a state is $S_t$ and the time step is $t$.
Usually, the extractive method extracts a fixed number of sentences. However, GoSum uses a stop mechanism, which  determines the point at which to stop extracting itself. So the policy likelihood can be written in the following form:
\begin{align}
    \pi(A_t|S_t,\theta) = p(\text{stop} | S_t, \theta) p(A_t|\text{stop}, S_t,\theta)
\end{align}

In each step, the policy net first outputs a probability $p_{stop}$. If $p_{stop}$ is greater than a pre-defined threshold, then the model will stop extracting, otherwise, the model continues to find the next sentence.


\subsection{State Encoder}

\subsubsection{Graph-based Discourse Awareness Encoder}

\noindent \textbf{Graph Construction:}
GoSum constructs a heterogeneous graph that represents sections and sentences of a document at the discourse level.
There are only two kinds of nodes in the graph: sentence nodes and section nodes.
The way we build the graph is slightly different from the previous graph-based approach ~\cite{discourse_2021,hsg_2020,hahsum_2020} in that we discard the word nodes.
As reinforcement learning is  time-consuming, removing word nodes can significantly improve the running time  of GoSum, 
In addition, the information transferred from word nodes to sentence nodes is essentially about the representation of the sentence's local content.  Therefore, the use of a simple encoder is sufficient, such as LSTM.
We connect edges between each sentence and the section containing the sentence.
Also, a fully-connected subgraph is built among each section.


\noindent \textbf{Graph Initialization:}
After the graph is constructed, we give each node an initial representation.
Suppose that  a sentence in a document consists of $s$ words: $(sw_1,sw_2,...,sw_s)$, and the text of a section (e.g. "Related work") is composed of $c$ words: $(cw_1,cw_2,...,cw_c)$.
We   first employ  Glove\cite{glove_2014} word embeddings to embed these words, then use BiLSTM \cite{lstm_1997} with Multi-head pooling (MHP) to produce sentence representation $h_s^0$ and section representation $h_c^0$:
\begin{align}
    h_c^0 = \text{MHP} (\text{LSTM} ( \text{Glove} (cw_1,cw_2,...,cw_c) ) ) \\
    h_s^0 = \text{MHP} (\text{LSTM} ( \text{Glove} (sw_1,sw_2,...,sw_s) ) )
\end{align}



\noindent \textbf{Graph Attention Networks:}
With the available graph $G$ and its node features, we use a graph attention layer (GAT) \cite{gat_2017} to update our semantic nodes. The expressions of GAT are as follows:

\begin{align}
    e_{ij} &= \text{LeakyRELU} (W_a[W_q h_i; W_k h_j]) \\
    \alpha_{ij} &= \frac{\exp (e_{ij})}{\sum_{k\in \mathcal{N}_i} \exp (e_{ik})} \\
    h_i^{\prime} &= \sigma (\sum_{j\in \mathcal{N}_i} \alpha_{ij} W_v h_i) + h_i
\end{align}

where $W_a, W_q, W_k,$ and $ W_v$ are trainable weights, and $h_i$ is the node representation of the $i-th$ node in the graph. $\mathcal{N}_i$ is the neighbor nodes of node $i$.

\noindent \textbf{Message Passing:} We first update section nodes with their neighbor sentence nodes via the GAT and Feed Forward Net (FFN) layers:
\begin{align}
    U_{s\rightarrow c} &= \text{GAT} ({H}_c^0, {H}_s^0, {H}_s^0) \\
    {H}_c^1 &= \text{FFN}(U_{s\rightarrow c} + {H}_c^0)
\end{align}
where ${H}_s^0$ is the initialized representation of sentence nodes, and ${H}_c^0$ is for section nodes.
 $\text{GAT} ({H}_c^0, {H}_s^0, {H}_s^0)$ denotes ${H}_c^0$ as an attention query, and ${H}_s^0$ as a key and value.
We continue to update section nodes by section to section edges:

\begin{align}
    U_{c\rightarrow c} &= \text{GAT} ({H}_c^1, {H}_c^1, {H}_c^1) \\
    {H}_c^2 &= \text{FFN}(U_{c\rightarrow c} + {H}_c^1)
\end{align} \label{eq:mp3}
After a section node is updated, it already has section-level discourse information. We then pass this discourse information to each corresponding sentence node:
\begin{align}
    U_{c\rightarrow s} &= \text{GAT} ({H}_s^0, {H}_c^2, {H}_c^2) \\
    {H}_s^1 &= \text{FFN}(U_{c\rightarrow s} + {H}_s^0)
\end{align}
Since GoSum uses only one-layer GAT, the output is ${H}_s^1$.

\subsubsection{Global Context Encoder}
After that, a Bi-LSTM takes ${H}_s^1$ as input to produce sentence embeddings ${H}_s^g$ that encodes global contextual information. 
This module encodes global contextual information such as the sentenceâ€™s position in the document and information on neighboring sentences.


\subsubsection{Extraction History Encoder}
In extractive summarization, extracting sentences by an extraction history encoder is first used in NeuSum \cite{neusumm_history_2018}, in order to avoid redundancy.
Comparing the extracted sentences and the remaining unextracted sentences, an extraction history encoder(EHE) generates the embedding for each of the remaining sentences. The result is used to guide the scoring of those unextracted sentences.
Our design of the extraction history encoder(EHE) in GoSum follows \cite{memsum_2022}.
It consists of a series of $N_h$ identical layers.
Each layer first performs a multi-head self-attention between the remaining sentences, followed by another multi-head self-attention performed on the sentences that have been extracted.
Two attention sublayers capture the information of both extracted and remaining sentences.
For those sentences that have not been extracted yet in time step $t$, an extraction history embedding ${H}_e^t$ is obtained.




\subsection{Extractor}
As shown in Eq(3), the extractor decides whether to stop extraction or generate the score of each remaining sentence according to the state.
The state $S_t$ is described by concatenating three types of vectors:
sentence representation from discourse graph $H_s^1$,
sentence global content representation ${H}_s^g$,
and extraction history embedding ${H}_e^t$ as:
\begin{align}
    S_t = [H_s^1;H_s^g;H_e^t]
\end{align}
A multi-head pooling followed by a multi-layer perceptrons (MLP) is used to compute stop signial of extraction. Another MLP decides to extract which sentence.





\begin{algorithm}[tb]
\caption{Training procedure in one iteration}
    \label{alg:algorithm}
    \textbf{Input}: Document-Summary pair $<D, S>$\\
    \textbf{Parameters}: Learning rate: $l$, and model parameters: $\theta$ \\
    \begin{algorithmic}[1]
        \STATE A label sequence ${Y}=\{y_1, y_2,...,y_T\}$ is sampled using beam search, with corresponding summaries having ROUGE scores $r$ against $S$.
        \STATE Obtain discourse-aware sentence embedding $H_s^1$:
        \STATE \quad Initialize Graph $H_s^0, H_c^0$
        \STATE \quad Message passing $(H_s^0, H_c^0) \rightarrow H_c^1$
        \STATE \quad Message passing $(H_c^1, H_c^1) \rightarrow H_c^2$
        \STATE \quad Message passing $(H_c^2, H_s^0) \rightarrow H_s^1$
        \STATE Obtain global-content sentence embedding $H_s^g$
        \STATE Let $t=1$.
        \WHILE{$t$ is no larger than $T$}
            \STATE Produce extraction history embedding $H_e^t$ for the remaining sentences.
            \STATE Output the probability of the sentence from the Extractor to select $y_t$ and $p_{stop}$ by using state $S_t= [ H_s^1, H_s^g, H_e^t ]$.
            \STATE Update policy gradient: $\theta \leftarrow \theta + l \cdot r \nabla \pi(A_t|S_t,\theta) $
            \STATE $t \leftarrow t + 1$
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}
 

\subsection{Training}
Usually, the training samples of reinforcement learning algorithms are obtained by sampling the policy net that is currently being trained. Since the golden standard is already known at the time of training for extractive summarization, we can obtain high-quality training samples by performing beam search sampling in advance.
This saves the time spent on sampling and allows the model to converge more quickly.
The flow of the training process in GoSum is shown in Algorithm 1.
%
 
\section{Experiments}
\label{sec:exp}


\subsection{Summarization Datasets}
We evaluate our model on the two scientific paper datasets: PubMed and arXiv  \cite{pubmedarxiv_2018}.
Both datasets provide information about the structures of the papers.
The inputs of these datasets are the full text of scientific papers except for the abstract, and the gold summaries are the corresponding abstracts.
As can be seen from Table ~\ref{tab:dataset}, both datasets are relatively large in size, especially the arXiv dataset.


\begin{table}[h]
  \centering
  \begin{tabular}{l|p{12mm}p{12mm}}\toprule
                                & PubMed    &   arXiv       \\ \midrule
     avg. \# sents of doc        & 89        &   207         \\
     avg. \# sents of summ       & 6.8         &   9.8          \\
     avg. \# tokens of doc       & 2730      &   5206        \\
     avg. \# tokens of summ      & 181       &   238         \\
     avg. \# sections of doc     & 6.0      &   5.5         \\\midrule
     \# Train                   & 116 937   &   202 880      \\
     \# Valid                   & 6633      &   6436        \\
     \# Test                    & 6658      &   6440        \\
     \bottomrule
\end{tabular}
\caption{The datasets we used in the expertiments.}\label{tab:dataset}
\end{table}
 
\subsection{Experimental Setup}

\subsubsection{Evaluation Metrics}
ROUGE score \cite{rouge_2004} is used to evaluate the model performance.
We report the F1 score of unigram, bigram overlap (ROUGE-1, ROUGE-2), and the longest common subsequence (ROUGE-L).

\subsubsection{Training data Sampling}
The original PubMed and arXiv datasets do not provide extractive training labels.
We use beam search to obtain extractive oracle summaries. For each document-abstract pair, the algorithm generates at most 15 different summaries with the largest ROUGE score. For the PubMed and arXiv datasets, we set the maximum sequence length of extracted summaries to 7 and 8, respectively.

\subsubsection{Implementation Details}
Our model is trained using adam\cite{adam_2015} optimizer with the learning rate $1e-4$, $\beta_1 = 0.9$, and $\beta_2=0.999$.
GoSum and its variants are trained from 20 epochs on the both pubmed and arxiv dataset.
In each iteration, for each input document, we randomly sample one pre-prepare label for training.
Model checkpoints are saved and evaluated every 10,000 steps.
During the testing phase, the threshold of $p_{stop}$ for PubMed and arXiv is set to $0.6,$ and $ 0.45$, respectively.
GoSum and its variants are all trained on four TITAN XP GPUs.


\begin{table}[t]
  \centering
  \begin{tabular}{p{36mm}|p{8mm}p{8mm}p{8mm}}\toprule
        Models          &  R-1  &  R-2  &  R-L   \\ \midrule
        Oracle                          & 60.00 & 30.60 & 53.03  \\  \midrule
        \multicolumn{4}{c}{Extractive models}  \\ \midrule
Lead-10                       & 30.52 & 10.33 & 31.44  \\
          Local-Global                  & 43.77 & 17.50 & 38.71  \\
          \quad + RdLoss                & 44.01 & 17.79 & 39.09  \\
          Sent-CLF                      & 34.01 &  8.71 & 30.41  \\
          Sent-PTR                      & 42.32 & 15.63 & 38.06  \\
          HEROS                         & 47.74 & 20.46 & 42.39  \\
          \quad w/o content ranking     & 45.90 & 18.33 & 40.78  \\
          Topic-GraphSum                & 46.05 & 19.97 & 33.61  \\
          MemSum                        & 48.42 & 20.30 & 42.54  \\
          \textbf{GoSum (ours)}         & \textbf{48.61} & \textbf{20.53} & \textbf{42.80}  \\ \midrule
          \multicolumn{4}{c}{Abstractive models}  \\ \midrule
          PEGASUS                       & 44.21 & 16.95 & 38.83  \\
          BigBird-base                  & 41.22 & 16.43 & 36.96  \\
          BigBird-large                 & 46.63 & 19.02 & 41.17  \\
          Dancer                        & 45.01 & 17.60 & 40.56  \\
          HAT                           & 46.68 & 19.07 & 42.17  \\
          Hepos-Sinkhorn                & 47.87 & 20.00 & 41.50  \\
          Hepos-LSH                     & 48.24 & 20.26 & 41.78  \\
          \bottomrule
  \end{tabular}
  \caption{Results on arXiv Dataset.} \label{tab:sota_arxiv}
\end{table}




 \subsubsection{Baselines}
We compare GoSum with state-of-the-art extractive methods and abstractive methods on the two datasets mentioned above.
In particular, the extractive baselines are \textit{Local-Global}~\shortcite{localglobal_2019}  that incorporates local and global contexts to extract summaries, and \textit{Local-Global+RdLoss}~\shortcite{rdloss_2020}. 
that further adds a redundancy reinforcement learning loss. 
\textit{HEROS}~\cite{discourse_2021} use heterogeneous graph-based with nodes from different discourse levels.
To ensures that the input is consistent with other baseline, we also record its results without content ranking module. 
\textit{NeuSum}~\shortcite{neusumm_history_2018} is a model that considers the extraction history. 
\textit{MemSum}~\shortcite{memsum_2022} is a reinforcement-learning-based extractive summarizer.
\textit{Sent-CLF} and \textit{Sent-PTR} \cite{clfptr_2020} are a LSTM based sentence classifier and a hierarchical seq2seq sentence pointer.

For the abstractive methods, we compare GoSum with the following methods:
\textit{PEGASUS}~\cite{pegasus_2020} is a pre-trained language model for summarization.
\textit{Dancer}~\cite{dancer_2020} is a divide-and-conquer method.
\textit{BigBird}~\shortcite{bigbrid_2020} uses sparse and windowed attentions to handle long input sequences.
\textit{Hepos}~\cite{hepo_2021} uses the efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source.
\textit{HAT}~\cite{hat_2021} adds hierarchical attention layers to an encoder-decoder model to summarize long documents.







 


\begin{table}[t]
  \centering
  \begin{tabular}{p{36mm}|p{8mm}p{8mm}p{8mm}}\toprule
        Models          &  R-1  &  R-2  &  R-L   \\ \midrule
        Oracle                          & 61.99 & 34.95 & 56.76  \\  \midrule
        \multicolumn{4}{c}{Extractive models}  \\ \midrule
          Lead-10                       & 37.45 & 14.19 & 34.07  \\
          Local-Global                  & 45.18 & 20.20 & 40.72  \\
          \quad + RdLoss                & 45.30 & 20.42 & 40.95  \\
          Sent-CLF                      & 45.01 & 19.91 & 41.16  \\
          Sent-PTR                      & 43.30 & 17.92 & 39.47  \\
          HEROS                         & 48.14 & 21.82 & 43.33  \\
          \quad w/o content ranking     & 46.63 & 20.63 & 42.01  \\
          Topic-GraphSum                & 48.85 & 21.76 & 35.19  \\
          NeuSum                        & 47.46 & 21.92 & 42.87  \\
          MemSum                        & 49.25 & 22.94 & 44.42  \\
          \textbf{GoSum (ours)}         & \textbf{49.83} & \textbf{23.56} & \textbf{45.10}  \\ \midrule
          \multicolumn{4}{c}{Abstractive models}  \\ \midrule
          PEGASUS                       & 45.97 & 20.15 & 41.34  \\
          BigBird-base                  & 43.70 & 19.32 & 39.99  \\
          BigBird-large                 & 46.32 & 20.65 & 42.33  \\
          Dancer                        & 46.34 & 19.97 & 42.42  \\
          HAT                           & 48.36 & 21.43 & 37.00  \\
          Hepos-Sinkhorn                & 47.93 & 20.74 & 42.58  \\
          Hepos-LSH                     & 48.12 & 21.06 & 42.72  \\ \bottomrule
  \end{tabular}
  \caption{Results on PubMed Dataset.} \label{tab:sota_pubmed}
\end{table}


 
\section{Results}

\subsection{Performance Comparisons}
Tables~\ref{tab:sota_arxiv} and ~\ref{tab:sota_pubmed} report the results of our model on arXiv and PubMed datasets, respectively.
On both datasets, GoSum outperforms state-of-the-art extractive and abstractive baselines. 
RL-based methods like GoSum, MemSum and LG-RdLoss show substantial performance gain, demonstrating the effectiveness of the reinforcement learning.
Compared with MemSum, GoSum has better performance. The results depend on two factors: 1) the use of the structural information from the input articles; and 2) the use of graphs to model sentences and sections. In this way, sentences can obtain more abundant information from sections, and sections can share and propagate their topical information.
GoSum has more performance improvement on PubMed dataset compared to arXiv dataset. One reason for this may be that the section information provided by the pubmed dataset is more accurate, as explained in more detail in section 5.3.



\subsection{Ablation Studies}

In table~\ref{tab:ablation_pubmed}, we conduct ablation studies by comparing GoSum with its variants.


To validate the performance of the graph structure, we set the following GoSum variants:
\textbf{GoSum w/o sec2sec edges} remove section-to-section edges in graph construction, and take $H_c^1$ as a key input in Eq(13).
\textbf{GoSum w/o graph} has no graph modeling. In particular, the global contextual embedding $H_s^g$ is obtained directly using $H_s^0$. State representation $S_t$ in Eq(15) includes one more embedding $H_c^0$ to capture section information.
\textbf{GoSum w/o sec \& graph} does not use document structural information and graph modeling.

Improvements from \textbf{GoSum w/o graph} to \textbf{GoSum w/o sec2sec edges} demonstrate that the addition of paper structure information can slightly improve GoSum. The performance of GoSum has a greater improvement if using graphs to model the relationships between sentences and sections.

Next, we examine the effects of different embeddings on the performance of GoSum.
For \textbf{GoSum w SecE}, the extractor takes additional section representation $H_c^2$ in Eq(12).
\textbf{GoSum w/o GCE}, \textbf{GoSum w/o DLE}, and \textbf{GoSum w/o EHE}
remove Global Context Embedding $H_s^g$, Discourse aware Local sentence Embedding $H_s^1$, and Extraction History Embedding $H_s^e$ in Eq(12), respectively.

Although \textbf{GoSum w SecE} adds an extra embedding, the resulting scores instead slightly decrease. This indicates that the information about section nodes has been incorporated into the local content embedding during the graph update process so that adding section embedding will be redundant with possible over-fitting.
If the other three embeddings are removed, the performance drops. \textbf{GoSum w/o DLE} with removing $H_s^1$ results in the most decrease. This  also indicates that the discourse-aware local sentence embedding contains more useful information.

\begin{table}[t]
  \centering
  \begin{tabular}{l|p{13mm}p{13mm}p{13mm}}\toprule
                              & \hfil R-1  & \hfil R-2  & \hfil R-L    \\ \midrule
        GoSum                 & \hfil 49.83 & \hfil 23.56 & \hfil 45.10   \\ \midrule
        \quad w/o sec2sec edges     & \hfil 49.72 & \hfil 23.46 & \hfil 44.99   \\
        \quad w/o graph             & \hfil 49.44 & \hfil 23.24 & \hfil 44.74   \\
        \quad w/o sec \& graph      & \hfil 49.22 & \hfil 23.02 & \hfil 44.49   \\ \midrule
        \quad w   SecE              & \hfil 49.80 & \hfil 23.56 & \hfil 45.08   \\
        \quad w/o GCE               & \hfil 48.80 & \hfil 22.34 & \hfil 44.16   \\
        \quad w/o DLE               & \hfil 48.01 & \hfil 21.84 & \hfil 43.57   \\
        \quad w/o EHE               & \hfil 49.24 & \hfil 23.09 & \hfil 44.28   \\ \bottomrule
  \end{tabular}
  \caption{Abaltion studies on PubMed dataset.} \label{tab:ablation_pubmed}
\end{table}
 \subsection{What exactly enhances GoSum?}
\noindent \textbf{Aspects of graph-organized discourse states:}
With the use of reinforcement learning and graph neural networks, GoSum has achieved a significant performance improvement over other extractive approaches.
A nature question of what exact reasons why GNN can enhance GoSum is raised. There may be two reasons:
One is that the use of a graph captures the input discourse information; another could be that the hierarchy of this graph makes  section nodes to be a sink of information diffusion of their sentences.
To validate these answers, we examine the  performance of GoSum by gradually scrambling the section attribution information of the input sentences without changing the other parameters of the model. The disruption of section attribution information can blur the discourse information, but the graph still keeps the hierarchical structure of documents.


For the above reasons, we disrupt the section attribution of the input sentences proportionally, with an increment of 10\% in each experiment.
Since experimenting with the full data set is too time-consuming, we select 10,000 samples from each PubMed and arXiv datasets for training.
As seen from  Fig~\ref{fig:exp1}, the performance of GoSum decreases rapidly with the declining amount of discourse information.
Because of the small number of training samples and the instability of reinforcement learning, the performance of GoSum fluctuates slightly from dataset to dataset but shows a slow decreasing trend overall.
The performance of GoSum decreases significantly at the beginning, which indicates that GoSum is sensitive to the accuracy of section information. It also confirms that accurate discourse information is required to improve the performance of GoSum.

\begin{table}[t]
  \centering
  \begin{tabular}{l|p{13mm}p{13mm}p{13mm}}\toprule
                    & \hfil R-1  & \hfil R-2  & \hfil R-L    \\ \midrule
                    \multicolumn{4}{c}{PubMed}     \\ \midrule
        GoSum                       & \hfil 49.83 & \hfil 23.56 & \hfil 45.10   \\
        \quad w/o SecTitle           & \hfil $\downarrow$ 0.20 & \hfil $\downarrow$ 0.11 & \hfil $\downarrow$ 0.12   \\ \midrule
                    \multicolumn{4}{c}{arXiv}     \\ \midrule
        GoSum                       & \hfil 48.61 & \hfil 20.53 & \hfil 42.80   \\
        \quad w/o SecTitle           & \hfil $\downarrow$ 0.08 & \hfil $\downarrow$ 0.03 & \hfil $\downarrow$ 0.06   \\ \bottomrule
  \end{tabular}
   \caption{Comparisions between GoSum trained with the complete datasets and those without the section titles.} \label{tab:exp2}
\end{table} 
In addition to the discourse information of the literature, which divides the sentences into different sections, there are also section-specific names, such as ``introduction", ``methodology" etc.
These specific text title contexts contain semantic information, which helps to improve the performance of GoSum.
First, we set up a control model of \textbf{GoSum w/o SecTitle}, which has the same architecture as GoSum, but the section title in the training data is replaced with a meaningless text "section \#id".
The experimental results in Table~\ref{tab:exp2} show that the performance of  \textbf{GoSum w/o SecTitle} is slightly worse than that of GoSum. This indicates that the semantic information about section title text is useful but not essential. The key to performance improvement is the discourse hierarchies of documents.
Moreover, \textbf{GoSum w/o SecTitle} drops more significantly on the PubMed dataset. The difference in the performance between \textbf{GoSum w/o SecTitle}  and GoSum on the arXiv dataset is not significant, probably because the title quality of the documents in the arXiv dataset is not satisfied.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.1\columnwidth]{./miscs/exp1.pdf} \caption{GoSum performance varies as section information is corrupted at a rate (x-axis). Y-axis is the average ROUGE score. The green dots show the scores of GoSum on the PubMed dataset, while the blue dots show the results of GoSum on the arXiv dataset.}
    \label{fig:exp1}
\end{figure}
 



\noindent \textbf{Aspects of reinforcement learning:}
There are two factors that can improve the performance of GoSum by using reinforcement learning: First, more sampling is performed, which is equivalent to data augmentation; and second, the model gives a feedback reward to different samples during training which helps to distinguish between good and bad samples. The experimental results on investigating the impact of these two factors on the GoSum performance are reported in Table 6. 

\textbf{w/o reward} sets rewards of all samples to 1, and the experimental results are slightly lower than those of the complete RL model.
\textbf{Complete RL} samples an average of 6.52 label sequences per document-abstract pair.
The \textbf{sample top-k} indicates that GoSum is trained with only the $k$ highest sampled label sequences of an input document.
As the number of samples increases, the  performance of GoSum improves significantly. In conclusion, the experimental results on RL verified our conjecture.



 

\section{Conclusion}
\label{sec:conclude}

In this paper, we have presented a novel approach called GoSum for extracting summaries from long documents. It effectively integrates reinforcement learning with a graph neural network. In particular, we have shown how graph-organized discourse information can  be applied in reinforcement learning-based extractive summarization.
Experimental results on the arXiv and PubMed datasets have demonstrated that GoSum achieves state-of-the-art performance. The ablation experiments examine the effect of discourse information on GoSum. The results show that the performance of GoSum comes from the use of the hierarchical attribution of sentences and the semantic information about section titles of documents.
With achieving satisfactory results in scientific literature, GoSum requires  hierarchical discourse information about long texts as its inputs. In the future, we will attempt to automatically generate discourse information from documents.


\begin{table}[t]
  \centering
  \begin{tabular}{l|p{13mm}p{13mm}p{13mm}}\toprule
                    & \hfil R-1  & \hfil R-2  & \hfil R-L    \\ \midrule
        Complete RL                  & \hfil 49.83 & \hfil 23.56 & \hfil 45.10   \\
        \quad w/o reward             & \hfil 49.64 & \hfil 23.37 & \hfil 44.96   \\
        \quad sample top-1          & \hfil 49.10 & \hfil 23.00 & \hfil 44.42   \\
        \quad sample top-2          & \hfil 49.27 & \hfil 23.07 & \hfil 44.61   \\
        \quad sample top-4          & \hfil 49.64 & \hfil 23.33 & \hfil 44.96   \\ \bottomrule
  \end{tabular}
   \caption{ GoSum performance by reinforcement learning with different settings on PubMed dataset.} \label{tab:exp3}
\end{table}  





\bibliography{anthology}
\bibliographystyle{acl_natbib}






\end{document}
