



\begin{figure*}[t]
\centering
\includegraphics[trim=5 15 5 0, clip,width=0.99\textwidth]{figs/model_scale_nocaps.pdf}
\caption{\textbf{Image captioning performance on~\nocaps~when upscaling model for each dataset size}. The results of the same models evaluated on COCO are shown in Figure \ref{fig:model_size}. The x-axis plots the number of parameters for each model size (\emph{e.g.}, tiny, small, huge) in a logarithmic scale. The definition of model sizes is detailed in Table \ref{tab:arch_details}. }
\label{fig:model_size_nocaps}
\end{figure*}

\section{Related Work on Image Captioning}
There is a rich literature on image captioning studying different model structures and learning approaches. Recent works have proposed enhanced attention-based models to improve the performance, such as ORT~\cite{herdade2019image}, AoANet~\cite{huang2019attention}, M Transformer~\cite{cornia2020meshed}, X-LAN~\cite{pan2020x}, and RSTNet~\cite{zhang2021rstnet}. Besides, researchers have explored to leverage semantic attributes~\cite{yao2017boosting,li2019entangled}, scene graphs~\cite{yang2019auto}, and graph convolutional networks~\cite{yao2018exploring} for captioning.
While those methods focus on learning from well-annotated captions in relatively small datasets, such as COCO Caption, we investigate the impact of data scales with a much larger and noisier dataset. This allows the model to learn more diverse visual concepts, and makes one step further towards in-the-wild captioning.

\section{Comparison of Image-Text Datasets}
Besides CC3M~\cite{sharma2018conceptual} and CC12M~\cite{changpinyo2021conceptual}, we also compare with other web-crawled image-text datasets as described in the following.
\begin{itemize}[leftmargin=*]
\item The dataset used in \textbf{CLIP}~\cite{radford2021learning} has  million image-text pairs. Unlike our dataset crawled from web without re-balancing, their dataset is built upon a set of  queries. The queries include all the words occurring at least  times in
the English version of Wikipedia and are augmented with bi-grams. The image-text pairs are searched such that the text includes one of the queries. The final results are also balanced to include up to  image-text pairs per query.

\item The dataset used in \textbf{ALIGN}~\cite{jia2021scaling} has  billion image-text pairs. A later work SimVLM~\cite{wang2021simvlm} also uses this dataset. The data collection pipeline is similar to that used in Conceptual Captions~\cite{sharma2018conceptual,changpinyo2021conceptual}, but relaxes most cleaning steps. Only some rule-based filters are applied, such as image size, alt-text frequencies, and rare words.

\item The Wikipedia-based Image Text Dataset (\textbf{WIT})~\cite{srinivasan2021wit} is composed of  million unique images and  million texts. Different from all other listed datasets, it features multilingual texts across  languages. The images and texts are collected from the Wikipedia content pages. It provides texts from multiple sources, such as reference, attribution and
alt-texts, and texts in different languages for the same image.

\item \textbf{WenLan}~\cite{huo2021wenlan} has  million image-text pairs. The web-collected pairs have gone through an elaborate cleaning process. For each data source, they also use topic
models to extract topic words, and analyze the topic distribution to help with selecting desired contents.

\item \textbf{LAION-400M}~\cite{schuhmann2021laion} has  million image-text pairs, and is recently released to public. Instead of applying human designed heuristics in data cleaning, this dataset relies on the CLIP~\cite{radford2021learning} model to filter image-text pairs, where the cosine similarity scores between image and text embeddings are calculated and filtered by . As CLIP~\cite{radford2021learning} itself is also trained on noisy image-text pairs, it is yet unknown about the quality of this dataset compared to others cleaned by heuristic-based pipelines.

\end{itemize}

We summarize the characteristics of existing image-text datasets from three perspectives:
\begin{itemize}
    \item \textit{Accessibility}: The datasets CC3M~\cite{sharma2018conceptual}, CC12M~\cite{changpinyo2021conceptual}, WIT~\cite{srinivasan2021wit} and LAION-400M~\cite{schuhmann2021laion} are released to public with the image URL and associated meta files. Other datasets are proprietary.
    \item \textit{Scale}: Except LAION-400M~\cite{schuhmann2021laion}, the other released datasets have tens of millions of images, which are not enough for the scaling purpose.
    \item \textit{Collection pipeline}: LAION-400M~\cite{schuhmann2021laion} is mainly filtered by the CLIP~\cite{radford2021learning} model, while all the other datasets are filtered by a series of rules including expert designed heuristics and/or complicated models. As to the data sources, WIT~\cite{srinivasan2021wit} is collected from Wikipedia. The other datasets are crawled from Internet. 
\end{itemize}

In a nutshell, we construct ALT200M to study the scaling behavior, as no such large-scale datasets are available (LAION-400M~\cite{schuhmann2021laion} is concurrent with ours). Following prior works, we crawl images and the alt attributes from Internet, and apply minimal rule-based filters to retain as many images as possible. Our experiments show that the web-collected data can help to substantially improve the performance of captioning models.

\section{Detailed Hyperparameters}

\begin{table}
\small
\centering
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Model } & \multicolumn{2}{c|}{Pre-training} & \multicolumn{2}{c}{Finetuning}\\
 &  BS & LR & LR (M) & LR (M) \\
\midrule
tiny &  &   & &  \\
tiny12 &  &   &  & \\
small &  &   &  & \\
small24 &  &   &  & \\
base &  &   &  & \\
base24 &  &   &  & \\
large &  &   &  & \\
huge &  &   & - & \\
\bottomrule
\end{tabular}
\caption{\textbf{Hyperparameters used for all model sizes}. BS: effective batch size. LR: learning rate. LR(M) is used for finetuning checkpoints pre-trained on the M subset. LR(M) is used for other finetuning.}
\label{tab:hyperparam}
\end{table}

We include a table of important hyperparameters in Table~\ref{tab:hyperparam} for all model sizes as defined in Table~\ref{tab:arch_details}. All the models are pre-trained for  epochs on subsets of ALT200M, and finetuned for  epochs on COCO with batch size . During pre-training, the learning rate is warmed up for the first  steps to the peak value, then linearly decaying to . During finetuning, the learning rate linearly decays from the initial value to  without warm up. We use AdamW optimizer with weight decay . The cross-entropy loss is calculated with label smoothing .

The evaluation results on COCO ``Karpathy'' test split and~\nocaps~validation set are plotted in Figure~\ref{fig:model_size},~\ref{fig:data_size}, and~\ref{fig:model_size_nocaps}.

\section{More Qualitative Examples}


In this section, we provide more qualitative examples of our web-crawled dataset ALT200M and the captioning results of our LEMON model.

Figure~\ref{fig:visual_alt200m} shows some examples of the image-text pairs in ALT200M. While some of the alt attributes are descriptive sentences that can serve as good training targets, \eg, Figure~\ref{fig:visual_alt200m} (7), (8), (9), it is noted that some texts are not semantically well formed, \eg, Figure~\ref{fig:visual_alt200m} (10). Some texts are very short phrases containing only 2 to 4 words, \eg, Figure~\ref{fig:visual_alt200m} (1) - (6). We also observe that some texts do not precisely describe what content is shown in the image, but mention external knowledge or information. For example, Figure~\ref{fig:visual_alt200m} (12) shows a woman pointing at the camera, but the text is ``I got you''. And the text of  Figure~\ref{fig:visual_alt200m} (11) are likely to be extracted from news. These might put challenges for the model to learn from noisy text supervision. However, there are indeed a large variety of (fine-grained) visual objects present in the images and texts, such as burdock leaf, karate, mahjong, and great blue heron. Compared to human-annotated datasets, these web-collected data provide much richer training resources, especially for the long-tailed concepts. 

After training on the ALT200M dataset, our LEMON model has achieved impressive results, even in zero-shot manner. We present more examples of generated captions in Figure~\ref{fig:visualsupp}, in addition to Figure~\ref{fig:visual} in the main paper. Compared to the baseline model trained on COCO only, the LEMON model can recognize much more fine-grained objects, as highlighted in red.

\begin{figure*}[t]
\centering
\includegraphics[trim=0 0 120 0, clip,width=0.9\textwidth]{figs/visu_alt200m_new.pdf}
\caption{\textbf{ALT200M} examples of images and the associated alt attributes. The image-text pairs cover rich visual concepts, but some texts are not well formed (\eg, Figure (10)), or do not directly reflect the image content (\eg, Figure (11) - (14)). }
\label{fig:visual_alt200m}
\vspace{-4mm}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[trim=0 130 0 0, clip,width=0.8\textwidth]{figs/visusupp.pdf}
\caption{More examples of captions generated by our LEMON model in a zero-shot manner (\textbf{Z}) or after finetuning on COCO (\textbf{F}). The notation is the same as described in Figure \ref{fig:visual}.}
\label{fig:visualsupp}
\vspace{-4mm}
\end{figure*}