
\documentclass[10pt]{article} \usepackage[preprint]{tmlr}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{cuted}
\usepackage{multicol}
\usepackage{tabularx}
\usepackage{booktabs, arydshln}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{wrapfig}


\title{BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks}


\RequirePackage{authblk}
\setcounter{Maxaffil}{0}
\setlength{\affilsep}{0.2em}
\renewcommand\Authfont{\normalfont\bfseries\fontsize{9}{13}\selectfont}
\renewcommand\Affilfont{\normalfont\fontsize{8}{10}\selectfont}
\makeatletter
\renewcommand\AB@affilsepx{, \protect\Affilfont}
\makeatother
\renewcommand\Authands{ and }

\author[1,$\dagger$]{Kai Zhang}
\author[1]{Jun~Yu}
\author[1]{Zhiling~Yan}
\author[1]{Yixin~Liu}
\author[1]{Eashan~Adhikarla}
\author[2]{Sunyang~Fu}
\author[3]{Xun~Chen}
\author[4]{Chen~Chen}
\author[5]{Yuyin~Zhou}
\author[6]{Xiang~Li}
\author[1]{Lifang~He}
\author[1]{Brian~D. Davison}
\author[6]{Quanzheng~Li}
\author[7]{Yong~Chen}
\author[2]{Hongfang~Liu}
\author[1,$\dagger$]{Lichao~Sun}



\affil[1]{Lehigh University}
\affil[2]{Mayo Clinic}
\affil[3]{Samsung Research America}
\affil[4]{University of Central Florida}
\affil[5]{University of California, Santa Cruz}
\affil[6]{Massachusetts General Hospital and Harvard Medical School}
\affil[7]{University of Pennsylvania}




\affil[$\dagger$]{Corresponding authors \hskip 1.5cm \textit{\{kaz321, lis221\}@lehigh.edu}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{11}  \def\year{2022} \def\openreview{\url{https://openreview.net/forum?id=1ikK0kHjvj}} 


\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{xcolor}
\usepackage{subcaption,booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{listings}
\usepackage{array}
\usepackage{algpseudocode}
\usepackage{tocloft}
\usepackage{etoc}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\usepackage{float}
\usepackage{graphicx}
\usepackage[justification=raggedright,font=small]{caption}
\usepackage{makecell}





\graphicspath{{fig/}}

\begin{document}

\maketitle

\begin{abstract}
In this paper, we introduce a unified and generalist \textbf{\underline{Biomed}}ical \textbf{\underline{G}}enerative \textbf{\underline{P}}re-trained \textbf{\underline{T}}ransformer (\textbf{BiomedGPT}) model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks. Our experiments demonstrate that BiomedGPT delivers expansive and inclusive representations of biomedical data, outperforming the majority of preceding state-of-the-art models across five distinct tasks with 20 public datasets spanning over 15 unique biomedical modalities. Through the ablation study, we also showcase the efficacy of our multi-modal and multi-task pretraining approach in transferring knowledge to previously unseen data. Overall, our work presents a significant step forward in developing unified and generalist models for biomedicine, with far-reaching implications for improving healthcare outcomes.
\end{abstract}

\begin{figure*}[ht]
\vskip -0.35cm
    \includegraphics[width=\textwidth]{fig/overview.png}
    \caption{Illustration of the diverse range of tasks supported by BiomedGPT during pretraining and subsequent fine-tuning. During the pretraining phase, we employ prevalent unimodal strategies, including masked language modeling and masked image infilling, and multimodal techniques, such as visual question answering and captioning. Object detection is also incorporated into the pretraining to infuse locational data. Following pretraining, the enhanced model is leveraged for a suite of five downstream tasks, encompassing image classification and natural language inference, demonstrating its efficient utilization of data.}
\vskip -0.35cm
\end{figure*}

\section{Introduction}
\label{sec:intro}


In the rapidly evolving landscape of artificial intelligence (AI), transformer-based foundation models \citep{vaswani2017attention, dosovitskiy2020image, bommasani2021opportunities, zhou2023comprehensive} have emerged as a powerful tool for solving a wide range of biomedical challenges, e.g., radiograph analysis \citep{park2022self, zhou2022generalized}, biomedical text generation \citep{luo2022biogpt}, disease prediction \citep{rasmy2021med}, and cell type annotation \citep{yang2022scbert}. The prevailing paradigm for biomedical foundation models is the \textit{pretraining then fine-tuning}. Specifically, a model is first pre-trained on a large-scale dataset and then fine-tuned on downstream datasets, facilitating knowledge transfer from the source domain to the target domain \citep{niu2020decade}. Instead of supervised pretraining, which is constrained by the availability of labeled data, self-supervised approaches that can learn from vast amounts of data without explicit human labeling have gained widespread adoption \citep{balestriero2023cookbook, huang2023self, nadif2021unsupervised, krishnan2022self}. For example, BERT-derived \citep{devlin2018bert,rasmy2021med, lee2020biobert, gu2021domain, chakraborty2020biomedbert, alsentzer2019publicly} and GPT-derived \cite{radford2019language, luo2022biogpt, kraljevic2021medgpt} models have been extensively studied in biomedical natural language processing and gained improved performance over prior methods. In biomedical imaging analysis, the Vision Transformer (ViT) \citep{dosovitskiy2020image} is regarded as the pretraining backbone for various tasks, including image segmentation, detection, classification, and synthesis, and also achieves promising performance \citep{shamshad2023transformers, valanarasu2021medical, kong2021breaking, manzari2023medvit}. 

In recent years, the increasing availability of biomedical data has set the stage for the development of multimodal AI solutions that capture the intricacies of human health and disease \citep{acosta2022multimodal}. Given biomedical data's complexity and high dimensionality, most efforts focus on vision-language pretraining instead of omni-modal fusion \citep{selivanov2023medical, chambon2022adapting}. To enable multimodal models to effectively understand both images and textual contexts, as well as to infer the associations between them accurately, researchers typically pre-train visual/textual embedders and cross-modal modules using image-text matching and masked language modeling objectives on images and their corresponding descriptions \citep{kim2021vilt, li2020comparison, delbrouck2022vilmedic, yan2022clinical, khare2021mmbert, chen2022multi}. The CLIP architecture and its underlying contrastive pretraining \citep{radford2021learning, jia2021scaling}, which aims to match paired image and caption embeddings while pushing others apart for improved representation transferability, has also been applied in biomedical AI, yielding acceptable zero-shot performance \citep{zhang2022contrastive, huang2021gloria, wang-etal-2022-medclip, eslami2023pubmedclip, zhang2023large}.

However, due to the limited volume and modalities in existing labeled biomedical datasets, previous works have primarily focused on either task/domain-specific or modality-specific applications, significantly restricting their practical utility\footnote{In this work, we build a hierarchy considering tasks, domains, and modalities. For instance, in the early-stage Alzheimer's prediction task, the task is general and requires domain-specific inputs, such as radiological imaging, which can be further specified into diverse modalities, including computed tomography (CT), magnetic resonance imaging (MRI) and others.}. Considering the disease classification task, the International Classification of Diseases, tenth Revision (ICD-10) currently covers approximately 69,832 diagnosis codes\footnote{\url{https://www.cdc.gov/nchs/icd/icd10.htm}}, thus it is impractical and uneconomical to develop different models for each disease. Besides, such a specialized schema deviates from the demand of drawing a comprehensive picture in healthcare~\citep{lindberg1993unified} as many seemingly irrelevant diseases or symptoms coexist and interact, e.g., diabetes is becoming the leading cause of new cases of blindness among adults~\citep{fong2004retinopathy}. Researchers typically tackle this issue from the perspective of multitasking~\citep{peng2020empirical} and transferring~\citep{raghu2019transfusion}. However, these practices compel the available datasets to obey strong assumptions like homogeneous structures or overlapping distributions. 


Recent breakthroughs have produced a new class of unified and generalist AI models capable of performing diverse tasks with a unified architecture and shared parameters ~\citep{alayrac2022flamingo, li2022uni, wang2022ofa, wang2022git, lu2022unified, chen2022unified, reed2022generalist, gupta2022towards, chen2023towards}. This remarkable advancement holds significant potential for the field of biomedicine, as it eliminates the need for specific models in specialized domains (inductive biases) by employing large-scale global-attention-based transformers ~\citep{vaswani2017attention, yun2019graph, dosovitskiy2020image}. Inspired by OFA~\citep{wang2022ofa}, we propose BiomedGPT, a unified and generalist model designed for handling various types of data through straightforward serialization integrated with task-oriented prompts. Specifically, BiomedGPT embeds data from diverse input types within a common, multimodal vocabulary that can be applied across all tasks. This model utilizes a unified sequence-to-sequence abstraction for both the pretraining and fine-tuning stages. In addition, we infuse the task instructions directly into the input as plain text, obviating the need for extra parameters. This architectural design fosters efficient task performance, offering a seamless process regardless of the data modality or task. We pre-train and fine-tune BiomedGPT with a variety of biomedical datasets and tasks. Through comprehensive experiments, we demonstrate that BiomedGPT can effectively transfer knowledge across tasks and even compete with specialist models trained on single-domain or single-modality datasets. This is particularly evident in vision-language tasks, such as image captioning and visual question answering, where BiomedGPT achieves new state-of-the-art (SOTA) performance. 

By harnessing the power of our generalist biomedical model to analyze complex data, researchers can unlock a wealth of insights and advance our understanding of the biological mechanisms underlying human health and disease, paving the way for exciting new possibilities in diagnosing, treating, and preventing diseases. Our contributions are summarized as follows:

\begin{itemize}[noitemsep]
    \item We present BiomedGPT, which, to our knowledge, is the first generalist AI model for biomedicine capable of accommodating various modalities, such as CT images and clinical notes, among others. It demonstrates an impressive performance across various downstream tasks, including a vision-only task, two language-only tasks, and two vision-language tasks.
    \item BiomedGPT is designed to encompass a wide range of domains in biomedicine. Our experimental results set a new benchmark, illustrating the feasibility of pretraining across diverse biomedical fields such as pathology, radiology, and academic literature. This is coupled with an ability to handle various body parts across different modalities.
    \item We conduct extensive studies to underline the efficacy and limitations of BiomedGPT. The insights gleaned from our study, summarized in Section \ref{sec:discussion}, stand to contribute significantly to the development of future iterations of generalist biomedical AI models for the wider research community.
    \item We commit to open-source practices by providing access to our codes\footnote{\url{https://github.com/taokz/BiomedGPT}}. This includes all processes, such as data preprocessing, pretraining, and fine-tuning, to ensure reproducibility and encourage further development.
\end{itemize}




\section{BiomedGPT Pipeline}


\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{fig/architecture.png}
    \caption{Illustration of the BiomedGPT model. This showcases two examples of pretraining through image infilling using a masked image and through PrefixLM \citep{wangsimvlm} using an image-text pair. For text-only corpora, we can easily exclude the image patches and use only textual tokens. For MLM pretraining, we will mask partial tokens in the text input.}
    \label{fig:arch}
    \vskip -0.4cm
\end{figure*}

Our proposed BiomedGPT is a transformer-based architecture specifically designed for the biomedical field, built upon the success of existing unified models for general data. We adhere to the fundamental principles of a generalist model: 1) Modality-Agnostic, 2) Task-Agnostic, and 3) Modality and Task Comprehensiveness. By discretizing data into patches or tokens, we achieve input/output unification using ideas from Vision Transformer (ViT) ~\citep{dosovitskiy2020image} and Language Models ~\citep{lewis-etal-2020-bart}. We pre-train our model on a diverse set of biomedical modalities and tasks to enhance its transferability. Our encoder-decoder architecture maps multi-modal data with task-related instructions into a common representation space, which helps to address discrepancies among biomedical modalities. Figure~\ref{fig:arch} showcases a snapshot of the BiomedGPT training process, highlighting its comprehensive and generalist nature.

\subsection{Architecture Selection}
There are three mainstream architectures of the pre-trained foundation models (PFMs) \citep{sarrouti2022comparing, fu2023decoder}: 1) encoder-only, 2) decoder-only, and 3) encoder-decoder. 



The encoder-only models use only the encoder of a transformer, which focuses on learning useful representations of the inputs. The most popular model in the encoder-only family is BERT \citep{devlin2018bert}, which has several variants \citep{lan2019albert, sanh2019distilbert, liu2019roberta} and subsequent studies \citep{Clark2020ELECTRA, dosovitskiy2020image}. To perform diverse tasks, extra learnable modules, such as classification head and task-specific decoder, are required in fine-tuning. This means that encoder-only models may not be able to align the inputs and outputs of qualitatively different modalities and cannot conduct complicated zero-shot prediction or generation. On the other hand, the decoder-only models, such as OpenAI's GPTs \citep{radford2018improving, radford2019language, brown2020language} use only the decoder of a Transformer. Although a decoder-only model could do multi-tasking in a unified manner, it typically requires pre-existing representations or encodings of the input data to function. However, we have not seen an encoder that is sufficiently strong to encode all biomedical modalities. Besides, the decoder-only approach lacks the ability to learn joint representations across modalities and tasks, as it is focused solely on generating output based on a fixed input encoding. This can result in reduced model flexibility and suboptimal performance when faced with novel or complex tasks. Thus, we propose the adoption of an encoder-decoder architecture for BiomedGPT due to its exceptional ability to effectively map various modalities into a unified semantic representation space while successfully handling a wide range of diverse tasks.

We follow OFA \citep{wang2022ofa} to design BiomedGPT, which takes BART \citep{lewis2019bart} as the backbone that is implemented as a sequence-to-sequence model with a BERT-style encoder over corrupted text and a GPT-style left-to-right autoregressive decoder. We make a few architectural changes to adapt the BART architecture for BiomedGPT. First, to improve the convergence efficiency and stability in the pretraining, we add three normalization operations to each layer: a post-attention Layer Norm (LN) \citep{ba2016layer}, post-first-FFN LN, and head-wise scaling within self-attention, following \citep{shleifer2021normformer}. To encode positional information, we incorporate two sets of absolute position embeddings for both text and images. Rather than merely combining these embeddings with token and patch embeddings, we implement a decoupling method to separate position correlation \citep{kitaev2018constituency, kerethinking}. Furthermore, we also incorporate 1D relative position bias for text and 2D relative position bias for image, as described in previous works \citep{raffel2020exploring, dai2021coatnet, wangsimvlm}.



\subsection{Input/Output Unification} \label{sec:unified_io}
To enable inputs with a wide range of modalities, including images, language, and bounding boxes, to be processed within a single model, it is necessary to embed them in a shared and unified space. For visual inputs, we directly apply CNN backbones to relax the heavy image feature extraction process, including object detection, following \citep{kim2021vilt}. Specifically, BiomedGPT receives the raw image $\mathbf{x}_v \in \mathbb{R}^{H \times W \times C}$ and maps it into a flattened 1D sequence of patches $\mathbf{x}_p \in \mathbb{R}^{N \times D}$ via a ResNet module as input for the transformer, where $N = \frac{HW}{P^2}$ is the number of patches given the patch size of $P \times P$, and $D$ is the fixed hidden size of the transformer layers. We choose the first three blocks of ResNet layers due to their observed advantages over 1x1 convolution or naive linear projection used in ViT \citep{wangsimvlm}. For linguistic inputs, we used byte-pair encoding (BPE) \citep{sennrich2016neural} to perform the subword tokenization. The subwords are then embedded into the input features.

To handle diverse modalities without relying on task-specific output structures, we represent them with tokens drawn from a unified and finite vocabulary. To achieve this, we utilize the frozen image quantization \citep{van2017neural, esser2021taming} and object descriptor \citep{chen2022pixseq, chen2022unified} to discretize the images and objects on the target side, respectively. As to the text outputs, such as object labels and summarizations, we represent them with BPE tokens. To be more specific, the image with $256 \times 256$ resolution is sparsely encoded into a sequence of $16 \times 16$, which is strongly correlated with the corresponding patch \citep{bao2021beit} and can effectively reduce the sequence length of the image representation. The bounding boxes of objects in an image are expressed as sequences of location tokens in the format of integers. We hereby build a unified vocabulary for all tokens of multi-modal outputs.

\begin{table*}[htbp]
\centering
\caption{Statistics of datasets for pretraining. ``\#Image'' represents the total number of distinct images, and ``\#Sample'' represents the number of training samples (e.g., the image-caption pair).}
\label{tab:pretraining}
\scalebox{0.75}{
\begin{tabular}{cccccc}
\toprule
Type & Pretraining & Source & Domain / Modality & \#Images & \#Sample \\ \midrule
\multirow{5}{*}{Vision \&} & \multirow{3}{*}{Captioning} & MedICat & Radiology, histology, scope procedures, others & 217,060  & 217,060  \\
\multirow{5}{*}{Language} & & IU X-ray & Chest x-ray & 7,470 & 7,470\\
&& Peir Gross & Pathology / clinical photographs & 7.442 & 7.442\\
\cdashline{2-6} 
 & \multirow{2}{*}{VQA} & SLAKE & Radiology (head, neck, chest, abdomen, pelvic cavity) & 642 & 7,033 (EN) \\ 
 & & PathVQA & The entire domain of pathology \citep{he2020pathvqa} & 4,998 & 32,799 \\ \midrule
\multirow{6}{*}{Vision} & \multirow{2}{*}{Detection} & DeepLesion & CT (lung nodules, liver tumors, lymph nodes, etc) &32,120 & 32,735 \\
& & OIA-DDR & Fundus cameras & 755 & 13,673 \\
\cdashline{2-6} 
 & \multirow{4}{*}{Image Filling} & CheXpert & Chest radiograph & 224,315  &-  \\ 
 & & CytoImageNet & Microscopy & 890K &- \\
 & & ISIC (2020) & Dermoscopy & 33,126 &- \\
 & & Retinal Fundus & Ophthalmology & 5,126 &- \\
 \midrule
\multirow{3}{*}{Language} & \multirow{3}{*}{MLM} & PubMed Abstracts & Biomedcial articles &-  & 181 M \\ 
 & &  NCBI BioNLP  & Chemicals annotations, biomedical articles & - & 52,976 \\
 & & MIMIC-III Clinic Notes & Medical records &- &1.8 M \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Natural Language as a Task Manager}

Multitasking is a key attribute of a unified and generalist model. Following previous literature on language models using prompt / instruction learning \citep{brown2020language, liu2023pre, weifinetuned, gao2021making, schick2021s}, and the existing unified frameworks to eliminate task-specific modules, we specify each task with a handcrafted instruction excluding some tasks like Visual Question Answering (VQA), which are fully specified by their text inputs. BiomedGPT supports abstractions of several tasks, including vision-only, text-only, and vision-language, to achieve task comprehensiveness. We provide details of the pretraining tasks, fine-tuning / inference tasks, as well as their corresponding instructions in the following. 
\paragraph{Pretraining Tasks.} We consider two vision-only tasks in the pretraining: for masked image modeling (MIM) as well as image infilling, we borrow the idea of blockwise masking \citep{DBLP:conf/iclr/Bao0PW22} and let the model recover the masked patches in the middle part by generating the corresponding codes. The corresponding instruction is \textit{``What is the image in the middle part?''}. For object detection (OD), the model learns to generate the bounding box of an object with the instruction of \textit{``What are the objects in the image?''}. As to the text-only task, we adopt the commonly-used masked language modeling (MLM) while the instruction is \textit{``What is the complete text of `A case of oral $<$mask$>$ anaphylaxis' ?''}. Two types of multi-modal tasks are selected, including image captioning with the instruction of \textit{``What does the image describe?''} and VQA with the instruction of \textit{``\{\textbf{Question}\}''}. The addition of OD for pretraining BiomedGPT serves to enhance visual learning inspired by \citep{xu2021e2e}.
\paragraph{Fine-tuning and Inference Tasks.} Besides image captioning and VQA used in pretraining, we cover one more vision-only task and two more text-only tasks. Specifically, we use the instruction \textit{``What does the image describe?''} to differentiate image classification. \textit{``What is the summary of text `\{\textbf{Text}\}'?''} and \textit{``Can text1 `\{\textbf{Text1}\}' imply text2 `\{\textbf{Text2}\}'?''} are exploited for text summarization and natural language inference, respectively. 

\subsection{Gnerative Pretraining via Seq2seq} \label{sec:autoreg}
Autoregressive or sequence-to-sequence (seq2seq) is widely used in sequential modeling \citep{sutskever2014sequence, cho-etal-2014-learning, oord2016wavenet}, such as large language modeling \cite{lewis2019bart, raffel2020exploring}. Formally, suppose we are given a sequence of tokens $\mathbf{x}_{i,b}$ as input, where $i=1,\cdots, I$ indexes the tokens in a data sample and $b=1,\cdots, B$ indexes a sample in a training batch. BiomedGPT's architecture is parametrized by $\theta$. Then we autoregressively train the model via the chain rule as follows:
\begin{align}
    \mathcal{L}_\theta(\mathbf{x}_{1,1},\cdots,\mathbf{x}_{i,b}) &= -\sum_{b=1}^B\log\prod_{i=1}^{I}p_{\theta}(\mathbf{x}_{i,b}|\mathbf{x}_{1,b},\cdots,\mathbf{x}_{i-1,b})\nonumber =-\sum_{b=1}^B\sum_{i=1}^{I}\log p_{\theta}(\mathbf{x}_{i,b}|\mathbf{x}_{<i,b})
\end{align}

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{fig/trie_search.png}
  \end{center}
  \caption{The process example of trie-based beam search (beam size: 3, the maximum length of an output sequence: 4). Such search strategy can boost both the effectiveness and efficiency of BiomedGPT in various downstream tasks.} 
  \label{fig:trie}
\end{wrapfigure}
In the context of BiomedGPT, $\mathbf{x}$ could refer to both linguistic and visual tokens in the pretraining tasks, including subwords, image codes, and location tokens, as we mentioned in Section \ref{sec:unified_io}. Specifically, subwords are extracted by a BPE tokenizer, and we mask $30\%$ of the tokens of the subwords in input in the MLM task as these medical words show relatively high overlapping degrees. For the OD task, location tokens are generated with Pix2Seq \citep{chen2022pixseq} conditioned on the observed pixel inputs. We need data preprocessing for quantizing biomedical images using VQ-GAN \citep{esser2021taming} because they are surrounded by trivial semantics, e.g., black background and the unmet input size. Therefore, we first remove the trivial background and crop the image to the bounding box of the object of interest, then resize the cropped image to be $256 \times 256$, and feed the center part with $128 \times 128$ resolution into the pre-trained VQ-GAN to generate the corresponding sparse image codes, which are the target output in MIM task. Vision-language tasks follow the same tokenization flow. Note that for fine-tuning, we also apply seq2seq learning but with different datasets and tasks, as shown in Table \ref{tab:pretraining} and Table \ref{tab:experiments}. More implementation details are described in Section \ref{sec:implementation}. 

\begin{table*}
\centering
\caption{Detailed model configuration of BiomedGPT. During the pretraining phase, image processing involves resizing and cropping the images to varying resolutions, corresponding to the input sizes listed in the table. It should be noted that during fine-tuning and inference stages, the input resolution of BiomedGPT can be flexibly adjusted according to the specific requirements of the task.}
\label{tab:scaling}
\scalebox{0.85}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2.3}{*}{Model Scale}
 & \multirow{2.3}{*}{\#Params.} & \multicolumn{2}{c}{Image Projection} & \multicolumn{2}{c}{Representation (Size)} & \multicolumn{3}{c}{Backbone (\#)}\\ 
 \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-9}
 & & \small{Input Size} & \small{Base Model} & \small{Emb.} & \small{Hidden} & \small{Att. Head} & \small{Enc. Layer} & \small{Dec. Layer}\\
 \midrule
$\text{BiomedGPT}_{\text{Small}}$ & 33M & $256\times256$ & ResNet50 & 256 & 1024 & 4 & 4 & 4 \\
\midrule
$\text{BiomedGPT}_{\text{Medium}}$ & 93M & $256\times256$ & ResNet101 & 512 & 2048 & 8 & 4 & 4 \\
\midrule
$\text{BiomedGPT}_{\text{Base}}$ & 182M & $384\times384$ & ResNet101 & 768 & 3072 & 12 & 6 & 6 \\
\bottomrule
\end{tabular}
}
\end{table*}



\subsection{Autoregressive Inference} 
Inference in large language models often relies on decoding strategies like beam search to improve generation quality. However, such an approach poses challenges for classification tasks, including unnecessary optimization of the entire vocabulary and the possibility of generating invalid labels beyond the closed label set. To tackle these issues, we apply a beam search strategy that incorporates a prefix tree (also known as a trie), limiting the number of candidate tokens and resulting in more efficient and accurate decoding. Figure \ref{fig:trie} demonstrates an example of trie-based beam search; along the path across ``Lipid'' and ``breakdown'', BiomedGPT sets logits for all invalid tokens (``mechanism'' and ``pathway'') to $-\infty$ when computing log-probabilities for the target token ``in''. It is worth noting that trie-based search is also applied during the validation phase of the fine-tuning stage for acceleration (approximately $16\times$ increase in speed in our experiments).


\begin{table*}[]
\centering
\caption{Experimental results. The results of OrganMNIST consist of OrganAMNIST, OrganCMNIST and OrganSMNIST that correspond to axial / coronal / sagittal central slices. We only present state-of-the-art approaches if they provide open-sourced codes for guaranteed reproducibility. We did not re-run the methods but relied on the metrics reported in their original work, e.g., the SOTA results on MedMNIST v2 come from \citep{medmnistv2}. Due to the absence of severe class imbalance on the selected downstream datasets, accuracy is appropriate to be used.}
\label{tab:experiments}
\scalebox{0.65}{
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Task} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{Domain / Modality} & \multirow{2}{*}{Metric} & \multicolumn{2}{c}{SOTA} & \multicolumn{3}{c}{BiomedGPT} \\ 
\cmidrule(lr){5-6}\cmidrule(lr){7-9}
&&&& Model & Result & Small & Medium & Base \\
\midrule
\multirow{10}{*}{\begin{tabular}[c]{@{}c@{}}Image \\ Classification\end{tabular}} & PathMNIST & Colon Pathology & Accuracy & ResNet-50 (28) & 91.1 & 89.4 & 92.1 & \textbf{92.6} &  \\
 & DermaMNIST & Dermatoscope & Accuracy & Google AutoML & 76.8 & 75.2 & 78.0 & \textbf{78.6} & \\
 & OCTMNIST & Retinal OCT & Accuracy & ResNet-50 (224) & 77.6 & 79.5 & \textbf{81.9} & 81.6 & \\
 & PneumoniaMNIST & Chest X-Ray & Accuracy & Google AutoML & 94.6 & 91.8 & 93.4 & \textbf{96.7} & \\
 & RetinaMNIST & Fundus Camera & Accuracy & Google AutoML & \textbf{53.1} & 51.8 & 52.0 & 51.8 & \\
 & BreastMNIST & Breast Ultrasound & Accuracy & ResNet-18 (28) & 86.3 & 84.6 & 87.8 & \textbf{87.8} & \\
 & BloodMNIST & Blood Cell Microscope & Accuracy & Google AutoML & 96.6 & 94.2 & 97.2 & \textbf{97.7} & \\
 & OrganAMNIST & Abdominal CT & Accuracy &  ResNet-18 (224) & 95.1 & 92.6 & 94.7 & \textbf{95.2} & \\ 
 & OrganCMNIST & Abdominal CT & Accuracy & ResNet-18 (224) & 92.0 & 92.2 & 92.3 & \textbf{93.1} & \\ 
 & OrganSMNIST & Abdominal CT & Accuracy  & ResNet-18 (224) & 81.3 & 80.0 & 82.0 & \textbf{82.3} & \\ \midrule \midrule 
\begin{tabular}[c]{@{}c@{}}Text Understanding\end{tabular} & MedNLI & Clinical Notes & Accuracy & SciFive \citep{phan2021scifive} & \textbf{86.5} & 70.5 & 74.8 &  78.6 &   \\  
\cdashline{1-10} 
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Text Summarization\end{tabular}} 
& MeQSum & Medical Questions & ROUGE\_L & BioBART-L \citep{yuan2022biobart} & \textbf{53.2} & 42.2 & 51.3 & 52.3 & \\
& iCliniq & Doctor Consultation Clinic & ROUGE-L & BART-B \citep{yuan2022biobart} & \textbf{59.7} & 55.2 & 57.2 & 56.2 & \\
& HealthCareMagic & Doctor Consultation Clinic & ROUGE-L & BART-L \citep{yuan2022biobart} & \textbf{44.7} & 39.8 & 41.9 & 42.0 & \\
\midrule \midrule
\multirow{9}{*}{\begin{tabular}[c]{@{}c@{}}Image \\ Captioning\end{tabular}} & \multirow{3}{*}{IU X-ray} & \multirow{3}{*}{Chest X-Ray} 
& METEOR & \multirow{3}{*}{PPKED \citep{liu2021exploring}} & - & 11.0 & 11.0 & \textbf{14.6} & \\ 
 &  &  & ROUGE-L & & \textbf{37.6} & 26.8 & 28.0 & 30.2 & \\
&  &  & CIDEr & & 35.1 & 29.6 & 31.3 & \textbf{36.0} &\\ \cdashline{2-10} 
& \multirow{3}{*}{Peir Gross} & \multirow{3}{*}{Digital Camera} 
& METEOR & \multirow{3}{*}{CoAttention \citep{jing2018automatic}} & 14.9 & 12.0 & 14.7 & \textbf{15.4} & \\
 &  &  & ROUGE-L & & 27.9 & 25.8 & 24.0 & \textbf{36.0} & \\
&  &  & CIDEr & & 32.9 & 22.0 & 25.8 & \textbf{122.7} &\\ \cdashline{2-10} 
& \multirow{3}{*}{ROCO} & \multirow{3}{*}{Radiology} 
& METEOR & - & - & 6.2 & 7.0 & 7.8 & \\
&  &  & ROUGE-L & - & - & 16.4 & 17.0 & 18.2 & \\ 
 &  &  & CIDEr & - & - & 13.2 & 17.6 & 24.2 &\\  \midrule \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Visual Question \\ Answering\end{tabular}} & SLAKE (EN) & Radiology  & Accuracy & VGG+SAN \citep{liu2021slake} & 75.4 & 68.9 & 71.5 & \textbf{81.9} &  \\
 & PathVQA & Pathology & Accuracy & MMQ \citep{do2021multiple} & 48.8 & 48.9 & 49.0 & \textbf{57.9} &  \\
 & VQA-RAD & Radiology & Accuracy & PubMedCLIP \citep{eslami2021does} & \textbf{72.1} & 40.3 & 69.4 & 71.6 &  \\ 
 \bottomrule
\end{tabular}
}
\end{table*}

\section{Experiments}


In this section, we showcase the experimental design and implementation details of BiomedGPT, along with its superior performance compared to previous state-of-the-art methods across various downstream tasks and datasets. We deliberately select data from different domains to show the promising generalization of our method.

\subsection{Implementation Details} \label{sec:implementation}
The total vocabulary size is 59457, with 50265 language tokens, 1000 location tokens, and 8192 vision tokens. The number of vision tokens is determined by the variant of the pre-trained VQ-GAN models used in the BiomedGPT, specifically, the OpenImages\citep{kuznetsova2020open}-trained VQ-GAN with patch size of 8 and vocabulary size of 8192 using the Gumbel softmax \citep{jangcategorical2017, maddisonconcrete2017} quantization. During training, we randomly subsample 196 image patches for pretraining. The truncation to max model input length is set as 512.

To pretrain our BiomedGPT, we use the AdamW \citep{loshchilovdecoupled} optimizer with hyperparameters $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon = 1\mathrm{e}{-8}$. The peak learning rate is set to $1\mathrm{e}{-4}$, and we apply a linear decay scheduler with a warmup ratio of 0.01 to control the learning rate. For regularization, we set dropout to 0.1 and use a weight decay of 0.01. To enhance the training process, we use stochastic depth with a rate of 0.1 applied to the encoder and decoder, except for convolution blocks. Furthermore, we employ a diversified approach in mixing all pretraining data within each batch. This includes an assortment of multi-modal, text-only, vision-only, and OD samples. The ratio applied is 8:2:1:1, which emphasizes learning and enhancing the interaction between vision and language. The models are pre-trained with 10 Nvidia A5000 GPUs and mixed precision, except for the models used for fine-tuning downstream tasks. The experimental settings, dependent on the task and data used, are described in detail in Appendix \ref{appx:hyperparam}.

In order to investigate the performance of BiomedGPT for tasks at different scales, we explicitly design three scaling models, i.e., $\text{BiomedGPT}_{\text{Small}}$, $\text{BiomedGPT}_{\text{Medium}}$,
and $\text{BiomedGPT}_{\text{Base}}$. The configurations for each model are detailed in Table~\ref{tab:scaling}. It can be observed, from the results in Table ~\ref{tab:experiments}, that in most cases, larger models tend to perform better. However, in practical deployment, scaling models for better performance may not be economical or parameter-efficient in some datasets. For instance, the experimental results on BreastMNIST demonstrate that the medium-size model shows only a 0.2\% accuracy improvement compared to the small model, but it requires approximately 3$\times$ more parameters. 

\subsection{Results on Unimodal Datasets}
We select three unimodal tasks over 14 datasets, as shown in Table \ref{tab:experiments}. For the image classification task, results show the classification accuracy on MedMNIST v2 \cite{medmnistv2} (a set of benchmark datasets) covering several biomedical domains. Our $\text{BiomedGPT}_{\text{Base}}$ model achieves state-of-the-art accuracy on 9 out of 10 image-only datasets. Although we are behind the state-of-the-art by a small margin on RetinaMNIST, it is important to note that the dataset was initially designed for a regression task, and thereby the  performance gap could be expected.

We further investigate the model performance on text-only datasets. Our results show that for the natural language inference task with the MedNLI dataset, our model achieved an accuracy of 78.6\%, which is lower than the state-of-the-art (SOTA) result of 86.5\%. For text summarization, we focused on doctor-patient conversations and aimed to summarize the patient's medical questions. We used the ROUGE-L metric to measure performance. However, similar to the results in MedNLI, our model does not achieve satisfactory performance. There are several potential reasons for the difference in performance observed between our model and previous state-of-the-art models. First, our model has a constrained scale with fewer parameters than the SOTA models. Specifically, SciFive is based on T5-Large \citep{raffel2020exploring} with 770 million parameters, and BioBart-Large has nearly 400 million parameters, while our largest model has only 182 million parameters. Second, our corpus scale is also constrained compared to SciFive, which processed a larger-scale corpus, including Colossal Clean Crawled Corpus (C4) \citep{raffel2020exploring}, PubMed Abstract\footnote{\url{https://pubmed.ncbi.nlm.nih.gov}}, and PubMed Central (PMC)\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc}}. Third, divergent data impact may have played a role. Our model was trained on a corpus that covers both biomedical articles and clinical notes, with the goal of building a unified and comprehensive model. However, it has been reported that models pre-trained on clinical notes can perform poorly on language tasks based on biomedical articles, and vice versa \citep{gu2021domain, alsentzer-etal-2019-publicly, lehman2023we}. Addressing the substantial differences between text modalities is an open question that requires further investigation to improve the transferability of biomedical language models.

\subsection{Results on Multimodal Datasets}
BiomedGPT aims to tackle two multimodal tasks: image captioning and visual question answering, for which we have chosen three corresponding datasets each, as presented in Table \ref{tab:experiments}. Specifically, we use SLAKE (English text only), PathVQA, VQA-RAD for VQA, IU X-ray, PEIR Gross, and ROCO for image captioning. Due to the limited availability of public multimodal biomedical datasets, we only pre-train our model on the training sets of some datasets and evaluate our prediction capability on the unseen testing sets. 

For evaluating image captioning, we employ three metrics, namely METEOR, ROUGE-L, and CIDEr, for a comprehensive comparison. During inference, we select the checkpoint with the highest CIDEr score obtained during fine-tuning. Our approach outperforms state-of-the-art methods in terms of CIDEr, particularly on the Peir Gross dataset, with a remarkable improvement of 273\%. Although BiomedGPT falls short of PPKED regarding ROUGE-L on the IU X-ray dataset, it is crucial to note that this can be attributed to our model selection based on CIDEr rather than ROUGE-L during the validation phase. These two evaluation metrics accentuate different aspects of performance, as comprehensively discussed in Appendix \ref{sec:eval_metic}. Moreover, to our knowledge, no previous work is reported on the ROCO dataset; hence, we report our results without comparison.

To evaluate the performance of VQA, we report the overall accuracy, which includes closed- and open-ended question-answer pairs. Our proposed model, BiomedGPT, achieves a significant improvement over previous state-of-the-art models on SLAKE (EN) and PathVQA datasets. We also observe that the performance improvement has not reached a plateau, and we believe that we can further improve the accuracy by increasing the number of fine-tuning epochs. Besides, BiomedGPT obtain comparable predictions on the VQA-RAD dataset. In our case analysis illustrated in Fig. \ref{fig:vqa_compare}, we observe that BiomedGPT often generates semantically correct outputs. An example is the middle image with the question indexed by (3), where ``just one'' is correctly inferred. However, the current evaluation framework counts only absolute matches, which may not fully reflect the model's capability to produce semantically accurate answers.

\begin{figure*}[]
    \centering
    \includegraphics[width=1\textwidth]{fig/vqa_compare.png}
    \caption{Examples from VQA-RAD with PubMedCLIP and our BiomedGPT. It is worth mentioning that, similar to PubMedCLIP, previous studies \citep{nguyen2019overcoming, zhan2020medical} have also struggled to correctly interpret the question, resulting in irrelevant answers for the first three samples.}
    \label{fig:vqa_compare}
\end{figure*}

\subsection{Results on Intra- \& Inter-distribution Transfer}

We conducted intra- and inter-distribution inference experiments, performing zero-shot inference on seen and unseen datasets using our pre-trained checkpoints. These checkpoints were pre-trained on the training sets of SLAKE and PathVQA, and we evaluated their performance on the testing sets of those two datasets and VQA-RAD. Our results in Table \ref{tab:zero-shot} indicate that BiomedGPT achieves excellent performance, particularly when using a larger model, on SLAKE and PathVQA. However, pre-trained on general datasets like ImageNet, the baseline OFA model experienced substantial performance degradation on these two datasets. In comparison, OFA attains performance similar to BiomedGPT on the VQA-RAD dataset, but neither model excels compared to the post-fine-tuning outcomes presented in Table \ref{tab:experiments}. We hypothesize that the models may overfit the familiar intra-domain distribution and have difficulties handling out-of-distribution data. Upon examining the generated answers from BiomedGPT, we also observe its tendency to overfit intra-distribution instructions, leading to challenges in comprehending unseen questions and producing uncontrolled outputs. As an example, when presented with the 14th question \textbf{\textit{``Where is the pathology in this image?"}} from the VQA-RAD dataset, $\text{BiomedGPT}_{\text{Base}}$ predicts $<code\_{7948}> <code\_{5859}> <code\_{771}> \dots <code\_7077><code\_7077>$, indicating that the model had mistakenly interpreted the VQA task as an image generation task. Although both $\text{BiomedGPT}_{\text{Medisum/Small}}$ and $\text{OFA}_{\text{Base}}$ generate text-only answer ``\textit{No}", it still does not match the open-ended ground truth of ``Head''. Figure~\ref{fig:zero_shot} displays the open- and closed-ended accuracies in transfer learning on VQA-RAD data. We observed that models exhibit catastrophic performance on open-ended questions. Note that there are 251 out of 451 closed-ended QA pairs (either ``\textit{Yes}'' or ``\textit{No}'') in the VQA-RAD test set, with the remainder being open questions.

These observations shed light on the instruction-sensitivity challenge that arises in instruction-guided pretraining when building a unified and generalist biomedical model. To delve deeper into this issue, we present a case study in Appendix \ref{appx:hyperparam}. Additionally, our findings from the intra- and inter-distribution transfer learning experiments suggest that data diversity and scale are crucial factors in enhancing the generalist intelligence of BiomedGPT. However, due to the limited volume and diversity of existing biomedical datasets, as well as concerns regarding data privacy and imbalance, we plan to explore data augmentation and synthetic biomedical datasets in our future work.



\begin{table*}[htbp]
\centering
\caption{Intra- \& inter-distribution transfer performance on VQA tasks in terms of accuracy. The best results are highlighted with \textbf{BOLD} values.} \label{tab:zero-shot}
\begin{tabular}{llcccc}
\toprule
Dataset & Model & Small & Medium & Base & Large \\ \midrule
\multirow{2}{*}{SLAKE (EN)} & Ours & 46.18 & 71.62 & \textbf{72.66} & - \\
 & OFA & 1.89 & 1.04 & 1.79 & 1.60 \\ \cdashline{1-6} 
\multirow{2}{*}{PathVQA} & Ours & 33.74 & \textbf{48.22} & 45.66 & - \\
 & OFA & 21.08 & 24.66 & 28.98 & 29.30 \\
 \cdashline{1-6} 
\multirow{2}{*}{VQA-RAD} & Ours & 30.38 & 31.49 & 33.26 & - \\
 & OFA & 31.49 & \textbf{34.81} & 30.82 & 33.92 \\ \bottomrule
\end{tabular}
\end{table*}

\begin{wrapfigure}{r}{0.45\textwidth}
  \begin{center}
    \includegraphics[width=0.43\textwidth]{fig/zero_shot.png}
  \end{center}
  \vspace{-10pt}
  \caption{The zero-shot performance of pre-trained BiomedGPT and OFA with different model scales. Here, the model sizes are denoted by `L', `B', `M', and `S', which stand for large-, base-, medium-, and small-sized models, respectively.} 
  \label{fig:zero_shot}
  \vspace{-10pt}
\end{wrapfigure}

\subsection{Ablation Study on Pretraining Tasks} \label{sec:ablation_tasks}

In this section, we demonstrate the effectiveness of pretraining modules. Table \ref{tab:ablation} shows the results of the same setting for each model on the same data. It is important to note that the evaluation data used in the table was not seen during pretraining for a fair comparison. Overall, we observe that pretraining without using biomedical data (w/o PTB) will result in failure on multimodal tasks such as image captioning on ROCO and VQA on VQA-RAD. Additionally, we found that the absence of MLM and MIM will affect the performance of text-only and image-only tasks, respectively. Another interesting observation is that pre-trained OFA can achieve better performance on text-only datasets, MeQSum and NLI. This performance may be related to the fact that the text information is not ``professional'' enough and contains too much natural / general language. Furthermore, if we remove MIM, we observe better performance. This suggests that multi-modal pretraining may influence the unimodal tasks, especially text-only tasks, as the image is not necessarily required. In contrast, for image-only tasks, at least a dictionary of text tokens is needed for label generation.

\begin{table}
\centering
\caption{Ablation study on holding out task groups. All the results are obtained from the small-scale model.}
\label{tab:ablation}
\scalebox{0.95}{
    \begin{tabular}{cccccccccc}
    \toprule
    Model & Pneumonia & ROCO & VQA-RAD & MeQSum & MedNLI \\
    \midrule
    $\text{Ours}_{\text{Small}}$ & \textbf{91.8} & \textbf{13.2} & 37.5 & 42.2 & 69.3 \\
    \midrule
    w/o MLM & 87.0 & 12.0 & 32.4 & 19.1 & 68.6 \\
    w/o MIM & 88.3 & 12.2 & 33.5 & 44.3 & 69.9 \\
    w/o OD & 88.3 & 12.7 & \textbf{37.7} & 44.8 & 68.2 \\
    w/o PTB & 88.9 & 6.8 & 2.5 & \textbf{46.6} & \textbf{72.6} \\
    \bottomrule
    \end{tabular}
}
\end{table}

\subsection{Ablation Study on Pretraining Modalities} \label{sec:modality_main}

This section addresses the query: \textit{``Can the proposed model handle unseen data modalities (e.g., images from a new different imaging device like an ultrasound)?''} To investigate this, we have adjusted our dataset selection for both pretraining and downstream tasks. Specifically, we've drawn 3,489 and 6,461 chest X-Ray image-text pairs from SLAKE (EN) and IU X-ray datasets, respectively. Additionally, we selected an equal quantity of images (7,452) from CheXpert while disabling MLM and OD during pretraining for simplification.

\begin{wrapfigure}{r}{0.45\textwidth}
  \begin{center}
    \includegraphics[width=0.43\textwidth]{fig/modality_transfer.pdf}
  \end{center}
  \caption{The classification accuracy of BiomedGPT, fine-tuned with both seen (x-ray) and unseen (ultrasound and CT) modalities, is depicted in this comparison. Here, ResNet-50, trained from scratch according to the protocol in \citep{medmnistv2}, serves as a reference baseline.} 
  \label{fig:radiology_transfer}
\end{wrapfigure}

The pre-trained $\text{BiomedGPT}_{\text{Small}}$ on X-Ray modality is subsequently fine-tuned on chest x-ray (x-ray), breast ultrasound (breast) and liver CT (organc and organs) within the radiology field. The process consists of 500 update steps, and the resulting performance is depicted in Figure \ref{fig:radiology_transfer}. These outcomes underscore the impressive in-domain transferability of BiomedGPT. Furthermore, our findings indicate that BiomedGPT can perform well with out-of-domain modalities when the subjects of the medical imaging are anatomically adjacent, such as chest versus breast, in our experiment. We intentionally limit the update steps to observe the efficiency of transfer between pretraining and downstream modalities. While BiomedGPT may not have achieved superior accuracies on the Liver CT datasets, we nevertheless observe performance saturation with increasing training steps. This suggests that BiomedGPT is capable of delivering solid results with more differentiated biomedical modalities across different body parts, albeit requiring additional training steps. Additional results are detailed in Appendix \ref{sec:modality_additional}.




\section{Discussion} 
\label{sec:discussion}


\noindent \textbf{Main Findings.} In this study, we have shown that BiomedGPT can achieve competitive performance across various tasks, spanning vision, language, and multimodal domains. This is achieved by integrating a diverse range of biomedical modalities and tasks within a unified seq2seq pretraining framework. Our comprehensive experiments and ablation studies underscore the pivotal role of incorporating a wide variety of tasks and modalities in the construction of a generalist biomedical AI model. Notably, the inclusion of a broad spectrum of biomedical tasks and modalities in the pretraining phase significantly enhances the fine-tuning efficiency and ultimately bolsters the overall performance of the model. This improvement is attributed to the implicit interactions among these different factors. We noticed a fascinating observation in our studies: while OFA---a generalist model pre-trained with generic data---exhibits impressive zero-shot performance on VQA-RAD data as outlined in Table \ref{tab:zero-shot}, it encounters difficulty when attempting to align image-text pairs during the fine-tuning phase. This challenge is evident in Table \ref{tab:ablation}, where OFA's performance within a restricted number of epochs is distinctly low, achieving only 6.8 CIDEr and 2.6 accuracy for captioning and VQA tasks, respectively. This observation underscores the phenomenon that an effective zero-shot model does not necessarily translate into a superior starting point for fine-tuning tasks. BiomedGPT manages to surmount these limitations associated with multi-modal, multi-task pretraining. Furthermore, our exploration of the scaling laws proposed by Kaplan et al. \citep{kaplan2020scaling} reveals that enlarging the scale of the model leads to a considerable boost in performance. In conclusion, by expanding the scale of data, tasks, and the model, we foresee substantial enhancements in BiomedGPT's few-shot and zero-shot inference capabilities.


\noindent \textbf{Limitations and Suggestions.} Our extensive experiments have revealed several limitations of BiomedGPT. A primary concern is the model's sensitivity toward instructions. There are instances where the model fails to understand the instructions and makes catastrophic predictions, even producing irrelevant data types such as image codes for a VQA task. A straightforward solution could be to broaden the diversity of high-quality instruction sets during pretraining. Additionally, we must investigate methods for achieving a balance in data diversity. This encompasses aspects such as establishing suitable size ratios for data within different biomedical modalities in a batch and throughout the entire pretraining dataset and determining the optimal sequence for training with diverse inputs. Another potential avenue is to align BiomedGPT with human intent via reinforcement learning from human or AI feedback (RLF) \citep{ouyang2022training, bai2022constitutional, zhou2023comprehensive}, a strategy employed by the latest dialogue agents such as ChatGPT\footnote{\url{https://openai.com/blog/chatgpt}} and Claude\footnote{\url{https://www.anthropic.com/index/introducing-claude}}. However, creating specific biomedical RLF datasets would be expensive, given the extensive need for domain experts.

A further significant limitation arises from two specific inputs for text-only downstream tasks: the considerable difference between clinical notes and general-domain \& biomedical text \citep{lehman2023we, gu2021domain}, and the presence of vision-only tasks, which can impede the model's pattern extraction from the pure text during pretraining, as highlighted in our ablation study in Section \ref{sec:ablation_tasks}. Generating a representative vocabulary from all domains and increasing the ratio of text inputs during pretraining may help address these issues. However, this is a balancing act as it may influence vision-related tasks. 

Lastly, there is the issue of fine-tuning efficiency with respect to training speed and memory bottleneck, particularly as we aim to develop a large-scale generalist biomedical model. An emerging research direction that could address this is parameter-efficient fine-tuning (PEFT) \citep{ding2023parameter, liu2022few, lialin2023scaling}, which fine-tunes a small number of (extra) model parameters while keeping most parameters of the pre-trained models frozen. In our work, we attempt to apply prompt tuning but we do not receive the expected results; details are described in Appendix \ref{sec:prompt_tuning}.

\section{Conclusion}
We have presented BiomedGPT, a unified and generalist framework modeling multimodal tasks in medicine together, including radiographs, digital images, text, and bounding boxes. This task-, domain- and modality-agnostic model learns the universal comprehensiveness across different tasks and supports the unification of architecture. In addition, there's no requirement for further modifications to be specified during the finetuning phase, which can save valuable time and effort. The experiments conducted on approximately 20 public biomedical datasets validate that BiomedGPT can compete with previous SOTAs in recent years, which is exciting and provides the possibility of comprehensive representations used for versatile biomedical tasks. Our combined training approach has the potential to facilitate data-driven solutions to real-world problems in the biomedical domain. We further test zero-shot learning performance in domain/task transfer, verifying its effectiveness on biomedical tasks. Additionally, we conduct ablation studies in which we exclude certain task and modality groups. The objective is to examine the effects of various pretraining tasks and modalities on downstream performance. This area of study, currently open and ripe for further exploration, could provide valuable insights into the interactions between task and modality in the pretraining process. In the future, we hope to combine more meaningful tasks in medicine (e.g., segmentation, relation extraction) and more modalities into BiomedGPT and endeavor to understand the reason why universal representations can work well.




\newpage
\bibliography{main}
\bibliographystyle{tmlr}

\clearpage
\appendix 
\label{sec:appendix}

{\LARGE \bf \noindent Appendix}

\section{Hyperparameters and Sensitivity Analysis} \label{appx:hyperparam}

Sensitivity, particularly hyperparameter sensitivity and instruction/prompt sensitivity, poses a significant challenge for developing a unified model, not just for biomedical generalist AI. In the following, we use examples to discuss exploiting prior knowledge for selecting hyperparameters and instructions to achieve good solutions. 

\subsection{Hyperparameters for Fine-tuning} 
If not otherwise specified, we fine-tune BiomedGPT with 50 epochs and a learning rate of 7e-5 in terms of a batch size of 128, 64, and 32 for small-, medium-, and base-size models, respectively. The input image resolution is set to $256 \times 256$, dropout \citep{srivastava2014dropout} rate is set to 0.1and the other hyper-parameters remain the same as for pretraining. 

\vspace{5pt}
\noindent \textbf{Image Classification.} In our model configuration, we assign a label smoothing \citep{szegedy2016rethinking} ratio of 0.1. Adopting data augmentation strategies outlined in prior research \cite{wang2022ofa, bao2021beit}, we use random resize cropping, random flipping, RandAug \citep{cubuk2020randaugment}, and random erasing \citep{zhong2020random}. Furthermore, we incorporate Mixup \cite{zhang2018mixup} and CutMix \citep{yun2019cutmix} augmentations, each presenting a 50\% chance of being applied to every batch. We set the alpha parameters for Mixup and CutMix at 0.8 and 1.0, respectively.

\vspace{5pt}
\noindent \textbf{NLI and Text Summarization.} For the NLI fine-tuning, we configure the maximum training epoch at 200, establish a learning rate of 7e-5, and apply a weight decay of 0.01. The maximum length of the target output is set at 30. For text summarization tasks, we implement variable learning rates of 1e-3, 5e-4, and 1e-4 for the MeQSum, Icliniq, and HealthCareMagic datasets, respectively, corresponding to a noise ratio of 0.2 each. We cap the maximum input text sequence length at 512 for each dataset, with a corresponding output text sequence length of 128. We also set the training epoch at 300, apply a length penalty of 0.7, and utilize a label smoothing factor of 0.1.

\vspace{5pt}
\noindent \textbf{Image Captioning.} The input image is resized to a resolution of $480 \times 480$. The maximum output text lengths for IU-Xray, Peir Gross, and ROCO are set at 45, 20, and 30, respectively. The rationale behind the selection of these specific output lengths is detailed in Section \ref{sec:hyper_infer}. By default, we set the beam search size to 10.

\vspace{5pt}
\noindent \textbf{Visual Question Answering.} In our experiments, we resize the input image to a fixed resolution of $480 \times 480$. The maximum length for the output text is standardized to 30 across all datasets. Different datasets, however, necessitate varying fine-tuning epochs. Specifically, SLAKE and PathVQA are fine-tuned over 50 epochs, while VQA-RAD requires a more extensive fine-tuning period of 500 epochs due to the unseen data distribution in the pretraining and the lack of reported SOTA results.




\subsection{Hyperparameter Selection for Inference} \label{sec:hyper_infer}

We primarily discuss hyperparameter selection in the inference stage for tasks such as text summarization, image captioning, and VQA, as we observe that the model's performance is highly sensitive to beam search size and output length constraints. We use the IU X-ray dataset as an example. The default maximum target output is constrained to 20. If we select beam search sizes of 5, 10, and 20, respectively, we observe CIDEr increases from 0.164 to 0.294 and then 0.298. In most of our experiments, increasing beam search size typically improves predictions. Another component to consider is the maximum target output, as the default value may not be optimal for every dataset. Specifically, we calculate the length of answers in the training and validation sets, obtaining corresponding $\text{mean} \pm \text{std}$ values of $(39.57 \pm 19.95)$ and $(39.25 \pm 20.62)$, respectively. Based on these statistics, we set the maximum target output as 40, 45, 50, and 60 with a beam size of 10 (the best in the initial experiments) for both fine-tuning and inference. The corresponding performance in terms of CIDEr is 0.351, 0.360, 0.311, and 0.315. We follow the same strategy for other downstream datasets.

\subsection{Task-aware Instruction Generalization} \label{appx:instruct_generalization}

We use two other datasets -- Indian Movie Face database (IMFDB) \citep{imfdb} and FG-NET \citep{lanitis2002toward} to fine-tune BiomedGPT$_{base}$ for age prediction. There are 19,906 images with three classes in IMFDB and 1,002 images with 62 classes in the FG-NET dataset. We perform this task using task-aware input formats for both image classification and VQA. Specifically, we use the instructions \textit{``What does the image describe?''} and \textit{``What is the chronological age?''}, respectively. An interesting observation is that BiomedGPT can achieve 94.8\% and 19.9\% accuracy on these two datasets with the first and default instruction, but fails to answer the question (0\% accuracies) with the second and unseen instruction in the pretraining. This suggests that even if the instruction \textit{``What does the image describe?''} is not strongly correlated with the age prediction task or other new classification tasks, we can still use it for general image classification since BiomedGPT can fully understand it from the pretraining stage. However, addressing task-aware instruction sensitivity remains an open question.


\section{Additional Experiments}
In this section, we discuss several additional experiments and their respective results. Although these experiments may not be comprehensive, they either address some limitations of the current BiomedGPT version or showcase performance comparisons with larger model scales and supplementary techniques.

\subsection{Performance with Larger Model}
We evaluate the performance of $\text{BiomedGPT}{\text{Large}}$, which has approximately 472 million parameters with 16 attention heads, 12 encoder layers, and 12 decoder layers for image classification tasks. The corresponding input size, visual backbone, embedding size, and hidden size are $480\times480$, ResNet152, 1024, and 4096, respectively. Table \ref{tab:biomedgpt_large} presents the results on selected MedMNIST datasets. We observe that $\text{BiomedGPT}{\text{Large}}$ generally achieves superior performance compared to smaller models, except on ChestMNIST, where all models exhibit limited performance. We are in the process of refining the $\text{BiomedGPT}_{\text{Large}}$ model, and potentially even a Huge-size model, by optimizing its parameters to fully exploit its potential. A comprehensive evaluation of its performance across a diverse range of biomedical tasks will be conducted, and the results will be analyzed to gain a deeper understanding of the strengths and limitations of larger models. This will contribute to the development of more robust and effective generalist biomedical AI models.

\begin{table}[htbp]
\centering
\caption{Examples of image classification using BiomedGPT with different model scales.} \label{tab:biomedgpt_large} 
\begin{tabular}{lcccc}
\toprule
Dataset & Small & Medium & Base & Large \\ \midrule
TissueMNIST & 36.4 & 36.4 & 53.2 & 69.7 \\
OrganCMNIST & 92.2 & 92.3 & 93.1 & 93.3 \\
ChestMNIST & 89.2 & 89.2 & 89.2 & 89.2 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Classification with High-resolution Images}
In previous evaluations, we showcased BiomedGPT's proficiency in image classification tasks across different modalities using the MedMNIST v2 dataset. This dataset, however, consists of pre-processed images of dimensions $28 \times 28$, which contrasts with the high-resolution images typically output by medical imaging devices. To address this, we turned to two public chest X-ray datasets released by NIH \citep{jaeger2014two} in this section: the Shenzhen chest X-ray set (SZ-CXR) and the Montgomery County chest X-ray set (MC-CXR). The SZ-CXR contains 662 images with dimensions $3000 \times 2919$, and the MC-CXR has 138 images at $4892 \times 4020$.

For comparison and following the approach of \citep{tasci2021voting}, we employed these datasets for binary classification (normal vs.\ abnormal) and divided the dataset into a 70\% / 10\% / 20\% split for training, validation, and testing, respectively. Table \ref{tab:high_resolution} displays the accuracy of BiomedGPT at different scales. It is evident that BiomedGPT maintains robust performance even on high-resolution biomedical images. Particularly noteworthy is its performance on the larger downstream dataset (SZ-CXR), where it achieves approximately a 10\% improvement compared to the reference model, Inception v3. BiomedGPT also exhibits competitive performance on the MC-CXR dataset, securing 26 correct predictions out of 29 samples in the testing set, compared to 27 correct predictions by Inception v3 when using augmentation techniques.

\begin{table}[htbp]
\centering
\caption{We present the prediction accuracies of BiomedGPT when dealing with high-resolution medical images. Our report showcases the superior average results yielded by a single model on these two datasets, as documented in the existing literature. Moreover, the implementation of a voting-based ensemble method, as described in \citep{tasci2021voting}, enhances the performance even further, achieving impressive results in excess of 95\% for both datasets. For this analysis, the Inception v3 model was pre-trained.}\label{tab:high_resolution} 
\begin{tabular}{lcc}
\toprule
Model & MC-CXR & SZ-CXR \\ \midrule
$\text{BiomedGPT}_{\text{Small}}$ & 75.86 & 83.46 \\
$\text{BiomedGPT}_{\text{Medium}}$ & 82.76 & \textbf{96.99} \\
$\text{BiomedGPT}_{\text{Base}}$ & 89.65 & 96.24 \\
Inception v3 \citep{szegedy2016rethinking} & 67.85 & 84.96 \\
{    } + CLAHE~\citep{pizer1986psychovisual, pizer1987adaptive} & 75.00 & 87.61 \\
{    } + RandXScale & \textbf{92.85} & 87.61 \\ \bottomrule
\end{tabular}
\end{table}

\begin{wrapfigure}{r}{0.4\textwidth}
  \begin{center}
    \includegraphics[width=0.38\textwidth]{fig/prompt_tuning.png}
  \end{center}
  \caption{The image classification accuracy with BiomedGPT via prompt tuning. The x-axis represents the prompt length, and the y-axis is the prediction accuracy.} 
  \label{fig:prompt_tuning}
\end{wrapfigure}

\subsection{Lightweight Prompt Tuning} \label{sec:prompt_tuning}
Prompt tuning \citep{lester2021power} is a simple yet effective parameter-efficient fine-tuning (PEFT) mechanism \citep{chen2023parameterefficient}, which learns ``soft prompts'' or extra model parameters for each task instead of making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. We must mention that the addition of soft prompts is contrary to the design principle of the generalist model. In our experiments, we inject two prompt layers into the encoder and decoder, respectively following \citep{yang2022prompt}, and vary the prompt length $\{20, 40, 60, 80, 100\}$ to investigate the performance comparison against full-model fine-tuning using $\text{BiomedGPT}_{\text{Base}}$.  
The accuracy of image classification on PathMNIST, BloodMNIST, and PneumoniaMNIST was obtained after 100, 512, and 55 training epochs respectively, all with a consistent batch size of 512. Initial results in Figure \ref{fig:prompt_tuning} indicate that as the prompt length increases, the model performance tends to improve. However, despite an increased number of tuning epochs compared with fine-tuning on the original BiomedGPT, the performance after prompt tuning significantly lags behind that of model fine-tuning. Specifically, even when considering only the best results in prompt tuning, there are substantial accuracy reductions of 32.3\%, 54.6\%, and 32.6\% on these three datasets, respectively.

What we need to say here is prompt tuning is typically used in fine-tuning large-scale models and becomes more competitive with scale: as models exceed billions of parameters, their method matches the strong performance of model tuning \citep{lester2021power}. Our implementation didn't achieve the expectation of memory efficiency, and we are working on the progress of the implementation optimization, that's why we didn't use prompt tuning for the larger model.




\subsection{Cross-modality Transferability} \label{sec:modality_additional}
In this section, we explore the aspect of cross-domain transferability. Specifically, we fine-tune the BiomedGPT model, pre-trained solely with X-Ray (Radiology) data as described in Section \ref{sec:modality_main}, using datasets from other domains such as Microscopy. Additionally, we select MRI-only and CT-only image-text pairs from SLAKE (EN), and conduct Visual Question Answering (VQA) fine-tuning. The results, measured in terms of accuracy, are presented in Table \ref{tab:cm_transfer}. Our findings indicate that cross-modality transfer with BiomedGPT is feasible, albeit with potentially significant performance degradation (for example, an absolute accuracy reduction of 8\% with the base-size model on the DermaMNIST dataset). It's noteworthy that we had to double the training epochs as compared to the previous fine-tuning with a pre-trained model encompassing all modalities (100 vs.\ 50). Therefore, we conclude that modality comprehensiveness is critical for a generalist biomedical model to facilitate efficient knowledge transfer.

\begin{table}[htbp]
\centering
\caption{Examples of image classification using BiomedGPT with different model scales. ``Base w/ all'' represents that the $\text{BiomedGPT}_{\text{Base}}$ pre-trained with all modalities. The VQA results are incomplete and we are working in progress for them.} \label{tab:cm_transfer} 
\begin{tabular}{lcccc}
\toprule
Datasets & Small & Medium & Base & Base w/ all \\ \midrule
BloodMNIST & 93.19 & 96.46 & 96.84 & 97.69 \\
DermaMNIST & 68.08 & 67.83 & 70.52 & 78.60 \\
SLAKE-MRI & 25.88  & 43.42 & 66.23 & 68.42 \\
SLAKE-CT & 41.95 & 57.20 & 65.25 & 80.51 \\ \bottomrule
\end{tabular}
\end{table}

\section{Datasets}
In this section, we present the datasets utilized in our study, and we also engage in a detailed discussion on the preprocessing strategies tailored specifically for biomedical data.

\subsection{Pretraining Datasets}
We construct pretraining datasets by incorporating vision and language data (i.e., image-text pairs), vision-only data (i.e., raw image data), and language data (i.e., plain texts). For replication, the training datasets are publicly available or easily accessed after request. For the datasets used in downstream tasks, we carefully filter our pretraining data and exclude images that appear in the validation and test sets to avoid data leakage. The statistics of the pretraining datasets are listed in Table \ref{tab:pretraining}. The detailed description of these datasets is shown as follows.
\vspace{5pt}

\noindent \textbf{CheXpert} \citep{irvin2019chexpert} is a large dataset that contains 224,315 chest radiographs of 65,240 patients labeled for the presence of 14 common chest radiographic observations. 
\vspace{5pt}

\noindent \textbf{CytoImageNet} \citep{hua2021cytoimagenet} contains 890K microscopy images with 894 classes, which are sourced from 40 openly available datasets such as 1) Recursion, 2) Image Data Resource \citep{williams2017image}, 3) Broad Bioimage Benchmark Collection \citep{ljosa2012annotated}, 4) Kaggle and 5) Cell Image Library.
\vspace{5pt}

\noindent \textbf{International Skin Imaging Collaboration (ISIC)} datasets have become a leading repository in skin cancer detection and malignancy assessment. In our pretraining, we used ISIC 2020 challenge dataset (33,126 images) that focused on melanoma detection \citep{rotemberg2021patient}.
\vspace{5pt}

\noindent \textbf{Retinal Fundus} comes from Kaggle Diabetic Retinopathy competition \citep{diabetic-retinopathy-detection}. The dataset contains 35,126 images acquired from different cameras under varying exposures with 5 categories.
\vspace{5pt}

\noindent \textbf{MedICaT} \citep{subramanian2020medicat} is created via extracting figures and captions from open-access papers in PubMed Central. 75\% of its figures are compound figures, including several sub-figures. It contains over 217,000 images with captions and inline textual references.
\vspace{5pt}

\noindent \textbf{IU X-ray} \citep{demner2016preparing} is a set of chest X-ray images paired with their corresponding diagnostic reports. The dataset contains 7,470 pairs of images and reports.
\vspace{5pt}

\noindent \textbf{Peir Gross} \citep{jing2018automatic} was collected with descriptions in the Gross sub-collection from PEIR digital library, resulting in 7.442 image-caption pairs from 21 different sub-categories. Each caption contains only one sentence.
\vspace{5pt}

\noindent \textbf{SLAKE} \citep{liu2021slake} is a large bilingual (English \& Chinese) dataset with comprehensive semantic labels annotated by experienced physicians. We only use English VQA samples of 642 images and 7K QA pairs in our pretraining. 
\vspace{5pt}

\noindent \textbf{PathVQA} \citep{he2020pathvqa} is a pathology VQA dataset consisting of 4,998 images and 32,799 question-answer pairs. There are 7 categories of questions: what, where, when, whose, how, how much/how many, and yes/no. The questions in the first 6 categories are open-ended while the rest are close-ended. 
\vspace{5pt}



\noindent \textbf{DeepLesion} \citep{yan2018deeplesion}  is a dataset with 32,735 lesions in 32,120 CT slices from 10,594 studies of 4,427 unique patients. There are a variety of lesion types in this dataset, such as lung nodules, liver tumors, enlarged lymph nodes, etc.
\vspace{5pt}

\noindent \textbf{PubMed Abstracts} are provided by BLUE benchmark \citep{peng2019transfer} containing 181 million sentences. We do not use PMC full articles because the prior works \citep{gu2021domain}, training on both corpora of abstracts and articles surprisingly leads to a slight degradation in performance compared to sole training on PubMed abstracts.
\vspace{5pt}

\noindent \textbf{NCBI BioNLP Corpus}\footnote{\url{https://www.ncbi.nlm.nih.gov/research/bionlp/Data/}} is developed and maintained by NLM/NCBI BioNLP Research Group. For our pretraining process, we selectively employ the abstracts and full-text data, opting not to incorporate interaction-related information.
\vspace{5pt}

\noindent \textbf{MIMIC-III Clinical Notes} are extracted and de-identified from the MIMIC-III database \citep{goldberger2000physiobank, johnson2016mimic}, encompassing around 1.8 million samples. We refrained from any preprocessing techniques \citep{nuthakki2019natural}, which might have presented certain challenges during the pretraining process due to the presence of anonymized and irrelevant data, such as patients' names being replaced with ``Xxxx''. Furthermore, the language structure in clinical notes substantially deviates from the conventional domain text found in language model training corpora. Such notes frequently contain grammatical errors and use domain-specific terminology. These unique aspects result in notable disparities between clinical text and biomedical text (such as PubMed), despite both belonging to the medical domain. Biomedical text, on the other hand, tends to be more coherent, edited, and refined. As part of our future work, we plan to improve the processing of inputs derived from clinical notes.

\subsection{Downstream Datasets}
We verify the capability of BiomedGPT on various downstream tasks in fine-tuning and zero-shot settings. The following lists different downstream datasets excluding ones that are shown in pretraining. 
\vspace{5pt}



\noindent \textbf{MedMNIST 2D v2.} \citep{medmnistv2} is a large-scale MNIST-like dataset collection of standardized biomedical images, designed to perform classification. In this work, we select ten 2D datasets, which are pre-processed into a small size of $28 \times 28$ with the corresponding classification labels. 
\vspace{5pt}

\noindent \textbf{MedNLI} \citep{romanov2018lessons} is annotated by doctors, and considered a natural language inference task (NLI), grounded in the medical history of patients. NLI is a task of determining whether the given hypothesis and premise logically follow (entailment) or unfollow (contradiction) or are undetermined (neutral) to each other.
\vspace{5pt}

\noindent \textbf{MeQSum, iCliniq, HealthCareMagic} \citep{abacha2019summarization, zeng2020meddialog} are abstractive summarization datasets and we preprocess them following BioBART \citep{yuan2022biobart}. Specifically, MeQSum contains 1000 refined patients health questions selected from a collection distributed by the U.S. National Library of Medicine \citep{kilicoglu2018semantic}. iCliniq contains 31,062 patient-written summaries, and HealthCareMagic 226,405 abstractive samples written in a formal style. Each sample is comprised of a summary and corresponding dialogues between a patient and a doctor.

\vspace{5pt}

\noindent \textbf{Radiology Objects in COntext (ROCO)} \citep{pelka2018radiology} is a large-scale medical and multimodal imaging dataset. The ROCO images are from publications on the PubMed Central Open Access FTP mirror, which were automatically detected as non-compound and either radiology or non-radiology. ROCO is typically widely used in pretraining, and we are also considering incorporating it into the pretraining of our subsequent work.
\vspace{5pt}

\noindent \textbf{VQA-RAD} \citep{lau2018dataset} is the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers. We remove inappropriate characters in the original data, e.g., ``slee\textbackslash t n'' $\rightarrow$ ``sleen''.
\vspace{5pt}

\section{Evaluation Metrics} \label{sec:eval_metic}
In this study, we primarily employ four evaluation metrics to assess the performance of our models. These include accuracy for tasks such as image classification, natural language inference, and visual question answering; ROUGE-L \citep{lin2004rouge} for text summarization and image captioning tasks, which concentrates on the overlap of n-grams between system-generated and reference summaries; METEOR \citep{banerjee2005meteor}, a metric that calculates precision and recall using n-gram alignment between the hypothesis and the reference; and CIDEr \citep{vedantam2015cider}, a metric known for its high agreement with human-assessed consensus in image captioning tasks.

In our work, our primary focus is optimizing for CIDEr. We select the model checkpoint that yields the highest CIDEr score in the validation phase for downstream tasks. Detailed mathematical expressions for captioning metrics are provided subsequently in this section.

\vspace{5pt}
\noindent \textbf{ROUGE-L} stands for recall-oriented understudy for gisting evaluation with the longest common subsequence. Given the candidate $C$ and reference $R$, let $LCS(C,R)$ be the length of the longest common subsequence, which is determined by using dynamic programming, it can be an expression as:

\begin{equation}
    \text{ROUGE-L} = \frac{(1+\beta^2) R_{LCS} P_{LCS}}{R_{LCS} + \beta^2 P_{LCS}},
\end{equation}
where $R_{LCS}=\frac{LCS(C,R)}{c}$, $P_{LCS}=\frac{LCS(C,R)}{r}$, $\beta=\frac{P_{LCS}}{R_{LCS}}$. $c$ and $r$ represent the length of the candidate and reference.

\vspace{5pt}
\noindent \textbf{METEOR} stands for metric for evaluation of translation with explicit ordering. We represent precision and recall as $P=\frac{m}{c}$ and $R=\frac{m}{r}$ and let $m$ be the number of common words in the candidate $C$ and the reference $R$ with the number of words of $c$ and $r$, respectively. The METEOR is calculated via

\begin{equation}
    \text{METEOR} = (1 - p) \times \frac{PR}{\alpha P + (1-\alpha) R},
\end{equation}
where $p$ is the penalty factor and is denoted as $p=\gamma (\frac{ch}{m})^{\theta}$, $ch$ is the number of chunks, which means a contiguous ordered block. $\alpha, \theta, \gamma$ are hyperparameters determined according to different datasets.

\vspace{5pt}
\noindent \textbf{CIDEr} stands for consensus-based image description evaluation. let $c$ be a candidate caption, $S$ be a set of reference captions, and CIDEr is obtained by averaging the similarity of different lengths:

\begin{equation}
    \text{CIDEr}_n (c,S) = \frac{1}{M} \sum_{i=1}^M \frac{g^n(c) \times g^n(S_i)}{||g^n(c)|| \times ||g^n(S_i)||},
\end{equation}
where $M$ denotes the number of reference captions and $g^n(\cdot)$ denotes an $n$-gram-based TF-IDF vector.

\section{Detailed Experimental Outputs}
In this section, we present selected experimental outputs for the tasks of text summarization and image captioning, rather than merely reporting their quantitative results. This decision is based on the recognition that existing evaluation metrics for these tasks may not always correlate well with human judgment \citep{kilickaya2017re, cui2018learning}. We should note that the examples we present here have been randomly selected from each test set (with the exception of Peir Gross due to the potentially unsettling nature of its images), rather than carefully curated. Figure \ref{fig:example_iu_xray} and Figure \ref{fig:example_roco} provide examples of image captioning from IU-Xray and ROCO, respectively, while Table \ref{tab:example_summary} illustrates examples of text summarization.

\clearpage
\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{fig/iu_xray_example.png}
    \caption{Examples of IU-Xray image captioning using BiomedGPT$_{\text{Base}}$.}
    \label{fig:example_iu_xray}
\end{figure*}

\clearpage
\begin{figure*}
    \centering
    \includegraphics[width=0.85\textwidth]{fig/roco_example.png}
    \caption{Examples of ROCO image captioning using BiomedGPT$_{\text{Base}}$.}
    \label{fig:example_roco}
\end{figure*}

\clearpage
\begin{table*}
    \centering
    \small
    \caption{Examples of text summarization using BiomedGPT$_{\text{Base}}$.} \label{tab:example_summary}
    \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{0.7in}|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}p{1.2in}|>{\raggedright\arraybackslash}p{1.2in}|}
    \hline
    \textbf{Dataset} & \textbf{Input} & \textbf{Reference} & \textbf{Hypothesis (Generation)} \\
    \hline
    MeQSum & SUBJECT: just a question. MESSAGE: hi..just wanna ask... 1.how the aspirin can affect the ear? 2. what is the cause of suddenly ringging in the ear? isn't dangerous? tq.. :) & what causes ringing in the ear, and can aspirin affect the ear? & what are the causes of ringingging in the ear? \\
    \cline{2-4}
     & SUBJECT: cosmetic leg shortening surgery. MESSAGE: Hi,  I am a tall girl(5'8"), who wants to undergo leg shortening sugery of 2 inches for cosmetic purpose. It would be good if I can get more information about it. I would like to know the cost of this surgery, the recovery time and the risks associated with it. How long should I stay in the hospital?  Thanks and regards & where can i find information on leg shortening surgery, including risks, cost, and recovery time? & where can i find information on leg shortening surgery? \\
    \hline
    iCliniq & Patient: Hi doctor, I have red circular lesions on lower extremities. Currently I am taking Amlodipine 10 mg qd, Benazepril 20 mg and Clonidine 0.2 mg. I have not yet taken any tests. Kindly suggest what can it be? Doctor: Hi. Kindly revert back with the pictures and answers to a dermatologist online --$>$ https://www.icliniq.com/ask-a-doctor-online/dermatologist & how to treat red circular lesions on lower extremities? & what can be the reason for red circular lesions on lower extremities? \\
    \cline{2-4}
     & Patient: Hi doctor, During masturbation I just rub the tip of the penis and not the entire penis. Is it a wrong way of doing? I do notget excited during sex and unable to ejaculate. Only, rubbing the tip of the penis gives me excitement. Also, two weeks ago, I have undergone circumcision as my foreskin did not retract. Before circumcision too, I have the same problem. Please help. Doctor: Hi. For further doubts consult a sexologist online --$>$https://www.icliniq.com/ask-a-doctor-online/sexologist & i masturbate only by rubbing the tip of the penis. is it a wrong way? & i masturbate only by rubbing the tip of the penis. is it a wrong way? \\
    \hline
    Healthcare Magic & Patient: I have a rash behind my ears that just developed this week. It started out on one ear and then the next day went to the other..its been a few days and I try not to touch them. But I work at a call center and have to wear headphones all day. And by the end of my shift my ears are itching and burning and inflamed. Now they are starting to seep and ooze, they are so itchy and the skin looks white and scaly. Ive read about all the dermatitis and psoriasis stuff. I find it hard for me to have that, I hav e never once had a skin problem in my life. Need help! Any answers? Doctor: Hi.As per your case history you are having allergic dermatitis.My treatment advice is  1. Avoid using any new products like soap or perfume.2. Apply a mild antibiotic plus steroid cream like fusiderm-B cream twice daily on it.3. Take an antihistamine like levocetirizine for 7-10days .4. Other treatment options are oral steroid and hydroxyzine given only after consulting a dermatologist.Thanks.Dr.Harshit Bhachech.MBBS, DDVL. & what causes itchy rash with discharge behind the ears? & suggest treatment for itchy rashes behind ears. \\
    \hline
    \end{tabularx}
\end{table*}

\end{document}
