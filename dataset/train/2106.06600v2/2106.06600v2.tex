\section{Experiments}
\label{sec:experiments}

We evaluate our approach on two code repair datasets: a common benchmark \textbf{DeepFix}\footnote{{\url{https://bitbucket.org/iiscseal/deepfix}}} \citep{gupta2017deepfix}, and \textbf{GitHub-Python}, a bigger dataset we collect in this paper.


\subsection{Dataset and setup}
\label{sec:exp-setup}

We first describe the detail and experimental setup for GitHub-Python and DeepFix.

\subsubsection{Github-Python}
\heading{Dataset.}
To obtain an unlabeled dataset of code, we collected Python3 files from GitHub public repositories.\footnote{\url{https:github.com}\vspace{0mm}}
We then tokenize each code file using the builtin Python tokenizer, and keep code snippets of length 10--128 tokens, resulting in 3M code snippets.
As the critic $c$, we use the Python AST parser,\footnote{\url{https://docs.python.org/3/library/ast.html}} 
which catches unbalanced parentheses, indentation errors, and other syntax errors. Concretely, we define $c(x) = 1$ (good) if the AST parser returns no errors for input code $x$, and $c(x) = 0$ (bad) otherwise.
Using this critic, we obtain 38K snippets of bad code and 3M snippets of good code.
From the 38K bad examples, we holdout 15K as the final test set, and make the remaining 23K bad examples available for BIFI.
Our goal is to learn a fixer that repairs AST parse errors. We define that the fixer's repair is successful if the output code has no AST parse errors and has Levenshtein edit-distance \cite{levenshtein1966binary} less than 5 tokens from the input code.
The evaluation metric is the fixer's repair accuracy on the test set, \ie, the heldout 15K examples of real bad code.

\begin{table}[!t]
\newcolumntype{G}{>{\columncolor{LightGray}}c}
\hspace{-1mm}
\scalebox{0.85}{
\begin{tabular}{llGccc}
    \toprule  
    \multirow{2}{*}{\textbf{Method}\vspace{-5mm}} & 
    &\multicolumn{4}{c}{\textbf{Test accuracy}}\\
    \cmidrule(lr){3-6}
      & &  
    \!\!\textbf{Total}\!\! &  \!\!\begin{tabular}{@{}c@{}}\scalebox{0.9}{Unbalanced}\\[-1mm]\scalebox{0.9}{Parentheses}\end{tabular}\!\!\! &  \begin{tabular}{@{}c@{}}\!\scalebox{0.9}{Indentation}\!\!\!\\[-1mm]\scalebox{0.9}{Error}\end{tabular} &  \begin{tabular}{@{}c@{}}\scalebox{0.9}{Invalid}\\[-1mm]\scalebox{0.9}{Syntax}\end{tabular}\\
    \midrule  
    {Initial} & 
    Round-0~~~ & \!62.0\%\! & 87.7\% & 39.4\% & 70.5\% \\
    \hdashline[2pt/1.5pt]&\\[-3.7mm]
    \multirow{2}{*}{\begin{tabular}{@{}l@{}}\textbf{FixerOnly}\end{tabular}}\!\! 
    & Round-1~~ & \!86.8\%\! & 93.3\% & 79.5\% & 90.9\% \\
    & Round-2~~~ & \!88.6\%\! & 92.4\% & 83.7\% & 92.0\% \\
    \hdashline[2pt/1.5pt]&\\[-3.7mm]
    \multirow{2}{*}{\begin{tabular}{@{}l@{}}\textbf{BIFI}\end{tabular}}
    & Round-1~~~ & \!88.0\%\! & 94.1\% & 81.3\% & 91.6\%  \\
    & Round-2~~~ & \!\textbf{90.5\%}\! & \textbf{94.2\%} & \textbf{85.9\%} & \textbf{93.5\%}  \\
    \bottomrule 
\end{tabular}
}\vspace{-3mm}
\caption{
\textbf{Repair accuracy on the GitHub-Python test set}. 
The initial fixer is trained on synthetic bad code. 
Our proposed method (BIFI) enables the fixer to be fine-tuned on \textit{real} bad code and bad code generated by the learned breaker. The result shows that BIFI outperforms the initial fixer by a large margin.
}\vspace{-2mm}
\label{tbl:result_python}
\end{table}
 \begin{table}[!tb]
\centering
\scalebox{0.85}{
\begin{tabular}{llc}
\toprule
\multicolumn{2}{l}{\textbf{Method}}
& \textbf{Test accuracy}\\
\midrule
\multicolumn{2}{l}{DeepFix \scalebox{0.9}{~\hspace{3.1mm}\citep{gupta2017deepfix}}} & 33.4\% \\
\multicolumn{2}{l}{RLAssist \scalebox{0.9}{~\hspace{2.1mm}\citep{gupta2018deep}}} & 26.6\%\\
\multicolumn{2}{l}{SampleFix \scalebox{0.9}{~\hspace{-0.3mm}\citep{hajipour2019samplefix}}} & 45.3\% \\
\multicolumn{2}{l}{DrRepair \scalebox{0.9}{~\hspace{2.1mm}\citep{yasunaga2020repair}}}~~~ & 66.1\% \\
\midrule
Our Initial & Round-0  ~~(=\,DrRepair)& 66.1\% \\
\hdashline[2pt/1.5pt]&\\[-3.7mm]
\multirow{2}{*}{\begin{tabular}{@{}l@{}}\textbf{Our FixerOnly}\end{tabular}} & Round-1 & 68.6\% \\
& Round-2 & 70.5\% \\
\hdashline[2pt/1.5pt]&\\[-3.7mm]
\multirow{2}{*}{\begin{tabular}{@{}l@{}}\textbf{Our BIFI}\end{tabular}} & Round-1 & 70.8\% \\
& Round-2 & \textbf{71.7\%} \\
\bottomrule
\end{tabular}
}\vspace{-1mm}
\caption{\textbf{Repair accuracy on the DeepFix test set}. 
We define our initial fixer (Round 0) to be the existing best system DrRepair. Note that DrRepair's training procedure coincides with the initialization step of our BIFI algorithm, with heuristic perturbations used in Eq \ref{eq:P_syn}. 
We then apply BIFI on top of it for Round 1, 2. BIFI outperforms DrRepair, achieving a new state-of-the-art.
}\vspace{-2mm}
\label{tbl:result_deepfix}
\end{table}

 

\heading{BIFI implementation details.}
For the architecture of the fixer and breaker, we use the encoder-decoder Transformer \cite{vaswani2017attention} with 4 layers, 8 attention heads, and hidden states of size 256.
The model parameters are optimized by Adam \cite{kingma2015adam}, with batch size of 27,000 tokens, learning rate 0.001, and gradient clipping 1.0 \cite{pascanu2013difficulty}, on one GPU (GTX Titan X).
For generation, we use beam search with beam size 10,
and keep predictions with Levenshtein edit-distance less than 5 tokens from the input.



To train the initial fixer $f_0$, we use random perturbations for the corruption procedure $b_\text{synthetic}$ (Eq \ref{eq:P_syn}), which drops, inserts, or replaces 1--3 tokens in code with uniform distribution. We apply $b_\text{synthetic}$ 8 times to each of the 2.6M good code snippets to prepare the initial training data $\gP_\text{synthetic}$. We holdout 1\% of $\gP_\text{synthetic}$ as our dev set, which we use to perform early stopping.
We then run the BIFI algorithm for $K = 2$ rounds.



\subsubsection{DeepFix}
\heading{Dataset.}
\label{sec:exp-setup-deepfix}
DeepFix \cite{gupta2017deepfix} contains C code submitted by students in an introductory programming course, of which 37K snippets are good (no compiler error) and 7K are bad (have compiler errors). Each code snippet has 25 lines on average.
Within the 7K bad examples, we take 20\% as a heldout test set. We make the remaining 80\% available for BIFI.
The goal is to learn a fixer that repairs compiler errors. Repair is successful if the output code has no compiler errors. The evaluation metric is the fixer's repair accuracy on the test set.


\heading{BIFI implementation details.}
We define the initial fixer $f_0$ as DrRepair \cite{yasunaga2020repair} (the existing best system on DeepFix), which is an encoder-decoder model trained in a procedure that corresponds exactly to the initialization step of BIFI (\S \ref{sec:approach-init}).
Specifically, to train DrRepair, \citet{yasunaga2020repair} design heuristic perturbations for the corruption procedure $b_\text{synthetic}$, which mimics common code errors beginner and experienced programmers make (\eg, typos, punctuation, keyword and type errors).
We use the same training \!/\! dev data prepared and released by the authors to train the initial fixer. 
We then run the BIFI algorithm for $K=2$ rounds. Our fixer and breaker have the same model architecture as DrRepair. 
At test time, following the original DrRepair, we repeatedly apply the fixer while the code still has errors, up to a maximum of 5 times.



\subsection{Main results}

\begin{table}[!t]
\newcolumntype{G}{>{\columncolor{LightGray}}c}
\centering
\scalebox{0.85}{
\begin{tabular}{llcccc}
    \toprule  
    \multirow{2}{*}{\textbf{Method}\vspace{-5mm}} & 
    &\multicolumn{4}{c}{\textbf{Test accuracy}}\\
    \cmidrule(lr){3-6}
      & &  
    \begin{tabular}{@{}c@{}}\scalebox{0.95}{Bad}\\[-0.7mm]\scalebox{0.9}{100\%}\end{tabular} &  \begin{tabular}{@{}c@{}}\scalebox{0.95}{Bad}\\[-0.7mm]\scalebox{0.9}{50\%}\end{tabular} &  \begin{tabular}{@{}c@{}}\scalebox{0.95}{Bad}\\[-0.7mm]\scalebox{0.9}{10\%}\end{tabular} &  \begin{tabular}{@{}c@{}}\!\!\!\!\scalebox{0.9}{ref. Synthetic}\\[-0.7mm]\scalebox{0.95}{bad only}\end{tabular}\\
    \midrule  
    {Initial} & 
    Round-0~~~ & 62.0\% & 62.0\% & 62.0\% & 62.0\% \\
    \hdashline[2pt/1.5pt]&\\[-3.7mm]
    FixerOnly & Round-2~~~ & 88.6\% & 84.7\% & 78.5\% & 62.7\% \\
    \hdashline[2pt/1.5pt]&\\[-3.7mm]
    BIFI & Round-2~~~ & {90.5\%} & 89.0\% & 86.7\% & 63.3\% \\
    \bottomrule 
\end{tabular}
}
\vspace{-3mm}
\caption{
\textbf{Analysis varying the amount of (real) bad code on GitHub-Python}. ``Bad 100\%'' is our original setting (with 23K real bad examples available for BIFI) and ``Bad 50\%'' means only 50\% of them (11.5K) are made available. ``Synthetic bad only'' means keeping training the fixer on synthetic bad examples. The result shows that (i) even when the amount of real bad examples is small (\eg, ``Bad 10\%''), they are still useful, allowing FixerOnly \slash BIFI to perform better than using synthetic data alone; (ii) when the amount of real bad examples is small, BIFI exceeds FixerOnly by a larger margin, highlighting the usefulness of the bad examples generated by the learned breaker.
}\vspace{-3mm}
\label{tbl:result_analysis_bad_amount}
\end{table}
 \begin{table}[!tb]
\centering
\scalebox{0.85}{
\begin{tabular}{llcc}
\toprule
\multicolumn{2}{l}{\textbf{Method}}
& \textbf{Test accuracy}\\
\midrule
Initial 
& Round-0~~\hspace{5mm} & 62.0\% \\
\midrule
BIFI (\textbf{ours}) & Round-2~~ & \textbf{90.5\%} \\[0.3mm]
\hdashline[2pt/1.5pt]&\\[-3.4mm]
~-- real bad & Round-2~~ & {84.6\%} \\[0.3mm]
\hdashline[2pt/1.5pt]&\\[-3.4mm]
~-- critic & Round-2~~ & {84.0\%} \\[0.3mm]
\hdashline[2pt/1.5pt]&\\[-3.4mm]
\begin{tabular}{@{}l@{}}~-- both\\[-0.5mm]
(backtranslation)\end{tabular}\!\! & Round-2~~ & {80.1\%} \\
\bottomrule
\end{tabular}
}\vspace{-2mm}
\caption{
\textbf{Performance comparison with backtranslation} on GitHub-Python. Backtranslation is equivalent to removing two components from BIFI: (i) using the critic to verify fix \slash break attempts (``critic'') and (ii) training the fixer on real bad examples in addition to examples generated by the breaker (``real bad''). The result suggests that both of these components improve the learning of a fixer, and consequently BIFI outperforms backtranslation.
}\vspace{-2mm}
\label{tbl:analysis_backtranslation}
\end{table}

 \begin{table*}[!t]
\centering
\scalebox{0.78}{
\small
\begin{tabular}{lrccccc}
    \toprule  
    \multirow{2}{*}{\textbf{Code error category}\vspace{-1mm}} &
    \multirow{2}{*}{\textbf{\#Examples}\vspace{-1mm}} & \multirow{2}{*}{\begin{tabular}{@{}c@{}}\textbf{Initial}\\[-0.mm]\textbf{Round-0 Acc.}\end{tabular}\vspace{-1.5mm}
    } &\multicolumn{2}{c}{ \textbf{FixerOnly}}&\multicolumn{2}{c}{ \textbf{BIFI}}\\
    \cmidrule(lr){4-5} \cmidrule(lr){6-7}
     &  & & \textbf{Round-1 Acc.} & \textbf{Round-2 Acc.} & \textbf{Round-1 Acc.} & \textbf{Round-2 Acc.}\\
    \midrule  
    \rowcolor{Gray}Total  & 15055~~                                & 62.0\% & 86.8\% & 92.4\% & 88.0\% & 90.5\% \\
    \midrule
    \rowcolor{LightGray}Unbalanced Parentheses & 3999~~            & 87.7\% & 93.3\% & 92.4\% & 94.1\% & 94.2\% \\
    ~~~~~Unclosed left parenthesis (not nested)\! & 226~~          & 92.5\% & 94.6\% & 94.6\% & 94.7\% & 94.4\% \\
    ~~~~~Unclosed left parenthesis (nested) & 3014~~               & 85.8\% & 92.8\% & 91.7\% & 93.8\% & 93.8\% \\
    ~~~~~Redundant right parenthesis & 759~~                       & 93.8\% & 95.3\% & 94.7\% & 95.4\% & 95.7\% \\
    \midrule
    \rowcolor{LightGray}Indentation Error & 6307~~                 & 39.4\% & 79.5\% & 83.7\% & 81.3\% & 85.9\% \\
    ~~~~~Expected indent & 4311~~                                  & 46.4\% & 81.2\% & 84.1\% & 82.0\% & 85.3\% \\
    ~~~~~Unexpected indent & 1966~~                                & 24.6\% & 76.8\% & 83.8\% & 80.9\% & 88.4\% \\
    \midrule
    \rowcolor{LightGray}Invalid Syntax  & 4749~~                   & 70.5\% & 90.9\% & 92.0\% & 91.6\% & 93.5\% \\
    ~~~~~Missing colon & 663~~                                     & 98.3\% & 97.3\% & 97.4\% & 98.2\% & 98.0\% \\
    ~~~~~Missing comma (single-line list/tuple/dict)\!\!\! & 694~~ & 95.4\% & 98.1\% & 97.4\% & 98.4\% & 98.3\% \\
    ~~~~~Missing comma (multi-line list/tuple/dict)\!\!    & 451~~ & 88.9\% & 92.5\% & 92.0\% & 94.5\% & 94.9\% \\
    ~~~~~Missing newline & 52~~                                    & 84.6\% & 86.5\% & 88.5\% & 86.5\% & 88.5\% \\
    ~~~~~Missing parenthesis pair & 634~~                          & 82.5\% & 85.0\% & 86.4\% & 87.1\% & 88.3\% \\
    ~~~~~Redundant comma & 152~~                                   & 73.7\% & 84.2\% & 91.4\% & 84.9\% & 92.1\% \\
    ~~~~~Redundant parenthesis pair & 698~~                        & 13.8\% & 80.7\% & 86.1\% & 80.1\% & 89.4\% \\
    ~~~~~\begin{tabular}{@{}l@{}}Invalid use of comma\\[-1.2mm]~~~~~\scalebox{0.7}{(\eg, ``\texttt{raise OSError, \!"msg"}'' $\rightarrow$ ``\texttt{raise OSError("msg")}'')}\end{tabular}\!\!\!\!
    & 1138~~ 
                                                                   & 61.3\% & 98.8\% & 99.1\% & 98.7\% & 99.4\% \\
    ~~~~~Other & 267~~                                             & 60.7\% & 66.3\% & 64.4\% & 67.4\% & 66.7\% \\
    \bottomrule 
\end{tabular}
}
\vspace{-1mm}
\caption{
\textbf{Code error categories in GitHub-Python, and repair accuracy}. 
Due to the mismatch between real errors and synthetic perturbations used for training, the initial fixer has lower accuracy on ``nested'' than ``not nested'' for ``unbalanced parentheses'' errors, but it catches up in BIFI round 1,\,2. Similarly, the initial fixer's repair accuracy is very low for ``redundant parenthesis pair'' and ``indentation error'', but it improves significantly in round 1,\,2.
This result illustrates the effect of BIFI adapting the fixer towards real errors.
See \S \ref{sec:analysis-adapt} for more analysis.
} \vspace{-2mm}
\label{tbl:result_python_detail}
\end{table*} 
We study the fixer's repair accuracy on GitHub-Python and DeepFix. Here ``round $k$ accuracy'' means the repair accuracy of the fixer learned in round $k$, \ie, $f_k$.


\heading{GitHub-Python.}
Table \ref{tbl:result_python} shows the test results on GitHub-Python. We show the overall repair accuracy (``Total'') as well as the breakdown over the error categories in the Python AST parser (table right).
The initial fixer $f_0$ (``Initial'') 
is trained on randomly perturbed, synthetic bad code. 
Our proposed method (BIFI) enables the initial fixer to be further trained on real bad code and bad code generated by the learned breaker, which outperforms the initial fixer significantly: +28.5\% in overall repair accuracy, and consistently across all error categories.
This result suggests that even if we start from a very simple initial fixer trained with random perturbations, BIFI can automatically turn it into a usable fixer with high repair accuracy---90.5\% accuracy on real bad code.
We also experimented with continuing training the initial fixer $f_0$ with synthetic perturbations only, for the same rounds of BIFI (hence, controlling the amount of training data seen by the fixer); however, this did not provide an improvement, suggesting that there is a performance ceiling if we only train on synthetic data.




\heading{DeepFix.}
Table \ref{tbl:result_deepfix} shows the test results on DeepFix, along with prior works.
Here we use the existing best system DrRepair as our initial fixer (``Initial''). BIFI outperforms the initial fixer by a substantial margin (+5.6\% absolute over DrRepair), attaining a new state-of-the-art accuracy of 71.7\%.
It is notable that DrRepair was trained with manually-designed heuristic perturbations, where the authors \cite{yasunaga2020repair} mimicked various code errors beginner and experienced programmers make (\eg, typos, punctuation and type errors). Nevertheless, our result suggests that there is still room for improving the adaptation to a more realistic distribution of coding errors, and BIFI boosts repair accuracy without additional manual effort.



\subsection{Analysis}
\label{sec:analysis}

\begin{figure}[!t]
\hspace{-2mm}
    \includegraphics[width=0.49\textwidth]{fig_breaker_pred.pdf}\vspace{-7mm}
    \caption{
    \textbf{Example of breaker outputs}. Given good code on the left, we sampled two outputs made by the breaker learned in BIFI round 1 (right). We observe that the breaker places high probability on errors seen in real bad code (\ie, obsolete usage of \texttt{raise}, unbalanced parentheses in nested context).
    } \vspace{-3mm}
  \label{fig:breaker_pred}
\end{figure}

\begin{figure}[!t]
\hspace{-2mm}
    \includegraphics[width=0.49\textwidth]{fig_fixer_pred_v2.pdf}\vspace{-7mm}
    \caption{
    \textbf{Example of fixer outputs}. Given the bad code on the left (with an indentation error), the initial fixer (center) attempts to fix it by inserting an indent token (\texttt{$\langle$I$\rangle$}) to line 3, but fails to adjust (delete) the indent token on the next line. The initial fixer commonly makes this mistake due to the distribution mismatch between real errors and synthetic perturbations on which the initial fixer was trained (see \S \ref{sec:analysis-adapt}). After one round of BIFI, the fixer in round 1 (right) learns to insert and delete the correct pair of indent tokens, fixing the error.
    } \vspace{-2mm}
  \label{fig:fixer_pred}
\end{figure}



We now analyze the key insights of BIFI. As the main properties of BIFI are to (i) add real bad examples to the training data if the critic accepts the fixer's output (the FixerOnly version), and to (ii) train the breaker to generate realistic bad examples (BIFI), we analyze their effects in \S \ref{sec:analysis-realbad} and \S \ref{sec:analysis-breaker}.
We also compare with backtranslation in \S \ref{sec:analysis-BT}.
We then analyze how our fixer adapts towards real code distributions through quantitative and qualitative studies (\S \ref{sec:analysis-adapt}).



\subsubsection{Effect of real bad examples}
\label{sec:analysis-realbad}

FixerOnly enables training the fixer on real bad examples (but does not use the bad examples generated by the breaker). As Table \ref{tbl:result_python} and \ref{tbl:result_deepfix} show, FixerOnly outperforms the initial fixer by a large margin, \eg, +27\% on GitHub-Python.
This result highlights the importance of training on real bad examples.


We further analyze the effect of varying the \textit{amount} of real bad examples, shown in Table \ref{tbl:result_analysis_bad_amount}. Here, ``Bad 100\%'' is our original setting (with 23K real bad examples available for BIFI and FixerOnly) and ``Bad 50\%'' means only 50\% of them (11.5K) are made available. ``Synthetic bad only'' means keeping training the fixer on synthetic bad examples only. We find that while the repair accuracy drops as we decrease the amount of available real bad examples, a small amount of real bad examples (\eg, ``Bad 10\%'') is still useful, making FixerOnly perform better than using synthetic data alone (\ie, 78.5\% vs 62.7\%).



\subsubsection{Effect of bad examples generated by breaker}
\label{sec:analysis-breaker}

Recall that BIFI trains the fixer on both real bad examples and bad examples generated by the learned breaker. As Table \ref{tbl:result_python} and \ref{tbl:result_deepfix} show, BIFI consistently provides an extra boost over FixerOnly, suggesting that the use of breaker outputs improves the fixer. Moreover, another benefit of the breaker is that one can sample many bad examples from the breaker to augment real bad examples, if their amount is limited.
In Table \ref{tbl:result_analysis_bad_amount} we find that BIFI is especially stronger than FixerOnly when the amount of available real bad examples is small (\eg, ``Bad 10\%'').

Figure \ref{fig:breaker_pred} shows sample outputs made by the learned breaker $b_1$ given the good code on the left. We observe that the breaker places high probability on errors seen in real bad code, \ie, obsolete usage of \texttt{raise} in Python3 (center) and unbalanced parentheses in nested context (right). Compared to random perturbations that arbitrarily drop \slash insert tokens, the learned breaker improves the coverage and efficiency of the training data for the fixer.




\subsubsection{Comparison with backtranslation} 
\label{sec:analysis-BT}

Table \ref{tbl:analysis_backtranslation} compares our method (BIFI) with backtranslation.
As discussed in \S \ref{sec:approach-BT}, backtranslation is equivalent to removing two components from BIFI: (i) using the critic to verify fix \slash break attempts in data generation (``critic'' in Table) and (ii) training the fixer on real bad examples besides examples generated by the breaker (``real bad''). We find that removing each component from BIFI hurts the performance (\eg, 90\%$\rightarrow$84\%), which suggests that both components are important to improve the quality of training data. With these two innovations, BIFI outperforms backtranslation by a large margin (+10\% absolute on GitHub-Python). 




\subsubsection{How does the fixer adapt?}
\label{sec:analysis-adapt}

We take a closer look at how our fixer performs and adapts towards the real distribution of bad code. Table \ref{tbl:result_python_detail} shows fine-grained categories of code errors seen in GitHub-Python, and their repair accuracy.

In this categorization, we observe two examples of distribution mismatch between the real errors and synthetic perturbations: (i) random perturbations can generate this category of errors with high probability but with a wrong ``sub-distribution'' within it (\eg, can generate ``unbalanced parentheses'' or ``missing comma'' errors but do not match the low-level distribution of real bad code, such as errors occurring more often in nested parentheses or in a multi-line list/tuple/dict; recall the Figure \ref{fig:challenge} top example); 
(ii) random perturbations can only generate this category of errors with very small probability (\eg, ``redundant parenthesis pair'' and ``indentation error'', for which an \textit{exact pair} of parentheses or indents/dedents need to be dropped or inserted; recall the Figure \ref{fig:challenge} bottom example).
For (i), the result shows that the initial fixer trained with random perturbations has lower accuracy on ``nested'' than ``not nested'' for ``unbalanced parentheses'' errors, and on ``multi-line'' than ``single-line'' for ``missing comma'' errors; but the performance catches up in round 1,\,2, suggesting the effect of BIFI for addressing the low-level distribution mismatch.
For (ii), the result shows that the initial fixer's repair accuracy is very low for ``redundant parenthesis pair'' and ``indentation error'', but it achieves significantly higher performance in round 1,\,2 (\eg, 39\%$\rightarrow$85\% for indentation errors). A possible explanation is that as BIFI iteratively adds the successfully repaired cases to training, the fixer adapts to up-weight this category of error fixing, leading to improved accuracy.

 
Figure \ref{fig:fixer_pred} provides examples of fixer outputs. Given the bad code on the left (with an indentation error), the initial fixer (center) attempts to fix it by inserting an indent token (\texttt{$\langle$I$\rangle$}) to line 3 but fails to adjust (delete) the indent token on the following line. The initial fixer commonly makes this mistake for indentation errors, due to the mismatch between real errors and synthetic perturbations discussed above. After one round of BIFI, the fixer (Figure \ref{fig:fixer_pred} right) learns to insert and delete the correct pair of indents, fixing the error.
