\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{amsmath}
\usepackage{algpseudocode}

\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{hhline}
\usepackage{arydshln}
\usepackage{url}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{capt-of}\usepackage{varwidth}
\usepackage{wrapfig}
\usepackage{ctable}
\usepackage{adjustbox}
\usepackage{float}
\restylefloat{table}
\usepackage{subfiles}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{array}
\usepackage{cleveref}


\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{darkterracotta}{rgb}{0.8, 0.31, 0.36}
\newcommand\acdraft[1]{{\color{darkterracotta}#1}}


\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{microtype}

\usepackage{subfiles} 
\aclfinalcopy 
\usepackage{soul}

\newcommand{\hlc}[2][yellow]{{\colorlet{foo}{#1}\sethlcolor{foo}\hl{#2}}}

\definecolor{cadmiumorange}{rgb}{0.93, 0.53, 0.18}
\definecolor{darkspringgreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{chamoisee}{rgb}{0.63, 0.47, 0.35}

\title{Cross-Document Language Modeling}

\author{Avi Caciularu\hspace{1em} Arman Cohan\hspace{1em} Iz Beltagy\\
\textbf{Matthew E. Peters\hspace{1em} Arie Cattan\hspace{1em} Ido Dagan} \vspace{6pt}\\  
    Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel\\
    Allen Institute for Artificial Intelligence, Seattle, WA\\
    {\tt avi.c33@gmail.com, \{armanc,beltagy,matthewp\}@allenai.org} \\
    {\tt arie.cattan@gmail.com, dagan@cs.biu.ac.il }
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our cross-document language model (CD-LM) improves masked language modeling for these tasks with two key ideas.  First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships.  Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including cross-document event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works\footnote{GitHub repository with code and models: \url{https://github.com/aviclu/CD-LM}}. 




\end{abstract} \section{Introduction}
\label{sec:intro}

The majority of NLP research addresses a \textit{single} text, typically at the sentence or document level. This has been the case for both infrastructure language analysis tasks, such as syntactic, semantic and discourse analysis, as well as applied tasks, such as question answering (and its reading comprehension variant \cite{xu-etal-2019-bert}), information extraction, sentiment analysis etc., where the system output is typically extracted from a single document. Yet, there are important applications which are concerned with aggregated information spread across multiple texts, e.g., multi-document summarization~\cite{fabbri-etal-2019-multi}, cross-document coreference resolution~\cite{cybulska2014using}, and multi-hop question answering~\cite{yang-etal-2018-hotpotqa}. While providing state-of-the-art results for cross-document tasks, current pretraining methods, developed for a single text, are not geared to fully address the needs of cross-document tasks. As an alternate approach, we propose Cross-Document Language Model (CD-LM), a new language model (LM) that is trained in a cross-document manner. We show that this significantly outperforms previous approaches, resulting in state-of-the-art results for event and entity cross-document coreference resolution, paper citation recommendation, and documents plagiarism detection.

Tasks that consider multiple documents typically require mapping or linking between pieces of information \emph{across} documents. Such input documents usually contain overlapping information, e.g., Doc 1 and 2 in Fig.~\ref{fig:comb_coref_examples}. Desirably, LMs should be able to align between overlapping elements across these related documents. For example, one would expect a competent model to correctly align the events around ``name'' and ``nominates'' in Doc 1 and Doc 2, effectively recognizing their relation even when they are in separate documents. Yet, existing LM pretraining methods do not expose the model to learn such information. Here, we propose a scheme to integrate cross-document knowledge already in pretraining, thus allowing the LM to learn to encode relevant cross-document relationships implicitly.

\begin{figure}
    \centering
    \begin{tabular}{p{7cm}}
    \toprule
    \textbf{Doc 1: }``\emph{... \textbf{\textcolor{blue}{President Obama}} will \textbf{\underline{\textcolor{darkspringgreen}{name}}} \textbf{\textcolor{chamoisee}{Dr. Regina Benjamin}} as U.S. Surgeon General in a Rose Garden announcement late this morning. ...}''\\
    \textbf{Doc 2: }``\emph{... \textbf{\textcolor{blue}{Obama}} \textbf{\underline{\textcolor{darkspringgreen}{nominates}}} new surgeon general: \textbf{\textcolor{chamoisee}{MacArthur ``genius grant" fellow Regina Benjamin.}} ...}'' \\

    \bottomrule
    \end{tabular}
      \caption{Various document examples from the ECB+ dataset. In Doc 1 and Doc 2, underlined words represent coreferering events and the same color represents a coreference cluster: The entity clusters are (``\emph{Dr. Regina Benjamin}'', ``\emph{MacArthur “genius grant” fellow Regina Benjamin}'') and (``\emph{President Obama}'',``\emph{Obama}''), and the single event cluster is (``\emph{name}'',``\emph{nominates}''). These examples are adopted from~\citet{Cattan2020StreamliningCC}.}
    \label{fig:comb_coref_examples}
    \vspace{-3mm}
\end{figure} 
To allow our CD-LM to address large contexts across multiple documents, we leverage the recent appealing architecture of the Longformer model~\cite{beltagy2020longformer}, designed to address long inputs. Specifically, we leverage its global attention mechanism, originally utilized only during task-specific fine-tuning, and extend its use already in pretraing, enabling the model to consider cross-document, as well as long-range within-document, information. While using this mechanism, we introduce a \textit{cross-document masking} approach. This approach considers as input multiple-documents containing related, partly overlapping, information. The model is then challenged to unmask the masked tokens while attending to information in both the same as well as the related documents. This way, the model is encouraged to ``peek" at other documents and map cross-document information, in order to yield better unmasking. Our pretraining procedure yields a generic cross-document language model, which may be leveraged for various cross-document downstream tasks that would need to map information across related texts. As mentioned above, our experiments assess utility of our CD-LM for a range of cross-document tasks, resulting with significant improvements, suggesting its appeal for future work in the cross-document setting. \section{Background}
\label{sec:background}
Transformer-based language models (LMs) \cite{devlin-etal-2019-bert, liu2019roberta, yang2019xlnet} have led to significant performance gains in various natural language understanding tasks, mainly for within-document-related tasks.  They use multiple self-attention layers in order to learn to produce high-quality token representations. They were shown to incorporate contextual knowledge by assigning a representation that is an attentive function of the entire input context. Such models are trained using the Masked Language Modeling (MLM) objective (known as the pretraining phase) - given a piece of text, a model uses the context words surrounding a masked token to try to predict it, and by that, maximizing the likelihood of the input words. 

These models have significantly advanced the state-of-the-art in various NLP tasks, mostly using post-pretraining, finetuning approaches, e.g., question answering \cite{yang2019xlnet}, coreference resolution \cite{joshi-etal-2019-bert}, such as those of the GLUE benchmark \cite{wang-etal-2018-glue}. Importantly, pretrained LMs eliminate
the need for many heavily-engineered and hand-crafted task-specific architectures for downstream tasks. Additionally, \citet{clark-etal-2019-bert} show that BERT’s~\cite{devlin-etal-2019-bert} attention heads encode a substantial amount of linguistic knowledge, such as the ability to represent within-document coreference relations. This enables better performance over downstream tasks, with limited resources of labeled training data.  Despite such models' success in within-document tasks, due to memory and time constraints, they limit the input size and are only able to support a rather small context. Thus, they cannot be readily applied in cross-document tasks where the input size is large.

Recently, several models were suggested to handle these issues and bypass the length constraint, by employing techniques for dealing with the computational and memory obstacles~\cite{tay2020long}. Examples to such architectures include the Longformer~\cite{beltagy2020longformer} BigBird~\cite{zaheer2020big}, and LinFormer~\cite{wang2020linformer}, which were introduced to extend the range of context that can be used, both for the pretraining and fine-tuning stages. Specifically for the Longformer model, which we utilize in this work, a localized sliding window-based attention, termed local attention, was proposed for reducing computation and extending the previous LMs to support longer sequences. This enabled the handling of long context processing by removing the restrictions of long inputs.
In addition, the authors introduced the \textit{global} attention mode, which allows the LM to build representations based on the full input sequence for prediction, and is used during fine-tuning only. Both the local attention and the global attention modes rely on the known self-attentive score~\cite{vaswani2017attention} which is given by:

where the learned linear projection matrices  are partitioned into two distinct sets;  and , for the local and the global attention modes, respectively. During pretraining, the Longformer assigns the \emph{local attention mode} for all tokens to optimize the MLM objective. Before task-specific finetuning, the attention mode is predetermined for each input token, assigning global attention to few targeted tokens (e.g., special tokens) to avoid computational inefficiency.

We hypothesize that the global attention mechanism is useful for learning meaningful representations for modeling cross-document relationships. We propose augmenting the pretraining phase to exploit the global attention mode, rather than using it only for finetuning, as further described in the next section. \section{Cross-Document Language Model}
\label{sec:cdmlm}

Documents that describe the same event, e.g., different news articles that discuss the same story, usually contain overlapping information. Accordingly, many cross-document tasks may leverage from LM infrastructure that encodes information regarding alignment and mapping across texts. For example, for the cross-document coreference resolution task, consider the underlined predicate examples in Fig.~\ref{fig:comb_coref_examples}. One would expect a model to correctly align the events around ``\emph{name}'' and ``\emph{nominates}'', effectively recognizing their coreference relation even when they are in separate documents.


Our approach to cross-document language modeling is based on training a Transformer-based LM on sets (clusters) of documents, all describing the same event. 
Such document clusters are readily available in a variety of existing datasets for cross-document benchmarks, such as summarization (e.g., MultiNews~\cite{fabbri2019multinews}), cross-document coreference resolution (e.g., ECB+~\cite{cybulska-vossen-2014-using}) and cross-documents alignment benchmarks~\cite{zhou-etal-2020-multilevel}.
Training the LM over a set of related documents provides the potential to learn cross-text mapping and alignment capabilities, as part of the contextualization process. 
Indeed, we show that this cross-document pretraining strategy directs the model to utilize information across documents for predicting masked tokens, and helps in multiple cross-document downstream tasks. 

To support contextualizing information across multiple documents, we need to use efficient Transformer models that scale linearly with input length. Thus, we base our model on the Longformer~\cite{beltagy2020longformer}. As described in Sec.~\ref{sec:background}, this is an efficient Transformer model for long sequences that uses a combination of \emph{local attention} (self-attention restricted to a local sliding window) and \emph{global attention} (a small set of pre-specified input locations with direct global attention access).

\begin{figure*}[tb!!]
\centering
\includegraphics[width=0.98\linewidth]{figures/cdmlm_fig}
\caption{CD-LM pretraining: The input consists of concatenated documents, separated by the new special document separator tokens. The token colored in yellow represents global attention, and tokens colored in blue represent local attention. The goal is to predict the masked token given the output representation , based on the global context, i.e, the entire set of documents in the sample.} \label{fig:cdmlm}
    \vspace{-3mm}
\end{figure*}

\paragraph{Cross-Document Masking} In pretraining, we concatenate the document set using new special document separator tokens, {\tt doc-s} and {\tt \textbackslash doc-s}, for marking document boundaries. We apply a similar masking procedure as in BERT: For each training example, we chose randomly a sample of tokens (15\%) to be masked\footnote{For more details, see the masking procedure of BERT~\cite{devlin-etal-2019-bert}}; for each of those, our proposed model tries to predict it considering the \emph{full} document set, by assigning them global attention. This allows the global attention parameters of the Longformer to contextualize encompassing both information across documents, and within-document long dependencies. The non-masked tokens use local attention, as usual. 

An illustration of the full cross-document masking procedure is depicted in Fig.~\ref{fig:cdmlm}, where the masked token associated with ``\emph{nominates}'' (colored in orange) globally attends to the whole sequence, and the non-masked token hiding the word ``\emph{new}'' (colored in blue) attends to the local context. With regard to the example in Fig.~\ref{fig:comb_coref_examples}, this masking approach aims to implicitly compel the model to learn to correctly predict the word ``\emph{nominates}'' by looking at the second document, optimally at the phrase ``\emph{name}'', and thus enforce the alignment between the events. 

The loss function induced by the above masking method requires a MLM objective which accounts for the entire sequence, namely, the concatenated documents. We mimic the LM bidirectional conditioning from BERT~\cite{devlin-etal-2019-bert} but instead of using constant model weights for all tokens, we assign the global attention weights  for the masked tokens, so the model can predict the target token in a multi-document context. The unmasked tokens use the local attention weights, . We dub this method \emph{Cross-document masked language modeling} (CD-MLM). The resulting model includes the following new components: new pretrained special document separator, and pretrained sets of both global and local attention weights that form the cross-document language model (CD-LM). The document separator tokens can be useful for downstream tasks for marking the document bounderies while the global attention weights provide better encoding of cross-document self-attentive information.

\paragraph{Finetuning} During finetuning on downstream cross-document tasks, we utilize our model by concatenating the tokens of relevant input documents using the document separator tokens, along with the classification token (referred to as {\tt CLS}) at the beginning of the input sequence. Moreover, for token-level tasks such as coreference resolution, we assign global attention to several explicit spans of text, as described in Section~\ref{subsec:fdcdcoref}. Using global attention on at least one token ensures that the distribution of the data during finetuning is similar to the distribution during pretraining, which avoids pretraining-finetuning discrepancy.
Note that this method is much simpler than existing task-specific cross-document models. \section{CD-LM Implementation}
\label{sec:res}

In this section, we provide experimental details used for pretraining our CD-LM model, and detail the ablations we used. 

\paragraph{Corpus data} We use the Multi-News dataset~\cite{fabbri-etal-2019-multi} as the source of related documents for the pretraining. This large-scale dataset contains 44,972 training documents-summary clusters which are originally intended for multi-document summarization. The number of related source documents (that describe the same event) per summary varies from 2 to 10, as detailed in Appendix~\ref{subsec:multinews_data}. We discard the summaries and consider each cluster of related documents, of at least 3 documents, for our cross-document pretraining scheme. We compiled the training corpus by concatenating related documents that were sampled randomly from each cluster, until reaching the input sequence length limit of 4,096 tokens per sample.  The average input contains ~2.5k tokens and the 90th percentile of input lengths is ~3.8K tokens.

\paragraph{Training and hyperparameters}
We pretrain the model according to our CD-MLM strategy described in Section~\ref{sec:cdmlm}. To that end, we employ the Longformer-base model \cite{beltagy2020longformer}\footnote{HuggingFace implmentation, \url{https://github.com/huggingface/transformers}.} and continue its pretraining for additional 25k steps. We use the same hyperparameters and follow the exact setting as in \citet{beltagy2020longformer}: Input sequences are of the length of 4,096, effective batch size of 64 (using gradient accumulation and batch size of 8), a maximum learning rate of 3e-5, and a linear warmup of 500 steps, followed by a power 3 polynomial decay. For speeding up the training and reducing memory consumption, we used the mixed-precision (16-bits) training mode.\footnote{The pretraining took 8 days, using eight 48GB RTX8000 GPUs.} The rest of the hyperparameters are the same as for RoBERTa~\cite{liu2019roberta}.

\paragraph{Baseline Language Models} In addition to our proposed \textbf{\textsc{CD-LM}} model and the state-of-the-art models detailed in the next sections, we considered the following LM variations in our evaluations, as ablations for our model:
\begin{itemize}[leftmargin=8pt]
 \setlength\itemsep{0em}
\item The plain \textbf{\textsc{Longformer}}-base model, without further pretraining.
\item The \textbf{\textsc{Rand CD-LM}} model based on the Longformer-base model, with the additional CD-MLM pretraining but using random, unrelated documents from various clusters. The amounts of data and pretraining hypyerparameters are the same as the ones of CD-LM. This baseline model can asses whether pretraining using related documents is beneficial.
\end{itemize}

When finetuning each one of the above models, we restrict each input segment (document/abstract/passage) to include a maximal length of 2,047 tokens, so that the entire input length, including the {\tt CLS} token, will have no more than 4,096 input tokens. \section{Evaluations and Results}
\label{sec:eval}
This section presents the intrinsic and extrinsic experiments conducted to evaluate our CD-LM. For the intrinsic evaluation we measure the perplexity of the models, while for extrinsic evaluations we considered the event and entity cross-document coreference resolution, paper citation recommendation, and the document plagiarism detection tasks.
\subsection{Cross-Document Perplexity}
\label{subsec:cdppl}
First, we conduct a cross-document perplexity experiment, in a task-independent manner, to asses the contribution of the pretraining process. We used the MultiNews validation and test sets, each of them containing 5,622 documents-summary clusters, to construct the evaluation corpora. Then we followed the same protocol from the pretraining phase - random 15\% of the input tokens are masked and assigned with global attention, and the challenge is to predict the masked token given all documents in the input sequence. The perplexity is then measured by computing exponentiation of the loss. 


The results are depicted in Table~\ref{tab:ppl}. The \textsc{CD-LM} model outperforms the baselines. In particular, the advantage over \textsc{Rand CD-LM}, which was pretrained equivalently over an equivalent amount of (unrelated) cross-document data, confirms that cross-document masking, in pretraining over related documents, indeed helps for cross-document masked token prediction across such document. The \textsc{CD-LM} is encouraged to look at the full sequence, when predicting a masked token. Therefore, it exploits related information in other documents as well, and not just local context. The \textsc{Rand CD-LM} is inferior since, in its pretraining phase, it was not exposed to such overlapping useful information. The plain \textsc{Longformer} model, which is reported just as a reference point, is expected to have difficulty to predict cross-document tokens, in addition to the reason above, since the document separators we used are not part of its embedding set and are randomly initialized during this task. Moreover, recall that the \textsc{CD-LM} and the \textsc{Rand CD-LM} models have two pretrained sets of linear projection weights - one for local attention and one for global attention. The plain \textsc{Longformer} model uses the same weights for the two modes, and therefore it is reasonable that it will fail at long-range mask prediction.

\begin{table}[tb!]
\centering
    \small
  \def\arraystretch{1.12}\tabcolsep=6pt    
\begin{tabular}{lll}
                      \toprule
Model               & Validation           & Test     \\ \toprule
\textsc{Longformer} & 2.03 & 2.34\\
\textsc{Rand CD-LM} & 1.88 & 1.93\\
\textsc{CD-LM} & \textbf{1.69} & \textbf{1.76}\\

\bottomrule
\end{tabular}
\caption{Cross-document perplexity evaluation on the validation set and test set of MultiNews. The lower is better.}
\label{tab:ppl}
\vspace{-3mm}
\end{table}
  \subsection{Cross-Document Coreference Resolution}
\label{subsec:fdcdcoref}


Cross-document (CD) coreference resolution deals with identifying and clustering together textual mentions across multiple documents that refer to the same concept (see examples in Doc 1 and Doc 2 in Fig.~\ref{fig:comb_coref_examples}). The considered mentions can be both entity mentions, usually noun phrases, e.g., ``\emph{Obama}'' and ``\emph{President Obama}'', and event mentions, which are mostly verbs or nominalizations that appear in the text, e.g., ``\emph{name}'' and ``\emph{nominates}''. 

\paragraph{Benchmark.} For assessing our CD-LM on CD coreference resolution, we utilized it for an evaluation over the ECB+ corpus~\cite{cybulska2014using}, which is the most commonly used dataset for the task. ECB+ consists of within- and cross-document coreference annotations for entities and events. The ECB+ dataset statistics are described in Appendix~\ref{subsec:ecb_data}. Following previous work, for comparison, we conduct our experiments on gold event and entity mentions.

For evaluating the performance of coreference clustering we follow the standard coreference resolution evaluation metrics: \textbf{MUC}~\cite{vilain1995model}, ~\cite{bagga1998algorithms}, \textbf{CEAFe}~\cite{luo2005coreference}, their average \textbf{CoNLL F1}, and the more recent \textbf{LEA} metric~\cite{moosavi-strube-2016-coreference}.



\paragraph{Algorithm.} 
Recent approaches for CD coreference resolution train a pairwise scorer to learn the likelihood that two mentions are coreferring. At inference time, an agglomerative clustering based on the pairwise scores is applied, to form the coreference clusters. Next, we detail our proposed modifications for the pairwise scorer.
The current state-of-the-art models~\cite{zeng-etal-2020-event,yu2020paired} train the pairwise scorer by including only the local contexts (containing sentences) of the candidate mentions. They concatenate the two input sentences and feed them into a transformer-based LM. Then, part of the resulting tokens representations are aggregated into a single feature vector which is passed into an additional MLP-based scorer to produce the coreference probability estimate.
To accommodate our proposed CD-LM model, we modify this modeling, as illustrated in Fig~\ref{fig:cdcoref}. We include the entire documents containing the two candidate mentions, instead of just their containing sentences. We concatenate the relevant documents using the special document separator tokens, then encode them using our CD-LM along with the {\tt s} token (corresponding to the {\tt CLS} token) at the beginning of this sequence, as suggested in Section~\ref{sec:cdmlm}. For within-document coreference candidate examples, we use just the single containing document with the document separator. Inspired by \citet{yu2020paired}, we use candidate mention marking: we wrap the mentions with special tokens {\tt m} and {\tt \textbackslash m} in order to direct the model to specifically pay attention to the candidates representations. Additionally, we assign global-attention to {\tt s}, {\tt m}, {\tt \textbackslash m}, and the mention tokens themselves, according to the finetuning strategy proposed in Section~\ref{sec:cdmlm}. Our final pairwise-mention representation is formed like in~\citet{zeng-etal-2020-event} and~\citet{yu2020paired}: We concatenate the cross-document contextualized representation vectors for the  sample:

where  denotes the concatenation operator,  is the cross-document contextualized representation vector of the {\tt CLS} token, and each of  and  is the sum of candidate tokens of the corresponding mentions ( and ). Then, we train the pairwise scorer according to the suggested finetuning scheme. At test time, similar to most recent works, we apply agglomerative clustering to merge the most similar cluster pairs. The hyperparameters and further details are elaborated in Appendix~\ref{subsec:app_coref}.

\begin{figure}[tb]
\centering
\includegraphics[width=0.99\linewidth]{figures/cd-coref}
\caption{CD-coreference resolution pairwise mention representation, using the CD-LM. ,  and  are the cross-document contextualized representation vectors for mentions  and , and of the {\tt CLS} token, respectively.  is the element-wise product between  and .  is the final produced pairwise-mention representation. The tokens colored in yellow represent global attention, and tokens colored in blue represent local attention.} 
\label{fig:cdcoref}
\end{figure}


\paragraph{Baselines.} We consider recent, state-of-the-art baselines that reported results over the ECB+ benchmark. The following baselines were used for both event and entity coreference resolution:





\setlength{\tabcolsep}{2.2pt}

\begin{table*}[!ht]
    \centering
    \small
    \begin{tabular}{@{}lcccccccccccccccclc@{}}
    \toprule
    \multicolumn{1}{c}{Model}&\phantom{abc}& \multicolumn{3}{c}{MUC} && \multicolumn{3}{c}{} & & \multicolumn{3}{c}{} && \multicolumn{3}{c}{LEA} && CoNLL\\
    \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-13} \cmidrule{15-17} \cmidrule{19-19}
    && R & P &  && R & P &  && R &P &  && R &P &  &&   \\ 
   \midrule
        Same Head-Lemma &&  76.5 & 80.0 & 78.2 && 71.8 & 85.0 & 77.8 && 75.5 & 71.8 & 73.6 && 57.0 & 72.0 & 63.7 && 76.5\\
        \citet{barhom-etal-2019-revisiting} && 78.1 & 84.0 & 80.9 && 76.8 & 86.1 & 81.2 && 79.6 & 73.3 & 76.3 && 64.6 & 72.3 & 68.3 && 79.5\\
        \citet{meged-etal-2020-paraphrasing} && 78.8 & 84.7 & 81.6 && 75.9 & 85.9 & 80.6 && 81.1 & 74.8 & 77.8 && 64.7 & 73.4 & 68.8 && 80.0\\
        \citet{Cattan2020StreamliningCC} &&  85.1 & 81.9 & 83.5 && 82.1 & 82.7 & 82.4 && 75.2 & 78.9 & 77.0 && 68.8 & 72.0 & 70.4 && 81.0 \\
        \citet{zeng-etal-2020-event} &&  85.6 & 89.3 & 87.5 && 77.6 & 89.7 & 83.2 && 84.5 & 80.1 & \textbf{82.3}  && - & - & - &&  84.3   \\
        \citet{yu2020paired} &&  88.1 & 85.1 & 86.6 && 86.1 & 84.7 & 85.4 && 79.6 & 83.1 & 81.3 && - & - & - &&  84.4   \\
        \midrule
        \textsc{Longformer} && 85.2  & 88.8 & 86.9 && 83.8 & 87.7 & 85.7 && 83.4 & 79.0 & 81.1 && 75.1 & 77.3 & 76.2 && 84.6 \\
        Rand CD-LM && 81.8  & 88.8 & 85.2 && 83.9 & 84.5 & 84.2 && 83.3 & 79.1 & 81.1 && 72.6 & 74.4 & 73.5 && 83.5\\
        CD-LM && 87.1  & 89.2 & \textbf{88.1} && 84.9 & 87.9 & \textbf{86.4} && 83.3 & 81.2 & 82.2 && 76.7 & 77.2 & \textbf{76.9} && \textbf{85.6} \\

    \bottomrule
    \end{tabular}
    \caption{Results on event cross-document coreference on ECB+ test set.}
    \label{tab:subtopic_results_event}
        \vspace{-3mm}

\end{table*}



\begin{table*}[!ht]
    \centering
        \small
    \begin{tabular}{@{}lcccccccccccccccclc@{}}
    \toprule
    \multicolumn{1}{c}{Model}&\phantom{abcd}& \multicolumn{3}{c}{MUC} && \multicolumn{3}{c}{} & & \multicolumn{3}{c}{} && \multicolumn{3}{c}{LEA} && CoNLL\\
    \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-13} \cmidrule{15-17} \cmidrule{19-19}
    && R & P &  && R & P &  && R &P &  && R &P &  &&   \\ 
    \midrule
        Same Head-Lemma &&   71.3 & 83.0 & 76.7 && 53.4 & 84.9 & 65.6 && 70.1 & 52.5 & 60.0 && 40.6 & 69.1 & 51.1 && 67.4\\
        \citet{barhom-etal-2019-revisiting} &&  81.0 & 80.8 & 80.9 && 66.8 & 75.5 & 70.9 && 62.5 & 62.8 & 62.7 && 53.5 & 63.8 & 58.2 && 71.5\\
        \citet{Cattan2020StreamliningCC} &&  85.7 & 81.7 & 83.6 && 70.7 & 74.8 & 72.7 && 59.3 & 67.4 & 63.1 && 56.8 & 65.8 & 61.0 && 73.1\\
        \midrule
        \textsc{Longformer} && 84.3  & 91.8 & 87.9 &&  82.7 & 81.7  & \textbf{82.2} &&  70.9 & 71.1   & 71.0  &&  72.5 & 73.1  & 72.8 && 80.4 \\
        \textsc{Rand CD-LM} &&  84.4  & 91.2  & 87.7 &&  80.1 & 80.8  & 80.4 &&  74.1 & 71.0  & 72.5 && 72.1 & 75.2  & 73.6 && 80.2\\
        \textsc{CD-LM} && 88.1  & 91.8 &\textbf{89.9} && 82.5 & 81.7 & 82.1 && 81.2 & 72.9 & \textbf{76.8} && 76.4 & 73.0 & \textbf{74.7} && \textbf{82.9} \\

    \bottomrule
    \end{tabular}
    \caption{Results on entity cross-document coreference on ECB+ test set.}
    \label{tab:subtopic_results_entity}
    \vspace{-3mm}
\end{table*} 

{\bf Same Head-Lemma} is a simple baseline that merges mentions sharing the same syntactic head-lemma into the same coreference cluster.

{\citet{barhom-etal-2019-revisiting}} is a model trained jointly for solving both event and entity coreference as a single task. 

{\citet{Cattan2020StreamliningCC}} is a model trained in an end-to-end manner (jointly learning mention detection and coreference), employing the RoBERTa-large model to encode separately each document and to train a pair-wise scorer on top of these representations. 

\noindent The following baselines were used only for event coreference resolution. They all integrate external linguistic information as additional features to the model:

{\citet{meged-etal-2020-paraphrasing}} is an extension of \citet{barhom-etal-2019-revisiting}, leveraging additional side-information acquired by a  paraphrase resource~\cite{shwartz-etal-2017-acquiring}.

{\citet{zeng-etal-2020-event}} is an end-to-end model, encoding the concatenated two sentences containing the two mentions, by the BERT-large model. Similarly to our proposed algorithm, they feed a MLP-based pairwise scorer with the {\tt CLS} contextualized token representation and an attentive function of the contextualized representation vectors of the candidate mentions.

{\citet{yu2020paired}} is an end-to-end model similar to \citet{zeng-etal-2020-event}, but uses rather RoBERTa-large and does not consider the {\tt CLS} contextualized token representation for the pairwise classification. This is a non-attentive version of Zeng et al's mechanism  for paraphrase detection.

\paragraph{Results.} The results on event and entity CD coreference resolution are depicted in Tables \ref{tab:subtopic_results_event} and \ref{tab:subtopic_results_entity}. All results are statistically significant using bootstrap and permutation tests with ~\cite{dror-etal-2018-hitchhikers}. 
Our CD-LM outperforms the sentence based models~\cite{zeng-etal-2020-event,yu2020paired} on event coreference (+1.2 CoNLL F1) and largely surpasses state-of-the-art results on entity coreference (+9.8 CoNLL F1), even though these models utilize external linguistic argument information,\footnote{They utilized semantic role labeling to add features related to the  arguments of each event mention.} and include many more parameters (large models vs our base model). 
Finally, the \textsc{Rand CD-LM} is inferior to the plain \textsc{Longformer} model, despite the fact that it already has pretrained document separator embeddings. This emphasizes the requirement of pretraining on related documents rather than random ones, which allows better alignment and paraphrasing capabilities, required for coreference detection.

 \subsection{Paper Citation Recommendation \& Plagiarism Detection}
\label{subsec:aan}
We evaluate our CD-LM over citation recommendation and plagiarism detection benchmarks~\citet{zhou-etal-2020-multilevel}, a recently released benchmark for cross-document tasks. These tasks share the same objective - categorizing whether a particular relationship holds between two input documents, and therefore, correspond to binary classification problems.

Citation recommendation deals with detecting whether one reference document should cite the other one, while the plagiarism detection task infers whether one document plagiarizes the other one. To compare with recent state-of-the-art models, we utilized the setup and data selection from~\citet{zhou-etal-2020-multilevel}, which provides three datasets for citation recommendation and one for plagiarism detection. 

\paragraph{Benchmarks.} For citation recommendation, we used the ACL Anthology Network Corpus \citep[AAN;][]{radev2013acl}, the Semantic Scholar Open Corpus \citep[OC;][]{bhagavatula-etal-2018-content}, and the Semantic Scholar Open Research Corpus \citep[S2ORC;][]{lo-etal-2020-s2orc}. For plagiarism detection, we used the Plagiarism Detection Challenge \citep[PAN;][]{potthast2013overview}. 

AAN is composed of computational linguistics papers which were published on the ACL Anthology from 2001 to 2014, OC is composed of computer science and neuroscience papers, S2ORC is composed of open access papers across broad domains of science, and PAN is composed of web documents that contain several kinds of plagiarism phenomena. For further dataset prepossessing details and statistics, see Appendix~\ref{subsec:cda_data}.

\paragraph{Algorithm.} For our models we added the {\tt CLS} token at the beginning of the input sequence and concatenated the pair of texts together, according to the finetuning setup discussed in Section~\ref{sec:cdmlm}. The hyperparameters are further detailed in Appendix~\ref{subsec:app_aan}.

\paragraph{Baselines.} We consider the reported results of the following recent baselines:

{\bf \textsc{SMASH}} \cite{jiang2019semantic} is an attentive hierarchical RNN model, used for tasks related to long-document.  

{\bf \textsc{SMITH}} \cite{Yang-Siamese-2020} is a BERT-based hierarchical model, similar to the previously suggested hierarchical attentive networks (HANs~\cite{yang-etal-2016-hierarchical}).  

{\bf \textsc{BERT-HAN+CDA}} \cite{zhou-etal-2020-multilevel} is a cross-document attentive mechanism (CDA) built on top of Hierarchical Attention Networks (HANs), based on BERT. For more details, see Section~\ref{sec:related}. We report the results of their finetuned model over the datasets \cite[Section~5.2]{zhou-etal-2020-multilevel}. 

Note that both SMASH and SMITH reported results only over the ANN benchmark. In addition, they used a slightly different version of the AAN dataset, and included the full documents, unlike the dataset that \textsc{BERT-HAN+CDA} used, which we utilized as well, that considers only the documents' abstracts.  

\begin{table}[bt]
\centering
\small
  \def\arraystretch{1.12}\tabcolsep=6pt    
\begin{tabular}{llll}
                      \toprule
Model               & Acc           &    \\ \toprule
\textsc{SMASH}* & 80.7       & 80.8 \\
\textsc{SMITH}* & 85.4      & 85.4\\
\textsc{BERT-HAN+CDA} & 82.0      & 82.1\\
\midrule
\textsc{Longformer} & 85.5 & 85.4 \\
\textsc{Rand CD-LM} & 85.7 & 85.7\\
\textsc{CD-LM} & \textbf{88.9} & \textbf{88.8} \\

\bottomrule
\end{tabular}
\caption{Accuracy and  scores of various baselines on the AAN test set. * indicates using a different version of the dataset\footnotemark.}\smallskip 
\label{tab:aan}
\vspace{-3mm}
\end{table}
\footnotetext{Following the most recent work of \citet{zhou-etal-2020-multilevel}, we evaluate our model on their version of the dataset. We also quote the results of \textsc{SMASH} and \textsc{SMITH} methods, even though they used a somewhat different version of this dataset, hence their results are not fully comparable to the results of our model and those of \textsc{BERT-HAN+CDA}.}


\begin{table}[bt]
\centering
\small
  \def\arraystretch{1.12}\tabcolsep=4pt    
\begin{tabular}{lllllll}
                      \toprule
                      & \multicolumn{2}{c}{\textbf{OC}} & \multicolumn{2}{c}{\textbf{S2ORC}} &  \multicolumn{2}{c}{\textbf{PAN}}\\
Model                     & Acc              &            & Acc           &       & Acc &  \\ \toprule
  \textsc{BERT-HAN+CDA}    &            86.7      &        87.2      &        85.1     &    86.3    & 56.8 &   64.6 \\ 
  \midrule
 \textsc{Longformer}    &         93.2         &        93.4      &       95.7       &    95.8    & 81.2  &  80.4  \\ 
 \textsc{Rand CD-LM}    &         93.4         &      93.5        &    94.6          &   94.6     &  81.3 &  79.4  \\ 
 \textsc{CD-LM}         &          \textbf{95.4}       &       \textbf{95.3 }     &      \textbf{96.5}      &  \textbf{96.5}     & \textbf{83.9} & \textbf{82.9}\\ 
\bottomrule
\end{tabular}
\caption{Accuracy and  scores of various models on OC, S2ORC and PAN test sets.}\smallskip
\label{tab:aan2}
\vspace{-3mm}

\end{table}
 
\paragraph{Results.} The results on the citation recommendation over the AAN dataset are depicted in Table~\ref{tab:aan}. We observe that even though several baselines reported results using the full documents, our model outperforms them, using the partial version of the dataset, as in~\cite{zhou-etal-2020-multilevel}. Moreover, unlike our model, the CDA is task-specific since it trains new cross-document weights for each task, yet it is still inferior to our model. The results on the rest of the benchmarks are reported in Table~\ref{tab:aan2}, and as can be seen, our \textsc{CD-LM} consistently outperforms both the prior baseline as well as the \textsc{Longformer} and \textsc{Rand CD-LM} models.   \section{Related Work}
\label{sec:related}

Recently, several works proposed equipping LMs with cross-document processing capabilities, mostly by harnessing sequence-to-sequence architectures.
\citet{lewis2020pre} suggested to pretrain a LM by means of reconstructing a document, given, and conditioned on, related documents. 
They showed that this technique forced the model to learn how to paraphrase the original reconstructed document, leading to significant performance gains on multi-lingual document summarization and retrieval. This work considers a basic retrieval model, that does not consider cross-document interactions at all.\citet{Zhang2020PEGASUSPW} proposed an
end-to-end architecture for improving abstractive summarization. Unlike standard LMs, in their pretraining, several sentences (and not just tokens) are removed from documents, and the model's task is to recover them. A similar approach was also suggested for single document summarization~\cite{zhang-etal-2019-hibert}. The advantage of such self-supervision approaches is that they were proved to produce high-quality summaries without any human annotation, often the bottleneck in purely supervised summarization systems. While these approaches advanced the state-of-the-art sequence-to-sequence tasks, the encoders they employed support the encoding of a single document at a time. In our work, we allow inputs comprised of multiple documents in each sample, to support cross-document contextualization. Nevertheless, the main drawback of such sequence-to-sequence architectures is that they require a massive amount of data and training time in order to obtain a plausibly trained model, while we used a relatively small corpus. 

The closest work to our proposed model is the recent Cross-Document Attention model (CDA)~\cite{zhou-etal-2020-multilevel}. They introduced a cross-document component, that enables document-to-document and sentence-to-document alignments. This model is set on top of existing hierarchical document encoding models~\cite{sun-etal-2018-stance,liu-lapata-2019-hierarchical,guo-etal-2019-hierarchical}, that do not consider information across documents by themselves. CDA suggests influencing the document and sentence representations, by those of other documents, without considering word-to-word information across documents (which might require an additional quadratic number of parameters). This makes such modeling unsuitable for token-level alignment tasks, such as cross-document coreference resolution. Moreover, unlike our proposed model, which employs a generic cross-document pretraining, the CDA mechanism requires learning from scratch the cross-document parameters for each downstream task. Further, they support cross-document attention between two documents, while our method does not restrict the number of input documents, as long as they fit the input length of the Longformer. \section{Conclusion}
We presented a novel pretraining strategy and technique for cross-document language modeling, providing better encoding for cross-document downstream tasks. 
Our primary contributions include cross-document masking over clusters of related documents, driving the model to encode cross-document relationships. This was achieved by extending the use of the global attention mechanism of the Longformer model \cite{beltagy2020longformer} in pretraining, attending to long-range information across and within documents. 
Our experiments assess that leveraging our cross-document language model yields new state-of-the-art results over several cross-document benchmarks, including the fundamental task of cross-document entity and event coreference, while, in fact, employing substantially smaller models. We suggest the attractiveness of our CD-LM for neural encoding in cross-document tasks, and propose future research to extend this framework to support sequence-to-sequence cross-document tasks, such as multi-document abstractive summarization. \section*{Acknowledgments}
We thank Yoav Goldberg and Luke Zettlemoyer for fruitful discussions and helpful feedback. 
\bibliography{anthology,acl2021}
\bibliographystyle{acl_natbib}

\clearpage
\appendix
\section{Dataset Statistics and Details}

In this section, we provide more details about the datasets of the corpus and benchmarks we used during our experiments.

\subsection{MultiNews Corpus}
\label{subsec:multinews_data}
In Table~\ref{tab:multi_stats} we list the number of related documents articles per cluster. This follows the original dataset construction. Note that the datasets and the statistics are taken from~\citet{fabbri-etal-2019-multi}.
\begin{table}[htp]
\centering
    \small
  \def\arraystretch{1.12}\tabcolsep=7pt    
\begin{tabular}{ll}
                      \toprule
\# of docs in cluster               &  Frequency            \\ \toprule
 3 & 12,707      \\
 4 & 5,022     \\
 5 &  1,873      \\
 6 & 763     \\
 7 & 382     \\
 8 & 209     \\
 9 & 89     \\
10 & 33     \\
\midrule
\textbf{Total}  & 21,078     \\

\bottomrule
\end{tabular}
\caption{MultiNews training set statistics.}\smallskip 
\label{tab:multi_stats}
\end{table}
 
\subsection{ECB+ Dataset}
\label{subsec:ecb_data}
In Table~\ref{tab:ecb_stat} we list the statistics about training, development and test splits regarding the topics, documents, mentions and coreference clusters. We follow the data split by previous works~\cite{cybulska2015bag, kenyon-dean-etal-2018-resolving,barhom-etal-2019-revisiting}: Training topics: 1, 3, 4, 6-11, 13- 17, 19-20, 22, 24-33; Validation topics: 2, 5, 12, 18, 21, 23, 34, 35; Test topics: 36-45.
\begin{table}[!hbt]
    \centering
        \small
    \begin{tabular}{@{}lccc@{}}
    \toprule
    & Train & Validation & Test \\
    \midrule
    Topics & 25 & 8 & 10 \\
    Docs & 594 & 196 & 206 \\
    Mentions & 3808/4758 & 1245/1476 & 1780/2055 \\
    Clusters & 411/472 & 129/125 & 182/196 \\
    \bottomrule
    \end{tabular}
    \caption{ECB+ dataset statistics. The slash numbers for Mentions and Clusters represent event/entity statistics.}
    \label{tab:ecb_stat}
\end{table} 
\subsection{Paper Citation Recommendation \& Plagiarism Detection Datasets}
\label{subsec:cda_data}
In Table~\ref{tab:splits} we list the statistics about training, development and test splits for each benchmark, and in Table~\ref{tab:counts} we list the document and examples counts for each benchmark. The statistics are taken from~\citet{zhou-etal-2020-multilevel}.
\begin{table}[htp]
\centering
    \small
  \def\arraystretch{1.2}\tabcolsep=7.5pt    
\begin{tabular}{lccc}
\toprule
  Dataset  & Train& Validation & Test \\
  \toprule
AAN         & 106,592 & 13,324   & 13,324  \\ 
OC          & 240,000 & 30,000  & 30,000 \\ 
S2ORC       & 152,000   & 19000   & 19000  \\
PAN         & 17,968 & 2,908 & 2,906 \\
\toprule
\end{tabular}
\caption{Document-to-Document benchmarks statistics: Details regrading the training, validation, and test splits.} 
\label{tab:splits}
\end{table}

\begin{table}[htp]
      \centering
          \small
  \def\arraystretch{1.2}\tabcolsep=12pt
\begin{tabular}{lcc}
\toprule
    Dataset& \# of doc pairs& \# of docs \\  \toprule
AAN         & 132K & 13K     \\ 
OC & 300K   & 567K    \\ 
S2ORC       & 190K    & 270K    \\
PAN         & 34K & 23K   \\
\toprule
\end{tabular}
\caption{Document-to-Document benchmarks statistics: The reported numbers are the count of document pairs and the count of unique documents.}
\label{tab:counts}
\end{table}
 The preprocessing of the datasets performed by~\citet{zhou-etal-2020-multilevel} includes the following steps: For AAN, only pairs of documents that include abstracts are considered, and only their abstracts are used. For OC, only one citation per paper is considered and the dataset was downsampled significantly. For S2ORC, formed pairs of citing sections and the corresponding abstract in the cited paper are included, and the dataset was downsampled significantly. For PAN, pairs of relevant segments out of the test were extracted. For all the datasets, negative pairs are sampled randomly. Then, a standard preprocessing that includes filtering out characters that are not digits, letters, punctuation, or white space in the texts was performed. The complete dataset statistics are described in Appendix~\ref{subsec:cda_data}. 

\section{Hyperparameters Setting and Training Details}

In this section, we elaborate the hyperparameter choices for our experiments.

\subsection{Cross-Document Coreference Resolution}
\label{subsec:app_coref}
We adopt the same protocol as suggested in~\citet{Cattan2020StreamliningCC}\footnote{we used the implementation taken from \url{https://github.com/ariecattan/cross_encoder}}: Our training set is composed of positive instances which consist of all the pairs of mentions that belong to the same coreference cluster, while the negative examples are randomly sampled. We fine-tune our models for 10 epochs, with an effective batch size of 128. The feature vector is passed through a MLP pairwise scorer that is composed of one hidden layer of the size of 1024, followed by the Tanh activation.

\subsection{Paper Citation Recommendation \& Plagiarism Detection}
\label{subsec:app_aan}
We fine-tune our models and BERT-HAN + CDA for 8 epochs, using a batch size of 32, and used the same hyperparameter setting from \citet[Section~5.2]{zhou-etal-2020-multilevel}\footnote{we used the script \url{https://github.com/XuhuiZhou/CDA/blob/master/BERT-HAN/run_ex_sent.sh}}. We used the mixed-precision training mode, to reduce time and memory consuption. 
\end{document}
