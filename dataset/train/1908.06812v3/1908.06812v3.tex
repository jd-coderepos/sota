\section{Results}
\label{sec:results}

In this section, we describe the testing dataset and the evaluation protocol. We then compare state-of-the-art detectors, quantitatively and qualitatively to our proposed \ac{GLAMpoints}.

\subsection{Testing datasets}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{images/ex_images_dataset.png}
\vspace{-2mm}\caption{Examples of images from the \textit{slit lamp} dataset showing challenging conditions for registration. From left to right: low vascularization and over-exposure leading to weak contrasts and lack of corners, motion blur, focus blur, acquisition artifacts and reflections.}
\label{fig:ex_dataset}
\vspace{-2mm}
\end{figure}

In this study we used the following test datasets:
\begin{enumerate}
\item The \textit{slit lamp} dataset: from retinal videos of 3 patients (different from the ones used for training), a random set of 206 frame pairs was selected as testing samples, with size \SIrange{338}{660}{px} by \SIrange{190}{350}{px}. Examples are shown in Figure~\ref{fig:ex_dataset}. The pairs were selected to have an overlap ranging from 20 to 100\%. They are related by affine transformations and rotations up to 15 degrees. 
Using a dedicated software tool, all pairs of images were manually annotated following common procedures~\cite{chen} with at least 5 corresponding points, which were then used to estimate the ground truth homographies relating the pairs. As the slit lamp images depict small area of retina, it is justified to apply the planar assumption in generating homographies~\cite{Cattin,giancardo}.

\item The \textit{\ac{FIRE}} dataset~\cite{Hernandez-matas2017}: a publicly available retinal image registration dataset with ground truth annotations. It consists of 129 retinal images forming 134 image pairs. The original images of 2912x2912 pixels were-down scaled to 15\% of their original size, to match the resolution of the training set. Examples of such images are shown in Figure~\ref{fig:matches}.
\end{enumerate}
As a pre-processing step for testing on fundus images, we isolated the green channel, applied adaptive histogram equalization and a bilateral filter to reduce noise and enhance the appearance of edges as proposed in~\cite{Zanet}. The effect of pre-processing can be seen in Figure~\ref{matches-slitlamp}.


Even though the focus of this paper is on the retinal images, we also tested the generalization capabilities of our model by evaluating it on natural images. We used the ~\cite{oxford}, ~\cite{Zitnik}, ~\cite{Verdie2015, Jacobs2007} and ~\cite{Yi2015} datasets. More details are provided in the supplementary material.

\subsection{Evaluation criteria}

We evaluated the performance using the following metrics:
\begin{enumerate}
    \item \textbf{Repeatability} describes the percentage of detected points  in image  that are within an -distance () to points  in  after transformation with , where  and  are the sets of extracted points found in common regions to both images:



    \item \textbf{Matching performance.} Matches were found using the \ac{NNDR} strategy, as proposed in~\cite{Lowe2004}: two keypoints are matched if the descriptor distance ratio between the first and the second nearest neighbor is below a certain threshold .  Then, the following metrics were evaluated:
    
\begin{enumerate}[label=(\alph*)]
    \item \textbf{AUC}, area under the ROC curve created by varying the value of , following ~\cite{Dahl2011,Winder,Windera}.  
    
    \item \textbf{M.score}, the ratio of correct matches over the total number of keypoints extracted by the detector in the shared viewpoint region~\cite{Mikolajczyk2005}.
    
    \item \textbf{Coverage fraction}, measures the coverage of an image by correctly matched key points. A coverage mask was generated from true positive key points, each one adding a disk of fixed radius (25px) as in~\cite{Aldana-iuit}. 
\end{enumerate}

We computed the homography  relating the reference to the transformed image  by applying \ac{RanSaC} algorithm to remove outliers from the detected matches.

\item \textbf{Registration success rate.} We furthermore evaluated the  registration accuracy achieved after using key points computed by different detectors as in~\cite{Chen2010,Wang2015}.  To do so, we compared the reprojection error of six fixed points of the reference image (denoted as ) onto the other. For each image pair for which a homography was found, the quality of the registration was assessed with 
the median error (\textbf{MEE}) and the maximum error (\textbf{MAE}) of the distances between corresponding points after transformation.


Using these metrics, we defined different thresholds on MEE and MAE that define ,  and  registrations. We consider registration  if not enough keypoints or matches were found to compute a homography (minimum 4), if it involves a flip or if the estimated scaling component is greater than 4 or smaller than . We classified the result as  when  and  and as  otherwise. The values for the thresholds were found empirically by post-viewing the results. 
Using the above definitions, we calculated the \textbf{success rate of each class}, equal to the percentage of image pairs for which the registration falls into each category. These metrics are the most important quantitative evaluation criteria of the overall performance in a real-world setting.

\end{enumerate}

\subsection{Baselines and implementation details}

To evaluate the performance of our GLAMpoints detector associated with root-SIFT descriptor, we compared its matching ability and registration quality against well known detectors and descriptors. Among them, SIFT~\cite{sift}, KAZE~\cite{Alcantarilla2012} and LIFT~\cite{LIFT} were shown to perform well on fundus images by Truong \etal~\cite{truong}. Moreover, we compared our method to other CNN-based detectors-descriptors: \ac{LF-NET}~\cite{Trulls} and SuperPoint~\cite{Detone}. We used the authors' implementation of LIFT (pretrained on Picadilly), SuperPoint and \ac{LF-NET} (pretrained on indoor data, which gives better results on fundus images than the version pretrained on outdoor data) and OpenCV implementation for \ac{SIFT} and KAZE. A rotation-dependent version of root-\ac{SIFT} descriptor is used due to its better performance on our test set compared to the rotation invariant version. For the remainder of the paper, \ac{SIFT} descriptor refers to root-SIFT, rotation-dependent, except if otherwise stated.



Training of \ac{GLAMpoints} was performed using Tensorflow \cite{tensorflow2015-whitepaper} with mini-batch size of 5 and the Adam optimizer \cite{Kingma2015} with learning rate  and  = (0.9, 0.999) for 35 epochs. For each batch we randomly cropped  patches of the full-resolution image to speed up the computation. \ac{GLAMpoints} (NMS10) was trained and tested with a \ac{NMS} window  equal to 10px. It must be noted that other \ac{NMS} windows can be applied, which obtain similar performance.

\subsection{Quantitative results on the \textbf{\textit{slit lamp}} dataset}

\begin{figure*}
    \centering
    \includegraphics[width=0.99\textwidth]{images/merged_results_files_bar_plot_6.pdf}
    \caption{Summary of detector/descriptor performance metrics evaluated over 206 pairs of the \textit{slit lamp} dataset.}
    \label{results-slitlamp}
\end{figure*}


Table~\ref{tab:slitlamp_quantitative} presents the success rate of registration evaluated on the \textit{slit lamp} dataset. Without pre-processing, most detectors show lower performance compared to the pre-processed images, but GLAMpoints performs well even on raw images. While the success rate of acceptable registrations of \ac{SIFT}, KAZE and SuperPoint drops by 20 to 30\% between pre-processed and raw images, \ac{GLAMpoints} as well as \ac{LIFT} and \ac{LF-NET} show only a decrease of 3 to 6\%. Besides, \ac{LF-NET}, \ac{LIFT} and \ac{GLAMpoints} detect a steady average number of keypoints (around 485 for preprocessed and 350 non-preprocessed) independently of the pre-processing, whereas the other detectors see a reduction half. In general, GLAMpoints shows the highest performance for both raw and pre-processed images in terms of registration success rate. The robust results of our method indicate that while our detector performs as well or better on good quality images compared to the heuristic-based methods, its performance does not drop on lower quality images.


\begin{table}[t]
\centering
\caption{Success rates (\%) per registration class for each detector on the 206 images of the \textit{slit lamp} dataset. When the original descriptor is not used in association with detector, the descriptor used is indicated in parenthesis.}
(a) Raw data \\
\resizebox{0.44\textwidth}{!}{\begin{tabular}{llll}
\toprule
                  & Failed {[}\%{]} & Inaccurate {[}\%{]} & Acceptable {[}\%{]} \\ \midrule
SIFT        & 14.56  & 63.11                 & 22.33                 \\
KAZE        & 24.27  & 61.65    & 14.08                 \\
SuperPoint & 17.48  & 48.54                 & 33.98                 \\
LIFT        & 0.0    & 43.69                 & 56.31                 \\
\ac{LF-NET}    & 0.0                & 39.81                 & 60.19                 \\
GLAMpoints (SIFT)  & 0.0    & 36.41                 & \textbf{63.59}                 \\
\bottomrule
\end{tabular}}
\centering
\vspace{3mm}\\ (b) Pre-processed data \\
\resizebox{0.44\textwidth}{!}{\begin{tabular}{llll}
\toprule
Detector &  Failed {[}\%{]} & Inaccurate {[}\%{]} & Acceptable {[}\%{]} \\ \midrule
ORB  & 9.71 & 83.01 & 7.28 \\
GLAMpoints (ORB) & 0.0 & 88.35 & \textbf{11.65} \\
BRISK & 16.99 & 66.02 & 16.99 \\
GLAMpoints (BRISK) & 1.94 & 75.73 & \textbf{22.33}
\\ \midrule\midrule
SIFT & 1.94 & 47.75 & 50.49 \\
KAZE  & 1.46            & 54.85                          & 43.69  \\
KAZE (SIFT)  & 4.37               & 57.28                 & 38.35   \\
SuperPoint & 7.77            & 51.46                          & 40.78\\
SuperPoint (SIFT) & 6.80  & 54.37                 & 38.83  \\
LIFT & 0.0 & 39.81 & 60.19 \\
LF-NET & 0.0                & 36.89                 & 63.11       \\
LF-NET (SIFT) & 0.0                & 40.29                 & 59.71 \\
GLAMpoints (SIFT)  & 0.0             & 31.55                          & \textbf{68.45}
\\\midrule\midrule
Random grid (SIFT)  & 0.0    & 62.62                 & 37.38  \\
\bottomrule
\end{tabular}}
\label{tab:slitlamp_quantitative}\vspace{-3mm}
\end{table}




While \ac{SIFT} extracts a large number of keypoints (205.69 on average for unprocessed images and 431.03 for pre-processed), they appear in clusters as shown in Figure~\ref{matches-slitlamp}. As a result, even if the repeatability is relatively high, the close positioning of the interest points leads to a large number of rejected matches, as the nearest-neighbours are very close to each other. This is evidenced by the low coverage fraction,  and  (Figure~\ref{results-slitlamp}). With a similar value of repeatability, our approach extracts interest points widely spread and trained for their matching ability (highest \textit{coverage fraction}), resulting in more true positive matches (second highest \textit{M.score} and \textit{AUC}), as shown in Figure~\ref{results-slitlamp}. 

\ac{LF-NET}, similar to SIFT, shows high repeatability, which can be explained by its training strategy, which preferred repeatability over accurate matching objective. However, its \textit{M.score} and \textit{AUC} are in the bottom part of the ranking (Figure~\ref{results-slitlamp}). While the performance of \ac{LF-NET} may increase if it was trained on fundus images, its training procedure requires images pairs with their relative pose and corresponding depth maps, which would be extremely difficult - if not impossible - to obtain for fundus images. 

It is worth noting that SuperPoint scored the highest  and  but in this case the metrics are artificially inflated because very few keypoints are detected (35,88 and 59,21 on average for raw and pre-processed images respectively). This translates to  relatively small coverage fraction and one of the lowest repeatability, leading to few possible correct matches. 


As part of an ablation study, we trained GLAMpoints with different descriptors (Table~\ref{tab:slitlamp_quantitative}b, top). While it performs best with the SIFT descriptor, the results show that for every considered descriptor (SIFT, ORB, BRISK), GLAMpoints improves upon the corresponding original detector.

To benchmark the detection results, we used the descriptors that were developed/trained jointly with the given detector and thus can be considered as optimal. For instance in~\cite{LIFT}, the combination of the LIFT/LIFT detector/descriptor outperformed LIFT/SIFT. For completeness, we present the registration results of baseline detectors combined with root-SIFT descriptor in Table~\ref{tab:slitlamp_quantitative}b, center.  As can be seen, using root-SIFT descriptor does not improve the result compared to the original descriptor.

Finally, to verify that the performance gain of GLAMpoints does not come solely from the uniform and dense spread of the detected keypoints, we computed the success rate for keypoints in a random, uniformly distributed grid (Table~\ref{tab:slitlamp_quantitative}b, bottom), which underperforms in comparison. This shows that our detector predicts not only uniform but also \textit{significant} points.



\begin{table}[t]
\centering
\caption{Success rates (\%) of each detector on \textit{FIRE}.}
\label{tab:success-rate-FIRE}
\vspace{-2mm}\resizebox{0.44\textwidth}{!}{\begin{tabular}{lllll}
\toprule
                                  & Failed {[}\%{]} & Inaccurate {[}\%{]} & Acceptable {[}\%{]} \\ \midrule
SIFT                              & 2.24 & 36.57                 & 61.19                 \\
KAZE                              & 14.18 & 58.21    & 27.61                 \\
SuperPoint                        & 0.0                & 13.43                 & 86.57     \\
LIFT                              & 0.0                & 10.45                 & 89.55           \\
LF-NET                            & 0.0 & 38.06 & 61.94 \\
GLAMpoints (OURS)                 & 0.0                & 5.22    & \textbf{94.78}        \\ \bottomrule
\end{tabular}}
\vspace{-4mm}
\label{tab:FIRE_threshold}
\end{table}

\subsection{Quantitative results on \textbf{\textit{FIRE}} dataset}

Table~\ref{tab:success-rate-FIRE} shows the results for success rates of registrations on FIRE. Our method outperforms baselines both in terms of success rate and global accuracy of non-failed registrations. As all the images in FIRE dataset present good quality with highly contrasted vascularization, we did not apply pre-processing. We also did not find it necessary to use the available background masks to filter out keypoints detected outside of the retina as generally they were not matched and did not contribute to the final registration. 

It is interesting to note the gap of 33.6\% in the success rate of acceptable registrations between \ac{GLAMpoints} and \ac{SIFT}. As both use the same descriptor, this difference can be only explained by the quality of the detector. As can be seen in Figure~\ref{fig:matches}, \ac{SIFT} detects a restricted number of keypoints densely positioned solely on the vascular tree and in the image borders, while \ac{GLAMpoints} extracts interest points over the entire retina, including challenging areas such as the fovea and avascular zones, leading to a substantial rise in the number of correct matches. 

Even though \ac{GLAMpoints} outperforms all other detectors, \ac{LIFT} and SuperPoint also present high performance on the \textit{\ac{FIRE}} dataset. This dataset contains images with well-defined corners on a clearly contrasted vascular tree and \ac{LIFT} extracts keypoints spread over the entire image, while SuperPoint was trained to detect corners on synthetic primitive shapes. However, as evidenced on the \textit{slit lamp} dataset, the performance of SuperPoint strongly deteriorates on images with less clear features. 

\begin{figure*}
\centering
\begin{tabular}{c c}
\includegraphics[width=0.37\textwidth]{images/SIFT_FIRE.png} &
\includegraphics[width=0.37\textwidth]{images/GLAM_FIRE.png} \\
\includegraphics[width=0.37\textwidth]{images/notredame_match_pair3_SIFT_2.png} &
\includegraphics[width=0.37\textwidth]{images/notredame_match_pair3_reti_2.png} \\
a) SIFT & b) GLAMpoints\\
\end{tabular}
\vspace{-2mm}
\caption{Interest points detected by a) SIFT  and b) GLAMpoints and corresponding matches for a pair of images from the \textit{\ac{FIRE}} (top) and \textit{Oxford} (bottom) datasets. Detected points are in white, green matches correspond to true positive, red to false positive. GLAMpoints finds considerably more true positive points than SIFT. }
\label{fig:matches}
\vspace{-3mm}
\end{figure*}



\subsection{Results on natural images}

To further demonstrate a possible extension of our method to other image domains, we computed its predictions on natural images. Note that we used the same GLAMpoints model trained on slit lamp images. 

Globally, \ac{GLAMpoints} reaches a success rate of 75.38\% for acceptable registrations, against 85.13\% for the best performing detector - SIFT with rotation invariance - and 83.59\% for SuperPoint. In terms of ,  and \textit{coverage fraction} it scores respectively second, second and first best. In contrast,  of \ac{GLAMpoints} is only second to last after \ac{SIFT}, KAZE and \ac{LF-NET} even though it successfully registers more images. This result shows once again that repeatability is not the most adequate metric to measure the performance of a detector. The detailed results can be found in the supplementary material. 

Finally, it should be noted that the outdoor images of this dataset are significantly different from medical fundus images and contain much greater variability of structures, which indicates a promising generalization of our model to unseen image domains. 


\subsection{Qualitative results}

In case of slit lamp videos, the end goal is to create retinal mosaics. Using 10 videos containing 25 to 558 images, we generated mosaics by registering consecutive frames using keypoints detected by different methods. We calculated the average number of frames before the registration failed (due to the lack of extracted keypoints or correct matches between a pair of images). Over those 10 videos, the average number of registered frames before failure is 9.98 for \ac{GLAMpoints} and only 1.04 for \ac{SIFT}. 

Example mosaics are presented in Figure~\ref{mosaics}. For the same video, \ac{SIFT} failed after 34 frames when the data was pre-processed and only after 11 frames on the original data. In contrast, \ac{GLAMpoints} successfully registered 53 consecutive raw images, without visual errors. The mosaics were created with frame to frame matching with the blending method of~\cite{Zanet} and without bundle adjustment.

\begin{figure}[t]
\centering
\begin{tabular}{c c c}
    \includegraphics[height=0.16\textwidth]{images/mosaic_SIFT_pre.png} &
    \includegraphics[height=0.16\textwidth]{images/mosaic_SIFT_preproc.png} &
    \includegraphics[height=0.16\textwidth]{images/mosaic_GLAM.png} \\
    a) 11 frames & b)  34 frames & c) 53 frames\\
\end{tabular}
\vspace{-2mm}\caption{Mosaics obtained from registration of consecutive images until failure. a) SIFT, raw images; b) SIFT, pre-processed data; c) \ac{GLAMpoints}, raw data.}
\label{mosaics}
\end{figure}


\subsection{Run time}

The run time of detection is computed over 84 pairs of images with a resolution of 660px by 350px. The \ac{GLAMpoints} architecture was run on a Nvidia GeForce GTX 1080 GPU while \ac{NMS} and \ac{SIFT} used CPU. Mean and standard deviation of run time for \ac{GLAMpoints} and \ac{SIFT} are presented in Table~\ref{run_time}. GLAMpoints is on average significantly faster than SIFT. Importantly, it does not require any time-consuming pre-processing step. 

\begin{table}
\centering
\caption{Average detection run time [ms] per image for GLAMpoints and \ac{SIFT} detectors.}
\vspace{-1mm}\resizebox{0.45\textwidth}{!}{\begin{tabular}{lll}
\toprule
                                    & \ac{GLAMpoints}         & SIFT              \\ \midrule
Pre-processing                      & 0.0                 & 16.64  0.93  \\
Detection image I                   & \hspace{-2mm}\begin{tabular}{l} \textit{\ac{CNN}:} 16.28  96.86 \\ 
\textit{NMS:} 11.2  1.05\end{tabular} & 28.94  1.88  \\

\textbf{Total}                  & \textbf{27.48}  98.74 & 45.58  4.69  \\\bottomrule
\end{tabular}}\label{run_time}
\end{table}

