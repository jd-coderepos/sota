

\documentclass[final]{latex/cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{nopageno}

\newcommand{\sr}[1]{\textcolor{red}{#1}}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}

\newcommand{\drule}{\specialrule{0.2pt}{1pt}{1pt}\specialrule{0.2pt}{0pt}{\belowrulesep}}
\newenvironment{myindentpar}[1]{\begin{list}{}{\setlength{\leftmargin}{#1}}\item[]}
  {\end{list}}
  
\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\texttt{#1}}
\usepackage{kotex}
\newcommand{\todoc}[2]{{\textcolor{#1} {\textbf{[[#2]]}}}}
\newcommand{\todos}[2]{{\textcolor{#1} {\footnotesize[[#2]]}}}
\newcommand{\todoblue}[1]{\todos{blue}{[[#1]]}}
\newcommand{\todomagenta}[1]{\todos{magenta}{[[#1]]}}
\newcommand{\modblue}[1]{\todoc{blue}{[[#1]]}}
\newcommand{\todoreds}[1]{\todos{red}{[[#1]]}}
\newcommand{\todored}[1]{\todoc{red}{\textbf{[[#1]]}}}
\newcommand{\jg}[1]{\todored{JG: #1}}
\newcommand{\choi}[1]{\todomagenta{CHOI: #1}}
\newcommand{\out}[1]{\todoblue{Outline: #1}}
\newcommand{\sh}[1]{\todomagenta{JUNG: #1}}
\newcommand{\yun}[1]{\todomagenta{YUN: #1}}
\newcommand{\tr}[1]{\todomagenta{taery: #1}}
\newcommand{\shmod}[1]{\modblue{#1}}
\newcommand{\todo}[1]{\textcolor{blue}{#1}}
\newcommand{\todow}[1]{\textcolor{red}{#1}}
\newcommand{\todogreen}[1]{\textcolor{cyan}{#1}}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}





\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{0872} \def\confYear{CVPR 2021}


\begin{document}

\title{RobustNet: Improving Domain Generalization in Urban-Scene Segmentation\\via Instance Selective Whitening}



\author{
{Sungha Choi}\affmark[1,3]\,
{Sanghun Jung}\affmark[2]\,
{Huiwon Yun}\affmark[4]\,
{Joanne T. Kim}\affmark[3]
\\
{Seungryong Kim}\affmark[3]\,
{Jaegul Choo}\affmark[2]
\vspace*{0.3cm}
\\
\affaddr\affmark[1]LG AI Research\,
\affmark[2]KAIST\,
\affmark[3]Korea University\,
\affmark[4]Sogang University\\
\vspace*{-0.5cm}
}

\makeatletter
\g@addto@macro\@maketitle{
  \begin{figure}[H]
  \vspace{-0.5cm}
  \setlength{\linewidth}{\textwidth}
  \setlength{\hsize}{\textwidth}
  \centering
  \includegraphics[width=\linewidth]{latex/figures/fig_supp_main.pdf}
\vspace{-0.3cm}
  \caption{Segmentation results on \emph{unseen} domains (\textit{i.e.,} BDD-100K~\cite{yu2020bdd100k} and RainCityscapes~\cite{hu2019depth}) with the models trained on Cityscapes~\cite{Cordts2016Cityscapes}. Note that Cityscapes does not contain the following types of images:
(a) low-illuminated, (b) rainy, and (c) unexpected scenes. Our method makes reasonable predictions in these three cases, while the baseline~\cite{chen2018encoder} model completely fails on them.} 
\label{fig:supp_main}
  \vspace{-0.0cm}
  \end{figure}
}
\makeatother


\maketitle
\blfootnote{\hspace*{-0.15cm}\vspace{0.0cm}* indicates equal contribution}

\vspace{-0.4cm}
\begin{abstract}
\vspace{-0.2cm}
   Enhancing the generalization capability of deep neural networks to unseen domains is crucial for safety-critical applications in the real world such as autonomous driving.
   To address this issue, this paper proposes a novel instance selective whitening loss to improve the robustness of the segmentation networks for unseen domains. 
   Our approach disentangles the domain-specific style and domain-invariant content encoded in higher-order statistics (\textit{i.e.,} feature covariance) of the feature representations and selectively removes only the style information causing domain shift. As shown in Fig.~\ref{fig:supp_main}, our method provides reasonable predictions for (a) low-illuminated, (b) rainy, and (c) unseen structures. These types of images are not included in the training dataset, where the baseline shows a significant performance drop, contrary to ours.
   Being simple yet effective, our approach improves the robustness of various backbone networks without additional computational cost. 
   We conduct extensive experiments in urban-scene segmentation and show the superiority of our approach to existing work. Our code is available at this link\footnote{\vspace{-0.25cm} \url{https://github.com/shachoi/RobustNet}.}.
\vspace{-0.4cm}
\end{abstract}


\vspace{-0.1cm}
\section{Introduction}\label{introduction}
\vspace{-0.05cm}
When deploying deep neural networks (DNNs) trained on a \emph{given} dataset (\textit{i.e.,} source domain) in real-world \emph{unseen} data (\textit{i.e.,} target domain), DNNs often fail to perform properly due to the domain shift. Overcoming this issue is crucial, especially for safety-critical applications such as autonomous driving. In particular, real-world data consist of unexpected and unseen samples, for example, those images taken under diverse illumination, adverse weather conditions, or from different locations. It is generally impossible to model such a full data distribution with limited training data, so reducing the domain gap between source and target domains has been a long-standing problem in computer vision.



\begin{figure}[t!]
\begin{center}
  \includegraphics[width=1.00\linewidth]{latex/figures/fig_introduction_v3.pdf}
\end{center}
\vspace*{-0.1cm}
   \caption{\textbf{Overview of our motivation.} (a) We first identify the feature covariance sensitive to the photometric transformation and examine the tendency of the images in each group.
   (b) Sensitive covariances: Illumination (\textit{i.e.,} style) tends to significantly vary.
   (c) Insensitive covariances: Sensitive to scene structure differences (\textit{i.e.,} content) but unaffected by the photometric transformation.
Accordingly, we aim to \emph{selectively} remove only the style-sensitive covariances that may cause the domain shift.}
\label{fig:overview}
\vspace*{-0.45cm}
\end{figure}

Domain adaptation (DA) is an approach to mitigate the performance degradation caused by such a domain gap~\cite{ben2007analysis,ganin2016domain,hoffman2018cycada,ganin2015unsupervised,zou2018unsupervised,murez2018image,vu2019advent,pan2020unsupervised,saito2018maximum}.
Generally, DA focuses on adapting the source domain distribution to that of the target domain, but it requires access to the samples in the target domain, which limits their applicability. 
When we set the entire real world as a target domain, it is difficult in pactice to obtain data samples that fully cover the target domain.




Domain generalization (DG) overcomes this limitation by improving the robustness of DNNs to arbitrary unseen domains. 
In general, most DG methods~\cite{li2018domain,seo2019learning,dou2019domain,motiian2017unified,muandet2013domain,Ghifary_2015_ICCV,li2017learning,balaji2018metareg,li2019episodic,li2019feature} accomplish this through the learning of a shared representation across multiple source domains. However, collecting such multi-domain datasets is costly and labor-intensive, and furthermore, the performance highly depends on the number of source datasets.


A recent study~\cite{pan2018two} has shown that the DG problem can be addressed by exploiting instance normalization layers~\cite{ulyanov2016instance} instead of relying on multiple source domains, leading to a simple and cost-effective training process. The instance normalization just standardizes features while not considering the correlation between channels.
However, a number of studies~\cite{gatys2015texture,gatys2016image,li2017universal,siarohin2018whitening,roy2019unsupervised,luo2017learning,cho2019image,pan2019switchable,sun2016deep} claim that feature covariance contains domain-specific style such as texture and color. This implies that applying instance normalization to the networks may not be sufficient for domain generalization, because the feature covariance is not considered.
A whitening transformation is a technique that removes feature correlation and makes each feature have unit variance. It has been proven that the feature whitening effectively eliminates domain-specific style information as shown in image translation~\cite{cho2019image}, style transfer~\cite{li2017universal}, and domain adaptation~\cite{pan2019switchable,sun2016deep,roy2019unsupervised}, and thus it may improve the generalization ability of the feature representation, but not yet fully explored in DG.
However, simply adopting the whitening transformation to improve the robustness of DNNs is not straightforward, since it may eliminate domain-specific style and domain-invariant content at the same time. Decoupling the two factors and selectively removing the domain-specific style is the main scope of this paper.


In this paper, we present an instance \emph{selective} whitening loss that alleviates the limitations of the existing whitening transformation for domain generalization, by selectively removing information that causes a domain shift while maintaining a discriminative power of feature within DNNs. Our method does not rely on an explicit \emph{closed-form} whitening transformation, but implicitly encourage the networks to learn such a whitening transformation through the proposed loss function, thus requiring negligible computational cost.
As illustrated in Fig.~\ref{fig:overview}, our method selectively removes only those feature covariances that respond sensitively to photometric augmentation such as color transformation.
Our experiments on urban-scene segmentation in DG settings, performed using several backbone networks, show evidence that our approach consistently boosts the DG performance.


The main contributions include the following:
\vspace{-0.0cm}
\begin{itemize}
\vspace{-0.2cm}
    \item We propose an instance selective whitening loss for domain generalization, which disentangles domain-specific and domain-invariant properties from higher-order statistics of the feature representation and  selectively suppresses domain-specific ones.
\vspace{-0.2cm}
    \item Our proposed loss can easily be used in existing models and significantly improves the generalization ability with negligible computational cost.
\vspace{-0.2cm}
    \item We apply the proposed loss to urban-scene segmentation in a DG setting and show the superiority of our approach over existing approaches in both a qualitative and quantitative manner.
\end{itemize}





























\section{Related Work}
\vspace{-0.15cm}
\paragraph{Domain adaptation and generalization}
It is well known that significant labeling efforts are required so as to ensure the reliable performance of various tasks such as semantic segmentation~\cite{long2015fully,badrinarayanan2017segnet,chen2017rethinking,zhu2019improving,choi2020cars}. To tackle this challenge, domain adaptation (DA) methods were proposed to transfer the knowledge learned from abundant labeled data (\textit{i.e.,} a source domain) to a target domain where labeled data are scarce. In contrast to DA, domain generalization (DG) methods assume that the model cannot access the target domain during training and aim to improve the generalization ability to perform well in an unseen target domain. Various approaches such as meta-learning~\cite{li2017learning,balaji2018metareg,li2019episodic,li2019feature}, 
adversarial training~\cite{li2018domain,li2018deep,rahman2020correlation}, autoencoder~\cite{Ghifary_2015_ICCV,li2018domain}, metric learning~\cite{dou2019domain,motiian2017unified}, 
data augmentation~\cite{yue2019domain,gong2019dlow,zhou2020learning} have been proposed to learn domain-agnostic feature representations.
Recently, several studies~\cite{pan2018two,seo2019learning} have shown the effectiveness of exploiting both batch normalization (BN)~\cite{ioffe2015batch} and instance normalization (IN)~\cite{ulyanov2016instance} within DNNs to solve the DG problem. These studies show that BN improves discriminative ability on features, while IN prevents overfitting on training data, so that generalization performance is improved on unseen domains by combining BN and IN. Especially, IBN-Net~\cite{pan2018two} shows a significant performance improvement with the marginal architectural modification that incorporates the IN layers through training on a single source domain, unlike most DG methods that require multiple source domains. 
This normalization based DG method is attractive because it can be applied as a complement to other DG methods based on multiple source domains.\vspace{-0.45cm}
\paragraph{Semantic segmentation in DG}
Based on the synthetic data such as GTAV~\cite{Richter_2016_ECCV} and SYNTHIA~\cite{Ros_2016_CVPR}, numerous DA studies~\cite{pan2020unsupervised,vu2019advent,saleh2018effective,chen2018road,zou2018unsupervised,hoffman2018cycada,tsai2018learning,ma2018exemplar,zhang2017curriculum} have been proposed in semantic segmentation, but only a few DG studies~\cite{yue2019domain,pan2018two} address semantic segmentation, as the majority of the DG methods mainly focused on image classification.
DA, which can access the target domains, generally has better performance than DG, but DG methods that can handle an arbitrary unseen domain without access to the target domain are mandatory in the real world. This paper focuses on the DG method practically helpful in semantic segmentation where various conditions exist such as adverse weather, diverse illumination, location differences, and so on.

\vspace{-0.45cm}
\paragraph{Feature covariance}
The seminal studies~\cite{gatys2015texture,gatys2016image} have demonstrated that feature correlations (\textit{i.e.,} a gram matrix or covariance matrix) take style information of images. Since then, numerous studies exploit the feature correlation in style transfer~\cite{li2017universal}, image-to-image translation~\cite{cho2019image}, domain adaptation~\cite{roy2019unsupervised,sun2016deep} and networks architecture~\cite{luo2017learning,pan2019switchable,huang2018decorrelated,siarohin2018whitening}. Especially, the whitening transformation that removes feature correlation and makes each feature have unit variance, has been known to help to remove the style information from the feature representations~\cite{li2017universal,pan2019switchable,cho2019image}. Our work explores the whitening transformation to improve domain generalization performance. To the best of our knowledge, this is the first attempt to apply whitening to DG.


\begin{figure*}[ht!]
\vspace*{-0.0cm}
  \centering\includegraphics[width=0.97\linewidth]{latex/figures/fig_method_01.pdf}
  \vspace*{-0.0cm}
  \caption{\textbf{Overall process of our proposed method.} (a) Instance standardization. (b) Deriving a covariance matrix from a standardized feature map. (c) Leaving only the covariance to which the whitening loss is applied. 
  (d) Applying the criterion that measures the mean absolute error between the remaining covariance values and zero. No additional computation is required for inference as the operations in red are used only for training.
  Notations; : intermediate feature map, : standardized feature map, : covariance matrix of the standardized feature map, : matrix for masking, : our proposed instance whitening loss.
}
\label{fig:whitening_overview}
\vspace*{-0.4cm}
\end{figure*}


\vspace*{-0.05cm}
\section{Preliminaries}
\vspace*{-0.1cm}
\paragraph{Whitening transformation (WT)} Let  denote the intermediate feature map, where  is the number of channels,  and  are the spatial dimensions of the feature map, height and width, respectively.\vspace{0.02cm}
WT is a linear transformation that makes the variance term of each channel equal to one and the covariances between each pair of channels equal to zero.
A whitening-transformed feature map\vspace{0.05cm}
 from  satisfies that , where  denotes the identity matrix, and can be computed as

where  is a column vector of ones, and  and  are the mean vector and the covariance matrix, respectively, \textit{i.e.,} 
\vspace{-0.1cm}


Since the covariance matrix  can be further eigen-decomposed such that , where  is the orthogonal matrix of eigenvectors, and  is the diagonal matrix that contains each eigenvalue of the corresponding eigenvector from , we can calculate an inverse square root of the covariance matrix  as
\vspace{-0.05cm}

It has been known that WT can effectively remove style information by being applied to each instance in style transfer~\cite{li2017universal}.

\vspace{-0.4cm}
\paragraph{Limitations of WT}
We can compute the whitening transformation matrix  analytically through Eq.~\eqref{eq_inverse_square_root}, but eigenvalue decomposition is computationally expensive, leading to slow training and inference speed 
and prevents the gradient back-propagation~\cite{huang2018decorrelated, cho2019image}.
To alleviate these problems, previous studies have shown that the goal of WT can be achieved without the eigen-decomposition through the whitening loss~\cite{cho2019image} or approximating the whitening transformation matrix using Newton's iteration~\cite{huang2019iterative,huang2018decorrelated,pan2019switchable}.

Especially, GDWCT~\cite{cho2019image} proposes the deep whitening transformation (DWT) that implicitly makes the covariance matrix  close to the identity matrix  by means of the loss defined as
\vspace{-0.1cm}

where  denotes the arithmetic mean.
GDWCT applies this loss to image-to-image translation for more significant style changes than other methods~\cite{huang2018multimodal,lee2018diverse} of aligning only the first-order statistics (\textit{i.e.,} channel-wise mean and variance).
However, applying these alternative methods of WT to DG is not straightforward. Whitening all covariance elements may diminish feature discrimination~\cite{pan2019switchable,wadia2020whitening} and distort the boundary of an object~\cite{li2018closed,li2017universal} because domain-specific style and domain-invariant content are simultaneously encoded in the covariance of the feature map.



\vspace{-0.0cm}
\section{Proposed Method}\label{sec:proposed_method}
\vspace{-0.05cm}
This section presents our approach to solve the domain generalization problem through whitening the feature representation by mitigating undesirable effects of a whitening transformation.
Our method disentangles the covariance into the encoded style and content so that only the style information can be selectively removed, thus increasing the domain generalization ability. We firstly propose an instance whitening and instance-relaxed loss in Section~\ref{method:deep_instance_whitening} and then finally propose our novel instance selective whitening loss in Section~\ref{method:deep_instance_selective_whitening}.
\vspace{-0.05cm}
\subsection{Instance Whitening Loss}\label{method:deep_instance_whitening}
\vspace{-0.1cm}
This subsection describes a series of steps to transform the input feature into the whitening transformed feature as shown in Fig.~\ref{fig:whitening_overview}. Note that our method is applied to each instance, not to a mini-batch.
Let  denote a diagonal element () and  denote an off-diagonal element () of the covariance matrix  of the intermediate feature map, where , .
The DWT loss in Eq.~\eqref{eq_whitening_loss} 
can be decomposed as
\vspace{-0.05cm}


where  denotes the -th channel of the intermediate feature map . 
Note that Eq.~\eqref{eq_cov_elements_loss1} applies to the diagonal elements, and Eq.~\eqref{eq_cov_elements_loss2} applies to the off-diagonal elements of the covariance matrix.
The optimization process for the whitening loss should minimize both Eq.~\eqref{eq_cov_elements_loss1} and Eq.~\eqref{eq_cov_elements_loss2} simultaneously, but there exists a limitation on it.
The scale of each channel (\textit{i.e.,} ) is forced to increase to the value of  by Eq.~\eqref{eq_cov_elements_loss1} and decrease to zero by Eq.~\eqref{eq_cov_elements_loss2}. 
Therefore, forcing the diagonal and off-diagonal of the covariance matrix to be one and zero, respectively, conflicts with each other, so it is difficult to optimize both at the same time.


To address this issue, the feature map  can first be standardized into  through an instance normalization~\cite{ulyanov2016instance}:
\vspace{-0.1cm}

where  is an element-wise multiplication, and  denotes the column vector consisting of diagonal elements in the covariance matrix. Note that each diagonal element is copied along with the spatial dimension  for element-wise multiplication.
Since the scale of each feature vector is already fixed as the unit value after the instance standardization, the whitening loss only affects the  term in Eq.~\eqref{eq_cov_elements_loss2}. In the end, this approach fits the purpose of the whitening transformation to decorrelate the features.

After standardization of the intermediate feature map, the covariance matrix is calculated as

where  is the standardized feature map.
Thanks to the standardization process, diagonal elements of the covariance matrix are already set as unit values. Thus, we only need to make the off-diagonals of the covariance matrix close to zero, which makes it easy to optimize for the whitening process, and the aforementioned conflict can thus be resolved. Since the covariance matrix is symmetric, the loss can be applied only to the strict upper triangular part. Our instance whitening (IW) loss is formulated as

where  denotes the arithmetic mean and  denotes a strict upper triangular matrix, \textit{i.e.,}



\begin{figure*}[ht!]
\vspace*{-0.0cm}
  \centering\includegraphics[width=0.97\linewidth]{latex/figures/fig_method_02_v1.pdf}
  \vspace*{-0.0cm}
  \caption{\textbf{Instance selective whitening loss}. 
  (a) 
  The variance matrix  is computed out of the covariance matrices of the -th image  and its photometric transformed image  to identify those elements sensitive to the transformation (blue boxes).
  Note that these matrices are symmetric.
  (b) The covariance matrix  is masked by the matrix  to selectively suppress style-sensitive covariances by .
}
\label{fig:disentangling_covariance}
\vspace{-0.45cm}
\end{figure*}









\vspace*{-0.18cm}
\subsection{Margin-based relaxation of whitening loss}
\vspace*{-0.12cm}
The instance whitening loss (Eq.~\eqref{eq_our_whitening_loss}) suppresses all covariance elements to zero, so it can adversely affect the discriminative power of features within DNNs. To address this issue, we propose an instance-\emph{relaxed} whitening (IRW) loss to sustain the covariance elements essential in maintaining the discriminative power. The IRW loss is designed so that the expected value of the total covariance lies within a specified margin  rather than being close to zero, \textit{i.e.,} 

The loss  allows the covariance to have a certain level of values, so it gives room to keep discriminative features intact. The empirical effect of the IRW loss can be found in Section~\ref{exp:effectiveness}. It shows better performance compared to the IW loss not including margin  (Eq.~\eqref{eq_our_whitening_loss}). Nonetheless, it may not be sufficient because we cannot guarantee that only the covariance useful for generalization performance remains through the margin relaxation.







\vspace*{-0.05cm}
\subsection{Separating Covariance Elements}\label{method:deep_instance_selective_whitening}
\vspace*{-0.1cm}
To further improve our approach, we need to separate the covariance terms into two groups: domain-specific style and domain-invariant content. We propose to selectively suppress only the style-encoded covariances that cause the domain shift. Assuming that the domain shift includes changes in color and blurriness, we simulate the domain shift through photometric augmentation such as color jittering and Gaussian blurring.

First, we add only the instance standardization layer into the networks (Fig.~\ref{fig:whitening_overview}(a)) and train them during the  initial epochs
without the whitening loss to get the pure statistics of the covariance matrices from training images.  is a hyper-parameter, which we empirically set to 5. Afterwards, we extract two covariance matrices by inferring from two input images, namely an original and a photometric-transformed image, and calculate the variance matrix from the differences between two different covariance matrices. 
Formally, the variance matrix  is defined as
\vspace*{-0.1cm}

from mean  and variance  for each element from two different covariance matrices of the -th image, \textit{i.e.,}
\vspace*{-0.1cm}


where  denotes the number of image samples,  is the -th image sample,  is a photometric transformation, and  extracts the covariance matrix of the intermediate feature map from an input image.
As a result,  consists of elements of the variance of each covariance element across various photometric transformations.


We assume that the variance matrix  implies the sensitivity of the corresponding covariance to the photometric transformation. This means that the covariance elements with high variance value contain the domain-specific style such as color and blurriness.
To identify such elements, we apply \textit{k}-means clustering on the strict upper triangular elements  of the variance matrix  to assign the elements into  clusters  with respect to the value. 
Next, we split the  clusters into two groups,  with low variance value and  with high variance value.
The hyper-parameters  and  are empirically set to 3 and 1, respectively. More details can be found in the supplementary Section \todow{A.2}.
We assume that  contains the domain-specific style and  contains domain-invariant content.


Finally, we propose an instance \emph{selective} whitening (ISW) loss that selectively suppresses only to the style-encoded covariances.
Let the mask matrix  in Eq.~\eqref{eq_mask} change to  for the ISW loss as
\vspace{-0.1cm}

The ISW loss is defined as
\vspace*{-0.05cm}

The networks continue training for the remaining epochs incorporating the proposed ISW loss.

\begin{figure}[b!]
\vspace{-0.3cm}
\begin{center}
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_method_03.pdf}
\end{center}
\vspace*{-0.3cm}
   \caption{\textbf{Architecture comparison with other methods}: (a) the original residual block~\cite{he2016deep}; (b) IBN-b~\cite{pan2018two} combining instance normalization with batch normalization; (c) IterNorm~\cite{huang2019iterative} employing Newton's iterations for efficient whitening; (d) Our proposed ISW loss applied to instance normalization.
}
\label{fig:architectural_comparison}
\end{figure}

\vspace*{-0.0cm}
\subsection{Network architecture with proposed ISW loss}\label{sec:network_architecture}
\vspace*{-0.05cm}
IBN-Net~\cite{pan2018two} has explored a number of ResNet~\cite{he2016deep}-based architectures to combine instance normalization with batch normalization and proposed several IBN blocks based on a residual block (Fig.~\ref{fig:architectural_comparison}(a)). Among the proposed blocks, IBN-b, which adds an instance normalization layer right after the addition operation of a residual block (Fig.~\ref{fig:architectural_comparison}(b)), shows the best generalization performance on semantic segmentation tasks. After all, they add three instance normalization layers after the first three convolution groups (\textit{i.e.,} conv1, conv2\_x, and conv3\_x). We follow this architectural approach as our baseline. As shown in Fig.~\ref{fig:architectural_comparison}(d), we simply add our proposed ISW loss to the instance normalization layer.
Our loss in total is described as
\vspace{-0.25cm}

where  denotes the weight of our ISW loss and is empirically set to 0.6,  is the task loss (\textit{e.g.,} a per-pixel cross-entropy loss for semantic segmentation),  indicates the layer index, and  is the number of layers to which the ISW loss is applied. The hyper-parameter  is analyzed in the supplementary Section \todow{A.2}.  is set to three by following IBN-Net.
An affine transformation is not used since the subsequent convolution operation after a whitening transformation can do the equivalent job, and empirically, we found no performance gain by explicitly adding the affine transformation.
\vspace{-0.1cm}










\vspace{-0.0cm}
\section{Experiments}
\vspace{-0.1cm}
This section describes the experimental setup and presents evaluation results to assess the effectiveness of our proposed methods on semantic segmentation with comparison to other methods. Furthermore, we provide an in-depth analysis of our results including the covariance matrices.
\vspace{-0.07cm}
\subsection{Experimental Setup}
\vspace{-0.08cm}
We train our model on several datasets (\textit{e.g.,} Cityscapes) and show its performance on other datasets (\textit{e.g.,} BDD-100K, Mapillary, GTAV, and SYNTHIA) to measure the generalization capability on unseen domains.
For fair comparisons with other normalization techniques, we re-implement IBN-Net~\cite{pan2018two} and IterNorm~\cite{huang2019iterative} on our baseline models and compare them with our methods.
As described in Section~\ref{sec:network_architecture}, our proposed loss can easily be added to existing models, so we apply our methods to various backbone networks such as 
ResNet~\cite{he2016deep}, ShuffleNetV2~\cite{ma2018shufflenet} and MobileNetV2~\cite{sandler2018mobilenetv2} and show wide applicability of the proposed methods.
For all the quantitative experiments, mean Intersection over Union (mIoU) is used to measure the segmentation performance.
\vspace{-0.4cm}
\subsubsection{Implementation details}
\vspace{-0.1cm}
We adopt DeepLabV3+~\cite{chen2018encoder} for a semantic segmentation architecture, and SGD optimizer with an initial learning rate of 1e-2 and momentum of 0.9 is used. Besides, we follow the polynomial learning rate scheduling~\cite{liu2015parsenet} with the power of 0.9. We train all the models for 40K iteration, except for multi-source models, which are trained for 110K iterations. To prevent the model from overfitting, color and positional augmentations such as color jittering, Gaussian blur, random cropping, random horizontal flipping, and random scaling with the range of [0.5, 2.0] are conducted. For the photometric transformation in ISW, we apply color jittering and Gaussian blur.
Also, as suggested by IBN-Net, we add three instance normalization layers after the first three convolution groups and apply our proposed loss.
Further details are provided in the supplementary Section \todow{A.3}.
\vspace{-0.35cm}
\subsubsection{Datasets}
\vspace{-0.2cm}
To verify the generalization capability of our methods, we conduct the experiments on five different datasets.
\vspace{-0.48cm}
\paragraph{Real-world datasets} Cityscapes~\cite{Cordts2016Cityscapes} is a large-scale dataset containing high-resolution (\textit{e.g.,} 20481024) urban scene images collected from 50 different cities in primarily Germany. It provides 3,450 finely-annotated images and 20,000 coarsely-annotated images. We use only a finely-annotated set for training and validation.
BDD-100K~\cite{yu2020bdd100k} is another real-world dataset that contains diverse urban driving scene images with the resolution of 1280720. The images are collected from various locations in the US. For a semantic segmentation task, 7,000 training and 1,000 validation images are provided.
The last real-world dataset we use is Mapillary~\cite{neuhold2017mapillary}, a diverse street-view dataset consisting of 25,000 high-resolution images with a minimum resolution of 19201080 collected from all around the world.


\vspace{-0.48cm}
\paragraph{Synthetic datasets} GTAV~\cite{Richter_2016_ECCV} is a large-scale dataset containing 24,966 driving-scene images generated from Grand Theft Auto V game engine. It has 12,403, 6,382, and 6,181 images of size 19141052 for a train, a validation, and a test set, respectively. It has 19 object categories compatible with Cityscapes.
Also, we use SYNTHIA~\cite{ros2016synthia}, composed of photo-realistic synthetic images containing 9,400 samples with a resolution of 960720. 


\vspace{-0.1cm}
\subsection{Quantitative Evaluation}
\vspace{-0.1cm}
This subsection provides ablation studies, the comparisons of our results against other normalization methods, the evaluation on multiple source domains, and the analysis of computational cost. Since the experiments follow domain generalization settings, the model cannot access any datasets other than the source data.
\vspace{-0.38cm}
\subsubsection{Effectiveness of instance selective whitening loss}\label{exp:effectiveness}
\vspace{-0.17cm}
To verify the effectiveness of our methods, we conduct comparisons with other normalization methods and ablation studies on instance whitening (IW), instance-relaxed whitening (IRW), and instance selective whitening (ISW).
\textbf{Note that all the experiments in this subsection are performed three times and averaged for fair comparisons}. 

\begin{table}[t!]
\vspace*{-0.0cm}
\begin{center}
\footnotesize
\begin{tabular}{c|c|c|c|c||c}
\toprule
Models (GTAV) & C & B & M & S & G\\
\drule
Baseline  & 28.95        & 25.14      & 28.18      & 26.23      & 73.45 \\ 
\midrule
SW~\cite{pan2019switchable} & 29.91      & 27.48      & 29.71      & 27.61      & \textbf{73.50}     \\ 
\midrule
IBN-Net~\cite{pan2018two}      & 33.85      & 32.30      & 37.75      & 27.90 & 72.90      \\ 
\midrule
IterNorm~\cite{huang2019iterative} & 31.81      & 32.70      & 33.88      & 27.07      & 73.19     \\ 
\midrule
Ours (IW) & 33.21      & 32.67      & 37.35      & 27.57      & 72.06      \\ 
\midrule
Ours (IRW)                   & 33.57      & 33.18      & 38.42      & 27.29      & 71.96      \\ 
\midrule
Ours (ISW)                   & \textbf{36.58}      & \textbf{35.20} & \textbf{40.33} & \textbf{28.30}      & 72.10      \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.15cm}
\caption{Comparison of mIoU(\%). Compared models are trained on GTAV train set, and validated on Cityscapes (C), BDD-100K (B), Mapillary (M), SYNTHIA (S) and GTAV (G) validation sets. ResNet-50 with an output stride of 16 is used.
 denotes our own re-implemented models. SW denotes Switchable Whitening~\cite{pan2019switchable}.}
\label{tab_best_model_16_gtav}
\vspace*{-0.53cm}
\end{table}

Table~\ref{tab_best_model_16_gtav} shows the generalization performance of the models trained on GTAV dataset. ISW outperforms other methods on all datasets except the source dataset (\textit{i.e.,} GTAV). Especially, ISW shows a significant improvement on real-world datasets (i.e., Cityscapes, BDD-100K, and Mapillary). Table~\ref{tab_best_model_16_city} shows the generalization performance of those models trained on Cityscapes dataset. 
Although IterNorm outperforms our models on GTAV, the performance gap is minimal. 
ISW outperforms other normalization and baseline models on BDD-100K, Mapillary, and SYNTHIA datasets. 

\begin{table}[b!]
\vspace{-0.4cm}
\begin{center}
\footnotesize
\begin{tabular}{c|c|c|c|c||c}
\toprule
Models (Cityscapes) & B & M & G & S & C \\
\drule
Baseline  & 44.96      & 51.68      & 42.55      & 23.29      & \textbf{77.51}  \\ 
\midrule
SW~\cite{pan2019switchable} & 48.49      & 55.82 & 44.87      & 26.10      & 77.30       \\ 
\midrule
IBN-Net~\cite{pan2018two}      & 48.56      & 57.04      & 45.06      & 26.14      & 76.55      \\ 
\midrule
IterNorm~\cite{huang2019iterative} & 49.23      & 56.26 & \textbf{45.73}      & 25.98      & 76.02       \\ 
\midrule
Ours (IW) & 48.19      & 58.90      & 45.21      & 25.81      & 76.06      \\ 
\midrule
Ours (IRW)                   & 48.67 & \textbf{59.20} & 45.64
& 26.05      & 76.13      \\ 
\midrule
Ours (ISW)                   & \textbf{50.73} & 58.64 & 45.00 & \textbf{26.20} & 76.41      \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.17cm}
\caption{Comparison of mIoU(\%). The models are trained on Cityscapes train set. ResNet-50 with an output stride of 16 is used.
 denotes re-implemented models.}
\label{tab_best_model_16_city}
\vspace{-0.2cm}
\end{table}

Baseline, Switchable Whitening (SW), and IBN-Net, which are less generalizable than our method, tend to overfit the source domain, suffering from performance degradation on the target domain due to the large domain shift. 
Our method may sacrifice the performance on the source domains (\textit{i.e.,} training and evaluating on the same dataset) as shown in the last column in Table~\ref{tab_best_model_16_gtav} and ~\ref{tab_best_model_16_city}. 
However, our models shows good generalizability, which is critical when deployed in the wild, where large domain-shift is expected.



Table~\ref{tab_various_backbone_cityscapes} explains the wide applicability of our work. The first group is reported by adopting ShuffleNetV2, and the second group is using MobileNetV2 as backbone networks. In both cases, our model with ISW outperforms the baseline and IBN-Net on real-world datasets.
To further validate the capability of our method, we present the comparison with baselines trained on multiple synthetic domains, GTAV, and SYNTHIA. For the training, we aggregate the training domains without any joint training methodologies.
Learning domain-invariant features across multiple datasets is essential to optimize the model on different distributions of multiple datasets. Table~\ref{tab_multi_source_best} shows our model trained on multiple datasets performs better than other models due to its generalization ability by extracting domain-invariant features during training.


\begin{table}[t!]
\vspace*{-0.0cm}
\begin{center}
\setlength\tabcolsep{5.2pt}
\footnotesize
\begin{tabular}{c|c|c|c|c||c}
\toprule
Models (GTAV) & C & B & M & S & G \\
\drule
Baseline              & 25.56      & 22.17      & 28.60      & 23.33      & \textbf{66.47} \\
\midrule
IBN-Net~\cite{pan2018two} & 27.10     & 31.82 & 34.89      & \textbf{25.56}      & 65.44      \\
\midrule
Ours (ISW)                & \textbf{30.98} & \textbf{32.06}      & \textbf{35.31} & 24.31 & 64.99      \\
\toprule
Baseline              & 25.92      & 25.73      & 26.45  & 24.03      & \textbf{68.12} \\
\midrule
IBN-Net~\cite{pan2018two} & 30.14      & 27.66      & 27.07      & \textbf{24.98}      & 67.66      \\
\midrule
Ours (ISW)              & \textbf{30.86} & \textbf{30.05} & \textbf{30.67} & 24.43 & 67.48      \\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.15cm}
\caption{Comparison of mIoU(\%). The models are trained on GTAV train set. The backbone networks of the first group are ShuffleNetV2~\cite{ma2018shufflenet} and the second group is MobileNetV2~\cite{sandler2018mobilenetv2}.}\label{tab_various_backbone_cityscapes}
\vspace*{-0.55cm}
\end{table}


\begin{table}[h]
\vspace*{-0.15cm}
\begin{center}
\footnotesize
\begin{tabular}{c|c|c|c||c|c}
\toprule
Models (G + S) & C & B & M & G & S \\
\drule
Baseline & 35.46 & 25.09 & 31.94 & 68.48 & 67.99 \\ 
\midrule
IBN-Net & 35.55 & 32.18 & 38.09 & \textbf{69.72} & 66.90 \\
\midrule
\textbf{Ours} & \textbf{37.69} & \textbf{34.09} & \textbf{38.49} & 68.26 & \textbf{68.77} \\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.1cm}
\caption{Comparison of mIoU(\%). The models are trained on multiple synthetic domains. The backbone is ResNet-50 with an output stride of 16.  denotes re-implemented models.}
\label{tab_multi_source_best}
\vspace*{-0.5cm}
\end{table}




\begin{table}[b!]
\vspace*{-0.0cm}
\begin{center}
\setlength\tabcolsep{4.2pt}
\footnotesize
\begin{tabular}{c|cc|cc|cc}
\toprule
Models (GTAV) & \multicolumn{2}{c|}{C} & \multicolumn{2}{c|}{B} & \multicolumn{2}{c}{M} \\
\drule
Baseline & 22.20   & \multirow{2}{*}{7.40 } & \multicolumn{2}{c|}{\multirow{2}{*}{N/A}}  & \multicolumn{2}{c}{\multirow{2}{*}{N/A}} \\ 
IBN-Net~\cite{pan2018two} & 29.60   & & & &  & \\ 
\midrule
Baseline          & 32.45 & \multirow{2}{*}{4.97} & 26.73 & \multirow{2}{*}{5.41} & 25.66 & \multirow{2}{*}{8.46} \\ 
DRPC~\cite{yue2019domain} & \textbf{37.42} & & 32.14 &  & 34.12 &     \\ 
\midrule
Baseline & 28.95 & \multirow{2}{*}{\textbf{7.63}} & 25.14 & \multirow{2}{*}{\textbf{10.06}} & 28.18  & \multirow{2}{*}{\textbf{12.15}}  \\ 
Ours (ISW)   & 36.58 & & \textbf{35.20} &  & \textbf{40.33} & \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.15cm}
\caption{mIoU(\%) comparison with IBN-Net and DRPC trained on GTAV train set. The backbone is ResNet-50. Note that IBN-Net does not report the performance on BDD-100K and Mapillary.}
\label{tab_best_model_8}
\vspace*{-0.2cm}
\end{table}


\vspace{-0.35cm}
\subsubsection{Comparison with other DG and DA methods}
\vspace{-0.2cm}
This subsection compares our method with two existing DG methods on semantic segmentation task, based on the results reported in the papers~\cite{pan2018two,yue2019domain}. DRPC~\cite{yue2019domain} proposes a domain randomization method, which maps the synthetic images to multiple auxiliary real domains using image-to-image translation with the style of real images (\textit{e.g.,} ImageNet). As shown in Table~\ref{tab_best_model_8}, our model gains the largest performance increase on average, compared to other methods such as IBN-Net~\cite{pan2018two} and DRPC~\cite{yue2019domain}. Our method shows a large amount of performance improvement on BDD-100K and Mapillary datasets that involve significantly more diverse driving scenes than Cityscapes.

In addition, we compare the result of our method with those reported from several domain adaptation methods. See the supplementary Section \todow{A.1}.















\begin{table}[t!]
\vspace*{-0.0cm}
\begin{center}
\setlength\tabcolsep{4.2pt}
\footnotesize
\begin{tabular}{c|c|c|c}
\toprule
Models & \# of Params & GFLOPS & Inference Time (ms) \\
\drule
Baseline  & 45.082M & 554.31 &  10.48\\ 
\midrule
IBN-Net~\cite{pan2018two}      & 45.083M & 554.31 &  10.51\\
\midrule
IterNorm~\cite{huang2019iterative} & 45.081M & 554.31 &  40.31\\ 
\midrule
Ours    & 45.081M & 554.31 &  10.43 \\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.2cm}
\caption{Comparison of computational cost. Tested with the image size of 20481024 on NVIDIA A100 GPU. The inference time is averaged over 500 trials.  denotes re-implemented models.}
\label{tab_computational_cost}
\vspace*{-0.5cm}
\end{table}

\vspace{-0.3cm}
\subsubsection{Computational cost analysis}
\vspace{-0.15cm}
To ensure our method requires no additional computational cost, we report the number of parameters, GFLOPS, and inference time. As seen in Fig.~\ref{fig:architectural_comparison}, all the models in Table~\ref{tab_computational_cost} share the same network architecture, but with different normalization methods.
As shown in Table~\ref{tab_computational_cost}, our approach performs a whitening transformation without additional computational cost.

\vspace*{-0.0cm}
\subsection{Qualitative Analysis}
\vspace*{-0.05cm}
\paragraph{Comparison of covariance matrices} To show how the covariance matrix is selectively whitened, we visualize the covariance matrix of intermediate feature maps from IBN-Net~\cite{pan2018two} and our model with ISW. As shown in Fig.~\ref{fig:covariance_matrix}, the first pair of covariance matrices are from the first convolution layer and the others are from the second convolution layer. Note that the style information mainly exists in the early layers of the network as pointed out in IBN-Net. Moreover, the style information is encoded as a form of the features covariance as revealed in previous studies~\cite{gatys2015texture, gatys2016image}. Hence, the covariance matrices are sparser at the second pair, compared to the first ones. By comparing the covariance maps from IBN-Net and ISW, we can find the ones from ours are whitened but a small number of covariance elements remain large, showing our ISW selectively eliminates the covariance.

\begin{figure}[b!]
\vspace*{-0.3cm}
\begin{center}
  \includegraphics[width=0.98\linewidth]{latex/figures/fig_visualization_covariance.pdf}
\end{center}
\vspace*{-0.2cm}
   \caption{Visualization of covariance matrices extracted from IBN-Net and our model. The first and the second pairs are extracted from the first and the second convolution layers, respectively.}
\label{fig:covariance_matrix}
\vspace*{-0.0cm}
\end{figure}




\vspace*{-0.45cm}
\paragraph{Reconstructing images with whitened features} For in-depth analysis, we reconstruct input images from the whitened feature maps of our ISW model. For the experiment, we adopt U-Net~\cite{ronneberger2015u} as reconstruction networks. To newly train a decoder, we append the decoder to the backbone of a pre-trained baseline and train the decoder. We then replace the backbone network with the pre-trained ISW model. As seen in Fig.~\ref{fig:reconstruction}, generated images preserve the relevant content information for segmentation while the style information such as illumination and colors is suppressed. These examples support the validity of our approach that selectively suppresses the style information.



\begin{figure}[t!]
\begin{center}
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_reconstruction.pdf}
\end{center}
\vspace*{-0.3cm}
   \caption{Reconstructed images from ISW-whitened feature maps using U-Net; the first row: a baseline backbone, the second row: an ISW model backbone. The image contents are properly maintained while the style such as illumination and colors vanish.}
\label{fig:reconstruction}
\vspace*{-0.45cm}
\end{figure}

\vspace{-0.1cm}
\section{Discussions}
\vspace{-0.1cm}
In this section, we discuss potential issues and improvements of our approach for further research. \vspace{-10pt}
\vspace{-0.1cm}
\paragraph{Affine parameters.} Most of the normalization layers contain affine parameters to recover the original distribution and enhance the representation of a network. We attempted to deploy this by adding affine parameters or a 11 convolution layer after the normalization layer incorporating our proposed whitening loss. Despite our effort, this approach did not improve our method. We conjecture it is because affine parameters or a 11 convolution layer do not have sufficient complexity in recovering the original distribution.\vspace{-20pt}
\vspace{-0.1cm}
\paragraph{Photometric transformation.} Our method adopted photometric transformation to separate the style and content information, where we found that applying color transform and Gaussian blur does not harm the content information. We expect our approach can be further improved by exploring various photometric augmentation techniques.

\vspace{-0.1cm}
\section{Conclusions}
\vspace{-0.1cm}
This paper proposed a novel instance selective whitening (ISW) loss, which facilitates disentangling the covariances of the intermediate features into the style- and content-related ones and suppressing only the former to learn the domain-invariant feature representation.
We focused on solving the domain generalization problem in urban-scene segmentation, which has practical impact when deployed in the wild but has not been studied much. 
In this regard, we strive to promote the importance of the domain generalization and inspire new research paths in this area.


\vspace*{0.15cm}
{
\small
\noindent\textbf{Acknowledgments}
This work was partially supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program(KAIST) and No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques), the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420).
}


\clearpage
{\small
\bibliographystyle{latex/ieee_fullname}
\bibliography{egbib}
}
\clearpage
\def\thesection{\Alph{section}}
\setcounter{section}{0}
\section{Supplementary Material} \label{supple}
\vspace{-0.1cm}
This supplementary section provides additional quantitative results to examine hyper-parameter impacts, further implementation details, and qualitative results. 

Comparison of segmentation results is shown in Fig.~\ref{fig:seg_map}. Our method makes reasonable predictions, while the baseline completely fails on them.


\begin{figure}[h!]
\vspace*{-0.15cm}
  \centering\includegraphics[width=1.0\linewidth]{latex/figures/fig_seg_map.pdf}
  \vspace*{-0.2cm}
  \caption{Segmentation results on BDD-100K with the models trained on Cityscapes. The upper image contains dust and water drops on the windshield, and the lower one has an extreme domain shift (\textit{i.e.}, night and snow). Note that Cityscapes does not contain any images taken at night or under a snow condition.
  }
\label{fig:seg_map}
\vspace*{-0.3cm}
\end{figure}

\vspace{-0.0cm}
\subsection{Comparison with DA methods}
\vspace{-0.05cm}
We compare the result of our method with those reported from several domain adaptation (DA) methods under various settings. Fig.~\ref{fig:da_comparison} shows the increase in mIoU from the baseline for each method. Although our method may not be the top performer, it shows comparable results to other DA methods. Note that DA methods require access to the target domain to solve DA problems. In contrast, our method is designed to improve generalization performance on an arbitrary \emph{unseen} domain under the assumption of no access to the target domain, so we believe a comparison with DA methods under the same setting is impossible. However, we expect to solve DA by extending our key idea of \emph{selectively} removing style-sensitive covariances to \emph{selectively} matching such covariances between source and target domain.


\begin{figure}[h]
\centering
\begin{center}
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_da_comparison.pdf}
\end{center}
\vspace*{-0.3cm}
   \caption{Comparison of mIoU gain() from the baseline for each method. Other methods compared to ours are FCN Wild~\cite{hoffman2016fcns}, CDA~\cite{zhang2017curriculum}, DCAN~\cite{wu2018dcan}, DTA~\cite{lee2019drop}, IBN-Net~\cite{pan2018two}, and DRPC~\cite{zhao2017pyramid}.}
\label{fig:da_comparison}
\vspace*{-0.2cm}
\end{figure}

\begin{table}[b!]
\vspace{-0.35cm}
\begin{center}
\footnotesize
\begin{tabular}{c|c|c|c|c||c}
\toprule
Models (GTAV) & C & B & M & S & G\\
\drule
Baseline & 28.95 & 25.14 & 28.18 & 26.23 & \textbf{73.45}  \\ 
\midrule
Ours (ISW), =2 & 35.46 & 35.00 & 39.38 & 27.70 & 72.08 \\ 
\midrule
Ours (ISW), =3 & \textbf{36.58} & \textbf{35.20} & \textbf{40.33} & \textbf{28.30} & 72.10 \\ 
\midrule
Ours (ISW), =5 & 34.84 & 33.58 & 39.25 & 27.52 & 72.31 \\ 
\midrule
Ours (ISW), =10 & 33.58 & 33.76 & 38.96 & 27.68 & 72.24 \\ 
\midrule
Ours (ISW), =20 & 33.66 &  33.29 & 38.70  &  27.47  &  72.10     \\ 
\midrule
Ours (IW) & 33.21 & 32.67 & 37.35 & 27.57 & 72.06 \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.0cm}
\caption{Comparison of mIoU(\%) on five different validation sets according to  value. Cityscapes (C), BDD-100K (B), Mapillary (M), SYNTHIA (S), and GTAV (G). The models are trained on GTAV. ResNet-50 is adopted, and an output stride of 16 is used.  denotes re-implemented models. These experiments are conducted three times, and the average results are reported.}
\label{tab_hyper_parameters1}
\vspace{-0.0cm}
\end{table}


\begin{figure}[t!]
\vspace{-0.1cm}
\centering
  \includegraphics[width=0.98\linewidth]{latex/figures/fig_supp_clustering.pdf}
  \vspace*{-0.0cm}
  \caption{The curves denote the magnitude of the variance of each covariance element across the photometric transformations. The vertical dashed lines represent the threshold to separate the covariance elements. The magnitudes of the variance are extracted from the covariance matrix calculated in the input convolutional layer. The y-axis is in log-scale.}
\label{fig:supp_cov_clustering}
\vspace*{-0.3cm}
\end{figure}

\vspace{-0.0cm}
\subsection{Hyper-parameter Impacts}
\vspace{-0.05cm}
\paragraph{Criteria for separating covariance elements}
We adopt -means clustering to separate covariance elements into two groups, domain-specific style and domain-invariant content, according to the variance of each covariance element across various photometric transformations such as color jittering and Gaussian blur. As specified in Section \todow{4.3}, after dividing the covariance elements into  clusters by the magnitude of the variance, the clusters from the first to the -th are considered to be insensitive, and the remaining clusters are considered sensitive to photometric transformation. We set  to one and search the optimal  through the hyper-parameter search. Fig.~\ref{fig:supp_cov_clustering} shows the threshold where the covariances are divided into two groups depending on the  value.
Table~\ref{tab_hyper_parameters1} shows the changes in mIoU performance according to the  values, suggesting the optimal  as 3. Also, we can see that ours (ISW) performs better than IBN-Net or ours (IW) for all  values. Note that ours (IW) applies instance whitening loss to all covariance elements, while ours (ISW) applies it to a part of the covariance elements according to the  value.

\paragraph{Margin  in instance-relaxed whitening (IRW) loss}
\vspace*{-0.3cm}
As described in Section \todow{4.2}, we propose margin-based relaxation of whitening loss. Table~\ref{tab_hyper_parameters2} shows the performance of ours (IRW) according to the margin .

\begin{table}[t!]
\vspace{-0.0cm}
\begin{center}
\footnotesize
\begin{tabular}{c|c|c|c|c||c}
\toprule
Models (GTAV) & C & B & M & S & G \\
\drule
Baseline & 28.95 & 25.14 & 28.18 & 26.23 & \textbf{73.45}  \\ 
\midrule
Ours (IRW), =116 &  32.49 & 32.53 & 37.51 &  \textbf{27.77} & 72.18      \\ 
\midrule
Ours (IRW), =132 & 33.30 & 33.17  &  38.03   &   27.43   &   71.96   \\ 
\midrule
Ours (IRW), =164 & \textbf{33.57} & \textbf{33.18}   & \textbf{38.42}  &  27.29   &  71.96 \\ 
\midrule
Ours (IRW), =1128 & 32.85 & 32.40 & 37.36 & 27.43 & 72.21      \\ 
\midrule
Ours (IRW), =1256 & 32.45 & 32.32 & 37.93  & 27.48 & 72.12   \\ 
\midrule
Ours (IW) & 33.21 & 32.67 & 37.35 & 27.57 & 72.06 \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.2cm}
\caption{Comparison of mIoU(\%) on five different validation sets according to  value. The models are trained on GTAV train set. ResNet-50 is adopted and an output stride of 16 is used. These experiments are conducted three times, and the average results are reported.}
\label{tab_hyper_parameters2}
\vspace{-0.2cm}
\end{table}

\paragraph{Weight  of instance-selective whitening (ISW) loss}
\vspace*{-0.3cm}
As described in Section \todow{4.4}, we empirically set the weight  of the proposed ISW loss as 0.6. Table~\ref{tab_hyper_parameters3} shows the impact of changing .

\begin{table}[h!]
\vspace{-0.0cm}
\begin{center}
\footnotesize
\begin{tabular}{c|c|c|c|c||c}
\toprule
Models (GTAV) & C & B & M & S & G \\
\drule
Ours (ISW), =0.4 & 35.60 & 34.07 & 38.98 &  28.10 & 71.96      \\ 
\midrule
Ours (ISW), =0.6 & \textbf{36.58} & \textbf{35.20}  & \textbf{40.33}   & \textbf{28.30}   &   \textbf{72.10}   \\ 
\midrule
Ours (ISW), =0.8 & 35.73 & 34.01 & 39.69  &  27.44   &  71.96 \\ 
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.2cm}
\caption{Comparison of mIoU(\%) on five different validation sets according to  value. The models are trained on GTAV train set. ResNet-50 is adopted and an output stride of 16 is used. These experiments are conducted three times, and the average results are reported.}
\label{tab_hyper_parameters3}
\vspace{-0.1cm}
\end{table}

\vspace*{-0.2cm}
\subsection{Further Implementation Details}
\vspace*{-0.0cm}
Fig.~\ref{fig:supp_detailed} shows the detailed architecture of the semantic segmentation networks based on ResNet and DeepLabV3+. We adopt the auxiliary per-pixel cross-entropy loss proposed in PSPNet~\cite{zhao2017pyramid} and concatenate the low-level features from the ResNet stage 1 to the high-level features according to the encoder-decoder architecture proposed in DeepLabV3+. Instance normalization (IN) with ISW loss replaces batch normalization (BN) 
in the input convolutional layer, and these ones are added after the skip-connection of the last residual block for each ResNet stage.
As IBN-Net~\cite{pan2018two} pointed out, earlier layers tend to encode the style information, hence we only adopt the ISW loss to the input convolutional layer and ResNet stage 1 and 2. In the end, the final loss  is formulated as,

where the  is 0.4 and the  is 0.6. We set the batch size to 8 for Cityscapes and 16 for GTA. For the photometric transformation, we apply Gaussian blur and color jittering implemented in Pytorch with a brightness of 0.8, contrast of 0.8, saturation of 0.8, and hue of 0.3.
\begin{figure}[h!]
\vspace*{-0.1cm}
\centering
  \includegraphics[width=1.00\linewidth]{latex/figures/fig_detailed_architecture.pdf}
  \vspace*{-0.2cm}
  \caption{Detailed architecture of the segmentation model.}
\label{fig:supp_detailed}
\vspace*{-0.3cm}
\end{figure}

\begin{figure*}[!b]
\vspace*{-0.2cm}
\centering
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_supp_failure.pdf}
\caption{Comparison of failure cases of our method and the baseline.}
\label{fig:supp_failure_results}
\end{figure*}

\subsection{Additional Qualitative Results}
\vspace*{-0.1cm}
This section demonstrates additional qualitative results. We first present the comparison of the segmentation results on a \emph{seen} domain (\textit{i.e.,} Cityscapes) and diverse driving conditions in BDD-100K, and then show the failure cases of our method. Besides, we show the effects of the whitening by comparing the reconstructed images from our proposed approach and the baseline. Finally, we provide the tendency of images from the most sensitive and insensitive covariance elements to the photometric transformation.

\begin{figure*}[!t]
\centering
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_supp_seg_results_city.pdf}
\caption{Segmentation results on \emph{seen} domain images (\textit{i.e.,} Cityscapes).}
\label{fig:supp_seg_result_city}
\end{figure*}

\begin{figure*}[!b]
\vspace*{-0.2cm}
\centering
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_supp_seg_results.pdf}
  \caption{Segmentation results under illumination changes (\textit{i.e.,} dusk, night, and shadow) in BDD-100K with the models trained on Cityscapes.}
\label{fig:supp_seg_result_illumination}
\end{figure*}

\paragraph{Comparison of segmentation results}
\vspace*{-0.3cm}
To qualitatively describe the effect of our method, we compare the segmentation results from the baseline and ours.
Fig.~\ref{fig:supp_seg_result_city} presents the segmentation results on a \emph{seen} domain (\textit{i.e.,} Cityscapes).
Similar to the quantitative results reported in Section \todow{5}, even with qualitative results, our model shows comparable performance to the baseline model on the \emph{seen} domain.
Fig.~\ref{fig:supp_seg_result_illumination} shows the segmentation results under illumination changes on an \emph{unseen} domain (\textit{i.e.,} BDD-100K). 
Note that Cityscapes dataset only contains images taken at the daytime. 
The first group images are taken at the dusk.
We can see that the baseline model is vulnerable to these changes, but in contrast, our model outputs less damaged maps and reasonably predicts roads and cars.
In extreme cases such as at night, both models fail to predict the sky, but our method still finds key components such as roads and cars well. 
In addition, our method produces reasonable segmentation results even for drastic changes in lighting such as shadows, as seen in the third group.
Fig.~\ref{fig:supp_seg_result_various} shows the segmentation results under the adverse weather conditions, unseen structures, and lush vegetation.
Our model successfully predicts a partially snowy sidewalk, whereas the baseline model incorrectly predicts it as a building.
The second case in the first group shows a foggy urban scene. The baseline fails to cope with these weather changes, while ours still shows fair results. Under the structural changes as shown in the second group, our method finds the road and sidewalk better than the baseline. Moreover, the baseline totally fails to detect the parking lot. In the last case, which is lush vegetation, the baseline produces noisy segmentation results and confused the road as a car. On the other hand, our model shows reasonable performance in both cases. Fig.~\ref{fig:supp_failure_results} shows the failure cases caused by a large domain shift.


\vspace*{-0.4cm}
\paragraph{Covariance effects in images}
To reveal the information that the covariance represents, 
we first identify the most sensitive and insensitive covariances to the photometric transformation. Then, we sort the BDD-100K images according to the magnitude of the identified covariances.
The results are described in Fig.~\ref{fig:supp_cov_results}. In the left group, the images are getting dark as the most sensitive covariance is getting smaller. We conjecture that the corresponding covariance tends to represent the \emph{illumination} information. On the other hand, the right group shows the sorted images along with the most insensitive covariance. The scenes are getting simpler as the covariance gets smaller, which implies that the most insensitive covariance tends to represent the \textit{scene complexity}.





\begin{figure*}[!t]
\vspace*{-0.3cm}
\centering
  \includegraphics[width=1.0\linewidth]{latex/figures/fig_supp_seg_results2.pdf}
  \caption{Segmentation results under various circumstances in BDD-100K with the models trained on Cityscapes. Circumstances include adverse weather conditions (\textit{i.e.,} snow and fog), unseen structures (\textit{i.e.,} parking lot and overpass), and vegetation.}
\label{fig:supp_seg_result_various}
\vspace*{-0.3cm}
\end{figure*}

\begin{figure*}[!t]
\centering
  \includegraphics[width=\linewidth]{latex/figures/fig_cov_results.pdf}
\caption{Tendency of images in BDD-100K dataset along with the covariance changes.}
\label{fig:supp_cov_results}
\vspace*{+1.0cm}
\end{figure*}





\end{document}
