

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{enumitem}

\usepackage{url}

\usepackage{booktabs} \usepackage{diagbox}

\usepackage{multirow}
\usepackage{balance}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{wrapfig,lipsum}
\usepackage[noend,ruled]{algorithm2e}
\usepackage{makecell}
\usepackage[T1]{fontenc}


\usepackage{tabularx, booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\arraybackslash}X}
\aclfinalcopy 



\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\newcommand{\our}{\mbox{\sf CrossWeigh}\xspace}
\newcommand{\consistenterror}{\mbox{systematic mistakes}\xspace}
\newcommand{\Consistenterror}{\mbox{Systematic mistakes}\xspace}
\newcommand{\inconsistenterror}{\mbox{messy mistakes}\xspace}
\newcommand{\Inconsistenterror}{\mbox{Messy mistakes}\xspace}
\newcommand{\TODO}[1]{{\color{blue}{TODO: #1}}}
\newcommand{\liu}[1]{\textbf{{\color{red}{Liu: }\color{blue}{#1}}}}
\newcommand{\smallsection}[1]{{\noindent\textbf{#1.}}}


\newcommand{\FIX}[1]{{\color{blue}{#1}}}
\usepackage[normalem]{ulem}


\title{CrossWeigh: Training Named Entity Tagger from Imperfect Annotations}

\author{
Zihan Wang\thanks{\; Equal Contributions.}
Jingbo Shang
Liyuan Liu
Lihao Lu 
Jiacheng Liu 
Jiawei Han
\
            \mathcal{J} = \sum_{i = 1}^{n} w_i \cdot l(M(x_i), y_i)
        
        \label{eq:test_entities}
            \mbox{test\_entities}_i = \bigcup_{x_j \in D_i} e_{j}
        
        \label{eq:train_set}
            \mbox{train\_set}_i = \{ x_j | e_j \cap \mbox{test\_entities}_i = \emptyset, 1 \leq j \leq n \}
        \label{eq:weight}
            w_i = \epsilon^{c_i}
        
        where  is a parameter.
        In practice, it can be chosen according to the quality of mistake estimation module. 
        Particularly, we first estimate the precision of the detected mistakes of a single iteration. 
        Let  be the ratio of the number of true detected label mistakes over the number of detected label mistakes.
         can be roughly estimated through a manual check of a random sample from the detected label mistakes.
        Then, we choose , because  represents the fraction of these detected label mistakes that might be still useful during the model training. 
        Therefore, for the sentences that are marked as \textit{potentially mistake} in that iteration,  of them are actually correct. 
        With more iterations, the confidence of being correct lowers like a binomial distribution, which is the reason that we chose an exponential decaying weight function in Equation~\ref{eq:weight}.  \section{Experiments}\label{sec:exp}

In this section, we conduct several experiments to show effectiveness of our \our framework.
We first evaluate the overall performance of \our on benchmark NER datasets, by plugging it into three base NER models.
Since we have two modules in \our, we then dive into each module and explore different variants and ablations.
In addition, we further verify the effectiveness of \our on two more datasets: an emerging-entity NER dataset from WNUT'17 and a low-resource language NER dataset of the Sinhalese language.

    \subsection{Experimental Settings}
    
        \smallsection{Dataset}
        We use both the original and corrected CoNLL03 datasets.
        We follow the standard train/dev/test splits and use both the train set and dev set for training ~\cite{peters2017semi, akbik2018contextual}.
        Entity-wise F score on the test set is the evaluation metric.
    
        \smallsection{Base NER Algorithm}
        We mainly choose Flair as our base NER algorithm. Flair is a strong NER algorithm using external resources (large corpus to train a language model). While Pooled-Flair has even better performance, its computational cost refrains us from doing extensive experiments. 


    
        \smallsection{Default Parameters in \our}
        For all NER algorithms we experiment with, their default parameters are used.  
        For \our parameters, by default, we set , , and . 
We decide  because among  randomly sampled sentences with \textit{potentially mistake}, we find that  of them really contain label mistakes (i.e., the probability of one annotation to be correct is roughly ). We use both train and development set to train the models, and report average F and its standard deviation on both original test set and our corrected test set across 5 different runs~\cite{peters2017semi}. 
    
    \subsection{Overall Performance}
        
        We pair \our with our base algorithm (i.e. Flair) and two best-performing NER algorithms with or without language models in Table~\ref{tbl:reevaluation} (i.e. Pooled-Flair and VanillaNER), and evaluate their performance.
As shown in Table~\ref{tbl:performance}, compared with the three algorithms, applying \our always leads to a higher F score and a comparable, sometimes even smaller, standard deviation.
        Therefore, it is clear that \our can improve the performance of NER models. 
        The smaller standard deviations also imply that the models trained with \our are more stable.
        All these results illustrate the superiority of training with \our.


\begin{table}[t]
\center
\small
\begin{tabularx}{1\linewidth}{l *{2}{Y}}
\toprule
\textbf{Method} & \textbf{Original} & \textbf{Corrected}\\
\midrule
w/o \our & 92.87 (0.08) & 93.89 (0.06)  \\
\midrule
w/ \our & 93.19 (0.09) & 94.18 (0.06) \\
  Entity Disjoint & 92.88 (0.11) & 93.84 (0.08) \\
  Random Discard & 93.01 (0.10) & 93.94 (0.10)\\
\bottomrule
\end{tabularx}
\vspace{-0.15cm}
\caption{Importance of Entity Disjoint Filtering.}
\label{tbl:entity_disjoint}
\vspace{-0.3cm}
\end{table}



%
 
    \subsection{Ablations and Variants}
We pick Flair as the base algorithm to conduct ablation study.
        
        \smallsection{Entity Disjoint Filtering}
        There is an entity disjoint filtering step, when we are collecting training data  for the NER model  during the mistake estimation step. 
        To study its importance, we have done a few ablation experiments. 
        
        We have evaluated the following variants:
        \begin{itemize}[leftmargin=*,nosep]
            \item Flair w/ \our \textbf{-- Entity Disjoint}: Skip the entity disjoint filtering step.
            \item Flair w/ \our \textbf{+ Random Discard}:
            Instead of entity disjoint filtering, randomly discard the same number of sentences from each  as it would do.
        \end{itemize}
        
        The results are listed in Table~\ref{tbl:entity_disjoint}.
        One can easily observe that without the entity disjoint filtering, the F scores are very close to the raw Flair model. 
        This demonstrates that the entity disjoint filtering is critical to reduce the over-fitting risk in the mistake estimation step.
        Also, our proposed entity disjoint filtering strategy works more effective than random discard. 
        This further confirms the effectiveness of entity disjoint filtering.
    
        \smallsection{Variants in Computing }
        There is definitely more than one way to determine . Let  be the number of ``potentially mistake''s among the  estimations, we can apply any of the following heuristics:
        \begin{itemize}[leftmargin=*,nosep]
            \item \textbf{Ratio}:  is the number of  ``potentially mistake'' (i.e. ). This is the method mentioned in Section~\ref{sec:crossweigh}, and used by default.
            \item \textbf{At Least One}:  is the indicator of at least one estimation being ``potentially mistake'' (i.e. ).
            \item \textbf{Majority}:  is the indicator of at least  estimations being ``potentially mistake''
            (i.e. ).
            \item \textbf{All}:  is the indicator of all  estimations being ``potentially mistake'' (i.e. ).
        \end{itemize}
        We evaluate the performance of these heuristics when used in \our, as shown in Table~\ref{tbl:different_c_i}.
        There is not much difference across these heuristics, while our default choice ``Ratio'' is the most stable.


    \subsection{Label Mistake Identification Results}

        Another usage of \our is to identify potential label mistakes during label annotation process, thus improving the annotation quality.
        This could be also helpful to active learning.
        
        Specifically in this experiment, we apply our noise estimation module to the concatenation of training and testing data. 
        As we have manually corrected the label mistakes in the testing set, we are able to report the number of true mistakes among the potential mistakes discovered in the test set.
        
        The results are presented in  Table~\ref{tbl:noise_estimation}. 
        The potential mistakes are the total number of mistakes identified by \our, and actual mistakes is the true positives among all identifications.
        From the results, we can see that when the base model is Flair, \our is able to spot more than 75\% of label mistakes, 
        while maintaining a precision about 25\%.
        It is worth noting that 25\% is a reasonably high precision, given that the label mistake ratio is only 5.38\%.
        The 75\% recall indicates that \our is able to identify most of the label mistakes, which are extremely valuable to improve the annotation quality. 
        
        



\begin{table}[t]
\center
\small
\begin{tabularx}{\linewidth}{l *{2}{Y}}
\toprule
\textbf{Heuristic} & \textbf{Original} & \textbf{Corrected}\\

\midrule
At Least One & 93.10 (0.10) & 94.16 (0.07) \\
Majority & 93.20 (0.09) & 94.12 (0.07) \\
All & 93.16 (0.09) & 94.11 (0.09) \\
\midrule
Ratio & 93.19 (0.09) & 94.18 (0.06) \\
\bottomrule
\end{tabularx}
\vspace{-0.15cm}
\caption{Different  Estimation Heuristics.}
\label{tbl:different_c_i}
\vspace{-0.3cm}
\end{table} 
\begin{table}[t]
\center
\small
\begin{tabularx}{1\linewidth}{l *{2}{Y}}
\toprule           & \textbf{Original} & \textbf{Corrected}\\

\midrule
 1 & 93.09 (0.14) & 94.07 (0.09)  \\
 5 & 93.23 (0.10) & 94.14 (0.08)  \\
 \midrule
 3 & 93.19 (0.09) & 94.18 (0.06)  \\


\bottomrule
\end{tabularx}
\vspace{-0.15cm}
\caption{Different Numbers of Iterations .}
\label{tbl:noise_t}
\vspace{-0.3cm}
\end{table}

 
\begin{table}[t]
\center
\small
\begin{tabularx}{1\linewidth}{l *{2}{Y}}
\toprule          & \textbf{Original} & \textbf{Corrected}\\

\midrule
 2 & 92.11 (0.24) & 92.88 (0.11)  \\
 5 & 93.12 (0.08) & 94.12 (0.08)  \\
 \midrule
 10 & 93.19 (0.09) & 94.18 (0.06)  \\


\bottomrule
\end{tabularx}
\vspace{-0.15cm}
\caption{Different Numbers of Folds .}
\label{tbl:noise_k}
\vspace{-0.3cm}
\end{table}

 
\begin{table}[t]
\center
\small
\begin{tabularx}{1\linewidth}{l *{2}{Y}}
\toprule           & \textbf{Original} & \textbf{Corrected}\\

\midrule
 0.3 & 92.79 (0.14) & 93.68 (0.15)  \\
 0.5 & 93.21 (0.09) & 94.18 (0.07)  \\
 0.9 & 93.01 (0.10) & 93.96 (0.09)  \\
 \midrule
 0.7 & 93.19 (0.09) & 94.18 (0.06)  \\


\bottomrule
\end{tabularx}
\vspace{-0.15cm}
\caption{Different Weight Adjustments .}
\label{tbl:noise_reweigh}
\vspace{-0.3cm}
\end{table}

 
\begin{table*}[t]
\begin{center}
\vspace{-0.3cm}
\scalebox{0.77}{
\begin{tabularx}{1.27\linewidth}{l *{5}{Y}}
\toprule
 Algorithm & 
\begin{tabular}[c]{@{}l@{}}
Potential Mistakes
\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}
Actual Mistakes
\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}
Precision
\end{tabular} & 
\begin{tabular}[c]{@{}l@{}}
Recall
\end{tabular} &
\begin{tabular}[c]{@{}l@{}}
F1
\end{tabular}
\\\midrule


\begin{tabular}[c]{@{}c@{}}
\textbf{Flair}
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
573.0
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
144.0
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
0.2513
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
0.7742
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
0.3794
\end{tabular}
\\\midrule


\begin{tabular}[c]{@{}c@{}}
\textbf{VanillaNER}
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
821.67
\end{tabular}
&
\begin{tabular}[c]{@{}l@{}}
146.33
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
0.1781
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
0.7867
\end{tabular}
& 
\begin{tabular}[c]{@{}l@{}}
0.2904
\end{tabular}
\\
\bottomrule
\end{tabularx}
}
\end{center}
\caption{Quality of noise estimation. The number of true mistakes, based on our manual correction, is . The potential mistakes are counted based on average of 3 runs.}
\label{tbl:noise_estimation}
\end{table*}



























%
     

    \subsection{Parameter Study}
        We study how \our performs with different hyper-parameters, i.e.,  (the number of iterations that we run mistake estimation),  (the number of folds in mistake estimation), and  (the weight scaling factor of identified potential mistakes).


        In principle, 
a larger  usually gives us a more stable mistake estimation.
        However, a larger  also requires more computation resources. In our experiments (see Table~\ref{tbl:noise_t}), we find that  provides a good enough result.
        
        Specifically, during mistake estimation, we have to choose the number of folds to partition the data.
        The more partitions made, the smaller each  is and the fewer sentences will be filtered, leading to more training data  and better trained . 
        On the other hand, this is at the cost of higher computational expense. 
        As shown in Table~\ref{tbl:noise_k}, we observe that  are significantly better than . 
        In fact, when , each  has only around 5000 sentences and 1500 entities inside.
        These numbers become 7000 and 4000 when , and 9000 and 7000 when .


        As we mentioned before, the value  can be chosen by estimating the quality of mistake estimation. 
        Table~\ref{tbl:noise_reweigh} presents some results when other values are used.
 leads to the worst performance.
        Since our estimation does not have high precision, assigning  to a low value like  may not be a good choice.
        Interestingly  performs on par with , and even slightly better in the original test set.
We hypothesize that this is because there are some ambiguous sentences that we did not count during estimating the quality of mistake estimation, see Section \ref{sec:case}, and the actual precision could be higher.


        
    \subsection{Other Datasets}
        To show the generalizability of our method across domains and languages, we further evaluate \our on an emerging-entity NER dataset from WNUT'17 and a Sinhalese NER dataset from LORELEI\footnote{LDC2018E57}.
        Sinhalese is a low-resource, morphology-rich language.
        For WNUT'17, we use the Flair as our base NER algorithm. For Sinhalese, we use BERT~\cite{devlin2018bert} followed by a BiLSTM-CRF as our base NER algorithm. 
        We use the same parameters as used in the previous CoNLL03 experiments, namely . 
        
\begin{table}[t]
\center
\small
\begin{tabularx}{1\linewidth}{l *{2}{Y}}
\toprule   \textbf{Dataset}        & \textbf{w/o CrossWeigh} & \textbf{w/ CrossWeigh}\\

\midrule
 WNUT'17 & 48.96 (0.97) & 50.03 (0.40)  \\
 \midrule
 Sinhalese & 66.34 (0.34) & 67.68 (0.21)  \\


\bottomrule
\end{tabularx}
\vspace{-0.15cm}
\caption{Applying CrossWeigh on other datasets}
\label{tbl:other}
\vspace{-0.3cm}
\end{table}

 
        The results averaged across 5 runs are reported in Table~\ref{tbl:other}. 
        One can observe quite similar results as those in the previous CoNLL03 experiments.
        Training with \our leads to a significantly higher F1 and a smaller standard deviation.
        This suggests that \our works well in other datasets and languages. 
\begin{table*}[t]
\centering
\small
\scalebox{0.9}{
\begin{tabularx}{1.1\linewidth}{c ll}
\toprule
              & \multicolumn{1}{c}{\textbf{Training Set}} & 
              \multicolumn{1}{c}{\textbf{Test Set}} \\
\midrule
\textbf{Text} & Hapoel Haifa 3 Maccabi Tel Aviv 1 & Hapoel Jerusalem 0 Maccabi Tel Aviv 4 \\
\midrule
\midrule
\textbf{Original Annotations} & [Hapoel Haifa]\{ORG\}, \color{red}{\textbf{[Tel Aviv]\{ORG\}}} & [Hapoel Jerusalem]\{ORD\}, [Maccabi Tel Aviv]\{ORG\}\\
\midrule
\textbf{Correct Annotations} & [Hapoel Haifa]\{ORG\}, [Maccabi Tel Aviv]\{ORG\} & [Hapoel Jerusalem]\{ORD\}, [Maccabi Tel Aviv]\{ORG\}\\
\midrule
\midrule
 & \multicolumn{1}{c}{\textbf{Action}} & \multicolumn{1}{c}{\textbf{Result}} \\\midrule
\textbf{Flair}  &  Assumes this sentence is equally reliable as others. & [Hapoel Jerusalem]\{ORD\}, \color{red}{\textbf{[Tel Aviv]\{ORG\}}} \\
\midrule
\textbf{Flair w/ CrossWeight} & Lowers the weight of this sentence as mistakes. &  [Hapoel Jerusalem]\{ORD\}, [Maccabi Tel Aviv]\{ORG\}\\
\bottomrule
\end{tabularx}
}
\vspace{-0.15cm}
\caption{Case Study on the CoNLL03 dataset. Errors are marked with red}
\label{tbl:case_study}
\vspace{-0.4cm}
\end{table*}

\section{Case Studies} \label{sec:case}
\smallsection{Test Set Correction}
Despite the label mistakes that we have corrected, we also find some ambiguous but consistent cases.
For instances,
(1) All NBA/NHL divisions such as ``CENTRAL DIVISION'', ``WESTERN DIVISION'' were annotated as \texttt{MISC}, while all European leagues, such as ``SPANISH FIRST DIVISION'' and ``ENGLISH PREMIER LEAGUE'', are not marked as \texttt{MISC} correctly --- only ``SPANISH'' and ``ENGLISH'' are labelled as \texttt{MISC}. 
And (2) ``Team A at Team B'' is a way to say ``Team A'' as an away team playing with Team B as a home team. 
However, in almost all cases (only 1 exception out of more than 100), ``Team A'' was labelled as \texttt{ORG} while ``Team B'' was labelled as \texttt{LOC}. 
For example, in ``MINNESOTA AT MILWAUKEE'', ``NEW YORK AT CALIFORNIA'', and ``ORLANDO AT LA LAKERS'', the second sports team ``MILWAUKEE'', ``CALIFORNIA'' and ``LA LAKERS'' were always labelled as \texttt{LOC}.
Because these parts behave consistently and generally follow the annotation guideline,
we didn't touch them during the test set correction. 


\smallsection{CrossWeigh Framework}
    The mistakes in the training set can harm the generalizability of the trained model. 
    For example, in Table~\ref{tbl:case_study}, the original training sentence ``Hapoel Haifa 3 Maccabi Tel Aviv 1'' contains a label mistake, because ``Maccabi Tel Aviv'' is a sports team but was not annotated completely. 
    Interestingly, there is a similar sentence in the test set -- ``Hapoel Jerusalem 0 Maccabi Tel Aviv 4''.
    In all 5 different runs of the original Flair model, they failed to predict correctly that ``Maccabi Tel Aviv'' in the test sentence as \texttt{ORG} because of the label mistake in the training sentence, even though ``\texttt{ORG} number \texttt{ORG} number'' is an obvious pattern in the training set.
In \our, this label mistake in the training set was detected in all  iterations and therefore assigned a very low weight during training. 
    After that, in all 5 different runs of Flair w/ \our, they successfully predict that ``Maccabi Tel Aviv'' is \texttt{ORG} as a whole.





 \section{Related Work}\label{sec:rel}
In this section, we review related works from three aspects, mistake identification, cross validation \& boosting, and NER algorithms.
\subsection{Mistake Identification}
Researchers have noticed the label mistakes in sophisticated natural language processing tasks for a while.
    For example, it is reported that the inter-annotator agreement is about 97\% on the Penn Treebank POS tagging dataset~\cite{manning2011part, subramanya2010efficient}.
    
    There are a few attempts towards detecting label mistakes automatically.
    For example, \citet{nakagawa2002detecting} designed a support vector machine-based model to assign weights to examples that were hard to classify in the POS tagging task.
    \citet{loftsson2009correcting} further applied previous detection models and manually corrected Icelandic Frequency Dictionary~\cite{pind1991islensk} POS tagging dataset.
    However, these two methods are specifically developed for POS tagging and cannot be directly applied to NER. 
    
    Recently, \citet{rehbein2017detecting} extends variational inference with active learning to detect label mistakes in ``silver standard'' data generated by machines.
    In this paper, we focus on detecting label mistakes in ``gold standard'' data, which is a different scenario.






\subsection{Cross Validation \& Boosting}
    Our mistake estimation module shares some similarity with cross validation. Applying cross validation to the training set is the same as our mistake estimation module, except that we have an \textit{entity disjoint filtering} step. Experiments in Table~\ref{tbl:entity_disjoint} show that this step is crucial to our performance gain. The choice of ten folds also stems from cross validation~\cite{kohavi1995study}.


    Another similar thread of work is boosting, such as Adaboost~\cite{freund1999short, schapire1999improved}. 
    For example, \citet{abney1999boosting} has applied Adaboost on the Penn Treebank POS tagging dataset and gained encouraging results on model performance.
    In boosting algorithms, the training data is assumed to be perfect.
    Therefore, it trains models using the full training set and then increases the weights of training instances that fails the current model in the next round of learning.
    In contrast, we decrease the weights of sentences that differ from the model built upon the entity disjoint training set.
    More importantly, our framework is a better fit for neural models, because they can likely overfit the training data and thus being bad choices as weak classifiers in boosting.







\subsection{NER Algorithms}
Neural models have been widely used for Named Entity Recognition, and the state-of-the-art models integrate LSTMs, conditional random field and language models~\cite{lample2016neural, ma2016end, liu2018empower, peters2018deep, akbik2018contextual}. 
In this paper, we focus on improving the annotation quality for NER, and our method has a big potential to help other methods, especially for noisy datasets~\cite{shang2018learning}. 
 \section{Conclusion \& Future work}\label{sec:con}



In this paper, we explore and correct the label mistakes in the CoNLL03 NER dataset. 
Based on the corrected test set, we re-evaluate most of recent NER models.
We further propose a novel framework, \our, that is able to detect label mistakes in the training set and then train a more robust NER model accordingly.
Extensive experiments demonstrate the effectiveness of \our on three datasets and also indicate the potentials of using \our to improve the annotation quality during the label curation process.

In future, we plan to extend our framework into an iterative setting, similar to those boosting algorithms.
The bottleneck of doing this lies in the efficiency problems of training multiple deep neural models hundreds of times.
One solution to overcome it is to apply meta learning. 
We can first train a meta model and only fine-tune on different training data on each fold.
In this way, we can identify label mistakes more accurately and obtain a series of weighted models at the end.

%
 
\section*{Acknowledge}
We thank all reviewers for valuable comments and suggestions that brought improvements to our final version.
Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, DTRA HDTRA11810026,  Google Ph.D. Fellowship and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). Any opinions, findings, and conclusions or recommendations expressed in this document are those of the author(s) and should not be interpreted as the views of any U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.
This research was supported by grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov). This work was supported by Contracts HR0011-15-C-0113 and HR0011-18-2-0052 with the US Defense Advanced Research Projects Agency (DARPA).


\bibliography{citation.bib}
\bibliographystyle{acl_natbib}

\end{document}