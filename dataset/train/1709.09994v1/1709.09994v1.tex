\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[final]{nips_2017}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}      \usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{amsmath}



\title{Premise Selection for Theorem Proving \\by Deep Graph Embedding}



\author{Mingzhe Wang\thanks{Equal contribution.} \quad Yihe Tang\footnotemark[1] \quad  Jian Wang \quad  Jia Deng \\ University of Michigan, Ann Arbor}

\begin{document}
\bibliographystyle{unsrt}       \maketitle

\begin{abstract}
We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula
as a graph that is invariant to variable renaming but still fully preserves syntactic and
semantic information. We then embed the graph into a vector via a novel
embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83\% to 90.3\%.
\end{abstract}

\section{Introduction}

Automated reasoning over mathematical proofs is a core question of artificial intelligence
that dates back to the early days of computer science~\cite{robinson2001handbook}. It not only constitutes a key aspect
of general intelligence, but also underpins a broad set of applications ranging from circuit
design to compilers,  where it is critical to verify the correctness of a computer
system~\cite{kern1999formal,klein2009sel4,leroy2009formal}. 

A key challenge of theorem proving is \emph{premise selection}~\cite{deepmath}: selecting 
 relevant statements that are useful for proving a given conjecture. Theorem proving is
 essentially a search problem with the goal of finding a sequence of deductions leading from
 presumed facts to the given conjecture. The space of this search is combinatorial---with
 today's large mathematical knowledge bases~\cite{Naumowicz2009, Harrison2009}, the search can 
 quickly explode beyond the capability of modern automated theorem provers, despite the
 fact that often only a small fraction of facts in the knowledge base are relevant for
 proving a given conjecture. 
Premise selection thus plays a critical role in narrowing down the search space and making it tractable. 

Premise selection has been traditionally tackled as hand-designed 
heuristics based on comparing and analyzing symbols~\cite{hoder2011sine}. Recently, machine
learning methods have emerged as a promising alternative for premise selection, which can naturally be cast as
a classification or ranking problem. Alama et al.~\cite{alama2014premise} trained 
a kernel-based classifier using essentially bag-of-words features, and demonstrated
large improvement over the state of the art system. Alemi et al.~\cite{deepmath} were the first to apply
deep learning approaches to premise selection and demonstrated competitive results without
manual feature engineering. Kaliszyk et al.~\cite{holstep} introduced HolStep, a large dataset of
higher-order logic proofs, and provided baselines based on logistic regression and deep
networks. 

In this paper we propose a new deep learning approach to premise selection. 
The key idea of our approach is to represent mathematical formulas as graphs and embed them into
vector space. This is different from prior work on premise selection that 
directly applies deep networks to sequences of characters or tokens~\cite{deepmath, holstep}. Our approach is motivated by the
observation that a mathematical formula can be represented as a graph that encodes
 the syntactic and semantic structure of the formula. For example, the
formula  can be expressed as the graph shown in
Fig.~\ref{fig1}, where edges link terms to their constituents and connect quantifiers to their
variables. 

\begin{figure}
  \centering
  \includegraphics[width = 12cm]{fig/fig1.pdf}\\
  \caption{The formula  can be represented as a
    graph. }\label{fig1}
     \vspace{-2mm}
\end{figure}

Our hypothesis is that such graph representations are better than sequential
forms because a graph makes explicit key syntactic and semantic structures such as
composition, variable binding, and co-reference. Such an explicit
representation helps the learning of invariant feature representations. For
example,  and 
share the same top level structure , but such similarity would be
less apparent and harder to detect from a sequence of tokens because
syntactically close terms can be far apart in the
sequence. 

Another benefit of a graph representation is that we can make it invariant to
variable renaming while preserving the semantics. For example, the graph
for  
(Fig.~\ref{fig1}) is the same regardless of how the variables are named in the formula, but the
semantics of quantifiers and co-reference is completely preserved---the quantifier  binds a variable that
is the first argument of both  and , and the quantifier  binds a variable that is the
second argument of . 

It is worth noting that although a sequential form encodes the
same information, and a neural network may well 
 be able to learn to convert a sequence of tokens into a graph, such a neural conversion is
unnecessary---unlike parsing natural language sentences, constructing a graph out of a
formula is straightforward and unambiguous. 
Thus there is no obvious benefit to be gained through an end-to-end approach that starts
from the textual representation of formulas. 

To perform premise selection, we convert a formula into a graph, embed the graph into a
vector, and then classify the relevance of the formula. To embed a graph into a vector, we
assign an initial embedding vector for each node of the graph, and then iteratively update
the embedding of each node using the embeddings of its neighbors. We then pool the
embeddings of all nodes to form the embedding of the entire graph. The parameters of each
update are learned end to end through backpropagation. In other words, we learn a deep
network that embeds a graph into a vector; the topology of the unrolled network is
determined by the input graph. 

We perform experiments using the HolStep dataset~\cite{holstep}, which consists of over two
million conjecture-statement pairs that can be used to evaluate premise selection. The
results show that our graph-embedding approach achieves large improvement over sequence-based
models. In particular, our approach improves the state-of-the-art accuracy on HolStep by 7.3\%.

Our main contributions of this work are twofold. First, we propose a novel approach to premise selection
that represents formulas as graphs and embeds them into vectors. To the best our
knowledge, this is the first time premise selection is approached using deep graph
embedding. Second, we improve the state-of-the-art classification accuracy on the HolStep dataset from 83\% to 90.3\%.

\section{Related Work}

Research on automated theorem proving has a long history~\cite{harrison2014history}. 
Decades of research has resulted in a variety of well-developed automated theorem provers
such as Coq~\cite{coq}, Isabelle~\cite{wenzel2008isabelle}, and
E~\cite{schulz2002brainiac}. However, no existing automated provers can scale to 
large mathematical libraries due to combinatorial explosion of the search
space. This limitation gave rise to the development of interactive theorem
proving~\cite{harrison2014history}, which combines humans and machines in theorem proving and has led to impressive 
achievements such as the proof of the Kepler conjecture~\cite{hales2015formal} and the formal
proof of the Feit-Thompson problem~\cite{Gonthier2013}. 

Premise selection as a machine learning problem was introduced by Alama et
al.~\cite{alama2014premise}, who constructed a corpus of proofs to train a kernelized classifier
using bag-of-word features that represent the occurrences of terms in a
vocabulary.
Deep learning techniques were first applied to premise selection in the DeepMath work by Alemi et al.~\cite{deepmath},
who applied recurrent networks and convolutional to formulas represented as textual
sequences, and
showed that deep learning approaches can achieve competitive results against baselines using
hand-engineered features. Serving the needs for large datasets for training deep models,
Kaliszyk et al.~\cite{holstep} introduced the HolStep dataset that consists of 2M statements
and 10K conjectures, an order of magnitude larger than the DeepMath dataset~\cite{deepmath}. 

A related task to premise selection is \emph{proof
 guidance}~\cite{suttner1990automatic, denzinger1999learning, schulz2000learning, farber2016internal, kaliszyk2015femalecop, whalen2016holophrasm}, the selection of the next clause to process \emph{inside} an automated theorem
prover. Proof guidance differs from premise selection in that proof guidance 
 depends on the logical representation, inference algorithm, and current state inside a theorem
prover, whereas premise selection is only about picking relevant statements as the initial
input to a theorem prover that is treated as a black box. Because proof guidance is
tightly integrated with proof search and is invoked repeatedly, efficiency is as important
as accuracy, whereas for premise selection efficiency is not as critical. 

Loos et al.~\cite{deep_guide_proof} were the first to apply deep networks to proof
guidance. They experimented with both
sequential representations and tree representations (recursive neural networks~\cite{SocherEtAl2011:PoolRAE,
  socher2011parsing}). Note that their tree representations are simply the parse trees,
which, unlike our graphs, are not invariant to variable renaming and do not capture how
quantifiers bind variables. Whalen et al.~\cite{whalen2016holophrasm} used GRU networks to guide the exploration of partial proof trees, with formulas represented as sequences of tokens.

In addition to premise selection and proof guidance, other aspects of theorem proving have
also benefited from machine learning. For example, K\"uhlwein et
al.~\cite{kuhlwein2015males} applied kernel methods to strategy
finding, the problem of searching for good parameter configurations for an automated
prover. Similarly, Bridge et al.~\cite{Bridge2014} applied SVM and Gaussian Processes to select good heuristics, which are collections of standard settings for parameters and other decisions.

Our graph embedding method is related to a large body of
prior work on embeddings and graphs. Deepwalk~\cite{perozzi2014deepwalk}, LINE~\cite{tang2015line} and
Node2Vec~\cite{grover2016node2vec} focus on learning node embeddings. Similar to 
Word2Vec~\cite{mikolov2013distributed,mikolov2013efficient}, they optimize the embedding
of a node to predict nodes in a neighborhood. Recursive neural networks~\cite{goller1996learning, socher2011parsing} and Tree
LSTMs~\cite{tai2015improved} consider embeddings of trees, a special type of graphs.  
Neural networks on general graphs were first introduced by Gori et
al~\cite{gori2005new} and Scarselli et al~\cite{scarselli2009graph}.  Many follow-up
works~\cite{duvenaud2015convolutional, li2015gated, jain2016structural, henaff2015deep, defferrard2016convolutional, kipf2016semi} proposed specific
architectures to handle graph-based input by extending recurrent neural network to graph
data~\cite{gori2005new,li2015gated,jain2016structural} or making use of graph convolutions
based on spectral graph theories~\cite{duvenaud2015convolutional, henaff2015deep,
  defferrard2016convolutional, kipf2016semi, niepert2016learning}. Our approach is most
similar to the work of \cite{duvenaud2015convolutional}, where they encode molecular
fragments as neural fingerprints with graph-based convolutions for chemical applications. 
But to the best of our knowledge, no previous deep learning approaches on general graphs
 preserve the order of edges. In contrast, we propose a novel way of graph embedding that
can preserve the information of edge ordering, and demonstrate its effectiveness for
premise selection. 

\section{FormulaNet: Formulas to Graphs to Embeddings}

\subsection{Formulas to Graphs}

We consider formulas in higher-order logic~\cite{church1940formulation}. A higher-order
formula can be defined recursively based on a vocabulary of constants, variables, and
quantifiers. A variable or a constant can act as a value or a function. For example,
 is a higher-order formula where  and
 are quantifiers,  is a constant value,  are constant
functions,  is a variable value, and  is both a variable function and a
variable value. 

To construct a graph from a formula, we first parse the formula into a tree, where each
internal node represents a constant function, a variable function, or a quantifier, and each leaf node represents a variable value 
or a constant value. We then add edges that connect a quantifier node to all instances of its quantified
variables, after which we merge (leaf) nodes that represent the same constant or
variable. Finally, for each occurrence of a variable, we replace its original 
name with , or  if it acts as a function. Fig.~\ref{fig:example} illustrates these
steps.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width = 14.2cm]{fig/transform_p.pdf}\\
  \caption{From a formula to a graph: (a)
    the input formula; (b) parsing the formula into a tree; (c) merging leaves and
    connecting quantifiers to variables; (d) renaming variables.}
    \label{fig:example}
    \vspace{-2mm}
\end{figure}

Formally, let  be the set of all formulas,  be the set of
constant values,  the set of constant functions,  the set of
variable values,  the set of variable functions, and  the set
of quantifiers. Let  be a higher-order logic formula with no free variables---any free
variables can be bounded by adding quantifiers  to the front of the formula. The graph  of formula  can be recursively
constructed as follows: 

\begin{itemize}
\item if , where , then , i.e.\@ the graph contains a single node . 
\item if , where  and , then we perform  followed by  ,  where  is the
``head node'' of  and   is an operation that merges the same
  constant (leaf) nodes in the graph. 
\item if , where , , , then we perform
, followed by  if  and
  , where  is the nodes that
  represent the variable  in the graph of ,   is an operation that
  merges all nodes representing the variable  into a single node, and
   is an operation that renames  to 
  (or  if  acts as a function). 
\end{itemize}

By construction, our graph is invariant to variable renaming, yet no
syntactic or semantic information is lost. This is because for a variable node (either as a function or value), its
original name in the formula is irrelevant in the graph---the graph structure already
encodes where it is syntactically and which quantifier binds it.

\subsection{Graphs to Embeddings} \label{sec:emb}

To embed a graph to a vector, we take an approach similar to performing convolution or
message passing on graphs~\cite{duvenaud2015convolutional}. The overall idea is to
associate each node with an initial embedding and iteratively update them. As shown in Fig. \ref{fig:binary},
suppose   and each node around  has an initial embedding. We update the embedding of  by the node
embeddings in its neighborhood. After multi-step updates, the embedding of  will contain information from its local strcuture.
Then we max-pool the node embeddings across all of nodes in the graph to form an embedding for the graph. 

To initialize the embedding for each node, we use the one-hot vector that represents the
name of the node. Note that in our graph all variables have the
same name  (or  if the variable acts as a function), so their initial embeddings are the
 same. All other nodes (constants and quantifiers) each have their
names and thus their own one-hot vectors. 

We then repeatedly update the embedding of each node using the embeddings of its
neighbors. Given a graph , at step  we update the embedding  of node 
as follows: 
 
where  is the degree of node ,  and  are update functions using incoming
edges and outgoing edges, and  is an update function to conbine the old embeddings with the new update from neighbor nodes.  We parametrize these update functions as neural networks; the
detailed configurations will be given in Sec.~\ref{config}.

It is worth noting that all node embeddings are updated in parallel using the same update
functions, but the update functions can be different across steps to allow more flexibility. Repeated updates allow each embedding to incorporate information from a bigger neighborhood and thus
capture more global structures. Interestingly, with zero updates, our model reduces to a bag-of-words
representation, that is, a max pooling of individual node embeddings. 

To predict the usefulness of a statement for a conjecture, we send the concatenation of
their embeddings to a classifier. The classification can also be done in the
unconditional setting where only the statement is given; in this case we directly
send the embedding of the statement to a classifier. The parameters of the update
functions and the classifiers are learned end to end through
backpropagation. 

\subsection{Order-Preserving Embeddings} \label{sec:ob}
For functions in a formula, the order of its arguments matters. That is,
 cannot generally be presumed to mean the same as . But our current embedding update as defined in
Eqn.~\ref{eqn:1} is invariant to the ordering of arguments. Given that it is possible that
the ordering of arguments can be a useful feature for premise selection, we now consider
a variant of our basic approach to make our graph embeddings sensitive to the
ordering of arguments. In this variant, we update each node considering the ordering of
its incoming edges and outgoing edges.

Before we define our new update equation, we need to introduce the notion
of a \emph{treelet}. Given a node  in graph , let  be an outgoing
edge of , and let  be the rank of edge  among all outgoing edges of . We define a
\emph{treelet} of graph  as a tuple of nodes  such
that (1) both 
and  are edges in the graph and (2)  is ranked before  among all outgoing edges of . In other words, a
treelet is a subgraph that consists of a head node , a left child  and a right child .  We use 
 to denote all treelets of graph , that is, . 

Now, when we update a node embedding, we consider not only its direct neighbors, but also
its roles in all the treelets it belongs to: 

where  is the number of total treelets containing . 
In this new update equation,  is an update function that considers a treelet where
node  is the left child. Similarly,  considers a treelet where node  is the
head and  considers a treelet where node  is the right
child. As in Sec.~\ref{sec:emb}, the same update functions are applied to all nodes at each step,
but across steps the update functions can be different.  Fig.~\ref{fig:binary} shows the
update equation of a concrete example. 

\begin{figure}
  \centering
    \includegraphics[width=\textwidth]{fig/binary2.pdf}
  \caption{An example of applying the order-preserving updates in Eqn.~\ref{eqn:2}. To
    update node , we consider its neighbors and its position in all \emph{treelets} (see
    Sec.~\ref{sec:ob}) it belongs to. }
  \label{fig:binary}
  \vspace{-4mm}
\end{figure}

Our design of Eqn.~\ref{eqn:2} now allows a node to be embedded differently dependent on
the ordering of its own arguments and dependent on which argument slot it takes in a parent
function. For example, the function node  can now be embedded differently for 
and  because of the output of  can be different. As another example, in the formula
, there are two function nodes with the same name , same parent , and
same child , but they can be embedded differently because only  will be applied to the  as the first
argument of  and only  will be applied to the  as the second argument of
. 

To distinguish the two variants of our approach, we call the method with the treelet
update terms \emph{FormulaNet}, as opposed to the basic \emph{FormulaNet-basic}
without considering edge ordering. 

\section{Experiments}

\subsection{Dataset and Evaluation} 
We evaluate our approach on the HolStep dataset~\cite{holstep}, a recently introduced
benchmark for evaluating machine learning approaches for theorem proving. It was
constructed from the proof trace files of the HOL Light theorem prover~\cite{Harrison2009} on its
multivariate analysis library~\cite{harrison2013hol} and the formal proof of the Kepler conjecture. 
The dataset contains 11,410 conjectures, including 9,999 in the training set and 1,411 in
the test set. Each conjecture is associated with a set of statements, each with a ground
truth label on whether the statement is useful for proving the conjecture. There are 2,209,076
conjecture-statement pairs in total. We hold out 700 conjectures from the training set as the validation set to
tune hyperparameters and perform ablation analysis. 

Following the evaluation setup proposed in~\cite{holstep}, we treat premise selection as
a binary classification task and evaluate classification accuracy. Also following ~\cite{holstep},
we evaluate two settings, the \emph{conditional} setting where both the conjecture and the statement are given, and
the \emph{unconditional} setting where the conjecture is ignored. In HolStep, each
conjecture is associated with an equal number of positive statements and 
 negative statements, so the accuracy of random prediction is . 

\subsection{Network Configurations}\label{config}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{fig/network_config.pdf}
	\caption{Configurations of the update functions and classifiers: (a)  in  Eqn.~\ref{eqn:1} and~\ref{eqn:2}; (b)  in Eqn.~\ref{eqn:1} and ~\ref{eqn:2}, and  in  Eqn.~\ref{eqn:2}; (c) conditional classifier; (d) unconditional classifier.}
	\label{fig:network_config}
	\vspace{-4mm}
\end{figure}

\label{sec:network_config}
The initial one-hot vector for each node has 1909 dimensions, representing
   1909 unique tokens. These 1909 tokens include 1906 unique
  constants from the training set and three special tokens, "VAR", "VARFUNC", and
  "UNKNOWN" (representing all novel tokens during testing). 

The update functions in Eqn.~\ref{eqn:1} and Eqn.~\ref{eqn:2} are parametrized as neural
networks. Fig.~\ref{fig:network_config} (a), (b) shows their configurations. All update functions are
configured the same: concatenation of inputs followed by two fully connected layers with
ReLUs, Batch Normalizations~\cite{ioffe2015batch}.  

The classifier for the conditional setting takes in the embeddings from the conjecture
and the statement. Its configuration is shown in Fig.~\ref{fig:network_config} (c). The classifier for
the unconditional setting uses only the embedding of the statement; its configuration is
shown in Fig.~\ref{fig:network_config} (d). 

\subsection{Model Training}

We train our networks using RMSProp~\cite{hinton2012lecture} with 0.001 learning rate and
 weight decay. We lower the learning rate by 3X after each epoch. We train all
models for five epochs and all networks converge after about three or four epochs. 

It is worth noting that there are two levels of batching in our approach: intra-graph
batching and inter-graph batching. Intra-graph batching arises from the fact that to embed
a graph, each update function ( in Eqn.~\ref{eqn:2}) is applied to all nodes in
parallel. This is the same as training each update function as a standalone network with a
batch of input examples. Thus batch normalization can be applied to the
inputs of each update function \emph{within a single graph}. 

Furthermore, this batch
normalization within a graph can be run in the training mode even when we are
only performing inference to embed a graph, because there are multiple input examples to each update function within a
graph. Another level of batching is the regular batching of multiple graphs in training,
as is necessary for training the classifier. As usual, batch normalization across graphs is done in
 the evaluation mode in test time. 

We also apply intermediate supervision after each step of embedding update using a separate classifier. For training,
our loss function is the sum of cross-entropy losses for each step. We use the prediction
from the last step as our final predictions. 

\subsection{Main Results}

Table~\ref{tab:test} compares the accuracy of our approach versus  the best existing results~\cite{holstep}. Our approach improves the best existing result by a large margin from
 to  in the conditional setting and from  to  in the
unconditional setting. We also see that FormulaNet gives a  improvement over the FormulaNet-basic, validating our hypothesis that the order of function arguments provides useful cues. 

Consistent with prior work [10], conditional and unconditional selection have similar
performances. This is likely due to the data distribution in HolStep. In the
training set, only 0.8\% of the statements appear in both a positive statement-conjecture
pair and a negative statement-conjecture pair, and the upper performance bound of unconditional selection is 97\%. In addition, HolStep
contains 9,999 unique conjectures but 1,304,888 unique statements for training, so it is
likely easier for the network to learn useful patterns from statements than from
conjectures. 


We also apply Deepwalk~\cite{perozzi2014deepwalk}, an unsupervised approach for
  generating node embeddings that is purely based on graph topology without considering
  the token associated with each node. For each formula graph, we max-pool its node embeddings and train a
  classifier. The accuracy is 61.8\% (conditional) and 61.7\%
  (unconditional). This result suggests that for embedding formulas it is important to use token information
  and end-to-end supervision. 

\begin{table}[t]
\caption{Classification accuracy on the test set of our approach versus baseline methods on HolStep in the unconditional setting
          (conjecture unknown) and the
          conditional setting (conjecture given). }
	\centering
	\begin{tabular}{c c c c c }\hline
	& CNN~\cite{holstep} & CNN-LSTM~\cite{holstep}  &FormulaNet-basic& FormulaNet \\ \hline
	Unconditional & 83 & 83 &  89.0 & \textbf{90.0} \\ 
	Conditional     & 82 & 83 &  89.1 & \textbf{90.3} \\ \hline
	\end{tabular}
    \label{tab:test}
    \vspace{-4mm}
\end{table}

\subsection{Ablation Experiments}

\noindent \textbf{Invariance to Variable Renaming} One motivation for our graph representation is that the meaning of formulas should be invariant to
the renaming of variable values and variable functions. To achieve such invariance, we perform
two main transformations of a parse tree to generate a graph: (1) we convert
the tree to a graph by linking quantifiers and variables, and (2) we discard the variable names.

We now study the effect of these steps on the premise selection task. 
We compare FormulaNet-basic with the following three variants whose only difference is the
 format of the input graph: 
\begin{itemize} 
	\item \emph{Tree-old-names}: Use the parse tree as the graph and keep all original 
          names for the nodes. An example is the tree in Fig.~\ref{fig:example} (b). 
	\item \emph{Tree-renamed}:  Use the parse tree as the graph but rename all 
          variable values to  and variable functions to .
	\item \emph{Graph-old-names}: Use the same graph as FormulaNet-basic but keep all
          original names for the nodes, thus making the graph embedding dependent on the original
          variable names. An example is the graph in Fig.~\ref{fig:example} (c). 
\end{itemize}
We train these variants on the same training set as FormulaNet-basic. To compare with
FormulaNet-basic, we evaluate them on the same held-out validation set. In addition, we generate
a new validation set (Renamed Validation) by randomly permutating the variable names in
the formulas---the textual representation is different but the semantics remains the
same. We also compare all models on this renamed validation set to evaluate their
robustness to variable renaming.

Table~\ref{tab:rename} reports the results. If we use a tree
with the original names, there is a slight drop when evaluate on the original validation
set, but there is a very large drop when evaluated on the renamed validation set. This
shows that there are features exploitable in the original variable names and the
model is exploiting it, but the model is essentially overfitting to the bias in the
original names and cannot generalize to renamed formulas. The same applies to the model trained on graphs with the
original names, whose performance also drops drastically on renamed formulas. 

It is also interesting to note that the model trained on renamed trees performs poorly,
although it is invariant to variable renaming. This shows that the syntactic and semantic
information encoded in the graph on variables---particularly their quantifiers and
coreferences---is important.  

\begin{table}[t]
\caption{The accuracy of FormulaNet-basic and its ablated versions on original and renamed validation set. }
	\centering
	\vspace{-1mm}
	\begin{tabular}{c c c c c }\hline
	 & Tree-old-names & Tree-renamed & Graph-old-names & Our Graph \\ \hline
	Original Validation & 89.7 & 84.7 & 89.8 & 89.9\\ 
	Renamed Validation & 82.3 & 84.7 & 83.5 & 89.9\\ \hline
	\end{tabular}
	\label{tab:rename}
\end{table} 

\begin{table}[t]
	\caption{Validation accuracy of proposed models with different numbers of update steps on conditional premise selection.}
	\centering
	\vspace{-2mm}
	\begin{tabular}{l c c c c c }
	\hline
	Number of steps & 0 & 1 & 2 & 3 & 4 \\ \hline
	FormulaNet-basic  & 81.5 & 89.3 & 89.8 & 89.9 & 90.0 \\ 
	FormulaNet & 81.5 & 90.4 & 91.0 & 91.1 & 90.8 \\
	\hline
	\end{tabular}
	\label{tab:multisteps}
	\vspace{-4mm}
\end{table}

\noindent \textbf{Number of Update Steps}
An important hyperparameter of our approach is the number of steps to update the
embeddings. Zero steps can only embed a bag of unstructured tokens, while more steps can
embed information from larger graph structures. Table~\ref{tab:multisteps} compares the accuracy of models with different numbers of
update steps. Perhaps surprisingly, models with zero steps can already achieve an accuracy of 
, showing that much of the performance comes from just the names of 
constant functions and values. More steps lead to notable increases of accuracy, showing that the
structures in the graph are important. There is a diminishing return after 3 steps, but
this can be reasonably expected because a
radius of 3 in a graph is a fairly sizable neighborhood and can encompass reasonably complex
expressions---a node can influence its grand-grandchildren and
grand-grandparents. In addition, it would naturally be more difficult to learn generalizable
features from long-range patterns because they are more varied and each of them occurs
much less frequently. 

\begin{figure}[b]
	\centering
	\includegraphics[width=\textwidth]{fig/similarity_dashed.pdf}
	\caption{Nearest neighbors of
          node embeddings after step 1 with FormulaNet. Query nodes are in the first
          column. The color of each node is coded by a t-SNE~\cite{maaten2008visualizing} projection of its
          step-0 embedding into  2D. The closer the colors,  the nearer two nodes are in
          the step-0 embedding space. }
	\label{fig:nearestneighbor}
	\vspace{-4mm}
\end{figure}
\subsection{Visualization of Embeddings}

To qualitatively examine the learned embeddings, we find out a set of nodes with
similar embeddings and visualize their local structures in
Fig.~\ref{fig:nearestneighbor}. In each row, we use a node as the query and find the
nearest neighbors across all nodes from different graphs. We can see that the nearest neighbors have similar
structures in terms of topology and naming. This demonstrates that our
graph embeddings can capture syntactic and semantic structures of a formula. 



\section{Conclusion}
In this work, we have proposed a deep learning-based approach to premise selection. We represent a higher-order logic formula
as a graph that is invariant to variable renaming but fully preserves syntactic and
semantic information. We then embed the graph into a continuous vector through a novel
embedding method that preserves the information of edge ordering. Our approach has achieved
 state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83\% to 90.3\%.

\paragraph{Acknowledgements}
This work is partially supported by the National Science Foundation under Grant No. 1633157.

\small
\begin{thebibliography}{10}

\bibitem{robinson2001handbook}
Alan~JA Robinson and Andrei Voronkov.
\newblock {\em Handbook of automated reasoning}, volume~1.
\newblock Elsevier, 2001.

\bibitem{kern1999formal}
Christoph Kern and Mark~R Greenstreet.
\newblock Formal verification in hardware design: a survey.
\newblock {\em ACM Transactions on Design Automation of Electronic Systems
  (TODAES)}, 4(2):123--193, 1999.

\bibitem{klein2009sel4}
Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock,
  Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael
  Norrish, et~al.
\newblock sel4: Formal verification of an os kernel.
\newblock In {\em Proceedings of the ACM SIGOPS 22nd symposium on Operating
  systems principles}, pages 207--220. ACM, 2009.

\bibitem{leroy2009formal}
Xavier Leroy.
\newblock Formal verification of a realistic compiler.
\newblock {\em Communications of the ACM}, 52(7):107--115, 2009.

\bibitem{deepmath}
Alexander~A Alemi, Francois Chollet, Niklas Een, Geoffrey Irving, Christian
  Szegedy, and Josef Urban.
\newblock Deepmath - deep sequence models for premise selection.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  2235--2243. Curran Associates, Inc., 2016.

\bibitem{Naumowicz2009}
Adam Naumowicz and Artur Korni{\l}owicz.
\newblock {\em A Brief Overview of Mizar}, pages 67--72.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2009.

\bibitem{Harrison2009}
John Harrison.
\newblock {\em HOL Light: An Overview}, pages 60--66.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2009.

\bibitem{hoder2011sine}
Kry{\v{s}}tof Hoder and Andrei Voronkov.
\newblock Sine qua non for large theory reasoning.
\newblock In {\em International Conference on Automated Deduction}, pages
  299--314. Springer, 2011.

\bibitem{alama2014premise}
Jesse Alama, Tom Heskes, Daniel K{\"u}hlwein, Evgeni Tsivtsivadze, and Josef
  Urban.
\newblock Premise selection for mathematics by corpus analysis and kernel
  methods.
\newblock {\em Journal of Automated Reasoning}, 52(2):191--213, 2014.

\bibitem{holstep}
Cezary Kaliszyk, Fran{\c{c}}ois Chollet, and Christian Szegedy.
\newblock Holstep: A machine learning dataset for higher-order logic theorem
  proving.
\newblock {\em arXiv preprint arXiv:1703.00426}, 2017.

\bibitem{harrison2014history}
John Harrison, Josef Urban, and Freek Wiedijk.
\newblock History of interactive theorem proving.
\newblock In {\em Computational Logic}, volume~9, pages 135--214, 2014.

\bibitem{coq}
Gilles Dowek, Amy Felty, Hugo Herbelin, G{\'e}rard Huet, Chetan Murthy,
  Catherin Parent, Christine Paulin-Mohring, and Benjamin Werner.
\newblock {\em The COQ Proof Assistant: User's Guide: Version 5.6}.
\newblock INRIA, 1992.

\bibitem{wenzel2008isabelle}
Makarius Wenzel, Lawrence~C Paulson, and Tobias Nipkow.
\newblock The isabelle framework.
\newblock In {\em International Conference on Theorem Proving in Higher Order
  Logics}, pages 33--38. Springer, 2008.

\bibitem{schulz2002brainiac}
Stephan Schulz.
\newblock E--a brainiac theorem prover.
\newblock {\em Ai Communications}, 15(2, 3):111--126, 2002.

\bibitem{hales2015formal}
Thomas Hales, Mark Adams, Gertrud Bauer, Dat~Tat Dang, John Harrison, Truong~Le
  Hoang, Cezary Kaliszyk, Victor Magron, Sean McLaughlin, Thang~Tat Nguyen,
  et~al.
\newblock A formal proof of the kepler conjecture.
\newblock {\em arXiv preprint arXiv:1501.02155}, 2015.

\bibitem{Gonthier2013}
Georges Gonthier, Andrea Asperti, Jeremy Avigad, Yves Bertot, Cyril Cohen,
  Fran{\c{c}}ois Garillot, St{\'e}phane Le~Roux, Assia Mahboubi, Russell
  O'Connor, Sidi Ould~Biha, Ioana Pasca, Laurence Rideau, Alexey Solovyev,
  Enrico Tassi, and Laurent Th{\'e}ry.
\newblock {\em A Machine-Checked Proof of the Odd Order Theorem}, pages
  163--179.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.

\bibitem{suttner1990automatic}
Christian Suttner and Wolfgang Ertel.
\newblock Automatic acquisition of search guiding heuristics.
\newblock In {\em International Conference on Automated Deduction}, pages
  470--484. Springer, 1990.

\bibitem{denzinger1999learning}
J{\"o}rg Denzinger, Matthias Fuchs, Christoph Goller, and Stephan Schulz.
\newblock Learning from previous proof experience: A survey.
\newblock {\em Citeseer}, 1999.

\bibitem{schulz2000learning}
SA~Schulz.
\newblock {\em Learning search control knowledge for equational deduction},
  volume 230.
\newblock IOS Press, 2000.

\bibitem{farber2016internal}
Michael F{\"a}rber and Chad Brown.
\newblock Internal guidance for satallax.
\newblock In {\em International Joint Conference on Automated Reasoning}, pages
  349--361. Springer, 2016.

\bibitem{kaliszyk2015femalecop}
Cezary Kaliszyk and Josef Urban.
\newblock Femalecop: fairly efficient machine learning connection prover.
\newblock In {\em Logic for Programming, Artificial Intelligence, and
  Reasoning}, pages 88--96. Springer, 2015.

\bibitem{whalen2016holophrasm}
Daniel Whalen.
\newblock Holophrasm: a neural automated theorem prover for higher-order logic.
\newblock {\em arXiv preprint arXiv:1608.02644}, 2016.

\bibitem{deep_guide_proof}
Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk.
\newblock Deep network guided proof search.
\newblock {\em arXiv preprint arXiv:1701.06972}, 2017.

\bibitem{SocherEtAl2011:PoolRAE}
Richard Socher, Eric~H. Huang, Jeffrey Pennin, Christopher~D Manning, and
  Andrew~Y. Ng.
\newblock Dynamic pooling and unfolding recursive autoencoders for paraphrase
  detection.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, {\em Advances in Neural Information Processing
  Systems 24}, pages 801--809. Curran Associates, Inc., 2011.

\bibitem{socher2011parsing}
Richard Socher, Cliff~C Lin, Chris Manning, and Andrew~Y Ng.
\newblock Parsing natural scenes and natural language with recursive neural
  networks.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 129--136, 2011.

\bibitem{kuhlwein2015males}
Daniel K{\"u}hlwein and Josef Urban.
\newblock Males: A framework for automatic tuning of automated theorem provers.
\newblock {\em Journal of Automated Reasoning}, 55(2):91--116, 2015.

\bibitem{Bridge2014}
James~P. Bridge, Sean~B. Holden, and Lawrence~C. Paulson.
\newblock Machine learning for first-order theorem proving.
\newblock {\em Journal of Automated Reasoning}, 53(2):141--172, 2014.

\bibitem{perozzi2014deepwalk}
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
\newblock Deepwalk: Online learning of social representations.
\newblock In {\em Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 701--710. ACM, 2014.

\bibitem{tang2015line}
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
\newblock Line: Large-scale information network embedding.
\newblock In {\em Proceedings of the 24th International Conference on World
  Wide Web}, pages 1067--1077. ACM, 2015.

\bibitem{grover2016node2vec}
Aditya Grover and Jure Leskovec.
\newblock node2vec: Scalable feature learning for networks.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 855--864. ACM, 2016.

\bibitem{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In {\em Advances in neural information processing systems}, pages
  3111--3119, 2013.

\bibitem{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock {\em arXiv preprint arXiv:1301.3781}, 2013.

\bibitem{goller1996learning}
Christoph Goller and Andreas Kuchler.
\newblock Learning task-dependent distributed representations by
  backpropagation through structure.
\newblock In {\em Neural Networks, 1996., IEEE International Conference on},
  volume~1, pages 347--352. IEEE, 1996.

\bibitem{tai2015improved}
Kai~Sheng Tai, Richard Socher, and Christopher~D Manning.
\newblock Improved semantic representations from tree-structured long
  short-term memory networks.
\newblock {\em arXiv preprint arXiv:1503.00075}, 2015.

\bibitem{gori2005new}
Marco Gori, Gabriele Monfardini, and Franco Scarselli.
\newblock A new model for learning in graph domains.
\newblock In {\em Neural Networks, 2005. IJCNN'05. Proceedings. 2005 IEEE
  International Joint Conference on}, volume~2, pages 729--734. IEEE, 2005.

\bibitem{scarselli2009graph}
Franco Scarselli, Marco Gori, Ah~Chung Tsoi, Markus Hagenbuchner, and Gabriele
  Monfardini.
\newblock The graph neural network model.
\newblock {\em IEEE Transactions on Neural Networks}, 20(1):61--80, 2009.

\bibitem{duvenaud2015convolutional}
David~K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell,
  Timothy Hirzel, Al{\'a}n Aspuru-Guzik, and Ryan~P Adams.
\newblock Convolutional networks on graphs for learning molecular fingerprints.
\newblock In {\em Advances in neural information processing systems}, pages
  2224--2232, 2015.

\bibitem{li2015gated}
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
\newblock Gated graph sequence neural networks.
\newblock {\em arXiv preprint arXiv:1511.05493}, 2015.

\bibitem{jain2016structural}
Ashesh Jain, Amir~R Zamir, Silvio Savarese, and Ashutosh Saxena.
\newblock Structural-rnn: Deep learning on spatio-temporal graphs.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5308--5317, 2016.

\bibitem{henaff2015deep}
Mikael Henaff, Joan Bruna, and Yann LeCun.
\newblock Deep convolutional networks on graph-structured data.
\newblock {\em arXiv preprint arXiv:1506.05163}, 2015.

\bibitem{defferrard2016convolutional}
Micha{\"e}l Defferrard, Xavier Bresson, and Pierre Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3837--3845, 2016.

\bibitem{kipf2016semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock {\em arXiv preprint arXiv:1609.02907}, 2016.

\bibitem{niepert2016learning}
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.
\newblock Learning convolutional neural networks for graphs.
\newblock In {\em Proceedings of the 33rd annual international conference on
  machine learning. ACM}, 2016.

\bibitem{church1940formulation}
Alonzo Church.
\newblock A formulation of the simple theory of types.
\newblock {\em The journal of symbolic logic}, 5(02):56--68, 1940.

\bibitem{harrison2013hol}
John Harrison.
\newblock The hol light theory of euclidean space.
\newblock {\em Journal of Automated Reasoning}, pages 1--18, 2013.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{hinton2012lecture}
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
\newblock Lecture 6a overview of mini--batch gradient descent.
\newblock {\em Coursera Lecture slides https://class. coursera.
  org/neuralnets-2012-001/lecture,[Online}, 2012.

\bibitem{maaten2008visualizing}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of Machine Learning Research}, 9(Nov):2579--2605, 2008.

\end{thebibliography}
 
\end{document}
