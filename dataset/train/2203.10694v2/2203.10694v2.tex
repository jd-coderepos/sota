

\documentclass[runningheads]{llncs}



\usepackage{graphicx}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage[accsupp]{axessibility}  


\usepackage{ruler}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}


\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother


\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{paralist}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\definecolor{RowColorCode}{rgb}{0.61,0.57,0.89}
\usepackage{color, colortbl}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{wrapfig}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\begin{document}

\newcommand{\bigO}[1]{\mathcal{O}(#1)}
\newcommand{\sota}{state-of-the-art}
\newcommand{\tian}[1]{\textcolor{orange}{#1}}
\newcommand{\divya}[1]{\textcolor{blue}{#1}}
\newcommand{\model}{FAR}
\newcommand{\datanames}{Sub-$k$ matrices}
\newcommand{\G}{$G$}
\newcommand{\V}{$V$}
\newcommand{\E}{$E$}
\newcommand{\brr}[1]{\left( #1 \right)}
\newcommand{\bcc}[1]{ \left{ #1 \right} }
\newcommand{\bss}[1]{\left[ #1 \right]}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\sg}{\mathcal{L}}
\newcommand{\li}{\sg}
\newcommand{\vts}[1]{\lvert #1 \rvert}
\newcommand{\Vts}[1]{\lVert #1 \rVert}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand\inv[1]{#1\raisebox{1.05ex}{$\scriptscriptstyle-\!1$}}
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         \newcommand\Bstrut{\rule[-1.3ex]{0pt}{0pt}}   \newcommand\Bstrutfrac{\rule[-0.7ex]{0pt}{0pt}}   \newcommand\Tstrutfrac{\rule{0pt}{1.7ex}}         \newcommand\mathdash{\text{\normalfont --}}
\newcommand{\cost}{\bigO{ \vts{\li^{\scriptscriptstyle -1}_t}k }}
\newcommand{\cm}{\mathcal{M}_{\Delta t}(u)}
\newcommand{\pc}{\zeta_c(t)}
\newcommand{\pd}{\zeta_d(t)}
\newcommand{\pe}{\zeta_e(t)}
\newcommand{\costk}{\bigO{ \vts{\li^{\scriptscriptstyle -1}_t} }}
\newcommand\setrow[1]{\gdef\rowmac{#1}#1\ignorespaces}
\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother
\newcommand{\size}{\bigO{d}}
\newcommand{\shorteq}{\settowidth{\@tempdima}{-}\resizebox{\@tempdima}{\height}{=}}
\newcommand*\midpoint[1]{\overline{#1}}
\newcommand{\mysetminus}{\mathbin{\fgebackslash}}



\let\proof\relax
\let\endproof\relax

\linespread{0.97}
\setlength{\parskip}{-0.1em}
\mathchardef\mhyphen="2D

\newcommand{\minus}{\scalebox{0.75}[1.0]{$-$}} 
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{4270}  

\title{Supplementary Material for FAR: Fourier Aerial Video Recognition}

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{FAR: Fourier Aerial Video Recognition}
\author{Divya Kothandaraman\inst{1}\orcidID{0000-0002-6276-4968} \and
Tianrui Guan\inst{2,3}\orcidID{0000-0002-6892-9778} \and
Xijun Wang\inst{3} \and
Shuowen Hu\inst{3} \and
Ming Lin\inst{3}\orcidID{0000-0003-3736-6949} \and
Dinesh Manocha\inst{3}\orcidID{0000-0001-7047-9801}}
\authorrunning{D. Kothandaraman et al.}
\institute{University of Maryland College Park, United States \and
Army Research Laboratory, United States \\
\email{dkr@umd.edu}\\
\url{https://gamma.umd.edu/far} }
\maketitle

\section{Datasets}

We describe the UAV datasets used for evaluating FAR. 

\paragraph{UAV Human RGB~\cite{li2021uav}:} UAV Human is the largest UAV-based human behavior understanding dataset. Split $1$ contains $15172$ and $5556$ images for training and testing respectively. This challenging dataset covers human actions captured under varying illumination, time of day (daytime, nighttime), different subjects and backgrounds, weathers, occlusions, etc, across $155$ diverse human actions. UAV Human RGB is collected by drones with an Azure Kinect DK camera. The videos are of resolution $1920\times1080$. The dataset is available at  https://sutdcv.github.io/uav-human-web/. 

\paragraph{UAV Human Night Camera~\cite{li2021uav}:} UAV Human Night Camera contains videos similar to UAV Human RGB captured using a night-vision camera. The night vision camera captures videos in color mode in the daytime, and grey-scale mode in the nighttime. The resolution of the videos is $640\times480$. The dataset is available at  https://sutdcv.github.io/uav-human-web/. 


\paragraph{Drone Action~\cite{perera2019drone}:} Drone Action is an outdoor drone video dataset captured using a free flying drone. It has $240$ HD RGB videos with $66919$ frames, across $13$ human actions. The dataset is available at https://asankagp.github.io/droneaction/.

\paragraph{NEC Drone~\cite{choi2020unsupervised}:} NEC Drone dataset is an indoor UAV video dataset with $16$ human actions captured by a DJI Phantom 4.0 pro v2 drone, performed by human subjects in an unconstrained manner. The dataset contains $2079$ labeled videos at a resolution of $1920\times1080$. It has $10$ single person actions such as walk, run, jump, etc, and $6$ two person actions such as shake hands, push a person, etc. The dataset is available at https://www.nec-labs.com/~mas/NEC-Drone/. 

\section{Implementation Details}

In the interest of reproducibility, we will make all code and pretrained models publicly available upon acceptance of the paper. We also attach the codes used in our experiments with the supplementary zip folder submitted for review. 

\label{exp:implementation}
\paragraph{Backbone network architecture:} We benchmark our models using two state-of-the-art video recognition backbone architectures (i) I3D \cite{carreira2017quo} (CVPR 2017) (ii) X3D-M \cite{feichtenhofer2020x3d} (CVPR 2020). I3D is a 3D inflated CNN, based on 2D CNN inflation, and enables the learning of spatial-temporal features. X3D is also a 3D inflated CNN, and progressively expands a 2D CNN along multiple network axes such as space, time, width and depth. 
For both X3D and I3D, we extract mid-level features after the second layer.

\paragraph{Training details:} Our models were trained using NVIDIA GeForce 1080 Ti GPUs, and NVIDIA RTX A5000 GPUs. Initial learning rates were \{$0.01$, and $0.001$\} across datasets. We use cosine annealing and poly annealing for learning rate decay in X3D and I3D respectively, 
We use the Stochastic Gradient Descent (SGD) optimizer with weight decay of $0.0005$ and momentum of $0.9$, and cosine/ poly annealing for learning rate decay. The final softmax predictions of all our models were constrained using multi-class cross entropy loss. 

\section{Fourier Disentanglement}

Videos depicting human action have four types of entities: moving salient regions (typically corresponding to moving object), static salient regions (typically corresponding to static object), moving non-salient regions (typically corresponding to dynamic background), and static non-salient regions (typically corresponding to static background). Robust action recognition systems should learn features that heavily amplify moving objects, followed by static objects (that provide contextual cues and are relevant to the prediction). This should be followed by background entities. According to our formulation, dynamic salient regions are amplified the most. This is because the Fourier mask highlights dynamic regions, and the features learnt by the network have a higher amplitude at the salient regions. Static non-salient regions are at the other end of the spectrum because the Fourier mask suppresses these regions, as well as the features learnt by the network have a lower amplitude at the non-salient regions. Static-salient and dynamic salient regions lie at the middle of the spectrum. The final equation for Fourier disentanglement uses the $l2$ operation in the computation of $M_{FO}$ and linear application of $f$. This implies that static salient regions have a higher amplitude than the dynamic non-salient regions. Thus, the ordering of amplitudes that is formed as: dynamic-salient $>$ static-salient $>$ dynamic-non-salient $>$ static-non-salient, in concordance with the relevance for decision making for action recognition. Thus, static as well as dynamic background regions have lower amplitudes than static and dynamic regions of the object executing action. 

In addition, the video may contain noise (light noise or otherwise) and camera movement. In regions of the video where there is noise, the amplitude of the feature map depicting saliency will be low. Hence, noise gets suppressed. Any movement of non-salient pixels due to camera motion gets suppressed since they are a part of dynamic non-salient regions. Moreover, camera motion is generally uniform across the spatial dimensions of the video (covering salient as well as non-salient regions). Thus, it doesn't impact the decision making ability of the aerial video recognition system.  

\noindent \textbf{Comparisons with motion-based methods.} Motion-based methods either model spatial and temporal information separately using two-stream 2D CNNs \cite{lee2018motion} or use motion representation as an auxiliary guiding factor to 3D CNNs. The latter is very expensive \cite{piergiovanni2019representation}. In contrast, we jointly model space and time using a 3D backbone, and then disentangle the moving human actor from the background using FO. Prior work has demonstrated the superiority \cite{feichtenhofer2019slowfast,feichtenhofer2020x3d} of 3D CNNs over two-stream 2D CNNs. FO imparts a relative improvement of $22.93\%$ over the 3D I3D backbone and can be used with any 3D CNN to achieve state-of-the-art performance.

\section{Fourier Attention}

\begin{lemma}
Given an input matrix A, Fourier attention as well self-attention \cite{vaswani2017attention,bertasius2021space} encapsulate long-range relationships for global mixing by computing outer products.
\end{lemma}
\paragraph{Proof}
\textbf{Self-attention:} Without loss of generality, let $[a_{ij}]$ denote the elements of a square matrix A (with dimensions $N$) in $2D$. $f$, $g$, $h$ represent $1\times 1$ convolutions for key, query, value computations in self-attention. Hence, key, query and value vectors are $[fa_{ij}]$, $[ga_{ij}]$ and $[ha_{ij}]$ respectively. The first step of self-attention is the computation of sub-attention, which is the matrix multiplication of the transpose of query with key, which is $[ga_{ij}]^{T} \odot [fa_{ij}]$, which is equal to $\sum_{i=1}^{N} ga_{mi} \times fa_{in}$. The next step is the computation of self-attention, which is the matrix multiplication of the value vector with the transpose of sub-attention, which is equal to $[ha_{ij}] \odot \sum_{k=1}^{N} ga_{lk} \times fa_{kn}$. Hence, the self-attention matrix $S_{mn}$ is:
\begin{equation}
    S_{mn} = \sum_{l=1}^{N} ha_{ml} \sum_{k=1}^{N} [ ga_{lk} \times fa_{kn}]
    \label{eq:sa}
\end{equation}
\textbf{Fourier-attention:} Without loss of generality, let $[a_{ij}]$ denote the elements of a square matrix A (with dimensions $N$) in $2D$. The Fourier transform is $\sum_{i=1}^{N} \sum_{j=1}^{N} \exp(\minus 2\pi mi/N) \exp(\minus 2\pi nj/N)$. Multiplication of the Fourier transform with its conjugate transpose, and inverse FFT gives us $\sum_{b=1}^{N} \sum_{c=1}^{N} \exp(\minus 2\pi mc/N) \exp(\minus 2\pi nb/N)a_{mn}  \times \nonumber  \{\sum_{j=1}^{N} \sum_{i=1}^{N} \exp(\minus 2\pi j(b\minus c)/N) a_{ij} \times \exp(\minus 2\pi i(c\minus b)/N)a_{ij}\}$. Finally, weighted multiplication of the above term with $[a_{ij}]$ and a careful rearrangement of the terms involved leads us to the final expression for Fourier attention. Fourier attention $F_{mn}$ is:
\begin{align}
    F_{mn} = \sum_{b=1}^{N} \sum_{c=1}^{N} \overbrace{\exp(\minus 2\pi mc/N) \exp(\minus 2\pi nb/N)}^{h_{mn}(b,c)}a_{mn}  \times \nonumber \\[-10pt] \{\sum_{j=1}^{N} \sum_{i=1}^{N} \underbrace{\exp(\minus 2\pi j(b\minus c)/N)}_{f_{mn}(b,c)} a_{ij} \times \underbrace{\exp(\minus 2\pi i(c\minus b)/N)}_{g_{mn}(b,c)}a_{ij}\}
    \label{eq:fa}
\end{align}

$f$, $g$, $h$ in Equation \ref{eq:sa} are $1\times 1$ convolutions, and that the exponential terms span the entire spectrum of frequencies lets us define $f$, $g$, $h$ for Fourier attention as shown in Equation \ref{eq:fa}. Thus, the equation for Fourier attention can be simplified as:
\begin{align}
    F_{mn} = \sum_{b=1}^{N} \sum_{c=1}^{N} h_{mn}(b,c)a_{mn} \times \nonumber \\[-10pt] \{\sum_{j=1}^{N} \sum_{i=1}^{N} f_{mn}(b,c)a_{ij} \times g_{mn}(b,c)a_{ij}\}
    \label{eq:fa1}
\end{align}
In self-attention, f,g,h are learnable. In contrast, in Fourier attention, f,g,h are pre-defined by the Fourier spectrum. Nonetheless, they exhaustively cover the Fourier spectrum. Moreover, the terms involved and the structure of computations (multiplications followed by summation) in Equations \ref{eq:sa} and \ref{eq:fa1} are similar, both promote global mixing and encapsulate long-range relationships. 

\begin{figure*}[t]
    \centering
    \captionsetup[subfigure]{labelformat=empty, font=tiny}
\begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.09]{Figures/UAVHumanRGB2/Image395_Class112_frame2.png}
    \caption{rear rt.turn}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image395_Class112_frame2_before.png}
    \caption{Before FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image395_Class112_frame2_objbackdisentangle.png}
    \caption{After FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.09]{Figures/UAVHumanRGB2/Image392_Class133_frame2.png}
    \caption{chaseHumn}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image392_Class133_frame2_before.png}
    \caption{Before FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image392_Class133_frame2_objbackdisentangle.png}
    \caption{After FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.09]{Figures/UAVHumanRGB2/Image385_Class91_frame3.png}
    \caption{Drink toast}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image385_Class91_frame3_before.png}
    \caption{Before FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image385_Class91_frame3_objbackdisentangle.png}
    \caption{After FO}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.09]{Figures/UAVHumanRGB2/Image362_Class122_frame3.png}
    \caption{Dig a hole}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image362_Class122_frame3_before.png}
    \caption{Before FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image362_Class122_frame3_objbackdisentangle.png}
    \caption{After FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.09]{Figures/UAVHumanRGB2/Image342_Class60_frame3.png}
    \caption{Kick aside}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image342_Class60_frame3_before.png}
    \caption{Before FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image342_Class60_frame3_objbackdisentangle.png}
    \caption{After FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.09]{Figures/UAVHumanRGB2/Image292_Class104_frame2.png}
    \caption{Move left}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image292_Class104_frame2_before.png}
    \caption{Before FO}
    \end{subfigure}
    \begin{subfigure}[b]{0.1\textwidth}
    \includegraphics[scale=0.72]{Figures/UAVHumanRGB2/Image292_Class104_frame2_objbackdisentangle.png}
    \caption{After FO}
    \end{subfigure}
   
    \caption{\small{\textbf{Qualitative results on UAV Human RGB.} We show the effect of our Fourier Object Disentanglement (FO) method. In each sample, the images, in order, correspond to a frame from the video, feature representation before disentanglement and the feature representation after disentanglement respectively. Notice the effectiveness of FO in scenes with light noise, dim light, dynamic camera and dynamic background. Regions of the scene corresponding to moving human actor (or salient dynamic) are amplified most (solid yellow). Static background is completely suppressed (solid purple). Static salient regions are slightly amplified, and dynamic backgrounds are suppressed to a great extent. We show videos depicting various complexities along with the predictions in the video file attached with the supplementary.}}
    \label{fig:visualisations_uavhumanrgb1}
    
\end{figure*}

 
\begin{figure*}[t]
    \centering
\begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image0_PredClass3_GTClass56_frame1.png}
    \caption{Predicted: Drop something \\ GT: Put hands on hips}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image10_PredClass44_GTClass109_frame1.png}
    \caption{Predicted: Open the bottle \\ GT: Decelerate \\}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image11_PredClass42_GTClass18_frame1.png}
    \caption{Predicted: Punch with fists \\ GT: Cheer \\}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image2_PredClass76_GTClass82_frame1.png}
    \caption{Predicted: Pushing someone \\ GT: Rob something from someone}
    \end{subfigure}\\
    
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image5_PredClass125_GTClass48_frame1.png}
    \caption{Predicted: Smoke \\ \\ GT: Apply cream to hands}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image7_PredClass25_GTClass8_frame1.png}
    \caption{Predicted: Play with cell phones \\ GT: Applaud \\}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image8_PredClass65_GTClass121_frame1.png}
    \caption{Predicted: Blow nose \\ GT: Throw litter \\}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[scale=0.2]{Figures/UAVHumanRGBFail/Image4_PredClass8_GTClass32_frame1.png}
    \caption{Predicted: Applaud \\ GT: Cross palms together}
    \end{subfigure}
    \caption{\textbf{Failure cases on UAV Human RGB.} We show frames from UAV Human RGB videos where \model~predicts the wrong class. In many cases, we observe that the predicted class has pixel level interactions similar to the ground truth. For instance, in case (d), both, predicted class and GT are two-person actions, and entail one person harming the other. Similarly, in video (h), both actions involve interaction between the two hands of a person. In video (a), both actions correspond to a human standing straight with hands at hip level. It would be interesting to explore learning distinguishable feature representations for the $155$ classes as a part of future work.}
    \label{fig:fail_uavhumanrgb}
    
\end{figure*}

 
\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
