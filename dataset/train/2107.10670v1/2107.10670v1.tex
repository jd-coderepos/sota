\documentclass[sigconf]{acmart}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\usepackage{color} \usepackage{xspace} \usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{tikz} \usepackage{ulem}
\usepackage{color}
\usepackage{graphicx}




\usepackage[ruled,linesnumbered]{algorithm2e}




\newcommand{\zhou}[1]{{\color{blue}#1}}
\newcommand{\zhoucom}[1]{{\color{red}(zhoucom:#1)}} \newcommand{\li}[1]{{\color{green} \sout{#1}}} \newcommand{\old}[1]{{\color{black}#1}} \newcommand{\licom}[1]{{\color{black}#1}} \newcommand{\hide}[1]{} 

\newcommand{\note}[1]{{\color{red}({\bf{note:}} \emph{#1}})}
\newcommand{\B}[1]{{\bfseries #1}}
\newcommand{\model}{\textsf{SIGN}\xspace}
\newcommand{\gnn}{PGAL\xspace}
\newcommand{\pool}{PiPool\xspace}

\newcommand{\graph}{\ensuremath{\mathcal{G}_I}}
\newcommand{\proteinV}{\ensuremath{\mathcal{V}^P}}
\newcommand{\ligandV}{\ensuremath{\mathcal{V}^L}}
\newcommand{\proteinM}{\ensuremath{M^P}}
\newcommand{\ligandM}{\ensuremath{M^L}}

\newcommand{\angleD}{\ensuremath{\bm{D_A}}}
\newcommand{\cat}{\ensuremath{\mathbin\Vert}}

\newcommand{\dta}{protein-ligand binding affinity}

\newcommand{\mycaption}[1]{\caption{\normalfont{#1}}}
\newcommand{\tup}[0]{\ensuremath{^\triangle}\xspace}
\newcommand{\btup}[0]{\ensuremath{^\blacktriangle}\xspace}
\newcommand{\tdown}[0]{\ensuremath{^\triangledown}\xspace}
\newcommand{\btdown}[0]{\ensuremath{^\blacktriangledown}\xspace}
 
\copyrightyear{2021} 
\acmYear{2021} 
\setcopyright{acmcopyright}\acmConference[KDD '21]{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}{August 14--18, 2021}{Virtual Event, Singapore}
\acmBooktitle{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21), August 14--18, 2021, Virtual Event, Singapore}
\acmPrice{15.00}
\acmDOI{10.1145/3447548.3467311}
\acmISBN{978-1-4503-8332-5/21/08}





\settopmatter{printacmref=true}

\begin{document}
\fancyhead{}


\title{Structure-aware Interactive Graph Neural Networks \\ for the Prediction of Protein-Ligand Binding Affinity}

\author{Shuangli Li{}, Jingbo Zhou{}, Tong Xu{}, Liang Huang{}, Fan Wang{}}
\author{Haoyi Xiong{}, Weili Huang{}, Dejing Dou{}, Hui Xiong{}}
\thanks{This work was done when the first author was an intern in Baidu Research under the supervision of the second author.}
\thanks{Corresponding authors.}

\affiliation{University of Science and Technology of China,Business Intelligence Lab, Baidu Research\country{}}
\affiliation{ Baidu Inc., Baidu Research USA, Oregon State University, HWL Consulting LLC, Rutgers University\country{}}
\email{lsl1997@mail.ustc.edu.cn, {zhoujingbo, wangfan04, xionghaoyi, doudejing}@baidu.com}
\email{tongxu@ustc.edu.cn, {liang.huang.sh, lwlily99}@gmail.com, hxiong@rutgers.edu}






\begin{abstract} \label{sec-abstract}





Drug discovery often relies on the successful prediction of protein-ligand binding affinity. Recent advances have shown great promise in applying graph neural networks (GNNs) for better affinity prediction by learning the representations of protein-ligand complexes. However, existing solutions usually treat protein-ligand complexes as topological graph data, thus\hide{and} the biomolecular structural information\hide{ of atoms} is not fully utilized. The essential long-range interactions among atoms are also neglected in\hide{ existing} GNN models. To this end, we propose a structure-aware interactive graph neural network (\model) which consists of two components: polar-inspired graph attention layers (\gnn) and pairwise interactive pooling (\pool).  Specifically, \gnn iteratively performs the node-edge aggregation process to update\hide{ the} embeddings of nodes and edges while preserving the distance and angle information among atoms\hide{ in protein-ligand complexes}. Then, \pool is adopted to gather interactive edges\hide{ based on atomic types} with a subsequent reconstruction loss to reflect the global interactions\hide{ in the complex}. 
Exhaustive experimental study on two benchmarks verifies the superiority of \model.







\end{abstract}

%
 




\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10010294</concept_id>
<concept_desc>Computing methodologies~Neural networks</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010444.10010450</concept_id>
<concept_desc>Applied computing~Bioinformatics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010405.10010444.10010087</concept_id>
<concept_desc>Applied computing~Computational biology</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{Applied computing~Bioinformatics}
\ccsdesc[300]{Applied computing~Computational biology}

\keywords{Binding Affinity Prediction; Graph Neural Network;Drug Discovery}

\maketitle

\section{Introduction} \label{sec-introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure/example/intro-exp.pdf}
\vspace{-4mm}
\caption{A brief summary for protein-ligand binding affinity prediction. (1) Top left: An example of protein-ligand complex (PDB: 5HMI). (2) Top right: Various representations of complex. (3) Bottom left: Traditional Methods. (4) Bottom right: Machine learning and deep learning methods.}
\label{fig-intro}
\vspace{-5mm}
\end{figure}

The prediction of protein-ligand binding affinity\hide{Protein-ligand binding affinity prediction} has been widely considered as one of the most important tasks in computational drug discovery \cite{kitchen2004docking}. Here ligands are usually drug candidates including small molecules and biologics which can interact with proteins as agonists\hide{, antagonists} or inhibitors in the biological processes to cure diseases. The binding affinity, defined as the strength of the binding interaction between a protein and a ligand (e.g., drug), can be measured by experimental methods. However, those biological tests are laborious and time-consuming. \hide{Since there are possibly  potential candidates for developing new drugs \cite{polishchuk2013estimation}, }With the computer aided simulation \hide{models}\old{methods and the data-driven learning models}, binding affinities can be predicted in the early stage of drug discovery. Instead of applying costly biological methods directly to screen numerous candidate molecules, the prediction of binding affinity can help to rank\hide{all } drug candidates and prioritize the appropriate ones for subsequent testing to accelerate the process of drug screening \cite{PMID:28150235}.

With the development of structural biology and protein structure prediction, especially the recent Alphafold II model \cite{callaway2020will}, there are growing three-dimensional (3D) structure protein data, which enables a new paradigm for structure-based drug discovery \licom{\cite{jhoti2007structure,meng2011molecular,batool2019structure}}. It has been demonstrated that 3D structural information can effectively contribute to the drug design \cite{leach2006prediction}. Indeed, since there are already many accurate and robust algorithms to find poses of protein-ligand complexes (e.g., binding site prediction methods and docking methods), it is significant to focus on the much harder task of binding affinity prediction\hide{binding affinity prediction task} \cite{ballester2010machine}. \hide{Therefore, }To \hide{ take advantage of different representations}learn\hide{ such} useful 3D structure from a protein-ligand complex, as illustrated in Figure \ref{fig-intro}, many efforts have been devoted to estimating more accurate binding affinity for effective drug design. Docking methods \hide{ruiz2014rdock(hidden)}\cite{jain2003surflex,trott2010autodock,allen2015dock\hide{,zhang2020edock}} play an important role to predict how a specific ligand binds to the target protein with affordable computational costs. While the docking process can identify the binding \hide{site}pose of the protein-ligand complex with relatively high accuracy, its prediction of binding affinity is inaccurate and unreliable \hide{jain2008bias(hidden)}\cite{moitessier2008towards,ballester2010machine} due to poor scoring functions, which limits the applicability of docking methods in drug discovery. Compared to docking calculations, traditional machine learning methods \cite{ballester2010machine,kinnings2011machine} have improved the performance by learning the extracted features from protein-ligand complexes. However, these approaches with limited generalizability require expert knowledge and heavily rely on feature engineering.

Recently, deep learning for binding affinity prediction have become\hide{becomes} an emerging research area, which represents the complex as sequence data \cite{ozturk2018deepdta}, 3D grid-like data \cite{wallach2015atomnet} or graph data \cite{10.1093/bioinformatics/btaa921} to employ various neural networks. One of the key challenges of deep learning in structural biology is how to model the 3D spatial structure for better performance. To this end, most of the existing works \hide{wallach2015atomnet}\licom{\cite{wallach2015atomnet,ragoza2017protein, stepniewska2018development}} attempt to apply 3D convolutional neural networks (3D CNNs) by treating the complex as a 3D-grid representation. However, the cost of these models is huge, especially when considering long-range interactions. What's more, both the absence of topological information and the sensitivity to rotation in the complex have a negative effect on the prediction results. 

Despite the powerful ability of graph neural networks (GNNs) to learn graph representations \cite{li2020competitive,zheng2021drug,liu2021vldb}, there are only a few studies \cite{lim2019predicting,10.1093/bioinformatics/btaa921} on\hide{making use of} using GNNs to predict the protein-ligand binding affinity\hide{interactions between protein and ligand}. By contrast, many researchers have greatly developed GNN models in other fields of drug discovery \licom{\cite{sun2020graph,zang2020moflow}}, such as predicting molecular property \cite{yang2019analyzing,maziarka2020molecule,klicpera_dimenet_2020} and chemical reaction \cite{do2019graph}. Nevertheless, these domain-specific models \hide{always}tend to lose their effectiveness when modeling the larger biomolecules, e.g., protein-ligand complexes. In general, most of the existing GNNs in drug design aim to learn the spatial structure by incorporating the distance information, which is insufficient to model the 3D structure of complex. Moreover, the fundamental long-range interactive information between proteins and ligands\hide{ (e.g., Carbon-Carbon co-occurrence interaction)}, which is valuable for predicting the binding affinity \cite{leckband1992long}, cannot be handled under the current GNN framework.

To overcome the above limitations, we propose a novel \underline{S}tructure-aware \underline{I}nteractive \underline{G}raph Neural \underline{N}etwork (\model) to learn the constructed complex graph for predicting the protein-ligand binding affinity. \model is equipped with two designed components to correspondingly address the challenges, namely the \textit{polar-inspired graph attention layers (\gnn)} for modeling 3D spatial structure and the \textit{pairwise interactive pooling (\pool)} for leveraging long-range interactions.
Firstly, the key idea of \gnn is to establish a polar coordinate system for each central target and to preserve both distance and angle information of neighbors when performing the aggregation process. More specifically, we apply the node-edge interactive scheme \hide{recursively}iteratively with graph attention to integrate spatial factors for effectively learning the 3D structure of complex. 

\hide{Considering the large size of protein and the redundancy problem,}\hide{Based on}In view of the large size of the protein, it is \hide{needless}redundant to contain the complete protein structure in the complex graph, but in this way the long-range interactive information between the protein and the ligand is also lost. To deal with this issue, \pool, the secondary part of \model is designed to incorporate such global interactions into our model, which employs an atomic type-aware pooling process on edges with introducing an auxiliary learning task to reconstruct the atomic interaction matrix. By this means, \model can enhance the representation learning for complexes with involving both 3D spatial structures and global interactions. To summarize, the main contributions of this paper are as follows:
\begin{itemize}[leftmargin=*,topsep=3pt]
    \item To the best of our knowledge, we are among the first to develop graph neural networks from the perspective of polar coordinates for structure-based binding affinity prediction\hide{ problem}. 
    \item We propose a novel structure-aware interactive graph neural network (\model), which can capture not only 3D spatial information through polar-inspired graph attention layers (\gnn), but also global long-range interactions through pairwise interactive pooling (\pool) in a semi-supervised manner.
     \item We conduct extensive experiments using two benchmark datasets to evaluate the performance of the proposed model, which demonstrates the effectiveness of our \model with better generalizability.
\end{itemize}

\vspace{-5mm}

















\hide{
\zhou{Protein-ligand binding affinity prediction has been widely considered as one of the most important tasks in computational drug discovery for a long time \cite{drews2000drug}. Here ligands usually are drugs and small molecules which can react with protein.
The binding affinity, which is the quantity of binding strength measured by a real number between protein and ligand, indicates the probability to activate or inhibit a biological process to cure a disease. The prediction of binding affinity is one of the crucial steps in the early stage \cite{kitchen2004docking} of drug discovery.
For example, predicting the binding affinity can help to rank \hide{all }candidate drugs and prioritize the appropriate ones for subsequent testing to accelerate the process of virtual screening \cite{PMID:28150235}. 
How to accurately predict the protein-ligand binding affinity has attracted tenuous research interest in the past decades from both machine learning and computational biology communities \cite{bohm1994development,gohlke2000knowledge, wang2002further,wang2003comparative,sousa2006protein,jacob2008protein,trott2010autodock,colwell2018statistical,ozturk2018deepdta,ozturk2019widedta}. 
}
With the development of structural biology and protein structure prediction, especially the recent Alphafold II model \cite{callaway2020will} designed by DeepMind group \footnote{https://deepmind.com/research/case-studies/alphafold}, there are 
growing three-dimensional (3D) structure protein data, which enables a new paradigm for structure-based drug discovery \cite{jhoti2007structure,meng2011molecular,batool2019structure}. It has been demonstrated that 3D structure information can effectively contribute to the drug design \cite{leach2006prediction}. During this process, the prediction of binding affinity, which indicates the strength of the interaction between protein and ligand (i.e., drug), is one of the crucial steps in the early stage \cite{kitchen2004docking}.
For example, accurately predicting the binding affinity can help to rank \hide{all }candidate drugs and prioritize the appropriate ones for subsequent testing to accelerate the process of virtual screening \cite{PMID:28150235}. 
} \section{Related Work} \label{sec-related}
In this section, we first review the related literatures about predicting protein-ligand binding affinity and then detail recent advances in graph neural networks for drug discovery.

{\bfseries Protein-Ligand Binding Affinity Prediction.}
As a crucial stage in drug discovery, predicting protein-ligand binding affinity has been intensively studied for a long time \licom{\cite{sousa2006protein,jacob2008protein}}, which is of great importance for efficient and accurate drug screening. The earlier empirical-based methods \hide{bohm1994development,wang2003comparative(hidden)}\cite{gohlke2000knowledge, wang2002further,trott2010autodock} design docking and scoring functions specially to make predictions, while expert domain knowledge is required to encode internal biochemical interactions. Later on, statistical and machine learning-based methods \cite{colwell2018statistical} are developed to predict binding affinity based on data-driven learning, which attempt to extract protein-ligand features and use classic models for regression, such as random forest \cite{ballester2010machine} and SVM \cite{kinnings2011machine}. These approaches are dependent on the quality of hand-crafted features and lack of generality on the larger dataset. Recently, several deep learning-based models \hide{ozturk2019widedta(hidden)}\cite{ozturk2018deepdta} \old{utilize 1D convolutions and pooling to capture potential patterns} from raw sequence information of both ligand and protein. However, only using separate character representations fails to achieve desirable performance.

With the recent advances in predicting structures of proteins \cite{callaway2020will} and the increasing availability of 3D-structure protein-ligand data \cite{wang2005pdbbind}, there is another hot research \hide{line}area of studying structure-based approaches, which focus on learning from 3D-structure protein-ligand complexes to predict binding affinity. Some recent works \cite{ragoza2017protein, stepniewska2018development} represent the protein-ligand complex as 3D grid-like data and \old{use 3D convolutions (3D-CNNs) to take advantage of spatially-local correlations. Though these \hide{3D-CNN }approaches can learn spatial information, one limitation is that positions of proteins and ligands in different complexes are changeable, such as different angle rotations, which means the spatial structure of 3D grid-like modeling is inevitably incomplete.} More recently, OnionNet \cite{zheng2019onionnet} employs CNN models to learn the complex representation from the extracted element-specific interaction features between a protein and its ligand. However, all the above models neglect the critical topological structure information of complex. In the work \cite{lim2019predicting}, a protein-ligand complex is represented as a weighted graph with distance information. Then graph attention networks are applied to predicting the interactions. Nevertheless, only distance information between atoms is not adequate to model 3D-structure interactions. In this paper, we also focus on the structure-based prediction of protein-ligand binding affinity with incorporating abundant spatial information.

{\bfseries Graph Neural Networks for Drug Discovery.}
Inspired by the great advantage of graph neural networks (GNNs) in modeling graph data, much attention has been devoted to applying them in computational drug discovery \cite{sun2020graph}, such as the prediction of molecular property \cite{hao2020asgn} and protein interface \cite{liu2020deep}. Treating the molecule as a graph, GNNs can learn the graph-level representation for drug or protein by aggregating structural information. GraphDTA \cite{10.1093/bioinformatics/btaa921} adopts GNN models \cite{kipf2017semi,velivckovic2018graph,xu2018powerful} to learn drug presentation with combining the protein representation from 1D convolutions to predict binding affinity. In attributed molecular graphs, the edges between atoms contain valuable information, such as distance or bond order. To leverage rich attributes in the molecule, edge-oriented message passing neural networks \cite{yang2019analyzing,song2020communicative,zhou2020distance} are proposed to update both node and edge embeddings \hide{ in an interactive manner}. Meanwhile, there are\hide{is} also some efforts to model the 3D-structure of molecule by improving GNNs with spatial information, such as distance \cite{lim2019predicting,maziarka2020molecule}, angle \cite{klicpera_dimenet_2020}, and 3D coordinate \cite{danel2020spatial}. However, these models fail to consider the spatial interactions between proteins and ligands\hide{ligand and protein}. In addition, the function of learning angle information in \cite{klicpera_dimenet_2020} is designed for density functional theory, which is only beneficial for predicting molecular properties \hide{instead of}rather than protein-ligand binding affinity. To overcome these limitations, we propose an interaction-aware GNN framework with integrating both distance and angle factors harmoniously. \section{Preliminaries}\label{sec-pre}
In this section, we \hide{first} introduce some definitions used in our model and formulate the structure-based prediction problem for protein-ligand binding affinity\hide{prediction problem}. \hide{Then we describe the process of constructing the complex graph.} The frequently used notations in this paper are summarized in Table \ref{table-symbol}.



\begin{table}[t]
	\mycaption{Mathematical notations.
	}
	\vspace{-3ex}
	\label{table-symbol}
	\begin{tabular}{cl}
		\toprule
Notation	&	Description	\\
		\midrule
\proteinV,\ligandV	& The atom node sets of protein and ligand \\
		\proteinM,\ligandM	& The 3D position matrices of protein and ligand \\
		\graph	& The complex interaction graph	\\
			& The -th atom node in \graph \\
		 & The directed edge from atom  to atom  \\
		 & The neighboring edges of atom  \\
		 & The neighboring edges of edge  \\
		,  & The embedding vectors of atom  and edge   \\
		 & The spatial embedding vector between  and  \\
\bottomrule
        
	\end{tabular}
	\vspace{-3ex}
\end{table}

 
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure/example/graph-exp.pdf}
\vspace{-4mm}
\caption{An illustrative example of converting the protein-ligand complex into a complex interaction graph.}
\label{fig-graph}
\vspace{-4mm}
\end{figure}

\begin{definition}
\B{Complex Interaction Graph.}
Given a protein-ligand complex as shown in Figure \ref{fig-graph}(a), we define the atom node sets of protein and ligand as  and  with the position matrix  and  for 3D atomic coordinates, respectively. Then we define the complex interaction graph as a directional graph , where the vertex set  and the edge set  are\hide{is} constructed based on the spatial positions of atoms\hide{complex}. The graph construction process is introduced in Appendix \ref{a-graph-constrcut}.




\end{definition}

\begin{definition}
\B{Edge-oriented Neighbors.} Given an atom node  or a directed edge  (i.e., ) in the complex interaction graph \graph, the edge-oriented neighbors  of  or  are defined as the sets of directed edges  which point to the target atom  or the target edge .
\end{definition}
Taking Figure \ref{fig-graph}(b) as an example, the edges  and  are connected to the edge  via the common node , the edge-oriented neighbors of  are denoted as . Similarly, the edges ,  and  point to the atom node , resulting in the neighbors set .



\begin{definition}
\B{Structure-based Protein-Ligand Binding Affinity Prediction.\hide{Structure-based Prediction of Protein-Ligand Binding Affinity.}} Given a protein-ligand complex with 3D structure, i.e., the complex interaction graph \graph \ and the 3D position matrix  consisting of \proteinM and \ligandM, our goal is to learn a model  to precisely predict the binding affinity  with preserving the spatial structure.


\end{definition}

 \section{Model Framework}\label{sec-model}



\begin{figure}[t]
\centering
\subfigure[All edges]{
    \label{dist-all} \includegraphics[width=0.32\columnwidth]{figure/example/dist_bar_edges_big.pdf}}
\hspace{-2mm}
  \subfigure[Single bonds]{
    \label{dist-single} \includegraphics[width=0.33\columnwidth]{figure/example/dist_bar_bond1_big.pdf}}
 \hspace{-3mm}
  \subfigure[Double bonds]{
    \label{dist-double} \includegraphics[width=0.33\columnwidth]{figure/example/dist_bar_bond2_big.pdf}}
\vspace{-5mm}
\caption{The distribution of distance between atoms within 5 Å in the protein-ligand complex from PDBbind dataset.}
\label{fig-dist}
\vspace{-5mm}
\end{figure}


\begin{figure*}[t]
\centering
\includegraphics[width=1.\textwidth]{figure/framework-new-all-3d-.pdf}
\vspace{-7mm}
\caption{Illustration of the proposed \model framework. 
(a), (b): The two key components \gnn and \pool. (c), (d): The two inner structures of component \gnn. (e), (f): The aggregation processes in \textit{nodeedge} and \textit{edgenode} interaction layers.
}
\label{fig-model}
\vspace{-4mm}
\end{figure*}


In this section, we present the proposed \model\hide{\footnote{Code is available at: https://github.com/agave233/SIGN}} model for protein-ligand binding affinity prediction. We first introduce the overall framework and then describe the details of each component.

\subsection{Overview}
To make accurate predictions for \dta, there are two challenges. Firstly, as shown in Figure \ref{fig-graph}, the complex graph has the unique spatial structure, which is different with general graph. Secondly, the long-range interactions between \hide{atoms of} protein and ligand are also critical to the binding affinity \cite{leckband1992long}. However, the existing GNNs are incapable of capturing such spatial information and interactions. To overcome the\hide{ above two challenges} limitations, we propose a novel \textit{\underline{S}tructure-aware \underline{I}nteractive \underline{G}raph Neural \underline{N}etwork (\model)} to model the 3D structural complex and protein-ligand spatial interactions. 

Figure \ref{fig-model} exhibits the architecture which takes the complex interaction graph \graph \ as input. We start with \hide{spatial relation embedding module for encoding the distance between atoms, then present }the polar-inspired graph attention layers (\textit{\gnn}), which are composed of \textit{nodeedge} and \textit{edgenode} \hide{aggregation}interaction layers. \gnn can propagate the node's and edge's embeddings alternately with learning the spatial distance and angle information. The two parts of \gnn play a synergistic effect on modeling the spatial structure of the complex. After that, we apply a pairwise interactive pooling layer (\textit{\pool}) which performs on the edges' representations to obtain the atomic type-based interaction matrix of the complex. From a global view, \pool aims to approximate the overall interactions between proteins and ligands to improve the prediction performance. Finally, the model is trained through multi-task learning with augmented constraints for the interaction matrix, which serves as a self-supervised task.


\subsection{Polar Coordinate-Inspired Graph Attention}

Standard GNNs have shown great advantages in learning topological structure of the general graph, which cannot take atom's spatial position into account in the 3-dimensional space. To model the 3D structure of a complex, an intuitive method is to provide atom's 3-dimensional coordinate in the GNN architecture \cite{danel2020spatial}. However, the position information under the Cartesian coordinate system is sensitive to both translations and rotations, causing poor generalization of model when learning the complex representation. Several models, such as GNN-DTI \cite{klicpera_dimenet_2020} and MAT \cite{maziarka2020molecule}, manage to combine the distance information in the aggregation process, while only pairwise distance is not adequate. Different from DimeNet \cite{klicpera_dimenet_2020}, which specially designs Bessel functions in GNN for density functional theory (DFT) approximation with limited ability to model the larger biological complex\hide{\cite{zhang2020molecular}}, we employ iterative \textit{nodeedge} and \textit{edgenode} interaction layers to incorporate both distance and angle information from a spatial distribution perspective.

\subsubsection{Polar-Inspired Attentive Learning Architecture.}
\label{subsec-embed}
Inspired by polar coordinate which is composed of radial distance  and polar angle , we develop an interaction-based graph attention network to leverage both the distance between nodes and the angle between edges in a collaborative framework. As illustrated in Figure \ref{fig-model}(e)\hide{\ref{fig-angle}}, \hide{when aggregating for , we treat the specific target edge  as polar axis .}when aggregating for edge , we treat it as the polar axis . Under such a definite polar coordinate system, the edge-oriented neighbors are distributed around  with unique identifying coordinates . Through the method of dividing angle domains, the spatial distribution for the complex can be taken into account by means of angle-oriented attention in the first aggregation stage for edges.

Moreover, the distance factor is also helpful for structure modeling, which reveals spatial correlations. Figure \ref{fig-dist} shows the statistical distribution of distance between atoms. It can be seen that the distances of covalent bonds mainly range from 1 to 2 {\AA}, while noncovalent interactions, \hide{such as hydrogen bonds\hide{bonding}, hydrophobic interaction, and Van der Waals,}like hydrophobic and van der Waals interactions, and hydrogen bonds, are distributed over longer distances. Atomic interactions in the complex vary from different distances, which indicate different spatial relations for atom pairs. Given the radial distance  between atoms  and , as shown in Figure \ref{fig-model}(f), we first map  to a bucket (i.e., a distance domain corresponding to a type of relation) and obtain the one-hot vector . Then we apply a dense layer transformation to get the spatial relation embedding:

where  is the transformation weight matrix and  is the number of buckets (i.e., spatial relations). To factor in these correlations, we design the distance-aware attention \hide{mechanism }in the second aggregation stage for nodes. As shown in Figure \ref{fig-model}(a), the overall attentive interaction process at -th layer\hide{between node and edge} is defined as:

where  is the edge embedding,  is the node (atom) embedding,  and  are interaction functions of \textit{nodeedge} and \textit{edgenode} layers,  and  are the edge-oriented neighbors of edge  and node  respectively.


\subsubsection{Angle-oriented NodeEdge Interaction Layer.}



Failing to distinguish neighbor nodes from different directions in the aggregation process is \hide{\li{an obvious} \zhou{a}}a weakness of the existing GNN models. To overcome this inadequacy, we adopt an angle-oriented graph attention layer to update the edge representations with integrating spatial angle information. Since the angle exists between the two edges, as shown in Figure \ref{fig-model}(c), we first get the edge embedding through aggregating the node features:

where  is the transformation matrix for atomic combination, the operator  represents concatenation, and  is the Relu \hide{activation }function.



After obtaining the representations  of edge  and its neighbors, we further separate the neighboring edges in 3-dimensional space by applying an angle-domain divider \angleD, which plays an intermediate role to assign each neighbor to the specific angle domain. For example, in Figure \ref{fig-model}(e)\hide{\ref{fig-angle}}, there are four edge-oriented neighbors  and  around the central target edge . These neighboring edges are located in three different local angle domains according to the angles between edge  and its neighbors. Given the number of angle domains  (e.g.,  in Figure \ref{fig-model}(e)) and the target edge  for aggregation, \angleD \  can map each neighbor  to the located angle domain index:

where  denotes rounding operation to get the integer index,  is the calculated angle between edges  and . Then the subset of edge-oriented neighbors which are located in -th angle domain can be defined as:

After reorganizing the neighbors of  through divider \angleD\  based on the polar coordinate system, we then feed  neighbor subsets from different angle domains into  independent propagation layers to capture long-range dependencies in the complex interaction graph. Firstly, we devise the domain-specific aggregation process along edges for the -th angle domain:

where  is the -th local aggregated edge representation at -th layer,  is the attention weight of the neighboring edge  across the -th angle domain. Concretely, we apply the angle-oriented attention mechanism, which first uses  function to calculate the coefficient between two edges and then adopts the softmax function for normalization:

where ,  and  are the learnable attention parameters of the specific -th angle domain, and we use tanh as the nonlinear activation function.

Secondly, we combine all aggregated edge embeddings obtained from Eq. (\ref{eq-e2e}). To completely preserve the spatial information in different local angle domains, we concatenate the representations as the global aggregation to update the angle-aware edge embedding:



\subsubsection{Distance-aware EdgeNode Interaction Layer.}
After injecting the angle information into the edge embedding , we make further efforts to develop an attention-based edgenode interaction layer to incorporate another spatial factor in the polar coordinate system, that is distance. As we stated in Section \ref{subsec-embed}, the distance between atoms is implicated in different meaningful correlations. Therefore, it's momentous to explore the influence of distance while learning representations for protein-ligand complexes. Specifically, since edges and nodes (atoms) have different feature spaces, \old{we first convert the edge embedding and node embedding into the hidden representation  and  in the same vector space:}

where  and  are linear transformation matrices,  is the embedding of atom  from -th layer. 

\hide{Not only because of atomic attributes, but the variant distances}As a result of the variant distances and atomic attributes, the neighboring edges have different impacts on the target \hide{atomic }node. However, the existing GNN models cannot effectively capture the influence of the distance factor. Hence, as shown in Figure \ref{fig-model}(d) and \ref{fig-model}(f), we propose to extend the original GAT \cite{velivckovic2018graph} with the distance-aware attention \old{to fuse the distance information with the capability of discriminating multiple spatial relations among atoms:}

where  is the parameter of edgenode attention at -th layer,  is the trainable parameter matrix for distance transformation, the final calculated attention weight  reflects how important the edge  is for the node . Then \old{we develop the distance-aware attention to multi-head attention version as GAT for better stability} and apply the aggregation process from edge to node:

where  is the number of independent attention heads. Due to the angle injection for edge embedding  and the distance injection for attention weight , our proposed model can comprehensively incorporate spatial information in the complex.

After performing  polar-inspired graph attention layers, we obtain the node embedding  for atom  and the edge embedding  between atoms  and .
\hide{
\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figure/spatial-inject-cut.pdf}
\vspace{-4mm}
\caption{Injection of angle and distance information.}
\label{fig-inject}
\vspace{-4mm}
\end{figure}
}

\subsection{Pairwise Interactive Pooling Constraint}
As introduced in Appendix \ref{a-graph-constrcut}, the constructed complex graph \graph \ only contains the partial protein structure due to the limitation of graph size and needless noise. However, the long-range intermolecular interactions between protein and ligand have\hide{a beneficial} effects on the binding affinity \cite{ballester2010machine, leckband1992long}, while \graph \ cannot provide such interactive information. To capture the \hide{distant-range}long-range interactions in the complex (e.g., the Carbon-Carbon co-occurrence interaction), we design an atomic type-aware pooling layer for edges between the protein and the ligand, which generates a proximity interaction matrix of atom type pair and enhances the representation learning process through the additional self-supervised training. 

Specifically, we first construct the pairwise interaction matrix  from the complete protein and its ligand, where  and  are atomic type sets of the protein and its ligand. Each element  in  or  in  represents the atomic number (e.g., 6) of a certain atom (e.g., carbon atom C). Following the previous work \cite{ballester2010machine}, we calculate the number of occurrences for a specific atomic type pair  (e.g., (6, 7) for <C,N> pair) within a certain distance and normalize the result to get the matrix :

where the function  returns the atomic type of ,  is a Kronecker delta function which outputs 1 only if the type of atom is  (or ) and 0 otherwise,  is referred to as the \hide{long }interaction cutoff distance and a Heaviside step function  is adopted to count protein–ligand atomic type pairs within the distance . 

Secondly, we take the edge embeddings obtained from \gnn as input to the atomic type-aware pooling layer, which is shown in Figure \ref{fig-model}(b). There are  pooling blocks for type pairs. One block to gather edge representations belonging to atomic type pair  can be formulated as:

where  is the shared parameter matrix for edge pooling,  contains all the intermolecular edges in the complex ,  and  are atom nodes connected by , the two  functions act as a divider to pick up the corresponding edges. Then we calculate each value of the approximate interaction matrix:

where  is the trainable parameter. In the training stage, we use an additional proximity loss to draw the interaction matrix  and  closer:

where  is the flatten operation for matrix,  is the training set.





\subsection{Optimization Objective}
At the last part, we add together node (atom) embeddings to get the complex representation and use MLP layers as the regressor to predict the protein-ligand binding affinity:


Then the absolute error between the predicted binding affinity  and the measured ground truth  is used to calculate the loss. Thus, we adopt the L1 loss function to optimize the model: 

where  contains all the protein-ligand complexes with binding affinities. To integrate the interaction effectiveness for better complex representation learning, we further combine with the complex interaction constraint in Eq. (\ref{eq-inter-loss}) and reach the following overall objective function:

where  is the balancing hyper-parameter to control the strength of interaction loss. The detailed process for training the proposed \model is provided in Algorithm \ref{alg-training}.

















%
 \section{Experiments} \label{sec-exp}
\begin{table*}[t]
\caption{Performance comparison on \textit{PDBbind core set} and \textit{CSAR-HiQ set}.}
	\vspace{-3ex}
	\label{table-main-exp}
	\centering
	\scalebox{0.83}{
	\begin{tabular}{cl|cccc|cccc}
		\toprule
		\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & \multicolumn{4}{c|}{PDBbind core set} & \multicolumn{4}{c}{CSAR-HiQ set} \\
		\cmidrule{3-10}
		&   & RMSE   & MAE   & SD   & R 	& RMSE   & MAE   & SD   & R   \\ \midrule \multirow{3}{*}{\shortstack{ML-based \\ Methods}}
		& LR &1.675 (0.000)  &1.358 (0.000)   &1.612 (0.000)  &0.671 (0.000)  &2.071 (0.000)  & 1.622 (0.000)  &1.973 (0.000)  &0.652 (0.000) \\  
		& SVR &1.555 (0.000)  &1.264 (0.000)  &1.493 (0.000)  &0.727 (0.000)  &1.995 (0.000)  &1.553 (0.000)  &1.911 (0.000)  &0.679 (0.000) \\
		& RF-Score &1.446 (0.008)  &1.161 (0.007)  &1.335 (0.010)  &0.789(0.003)  &1.947 (0.012)  &1.466 (0.009)  &1.796 (0.020)  &0.723 (0.007)
		\\ \midrule 

		\multirow{2}{*}{\shortstack{CNN-based \\ Methods}}
		& Pafnucy & 1.585 (0.013)  &1.284 (0.021)  &1.563 (0.022)  &0.695 (0.011)  &1.939 (0.103)  &1.562 (0.094)  &1.885 (0.071) &0.686 (0.027) \\  
		& OnionNet &1.407 (0.034)  &1.078 (0.028)  &1.391 (0.038)  &0.768 (0.014)  &1.927 (0.071)  &1.471 (0.031)  &1.877 (0.097)  &0.690 (0.040)
		\\ \midrule 

		\multirow{4}{*}{\shortstack{GraphDTA \\ Methods}}
		& GCN &1.735 (0.034)  &1.343 (0.037)  &1.719 (0.027)  &0.613 (0.016)  &2.324 (0.079)  &1.732 (0.065)  &2.302 (0.061)  &0.464 (0.047) \\  
		& GAT &1.765 (0.026)  &1.354 (0.033)  &1.740 (0.027)  &0.601 (0.016)  &2.213 (0.053)  &1.651 (0.061)  &2.215 (0.050)  &0.524 (0.032) \\
		& GIN &1.640 (0.044)  &1.261 (0.044)  &1.621 (0.036)  &0.667 (0.018)  &2.158 (0.074)  &1.624 (0.058)  &2.156 (0.088)  &0.558 (0.047) \\
		& GAT-GCN &1.562 (0.022)  &1.191 (0.016)  &1.558 (0.018)  &0.697 (0.008)  &1.980 (0.055)  &1.493 (0.046)  &1.969 (0.057)  &0.653 (0.026)
		\\ \midrule 

		\multirow{5}{*}{\shortstack{GNN-based \\ Methods}}
		& SGCN &1.583 (0.033)  &1.250 (0.036)  &1.582 (0.320)  &0.686 (0.015)  &1.902 (0.063)  &1.472 (0.067)  &1.891 (0.077)  &0.686 (0.030) \\  
		& GNN-DTI &1.492 (0.025)  &1.192 (0.032)  &1.471 (0.051)  &0.736 (0.021)  &1.972 (0.061)  &1.547 (0.058)  &1.834 (0.090)  &0.709 (0.035) \\
		& DMPNN &1.493 (0.016)  &1.188 (0.009)  &1.489 (0.014)  &0.729 (0.006)  &1.886 (0.026)  &1.488 (0.054)  &1.865 (0.035)  &0.697 (0.013) \\
		& MAT &1.457 (0.037)  &1.154 (0.037)  &1.445 (0.033)  &0.747 (0.013)  &1.879 (0.065)  &1.435 (0.058)  &1.816 (0.083)  &0.715 (0.030) \\
		& DimeNet &1.453 (0.027)  &1.138 (0.026)  &1.434 (0.023)  &0.752 (0.010)  &1.805 (0.036)  &1.338 (0.026)  &1.798 (0.027)  &0.723 (0.010) \\
		& CMPNN &1.408 (0.028)  &1.117 (0.031)  &1.399 (0.025)  &0.765 (0.009)  &1.839 (0.096)  &1.411 (0.064)  &1.767 (0.103)  &0.730 (0.052)
		\\ \midrule Ours & \model &\textbf{1.316 (0.031)}  &\textbf{1.027 (0.025)}  &\textbf{1.312 (0.035)}  &\textbf{0.797 (0.012)}  &\textbf{1.735 (0.031)}  &\textbf{1.327 (0.040)}  &\textbf{1.709 (0.044)}  &\textbf{0.754 (0.014)}
		\\ \bottomrule 

		
	\end{tabular}}
	\vspace{-4ex}
\end{table*} \begin{figure*}
\setlength{\abovecaptionskip}{2.mm}
\setlength{\belowcaptionskip}{-0.cm}
  \centering
  \subfigure{
    \label{general-exp-rmse} \includegraphics[width=.98\columnwidth]{figure/general/general-RMSE-small-new.pdf}}
\subfigure{
    \label{general-exp-mae} \includegraphics[width=.98\columnwidth]{figure/general/general-MAE-small-new.pdf}}
  \vspace{-5mm}
  \caption{Performance improvements on PDBbind benchmark when training on \textit{general set}.}
  \vspace{-5mm}
  \label{fig-general} \end{figure*}
In this section, we conduct experiments on two standard datasets\hide{ to evaluate our proposed model which aim} to investigate the following research questions:
\begin{itemize}[leftmargin=*,topsep=3pt]
\item \B{RQ1.} How does the proposed \model model perform compared against the state-of-the-art methods?
    \item \B{RQ2.} How does the generalizability of \model and competitors when trained\hide{training} on the larger but lower-quality dataset? \hide{variable datasets with different scales?}
\item \B{RQ3.} Do the spatial and interactive factors benefit the prediction?
    \item \B{RQ4.} How do the parameter settings (e.g., the cutoff distance and angle domain divisions) affect the prediction result?
\end{itemize}

\setlength{\skip\footins}{3mm}
\subsection{Experiment Settings}
\subsubsection{Datasets.}
We evaluate all models on the following \hide{publicly}public standard datasets for protein-ligand binding affinity prediction.




\B{PDBbind\footnote{http://www.pdbbind-cn.org}} is a well-known public dataset \cite{wang2005pdbbind} in development which provides 3D binding structures of protein-ligand complexes with experimentally determined binding affinities (refer to Appendix \ref{a-pka}). In our experiment, we mainly use the PDBbind v2016 dataset, which is most frequently used in recent works \cite{stepniewska2018development,zheng2019onionnet}. Specifically, it includes three overlapping subsets, i.e., \textit{general}, \textit{refined} and \textit{core set}. The \textit{general set} contains all 13,283 protein-ligand complexes, while the 4,057 complexes in \textit{refined set} are selected out of the \textit{general set} with better quality. Moreover, the \textit{core set} with 290 complexes serves as the highest quality\hide{CASF-2016} benchmark for testing through a careful selection process \cite{su2018comparative}. Conveniently, we call the difference between the \textit{refined} and \textit{core} subsets, that is 3,767 complexes, as \textit{refined set} of PDBbind in the following.
    
\B{CSAR-HiQ\footnote{http://www.csardock.org}} is an additional benchmark dataset \cite{dunbar2011csar}, containing two subsets with 176 and 167 protein-ligand complexes. We use this external dataset from an independent source to further evaluate the generalization ability of models.
    
\subsubsection{Setup.}
\label{exp-setup}
Following \cite{ballester2010machine}, we choose the \textit{refined set} of PDBbind as our primary training data \hide{with considering the reason that}because there is considerable overlap between the full \textit{general set} and CSAR-HiQ dataset. We\hide{first} randomly split the protein-ligand complexes in \textit{refined set} with a ratio of 9:1 for training and validation. For testing sets, we use the \textit{core set} and CSAR-HiQ \hide{data}set with removing the complexes present in \textit{refined set}.

Since the lower-quality data of \textit{general set} can still improve the performance of models \cite{li2015low}, we conduct the supplemental experiment on the full \textit{general set} which is larger but of worse quality to analyze the generalizability of our model. As stated above, we can only evaluate the performance on the \textit{core set} due to the overlapping problem of CSAR-HiQ dataset. Following \cite{stepniewska2018development,zheng2019onionnet}, we randomly select 1,000 complexes from \textit{refined set} as the validating set. The remaining 11,993 complexes in \textit{general set} are used for training\hide{as training set}.
\hide{
The statistics of datasets are summarized in Table \ref{table-dataset}.
\begin{table}[t]
	\caption{Statistical sizes of datasets.}
	\vspace{-3mm}
	\label{table-dataset}
	\centering
	\begin{tabular}{cccccc}
		\toprule
		\textbf{Dataset} & \textbf{training} & \textbf{validation} & \textbf{test (core)} & \textbf{test (csar)} \\
		\midrule
		\textit{refined set}	&	3390  &	377	&	290	&	104	\\
		\textit{general set}	&	11993	&	1000	&   290	&	/	\\
		\bottomrule
	\end{tabular}
	\vspace{-5mm}
\end{table} }
\subsubsection{Evaluation Metrics.}
\old{
\hide{
To comprehensively evaluate the model performance, following \cite{stepniewska2018development, zheng2019onionnet}, we use Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) to measure the prediction error. The performance of a model is also quantitatively evaluated by the classic Pearson's correlation coefficient (R) and the standard deviation (SD) in regression to measure the linear correlation between predictions and the experimental binding constants. The detail is introduced in Appendix \ref{a-implement}.
}
To comprehensively evaluate the model performance, following \cite{stepniewska2018development, zheng2019onionnet}, we use Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Pearson's correlation coefficient (R) and the standard deviation (SD) in regression to measure the prediction error. The detail is introduced in Appendix \ref{a-implement}.
\hide{
As introduced in \cite{stepniewska2018development}, SD is defined as \hide{follows:
}: ,
where  and  respectively represent the predicted and experimental value of the -th complex in dataset , and  and  are the intercept and the slope of the regression line, respectively.
}
}
\vspace{-1mm}
\subsubsection{Baselines.}
We compare our proposed model with comparative methods including machine learning-based methods (\B{LR}, \B{SVR}, and \B{RF-Score} \cite{ballester2010machine}), CNN-based methods (\B{Pafnucy} \cite{stepniewska2018development} and \B{OnionNet} \cite{zheng2019onionnet}), and GNN models \B{GraphDTA} \cite{10.1093/bioinformatics/btaa921} for protein-ligand binding affinity prediction. Moreover, various state-of-the-art GNN-based models (\B{SGCN} \cite{danel2020spatial}, \B{GNN-DTI} \cite{lim2019predicting}, \B{DMPNN} \cite{yang2019analyzing}, \B{MAT} \cite{maziarka2020molecule}, \B{DimeNet} \cite{klicpera_dimenet_2020}, and \B{CMPNN} \cite{song2020communicative}) which also consider the spatial information for molecular modeling are compared to \hide{demonstrate the effectiveness}evaluate the performance of \model. The details of \hide{implementation, }experiment settings and baseline descriptions are provided \hide{below} in Appendix \ref{a-implement} and \ref{a-baseline}.

\hide{
\subsubsection{Implementation Details.}
We implement our experiments based on Pytorch, except for Pafnucy \cite{stepniewska2018development} where we use the authors’ Tensorflow implementation. We train all models on 24 Intel CPUs and a Tesla K80 GPU with 12 GB memory. Note that DimeNet \cite{klicpera_dimenet_2020} is not compared in our experiment. \zhoucom{change lager} \li{ because it can not model the larger molecules and will raise the out-of-memory error when training protein-ligand complexes.} 
The model input and parameter settings for baselines and our method are introduced in Appendix \ref{a-implement}.
}
\vspace{-1mm}

\subsection{Performance Evaluation}
\subsubsection{Overall Comparison (RQ1).}
We first compare our proposed \model with baseline approaches on two benchmark datasets. As shown in Table \ref{table-main-exp}, the average and the standard deviation of four indicators for testing performance are reported across five random runs. In general, we can observe that \model achieves the best performance on two datasets, with 6.5\% and 3.9\% improvement of RMSE over the best baseline models on PDBbind and CSAR-HiQ datasets, respectively. We further have the following observations.

Among all baselines, GraphDTA methods show relatively poor performance due to the failure of considering the spatial structure and interactions between proteins and ligands. It indicates that simply modeling the molecular graph with protein sequence information is not capable of predicting structure-based protein-ligand binding affinity. By contrast, from the perspective of interaction modeling, the machine learning-based methods and OnionNet model take advantage of long-range interaction features and achieve better results. However, these data-driven approaches relying on feature engineering ignore the informative spatial structures of complexes and have limited generalization capability on the additional CSAR-HiQ dataset.
From the perspective of spatial structural modeling, we find that SGCN and GNN-DTI which incorporate position and distance information exhibit considerable improvement over the vanilla GCN and GAT. Since SGCN takes atomic position coordinates as input directly, it will be easily affected by the rotation and translation of atoms, and the 3D CNN model Pafnucy suffers from a similar issue. \hide{As a result}Thus, the prediction results \hide{of them }are not ideal. Despite leveraging a transformer-like attention mechanism to handle the spatial structure, MAT is not better than RF-Score and OnionNet, suggesting the importance of combining spatial and interactive information. The edge-oriented model CMPNN outperforms \hide{other GNN-based}the above methods because it enhances DMPNN with communication while propagating the distance information, which shows the significance of node-edge message passing process. Although DimeNet can learn from angle information and perform slightly better\hide{better slightly}, the performance is still not ideal due to its limited ability of modeling larger biomolecules. \hide{its limited ability of modeling larger biomolecules causes unsatisfactory performance on the binding affinity prediction.}Our proposed \model can not only capture more comprehensive angle-enhanced structural information instead of \hide{only}just distance, but also handle interactions in the complex through multi-task learning framework. Therefore, \model is much effective for modeling the protein-ligand complex and can accurately predict the binding affinity.



\subsubsection{Generalizability Comparison (RQ2).}



There is increasing 3D structure-based protein-ligand data with binding affinity, whereas the amount of high-quality data in \textit{refined set} is relatively small. Thus, the ability of utilizing \hide{the }more lower-quality data to improve performance\hide{for performance improvement} shows the generalizability of model, which is another necessary measurement of performance evaluation. As introduced in Section \ref{exp-setup}, we conduct the extra experiment of generalizability on the \textit{general set} of PDBbind dataset.
As illustrated in Figure \ref{fig-general}, we compare the proposed \model with major 
competitive baselines on two training sets. The results show that \model gets the lowest prediction error remarkably under both training settings\hide{on both \textit{refined set} and \textit{general set}}. More importantly, our model improves the performance by around 8\% when \hide{training}trained on the \textit{general set} and it further expands the prediction advantage compared to baselines. Therefore, \model is proved to be more generalizable to more data in large quantity but poor quality. \hide{it demonstrates \model is more generalizable when we have access to more data with large quantity but poor quality.}


\subsubsection{Impact of Spatial and Interactive Factors (RQ3).}

\begin{figure}
\setlength{\abovecaptionskip}{2.mm}
\setlength{\belowcaptionskip}{-0.cm}
  \centering
  \subfigure{
    \label{component-exp-rmse} \includegraphics[width=0.47\columnwidth]{figure/ablation/RMSE-RedOr-new.pdf}}
      \subfigure{
    \label{component-exp-mae} \includegraphics[width=0.47\columnwidth]{figure/ablation/MAE-RedOr-new.pdf}}
\
    RMSE = \sqrt{\frac{1}{|\mathcal{D}|}\sum_{i=1}^{|\mathcal{D}|} (\hat{y}_i-y_i)^2},\ MAE = \frac{1}{|\mathcal{D}|}\sum_{i=1}^{|\mathcal{D}|} |\hat{y}_i-y_i| 

    R = \frac{\sum_{i=1}^{|\mathcal{D}|}(\hat{y}_i-\bar{\hat{y}})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{|\mathcal{D}|}(\hat{y}_i-\bar{\hat{y}})^2(y_i-\bar{y})^2}}

    SD = \sqrt{\frac{1}{|\mathcal{D}|-1}\sum_{i=1}^{|\mathcal{D}|} [y_i - (a+b \hat{y}_i)]^2}

where  and  are the intercept and the slope of the regression line, respectively. 




\normalem
\begin{algorithm}[t]
    \SetAlgoLined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
\Input{Training set }
    \Output{Trained model parameters }
    Randomly initialize the parameter  of \model;
    
    \For{iteration = 1, 2, ...}{
        \For{each batch from training samples}{
            Calculate spatial relation embeddings  using Eq. (\ref{eq-embed})\;
            \For{l = 1...L}{
                Obtain edge embeddings  using Eq. (\ref{eq-a2e})-(\ref{eq-e-combine})\;
                Obtain node embeddings  using Eq. (\ref{eq-node-trans})-(\ref{eq-e2a})\;
            }
            Build the \hide{global}interaction matrix  using Eq. (\ref{eq-im1})-(\ref{eq-im2})\;
            Estimate the interaction matrix  using Eq. (\ref{eq-pool})-(\ref{eq-im-normal})\;
            Calculate the interaction loss  using Eq. (\ref{eq-inter-loss})\;
            Calculate the prediction  using Eq. (\ref{eq-predict})\;
            Calculate the prediction loss  using Eq. (\ref{eq-pred-loss})\;
            Update parameters  according to the gradient of \;
        }
    }
    \textbf{return} 
    \caption{Training Procedure for \model.}
    \label{alg-training}
\end{algorithm} \subsubsection{Input Graph and Features}
For all GNN-based methods, we use the same input complex graph as introduced in Appendix \ref{a-graph-constrcut} for protein-ligand binding affinity prediction. For GraphDTA models, we input the protein sequence as well as the ligand molecular graph or our constructed complex graph. In this paper, we report the best result when using the complex graph as input. \old{For 3D-CNN and GNN models, the atom features used according to \cite{stepniewska2018development} include atomic types, hybridization, the number of bonds with other heavy-atoms and hetero-atoms, atom properties such as aromaticity, and the partial charge. In total, 18 features are used to describe an atom. Considering the heterogeneity in the complex graph, we further extend atom features to a 36-dimension vector with zero-padding, where the 1st to 18th elements represent the features of ligand atoms and the 19th to 36th elements represent the features of protein atoms.} As to edge features for edge-based GNN models, we combine the two atom features and encoded distance features between atoms as the input vector. In brief, we provide the same input complex graph and atomic features for our \model and all GNN-based baselines to make fair comparisons. For ML-based methods and OnionNet, they cannot take the complex graph or grid-like data as input and only receive the specific molecule-level features. We extract the input feature vectors based on the distance as described in their original papers \cite{ballester2010machine, zheng2019onionnet}. Note that these extracted features also reflect the structural information of complex in a global view.

\begin{figure}
\setlength{\abovecaptionskip}{2.mm}
\setlength{\belowcaptionskip}{-0.cm}
  \centering
  \subfigure{
    \label{parameter-core-layer} \includegraphics[width=0.47\columnwidth]{figure/parameter/layers-big-newfont.pdf}}
      \subfigure{
    \label{parameter-core-lambda} \includegraphics[width=0.47\columnwidth]{figure/parameter/lambda-big-newfont.pdf}}
  \vspace{-2mm}
  \caption{Parameter analysis on PDBbind \textit{core set}.}
  \vspace{-4mm}
  \label{fig-parameter-add} \end{figure}



\subsubsection{Parameter Settings}
For the proposed \model, we use Adam optimizer for model training with a learning rate of 0.001 and set the batch size as 32. The balancing hyper-parameter  is set to 1.75 according to the performance on validation set. We construct the complex graph and interaction matrix with cutoff-threshold  and  as suggested in \cite{muegge1999general}, respectively. The basic dimensions of node and edge embeddings are both set to 128. The number of buckets for spatial relation  is set to 4 with the splitting granularity of 1Å. For \gnn layers, we set the number of attention heads  to 4, the dropout rate to 0.2, and the number of angle domains  to 6. For \pool layer, there are 36 pooling blocks in total, where the two atomic type sets  and  are defined as stated in \cite{ballester2010machine}. 


For baseline models, we tune the parameters based on the default settings to get optimal performance. Specifically, the number of decision trees in RF-score is set to 100, the max-depth of trees is set to 5, the maximum number of features is set to 3 and the minimum number of samples required to split is set to 10. For the CNN-based models, we set the channels of three-layer 3D convolutions for Pafnucy as 64,128 and 256. For OnionNet, the number of input features is 3840 and there are 32, 64, and 128 filters in the three convolutional layers with the kernel size as 4. The maximum length of protein sequences is set to 1000 in GraphDTA. For GNN-based models, the number of filters in SGCN is set to 32 with the dimension as 36. We also apply the data augmentation process to ensure optimal performance. For fair comparison, the embedding dimension of other baselines is set to 128 (same as \model). For GNN-DTI, the initial  and  for distance learning in GAT layers are set to 4.0 and 1.0, respectively. For DimeNet, the number of spherical harmonics and radial basis functions are set to 4 and 3, respectively. We use two-layer interaction blocks and three-layer bilinear layers to make DimeNet work in our experiment. For DMPNN and CMPNN, the layer of edge-oriented message passing layers is set to 3 and we use MLP as the communication module in CMPNN. The weighting 
coefficients for self-attention, distance, and adjacency matrices in MAT are set to 0.3, 0.3, and 0.4, respectively.
\vspace{-2mm}
\subsection{Baseline Descriptions}
We compare our \model model with the following methods to predict the protein-ligand binding affinity:
\label{a-baseline}
\begin{itemize}[leftmargin=*,topsep=3pt]
    \item \B{ML-based methods} include linear regression (LR), support vector regression (SVR), and random forest (RF). These methods take the inter-molecular interaction features introduced in RF-Score \cite{ballester2010machine} as input and predict the protein-ligand binding affinity.
    \item \B{Pafnucy} \cite{stepniewska2018development} is a representative 3D CNN-based model which can learn the spatial structure of protein-ligand complexes.
    \item \B{OnionNet} \cite{zheng2019onionnet} generates two-dimensional interaction features based on rotation-free element-pair contacts in complexes and adopts CNN to learn representations for prediction.
    \item \B{GraphDTA} \cite{10.1093/bioinformatics/btaa921} introduces GNN models to learn the complex graph and uses CNN to learn the protein sequence. It has four variants with different GNN models: \B{GCN} \cite{kipf2017semi}, \B{GAT} \cite{velivckovic2018graph}, \B{GIN} \cite{xu2018powerful} and \B{GAT-GCN} which combines the former two models.
    \item \B{SGCN} \cite{danel2020spatial} leverages node positions based on graph convolutional network, which directly utilizes atomic coordinates.
    \item \B{GNN-DTI} \cite{lim2019predicting} is a distance-aware graph attention network with considering 3D structural information to learn the intermolecular interactions for protein-ligand complexes.
    \item \B{DMPNN} \cite{yang2019analyzing} is an edge-based message passing neural network. It can incorporate the spatial information between atoms by applying the aggregation process for edges.
    \item \B{MAT} \cite{maziarka2020molecule} employs a molecule-augmented attention mechanism based on transformer for graph representation learning with using the inter-atomic distances.
    \item \B{DimeNet} \cite{klicpera_dimenet_2020} is a recent state-of-the-art model for small molecular graph learning using directional message passing scheme. Bessel functions are employed to encode the angle and distance information in graph neural network. 
    \item \B{CMPNN} \cite{song2020communicative} further develops DMPNN to build a communicative message passing scheme between nodes and edges for better molecular representation learning.
\end{itemize}
\vspace{-2mm}
\subsection{Additional Parameters Analysis}
\label{a-para}



\B{Number of \gnn layers }. As shown in Figure \ref{fig-parameter-add}, we first present the influence of multi-hop propagation with stacking node-edge interaction layers from 1 to 4. We observe that increasing the number of layers would not always give rise to a better result. The model with one \gnn layer has limited ability to model high-order information in the complex. As a result of over-fitting, the performance of the model using more than 3 layers starts to degenerate gradually. Therefore, applying two interaction layers in \model is enough to capture sufficient spatial information.

\B{Balancing coefficient }. Moreover, we change the coefficient  to control the trade-off between the prediction loss and interaction loss. From the results, we observe that the performance first tends to get better with incorporating more interactive information for long-range dependencies, and then begins to drop off slightly. In general, our model is stable with varying coefficients and always achieves better performance than all baseline methods. 
\end{document}
