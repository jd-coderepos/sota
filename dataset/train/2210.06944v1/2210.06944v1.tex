\pdfoutput=1

\documentclass{article}











\usepackage[final, nonatbib]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[numbers]{natbib}
\usepackage{hyperref}       \usepackage[capitalize]{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{colortbl}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{footmisc}

\title{SageMix: Saliency-Guided Mixup for Point Clouds}




\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{Sanghyeok Lee\thanks{First two authors have equal contribution.} \\
  Korea University \\
  \texttt{cat0626@korea.ac.kr} \\
  \And
  Minkyu Jeon\samethanks \\
  Korea University \\
  \texttt{jmk94810@korea.ac.kr} \\
  \AND
  Injae Kim \\
  Korea University \\
  \texttt{dna9041@korea.ac.kr} \\
  \And
  Yunyang Xiong \\
  Meta Reality Labs \\
  \texttt{yunyang@fb.com} \\
  \And
  Hyunwoo J. Kim\thanks{Corresponding author.} \\
  Korea University \\
  \texttt{hyunwoojkim@korea.ac.kr} \\
}





\begin{document}


\maketitle
\newcommand{\Point}{\textcolor[rgb]{0,0,0}{\mathcal{P}}}
\newcommand{\point}{\textcolor[rgb]{0,0,0}{p}}
\newcommand{\gt}{\textcolor[rgb]{0,0,0}{y}}


\newcommand{\green}{\textcolor[rgb]{0,0.5,0}}
\newcommand{\red}{\textcolor[rgb]{1,0.8,0.8}}
\newcommand{\yellow}{\textcolor[rgb]{1,1,0.8}}


\newcommand{\ie}{\textit{i.e.,} }
\newcommand{\eg}{\textit{e.g.,} }

\crefname{section}{Sec.}{Secs.}
\crefname{table}{Tab.}{Tabs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{equation}{Eq.}{Eqs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\Crefname{figure}{Figure}{Figures}
\Crefname{equation}{Equation}{Equations}

\definecolor{1}{rgb}{1,0.8,0.8} \definecolor{2}{rgb}{1,1,0.8} \definecolor{LightBlue}{rgb}{0.8,0.9,1} \definecolor{LightOrange}{rgb}{1,0.9,0.8} 

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark} 
\begin{abstract}
Data augmentation is key to improving the generalization ability of deep learning models. 
Mixup is a simple and widely-used data augmentation technique that has proven effective in alleviating the problems of overfitting and data scarcity.
Also, recent studies of saliency-aware Mixup in the image domain show that preserving discriminative parts is beneficial to improving the generalization performance.
However, these Mixup-based data augmentations are underexplored in 3D vision, especially in point clouds.
In this paper, we propose \textbf{SageMix}, a saliency-guided Mixup for point clouds to preserve salient local structures.
Specifically, we extract salient regions from two point clouds and smoothly combine them into one continuous shape.
With a simple sequential sampling by re-weighted saliency scores, SageMix preserves the local structure of salient regions.
Extensive experiments demonstrate that the proposed method consistently outperforms existing Mixup methods in various benchmark point cloud datasets.
With PointNet++, our method achieves an accuracy gain of 2.6\% and 4.0\% over standard training in 
3D Warehouse dataset (MN40) and ScanObjectNN, respectively. 
In addition to generalization performance, SageMix improves robustness and uncertainty calibration. 
Moreover, when adopting our method to various tasks including part segmentation and standard 2D image classification, our method achieves competitive performance.
Code is available at \url{https://github.com/mlvlab/SageMix}. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
Deep neural networks have achieved high performance in various domains including image, video, and speech.
Recent researches~\cite{li2018pointcnn, liu2019relation,qi2017pointnet,qi2017pointnet++,thomas2019kpconv,wang2019dynamic} have been proposed to employ deep learning model for 3D vision, especially in point clouds.
However, in the point cloud domain, deep learning models are prone to suffer from weak-generalization performance due to the limited availability of data compared to the image datasets, which contain almost millions of training samples.
For alleviating the data scarcity issue, data augmentation is a prevalent solution to increase the training data.

In the image domain, Mixup-based methods~\cite{verma2019manifold, yun2019cutmix, DBLP:conf/iclr/ZhangCDL18} combine two training images to generate augmented samples. More recent Mixup methods~\cite{gong2021keepaugment,kim2020puzzle, DBLP:conf/iclr/KimCJS21, DBLP:conf/iclr/UddinMSCB21} focus on leveraging saliency to preserve the discriminative regions such as foreground objects. Despite the success of saliency-aware Mixup methods, it has been less studied in point clouds due to its unordered and non-grid structure.
Moreover, saliency-aware augmentation methods require solving an additional optimization problem~\cite{kim2020puzzle, DBLP:conf/iclr/KimCJS21}, resulting in a considerable computational burden.

Recently, several works~\cite{chen2020pointmixup, lee2021regularization,zhang2022pointcutmix} have attempted to extend the concept of Mixup to point clouds. PointMixup~\cite{chen2020pointmixup} enables the linear interpolation of point clouds based on optimal assignment. RSMix~\cite{lee2021regularization} proposed a shape-preserving Mixup framework that extracts and merges the rigid subsets of each sample.
However, these approaches have limitations. 
PointMixup generates samples without preserving the local structure of the original shapes. 
For example, in the top row of~\Cref{fig:figure_intro}, the structure of the guitar is not preserved in generated samples.
RSmix generates discontinuous samples and these artifacts often hinder effective training.
Furthermore, these methods disregard the saliency, thereby causing the loss of the discriminative local structure in the original point cloud.

In this paper, we propose a \textbf{Sa}liency-\textbf{G}uid\textbf{e}d \textbf{Mix}up for point clouds (\textbf{SageMix}) that preserves discriminative local structures and generates continuous samples with smoothly varying mixing ratios.
For saliency estimation, we measure the contribution of each point by the gradients of a loss function.
Through a simple sequential sampling via re-weighted saliency scores, SageMix samples the query points to extract salient regions without solving additional optimization problems.
Then, SageMix smoothly combines point clouds, considering the distance to query points to minimize the loss of discriminative local structures as illustrated in \Cref{fig:figure_intro} (\eg the neck of a guitar and the back of a chair, etc.).

In summary, our \textbf{contributions} are fourfold:
\begin{itemize}
    \item We propose a novel saliency-guided Mixup method for point clouds. To the best of our knowledge, this is the first work that utilizes saliency for Mixup in point clouds.
    \item We design a Mixup framework that preserves the salient local structure of original shapes while smoothly combining them into one continuous shape.
    \item Extensive experiments demonstrate that SageMix brings consistent and significant improvements over state-of-the-art Mixup methods in generalization performance, robustness, and uncertainty calibration. \item We demonstrate that the proposed method is extensible to various tasks including part segmentation and standard image classification.
\end{itemize}
\begin{figure}[t]
\centering
\includegraphics[width=13.97cm, bb=0 0 1348 576, trim =0 0 0 0, clip]{Figures/figure_intro2.png}
\caption{\textbf{Comparison of Mixup methods on point clouds.} Given two original samples, (left) guitar and (right) chair, we generate samples by various Mixup methods. (Top) PointMixup does not preserve the discriminative structure. (Middle) The samples generated by RSMix contain the local structure of each sample, but the discontinuity occurs at the border. (Bottom) Our method, SageMix, generates a continuous mixture preserving the local structure of original shapes.}
\label{fig:figure_intro}
\end{figure} 

%
 \section{Preliminaries}
In this section, we briefly review the basic concept of Mixup and summarize the variants of Mixup for images and point clouds (\Cref{table:mixup}).
\begin{table}[t]
  \centering 
\renewcommand{\arraystretch}{1.1}
  \caption{\textbf{Variants of Mixup.}}
  \label{table:mixup} 
  \begin{tabular}{cl|l}
  \toprule
  \multicolumn{2}{c|}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{Equation}}\\
  \midrule
  \midrule
  \multirow{5}[2]*{Image}&mixup~\cite{DBLP:conf/iclr/ZhangCDL18} &  \\
  &Manifold mixup~\cite{verma2019manifold} & \\
  &CutMix~\cite{yun2019cutmix} & \\
  &SaliencyMix~\cite{DBLP:conf/iclr/UddinMSCB21} &\\ 
  &Puzzle Mix~\cite{kim2020puzzle} & \\
  &Co-Mixup~\cite{DBLP:conf/iclr/KimCJS21}&\\
  \midrule
  \multirow{3}[2]*{Point cloud}&PointMixup~\cite{chen2020pointmixup} & \\
  &RSMix~\cite{lee2021regularization} & \\
  &\textbf{SageMix} & \\
  \bottomrule
  \end{tabular}
\end{table}  \label{sec:pre}

\paragraph{Vicinal risk minimization with Mixup.} 
Given observed data  and a function , \citet{chapelle2000vicinal} learns the function  by minimizing the empirical vicinal risk: , where  is the loss function, and  is the virtual feature-target pair from the vicinal distribution  of the observed data . 
To construct an effective vicinal distribution in the image domain, \citet{DBLP:conf/iclr/ZhangCDL18} introduced Mixup:

where  are two pairs of data in training dataset , and  is a mixture ratio. Following \cite{DBLP:conf/iclr/ZhangCDL18},
\citet{verma2019manifold} proposed Manifold Mixup that applies Mixup in hidden representations (\ie  and CutMix~\cite{yun2019cutmix} generates samples via cut-and-paste manner with binary mask  (\ie , where  indicates the element-wise product). 
Combining with saliency,  SaliencyMix~\cite{DBLP:conf/iclr/UddinMSCB21} improved CutMix by selecting the patch  with the maximum saliency values.
Puzzle Mix optimizes the mask  and transport  for maximizing the saliency of the mixed sample (\ie ). 
Beyond two samples, Co-Mixup mixes multiple samples in a mini-batch by optimizing multiple masks (\ie ).

While saliency-aware Mixup methods boost the generalization of deep learning models, they are originally designed for the image domain. Hence, these methods are not directly applicable to point clouds due to the unordered and irregular structure.


\paragraph{Mixup in point cloud.} Several works~\cite{chen2020pointmixup, lee2021regularization} tried to leverage the Mixup in point cloud. PointMixup~\cite{chen2020pointmixup} linearly interpolates two point clouds by

where  is the set of points with ,  is the number of points, and  is the optimal bijective assignment between two point clouds.
In RSMix~\cite{lee2021regularization}, they generate an augmented sample by merging the subsets of two objects, defined as , where  is the rigid subset and  denotes  translated to the center of . 

Although these methods have shown that Mixup is effective for point clouds, some limitations have remained unresolved: loss of original structures, discontinuity at the boundary, and loss of discriminative regions.
Here, to address these issues, we propose a new Mixup framework in the following section.
 \section{Method}
\label{sec:method}
The main goal of SageMix is to generate an augmented sample that 1) maximally preserves the salient parts, 2) keeps the local structure of original shapes, and 3) maintains the continuity in the boundary.
To achieve this goal, SageMix extracts salient regions centered at query points and smoothly combines them through continuous weights in Euclidean space. 
The overall pipeline is illustrated in~\Cref{fig:figure_main} and pseudocode is provided in~\Cref{algo:1} . In this section, we delineate the process for selecting the query point based on the saliency map in~\Cref{sec:method.1} and Mixup methods for preserving the salient local structure in~\Cref{sec:method.2}.

\begin{figure}[t]
\centering
\includegraphics[width=13.97cm, bb=0 0 1290 516, trim =10 0 10 0, clip]{Figures/figure_main2.png}
\caption{\textbf{Illustration of SageMix pipeline.} 
Given a pair of samples , and , SageMix sequentially samples the query points , and  (\green{green}) based on saliency using ~\Cref{eq:prob_alpha} and~\Cref{eq:prob_beta}, respectively. 
Then, using smoothly varying weights with respect to the distance to query points, SageMix generates an augmented sample  preserving the salient local structures.}



\label{fig:figure_main}
\end{figure} \subsection{Saliency-guided sequential sampling}
\label{sec:method.1}

We first introduce a query point  that is considered the center of the region for preserving the local shape. A na\"ive random sampling for a query point is simple, but it does not guarantee that it is placed in a salient region. Thus, we propose saliency-guided query point selection for maximally maintaining the salient part.
We denote the saliency  of the input  by the norm of the gradient (\ie ), following~\cite{kim2020puzzle,DBLP:conf/iclr/KimCJS21}.
Deterministically selecting a query point with a maximum saliency score (\ie , where ) sounds promising but in practice, this makes it difficult to generate diverse samples since query points are always located at the same position.
Moreover, if two query points  are closely located in the Euclidean space, it is challenging to preserve the local structure of each sample because of the significant overlap.


To address these issues, SageMix sequentially selects the query point based on the saliency scores. The first step is to extract the query point . To maximize the diversity, the query point is sampled with respect to the probability distribution defined as

where  is the saliency of  and  is a random variable for index (). 
That is, the points in a salient region have a high chance to be chosen as a query point.
This simple sampling method efficiently provides diverse query points, thereby minimizing redundant selections.
Given query point , the next step is to define a sampling method for  to alleviate the overlap between selected parts.
We encourage the sampler to distance  from  by reweighting the saliency scores:  

Assuming the point , the probability of selecting the point  decreases.
This implies that SageMix samples a query point considering both distance and saliency.
Finally, with ~\Cref{eq:prob_alpha} and~\Cref{eq:prob_beta}, we can obtain the remotely located query points with high saliency scores, resulting in augmented samples preserving the discriminative structures of original shapes.


\subsection{Shape-preserving continuous Mixup}
\label{sec:method.2}
\begin{algorithm}[t]
\caption{{\label{algo:1}} \textbf{A saliency-guided Mixup for point clouds}} 
\textbf{Input:} , \newline
 : set of points,  : target,  : saliency values,  : bandwidth,  : assignment function,  : shape parameter\newline
\textbf{Output:} \newline

\begin{algorithmic}[1]
\State Draw  from  w.r.t.  \Comment{\Cref{eq:prob_alpha}}
\State Draw  from  w.r.t.  \Comment{\Cref{eq:prob_beta}}
\For { to }
    \State  \Comment{\Cref{eq:RBF}}
    \State   \Comment{, \Cref{eq:lambda_final}}
    \State  \Comment{\Cref{eq:temp}}
\EndFor
\State 
\State 
\State \Return 
\end{algorithmic}
\vspace{10pt}
\end{algorithm}
 Since our objective is to preserve the region around the query point , we need to impose high weight on the points near the query point. Further, to alleviate the discontinuity between two samples, the weight should smoothly vary in Euclidean space. Herein, we use a Gaussian Radial Basis Function (RBF) kernel to calculate weights: 

where  is the weight on the point  and  is a bandwidth for kernel. 
The weight  smoothly increases as the distance to  decreases, resulting in the region around the query point being prone to respect its original shape.
As shown in (\Cref{fig:figure_main}), the parts with higher weights maintain their original local structure more in a mixed sample, \eg the body of the guitar and the tail of the airplane.

Given two point clouds () and their corresponding weights , we generate an augmented sample via point-wise interpolation. 
For differentially mixing points, we define the mixing ratio for the -th point pair as

where  is the assignment in \Cref{eq:OA}. Note, that the point-wise mixing ratio  is the ratio between the weight of two paired points (\ie ), thus enabling linear interpolation. For simplicity, we use the notation  instead of .
Then, we generate the virtual point cloud by modifying the \Cref{eq:pointmixup} to

where  can be interpreted as the overall mixing ratio that is used in label interpolation. 
In Mixup, the distribution of  is a key factor for model training~\cite{DBLP:conf/iclr/ZhangCDL18}.
To control the distribution of , we introduce a prior factor : 



\textit{Remarks.} 
Our SageMix can emulate both PointMixup and RSMix depending on the bandwidth .
If the bandwidth  is sufficiently large, , where , resulting in . That is, all points are mixed in the same ratio as PointMixup. 
Conversely, when the bandwidth becomes smaller, SageMix changes the mixing ratio drastically around the boundary between the two shapes and generates an augmented sample like RSMix. 
Qualitative results are available in ~\Cref{sec:exp.2}. Additionally, with a minor change, SageMix is applicable in feature space as Manifold Mixup, see~\Cref{sec:manifold} for more discussion.
 \section{Experiments}
\label{sec:exp}
In this section, we demonstrate the effectiveness of our proposed method SageMix with various benchmark datasets. First, for 3D shape classification, we evaluate the generalization performance, robustness, and calibration error in \Cref{sec:exp.1}. Next, we provide an ablation study and analyses of SageMix in \Cref{sec:exp.2}. Lastly, we study the extensibility of our method in \Cref{sec:exp.3}. Implementation details are provided in~\Cref{sec:implementation}.
\begin{table}[t!]
  \centering 
  \setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.1}
  \caption{\textbf{3D shape classification performance on MN40/OBJ\_ONLY/PB\_T50\_RS.}}
  \label{table:main} 
  \begin{tabular}{cl|ccc}
  \toprule
  \multirow{2}[2]*{\textbf{Model}} & \multicolumn{1}{c|}{\multirow{2}[2]*{\textbf{Method}}}& \multicolumn{3}{c}{\textbf{Dataset}}\\
  \cmidrule(lr){3-5}
  &   & MN40 & OBJ\_ONLY & PB\_T50\_RS \\
  \midrule
  \midrule
  \multirow{4}[3]*{PointNet~\cite{qi2017pointnet}} 
  &Base & 89.2 & 79.1 & 65.4\\
  \cmidrule(lr){2-5}

  &\textbf{+} PointMixup~\cite{chen2020pointmixup}& \cellcolor{2}89.9 & \cellcolor{2}79.4 &  \cellcolor{2}65.7 \\
  &\textbf{+} RSMix~\cite{lee2021regularization}&  88.7 & 77.8 &  \cellcolor{2}65.7 \\
  \cmidrule(lr){2-5}
  &\textbf{+} \textbf{SageMix}&\cellcolor{1}\textbf{90.3}&\cellcolor{1}\textbf{79.5}&\cellcolor{1}\textbf{66.1}\\
  \midrule
  \midrule
  \multirow{4}[3]*{PointNet++~\cite{qi2017pointnet++}}
  &Base & 90.7 & 86.5 & 79.7\\
  \cmidrule(lr){2-5}
  &\textbf{+} PointMixup~\cite{chen2020pointmixup}& \cellcolor{2}92.3 & \cellcolor{2}87.6 & 80.2 \\
  &\textbf{+} RSMix~\cite{lee2021regularization}& 91.6 & 87.4 & \cellcolor{2}81.1\\
  \cmidrule(lr){2-5}
  &\textbf{+} \textbf{SageMix}&\cellcolor{1}\textbf{93.3}&\cellcolor{1}\textbf{88.7}&\cellcolor{1}\textbf{83.7}\\
  \midrule
  \midrule
  \multirow{4}[3]*{DGCNN~\cite{wang2019dynamic}} 
  &Base & 92.9 & 86.2 & 79.9\\
  \cmidrule(lr){2-5}
  &\textbf{+} PointMixup~\cite{chen2020pointmixup}& 92.9 & \cellcolor{2}86.9 & \cellcolor{2}82.5 \\
  &\textbf{+} RSMix~\cite{lee2021regularization}&\cellcolor{2}93.5&86.6& 82.2\\
  \cmidrule(lr){2-5}
  &\textbf{+} \textbf{SageMix}&  \cellcolor{1}\textbf{93.6}& \cellcolor{1}\textbf{88.0} & \cellcolor{1}\textbf{83.6}\\
  \bottomrule
  \end{tabular}

\vspace{8pt}
\end{table}  \paragraph{Data.} We use two benchmark dataset: 3D Warehouse dataset (\textbf{MN40})~\cite{xu2021image2point} and ScanObjectNN~\cite{uy2019revisiting}. MN40 is a synthetic dataset containing 9,843 CAD models for training and 2,468 CAD models for evaluation. Each CAD model of MN40 is obtained from 3D Warehouse~\cite{3dwarehouse}. ScanObjectNN, obtained from SceneNN~\cite{hua2016scenenn} and ScanNet\cite{dai2017scannet}, is a real-world dataset that is split into 80\% for training and 20\% for evaluation. Among the variants of ScanObjectNN, we adopt the simplest version (\textbf{OBJ\_ONLY}) and the most challenging version (\textbf{PB\_T50\_RS}). For training models, we use only coordinates (x,y,z) of 1024 points without additional information such as the normal vector.

\paragraph{Baselines.} For a comparison with previous studies, we use three backbone models: \textbf{PointNet}~\cite{qi2017pointnet}, \textbf{PointNet++}~\cite{qi2017pointnet++}, and \textbf{DGCNN}~\cite{wang2019dynamic}. We compare SageMix with the model under default augmentation in \cite{qi2017pointnet,qi2017pointnet++,wang2019dynamic, uy2019revisiting} (\textbf{Base}), 
and other Mixup approaches (\textbf{PointMixup}~\cite{chen2020pointmixup}, \textbf{RSMix}~\cite{lee2021regularization}). 
We report the performance in overall accuracy. We highlight the best performance in red and second-best performance in yellow.


\subsection{3D shape classification.}

\label{sec:exp.1}
\paragraph{Generalization performance.} \Cref{table:main} summarizes the experimental results of 3D shape classification on three datasets. Our framework significantly outperforms all of the previous methods in every dataset and model.
Although the datasets are quite saturated, the averages of the improvements against the Base with PointNet, PointNet++, and DGCNN are 0.7\%, 2.9\%, and 2.1\%, respectively.
With PointNet++, SageMix improves the overall accuracy by 2.6\%, 2.2\%, and 4.0\% compared to Base in MN40, OBJ\_ONLY, and PB\_T50\_RS, respectively. We observe similar performance improvements in DGCNN by 0.7\%, 1.8\%, and 3.7\% over Base. These consistent improvements demonstrate the effectiveness of our framework.

\begin{table}[t!]
  \centering 
  \setlength{\tabcolsep}{2pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{\textbf{Robustness and calibration with DGCNN on OBJ\_ONLY.}}
  \label{table:robustness} 
  \begin{tabular}{l|cc|cc|cc|cc|c}
  \toprule
  \multicolumn{1}{c|}{\multirow{2}[2]*{\textbf{Method}}}&\multicolumn{2}{c|}{\textbf{Gaussian noise.}}& \multicolumn{2}{c|}{\textbf{Rotation 180\textdegree}} &\multicolumn{2}{c|}{\textbf{Scaling}} & \multicolumn{2}{c|}{\textbf{Dropout}} & \multicolumn{1}{c}{\textbf{Calibration}}\\
  \cmidrule(lr){2-10}
  &   &  & X-axis & Z-axis &  & & 25\% & 50\% & ECE(\%) \\\midrule
  \midrule
  DGCNN~\cite{wang2019dynamic} & 84.9 & 48.4 & 32.5& 32.4 & 73.7 & 73.0& 83.3 & \cellcolor{2}75.7 & 19.8 \\+ PointMixup~\cite{chen2020pointmixup} & \cellcolor{2}85.0 & \cellcolor{1}\textbf{61.3 }& 31.7& \cellcolor{2}32.7 & 73.8 &73.0& \cellcolor{2}84.2 & 74.9 & \cellcolor{2}6.8 \\+ RSMix~\cite{lee2021regularization} & 84.2 & 49.1 & \cellcolor{2}32.7& 32.6 & \cellcolor{2}75.0 & \cellcolor{2}74.5& 84.0 & 73.6 &18.9 \\
  + \textbf{SageMix}& \cellcolor{1}\textbf{85.7}&\cellcolor{2}51.2 & \cellcolor{1}\textbf{36.5} & \cellcolor{1}\textbf{37.9} & \cellcolor{1}\textbf{75.6} & \cellcolor{1}\textbf{75.2} & \cellcolor{1}\textbf{84.9} & \cellcolor{1}\textbf{79.0} &\cellcolor{1} \textbf{5.1} \\

  \bottomrule
  \end{tabular}
  
  
\end{table}  \paragraph{Robustness.}
We adopt DGCNN and OBJ\_ONLY to evaluate the robustness of models trained by our method. We compare our method with previous methods~\cite{chen2020pointmixup, lee2021regularization} on four types of corruption: (1) jittering the point cloud with \textbf{Gaussian noise} (), (2) \textbf{Rotation 180\textdegree }, (3) \textbf{Scaling} with a factor in \{0.6, 2.0\}, and (4) \textbf{Dropout} 25\% or 50\% of all points. As shown in~\Cref{table:robustness}, SageMix consistently improves the robustness in various corruption. Except for Gaussian noise with , DGCNN trained with SageMix shows the best robustness with significant gains compared to previous methods. Specifically, SageMix outperforms the other methods with gains of 5.5\% for rotation (Z-axis), 2.2\% for scaling (), and 3.3\% for dropout (50\%) over base DGCNN.

\paragraph{Calibration.}
Previous works~\cite{guo2017calibration, kumar2018trainable} have proven that deep neural networks tend to be overconfident resulting in poorly calibrated models. 
In other words, a well-calibrated model should provide an accurate probability according to its predictions.
Here, we report \textit{Expected Calibration Error} (ECE)~\cite{guo2017calibration} to estimate the uncertainty calibration of models.
We use the same setting as the robustness test for reporting ECE (\Cref{table:robustness}).
Overall, this result reveals that SageMix provides the best performance on uncertainty calibration compared to the other methods. See~\Cref{sec:calibration} for more results.

\subsection{Ablation study and analyses}

\label{sec:exp.2}
We provide various quantitative and qualitative analyses for a better understanding of SageMix.
We use DGCNN and OBJ\_ONLY for an ablation study and MN40 for visualization.


\begin{table}[t!]
  \centering
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
    \caption{\textbf{Ablation of Saliency-guided sequential sampling.}}
  \begin{tabular}{c|c|ccccc}
    \toprule
    \textbf{Metric} & DGCNN~\cite{wang2019dynamic} & Uniform & Max & Saliency only & \textbf{\textbf{SageMix}}\\
    \midrule
    \midrule
    OA & 86.2 & 86.8 & 86.1 & 87.8  & \textbf{88.0} \\
    \bottomrule
  \end{tabular}
  \label{table:ablation}
\end{table} 
 \paragraph{Ablation on query point sampling.} 
We explore the effectiveness of saliency-guided sequential query point sampling. 
\Cref{table:ablation} shows the results with various sampling methods for query points. 
A na\"ive uniform sampling for query point (\textbf{Uniform}), without considering the saliency, introduces +0.6\% gains over base DGCNN. 
Interestingly, when SageMix (deterministically) selects the query point with maximum saliency values (\textbf{Max}), the performance is even degraded (-0.1\%). 
With our saliency-guided random sampling for two query points based only on~\Cref{eq:prob_alpha} (\textbf{Saliency Only}), the performance is significantly improved to 87.8\% (+1.6\%).
These results suggest that although saliency is key to effective training, we also have to consider the diversity for generating a new sample. 
Finally, the best accuracy of 88.0\% (+1.8\%) is obtained by our saliency-guided sequential sampling considering saliency and the distance between query points together.

\begin{figure}[t]
\centering
\includegraphics[width=13.97cm, bb=0 0 2172 799, trim =0 0 0 0, clip]{Figures/figure_analysis2.png}
\caption{\textbf{Qualitative analyses on prior factor  and the bandwidth }. 
Given two samples (left) table and airplane, SageMix generates a sample based on (Top) various  with a fixed  and (Bottom) various  with a fixed . 
Note that the \green{green points} indicate query points.
}
\label{fig:figure_analysis} 
\end{figure} \paragraph{Prior factor.} We introduce the parameter  as a prior factor for mixing ratio . 
The top row of ~\Cref{fig:figure_analysis} is the visualization of samples generated by SageMix with various prior factor . 
Given two samples and their corresponding query points colored in green, SageMix generates a sample with the salient local regions from both samples, \eg the head and right-wing of an airplane and the back of a table are preserved. 
We observe that the generated sample gets close to the airplane as , and vice versa. In short, SageMix controls the distribution of mixing ratio  based on .

\begin{table}[t!]
  \centering 
  \setlength{\tabcolsep}{8pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{\textbf{Quantitative analysis on the bandwidth }}
  \label{table:bandwidth} 
  \begin{tabular}{c|ccccc}
  \toprule
   & 0.1 & 0.3 & 0.5 & 1.0 & 2.0 \\
  \midrule
  \midrule
  OA & 87.2 & \textbf{88.0} & 87.6 & 87.3 & 87.6 \\
  \bottomrule
  \end{tabular}
  
\end{table}  \paragraph{Bandwidth.} The bandwidth  of the RBF kernel controls the change of point-wise mixing ratios in SageMix. 
As mentioned in~\Cref{sec:exp.2}, when the bandwidth is sufficiently large (, SageMix emulates PointMixup~\cite{chen2020pointmixup}.
SageMix tends to yield globally even weights and constant mixing ratios for all points rather than focusing on local parts. 
In contrast, as the bandwidth gets smaller, SageMix tends to impose higher weights around the query point preserving the local structure more precisely. 
These are well exemplified in the bottom row of~\Cref{fig:figure_analysis}. For instance, when , we notice the steep change in the boundary of two samples while preserving the salient local structure. 
This allows generating augmented samples like RSMix.
In short, our SageMix can exhibit similar behaviors as PointMixup and RSMix depending on the bandwidth . 
We also share the quantitative analysis of the bandwidth with DGCNN and OBJ\_ONLY in~\Cref{table:bandwidth}. We observed that SageMix with a wide range of bandwidth  (0.1 to 2.0) consistently outperforms previous Mixup methods (e.g., 86.9\%, 86.6\% for PointMixup, RSMix). 


\subsection{Extensions to part segmentation and 2D image classification.}
\label{sec:exp.3}

\begin{table}[t!]
  \centering 
  \setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.1}
  \caption{\textbf{Part segmentation performance on SN-Parts~\cite{xu2021image2point}.}}
  \label{table:part} 
  \begin{tabular}{c|ccc|c}
  \toprule
  \textbf{Model} & Base & PointMixup~\cite{chen2020pointmixup} & RSMix~\cite{lee2021regularization} & \textbf{SageMix} \\
  \midrule
  \midrule 
  PointNet++~\cite{qi2017pointnet++}& 85.1 & 85.5 & 85.4 & \textbf{85.7}\\
  DGCNN~\cite{wang2019dynamic}& 85.1 & 85.3 & 85.2 & \textbf{85.4}\\


  \bottomrule
  \end{tabular}
  
  
\end{table} 

 \paragraph{Part segmentation.} For part segmentation, we train DGCNN on 3D Warehouse (\textbf{SN-Parts}) ~\cite{3dwarehouse,xu2021image2point}. In part segmentation, since a model predicts a label for each point , we generate point-wise ground truth, \ie . Also, for a comparison with previous methods, we used the official code by the authors of PointMixup and RSMix with minor modifications for generating point-wise ground truth.
We follow the settings in~\cite{qi2017pointnet++, wang2019dynamic} to evaluate our method and reports the results in~\Cref{table:part}. Note that although the gain seems small, SageMix outperforms previous Mixup methods. Also, considering the already saturated performance, we believe that the improvement (+0.6\%, +0.3\% in PointNet++, DGCNN) over the base model is noteworthy. 

\begin{table}[t!]
  \centering 
  \setlength{\tabcolsep}{2pt}
  \renewcommand{\arraystretch}{1.1}
  \caption{\textbf{2D image classification performance with PreActResNet18~\cite{he2016identity} on CIFAR-100.}}
  \label{table:2dclassification} 
  \begin{tabular}{l|ccccccc|c}
  \toprule
  \multicolumn{1}{c|}{\textbf{Dataset}}&\multicolumn{1}{c}{Vanilla}& \multicolumn{1}{c}{Mixup} &\multicolumn{1}{c}{Manifold} &
  \multicolumn{1}{c}{CutMix} &
  \multicolumn{1}{c}{SaliencyMix}&
  \multicolumn{1}{c}{Puzzle Mix} & \multicolumn{1}{c|}{Co-Mixup} &  \multicolumn{1}{c}{\textbf{SageMix}} \\
  \midrule
  \midrule
  CIFAR-100 & 76.41 & 77.57 & 78.36 & 78.71 & 79.06 & 79.38 & 80.13 & \textbf{80.16}\\
  \bottomrule
  \end{tabular}
  
\end{table} 




 \paragraph{2D classification.}
Our framework is also applicable to 2D image classification. Following~\cite{DBLP:conf/iclr/KimCJS21}, we used PreActResNet18~\cite{he2016identity} in the CIFAR-100 for our experiments. 
We compare our method with several Mixup baselines in the image domain~\cite{kim2020puzzle, DBLP:conf/iclr/KimCJS21,  DBLP:conf/iclr/UddinMSCB21, verma2019manifold, DBLP:conf/iclr/ZhangCDL18, yun2019cutmix}. 
Although our method is designed for point clouds, it shows competitive performance. 
SageMix achieves the best accuracy of 80.16\% with PreActResNet18 on the CIFAR-100 dataset (\Cref{table:2dclassification}). 
It is worth noting that PuzzleMix and Co-Mixup require an additional optimization which introduces a considerable computational overhead. 
Specifically, SageMix is 6.05 faster than Co-Mixup~\cite{DBLP:conf/iclr/KimCJS21} per epoch in the CIFAR-100 dataset.
We believe that our simple sampling technique is helpful to improve the generalization power of the model. For more details, see~\Cref{sec:detailed2D}.
 \section{Related works}
\label{sec:rel}
\textbf{Deep learning on point clouds.}
PointNet ~\cite{qi2017pointnet} is a pioneering work that designs a novel deep neural network for processing unordered 3D point sets with a multi-layer perceptron. Inspired by CNNs, \citet{qi2017pointnet++} propose PointNet++ with a hierarchical architecture. In DGCNN, \citet{wang2019dynamic} introduce EdgeConv which utilizes edge features from the dynamically updated graph.
Additionally, various works have focused on point-wise multi-layer perceptron~\cite{joseph2019momen, yan2020pointasnl, zhao2019pointweb}, convolution~\cite{hua2018pointwise,   lan2019modeling,lei2019octree,li2018pointcnn,liu2019relation, thomas2019kpconv, wu2019pointconv}, and graph-based methods~\cite{shen2018mining, zhang2018graph} to process point clouds. Parallel to these approaches, other recent works~\cite{chen2020pointmixup, kim2021point, lee2021regularization,li2020pointaugment, yun2019cutmix} focus on data augmentation to improve the generalization power of deep neural networks in point clouds.

\paragraph{Mixup.}
Mixup~\cite{DBLP:conf/iclr/ZhangCDL18} is a widely used regularization technique, which linearly interpolates a pair of images to generate an augmented sample. 
Following this work, \citet{verma2019manifold} propose Manifold Mixup that extends Mixup to the hidden representations. CutMix ~\cite{yun2019cutmix} replaces a part of an image with a part of another one. 
More recent studies~\cite{kim2020puzzle, DBLP:conf/iclr/KimCJS21, DBLP:conf/iclr/UddinMSCB21} have been proposed to preserve the saliency while mixing samples. 
In point clouds, PointMixup~\cite{chen2020pointmixup} is the first approach that adapts the concept of Mixup in point clouds with the optimal assignments. Instead of linear interpolation, \citet{lee2021regularization} propose RSMix which merges the subsets of two point clouds inspired by CutMix.

\paragraph{Saliency.}
Measuring the saliency of data using neural networks has been studied to obtain a more precise saliency map~\cite{selvaraju2017grad, wang2015deep, zhao2015saliency}. The saliency has been prevalent in various fields such as object detection, segmentation, and speech recognition~\cite{gong2021keepaugment, jung2011unified, kalinli2007saliency, ren2013region, wei2017object}. Similarly, PointCloud Saliency Map~\cite{zheng2019pointcloud} constructed the saliency map to identify the critical points via building a gradient-based saliency map. Recently, saliency has been used in Mixup framework ~\cite{kim2020puzzle, DBLP:conf/iclr/KimCJS21, DBLP:conf/iclr/UddinMSCB21} to prevent generating samples only with background or irrelevant regions to the target objects.
 \section{Conclusion}
\label{sec:con}
We propose SageMix, a novel saliency-guided Mixup for point clouds to preserve salient local structures. Our method generates an augmented sample with a continuous boundary while preserving the discriminative regions.
Additionally, with a simple saliency-guided sequential sampling, SageMix achieves state-of-the-art performance in various metrics (\eg generalization, robustness, and uncertainty calibration). Moreover, we demonstrate that the proposed method is extensible to various tasks: part segmentation and standard 2D image classification.
The visualization supports that SageMix generates a continuous mixture while respecting the salient local structure.


\begin{ack}
This work was partly supported by ICT Creative Consilience program (IITP-2022-2020-0-01819) supervised by the IITP; the National Supercomputing Center with supercomputing resources including technical support (KSC-2022-CRE-0261); and IITP grant funded by the Korea government (MSIT) (No.2021-0-02312, Efficient Meta-learning Based Training Method and Multipurpose Multi-modal Artificial Neural Networks for Drone AI).
\end{ack}
 
\bibliographystyle{unsrtnat} \bibliography{reference}


\newpage


\section*{Checklist}





\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{See abstract and~\Cref{sec:intro}.}
  \item Did you describe the limitations of your work?
    \answerYes{See supplement.}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{See supplement.}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
        \item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{See supplement.}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{See supplement.}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerYes{See supplement.}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{See supplement.}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{See supplement.}
  \item Did you mention the license of the assets?
    \answerYes{See supplement.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{See supplement.}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerYes{We use the publicly available benchmark datasets.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerYes{We use the publicly available benchmark datasets.}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate}

\newpage

\appendix
\section*{Appendix}

\section{Implementation details}
\label{sec:implementation}
We conduct experiments using Python and PyTorch\footnote{\copyright 2016 Facebook, Inc (Adam Paszke). Licensed under BSD-3-Clause License}~\cite{paszke2019pytorch} with a single NVIDIA TITAN RTX for point clouds and NVIDIA RTX 3090 for 2D image classification.
Following the original configuration in~\cite{qi2017pointnet,qi2017pointnet++,wang2019dynamic}, we use the Adam~\cite{DBLP:journals/corr/KingmaB14} optimizer with an initial learning rate of  for PointNet\footnote{\label{note1}\copyright 2017 Charles R. Qi. Licensed under MIT License}~\cite{qi2017pointnet} and PointNet++\footref{note1}~\cite{qi2017pointnet++} and SGD with an initial learning rate of  for DGCNN\footnote{\copyright 2019 Yue Wang. Licensed under MIT License }~\cite{wang2019dynamic}. We train models with a batch size of 32 for 500 epochs.
For a fair comparison with previous works~\cite{chen2020pointmixup,lee2021regularization}, we also adopt conventional data augmentations with our framework (\ie scaling and shifting for MN40~\cite{3dwarehouse} and rotation and jittering for ScanObjectNN~\footnote{\copyright 2019 Vision \& Graphics Group, HKUST. Licensed under MIT License}\cite{uy2019revisiting}). When the performance of a baseline on ScanObjectNN is unavailable in the original paper of PointMixup~\cite{chen2020pointmixup} and RSMix\footnote{\copyright 2020 dogyoonlee. Licensed under MIT License}~\cite{lee2021regularization}, we reproduce the results based on their official code.
For hyperparameters of SageMix, we opt  in entire experiments. Regarding the bandwidth for RBF kernel, we opt  for PointNet and  for PointNet++ and DGCNN.
 \section{Additional Experiments}
\label{sec:experiments}

\subsection{Error bars}
\label{sec:errorbars}
Performance oscillation is an important issue in point cloud benchmarks. However, for a fair comparison with the numbers reported in PointMixup~\cite{chen2020pointmixup} and RSMix~\cite{lee2021regularization}, we followed the prevalent evaluation metric in point clouds, which reports the best validation accuracy. Apart from this, we here provide the additional results with five runs on OBJ\_ONLY. The mean and standard deviation are presented in~\Cref{table:meanstd}.

\begin{table}[h!]
  \centering 
  \setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.1}
  \caption{\textbf{Mean and standard deviation measures on OBJ\_ONLY.}}
  \label{table:meanstd} 
  \begin{tabular}{l|ccc}
  \toprule
  \multicolumn{1}{c|}{\multirow{2}[2]*{\textbf{Method}}}& \multicolumn{3}{c}{\textbf{Model}}\\
  \cmidrule(lr){2-4}
    & PointNet~\cite{qi2017pointnet} & PointNet++~\cite{qi2017pointnet++} & DGCNN~\cite{wang2019dynamic} \\
  \midrule
  \midrule
    Base & 78.560.51 & 86.140.39 & 85.720.44\\
    + PointMixup~\cite{chen2020pointmixup} & 78.880.28 & 87.500.26 & 86.260.34\\
    + RSMix~\cite{lee2021regularization} & 77.600.56 & 87.300.65 & 85.880.59\\
    + \textbf{SageMix} & \textbf{79.140.30} & \textbf{88.420.26} & \textbf{87.320.53}\\
  \bottomrule
  \end{tabular}
\end{table}  
\subsection{Manifold mixup}
\label{sec:manifold}
We train DGCNN~\cite{wang2019dynamic} to validate the SageMix in a feature space. Following manifold Mixup~\cite{verma2019manifold}, we apply SageMix in a randomly selected layer. The results are summarized in~\Cref{table:manifold}. We observe the competitiveness of SageMix in feature space with the performance improvements by 0.6\%, 1.5\%, 3.3\% in MN40, OBJ\_ONLY, and PB\_T50\_RS, respectively. 

\begin{table}[h!]
  \centering 
  \setlength{\tabcolsep}{4pt}
  \caption{\textbf{SageMix in input and feature space.}}
  \label{table:manifold} 
  \begin{tabular}{l|c c c}
  \toprule
  \multicolumn{1}{c|}{\textbf{Method}}&\multicolumn{1}{c}{\textbf{MN40}}& \multicolumn{1}{c}{\textbf{OBJ\_ONLY}} &\multicolumn{1}{c}{\textbf{PB\_T50\_RS}} \\
  \midrule
  \midrule
  DGCNN~\cite{wang2019dynamic}& 92.9 & 86.2 & 79.9\\
  + \textbf{SageMix} (Input Space) & 93.6 & 88.0 & 83.6 \\
  + \textbf{SageMix} (Feature Space)& 93.5 & 87.7& 83.2\\
  \bottomrule
  \end{tabular}
\end{table}  
\subsection{Uncertainty calibration}
\label{sec:calibration}
In this section, we measure the Expected Calibration Error (ECE)~\cite{guo2017calibration} of the model on three datasets.
As shown in~\Cref{table:calibration}, our model consistently has the lowest calibration error on every dataset. Specifically, SageMix lowers ECE by 16.1\%, 14.7\%, and 15.6\% compared to vanilla DGCNN in MN40, OBJ\_ONLY, and PB\_T50\_RS, respectively.  

\begin{table}[h!]
  \centering 
  \setlength{\tabcolsep}{5pt}
  \caption{\textbf{Expected calibration error with DGCNN.}}
  \label{table:calibration} 
  \begin{tabular}{l|ccc|c}
  \toprule
  \multicolumn{1}{c|}{\textbf{Dataset}} & Vanilla& PointMixup~\cite{chen2020pointmixup} & RSMix~\cite{lee2021regularization} & \textbf{SageMix}\\

  \midrule
  \midrule
  MN40& 18.3&2.4&24.2&\textbf{2.2}\\
  OBJ\_ONLY & 19.8 & 6.8 & 18.9 & \textbf{5.1}\\
  PB\_T50\_RS& 18.9 &4.2&16.7&\textbf{3.3} \\
  \bottomrule
  \end{tabular}
  
  
\end{table}  
\subsection{Detailed results of 2D classification}
\label{sec:detailed2D}
We largely follow the setting in Co-Mixup\footnote{\copyright 2021 Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Licensed under MIT License}~\cite{DBLP:conf/iclr/KimCJS21} except for the learning rate. We trained 300 epochs with the batch size of 128. We adopt SGD as an optimizer with an initial learning rate of 0.1. We set the weight decay and the momentum as  and 0.9, respectively. We consider the column number and the row number as the coordinates of each pixel.
For SageMix, we use  and  .
In~\Cref{table:2dcls_supple}, we report the accuracy and latency for each method. The second row of the table shows the running time per epoch. Our method is 6.05 faster than Co-Mixup~\cite{DBLP:conf/iclr/KimCJS21}. 
It is worth noting that our framework achieves state-of-the-art performance with a tolerable computational cost considering the improvements.

\begin{table}[h!]
  \centering 
  \setlength{\tabcolsep}{2pt}
  \caption{\textbf{2D classification with PreActResNet18~\cite{he2016identity} on CIFAR-100.}}
  \label{table:2dcls_supple} 
  \begin{tabular}{l|ccccccc|c}
  \toprule
  \multicolumn{1}{c|}{}&\multicolumn{1}{c}{\textbf{Vanilla}}& \multicolumn{1}{c}{\textbf{Mixup}} &\multicolumn{1}{c}{\textbf{Manifold}} &
  \multicolumn{1}{c}{\textbf{CutMix}} &
  \multicolumn{1}{c}{\textbf{SaliencyMix}}&
  \multicolumn{1}{c}{\textbf{Puzzle Mix}} & \multicolumn{1}{c|}{\textbf{Co-Mixup}} &  \multicolumn{1}{c}{\textbf{Ours}} \\
  \midrule
  \midrule
  ACC. (\%) & 76.41 & 77.57 & 78.36 & 78.71 & 79.06 & 79.38 & 80.13 & \textbf{80.16}\\
  Time.(sec) & 13.1 & 20.4 & 20.8 & 23.4 & 21.1 & 34.9 & 147.0 & 24.3 \\

  \bottomrule
  \end{tabular}
  
\end{table} 
  \section{Qualitative results}
\label{sec:Qualitative}
\subsection{Visualization}
In this section, we provide the qualitative results of SageMix. As in~\Cref{fig:figure_supplementary1} and~\Cref{fig:figure_supplementary2}, given original samples (left and right), SageMix generate the augmented samples (middle). Also, we qualitatively compare SageMix with other baselines in~\Cref{fig:figure_supplementarycompare}.


\begin{figure}[t]
\centering
\includegraphics[width=13.97cm, bb=0 0 593 816, trim =10 0 10 0, clip]{Figures/figure_supplementary_12.png}
\caption{\textbf{Visualization of augmented samples by SageMix.} Given two samples (left and right), SageMix generates a sample (middle) based on \green{query points}.}



\label{fig:figure_supplementary1}
\end{figure} \begin{figure}[t]
\centering
\includegraphics[width=13.97cm, bb=0 0 593 816, trim =10 0 10 0, clip]{Figures/figure_supplementary_22.png}
\caption{\textbf{Visualization of augmented samples by SageMix.} Given two samples (left and right), SageMix generates a sample (middle) based on \green{query points}.}



\label{fig:figure_supplementary2}
\end{figure} \begin{figure}[t]
\centering
\includegraphics[width=13.3cm, bb=0 0 1299 2035,trim =0 0 0 0, clip]{Figures/figure_supplement_compare2.png}
\caption{\textbf{Qualitative results with SageMix and baselines.} Given two source samples(left), PointMixup does not preserve the salient structure and RSMix loses the continuity. SageMix generates a continuous mixture preserving the local structure of original shapes(right).}


\label{fig:figure_supplementarycompare}
\end{figure} 

 \input{Appendix/limitation}












\end{document}