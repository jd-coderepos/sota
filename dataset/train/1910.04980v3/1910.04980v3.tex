\documentclass[preprint,3pt]{elsarticle}

\usepackage{hyperref}



\usepackage{amsmath, amssymb}
\usepackage[capitalize]{cleveref}
\usepackage{color,soul}
\usepackage{tabularx}
\usepackage{MnSymbol}
\usepackage[capitalize]{cleveref}
\usepackage{amsfonts}
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{url}
\usepackage{color,soul}
\usepackage{bm}
\usepackage{multirow}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{arydshln}
\usepackage{url}
\usepackage[shortlabels]{enumitem}
\usepackage{bigstrut}
\usepackage{array}
\usepackage{microtype}
\usepackage{pifont}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{flushend}
\usepackage{booktabs,dcolumn}



\newcolumntype{d}[1]{D..{#1}}
\newcommand\mc[1]{\multicolumn{1}{c}{#1}} \newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\journal{Journal of Elsevier}










\begin{document}

\begin{frontmatter}


\title{Conversational Transfer Learning for Emotion Recognition}



\author[address1]{Devamanyu Hazarika}
\ead{hazarika@comp.nus.edu.sg}

\author[address3]{Soujanya Poria\corref{correspondingauthor}}
\cortext[correspondingauthor]{Corresponding author.}
\ead{soujanya\_poria@sutd.edu.sg}

\author[address1]{Roger Zimmermann}
\ead{rogerz@comp.nus.edu.sg}

\author[address2]{Rada Mihalcea}
\ead{mihalcea@umich.edu}





\address[address1]{School of Computing, National University of Singapore}
\address[address2]{Computer Science \& Engineering, University of Michigan, USA}
\address[address3]{Information Systems Technology and Design, Singapore University of Technology and Design}





\begin{abstract}
Recognizing emotions in conversations is a challenging task due to the presence of contextual dependencies governed by self- and inter-personal influences. Recent approaches have focused on modeling these dependencies primarily via supervised learning. However, purely supervised strategies demand large amounts of annotated data, which is lacking in most of the available corpora in this task. To tackle this challenge, we look at transfer learning approaches as a viable alternative. Given the large amount of available conversational data, we investigate whether generative conversational models can be leveraged to transfer affective knowledge for detecting emotions in context. We propose an approach, \textit{TL-ERC}, where we pre-train a hierarchical dialogue model on multi-turn conversations (source) and then transfer its parameters to a conversational emotion classifier (target). In addition to the popular practice of using pre-trained sentence encoders, our approach also incorporates recurrent parameters that model inter-sentential context across the whole conversation. Based on this idea, we perform several experiments across multiple datasets and find improvement in performance and robustness against limited training data. TL-ERC also achieves better validation performances in significantly fewer epochs. Overall, we infer that knowledge acquired from dialogue generators can indeed help recognize emotions in conversations.
\end{abstract}

\begin{keyword}
Emotion Recognition in Conversations \sep Transfer Learning \sep Generative Pre-training \sep Conversation Modeling
\end{keyword}

\end{frontmatter}





\section{Introduction} \label{sec:intro}

Emotion Recognition in Conversations (ERC) is the task of detecting emotions from utterances in a conversation. It is an important task with applications ranging from dialogue understanding to affective dialogue systems~\cite{poria2019emotion}. Apart from the traditional challenges of dialogue understanding, such as intent-detection, contextual grounding, and others~\cite{chen2017survey}, ERC presents additional challenges as it requires the ability to model emotional dynamics governed by self- and inter-speaker influences at play~\cite{DBLP:conf/naacl/HazarikaPZCMZ18}. Further complications arise due to the limited availability of annotated data --- especially in multimodal ERC --- and the variability in annotations owing to the subjectivity of annotators in interpreting emotions. 


In this work, we focus on these issues by investigating a framework of sequential inductive \textit{transfer learning} (TL)~\cite{pan2010survey}. In particular, we attempt to transfer contextual affective information from a generative conversation modeling task to ERC. We name this framework TL-ERC.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{images/controlling_vars.pdf}
	\caption{\footnotesize{Dyadic conversation --- between person X and Y --- are governed by interactions between several latent factors. Emotions are a crucial component in this generative process. In the illustration,  represents the personality of the speaker;  represents speaker-state;  denotes the intent of the speaker;  refers to the speaker's emotional state, and  refers to the observed utterance. Speaker personality and the topic always condition upon the variables. At turn , the speaker conceives several pragmatic concepts such as argumentation logic, viewpoint, and inter-personal relationship - which we collectively represent using the speaker-state ~\cite{hovy1987generating}. Next, the intent  of the speaker gets formulated based on the current speaker-state and previous intent of the same speaker (at ). These two factors influence the emotional feeling of the speaker, which finally manifests as the spoken utterance~\cite{poria2019emotion}. }}
	\label{fig:controlling_vars}
\end{figure}

\textit{But why should generative modeling of conversations acquire knowledge on emotional dynamics?} To answer this question, we first observe the role of emotions in conversations. Several works in the literature have indicated that emotional goals and influences act as latent controllers in dialogues~\cite{weigand2017emotions,sidnell2012handbook}. \citet{poria2019emotion} demonstrated the interplay of several factors, such as the topic of the conversation, speakers' personality, argumentation-logic, viewpoint, and intent, which modulate the emotional state of the speaker and finally lead to an utterance.~\cref{fig:controlling_vars} illustrates these dependencies, which elaborate emotional factor as a critical latent state in the overall generative process of dialogues.

The interactions between these latent factors lead to diverse emotional dynamics within the conversations.~\cref{fig:examples} provides some examples demonstrating such patterns. In the figure, conversation (a) illustrates the presence of \textit{emotional inertia}~\cite{koval2012changing} which occurs though self-influences in emotional states. The character \textit{Snorri} maintains a frustrated emotional state by not being affected/influenced by the other speaker. Whereas, conversation (b) and (c) demonstrate the role of inter-speaker influences in emotional transitions across turns. In (b), the character \textit{John} is triggered for an \textit{emotional shift} due to influences based on his counterpart's responses, while (c) demonstrates the effect of \textit{mirroring}~\cite{navarretta2016mirroring} which often arises due to topical agreement between speakers. All these examples demonstrate the presence of such emotional dynamics that are not just inherent in the conversations but also help shape them up~\cite{poria2019emotion}.



To model such conversations, a generator would require the ability to 1) interpret latent emotions from its contextual turns and 2) model the complex dynamics governing them. In addition, it would also need to interpret other factors such as topic of the conversations, speaker personalities, intents, etc. Such a model would then be a \textit{perfect} dialogue generator. We illustrate this in \cref{fig:perfect_dialgoue_gen}, where the model generating utterance  would require to understand the emotions of the context arising from the utterances  and so on. Thereby, we hypothesize that a trained dialogue generator would possess the ability to model implicit affective patterns across a conversation~\cite{shimizu2018pretraining}. Consequently, we propose a framework that uses TL to transfer this affective knowledge into our target discriminative task, i.e., ERC.


\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{images/cornell_examples}
	\caption{\footnotesize{Samples from \textit{Cornell Movie Dialog Corpus}~\cite{danescu2011chameleons}. The examples demonstrate various kinds of emotional influences, such as \textit{emotional inertia}, \textit{mirroring}, etc., that manifest in natural conversations. }}
	\label{fig:examples}
\end{figure*}


In our approach, we first pre-train a hierarchical generative dialogue model on the \textit{source task} of conversation modeling. Being an unsupervised (or self-supervised) task, conversation modeling typically benefits from a large amount of data in the form of multi-turn chats. Next, we \textit{adapt} our model to the \textit{target task} (ERC) by transferring the inter-sentence contextual parameters~\footnote{In this paper, \textit{context} refers to the inter-sentential context in conversations, i.e. the sequential information acquired from utterances of speakers in a conversation.} from the trained source model. For sentence encoding, we choose the BERT model~\cite{devlin2018bert}, which is pre-trained on masked language modeling and next sentence prediction objectives.

Although we acknowledge that training a \textit{perfect} dialogue generator is presently challenging, we demonstrate that benefits can be observed even with a popular baseline generator. In the bigger picture, our approach can enable the co-evolution of both generative and discriminative models for the tasks mentioned above. This is possible since improving an emotional classifier using a dialogue model can, in turn, be utilized to enhance dialogue models with emotional intelligence further, leading to an iterative cycle of improvements for both the applications.

Overall, our contributions are summarized as follows:

\begin{itemize}
	\item We propose \textit{TL-ERC}, which pre-trains a hierarchical generative dialogue model on multi-turn conversations (\textit{source}) and subsequently transfers affective knowledge to the \textit{target} task of ERC. Despite the active role of TL in providing state-of-the-art token and sentence encoders, its use in leveraging multi-turn contextual knowledge --- across utterances --- has been mostly unexplored. Our work stands as one of the first in this direction.
    \item Through our experiments, we observe the promising effects of using these pre-trained weights. Our models, initialized with the acquired knowledge, converge faster compared to randomly initialized counterparts and also demonstrate robust performance in limited training-data scenarios.
    \item We identify various challenges observed in using TL for ERC. These points raise essential research questions and provide a roadmap for future research in this topic.
\end{itemize}


\begin{figure}[t]
	\centering
	\includegraphics[width=.5\linewidth]{images/perfect_dialogue_gen}
	\caption{\footnotesize{The figure illustrates how a perfect dialogue generator requires emotional understanding from its context -- a transferrable knowledge into ERC.}}
	\label{fig:perfect_dialgoue_gen}
\end{figure}

In the remaining paper, \cref{sec:related_works} first discusses the works in the literature related to our task and our approach. Next, \cref{sec:model_design} provides information on the TL setup along with details on the design of the framework. Experimental details are mentioned in \cref{sec:experimental_setup}; results and extensive analyses are provided in \cref{sec:results}.~\cref{sec:challenges} provides some challenges observed in the proposed framework, casting light for future research efforts. Finally, \cref{sec:conclusion} concludes the paper.







\section{Related Works} \label{sec:related_works}

We proceed to discuss the use of transfer learning by the available literature in NLP. First, we enlist some of the famous works that have benefited from TL, and then we focus on works that attempt to frame TL in the context-level hierarchy. Next, we look at recent works on emotion/sentiment analysis, including works that have employed TL. Finally, we attempt to position our contribution amidst the latest developments in ERC.

\subsection{Transfer Learning in NLP}

Transfer learning has played a critical role in the success of modern-day NLP systems. As a matter of fact, the key milestones in the recent history of NLP are provided by works using TL. NLP has particularly benefited by inductive TL, where unlabelled data is utilized to leverage knowledge for labeled downstream tasks. Early works, such as~\citet{DBLP:journals/jmlr/AndoZ05}, introduced this concept, which was heavily adopted by the community and has shown tremendous success ever since~\cite{DBLP:conf/naacl/RuderPSW19}.

Modern breakthroughs, such as neural word embeddings, followed similar modeling by utilizing unlabeled textual data to learn the embeddings~\cite{mikolov2013distributed}. Of late, there has been a significant interest in using language models (LMs) to learn contextual embeddings~\cite{mccann2017learned,peters2018deep}. TL through LM pre-training has also provided state-of-the-art text classifiers with high quality sentence encoders~\cite{dai2015semi, devlin2018bert,DBLP:journals/corr/abs-1906-08237}. Consequently, several works have explored improving this framework by either modifying the LM pre-training approach or weight adaptation in the downstream tasks~\cite{howard2018universal,DBLP:journals/corr/abs-1907-11692}.

\paragraph{Context-level Transfer Learning}

Availability for works that explore TL for inter-sentence or sequential learning is limited. Some of these include sentence-level sequence tagging tasks~\cite{chen2019transfer} or inter-sentence supervised tasks such as query matching in conversations~\cite{qiu2018transfer}, next sentence prediction~\cite{devlin2018bert}, etc. Recent works that address the topic of pre-training sentence representations or multi-turn conversations follow either a retrieval-based or a generative strategy. For the former, strategies include contrastive sentence selection (ToD-BERT~\cite{DBLP:journals/corr/abs-2004-06871}, ConveRT~\cite{DBLP:journals/corr/abs-1911-03688}), sentence ordering (ALBERT~\cite{DBLP:conf/iclr/LanCGGSS20}), and semantical sentence matching (Sentence-BERT~\cite{DBLP:conf/emnlp/ReimersG19}) objectives. Whereas, generative models attempt to learn a probabilistic model for the conversations directly. DialoGPT~\cite{DBLP:journals/corr/abs-1911-00536} is a recently proposed model that proposes a generative model based on the GPT architecture~\cite{radfordlanguage}. Our pre-training model is similar in spirit to DialoGPT. However, we do not flatten the conversation and instead opt for a hierarchical conversation model. This also suits our downstream task of conversational emotion recognition. Additionally, we analyze the joint pre-training of full conversations in a self-supervised setting and attempt to observe its efficacy in transferring affective knowledge.

\subsection{Affect Analysis}

Affect, in particular emotions, are an integral part of human life and modulate our day-to-day behavior and activities~\cite{DBLP:journals/cim/CambriaPHL19}. The interest in understanding emotions is multi-disciplinary and covers a long history of research. The importance of modeling emotions has multiple benefits across applications such as e-learning~\cite{DBLP:journals/jnca/ImaniM19}, human-computer interaction~\cite{DBLP:conf/interspeech/LiscombeRH05}, user profiling~\cite{DBLP:series/lncs/SchiaffinoA09}, etc.

From a computational perspective, emotions are typically studied across various media formats, covering applications such as facial emotion recognition~\cite{DBLP:journals/corr/abs-1804-08348,DBLP:journals/ijon/WangPDZ18}, emotions in speech~\cite{DBLP:conf/webist/DrakopoulosPSP19,DBLP:journals/air/AnagnostopoulosIG15}, or multimodal emotion recognition~\cite{DBLP:series/lncs/MarechalMTPBAW19}. In text-based applications, machine learning has played a crucial role in mining emotions~\cite{DBLP:conf/naacl/AlmRS05}. Earlier approaches designed hand-crafted features that included emotional keyword spotting~\cite{DBLP:conf/sac/StrapparavaM08}, affect-based lexical resources (WordNet-Affect~\cite{DBLP:conf/lrec/StrapparavaV04}, SentiWordNet~\cite{DBLP:conf/lrec/Esuli006}), and distant supervision via hashtags~\cite{DBLP:conf/socialcom/0002CTS12}. In the present deep-learning era, 

In the present deep learning era, approaches have diverged from hand-crafted features and moved towards automated feature learning. Modern approaches consider advanced neural architectures, such as convolutional networks~\cite{choi-etal-2018-convolutional}, recurrent networks~\cite{DBLP:journals/corr/ChernykhSP17}, and attention mechanisms~\cite{DBLP:conf/icassp/MirsamadiBZ17} for emotion detection. Recent times have also seen approaches that address practical scenarios such as domain awareness~\cite{DBLP:journals/cim/Dragoni19}, and utilize alternate training strategies, such as adversarial approaches~\cite{DBLP:journals/cim/HanZS19}. Complementary to these issues, we address data scarcity issues in ERC and leverage transfer learning for the same. We discuss the related works aligned to these topics next.


\paragraph{Transfer Learning for Affect}

TL for affective analysis has gained momentum in recent years, with several works adopting TL-based approaches for their respective tasks. These works leverage diverse source tasks, such as, sentiment/emotion analysis in text~\cite{yu2018improving,DBLP:conf/semeval/Daval-FrerotBM18,bouchekif2019epita}, large-scale image classification in vision~\cite{ng2015deep}, sparse auto-encoding in speech~\cite{deng2013sparse}, etc. \citeauthor{DBLP:conf/emnlp/FelboMSRL17} utilize emojis present in online platforms to pre-train models and transfer knowledge for emotion recognition. Using layer-wise fine-tuning, they also transfer knowledge into related tasks of sarcasm and sentiment detection. A similar approach is taken by~\citeauthor{DBLP:conf/semeval/Daval-FrerotBM18}. Similar to these works, our approach also leverages TL for knowledge transfer. However, our task is in a sequential setting at the conversational level. To the best of our knowledge, our work is one of the first that explores TL in ERC and utilizes generative conversation modeling as a pre-training objective.
    
\paragraph{Emotion Recognition in Conversations} 

ERC is an emerging sub-field of affective computing and is developing into an active area of research. Current works try to model contextual relationships amongst utterances in a supervised fashion to model the implicit emotional dynamics. Strategies include modeling speaker-based dependencies using recurrent neural networks~\cite{DBLP:conf/semeval/Gonzalez-Garduno19,DBLP:conf/naacl/JiaoYKL19}, memory networks~\cite{DBLP:conf/naacl/HazarikaPZCMZ18,hazarika2018icon}, graph neural networks~\cite{DBLP:journals/corr/abs-1908-11540,DBLP:conf/ijcai/ZhangWSLZZ19}, quantum-inspired networks~\cite{DBLP:conf/ijcai/ZhangL0ZW19}, amongst others. Some of these works also explore challenges such as multi-speaker modeling~\cite{DBLP:conf/aaai/MajumderPHMGC19}, multimodal processing~\cite{hazarika2018icon}, and knowledge infusion~\cite{DBLP:journals/corr/abs-1909-10681}. BERT-based sentence encoding has also been heavily adopted by the latest works in this area~\cite{DBLP:conf/semeval/ChatterjeeNJA19}. Works like EmotionX-IDEA~\cite{DBLP:journals/corr/abs-1908-06264} and PT-Code~\cite{DBLP:journals/corr/abs-1910-08916}, developed concurrently to ours, follow a similar vein by transferring emotional knowledge from BERT pre-training. However, in these works, either the conversations are limited to utterance-reply pairs or follow a contrastive utterance retrieval objective. Our work, in contrast, pre-trains a whole conversation jointly using a hierarchical generative model. Overall, we find that there is a dearth of works that consider scarcity issues for annotated data and leverage TL. Our work strives to fill this gap by providing a systematic study for TL in ERC.

\section{Methodology} \label{sec:model_design}


\begin{figure*}[t]
	\centering
	\includegraphics[width=.8\linewidth]{images/framework}
	\caption{\footnotesize{Proposed framework for ERC using TL parameters. The model on the left is a conversational response generator which is used as a pre-trained model. The parameters are transferred to the target model as shown on the right side.}}
	\label{fig:framework}
\end{figure*}


Our proposed framework, TL-ERC, is summarized in \cref{fig:framework}. First, we define the source generative model trained as a dialogue generator, followed by a description of the target model, which performs hierarchical context encoding --- for the task of ERC --- using BERT-based sentence encoders and learned context weights from the source model.



\subsection{Source: Generative Conversation Modeling} To perform the generative task of conversation modeling, we use the \textit{Hierarchical Recurrent Encoder-Decoder} (HRED) architecture~\cite{serban2016building}. HRED is a classic framework for seq2seq conversational response generation that models conversations in a hierarchical fashion using three sequential components: \textit{encoder} recurrent neural networks (RNNs) for sentence encoding, \textit{context} RNNs for modeling the conversational context across sentences, and \textit{decoder} RNNs for generating the response sentence. 

For a given conversation context with sentences , ... , HRED generates the response  as follows: 

	\begin{enumerate}[leftmargin=*]
		\item \textbf{Sentence Encoder:} It encodes each sentence in the context using an \textit{encoder RNN}, such that,
			
		
		\item \textbf{Context Encoder:} The sentence representations are then fed into a \textit{context RNN} that models the conversational context until time step  as 
			
		
		\item \textbf{Sentence Decoder:} Finally, an auto-regressive \textit{decoder RNN} generates sentence  conditioned on , i.e., 
			
			
	\end{enumerate}

With the  conversation being a sequence of utterances , HRED trains all the conversations in the dataset together by using the maximum likelihood estimation objective .



The HRED model provides the possibility to introduce multiple complexities in the form of multi-layer RNNs and other novel encoding strategies. In this work, we choose to experiment with the original version of the architecture with single-layer components so that we can analyze the hypothesis without unwanted contribution from the added complexities. In our source model,  can be any RNN function, which we model using the bi-directional \textit{Gated Recurrent Unit} (GRU) variant~\cite{cho2014learning} to encode each sentence. We call the parameters associated with this GRU function as . For both the \textit{context RNN} () and \textit{decoder RNN}, we use uni-directional GRUs --- with parameters  and , respectively --- and complement the decoder with beam-decoding for generation~\footnote{Model implementations are adapted from \protect\url{https://github.com/ctr4si/}}.




\subsection{Target: Emotion Recognition in Conversations}

The input for this task is also a conversation  with constituent utterances . Each  is associated with an emotion label . We adopt a setup similar to the three components described for the source task, as in ~\citet{poria2017context}. However, the  in this setup is replaced by a discriminative mapping to the label space instead of a generative network. Below, we describe the different initialization parameters that we consider for the first two stages of the network:

\subsubsection{Sentence Encoding} 
To encode each utterance in the conversation, we consider the state-of-the-art universal sentence encoder BERT~\cite{devlin2018bert}, with its parameters represented as . We choose BERT over the HRED sentence encoder () as it provides better performance (see~\cref{tab:glove_vs_bert}). Also, BERT includes the task of next sentence prediction as one of its training objectives which aligns with the inter-sentence level of abstraction that we consider in this work. 

We choose the \textit{BERT-base uncased} pre-trained model as our sentence encoder~\footnote{\url{https://github.com/huggingface/pytorch-pretrained-BERT}}. Though this model contains  transformer layers, to limit the total number of parameters in our model, we restrict to the first 4 transformer layers. To get a sentential representation, we use the hidden vectors of the first token [CLS] across the considered transformer layers (see~\citet{devlin2018bert}) and mean-pool them to get the final sentence representation.

\subsubsection{Context Encoding} 
We use a similar context encoder RNN as the source HRED model with the option to transfer the learned parameters . For input sentence representation  provided by the \textit{encoder RNN}, the \textit{context RNN} transforms it as follows:
	
	Here,  are parameters for the GRU function and  are additional parameters of a dense layer. For our setup, adhering to size considerations, we consider our transfer parameters to be .

\subsubsection{Classification} 

For each turn in the conversation, the output from the context RNN is projected to the label-space, which provides the predicted emotion for the associated utterance. Similar to HRED, we train for all the utterances in the conversation together using the standard \textit{Cross Entropy} loss. For regression targets, we utilize the \textit{Mean Square Error (MSE)} loss, instead.


\section{Experimental Setup} \label{sec:experimental_setup}

In this section, we define the experimental setup followed in this work.
First, we detail the datasets that we utilize and mention their properties. Further, we provide information on the metrics used for evaluation, the training setup, and the model variants considered to test our hypothesis.

\subsection{Datasets}

\subsubsection{Source Task} 

For pre-training with the source task of conversation modeling, we consider two large-scale benchmark datasets: 
	
	\begin{itemize}
		\item  \textit{Cornell Movie Dialog Corpus}~\cite{danescu2011chameleons} is a popular collection of fictional conversations extracted from movie scripts. In this dataset, conversations are sampled from a diverse set of  movies leading to over k dialogues. 
        \item  \textit{Ubuntu Dialog Corpus}~\cite{lowe2015ubuntu} is a larger corpus with around 1 million dialogues, which, like the Cornell corpus, comprises of unstructured multi-turn dialogues based on Ubuntu chat logs (Internet Relay Chat).
	\end{itemize}
	
	Both datasets contain dyadic, i.e. two-party conversations. For brevity, throughout the paper, we mention these datasets as Cornell and Ubuntu, respectively. The data splits for training are created as per ~\citet{park2018hierarchical}.

\subsubsection{Target Task} 

For the target task of ERC, we experiment with three datasets popular in this area of research:
	
	\begin{itemize}
		\item Primarily, we consider the textual modality of a small-sized multimodal dataset \textit{IEMOCAP}~\cite{busso2008iemocap} consisting of dyadic conversations between 10 speakers. Each pair is assigned one of many diverse conversational scenarios, with a total of five sessions across the dataset. Each conversational video is segmented into utterances and annotated with the following emotion labels: \textit{anger, happiness, sadness, neutral, excitement,} and \textit{frustration}. Split creating scheme is based on~\citet{hazarika2018icon}.
		\item We also analyze results on a moderately-sized emotional dialogue dataset \textit{DailyDialog}~\cite{li2017dailydialog} with labeled emotions: \textit{anger, happiness, sadness, surprise, fear disgust} and \textit{no\_emotion}. Unlike spoken utterances in IEMOCAP, the conversations are chat-based based on daily life topics. For creating the splits, we follow the original split details provided by~\citet{li2017dailydialog}.
		\item Finally, we choose a regression-based dataset \textit{SEMAINE}, which is a video-based corpus of human-agent emotional interactions. We use the split configuration detailed in AVEC 2012's \textit{fully continuous sub-challenge}~\cite{schuller2012avec} for the prediction of affective dimensions: \textit{valence}, \textit{arousal}, \textit{power}, and \textit{expectancy}. Annotation configuration is based on~\citet{hazarika2018icon}.
	\end{itemize}
	
	\cref{tab:dataset_stats} provides the sizes along with split distributions for the above-mentioned datasets. For both IEMOCAP and SEMAINE, we generate the validation sets by random-sampling of 20\% dialogue videos from the training sets. The class distribution for the categorical emotions in IEMOCAP and DailyDialog are presented in \cref{tab:class_dist}. From the table, IEMOCAP is observed a fairly balanced dataset whereas DailyDialog is highly skewed towards sentences with no emotion. As such, we decide upon different metrics for each dataset as discussed next.


\begin{table}
\centering
\makebox[0pt][c]{\parbox{\textwidth}{\begin{minipage}[t]{0.48\hsize}\centering
        
        \resizebox{0.95\linewidth}{!}{
    	\begin{tabular}{|llc|ccc|}
    		\hline
    	    &\multicolumn{2}{c|}{\multirow{2}{*}{Dataset}} & \multicolumn{3}{c|}{Dataset splits} \\
    		& && \small{train} & \small{validation} & \small{test} \\
    		\hline
    		\hline
    		\multirow{4}{*}{\rotatebox{90}{\small{Source}}}&\multirow{2}{*}{Cornell} &\small{\#D}& 66,477 & 8,310 & 8,310 \\
    		&&\small{\#U}& 244,030 & 30,436 & 30,247 \\
    		&\multirow{2}{*}{Ubuntu} &\small{\#D}& 898,142 & 18,920 & 19,560 \\
    		& &\small{\#U}&  6,893,060 & 135,747 & 139,775 \\
    		\hline
    		\multirow{4}{*}{\rotatebox{90}{\small{Target }}}&\multirow{2}{*}{IEMOCAP} &\small{\#D}& \multicolumn{2}{c}{120} & 31 \\
    		& &\small{\#U}& \multicolumn{2}{c}{5810} & 1,623 \\
    		\multirow{4}{*}{\rotatebox{90}{\small{Target }}}&\multirow{2}{*}{SEMAINE} &\small{\#D}& \multicolumn{2}{c}{58} & 22 \\
    		& &\small{\#U}& \multicolumn{2}{c}{4386} & 1,430 \\
    		&\multirow{2}{*}{Dailydialog} &\small{\#D}& 11,118 & 1,000 & 1,000 \\
    		&&\small{\#U}& 87,170 & 7,740 & 8,069 \\
    		\hline
    	\end{tabular}}
    	\caption{\footnotesize{Table illustrates the sizes of the datasets used in this work. \#D represents the number of dialogues whereas \#U represents the total number of constituting utterances.}}
    	\label{tab:dataset_stats}
        
        
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\hsize}\centering
        
        \resizebox{0.95\linewidth}{!}{

    	\begin{tabular}{|l|c:c|c:c:c|}
    		\hline
    	    &\multicolumn{2}{c|}{\textbf{Iemocap}} & \multicolumn{3}{c|}{\textbf{Dailydialog}}\\
    		&\small{train/val} & \small{test} & \small{train} & \small{val} & \small{test} \\
    		\hline
    		\hline
    		hap & 504 & 144 & 11182  & 684 & 1019\\
    		sad & 839 & 245 & 969 & 79  & 102\\
    		neu & 1324 & 384 & 72143 & 7108 & 6321\\
    		ang & 933 & 170 & 827 & 77 & 118 \\
    		exc & 742 & 299 & - & - & - \\
    		frus & 1468 & 381 & - & - & - \\
    		surp & - & -  & 1600  & 107 & 116 \\
    		fear & - & - & 146 &  11 & 17 \\
    		disg & - & - & 303  & 3 & 47 \\
    		\hline
    	\end{tabular}
    	}
    	\caption{\footnotesize{Category-wise distribution of utterances. \textit{hap}: happiness; \textit{neu}: neutral or no emotion; \textit{ang}: anger; \textit{exc}: excitement; \textit{frus}: frustration; \textit{surp}: surprise; \textit{disg}: disgust.}}
    	\label{tab:class_dist}
    	
    \end{minipage}
}}
\end{table}








\subsubsection{Metrics} We choose the pre-training weights from the source task based on the best validation perplexity score~\cite{park2018hierarchical}. For ERC, we use \textit{weighted-F-score} metric for the classification tasks on IEMOCAP and DailyDialog. For DailyDialog, we remove \textit{no\_emotion} class from the F-score calculations due to its high majority (/ occupancy in training/testing set) which hinders evaluation of other classes\footnote{Evaluation strategy adapted from Semeval 2019 ERC task: \protect\url{www.humanizing-ai.com/emocontext.html}}. For the regression task on SEMAINE, we take the Pearson correlation coefficient () as its metric.

We also provide the average \textit{best epoch} (BE) on which the least validation losses --- across the multiple runs --- are observed, and the testing evaluations are performed. A lower BE represents the model's ability to reach optimum performance in lesser training epochs.



\subsection{Model Size}
We consider two versions of the source generative model: \textbf{HRED-small} and \textbf{HRED-large} with  and -dimensional hidden state sizes, respectively. While testing the performance of both the models on the IEMOCAP dataset, we find the context weights from \textit{HRED-small} (Cornell dataset) to provide better performance on average ( F-score ) over \textit{HRED-large} ( F-score). Following this observation, and also to avoid over-fitting on the small target datasets due to increased parameters, we choose the \textit{HRED-small} model as the source task model for our TL procedure.

\subsection{Model Variants and Baselines} \label{sec:baselines}

\begingroup
\renewcommand{\arraystretch}{1.3} \begin{table}[h]
	\centering
	\resizebox{0.9\linewidth}{!}{

	\begin{tabular}{|c|c:c|c|}
		\hline
	    \multirow{2}{*}{Variant}&\multicolumn{2}{c|}{\small{\textbf{Initial Weight}}} & \multirow{2}{*}{\small{\textit{Model Description}}}  \\
		& \multicolumn{1}{c:}{sent} & cxt & \\ \hline \hline
		\multirow{2}{*}{} & \multirow{2}{*}{-} & \multirow{2}{*}{-} & \small{Sentence encoders -- \textit{randomly} initialized.} \\
		& & &\small{Context encoders -- \textit{randomly} initialized.}\\ 
		\hline
		
		\multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{-} & \small{Sentence encoders -- BERT parameters.} \\
		& & & \small{Context encoders -- \textit{randomly} initialized.}\\ 
		\hline
		\multirow{4}{*}{} & \multirow{4}{*}{} & \multirow{4}{*}{} & \small{\textbf{TL-ERC}}   \\
		& & & \small{Sentence encoders -- BERT parameters.}\\
		& & & \small{Context encoders -- initialized from generative models}\\
		& & & \small{pre-trained on Ubuntu/Cornell corpus.}\\
		\hline
		
	\end{tabular}
	}
	\caption{\footnotesize{Variants of the model used in the experiments. Variant (3) is the proposed TL-ERC model.}}
	\label{tab:model_variants}
\end{table}
\endgroup




The primary goal of this paper is to analyze the effect of TL at the conversation level for ERC. For this, we experiment on different variants of our model based on the parameter initialization procedure. We provide a summary of these variants in~\cref{tab:model_variants}. In the table, \textit{Variant 1} is the model with randomly initialized parameters. In \textit{Variant 2}, we replace the sentence encoders with the BERT model including its original pre-trained parameters. Finally, in \textit{Variant 3}, in addition to BERT sentence encoders, we also initialize the context-RNN parameters learned from the source task. Different results and analyses amongst these variants are provided in~\cref{sec:results}.

Next, to compare our model with the existing literature, we select some prior state-of-the-art models evaluated on the target datasets:

\begin{itemize}[leftmargin=*]
    \item CNN~\cite{DBLP:conf/emnlp/Kim14} extracts textual features based on Convolutional Neural Networks (CNN). This is a non-contextual model, which evaluates each utterance in a conversation independently.
    
    \item Memnet~\cite{DBLP:conf/nips/SukhbaatarSWF15} assigns dedicated memory for each historical utterance and performs multi-hop inference on them to get final representations for emotion classification.
    
    \item c-LSTM~\cite{poria2017context} is a popular model which is similar to our target model. It employs a bi-directional LSTM~\cite{DBLP:journals/neco/HochreiterS97} to capture inter-utterance dependencies.
    
    \item c-LSTM+Att~\cite{DBLP:conf/icdm/PoriaCHMZM17} enhances the c-LSTM model with inter-modality and inter-utterance attention mechanisms.
    
    \item CMN~\cite{DBLP:conf/naacl/HazarikaPZCMZ18}, the Conversational Memory Network, is an extension to the Memnet model which allots separate memories to both speakers in a dyadic conversational exchange.
    
    \item DialogueRNN~\cite{DBLP:conf/aaai/MajumderPHMGC19} is a strong state-of-the-art baseline which employs three stages of recurrent units comprising global, speaker-state, and emotional units. The global RNN models the conversational context, speaker-state RNN models the individual speaker-states, and emotion RNN models the final emotional representations used for classification. For a fair comparison with our model, we chose the basic version of DialogueRNN without bi-directional RNNs and inter-utterance attention mechanisms.
\end{itemize}

\noindent Results on these baselines are provided in~\cref{sec:baseline_results}.


\subsection{Training Criteria}

\paragraph{Hyper-parameter search} For each target dataset-model combination, we perform grid-search to select the appropriate hyper-parameters. In the search procedure, we keep the model architecture constant but vary learning rate (1e-3, 1e-4, and 1e-5), optimizer (Adam, RMSprop~\cite{Tieleman2012}), batch size (2-40 videos/batch), and dropout (\{0.0, 0.5\}. BERT-parameters contain dropout of 0.1 as in \citet{devlin2018bert}). For a particular dataset-model pair, the final hyper-parameter configuration is chosen based on the best performance on the respective validation set. In the case of negligent difference between the combinations, we use the Adam optimizer~\cite{kingma2014adam} as the default variant with  and learning rate .

\paragraph{Inference} 
We train our models on each target dataset for multiple runs (10:IEMOCAP, 5:DailyDialog, 5:SEMAINE). In each run,
the training proceeds with an \textit{early stopping} criterion of patience 10. During this training loop, the parameters with the least validation loss are finally chosen for the testing-set inference and evaluation.




\begingroup
\renewcommand{\arraystretch}{1.3} \begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{

	\begin{tabular}{|c|cc|cc|cc|cc|cc|}
		\hline
	    \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Variant}}} & & & \multicolumn{8}{c|}{Dataset: \textbf{IEMOCAP}} \\
	    &\multicolumn{2}{c|}{\small{\textbf{Initial Weights}}}  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}\\
		& sent & cxt & \small{F-Score} & \small{BE} & \small{F-Score} & \small{BE} & \small{F-Score} & \small{BE} & \small{F-Score} & \small{BE} \\
		\hline
		\hline
		(1) & - & -  & 23.2    & 48.4 & 41.6  & 72.5  & 48.4  & 75.1 & 53.8   & 13.8  \\ \cline{1-3}
		(2) & \multirow{1}{*}{} & - & 32.4  & \textbf{11.0} & 41.9  & \textbf{8.0} & 49.2  & \textbf{6.3} & 55.1  & \textbf{5.0} \\ 
		\cline{1-3}
		\multirow{2}{*}{(3)} & \multirow{2}{*}{} &   & 35.7  & 14.2 &  45.9  & 11.2  & \textbf{53.1}   & 7.8  & \textbf{58.8 }  & 5.4  \\
		 & &   & \textbf{36.3 } & 17.0 & \textbf{46.0}  & 11.2 & 50.9  & 8.2 & 58.5  & \textbf{5.0}\\
		\hline
	\end{tabular}
	}
	\caption{\footnotesize{IEMOCAP results. Metric: Weighted-Fscore averaged over 10 random runs. BE = Best Epoch. Results span across different amount of available training data. Validation and testing splits are fixed across configurations.  represents significant difference with  over randomly initialized model as per two-tailed Wilcoxon rank sum hypothesis test~\cite{nachar2008mann}.}}
	\label{tab:iemocap_results}
\end{table*}
\endgroup


\begingroup
\renewcommand{\arraystretch}{1.3} \begin{table}[t]
	\centering
	\resizebox{0.7\linewidth}{!}{

	\begin{tabular}{|c|cc|cc|cc|}
		\hline
	    \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Variant}}} & & & \multicolumn{4}{c|}{Dataset: \textbf{DailyDialog}} \\
	    & \multicolumn{2}{c|}{\small{\textbf{Initial Weights}}} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}\\
		& sent & cxt &  \small{F-score} &\small{BE} & \small{F-score} &\small{BE}  \\
		\hline
		\hline
		(1) & - & -  &  33.5    & 12.3  &  45.3  & 7.9  \\ \cline{1-3}
		(2) & \multirow{1}{*}{}  & - &  37.5    &\textbf{ 2.6} &  47.4  & \textbf{2.4}\\ \cline{1-3}
		\multirow{2}{*}{(3)} & \multirow{2}{*}{} &   & 37.7    & 3.1 & 47.1   & \textbf{2.4} \\
	    & &   & \textbf{38.5}   & 3.2 &  \textbf{48.0}  & \textbf{2.4} \\
		\hline
	\end{tabular}
	}
	\caption{\footnotesize{DailyDialog results. Metric: Weighted-Fscore averaged over 5 random runs. BE = Best Epoch.  represents significant difference with  over random initialized model as per two-tailed Wilcoxon rank sum hypothesis test~\cite{nachar2008mann}.}}
	\label{tab:dailydialog_results}
\end{table}
\endgroup

\begingroup
\renewcommand{\arraystretch}{1.3} \begin{table}[t]
	\centering
	\resizebox{0.8\linewidth}{!}{

	\begin{tabular}{|c|cc|cc|cc|cc|cc|}
		\hline
	    \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Variant}}} & \multicolumn{2}{c|}{\small{\textbf{Initial Weights}}} & \multicolumn{8}{c|}{Dataset: \textbf{SEMAINE}} \\
	    & & & \multicolumn{2}{c}{DV} &  \multicolumn{2}{c}{DA}  &   \multicolumn{2}{c}{DP}  &   \multicolumn{2}{c|}{DE}\\
		& sent & cxt &  \small{r} &\small{BE} &  \small{r} &\small{BE} &  \small{r} &\small{BE} &  \small{r} &\small{BE}\\
		\hline
		\hline
		(1) & - & -  & 0.14  & \textbf{4} & 0.27 & 6.2 & 0.18 & 12.8 & -0.03 & 287.4\\ \cline{1-3}
		(2) & \multirow{1}{*}{}  & - & 0.64  & 13.8 & 0.36  & 7.8 &  0.33  & 4.8 &  -0.03 &  23   \\ \cline{1-3}
	    \multirow{2}{*}{(3)} & \multirow{2}{*}{}  &   & \textbf{0.66}  & 10.2 & 0.41 & \textbf{6} & 0.34  & 3.8 &  -0.03 &   23   \\
	    & &   & 0.65  & 10.2 & \textbf{0.42} & 8.8 & \textbf{0.35} & \textbf{3.4} & -0.029  &\textbf{22.7}     \\
		\hline
	\end{tabular}
	}
	\caption{\footnotesize{SEMAINE results. Metric (r): Pearson correlation coefficients averaged over 5 random runs. DV = Valence, DA = Activation/Arousal, DP = Power, DE =Anticipation/Expectation.}}
	\label{tab:avec_results}
\end{table}
\endgroup




\section{Results and Analyses} \label{sec:results}

\cref{tab:iemocap_results} and \ref{tab:dailydialog_results} provide the performance results of ERC on classification datasets IEMOCAP and DailyDialog, respectively. In both the tables, we observe clear and statistically significant improvements of the models that use pre-trained weights over the randomly initialized variant. We see further improvements when context-modeling parameters from the source task () are transferred, indicating the benefit of using TL in this context-level hierarchy.

Similar trends are observed in the regression task based on the SEMAINE corpus (see~\cref{tab:avec_results}). For \textit{valence}, \textit{arousal}, and \textit{power} dimensions, the improvement is significant. For \textit{expectation}, the performance is marginally better but at a much lesser BE, indicating faster generalization.

In the following sections, we take a closer look at various aspects of our approach that include checking robustness towards limited-data scenarios, generalization time, and questioning design choices. We also provide additional analyses that probe the existence of data-split bias, domain influence, and effect of fine-tuning strategies.


\subsection{Target Data Size}

Present approaches in ERC primarily adopt supervised learning strategies that demand a high amount of annotated data. However, the publicly available datasets in this field belong to the small-to-medium range in the spectrum of dataset sizes. For example, other applications of NLP have datasets of much larger sizes -- over  instances in SQuAD for Question Answering~\cite{DBLP:conf/acl/RajpurkarJL18}, over  instances in MNLI for language inference~\cite{N18-1101}, and so on. This constraint inhibits the true potential of systems trained on these datasets. As a result, approaches that provide higher performance in a limited training-data scenario tend to be highly desirable, particularly for ERC.

We design experiments to check the robustness of our models against such limited settings. To limit the amount of available training data, we create random subsets of the training dialogues while maintaining the original label-distribution. In both \cref{tab:iemocap_results} and \ref{tab:dailydialog_results}, we observe that the pre-trained models are significantly more robust against limited training resources compared to models trained from scratch.

\paragraph{Effect of bias in random splits}

We investigate the possibility of bias in the random splits, which aid in supporting our hypothesis. To eliminate this possibility, we further check if the improvement in our TL-based approach --- for the limited-data scenarios --- are triggered by such data-split bias. In other words, we pose the following question, \textit{if another training split is sampled from the original dataset, would our model provide similar improvements?} We provide evidence that this is indeed true.

\cref{tab:split_comparisons} presents the results where for  and  training-data setup, we sample  independent splits from the IEMOCAP dataset. As seen from the table, different splits provide different results, which is expected owing to the variances in the samples and their corresponding labels. However, the relative performance within each split follows similar trends of improvement for TL-based models. This observation nullifies the potential existence of bias in the reported results.


\subsection{Target Task's Training Time}

\begin{figure}[h]
    \centering
	\includegraphics[width=0.5\linewidth]{images/val_loss.pdf}
	\caption{\footnotesize{Validation loss across epochs in training for different weight-initialization settings on the IEMOCAP dataset. Part a) represents results when trained on  training data b)  training data split. For fair comparison, optimizer learning rates are fixed at 1e-4.}}
	\label{fig:val_loss}
\end{figure}

In all the configurations in \cref{tab:iemocap_results} and \ref{tab:dailydialog_results}, we observe that the presence of  weight initialization leads to faster convergence in terms of the best validation loss. \cref{fig:val_loss} demonstrates the trace of the validation loss on training data configurations of the IEMOCAP dataset. As observed, the pre-trained models achieve their best epoch in a significantly shorter time which indicates that the transferred weights are helping the model better guide to its optimal performance. 

\subsection{Encoder Initialization}

\cref{tab:glove_vs_bert} provides a comparative study between the performance of models initialized with HRED-based sentence encoders () versus the BERT encoders () that we use in our final networks. Results demonstrate that BERT provides better representations, which leads to better performance. Moreover, the positive effects of the context parameters are observed when coupled with the BERT encoders. This behavior indicates that the performance boosts provided by the context-encoders is contingent on the quality of sentence encoders. Observing this empirical evidence, we choose BERT-based sentence encoders in our final network.

















	




	







\begingroup
\renewcommand{\arraystretch}{1.3} \begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
	\begin{tabular}{|c|cc|c:c:c:c|c:c:c:c|}
		\hline
	    \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Variant}}}&&&\multicolumn{8}{c|}{Dataset: \textbf{IEMOCAP}} \\
	    &\multicolumn{2}{c|}{\small{\textbf{Initial Weight}}}  & \multicolumn{4}{c|}{}& \multicolumn{4}{c|}{}\\
		&\multirow{1}{*}{sent} & \multirow{1}{*}{cxt} & split & split & split & split & split & split & split & split \\ \cline{3-10}
		\hline
		\hline
		(1) &- & -  & 23.2  & 31.5  & 25.0  & 8.8  & 48.4  & 48.5  & 49.1  & 51.3  \\ \cline{1-3}
		(2) &\multirow{1}{*}{} & - &  32.4   &  31.6   & 30.5  & 23.65  & 49.2  & 49.0  & 48.8  & 51.4  \\ \cline{1-3}
		\multirow{2}{*}{(3)} & \multirow{2}{*}{} &  & 35.7  & 32.0  & \textbf{39.0}  & \textbf{24.90}  & \textbf{53.1}   & 53.2   & 52.9  & 54.2  \\
		& &  & \textbf{36.3}   & \textbf{34.2}  & 35.7  & 24.70  & 50.9   & \textbf{54.3}  & \textbf{ 53.5}  & \textbf{55.4}  \\
		\hline
		\multicolumn{6}{l}{\footnotesize{ primary split}}\\
	\end{tabular}
	}
	
	\caption{\footnotesize{Table to investigate if split randomness incurs bias in results. Comparisons are held between two limited training data scenarios comprising  and  available training data. For both the cases,  independent splits are sampled and compared against. Metric: Weighted-Fscore averaged over 10 random runs.}}

    \label{tab:split_comparisons}
\end{table}
\endgroup




\begin{table}[h]
	\centering
	\resizebox{0.5\linewidth}{!}{

	\begin{tabular}{|cc|c|c|}
		\hline
	    & & \multicolumn{2}{c|}{Dataset: \textbf{IEMOCAP}} \\
	    \multicolumn{2}{|c|}{\small{\textbf{Initial Weight}}} &  & \\
		sent & cxt &  \small{F-score} & \small{F-score}  \\
		\hline
		\hline
		- & -  &  23.2    &  53.8   \\ \hline
		\multirow{2}{*}{} & - & 26.3   &  54.9  \\ 
		 &   &  27.5  & 55.1  \\
		\hline
		\multirow{2}{*}{} & - & 24.6  & 53.2 \\ 
		 &   &  23.3  & 53.7  \\
		\hline
		\multirow{3}{*}{} & - &  32.4   &   55.1  \\
		 &   & 35.7  &  \textbf{58.8 }  \\
	     &   & \textbf{36.3 } &  58.5  \\ \hline
		
	\end{tabular}
	}
	\caption{\footnotesize{Table to analyze HRED encoder vs BERT. Metric: Weighted-Fscore averaged over 10 random runs. BE = Best Epoch (average). }}
	\label{tab:glove_vs_bert}
\end{table}









\subsection{Impact of Source Domain.}

\begin{figure}[h]
    \centering
	\includegraphics[width=0.6\linewidth]{images/emotion_color}
	\caption{a) Frequency of emotive words from source datasets: Cornell and Ubuntu. b) randomly sampled words from Cornell associated to mentioned emotions.}
	\label{fig:emotion_color}
\end{figure}

We investigate if the choice of source datasets incur any significant change in the results. First, we define an emotional profile for the source datasets and observe whether any correlation is found between their emotive content versus the performance boost achieved by pre-training on them.

To set up an emotional profile, we look at the respective vocabularies of both corpora. For each token, we check its association with any emotion by using the emotion-lexicon provided by~\citet{mohammad2013crowdsourcing}. The NRC Emotion Lexicon contains 6423 words belonging to emotion categories: \textit{fear, trust, anger, sadness, anticipation, joy, surprise}, and \textit{disgust}. It also assigns two broad categories: \textit{positive} and \textit{negative} to describe the type of connotation evoked by the words. We enumerate the frequency of each emotion category amongst the tokens of the source dataset's vocabulary. To compose the vocabulary of both the source datasets, we set a minimum frequency threshold of 5, which provides  and  unique tokens for Cornell and Ubuntu, respectively. Each of the unique tokens is then lemmatized~\footnote{\protect\url{https://www.nltk.org/_modules/nltk/stem/wordnet.html}} and cross-referenced with the lexicon, which provides  (Cornell) and  (Ubuntu) tokens with associated emotions.

\cref{fig:emotion_color} presents the emotional profiles,  which indicate that the Cornell dataset has a higher number of emotive tokens in its vocabulary. However, the results illustrated in \cref{tab:iemocap_results}, \ref{tab:dailydialog_results}, and \ref{tab:avec_results} \textit{do not} present any significant difference between the two sources. A possible reason for this behavior attributes to the fact that such emotional profile relies on surface emotions derived from the vocabularies. However, as per our hypothesis, response generation includes emotional understanding as a latent process. This reasoning leads us to believe that surface emotions need not necessarily correlate to performance increments. Rather, the quality of generation would include such properties intrinsically.











\begin{table}
\centering
\makebox[0pt][c]{\parbox{\textwidth}{\begin{minipage}[t]{0.38\hsize}\centering
        
        \resizebox{0.95\linewidth}{!}{

    	\begin{tabular}{|l|c|c|}
    		\hline
    	    \multicolumn{1}{|c|}{\small{Adapt Strategy}} &\multicolumn{1}{c|}{\textbf{Iemocap}} & \multicolumn{1}{c|}{\textbf{DD}}\\
    		\small{\textbf{Fixed weights}} &  \small{F-Score} & \small{F-Score}   \\
    		\hline
    		\hline
    		- & \textbf{58.5} &  \textbf{48.0} \\
    		 & 17.0 & 32.1 \\
    	      & 9.3 &  4.5 \\
    		\hline
    	\end{tabular}
    	}
    	\caption{\footnotesize{Average performance on ERC with pre-trained weights: . Note: DD here means DailyDialog.}}
    	\label{tab:freezing_results}
        
        
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.58\hsize}\centering
        
        \resizebox{0.95\linewidth}{!}{

    	\begin{tabular}{|l|c|c:c:c:c|}
    		\hline
    	    &\multicolumn{1}{c|}{\textbf{Iemocap}} & \multicolumn{4}{c|}{\textbf{SEMAINE}}\\
    	     &   & \small{DV} & \small{DA} & \small{DP} & \small{DE} \\
    		\small{\textbf{Models}} &  \small{F-Score} & \small{r} & \small{r} & \small{r} & \small{r} \\
    		\hline
    		\hline
    		CNN & 48.1 & -0.01 & 0.01 & -0.01 & 0.19 \\
    		Memnet & 55.1 & 0.16 & 0.24 & 0.23 & 0.05 \\
    		c-LSTM & 54.9 & 0.14 & 0.23 & 0.25 & -0.04 \\
    		c-LSTM + Att & 56.1 & 0.16 & 0.25 & 0.24 & 0.10 \\
    		CMN & 56.1 & 0.23 & 0.29 & 0.26 & -0.02 \\
    		DialogueRNN & \textbf{59.8} &  0.28 & 0.36 & 0.32 & \textbf{0.31} \\ 
    		\hline
    	    TL-ERC  & 58.8 & \textbf{0.66} & \textbf{0.42} & \textbf{0.35} & -0.02 \\
    		\hline
    	\end{tabular}
    	}
    	\caption{\footnotesize{Average performance of TL-ERC compared to previous state-of-the-art models.}}
    	\label{tab:previous_work}
    	
    \end{minipage}
}}
\end{table}


\subsection{Comparison with previous work.} \label{sec:baseline_results}

\cref{tab:previous_work} provides the results for various baselines detailed in~\cref{sec:baselines}. As seen, our proposed TL-ERC comfortably outperforms both non-contextual and contextual baselines. It achieves this without the aid of attention mechanisms that is used in c-LSTM + Att, multi-hop memory networks used in Memnet, and CMN. It also achieves competitive performance against DialogueRNN, which has three layers of inter-utterance recurrent layers, while TL-ERC has one. These trends indicate TL to be effective in our setup and provided promising directions for future research.


\section{Challenges} \label{sec:challenges}

In this section, we enlist the different challenges that we observed while experimenting with the proposed idea. These challenges provide roadmaps for further research on this topic to build better and robust systems.



\subsection{Adaptation Strategies}

We try two primary adaptation techniques used in inductive TL, \textit{freezed} or \textit{fine-tuned}. In the former setting, the borrowed weights are used for feature extraction, while in the latter,  we train the weights along with the other new parameters of the target task's model. Fine-tuning can also be performed using other techniques such as gradual unfreezing~\cite{peters2019tune}. In \cref{tab:freezing_results}, we experiment with freezing different amounts of transferred weights in our ERC model. We notice a degradation in performance with more frozen parameters. The datasets in ERC contain multi-class annotations with varying label distributions. With frozen parameters, our transferred model is unable to account for the label distribution and results in low recall for infrequent classes. We thus find the fine-tuning approach to be more effective in this setup.

However, fine-tuning all parameters also present higher susceptibility to over-fitting~\cite{howard2018universal}. We observe this trait in \cref{fig:val_loss}, where the validation loss shoots up at a faster rate than the random counterpart. Finding a fine-balance in this trade-off remains an open problem.

\subsection{Stability Issues}

In the results, we observe that the variability of the models across multiple runs (in terms of the standard error) is relatively higher for the proposed models as compared to randomly initialized weights. Though, on average, our models perform significantly better, there remains a scope for improvement to achieve more stable training.

\subsection{Variational Models} 

Many works utilize variational networks to model the uncertainties and variability in latent factors. For dialogue modeling, networks such as VHRED~\cite{SerbanSLCPCB17} incorporate such variational properties to model its latent processes. Emotional perception, in particular, has been argued to contain shades of multiple affective classes instead of a hard label assignment~\cite{DBLP:journals/taslp/MowerMN11}. We, thus, posit that variational dialogue models such as VHRED also hold the potential for improving affective knowledge transfer. 

We experiment on this concept by using VHRED as the source model. VHRED uses additional paramteres to model its prior latent state , which is then concatenated with  as follows:



As a result, our set of transferred parameters contain the additional parameters of MLP, included in . \cref{tab:vhred_results} presents the result of using VHRED parameters. Unfortunately, we do not find significant difference between the parameters from VHRED as opposed to HRED. However, the lack of degradation in the results promise possible future improvements in such designs.








\begin{table}
\centering
\makebox[0pt][c]{\parbox{\textwidth}{\begin{minipage}[t]{0.48\hsize}\centering
        
        \resizebox{0.95\linewidth}{!}{
    	\begin{tabular}{|l|c|c|}
    	    \hline
    	    \small{\textbf{Initial Weights}} & \multicolumn{1}{c|}{\textbf{IEMOCAP}} &  \multicolumn{1}{c|}{\textbf{Dailydialog}}\\
    	     &  \small{F-Score} & \small{F-Score} \\
    		\hline
    		\hline
    	    HRED  & 58.5 & 48.0 \\
    	    VHRED  & 58.6 & 48.4 \\
    		\hline
    	\end{tabular}
    	}
    	\caption{\footnotesize{Average performance on ERC with pre-trained weights:  for VHRED,  contain additional parameters modeling the latent prior state.}}
    	\label{tab:vhred_results}
        
        
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\hsize}\centering
        
        \resizebox{0.95\linewidth}{!}{
    	\begin{tabular}{|l|c|c|}
    		\hline
    	    \small{\textbf{Generative}} & \multicolumn{1}{c|}{\textbf{IEMOCAP}} &  \multicolumn{1}{c|}{\textbf{Dailydialog}}\\
    	    \small{\textbf{Training}} &  \small{F-Score} & \small{F-Score} \\
    		\hline
    		\hline
    	    Source  & 58.5 & 48.0 \\
    	    Source + Target & 58.0 & 47.2 \\
    		\hline
    	\end{tabular}
    	}
    	\caption{\footnotesize{Average performance on ERC with pre-trained weights: .}}
    	\label{tab:in-domain_results}
    	
    \end{minipage}
}}
\end{table}


\subsection{In-domain Generative Fine-Tuning}





We try in-domain tuning of the generative HRED model by performing conversation modeling on the ERC resources. Finally, we transfer these re-tuned weights for the discriminative ERC task. However, we do not find this procedure to be helpful ( \cref{tab:in-domain_results} ). TL between generative tasks, especially with small-scale target resources, is a challenging task. As a result, we find sub-optimal generation in ERC datasets whose further transfer for the classification does not provide any improvement.

\subsection{Quality of Generative Models}

Despite their recent developments, generative dialogue models still suffer from numerous shortcomings. Challenges include lack of diversity in the responses, which results in the generation of universal sentences, such as \textit{I don't know}~\cite{li-etal-2016-diversity,DBLP:conf/ijcai/SongLNZZY18}. Coherence in topic and emotions are also difficult to maintain while generating responses~\cite{DBLP:conf/aaai/ZhouHZZL18}. Similar traits are observed in our pre-training experiments. 

Although TL-ERC obtains significant improvement in the results, we obtain it with a simple dialogue model. We, thus, believe that further improvements are possible and is contingent on the quality of the dialogue generator. As research in dialogue systems inch towards the \textit{perfect dialogue generator}, it would also benefit ERC via our proposed TL-ERC framework.

\section{Conclusion} \label{sec:conclusion}

In this paper, we presented a novel framework of transfer learning (TL-ERC) for ERC that uses pre-trained affective information from dialogue generators. We presented various experiments with different scenarios to investigate the effect of this procedure. We found that using such pre-trained weights help the overall task and also provide added benefits in terms of lesser training epochs for good generalization. We primarily experimented on dyadic conversations both in the source and the target tasks. In the future, we aim to investigate the more general setting of multi-party conversations. This setting will increase the complexity of the task, as pre-training would require multi-party data and special training schemes to capture complex influence dynamics.

Code used for this work is publicly available at \url{https://github.com/SenticNet/conv-emotion}.

\section*{Acknowledgement}
This research is supported by Singapore Ministry of Education Academic
Research Fund Tier 1 under MOE's official grant number T1 251RES1820.
We also gratefully acknowledge the support of NVIDIA Corporation with the
donation of a Titan Xp GPU used for this research.

\section*{References}

\bibliographystyle{elsarticle-num-names}
\bibliography{mybibfile}

\end{document}