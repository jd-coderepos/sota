
\section{Experiments}
\label{sec:exp}












\begin{table}[tb]
\caption{Dataset statistics.}
\label{tab:stat}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{crrr}
Dataset & \#classes & \#nodes & \#edges \\
\hline
\abovespace
Citeseer & 6 & 3,327 & 4,732 \\
Cora & 7 & 2,708 & 5,429 \\
Pubmed & 3 & 19,717 & 44,338 \\
DIEL & 4 & 4,373,008 & 4,464,261 \\
NELL & 210 & 65,755 & 266,144 \\
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[tb]
\caption{Accuracy on text classification. Upper rows are inductive methods and lower rows are transductive methods.}
\label{tab:text}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{clll}
Method & Citeseer & Cora & Pubmed \\
\hline
\abovespace
Feat & 0.572 & 0.574 & 0.698 \\
ManiReg & 0.601 & 0.595 & 0.707 \\
SemiEmb & 0.596 & 0.590 & 0.711 \\
Planetoid-I & \textbf{0.647} & 0.612 & \textbf{0.772} \\
\hline
\abovespace
TSVM & 0.640 & 0.575 & 0.622 \\
LP & 0.453 & 0.680 & 0.630 \\
GraphEmb & 0.432 & 0.672 & 0.653 \\
Planetoid-G & 0.493 & 0.691 & 0.664 \\
Planetoid-T & 0.629 & \textbf{0.757} & 0.757 \\
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[tb]
\caption{Recall@$k$ on DIEL distantly-supervised entity extraction. Upper rows are inductive methods and lower rows are transductive methods. Results marked with $*$ are taken from the original DIEL paper \cite{bingimproving} with the same data splits.}
\label{tab:distant}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{cl}
Method & Recall@$k$ \\
\hline
\abovespace
$^*$Feat & 0.349 \\
ManiReg & 0.477 \\
SemiEmb & 0.486 \\
Planetoid-I & \textbf{0.501} \\
\hline
\abovespace
$^*$DIEL & 0.405 \\
$^*$LP & 0.162 \\
GraphEmb & 0.258 \\
Planetoid-G & 0.394 \\
Planetoid-T & \textbf{0.500} \\
\hline
\abovespace
$^*$Upper Bound & 0.617 \\
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[tb]
\caption{Accuracy on NELL entity classification with labeling rates of 
$0.1$, $0.01$, and $0.001$. Upper rows are inductive methods and lower rows are transductive methods.}
\label{tab:entity}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{clll}
Method & 0.1 & 0.01 & 0.001 \\
\hline
\abovespace
Feat & 0.621 & 0.404 & 0.217 \\
ManiReg & 0.634 & 0.413 & 0.218 \\
SemiEmb & 0.654 & 0.438 & 0.267 \\
Planetoid-I & 0.702 & 0.598 & 0.454 \\
\hline
\abovespace
LP & 0.714 & 0.448 & 0.265 \\
GraphEmb & 0.795 & 0.725 & 0.581 \\
Planetoid-G/T & \textbf{0.845} & \textbf{0.757} & \textbf{0.619} \\
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{figure*}[tb]
\vskip 0.2in
\centering
\subfigure[GraphEmb]{\label{fig:deepwalk} \includegraphics[width = 0.32\textwidth]{figures/cora_deepwalk.png}}
\subfigure[Planetoid-T]{\label{fig:pt} \includegraphics[width = 0.32\textwidth]{figures/cora_pt.png}}
\subfigure[SemiEmb]{\label{fig:semi} \includegraphics[width = 0.32\textwidth]{figures/cora_semi.png}}
\caption{t-SNE Visualization of embedding spaces on the Cora dataset. Each color denotes a class.}
\label{fig:vis}
\vskip -0.2in
\end{figure*}

In our experiments, \textit{Planetoid-T} and \textit{Planetoid-I} denote the transductive and inductive formulation of our approach.
We compare our approach with label propagation (LP) \cite{zhu2003semi}, semi-supervised embedding (SemiEmb) \cite{weston2012deep}, manifold regularization (ManiReg) \cite{belkin2006manifold}, TSVM \cite{joachims1999transductive}, and graph embeddings (GraphEmb) \cite{perozzi2014deepwalk}.
Another baseline method, denoted as \textit{Feat}, is a linear softmax model that takes only the feature vectors $\mathbf{x}$ as input. We also derive a variant \textit{Planetoid-G} that learns embeddings to jointly predict class labels and graph context without use of feature vectors. The architecture of Planetoid-G is similar to Figure \ref{fig:trans} except that the \textit{input feature} and the corresponding hidden layers are removed.
Among the above methods, LP, GraphEmb and Planetoid-G do not use the features $\mathbf{x}$, while TSVM and Feat do not use the graph $A$. We include these methods into our experimental settings to better evaluate our approach.
Our preliminary experiments on the text classification datasets show that the performance of our model is not very sensitive to specific choices of the network architecture\footnote{We note that 
it is possible to develop other architectures for different 
applications, such as using a shared hidden layer for feature vectors and embeddings.}.
We adapt the implementation of GraphEmb\footnote{\url{https://github.com/phanein/deepwalk}} to our Skipgram implementation. We use the Junto library \cite{talukdar2009new} for label propagation, and SVMLight\footnote{\url{http://svmlight.joachims.org/}} for TSVM. We also use our own implementation of ManiReg and SemiEmb by modifying the symbolic objective function in Planetoid. In all of our experiments, we set the model hyper-parameters to $r_1 = 5/6$, $q = 10$, $d = 3$, $N_1 = 200$ and $N_2 = 200$ for Planetoid.
We use the same $r_1$, $q$ and $d$ for GraphEmb, and the same $N_1$ and $N_2$ for ManiReg and SemiEmb. We tune $r_2$, $T_1$, $T_2$, the learning rate and hyper-parameters in other models based on an additional data split with a different random seed.


The statistics for five of our benchmark datasets are reported in Table \ref{tab:stat}.
For each dataset, we split all instances into three parts, labeled data, unlabeled data, and test data. Inductive methods are trained on the labeled and unlabeled data, and tested on the test data. Transductive methods, on the other hand, are trained on the labeled, unlabeled data, and test data without labels.






\subsection{Text Classification}



We first considered three text classification datasets\footnote{\url{http://linqs.umiacs.umd.edu/projects//projects/lbc/}}, Citeseer, Cora and Pubmed \cite{sen2008collective}. Each dataset contains bag-of-words representation of documents and citation links between the documents. We treat the bag-of-words as feature vectors $\mathbf{x}$. We construct the graph $A$ based on the citation links; if document $i$ cites $j$, then we set $a_{ij} = a_{ji} = 1$. The goal is to classify each document into one class.
We randomly sample $20$ instances for each class as labeled data, $1,000$ instances as test data, and the rest are used as unlabeled data. The same data splits are used for different methods, and we compute the average accuracy for comparison.

The experimental results are reported in Table \ref{tab:text}. Among the inductive methods, Planetoid-I achieves the best performance on all the three datasets with the improvement of up to $6.1\%$ on Pubmed, which indicates that our embedding techniques are more effective than graph Laplacian regularization.
Among the transductive methods, Planetoid-T achieves the best performance on Cora and Pubmed, while TSVM performs the best on Citeseer. However, TSVM does not perform well on Cora and Pubmed. 
Planetoid-I slightly outperforms Planetoid-T on Citeseer and Pubmed, while Planetoid-T gets up to $14.5\%$ improvement over Planetoid-I on Cora. We conjecture that in Planetoid-I, the feature vectors impose constraints on the learned embeddings, since they are represented by a parameterized function of the input feature vectors. If such constraints are appropriate, as is the case on Citeseer and Pubmed, it improves the non-convex optimization of embedding learning and leads to better performance. However, if such constraints rule out the optimal embeddings, the inductive model will suffer.

Planetoid-G consistently outperforms GraphEmb on all three datasets, which indicates that joint training with label information can improve the performance over training the supervised and unsupervised objectives separately.
Figure~\ref{fig:vis} displays the $2$-D embedding spaces on the Cora dataset
using t-SNE~\cite{van2008visualizing}. Note that  
different classes are better separated in the embedding space of Planetoid-T than that of GraphEmb and SemiEmb, which is consistent with our empirical findings. We also observe similar results for the other two datasets.  



\subsection{Distantly-Supervised Entity Extraction}


We next considered the DIEL (Distant Information Extraction using coordinate-term Lists) dataset \cite{bingimproving}.
The DIEL dataset contains pre-extracted features for each entity mention in text, and a graph that connects entity mentions to coordinate lists. The goal is to extract medical entities from text given feature vectors and the graph.

We follow the exact experimental setup as in the original DIEL paper \cite{bingimproving}, including data splits of different runs, preprocessing of entity mentions and coordinate lists, and evaluation. We treat the top-$k$ entities given by a model as positive instances, and compute recall@$k$ for evaluation ($k$ is set to $240,000$ following the DIEL paper). We report the average result of 10 runs in Table \ref{tab:distant}, where \textit{Feat} refers to a result obtained by SVM (referred to as DS-Baseline in the DIEL paper). The result of LP was also taken from~\cite{bingimproving}. \textit{DIEL} in Table \ref{tab:distant} refers to the method proposed by the original paper, which is an improved version of label propagation that trains classifiers on feature vectors based on the output of label propagation. We did not include TSVM into the comparison since it does not scale.
Since we use Freebase as ground truth and some entities are not present in text, the upper bound of recall as shown in Table \ref{tab:distant} is $0.617$.

Both Planetoid-I and Planetoid-T significantly outperform all other methods.
Each of Planetoid-I and Planetoid-T achieves the best performance in 5 out of 10 runs, and they give a similar recall on average, which indicates that there is no significant difference between these two methods on this dataset. Planetoid-G clearly outperforms GraphEmb, which again shows the benefit of joint training.

\subsection{Entity Classification}

We sorted out an entity classification dataset from the knowledge base of Never Ending Language Learning (NELL)
\cite{carlson2010toward} and a hierarchical entity classification dataset \cite{dalvi2014hierarchical}
that links NELL entities to text in ClueWeb09. We extracted the entities and the relations between entities from the NELL knowledge base, and then obtained text description by linking the entities to ClueWeb09. We use text bag-of-words representation as feature vectors of the entities.

We next describe how to construct the graph based on the knowledge base. We first remove relations that are not populated in NELL, including ``generalizations'', ``haswikipediaurl'', and ``atdate''. In the knowledge base, each relation is denoted as a triplet $(e_1, r, e_2)$, where $e_1$, $r$, $e_2$ denote head entity, relation, and tail entity respectively. We treat each entity $e$ as a node in the graph, and each relation $r$ is split as two nodes $r_1$ and $r_2$ in the graph. For each $(e_1, r, e_2)$, we add two edges in the graph, $(e_1, r_1)$ and $(e_2, r_2)$.

We removed all classes with less than $10$ entities. The goal is to classify the entities in the knowledge base into one of the $210$ classes given the feature vectors and the graph. Let $\beta$ be the labeling rate. We set $\beta$ to $0.1$, $0.01$, and $0.001$. $\max(\beta N, 1)$ instances are labeled for a class with $N$ entities, so each class has at least one entity in the labeled data.

We report the results in Table \ref{tab:entity}. We did not include TSVM since it does not scale to such a large number of classes with the one-vs-rest scheme. Adding feature vectors does not improve the performance of Planetoid-T, so we set the feature vectors for Planetoid-T to be all empty, and therefore Planetoid-T is equivalent to Planetoid-G in this case.

Planetoid-I significantly outperforms the best of the other compared inductive methods---i.e., SemiEmb---by $4.8\%$, $16.0\%$, and $18.7\%$ respectively with three labeling rates. As the labeling rate decreases, the improvement of Planetoid-I over SemiEmb becomes more significant.

Graph structure is more informative than features in this dataset, so inductive methods perform worse than transductive methods. Planetoid-G outperforms GraphEmb by $5.0\%$, $3.2\%$ and $3.8\%$.





