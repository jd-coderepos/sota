

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{array}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\long\def\comment#1{}

\newcommand{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\newcommand{\ada}{GloGNN}

\icmltitlerunning{Submission and Formatting Instructions for ICML 2022}

\begin{document}

\twocolumn[
\icmltitle{Finding Global Homophily in Graph Neural Networks When Meeting Heterophily}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xiang Li}{yyy}
\icmlauthor{Renyu Zhu}{yyy}
\icmlauthor{Yao Cheng}{yyy}
\icmlauthor{Caihua Shan}{comp}
\icmlauthor{Siqiang Luo}{sch}
\icmlauthor{Dongsheng Li}{comp}
\icmlauthor{Weining Qian}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Data Science and Engineering, East China Normal University, Shanghai, China}
\icmlaffiliation{comp}{Microsoft Research Asia, Shanghai, China}
\icmlaffiliation{sch}{School of Computer Science and Engineering, Nanyang Technological University, Singapore}

\icmlcorrespondingauthor{Xiang Li}{xiangli@dase.ecnu.edu.cn}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
We investigate graph neural networks on graphs with heterophily.
Some existing methods 
amplify a node's neighborhood with multi-hop neighbors
to include more nodes with homophily.
However,
it is a significant challenge to set personalized neighborhood sizes for different nodes.
Further,
for other homophilous nodes excluded in the neighborhood,
they are ignored for information aggregation.
To address these problems,
we propose two models \ada\ and \ada++,
which generate a node's embedding by aggregating information from global nodes in the graph.
In each layer,
both models learn a coefficient matrix to capture the correlations between nodes,
based on which neighborhood aggregation is performed.
The coefficient matrix allows signed values and is derived from an optimization problem that has a closed-form solution.
We further accelerate neighborhood aggregation 
and derive a linear time complexity.
We theoretically explain the models' effectiveness by proving that both the coefficient matrix 
and the generated node embedding matrix have the desired grouping effect.
We conduct extensive experiments to 
compare our models against 11 other competitors on 15 benchmark datasets in a wide range of 
domains, scales and graph heterophilies.
Experimental results show that
our methods achieve superior performance and are also very efficient.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Graph-structured data is ubiquitous 
in a variety of domains including chemistry, biology and sociology.
In graphs (networks),
nodes and edges represent entities and their relations, respectively. To enrich the information of graphs, 
nodes are usually associated with various features.
For example,
on Facebook,
users are connected by the \emph{friendship} relation and each user has features like \emph{age}, \emph{gender} and \emph{school}.
Both node features and graph topology provide sources of information for graph-based learning.
Recently,
graph neural networks (GNNs)~\cite{kipf2016semi,velivckovic2017graph,hamilton2017inductive}
have received significant attention for the capability to seamlessly integrate the two sources of information
and they have been shown to serve as
effective tools for representation learning on graph-structured data.

Based on the implicit graph homophily assumption,
traditional GNNs~\cite{kipf2016semi}
adopt a non-linear form of smoothing operation and 
generate node embeddings by aggregating information from a node's neighbors.
Specifically,
\emph{homophily} is a key characteristic in a wide range of real-world graphs, 
where linked nodes tend to share similar features or have the same label.
These graphs include friendship networks~\cite{mcpherson2001birds}, political networks~\cite{gerber2013political} and citation networks~\cite{ciotti2016homophily}. 
However, 
in the real world,
there also exist graphs with \emph{heterophily},
where nodes with dissimilar features or different labels are more likely to be connected.
For example,
different amino acid types are connected in protein structures;
predator and prey are related in the ecological food webs.
In these networks,
due to the heterophily,
the smoothing operation could generate similar representations for nodes with different labels,
which lead to the poor performance of 
GNNs. 



\begin{figure}[!htbp]
    \centering
        \includegraphics[width = 0.7\linewidth]{fig/intro.pdf}
        \caption{A toy example to show global homophily. All the homophilous nodes express the global homophily of the center user.}
        \label{fig:toy}
\end{figure}

To generalize GNNs to heterophilous graphs,
some recent works~\cite{zhu2020beyond,bo2021beyond,chien2020adaptive} have been proposed
to leverage high-pass convolutional filters and multi-hop neighbors to address the heterophily issue. 
On the one hand,
the high-pass filters can be used to push away a node's feature vector from its neighbors' while 
the low-pass filters used by traditional GNNs 
do the opposite.
The combination of low-pass and high-pass filters in these models 
enforces the learned representation of a node 
to be close to its homophilous neighbors' and distant from heterophilous ones'.
On the other hand,
in heterophilous graphs,
linked nodes are more likely to be dissimilar while distant nodes could share a certain similarity,
so we have to jump the \emph{locality} of a node 
to find its homophilous neighbors.
As shown in Figure~\ref{fig:toy},
a user has only one \emph{local} neighbor with homophily,
while three homophilous nodes exist multi-hop away.
All these four nodes 
exhibit the \emph{global homophily} for the user,
which
can be used to help predict her label.
Meanwhile,
since it has been pointed out in~\cite{zhu2020beyond} that
the 2-hop neighborhood of a node under some mild condition will be homophily-dominant in expectation,
some models
amplify a node's neighborhood with multi-hop neighbors.
However,
how large the neighborhood size should be set for different nodes is a challenge.
Further, 
for those homophilous nodes excluded in the 
neighborhood,
they will not be utilized in information aggregation.
Therefore,
we propose to leverage 
the {global} homophily for a node in the graph by adding all the nodes to its neighborhood.
As a side effect,
in this case,
the traditional neighborhood aggregation will have a quadratic time complexity,
which is practically infeasible.
Further,
more heterophilous nodes will be included in the neighborhood, 
which could adversely affect the model performance.
Therefore,
a research question arises:
\emph{
Can we find global homophily for a node
and
develop a GNN model that is both effective and efficient for heterophilous graphs?}








In this paper,
to find \textbf{Glo}bal homophily for nodes in graphs with heterophily,
we propose an effective and scalable \textbf{GNN} model, namely, {\ada}.
In the -th convolutional layer,
inspired by the linear subspace model~\cite{liu2012robust},
we linearly characterize each node by all the nodes in the graph and derive a coefficient matrix  
such that  describes the importance of node  to node .
We formulate the characterization problem as an optimization problem 
that has a closed-form solution for .
After that,
taking  as the weight matrix,
we generate node embedding matrix  by 
aggregating information from global nodes.
Note that
directly computing such  and -based neighborhood aggregation
lead to cubic and quadratic time complexities w.r.t. the number of nodes, respectively.
Hence, 
we avoid calculating  directly and reorder matrix multiplication in neighborhood aggregation,
which effectively reduces the time complexity to linear.
Finally,
we mathematically show that 
both  and the generated node embedding matrix  
have the \emph{grouping effect}~\cite{lu2012robust}, i.e., 
for any two nodes in a graph, no matter how distant they are,
if they share similar features and local structures,
their embedding vectors will be close to each other.
This helps explain the effectiveness of our models.
We summarize the main contributions of our paper as:












\noindent{\small}
We propose two effective and efficient GNN models, \ada\ and \ada++.




\noindent{\small}
We theoretically show that both  and the generated node embedding matrix 
have the {grouping effect}.


\noindent{\small}
We combine
low-pass and high-pass convolutional filters in neighborhood aggregation,
as  allows signed values.




\noindent{\small}
We show the superiority of our models against 11 other methods on 15 benchmark datasets of diverse domains, sizes and graph heterophilies.




























 \section{Related Work}
\label{sec:related}
GNNs have recently received significant interest 
for the superior performance on graph-based learning.
The early model GCN~\cite{kipf2016semi} extends
the convolution operation from regular data to irregular graph-structured data.
GCN is a spectral model~\cite{bruna2013spectral,defferrard2016convolutional}, 
which decomposes graph signals via graph Fourier transform and convolves on the spectral components.
There are also a class of spatial GNN models that directly aggregate information from spatially nearby neighbors of a node.
For example,
GraphSAGE~\cite{hamilton2017inductive} generates a node's embedding by aggregating information from a fixed number of neighbors.
GAT~\cite{velivckovic2017graph} introduces the attention mechanism to learn the importance of a node's neighbors
and aggregates information from these neighbors based on the learned weights.
Further,
GNNs have been widely studied from various perspectives,
such as the over-smoothing problem~\cite{zhao2019pairnorm,rong2019dropedge},
the adversarial attack
and defense~\cite{dai2018adversarial,zhu2019robust},
and the model explainability~\cite{vu2020pgm,shan2021reinforcement}.




There are also works~\cite{zhu2020beyond,bo2021beyond,chien2020adaptive,yan2021two,suresh2021breaking,pei2020geom,dong2021graph,lim2021large,yang2021graph,luan2021heterophily,zhu2020graph,liu2021non}
that extend
GNNs to heterophilous graphs. 
Some methods propose to leverage both low-pass and high-pass convolutional filters 
in neighborhood aggregation.
For example,
GPR-GNN~\cite{chien2020adaptive}
adapts to the homophily/heterophily structure of a graph
by learning signed weights for node embeddings in different propagation steps.
ACM-GCN~\cite{luan2021heterophily} applies both low-pass and high-pass filters for each node in a layer,
and adaptively fuses the generated node embeddings from each filter.
Further,
some methods enlarge the node neighborhood size to include more homophilous nodes.
As a representative model, 
HGCN~\cite{zhu2020beyond} 
presents three designs to improve the performance of GNNs under heterophily:
ego and neighbor embedding separation, 
higher-order neighborhood utilization and 
intermediate representation combination.
WRGAT~\cite{suresh2021breaking}
transforms the original graph into a multi-relational one that 
contains both raw edges and newly constructed edges.
The new edges 
can connect distant nodes and are weighted by 
node local structural similarity.
There also exist GNNs that study the graph heterophily issue from other perspectives.
For example,
to jointly study
the heterophily and over-smoothing problems,
GGCN~\cite{yan2021two} allows for signed messages to be propagated from a node's neighborhood.
On the other hand,
it adopts a degree correction mechanism to rescale node degrees and further alleviate the over-smoothing problem.
To generalize GNNs to large-scale graphs,
LINKX~\cite{lim2021large}
separately embeds node features and graph topology.
After that,
the two embeddings are combined with MLPs to generate node embeddings.
Different from all these methods,
\ada\ performs node neighborhood aggregation from the whole set of nodes in the graph,
which takes more nodes in the same class as neighbors and thus boosts the performance of GNNs on graphs with heterophily.










 \section{Preliminaries}
\label{sec:preliminary}
In this section we introduce notations and concepts used in this paper.


\noindent \textbf{[Notations]}.
We denote an undirected graph without self-loops as ,
where  is a set of nodes and
 is a set of edges.
Let  denote the adjacency matrix such that 
 represents the weight of edge  between nodes  and .
For each node ,
we use  to denote 's neighborhood,
which is the set of nodes directly connected to .
We further construct a diagonal matrix  where .
We denote the node representation matrix in the -th layer as ,
where the -th row is the embedding vector  of node .
For the initial node feature matrix,
we denote it as .
We use  to denote the ground-truth node label matrix,
where  is the number of labels in node classification and
the -th row  is the one-hot encoding of node 's label.


\noindent \textbf{[Homophily/Heterophily]}.
The homophily/heterophily of a graph is typically defined 
based on the similarity/dissimilarity between two connected nodes w.r.t. node features or node labels.
In this paper,
we focus on homophily/heterophily in node labels.
There have been some metrics of homophily proposed.
For example,
\emph{edge homophily}~\cite{zhu2020beyond} is defined as the fraction of edges that connect nodes with the same label.
Further,
high homophily indicates low heterophily, and vice versa.
We thus interchangeably use these two terms in this paper.


\noindent \textbf{[GNN basics]}.
The convolution operation in GNNs is typically composed of two steps:
(1) feature propagation and aggregation:
;
(2) node embedding updating:
.
One of the most widely used GNN models is  
vanilla GCN~\cite{kipf2016semi},
which adopts a renormalization trick to add a self-loop to each node in the graph.
After that,
the normalized affinity matrix ,
where ,  and  is the identity matrix.
Note that  is a low-pass filter while the corresponding Laplacian matrix  is a high-pass filter.
The output in the -th layer of vanilla GCN is 
,
where  is a learnable weight matrix and  is the \texttt{Relu} function.
After  layers,
 is then subsequently fed into a \texttt{softmax} layer to generate label probability logits and 
a \texttt{cross-entropy} function for node classification.






















 \section{Algorithm}
\label{sec:algorithm}
In this section,
we describe our models.
We first show how to capture node correlations and derive the coefficient matrix  in each layer.
After that,
we introduce how to accelerate neighborhood aggregation based on .
Further,
we
theoretically prove that both  and  have the desired grouping effect.
Finally,
we summarize \ada\ and upgrade the model to \ada++. 
The overall framework of \ada\ is given in Figure~\ref{fig:framework}.


\begin{figure}[!htbp]
    \centering
        \includegraphics[width = 0.8\linewidth]{fig/framework.pdf}
        \caption{The overall framework of \ada. In each layer,
        we derive a coefficient matrix, based on which a node's embedding is generated by aggregating information from global nodes.}
        \label{fig:framework}
\end{figure}

\subsection{Coefficient matrix}
In graphs with heterophily,
connected nodes are more likely to have different labels
while distant nodes could be from the same class.
We thus have to enlarge a node's neighborhood to leverage more distant nodes.
A straightforward way is to use all the nodes in the graph.
Inspired by the linear subspace model~\cite{liu2012robust},
we characterize each node by all the nodes in the graph.
Specifically,
in the -th layer,
we can determine a coefficient matrix :

where  is a noise matrix.
One can interpret  as a value that reflects how well node  characterizes ,
so  plays the role of a weight matrix.
Note that  allows signed values.
On the one hand,
the more similar two nodes are,
the more likely that one node can be represented by the other.
Homophilous neighbors will thus be assigned large positive coefficients.
On the other hand,
heterophilous neighbors will be given small positive or negative coefficients due to the dissimilarities.
Further,
since
it has been pointed out in~\cite{liu2020towards} that the over-smoothing problem
is caused by the coupling of neighborhood aggregation and feature transformation,
we decouple these two processes by 
first performing
feature transformation to generate .
Specifically,
we use MLPs to map feature matrix and adjacency matrix 
into  and , respectively:

Here,  is the number of labels.
Then
we derive the initial node embedding matrix :

where  is the term weight.
Note that  captures the information of both nodes' features and connectivities. 
Inspired by~\cite{klicpera2018predict},
we use skip connection 
and further modify  in Equation~\ref{eq:coe} into:

where  is a hyper-parameter that balances the term importance.
Here,
we characterize node correlations based on .
However,
as suggested in~\cite{suresh2021breaking}, if two nodes share similar local graph structures,
they are more likely to have the same label.
We thus measure node correlations in terms of both feature similarity and topology similarity
by further regularizing  with nodes' multi-hop reachabilities.
We have the following objective function:

where  and  are weighting factors for adjusting the importance 
of different components,
and  is the maximum hop count.
To show the importance of the -hop graph connectivity,
we further introduce a
learnable parameter
.
The objective function consists of three terms.
The first term 
reduces noise and
drives the linear representation for nodes to be close to their own embeddings,
the second term is a Frobenius norm, 
and the third term regularizes  by the multi-hop regularized graph adjacency matrices.
A closed-form solution  to the optimization problem is 



\subsection{Aggregation acceleration}
\label{sec:acc}
Based on ,
we can write neighborhood aggregation in the -th layer: 

However,
directly updating  by Equation~\ref{eq:conv} is infeasible
due to the cubic time complexity in computing
 and the quadratic time complexity in calculating .
To accelerate the updates of ,
instead of directly calculating ,
we transform Equation~\ref{eq:conv} into (see Section~\ref{sec:agg} in the appendix):
{\begin{scriptsize}

\end{scriptsize}
}
where 
\begin{scriptsize}

\end{scriptsize}
Here,
,  is the identity matrix and  is the number of labels.
In this way,
we avoid computing  directly and can
accelerate the updates of  by matrix multiplication reordering.
Specifically,
we first compute ,
where the second term is calculated from right to left.
In particular,
the matrix inversion
is
performed on a matrix in ,
whose time complexity
is only  and  is generally a very small number.
This significantly improves the model efficiency. 
The overall time complexity of updating  is , where .
After  is calculated,
we  
then update .
We compute each term of  in a similar right-to-left manner.
For example,
to calculate ,
we first compute  but not .
This reduces the time complexity to be , but not .
When calculating ,
we can first compute  due to the sparsity of  for a general graph,
which only requires a time complexity of .
Here,  is the average number of nonzero entries in a row of .
While  generates a dense matrix,
we can further employ the sparsity of  to get .
In this way,
we can sequentially derive , , ...,  in .
In summary,
the total time complexity 
to update  by Equation~\ref{eq:Hexpansion}
is , where  is a coefficient and .




\subsection{Grouping effect}
\label{sec:ge}
For a node ,
we denote  as
the -th row of ,
which represents 's -hop node reachability in a graph.
Given two nodes  and ,
if they have similar feature vectors and local graph structures,
their characterizations from other nodes are expected to be similar.
Formally, we have


\begin{definition}
\textbf{(Grouping effect~\cite{li2020cast})}.
Given a set of nodes ,
let  denote the condition that 
(1) 
and (2) .
A matrix  is said to have grouping effect if 

\end{definition}

We next theoretically show the grouping effect of ,  and 
by giving the following lemmas.
Proofs of all these lemmas
are deferred to Section~\ref{sec:proof} in the appendix.


\begin{lemma}
\label{lemma2}
,
\begin{scriptsize}

\end{scriptsize}
where {\small }.
\end{lemma}


\begin{lemma}
\label{lemma3}
,
\begin{scriptsize}

\end{scriptsize}
where {\small }.
\end{lemma}


\begin{lemma}
\label{lemma:z-star}
Matrices ,  and  all have grouping effect.
\end{lemma}



The grouping effect of ,  and 
indeed explains the effectiveness of our model.
In fact, 
for any two nodes  and ,
no matter how distant they are in a graph,
if they share similar feature vectors and local structures,
we conclude that
(1) they will be given similar coefficient vectors;
(2) they will play similar roles in characterizing other nodes;
and (3) they
will be given similar representation vectors.
On the other hand,
in graphs with heterophily,
adjacent nodes are more likely to be dissimilar
and they will thus be given different embeddings.
Further,
for two nodes with low feature similarity,
using one to characterize the other can be enhanced by the regularization term of local graph structure,
if they share high structural similarity.
This also applies to nodes that have high feature similarity but low structural similarity.
After  convolutional layers,
we derive .
We then normalize   by a \texttt{Softmax} layer,
whose results are further fed into the \texttt{Cross-entropy} function for classification.
Finally,
we summarize the pseudocodes of \ada\ in Algorithm~\ref{alg} (Section~\ref{sec:sup} of the appendix).


\subsection{\ada++}
Given ,
the coefficient matrix  in Equation~\ref{eq:conv} plays the role of ``longitudinal'' attention
that characterizes the importance of a node to another.
In neighborhood aggregation,
not only varies the importance of a node's neighbors, but also that of hidden features.
For example,
in node classification,
the imbalance of node labels could lead to the various importance of hidden features corresponding to different labels.
Therefore,
we further upgrade our model by considering ``horizontal'' attention w.r.t. hidden features.
We introduce a diagonal matrix  such that 
 describes the importance of the -th dimension in .
We modify Equation~\ref{eq:obj} into:

and derive the optimal solution :

Following the same procedure in Sec.~\ref{sec:acc} and~\ref{sec:ge},
we can also accelerate neighborhood aggregation and further prove that such ,  and  have the desired grouping effect.
We omit the details due to the space limitation.

\subsection{Discussion}
We next discuss 
the major differences between our models and the adapted GAT model that takes global nodes as a node's neighbors.
First,
the attention weights in GAT are automatically learned and lack of interpretability, 
but  in our models is derived from a well-designed optimization problem and has a closed-form solution.
Second,
the attention weights in GAT are always non-negative values while  in our methods allows signed values. 
Therefore,
GAT only employs low-pass convolutional filters while our methods combine both low-pass and high-pass filters.
Third,
for each node,
the neighborhood aggregation performed by GAT over all the nodes in the graph
is computationally expensive, 
which has a quadratic time complexity w.r.t. the number of nodes.
However,
our methods
accelerate the aggregation and derive a linear time complexity.




 \section{Experiments}
\label{sec:exp}
In this section,
we comprehensively evaluate the performance of \ada\ and \ada++.
In particular,
we compare them with 11 other methods on 15 benchmark datasets,
to show the effectiveness and efficiency of our models.
Due to the space limitation,
we move experimental setup (Sec.~\ref{sec:ab}) and ablation study (Sec.~\ref{sec:setup}) to the appendix.

\subsection{Datasets}
For fairness,
we conduct experiments on 15 benchmark datasets,
which include 9 small-scale datasets released by~\cite{pei2020geom} and 6 large-scale datasets from~\cite{lim2021large}.
We use the same training/validation/test splits as provided by the original papers.
In particular,
these datasets
span various domains, scales and graph heterophilies.
The statistics of these datasets are summarized in Tables~\ref{tab:result_small} and~\ref{tab:result_large}.
Details on these datasets can be found in Section~\ref{sec:datasets} of the appendix. 






\begin{table*}[!htbp]
\centering
\caption{The classification accuracy (\%) over the methods on 9 small-scale datasets released in~\cite{pei2020geom}.
The error bar () denotes the standard deviation score of results over 10 trials. 
We highlight the best score on each dataset in bold and the runner-up score with underline.
Note that Edge Hom.~\cite{zhu2020beyond} is defined as the fraction of edges that connect nodes with the same label.}
\label{tab:result_small}
\resizebox{\linewidth}{!}
{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c||c|c|}
\hline
                    & \textbf{Texas} & \textbf{Wisconsin}     & \textbf{Cornell} & \textbf{Actor} & \textbf{Squirrel} & \textbf{Chameleon} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} &  \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Avg. Rank}}}} \\
\textbf{ Edge Hom.}  &   0.11    &      0.21     &  0.30  &  0.22  & 0.22  &  0.23 &  0.81 &  0.74 &  0.80 &   \\
\textbf{\#Nodes}  &   183    &   251        &  183  &  7,600  & 5,201 & 2,277  & 2,708 & 3,327 &  19,717 &  \\
 \textbf{\#Edges}  &    295   &    466       &  280  &  26,752  & 198,493 & 31,421  &  5,278 & 4,676  &  44,327 &   \\
 \textbf{\#Features}  &   1,703    &    1,703       &  1,703   &  931  &  2,089 &  2,325 &  1,433 & 3,703  & 500  &   \\
 \textbf{\#Classes} &    5   &     5      & 5  &  5  & 5 & 5  & 6 & 7  &  3  & \\ \hline   
     MLP                   &    &             &    &    &   &   &   &   &     & 9.72 \\
     GCN                    &      &            &  &  &     &  &  &      &    & 10.22 \\
     GAT                    &     &             & &  &   &    &  &  &   &  11.11  \\    
      MixHop               &      &            &  &  &  &   &  &  &   &  10.11   \\  
GCN\rom{2}                 &   &             &  &    &  &   &  &  &     & 5.89 \\
    HGCN               &      &            &  &  &  &   &  &   &     &  6.72 \\ 
    WRGAT               &      &            &  &  &  &   &  &   &     & 6.17  \\ 
    GPR-GNN              &      &            &  &  &  &   &  &  &   &  8.83  \\ 
    GGCN              &      &            &  &  &  &   &  &  &   &  3.89  \\ 
       ACM-GCN               &     &            &  &  &  &   &  &  &   &  3.78   \\  
LINKX               &      &            &  &  &  &   &  &  &  0.77 &  8.78  \\  \hline
    \ada               &      &            &  &  &  &   &  &  &   &    \\  
      \ada++               &      &            &  &  &  &   &  &  &   &    \\  \hline
      
           
\end{tabular}
}
\end{table*}

\begin{table*}[h]
\centering
\caption{The classification results (\%) over the methods on 6 large-scale datasets released in~\cite{lim2021large}.
Note that 
we compare the AUC score on 
genius as in~\cite{lim2021large}.
For other datasets, 
we show the classification accuracy.
The error bar () denotes the standard deviation score of results over 5 trials. 
We highlight the best score on each dataset in bold and the runner-up score with underline.
Note that OOM refers to the out-of-memory error.}
\label{tab:result_large}
\resizebox{0.7\linewidth}{!}
{
\begin{tabular}{|c|c|c|c|c|c|c||c|}
\hline
                    & \textbf{Penn94} & \textbf{pokec}     & \textbf{arXiv-year} & \textbf{snap-patents} & \textbf{genius} & \textbf{twitch-gamers} &  \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Avg. Rank}}}} \\
\textbf{ Edge Hom.}  &  0.47     &    0.44       &  0.22  & 0.07   & 0.61  &  0.54 &  \\
\textbf{\#Nodes}  &   41,554    &      1,632,803     & 169,343   &  2,923,922 &  421,961 & 168,114  &  \\
 \textbf{\#Edges}  &  1,362,229     &    30,622,564       &  1,166,243  &  13,975,788  & 984,979  &  6,797,557 &   \\
  \textbf{\#Features}  & 5     &     65      &    128 &  269   & 12  &  7 &   \\
 \textbf{\#Classes} &    2   &     2      & 5  &  5  & 2 & 2  & \\ \hline   
     MLP                   &    &             &    &    &   &   & 10.00\\
     GCN                     &      &            &  &  &     &  & 7.00\\
     GAT                    &     &             & &  &   &    &  8.50 \\    
      MixHop                &      &            &  &  &  &   &  4.17 \\  
GCN\rom{2}                  &   &             &  &    &  &   & 6.00 \\
   HGCN               &       &    OOM       &   &  OOM  &  OOM  & OOM   &  10.50 \\ 
   WRGAT               &        &   OOM          &  OOM &  OOM  & OOM  & OOM  &  11.92  \\ 
    GPR-GNN              &      &            &  &  &  &   &  7.83 \\ 
    GGCN               &    OOM    &     OOM        & OOM &   OOM & OOM & OOM  &   12.25  \\ 
    ACM-GCN               &        &             &  &    &   &  &   6.83 \\ 
       LINKX                &      &            &  &  &  &   &   2.50  \\  \hline
     \ada               &      &         &  &  &  &   & 2.17  \\  
      \ada++               &      &         &  &  &  &   &  1.33   \\  \hline
      
           
\end{tabular}
}
\end{table*}


\subsection{Algorithms for comparison}
We compare \ada\ and \ada++ with 11 other baselines,
including
(1) MLP;
(2) general GNN methods: 
GCN~\cite{kipf2016semi}, GAT~\cite{velivckovic2017graph},
MixHop~\cite{abu2019mixhop} and GCN\rom{2}~\cite{chen2020simple};
(3) heterophilous-graph-oriented methods:
HGCN~\cite{zhu2020beyond}, 
WRGAT~\cite{suresh2021breaking},
GPR-GNN~\cite{chien2020adaptive}, GGCN~\cite{yan2021two},
ACM-GCN~\cite{luan2021heterophily} and LINKX~\cite{lim2021large}.
For these methods specially designed for heterophilous graphs,
LINKX is a MLP-based method while others are GNN models.
Further,
although several ACM-variants are proposed in~\cite{luan2021heterophily},
ACM-GCN is reported to achieve the overall best performance on the same splits of benchmark datasets from~\cite{pei2020geom},
so we choose it as the baseline.
For other models like Geom-GCN~\cite{pei2020geom} and FAGCN~\cite{bo2021beyond},
since they have been shown to be outperformed by the state-of-the-arts,
we exclude their results in our paper.











\subsection{Performance results}
Tables~\ref{tab:result_small} and~\ref{tab:result_large} summarize the performance results of all the methods on 15 benchmark datasets.
Note that 
we compare the AUC score on 
genius as in~\cite{lim2021large}.
For other datasets, 
we show the results of classification accuracy.
Each column in the tables corresponds to one dataset.
For each dataset,
we highlight the winner's score in bold and the runner-up's with underline.
From the tables,
we make the following observations:








(1) 
MLP that uses only node features performs surprisingly well on some datasets with large heterophily, such as Actor. 
This
shows the importance of node features for node classification in heterophilous graphs. 

(2)
Compared with the plain GNN models GCN and GAT,
MixHop and GCN\rom{2} generally perform better.
For example,
on Wisconsin, the accuracy scores of MixHop and GCN\rom{2} are  and , respectively,
which significantly outperform that of GCN and GAT. 
On the one hand,
MixHop 
amplifies a node's neighborhood with its multi-hop neighbors.
This introduces more homophilous neighbors for the node.
On the other hand,
the initial residual and identity mapping mechanisms in GCN\rom{2}
implicitly combine intermediate node representations to boost the model performance.




(3) Although HGCN, WRGAT and GGCN can achieve good performance on small-scale datasets,
they fail to run on very large-scale datasets due to the out-of-memory (OOM) error.
This hinders the wide application of these models.
For ACM-GCN and LINKX,
they cannot consistently provide superior results. 
For example,
LINKX ranks third on large-scale datasets,
but performs poorly on small-scale ones (rank 8th).
ACM-GCN is the winner on Texas, but its accuracy score on pokec is only  (the best result is ).
While GPR-GNN leverages both low-pass and high-pass filters,
it only utilizes one type of convolutional filters in each layer,
which restricts its effectiveness.


(4) \ada++ achieves the first average rank over all the datasets while \ada\ is the runner-up. 
This shows that 
both of them can consistently provide superior results on datasets in a wide range of diversity.
On the one hand,
both methods learn to utilize more neighbors with homophily for node neighborhood aggregation.
This
boosts the model performance.
On the other hand,
\ada++
further 
learns the importance of hidden features of nodes,
which improves the classification accuracy.


\subsection{Efficiency study}
In this section,
we study \ada's efficiency.
For fairness,
we compare the training time 
for methods that are specially designed for graphs with heterophily.
In particular,
we make the comparison on the large-scale datasets for better efficiency illustration.
For all these methods,
we use the same training set on each dataset and run the experiments for 500 epochs.
We repeat the experiments three times and show the average training time of these methods
w.r.t. accuracy/AUC scores on the validation set in Figure~\ref{figure:runtime}.
Note that
due to the OOM error,
GGCN fails to run on these datasets and
we exclude it for comparison.
We also drop WRGAT because it takes long time to precompute the multi-relational graph.
For example,
with default hyper-parameter settings,
WRGAT takes around  seconds to compute the multi-relational graph of Penn94 on a server with 48 CPUs.
However,
the runs of 
all other models that perform on the original graph are finished within the period.
We next recap the major difference of these methods:
all the methods except LINKX are GNN models.
Specifically,
for each node,
ACM-GCN and GPR-GNN perform convolution directly from its adjacent neighbors
while 
HGCN, \ada\ and \ada++ amplify the neighborhood of the node. 
Here,
HGCN 
considers multi-hop neighbors in the node's neighborhood while
both \ada\ and \ada++ employ the whole set of nodes in the graph.
Compared with \ada,
\ada++ further incorporates an attention mechanism 
to learn the importance of node hidden features.
Finally,
LINKX is a simple MLP-based model that does not include the graph convolution operation.


\begin{figure}[!htbp]
    \centering
    \subfigure[arXiv-year]{\includegraphics[width=0.2\textwidth]{fig/arxiv-year_valid.pdf}}    
    \subfigure[Penn94]{\includegraphics[width=0.2\textwidth]{fig/fb100_valid.pdf}} 
    \subfigure[genius]{\includegraphics[width=0.2\textwidth]{fig/genius_valid.pdf}} 
    \subfigure[pokec]{\includegraphics[width=0.2\textwidth]{fig/pokec_valid.pdf}}    
    \subfigure[snap-patents]{\includegraphics[width=0.2\textwidth]{fig/snap-patents_valid.pdf}} 
    \subfigure[twitch-gamers]{\includegraphics[width=0.2\textwidth]{fig/twitch-gamer_valid.pdf}} 
     \caption{Efficiency study: x-axis shows the training time and y-axis is the accuracy/AUC score on the validation set.}
     \label{figure:runtime}
\end{figure}

From Fig.~\ref{figure:runtime}, 
we see that \ada\ and \ada++ converge very fast to the best/runner-up results over all the datasets.
While GPR-GNN runs faster,
it generally performs poorly. 
For LINKX,
the
MLP-based model structure instead of GNN-based explains its scalability.
However,
the 8th-ranked accuracy score on datasets in Table~\ref{tab:result_small}
restricts its wide usage.
For HGCN and ACM-GCN,
they are slower than \ada\ and \ada++.
For example,
\ada++ achieves almost  speedup than ACM-GCN on genius;
it is also  faster than HGCN on Penn94.
These results show that \ada\ and \ada++
are highly effective and also efficient;
hence they can be widely applied to large-scale datasets.








\subsection{Grouping effect}


Lemma~\ref{lemma:z-star} shows that both the coefficient matrix  and the node embedding matrix  have the desired grouping effect.
Considering the dataset size for clear illustration,
we choose Texas, Wisconsin and Cornell as representatives to show the grouping effect of  in Figure~\ref{figure:z} (a)-(c).
All these datasets contain nodes in five labels.
In 
each sub-figure, 
rows and columns are reordered by ground-truth labels.
We use 
red and blue to indicate positive and negative values, respectively.
We further use pixel color brightness to show the positive/negative degree of a value.
The brighter a pixel,
the larger the degree. 
From the figures,
the matrix exhibits the well-defined block diagonal structure.
This shows the grouping effect of .
For Texas and Cornell,
we see only four blocks along the diagonal.
This 
is because in both datasets, 
there exists one object class that includes only one node.
Similarly,
Figure~\ref{figure:z} (d)-(f) further show the grouping effect of 
the output node embedding matrix  on these datasets.
We reorder columns by gold-standard classes.
Each column in the matrix corresponds to a node's embedding vector.
For nodes in the same class,
their embedding vectors are close to each other.
This further explains the superior performance of our models.

\begin{figure}[!htbp]
    \centering
     \subfigure[Texas]{\includegraphics[height=0.15\textwidth]{fig/texas_z.pdf}}    
    \subfigure[Wisconsin]{\includegraphics[height=0.15\textwidth]{fig/wisconsin_z.pdf}} 
    \subfigure[Cornell]{\includegraphics[height=0.15\textwidth]{fig/cornell_z.pdf}} 
     \subfigure[Texas]{\includegraphics[width=0.5\textwidth]{fig/texas_h.pdf}}    
    \subfigure[Wisconsin]{\includegraphics[width=0.5\textwidth]{fig/wisconsin_h.pdf}} 
    \subfigure[Cornell]{\includegraphics[width=0.5\textwidth]{fig/cornell_h.pdf}} 
     \caption{The grouping effect of  (a)-(c) and  (d)-(f) on Texas, Wisconsin and Cornell (better view in color).}
     \label{figure:z}
\end{figure}






\subsection{Global homophily}
We end this section with a study to show how \ada\ finds global homophily for nodes in the graph.
Given a graph,
we first calculate the average number of -hop neighbors that share the same label with a node. 
We further inspect the average number of positive  values for these neighbors.
After that,
we compare the results on 6 graphs with large heterophily in Figure~\ref{figure:globalfriends}.
We see that
for each node in these datasets,
the average number of adjacent neighbors in the same class
is less than that of multi-hop
ones (2-hop to 6-hop).
There also exist many -hop neighbors that can be used to predict a node's label.
This necessitates jumping the locality of a node and finding its global homophily.
Further,
for each node,
our model \ada\ can correctly assign positive values to the global nodes in the same class,
including both adjacent neighbors
and those that are distant.
This also explains the effectiveness of our models.

\begin{figure}[!htbp]
    \centering  
    \subfigure[Texas]{\includegraphics[width=0.15\textwidth]{fig/case_study_texas.pdf}} 
    \subfigure[Wisconsin]{\includegraphics[width=0.15\textwidth]{fig/case_study_wisconsin.pdf}} 
     \subfigure[Cornell]{\includegraphics[width=0.15\textwidth]{fig/case_study_cornell.pdf}}
     \subfigure[Actor]{\includegraphics[width=0.15\textwidth]{fig/case_study_film.pdf}}  
     \subfigure[Squirrel]{\includegraphics[width=0.15\textwidth]{fig/case_study_squirrel.pdf}} 
      \subfigure[Chameleon]{\includegraphics[width=0.15\textwidth]{fig/case_study_chameleon.pdf}}     
     \caption{Global homophily study}
     \label{figure:globalfriends}
\end{figure}




















 \section{Conclusions}
\label{sec:conclusion}
In this paper,
we generalized GNNs to graphs with heterophily. 
We 
proposed \ada\ and \ada++,
which 
generate a node's embedding by aggregating information from global nodes in the graph.
In each layer,
we formulated an optimization problem to derive
a coefficient matrix  that describes the relationships between nodes.
Neighborhood aggregation is then performed based on .
We accelerated the aggregation process by matrix multiplication reordering without explicitly calculating .
We mathematically proved that both  and the generated node embedding matrix  have the desired grouping effect,
which explains the model effectiveness.
We conducted extensive experiments to evaluate the performance of our models.
Experimental results show that our methods performs favorably against other 11 competitors over 15 datasets of diverse heterophilies;
they are also efficient
and converge very fast.

 

\clearpage 

\bibliography{sample-base}
\bibliographystyle{icml2022}




\appendix
\onecolumn

\section{Pseudocodes}
\label{sec:sup}

Given a graph  and a label set  with ,
let ,
where  is a set of labeled objects and  is a set of unlabeled
ones, the node classification problem is to learn a mapping :  to 
predict the labels
of nodes in .
We next summarize the pseudocodes of \ada\ as follows.

\begin{algorithm}
\caption{\ada}
\label{alg}
\begin{algorithmic}[1]
 \STATE {\bfseries Input:} , , , , , , 
 \STATE {\bfseries Output:} the label matrix  of unlabeled nodes
\STATE Calculate  and  by Eq.~\ref{eq:h0xa}
\STATE Calculate  by Eq.~\ref{eq:h0}
\FOR { to }
\STATE Calculate  by Eq.~\ref{eq:q}
\STATE Calculate  by Eq.~\ref{eq:Hexpansion}
\ENDFOR
\STATE Normalize  with the \texttt{Softmax} function and feed the results into the \texttt{Cross-entropy} function
\STATE Optimize the objective function to update weight matrices
\STATE {\bfseries Return:} 
\end{algorithmic}
\end{algorithm}


\section{Datasets}
\label{sec:datasets}
We first use 9 small-scale datasets from~\cite{pei2020geom} 
and divide them into the following four categories:

\noindent{\small}
\textbf{[Citation network]}.
\emph{Cora}, 
\emph{Citeseer} and \emph{Pubmed} are citation graphs,
where each node represents a scientific paper.
These graphs use 
bag-of-words representations as the feature vectors of nodes.
Each node is assigned a label indicating the research field.
Note that these three datasets are homophilous graphs.

\noindent{\small}
\textbf{[WebKB]}.
\emph{Texas}, \emph{Wisconsin} and \emph{Cornell} 
are web page datasets collected 
from computer science departments of various universities.
In these datasets, 
nodes are web pages and 
edges represent hyperlinks between them.
We take bag-of-words representations as nodes' feature vectors.
The task is to classify the web pages into five categories including \emph{student}, \emph{project}, \emph{course}, \emph{staff} and \emph{faculty}.




\noindent{\small}
\textbf{[Actor co-occurrence network]}.
\emph{Actor} is a graph induced from the film-director-actor-writer network in~\cite{tang2009social},
which describes the co-occurrence relation between actors in Wikipedia pages.
Node features are constructed by keywords contained in the Wikipedia pages of actors.
The task is to classify actors into five categories.




\noindent{\small}
\textbf{[Wikipedia network]}.
\emph{Squirrel} and \emph{Chameleon}
are two subgraphs of web pages in Wikipedia.
Our task is to classify nodes into five categories based on their average amounts of monthly traffic.

To further show the effectiveness and efficiency of our models,
we also use 6 large-scale datasets released by~\cite{lim2021large}:

\noindent{\small}
\textbf{[Social network]}.
\emph{Penn94} is a subgraph extracted from Facebook whose nodes are students.
Node features include major, second major/minor, dorm/house, year and high school.
We take students' genders as nodes' labels. 
\emph{Pokec} is a friendship network from a Slovak online social network,
whose nodes are users and edges represent directed friendship relations.
We construct node features from users' profiles, 
such as geographical region, registration time, age.
The task is to classify users based on their genders.
\emph{genius} is a subnetwork extracted from genius.com,
which is a website for crowdsourced annotations of song lyrics.
In the graph,
nodes are users and edges connect users that follow each other.
User features include expertise scores, 
counts of contributions, roles held by users, etc.
Some users are marked with a ``gone'' label on the site, 
which are more likely to be spam users.
Our goal is to predict whether a user is marked with ``gone''.
\emph{twitch-gamers}
is a subgraph from the streaming platform Twitch,
where nodes are users and edges connect mutual followers.
Node features include 
the number of views,
the creation and update dates,
language,
life time 
and whether the account is dead.
The task is to predict whether the channel has explicit content.



\noindent{\small}
\textbf{[Citation network]}.
\emph{arXiv-year} is a directed subgraph of ogbn-arXiv, 
where nodes are arXiv papers and edges represent the citation relations.
We construct node features by 
taking the averaged word2vec embedding vectors of tokens 
contained in both the title and abstract of papers.
The task is to classify these papers into five labels that are constructed based on their posting year.
\emph{snap-patents}
is a US patent network whose nodes are patents and edges are citation relations.
Node features are constructed from patent metadata.
Our goal is to classify the patents into five labels based on the time when they were granted.


\section{Aggregation acceleration}
\label{sec:agg}
To accelerate the updates of  in Equation~\ref{eq:conv},
we first follow the Woodbury formula~\cite{max1950inverting} to derive
\begin{small}

\end{small}
After that,
based on Eq.~\ref{eq:zstar} and Eq.~\ref{eq:inverse},
we can easily transform Eq.~\ref{eq:conv} into Eq.~\ref{eq:Hexpansion}.



\section{Proof}
\label{sec:proof}

In this section, 
we prove Lemma~\ref{lemma2}, Lemma~\ref{lemma3} and Lemma~\ref{lemma:z-star}, respectively.
In the following discussion,
we use 
 to denote the -th row of ,
which is the coefficient vector for representing node ;
we denote  as
the -th row of ,
which represents 's -hop node reachability in a graph.
We first consider Lemma~\ref{lemma1}:
\begin{lemma}
\label{lemma1}
,
the optimal solution  in Eq.~\ref{eq:obj} satisfies

\end{lemma}
\begin{proof}
For ,
we define .
Since  is the optimal solution of Equation~\ref{eq:obj}, 
we have .
We take the derivative and get 
,
which induces Eq.~\ref{eq:zi}.
\end{proof}


Based on Lemma~\ref{lemma1}, 
we first prove Lemma~\ref{lemma2}:
\begin{proof}
From Equation~\ref{eq:zi}, we get 

Since 

and

let  and
we derive

We further have 

\end{proof}

We next prove Lemma~\ref{lemma3}:

\begin{proof}
From Equation~\ref{eq:zi}, we get 

That implies 

Since  is the optimal solution to Equation~\ref{eq:obj}, we have

Hence, 

Equation~\ref{eq:zizj} can be further simplified as

\end{proof}


The proof of Lemma~\ref{lemma:z-star} is given as follows:
\begin{proof}
Given two nodes  and , if ,
we can get by definition
(1) 
and (2) .
Then based on Equations~\ref{eq:h0xa} and~\ref{eq:h0},
we can easily get .
Hence  has grouping effect.
We next show that  has grouping effect.
Since , 
then  and
 (due to the symmetry of ).
According to Equation~\ref{eq:ip_app},
the R.H.S. of the equation will become close to 0, 
which induces that 
and
 thus has grouping effect.
Similarly,
the R.H.S. of Equation~\ref{eq:pi_app} also approaches 0,
leading to the grouping effect of .
Then we show  has grouping effect.
From Eq.~\ref{eq:conv},
 is updated based on  and .
Due to the grouping effect of  and ,
the linear representation  also has grouping effect,
which further induces that
 has grouping effect.
In this way,
we can inductively prove that ,  and  all have grouping effect.


\end{proof}

\section{Ablation study}
\label{sec:ab}
We next conduct an ablation study
to understand the main components of \ada.
To construct the initial node embedding matrix ,
\ada\ first
transforms both feature matrix and adjacency matrix into low-dimensional embedding vectors, respectively.
To show the importance of feature matrix in constructing ,
we set  in Equation~\ref{eq:h0} and derive .
We call this variant \textbf{\ada-na} (no adjacency matrix).
Similarly,
to understand the importance of feature matrix,
we set  and call the variant \textbf{\ada-nf} (no feature matrix).
Further,
to utilize the local structural information of a node,
\ada\ regularizes the coefficient matrix  with multi-hop graph adjacency matrices, as shown in Equation~\ref{eq:obj}.
We thus consider a variant \textbf{\ada-nl} (no local regularization)
by removing the regularization term to study
the importance of local graph structures of nodes.
Finally,
we compare \ada\ with these variants on all the benchmark datasets and show the results in Fig.~\ref{fig:ab}.
From the figure,
we see 

(1)
While \ada-na and \ada-nf can achieve comparable performance with \ada\ on some datasets,
\ada\ significantly outperforms them on others. 
This shows the necessity of 
\ada\ 
to adaptively learn the importance of feature matrix and adjacency matrix 
when constructing the initial node embedding vectors on various datasets.

(2)
\ada\ generally performs better than \ada-nl.
Since \ada-nl ignores the local regularization term,
it could fail to identify two homophilous nodes that share similar local graph structures.
On the other hand,
\ada\ measures node similarity in terms of both node features and local graph structures,
which further explains \ada's robustness towards graphs with various heterophilies.




\begin{figure*}[!htbp]
    \centering
        \includegraphics[width = \linewidth]{fig/ab_study-eps-converted-to.pdf}
        \caption{Ablation study}
        \label{fig:ab}
\end{figure*}


\section{Experimental setup}
\label{sec:setup}
We implemented \ada\ by PyTorch.
For fairness,
we run the experiments of 9 small-scale datasets on CPUs 
and optimize the models by Adam as in~\cite{yan2021two}.
Meanwhile,
we run the experiments of 6 large-scale datasets on a single Tesla V100 GPU with 32G memory
and use AdamW as the optimizer following~\cite{lim2021large}.
We perform a grid search to tune hyper-parameters based on the results on the validation set.
Details of these hyper-parameters are listed in Tables~\ref{table:grid_search_small} and~\ref{table:grid_search_large}.
Further,
since the results of most baseline methods on these benchmark datasets are public,
we directly report these results.
For those cases where the results are absent,
we use the original codes released by their authors and fine tune the model parameters as suggested in~\cite{yan2021two,lim2021large,suresh2021breaking}.
We provide our code and data at \url{https://github.com/RecklessRonan/GloGNN}.


\begin{table}[!htbp]
\caption{Grid search space on small-scale datasets}
\centering
\begin{tabular}{|c|c|}
    \hline
    {\bf Notation} & {\bf Range} \\ \hline
     lr  &  \\ \hline 
      dropout &  \\ \hline 
       early\_stopping  &  \\ \hline 
        weight\_decay  &  \\ \hline 
           &  \\ \hline 
           &  \\ \hline 
           &  \\ \hline  
            &  \\ \hline 
            norm\_layers  &  \\ \hline 
             max\_hop\_count   &  \\ \hline 
\end{tabular}
\label{table:grid_search_small}
\end{table}


\begin{table}[!htbp]
\caption{Grid search space on large-scale datasets}
\centering
\begin{tabular}{|c|c|}
    \hline
    {\bf Notation} & {\bf Range} \\ \hline
     lr  &  \\ \hline 
      dropout &  \\ \hline 
        weight\_decay  &  \\ \hline 
           &  \\ \hline 
           &  \\ \hline 
           &  \\ \hline  
            &  \\ \hline 
            norm\_layers  &  \\ \hline 
             max\_hop\_count   &  \\ \hline   
\end{tabular}
\label{table:grid_search_large}
\end{table}






 

\end{document}
