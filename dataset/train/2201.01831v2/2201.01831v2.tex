

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{arxiv}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{placeins}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\usepackage{array}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{bbold}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{comment}



\newcommand\OURS{{POCO}}
\newcommand{\titletext}{\OURS: Point Convolution for Surface Reconstruction}
\newcommand{\titletextsupp}{\OURS: Point Convolution for Surface Reconstruction\\ --- Supplementary material ---}

\newcommand{\footnoteref}[1]{}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\closest}{closest}

\newcommand{\dist}{d}
\newcommand{\decoder}{D}
\newcommand{\encoder}{E}
\newcommand{\latentv}{{\mathbf{z}}}
\newcommand{\neighbors}{{\mathcal{N}}}
\newcommand{\occupancy}{\mathbf{o}}
\newcommand{\occupancyp}{{o}}
\newcommand{\occupancyf}{{\omega}}
\newcommand{\pointcloud}{{\mathcal{P}}}
\newcommand{\point}{{\mathbf{p}}}
\newcommand{\qpoint}{{\mathbf{q}}}
\newcommand{\rpoint}{{\mathbf{r}}}
\newcommand{\Real}{{\mathbb{R}}}
\newcommand{\relevance}{{\mathbf{r}}}
\newcommand{\relativizer}{R}
\newcommand{\significance}{s}
\newcommand{\significancev}{{\mathbf{s}}}
\newcommand{\score}{s}
\newcommand{\softmax}{{\mathrm{softmax}}}
\newcommand{\sigmoid}{{\mathrm{sigmoid}}}
\newcommand{\weight}{w}
\newcommand{\weightv}{{\mathbf{w}}}
\newcommand{\PointConv}{Point++}
\newcommand{\ConvONet}{ConvONet}


\newcommand{\todo}[1]{\textcolor{red}{\textbf{[TODO: #1]}}}
\newcommand{\alex}[1]{\textcolor{BrickRed}{#1}}
\newcommand{\renaud}[1]{\textcolor{blue}{#1}}
\newcommand{\balex}{\bgroup\color{BrickRed}}
\newcommand{\ealex}{\egroup}
\newcommand{\brenaud}{\bgroup\color{blue}}
\newcommand{\erenaud}{\egroup}

\definecolor{lightgreen}{rgb}{0.95,1,0.95}


\newcommand{\tabfirst}{\cellcolor{blue!25}}
\newcommand{\tabsecond}{\cellcolor{blue!10}}
\newcommand{\tabthird}{\cellcolor{blue!5}}

\newcommand{\Ntrain}{N_{\text{train}}}
\newcommand{\Ntest}{N_{\text{test}}}
\newcommand{\Nview}{N_{\text{view}}} \usepackage[accsupp]{axessibility}  \usepackage{tocloft}
\setlength{\cftbeforesecskip}{5pt}

\usepackage{scrwfile}
\TOCclone[\contentsname~(\appendixname)]{toc}{atoc}
\newcommand\StartAppendixEntries{}
\AfterTOCHead[toc]{\renewcommand\StartAppendixEntries{\value{tocdepth}=-10000\relax}}
\AfterTOCHead[atoc]{\edef\maintocdepth{\the\value{tocdepth}}\value{tocdepth}=-10000\relax \renewcommand\StartAppendixEntries{\value{tocdepth}=\maintocdepth\relax}}
\newcommand*\appendixwithtoc{\cleardoublepage
  \appendix
  \addtocontents{toc}{\protect\StartAppendixEntries}
  \listofatoc
}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\hyphenation{back-bone data-set data-sets ver-tex}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\begin{document}

\title{\titletext}

\author{
Alexandre Boulch
\and 
Renaud Marlet
\and
\large
\hspace{-3mm}\textsuperscript{1}Valeo.ai, Paris, France  \hspace{1mm} \textsuperscript{2}LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vall\'ee, France
}

\maketitle

\begin{abstract}
Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosurface function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches infer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries. In doing so, they lose the direct connection with the input points sampled on the surface of objects, and they attach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. To address these issues, we propose to use point cloud convolutions and compute latent vectors at each input point. We then perform a learning-based interpolation on nearest neighbors using inferred weights. Experiments on both object and scene datasets show that our approach significantly outperforms other methods on most classical metrics, producing finer details and better reconstructing thinner volumes.
The code is available at \url{https://github.com/valeoai/POCO}. \end{abstract}

\section{Introduction}

Constructing a surface or volume representation from 3D points sampled at the surface of an object or scene has numerous applications, from digital twins processing to augmented and virtual reality. Cheaper sensors directly producing 3D points (depth cameras, low-cost lidars) and
mature multi-view stereo techniques \cite{Schoenberger2016CVPR,Schoenberger2016ECCV} operating on images offer increasing opportunities for such reconstructions.


\begin{figure}[t]
    \centering\setlength{\tabcolsep}{2pt}
    \begin{tabular}{ccc}
        Input (65536 pts) & SA-ConvONet & \OURS~(ours) \\
        \midrule
        \includegraphics[width=0.31\linewidth, trim=0 0 0 20, clip]{images/matterport/17DRP5sb8fy_pts_global1.png}&
        \includegraphics[width=0.31\linewidth, trim=0 0 0 20, clip]{images/matterport/17DRP5sb8fy_saconvonet_global1.png}&
        \includegraphics[width=0.31\linewidth, trim=0 0 0 20, clip]{images/matterport/17DRP5sb8fy_ours_global1.png}\\
        \includegraphics[width=0.31\linewidth]{images/matterport/17DRP5sb8fy_pts_cropped1.png} & 
        \includegraphics[width=0.31\linewidth]{images/matterport/17DRP5sb8fy_saconvonet_cropped1.png} &
        \includegraphics[width=0.31\linewidth]{images/matterport/17DRP5sb8fy_ours_cropped1.png} \\
        \multicolumn{1}{l}{\small (a) Scene 1} & \small 56\,min 40\,s & \small 10\,min 19\,s \\
        \midrule
        \includegraphics[width=0.31\linewidth, trim=0 0 0 20, clip]{images/matterport/JmbYfDe2QKZ_pts_global1.png}&
        \includegraphics[width=0.31\linewidth, trim=0 0 0 20, clip]{images/matterport/JmbYfDe2QKZ_saconvonet_global1.png}&
        \includegraphics[width=0.31\linewidth, trim=0 0 0 20, clip]{images/matterport/JmbYfDe2QKZ_ours_global1.png}\-3pt]
    & & \small 38s & \small 5\,min 09\,s & \small 5\,min 22\,s & \small 17\,min\,32\,s\-3pt]
    & & 1\small \,min 21\,s& \small 8\,min 11\,s & \small 5\,min 12\,s & \small 18\,min\,04\,s\-3pt]
    & & \small 3\,min 55\,s& \small 25\,min 17\,s & \small 5\,min 00\,s & \small 20\,min\,44\,s\-3pt]
    & & \small 5\,min\,05\,s & \small 46\,min\,44\,s & \small 5\,min\,08\,s & \small 23\,min\,59\,s
    \end{tabular}
    \vspace{-8pt}
    \caption{\textbf{SceneNet.} Partial view of a full scene. The color on point clouds indicates the orientation of normals.} 
    \label{fig:scenenet}
    \vspace{-3mm}
\end{figure*}

\subsection{Convolutions for implicit representations}
\label{sec:convmethods}

LIG \cite{Jiang2020CVPR} divides the input point cloud along a regular 3D grid to create 3D patches and capture local geometric shapes shared by several objects at a medium scale. For each of these patches, a 3D CNN then computes a local feature vector, which goes through a reduced IM-NET \cite{Chen2019Learning} for SDF decoding. However, later on, only the learned decoder is exploited; no local embedding is inferred. Given an input point cloud, latent vectors on the grid are optimized from scratch to minimize an objective function similar to the loss used for training. LIG additionally requires to be provided with oriented normals to make use of points known to be inside or outside the shape. This, however, may introduce artificial back-faces, which can partly be addressed in a postprocessing stage.
In contrast, we can work without normals, we directly operate with convolutions on surface points rather than on a regular grid, and we directly use inferred embeddings without any heavy optimization.

IF-Net \cite{Chibane2020CVPR} introduces a multi-scale pyramid of 3D convolutional encoders aligned on a discrete voxel grid and trained on voxels at different scales. The occupancy of a query point is decided by a decoder taking as input the interpolated features extracted at this point for each pyramid level.
In contrast, we do not discretize into voxels; we use point cloud convolution. Also, we learn how to interpolate the latent vectors rather than use a basic trilinear interpolation. Last, we provide results on scenes, not just on objects.

NDF \cite{Chibane2020Neural} uses the same multi-scale encoding as IF-Net but relies on a UDF rather than occupancy for decoding. It allows the generation of very dense points clouds that can directly be meshed into possibly open surfaces.

SG-NN \cite{dai2020sgnn} uses a sparse 3D convolution \cite{choy20194d} to learn a TSDF in a self-supervised setting, training for completion from partial scans.
In contrast, we use point convolution and infer occupancy rather than SDF, which is easier to learn.

ConvONet \cite{Peng2020ECCV} also uses a grid-based convolution, training an autoencoder that predicts occupancy. (It generalizes ONet \cite{Mescheder2019CVPR}, which only uses a single encoding and full connection.) For input point clouds, the encoder is a shallow PointNet \cite{Qi2017CVPR} operating on points rather than on a voxelized discretization, and the decoder is a 3D U-Net \cite{Cciccek2016MICCAI}. The occupancy of a 3D point is inferred from a trilinear interpolation of grid features. Besides 3D convolution, variants based on a combination of 2D convolutions in a few spatial directions are proposed. DP-ConvONet \cite{Lionar_2021_WACV} is a variant that considers a dynamic family of such directions.
SA-ConvONet \cite{tang2021sign} overfits a pre-trained ConvONet model on the input using a sign-agnostic optimization of the implicit field. It improves accuracy at the cost of computation time. 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/network2.pdf}
    \vspace{-6mm}
    \caption{\textbf{Architecture.} The latent vectors  (red squares) produced by the convolution-based encoder  of  neighboring points  of a query point  are:
    (1)~augmented with the relative query position  (yellow squares), 
    (2)~re-encoded with a 3-layer point-wise MLP  (green frame) into relative latent vectors~ (green squares), 
    (3)~combined (blue frame) with inferred weights  (gray squares) into a latent vector  (blue squares),
    (4)~decoded with a linear layer  (pink frame) into occupancy logits  and probablities  (pink squares).
    } 
    \label{fig:decoder}
    \vspace{-2mm}
\end{figure*}


As inference applies to grids, whose vertices or centers may be far from input points, the above methods lose the direct connection with the input surface samples. They are also suboptimal in that the latent vectors holding the information are uniformly distributed in space rather than concentrated where it matters the most, i.e., near the surface.
To address these issues, we use point convolution and compute latent vectors at each input point. We then interpolate occupancy decisions of nearest neighbors using learned weights.

AdaConv \cite{Ummenhofer2021Adaptive} uses point convolution like us but aggregates multi-scale information on an adaptive voxel grid, while we attach features to points, closer to the surface. Besides, it requires oriented normals, contrary to us.

RetrievalFuse \cite{Siddiqui2021RetrievalFuse} splits a scene along a regular grid and encodes each 3D chunk as a latent vector via convolutional layers. But rather than using them for decoding, it retrieves similar chunks from the training set and combines their distance field to create a surface, enhancing the completion capability.
In contrast, we are fully convolutional and the implicit function is directly obtained by interpolating inferred features, without the need to maintain the dataset samples used for training and with more generalization capacity.

Points2Surf \cite{Erler2020Points2Surf} collects, for each query point, both a patch of neighbors (which gives a convolution flavor) and globally-sampled input points to help to provide a sign to the local distance field. The local patch and the global sub-sampling go through an MLP to create latent vectors that are concatenated and decoded into a signed distance.
In contrast, we directly get non-local information as our receptive field is much larger. Besides, we are faster as we only compute a limited number of latent vectors (one per input point) that we later use for interpolation given a query point, while Points2Surf samples local+global points and goes through the whole encoder for each query point, i.e., a large number of times, that grows with the Marching-cubes resolution.

To infer occupancy or distance of a query point, methods that compute several latent vectors for a single object or scene either select the most appropriate latent vector to decode, typically in a multi-scale grid \cite{Ummenhofer2021Adaptive}, or interpolate the latent vectors of query neighbors \cite{Chibane2020CVPR, Jiang2020CVPR, Peng2020ECCV, Chibane2020Neural, Lionar_2021_WACV, tang2021sign}. We perform interpolation too, based on features computed on input points. However, given a query point, we do not interpolate the features themselves but the occupancy logits, as our experiments shows it leads to better results. Besides, we use a learned interpolation rather than the usual tri-linear interpolation \cite{Chibane2020CVPR, Jiang2020CVPR, Peng2020ECCV, Chibane2020Neural, Lionar_2021_WACV, tang2021sign} or the inverse-distance distance weighting \cite{Qi2017NIPS}. Although different in nature, learning has also been used in \cite{Siddiqui2021RetrievalFuse} to blend retrieved chunks.
 
\section{Our method}

\textbf{Goal.} Given as input a set of  3D points  sampled on a surface, possibly with noise, our goal is to construct a continuous function  indicating the probability of occupancy  at any given query point . We learn this function with a neural network using data consisting of point clouds sampled in the whole space and labeled with  (in empty space) or  (within the shape). The surface of the shape can then be extracted as the isosurface of the implicit function~ with occupancy level~0.5.

\textbf{Overview.} Our method consist of the following steps:
\begin{enumerate}[itemsep=-2pt,topsep=-1pt]

    \item We encode input points  into latent vectors .
    
    \item Given an arbitrary query point , we consider a neighborhood  of input points in  to  interpolate from.

    \item For each neighbor , we construct a relative latent vector  from  and local coordinates .\label{decodefirst}
    
    \item We extract significance weights  to sum the relative latent vectors : .
    
    \item We decode the resulting feature vector  as two full-empty logits , and turn them into probabilities~.
    \label{decodelast}
    
\end{enumerate}\vspace{3pt}
These steps, illustrated on Figure~\ref{fig:decoder}, are detailed below.

\textbf{Absolute encoding.} 
A point convolution first produces a latent vector  for each input point . The encoder  can be implemented by any point cloud segmentation backbone, only changing the last layer to yield a vector of some chosen dimension~ as the size of vectors . (In our experiments, the convolution backbone is FKAConv \cite{Boulch2020ACCV} and .) To also use normals (optionally), the input points are just augmented with the 3 normal coordinates.

\textbf{Query neighborhood.} 
Given an arbitrary query point~ (when training or to predict occupancy at test time), we construct a set of neighbors  from input points . (In our experiments,  is the  nearest neighbors of , with .)

\textbf{Relative encoding.} 
We augment the latent vector  of
each neighbor  with the local coordinates  of query point  relatively to~. These augmented latent vectors are then processed by an MLP  to produce relative latent vectors , where  is the concatenation. (In our experiments,  and  have size .)


\textbf{Feature weighting.}
As PRNet \cite{Wang2019NeurIPS}, we observe that the norm of embeddings  tends to correlate with their significance, hinting how much an input point~ matters for deciding the occupancy of query point~, given 's neighbors and the position of  w.r.t.~.
We use it to infer significance weights for relative latents vectors . Concretely, we use an attention mechanism (blue frame in Fig.\,\ref{fig:decoder}): The relative embeddings  go through a linear layer parameterized by a weight vector~, also of size~, producing relative weights , that are normalized by softmax over  into positive interpolation weights  summing to~.
We actually use a multi-head strategy to obtain a form of ensembling. We learn  independent linear layers, parameterized by  corresponding weight vectors , producing  relative weights , that are finally softmaxed as  and averaged as . (In our experiments, we use .)

\textbf{Interpolation.} The feature vector 
 at query point  is interpolated from the relative latent vectors  of neighbors , as the weighted sum .

\textbf{Decoding.} A linear layer  decodes the feature vector 
 into occupancy scores , which is a two-logit vector classifying position  as occupied or not, that is then turned via softmax into occupancy probabilities~.

\textbf{Loss function.} To train the network, we use a cross-entropy loss that penalizes wrong occupancy predictions. Please note that using a binary cross-entropy, like in IF-Net \cite{Chibane2020CVPR} or ConvONet \cite{Peng2020ECCV}, leads to identical 
results.




















\section{Refinements}

\textbf{Adapting to high density.}
We train our network with a fixed number  of input points for easy mini-batching. (In our experiments, 3k  or  10k.) 
At test time, if the surface is more densely sampled, the receptive field of the backbone may lack enough global context to decide which side of the surface is full or empty, unless oriented normals are also provided with points. 
A way to broaden enough the receptive field is to downsample the input point cloud, but it then naturally leads to a loss of details.

To reduce this effect, we rely on test-time augmentation (TTA) \cite{krizhevsky2012imagenet}, which can be seen as a form of ensembling: we average several runs on different subsamples.
However, aggregating final results, as often done in TTA \cite{Shanmugam2021BetterAggregation}, would be very time consuming in our case as we would have to do it to answer the occupancy of each query, basically multiplying the inference running time by the number of subsamples.

Instead, we perform TTA at latent vector level, thus running several times only the first step of our approach (absolute encoding), before query decoding. It depends on the number of input points (to attach a latent vector on), rather than on the number of query points, which is much larger.
Concretely, we randomly create enough subsamples so that each point  is seen at least  times, and average each  over all samples. (In experiments, .) The subsamples are randomly generated by sequentially picking a point  with a priority that is the opposite of the number of times  appears in previous subsamples.

\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{cccc}
        GT & Input & ConvONet & \OURS\ (ours)  \
    \text{Chamfer}(P_1, P_2) =& \> \frac{1}{2\,|P1|} \sum_{p_1 \in P_1} \min_{p_2 \in P_2} d(p_1, p_2) \\ & + \frac{1}{2\,|P2|} \sum_{p_2 \in P_2} \min_{p_1 \in P_1} d(p_1, p_2)

    \text{NC}(P_1, P_2) =& \> \frac{1}{2\,|P1|} \sum_{p_1 \in P_1} n_{p_1} . n_{\closest(p_1,P_2)} \\ 
                & + \frac{1}{2\,|P2|} \sum_{p_2 \in P_2} n_{p_2} . n_{\closest(p_2, P_1)}

    \closest(p,P) =& \argmin_{p' \in P} d(p, p')

    \text{FS}(t, P_1, P_2) = \frac{2\, \text{Recall} \; \text{Precision} }{\text{Recall}+ \text{Precision}}

    \text{Recall}(t, P_1, P_2) = \left| \left\{ p_1 \in P_1, \; \text{s.t.} \min_{p_2 \in P_2} d(p_1, p_2) < t \right\} \right|
    \\
    \text{Precision}(t, P_1, P_2) = \left| \left\{ p_2 \in P_2, \; \text{s.t.} \min_{p_1 \in P_1} d(p_2, p_1) < t \right\} \right|

    \text{IoU} = \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}
1mm] 
        Category    & PointConv & ONet & \!ConvONet\! & \OURS & PointConv & ONet & \!ConvONet\! & \OURS \\
        \midrule
        Airplane    & 0.579 & 0.734 & 0.849 & \bf 0.902 & 1.40 & 0.64 & 0.34 & \bf 0.23	\\
        Bench       & 0.537 & 0.682 & 0.830 & \bf 0.865 & 1.20 & 0.67 & 0.35 & \bf 0.28\\
        Cabinet     & 0.824 & 0.855 & 0.940 & \bf 0.960 & 1.15 & 0.82 & 0.46 & \bf 0.37\\
        Car         & 0.767 & 0.830 & 0.886 & \bf 0.921	& 1.49 & 1.04 & 0.75 & \bf 0.41\\
        Chair       & 0.667 & 0.720 & 0.871 & \bf 0.919	& 1.29 & 0.95 & 0.46 & \bf 0.33\\
        Display     & 0.743 & 0.799 & 0.927 & \bf 0.956	& 1.06 & 0.82 & 0.36 & \bf 0.28\\
        Lamp        & 0.495 & 0.546 & 0.785 & \bf 0.877	& 2.15 & 1.59 & 0.59 & \bf 0.33\\
        Loudspeaker & 0.807 & 0.826 & 0.918 & \bf 0.957	& 1.48 & 1.18 & 0.64 & \bf 0.41\\
        Rifle       & 0.565 & 0.668 & 0.846 & \bf 0.897	& 0.98 & 0.66 & 0.28 & \bf 0.19\\
        Sofa        & 0.811 & 0.865 & 0.936 & \bf 0.963	& 1.04 & 0.73 & 0.42 & \bf 0.30\\
        Table       & 0.654 & 0.739 & 0.888 & \bf 0.924	& 1.13 & 0.76 & 0.38 & \bf 0.31\\
        Telephone   & 0.856 & 0.896 & 0.955 & \bf 0.968	& 0.61 & 0.46 & 0.27 & \bf 0.22\\
        Vessel      & 0.652 & 0.729 & 0.865 & \bf 0.927	& 1.38 & 0.94 & 0.43 & \bf 0.25\\
        \midrule
        Mean        & 0.689 & 0.761 & 0.884 & \bf 0.926	& 1.26 & 0.87 & 0.44 & \bf 0.30\\
    \end{tabular}
    
    \bigskip
    
    \begin{tabular}{l|cccc|cccc}
                    & \multicolumn{4}{c|}{NC\,} & \multicolumn{4}{c}{FS}\1mm] 
        Category    & PointConv & ONet & \!ConvONet\! & \OURS & PointConv & ONet & \!ConvONet\! & \OURS \\
        \midrule
        Airplane    & 0.819 & 0.886 & 0.931 & \bf 0.944	& 0.562 & 0.829 & 0.965 & \bf 0.994\\
        Bench       & 0.811 & 0.871 & 0.921 & \bf 0.928	& 0.617 & 0.827 & 0.964 & \bf 0.988 \\
        Cabinet     & 0.895 & 0.913 & 0.956 & \bf 0.961 & 0.719 & 0.833 & 0.956 & \bf 0.979 \\
        Car         & 0.845 & 0.874 & 0.893 & \bf 0.894 & 0.577 & 0.747 & 0.849 & \bf 0.946 \\
        Chair       & 0.851 & 0.886 & 0.943 & \bf 0.956 & 0.618 & 0.730 & 0.939 & \bf 0.985 \\
        Display     & 0.910 & 0.926 & 0.968 & \bf 0.975 & 0.679 & 0.795 & 0.971 & \bf 0.994 \\
        Lamp        & 0.779 & 0.809 & 0.900 & \bf 0.929 & 0.453 & 0.581 & 0.892 & \bf 0.975 \\
        Loudspeaker & 0.894 & 0.903 & 0.939 & \bf 0.952 & 0.647 & 0.727 & 0.892 & \bf 0.964 \\
        Rifle       & 0.796 & 0.849 & 0.929 & \bf 0.949 & 0.682 & 0.818 & 0.980 & \bf 0.998 \\
        Sofa        & 0.900 & 0.928 & 0.958 & \bf 0.967 & 0.697 & 0.832 & 0.953 & \bf 0.989 \\
        Table       & 0.878 & 0.917 & 0.959 & \bf 0.966 & 0.694 & 0.824 & 0.967 & \bf 0.991 \\
        Telephone   & 0.961 & 0.970 & 0.983 & \bf 0.985 & 0.880 & 0.930 & 0.989 & \bf 0.998 \\
        Vessel      & 0.817 & 0.857 & 0.919 & \bf 0.940 & 0.550 & 0.734 & 0.931 & \bf 0.989 \\
        \midrule
        Mean        & 0.858 & 0.891 & 0.938 & \bf 0.950 & 0.644 & 0.785 & 0.942 & \bf 0.984
    \end{tabular}
    
    \caption{\textbf{Classwise ShapeNet reconstruction.} All models are trained on 3k noisy points. Results for methods other than {\OURS} are reported from the supplementary material of ConvONet~\cite{Peng2020ECCV}.}
    \label{tab:shapenet_per_class}
\end{table*}

\FloatBarrier

\section{Use of existing assets}

\subsection{Pre-existing code}

The implementation of our approach has several dependencies, that are all free to use for research purposes. The main dependencies of our code are as follows:
\begin{itemize}[itemsep=4pt,topsep=4pt, parsep=0pt]

    \item \textbf{FKAConv}\footnote{\label{foot:fkaconv}\url{https://github.com/valeoai/FKAConv}} \cite{Boulch2020ACCV}, under Apache License v2.0.

    \item \textbf{PyTorch}\footnote{\label{foot:pytorch}\url{https://pytorch.org/}}, under the Apache CLA,

    \item \textbf{PyTorch-Geometric}\footnote{\label{foot:pytorchgeom}\url{https://pytorch-geometric.readthedocs.io/}}, under the MIT License,

\end{itemize}
The code of {\OURS}\footnote{\url{https://github.com/valeoai/POCO}} itself is freely available, under Apache License v2.0.

\subsection{Datasets}\label{sec:assetsdatasets}

For the experiments, we used several datasets that are freely available for research purpose:
\begin{itemize}[itemsep=4pt,topsep=4pt, parsep=0pt]

    \item \textbf{ABC}\footnote{\label{foot:abc}\url{https://deep-geometry.github.io/abc-dataset/}} is under the Onshape Terms of Use\footnote{\url{https://www.onshape.com/en/legal/terms-of-use}}. We used the subset preprocessed and made available by the authors of Points2Surf\footnoteref{foot:points2surf} \cite{Erler2020Points2Surf}.
    
    \item \textbf{Famous} is a set of shapes of various origins, among which the Stanford 3D Scanning Repository\footnote{\url{http://graphics.stanford.edu/data/3Dscanrep/}} \cite{krishnamurthy1996fitting}. This set of shapes is described, preprocessed and made available by the authors of Points2Surf\footnoteref{foot:points2surf} \cite{Erler2020Points2Surf}.

    \item \textbf{MatterPort3D}\footnote{\label{foot:matterport3d}\url{https://niessner.github.io/Matterport/}} \cite{chang2017matterport3d} is under a user license agreement for academic use. We used scenes preprocessed by the authors of SA-ConvONet\footnoteref{foot:saconvonet} \cite{tang2021sign}.

    \item \textbf{Real-World} point clouds used in the paper are described, preprocessed and made available by the authors of Points2Surf\footnoteref{foot:points2surf} \cite{Erler2020Points2Surf}.

    \item \textbf{SceneNet}\footnote{\url{https://robotvault.bitbucket.io/}} \cite{handa2015scenenet, handa2016scenenet, Handa2016Understanding} is under the CC BY-NC 4.0,
    for research purposes only. We made meshes watertight using \textbf{Watertight Manifold}\footnote{\label{foot:watertightmanifold}\url{https://github.com/hjwdzh/Manifold}} \cite{huang2018robust}, that enables code use under mild conditions.

    \item \textbf{ShapeNet}\footnote{\label{foot:shapenet}\url{https://shapenet.org/}} \cite{Chang2015ARXIV} has a licence for non commercial research or educational purposes. We used the version of ShapeNet as preprocessed by the authors of \textbf{ONet}\footnote{\label{foot:onet}\url{https://github.com/autonomousvision/occupancy_networks}}  \cite{Mescheder2019CVPR}, which itself reuses the preprocessing of the authors of \textbf{3D-R2N2}\footnote{\label{foot:3dr2n2}\url{https://github.com/chrischoy/3D-R2N2}} \cite{Choy2016ECCV}.

    \item \textbf{Synthetic Rooms}\footnoteref{foot:convonet} is a dataset created by the authors of ConvONet \cite{Peng2020ECCV} based on ShapeNet models.

    \item \textbf{Thingi10K}\footnote{\label{foot:thingi10k}\url{https://ten-thousand-models.appspot.com}} \cite{zhou2016thingi10k} is a freely available collection of shapes under various licences. We used the subset preprocessed and made available by the authors of Points2Surf\footnoteref{foot:points2surf} \cite{Erler2020Points2Surf}.

\end{itemize}


\subsection{Methods}\label{sec:assetsmethods}

We compared to a number of reconstruction methods, reusing the code made available by their authors:
\begin{itemize}[itemsep=4pt,topsep=4pt, parsep=0pt]

\item \textbf{ConvONet}\footnote{\label{foot:convonet}\url{https://github.com/autonomousvision/convolutional_occupancy_networks}} \cite{Peng2020ECCV} under the MIT License.

\item \textbf{LIG}\footnote{\label{foot:lig}\url{https://github.com/tensorflow/graphics/tree/master/tensorflow_graphics/projects/local_implicit_grid}} \cite{Jiang2020CVPR} probably under Apache License v2,

\item \textbf{Neural Splines}\footnote{\label{foot:neuralsplines}\url{https://github.com/fwilliams/neural-splines}} \cite{Williams2021NeuralSplines}  under the
MIT License,

\item \textbf{Points2Surf}\footnote{\label{foot:points2surf}\url{https://github.com/ErlerPhilipp/points2surf}} \cite{Erler2020Points2Surf} under the MIT License,

\item \textbf{SA-ConvONet}\footnote{\label{foot:saconvonet}\url{https://github.com/tangjiapeng/SA-ConvONet}} \cite{tang2021sign} under the MIT License,

\item \textbf{SPR}\footnote{\label{foot:spr}\url{https://github.com/mkazhdan/PoissonRecon}} \cite{Kazhdan2013SIGGRAPH} under the MIT License.

\end{itemize}
We also compared to \textbf{AtlasNet} \cite{Groueix2018CVPR}, \textbf{DeepSDF} \cite{Park2019CVPR}, \textbf{DP-ConvONet} \cite{Lionar_2021_WACV}, \textbf{ONet} \cite{Mescheder2019CVPR}, but only reusing the numbers mentioned in \cite{Peng2020ECCV, Lionar_2021_WACV}.

\medskip

Here are some methods we would have liked to compare to, but could not in practice:
\begin{itemize}[itemsep=-2pt,topsep=1pt]

\item \textbf{AdaConv}\footnote{\label{foot:adaconv}\url{https://github.com/isl-org/adaptive-surface-reconstruction}} \cite{Ummenhofer2021Adaptive}: The repository provides raw code but no pre-trained model nor instructions or scripts to train or to test, which may lead to misuses and wrong comparisons. 

\item \textbf{NDF}\footnote{\url{https://github.com/jchibane/ndf}} \cite{Chibane2020Neural}: The repository provides code but only a pre-trained model for ShapeNet cars. For scene reconstruction, it does not offer preprocessed data or any data preprocessing procedure to retrain a model, nor instructions to run NDF using a sliding window scheme, as alluded to in the supplementary material.

\end{itemize}
As indicated in Table~\ref{tab:datasets}, some authors also have not made their code or their model available to allow comparisons.


\section{Societal impact}

We believe our 3D reconstruction approach has \textbf{very little potential for malicious uses} (including disinformation, surveillance, invasion of privacy, endangering security), not more, e.g., than image enhancement methods in the 2D data case, and not more than hundreds of previously published 3D reconstruction methods. Besides, we are not bound nor promoting any dataset that would lead to unfairness in any sense. The use of our method has a \textbf{modest environmental impact} as the training time (a few days on a single GPU for a large dataset) and the inference times (minutes, or hours for very large point clouds) are somewhat moderate, and favorably compare to many learning-based approaches.

On the contrary, applications of our method can be found in various domains, with positive societal impacts:

\textbf{Heritage preservation.}
Digitizing cultural objects and monuments allows a form of heritage preservation and enables virtual museums to make works of art and culture more widely accessible.
    
\textbf{Infrastructure and building maintenance.}
Reconstructing models of existing infrastructures and buildings is of high interest for the construction industry. These models are particularly useful to plan and organize maintenance. This is particularly useful in a context of aging infrastructures and building renovation for energy-saving insulation.

\textbf{Augmented and virtual reality.}
Surface and volume reconstruction are useful assets for augmented and virtual reality, whether it is for professional use (e.g., on-site maintenance of equipment) or entertainment (video games, special effects for the film industry), which is however to be consumed in moderation.

 

\end{document}
