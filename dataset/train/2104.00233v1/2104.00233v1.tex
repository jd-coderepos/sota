
        
\subsection{Problem Formalization}


We consider multi-class visual categorization, where a specific example  belongs to one of  predefined visual concepts. Ground truth of  is indicated by , a -dimensional one-hot vector. A deep visual classification network  classifies  by first employing a feature extractor  to obtain a vectorized feature representation  from its raw pixels. Then,  is fed into a -way classifier  to produce a categorical probability vector , where the value of its -th dimension is the probability of the example belonging to the -th concept, \ie

Such a paradigm as expressed in Eq. \ref{eq:clf} remains valid to this day, even though  and  have now been jointly deployed and end-to-end trained by deep learning.



As UDE is derived from UDA, we adopt common notations from the latter for the ease of consistent description. 
For both UDE and UDA, we have access to a set of  labeled training examples  from a source domain  and a set of  unlabeled training examples  from a target domain . However, different from UDA that focuses on , UDE treats the expanded domain  as its ``target'' domain. Therefore, our goal is to train a unified model that can accurately classify novel examples regardless of their original domains.









\subsection{UDE by Knowledge Distillation} \label{ssec:kdde_model}

We propose a knowledge distillation based method for UDE, which we term \textit{KDDE}. As illustrated in Fig. \ref{fig:method}, KDDE is performed in two steps. In the first step, two domain-specific classifiers, denoted as  and , are trained for the source and target domains, respectively. Note that we use the notation  to emphasize that as  is unlabeled, any domain-adapted classifier shall departure from . In the second step, we treat  and  as two teacher models and transfer their ``dark'' knowledge into a student model  via a knowledge distillation process. In particular, by mimicking  for classifying examples from  and  for ,  essentially becomes domain-invariant.  We detail the two steps as follows.


\begin{figure*}[tbh!]
\setlength{\abovecaptionskip}{5pt}
\centering
\includegraphics[width=\textwidth]{diagrams/kdde}
\caption{\textbf{Conceptual diagram of the proposed KDDE method for UDE}. KDDE is run in two steps. First, a model  is trained on the \textit{labeled} source domain by standard supervised learning, while another model  for the \textit{unlabeled} target domain is trained by unsupervised domain adaptation (here CDAN~\cite{nips18-cdan} as a running example). Then, knowledge distillation is performed to transfer ``dark'' knowledge from the two domain-specific models into a student model  to make it applicable to both domains.}
\label{fig:method}
\end{figure*} 

\subsubsection{Step 1(a): Training a source-only model }

Learning a classifier from labeled data is relatively simple. We adopt the cross-entropy loss, a common classification loss used for training a multi-class deep neural network. Given  as , the classification loss is written as
 
By minimizing , we obtain a domain-specific model for  as





\subsubsection{Step 1(b): Training a domain-adapted model }





Recall that KDDE, as a two-stage solution, is agnostic to the implementation of a specific UDA model used in its first stage.
Hence, for obtaining , any method for unsupervised domain adaptation can, in principle, be adopted. 
A typical process of model adaptation tries to strike a proper balance between a model's discriminability, as reflected by the classification loss on , and its domain-invariant representation ability, as measured by inter-domain discrepancy between  and . More formally, we have 

where  is a domain discrepancy based loss, and  is a positive hyper-parameter. Algorithm \ref{alg:da} describes the domain adaptation process at a high level. In what follows, we use the state-of-the-art CDAN model~\cite{nips18-cdan} as a running example to instantiate . 

\begin{algorithm}
\caption{Training a domain-adapted model }
\label{alg:da}
\KwIn{, }
\KwOut{}
Set hyper-parameters: \;
Compute the number of iterations per epoch , given  and  \;
Initialize {} with an ImageNet-pretrained model\;
\For{}{
    \For{}{
      Sample a mini-batch  from \;
      Sample a mini-batch  from \;
      Compute the classification loss  \;
      Compute the inter-domain discrepancy  \;
      Update  by back propagation based on 
      }
    }
\end{algorithm}
 





CDAN reduces domain discrepancy by adversarial learning, where a feed-forward neural network is used as a discriminator  to disentangle the source examples  from the target examples . Different from previous adversarial learning based methods where only the intermediate features  and  are considered, CDAN uses multilinear conditioning of the features and class prediction  as the input of . The output of  is the probability of a given example coming from the source domain. Accordingly, we have the discriminator reward  as 
 
The training process of CDAN is implemented as a two-player minimax game as follows:
















\subsubsection{Step 2. Training a domain-expanded model }

Given the domain-specific models  and , we now transfer their capabilities in their own domains into a new model  by knowledge distillation. 

In a standard scenario where one wants to distill the knowledge in a big teacher model into a relatively small student model~\cite{nips15-kd}, both ground-truth hard labels and soft labels predicted by the teacher model are available for computing the distillation loss. By contrast, in the setting of UDE, the expanded domain has only partial ground-truth labels by definition. More importantly, in order to make  domain-invariant, we shall not only treat  and  equally, but also exploit training examples from  and  in the same manner. To that end, we opt to compute the distillation loss fully based on the soft labels. 

Specifically, we adopt the Kullback-Leibler (KL) divergence, as previously used to quantify how a student's output matches with its teacher~\cite{nips15-kd,zhang2018deep_mutual}. Per training example, the soft labels produced by the teacher / student model is essentially a probability distribution over the  concepts. The KL divergence provides a natural measure of how the probability distribution produced by the student is different from that of the teacher, making it a popular loss for knowledge distillation \cite{nips15-kd,zhang2018deep_mutual,mirzadeh2020improved,tian2019contrastive,goldblum2020adversarially,zhang2019your,shi2019knowledge}. Indeed, our ablation study in Section \ref{sssec:kd-loss} shows that the KL divergence loss is better than other losses such as cross-entroy and . Let  be a -dimensional categorical distribution estimated based on soft labels of an example set  produced by a specific network . Accordingly, the KL divergence from  to each of the two teachers is defined as  and , respectively. 
The knowledge distillation loss  is defined as 


Note that the  terms in Eq. \ref{eq:loss-kd} are practically computed by a mini-batch approach. As demonstrated in Fig. \ref{fig:method}, in each iteration two mini-batches are independently and randomly sampled from  and . They are then fed into  and  to get the soft labels, which are used to approximate  and , respectively. Meanwhile, the two batches are also fed to the student model. Minimizing  lets the student model mimic the teachers' behaviors on both  and . 
Consequently, we obtain the domain-invariant model as 


A high-level description of the training process is given in Algorithm \ref{alg:kdde}. Note that during training, we need to store three models (two teachers  and  and one student ). We consider such storage overhead affordable as one typically has access to more computational resources in the training stage than in the inference stage. Moreover, as the teachers are only used to product the soft labels, their weights are frozen, meaning the GPU footprint is much less than simultaneously training all the three models. Also note that in the inference stage.  has the same model size and computation overhead as its teachers. Hence, the proposed method is feasible in real-world scenarios.

\begin{algorithm}
\caption{Training a domain-expanded model  by KDDE}
\label{alg:kdde}
\KwIn{}
\KwOut{}
Set hyper-parameters: \;
Compute  given  and  \;
Initialize {} with an ImageNet-pretrained model\;
\For{}{
    \For{}{
        Sample  from \; 
        Sample  from \;
        Compute  using Eq. \ref{eq:loss-kd}\;
        Update  by back propagation based on \;
        }
    }
\end{algorithm}
 
\subsection{Theoretical Analysis}

Comparing Eq. \ref{eq:general-da} and Eq. \ref{eq:general-de}, we see that UDA essentially tries to simultaneously optimize two distinct and sometimes conflictive objectives, \ie discriminability and domain-invariance. By contrast, our KDDE optimizes a single objective. We believe such a property improves the cross-domain generalization ability of KDDE. Nonetheless, because  is learned from the source model  and the domain-adapted model  in an unbiased manner, domain expansion is obtained at the cost of certain performance drop in the source domain. In other words,  will be less effective than its teacher  on the source domain, but perform better on the expanded domain, which is the goal of this research.

As for the target domain,  will be better than its teacher . According to Ben-David \etal \cite{ben2007analysis}, the target domain error of a UDA model is bounded mainly by its classification error in the source domain and the divergence between the induced source marginal and the induced target marginal. In theory, a UDA model shall be trained to simultaneously minimize the two terms and reduce accordingly the up bound of the target domain error. In practice, however, the classification error in the source domain is not effectively reduced when compared to the source-only model. This is confirmed by our experiments in Section \ref{sec:exp} that a number of present-day UDA models including DDC~\cite{ddc}, DANN~\cite{jmlr16-dann}, DAAN~\cite{icdm19-daan} and CDAN~\cite{nips18-cdan} suffer performance loss in the source domain. With knowledge distillation,  effectively integrates the merits of  for minimizing the domain divergence and  for minimizing the source error, and thus lowers the up bound of the target domain error. 

Through knowledge distillation, KDDE injects the dark knowledge of the source-only model , which performs well on the source domain, and the domain-adapted model , which is supposed to perform well on the target domain, into a single model . The effect of knowledge distillation on  is visualized in Fig. \ref{fig:heatmaps}. By contrast, although CDAN also uses one classifier for both domains, the classifier is essentially  used in KDDE. Hence, it is less effective than  to handle the expanded domain. Also notice that the purpose of knowledge distillation is not to reduce the difference between the two teacher models, see Eq. \ref{eq:loss-kd}. Therefore, KDDE is conceptually different from Maximum classifier discrepancy (MCD) \cite{cvpr2018-mcd}, which is to reduce the discrepancy between two domain-adapted classifiers via a novel adversarial learning mechanism.


\begin{figure*}[tbh!]
\setlength{\abovecaptionskip}{5pt}
\centering
\includegraphics[width=0.55\textwidth]{diagrams/heatmaps}
\caption{\textbf{Grad-CAM heatmaps of ResNet-50, CDAN and KDDE}. Heatmaps highlight regions important for a model to make predictions. Test images in the first three rows are from the source domain (\textit{clipart}), while images in the last three rows are from the target domain (\textit{real}). Activated regions of KDDE () match well with those of ResNet-50 () on the source domain and those of CDAN () on the target domain, suggesting the ability of KDDE to adaptively learn from the two domain-specific models. Data from DomainNet.}
\label{fig:heatmaps}
\end{figure*} 














%
