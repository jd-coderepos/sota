

\section{Experiments}
\label{sec:experiments}

\noindent \textbf{Datasets and evaluation measures.} For evaluation of methods, we employed two popular challenging benchmark datasets for Aerial Image Detection, namely the VisDrone \cite{zhu-VisDrone-2018} and DOTA \cite{xia-DOTA-2018} datasets. The measure used for assessing and comparing the performance of methods is COCO style average precision (AP) \cite{mscoco-Lin-2014}. The AP of small, medium and large objects are also reported, particularly to understand the performance of our method for small object detection. Finally, the number of frames per second (FPS) is reported as a measure of time complexity.

\noindent \textbf{VisDrone.} This dataset contains  8,599 drone-captured images (6,471
for training, 548 for validation, and 1,580 for testing) with a resolution of about 2000 $\times$1500 pixels. The objects are from ten categories with 540k instances annotated in the training set, mostly containing different categories of vehicles and pedestrians observed from drones. It has an extreme class imbalance and scale imbalance making it an ideal benchmark for studying small object detection problems. As the evaluation server is closed now, following the existing works, we used the validation set for evaluating the performance.

\noindent \textbf{DOTA.} This dataset is comprised of satellite images. The images in this dataset have a resolution ranging from 800$\times$800 to 4000$\times$4000. Around 280k annotated instances are present in the dataset. The objects are from fifteen different categories, with movable objects such as planes, ships, large vehicles, small vehicles, and helicopters. The remaining ten categories are roundabouts, harbors, swimming pools, etc. Many density crop based detection papers reports results only on movable objects\cite{clusnet-Yang-2019} with the assumption that immovable objects usually won't appear crowded. But they are also small objects, so we kept all classes to assess the improvement in small object detection. The training and validation data contain 1411 images and 458 images, respectively.

\noindent \textbf{Implementation details.} The Detectron2 toolkit \cite{detectron2-wu-2019} was used to implement our CZ detector. The backbone detector used in our study is primarily Faster RCNN \cite{faster_rcnn-Ren-2015}, but we also show results on the modern anchor-free one-stage detector FCOS \cite{fcos-Tian-2019}. This validates our claim that our approach gives a consistent improvement in performance regardless of the detector used. We used Feature Pyramid Network (FPN) \cite{fpn-Lin-2017} backbone with ResNet50 \cite{resnet-He-2016}  pre-trained on ImageNet \cite{imagenet-Russakovsky-2015} dataset for our experimental validation. For data augmentation, we resized the shorter edge to one randomly picked from (800, 900, 1000, 1100, 1200), and applied horizontal flip with a 50\% probability. The model was trained on both datasets for 70k iterations. The initial learning rate is set to 0.01 and decayed by 10 at 30k and 50k iterations. For training, we used one NVIDIA A100 GPU with 40 GB of memory.


\subsection{Comparison with Baselines}

Table \ref{table:comparison_with_uniform_crops} presents a comparison between uniform cropping and density cropping on the VisDrone dataset, with and without the last feature map of the feature pyramid (P2), which has a strong impact in memory and computation \cite{querydet-Yang-2022}. For the uniform cropping, we crop the original image into 4 equal-sized crops by splitting at half height and width. In order to have a fair comparison, we use our method with a confidence threshold of 0.7 to obtain an average of  1-3 crops per image. The observations in the table suggest that uniform cropping improves performance compared to vanilla training on the whole image, but it is still inferior to our density-based cropping. When high-resolution feature maps P2 are not used, density cropping gains more than 3.5 points in AP and the AP of small objects is improved by 3.4 points. It is worth noting that compared to uniform cropping, our approach introduces additional parameters to recognize one extra class and no changes in learning and inference dynamics. So this can be easily used as a plug-and-play replacement for the uniform crop-based training, popular among the community. In terms of frame rate, our approach is slightly slower than uniform crops. However, we observe that our method without the expensive P2 features performs better than uniform crops with P2, while also being faster. In Figure \ref{fig:clusters}, a visual comparison of the highly confident detections between the baseline model and our density crop-based model is shown. When the density crops are used, we can observe an increase in the number of detections. It can be observed that more objects are getting discovered in the crop regions when the detection results from the second inference are augmented. This explains the impact of our zoom-in detector for small object detection in high-resolution images.

\setlength{\textfloatsep}{0.3cm}
\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{l||rrr|rrr|r}
    \textbf{Settings} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} & \textbf{FPS}\\
    \hline
    \emph{Without P2} & & & & & & & \\
    Baseline  & 29.48 & 51.68 & 29.55 & 22.33 & 38.66 & 39.30 & 26.31\\
    Uniform crops & 30.68 & 54.44 & 30.54 & 22.91 & 40.62 & 41.03 & 12.30\\
    CZ Det. (ours) & 33.02 & 57.87 & 33.09 & 25.74 & 42.93 & 41.44 & 11.64\\
    \hline    
    \emph{With P2} & & & & & & & \\
    Baseline & 30.81 & 55.06 & 30.68 & 23.97 & 39.19 & 41.17 & 18.25\\    
    Uniform crops & 31.73 & 56.31 & 31.57 & 25.13 & 40.41 & 41.06 & 9.85\\
    CZ Det. (ours) & 33.22 & 58.30 & 33.16 & 26.06 & 42.58 & 43.36 & 8.44\\
    
    \end{tabular}}
    \caption{Comparison of detection performance between a baseline detector, uniform crops and density crops on VisDrone dataset (1.5K pixel resolution). }
    \label{table:comparison_with_uniform_crops}
\end{table}

\begin{figure*}
\centering
\begin{tabular}{c @{\hspace{-0.3cm}} c  @{\hspace{-0.3cm}} c}
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000154_00801_d_0000001_GT.jpg} &
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000154_00801_d_0000001_det_Base.jpg} &
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000154_00801_d_0000001_det_Dcrop_0.6.jpg} \\
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000155_00001_d_0000001_GT.jpg} &
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000155_00001_d_0000001_det_Base.jpg} &
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000155_00001_d_0000001_det_Dcrop_0.6.jpg} \\
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000330_00201_d_0000801_GT.jpg} &
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000330_00201_d_0000801_det_Base.jpg} &
\includegraphics[width=0.32\linewidth, height=3cm]{images/clusters/0000330_00201_d_0000801_det_Dcrop_0.6.jpg} \\
 (a) image \& GT & (b) baseline detection & (c) detection with crops \\
 \end{tabular}
 \caption{Visualization of density crop-based detection. (a) the original image and its GT. (b) detection with the baseline detector. (c) detection with density crops; the density crops are shown in red color. Our method detects more objects, especially inside the crop regions.}
 \label{fig:clusters}
\end{figure*}

To further verify the observations, we repeated the same type of study in the satellite images of the DOTA dataset. In this dataset images are at higher resolution(4k pixels), thus due to memory constraints, the baselines are already performing uniform cropping. Table \ref{table:dota_results} shows the results of a uniform cropping baseline and our CZ detector for two different configurations. Similar to VisDrone, significant improvement is seen in the case of not using high-resolution features P2, with a gain of 2.9 points. APs of small and medium objects are improved by 3.0 and 4.0 points respectively from the baseline without using high-resolution features. In terms of computation, we can see that, as expected, our approach has a slightly slower frame rate than the baseline. However, this is compensated by the higher detection accuracy. We see for instance that the best baseline with P2 features has an AP of $33.44\%$ with an FPS of $0.49$, while our CZ detector without P2 features has a higher AP ($34.14\%$) while being also faster (0.62 FPS).
\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{l||rrr|rrr|r}
    \textbf{Settings} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} & 
    \textbf{FPS} \\  
    \hline
    \emph{Without P2} & & & & & &\\
    Baseline  & 31.29 & 51.57 & 33.10 & 12.69 & 34.04 & 42.83 & 0.93 \\
    CZ Det. (Ours) & 34.14 & 56.69 & 35.69 & 15.66 & 38.16 & 44.20 & 0.62 \\
    \hline
\emph{With P2} &&&&&& \\
    Baseline & 33.44 & 54.03 & 35.56 & 16.86 & 36.76 & 43.65 & 0.49 \\
    CZ Det. (Ours) & 34.62 & 56.86 & 36.17 & 18.17 & 37.84 & 43.83 & 0.30 \\
    \end{tabular}}
    \caption{Performance comparison of our method against baselines on DOTA dataset (4K pixel resolution).}
    \label{table:dota_results}
\end{table}

\subsection{Ablation Studies}



The effectiveness of our proposed CZ detector is characterized by ablation experiments on the VisDrone dataset. Additional studies showing the impact of hyperparameters in the crop labeling algorithm, different backbone networks for the detector, and an analysis of the detection errors are presented in the supplementary material.

\noindent \textbf{a) Density Crops effect at Training and Inference.}  
We used density crops at the training and test time to achieve optimal performance. In particular, while training, the rescaled density crops are augmented with the training images; while testing we do the two-stage inference where stage one perform inference on the whole image and stage two performs inference on the density crops. In this section, we study the importance of this configuration. Table \ref{table:with_and_without_crops} shows the results. When the density crops are not augmented with the training set but only used in the two-stage inference, the improvement is marginal over the baseline(most importantly, $\textrm{AP}_{s}$ has no change). This is because the scale imbalance in the input image is not mitigated as the detector is not seeing the small objects at a bigger scale. When density crops are added to the training set, the detection accuracy improves significantly. However, the inference is still happening on the whole image so the detection accuracy of small objects is affected. When inference is performed on the density crops and fused with the detection on the whole image, we get the best results.
\setlength{\textfloatsep}{0.3cm}
\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{rr||rrr|rrr}
    \textbf{Train} & \textbf{Test} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} \\  
    \hline
     &  & 29.48 & 51.68 & 29.55 & 22.33 & 38.66 & 39.30 \\
      & $\checkmark$ & 29.93 & 53.29 & 29.52 & 22.33 & 39.35 & 39.46 \\
    $\checkmark$ &  & 32.64 & 57.36 & 32.78 & 24.81 & 43.04 & 41.07 \\
    $\checkmark$ & $\checkmark$ & 33.02 & 57.87 & 33.09 & 25.74 & 42.93 & 41.44 \\
    \end{tabular}}
    \caption{Detection results with and without density crops at train time and test time.}
    \label{table:with_and_without_crops}
\end{table}


\noindent \textbf{b) Impact of the Quality of Crops.}  
Figure \ref{fig:conf_impact} illustrates how the confidence of crops impacts the detection accuracy and the number of density crops extracted. The impact is studied for two settings, with and without the high-resolution features P2. This is to verify how the density crops aid small object detection with and without utilizing expensive high-resolution feature maps. The crop confidence, which is used as the proxy for crop quality,  is varied from 0.1 to 0.9. In general, with lower confidence values, we are observing more crops but many of them are noisy and redundant even after Non-Maximal Suppression. So when the quality of the crops is low, the detection accuracy decreases (Figure \ref{fig:conf_impact} left). When the quality is increased, the accuracy increases until 0.7, and then it is gradually coming down as we use very few crops in that case. The trend is the same with and without P2.

From Tables \ref{table:comparison_with_uniform_crops} and \ref{table:dota_results}, we observed that density crops obtained better gain in detection accuracy over the baseline without high-resolution features. Though this is expected, we decided to understand how exactly this is happening. We analyzed the number of density crops retained after filtering out the low-quality crops at multiple confidence levels ranging from 0.1 to 0.9. Figure \ref{fig:conf_impact} right shows the results with and without high-resolution features P2. It can be observed that for "without P2", we are getting more density crops at all confidence levels. With higher crop confidence levels, we get more high-quality crops for "without P2" case, hence we observe a better gain in detection accuracy over the baseline. We used a confidence of 0.7 in all our experiments to have the best trade-off between detection precision and speed. While other methods use post-processing on the crop detections \cite{clusnet-Yang-2019} or density maps \cite{dmap-Li-2020} to filter the noisy crops during inference, we can filter them out based on their confidence score simplifying the inference procedure.
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.39]{images/conf_impact_line.png} 
  \caption{Change in detection precision and the number of crops according to crop confidence. The crop confidence is varied from 0.1 to 0.9. The crop confidence for best detection accuracy is 0.7.}
  \label{fig:conf_impact}
\end{figure}

\noindent \textbf{c) Why Iterative Merging for Crop Discovery?}  
Simply scaling and doing a one-step merging operation to create density crops results in sub-optimal crops. We empirically verify this with multiple scaling strategies and argue that the iterative merging strategy is superior to them. Authors of \cite{clusnet-Yang-2019} also used iterative crop merging on the output of their crop detection module to reduce the redundant crops. This has to be performed at training and test time to refine the initial crop detections. To label the crops for training, they used a single-step aggregation. Our iterative merging for labeling crops can be performed as a pre-processing step before training. We avoided redundant crops at inference, by filtering them out based on the confidence score.

Table \ref{table:scale_by_factor} top provides the comparative results of single-step merging with our iterative merging strategy where GT boxes are scaled by a scaling factor. Using a low scaling factor creates too many crops, containing fewer objects. More specifically, it is producing multiple small crops containing fewer objects in crowded regions in the image. When the scaling factor is increased, the number of crops decreases and performance increases up to a point but declines later as the crops become too big and the object density of the image is less respected. This is because large scaling factors significantly blow up the big GT boxes and it alters the density of the crops. The detection performance obtained is also far below our iterative merging. Table \ref{table:scale_by_factor} bottom shows the same comparison when GT boxes are scaled by constant pixel values. As this avoids the blowing of large bounding boxes due to the constant scaling, the detection performance is better than the former one. Iterative merging produces the optimal number of crops with the best performance. The scaling used in the iterative merging is small and only performed at the first stage of merging. We used 20 pixels as the scaling magnitude. Large values are not possible here since the \texttt{filter\_size} operation while restricting the crop size will reduce the number of crops. Thus it is easy to set. In the supplementary material, we provide more experiments studying the ease of tuning the hyperparameters $N, \pi, \theta$ of the crop labeling algorithm, and the visualization of the crops obtained by different merging strategies.


\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r||r|rrr|rrr}
    \textbf{Scaling} & \textbf{\# crops} &  \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} \\  
    \hline  
    Baseline & 0 & 29.48 & 51.68 & 29.55 & 22.33 & 38.66 & 39.30 \\
    \hline
    factor = 2.0 & 74417 & 24.39 & 44.38 & 23.73 & 15.96 & 34.96 & 47.24 \\
    3.0 & 67906 & 30.64 & 53.71 & 30.68 & 23.23 & 40.64 & 39.35 \\
    6.0 & 43300 & 31.30 & 55.33 & 31.42 & 23.79 & 41.06 & 38.40 \\
    8.0 & 34663 & 30.95 & 55.18 & 30.31 & 23.38 & 40.93 & 39.24 \\
    \hline
    pixels = 30 & 62677 & 31.26 & 54.55 & 31.50 & 23.83 & 40.78 & 50.07 \\
    60 & 46753 & 31.98 & 55.84 & 32.07 & 25.12 & 41.11 & 45.52 \\
    90 & 35442 & 31.47 & 55.62 & 30.96 & 24.03 & 41.27 & 44.08 \\
    120 & 25146 & 31.07 & 54.74 & 30.95 & 23.18 & 41.43 & 42.39 \\
    \hline
    Ours & 14018 & 33.02 & 57.87 & 33.09 & 25.74 & 42.93 & 41.44 \\
    \end{tabular}}
    \caption{Comparison of iterative merging strategy with single-step merging where GT boxes are scaled according to scaling factors, and scaled uniformly by pixel values.}
    \label{table:scale_by_factor}
\end{table}

\subsection{Results with Other Detectors}
To validate the effectiveness of our approach with other detection architectures, we conducted experiments on the modern anchor-free one-stage detector FCOS \cite{fcos-Tian-2019}. Table \ref{table:fcos_result} shows the performance comparison of the vanilla FCOS detector with our density crop based FCOS detector. Similar to the results in table \ref{table:comparison_with_uniform_crops}, \textrm{AP} is improved by a significant margin, and $\textrm{AP}_{s}$ has gained almost 5 points. 

\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r||rrr|rrr|r}
    \textbf{Settings} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} & \textbf{FPS} \\  
    \hline
    Base FCOS & 29.51 & 50.40 & 29.92 & 21.25 & 40.51 & 37.29 & 26.01\\
    CZ FCOS Det. & 33.67 & 56.20 & 34.15 & 26.16 & 43.98 & 46.87 & 12.69\\
    \end{tabular}}
    \caption{Results with anchor free detector FCOS on the Visdrone dataset. All results are without using P2.}
    \label{table:fcos_result}
\end{table}

\subsection{Comparison with State-of-the-Art Methods}
Table \ref{table:visdrone_sota_comparison} compares our approach with the existing methods on the VisDrone dataset. Similarly to us, some methods perform density cropping \cite{clusnet-Yang-2019,dmap-Li-2020,dmap-Duan-2021,glsan-Deng-2020}, while QueryNet\cite{querydet-Yang-2022} and CascadeNet \cite{cascadenet-Zhang-2019} use other approaches to improve the detection performance on aerial images. We obtained the best detection AP among the state-of-the-art methods. Only for large objects, DensityMap performs better than our approach. This is probably because our method gets biased to detect small objects, thanks to the additional crops on training. In fact, for small object detection, we obtained the best $\textrm{AP}_{s}$, significantly outperforming all existing approaches. $\textrm{AP}_{m}$ also shows a good improvement of more than 2 points.

\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{l||rrr|rrr}
    \textbf{Method} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} \\ 
    \hline   
    ClusterNet \cite{clusnet-Yang-2019} & 26.72 & 50.63 & 24.70 & 17.61 & 38.92 & 51.40 \\
    DensityMap \cite{dmap-Li-2020} & 28.21 & 47.62 & 28.90 & 19.90 & 39.61 & \textbf{55.81} \\
    CDMNet \cite{dmap-Duan-2021} & 29.20 & 49.50 & 29.80 &  20.80 & 40.70 & 41.60 \\
    GLSAN \cite{glsan-Deng-2020} & 30.70 & 55.40 & 30.00 & - & - & - \\ 
    QueryDet \cite{querydet-Yang-2022} & 28.32 & 48.14 & 28.75 & - & - & - \\
    CascadeNet \cite{cascadenet-Zhang-2019} & 28.80 &  47.10 & 29.30 & - & - & - \\
    CascNet+MF \cite{cascadenet-Zhang-2019} & 30.12 &  58.02 & 27.53 & - & - & - \\    
    \hline 
CZ Det. (Ours) & \textbf{33.22} & \textbf{58.30} & \textbf{33.16} & \textbf{26.06} & \textbf{42.58} & 43.36 \\
    \end{tabular}}
    \caption{Performance of our proposed method compared against state-of-art approaches with Faster RCNN detector on the VisDrone validation set. "MF" stands for model fusion. The best results in each column are highlighted in \textbf{bold}.}
    \label{table:visdrone_sota_comparison}
\end{table}


\begin{comment}
\begin{figure*}
\centering
\begin{tabular}{c @{\hspace{0.5ex}} c @{\hspace{0.5ex}} c @{\hspace{0.5ex}} c}
\includegraphics[width=0.24\linewidth]{images/clusters/cluster1_GT.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster1.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster1_no_crop.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster1_det_with_cluster.png} \\
\includegraphics[width=0.24\linewidth]{images/clusters/cluster5_GT.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster5.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster5_no_crop.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster5_det_with_cluster.png} \\
\includegraphics[width=0.24\linewidth]{images/clusters/cluster7_GT.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster7.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster7_no_crop.png} &
\includegraphics[width=0.24\linewidth]{images/clusters/cluster7_det_with_cluster.png} \\

 (a) image \& GT & (b) Crop regions & (c) baseline detection & (d) detection with crops\\
 \end{tabular}
 \caption{Visualization of the impact of using density crops. (a) the original image and its GT annotations. (b) the dense object regions detected in the image. (c) detection on the full image with the baseline detector. (d) detection results when the results from original images and density crops are fused. It can be observed that with density crops, the method is detecting more objects, especially from inside the crop regions discovered.}
 \label{fig:clusters}

\subsection{Discussion}
Empirical studies reiterate the benefits of our cascaded zoom-in detection. As our approach shifts its focus towards small object detection, the AP of large objects are slightly affected. From table \ref{table:visdrone_sota_comparison}, we can observe that $\textrm{AP}_l$ of our method is lower than other approaches. As the aerial images are dominated by small objects, we are still seeing an improvement in overall AP. For the crop-labeling, we used a simple iterative merging algorithm as outlined in \ref{alg:crop_discovery}. Ablation studies in table \ref{table:scale_by_factor} \& \ref{table:scale_by_pixel} indicate that, although sub-optimal, simple scale and merge also work and results are still better than other approaches and uniform crops for many scaling parameters. This confirms our belief that we need a consistent signal of what is a "density crop" during the training. In this regard, if there are other better algorithms for crop labeling than our iterative merging, we can obtain more improvements.

\begin{comment}
\begin{figure}
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/tide_analysis/baseline_27_2.png}
         \caption{Baseline}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/tide_analysis/with_crop_29_2.png}
         \caption{With density crops}
     \end{subfigure}
        \caption{TIDE error analysis: baseline vs With density crops}
        \label{fig:tide_error_analysis}
\end{figure}



\begin{table}
    \centering
    \resizebox{.48\textwidth}{!}{\begin{tabular}{r|rrr|rrr}
    \textbf{Backbone} & \textbf{AP} & \textbf{$\textrm{AP}_{50}$} & \textbf{$\textrm{AP}_{75}$} & \textbf{$\textrm{AP}_{s}$}  & \textbf{$\textrm{AP}_{m}$} & \textbf{$\textrm{AP}_{l}$} \\
    \hline   
    ResNet50(without P2) & 33.02 & 57.87 & 33.09 & 25.74 & 42.93 & 41.44 \\
    ResNet50(with P2) & 33.22 & 58.30 & 33.16 & 26.06 & 42.58 & 43.36 \\
    \hline
    ResNet101(without P2) & 33.91 & 59.07 & 33.87 & 26.14 & 44.64 & 45.03 \\
    ResNet101(with P2) & 34.36 & 59.65 & 34.55 & 26.96 & 44.23 & 42.14 \\    
    \end{tabular}}
    \caption{With ResNet-101 backbone}
    \label{table:with_other_backbones}
\end{table}
\end{comment}
