\begin{table*}[]
\scriptsize
    \centering
\begin{tabular}{l|ll|l|llll|l|l}
            
            
            \toprule
            &\multicolumn{3}{c|}{VL Checklist } & \multicolumn{5}{c|}{ARO } & Zero-Short\\
            & Relation & Attribute & \textbf{Average} & VG-Rel. & VG-Att. & Flickr30k & COCO & \textbf{Average} & (21 tasks)\\
            \midrule            

            CLIP & 63.57 & 67.51 & 65.54 & 58.84 & 63.19 & 47.20 & 59.46 & 57.17 & 56.07\\
            CyCLIP & 61.15 & 66.96 & 64.06 & 59.12 & 65.41 & 20.82 & 29.54 & 43.72 & 55.99\\
            \midrule
            \ours{}CLIP & 69.39\gcol{+5.82} 	& 70.37\gcol{+2.86} 	& 69.88\gcol{+4.34} 	& 71.40\gcol{+12.56}	&	66.94\gcol{+3.75}	&	59.06\gcol{+11.86}	&	70.96\gcol{+11.5} & 67.09\gcol{+9.9} & 55.27\rcol{-0.8} \\
            \ours{}CyCLIP & 65.73\gcol{+4.58} & 68.06\gcol{+1.1} & 66.89\gcol{+2.83} & 69.02\gcol{+9.9} & 63.65\rcol{-1.76} & 49.17\gcol{+28.35} & 59.36\gcol{+29.82} & 60.30\gcol{+16.58} & 55.40 \rcol{-0.6}\\
            \bottomrule
    \end{tabular}
    
\vspace{0.15in}
    \caption{Performance of \ours{}\textit{model}s -- finetuned on \ourdataset{} using our proposed recipe, measured on \vlchecklist{}~\cite{vlc} and \ARO{}~\cite{aro}. Gains and losses are highlighted in \green{green} and \red{red} respectively.}
    \label{tab:main_res_vl1}
\end{table*}

\begin{table}[]
\scriptsize
    \centering
\begin{tabular}{l|lll|lll}
            
            
            \toprule
            & \multicolumn{3}{c|}{Winoground}& \multicolumn{3}{c}{Winoground}\\
            & Text & Image & \textbf{Group} & Text & Image & \textbf{Group}\\
            \midrule

            CLIP &  31.25 & 10.50 & 8.00 & 31.58 &	10.53 & 8.19\\
            \midrule
            \ours{}CLIP & 30.00 & 11.50 & 9.50\gcol{+1.50} & 29.82 & 12.28 & 9.94\gcol{+1.75} \\
            \bottomrule
    \end{tabular}
    
\vspace{0.15in}
    \caption{\winoground{}~\cite{winoground} performance of \ours{}CLIP -- finetuned on \ourdataset{}.
    The \ours{}CyCLIP results on \winoground{} are provided in the Supplementary.
     `clean' (no-tag) subset of valid \winoground{} samples from~\cite{why_is_winoground_hard}}
    \label{tab:main_res_vl2}
    \vspace{-0.2in}
\end{table}




 
\section{Experiments} \label{sec:exp}
\subsection{Implementation details}\label{sec:impl}
For CLIP, we use the original OpenAI CLIP implementation and checkpoints. We modify their codebase to include LoRA adapters (Sec. 3.2), and use rank 16 in all our experiments. For CyCLIP, we adapt the implementation used in~\cite{ours_teaching}~\footnote{Code and checkpoints kindly shared by the authors.}. For both CLIP and CyCLIP, we use a 5e-7 initial learning rate for finetuning and follow a cosine annealing learning rate schedule~\cite{cosine_annealing} using an Adam~\cite{adam} optimizer. For all experiments, we use ViT/32-B as the model architecture and fine-tune it for six epochs on one A100 GPU with a total batch size of  image-caption pairs. In addition to the original CLIP data augmentation transforms, we apply heavy random augmentation policies including manipulations in image inversion, contrast, sharpness, equalization, posterization, colorization, brightness, and solarization.

\subsection{Datasets}\label{sec:datasets}
To test the effectiveness of our proposed \ourdataset{} synthetic dataset and the accompanying finetuning approach for improving \vl{} models' \vlc{} understanding and compositional reasoning capabilities we have evaluated on 3 benchmarks (\winoground{}~\cite{winoground}, \vlchecklist{}~\cite{vlc}, and \ARO{}~\cite{aro}) consisted of 7 datasets total.

\noindent\textbf{\vlchecklist{}~\cite{vlc}} -- is a large-scale dataset comprised of: Visual Genome~\cite{vg}, SWiG~\cite{swig}, VAW~\cite{vaw}, and HAKE~\cite{hake}. Each image of these datasets is associated with two captions, a positive and a negative. The positive caption corresponds to the image and is taken from the source dataset. The negative caption is made from the positive caption by changing one word, so the resulting sentence no longer corresponds to the image. Depending on the word that was changed, VL-Checklist evaluates 7 types of \vlc{} that can be divided into two main groups: (1) Attributes -- color, material, size, state, and action, and (2) Relations -- spatial or action relation between two objects and/or humans. In the following, we report average results for each of the main (Rel. and Attr.) groups on the combined \vlchecklist{} dataset. We also detail the individual improvements on all 7 \vlc{} types in Fig.~\ref{fig:radar} (left).

\noindent\textbf{\winoground{}~\cite{winoground}} -- is a small dataset that evaluates the ability of \vl{} models for compositional reasoning, specifically understanding the meaning of the sentence after changing the order of its words. The dataset has 400 samples, each comprised of two images and two texts. The texts have the same words in a different order, each text corresponding to one image in the sample. The \winoground{} metrics include (a) image score - percent of samples where the model picks the correct text for each image; (b) text score - percent of samples where the model picks the correct image for each text; (c) group score - percent of samples where both text and image score conditions are satisfied jointly. Recently,~\cite{why_is_winoground_hard} has analyzed \winoground{} for the source of its difficulty and found that only  of its 400 samples are a valid subset. Other samples are not compositional, ambiguous, related to invisible details, have highly uncommon images or text, or require complex reasoning beyond compositionality. We report results on both the full \winoground{} and the `clean' 171 images subset from~\cite{why_is_winoground_hard}.

\noindent\textbf{\ARO{}~\cite{aro}} -- or the Attribution, Relation, and Order benchmark, is a large dataset designed to evaluate the ability of \vl{} models to understand four different types of skills. It consists of Visual Genome Attribution and Visual Genome Relation, which leverages the Visual Genome~\cite{vg} dataset along with the GQA~\cite{gqa} annotations to test the understanding of properties and relational understanding of objects in complex natural scenes. VG-Relation includes  distinct relations with  test cases, and VG-Attribution includes  unique attribute pairs with  test cases. It also leverages the COCO~\cite{coco} and Flickr30k~\cite{flickr30k} datasets to evaluate the model sensitivity to select the right caption after applying four different shuffling perturbations (e.g., exchanging nouns and adjectives, or by shuffling trigrams). These tests are performed on the  and the  images from the respective COCO and Flickr30k test splits.

\vspace{-.05em}
\subsection{Results}
\label{sec:results}
The main results of finetuning CLIP~\cite{clip}, CyCLIP~\cite{cyclip} -- one of CLIP's most recent improvements
are summarized in Tables~\ref{tab:main_res_vl1} and~\ref{tab:main_res_vl2}. All models were finetuned using our proposed approach and \ourdataset{} synthetic data to obtain their \ours{}\textit{model} variants. Each model is compared to its respective source model pre-trained on large-scale real data before finetuning on \ourdataset{}. As we can observe, our \ourdataset{} synthetic data and the proposed finetuning recipe on this data demonstrate significant improvements over their source baselines. E.g. for CLIP obtaining , , and  average absolute improvement in \winoground{} group score (most difficult average metric), \vlchecklist{} and \ARO{} respectively. In addition, we illustrate the individual \vlc{} metrics improvements obtained for CLIP in \vlchecklist{} and \ARO{} benchmarks in Fig.~\ref{fig:radar} showing up to  and  respective absolute improvements. This underlines the effectiveness and promise of our method and \ourdataset{} synthetic data towards improving \vlc{} understanding and compositional reasoning in \vl{} models. Importantly, as we can see from Table~\ref{tab:main_res_vl1}, these strong gains come at a very small (under 1\%) cost in the zero-shot performance of the respective \vl{} models measured using the standard Elevater~\cite{elevater} benchmark using 21 diverse zero-shot tasks.

\begin{table}[]
\scriptsize
    \centering
\begin{tabular}{cc|ll|l}
            \toprule
            objects with attr. & humans & \multicolumn{3}{c}{VL Checklist} \\
            randomization & & Relation & Attribute & Average \\
            \midrule            
            \multicolumn{2}{c|}{CLIP} & 63.57 & 67.51 & 65.54 \\ 
            \midrule
            \xmark\ & \cmark\ & 64.03	& 67.09	& 65.56 \\ 
            \cmark\ & \xmark\ & 65.00 & 68.15 & 65.95 \\ 
            \cmark\ & \cmark\ & \textbf{69.39} & \textbf{70.37} & \textbf{69.88} \\

            \bottomrule
    \end{tabular}
\vspace{0.15in}
    \caption{Importance of human avatars, objects, and object attribute variations, evaluated on \vlchecklist{} and CLIP
    }
    \label{tab:abl-humans-objects}
\vspace{-2em}
\end{table}


\subsection{Ablations}\label{sec:ablations}
We extensively ablate our \ourdataset{} synthetic data and the proposed \vl{} models finetuning approach on this data according to the following points. We use the most popular CLIP model finetuned on our \ourdataset{} synthetic dataset evaluated on the largest of the benchmarks - the \vlchecklist{} to perform our ablations.

\noindent\textbf{\ourdataset{} - objects, humans, object attribute randomization} -- we evaluate the major components that comprise our \ourdataset{} synthetic data, namely the importance of the synthetic data to contain humans performing various motions and actions, the importance of having objects with randomized attributes (Sec.~\ref{sec:syndata}), and the final result of having all types of data combined. The results of this ablation are summarized in Tab.~\ref{tab:abl-humans-objects}. As expected, humans alone cannot teach the model the needed skills only improving relations \vlc{} by a small margin. Additionally, having only objects with randomized attributes improves attribute \vlc{}, yet only improves relations by  which is also expected, as many of the relations involve human actions. The best result is observed on the combined dataset with all the components.

\begin{table}[]
\scriptsize
    \centering
\begin{tabular}{cc|ll|l}
            \toprule
            SURREAL & Multi-Garment & \multicolumn{3}{c}{VL Checklist} \\
            & & Relation & Attribute & Average \\
            \midrule            
            \multicolumn{2}{c|}{CLIP} & 63.57 & 67.51 & 65.54  \\
            \midrule
            \xmark\ & \xmark\ & 67.56 & 68.73 & 68.15 \\ 
            \cmark\ & \xmark\ & 67.56 	& 67.26 	& 67.41 \\
            \xmark\ & \cmark\ & \textbf{69.39} & \textbf{70.37} & \textbf{69.88} \\
            \bottomrule
    \end{tabular}
\vspace{0.15in}
    \caption{Importance of human avatar clothing choices between SURREAL, Multi-Garment, and simple color textures (corresponding to none), evaluated on \vlchecklist{} and CLIP}
    \label{tab:abl-clothing}
\vspace{-0.1in}
\end{table}

\noindent\textbf{\ourdataset{} - human clothing} -- we evaluate the diversity of human clothing comparing 3 levels of diversity: (i) none - using only a uniform color for human models; (ii) basic - using less diverse texture maps from SURREAL~\cite{varol17_surreal}; and (iii) most diverse - using texture maps from Multi-Garment~\cite{bhatnagar2019mgn}, enriched with clothing colors, human age, and hair color annotations (manually done by us for the textures) which increase captions' expressivity. Results are presented in Table~\ref{tab:abl-clothing}. As expected, the most diverse human textures deliver the best result underlining the importance of this factor. Surprisingly, better human textures improve \vlchecklist{} Relations metric performance, likely due to the significantly better realism of the Multi-Garment textures. 

\noindent\textbf{\ourdataset{} - types of object attributes to randomize} -- Table~\ref{tab:abl-attributes-randomization} examines how randomizing different object attributes affects performance. Specifically, we evaluate the randomization of size, material, and color. Interestingly, we find that the best performance is achieved without color randomization. We suspect it is due to unnatural color-object combinations that arise under such randomization, which teach the model wrong beliefs on real objects' color distributions and go against true object-color associations existing in the \vl{} model following pre-training on the original \vl{} data.

\noindent\textbf{\ourdataset{} - types of captioning} -- we have investigated several variants of ways to obtain textual captions from \ourdataset{} metadata (Sec.~\ref{sec:syndata}). Results are summarized 
the Supplementary.
We compared our proposed metadata grammar-based approach to two cascade methods that paraphrase the captions resulting from the grammar using zero-shot LLM inference (in-context learning). The paraphrased caption is then appended to the original grammar-based caption and consumed through our caption-splitting module (as standalone, open LLM-based paraphrasing is not very high quality). As can be seen, currently paraphrasing has minimal effect, but we posit it will become an important tool as stronger LLMs will become openly available.

\noindent\textbf{\ourdataset{} - importance of physics and number of humans in the scene} -- we also looked into to which extent reliable physical simulation (made available in \ourdataset{} through TDW and Unity capabilities) and human-human positional and other relations are important for the observed \vl{} model improvements. In Table~\ref{tab:abl-physics-interactions} we evaluate the effects of removing the physics (resulting in humans or objects floating in space) or removing the multi-human scenes (thus preventing all human-human relations from appearing in the data). As expected, both reliable physics simulation and human-human relations (interactions of sorts) are mostly important to the gains in the Relations metric.

\begin{table}[]
\scriptsize
    \centering
\begin{tabular}{ccc|ll|l}
            \toprule
            color & size & material & \multicolumn{3}{c}{VL Checklist} \\
            & & & Relation & Attribute & Average  \\
            \midrule            
            \multicolumn{3}{c|}{CLIP} & 63.57 & 67.51 & 65.54  \\
            \midrule
            \cmark\ & \xmark\ & \xmark\ & 67.71 & 64.61 & 66.16  \\ 
            \xmark\ & \cmark\ & \xmark\ & 68.58 & 68.23 & 68.40  \\ 
            \xmark\ & \xmark\ & \cmark\ & 65.23 & 67.01 & 66.12  \\ 
            \cmark\ & \cmark\ & \cmark\ & 66.67 & 65.97 & 66.32 \\ 
            \xmark\ & \cmark\ & \cmark\ & \textbf{69.39} & \textbf{70.37} & \textbf{69.88} \\ 
            \bottomrule
    \end{tabular}
\vspace{0.15in}
    \caption{Importance of different kinds of object attributes randomization, evaluated on \vlchecklist{} and CLIP}
    \label{tab:abl-attributes-randomization}
\vspace{-0.1in}
\end{table}

\noindent\textbf{\ourdataset{} - number of models and number of samples} -- for lack of space, this is explored in the Supplementary.

\begin{table}[b]
\scriptsize
    \centering
\begin{tabular}{cc|ll|l}
            \toprule
            physics & multi-human & \multicolumn{3}{c}{VL Checklist} \\ 
            & & Relation & Attribute & Average  \\
            \midrule            
             \multicolumn{2}{c|}{CLIP} & 63.57 & 67.51 & 65.54 \\ 
            \midrule
            \cmark\ & \xmark\ & 67.66 & 69.24 & 68.45 \\ 
            \xmark\ & \cmark\ & 65.91 & 69.01 & 67.46 \\ 
            \cmark\ & \cmark\ & 69.39 & 70.37 & 69.88 \\ 
            \bottomrule
    \end{tabular}
\vspace{0.15in}
    \caption{Importance of physics simulation and multi-human relations in \ourdataset{}, evaluated on \vlchecklist{} and CLIP}
    \label{tab:abl-physics-interactions}
\end{table}

\begin{table*}[]
\scriptsize
    \centering
\begin{tabular}{l|ccccc|ll|l|c|c}
            \toprule
            \# & LoRA & freezing & model & DA & Caption &\multicolumn{3}{c|}{VL Checklist} & ARO & ZS \\
            & &  & averaging & styl. & split emb. & Rel. & Attr. & Avg. & Avg. & (21 tasks) \\
            \midrule            
            & \multicolumn{5}{c|}{CLIP} & 63.57 & 67.51 & 65.54 & 57.17 & 56.07 \\
            \midrule
            1 & \xmark\ & \xmark\ & \xmark\ & \xmark\ & \xmark\ & 67.10 & 65.45 & 66.28 & 62.83 & 53.84 \\
            2 & \xmark\ & \xmark\ & \xmark\ & \xmark\ & \cmark\ & 67.76 & 67.51 & 67.64 & 59.27 & 53.87 \\
            3 & \cmark\ & \xmark\ & \xmark\ & \xmark\ & \cmark\ & \blue{69.32} & \blue{69.46} & \blue{69.39} & \blue{64.34} & 53.16 \\
            \midrule
            4 & \cmark\ & \xmark\ & \xmark\ & \cmark\ & \cmark\ & \textbf{69.39} & \textbf{70.37} & \textbf{69.88} & \textbf{67.09} & \blue{55.27} \\
            \midrule
            5 & \cmark\ & \cmark\ & \xmark\ & \cmark\ & \cmark\ & 63.54 & 68.20 & 65.87 & 60.29 & 54.54 \\
            6 & \cmark\ & \xmark\ & \cmark\ & \cmark\ & \cmark\ & 66.70 & \blue{69.62} & 68.16 & 63.76 & \textbf{55.72} \\
            7 & \cmark\ & \cmark\ & \xmark\ & \cmark\ & \xmark\ & 65.16 & 66.88 & 66.02 & 57.55 & 52.69 \\
            \bottomrule
    \end{tabular}
\vspace{0.15in}
    \caption{Importance of the finetuning recipe components, evaluated on \vlchecklist{} and CLIP}
    \label{tab:abl-finetuning-recipe}
    \vspace{-0.15in}
\end{table*}

\noindent\textbf{\ourdataset{} - finetuning recipe components} -- in Table~\ref{tab:abl-finetuning-recipe} we extensively evaluate the different components of the proposed finetuning approach on \ourdataset{} that leads to significant improvements on \vlchecklist{}, \ARO, and \winoground{} \vlc{} and compositional reasoning metrics. We start with vanilla CLIP finetuning on \ourdataset{} (row \#1), already showing some improvement in \vlchecklist{} relations metrics, and on the \ARO{} benchmark, yet losing to base CLIP on \vlchecklist{} attributes metrics. Adding our caption splitting module (Sec.~\ref{sec:training}) allows handling long (arbitrary size) texts outputted by our metadata-driven grammar and consequently utilizes all the caption information re-gaining the attributes performance (row \#2). Adding parameter-efficient finetuning (LoRA, Sec.~\ref{sec:training}) regularizes finetuning by forcing smaller (low-rank, low-parameters) updates of the large-scale pre-trained CLIP model, consequently somewhat handling the expected domain gap between the synthetic data of \ourdataset{} and the downstream evaluation (real data) tasks. Notably, LoRA does not add any additional parameters to the model, all LoRA adapters are collapsed into the model weights after finetuning. Consequently, we observed significant improvements from adding LoRA in all metrics (row \#3) with only minor degradation (0.7\%) in ZS evaluation. With adding domain stylization (Sec.~\ref{sec:training}) we observe the best results in all \vlc{} and compositional reasoning metrics improving \ARO{} by 2.8\% and keeping (even slightly improving) the advantages on \vlchecklist{}. Next, we investigate the variations of our best approach (LoRA + domain stylization + caption splitting). First, we investigate a strategy inspired by the LiT~\cite{lit} approach (row \#5). Freezing the visual encoder  as expected provides a (small) boost in ZS performance, but the reduced plasticity of the model comes at the price of observing smaller (only 3.1\%) improvements on \ARO{} and almost no improvements on the \vlchecklist{}. This leads us to conclude, that freezing the visual encoder is not a good strategy for \ourdataset{} finetuning. Next, we check the model averaging strategy (Sec.~\ref{sec:training}) inspired by~\cite{wortsman2022robust} (row \#6). This does a better job of mitigating ZS forgetting, while at the same time keeping more of the gains on \vlchecklist{} and \ARO{}. We conclude that model averaging is a good strategy to complement \ourdataset{} finetuning, allowing a soft trade-off between mitigating ZS forgetting and \vlc{} and compositionality metrics gains. Finally, we again explore the importance of caption splitting for the best finetuning configuration of \ourdataset{} (row \#7) and re-confirm its significance as performance drops without it.


\begin{figure}[]
\includegraphics[width=0.45\linewidth]{Images/fig_Objs_vs_Actions_CLIPv2.pdf}
\hfill \includegraphics[width=0.50\linewidth]{Images/fig_ARO_Base_vs_Best.pdf}
\label{fig:abl-diversity}
    \caption{\textbf{(left)} detailed evaluation of \ours{}CLIP on the 7 separate \vlchecklist{}~\cite{vlc} metrics; \textbf{(right)} detailed evaluation of \ours{}CLIP on all the Compositional Tasks proposed in \ARO{}~\cite{aro}. } 
    \label{fig:radar}
    \vspace{-1em}
\end{figure}