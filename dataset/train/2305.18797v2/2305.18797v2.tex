






\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{} \pagestyle{plain}



\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{color}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{stfloats}
\usepackage{amsmath, bm}
\usepackage{ragged2e} 
\usepackage{booktabs,makecell, multirow, tabularx}
\usepackage{hyperref}
\usepackage{bbding}
\usepackage{subfigure}         
\usepackage{natbib}
\usepackage{geometry}
\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}

\newcommand{\etal}{{\emph{et al. }}}
\newcommand{\ie}{{\emph{i.e., }}}
\newcommand{\eg}{{\emph{e.g., }}}
\newcommand{\vecc}[1]{{\boldsymbol{\mathbf{#1}}}}
\newcommand{\vX}{{\vecc{X}}}
\newcommand{\vY}{{\vecc{Y}}}
\newcommand{\vx}{{\vecc{x}}}
\newcommand{\vy}{{\vecc{y}}}
\newcommand{\vs}{{\vecc{s}}}
\newcommand{\vV}{{\vecc{V}}}
\newcommand{\vE}{{\vecc{E}}}
\newcommand{\ve}{{\vecc{e}}}
\newcommand{\vsbar}{{\bar{\vs}}}
\newcommand{\vh}{{\vecc{h}}}
\newcommand{\vhbar}{{\bar{\vh}}}
\newcommand{\vz}{{\vecc{z}}}
\newcommand{\vu}{{\vecc{u}}}
\newcommand{\valpha}{\vecc{\alpha}}
\newcommand{\vbeta}{\vecc{\beta}}
\newcommand{\vomega}{\vecc{\omega}}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\Z}{Z_{\vecc{\theta}}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} \newcommand{\Pbar}{{\overbar{P}}}
\newcommand{\gbar}{{\overbar{g}}}
\newcommand{\fbar}{{\overbar{f}}}
\newcommand{\T}{{\mathcal{T}}}
\newcommand{\semiringone}{{\overbar{\vone}}}
\newcommand{\semiringzero}{{\overbar{\vzero}}}
\newcommand{\phat}{{\hat{p}}}
\newcommand{\bos}{\textsc{bos}\xspace}
\newcommand{\eos}{\textsc{eos}\xspace}
\usepackage{stmaryrd}  \newcommand{\norm}[1]{\llbracket #1 \rrbracket}
\newcommand{\psup}[1]{^{(#1)}}   \usepackage{tipa}
\newcommand{\markred}[1]{{\color{RubineRed} #1}}
\newcommand{\markblue}[1]{{\color{Blue} #1}}
\newcommand{\edit}{x}
\newcommand{\defn}[1]{\textbf{#1}}   \newcommand{\Real}{\mathbb{R}}
\newcommand{\E}[2][]{\mathbb{E}_{{#1}}[#2]}   \newcommand{\str}[1]{\vecc{#1}}  


\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\Ent}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\Rel}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\Ent}{\mathcal{E}}
\newcommand{\Rel}{\mathcal{R}}



\newcommand{\poincare}{Poincar\'e }
\newcommand{\lorentz}{\mathbb{L}^n_K}
\newcommand{\linner}[2]{\langle \mathbf{#1},\mathbf{#2}\rangle_\Lc}
\newcommand{\lnorm}[1]{\lVert #1\rVert_\Lc}
\newcommand{\tangent}[1]{\mathcal{T}_{\mathbf{#1}}\mathbb{L}^n_K}
\newcommand{\point}{$\mathbf{p}$}
\newcommand{\pt}[3]{\text{PT}^K_{\mathbf{#1}\rightarrow\mathbf{#2}}(\mathbf{#3})}
\newcommand{\ptmath}[3]{\text{PT}^K_{\mathbf{#1}\rightarrow\mathbf{#2}}(\mathbf{#3})}
\newcommand{\logmap}[2]{\log_#1^K(#2)}
\newcommand{\expmap}[2]{\exp_\mathbf{#1}^K(\mathbf{#2})}
\newcommand{\riemanntensor}[1][x]{\mathfrak{g}_\mathbf{x}^K}
\newcommand{\lorentzdist}{d_{\mathcal{L}}^2}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}







\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}



\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}



\acmConference[arXiv ]{Make sure to enter the correct
conference title from your rights confirmation emai}{preprint}{2023}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\acmSubmissionID{}





\begin{document}






\title{Learning Weakly Supervised Audio-Visual Violence Detection in Hyperbolic Space}




\author{Xiaogang Peng\textsuperscript{1}\footnotemark[1], Hao Wen\textsuperscript{2}\footnotemark[1], Yikai Luo\textsuperscript{1}\footnotemark[1], Xiao Zhou\textsuperscript{1}, Keyang Yu\textsuperscript{1}, Yigang Wang\textsuperscript{1}, Zizhao Wu\textsuperscript{1}\footnotemark[2]}

\affiliation{
\textsuperscript{1}Deparment of Digital Media Technology, Hangzhou Dianzi University, China
\\\textsuperscript{2}Academy for Engineering and Technology, National University of Defense Technology, China
\country{}}

\email{{pengxiaogang, luoyikai, zhouxiao, yukeyang, yigang.wang, wuzizhao}@hdu.edu.cn}
\email{hao.wen@nudt.edu.cn}





















\renewcommand{\shortauthors}{Peng and Wen and Luo, et al.}


\begin{abstract}
In recent years, the task of weakly supervised audio-visual violence detection has gained considerable attention. The goal of this task is to identify violent segments within multimodal data based on video-level labels. Despite advances in this field, traditional Euclidean neural networks, which have been used in prior research, encounter difficulties in capturing highly discriminative representations due to limitations of the feature space. To overcome this, we propose \textbf{HyperVD}, a novel framework that learns snippet embeddings in hyperbolic space to improve model discrimination. Our framework comprises a detour fusion module for multimodal fusion, effectively alleviating modality inconsistency between audio and visual signals. Additionally, we contribute two branches of fully hyperbolic graph convolutional networks that excavate feature similarities and temporal relationships among snippets in hyperbolic space. By learning snippet representations in this space, the framework effectively learns semantic discrepancies between violent and normal events. Extensive experiments on the XD-Violence benchmark demonstrate that our method outperforms state-of-the-art methods by a sizable margin. Code for this paper are at \href{https://github.com/xiaogangpeng/HyperVD}{https://github.com/xiaogangpeng/HyperVD.}
\end{abstract}



\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}



\ccsdesc[100]{Computing methodologies~Scene anomaly detection}





\keywords{Violence Detection, Multi-Modality, Hyperbolic Geometry}



\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal Contribution.}
\footnotetext[2]{Corresponding author.}

\section{Introduction}


With the increase in the volume of digital content and the proliferation of social media platforms, automated violence detection has become increasingly important in various applications such as security and surveillance systems, crime prevention, and content moderation. However, annotating each frame in a video is a time-consuming and expensive process. To address this, current methods often utilize weakly supervised settings to formulate the problem as a multiple-instance learning (MIL) task \cite{c:1,c:2,c:3,c:4,c:5,c:6,c:8,c:13}. These methods treat a video as a bag of instances (\ie snippets or segments), and predict their labels based on the video-level annotations\cite{c:7}. 


\begin{figure}[t]
    \centering
\includegraphics[width=0.45\textwidth]{figures/motivation.png} \caption{Intuitively, there are implicit hierarchical relationships and substantial semantic discrepancies between violent instances and normal instances. These discrepancies can be difficult to capture using traditional Euclidean space methods, which may not be well-suited to represent complex hierarchical structures.}
    \label{fig1}
\end{figure}


Following the MIL paradigm, a number of weakly supervised violence detection methods have been proposed. For example, Zhu \etal\cite{Zhu_Newsam_2019} proposed a temporal augmented network to learn motion-aware features using attention blocks, while Tian \etal\cite{c:1} developed the Robust Temporal Feature Magnitude (RTFM) method to enhance model robustness through temporal attention and magnitude learning. Li \etal\cite{c:9} introduced a transformer-based framework and utilized multiple sequence learning to reduce the probability of selection errors. Furthermore, several multimodal approaches have been proposed, which jointly learn audio and visual representations to improve performance by leveraging complementary information from different modalities\cite{c:2,c:4,pang2021violence,Pu_Wu_2022}. For instance, Wu \etal\cite{c:2} proposed a GCN-based method to learn multimodal representations via graph learning, while \etal \cite{c:4} presented a method that addresses modality asynchrony via modality-aware multiple instance learning. 




Though the above-mentioned approaches have gained promising results, these multimodal methods may suffer heavy modality inconsistency due to the presence of noise in audio signals collected from real-world scenarios. In this case, auditory modality contribute less than visual ones for violence detection. In addition, previous methods have demonstrated the effectiveness of using graph representation learning to detect violent events by regarding each instance as a node in a graph \cite{c:2, c:3}, but they still struggle to differentiate violent and non-violent instances. 


In this paper, we propose a new approach to address these limitations via graph representation learning. To our best knowledge, all the previous methods learn feature representation with deep neural networks in Euclidean space. However, graph-like data is proved to exhibit a highly non-Euclidean latent structure \cite{Bronstein2016GeometricDL, Ying2018HierarchicalGR} that challenges current Euclidean-based deep neural networks. As shown in Figure \ref{fig1}, there exist implicit hierarchical relationships and substantial semantic discrepancies between normal and violent instances, which are difficult to distinguish in Euclidean space. We argue that learning instance representations directly in a data-related space, such as Hyperbolic manifolds, can favor the model discrimination, as it enables the model to capture and differentiate between subtle semantic differences that may hard to be explored in Euclidean space. 







Recently, hyperbolic geometry has shown promising performance in neural networks for modeling complex graph data due to its highly efficient capacity, unique metric properties and geometries. Motivated by these findings, we propose a novel \textbf{HyperVD} framework based on the Lorentz model \cite{Nickel_Kiela_2018} of hyperbolic geometry for weakly supervised audio-visual violence detection. Building the framework on hyperbolic geometry can benefit from the hyperbolic distance, which exponentially increases the distance between irrelevant samples compared to the distance between similar samples. In particular, our approach includes a detour fusion module to address the information inconsistency of the two modalities during fusion, followed by projecting the fused embeddings of audio-visual features onto the hyperbolic manifold. Then we leverage two branches of fully hyperbolic graph convolutional networks to extract feature similarities and temporal relationships among instances in hyperbolic space. Finally, we concatenate the output embeddings from the two branches and use them as input to a hyperbolic classifier for violence prediction. 


To evaluate the effectiveness of our proposed approach, we conduct experiments on the XD-Violence dataset. Under weak supervision, our method can achieve the best performance of 85.67\% AP, outperforming the previous state-of-the-art method by 2.27\%. Extensive ablations also demonstrate the effectiveness of instance representation learning in hyperbolic space. 

In summary, the main contributions are stated as follows:
\begin{itemize}[leftmargin=*]
\item[$\bullet$]We analyze the modality inconsistency in the audio-visual fusion process and the weakness of learning instance representations using traditional Euclidean-based methods.
\item[$\bullet$]We present a novel HyperVD framework for weakly supervised audio-visual violence detection to effectively explore the semantic discrepancy between violent instances and non-violent ones via hyperbolic geometry, leading to more powerful discrimination.
\item[$\bullet$]Experimental results show our framework outperforms the state-of-the-art methods on the XD-Violence dataset. The ablation study further gives insights into how each proposed component contributes to the success of the model.
\end{itemize}



\section{Related Works}
We now describe relevant works related to weakly-supervised violence detection (Sec. 2.1) and neural networks in hyperbolic space (Sec. 2.2).
\subsection{Weakly-Supervised Violence Detection}
Weakly-supervised violence detection aims to identify violent segments in videos by utilizing video-level labels. Since the publication of the first paper~\cite{Ding2014ViolenceDI} utilizing deep learning methods, the field of violence detection has made tremendous strides. To eliminate irrelevant information and enhance the accuracy of detection, the MIL~\cite{maron1997framework} framework is widely employed in this process. Most existing works~\cite{RendnSegador2023CrimeNetNS, bermejo2011violence, deniz2014fast, feng2021mist, peixoto2019toward, ristea2021self, c:8, zhang2019temporal, zhang2016new} consider violence detection as solely a visual task, and CNN-based networks are utilized to encode visual features. Several methods are proposed to enhance the robustness of MIL, including feature integration and refinement techniques. Sultani \etal \cite{c:7} propose a MIL ranking loss with sparsity and smoothness constraints for deep learning networks to learn the anomaly scores in video segments. Li \etal \cite{c:9} develope a multi-sequence learning model based on Transformer \cite{Vaswani2017AttentionIA} to reduce the probability of selection errors.  
However, these methods ignore the cross-modal interaction and corresponding audio information, which may potentially hinder the accuracy of violence detection.

A recent research \cite{c:2} releases a large-scale audio-visual violence dataset termed XD-Violence and establishes an audio-visual benchmark. To facilitate inter-modality interactions, Yu \etal \cite{c:4} propose a lightweight two-stream network and utilize modality-aware contrast and self-distillation to achieve discriminative multimodal learning. To focus on the implication of normal data, Zhou \etal \cite{c:10} propose an dual memory units module with uncertainty
regulation to learn both the representations of normal data and the discriminative features of abnormal data. Different from prior methods, we project the fused embeddings of audio-visual features on the hyperbolic manifold, and employ fully hyperbolic graph convolutional networks to effectively excavate the semantic discrepancy between violent and non-violent instances. 




\subsection{Neural Networks in Hyperbolic Space}
The conventional approaches for deep learning usually involve embedding similarity functions and other policies into Euclidean vector space to extract latent semantic representation, as our comprehension of the physical world is highly correlated with Euclidean space. Although these Euclidean representation models have proved successful in various tasks, they suffer from an inherent limitation: their capability of learning complex hierarchies is limited by the dimensionality of the Euclidean space. An increasing number of studies have indicated that many types of data demonstrate non-Euclidean structures \cite{Bronstein2016GeometricDL}. In many fields, e.g., recommender systems \cite{Chen2021ModelingSG}, biological sequences \cite{Corso2021NeuralDE}, and skeleton-based action recognition \cite{Peng2020MixDI}, the hierarchical structure of the data is well defined by non-Euclidean space such as hyperbolic space.

Hyperbolic space is a kind of non-Euclidean space with constant negative Gaussian curvature. Recently, hyperbolic space has been drawing increasing interest in machine learning and neural information science due to its appealing properties in representing data with hidden hierarchies \cite{Nickel2017PoincarEF, Sala2018RepresentationTF, Nickel2018LearningCH, Wang2019HyperbolicHI}. Nickel et al. \cite{Nickel2017PoincarEF} conduct a groundbreaking study of learning representation in hyperbolic spaces using the Poincar{\'e} ball model. Sala et al. \cite{Sala2018RepresentationTF} analyse the trade-offs of embedding size and numerical precision in these different models and Ganea et al. \cite{Ganea2018HyperbolicEC} extend these methods to undirected graphs. On this basis, Ganea et al. \cite{Ganea2018HyperbolicNN} define a hyperbolic neural network, which bridges the gap between hyperbolic space and deep learning. Nickel et al. \cite{Nickel2018LearningCH} and Wilson et al. \cite{Wilson2018GradientDI} demonstrate that using the Lorentzian model of hyperbolic space can result in more efficient and simpler optimizers compared to the Poincaré ball. In recent research \cite{gu2019learning}, neural networks have been developed based on Cartesian products of isotropic spaces. In fact, hyperbolic space has been well incorporated into recent advanced deep learning models such as the recurrent neural network \cite{Ganea2018HyperbolicNN}, graph neural network \cite{Liu2019HyperbolicGN}, and attention network \cite{Glehre2019HyperbolicAN}. Based on these studies for deep learning paradigms, we investigate the effectiveness of learning weakly-supervised audio-visual violence detection in hyperbolic space using hyperbolic neural networks.



\begin{figure*}[t]
    \centering
\includegraphics[width=1.0\textwidth]{figures/pipeline.png} \caption{Overview of our HyperVD framework. Our approach consists of fours parts: detour fusion, hyperbolic feature similarity graph branch, hyperbolic temporal relation graph branch and hyperbolic classifier. Taking audio and visual features extracted from pretrained networks as inputs, we design a simple yet effective module to fuse audio-visual information. Then two hyperbolic graph branches learn instance representations via feature similarity and temporal relation in hyperbolic space. Finally, a hyperbolic classifier is deployed to predict violent scores for each instance. The entire framework is trained jointly in a weakly supervised manner, and we adopt the multiple instance learning (MIL) strategy for optimization.}
    \label{fig2}
\end{figure*}



\section{Preliminaries}
Before describing our method's details, in this section, we will introduce background knowledge of the hyperbolic geometry with its modeling, \ie Lorentz model, and the hyperbolic graph convolutional networks that we adopt in this work.
\subsection{Hyperbolic Geometry}
\label{subsec:3_1}
Hyperbolic geomerty is a non-Euclidean geometry with a constant negative curvature $K$. The hyperbolic geometry models have been applied in previous studies: the \poincare ball (\poincare disk) \cite{ganea2018hyperbolic}, the \poincare half-plane model \cite{tifrea2018poincar}, the Klein model \cite{gulcehre2018hyperbolic}, and the Lorentz (Hyperboloid) model \cite{Nickel_Kiela_2018}. We select the Lorentz model as the framework base, considering the numerical stability and calculation simplicity of its exponential and logarithmic maps and distance function.

We denote $\lorentz=(\Lc^n, \riemanntensor)$ as an n-dimensional Lorentz model with constant negative curvature K. $\Lc^n$ is a point set satisfying:
\begin{align}
\label{eq1}
  \Lc^n:=\{{x\in\mathbb{R}^{n+1}: <x,x>_{\Lc}=\frac{1}{K}, x_i>0}\}.
\end{align}
The Lorentzian scalar product is defined as:
\begin{align}
\label{eq2}
<x, y>_{\Lc}:=-x_0y_0+\sum_{i=1}^{n}{x_iy_i}, 
\end{align}
where $\Lc^n$ is the upper sheet of hyperboloid in an (n+1)-dimensional Minkowski space with the origin $(\sqrt{-1/K}, 0,...,0)$. For simplicity, we denote point $x$ in the Lorentz model as $x\in\mathbb{L}^n_K$.\\

\noindent\textbf{Tangent Space.} The tangent space at $x$ is defined as an n-dimensional vector space approximating $\lorentz$ around $x$,
\begin{align}
\label{eq3}
\tangent{x}\coloneqq\{\mathbf{y}\in \Real^{n+1} \mid \linner{y}{x}=0\}.
\end{align}
Note that $\tangent{x}$ is a Euclidean subspace of $\mathbb{R}^{n+1}$.\\

\noindent\textbf{Exponential and Logarithmic Maps.} The mapping of points between the hyperbolic space $\lorentz$ and the Euclidean subspace $\tangent{x}$ can be done by exponential map and logarithmic map. The exponential map can map any tangent vector $z\in\tangent{x}$ to $\lorentz$, and the logarithmic map is reverse map that maps back to the tangent space. These two maps can be written as:
\begin{align}
\label{eq4}
\expmap{x}{z} &= \cosh(\sqrt{-K}\lVert\mathbf{z}\rVert_\Lc)\mathbf{x} + \sinh(\sqrt{-K}\lVert\mathbf{z}\rVert_\Lc)\frac{\mathbf{z}}{\sqrt{-K}\lVert\mathbf{z}\rVert_\Lc},\\
\label{eq5}
\logmap{\mathbf{x}}{\mathbf{y}} &= d_{\mathbb{L}}^{K}(\mathbf{x},\mathbf{y})\frac{\mathbf{y}-K<\mathbf{x},\mathbf{y}>_{\mathcal{L}}}{\lVert \mathbf{y}-K<\mathbf{x},\mathbf{y}>_{\mathcal{L}}\rVert _{\mathcal{L}}},
\end{align}
where $\lVert\mathbf{z}\rVert_\Lc = \sqrt{\linner{z}{z}}$ denotes Lorentzian norm of $\mathbf{z}$ and $ d_{\mathbb{L}}^{K}(\cdot,\cdot)$ denotes the Lorentzian intrinsic distance function between two points $\mathbf{x},\mathbf{y}\in \lorentz$, which is given as:
\begin{align}
\label{eq6}
d_{\mathbb{L}}^{K}(\mathbf{x},\mathbf{y}) = arccosh(K<\mathbf{x},\mathbf{y}>_{\mathcal{L}}).
\end{align}

\subsection{Hyperbolic Graph Convolutional Networks}
Recently, several hyperbolic GCNs have been proposed to extend Euclidean graph convolution to the hyperboloid model and have obtained promising results in a wide range of scenarios\cite{c:14}. In order to adapt widely-used Euclidean neural operations, such as matrix-vector multiplication, to hyperbolic spaces, existing methods formalize most of the operation in a hybrid way that involves transforming features between hyperbolic spaces and tangent spaces using logarithmic and exponential maps, and performing neural operations in tangent spaces. For instance, in HGCN \cite{c:13}, let $h_{i,K}^{n} \in\mathbb{H}_{K}^{n}$ be a $n$-dimensional node features of node $i$ on hyperboloid manifold $\mathbb{H}_{K}^{n}$, $N(i)$ be a set of its neighborhoods with adjacent matrix $A_{ij}$, and $W$ be a weight matrix. Its message passing rules consist of \emph{feature transformation}:
\begin{align}
\label{eq7}
    h_{i,K}^{d} = {\rm exp}_{0}^{K}({   W\logmap{0}{h_{i,K}^n}   }),
\end{align}
and \emph{neighborhood aggregation}:
\begin{align}
\label{eq8}
    \rm{Agg}(h_{i,K}^d) &= {\rm exp}_{h_i}^{K}({\sum_{j\in N(i)\cup{i}} {A_{ij} \logmap{{h_i}}{{h_{i,K}^d}} }} ),
\end{align}
where $\expmap{0}{\cdot}$ and $\logmap{0}{\cdot}$ is logarithmic and exponential maps of the $\mathbb{H}_{K}^{n}$. The above hybrid manner does not fully satisfy hyperbolic geometry, causing distortion for the node features of graphs and weakening the stability of models \cite{c:15, c:16}. 

Therefore, Chen \etal \cite{c:16} proposed a fully hyperbolic neural networks based on Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential neural operations and proved that linear transformation in the tangent space at the origin of hyperbolic spaces is equivalent to performing a Lorentz rotation with relaxed restrictions. Readers could refer to \cite{c:16} for more detailed derivation. For simplicity, they provide a morel general formula \footnote{This general formula is no longer fully hyperbolic. It is a relaxation in implementation, while the input and output are still guaranteed to lie in the Lorentz model \cite{c:16}.} of their hyperbolic linear layer for \emph{feature transformation} with activation, dropout, bias and normalization,
\begin{align}
\label{eq9}
   \mathbf{y} = {\rm HL}(\mathbf{x}) = \left[\begin{smallmatrix}
\sqrt{\lVert \phi(\mathbf{Wx, v}) \rVert^2 - 1/K}  \\
\phi(\mathbf{W}\mathbf{x}, \mathbf{v})
\end{smallmatrix}\right], 
\end{align}
where $\mathbf{x} \in\lorentz$, $\mathbf{W}\in\mathbb{R}^{d\times(n+1)}$, $\mathbf{v} \in \mathbb{R}^{n+1}$ denotes a velocity (ratio to the speed of light) in the Lorentz transformations, and $\phi$ is an operation function: for the dropout, the function is $\phi(\mathbf{Wx, v}) = \mathbf{W}{Dropout}(\mathbf{x})$; for the activation and normalization $\phi(\mathbf{Wx, v}) = \frac{\lambda \sigma(\mathbf{v}^\intercal\mathbf{x}+b')}{\lVert \mathbf{W}h(\mathbf{x})+b \rVert}(\mathbf{W}h(\mathbf{x})+b)$, where $\sigma$ is the sigmoid function, $b$ and $b'$ are bias terms, $\lambda > 0$ controls the scaling range, $h$ is the activation function. Further, their proposed \emph{neighborhood aggregation} can be defined as:
\begin{align}
\label{eq10}
    {\rm HyperAgg}(\mathbf{y}_i) = \frac{\sum_{j=1}^{m}A_{ij}\mathbf{y}_j}{\sqrt{-\textit{K}} \big | \lnorm{\sum_{k=1}^{m}A_{ik}\mathbf{y}_k} \big|} ,
\end{align}
where $\rm{m}$ is the number of points. The non-linear activation of this method is omitted in the last operation for it is already integrated into the hyperbolic linear layer. In our study, we adapt the fully hyperbolic graph convolutional network into our framework to explore the efficacy of instance representation learning in hyperbolic space.








\section{Method}
In this section, we first define the formulation and problem statement. Then we introduce our proposed framework in detail, which mainly consists of four parts: detour fusion, hyperbolic feature similarity graph branch, hyperbolic temporal relation graph branch and hyperbolic classifier. The illustration of the framework is shown in Fig \ref{fig2}.
 
 \subsection{Formulation and Problem Statement}
Given an audio-visual video sequence $M=\{M^V_i, M^A_i\}_{i=1}^T$ with $T$ non-overlapping multimodal segments, where each segment contains 16 frames, and $M^V_i$ and $M^V_i$ denotes visual and audio segment, respectively. The annotated video-level label $Y \in \{1, 0\}$ indicates whether an violent event exists in this video. To avoid additional training overhead, we utilize the well-trained backbones (I3D\cite{c:12} and VGGish \cite{Gemmeke_Ellis, Hershey_Chaudhuri}) to extract visual features $X^V \in{\mathbb{R}^{T\times D}}$ and audio features $X^A \in{\mathbb{R}^{T\times d}}$, respectively, where $D$ and $d$ are the feature dimensions. Like prior works \cite{c:2, c:4, c:7, c:9}, our method aims to employ the multiple instance learning (MIL) procedure to distinguish whether it contains violent events (instances) in a weakly-supervised manner, utilizing just video-level labels $Y$ for optimization.

\subsection{Multimodal Fusion}
Here we discuss several commonly-used multimodal fusion manners in the early and middle stages and then describe our proposed detour fusion. Let $X^V$ and $X^A$ denote the auditory and visual features extracted by the backbones, and $X = \{x_i\}_{i=1}^T$ denote the fusion of the features from the two modalities.

\noindent\textbf{Concat Fusion.}
A straightforward approach is to simply concatenate all the features of both modalities and then fuse them via fully-connected layers (FC). The output $X$ of the concat fusion scheme can be expressed as $X = f(X^A \oplus X^V)$, where $f(\cdot)$ is two-layered FC and $\oplus$ is concatenation operation.

\noindent\textbf{Additive Fusion.}
We combine the information from both modalities using component-wise addition, \ie $X = f_a(X^{A}) + f_v(X^{V})$, where $f_a(\cdot)$ and $f_v(\cdot)$ are two corresponding FC to keep dimension of input features identical.  

\noindent\textbf{Gated Fusion.}
We investigate a gated fusion method proposed in \cite{kiela2018efficient}, which allows one modality to “gate” or “attend” over the other modality, via a sigmoid non-linearity, \ie $X = W(UX^A * VX^V)$, where $U$,$V$, and $W$ are weight matrices. One can think of this approach as performing attention from one modality over the other. 

\noindent\textbf{Bilinear \& Concat.}
We utilize two linear layers for both input features of two modalities and keep their dimension identical, followed by a concatenation operation, \ie $X = UX^A \oplus VX^V$, where $U$ and $V$ are weight matrices.

\noindent\textbf{Detour Fusion.}
In audio-visual violence detection, there may exist a significant information inconsistency between auditory and visual signals, as is often the case in other multimodal tasks.  In outdoor scenes, audio signals are often corrupted by noise, while visual signals are more informative and reliable for detecting violence. Based on this intuition, the visual modality may be expected to contribute more to violence detection, compared to the auditory modality. Therefore, we propose a simple and efficient detour fusion method that only feed visual features into FC layers, ensuring that the visual features have the same dimension as the audio ones. Then, we concatenate the visual and audio features to form a joint representation, denoted as $X = f_v(X^V) \oplus X^A$, where $f_v$ is a two-layered FC and $X \in \mathbb{R}^{T\times 2d}$. To a certain extent, this detour operation can suppress the negative contribution of the audio modality on the violence detection performance by giving more importance to the visual modality. We will support our argument with experimental results and comparisons with other fusion methods.


\subsection{HFSG Branch}
Prior works have shown promising power of GCNs for video understanding \cite{Wang_Gupta_2018, c:3, Zeng_Huang_Tan_Rong_Zhao_Huang_Gan_2019, c:2}. Here, we leverage the fully hyperbolic GCN to learn discriminative representations via hyperbolic geometry. We first project fused features $X$ into hyperbolic space by exponential map $\expmap{x}{\cdot}$ and have $\hat{X}\in \mathbb{L}_{K}^{T\times 2d}$. Then we define adjacent matrix $A^{\mathbb{L}} \in \mathbb{R}^{T \times T}$ via hyperbolic feature similarity:
\begin{align}
\label{eq11}
    A_{ij}^{\mathbb{L}} = softmax(g(\hat{x}_i, \hat{x}_j)),\\
\label{eq12}
    g(\hat{x}_i, \hat{x}_j)= exp(-d_{\mathbb{L}}^{K}(\hat{x}_i, \hat{x}_j)),
\end{align}
where the element $A_{ij}^{\mathbb{L}}$ measures the hyperbolic feature similarity between the $i$th and $j$th snippets via Lorentzian intrinsic distance $d_{\mathbb{L}}^{K}(\cdot,\cdot)$ instead of cosine similarity or other Euclidean metrics. Since an adjacency matrix should be non-negative, we bound the similarity to the range (0, 1] with a exponential function $exp(\cdot)$. Before $softmax$ normalization, we also employ the thresholding operation to eliminate weak relations and strengthen correlations of more similar pairs in hyperbolic space. The thresholding can be defined as:
\begin{equation}
\label{eq13}
  g(\hat{x}_i, \hat{x}_j) = \left\{
	\begin{aligned}
	g(\hat{x}_i, \hat{x}_j), \quad g(\hat{x}_i, \hat{x}_j)\textgreater\tau \\
	0, \quad g(\hat{x}_i, \hat{x}_j)\leq\tau \\ 
	\end{aligned}
	\right.
\end{equation}
where $\tau$ is thresh value.

Given the hyperbolic embeddings $\hat{X}$, we leverage the hyperbolic linear layer $\rm{HL}(\cdot)$ for \emph{feature transformation}, which incorporates an activation layer for non-linear activation, followed by \emph{neighborhood aggregation} $\rm{HyperAgg}$ as elaborated in equation \ref{eq10}. The overall operations are as follows: 
\begin{align}
\label{eq14}
   \hat{x}_{i}^{l} = \frac{\sum_{j=1}^{T}A_{ij}^{\mathbb{L}} {\rm HL}( \hat{x}_{i}^{l-1}) }{\sqrt{-\textit{K}} \big | \lnorm{\sum_{k=1}^{T}A_{ik}^{\mathbb{L}} {\rm HL}(\hat{x}_{i}^{l-1}) } \big|}  ,   
\end{align}
where $\hat{x}_{i}^{l}$ refers to the hyperbolic representation of the $i$th snippet at the layer $l$. The output of this branch is computed as:
\begin{align}
\label{eq15}
    \hat{X}^{\mathbb{L}} = Dropout(LeakyReLU(\hat{X}^{l+1})).
\end{align}





\subsection{HTRG Branch}
Although the hyperbolic feature similarity branch can capture long-range dependencies by measuring the similarity of snippets between any two positions, irrespective of their temporal position information, the temporal relation is also crucial for numerous video-based tasks. To address this issue, we construct a temporal relation graph directly based on the temporal structure of a video and learn the temporal relation among snippets in hyperbolic space. Its adjacency matrix $A^{\mathbb{T}} \in \mathbb{R}^{T \times T} $is only dependent on temporal positions of the $i$th and $j$th snippets, which can be defined as:
\begin{align}
\label{eq16}
    A^{\mathbb{T}}_{ij} = exp(- \lVert i-j\rVert^{\gamma}),
\end{align}
where $\gamma$ is a hyper-parameter that controls the scope of temporal distance.

Likewise, we obtain hyperbolic embeddings via $\hat{X}=\expmap{x}{\emph{X}}$, and forward $\hat{X}$ and $A^{\mathbb{T}}$ into the hyperbolic GCN to learn temporal relationships in hyperbolic space via equation \ref{eq14}. The final output is also computed as:
\begin{align}
\label{eq17}
    \hat{X}^{\mathbb{T}} = Dropout(LeakyReLU(\hat{X}^{l+1})).
\end{align}




\subsection{Hyperbolic Classifier}
The output embeddings of the two branches still reside on the hyperbolic manifold, where it is not feasible to directly classify using a Euclidean-based linear layer. As shown in Figure \ref{fig2}, to predict violent scores $S\in \mathbb{R}^{T\times 1}$, we concatenate the embeddings and input them into a hyperbolic classifier, which can be formalized as:
\begin{align}
\label{eq18}
S = \sigma((\epsilon + \epsilon <\hat{X}^{\mathbb{L}} \oplus \hat{X}^{\mathbb{T}}, W>_{\mathcal{L}}) + b),
\end{align}
where $\sigma$ is sigmoid function and $W$ is weight matrices. $b$ and $\epsilon$ denotes bias term and hyper-parameter, respectively.


\subsection{Objective Function}
In this paper, violent detection is treated as a MIL task under weak supervision. Following \cite{c:2,c:7}, we use the mean value of the $k$-max predictive scores in a video bag as the violent score, where $k = \lfloor \frac{T}{q} + 1\rfloor $. High scoring $k$-max predictions in the positive bag are more likely to include violent events, whereas the $k$-max predictions in the negative bag are typically hard samples. Consequently, the objective function is as follows:
\begin{equation}
\small{
L_{MIL} = \frac{1}{N}\sum_{i=1}^{N}{-Y_{i}log(\Bar{S})},
}
\end{equation}
where $\Bar{S}$ is the average value of the $k$-max predictions in the video bag and $Y_i$ is the binary video-level annotation.




\begin{table}[t]
\caption{Comparison of the frame-level AP performance on XD-Violence. Bold numbers indicate the best performances. The method with † and * are re-implemented and reported by \cite{c:4}. The top performance is highlighted in bold, while the second-best performance is highlighted by underlining.}
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\hline
Manner                  &  Method                       & Modality      & AP(\%)         & Param.(M)\\ \hline
\multirow{3}{*}{Unsup}   & SVM baseline                 &   -          & 50.78           &  -     \\
                        &  OCSVM \cite{c:5}             &   -          & 27.25            & -  \\
                        & Hasan \etal\cite{c:6}         &   -          & 30.77           & -  \\ \hline
\multirow{6}{*}{W.Sup}& Sultani \etal† (2018) \cite{c:7} & V            & 75.68           & -  \\ 
                        & Wu \etal (2020) \cite{c:2}        & A + V            & 78.64     & 0.843  \\
                        & Wu \etal† (2020) \cite{c:2}        & A + V            & 78.66       & 1.539  \\ 
                        & Wu \etal (2021) \cite{c:8}    & V            & 75.90            & -  \\
                        & RTFM (2021) \cite{c:1}        & V            & 77.81            & 12.067 \\
                        & RTFM* (2021) \cite{c:1}       & A + V        & 78.54           & 13.510 \\
                        & RTFM† (2021) \cite{c:1}       & A + V        & 78.54           & 13.190 \\
                        & Pang \etal (2021) \cite{pang2021violence}  & A + V       & 81.69            & 1.876 \\  
                        & MSL \etal (2022) \cite{c:9}    & V           & 78.28            & - \\            
                        & S3R (2022) \cite{c:11}        & V            & 80.26            & -  \\       
                        & MACIL-SD (2022) \cite{c:4}       & A + V       & \underline{83.40}            & \underline{0.678} \\ 
                        & UR-DMU (2023) \cite{c:10}     &  V       &  81.66           & -     \\
                        & UR-DMU (2023) \cite{c:10}     &  A+ V       &  81.77           & -     \\
                        & Zhang (2023)  \etal\cite{zhang2022exploiting} & V       &  78.74            & -     \\
                        & Zhang (2023) \etal\cite{zhang2022exploiting}  & A + V       &  81.43            & -     \\\hline
\multirow{1}{*}{W.Sup} & HyperVD (ours)                           &  A + V        & \textbf{85.67}   & \textbf{0.607} \\ \hline

\end{tabular}}
\label{table1}
\end{table}

\section{Experiments}
In this section, we evaluate our approach on the benchmark dataset (XD-Violence). We first give implementation details and then introduce the dataset. Finally, we present comparisons of the proposed method with recent state-of-the-art (SOTA) methods and ablation results.






\subsection{Implementation Details}
\noindent\textbf{Feature Extraction.} To make a fair comparison, we employ the same procedure for feature extraction as previous methods \cite{c:1,c:2, pang2021violence,c:4}. Specifically, to extract visual features, we use the I3D network \cite{Carreira_Zisserman_2017} pretrained on the Kinetics-400 dataset. For audio features, we employ the VGGish network \cite{Gemmeke_Ellis, Hershey_Chaudhuri}, which was pretrained on a large dataset of YouTube videos. Visual features are extracted at a sample rate of 24 frames per second, using a sliding window approach with a window size of 16 frames. For the auditory data, we divide each audio recording into 960-millisecond segments with overlap, and then compute the log-mel spectrogram using a resolution of 96 x 64 bins. This allows us to extract rich and informative auditory features that can be combined with the visual features to enhance the performance of our violence detection model.



\noindent\textbf{HyperVD Architecture and Settings.} For detour fusion module, we apply two 1D convolutional layers with LeakyReLU activation and dropout to learn the visual features. In the hyperbolic space, we utilize two hyperbolic graph convolutional layers for the HSFG and HTRG branches. The input dimensions for both branches are 257, and the hidden dimensions are set to 32. The negative curvature constant, denoted as $K$, is a fixed value of -1.

\noindent\textbf{Training Details.} The entire network is trained on an NVIDIA RTX 3090 GPU for 50 epochs. We set the batch size as 128 during training, and set the initial learning rate as 5e-4, which is dynamically adjusted by a cosine annealing scheduler. For hyper-parameters, we set $\gamma$ as 1, $\epsilon$ as 2, and dropout rate as 0.6. We use Adam as the optimizer without weight decay. For the MIL, we set the value $k$ of $k$-max activation as $\lfloor \frac{T}{16} + 1\rfloor$, where $T$ denotes the length of input feature.

\begin{table}[t]
  \caption{Ablation studies for different multimodal fusion manners. The method with * is re-implemented by us by replacing its original concat fusion with our detour fusion.}
  \label{table2}
  \begin{tabular}{cclc}
    \toprule
    Index    & Manner  & AP(\%) & Param(M)\\
    \midrule 
     1 &     Wu \etal* \cite{c:2}  & 79.86 \textcolor{red}{($\uparrow1.22$)} & 0.851  \\\midrule 
     2 &      Concat Fusion &	83.35	& 0.758\\
     3 &     Additive Fusion &	82.41	& 0.594 \\
     4 &      Gated Fusion	&   82.51	& 0.657  \\
     5 &     Bilinear \& Concat	& 81.33	& 0.644\\
     6 &     Detour Fusion (Ours)& 85.67 \textcolor{red}{($\uparrow2.32$)}	& 0.607 \\ 
    \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/predictions_visualization.pdf} \caption{Visualization of anomaly score curves. The horizontal axis represents the time, and the vertical axis represents the anomaly scores. The first row includes two samples of videos containing violent events, and the second row includes samples from normal videos. The blue curves indicate the predicted abnormal scores of the video frames, and the red areas indicate the locations of abnormal events.}
	\label{fig3}
\end{figure*}






\subsection{Dataset}
XD-Violence\cite{c:2} is a recently released large-scale audio-visual violence detection dataset, compiled from real-world movies, web videos, sport streaming, security cameras, and CCTVs. This dataset contains 4754 untrimmed films with video-level labels in the training set and frame-level labels in the testing set, for a total runtime of nearly 217 hours. Following \cite{c:2, pang2021violence, c:4}, we select this XD-Violence dataset as our benchmark to verify the efficiency of our proposed multimodal framework.

During inference, we use the Average Precision (AP) metric for evaluation following previous works\cite{c:1,c:2, pang2021violence,c:4}. It is important to note that higher values of AP correspond to better performance on the dataset.



\begin{table}[t]
  \caption{Ablation studies for utilizing various GCNs with different geometry models and different feature similarity metrics. $\mathbb{E}$, $\mathbb{B}$ and $\mathbb{L}$ indicate Euclidean, \poincare and Lorentz model, respectively.}
  \label{table3}
  \begin{tabular}{ccccc}
    \toprule
    Index  & Network & Model & Feature Similarity & AP(\%)\\
    \midrule
    1 &  GCN & $\mathbb{E}$ & Cosine Similarity & 79.85\\
    2 & HGCN & $\mathbb{B}$ & Cosine Similarity & 81.62\\
    3 & HGCN & $\mathbb{B}$ & \poincare Distance & 82.88 \\ \hline
    4  &  FHGCN & $\mathbb{L}$ & Cosine Similarity & 83.25\\
    5 & FHGCN & $\mathbb{L}$ & Lorentzian Distance & \textbf{85.67}\\
  \bottomrule
\end{tabular}
\end{table}



\subsection{Quantitative Results}
We compare our proposed approach with previous state-of-the-art methods, including (1) unsupervised methods: SVM baseline, OCSVM\cite{c:5}, and Hasan \etal\cite{c:6}; (2) unimodal weakly-supervised methods: Sultani \etal \cite{c:7}, Wu \etal \cite{c:8} RTFM \cite{c:1},  MSL \cite{c:9}, S3R \cite{c:11}, UR-DMU \cite{c:10} and Zhang \etal\cite{zhang2022exploiting}; (3) audio-visual weakly-supervised method: Wu \etal \cite{c:2},  Pang \etal \cite{pang2021violence}, MACIL-SD \cite{c:4}, UR-DMU \cite{c:10} and Zhang \etal\cite{zhang2022exploiting}. The AP results on XD-Violence dataset are presented in Table \ref{table2}. 

When evaluated on video-level labels for supervision, our approach achieves state-of-the-art performance, surpassing all unsupervised methods by a significant margin in AP. Compared with previous weakly-supervised unimodal methods, our approach achieves a minimum of 4.01\% improvement over their results.

When compared with the state-of-the-art weakly-supervised multimodal method, MACIL-SD\cite{c:4}, our approach achieves a substantial improvement of 2.27\%. Besides, Wu \etal \cite{c:2} also propose a GCN-based method for multimodal violence detection, but our approach outperforms it by an impressive 7.03\%. These results demonstrate the effectiveness of our proposed method for learning instance representations in hyperbolic space, and its potential for enhancing the performance of violence detection models.

\begin{figure}[t]
\centering
{\includegraphics[width=0.47\columnwidth, clip=true, trim=0 0 0 0]{figures/vanilla-SNE.png}}\vspace{-0.05cm}
{\includegraphics[width=0.47\columnwidth, clip=true, trim=0 0 0 0]{figures/trained-SNE.png}}\vspace{-0.05cm}
\subfigure[Vanilla Embeddings]{\includegraphics[width=0.47\columnwidth, clip=true, trim=0 0 0 0]{figures/vanilla-SNE1.png}}
\subfigure[Trained Embeddings]{\includegraphics[width=0.47\columnwidth, clip=true, trim=0 0 0 0]{figures/trained-SNE1.png}}
\caption{The projection of high-dimensional vanilla embeddings and output hyperbolic embeddings of our model in a two-dimensional features space with CO-SNE \cite{guo2022co}, which can preserve the hierarchical and similarity structure of the high-dimensional hyperbolic data points. The red points indicate violent embeddings and the blue points indicate non-violent embeddings.}\vspace{-0.4cm}
\label{fig4}
\end{figure}


\begin{table}[h]
  \caption{Ablation studies for the proposed Hyperbolic Feature Similarity Graph (HFSG) branch and Hyperbolic Temporal Relation Graph (HTRG) branch.}
  \label{table4}
  \begin{tabular}{cccc}
    \toprule
    Index  & HFSG Branch & HTRG Branch &  AP(\%)\\
    \midrule
    1 &  - & \checkmark & 80.58\\
    2 & \checkmark & - & 69.01\\
    3 &  \checkmark & \checkmark & \textbf{85.67} \\ 
  \bottomrule
\end{tabular}
\end{table}

\subsection{Qualitative Results}
To further evaluate our method, we first visualize prediction results on XD-Violence and shown them in Figure \ref{fig3}. The predictions are interpolated using cubic interpolation to generate smooth curves. As exhibited in the figures for violent videos, our method not only produces a precise detection area but also generates higher anomaly scores than normal ones. In non-violent videos, our method produces almost zero predictions for normal snippets. 

In addition, we provide feature space visualizations of the vanilla and trained feature distributions on the XD-Violence test sets. The common way is adopting standard t-SNE \cite{Maaten_Hinton_2008} that uses Euclidean distances to measure similarities in high-dimensional space, while hyperbolic distances grow much faster than Euclidean distances. For high-dimensional hyperbolic datapoints which are close to boundary of the \poincare ball, the standard t-SNE wrongly underestimates the distance between them and would lead low-dimensional embeddings collapse into one point, resulting in poor visualization\cite{guo2022co}. Therefore, we apply CO-SNE \cite{guo2022co} designed for hyperbolic data to visualize the vanilla embeddings and trained embeddings produced by the hyperbolic neural network. Specifically, we adopt transformation function to project the embeddings of the Lorentz model into \poincare space and then utilize CO-SNE for visualization. As shown in Figure \ref{fig4}, where the left column shows vanilla embeddings without trainning and the right column shows trained embeddings by our model, we can observe that violent and non-violent features are well separated after training, \eg violent features are close to the center while non-violent features are pushed away to the boundary. 

Finally, we provide comparative results of the accuracy curves in 50 epochs during training as shown in Figure \ref{fig5}. Notably, the similarity matrics of hyperbolic feature similarity branch in HGCN and FHGCN are measuring by \poincare distance and Lorentzian distance metrics, respectively. As shown, the GCN-based method produces significant jitter results. Thanks to the numerical stability of the Lorentz model, our method that is equipped with FHGCN is more steady compared to other methods during the whole training process. 

\subsection{Complexity Analysis}
Our method is also designed to be computationally efficient, without introducing an excessive number of parameters. The detour fusion module, which learns the visual features by fully-connected layers, contains the primary model parameters. In contrast, the HFSG and HTRG branches are comparatively lightweight, consisting mainly of hyperbolic graph convolution layers that operate on the learned embeddings. In comparison to other methods, our approach has the smallest model size (0.607M), while still outperforming all previous methods. These results demonstrate the efficiency of our framework, which leverages a simpler network architecture while achieving superior performance.





\subsection{Ablation Studies}
To investigate the contribution of key components in the proposed framework, we further conduct extensive ablation studies to demonstrate its efficiency. 

We first conduct comparative experiments on different multimodal fusion manners, and the results are shown in Table \ref{table2}. We argue that our detour fusion can effectively alleviate information inconsistency between audio and visual modalities, and therefore achieve a performance of 85.67\% with a 2.32\% improvement than simply utilizing concat fusion. Besides, Wu \etal \cite{c:2} adopt a concatenation manner of early fusion strategy. We re-implement their method using our detour fusion module and get the improvement of 1.22\%.

Then we investigate the contribution of Fully Hyperbolic GCN (FHGCN) to our framework with results in Table \ref{table3}, revealing a remarkable performance boost from 76.87\% to 85.67\% compared to standard GCN in Euclidean space. These results provide evidence that our proposed method can effectively learn discriminative representations in hyperbolic space. Moreover, the numerical stability of FHGCN equipped with the Lorentz model enabled our method to outperform HGCN with the Poincare model, achieving a 2.79\% improvement. As shown in Table \ref{table3},  we also evaluate the model performance using diverse feature similarity metrics. Our findings demonstrate that using Lorentzian distance for the Lorentz model yields a superior capacity for capturing feature similarity in the hyperbolic space and consequently, it outperforms alternative methods.

Ultimately, the contributions of the proposed HFSG branch and HTRG branch are analyzed. For the HFSG branch, we eliminate it, i.e., hyperbolic feature similarities will not be available, and immediately output embeddings of the HTRG branch for the classifier below. Without the branch, the AP value falls to 80.58 \%, as seen in the $1^{th}$ row of Table \ref{table4}. Similarly, when the HTRG is deleted and no temporal information is present, the AP value reduces dramatically to 69.01\% as shown in the $2^{th}$ row of Table \ref{table4}, indicating the importance of temporal relationships between snippets. When equipped with both branches, our method achieves the best performance of 85.67 \% AP.





\begin{figure}[t]
    \centering
\includegraphics[width=0.45\textwidth, clip=true, trim=0 0 0 25]{figures/gcn_ap_ablation.png} \caption{Comparative results of the accuracy curves in 50 epochs during training.}
    \label{fig5}
\end{figure}

\section{Conclusion}
In this paper, we investigate the modality inconsistency under audio-visual scenarios and the weakness of learning instance representations in Euclidean space. Then a HyperVD framework incorporated with a detour fusion module and two hyperbolic graph learning branches is proposed to address the above issues. To be specific, we design a detour fusion strategy to suppress the negative impacts of audio signals to alleviate information inconsistency across modalities. Furthermore, a hyperbolic feature similarity graph branch and a hyperbolic temporal relation graph branch are proposed to learn similar characteristics and temporal relationships among snippets, respectively. Our HyperVD greatly outperforms previous methods on the XD-Violence dataset, demonstrating the superiority of instance representation learning in hyperbolic space.
\noindent\textbf{Future Work.} We are convinced that hyperbolic geometry holds great potential for various video understanding and interpretation tasks, such as video anomaly detection and event localization. We are committed to further exploring the power of hyperbolic geometry in these and other related areas in the future.






\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}



















\end{document}
