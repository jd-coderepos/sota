In this section, we mainly evaluate our GuidedMix-Net on the semi-supervised semantic segmentation task, and provide the performances of the proposed approach and other state-of-the-art methods on different datasets.

\subsection{Dataset and Evaluation Metrics}
\textbf{PASCAL VOC 2012.} This dataset is widely used for semantic segmentation and object detection.
It consists of 21 classes including background.
We use 1,464 training images and 1,449 validation images from the original PASCAL dataset, and also leverage the augmented annotation dataset (involving 9,118 images) \cite{Hariharan2011Semantic} like \cite{2018Weakly,zhao2017pyramid}.
\\\textbf{Cityscapes.} We use Cityscapes to further evaluate our model. 
This dataset provides different driving scenes distributed in 19 classes, with 2,975, 500, 1,525 densely annotated images for training, validation and testing.
\\\textbf{PASCAL Context.} Compared with the above datasets, PASCAL Context is more difficult to implement. 
We use it to demonstrate that the proposed GuidedMix-Net can generalize to various scenes.
Pascal Context provides a set of additional annotations for PASCAL VOC 2010, with labels for more than 400 categories.
There are 4,998 images for training and 5,105 images for validation.
For semantic segmentation, 59 semantic classes and 1 background class are used in training and validation, respectively.
\\\textbf{Evaluation Metric.} Common data augmentation methods are used during our training procedure, which including random resizing (scale: 0.52.0), cropping (321321 for PASCAL VOC 2012, 513513 for Cityscapes, 480480 for PASCAL Context), horizontal flipping and slight rotation. 
We evaluate different methods by measuring the averaged pixel intersection-over-union (IoU).

\subsection{Network Architecture and Training Details}
\textbf{Encoder.} The encoder is based on ResNet \cite{he2016deep} pretrained on ImageNet \cite{krizhevsky2012imagenet}, and also includes the PSP module \cite{zhao2017pyramid} after the last layer.
\\\textbf{Decoder.} Inspired by MixUp, GuidedMix-Net combines labeled and unlabeled data, setting the new SOTA, for semi-supervised semantic segmentation. 
To avoid the disintegrates of details for the mixed pairs, we employ a skip connection in the decoder, as done in U-Net \cite{Ronneberger2015UNetCN}.
A pixel shuffle layer \cite{shi2016real} is also utilized to restore the spatial resolution of features.
\\\textbf{Efficient Training.} The proposed method is encouraged to recognize complex objects from the mixed data and decoupled unlabeled data masks by using single decoder.
In contrast, CCT \cite{ouali2020semi} is computationally expensive due to its vast number of decoders, making it difficult to train an efficient model. 
Our method, in contrast, is encouraged to recognize complex objects from the mixed data and decoupled unlabeled data masks using a single decoder.
Therefore, has s much lower time cost during training was shown in Table \ref{comparison with CCT}) and requires only half the memory compared with CCT.
\\\textbf{Training Details.}
Similar to \cite{chen2017deeplab}, we use a ``poly'' learning rate policy, where the base learning rate is multiplied by  and . 
Our segmentation network is optimized using the stochastic gradient descent (SGD) optimizer with a base learning rate of 1e-3, momentum of 0.9 and a weight decay of 1e-4.
The model is trained over 40,000 iterations for all datasets, and the batch-size is set to 12 for PASCAL VOC 2012, and 8 for Cityscapes and PASCAL Context. 
We conduct all our experiments on a Tesla V-100s GPU.

\subsection{Results on Pascal VOC 2012}
\subsubsection{Ablation Studies} 
Our ablation studies examine the effect of different values of  and the impact of different components in our framework.

\textbf{Different Values of .}
The results under different values of  are reported in Table \ref{impact_lambda}. 
An can be see, changing  used in Sec. \ref{sec:LUPI} impacts the results, because  controls the intensity of pixels in the mixed input data.
As shown in Table \ref{impact_lambda}, too high or too low a  is not conducive to model optimization.
A high  value leads to the labeled information being discarded, while a low  value results in the unlabeled data being covered.
When , GuidedMix-Net provides the best performance on PASCAL VOC 2012.
We thus select  for the remaining experiments on PASCAL VOC 2012.
\begin{table}[h]
\small
\centering
\caption{The impact of different lambda values on the experimental results of PASCAL VOC 2012.}
\begin{tabular}{c|c|c|c|c}
\hline 
\multirow{2}{*}{} & \multirow{2}{*}{backbone} & \multicolumn{2}{c|}{Data} & \multirow{2}{*}{mIoU} \\\cline{3-4}
                        &      &  labels   & unlabels        &                       \\\hline
 0.1 & \multirow{5}{*}{ResNet50} &  \multirow{5}{*}{1464} & \multirow{5}{*}{9118}   & 67.9 \\
 0.2 &  &  &      & 69.5 \\
 0.3 &  &  &      & 70.7 \\
 0.4 &  &  &      & 71.4 \\
 0.5 &  &  &      & \textbf{73.7} \\\hline
\end{tabular}
\label{impact_lambda}
\end{table}

\textbf{Different Components of GuidedMix-Net.}
We evaluate the influence of different components of GuidedMix-Net by training with 1,464 labeled images and 9,118 unlabeled samples.
For fair comparison, we evaluate one component per experiment and freeze the others.
\begin{table}[h]
\small
\centering
\caption{Ablation studies of using similar image pairs, MITrans, and hard  soft decoupling modules in GuidedMix-Net. 
We train the models on ResNet50 and test them on the validation set of PASCAL VOC 2012.}
\begin{tabular}{c|c|c|c|c}
\hline 
\multirow{2}{*}{Method} & \multirow{2}{*}{used} & \multicolumn{2}{c|}{Data} & \multirow{2}{*}{mIoU} \\\cline{3-4}
						&      &  labels   & unlabels        &                       \\\hline
FCN \cite{long2015fully}      &      &  1464     & None         & 69.9                       \\\hline
\multirow{2}{*}{Similar Pair} & & \multirow{7}{*}{1464} & \multirow{7}{*}{9118} & 71.9 \\
								&  &      &      & \textbf{73.7} \\\cline{1-2}\cline{5-5}
\multirow{2}{*}{MITrans}      & &      &      & 72.7 \\
								&  &      &      & \textbf{73.7} \\\cline{1-2}\cline{5-5}
Hard Decoupling               &  &      &      & 71.4 \\
Soft Decoupling               &  &      &      & \textbf{73.7} \\\hline
\end{tabular}
\label{ab_each-com}
\end{table}


\begin{table}[t]
\centering
\caption{Results on different classes of the PASCAL VOC 2012 validation set.
The models are trained on 1464 labeled images and use ResNet50 as their backbone.
GuidedMix-Net outperforms CCT in almost all classes, with a performance increase of over 4.3.}
\begin{tabular}{c|c|c|c|c|c}
\hline
Method & mIoU & aero & bicycle & bird & boat  \\\hline
CCT    & 69.4 & 87.0 & 58.1    & 83.2 & 63.4    \\\hline
Ours   & 73.7 & 89.9 & 60.3   & 83.4  & 71.1   \\\hline
&\textbf{4.3}&\textbf{2.9}&\textbf{2.2}&\textbf{0.2}&\textbf{7.7}\\\hline\hline

Method & mIoU & bottle & bus & car & cat\\\hline
CCT    & - & 64.4 & 89.6 & 81.0& 83.5\\\hline
Ours   & - & 74.4 & 88.1  & 83.2  & 86.9\\\hline
&- &\textbf{10.0}&1.5&\textbf{2.2}&\textbf{3.4}\\\hline\hline

Method & mIoU & chair & cow & table & dog\\\hline
CCT    & - & 28.9 & 76.1    & 46.9 & 74.1\\\hline
Ours   & - & 30.5 & 74.5   & 57.8  & 84.6\\\hline
&- &\textbf{1.6}&1.6&\textbf{10.9}&\textbf{10.5}\\\hline\hline

Method & mIoU & horse & mbike & person & plan\\\hline
CCT    & - & 69.5 & 78.5    & 78.4 & 48.2\\\hline
Ours   & - & 78.1 & 81.6   & 83.5  & 53.2\\\hline
&- &\textbf{8.6}&\textbf{3.1}&\textbf{5.1}&\textbf{5.0}\\\hline\hline

Method & mIoU & sheep & sofa & train & tv\\\hline
CCT    & - & 72.7 & 42.4    & 77.4 & 61.7\\\hline
Ours   & - & 79.1 & 48.7   & 77.6  & 67.3\\\hline
&- &\textbf{6.4}&\textbf{6.3}&\textbf{0.2}&\textbf{5.6}\\\hline
\end{tabular}
\label{perclass_miou}
\end{table}


\begin{table}[h]
\small
\centering
\caption{Comparison with CCT using fewer labeled samples.}
\begin{tabular}{c|c|c|c|c}
\hline
	SSL & 500 & 1000 & 1464 & Memory Size\\
	Methods& labels & labels & labels & in Training \\\hline
	MT \cite{2017Mean} & 51.3 & 59.4 & - & -\\
	VAT \cite{2018Virtual}& 50.0 & 57.9 & - & - \\
	CCT \cite{ouali2020semi}& 58.6 & 64.4 & 69.4 & 24kM \\
	Ours & \textbf{65.4} & \textbf{68.1} & \textbf{73.7} & \textbf{15kM} \\\hline
\end{tabular}
\label{comparison with CCT}
\end{table}


Firstly, we investigate different strategies for constructing image pairs, , random selection of similar pairs.
i) The first strategy randomly selects a labeled image for each unlabeled image in a mini-batch.
ii) To seek similar pairs of labeled and unlabeled images, we add a classifier after the encoder model. 
We match the most similar unlabeled images for each labeled sample according to the Euclidean distance between the features.
As shown in the third row of Table \ref{ab_each-com}, the similar pairs bring a 2.5 mIoU gain over the plain random selection (71.9 vs. 73.7). 
The construction of similar pairs provides context for the target objects in the subsequent segmentation task, and assists GuidedMix-Net in transferring knowledge from the labeled images to the unlabeled samples, with little increasing complexity.

Secondly, we explore whether MITrans is useful for knowledge transfer.
The results provided in the fourth row of Table \ref{ab_each-com} clearly show that MITrans achieves a significant mIoU gain of 1.4 (72.7 vs. 73.7) by explicitly referencing similar and high-confidence non-local feature patches to refine the coarse features of the unlabeled samples.

Finally, as shown in the fifth row Table \ref{ab_each-com}, the performance of soft decoupling is 3.2 better than hard decoupling (71.4 vs. 73.7), since soft decoupling considers the overlapping that occurs in the mixed data and tends to preserve local details.
The various components used in our GuidedMix-Net are beneficial alone, and therefore combining them leads to significantly improved optimization and better performance than even fully supervised learning, with an increase of over  in mIoU (as shown in the second row of Table \ref{ab_each-com}).

\begin{figure}[t!]
\centering
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_input1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_gt1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_cct1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_lupimd1.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_input2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_gt2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_cct2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_lupimd2.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_input10.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_gt10.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_cct10.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_our10.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_input9.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_gt9.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_cct9.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_our9.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_input2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_gt2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_cct2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/qr_pascal_our2.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_input3.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_gt3.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_cct3.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/occ_lupimd3.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_input2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_gt2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_cct2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_lupimd2.pdf}}\\\vspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_input1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_gt1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_cct1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_lupimd1.pdf}}\\\vspace{0.1mm}
\setcounter{subfigure}{0}
\subfloat[Input]{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_input3.pdf}}\hspace{0.1mm}
\subfloat[GT]{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_gt3.pdf}}\hspace{0.1mm}
\subfloat[CCT]{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_cct3.pdf}}\hspace{0.1mm}
\subfloat[Ours]{\includegraphics[width=2.1cm, height=1.6cm]{imgs/sc_lupimd3.pdf}}\\
\caption{Experimental eesults on Pascal VOC 2012. 
Compared to CCT, our method is able to predict more complete contours and correct semantics.
The visual results represent for tiny object parts (the predictions of airplane tails and bicycle handlebars provide more complete contours) and objects occluded by the foreground (shown in the second row).
In terms of category semantics, results show that CCT easing misinterprets the semantic information of objects under complex environments, while our method achieves better results (an example is shown in the last row).}
\label{OCCExam}
\end{figure}


\subsubsection{Semi-Supervised Semantic Segmentation} 
The experimental setting of CCT \cite{ouali2020semi} is different from other semi-supervised semantic segmentation methods \cite{2018Adversarial, zhai2019s4l, ke2020guided, french2020semi}.
Here, we provide detailed comparison experiments.

\textbf{Comparing with Other State of the Arts.}
We explore the performance using the deeper backbone of ResNet101 for the semi-supervised semantic segmentation task.
The settings and training images are the same as above (10,582). 
Following GCT \cite{ke2020guided}, we divide the training images into 1/16 labels, 1/8 labels, 1/4 labels, 1/2 labels, and show the results in Table \ref{comparison with other ssl}.
GuidedMix-Net outperforms the current methods for semi-supervised image segmentation by , , and  on 1/8 labels, 1/4 labels, 1/2 labels, respectively.
The significant performance gains on different ratios of labeled data demonstrate that GuidedMix-Net is a generally efficient and effective semi-supervised semantic segmentation method.  

\begin{table}[h]
\small
\centering
\caption{Comparison with other state-of-the-art semi-supervised semantic segmentation methods under different ratios of labeled data on PASCAL VOC 2012.}
\begin{tabular}{c|c|c|c}
\hline
	SSL Methods & 1/8 labels & 1/4 labels & 1/2 labels\\\hline
	MT \cite{2017Mean}  & 68.9 & 70.9 & 73.2 \\
	AdvSSL \cite{2018Adversarial} & 68.4 & 70.8 & 73.3 \\
	S4L \cite{zhai2019s4l} & 67.2 & 68.4 & 72.0 \\
	GCT \cite{ke2020guided} & 70.7 & 72.8 & 74.0 \\
	CutMix \cite{french2020semi} & 70.8 & 71.7 & 73.9 \\
	Ours & \textbf{73.4} & \textbf{75.5} & \textbf{76.5}\\\hline
\end{tabular}
\label{comparison with other ssl}
\end{table}


\begin{figure*}[t]
\centering
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/input-city1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/gt-city1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/cm-city1.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/ours-city1.pdf}}\\\vspace{0.3mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/input-city2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/gt-city2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/cm-city2.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/ours-city2.pdf}}\\\vspace{0.3mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/input-city3.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/gt-city3.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/cm-city3.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/ours-city3.pdf}}\\\vspace{0.3mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/input-city4.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/gt-city4.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/cm-city4.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/ours-city4.pdf}}\\\vspace{0.3mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/input-city5.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/gt-city5.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/cm-city5.pdf}}\hspace{0.1mm}
\subfloat{\includegraphics[width=4.2cm, height=2.5cm]{imgs/ours-city5.pdf}}\\\vspace{0.3mm}
\setcounter{subfigure}{0}
\subfloat[Input]{\includegraphics[width=4.2cm, height=2.5cm]{imgs/input-city6.pdf}}\hspace{0.1mm}
\subfloat[GT]{\includegraphics[width=4.2cm, height=2.5cm]{imgs/gt-city6.pdf}}\hspace{0.1mm}
\subfloat[ClassMix]{\includegraphics[width=4.2cm, height=2.5cm]{imgs/cm-city6.pdf}}\hspace{0.1mm}
\subfloat[Ours]{\includegraphics[width=4.2cm, height=2.5cm]{imgs/ours-city6.pdf}}\\
\caption{Visual results on Cityscapes using 1/8 labeled examples. 
The proposed semi-supervised approach generates improved results compared to ClassMix \cite{olsson2021classmix}.
(a) Input image. 
(b) Ground-truth.
(c) Segmentation results of ClassMix.
(d) Segmentation results of GuidedMix-Net.}
\label{qr_city}
\end{figure*}


\subsection{Results on Cityscapes}
Cityscapes has 2,975 training images.
In our experiments, we divide them into 1/8 labels and 1/4 labels, while the remaining data are treated as unlabeled.
We use ResNet101 as the backbone to train the models.
Since the optimal value of  varies with the training dataset, we conduct experiments on Cityscapes leveraging 1/8 labeled images as the training data to explore the impact of  on this dataset, and show the results in Table \ref{impact_lambda_city}.
For Cityscapes, when , GuidedMix-Net achieves 65.8 mIoU on the validation dataset, which is better than other selected value ranges.
We thus fix the value of  to be less than 0.3, and verify the gap between GuidedMix-Net and other approaches.
Relevant results are presented in Table \ref{the experiments of city}.
GuidedMix-Net yields considerable improvements on Cityscapes over other semi-supervised semantic segmentation methods, \textit{i.e.}, mIoU increases of , ,  and  for the 100, 1/8, 1/4 and 1/2 labels, respectively.
The distribution of different classes on Cityscapes is highly imbalanced.
The vast majority of classes are present in almost every image, and the few remaining classes occur scarcely.
As such, inserting a classifier after the encoder to semantically enhance features and assist in matching similar images is unhelpful.
Thus, we use the mixture of randomly selected image pairs in GuidedMix-Net.

We also provide the visul results on Cityscapes in Fig. \ref{qr_city} using only 1/8 labeled images.
The visual differences are subtle, and therefore we follow s4GAN \cite{mittal2019semi} to add a zoomed-in view of informative areas.

\begin{table}[h]
\small
\centering
\caption{Influence of different lambda values on the experimental results of Cityscapes.}
\begin{tabular}{c|c|c|c|c}
\hline 
\multirow{2}{*}{} & \multirow{2}{*}{backbone} & \multicolumn{2}{c|}{Data} & \multirow{2}{*}{mIoU} \\\cline{3-4}
						&      &  labels   & unlabels        &                       \\\hline
 0.1 & \multirow{5}{*}{ResNet101} & \multirow{5}{*}{1/8} & \multirow{5}{*}{7/8} & 64.3\\
 0.2 &  &  &      & 64.0 \\
 0.3 &  &  &      & \textbf{65.8} \\
 0.4 &  &  &      & 65.5 \\
 0.5 &  &  &      & 65.7 \\\hline
\end{tabular}
\label{impact_lambda_city}
\end{table}


\begin{table}[h]
\small
\centering
\caption{Comparison with other semi-supervised semantic segmentation methods under different ratios of labeled data on Cityscapes.}
\begin{tabular}{c|c|c|c|c}
\hline
	SSL & 100 & 1/8 & 1/4 & 1/2 \\
	Methods & labels & labels & labels & labels \\\hline
	AdvSSL \cite{2018Adversarial} & - & 57.1 & 60.5 & - \\
	s4GAN \cite{mittal2019semi} & - & 59.3 & 61.9 & - \\
	CutMix \cite{french2020semi} & 51.2 & 60.3 & 63.9 & - \\
	ClassMix \cite{french2020semi} & 54.1 & 61.4 & 63.6 & 66.3 \\
	Ours & \textbf{56.9} & \textbf{65.8} & \textbf{67.5} & \textbf{69.8}\\\hline
\end{tabular}
\label{the experiments of city}
\end{table}


\begin{table}[h]
\small
\centering
\caption{Influence of different lambda values on the experimental results of PASCAL Context.}
\begin{tabular}{c|c|c|c|c}
\hline 
\multirow{2}{*}{} & \multirow{2}{*}{backbone} & \multicolumn{2}{c|}{Data} & \multirow{2}{*}{mIoU} \\\cline{3-4}
						&      &  labels   & unlabels        &                       \\\hline
 0.1 & \multirow{5}{*}{ResNet101} & \multirow{5}{*}{1/8} & \multirow{5}{*}{7/8} & 37.6\\
 0.2 &  &  &      & 39.4 \\
 0.3 &  &  &      & 39.8 \\
 0.4 &  &  &      & \textbf{40.3} \\
 0.5 &  &  &      & 40.1 \\\hline
\end{tabular}
\label{impact_lambda_context}
\end{table}
\begin{table}[h]
\small
\centering
\caption{Comparison with other semi-supervised semantic segmentation methods under different ratios of labeled data on PASCAL Context.}
\begin{tabular}{c|c|c|c}
\hline
	SSL & \multirow{2}{*}{backbone} & 1/8 & 1/4 \\
	Methods & & labels & labels \\\hline
	AdvSSL \cite{2018Adversarial} & \multirow{3}{*}{ResNet101} & 32.8 & 34.8\\
	s4GAN \cite{mittal2019semi} &  & 35.3 & 37.8 \\
	Ours & & \textbf{40.3} & \textbf{41.7} \\\hline
\end{tabular}
\label{the experiments of context}
\end{table}


\subsection{Results on PASCAL Context}
In addition to Cityscapes, GuidedMix-Net also successfully generalizes to whole scene parsing on PASCAL Context. 
Table \ref{impact_lambda_context} shows the impact of different  values on PASCAL Context when training with the limited 1/8 labeled images.
We select   for our two experiments on PASCAL Context.
Results are shown in Table \ref{the experiments of context}.
PASCAL Context is smaller, but more difficult than PASCAL VOC 2012. 
The proposed method also achieves great improvements over other semi-supervised semantic segmentation approaches with mIoU gains of 14.2 and 10.3 for 1/8 and 1/4 labeled data, respectively.

\subsection{Gap Between GuidedMix-Net and Fully Supervised Image Segmentation Methods}
The availability of large amounts of data is crucial for deep learning approaches.
However, labeling such data is expensive and time-consuming.
Half or even weak annotations, in the form of bounding boxes or image-level labels, are far easier to collect than detailed pixel-level annotations. 
Thus, there is growing interest in semi-supervised learning, which improves performance under low-data conditions. 
However, with the increase of labeled data, semi-supervised training often move towards from "available" to "unavailable".
This is because most semi-supervised methods do not allow the structural information of unlabeled data to be mined, resulting in accumulated errors.
To the best of our knowledge, GuidedMix-Net is the first algorithm to explicitly mine the structural information of unlabeled data, by using labeled data as the guidance to model the unlabeled samples learning.
GuidedMix-Net generates high-quality pseudo masks, in which the distribution of the generation is close to the mask of the labeled samples.
Our experimental results show that the proposed approach has exceeds state-of-the-art results on different datasets.
We also explore the gap between GuidedMix-Net and various fully supervised image segmentation methods, , FCN \cite{long2015fully}, DeepLabV3 \cite{chen2017rethinking}, and DeepLabV3+ \cite{Chen2018EncoderDecoderWA}, on PASCAL VOC 2012.

\begin{table}[t!]
\small
\centering
\caption{Comparison with fully-supervised methods on PASCAL VOC 2012 by used ResNet101.}
\begin{tabular}{c|c|c|c|c}
\hline
	SSL & 1/8 & 1/4 & 1/2 & Full \\
	Methods & labels & labels & labels & labels \\\hline
	FCN \cite{long2015fully} & - & - & -  & 69.9 \\
	DeepLabV3 \cite{chen2017rethinking} & - & - & -  & 77.9 \\
	ANNet \cite{annn} & - & - & -  & 76.7 \\
	DeepLabV3+ \cite{Chen2018EncoderDecoderWA} & - & - & - & 78.6 \\\hline\hline
	GuidedMix-Net & 75.2 & 76.5 & 77.1  & - \\
	+MS and Flip & 76.4 & 77.8 & 78.2  & - \\\hline
\end{tabular}
\label{the gap}
\end{table}


For the experimental settings, it is common to randomly crop 321321 patches for semi-supervised learning, and 512512 patches for supervised learning.
This will widen the performance gap between fully supervised and semi-supervised methods, and therefore we set the same input size for GuidedMix-Net training.
As shown in Table \ref{the gap}, when using an input size of 512512 for training, GuidedMix-Net outperforms fully supervised methods, obtaining mIoUs of 75.2, 76.5 and 77.1 for varying proportions of limited labels of 1/8, 1/4, and 1/2.
The fully supervised methods training with the available labels, for example, FCN \cite{long2015fully} only achieved 69.9 mIoU, while GuidedMix-Net demonstrated excellent results with 1/8 labels (69.9 vs. 75.2), which outperforms FCN by 7.7.
On the  other hand, the performance of GuidedMix-Net relatively lower than DeepLabV3 \cite{chen2017deeplab}, ANNet \cite{annn}, and DeepLabV3+ \cite{Chen2018EncoderDecoderWA} when using 1/8 labels.
We can see that, with an increasing amount of labels, our method generates comparable results by training with 1/2 labels (77.1 vs. 78.6).
To boost the performance of GuidedMix-Net, we apply an external data augmentation process to the validation set.
The results are shown in the last row of Table \ref{the gap}.
The data augmentation techniques include multi-scale (MS) resize (0.5x, 0.75x, 1.0x, 1.25x, 1.5x, 1.75x), and flipping.
These tricks enhance the performance of GuidedMix-Net without adding more labeled data, and obtained 76.4, 77.8, and 78.2 mIoU for 1/8, 1/4, 1/2 labels.
We note that DeepLabV3+ presents 78.6 mIoU, and GuidedMix-Net provides 78.2 mIoU while using only 1/2 labels.
GuidedMix-Net is thus a promising alternative to fully supervised methods, using less labeled data.
