\documentclass[11pt]{article}

\usepackage{setspace}
\usepackage{amssymb,amsmath}
\usepackage{url}
\usepackage{float}


\floatstyle{boxed}
\restylefloat{figure}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{observation}[theorem]{Observation}

 \newenvironment{sketch}{\noindent{\bf Proof Sketch:}}{
 \hspace*{\fill}  \vskip \belowdisplayskip}


\newcommand{\eps}{\varepsilon}
\newcommand{\vp}{\varphi}
\newcommand{\cm}[1]{}
\newcommand{\comment}[1]{}
\newcommand{\ac}{{\cal A}_C}
\newcommand{\lset}{\left \{}
\newcommand{\rset}{\right \}}
\renewcommand{\baselinestretch}{1.125}
\newcommand{\ceil}[1]{\lceil #1 \rceil }
\newcommand{\alg}[1]{\mbox{\sf #1}}
\newcommand{\algsub}[1]{\mbox{\sf\scriptsize #1}}
\newcommand{\lone}[1]{ {\abs{#1}_1}}

\def \From{From }
\newcommand{\hbs}{{\hat{\bar{s}}}}
\newcommand{\bhs}{{\bar{\hat{s}}}}
\newcommand{\hs}{\hat{s}}
\newcommand{\hsir}{{\hat{s}}_{i,r}}
\newcommand{\bx}{\bar{x}}
\newcommand{\bX}{\bar{X}}
\newcommand{\by}{\bar{y}}
\newcommand{\bs}{\bar{s}}
\newcommand{\bb}{\bar{b}}
\newcommand{\bz}{\bar{z}}
\newcommand{\bk}{\bar{k}}
\newcommand{\bxf}{\bar{x}^{f}}
\newcommand{\xf}{x^{f}}
\newcommand{\mOf}{\mO^f}
\newcommand{\br}{B_r}
\newcommand{\byf}{\bar{y}^{f}}
\newcommand{\yf}{y^{f}}
\newcommand{\lr}{L_r}
\newcommand{\lrg}{L^g_r}
\newcommand{\brg}{B^g_r}
\newcommand{\tbr}{{\tilde B}_r}
\newcommand{\tlr}{{\tilde L}_r}
\newcommand{\he}{\hat \eps}
\newcommand{\zr}{Z_r}
\newcommand{\zro}{Z_{r,1}}
\newcommand{\zrt}{Z_{r,2}}

\newcommand{\bS}{S}
\newcommand{\bT}{T}
\newcommand{\bR}{R}
\newcommand{\bK}{K}
\newcommand{\cD}{{\cal D}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cA}{{\cal A}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cf}{{\cal f}}
\newcommand{\bL}{\bar{L}}
\newcommand{\bc}{\bar{c}}
\newcommand{\bw}{\bar{w}}
\newcommand{\bW}{\bar{W}}
\newcommand{\bB}{\bar{B}}
\newcommand{\ing}{\vdash}
\newcommand{\ning}{\nvdash}
\newcommand{\argmax}{\mbox{argmax}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\SUB}{\mbox{SUB}}
\newcommand{\MC}{\mbox{MC}}
\newcommand{\RSAP}{\mbox{RSAP}}
\newcommand{\PCS}{\mbox{PCS}}
\newcommand{\MPCS}{\mbox{PCS}}
\newcommand{\AMLC}{{\cA}_{{MLC}}}
\newcommand{\alphaf}{\alpha_\varphi}
\newcommand{\f}{\varphi}
\newcommand{\tmO}{\tilde{\mathcal{O}}}
\newcommand{\Prb}[1]{{Pr\left[ #1\right] }}
\newcommand{\abs}[1]{ \left| #1 \right |}
\setstretch{1.00}

\newcommand{\Tabs}{xx\= xx\= xx\= xx\= xxxx\= xxxx\= xxxx\= xxxx\=
xxxx\= xxxx\= xxxx\= xxxx\= \kill}
\newcommand{\program}[3]{\begin{figure}\begin{center}
             \fbox{
             \begin{minipage}{\textwidth}
             \begin{tabbing}
                         \Tabs
             #3
             \end{tabbing}
             \end{minipage}
             }
             \end{center}
             \caption{\label{#1} #2}
             \end{figure}}

\newenvironment{proof}{\noindent{\bf Proof:}}{
 \hspace*{\fill}  \vskip \belowdisplayskip}

\newenvironment{dl_proof}[1]{\noindent{\bf Proof of Lemma #1:}}{
 \hspace*{\fill}  \vskip \belowdisplayskip}
 \newenvironment{dl_claim_proof}[1]{\noindent{\bf Proof of Claim #1:}}{
 \hspace*{\fill}  \vskip \belowdisplayskip}
\newenvironment{dl_thm_proof}[1]{\noindent{\bf Proof of Theorem #1:}}{
 \hspace*{\fill}  \vskip \belowdisplayskip}

\newcommand{\qedsymb}{\hfill{\rule{2mm}{2mm}}}
 \def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}
\newcommand{\qed}{\hspace*{\fill}
\vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}\smallskip}
\def\eod{\vrule height 6pt width 5pt depth 0pt}
\newcommand{\A}{\mbox{}}

\newlength{\tablength}
\newlength{\spacelength}
\settowidth{\tablength}{\mbox{\ \ \ \ \ \ \ \ }}
\settowidth{\spacelength}{\mbox{\ }}
\newcommand{\tab}{\hspace{\tablength}}
\newcommand{\tabstar}{\hspace*{\tablength}}
\newcommand{\spacestar}{\hspace*{\spacelength}}
\def\obeytabs{\catcode`\^^I=\active}
{\obeytabs\global\let^^I=\tabstar}
{\obeyspaces\global\let =\spacestar}
\newenvironment{display}{\begingroup\obeylines\obeyspaces\obeytabs}{\endgroup}
\newenvironment{prog}{\begin{display}\parskip0pt\sf}{\end{display}}
\newenvironment{pseudocode}{\smallskip\begin{quote}\begin{prog}}{\end{prog}\end
{quote}\smallskip}

\newcommand{\negA}{\vspace{-0.07in}}  \newcommand{\negB}{\vspace{-0.12in}}  \newcommand{\negC}{\vspace{-0.15in}}  \newcommand{\posA}{\vspace{0.08in}}





\setlength{\textwidth}{6.2in}
\setlength{\textheight}{8.9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{10pt}
\setlength{\topsep}{0in}
\setlength{\topmargin}{0.3in} \setlength{\itemsep}{0in}
\setlength{\footskip}{0.4in} 

\begin{document}
\title{\Large
Approximations for Monotone and Non-monotone Submodular Maximization
with
Knapsack Constraints\footnote{A preliminary version of this paper appeared in the Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete
Algorithms, New York, January 2009.}}
\author{
Ariel Kulik\thanks{Computer Science Department, Technion, Haifa 32000,
Israel. \mbox{E-mail: {\tt ariel.kulik@gmail.com}}} \and Hadas
Shachnai\thanks{Computer Science Department, Technion, Haifa 32000,
Israel. \mbox{E-mail: {\tt hadas@cs.technion.ac.il}}. Work partially supported
by the Technion V.P.R. Fund, by Smoler Research Fund, and by the Ministry of Trade and Industry MAGNET
program through the NEGEV
Consortium (www.negev-initiative.org). }
\and Tami Tamir
\thanks{ School of Computer Science, The
Interdisciplinary Center, Herzliya, Israel.
\mbox{E-mail: {\tt tami@idc.ac.il}}}
}
\date{}

\maketitle


\begin{abstract}

Submodular maximization generalizes many fundamental problems in
discrete optimization, including Max-Cut in directed/undirected
graphs, maximum coverage, maximum facility location and marketing
over social networks.

In this paper we consider the problem of maximizing any submodular
function subject to  knapsack constraints, where  is a fixed
constant. We establish a strong relation between the discrete
problem and its continuous relaxation, obtained through {\em
extension by expectation} of the submodular function. Formally, we
show that, for any non-negative submodular function, an
-approximation algorithm for the continuous relaxation
implies a randomized -approximation algorithm for
the discrete problem. We use this relation to improve the best
known approximation ratio for the problem to , for any
, and to obtain a nearly optimal
approximation ratio for the monotone case, for
any .
We further show that the probabilistic domain defined by a
continuous solution can be reduced to yield a polynomial size
domain, given an oracle for the extension by expectation. This
leads to a deterministic version of our technique.
\end{abstract}
\bigskip



\section{Introduction} \label{sec:intro}


 A real-valued function , whose domain is all the
 subsets of a universe , is called {\em submodular} if,
 for any ,

The concept of submodularity, which can be viewed as a discrete analog of convexity,
plays a central role in combinatorial theorems and algorithms
(see, e.g., \cite{f99}
and the references therein, and the comprehensive
surveys in \cite{FMV07,Vo08,LMNS09}).
Submodular maximization generalizes many fundamental problems in
discrete optimization, including Max-Cut in directed/undirected
graphs, maximum coverage, maximum facility location and marketing
over social networks (see, e.g., \cite{HMS08}).

In many settings, including set covering or matroid optimization, the
underlying submodular functions are monotone, meaning that  whenever . In other settings, the function 
is not necessarily monotone.
A classic
example of such a submodular function is ,
where  is a cut in a graph (or hypergraph) 
induced by a set of vertices , and  is the weight of an edge .
An example for a monotone submodular function is , defined on a subset of vertices in bipartite graph
. For any , , where  is the neighborhood function (i.e.,  is the
set of neighbors of ), and  is the profit of , for any .
The problem
 is classical maximum coverage.


In this paper we consider the following
problem of maximizing a non-negative {\em submodular}
set function subject to 
{\em knapsack constraints} ({). Given a -dimensional
budget vector , for some , and an oracle for a
non-negative submodular set function  over a universe ,
where each element  is associated with a -dimensional
cost vector , we seek a subset of elements 
whose total cost is at most , such that  is maximized.

There has been extensive work on maximizing submodular {\em monotone}
functions subject to matroid constraint.\footnote{A (weighted)
matroid is a system of `independent subsets' of a universe, which
satisfies certain {\em hereditary} and {\em exchange} properties
\cite{Sch03}.} For the special case of uniform matroid,
i.e., the problem
, for some , Nemhauser et. al
showed in \cite{NWF78}
that a greedy algorithm yields a ratio of  to the
optimum. Later works presented greedy algorithms that achieve this
ratio for other special matroids or for
variants of maximum coverage (see, e.g., \cite{as04,
kmn99,s04,CK04}).
For a general matroid constraint, Calinescu et al. showed in
\cite{ccpv07} that a scheme based on solving a continuous
relaxation of the problem followed by {\em pipage rounding} (a
technique introduced by Ageev and Sviridenko \cite{as04}) achieves
the ratio of  for maximizing submodular monotone
functions that can be expressed as a sum of weighted rank
functions of matroids. Subsequently, this result was extended by
Vondr\'{a}k \cite{Vo08} to general monotone submodular functions.

 The bound of  is the best possible for all of the
above problems. This follows from the lower bound of Nemhauser and
Wolsey \cite{nw78} in the oracle model, and the later result of
Feige \cite{f98} for the specific case of maximum coverage, under
the assumption that .

Other variants of monotone submodular optimization were also considered.
In~\cite{BKNS10}, Bansal et al. studied the problem of maximizing a monotone submodular function
subject to  knapsack constraints, for arbitrary , where each element appears in up to 
constraints, and  is fixed. The paper presents a  and
 approximations for
this problem. Demaine and Zadimoghaddam \cite{DZ10} studied
bi-criteria approximations for monotone submodular set function
optimization.

The problem of maximizing a {\em non-monotone} submodular function has been studied as well. Feige et al. \cite{FMV07}
 considered (unconstrained) maximization of a  general non-monotone submodular function.
 The paper gives several (randomized and deterministic)
approximation algorithms, as well as hardness results, also for the special case
where the function is {\em symmetric}.

Lee et al. \cite{LMNS09} studied the problem of
maximizing a general submodular function under linear and matroid
constraints. They proposed algorithms that achieve approximation
ratio of  for  the problem with  linear constraints
and a ratio of  for  matroid constraints,
for any fixed integer .

Improved lower and upper bounds for non-constrained and constrained submodular maximization
were recently derived by Gharan and Vondr\'{a}k
\cite{SV10}. However, this paper does not consider knapsack constraints.


\comment{ Submodular maximization problem where the focus of many
works. The best approximation ratio for maximizing non-monotone
submodular function without any constraints is  general
functions, and  for symmetric function, both algorithm
are due to \cite{FMV07}. { {\bf write a survey of results for
constraints maximization}. For monotone functions mention the
classic greedy algorithm for cardinality and knapsack constraint.
Later matroid constraints, and afterwards our results. For
non-monotone mention Sviridenko. } }

Several fundamental algorithms for submodular maximization
(see, e.g., \cite{as04,ccpv07,Vo08,LMNS09})
use a continuous extension of submodular function, to which we
refer as \emph{extension by expectation}. Given a submodular
function , we define .
For any , let   be a random
variable such that  with probability  (we say that
). Then

The general framework of these algorithms is to obtain first a
fractional solution for the continuous extension, followed by
rounding which yields a solution for the discrete problem.

Using the definition of , we define the continuous relaxation of our problem called
{\em continuous \SUB}. Let  be the polytope of the instance, then the problem
is to find  for which  is
maximized. For , an algorithm  yields -approximation
 for the continuous problem with respect to a submodular
function , if for any assignment of non-negative costs to the elements, and for any non-negative budget,  finds a feasible solution for
continuous  of value at least , where  is the value of an optimal
(integral) solution for  with the given costs and budget.


For some specific families of submodular functions,
linear programming can be used to derive such approximation algorithms
(see e.g  \cite{as04,ccpv07}). For monotone submodular functions,
Vondr\'{a}k presented in \cite{Vo08} a
-approximation algorithm for the continuous
problem. Subsequently,
Lee et al. \cite{LMNS09} considered the problem of maximizing {\em any}
submodular function with multiple knapsack constraints and developed
a -approximation algorithm for the continuous
problem; however, noting that the rounding method of \cite{KST09},\footnote{
The paper \cite{KST09} is a preliminary version of this paper.}
which proved useful for monotone functions, cannot be applied in the non-monotone case,
a -approximation was obtained for the discrete
problem, by using simple randomized rounding.
This gap of approximation ratio between the continuous and the
discrete case led us to further develop the technique in \cite{KST09},
so that it can be applied also for non-monotone functions.


\subsection{Our Results}
In this paper
we establish a strong relation
between the problem of maximizing any submodular function subject
to  knapsack constraints and its continuous relaxation.
Formally, we show (in Theorem~\ref{thm:continuous_eq}) that for
any non-negative submodular function, an -approximation
algorithm for the continuous relaxation implies a randomized
-approximation algorithm for the discrete
problem. We use this relation to
obtain approximation ratio of  for {\SUB}, for any
, thus improving the best known result for the problem,
due to Lee et al. \cite{LMNS09}. For the case where the objective
function is monotone, we use this relation to obtain a nearly
optimal  approximation, for any . An
important consequence of the above relation is that for any
class of submodular functions,
a future improvement of the approximation ratio for the continuous
problem, to a factor of ,
immediately implies an approximation ratio of  for
the original instance.

Our technique applies random sampling on the solution space, using
a distribution defined by the fractional solution for the problem.
In Section \ref{mcmb:deter} we show how to convert a feasible
solution for the continuous problem to another feasible solution
with up to  fractional entries, given an oracle to
the extension by expectation. This facilitates the usage of
exhaustive search instead of sampling, which leads to a
deterministic version of our technique. Specifically, we obtain a
deterministic -approximation for general instances
and -approximation for instances where the
submodular function is monotone.
For the special case of maximum
coverage with  knapsack constraints,
that is,  where the objective
function is   for a given bipartite graph  and
profits , this result leads to a deterministic approximation algorithm, since the extension by
expectation of  can be deterministically evaluated.
Some basic properties of submodular functions are given in
Appendix \ref{app:basic_props}.

\subsection{Recent Developments}

Subsequent to our study of maximizing monotone submodular
functions subject to multiple knapsack constraints \cite{KST09},
Chekuri et al. \cite{CVZ10}
showed that, by using a more
sophisticated rounding technique,
the algorithm in
\cite{KST09} can be applied to derive a
-approximation for
maximizing a submodular function subject to  knapsack
constraints and a matroid constraint.
Specifically, given a fractional solution for the problem, the
authors
define a probability distribution over the solution space, such
that all of elements in the domain of the distribution are inside
the matroid; these elements also satisfy Chernoff-type
concentration bounds, which can be used to prove some of the
probabilistic claims in \cite{KST09}.
The desired approximation ratio is obtained by using the algorithm
of \cite{KST09} with sampling replaced by the above distribution
in the rounding step.
Recently, the same set of authors improved in \cite{CVZ10a} the
bound of  presented here to .


\comment{
 In~\cite{CVZ10} Chekuri, Vondr\'{a}k and Zenklusen
presented a -approximation algorithm for monotone
maximization subject to  knapsack constraints and a matroid
constraint. Their main tool was to provide a distribution (given a
fractional solution), such that every element in the
distribution's domain is inside the matroid, and also maintains a
Chernoff-type concentration bounds which can be used to prove our
probabilistic claims. By applying this distribution over our
algorithm (while referring to \cite{KST09}), instead of a
sampling, the desired approximation ratio was attained. }


\section{Maximizing Submodular Functions}


\label{sec:randomized_mcmb}

In this section we describe our framework for maximizing a
submodular set function subject to multiple linear constraints.
For short, we call this problem .

\subsection{Preliminaries}
\label{sec:prel}

\paragraph{Notation:}
An essential component in our framework is the distinction between
elements by their costs. We say that an element 
is \emph{small} if ; otherwise,
the element is \emph{big}.




Given a universe , we call a subset of elements 
{\em feasible} if the total cost of elements in  is bounded by
. We say that   is \emph{-nearly feasible} (or
\emph{nearly feasible}, if  is known from the context) if the total
cost of the elements in  is bounded by . We refer
to  as the value of . Similar to the discrete case,
 is feasible if .

For any subset , we define  by . It is easy to
verify that if  is a submodular set function then  is also
a submodular set function. Finally, for any set ,
we define , where , and
 .  For a fractional solution , we
define  and
.


\paragraph{Overview:}
Our algorithm consists of two phases, to which we refer as
{\em rounding procedure} and {\em profit enumeration}. The rounding procedure
yields an -approximation for
instances in which there are no big elements, using an
-approximate solution for the continuous problem. It relies heavily
on Theorem \ref{thm:prb_claim} that gives some conditions on the
probabilistic domain of solutions; these conditions guarantee that the
expected profit of the resulting nearly feasible solution is high. This
solution is then converted to a feasible one, by using a
fixing procedure. We first present a randomized version
and later show
how to derandomize the rounding procedure.


The profit enumeration phase uses enumeration over the most
profitable elements in an optimal solution; then it reduces
a general instance to another instance with no big elements,
on which we apply the rounding procedure.

Finally, we combine the above results with an algorithm for the
continuous problem (e.g., the algorithm of \cite{Vo08}, or
\cite{LMNS09}) to obtain approximation algorithm for .

\subsection{A Probabilistic Theorem}
\label{sec:prb_claim}

We first prove a general probabilistic
theorem which refers to a slight generalization of our problem
(called {\em generalized }).
In addition to the standard
input for the problem, there is also a collection of subsets ,
such that if  and 
then . The goal is  to find
a subset , such that  
and  is maximized.

\begin{theorem}
\label{thm:prb_claim}
For a given input of generalized ,
let  be a distribution over 
and  a random variable , such that
\begin{enumerate}
\item , where  is an optimal
solution for the given instance.
\item For any , 
\item For any , , where
  and  are independent random
variables.
\item
\label{thm:prb_claim:c4}
For any  and , it holds that
either  or  is fixed.
\end{enumerate}
Let  if  is -nearly feasible, and  otherwise. Then  is
always -nearly feasible, , and
.
\end{theorem}

To prove the results in this section, it suffices to use a special case of Theorem
 \ref{thm:prb_claim} (formulated as our next result). We use
this theorem in its full generality in \cite{full}, in developing approximation
 algorithms for variants of maximum coverage and GAP.

\begin{lemma}
\label{lemma:expected_random}
Let  be a feasible fractional solution
such that
, where  is the optimal solution for generalized .
 Let  be a random set such
that  (i.e., for all ,  with probability ),
 and
let  be a random set such that  if  is -nearly feasible, and  otherwise.
Then  is always -nearly feasible, and
.
\end{lemma}

\begin{dl_thm_proof}{\ref{thm:prb_claim}}
Define an indicator random variable  such that  if  is -nearly
feasible, and  otherwise.

\begin{claim}
\label{lemma:prob_F}
.
\end{claim}
\begin{proof}
For any dimension ,
it holds that .
Define . Then,

The first inequality holds since , and the second
inequality follows from the fact that  for
.
Recall that, by the Chebyshev-Cantelli inequality, for any  and a random
variable ,

Thus,

By the union bound, we have that

\end{proof}

For any dimension , let ,
and define , then
 denotes the maximal relative deviation of the cost from the -th entry
in the budget vector, where the maximum is taken over .

\begin{claim}
\label{lemma:prob_l} For any ,

\end{claim}
\begin{proof}
By the Chebyshev-Cantelli inequality we have that, for any dimension
,

and by the union bound, we get that

\end{proof}


\begin{claim}
\label{lemma:bounding_via_R}
For any integer , if  then

\end{claim}

\begin{proof}
The set  can be partitioned to  sets
 such that each of these
sets is a feasible solution. Hence, .
By Lemma~\ref{lemma:submodular_summation}, we have that .
\end{proof}
Combining the above results we have

\begin{claim}
\label{lemma:mcmb_nearly_feasible}
.
\end{claim}

\begin{proof}
By Claims \ref{lemma:prob_F} and \ref{lemma:prob_l}, we have that

Since the last summation is a constant, and , we have that
 
where  is some constant. It follows that
 
Finally, since  if  and  otherwise, we have that

 \end{proof}
By definition,  is always -nearly feasible, and . This
completes the proof of the theorem.
\end{dl_thm_proof}


\subsection{Rounding Instances with No Big Elements}
\label{sec:random_mcmb}
In this section we present an -approximation
algorithm for  inputs with no big elements, given an
 -approximate solution for the continuous problem.
 Inputs with no big elements are easier to tackle. Indeed, any nearly feasible solution
 for such input can be converted to a feasible one, with only
 a small harm to the total value.


\begin{lemma}
\label{lemma:nearly_fix}
Let  be an -nearly feasible solution
with no big elements,
then  can
be converted in polynomial time to a feasible solution ,
such that .
\end{lemma}

\begin{proof}
In fixing the solution  we handle each dimension separately.
For any dimension , if   then
no modification is needed; otherwise, . Since all elements
in  are small, we can partition  into  disjoint subsets 
such that  for any
 , where .
Since the function  is submodular, by Lemma \ref{lemma:submodular_summation_inequality},
 we have that
. Hence, there exists a value  such that  (note
that  may be negative). Now, , and . We repeat this step for all  to obtain a feasible set  satisfying
.
\end{proof}
Combined with Theorem \ref{thm:prb_claim},
we have the following rounding algorithm. \\ \\
\noindent
{\bf Randomized Rounding Algorithm for  with No Big Elements} \\
\noindent
{\bf Input:} A  instance, a feasible solution   for the continuous problem, with
.
\begin{enumerate}
\item Define a random set . Let  if
 is -nearly feasible, and  otherwise.
\item Convert  to a feasible
set  as in the proof of Lemma \ref{lemma:nearly_fix} and return .
\end{enumerate}

Clearly, the algorithm returns a feasible solution for the problem.
By Theorem \ref{thm:prb_claim}, . By
Lemma~\ref{lemma:nearly_fix},
. Hence, we have

\begin{lemma}
\label{thm:nobig_alg}
For any instance of  with no big elements,
any feasible solution  for the continuous problem with 
can be converted to a feasible solution for  in polynomial running time
with expected profit at least .
\end{lemma}


\subsection{A Randomized Approximation Algorithm}
\label{sec:mcmb}


Given an instance of  and a subset ,
define another instance of , to which we refer as the {\em residual
problem with respect to} , with  remaining the objective function.
The budget for the residual problem is
,
and the universe  consists of all elements 
such that ,
and all elements in .
Formally,

The new cost of element  is  for any , and
 for any . It follows that there
are no big elements in the residual problem.
Let  be a feasible solution for the residual problem with respect
to . Then . Thus,
any feasible solution for the residual problem is also feasible for
the original instance.

Consider the following algorithm.
\\ \\
\noindent
{\bf A Randomized Approximation Algorithm for }\\
\noindent
{\bf Input:} A  instance and an -approximation algorithm
  for continuous  with respect to
the function .
\begin{enumerate}
\item For any  such that 
\begin{enumerate}
\item
Use  to obtain an -approximate solution  for the
continuous residual problem with respect to .
\item Use the Randomized Rounding Algorithm of Section~\ref{sec:random_mcmb}
to convert  to a feasible solution  for
the residual problem.
\end{enumerate}
\item Return the best solution found.
\end{enumerate}

\begin{lemma}
The above approximation algorithm returns an
-approximate solution for 
and uses a polynomial number of calls to algorithm .
\end{lemma}

\begin{proof}
By Lemma~\ref{thm:nobig_alg},
in each iteration the algorithm finds a feasible solution  for the residual problem.
Hence, the algorithm always
returns a feasible solution for the given  instance.

Let   be an
optimal solution for the input  (we use  to denote both an
optimal sub-collection of elements and the optimal value).
For , let
,
and assume that the elements  are ordered by their residual profits, i.e.,
.

Consider the iteration in which ,
and define  . The set  is clearly a feasible solution
for the residual problem with respect to .
We show a lower bound for .
The set 
consists of elements in  that are big with
respect to the residual instance. The total cost of elements in  is
bounded by  (since  is a feasible solution), and thus
.

Since , for any  it holds that
, and we get
.
Thus,  .
Since , we have that
.

Thus, in this iteration we get a solution  for the residual problem
with , and the solution  obtained after the
rounding satisfies .

\end{proof}
We summarize in the next result.

\begin{theorem}
\label{thm:continuous_eq} Let  be a submodular function, and
suppose there is a polynomial time -approximation
algorithm for the continuous problem with respect to . Then
there is a polynomial time randomized
-approximation algorithm for  with respect to
, for any .
\end{theorem}
Since there is a -approximation algorithm
for general instances of continuous 
\cite{LMNS09},
we have

\begin{theorem}
\label{thm:rand_alg_non_monotone}
There is a polynomial time randomized -approximation
algorithm for , for any  .
\end{theorem}
Since there is a  approximation algorithm for  with monotone objective function \cite{Vo08}
we have

\begin{theorem}
There is a polynomial time randomized -approximation
algorithm for  with monotone objective function, for any  .
\end{theorem}




\section{A Deterministic Approximation Algorithm}
\label{mcmb:deter}
In this section we show how the
algorithm of Section \ref{sec:random_mcmb} can be derandomized,
assuming we have
an oracle for , the extension by expectation of .
For some families
of submodular functions,  can
be directly evaluated; for a general function ,  can be
evaluated with high accuracy by sampling , as in \cite{Vo08}.

The main idea is to reduce the number of fractional entries in the
fractional solution , so that the number of values a random set 
can get is polynomial in the input size (for a fixed value of ).
Then,
we go over all the possible values, and we are promised to obtain a solution
of high value.

A key tool in our derandomization is the \emph{pipage rounding}
technique of Ageev and Sviridenko \cite{as04}. We give below a brief overview
of the technique.
For any element , define the unit vector , in which

for any , and .
Given a fractional solution  for the problem
and two elements , such that  and  are both
fractional, consider the vector function

(Note that  is equal to  in all entries
except ).
Let   and   (for short,  and  )
be the maximal and minimal value of  for which .
In both , the entry of either  or  is
integral.

Define   over the domain . The
function  is convex (see \cite{ccpv10} for a detailed proof),
thus

has fewer fractional entries than , and .
By appropriate selection of , such that  maintains feasibility (in some sense),
we can repeat the above step
to gradually decrease the number of fractional entries.
We use the technique to prove the next result.

\begin{lemma}
\label{lemma:derand1}
Let  be a solution having  or less fractional entries (i.e.,
),
and  for some . Then  can be converted to a vector
 with at most 
fractional entries,
such that ,
and , in time polynomial in .
\end{lemma}

\begin{proof}
Let  be the set of all fractional entries.
We define a new cost function  over the elements in .



Note that for any , , and

for all .
The number of different values  can get for 
is bounded by  (since all elements are
small, and ). Hence the number of different
values  can get for  is bounded by
.

We start with , and while there are  such that  and  are both
fractional and , define 
and .
Since  and  have the same cost (by ), it holds that
.
If ,
then set , otherwise .
In both cases  and . Now, repeat
this step with .
Since in each iteration the number of fractional entries in  decreases,
 the process will terminate (after at most  iterations)
with a vector  such that ,
,
and there are no two elements  with , where
 and  are both fractional. Also, for any , the entry
 is integral (since  was integral and the entry was not modified by
the process). Thus, the number of fractional entries in  is
at most .
Now, for any dimension ,

This completes the proof.
\end{proof}


Using the above lemma, we can reduce the number of fractional
entries in  to a number that is poly-logarithmic in .
However, the number of values 
 remains super-polynomial. To reduce further the number
of fractional entries, we apply the above step twice,
that is, we convert  with at most  fractional entries to 
with at most
. We
can then
apply the conversion again, to obtain  with at most
 fractional entries.

\begin{lemma}
\label{lemma:multiple_pipage}
Given a vector  and a constant ,
let  be a vector satisfying
.
Then  can be converted in time polynomial in  to a vector
 with at most 
  fractional entries, such that ,
and ,
\end{lemma}


The next result follows immediately from
Lemma~\ref{lemma:expected_random} ( is the value of an optimal solution for ).

\begin{lemma}
\label{lemma:expected_enumeration}
Given  such that  is a feasible fractional solution
with ,
there exists
a realization of the random variable , such that the solution
 is nearly feasible, and .
\end{lemma}
}

Consider the following rounding algorithm.\\

\noindent
{\bf Deterministic Rounding Algorithm for  with No Big Elements} \\
\noindent
{\bf Input:} A  instance, a feasible solution   for the continuous problem, with
.
\begin{enumerate}
\item
Define 
(note that ).
\item Convert  to 
such that  is fractionally feasible, the number
of fractional entries in  is , and
,
as in Lemma \ref{lemma:multiple_pipage}.
\item Enumerate over all possible realizations of .
For each such realization,
if the solution  is -nearly feasible
convert it to a feasible solution  (see Lemma \ref{lemma:nearly_fix}).
Return the solution with maximum value among the feasible solutions found.
\end{enumerate}

By Theorem~\ref{thm:prb_claim}, the algorithm returns a feasible solution of value at least .
Also, the running time of the algorithm is polynomial when  is a fixed
constant. Replacing the
randomized rounding step in the
algorithm of Section~\ref{sec:mcmb} with the above Deterministic Rounding Algorithm,
we get the following result.


\begin{theorem}
\label{thm:continuous_det_eq} Let  be a submodular function,
and assume we have an oracle for . If there is a deterministic
polynomial time -approximation algorithm for the
continuous problem with respect to , then there is a polynomial
time deterministic -approximation algorithm for
 with respect to , for any .
\end{theorem}

We note that,
given an oracle to , both the algorithms of \cite{Vo08} and \cite{LMNS09} for the
continuous problem are deterministic, thus we get
the following.

\begin{theorem}
\label{thm:det_alg}
Given an oracle for ,
there is a polynomial time deterministic  -approximation
algorithm for  with a monotone function, for any .
\end{theorem}

\begin{theorem}
\label{thm:det_alg_non_monotone}
Given an oracle for ,
there is a polynomial time deterministic  -approximation
algorithm for  for any .
\end{theorem}

For the problem of maximum coverage with  knapsack
constraints, i.e.,   where the objective function is
, for a given bipartite graph  and profits
, the function  can be evaluated deterministically
(see~\cite{as04}). This yields the following result.

\begin{theorem}
There is a polynomial time deterministic
-approximation algorithm for maximum coverage with
 knapsack constraints.
\end{theorem}

\comment{
\section{Maximum Coverage with Multiple Packing and Cost
Constraints
}
\label{sec:budgeted_max_coverage}
In this section we consider the problem of {\em maximum coverage with multiple
packing and cost constraints ()}.
Let 
denote the maximal number of sets to which an element belongs, and let

We give below an -approximation algorithm for the problem.
 We note that, for any , . In
solving , our algorithm uses the following continuous version of the problem. Let  and .
For short, we write  and . Given an input for , we say that 
\comment{
Recall that the problem of {\em maximum coverage with multiple packing and cost
constraints ()} is
the following generalization of the maximum coverage problem.
Given is a collection of sets  over a ground set
. Each element  has a profit 
and a -dimensional size vector , such that  for all
. Each set  has -dimensional
weight vector .
Also given is a -dimensional capacity vector ,
and a -dimensional weight bound vector .
A solution for the problem is a collection of subsets 
and a subset of elements , such that for any 
there is  such that .
A solution is feasible if the total weight of subsets in  is
bounded by  and the total size of elements in  is
bounded by . The profit of a solution  is
the total profit of elements in . The objective
of the problem is to find a feasible solution with
maximal profit.

Denote the maximal number of sets a single element belongs
to by , and let . Our
objective is to obtain a -approximation algorithm for the
problem. Note that for any  it holds that .

For solving the problem, our algorithm uses a slightly different point of view.
Given an input for the problem, we say that a pair  where  and  } is a solution
if, for any  and  it holds that  (for short, we write ), and for any 
and  it holds that . Intuitively,  is an indicator for the selection of the set  into the solution, and
 is an indicator for the selection of the element  by the set  into the solution. We say that such a solution is feasible if,
for any   it holds that  (the total size of elements does
not exceed the capacity), and for any  it holds that  (the total weight of
subsets does not exceed the weight bound).
The value (or profit)  of the solution  is defined by . By the above definition, a solution consists of fractional values. We say that a solution
 is \emph{semi-fractional}
if 
(that is, sets cannot be fractionally selected, but elements can be). Also,
we say that a solution is \emph{integral} if both 
and .

Two computational problems arise from the above definitions. The first
is to find a semi-fractional solution of maximal profit. We refer to this
problem as the {\em semi-fractional problem}. The second
is to find an integral solution of maximal profit, to which we refer as
the {\em integral problem}. It is easy to see that the integral problem is
equivalent to , therefore our objective is to find an optimal solution
for the integral problem.

\paragraph{Overview:} To obtain an approximation algorithm for the integral problem,
we first show how it relates to the semi-fractional problem. In particular, we show
that, given an -approximation algorithm for the
semi-fractional problem, we can derive approximation algorithm with
the same approximation ratio for the integral problem.
Next, we interpret the semi-fractional problem as a  instance
whose universe has infinite size.
We then use the framework developed in Section \ref{sec:randomized_mcmb}
to solve this problem. As direct enumeration over the most
profitable elements in an optimal solution is impossible here, we
guess which sets are the most profitable in an optimal solution.
We use this guessing to obtain a fractional solution (with
polynomial number of non-zero entries), such that the conditions of
Theorem \ref{thm:prb_claim} are satisfied. Together with a fixing procedure,
applied to the resulting nearly feasible solution, this leads to our
approximation algorithm. The process can be derandomized by using the
same tools as in Section \ref{mcmb:deter}.

\subsection{Reduction to the Semi-fractional Problem}

We first show that any semi-fractional solution for the problem can
be converted to a solution with at least the
same profit and at most  fractional
entries. Next, we show how this property
enables to enumerate over the most profitable elements in an optimal solution.
Throughout this section we assume that, for some constant
, we have
an -approximation algorithm for the semi-fractional problem.

\begin{lemma}
\label{lemma:frac_fix}
Let  be a feasible semi-fractional solution. Then
 can be converted in polynomial time to another feasible semi-fractional
solution  with at most  fractional entries, such
that .
\end{lemma}

\begin{proof}
Let  be a semi-fractional feasible solution. W.l.o.g,
we assume that, for any , ,
and if  then for any 
it holds that . Note that  any
solution can be easily converted to such a solution having
the same profit.
If there are more than  fractional entries,
let  be the size
vectors of the corresponding elements, and
let  be the corresponding sets.
Since ,
there must be a linear dependency between the size
vectors. W.l.o.g we can write
 for .
We can define  by

for , and  for any other
entry. As long
as ,  is
a semi-fractional feasible solution. Let  and  be
the maximal and minimal values of  for which
. The number of
fractional entries in  and  is
smaller than the number of fractional entries in .
Also,  is a linear function,
thus either  or
. Thus, we can
convert  to a feasible solution  that has less fractional
entries, such that .
By repeating the above process
we can obtain a fractional
solution with at most  fractional entries.
\end{proof}
We use Lemma \ref{lemma:frac_fix} to prove the next result.

\begin{lemma}
Given an -approximation algorithm for the semi-fractional
problem, an -approximation algorithm for the integral problem can be
derived in polynomial time.
\end{lemma}

\begin{proof}
Given a collection  of pairs  of an element
 and a set , such that ,
denote the collection of sets in  by ,
and the collection of elements in  by . We define a
residual instance for the problem as follows. The
elements are:

where the size of  is , and the profit
of  is .
The sets are , where
, and


The weight
bound of the residual instance is ,
where , and
the capacity is , where
.

Clearly, a solution of profit  for the
residual instance with respect to a collection 
gives a solution of profit 
for the original instance, where .
Let  be an optimal solution for the integral problem.
W.l.o.g. we assume that for all , 
(that is, no element is selected by more than one set).
Let  be the collection of 
most profitable elements   for which there exists 
such that  (note that there is a unique set  for
each ). Define .
It is easy to verify that the optimal integral solution for
the residual problem with respect to  is
.

Now, assume that we have an -approximation
algorithm for the semi-fractional problem. Then the algorithm
returns a fractional solution  with
.
By Lemma
\ref{lemma:frac_fix}, this solution can be converted
to a solution  with up to  fractional
entries satisfying
. Now, consider rounding
down to zero the value of each fractional entry in
.
This results in a new feasible integral solution

with  (since
the profit of any element in the residual solution is bounded by
).
Hence, we obtain a solution for the integral problem of value
at least

Thus, we have an -approximation for the optimum of the integral problem.
To apply this
technique, we need to guess the correct set , which
can be done in time  for a constant .
\end{proof}

In Theorem \ref{thm:semi_frac} we show that there is a polynomial time
-approximation algorithm for the semi-fractional problem,
where  is defined in (\ref{eq:def_alphaf}).
Thus, we have the following.

\begin{theorem}
There is a polynomial time -approximation algorithm
for  for any , where   is the maximal
number of sets to which a single element belongs.
\end{theorem}



\subsection{Solving the Semi-fractional Problem}
\subsubsection{A Submodular Point of View}

Let 
be an optimal solution for an instance of the semi-fractional problem. W.l.o.g.,
we may assume that for any element , .
For any , we define the profit of  with respect
to the solution  by 
(note that if  then ).
Since   holds for any , this
means that .

Given , the collection of the 
most profitable sets in , we define a maximization
problem for a submodular function, with 
linear constraints. Let 
(we guess the set ).
\begin{itemize}
\item
For each  and any vector  satisfying:
 for all ,
add the element
 to the universe . The cost of this
element is , where  for ,
and  for .
\item
For each set  and a vector  satisfying
\begin{enumerate}
\item[1.]
 for all ,
\item[2.]
for any 
it holds that , and
\item[3.]
for any  it holds that ,
\end{enumerate}
add   to .
The cost of this element is , where
 for ,
and  for .
\end{itemize}

The budget vector  for this instance is the vector 
concatenated to ,
that is,  for , and 
for .
Define  for any ,
by

and    by .
Since each  is a submodular non-decreasing set function,  is
a submodular non-decreasing set function as well.
It is easy to see  that a solution of value  for the above instance of
  yields a solution of the same value (profit) for the
semi-fractional problem.

Let the size of a set

(with respect to the solution )
be . We say that
a set  is \emph{small} if
 and ; otherwise it is
{\em big}.
Consider the following solution for the above  instance.
For each , define the vector  by ,
for all , and add
the element  to the solution; for each
 such that  is {\em small},
define  by  and add  to the solution.
Denote the resulting solution by .
It can be easily verified that  is a feasible solution,
and it holds that

The last inequality holds since the number of big sets in 
is bounded by , and the profit of each of these
 sets is bounded by . This implies that the value of the optimal
solution for the  instance
 is between  and .

\subsubsection{Obtaining a Distribution on the Universe of the Submodular
Problem}
We now use the technique of Section \ref{sec:randomized_mcmb}
for solving the  instance. To do so, we first need to obtain a fractional
feasible solution. As the size of  may be unbounded, we cannot
use the algorithm of Vondr\'{a}k \cite{Vo08}. Thus, we obtain a fractional
solution by using
a linear programming formulation of the problem. Let 
be the collection of all sets  in  such that ,
or
.
Consider the following linear program:




Let  be an optimal solution for  of value
. Clearly,  is greater or equal to the
optimal solution of the  instance; thus, by (\ref{eq:lb_fV}),
. We use this solution to
generate fractional solution for the  instance. Define  as follows.
For any  such that , let ,
where ,
and we set
. For any other , set .
For any ,
define , then clearly
.
\begin{lemma}
\label{lp_round1}
Let  be a random variable such that
, then for any , , where  is defined in (\ref{eq:def_fj}).
\end{lemma}
We use in the proof the next claim.
\begin{claim}
\label{claim:lb_Yj_claim}
For any  and ,

where  is defined in (\ref{eq:def_alphaf}).
\end{claim}
\begin{proof}
Let , then .
Also,  for .
Hence,  for , and the claim holds.
\end{proof}

\begin{dl_proof} {\ref{lp_round1}}
For the case where  the claim trivially holds, thus
we assume below that .
Let  be an indicator random variable for , for
any . The random variables  are independent,
and .  Then,


Let 
 and .
Let  be an integer
such that, for any  with ,
 is an integral multiple of 
 (assuming all values are rational, such a value of  exists).
 Let  be a set of indicator random variables used as follows.
Whenever  is selected in our random process (i.e., ),
randomly select 
 indicators among  with uniform distribution.
 For ,  if  was selected by some , 
(we say that  is {\em selected} in this case), otherwise .
 In this process, the probability of a specific indicator  to be
 selected by a specific  is zero when , and
 otherwise.
Hence, we get that, for all ,

The first inequality follows from the inequality of the three means, and the
second inequality follows from Claim \ref{claim:lb_Yj_claim}.

Let  be  times the number of selected indicators.
An important property of  is that . Indeed, if

then , since there are only 
indicators, and if , then no more than 
indicators are selected; therefore,
.
From (\ref{eq:mean_Zh}), we have that

Hence, 
as desired.
 \end{dl_proof}

The next lemma follows immediately from Lemma \ref{lp_round1}
and (\ref{eq:lb_fV}).
Recall that  is an extension by expectation of , then
\begin{lemma}
.
\end{lemma}

It is easy to verify that  is a feasible fractional solution for the  instance. Recall that the element  is big if the
cost  of this element in some dimension  is larger than  times the budget in this dimension. We note that if
 is big then it holds that , and .
Let  be a random set such that . Also,
let  if  is -nearly feasible, and  otherwise.
 By Theorem \ref{thm:prb_claim}, we get that   is always
-nearly feasible, and


Given a nearly feasible fractional solution for the  instance, we
now show that it can be converted to a feasible one.
\begin{lemma}
\label{lemma:nearly_fix_mcmp}
Let  be an -nearly feasible solution for the  instance,
then  can be converted to a feasible solution  such that
.
\end{lemma}
\begin{proof}
Our conversion will be done in two steps.
First, let .
Clearly,  is feasible in the first  dimensions
(for any , );
also, .
Next, we note that for any , for any element
 it holds that . Thus, we can apply
in these dimensions the fixing procedure of Lemma \ref{lemma:nearly_fix}.
Hence, we can convert  to  as desired.
\end{proof}

We now summarize the steps of the algorithm.
\\
\noindent
{\bf Randomized Approximation algorithm for the Semi-fractional Problem}
\begin{enumerate}
\item For any subset  of size at most :
\begin{enumerate}
\item Solve , let  be the solution found.
\item Define , and let  be a random set ,
then   if
 is -nearly feasible, and  otherwise.
\item Convert  to a feasible
set  as in the proof of Lemma \ref{lemma:nearly_fix_mcmp}.
\end{enumerate}
\item
Let  be the solution of maximal profit found for the
 instance. Covert it to a solution for the
semi-fractional problem and return this solution.
\end{enumerate}

Clearly, the algorithm returns a feasible solution for the problem. Consider
the iteration in which  is the set of  most profitable elements in .
In this iteration,  .
Hence, by Lemma~\ref{lemma:nearly_fix_mcmp},
.
By a properly selecting the value of ,
we get the following.

\begin{theorem}
\label{thm:semi_frac}
There is a polynomial time randomized -approximation algorithm for
the semi-fractional problem, for any fixed .
\end{theorem}

The algorithm can be derandomized, using
the technique in Section \ref{mcmb:deter}.
Note that here, the extension by expectation 
can be deterministically evaluated in polynomial time,
since the number of non-zero entries in  is polynomial.
\section{The Budgeted Generalized Assignment Problem}
\label{sec:bgap}


In this section we develop an approximation algorithm for BGAP, and for its generalization, BSAP.
Recall that a BSAP instance consists of  items 
and  bins, such that bin  has a -dimensional capacity . Each item  has a
-dimensional size , a -dimensional cost vector , and
a profit  that is gained when  is assigned to bin .
Also, given is a -dimensional budget vector .

We say that a subset of items  is a \emph{feasible assignment} for bin  if
. We define the
 cost and profit of assigning  to bin  by
, and
, respectively.
A \emph{solution} for the problem is a tuple of  {\em disjoint} subsets of items ,
such that each set  is a feasible assignment for bin .
We define the cost of  by
,
and its profit by
.
We say that a solution  is \emph{feasible} if .
The goal is to find a feasible solution of maximal profit.


As before, we say that a solution  is -nearly feasible
if . An item  is \emph{small} if, for
any bin , it holds that ;
otherwise,  is \emph{big}.
Also, an assignment  of items to bin 
is \emph{small} if
. Our algorithm uses two special cases of
BSAP. The first is
\emph{small items BSAP}, in which all items are small; the second is
 \emph{small assignments BSAP}
in which, for any bin  and a feasible assignment , it holds that  is a small assignment
for bin .

\paragraph{Overview:}

Our algorithm proceeds in four  stages.
The first stage obtains an -nearly feasible solution with high profit
for small assignments instances of BSAP,
 by using Theorem \ref{thm:prb_claim}.
To do so, we use an interpretation of the classic SAP problem as a submodular optimization
problem and the technique of ~\cite{fgms06} to obtain
a distribution over its solution space. The small assignments property is used
 to show that the conditions of Theorem~\ref{thm:prb_claim} are
satisfied.

The second stage shows how enumeration can be used to reduce a small items instance of BSAP into a small assignments instance, so that the
algorithm of the first stage can be applied. The main idea is to guess the most profitable bins in some optimal solution and the approximate
cost of these bins in this solution. The algorithm uses this guess
 to eliminate all big assignments, by adding  linear
constraints for each bin. The result of this stage is a -nearly feasible solution of high profit.

The third stage handles small items instances. The fact that all items are small is essential for converting the nearly feasible solution of the
previous stage to a feasible one. The fourth and last stage gives a reduction from general instances to small items instances. This is done by
simple enumeration, as in Section \ref{sec:random_mcmb}.



\subsection{Small Assignments Instances of BSAP}
\label{sec:SABSAP}


We solve BSAP instances with small assignments by casting BSAP as an instance
 of generalized  (see Section \ref{sec:prb_claim}).\footnote{The submodular interpretations
of GAP and SAP are well-known (see, e.g., \cite{FV06} and \cite{fgms06}).}
 Let  be the set of feasible assignments for bin , and . Define  for any  by

where the maximum over an empty set is equal
to zero. Also, define  by 

It is easy to verify that  is a non-decreasing submodular function. We define the (-dimensional) cost of an element   by
, and as in Section~\ref{sec:randomized_mcmb}, the cost of a subset  is . Let  denote the collection of all subsets in which there is a single assignment to each bin. Formally,

We consider the following instance of generalized : Maximize  subject to the constraints  and .

We note that any set  can be converted to a solution  for  with  and . To do so, we
assign each item  to a bin  if there exists 
 such that  and . If no such bin  exists, we
do not assign  to any bin. Similarly, any solution  for
  can be converted to a set  such that  
and .

Now, we use a technique of~\cite{fgms06} to obtain a fractional solution
 for the generalized  instance. Consider the linear program
LP-BSAP over the variables , for all  and .
\begin{figure}
\begin{center}

\end{center}
\end{figure}
We note that the optimal solution for LP-BSAP is at
least , where  is an optimal solution for the BSAP
instance.
Since we have  linear constraints over the bins, where  is some constant,
a feasible solution of value  times the optimal can be found in polynomial time (see \cite{fgms06} for more details). Let  be
such a solution, then the value of the solution  is at least .

Let  be a random variable over  with . Define a random set ,
and let  be the distribution of . In~\cite{fgms06} it is shown that  is at least  times the value of the
solution ; thus, .
It is easy to verify that the conditions of  Theorem~\ref{thm:prb_claim} are satisfied for the distribution .
We define  if  is -nearly feasible, and  otherwise. Then, by Theorem~\ref{thm:prb_claim},  is always
-nearly feasible and . As before,  can be converted to a solution  for the BSAP
instance, such that , and . We summarize the above steps in the following.
\\
\noindent
{\bf Approximation Algorithm for Small Assignments BSAP Instances}
\begin{enumerate}
\item Find a -approximate solution  for LP-BSAP.
\item For any bin ,  select an assignment  with probability 
and define 
\item If  is -nearly feasible return  as the solution,
else return an empty assignment.
\end{enumerate}
\begin{lemma}
\label{thm:SABSAP} The above Approximation Algorithm for Small Assignments
 BSAP Instances outputs in polynomial time an -nearly feasible
solution with  expected profit at least , where  is an optimal solution.
\end{lemma}

\subsection{Reduction to Small Assignments BSAP Instances}

We now describe an algorithm which reduces a general BSAP instance
to a small assignments instance. We use this reduction to obtain an
augmentation algorithm for general instances, by applying Algorithm for Small Assignment Instances of
Section \ref{sec:SABSAP}.

Let  be an optimal solution for an instance of BSAP. We say that the profit of bin  (with respect to ) is
, and the cost of bin  is . The first step in our algorithm is to guess the set  of
 most profitable bins in the solution .
 We then guess the cost of any bin  in each dimension .
with
accuracy .
That is,
for each bin  we guess a -dimensional vector of integers
 such that, for any ,

As the number of values  can get is at most  , we can
go over all possible cost vectors in polynomial time, for some constant .

We use our guess of the set  and the vectors  to define a residual instance of BSAP. The ground set of elements is , and there are
 bins. We define the  budget of the residual instance, , to be , for .
For any , the feasible set of assignments for bin  is , and for any
 the feasible set is . In both cases, the set of
feasible assignments for bin  is defined by  linear constraints. The new cost of  when assigned to bin  is  if  and  if . The new profit of  when assigned to bin  is .

A crucial property of the residual instance is that it is a small assignments instance.
 For any bin
 and  it holds that . For any bin  and ,
by the definition of  it holds that .
The relation between the solutions of the residual and original
problems is stated in the next two lemmas.

\begin{lemma}
\label{thm:reduction_lemma1}
Let  be an -nearly feasible  solution for the residual problem with respect
to a set  (of size at most ) and a collection of vectors . Then  is a
-nearly feasible  solution for the original problem with .
\end{lemma}

\begin{proof}
Clearly,  is a solution for the original problem since, for every bin , it holds that . Also,
for any bin  and  it holds that . Hence, .
As for the cost, for any , we have

\end{proof}
\begin{lemma}
\label{thm:reduction_lemma2} Let  be the set of  most profitable bins in an optimal
 solution , and let  be the vector of cost
guesses for which \eqref{eqn:reduction_guess} holds. Then there is a feasible
 solution  for the residual  problem with respect to  and
 whose profit is .
\end{lemma}

\begin{proof}
Let  be an optimal solution for the original problem. We define a
solution  for the residual problem by  if   and
 otherwise. Clearly,  is a solution for the residual problem.
For any dimension ,
 This implies that
.  Now, for any  it holds that , and for any  it holds that . Thus, we have that

i.e.,  is a feasible solution for the residual problem.

Let  be the set of bins for which . Since
 \eqref{eqn:reduction_guess} holds for the vectors , clearly, for any , we have that , and thus .
 By (\ref{eq:cost_not_T}), the total cost of bins not in  is bounded by .
Hence, for all of them except for at most , it holds that , for
 . In other words, for
every , except at most ,
 it holds that  and.
It follows that , and that none of the bins in  is in .
 The profit of any bin that is not in  is
smaller than  (since  is the set of most profitable bins). Therefore,

It follows that

\end{proof}

We summarize with the following algorithm.\\

\noindent
{\bf Nearly Feasible Algorithm for BSAP}
\begin{enumerate}
\item Enumerate over all subsets ,  , and
cost vectors  for any .
\begin{enumerate}
\item Define a residual instance with respect to  and ,
 and run Algorithm for Small Assignments BSAP on the residual instance.
Consider the resulting solution as a solution for the original problem.
\end{enumerate}
\item Return the best solution found.
\end{enumerate}

\begin{theorem}
\label{thm:reduction_theorem}
Let  be an optimal solution for a BSAP instance. Then
the Nearly Feasible Algorithm for BSAP returns in polynomial time a -nearly
feasible solution ,
with expected profit at least  .
\end{theorem}
\begin{proof}
As the number of possibilities for  and the vectors 
is polynomial for fixed values of , and , and Algorithm for Small
Assignments BSAP is polynomial as well, we get that the Nearly Feasible Algorithm  for BSAP runs in polynomial time.

Since the algorithm for Small Assignments BSAP always returns an
 -nearly feasible solution
(for the residual problem),
by Lemma \ref{thm:reduction_lemma1}, we get that the solutions output
 by the algorithm
are always -nearly feasible for the original problem. Finally,
 in the iteration where the set  and the vectors  satisfy the
conditions of Lemma \ref{thm:reduction_lemma2}, the optimal solution for
 the residual problem has profit at least . Thus, by Lemma \ref{thm:SABSAP}, the
expected profit of the solution returned by Algorithm for Small Assignments BSAP is at least
.

As the expected profit of the solution returned  by the algorithm is at least the
expected profit in any iteration, this yields the statement of the theorem.
\end{proof}

\subsection{Small Items BSAP Instances}

We now consider inputs in which all items are small. For such inputs we can
 fix a nearly feasible solution. The proof of the next result is similar to the proof of
Lemma \ref{lemma:nearly_fix} (details omitted).


\begin{lemma}
\label{lemma:nearly_fix_bsap}
Let  be an -nearly feasible solution
for a small items instance of BSAP. Then  can be converted to a feasible
solution  such that .
\end{lemma}

Lemma \ref{lemma:nearly_fix_bsap} can be easily coupled with the Nearly Feasible Algorithm
to obtain an approximation algorithm for small items instances of BSAP, which always returns a feasible
solution.
\\
\\
\noindent
{\bf Algorithm for Small Items BSAP}
\begin{enumerate}
\item Run the Nearly Feasible Algorithm for BSAP. Let  be the resulting solution.
\item Convert  to a feasible solution  (as in
Lemma \ref{lemma:nearly_fix_bsap}) and return .
\end{enumerate}
The properties of the algorithm follow immediately from Lemmas \ref{thm:reduction_theorem}
 and \ref{lemma:nearly_fix_bsap}.

\begin{lemma}
Algorithm for Small Items BSAP returns a feasible solution for the
 problem with expected profit of at least , where
 is an optimal solution.
\end{lemma}

\subsection{General Inputs}
We now use Algorithm for Small Items BSAP to obtain -approximation for general inputs. Given an input for BSAP, let  be an optimal solution. We say that the profit of an element  in  is zero if it is not assigned to any bin, and
 if it is assigned to bin .

Let  be a feasible assignment of elements to bins. Given , we define a residual instance of the problem with
respect to . The new budget is , the new set of elements  is the collection of all elements in  which are not
assigned in  to any bin. Consider the assignment of   to bin .
 If  , then  and
; otherwise, set  and .
 The size remains . The new capacity of bin 
is .

Clearly, the residual instance with respect to  is
small items instance. It is easy to verify that,
 given a feasible assignment for the residual
problem with respect to  of profit , we can derive a feasible assignment for the original problem of profit . Now, consider
 to be the assignment of the  most profitable elements in . Then there is a solution for the residual problem of
value  (the proof is similar to the proof of Lemma
 \ref{thm:reduction_lemma2}). Thus, if we guessed  correctly,
Algorithm for Small Items BSAP can be used to obtain a solution with expected  profit at least  for the residual
problem, from which we can derive a solution for the original instance
of profit .

WE summarize with an algorithm for general inputs.
\\
\\
\noindent {\bf Approximation Algorithm for BSAP}
\begin{enumerate}
\item For any feasible assignment  of at most  elements:
\begin{enumerate}
\item Find a -approximate solution  for
the residual problem with respect to .
\item Derive from  a solution for the original problem
of profit .
\end{enumerate}
\item
Return  the solution of maximal profit found.
\end{enumerate}

\begin{theorem}
\label{thm:alg_bgap}
There is a polynomial time randomized -approximation algorithm for BSAP,
for any fixed .
\end{theorem}
}
\section{Discussion}

In this paper we established a strong relation between the
continuous relaxation of  and the discrete problem. This
relation is nearly optimal and suggests that future research
should focus on deriving better approximation ratios for the
continuous problem.

The question whether better rounding exists remains open; namely, is it
 possible to obtain an approximation algorithm for , given
an  approximation algorithm for the continuous problem?
And more specifically, is there a polynomial time approximation
for  with monotone objective function?

Finally, the running times of our algorithms are exponential in , thus rendering them impractical.
 Yet, the hardness results for -dimensional Knapsack (see, e.g., \cite{KPP04,MC84,KS10}),
    a special case of ,
  hint that significant improvements over these running times may be impossible.







\bibliographystyle{abbrv}
\bibliography{bgap}

\appendix

\section{Basic Properties of Submodular Functions}
\label{app:basic_props}
In this section we give some simple properties of submodular functions.
Recall that  is a submodular function
if  for any .
We define .
\begin{lemma}
\label{lemma:submodular_summation}
Let  be a submodular function with ,
and let , where  are disjoint sets.
Then 
\end{lemma}
\begin{proof}
By induction on . For , since  is a submodular function, we have that

and since , we get that .

For , using the induction hypothesis twice, we have

\end{proof}
\begin{lemma}
\label{lemma:submodular_decreasing}
Let  be a submodular function, and let 
such that  and . Then,
.
\end{lemma}
\begin{proof}
Since  is submodular,

Hence, .
\end{proof}
\begin{lemma}
\label{lemma:submodular_summation_inequality}
Let  be a submodular function, and let
,
where  are disjoint sets. Then,

\end{lemma}
\begin{proof}
We note that

By Lemma~\ref{lemma:submodular_decreasing}, for each ,
.
Hence,

\end{proof}

\comment{
\section{General Inputs for BSAP}
\label{sec:bgap_general}
In this section we use the algorithm for small items BSAP to obtain
-approximation for arbitrary inputs.
Given an input for BSAP, let  be an optimal
solution.


Let  be a feasible assignment of elements to bins.
We define a residual instance with respect to .
The new budget is ,
the new set of elements  is the collection of all elements in 
which were not assigned in  to any bin.
Consider the assignment of   to bin .
If  , then 
and . Otherwise, set  and
. The size remains . The new capacity of bin
 is .

The residual instance with respect to  is small items
instance. Any feasible assignment to the residual problem (with
respect to ) of profit  can be converted to a feasible
assignment to original problem of profit .
Now, consider   to be the assignment of the 
most profitable elements in an optimal solution 
(the profit of  in  is  if it was assigned to bin  and zero otherwise).
 Then there is a solution
to the residual problem of value 
(the proof is similar to the proof of Lemma \ref{thm:reduction_lemma2}).
Thus, if we guessed  correctly, the algorithm for small items BSAP can be used to
obtain a solution with expected  profit of at least  for the residual
problem, which we can covert to a solution for the original instance
of profit at least .
And we get the following algorithm.


\noindent
{\bf Approximation algorithm for BSAP}
\begin{enumerate}
\item For any feasible assignment  of at most  elements:
\begin{enumerate}
\item Find a -approximate solution  for
the residual problem with respect to .
\item Derive from  a solution for the original problem
of profit .
\end{enumerate}
\item
Return  the solution of maximal profit found.
\end{enumerate}

\begin{theorem}
There is a polynomial time randomized -approximation algorithm for BSAP, for any fixed .
\end{theorem}


\section{Some Proofs}
\label{app:proofs}


\begin{dl_claim_proof}{\ref{lemma:prob_F}}
For any dimension ,
it holds that .
Define . Then,

The first inequality holds since , and the second
inequality follows from the fact that  for
.
Recall that, by the Chebyshev-Cantelli inequality, for any  and a random
variable ,

Thus,

By the union bound, we have that

\end{dl_claim_proof}




\begin{dl_claim_proof}{\ref{lemma:prob_l}}
By the Chebyshev-Cantelli inequality we have that, for any dimension
,

and by the union bound, we get that

\end{dl_claim_proof}

\begin{dl_claim_proof}{\ref{lemma:bounding_via_R}}
The set  can be partitioned to  sets
 such that each of these
sets is a feasible solution. Hence, .
Thus by lemma~\ref{lemma:submodular_summation}, .
\end{dl_claim_proof}



\begin{dl_claim_proof}{\ref{lemma:mcmb_nearly_feasible}}
By Claims \ref{lemma:prob_F} and \ref{lemma:prob_l}, we have that

Since the last summation is a constant, and , we have that
 
where  is some constant. It follows that
 
Finally, since  if  and  otherwise, we have that

 \end{dl_claim_proof}



\begin{dl_proof}{\ref{lemma:derand1}}
Let  be the set of all fractional entries.
We define a new cost function  over the elements in .



Note that for any , , and

for all .
The number of different values  can get for 
is bounded by  (since all elements are
small, and ). Hence the number of different
values  can get for  is bounded by
.

We start with , and while there are  such that  and  are both
fractional and , define 
and .
Since  and  have the same cost (by ), it holds that
.
If ,
then set , otherwise .
In both cases  and . Now, repeat
the step with .
Since in each iteration the number of fractional entries in  decreases,
 the process will terminate (after at most  iterations)
with a vector  such that ,

and there are no two elements  with  where
 and  are both fractional. Also, for any , the entry
 is integral (since  was integral and the entry was not modified by
the process). Thus, the number of fractional entries in  is
at most .
Now, for any dimension ,

This completes the proof.
\end{dl_proof}

\begin{dl_proof}{\ref{thm:reduction_lemma1}}
Clearly,  is a solution for the original problem since for every bin  it holds that .
For any bin  and  it holds that , then clearly .
As for the cost, for any :

\end{dl_proof}

\begin{dl_proof}{\ref{thm:reduction_lemma2}}
Let  be an optimal solution for the original problem. We define a
solution  for the residual problem by  if   and
 otherwise. Clearly,  is a solution for the residual problem.
For any dimension ,

This implies that .  Now, for any  it holds that , and
for any  it holds that . Thus, we conclude that

i.e.,  is a feasible solution.

Let  be the set of bins for which .
Since  \eqref{eqn:reduction_guess} holds for the vectors ,
it is clear that for any , , and thus .
The total cost of bins not in  is bounded by  (since ).
Hence, for all of them except at most  it holds that ,
for some .
This means that for every  except at most 
 it holds that  and.
From this, we conclude that , and that none of the bins in  is
in . The profit of any bin which is not in  is smaller
than  (since  is the set of most profitable bins).
Thus,
.
Thus,

\end{dl_proof}

\begin{dl_proof}{\ref{thm:reduction_theorem}}
The properties of the algorithm for small assignments BSAP are
described in Lemma \ref{thm:SABSAP}. As the number of
possibilities for  and the vectors  is polynomial for
fixed values of , and , and the algorithm for
Small Assignments BSAP is polynomial as well, we get that the
Nearly Feasible Algorithm  for BSAP runs in polynomial time.

Since the algorithm for Small Assignments BSAP always returns an
 -nearly feasible solution
(for the residual problem),
by Lemma \ref{thm:reduction_lemma1}, we get that the solutions output
 by the algorithm
are always -nearly feasible for the original problem.
Finally, in the iteration for which the set  and vectors  satisfy the
requirements of Lemma \ref{thm:reduction_lemma2}, the optimal solution for the
residual problem is of profit at least .
Thus, the expected profit of the solution returned by the small assignments algorithm is
at least .

As the expected profit of the solution returned  by the algorithm is at least the
expected profit at any iteration, we get that the expected profit
of the solution returned by the algorithm is .
\end{dl_proof}
}
\end{document}
