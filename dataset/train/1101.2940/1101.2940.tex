\documentclass[11pt]{article}

\usepackage{setspace}
\usepackage{amssymb,amsmath}
\usepackage{url}
\usepackage{float}


\floatstyle{boxed}
\restylefloat{figure}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{observation}[theorem]{Observation}

 \newenvironment{sketch}{\noindent{\bf Proof Sketch:}}{
 \hspace*{\fill} $\Box$ \vskip \belowdisplayskip}


\newcommand{\eps}{\varepsilon}
\newcommand{\vp}{\varphi}
\newcommand{\cm}[1]{}
\newcommand{\comment}[1]{}
\newcommand{\ac}{{\cal A}_C}
\newcommand{\lset}{\left \{}
\newcommand{\rset}{\right \}}
\renewcommand{\baselinestretch}{1.125}
\newcommand{\ceil}[1]{\lceil #1 \rceil }
\newcommand{\alg}[1]{\mbox{\sf #1}}
\newcommand{\algsub}[1]{\mbox{\sf\scriptsize #1}}
\newcommand{\lone}[1]{ {\abs{#1}_1}}

\def \From{From }
\newcommand{\hbs}{{\hat{\bar{s}}}}
\newcommand{\bhs}{{\bar{\hat{s}}}}
\newcommand{\hs}{\hat{s}}
\newcommand{\hsir}{{\hat{s}}_{i,r}}
\newcommand{\bx}{\bar{x}}
\newcommand{\bX}{\bar{X}}
\newcommand{\by}{\bar{y}}
\newcommand{\bs}{\bar{s}}
\newcommand{\bb}{\bar{b}}
\newcommand{\bz}{\bar{z}}
\newcommand{\bk}{\bar{k}}
\newcommand{\bxf}{\bar{x}^{f}}
\newcommand{\xf}{x^{f}}
\newcommand{\mOf}{\mO^f}
\newcommand{\br}{B_r}
\newcommand{\byf}{\bar{y}^{f}}
\newcommand{\yf}{y^{f}}
\newcommand{\lr}{L_r}
\newcommand{\lrg}{L^g_r}
\newcommand{\brg}{B^g_r}
\newcommand{\tbr}{{\tilde B}_r}
\newcommand{\tlr}{{\tilde L}_r}
\newcommand{\he}{\hat \eps}
\newcommand{\zr}{Z_r}
\newcommand{\zro}{Z_{r,1}}
\newcommand{\zrt}{Z_{r,2}}

\newcommand{\bS}{S}
\newcommand{\bT}{T}
\newcommand{\bR}{R}
\newcommand{\bK}{K}
\newcommand{\cD}{{\cal D}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cA}{{\cal A}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cf}{{\cal f}}
\newcommand{\bL}{\bar{L}}
\newcommand{\bc}{\bar{c}}
\newcommand{\bw}{\bar{w}}
\newcommand{\bW}{\bar{W}}
\newcommand{\bB}{\bar{B}}
\newcommand{\ing}{\vdash}
\newcommand{\ning}{\nvdash}
\newcommand{\argmax}{\mbox{argmax}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\SUB}{\mbox{SUB}}
\newcommand{\MC}{\mbox{MC}}
\newcommand{\RSAP}{\mbox{RSAP}}
\newcommand{\PCS}{\mbox{PCS}}
\newcommand{\MPCS}{\mbox{PCS}}
\newcommand{\AMLC}{{\cA}_{{MLC}}}
\newcommand{\alphaf}{\alpha_\varphi}
\newcommand{\f}{\varphi}
\newcommand{\tmO}{\tilde{\mathcal{O}}}
\newcommand{\Prb}[1]{{Pr\left[ #1\right] }}
\newcommand{\abs}[1]{ \left| #1 \right |}
\setstretch{1.00}

\newcommand{\Tabs}{xx\= xx\= xx\= xx\= xxxx\= xxxx\= xxxx\= xxxx\=
xxxx\= xxxx\= xxxx\= xxxx\= \kill}
\newcommand{\program}[3]{\begin{figure}\begin{center}
             \fbox{
             \begin{minipage}{\textwidth}
             \begin{tabbing}
                         \Tabs
             #3
             \end{tabbing}
             \end{minipage}
             }
             \end{center}
             \caption{\label{#1} #2}
             \end{figure}}

\newenvironment{proof}{\noindent{\bf Proof:}}{
 \hspace*{\fill} $\Box$ \vskip \belowdisplayskip}

\newenvironment{dl_proof}[1]{\noindent{\bf Proof of Lemma #1:}}{
 \hspace*{\fill} $\Box$ \vskip \belowdisplayskip}
 \newenvironment{dl_claim_proof}[1]{\noindent{\bf Proof of Claim #1:}}{
 \hspace*{\fill} $\Box$ \vskip \belowdisplayskip}
\newenvironment{dl_thm_proof}[1]{\noindent{\bf Proof of Theorem #1:}}{
 \hspace*{\fill} $\Box$ \vskip \belowdisplayskip}

\newcommand{\qedsymb}{\hfill{\rule{2mm}{2mm}}}
 \def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}
\newcommand{\qed}{\hspace*{\fill}
\vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}\smallskip}
\def\eod{\vrule height 6pt width 5pt depth 0pt}
\newcommand{\A}{\mbox{$\cal A$}}

\newlength{\tablength}
\newlength{\spacelength}
\settowidth{\tablength}{\mbox{\ \ \ \ \ \ \ \ }}
\settowidth{\spacelength}{\mbox{\ }}
\newcommand{\tab}{\hspace{\tablength}}
\newcommand{\tabstar}{\hspace*{\tablength}}
\newcommand{\spacestar}{\hspace*{\spacelength}}
\def\obeytabs{\catcode`\^^I=\active}
{\obeytabs\global\let^^I=\tabstar}
{\obeyspaces\global\let =\spacestar}
\newenvironment{display}{\begingroup\obeylines\obeyspaces\obeytabs}{\endgroup}
\newenvironment{prog}{\begin{display}\parskip0pt\sf}{\end{display}}
\newenvironment{pseudocode}{\smallskip\begin{quote}\begin{prog}}{\end{prog}\end
{quote}\smallskip}

\newcommand{\negA}{\vspace{-0.07in}}  \newcommand{\negB}{\vspace{-0.12in}}  \newcommand{\negC}{\vspace{-0.15in}}  \newcommand{\posA}{\vspace{0.08in}}





\setlength{\textwidth}{6.2in}
\setlength{\textheight}{8.9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{10pt}
\setlength{\topsep}{0in}
\setlength{\topmargin}{0.3in} \setlength{\itemsep}{0in}
\setlength{\footskip}{0.4in} 

\begin{document}
\title{\Large
Approximations for Monotone and Non-monotone Submodular Maximization
with
Knapsack Constraints\footnote{A preliminary version of this paper appeared in the Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete
Algorithms, New York, January 2009.}}
\author{
Ariel Kulik\thanks{Computer Science Department, Technion, Haifa 32000,
Israel. \mbox{E-mail: {\tt ariel.kulik@gmail.com}}} \and Hadas
Shachnai\thanks{Computer Science Department, Technion, Haifa 32000,
Israel. \mbox{E-mail: {\tt hadas@cs.technion.ac.il}}. Work partially supported
by the Technion V.P.R. Fund, by Smoler Research Fund, and by the Ministry of Trade and Industry MAGNET
program through the NEGEV
Consortium (www.negev-initiative.org). }
\and Tami Tamir
\thanks{ School of Computer Science, The
Interdisciplinary Center, Herzliya, Israel.
\mbox{E-mail: {\tt tami@idc.ac.il}}}
}
\date{}

\maketitle


\begin{abstract}

Submodular maximization generalizes many fundamental problems in
discrete optimization, including Max-Cut in directed/undirected
graphs, maximum coverage, maximum facility location and marketing
over social networks.

In this paper we consider the problem of maximizing any submodular
function subject to $d$ knapsack constraints, where $d$ is a fixed
constant. We establish a strong relation between the discrete
problem and its continuous relaxation, obtained through {\em
extension by expectation} of the submodular function. Formally, we
show that, for any non-negative submodular function, an
$\alpha$-approximation algorithm for the continuous relaxation
implies a randomized $(\alpha - \eps)$-approximation algorithm for
the discrete problem. We use this relation to improve the best
known approximation ratio for the problem to $1/4- \eps$, for any
$\eps > 0$, and to obtain a nearly optimal
$(1-e^{-1}-\eps)-$approximation ratio for the monotone case, for
any $\eps>0$.
We further show that the probabilistic domain defined by a
continuous solution can be reduced to yield a polynomial size
domain, given an oracle for the extension by expectation. This
leads to a deterministic version of our technique.
\end{abstract}
\bigskip



\section{Introduction} \label{sec:intro}


 A real-valued function $f$, whose domain is all the
 subsets of a universe $U$, is called {\em submodular} if,
 for any $S,T \subseteq U$,
\[
 f(S) + f(T) \geq f(S \cup T) + f(S \cap T).
 \]
The concept of submodularity, which can be viewed as a discrete analog of convexity,
plays a central role in combinatorial theorems and algorithms
(see, e.g., \cite{f99}
and the references therein, and the comprehensive
surveys in \cite{FMV07,Vo08,LMNS09}).
Submodular maximization generalizes many fundamental problems in
discrete optimization, including Max-Cut in directed/undirected
graphs, maximum coverage, maximum facility location and marketing
over social networks (see, e.g., \cite{HMS08}).

In many settings, including set covering or matroid optimization, the
underlying submodular functions are monotone, meaning that $f(S)
\leq f(T)$ whenever $S \subseteq T$. In other settings, the function $f(S)$
is not necessarily monotone.
A classic
example of such a submodular function is $f(S)= \sum_{e \in \delta(S)} w(e)$,
where $\delta(S)$ is a cut in a graph (or hypergraph) $G=(V,E)$
induced by a set of vertices $S \subseteq V$, and $w(e)$ is the weight of an edge $e \subseteq E$.
An example for a monotone submodular function is $f_{G,\bar{p}}:2^L \rightarrow \mathbb{R}$, defined on a subset of vertices in bipartite graph
$G=(L,R,E)$. For any $S \subseteq V$, $f_{G,\bar{p}}(S)= \sum_{v \in N(S)} p_v$, where $N(S)$ is the neighborhood function (i.e., $N(S)$ is the
set of neighbors of $S$), and $p_v \geq 0$ is the profit of $v$, for any $v \in R$.
The problem
$\max\{f_{G,\bar{p}}(S) | \abs{S} \leq k\}$ is classical maximum coverage.


In this paper we consider the following
problem of maximizing a non-negative {\em submodular}
set function subject to $d$
{\em knapsack constraints} ({$\SUB$). Given a $d$-dimensional
budget vector $\bL$, for some $d \geq 1$, and an oracle for a
non-negative submodular set function $f$ over a universe $U$,
where each element $i \in U$ is associated with a $d$-dimensional
cost vector $\bc(i)$, we seek a subset of elements $S \subseteq U$
whose total cost is at most $\bL$, such that $f(S)$ is maximized.

There has been extensive work on maximizing submodular {\em monotone}
functions subject to matroid constraint.\footnote{A (weighted)
matroid is a system of `independent subsets' of a universe, which
satisfies certain {\em hereditary} and {\em exchange} properties
\cite{Sch03}.} For the special case of uniform matroid,
i.e., the problem
$\{ \max f(S): |S| \leq k \}$, for some $k >1$, Nemhauser et. al
showed in \cite{NWF78}
that a greedy algorithm yields a ratio of $1-e^{-1}$ to the
optimum. Later works presented greedy algorithms that achieve this
ratio for other special matroids or for
variants of maximum coverage (see, e.g., \cite{as04,
kmn99,s04,CK04}).
For a general matroid constraint, Calinescu et al. showed in
\cite{ccpv07} that a scheme based on solving a continuous
relaxation of the problem followed by {\em pipage rounding} (a
technique introduced by Ageev and Sviridenko \cite{as04}) achieves
the ratio of $1-e^{-1}$ for maximizing submodular monotone
functions that can be expressed as a sum of weighted rank
functions of matroids. Subsequently, this result was extended by
Vondr\'{a}k \cite{Vo08} to general monotone submodular functions.

 The bound of $1 - e^{-1}$ is the best possible for all of the
above problems. This follows from the lower bound of Nemhauser and
Wolsey \cite{nw78} in the oracle model, and the later result of
Feige \cite{f98} for the specific case of maximum coverage, under
the assumption that $P \neq NP$.

Other variants of monotone submodular optimization were also considered.
In~\cite{BKNS10}, Bansal et al. studied the problem of maximizing a monotone submodular function
subject to $n$ knapsack constraints, for arbitrary $n \geq 1$, where each element appears in up to $k$
constraints, and $k$ is fixed. The paper presents a $\frac{8ek}{e-1}$ and
$\frac{e^2k}{e-1} +o(k)$ approximations for
this problem. Demaine and Zadimoghaddam \cite{DZ10} studied
bi-criteria approximations for monotone submodular set function
optimization.

The problem of maximizing a {\em non-monotone} submodular function has been studied as well. Feige et al. \cite{FMV07}
 considered (unconstrained) maximization of a  general non-monotone submodular function.
 The paper gives several (randomized and deterministic)
approximation algorithms, as well as hardness results, also for the special case
where the function is {\em symmetric}.

Lee et al. \cite{LMNS09} studied the problem of
maximizing a general submodular function under linear and matroid
constraints. They proposed algorithms that achieve approximation
ratio of $1/5 - \eps$ for  the problem with $d$ linear constraints
and a ratio of $1/(d+2+ 1/d + \eps)$ for $d$ matroid constraints,
for any fixed integer $d \geq 1$.

Improved lower and upper bounds for non-constrained and constrained submodular maximization
were recently derived by Gharan and Vondr\'{a}k
\cite{SV10}. However, this paper does not consider knapsack constraints.


\comment{ Submodular maximization problem where the focus of many
works. The best approximation ratio for maximizing non-monotone
submodular function without any constraints is $2/5-o(1)$ general
functions, and $1/2-\eps$ for symmetric function, both algorithm
are due to \cite{FMV07}. { {\bf write a survey of results for
constraints maximization}. For monotone functions mention the
classic greedy algorithm for cardinality and knapsack constraint.
Later matroid constraints, and afterwards our results. For
non-monotone mention Sviridenko. } }

Several fundamental algorithms for submodular maximization
(see, e.g., \cite{as04,ccpv07,Vo08,LMNS09})
use a continuous extension of submodular function, to which we
refer as \emph{extension by expectation}. Given a submodular
function $f: 2^U \rightarrow \mathbb{R}$, we define $F: [0,1]^U
\rightarrow \mathbb{R}$.
For any $\bar{y} \in [0,1]^U$, let $R \subseteq U$  be a random
variable such that $i\in R$ with probability $y_i$ (we say that
$R\sim \bar{y}$). Then
\begin{equation*}
F(\bar{y})= E[f(R)] =
\sum_{R \subseteq U} \left( f(R) \prod_{i\in R} y_i \prod_{i \notin
R} (1-y_i) \right).
\end{equation*}
The general framework of these algorithms is to obtain first a
fractional solution for the continuous extension, followed by
rounding which yields a solution for the discrete problem.

Using the definition of $F$, we define the continuous relaxation of our problem called
{\em continuous \SUB}. Let $P=\{\bar{y} \in [0,1]^U |
\sum_{i\in U} y_i{\bc}(i) \leq \bL\}$ be the polytope of the instance, then the problem
is to find $\bar{y}\in P$ for which $F(\bar{y})$ is
maximized. For $\alpha \in (0,1]$, an algorithm $\cA$ yields $\alpha$-approximation
 for the continuous problem with respect to a submodular
function $f$, if for any assignment of non-negative costs to the elements, and for any non-negative budget, $\cA$ finds a feasible solution for
continuous ${\SUB}$ of value at least $\alpha \mO$, where $\mO$ is the value of an optimal
(integral) solution for ${\SUB}$ with the given costs and budget.


For some specific families of submodular functions,
linear programming can be used to derive such approximation algorithms
(see e.g  \cite{as04,ccpv07}). For monotone submodular functions,
Vondr\'{a}k presented in \cite{Vo08} a
$(1-e^{-1}-o(1))$-approximation algorithm for the continuous
problem. Subsequently,
Lee et al. \cite{LMNS09} considered the problem of maximizing {\em any}
submodular function with multiple knapsack constraints and developed
a $(\frac{1}{4}-o(1))$-approximation algorithm for the continuous
problem; however, noting that the rounding method of \cite{KST09},\footnote{
The paper \cite{KST09} is a preliminary version of this paper.}
which proved useful for monotone functions, cannot be applied in the non-monotone case,
a $(\frac{1}{5}-\eps)$-approximation was obtained for the discrete
problem, by using simple randomized rounding.
This gap of approximation ratio between the continuous and the
discrete case led us to further develop the technique in \cite{KST09},
so that it can be applied also for non-monotone functions.


\subsection{Our Results}
In this paper
we establish a strong relation
between the problem of maximizing any submodular function subject
to $d$ knapsack constraints and its continuous relaxation.
Formally, we show (in Theorem~\ref{thm:continuous_eq}) that for
any non-negative submodular function, an $\alpha$-approximation
algorithm for the continuous relaxation implies a randomized
$(\alpha - \eps)$-approximation algorithm for the discrete
problem. We use this relation to
obtain approximation ratio of $1/4- \eps$ for {\SUB}, for any
$\eps > 0$, thus improving the best known result for the problem,
due to Lee et al. \cite{LMNS09}. For the case where the objective
function is monotone, we use this relation to obtain a nearly
optimal $(1-e^{-1}-\eps)$ approximation, for any $\eps>0$. An
important consequence of the above relation is that for any
class of submodular functions,
a future improvement of the approximation ratio for the continuous
problem, to a factor of $\alpha$,
immediately implies an approximation ratio of $(\alpha -\eps)$ for
the original instance.

Our technique applies random sampling on the solution space, using
a distribution defined by the fractional solution for the problem.
In Section \ref{mcmb:deter} we show how to convert a feasible
solution for the continuous problem to another feasible solution
with up to $O(\log |U|)$ fractional entries, given an oracle to
the extension by expectation. This facilitates the usage of
exhaustive search instead of sampling, which leads to a
deterministic version of our technique. Specifically, we obtain a
deterministic $(1/4 -\eps)$-approximation for general instances
and $(1- e^{-1} -\eps)$-approximation for instances where the
submodular function is monotone.
For the special case of maximum
coverage with $d$ knapsack constraints,
that is, $\SUB$ where the objective
function is $f=f_{G,\bar{p}}$  for a given bipartite graph $G$ and
profits $\bar{p}$, this result leads to a deterministic $(1-
e^{-1} -\eps)-$approximation algorithm, since the extension by
expectation of $f_{G,\bar{p}}$ can be deterministically evaluated.
Some basic properties of submodular functions are given in
Appendix \ref{app:basic_props}.

\subsection{Recent Developments}

Subsequent to our study of maximizing monotone submodular
functions subject to multiple knapsack constraints \cite{KST09},
Chekuri et al. \cite{CVZ10}
showed that, by using a more
sophisticated rounding technique,
the algorithm in
\cite{KST09} can be applied to derive a
$(1-e^{-1}-\eps)$-approximation for
maximizing a submodular function subject to $d$ knapsack
constraints and a matroid constraint.
Specifically, given a fractional solution for the problem, the
authors
define a probability distribution over the solution space, such
that all of elements in the domain of the distribution are inside
the matroid; these elements also satisfy Chernoff-type
concentration bounds, which can be used to prove some of the
probabilistic claims in \cite{KST09}.
The desired approximation ratio is obtained by using the algorithm
of \cite{KST09} with sampling replaced by the above distribution
in the rounding step.
Recently, the same set of authors improved in \cite{CVZ10a} the
bound of $(1/4- \eps)$ presented here to $0.325$.


\comment{
 In~\cite{CVZ10} Chekuri, Vondr\'{a}k and Zenklusen
presented a $(1-e^{-1}-\eps)$-approximation algorithm for monotone
maximization subject to $d$ knapsack constraints and a matroid
constraint. Their main tool was to provide a distribution (given a
fractional solution), such that every element in the
distribution's domain is inside the matroid, and also maintains a
Chernoff-type concentration bounds which can be used to prove our
probabilistic claims. By applying this distribution over our
algorithm (while referring to \cite{KST09}), instead of a
sampling, the desired approximation ratio was attained. }


\section{Maximizing Submodular Functions}


\label{sec:randomized_mcmb}

In this section we describe our framework for maximizing a
submodular set function subject to multiple linear constraints.
For short, we call this problem $\SUB$.

\subsection{Preliminaries}
\label{sec:prel}

\paragraph{Notation:}
An essential component in our framework is the distinction between
elements by their costs. We say that an element $i$
is \emph{small} if $\bc (i) \leq \eps^3 \bL$; otherwise,
the element is \emph{big}.




Given a universe $U$, we call a subset of elements $S\subseteq U$
{\em feasible} if the total cost of elements in $S$ is bounded by
$\bL$. We say that  $S$ is \emph{$\eps$-nearly feasible} (or
\emph{nearly feasible}, if $\eps$ is known from the context) if the total
cost of the elements in $S$ is bounded by $(1+\eps) \bL$. We refer
to $f(S)$ as the value of $S$. Similar to the discrete case,
$\bar{y}\in [0,1]^U$ is feasible if $\bar{y} \in P$.

For any subset $T \subseteq U$, we define $f_T: 2^{U} \rightarrow
\mathbb{R}_{+}$ by $f_T(S) = f(S \cup T) - f(T)$. It is easy to
verify that if $f$ is a submodular set function then $f_T$ is also
a submodular set function. Finally, for any set $S \subseteq U$,
we define $c_r(S)=\sum_{i \in S} c_r(i)$, where $1 \leq r \leq d$, and
 $\bc(S)= \sum_{i \in S} \bc(i)$.  For a fractional solution $\by \in [0,1]^U$, we
define $c_r(\bar{y}) = \sum_{i\in U} c_r(i) \cdot y_i$ and
$\bc(\bar{y})= \sum_{i\in U} \bc(i)\cdot y_i$.


\paragraph{Overview:}
Our algorithm consists of two phases, to which we refer as
{\em rounding procedure} and {\em profit enumeration}. The rounding procedure
yields an $(\alpha-O(\eps))$-approximation for
instances in which there are no big elements, using an
$\alpha$-approximate solution for the continuous problem. It relies heavily
on Theorem \ref{thm:prb_claim} that gives some conditions on the
probabilistic domain of solutions; these conditions guarantee that the
expected profit of the resulting nearly feasible solution is high. This
solution is then converted to a feasible one, by using a
fixing procedure. We first present a randomized version
and later show
how to derandomize the rounding procedure.


The profit enumeration phase uses enumeration over the most
profitable elements in an optimal solution; then it reduces
a general instance to another instance with no big elements,
on which we apply the rounding procedure.

Finally, we combine the above results with an algorithm for the
continuous problem (e.g., the algorithm of \cite{Vo08}, or
\cite{LMNS09}) to obtain approximation algorithm for $\SUB$.

\subsection{A Probabilistic Theorem}
\label{sec:prb_claim}

We first prove a general probabilistic
theorem which refers to a slight generalization of our problem
(called {\em generalized $\SUB$}).
In addition to the standard
input for the problem, there is also a collection of subsets $\mM\subseteq 2^U$,
such that if $T\in \mM$ and $S \subseteq T$
then $S\in \mM$. The goal is  to find
a subset $S \subseteq \mM$, such that  $\bc(S) \leq \bL$
and $f(S)$ is maximized.

\begin{theorem}
\label{thm:prb_claim}
For a given input of generalized $\SUB$,
let $\chi$ be a distribution over $\mM$
and $D$ a random variable $D \sim \chi$, such that
\begin{enumerate}
\item $E \left[ f(D) \right] \geq \mO/5 $, where $\mO$ is an optimal
solution for the given instance.
\item For any $1\leq r \leq d$, $E[c_r(D) ]  \leq L_r$
\item For any $1 \leq r \leq d$, $c_r(D)= \sum_{k=1}^{m} c_r(D_k)$, where
 $D_k \sim \chi_k$ and $D_1, \ldots , D_m$ are independent random
variables.
\item
\label{thm:prb_claim:c4}
For any $1\leq k  \leq m$ and $1\leq r \leq d$, it holds that
either $c_r(D_k) \leq \eps ^3 L_r$ or $c_r(D_k)$ is fixed.
\end{enumerate}
Let $D'=D$ if $D$ is $\eps$-nearly feasible, and $D'=\emptyset$ otherwise. Then $D'$ is
always $\eps$-nearly feasible, $D' \in \mM$, and
$E[f(D')] \geq (1-O(\eps) ) E[f(D)]$.
\end{theorem}

To prove the results in this section, it suffices to use a special case of Theorem
 \ref{thm:prb_claim} (formulated as our next result). We use
this theorem in its full generality in \cite{full}, in developing approximation
 algorithms for variants of maximum coverage and GAP.

\begin{lemma}
\label{lemma:expected_random}
Let $\bx \in [0,1]^U$ be a feasible fractional solution
such that
$F(\bx) \geq \mO /5$, where $\mO$ is the optimal solution for generalized ${\SUB}$.
 Let $D \subseteq U$ be a random set such
that $D \sim \bx$ (i.e., for all $i \in U$, $i \in R$ with probability $x_i$),
 and
let $D'$ be a random set such that $D'=D$ if $D$ is $ \eps$-nearly feasible, and $D'=\emptyset$ otherwise.
Then $D'$ is always $\eps$-nearly feasible, and
$E[f(D')] \geq (1-O(\eps) ) F(\bx)$.
\end{lemma}

\begin{dl_thm_proof}{\ref{thm:prb_claim}}
Define an indicator random variable $F$ such that $F=1$ if $D$ is $\eps$-nearly
feasible, and $F=0$ otherwise.

\begin{claim}
\label{lemma:prob_F}
$Pr[F=0] \leq d \eps$.
\end{claim}
\begin{proof}
For any dimension $ 1 \leq r \leq d$,
it holds that $E[ c_r(D) ] = \sum_{k=1}^{m} E[c_r(D_k)] \leq L_r$.
Define $V_r=\{k | c_r(D_k) \mbox{~is not fixed} \}$. Then,
\[
\begin{array}{ll}
Var[c_r(D)] & = \displaystyle{\sum_{k=1}^{m} Var[c_r(D_k)] \leq \sum_{k\in V_r} E[c_r^2(D_k)]}
\\  & \leq \displaystyle{
\sum_{k\in V_r} E [c_r(D_k)] \cdot \eps^3 L_r \leq  \eps^3 L_r \sum_{k=1}^{m} E [c_r(D_k)] \leq
\eps^3 L^2_r}.
\end{array}
\]
The first inequality holds since $Var[X] \leq E[X^2]$, and the second
inequality follows from the fact that $c_r(D_k) \leq \eps^3 L_r$ for
$k\in V_r$.
Recall that, by the Chebyshev-Cantelli inequality, for any $t > 0$ and a random
variable $Z$,
$$Pr\left[Z- E[Z] \geq t\right] \leq \frac{Var[Z]}{Var[Z]+ t^2}.$$
Thus,
\begin{eqnarray*}
Pr\left[c_r(D) \geq (1+\eps) L_r\right] &=&
Pr\left[ c_r(D) - E[c_r(D)] \geq (1+\eps) L_r -E[c_r(D)] \right]  \\
&\leq& Pr \left[ c_r(D) - E[c_r(D)] \geq \eps \cdot  L_r \right]
\leq \frac{ \eps^3 L^2_r}{\eps^2 L^2_r} = \eps.
\end{eqnarray*}
By the union bound, we have that
$$ Pr[F=0] \leq \sum_{r=1}^{d} Pr[c_r(D) \geq (1+\eps) L_r] \leq
d \eps.$$
\end{proof}

For any dimension $1 \leq r \leq d$, let $R_r = \frac{c_r(D)}{\lr}$,
and define $R= \max_r R_r$, then
$R$ denotes the maximal relative deviation of the cost from the $r$-th entry
in the budget vector, where the maximum is taken over $1 \leq r \leq d$.

\begin{claim}
\label{lemma:prob_l} For any $\ell> 1$,
$$Pr[R > \ell] <\frac{d\eps^3}{(\ell-1)^2}.$$
\end{claim}
\begin{proof}
By the Chebyshev-Cantelli inequality we have that, for any dimension
$1 \leq r \leq d$,
\begin{eqnarray*}
Pr[R_r > \ell ] &=&
Pr[c_r(D) > \ell \cdot \lr]
 \\
&\leq&
Pr \left[ c_r(D) -E[c_r(D)]  > (\ell-1) \lr \right] \leq \\
&\leq &
\frac{\eps^3 \lr^2}{(\ell-1)^2 \lr^2} \leq
\frac{\eps^3}{(\ell-1)^2},
\end{eqnarray*}
and by the union bound, we get that
$$Pr[R > \ell] \leq \frac{d\eps^3}{(\ell-1)^2}.$$
\end{proof}


\begin{claim}
\label{lemma:bounding_via_R}
For any integer $\ell >1$, if $R \leq \ell$ then
$$f(D) \leq 2d \ell \cdot \mO.$$
\end{claim}

\begin{proof}
The set $D$ can be partitioned to $2 d \ell$ sets
$D_1, \ldots D_{2 d \ell}$ such that each of these
sets is a feasible solution. Hence, $f(D_i) \leq \mO$.
By Lemma~\ref{lemma:submodular_summation}, we have that $f(D) \leq f(D_1) +\ldots +f(D_{2 d \ell})
\leq 2 d \ell \mO$.
\end{proof}
Combining the above results we have

\begin{claim}
\label{lemma:mcmb_nearly_feasible}
$E[f(D')]\geq (1-O(\eps))
E[f(D)]$.
\end{claim}

\begin{proof}
By Claims \ref{lemma:prob_F} and \ref{lemma:prob_l}, we have that
\begin{eqnarray*}
E[f(D)]    &=&
E\left[f(D) | ~ F=1\right] \cdot \Prb{F=1} +
 E\left[f(D) |~ F=0 \land (R < 2) \right]\cdot \Prb{F=0 \land (R   < 2)} \\
   &+& \sum_{\ell \geq 1}  E\left[f(D)|~  F=0 \land (2 ^\ell \leq R <  2^{\ell+1})\right]
        \cdot
         \Prb{F=0 \land (2 ^\ell \leq R < 2^{\ell+1})} \\
 &\leq& E[f(D) |~ F=1] \cdot \Prb{F=1} + 4d^2 \eps \cdot \mO
+ ~d^2 \eps ^3 \cdot \mO \cdot \sum_{\ell \geq 1}  \frac{2^{\ell+2}}
      { (2^{\ell-1})^2 }. \\
 \end{eqnarray*}
Since the last summation is a constant, and $E[f(D)]\geq \mO/2$, we have that
 $$E[F(D)] \leq E[f(D) | F=1 ] \Prb{F=1} + \eps \cdot c \cdot E[F(D)],$$
where $c>0$ is some constant. It follows that
 $$(1-O(\eps)) E[f(D)] \leq E[f(D) | F=1 ] \cdot \Prb{F=1}.$$
Finally, since $D' = D$ if $F=1$ and $D' =0$ otherwise, we have that
$$E[f(D')] = E[f(D)|F=1] \cdot \Prb{F=1} \geq (1- O(\eps)) E[f(D)]. $$
 \end{proof}
By definition, $D'$ is always $\eps$-nearly feasible, and $D'\in \mM$. This
completes the proof of the theorem.
\end{dl_thm_proof}


\subsection{Rounding Instances with No Big Elements}
\label{sec:random_mcmb}
In this section we present an $(\alpha - O(\eps))$-approximation
algorithm for ${\SUB}$ inputs with no big elements, given an
 $\alpha$-approximate solution for the continuous problem.
 Inputs with no big elements are easier to tackle. Indeed, any nearly feasible solution
 for such input can be converted to a feasible one, with only
 a small harm to the total value.


\begin{lemma}
\label{lemma:nearly_fix}
Let $S\subseteq U$ be an $\eps$-nearly feasible solution
with no big elements,
then $S$ can
be converted in polynomial time to a feasible solution $S' \subseteq S$,
such that $f(S') \geq \left(1-O(\eps) \right) f(S)$.
\end{lemma}

\begin{proof}
In fixing the solution $S$ we handle each dimension separately.
For any dimension $ 1\leq r \leq d$, if $c_r(S)\leq L_r$  then
no modification is needed; otherwise, $c_r(S) > L_r$. Since all elements
in $S$ are small, we can partition $S$ into $\ell$ disjoint subsets $S_1,S_2,\ldots,S_\ell$
such that $\eps L_r \leq c_r(S_j) < (\eps +\eps^3) L_r$ for any
 $1\leq j \leq \ell$, where $\ell = \Omega( \eps ^{-1})$.
Since the function $f$ is submodular, by Lemma \ref{lemma:submodular_summation_inequality},
 we have that
$ f(S) \geq \sum_{j=1}^{\ell} f_{S \setminus S_j} (S_j)$. Hence, there exists a value $j \in \{ 1, 2 \ldots , \ell \}$ such that $f_{S \setminus
S_j} (S_j) \leq \frac{f(S)}{\ell}= f(S) \cdot O(\eps)$ (note
that $f_{S \setminus S_j} (S_j)$ may be negative). Now, $c_r(S \setminus S_j) \leq
L_r$, and $f(S \setminus S_j) \geq (1-O(\eps)) f(S)$. We repeat this step for all $1 \leq r \leq d$ to obtain a feasible set $S'$ satisfying
$f(S') \geq (1-O(\eps)) f(S)$.
\end{proof}
Combined with Theorem \ref{thm:prb_claim},
we have the following rounding algorithm. \\ \\
\noindent
{\bf Randomized Rounding Algorithm for $\SUB$ with No Big Elements} \\
\noindent
{\bf Input:} A $\SUB$ instance, a feasible solution  $\bar{x}$ for the continuous problem, with
$F(\bar{x}) \geq \mO /5$.
\begin{enumerate}
\item Define a random set $D \sim \bx$. Let $D'=D$ if
$D$ is $\eps$-nearly feasible, and $D'=\emptyset$ otherwise.
\item Convert $D'$ to a feasible
set $D''$ as in the proof of Lemma \ref{lemma:nearly_fix} and return $D''$.
\end{enumerate}

Clearly, the algorithm returns a feasible solution for the problem.
By Theorem \ref{thm:prb_claim}, $E[f(D')] \geq (1- O(\eps)) F(\bar{x})$. By
Lemma~\ref{lemma:nearly_fix},
$E[f(D'')] \geq (1- O(\eps)) F(\bar{x})$. Hence, we have

\begin{lemma}
\label{thm:nobig_alg}
For any instance of $\SUB$ with no big elements,
any feasible solution $\bar{x}$ for the continuous problem with $F(\bar{x}) \geq \mO /5$
can be converted to a feasible solution for $\SUB$ in polynomial running time
with expected profit at least $(1-O(\eps))\cdot F(\bar{x})$.
\end{lemma}


\subsection{A Randomized Approximation Algorithm}
\label{sec:mcmb}


Given an instance of $\SUB$ and a subset $T \subseteq U$,
define another instance of $\SUB$, to which we refer as the {\em residual
problem with respect to} $T$, with $f$ remaining the objective function.
The budget for the residual problem is
${\bL}'={\bL}-{\bc}(T)$,
and the universe $U'$ consists of all elements $i \in U \setminus T$
such that $\bc(i) \leq \eps^3 \bL'$,
and all elements in $T$.
Formally,
$$U' = T \cup \left\{ i\in U\setminus T |~ \bc(i) \leq \eps^3 \bL'\right\}.
$$
The new cost of element $i$ is $c'(i)=c(i)$ for any $i\in U' \setminus T$, and
$c'(i) =0 $ for any $i\in T$. It follows that there
are no big elements in the residual problem.
Let $S$ be a feasible solution for the residual problem with respect
to $T$. Then $\bc(S) \leq \bc'(S) +\bc(T) \leq \bL' +\bc(T) =\bL$. Thus,
any feasible solution for the residual problem is also feasible for
the original instance.

Consider the following algorithm.
\\ \\
\noindent
{\bf A Randomized Approximation Algorithm for $\SUB$}\\
\noindent
{\bf Input:} A $\SUB$ instance and an $\alpha$-approximation algorithm
 $\cA$ for continuous $\SUB$ with respect to
the function $f$.
\begin{enumerate}
\item For any $T \subseteq U$ such that $|T| \leq h=\ceil{d \cdot \eps^{-4}}$
\begin{enumerate}
\item
Use $\cA$ to obtain an $\alpha$-approximate solution $\bx$ for the
continuous residual problem with respect to $T$.
\item Use the Randomized Rounding Algorithm of Section~\ref{sec:random_mcmb}
to convert $\bx$ to a feasible solution $S$ for
the residual problem.
\end{enumerate}
\item Return the best solution found.
\end{enumerate}

\begin{lemma}
The above approximation algorithm returns an
$(\alpha -O(\eps))$-approximate solution for $\SUB$
and uses a polynomial number of calls to algorithm $\cA$.
\end{lemma}

\begin{proof}
By Lemma~\ref{thm:nobig_alg},
in each iteration the algorithm finds a feasible solution $S$ for the residual problem.
Hence, the algorithm always
returns a feasible solution for the given $\SUB$ instance.

Let $\mO=\{ i_1,\ldots ,i_k \}$  be an
optimal solution for the input $I$ (we use $\mO$ to denote both an
optimal sub-collection of elements and the optimal value).
For $\ell \geq 1$, let
${\bK}_\ell = \{ i_1,\ldots,i_\ell \}$,
and assume that the elements  are ordered by their residual profits, i.e.,
$i_\ell = \argmax_{i\in \mO \setminus  \bK_{\ell-1}}
f_{\bK_{\ell-1}} (\{i\})$.

Consider the iteration in which $T=\bK_h$,
and define  $\mO'= \mO \cap U'$. The set $\mO'$ is clearly a feasible solution
for the residual problem with respect to $T$.
We show a lower bound for $f(\mO')$.
The set $R=\mO \setminus \mO'$
consists of elements in $\mO \setminus T$ that are big with
respect to the residual instance. The total cost of elements in $R$ is
bounded by $\bL'$ (since $\mO$ is a feasible solution), and thus
$|R| \leq \eps^{-3} \cdot d$.

Since $T=\bK_h$, for any $j \in \mO \setminus T$ it holds that
$f_T(j) \leq \frac{f(T)}{|T|}$, and we get
$f_T(R) \leq \sum_{j\in R} f_T(\{j\}) \leq \eps^{-3} \cdot d \frac{f(T)}{|T|}=
\eps f(T) \leq \eps \mO$.
Thus,  $f_{\mO'} (R) \leq f_T(R) \leq \eps \mO$.
Since $f(\mO)= f(\mO') + f_{\mO'}(R) \leq f(\mO') +\eps f(\mO)$, we have that
$f(\mO') \geq (1-\eps) f(\mO)$.

Thus, in this iteration we get a solution $\bar{x}$ for the residual problem
with $F(\bar{x}) \geq \alpha (1-\eps)f(\mO)$, and the solution $S$ obtained after the
rounding satisfies $f(S) \geq (1-O(\eps)) \alpha f(\mO)$.

\end{proof}
We summarize in the next result.

\begin{theorem}
\label{thm:continuous_eq} Let $f$ be a submodular function, and
suppose there is a polynomial time $\alpha$-approximation
algorithm for the continuous problem with respect to $f$. Then
there is a polynomial time randomized
$(\alpha-\eps)$-approximation algorithm for $\SUB$ with respect to
$f$, for any $\eps>0$.
\end{theorem}
Since there is a $(1/4-o(1))$-approximation algorithm
for general instances of continuous ${\SUB}$
\cite{LMNS09},
we have

\begin{theorem}
\label{thm:rand_alg_non_monotone}
There is a polynomial time randomized $(1/4-\eps)$-approximation
algorithm for $\SUB$, for any  $\eps>0$.
\end{theorem}
Since there is a $(1-e^{-1}-o(1))$ approximation algorithm for $\SUB$ with monotone objective function \cite{Vo08}
we have

\begin{theorem}
There is a polynomial time randomized $(1-e^{-1}-\eps)$-approximation
algorithm for $\SUB$ with monotone objective function, for any  $\eps>0$.
\end{theorem}




\section{A Deterministic Approximation Algorithm}
\label{mcmb:deter}
In this section we show how the
algorithm of Section \ref{sec:random_mcmb} can be derandomized,
assuming we have
an oracle for $F$, the extension by expectation of $f$.
For some families
of submodular functions, $F$ can
be directly evaluated; for a general function $f$, $F$ can be
evaluated with high accuracy by sampling $f$, as in \cite{Vo08}.

The main idea is to reduce the number of fractional entries in the
fractional solution $\bx$, so that the number of values a random set $D \sim \bx$
can get is polynomial in the input size (for a fixed value of $\eps$).
Then,
we go over all the possible values, and we are promised to obtain a solution
of high value.

A key tool in our derandomization is the \emph{pipage rounding}
technique of Ageev and Sviridenko \cite{as04}. We give below a brief overview
of the technique.
For any element $i\in U$, define the unit vector $\bar{i}\in \{0,1\}^U$, in which
${i}_j=0$
for any $j \neq i$, and ${i}_i=1$.
Given a fractional solution $\bx$ for the problem
and two elements $i,j$, such that $x_i$ and $x_j$ are both
fractional, consider the vector function
$\bx_{i,j}(\delta)= \bx + \delta \bar{i} - \delta \bar{j}$
(Note that $\bx_{i,j}(\delta)$ is equal to $\bx$ in all entries
except $i,j$).
Let $\delta_{\bx,i,j}^{+}$  and  $\delta_{\bx,i,j}^{-}$ (for short, $\delta^{+}$ and  $\delta^{-}$)
be the maximal and minimal value of $\delta$ for which $\bx_{i,j}(\delta) \in [0,1]^U$.
In both $\bx_{i,j}(\delta^{+}), \bx_{i,j}(\delta^{-})$, the entry of either $i$ or $j$ is
integral.

Define  $F^{\bx}_{i,j}(\delta) = F(\bx_{i,j}(\delta))$ over the domain $[\delta^{-},\delta^{+}]$. The
function $F^{\bx}_{i,j}$ is convex (see \cite{ccpv10} for a detailed proof),
thus
$\bx'=\argmax_{\{\bx_{i,j}(\delta^{+}),\bx_{i,j}(\delta^{-})\}} F(\bx)$
has fewer fractional entries than $\bx$, and $F(\bx') \geq F(\bx)$.
By appropriate selection of $i,j$, such that $\bx'$ maintains feasibility (in some sense),
we can repeat the above step
to gradually decrease the number of fractional entries.
We use the technique to prove the next result.

\begin{lemma}
\label{lemma:derand1}
Let $\bx \in [0,1]^U$ be a solution having $k$ or less fractional entries (i.e.,
$|\{i~|~ 0<x_i<1\}| \leq k$),
and $\bc(\bx) \leq \bL$ for some $\bL$. Then $\bx$ can be converted to a vector
$\bx'$ with at most $k'= \left( \frac{8 \ln (2k)}{\eps} \right)^d$
fractional entries,
such that $\bc(\bx')\leq (1+\eps) \bL$,
and $F(\bx') \geq F(\bx)$, in time polynomial in $k$.
\end{lemma}

\begin{proof}
Let $U' = \{ i~ |~ 0 < x_i < 1\}$ be the set of all fractional entries.
We define a new cost function $\bc'$ over the elements in $U$.

\[
\begin{array}{lcr}
c'_r(i) &=& \left\{
\begin{array}{lcl}
c_r(i) &~~~~ &i\notin U' \\
0 & & \displaystyle{ c_r(i) \leq \frac{\eps \cdot L_r}{2k}}\\
\displaystyle{\frac{\eps \cdot L_r}{2k} (1+\eps/2)^j} & &
\displaystyle{\frac{\eps \cdot L_r}{2k} (1+\eps/2)^j \leq c_r(i)<\frac{\eps \cdot L_r}{2k} (1+\eps/2)^{j+1}}
\end{array}
\right.
\end{array}
\]

Note that for any $i\in U'$, $\bc'(i)\leq \bc(i)$, and
$$c_r(i)\leq  \displaystyle{(1+\frac{\eps}{2}) c'_r(i) + \frac{\eps \cdot L_r}{2k}},$$
for all $1 \leq r \leq d$.
The number of different values $c'_r(i)$ can get for $i\in U'$
is bounded by $\frac{8 \ln (2k) }{\eps}$ (since all elements are
small, and $\ln(1+x) \geq x/2$). Hence the number of different
values $\bc'(i)$ can get for $i\in U'$ is bounded by
$k'= \left( \frac{8 \ln (2k) }{\eps} \right) ^d$.

We start with $\bx'=\bx$, and while there are $i,j \in U'$ such that $x'_i$ and $x'_j$ are both
fractional and $\bc'(i)=\bc'(j)$, define $\delta^{+} = \delta_{\bx',i,j}^{+}$
and $\delta^{-}=\delta_{\bx',i,j}^{-}$.
Since $i$ and $j$ have the same cost (by $\bc'$), it holds that
$\bc' \left( \bx_{i,j}(\delta^{+}) \right) =
\bc' \left( \bx_{i,j}(\delta^{-}) \right) =\bc'(\bx)$.
If $F^{\bx}_{i,j}(\delta^{+})\geq F(\bx)$,
then set $\bx''= \bx_{i,j}(\delta^{+})$, otherwise $\bx''= \bx_{i,j}(\delta^{-})$.
In both cases $F(\bx'')\geq F(\bx')$ and $\bc'(\bx'')= \bc'(\bx')$. Now, repeat
this step with $\bx'=\bx''$.
Since in each iteration the number of fractional entries in $\bx'$ decreases,
 the process will terminate (after at most $k$ iterations)
with a vector $\bx'$ such that $F(\bx')\geq F(\bx)$,
$\bc'(\bx')=\bc'(\bx)\leq \bL$,
and there are no two elements $i,j\in U'$ with $\bc'(i)=\bc'(j)$, where
$x'_i$ and $x'_j$ are both fractional. Also, for any $i\notin U'$, the entry
$x'_i$ is integral (since $x_i$ was integral and the entry was not modified by
the process). Thus, the number of fractional entries in $\bx'$ is
at most $k'$.
Now, for any dimension $1 \leq r \leq d$,
\begin{eqnarray*}
c_r(\bx')&=& \sum_{i\notin U'} x'_i c_r(i) + \sum_{i\in U'} x'_i c_r(i) \\
&\leq&
(1+ \eps/2
) \cdot \sum_{i\notin U'} x'_i  \cdot c'_r(i) +
   \sum_{i\in U'} x'_i \left( (1+\eps/2) c'_r(i) + \frac{\eps \cdot L_r}{2k} \right)\\
&=&
(1+\eps /2) \cdot \sum_{i\in U} x'_i  \cdot c'_r(i) +
   \sum_{i\in U'} x_i \frac{\eps \cdot L_r}{2k} \leq (1+\eps) L_r.
\end{eqnarray*}
This completes the proof.
\end{proof}


Using the above lemma, we can reduce the number of fractional
entries in $\bx$ to a number that is poly-logarithmic in $k$.
However, the number of values $D\sim \bx$
 remains super-polynomial. To reduce further the number
of fractional entries, we apply the above step twice,
that is, we convert $\bx$ with at most $|U|$ fractional entries to $\bx'$
with at most
$k'=\left(8 \ln (2 |U| )/\eps \right) ^d$. We
can then
apply the conversion again, to obtain $\bx''$ with at most
$k''= O(\log |U|)$ fractional entries.

\begin{lemma}
\label{lemma:multiple_pipage}
Given a vector $\bL$ and a constant $\eps > 0$,
let $\bx \in [0,1]^U$ be a vector satisfying
$\bc(\bx) \leq \bL$.
Then $\bx$ can be converted in time polynomial in $|U|$ to a vector
$\bx'$ with at most $k''=O(\log |U|)$
  fractional entries, such that $\bc(\bx')\leq (1+\eps)^2 \bL$,
and $F(\bx') \geq F(\bx)$,
\end{lemma}


The next result follows immediately from
Lemma~\ref{lemma:expected_random} ($\mO$ is the value of an optimal solution for ${\SUB}$).

\begin{lemma}
\label{lemma:expected_enumeration}
Given $\bx \in [0,1]^U$ such that $\bx$ is a feasible fractional solution
with $F(\bx) \geq \mO /5$,
there exists
a realization of the random variable $D \sim \bx$, such that the solution
$\cD$ is nearly feasible, and $F(\cD) \geq (1-O(\eps) ) F(\bx)$.
\end{lemma}
}

Consider the following rounding algorithm.\\

\noindent
{\bf Deterministic Rounding Algorithm for $\SUB$ with No Big Elements} \\
\noindent
{\bf Input:} A $\SUB$ instance, a feasible solution  $\bar{x}$ for the continuous problem, with
$F(\bar{x}) \geq \mO /5$.
\begin{enumerate}
\item
Define $\bx'= (1+\eps)^{-2} \cdot \bar{x}$
(note that $F(\bx') \geq (1+\eps)^{-2} \cdot F(\bx)$).
\item Convert $\bx'$ to $\bx''$
such that $\bx''$ is fractionally feasible, the number
of fractional entries in $\bx''$ is $O(\log |U|)$, and
$F(\bx)\geq (1+\eps)^{-2} F(\bx'')\geq (1-e^{-1}-O(\eps)) \mO$,
as in Lemma \ref{lemma:multiple_pipage}.
\item Enumerate over all possible realizations of $D\sim \bx''$.
For each such realization,
if the solution $\cD$ is $\eps$-nearly feasible
convert it to a feasible solution $\cD'$ (see Lemma \ref{lemma:nearly_fix}).
Return the solution with maximum value among the feasible solutions found.
\end{enumerate}

By Theorem~\ref{thm:prb_claim}, the algorithm returns a feasible solution of value at least $(1-O(\eps)) F(\bar{x})$.
Also, the running time of the algorithm is polynomial when $\eps$ is a fixed
constant. Replacing the
randomized rounding step in the
algorithm of Section~\ref{sec:mcmb} with the above Deterministic Rounding Algorithm,
we get the following result.


\begin{theorem}
\label{thm:continuous_det_eq} Let $f$ be a submodular function,
and assume we have an oracle for $F$. If there is a deterministic
polynomial time $\alpha$-approximation algorithm for the
continuous problem with respect to $f$, then there is a polynomial
time deterministic $(\alpha-\eps)$-approximation algorithm for
$\SUB$ with respect to $f$, for any $\eps>0$.
\end{theorem}

We note that,
given an oracle to $F$, both the algorithms of \cite{Vo08} and \cite{LMNS09} for the
continuous problem are deterministic, thus we get
the following.

\begin{theorem}
\label{thm:det_alg}
Given an oracle for $F$,
there is a polynomial time deterministic  $(1-e^{-1} - \eps)$-approximation
algorithm for $\SUB$ with a monotone function, for any $\eps>0$.
\end{theorem}

\begin{theorem}
\label{thm:det_alg_non_monotone}
Given an oracle for $F$,
there is a polynomial time deterministic  $(1/4- \eps)$-approximation
algorithm for $\SUB$ for any $\eps>0$.
\end{theorem}

For the problem of maximum coverage with $d$ knapsack
constraints, i.e.,  $\SUB$ where the objective function is
$f=f_{G,\bar{p}}$, for a given bipartite graph $G$ and profits
$\bar{p}$, the function $F$ can be evaluated deterministically
(see~\cite{as04}). This yields the following result.

\begin{theorem}
There is a polynomial time deterministic
$(1-e^{-1}-\eps)$-approximation algorithm for maximum coverage with
$d$ knapsack constraints.
\end{theorem}

\comment{
\section{Maximum Coverage with Multiple Packing and Cost
Constraints
}
\label{sec:budgeted_max_coverage}
In this section we consider the problem of {\em maximum coverage with multiple
packing and cost constraints ($\MC$)}.
Let $\f$
denote the maximal number of sets to which an element belongs, and let
\begin{equation}
\label{eq:def_alphaf} \alphaf = 1- \left( 1- \frac{1}{\f} \right ) ^\f.
\end{equation}
We give below an $(\alphaf - \eps)$-approximation algorithm for the problem.
 We note that, for any $\f \geq 1$, $\alphaf > 1 -e^{-1}$. In
solving $\MC$, our algorithm uses the following continuous version of the problem. Let $\by \in  [0,1]^{ \bS \times A}$ and $\bx \in [0,1]^\bS$.
For short, we write $y_{i,j}=y_{S_i,a_j}$ and $x_i=x_{S_i}$. Given an input for $\MC$, we say that $(\by,\bx)$
\comment{
Recall that the problem of {\em maximum coverage with multiple packing and cost
constraints ($\MC$)} is
the following generalization of the maximum coverage problem.
Given is a collection of sets $\bS = \{ S_1, ..., S_m \}$ over a ground set
$A=\{ a_1,...,a_n \}$. Each element $a_j$ has a profit $p_j\geq 0$
and a $d_1$-dimensional size vector $\bar{s}_j =
(s_{j,1}, \ldots , s_{j,d_1})$, such that $s_{j,r} \geq 0$ for all
$1 \leq r \leq d_1$. Each set $S_i$ has $d_2$-dimensional
weight vector $\bw_i = (w_{i,1},\ldots ,w_{i,d_2})$.
Also given is a $d_1$-dimensional capacity vector $\bB =(B_1, \ldots , B_{d_1})$,
and a $d_2$-dimensional weight bound vector $\bW= (W_1,\ldots,W_{d_2})$.
A solution for the problem is a collection of subsets $H$
and a subset of elements $\cE$, such that for any $a_j \in \cE$
there is $S_i \in H$ such that $a_j \in S_i$.
A solution is feasible if the total weight of subsets in $H$ is
bounded by $\bW$ and the total size of elements in $\cE$ is
bounded by $\bB$. The profit of a solution $(H,\cE)$ is
the total profit of elements in $\cE$. The objective
of the problem is to find a feasible solution with
maximal profit.

Denote the maximal number of sets a single element belongs
to by $f$, and let $\alpha_f = 1- \left( 1- \frac{1}{f} \right ) ^f $. Our
objective is to obtain a $(\alpha_f -\eps)$-approximation algorithm for the
problem. Note that for any $f \geq 1$ it holds that $\alpha_f > 1-e^{-1}$.

For solving the problem, our algorithm uses a slightly different point of view.
Given an input for the problem, we say that a pair $(\by,\bx)$ where $\by \in  [0,1]^{ \bS \times A}$ and $\bx \in  [0,1]^\bS$ } is a solution
if, for any $S_i \in \bS$ and $a_j \notin S_i$ it holds that $y_{i,j}=0$ (for short, we write $y_{i,j}=y_{S_i,a_j}$), and for any $S_i \in \bS$
and $a_j \in A$ it holds that $y_{i,j} \leq x_i$. Intuitively, $x_i$ is an indicator for the selection of the set $S_i$ into the solution, and
$y_{i,j}$ is an indicator for the selection of the element $a_j$ by the set $S_i$ into the solution. We say that such a solution is feasible if,
for any  $1\leq r \leq d_1$ it holds that $\sum_{a_j \in A} s_{j,r} \cdot \sum_{S_i \in \bS} y_{i,j}  \leq B_r$ (the total size of elements does
not exceed the capacity), and for any $1\leq r \leq d_2$ it holds that $\sum_{S_i \in \bS} x_i \cdot w_{i,r} \leq W_r$ (the total weight of
subsets does not exceed the weight bound).
The value (or profit)  of the solution  is defined by $p(\by,\bx)= p(\by) = \sum_{a_j \in A}  \min
\{1, \sum_{S_i \in \bS} y_{i,j} \} \cdot p_j$. By the above definition, a solution consists of fractional values. We say that a solution
$(\bx,\by)$ is \emph{semi-fractional}
if $\bx \in \{0,1\}^{\bS}$
(that is, sets cannot be fractionally selected, but elements can be). Also,
we say that a solution is \emph{integral} if both $\bx \in \{0,1\}^{\bS}$
and $\by \in \{0,1\}^{ \bS \times A}$.

Two computational problems arise from the above definitions. The first
is to find a semi-fractional solution of maximal profit. We refer to this
problem as the {\em semi-fractional problem}. The second
is to find an integral solution of maximal profit, to which we refer as
the {\em integral problem}. It is easy to see that the integral problem is
equivalent to $\MC$, therefore our objective is to find an optimal solution
for the integral problem.

\paragraph{Overview:} To obtain an approximation algorithm for the integral problem,
we first show how it relates to the semi-fractional problem. In particular, we show
that, given an $(\alphaf-O(\eps))$-approximation algorithm for the
semi-fractional problem, we can derive approximation algorithm with
the same approximation ratio for the integral problem.
Next, we interpret the semi-fractional problem as a $\SUB$ instance
whose universe has infinite size.
We then use the framework developed in Section \ref{sec:randomized_mcmb}
to solve this problem. As direct enumeration over the most
profitable elements in an optimal solution is impossible here, we
guess which sets are the most profitable in an optimal solution.
We use this guessing to obtain a fractional solution (with
polynomial number of non-zero entries), such that the conditions of
Theorem \ref{thm:prb_claim} are satisfied. Together with a fixing procedure,
applied to the resulting nearly feasible solution, this leads to our
approximation algorithm. The process can be derandomized by using the
same tools as in Section \ref{mcmb:deter}.

\subsection{Reduction to the Semi-fractional Problem}

We first show that any semi-fractional solution for the problem can
be converted to a solution with at least the
same profit and at most $d_1$ fractional
entries. Next, we show how this property
enables to enumerate over the most profitable elements in an optimal solution.
Throughout this section we assume that, for some constant
$\alpha \in (0,1)$, we have
an $\alpha$-approximation algorithm for the semi-fractional problem.

\begin{lemma}
\label{lemma:frac_fix}
Let $(\byf,\bxf)$ be a feasible semi-fractional solution. Then
$\byf$ can be converted in polynomial time to another feasible semi-fractional
solution $(\by,\bxf)$ with at most $d_1$ fractional entries, such
that $p(\by)\geq p(\byf)$.
\end{lemma}

\begin{proof}
Let $(\byf,\bxf)$ be a semi-fractional feasible solution. W.l.o.g,
we assume that, for any $a_j \in A$, $\sum_{S_i \in \bS} \yf_{i,j} \leq 1$,
and if $\yf_{i,j} \neq 0$ then for any $S_{i'}\neq S_i$
it holds that $\yf_{i',j} =0$. Note that  any
solution can be easily converted to such a solution having
the same profit.
If there are more than $d_1$ fractional entries,
let $\bs_{j_1},\ldots \bs_{j_k}$ be the size
vectors of the corresponding elements, and
let $S_{i_1} \ldots S_{i_k}$ be the corresponding sets.
Since $k > d_1$,
there must be a linear dependency between the size
vectors. W.l.o.g we can write
$\lambda_1 \bs_{j_1} + \ldots \lambda_{p} \bs_{j_p} = 0$ for $p=d_1 +1$.
We can define $\byf(\eps)$ by
$\yf_{i_\ell,j_\ell}(\eps) =  \yf_{i_\ell,j_\ell} +\eps \lambda_\ell$
for $1\leq \ell \leq p$, and $\yf_{i,j}(\eps) =\yf_{i,j}$ for any other
entry. As long
as $\byf(\eps) \in [0,1] ^{ \bS \times A}$, $\byf(\eps)$ is
a semi-fractional feasible solution. Let $\eps^{+}$ and $\eps^{-}$ be
the maximal and minimal values of $\eps$ for which
$\byf(\eps) \in [0,1] ^{ \bS \times A}$. The number of
fractional entries in $\byf(\eps^{+})$ and $\byf(\eps^{-})$ is
smaller than the number of fractional entries in $\byf$.
Also, $p(\eps)= p(\byf(\eps))$ is a linear function,
thus either $p(\byf(\eps^{+})) \geq p(\byf)$ or
$p(\byf(\eps^{-})) \geq p(\byf)$. Thus, we can
convert $\byf$ to a feasible solution ${\by}'$ that has less fractional
entries, such that $p({\by}') \geq p(\byf)$.
By repeating the above process
we can obtain a fractional
solution with at most $d_1$ fractional entries.
\end{proof}
We use Lemma \ref{lemma:frac_fix} to prove the next result.

\begin{lemma}
Given an $\alpha$-approximation algorithm for the semi-fractional
problem, an $\alpha$-approximation algorithm for the integral problem can be
derived in polynomial time.
\end{lemma}

\begin{proof}
Given a collection $T$ of pairs $(a_j,S_i)$ of an element
$a_j$ and a set $S_i$, such that $a_j \in S_i$,
denote the collection of sets in $T$ by $T_S$,
and the collection of elements in $T$ by $T_\cE$. We define a
residual instance for the problem as follows. The
elements are:
$$A_T=\{a_j \in A ~|~ \mbox{$a_j \notin  T_\cE$, for
any $a_{j'} \in T_\cE$, $p_j\leq p_{j'}$ } \},$$
where the size of $a_j \in A_T$ is $\bs_j$, and the profit
of $a_j$ is $p_j$.
The sets are $\bS_T=\{S'_1,\ldots,S'_m\}$, where
$S'_i=S_i \cap A_T$, and
\[
\begin{array}{lcr}
\bw'_i &=& \left\{
\begin{array}{lcl}
\bw_i &~~~~ &S_i \notin T_S \\
0 & & \mbox{otherwise}
\end{array}
\right.
\end{array}
\]

The weight
bound of the residual instance is $\bW_T = \bW - w(T_S)$,
where $w(T_S) = \sum_{S_i \in T_S} \bw_i$, and
the capacity is $\bB_T = \bB - s(T_\cE)$, where
$s(T_\cE) = \sum_{a_j \in T_\cE} \bs_j$.

Clearly, a solution of profit $v$ for the
residual instance with respect to a collection $T$
gives a solution of profit $v +  p(T_\cE)$
for the original instance, where $p(T_\cE) = \sum_{a_j\in T_\cE} p_j$.
Let $\mO = (\bx,\by)$ be an optimal solution for the integral problem.
W.l.o.g. we assume that for all $a_j \in A$, $\sum_{S_i \in \bS} y_{i,j} \leq 1$
(that is, no element is selected by more than one set).
Let $R$ be the collection of $h= \frac{d_1}{1-\alpha}$
most profitable elements $a_j$  for which there exists $S_i$
such that $y_{i,j}=1$ (note that there is a unique set $S_i$ for
each $a_j$). Define $T^\mO= \{ (a_j, S_i) | a_j \in R \land y_{i,j}=1\}$.
It is easy to verify that the optimal integral solution for
the residual problem with respect to $T^\mO$ is
$\mO - p(T_\cE^\mO)$.

Now, assume that we have an $\alpha$-approximation
algorithm for the semi-fractional problem. Then the algorithm
returns a fractional solution $(\byf,\bxf)$ with
$p(\byf) \geq \alpha ( \mO - p(T_\cE^{\mO}))$.
By Lemma
\ref{lemma:frac_fix}, this solution can be converted
to a solution $(\bz,\bxf)$ with up to $d_1$ fractional
entries satisfying
$p(\bz)
\geq p(\byf)$. Now, consider rounding
down to zero the value of each fractional entry in
$\bz'$.
This results in a new feasible integral solution
$\bz''$
with $p(\bz') \geq p(\byf) - \frac{d_1 \cdot p(T_\cE^{\mO})}{|T|}$ (since
the profit of any element in the residual solution is bounded by
$\frac{p(T_\cE^{\mO})}{|T|}$).
Hence, we obtain a solution for the integral problem of value
at least
$$p(T_\cE^{\mO})+p(\byf) - \frac{d_1 \cdot p(T_\cE^{\mO})}{|T|}
\geq p(T_\cE^{\mO}) \left( 1- \frac{d_1}{|T|} \right) + \alpha  (\mO -
p(T_\cE^{\mO}))\geq \alpha \mO .$$
Thus, we have an $\alpha$-approximation for the optimum of the integral problem.
To apply this
technique, we need to guess the correct set $T$, which
can be done in time $(n\cdot m)^{O(1)}$ for a constant $\alpha$.
\end{proof}

In Theorem \ref{thm:semi_frac} we show that there is a polynomial time
$(\alphaf -\eps)$-approximation algorithm for the semi-fractional problem,
where $\alphaf$ is defined in (\ref{eq:def_alphaf}).
Thus, we have the following.

\begin{theorem}
There is a polynomial time $(\alphaf- \eps)$-approximation algorithm
for $\MC$ for any $\eps > 0$, where  $\f$ is the maximal
number of sets to which a single element belongs.
\end{theorem}



\subsection{Solving the Semi-fractional Problem}
\subsubsection{A Submodular Point of View}

Let $\tmO=(\by,\bx)$
be an optimal solution for an instance of the semi-fractional problem. W.l.o.g.,
we may assume that for any element $a_j \in A$, $\sum_{S_i \in \bS} y_{i,j}
\leq 1$.
For any $S_i \in S$, we define the profit of $S_i$ with respect
to the solution $\tmO$ by $p_{\tmO} (S_i)= p(S_i)= \sum_{a_j \in A} y_{i,j} p_j$
(note that if $x_i=0$ then $p(S_i) = 0$).
Since  $\sum_{S_i \in \bS} y_{i,j} \leq 1$ holds for any $a_j \in A$, this
means that $p(\tmO) = \sum_{S_i \in \bS} p_{\tmO} (S_i)$.

Given $T$, the collection of the $h=\eps^{-4} \cdot d$
most profitable sets in $\tmO$, we define a maximization
problem for a submodular function, with $d= d_1 +d_2$
linear constraints. Let $\bW' = \bW- w(T)$
(we guess the set $T$).
\begin{itemize}
\item
For each $S_i \in T$ and any vector $\bz \in [0,1]^{A}$ satisfying:
$z_j = 0$ for all $a_j \notin S_i$,
add the element
$(S_i,\bz)$ to the universe $U$. The cost of this
element is $\bc$, where $c_r= \sum_{a_j \in A} z_j \cdot
s_{i,j}$ for $1\leq r \leq d_1$,
and $c_r=0$ for $d_1+1\leq r \leq d_1+d_2=d$.
\item
For each set $S_i \notin T$ and a vector $\bz \in [0,1]^{A}$ satisfying
\begin{enumerate}
\item[1.]
$z_j = 0$ for all $a_j \notin S_i$,
\item[2.]
for any $1\leq r \leq d_1$
it holds that $\sum_{a_j \in A} z_j \cdot s_{j,r} \leq \eps^3 B_r$, and
\item[3.]
for any $1\leq r \leq d_2$ it holds that $ w_{i,r} \leq \eps^3 W'_r$,
\end{enumerate}
add  $(S_i,\bz)$ to $U$.
The cost of this element is $\bc$, where
$c_r= \sum_{a_j \in A} z_j \cdot s_{j,r}$ for $1\leq r\leq d_1$,
and $c_{d_1+r} = w_{i,r}$ for $1\leq r \leq d_2$.
\end{itemize}

The budget vector $\bL$ for this instance is the vector $\bB$
concatenated to $\bW'$,
that is, $L_r = B_r$ for $1\leq r \leq d_1$, and $L_{d_1 +r } = W'_r$
for $1\leq r \leq d_2$.
Define $f_j: 2^U \rightarrow \mathbb{R}$ for any $a_j \in A$,
by
\begin{equation}
\label{eq:def_fj}
f_j(V)= \min \{ \sum_{(S_i,\bz) \in V} z_j ,1\},
\end{equation}
and  $f: 2^U \rightarrow \mathbb{R}$  by $f(V) =\sum_{a_j \in A} f_j(V) \cdot p_j$.
Since each $f_j$ is a submodular non-decreasing set function, $f$ is
a submodular non-decreasing set function as well.
It is easy to see  that a solution of value $v$ for the above instance of
 ${\SUB}$ yields a solution of the same value (profit) for the
semi-fractional problem.

Let the size of a set
$S_i \in S \setminus T$
(with respect to the solution $\tmO$)
be $s(S_i) = \sum_{a_j \in A} y_{i,j} \cdot \bs_j$. We say that
a set $S_i \in \bS \setminus T$ is \emph{small} if
$s(S_i)\leq \eps^3 \bB$ and $\bw_i \leq \eps^3 \bW'$; otherwise it is
{\em big}.
Consider the following solution for the above ${\SUB}$ instance.
For each $S_i \in T$, define the vector $\bz$ by $z_j=y_{i,j}$,
for all $a_j \in A$, and add
the element $(S_i,\bz)$ to the solution; for each
$S_i \notin T$ such that $S_i$ is {\em small},
define $\bz$ by $z_j=y_{i,j}$ and add $(S_i,\bz)$ to the solution.
Denote the resulting solution by $V$.
It can be easily verified that $V$ is a feasible solution,
and it holds that
\begin{eqnarray}
f(V)&=& \sum_{S_i \in T} p(S_i) +
\sum_{S_i \in \bS \setminus T \mbox{, $S_i$ is small}} p(S_i) =
\tmO -
\sum_{S_i \in \bS \setminus T \mbox{, $S_i$ is big}} p(S_i)
\nonumber \\
&\geq& \tmO - \frac{d\cdot \eps ^{-3}}{h} \tmO \geq (1-\eps) \tmO.
\label{eq:lb_fV}
\end{eqnarray}
The last inequality holds since the number of big sets in $\tmO$
is bounded by $d \cdot \eps^{-3}$, and the profit of each of these
 sets is bounded by $\tmO /h$. This implies that the value of the optimal
solution for the $\SUB$ instance
 is between $(1-\eps) \tmO$ and $\tmO$.

\subsubsection{Obtaining a Distribution on the Universe of the Submodular
Problem}
We now use the technique of Section \ref{sec:randomized_mcmb}
for solving the $\SUB$ instance. To do so, we first need to obtain a fractional
feasible solution. As the size of $U$ may be unbounded, we cannot
use the algorithm of Vondr\'{a}k \cite{Vo08}. Thus, we obtain a fractional
solution by using
a linear programming formulation of the problem. Let $\bS'$
be the collection of all sets $S_i$ in $\bS$ such that $S_i \in T$,
or
${\bw}_i \leq \eps^3 \bW'$.
Consider the following linear program:


\[
\begin{array}{llll}
(LP(T))& \mbox{maximize} &

 \displaystyle{\sum_{S_i \in \bS'}  \sum_{a_j \in A}  y_{i,j} \cdot  p_j } \\
& \mbox{subject to:}\\ \\
  & \forall S_i \in \bS', a_j\in A  & \displaystyle{0 \leq y_{i,j}\leq x_i }\\
& \forall  S_i \in \bS',a_j\notin
S_i
 & \displaystyle{y_{i,j}=0 }\\
& \forall a_j\in A & \displaystyle{ \sum_{S_i \in \bS'} y_{i,j} \leq 1}\\
&\forall 1\leq r\leq d_1 & \displaystyle{\sum_{S_i \in \bS'}  \sum_{a_j \in A}  y_{i,j} \cdot s_{j,r} \leq B_r}\\
& \forall 1\leq r\leq d_2&
  \displaystyle{ \sum_{S_i \in \bS' \setminus T} w_{i,r}\cdot x_i \leq W'_r} \\
& \forall S_i\in T & \displaystyle{x_i =1} \\
& \forall S_i \in \bS'\setminus T \mbox{ and } \forall 1\leq r\leq
d_1 &
\displaystyle{\sum_{a_j \in A} y_{i,j}\cdot s_{j,r} \leq \eps^3 B_r \cdot x_i}\\
\end{array}
\]

Let $\bxf,\byf$ be an optimal solution for $LP(T)$ of value
$\mOf$. Clearly, $\mOf$ is greater or equal to the
optimal solution of the $\SUB$ instance; thus, by (\ref{eq:lb_fV}),
$\mOf \geq (1-\eps) \tmO$. We use this solution to
generate fractional solution for the $\SUB$ instance. Define $\bar{X}
\in [0,1]^{U}$ as follows.
For any $S_i\in \bS'$ such that $\xf_i > 0$, let $\bz_i \in [0,1]^{A}$,
where $z_{i,j}  =\frac{ \yf_{i,j}} {\xf_i}$,
and we set
$X_i= X_{(S_i,\bz_i)} = \xf_i$. For any other $u \in U$, set $X_u=0$.
For any $a_j \in A$,
define $Y_j = \sum_{S_i \in \bS'} \yf_{i,j}$, then clearly
$\mOf= \sum_{a_j \in A} {Y_j \cdot p_j}$.
\begin{lemma}
\label{lp_round1}
Let $D$ be a random variable such that
$D\sim \bar{X}$, then for any $a_j \in A$, $E[f_j(D)] \geq \alpha_f \cdot
Y_j$, where $f_j$ is defined in (\ref{eq:def_fj}).
\end{lemma}
We use in the proof the next claim.
\begin{claim}
\label{claim:lb_Yj_claim}
For any $x\in [0,1]$ and $\f \in \mathbb{N}$,
\begin{equation*}
 1- \left(1-\frac{x}{\f}\right)^{\f} \geq x \cdot \alphaf,
 \end{equation*}
where $\alphaf$ is defined in (\ref{eq:def_alphaf}).
\end{claim}
\begin{proof}
Let $h(x) = 1- \left(1-\frac{x}{\f}\right)^{\f} -  x \cdot \alphaf$, then $h(0)=h(1)=0$.
Also, $h''(x) = - \frac{\f-1}{\f} \left (1- \frac{x}{\f} \right )^{\f-2} \leq 0$ for $x\in [0,1]$.
Hence, $h(x)\geq 0 $ for $x\in [0,1]$, and the claim holds.
\end{proof}

\begin{dl_proof} {\ref{lp_round1}}
For the case where $Y_j = 0$ the claim trivially holds, thus
we assume below that $Y_j \neq 0$.
Let $X_i$ be an indicator random variable for $(S_i,\bz_i) \in D$, for
any $S_i \in \bS'$. The random variables $X_i$ are independent,
and $Pr[X_i=1] = \xf_i$.  Then,
$$
f_j(D)= \min\left\{ 1, \sum_{(S_i,\bz_i) \in D} z_{i,j} \right\}=
\min\left\{1, \sum_{S_i\in \bS', a_j \in S_i} \frac{
\yf_{i,h
}}{\xf_i} \cdot X_i\right\}$$

Let $S'[j]
=\left \{S_i \in \bS'~|~ a_j \in S_i \right\}$
 and $\tau_j = \abs{S'[j]}$.
Let $H \geq 1$ be an integer
such that, for any $S_i\in S'[j]$ with $\xf_i \neq 0$,
$\frac{\yf_{i,j}}{\xf_i}$ is an integral multiple of $\delta = 1/H$
 (assuming all values are rational, such a value of $H$ exists).
 Let $Z_1,\ldots, Z_H$ be a set of indicator random variables used as follows.
Whenever $X_i$ is selected in our random process (i.e., $X_i=1$),
randomly select $\frac{\yf_{i,j}}{\xf_i} \cdot \delta^{-1}$
 indicators among $Z_1, \ldots, Z_H$ with uniform distribution.
 For $1 \leq h \leq H$, $Z_h=1$ if $Z_h$ was selected by some $X_i$, $i \in
 S'[j]$
(we say that $Z_h$ is {\em selected} in this case), otherwise $Z_h=0$.
 In this process, the probability of a specific indicator $Z_h$ to be
 selected by a specific $X_i$ is zero when $x_i=0$, and
$\xf_i \cdot \frac{\yf_{i,j}}{\xf_i} = \yf_{i,j}$ otherwise.
Hence, we get that, for all $1 \leq h \leq H$,
\begin{eqnarray}
E[Z_h] = Pr(Z_h =1) &=& 1- \prod_{S_i\in S[j]} \left(1-\yf_{i,j}
 \right ) \nonumber \\
& \geq &
 1-\left( \frac{1}{\tau_j} \sum_{S_i\in S'[j]} \left (  1- \yf_{i,j}
 \right ) \right) ^ {\tau_j} \nonumber \\
& =& 1- \left( 1- \frac{ Y_j}{\tau_j} \right ) ^{\tau_j} \geq Y_j \alphaf.
\label{eq:mean_Zh}
\end{eqnarray}
The first inequality follows from the inequality of the three means, and the
second inequality follows from Claim \ref{claim:lb_Yj_claim}.

Let $Y'_j= \delta \cdot \sum_{h=1}^{H} Z_h$ be $\delta$ times the number of selected indicators.
An important property of $Y'_j$ is that $Y'_j \leq f_j(D)$. Indeed, if
$f_j(D)=1$
then $Y'_j\leq 1$, since there are only $H=\delta^{-1}$
indicators, and if $f_j(D) <1$, then no more than $f_j(D)\delta^{-1}$
indicators are selected; therefore,
$E[Y'_j] \leq
E[f_j(D)]$.
From (\ref{eq:mean_Zh}), we have that
\[
E[Y'_j]=
\delta \sum_{h=1}^{H}
 E[Z_h] \geq \delta \cdot H \cdot   Y_j \alphaf = Y_j\cdot
 \alphaf.
 \]
Hence, $E[f_j(D)]\geq  \alphaf \cdot
Y_j$
as desired.
 \end{dl_proof}

The next lemma follows immediately from Lemma \ref{lp_round1}
and (\ref{eq:lb_fV}).
Recall that $F$ is an extension by expectation of $f$, then
\begin{lemma}
$F(\bar{X}) \geq \alphaf \cdot \mOf \geq (1-\eps) \cdot\alphaf \cdot \tmO$.
\end{lemma}

It is easy to verify that $\bar{X}$ is a feasible fractional solution for the $\SUB$ instance. Recall that the element $(S_i,\bz)$ is big if the
cost $c_r$ of this element in some dimension $1 \leq r \leq d$ is larger than $\eps^3$ times the budget in this dimension. We note that if
$(S_i,\bz)$ is big then it holds that $S_i \in T$, and $X_{(S_i,\bz)} =1$.
Let $D \subseteq U$ be a random set such that $D \sim \bar{X}$. Also,
let $D'=D$ if $D$ is $ \eps$-nearly feasible, and $D'=\emptyset$ otherwise.
 By Theorem \ref{thm:prb_claim}, we get that  $D'$ is always
$\eps$-nearly feasible, and
$$ E[f(D')] \geq (1-O(\eps) ) F(\bx) \geq (1-O(\eps)) \alphaf \tmO .$$

Given a nearly feasible fractional solution for the $\SUB$ instance, we
now show that it can be converted to a feasible one.
\begin{lemma}
\label{lemma:nearly_fix_mcmp}
Let $D$ be an $\eps$-nearly feasible solution for the $\SUB$ instance,
then $D$ can be converted to a feasible solution $D'$ such that
$f(D') \geq (1-O(\eps)) f(D)$.
\end{lemma}
\begin{proof}
Our conversion will be done in two steps.
First, let $D_1 = \{ (S_i,\frac{\bz}{(1+\eps)}) | (S_i,\bz) \in D\}$.
Clearly, $D_1$ is feasible in the first $d_1$ dimensions
(for any $1\leq r \leq d_1$, $c_r (D_1) \leq L_r$);
also, $f(D_1) \geq (1-\eps) f(D)$.
Next, we note that for any $d_1 +1 \leq r \leq d$, for any element
$u\in U$ it holds that $c_{u,r}\leq \eps^3 L_r$. Thus, we can apply
in these dimensions the fixing procedure of Lemma \ref{lemma:nearly_fix}.
Hence, we can convert $D$ to $D'$ as desired.
\end{proof}

We now summarize the steps of the algorithm.
\\
\noindent
{\bf Randomized Approximation algorithm for the Semi-fractional Problem}
\begin{enumerate}
\item For any subset $T\subseteq \bS$ of size at most $h= d\cdot \eps^{-4}$:
\begin{enumerate}
\item Solve $LP(T)$, let $\bxf,\byf$ be the solution found.
\item Define $\bar{X}$, and let $D$ be a random set $D \sim \bar{X}$,
then  $D'=D$ if
$D$ is $\eps$-nearly feasible, and $D'=\emptyset$ otherwise.
\item Convert $D'$ to a feasible
set $D''$ as in the proof of Lemma \ref{lemma:nearly_fix_mcmp}.
\end{enumerate}
\item
Let $D''$ be the solution of maximal profit found for the
$\SUB$ instance. Covert it to a solution for the
semi-fractional problem and return this solution.
\end{enumerate}

Clearly, the algorithm returns a feasible solution for the problem. Consider
the iteration in which $T$ is the set of $h$ most profitable elements in $\tmO$.
In this iteration,  $E[f(D')] \geq (1- \eps) \alphaf \tmO$.
Hence, by Lemma~\ref{lemma:nearly_fix_mcmp},
$E[f(D'')] \geq (1 - O(\eps))\alphaf \tmO$.
By a properly selecting the value of $\eps$,
we get the following.

\begin{theorem}
\label{thm:semi_frac}
There is a polynomial time randomized $(1-\eps)\alphaf$-approximation algorithm for
the semi-fractional problem, for any fixed $\eps >0$.
\end{theorem}

The algorithm can be derandomized, using
the technique in Section \ref{mcmb:deter}.
Note that here, the extension by expectation $F(\bar{X})$
can be deterministically evaluated in polynomial time,
since the number of non-zero entries in $\bar{X}$ is polynomial.
\section{The Budgeted Generalized Assignment Problem}
\label{sec:bgap}


In this section we develop an approximation algorithm for BGAP, and for its generalization, BSAP.
Recall that a BSAP instance consists of $n$ items $A=\{a_1,\ldots,a_n\}$
and $m$ bins, such that bin $i$ has a $d_2$-dimensional capacity $\bb_i$. Each item $a_j$ has a
$d_2$-dimensional size $\bs_{i,j}\geq 0$, a $d_1$-dimensional cost vector $\bc_{i,j}\geq 0$, and
a profit $p_{i,j}\geq 0$ that is gained when $a_j$ is assigned to bin $i$.
Also, given is a $d_1$-dimensional budget vector $\bL$.

We say that a subset of items $S_i \subseteq A$ is a \emph{feasible assignment} for bin $i$ if
$\sum_{a_j \in S_i} \bs_{i,j} \leq \bb_i$. We define the
 cost and profit of assigning $S_i$ to bin $i$ by
$\bc(i,S_i) = \sum_{a_j \in S_i} \bc_{i,j}$, and
$p(i,S_i) = \sum_{a_j\in S_i} p_{i,j}$, respectively.
A \emph{solution} for the problem is a tuple of $m$ {\em disjoint} subsets of items $\bS=(S_1,\ldots,S_m)$,
such that each set $S_i$ is a feasible assignment for bin $i$.
We define the cost of $\bS$ by
$\bc(\bS) = \sum_{i=1}^{m} \bc(i,S_i) = \sum_{i=1}^{m} \sum_{a_j\in S_i} \bc_{i,j}$,
and its profit by
$p(\bS) = \sum_{i=1}^{m} p(i,S_i) = \sum_{i=1}^{m} \sum_{a_j\in S_i} p_{i,j}$.
We say that a solution $\bS$ is \emph{feasible} if $\bc(\bS)  \leq \bL$.
The goal is to find a feasible solution of maximal profit.


As before, we say that a solution $S$ is $\eps$-nearly feasible
if $\bc(\bS) \leq (1+\eps) \bL$. An item $a_j$ is \emph{small} if, for
any bin $1 \leq i \leq m$, it holds that $\bc_{i,j} \leq \eps^3 \bL$;
otherwise, $a_j$ is \emph{big}.
Also, an assignment $S_i \subseteq A$ of items to bin $i$
is \emph{small} if
$\bc(i,S_i) \leq \eps^3 \bL$. Our algorithm uses two special cases of
BSAP. The first is
\emph{small items BSAP}, in which all items are small; the second is
 \emph{small assignments BSAP}
in which, for any bin $i$ and a feasible assignment $S_i$, it holds that $S_i$ is a small assignment
for bin $i$.

\paragraph{Overview:}

Our algorithm proceeds in four  stages.
The first stage obtains an $\eps$-nearly feasible solution with high profit
for small assignments instances of BSAP,
 by using Theorem \ref{thm:prb_claim}.
To do so, we use an interpretation of the classic SAP problem as a submodular optimization
problem and the technique of ~\cite{fgms06} to obtain
a distribution over its solution space. The small assignments property is used
 to show that the conditions of Theorem~\ref{thm:prb_claim} are
satisfied.

The second stage shows how enumeration can be used to reduce a small items instance of BSAP into a small assignments instance, so that the
algorithm of the first stage can be applied. The main idea is to guess the most profitable bins in some optimal solution and the approximate
cost of these bins in this solution. The algorithm uses this guess
 to eliminate all big assignments, by adding $d_1$ linear
constraints for each bin. The result of this stage is a $2\eps$-nearly feasible solution of high profit.

The third stage handles small items instances. The fact that all items are small is essential for converting the nearly feasible solution of the
previous stage to a feasible one. The fourth and last stage gives a reduction from general instances to small items instances. This is done by
simple enumeration, as in Section \ref{sec:random_mcmb}.



\subsection{Small Assignments Instances of BSAP}
\label{sec:SABSAP}


We solve BSAP instances with small assignments by casting BSAP as an instance
 of generalized $\SUB$ (see Section \ref{sec:prb_claim}).\footnote{The submodular interpretations
of GAP and SAP are well-known (see, e.g., \cite{FV06} and \cite{fgms06}).}
 Let $\cI_i$ be the set of feasible assignments for bin $i$, and $U =
\{ (i,S_i) |~ S_i \in \cI_i\}$. Define $f_j:2^U \rightarrow \mathbb{R}_+$ for any $a_j \in A$ by
$$f_j (V) = \max \{ p_{i,j} | ~(i,S_i)\in V, a_j \in S_i \},$$
where the maximum over an empty set is equal
to zero. Also, define $f:2^U \rightarrow \mathbb{R}_+$ by $$f(V) = \sum_{a_j \in A} f_j(V).$$

It is easy to verify that $f$ is a non-decreasing submodular function. We define the ($d_1$-dimensional) cost of an element  $(i,S_i) \in U$ by
$\bc((i,S_i))=\bc(i,S_i)$, and as in Section~\ref{sec:randomized_mcmb}, the cost of a subset $V\in 2^U$ is $\bc(V)= \sum_{(i,S_i) \in V }
\bc(i,S_i)$. Let $\mM$ denote the collection of all subsets in which there is a single assignment to each bin. Formally,
$$\mM = \{ V \in 2^U | \mbox{ there is no $i$ such that $(i,S_{i_1}),(i,S_{i_2}) \in V$, where $S_{i_1} \neq S_{i_2}$} \}. $$
We consider the following instance of generalized $\SUB$: Maximize $f(V)$ subject to the constraints $V\in \mM$ and $\bc(V) \leq \bL$.

We note that any set $V \in \mM$ can be converted to a solution $\bS$ for $BSAP$ with $\bc(\bS) \leq \bc(V)$ and $p(\bS) = f(V)$. To do so, we
assign each item $a_j$ to a bin $i$ if there exists $(i,S_i) \in V$
 such that $a_j\in S_i$ and $f_j(V) = p_{i,j}$. If no such bin $i$ exists, we
do not assign $a_j$ to any bin. Similarly, any solution $\bS$ for
 $BSAP$ can be converted to a set $V\in \mM$ such that  $\bc(\bS) = \bc(V)$
and $p(\bS) = f(V)$.

Now, we use a technique of~\cite{fgms06} to obtain a fractional solution
 for the generalized ${\SUB}$ instance. Consider the linear program
LP-BSAP over the variables $X_i^{S}$, for all $1\leq i\leq m$ and $S\in \cI_i$.
\begin{figure}
\begin{center}
\[
\begin{array}{llll}
\mbox{(LP-BSAP)}& \mbox{maximize} &
 \displaystyle{\sum_{i=1}^{m}  \sum_{S \in  \cI_i}  X_i^{S} \cdot p(i,S) } \\
 \mbox{subject to:}
  & \forall a_j\in A  & \displaystyle{\sum_{i=1}^{m} \sum_{S\in \cI_i| a_j \in S  } X_i^{S} \leq 1}\\
  &\forall 1\leq i \leq m &\displaystyle{ \sum_{S \in \cI_i} X_i^{S} = 1 } \\
  &\forall 1\leq r \leq d &\displaystyle{ \sum_{i=1}^{m} \sum_{S \in \cI_i} X_i^{S} c_r(i,S) \leq L_r }
\end{array}
\]
\end{center}
\end{figure}
We note that the optimal solution for LP-BSAP is at
least $p(\mO)$, where $\mO$ is an optimal solution for the BSAP
instance.
Since we have $d_2$ linear constraints over the bins, where $d_2 \geq 1$ is some constant,
a feasible solution of value $(1-\eps)$ times the optimal can be found in polynomial time (see \cite{fgms06} for more details). Let $\bX$ be
such a solution, then the value of the solution $\bX$ is at least $(1-\eps) \cdot p(\mO)$.

Let $\cD_i$ be a random variable over $\cI_i$ with $Pr[\cD_i = S] = X_i^S$. Define a random set $\cD =\{(1,\cD_1),(2,\cD_2),\ldots,(m,\cD_m)\}$,
and let $\chi$ be the distribution of $\cD$. In~\cite{fgms06} it is shown that $E[f(\cD)]$ is at least $(1-e^{-1})$ times the value of the
solution $\bX$; thus, $E[f(\cD)] \geq (1-e^{-1})\cdot (1-\eps) \cdot p(\mO)$.
It is easy to verify that the conditions of  Theorem~\ref{thm:prb_claim} are satisfied for the distribution $\chi$.
We define $\cD'=\cD$ if $\cD$ is $\eps$-nearly feasible, and $\cD'=\emptyset$ otherwise. Then, by Theorem~\ref{thm:prb_claim}, $\cD'$ is always
$\eps$-nearly feasible and $E[f(\cD')] \geq (1-e^{-1})(1-O(\eps))p(\mO)$. As before, $\cD'$ can be converted to a solution $\bS$ for the BSAP
instance, such that $p(\bS)= f(\cD')$, and $\bc(\bS) \leq \bc(\cD')$. We summarize the above steps in the following.
\\
\noindent
{\bf Approximation Algorithm for Small Assignments BSAP Instances}
\begin{enumerate}
\item Find a $(1-\eps)$-approximate solution $\bX$ for LP-BSAP.
\item For any bin $1 \leq i \leq m$,  select an assignment $\cD_i =S_i$ with probability $X_i^{S_i}$
and define $\cD = \{(1,\cD_1),\ldots,(m,\cD_m)\}$
\item If $\cD$ is $\eps$-nearly feasible return $\cD$ as the solution,
else return an empty assignment.
\end{enumerate}
\begin{lemma}
\label{thm:SABSAP} The above Approximation Algorithm for Small Assignments
 BSAP Instances outputs in polynomial time an $\eps$-nearly feasible
solution with  expected profit at least $(1-e^{-1})(1-O(\eps))p(\mO)$, where $\mO$ is an optimal solution.
\end{lemma}

\subsection{Reduction to Small Assignments BSAP Instances}

We now describe an algorithm which reduces a general BSAP instance
to a small assignments instance. We use this reduction to obtain an
augmentation algorithm for general instances, by applying Algorithm for Small Assignment Instances of
Section \ref{sec:SABSAP}.

Let $\mO= (S_1,\ldots,S_m)$ be an optimal solution for an instance of BSAP. We say that the profit of bin $i$ (with respect to $\mO$) is
$p^{\mO} (i)= p(i,S_i)$, and the cost of bin $i$ is $\bc^{\mO}(i) = \bc(i,S_i)$. The first step in our algorithm is to guess the set $T$ of
$h=d_1\cdot \eps^{-4} $ most profitable bins in the solution $\mO$.
 We then guess the cost of any bin $i \in T$ in each dimension $1 \leq r \leq d_1$.
with
accuracy $\delta= \eps \cdot h^{-1}$.
That is,
for each bin $i\in T$ we guess a $d_1$-dimensional vector of integers
$\bk_i=(k_{i,1},\ldots,k_{i,d_1})$ such that, for any $1\leq r \leq d_1$,
\begin{equation}
\label{eqn:reduction_guess}
k_{i,r} \cdot \delta \cdot L_r \leq c^{\mO}(i) \leq (k_{i,r}+1) \cdot  \delta \cdot L_r
\end{equation}
As the number of values $k_{i,r}$ can get is at most  $\lceil \delta^{-1} \rceil$, we can
go over all possible cost vectors in polynomial time, for some constant $\eps > 0$.

We use our guess of the set $T$ and the vectors $\bk_i$ to define a residual instance of BSAP. The ground set of elements is $A$, and there are
$m$ bins. We define the  budget of the residual instance, $\bL'$, to be $L'_r = L_r (1-\sum_{i\in T} k_{i,r} \delta )$, for $1 \leq r \leq d_1$.
For any $i\notin T$, the feasible set of assignments for bin $i$ is $\cI'_i= \{ S~|~ S\in \cI_i,~ \bc(i,S_i)\leq \eps^3 \bL' \}$, and for any
$i\in T$ the feasible set is $\cI'_i = \{ S |~ S \in \cI_i, \bc(i,S_i)  \leq (k_{i,r}+1)\cdot \delta \cdot  L_r \}$. In both cases, the set of
feasible assignments for bin $i$ is defined by $d_1+d_2$ linear constraints. The new cost of $a_j$ when assigned to bin $i$ is $\bc'_{i,j} =
\bc_{i,j}$ if $i \notin T$ and $\bc'_{i,j} = 0$ if $i\in T$. The new profit of $a_j$ when assigned to bin $i$ is $p'_{i,j}= p_{i,j}$.

A crucial property of the residual instance is that it is a small assignments instance.
 For any bin
$i\in T$ and $S_i \in \cI'_i$ it holds that $\bc'(i,S_i) = 0$. For any bin $i\notin T$ and $S_i \in \cI'_i$,
by the definition of $\cI'_i$ it holds that $\bc'(i,S_i)= \bc(i,S_i)\leq \eps^3 \bL'$.
The relation between the solutions of the residual and original
problems is stated in the next two lemmas.

\begin{lemma}
\label{thm:reduction_lemma1}
Let $\bS=(S_1,\ldots,S_m)$ be an $\eps$-nearly feasible  solution for the residual problem with respect
to a set $T$ (of size at most $h$) and a collection of vectors $\bk_i$. Then $\bS$ is a
$(2\eps)$-nearly feasible  solution for the original problem with $p(\bS)=p'(\bS)$.
\end{lemma}

\begin{proof}
Clearly, $\bS$ is a solution for the original problem since, for every bin $i$, it holds that $\cI'_i \subseteq \cI_i$. Also,
for any bin $i$ and $a_j\in A$ it holds that $p_{i,j}=p'_{i,j}$. Hence, $p(\bS)=p'(\bS)$.
As for the cost, for any $1\leq r \leq d_1$, we have
\begin{eqnarray*}
c_r(\bS) &=& \sum_{i\in T} c_r(i,S_i) + \sum_{i\notin T} c_r'(i,S_i) \\
&\leq& \sum_{i \in T} (k_{i,r}+1)\cdot \delta L_r +\sum_{i\notin T} c_r'(i,S_i) \\
&\leq& \abs{T} \cdot \delta  L_r + \sum_{i \in T} k_{i,r}\cdot \delta  L_r
               +\sum_{1\leq i\leq m} c_r'(i,S_i) \\
&\leq& h \cdot \delta   L_r + \sum_{i \in T} k_{i,r}\cdot \delta  L_r
               +(1+\eps) L'_r \\
& \leq &
\eps \cdot L_r + L_r + \eps L'_r  \leq (1+2\eps) L_r
\end{eqnarray*}
\end{proof}
\begin{lemma}
\label{thm:reduction_lemma2} Let $T$ be the set of $h$ most profitable bins in an optimal
 solution $\mO$, and let $\bk_i$ be the vector of cost
guesses for which \eqref{eqn:reduction_guess} holds. Then there is a feasible
 solution $\bS'$ for the residual  problem with respect to $T$ and
$\bk_i$ whose profit is $p'(\bS')\geq (1-\eps)p(\mO)$.
\end{lemma}

\begin{proof}
Let $\mO=(S_1,\ldots,S_m)$ be an optimal solution for the original problem. We define a
solution $\bS'=(S'_1,\ldots,S'_m)$ for the residual problem by $S'_i=S_i$ if  $S_i \in \cI'_i$ and
$S'_i=\emptyset$ otherwise. Clearly, $\bS'$ is a solution for the residual problem.
For any dimension $1\leq r \leq d_1$,
$$ L_r \geq c_r^{\mO} = \sum_{i \notin T} c_r(i,S_i)  + \sum_{i \in T} c_r(i,S_i) \geq
\sum_{i \notin T} c_r(i,S_i)  + \sum_{i \in T} k_{i,r} \cdot \delta \cdot L_r = \sum_{i \notin T} c_r(i,S_i) + (L_r-L'_r)$$ This implies that
$\sum_{i \notin T} c_r(i,S_i) \leq L'_r$.  Now, for any $i\in T$ it holds that $c'_r(i,S'_i) = 0$, and for any $i$ it holds that $c'_r(i,S'_i)
\leq c_r(i,S_i)$. Thus, we have that
\begin{eqnarray}
c_r'(\bS') & =& \sum_{i\in T} c'_r(i,S_i) + \sum_{i \notin T} c'_r(i,S_i) \nonumber \\
 & \leq &  \sum_{i \notin T} c_r(i,S_i) \leq L'_r,
 \label{eq:cost_not_T}
\end{eqnarray}
i.e., $\bS'$ is a feasible solution for the residual problem.

Let $H$ be the set of bins for which $S_i \neq S'_i$. Since
 \eqref{eqn:reduction_guess} holds for the vectors $\bk_i$, clearly, for any $i\in
T$, we have that $S'_i = S_i$, and thus $H \cap T =\emptyset$.
 By (\ref{eq:cost_not_T}), the total cost of bins not in $T$ is bounded by $\bL'$.
Hence, for all of them except for at most $d_1 \cdot \eps^{-3} $, it holds that $c_r(i) \leq \eps^3 L'_r$, for
 $1 \leq r \leq d_1$. In other words, for
every $i\notin T$, except at most $d_1 \cdot \eps^{-3} $,
 it holds that $S_i \in \cI'_i$ and$S_i = S'_i$.
It follows that $\abs{H} \leq d_1 \cdot \eps^{-3}$, and that none of the bins in $T$ is in $H$.
 The profit of any bin that is not in $T$ is
smaller than $p(\mO)/ \abs{T}$ (since $T$ is the set of most profitable bins). Therefore,
$$\sum_{i\in H} p^\mO(i) \leq \frac{d_1 \cdot \eps^{-3}\cdot  p(\mO)}{\abs{T}} =
  p(\mO) \cdot \frac{d_1 \cdot \eps^{-3}} {h} =\eps \cdot p(\mO).$$
It follows that
$$p'(\bS')=p(\bS')= \sum_{i=1}^{m} p(i,S'_i) = \sum_{i=1}^{m} p(i,S_i) - \sum_{i\in H} p(i,S_i)
= p(\mO) - p(H) \geq (1-\eps)\cdot p(\mO).$$
\end{proof}

We summarize with the following algorithm.\\

\noindent
{\bf Nearly Feasible Algorithm for BSAP}
\begin{enumerate}
\item Enumerate over all subsets $T$,  $\abs{T}\leq h = d_1 \cdot \eps^{-4}$, and
cost vectors $\bk_i$ for any $i\in T$.
\begin{enumerate}
\item Define a residual instance with respect to $T$ and $\bk_i$,
 and run Algorithm for Small Assignments BSAP on the residual instance.
Consider the resulting solution as a solution for the original problem.
\end{enumerate}
\item Return the best solution found.
\end{enumerate}

\begin{theorem}
\label{thm:reduction_theorem}
Let $\mO$ be an optimal solution for a BSAP instance. Then
the Nearly Feasible Algorithm for BSAP returns in polynomial time a $(2\eps)$-nearly
feasible solution $\bS$,
with expected profit at least  $E[p(\bS)] \geq (1-e^{-1})(1-O(\eps))p(\mO)$.
\end{theorem}
\begin{proof}
As the number of possibilities for $T$ and the vectors $\bk_i$
is polynomial for fixed values of $d_1$,$d_2$ and $\eps$, and Algorithm for Small
Assignments BSAP is polynomial as well, we get that the Nearly Feasible Algorithm  for BSAP runs in polynomial time.

Since the algorithm for Small Assignments BSAP always returns an
 $\eps$-nearly feasible solution
(for the residual problem),
by Lemma \ref{thm:reduction_lemma1}, we get that the solutions output
 by the algorithm
are always $(2\eps)$-nearly feasible for the original problem. Finally,
 in the iteration where the set $T$ and the vectors $\bk_i$ satisfy the
conditions of Lemma \ref{thm:reduction_lemma2}, the optimal solution for
 the residual problem has profit at least $(1-\eps)p(\mO)$. Thus, by Lemma \ref{thm:SABSAP}, the
expected profit of the solution returned by Algorithm for Small Assignments BSAP is at least
$(1-e^{-1})(1-O(\eps))(1-\eps)p(\mO)=(1-e^{-1})(1-O(\eps))p(\mO)$.

As the expected profit of the solution returned  by the algorithm is at least the
expected profit in any iteration, this yields the statement of the theorem.
\end{proof}

\subsection{Small Items BSAP Instances}

We now consider inputs in which all items are small. For such inputs we can
 fix a nearly feasible solution. The proof of the next result is similar to the proof of
Lemma \ref{lemma:nearly_fix} (details omitted).


\begin{lemma}
\label{lemma:nearly_fix_bsap}
Let $\bS=(S_1,\ldots,S_m)$ be an $\eps$-nearly feasible solution
for a small items instance of BSAP. Then $\bS$ can be converted to a feasible
solution $S'$ such that $p(S') \geq (1- O(\eps) ) p(S)$.
\end{lemma}

Lemma \ref{lemma:nearly_fix_bsap} can be easily coupled with the Nearly Feasible Algorithm
to obtain an approximation algorithm for small items instances of BSAP, which always returns a feasible
solution.
\\
\\
\noindent
{\bf Algorithm for Small Items BSAP}
\begin{enumerate}
\item Run the Nearly Feasible Algorithm for BSAP. Let $\bS$ be the resulting solution.
\item Convert $\bS$ to a feasible solution $\bS'$ (as in
Lemma \ref{lemma:nearly_fix_bsap}) and return $\bS'$.
\end{enumerate}
The properties of the algorithm follow immediately from Lemmas \ref{thm:reduction_theorem}
 and \ref{lemma:nearly_fix_bsap}.

\begin{lemma}
Algorithm for Small Items BSAP returns a feasible solution for the
 problem with expected profit of at least $(1-e^{-1})(1-O(\eps))p(\mO)$, where
$\mO$ is an optimal solution.
\end{lemma}

\subsection{General Inputs}
We now use Algorithm for Small Items BSAP to obtain $(1-e^{-1} - \eps)$-approximation for general inputs. Given an input for BSAP, let $\mO=
(S_1,\ldots,S_m)$ be an optimal solution. We say that the profit of an element $a_j$ in $\mO$ is zero if it is not assigned to any bin, and
$p_{i,j}$ if it is assigned to bin $i$.

Let $T^*=(T^*_1,\ldots,T^*_m)$ be a feasible assignment of elements to bins. Given $T^*$, we define a residual instance of the problem with
respect to $T^*$. The new budget is $\bL^*=\bL - \bc(T^*)$, the new set of elements $A^*$ is the collection of all elements in $A$ which are not
assigned in $T^*$ to any bin. Consider the assignment of  $a_j$ to bin $i$.
 If  $\bc_{i,j} \leq \eps^3 \bL^*$, then $p^*_{i,j}=p_{i,j}$ and
$\bc^*_{i,j}=\bc_{i,j}$; otherwise, set $p^*_{i,j}=0$ and $\bc^*_{i,j}=0$.
 The size remains $\bs^*_{i,j}=\bs_{i,j}$. The new capacity of bin $i$
is $\bb^*_i= \bb_i -\sum_{a_j \in T^*_i} \bs_{i,j}$.

Clearly, the residual instance with respect to $T^*$ is
small items instance. It is easy to verify that,
 given a feasible assignment for the residual
problem with respect to $T^*$ of profit $v$, we can derive a feasible assignment for the original problem of profit $p(T^*) + v$. Now, consider
$T^*$ to be the assignment of the $h=d_1\cdot \eps^{-4}$ most profitable elements in $\mO$. Then there is a solution for the residual problem of
value $p(\mO)- (1+\eps) p(T^*)$ (the proof is similar to the proof of Lemma
 \ref{thm:reduction_lemma2}). Thus, if we guessed $T^*$ correctly,
Algorithm for Small Items BSAP can be used to obtain a solution with expected  profit at least $(1-e^{-1}-O(\eps))
 (p(\mO)- (1+\eps) f(T^*))$ for the residual
problem, from which we can derive a solution for the original instance
of profit $(1-e^{-1}-O(\eps)) (p(\mO)- (1+\eps) p(T)) +p(T) \geq (1-e^{-1} - O(\eps) ) p(\mO)$.

WE summarize with an algorithm for general inputs.
\\
\\
\noindent {\bf Approximation Algorithm for BSAP}
\begin{enumerate}
\item For any feasible assignment $T^*$ of at most $h=d \cdot \eps^{-4}$ elements:
\begin{enumerate}
\item Find a $(1-e^{-1} -O(\eps))$-approximate solution $\bS^*$ for
the residual problem with respect to $T^*$.
\item Derive from $\bS^*$ a solution for the original problem
of profit $p(\bS^*)+p(T^*)$.
\end{enumerate}
\item
Return  the solution of maximal profit found.
\end{enumerate}

\begin{theorem}
\label{thm:alg_bgap}
There is a polynomial time randomized $(1-e^{-1} - \eps)$-approximation algorithm for BSAP,
for any fixed $\eps > 0$.
\end{theorem}
}
\section{Discussion}

In this paper we established a strong relation between the
continuous relaxation of $\SUB$ and the discrete problem. This
relation is nearly optimal and suggests that future research
should focus on deriving better approximation ratios for the
continuous problem.

The question whether better rounding exists remains open; namely, is it
 possible to obtain an $\alpha-$approximation algorithm for $\SUB$, given
an $\alpha<1$ approximation algorithm for the continuous problem?
And more specifically, is there a polynomial time $(1-e^{-1})-$approximation
for $\SUB$ with monotone objective function?

Finally, the running times of our algorithms are exponential in $1/\eps$, thus rendering them impractical.
 Yet, the hardness results for $d$-dimensional Knapsack (see, e.g., \cite{KPP04,MC84,KS10}),
    a special case of $\SUB$,
  hint that significant improvements over these running times may be impossible.







\bibliographystyle{abbrv}
\bibliography{bgap}

\appendix

\section{Basic Properties of Submodular Functions}
\label{app:basic_props}
In this section we give some simple properties of submodular functions.
Recall that $f : 2^U\rightarrow \mathbb{R}$ is a submodular function
if $f(S) + f(T) \geq f(S \cup T) + f(T \cap S)$ for any $S,T \subseteq U$.
We define $f_T(S) = f(S\cup T) - f(T)$.
\begin{lemma}
\label{lemma:submodular_summation}
Let $f: 2^U\rightarrow \mathbb{R}$ be a submodular function with $f(\emptyset)\geq 0$,
and let $S= S_1 \cup S_2 \cup \ldots \cup S_k$, where $S_i$ are disjoint sets.
Then $$f(S) \geq f(S_1) +f(S_2)+\ldots f(S_k).$$
\end{lemma}
\begin{proof}
By induction on $k$. For $k=2$, since $f$ is a submodular function, we have that
$$f(S_1) + f(S_2) \geq f(S_1 \cup S_2) + f(S_1 \cap S_2)= f(S) +f(\emptyset),$$
and since $f(\emptyset) \geq 0$, we get that $f(S) \leq f(S_1) + f(S_2)$.

For $k>2$, using the induction hypothesis twice, we have
$$f(S)\leq f(S_1)+f(S_2)+\ldots f(S_{k-2})  + f(S_{k-1}\cup S_{k}) \leq
f(S_1) +f(S_2)+\ldots f(S_k).$$
\end{proof}
\begin{lemma}
\label{lemma:submodular_decreasing}
Let $f: 2^U\rightarrow \mathbb{R}_+$ be a submodular function, and let $S,T_1,T_2 \subseteq U$
such that $T_1 \subseteq T_2$ and $S \cap T_2 =\emptyset$. Then,
$f_{T_2}(S) \leq f_{T_1}(S)$.
\end{lemma}
\begin{proof}
Since $f$ is submodular,
$$f(S\cup T_1) +f(T_2) \geq f(S\cup T_1 \cup T_2) + f( (S \cup T_1)\cap T_2) =
f(S \cup T_2) +f(T_1).$$
Hence, $f_{T_2}(S) \leq f_{T_1}(S)$.
\end{proof}
\begin{lemma}
\label{lemma:submodular_summation_inequality}
Let $f: 2^U\rightarrow \mathbb{R}_+$ be a submodular function, and let
$S= S_1 \cup S_2 \cup \ldots \cup S_k$,
where $S_i$ are disjoint sets. Then,
$$f(S) \geq \sum_{i=1}^{k} f_{S\setminus S_i} (S_i).$$
\end{lemma}
\begin{proof}
We note that
$$f(S) = \sum_{i=1}^{k} f_{S_1 \cup \ldots \cup S_{i-1}} (S_i).$$
By Lemma~\ref{lemma:submodular_decreasing}, for each $i >1$,
$f_{S_1 \cup \ldots \cup S_{i-1}} (S_i) \geq  f_{S\setminus S_i} (S_i)$.
Hence,
$$f(S) \geq \sum_{i=1}^{k} f_{S\setminus S_i} (S_i).$$
\end{proof}

\comment{
\section{General Inputs for BSAP}
\label{sec:bgap_general}
In this section we use the algorithm for small items BSAP to obtain
$(1-e^{-1} - \eps)$-approximation for arbitrary inputs.
Given an input for BSAP, let $\mO= (S_1,\ldots,S_m)$ be an optimal
solution.


Let $T^*=(T^*_1,\ldots,T^*_m)$ be a feasible assignment of elements to bins.
We define a residual instance with respect to $T^*$.
The new budget is $\bL^*=\bL - \bc(T^*)$,
the new set of elements $A^*$ is the collection of all elements in $A$
which were not assigned in $T^*$ to any bin.
Consider the assignment of  $a_j$ to bin $i$.
If  $\bc_{i,j} \leq \eps^3 \bL'$, then $p^*_{i,j}=p_{i,j}$
and $\bc^*_{i,j}=\bc_{i,j}$. Otherwise, set $p^*_{i,j}=0$ and
$\bc^*_{i,j}=0$. The size remains $\bs^*_{i,j}=\bs_{i,j}$. The new capacity of bin
$i$ is $\bb^*_i= \bb_i -\sum_{a_j \in T^*_i} \bs_{i,j}$.

The residual instance with respect to $T^*$ is small items
instance. Any feasible assignment to the residual problem (with
respect to $T^*$) of profit $v$ can be converted to a feasible
assignment to original problem of profit $p(T^*)+v$.
Now, consider  $T^*$ to be the assignment of the $h=d_1\cdot \eps^{-4}$
most profitable elements in an optimal solution $\mO$
(the profit of $a_j$ in $\mO$ is $p_{i,j}$ if it was assigned to bin $i$ and zero otherwise).
 Then there is a solution
to the residual problem of value $p(\mO)- (1+\eps) f(T^*)$
(the proof is similar to the proof of Lemma \ref{thm:reduction_lemma2}).
Thus, if we guessed $T^*$ correctly, the algorithm for small items BSAP can be used to
obtain a solution with expected  profit of at least $(1-e^{-1}-O(\eps))
 (p(\mO)- (1+\eps) f(T^*))$ for the residual
problem, which we can covert to a solution for the original instance
of profit at least $(1-e^{-1} - O(\eps) ) p(\mO)$.
And we get the following algorithm.


\noindent
{\bf Approximation algorithm for BSAP}
\begin{enumerate}
\item For any feasible assignment $T^*$ of at most $h=d \cdot \eps^{-4}$ elements:
\begin{enumerate}
\item Find a $(1-e^{-1} -O(\eps))$-approximate solution $\bS^*$ for
the residual problem with respect to $T^*$.
\item Derive from $\bS^*$ a solution for the original problem
of profit $p(\bS^*)+p(T^*)$.
\end{enumerate}
\item
Return  the solution of maximal profit found.
\end{enumerate}

\begin{theorem}
There is a polynomial time randomized $(1-e^{-1} -
\eps)$-approximation algorithm for BSAP, for any fixed $\eps>0$.
\end{theorem}


\section{Some Proofs}
\label{app:proofs}


\begin{dl_claim_proof}{\ref{lemma:prob_F}}
For any dimension $ 1 \leq r \leq d$,
it holds that $E[ c_r(D) ] = \sum_{k=1}^{m} E[c_r(D_k)] \leq L_r$.
Define $V_r=\{k | c_r(D_k) \mbox{~is not fixed} \}$. Then,
$$Var[c_r(D)] = \sum_{k=1}^{m} Var[c_r(D_k)] \leq \sum_{k\in V_r} E[c_r^2(D_k)] \leq
\sum_{k\in V_r} E [c_r(D_k)] \cdot \eps^3 L_r \leq  \eps^3 L_r \sum_{k=1}^{m} E [c_r(D_k)] \leq
\eps^3 L^2_r.$$
The first inequality holds since $Var[X] \leq E[X^2]$, and the second
inequality follows from the fact that $c_r(D_k) \leq \eps^3 L_r$ for
$k\in V_r$.
Recall that, by the Chebyshev-Cantelli inequality, for any $t > 0$ and a random
variable $Z$,
$$Pr\left[Z- E[Z] \geq t\right] \leq \frac{Var[Z]}{Var[Z]+ t^2}.$$
Thus,
\begin{eqnarray*}
Pr\left[c_r(D) \geq (1+\eps) L_r\right] &=&
Pr\left[ c_r(D) - E[c_r(D)] \geq (1+\eps) L_r -E[c_r(D)] \right]  \\
&\leq& Pr \left[ c_r(D) - E[c_r(D)] \geq \eps \cdot  L_r \right]
\leq \frac{ \eps^3 L^2_r}{\eps^2 L^2_r} = \eps.
\end{eqnarray*}
By the union bound, we have that
$$ Pr[F=0] \leq \sum_{r=1}^{d} Pr[c_r(D) \geq (1+\eps) L_r] \leq
d \eps.$$
\end{dl_claim_proof}




\begin{dl_claim_proof}{\ref{lemma:prob_l}}
By the Chebyshev-Cantelli inequality we have that, for any dimension
$1 \leq r \leq d$,
\begin{eqnarray*}
Pr[R_r > \ell ] &=&
Pr[c_r(D) > \ell \cdot \lr]
 \\
&\leq&
Pr \left[ c_r(D) -E[c_r(D)]  > (\ell-1) \lr \right] \leq \\
&\leq &
\frac{\eps^3 \lr^2}{(\ell-1)^2 \lr^2} \leq
\frac{\eps^3}{(\ell-1)^2},
\end{eqnarray*}
and by the union bound, we get that
$$Pr[R > \ell] \leq \frac{d\eps^3}{(\ell-1)^2}.$$
\end{dl_claim_proof}

\begin{dl_claim_proof}{\ref{lemma:bounding_via_R}}
The set $D$ can be partitioned to $2 d \ell$ sets
$D_1, \ldots D_{2 d \ell}$ such that each of these
sets is a feasible solution. Hence, $f(D_i) \leq \mO$.
Thus by lemma~\ref{lemma:submodular_summation}, $f(D) \leq f(D_1) +\ldots +f(D_{2 d \ell})
\leq 2 d \ell f(\mO)$.
\end{dl_claim_proof}



\begin{dl_claim_proof}{\ref{lemma:mcmb_nearly_feasible}}
By Claims \ref{lemma:prob_F} and \ref{lemma:prob_l}, we have that
\begin{eqnarray*}
E[f(D)]    &=&
E\left[f(D) | F=1\right] \cdot \Prb{F=1} +
 E\left[f(D) | F=0 \land R < 2 \right]\cdot \Prb{F=0 \land (R   < 2)} \\
   &+& \sum_{\ell=1}^{\infty}  E\left[f(D)|  F=0 \land (2 ^\ell \leq R \leq 2^{\ell+1})\right]
        \cdot
         \Prb{F=0 \land (2 ^\ell \leq R \leq 2^{\ell+1})} \\
 &\leq& E[f(D) | F=1] \cdot \Prb{F=1} + 4d^2 \eps \cdot \mO
+ ~d^2 \eps ^3 \cdot \mO \cdot \sum_{\ell=1}^{\infty}  \frac{2^{\ell+2}}
      { (2^{\ell-1})^2 }. \\
 \end{eqnarray*}
Since the last summation is a constant, and $E[f(D)]\geq \mO/2$, we have that
 $$E[F(D)] \leq E[f(D) | F=1 ] \Prb{F=1} + \eps \cdot c \cdot E[F(D)],$$
where $c>0$ is some constant. It follows that
 $$(1-O(\eps)) E[f(D)] \leq E[f(D) | F=1 ] \Prb{F=1}.$$
Finally, since $D' = D$ if $F=1$ and $D' =0$ otherwise, we have that
$$E[f(D')] = E[f(D)|F=1] \cdot \Prb{F=1} \geq (1- O(\eps)) E[f(D)]. $$
 \end{dl_claim_proof}



\begin{dl_proof}{\ref{lemma:derand1}}
Let $U' = \{ i~ |~ 0 < x_i < 1\}$ be the set of all fractional entries.
We define a new cost function $\bc'$ over the elements in $U$.

\[
\begin{array}{lcr}
c'_r(i) &=& \left\{
\begin{array}{lcl}
c_r(i) &~~~~ &i\notin U' \\
0 & &c_r(i) \leq \frac{\eps \cdot L_r}{2k}\\
\frac{\eps \cdot L_r}{2k} (1+\eps/2)^j & &
\frac{\eps \cdot L_r}{2k} (1+\eps/2)^j \leq c_r(i)<\frac{\eps \cdot L_r}{2k} (1+\eps/2)^{j+1}
\end{array}
\right.
\end{array}
\]

Note that for any $i\in U'$, $\bc'(i)\leq \bc(i)$, and
$$c_r(i)\leq (1+\eps /2) c'_r(i) + \frac{\eps \cdot L_r}{2k},$$
for all $1 \leq r \leq d$.
The number of different values $c'_r(i)$ can get for $i\in U'$
is bounded by $\frac{8 \ln (2k) }{\eps}$ (since all elements are
small, and $\ln(1+x) \geq x/2$). Hence the number of different
values $\bc'(i)$ can get for $i\in U'$ is bounded by
$k'= \left( \frac{8 \ln (2k) }{\eps} \right) ^d$.

We start with $\bx'=\bx$, and while there are $i,j \in U'$ such that $x'_i$ and $x'_j$ are both
fractional and $\bc'(i)=\bc'(j)$, define $\delta^{+} = \delta_{\bx',i,j}^{+}$
and $\delta^{-}=\delta_{\bx',i,j}^{-}$.
Since $i$ and $j$ have the same cost (by $\bc'$), it holds that
$\bc' \left( \bx_{i,j}(\delta^{+}) \right) =
\bc' \left( \bx_{i,j}(\delta^{-}) \right) =\bc'(\bx)$.
If $F^{\bx}_{i,j}(\delta^{+})\geq F(\bx)$,
then set $\bx''= \bx_{i,j}(\delta^{+})$, otherwise $\bx''= \bx_{i,j}(\delta^{-})$.
In both cases $F(\bx'')\geq F(\bx')$ and $\bc'(\bx'')= \bc'(\bx')$. Now, repeat
the step with $\bx'=\bx''$.
Since in each iteration the number of fractional entries in $\bx'$ decreases,
 the process will terminate (after at most $k$ iterations)
with a vector $\bx'$ such that $F(\bx')\geq F(\bx)$,
$\bc'(\bx')=\bc'(\bx)\leq \bL$
and there are no two elements $i,j\in U'$ with $\bc'(i)=\bc'(j)$ where
$x'_i$ and $x'_j$ are both fractional. Also, for any $i\notin U'$, the entry
$x'_i$ is integral (since $x_i$ was integral and the entry was not modified by
the process). Thus, the number of fractional entries in $\bx'$ is
at most $k'$.
Now, for any dimension $1 \leq r \leq d$,
\begin{eqnarray*}
c_r(\bx')&=& \sum_{i\notin U'} x'_i c_r(i) + \sum_{i\in U'} x'_i c_r(i) \\
&\leq&
(1+\eps/2) \cdot \sum_{i\notin U'} x'_i  \cdot c'_r(i) +
   \sum_{i\in U'} x'_i \left( (1+\eps/2) c'_r(i) + \frac{\eps \cdot L_r}{2k} \right)\\
&=&
(1+\eps/2) \cdot \sum_{i\in U} x'_i  \cdot c'_r(i) +
   \sum_{i\in U'} x_i \frac{\eps \cdot L_r}{2k} \leq (1+\eps) L_r.
\end{eqnarray*}
This completes the proof.
\end{dl_proof}

\begin{dl_proof}{\ref{thm:reduction_lemma1}}
Clearly, $\bS$ is a solution for the original problem since for every bin $i$ it holds that $\cI'_i \subseteq \cI_i$.
For any bin $i$ and $a_j\in A$ it holds that $p_{i,j}=p'_{i,j}$, then clearly $p(\bS)=p'(\bS)$.
As for the cost, for any $1\leq r \leq d_1$:
\begin{eqnarray*}
c_r(\bS) &=& \sum_{i\in T} c_r(i,S_i) + \sum_{i\notin T} c_r'(i,S_i) \\
&\leq& \sum_{i \in T} (k_{i,r}+1)\cdot \delta L_r +\sum_{i\notin T} c_r'(i,S_i) \\
&\leq& \abs{T} \cdot \delta  L_r + \sum_{i \in T} k_{i,r}\cdot \delta  L_r
               +\sum_{1\leq i\leq m} c_r'(i,S_i) \\
&\leq& h \cdot \delta   L_r + \sum_{i \in T} k_{i,r}\cdot \delta  L_r
               +(1+\eps) L'_r \\
&=& \eps \cdot L_r + L_r + \eps L'_r  \leq (1+2\eps) L_r
\end{eqnarray*}
\end{dl_proof}

\begin{dl_proof}{\ref{thm:reduction_lemma2}}
Let $\mO=(S_1,\ldots,S_m)$ be an optimal solution for the original problem. We define a
solution $\bS'=(S'_1,\ldots,S'_m)$ for the residual problem by $S'_i=S_i$ if  $S_i \in \cI'_i$ and
$S'_i=\emptyset$ otherwise. Clearly, $\bS'$ is a solution for the residual problem.
For any dimension $1\leq r \leq d_1$,
$$ L_r \geq c_r^{\mO} = \sum_{i \notin T} c_r(i,S_i)  + \sum_{i \in T} c_r(i,S_i) \geq
\sum_{i \notin T} c_r(i,S_i)  + \sum_{i \in T} k_{i,r} \cdot \delta \cdot L_r = \sum_{i \notin T} c_r(i,S_i) + (L_r-L'_r)$$
This implies that $\sum_{i \notin T} c_r(i,S_i) \leq L'_r$.  Now, for any $i\in T$ it holds that $c'_r(i,S'_i) = 0$, and
for any $i$ it holds that $c'_r(i,S'_i) \leq c_r(i,S_i)$. Thus, we conclude that
$$ c_r'(\bS') = \sum_{i\in T} c'_r(i,S_i) + \sum_{i \notin T} c'_r(i,S_i) \leq  \sum_{i \notin T} c_r(i,S_i) \leq L'_r,$$
i.e., $\bS'$ is a feasible solution.

Let $H$ be the set of bins for which $S_i \neq S'_i$.
Since  \eqref{eqn:reduction_guess} holds for the vectors $\bk_i$,
it is clear that for any $i\in T$, $S'_i = S_i$, and thus $H \cap T =\emptyset$.
The total cost of bins not in $T$ is bounded by $\bL'$ (since $\sum_{i \notin T} c_r(i,S_i) \leq L'_r$).
Hence, for all of them except at most $d_1 \cdot \eps^{-3} $ it holds that $c_r(i) \leq \eps^3 L'_r$,
for some $1 \leq r \leq d_1$.
This means that for every $i\notin T$ except at most $d_1 \cdot \eps^{-3} $
 it holds that $S_i \in \cI'_i$ and$S_i = S'_i$.
From this, we conclude that $\abs{H} \leq d_1 \cdot \eps^{-3}$, and that none of the bins in $T$ is
in $H$. The profit of any bin which is not in $T$ is smaller
than $p(\mO)/ \abs{T}$ (since $T$ is the set of most profitable bins).
Thus,
$$\sum_{i\in H} p^\mO(i) \leq d_1 \cdot \eps^{-3}\cdot  p(\mO)/ \abs{T} =
  p(\mO) \cdot \frac{d_1 \cdot \eps^{-3}} {h} =\eps \cdot p(\mO)$$.
Thus,
$$p'(\bS')=p(\bS')= \sum_{i=1}^{m} p(i,S'_i) = \sum_{i=1}^{m} p(i,S_i) - \sum_{i\in H} p(i,S_i)
= p(\mO) - p(H) \geq (1-\eps)\cdot p(\mO)$$
\end{dl_proof}

\begin{dl_proof}{\ref{thm:reduction_theorem}}
The properties of the algorithm for small assignments BSAP are
described in Lemma \ref{thm:SABSAP}. As the number of
possibilities for $T$ and the vectors $\bk_i$ is polynomial for
fixed values of $d_1$,$d_2$ and $\eps$, and the algorithm for
Small Assignments BSAP is polynomial as well, we get that the
Nearly Feasible Algorithm  for BSAP runs in polynomial time.

Since the algorithm for Small Assignments BSAP always returns an
 $\eps$-nearly feasible solution
(for the residual problem),
by Lemma \ref{thm:reduction_lemma1}, we get that the solutions output
 by the algorithm
are always $(2\eps)$-nearly feasible for the original problem.
Finally, in the iteration for which the set $T$ and vectors $\bk_i$ satisfy the
requirements of Lemma \ref{thm:reduction_lemma2}, the optimal solution for the
residual problem is of profit at least $(1-\eps)p(\mO)$.
Thus, the expected profit of the solution returned by the small assignments algorithm is
at least $(1-e^{-1})(1-O(\eps))(1-\eps)p(\mO)=(1-e^{-1})(1-O(\eps))p(\mO)$.

As the expected profit of the solution returned  by the algorithm is at least the
expected profit at any iteration, we get that the expected profit
of the solution returned by the algorithm is $(1-e^{-1})(1-O(\eps))p(\mO)$.
\end{dl_proof}
}
\end{document}
