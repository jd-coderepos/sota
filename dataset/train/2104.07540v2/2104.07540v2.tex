\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{emnlp2021}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage[letterspace=-26]{microtype}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{arydshln}
\usepackage{booktabs,colortbl}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{dashbox}\usepackage{textcomp}
\usepackage{tcolorbox}
\usepackage{adjustbox}
\usepackage{lipsum}
\usepackage[inline]{enumitem}
\usepackage{menukeys}
\usepackage{pmboxdraw}
\usepackage{newunicodechar}
\usepackage{graphicx}
\usepackage[shortcuts]{extdash}

\pgfplotsset{compat=1.13}

\definecolor{c0}{cmyk}{1,0.3968,0,0.2588} 
\definecolor{c1}{cmyk}{0,0.6175,0.8848,0.1490} 
\definecolor{c2}{cmyk}{0.3081,0,0.7209,0.3255}
\definecolor{c3}{cmyk}{0.1127,0.6690,0,0.4431} 
\definecolor{c4}{cmyk}{0.6765,0.2017,0,0.0667} 
\definecolor{c5}{cmyk}{0,0.8765,0.7099,0.3647} 

\definecolor{darkgrey}{RGB}{149,149,149}
\definecolor{decentgrey}{RGB}{212,212,212}

\usetikzlibrary{calc,fit,positioning,arrows,arrows.meta,backgrounds,decorations.pathreplacing}
\pgfdeclarelayer{bg}
\pgfsetlayers{bg,main}

\newtcbox{\hlprimary}{on line,colback=c0!10,colframe=white,size=fbox,arc=3pt, box align=base,before upper=\strut, top=-2pt, bottom=-4pt, left=-1pt, right=-1pt, boxrule=0pt}
\newtcbox{\hlprimarytab}{on line, box align=base, colback=c0!10,colframe=white,size=fbox,arc=3pt, before upper=\strut, top=-2pt, bottom=-4pt, left=-2pt, right=-2pt, boxrule=0pt}
\newtcbox{\hlsecondary}{on line,colback=c1!10,colframe=white,size=fbox,arc=3pt, box align=base,before upper=\strut, top=-2pt, bottom=-4pt, left=-1pt, right=-1pt, boxrule=0pt}
\newtcbox{\hlsecondarytab}{on line, box align=base, colback=c1!10,colframe=white,size=fbox,arc=3pt, before upper=\strut, top=-2pt, bottom=-4pt, left=-2pt, right=-2pt, boxrule=0pt}
\newtcolorbox{hlmultiline}{on line,colback=decentgrey!75,colframe=white,size=fbox,arc=3pt, box align=base, top=0pt, bottom=2pt, boxrule=0pt, before=\adjustbox{valign=c}\bgroup, after=\egroup, before upper=\strut}

\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{Z}{>{\raggedleft\arraybackslash}X}

\renewcommand{\floatpagefraction}{.9}

\newcommand\mask{\_\_}

\newunicodechar{ðŸ¦•}{\includegraphics[height=1.25\fontcharht\font`A]{1f995}}


\newcommand\ours{\textsc{Dino}}
\newcommand\oursLower{dino}
\newcommand\oursFull{Datasets from Instructions}
\newcommand\oursFullHl{\textbf{D}atasets from \textbf{In}structi\textbf{o}ns}
\newcommand\ourDsFull{STS\=/ðŸ¦•}
\newcommand\ourDsSemi{STSb\=/ðŸ¦•}

\newcommand\given{\,{\mid}\,}
\newcommand{\bt}{\fontseries{b}\selectfont}
\newcommand{\dashifted}{\raisebox{0.5\depth}{\tiny}}
\newcommand{\da}[1]{{\scriptsize\hlprimarytab{\dashifted{#1}\%}}}
\newcommand{\dan}[1]{{\scriptsize\hlprimarytab{\,\dashifted\,{#1}}}}
\newcommand{\uashifted}{\raisebox{0.5\depth}{\tiny}}
\newcommand{\ua}[1]{{\scriptsize\hlsecondarytab{\,\uashifted\,{#1}}}}
\newcommand{\todo}[1]{{\leavevmode\color{red}TODO [#1]}}
\newcommand{\negphantom}[1]{\settowidth{\dimen0}{#1}\hspace*{-\dimen0}}







\title{Generating Datasets with Pretrained Language Models}



\author{
	Timo Schick \and Hinrich Sch\"utze \\
	Center for Information and Language Processing \\ LMU Munich, Germany \\
	{\tt schickt@cis.lmu.de}
}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter}  ##1: ##2
\hspace{1cm}}}}}

\enotesoff


\begin{document}
	\maketitle
	\begin{abstract}
		To obtain high-quality sentence embeddings
	from pretrained language models (PLMs), they must either be
	augmented with additional pretraining objectives or
	finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size.
		In this paper, we show how large PLMs can be leveraged to obtain high-quality embeddings without requiring any labeled data, finetuning or modifications to the
pretraining objective: We utilize the generative abilities
of PLMs to generate entire datasets of labeled text pairs from scratch, which can then be used for regular finetuning of much smaller models. Our fully unsupervised approach outperforms strong  baselines on several English semantic textual similarity datasets.\footnote{Our implementation is publicly available at \url{https://github.com/timoschick/\oursLower}}	\end{abstract}	
\section{Introduction}

While pretrained language models (PLMs) achieve strong results for a variety of NLP tasks
\citep{peters2018deep,radford2018improving,devlin2018bert}, they do not produce good sentence embeddings out of the box \citep{reimers-gurevych-2019-sentence}. Recent approaches try to address this by augmenting or replacing the language modeling objective with likewise unsupervised sentence-level objectives \citep[e.g.,][]{zhang-etal-2020-unsupervised,li-etal-2020-sentence}, but they typically lag behind their supervised counterparts trained on human-annotated sentence pairs. Unfortunately, obtaining large amounts of high-quality training data can be both difficult and prohibitively expensive \citep{bowman-etal-2015-large,agirre-etal-2016-semeval}. Furthermore, as model sizes continually increase \citep{radford2018language,raffel2019exploring,brown2020language,fedus2021switch}, it becomes increasingly expensive and challenging to finetune PLMs.

\begin{figure}
	\tikzset{
		instruction/.style={
			thick, font=\sffamily\small\lsstyle, text width=0.9347\linewidth, align=left, draw=decentgrey, inner xsep=6pt, inner ysep=8pt, rounded corners=2pt,
		},
	}
	\centering
	\begin{tikzpicture}
	
	\node[instruction](instruction-0){
		\textbf{Task}:\,Write two sentences that \textcolor{c0}{mean the same thing}. \0.2cm]
		\textbf{Sentence 2}: ``\textcolor{c1}{He's playing a flute.''}
	};
	
	\node[instruction, below=0.2cm of instruction-0](instruction-1){
		\textbf{Task}:\,Write two sentences that \textcolor{c0}{are somewhat similar}. \0.2cm]
		\textbf{Sentence 2}: ``\textcolor{c1}{A woman has been playing the violin.''}
	};
	
	\node[instruction, below=0.2cm of instruction-1](instruction-2){
		\textbf{Task}: Write two sentences that \textcolor{c0}{are on completely different topics}. \0.2cm]
		\textbf{Sentence 2}: ``\textcolor{c1}{A woman is walking down the street.''}
	};	
	\end{tikzpicture}
	\caption{\textcolor{c1}{Continuations} generated by GPT2-XL with \ours{} for three different \textcolor{c0}{task descriptions}. 
We investigate two different unsupervised approaches to
generating sentence-similarity datasets in this paper. (i) 
The \textcolor{c2}{input sentence} is given and only
the \textcolor{c1}{continuation} is generated. This requires
that an (unlabeled) set of input sentences is
available. (ii) Both \textcolor{c2}{input sentence}
and \textcolor{c1}{continuation} are generated. This does
not rely on the availability of any resources.}	\label{figure:motivational-example}
\end{figure}


To alleviate both problems, we explore a
novel 
approach to obtaining high-quality sentence embeddings:
Inspired by how existing NLI datasets have been created by
human
crowdworkers \citep{bowman-etal-2015-large,williams2018mnli},
we loosely replicate this setup but replace human annotators
with large PLMs. This allows us to automatically create
entire datasets from scratch that can be used for supervised
training of much smaller models. Not only does this solve
the problem of limited training data, it also provides a
viable path to leverage big models like
GPT-3 \citep{brown2020language} without requiring any
updates to their parameters. As illustrated in
Figure~\ref{figure:motivational-example}, our approach is
based on recent methods for providing instructions to
PLMs \citep[e.g.,][]{radford2018language,brown2020language,schick2020fewshot,schick2020exploiting}. We
use the \emph{self-debiasing} approach
of \citet{schick2021selfdiagnosis} to ensure that each
generated text pair is not only a good fit for a given
similarity label, but also \emph{not} a good fit for other
labels. We refer to our method as \oursFullHl{} (\ours{}).

In summary, our contributions are as follows:
\begin{itemize} 
\item We introduce \ours{}, a method for automatically generating labeled datasets of arbitrary size by providing PLMs with instructions.
\item We release \ourDsFull{} (read as ``STS-Dino''), the
first textual similarity dataset generated
completely automatically,
without any human annotation effort.
\item We show that Sentence-RoBERTa \citep{reimers-gurevych-2019-sentence} trained on \ourDsFull{} outperforms strong baselines on several semantic textual similarity datasets.
\end{itemize}

\section{Related Work}

There are many unsupervised approaches to obtaining sentence
embeddings, for example by averaging word
embeddings \citep{mikolov2013word2vec,Glove,bojanowski2017enriching}
or with carefully designed sentence-level
objectives \citep{pmlr-v32-le14,NIPS2015_f442d33f}.
Ensembling several such methods improves embedding quality
\citep{porner19domainadapt,porner20meta}.
Several recent approaches obtain sentence representations by supplementing BERT \citep{devlin2018bert} or other PLMs with additional unsupervised objectives \citep{zhang-etal-2020-unsupervised,li-etal-2020-sentence,wu2020clear,giorgi2020declutr}. Often, labeled datasets such as paraphrase databases \citep{wieting-gimpel-2018-paranmt} or natural language inference datasets \citep{conneau-etal-2017-supervised,cer-etal-2018-universal,reimers-gurevych-2019-sentence} are used.

Some approaches \emph{augment} existing datasets with
automatically generated
examples \citep{Anaby-Tavor_Carmeli_Goldbraich_Kantor_Kour_Shlomov_Tepper_Zwerdling_2020,papanikolaou2020dare,yang-etal-2020-generative,kumar2021data},
but in contrast to our work, all of these approaches require
that there already exists a labeled dataset for finetuning the generator.
Providing PLMs with task descriptions for zero- or few-shot learning has been studied extensively \citep[e.g.,][]{radford2018language,puri2019zeroshot,opitz2019argumentative,brown2020language,schick2020fewshot,schick2020just,schick2020exploiting,Weller_2020,gao2020making,tam2021improving}. However, none of these approaches is suitable for generating sentence embeddings.

Closely related to our work, \citet{efrat2020turking} examine the ability of PLMs to follow natural language instructions for generating examples in place of human crowdworkers, but they use a different approach and find that PLMs perform poorly across all tasks considered.

\section{\oursFull{}}
\label{section:method}

Let  be a PLM with vocabulary ,  the set of all token sequences and   a finite set of semantic similarity labels. Our aim is to generate a dataset  of text \emph{pairs}  with corresponding similarity labels . For  and , we denote with  the probability that  assigns to  as a continuation of .

\begin{figure}
	\tikzset{
		instruction/.style={
			thick, font=\sffamily\small, text width=0.9347\linewidth, align=left, draw=decentgrey, inner xsep=6pt, inner ysep=8pt, rounded corners=2pt,
		},
	}
	\centering
	\begin{tikzpicture}
	
	\node[instruction](instruction-0){
		\textbf{Task}:\,Write two sentences that \textcolor{c0}{\normalsize}. \0.2cm]
		\textbf{Sentence 2}: ``
	};
	\end{tikzpicture}
	\caption{Instruction template 
		for similarity label  and input sentence
		;  is described in
		Section~\ref{section:method}. See
		Figure~\ref{figure:motivational-example} for
		example instantiations of the template.}
	\label{figure:instruction-template}
\end{figure}

We first assume that we already have access to a set
 of texts (e.g., a set of sentences that are
typical of the domain of interest);
this is a realistic setting for many real-world applications, where large amounts of unlabeled text are abundant, but it is difficult to obtain \emph{interesting} text pairs and corresponding labels. \ours{} requires a set of \emph{instructions}  where each  is a function that, given an input , prompts its recipient to generate an appropriate second text . We use the instructions shown in Figure~\ref{figure:instruction-template} and consider three levels of similarity (), where

is loosely based on the five-level similarity scheme of \citet{cer-etal-2017-semeval}. Note that for all ,  ends with an (opening) quotation mark. This is because we need to know when the PLM has completed the requested text; the opening quotation mark allows us to treat the 
first
quotation mark generated by the model as a sign that it
is done.

For a given  and , we could
directly use the  instructions

to obtain  by continuously sampling tokens

starting from  until  is a quotation mark and
setting . However, we
may want the PLM to generate a text  that is
not only a good fit for instruction , but
also explicitly \emph{not} a good fit for some other
instruction . We refer to the
corresponding label  as a \emph{counterlabel} for 
and denote the set of all counterlabels for  as
. For example,  means that
for , we want  to generate a sentence
 that is somewhat similar to (), but at
the same time does \emph{not}
have the same meaning () as sentence
. We achieve this goal using the
self-debiasing algorithm of \citet{schick2021selfdiagnosis}:
When sampling the token , we consider not just ,
's  probability given , but also , 's probability given  for each . We penalize each token  for which  is lower than \emph{any}  by multiplying its probability with a factor
 where 
 is the difference between 's probability given  and its maximum probability given  for any , and the \emph{decay constant}  is a hyperparameter.

For settings where no set of unlabeled texts  is
available, a straightforward approach would be to use the
phrase shown in Figure~\ref{figure:instruction-template} up
to and including the first quotation mark as an instruction
to let the PLM generate both  and
. However, this approach has at least two
issues: First, generated texts may not match the schema
shown in Figure~\ref{figure:motivational-example} (e.g., the
model may never produce the string ``Sentence 2:''). Second,
the set of texts  should ideally be highly
diverse, whereas we want to give the model less leeway when
generating , so
we may want to use different sampling strategies for
 and .

We solve the two problems as follows: We first use 
(Figure~\ref{figure:instruction-template}) up to and
including the first quotation mark
(the one right after ``Sentence 1:'')
to generate
; we stop as soon as the model produces a
quotation mark.
We run this procedure repeatedly until we have a sufficient
number of  sentences. These are gathered into
a set  and then we proceed exactly as in the case where 
is already given.

\section{Experiments}

\begin{table*}
	\small
	\begin{tabularx}{\linewidth}{llcYYYYYYYY}
		\toprule
		& \textbf{Model} & \textbf{UD} & \textbf{STS12} & \textbf{STS13} & \textbf{STS14} & \textbf{STS15} & \textbf{STS16} & \textbf{STSb} & \textbf{SICK} & \textbf{Avg.} \\
		\midrule
		\multirow{4}{*}{\rotatebox[origin=c]{90}{sup.}}
		& InferSent, Glove & -- & 52.86 & 66.75 & 62.15 & 72.77 & 66.87 & 68.03 & 65.65 & 65.01 \\
		& USE & -- & 64.49 & 67.80 & 64.61 & 76.83 & 73.18 & 74.92 & 76.69 & 71.22 \\
		& S-BERT (base) & -- & 70.97 & 76.53 & \underline{73.19} & 79.09 & 74.30 & 77.03 & 72.91 & 74.89 \\
		& S-RoBERTa (base) & -- & \underline{71.54} & 72.49 & 70.80 & 78.74 & 73.69 & 77.77 & \underline{74.46}  & 74.21 \\
		\midrule
		\multirow{8}{*}{\rotatebox[origin=c]{90}{unsup.}}
		& Avg. GloVe & -- & 55.14 & 70.66 & 59.73 & 68.25 & 63.66 & 58.02 & 53.76 & 61.32 \\
		& Avg. BERT & -- & 38.78 & 57.98 & 57.98 & 63.15 & 61.06 & 46.35 & 58.40 & 54.81 \\
		& BERT CLS & -- & 20.16 & 30.01 & 20.09 & 36.88 & 38.08 & 16.50 & 42.63 & 29.19 \\	
		& \citet{zhang-etal-2020-unsupervised} & NLI & 56.77 & 69.24 & 61.21 & 75.23 & 70.16 & 69.21 & 64.25 & 66.58 \\
		& \citet{li-etal-2020-sentence} & NLI & 59.54 & 64.69 & 64.66 & 72.92 & 71.84 & 58.56 & 65.44 & 65.38 \\
		& \citet{li-etal-2020-sentence} & STS & 63.48 & 72.14 & 68.42 & 73.77 & 75.37 & 70.72 & 63.11 & 69.57 \\
		& \ours{} (\ourDsFull) & -- & 64.87 & 78.30 & 66.38 & 79.60 & 76.47 & 76.51 & \textbf{74.26} & 73.77 \\
		& \ours{} (\ourDsSemi) & STS & \textbf{70.27} & \textbf{\underline{81.26}} & \textbf{71.25} & \textbf{\underline{80.49}} & \textbf{\underline{77.18}} & \textbf{\underline{77.82}} & 68.09 & \textbf{\underline{75.20}} \\
		\bottomrule
	\end{tabularx}
	\caption{Spearman's rank correlation on STS12--16,
	STSb and SICK without finetuning on task-specific examples for
	models with NLI supervision (``sup.'') and fully
	unsupervised (``unsup.'') models using the same
	evaluation setup
	as \citet{reimers-gurevych-2019-sentence}. The
	second column shows which unlabeled data (``UD'') is
	used by unsupervised approaches in addition to
	original pretraining data; the final column shows
	average performance. Results for all baselines
	except \citet{zhang-etal-2020-unsupervised}
	and \citet{li-etal-2020-sentence} are
	from \citet{reimers-gurevych-2019-sentence}. The
	best unsupervised result is shown in bold, the best
	overall result is underlined.
\ours{}  outperforms all unsupervised approaches and,
	surprisingly, also (M)NLI-supervised approaches on the STS datasets.}
	\label{table:main-results}
\end{table*}

We evaluate \ours{} on the following English semantic
textual similarity datasets: the STS tasks 2012 -
2016 \citep{10.5555/2387636.2387697,agirre-etal-2013-sem,agirre-etal-2014-semeval,agirre-etal-2015-semeval,agirre-etal-2016-semeval},
the STS benchmark (STSb) \citep{cer-etal-2017-semeval}, and
the SICK-Relatedness dataset
(SICK) \citep{marelli-etal-2014-sick}. For all tasks, we
adopt the unsupervised setting without task-specific training examples.

We use \ours{} to generate two datasets: \ourDsSemi, a
dataset for which we make use of STSb to obtain a set of
texts , and \ourDsFull{}, where  is generated from
scratch. For both datasets, we use GPT2-XL as PLM with the
set of counterlabels defined as 
and a decay constant of ,
following \citet{schick2021selfdiagnosis}. We apply
top- \citep{Holtzman2020The} and
top- \citep{fan-etal-2018-hierarchical,holtzman-etal-2018-learning}
sampling with ,  and generate up to 40
output tokens. For each  and , we generate up to two corresponding
's.\footnote{As the PLM does not always
generate a closing quotation mark within the first 40
tokens, we use a maximum of five tries to generate the two
's for each  and . We discard
generations that are not closed by a quotation mark.} For \ourDsFull{}, we obtain  be generating 15,000 sentences using only top- sampling (again with ) and no top- sampling to ensure more diversity in the generated outputs. For both datasets, we remove all examples where  as those provide no training signal to a Siamese model architecture. We split both datasets into training and validation sets using a 90/10 split.

To assess the quality of the generated datasets, we use them to train Sentence-RoBERTa \citep{reimers-gurevych-2019-sentence}, a Siamese network architecture based on RoBERTa (base) \citep{liu2019roberta}. As our datasets contain many noisy examples, we use a technique similar to label smoothing \citep{szegedy2016rethinking} with  and replace each  with
 which simplifies to .
Additionally, for each , we sample two
's from \emph{other} dataset entries and
augment the dataset with . We use the default parameters
of \citet{reimers-gurevych-2019-sentence} with a batch size
of 32 and train for at most one epoch; the exact number of
training steps is determined based on Spearman's rank correlation on the validation set.

\paragraph{Results}

\begin{table}
	\small
	\setlength\tabcolsep{3pt}
	\begin{tabularx}{\linewidth}{lcYY}
		\toprule
		\textbf{Model} & \textbf{STS12-16} & \textbf{STSb} & \textbf{SICK} \\
		\midrule
		\ours{} (\ourDsSemi) & \textbf{76.09} & \textbf{77.82} & \textbf{68.09} \\
		\ â”” decay constant    & 65.50 & 70.71 & 67.60 \\
		\ â”” decay constant  & 75.40 & 77.49 & 66.83 \\
		\ â”” no label smoothing & 74.50 & 76.26 & 66.23 \\
		\ â”” no augmentation & 70.90 & 73.81 & 63.98 \\
		\bottomrule
	\end{tabularx}
	\caption{Effect of removing self-debiasing () or increasing the decay constant (), using no label smoothing and performing no data augmentation (sampling random 's for each ) on the performance of \ours{} on STS12-16 (avg), STSb and SICK}
	\label{table:self-debiasing}
\end{table}

\begin{table}
	\small \setlength\tabcolsep{3pt} \lsstyle \begin{tabularx}{\linewidth}{lXX} \toprule
	 & \multicolumn{1}{c}{}
	& \multicolumn{1}{c}{} \\ \midrule 2 &
	US closes embassy in Syria & US Embassy in Syria \\
	2 & A man is playing the cello. & The cello is
	playing the man. \\ 2 & A plane is taking off. & I
	want to be a pilot. \\ \midrule 0 & Closed roads in
	Armenia & Open roads in Azerbaijan \\ 0 & The man is
	playing the guitar. & I'm not a guitar player. \\ 0
	& A man is playing a large flute. & A man is
	listening to a small
	flute. \\ \bottomrule \end{tabularx} \caption{A
	selection of low-quality examples
	in \ourDsSemi{}. Many sentence pairs for 
are not similar and have quite different meanings.
Some sentence pairs
	for  are not on completely different
	topics.}  \label{table:qualitative-analysis}
\end{table}

We compare S-RoBERTa (base) trained on datasets generated
with \ours{} to S-BERT and S-RoBERTa finetuned on NLI data
as well as Universal Sentence Encoder
(USE) \citep{cer-etal-2018-universal} and
InferSent \citep{conneau-etal-2017-supervised}, all of which
are trained on hundreds of thousands of labeled text pairs
from SNLI \citep{bowman-etal-2015-large} and
MNLI \citep{williams2018mnli}. We additionally compare to
the following fully unsupervised approaches: averaging word-level GloVe \citep{Glove} or BERT \citep{devlin2018bert} embeddings, using BERT's CLS token, and recent methods by \citet{zhang-etal-2020-unsupervised} and \citet{li-etal-2020-sentence} based on pretrained BERT models. As shown in Table~\ref{table:main-results}, training on datasets generated with \ours{} clearly outperforms the fully unsupervised baselines; on average, training on \ourDsSemi{} even outperforms all approaches with NLI supervision. \ourDsSemi{} gives better results than \ourDsFull{} on all STS datasets as its examples are -- by design -- very similar to examples found in these datasets, while training on \ourDsFull{} gives better results on SICK.

We investigate the importance of self-debiasing \citep{schick2021selfdiagnosis} in Table~\ref{table:self-debiasing} (top); as can be seen, removing self-debiasing () dramatically hurts performance. Table~\ref{table:self-debiasing} (bottom) shows that training on \ours{} datasets requires some measures to limit the effect of noisy labels: removing label smoothing and performing no data augmentation (i.e., not generating additional pairs  by sampling random 's for each ) clearly hurts performance.

We finally take a qualitative look at some of the errors contained in the datasets created with \ours{}. As shown in Table~\ref{table:qualitative-analysis}, for  the PLM often generates sentences that omit or mix up important information, and sometimes produces sentences with an entirely different meaning. For , we found the model to often simply flip words (``closed''  ``open'', ``large''  ``small'') instead of producing sentences on completely different topics.

\section{Conclusion}

We have introduced \ours{}, a method for using large PLMs to generate entire datasets of sentence pairs with similarity labels from scratch, requiring no parameter updates. This is achieved by providing instructions in natural language, combined with the self-debiasing method of \citet{schick2021selfdiagnosis}. With appropriate measures for limiting the influence of noisy data, models trained on datasets generated with \ours{} achieve strong results on several semantic textual similarity datasets. 

For future work, it would be interesting to see whether the noise in datasets generated with \ours{} can further be reduced, e.g., by using different sets of instructions \citep{jiang2019know,schick2020exploiting}.

\bibliography{literatur}
\bibliographystyle{acl_natbib}

\end{document}
