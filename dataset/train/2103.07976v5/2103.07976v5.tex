\section{Experiments}
\label{sec:exp}

In this section, we first introduce the detailed setup including datasets and training hyper-parameters. Quantitative analysis is then given followed by ablation studies. We further give qualitative analysis and visualization results to show the interpretability of our model.

\subsection{Experiments Setup}
\label{sec:setup}

\textbf{Datasets.} We evaluate our proposed TransFG on five widely used fine-grained benchmarks, i.e., CUB-200-2011 \cite{WahCUB_200_2011}, Stanford Cars \cite{KrauseStarkDengFei-Fei_3DRR2013}, Stanford Dogs \cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}, NABirds \cite{van2015building} and iNat2017 \cite{van2018inaturalist}. We also exploit its usage in large-scale challenging fine-grained competitions.

\noindent \textbf{Implementation details.}
Unless stated otherwise, we implement TransFG as follows. First, we resize input images to $448 * 448$ except $304 * 304$ on iNat2017 for fair comparison (random cropping for training and center cropping for testing). We split image to patches of size 16 and the step size of sliding window is set to be 12. Thus the $H, W, P, S$ in Eq \ref{equ:split} are 448, 448, 16, 12 respectively. The margin $\alpha$ in Eq \ref{equ:con} is set to be 0.4. We load intermediate weights from official ViT-B\_16 model pretrained on ImageNet21k. The batch size is set to 16. SGD optimizer is employed with a momentum of 0.9. The learning rate is initialized as 0.03 except 0.003 for Stanford Dogs dataset and 0.01 for iNat2017 dataset. We adopt cosine annealing as the scheduler of optimizer.

All the experiments are performed with four Nvidia Tesla V100 GPUs using the PyTorch toolbox and APEX.

\subsection{Quantitative Analysis}
\label{sec:quan}

We compare our proposed method TransFG with state-of-the-art works on above mentioned fine-grained datasets. The experiment results on CUB-200-2011 and Stanford Cars are shown in Table \ref{tab:cub}. From the results, we find that our method outperforms all previous methods on CUB dataset and achieve competitive performance on Stanford Cars.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on CUB-200-2011, Stanford Cars.}
    \label{tab:cub}
    \begin{tabular}{c|c|c|c}
    \hline
    Method & Backbone & CUB & Cars \\ \hline
    ResNet-50 & ResNet-50 & 84.5 & - \\
    NTS-Net & ResNet-50 & 87.5 & 93.9 \\ 
    Cross-X & ResNet-50 & 87.7 & 94.6 \\
    DBTNet & ResNet-101 & 88.1 & 94.5 \\
    FDL & DenseNet-161 & 89.1 & 94.2 \\
    PMG & ResNet-50 & 89.6 & 95.1 \\ 
    API-Net & DenseNet-161 & 90.0 & \textbf{95.3} \\
    StackedLSTM & GoogleNet & 90.4 & - \\ \hline
    DeiT & DeiT-B & 90.0 & 93.9 \\
    ViT & ViT-B\_16 & 90.3 & 93.7 \\ 
    TransFG & ViT-B\_16 & \textbf{91.7} & 94.8 \\ \hline
    \end{tabular}
\end{table}

To be specific, the third column of Table \ref{tab:cub} shows the comparison results on CUB-200-2011. Compared to the best result StackedLSTM \cite{Ge_2019_CVPR} up to now, our TransFG achieves a \textbf{1.3\%} improvement on Top-1 Accuracy metric and 1.4\% improvement compared to our base framework ViT \cite{dosovitskiy2020image}. Multiple ResNet-50 are adopted as multiple branches in \cite{ding2019selective} which greatly increases the complexity. It is also worth noting that StackLSTM is a very messy multi-stage training model which hampers the availability in practical use, while our TransFG maintains the simplicity.

The fourth column of Table \ref{tab:cub} shows the results on Stanford Cars. Our method outperforms most existing methods while performs worse than PMG \cite{du2020fine} and API-Net \cite{zhuang2020learning} with small margin. We argue that the reason might be the much more regular and simpler shape of cars. However, even with this property, our TransFG consistently gets \textbf{1.1\%} improvement compared to the standard ViT model.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on Stanford Dogs.}
    \label{tab:dog}
    \begin{tabular}{c|c|c}
    \hline
    Method & Backbone & Dogs \\ \hline
    MaxEnt & DenseNet-161 & 83.6 \\ 
    FDL & DenseNet-161 & 84.9 \\
    Cross-X & ResNet-50 & 88.9 \\
    API-Net & ResNet-101 & 90.3 \\ \hline
    ViT & ViT-B\_16 & 91.7 \\ 
    TransFG & ViT-B\_16 & \textbf{92.3} \\ \hline
    \end{tabular}
\end{table}

The results of experiments on Stanford Dogs are shown in Table \ref{tab:dog}. Stanford Dogs is a more challenging dataset compared to Stanford Cars with its the more subtle differences between certain species and the large variances of samples from the same category. Only a few methods have tested on this dataset and our TransFG outperforms all of them. While ViT \cite{dosovitskiy2020image} outperforms other methods by a large margin, our TransFG achieves 92.3\% accuracy which outperforms SOTA by \textbf{2.0\%} with its discriminative part selection and contrastive loss supervision.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on NABirds.}
    \label{tab:na}
    \begin{tabular}{c|c|c}
    \hline
    Method & Backbone & NABirds \\ \hline
    Cross-X & ResNet-50 & 86.4 \\ 
    API-Net & DenseNet-161 & 88.1 \\ 
    CS-Parts & ResNet-50 & 88.5 \\ 
    FixSENet-154 & SENet-154 & 89.2 \\ \hline
    ViT & ViT-B\_16 & 89.9 \\
    TransFG & ViT-B\_16 & \textbf{90.8} \\ \hline
    \end{tabular}
\end{table}

NABirds is a much larger birds dataset not only from the side of images numbers but also with 355 more categories which significantly makes the fine-grained visual classification task more challenging. We show our results on it in Table \ref{tab:na}. 
We observe that most methods achieve good results by either exploiting multiple backbones for different branches or adopting quite deep CNN structures to extract better features. 
While the pure ViT \cite{dosovitskiy2020image} can directly achieve 89.9\% accuracy, our TransFG constantly gets 0.9\% performance gain compared to ViT and reaches 90.8\% accuracy which outperforms SOTA by \textbf{1.6\%}.

\begin{table}[]
    \small
    \centering
    \caption{Comparison of different methods on iNat2017.}
    \label{tab:inat}
    \begin{tabular}{c|c|c}
    \hline
    Method & Backbone & iNat2017 \\ \hline
    ResNet152 & ResNet152 & 59.0 \\
    IncResNetV2 & IncResNetV2 & 67.3 \\ 
    TASN & ResNet101 & 68.2 \\ \hline
    ViT & ViT-B\_16 & 68.7 \\ 
    TransFG & ViT-B\_16 & \textbf{71.7} \\ \hline
    \end{tabular}
\end{table}

iNat2017 is a large-scale dataset for fine-grained species recognition. Most previous methods do not report results on iNat2017 because of the computational complexity of the multi-crop, multi-scale and multi-stage optimization. With the simplicity of our model pipeline, we are able to scale TransFG well to big datasets and evaluate the performance which is shown in Table \ref{tab:inat}. This dataset is very challenging for mining meaningful object parts and the background is very complicated as well. We find that Vision Transformer structure outperforms ResNet structure a lot in these large challenging datasets. ViT outperformes ResNet152 by nearly 10\% and similar phenomenon can also be observed in iNat2018 and iNat2019. Our TransFG is the only method to achieve above 70\% accuracy with input size of 304 and outperforms SOTA with a large margin of \textbf{3.5\%}.

For the just ended iNat2021 competition which contains 10,000 species, 2.7M training images, our TransFG achieves very high single model accuracy of 91.3\%. (The final performance was obtained by ensembling many different models along with multi-modality processing) As far as we know, at least two of the Top5 teams in the final leaderboard adopted TransFG as one of their ensemble models. This clear proves that our model can be further extended to large-scale challenging scenarios besides academy datasets.

\subsection{Ablation Study}
\label{sec:ablation}

We conduct ablation studies on our TransFG pipeline to analyze how its variants affect the fine-grained visual classification result. All ablation studies are done on CUB-200-2011 dataset while the same phenomenon can be observed on other datasets as well.

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on split way of image patches on CUB-200-2011 dataset.}
    \label{tab:absplit}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Method & Patch Split & Accuracy (\%) & Training Time (h)\\ \hline
    ViT & Non-Overlap & 90.3 & 1.30 \\ 
    ViT & Overlap & \textbf{90.5} & 3.38  \\ \hline
    TransFG & Non-Overlap & 91.5 & 1.98 \\
    TransFG & Overlap & \textbf{91.7} & 5.38 \\ \hline
    \end{tabular}
\end{table}

\noindent \textbf{Influence of image patch split method.} We investigate the influence of our overlapping patch split method through experiments with standard non-overlapping patch split. As shown in Table \ref{tab:absplit}, both on the pure Vision Transformer and our improved TransFG framework, the overlapping split method bring consistently improvement, i.e., 0.2\% for both frameworks. The additional computational cost introduced by this is also affordable as shown in the fourth column. 

\begin{table}[ht]
    \small
    \centering
    \caption{Ablation study on Part Selection Module (PSM) on CUB-200-2011 dataset.}
    \label{tab:abpsm}
    \begin{tabular}{|c|c|}
    \hline
    Method & Accuracy (\%) \\ \hline
    ViT & 90.3 \\ 
    TransFG & \textbf{91.0} \\ \hline
    \end{tabular}
\end{table}

\noindent \textbf{Influence of Part Selection Module.} As shown in Table \ref{tab:abpsm}, by applying the Part Selection Module (PSM) to select discriminative part tokens as the input for the last Transformer layer, the performance of the model improves from 90.3\% to 91.0\%. We argue that this is because in this way, we sample the most discriminative tokens as input which explicitly throws away some useless tokens and force the network to learn from the important parts.

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on contrastive loss on CUB-200-2011 dataset.}
    \label{tab:abdup}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Method & Contrastive Loss & Acc (\%) \\ \hline
    ViT & & 90.3 \\
    ViT & \checkmark & \textbf{90.7} \\ \hline
    TransFG & & 91.0 \\ 
    TransFG & \checkmark & \textbf{91.5} \\ \hline
    \end{tabular}
\end{table}

\noindent \textbf{Influence of contrastive loss.} The comparisons of the performance with and without contrastive loss for both ViT and TransFG frameworks are shown in Table \ref{tab:abdup} to verify the effectiveness of it. We observe that with contrastive loss, the model obtains a big performance gain. Quantitatively, it increases the accuracy from 90.3\% to 90.7\% for ViT and 91.0\% to 91.5\% for TransFG. We argue that this is because contrastive loss can effectively enlarge the distance of representations between similar sub-categories and decrease that between the same categories which can be clearly seen in the comparison of confusion matrix in Fig \ref{fig:confusion}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth,height=4cm]{contrast.jpg}
    \caption{Illustration of contrastive loss. Confusion matrices without and with contrastive loss of a batch with four classes where each contains four samples are shown. The metric of confusion matrix is cosine similarity. Best viewed in color.}
    \label{fig:confusion}
\end{figure}

\begin{table}[]
    \small
    \centering
    \caption{Ablation study on value of margin $\alpha$ on CUB-200-2011 dataset.}
    \label{tab:abalpha}
    \begin{tabular}{|c|c|c|}
    \hline
    Method & Value of $\alpha$ & Accuracy (\%) \\ \hline
    TransFG & 0 & 91.1 \\ 
    TransFG & 0.2 & 91.4 \\ 
    TransFG & 0.4 & \textbf{91.7} \\ 
    TransFG & 0.6 & 91.5 \\ \hline
    \end{tabular}
\end{table}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{cub_21.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{dog_21.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_02.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_12.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_22.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{car_21.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_01.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_11.jpg}
    \end{subfigure}
    \begin{subfigure}{0.16\linewidth}
        \centering
		\includegraphics[width=\linewidth]{na_21.jpg}
    \end{subfigure}
    \caption{Visualization results of TransFG on CUB-200-2011, Stanford Dogs, Stanford Cars and NABirds datasets. Two kinds of visualization are given, where the first and the third row show the selected Top-4 token positions while the second and fourth rows show the overall global attention maps. See examples from NABirds dataset where birds are sitting on twigs. The bird parts are lighted while the occluded twigs are ignored. Best viewed in color.}
    \label{fig:vis}
\end{figure*}

\noindent \textbf{Influence of margin $\alpha$.} The results of different setting of the margin $\alpha$ in Eq \ref{equ:con} is shown in Table \ref{tab:abalpha}. We find that a small value of $\alpha$ will lead the training signals dominated by easy negatives thus decrease the performance while a high value of $\alpha$ hinder the model to learn sufficient information for increasing the distances of hard negatives. Empirically, we find 0.4 to be the best value of $\alpha$ in our experiments. 

\subsection{Qualitative Analysis}
\label{sec:qual}

We show the visualization results of proposed TransFG on the four benchmarks in Fig \ref{fig:vis}. We randomly sample three images from each dataset. Two kinds of visualizations are presented. The first and the third row of Fig \ref{fig:vis} illustrated the selected tokens positions. For better visualization results, we only draw the Top-4 image patches (ranked by the attention score) and enlarge the square of the patches by two times while keeping the center positions unchanged. The second and fourth rows show the overall attention map of the whole image where we use the same attention integration method as described above to first integrate the attention weights of all layers followed by averaging the weights of all heads to obtain a single attention map. The lighter a region is, the more important it is. From the figure, we can see that our TransFG successfully captures the most important regions for an object, i.e., head, wings, tail for birds; ears, eyes, legs for dogs; lights, doors for cars. At the same time, our overall attention map maps the entire object precisely even in complex backgrounds and it can even serves as a segmentation mask in some simple scenarios. These visualization results clearly prove the interpretability of our proposed method.