\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[preprint]{neurips_2021}







\usepackage{amsfonts}       \usepackage{amsmath}
\usepackage{booktabs}       \usepackage[T1]{fontenc}    \usepackage{gensymb}
\usepackage{graphicx}
\usepackage{hyperref}       \usepackage[utf8]{inputenc} \usepackage{microtype}      \usepackage{nicefrac}       \usepackage{subfigure}
\usepackage{url}            \usepackage{wrapfig}
\usepackage{xcolor}         

\newcommand{\btv}{\texttt{baller2vec}}
\newcommand{\btvpp}{\texttt{baller2vec++}}

\newcommand{\anh}[1]{\textcolor{blue}{#1}}

\title{\btvpp{}: A Look-Ahead Multi-Entity Transformer For Modeling Coordinated Agents}



\author{
  Michael A. Alcorn\\\\
  Department of Computer Science and Software Engineering\\
  Auburn University\\
  Auburn, AL 36849 \\
  \texttt{alcorma@auburn.edu} \\
  \And
  Anh Nguyen \\\\
  Department of Computer Science and Software Engineering\\
  Auburn University\\
  Auburn, AL 36849 \\
  \texttt{anh.ng8@gmail.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
In many multi-agent spatiotemporal systems, the agents are under the influence of shared, \textit{unobserved} variables (e.g., the play a team is executing in a game of basketball).
As a result, the trajectories of the agents are often statistically \textit{dependent} at any given time step; however, almost universally, multi-agent models implicitly assume the agents' trajectories are statistically \textit{independent} at each time step.
In this paper, we introduce \btvpp{}\footnote{All data and code for the paper are available at: \url{https://github.com/airalcorn2/baller2vecplusplus}.}, a multi-entity Transformer that can effectively model coordinated agents.
Specifically, \btvpp{} applies a specially designed self-attention mask to a \textit{mixture} of location \textit{and} ``look-ahead'' trajectory sequences to learn the distributions of statistically dependent agent trajectories.
We show that, unlike \btv{} (\btvpp{}'s predecessor), \btvpp{} can learn to emulate the behavior of perfectly coordinated agents in a simulated toy dataset.
Additionally, when modeling the trajectories of professional basketball players, \btvpp{} outperforms \btv{} by a wide margin. \end{abstract}

\section{Introduction and Related Work}
Whether it is a team executing a play in a game of basketball, a family navigating to an attraction in a theme park, or friends posting about a birthday party on a social media platform, humans frequently coordinate their behavior in response to shared information.
When this coordinating information is unobserved (which is often the case in many machine learning datasets), the individuals' observed behaviors become correlated, i.e., the behavior of one individual at a specific time contains information about the behavior of another individual \textit{at the same time}.
In the context of forecasting agent trajectories in multi-agent spatiotemporal systems, this property translates to the trajectories being statistically dependent at each time step.
However, nearly all multi-agent spatiotemporal models (e.g., \cite{felsen2018will, gupta2018social, sadeghian2019sophie, yeh2019diverse, YuMa2020Spatio, alcorn2021baller2vec}) implicitly (through their loss functions) assume the trajectories of the agents at each time step are statistically \textit{independent} given the agents' previous locations (Figure \ref{fig:ind_dep}).

\citet{zhan2018generating} explicitly focused on modeling coordinated multi-agent trajectories, using ``macro-intents'' \cite{zheng2016generating} that are shared across agents to do so.
\begin{wrapfigure}{l}{0.6\linewidth}
\includegraphics[width=\linewidth]{statistical_dependency_closer}
\caption{\textbf{Left}: most multi-agent systems implicitly assume the trajectories of the agents at each time step () are conditionally independent given the agents' previous positions ().
\textbf{Right}: however, the various decompositions of the joint probability of the trajectories, e.g.,  (note, we omit the conditional  terms for brevity), suggest more complex statistical dependencies between the agents' trajectories can exist (i.e., the independence assumption is an extremely strong one).
Indeed, there are often shared unobserved variables influencing the spatiotemporal behaviors of agents—such as the play that the players on a basketball team are executing, or events occurring in a pedestrian environment—which suggests statistical dependencies between the agents' trajectories are likely.
}
\label{fig:ind_dep}
\end{wrapfigure}
The macro-intents are generated from a separately trained recurrent neural network (RNN) that learns to predict a future, coarse, ``stationary'' location for each agent at each time step.
The macro-intents for all of the agents at a specific time step are concatenated together to form a single, shared, macro-intent variable, which is then provided as input to the trajectories-generating model at that time step.
However, similar to the previously mentioned multi-agent trajectory models, the macro-intents model implicitly assumes the macro-intents for the agents at each time step are statistically independent, i.e., the generated macro-intent for one agent does not depend on the generated macro-intents of the other agents.\footnote{See the authors' implementation here: \url{https://github.com/ezhan94/multiagent-programmatic-supervision/blob/a1d9152d4c8a287474953cba093c28fef2a05979/models/macro_vrnn.py\#L101}.}
Further, the trajectories-generating model still implicitly assumes the trajectories of the agents at each time step are independent, which is only true if the shared macro-intent variable perfectly captures all of the unobserved information that could cause the agents' trajectories to be correlated.

Notably, Social-BiGAT \cite{NEURIPS2019_d09bf415} \textit{does} partially account for trajectory correlations through a global adversarial loss.
Specifically, the trajectories for each agent are separately passed through an encoder RNN, and the final hidden states for each agent are then processed with a graph attention network (GAT) \cite{velickovic2018graph}.
The output of the GAT is then used as an input to a discriminator that classifies whether or not the input trajectories are real or generated.
This global adversarial loss, however, is only a single component of the full Social-BiGAT loss function, and other components of the loss function \textit{do} implicitly make the independence assumption.
Further, interestingly, adding the global discriminator to a baseline model only improved the model's performance for one out of six pedestrian datasets.

In this paper, we describe a novel multi-agent spatiotemporal model that integrates information about \textit{concurrent} actions of agents to predict statistically dependent distributions of trajectories.
Specifically, we extend the recently introduced multi-entity Transformer \btv{} \cite{alcorn2021baller2vec} by: (1) augmenting its input with a parallel sequence of ``look-ahead'' agent trajectories and (2) using a specially designed self-attention mask, which allows our model to exploit the chain rule of probability (Section \ref{sec:baller2vec++}).
We find that:

\begin{enumerate}
    \item \btvpp{} is an effective learning algorithm for modeling coordinated agents.
    Unlike \btv{}, \btvpp{} can learn to emulate perfectly coordinated agents from a simulated toy dataset (Section \ref{sec:effective}).
    Further, \btvpp{} outperforms \btv{} by a wide margin (8.9\%) when modeling the trajectories of professional basketball players (Section \ref{sec:effective}).
    \item \btvpp{} makes better forecasts when conditioned on concurrent trajectory information from other agents, supporting our proposition that the commonly used independence assumption for agent trajectories is overly strong (Section \ref{sec:conditioning}).
    \item Lastly, the joint probability assigned to a sequence by \btvpp{} is approximately permutation invariant with respect to the order of the agents, i.e., \btvpp{} respects the properties of the chain rule (Section \ref{sec:permutation}).
\end{enumerate} 
\section{Background}\label{sec:background}
\subsection{Multi-agent trajectory modelling}

Our problem description closely follows \citet{alcorn2021baller2vec}, whom we quote here:

\begin{quote}
Let  be a set indexing  agents and  be the  agents involved in a particular sequence.
[Let]  [be] an unordered set of  coordinate pairs such that  are the coordinates for agent  at time step .
The ordered sequence of sets of coordinates , together with , thus defines the trajectories for the  agents over  time steps.
\end{quote}

In multi-agent trajectory modeling, the goal is to model a joint probability of the form:



\noindent
i.e., the joint probability of the  agents' trajectories  at time step  given the agents' location histories .
We note here that the common practice of \textit{simultaneously} predicting the trajectories for all of the agents at a specific time step is not required by theory.
Using the chain rule of probability, the joint probability of the agents' trajectories can be factorized as, e.g.:



where we omit the conditional historical trajectories for brevity.
As a result, it is perfectly acceptable to generate trajectories agent-wise, using the previously generated trajectories as additional conditioning information when generating the trajectories for later agents (see Figure \ref{fig:legal_dep}).

\begin{figure}[ht]
\centering
\subfigure{\includegraphics[width=\columnwidth]{legal}}
\caption{
At inference time, a model is not \textit{required} to \textit{simultaneously} generate the trajectories for all of the agents at a specific time step.
An alternative strategy is to allow the model to generate the agents' trajectories one at a time, and let the model use the previously generated trajectories to inform the trajectories it generates for the remaining agents.
}
\label{fig:legal_dep}
\end{figure}

\subsection{\btv{} is a (conditional) generative model.}\label{sec:baller2vec_generative}

\btv{} is a recently described \textit{multi-entity} Transformer that can model sequences of \textit{sets} (the underlying data structure for multi-agent spatiotemporal systems), as opposed to sequences of individual inputs (like words in a sentence).
In the case of basketball, the input at each time step is a set of feature vectors where each feature vector contains information about the identity and location of a player on the court.
\btv{} maps each input feature vector to an output feature vector, which is then used to ``classify'' the binned trajectory for that specific player at that specific time step.

Here, we provide a probabilistic interpretation of \btv{}, which establishes the theoretical grounds for using the chain rule to generate trajectories agent-wise at each time step in \btvpp{}.
Without loss of generality, we only consider one-dimensional trajectories for a single agent here.
To briefly summarize, the outputs of the softmax function over the  binned trajectories in \btv{} can be interpreted as mixture proportions for a mixture of uniform distributions with predetermined bounds that partition the Euclidean trajectory space.
Further, because , \btv{} is in fact a conditional generative model that assigns a probability to a sequence of trajectories given the initial position of the agent, i.e., .
Using the chain rule, we decompose the joint probability of the trajectories as:



\noindent
to reflect their temporal structure (we omit the conditional initial position term for brevity).
Therefore, new trajectories can be generated from \btv{} with the following procedure (see Figure \ref{fig:graphical_model}):

\begin{enumerate}
    \item First, sample one of the  different mixture components using the mixture proportions output from the classifier  (i.e., \btv{}) conditioned on the agent's current position, i.e.,  where .
    \item Next, sample a trajectory from the uniform distribution associated with the sampled component, i.e., .
    \item Finally, add the sampled trajectory to the agent's input position to generate the agent's position at the start of the next time step, i.e., .
\end{enumerate}

\begin{wrapfigure}{r}{0.5\linewidth}
\includegraphics[width=\linewidth]{graphical_model_tight}
\caption{
\btv{} can be viewed as a conditional generative model that assigns a probability to a sequence of trajectories given the initial positions of the agents.
Here, we show a graphical model depiction of a \btv{} model that generates a sequence of one-dimensional trajectories for a single agent.
Given the initial position of the agent (the circle containing ), one of  different uniform distributions (the square containing ) is sampled using the mixture proportions () output by \btv{} ().
The agent's trajectory (the diamond containing ) is then sampled from the selected uniform distribution, which has bounds .
At the start of the next time step, the agent's position is .
Maximizing the likelihood of \btv{} as a classifier over the binned trajectories is thus equivalent to maximizing its likelihood when assuming the trajectories are generated from a mixture of uniform distributions that partition the Euclidean trajectory space (see Section \ref{sec:baller2vec_generative} for details).
}
\label{fig:graphical_model}
\end{wrapfigure}
Let  be an interval on the real line such that any trajectory  or  has zero density (i.e., such trajectories are humanly impossible).
Let  be a set of  intervals that partition the interval  into  bins, i.e.,  and .
Recall that the probability density function (PDF) for a uniform distribution with bounds  is:



\noindent
Letting , the PDF for a mixture of uniforms with these bounds is thus:



\noindent
where  is the density assigned to  by the mixture,  is the mixture proportion for the mixture component indexed by  (i.e.,  and ), and  is the density assigned to  by the uniform distribution with bounds .
Because the bounds of the uniform distributions partition , Equation \eqref{eq:mix_uniforms} reduces to:



\noindent
where  (because the other uniform distributions will assign a density of zero to ).
The likelihood for data  (with ) is then:



\noindent
where  is the mixture proportion assigned to the component with   and  is the associated density.
Taking the negative logarithm of the likelihood gives:



\noindent
Because the bounds are fixed, the second summation is a constant, and Equation \eqref{eq:mixture_nll} becomes:



\noindent
where .
Therefore, minimizing the loss of \btv{} as a classifier of binned trajectories is equivalent to minimizing the loss of the model when assuming the trajectories are generated from a mixture of uniform distributions as specified in Equation \eqref{eq:mix_uniforms}. 
\section{Model Architecture}\label{sec:baller2vec++}
\begin{figure}[ht]
\centering
\subfigure{\includegraphics[width=\columnwidth]{information_leak}}
\vskip -0.1in
\caption{A naive strategy for learning to predict statistically dependent agent trajectories is to adapt the \btv{} self-attention mask so that \btv{} can ``look ahead'' at future positions of agents whose trajectories are generated \textit{prior} to the agent being processed in the current time step.
However, this look-ahead self-attention mask cannot be used with multi-layer Transformers because doing so necessitates ``seeing the future''.
For example, after the model attends to the blue agent's position at time step  when processing the yellow agent at time step , the yellow agent's resultant feature vector contains information about the blue agent's future position.
As a result, when the model attends to the yellow agent while processing the blue agent at the next level, the model is seeing the future.
}
\label{fig:information_leak}
\end{figure}
\vskip -0.1in

We motivate our \btvpp{} architecture by first highlighting an issue that arises in \btv{} when trying to model agent trajectories using the chain rule.
The \btv{} self-attention mask can be adapted so that \btv{} ``looks ahead'' at the future positions of agents whose trajectories are generated prior to the agent being processed in the current time step (Figure \ref{fig:information_leak}).
However, this look-ahead self-attention mask can only be used with the final layer of the Transformer; otherwise, the model needs to see the future (Figure \ref{fig:information_leak}).
As a result, \btv{} is severely limited in the conditional distribution functions it can learn.

\begin{figure}[ht]
\centering
\subfigure{\includegraphics[width=0.85\columnwidth]{baller2vec++}}
\vskip -0.1in
\caption{To learn statistically dependent agent trajectories, \btvpp{} uses a specially designed self-attention mask to simultaneously process three different sets of features vectors in a single Transformer.
The three sets of feature vectors consist of location feature vectors like those found in \btv{} (), look-ahead trajectory feature vectors (), and starting location feature vectors (; not shown).
As can be seen in these partial depictions of \btvpp{} and the \btvpp{} self-attention mask, this design allows the model to integrate information about \textit{concurrent} agent trajectories through \textit{multiple} Transformer layers without seeing the future.
}
\label{fig:baller2vec++}
\end{figure}

\btvpp{} (Figure \ref{fig:baller2vec++}) overcomes this limitation by: (1) augmenting the \btv{} input with two other sets of feature vectors and (2) using a specially designed self-attention mask.
The three sets of feature vectors in \btvpp{} take the following forms:

\begin{enumerate}
    \item  \hfill (\textbf{current location information})
    \item  \hfill (\textbf{``look-ahead'' information})
    \item  \hfill (\textbf{initial location information})
\end{enumerate}

where , , and  are multilayer perceptrons (MLPs),  is an agent embedding layer, and  is a vector of optional contextual features for agent  at time step .
 is the same location feature vector used in \btv{} and contains information about a specific agent's identity and the agent's location at time step .
 is a ``look-ahead'' trajectory feature vector that contains information about a specific agent's identity, the agent's location at the \textit{next time step} , and the agent's trajectory at time step , i.e., .
Lastly,  is a starting location feature vector that contains information about a specific agent's identity and the agent's location at time step .
The  feature vectors are necessary so that \btvpp{} can ``see'' the initial locations of \textit{all the agents} when processing the agents agent-wise in the first time step.

These three sets of feature vectors are combined to form a  matrix  such that the first  rows consist of the   feature vectors, and the remaining  rows consist of the   and   feature vectors interleaved with one another, i.e., each  is followed by its corresponding  in the matrix.
This matrix is passed into the Transformer along with the specially designed self-attention mask, which encodes the following relationships (see Figure \ref{fig:baller2vec++}):

\begin{enumerate}
    \item When processing , \btvpp{} is exclusively allowed to ``look'' at each  (i.e., \btvpp{} cannot look at any location or look-ahead feature vectors when processing ).
    \item When processing , \btvpp{} is allowed to ``look'' at: (i) each , (ii) any  where (a)  \textbf{or} (b)  \textbf{and} , and (iii) any  where (a)  \textbf{or} (b)  \textbf{and} .
    \item When processing , \btvpp{} is allowed to ``look'' at: (i) each , (ii) any  where (a)  \textbf{or} (b)  \textbf{and} , and (iii) any  where (a)  \textbf{or} (b)  \textbf{and} .
\end{enumerate}

Each processed  feature vector is then passed through a linear layer followed by a softmax to classify the trajectory bin  (where  is an integer from one to ) for agent  at time step .
As with \btv{}, the loss for each sample is:



\noindent
where  is the probability assigned to the trajectory bin  by , i.e., Equation \eqref{eq:traj_loss} is the negative log-likelihood (NLL) of the data according to the model.

Because any ordering of a chain rule decomposition of a joint probability produces the same value, e.g.:



like \cite{yang2019xlnet}, we shuffled the order of the agents in each training sequence to encourage the model to learn joint probabilities of the agent trajectories that are permutation invariant with respect to the ordering of the agents. 
\section{Experiments}
We tested \btvpp{} on two different datasets.
To highlight the pathological behavior of models that assume agent trajectories are statistically independent at each time step, we trained scaled down versions of \btvpp{} and \btv{} on a toy dataset consisting of simulated trajectories for two perfectly coordinated agents.
Additionally, to demonstrate the efficacy of \btvpp{} in real world settings, we trained \btvpp{} and \btv{} on a dataset of trajectories for professional basketball players.

\subsection{Toy dataset}
Each training sample was initialized with the agents starting at  and  on a grid in random order (i.e., the first agent could be placed to either the left or the right of the origin).
At each time step, one of nine actions (corresponding to the 33 grid surrounding the agent) was sampled from a uniform distribution, and each of the agents was translated along this trajectory.
This process was repeated for 20 time steps (see Figure \ref{fig:toy}(a) for a sample).

\subsection{Basketball dataset}

We used the same National Basketball Association (NBA) dataset\footnote{\url{https://github.com/linouk23/NBA-Player-Movements}} employed by \citet{alcorn2021baller2vec}, whom we paraphrase here:

\begin{quote}
    The NBA dataset consists of trajectories from 631 games from the 2015-2016 season, which were split into 569/30/32 training/validation/test games, respectively.
    During training, each sequence was sampled using the following procedure: (1) randomly select a training game, (2) randomly select a starting time from the game, (3) take the following four seconds of data and downsample it to 5 Hz from the original 25 Hz, and then (4) randomly (with a probability of 0.5) rotate the court 180\degree.
    This sampling procedure gave us access to on the order of 82 million different (albeit overlapping) training sequences.
    For both the validation and test sets, 1,000 different, \textit{non-overlapping} sequences were selected for evaluation by dividing each game into  non-overlapping chunks (where  is the number of games), and using the starting four seconds from each chunk as the evaluation sequence.
\end{quote}

\subsection{Model}\label{sec:model}

Our \btvpp{} and \btv{} models for the basketball dataset closely followed \cite{alcorn2021baller2vec}, and so largely resemble the original Transformer architecture \cite{vaswani2017attention}.
Specifically, the Transformer settings were:  (the dimension of the input and output of each Transformer layer), eight attention heads,  (the dimension of the inner feedforward layers), six layers, no dropout, and no positional encoding.
Each MLP (i.e., , , and ) had 128, 256, and 512 nodes in its three layers, respectively, and a ReLU nonlinearity following each of the first two layers.
The player embeddings \cite{alcorn20182vec} had 20 dimensions, and  was a binary variable indicating the side of the frontcourt for player  (i.e., the direction of his team's hoop) at time step .
Lastly, the  2D Euclidean trajectory space was binned into 121  squares.

We used the Adam optimizer \cite{kingma2014adam} with an initial learning rate of , , , and  to update the model parameters, of which there were 19 million.
The learning rate was reduced to  after 20 epochs of the validation loss not improving.
Models were implemented in PyTorch and trained on a single NVIDIA GTX 1080 Ti GPU for 650 epochs (seven days) where each epoch consisted of 20,000 training samples, and the validation set was used for early stopping.

For the toy dataset, we used scaled down versions of the basketball models with , four attention heads, , and two layers in the Transformer.
Additionally, each MLP had two layers with 64 and 128 nodes, respectively.
The models were trained for 50 epochs of 500 samples per epoch (10.5 minutes) using a single learning rate of . 
\section{Results}
\subsection{\btvpp{} can effectively model coordinated agents in both simulated and real settings}\label{sec:effective}

For the toy dataset, the training loss for \btv{} converged to 2.2 , i.e., the model was simply independently guessing the trajectories for both agents at every time step.
In contrast, the training loss for \btvpp{} converged to 1.1 , which is the expected loss for a model that perfectly learns the deterministic relationship between the agents' trajectories (because the prediction for the second agent will always contribute  to the loss).
\begin{wrapfigure}{r}{0.7\linewidth}
\centering
\subfigure[Training sample.]{\includegraphics[width=0.2\columnwidth]{train_toy}}
\hskip 0.2cm
\subfigure[\btv{}.]{\includegraphics[width=0.2\columnwidth]{b2v_toy}}
\hskip 0.2cm
\subfigure[\btvpp{}.]{\includegraphics[width=0.2\columnwidth]{b2v++_toy}}
\caption{When trained on a dataset of perfectly coordinated agent trajectories (a),  the trajectories generated by \btv{} are completely \textit{uncoordinated} (b) while the trajectories generated by \btvpp{} are perfectly coordinated (c).
Animated versions can be found in the code repository.
}
\vskip -0.1in
\label{fig:toy}
\end{wrapfigure}
When generating trajectories with \btv{}, the agents are completely uncoordinated, with each agent following an independent random walk around the grid (Figure \ref{fig:toy}(b)).
In contrast, trajectories generated by \btvpp{} display the same coordinated agent behavior as the training data (Figure \ref{fig:toy}(c)).

For the basketball dataset, \btvpp{} achieved an average NLL of 0.472 on the test set, 8.9\% better than the average NLL for \btv{} (0.518).
As can be seen in Figure \ref{fig:basketball}, the trajectories generated by \btvpp{} are often more realistic than those generated by \btv{}.
As was observed in \cite{alcorn2021baller2vec}, the trajectory forecast distributions for \btv{} become much more certain after observing a portion of the sequence (Figure \ref{fig:plots}), which suggests \btv{} may be inferring some of the shared hidden variables (e.g., plays) influencing the players.
If that hypothesis was true, the performance gap between \btvpp{} and \btv{} should be largest at the beginning of the sequence (before any shared hidden variables can be inferred by \btv{}).
Indeed, the average NLL for \btvpp{} in the \textit{first time step} of each test set sequence (1.567) is 16.1\% better than the average NLL for \btv{} (1.869), while the average NLL for \btvpp{} in the \textit{last time step} of each test set sequence (0.420) is only 9.7\% better than the average NLL for \btv{} (0.465) (see Figure \ref{fig:plots}).

\begin{figure}[h]
\centering
\subfigure[Ground truth.]{\includegraphics[width=0.19\columnwidth]{267_3_truth}}
\subfigure[\btv{}.]{\includegraphics[width=0.19\columnwidth]{267_3_gen_baller2vec_7}}
\subfigure[\btv{}.]{\includegraphics[width=0.19\columnwidth]{267_3_gen_baller2vec_1}}
\subfigure[\btvpp{}.]{\includegraphics[width=0.19\columnwidth]{267_3_gen_baller2vec++_7}}
\subfigure[\btvpp{}.]{\includegraphics[width=0.19\columnwidth]{267_3_gen_baller2vec++_8}}
\caption{While \btv{} occasionally generates realistic trajectories for the red defender (b), it also makes egregious errors (c).
In contrast, the trajectories generated by \btvpp{} often seem plausible (d and e).
For both the \btv{} and \btvpp{} generated trajectories, the ground truth trajectories (a) for all of the non-red players were used as input at each time step.
The red player was placed \textit{last} in the player order when generating his trajectory with \btvpp{}.
Animated versions can be found in the code repository.
}
\label{fig:basketball}
\end{figure}

\begin{figure}[h]
\centering
\subfigure{\includegraphics[width=\columnwidth]{plots}}
\caption{\textbf{Left}: when modeling the trajectories of professional basketball players, the performance gap between \btvpp{} and \btv{} is largest at the beginning of the sequence, before shared unobserved variables can be inferred by \btv{}.
Each bar indicates a model's average NLL over the entire test set for that particular time step.
For full sequences, \btvpp{} outperforms \btv{} by 8.9\%.
\textbf{Right}: the joint probability assigned to a sequence by \btvpp{} is approximately permutation invariant with respect to the order of the agents.
For each point, its  value indicates \btvpp{}'s average NLL for a test set sequence using the \textit{original} order of the agents in the sequence, while its  value indicates \btvpp{}'s average NLL \textit{for the same sequence} with the order of the agents \textit{shuffled}.
The shuffled average NLLs are highly correlated with their corresponding unshuffled average NLLs.
}
\label{fig:plots}
\end{figure}

\subsection{\btvpp{} makes better forecasts when conditioned on concurrent trajectory information from other agents}\label{sec:conditioning}

Implicit in much of our discussion has been the intuition that providing a model with additional (relevant) information will improve its performance.
To empirically test this conjecture, we compared the performance of \btvpp{} when predicting the trajectory of a specific basketball player placed in the \textit{first position} of the player order (i.e., when ) vs. predicting the trajectory for that \textit{same player} placed in the \textit{last position} (i.e., when ).
Specifically, for each player in each test sequence, we calculated the NLL of the player's trajectory in the first time step\footnote{Because, as previously discussed, the benefits of \btvpp{} were most pronounced in the first time step.} with the player in the \textit{first position} of the player order.
Next, we moved the player to the \textit{last position} of the player order, and then randomly shuffled the remaining nine players 10 times, calculating the NLL for the player in the last position each time.
Finally, we calculated the average percent change in the last position NLLs relative to their corresponding first position NLLs.
On average, moving a player from the first to the last position improved the NLL for the player's trajectory by 14.6\%.

\subsection{The joint probability assigned to a sequence by \btvpp{} is approximately permutation invariant with respect to the order of the agents}\label{sec:permutation}

To determine whether or not \btvpp{} respects the fact that any ordering of a chain rule decomposition of a joint probability produces the same value, we measured how much the average NLL for each test sequence in the basketball dataset varied when the order of the agents changed.
Specifically, for each test set sequence, we shuffled the order of the agents 10 times.
Then, for each permuted sequence, we calculated the percent error\footnote{See: \url{https://en.wikipedia.org/wiki/Approximation_error\#Formal_Definition}.} in the average NLL relative to the original, \textit{unshuffled} sequence.
Across all test sequences, the average percent error was only \%.
Further, as can be seen in Figure \ref{fig:plots}, the shuffled average NLLs are highly correlated with their corresponding unshuffled average NLLs (Pearson correlation coefficient = 0.997), i.e., the joint probability assigned to a sequence by \btvpp{} is approximately permutation invariant with respect to the order of the agents. 
\section{Conclusion and Future Work}\label{sec:conclusion}
In this paper, we have shown how the commonly used independence assumption of many multi-agent spatiotemporal models can severely limit their ability to learn to emulate coordinated agents.
By relaxing this independence assumption in \btv{}, \btvpp{} was able to more accurately model the trajectories of professional basketball players.
Models for other multi-agent spatiotemporal environments, such as pedestrian traffic (see \cite{rudenko2020human} for a survey) and vehicle traffic (e.g., \cite{deo2018convolutional, Chang_2019_CVPR, Zhao_2019_CVPR, Chandra_2019_CVPR}), may also benefit from the look-ahead approach used by \btvpp{}.
Additionally, the interleaved input design of \btvpp{} could be useful when modeling other systems involving many entities interacting through time, such as social media platforms (e.g., \cite{tgn_icml_grl2020}).
However, confronting the quadratic complexity of the Transformer attention mechanism as the number of entities grows large in these datasets is an open problem, but recent work in sparse Transformers (e.g., \cite{child2019generating, NEURIPS2020_c8512d14, beltagy2020longformer, Kitaev2020Reformer:}) shows encouraging progress. 
\section{Acknowledgements}
We would like to thank Jan Van Haaren for his helpful suggestions on how to improve the manuscript.

\bibliography{bib}
\bibliographystyle{unsrtnat}



\end{document}