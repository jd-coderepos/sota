

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{times}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{bm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\dpar}[2]{\frac{\partial{#1}}{\partial{#2}}}

\newcommand{\thetab}{\bm{\theta}}
\newcommand{\betab}{\bm{\beta}}
\newcommand{\mub}{\bm{\mu}}
\newcommand{\mypar}[1]{\vspace{2mm}\noindent\textbf{#1}}
\newcommand{\Beta}{B}
\newcommand{\Betab}{\mathbf{\Beta}}
\newcommand{\Thetab}{\mathbf{\Theta}}

\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bomega}{\boldsymbol{\omega}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}


\newcommand{\sss}{\mathbf{s}}
\newcommand{\cb}{\mathbf{c}}
\newcommand{\tb}{\mathbf{t}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\nn}{\mathbf{n}}
\newcommand{\mm}{\mathbf{m}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\ff}{\mathbf{f}}
\newcommand{\rr}{\mathbf{r}}
\newcommand{\ttt}{\mathbf{t}}
\newcommand{\hh}{\mathbf{h}}
\newcommand{\qq}{\mathbf{q}}

\newcommand{\FF}{\mathbf{F}}
\newcommand{\AB}{\mathbf{A}}
\newcommand{\DD}{\mathbf{D}}
\newcommand{\KK}{\mathbf{K}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Jb}{\mathbf{J}}
\newcommand{\Sb}{\mathbf{S}}
\newcommand{\TT}{\mathbf{T}}
\newcommand{\RR}{\mathbf{R}}
\newcommand{\RRp}{\mathbf{R}^{\prime}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Pc}{\mathcal{P}}

\newcommand{\Ms}{\mathbf{M_{s}}}
\newcommand{\Mt}{\mathbf{M_{t}}}
\newcommand{\MM}{\mathbf{M}}
\newcommand{\MMps}{\mathbf{M_{s}^{\prime}}}
\newcommand{\MMpt}{\mathbf{M_{t}^{\prime}}}
\newcommand{\Vs}{\mathbf{V_{s}}}
\newcommand{\Vt}{\mathbf{V_{t}}}
\newcommand{\VVps}{\mathbf{V_{s}^{\prime}}}
\newcommand{\VVpt}{\mathbf{V_{t}^{\prime}}}
\newcommand{\Fs}{\mathbf{F_{s}}}
\newcommand{\Ft}{\mathbf{F_{t}}}
\newcommand{\abt}{\mathbf{a}_{t}}
\newcommand{\abs}{\mathbf{a}_{k}}
\newcommand{\fbt}{\mathbf{f}_{t}}
\newcommand{\fbs}{\mathbf{f}_{k}}
\newcommand{\vbs}{\mathbf{v}_{k}}
\newcommand{\vbstilde}{\widetilde{\mathbf{v}}_{k}}
\newcommand{\vbpstilde}{\widetilde{\mathbf{v}}_{k}^{\prime}}
\newcommand{\VVstilde}{\widetilde{\mathbf{V}}_{s}}
\newcommand{\VVpstilde}{\widetilde{\mathbf{V}}_{s}^{\prime}}
\newcommand{\vbt}{\mathbf{v}_{t}}
\newcommand{\VV}{\mathbf{V}}
\newcommand{\JJ}{\mathbf{J}}

\usepackage{xspace}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{8459} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{HSPACE: Synthetic Parametric Humans Animated in Complex Environments}

\author{
Eduard Gabriel Bazavan\quad Andrei Zanfir\quad Mihai Zanfir \\ William T. Freeman \quad Rahul Sukthankar \quad Cristian Sminchisescu\\
\and
{\bf Google Research}\\
{\tt\small \{egbazavan, andreiz, mihaiz, wfreeman, sukthankar, sminchisescu\}@google.com}\\
}


\twocolumn[{\renewcommand\twocolumn[1][]{#1}\maketitle
\begin{center}
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/suntemple_sample_0025.jpeg}
    \captionof{figure} {\small HSPACE contains dynamic scenes with multiple moving people, with diverse body shapes and poses, placed in realistic environments, under complex lighting. Human animations are driven by GHUM\cite{ghum2020}. For all frames we provide 3d pose and shape ground truth, as well as other rich image annotations including human segmentation, body part localisation semantics, and temporal correspondences.}
    \label{fig:teaser_figure}
\end{center}
}]

\begin{abstract}
   Advances in the state of the art for 3d human sensing 
are currently limited by the lack of visual datasets with 3d ground truth, including multiple people, in motion, operating in real-world environments, with complex illumination or occlusion, and potentially observed by a moving camera. Sophisticated scene understanding would require estimating human pose and shape as well as gestures, towards representations that ultimately combine useful metric and behavioral signals with free-viewpoint photo-realistic visualisation capabilities. To sustain progress, we build a large-scale photo-realistic dataset, Human-SPACE (HSPACE), of animated humans placed in complex synthetic indoor and outdoor environments. We combine a hundred diverse individuals of varying ages, gender, proportions, and ethnicity, with hundreds of motions and scenes, as well as parametric variations in body shape (for a total of 1,600 different humans), in order to generate an initial dataset of over 1 million frames. Human animations are obtained by fitting an expressive human body model, GHUM, to single scans of people, followed by novel re-targeting and positioning procedures that support the realistic animation of dressed humans, statistical variation of body proportions, and jointly consistent scene placement of multiple moving people. Assets are generated automatically, at scale, and are compatible with existing real time rendering and game engines. The dataset\footnote{\url{https://github.com/google-research/google-research/tree/master/hspace}} with evaluation server will be made available for research. Our large-scale analysis of the impact of synthetic data, in connection with real data and weak supervision, underlines the considerable potential for continuing quality improvements and limiting the sim-to-real gap, in this practical setting, in connection with increased model capacity.
\end{abstract}

\section{Introduction}

Progress in 3d human pose and shape estimation has been sustained over the past years as several statistical 3d human body models\cite{ghum2020,pavlakoscvpr2019,SMPL2015}, as well as learning and inference techniques have been developed\cite{sminchisescu_ijrr03,bogo2016,SMPL2015,ghum2020,dmhs_cvpr17,zanfir2018monocular,Rhodin_2018_ECCV,Kanazawa2018,kolotouros2019learning,ExPose:2020,zanfir2020neural}. More recently there has been interest in human interactions, self-contact \cite{Mueller:CVPR:2021, fieraru2021learning}, and human-object interactions, as well as in frameworks to jointly reconstruct multiple people\cite{jiang2020coherent, zhang2021body, zanfir2018monocular, fieraru2021remips}. As errors steadily decreased on some of the standard 3d estimation benchmarks like Human3.6M\cite{Ionescu14pami} and HumanEva\cite{sigal2010humaneva}, other laboratory benchmarks recently appeared \cite{Fieraru_2020_CVPR,fieraru2021learning}, more complex in terms of motion, occlusion and scenarios, \eg capturing interactions. 

While good quality laboratory benchmarks remain essential to monitor and track progress, as a rich source of motion data to construct pose and dynamic priors, or to initially bootstrap models trained on more complex imagery, overall there is an increasing need to bridge the gap between the inevitably limited subject, clothing, and scene diversity of the lab, and the complexity of the real world. It is also desirable to go beyond skeletons and 3d markers towards more holistic models of humans with estimates of shape, clothing, or gestures. While several recent self-supervised and weakly-supervised techniques emerged, with promising results in training with complex real-world image data\cite{zanfir2020weakly,joo2020exemplar,kolotouros2019learning}, their quantitative evaluation still is a challenge, as accurate 3d ground truth is currently very difficult to capture outside the lab. This either pushes quantitative assessment back to the lab, or makes it dominantly qualitative and inherently subjective. It is also difficult to design visual capture scenarios systematically in order to improve performance, based on identified failure modes.

HSPACE (Synthetic Parametric Humans Animated in Complex Environments) is a large-scale dataset that contains high resolution images and video of multiple people together with their ground truth 3d representation based on GHUM -- a state-of-the art full-body, expressive statistical pose and shape model. HSPACE contains multiple people in diverse poses and motions (including hand gestures), at different scene positions and scales, and with different body shapes, ethnicity, gender and age. People are placed in synthetic complex scenes and under natural or artificial illumination, as simulated by accurate light transport algorithms. The dataset also features occlusion due to other people, objects, or the environment, and camera motion. 

In order to produce HSPACE, we rely a corpus of 100 diverse 3d human scans (purchased from RenderPeople\cite{Renderpeople.com}), with parametric varying shape, animated with over a 100 real human motion capture snippets (from the CMU human motion capture dataset), and placed in 100 different synthetic indoor and outdoor environments, available from various free and commercial sources. We automatically animate the static human scans and we consistently place multiple people and motions, sampled from our asset database, into various scenes. We then render the resulting scenes for different cinematic camera viewpoints at 4K/HDR, using a realistic, high-quality game-engine.

Our contribution is the construction of a large scale automatic system, which requires considerable time as well as human and material resources in order to perfect. The system supports our construction of a 3d dataset, HSPACE, unique in its large-scale, complexity and diversity, as well as accuracy and ground-truth granularity. Such features complement and considerably extend the current dataset portfolio of our research community, being essential for progress in the field. To make the approach practical and scalable we also develop: (1) procedures to fit GHUM to complex 3d human scans of dressed people with capacity to retarget and animate both the body and clothing, automatically, with realistic results, and (2) automatic 3d scene placement methodology to temporally avoid collisions between people, as well as between people and the environment. Finally, we present large-scale studies revealing insight into practical uses of synthetic data, the importance of using weakly-supervised real data in bridging the sim-to-real gap, and the potential for improvement as model capacity increases. The dataset and an evaluation server will be made available for research and performance evaluation.

\section{Related Work}
    There are quite a few people datasets with various degrees of supervision: 2d joint
annotations, semantic segmentations \cite{MsCOCO, OpenImages}, or 3d by fitting a statistical body model or from multi-camera views\cite{zhang2020object, STRAPS2020BMVC, joo2020exemplar, mehta2017monocular},  dense pose \cite{Guler2018DensePose}, indoor mocap datasets with 3d pose ground truth for single or multiple people \cite{sigal2010humaneva, Ionescu14pami, Fieraru_2020_CVPR, fieraru2020three, fieraru2021learning, fieraru2021aifit}, in the wild datasets where IMUs and mobile devices were used to recover 3d pseudo ground truth joints \cite{vonMarcard2018}. All these datasets contain real images, however the variability of the scenes and the humans is limited and the 3d ground truth accuracy is subject to annotators bias, joint positioning errors (for mocap) or IMUs sensor data optimization errors. It is also difficult to increase the diversity of a real dataset, as one cannot capture the same exact sequence from e.g. a different camera viewpoint. 

In order to address the above-mentioned issues, efforts have been made to generate data synthetically using photorealistic 3d assets (scenes, characters, motions). Some synthetic datasets compose statistical body meshes or 3d human scans with realistic human textures on top of random background images, HDRI backdrops or 3d scenes with limited variability \cite{varol2017learning, yan2021ultrapose, Patel:CVPR:2021, zhu2020simpose}, or rely on game engine simulations to recover human motions and trajectories \cite{caoHMP2020}. Table \ref{tbl:datasets_comparison} reviews some of the most popular datasets along several important diversity axes.
Our HSPACE dataset addresses some of the limitations in the state of the art by diversifying over people, poses, motions and scenes, all within a realistic rendering environment and by providing a rich set of 2d and 3d annotations. 

\begin{table*}[!htbp]
\setlength{\tabcolsep}{0.8em} 
\begin{center}
\scalebox{0.78}{
\begin{tabular}{lllllllllll}
Dataset            & \#Frames & \#Views & \#Subj.          & \#Motions & Complexity                                        & Image      & GT format           \\ \hline\hline
HumanEva \cite{sigal2010humaneva}          &          &     4/7     & 4                 &      6      & 1 subject, no occlusion                           & lab        & 3DJ  \\
Human3.6m  \cite{Ionescu14pami}        &          &     4     & 11                &       15     & 1 subject, minor occlusion                        & lab        & 3DJ, GHUM/L  \\ 
CHI3D \cite{fieraru2020three}             &          &     4     &       6            &        120    &         multiple interacting subjects, lab                                          &     lab       &           3DJ, GHUM/L. CS         \\
HumanSC3D \cite{fieraru2021learning}         &          &     4     &         6          & 172           &     1 subject, frequent self-contact                                              &     lab       &            3DJ, GHUM/L, CS         \\
Fit3D \cite{fieraru2021aifit}             &           &      4    &        13           &  47  &     1 subject, extreme poses                    &      lab      &        3DJ, GHUM/L             \\
TotalCapture \cite{trumble2017total}       &          &      8    & 5                 &       10     & 1 subject, no occlusion                           & lab        & 3DJ  \\
PanopticStudio \cite{Joo_2015_ICCV}     &          &          &         &             & multiple subjects, furniture                      & lab        & 3DJ  \\
HUMBI \cite{yu2020humbi}             &          &          & 772               &       772     & 1 subject, no occlusion                           & lab        & meshes, SMPL        \\
3DPW \cite{vonMarcard2018}              &          &          & 18                &            & multiple subjects in the wild                     & natural    & SMPL                \\
MuPoTS-3D \cite{singleshotmultiperson2018}         &          &          & 8                 &            & multiple subjects in the wild                     & natural    & 3DJ  \\
EFT \cite{joo2020exemplar}               &          &       1   & \textgreater 1000 &    0        & multiple subjects, in the wild                    & natural    & SMPL                \\
STRAPS \cite{STRAPS2020BMVC}      &    331      &   1    & 62                &     0       & 1 subject, in the wild                            & natural    & SMPL                \\ \hline
MPI-INF-3DHP-Train \cite{mehta2017monocular} &           &             & 8                 &       8     & 1 subject, minor occlusion                        & composite & 3DJ  \\
SURREAL \cite{varol2017learning}           &          &       1   & 145               &      \textgreater{}2000      & 1 subject, no occlusion                           & composite & SMPL                \\
3DPeople \cite{pumarola20193dpeople}           &     2.5M     &   4  & 80                &      70      & 1 subject, no occlusion                           & composite & 3DJ  \\
UltraPose\cite{yan2021ultrapose}              &          &     1     & \textgreater{}1000 & 0 & 1 subject, minor occlusions & composite & DeepDaz, DensePose \\
AGORA\cite{Patel:CVPR:2021}              &          &  1        & \textgreater{}350 &      0      & multiple subjects, occlusion & realistic & SMPL-X, masks \\
\hline
\textbf{HSPACE}            &           &  5 (var)        &    10016               & 100            & multiple subjects, occlusion                                                  &  realistic          & GHUM/L, masks \\
\hline
\end{tabular}
}
\end{center}
\vspace{-5mm}
\caption{\small Comparison of different human sensing datasets. From left to right columns represent dataset name, number of different frames, average number of views for each frame, number of different subjects, number of motions, the complexity of the scenes, whether the images are captured in indoor lab environments, in the wild natural scenes or are a composite of synthetic and natural images, as well as the type of ground truth offered e.g. 3d joints, type of statistical body mode (SMPL or GHUM), or 3d surface contact signatures (CS).}
\label{tbl:datasets_comparison}
\end{table*}

\section{Methodology}

Our methodology consists of (1) procedures to fit the GHUM body model to a dressed human scan, as well as realistically repose and reshape it (repose and reshape logic), and (2) methods to place multiple moving (dressed) scans animated using GHUM, into a scene in a way that is physically consistent so that people do not collide with each other and with the environment (dynamic placement logic).

\noindent{\bf Statistical GHUM Body Model.} We rely on GHUM \cite{ghum2020}, a recently introduced statistical body model in order to represent and animate the human scans in the scene. The shape space  of the model is represented by a variational auto-encoder.
The pose space  is represented using normalizing flows \cite{zanfir2020weakly} with separate components for global rotation   \cite{zhou2018continuity} and translation . The output of the model is a mesh , where  are the vertices and  are the  faces.

\subsection{Fitting GHUM to Clothed Human Scans}
The first stage in our pipeline is to fit the GHUM\cite{ghum2020} model to an initial 3d scan of a person  containing  vertices , faces  and texture information  containing per vertex  coordinates and normal, diffuse and specular maps. The task is to find a set of parameters  such that the target GHUM\cite{ghum2020} mesh  is an accurate representation of the underlying geometry of . For the sake of simplicity, we drop the dependence on the parameters  and . As illustrated in fig. \ref{fig:fitting_pipeline}, we uniformly sample camera views around the subject and render it using the texture information associated to . Image keypoints for the body, face, and hands are predicted for each view using a standard regressor \cite{ghum2020,bazarevsky2020blazepose} and we triangulate to obtain a 3d skeleton  for the source mesh. The fitting procedure of the GHUM mesh  to  is formulated as a nonlinear optimization problem with the following objective

In \eqref{eq:fitting},  are the skeleton joints for the posed mesh  and   is the 3d mean per joint position error between the 3d joints of the source and those of the target.  is an adaptive iterative closest point loss between the target vertices  and the source vertices . At each optimization step we split the vertices  into two disjoint subsets: the vertices  that are inside  and the vertices  which are outside of . In order to classify a vertex as inside or outside, we rely on a fast implementation of the generalized winding number test \cite{Jacobson:WN:2013, fieraru2021remips}. Given the closest distance  between a point  and a vertex set 

we define  as follows


We set , enforcing the reconstructed mesh  to be inside of , but close to the surface. We add regularization for pose and shape based on their native latent space priors   in order to penalize deviations from the mean of their Gaussian distributions.

\subsection{Reposing and Reshaping Clothed People}
\label{sec:reposing}
\begin{figure}[!htbp]
\begin{center}
    \includegraphics[width=0.9\linewidth]{Figures/local_coordinate.png}
\end{center}
\vspace{-4mm}
\caption{\small \textbf{Reposing and Reshaping Clothed People.} We compute displacements from GHUM to the scanned mesh in a local coordinate system. For each vertex of the scan, we consider its nearest neighbor point on the GHUM mesh. This point is parameterized by barycentric coordinates. When the GHUM mesh is generated for different pose and shape parameters, its local geometry rotates and scales. We want displacements between the scan and the updated GHUM geometry be preserved. We use a tangent space coordinate system that allows equivariance to rotations. Furthermore, due to the way the tangent space is computed, based on triangle surface area, we are also invariant to scale deformations.}
\label{fig:local_coordinate}
\end{figure}
We design an automated process of generating large-scale animations of the same subject's scan, but with different shape characteristics.
We need the animation process to be compatible with LBS pipelines, such as Unreal Engine, in order to automate the rigging and rendering process for large-scale data creation. This is a non-physical process in the absence of explicit clothing models, but we aim for automation and scalability, rather than perfect simulation fidelity. 
We aim not only for animation diversity, but also for shape diversification. We support transformations in the tangent-space of local surface geometry  that can accommodate changes in both shape and pose – this is different from inverse skinning methods ~\cite{huang2020arch} that only handle the latter.

\vspace{3mm}

\noindent{\bf Tangent-space representation.} Given a source scan with mesh  and its fitted GHUM mesh , we compute a displacement field  from  to . For each vertex  we compute its closest point  on  and we denote by  its barycentric coordinates on the projection face . From all values  we build a sparse connection matrix . The displacement field 
 from  to  is defined as 

where  are all the stacked closest points  and . 

We want each of the displacement vectors  in  to reside in a local coordinate system determined by the supporting local geometry . Hence, we compute associated normal , tangent  and bitangent  vectors. The normals and tangents are interpolated given per-vertex information available for the faces , . Per-vertex tangents are a function of the UV coordinates. For more details on the usage of UV coordinates to obtain tangents, see \cite{premecz2006iterative}. After Gram–Schmidt orthonormalization of tangents and normals, we derive a rotation matrix  representing a local coordinate system for each displacement vector . We stack the rotation matrices for all displacement vectors and construct .

\noindent{\bf Controlling shape and pose.} For a target set of pose and shape parameters  of GHUM, let  be the new target GHUM posed mesh with vertices . The task is to find  which would correspond to the same change in pose and shape for . For that, we first compute . Using  and  we get updated local orientations  for each  from the normal, tangent and bitangent vectors similarly to . Note  gives the change of orientation for the supporting faces  from  to . We use them to compute the change in orientation for the displacement field  

and obtain the corresponding mesh . 

\paragraph{Rendering engine compatibility} Rendering engines use linear blend skinning to display realtime realistic animations, so we cannot incorporate tangent-space transformations to drive the animation. Instead, we use tangent-space transformations to compute a new target rest mesh (this is equivalent to unposing and reshaping), with different body shapes sampled from the latent distribution of the GHUM model, and then continue the animation by LBS. We compute the skinning weights for  as , where  are the skinning weights for . The skeleton animation posing values, skinning matrix  and updated rest mesh  are sufficient for animation export.

The limitations of our animation method lie in the hair or clothing simulation which lacks physical realism. However, this geometric animation process is efficient and easy to compute and, as can be seen in fig. \ref{fig:sample_sequences}, results are visually plausible within limits. Our quantitative experiments show that such synthesis methodology improves performance on challenging tasks like 3d pose and shape estimation.

\subsection{Scene Placement Logic}
\label{sec:scene_placement}
In order to introduce multiple animated scans into scenes, we develop a methodology for automatic scene placement based on free space calculations. Typically, we sample several people, their shape, and their motions as well as a bounded, square region of the synthetic scene, so it can be comfortably observed by 4 cameras placed in the corners of the square at different elevations. This is important as some synthetic scenes could be very large, and sampling may generate people spread too far apart or not even visible in any of the virtual cameras. 

The union of tightly bounding parallelepipeds for each human shape at each timestep of their animation defines a 
\emph{motion volume}. These are aligned with a global three-dimensional grid. 
The objective is to estimate a set of positions and planar orientations for the motion volumes, such that no two persons occupy the same unit volume at the same motion timestep (as otherwise trajectories from different people at different timesteps can collide).

Given a scene (3d bounding boxes around any objects including the floor/ground), we sample a set of random motion volumes and initially place them into the scene such that the mid point of the motion paths is in the middle of the scene. We define a loss function which is the sum of {\it a)} number of collision between the sequences (defined as their time-varying 3d bounding boxes intersecting or intersecting with object bounding boxes) and {\it b)} the number of time steps when they are outside the scene bounding box. 


The input to the loss function is a set of per-sequence translation variables, as well as rotations around the axis of ground normal. We then minimize this loss function using a non-differentiable covariance matrix adaptation optimization method (CMA)\cite{Hansen2006} over the initial translation and rotation of the motion volumes, 
and only accept solutions where the physical loss is 0 (\ie has no collisions and all sequences are inside the scene bounding box). While the scene placement model can be improved in a number of ways, including the use of physical models or environmental semantics it provides an automation baseline for initial synthesis.
See fig.\ref{fig:placement} for an illustration.

\begin{figure}[!htbp]
\begin{center}
    \includegraphics[width=\linewidth]{Figures/placement_all_numbered.png}
\end{center}
\vspace{-4mm}
\caption{\small Dynamic placement logic ensures that multiple moving people follow plausible human motions, and are positioned in a scene in way that is consistent with spatial occupancy from other objects or people. An optimization algorithm ensures no two people occupy the same scene location at the same motion timestep. Trajectories are shown in color, with start/end denoted by A/B.}
\label{fig:placement}
\end{figure}


\noindent{\bf Automatic Pipeline.} We designed a pipeline, such that given a query for a specific body scan asset, animation and scene, we produce a high quality rendering placed automatically, at a physically plausible location.


\subsection{HSPACE dataset}

\noindent{\bf Dataset Statistics.} Our proposed HSPACE dataset was created by using  unique photogrammetry scans of people from the commercial dataset RenderedPeople~\cite{renderpeople}. We reshape the scans using our proposed methodology (see section~\ref{sec:reposing}), with  uniformly sampled shape parameters sampled from GHUM's VAE shape space. For animation, we use  CMU motion capture sequences for which we have corresponding GHUM pose parameters. For background variation, we use  complex, good quality 3d scenes. These include both indoor and outdoor environments. To create a sequence in our dataset, we randomly sample from all factors of variation and place the animations in the scene using our scene placement method (see section~\ref{sec:scene_placement}). In total, we collect  unique rendered frames, each consisting of  subjects on average. An example of a scene with multiple dynamic people is shown in fig.\ref{fig:teaser_figure}.


\noindent{\bf Rendering.} HSPACE images and videos are rendered using Unreal Engine 5 at 4k resolution. The rendering uses ray-tracing, high resolution light mapping, screen-space ambient occlusion, per-category shader models (e.g. Burley subsurface scattering for human skin), temporal anti-aliasing and motion blur. For each frame we capture the ground truth 3d pose of the various people inserted in the scene and save render passes for the finally rendered RGB output, as well as segmentation masks. On average, our system renders at 1 frame/s including saving data on disk. All of our dataset was rendered on 10 virtual machines with GPU support running in the cloud.

\begin{figure*}[!htbp]
\begin{center}
    \includegraphics[width=0.78\linewidth]{Figures/appearance_shape.png}
\end{center}
\vspace{-4mm}
\caption{\small Three scans with different appearance and body mass index, synthesised using GHUM statistical shape parameters, based on a single scan of each subject. Notice plausible body shape variations and reasonable automatic clothing deformation as body mass varies.}
\label{fig:appearance_shape}
\end{figure*}

\begin{figure*}[!htbp]
\begin{center}
    \includegraphics[width=0.78\linewidth]{Figures/processing_pipeline.png}
\end{center}
\vspace{-4mm}
\caption{\small Main processing pipeline for our synthetic human animations. Given a single 3d scan of a dressed person, we automatically fit GHUM to the scan, and build a representation that supports the plausible animation of both the body and the clothing based on different 3d human motion capture signals. Shape can be varied too -- notice also plausible positioning for the fringes of the long blouse outfit.}
\label{fig:fitting_pipeline}
\end{figure*}


\begin{figure*}[!htbp]
\begin{center}
    \includegraphics[width=0.94\linewidth ]{Figures/animation_sequence_00.jpeg}
    \includegraphics[width=0.94\linewidth]{Figures/animation_sequence_01.jpeg}
\end{center}
\vspace{-4mm}
\caption{\small Frames from HSPACE sequences with companion GHUM ground truth. Highly dynamic motions work best with characters wearing tight fitted clothing, the animated sequences look natural and smooth (bottom rows) but also notice good performance for less tight clothing (top rows). See our Sup. Mat. for videos.}
\label{fig:sample_sequences}
\end{figure*}

\begin{figure*}[!htbp]
\begin{center}
    \includegraphics[width=0.96\linewidth]{Figures/sample_render_passes2.jpeg}
\end{center}
\vspace{-4mm}
\caption{\small Human scans animated and placed in complexly lit 3d scenes with background motion (e.g. curtains, vegetation).}
\label{fig:sample_render_passes}
\end{figure*}


\section{Experiments}

We validate the utility of HSPACE for both training and evaluation of 3d human pose and shape reconstruction models.
We split HSPACE 80/20 into training and testing subsets, respectively. We use different people and animation assets for each split.

We additionally employ a dataset with images in-the-wild, \textbf{Human Internet Images (HITI)} (100,000 images), of more than 20,000 different people performing activities with highly complex poses (e.g. yoga, sports, dancing). This dataset was collected in-house and is annotated with both 2d keypoints and body part segmentation. We use it our experiments for training in a weakly supervised regime. The test version of this dataset, \textbf{Human Internet Images (HITI-TEST)}, consists of 40,000 images with fitted GHUM parameters under multiple depth-ordering constraints that we can use as pseudo ground-truth for evaluation in-the-wild (see our Sup. Mat. for details). 

\noindent{\bf Evaluation of GHUM Fitting to Human Scans.} In order to evaluate our GHUM fitting procedure, we compute errors of the nonlinear optimization fit in \eqref{eq:fitting} with keypoints only (), as well as for the full optimization () with  as well. Results are given in table \ref{tbl:ghum_fitting_errrors}.

\begin{table}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|}
    \hline
    \textbf{Fitting Method}  & V2V & Chamfer \\
    \hline
     &  &  \\
    \hline
     &  &  \\
    \hline
    \end{tabular}

    \caption{\small Fitting evaluation with vertex to vertex errors and bidirectional Chamfer distance. Values are reported in mm. Please see fig. \ref{fig:sample_sequences} and our Sup. Mat. for qualitative visualizations.}
\label{tbl:ghum_fitting_errrors}
\end{table}


In all experiments we train models for 3d human pose and shape estimation based on the THUNDR architecture\cite{Zanfir_2021_ICCV}. We report standard 3d reconstruction errors used in the literature: mean per joint position errors with and without Procrustes alignment (MPJPE, MPJE-PA) for the 3d pose and mean per vertex errors with and without Procrustes alignment (MPVPE, MPVPE-PA) for the 3d shape, as well as global translation errors. 

We present the experimental results on the test set of \textbf{HSPACE} in table \ref{tbl:ablation_space}. First we report results for various state of the art 3d pose and shape estimation models such as HUND\cite{zanfir2020neural}, THUNDR\cite{Zanfir_2021_ICCV}, SPIN\cite{kolotouros2019learning} and VIBE\cite{kocabas2019vibe}. The first two methods estimate GHUM mesh parameters, while the last two methods output SMPL mesh parameters. Both SPIN\cite{kolotouros2019learning} and VIBE\cite{kocabas2019vibe} use orthographic projection camera models so we can not report translation errors. We train a  weakly supervised (WS) version  of THUNDR on the HITI training dataset and fine tune it on HSPACE in a fully supervised (FS) regime. This model performs better than all other state of the art methods. The best reconstruction results are obtained by a modified temporal version of THUNDR (labeled as T-THUNDR in table \ref{tbl:ablation_space}) with the same number of parameters as the single-frame version. We provide details of this architecture in the Sup. Mat.

We also train and evaluate on a widely used dataset in the literature, the \textbf{Human3.6M}~\cite{Ionescu14pami} dataset. 
This is an indoor benchmark with ground-truth 3d joints obtained from a motion capture system. 
We report results on protocol P1 (100,000 images) where subjects S1, S5-S8 are used for training, and subjects S9 and S11 are used for testing. In table~\ref{tbl:H36MP1} we show that a refined variant of the THUNDR\cite{Zanfir_2021_ICCV} architecture on HSPACE training data  achieves the lowest reconstruction errors under all metrics. 

We also performed a comprehensive study in order to understand the impact of increasing the size of synthetic data on model performance. Other important factors are the sim-to-real gap, the importance of real data, and the influence of model capacity on performance. One of the most practical approaches would be to use large amounts of supervised synthetic data, as well as potentially large amounts of real images without supervision. The question is whether this combination would help and how would the different factors (synthetic data, real data, model capacity, initialisation and curriculum ordering) play on performance.

We trained a battery of models with different fractions of weakly supervised real data (10\%, 30\% or 100\% of HITI-TRAIN), fully supervised synthetic data (0\%, 10\%, 30\%, 60\%, 100\% of HSPACE-TRAIN), and for two model sizes (small THUNDR model with a transformer component of 1.9M parameters, and a big THUNDR model with a transformer component of 3.8M parameters). All models were evaluated on HSPACE-TEST (first and second columns in figure \ref{fig:thundr_ws_fs_ablations}) as well as on HITI-TEST for complex real images. Results are presented in fig. \ref{fig:thundr_ws_fs_ablations}. 

Empirically we found that models trained on synthetic data alone do not perform the best, not even when tested on synthetic data. Moreover, we found that pre-training with real data and refining on synthetic data produces better results than vice-versa. 
Large volumes of synthetic data improve model performance in conjunction with increasing amounts of weakly annotated real data, which is important as this is a practical setting and the symbiosis of synthetic and real data during training appears to address the sim-to-real gap. An increase in model capacity seems however necessary in order to take advantage of larger datasets. 

\begin{table}[!htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|r|}
    \hline
    \textbf{Method}  & {MPJPE-PA} & {MPJPE} & {MPJPE-T} \\ 
    \hline
    \hline
    HMR \cite{Kanazawa2018} &  & & NR \\
    \hline
    HUND \cite{zanfir2020neural} &  & &  \\
    \hline
    THUNDR \cite{Zanfir_2021_ICCV} &  &  &  \\
    \hline
    \hline
    THUNDR (HSPACE) &  &  &  \\
    \hline
    \end{tabular}
    \caption{\small Results obtained when refining THUNDR \cite{Zanfir_2021_ICCV} on the HSPACE training set and evaluated on Human3.6M under training/testing assumptions of protocol P1 (100K testing samples). Refining on HSPACE improves over the previous SOTA under MPJPE-PA, MPJPE and translation errors (MPJPE-T).}
\label{tbl:H36MP1}
\end{table}


\begin{figure*}[!htbp]
\begin{center}
    \includegraphics[width=0.245\linewidth ]{Figures/Plots/mpjpe_pa_space_test.png}
    \includegraphics[width=0.245\linewidth ]{Figures/Plots/mpjpe_space_test.png}   
    \includegraphics[width=0.245\linewidth ]{Figures/Plots/mpjpe_pa_hiti_test.png}
    \includegraphics[width=0.245\linewidth ]{Figures/Plots/mpjpe_hiti_test.png}   
\end{center}
\caption{\small Performance on HSPACE-TEST set (plots in the first and second rows) and HITI-TEST set (plots in third and fourth rows) for THUNDR (WS+FS) models with different capacities (SMALL for a THUNDR model with a transformer component of 1.9M parameters and BIG for a THUNDR model with a transformer component of 3.8M parameters, see supplementary material for more details) trained with various percentages of HITI (real) and HSPACE (synthetic) data. The THUNDR models were first trained in weakly supervised (WS) regime on the percentage of HITI data indicated in the legend and then refined in a fully supervised (FS) regime on different amounts of HSPACE data as well. We report MPJPE-PA and MPJPE metrics. We observe performance improvements when adding greater amounts of both synthetic and real data, as well as when increasing the model capacity.}
\label{fig:thundr_ws_fs_ablations}
\end{figure*}

\begin{table*}[htbp]
    \small
    \centering
    \begin{tabular}[t]{|l||r|r|r|r|r|r|r|r|}
    \hline
    \textbf{Method}  & {MPJPE-PA} & {MPJPE} & {MPVPE-PA} & {MPVPE} & {MPJPE-T} & R\#{2D} & R\#{2D-3D} & S\#{3D} \\
    \hline
    SPIN \cite{kolotouros2019learning} &  &  & N/A & N/A & N/A & 111K & 300K & \\
    \hline
    VIBE \cite{kocabas2019vibe} &  &  & N/A & N/A & N/A & 150K & 250K  &  \\
    \hline
    HUND \cite{zanfir2020neural} &  &  &  &  &  & 80K & 150K &  \\    
    \hline
    THUNDR \cite{Zanfir_2021_ICCV} &  &  &  &  &  & 80K & 150K &  \\
    \hline
    \hline
    THUNDR (HITI + HSPACE) &  &  &  &  & \textbf{180} & 100K &  &  \\
    \hline
    T-THUNDR (HITI + HSPACE) &  &  &  &  & \textbf{171} & 100K &  &  \\
    \hline
    \end{tabular}

    \caption{\small Results on the \textbf{HSPACE} test set. All current state of the art methods do not perform well when tested on the HSPACE test set. However performance improves significantly when training on HSPACE. We report mean per joint positional errors (with and without Procrustes alignment) (MPJPE-PA, MPJPE), mean per joint vertex displacement error (with and without Procrustes alignment) (MPVPE-PA, MPVPE) computed against ground truth GHUM meshes and translation error (MPJPE-T) computed against the pelvis joint. We also report the number of real images and the type of annotations used during the training of the listed models, e.g. number of real images with 2d annotations (R\#2D), number of real  images with paired 2d-3d annotations (R\#2D-3D) used during training and number of synthetic images with full 3d supervision (S\#3D). See our Sup. Mat. for additional detail and for qualitative visualisations of 3d human pose and shape reconstruction.}
\label{tbl:ablation_space}
\end{table*}

\noindent{\bf Ethical Considerations.} Our dataset creation methodology aims at diversity and coverage in order to build synthetic ground-truth for different human body proportions, poses, motions, ethnicity, age, or clothing.  By generating people in new synthetic poses, and by controlling different body proportions in various scenes, we can produce considerable diversity by largely relying on synthetic assets and by varying the parameters of a statistical human pose and shape model (GHUM). This supports, in turn, our long-term goal to build inclusive models that work well for everyone especially in cases where real human data as well as forms of 3d ground truth are difficult to collect.

\section{Conclusions}

We have introduced HSPACE, a large-scale dataset of humans animated in complex synthetic indoor and outdoor environments. We combine diverse individuals of varying ages, gender, proportions, and ethnicity, with many motions and scenes, as well as parametric variations in body shape, as well as gestures, in order to generate an initial dataset of over 1 million frames. Human animations are obtained by fitting an expressive human body model, GHUM, to single scans of people, followed by re-targeting and re-posing procedures that support realistic animation, statistic variations of body proportions, and jointly consistent scene placement for multiple moving people. All assets are generated automatically, 
being compatible with existing real time rendering engines. 
The dataset and an evaluation server will be made available for research.

Our quantitative evaluation of 3d human pose and shape estimation in synthetic and mixed (sim-real) regimes, underlines (1) the importance of synthetic, large-scale datasets, but also (2) the need for real data, within weakly supervised training regimes, as well as (3) the effect of increasing (matching) model capacity, for domain transfer, and continuing performance improvement as datasets grow. 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
