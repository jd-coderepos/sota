\documentclass{article}

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{paralist}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing, shapes.geometric, positioning, arrows}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{authblk}
\usepackage[noend,ruled]{algorithm2e}
\usepackage{colortbl}
\usepackage[rightcaption]{sidecap}
\sidecaptionvpos{figure}{t}

\newcommand{\np}{{\mathsf{NP}}}
\newcommand{\fpt}{{\mathsf{FPT}}}
\newcommand{\ilpfpt}{{\mathsf{ILP}\textrm{-}\mathsf{FPT}}}
\newcommand{\xp}{{\mathsf{XP}}}
\newcommand{\wone}{{\mathsf{W[1]}}}
\newcommand{\wtwo}{{\mathsf{W[2]}}}
\newcommand{\w}{{\mathsf{W}}}
\newcommand{\p}{{\mathsf{P}}}
\newcommand{\mytabref}[1]{\autoref{#1}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\dcl}{cl_{det}}
\DeclareMathOperator{\idcl}{\ensuremath{cl_{det}^{-1}}}
\DeclareMathOperator{\argmin}{argmin}

\renewcommand*{\algorithmautorefname}{Algorithm}
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\def\corollaryautorefname{Corollary}
\def\obsautorefname{Observation}
\def\lemmaautorefname{Lemma}



\makeatletter
\def\NAT@spacechar{~}\makeatother

\newcommand{\mypropref}[1]{\scriptsize{Prop.~\ref{#1}}}

\newcommand{\probDef}[3]{
  \begin{quote}
    #1 \\
    \textbf{Input:} #2 \\
    \textbf{Question:} #3
  \end{quote}
}

\newcommand{\probDefLong}[4]{
  \begin{quote}
    #1 (#2) \\
    \textbf{Input:} #3 \\
    \textbf{Question:} #4
  \end{quote}
}

\newcommand{\probSharpDef}[3]{
  \begin{quote}
    #1 \\
    \textbf{Input:} #2 \\
    \textbf{Compute:} #3
  \end{quote}
}

\newcommand{\probSharpDefLong}[4]{
  \begin{quote}
    #1 (#2) \\
    \textbf{Input:} #3 \\
    \textbf{Compute:} #4
  \end{quote}
}

\newcommand{\probSetCover}{\textsc{Set Cover}\xspace}
\newcommand{\probDominatingSet}{\textsc{Dominating Set}\xspace}
\newcommand{\probColorClique}{\textsc{Multi-Colored Clique}\xspace}
\newcommand{\probExactThreeSetCover}{\textsc{Exact 3-Set Cover}\xspace}
\newcommand{\probSTConnectness}{\textsc{s-t~Connectedness}\xspace}
\newcommand{\probMaxIndependentSet}{\textsc{Independent Set}\xspace}
\newcommand{\probClosure}{\textsc{Maximum Weight Closure}\xspace}
\newcommand{\probEffectors}{\textsc{Effectors}\xspace}
\newcommand{\probCost}{\textsc{Effectors-Cost}\xspace}
 
\begin{document}

\title{The Complexity of Finding Effectors\thanks{An extended
    abstract appeared in \emph{Proceedings of the 12th Annual
      Conference on Theory and Applications of Models of Computation
      (TAMC '15)}, Volume~9076 of LNCS, pages 224--235, Springer, 2015. This article provides all proofs in full detail.}}

\iffalse
\author{Laurent Bulteau\thanks{Laurent.Bulteau@u-pem.fr,IGM-LabInfo, CNRS UMR 8049, Universit\'e Paris-Est Marne-la-Vall\'ee, France.Supported by the Alexander von Humboldt Foundation, Bonn, Germany.Main work done while affiliated with  TU~Berlin.} \and Stefan Fafianie\thanks{stefan.fafianie@tu-berlin.de,Institut f\"ur Informatik, Universit\"at Bonn, Germany. Supported by the DFG Emmy Noether-program (KR 4286/1).Main work done while affiliated with TU~Berlin.}}
  \fi
  
\author{Laurent Bulteau\thanks{(Laurent.Bulteau@u-pem.fr) Supported by the Alexander von Humboldt Foundation, Bonn, Germany.
  Main work done while affiliated with  TU~Berlin.}}
  
\affil{IGM-LabInfo, CNRS UMR 8049, Universit\'e Paris-Est Marne-la-Vall\'ee, France.}
    
\author{Stefan Fafianie\thanks{(stefan.fafianie@tu-berlin.de) Supported by the DFG Emmy Noether-program (KR 4286/1).
    Main work done while affiliated with TU~Berlin.}}

\affil{Institut f\"ur Informatik, Universit\"at Bonn, Germany.}

\author{Vincent Froese\thanks{(vincent.froese@tu-berlin.de) Supported by the DFG, project DAMM (NI 369/13).}}
\author{Rolf Niedermeier\thanks{(rolf.niedermeier@tu-berlin.de)}}
\author{Nimrod Talmon\thanks{(nimrodtalmon77@gmail.com) Supported by DFG Research Training Group ``Methods for Discrete Structures''~(GRK~1408).}}

\affil{Institut f\"ur Softwaretechnik und Theoretische Informatik, TU Berlin, Germany.}
  


\iffalse
\and Stefan Fafianie\thanks{Stefan
    Fafianie was supported by the DFG Emmy Noether-program (KR
    4286/1). Main work done while affiliated with TU~Berlin.}
    \and Vincent Froese\thanks{Vincent Froese was supported by the DFG, project DAMM (NI 369/13).}
    \and Rolf Niedermeier \and Nimrod Talmon\thanks{Nimrod Talmon was supported by DFG Research Training Group ``Methods for Discrete Structures''~(GRK~1408).}}


  \address{Laurent Bulteau \at
             IGM-LabInfo, CNRS UMR 8049, Universit\'e Paris-Est Marne-la-Vall\'ee, France.\\
             \email{Laurent.Bulteau@u-pem.fr}
               \and
             Stefan Fafianie \at
             Institut f\"ur Informatik,
             Universit\"at Bonn, Germany.\\
             \email{stefan.fafianie@tu-berlin.de}
               \and
             Vincent Froese, Rolf Niedermeier, and Nimrod Talmon \at
             Institut f\"ur Softwaretechnik und Theoretische Informatik,
             TU Berlin, Germany.\\
             \email{\{vincent.froese,rolf.niedermeier\}@tu-berlin.de, nimrodtalmon77@gmail.com}
           }
  \fi
  
           
           
\date{}

\maketitle

\begin{abstract}
The NP-hard \probEffectors problem on directed graphs is motivated by applications in network mining,
particularly concerning the analysis of probabilistic information-propagation processes in social networks.
In the corresponding model the arcs carry probabilities
and there is a probabilistic diffusion process activating 
nodes by neighboring activated nodes with probabilities as specified by the arcs. 
The point is to explain a given network activation state as well as possible by
using a minimum number of ``effector nodes'';
these are selected before the activation process starts.

We correct, complement, and extend 
previous work from the data mining community by a 
more thorough computational complexity analysis of \probEffectors,
identifying both tractable and intractable cases.
To this end, we also exploit a parameterization measuring the ``degree
of randomness'' (the number of `really' probabilistic arcs) 
which might prove useful for analyzing
other probabilistic network diffusion problems as well.
\end{abstract}


\section{Introduction}

To understand and master the dynamics
of information propagation in networks (biological, chemical,
computer, information, social) is a core research topic in data mining and related fields.
A prominent problem in this context is the -hard 
problem \probEffectors~\cite{LTGMH10}: The input is a 
directed (influence) graph with a subset of nodes 
marked as active (the target nodes) and each arc of 
the graph carries an 
influence probability greater than~0 and at most~1.
Assuming a certain diffusion process on the graph,
the task is to  find few ``effector nodes'' that can ``best explain'' 
the set of given active nodes,
that is, the activation state of the graph.


Specifically,
consider a set of nodes in the graph which are initially active.
Then,
due to a certain diffusion process,
several other nodes in the graph,
which initially were not active,
might become active as a result.
The diffusion model we consider
(and which is known as the independent cascade model~\cite{KKT15})
is such that,
at each time step,
a newly activated node
(initially only the chosen effectors are active)
has one chance to activate each non-active out-neighbor with the corresponding arc probability.
If an out-neighbor was successfully activated in the last time step,
then the propagation continues and this node has the chance to further activate its out-neighbors.
The propagation process terminates when there are no newly activated nodes.
\autoref{fig:intro-example} shows an example of a possible propagation process.
Given the activation state of the graph at the end of the propagation process,
we ask for the set of nodes, the effectors, which could best explain the current activation state.

\begin{figure}
  \centering
  \begin{tikzpicture}[>=stealth,scale=0.6]
      \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
      \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]

      \node at (0,1) {};
      \node[active] (v1) at (0,6) {};
      \node[inactive] (v2) at (1.5,4) {};
      \node[inactive] (v3) at (-1.5,4) {};
      \node[inactive] (v4) at (0,2) {};

      \draw[->, very thick] (v1) -- (v2) node[midway, right] {0.5};
      \draw[->] (v1) -- (v3) node[midway, left] {0.8};
      \draw[->] (v2) -- (v3) node[midway, above] {0.1};
      \draw[->] (v3) -- (v4) node[midway, left] {1};
      \path[->] (v2) edge[bend right] node[midway, left] {0.3} (v4);
      \path[->] (v4) edge[bend right] node[midway, right] {0.9} (v2);
    \end{tikzpicture}
    \hspace{2em}
    \begin{tikzpicture}[>=stealth,scale=0.6]
      \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
      \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]

      \node at (0,1) {};
      \node[active] (v1) at (0,6) {};
      \node[active] (v2) at (1.5,4) {};
      \node[inactive] (v3) at (-1.5,4) {};
      \node[inactive] (v4) at (0,2) {};

      \draw[->,help lines] (v1) -- (v2) node[midway, right] {0.5};
      \draw[->, help lines] (v1) -- (v3) node[midway, left] {0.8};
      \draw[->] (v2) -- (v3) node[midway, above] {0.1};
      \draw[->] (v3) -- (v4) node[midway, left] {1};
      \path[->, very thick] (v2) edge[bend right] node[midway, left] {0.3} (v4);
      \path[->] (v4) edge[bend right] node[midway, right] {0.9} (v2);
    \end{tikzpicture}
    \hspace{2em}
    \begin{tikzpicture}[>=stealth,scale=0.6]
      \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
      \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]

      \node at (0,1) {};
      \node[active] (v1) at (0,6) {};
      \node[active] (v2) at (1.5,4) {};
      \node[inactive] (v3) at (-1.5,4) {};
      \node[active] (v4) at (0,2) {};

      \draw[->,help lines] (v1) -- (v2) node[midway, right] {0.5};
      \draw[->, help lines] (v1) -- (v3) node[midway, left] {0.8};
      \draw[->, help lines] (v2) -- (v3) node[midway, above] {0.1};
      \draw[->] (v3) -- (v4) node[midway, left] {1};
      \path[->, help lines] (v2) edge[bend right] node[midway, left] {0.3} (v4);
      \path[->, help lines] (v4) edge[bend right] node[midway, right] {0.9} (v2);
    \end{tikzpicture}
    \caption{An example depicting the information propagation according to the independent cascade model. The influence graph is a directed graph where the arcs are labeled with influence probabilities. Initially, at time~, only the top node is active (black) and has a chance to independently activate the left and right node with the corresponding arc probabilities.
      In the example, the right node is activated (thick arc) while the left node is not. The probability of this event is thus~. The propagation then continues and the right node has a chance to activate its out-neighbors at time~. Every activated node has only one chance (namely, after it became active the first time) to activate other inactive nodes. Note that at time~ the bottom node cannot activate any new nodes. Hence, the propagation process terminates. The overall probability of this particular propagation (and of this particular activation state) equals .
    }
  \label{fig:intro-example}
\end{figure}

Being able to efficiently compute the set of effector nodes is helpful in many scenarios.
The paper by~\citet{LTGMH10} mentions several of them,
including being able to better understand how information propagates in social networks,
or finding those countries which are more prominent for spreading epidemics
(here, one might assume a graph where each country is a node,
and,
given the current state of some plague,
the effector nodes are those countries which explain this current state).
Motivated also by the scenario from epidemics,
one might be interested in providing shields against such plagues.
One possible way to achieve this is by finding the set of effectors,
and vaccinating the people in those countries.
Taking monetary costs into account,
it is desirable to find a small set of effectors;
thus,
in the \probEffectors problem,
the goal is to find a set of effectors of small size.



It is important to note that we allow effectors to be chosen from
the \emph{whole} set of graph nodes and not only from the set of target nodes.
This makes our model, in a sense, more general than the original one by~\citet{LTGMH10}.\footnote{We
conjecture that both models coincide if we are allowed to choose an unlimited number of effectors,
that is,
if the number of chosen effectors does not matter.
On the contrary, they do not coincide if the number of effectors is bounded,
see~\autoref{sect:prelim}.}
See~\autoref{sect:prelim} for definitions of the main problems,
formal definition of our model, and a discussion about our model and its difference to that of~\citet{LTGMH10}.


Our main contribution is to extend and clarify research 
on the computational complexity status of \probEffectors, which has 
been initiated by~\citet{LTGMH10}.
In short,~\citet{LTGMH10} have shown that \probEffectors is generally -hard and hard to approximate,
developed an algorithm that is efficient on trees,
and used it to develop an efficient heuristic.
As \emph{probabilistic} information propagation is central in the independent cascade information-propagation model
which is in the heart of the \probEffectors problem (as well as in several other information-propagation models),
we put particular emphasis on studying how the ``degree of randomness''
in the network governs the computational complexity.
Moreover, compared to previous work, we make an effort to present
the results in a more formal setting, conducting a rigorous mathematical analysis.


Informally speaking (concrete statements of our results appear 
in \autoref{sect:prelim} after having provided formal definitions), 
we have gained the following main insights (also refer to \autoref{table:results} in
\autoref{sect:prelim}).
\begin{itemize}
  \item With unlimited degree of randomness, finding effectors is 
  computationally very hard. In fact, even computing the ``cost'' (how well 
  does a set of effectors explain a given activation state) of a \emph{given}
  set of effectors is intractable.
  This significantly differs from
  deterministic models.
  \item Even if the directed input graph is acyclic, then this does \emph{not}
  lead to a significant decrease of the computational complexity. 
  \item Bounding the degree of randomness (in other words, bounding the number 
  of arcs with probability different from~1), that is,
  \emph{parameterizing on the degree of randomness}, yields some 
  encouraging (fixed-parameter) tractability 
  results for otherwise intractable cases.
  \item We identify some flaws in the work of \citet{LTGMH10}
  (see~\autoref{sect:flawDetails} for details),
  who claim one case to be intractable which in fact is tractable and one case the other
  way around.
\end{itemize}


Admittedly, in real-world applications (where influence probabilities
are determined through observation and simulation, often involving noise) 
the number of probabilistic arcs may be high, thus, at first sight,
rendering the parameter ``number of probabilistic arcs'' doubtful.
However, note that finding effectors is computationally very hard
(also in terms of polynomial-time approximability; the approximation hardness of \probEffectors 
is mentioned by~\citet{LTGMH10} and follows, for example, from the reductions which use the \probSetCover problem).
So, in order to make the computation of a solution more feasible one might 
round up (to~1) arc probabilities which are close to~1 and round down (to~0) arc
probabilities which are close to~0. Thus, one can achieve a trade-off between running time and accuracy of the result.
Depending on the degree of rounding 
(as much as a subsequent fixed-parameter algorithm exploiting 
the mentioned parameter would ``allow''),
in this way one might at least find
a good approximation of an optimal set of effectors in reasonable time.


\paragraph{Related work.}
Our main point of reference is the work of~\citet{LTGMH10}.
Indeed, we use a slightly different problem definition:
They define the effectors to be necessarily a subset of the target nodes, 
whereas we allow the effectors to form an arbitrary
subset of the nodes. It turns out that these two definitions really
yield different problems, in the sense that a solution for one problem might not be a solution for the other
(see~\autoref{sec:Model} for an extensive discussion of the differences between these two models
and for an explanation on why we have chosen to define our model as it is defined).


The special case where all nodes are target nodes 
(and hence where the two models above clearly coincide) is
called \textsc{Influence Maximization} and is well studied in the
literature~\cite{BSKDSM07, DPRM01, KKT15}.
Specifically,
it is known that the \textsc{Influence Maximization} problem is -hard,
and a polynomial-time -approximation algorithm for this problem is given by~\citet{KKT15}.


Finally, a closely related deterministic version (called
\textsc{Target Set Selection}) with the 
additional difference of having node-individual 
thresholds specifying how many neighboring nodes need to be active 
to make a node active has also been extensively studied,
in particular from a parameterized complexity point of 
view~\cite{BCNS14-comp,BCNS14-jda,Ben-ZwiHLN11,CNNW14,NNUW13}.
\textsc{Target Set Selection} is -hard in general,
and hard to approximate, also in the parameterized sense
(specifically,
cannot be approximated even in -time (see~\autoref{sect:prelim}) with respect to the solution size).
It is -hard even on graphs of diameter ~\cite{NNUW13},
and it is tractable on some restricted graph classes such as trees~\cite{Ben-ZwiHLN11} and cliques~\cite{NNUW13}.


\section{Preliminaries}\label{sect:prelim}
In this section, we provide definitions used throughout the work.
We basically use the same definitions as~\citet{LTGMH10},
except for few differences in notation.

\paragraph{Graph Theory.}
We consider simple directed graphs~ with a set~ of nodes
and an arc set~.
If there is an arc~, then we call~ an \emph{in-neighbor} of~ and we call~ an \emph{out-neighbor} of~. For a subset~, we denote by~ the subgraph of~ induced by~, where .
An undirected graph~ consists of a vertex set~ and an edge set~.

We use the acronym DAG for directed acyclic graphs.
An undirected \emph{tree} is a connected acyclic graph.
A \emph{directed tree} is an arbitrary orientation of an undirected tree.
The \emph{condensation} of a directed graph~ is a DAG containing a node~ for each
strongly connected component~ of~ and there is an arc~
if and only if there exists at least one arc from a node in~ to a node in~.


\paragraph{Influence Graphs.}
An \emph{influence graph}~ is a simple directed
graph equipped with a function~
assigning an \emph{influence weight} to each arc 
which represents the \emph{influence of node  on node }. Strictly speaking,
the influence is the probability that~ propagates some information to~.
We denote the number of nodes in  by  and the
number of arcs in  by~.


\paragraph{Information Propagation.}
We consider the following information-propaga\-tion process,
called the \emph{Independent Cascade (IC)} model \cite{KKT15}.
Within this model, each node is in one of two states: \emph{active} or \emph{inactive}.
When a node~ becomes active for the first time, at time step~,
it gets a single chance to activate its inactive out-neighbors.
Specifically,  succeeds in activating a neighbor~ with probability~.
If  succeeds, then  will become active at step .
Otherwise,  cannot make any more attempts to activate  in any subsequent round.
The propagation process terminates when there are no newly activated nodes,
that is,
when the graph becomes static.


We remark that,
since our algorithms need to manipulate the probabilities determined by the function~,
technically (and as usually)
we assume that the precision of the probabilities determined by this function 
is polynomially upper-bounded in the number~ of nodes of the input graph,
and we ignore the time costs for adding or multiplying rational numbers assuming that these operations take constant time.



\paragraph{Cost Function.}
For a given influence graph~,
a subset  of effectors,
and a subset  of active nodes,
we define a cost function

where for each , we define  to be the probability of
 being active after the termination of the information-propagation process starting
with~ as the active nodes.
An alternative definition is that ,
where  if  and  if .
One might think of this cost function as computing the expected number of nodes
which are incorrectly being activated or unactivated.


\paragraph{Main Problem Definition.}
Our central problem \probEffectors is formulated as a decision problem---it 
relates to finding few nodes which best explain (lowest cost) the given 
network activation state specified by a subset~ of nodes.
\probDef
  {\probEffectors}
  {An influence graph , a set of target nodes , a budget , and a cost .}
  {Is there a subset  of effectors with  and cost ?}
We will additionally consider the related problem 
\probCost (see~\autoref{section:computingCost}) where the set~ 
of effectors 
is already given and one has to determine its cost.

\paragraph{Parameters.}
The most natural parameters to consider for a parameterized computational complexity 
analysis are the maximum number~ of
effectors, the cost value~, and the number~ of target nodes.
Moreover, we will be especially interested in quantifying the amount of
randomness in the influence graph.
To this end, consider an arc :
if ,
then this arc is not probabilistic.
We define the parameter number~ of probabilistic arcs, that is,
.


\paragraph{Parameterized Complexity.}
We assume familiarity with the basic notions of algorithms and complexity.
Several of our results will be cast using the framework of parameterized complexity analysis.
An instance~ of a parameterized problem consists of the classical
instance~ and an integer~ being the \emph{parameter} \cite{DF13,FG06,Nie06,Cyg15}.
A parameterized problem is called \emph{fixed-parameter tractable} (FPT) if there is an algorithm solving it in~ time, whereas an algorithm with running time~ only shows membership in the class~XP (clearly, FPTXP).
One can show that a parameterized problem~ is (under certain complexity-theoretic assumptions) not fixed-parameter tractable by devising a \emph{parameterized reduction} from a
W[1]-hard or W[2]-hard problem (such as \textsc{Clique} or \textsc{Set
Cover}, respectively, each parameterized by the solution size) to~.
A parameterized reduction from a parameterized problem~ to another parameterized problem~ is a function that, given an instance~, computes in~ time an instance~~with~ such that~.
The common working hypothesis is that FPTW[1]. In fact, it is assumed that there is an infinite hierarchy

called the -hierarchy. Thus, for a parameterized problem to be~W[2]-hard is even stronger
in the sense that even if~FPTW[1] holds, it is still possible that~FPTW[2].

\paragraph{Counting Complexity.}
We will also consider so called counting problems of the form ``Given~, compute .'', where~ is some function~ (see \citet[Chapter~9]{AB09} for an introduction to counting complexity).
The class~ consists of all such functions~ such that~ equals the number of accepting computation paths of a nondeterministic polynomial-time Turing machine on input~.
Informally speaking, we can associate a decision problem in  (which asks weather there exists a solution or not) with a counting problem in~ (which asks for the number of solutions).
Clearly, if all counting problems in~ can be solved in polynomial time, then this implies .
Analogously to -hardness, showing that a function
is -hard gives strong evidence for its computational intractability.
A function~ is -hard if a polynomial-time algorithm for~ implies that all counting problems in~ are polynomial-time solvable.

\paragraph{Organization.}
Before we discuss our model and the one by \citet{LTGMH10}, 
we overview our main results in \autoref{table:results}.
We will treat the sub-problem \textsc{Eff\-ectors-Cost}
in~\autoref{section:computingCost},
and \probEffectors in~\autoref{section:findingEffectors}.
Note that most of our results transfer to the model of \citet{LTGMH10}.
In particular, this implies that their claims that the ``zero-cost'' 
special case is -hard \cite[Lemma~1]{LTGMH10} and that the deterministic version is polynomial-time solvable
are both flawed, because from our results exactly the opposite follows
(see the last part of~\autoref{sect:flawDetails} for details).

  \newcommand{\with}[1]{ {\scriptsize wrt.~}}
  \newcommand{\see}[1]{ {\scriptsize \mytabref{#1}}}
  \newcommand{\seecite}[1]{ {\scriptsize \cite{#1}}}
  \newcommand{\clippedPath}[2][] {
    \begin{scope}
      \clip #2;
      \draw[#1] #2;
    \end{scope}
    \draw[white, line width=1pt] #2;
  }
  
  \newcommand{\resultBox}[2]{    
    \clippedPath[line width=4pt, #1!75!black, fill= #1!5]{#2}    
  }
  
  \newcommand{\titleBox}[1][1]{    
    \clippedPath[line width=3pt, black ]{(-1.4,0) -- (-1.4,#1) -- (0,#1) -- (0,0) --cycle }        
  } 
  \newcommand{\titleNode}[2][0.5]{    
    \node[rectangle,align=center] () at (-0.7,#1) {#2};    
  }  
  
  \newcommand{\headBox}[2]{    
    \clippedPath[line width=3pt, black ]{(#1,0) -- (#2,0) -- (#2,1.1) -- (#1,1.1) --cycle }        
  }   
  \newcommand{\headNode}[2]{    
    \node[multiline] () at (#1,0.55) {#2};    
  } 
 \begin{table}[t]
  \centering
  \caption{Computational complexity of the different variants of \probEffectors.
    Note that all hardness results hold also for DAGs.
    The parameter  stands for the number of active nodes,
     for the budget,
     for the cost value,
    and  for the number of probabilistic arcs.}
  \label{table:results}
  \begin{tikzpicture}[>=stealth, xscale=1.65,yscale=0.9]
    \tikzstyle{multiline}=[rectangle,align=center]
    
    \begin{scope}[yshift=1cm] 
      \headBox02
      \headNode1{Deterministic\\{\footnotesize ()}}
      \headBox24
      \headNode3{Parameterized\\{\footnotesize (by )}}
      \headBox46
      \headNode5{Probabilistic\\{\footnotesize (arbitrary )}}      
    \end{scope}
    \begin{scope}
      \titleBox \titleNode{{\sc Effectors-}\\{\sc Cost} };
      \resultBox{green}{(0,0) -- (0,1) -- (4,1) -- (4,0) --cycle }     
      \resultBox{red}{(4,0) -- (4,1) -- (6,1) -- (6,0) --cycle }
\node () at (2,0.5) {\with{r},\see{thm:costFPTr}};
      \node () at (5,0.5) {-hard,\see{thm:costNP}};      
    \end{scope}
    
    \begin{scope}[yshift=-2cm]    
      \titleBox[2]   
      \titleNode[1] {\probEffectors \\ (general case)};
      \resultBox{red}{(0,0) -- (0,1) -- (2,1) -- (2,2) -- (6,2) -- (6,0) --cycle }
\node[multiline] () at (4.0,0.9) { -hard\with{b+c}, \see{thm:combinedHardness} \\ -hard\with{a+b+c}, \see{thm:combinedHardness} };
      \resultBox{blue}{(0,1) -- (0,2) -- (2,2) -- (2,1) --cycle }     
      \node[multiline] () at (1,1.5) {\with{\min(a,b,c)},\-0.4em]\see{thm:infmax}};
\end{scope}
  \end{tikzpicture}
\end{table}

\section{Model Discussion}
\label{sec:Model}
Our definition
of \probEffectors differs from the problem definition of
\citet{LTGMH10} in that we do not require the effectors to be chosen
among the target nodes.
Before pointing out possible advantages and motivating our problem definition,
we give a simple example illustrating the difference between these two definitions.

Consider the influence graph in \autoref{fig:ModelDiscExample},
consisting of one non-target node (white) having three outgoing arcs
with probability~1 each to three target nodes (black).
Clearly, for~, this is a ``no''-instance if we are
only allowed to pick target nodes as effectors since
the probability of being active will be~0 for two of the three target
nodes in any case, which yields a cost of at least~2.
According to our problem definition, however, we are allowed to select the
non-target node, which only incurs a cost of~1, showing that this
is a ``yes''-instance.

Let us compare the two models.
First,
we think that our model captures the natural assumption that
an effector node does not have to remain active forever\footnote{Notably, in our model it actually remains active. The point is that before the whole computation starts (and after it ends) nodes may (have) become inactive again. Still, ``temporary activeness'' may make a node an effector that helps
explaining the currently observed network activation state.}.
Indeed, the modeling of \citet{LTGMH10} might be interpreted as a
``monotone version'' as for example discussed by \citet{ABS14}, 
while in this sense our model allows for ``non-monotone explanations''.
Second,
our model is more resilient to noise;
consider, for example, \autoref{fig:ModelDiscExample}.
It might be the case that indeed the top node is activated,
however,
due to noisy sampling methods,
it looks to us as if this top node is inactive.
In this simple example,
a solution according to the model of \citet{LTGMH10} would have to use three effectors to wrongly explain the data,
while a solution according to our model would be compute a correct and optimal solution with only one effector.


Clearly, if all nodes are target nodes (this particular setting is called \textsc{Influence Maximization}), 
then the two models coincide. 
Furthermore, we strongly conjecture that if we have an unlimited budget,
then it suffices to search for a solution
among the target nodes, that is, for~, we believe that the two
problem definitions are also equivalent:

\begin{SCfigure}[2.3][t]
  \centering
  \begin{tikzpicture}[>=stealth, scale=0.8]
    \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
    \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]
    \node[inactive] (V1) at (1,1.5) {};
    \node[active] (V2) at (0,0.5) {};
    \node[active] (V3) at (1,0.25) {};
    \node[active] (V4) at (2,0.5) {};

    \draw[thick,->] (V1) -- (V2) node[midway, left] {1};
    \draw[thick,->] (V1) -- (V3) node[midway, right] {1};
    \draw[thick,->] (V1) -- (V4) node[midway, right] {1};
  \end{tikzpicture}
  \label{fig:ModelDiscExample}
  \caption{Example where it is optimal to choose a non-target node as effector.}
\end{SCfigure}

\begin{conjecture}
  \label{con:infinite-budget}
  For , it holds that every ``yes''-instance~ of
  \probEffectors has a solution~.
\end{conjecture}

At least for directed trees
(that is, the underlying undirected graph is a tree---these also have
been studied by \citet{LTGMH10}) 
we can prove~\autoref{con:infinite-budget}.
The idea of proof is that if an optimal solution contains a non-target
node~, then this node only influences nodes reachable from it via paths that do not visit other nodes in the solution. Within this smaller tree of influenced nodes there must be some subtrees rooted at target nodes such that the expected cost for such a subtree is smaller if its target root node is activated during the propagation process compared to the case when it is not. Choosing these target nodes directly as effectors, replacing the non-target node~, yields another optimal solution with fewer non-target~nodes.

\begin{theorem}\label{thm:conjectureForTrees}
  \autoref{con:infinite-budget} holds for directed trees.
\end{theorem}

\begin{proof}
  Before proving the actual theorem, let us have a brief look on the
  probabilistics of the information-propagation process in directed
  trees.
  Clearly, in any influence graph, a node~ can activate another
  node~ only if there is a directed path from~ to~.
  Note that in a directed tree this path is unique if it exists.
  Moreover, the probability~ only depends on
  those nodes~ that are connected to~ by a directed path that
  contains no other node from~.
  To see that this is true, consider a node~ such that all directed paths from~ to~ contain another node from~. Then, on each of these paths the corresponding node  has only one chance to activate~ via propagation along the path. Since~ cannot ``re-activate''~ ( is already active from the beginning), the activation probability of~ does not depend on~. 
  For a node~, let  denote the \emph{closure} of~,
  that is, the set of all nodes~ for which there exists a directed
  path from~ to~ (including~ itself, that is, ).

  Let~ with~ be an input instance of
  \probEffectors, where~ is an arbitrary directed tree.
  Let~ be an optimal solution with~,
  that is, there exists a node~.
  We show that there is an optimal solution~ containing
  fewer non-target nodes than~.
  More formally, we show that there exists a solution~
  with~ and~ such that
  . Recursively applying this argument
  then proves the theorem.

  First, note that if~ holds for~,
  then we are done. Thus, we can assume , or, equivalently:
  
  Now, consider a node~ that is not in the closure
  of~. Clearly, it holds that~ since there is no directed path
  from~ to~, and thus~ cannot change the 
  probability of~ becoming active during the information-propagation process.
  Therefore, if we let~ and~,
  then Inequality~\eqref{eq:C'-C} can be rewritten as
  
  For a directed tree~, the subgraph~ induced by the
  closure of~ is a rooted directed tree with root~,
  where all the arcs are directed from~ to the leaves (that is, an
  \emph{out-tree}).
  Moreover, for a node~, there is exactly one directed path
  from~ to~ in~. Let~ be the subset of target
  nodes~ in the closure of~ such that the directed path from~ to~
  contains no other target node from~.
  Then, we can write the closure of~ as the disjoint union
  ,
  where~.
  Note that~.
  Therefore, we can write Inequality~\eqref{eq:C'-C2} as
  
  Note that~ holds for all~
  since~, which yields
  
  Therefore, the following holds
  
  Now, let~ denote the probability that a
  node~ is not activated given that the nodes in~ are active
  and let~ be the probability
  of~ being activated given that~ is inactive
  and the nodes in~ are active.

  Note that, for  and~,
  the probability of  being active conditioned on~ does
  not depend on~ since~ lies on the directed path from~
  to~, that is,  and~.
  Hence, we have
  
  and
  
  This yields
  
  Thus, for each~, we have 
  
  In the following, let
  
  
  Consider now Inequality~\eqref{eq:C'-C3} again.
  Since the outer summation in Inequality~\eqref{eq:C'-C3} over all nodes  is positive,
  there must be some nodes  for which the summand (that
  is, the right-hand side product of Equation~\eqref{eq:inner-prod})
  is positive.
  Note that~ since~ for all~.
  Hence, the set~ is non-empty
  since these are the nodes for which the above product is positive.
  Furthermore, we define the new set of effectors~,
  which does not include the non-target node~.

  Now, consider the difference~.
  Since~, it follows~
  for all~.
  Thus, analogously to the above steps, we can write
   as
  
  Note that, for each~, it holds for
  all~ that~.
  Hence, , which implies
  
  Thus, we obtain the following inequality
  
  As in Equation~\eqref{eq:inner-prod}, we can rewrite the right-hand side of Inequality~\eqref{eq:C-C*1} to
  
  Clearly, for~, the probability of~ being
  active conditioned on~ does not depend on~,
  that is, it holds
     and
    .
  By substituting these probabilities into~\eqref{eq:C-C*2} we
  arrive at

  

  Now, for each node~, it holds~ and
  , and thus
  
  For each~, it holds~ and
  ,
  and thus 
  
  Hence,  and, clearly, , and we are done.   
\end{proof}


The last theorem shows that our model for the \probEffectors problem and that of~\citet{LTGMH10} sometimes coincide.
In general,
however,
it is not completely clear how the computational complexity of our model for the \probEffectors problem
differs from that of~\citet{LTGMH10}.
We do \emph{mention} that our algorithmic results
(\autoref{lem:det_c0_p},
\autoref{prop:generalZEROrXP},
\autoref{thm:inftyFPTr})
easily transfer to the model of~\citet{LTGMH10},
as well as \autoref{thm:infmax}.


\section{Computing the Cost Function}\label{section:computingCost}
We consider the problem of computing the cost for a given set of effectors.
\probSharpDef
  {\probCost}
  {An influence graph , a set of target nodes
    , and a set of effectors .}
  {The cost .}

\probCost is polynomial-time solvable on directed trees~\cite{LTGMH10}.
By contrast, 
\probCost is unlikely to be polynomial-time solvable even on DAGs.
This follows from a result by~\citet[Theorem 1]{wang2012scalable}.
They show that computing the expected number of activated nodes for a
single given effector is -hard on DAGs.
Note that for the case~ (that is, ), the cost equals the expected
number of activated nodes at the end of the propagation process.
Hence, we obtain the following corollary of~\citet{wang2012scalable}.

\begin{corollary}\label{thm:costNP}
  \probCost on directed acyclic graphs is -hard even for~ and~.
\end{corollary}

Note that \autoref{thm:costNP} implies that \probCost on DAGs is not fixed-parameter tractable
with respect to the combined parameter~.

On the positive side, \probCost is fixed-parameter tractable with respect to the
number~ of probabilistic arcs.
The general idea is to recursively simulate the propagation process,
branching over the probabilistic arcs,
and to compute a weighted average of the final activation state of the graph.

\begin{theorem}\label{thm:costFPTr}
  Given an instance~ of \probCost, the probability~ for a given node~ can be computed in~ time, where~ is the number of probabilistic arcs.

  Accordingly, \probCost can be solved in  time.
\end{theorem}

\begin{proof}
  The overall idea of the proof is as follows.
  For each subset of the probabilistic arcs,
  we compute the cost,
  conditioned on the event that the propagation process was successful on these arcs,
  but not successful on the other probabilistic arcs.
  For each such subset we also compute the probability that this event happens.
  Then,
  by applying the law of total probability,
  it follows that the overall cost equals
  the weighted average of these conditioned costs,
  weighted by the probability of these events.
  
  We present the algorithm in a recursive way,
  mainly for the sake of having a formal proof for its correctness.
  To this end,
  let~ be an input instance of \probCost.
  Note that in order to compute the cost~, we
  compute the probability~ for each node~,
  because given all these probabilities
  it is straightforward to compute the cost in polynomial time.
  Hence, we prove the theorem by showing that computing~
  is fixed-parameter tractable with respect to~
  using a search-tree algorithm that computes~
  for a given node~ by recursively ``simulating'' all possible scenarios which
  could appear during the propagation process.
  
  To this end,
  we define an auxiliary function
   denoting the probability that~ is
  activated during the propagation process given that exactly the
  nodes in~ are active but only the nodes in~ are
  allowed to activate further nodes in the next step,
  whereas the nodes in~ can never activate any other node
  (indeed,~).

  We now show how to compute~.
  First, if~, then , as it is already activated.
  Otherwise, if~ and~ is closed (that is,~ has no
  outgoing arcs to~),
  then there is no propagation at all and thus~.
  Otherwise, if~ is not closed, then let~ denote
  the set of nodes in~ that have an
  incoming arc from some node in~.
  Further, let~ be the set of nodes that
  have at least one deterministic incoming arc from~, and
  let~.
  Also, let~ be the set of probabilistic arcs from~ to~.
  Clearly, all nodes in~ will be active in the next step of the
  propagation process, while the nodes in~ will be active in the
  next step only with some positive probability.
  We can use the law of total probability on the subsets of ,
  and write
  
  where~ denotes the set of active nodes in the next time step,
   denotes the set of newly active nodes in the next time step,
  and~ denotes the probability that \emph{exactly} the nodes
  in~ are active in the next step given that \emph{exactly} the nodes
  in~ are active.
  Note that, for each subset~,
  
  is polynomial-time computable.
  As a result,
  we end up with the following recursive formula:
  
  
  \begin{algorithm}[t]
    \normalsize
    \SetAlgoNoLine
    \caption{Pseudocode for .}
    \label{alg:costFPTr}
    \If{}{
      \Return  \\
    }
    \If{ and  is closed}{
      \Return  \\
    }
    \ForEach{}{
      compute  \\
      compute  recursively \\
    }
    \Return 
  \end{algorithm}
  
  \autoref{alg:costFPTr} presents the pseudocode for computing .
  For the running time, consider the recursion tree corresponding to
  the computation of~, where each vertex corresponds
  to a call of~.
  
  For the running time,
  note that the inner computation (that is, without further recursive calls) of each node in the recursion tree
  can be done in time .  
  Moreover, for each call, either at least one node is inserted to~,
  or the recursion stops.
  Therefore, the height of the recursion tree is upper-bounded by the number~ of nodes.
  Lastly, each leaf in the recursion tree corresponds to a distinct subset of the probabilistic arcs,
  specifically, to those probabilistic arcs along which the propagation process carried on.
  Since there are~ different subsets of probabilistic arcs,
  it follows that the number of leaves of the recursion tree is upper-bounded by~.
  Thus, the overall size of the recursion tree is upper-bounded by ,
  and hence,
  the running time is~.
\end{proof}

\section{Finding Effectors}\label{section:findingEffectors}
We treat the general variant of \probEffectors in~\autoref{section:noAssumptions}, 
the special case of unlimited budget in~\autoref{section:infiniteBudget},
and the special case of influence maximization 
in~\autoref{section:everythingActive}.

\subsection{General Model}\label{section:noAssumptions}
We study how the parameters number~ of target nodes, budget~, and cost value~ influence the computational complexity of \probEffectors.
We first observe that if at least one of them equals zero, then
\probEffectors is polynomial-time solvable.
This holds trivially for parameters  and ; simply choose the
empty set as a solution.
This is optimal for , and the only feasible solution for .
For parameter , the following holds, using a simple decomposition into 
strongly connected components.

\begin{lemma}\label{lem:det_c0_p}
  For , \probEffectors can be solved in linear time.
\end{lemma}

\begin{proof}
  If there is a directed path from a target node to a non-target node,
  then we have a ``no''-instance.
  Now every target node must be activated with probability~1,
  which is only possible along deterministic arcs.
  Let  be the condensation (that is, the DAG of strongly connected
  components) of the influence graph~ after
  removing all probabilistic arcs.
  Then, we consider only the strongly connected components which
  contain at least one target node
  (note that all nodes in this component must be targets).
  Finally, if there are more than~ of these target components that
  are sources in~, then we have a ``no''-instance.
  Otherwise, we arbitrarily pick a node from each component corresponding to a source,
  and return a positive answer.
  Each step requires linear time. 
\end{proof}

\noindent 
Based on \autoref{lem:det_c0_p},
by basically checking all possibilities in a brute-force manner,
we obtain simple polynomial-time algorithms for
\probEffectors in the cases of a constant number~ of target
nodes, budget~, or cost~.

\begin{proposition}\label{prop:generalZEROrXP}
  For , \probEffectors is in  with respect to each of
  the parameters~, , and~.
\end{proposition}

\begin{proof}
  Containment in  for the parameter  is straightforward:
  For each possible set of effectors, we compute the cost in linear time and
  then return the best set of effectors.

  Note that for the case , we can assume that .
  To see this, let~ be a solution of size~ and let~ be the subset of target nodes that are activated by choosing~.
  Clearly, choosing~ as effectors is a better solution since it activates the
  same target nodes and only activates a subset of the non-target nodes activated by~.
  Therefore, we also have containment in  with respect to~.

  It remains to show the claim for parameter~.
  First, we choose which  nodes incur a cost.
  Among these nodes, we set the target nodes to be non-targets, and vice versa.
  Then, we run the polynomial-time algorithm of \autoref{lem:det_c0_p} with cost~0.
  We exhaustively try all possible  choices to find a
  positive answer and return a negative answer otherwise. 
\end{proof}

In the following, we show that, even for  and the influence
graph being a DAG, \probEffectors is
-hard with respect to the \emph{combined} parameter ,
and even -hard with respect to the \emph{combined} parameter .

\begin{theorem}\label{thm:combinedHardness}
  \mbox{}
  \begin{enumerate}
    \item \probEffectors,
      parameterized by the combined parameter ,
      is -hard,
      even if 
      and the influence graph is a DAG.
    \item \probEffectors,
      parameterized by the combined parameter ,
      is -hard,
      even if 
      and the influence graph is a DAG.
  \end{enumerate}
\end{theorem}

\begin{proof}
  We begin with the first statement,
  namely,
  that \probEffectors,
  parameterized by the combined parameter ,
  is -hard,
  even if  and
   is a DAG.
  We describe a parameterized reduction from the following -hard problem~\cite{FHRV09}.
  \probDef
    {\probColorClique}
    {A simple and undirected graph  with  colors on the vertices and .}
    {Is there a -vertex clique with exactly one occurrence of each
    color in the clique?}
    
    
  \begin{figure}[t]
  \centering
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
    \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]
    
    \node[inactive] (v1) at (1,4) {};
    \node[rectangle, draw] (l1) at (1, 4.5) {};
    \draw (v1) -- (l1);
    \node[inactive] (v2) at (1,3) {};
    \node [rectangle, draw] (l2) at (1, 3.5) {};
    \draw (v2) -- (l2);
    \node at (1,2) {};
    \node[inactive] (vn) at (1,1) {};
    
    \node[inactive] (e1) at (3,4.5) {};
    \node [rectangle, draw] (l3) at (3,5.1) {};
    \draw (e1) -- (l3);
    \node[inactive] (e2) at (3,3.5) {};  
    \node[inactive] (e3) at (3,2.5) {};
    \node at (3,1.5) {};
    \node[inactive] (em) at (3,0.5) {};
    
    \node [rectangle, draw] (l4) at (5,2) {};
    \node[active] (c11) at (5,4) {};
    \draw (l4) -- (5, 2.6);
    \node[active] (c12) at (6,4) {};
    \node[active] (c13) at (7,4) {};
    \node at (8,4) {};
    \node[active] (c1c) at (9,4) {};
    
    \node[active] (c21) at (5,3) {};
    \node[active] (c22) at (6,3) {};
    \node[active] (c23) at (7,3) {};
    \node at (8,3) {};
    \node[active] (c2c) at (9,3) {};
    
    \node[active] (cc1) at (5,1) {};
    \node[active] (cc2) at (6,1) {};
    \node[active] (cc3) at (7,1) {};
    \node at (8,1) {};
    \node[active] (ccc) at (9,1) {};
    
    \draw[rounded corners=8pt,line width=1.2pt] (4.5,4.4) rectangle (9.5,3.6);
    \draw[rounded corners=8pt,line width=1.2pt] (4.5,3.4) rectangle (9.5,2.6);
    \node at (7,2) {};
    \draw[rounded corners=8pt,line width=1.2pt] (4.5,1.4) rectangle (9.5,0.6);
    
    \draw[thick,->] (e1) -- (v1);
    \draw[thick,->] (e1) -- (v2);
    \draw[thick,->] (e1) -- (4.5,3);
    
    \draw[decorate,decoration={brace,mirror,raise=6pt,amplitude=10pt}, thick] (0.8,4.2)--(0.8, 0.8);
    \node at (-0.3, 2.67) {vertex};
    \node at (-0.3, 2.43) {nodes};
    \draw[decorate,decoration={brace,mirror,raise=6pt,amplitude=10pt}, thick] (2.8,4.7)--(2.8, 0.3);
    \node at (1.7, 2.67) {edge};
    \node at (1.7, 2.43) {nodes};
    \draw[decorate,decoration={brace,raise=6pt,amplitude=10pt}, thick] (9.5,4.2)--(9.5, 0.8);
    \node at (10.8, 2.5) { pairs};
    \draw[decorate,decoration={brace,raise=6pt,amplitude=10pt}, thick] (4.6,4.4)--(9.4, 4.4);
    \node at (7, 5.2) {};
    \end{tikzpicture}
  \caption{Illustration of the influence graph used in the reduction from \probColorClique.
  In this example arcs are shown for one of the edge nodes.
  An arc from an edge node to a set of color pair nodes is used to represent the  arcs to all nodes for this color pair.
  All arcs have an influence weight of 1.}
  \label{fig:MultiColoredCliqueReduction}
\end{figure}    
    
    
  Consider an instance  of \probColorClique.
  We assume that , otherwise the instance can be solved trivially.
  We construct an instance of \probEffectors with ,  and an influence graph (see \autoref{fig:MultiColoredCliqueReduction} for an illustration) defined
  as follows.
  Add  nodes for each unordered pair of distinct colors.
  Let us call these nodes \emph{color-pair nodes}. These color-pair nodes are the target nodes~, thus~.
  Now, add a \emph{vertex node}  for each ,
  add an \emph{edge node}  for each ,
  and add arcs .
  For each edge~, let  be the color-pair nodes corresponding to the
  colors of  and  and add arcs .
  Finally, set the influence weights of all arcs to 1.
  
  Let  be the influence graph obtained by the above construction
  and notice that  is a DAG.
  We show that there is a -vertex multi-colored clique in~ if
  and only if there is a size- set of effectors that incurs a
  cost of at most~ in~.
   
  Suppose that there is a multi-colored clique with  vertices in
  . Let  be the edge nodes corresponding to the edges of this
  clique.
  Clearly, . These effectors activate all
  color-pair nodes, that is, the complete target set  with
  probability 1.
  Furthermore, the non-active edge and vertex nodes corresponding to the
  clique are activated, and a total cost of  is
  incurred.
   
  For the reverse direction, let  be a size- set of effectors
  that incurs a cost of at most  in . Directly picking a vertex
  node is not optimal, since they are non-target nodes without outgoing arcs.
  Hence, they can only increase the cost.
  Also, without loss of generality, we can assume that~ does not contain a color-pair node .
  To see this, assume the contrary and
  suppose that~ contains at least one edge node which 
  influences . Then  is a solution with equivalent cost
  and smaller budget. In the other case, suppose that no such edge node is in .   
  Then, we pay for at least  other nodes corresponding to the
  same color-pair as  since we can only take  out of
   nodes. Directly picking an edge node instead of  incurs a cost of at most 3.
  By assumption, , that is, any optimal solution
  can be replaced by one that chooses only edge nodes as effectors.
  Now, in order to avoid a cost higher than , 
  every color-pair node must be directly activated by an edge node.
  Then  must contain exactly  edge nodes, one for each color pair. A cost of at most  is only obtained if they activate at most  vertex nodes, i.e., the edges corresponding to the chosen edge nodes must form
  a multi-colored clique with  vertices.

  We continue with the second statement,
  namely, that \probEffectors, parameterized by the combined parameter ,
  is -hard, even if  and~ is a DAG.
  We provide a parameterized reduction from the -complete
  \probDominatingSet problem~\cite{DF13}.
  \probDef
    {\probDominatingSet}
    {A simple and undirected graph , .}
    {Is there a vertex subset  such that  and for each  either  or  such that ?}
  Consider an instance  of \probDominatingSet. We construct an instance for \probEffectors with , and obtain the influence graph (see \autoref{fig:DominatingSetReduction} for an illustration) as follows:
  Add a node  and a set of nodes  for each vertex . Let us call these the \emph{initiator} and \emph{copies} of , respectively. We connect each initiator of  to all of its copies by adding arcs . In a similar fashion, for each edge , we connect the initiator of  to all copies of  and vice versa. Finally, let the set of target nodes  contain all copies of vertices and set the influence weight of all arcs to 1.
  
  \begin{figure}[t]
  \centering
  \begin{tikzpicture}[>=stealth, scale=0.8]
    \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
    \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]
    
    \node[inactive] (i1) at (0,4) {};
    \node[inactive] (i2) at (0,3) {};
    \node at (0,2) {};
    \node[inactive] (in) at (0,1) {};
    
    \node[active] (c10) at (3,4) {};
    \node[active] (c11) at (4,4) {};
    \node at (5,4.0) {};
    \node[active] (c1k) at (6,4) {};
    
    \node[active] (c20) at (3,3) {};
    \node[active] (c21) at (4,3) {};
    \node at (5,3) {};
    \node[active] (c2k) at (6,3) {};
    
    \node[active] (cn0) at (3,1) {};
    \node[active] (cn1) at (4,1) {};
    \node at (5,1) {};
    \node[active] (cnk) at (6,1) {};
     
    \draw[rounded corners=8pt,line width=1.2pt] (2.5,4.4) rectangle (6.5,3.6);
    \draw[rounded corners=8pt,line width=1.2pt] (2.5,3.4) rectangle (6.5,2.6);
    \node at (4.5,2) {};
    \draw[rounded corners=8pt,line width=1.2pt] (2.5,1.4) rectangle (6.5,0.6);
   
    \draw[thick,->] (i1) -- (2.5,4);
    \draw[thick,->] (i2) -- (2.5,3);
    \draw[thick,->] (in) -- (2.5,1);
    
    \draw[thick,->] (i1) -- (2.5,3);
    \draw[thick,->] (i2) -- (2.5,4);
    
    \draw[decorate,decoration={brace,mirror,raise=6pt,amplitude=10pt}, thick] (-0.2,4.2)--(-0.2, 0.8);
    \node[left] at (-1, 2.5) {initiators};
    
    \draw[decorate,decoration={brace,raise=6pt,amplitude=10pt}, thick] (2.6,4.4) -- (6.4,4.4);
    \node at (4.5,5.3) { copies};
  \end{tikzpicture}
  \caption{Illustration of the influence graph in the reduction from
    \probDominatingSet. The vertices corresponding to the two
    initiators at the top are neighbors in the input graph.
    An arc from an initiator to a set of copies is used to represent
     arcs, one to each copy.
    All arcs have an influence weight of 1.}
  \label{fig:DominatingSetReduction}
\end{figure}
  
  Let  be the influence graph obtained in the construction and
  note that~ is a DAG.
  We show that there is a size- set~ that dominates all vertices in~ if and only if there is a size- set~ of effectors that incurs a cost of at most~ in~.
  Suppose that~ is a -dominating set for . Let  be the initiators of vertices in~. These effectors activate all copies of vertices, i.e., the complete target set~ with probability 1. Clearly,  and a cost of  is incurred for picking the initiators as effectors.
  
  For the reverse direction, let  be a size- set of effectors
  that incur a cost of at most~ in . Consider a solution in which we directly
  pick a copy~ of a vertex~ as an effector. Suppose that~ contains the initiator of~ or one of its neighbors. 
  Then  is a solution with equivalent cost and smaller budget. In the other case, suppose that  contains no such initiator. Then, we pay for at least
  one other copy of  since we can take at most  out of 
  copies.
  Therefore, any optimal solution can be replaced by one that chooses
  only initiators as effectors. Now, every copy must be directly
  activated by an initiator to avoid a cost higher than . 
  Furthermore,  can contain at most  initiators. These initiators can 
  only influence copies of their corresponding vertex or its neighbors,  
  that is, the vertices corresponding to the chosen initiators are a
  -dominating set.   
\end{proof}

\subsection{Special Case: Unlimited Budget}\label{section:infiniteBudget}
Here, we concentrate on a model variant where we are allowed to choose any number of effectors,
that is, the goal is to minimize the overall cost with an unlimited budget of effectors.
In general, \probEffectors with unlimited budget remains intractable, though.

\begin{theorem}\label{thm:inftyNP}
  If ,
  then \probEffectors, even with unlimited budget, is not
  polynomial-time solvable on DAGs.
\end{theorem}

\begin{proof}
  We consider the following -hard~\cite{VLG1979A} counting problem.
  \probSharpDef
    {\probSTConnectness}
    {A directed acyclic graph , two vertices .}
    {Number of subgraphs of  in which there is a directed path from  to .}
  
  In the following, let~ denote the number of subgraphs
  of~ in which there exists a directed path from~ to~ (where distinct isomorphic subgraphs are considered different).
  We give a polynomial-time reduction from the decision version of
  \probSTConnectness, which asks whether~ is at least
  a given integer~.
  
  Let~ be an instance of the decision version of \probSTConnectness.
  We create an \probEffectors instance 
  as follows.
  Let~ be the set of vertices that
  lie on some directed path from~ to~ and let~ be the set of arcs of all directed paths from~ to~.
  Further, let~.
  Clearly, it holds~ since~.
  Thus, in order to decide whether~, we have to
  decide whether~, where~.
  
  We initialize  as the induced subgraph~
  and set~ for each~.
  We further create a copy  for each vertex ,
  and add the arc~ with .
  We also create a copy  of ,
  and add the arc~ with , where .
  Finally, we set , , and .
  The construction is illustrated in \autoref{fig:inftyNP}.
  \begin{figure}[t]
    \centering
    \begin{tikzpicture}[>=stealth,scale=0.6]
      \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
      \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]

\node[inactive,label=above:,fill=lightgray] (s) at (0,6) {};
      \node[inactive,label=below:,fill=lightgray] (t) at (0,0) {};
      \node[inactive,fill=lightgray] (v1) at (-1,5) {};
      \node[inactive,fill=lightgray] (v2) at (1,4) {};
      \node[inactive,fill=lightgray] (v3) at (0,3) {};
      \node[inactive,fill=lightgray] (v4) at (-1,2) {};
      \node[inactive,fill=lightgray] (v5) at (1,1) {};
\node[inactive] (v6) at (-2,6) {};
      \node[inactive] (v7) at (2,2) {};
      \node[inactive] (v8) at (-2,3) {};

\draw[->] (s) -- (v1);
      \draw[->] (s) -- (v2);
      \draw[->] (s) -- (v3);
      \draw[->] (v1) -- (v4);
      \draw[->] (v2) -- (v5);
      \draw[->] (v3) -- (v2);
      \draw[->] (v3) -- (v4);
      \draw[->] (v4) -- (t);
      \draw[->] (v5) -- (t);
\draw[->] (v6) -- (s);
      \draw[->] (v6) -- (v8);
      \draw[->] (v1) -- (v8);
      \draw[->] (v2) -- (v7);        
    \end{tikzpicture}
    \hspace{8em}
    \begin{tikzpicture}[>=stealth,scale=0.6]
      \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
      \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]

\node[active,label=above:] (s) at (0,6) {};
      \node[inactive,label=above:] (s') at (3,6) {};
      \node[inactive,label=below:] (t) at (0,0) {};
      \node[active] (v1) at (-1,5) {};
      \node[active] (v2) at (1,4) {};
      \node[active] (v3) at (0,3) {};
      \node[active] (v4) at (-1,2) {};
      \node[active] (v5) at (1,1) {};
      \node[inactive] (v1') at (3,5) {};
      \node[inactive] (v2') at (3,4) {};
      \node[inactive] (v3') at (3,3) {};
      \node[inactive] (v4') at (3,2) {};
      \node[inactive] (v5') at (3,1) {};

\draw[->,dashed] (s) -- (v1);
      \draw[->,dashed] (s) -- (v2);
      \draw[->,dashed] (s) -- (v3);
      \draw[->,dashed] (v1) -- (v4);
      \draw[->,dashed] (v2) -- (v5);
      \draw[->,dashed] (v3) -- (v2);
      \draw[->,dashed] (v3) -- (v4);
      \draw[->,dashed] (v4) -- (t);
      \draw[->,dashed] (v5) -- (t);
\draw[->] (s) -- (s') node[midway, above] {};
      \draw[->,thick] (v1) -- (v1');
      \draw[->,thick] (v2) -- (v2');
      \draw[->,thick] (v3) -- (v3');
      \draw[->,thick] (v4) -- (v4');
      \draw[->,thick] (v5) -- (v5');
    \end{tikzpicture}
    \caption{Example illustrating the construction in the proof of~\autoref{thm:inftyNP}. 
      Left: A directed acyclic graph with two distinguished vertices~ and~, where the gray vertices lie on a directed~--path.
      Right: The corresponding influence graph with target nodes colored in black.
      Dashed arcs have an influence weight of~1/2 and thick arcs have an influence weight of~1.
    }
    \label{fig:inftyNP}
  \end{figure}
  
  In the following, we prove two claims used to show the correctness
  of the above reduction.
  First, we claim that an optimal solution~ of~ either
  equals~ or~.
  This can be seen as follows.
  Choosing~, , or any copy  to be an effector is never optimal
  as these are all non-target nodes without outgoing arcs.
  Now, assume that~ contains a node~ and let~.
  Then, we have
  
  Since~ is a DAG, it holds that there is no directed path from~ to~ and thus
   and consequently also~,
  Moreover, note that  and~ holds for all~,
  and~ clearly holds since~.
  Hence,  and
  therefore~ is also an optimal solution not containing~,
  which proves the claim.

  Next, we claim that~.
  To prove this, we define an~\emph{--scenario}~ to be a subset of arcs such that~ and there
  is a directed path from~ to each~ in~.
  Let~
  denote the set of all outgoing arcs from nodes in~.
  We denote the set of all --scenarios by~.
  Note that each scenario~ constitutes a possible propagation
  in which exactly the arcs in~ activated their endpoints and
  the arcs in~ did not activate their endpoints.
  The probability~ for a given --scenario~ to occur is
  thus~.
  Clearly, we can write
  

  Now, for a subset~ of arcs where~ is
  connected to~ in the subgraph~, let~ denote
  the scenario~ where~ and
   for all~ such that~.
  It holds that~, where~ and
  .
  Hence, we have~, which proves the claim.

  We now decide the instance~ as follows.
  Note that~
  and~.
  Therefore, if~ is a ``yes''-instance, then~ is the
  optimal solution with , which implies~.
  It follows that .
  Therefore, ~is a ``no''-instance.
  If~ is a ``no''-instance, then~, which implies~,
  hence~ is a ``yes''-instance.     
\end{proof}

With unlimited budget, however, \probEffectors becomes fixed-parameter
tract\-able with respect to the parameter number  of probabilistic arcs.

\begin{theorem}\label{thm:inftyFPTr}
  If , 
  then \probEffectors is solvable in  time,
  where~ is the number of probabilistic arcs.
\end{theorem}

\newcommand{\Xplus}{X_{o}}
\newcommand{\Xp}{X_p}
\newcommand{\Yplus}{Y_{o}}
\newcommand{\Yp}{Y_p}
\newcommand{\Vplus}{V_{o}}
\newcommand{\Vp}{V_p}

\begin{proof}
\begin{figure}[t]
  \centering
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{active}=[circle,draw,fill=black,minimum size=5pt,inner sep=3pt]
    \tikzstyle{inactive}=[circle,draw,minimum size=5pt,inner sep=3pt]
    \tikzstyle{xaura}=[circle,draw,minimum size=5pt,inner sep=3.5pt, line width=3pt,color=black!40]
    \tikzstyle{prob}=[circle,draw,minimum size=5pt,inner sep=0pt]
                \foreach \x/\y [count = \xi] in {1.5/3,3.5/3}{
                   \node[xaura] () at (\x,\y) {};
                   \node[prob] (Vp\xi) at (\x,\y) {\sf X};
    }           
                \foreach \x/\y [count = \xi from 3] in {7.5/3,9.5/3}{
                   \node[prob] (Vp\xi) at (\x,\y) {\sf X};
    }   
                \foreach \x/\y [count = \xi] in {0/2, 1/5, 4.8/0.5, 5.5/1.5, 6.2/0.5,5.5/4, 8/5,8/1,10/5,10/1 }{
                   \node[inactive] (Vy\xi) at (\x,\y) {};
    }
                \foreach \x/\y [count = \xi] in {1/1,3/1,3/5,0/4,0/2, 4.5/5,6/5}{
                   \node[xaura] () at (\x,\y) {};
                   \node[inactive] (Vx\xi) at (\x,\y) {};
    }   
                \foreach \s/\t in {p1/x1,p1/x2,p2/x2,p4/y8,x4/x5, x3/p1,x3/p2, y2/x3, y3/y5,y5/y4,y4/y3,y6/p2, y7/p3,y7/p4,y9/p4,x6/x7} {
                                \draw[->] (V\s) -- (V\t);
                }
                \foreach \s/\t in {p1/y1,p2/y3,p2/p3,p3/y5,p3/y6, p4/y10} {
                                \draw[->, dashed] (V\s) -- (V\t);
                }
                
                \draw[rounded corners=8pt,line width=1.2pt] (0.4,3.6) rectangle (10.6,2.4);
                \node at (5.5,3.2) {};
\draw[rounded corners=8pt,line width=1.2pt] (0.5,3.5) rectangle (4.0,0.5);
                \node at (2.5,3) {};
                \node at (1.7,1.8) {};
                \draw[rounded corners=8pt,line width=1.2pt] (7.0,5.5) rectangle (10.5,2.5);
                \node at (8.5,3) {};
                \node at (9.2,4.2) {};
                
                
                \node at (6.5,4) {};
                
  \end{tikzpicture}
  \caption{Illustration for~\autoref{thm:inftyFPTr}.
  Effectors of a solution are marked with an aura. 
  Probabilistic arcs are dashed, and nodes of  (with an outgoing probabilistic arc) are marked with a cross. 
  For readability, target nodes are not represented. Intuitively, the algorithm guesses the partition of  
  into  (effectors) and  (non-effectors). Node set  (respectively,~) is then extended to its closure 
   (respectively, its closure  in the reverse graph). The remaining nodes form a deterministic subgraph , 
  in which effectors, forming the set~, 
  are selected by solving an instance of {\sc Maximum Weight Closure}. }
  \label{fig:inftyFPTr}
\end{figure}

  The general idea is to fully determine the probabilistic aspects of the graph,
  and then to remove all of the corresponding nodes and arcs.
  We can show that this leaves an equivalent ``deterministic graph'' that we can solve using a reduction to 
  the problem \probClosure,
  which is itself polynomial-time solvable by a polynomial-time reduction to a flow maximization problem~\cite[Chapter~19]{AMO93}.
  \probSharpDef
    {\probClosure}
    {A directed graph  with weights on the vertices.}
    {A maximum-weight set of vertices   with no arcs going out of the set.}    
  We start with some notation (see~\autoref{fig:inftyFPTr} for an illustration).
  For an input graph ,
  let  denote the set of
  probabilistic arcs
  and let  denote the set of
  nodes with at least one outgoing probabilistic arc.
  For a node , let
  
  ()
  denote the set of all nodes  such that there exists at least
  one deterministic path
  from  to 
  (respectively, from  to ),
  where a deterministic path is a path containing only deterministic arcs.
  We extend the notation to subsets  of  and write
  
  and
  .
  We call a subset~ of nodes
  \emph{deterministically closed} if and only if~,
  that is, there are no outgoing deterministic arcs from~ to~.
  
  Our algorithm will be based on a closer analysis of
  the structure of an optimal solution.
  To this end, let~ be an input graph with a
  set~ of target nodes and let~ be an
  optimal solution with minimum cost~.
  Clearly, we can assume that~ is deterministically closed, that
  is,~, since we have an infinite budget~.
         
        We write~ as a disjoint union of  and .
	We also use , , and~.
	Since  is deterministically closed, we have that  and .
	We write  and . 
	Note that  is deterministically closed in~ and that~ contains only deterministic arcs.
	Moreover, note that the sets , , , , and~, are directly deduced from the choice of ,
	and that for a given~, the set  can be any deterministically
        closed subset of .
  
  We first show that the nodes in  are only influenced by effectors in~, that is, for any node~, it holds that
  . This is clear for , since in this case .
  Assume now that there is a node~ with a directed path to~ 
	that does not contain  any node from~ (if every directed path from~ to~ passes through~, then clearly~ does not influence~). 
	Two cases are possible, depending on whether this path is deterministic.
	If it is, then, since , there exists a deterministic path from  to some , via .
	Hence, , yielding a contradiction.
	Assume now that the path from  to~ has a probabilistic arc and write  for the first such arc. Hence,  and . Since we assumed that the path does not contain any node from~, we have , and therefore . Again, we have , yielding a contradiction. 
        Hence, the nodes in~ are not influenced by the
        nodes in~.
        
   Now consider the nodes in . Note that we have~ for~ and 
  for~, since~ is deterministic and~~is
  deterministically closed.
   \iffalse
  Thus, we can write the cost~ as  
  
\fi
    Overall,  for all . The total cost of solution~ can now be written as
  
where
  
  We further define, for all ,
   if , and
   if .
  Note that, for , the
  difference~
  is exactly , hence .
 
  \iffalse
  Thus, by comparison with the solution where no vertex in  is activated, 
  activating a vertex~ decreases the
  cost by~, while activating a vertex~
  increases the cost by~.
  
  An optimal solution~ thus fulfills the following:
  
  where we define
  
  for
  
  and
    
  \fi

  \begin{algorithm}[t]
    \normalsize
    \SetAlgoNoLine
    \caption{\label{algo:inftyFPTr}Pseudocode for \probEffectors with }
    \label{alg:inftyFPTr}
    \ForEach{}{
      compute  \\
      compute  \\
      \ForEach{}{
        compute  (using~\autoref{thm:costFPTr}) and 
      }
      compute  \\
      compute  maximizing  using \probClosure on~, with weights  \\
    }
    \Return the  which gives the minimum 
  \end{algorithm}

  The algorithm can now be described
  directly based on the above formulas.
  Specifically, we branch over all subsets~ (note that the number of these subsets is upper-bounded by ).
  For each such subset~,
  we can compute  and~ in linear time because this
  involves propagation only through deterministic arcs
  (outgoing for  and ingoing for ).
  Then, for each node , 
  we compute~ using~\autoref{thm:costFPTr}
  in  time.
  This yields the values~ and~ for each .
  By the discussion above, it remains to select a closed subset~
  such that the cost  is minimized. 
  That is, we have to select the subset~ that maximizes the value of~.
  Hence, the subset~ can be computed as the solution of \probClosure on  (which is solved by a maximum flow computation in  time),
  where the weight of any  is~.
  Finally, we return the set~ that yields the minimum cost .
  A pseudocode is given in~\autoref{alg:inftyFPTr}. 
\end{proof}

\subsection{Special Case: Influence Maximization}\label{section:everythingActive}
In this section, we consider the special case of \probEffectors, called \textsc{Influence Maximization}, where all nodes are targets ().
Note that in this case the variant with unlimited budget and the
parameterization by the number of target nodes are irrelevant.

In the influence maximization case, on deterministic instances, one should intuitively choose 
effectors among the ``sources'' of the influence graph, that is, nodes 
without incoming arcs (or among strongly connected components without incoming arcs). 
Moreover, the budget~ bounds the number of sources that can be selected,
and the cost~ bounds the number of sources that can be left out. 
In the following theorem,
we prove that deterministic \probEffectors remains intractable even if either one of
these parameters is small,
but, by contrast,
having  as a parameter yields fixed-parameter tractability in the deterministic case.
We mention that the first statement is proven by a reduction from the -hard \textsc{Set Cover} problem,
while the second statement is proven by a reduction from the -hard \textsc{Independent Set} problem.

\begin{theorem}
  \label{thm:infmax}
  \mbox{}
  \begin{enumerate}
    \item \textsc{Influence Maximization},
      parameterized by the maximum number  of effectors,
      is -hard, even if  is a deterministic () DAG.
    \item \textsc{Influence Maximization},
      parameterized by the cost ,
      is -hard, even if  is a deterministic () DAG.
    \item If ,
      then \textsc{Influence Maximization} can be solved in~ time.
  \end{enumerate}
\end{theorem}

\begin{proof}
  We begin with the first statement,
  namely,
  that \textsc{Influence Maximization}
  (which is equivalent to \probEffectors where all nodes are target nodes, that is, where ),
  parameterized by the maximum number  of effectors,
  is -hard, even if  is a deterministic () DAG.
  We provide a parameterized reduction from the -complete \probSetCover problem~\cite{DF13}.
  \probDef
    {\probSetCover}
    {Sets  over elements , and parameter .}
    {Is there  such that  and ?}
  Given an instance for \probSetCover, we
  create an instance for \textsc{Influence Maximization} as follows.
  Add a node  for each set  and write .
  Add a node  for each element  and write .
  For each , add an arc  with influence
  probability~.
  Set , , and .
  
  We can assume that any solution~ is such that   and  . 
  Note that all nodes of  are activated if and only if  is a set cover for .
  Hence, any solution with cost 
   needs to pay only for the unselected nodes of , and yields a set cover of .
  Reversely, for any set cover~ for~ of size~, the set~
  is a set of effectors with cost at most~.
  
  We continue with the second statement,
  namely,
  that \textsc{Influence Maximization},
  parameterized by the cost ,
  is -hard, even if  is a deterministic () DAG.
  We provide a reduction from the following -complete problem~\cite{DF13}.
  \probDef
    {\probMaxIndependentSet}
    {An undirected graph   and parameter .}
    {Is there an independent set  (i.e., no edge has both endpoints in~) such that ?}
  Consider an instance  of \probMaxIndependentSet. Create an influence graph as follows:
  For each vertex , add a node  and for each edge~, add a node . 
  Let  and .
  Add an arc  with influence probability~1 for each edge  incident to vertex  in . 
  Set , , and .
 
  Consider any solution with cost . 
  Note that we can assume any optimal solution to choose only nodes from , since for any edge~ it is always better to choose either~ or~ instead of the node~.
  Write  for the set of effectors, , and .
  We have  and .
  Since the cost equals~, it follows that~ and only the nodes in~ are left inactive. Hence, no edge  has both endpoints in . That is,  is an independent set of size . 
  Conversely, any independent set  of  directly translates into a set of effectors  for the created influence graph.  

  We finish with the third statement, namely, that if ,
  then \textsc{Influence Maximization} can be solved in~ time.
  To start with, let~ be the condensation of  (that is, the DAG obtained by contracting each strongly connected component (SCC) of~ into one node). 
  Note that since , we can assume that any minimal solution selects at most one node from each SCC in~. Moreover, it does not matter which node of an SCC is selected since they all lead to the same activations.
  Hence, in the following, we solve \textsc{Influence Maximization} on the condensation~,
  where selecting a node means to select an arbitrary node in the corresponding SCC in~.
  
  Let  denote the set of nodes of  with in-degree zero. 
  Note that any node in~ not chosen as an effector yields a cost of at least~1 in~, since the nodes in its corresponding SCC cannot be activated by in-neighbors.
  Hence, we can assume that , because otherwise the instance is a ``no''-instance.
  Moreover, we can assume that all effectors are chosen from~. 
  Indeed, consider any solution selecting a node~ as effector. 
  Then,  has at least one in-neighbor~ and selecting~ instead yields the same number of effectors, while the cost can only be reduced (since at least as many nodes are activated). 
  Since the graph~ is a DAG, repeating this process yields a solution with smaller cost having only effectors in~.
  Hence, it is sufficient to enumerate all possible choices of
  size- subsets of the  nodes in , 
  and check in polynomial time whether the chosen set of effectors in~
  yields a cost of at most~.  
\end{proof}



\subsection{Results in Contradiction with \citet{LTGMH10}.}\label{sect:flawDetails}
The following two claims of \citet{LTGMH10} are contradicted by the results presented in our work.

According to~\citet[Lemma 1]{LTGMH10}, in the {\sc Influence Maximization} case with cost value , 
\probEffectors is NP-complete.
The reduction is incorrect: it uses a target node  which influences all other vertices with probability 1 (in at most two steps). 
It suffices to select  as an effector in order to activate all
vertices, so such instances always have a trivial solution
(), and the reduction collapses.
On the contrary, we prove in our \autoref{lem:det_c0_p} that all instances with  can be solved in linear time. 

According to the discussion of~\citet{LTGMH10} following
their Corollary~1, there exists a polynomial-time algorithm for \probEffectors with deterministic instances (meaning ).
Their model coincides with our model in the case of 
{\sc Influence Maximization}. However, the given algorithm is flawed: it does not consider the influence 
\emph{between} different strongly connected components. Indeed, as we prove in \autoref{thm:infmax}, 
finding effectors under the deterministic model is NP-hard, even in
the case of {\sc Influence Maximization}.

\section{Conclusion}
Inspired by work of \citet{LTGMH10}, we contributed a fine-grained computational complexity analysis of a ``non-monotone version'' of finding effectors in networks.
Indeed, we argued why we believe this to be at least as natural as the more restricted ``monotone model'' due to \citet{LTGMH10}. A particular case for this is that we may find more robust solutions, that is, solutions that are more resilient against noise.
The central point is that, other than \citet{LTGMH10}, we allow non-target nodes to be effectors as well, motivated by the assumption that knowledge about the state of a node may get lost from time to time (see \autoref{sec:Model} for further discussion). Altogether, we observed that both models suffer from computational hardness even in very restricted settings. For the case of unlimited budget, we believe that both models coincide with respect to solvability and hence with respect to a fine-grained computational complexity classification.

Our work is of purely theoretical and classification nature. One message for practical solution approaches we can provide is that it may help to get rid of some probabilistic arcs by rounding them up to 1 (making them deterministic) or rounding them down to~0 (deleting the arcs)---this could be interpreted as some form of approximate computation of effectors. Network structure restrictions seem to be less promising since our hardness results even hold for directed acyclic graphs. Still, there is hope for finding further islands of tractability, for instance by ignoring budget constraints and restricting the degree of randomness.

We leave several challenges for future research.
First, it remains to prove or disprove \autoref{con:infinite-budget}.
Moreover,
while we considered most of the parameterizations for most of the variants of the \probEffectors problem,
we have left some work for future research,
specifically the parameterized complexity of \probEffectors where there is infinite budget and arbitrarily-many probabilistic arcs
(see the corresponding open question in~\autoref{table:results}).
A further,
more general direction would be to consider other diffusion models and other cost functions.
For example,
it is also natural to maximize the probability that precisely the current activation state is achieved
when selecting the effectors to be initially active.
Moreover,
it seems as if the current diffusion model and its somewhat complicated probabilistic nature
is one of the main reasons for the intractability of our problem.
It would be interesting to consider other diffusion models,
possibly simpler ones,
and see whether it is possible to push the tractability results to apply for more cases.
Specifically,
it would be interesting to extend our results concerning the parameter ``degree of randomness'' to such models.




\bibliographystyle{abbrvnat}
\bibliography{bibliography}

\end{document}
