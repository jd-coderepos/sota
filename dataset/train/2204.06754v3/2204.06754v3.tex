 \pdfoutput=1

\documentclass[11pt]{article}
\RequirePackage[OT1]{fontenc}
\RequirePackage{amsthm,amsmath}
\RequirePackage[numbers]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks]{hyperref} 
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amsmath}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}
\usepackage{bbm}
\usepackage{amssymb, enumerate}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{extarrows}
\usepackage{arydshln}

\usepackage{caption}
\usepackage{multirow}



\usepackage{amsmath,amsfonts,amsthm, amssymb, enumerate}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{amssymb,graphicx,subfigure}
\usepackage[bottom]{footmisc}
\usepackage[dvipsnames]{xcolor}
\usepackage{array,multirow}
\usepackage{enumitem}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amssymb}\usepackage{pifont}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}  
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsfonts, amsthm, enumerate} 
\usepackage[bottom]{footmisc} 
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{extarrows}
\usepackage{makecell}
\usepackage{bbm}  
\usepackage{textcomp}
\usepackage{xcolor} 
\usepackage{caption} 




\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\simi}{\operatornamewithlimits{sim}}

\newcommand{\minmax}{\operatornamewithlimits{mnmx}}
\newcommand{\average}{\operatornamewithlimits{avg}}



\usepackage[font=small,labelfont=bf]{caption}


\newcommand{\sh}[1]{\textcolor{blue}{(SH)#1}}
\newcommand{\ks}[1]{\textcolor{red}{(KS)#1}}
\newcommand{\iij}[1]{\textcolor{magenta}{(IJ)#1}}
 

\usepackage{algorithm,algpseudocode}
\algnewcommand\TR{\item[{\textbf{Training phase}}]}
\algnewcommand\TE{\item[{\textbf{Test phase}}]}
\algnewcommand\Input{\item[{{Input:}}]}
\algnewcommand\Output{\item[{{Output:}}]}
\algnewcommand\Initialize{\item[{{Initialize:}}]}
\algnewcommand{\return}[1]{
	\State \textbf{return:}
	\Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}
\makeatletter
\newcommand{\algrule}[1][.2pt]{\par\vskip.5\baselineskip\hrule height #1\par\vskip.5\baselineskip}
\makeatother

\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\addtolength{\topmargin}{-.67in}
\addtolength{\textheight}{1.05in}
\usepackage{setspace}
\pdfminorversion=5 
\pdfcompresslevel=9
\pdfobjcompresslevel=2

\begin{document}
\title{RecurSeed and EdgePredictMix: Single-stage Learning is Sufficient for Weakly-Supervised Semantic Segmentation}
	\date{}
	\author{
		Sanghyun Jo\thanks{Equal contribution}, In-Jae Yu\footnotemark[1], Kyungsu Kim\thanks{Corresponding author: Kyungsu Kim (kskim.doc@gmail.com)}\\ 
		{\small OGQ GYN, Seoul, Korea}\\
		{\small Samsung Electronics, Suwon, Korea}\\
		{\small Medical AI Research Center, Samsung Medical Center, Seoul, Korea}\\
		{\small Sungkyunkwan University School of Medicine, Seoul, Korea}
} 
	 
	
	\maketitle 
	
\begin{abstract}
	Although weakly-supervised semantic segmentation using only image-level labels (WSSS-IL) is potentially useful, its low performance and implementation complexity still limit its application. The main causes are (a) non-detection and (b) false-detection phenomena: (a) The class activation maps refined from existing WSSS-IL methods still only represent partial regions for large-scale objects, and (b) for small-scale objects, over-activation causes them to deviate from the object edges. We propose RecurSeed which alternately reduces non- and false detections through recursive iterations, thereby implicitly finding an optimal junction that minimizes both errors. 
	We also propose a novel data augmentation (DA) approach called EdgePredictMix, which further expresses an object's edge by utilizing the probability difference information between adjacent pixels in combining the segmentation results, thereby compensating for the shortcomings when applying the existing DA methods to WSSS.
	We achieved new state-of-the-art performances on both the PASCAL VOC 2012 and MS COCO 2014 benchmarks (VOC \emph{val} , COCO \emph{val} ). The code is available at \url{https://github.com/shjo-april/RecurSeed_and_EdgePredictMix}.
\end{abstract}
	
\section{Introduction}
\label{section:intro}

Semantic segmentation is an essential task in computer vision, including object tracking and medical image analysis. {Most approaches \cite{chen2017deeplab, chen2018encoder, liu2019auto} employ pixel-wise annotations that require labor-intensive and time-consuming tasks. In addition, image- and pixel-level annotations have been reported \cite{bearman2016s} to require 20 and 239 seconds per image,  respectively.} To alleviate human labor, numerous efforts have been made to develop weakly supervised semantic segmentation (WSSS), to employ weak labels such as image-level class labels \cite{ahn2018learning, lee2019ficklenet, lee2021anti}, scribbles \cite{lin2016scribblesup}, points \cite{bearman2016s}, and bounding boxes \cite{khoreva2017simple, dai2015boxsup}. We concentrate on WSSS through image-level class labels (WSSS-IL) as the most economical among the aforementioned weak supervisions.


Most WSSS-IL approaches rely on a class activation map (CAM) \cite{zhou2016learning}, which is designed to visualize only the most discriminative part of an object, causing many false negatives (FNs). 
{Existing studies on WSSS-IL employ single- or multi-stage learning frameworks to alleviate this drawback. Although the latest major approaches \citep{zhou2022regional, chen2022self, jiang2022l2g} to WSSS-IL depend on a multi-stage learning framework (MLF), MLFs require additional training stages for distinct purposes, causing sophisticated training procedures. Furthermore, advanced MLFs employ external saliency supervision with image-level class labels (i.e., beyond the scope of WSSS-IL) to detect prominent foreground objects. However, image saliency ignores non-salient foreground objects (e.g., small or low-contrast objects), hindering the broadening of various datasets \cite{everingham2010pascal, lin2014microsoft}.  Although {a single-stage learning framework (SLF)} solves these limitations of an MLF, the performances of existing SLFs \cite{papandreou2015weakly, roy2017combining, zhang2020reliability, araslanov2020single} are generally still far behind those of recent MLFs. To compensate for these limitations, some researchers \cite{kolesnikov2016seed, huang2018weakly, zhang2020reliability} have applied a conditional random field (CRF) in the training loop of SLF. However, a CRF significantly increases the training time as a fatal drawback. To refine the initial CAM with minimal computational complexity, \citet{araslanov2020single} proposed an alternative called pixel-adaptive mask refinement (PAMR).} 

Despite advances in post-processing algorithms such as a CRF, a limitation remains regarding an \textit{insufficient spread} covering the object, resulting in many FNs in current studies.  
{The self-correlation map generation (SCG) proposed by \citet{pan2021unveiling} complements the insufficient spread phenomenon by leveraging the second-order correlation of the feature maps.}
Although the intrinsic properties of an SCG, considering the high-order correlation, effectively reduce the FNs of the CAM on large-scale objects (e.g., ImageNet and CUB-200-2011), they also tend to increase the number of false positives (FPs) on middle- or small-scale objects (e.g., PASCAL VOC and MS COCO). These mismatching issues of FNs and FPs on WSSS-IL are called FPN-WSSS-IL. 

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{Figure_Seed_v3.pdf}
\caption{
      {Comparison of localization maps produced by using conventional CAMs, existing single-stage frameworks (SLFs), and our SLF.  
      Our results represent both foreground and background regions more accurately than other localization maps through a recursive learning strategy of SLF that minimizes both non- and false-detection of the  foreground.  
      } 
  }
  \label{fig:intro}


\end{figure}

{Meanwhile}, data augmentation (DA) is a simple method that significantly improves data efficiency. 
Notable examples include regional-level augmentation methods, such as Cutout \cite{devries2017improved} and CutMix \cite{yun2019cutmix}, which {cut and paste} randomly selected rectangular regions of the images, or SaliencyGrafting (SG) \cite{park2021saliency}, CDA \cite{su2021context}, and ClassMix \cite{olsson2021classmix}, which directly {utilize predicted masks from the model for the mix. However, in the WSSS-IL setup, the simple synthesis without any refinements using predicted masks inevitably accelerates the ambiguity of mixed results due to \textit{insufficient spread}}. We named this issue for {incomplete} recognition of the object boundary region in the existing DA approaches as IBDA-WSSS-IL.

{We propose RecurSeed (RS) and EdgePredictMix (EPM) to resolve FPN-WSSS-IL and IBDA-WSSS-IL simultaneously. RS is a novel approach that recursively rectifies the initial CAM by leveraging both SCG and PAMR beyond their original purpose of post-processing. EPM is the first DA approach to resolve IBDA-WSSS-IL, which remedies the uncertainty of mixed results by reviving edge information of target objects. Figure \ref{fig:intro} presents several examples of PASCAL VOC 2012  set, showing the quality of our SLF method compared with the conventional CAM and other SLF approaches.} 
Their main characteristics and contributions are summarized as follows.

\begin{itemize}
     \item 
{We \textit{newly apply recursion} to the combination of post-processing SCG/PAMR to mitigate FPN-WSSS-IL.} Because SCG discovers the spatial information in the the second-order based correlation features, it reduces the FNs by covering the sufficient foreground of an object that the CAM does not capture. By contrast, PAMR quickly reduces the FPs by capturing the exact boundary of the initial seed. Accordingly, {RS recursively transfers the knowledge of the seed refinement result considering the high-order correlation to the network;} it integrates the SCG and PAMR recursively to exploit both advantages making a seed that minimizes both the FP and FN. {This substantially improves (e.g., by 11.4\%) the performance limitations of {existing studies, the recursion based on the first-order correlation \cite{wang2020self, zhang2021complementary, chen2022self}}, by distillating more accurate information to the network. } 
\item  
To address the IBDA-WSSS-IL, we \textit{propose a novel edge refinement (EP)} technique suitable for mixing augmentation (EPM). Our EPM refines predicted masks before mask-to-mask synthesis by exploiting the absolute and relative per-pixel probability information. This diversity gain allowed for restoring the mask's boundary more accurately, maximizing the mixing effect and significantly improving the WSSS-IL performance. 
    \item  
        Our SLF, composed of RS and EPM, recorded the latest WSSS-IL performance of mIoU 70.6\% {on the PASCAL VOC 2012 test set}, whereas existing SLFs did not exceed 70.0\%. This result implies that our SLF suffices to accomplish advanced performance on WSSS-IL without the complicated learning configuration (i.e., MLF). Furthermore, the straightforward extension of our SLF to MLF using {a conventional method (RW)} also achieved the state-of-the-art performance of mIoU 74.4\% {on the PASCAL VOC 2012 validation set} (i.e., improved by more than 3.5\% over recent MLFs), validating the scalability of the proposed SLF as MLF. We also supported the superiority by proving that the proposed SLF or MLF showed the latest performance even in MS COCO data. 
\end{itemize}

\section{Related work}
\label{section:related}













\subsection{Weakly-supervised semantic segmentation}

The dominant approaches for WSSS-IL follow a multi-stage learning procedure: producing an initial CAM, generating pseudo masks, and training the segmentation model. In particular, gold standards employed region growing \cite{kolesnikov2016seed,huang2018weakly} or random walk (RW) \cite{ahn2018learning,ahn2019weakly} to produce pseudo labels from the initial seeds (e.g., CAM).

Most studies \cite{lee2019ficklenet, jiang2022l2g, lee2021reducing, wu2021embedded, xie2022clims, du2022weakly, lee2022threshold} concentrated on enhancing the quality of the initial seed (e.g., CAM). FickleNet \cite{lee2019ficklenet}, Puzzle-CAM \cite{jo2021puzzle}, and L2G \cite{jiang2022l2g} enlarged seed regions by expanding the CAMs through the patch-based dropout principle. AdvCAM \cite{lee2021anti} and RIB \cite{lee2021reducing} proposed anti-adversarial frameworks to manipulate the CAMs by exploiting images or fully trained weights. MCIS \cite{sun2020mining}, EDAM \cite{wu2021embedded}, and RCA \cite{zhou2022regional} capture information from several images by reflecting cross-image semantic similarities and differences. {To mitigate the FN of CAM, MCTformer \cite{xu2022multi} proposed a transformer-based architecture by exploiting global information. {Our method is a model-agnostic learning technique without structural dependencies such as transformers, and achieves the latest performance even on a traditional convolutional neural network-based backbone with fewer parameters.} AM \cite{xie2022c2am}, ADEHE \cite{liu2022adaptive}, and SANCE \cite{li2022towards} were independent studies to eliminate an erroneous region in pseudo masks. All aforementioned studies did not consider a recursion process unlike our RS.}






{Similar to the proposed RS, SEAM \cite{wang2020self}, CPN \cite{zhang2021complementary}, and SIPE \cite{chen2022self} also consider the recursion process but they only applied it to the pseudo label obtained by considering the first-order correlation (e.g., a pixel correlation module), causing insufficient improvement of FN. By contrast, our recursion in RS enables the reduction of both FP and FN by considering the higher-order correlation and its subsequent refinement by the simple combination of SCG and PAMR, respectively.}



{For the simplicity and usability of SLF, EM-Adapt \cite{papandreou2015weakly} first developed an SLF of WSSS-IL by applying an expectation-maximization. RRM \cite{zhang2020reliability} and SSSS \cite{araslanov2020single} proposed an SLF by refining the naive CAM with the CRF (or PAMR) in the training loop, increasing the FN. AFA \cite{ru2022learning} integrated the standard transformer into SLF to exploit the inherent advantage of the multi-head attention. {Despite these attempts towards advancement}, recent SLF approaches \cite{zhang2020reliability, araslanov2020single, ru2022learning} have been getting less attention lately, owing to the significant gaps between the SLF and MLF.}
Our study also belongs to an SLF {but surprisingly mitigates FPN-WSSS-IL by utilizing the recursive update and a novel edge refinement in mixing augmentation. When extending the proposed SLF to MLF (e.g., applying RW), our approach improved over recent WSSS methods by a large margin (+3.5\%).}

{AuxSegNet \cite{xu2021leveraging}, W-OoD \cite{lee2022weakly}, and CLIMS \cite{xie2022clims} employed additional supervison, such as external saliency module and datasets, to remedy the shortcomings of the initial CAM. Our results achieved state-of-the-art performance without the use of such additional supervision.}



\subsection{Data augmentation}
{CutMix \cite{yun2019cutmix} crops a random region of an image and pastes it on another. Saliency Grafting (SG) \cite{park2021saliency} reflects the degree of occlusion in mixed class labels according to the saliency map generated from the CAM to prevent misleading labels. CDA \cite{su2021context} is the first DA technique for improving the WSSS performance. ClassMix \cite{olsson2021classmix} mixes unlabeled images with decoder outputs by training the part of pixel-wise annotations.}  



{Previous works, including all WSSS studies, employed affine transformation \cite{wang2020self, du2022weakly}, cropping \cite{jo2021puzzle, jiang2022l2g}, and mixing \cite{su2021context, olsson2021classmix} for data augmentation. To the best of our knowledge, our EPM only considered an edge refining method to enhance the quality of pseudo labels for data augmentation. Precisely, our EP method} compensated uncertain regions by leveraging the edge information derived from indistinct cues of predicted masks. Table \ref{tab:seed} compares the proposed EPM with the other related DA works in terms of critical properties.

\begin{table}[t] 
  \caption{  
    Differentiation in {the proposed EPM and the existing DA methods}: CutMix \cite{yun2019cutmix}, SG \cite{park2021saliency}, CDA \cite{su2021context}, ClassMix \cite{olsson2021classmix}.
  } 
  \centering
  \begin{scriptsize} 
  \begin{tabular}{p{0.20\textwidth} || c c c c c}
    \toprule
    Properties & CutMix & SG & CDA & ClassMix & \textbf{EPM} \\  
    \hline 


    Use predicted mask in mix&   &  \checkmark &  \checkmark &  \checkmark &  \checkmark \\
    Use mixed mask in learning  &   &  &  &  \checkmark &  \checkmark \\
    Consider mix in WSSS &   &  &  \checkmark &  &  \checkmark \\
    Refine predicted mask &  &  &  &  &  \checkmark \\
    
    \bottomrule
  \end{tabular}
  \label{tab:seed}
  \end{scriptsize}
\end{table}


\section{Preliminaries}

\subsection{SCG}
\label{section:scg}
The self-correlation map generating module (SCG) \cite{pan2021unveiling} exploits the second-order self-correlation {(SC)} of the feature maps to extract their inherent structural information. SCG uses the first- and second-order SCs  for a given feature map  of the -th layer of the network.
\begin{small}

\end{small} 
where  denotes the min-max normalization operator along the -th axis,  denotes the function taking the mean along the -th axis, and  denotes the element-wise product.
Then, SCG combines the first- and second-order SCs for L different layers into the following SC, called HSC:
\begin{small}

\end{small} 
and applies HSC to obtain the refined CAM as .
In addition, the proposed RS includes the SCG in the training loop to maximize the effectiveness of the SCG. 
However, the SCG is designed to reduce the FNs of the CAM only at the inference time.
{We additionally reported the effects of the high-order feature correlation for SCG in Appendix.}

where  denotes the set of () indices satisfying  and  denotes the average activation maps generated from each pixel in  based on the HSC,
\vspace{0.1cm}
\begin{small}

\end{small} 



\subsection{PAMR}
\label{section:pamr}
Pixel-adaptive mask refinement (PAMR) proposed by \citet{araslanov2020single} iteratively refines the CAM  as  by exploiting the image pixel-level affinity matrix  for  of all pixels: \begin{small}

\end{small} 
where , , . Here,  is the average affinity value  across the RGB channels,  denotes the normalized distance between the -th and -th pixel values,  denotes the -th pixel value of the original image , and  denotes the standard deviation of the image intensity computed locally for the affinity kernel. In addition,  for  is set to zero.

Compared with existing methods (e.g., pixel adaptive convolution (PAC) \cite{su2019pixel} or CRF \cite{krahenbuhl2011efficient}), PAMR effectively reduces the computational complexity by narrowing the affinity kernel computation  to regions of contiguous pixels  rather than all pixels . 


\section{Method}

\begin{figure*}[t]\centering \includegraphics[width=1.00\linewidth]{./images/Figure_Overview_v6.pdf}
\caption{ 
Overview of the proposed SLF with RS and EPM. {Based on extracted features from an encoder, the classifier and decoder produce the CAMs () in \eqref{cam} and fine-grained segmentation masks () in \eqref{dec_output}, respectively. The proposed RS provides further refined CAMs () in \eqref{mrs} and pseudo masks () in \eqref{certainfilter} by making the combination result of PAMR and SCG fed back to CAMs. The encoder and decoder recursively update with them in  \eqref{recurseed_label},  \eqref{recurseed_seg}, and  \eqref{enc_rec_loss}. EPM synthesizes two images (), mixed CAMs (), and pseudo masks () refined by EP in \eqref{ep_alg} to improve WSSS performance further. Finally, the classifier and decoder produce the predictions (, ) from a mixed image and update with them in  \eqref{mix_seg} and  \eqref{mix_rec}.}
    }
    \label{fig:overview}


\end{figure*}

\subsection{Overview} 

The proposed method has parallel classification and segmentation branches. We propose two novel components relevant to our task: (i) RS and (ii) EPM. RS recursively updates the initial CAM with SCG and PAMR to discover an unseen foreground not detected in the previous iteration. {We then apply the CertainFilter (CF), commonly used in WSSS-IL, like regular training. EPM mixes two images and pseudo masks refined by EP, which disentangles foreground and background regions by using edge information in the per-pixel class probability domain, thereby leading to sample diversification and significantly improving performance for WSSS.} The overall framework is illustrated in Figure \ref{fig:overview}.


\begin{figure}[t]\centering \includegraphics[width=0.8\linewidth, height=0.7\linewidth]{Figure_RecurSeed_Improvement_v6.pdf}
\caption{
        Comparison of the quality of pseudo masks produced from CAM, simple post-processing on CAM (PAMR or SCG), and the proposed RecurSeed (RS). Unlike the simple post-processing schemes for CAM, the proposed RS recursively updates the CAM, thereby taking advantage of both the strengths of SCG (i.e., reducing FN) and PAMR (i.e., reducing FP).
    }
    \label{fig:rs}


\end{figure}

\subsection{RecurSeed}
\label{section:recurseed}
We observed that the complementary integration of the SCG and PAMR effectively moderated the FPN-WSSS-IL. 
In addition, the performance of the SCG depends highly on feature maps owing to its ability to update the CAM from SCG and PAMR recursively. In this section, we introduce RS  as follows: For the -th epoch, where ,
 
where  is the initial CAM,  denotes {its refined result through RS}, and { is set as the active region of }. 
In particular, we improved the result of  recursively by training the network such that the CAM updated at the next step  becomes the result , aiming at  for every epoch . As shown in Figure \ref{fig:rs}, the proposed RS gradually updates the initial CAM to remedy the shortcomings of the SCG and PAMR in \eqref{mrs}.

To achieve the objective of , we minimize the reconstruction loss in \eqref{recurseed_seg} {(i.e., in  decoder domain) and \eqref{enc_rec_loss} (i.e., in encoder domain)} along with the classification loss in \eqref{recurseed_label}, and the total net parameter  is updated at step  as follows:
\begin{small}

\end{small} 
where  denotes a mini-batch of size ,  denotes their truth class labels,  
\begin{small}

\end{small}
and 
\begin{small}

\end{small}
{where 0 and 255 represent the background class and the ignored class respectively.} As shown in Figure \ref{fig:overview}, our network consists of an encoder  and a decoder  with outputs of  in \eqref{enc_output} and  in \eqref{dec_output}, respectively. The refined CAM  is then obtained as  by adding layer , scaling the number of channels to the number of classes. We apply global average pooling (GAP) and sigmoid  to estimate the class labels as  in \eqref{label_est}. {Following the common practice, we employ the multi-label soft margin loss for the classification loss , the commonly used cross-entropy loss for segmentation loss , and the L1 loss for the reconstruction loss .}

Our network narrows the gap between the decoder output  (or CAM ) and RS  through \eqref{recurseed_seg} (or \eqref{enc_rec_loss}), leading to indirect (or direct) improvements in the CAM  in the next step as the latent feature of the decoder. For stable training, CF is applied to generate the pseudo label  in \eqref{certainfilter} and to avoid the influence of uncertain labels. Through this process, CF filters certain and uncertain regions of the RS, thereby enhancing the reliability of the learning.
        
\begin{figure}[t]\centering \includegraphics[width=0.75\linewidth, height=0.75\linewidth]{Figure_EdgePredictMix_v9.pdf}
\caption{
        (a) Illustration of EP. We create a superpixel separated from the edge between class probabilities (e.g., simply by using Canny and CCL) and newly define the superpixel-based label to correct the original label in the uncertain area. (b) The proposed EPM compares to the conventional mixing method (ClassMix). Our EP  further corrects for uncertain regions of pseudo masks by superpixel-based relabeling so that unlike ClassMix, our EPM synthesizes this refined image/mask, resulting in better WSSS performance.   
    }
    \label{fig:ep}


\end{figure}
        
\subsection{EdgePredictMix}
\label{section:certainmix} 

We proposed a novel DA method, EPM, to moderate the IBDA-WSSS-IL. EPM is composed of refining the mask and synthesizing refined masks. The first step of EPM is to perform the mask refinement by using absolute and relative per-pixel probability values. We name this mask refinement EP, whose detailed process is as follows: suppose that there is an uncertain region given through thresholding of absolute probabilities per pixel. {We utilize separate information on the relative difference between adjacent per-pixel probability values by deriving an edge from the mask and obtaining superpixels from the edge. We then remedy the uncertainty of low absolute probabilities by singling out the most dominant class within each superpixel, resulting in a boundary-aware mask. To extract the edge and superpixels}, we simply used conventional algorithms named Canny \cite{canny1986computational} and Connected-component labeling (CCL) \cite{rosenfeld1966sequential}, respectively. The second step of EPM is to blend two EP-refined masks (and their corresponding original images). EPM is illustrated in Figure \ref{fig:ep} and formulated in the following steps for two arbitrary indices  in a mini-batch. 
 

The first step of EPM is to supplement {the uncertain mask produced from the model} by EP so that the decoder output  in the current epoch  is converted to  as in \eqref{ep_alg}. Then, we extract the union of all EP-refined foregrounds for image  as  in \eqref{mask_fg}.  
\begin{small}

\end{small}
{where  denotes the element-wise indicator operator and { is set as the active region of }.} The second step is pasting certain regions   of image  and its corresponding EP-refined-mask set  (or its RS seed as encoder output ) onto image  and its EP-refined-mask set  (or its RS seed ), respectively, as shown in  in \eqref{image_ij} and  in \eqref{mask_ij} (or  in \eqref{dec_mask_ij}). In other words, , , and  correspond to the (EP-based) synthesizing results in the original image, decoder, and encoder, respectively. 
\begin{small}

\end{small} 
We then make the network train the mixed images and labels as follows:
\begin{small}

\end{small}
where  refers to all terms in  \eqref{recurseed_label}, \eqref{recurseed_seg}, and \eqref{enc_rec_loss} and 
\begin{small} 

\end{small}
Using the proposed EPM, we obtained another mini-batch  of  mixed images by selecting each sample in the mini-batch , choosing another sample at random from the remaining samples, and mixing them. 
{The pseudo masks and the decoder outputs} for  at epoch  are given by  in \eqref{m_mix} and  in \eqref{m_mix_pred}, respectively, through the additional optimizations specified as follows: 
The first loss term in \eqref{mix_seg} (or second loss term in \eqref{mix_rec}) corresponds to {the cross-entropy loss (or the reconstruction loss) that makes the decoder output  (or the encoder output ) reproduce the mixed mask refined by EP  (or the mixed RS ).}






\section{Experiments}
\label{section:exp}

We evaluated our method on the PASCAL VOC 2012 \cite{everingham2010pascal} and MS COCO 2014 datasets \cite{lin2014microsoft}. Similar to \citet{ahn2018learning, ahn2019weakly, wang2020self, lee2021anti, 9506058}, we used the mIoU to characterize our experiments. Implementation details are given in Appendix.

\begin{table}[t]
    \centering
    \caption{ 
    Performance comparison of WSSS methods in terms of mIoU () on PASCAL VOC 2012 and COCO 2014. {* and  indicate the backbone of VGG-16 and ResNet-50, respectively.} Sup., supervision; , image-level class labels; , the saliency supervision; {, the external dataset.}
  }
  \begin{scriptsize}
\begin{tabular}{p{0.25\textwidth} c c c c c}
    \toprule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Method\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Backbone\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sup.\end{tabular}} & \multicolumn{2}{c}{VOC}  & COCO \\
           &          &      & \emph{val} & \emph{test} & \emph{val} \\
    \hline \hline
    Single stage: \\
    EM {\tiny ICCV'15} \cite{papandreou2015weakly} & VGG16 &  & 38.2 & 39.6 & - \\
RRM {\tiny AAAI'20} \cite{zhang2020reliability} & WR38 &  & 62.6 & 62.9 & - \\
    SSSS {\tiny CVPR'20} \cite{araslanov2020single} & WR38 &  & 62.7 & 64.3 & - \\
    AFA {\tiny CVPR'22} \cite{ru2022learning} & MiT-B1 &  & 66.0 & 66.3 & 38.9 \\
Ours (single-stage, RS) & R50 &  & 66.5 & 67.9 & 40.0 \\ 
Ours (single-stage, RS+EPM) & R50 &  & \textbf{69.5} & \textbf{70.6} & \textbf{42.2} \\
    \hline 
Multiple stages: \\
    DSRG {\tiny CVPR'18} \cite{huang2018weakly} & R101 & + & 61.4 & 63.2 & 26.0* \\
    FickleNet {\tiny CVPR'19} \cite{lee2019ficklenet} & R101 & + & 64.9 & 65.3 & - \\
NSRM {\tiny CVPR'21} \cite{yao2021non} & R101 & + & 68.3 & 68.5 & - \\
    AuxSegNet {\tiny ICCV'21} \cite{xu2021leveraging} & WR38 & + & 69.0 & 68.6 & 33.9 \\
    EDAM {\tiny CVPR'21} \cite{wu2021embedded} & R101 & + & 70.9 & 70.6 & - \\
    DRS {\tiny AAAI'21} \cite{kim2021discriminative} & R101 & + & 71.2 & 71.4 & - \\
    CLIMS {\tiny CVPR'22} \cite{xie2022clims} & R50 & + & 69.3 & 68.7 & - \\
    W-OoD {\tiny CVPR'22} \cite{lee2022weakly} & R101 & + & 70.7 & 70.1 & - \\
    EPS {\tiny CVPR'21} \cite{lee2021railroad} & R101 & + & 70.9 & 70.8 & 35.7* \\
    L2G {\tiny CVPR'22} \cite{jiang2022l2g} & R101 & + & 72.1 & 71.7 & 44.2 \\
    RCA {\tiny CVPR'22} \cite{zhou2022regional} & R101 & + & 72.2 & 72.8 & 36.8* \\
    PPC {\tiny CVPR'22} \cite{du2022weakly} & R101 & + & 72.6 & \textbf{73.6} & - \\
    \hline
    PSA {\tiny CVPR'18} \cite{ahn2018learning} & WR38 &  & 61.7 & 63.7 & - \\
    IRNet {\tiny CVPR'19} \cite{ahn2019weakly} & R50 &  & 63.5 & 64.8 & - \\
SEAM {\tiny CVPR'20} \cite{wang2020self} & WR38 &  & 64.5 & 65.7 & 31.9 \\
AdvCAM {\tiny CVPR'21} \cite{lee2021anti} & R101 &  & 68.1 & 68.0 & - \\
    CSE {\tiny ICCV'21} \cite{kweon2021unlocking} & WR38 &  & 68.4 & 68.2 & 36.4 \\
    CPN {\tiny ICCV'21} \cite{zhang2021complementary} & WR38 &  & 67.8 & 68.5 & - \\
    RIB {\tiny NIPS'21} \cite{lee2021reducing} & R101 &  & 68.3 & 68.6 & 43.8 \\
    ReCAM {\tiny CVPR'22} \cite{chen2022class} & R101 &  & 68.5 & 68.4 & - \\
    ADEHE {\tiny CVPR'22} \cite{liu2022adaptive} & R101 &  & 68.6 & 68.9 & - \\
AMR {\tiny AAAI'22} \cite{qin2021activation} & R101 &  & 68.8 & 69.1 & - \\
    URN {\tiny AAAI'22} \cite{li2021uncertainty} & R101 &  & 69.5 & 69.7 & 40.7 \\ 
    SIPE {\tiny CVPR'22} \cite{chen2022self} & R101 &  & 68.8 & 69.7 & 40.6 \\
    AMN {\tiny CVPR'22} \cite{lee2022threshold} & R101 &  & 69.5 & 69.6 & 44.7 \\
    MCTformer {\tiny CVPR'22} \cite{xu2022multi} & WR38 &  & 71.9 & 71.6 & 42.0 \\
    SANCE {\tiny CVPR'22} \cite{li2022towards} & R101 &  & 70.9 & 72.2 & 44.7 \\
Ours (multi-stage, RS) & R101 &  & 72.8 & 72.8 & 45.8 \\ 
Ours (multi-stage, RS+EPM) & R101 &  & \textbf{74.4} & \textbf{73.6} & \textbf{46.4} \\
    \bottomrule
  \end{tabular} 
  \label{tab:comp_voc_coco}
  \end{scriptsize}
\end{table}

\subsection{Comparison with state-of-the-art approaches}
\label{ssec:results}

When comparing SLFs or MLFs using the same supervision, the proposed method outperformed the other methods even if only RS was used, regardless of PASCAL VOC 2012 or MS COCO 2014. Furthermore, if additionally considering the EPM, the performance is consistently further enhanced than the proposed one using only the RS, thereby proving the effectiveness of both RS and EPM.

Specifically, our single- and multi-stage (e.g., using RW) methods achieved mIoUs of  and  on the PASCAL VOC 2012 \emph{val} set, improving mIoUs by  and  compared to the latest studies, respectively.
In particular, our single-stage method (test mIoU ) performs similar to or better than recent single- or multi-stage methods  using the same supervision, even though we use a backbone with fewer parameters (i.e., R50). This indicates for the first time through the proposed method that the latest performance can be achieved only with a single-stage (without additional stages such as RW).

In contrast with PASCAL VOC 2012, MS COCO 2014 is more challenging because it has four times categories and requires to detection of non-salient foreground objects.  
As shown in the last column of Table \ref{tab:comp_voc_coco}, even for MS COCO 2014, our single- and multi-stage methods achieved new state-of-the-art mIoUs of  and , improving mIoUs by  and  compared to the latest studies, respectively.

\subsection{Analysis}
\label{section:disc}

In this section, we empirically study the proposed RS and EPM. We also refer to Appendix for additional studies. 

\subsubsection{Novelty of RS.} 
Existing SCG and PAMR studies, which use a fully trained network, were individually developed for the post-processing method on the CAM. As shown in Table \ref{tab:rs}, the simple integration of SCG and PAMR (i.e., w. SCG and PAMR but w.o. RS) shows a marginal improvement, but it fails to reduce FP and FN simultaneously. By contrast, we observed a decrease in both FP and FN when applying our RS. This indicates that RS makes both SCG and PAMR being utilized uniquely as \textit{recursively} together to mitigate FPN-WSSS-IL successfully. Therefore, the proposed RS achieved higher performance than the simple integration, verifying the validity and novelty of the proposed recursive update in RS.

\begin{table}
    \centering
  \caption{ 
    Element-wise component analysis on PASCAL VOC 2012  set. With RS, SCG and SCG+PAMR denote  in \eqref{mrs} by removing and preserving operator  throughout the training, respectively. RS indicates  ( ) or  (\checkmark).  denotes the decoder map result.
  }
  \begin{scriptsize}
  \begin{tabular}{p{0.005\textwidth} p{0.02\textwidth} p{0.05\textwidth} | p{0.04\textwidth} p{0.115\textwidth} p{0.115\textwidth}}
    \toprule
    RS      & SCG     & PAMR    & mIoU & FP & FN \\
    \hline \hline


         &  \checkmark &         & 58.0 & 0.268 & 0.165 \\
         &  \checkmark &  \checkmark & 59.3 & 0.225 (\textcolor{blue}{ 0.043}) & 0.194 (\textcolor{red}{ 0.029})\\
    \hline 
     \checkmark &  \checkmark &       & 65.2 & 0.216 & 0.143 \\
     \checkmark &  \checkmark &  \checkmark & 65.9 & 0.210 (\textcolor{blue}{ 0.006}) & 0.141 (\textcolor{blue}{ 0.002})\\
    \hline 
     \checkmark &  \checkmark &      & *67.4 & *0.196 & *0.141 \\
     \checkmark &  \checkmark &  \checkmark & *70.7 & *0.171 (\textcolor{blue}{ 0.025}) & *0.134 (\textcolor{blue}{ 0.007}) \\
    
    \bottomrule
  \end{tabular}
  \label{tab:rs}
    \end{scriptsize}
\end{table}

\subsubsection{Novelty of EPM.} 

The previous DA methods rely on unsuitable seeds (e.g., CAM) or ideal annotations (e.g., pixel-wise annotations). Nevertheless, {we applied the proposed RS to pre-process our EPM and other DA methods for a fair comparison.} In Table \ref{tab:mix}, CutMix \cite{yun2019cutmix}, SaliencyGrafting \cite{park2021saliency}, and CDA \cite{su2021context} improved  with mixed class labels but decreased approximately 2\% of mIoU. ClassMix \cite{olsson2021classmix} showed a marginal improvement despite the WSSS setting. However, mixing predicted masks without the mask refinement leads to an insufficient improvement. Our EPM not only trains mixed masks (like ClassMix) but also further refines pseudo masks by leveraging the reconstituted edge from uncertain regions, thereby resulting in the highest WSSS performance (mIoU ) compared to other DA techniques (the best mIoU ). {For simplicity, we did not use the mixed class loss but observed that the  performance was similar to the existing DA methods, proving its justification.}

\begin{table}
\caption{
    Performance comparison with the proposed EPM and existing mixing methods in terms of  (\%) and mIoU (\%) on PASCAL VOC 2012  set. * denotes our implementation for a fair comparison.
  } 
  \centering
  \begin{scriptsize} 
  \begin{tabular}{p{0.27\textwidth} c c c}
    \toprule
    Method & Backbone &  & mIoU \\
    \hline \hline
RecurSeed & R50 & 94.7 & 70.7 \\
    RecurSeed + *CutMix \cite{yun2019cutmix} & R50 & 95.6 & 68.5 \\
    RecurSeed + *SaliencyGrafting \cite{park2021saliency} & R50 & \textbf{96.8} & 68.6 \\
    RecurSeed + *CDA \cite{su2021context} & R50 & 96.0 & 69.0 \\
    RecurSeed + *ClassMix \cite{olsson2021classmix} & R50 & 94.6 & 71.2 \\
RecurSeed + EdgePredictMix & R50 & 95.2 & \textbf{75.2} \\
    \bottomrule
  \end{tabular}
  \label{tab:mix}
  \end{scriptsize}
    \vspace{-3mm}

\end{table}
 
\subsubsection{Importance of the proposed components.}  
To clarify the effect of the proposed RS and EPM, we conducted ablation studies related to loss functions and EP on our single-stage network. In Table \ref{tab:loss}, we report different combinations as we add or remove components. First, as shown in row 3, we observe that training both the encoder and decoder with our RS significantly improves the decoder's performance and brings an explicit gain for the CAM as the recursively advanced features lead to better pseudo masks in the next step.    
Second, in rows 5 and 7, training mixed results without EP shows a certain level of improvements and outperforms ClassMix \cite{olsson2021classmix} as the model only trained reliable labels from CF. {We present samples of qualitative segmentation results on PASCAL VOC 2012 and MS COCO 2014 sets in Appendix.} {We achieved the best performance (the last row) when applying essential components together with EP by exploiting absolute and relative per-pixel probability information.} {In addition, RS with EP (row 4) achieved 71.2\% of Decoder. The result shows that EP itself achieved meaningful improvement even without mixing augmentation.} Therefore, we demonstrate the novelty and validity of the proposed components (i.e., RS and EPM).

\begin{table}
  \centering
  \caption{ 
    Effect of key components in terms of mIoU (\%) on PASCAL VOC 2012  set.  
  }
  \begin{scriptsize}
  \begin{tabular}{p{0.001\textwidth} c c c | c c c | c c}
    \toprule
    &   &   &   &  &  & EP         & CAM  & Decoder \\
    \hline \hline
    1 &  \checkmark &       &       &            &            &       & 46.9 & 17.4    \\
2 &  \checkmark &  \checkmark &       &            &            &       & 51.6 & 65.8    \\
    3 &  \checkmark &  \checkmark &  \checkmark &            &            &       & 57.9 & 70.7    \\
    4 &  \checkmark &  \checkmark &  \checkmark &            &            & \checkmark & 59.0 & 71.7    \\
    \hline
5 &  \checkmark &  \checkmark &  \checkmark &  \checkmark      &            &       & 61.0 & 73.5    \\
    6 &  \checkmark &  \checkmark &  \checkmark &  \checkmark      &            &  \checkmark & 60.8 & 74.5    \\
    7 &  \checkmark &  \checkmark &  \checkmark &  \checkmark      &  \checkmark      &       & 61.9 & 73.6    \\
    8 &  \checkmark &  \checkmark &  \checkmark &  \checkmark      &  \checkmark      &  \checkmark & \textbf{63.4} & \textbf{75.2}    \\
    \bottomrule
  \end{tabular}
  \label{tab:loss}
    \end{scriptsize}
\end{table}

\paragraph{Additional comparison with existing MLFs using RW.}   
{In Table \ref{tab:rw}, we compared the performance of the proposed SLF with existing MLFs \cite{wang2020self, ahn2019weakly, lee2021reducing, du2022weakly, lee2022threshold} by extending it to MLF with the same configuration of RW.}
Surprisingly, in all cases before or after the application of RW, our method outperforms the related works by achieving mIoUs of 75.2\% and 76.7\% without and with RW, respectively, exhibiting the performance improvement of at {least 13\% and 6\% compared to previous state-of-the-art methods (PPC \cite{du2022weakly}: 61.5\% and RIB \cite{lee2021reducing}: 70.6\%)}. This also proves the superiority of the proposed RS and EPM.  

\begin{table}
\caption{
    mIoUs () of single-stage results produced from a trained network (Seed) and pseudo masks generated by RW on PASCAL VOC 2012  set. }
\centering
  \begin{scriptsize} 
  \begin{tabular}{p{0.25\textwidth} c c c}
    \toprule
    Method & Backbone & Seed & RW \\
    \hline \hline
    SEAM \cite{wang2020self} & WR38 & 55.4 & 63.6 \\
    IRNet \cite{ahn2019weakly} & R50 & 48.8 & 66.3 \\
    CSE \cite{kweon2021unlocking} & WR38 & 56.0 & 66.9 \\
    CDA \cite{su2021context} & R50 & 50.8 & 67.7 \\
    CPN \cite{zhang2021complementary} & WR38 & 57.4 & 67.8 \\
    CONTA \cite{zhang2020causal} & R50 & 48.8 & 67.9 \\
    AMR \cite{qin2021activation} & R50 & 56.8 & 69.7 \\
    AdvCAM \cite{lee2021anti} & R50 & 55.6 & 69.9 \\
    PPC \cite{du2022weakly} & WR38 & 61.5 & 70.1 \\
    RIB \cite{lee2021reducing} & R50 & 56.5 & 70.6 \\ 
Ours (RS) & R50 & 70.7 & 74.8 \\
Ours (RS+EPM) & R50 & \textbf{75.2} & \textbf{76.7} \\
    \bottomrule
  \end{tabular}
  \label{tab:rw}
  \end{scriptsize}
  \vspace{-3mm}
\end{table}

\section{Conclusion}
\label{section:conc}
{In this study, we proposed RS and EPM to improve the WSSS-IL performance, achieving new state-of-the-art performances on the PASCAL VOC 2012 and MS COCO 2014 benchmarks. Furthermore, because RS and EPM are learning methods generally applicable to SLFs consisting of an arbitrary encoder and decoder, one can upgrade this backbone to achieve a higher SLF performance. We also expect higher MLF performance can be derived by performing multi-stage extension from the proposed SLF to the latest techniques other than RW or applying our RS and EPM to the individual learning of any recent MLF.  
From this perspective, we expect that our single- and multi-stage methods have high utility and scalability in weakly or semi-supervised tasks.}
 
 
\clearpage 
\bibliographystyle{unsrtnat}
\bibliography{main.bib}
\clearpage 
	
	
	 
\appendix
	 
 

\section{Reproducibility}

\paragraph{Details for datasets.} 
PASCAL VOC 2012 has 21 classes (including a background class) and three subsets (i.e., training (\emph{train}), validation (\emph{val}), and test with 1,464, 1,449, and 1,456 images, respectively). Following common practice, \emph{e.g}., \citet{ahn2018learning, wang2020self, araslanov2020single, zhang2020reliability, lee2021anti, zhang2021complementary}, we used additional annotations from the Semantic Boundary Dataset \cite{hariharan2011semantic} to build an augmented dataset with 10,582 images for training. MS COCO 2014 contains 81 classes, including a background class. This dataset contains 82,783 training images and 40,504 validation images. Although the two datasets contain labels for object detection and semantic segmentation, we only used image-level class labels for the WSSS. The results of the PASCAL VOC 2012 \emph{val} and \emph{test} sets were obtained from the official PASCAL VOC online evaluation server. 
 

\paragraph{Implementation details.} 


To train and evaluate the PASCAL VOC 2012 set, we extended our SLF model to an MLF by applying an RW \cite{ahn2019weakly} to generate pseudo masks from the SLF results and build the MLF by applying ResNet101 with DeepLabv3+ \cite{chen2018encoder}. In the case of MS COCO 2014, we did not apply an RW because our SLF results sufficiently cover foregrounds on MS COCO 2014, even without using an RW. 

We also applied CF with  and . To train an SLF, we employ ResNet50 \cite{he2016deep} and add DeepLabv3+ as in \citet{araslanov2020single}. For testing, we adopted a multi-scale strategy, and CRF \cite{krahenbuhl2011efficient} with the hyperparameters suggested in \citet{chen2017deeplab}. We used all hyperparameters when training the MS COCO 2014 dataset, the same as the PASCAL VOC 2012 dataset. For training, we used a stochastic gradient descent (SGD) optimizer with weight decay . The initial learning rate was set to 0.1, and it decayed polynomially at a rate of 0.9. The training images were augmented by random scaling, horizontal flipping, color jittering, and random cropping to a pixel resolution of . We used a single RTX A6000 GPU to conduct all experiments and implemented our method in PyTorch. In addition, our method takes less than 24 hours for training the PASCAL VOC 2012. {The proposed modules are only used for trianing. Thus, ours has the same runtime as other SLF methods \cite{araslanov2020single} in evaluation.}

\section{Additional analysis}
\label{section:analysis}

\subsection{Effect of joint consideration for RecurSeed and EdgePredictMix}
\label{ssec:effect}

\begin{table}[t]
  \caption{ 
    mIoUs () of CAM, SCG, and the prediction from the segmentation branch (Decoder) on PASCAL VOC 2012 and MS COCO 2014 \emph{train} images. RS, RecurSeed; EPM, EdgePredictMix.  
  }
  \centering
  \begin{scriptsize} 
  \begin{tabular}{c | c c | c c c}
    \toprule
    Dataset & RS & EPM & CAM  & SCG  & Decoder \\
    \hline \hline
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}VOC\end{tabular}} &   &   & 47.4 & 58.0 & 17.4 \\
    & \checkmark &       & 57.9 & 65.9 & 70.7 \\
    & \checkmark & \checkmark & \textbf{63.4} & \textbf{69.0} & \textbf{75.2} \\
    \hline 
    \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}COCO\end{tabular}} &   &   & 32.1 & 37.8 & 10.6 \\
    & \checkmark &       & 40.0 & 41.5 & 47.2 \\
    & \checkmark & \checkmark & \textbf{42.3} & \textbf{43.2} & \textbf{50.3} \\
    \bottomrule
  \end{tabular}
  \label{tab:ablation}
  \end{scriptsize}
\end{table}

\paragraph{Quantitative results.}
To show the effects of RS and EPM, we conducted ablation studies with and without RS and EPM individually on our single-stage network. Without RS and EPM, training is conducted only by minimizing the classification loss . 
Table \ref{tab:ablation} compares the network's CAM, SCG, and decoder results for up to 30 epochs. For the PASCAL VOC 2012 and MS COCO 2014 datasets, we observed an additional improvement in the mIoU of CAM and SCG results by approximately 3\%--5\% with each sequential application of RS and EPM, thereby demonstrating the individual necessity and effectiveness of RS and EPM. In addition, we verified the validity of the proposed decoder by showing that its results were consistently better than those of CAM and SCG by more than approximately 5\%.

\paragraph{Hyperparameter analysis.}
To validate the effects of the proposed RS and EPM, we evaluated our single-stage results for each epoch, as shown in Figure \ref{fig:training}(a). Initially, the mIoU increased steeply through the RS between the 5th and 10th epochs. However, the mIoU then saturated between the 10th and 15th epochs. To address this saturation, we applied EPM after the 15th epoch. Here,  and  in \eqref{certainfilter} control the foreground and background regions. Although we observed a sufficiently high mIoU above a certain level irrespective of changes of  and  in Figures \ref{fig:training}(b)-(c), the best performance can be found through their adjustment, proving the validity of two thresholds.



\begin{figure}[t]\centering 
    \includegraphics[width=1.00\linewidth]{Figure_Hyperparameters_v8.pdf}
    \caption{
        (a) Effects of RS and EPM per epoch.
        (b) Effect of foreground threshold .
        (c) Effect of background threshold .
    }
    \label{fig:training}
\end{figure}

\begin{figure}[t]\centering \includegraphics[width=0.8\linewidth]{Figure_RecurSeed_Improvement_For_supple_v1.pdf}
    \caption{
        Visualization of attention maps and pseudo masks with recursive improvements.
    }
    \label{fig:impro}
\end{figure}


\paragraph{Qualitative results.}

Figure \ref{fig:impro} illustrates the results produced by our single-stage method (including CAM, RS, and Decoder). We then generated pseudo masks from the decoder output by using CF. For the initial training step shown in Figure \ref{fig:impro}(a), CAM and SCG cover the most discriminative part of an object. In addition, the decoder output from the segmentation branch fails to detect all of the foregrounds owing to insufficient training steps. After we apply RS to train the segmentation branch (Figures \ref{fig:impro}(b)-(c)), the predicted masks were better than those at the initial step (). As the learning steps progress through EPM (Figure \ref{fig:impro}(d)), the CAM and SCG represent more integral regions of an object, and the segmentation branch produces an accurate seed. Specifically, we secured the data diversity and increased the mIoU by more than 5\%. The qualities of pseudo masks (Decoder+CF) are improved progressively in Figures \ref{fig:impro}(a)-(d), and the final mask in Figure \ref{fig:impro}(d) is then close to the ground-truth labels.

\subsection{Effect of RecurSeed}
\label{ssec:effect_of_high_order}
\paragraph{Usage of high-order feature correlation in SCG.}
In Table \ref{tab:scg}, we show the SCG results obtained by varying the layer combination in the HSC. For each combination, we displayed the mIoU of both cases, i.e., the first case in which the SCG result is generated from the raw CAM (i.e., ) and the second case in which the SCG result is updated by the proposed RS (i.e., ). In both cases, we observed that combining more layers in HSC sequentially improves the mIoU (i.e., from 49.7\% to 58.0\% for , and 67.5\% to 69.0\% for ). From this observation, we used all layers from layer1 to layer5 for training the RS. In particular, the proposed RS and EPM consistently improve the performance under all combinations of layers (e.g., the performance improves by 11\%, from 58.0\% to 69.0\%, when combining all layers), validating the usefulness of our method.

\begin{table}
  \centering
  \caption{ 
    {Ablation study for each combination of the SCG. Our method boosts the overall performance of SCG applied in various layers. \checkmark indicates that SCG is applied.}
  }
  \begin{scriptsize} 
  \begin{tabular}{p{0.025\textwidth} p{0.025\textwidth} p{0.025\textwidth} p{0.025\textwidth} p{0.03\textwidth} | c c}
    \toprule
layer5     & layer4     & layer3     & layer2     & layer1     & =1 (w.o. RS and EPM) & =30 (w. RS and EPM) \\
    \hline \hline
    \checkmark &       &       &       &       & 49.7 & \textbf{67.5} \\
    \checkmark & \checkmark &       &       &       & 57.0 & \textbf{68.4} \\
    \checkmark & \checkmark & \checkmark &       &       & 57.7 & \textbf{68.8} \\
    \checkmark & \checkmark & \checkmark & \checkmark &       & 57.9 & \textbf{69.0} \\
    \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 58.0 & \textbf{69.0} \\ 
    \bottomrule
  \end{tabular}
  \end{scriptsize}
  \label{tab:scg} 
\end{table}

In Table \ref{tab:scg2}, we add both FN and FP results into Table \ref{tab:scg}. Note that the closer the network layer is to the input, the more discriminative feature maps are extracted. Therefore, using high-order features (i.e., adding low layer maps L4-L1) reduces FP as it focuses on the target's discriminative area; however, it has the side effect of increasing FN in general. If RS is not used, this trend appears as shown in Table \ref{tab:scg}, but if RS is used, FP decreased () without increasing FN (). As RS repeatedly gives diverse discriminative information to the network, the network can ideally accumulate all this information, thus compensating for the weakness of increasing FN. As such, RS does not increase FN but does not play a key role in reducing FN. However, it can be observed in Table \ref{tab:scg2} that FN is also significantly reduced () when EPM is added to RS.  
The reason is that EPM can effectively reduce FN by letting the network better recognize a detailed region of each object. As a result, both FN and FP achieved the lowest values of 0.133 and 0.187, respectively, when RS and EPM were considered together.  

\begin{table}
  \centering
  \caption{ 
    {Extended results of Table \ref{tab:scg} with mIoU, FP, and FN: w/o RS (), w/ RS (), L5 (layer5), and L4-L1 (layers 4+3+2+1).}
  } 
  \begin{scriptsize} 
     \begin{tabular}{c c | c c | c c c}
        \toprule
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}RS\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}EPM\end{tabular}} & Low       & High        & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}mIoU\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}FP\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}FN\end{tabular}} \\
          & & L5        & L4-L1       &      &    &    \\
        \hline \hline
          &   & \checkmark &    & 49.7 & 0.358 & 0.159 \\
          &   & \checkmark & \checkmark & 58.0 & 0.268 (\textcolor{blue}{ 0.090}) & 0.165 (\textcolor{red}{ 0.006}) \\
        \hline
        \checkmark &   & \checkmark &    & 61.9 & 0.232 & 0.161 \\
        \checkmark &   & \checkmark & \checkmark & 65.9 & 0.210 (\textcolor{blue}{ 0.022}) & 0.141 (\textcolor{blue}{ 0.020}) \\
        \hline
        \checkmark & \checkmark & \checkmark &    & 67.5 & 0.188 & 0.148 \\
        \checkmark & \checkmark & \checkmark & \checkmark & \textbf{69.0} & \textbf{0.187} (\textcolor{blue}{ 0.001}) & \textbf{0.133} (\textcolor{blue}{ 0.015}) \\
        \bottomrule
      \end{tabular}
      \label{tab:scg2}
    \end{scriptsize}
\end{table}






\section{Additional results}

\paragraph{Quantitative results.} We show the per-class segmentation results from the both PASCAL VOC 2012 and MS COCO 2014 datasets in Tables \ref{tab:voc_val_detail}, \ref{tab:voc_test_detail}, and \ref{tab:coco_val_detail}. These quantitative results indicate that our method outperforms existing state-of-the-art methods in most class categories. 

\paragraph{Qualitative results.}  We present additional examples of qualitative segmentation results produced by our method on both the PASCAL VOC 2012 and MS COCO 2014  sets in Figures \ref{fig:additional_voc_results} and \ref{fig:additional_coco_results}, respectively. These results show that our method not only performs well for different complex scenes, small objects, or multiple instances but also can achieve a satisfactory segmentation performance for various challenging scenes. We also visualize more examples of attention maps for each step () on the PASCAL VOC 2012 and MS COCO 2014 \emph{train} sets, as shown in Figures \ref{fig:additional_voc_att_results} and \ref{fig:additional_coco_att_results}, respectively. The raw CAMs without RS (i.e., ) only focus on the local discriminative parts for large-scale objects, such as the head and hands of people or wheels of the vehicles. However, when applying the proposed RS (i.e., ), our single-stage results cover more object regions, including those less discriminative regions for large-scale objects, and also capture the exact boundaries of small-scale objects (i.e., decreasing the FP). Furthermore, with more training steps, including the proposed EPM (i.e., ), our final results produce more accurate boundaries. This demonstrates the sequential improvements through the proposed RS and EPM.

\clearpage




\begin{table}[t]
  \centering
  \caption{
    Per-class performance comparisons with WSSS methods in terms of IoUs (\%) on the PASCAL VOC 2012 \emph{val} set.
  }
  \hspace*{-4em}
  \begin{scriptsize}
\begin{tabular}{
    p{0.20\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    c }
    \toprule
    Method & \rotatebox[origin=l]{90}{bkg} & \rotatebox[origin=l]{90}{aero} & \rotatebox[origin=l]{90}{bike} & \rotatebox[origin=l]{90}{bird} & \rotatebox[origin=l]{90}{boat} & \rotatebox[origin=l]{90}{bottle} & \rotatebox[origin=l]{90}{bus} & \rotatebox[origin=l]{90}{car} & \rotatebox[origin=l]{90}{cat} & \rotatebox[origin=l]{90}{chair} & \rotatebox[origin=l]{90}{cow} & \rotatebox[origin=l]{90}{table} & \rotatebox[origin=l]{90}{dog} & \rotatebox[origin=l]{90}{horse} & \rotatebox[origin=l]{90}{mbk} & \rotatebox[origin=l]{90}{person} & \rotatebox[origin=l]{90}{plant} & \rotatebox[origin=l]{90}{sheep} & \rotatebox[origin=l]{90}{sofa} & \rotatebox[origin=l]{90}{train} & \rotatebox[origin=l]{90}{tv}  & mIoU  \\
    \hline \hline
    EM-Adapt & 67.2 & 29.2 & 17.6 & 28.6 & 22.2 & 29.6 & 47.0 & 44.0 & 44.2 & 14.6 & 35.1 & 24.9 & 41.0 & 34.8 & 41.6 & 32.1 & 24.8 & 37.4 & 24.0 & 38.1 & 31.6 & 33.8 \\
    MIL-LSE & 79.6 & 50.2 & 21.6 & 40.9 & 34.9 & 40.5 & 45.9 & 51.5 & 60.6 & 12.6 & 51.2 & 11.6 & 56.8 & 52.9 & 44.8 & 42.7 & 31.2 & 55.4 & 21.5 & 38.8 & 36.9 & 42.0 \\
    SEC & 82.4 & 62.9 & 26.4 & 61.6 & 27.6 & 38.1 & 66.6 & 62.7 & 75.2 & 22.1 & 53.5 & 28.3 & 65.8 & 57.8 & 62.3 & 52.5 & 32.5 & 62.6 & 32.1 & 45.4 & 45.3 & 50.7 \\
    TransferNet & 85.3 & 68.5 & 26.4 & 69.8 & 36.7 & 49.1 & 68.4 & 55.8 & 77.3 & 6.2 & 75.2 & 14.3 & 69.8 & 71.5 & 61.1 & 31.9 & 25.5 & 74.6 & 33.8 & 49.6 & 43.7 & 52.1 \\
    CRF-RNN & 85.8 & 65.2 & 29.4 & 63.8 & 31.2 & 37.2 & 69.6 & 64.3 & 76.2 & 21.4 & 56.3 & 29.8 & 68.2 & 60.6 & 66.2 & 55.8 & 30.8 & 66.1 & 34.9 & 48.8 & 47.1 & 52.8 \\
    WebCrawl & 87.0 & 69.3 & 32.2 & 70.2 & 31.2 & 58.4 & 73.6 & 68.5 & 76.5 & 26.8 & 63.8 & 29.1 & 73.5 & 69.5 & 66.5 & 70.4 & 46.8 & 72.1 & 27.3 & 57.4 & 50.2 & 58.1 \\
    DSRG & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 61.4 \\
    PSA & 87.6 & 76.7 & 33.9 & 74.5 & 58.5 & 61.7 & 75.9 & 72.9 & 78.6 & 18.8 & 70.8 & 14.1 & 68.7 & 69.6 & 69.5 & 71.3 & 41.5 & 66.5 & 16.4 & 70.2 & 48.7 & 59.4 \\
    FickleNet & 89.5 & 76.6 & 32.6 & 74.6 & 51.5 & 71.1 & 83.4 & 74.4 & 83.6 & 24.1 & 73.4 & 47.4 & 78.2 & 74.0 & 68.8 & 73.2 & 47.8 & 79.9 & 37.0 & 57.3 & \textbf{64.6} & 64.9 \\
    SSDD & 89.0 & 62.5 & 28.9 & 83.7 & 52.9 & 59.5 & 77.6 & 73.7 & 87.0 & 34.0 & 83.7 & 47.6 & 84.1 & 77.0 & 73.9 & 69.6 & 29.8 & 84.0 & 43.2 & 68.0 & 53.4 & 64.9 \\
    MCIS & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 66.2 \\
    RRM & 87.9 & 75.9 & 31.7 & 78.3 & 54.6 & 62.2 & 80.5 & 73.7 & 71.2 & 30.5 & 67.4 & 40.9 & 71.8 & 66.2 & 70.3 & 72.6 & 49.0 & 70.7 & 38.4 & 62.7 & 58.4 & 62.6 \\
    SSSS & 88.7 & 70.4 & 35.1 & 75.7 & 51.9 & 65.8 & 71.9 & 64.2 & 81.1 & 30.8 & 73.3 & 28.1 & 81.6 & 69.1 & 62.6 & 74.8 & 48.6 & 71.0 & 40.1 & 68.5 & 64.3 & 62.7 \\
    CONTA & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 66.1 \\
    SEAM & 88.8 & 68.5 & 33.3 & 85.7 & 40.4 & 67.3 & 78.9 & 76.3 & 81.9 & 29.1 & 75.5 & 48.1 & 79.9 & 73.8 & 71.4 & 75.2 & 48.9 & 79.8 & 40.9 & 58.2 & 53.0 & 64.5 \\
    CIAN & 88.2 & 79.5 & 32.6 & 75.7 & 56.8 & 72.1 & 85.3 & 72.9 & 81.7 & 27.6 & 73.3 & 39.8 & 76.4 & 77.0 & 74.9 & 66.8 & 46.6 & 81.0 & 29.1 & 60.4 & 53.3 & 64.3 \\
    NSRM & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 68.3 \\
    EDAM & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 70.9 \\
    AdvCAM & 90.0 & 79.8 & 34.1 & 82.6 & 63.3 & 70.5 & 89.4 & 76.0 & 87.3 & 31.4 & 81.3 & 33.1 & 82.5 & 80.8 & 74.0 & 72.9 & 50.3 & 82.3 & 42.2 & \textbf{74.1} & 52.9 & 68.1 \\
    CSE & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 68.4 \\
    DRS & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 70.4 \\
    CPN & 89.9 & 75.0 & 32.9 & 87.8 & 60.9 & 69.4 & 87.7 & 79.4 & 88.9 & 28.0 & 80.9 & 34.8 & 83.4 & 79.6 & 74.6 & 66.9 & 56.4 & 82.6 & 44.9 & 73.1 & 45.7 & 67.8 \\
    RIB & 90.3 & 76.2 & 33.7 & 82.5 & \textbf{64.9} & 73.1 & 88.4 & 78.6 & 88.7 & 32.3 & 80.1 & 37.5 & 83.6 & 79.7 & 75.8 & 71.8 & 47.5 & 84.3 & 44.6 & 65.9 & 54.9 & 68.3 \\
    \hline
    Ours (single-stage, RS)& 89.7 & 80.0 & 36.1 & 87.7 & 40.1 & 65.2 & 82.6 & 75.3 & 88.6 & 30.1 & 74.4 & 48.9 & 82.9 & 79.0 & 75.0 & 80.9 & 46.0 & 75.7 & 47.1 & 52.1 & 58.3 & 66.5 \\
    Ours (single-stage, RS+EPM)& 91.3 & 85.7 & \textbf{40.0} & \textbf{88.1} & 52.7 & 67.3 & 85.9 & 80.1 & 89.2 & 32.4 & 78.5 & 48.7 & 83.9 & 81.2 & 77.3 & \textbf{84.7} & 52.2 & 83.3 & 46.7 & 51.2 & 59.4 & 69.5 \\
    Ours (multi-stage, RS)& 91.7 & 85.0 & 32.5 & 87.5 & 48.3 & 79.4 & 91.7 & 83.0 & 92.5 & 38.2 & 88.2 & \textbf{60.8} & 89.7 & 86.2 & 79.9 & 83.3 & 56.0 & 85.3 & \textbf{57.2} & 55.5 & 56.3 & 72.8 \\
    Ours (multi-stage, RS+EPM)& \textbf{92.2} & \textbf{88.4} & 35.4 & 87.9 & 63.8 & \textbf{79.5} & \textbf{93.0} & \textbf{84.5} & \textbf{92.7} & \textbf{39.0} & \textbf{90.5} & 54.5 & \textbf{90.6} & \textbf{87.5} & \textbf{83.0} & 84.0 & \textbf{61.1} & \textbf{85.6} & 52.1 & 56.2 & 60.2 & \textbf{74.4} \\
    \hline
    \bottomrule
  \end{tabular}
\end{scriptsize}
  \label{tab:voc_val_detail}
\end{table}




\begin{table*}[t]
  \centering
  \caption{
    Per-class performance comparisons with WSSS methods in terms of IoUs (\%) on the PASCAL VOC 2012 \emph{test} set.
  }
  \hspace*{-4em}
  \begin{scriptsize}
  \begin{tabular}{
    p{0.20\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} p{0.015\textwidth} 
    c }
    \toprule
    Method & \rotatebox[origin=l]{90}{bkg} & \rotatebox[origin=l]{90}{aero} & \rotatebox[origin=l]{90}{bike} & \rotatebox[origin=l]{90}{bird} & \rotatebox[origin=l]{90}{boat} & \rotatebox[origin=l]{90}{bottle} & \rotatebox[origin=l]{90}{bus} & \rotatebox[origin=l]{90}{car} & \rotatebox[origin=l]{90}{cat} & \rotatebox[origin=l]{90}{chair} & \rotatebox[origin=l]{90}{cow} & \rotatebox[origin=l]{90}{table} & \rotatebox[origin=l]{90}{dog} & \rotatebox[origin=l]{90}{horse} & \rotatebox[origin=l]{90}{mbk} & \rotatebox[origin=l]{90}{person} & \rotatebox[origin=l]{90}{plant} & \rotatebox[origin=l]{90}{sheep} & \rotatebox[origin=l]{90}{sofa} & \rotatebox[origin=l]{90}{train} & \rotatebox[origin=l]{90}{tv}  & mIoU  \\
    \hline \hline
    EM-Adapt & 76.3 & 37.1 & 21.9 & 41.6 & 26.1 & 38.5 & 50.8 & 44.9 & 48.9 & 16.7 & 40.8 & 29.4 & 47.1 & 45.8 & 54.8 & 28.2 & 30.0 & 44.0 & 29.2 & 34.3 & 46.0 & 39.6 \\
    MIL-LSE & 78.7 & 48.0 & 21.2 & 31.1 & 28.4 & 35.1 & 51.4 & 55.5 & 52.8 & 7.8 & 56.2 & 19.9 & 53.8 & 50.3 & 40.0 & 38.6 & 27.8 & 51.8 & 24.7 & 33.3 & 46.3 & 40.6 \\
    SEC & 83.5 & 56.4 & 28.5 & 64.1 & 23.6 & 46.5 & 70.6 & 58.5 & 71.3 & 23.2 & 54.0 & 28.0 & 68.1 & 62.1 & 70.0 & 55.0 & 38.4 & 58.0 & 39.9 & 38.4 & 48.3 & 51.7 \\
    TransferNet & 85.7 & 70.1 & 27.8 & 73.7 & 37.3 & 44.8 & 71.4 & 53.8 & 73.0 & 6.7 & 62.9 & 12.4 & 68.4 & 73.7 & 65.9 & 27.9 & 23.5 & 72.3 & 38.9 & 45.9 & 39.2 & 51.2 \\
    CRF-RNN & 85.7 & 58.8 & 30.5 & 67.6 & 24.7 & 44.7 & 74.8 & 61.8 & 73.7 & 22.9 & 57.4 & 27.5 & 71.3 & 64.8 & 72.4 & 57.3 & 37.3 & 60.4 & 42.8 & 42.2 & 50.6 & 53.7 \\
    WebCrawl & 87.2 & 63.9 & 32.8 & 72.4 & 26.7 & 64.0 & 72.1 & 70.5 & 77.8 & 23.9 & 63.6 & 32.1 & 77.2 & 75.3 & 76.2 & 71.5 & 45.0 & 68.8 & 35.5 & 46.2 & 49.3 & 58.7 \\
    DSRG & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 63.2 \\
    PSA & 89.1 & 70.6 & 31.6 & 77.2 & 42.2 & 68.9 & 79.1 & 66.5 & 74.9 & 29.6 & 68.7 & 56.1 & 82.1 & 64.8 & 78.6 & 73.5 & 50.8 & 70.7 & 47.7 & 63.9 & 51.1 & 63.7 \\
    FickleNet & 90.3 & 77.0 & 35.2 & 76.0 & 54.2 & 64.3 & 76.6 & 76.1 & 80.2 & 25.7 & 68.6 & 50.2 & 74.6 & 71.8 & 78.3 & 69.5 & 53.8 & 76.5 & 41.8 & \textbf{70.0} & 54.2 & 65.0 \\
    SSDD & 89.5 & 71.8 & 31.4 & 79.3 & 47.3 & 64.2 & 79.9 & 74.6 & 84.9 & 30.8 & 73.5 & 58.2 & 82.7 & 73.4 & 76.4 & 69.9 & 37.4 & 80.5 & 54.5 & 65.7 & 50.3 & 65.5 \\
    MCIS & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 66.9 \\
    RRM & 87.8 & 77.5 & 30.8 & 71.7 & 36.0 & 64.2 & 75.3 & 70.4 & 81.7 & 29.3 & 70.4 & 52.0 & 78.6 & 73.8 & 74.4 & 72.1 & 54.2 & 75.2 & 50.6 & 42.0 & 52.5 & 62.9 \\
    SSSS & 88.7 & 70.4 & 35.1 & 75.7 & 51.9 & 65.8 & 71.9 & 64.2 & 81.1 & 30.8 & 73.3 & 28.1 & 81.6 & 69.1 & 62.6 & 74.8 & 48.6 & 71.0 & 40.1 & 68.5 & \textbf{64.3} & 62.7 \\
    CONTA & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 66.7 \\
    SEAM & 88.8 & 68.5 & 33.3 & 85.7 & 40.4 & 67.3 & 78.9 & 76.3 & 81.9 & 29.1 & 75.5 & 48.1 & 79.9 & 73.8 & 71.4 & 75.2 & 48.9 & 79.8 & 40.9 & 58.2 & 53.0 & 64.5 \\
    NSRM & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 68.5 \\
    EDAM & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 70.6 \\
    AdvCAM & 90.1 & 81.2 & 33.6 & 80.4 & 52.4 & 66.6 & 87.1 & 80.5 & 87.2 & 28.9 & 80.1 & 38.5 & 84.0 & 83.0 & 79.5 & 71.9 & 47.5 & 80.8 & \textbf{59.1} & 65.4 & 49.7 & 68.0 \\
    CSE & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 68.2 \\
    DRS & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 70.7 \\
    CPN & 90.4 & 79.8 & 32.9 & 85.7 & 52.8 & 66.3 & 87.2 & 81.3 & 87.6 & 28.2 & 79.7 & 50.1 & 82.9 & 80.4 & 78.8 & 70.6 & 51.1 & 83.4 & 55.4 & 68.5 & 44.6 & 68.5 \\
    RIB & 90.4 & 80.5 & 32.8 & 84.9 & 59.4 & 69.3 & 87.2 & 83.5 & 88.3 & 31.1 & 80.4 & 44.0 & 84.4 & 82.3 & 80.9 & 70.7 & 43.5 & 84.9 & 55.9 & 59.0 & 47.3 & 68.6 \\
    \hline
    Ours (single-stage, RS)& 89.8 & 83.8 & 33.4 & 87.5 & 39.8 & 67.1 & 85.2 & 78.5 & 91.2 & 29.3 & 77.9 & 54.1 & 84.3 & 81.7 & 78.5 & 78.9 & 53.4 & 78.4 & 53.9 & 42.2 & 56.9 & 67.9 \\
    Ours (single-stage, RS+EPM)& 91.2 & 86.8 & \textbf{37.3} & 80.6 & 52.1 & 71.3 & 87.8 & 81.3 & 90.9 & 32.2 & 80.4 & 54.5 & 86.4 & 88.6 & \textbf{83.6} & 80.8 & 59.0 & 82.8 & 54.2 & 42.6 & 59.1 & 70.6 \\
    Ours (multi-stage, RS)& 91.4 & 89.5 & 36.2 & \textbf{88.7} & 46.2 & 69.1 & 93.1 & \textbf{85.7} & \textbf{91.8} & 34.0 & 86.7 & \textbf{66.3} & \textbf{89.0} & 88.8 & 82.9 & \textbf{81.3} & 57.3 & 89.0 & 57.5 & 46.9 & 57.9 & 72.8 \\
    Ours (multi-stage, RS+EPM)& \textbf{91.9} & \textbf{89.7} & 37.3 & 88.0 & \textbf{62.5} & \textbf{72.1} & \textbf{93.5} & 85.6 & 90.2 & \textbf{36.3} & \textbf{88.3} & 62.5 & 86.3 & \textbf{89.1} & 82.9 & 81.2 & \textbf{59.7} & \textbf{89.2} & 56.2 & 44.5 & 59.4 & \textbf{73.6} \\
    \hline
    \bottomrule
  \end{tabular}
  \end{scriptsize}
  \label{tab:voc_test_detail}
\end{table*}

\clearpage

\begin{table*}[t]
  \centering
  \caption{
    Per-class performance comparisons with WSSS methods in terms of IoUs (\%) on the MS COCO 2014 \emph{val} set.
  }
  \hspace*{-6em}
  \begin{scriptsize}
  \begin{tabular}{
    p{0.080\textwidth} c c c c | p{0.080\textwidth} c c c c}
    \toprule
    Class & SEC & DSRG & Ours (multi-stage, RS) & Ours (multi-stage, RS+EPM) & Class &  SEC & DSRG & Ours (multi-stage, RS) & Ours (multi-stage, RS+EPM) \\
    \hline \hline
    background & 74.3 & 80.6 & 82.6 & \textbf{83.6} & wine glass & 22.3 & 24.0 & 37.5 & \textbf{39.8} \\
    person & 43.6 & - & 74.8 & \textbf{74.9} & cup & 17.9 & 20.4 & \textbf{39.1} & 38.9 \\
    bicycle & 24.2 & 30.4 & 53.1 & \textbf{55.0} & fork & 1.8 & 0.0 & \textbf{19.9} & 4.9 \\
    car & 15.9 & 22.1 & 48.6 & \textbf{50.1} & knife & 1.4 & 5.0 & \textbf{19.9} & 9.0 \\
    motorcycle & 52.1 & 54.2 & 72.4 & \textbf{72.9} & spoon & 0.6 & 0.5 & \textbf{5.5} & 1.1 \\
    airplane & 36.6 & 45.2 & 73.4 & \textbf{76.5} & bowl & 12.5 & 18.8 & \textbf{26.8} & 11.3 \\
    bus & 37.7 & 38.7 & 71.2 & \textbf{72.5} & banana & 43.6 & 46.4 & 66.4 & \textbf{67.0} \\
    train & 30.1 & 33.2 & 44.8 & \textbf{47.4} & apple & 23.6 & 24.3 & 43.0 & \textbf{49.2} \\
    truck & 24.1 & 25.9 & \textbf{46.5} & \textbf{46.5} & sandwich & 22.8 & 24.5 & \textbf{39.7} & 33.7 \\
    boat & 17.3 & 20.6 & 32.1 & \textbf{44.1} & orange & 44.3 & 41.2 & 59.8 & \textbf{62.3} \\
    traffic light & 16.7 & 16.1 & 23.6 & \textbf{60.8} & broccoli & 36.8 & 35.7 & 46.5 & \textbf{50.4} \\
    fire hydrant & 55.9 & 60.4 & 79.0 & \textbf{80.3} & carrot & 6.7 & 15.3 & \textbf{35.1} & 35.0 \\
    stop sign & 48.4 & 51.0 & 79.0 & \textbf{84.1} & hot dog & 31.2 & 24.9 & \textbf{49.0} & 48.3 \\
    parking meter & 25.2 & 26.3 & 72.2 & \textbf{77.8} & pizza & 50.9 & 56.2 & \textbf{69.9} & 68.6 \\
    bench & 16.4 & 22.3 & 40.3 & \textbf{41.2} & donut & 32.8 & 34.2 & \textbf{62.6} & 62.3 \\
    bird & 34.7 & 41.5 & \textbf{65.2} & 62.6 & cake & 12.0 & 6.9 & \textbf{50.7} & 48.3 \\
    cat & 57.2 & 62.2 & \textbf{79.2} & \textbf{79.2} & chair & 7.8 & 9.7 & 26.9 & \textbf{28.9} \\
    dog & 45.2 & 55.6 & \textbf{73.4} & 73.3 & couch & 5.6 & 17.7 & \textbf{47.0} & 44.9 \\
    horse & 34.4 & 42.3 & 74.4 & \textbf{76.1} & potted plant & 6.2 & 14.3 & \textbf{20.3} & 16.9 \\
    sheep & 40.3 & 47.1 & 76.4 & \textbf{80.0} & bed & 23.4 & 32.4 & \textbf{54.8} & 53.6 \\
    cow & 41.4 & 49.3 & 78.4 & \textbf{79.3} & dining table & 0.0 & 3.8 & \textbf{31.4} & 24.6 \\
    elephant & 62.9 & 67.1 & 84.7 & \textbf{85.6} & toilet & 38.5 & 43.6 & \textbf{71.1} & \textbf{71.1} \\
    bear & 59.1 & 62.6 & \textbf{84.9} & 82.9 & tv & 19.2 & 25.3 & 49.5 & \textbf{49.9} \\
    zebra & 59.8 & 63.2 & 85.2 & \textbf{87.0} & laptop & 20.1 & 21.1 & \textbf{57.0} & 56.6 \\
    giraffe & 48.8 & 54.3 & 79.8 & \textbf{82.2} & mouse & 3.5 & 0.9 & 7.9 & \textbf{17.4} \\
    backpack & 0.3 & 0.2 & \textbf{21.6} & 9.4 & remote & 17.5 & 20.6 & 50.3 & \textbf{54.8} \\
    umbrella & 26.0 & 35.3 & 71.1 & \textbf{73.4} & keyboard & 12.5 & 12.3 & \textbf{51.1} & 48.8 \\
    handbag & 0.5 & 0.7 & \textbf{9.3} & 4.6 & cell phone & 32.1 & 33.0 & \textbf{61.2} & 60.8 \\
    tie & 6.5 & 7.0 & \textbf{18.0} & 17.2 & microwave & 8.2 & 11.2 & \textbf{47.7} & 43.6 \\
    suitcase & 16.7 & 23.4 & \textbf{54.6} & 53.9 & oven & 13.7 & 12.4 & \textbf{42.2} & 38.0 \\
    frisbee & 12.3 & 13.0 & 55.1 & \textbf{57.7} & toaster & 0.0 & 0.0 & \textbf{0.2} & 0.0 \\
    skis & 1.6 & 1.5 & \textbf{9.0} & 8.2 & sink & 10.8 & 17.8 & \textbf{38.8} & 36.9 \\
    snowboard & 5.3 & 16.3 & \textbf{28.6} & 24.7 & refrigerator & 4.0 & 15.5 & \textbf{59.4} & 51.8 \\
    sports ball & 7.9 & 9.8 & 24.5 & \textbf{41.6} & book & 0.4 & 12.3 & \textbf{28.6} & 27.3 \\
    kite & 9.1 & 17.4 & 31.2 & \textbf{62.6} & clock & 17.8 & 20.7 & \textbf{27.8} & 23.3 \\
    baseball bat & 1.0 & \textbf{4.8} & 1.4 & 1.5 & vase & 18.4 & 23.9 & \textbf{27.1} & 26.0 \\
    baseball glove & 0.6 & \textbf{1.2} & 1.1 & 0.4 & scissors & 16.5 & 17.3 & 46.6 & \textbf{47.1} \\
    skateboard & 7.1 & 14.4 & 30.4 & \textbf{34.8} & teddy bear & 47.0 & 46.3 & \textbf{69.2} & 68.8 \\
    surfboard & 7.7 & 13.5 & 11.1 & \textbf{17.0} & hair drier & 0.0 & 0.0 & 0.0 & 0.0 \\
    tennis racket & 9.1 & 6.8 & \textbf{14.8} & 9.0 & toothbrush & 2.8 & 2.0 & \textbf{23.3} & 19.7 \\\cline{6-10}
    bottle & 13.2 & 22.3 & \textbf{41.3} & 38.1 & \textbf{mIoU} & 22.4 & 26.0 & 45.8 & \textbf{46.4} \\
    \bottomrule
  \end{tabular}
  \end{scriptsize}
  \label{tab:coco_val_detail}
\end{table*}

\clearpage





\begin{figure*}[t]\centering \includegraphics[width=\linewidth]{Figure_Supple_VOC_qualitative_results_v3.pdf}
    \caption{
        Qualitative segmentation results of PASCAL VOC 2012 \emph{val} set.
    }
    \label{fig:additional_voc_results}
\end{figure*}



 

\begin{figure*}[t]\centering \includegraphics[width=\linewidth]{Figure_Supple_COCO_qualitative_results_v4.pdf}
    \caption{
        Qualitative segmentation results of MS COCO 2014 \emph{val} set.
    }
    \label{fig:additional_coco_results}
\end{figure*}

\clearpage

\begin{figure*}[t]\centering \includegraphics[width=0.8\linewidth]{Figure_Supple_VOC_Improvement_v3.pdf}
    \caption{
        Visualization of attention maps with recursive improvements on the PASCAL VOC 2012 \emph{train} set.
    }
    \label{fig:additional_voc_att_results}
\end{figure*}

\clearpage

\begin{figure*}[t]\centering \includegraphics[width=0.8\linewidth]{Figure_Supple_COCO_Improvement_v3.pdf}
    \caption{
        Visualization of attention maps with recursive improvements on the MS COCO 2014 \emph{train} set.
    }
    \label{fig:additional_coco_att_results}
\end{figure*}
	
	
\end{document} 