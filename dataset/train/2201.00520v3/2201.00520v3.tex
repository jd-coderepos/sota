\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/fig2.pdf}
    \caption{An illustration of our deformable attention mechanism. (a) presents the information flow of deformable attention. In the left part, a group of reference points is placed uniformly on the feature map, whose offsets are learned from the queries by the offset network. Then the deformed keys and values are projected from the sampled features according to the deformed points, as shown in the right part. Relative position bias is also computed by the deformed points, enhancing the multi-head attention which outputs the transformed features. We show only 4 reference points for a clear presentation, there are many more points in real implementation \textit{de facto}. (b) reveals the detailed structure of the offset generation network, marked with sizes of feature maps.}
    \label{fig:fig2}
\end{figure*}
\section{Deformable Attention Transformer}

\subsection{Preliminaries}

We first revisit the attention mechanism in recent Vision Transformers. Taking a flattened feature map  as the input, a multi-head self-attention (MHSA) block with  heads is formulated as 

where  denotes the softmax function, and  is the dimension of each head.  denotes the embedding output from the -th attention head,  denote query, key, and value embeddings respectively.  are the projection matrices. To build up a Transformer block, an MLP block with two linear transformations and a GELU activation is usually adopted to provide nonlinearity.

With normalization layers and identity shortcuts, the -th Transformer block is formulated as

where LN is Layer Normalization \cite{LN}. 




\subsection{Deformable Attention}



Existing hierarchical Vision Transformers, notably PVT \cite{PVT} and Swin Transformer \cite{Swin} try to address the challenge of excessive attention. The downsampling technique of the former results in severe information loss, and the shift-window attention of the latter leads to a much slower growth of receptive fields, which limits the potential of modeling large objects. 
Thus a data-dependent sparse attention is required to flexibly model relevant features, leading to deformable mechanism firstly proposed in DCN \cite{DCNv1}. However, simply implementing DCN in Transformer models is a non-trivial problem.
In DCN, each element on the feature map learns its offsets individually, of which a  deformable convolution on an  feature map has the space complexity of . If we directly apply the same mechanism in  the attention module, the space complexity will drastically rise to , where  are the number of queries and keys and usually have the same scale as the feature map size , bringing approximately a biquadratic complexity.
Although Deformable DETR \cite{DeformableDETR} has managed to reduce this overhead by setting a lower number of keys with  at each scale and works well as a detection head, 
it is inferior to attend to such few keys in a backbone network because of the unacceptable loss
of information (see detailed comparison in Appendix). In the meantime, the observations in \cite{deepvit, gcnet} have revealed that different queries have similar attention maps in visual attention models. Therefore, we opt for a simpler solution with shared shifted keys and values for each query to achieve an efficient trade-off.




Specifically, we propose deformable attention to model the relations among tokens effectively under the guidance of the important regions in the feature maps. These focused regions are determined by multiple groups of deformed sampling points which are learned from the queries by an offset network. 
We adopt bilinear interpolation to sample features from the feature maps, and then the sampled features are fed to the key and value projections to get the deformed keys and values. Finally, standard multi-head attention is applied to attend queries to the sampled keys and aggregate features from the deformed values. Additionally, the locations of deformed points provide a more powerful relative position bias to facilitate the learning of the deformable attention, which will be discussed in the following sections. 

\noindent
\textbf{Deformable attention module.}
As illustrated in Figure \ref{fig:fig2}(a), given the input feature map , a uniform grid of points  are generated as the references.
Specifically, the grid size is downsampled from the input feature map size by a factor , . The values of reference points are linearly spaced 2D coordinates , and then we normalize them to the range  according to the grid shape , in which  indicates the top-left corner and  indicates the bottom-right corner. To obtain the offset for each reference point, the feature maps are projected linearly to the query tokens ,
and then fed into a light weight sub-network  to generate the offsets .
To stabilize the training process, we scale the amplitude of  by some predefined factor  to prevent too large offset, \textit{i.e.,} . Then the features are sampled at the locations of deformed points as keys and values, followed by projection matrices:

 and  represent the deformed key and value embeddings respectively.
Specifically, we set the sampling function  to a bilinear interpolation to make it differentiable:

where  and  indexes all the locations on . As  would be non-zero only on the 4 integral points closest to , it simplifies Eq.\eqref{eq:bilinear} to a weighted average on 4 locations. 
Similar to existing approaches, we perform multi-head attention on  and adopt relative position offsets . The output of an attention head is formulated as:

where  correspond to the position embedding following previous work \cite{Swin} while with several adaptations. Details will be explained later in this section. Features of each head are concatenated together and projected through  to get the final output  as Eq.\eqref{eq:MHSA}.
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/fig3.pdf}
    \caption{An illustration of \ourmethod{} architecture.  to  are the numbers of stacked successive local attention and shift-window / deformable attention blocks.  and  denote the kernel size and stride of the convolution layer in patch embeddings. }
    \label{fig:fig3}
\end{figure*}

\noindent
\textbf{Offset generation.} 
As we have stated, a sub-network is adopted for offset generation which consumes the query features and outputs the offset values for reference points respectively. Considering that each reference point covers a local  region ( is the largest value for offset), the generation network should also have the perception of the local features to learn reasonable offsets. Therefore, we implement the sub-network as two convolution modules with a nonlinear activation, as depicted in Figure \ref{fig:fig2}(b). The input features are first passed through a  depthwise convolution to capture local features. Then, GELU activation and a  convolution is adopted to get the 2D offsets. It is also worth noticing that the bias in  convolution is dropped to alleviate the compulsive shift for all locations. 

\noindent
\textbf{Offset groups.} 
To promote the diversity of the deformed points, we follow a similar paradigm in MHSA, and split the feature channel into  groups. Features from each group use the shared sub-network to generate the corresponding offsets respectively. In practice, the head number  for the attention module is set to be multiple times of the size of offset groups ,
ensuring that multiple attention heads are assigned to one group of deformed keys and values.

\noindent
\textbf{Deformable relative position bias.} Relative position bias encodes the relative positions between every pair of query and key, which augments the vanilla attention with spatial information. Considering a feature map with shape , its relative coordinate displacements lie in the range of  and  at two dimensions respectively. In Swin Transformer \cite{Swin}, a relative position bias table  is constructed to obtain the relative position bias  by indexing the table with the relative displacements in two directions. Since our deformable attention has continuous positions of keys, we compute the relative displacements in the normalized range , and then interpolate  in the parameterized bias table  by the continuous relative displacements in order to cover all possible offset values.

\noindent
\textbf{Computational complexity.} Deformable multi-head attention (DMHA) has a similar computation cost as the counterpart in PVT or Swin Transformer. The only additional overhead comes from the sub-network that is used to generate offsets.
The complexity of the whole module can be summarized as:

where  is the number of sampled points. It can be immediately seen that the computational cost of the offset network has linear complexity \textit{w.r.t.} the channel size, which is comparably minor to the cost for attention computation. Typically, consider the third stage of a Swin-T \cite{Swin} model for image classification where , , , the computational cost for the attention module in a single block is 79.63M FLOPs. If equipped with our deformable module (with ), the additional overhead is 5.08M Flops, which is only  of the whole module. Additionally, by choosing a large downsample factor , the complexity will be further reduced, which makes it friendly to the tasks with much higher resolution inputs such as object detection and instance segmentation. 




\subsection{Model Architectures}
We replace the vanilla MHSA with our deformable attention in the Transformer (Eq.\eqref{eq:att_layer}), and combine it with an MLP (Eq.\eqref{eq:mlp_layer}) to build a deformable vision transformer block. In terms of the network architecture, our model, \textbf{Deformable Attention Transformer}, shares a similar pyramid structure with \cite{PVT,Swin,dpt,botnet}, which is 
broadly applicable to various vision tasks requiring multiscale feature maps. As illustrated in Figure \ref{fig:fig3}, an input image with shape  is firstly embedded by a 44 non-overlapped convolution with stride 4, followed by a normalization layer to get the  patch embeddings. Aiming to build a hierarchical feature pyramid, the backbone includes 4 stages with a progressively increasing stride. Between two consecutive stages, there is a non-overlapped 22 convolution with stride 2 to downsample the feature map to halve the spatial size and double the feature dimensions. In classification task, we first normalize the feature maps output from the last stage and then adopt a linear classifier with pooled features to predict the logits. In object detection, instance segmentation and semantic segmentation tasks, DAT plays the role of a backbone in an integrated vision model to extract multiscale features. We add a normalization layer to the features from each stage before feeding them into the following modules such as FPN \cite{fpn} in object detection or decoders in semantic segmentation. 


We introduce successive local attention and deformable attention blocks in the third and the fourth stage of DAT. The feature maps are firstly processed by a window-based local attention to aggregate information locally, and then passed through the deformable attention block to model the global relations between the local augmented tokens. This alternate design of attention blocks with local and global receptive fields helps the model learn strong representations, sharing a similar pattern in GLiT \cite{glit}, TNT \cite{tnt} and Pointformer \cite{pointformer}. 
Since the first two stages mainly learn local features, deformable attention in these early stages is less preferred.
In addition, the keys and values in the first two stages have a rather large spatial size, which greatly increase the computational overhead in the dot products and bilinear interpolations in deformable attention. Therefore, to achieve a trade-off between model capacity and computational burden, we only place deformable attention in the third and the fourth stage and adopt the shift-window attention in Swin Transformer \cite{Swin} to have a better representation in the early stages. We build three variants of DAT in different parameters and FLOPs for a fair comparison with other Vision Transformer models. We change the model size by stacking more blocks in the third stage and increasing the hidden dimensions. The detailed architectures are reported in Table \ref{tab:arch}.
Note that there are other design choices for the first two stages of DAT, 
e.g., the SRA module in PVT. We show the comparison results in Table \ref{tab:abl_ds}.

\begin{table}[t]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{center}
\setlength{\tabcolsep}{0.01mm}{
\renewcommand\arraystretch{1.0}
\begin{tabular}{c|c|c|c}
\thickhline
\multicolumn{4}{c}{\textbf{\ourmethod{} Architectures}} \\
 & \ourmethod-T & \ourmethod-S & \ourmethod-B \\
\hline
\multirow{3}{*}{

} 
&  &  &  \\
& window size: 7 & window size: 7 & window size: 7 \\
& heads: 3 & heads: 3 & heads: 4 \\
\hline
\multirow{3}{*}{

} 
&  &  &  \\
& window size: 7 & window size: 7 & window size: 7 \\
& heads: 6 & heads: 6 & heads: 8\\
\hline
\multirow{4}{*}{

} 
&  &  &  \\
& window size: 7 & window size: 7 & window size: 7 \\
& heads: 12 & heads: 12 & heads: 16\\
& groups: 3 & groups: 3 & groups: 4 \\
\hline
\multirow{4}{*}{

} 
&  &  &  \\
& window size: 7 & window size: 7 & window size: 7 \\
& heads: 24 & heads: 24 & heads: 32 \\
& groups: 6 & groups: 6 & groups: 8 \\
\thickhline
\end{tabular}}
\end{center}
\vskip -0.1in
\caption{Model architecture specifications. : Number of block at stage i. : Channel dimension. \textbf{window size}: Region size in local attention module. \textbf{heads}: Number of heads in DMHA. \textbf{groups}: Offset groups in DMHA.}
\label{tab:arch}
\vskip -0.2in
\end{table}