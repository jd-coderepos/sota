We now present experimental observations of the effects of label smoothing under label noise.
We then provide insights into why smoothing can successfully denoise labels,
by viewing smoothing as a form of \emph{shrinkage regularisation}.

\subsection{Denoising effects of label smoothing}

We begin by empirically answering the question: can label smoothing successfully mitigate label noise?
To study this,
we employ smoothing in settings where the training data is artificially injected with symmetric label noise.
This follows the convention in the label noise literature~\citep{Patrini:2017,Han:2018b,Charoenphakdee:2019}.

Specifically, 
we consider the \cifarT{}, \cifarH{} and \imagenet{} datasets, 
and add symmetric label noise at level  to the training (but \emph{not} the test) set;
i.e.,
we replace the training label with a uniformly chosen label  of the time.
On \cifarT{} and \cifarH{} we train two different models on this noisy data, \resnetT{} and \resnetF{},
with similar hyperparameters as~\citet{Muller:2019}.
Each experiment is repeated five times, and we report the mean and standard deviation of the \emph{clean} test accuracy.
On \imagenet{} we train \resnetIM{} with LARS~\cite{you2017large}.
We describe the detailed experimental setup in Appendix~\ref{sec:appx_hyper}.

As loss functions, 
our baseline
is
training with the softmax cross-entropy on the noisy labels.
We then employ label smoothing~\eqref{eqn:smoothing} ({\bf LS}) for various values of ,
as well as forward ({\bf FC}) and backward ({\bf BC}) correction~\eqref{eqn:backward-correction},~\eqref{eqn:forward-correction}  assuming symmetric noise for various values of .
We remark here that in the label noise literature, it is customary to \emph{estimate} ,
with theoretical optimal value ;
however, we shall here simply treat this as a tuning parameter akin to the smoothing , 
whose effect we shall study.

We now analyse the results along several dimensions.

\noindent {\bf Accuracy}:
In Figure \ref{fig:cifar-smoothing-vs-correction}, we plot the test accuracies of all methods on \cifarT{} and \cifarH{}.
Our first finding is that
\emph{label smoothing significantly improves accuracy over the baseline}.
We observe similar denoising effects on \imagenet{} in Table~\ref{tbl:imagenet}.
This confirms that empirically, label smoothing is effective in dealing with label noise. 



Our second finding is that, surprisingly,
\emph{choosing , the true noise rate, improves performance of all methods}.
This is in contrast to the theoretically optimal choice 
for loss correction approaches~\citep{Patrini:2017},
and indicates it is valuable to treat  as a tuning parameter.

Finally, we see that label smoothing is \emph{often competitive with loss correction}.
This is despite it minimising a fundamentally different loss to the unbiased backward correction,
as discussed in~\S\ref{sec:smoothing_vs_smearing}.
We note however that loss correction generally produces the best overall accuracy with high .

\begin{figure*}[!t]\centering
    \subfigure[CIFAR 100]{\label{fig:first}\includegraphics[height=1.75in]{figs/sweep_alpha_cifar100.pdf}}\qquad
    \subfigure[CIFAR 10]{\label{fig:second}\includegraphics[height=1.75in]{figs/sweep_alpha_cifar10.pdf}}\caption{Effect of  on smoothing and forward label correction test accuracies on \cifarH{} and \cifarT{} from \resnetT{}. Standard deviations are denoted by the shaded regions. Label smoothing (LS) significantly improves over baseline, and choosing , the true noise rate, improves even further. Forward correction (FC) outperforms LS and also benefits from choosing large values for . Backward correction (BC) is worse than baseline for small , and better than baseline for large . In Table~\ref{tbl:experiments_smearing} in appendix, we report additional results for \resnetF{} and \resnetT{} from different label smearing methods, including where confusion matrix is estimated by pre-training a model as in ~\citet{Patrini:2017}.}
    \label{fig:cifar-smoothing-vs-correction}
\end{figure*}

\begin{table}[!t]
    \centering
    
{
    \begin{tabular}{lccccc@{}}
        \toprule
\multicolumn{1}{c}{\textbf{}} & \textbf{} & \textbf{} & \multicolumn{1}{c}{} &  \multicolumn{1}{c}{} &\multicolumn{1}{c}{}\\
        \toprule
        LS & 70.86 & 71.12 & 71.55 & 70.95 & 70.59\\ 
        FC & 70.86 & 73.04 & 73.17 & 73.35 & 72.92\\ 
        \bottomrule
    \end{tabular}
    }
    
    \caption{Test accuracy on \imagenet{} trained with 
    
    label noise on \resnetIM{}, with
label smoothing (LS) and forward correction (FC)
    for varying . Both LS and FC successfully denoise, and thus improve over the baseline ().}
    \label{tbl:imagenet}
\end{table}

\begin{figure*}[!t]
    \centering
    \subfigure[Gap between true (unobserved) label logit and mean logit.]{\includegraphics[width=0.35\textwidth]{figs/logits/true_logit_to_avg_noisydatanomark_cifar100.pdf}\label{fig:noise_labellogit_vs_avglogit_true}
    }\qquad
    \subfigure[Gap between noisy (observed) label logit and mean logit.]{\includegraphics[width=0.35\textwidth]{figs/logits/noisy_logit_to_avg_noisydatanomark_cifar100.pdf}\label{fig:noise_labellogit_vs_avglogit_noisy}
    }    
    \caption{Density of differences between logit corresponding to the 
    true (left plot; corresponding to the ``true'' label, before injecting label noise) and 
    noisy label (right plot; corresponding to the ``noisy'' label, after injecting label noise) 
    and the average over all logits on the mis-labeled portion of the train data.
    Results are with  on \cifarH{}, and the \resnetT{} model.
    LS reduces confidence mostly on the noisy label, whereas FC and BC increase confidence mostly on the true label.
    See Figure~\ref{fig:maxlogit_vs_avglogit} for plots on full and clean data.}
    \label{fig:noise_labellogit_vs_avglogit}
\end{figure*}

\noindent {\bf Denoising}:
What explains the effectiveness of label smoothing for training with label noise? 
{Does it 
correct the predictions on noisy examples,
or does it 
only further improve the predictions 
on the clean (non-noisy) examples?}


To answer these questions, we separately inspect accuracies on the noisy and clean portions of the training data (i.e., on those samples whose labels are flipped, or not).
Table~\ref{tbl:acc_on_different_portions_of_train} reports this breakdown from the \resnetT{} model on \cifarH{}, for different values of . 
We see that as  increases, accuracy improves on both the noisy and clean parts of the data, 
with a more significant boost on the noisy part.
Consequently, smoothing \emph{systematically improves predictions} of both clean and noisy samples.


\noindent {\bf Model confidence}:
Predictive accuracy is only concerned with a model ranking the true label ahead of the others.
However, the \emph{confidence} in model predictions is also of interest,
particularly since a danger with label noise is 
being overly confident in predicting a noisy label.
How do smoothing and correction methods affect this confidence under noise?

To measure this,
in Figure~\ref{fig:noise_labellogit_vs_avglogit} 
we plot distributions of the differences between the logit activation  for a true/noisy label , 
and the average logit activation . 
Compared to the baseline, label smoothing significantly \emph{reduces confidence} in the \emph{noisy} label 
(refer to the left side of Figure~\ref{fig:noise_labellogit_vs_avglogit_noisy}).

To visualise this effect of smoothing, in Figure~\ref{fig:smoothing_logits} we plot pre-logits (penultimate layer output) of examples from 3 classes projected onto their class vectors as in~\citet{Muller:2019}, for a \resnetT{} trained on \cifarH{}.
As we increase , the confidences for noisy labels shrink, showing the denoising effects of label smoothing.

On the other hand, both forward and backward correction systematically \emph{increase confidence} in predictions.
This is especially pronounced for forward correction, demonstrated by the large spike for high differences in Figure~\ref{fig:noise_labellogit_vs_avglogit_noisy}.
At the same time, these techniques increase the confidence in predictions of the true label 
(refer to Figure~\ref{fig:noise_labellogit_vs_avglogit_true}):
forward correction in particular
becomes much more confident in the true label than any other technique.

In sum,
Figure~\ref{fig:noise_labellogit_vs_avglogit} illustrates both positive and adverse effects on confidence from label smearing techniques:
label smoothing becomes less confident in both the noisy and correct labels,
while forward and backward correction become more confident in both the correct labels and noisy labels. 


\begin{table}[!t]
    \centering
    
{
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \multicolumn{1}{c}{\textbf{}} & \multicolumn{1}{c}{\textbf{Full train}} & \textbf{Clean part of train} & \multicolumn{2}{c}{\textbf{Noisy part of train}}\\
         & true labels & true labels & true labels & noisy labels\\
        
        \toprule
        0.0 & 77.39 & 86.75 & 39.92 & 17.88 \\ 
        0.1 & 80.11 & 87.99 & 48.58 & 12.27\\
        0.2 & 81.22 & 88.27 & 53.01 & 8.32\\
        \bottomrule
    \end{tabular}
    }
    
\caption{Accuracy on different portions of the training set from \resnetT{}, trained with different label smoothing values  on \cifarH{}. As  increases, accuracy improves on both clean and noisy part of data. Interestingly, the improvement on the noisy part of data is greater than the reduction in fit to the noisy labels (compare the two rightmost columns in the table). Thus, there are noisy examples assigned neither to correct class nor to the observed \emph{noisy} class without LS, and which LS helps classify correctly.}
    \label{tbl:acc_on_different_portions_of_train}
\end{table}

\begin{table}[!t]
    \centering
    


{
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \multicolumn{1}{c}{\textbf{}} & \textbf{LS} & \multicolumn{1}{c}{\textbf{FC}} & \multicolumn{1}{c}{\textbf{BC}}\\
        \toprule
        0.0 & 0.111 & 0.111 & 0.111 \\ 
        0.1 & 0.108 & 0.153 & 0.214 \\ 
        0.2 & 0.156 & 0.165 & 0.266 \\ 
        \bottomrule
    \end{tabular}
    }
    
    \caption{Expected calibration error (ECE) computed on  bins on test set
    for \resnetT{} on \cifarH{}, trained with different label smearing techniques under varying values of .
Generally, label smearing is detrimental to calibration.}
    \label{tbl:ece}
\end{table}

\noindent {\bf Model calibration}:
To further tease out the impact of label smearing on model confidences,
we ask:
how do these techniques affect the \emph{calibration} of the output probabilities?
This measures how meaningful the model probabilities are in a frequentist sense~\citep{Dawid:1982}.

In Table~\ref{tbl:ece}, we report
the expected calibration error (ECE)~\citep{Guo:2017}
on the test set for each method. 
While smoothing improves calibration over the baseline with 
--- an effect noted also in~\cite{Muller:2019} ---
for larger , it becomes significantly \emph{worse}.
Furthermore, loss correction techniques significantly degrade calibration over smoothing.
This is in keeping with the above findings as to these methods sharpening prediction confidences.

\begin{figure*}[!t]
    \centering
    \subfigure[Label smoothing .]{\includegraphics[width=0.275\textwidth]{figs/logits/LS0_v0_train_bold.png}}
    \subfigure[Label smoothing .]{\includegraphics[width=0.275\textwidth]{figs/logits/LS2_v0_train_bold.png}}
\subfigure[Label smoothing .]{\includegraphics[width=0.275\textwidth]{figs/logits/LS7_v0_train_bold.png}}    
    \caption{Effect of label smoothing on pre-logits (penultimate layer output) under label noise.
    Here, we visualise the pre-logits of a \resnetT{} for three classes on \cifarH{}, using the procedure of~\citet{Muller:2019}.
    The black markers denote instances which have been labeled incorrectly due to noise.
    Smoothing is seen to have a denoising effect: the noisy instances' pre-logits become more uniform, and so the model learns to not be overly confident in their label.}
    \label{fig:smoothing_logits}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \subfigure[Label smoothing.]{\includegraphics[width=0.35\textwidth]{figs/smoothing_2d_asymmetric_noise}}
    \subfigure[ regulariser.]{\includegraphics[width=0.35\textwidth]{figs/shrinkage_2d_asymmetric_noise.pdf}}
    \caption{(a) Effect of label smoothing on logistic regression separator,
    on a synthetic problem with asymmetric label noise.
    The black line is the Bayes-optimal separator, found by logistic regression on the clean data.
    The other lines are separators learned by applying label smoothing with various  on the noisy data.
    Without smoothing, noise draws the separator towards the affected class;
    smoothing undoes this effect, and brings the separator back to the Bayes-optimal. 
    (b) Shrinkage () regularisation has a similar effect on the separator.}
    \label{fig:synth_asymmetric_noise}
\end{figure*}

\noindent \textbf{Summary}:
Overall, our results demonstrate that 
label smoothing is competitive with loss correction techniques in coping with label noise,
and that it is particularly successful in denoising examples
while preserving calibration.


















