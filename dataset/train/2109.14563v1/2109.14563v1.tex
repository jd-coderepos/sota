\documentclass{article}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 


\usepackage{arxiv}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cuted}

\newcommand{\infdiv}{\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclareMathOperator{\diag}{diag}
\renewcommand{\KL}[2]{\text{KL}(#1~\Vert~#2)}
\DeclareMathOperator{\ECR}{ECR}
\DeclareMathOperator{\JSD}{JSD}
\DeclareMathOperator{\RTE}{RTE}
\renewcommand{\R}{\mathbb{R}}
\renewcommand{\b}[1]{\textbf{#1}}
\renewcommand{\i}[1]{\textit{#1}}
\newcommand{\blue}[1]{{\color{blue} #1}}

\title{Robust Temporal Ensembling for Learning with Noisy Labels}



\author{Abel Brown \\
  NVIDIA \\
  \texttt{abelb@nvidia.com} \\
  \And
  Benedikt Schifferer \\
  NVIDIA \\
  \texttt{bschifferer@nvidia.com} \\
  \And
  Robert DiPietro \\
  NVIDIA \\
  \texttt{rdipietro@nvidia.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Successful training of deep neural networks with noisy labels is an essential capability as most real-world datasets contain some amount of mislabeled data.  Left unmitigated, label noise can sharply degrade typical supervised learning approaches.  In this paper, we present \emph{robust temporal ensembling} (RTE), which combines robust loss with semi-supervised regularization methods to achieve noise-robust learning.  We demonstrate that RTE achieves state-of-the-art performance across the CIFAR-10, CIFAR-100, ImageNet, WebVision, and Food-101N datasets, while forgoing the recent trend of label filtering and/or fixing. Finally, we show that RTE also retains competitive corruption robustness to unforeseen \emph{input} noise using CIFAR-10-C, obtaining a mean corruption error (mCE) of 13.50\% even in the presence of an 80\% noise ratio, versus 26.9\% mCE with standard methods on clean data.
\end{abstract}


\section{Introduction}

Deep neural networks have enjoyed considerable success across a variety of domains, and in particular computer vision, where the common theme is that more labeled training data yields improved model performance \cite{2017arXiv171200409H, 2018arXiv180500932M, 2019arXiv191104252X, 2019arXiv191211370K}.  However, performance depends on the quality of the training data, which is expensive to collect and inevitably imperfect. For example, ImageNet \cite{deng2009} is one of the most widely-used datasets in the field of deep learning and despite over 2 years of labor from more than 49,000 human annotators across 167 countries, it still contains erroneous and ambiguous labels \cite{li2017, karpathy2014}. It is therefore essential that learning algorithms in production workflows leverage noise robust methods.

Noise robust learning has a long history and takes many forms \cite{NIPS2013_5073,6685834,2020arXiv200708199S}.  Common strategies include loss correction and reweighting \cite{patrini2016,zhang2018,Menon2020Can}, label refurbishment \cite{reed2014,song2019}, abstention \cite{thulasidasan2019}, and relying on carefully constructed \emph{trusted} subsets of human-verified labeled data \cite{liiccv2017,NIPS2018_8246,zhang2020}.  Additionally, recent methods such as SELF \cite{nguyen2020} and DivideMix \cite{li2020} convert the problem of learning with noise into a semi-supervised learning approach by splitting the corrupted training set into clean labeled data and noisy unlabeled data at which point semi-supervised learning methods such as Mean Teacher \cite{tarvainen2017} and MixMatch \cite{48557} can be applied directly.  In essence, these methods effectively discard a majority of the label information so as to side-step having to learning with noise at all.  The problem here is that noisy label filtering tactics are imperfect resulting in corrupted data in the small labeled partition and valuable clean samples lost to the large pool of unlabeled data.  Moreover, caution is needed when applying semi-supervised methods where the labeled data is not sampled i.i.d. from the pool of unlabeled data \cite{NEURIPS2018_c1fea270}.  Indeed, filtering tactics can be biased and irregular, driven by specification error and the underlying noise process of the label corruption.  Recognizing the success of semi-supervised approaches, we ask: can we leverage the underlying mechanisms of semi-supervised learning such as entropy regularization for learning with noise without discarding our most valuable asset, the labels? 



\section{Robust Temporal Ensembling}
\subsection{Preliminaries}
\label{sec:prelim}
Adopting the notation of \cite{zhang2018}, we consider the problem of classification where  is the feature space and  is the label space where the classifier function is a deep neural network with a softmax output layer that maps input features to distributions over labels .  The dataset of training examples containing \emph{in-sample} noise is defined as  where  and  is the noisy version of the true label  such that .  We do not consider \emph{open-set} noise \cite{wang2018}, in which there is a particular type of noise that occurs on inputs, , rather than labels.  Following most prior work, we make the simplifying assumption that the noise is conditionally independent of the input, , given the true labels. In this setting, we can write  which is, in general, considered to be \emph{class dependent} noise\footnote{See \cite{lee2019} for treatment of conditionally dependent \emph{semantic} noise such that .}\footnote{Note that \cite{patrini2016} define the noise transition matrix  such that .}.

To aid in a simple and precise corruption procedure, we now depart from traditional notation and further decompose  as , where  is the probability of corruption of the -th class and  is the relative probability that corrupted samples of class  are labeled as class , with ,  and .  A noisy dataset with  classes can then be described as transition probabilities specified by

where  defines the system confusion or noise structure,  defines the noise intensity or ratio for each class, and  is the identity matrix.  When  the noise is said to be \emph{symmetric} and is considered \emph{asymmetric} otherwise. If ratio of noise is the same for all classes then  and the dataset is said to exhibit \emph{uniform} noise.  For the case of uniform noise, equation (\ref{eqn:F}) interestingly takes the familiar form of the Google matrix equation \cite{RevModPhys.87.1261} as

Note that, by this definition,  which prohibits .  This ensures a true effective noise ratio of .  For example, suppose there are  classes and we wish to corrupt labels with 80\% probability. Then if corrupted labels are sampled from  rather than ,  of the corrupted samples will not actually be corrupted, leading to a \emph{true} corruption rate of 72\%.  Therefore, despite prescribing , the true effective noise ratio would be , which in turn yields a  increase in clean labels, and this is indeed the case in many studies \cite{zhang2018,nguyen2020,li2020, zhang2020}.

\subsection{Methods}
\label{sec:methods}
Cross entropy based loss can achieve noise-robust properties by using a Box-Cox power transform to stabilize loss variance which can be shown to be a form of maximum likelihood estimation (MLE) \cite{ferrari2010}.  Additionally, pseudo-labeling \cite{10.2307/2285824} can be shown to be a form of entropy regularization \cite{Grandvalet2005} which in the framework of maximum a posterior (MAP) estimation encourages low-density separation between classes by minimizing the conditional entropy of the class probabilities of the noisy data \cite{Lee2013}.  That is, by minimizing entropy, the overlap of class probability distribution can be reduced.  The implicit assumption here is that classes are, in fact, well separated \cite{Chapelle2005}.  Moreover, MAP estimation itself acts as a regularization of MLE by incorporating a priori knowledge of related training examples in order to solve the ill-posed noisy learning objective and further prevent overfitting.  Indeed, entropy regularization is favorable in situations for which the joint distribution, , is mis-specified \cite{Grandvalet2005} which further underpins the motivation of pseudo-labeling as an apt basis for regularization. 


A noise-robust task loss is leveraged which can be seen as a generalization of mean absolute error (MAE) and categorical cross entropy (CCE) \cite{zhang2018}.  The idea is that CCE learns quickly, but more emphasis is put on difficult samples which is prone to overfit noisy labels, while MAE treats all samples equally, providing noise-robustness but learning slowly.  To exploit the benefits of both MAE and CCE, a negative Box-Cox transformation \cite{10.2307/2984418} is used to stabilize the loss variance as
  
where , and  denotes the -th element of .  Note that this loss becomes CCE for  and becomes MAE/unhinged loss when .

Consistency regularization works under the assumption that a model should output similar predictions given augmented versions of the same input. This regularization strategy is a common component of semi-supervised learning algorithms with the general form of  where  is the predicted class distribution produced by the model having parameters  for input  \cite{7780854, sajjadi2016}.  We build upon numerous variations from semi-supervised learning \cite{laine2017, tarvainen2017, 48557, Berthelot2020ReMixMatch:} and leverage an ensemble consistency regularization (ECR) strategy as

where  is the training example,  is stochastic augmentation function reevaluated for each term in the summation,  is a temporal moving average of model weights used to generate pseudo-label targets, and inputs are pre-processed with standard random horizontal flip and crop.  In practice, this consists of initializing a copy of the initial model and maintaining an exponential moving average as training progresses.  Some methods directly average multiple label predictions together at each optimization step to form a single pseudo-label target \cite{48557,li2020} but we find pseudo-label target distributions generated by  to be better suited for the learning with noise problem due to the intrinsic ensemble nature of the weight averaging process over many optimization steps \cite{tarvainen2017}.  In semi-supervised learning techniques, it is common to leverage a large batch-size of unlabeled data for consistency regularization.  However, we found that modulating , rather than the batch size of the consistency term, yields a monotonic increase in model performance consistent with related works \cite{Berthelot2020ReMixMatch:}.  Moreover, in semi-supervised learning, different batches are used for supervised and unsupervised loss terms but we find (see Section \ref{sec:ablation}) that for the case of learning with noise, batches synchronized with task loss term yields superior performance. 

The Jensen-Shannon consistency loss is used to enforce a flat response of the classifier by incentivizing the model to be stable, consistent, and insensitive across a diverse range of inputs \cite{7780854}. The Jensen-Shannon divergence (JSD) is minimized across distributions , , and  of the original sample  and its augmented variants  and  which can be understood to measure the average information that the sample reveals about the identity of its originating distribution \cite{hendrycks2020augmix}.  This JSD term is computed with  and is then

where  is Kullbackâ€“Leibler divergence from  to .  In this way, the JSD term improves the stability of training in the presence of noisy labels and heavy data augmentation with a modest contribution to final classifier test accuracy as shown in Table \ref{tab:ablation_loo}. 


\subsection{Putting It All Together}
We unify the various components defined in sections \ref{sec:methods} together under a single parsimonious loss function at training defined as

where the JSD term is synchronized with ECR by computing the clean distribution using . Final performance is reported using .  In practice we find AugMix \cite{hendrycks2020augmix} to be most performant at high levels of label noise as AugMix layers together several stochastically sampled augmentation chains in a convex combination which mitigates input degradation but also generates highly diverse transformations.  Because the ECR loss term is based on the Mean Squared Error between the probability predictions, its depends on the number of classes of the dataset since the average is calculated by the squared error per class. As we sum GCE, JSD and ECR terms, the weights  and  are adjusted so that associated loss terms have similar magnitudes.

Here, data augmentation serves dual purpose as a generic regularizer to mitigate over-fitting of noisy labels \cite{zhang2018mixup} as well as provides additional information about the vicinity or neighborhood of the training examples which is formalized by Vicinal Risk Minimization \cite{NIPS2000_ba9a56ce}. These augmented examples can be seen as drawn from a vicinity distribution of the training examples to enlarge support of the training distribution such that samples in the vicinity share the same class but does not model the relation across examples of different classes \cite{zhang2018mixup}.  Therefore, data augmentations approximate samples of nearby elements of the data manifold where the difference, , approximates elements of its tangent space \cite{athiwaratkun2018there}.  In this way, the ECR term can loosely be seen as generating a set of stochastic differential constraints at each optimization step of the classification task loss.  While stronger augmentation can enrich the vicinity distribution, augmentation methods such as MixUp \cite{zhang2018mixup} and RandAugment \cite{DBLP:conf/cvpr/CubukZSL20} can overly degrade training examples and drift off the data manifold \cite{hendrycks2020augmix}.  When learning with noise, it is therefore essential to leverage an augmentation process rich in variety but which also preserve the image semantics and local statistics so as to minimize the additional strain on an already ill-posed noisy learning objective.  Consistent with this understanding, although RandAugment has been successfully leveraged in semi-supervised learning \cite{Berthelot2020ReMixMatch:,49534,2019arXiv190412848X}, our experiments with RandAugment proved unsuccessful for extreme levels of label noise (Table \ref{tab:ablation_loo}).



\section{Related Work}

Some methods for learning with noise attempt to improve noisy learning performance head-on by leveraging augmentation as a strong regularizer to mitigate memorization of corrupted labels \cite{zhang2018mixup} while others attempt to refurbish corrupted labels to control the accumulation of noise from mislabeled data \cite{song2019}.  A recent theme in learning with noisy labels has been to transform the learning with noise problem into a semi-supervised one by removing the labels of training data determined to be corrupted to form the requisite dichotomy of clean labeled data and a pool of unlabeled data \cite{nguyen2020,li2020}; then directly applying semi-supervised approaches such as MixMatch \cite{48557} and MeanTeacher \cite{tarvainen2017}.  Other methods go so far as to require trusted human verified data and combine re-weighting with re-labeling into a meta optimization approach \cite{zhang2020}.  

Semi-supervised learning algorithms have advanced considerably in recent years, making heavy use of both data augmentation and consistency regularization.  MixMatch \cite{48557} proposed a low-entropy label-guessing approach for augmented unlabeled data and mixes labeled and unlabeled data using MixUp.  In MixMatch, pseudo-label targets are formed by averaging label distributions produce by the model on samples drawn from the vicinity distribution ().  However, this averaging requires artificial sharpening to generate low-entropy pseudo-labels.  From the MAP estimation perspective, sharpening does not add auxiliary a priori knowledge for the optimization step but rather prescribes a desirable property of the model generated label distribution.  Indeed, our experiments with the use of artificial label sharpening in RTE resulted in failed training at high levels of label noise and subsequent related work recognized that stronger augmentations can result in disparate predictions so their average may not generate meaningful targets \cite{Berthelot2020ReMixMatch:}.  ReMixMatch \cite{Berthelot2020ReMixMatch:} introduced augmentation anchoring which aims to minimize the entropy between label distributions produced by multiple weak and strong data augmentations of unlabeled data using a control theory augmentation approach.  While pseudo-label guessing and augmentation anchoring motivate the utility of multiple augmentations of the same data, our proposed ECR for learning with noise differs in the following important ways: ECR does not use distribution alignment for ``fairness'', distribution averaging, or label-sharpening; ECR forms pseudo-label targets using an exponential average of model weights and is batch-synchronized with the task loss term.  Finally, the recent work, FixMatch \cite{49534}, proposes a simplified semi-supervised approach where the consistency regularization term uses hard pseudo-labeling for low-entropy targets together with a filtering step to remove low-confidence unlabeled examples but does not leverage multiple strong augmentations.



\section{Experiments}
\label{sec:expt}
In this section we analyze the performance of RTE against various uniform noise configurations for both symmetric and asymmetric settings, and against real-world label noise.  For asymmetric noise, we test both the traditional configuration \cite{patrini2016}, typically reported by related works, and an additional configuration defined by (\ref{eqn:c_asym_rte}) which is in the spirit of \cite{lee2019}, where we define the asymmetric noise structure using the confusion matrix of a trained shallow network.  In all of these experiments, RTE outperforms existing methods.  Finally, we perform additional ablation studies to better understand the contribution and synergy of the terms in equation (\ref{eqn:rteloss}) as well as to probe the efficacy of ECR.

In our first set of experiments we consider the standard CIFAR-10, CIFAR-100, and ImageNet datasets \cite{Krizhevsky2009LearningML,deng2009}. CIFAR-10 and CIFAR-100 each contain 50,000 training and 10,000 test images of 10 and 100 classes, respectively; and ImageNet contains approximately 1,000,000 training images and 50,000 validation images of 1000 classes. Additionally, we test networks trained with noisy labels against unforeseen input corruptions using CIFAR-10-C \cite{hendrycks2019robustness} which was constructed by corrupting the original CIFAR-10 test set with a total of 15 noise, blur, weather, and digital corruptions under different severity levels and intensities.  Classifier performance is averaged across these corruption types and severity levels to yield \emph{mean corruption error} (mCE).  Since CIFAR-10-C is used to measure network behavior under data shift, these 15 corruptions are not included during the training procedure.  Here, CIFAR-10-C helps to establish a rigorous benchmark for image classifier robustness to better understand how models trained with noisy data might perform in safety-critical applications.

To mitigate the sensitivity of experimental results to empirical, and perhaps arbitrary, choices of hyperparameters, we present additional results that leverage Population Based Training (PBT) \cite{2017arXiv171109846J, 2019arXiv190201894L} which is a simple asynchronous optimisation algorithm that jointly optimize a population of models and their hyperparameters.  In particular, PBT discovers a per-epoch \emph{schedule} of hyperparameter settings rather than a static fixed configuration used over the entirety of training.  These PBT schedules, for example, allow task loss  to vary between CE and MAE loss dynamically during training and similarly the number of ECR terms  can be modulated to realize a form of curriculum learning.  Moreover, for our purposes, PBT schedules also provide a form of quasi-ablation study, as optimization of the task-loss parameter , the number of ECR terms , and the ECR weight  allows for the realization of a simplified loss which forgos these components if determined maximally beneficial.  We find, as in other studies, that this joint optimization of hyperparameter schedules typically results in faster wall-clock convergence and higher final performance. \cite{2019arXiv190505393H,2019arXiv190201894L}.

\subsection{Uniform Symmetric Noise}
\label{sec:expt:us}
\textbf{Training Setup.} Training details can be found in Section \ref{appendix:uniformsetup} of the Supplementary Material.

\textbf{Baselines}. To best interpret the effectiveness of RTE, we compare our results to many techniques for learning with noise (Table \ref{tab:uniformnoise}). A description of each baseline method can be found in Section \ref{appendix:baselines} in the Supplementary Material.  Only two of these references provide ImageNet results trained with label noise (Table \ref{tab:uniformnoiseimagenet}).

\textbf{Results}. Experimental results with uniform symmetric noise for both CIFAR-10 and CIFAR-100 are presented in Table \ref{tab:uniformnoise} with comparisons to related work, including current state-of-the-art methods. RTE establishes new state-of-the-art performance at all noise levels and exhibits especially large performance gaps at high noise levels. At 80\% noise, previous state-of-the-art was achieved by \cite{arazo2019} in the case of CIFAR-10 and by \cite{li2020} in the case of CIFAR-100. RTE improves performance over these methods by 7.0 absolute percentage points and 6.2 absolute percentage points, respectively. Of all of these works, only two report results on ImageNet training with noisy labels. These are included alongside RTE results in Table \ref{tab:uniformnoiseimagenet}, where once again we see that RTE performs favorably, improving state-of-the-art performance in terms of both top-1 accuracy and top-5 accuracy. As in \cite{arazo2019} and \cite{li2020}, we also include loss distributions over clean and corrupt labels in Figure \ref{fig:loss-correct-vs-corrupt}. Here we can see that RTE prevents rote memorization of noisy labels. Moreover, Table \ref{tab:mce} shows that RTE retains strong corruption robustness with an mCE of 12.05\% and 13.50\% at noise ratios of 40\% and 80\% respectively, as measured using CIFAR-10-C.  Put in context, experiments summarized in Table \ref{tab:mce}  indicate that even with extreme levels of mislabeled training data, RTE trained models have lower corruption error than models trained using standard methods using clean data.
\begin{table*}[!htbp]
\renewcommand{\arraystretch}{1.3}
\centering
\caption{Test accuracy on CIFAR-10 and CIFAR-100 under uniform symmetric label noise. Results in parentheses are upper bounds since they were computed using lower noise levels (see sect. \ref{sec:prelim} for discussion). Note, our GCE-only results use true noise of 80\%, rather than the 72\% from the original GCE paper \cite{zhang2018}.  The results for Reed-Hard, S-Model \cite{goldberger2016}, Forward T and Co-Teaching are from \cite{nguyen2020} and the results for MixUp and Meta-Learning are from \cite{li2020}.  Finally, Polulation Based Training (PBT, see sect. \ref{sec:expt} for discussion) was used \emph{only} for RTE (PBT) experiments. That is, all non-CIFAR experiments, as well as the 'manual' CIFAR experiments, including baseline configurations, were performed \emph{without} PBT. The configurations for RTE (manual) and alternative configurations based on \cite{Berthelot2020ReMixMatch:} and \cite{hendrycks2020augmix}.  RTE provides better robustness to label noise than all other methods. Higher is better.}\label{tab:uniformnoise}
\begin{tabular}{llllllll}
\toprule
Method                           &           &\multicolumn{3}{l}{CIFAR-10}    & \multicolumn{3}{l}{CIFAR-100}     \\
\hline
                                 & \# Params & \multicolumn{3}{l}{Noise Ratio} & \multicolumn{3}{l}{Noise Ratio} \\
                                 &           & 0\% & 40\%         & 80\%    &0\%      & 40\%           & 80\%        \\
\emph{(Prior Work)} \1ex]
\emph{(Our Work)} \1ex]
CE-only                          &13.1M&& 90.06  & 59.66 && 65.98   & 35.80 \\
GCE-only                         &13.1M&& 91.35  & 59.15 && 69.73   & 39.19 \\
CE+JSD+ECR                       &13.1M&& 95.45	& 76.08 && 71.89   & 40.43 \\ 
\hline
\emph{(Alternative RTE Configuration)} \-2ex]
    CIFAR-10  & .001      & .9      & .01     & 128 &      & 12, 1     &  & 10    & .99 \\
    CIFAR-100 & .0005     & .9      & .01     & 128 & 0.04                              & 5, 3      & 0.3                           & 8     & .99 \\
    ImageNet  & .001      & .9      & .00     & 256 &                  & 12, 10    & 0.3                           & 3     & .99 \\
    WebVision & .0001     & .9      & .00     & 256 &                               & 12, 5     & 0.3                           & 3     & .99 \\
    Food-101N & 0.01      & .9      & .00     & 128 &                  & 12, 1     & 0.1                           & 2     & .99 \\
    \bottomrule
    \end{tabular}
    \label{tab:manual_config}
\end{table*}



\section{Baselines for Table \ref{tab:uniformnoise}}
\label{appendix:baselines}
In this section we provide a brief summary of the baseline methods in the main text:

\b{\cite{reed2014}} introduce two methods for achieving prediction consistency, one based on reconstruction and one based on bootstrapping, and demonstrated empirically that bootstrapping leads to better robustness to label noise.
\b{\cite{goldberger2016}} model the correct label as latent and having gone through a parameterized corruption process. Expectation maximization is used to estimate both the parameters of the corruption process and the underlying latent label.
\b{\cite{jiang2018}} introduce the idea of \emph{learning} a curriculum-learning strategy with a \emph{mentor} model to train a \emph{student} model to be robust to label noise.
\b{\cite{patrini2016}} estimate the noise transition matrix (under the assumption of feature independent noise) and show that, given the \emph{true} noise transition matrix, optimizing for the true underlying labels is possible.
\b{\cite{wang2018}} introduce an iterative scheme that combines 1. outlier detection in feature space (acting as a proxy to noisy-label detection), 2. a Siamese network (taking either a clean, clean pair or a clean, noisy pair) to encourage separation, and 3. sample reweighting based on clean vs. noisy confidence levels in order to effectively filter out noisy labels during training. They focus primarily on \emph{open-set} noise, but they also report performance of their system when used in the \emph{closed-set} setting.
\b{\cite{ren2018}} use a meta-learning approach to dynamically weight examples to minimize loss \emph{using a set of validation examples with clean labels}, however they also report a competitive baseline using a randomized weighting scheme which requires no clean validation set.
\b{\cite{jenni2018}} formulate example weighting as a bilevel-optimization problem, in which performance on a validation set is maximized with respect to example weights, subject to the constraint that the model maximizes performance on the training set; and they argue that this approach should lead to better generalization when label noise is present. 
\b{\cite{zhang2018}} introduce a loss function that is a generalization of cross-entropy loss and mean absolute error, which is beneficial since each exhibits distinct desirable properties: cross-entropy exhibits better gradient properties for learning, while mean absolute error exhibits better theoretically-grounded robustness to noisy labels. 
\b{\cite{han2018}} leverage co-teaching such that two networks are trained together, in which each network 1. identifies high-confidence examples, 2. passes this information in a message to its peer, and 3. leverages the incoming message to optimize using the examples selected by its peer.
\b{\cite{zhang2018mixup}} train using convex combinations of both input images and their labels, arguing that this approach makes it more difficult for the network to memorize corrupt labels.
\b{\cite{song2019}} measure label consistency throughout training in order to determine which samples are `refurbishable', and these samples are then `corrected' by replacing their ground-truth label with the most frequently-predicted label.
\b{\cite{lee2019}} do not modify the training process of the underlying neural network but instead form a generative model over the final (pre-softmax) features of the neural network, and this generative distribution along with Bayes rule is then used to estimate a more robust conditional distribution over the label.
\b{\cite{arazo2019}} fit a beta mixture model over the \emph{loss} using two mixture components, representing \emph{clean} and \emph{noisy} labels, and each sample's underlying component probabilities are used to weight each sample's contribution during training. They combine this approach with MixUp \cite{zhang2018mixup}.
\b{\cite{yi2019}} maintain a direct estimate of a distribution over true underlying labels during training, and train the parameters of a neural network by minimizing reverse KL divergence (from the model's predicted distribution to these true-label estimates). Meanwhile a `compatibility loss' is introduced to ensure that the estimated label distribution stays close to the noisy labels provided with the training set.
\b{\cite{li2019}} subject a student model to artificial label noise during training and take alternating gradient steps and maintain a teacher model that is not subjected to such noise. Here, alternating gradient steps are taken to 1. minimize classification loss and 2. minimize the KL divergence from the student's predicted distributions to the teacher's predicted distributions.
\b{\cite{nguyen2020}} use discrepancy between an ensemble-based teacher model and labels to identify and filter out incorrect labels, and continue to leverage these samples without the labels. This is done in a semi-supervised fashion by maintaining consistency between the teacher's predictions and the student's predictions.
\b{\cite{li2020}} maintain two networks and for each network models \emph{loss} using a mixture of Gaussians with two components (\emph{clean} and \emph{noisy}). Each network estimates which samples belong to each component, and the \emph{other} network then uses the \emph{clean} samples in a supervised manner along with the \emph{noisy} labels in a semi-supervised manner.



\section{Uniform Symmetric Noise Experimental Setup}
\label{appendix:uniformsetup}

For CIFAR-10, we leverage equation (\ref{eqn:Fu}) with  and we employ a 28-layer residual network \cite{He_2016_CVPR} with a widening factor of 6 (WNR 28x6) \cite{2016arXiv160507146Z}, a dropout rate of 0.01 \cite{JMLR:v15:srivastava14a}, , AugMix with a mixture width and severity of 3, a batch size of 128, and 300 epochs of training. We optimize using SGD with Nesterov momentum of 0.9 \cite{nesterov}, a weight decay of 0.001, and a cosine learning rate \cite{sgdr2017:} of , where  is the current training step and  is the total number of training steps. The RTE loss function (\ref{eqn:rteloss}) is configured with static ,  and  of 12, 1, and 10, respectively, whereas  is scheduled according to  (which assigns small -values in early training epochs, reaches a maximum of  after 180 epochs, and decreases to  over the remaining 120 epochs). For CIFAR-100, the setup is similar, but different hyperparameters are used; details are included in the Appendix in Table \ref{tab:manual_config}. In addition to manual configurations, we consider PBT with a population size of 35 to optimize learning rate, weight decay, , ,  and .  Fastidious readers will find the complete PBT configuration defined in Appendix \ref{PBThyper}. For ImageNet, ResNet50 is used and trained with SGD for 300 epochs with a stepped learning rate of 0.1, 0.01 and 0.001 which begin at epochs 0, 100 and 200 respectively. ImageNet hyperparameters are also included in the Appendix in Table \ref{tab:manual_config}.



\section{Confusion Matrix for Uniform Asymmetric Noise}
\label{appendix:asymsetup}

The confusion matrix for uniform asymmetric noise is given in Equation \ref{eqn:c_asym_rte}.

\begin{table*}


\end{table*}



\section{PBT Experiments}
\label{PBThyper}

PBT sampling configurations are shown in Table \ref{table_pbtconfiguration}, and parameter schedules are shown in Figure \ref{pbt_all_schedules}.

\begin{table}
\centering
\caption{PBT sampling configuration for CIFAR-10 and CIFAR-100. We used a population size of 35, and permutation interval of 2 epochs. In the case a member inherits another checkpoint, each hyperparameter is resampled from its distribution with  or is multiplied with  within its parameter range with . In the case of , the previous/next hyperparameter from the ordered list is selected.}
\begin{tabular}{ll}
\toprule
Parameter    & Sample distribution \\
\midrule
Batch size        & 128                    \\
Dropout           & 0.01                    \\
             & 0.9                    \\
            & 0.99                    \\
LR                & Uniform(0.00001, 0.1)                     \\
weight decay      & Uniform(0.00005, 0.002)                    \\
               & Uniform(0.0, 1.0)                 \\
    & Uniform(0.0, 20.0)                    \\
    & Uniform(0.0, 5.0)                    \\
             & Uniform\{3, ..., 10\}  \\
\bottomrule
\label{table_pbtconfiguration}
\end{tabular}
\end{table}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{all_schedules_v2.png}
  \caption{Parameter schedules for , , JSD weight , ECR weight ,  and  for CIFAR-10 and CIFAR-100 with 40\% and 80\% uniform symmetric noise rates.}
  \label{pbt_all_schedules}
\end{figure*}



\section{Appendix: Uniform Asymmetric Noise on CIFAR-10}
\label{appendix:asym}

The matrix  in Equation \ref{eqn:c_asym_patrini} defines the noise structure for uniform asymmetric noise on CIFAR-10 with following labels: AIRPLANE, AUTOMOBILE, BIRD, CAT, DEER, DOG, FROG, HORSE, SHIP, TRUCK.

Class distributions are shown in Table \ref{tab:c_asym_sample_60}.

\begin{table*}

\end{table*}

\label{sec:c_asym_sample_60}
\begin{table*}[h]
\renewcommand{\arraystretch}{1.3}
\centering
  \caption{Overview of class distribution of total and correct labels after sampling noisy CIFAR-10 training labels with asymmetric noise defined by equation (\ref{eqn:c_asym_rte}) with a uniform 60\% noise ratio.\newline}
  
\begin{tabular}{lrrrr}
\toprule
          & \multicolumn{1}{l}{\# samples} & \multicolumn{1}{l}{\% samples} & \multicolumn{1}{l}{\# correct labels} & \multicolumn{1}{l}{\% correct labels} \\ \hline
\small{AIRPLANE}   & 5578                           & 11\%                           & 1958                                  & 35\%                                  \\
\small{AUTOMOBILE} & 4069                           & 8\%                            & 2003                                  & 49\%                                  \\
\small{BIRD}       & 6023                           & 12\%                           & 2017                                  & 33\%                                  \\
\small{CAT}        & 6205                           & 12\%                           & 2038                                  & 33\%                                  \\
\small{DEER}       & 5056                           & 10\%                           & 1986                                  & 39\%                                  \\
\small{DOG}        & 4480                           & 9\%                            & 1977                                  & 44\%                                  \\
\small{FROG}       & 5476                           & 11\%                           & 2019                                  & 37\%                                  \\
\small{HORSE}      & 4130                           & 8\%                            & 2028                                  & 49\%                                  \\
\small{SHIP}       & 3896                           & 8\%                            & 2024                                  & 52\%                                  \\
\small{TRUCK}      & 5087                           & 10\%                           & 1950                                  & 38\%                                  \\ \hline
TOTAL      & 50000                          & 100\%                          & 20000                                 & 40\%                                  \\ 
\bottomrule
\end{tabular}
\label{tab:c_asym_sample_60}
\end{table*}



\section{Appendix: Extended Data and Analysis}
\label{sec:ablation_more_data}

In Tables \ref{tab:fig1_left} and \ref{tab:fig1_right} we include test accuracy and mean corruption error on CIFAR-10 and CIFAR-10-C. In Figure \ref{fig:reliability-diagrams}, we include reliability diagrams using CIFAR-10.

\begin{table*}[h]
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption{RTE test accuracy and mean corruption error (mCE) on CIFAR-10 and CIFAR-10-C, respectively.  In this experiment, fixed batch size of  is used and the number of ECR terms,  is varied.  Training configuration of these data is described in section \ref{sec:expt:us}.  Test accuracy is presented in Figure \ref{fig:ecr_term_batch_size} (left).}\begin{tabular}{lcccccccc}
        \toprule
        &\multicolumn{8}{l}{CIFAR-10} \\
        &\multicolumn{8}{l}{Fixed batch-size: 128} \\
        &\multicolumn{8}{l}{Uniform Symmetric Noise: 80\%} \\
        \hline
        &\multicolumn{8}{l}{Vary the number of ECR terms: } \\
         & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
         \hline
          Test Acc & 91.51 & 91.90 & 92.57 & 92.65 & 92.77  & 93.14 & 93.09 & 93.21 \\
          mCE & 15.32 & 14.87 & 13.74 & 13.90 & 13.84 & 13.48 & 13.67 & 13.66 \\
         \bottomrule
    \end{tabular}
    \label{tab:fig1_left}
\end{table*}

\begin{table*}
    \renewcommand{\arraystretch}{1.3}
    \centering
    \caption{RTE test accuracy and mean corruption error (mCE) on CIFAR-10 and CIFAR-10-C, respectively.  In this experiment a single consistency loss term is used and vary the batch size of that term. This experiment with varying batch size is analogous to a more traditional semi-supervised approach where large batch size is used for unsupervised loss terms.  Training configuration for these data is described in section \ref{sec:expt:us}.  Test accuracy is presented in Figure \ref{fig:ecr_term_batch_size} (right).}\begin{tabular}{lcccccc}
        \\
        \toprule
        &\multicolumn{6}{l}{CIFAR-10} \\
        &\multicolumn{6}{l}{Fixed ECR terms: } \\
        &\multicolumn{6}{l}{Uniform Symmetric Noise: 80\%} \\
        \hline
        &\multicolumn{6}{l}{Vary the batch size:} \\
         & 32 & 64 & 128 & 256 & 512 & 1024  \\
         \hline
          Test Acc & 86.54 & 88.95 & 90.32 & 88.46 & 85.87 & 78.13 \\
          mCE & 19.77 & 17.78 & 16.41 & 18.20 & 20.42 & 28.57 \\
         \bottomrule
    \end{tabular}
    \label{tab:fig1_right}
\end{table*}

\begin{figure*}
  \centering
  \includegraphics[width=2.5in]{reliability-diagram-40.png}
  \includegraphics[width=2.5in]{reliability-diagram-80.png}
  \caption{Reliability diagrams for RTE training models on CIFAR-10 with 40\% uniform label noise (left) and 80\% label noise (right). Perfectly calibrated models follow the black line, whereas over-confident models lie below and under-confident models lie above. This figure indicates our RTE trained model is well calibrated when trained with 40\% label noise, while (perhaps justifiably) conservative when trained with a more extreme level of 80\% label noise.}
  \label{fig:reliability-diagrams}
\end{figure*}



\section{Appendix: Compute Resources}

We used an internal cluster of NVIDIA V100s for all experiments. We estimate that all experiments across all datasets (CIFAR-10, CIFAR-100, ImageNet, WebVision, and Food-101N) required approximately 2,000 GPU hours.



\end{document}
