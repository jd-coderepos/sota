
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{iccv} 


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{pifont}

\usepackage{multirow}
\usepackage{verbatim}
\usepackage{color}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabulary,multirow,overpic,xcolor}

\usepackage{pifont}\usepackage{epstopdf}
\epstopdfsetup{update} 

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{enumerate}

\usepackage{bm}
\usepackage{t1enc}
\usepackage{colortbl}
\usepackage{cite}
\usepackage{soul}

\usepackage{arydshln}


\newcommand{\kk}[1]{{\textcolor{magenta}{[kk]: #1}}}
\newcommand{\anurag}[1]{{\textcolor{blue}{[Anurag: #1]}}}
\newcommand{\arsha}[1]{{\textcolor{red}{[Arsha: #1]}}}
\definecolor{pos}{RGB}{0,153,0}
\definecolor{neg}{RGB}{0,0,0}
\definecolor{smpos}{RGB}{0,0,0}
\definecolor{row}{RGB}{240,240,240}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\ve}[1]{\mathbf{#1}} \newcommand{\ma}[1]{\mathrm{#1}} \newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Sec.~\ref{#1}}

\newcommand{\ours}{\texttt{VicTR}}

\newcommand{\bd}[1]{\textbf{#1}}
\newcommand{\app}{\raise.17ex\hbox{}}
\def\x{}
\newcommand{\blocket}[4]{\multirow{3}{*}{-.1em] \text{3, #2}\#4}
}
\newcommand{\Tau}{\mathcal{T}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth\global\arrayrulewidth1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@} {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother

\addtolength{\floatsep}{-2mm}
\addtolength{\textfloatsep}{-1mm}
\addtolength{\intextsep}{-1mm}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\iccvPaperID{6445} \def\confName{ICCV}
\def\confYear{2023}


\begin{document}

\title{\ours: Video-conditioned Text Representations for Activity Recognition}

\author{Kumara Kahatapitiya\textsuperscript{1}\thanks{Work done as a student researcher at Google.}\hspace{1mm}, Anurag Arnab\textsuperscript{2}, Arsha Nagrani\textsuperscript{2} and Michael S. Ryoo\textsuperscript{1,2} \vspace{1mm}\\ 
\textsuperscript{1}Stony Brook University \hspace{2mm} \textsuperscript{2}Google Research\\
{\tt\small kkahatapitiy@cs.stonybrook.edu \{aarnab,anagrani,mryoo\}@google.com}
}

\maketitle

\begin{abstract}
   Vision-Language models have shown strong performance in the image-domain--- even in zero-shot settings, thanks to the availability of large amount of pretraining data (i.e., paired image-text examples). However for videos, such paired data is not as abundant. Thus, video-text models are usually designed by adapting pretrained image-text models to video-domain, instead of training from scratch. All such recipes rely on augmenting visual embeddings with temporal information (i.e., image  video), often keeping text embeddings unchanged or even being discarded. In this paper, we argue that such adapted video-text models can benefit more by augmenting text rather than visual information. We propose \ours, which jointly-optimizes text and video tokens, generating `Video-conditioned Text' embeddings. Our method can further make use of freely-available semantic information, in the form of visually-grounded auxiliary text (e.g., object or scene information). We conduct experiments on multiple benchmarks including supervised (Kinetics-400, Charades), zero-shot and few-shot (HMDB-51, UCF-101) settings, showing competitive performance on activity recognition based on video-text models. \end{abstract}


\section{Introduction}
\label{sec:intro}

Video understanding poses significant challenges, often adding to the complications in image domain such as model complexity and annotation costs. The extra temporal dimension and multiple modalities of data introduce useful cues, but also can be redundant, raising interesting questions about trade-offs. Activity Recognition (i.e., classification) in particular--- as the prominent task in video understanding--- has long been explored by the community in these directions. Whether it is efficient architecture variants ranging from CNNs \cite{lin2019tsm, feichtenhofer2020x3d, ryoo2019assemblenet} to Transformers \cite{arnab2021vivit, bertasius2021timesformer, fan2021mvit}, training schemes from fully-supervised \cite{carreira2017i3d, feichtenhofer2019slowfast} to self-supervised \cite{qian2021spatiotemporal, feichtenhofer2021large, recasens2021brave} or data regimes from unimodal \cite{xie2017s3d, tran2018r2p1d} to multimodal \cite{han2020self, nagrani2021bottleneck}, the progress has been steady and exciting. More recently, with the availability of internet-scale paired image-text data, the direction of vision-language models (VLMs) \cite{radford2021clip, jia2021align} have emerged dominant, achieving strong performance across multiple benchmarks. However, the progress of VLMs in the video domain is yet to be caught-up to its potential.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\linewidth]{concept_fig.pdf}
    \caption{\textbf{Conceptual view of \ours:} A pretrained image-text model (eg: CLIP \cite{radford2021clip}) can be used to extract video (by temporal-pooling frame embeddings) and text embeddings. However, such text embeddings which are \underline{common for all videos} lack the flexibility to get aligned well in a shared latent space, when optimized based on the similarity (i.e., \textit{Affinity}) w.r.t. all videos. In \ours, we introduce \textit{Video-conditioned Text} representations, which specialize uniquely for each video. When embeddings are optimized, video-text affinities change accordingly, enabling better classifiers.
    }
    \vspace{-2mm}
    \label{fig:concept}
\end{figure}
        
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{intro_fig.png}
    \caption{\textbf{Overview of \ours:} First, we extract image (i.e., video-frame) and text tokens using pretrained image-text encoders such as CLIP \cite{radford2021clip}. Next, such tokens go through a joint video-text encoder, generating video tokens and \textit{video-conditioned} text tokens, based on which, we compute affinity-based logits for classification. Optionally, any semantic category (given as auxiliary text) can also be processed in a similar fashion, and used for classification. This is motivated based on the co-occurrence of semantics (eg: \texttt{rope, gym, one-person}) and categories-of-interest, i.e., activity classes in our setting (eg: \texttt{rope climbing}). Note the colors and patterns of tokens that visualize video-conditioning.}
    \label{fig:overview}
    \vspace{-2mm}
\end{figure}

Following seminal work CLIP \cite{radford2021clip} and ALIGN \cite{jia2021align}, image-text models have made significant strides across tasks such as image classification \cite{yuan2021florence, zhai2022lit, yan2022videococa}, open-vocabulary object detection \cite{gu2021vild, minderer2022simple}, text-to-image retrieval \cite{singh2022flava, yao2021filip} and robot manipulation \cite{jiang2022vima, zeng2022socratic}. These models are usually pretrained on paired image-text data based on a contrastive learning framework. The idea is to have two separate backbones--- an Image Encoder and a Text Encoder, which generate embeddings in a joint-latent space. To optimize this latent space, the corresponding pairs of embeddings are drawn closer, by increasing their similarity (i.e., \textit{Affinity}). The key advantage of such models is that, at inference, any semantic concept (given as a text input) can be embedded in the same space, giving intriguing zero-shot or few-shot transfer capabilities \cite{zeng2022socratic, alayrac2022flamingo}. For instance, CLIP \cite{radford2021clip} excels at classifying unseen attribute categories (eg: objects, scenes, human-subjects), or even counting such occurrences \cite{zeng2022socratic}. However, it does not perform well in tasks that require specialized knowledge, such as localizing (eg: detection/segmentation) or temporal reasoning (eg: activity recognition), at least not out-of-the-box. This is because, the original CLIP objective has not been focused on any location or temporal cues during its training. With task specific finetuning however, such models can be readily adapted to specialized domains \cite{gu2021vild, ma2022xclip}.

In the video domain, training a VLM from scratch may show a limited success \cite{xu2021videoclip}--- despite being expensive, due to lack of paired data at scale. As a compromise, it has become common practice to adapt already-pretrained image-text models as video-text models, by introducing temporal information. Such methods follow one of two directions: either (1) insert temporal modules within the image backbone itself to have cross-frame interactions \cite{ma2022xclip}, or (2) have a post-processing video head on-top of the image backbone \cite{luo2022clip4clip, wang2021actionclip, bain2022cliphitchhiker, lin2022evl}. In both cases, image embeddings are enhanced as video embeddings. However, the use of text embeddings differs in each approach. They may either be (1) discarded \cite{lin2022evl}, (2) kept unchanged \cite{luo2022clip4clip, wang2021actionclip}, (3) used as conditioning \cite{bain2022cliphitchhiker} (to further enhance video embeddings as \textit{text-conditioned video}), or (4) fully-updated along with video \cite{ma2022xclip}. More often than not, the main focus is on visual embeddings (i.e., converting image  video), and the impact of updating text embeddings has been limited.

Moreover, video understanding has shown to benefit from semantic information \cite{ji2020action, zeng2022socratic, wang2022language}. In fact, certain attributes (eg: objects, scene or human subjects) are directly tied with specific actions, and can simplify activity recognition. For instance, if we see attributes such as \{\texttt{object:rope, scene:gym, human:one-person}\} in a given clip, it can narrow-down potential activities to ones such as \texttt{battling ropes} or \texttt{rope climbing}. Refer to the example in \fref{fig:overview}. VLMs are especially suited to take advantage of such semantics. Any concept given as an \textit{auxiliary} input to the text encoder can be grounded on the visual modality to filter prominent attributes. Such visually-grounded semantics are cheap in-terms of annotation and compute costs, but can be highly-useful.

Motivated by the above, we propose \ours,~which focuses more on adapting text information to the video domain. More specifically, we generate \textit{Video-conditioned Text} embeddings (refer \fref{fig:concept}), while jointly-training both visual-text features generated by CLIP \cite{radford2021clip} backbones. 
We observe that finetuning text embeddings matters the most in our framework--- in contrast to finetuning visual embeddings. This observation is in-line with that of Locked-image text Tuning (LiT) \cite{zhai2022lit}. Also, we propose to benefit from auxiliary semantic information within our model, in the form of visually-grounded text embeddings. We consider attributes from a fixed vocabulary as such semantic categories. \fref{fig:overview} gives an overview of \ours~implementation. Our video-conditioned text embeddings are unique to each video, allowing a more-flexible latent space. Also optionally, video-conditioned auxiliary text may further help guide the positioning of text embeddings. We evaluate \ours~on multiple benchmarks including activity recognition (on Kinetics-400 \cite{kay2017kinetics}), long-form activity recognition (Charades \cite{sigurdsson2016hollywood}), zero-shot and few-shot recognition (HMDB-51 \cite{kuehne2011hmdb}, UCF-101 \cite{soomro2012ucf101}), showing competitive performance.


\section{Related Work}
\label{sec:related_work}

\paragraph{Video understanding} is about reasoning based on spatio-temporal inputs (i.e., videos). Compared to image inputs, videos bring additional cues such as motion and different modalities into play, but also become tricky to train on due to increased complexity of models and redundant information in data. Convolutional networks (CNNs) \cite{carreira2017i3d, xie2017s3d, tran2018r2p1d, wang2018nonlocal} and Recurrent models \cite{escorcia2016daps, yeung2018every} have been the state-of-the-art on videos, prior to the rise of Transformers \cite{arnab2021vivit, bertasius2021timesformer, liu2022videoswin, ryoo2021tokenlearner}. Multi-stream models \cite{carreira2017i3d, feichtenhofer2019slowfast} making use multiple spatio-temporal views \cite{feichtenhofer2019slowfast, recasens2021brave}, or multiple modalities (eg: optical-flow \cite{carreira2017i3d, han2020self}, audio \cite{nagrani2021bottleneck, huang2022mavil, recasens2023zorro}) thrived in this domain, while Neural Architecture Search (NAS) has enabled efficient model designs \cite{feichtenhofer2020x3d, ryoo2019assemblenet, ryoo2020assemblenet++}. To alleviate the high demand for annotated data, self-supervised methods have also emerged \cite{recasens2021brave, han2020self, qian2021spatiotemporal, feichtenhofer2021large}. Apart from the prominent benchmark task of activity recognition, there exist other variants such as activity localization \cite{sigurdsson2016hollywood, gu2018ava, yeung2018every} and text-to-video retrieval \cite{xu2016msrvtt}. To handle long-duration video input, models have focused on efficient long-term temporal modeling \cite{piergiovanni2018superevents, piergiovanni2019tgm, kahatapitiya2021coarsefine, dai2022ms}, or memory mechanisms \cite{wu2019long, wu2022memvit, ryoo2022ttm}. More recently, language-supervision has been of interest for video understanding.

\paragraph{Vision-Language Models (VLMs)} are usually trained on internet-scale paired visual-language (eg: image-text) data. Seminal work such as CLIP \cite{radford2021clip} and ALIGN \cite{jia2021align} have shed the light on the capabilities of such models, especially for zero-shot transfer. Since then, VLM literature has flourished, with applications in open-vocabulary object detection \cite{gu2021vild, minderer2022simple}, open-set classification \cite{qian2022multimodal}, retrieval \cite{singh2022flava, yao2021filip, bain2021frozen}, captioning \cite{yang2023vid2seq}, segmentation \cite{xu2022groupvit, ranasinghe2022perceptual}, robot manipulation \cite{zeng2022socratic, jiang2022vima, karamcheti2023language} and many other domains. Although VLMs are generally trained on image-text data, there are intuitive variants which are trained either only on images \cite{tschannen2022image} or only on text \cite{nukrai2022text}. The commonly-used similarity-based objective of VLMs has also been repurposed to specialized domains, through prompt learning \cite{zhou2022learning} or engineering \cite{gu2021vild, paiss2023teaching}. The text encoder of VLMs can be a powerful mapping from semantic concepts to latent embeddings \cite{menon2022visual}. Many foundation models \cite{yuan2021florence, alayrac2022flamingo, yu2022coca} follow similar design principles as VLMs, thriving in zero-shot \cite{guo2022calip} or few-shot \cite{zhou2022learning} settings. Recent work combining Large Language Models (LLMs) with VLMs present how language can act as a communication-medium between models \cite{zeng2022socratic, zhao2022learning, wang2022language}. In\cite{menon2022visual}, authors use an LLM represent object classes as a set of its semantic attributes, to learn a better classifier. 

As for VLMs in video domain, they are either trained from scratch on video-text data \cite{xu2021videoclip, yang2023vid2seq}, or more-often than not, finetuned starting from a pretrained image-text model \cite{cheng2022vindlu, yan2022videococa, lin2022egocentric}. Some are even trained on both image and video data paired with text \cite{bain2021frozen}. The success of VLMs in image domain has also fueled research in video domain.

\paragraph{Adapting image-text models to video} is a common practice when designing video VLMs. A general and effective recipe for such adaptation is proposed in \cite{cheng2022vindlu}. It consists of temporal modeling, multi-modal fusion, auxiliary training objectives, and both image/video data at scale. All others usually make use of a subset of these concepts. CLIP-ViP \cite{xue2022clipvip} is trained with different sources of data and multiple cross-modal training objectives. VideoCoCa \cite{yan2022videococa} extends CoCa \cite{yu2022coca} with attention-pooling of frame embeddings, which are used to decode text captions in a generative framework. MOV \cite{qian2022multimodal} is trained with additional audio/flow encoders through cross-modal attention, keeping image-text encoders frozen. Video-specific prompts can also be learned with such frozen encoders \cite{ju2022prompting}. Vi-Fi \cite{rasheed2022fine} shows that simply finetuning CLIP image-text encoders without any specialized modules can generate video representations efficiently.

Apart from the above, there exists a body of prior work that closely-relates to \ours. ActionCLIP \cite{wang2021actionclip} upgrades its CLIP image-encoder with (1) parameter-free temporal layers (TSM \cite{lin2019tsm}) within the backbone, and (2) a temporal transformer head, while keeping the text-encoder fixed. Similarly, CLIP4clip \cite{luo2022clip4clip} just uses a temporal transformer head to update visual embeddings. CLIPHitchhiker's \cite{bain2022cliphitchhiker} generates \textit{text-conditioned video} embeddings by temporal pooling frame embeddings, conditioned on each text query. In this case, a given video generates multiple different visual embeddings, one per each text embedding. EVL \cite{lin2022evl} completely discards text. It acts as an initialization for a visual-only backbone, consisting of CLIP image encoder and a temporal, class-conditioned decoder. X-CLIP \cite{ma2022xclip} introduces trainable temporal layers within its CLIP image encoder, and generates video-specific text prompts. Meaning, it finetunes both encoders similar to ours. However, it is shown to be benefiting more from updating visual embeddings. In contrast, we introduce \textit{video-conditioned text} embeddings (which are unique for each video), showing the effectiveness of text in video understanding.


\section{\textit{Video-conditioned Text} Representations}
\label{sec:method}

In this section, we will first introduce a general framework for adapting image-text models to video, and discuss how prior work fit into it. Next, we present \ours~in-detail.

\subsection{Background: image-text models to video}

CLIP \cite{radford2021clip} has become a prominent VLM in image domain since its release. With its easy-to-use codebase, released models and convincing performance, the community has adapted into many domains. It consists of two encoders: Image and Text, optimized together on internet-scale paired image-text data. Image Encoder () is a ViT \cite{dosovitskiy2020vit}. Given an input image , it is broken down to patch embeddings (i.e., tokens) and processed through multiple transformer layers. The class token  is sampled as the visual embedding . Text Encoder () is an causal transformer, operating on tokenized text. Each class-label (or, any semantic concept) given as text , is first converted into a prompt based on a template such as \texttt{``a photo of \{class\}.''}, and tokenized with Byte Pair Encoding (BPE) \cite{sennrich2015neural} at the input of Text Encoder. Following multiple causal transformer layers, the \texttt{[EOS]} token (end-of-sequence) is extracted as the text embedding . 

The two encoders are jointly-optimized with Cross-Entropy loss, where logits are computed based on the similarities (i.e., \textit{affinities}) between visual and text embeddings. The corresponding pairs of embeddings (i.e., positives) are drawn together ( affinity) in a joint embedding space, whereas the others (i.e., negatives) are pushed apart ( affinity).


When adapting this framework to the video domain, the above Image, Text encoders and the learning objective stays the same, in general. But now, video frames  become input to the Image encoder (where, each being processed separately), and further go through a Video Head  to induce temporal reasoning capabilities. Optionally, text embedding  may also be updated or used as a conditioning within the Video Head.

This Video Head may just be a temporal pooling layer, or a temporal transformer as shown in \cite{luo2022clip4clip, wang2021actionclip}, or even consist of more-specialized modules. Text embeddings could either be discarded as in \cite{lin2022evl}, used as a conditioning for video as in \cite{bain2022cliphitchhiker}, or jointly-updated with video embeddings as in \cite{ma2022xclip}. Finally, logits are computed based on affinities between video-text (if text is not discarded), or through a linear mapping of video embeddings (if text is discarded). This general framework is shown in \fref{fig:main} (top-left), along with variations of each prior work in (bottom-left).

\subsection{Overview of \bf\ours}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=1\linewidth]{main_fig.pdf}
    \vspace{-7mm}
    \caption{\textbf{Detailed view of \ours~compared to prior art:} There exist multiple closely-related work on adapting pretrained image-text models to video, such as CLIP4Clip \cite{luo2022clip4clip}, ActionCLIP \cite{wang2021actionclip}, CLIP Hitchhiker's \cite{bain2022cliphitchhiker}, EVL \cite{lin2022evl} and X-CLIP \cite{ma2022xclip}. All these follow a common framework (top-left). Text prompts and video frames are first encoded using two-separate encoders, and then fed into a video head to enable temporal reasoning. It is optional to use text tokens within the video head. Often, text information is kept unchanged \cite{luo2022clip4clip, wang2021actionclip}, or even discarded \cite{lin2022evl} (bottom-left). CLIP Hitchhiker's \cite{bain2022cliphitchhiker} however, use text as conditioning to generate \textit{text-conditioned video} embeddings. X-CLIP \cite{ma2022xclip}--- which is closest to our method, jointly-optimizes visual and text tokens. But, it shows a limited gain from updating text (i.e., learned as a video prompt). 
    In contrast, \ours~benefits more from updating text, while jointly-optimizing both text and visual information. We generate \textit{video-conditioned text} representations, i.e., text uniquely-specialized for each video (refer to \fref{fig:concept}). Our video head consists of three key operations: (1) \textit{Token-boosting}, (2) \textit{Cross-modal attention}, and (3) \textit{Affinity (re-)weighting}  (right). Token-boosting creates text tokens per timestep, weighted by per-frame affinities. These enable us to model variations of semantics (represented as text classes) over time. Affinity (re)-weighting highlights or down-plays each text class, grounded on visual information. Such affinity weights are similar to the ones in CLIP \cite{radford2021clip} learning objective, making the optimization easier. Cross-modal attention enables message passing between visual and text modalities. Also, optionally, \ours~can make use of auxiliary semantics (eg: object, scene, human-subjects) given as visually-grounded text (refer to \fref{fig:overview}). Such auxiliary semantics help align our video-conditioned activity-class embeddings in the latent space.
    }
    \label{fig:main}
    \vspace{-4mm}
\end{figure*}

In \ours, we adapt pretrained CLIP \cite{radford2021clip} to video, focusing more on text representations. Refer to \fref{fig:main} (right) for a detailed view. The original CLIP objective has not seen any temporal information during its training. While it obviously affects the temporal reasoning capabilities of visual embeddings--- which most prior work focus on addressing, it also limits text embeddings as well. The learnt latent space (and, the affinity-based objective) depends on both these embeddings. Thus, we consider text equally as important— if not more— in contrast to prior work

\ours~consists of a joint video-text model as , which takes in both visual and text embeddings from CLIP backbones. It outputs text embeddings uniquely-specified for each video, i.e., \textit{Video-conditioned Text} embeddings. It relies on three main processing components: (1) \textit{Token-boosting}, (2) \textit{Cross-modal attention}, and (3) \textit{Affinity (re-)weighting}. Optionally, it can benefit from any semantic concept available as auxiliary text, to optimize its latent embedding space. Following subsections look at each of these in detail.

Let us first introduce a few additional notations. Consider a fixed vocabulary of  activity-classes given by , and optional  auxiliary semantic categories given by . The corresponding extracted CLIP text embeddings can be denoted as  and .
Also, given an input video  of  frames, the extracted CLIP image embeddings can be denoted as . Note that the inputs to our Video Head are ,  and  tokens. As visual embeddings are extracted per-frame and the text embeddings per prompt, there is no interaction among frame tokens, among text tokens or, across frame-text tokens up to this point.

\paragraph{Token-boosting:}
We intend to create \textit{video-conditioned text} embeddings. With this motivation, we first replicate CLIP \cite{radford2021clip} text tokens, creating a dedicated set of text per video (as also done in \cite{ma2022xclip}). Going further, we also create text tokens per each frame. This is done by a weighting mechanism based on frame-text affinities. Formally, given  input text tokens, we end up with  dedicated text tokens per video. Refer \fref{fig:main} (right).

Here, \texttt{SigAffinity}() corresponds affinity-weights normalized in  range. We convert affinities given by \texttt{Affinity}() (which lie in ) to be affinity-weights, by scaling with a learnable weight and feeding through a sigmoid activation.

Although such affinity-weights based on original CLIP embeddings are not ideal for temporal reasoning, it initializes noisy-versions of our \textit{video-conditioned text} embeddings which get iteratively updated later in the network. Such token-boosting brings multiple other benefits. (1) More tokens means higher the model capacity. It can help learn better representations, but also adding a compute overhead (which we handle through other measures, as discussed later). (2) It also highlights relevant text tokens by grounding on CLIP visual embeddings, while diminishing irrelevant ones. Subsequent attention mechanisms attend less to such diminshed tokens, making the learning easier. In other words, it acts as a soft-selection of relevant semantics, specific to each video. (3) Finally, it enables our model to capture variations of semantic categories over time. Modeling how certain attributes appear (or, disappear) over time is an important motion cue for activity recognition. We concatenate such boosted text tokens with CLIP visual tokens (corresponding to  frames), feeding  tokens to subsequent layers.


Such boosted tokens  go through multiple transformer layers in our Video Head. Each layer () is a combination of temporal attention, cross-modal attention, affinity (re-)weighting and linear layers (MLP).

\paragraph{Cross-modal and Temporal attention:}
We consider our token representation to be two-dimensional (i.e., temporal and cross-modal), and use divided attention (MSA) for each axis as in \cite{arnab2021vivit, bertasius2021timesformer}. First comes a Cross-modal attention layer. Here, each visual token could attend all text tokens at the same timestep, and each text token could attend the visual token and other text tokens at the same timestep. As text tokens are already affinity-weighted, attention weights do not draw information from irrelevant semantic classes. Second comes a Temporal attention layer. A shared set of parameters for visual and text tokens are used, learning temporal cues in visual modality (i.e., ), and modeling variations of semantics across time in textual modality. 

Here, ) stands for LayerNorm operations. Having divided attention across two-axes instead of joint attention eases compute requirement of our model.

\paragraph{Affinity (re-)weighting:}
As previously mentioned, the original affinities based on CLIP embeddings can be noisy, in the context of temporal reasoning. Now, as we have updated both our video and text tokens with temporal and cross-modal information, they are in a better state to update affinities. Hence, we re-compute new affinity values and re-weight text tokens accordingly. Refer \fref{fig:main} (rightmost). First, we split video and text tokens, and temporally-pool text tokens to come up with a compressed representation.

Next, we perform affinity re-weighting. This is similar to token-boosting, but with updated video-text embeddings (which are already video-specific). Without loss of generality, the same operations apply for auxiliary text tokens.


Finally, this affinity-weighted and concatenated token representation (again, ) goes through an MLP projection.


Following  transformer layers in our Video Head, all embeddings get aggregated temporally. We end up with a single video embedding,  activity-text embeddings and  aux-text embeddings. We further aggregate auxiliary embeddings, leaving a single embedding per each of  semantic categories (eg: object, scene, human-subjects). We compute logits based on affinity similar to CLIP \cite{radford2021clip} objective, and use Cross-Entropy loss for optimization. 


\subsection{Discussion on design decisions}

\paragraph{Auxiliary semantic information:} \ours~can rely on optional semantics (or, attributes) in the form of visually-grounded auxiliary text, to improve our \textit{video-conditioned text} embeddings. This is guided by the loss on . The set of such auxiliary texts is fixed based on a vocabulary, and we use a soft-selection (done implicitly) to filter relevant semantics per video, by grounding them visually. 

\paragraph{Alternative weighting schemes:} Our text (re-)weighting method is in-line with the CLIP \cite{radford2021clip} learning objective, which is based on affinities between video-text pairs. We find this complementary nature beneficial. It highlights relevant text (and diminish irrelevant ones) within each intermediate layer of our Video Head. This iterative process help the noisy affinities resulting from the original CLIP embeddings to be fixed, when fused with better temporal cues in the subsequent layers. We also explored other weighting schemes such as using learnable weights or attention-based weights, which do not have a direct connection to the learning objective. They do not provide any improvements. 

\paragraph{Visual-only or Text-only classifiers:} We also explored different classifiers (i.e., how we compute logits): using either a visual-only classifier as in  \cite{lin2022evl}, a text-only classifier, or an affinity-based classifier as in \cite{radford2021clip, ma2022xclip}, the last of which performs best. Even though we primarily focus on updating text embeddings, it makes sense to rely on video-text affinities to be the training objective (or, classifier), as it is complementary to the processing in our Video Head.

\paragraph{Growth of compute requirement:} Token-boosting increases the footprint of our model. To manage the compute requirement, we use divided temporal and cross-modal attention instead of joint-attention. Also, the model size is reduced by sharing parameters of temporal attention layers across video-text tokens. When compared with CLIP \cite{radford2021clip} backbones, our Video Head consists of fewer layers (eg: 4-layers), minimizing the relative footprint. It has a comparable footprint to prior work such as \cite{lin2022evl, ma2022xclip}.


\section{Experiments}
\label{sec:experiments}

To validate the merits of \ours, we experiment on activity recognition (on Kinetics-400 \cite{kay2017kinetics}), long-term activity recognition (on Charades \cite{sigurdsson2016hollywood}), zero-shot and few-shot recognition (on HMDB-51 \cite{kuehne2011hmdb} and UCF-101 \cite{soomro2012ucf101}). Following sub-sections will detail our implementation, evaluation settings, datasets and the results.

\paragraph{Implementation details:} We use a pretrained CLIP \cite{radford2021clip} as our image-text backbone (either ViT-B/16 or ViT-L/14 variants). Our Video Head is randomly-initialized having 4 transformer blocks similar to \cite{wang2021actionclip}, which is applied on-top of CLIP backbones. We consider an embedding dimension of 512/768 (w/ heads 8/12) corresponding to B/16 and L/14 backbone variants. Our output video-text embeddings are further mapped into 256-dimensional embeddings prior to computing affinity-based logits. We use an AdamW \cite{loshchilov2017adamw} optimizer with a cosine schedule for training. On Kinetics-400 \cite{kay2017kinetics}, we finetune our model for 30 epochs with a batch size of 256 using 8e-6/8e-5 learning rates for backbone/newly-initialized parameters, similar to \cite{ma2022xclip}. On Charades \cite{sigurdsson2016hollywood}, we finetune for 50k iterations with a batch size of 64 using 5e-7/5e-4 learning rates for backbone/newly-initialized parameters, similar to \cite{bain2022cliphitchhiker}. We use augmentations and input sampling strategies similar to \cite{ma2022xclip} for Kinetics-400 and similar to \cite{lin2022evl} for Charades.

\paragraph{Evaluation settings:} In all our experiments, we compare with prior art on each dataset which either use large-scale image-text pretraining or not. Since the direction of adapting image-text models to video tasks is relatively-recent, their absolute performance may not be state-of-the-art in some cases, but we report numbers in comparable settings. For each experiment, we report pretraining settings, \#frames-per-view, \#views-at-inference, compute-per-view (GFLOPs) as supplementary metrics. We evaluate single-label activity recognition performance with Top-1 (\%) accuracy, and multi-label recognition with Average Precision (mAP\%). When reporting FLOPs, we consider the cost of a single affinty-based logit (i.e., cost for one video-text pair) similar to \cite{ma2022xclip}. 

\subsection{Activity Recognition}

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.95\linewidth}{!}{
		\begin{tabular}{lccccr}
		    \shline
			\multicolumn{1}{l}{Backbone}  & Pretrain & \#Frames & \#Views & GFLOPs & Top-1 \\
			\hline
			\multicolumn{6}{l}{\textit{Methods w/o image-text pretraining}} \\
			MViT-B \cite{fan2021mvit} & - & 32 & 51 & 170 & 80.2 \\
			Uniformer-B \cite{li2022uniformer} & IN-1K & 32 & 43 & 259 & 83.0 \\
			Video-Swin-S \cite{liu2022videoswin} & IN-1K & 32 & 43 & 166 & 80.6 \\
			TimeSformer-L \cite{bertasius2021timesformer} & IN-21K & 96 & 13 & 2380 & 80.7 \\
			MTV-B \cite{yan2022mtv} & IN-21K & 32 & 43 & 399 & 81.8 \\
			MTV-L \cite{yan2022mtv} & JFT-300M & 32 & 43 & 1504 & 84.3 \\
			MViTv2-B \cite{li2022mvitv2} & - & 32 & 51 & 225 & 82.9 \\
			ViViT-L FE \cite{arnab2021vivit} & JFT-300M & 32 & 13 & 3980 & 83.5 \\
			TokenLearner \cite{ryoo2021tokenlearner} & JFT-300M & 64 & 43 & 4076 & 85.4 \\
			CoVeR-L \cite{zhang2021cover}
			& JFT-3B & - & 13 & - & 87.2 \\
			
			\hline
			\multicolumn{6}{l}{\textit{Methods w/ image-text pretraining (ViT-B/16 backbone)}} \\
			ST-Adapter \cite{pan2022stadapter} & CLIP & 32 & 13 & 607 & 82.7 \\
			Text4Vis \cite{wu2022text4vis} & CLIP & 16 & 43 & - & 83.6 \\
			ActionCLIP \cite{wang2021actionclip} & CLIP & 32 & 103 & 563 & 83.8 \\
			EVL \cite{lin2022evl} & CLIP & 8 & 13 & 148 & 82.9 \\
			EVL \cite{lin2022evl} & CLIP & 16 & 13 & 296 & 83.6 \\
			EVL \cite{lin2022evl} & CLIP & 32 & 13 & 592 & 84.2 \\
			X-CLIP \cite{ma2022xclip} & CLIP & 8 & 43 & 145 & 83.8 \\
			X-CLIP \cite{ma2022xclip} & CLIP & 16 & 43 & 287 & 84.7 \\
                \rowcolor{row}\ours  & CLIP & 16 & 43 & 285 & 84.2 \\
			
			\hdashline
			\multicolumn{6}{l}{\textit{Methods w/ image-text pretraining (ViT-L/14 backbone)}} \\
			ST-Adapter \cite{pan2022stadapter} & CLIP & 32 & 13 & 2749 & 87.2 \\
			Text4Vis \cite{wu2022text4vis} & CLIP & 32 & 13 & 1662 & 87.1 \\
			EVL \cite{lin2022evl} & CLIP & 8 & 13 & 674 & 86.3 \\
			EVL \cite{lin2022evl} & CLIP & 16 & 13 & 1348 & 87.0 \\
			X-CLIP \cite{ma2022xclip} & CLIP & 8 & 43 & 658 & 87.1 \\
			\rowcolor{row}\ours & CLIP & 8 & 43 & 656 & 87.0  \\
			\shline
			
	\end{tabular}}
	\caption{\textbf{Activity Recognition on Kinetics-400 \cite{kay2017kinetics}:} We compare our method against prior art, reporting pretraining settings, input format, compute cost (GFLOPs) and top-1 accuracy (\%). Here, \#Frames represent number of frames per view, while \#Views represent \#Temporal\#Spatial crops during inference. The compute cost reported is per view. \ours~ shows competitive performance among the methods pretrained w/ internet-scale image-text data, under comparable settings.}
    \vspace{-3mm}
    \label{tab:kin}
\end{table}

\paragraph{Data:} Kinetics-400 \cite{kay2017kinetics} is a large-scale activity recognition dataset, which contains 240k videos for training and 20k for validation. Each clip is for a single human activity out of 400 categories, and consists of video-level annotations. Kinetics clips are usually 10s long.

\paragraph{Results:} We report the performance of \ours~on Kinetics-400 activity recognition in \tref{tab:kin}. We consider B/16 model variant with 16-frames per view and L/14 with 8-frames per view, while using  such views at inference similar to \cite{ma2022xclip}. Our method shows a competitive performance at a similar footprint to closely-related methods \cite{ma2022xclip, lin2022evl}. \ours-B/16 outperforms ST-Adapter \cite{pan2022stadapter} by , Text4Vis \cite{wu2022text4vis} by  and ActionCLIP \cite{wang2021actionclip} by . It is on-par with EVL \cite{lin2022evl}, while requiring less than half the FLOPs. It is competitive with X-CLIP \cite{ma2022xclip} (with a  difference) having the same footprint. The larger model \ours-L/14 is competitive with CoVeR-L \cite{zhang2021cover} which is trained with 10 more data. It comfortably outperforms MTV \cite{yan2022mtv} by , ViViT \cite{arnab2021vivit} by  and TokenLearner \cite{ryoo2021tokenlearner} by  which are trained on a similar scale of data, while being efficient.

\subsection{Long-term Activity Recognition}

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{1\linewidth}{!}{
		\begin{tabular}{lccccr}
		    \shline
			\multicolumn{1}{l}{Backbone}  & Pretrain & \#Frames & \#Views & GFLOPs & mAP \\
			\hline
			\multicolumn{6}{l}{\textit{Methods w/o image-text pretraining}} \\
			
			I3D + NL \cite{wang2018nonlocal} & K400 & 128 & 103 & 544 & 37.5 \\
			EvaNet \cite{piergiovanni2019evanet} & K400 & 64 & - & - & 38.1 \\
			LFB-101 \cite{wu2019lfb} & K400 & 32 & 103 & 529 & 42.5 \\
			SlowFast-50 \cite{feichtenhofer2019slowfast} & K400 & 8+32 & 103 & 66 & 38.0 \\
			SlowFast-101 + NL \cite{feichtenhofer2019slowfast} & K400 & 16+64 & 103 & 234 & 42.5 \\
			X3D-XL (312) \cite{feichtenhofer2020x3d} & K400 & 16 & 103 & 48 & 43.4 \\
			MViT \cite{fan2021mvit} & K400 & 32 & 103 & 237 & 47.7 \\
			AssembleNet-101 \cite{ryoo2019assemblenet} & - & 128 & 51 & 1200 & 58.6 \\
			MoViNets \cite{kondratyuk2021movinets} & - & 120 & 11 & 306 & 63.2 \\
			TokenLearner \cite{ryoo2021tokenlearner} & - & 64 & - & - & 66.3 \\
			
			\hline
			\multicolumn{6}{l}{\textit{Methods w/ image-text pretraining (ViT-B/16 backbone)}} \\
			ActionCLIP \cite{wang2021actionclip} & CLIP & 32 & 103 & 563 & 44.6 \\
			CLIP4clip \cite{luo2022clip4clip} & CLIP & 32 & 11 & - & 32.0 \\
			CLIP Hitchhiker's \cite{bain2022cliphitchhiker} & CLIP & 32 & 11 & - & 44.9 \\
			\rowcolor{row}\ours & CLIP & 32 & 41 & 567 & 50.1 \\
			
			\hdashline
			\multicolumn{6}{l}{\textit{Methods w/ image-text pretraining (ViT-L/14 backbone)}} \\
			\rowcolor{row}\ours & CLIP & 32 & 41 & 2602 & 57.6 \\

			\shline

	\end{tabular}}
	\caption{\textbf{Long-term Activity Recognition on Charades \cite{sigurdsson2016hollywood}:} We compare our method against prior art, reporting pretraining settings, input format, compute cost (GFLOPs) and mean Average Precision (mAP\%). The compute cost reported is per view (\#Views  \#Temporal\#Spatial-crops, each having \#Frames per view). \ours~ achieves SOTA results among the methods pretrained w/ image-text data by a considerable margin.}
    \vspace{-3mm}
	\label{tab:charades}
\end{table}

\paragraph{Data:} Charades \cite{sigurdsson2016hollywood} is a small-yet-challenging activity recognition dataset which consists of 9.8k long-term videos. It comes with frame-level annotations of 157 daily household activities performed according to a script. Data is split as 7.9k for training and 1.8k for validation. Each video contains multiple overlapping activities, with an average of 6.8 instances per clip. The average duration of a clip is of 30s.

\paragraph{Results:} We report the performance of \ours~on the Charades long-term activity recognition in \tref{tab:charades}. Here, we consider both B/16 and L/14 model variants with 32-frame per view, while having  such views at inference. Our method outperforms prior work w/ image-text pertaining by a considerable margin. In fact, \ours-B/16 shows  mAP boost over CLIP Hitchhiker's \cite{bain2022cliphitchhiker}, and  mAP boost over ActionCLIP \cite{wang2021actionclip} with a similar footprint. This is a significant improvement considering the challenging Charades settings. \ours-L/14 shows competitive results across state-of-the-art methods w/ different pretraining settings. Note that the SOTA models with smaller footprint such as MoViNets \cite{kondratyuk2021movinets} and TokenLearner \cite{ryoo2021tokenlearner} are not directly comparable as they are searched/optimized for efficiency.


\subsection{Zero-shot Transfer}

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.75\linewidth}{!}{
		\begin{tabular}{lcll}
		    \shline
			\multicolumn{1}{l}{Model} & \#Frames & HMDB-51 & UCF-101 \\
			\hline
			\multicolumn{4}{l}{\textit{Methods w/o image-text pretraining}} \\
			
			MTE \cite{xu2016mte} & - & 19.71.6 & 15.81.3 \\
			ASR \cite{wang2017asr} & 16 & 21.80.9 & 24.41.0 \\
            ZSECOC \cite{qin2017zsecoc} & - & 22.61.2 & 15.11.7 \\
            UR \cite{zhu2018ur} & 1 & 24.41.6 & 17.51.6 \\
            TS-GCN \cite{gao2019tsgcn} & 16 & 23.23.0 & 34.23.1 \\
            E2E \cite{brattoli2020e2e} & 16 & 32.7 & 48 \\
            ER-ZSAR \cite{chen2021erzsar} & - & 35.34.6 & 51.82.9 \\
            
            \hline
		\multicolumn{4}{l}{\textit{Methods w/ image-text pretraining (ViT-B/16)}} \\
            ActionCLIP \cite{wang2021actionclip} & 32 & 40.85.4 & 58.33.4 \\
            X-CLIP \cite{ma2022xclip} & 32 & 44.65.2 & 72.02.3 \\
            \rowcolor{row}\ours & 32 & 51.01.3 & 72.40.3 \\
            MOV \cite{qian2022multimodal} & 16 & 60.82.8 & 82.64.1 \\
            \hdashline
            \multicolumn{4}{l}{\textit{Methods w/ image-text pretraining (ViT-L/14)}} \\
            Text4Vis \cite{wu2022text4vis} & 16 & 58.15.7 & 85.83.3 \\
            VideoCoCa \cite{yan2022videococa} & 8 & 58.7 & 86.6 \\
            MOV \cite{qian2022multimodal} & 16 & 64.73.2 & 87.13.2\\
		\shline

	\end{tabular}}
	\caption{\textbf{Zero-shot Transfer on HMDB-51 \cite{kuehne2011hmdb} and UCF-101 \cite{soomro2012ucf101}:} We compare our method against prior art, reporting input format and top-1 accuracy (\%) -- mean/ std across three splits of test set as in \cite{radford2021clip}. We use models pretrained on Kinetics-400 for 10 epochs. \ours~outperforms similarly adapted image-text models. Models denoted with  use extra audio supervision and are not directly comparable.}
    \label{tab:zeroshot}
    \vspace{-3mm}
\end{table}

\paragraph{Data:} We use downstream datasets UCF-101 \cite{soomro2012ucf101} and HMDB-51 \cite{kuehne2011hmdb} to evaluate zero-shot transfer capabilities of our model. UCF-101 is a classification dataset with realistic actions from YouTube. It contains 13k clips annotated with 101 action classes, and three splits of training/test data. HMDB-51 is relatively small and contains 7k clips with 51 classes. It also has three splits of training/test data.

\paragraph{Results:} We report zero-shot transfer performance on HMDB-51 \cite{kuehne2011hmdb} and UCF-101 \cite{soomro2012ucf101} in \tref{tab:zeroshot}. We pretrain our model for 10 epochs on Kinetics-400 \cite{kay2017kinetics} with 32-frames per view, similar to  \cite{ma2022xclip} and transfer downstream. We report mean and standard deviation on three-splits. \ours-B/16 outperforms X-CLIP \cite{ma2022xclip} by  on HMDB-51 and by  on UCF-101. Also, the performance of our model is more stable across splits. This validates that the learned \textit{video-conditioned} text embeddings can be generalized, even w/o seeing the same categories during training.

\subsection{Few-shot Transfer}

\newcommand\myeq{\mkern1.5mu{=}\mkern1.5mu}
\begin{table}[h!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|cccc|cccc}
		    \shline
			\multicolumn{1}{l|}{Model} & \multicolumn{4}{c|}{HMDB-51} & \multicolumn{4}{c}{UCF-101} \\
            &  &  &  &  &  &  &  &  \\
			\hline
			\multicolumn{8}{l}{\textit{Methods w/o image-text pretraining}} \\

            TSM \cite{lin2019tsm} & 17.5 & 20.9 & 18.4 & 31.0 & 25.3 & 47.0 & 64.4 & 61.0 \\
            TimeSformer \cite{bertasius2021timesformer} & 19.6 & 40.6 & 49.4 & 55.4 & 48.5 & 75.6 & 83.7 & 89.4 \\
            Video-Swin-B \cite{liu2022videoswin} & 20.9 & 41.3 & 47.9 & 56.1 & 53.3 & 74.1 & 85.8 & 88.7 \\
            \hline
            \multicolumn{8}{l}{\textit{Methods w/ image-text pretraining (ViT-B/16)}} \\
            X-CLIP \cite{ma2022xclip} & 53.0 & 57.3 & 62.8 & 64.0 & 76.4 & 83.4 & 88.3 & 91.4 \\
            \rowcolor{row}\ours & 60.0 & 63.2 & 66.6 & 70.7 & 87.7 & 92.3 & 93.6 & 95.8 \\
		\shline

	\end{tabular}}
	\caption{\textbf{Few-shot Transfer on HMDB-51 \cite{kuehne2011hmdb} and UCF-101 \cite{soomro2012ucf101}:} We compare our method against prior art, reporting top-1 accuracy (on the first split among three test splits as in \cite{radford2021clip}). We use models pretrained on Kinetics-400 \cite{kay2017kinetics} for 10 epochs, and finetune on few-shot samples for 50 epochs. We randomly-sample  clips per class as few-shot training samples at each setting. \ours~shows a significant boost over X-CLIP \cite{ma2022xclip}.}
    \label{tab:fewshot}
    \vspace{2mm}
\end{table}

\paragraph{Data:} We consider the downstream datasets HMDB-51 \cite{kuehne2011hmdb} and UCF-101 \cite{soomro2012ucf101} to evaluate the few-shot performance of our model. In each setting, we randomly sample 2, 4, 8, or 16 clips per class to create our few-shot training sets. We use a model pretrained on Kinetics-400 \cite{kay2017kinetics} for 10 epochs and finetune on few-shot examples for 50 epochs, using 32-frames per view as in \cite{ma2022xclip}. 

\paragraph{Results:} In \tref{tab:fewshot}, we report top-1 accuracy on the first test split among three, in each dataset, using a single view at inference. \ours~significantly outperforms prior art, either w/o image-text pretraining (TSM \cite{lin2019tsm}, TimeSformer \cite{bertasius2021timesformer}, Video-Swin \cite{liu2022videoswin}) or w/ such pretraining (X-CLIP \cite{ma2022xclip}). This shows the effectiveness of our \textit{video-conditioned text} embeddings when generalizing to few-shot settings.

\subsection{Ablation Study}

\begin{table}[t!]
	\centering
	\tablestyle{1.8pt}{1.}
	\resizebox{0.6\linewidth}{!}{
	    \tablestyle{1.8pt}{1.}
        \fontsize{8.8}{11}\selectfont
		\begin{tabular}{lr}
		\shline
		Backbone & mAP \\
		\hline
		
		\rowcolor{row}\ours & 50.1 \\ \hline
		\ours~ (No Aux. Text) & 49.8 \\
		\ours~ (No Affinity weighting) & 48.8 \\
		\ours~ (w/ joint-attention) & 44.8 \\ \hline
		\ours~ (Text Classifier) & 41.2 \\
		\ours~ (Visual Classifier) & 43.1 \\ \hline
		\ours~ (w/ CLIP Visual emb.) & 49.7 \\
		\ours~ (w/ CLIP Text emb.) & 41.7 \\
		\shline
\end{tabular}
        }
	\caption{\textbf{Ablations on Charades \cite{sigurdsson2016hollywood}:} We evaluate different design decisions of \ours~ in this study, reporting mean Average Precision (mAP\%). Specifically, we measure the importance of auxiliary text prompts, affinity weighting and divided temporal/cross-modal attention. Also, we replace our visual-text affinity-based logits with simpler visual-only or text-only logits to show its effectiveness. Finally, we show that updating text is most critical in our framework (i.e., temporally-pooled CLIP image embeddings is as good as our video embeddings).}
    \vspace{-3mm}
    \label{tab:ablations}
\end{table}

In \tref{tab:ablations}, we present multiple variants of \ours~evaluated on Charades \cite{sigurdsson2016hollywood}, justifying our design decisions and revealing key insights.

\paragraph{Auxiliary semantics do help.} We rely on extra semantic information to guide our latent embedding space. We see that such auxiliary text is giving  mAP gain. In Charades setting with complex and overlapping activities, such an increment is meaningful.

\paragraph{Affinity-weighting and separable attention do help.} We see a   mAP performance increment by having our affinity (re-)weighting mechanism. While joint-attention may be more expressive compared to separable attention, it can incur training difficulties. As a result, we see separable attention enjoying a significant  mAP benefit.

\paragraph{Affinity-based classifier is required.} As we previously-mentioned, our affinity weighting mechanism makes more-sense in the context of the same affinity-based loss formulation. To verify this, we replace such affinity-based logits with text-only or visual-only logits, which are linear mappings of corresponding embeddings. These significantly underperforms, with  mAP and  mAP respectively.

\paragraph{Updating text embeddings is more effective than updating visual embeddings.} To evaluate which of our embeddings (video or \textit{video-conditioned text}) are critical, we replace them with the corresponding original CLIP \cite{radford2021clip} embeddings (temporally-pooled image embeddings or text). We see that the proposed \textit{video-conditioned text} are significantly-more effective, and when replaced, the performance drops  mAP. In contrast, when our video embeddings are replaced, the performance drops only  mAP. This means that CLIP image embeddings are as good as our video embeddings, but our \textit{video-conditioned text} embeddings are significantly better.


\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced \ours, a framework for adapting image-text models to video, with a focus on \textit{video-conditioned text} embeddings. It can also benefit from freely-available auxiliary semantic information in the form of visually-grounded text, to guide the learned latent space. Our evaluations verified the importance of updating text embeddings, across multiple benchmarks including supervised, zero-shot and few-shot settings. We believe this work will enable better insights into the use of language for temporal reasoning.


{\small
\balance
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
