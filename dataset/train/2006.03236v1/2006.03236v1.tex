\documentclass{article}


\usepackage[numbers]{natbib}
\usepackage[preprint,nonatbib]{neurips_2020}





\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{placeins}


\usepackage{fullpage}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2pt}

\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{stmaryrd} \usepackage{colonequals}
\usepackage{xifthen}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[inline]{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,pgfplots}

\usepackage[colorlinks = true]{hyperref}

\hypersetup{
	urlcolor  = NavyBlue,  linkcolor = cyan,      citecolor = purple,    filecolor = magenta    }

\usepackage{cancel}

\usepackage{thmtools}
\newtheoremstyle
	{custom}      {\topsep}     {\topsep}     {\normalfont} {}            {\bfseries}   {}            {\newline}    {\rule{\textwidth}{0.4pt} \\*\thmname{{#1}}~\thmnumber{{#2}}\thmnote{~(\textbf{#3})}. \
\label{eqn:attn}
\mb{h} &\leftarrow \text{LayerNorm}\rbr{\mb{h} + \text{S-Attn}(\text{Q}=\mb{h}, \text{KV}=\mb{h})}, \\
\label{eqn:pffn}
h_i &\leftarrow \text{LayerNorm}\rbr{h_i + \text{P-FFN}(h_i)},\quad \forall i = 1, \cdots, T.
 
\max_{\theta}\; \mc{J}_\text{MLM}(\theta) = 
	\mbb{E}_{\mb{x} \sim \mc{D}} \mbb{E}_{\mc{I}} \sum_{i \in \mc{I}} \log P_\theta(x_i \mid \hat{\mb{x}}_\mc{I}) = 
	\mbb{E}_{\mb{x} \sim \mc{D}} \mbb{E}_{\mc{I}} \sum_{i \in \mc{I}} \log 
	\frac{ \expo{ e(x_i)^\top h_{i}\rbr{\hat{\mb{x}}_\mc{I}} } }
		 { \sum_{x'} \expo{ e(x')^\top h_{i}\rbr{\hat{\mb{x}}_\mc{I}} } },

\label{eqn:pooling}
\mb{h}' \leftarrow \text{Pooling}(\mb{h}),

\label{eqn:poolq_only_attn}
\mb{h} \leftarrow \text{LayerNorm}\rbr{ \mb{h}' + \text{S-Attn}\rbr{\text{Q}=\mb{h}', \text{KV}=\mb{h}}}.

\label{eqn:upsample}
\forall i = 1, \cdots, T,\quad h^\text{up}_i = h^M_{i//2^{M-1}},\vspace{-0.25em}

\hat{\rvh} = \text{truncate}(\rvh) = \seq{h_{\cls}, h_1, \cdots,  h_{2^{p-1}-1}}

\label{eqn:rel_attn}
A_{ij} &= 
	\underbrace{ (W_Q h_i + v)^\top (W_K h_j) }_\text{ content term } + \underbrace{ (W_Q h_i + u)^\top (W_R r_{i-j}) }_\text{ position term }.
 
	\mb{A}^\text{content} = (\mb{H}W_Q + v) (\mb{H}W_K)^\top,

	\mb{A}^\text{position} = \texttt{einsum}(\texttt{"id,ijd->ij"}, \mb{H}W_Q + u, \hat{\mb{R}}W_R).
 \mb{R} = \seq{r_{T-1}, \dots, r_{0}, \cdots, r_{1-T}}  \mb{\hat{R}} = \text{gather}(\mb{R}, \mb{I}), \quad I_{ij} = T + i - j. 
\mb{A}^\text{position} 
&= \texttt{einsum}\rbr{ \texttt{"id,ijd->ij"}, \mb{H}W_Q + u, \green{(\mb{P}\mb{R})} W_R } \\
&= \texttt{einsum}\rbr[\Big]{ \texttt{"ijk,jk->ij"}, \green{\mb{P}}, \sbr{ (\mb{H}W_Q + v) (\mb{R}W_R)^\top } } \\
&= \texttt{gather}\rbr[\Big]{ (\mb{H}W_Q + v) (\mb{R}W_R)^\top, \mb{I} }

A^\text{position}_{ij} 
&=  (W_Q h_i + u)^\top (W_R r_{i-j}) \nonumber\\
&=  \sbr[\Big]{\; \underbrace{ W_R^\top \rbr{ W_Q h_i + u}  }_{q_i} \;}^\top r_{i-j} \nonumber\\
\label{eqn:factorized-0}
&= q_i^\top r_{i-j}.

\sin_{t} &= \seq{ \sin\rbr{t / 10000^{2/D}}, \sin\rbr{t / 10000^{4/D}}, \cdots, \sin\rbr{t / 10000^{D/D}} } \in \R^{D/2}, \\
\cos_{t} &= \seq{ \cos\rbr{t / 10000^{2/D}}, \cos\rbr{t / 10000^{4/D}}, \cdots, \cos\rbr{t / 10000^{D/D}} } \in \R^{D/2}.
 q_i = \texttt{cat}(q_i^{\sin}, q_i^{\cos}). 
A^\text{position}_{ij} &= q_i^\top r_{i-j} = {q_i^{\sin}}^\top \sin_{i-j} + {q_i^{\cos}}^\top \cos_{i-j}.

{q_i^{\sin}}^\top \sin_{i-j} 
&= {q_i^{\sin}}^\top \sbr{\sin_{i} \odot \cos_{j} - \cos_{i} \odot \sin_{j}} \\
&= {q_i^{\sin}}^\top \rbr{\sin_{i} \odot \cos_{j}} - {q_i^{\sin}}^\top \rbr{\cos_{i} \odot \sin_{j}} \\
&= \sbr{ q_i^{\sin} \odot \sin_{i} }^\top \cos_{j} + \sbr{ q_i^{\sin} \odot (-\cos_{i}) }^\top \sin_{j} \\
\intertext{and}
{q_i^{\cos}}^\top \cos_{i-j} 
&= {q_i^{\cos}}^\top \sbr{\cos_{i} \odot \cos_{j} + \sin_{i} \odot \sin_{j}} \\
&= {q_i^{\cos}}^\top \rbr{\cos_{i} \odot \cos_{j}} + {q_i^{\cos}}^\top \rbr{\sin_{i} \odot \sin_{j}} \\
&= \sbr{ q_i^{\cos} \odot \cos_{i} }^\top \cos_{j} + \sbr{ q_i^{\cos} \odot \sin_{i} }^\top \sin_{j}

q_i^\top r_{i-j} 
&= {q_i^{\sin}}^\top \sin_{i-j} + {q_i^{\cos}}^\top \cos_{i-j} \\
&= \sbr{ q_i^{\sin} \odot \sin_{i} }^\top \cos_{j} 
+ \sbr{ q_i^{\sin} \odot (-\cos_{i}) }^\top \sin_{j} 
+ \sbr{ q_i^{\cos} \odot \cos_{i} }^\top \cos_{j} 
+ \sbr{ q_i^{\cos} \odot \sin_{i} }^\top \sin_{j} \\
&= \cbr{ 
	\sbr{ q_i^{\sin} \odot \sin_{i} }^\top \cos_{j} + 
	\sbr{ q_i^{\cos} \odot \cos_{i} }^\top \cos_{j} } 
+ \cbr{
	\sbr{ q_i^{\sin} \odot (-\cos_{i}) }^\top \sin_{j} +
	\sbr{ q_i^{\cos} \odot \sin_{i} }^\top \sin_{j} } \\
&= \sbr[\bigg]{ 
		\underbrace{ \texttt{cat}( q_i^{\sin}, q_i^{\cos} ) }_{= q_i} \odot 
		\underbrace{ \texttt{cat}(\sin_{i}, \cos_{i}) }_{\coloneqq \phi_i} 
	}^\top \underbrace{ \texttt{cat}(\cos_{j}, \cos_{j} ) }_{\coloneqq \psi_j}\\ &\quad + 
   \sbr[\bigg]{ 
   		\underbrace{ \texttt{cat}(q_i^{\sin}, q_i^{\cos}) }_{= q_i} \odot 
   		\underbrace{ \texttt{cat}(-\cos_{i}, \sin_{i}) }_{\coloneqq \pi_i} 
   	}^\top \underbrace{ \texttt{cat}(\sin_{j}, \sin_{j})  }_{\coloneqq \omega_j} \\
&= \sbr{q_i \odot \phi_i }^\top \psi_j + \sbr{q_i \odot \pi_i}^\top \omega_j,

\mb{A}^\text{position} 
&= \cbr[\Big]{ \sbr[\big]{ \rbr{ \mb{H} W_Q + u} W_R^\top } \odot \mb{\Phi}  } \mb{\Psi}^\top 
+ \cbr[\Big]{ \sbr[\big]{ \rbr{ \mb{H} W_Q + u} W_R^\top } \odot \mb{\Pi}  } \mb{\Omega}^\top 

\rvh_\text{enc} &= \text{Encoder}(\rvx_\text{enc}), \\
\rvh_\text{dec} &= \text{Decoder}(\rvh_\text{enc}, \rvx_\text{dec}),

where  and  are the encoder input sequence and the \textit{optional} and \textit{problem-specific} decoder input, respectively.
The goal of encoder is to compressing the input sequence  into the hidden representations  with a reduced length.
Then, conditioned on the decoder input  if any, the decoder will extract relevant information/representations from  to solve the specific NLP problem at hand.
Next, we will how the general form of \name can be instantiated into specific forms to solve corresponding NLP problems.

\paragraph{Sequence-level prediction} This is essentially the case we consider in most of our experiments where we want to obtain a vectorial representation of the input sequence such as text classification.
In this case, we don't really need the decoder  (i.e. ) and the decoder simply extracts the hidden representation corresponding to the \cls token from  and feeds it into the task-specific structure (e.g. classifier).

\paragraph{Token-level prediction} In the token-level prediction tasks such as the MLM pretraining, SQuAD and sequence labeling, we need a decoder to recover the token-level representations from the compressed sequence .
In many cases,  could simply be the original sequence or a token-level hidden representation of it to provide fine grained low-level information of each token and hence ease the optimization.
In this paper, we utilize the last-layer hidden states of the 1st block (before the first pooling operation) as the additional decoder input.

But for problems that utilize additional input signals, such as the permutation order used for permuted language modeling in XLNet~\cite{yang2019xlnet}.
This additional information can be injected into Funnel-Transformer via the decoder input  to (approximately) recover some more complex control of attention mechanism.

\paragraph{Sequence-to-sequence problems} Another important category of NLP task is sequence-to-sequence problems, including machine translation, text summarization, and dialog generation, whose state-of-the-art solution is the conventional encoder-decoder framework.
Hence, \name naturally fits these tasks, where the decoder input  corresponds to the target text sequence and the encoder input  the source text sequence.
This way, the key difference compared to conventional models is the source side compression Funnel-Transformer provides.

Overall, we summarize some potential directions to extend \name presented in section \ref{sec:proposed} to NLP problems.
Finally, although we focus on discussion on the NLP tasks in this paper, \name  could be applied to any tasks dealing with sequential data, such as time series and video stream analysis.


\section{Experiment Setting and Hyper-parameters}
\label{sec:appendix-hparam}
\subsection{Preprocessing \& Tokenization}
For all experiments conducted in this work, we simply adapt the ``uncased'' word piece model originally used by BERT~\cite{devlin2018bert}, where the vocabulary size is about 30K.
Other than lower case and the default preprocessing included in the word piece tokenizer, the only additional preprocessing we perform is to remove some http symbols (e.g. \texttt{<b>}) in the 7 text classification tasks.

\subsection{Pretraining}
\begin{table}[h!]
	\centering
	\begin{tabular}{lcc}
		\toprule
		\bf Hparam          & \bf Base Scale & \bf Large Scale \\
		\midrule
		Hidden dropout      & \multicolumn{2}{c}{0.1} \\
		GeLU dropout        & \multicolumn{2}{c}{0.0} \\
		Attention dropout   & \multicolumn{2}{c}{0.1} \\
		Max sequence length & \multicolumn{2}{c}{512} \\
		Batch size          & 256  & 8192 \\
		Learning rate       & 1e-4 & 2e-4 \\
		Number of steps     & 1M   & 500K \\
		Warmup steps        & 10K  & 30K \\
		Optimizer 		    & \multicolumn{2}{c}{Adam Weight Decay} \\
		Learning rate decay & \multicolumn{2}{c}{Linear} \\
		Adam epsilon        & \multicolumn{2}{c}{1e-6} \\
		Weight decay        & \multicolumn{2}{c}{0.01} \\
		\bottomrule
	\end{tabular}
	\caption{Hyper-parameters for pretraining.}
	\label{tab:hp-pretrain}
	\vspace{-1em}
\end{table}

The hyper-parameters used for the two different pretraining settings are summarized in Table \ref{tab:hp-pretrain}.
One exception is the learning rate used for B10-10-10H1024 at the base scale. 
Specifically, we find the training can be unstable when the depth goes beyond 24 layers (in the case of B10-10-10H1024) at base scale, especially for the MLM objective.
Hence, we reduce the learning to 8e-5 for the B10-10-10H1024 F-TFM during base-scale pretraining.
This has a side effect of a slower training pace and potentially a slightly worse finetuning performance.
However, we does not observe such instability when the batch size is increased such as in the large-scale setting.

For ELECTRA, there are two additional important hyper-parameters, i.e., the discriminator loss coefficient and the relative size multiplier of the generator.
In this work, we does not tune these two hyper-parameters at all and simply use the numbers from the original paper, i.e., the discriminator loss coefficient of 50 and size multiplier of 1/4 for all architectures trained with ELECTRA.
In addition, in ELECTRA training, whenever F-TFM is used as the discriminator, the generator also uses the F-TFM.

In additional, in the all experiments, we only annotate the size of hidden states the rest of model sizes can be derived from on it:
\begin{itemize}
\item The embedding size = hidden size
\item The size of inner states of P-FFN is ``''.
\item The attention head dimension is always .
\item The number of attention heads is ``''.
\end{itemize}

Finally, another important element in pretraining is the mask sampling strategy.
For MLM training, following previous work, we always complete word span (up to 5 complete words) sampling.
However, for ELECTRA training, we notice a weird phenomenon that under the base-scale setting, the performance of both the Transformer and the F-TFM drops significantly if we use word span sampling rather than the single-token sampling.
On the other hand, under the large-scale setting, using word span sampling works fine.
Hence, we use single-token sampling for base-scale ELECTRA training, and word span sampling for large-scale ELECTRA training.

\subsection{Finetuning}

\begin{table}[h!]
\centering
\begin{tabular}{lcccccccc}
	\toprule
	\bf Hparam & \bf RTE & \bf MRPC	& \bf STS-B	& \bf CoLA & \bf SST-2 & \bf QNLI &\bf MNLI & \bf QQP \\
	\midrule
	Hidden dropout
	& \multicolumn{8}{c}{0.1} \\
	GeLU dropout
	& \multicolumn{8}{c}{0.0} \\
	Attention dropout
	& \multicolumn{8}{c}{0.1} \\
	Max sequence length
	& \multicolumn{8}{c}{128}  \\
	Batch size
	& 16 & 16 & 16 & 16 & 32 & 32 & 64 & 64 \\
	Number of epochs
	& 10 & 10 & 10 & 10 &  5 &  3 &  3 &  5 \\
	Learning rate decay & \multicolumn{8}{c}{Linear} \\
	Weight decay        & \multicolumn{8}{c}{0.01}   \\
	Warmup proportion   & \multicolumn{8}{c}{0.1}   \\
	Adam epsilon        & \multicolumn{8}{c}{1e-6}   \\
	\bottomrule
\end{tabular}
\begin{tabular}{lccccccc}
	\toprule
	\bf Hparam & \bf IMDB & \bf AG & \bf DBpedia & \bf Yelp-2 & \bf Yelp-5 & \bf Amazon-2 & \bf Amazon-5 \\
	\midrule
	Hidden dropout
	& \multicolumn{7}{c}{0.1} \\
	GeLU dropout
	& \multicolumn{7}{c}{0.0} \\
	Attention dropout
	& \multicolumn{7}{c}{0.1} \\
	Max sequence length
	& 512 & 128 & 128 & 512 & 512 & 512 & 512 \\
	Batch size
	& 32 & 32 & 64 & 128 & 128 & 128 & 128 \\
	Number of epochs
	& 5 & 3 & 3 & 3 & 3 & 3 & 3 \\
	Learning rate decay & \multicolumn{7}{c}{Linear} \\
	Weight decay        & \multicolumn{7}{c}{0.01}   \\
	Warmup proportion   & \multicolumn{7}{c}{0.1}   \\
	Adam epsilon        & \multicolumn{7}{c}{1e-6}   \\
	\bottomrule
\end{tabular}
\caption{Hyper-parameters for finetuning on the GLUE benchmark and 7 text classification datasets.}
\label{tab:hp-finetune-glue-textcls}
\end{table}
For all the finetuning experiments, we essentially inherit the hyper-parameters used by XLNet~\cite{yang2019xlnet}.
All the performance numbers reported are obtained on TPUs with TensorFlow 2.2.

\subsubsection{GLUE \& Text Classification}
For GLUE and text classification datasets, we first fix the values of most hyper-parameters shown in Table~\ref{tab:hp-finetune-glue-textcls}.
Then, we only search the learning rates from the set [1e-5, 2e-5, 3e-5], and choose the best one according to the validation set.

Following previous work~\cite{yang2019xlnet,liu2019roberta,clark2020electra}, all GLUE performances correspond to the median result of 5 runs from different random seeds in the base setting and 15 runs in the large setting, respectively.

For the text classification, the base-scale results are the median performance among 5 runs with different random seeds.
However, for the large-scale experiments, to be compatible with previous work~\cite{xie2019unsupervised,yang2019xlnet}, the results are the best performance among 5 random runs.

\subsubsection{Reading Comprehension}
Again, following XLNet~\cite{yang2019xlnet}, the hyper-parameters used for finetuning on the RACE and SQuAD datasets are summarized in Table~\ref{tab:hp-finetune}.
``Layer-wise decay'' means exponentially decaying the learning rates of individual layers in a top-down manner.
For example, suppose the -th layer uses a learning rate , and the Layer-wise decay rate is , then the learning rate of layer  is .
In addition, for the two versions of SQuAD, we simply reuse the model trained on SQuAD v2.0 when evaluated on SQuAD v1.1.
\begin{table}[!h]
\centering
\begin{tabular}{lcc}
	\toprule
	\bf Hparam & \bf RACE & \bf SQuAD \\
	\midrule
	Dropout                 & \multicolumn{2}{c}{0.1}    \\
	Attention dropout       & \multicolumn{2}{c}{0.1}    \\
	Max sequence length     & 512          & 512 \\
	Training epochs/steps   & 5 epochs     & 8000 steps \\
	Warmup proportion/steps & 0.1          & 1000 steps \\
	Batch size              & [16, 32]     & 48 \\
	Learning rate           & [1e-5, 2e-5] & 3e-5 \\
	Learning rate decay     & \multicolumn{2}{c}{linear} \\
	Weight decay            & \multicolumn{2}{c}{0.01}   \\
	Adam epsilon            & \multicolumn{2}{c}{1e-6}   \\
	Layer-wise lr decay     & 1.0 & 0.75 \\
	\bottomrule
\end{tabular}
\caption{Hyper-parameters for RACE and SQuAD.}
\label{tab:hp-finetune}
\end{table}

\section{Additional Experimental Results}
\label{sec:appendix-extra-exp}

\subsection{Text Classification at Large Scale}
\label{sec:appendix-textcls}
\begin{table}[!ht]
	\centering
	\begin{tabular}{lccccccc}
		\toprule
		\bf Model & \bf IMDB & \bf AG & \bf DBpedia & \bf Yelp-2 & \bf Yelp-5 & \bf Amazon-2 & \bf Amazon-5 \\
		\cmidrule(r){1-1} \cmidrule(l){2-8}
		BERT-Large
		& 4.51 & - & 0.64 & 1.89 & 29.32 & 2.63 & 34.17 \\
		ROBERTA-Large
		& 3.50 & - & -    & -    & -     & -    & -     \\
		XLNet-Large
		& \bf 3.20 & \bf 4.45 & 0.64 & 1.37 & \bf 27.05 & 2.11 & 31.67 \\
		B10-10-10H1024
		& 3.36 & 4.66 & \bf 0.60 & \bf 1.33 & 27.14 & \bf 2.10 & \bf 31.64 \\
		B8-8-8H1024
		& 3.42 & 4.96 & 0.63 & 1.39 & 27.20 & 2.14 & 31.74 \\
		\cmidrule(r){1-1} \cmidrule(l){2-8}
		MPNet
		& 4.40 & -  & -     & -   & -   & -     & -  \\
		B6-6-6H768
		& \bf 3.72 & \bf 5.00 & \bf 0.64 & \bf 1.50 & \bf 27.73 \bf & \bf 2.27 & \bf 32.11 \\
		B6-3x2-3x2H768
		& 3.82 & 5.12 & 0.64 & 1.58 & 27.96 & 2.32 & 32.23 \\
		B4-4-4H768
		& 4.12 & 5.09 & 0.67 & 1.70 & 28.40 & 2.35 & 32.46 \\  
		\bottomrule 
	\end{tabular}
	\caption{Text classification performance comparison under the large-scale pretraining.}
	\label{tab:large-scale-textcls}
\end{table}
Table \ref{tab:large-scale-textcls} includes the performance comparison on 7 text classification tasks under the large-scale training setting.
Similar to the GLUE benchmark results, compared with the previous result based on Transformer, with fewer FLOPs, the proposed F-TFM achieves comparable results.

\subsection{Training Cost Comparison}
\label{sec:appendix-training-cost}
In this section, we test the pretraining and finetuning speed of the F-TFM in comparison to the standard Transformer on the TPU and GPU platform.
For the pretraining speed evaluation, we test F-TFM on TPU v3-16 (16 cores x 16Gb) with TensorFlow. 
For the finetuning speed evaluation, we test F-TFM on TPU v2-8 (8 cores x 8Gb) with TensorFlow and on Nvidia-V100 (16Gb) GPU with the PyTorch.
The TensorFlow version is 2.2.0, and the PyTorch version is 1.5.0.
For the GPU experiments, we use an 8-GPU node on the Google Cloud Platform.
All running speeds are reported with the FP16 optimizer.
In the PyTorch implementation, we use ``O2'' options of AMP manager in the apex\footnote{\url{https://github.com/NVIDIA/apex}} package to handle the FP16 optimization.
For finetuning, we consider three different sequence lengths, namely 128, 256 and 512.
For pretraining, we only consider the sequence length 512.
In each case, we choose the maximum possible batch size allowed by the memory size of the device(s). 
We measure the actual model \textit{running time} by performing 1000 steps gradient descent with random input sequences with the fixed length. 

\begin{table}[!ht]
	\small
	\centering
	\begin{tabular}{@{}l|cc|c|cc|c|c|c|c@{}}
		\toprule
		Sequence length & \multicolumn{3}{c|}{128} & \multicolumn{3}{c|}{256} & \multicolumn{2}{c|}{512} & \\
		\midrule
		\multirow{2}{*}{Metrics}
		& \multicolumn{2}{c|}{Run time} & \multirow{2}{*}{Mem} 
		& \multicolumn{2}{c|}{Run time} & \multirow{2}{*}{Mem} 
		& \multicolumn{1}{c|}{Run time} & \multirow{2}{*}{Mem} 
		& \multirow{2}{*}{GLUE} \\
		& 1 GPU & 8 GPUs &  & 1 GPU & 8 GPUs & & 8 GPUs & &  \\
		\midrule\midrule
		Batch size / GPU & \multicolumn{3}{c|}{64} & \multicolumn{3}{c|}{32} & \multicolumn{2}{c|}{16} & \\
		\cmidrule(r){1-1} \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-9} \cmidrule(l){10-10}
		L12H768
		& 1.00x & 1.00x & 9.2G & 1.00x & 1.00x & 11.0G & 1.00x & 14.3G & 84.40 \\
		B6-6-6
		& 0.97x & 0.99x & 9.1G & 0.95x & 0.97x & 10.3G & 0.94x & 12.5G & 85.37 \\
		B6-3x2-3x2
		& 0.93x & 0.93x & 8.4G & 0.91x & 0.92x & 9.5G & 0.90x & 11.8G & 84.78 \\
		B4-4-4
		& 0.67x & 0.67x & 6.6G & 0.65x & 0.66x & 7.5G & 0.64x & 9.0G & 83.99 \\
		\midrule\midrule
		Batch size / GPU & \multicolumn{3}{c|}{32} & \multicolumn{3}{c|}{12} & \multicolumn{2}{c|}{4} & \\
		\cmidrule(r){1-1} \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-9} \cmidrule(l){10-10}
		L24H1024
		& 1.00x & 1.00x & 14.8G & 1.00x & 1.00x & 14.4G & 1.00x & 13.9G & 86.62 \\
		B10-10-10
		& 0.87x & 0.92x & 14.0G & 0.90x & 0.93x & 13.0G & 0.96x & 12.7G & 87.03 \\
		B8-8-8
		& 0.70x & 0.73x & 11.6G & 0.73x & 0.75x & 10.8G & 0.78x & 10.5G & 86.70 \\
		\bottomrule
	\end{tabular}
	\caption{
		Running time and memory consumption comparison between F-TFMs and the standard Transformer on the GPU. 
		In each model group, the standard Transformer (first model) is used as the benchmark for the rest of F-TFM models.
		Note that, given the same batch size per GPU, the memory consumption is roughly the same for 1 GPU and 8 GPUs.
	}
	\label{tab:gpu-time}
\end{table}

\begin{table}[!ht]
\centering
\begin{tabular}{l|c|c|c|c}
	\toprule
	Sequence length & \hspace{0.2in} 128 \hspace{0.2in} & \hspace{0.2in} 256 \hspace{0.2in} & \hspace{0.2in} 512 \hspace{0.2in} & \\
	\midrule
	Metrics
	& \multicolumn{3}{c|}{Run time on 8 TPU cores (TPUv2-8)} & GLUE \\
	\midrule\midrule
	Batch size / TPU core & 64 & 32 & 16 & \\
    \cmidrule(r){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(l){5-5}
	L12H768
	& 1.00x & 1.00x & 1.00x & 84.40 \\
	B6-6-6
	& 0.99x & 0.88x & 0.81x & 85.37 \\
	B6-3x2-3x2
	& 0.97x & 0.87x & 0.77x & 84.78 \\
	B4-4-4
	& 0.69x & 0.62x & 0.55x & 83.99 \\
	\midrule\midrule
	Batch size / TPU core & 16 & 8 & 4 & \\
	\cmidrule(r){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(l){5-5}
	L24H1024
	& 1.00x & 1.00x & 1.00x & 86.62 \\
	B10-10-10
	& 0.89x & 0.81x & 0.73x & 87.03 \\
	B8-8-8
	& 0.66x & 0.60x & 0.56x & 86.70 \\
	\bottomrule
	\end{tabular}
	\caption{
		Running time between F-TFMs and the standard Transformer on the TPU v2-8. 
		In each model group, the standard Transformer (first model) is used as the benchmark for the rest of F-TFM models.
	}
	\label{tab:tpu-time}
\end{table}

Firstly, we compare the model speed in the finetuning stage. 
Note that the decoder is not used in this setting.
Table~\ref{tab:gpu-time} and~\ref{tab:tpu-time} summarize the finetuning running time comparison on GPUs and TPUs, respectively. 
\begin{itemize}[leftmargin=*,itemsep=0em]
\item In the base model (L12H768) group, we observe that the speed of B6-6-6H768 is similar or faster than the base Transformer model, despite the fact that B6-6-6 is deeper, has more parameters. 
Moreover, B6-6-6H768 achieves better results compared with the base Transformer model.
The similar conclusion applies to the B6-3x2-3x2 model, which has the same amount of parameters as the base model.
The B4-4-4 model, which has the same depth and model parameters as the base model, is able to provide 30\%-50\% speedup without losing too much performance. 
\item In the large model (L24H1024) group, the conclusion is similar. The speed of the larger model B10-10-10 is almost the same as the large model, and the speed of B8-8-8 is significantly faster than the large model. In addition, when sequence length equals 512, the acceleration of F-TFM on the TPU  is more obvious than the GPU.
\item In the both groups, all the tested F-TFM variants have smaller memory footprint compared with the standard TFM models, showing the memory efficiency of F-TFM. 
\end{itemize}

Next, we compare the model speed during pretraining under the MLM objective in table \ref{tab:tpu-pre-time}, which has an additional cost due to the decoder.
The results show that the proposed method can still substantially improve the pretraining speed compared to the standard Transformer, though the speed gain is slightly smaller than the finetuning stage.
In summary, this study demonstrates that the proposed method is more efficient in both the finetuning and pretraining stages in modern parallel computing platforms. 
\begin{table}[!ht]
\centering
\begin{tabular}{lcc}
	\toprule
	Sequence Length
	& \multicolumn{2}{c}{512} \\
	\cmidrule(r){1-1} \cmidrule(l){2-3}
	& Running Time & FLOPs \\
	\cmidrule(r){1-1} \cmidrule(l){2-3}
	\#TPU cores / Total bsz 
	& \multicolumn{2}{c}{16 / 512} \\
	\cmidrule(r){1-1} \cmidrule(l){2-3}
	L12H768          & 1.00x & 1.00x \\
	B6-6-6H768D2     & 0.99x & 1.04x \\
	B6-3x2-3x2H768D2 & 0.97x & 1.04x \\
	B4-4-4H768D2     & 0.79x & 0.75x \\
	\midrule
	\#TPU cores / Total bsz
	& \multicolumn{2}{c}{16 / 128} \\
	\cmidrule(r){1-1} \cmidrule(l){2-3}
	L24H1024         & 1.00x & 1.00x \\
	B10-10-10H1024D2 & 0.83x & 0.81x \\
	B8-8-8H1024D2    & 0.71x & 0.66x \\
	\bottomrule
\end{tabular}
\caption{TPU pretraining speed comparison. The suffix ``D2'' means that the F-TFM model has 2 decoder layers.} 
\label{tab:tpu-pre-time}
\end{table} 

\end{document}
