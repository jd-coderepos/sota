\section{Experimental Results}
\label{sec:results}

In this section, we report the experiments carried out to verify the effectiveness of the \gls*{cnn}-based methods in the \dataset dataset and also in public datasets. 
\REV{All experiments were performed on a computer with an AMD Ryzen Threadripper X GHz CPU,  GB of RAM and an NVIDIA Titan Xp GPU (, CUDA cores and  GB of RAM).}

We first assess counter detection since the counter regions used for recognition are from the detection results, rather than cropped directly from the ground truth.
This is done to provide a realistic evaluation of the entire \gls*{amr} system, where well-performed counter detection is essential to achieve outstanding recognition results. 
Next, each approach for counter recognition is evaluated and a comparison between them is presented. 

Counter detection is evaluated in the \dataset dataset and also in two public datasets~\cite{vanetti2013gas,goncalves2016reconhecimento}, while counter recognition is assessed only in the \dataset dataset. This is because (i) two different sets of images were used to evaluate digit segmentation and recognition in Ref.~\citenum{vanetti2013gas}, and thus it is not possible to use these sets in the counter recognition approaches (where these stages are performed jointly); (ii) Ref.~\citenum{goncalves2016reconhecimento} performed digit recognition on a subset of their dataset which was not made publicly available.

We will finally evaluate the entire \gls*{amr} pipeline in a subset of  images () taken from the public dataset introduced by Vanetti et al.~\cite{vanetti2013gas}. This subset, called Meter-Integration, was used to perform an overall evaluation of the \gls*{amr} methods proposed in Refs.~\citenum{vanetti2013gas,gallo2015robust}. It should be noted that other subsets of the dataset, containing different images, were used to evaluate each \gls*{amr} stage independently and the training images (in the overall evaluation) are from these subsets~\cite{vanetti2013gas}. 
Aiming at a fair comparison, we employ the same protocol.

\subsection{Counter Detection}
\label{subsec:results_detection}

For evaluating counter detection, we employ the bounding box evaluation defined in the PASCAL VOC Challenge~\cite{everingham2010pascalvoc}, where the predicted bounding box is considered to be correct if its \gls*{iou} with the ground truth is greater than \% (\gls*{iou}~). 
This metric was also used in previous works~\cite{nodari2011multineural,vanetti2013gas}, being interesting once it penalizes both over- and under-estimated objects.

According to the detection evaluation described above, the network correctly detected \% of the counters with an average \gls*{iou} of , failing to locate the counter in just two images~(/). However,  in these two cases, it is still possible to recognize the digits from the detected counters, since they were actually detected (with \gls*{iou} ) and all digits are within the \gls*{roi} after adding a margin (as explained in Section~\ref{subsec:proposed_detection}). In the validation set, a margin of \% (of the bounding box size) is required so that all digits are within the \gls*{roi}. Thus, we applied a \% margin in the test set as well. Fig.~\ref{fig:detection_margin} shows both cases where the counters were detected with \gls*{iou}~~ before and after adding this margin. Note that, in this way, all counter digits are within the located region using Fast-YOLO. 

\captionsetup[subfigure]{position=bottom}
\begin{figure*}[!htb]
	\begin{center}
	\includegraphics[width=0.85\columnwidth]{Figure5.pdf}
	\end{center}
	\vspace{-3mm}
	\caption{Bounding boxes predicted by the Fast-YOLO model before~(a) and after~(b) adding the margin (\%~of the bounding box~size).}
	\label{fig:detection_margin}    
\end{figure*}

Some detection results achieved by the Fast-YOLO model are shown in Fig.~\ref{fig:results_detection}. 
As can be seen, well-located predictions were attained on counters of different types and under different conditions.

\begin{figure*}[!htb]
	\begin{center}
	\includegraphics[width=0.97\columnwidth]{Figure6.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{Samples of counter detection obtained with the Fast-YOLO model in the \dataset dataset.
	}
	\label{fig:results_detection}    
\end{figure*}


In terms of computational speed, the Fast-YOLO model takes about  ms per image ( \gls*{fps}). The model was trained using the Darknet framework~\cite{darknet13} and the following parameters were used for training the network:  iterations (max batches) and learning rate~=~[\textsuperscript{-},~\textsuperscript{-},~\textsuperscript{-}] with steps at  and  iterations.

\subsubsection{Counter Detection on Public Datasets}

To demonstrate the robustness of Fast-YOLO for counter detection, we employ it on the public datasets found in the literature~\cite{vanetti2013gas,goncalves2016reconhecimento} and compare the results with those reported in previous works.
Vanetti et al.~\cite{vanetti2013gas} employed a subset of  images of their dataset specially for the evaluation of counter detection, being  for training and  for testing. In Ref.~\citenum{goncalves2016reconhecimento}, a larger dataset (with  images) was introduced, but no split protocol was defined.

As the dataset introduced in Ref.~\citenum{vanetti2013gas} has a split protocol, we employed the same division in our experiments. We randomly removed  images from the training set and used them as validation. For the experiments performed in the dataset introduced in Ref.~\citenum{goncalves2016reconhecimento}, we perform -fold cross-validation with images assigned to folds randomly in order to achieve a fair comparison. Thus, in each run, we used  images~() for training and  images~() for each validation and testing, i.e., a  split protocol. 

As mentioned in the related work section, both datasets are composed of gas meter images.
Such a fact is relevant since gas meters usually have red decimal digits that should be discarded in the reading process~\cite{vanetti2013gas,gallo2015robust,goncalves2016reconhecimento,gomez2018cutting}. Therefore, we manually labeled, in each image, a bounding box containing only the significant digits for training Fast-YOLO. These annotations are also publicly available to the research community at \small \url{https://web.inf.ufpr.br/vri/databases/ufpr-amr/}\normalsize.

\begin{table}[!htb]
\caption{F-measure values obtained by Fast-YOLO and previous works in the public datasets found in the literature.}
\label{tab:detection_results}
\begin{center}
\begin{tabular}{@{}ccc@{}}
\toprule
\multirow{2}{*}{Approach} & \multicolumn{2}{c}{F-measure} \\ 
& Dataset~\cite{goncalves2016reconhecimento} & Dataset~\cite{vanetti2013gas} \\ \midrule
Nodari \& Gallo~\cite{nodari2011multineural} &  & \% \\
Gonçalves~\cite{goncalves2016reconhecimento} & \% & \% \\
Vanetti et al.~\cite{vanetti2013gas} &  & \% \\
\textbf{Fast-YOLO} & \% & \% \-10pt]
\REV{Fast-YOLO (\gls*{iou}  )} & \REV{\%} & \REV{\%} \\
\bottomrule
\end{tabular}\end{center}
\end{table}

The Fast-YOLO model correctly detected \% of the counters in both datasets, outperforming the results obtained in previous works, as shown in Table~\ref{tab:detection_results}. 
It is noteworthy the outstanding \gls*{iou} values attained: on average \% in the dataset proposed in Ref.~\citenum{vanetti2013gas} and \% in the dataset introduced in Ref.~\citenum{goncalves2016reconhecimento}. 
We believe that these excellent results are due to the fact that, in these datasets, the counter occupies a large portion of the image and the meters/counters are quite similar when comparing with the \dataset dataset. Fig.~\ref{fig:detection_public} shows a counter from each dataset detected using Fast-YOLO.

\begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=0.91\columnwidth]{Figure7.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{Examples of counter detection obtained with the Fast-YOLO model. Note that the counter region in the images of the Dataset~\cite{goncalves2016reconhecimento} (left) and Dataset~\cite{vanetti2013gas} (right) is quite larger than in the \dataset dataset.} 
	\label{fig:detection_public}    
\end{figure}

\REV{Additionally, we reported the result with a higher detection threshold (i.e., \gls*{iou}~~). It is remarkable that more than \% of the counters were located with an \gls*{iou} (with the ground truth) greater than  in both datasets. We noticed that the detections with a lower \gls*{iou} occurred mainly in cases where the meter/counter was inclined or tilted, as illustrated in Fig.~\ref{fig:detection_public_lower_iou}.
}


\begin{figure}
    \begin{center}
	\includegraphics[width=0.91\columnwidth]{Figure8.pdf}
	\end{center}	
	\vspace{-3mm} 
    \caption{Samples of counters detected with a lower \gls*{iou} with the ground truth. (a) and (b) show images of the datasets proposed in Refs.~\citenum{goncalves2016reconhecimento} and~\citenum{vanetti2013gas}, respectively. The predicted position and ground truth are outlined in red and green, respectively. Observe that all digits would be within the \gls*{roi} with the addition of a small margin.}
    \label{fig:detection_public_lower_iou}
\end{figure}

\subsection{Counter Recognition}
\label{subsec:results_recognition}

For this experiment, \REV{we report the mean of  runs for both digit and counter recognition accuracy.} While the former is the number of correctly recognized digits divided by the number of digits in the test set, the latter is defined as the number of correctly recognized counters divided by the test set size, since each image has only a single meter/counter. 
Additionally, all \gls*{cnn} models were trained with and without data augmentation, so that we can analyze how data augmentation (described in Section~\ref{subsubsec:data_augmentation}) affects the performance of each model.

For a fair comparison, we (i) generated , images and applied them for training all \glspl*{cnn} (more images were not generated due to hardware limitations); (ii) disabled the Darknet’s (hence CR-NET’s) built-in data augmentation, which creates a number of images with changed colors (hue, saturation, exposure) randomly cropped and resized; and (iii) evaluated different margin values (less than the \% applied previously) in the predictions obtained by Fast-YOLO, since each approach might work better with different margin values.

The recognition rates achieved by all models are shown in Table~\ref{tab:results_recognition}. \REV{We performed statistical paired t-tests at a significance level , which showed that there is a significant difference in the results obtained with different models.} As expected, the results are greatly improved when taking advantage of data augmentation. The best results were achieved with the CR-NET model, which correctly recognized \REV{\%} of the counters with data augmentation against \REV{\%} and \REV{\%} through \gls*{crnn} and Multi-Task Learning, respectively. This suggests that segmentation-free approaches require a lot of training data to achieve promising recognition rates, as in Ref.~\citenum{gomez2018cutting} where , images were used for training.

\begin{table}[!htb]
\caption{Recognition rates obtained in the \dataset dataset using Fast-YOLO for counter detection and each of the \gls*{cnn} models for counter recognition.}
\label{tab:results_recognition}
\vspace{-2mm}
\begin{center}
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Approach}} & \multicolumn{2}{c}{Accuracy (\%)} \\
\multicolumn{1}{c}{} & Digits & Counters \\ \midrule
Multi-Task (original training set) & \REV{} & \REV{} \\
\gls*{crnn} (original training set) & \REV{} &  \REV{} \\
CR-NET (original training set) & \REV{} & \REV{} \\ \midrule
Multi-Task (with data augmentation) & \REV{} & \REV{} \\
\gls*{crnn} (with data augmentation)  & \REV{} & \REV{} \\
\textbf{CR-NET (with data augmentation)} & \REV{} & \REV{} \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

It is important to highlight that it was not possible to recognize any counter when training the Multi-Task model without data augmentation. \REV{We performed several experiments reducing the size of the Multi-Task network to verify if a smaller network could learn a better discriminant function. However, better results were not achieved.}
This is because the dataset is biased and so is the recognition.
\REV{Even though the first digit has the strongest bias (given the large amount of  and s in that position), the other digits still have a considerable bias due to the low number of training samples. For example, the Multi-Task network may learn to predict the last digit/task as `' on every occasion it sees a particular combination of the other digits that is present in the training set. In other words, the network may learn correlations between the outputs that do not exist in practice (in other applications this may be beneficial, but in this case it is not). Such a fact explains why the segmentation-free approaches had a higher performance gain with data augmentation, which balanced the training set and eliminated the undesired correlation between the outputs.}

To assess the speed/accuracy trade-off of the three \gls*{cnn} models, we list in Table~\ref{tab:results_recognition_fps} the time required for each approach to perform the recognition stage. We report the \gls*{fps} rate achieved by each approach considering only the recognition stage and also considering the detection stage (in parenthesis), which takes about  ms per image using Fast-YOLO. 
The reported time is the average time spent processing all images, assuming that the network weights are already loaded. \MINOR{For completeness, for each network, we also list the number of parameters as well as the number of \gls*{bflop} required for a single forward pass over a single image.}

\begin{table}[!htb]
\caption{Results obtained in the \dataset dataset and the computational time required for each approach to perform counter recognition. In parentheses is shown the \gls*{fps} rate when considering the detection stage.}
\label{tab:results_recognition_fps}
\vspace{-2mm}
\begin{center}
\begin{tabular}{@{}cccccc@{}}
\toprule
Approach & \MINOR{\acrshort*{bflop}} & \MINOR{Parameters} & Time (ms) & \gls*{fps} & Accuracy (\%)\\ \midrule 
Multi-Task & \MINOR{} & \MINOR{M} &  &  () & \REV{} \\
\gls*{crnn} & \MINOR{} & \MINOR{M} &  &  () & \REV{} \\ 
CR-NET & \MINOR{} & \MINOR{M} &  &  () & \REV{} \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

The CR-NET and Multi-Task approaches achieved impressive \gls*{fps} rates. Looking at Table~\ref{tab:results_recognition_fps}, the difference between using each one of them is clear. The CR-NET model achieved an accuracy of \REV{\%} at  \gls*{fps}, while the Multi-Task model was capable of processing  \gls*{fps} with a recognition rate of \REV{\%}. When considering the time spent in the detection stage, it is possible to process  and  \gls*{fps} using the CR-NET and Multi-Task models, respectively. 

\MINOR{It is worth noting that: 
(i)~even though the Multi-Task network has many more parameters than CR-NET and \gls*{crnn}, it is still the fastest one;
(ii)~the \gls*{crnn} model requires a lower number of floating-point operations for a single forward pass than the CR-NET and Multi-Task networks, however, it is still the model that takes more time to process a single image.
In this sense, we claim that there are several factors (in addition to those mentioned above) that affect the time it takes for a network to process a frame, e.g., the input size, its specific characteristics and the framework in which it is implemented. For example, two networks may require exactly the same number of floating-point operations (or have the same number of parameters) and still one be much faster than the other.}
Although much effort was made to ensure fairness in our experiments, the comparison might not be entirely fair since we used different frameworks to implement the networks and there are probably differences in implementation and optimization between them. The CR-NET model was trained using the Darknet framework~\cite{darknet13}, whereas the \gls*{crnn} and Multi-Task models were trained using PyTorch~\cite{paszke2017automatic} and Keras~\cite{chollet2015keras}, respectively.

Fig.~\ref{fig:results_dataset} illustrates some of the recognition results obtained in the \dataset dataset when employing the CR-NET model (i.e., the one with the best accuracy). It is noticeable that the model is able to generalize well and correctly recognize counters from meters of different types and in different conditions. Regarding the errors, we noticed that they occurred mainly due to rotating digits and artifacts in the counter region, such as reflections and dirt.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.86\columnwidth]{Figure9.pdf}
	\vspace{1mm}
	\caption{Results obtained by the CR-NET model in the \dataset dataset. The first three rows show examples of successfully recognized counters, while the last two rows show samples of incorrectly recognized counters. Some images were slightly resized for display purposes.}
	\label{fig:results_dataset}  
	\vspace{-4mm} 
\end{figure}

\subsection{Overall Evaluation on the Meter-Integration Subset}

The Meter-Integration subset~\cite{vanetti2013gas} was used to evaluate the \gls*{amr} methods proposed in Refs.~\citenum{vanetti2013gas,goncalves2016reconhecimento}. Thus, we decided to perform experiments on this dataset and compare the results with those obtained in both works. As previously mentioned, the training images are from other subsets of the dataset proposed in Ref.~\citenum{vanetti2013gas}. Remark that there are only  and  training images for counter detection and recognition, respectively.

\REV{We employ only the CR-NET model in this experiment since it outperformed both Multi-Task and \gls*{crnn} models in the \dataset dataset. The mean accuracy of  runs is reported for both digit and counter recognition accuracy.} As the counters in the training set have from  to  digits and not a fixed number of digits, we adopted a  confidence threshold (we report it for sake of reproducibility) to deal with a variable number of digits, instead of always considering~ digits per counter.
This threshold was chosen based on  validation images (i.e., \%) randomly taken from the training set.
Table~\ref{tab:results_overall} shows the results obtained in previous works and using the Fast-YOLO and CR-NET networks for counter detection and recognition, respectively.

\vspace{2mm}
\begin{table}[!htb]
\caption{Results obtained in the Meter-Integration subset by previous works and using Fast-YOLO \& CR-NET.}
\label{tab:results_overall}
\vspace{-2mm}
\begin{center}
\begin{tabular}{@{}ccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Approach}} & \multicolumn{2}{c}{Accuracy (\%)} \\
\multicolumn{1}{c}{} & Digits & Counters \\ \midrule
Gallo et al.~\cite{gallo2015robust} (original training set) &  &  \\
Vanetti et al.~\cite{vanetti2013gas} (original training set) &  &  \\
Fast-YOLO \& CR-NET (original training set) & \REV{} & \REV{} \\ 
\textbf{Fast-YOLO \& CR-NET (data augmentation)} & \REV{} & \REV{} \\ \bottomrule 
 \end{tabular}\end{center}
\end{table}
\vspace{-5mm}

As expected, the recognition rate accomplished by our deep learning approach
was considerably better than those obtained in previous works (\% ), which employed methods based on conventional image processing with handcrafted features. It is noteworthy the ability of both Fast-YOLO and CR-NET models to generalize with very few training images in each stage, i.e.,  for counter detection and  for counter recognition. 
 
The results were improved when using data augmentation, as in the experiments carried out on the \dataset dataset.
The accuracy achieved was \REV{\%}, significantly outperforming the baselines.
\REV{It is worth noting that, on average, only - counters} were incorrectly classified and \REV{generally} the error occurred in the rightmost digit of the counter. 
Two samples of errors are shown in Fig.~\ref{fig:result_meter_integration}: the last digit  was incorrectly labeled as  in one of the cases, probably due to some noise in the image, while in the other case the last digit was detected/recognized with confidence lower than , apparently due to the m text touching the digit (there were no similar examples in the training~set). 

\begin{figure}[!htb]
	\begin{center}
	\includegraphics[width=0.81\columnwidth]{Figure10.pdf}
    \end{center}
    \vspace{-1mm}
    \caption{Incorrect readings obtained with the Fast-YOLO \& CR-NET approach, where the last digit was incorrectly classified (left), and the last digit was detected/recognized with a confidence value below the threshold (right).}
	\label{fig:result_meter_integration}  
\end{figure}