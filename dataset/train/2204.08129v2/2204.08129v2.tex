\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{url}
\usepackage{mathrsfs}
\usepackage[accsupp]{axessibility}
\usepackage{import}
\usepackage{bm}
\usepackage{float}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{algpseudocode}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{5075} \def\confName{CVPR}
\def\confYear{2022}

\begin{document}

\title{Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding}

\author{
Xun Long Ng{\textsuperscript{‡}}\quad 
Kian Eng Ong{\textsuperscript{‡}}\quad
Qichen Zheng{\textsuperscript{‡}}\quad
Yun Ni{\textsuperscript{‡}}\quad 
Si Yong Yeo \quad
Jun Liu\thanks{Corresponding author.} \\
\normalsize Information Systems Technology and Design, 
Singapore University of Technology and Design, Singapore
\\
{\tt\scriptsize \{xunlong\_ng, kianeng\_ong\}@mymail.sutd.edu.sg} ~~~~~~~
{\tt\scriptsize \{qichen\_zheng, ni\_yun, siyong\_yeo, jun\_liu\}@sutd.edu.sg}
}

\maketitle

\def\thefootnote{{\textsuperscript{‡}}}\footnotetext{These authors contribute equally to this work.}

\begin{abstract}
    \vspace{-0.3cm}
    
    Understanding animals' behaviors is significant for a wide range of applications. However, existing animal behavior datasets have limitations in multiple aspects, including limited numbers of animal classes, data samples and provided tasks, and also limited variations in environmental conditions and viewpoints. To address these limitations, we create a large and diverse dataset, Animal Kingdom, that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors. The wild animal footages used in our dataset record different times of the day in extensive range of environments containing variations in backgrounds, viewpoints, illumination and weather conditions. More specifically, our dataset contains 50 hours of annotated videos to localize relevant animal behavior segments in long videos for the video grounding task, 30K video sequences for the fine-grained multi-label action recognition task, and 33K frames for the pose estimation task, which correspond to a diverse range of animals with 850 species across 6 major animal classes. Such a challenging and comprehensive dataset shall be able to facilitate the community to develop, adapt, and evaluate various types of advanced methods for animal behavior analysis. Moreover, we propose a Collaborative Action Recognition (CARe) model that learns general and specific features for action recognition with unseen new animals. This method achieves promising performance in our experiments.
    Our dataset can be found at \tt\footnotesize{\url{https://sutdcv.github.io/Animal-Kingdom}}.
    \end{abstract}
    
    
    \vspace{-0.5cm}
    
    \section{Introduction}
    
    
    A better understanding of how animals behave and move in the wild is not only a cornerstone of behavioral sciences \cite{graving2019deepposekit} but also crucial for a wide range of applications. Animal behavioral analysis plays a pivotal role in conservation efforts \cite{singh2020animal, ani11020485} and wildlife management \cite{nguyen2017animal}. Each year, the world spends at least \\times\times\times\checkmark\times\times\times\times\times\times\times\times\times\times\times\times\times\times\times\times\checkmark\times\times\times\times\times\checkmark\checkmark\times\times\times\times\checkmark\checkmark\times\times\times\times\times\times\times\times\times\checkmark\times\times\times\times\times\checkmark\checkmark\times\times\times\times\checkmark\checkmark\times\times\checkmark\times\times\checkmark\times\times\times\times\times\times\times\times\checkmark\checkmark\checkmark\times\checkmark\checkmark\times\times\times\times\checkmark\times\times\times\times\times\times\times\times\times\checkmark\times\times\times\times\times\times\times\times\times\times\times\times\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\times\times\times\checkmark\checkmark\checkmark\times\checkmark\checkmark\times\checkmark\checkmark\checkmark\checkmark\times\times\times\times\times\times\times\times\times\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\times\times\times\times\checkmark\times\checkmark\checkmark\checkmark\times\times\times\times\times\times\times\times\times\times\times\checkmark\times\times\checkmark\times\times\times\times\times\times\times\checkmark\checkmark\times\times\times\times\times\times\times\times\times\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\times\checkmark\checkmark\times\checkmark\times\checkmark\checkmark\checkmark\times\times\times\times\times\times\times\times\times\checkmark\checkmark\checkmark\checkmark\checkmark\times\times\times\times\times\checkmark\times\checkmark\checkmark\checkmark\times\times\times\times\times\times\times\times\checkmark\times\times\times\times\times\times\times\times\times\times\times\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmark\checkmarkKCK\textbf{D}=\{D_k\}_{k=1}^{K}D_k=\{(x^k_n,y^k_n)\}_{n=1}^{N_k}x^k_ny^k_nN_kkE{F_{gen}}\{F^k_{specific}\}_{k=1}^{K}CEf_{base}=E(x)x{F_{gen}}f_{gen}={F_{gen}}(f_{base})F^k_{specific}f^k_{specific} = F^k_{specific}(f_{base})\textbf{D}x_n^kC\hat{y}_n^k\ell_\textrm{CE}KF_{gen}kF^k_{specific}KF^k_{specific}KRxKxEf_{base}K\{f^k_{specific}\}_{k=1}^K\textbf{w}=R\big(f_{base}, \{f^k_{specific}\}_{k=1}^K \big)\textbf{w} = \{w^k\}_{k=1}^KKxKw^kw^k \in \mathbb{R}^{h \times w}h \times wKC\textbf{D}RRRKK-1RR\textbf{w}\textbf{w}(\%)n\mun\mu\mun\munn\mu\in \{0.1,0.3,0.5,0.7\}1kk=12\alpha\alpha\alphak$-out    & 39.30 & 40.28                 \\
    \midrule
    \multirow{5}{*}{Protocol 3} & Mammals        & 61.59 & 62.50               \\
                       & Amphibians     & 56.74 & 57.85                 \\
                       & Reptiles       & 56.06 & 57.06               \\
                       & Birds          & 77.35 & 77.41                \\
                       & Fishes         & 68.25   & 69.96                   \\
    
    
    
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.5cm}
    \end{table}
    
    
    
    \section{Conclusion}
    
    We introduce a challenging animal behavioral analysis dataset, comprising over 850 species of animals in the animal kingdom for video grounding, action recognition, and pose estimation. We also introduce a Collaborative Action Recognition (CARe) model with improved ability to recognize actions of unseen new animal types. The variations within and between classes of animals for both action recognition and pose estimation illustrate the advantage of our dataset being diverse in nature, and yet at the same time highlight the challenges and reinforce the compelling need for further research in the areas of video grounding, animal action recognition and pose estimation. We expect that our work will inspire further works in animal behavioral analysis, and understanding animal behaviors can make our world a better place for both wildlife and humans.
    
    
    
    \setlength{\parindent}{0em}\setlength{\baselineskip}{0.1em}{
    {\scriptsize{
    \textbf{Acknowledgements.}
    We would like to thank Ang Yu Jie, Ann Mary Alen, Cheong Kah Yen Kelly, Foo Lin Geng, Gong Jia, Heng Jia Ming, Javier Heng Tze Jian, Javin Eng Hee Pin, Jignesh Sanjay Motwani, Li Meixuan, Li Tianjiao, Liang Junyi, Loy Xing Jun, Nicholas Gandhi Peradidjaya, Song Xulin, Tian Shengjing, Wang Yanbao, Xiang Siqi, and Xu Li for working on the annotations and conducting the quality checks for video grounding, action recognition and pose estimation. This project is supported by AI Singapore (AISG-100E-2020-065), National Research Foundation Singapore, and SUTD Startup Research Grant.}}}
    
    
    
    {\small
    \bibliographystyle{ieee_fullname}
    \bibliography{main.bib}
    }
    \end{document}
