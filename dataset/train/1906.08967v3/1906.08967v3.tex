\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}




\usepackage[preprint]{neurips_2019}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{amsmath} \usepackage{mathptmx} \setcitestyle{square}
\setcitestyle{comma}
\setlength{\belowcaptionskip}{-0.5cm}
\usepackage{multirow}
\usepackage{listings}
\usepackage{array}
\usepackage[dvipsnames]{xcolor}
\title{Deep RGB-D Canonical Correlation Analysis For Sparse Depth Completion}
\author{Yiqi Zhong\thanks{Both authors contributed equally to this work.} \\
  University of Southern California\\
  Los Angeles, California \\
  \texttt{yiqizhon@usc.edu} \\
  \And
  Cho-Ying Wu\footnotemark[1]\\
  University of Southern California\\
  Los Angeles, California \\
  \texttt{choyingw@usc.edu } \\
  \And
  Suya You \\
  US Army Research Laboratory\\
  Playa Vista, California \\
  \texttt{suya.you.civ@mail.mil} \\
  \And
  Ulrich Neumann\\
  University of Southern California\\
  Los Angeles, California \\
  \texttt{uneumann@usc.edu} \\
}       
\begin{document}

\maketitle

\begin{abstract}


In this paper, we propose our Correlation For Completion Network (CFCNet), an end-to-end deep learning model that uses the correlation between two data sources to perform sparse depth completion. CFCNet learns to capture, to the largest extent, the semantically correlated features between RGB and depth information. Through pairs of image pixels and the visible measurements in a sparse depth map, CFCNet facilitates feature-level mutual transformation of different data sources. Such a transformation enables CFCNet to predict features and reconstruct data of missing depth measurements according to their corresponding, transformed RGB features. We extend canonical correlation analysis to a 2D domain and formulate it as one of our training objectives (i.e. 2d deep canonical correlation, or â€œ2D$^2$CCA loss"). Extensive experiments  validate the ability and flexibility of our CFCNet compared to the state-of-the-art methods on both indoor and outdoor scenes with different real-life sparse patterns. Codes are available at: https://github.com/choyingw/CFCNet.
\end{abstract}
\section{Introduction}

Depth measurements are widely used in computer vision applications \cite{tateno2017cnn,wang2018depth,NIPS2019_8340}. However, most of the existing techniques for depth capture produce depth maps with incomplete data. For example, structured-light cameras cannot capture depth measurements where surfaces are too shiny; Visual Simultaneous Localization And Mappings (VSLAMs) are not able to recover depth of non-textured objects; LiDARs produce semi-dense depth map due to the limited scanlines and scanning frequency. Recently, researchers have introduced the sparse depth completion task, aiming to fill missing depth measurements using deep learning based methods \cite{Ma2017SparseToDense,shivakumar2019dfusenet,uhrig2017sparsity,qiu2018deeplidar, chen2018estimating,yang2019dense,zhang2019dfinenet}. Those studies produce dense depth maps by fusing features of sparse depth measurements and corresponding RGB images. However, they usually treat feature extraction of these two types of information as independent processes, which in reality turns the task they work on into "multi-modality depth prediction" rather than "depth completion." While the multi-modality depth prediction may produce dense outputs, they fail to fully utilize observable data. The depth completion task is unique in that part of its output is already observable in the input. Revealing the relationship between data pairs (i.e. between observable depth measurements and the corresponding image pixels) may help complete depth maps by emphasizing the information from image domain at the locations where the depth values are non-observable.

\begin{figure}[bt!]
    \centering
    \includegraphics[scale=0.4]{figure/introduction_horizontal.png}
    \caption{Sample results of CFCNet on different sparse patterns. We show in the order of RGB images, sparse depth maps with different sparse patterns, and dense depth maps completed by CFCNet. For the stereo pattern and the ORB pattern, we show the depth groundtruth in the last column.}
    \label{fig:introduction}
\end{figure}

To accomplish the depth completion task from a novel perspective, we propose an end-to-end deep learning based framework, Correlation For Completion Network (CFCNet). We view a completed dense depth map as composed of two parts. One is the sparse depth which is observable and used as the input, another is non-observable and recovered by the task. Also, the corresponding full RGB image of the depth map can be decomposed into two parts, one is called the sparse RGB, which holds the corresponding RGB values at the observable locations in the sparse depth. The other part is complementary RGB, which is the subtraction of the sparse RGB from the full RGB images. See Figure \ref{fig:network} for examples. During the training phase, CFCNet learns the relationship between sparse depth and sparse RGB and uses the learned knowledge to recover non-observable depth from complementary RGB.

To learn the relationship between two modalities, we propose a 2D deep canonical correlation analysis (2D$^2$CCA). In the proposed method, our 2D$^2$CCA tries to learn non-linear projections where the projected features from RGB and depth domain are maximally correlated. Using 2D$^2$CCA as an objective function, we could capture the semantically correlated features from the RGB and depth domain. In this fashion, we utilize the relationship of observable depth and its corresponding non-observable locations of the RGB input. We then use the joint information learned from the input data pairs to output a dense depth map. The pipeline of our CFCNet is shown in Figure \ref{fig:network}. Details of our method are described in Section \ref{approach}. The main contributions of CFCNet can be summarized as follows.
\begin{itemize}
\setlength\itemsep{0cm}
    \item Constructing a framework for the sparse depth completion task which leverages the relationship between sparse depth and its corresponding RGB image, using the complementary RGB information to complement the missing sparse depth information. 
    \item Proposing the 2D$^2$CCA which forces feature encoders to extract the most similar semantics from multiple modalities. Our CFCNet is the first to apply the two-dimensional approach in CCA with deep learning studies. It overcomes the small sample size problem in other CCA based deep learning frameworks on modern computer vision tasks.
    \item Achieving state-of-the-art of the depth completion on several datasets with a variety of sparse patterns that serve real-world settings.
\end{itemize}

\section{Related Work}
\label{RelatedWork}
\textbf{Sparse Depth Completion} is a task that targets at dense depth completion from sparse depth measurements and a corresponding RGB image. The nature of sparse depth measurements varies across scenarios and sensors. Sparse depth generated by the stereo method contains more information on object contours and less information on non-textured areas \cite{forsyth2003modern}. LiDAR sensors produce structured sparsity due to the scanning behavior \cite{geiger2012we}. Feature based SLAM systems (such as ORB SLAM \cite{mur2015orb}) only capture depth information at the positions of corresponding feature points. Besides these most popular three patterns, some other patterns have also been studied. For instance, \cite{liao2017parse} uses a line pattern to simulate partial observations from laser systems; \cite{zhang2018deep} culls the depth data of shiny surfaces area out of the dense depth map to mimic commodity depth cameras' output. \cite{chen2018estimating} uses uniform grid patterns. The latter appears a simplified and artificial pattern. Real-life situations require a more practical tool.

As for input sparsity, \cite{Ma2017SparseToDense} stacks sparse depth maps and corresponding RGB images together to build a four-channel (RGB-D) input before fed into a ResNet based depth estimation network. This treatment produces better results than monocular depth estimation with only RGB images. Other studies involve a two-branch encoder-decoder based framework which is similar to those used in RGB-D segmentation tasks \cite{yang2019dense,zhang2019dfinenet,imran2019depth,Jaritz_2018}. Their approaches do not apply special treatments to the sparse depth branch. They work well on the dataset where sparsity is not extremely severe, e.g. KITTI depth completion benchmark \cite{uhrig2017sparsity}. In most of the two-branch frameworks, features from different sources are extracted independently and fused through direct concatenations or additions, or using features from RGB branch to provide an extra guidance to refine depth prediction results.

\textbf{Canonical Correlation Analysis} is a standard statistical technique for learning the shared subspace across several original data spaces. For two modalities, from the shared subspace, each representation is the most predictive to the other representation and the most predictable by the other \cite{hotelling1992relations,anderson1962introduction}. To overcome the constraints of traditional CCA where the projections must be linear, deep canonical correlation analysis (DCCA) \cite{andrew2013deep,wang2015deep} has been proposed. DCCA uses deep neural network to learn more complex non-linear projections between multiple modalities. CCA, DCCA, and other variants have been widely used on multi-modal representation learning problems \cite{ding2016comprehensive,yang2017canonical,wang2015unsupervised,kan2016multi,yan2015deep,mroueh2015deep,katsurai2016image}.

The one-dimensional CCA method suffers from the singularity problem of covariance matrices in the case of high-dimensional space with small sample size (SSS). Existing works have extended CCA to a two-dimensional way to avoid the SSS problem. \cite{kukharev2010application,zou20072dcca,lee2007two} use a similar approach to building full-rank covariance matrices inspired by 2DPCA \cite{yang2004two} and 2DLDA \cite{li2004novel} on the face recognition task. However, those studies do not approximate complex non-linear projections as \cite{andrew2013deep, wang2015deep} attempt. Our CFCNet is the first to integrate two-dimensional CCA into deep learning frameworks to overcome the intrinsic problem of applying DCCA to modern computer vision tasks, detailed in Section \ref{2D2CCA}.

\section{Our Approach}
\label{approach}

Our goal is to leverage the relationship of the sparse depth and their corresponding pixels in RGB images in order to optimize the performance of the depth completion task. We try to complement the missing depth components using cues from RGB domain. Since CCA could learn the shared subspace with its predictive characteristics, we estimate the missing depth component using features from RGB domain through CCA. However, traditional CCA has SSS problem in modern computer vision task, detailed in Section \ref{2D2CCA}. We further propose the 2D$^2$CCA to capture similar semantics from both RGB/depth encoders. After encoders learning the semantically similar features, we use a transformer network to transform features from RGB to depth domain. This design not only enables the reconstruction of missing depth features from complementary RGB information but also ensures semantics similarity and the same numerical range of the two data sources. Based on this structure, the decoder in CFCNet is capable of using the reconstructed depth features along with the observable depth features to recover the dense depth map.

\subsection{Network Architecture}
\label{Network}
Proposed CFCNet structure is in Figure \ref{fig:network}. CFCNet takes in sparse depth map, sparse RGB, and complementary RGB. We use our Sparsity-aware Attentional Convolutions (SAConv, as shown in Figure \ref{fig:spconv}) in VGG16-like encoders. SAConv is inspired by local attention mask \cite{harley_segaware}. Harley \textit{et al.} \cite{harley_segaware} introduces the segmentation-aware mask to let convolution operators "focus" on the signals consistent with the segmentation mask. In order to propagate information from reliable sources, we use sparsity masks to make convolution operations attend on the signals from reliable locations. Difference of our SAConv and the local attention mask is that SAConv does not apply mask normalization. We avoid mask normalization because it affect the stability of our later 2D$^2$CCA calculations due to the numerically small extracted features it produces after several times normalization. Also, similar to \cite{uhrig2017sparsity}, we use maxpooling operation on masks after every SAConv to keep track of the visibility. If there is at least one nonzero value visible to a convolutional kernel, the maxpooling would evaluate the value at the position to 1.

Most multi-modal deep learning approaches simply concatenate or elementwisely add bottleneck features. However, when the extracted semantics and range of feature value differs among elements, direct concatenation and addition on multi-modal data source would not always yield better performance than single-modal data source, as seen in \cite{ngiam2011multimodal, Jaritz_2018}. To avoid this problem. We use encoders to extract higher-level semantics from two branches. We propose 2D$^2$CCA, detailed in \ref{2D2CCA}, to ensure the extracted features from two branches are maximally correlated. The intuition is that we want to capture the same semantics from the RGB and depth domains. Next, we use a transformer network to transform extracted features from RGB domain to depth domain, making extracted features from different sources share the same numerical range. During the training phase, we use features of sparse depth and corresponding sparse RGB image to calculate the 2D$^2$CCA loss and transformer loss.

We use a symmetric decoder structure to decode the embedded features. For the input, we concatenate the sparse depth features with the reconstructed missing depth features. The reconstructed missing depth features are extracted from complementary RGB image through the RGB encoder and the transformer. To ensure single-stage training, we adopt weight-sharing strategies as shown in Figure \ref{fig:network}.

\begin{figure}[bt!]
    \centering
    \includegraphics[scale=0.4]{figure/network_revised.png}
    \caption{Our network architecture. Here $\oplus$ is for concatenation operation. The input 0 - 1 sparse mask represents the sparse pattern of depth measurements. The complementary mask is complementary to the sparse mask. We separate a full RGB image into a sparse RGB and a complementary RGB by the mask and feed them with masks into networks.}
    \label{fig:network}
\end{figure}

\begin{figure}[tbp]
\centering
\begin{minipage}[t]{0.4\textwidth}
    \centering
    \includegraphics[scale=0.4]{figure/spconv.png}
    \caption{Our SAConv. The $\odot$ is for Hadamard product. The $\otimes$ is for convolution. The $+$ is for elementwise addition. The kernel size is $3\times3$ and stride is 1 for both convolution and maxpooling.}
    \label{fig:spconv}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[t]{0.5\textwidth}
      \centering
    \includegraphics[scale=0.25]{figure/sparsifier.png}
    \caption{Samples of sparsified depth maps for experiments with different sparsifiers. (a) The source dense depth map from NYUv2. (b) With uniform sparsifier (500 points). (c) With stereo sparsifier (500 points). (d) With ORB sparsifier. }
    \label{fig:sparsifier_sample}
\end{minipage}
\end{figure}

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.3]{figure/heat_comp.png}
    \caption{The visualizations of $\mathit{F_{I}}$ and $\mathit{F_{D}}$ using an example from NYUv2 dataset. Visuals are heat maps of the extracted features. Brighter color means a larger feature value. The values within a single map were normalized to [0, 1]. (a) The feed-forward heat maps at the first iteration. (b) The feed-forward heat maps after being trained with 10000 iterations. The figure shows the heat maps of the first 6 channels. The numbers under the heat maps represent the channel numbers. The example demonstrates that the 2D$^2$CCA is able to capture similar semantics from different sources.}
    \label{fig:heat_comp}
\end{figure}

\subsection{2D Deep Canonical Correlation Analysis (\textit{2D$^2$CCA)}}
\label{2D2CCA}
Existing CCA based techniques introduced in Section \ref{RelatedWork} have limitations in modern computer vision tasks. Since modern computer vision studies usually use very deep networks to extract information from images of relatively large resolution, the batch size is limited by GPU-memory use. Meanwhile, the latent feature representations in networks are high-dimensional, since the batch size is limited, using DCCA with one-dimensional vector representation would lead to SSS problem. Therefore, We propose a novel 2D deep canonical correlation analysis($2D^2 CCA$) to overcome the limitations.

We denote the completed depth map as $\mathit{D}$ with its corresponding RGB image as $\mathit{I}$. Sparse depth map in the input and the corresponding sparse RGB image are denoted as $\mathit{sD}$ and $\mathit{sI}$. RGB/Depth encoders are denoted as $\boldsymbol{f_{I}}$ and $\boldsymbol{f_{D}}$ where the parameters of the encoders are denoted as $\theta_{I}$ and $\theta_{D}$ respectively. As described in Section \ref{Network}, $\boldsymbol{f_{I}}$ and $\boldsymbol{f_{D}}$ use the SAConv to propagate information from reliable points to extract features from sparse inputs. We generate 3D feature grids embedding pair ( $\mathit{F_{sD}}\in \mathit{\mathbb{R}^{m\times n\times C }} $, $\mathit{F_{sI}}\in \mathit{\mathbb{R}^{m\times n\times C }} $) for each sparse depth map/image pair $\left(\mathit{sD},\mathit{sI}\right )$ by defining $\mathit{F_{sD}} = \boldsymbol{ f_{D}}\left(\mathit{sD} ;  \theta_{D}\right)$ and $\mathit{F_{sI}} = \boldsymbol{ f_{I}}\left(\mathit{sI} ;  \theta_{I}\right)$. Inside each feature grid pair, there are $\mathit{C}$ feature map pairs $\left( \mathit{F^{i}_{sD}}\in \mathit{\mathbb{R}^{m\times n}}, \mathit{F^{i}_{sI}}\in \mathit{\mathbb{R}^{m\times n}}\right ), \forall i < C$, and $C = 512$ in our network. Rather than analyzing the global correlation between any possible pairs of $(\mathit{F^{i}_{sD}},\mathit{F^{j}_{sI}})$, $\forall i \neq j$, we analyze the channelwise canonical correlation between the same channel number $\left(\mathit{F^{i}_{sD}},\mathit{F^{i}_{sI}}\right )$. This channelwise correlation analysis will result in getting features with similar semantic meanings for each modality, as shown in Figure \ref{fig:heat_comp}, which guides $\boldsymbol{f_{I}}$ to embed more valuable information related to depth completion.

Using 1-dimensional feature representation would lead to SSS problem in modern deep learning based computer vision task. We introduce the 2-dimensional approach similar to \cite{yang2004two} to generate full-rank covariance matrix $ \hat{\Sigma}_{sD,sI} \in \mathit{\mathbb{R}^{m\times n}}$, which is calculated as
\begin{equation}
    \hat{\Sigma}_{sD,sI} = \frac{1}{\mathit{C}}\sum_{\mathit{i}=0}^{C-1}\left [\mathit{F_{sD}^{i}}-\mathbf{E}\left [{F_{sD}} \right]\right]\left [\mathit{F_{sI}^{i}}-\mathbf{E}\left [{F_{sI}} \right]\right]^{T},
\end{equation}
in which we define $\mathbf{E}\left [{F} \right] = \frac{1}{\mathit{C}}\sum_{\mathit{i}=0}^{C-1}\mathit{F^{i}}$. Besides, we generate covariance matrices $ \hat{\Sigma}_{sD}$ (and respective $\hat{\Sigma}_{sI}$) with the regularization constant $\mathit{r_{1}}$ and identity matrix $\mathbf{I}$ as
\begin{equation}
    \hat{\Sigma}_{sD} = \frac{1}{\mathit{C}}\sum_{\mathit{i}=0}^{C-1}\left [\mathit{F_{sD}^{i}}-\mathbf{E}\left [{F_{sD}} \right]\right]\left [\mathit{F_{sD}^{i}}-\mathbf{E}\left [{F_{sD}} \right]\right]^{T} + \mathit{r_{1}}\mathbf{I}.
\end{equation}
The correlation between $\mathit{F_{sD}}$ and $\mathit{F_{sI}}$ is calculated as
\begin{equation}
     \mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})} =\left \|  ({\hat{\Sigma}_{sD}^{^{-\frac{1}{2}}}}) (\hat{\Sigma}_{sD,sI}) (\hat{\Sigma}_{sI}^{^{-\frac{1}{2}}})  \right \|  _{\mathbf{tr}}.
\end{equation}

The higher value of $\mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}$ represents the higher correlation between two feature blocks. Since $\mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}$ is an non-negative scalar, we use $\mathbf{-corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}$ as the optimization objective to guide training of two feature encoders. 
To compute the gradient of $\mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}$ with
respect to $\theta_{D}$ and $\theta_{I}$, we can compute its gradient with respect to $\mathit{F_{sD}}$ and $\mathit{F_{sI}}$ and then do the back propagation. The detail is showed following. Regarding to the gradient computation, we define
$\mathit{M}=(\hat{\Sigma}_{sD}^{-\frac{1}{2}}) (\hat{\Sigma}_{sD,sI})(\hat{\Sigma}_{sI}^{-\frac{1}{2}})$
and decompose $\mathit{M}$ as $\mathit{M} = \mathit{USV^{T}}$ using SVD decomposition. Then we define
\begin{equation}
    \frac{\partial \mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}}{\partial \mathit{{F}_{sI}}}=\frac{1}{C}\left ( 2\nabla_{sDsD}F_{sD}+\nabla_{sDsI}F_{sI} \right ),
\label{partial_equation}
\end{equation}
where $\nabla_{sDsI} = \hat{\Sigma}_{sD}^{^{-\frac{1}{2}}}UV^{T}\hat{\Sigma}_{sRGB}^{^{-\frac{1}{2}}}$ and $\nabla_{sDsD} = -\frac{1}{2}\hat{\Sigma}_{sD}^{^{-\frac{1}{2}}}UDU^{T}\hat{\Sigma}_{sD}^{^{-\frac{1}{2}}}$. $ \frac{\partial \mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}}{\partial \mathit{{F}_{sD}}}$ follows the similar calculations as $ \frac{\partial \mathbf{corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}}{\partial \mathit{{F}_{sI}}}$ in Equation (\ref{partial_equation}). 

\subsection{Loss Function}
We denote our channelwise 2D$^2$CCA loss as $L_{2D^2CCA} = \mathbf{-corr(\mathit{{F}_{sD}},\mathit{{F}_{sI}})}$. We denote the transformed component from sparse RGB to depth domain as $\hat{F}_{sD}$. The transformer loss describes the numerical similarity between RGB and depth domain. We use $L_2$ norm to measure the numerical similarity. Our transformer loss is $L_{trans} = \| {F}_{sD} - \hat{F}_{sD}\| _2^2$.

We also build another encoder and another transformer network which share weights with the encoder and transformer network for the spare RGB. The input of the encoder is a complementary RGB image. We use features extracted from complementary RGB image to predict features of non-observable depth using transformer network. For the complementary RGB image, we denote the extracted feature and transformed component as $\mathit{F}_{cI}$ and $\hat{F}_{cD}$. Later, we concatenate $\mathit{F}_{sD}$ and $\hat{F}_{cD}$, both of which are 512-channel. We got an 1024-channel bottleneck feature on depth domain. We pass this bottleneck feature into the decoder described in Section \ref{Network}. The output from the decoder is a completed dense depth map $\hat{D}$. To compare the inconsistency between the groundtruth $D_{gt}$ and the completed depth map, we use pixelwise $L_2$ norm. Thus our reconstruction loss is $L_{recon} = \| D_{gt}-\hat{D}\| ^2_2$.

Also, since bottleneck features have limited expressiveness, if the sparsity of inputs is severe, e.g. only $0.1\%$ sampled points of the whole resolution, the completed depth maps usually have griding effects. To resolve the griding effects, we introduce the smoothness term as in \cite{zhou2017unsupervised} into our loss function. $L_{smooth} = \| \nabla^2\hat{D}\| _1$, where $\nabla^2$ denotes the second-order gradients. 
Our final total loss function with weights becomes
\begin{equation}
\label{L_total}
     L_{total} = L_{2D^2CCA}+ w_t L_{trans} + w_r L_{recon} + w_s L_{smooth}.
\end{equation}

\section{Experiments}
\label{Experiments}
\subsection{Dataset and Experiment Details}
\label{Dataset and Experiment Details}

\textbf{Implementation details}. We use PyTorch to implement the network. Our encoders are similar to VGG16, without the fully-connected layers. We use ReLU on extracted features after every SAConv operation. Downsampling is applied to both the features and masks in encoders. The transformer network is a 2-layer network, size $3\times3$, stride 1, and 512-dimension, with our SAConv. The decoder is also a VGG16-like network using deconvolution to upsample. We use SGD optimizer. We conclude all the hyperparameter tuning in the supplemental material.

\textbf{Datasets.} We have done extensive experiments on outdoor scene datasets such as KITTI odometry dataset \cite{geiger2012we} and Cityscape depth dataset\cite{cordts2016cityscapes},
 and on indoor scene datasets such as NYUv2 \cite{silberman2012indoor} and SLAM RGBD datasets as ICL\_NUM \cite{handa2014benchmark} and TUM \cite{sturm2012benchmark}.
 
\begin{itemize}
\item\textbf{KITTI dataset}. The KITTI dataset contains both RGB and LiDAR measurements, total 22 sequences for autonomous driving use. We use the official split, where 46K images are for training and 46K for testing. We adopt the same settings described in \cite{Ma2017SparseToDense,cheng2018learning} which drops the upper part of the images and resizes the images to 912$\times$228.
\item\textbf{Cityscape dataset}. The Cityscape dataset contains RGB and depth maps calculated from stereo matching of outdoor scenes. We use the official training/validation dataset split. The training set contains 23K images from 41 sequences and the testing set contains 3 sequences. We center crop the images to the size of 900$\times$335 to avoid the upper sky and lower car logo. 
\item\textbf{NYUv2 dataset}. The NYUv2 dataset contains 464 sequences of indoor RGB and depth data using Kinect. We use the official dataset split and follow \cite{Ma2017SparseToDense} to sample 50K images as training data. The testing data contains 654 images.
\item\textbf{SLAM RGBD dataset}. We use the sequences of ICL-NUIM\cite{handa:etal:ICRA2014} and TUM RGBD SLAM datasets from stereo camera. \cite{sturm2012benchmark}. The former is synthetic, and the latter was acquired with Kinect. We use the same testing sequences as described in \cite{tateno2017cnn}.
\end{itemize}

\textbf{Sparsifiers}. A sparsifier describes the strategy of sampling the dense/semi-dense depth maps in the dataset to make them become the sparse depth input for the training and evaluation purposes. We define three sparsifiers to simulate different sparse patterns existing in the real-world applications. Uniform sparsifier uniformly samples the dense depth map, simulating the scanning effect caused by LiDAR which is nearly uniform. Stereo sparsifier only samples the depth measurements on the edge or textured objects in the scene to simulate the sparse patterns generated by stereo matching or direct VSLAM. ORB sparsifier only maintains the depth measurements according to the location of ORB features in the corresponding RGB images. ORB sparsifier simulates the output sparse depth map from feature based VSLAM. We set a sample number for uniform and stereo sparsifiers to control the sparsity. Since the ORB feature number varies in different images, we do not predefine a sample number but take all the depth at the ORB feature positions. 

\textbf{Error metrics}. We use the error metrics the same as in most previous works. (1) RMSE: root mean square error (2) MAE: mean absolute error (3) $\delta_i$: percentage of predicted pixels where the relative error is within 1.25$^i$. Most of related works adopt $i=1,2,3$. RMSE and MAE both measure error in meters in our experiments.

\textbf{Ablation studies}. To examine the effectiveness of multi-modal approach, we evaluate the network performance using four types of inputs, i.e. (1) dense RGB images; (2) sparse depth; (3) dense RGB image + sparse depth; (4) complementary RGB image + sparse depth. The evaluation results are demonstrated in Table \ref{table:source_nyu}. We could observe that the networks with single-modal input perform worse than those with multi-modal input, which validates our multi-modal design.
\begin{table}[hbt!]
\small
\begin{center}
\caption{Ablation study of using different data sources on NYUv2 dataset. Dense RGB means we feed in full RGB images. sD means sparse depth generated by 100-point stereo sparsifier . cRGB means complementary RGB image.}
\label{table:source_nyu}
\begin{tabular}{ p{3.5cm}<{\centering}  p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} }
\specialrule{.1em}{.05em}{.05em} 
Input data      & MAE   & RMSE  & $\delta_1$ & $\delta_2$ & $\delta_3$\\
\hline
Dense RGB        & 0.576 & 0.740 & 63.5       & 89.0       & 97.0\\
sD(100 pts)              & 0.524 & 0.700 & 68.1       & 90.2       & 97.0\\
Dense RGB+sD(100 pts)     & 0.479 & 0.638 & 73.0       & 92.4       & 97.7\\
cRGB+sD(100 pts)  & 0.473 & 0.631 & 72.4       & 92.6       & 98.1\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{center}
\end{table}
Besides, we observe that using dense RGB with sparse depth has similar but worse performance than using complementary RGB with sparse depth. The sparse depth inputs are precise. However, if we extract RGB domain features for the locations where we already have precise depth information, it would cause ambiguity thus the performance is worse than using complementary RGB information. We also conduct ablation studies for different loss combinations in our supplementary material on KITTI and NYUv2 dataset.

Furthermore, we conduct the ablation study with different sparsity on NYUv2 dataset. The stereo sparsifier is used to sample from dense depth maps to generate sparse depth data for training and testing. We show how different sparsity can affect the predicted depth map quality. The results are in Table \ref{table:sp_abl_nyu}.

\begin{table}[hbt!]
\begin{center}
\caption{Ablation study of different sample numbers on NYUv2 using stereo sparsifier.}
\label{table:sp_abl_nyu}
\begin{tabular}{ p{2cm}<{\centering}  p{2cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering} }
\specialrule{.1em}{.05em}{.05em} 
 Sample\# & MAE & RMSE & $\delta_1$ & $\delta_2$ & $\delta_3$\\
\hline
50      & 0.547 & 0.715 & 65.5 & 90.1 & 97.4\\
100     &0.426  & 0.580 & 77.5 & 94.1 & 98.4\\
200     & 0.385 & 0.531 & 80.9 & 95.1 & 98.7 \\
500     & 0.342 & 0.476 & 83.0 & 96.1 & 99.0\\
1000    & 0.290 & 0.419 & 87.0 & 97.0 & 99.2 \\
2000    & 0.242 & 0.352 & 91.3 & 98.2 & 99.6\\
5000    & 0.222 & 0.323 & 93.3 & 98.9 & 99.8\\
10000   & 0.151 & 0.231 & 96.6 & 99.5 & 99.9\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{center}
\end{table}

\subsection{Outdoor scene - KITTI odometry and Cityscapes}
For KITTI and Cityscapes these two outdoor datasets, we use the uniform sparsifier. For the KITTI dataset, we sample 500 points as sparse depth the same as some previous works. We compare with some state-of-the-art works, \cite{Ma2017SparseToDense,liu2017learning,cheng2018learning,wang2018pnp}. We follow the evaluation settings in these works, randomly choose 3000 images to calculate the numerical results. The results are in Table \ref{table:kitti_500}.
\begin{table}[hbt!]
\small
\begin{center}
\caption{500 points sparse depth completion on KITTI dataset.}
\label{table:kitti_500}
\begin{tabular}{ p{3.5cm}<{\centering}  p{1.6cm}<{\centering} p{1.6cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} }
\specialrule{.1em}{.05em}{.05em} 
  & MAE & RMSE & $\delta_1$ & $\delta_2$ & $\delta_3$\\
\hline
Ma \textit{et al.}\cite{Ma2017SparseToDense} & - & 3.378 & 93.5 & 97.6 & 98.9\\
SPN \cite{liu2017learning} & - & 3.243 & 94.3 & 97.8& 99.1 \\
CSPN\cite{cheng2018learning} & - & 3.029 & 95.5 & 98.0 & 99.0\\
CSPN+UNet\cite{cheng2018learning} & - & 2.977 & \textbf{95.7} & 98.0 & 99.1\\
PnP\cite{wang2018pnp} & \textbf{1.024} & 2.975 & 94.9 & 98.0 & 99.0 \\
CFCNet w/o smoothness & 1.233 & 2.967 & 94.1 & \textbf{98.1}& \textbf{99.3}\\
CFCNet w/ smoothness & 1.197 & \textbf{2.964} & 94.0 & 98.0 & \textbf{99.3}\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{center}
\end{table}
Next, we conduct experiments using both KITTI and Cityscape datasets. Some monocular depth prediction works use Cityscape dataset for training and KITTI dataset for testing. We choose this setting and use 100 uniformly sampled sparse depth as inputs. The results are shown in Table \ref{table:kitti_city}.
\begin{table}[hbt!]
\small
\begin{center}
\caption{Depth evaluation results: cap 50m means only taking the depth that smaller than 50m into consideration when doing the evaluation. CS->K means we train the network on the Cityscape dataset and we do the evaluation on the KITTI dataset. Comparing methods all train train/test with 100 pts.}
\label{table:kitti_city}
\begin{tabular}{ p{4cm}<{\centering}  p{1.4cm}<{\centering} p{1.4cm}<{\centering}  p{1.2cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering} }
\specialrule{.1em}{.05em}{.05em} 
 Methods &Input& Dataset & RMSE & $\delta$1& $\delta$2& $\delta$3\\
\hline
Zhou \textit{et al.} \cite{zhou2017unsupervised}&   RGB  & CS$\to$K&    7.580 & 57.7 & 84.0 &93.7\\
Godard \textit{et al.} \cite{godard2017unsupervised} & RGB & CS$\to$K&  14.445 & 5.3& 32.6 &86.2\\
Aleotti \textit{et al.} \cite{aleotti2018generative}& RGB  &  CS$\to$K &14.051&6.3& 39.4 &87.6\\
CFCNet(50 pts) & RGB+sD & CS $\to$K &  7.841 &78.3& 92.7 &97.0\\
CFCNet(100 pts) & RGB+sD & CS$\to$K & \textbf{5.827} &\textbf{82.6}& \textbf{94.7} &\textbf{97.9}\\

\hline
Zhou \textit{et al.} \cite{zhou2017unsupervised}(cap 50m) &  RGB  & CS$\to$K&  6.148 & 59.0 & 85.2 &94.5\\
CFCNet(50 pts, cap 50m) & RGB+sD & CS $\to$K &  6.334 &79.2& 93.2 &97.3\\
CFCNet(100 pts, cap 50m) & RGB+sD & CS $\to$K &  \textbf{4.524} &\textbf{83.7}& \textbf{95.2} &\textbf{98.1}\\
\hline
\hline
CFCNet(50 pts, cap 50m) & RGB+sD & CS$\to$CS&  9.019 & 82.8 & 94.1 &97.2\\
CFCNet(100 pts, cap 50m) & RGB+sD & CS$\to$CS&  6.887 & 88.9 & 96.1 &98.1\\
CFCNet(100 pts, cap 50m) & RGB+sD & K$\to$K&   3.157 & 91.0 & 97.1 &98.9\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{center}
\end{table}

\subsection{Indoor scene - NYUv2 and SLAM RGBD datasets}
For NYUv2 indoor scene dataset, we use the stereo sparsifier to sample points. We compare to the state-of-the-art \cite{Ma2017SparseToDense} with different sparsity using their publicly released code. The results are shown in Table \ref{table:sample_nyu}.

\begin{table}[hbt!]
\small
\begin{center}
\caption{Comparisons on NYUv2 dataset using stereo sparsifier.}
\label{table:sample_nyu}
\begin{tabular}{ p{1.5cm}<{\centering} p{1.5cm}<{\centering}  p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} p{1.5cm}<{\centering} }
\specialrule{.1em}{.05em}{.05em} 
 Sample\# & Methods & MAE & RMSE & $\delta_1$ & $\delta_2$ & $\delta_3$\\
\hline
100 & \cite{Ma2017SparseToDense} & 0.473 & 0.629 & 71.5 & 92.4 & 98.0\\
100 & CFCNet                       & \textbf{0.426} & \textbf{0.580} & \textbf{77.5} & \textbf{94.1} & \textbf{98.4}\\
200 & \cite{Ma2017SparseToDense} & 0.451 & 0.603 & 73.0 & 93.5 & 98.4\\
200 & CFCNet                       & \textbf{0.385} & \textbf{0.531} & \textbf{80.9} & \textbf{95.1} & \textbf{98.7}\\
500 & \cite{Ma2017SparseToDense} & 0.384 & 0.529 & 79.2 & 94.9 & 98.6\\
500 & CFCNet                       & \textbf{0.342} & \textbf{0.476} & \textbf{83.0} & \textbf{96.1} & \textbf{99.0}\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\vspace{-15pt}
\end{center}
\end{table}

Next, we conduct experiments on SLAM RGBD dataset. We follow the setting in the state-of-the-art, CNN-SLAM \cite{tateno2017cnn}, and do the cross-dataset evaluation. We train the model on NYUv2 using ORB sparsifier and evaluate on the SLAM RGBD dataset. We use the metric in CNN-SLAM, calculating the percentage of accurate estimations. Accurate estimations mean the error is within $\pm$10\% of the groundtruth. The results are in Table \ref{table:comparison}.\\

\begin{table}[hbt!]
\small
\begin{center}
\caption{Comparison in terms of percentage of correctly estimated depth on two SLAM RGBD datasets, ICL-NUIM and TUM. (TUM/seq1 name: \textit{fr3/long office household}, TUM/seq2 name: \textit{fr3/nostructure texture near withloop}, TUM/seq3 name: \textit{fr3/structure texture far})}
\vspace{-2pt}
\label{table:comparison}
\begin{tabular}{p{1.6cm}<{\centering} p{1cm}<{\centering} p{2.5cm}<{\centering} p{1.5cm}<{\centering} p{1.8cm}<{\centering} }
\specialrule{.1em}{.05em}{.05em} 
Sequence\# &CFCNet&CNN-SLAM\cite{tateno2017cnn}& Laina\cite{laina2016deeper}&Remode\cite{pizzoli2014remode}\\
\hline
ICL/office0& \textbf{41.97} & 19.41 & 17.19 & 4.47\\
ICL/office1& \textbf{43.86} & 29.15 & 20.83 & 3.13\\
ICL/office2& \textbf{63.64} & 37.22 & 30.63 & 16.70\\
ICL/living0& \textbf{51.76} & 12.84 & 15.00 & 4.47\\
ICL/living1& \textbf{64.34} & 13.03 & 11.44 & 2.42\\
ICL/living2& \textbf{59.07} & 26.56 & 33.01 & 8.68\\
TUM/seq1   & \textbf{54.70}& 12.47 & 12.98  & 9.54\\
TUM/seq2   & \textbf{66.30}& 24.07 & 15.41  & 12.65\\
TUM/seq3   & \textbf{74.61} & 27.39 & 9.45  & 6.73\\
\hline
Average    & \textbf{57.81} & 22.46 & 18.44  & 7.64\\
\specialrule{.1em}{.05em}{.05em} 
\end{tabular}
\end{center}
\end{table}
\vspace{-15pt}
\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.18]{figure/ex_k1.png}
    \caption{Visual results on KITTI dataset. (a) The RGB image (b) 500 points sparse depth as inputs. (c) Our Completed depth maps. (d) Results from \cite{Ma2017SparseToDense}. }
    \label{fig:heat_comp}
\end{figure}

\section{Conclusion}
In this paper, we directly analyze the relationship between the sparse depth information and their corresponding pixels in RGB images. To better fuse information, we propose 2D$^2$CCA to ensure the most similar semantics are captured from two branches and use the complementary RGB information to complement the missing depth. Extensive experiments using total four outdoor/indoor scene datasets show our results achieve state-of-the-art.


\small
\bibliographystyle{IEEEbib}
\bibliography{reference}















































\end{document}
