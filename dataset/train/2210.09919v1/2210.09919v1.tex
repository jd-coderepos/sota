\textbf{Datasets.} We use Cityscapes~\cite{cityscapes} and Pascal VOC 2012~\cite{voc} datasets for evaluating our method on semi-supervised semantic segmentation. Cityscapes consists of 2975 samples for training, with fine annotations for 19 classes, and 500 samples for evaluation. In addition, further 20000 samples are available in the \texttt{extra} set with coarse annotations, but we will use them later as unlabeled samples only. Pascal VOC consists of 1464 samples for training in the original set with annotations for 21 classes including background, and 1449 samples for evaluation in the validation set. Moreover, there are further 9118 labeled samples in the augmented set from SBD~\cite{sbd}. As is common in the literature, we simulate the semi-supervised setting with labeled and unlabeled splits of the training set with different labeled data regimes or ratios of labeled samples. For each split, we generate four different random splits with no guarantees of class balance. For Pascal VOC, we split the original set and use the augmented set as unlabeled data.



\begin{table*}
\adjustbox{max width=\textwidth}{
    \centering
    \begin{tabular}{lcccccccc}
    \toprule
          Method & Backbone & Sampling & \textbf{93} & \textbf{186} & \textbf{372} & \textbf{744} & \textbf{1488} & \textbf{2975} \\
         \midrule
         \multirow{2}{*}{Supervised} & RN-50 & &  &  &  &  &  &  \\
         \cmidrule{2-9}
          & RN-101 & &   & &  &  &  &  \\

         \midrule
         \multirow{4}{*}{\makecell[l]{Dense \\ FixMatch}} & \multirow{2}{*}{RN-50} & Explicit &  &  &  &  &  &  \\
         && Implicit &  & & & &\\
         \cmidrule{2-8}
         & \multirow{2}{*}{RN-101 } & Explicit &  &  &  & &   \\
         &  & Implicit &  &  &  & &   \\

    \bottomrule
    \end{tabular}}
    \caption{Results of Dense FixMatch on \textbf{Cityscapes} \texttt{val} set with few labeled samples on different amounts of labeled data, ResNet-50/101 backbones and DeepLabv3+, and either explicit or implicit mini-batch sampling settings. Dense FixMatch significantly improves over the baselines for both settings.
    }
    \label{tab:few}
    \vspace{-5pt}
\end{table*}
\begin{table*}
\adjustbox{max width=\textwidth}{	
    \centering	
    \begin{tabular}{lcccccccc}	
    \toprule	
          Method & Backbone & Sampling & \textbf{92} & \textbf{183} & \textbf{366} & \textbf{732} & \textbf{1464} & \textbf{10582} \\	
         \midrule	
         \multirow{2}{*}{Supervised} & RN-50 & &  &  &  &  &  &  \\	
         \cmidrule{2-9}	
          & RN-101 & &   & &  &  &  &  \\	
         \midrule	
         \multirow{4}{*}{\makecell[l]{Dense \\ FixMatch}} & \multirow{2}{*}{RN-50} & Explicit &  &  &  &  &  &  \\	
         && Implicit &  &  &  &  &  &  \\	
         \cmidrule{2-8}	
         & \multirow{2}{*}{RN-101 } & Explicit &  &  &  & &   \\	
         &  & Implicit &  &  &  & &   \\	
    \bottomrule	
    \end{tabular}}	
    \caption{Results of Dense FixMatch on \textbf{Pascal VOC 2012} \texttt{val} set with few labeled samples on different amounts of labeled data, ResNet-50/101 backbones and DeepLabv3+, and explicit or implicit settings.
    }	
    \label{tab:few_voc}
    \vspace{-10pt}
\end{table*}	


\noindent \textbf{Model.} Following the literature, we use DeepLabv3+~\cite{deeplabv3} on ResNet-50 or ResNet-101 backbones~\cite{resnet} to define our semantic segmentation model, giving predictions at the same spatial resolution as the input. The model is initialized with ImageNet~\cite{imagenet} pre-trained weights.

\noindent \textbf{Implementation details.} We implement and train our models using PyTorch~\cite{pytorch} with distributed training and mixed precision on up to four NVIDIA A100 GPUs, depending on the total batch size and the backbone used. We use the computer vision library Kornia~\cite{kornia} for implementing the data augmentation pipelines for its support of invertible geometric transforms and differentiable augmentations for multiple tasks, including semantic segmentation. We follow EMAN~\cite{eman} and use the exponential moving average of the Batch Normalization statistics of the student to update the teacher.


\noindent \textbf{Evaluation.} We evaluate the performance of our method using as the main metric the mean Intersection-over-Union (mIOU) over all classes. For simplicity, we use full-resolution, single-scale, and single-pass evaluation in contrast to the sliding evaluation approach used in other works~\cite{cps,ael,u2pl}. For stability, the model used for evaluation has weights following the EMA of the weights obtained during training. For most experiments, this means exactly using the teacher in the MT framework for evaluation. For each labeled data regime, we train our model on each of the four random data splits using also a different random seed for each training run. We take the best checkpoint of each run according to the mIOU and compute the mean and standard deviation over the four runs.

\noindent \textbf{Training details.} We follow the training details of \cite{u2pl}. We employ two different mini-batch sampling strategies for SSL: (a) the common \textit{explicit} setting in which labeled and unlabeled data are sampled separately, and (b) the alternative \textit{implicit} setting in which all data is sampled uniformly regardless of labels~\cite{analysis_fixmatch}.
We train each setting for the equivalent of 80 or 240 epochs on the full train set for the supervised baseline, i.e. 52910 or 89250 updates, for Pascal VOC and Cityscapes, respectively.

\subsection{Results on few labeled samples}
In Figure \ref{fig:intro_fig} and Tables \ref{tab:few} and \ref{tab:few_voc}, we show results for the common splits of 1/32 to 1/2 of all the full \texttt{train} set for Cityscapes and 1/16 to the full original \texttt{train} set for Pascal VOC 2012, respectively. We compare few-supervision baselines using only the labeled data with Dense FixMatch, which also uses the unlabeled samples in either the explicit or implicit settings. Our method performs better than the baselines for both mini-batch sampling approaches. The explicit setting is slightly better for more label-scarce regimes but both give similar results with more labeled data.




\begin{table*}[t]
    \centering
    \begin{minipage}{.67\textwidth}
        \adjustbox{max width=\columnwidth}{
    \centering
    \begin{tabular}{lccccc}
    \toprule
          Method & Backbone & & & \multicolumn{2}{c}{mIoU} \\
         \midrule
         \multirow{2}{*}{Supervised} & RN-50 & &  & \multicolumn{2}{c}{} \\
         \cmidrule{2-6}
          & RN-101 & &  & \multicolumn{2}{c}{}  \\

         \midrule
         & & \texttt{train}  & \texttt{extra} & Explicit & Implicit \\
         \cmidrule{2-6}
         \multirow{6}{*}{\makecell[l]{Dense \\ FixMatch}} & \multirow{3}{*}{RN-50} & \checkmark &  &  \multicolumn{2}{c}{}\\
         & & & \checkmark&  & \\
         & & \checkmark & \checkmark &  &  \\
         \cmidrule{2-6}
         & \multirow{3}{*}{RN-101} & \checkmark &  & \multicolumn{2}{c}{}\\
          && & \checkmark &  & \\
          && \checkmark & \checkmark  &  & \\
    \bottomrule
    \end{tabular}}
    \caption{\small{Results on Cityscapes full labeled set and \texttt{extra} unlabeled samples for the supervised baselines and semi-supervised Dense FixMatch in both the explicit and implicit mini-batch sampling settings for SSL. We also compare using our method as a regularization on the labeled data only and using it on both labeled and unlabeled data. is our setting for the main experiments.}}
    \label{tab:extra}
    \vspace{-10pt}
    \end{minipage}
    \hspace{1pt}
    \begin{minipage}{.3\textwidth}
    \centering 
    \adjustbox{max height=100pt}{
    \begin{tabular}{ccc}
        \toprule
         &  & mIoU\\
        \midrule
        \multirow{5}{*}{0} & 0.2 & 0.6566 \\
        & 0.5 & 0.6464 \\
        & 1 & \textbf{0.6594}\\
        & 2 & \textit{0.6581} \\
        & 5 & 0.5699\\
        \midrule
        \multirow{7}{*}{0.5} & 0.1 & 0.6447\\
          & 0.2 & \textbf{0.6609}\\
         & 0.5 & 0.6495\\
         & 1 & 0.6501\\
          & 2 & \textit{0.6554}\\
         & 5 & 0.6090\\
          & 10 & 0.5276\\
         \midrule
         0.8 & 1 & 0.6388\\
         \midrule
         0.95 & 1 & 0.6148\\
         \bottomrule \\
    \end{tabular}
    }
    \vspace{-10pt}
    \caption{\scriptsize{Ablation study on the pseudo-label confidence threshold  and the consistency loss weight . values used in the main experiments.}}
    \label{tab:conf_threshold_weight_ablation}
    \end{minipage}
\end{table*}




\begin{table}[h!]
        \centering
        \adjustbox{max width=\columnwidth}{
        \begin{tabular}{ccccc}
        \toprule
        Method &&&& mIoU \\
        \midrule
        Supervised & & & & 0.5409 \\
        \multicolumn{2}{l}{Mean Teacher}& & & 0.5820 \\
        \midrule
          & Crop relation & Augmentation& MT & \\
         \cmidrule{2-5}
\multirow{11}{*}{\makecell[l]{Dense \\ FixMatch}} &  Same & Crop+color& & 0.5794   \\
          & Same & Crop+color+cutout& & 0.6531 \\
          & Min. overlap & Crop+color+cutout& &  0.6542 \\
          & Min. overlap & Crop+geom. && 0.6539 \\
          & Min. overlap & Crop+geom.+cutout && 0.6517 \\
          & Min. overlap & Crop+color+geom. && 0.6579 \\
          & Same & Crop+color+geom. && 0.6157 \\
          & Any & Crop+color+geom.+cut. && 0.6300  \\
         & Same & Crop+color+geom.+cut. && \textbf{0.6660} \\
          & Min. overlap & Crop+color+geom.+cut. && \textit{0.6594} \\
          & Min. overlap & Crop+color+geom.+cut. & & 0.6275 \\
         \bottomrule \\
    \end{tabular}}
\caption{\small{Ablation on the use of Mean Teacher (MT) framework, the relation between crops in the two views of Dense FixMatch and the use of the geometric, color or all augmentations in RandAugment. Using MT on its own with the augmentation pipeline of the supervised baseline. We use a logistic warm-up schedule for the consistency weight during the first 60 epochs. is our setting for the main experiments.}}
    \label{tab:further_ablation}
\end{table}

\subsection{Results on full labeled set and extra unlabeled samples}
In addition, we evaluate Dense FixMatch in the more realistic setting where all labeled samples are used and extra unlabeled data is available in Cityscapes. We use all samples from the \texttt{extra} set but discard the coarse annotations and just treat them as unlabeled samples. We also compare to the fully-supervised baseline using only the labeled data and using the consistency loss as a regularization term only, i.e. computed on the same labeled data as the supervised loss. Results are shown in Table \ref{tab:extra}. Computing the loss on both labeled and unlabeled samples gives the best results.

\subsection{Ablations}
\label{sec:ablations}
In Table~\ref{tab:conf_threshold_weight_ablation}, we ablate the main hyper-parameters of our method using the 93 labeled samples regime of semi-supervised semantic segmentation with Cityscapes, with a single data split and seed, and the explicit setting.
For dense tasks such as semantic segmentation, other works have reported that using a low  or removing it altogether gives better final results~\cite{pseudoseg}, hypothesizing that discarding predictions of lower confidence makes the loss dominated by easy classes~\cite{pixmatch,ael}. Moreover, low-confidence predictions in semantic segmentation tend to concentrate in truly ambiguous regions such as the boundaries between objects of different classes~\cite{u2pl} so discarding them means removing supervision mainly from boundary pixels which are in fact the most informative. Values for  between 0.2 and 2 give comparable results.

In Table~\ref{tab:further_ablation} we ablate our design choices: the relation between crops in both views, the choice of augmentations, and the use of MT. The best results are obtained when using MT and all possible augmentations, although using the same crop instead of overlapping crops between views improves results. We hypothesize this is due to a larger number of valid locations in the matched pseudo-labels.



\begin{table*}[h!]
    \centering
    \adjustbox{max width=\textwidth}{
    \begin{tabular}{ccccccccccccccccccccc}
         \toprule
         \textbf{Method} & Road & Side. & Build. & Wall & Fence & Pole & T.light & T.sign & Veg. & Terr. & Sky & Person & Rider & Car & Truck & Bus & Train & Motor. & Bic. & \textbf{Mean}\\
         \midrule
         \% pixels & 36.02 &  7.06 &  25.61 &  0.41 &  0.81 &  1.16 &  0.18 &  0.59 &  14.08 &  1.22 &  3.80 &  1.08 &  0.09 &  6.73 &  0.25 &  0.23 &  0.21 &  0.14 &  0.34\\
         \midrule
         Supervised & .962 & .722 & .873 & .293 & .296 & .500 & .497 & .563 & .896 & .469 & .914 & .734 & .336 & .888 & .114 & .276 & .318 & .273 & .677 &  \\
         \midrule
         Dense FixMatch (E) & \textbf{.976} & .810 & \textbf{.902} & .414 & .424 & .574 & \textbf{.619} & \textbf{.710} & .906 & .575 & \textbf{.935} & \textbf{.778} & \textbf{.544} & \textbf{.924} & \textbf{.560} & \textbf{.699} & \textcolor{red}{.295} & \textbf{.488} & \textbf{.715} &  \\
         Dense FixMatch (I) & .974 & \textbf{.820} & .900 & \textbf{.467} & \textbf{.432} & \textbf{.578} & .603 & .688 & \textbf{.913} & \textbf{.587} & .931 & .769 & .448 & \textbf{.924} & .495 & .667 & \textbf{.471} & .475 & \textcolor{red}{.661} &  \\
         \bottomrule
    \end{tabular}}\\
\adjustbox{max width=\textwidth}{
    \begin{tabular}{ccccccccccccccccccccccc}
         \toprule
         \textbf{Method} & Bg. &  Plane &  Bicy. &  Bird &  Boat &  Bottle &  Bus &  Car &  Cat &  Chair &  Cow &  Table &  Dog &  Horse &  Motor. &  Person &  Pott. &  Sheep &  Sofa &  Train &  Tv & \textbf{Mean}\\
          \midrule
         \% pixels & 72.45 &  1.09 &  0.83 &  0.15 &  0.94 &  1.57 &  3.46 &  2.60 &  0.58 &  0.65 &  1.25 &  2.08 &  1.13 &  0.58 &  0.90 &  5.27 &  0.56 &  0.34 &  0.49 &  1.72 &  1.35\\
         \midrule
         Supervised & .883 & .702 & .379 & .227 & \textbf{.574} & .443 & .764 & .639 & .336 & .114 & .429 & .215 & .309 & .332 & .542 & .647 & .199 & .511 & .202 & .610 & .307 &  \\
         \midrule
         Dense FixMatch (E) & .890 &  \textbf{.784} &  \textbf{.523} &  \textbf{.639} &  \textcolor{red}{.500} & .555 &  \textbf{.843} &  .684 &  \textbf{.776} &  .188 &  \textbf{.514} &  \textbf{.439} &  .384 &  .388 &  \textbf{.752} &  \textcolor{red}{.622} &  .356 &  \textbf{.576} &  \textbf{.352} &  .743 &  \textbf{.553} & \\
         Dense FixMatch (I) & \textbf{.899 }&  .740 &  .459 &  \textcolor{red}{.010} &  \textcolor{red}{.559} & \textbf{ .583} &  .833 &  \textbf{.721} &  .431 &  \textbf{.192} &  .469 &  .347 &  \textbf{.418} &  \textbf{.559} &  .693 &  \textbf{.672} &  \textbf{.364} &  \textcolor{red}{.356} &  .333 &  \textbf{.747} &  .545 & \\
         \bottomrule
    \end{tabular}}
    \caption{Class-wise IoU on \texttt{val} set of Cityscapes (top) when training on a 93 labeled samples data split and on \texttt{val} set of Pascal VOC 2012 (bottom) when training on a 92 labeled samples data split. We show in \textbf{bold} the best result for each class and in \textcolor{red}{red} the classes that perform worse than the supervised baseline for both the \textit{explicit} (E) and \textit{implicit} (I) mini-batch sampling settings.}
    \label{tab:classwise}
    \vspace{-10pt}
\end{table*}

\begin{figure*}[h!]
\hspace{-18pt}
    \begin{tabular}{ccccc}
        \tiny{Input} & \tiny{Supervised} & \tiny{Dense FixMatch(E)} & \tiny{Dense FixMatch(I)} & \tiny{Ground truth}\\
         \includegraphics[width=.19\textwidth]{graphics/inp1.png} & \includegraphics[width=.19\textwidth]{graphics/qual_sup.png} & \includegraphics[width=.19\textwidth]{graphics/qual_explicit.png} & \includegraphics[width=.19\textwidth]{graphics/qual_implicit.png} & \includegraphics[width=.19\textwidth]{graphics/gt1.png} \\
         \includegraphics[width=.19\textwidth]{graphics/inp2.png} &
         \includegraphics[width=.19\textwidth]{graphics/qual_sup2.png} & \includegraphics[width=.19\textwidth]{graphics/qual_explicit2.png} &  \includegraphics[width=.19\textwidth]{graphics/qual_implicit2.png} & \includegraphics[width=.19\textwidth]{graphics/gt2.png} \\
    \end{tabular}
    \caption{Qualitative results for SSL training on Cityscapes with 93 labeled samples for the supervised baseline and Dense FixMatch in the \textit{explicit} (E) and \textit{implicit} (I) mini-batch sampling settings, shown on samples in the validation set.}
    \label{fig:qualitative}
    \vspace{-10pt}
\end{figure*}

\subsection{Class-wise analysis}

Semantic segmentation is known for its large class imbalance. In both Cityscapes and Pascal VOC the predominant classes appear in a few orders of magnitude more pixels than the least frequent ones~\cite{cityscapes,voc}. In Table~\ref{tab:classwise}, we give per-class results comparing the supervised baseline to Dense FixMatch on a single data split of 93 or 92 labeled samples for Cityscapes and Pascal VOC 2012 respectively. We also report the percentage of pixels for each class. For Cityscapes, our method improves significantly on average over the baseline, and does so while improving for all but one class. Importantly, it also reduces the effect of class-imbalance since the gap between the best performing classes and the worst performing ones is reduced significantly, as it can be seen by the lower standard deviation across classes. In Pascal VOC there is a very large imbalance between the background class and the rest. Dense FixMatch improves the results on average, but up to 3 classes get lower IoU. For the implicit setting, the least-frequent class \texttt{Bird} is never learnt.

 

\subsection{Qualitative results}

In Figure~\ref{fig:qualitative}, we give some examples for the 93 labeled samples experiments with Cityscapes comparing the baseline and Dense FixMatch for both the explicit and implicit settings. We see how both settings give similar results and outperform the supervised baseline, as seen in the more defined boundaries between road and side-walk and poles.

\vspace{-5pt} 