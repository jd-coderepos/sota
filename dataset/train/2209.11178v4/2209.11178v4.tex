\newpage

{\huge Appendix}


\def\E{{\bf E}}
\def\x{{\bf x}}
\def\r{{\bf r}}
\def\rhat{\hat{r}}

\section{Proofs}
\label{app:proofs}
\subsection{Formal Proof of Theorem 1}

Before proceeding to Theorem 1, we show a technical lemma that guarantees the existence-uniqueness of the solution to the Poisson equation, under some mild conditions. 
\begin{lemma}
\label{lemma:ex-uni}
{Given , assume that the source function , and  has a compact support. Then the the Poisson equation  on  with zero boundary condition at infinity~() has a unique solution  up to a constant.}
\end{lemma}

\begin{proof}

For the existence of the solution, one can verify that the analytical construction using the extension of Green's function in  dimensional space~(Lemma~\ref{lemma:green}), \ie , is one possible solution to the Poisson equation . Since  and , we conclude that .

The proof idea of the uniqueness is similar to the uniqueness theorems in electrostatics. Suppose we have two different solutions  which satisfy 

We define . Subtracting the above two equations gives

By the  vector differential identity we have

By the divergence theorem we have

where  denotes an  dimensional surface element at infinity, and the second equation holds due to zero boundary condition at infinity. Combining Eq.~(\ref{eq:unique_1})(\ref{eq:unique_2})(\ref{eq:unique_3}), we have

since this is an integral of a positive quantity, we must have , or , . This means  and  differ at most by a constant, but a constant does not affect gradients, so . \end{proof}


{In our method section~(Section~\ref{sec:augment}), we augmented the original -dimensional data with an extra dimension. The new data distribution in the augmented space is , where  is the Dirac delta function. The support of the data distribution is in the  hyperplane. In the following lemma, we show the existence and uniqueness of the solution to  outside the data support.}

\begin{lemma}
\label{lemma:data}
{Assume the support of the data distribution in the augmented space~() is a compact set on the  hyperplane,  and  . The Poisson equation  with zero boundary condition at infinity~() has a unique solution  for , up to a constant.}
\end{lemma}
\begin{proof}

Similar to the proof in Lemma~\ref{lemma:ex-uni}, one can easily verify that the analytical construction using Green's method, \ie , is one possible solution to the Poisson equation . Since  for   and , we conclude that .

For the uniqueness, suppose we have two different solutions  which satisfy 

We define . Subtracting the above two equations gives

By the vector differential identity we have

By the divergence theorem we have

where  denotes an  dimensional surface element at infinity, and the second equation holds due to zero boundary condition at infinity. Combining Eq.~(\ref{eq:2unique_1})(\ref{eq:2unique_2})(\ref{eq:2unique_3}), we have

The first equation holds because Lebesgue measure of  is zero. Since  is an integral of a positive quantity, we must have , or , . This means  and  differ at most by a constant function, but a constant does not affect gradients, so . \end{proof}


\begin{figure*}
    \centering
    \includegraphics[width=0.6\textwidth, trim=0cm 3cm 0cm 3cm]{img/proof_plot_2.pdf}
    \caption{Proof idea of Theorem~\ref{theorem2}. By Gauss's Law, the outflow flux  equals the inflow flux . The factor of two in  is due to the symmetry of Poisson fields in  and .
    }
        \label{fig:theorem2}
\end{figure*}

As illustrated in \Figref{fig:theorem2}, there is a bijective mapping between the upper hemisphere of radius  and the  plane, where each pair of corresponding points is connected by an electric field line. 
We will now formally prove that, in the  limit, this mapping transforms the arbitrary charge distribution in the source plane (that generated the electric field) into a uniform distribution on the hemisphere. 
\begin{theorem}
\label{theorem2}
 {Suppose particles are sampled from a uniform distribution on the upper () half of the sphere of radius  and evolved by the backward ODE  until they reach the  hyperplane, where the Poisson field  is generated by the source . In the  limit, {under the conditions in Lemma~\ref{lemma:data}}, this process generates a particle distribution , 
i.e., a distribution  in the  hyperplane.} 
\end{theorem}
\begin{proof}

{By Lemma~\ref{lemma:data}, we know that with zero boundary at infinity, the Poisson equation  has a unique solution  for . Hence , guaranteeing the existence-uniqueness of the solution to the ODE  according to Theorem 2.8.1 in \cite{Ricardo2002AMI}.}

Consider the tube in \Figref{fig:theorem2} connecting an area on  in the  hyperplane () to a solid angle  on the hemisphere (), with  as its side. The tube is the space swept by  following electric field , so by definition the electric field is parallel to the tangent space of the tube sides . The bottom of the tube  is located at , a bit above the  plane, so the tube does not enclose any charges.  We note that the divergence of Poisson field is zero in :

Denote the volume and surface of the tube as  and . According to divergence theorem, . Hence the net flux leaving the tube is zero:



There is no flux through the sides, i.e., , since  is orthogonal to the surface element  on the tube sides by definition. As a result, the flux  entering from below must equal the flux  leaving the other end. Denote the  norm of the vector  as . We first calculate the influx . To do so, we study a Gaussian pillbox whose top, side and bottom are ,  and .  and  are located at  and  (). Denote the volume and surface of the pillbox as  and . The pillbox contains charge , so according to Gauss's law , i.e.,

The flux on the sides , and  due to mirror symmetry of . So .  Note on the  surface, the outflux of the pillbox is exactly the influx of the tube, so we have:

inserting which and  to Eq.~(\ref{eq:tube_zeronetflux}) gives 

On the other hand, in the far-field limit , since  is bounded, the data distribution can be effectively seen as a point charge (see Appendix~\ref{sec:mul}). By Lemma~\ref{lemma:point}, we have . The resulting outflux on the hemisphere is

where  is the radial component of . Comparing \Eqref{eq:Phi_S1_1} and \Eqref{eq:out} yields 
.
In other words, the mapping from the  hyperplane to the hemisphere dilutes the charge density  up to a constant factor. Thus by change-of-varible, we conclude that the mapping transforms the data distribution into a uniform distribution on the infinite hemisphere. Since the ODE is reversible, the backward ODE transforms the uniform distributoin on the infinite hemisphere to the distribution .
\end{proof}



\subsection{Multipole Expansion}
\label{sec:mul}
We discuss the behaviors of the potential function in Poisson equation~(\Eqref{eq:poisson}) under different scenarios, utilizing the multipole expansion. Suppose we have a unit point charge  located at . We know that the potential function at another point  is  (ignoring a constant factor). Now we assume that  is close to the origin such that we can Taylor expand around :

where

In the case where the source is a distribution , the potential  can again be Taylor expanded:

where 

which are called monopole, dipole and quadrupole in physics, respectively. The gradient field  can be expanded in the same such that

It is easy to check that  decays as , which means higher-order corrections decay faster than leading terms. So when , only the monopole term  matters, which behaves like a point source.

In a more realistic setup, we only have a large but finite , so the question is: under what condition is the point source approximation valid? We examine ,  and  more carefully:

Since  is an odd function of , integrating  over  leads to zero (samples are normalized to zero mean). In machine learning applications,  is usually a large number (although in physics  is merely 3). If  is a random vector of length , then . So Eq.~(\ref{eq:phi_expansion}) can be approximated as 

Requiring  gives . So the condition for the point source approximation to be valid is:

Based on this condition, we can partition space into three zones: (1) the far zone , where the point source approximation is valid; (2) the intermediate zone , where the gradient field has moderate curvature; (3) the near zone , where the gradient field has high curvature. In practice, the initial value  is greater than 1000 (hence ) with high probability on CIFAR-10 and CelebA datasets, incidating that the initial samples lie in the far zone and gradually move toward the near zone where  ().

We summarize above observations in the following lemma in the  limit: 
\begin{lemma}
\label{lemma:point}
Assume the data distribution  has a compact support in , then the solution  to the Poisson equation  with zero boundary condition at infinity satisfies .
\end{lemma}
\begin{proof}
By Lemma~\ref{lemma:ex-uni}, the gradient of the solution has the following form:


Since  has a bounded support, we assume . On the other hand, we have

for  such that . Hence,


\end{proof}







\subsection{{Extension of Green's Function in -dimensional Space}}
\label{app:green}
In this section, we show that the function  defined in \Eqref{eq:poisson_solution} is the -dimensional extension of the Green's function,      solves the Poisson equation .
\begin{lemma}
\label{lemma:green}
Assume the dimension , and the source term satisfies . The extension of Green's function  solves the Poisson equation . In addition, with zero boundary condition at infinity~(),  solves the Poisson equation .
\end{lemma}

\begin{proof}
It is convenient to denote ,  and notice . Firstly, we calculate :

Then we calculate :

which is 0 for , but undermined for . So we are left with proving

where  denotes a ball centered at  with a radius . With the divergence theorem, we have

where the surface integral can be computed

in which we used . Together, we conclude that





Next we show that  solves . Taking the Laplacian operator of both sides gives:


In addition, we show that  is zero at infinity. Since  and has compact support, we know that  is bounded, and let .

The last equality holds since  is a compact set.
\end{proof}

\subsection{Proof for the Prior Distribution on  Hyperplane}\label{app:prior_distribution}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{./img/init_dist_proof.pdf}
    \caption{Diagram of the deviation in Proposition~\ref{prop:prior}}
    \label{fig:init_dist}
\end{figure}

We obtain the prior distribution  by projecting the uniform distribution  on the hemisphere  to the  hyperplane. In the following proposition, we show that the projected distribution is . 
\begin{proposition}
\label{prop:prior}
The radial projection of  on the hemisphere  to the  hyperplane is .
\end{proposition}
\begin{proof}
We calculate the change-of-variable ratio by comparing two associate areas. As illustrated in \Figref{fig:init_dist}, an area  on  is projected to an area  on the hyperplane in the  direction, and we have  We aim to calculate the ratio  below. We define the angle between  and  to be . We project  to the hyperplane orthogonal to  to get  where . Since  is parallel to  and they lie in the same cone from the origin , we have . Combining all the results gives

\end{proof}

In order to sample from , we first sample the norm~(radius)  from the distribution:

and then uniformly sample its angle. Sampling from  encompasses three steps. We first sample a real number  with parameters , \ie

Next, we set  such that  is effectively sampled from the inverse beta distribution~a(also known as beta prime distribution) with parameters . Finally, we set . To verify the pdf of  is , note that the pdf of inverse beta distribution is

Next, by change-of-variable, the pdf of  is


Hence we conclude that .

\section{Experimental Details}

\subsection{Training}
\label{app:training}

In this section we include more details about the training of PFGM and other baselines. We show the hyper-parameters settings for all the baselines~(Appendix~\ref{app:hyper-train}). All the experiments are run on a single NVIDIA A100 GPU.


\subsubsection{Additional Settings}
\label{app:hyper-train}

\paragraph{PFGM} We set the hyper-parameters , the larger batch size for calculating normalize field  in Algorithm~\ref{alg:pf}, and ,  and  in Algorithm~\ref{alg:ode}. We use the a batch size of , the same Adam optimizer and exponential moving average in \cite{Song2021ScoreBasedGM}. We center the data around the origin. The initial  components in the normalized field are approximately zero with small initial  values in Algorithm~\ref{alg:ode}. In this case, the trajectories of the forward ODE terminate at points that are unlikely traversed by the backward ODE, \ie points with large  and small . In light of this, we heuristically confine the maximum sampling step to  for points with the initial  smaller than . More principal solutions are left for future works.

For \textit{selecting  in more general settings}, we recommend the following rule-of-thumb. According to analysis in \Secref{sec:mul}, given a perturbation point  when setting the exponent  in Algorithm~\ref{alg:ode}, we can ensure the point source approximation by

where  is the data dimension and  is the data distribution. By WLLN, we have , and recall that 
where , . Together, we conclude . Substituting in \Eqref{eq:rule-M}, we have 

We empirically observe that setting  already gives good results, and the corresponding . For example, on CIFAR-10 datasets, , we have . 

Since we are operating in the augmented space, we add minor modifications to the DDPM++/DDPM++ deep architectures to accommodate the extra dimension. More specifically, we replace the conditioning time variable in VP/sub-VP with the additional dimension  in PFGM as the input to the positional embedding. We also need to add an extra scalar output representing the  direction. To this end, we add an additional output channel to the final convolution layer and take the global average pooling of this channel to obtain the scalar. {For LSUN bedroom dataset, we both experiments with the channel configurations suggested in NSCN++~\cite{Song2021ScoreBasedGM} and DDPM~\cite{Ho2020DenoisingDP}.}

\paragraph{VE/VP/sub-VP} We use the same set of hyper-parameters and the NCSN++/DDPM++ (deep) backbone and the continuous-time training objectives for forward SDEs in \cite{Song2021ScoreBasedGM}.

\subsection{Sampling}
\label{app:sampling}

We provide more details of PFGM and VE/VP sampling implementations in Appendix~\ref{app:sample-add}. We further discuss two techniques used in PFGM ODE sampler: change-of-variable formula~(Appendix~\ref{app:exp}) and the substitution of ground-truth Poisson field direction on ~(Appendix~\ref{app:sub}).


\subsubsection{Additional settings}
\label{app:sample-add}
\paragraph{PFGM} For RK-45 sampler, we use the function implemented in \texttt{scipy.integrate.solve\_ivp} with \texttt{atol}=, \texttt{rtol}=. For forward Euler method, we discretize the ODE with constant step size determined by the number of steps, \ie step size = /number of steps for the backward ODE~(\Eqref{eq:backode}). {As in [1], we set the terminal value of . We choose 64^2 to satisfy the condition  by the multipole expansion analysis in Appendix~\ref{sec:mul}. The condition ensures that the data distribution can be viewed roughly as a point source at origin. For example, we set  on CIFAR-10, and the corresponding  is greater than  with high probability. The hyperparameters work well without further fine tuning. Hence, we hypothesize that PFGM is insensitive to the choice of hyperparameters in a reasonable range, as shown in Table~\ref{table:fid-zmax}.} We clip the norms of initial samples into  for CIFAR-10,  for CelebA and  for LSUN bedroom.

For selecting  and clipping upper bound of norms for general datasets, we recommend the following rule-of-thumb. Recall that during the training perturbations~(\Eqref{eq:geo-ode}), given a random initial value , maximum  is

Hence we set . For example, on CIFAR-10, , and . The clipping upper value is similarity derived, by setting it to , where . By combining \Eqref{eq:rule-M}, we further have

where  is the data dimension and  is the data distribution. These formulas are easier for practitioner to apply PFGM on new datasets.



\paragraph{VE/VP/sub-VP} For the PC sampler in VE, we follow \cite{Song2021ScoreBasedGM} to set the reverse diffusion process as the predictor and the Langevin dynamics (MCMC) as the corrector. For VP/sub-VP, we drop the corrector in PC sampler since it only gives slightly better results~\cite{Song2021ScoreBasedGM}.
\begin{table*}[htb]
\begin{center}
\caption{FID scores versus  on PFGM w/ DDPM++}
\label{table:fid-zmax}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{} &   & & \\
		\midrule
        \textbf{FID score} & {2.49} & {2.48} &{2.48}\\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}
\subsubsection{Exponential Decay on  Dimension}
\label{app:exp}
Recall that in Section~\ref{sec:sampling}, we replace the vanilla backward ODE with a new ODE anchored by :

We further use the change-of-variable formula, \ie , to achieve exponential decay on the  dimension:


The trajectories of the two ODEs above are the same when . We compare the NFE and the sample quality of different ODEs in Table~\ref{table:exp}. We measure the NFE/FID of generating 50000 CIFAR-10 samples with the RK45 method in Scipy package~\cite{Virtanen2020SciPy1F}. The batch size is set to . All the numbers are produced on a single NVIDIA A100 GPU. We observe that the ODE with the anchor variable  not only accelerates the vanilla by 2 times, but has almost no harm to the sample quality measured by FID score.
\begin{table*}[htb]
\begin{center}
\caption{NFE and FID scores of different backward ODEs in PFGM}
\label{table:exp}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{Algorithm} &   &\\
		\midrule
        \textbf{NFE} &   & \\
        \textbf{FID score} & 2.53 & 2.48 \\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Substitute the Predicted  Direction with the Ground-truth}
\label{app:sub}
Since the neural network cannot perfectly learn the ground-truth  direction, we replace the predicted  with the ground-truth direction when  is small. More specifically, given , recall that the empirical field is  where . Hence we can rewrite the empirical field as 

where . Furthermore we have . Together, the  component in the empirical field is .
The predicted normalized field~(on ) is trained to approximate the normalized field~(on ), \ie 

The last approximation is due to . Solving for , we get . Hence the  component in the normalized field after substituting the ground-truth is . In our experiments, we therefore replace the original prediction  with  when  during the backward ODE sampling for CIFAR-10/CelebA /LSUN bedroom . 

Table~\ref{table:subz} reports the NFE and FID score w/o and w/ the above substitution. We observe that the usage of ground-truth  direction in the near field accelerates the sampling speed.

\begin{table*}[htb]
\begin{center}
\caption{NFE and FID scores of w/ and w/o substitution}
\label{table:subz}
\begin{tabular}{c c c c}
		\toprule
		\textbf{Algorithm} &  w/o substitution &w/ substitution\\
		\midrule
        \textbf{NFE} &   & \\
        \textbf{FID score} &  &  \\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsection{Evaluation}

We use FID~\cite{Heusel2017GANsTB} and Inception scores~\cite{Salimans2016ImprovedTF} to quantitatively measure the sample quality, and NFE~(number of evaluation steps) for the inference speed. {FID (Fréchet Inception Distance) score is the Fréchet distance between two multivariate Gaussians, whose means and covariances are estimated from the 2048-dimensional activations of the Inception-v3~\citep{Szegedy2016RethinkingTI} network for real and generated samples respectively.} Inception score is the exponential mutual information between the predicted labels of the Inception network and the images. We also report bits/dim for likelihood evaluation. It is computed by dividing the negative log-likelihood by the data dimension, \ie .

For CIFAR-10, we compute the Fréchet distance between 50000 samples and the pre-computed statistics of CIFAR-10 dataset in \cite{Heusel2017GANsTB}. For CelebA , we follow the setting in \cite{Song2020ImprovedTF} where the distance is computed between 10000 samples and the test set. For model selection, we follow \cite{Song2020ImprovedTF} and pick the checkpoint with smallest FID every 50k iterations on 10k samples for computing all the scores.

\subsection{Effects of Step Size: FID versus NFE}

For preciseness, Table~\ref{table:fig5c} reports the exact numbers in \Figref{fig:adapt}. 
\begin{table*}[htb]
\begin{center}
\caption{The FID scores in \Figref{fig:adapt} of different methods and NFE.}
\label{table:fig5c}
\begin{tabular}{c c c c c}
		\toprule
		\textbf{Method / NFE} &  10 & 20 & 50 & 100\\
		\midrule
			\textbf{VP-ODE} & &  && \\
			\textbf{DDIM} && & & \\
        	\textbf{PFGM} &   & &  &  \\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

Since in the ODE  of PFGM, the  variable is a function of ~(), we integrate the  in the Euler method to reduce the discretization error. The vanilla update from time  to time  is , and the new update is . We empirically observe that the new update scheme significantly improve the FID score.

\section{Failure of VE/VP-ODE on NCSNv2 backbone}
\label{app:failure}

In \Figref{fig:ve}, we demonstrate the trajectories of cleaner samples/noisier samples/noisier samples w/ corrector. We visualize these three groups in \Figref{fig:failure-a} and \Figref{fig:failure-b}. The noisier samples are marked with red boxes in \Figref{fig:failure-a} and the remaining images in \Figref{fig:failure-a} are cleaner samples. The samples within green boxes in \Figref{fig:failure-b} are noisier samples w/ corrector. Samples on the same spatial locations in the two figures are generated by identical initial latents. 

The Gaussian kernels in score-based models are ~(VE) and ~(VP) \cite{Song2021ScoreBasedGM}. When  is large, the norms of perturbed samples are approximately . The backward ODE could break down if the trajectories diverge from the norm- relation, as shown by the noisier samples' trajectories in \Figref{fig:ve}. In contrast, the norm distributions of PFGM is approximately  when  is large~(see deviation for  in Appendix~\ref{app:prior_distribution}), which have a wider span for high density region~(see \Figref{fig:compare_prior}). The weak correlation between norm and  makes PFGM more robust on the lighter NCSNv2 backbone.


\begin{figure*}
    \centering
    \subfigure[Samples from VE-ODE (Euler)]{    \label{fig:failure-a}\includegraphics[width=0.4\textwidth]{img/ve_eu_images.png}}\hspace{10pt}
    \subfigure[Samples from VE-ODE (Euler w/ corrector)]{\label{fig:failure-b}\includegraphics[width=0.4\textwidth]{img/ve_eu_correct.png}}
    \caption{\textbf{(a)} Samples from VE-ODE (Euler w/o corrector). We highlight the noisier images with red boxes. The rest are cleaner images. \textbf{(b)} Samples from VE-ODE (Euler w/ corrector). We mark the noisier samples after correction with green boxes.}
\end{figure*}

\section{Extra Experiments}
\label{app:extra-exp}
\subsection{LSUN Bedroom }
\label{app:exp-lsun}
We report the FID scores and NFEs for LSUN bedroom dataset in Table~\ref{tab:lsun}. We adopt the code base of \cite{Song2021ScoreBasedGM} in our experiments. In \cite{Song2021ScoreBasedGM}, they experimented on the LSUN bedroom 256 256 dataset only on VE-SDE using a deeper NCSN++ backbone. In our DDPM++ architecture, we directly borrow the configuration of channels from the NCSN++ architecture~\cite{Song2021ScoreBasedGM} in each residual block (PFGM w/ NCSN++ channel). We further change  to , as it empirically gives better sample quality.

We also evaluate the performance when using the configuration of channels in the DDPM~\cite{Ho2020DenoisingDP} architecture (PFGM w/ DDPM channel). We use the RK45~\citep{Dormand1980AFO} solver in the Scipy library~\citep{Virtanen2020SciPy1F} for PFGM sampling. We report the FID score using the evaluation protocol in \cite{dhariwal2021diffusion}.
\begin{table}[htbp]
\begin{center}
\caption{FID/NFE on LSUN bedroom }\label{tab:lsun}
\begin{tabular}{l c c}
\toprule
      &FID  & NFE \\
    \midrule
    StyleGAN~\cite{karras2019style} &  &  \\
    DDPM~\cite{Ho2020DenoisingDP} & & \\
    VE-SDE~\cite{Song2021ScoreBasedGM} &   &  \\
    \midrule
    PFGM w/ NCSN++ channel &    &  \\
    PFGM w/ DDPM channel&    &  \\
     \bottomrule
\end{tabular}
\end{center}
\end{table}

Table~\ref{tab:lsun} shows that PFGM has comparable performance with VE-SDE when using DDPM channel, while achieving around 15 acceleration. We observe that PFGM achieves a better FID score using the similar configuration in the DDPM model, and converges faster — 150k over the total 2.4M training iterations suggested in \cite{Song2021ScoreBasedGM}. Remarkably, the VE-ODE baseline — the method most comparable to ours — only produces noisy samples on this dataset. It suggests that PFGM is able to scale up to high resolution images when using advanced architectures. 
We also compare with the number reported in \cite{Ho2020DenoisingDP} using similar architecture. Note that DDPM requires 1000 NFE during sampling, and doesn’t possess invertibility compared to flow models. 



\subsection{Results on NCSNv2 Architecture}

In this section, we demonstrate the image generation on CIFAR-10 and CelebA , using NCSNv2 architecture~\cite{Song2020ImprovedTF}, which is the predecessor of NCSN++ and DDPM++~\cite{Song2021ScoreBasedGM} and has smaller capacity. Since the VE/VP-ODE has poor performance~(FID greater than 90), with the RK45 solver, we also apply the forward Euler method~(\textbf{Euler}) with fixed number of steps. We explicitly name the sampler, with forward Euler method as predictor and Langevin dynamics as corrector, as \textbf{Euler w/ corrector}. For Euler w/ corrector in VE/VP-ODE, we use the probability flow ODE (reverse-time ODE) as the predictor and the Langevin dynamics (MCMC) as the corrector. We borrow all the hyper-parameters from \cite{Song2021ScoreBasedGM} except for the signal-to-noise ratio. We empirically observe the new configurations in Table~\ref{table:s2n} give better results on the NCSNv2 architecture.

To accommodate the extra dimension  on NCSNv2, we concatenate the image with an additional constant channel with value  and thus the first convolution layer takes in four input channels. We also add an additional output channel to the final convolution layer and take the global average pooling of this channel to obtain the direction on .

\begin{table}[htbp]
\begin{center}
\caption{Signal-to-noise ratio of different dataset-method pairs}
\label{table:s2n}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{Dataset-Method} &  CIFAR-10 - VE &CIFAR-10 - VP & CelebA - VE & CelebA - VP\\
		\midrule
        \textbf{signal-to-noise ratio} &   &  & &  \\
        \bottomrule
\end{tabular}
\end{center}
\end{table}

\label{app:ncsnv2}
\subsubsection{CIFAR-10}

Table~\ref{tab:cifar-ncsnv2} reports the image quality measured by Inception/FID scores and the inference speed measured by NFE on CIFAR-10, using a weaker architecture NCSNv2~\cite{Song2020ImprovedTF}. We show that PFGM with the RK45 solver has competitive FID/Inception scores with the Langevin dynamics, which was the best model on the NCSNv2 architecture before, and requires  less NFE. In addition, PFGM performs better than all the other ODE samplers. Our method is more tolerant of sampling error. Among the compared ODEs, our backward ODE~(\Eqref{eq:backode}) is the only one that successfully generates high quality samples while the VE/VP-ODE fail w/o the Langevin dynamics corrector. The backward ODE still beats the baselines w/ corrector. 

\begin{table}[htbp]
\small
    \centering
    \caption{CIFAR-10 sample quality~(FID, Inception) and number of function evaluation~(NFE). All the methods below the \textit{NCSNv2 backbone} separator use the NCSNv2~\citep{Song2020ImprovedTF} network architecture as the backbone.}
    \begin{tabular}{l c c c}
    \toprule
         & Inception   &FID  & NFE \\
         \midrule
         PixelCNN~\citep{Oord2016ConditionalIG} &  &  & \\
        IGEBM~\citep{Du2019ImplicitGA} &  &  & \\
        WGAN-GP~\citep{Gulrajani2017ImprovedTO} &  & & \\
        SNGAN~\citep{Miyato2018SpectralNF} &  &  & \\
        NCSN~\citep{Song2019GenerativeMB} &  &  & \\
        \midrule
        \textit{\textbf{NCSNv2 backbone}}\\
        \midrule
        Langevin dynamics~\citep{Song2020ImprovedTF} &  &  & \\
        VE-SDE~\citep{Song2021ScoreBasedGM} & & &\\
        VP-SDE~\citep{Song2021ScoreBasedGM} & & &\\
        \midrule
VE-ODE~(Euler w/ corrector) &  &   &  \\
        VP-ODE~(Euler w/ corrector) &  &   &  \\
PFGM~(Euler)&   &   &  \\
PFGM~(RK45)&  &   &  \\
         \bottomrule
    \end{tabular}
    \label{tab:cifar-ncsnv2}
\end{table}


\subsubsection{CelebA}

In Table~\ref{tab:celeba}, we report the quality of images generated by models trained on CelebA , as measured by the FID scores, and the sampling speed, as measured
by NFE. We use this dataset as our preliminary experiments hence we only apply NCSNv2~\cite{Song2020ImprovedTF} for different baselines. As shown in Table~\ref{tab:celeba}, PFGM achieves best FID scores than all the baselines on CelebA dataset, while accelerating the inference speed around . Remarkably, PFGM outperforms the Langevin dynamics and reverse-time SDE samplers, which are usually considered better than their deterministic counterparts. 

\paragraph{Remark: On the FID scores on CelebA } {One interesting observation is that the samples of PFGM (RK45)~(\Figref{pfgm_ncsnv2_celeba}) contain more obvious artifacts than Langevin dynamics~(\Figref{fig:ncsnv2_celeba}), although PFGM has a lower FID score on the same architecture. We hypothesize that the diversity of samples has larger effects on the FID scores than the artifacts. As shown in \Figref{fig:ncsnv2_celeba} and \Figref{pfgm_ncsnv2_celeba}, samples generated by PFGM have more diverse background colors and hair colors than samples of Langevin dynamics. In addition, we evaluate the performance of PFGM on the DDPM++ architecture. We show that the FID score can be further reduced to  using the more advanced DDPM++ architecture. By examining the generated samples of PFGM on DDPM++~(\Figref{fig:extend-celeba}), we observe that the samples are diverse and exhibit fewer artifacts than PFGM on NCSNv2. It suggests that by using a more powerful architecture like DDPM++, we can remove the artifacts while retaining the diversity in PFGM.}




\begin{figure}[htbp]
    \centering
    \subfigure[Langevin dynamics~\cite{Song2019GenerativeMB}]{\label{fig:ncsnv2_celeba}\includegraphics[width=0.49\textwidth]{img/ncsnv2_celeba.png}}\hfill
    \subfigure[PFGM~(RK45)]{\label{pfgm_ncsnv2_celeba}\includegraphics[width=0.49\textwidth]{img/pfgm_ncsnv2_celeba.png}}\hfill
    \caption{Uncurated samples from Langevin dynamics~\cite{Song2019GenerativeMB} and PFGM~(RK45), both using the NCSNv2 architecture.}
    \label{fig:celeba_examine}
\end{figure}


\begin{table}[htbp]
\begin{center}
\caption{FID/NFE on CelebA }\label{tab:celeba}
\begin{tabular}{l c c}
\toprule
      &FID  & NFE \\
     \midrule
NCSN~\citep{Song2019GenerativeMB} &   & \\
     \midrule
    \textit{\textbf{NCSNv2 backbone}}\\
    \midrule
    Langevin dynamics~\citep{Song2020ImprovedTF} &   & \\
    VE-SDE~\citep{Song2021ScoreBasedGM} &  & \\
    VP-SDE~\citep{Song2021ScoreBasedGM} & & \\
    \midrule
VE-ODE~(Euler w/ corrector)  &   &  \\
    VP-ODE~(Euler w/ corrector)  &   &  \\
    PFGM~(Euler) &    &  \\
    PFGM~(RK45)&    &  \\
    \midrule
    \textit{\textbf{DDPM++ backbone}}\\
    \midrule
     PFGM~(RK45) &  & \\
     \bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{{Wall-clock Sampling Time}}

The main bottleneck of sampling time in each ODE step is the function evaluation of the neural network. Hence, for different ODE equations using similar neural network architectures, their inference times per ODE step are approximately the same.

We implement PFGM on the NCSNv2~\cite{Song2020ImprovedTF}, DDPM++~\cite{Song2021ScoreBasedGM}, and DDPM++ deep~\cite{Song2021ScoreBasedGM} architectures, with sight modifications to account for the extra dimension . In Table~\ref{table:wall-clock}, we report the sampling time per ODE step method with the DDPM++ backbone, as well as the total sampling time. We measure the sampling time of generating a batch of 1000 images on CIFAR-10. We compare PFGM, VP/sub-VP ODEs using the RK45 solver. As a reference, we also report the results of VP-SDE using the predictor-corrector sampler~\cite{Song2021ScoreBasedGM}. All the numbers are produced on a single NVIDIA A100 GPU.

\begin{table*}[htb]
\begin{center}
\caption{Wall-clock sampling time~(second)}
\label{table:wall-clock}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{Method} &  PFGM &VP-ODE& sub-VP-ODE& VP-SDE (PC)\\
		\midrule
        \textbf{NFE} & 110 & 134 &146 & 1000\\
		\midrule
        \textbf{Wall-clock time per step} & 0.526 & 0.522 &0.520 & 0.491\\
		\midrule
        \textbf{Total wall-clock time} & 57.81 & 69.97 &75.92&490.65\\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

As expected, ODEs using similar architectures and the same solver have nearly the same wall-clock time per ODE step. The table also shows that PFGM achieves the smallest total wall-clock sampling time. 

\subsection{Image Interpolations}
\label{app:interpolate}

The invertibility of the ODE in PFGM enables the interpolations between pairs of images. As shown in \Figref{fig:interpolation}, we adopt the spherical interpolations between the latent representations of the images in the first and last column.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/interpolation_ddpmpp.png}
     \caption{Interpolation on CelebA  by PFGM}\label{fig:interpolation}
\end{figure}

\subsection{Temperature Scaling}
\label{app:temp}

To demonstrate more utilities of the meaningful latent space of PFGM, we include the experiments of temperature scaling on CelebA  dataset. We linearly increase the norm of latent codes from  to  to get the samples in \Figref{fig:temperature}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/rescale_ddpmpp.png}
    \caption{Temperature scaling on CelebA  by PFGM}
    \label{fig:temperature}
\end{figure}

\section{Extended Examples}
\label{app:samples}
We provide extended samples from PFGM on CIFAR-10~(\Figref{fig:extend-cifar}), CelebA ~(\Figref{fig:extend-celeba}) and {LSUN bedroom ~(\Figref{fig:extend-bedroom-ddpm}) datasets.}


\section{Physical Interpretation of the ODEs in PFGM}\label{app:honey}

{In Section~\ref{section:bg}, in order to move the particles along the electric lines, we set the time derivative of  to the Poisson field :}

{{We give the interpretation of the ODEs from a physical perspective. Newton's law implies that the external force is proportional to the acceleration of the particle. In the overdamped limit, e.g., when the particle is moving in honey, the external force is instead proportional to the velocity of the particle, making the equation of motion a first-order ODE.} 
Denoting the viscosity of the fluid as , the dynamics of the particle under the influence of the electric field of the source  is}

{which has an overdamped limit   when we set  and . In this case, a particle with mass  and charge  would follow the electric field with velocity equal to , justifying Eq.~(\ref{eq:ode2}).}

\section{Limitations and Future Directions}
\label{app:limit}

In Section~\ref{sec:learning} we discuss the training paradigm of PFGM, including the normalized Poisson field and the discretized forward ODE. There are several potential improvements. First, the normalized field on mini-batch is biased. In this paper, we directly alleviate the bias by using a larger training batch. However, it does not solve the problem fundamentally. Some potential directions are incorporating more physical tools: we can exploit renormalization to make the Poisson field well-behaved in near fields. Another possibility is to replace a point charge with a quantum particle, whose position uncertainty fills the empty space among nearest neighbor data samples and makes the data manifold smoother. 


\section{Potential Social Impact}
\label{app:impact}

Generative models is a rapidly growing field of study with far-reaching implications for science and society. 
Our work proposes a new generative model that allows for high-quality samples, quick inference and adaptivity. Many downstream applications benefit from our PFGM models' powerful expressive capabilities, particularly those that need fast inference speed and good sample quality at the same time. The usage of these models might have both positive and negative outcomes depending on the downstream use case. 
For example, PFGM can be incorporated in producing good image/audio samples by the fast backward ODE. This, on the other hand, promotes \textit{deepfake} technology and leads to social scams. Generative models are also brittle and susceptible to backdoor adversarial attacks on publicly available training data, causing unanticipated failure.
Addressing the above concerns requires further research in providing robustness guarantees for generative models as well as close collaborations with researchers in socio-technical disciplines.




\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/cifar10_ddpmpp.png}
    \caption{CIFAR-10 samples from PFGM (RK45)}
    \label{fig:extend-cifar}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/celeba_ddpmpp.png}
    \caption{CelebA  samples from PFGM (RK45, NCSNv2 architecture)}
    \label{fig:extend-celeba}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/bedroom_ddpmpp.png}
    \caption{LSUN bedroom  samples from PFGM (RK45) using DDPM channel configuration.}
    \label{fig:extend-bedroom-ddpm}
\end{figure*}
\clearpage




