\newpage

{\huge Appendix}


\def\E{{\bf E}}
\def\x{{\bf x}}
\def\r{{\bf r}}
\def\rhat{\hat{r}}

\section{Proofs}
\label{app:proofs}
\subsection{Formal Proof of Theorem 1}

Before proceeding to Theorem 1, we show a technical lemma that guarantees the existence-uniqueness of the solution to the Poisson equation, under some mild conditions. 
\begin{lemma}
\label{lemma:ex-uni}
{Given $\Omega=\mathbb{R}^N, N\ge 3$, assume that the source function $\rho \in \gC^0(\Omega)$, and $\rho$ has a compact support. Then the the Poisson equation $\nabla^2\varphi(\mat{x})=-\rho(\mat{x})$ on $\Omega$ with zero boundary condition at infinity~($\lim_{\parallel \rvx \parallel_2 \to \infty}\varphi(\mat{x})=0$) has a unique solution $\varphi(\mat{x})\in \gC^2(\Omega)$ up to a constant.}
\end{lemma}

\begin{proof}

For the existence of the solution, one can verify that the analytical construction using the extension of Green's function in $N\ge 3$ dimensional space~(Lemma~\ref{lemma:green}), \ie $
    \varphi(\mat{x}) = \int G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y},   G(\mat{x},\mat{y}) = \frac{1}{(N-2)S_{N-1}(1)} \frac{1}{||\mat{x}-\mat{y}||^{N-2}}
$, is one possible solution to the Poisson equation $\nabla^2\varphi(\mat{x})=-\rho(\mat{x})$. Since $\rho\in \gC^0(\Omega)$ and $\nabla^2\varphi(\mat{x})=-\rho(\mat{x})$, we conclude that $\varphi(\mat{x})\in \gC^2(\Omega)$.

The proof idea of the uniqueness is similar to the uniqueness theorems in electrostatics. Suppose we have two different solutions $\varphi_1,\varphi_2\in \gC^2$ which satisfy 
\begin{equation}
    \nabla^2\varphi_1(\mat{x}) = -\rho(\mat{x}), \nabla^2\varphi_2(\mat{x}) = -\rho(\mat{x}).
\end{equation}
We define $\tilde{\varphi}(\mat{x})\equiv\varphi_2(\mat{x})-\varphi_1(\mat{x})$. Subtracting the above two equations gives
\begin{equation}\label{eq:unique_1}
    \nabla^2\tilde{\varphi}(\mat{x}) = 0, \forall \rvx \in \Omega.
\end{equation}
By the  vector differential identity we have
\begin{equation}\label{eq:unique_2}
    \tilde{\varphi}(\mat{x})\nabla^2\tilde{\varphi}(\mat{x}) = \nabla\cdot(\tilde{\varphi}(\mat{x})\nabla\tilde{\varphi}(\mat{x}))-\nabla\tilde{\varphi}(\mat{x})\cdot \nabla\tilde{\varphi}(\mat{x}),
\end{equation}
By the divergence theorem we have
\begin{equation}\label{eq:unique_3}
    \int_{\Omega} \nabla\cdot(\tilde{\varphi}(\mat{x})\nabla\tilde{\varphi}(\mat{x})) d^N\mat{x} = \oiint_{\partial\Omega} \tilde{\varphi}(\mat{x})\nabla\tilde{\varphi}(\mat{x})\cdot d^{N-1}\mat{S} = 0,
\end{equation}
where $d^{N-1}\mat{S}$ denotes an $N-1$ dimensional surface element at infinity, and the second equation holds due to zero boundary condition at infinity. Combining Eq.~(\ref{eq:unique_1})(\ref{eq:unique_2})(\ref{eq:unique_3}), we have
\begin{equation}
    \int_{\Omega} \nabla\cdot(\tilde{\varphi}(\mat{x})\nabla\tilde{\varphi}(\mat{x})) d^N\mat{x}=\int_{\Omega} ||\nabla\tilde{\varphi}(\mat{x})||^2 d^N\mat{x}=0,
\end{equation}
since this is an integral of a positive quantity, we must have $\nabla\tilde{\varphi}(\mat{x})=\mat{0}$, or $\tilde{\varphi}(\mat{x})=c$, $\forall\mat{x}\in\Omega$. This means $\varphi_1$ and $\varphi_2$ differ at most by a constant, but a constant does not affect gradients, so $\nabla\varphi_1(\mat{x})=\nabla\varphi_2(\mat{x})$. \end{proof}


{In our method section~(Section~\ref{sec:augment}), we augmented the original $N$-dimensional data with an extra dimension. The new data distribution in the augmented space is $\tilde{p}(\tilde{\mat{x}})=p(\mat{x})\delta(z)$, where $\delta$ is the Dirac delta function. The support of the data distribution is in the $z=0$ hyperplane. In the following lemma, we show the existence and uniqueness of the solution to $\nabla^2\varphi(\tilde{\rvx})=-\tilde{p}(\tilde{\rvx})$ outside the data support.}

\begin{lemma}
\label{lemma:data}
{Assume the support of the data distribution in the augmented space~($\textrm{supp}(\tilde{p}(\tilde{\rvx}))$) is a compact set on the $z=0$ hyperplane, $p(\rvx) \in \gC^0$ and  $N\ge 3$. The Poisson equation $\nabla^2\varphi(\tilde{\rvx})=-\tilde{p}(\tilde{\rvx})$ with zero boundary condition at infinity~($\lim_{\parallel \rvx \parallel_2 \to \infty}\varphi(\tilde{\rvx})=0$) has a unique solution $\varphi(\tilde{\rvx}) \in \gC^2$ for $\tilde{x} \in \mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))$, up to a constant.}
\end{lemma}
\begin{proof}

Similar to the proof in Lemma~\ref{lemma:ex-uni}, one can easily verify that the analytical construction using Green's method, \ie $\varphi(\tilde{\rvx}) = \int G(\tilde{\rvx},\tilde{\rvy})\tilde{p}(\tilde{\rvx})d\tilde{\rvy}, G(\tilde{\rvx},\tilde{\rvy}) = \frac{1}{(N-1)S_{N}(1)} \frac{1}{||\tilde{\rvx}-\tilde{\rvy}||^{N-1}}$, is one possible solution to the Poisson equation $\nabla^2\varphi(\tilde{\rvx})=-\tilde{p}(\tilde{\rvx})$. Since $\tilde{p}(\tilde{\rvx})=0$ for  $\tilde{\rvx} \in \mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))$ and $\nabla^2\varphi(\tilde{\rvx})=-\tilde{p}(\tilde{\rvx})$, we conclude that $\varphi(\tilde{\rvx})\in \gC^2(\mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx})))$.

For the uniqueness, suppose we have two different solutions $\varphi_1,\varphi_2\in \gC^2(\mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx})))$ which satisfy 
\begin{equation}
    \nabla^2\varphi_1(\tilde{\rvx}) = -\tilde{p}(\tilde{\rvx}), \nabla^2\varphi_2(\tilde{\rvx}) = -\tilde{p}(\tilde{\rvx}).
\end{equation}
We define $\tilde{\varphi}(\tilde{\rvx})\equiv\varphi_2(\tilde{\rvx})-\varphi_1(\tilde{\rvx})$. Subtracting the above two equations gives
\begin{equation}\label{eq:2unique_1}
    \nabla^2\tilde{\varphi}(\tilde{\rvx}) = 0, \forall \tilde{\rvx} \in \mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx})).
\end{equation}
By the vector differential identity we have
\begin{equation}\label{eq:2unique_2}
    \tilde{\varphi}(\tilde{\rvx})\nabla^2\tilde{\varphi}(\tilde{\rvx}) = \nabla\cdot(\tilde{\varphi}(\tilde{\rvx})\nabla\tilde{\varphi}(\tilde{\rvx}))-\nabla\tilde{\varphi}(\tilde{\rvx})\cdot \nabla\tilde{\varphi}(\tilde{\rvx}),
\end{equation}
By the divergence theorem we have
\begin{equation}\label{eq:2unique_3}
    \int_{\mathbb{R}^{N+1}} \nabla\cdot(\tilde{\varphi}(\tilde{\rvx})\nabla\tilde{\varphi}(\tilde{\rvx})) d^{N+1}\tilde{\rvx} = \oiint_{\partial\mathbb{R}^{N+1}} \tilde{\varphi}(\tilde{\rvx})\nabla\tilde{\varphi}(\tilde{\rvx})\cdot d^{N}\mat{S} = 0,
\end{equation}
where $d^{N}\mat{S}$ denotes an $N$ dimensional surface element at infinity, and the second equation holds due to zero boundary condition at infinity. Combining Eq.~(\ref{eq:2unique_1})(\ref{eq:2unique_2})(\ref{eq:2unique_3}), we have
\begin{align*}
    \int_{\mathbb{R}^{N+1}} \nabla\cdot(\tilde{\varphi}(\tilde{\rvx})\nabla\tilde{\varphi}(\tilde{\rvx})) d^{N+1}\tilde{\rvx} &= \int_{\mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))} \nabla\cdot(\tilde{\varphi}(\tilde{\rvx})\nabla\tilde{\varphi}(\tilde{\rvx})) d^{N+1}\tilde{\rvx}\\
    &=\int_{\mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))} ||\nabla\tilde{\varphi}(\tilde{\rvx})||^2 d^{N+1}\tilde{\rvx}=0,
\end{align*}
The first equation holds because Lebesgue measure of $\textrm{supp}(\tilde{p}(\tilde{\rvx}))$ is zero. Since $||\nabla\tilde{\varphi}(\tilde{\rvx})||^2$ is an integral of a positive quantity, we must have $\nabla\tilde{\varphi}(\tilde{\rvx})=\mat{0}$, or $\tilde{\varphi}(\tilde{\rvx})=c$, $\forall\tilde{\rvx}\in\mathbb{R}^{N+1}\setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))$. This means $\varphi_1$ and $\varphi_2$ differ at most by a constant function, but a constant does not affect gradients, so $\nabla\varphi_1(\tilde{\rvx})=\nabla\varphi_2(\tilde{\rvx})$. \end{proof}


\begin{figure*}
    \centering
    \includegraphics[width=0.6\textwidth, trim=0cm 3cm 0cm 3cm]{img/proof_plot_2.pdf}
    \caption{Proof idea of Theorem~\ref{theorem2}. By Gauss's Law, the outflow flux $d\Phi_{out}$ equals the inflow flux $d\Phi_{in}$. The factor of two in $p(\rvx)dA/2$ is due to the symmetry of Poisson fields in $z<0$ and $z>0$.
    }
        \label{fig:theorem2}
\end{figure*}

As illustrated in \Figref{fig:theorem2}, there is a bijective mapping between the upper hemisphere of radius $r$ and the $z=0$ plane, where each pair of corresponding points is connected by an electric field line. 
We will now formally prove that, in the $r\to\infty$ limit, this mapping transforms the arbitrary charge distribution in the source plane (that generated the electric field) into a uniform distribution on the hemisphere. 
\begin{theorem}
\label{theorem2}
 {Suppose particles are sampled from a uniform distribution on the upper ($z>0$) half of the sphere of radius $r$ and evolved by the backward ODE $\frac{d\mat{\tilde{x}}}{dt}=-\mat{E}(\mat{\tilde{x}})$ until they reach the $z=0$ hyperplane, where the Poisson field $\mat{E}(\mat{\tilde{x}})$ is generated by the source $\tilde{p}(\tilde{\mat{x}})$. In the $r\to \infty$ limit, {under the conditions in Lemma~\ref{lemma:data}}, this process generates a particle distribution $\tilde{p}(\tilde{\mat{x}})$, 
i.e., a distribution $p(\mat{x})$ in the $z=0$ hyperplane.} 
\end{theorem}
\begin{proof}

{By Lemma~\ref{lemma:data}, we know that with zero boundary at infinity, the Poisson equation $\nabla^2\varphi(\tilde{\rvx})=-\tilde{p}(\tilde{\rvx})$ has a unique solution $\varphi(\tilde{\rvx}) \in \gC^2$ for $\tilde{\rvx} \in \mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))$. Hence $\mat{E}(\mat{\tilde{x}})= -\nabla \varphi(\tilde{\rvx}) \in \gC^1$, guaranteeing the existence-uniqueness of the solution to the ODE $\frac{d\mat{\tilde{x}}}{dt}=-\mat{E}(\mat{\tilde{x}})$ according to Theorem 2.8.1 in \cite{Ricardo2002AMI}.}

Consider the tube in \Figref{fig:theorem2} connecting an area on $dA$ in the $z=\epsilon\to 0^+$ hyperplane ($S_3$) to a solid angle $d\Omega$ on the hemisphere ($S_1$), with $S_2$ as its side. The tube is the space swept by $dA$ following electric field $\mat{E}$, so by definition the electric field is parallel to the tangent space of the tube sides $S_2$. The bottom of the tube $S_3$ is located at $z=\epsilon\to 0^+$, a bit above the $z=0$ plane, so the tube does not enclose any charges.  We note that the divergence of Poisson field is zero in $\mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))$:
\begin{align*}
    \nabla\cdot \mat{E}(\mat{\tilde{x}}) = -\nabla^2\varphi(\tilde{\rvx})=\tilde{p}(\tilde{\rvx})=0, \forall \tilde{\rvx} \in \mathbb{R}^{N+1} \setminus \textrm{supp}(\tilde{p}(\tilde{\rvx}))
\end{align*}
Denote the volume and surface of the tube as $V$ and $\mat{B}$. According to divergence theorem, $ \oiint \mat{E}(\mat{\tilde{x}})\cdot d\mat{B}=\int_V \nabla\cdot \mat{E}(\mat{\tilde{x}}) dV=0$. Hence the net flux leaving the tube is zero:
\begin{equation}\label{eq:tube_zeronetflux}
    \Phi_{S_1} + \Phi_{S_2} + \Phi_{S_3} = 0, \quad
    \Phi_{S_i} \equiv \oiint_{S_i} \mat{E}(\mat{\tilde{x}})\cdot d\mat{B} \quad (i=1,2,3)
\end{equation}


There is no flux through the sides, i.e., $\Phi_{S_2}=0$, since $\mat{E}(\mat{\tilde{x}})$ is orthogonal to the surface element $d\mat{B}$ on the tube sides by definition. As a result, the flux $\Phi_{S_3}$ entering from below must equal the flux $\Phi_{S_1}$ leaving the other end. Denote the $l_2$ norm of the vector $\r$ as $r$. We first calculate the influx $\Phi_{S_3}$. To do so, we study a Gaussian pillbox whose top, side and bottom are $S_3$, $S_4$ and $S_5$. $S_3$ and $S_5$ are located at $z=\epsilon$ and $z=-\epsilon$ ($\epsilon\to 0^+$). Denote the volume and surface of the pillbox as $V'$ and $\mat{B}'$. The pillbox contains charge $p(\mat{x})dA$, so according to Gauss's law $ \oiint \mat{E}(\mat{\tilde{x}})\cdot d\mat{B}'=\int_{V'} \nabla\cdot \mat{E}(\mat{\tilde{x}}) dV'=\int_{V'}\tilde{p}(\tilde{\mat{x}})dV'=p(\mat{x})dA$, i.e.,
\begin{equation}
    \Phi'_{S_3} + \Phi'_{S_4} + \Phi'_{S_5} = p(\mat{x})dA,\quad \Phi'_{S_i} \equiv \oiint_{S_i} \mat{E}(\mat{\tilde{x}})\cdot d\mat{B}' \quad (i=3,4,5)
\end{equation}
The flux on the sides $\Phi'_{S_4}\propto\epsilon\to 0$, and $\Phi_{S_3}'=\Phi_{S_5}'$ due to mirror symmetry of $z=0$. So $\Phi_{S_3}'=\Phi_{S_5}'=p(\mat{x})dA/2$.  Note on the $S_3$ surface, the outflux of the pillbox is exactly the influx of the tube, so we have:
\begin{align*}
    \Phi_{S_3}=-\Phi_{S_3}' = -p(\x)dA/2\numberthis \label{eq:in},
\end{align*}
inserting which and $\Phi_{S_2}=0$ to Eq.~(\ref{eq:tube_zeronetflux}) gives 
\begin{equation}\label{eq:Phi_S1_1}
    \Phi_{S_1} = -\Phi_{S_3} = p(\mat{x})dA/2.
\end{equation}
On the other hand, in the far-field limit $r\to\infty$, since $\textrm{supp}(p(\rvx))$ is bounded, the data distribution can be effectively seen as a point charge (see Appendix~\ref{sec:mul}). By Lemma~\ref{lemma:point}, we have $\lim_{r\to\infty}\E(\r)=- \lim_{r\to\infty}\nabla\varphi(\r) = \frac{{\r}}{S_N(1) r^{N+1}}$. The resulting outflux on the hemisphere is
\begin{align*}
    \Phi_{S_1}=E_r r^N d\Omega=d\Omega/S_N(1) \numberthis \label{eq:out}
\end{align*}
where $E_r\equiv\mat{E}(\rvr) \cdot{\rvr}/{r}$ is the radial component of $\mat{E}$. Comparing \Eqref{eq:Phi_S1_1} and \Eqref{eq:out} yields 
$d\Omega/dA=p(\x)S_N(1)/2\propto p(\x)$.
In other words, the mapping from the $z=0$ hyperplane to the hemisphere dilutes the charge density ${p}({\mat{x}})$ up to a constant factor. Thus by change-of-varible, we conclude that the mapping transforms the data distribution into a uniform distribution on the infinite hemisphere. Since the ODE is reversible, the backward ODE transforms the uniform distributoin on the infinite hemisphere to the distribution $\tilde{p}(\tilde{\rvx})$.
\end{proof}



\subsection{Multipole Expansion}
\label{sec:mul}
We discuss the behaviors of the potential function in Poisson equation~(\Eqref{eq:poisson}) under different scenarios, utilizing the multipole expansion. Suppose we have a unit point charge $q=1$ located at $\mat{x}\in\mathbb{R}^N$. We know that the potential function at another point $\mat{y}\in\mathbb{R}^N$ is $\varphi(\mat{y}-\mat{x})=1/||\mat{y}-\mat{x}||^{N-2}$ (ignoring a constant factor). Now we assume that $\mat{x}$ is close to the origin such that we can Taylor expand around $\mat{x}=0$:
\begin{equation}
    \varphi(\mat{y}-\mat{x})= \varphi(\mat{y}) - \sum_{\alpha=1}^N \mat{x}_\alpha \varphi_\alpha(\mat{y})+\frac{1}{2}\sum_{\alpha=1}^N\sum_{\beta=1}^N \mat{x}_\alpha\mat{x}_\beta \varphi_{\alpha\beta}(\mat{y})-...
\end{equation}
where
\begin{equation}
\begin{aligned}
    &\varphi_\alpha(\mat{y})=\left(\frac{\partial \varphi(\mat{y}-\mat{x})}{\partial \mat{x}_\alpha}\right)_{\mat{x}=0}=(N-2)\frac{\mat{y}_\alpha}{||\mat{y}||^{N}} \\ &\varphi_{\alpha\beta}(\mat{y})=\left(\frac{\partial^2 \varphi(\mat{y}-\mat{x})}{\partial \mat{x}_\alpha\partial \mat{x}_\beta}\right)_{\mat{x}=0}=(N-2)\frac{N\mat{y}_\alpha\mat{y}_\beta-||\mat{y}||^2\delta_{\alpha\beta}}{||\mat{y}||^{N+2}}
\end{aligned}
\end{equation}
In the case where the source is a distribution $p(\mat{x})$, the potential $\varphi(\mat{y})$ can again be Taylor expanded:
\begin{equation}
        \varphi(\mat{y}) = q\varphi(\mat{y}) + \sum_{\alpha=1}^ Nq_\alpha\varphi_\alpha (\mat{y}) + \sum_{\alpha=1}^N\sum_{\beta=1}^N q_{\alpha\beta}\varphi_{\alpha\beta}(\mat{y}) -...
\end{equation}
where 
\begin{equation}
    q = \int p(\mat{x})d\mat{x}, q_\alpha = \int p(\mat{x})\mat{x}_\alpha d\mat{x}, q_{\alpha\beta} = \int p(\mat{x})\mat{x}_\alpha\mat{x}_\beta d\mat{x},
\end{equation}
which are called monopole, dipole and quadrupole in physics, respectively. The gradient field $\mat{E}\mat(y)=\nabla\Phi(\mat{y})$ can be expanded in the same such that
\begin{equation}
    \mat{E}(\mat{y}) = \mat{E}^{(0)}(\mat{y})+\mat{E}^{(1)}(\mat{y})+\mat{E}^{(2)}(\mat{y})+...
\end{equation}
It is easy to check that $||\mat{E}^{(i)}(\mat{y})||$ decays as $1/||\mat{y}||^{N-2+i}$, which means higher-order corrections decay faster than leading terms. So when $||\mat{y}||\to \infty$, only the monopole term $||\mat{E}^{(0)}(\mat{y})||$ matters, which behaves like a point source.

In a more realistic setup, we only have a large but finite $||\mat{y}||$, so the question is: under what condition is the point source approximation valid? We examine $\varphi^{(0)}$, $\varphi^{(1)}$ and $\varphi^{(2)}$ more carefully:
\begin{equation}\label{eq:phi_expansion}
\begin{aligned}
    &\varphi^{(0)} = \frac{1}{||\mat{y}||^{N-2}} \\
    &\varphi^{(1)} = \sum_{\alpha=1}^N (N-2)\frac{\mat{y}_\alpha\mat{x}_\alpha}{||\mat{y}||^N}=(N-2)\frac{\mat{x}^T\mat{y}}{||\mat{y}||^N}\\
    &\varphi^{(2)}=\frac{1}{2}\sum_{\alpha=1}^N\sum_{\beta=1}^N (N-2)\frac{N\mat{y}_\alpha\mat{y}_\beta-||\mat{y}||^2\delta_{\alpha\beta}}{||\mat{y}||^{N+1}}\mat{x}_\alpha\mat{x}_\beta=\frac{N-2}{2}\frac{N(\mat{x}^T\mat{y})^2-||\mat{x}||^2||\mat{y}||^2}{||\mat{y}||^{N+2}}
\end{aligned}
\end{equation}
Since $\varphi^{(1)}$ is an odd function of $\mat{x}$, integrating $\varphi^{(1)}$ over $\mat{x}$ leads to zero (samples are normalized to zero mean). In machine learning applications, $N$ is usually a large number (although in physics $N$ is merely 3). If $\mat{y}$ is a random vector of length $||\mat{y}||$, then $\mat{x}^T\mat{y}\sim (\frac{1}{\sqrt{N}}\pm\frac{1}{N})||\mat{x}||||\mat{y}||$. So Eq.~(\ref{eq:phi_expansion}) can be approximated as 
\begin{equation}
    \varphi^{(0)}\sim \frac{1}{||\mat{y}||^{N-2}}, \varphi^{(2)}\sim \frac{{\sqrt{N}}}{2}\frac{||\mat{x}||^2}{||\mat{y}||^N}
\end{equation}
Requiring $\int \varphi^{(0)} p(\rvx) d\rvx \gg \int \varphi^{(2)} p(\rvx) d \rvx$ gives $||\mat{y}||^2\gg \sqrt{N}\mathop{\mathbb{E}}_{p(x)}||\mat{x}||^2$. So the condition for the point source approximation to be valid is:
\begin{equation}
    \kappa=\frac{2||\mat{y}||^2}{\sqrt{N}\mathop{\mathbb{E}}_{p(x)}||\mat{x}||^2}\gg 1 
\end{equation}
Based on this condition, we can partition space into three zones: (1) the far zone $\kappa\gg1$, where the point source approximation is valid; (2) the intermediate zone $\kappa\sim O(1)$, where the gradient field has moderate curvature; (3) the near zone $\kappa\ll 1$, where the gradient field has high curvature. In practice, the initial value $||\rvy||$ is greater than 1000 (hence $\kappa \gg 1$) with high probability on CIFAR-10 and CelebA datasets, incidating that the initial samples lie in the far zone and gradually move toward the near zone where $||\rvy|| \approx ||\rvx||$ ($\kappa \ll 1$).

We summarize above observations in the following lemma in the $||\rvy|| \to \infty$ limit: 
\begin{lemma}
\label{lemma:point}
Assume the data distribution $p(\rvx) \in \gC^0$ has a compact support in $\mathbb{R}^N$, then the solution $\varphi$ to the Poisson equation $\nabla^2\varphi(\mat{x})=-p(\mat{x})$ with zero boundary condition at infinity satisfies $\lim_{\parallel \rvx \parallel_2 \to \infty} \nabla \varphi(\rvx) = -\frac{1}{S_{N-1}(1)}\frac{\rvx}{\parallel \rvx\parallel^N_2 }$.
\end{lemma}
\begin{proof}
By Lemma~\ref{lemma:ex-uni}, the gradient of the solution has the following form:
\begin{align*}
    \nabla \varphi(\mat{x}) = \int\ \nabla_\mat{x}G(\mat{x},\mat{y})p(\mat{y})d\mat{y},\quad \nabla_\mat{x} G(\mat{x},\mat{y}) = -\frac{1}{S_{N-1}(1)} \frac{\mat{x}-\mat{y}}{||\mat{x}-\mat{y}||^{N}}.
\end{align*}

Since $p(\rvx)$ has a bounded support, we assume $\max\{\parallel \rvx \parallel_2: p(\rvx) \not = 0\}<B$. On the other hand, we have
\begin{align*}
    \lim_{\parallel \rvx \parallel_2 \to \infty}\nabla_\mat{x} G(\mat{x},\mat{y}) =\lim_{\parallel \rvx \parallel_2 \to \infty} -\frac{1}{S_{N-1}(1)} \frac{\mat{x}-\mat{y}}{||\mat{x}-\mat{y}||^{N}}=\lim_{\parallel \rvx \parallel_2 \to \infty}-\frac{1}{S_{N-1}(1)} \frac{\mat{x}}{||\mat{x}||^{N}}
\end{align*}
for $\forall y$ such that $\parallel y \parallel_2 < B$. Hence,
\begin{align*}
    \lim_{\parallel \rvx \parallel_2 \to \infty} \nabla \varphi(\rvx) =\lim_{\parallel \rvx \parallel_2 \to \infty}\int\ \nabla_\mat{x}G(\mat{x},\mat{y})p(\mat{y})d\mat{y}&= \int\lim_{\parallel \rvx \parallel_2 \to \infty} \nabla_\mat{x}G(\mat{x},\mat{y})p(\mat{y})d\mat{y}\\
    &=-\frac{1}{S_{N-1}(1)}\frac{\rvx}{\parallel \rvx\parallel^N_2 }
\end{align*}

\end{proof}







\subsection{{Extension of Green's Function in $N$-dimensional Space}}
\label{app:green}
In this section, we show that the function $G(\rvx, \rvy)$ defined in \Eqref{eq:poisson_solution} is the $N$-dimensional extension of the Green's function,     $\varphi(\mat{x}) = \int G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y}$ solves the Poisson equation $\nabla^2\varphi(\mat{x}) = -\rho(\mat{x})$.
\begin{lemma}
\label{lemma:green}
Assume the dimension $N\ge 3$, and the source term satisfies $\rho \in \gC^0(\Omega), \int_{\mathbb{R}^N}\rho^2(\rvx)d\rvx<+\infty, \lim_{\parallel \rvx \parallel_2 \to \infty} \rho(\rvx) = 0$. The extension of Green's function $G(\mat{x},\mat{y})=\frac{1}{(N-2)S_{N-1}(1)}\frac{1}{||\mat{x}-\mat{y}||^{N-2}}$ solves the Poisson equation $\nabla^2_{\mat{x}} G(\mat{x},\mat{y})=-\delta(\mat{x}-\mat{y})$. In addition, with zero boundary condition at infinity~($\lim_{\parallel \rvx \parallel_2 \to \infty}\varphi(\mat{x})=0$), $\varphi(\mat{x}) = \int G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y}$ solves the Poisson equation $\nabla^2\varphi(\mat{x}) = -\rho(\mat{x})$.
\end{lemma}

\begin{proof}
It is convenient to denote $\mat{r}=\mat{x}-\mat{y}$, $r=||\mat{r}||$ and notice $\partial r/\partial \mat{x}=\mat{r}/r$. Firstly, we calculate $\nabla_{\mat{x}} G(\mat{x},\mat{y})$:
\begin{equation}
\begin{aligned}
    \nabla_{\mat{x}} G(\mat{x}, \mat{y}) & = \frac{1}{(N-2)S_{N-1}(1)}\nabla_{\mat{x}}(\frac{1}{r^{N-2}}) \\
    & = \frac{1}{(N-2)S_{N-1}(1)} \frac{\partial}{\partial r}(\frac{1}{r^{N-2}})\nabla_\mat{x}r \\
    & = -\frac{1}{S_{N-1}(1)}\frac{\mat{r}}{r^N}
\end{aligned}
\end{equation}
Then we calculate $\nabla_{\mat{x}}^2 G(\mat{x},\mat{y})$:
\begin{equation}
    \begin{aligned}
        \nabla_{\mat{x}}^2 G(\mat{x},\mat{y}) &\equiv \nabla_{\mat{x}}\cdot\nabla_{\mat{x}}G(\mat{x},\mat{y}) \\
        & = -\frac{1}{S_{N-1}(1)}\nabla_{\mat{x}}\cdot \frac{\mat{r}}{r^N} \\ 
        & = -\frac{1}{S_{N-1}(1)}(\nabla_{\mat{x}}(\frac{1}{r^N})\cdot\mat{r}+\frac{1}{r^N}\nabla_{\mat{r}}\cdot\mat{r}) \\
        & = -\frac{1}{S_{N-1}(1)}(-\frac{N}{r^N}+\frac{N}{r^N}) \\
        & = -\frac{0}{S_{N-1}(1)r^N}
    \end{aligned}
\end{equation}
which is 0 for $r>0$, but undermined for $r=0$. So we are left with proving
\begin{equation}
    \int_{S_\epsilon(\mat{y})}\nabla_{\mat{x}}^2 G(\mat{x},\mat{y}) d^N\mat{x} = -1,
\end{equation}
where $S_\epsilon(\mat{y})$ denotes a ball centered at $\mat{y}$ with a radius $\epsilon\to 0^+$. With the divergence theorem, we have
\begin{equation}
    \int_{S_\epsilon(\mat{y})}\nabla_{\mat{x}}^2 G(\mat{x},\mat{y}) d^N\mat{x} =  \oiint_{\partial S_\epsilon(\mat{y})} \nabla_{\mat{x}}G(\mat{x},\mat{y})\cdot d^{N-1}\mat{B}
\end{equation}
where the surface integral can be computed
\begin{equation}
    \oiint_{\partial S_\epsilon(\mat{y})} \nabla_{\mat{x}}G(\mat{x},\mat{y})\cdot d^{N-1}\mat{B}  = \oiint_{\partial S_\epsilon(\mat{y})} (-\frac{1}{S_{N-1}(1)}\frac{\mat{r}}{r^N})\cdot d^{N-1}\mat{B} = -\frac{1}{S_{N-1}(1)}\frac{S_{N-1}(\epsilon)}{\epsilon^{N-1}} = -1
\end{equation}
in which we used $\oiint_{\partial S_\epsilon(\mat{y})}\mat{r}\cdot d^{N-1}\mat{B}=\epsilon S_{N-1}(\epsilon)$. Together, we conclude that
\begin{align*}
    \nabla^2_{\mat{x}} G(\mat{x},\mat{y})=-\delta(\mat{x}-\mat{y}) \numberthis \label{eq:G-delta}
\end{align*}




Next we show that $\varphi(\mat{x}) = \int G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y}$ solves $\nabla^2 \varphi(\mat{x}) = -\rho(\rvx)$. Taking the Laplacian operator of both sides gives:
\begin{align*}
    \nabla^2_{\mat{x}} \varphi(\mat{x}) &= \nabla^2_{\mat{x}} \int G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y}\\
    &=\int \nabla^2_{\mat{x}} G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y}\\
    &= \int  -\delta(\mat{x}-\mat{y})\rho(\mat{y})d\mat{y}\quad \textrm{(By \Eqref{eq:G-delta})}\\
    &= -\rho(\mat{x})
\end{align*}

In addition, we show that $\varphi(\rvx)$ is zero at infinity. Since $\rho(\rvx) \in \gC^0$ and has compact support, we know that $\rho(\rvx)$ is bounded, and let $|\rho(\rvx)|<B$.
\begin{align*}
    \lim_{\parallel \rvx \parallel_2 \to \infty}\varphi(\mat{x}) &= \lim_{\parallel \rvx \parallel_2 \to \infty}\int G(\mat{x},\mat{y})\rho(\mat{y})d\mat{y}\\
    &\le B\lim_{\parallel \rvx \parallel_2 \to \infty}\int_{\textrm{supp}(\rho)}\frac{1}{(N-2)S_{N-1}(1)}\frac{1}{||\mat{x}-\mat{y}||^{N-2}}d\mat{y}\\
    &=0
\end{align*}
The last equality holds since $\textrm{supp}(\rho)$ is a compact set.
\end{proof}

\subsection{Proof for the Prior Distribution on $z=\zmax$ Hyperplane}\label{app:prior_distribution}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{./img/init_dist_proof.pdf}
    \caption{Diagram of the deviation in Proposition~\ref{prop:prior}}
    \label{fig:init_dist}
\end{figure}

We obtain the prior distribution $p_{\textrm{prior}}$ by projecting the uniform distribution $\gU(S_N^+(\zmax))$ on the hemisphere $S_N^+(\zmax)$ to the $z=\zmax$ hyperplane. In the following proposition, we show that the projected distribution is $p_{\textrm{prior}}(\mat{x})=\frac{2\zmax}{S_N(1)r^{N+1}}$. 
\begin{proposition}
\label{prop:prior}
The radial projection of $\gU(S_N^+(\zmax))$ on the hemisphere $S_N^+(\zmax)$ to the $z=\zmax$ hyperplane is $p_{\textrm{prior}}(\mat{x})=\frac{2\zmax}{S_N(1)r^{N+1}}$.
\end{proposition}
\begin{proof}
We calculate the change-of-variable ratio by comparing two associate areas. As illustrated in \Figref{fig:init_dist}, an area $dA_1$ on $S_N^+(\zmax)$ is projected to an area $dA_3$ on the hyperplane in the $(\rvx,\zmax)$ direction, and we have $$\gU(S_N^+(\zmax))dA_1=p_{\textrm{prior}}(\mat{x})dA_3$$ We aim to calculate the ratio $dA_1/dA_3$ below. We define the angle between $(\mat{0}, \zmax)$ and $\tilde{\mat{x}}=(\mat{x},\zmax)$ to be $\theta$. We project $dA_3$ to the hyperplane orthogonal to $\tilde{\mat{x}}$ to get $dA_2=dA_3{\rm cos}\theta=dA_3\zmax/r$ where $r\equiv ||\mat{\tilde{x}}||_2=\sqrt{||\mat{x}||_2^2+\zmax^2}$. Since $dA_1$ is parallel to $dA_2$ and they lie in the same cone from the origin $O$, we have $dA_2/dA_1=(r/\zmax)^N$. Combining all the results gives
\begin{align*}
    p_{\textrm{prior}}(\mat{x})=\gU(S_N^+(\zmax))\frac{dA_1}{dA_3} = \gU(S_N^+(\zmax))\frac{dA_1}{dA_2}\frac{dA_2}{dA_3}=\frac{2}{S_N(1)\zmax^N}(\frac{\zmax}{r})^N\frac{\zmax}{r}=\frac{2\zmax}{S_N(1)r^{N+1}}.
\end{align*}
\end{proof}

In order to sample from $p_{\textrm{prior}}(\rvx)$, we first sample the norm~(radius) $R=||\rvx||_2$ from the distribution:
\begin{align*}
        p_{\textrm{radius}}(R)&\propto R^{N-1}p_{\textrm{prior}}(\mat{x})\qquad \textrm{($p_{\textrm{prior}}$ is isotropic)}\\
        &\propto R^{N-1}/{(||\rvx||_2^2+\zmax^2)^{\frac{N+1}{2}}}\\
        &= R^{N-1}/{(R^2+\zmax^2)^{\frac{N+1}{2}}} \numberthis \label{eq:propto}
\end{align*}
and then uniformly sample its angle. Sampling from $p_{\textrm{prior}}$ encompasses three steps. We first sample a real number $r_1$ with parameters $\alpha=\frac{N}{2}, \beta=\frac12$, \ie
\begin{align*}
    R_1 \sim \textrm{Beta}(\alpha, \beta)
\end{align*}
Next, we set $R_2=\frac{R_1}{1-R_1}$ such that $R_2$ is effectively sampled from the inverse beta distribution~a(also known as beta prime distribution) with parameters $\alpha=\frac{N}{2}, \beta=\frac12$. Finally, we set $R_3 = \sqrt{\zmax^2 R_2}$. To verify the pdf of $R_3$ is $p_{\textrm{radius}}$, note that the pdf of inverse beta distribution is
\begin{align*}
    p(R_2) \propto R_2^{\frac{N}{2}-1}(1+R_2)^{-\frac{N}{2}-\frac12}
\end{align*}
Next, by change-of-variable, the pdf of $R_3=\sqrt{\zmax^2 R_2}$ is
\begin{align*}
    p(R_3) &\propto R_2^{\frac{N}{2}-1}(1+R_2)^{-\frac{N}{2}-\frac12}*\frac{2R_3}{\zmax^2}\\
    &\propto\frac{R_3R_2^{\frac{N}{2}-1}}{(1+R_2)^{\frac{N+1}{2}}}\\
    &= \frac{(R_3/\zmax)^{N-1}}{(1+(R_3^2/\zmax^2))^{\frac{N+1}{2}}}\\
    &\propto  \frac{R_3^{N-1}}{(1+(R_3^2/\zmax^2))^{\frac{N+1}{2}}}\\
    &\propto  \frac{R_3^{N-1}}{(\zmax^2+R_3^2)^{\frac{N+1}{2}}} \propto p_{\textrm{radius}}(R_3)\qquad \textrm{(By \Eqref{eq:propto})}
\end{align*}

Hence we conclude that $p(R_3) = p_{\textrm{radius}}(R_3)$.

\section{Experimental Details}

\subsection{Training}
\label{app:training}

In this section we include more details about the training of PFGM and other baselines. We show the hyper-parameters settings for all the baselines~(Appendix~\ref{app:hyper-train}). All the experiments are run on a single NVIDIA A100 GPU.


\subsubsection{Additional Settings}
\label{app:hyper-train}

\paragraph{PFGM} We set the hyper-parameters $\gamma=5$, the larger batch size for calculating normalize field $|\gB_L|=2048~\textrm{(CIFAR-10)}, 256~\textrm{(CelebA)}, 64~\textrm{(LSUN bedroom)}$ in Algorithm~\ref{alg:pf}, and $M=291~\textrm{(CIFAR-10, CelebA)}/356~\textrm{ (LSUN bedroom)}$, $\sigma=0.01$ and $\tau=0.03$ in Algorithm~\ref{alg:ode}. We use the a batch size of $|\gB|=128~\textrm{(CIFAR-10, CelebA)}/32~\textrm{ (LSUN bedroom)}$, the same Adam optimizer and exponential moving average in \cite{Song2021ScoreBasedGM}. We center the data around the origin. The initial $z$ components in the normalized field are approximately zero with small initial $|\epsilon_z|$ values in Algorithm~\ref{alg:ode}. In this case, the trajectories of the forward ODE terminate at points that are unlikely traversed by the backward ODE, \ie points with large $\parallel \rvx\parallel_2$ and small $z$. In light of this, we heuristically confine the maximum sampling step to $M=200~\textrm{ (CIFAR-10, CelebA)}/250~\textrm{ (LSUN bedroom)}$ for points with the initial $|\epsilon_z|$ smaller than $0.005$. More principal solutions are left for future works.

For \textit{selecting $M$ in more general settings}, we recommend the following rule-of-thumb. According to analysis in \Secref{sec:mul}, given a perturbation point $(\rvy,z)$ when setting the exponent $m=M$ in Algorithm~\ref{alg:ode}, we can ensure the point source approximation by
\begin{align*}
    {||\mat{y}||^2}\gg {\sqrt{N}{\mathbb{E}}_{p(\rvx)}||\mat{x}||^2}/2 \numberthis \label{eq:rule-M}
\end{align*}
where $N$ is the data dimension and $p(\rvx)$ is the data distribution. By WLLN, we have $||\epsilon_\rvx|| = \sqrt{N}\sigma$, and recall that $
    \rvy = \rvx+\parallel \epsilon_\rvx \parallel(1+\tau)^M\rvu
$
where $\epsilon = (\epsilon_\rvx, \epsilon_z) \sim \gN(0, \sigma^2I_{N+1\times N+1})$, $\rvu\sim \gU(S_N(1))$. Together, we conclude $||\rvy|| \approx \sqrt{N}\sigma(1+\tau)^M$. Substituting in \Eqref{eq:rule-M}, we have 
\begin{align*}
    M > \frac12 \log_{1+\tau}{\frac{\mathbb{E}_{p(\rvx)}||\rvx||^2}{2\sqrt{N}\sigma^2}} = \frac12 \frac{\ln{\frac{\mathbb{E}_{p(\rvx)}||\rvx||^2}{2\sqrt{N}\sigma^2}}}{\ln{1+\tau}}
\end{align*}
We empirically observe that setting $M=  \frac34 \frac{\ln{\frac{\mathbb{E}_{p(\rvx)}||\rvx||^2}{2\sqrt{N}\sigma^2}}}{\ln{1+\tau}}$ already gives good results, and the corresponding $||\rvy||\approx 3000$. For example, on CIFAR-10 datasets, $N=3072, \tau=0.03, \sigma=0.01, \mathbb{E}_{p(\rvx)}||\rvx||^2\approx 900$, we have $M=\frac34 \frac{\ln{\frac{\mathbb{E}_{p(\rvx)}||\rvx||^2}{2\sqrt{N}\sigma^2}}}{\ln{1+\tau}} \approx 291$. 

Since we are operating in the augmented space, we add minor modifications to the DDPM++/DDPM++ deep architectures to accommodate the extra dimension. More specifically, we replace the conditioning time variable in VP/sub-VP with the additional dimension $z$ in PFGM as the input to the positional embedding. We also need to add an extra scalar output representing the $z$ direction. To this end, we add an additional output channel to the final convolution layer and take the global average pooling of this channel to obtain the scalar. {For LSUN bedroom dataset, we both experiments with the channel configurations suggested in NSCN++~\cite{Song2021ScoreBasedGM} and DDPM~\cite{Ho2020DenoisingDP}.}

\paragraph{VE/VP/sub-VP} We use the same set of hyper-parameters and the NCSN++/DDPM++ (deep) backbone and the continuous-time training objectives for forward SDEs in \cite{Song2021ScoreBasedGM}.

\subsection{Sampling}
\label{app:sampling}

We provide more details of PFGM and VE/VP sampling implementations in Appendix~\ref{app:sample-add}. We further discuss two techniques used in PFGM ODE sampler: change-of-variable formula~(Appendix~\ref{app:exp}) and the substitution of ground-truth Poisson field direction on $z$~(Appendix~\ref{app:sub}).


\subsubsection{Additional settings}
\label{app:sample-add}
\paragraph{PFGM} For RK-45 sampler, we use the function implemented in \texttt{scipy.integrate.solve\_ivp} with \texttt{atol}=$1e-4$, \texttt{rtol}=$1e-4$. For forward Euler method, we discretize the ODE with constant step size determined by the number of steps, \ie step size = $(\log \zmax - \log \zmin)$/number of steps for the backward ODE~(\Eqref{eq:backode}). {As in [1], we set the terminal value of $z_{min}=1e-3$. We choose $\zmax = 40 \textrm{ (CIFAR-10)},60 \textrm{ (CelebA $64^2$)}, 100\textrm{ (LSUN bedroom)}$ to satisfy the condition $\kappa \gg 1$ by the multipole expansion analysis in Appendix~\ref{sec:mul}. The condition ensures that the data distribution can be viewed roughly as a point source at origin. For example, we set $z_{max}=40$ on CIFAR-10, and the corresponding $\kappa$ is greater than $50$ with high probability. The hyperparameters work well without further fine tuning. Hence, we hypothesize that PFGM is insensitive to the choice of hyperparameters in a reasonable range, as shown in Table~\ref{table:fid-zmax}.} We clip the norms of initial samples into $(0, 3000)$ for CIFAR-10, $(0,6000)$ for CelebA and $(0, 30000)$ for LSUN bedroom.

For selecting $\zmax$ and clipping upper bound of norms for general datasets, we recommend the following rule-of-thumb. Recall that during the training perturbations~(\Eqref{eq:geo-ode}), given a random initial value $\epsilon_z \sim \gN(0, \sigma^2)$, maximum $z$ is
\begin{align*}
     z=|\epsilon_z| (1+\tau)^M
\end{align*}
Hence we set $\zmax = \mathbb{E}[|\epsilon_z| (1+\tau)^M]=\sqrt{\frac{2}{\pi}}\sigma(1+\tau)^M$. For example, on CIFAR-10, $\tau=0.03, M=291$, and $\zmax\approx 43$. The clipping upper value is similarity derived, by setting it to $\mathbb{E}[||\epsilon_\rvx|| (1+\tau)^M] = \sqrt{N}\sigma (1+\tau)^M\approx 3000$, where $\epsilon_\rvx\sim \gN(0, \sigma^2I_{N\times N})$. By combining \Eqref{eq:rule-M}, we further have
\begin{align*}
    &\zmax = \sqrt{\frac{2}{\pi}}\sigma(1+\tau)^M = \sqrt{\frac{2}{\sigma\pi}}\left(\frac{\mathbb{E}_{p(\rvx)}||\mat{x}||^2}{2\sqrt{N}}\right)^{\frac34}\\
    &\textrm{clipping upper value}=\sqrt{N}\sigma (1+\tau)^M= \sqrt{\frac{N}{\sigma}}\left(\frac{\mathbb{E}_{p(\rvx)}||\mat{x}||^2}{2\sqrt{N}}\right)^{\frac34}
\end{align*}
where $N$ is the data dimension and $p(\rvx)$ is the data distribution. These formulas are easier for practitioner to apply PFGM on new datasets.



\paragraph{VE/VP/sub-VP} For the PC sampler in VE, we follow \cite{Song2021ScoreBasedGM} to set the reverse diffusion process as the predictor and the Langevin dynamics (MCMC) as the corrector. For VP/sub-VP, we drop the corrector in PC sampler since it only gives slightly better results~\cite{Song2021ScoreBasedGM}.
\begin{table*}[htb]
\begin{center}
\caption{FID scores versus $z_{max}$ on PFGM w/ DDPM++}
\label{table:fid-zmax}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{$\bm{z_{max}}$} &  $30$ &$40$ & $50$\\
		\midrule
        \textbf{FID score} & {2.49} & {2.48} &{2.48}\\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}
\subsubsection{Exponential Decay on $z$ Dimension}
\label{app:exp}
Recall that in Section~\ref{sec:sampling}, we replace the vanilla backward ODE with a new ODE anchored by $z$:
\begin{align*}
    d(\rvx,z) = (\frac{d \rvx}{dt}\frac{d t}{dz}dz,dz) = (\rvv(\tilde{\rvx})_\rvx\rvv(\tilde{\rvx})_z^{-1}, 1) dz
\end{align*}
We further use the change-of-variable formula, \ie $t'=-\log z$, to achieve exponential decay on the $z$ dimension:
\begin{align*}
{d(\rvx,z)} &= (\rvv(\tilde{\rvx})_\rvx\rvv(\tilde{\rvx})_z^{-1}z, z){dt'} 
\end{align*}

The trajectories of the two ODEs above are the same when $dt, dt' \to 0$. We compare the NFE and the sample quality of different ODEs in Table~\ref{table:exp}. We measure the NFE/FID of generating 50000 CIFAR-10 samples with the RK45 method in Scipy package~\cite{Virtanen2020SciPy1F}. The batch size is set to $1000$. All the numbers are produced on a single NVIDIA A100 GPU. We observe that the ODE with the anchor variable $t'$ not only accelerates the vanilla by 2 times, but has almost no harm to the sample quality measured by FID score.
\begin{table*}[htb]
\begin{center}
\caption{NFE and FID scores of different backward ODEs in PFGM}
\label{table:exp}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{Algorithm} &  $d(\rvx,z)/dz$ &$d(\rvx,z)/dt'$\\
		\midrule
        \textbf{NFE} &  $242$ &$104$ \\
        \textbf{FID score} & 2.53 & 2.48 \\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsubsection{Substitute the Predicted $z$ Direction with the Ground-truth}
\label{app:sub}
Since the neural network cannot perfectly learn the ground-truth $z$ direction, we replace the predicted $f_\theta(x)_z$ with the ground-truth direction when $z$ is small. More specifically, given $\tilde{\rvx} = (\rvx, z) \in \mathbb{R}^{N+1}$, recall that the empirical field is $    \hat{\mat{E}}(\tilde{\mat{x}}) = c(\tilde{\mat{x}})\sum_{i=1}^n \frac{\tilde{\mat{x}}-\tilde{\mat{x}}_i}{||\tilde{\mat{x}}-\tilde{\mat{x}}_i||^{N+1}}$ where $c(\tilde{\mat{x}})=1/\sum_{i=1}^n \frac{1}{||\tilde{\mat{x}}-\tilde{\mat{x}}_i||^{N+1}} $. Hence we can rewrite the empirical field as 
\begin{align*}
    \hat{\mat{E}}(\tilde{\mat{x}}) =\sum_{i=1}^n w(\tilde{\mat{x}},\tilde{\mat{x}}_i) ({\tilde{\mat{x}}-\tilde{\mat{x}}_i})
\end{align*}
where $\sum_{i=1}^n w(\tilde{\mat{x}},\tilde{\mat{x}}_i) = \sum_{i=1}^n \frac{\frac{1}{||\tilde{\mat{x}}-\tilde{\mat{x}}_i||^{N+1}}}{\sum_{j=1}^n\frac{1}{||\tilde{\mat{x}}-\tilde{\mat{x}}_j||^{N+1}}}=1$. Furthermore we have $\forall i, ({\tilde{\mat{x}}-\tilde{\mat{x}}_i})_z = z - 0 = z$. Together, the $z$ component in the empirical field is $\hat{\mat{E}}(\tilde{\mat{x}})_z =\sum_{i=1}^n w(\tilde{\mat{x}},\tilde{\mat{x}}_i) ({\tilde{\mat{x}}-\tilde{\mat{x}}_i})_z = z$.
The predicted normalized field~(on $\rvx$) is trained to approximate the normalized field~(on $\rvx$), \ie 
\begin{align*}
f_\theta(\tilde{\rvx})_\rvx &\approx -\sqrt{N}\hat{\mat{E}}(\tilde{\rvx})_\rvx/(\sqrt{\parallel \hat{\mat{E}}(\tilde{\rvx})_\rvx \parallel_2^2+ z^2}+\gamma)\\&\approx -\sqrt{N}\hat{\mat{E}}(\tilde{\rvx})_\rvx/(\sqrt{\parallel \hat{\mat{E}}(\tilde{\rvx})_\rvx \parallel_2^2}+\gamma)    
\end{align*}
The last approximation is due to $\parallel \hat{\mat{E}}(\tilde{\rvx})_\rvx \parallel_2 \gg z$. Solving for $\parallel \hat{\mat{E}}(\tilde{\rvx})_\rvx\parallel_2$, we get $
    \parallel \hat{\mat{E}}(\tilde{\rvx})_\rvx\parallel_2 \approx \frac{\gamma \parallel f_\theta(\tilde{\rvx})_\rvx \parallel_2/\sqrt{N}}{1 - \parallel f_\theta(\tilde{\rvx})_\rvx \parallel_2/\sqrt{N}}
$. Hence the $z$ component in the normalized field after substituting the ground-truth is $\hat{\mat{E}}(\tilde{\rvx})_z/(\sqrt{\parallel \hat{\mat{E}}(\tilde{\rvx})_\rvx\parallel_2^2+z^2}+\gamma)=z/(\sqrt{(\frac{\gamma \parallel f_\theta(\tilde{\rvx})_\rvx \parallel_2/\sqrt{N}}{1 - \parallel f_\theta(\tilde{\rvx})_\rvx \parallel_2/\sqrt{N}})^2+z^2} + \gamma)$. In our experiments, we therefore replace the original prediction $f_\theta(\tilde{\rvx})_z$ with $-\sqrt{N}z/(\sqrt{(\frac{\gamma \parallel f_\theta(\tilde{\rvx})_\rvx \parallel_2/\sqrt{N}}{1 - \parallel f_\theta(\tilde{\rvx})_\rvx \parallel_2/\sqrt{N}})^2+z^2} + \gamma)$ when $z< 5/1/0.1$ during the backward ODE sampling for CIFAR-10/CelebA $64^2$/LSUN bedroom $256^2$. 

Table~\ref{table:subz} reports the NFE and FID score w/o and w/ the above substitution. We observe that the usage of ground-truth $z$ direction in the near field accelerates the sampling speed.

\begin{table*}[htb]
\begin{center}
\caption{NFE and FID scores of w/ and w/o substitution}
\label{table:subz}
\begin{tabular}{c c c c}
		\toprule
		\textbf{Algorithm} &  w/o substitution &w/ substitution\\
		\midrule
        \textbf{NFE} &  $134$ &$104$ \\
        \textbf{FID score} & $2.48$ & $2.48$ \\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsection{Evaluation}

We use FID~\cite{Heusel2017GANsTB} and Inception scores~\cite{Salimans2016ImprovedTF} to quantitatively measure the sample quality, and NFE~(number of evaluation steps) for the inference speed. {FID (Fréchet Inception Distance) score is the Fréchet distance between two multivariate Gaussians, whose means and covariances are estimated from the 2048-dimensional activations of the Inception-v3~\citep{Szegedy2016RethinkingTI} network for real and generated samples respectively.} Inception score is the exponential mutual information between the predicted labels of the Inception network and the images. We also report bits/dim for likelihood evaluation. It is computed by dividing the negative log-likelihood by the data dimension, \ie $\textrm{bits/dim} = -\log p_{\textrm{prior}}(\rvx)/N$.

For CIFAR-10, we compute the Fréchet distance between 50000 samples and the pre-computed statistics of CIFAR-10 dataset in \cite{Heusel2017GANsTB}. For CelebA $64 \times 64$, we follow the setting in \cite{Song2020ImprovedTF} where the distance is computed between 10000 samples and the test set. For model selection, we follow \cite{Song2020ImprovedTF} and pick the checkpoint with smallest FID every 50k iterations on 10k samples for computing all the scores.

\subsection{Effects of Step Size: FID versus NFE}

For preciseness, Table~\ref{table:fig5c} reports the exact numbers in \Figref{fig:adapt}. 
\begin{table*}[htb]
\begin{center}
\caption{The FID scores in \Figref{fig:adapt} of different methods and NFE.}
\label{table:fig5c}
\begin{tabular}{c c c c c}
		\toprule
		\textbf{Method / NFE} &  10 & 20 & 50 & 100\\
		\midrule
			\textbf{VP-ODE} & $192.36$& $72.25$ &$38.18$& $19.73$\\
			\textbf{DDIM} &$13.36$& $6.48$& $4.67$& $4.16$\\
        	\textbf{PFGM} &  $14.98$ & $6.46$& $3.48$ & $2.89$ \\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

Since in the ODE $ {d(\rvx,z)} = -(\rvv(\tilde{\rvx})_\rvx\rvv(\tilde{\rvx})_z^{-1}z, z){dt'}$ of PFGM, the $z$ variable is a function of $t'$~($z=e^{t'}$), we integrate the $z$ in the Euler method to reduce the discretization error. The vanilla update from time $t'_i$ to time $t'_{i+1}$ is ${(\rvx_{i+1},z_{i+1})} = (\rvx_{i},z_{i})-(\rvv(\tilde{\rvx}_i)_\rvx\rvv(\tilde{\rvx}_i)_{z_i}^{-1}z_i, z_i)(t'_{i+1}-t'_i)$, and the new update is ${(\rvx_{i+1},z_{i+1})} = (\rvx_{i},z_{i})-(\rvv(\tilde{\rvx}_i)_\rvx\rvv(\tilde{\rvx}_i)_{z_i}^{-1}\int_{t'_i}^{t'_{i+1}}z(t')dt', \int_{t'_i}^{t'_{i+1}} z(t')dt')$. We empirically observe that the new update scheme significantly improve the FID score.

\section{Failure of VE/VP-ODE on NCSNv2 backbone}
\label{app:failure}

In \Figref{fig:ve}, we demonstrate the trajectories of cleaner samples/noisier samples/noisier samples w/ corrector. We visualize these three groups in \Figref{fig:failure-a} and \Figref{fig:failure-b}. The noisier samples are marked with red boxes in \Figref{fig:failure-a} and the remaining images in \Figref{fig:failure-a} are cleaner samples. The samples within green boxes in \Figref{fig:failure-b} are noisier samples w/ corrector. Samples on the same spatial locations in the two figures are generated by identical initial latents. 

The Gaussian kernels in score-based models are $\gN(\rvx, \sigma(t)^2)$~(VE) and $\gN(\sqrt{1-\sigma(t)^2}\rvx,\sigma(t)^2)$~(VP) \cite{Song2021ScoreBasedGM}. When $\sigma(t)$ is large, the norms of perturbed samples are approximately $\sqrt{N}\sigma(t)$. The backward ODE could break down if the trajectories diverge from the norm-$\sigma(t)$ relation, as shown by the noisier samples' trajectories in \Figref{fig:ve}. In contrast, the norm distributions of PFGM is approximately $p(\parallel\rvx\parallel)\propto {\parallel \rvx \parallel_2^{N-2}}/{(\parallel \rvx \parallel_2^2+z^2)^{\frac{N}{2}}}$ when $z$ is large~(see deviation for $p_{\textrm{prior}}$ in Appendix~\ref{app:prior_distribution}), which have a wider span for high density region~(see \Figref{fig:compare_prior}). The weak correlation between norm and $z$ makes PFGM more robust on the lighter NCSNv2 backbone.


\begin{figure*}
    \centering
    \subfigure[Samples from VE-ODE (Euler)]{    \label{fig:failure-a}\includegraphics[width=0.4\textwidth]{img/ve_eu_images.png}}\hspace{10pt}
    \subfigure[Samples from VE-ODE (Euler w/ corrector)]{\label{fig:failure-b}\includegraphics[width=0.4\textwidth]{img/ve_eu_correct.png}}
    \caption{\textbf{(a)} Samples from VE-ODE (Euler w/o corrector). We highlight the noisier images with red boxes. The rest are cleaner images. \textbf{(b)} Samples from VE-ODE (Euler w/ corrector). We mark the noisier samples after correction with green boxes.}
\end{figure*}

\section{Extra Experiments}
\label{app:extra-exp}
\subsection{LSUN Bedroom $256\times 256$}
\label{app:exp-lsun}
We report the FID scores and NFEs for LSUN bedroom dataset in Table~\ref{tab:lsun}. We adopt the code base of \cite{Song2021ScoreBasedGM} in our experiments. In \cite{Song2021ScoreBasedGM}, they experimented on the LSUN bedroom 256$\times$ 256 dataset only on VE-SDE using a deeper NCSN++ backbone. In our DDPM++ architecture, we directly borrow the configuration of channels from the NCSN++ architecture~\cite{Song2021ScoreBasedGM} in each residual block (PFGM w/ NCSN++ channel). We further change $z_{max}$ to $100$, as it empirically gives better sample quality.

We also evaluate the performance when using the configuration of channels in the DDPM~\cite{Ho2020DenoisingDP} architecture (PFGM w/ DDPM channel). We use the RK45~\citep{Dormand1980AFO} solver in the Scipy library~\citep{Virtanen2020SciPy1F} for PFGM sampling. We report the FID score using the evaluation protocol in \cite{dhariwal2021diffusion}.
\begin{table}[htbp]
\begin{center}
\caption{FID/NFE on LSUN bedroom $256\times 256$}\label{tab:lsun}
\begin{tabular}{l c c}
\toprule
      &FID $\downarrow$ & NFE $\downarrow$\\
    \midrule
    StyleGAN~\cite{karras2019style} & $\bm{2.65}$ & $\bm{1}$ \\
    DDPM~\cite{Ho2020DenoisingDP} &$6.86$ & $1000$\\
    VE-SDE~\cite{Song2021ScoreBasedGM} & $11.75$  &  $2000$\\
    \midrule
    PFGM w/ NCSN++ channel &  ${17.01}$  &  $134$\\
    PFGM w/ DDPM channel&   ${13.66}$ & ${122}$ \\
     \bottomrule
\end{tabular}
\end{center}
\end{table}

Table~\ref{tab:lsun} shows that PFGM has comparable performance with VE-SDE when using DDPM channel, while achieving around 15$\times$ acceleration. We observe that PFGM achieves a better FID score using the similar configuration in the DDPM model, and converges faster — 150k over the total 2.4M training iterations suggested in \cite{Song2021ScoreBasedGM}. Remarkably, the VE-ODE baseline — the method most comparable to ours — only produces noisy samples on this dataset. It suggests that PFGM is able to scale up to high resolution images when using advanced architectures. 
We also compare with the number reported in \cite{Ho2020DenoisingDP} using similar architecture. Note that DDPM requires 1000 NFE during sampling, and doesn’t possess invertibility compared to flow models. 



\subsection{Results on NCSNv2 Architecture}

In this section, we demonstrate the image generation on CIFAR-10 and CelebA $64 \times 64$, using NCSNv2 architecture~\cite{Song2020ImprovedTF}, which is the predecessor of NCSN++ and DDPM++~\cite{Song2021ScoreBasedGM} and has smaller capacity. Since the VE/VP-ODE has poor performance~(FID greater than 90), with the RK45 solver, we also apply the forward Euler method~(\textbf{Euler}) with fixed number of steps. We explicitly name the sampler, with forward Euler method as predictor and Langevin dynamics as corrector, as \textbf{Euler w/ corrector}. For Euler w/ corrector in VE/VP-ODE, we use the probability flow ODE (reverse-time ODE) as the predictor and the Langevin dynamics (MCMC) as the corrector. We borrow all the hyper-parameters from \cite{Song2021ScoreBasedGM} except for the signal-to-noise ratio. We empirically observe the new configurations in Table~\ref{table:s2n} give better results on the NCSNv2 architecture.

To accommodate the extra dimension $z$ on NCSNv2, we concatenate the image with an additional constant channel with value $z$ and thus the first convolution layer takes in four input channels. We also add an additional output channel to the final convolution layer and take the global average pooling of this channel to obtain the direction on $z$.

\begin{table}[htbp]
\begin{center}
\caption{Signal-to-noise ratio of different dataset-method pairs}
\label{table:s2n}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{Dataset-Method} &  CIFAR-10 - VE &CIFAR-10 - VP & CelebA - VE & CelebA - VP\\
		\midrule
        \textbf{signal-to-noise ratio} &  $0.16$ &$0.27$  &$0.12$ & $0.27$ \\
        \bottomrule
\end{tabular}
\end{center}
\end{table}

\label{app:ncsnv2}
\subsubsection{CIFAR-10}

Table~\ref{tab:cifar-ncsnv2} reports the image quality measured by Inception/FID scores and the inference speed measured by NFE on CIFAR-10, using a weaker architecture NCSNv2~\cite{Song2020ImprovedTF}. We show that PFGM with the RK45 solver has competitive FID/Inception scores with the Langevin dynamics, which was the best model on the NCSNv2 architecture before, and requires $10\times$ less NFE. In addition, PFGM performs better than all the other ODE samplers. Our method is more tolerant of sampling error. Among the compared ODEs, our backward ODE~(\Eqref{eq:backode}) is the only one that successfully generates high quality samples while the VE/VP-ODE fail w/o the Langevin dynamics corrector. The backward ODE still beats the baselines w/ corrector. 

\begin{table}[htbp]
\small
    \centering
    \caption{CIFAR-10 sample quality~(FID, Inception) and number of function evaluation~(NFE). All the methods below the \textit{NCSNv2 backbone} separator use the NCSNv2~\citep{Song2020ImprovedTF} network architecture as the backbone.}
    \begin{tabular}{l c c c}
    \toprule
         & Inception $\uparrow$  &FID $\downarrow$ & NFE $\downarrow$\\
         \midrule
         PixelCNN~\citep{Oord2016ConditionalIG} & $4.60$ & $65.93$ & $1024$\\
        IGEBM~\citep{Du2019ImplicitGA} & $6.02$ & $40.58$ & $60$\\
        WGAN-GP~\citep{Gulrajani2017ImprovedTO} & $7.86 \pm .07$ & $36.4$& $1$\\
        SNGAN~\citep{Miyato2018SpectralNF} & $8.22\pm .05$ & $21.7$ & $1$\\
        NCSN~\citep{Song2019GenerativeMB} & $\bm{8.87 \pm .12}$ & $25.32$ & $1001$\\
        \midrule
        \textit{\textbf{NCSNv2 backbone}}\\
        \midrule
        Langevin dynamics~\citep{Song2020ImprovedTF} & $8.40 \pm .07$ & $\bm{10.87}$ & $1161$\\
        VE-SDE~\citep{Song2021ScoreBasedGM} & $8.23 \pm .02$& $10.94$&$1000$\\
        VP-SDE~\citep{Song2021ScoreBasedGM} & $6.85 \pm .01$& $44.05$&$1000$\\
        \midrule
VE-ODE~(Euler w/ corrector) & $8.05 \pm .03$ &  $11.33$ &  $1000$\\
        VP-ODE~(Euler w/ corrector) & $7.33\pm .07$ &  $37.74$ & $1000$ \\
PFGM~(Euler)& $8.00\pm .09$  &  $11.78$ & $200$ \\
PFGM~(RK45)& $8.30 \pm .05 $ &  $11.22$ & $\bm{118}$ \\
         \bottomrule
    \end{tabular}
    \label{tab:cifar-ncsnv2}
\end{table}


\subsubsection{CelebA}

In Table~\ref{tab:celeba}, we report the quality of images generated by models trained on CelebA $64 \times 64$, as measured by the FID scores, and the sampling speed, as measured
by NFE. We use this dataset as our preliminary experiments hence we only apply NCSNv2~\cite{Song2020ImprovedTF} for different baselines. As shown in Table~\ref{tab:celeba}, PFGM achieves best FID scores than all the baselines on CelebA dataset, while accelerating the inference speed around $20\times$. Remarkably, PFGM outperforms the Langevin dynamics and reverse-time SDE samplers, which are usually considered better than their deterministic counterparts. 

\paragraph{Remark: On the FID scores on CelebA $64 \times 64$} {One interesting observation is that the samples of PFGM (RK45)~(\Figref{pfgm_ncsnv2_celeba}) contain more obvious artifacts than Langevin dynamics~(\Figref{fig:ncsnv2_celeba}), although PFGM has a lower FID score on the same architecture. We hypothesize that the diversity of samples has larger effects on the FID scores than the artifacts. As shown in \Figref{fig:ncsnv2_celeba} and \Figref{pfgm_ncsnv2_celeba}, samples generated by PFGM have more diverse background colors and hair colors than samples of Langevin dynamics. In addition, we evaluate the performance of PFGM on the DDPM++ architecture. We show that the FID score can be further reduced to $3.68$ using the more advanced DDPM++ architecture. By examining the generated samples of PFGM on DDPM++~(\Figref{fig:extend-celeba}), we observe that the samples are diverse and exhibit fewer artifacts than PFGM on NCSNv2. It suggests that by using a more powerful architecture like DDPM++, we can remove the artifacts while retaining the diversity in PFGM.}




\begin{figure}[htbp]
    \centering
    \subfigure[Langevin dynamics~\cite{Song2019GenerativeMB}]{\label{fig:ncsnv2_celeba}\includegraphics[width=0.49\textwidth]{img/ncsnv2_celeba.png}}\hfill
    \subfigure[PFGM~(RK45)]{\label{pfgm_ncsnv2_celeba}\includegraphics[width=0.49\textwidth]{img/pfgm_ncsnv2_celeba.png}}\hfill
    \caption{Uncurated samples from Langevin dynamics~\cite{Song2019GenerativeMB} and PFGM~(RK45), both using the NCSNv2 architecture.}
    \label{fig:celeba_examine}
\end{figure}


\begin{table}[htbp]
\begin{center}
\caption{FID/NFE on CelebA $64 \times 64$}\label{tab:celeba}
\begin{tabular}{l c c}
\toprule
      &FID $\downarrow$ & NFE $\downarrow$\\
     \midrule
NCSN~\citep{Song2019GenerativeMB} & $26.89$  & $1001$\\
     \midrule
    \textit{\textbf{NCSNv2 backbone}}\\
    \midrule
    Langevin dynamics~\citep{Song2020ImprovedTF} & $10
    .23$  & $2501$\\
    VE-SDE~\citep{Song2021ScoreBasedGM} & $8.15$ & $1000$\\
    VP-SDE~\citep{Song2021ScoreBasedGM} &$34.52$ & $1000$\\
    \midrule
VE-ODE~(Euler w/ corrector)  & $8.30$  &  $200$\\
    VP-ODE~(Euler w/ corrector)  &  $41.81$ &  $200$\\
    PFGM~(Euler) &  ${7.85}$  &  $\bm{100}$\\
    PFGM~(RK45)&   ${7.93}$ & ${110}$ \\
    \midrule
    \textit{\textbf{DDPM++ backbone}}\\
    \midrule
     PFGM~(RK45) & $\bm{3.68}$ & $110$\\
     \bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{{Wall-clock Sampling Time}}

The main bottleneck of sampling time in each ODE step is the function evaluation of the neural network. Hence, for different ODE equations using similar neural network architectures, their inference times per ODE step are approximately the same.

We implement PFGM on the NCSNv2~\cite{Song2020ImprovedTF}, DDPM++~\cite{Song2021ScoreBasedGM}, and DDPM++ deep~\cite{Song2021ScoreBasedGM} architectures, with sight modifications to account for the extra dimension $z$. In Table~\ref{table:wall-clock}, we report the sampling time per ODE step method with the DDPM++ backbone, as well as the total sampling time. We measure the sampling time of generating a batch of 1000 images on CIFAR-10. We compare PFGM, VP/sub-VP ODEs using the RK45 solver. As a reference, we also report the results of VP-SDE using the predictor-corrector sampler~\cite{Song2021ScoreBasedGM}. All the numbers are produced on a single NVIDIA A100 GPU.

\begin{table*}[htb]
\begin{center}
\caption{Wall-clock sampling time~(second)}
\label{table:wall-clock}
\begin{tabular}{c c c c c c c c}
		\toprule
		\textbf{Method} &  PFGM &VP-ODE& sub-VP-ODE& VP-SDE (PC)\\
		\midrule
        \textbf{NFE} & 110 & 134 &146 & 1000\\
		\midrule
        \textbf{Wall-clock time per step} & 0.526 & 0.522 &0.520 & 0.491\\
		\midrule
        \textbf{Total wall-clock time} & 57.81 & 69.97 &75.92&490.65\\
        \bottomrule
\end{tabular}
\end{center}
\end{table*}

As expected, ODEs using similar architectures and the same solver have nearly the same wall-clock time per ODE step. The table also shows that PFGM achieves the smallest total wall-clock sampling time. 

\subsection{Image Interpolations}
\label{app:interpolate}

The invertibility of the ODE in PFGM enables the interpolations between pairs of images. As shown in \Figref{fig:interpolation}, we adopt the spherical interpolations between the latent representations of the images in the first and last column.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/interpolation_ddpmpp.png}
     \caption{Interpolation on CelebA $64\times 64$ by PFGM}\label{fig:interpolation}
\end{figure}

\subsection{Temperature Scaling}
\label{app:temp}

To demonstrate more utilities of the meaningful latent space of PFGM, we include the experiments of temperature scaling on CelebA $64 \times 64$ dataset. We linearly increase the norm of latent codes from $1000$ to $6000$ to get the samples in \Figref{fig:temperature}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{img/rescale_ddpmpp.png}
    \caption{Temperature scaling on CelebA $64 \times 64$ by PFGM}
    \label{fig:temperature}
\end{figure}

\section{Extended Examples}
\label{app:samples}
We provide extended samples from PFGM on CIFAR-10~(\Figref{fig:extend-cifar}), CelebA $64 \times 64$~(\Figref{fig:extend-celeba}) and {LSUN bedroom $256 \times 256$~(\Figref{fig:extend-bedroom-ddpm}) datasets.}


\section{Physical Interpretation of the ODEs in PFGM}\label{app:honey}

{In Section~\ref{section:bg}, in order to move the particles along the electric lines, we set the time derivative of $x$ to the Poisson field $\mat{E}(x)$:}
\begin{equation}\label{eq:ode2}
    [q=1, {\rm forward\  ODE}]\quad \frac{d\mat{x}}{dt} = \mat{E}(\mat{x}), \quad [q=-1, {\rm backward\  ODE}]\quad \frac{d\mat{x}}{dt} = -\mat{E}(\mat{x})
\end{equation}
{{We give the interpretation of the ODEs from a physical perspective. Newton's law implies that the external force is proportional to the acceleration of the particle. In the overdamped limit, e.g., when the particle is moving in honey, the external force is instead proportional to the velocity of the particle, making the equation of motion a first-order ODE.} 
Denoting the viscosity of the fluid as $\gamma$, the dynamics of the particle under the influence of the electric field of the source $\rho(\mat{x})$ is}
$$
    m\frac{d^2\mat{x}}{dt^2}=-\gamma\frac{d\mat{x}}{dt} + q\mat{E}(\mat{x}),
$$
{which has an overdamped limit  $\frac{d\mat{x}}{dt}=q\mat{E}(\mat{x})$ when we set $t\to\gamma t$ and $\gamma\to\infty$. In this case, a particle with mass $m=1$ and charge $q=1$ would follow the electric field with velocity equal to $\mat{E}$, justifying Eq.~(\ref{eq:ode2}).}

\section{Limitations and Future Directions}
\label{app:limit}

In Section~\ref{sec:learning} we discuss the training paradigm of PFGM, including the normalized Poisson field and the discretized forward ODE. There are several potential improvements. First, the normalized field on mini-batch is biased. In this paper, we directly alleviate the bias by using a larger training batch. However, it does not solve the problem fundamentally. Some potential directions are incorporating more physical tools: we can exploit renormalization to make the Poisson field well-behaved in near fields. Another possibility is to replace a point charge with a quantum particle, whose position uncertainty fills the empty space among nearest neighbor data samples and makes the data manifold smoother. 


\section{Potential Social Impact}
\label{app:impact}

Generative models is a rapidly growing field of study with far-reaching implications for science and society. 
Our work proposes a new generative model that allows for high-quality samples, quick inference and adaptivity. Many downstream applications benefit from our PFGM models' powerful expressive capabilities, particularly those that need fast inference speed and good sample quality at the same time. The usage of these models might have both positive and negative outcomes depending on the downstream use case. 
For example, PFGM can be incorporated in producing good image/audio samples by the fast backward ODE. This, on the other hand, promotes \textit{deepfake} technology and leads to social scams. Generative models are also brittle and susceptible to backdoor adversarial attacks on publicly available training data, causing unanticipated failure.
Addressing the above concerns requires further research in providing robustness guarantees for generative models as well as close collaborations with researchers in socio-technical disciplines.




\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/cifar10_ddpmpp.png}
    \caption{CIFAR-10 samples from PFGM (RK45)}
    \label{fig:extend-cifar}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/celeba_ddpmpp.png}
    \caption{CelebA $64 \times 64$ samples from PFGM (RK45, NCSNv2 architecture)}
    \label{fig:extend-celeba}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/bedroom_ddpmpp.png}
    \caption{LSUN bedroom $256 \times 256$ samples from PFGM (RK45) using DDPM channel configuration.}
    \label{fig:extend-bedroom-ddpm}
\end{figure*}
\clearpage




