
\def\year{2022}\relax
\documentclass[letterpaper]{article}
\usepackage{aaai22} \usepackage{times} \usepackage{helvet} \usepackage{courier} \usepackage[hyphens]{url} \usepackage{graphicx} \usepackage{xcolor}
\urlstyle{rm} \def\UrlFont{\rm} \usepackage{graphicx} \usepackage{natbib} \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off}
\frenchspacing \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\usepackage{multirow}

\graphicspath{ {./images/} }

\pdfinfo{
/Title (Mining Adverse Drug Reactions (ADR) from Unregulated Mediums at Scale)
/Author (Hasham Ul Haq, Veysel Kocaman, David Talby)
/TemplateVersion (2022.1)
}


\title{Mining Adverse Drug Reactions from \\ Unstructured Mediums at Scale}
\author{
Hasham Ul Haq,
    Veysel Kocaman,
    David Talby
}
\affiliations{
John Snow Labs Inc.\\




    16192 Coastal Highway\\
    Lewes, DE , USA 19958\\
    \{hasham, veysel, david\} @ johnsnowlabs.com\\
}

\newcommand{\vk}[1]{{\color{blue}\textbf{VK}: #1}}

\begin{document}
\maketitle

\section{Abstract}
Adverse drug reactions / events (ADR/ADE) have a major impact on patient health and health care costs. Detecting ADR's as early as possible and sharing them with regulators, pharma companies, and healthcare providers can prevent morbidity and save many lives. While most ADR's are not reported via formal channels, they are often documented in a variety of unstructured conversations such as social media posts by patients, customer support call transcripts, or CRM notes of meetings between healthcare providers and pharma sales reps. In this paper, we propose a natural language processing (NLP) solution that detects ADR's in such unstructured free-text conversations, which improves on previous work in three ways. First, a new Named Entity Recognition (NER) model obtains new state-of-the-art accuracy for ADR and Drug entity extraction on the ADE, CADEC, and SMM4H benchmark datasets (\textbf{91.75\%}, \textbf{78.76\%}, and \textbf{83.41\%} F1 scores respectively). Second, two new Relation Extraction (RE) models are introduced - one based on BioBERT while the other utilizing crafted features over a Fully Connected Neural Network (FCNN) - are shown to perform on par with existing state-of-the-art models, and outperform them when trained with a supplementary clinician-annotated RE dataset. Third, a new text classification model, for deciding if a conversation includes an ADR, obtains new state-of-the-art accuracy on the CADEC dataset (\textbf{86.69\%} F1 score). The complete solution is implemented as a unified NLP pipeline in a production-grade library built on top of Apache Spark, making it natively scalable and able to process millions of batch or streaming records on commodity clusters.


\section{Introduction}
Adverse drug events are harmful side effects of drugs, comprising of allergic reactions, overdose response, and general unpleasant side effects. Approximately 2 million patients in the United States are affected each year by serious ADR's, resulting in roughly 100,000 fatalities \cite{leaman2010towards}, and making ADR's the fourth leading cause of death in the United States \cite{giacomini2007good}. Treatment related to ADR's has been estimated to cost \$ From the ADE dataset, we can sample negative relations by subtracting annotated drug-reaction pairs from all drug-reaction pairs in the same document. Table \ref{tab:re_data_stats} shows data distribution of the standard and enriched RE datasets. 

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\toprule
 \textbf{Dataset} & \textbf{\# Positive Rel.} & \textbf{\# Negative Rel.}\\
\midrule
 ADE Corpus & 6821 & 183 \\ 
 ADE with n2c2 & 12929 & 8935\\
\bottomrule
\end{tabular}
\caption{Statistics of the RE datasets used to train and validate the models. ADE Corpus is the standard dataset, which is then enriched with n2c2 data for more robust performance.}
\label{tab:re_data_stats}
\end{table}

The standard ADE Corpus does not have sufficient negative relations, raising the issue of class imbalance. To address this, we sampled and annotated ~2000 notes from 2018 n2c2 shared task on ADE and medication extraction in EHRs dataset \cite{henry20202018}, to create a supplementary dataset for relations. We keep the same entities (i.e., Drug and ADE) while annotating to align with our benchmark datasets. Also, to keep human bias at a minimum, we don't annotate entity spans; rather we use existing NER annotations to generate a dataset comprising of Drug and ADE pairs, and only classify each relation based on their context.

Following previous work, we evaluate the models using 10-fold cross validation, and report macro and micro averaged precision, recall, and F1 scores. Exact experimental and evaluation settings for each stage are described below.

\subsection{Experiments}

\begin{itemize}
    \item Keeping the Classification model architecture constant, we test two methods of embedding generation; For the first experiment we generated token level GLoVe and BioBERT embeddings for each token and averaged them to generate document embedding. This method did not produce accurate embeddings, resulting in lower F1 scores. Our second experiment utilised Sentence Bert Embeddings \cite{DBLP:journals/corr/abs-1908-10084} trained on the Pubmed dataset, namely Sentence BioBERT, which we further pretrained on the MedNLI dataset \cite{shivade2019mednli} for better performance. We thoroughly test the performance of each embedding generation approach, and report the findings in Table \ref{tab:cls_metrics}.
    
\begin{table*}[ht]
\centering
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{GLoVe (Avg.) Embeddings}} & \multicolumn{3}{c}{\textbf{BERT (Avg.) Embeddings}} & \multicolumn{3}{c}{\textbf{BERT Sentence Embeddings}} & \textbf{SOTA}\\
\multirow{2}{*}{} & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & F1 \\
\midrule
\multirow{2}{*}{ADE} & 75.96 & 79.53 & 76.86 & 76.91 & 84.96 & 79.37 & 87.41 & 84.72 & \textbf{85.96} & \textbf{87.0} \\
 & 86.84 & 81.22 & 83.43 & 88.13 & 84.38 & 85.38 & 90.97 & 91.20 & 91.03 & \\
 & & & & & & & & & \\
\multirow{2}{*}{CADEC} & 85.29 & 84.24 & 84.71 & 86.50 & 86.11 & 86.30 & 87.13 & 86.32 & \textbf{86.69} & \textbf{81.5}\\
 & 85.99 & 86.10 & 86.0 & 87.38 & 87.43  & 87.40 & 87.78 & 87.86 & 87.79 &  \\
\bottomrule
\end{tabular}
\caption{Classification Metrics on benchmark datasets. For each dataset, Macro and Micro averaged scores are displayed on first and second row respectively. SOTA metrics for ADE and CADEC datasets are obtained from \cite{huynh-etal-2016-adverse} and \cite{sota_cadec_cls} respectively.}
\label{tab:cls_metrics}
\end{table*}

\begin{table*}
\centering
\scalebox{0.95}{
\begin{tabular}{ lccccccccccccc}
\toprule
\multirow{3}{*}{Dataset} &\multicolumn{6}{c}{\textbf{GLoVe Embeddings}} &\multicolumn{6}{c}{\textbf{BERT Embeddings}} & \textbf{SOTA}\\


\multirow{2}{*}{} &\multicolumn{2}{c}{Precision} &\multicolumn{2}{c}{Recall} &\multicolumn{2}{c}{F1} &\multicolumn{2}{c}{Precision} &\multicolumn{2}{c}{Recall} &\multicolumn{2}{c}{F1} & F1 \\

\multicolumn{1}{c}{} & \multicolumn{1}{c}{strict} & \multicolumn{1}{c}{relax} & \multicolumn{1}{c}{strict} & \multicolumn{1}{c}{relax}& \multicolumn{1}{c}{strict} & \multicolumn{1}{c}{relax} & \multicolumn{1}{c}{strict} & \multicolumn{1}{c}{relax} & \multicolumn{1}{c}{strict} & \multicolumn{1}{c}{relax}& \multicolumn{1}{c}{strict} & \multicolumn{1}{c}{relax} \\
\midrule
\multirow{2}{*}{ADE} & 88.32 & 93.77 & 89.26 & 94.80 & 88.78 & 94.27 & 90.0 & 94.47 & 93.56 & 98.22 & \textbf{91.75} & 96.31 & \textbf{91.3}\\
 & 87.81 & 93.59 & 88.81 & 94.66 & 88.30 & 94.12 & 89.6 & 94.37 & 93.18 & 98.13 & 91.36 & 96.21 \\
 & & & & & & & & & & & & \\
\multirow{2}{*}{CADEC} & 78.14 & 89.04 & 77.14 & 88.01 & 77.62 & 88.50 & 78.53 & 88.63 & 79.03 & 89.32 & \textbf{78.76} & 88.95 & \textbf{71.9}\\
 & 71.87 & 86.36 & 71.67 & 86.13 & 71.75 & 86.23 & 72.38 & 86.14 & 73.64 & 87.66 & 72.99 & 86.88  \\
 & & & & & & & & & & & & \\
 \multirow{2}{*}{SMM4H} & 81.43 & 90.33 & 72.17 & 78.51 & 76.01 & 83.41 & 78.5 & 86.76 & 75.23 & 82.42 & \textbf{76.73} & 84.41 & \textbf{67.81}\\
 & 83.66 & 91.34 & 71.31 & 77.86 & 76.99 & 84.06 & 79.13 & 87.09 & 74.33 & 81.81 & 76.65 & 84.36\\
\bottomrule
\end{tabular}}
\caption{NER metrics on benchmark datasets. For each dataset, macro and micro averaged scores are displayed on first and second row respectively. SOTA metrics for ADE, CADEC, and SMM4H are obtained from \cite{yan2021partition}, \cite{stanovsky-etal-2017-recognizing}, and \cite{DBLP:journals/corr/abs-2003-09288} respectively, and are macro-averaged.}
\label{tab:ner_metrics}
\end{table*}

\begin{table}[ht]
\centering
\scalebox{0.93}{
\begin{tabular}{lcccccc}
\toprule
 \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{3}{c}{\textbf{BERT}} & \multicolumn{3}{c}{\textbf{GLoVe}}\\
 & Train & Infer & F1 & Train & Infer & F1\\
\midrule
 ADE & 1980s & 329s & 91.75 & 1417s & 22s & 88.78\\ 
 CADEC & 2475s & 351s & 78.76 & 1929s & 26s & 77.62\\
 SMM4H & 860s & 136s & 76.73 & 635s & 11s & 76.01\\
\bottomrule
\end{tabular}}
\caption{Training and inference time (in seconds) taken by the NER model on each dataset using different token embeddings with respect to overal performance on test set. Epoch count was kept constant for all datasets while training. The experiment was performed on an 8-core machine having 64gb memory.}
\label{tab:time_stats}
\end{table}
    
    \item For the NER model we align all the datasets in the standard CoNLL format and use IOB (Inside, Outside, Beginning) tagging scheme. We tested other tagging schemes as well, like BIOES, and found IOB to be the simplest and best performing scheme. Since our NER architecture requires word embeddings, we experiment with two types of embeddings. For the GLoVe embeddings, we use our 200-dimension embeddings, and leverage BioBERT embeddings for contextual embeddings.
    
    For thorough analysis of the NER model, we evaluate the results using both strict and relax approach. Under strict evaluation a label is considered as correct if the starting and ending tags exactly match with the gold labels, while under relax evaluation only an overlap between annotations is considered. Consequently, the ‘O’ tag is not included in the calculation. Hyperparameter values, and training code is explained in Appendix A \& B.
    
    \item For training RE models, we use standard NER spans and binary labels. For our base RE model we use 200-dimensional token-level GLoVe embeddings - the same embeddings we use for our base NER model.
    
    For our BERT based RE model, we don't use any explicit embeddings as the BERT model is trained in an end-to-end fashion. We do specify details of entity spans like starting, ending indices, entity types, and the context in between. The context is generally the entire document, but since the model architecture has a 128 token limit, we create context text by taking text in between the entities, and found this method to be more accurate.
    
    We also test a hypothesis that fine-tuning BioBERT model on similar Relation Extraction tasks would increase the overall performance on the benchmark datasets. To test this hypothesis, we train an end-to-end RE model on Disease and Drug datasets like the 2010 i2b2 challenge \cite{uzuner20112010} and saved it. We then use the same weights while discarding the final layers, and retrain the model on the benchmark dataset. Since the base model is trained on a similar taxonomy, the convergence was much faster, while being less prone to over-fitting.
    
     For Hyperparameter tuning we utilize the development set and use random search. Exact hyperparameter values, and the search space for all the models can be found in Appendix A.
    
\end{itemize}

\subsection{Results}

\begin{table*}
\centering
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} &\multicolumn{3}{c}{\textbf{Base (FCNN) RE}} &\multicolumn{3}{c}{\textbf{BERT RE}} & \textbf{SOTA}\\
 & Precision & Recall & F1 & Precision & Recall & F1 & F1\\
\midrule
ADE Corpus & 69.11 & 86.22 & \textbf{74.70} & 81.31 & 79.03 & \textbf{80.10} & \textbf{83.74} \\
ADE Enriched with n2c2 & 89.01 & 89.44 & \textbf{89.22} & 89.19 & 90.93 & \textbf{90.02} & \\
\bottomrule
\end{tabular}
\caption{Relation Extraction performance on the ADE benchmark dataset. The test set was kept standard for a fair comparison, and all scores are macro-averaged due to high class imbalance. SOTA metrics for RE on ADE corpus as reported by \cite{DBLP:journals/corr/abs-2002-06424}}
\label{tab:re_metrics}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{| c | c | c | c | c |}
\hline
\textbf{Document} & \textbf{Class} & \textbf{ADE Entity} & \textbf{Drug Entity} & \textbf{relation}\\
\hline
\multirow{2}{*}{I feel a bit drowsy \& have a little blurred vision after taking insulin.} & \multirow{2}{*}{ADE} & drowsy & insulin & Positive\\
&  & blurred vision & insulin & Positive\\
\hline
\multirow{2}{*}{@yho fluvastatin gave me cramps, but lipitor suits me!} & \multirow{2}{*}{ADE} & cramps & fluvastatin & Positive\\
&  & cramps & lipitor & Negative\\
\hline
I just took advil and haven't had any gastric problems so far. & NEG & -  & -  & - \\
\hline
\end{tabular}
\caption{Full pipeline results on sample texts. Documents having indication for ADR are classified as ADE, while positive relations represent causality between two entities (Drug and ADE). The last example is classified as negative - meaning it does not contain any ADE indication, so we don't process it further.}
\label{tab:examples}
\end{table*}

Despite using a shallow architecture for classification, we achieved metrics that are on-par with SOTA metrics by using more accurate Sentence Bert Embeddings, as shown in Table \ref{tab:cls_metrics}. While the performance difference between BioBERT and GLoVe embeddings is minor on the CADEC dataset, the difference is more prominent on the ADE dataset. This is primarily because of the complex intrinsic nature of biomedical text, where averaging token (GLoVe) embeddings does not efficiently capture the context of complex sentence structures.

Our NER architecture acheives new SOTA metrics on SMM4H, ADE, and CADEC NER datasets using contextual BioBERT embeddings as shown in Table \ref{tab:ner_metrics}. Since the NER model was kept constant during the experiment, and we tuned the hyper parameters for each experiment, the performance difference between embedding types can be attributed to the word embeddings alone. Being able to incorporate contextual information with attention mechanism, BioBERT embeddings peformed better than non-contextual GLoVe embeddings. However, it is worth noticing that the performance difference between the two is in a margin of ~1-2\%, proving that domain specific GLoVe embeddings can provide comparable performance while requiring significantly less memory and computational resources. Table \ref{tab:time_stats} provides side-by-side comparison of time and accuracy differences while using different embedding types. On average, the GLoVe embeddings are ~30\% faster compared to BioBERT embeddings during training, and more than 5x faster during inference, while being on-par in terms of f1 score. 

Our RE solutions perform on-par with existing SOTA systems, while being scalable and requiring less memory to train and test. The introduction of the extra data greatly improved results, enabling us to achieve SOTA on benchmark datasets as shown in Table \ref{tab:re_metrics}. While the more heavy BioBERT model outperformed our proposed RE model on the limited and imbalanced ADE dataset, the performance difference becomes diminutive when more data is added to the training data.

Sample output and visualization of NER and RE results can be seen in Table \ref{tab:examples} and Figure \ref{fig:example_1}.

\begin{figure*}[h!]
  \includegraphics[width=\textwidth]{images/example1.png}
  \caption{Visualization of Entity Recognition and Relation Extraction results on a sample text. 1 denotes positive relation and 0 denotes negative relation (not related).}
  \label{fig:example_1}
\end{figure*}

\section{Conclusion}
Despite the growing need and explosion of useful data for pharmacovigilance, there is a severe deficiency of production-ready NLP systems that can process millions of records while being accurate and versatile.

In this study we address the problem by introducing novel solutions for Classification, NER, and RE while leveraging the Spark ecosystem and contemplating on accuracy, scalability, and versatility. For which we explain how we build a modular structure comprising of different embedding types, a classification and NER model, and two approaches for RE. We trained custom GLoVe embeddings model on domain-specific dataset, and compare its performance to SOTA BioBERT embeddings. We show through extensive testing that our text classification model, for deciding if a conversation includes an ADR, obtains new state-of-the-art accuracy on the CADEC dataset (86.69\% F1 score). Our proposed NER architecture achieves SOTA results on multiple benchmark datasets. Namely, our proposed NER models obtain new state-of-the-art accuracy for ADR and Drug entity extraction on the ADE, CADEC, and SMM4H benchmark datasets (91.75\%, 78.76\%, and 83.41\% F1 scores respectively). Then we explain two different architectures for RE, one based on BioBERT while the other utilizing crafted features over a FCNN, test them individually, and show that a simpler RE architecture with bespoke features performed on-par with more sophisticated BERT solution. To improve our RE model, we built a new dataset by manual annotations, and achieved higher metrics on the RE test datasets.

Furthermore, we performed speed benchmarks to compare efficiency of two distinct embedding generation models to determine the ideal choice for deploying such solutions to process large quantities of data. In general, most pharmaceutical companies run on-premise servers which are geared towards general computation and do not utilise hardware acceleration like GPUs for running heavy models; In such cases where infrastructure is not mature enough to handle heavy models, lightweight glove-based models are a compelling alternative to BERT-based models, as they offer comparable performance while being memory and CPU efficient.

Finally, we implement all these algorithms in Apache Spark ecosystem for scalability, and shipped in a production grade NLP library: Spark NLP.

\appendix
\section{A. Hyperparameter Settings}

The following parameters provided best results on the \textbf{classification} development set (values within the parenthesis represent the
parameter ranges tested):
\begin{itemize}
    \item Dropout rate: 0.2 (0.2, 0.7)
    \item Batch size: 8 (4, 256)
    \item Learning rate: 0.0003 (0.01, 0.0003)
    \item Epoch: 25-30 (10, 100)
    \item Optimizer: Adam
    \item Learning rate decay coefficient (po) (real learning rate =
lr / (1 + po * epoch) : 0.005 (0.001, 0.01))
\end{itemize}

The following parameters provided best results on the \textbf{NER} development set (Values within the parenthesis represent the parameter ranges tested):
\begin{itemize}
        \item LSTM state size: 200 (200, 250)
        \item Dropout rate: 0.5 (0.3, 0.7)
        \item Batch size: 8 (4, 256)
        \item Learning rate: 0.001 (0.01, 0.0003)
        \item Epoch: 25-35 (10, 100)
        \item Optimizer: Adam
        \item Learning rate decay coefficient (po) (real learning rate =
    lr / (1 + po * epoch) Smith [2018] : 0.005 (0.001, 0.01))
\end{itemize}

The following parameters provided best results on the \textbf{RE} development set (Values within the parenthesis represent the parameter ranges tested):
\begin{itemize}
        \item Dropout rate: 0.5 (0.3, 0.7)
        \item Batch size: 8 (4, 256)
        \item Learning rate: 0.0001 (0.01, 0.0003)
        \item Epoch: 4-BERT (1-10), 50-FCNN (10-100) 
\end{itemize}

\section{B. Training Code}
Code for training an RE model is provided as a google colab notebook \cite{re_training_code}. 

\newpage
\bibliography{references}



\end{document}
