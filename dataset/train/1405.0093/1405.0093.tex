\documentclass[11pt,letter]{article}
\usepackage[margin=1in]{geometry}





\title{Parameterized Streaming Algorithms for Vertex Cover}



\author{
Rajesh Chitnis\thanks{Department of Computer Science , University of Maryland at
College Park, USA.  rchitnis@cs.umd.edu. Supported in part by NSF CAREER award 1053605, NSF grant CCF-1161626, ONR YIP award
N000141110662, DARPA/AFOSR grant FA9550-12-1-0423 and a Simons Award for Graduate Students in Theoretical Computer Science.}
\and
Graham Cormode \thanks{Department of Computer Science, University of
  Warwick, UK. g.cormode@warwick.ac.uk.}
\and
 MohammadTaghi Hajiaghayi\thanks{Department of Computer Science , University of Maryland,
 USA.  hajiagha@cs.umd.edu. Supported in part by NSF CAREER award 1053605, NSF grant CCF-1161626, ONR YIP award
N000141110662, and DARPA/AFOSR grant FA9550-12-1-0423.}
\and
Morteza Monemizadeh\thanks{Goethe-Universit\"{a}t Frankfurt, Germany and Department of Computer Science, University of Maryland at
College Park, USA.   monemizadeh@em.uni-frankfurt.de,
Supported in part by MO 2200/1-1.}
}




\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{latexsym}
\usepackage{subfigure}
\usepackage{graphicx, datetime}
\usepackage{times}
\usepackage[scaled=0.92]{helvet} \usepackage{euler}
\usepackage{fancybox}
\usepackage{hyperref}
\usepackage{ifpdf}
\ifpdf
\pdfpagewidth10.5in
\pdfpageheight13in
\fi








\newcount\shortyear\newcount\shorthour\newcount\shortminute
\shorthour=\time\divide\shorthour by 60\shortyear=\shorthour
\multiply\shortyear by 60\shortminute=\time\advance\shortminute by
-\shortyear \shortyear=\year\advance\shortyear by -1900

\def\zeit{\number\shorthour:\ifnum\shortminute<10 0\number\shortminute
\else\number\shortminute\fi}












\newenvironment{proofhack}{\noindent {\bf Proof}.\ }{}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{fact}[theorem]{Fact}


\newtheorem*{rep@theorem}{\rep@title} \newcommand{\newreptheorem}[2]{\newenvironment{rep#1}[1]{\def\rep@title{\bf #2 \ref{##1}}\begin{rep@theorem}}{\end{rep@theorem} } }
\makeatother
\newreptheorem{theorem}{Theorem}
\newreptheorem{lemma}{Lemma}




\newenvironment{proofof}[1]{\begin{trivlist} \item {\bf Proof
#1:~~}}
  {\qed\end{trivlist}}
\renewenvironment{proofof}[1]{\par\medskip\noindent{\bf Proof of #1: \ }}{\hfill\par\medskip}




\renewcommand{\labelenumi}{(\arabic{enumi})}
\renewcommand\labelitemii{\mbox{}}
\renewcommand\labelitemiii{\mbox{\tiny}}
\renewcommand{\O}{\ensuremath{{\cal O}}}
\renewcommand{\Pr}[1]{\ensuremath{\mathbf{Pr}[#1]}}

\renewcommand{\paragraph}[1]{\medskip \noindent {\bf #1}}



\newcommand{\COMMENTED}[1]{{}}
\newcommand{\junk}[1]{\COMMENTED{#1}}
\newcommand{\NAT}{\ensuremath{\mathbb{N}}}
\newcommand{\NATURAL}{\NAT}
\newcommand{\REAL}{\ensuremath{\mathbb{R}}}
\newcommand{\PPr}[1]{\ensuremath{\mathbf{Pr}\big[#1\big]}}
\newcommand{\Ex}[1]{\ensuremath{\mathbf{E}[#1]}}
\newcommand{\EEx}[1]{\ensuremath{\mathbf{E}\big[#1\big]}}
\newcommand{\poly}{{\mathrm{poly}}}




\newcommand{\sample}{\ensuremath{\textsf{\small Sample}}}






\newcommand{\Morteza}[1]{{{\footnote{{\sc\small {\color{red}\textbf{Morteza:}}} \color{blue}{#1}}}}}
\newcommand{\Rajesh}[1]{{{\footnote{{\sc\small {\color{red}\textbf{Rajesh:}}} \color{blue}{#1}}}}}
\newcommand{\Mohammad}[1]{{{\footnote{{\sc\small {\color{red}\textbf{Mohammad:}}} \color{blue}{#1}}}}}
\newcommand{\Graham}[1]{{{\footnote{{\sc\small {\color{red}\textbf{Graham:}}} \color{blue}{#1}}}}}


\newcommand{\todo}[1]{{\bf{TODO: #1}}}
















\newlength{\savedparindent}
\newcommand{\SaveIndent}{\setlength{\savedparindent}{\parindent}}
\newcommand{\RestoreIndent}{\setlength{\parindent}{\savedparindent}}

\newcommand{\InGray}[1]{\SaveIndent{} \noindent{} \fcolorbox[rgb]{0,0,0}{0.95,0.95,0.95}{
\begin{minipage}{0.965\linewidth} \RestoreIndent{}#1
\end{minipage}
} }

\newcommand{\InGrayMiddle}[1]{\SaveIndent{} \centerline{ \fcolorbox[rgb]{0,0,0}{0.95,0.95,0.95}{
\begin{minipage}{0.8\linewidth} \RestoreIndent{}#1
\end{minipage}
} } }

\newcommand{\InGrayNarrow}[1]{\SaveIndent{} \centerline{ \fcolorbox[rgb]{0,0,0}{0.95,0.95,0.95}{
\begin{minipage}{0.65\linewidth} \RestoreIndent{}#1
\end{minipage}
} } }

\newcommand{\InGraySmall}[1]{\SaveIndent{} \centerline{ \fcolorbox[rgb]{0,0,0}{0.95,0.95,0.95}{
\begin{minipage}{0.65} \RestoreIndent{}#1
\end{minipage}
} } }









\begin{document}





\sloppy
\maketitle


\begin{abstract}
As graphs continue to grow in size, we seek ways to effectively process such data at scale.
The model of streaming graph processing, in which a compact summary is maintained as each
edge insertion/deletion is observed, is an attractive one.
However, few results are known for optimization problems over such dynamic graph streams.

In this paper, we introduce a new approach to handling graph streams,
by instead seeking solutions for the parameterized versions of these problems
where we are given a parameter  and the objective is to decide
whether there is a solution bounded by .
By combining kernelization techniques with randomized sketch structures,
we obtain the first streaming algorithms for the parameterized versions of
the Vertex Cover problem. We consider two models for a graph stream on
 nodes:
the insertion-only model where the edges can only be added,
and the dynamic model where edges can be both inserted and deleted.
More formally, we show the following results:

\begin{itemize}
\item In the insertion only model, there is a one-pass deterministic algorithm
for the parameterized Vertex Cover problem which computes a sketch
using  space\footnote{, where  is the number of edges.} such that at each
  timestamp in time  it can either extract a solution of
  size at most  for the current instance, or report that no such solution exists.

\item In the dynamic model, and under the \emph{promise} that at each
timestamp there is a solution of size at most , there is a one-pass
algorithm for the parameterized Vertex Cover problem which computes
a sketch using  space such that in time  it can
extract a solution for the final instance with probability , where .
To the best of our knowledge, this is the first graph streaming algorithm that combines linear
sketching with sequential operations that depend on the graph at the current time.


\item In the dynamic model without any promise, there is a one-pass
randomized algorithm for the parameterized Vertex Cover problem
which computes a sketch using  space such that in time  it can either extract a solution of size
at most  for the final instance, or report that no such solution exists.
\end{itemize}
We also show a tight lower bound of  for the space
complexity of any (randomized) streaming algorithms for
the parameterized Vertex Cover, even in the insertion-only model.
\end{abstract}







\newpage

\section{Introduction}
Many large graphs are presented in the form of a sequence of edges.
This stream of edges may be a simple stream of edge arrivals, where
each edge adds to the graph seen so far, or may include a mixture of
arrivals and departures of edges.
In either case, we want to be able to quickly answer basic optimization
questions over the current state of the graph, such as finding a
(maximal) matching over the current graph edges, or finding a
(minimum) vertex cover, while storing only a limited amount of
information, sublinear in the size of the current graph.

The semi-streaming model introduced by
Feigenbaum, Kannan, McGregor, Suri and Zhang \cite{FKMSZ05}  is a classical
streaming model in which maximal matching and vertex cover are studied.
In the semi-streaming model we are interested to solve (mostly approximately) graph problems
using one pass over the graph and using  space.
Numerous problems have been studied in this setting, such as
maintaining random walks and page rank over large graphs~\cite{DGP08}.

However, in many real world applications, we often observe instances of graph problems
whose solutions are small comparing to the size of input. Consider for example
the problem of finding the minimum number of fire stations to cover an entire city,
or other cases where we expect a small number of facilities will serve
a large number of locations. In these scenarios, assuming that the number of fire
stations or facilities is a small number  is very practical. So, it will be interesting
to solve instances of graph problems (like minimal matching
and vertex cover) whose solutions are small (say, sublinear in the input size)
in a streaming fashion using space which is bounded with respect to
the size of their solutions, not the input size.




In this paper to solve graph problems whose solutions are small,
we {\em parameterize} problems with a parameter , and
solve the decision problem of finding whether there exists a solution whose size is bounded by .
We therefore seek \emph{parameterized streaming algorithms} whose space and time complexities
are bounded with respect to  , i.e.,  sublinear in the size of the input.



There are several ways to formalize this question, and we give results for the most natural formalizations.
The basic case is when the input consists of a sequence of edge
arrivals only, for which we seek a {\em parameterized streaming
  algorithm} (PSA).
More challenging problems arise when the input stream is more
dynamic, and can contain both deletions and insertions of edges. In this case we seek a {\em dynamic parameterized streaming
  algorithm} (DPSA).
The challenge here is that when an edge in the matching is deleted, we
sometimes need substantial work to repair the solution, and have to
ensure that the algorithm has enough information to do so, while keeping only a bounded amount of working space. If we are \emph{promised} that at every timestamp there is a solution of cost , then we seek a {\em promised dynamic parameterized streaming
  algorithm} (PDPSA).









\subsection{Parameterized Complexity}

Most interesting optimization problems on graphs are NP-hard,
implying that, unless P=NP, there is no polynomial time algorithm
that solves all the instances of an NP-hard problem exactly.
However as noted by Garey and Johnson~\cite{garey-johnson},
hardness results such as NP-hardness should merely constitute
the beginning of research.
The traditional way to combat  intractability is to design approximation
algorithms or randomized algorithms which run in polynomial time.
These methods have their own shortcomings:
we either get an approximate solution or lose the guarantee that
the output is always correct.


Parameterized complexity is essentially a two-dimensional analogue of ``P
vs NP". The running time is analyzed in finer detail:
instead of expressing it as a function of only the input size , one
or more parameters of the input instance are defined,
and we investigate the effects of these parameters on the running time.
The goal is to design algorithms that work efficiently if
the parameters of the input instance are small, even if the size of
the input is large. We refer the reader to~\cite{DF99, FG06} for more background.

A  \textit{parameterization} of a decision problem   is a function
that assigns an integer parameter  to each instance  of .
We assume that instance  of problem  has
the corresponding input 
consisting of elements  (e.g. edges defining a graph).
We denote the input size of instance   by .
In what follows, we assume that  and  are functions of an
integer parameter .


\begin{definition}[Fixed-Parameter Tractability (FPT) ]
\label{def:fpt}
A parameterized problem  is \emph{fixed-parameter tractable} (FPT) if there is an algorithm
that in time  returns a solution for each instance  whose  size
fulfills a given condition corresponding to  (say, at most   or at least )
or reports that such a solution does not exist.
\end{definition}






To illustrate this concept,
we define the parameterized version of Vertex Cover as follows.
A \textit{vertex cover} of an undirected graph  is a subset 
of vertices such that for every edge  at least one
of the endpoints (or vertices) of  is in .



\begin{definition}[Parameterized Vertex Cover ()]
\label{def:vc}
Given an instance  where  is an undirected graph  (with input size
 and ) and parameter ,  the goal in the
\textit{parameterized Vertex Cover} problem (  for short)
is to develop an algorithm that in time  either returns
a vertex cover of size at most  for , or reports that  does not have
any vertex cover of size at most .
\end{definition}

A simple branching method gives a  algorithm for :
choose any edge and branch on choosing either end-point of the edge into the solution.
The current fastest FPT algorithm for  is due to
Chen et al.~\cite{kanj-vc} and runs in time .






One of the techniques used to obtain FPT algorithms is
\textit{kernelization}. In fact, it is known that a problem is FPT if
and only if it has a kernel~\cite{FG06}.
Kernelization has been used to design efficient algorithms by using polynomial-time
preprocessing to replace the input by another equivalent input of smaller size. More formally, we have:

\begin{definition}[Kernelization]
\label{def:kernel}
For a parameterized problem , kernelization is a polynomial-time transformation
that maps an instance  of  to an instance  such that
\begin{itemize}
\item  is a yes-instance if and only if  is a yes-instance;
\item  for some computable function ;
\item the size of   is bounded by some computable function  of , i.e., .
\end{itemize}
The output  of a kernelization algorithm is called a \textit{kernel}.
\end{definition}

In Section~\ref{sec:kernel:VC} we review
the kernelization algorithm of Buss and Goldsmith~\cite{BG93} for the parameterized Vertex Cover problem
which relies on finding a maximal matching of a graph .
This kernel gives a graph with  vertices and  edges.
Another kernelization algorithm given in~\cite{FG06} exploits the half-integrality
property of LP-relaxation for vertex cover due to Nemhauser and Trotter,
and produces a graph with at most  vertices.




\subsection{Parameterized Streaming Algorithms: Our Results}

In order to state our results for \textit{parameterized streaming}
we first define the notion of a \textit{sketch}  in a very general form.


\begin{definition}[Sketch \cite{AMS99,FKSV02,I06JACM}]
\label{def:sketch}
A \it{sketch} is a sublinear-space data structure that supports
a fixed set of queries and updates.
\end{definition}




\paragraph{Insertion-Only Streaming.}
Let   be a problem parameterized by .
Let  be an instance of  that has the input
.
Let  be a stream of  (i.e., the insertion of an element )
operations of underlying instance .
In particular, stream  is a permutation
 for  of an input . Here we denote
the time when an input  is inserted by \textit{time} . At time ,
the input which corresponds to instance  is .



\begin{definition}({\sc Parameterized streaming algorithm (PSA)})
Given stream , let   be an algorithm  that computes a sketch for problem 
using -space
and with one pass over stream . Suppose at a time ,
algorithm  in time  extracts,  from the sketch,
a solution for input  (of instance ) whose  size fulfills
the condition corresponding to   or reports that such a solution does not exist.
Then we say  is a -PSA.
\label{defn:psa}
\end{definition}




For many problems, whether or not there is a solution of size at most
 is monotonic under edge additions, and so
if at time , algorithm  reports that a solution
for input  does not exist, then
there is also no solution for any input  of instance 
at all times .
Consequently, we can terminate the algorithm .
We state our result on the parameterized streaming
algorithm for Vertex Cover and prove it in Section \ref{sec:psa:vc}.


\begin{theorem}
\label{THM:VC:INSERTION}
Let  be a stream of  insertions of edges of an underlying graph .
Then there exists a \emph{deterministic} -PSA for  problem.
\end{theorem}



The best known kernel size for the  problem is  edges~\cite{BG93}.
In fact, Dell and van Melkebeek~\cite{dell-lower-bound} showed that it is not possible to get
a kernel for the  problem with  edges for any ,
under some assumptions from classical complexity.
Interestingly, the space complexity of our PSA of Theorem \ref{THM:VC:INSERTION} matches
this best known kernel size.
In Section \ref{sec:vc:bound} we show that the space complexity of above PSA
is optimal even if we use randomization.
More precisely, we prove the following result.

\begin{theorem}
\label{thm:vc:lower:bound}
Any (randomized) PSA for the  problem requires  space.
\label{thm:lower-bound-vc}
\end{theorem}








\paragraph{Dynamic Streaming.}
We define \textit{dynamic parameterized stream} as a generalization of \textit{dynamic graph stream}
introduced by Ahn, Guha and McGregor \cite{AGM12a}.



\begin{definition}[Dynamic Parameterized Stream]
Let   be a problem parameterized by . Let  be an instance of 
that has an input  with input size .
We say stream  is a dynamic parameterized stream if 
is  a stream of  (i.e., the insertion
of an element ) and  (i.e., the deletion of
an element ) operations applying to the underlying instance
 of .
\end{definition}



Now stream  is not simply a permutation
 for  of an input
, but rather a sequence of transactions that collectively define a
graph. We assume the size of stream 
is  for a constant  which means  
or asymptotically, . We denote the time which corresponds to
the -th update operation of  by \textit{time} . The -th update operation
can be  or  for 
(note that we can perform  only if  is
present at time ). At time ,
the input of instance  is a subset  of inputs
which are, up to time , inserted but not deleted.


We next define a \textit{promised} streaming model as follows.
Suppose we know for sure that at every time  of a dynamic parameterized
stream , the size of the vertex cover of underlying graph  (where 
is the set of edges that are inserted up to time  but not deleted)
is at most . We show that within the framework of the
promised streaming model we are able to develop a dynamic parameterized
streaming algorithm whose space usage matches the lower bound of
Theorem \ref{thm:vc:lower:bound} up to  factor.



We formulate a dynamic parameterized streaming algorithm within the framework of
the promised streaming model as follows.


\begin{definition}
({\sc Promised dynamic parameterized streaming algorithm (PDPSA)})
Let   be a promised dynamic parameterized stream, i.e., we are promised
that at every time , there is  a solution for input 
whose  size fulfills the condition corresponding to .
Let   be an algorithm  that computes a sketch for problem 
using -space in one pass over stream .
Suppose at the end of stream , i.e., time , algorithm 
in time  extracts,  from the sketch,
a solution for input  (of instance ) whose  size fulfills
the condition corresponding to .
We say  is an -PDPSA.
\end{definition}



Next, using the well-known connection (see, for example, Chapter 9 of~\cite{FG06})
between maximal matching and kernelization algorithms for parameterized Vertex Cover,
we show that kernels for matching can be implemented in data streams in small space, and
this in turn gives  a PDPSA for  problem.
We summarize this main result in the following theorem and
we develop it in Section \ref{sec:promised:dynamic}.


\begin{theorem}
\label{thm:vc:max:k}
Suppose at every timestep
the size of the vertex cover of underlying graph  is at most .
There exists a -PDPSA for  with probability ,
where  and  is a constant.
\end{theorem}

Our algorithm takes the novel approach of combining linear sketching
with sequential operations that depend on the current state of the
graph.
Prior work in sketching has instead only performed updates of sketches
for each stream update, and postponed insepecting them until the end
of the stream.


As a byproduct of this main theorem we have the following corollary.

\begin{corollary}
\label{cor:general:MM:dyn:promised}
Assume we are promised that a maximal matching of underlying graph 
at every time  of dynamic parameterized stream  is of size
at most  for .
Then, there exists a dynamic algorithm that maintains
a maximal matching of graph  using  space.
The update time (i.e., the time to maintain the sketch)
and query time (i.e., the time to maintain a maximal matching using the sketch)
of this algorithm are worst-case . For ,
this gives a dynamic  algorithm for maximal matching whose space,
worst-case update and query times
are ,  and , respectively.
\end{corollary}




Finally, we formulate a dynamic parameterized streaming algorithm without any promise as follows.

\begin{definition}
({\sc Dynamic parameterized streaming algorithm (DPSA)})
\label{defn:dpsa}
Let  be a dynamic parameterized stream .
Let   be an algorithm  that computes a sketch for problem 
using -space and with one pass over stream .
Suppose at the end of stream , i.e., time , algorithm 
in time  extracts,  from the sketch,
a solution for input  whose  size fulfills
the condition corresponding to   or reports that such a solution does not exist.
We say  is an -DPSA.
\end{definition}

We state our result on the DPSA (without any promise) for Vertex Cover
and prove it in Section~\ref{app:vc:dpsa}.

\begin{theorem}
\label{THM:VC:DPSA}
Let  be a dynamic parameterized stream of  insertions and deletions of edges of an underlying graph .
There exists a randomized -DPSA for  problem.
\end{theorem}

For graphs which are not  sparse (i.e., ) the algorithm of Theorem \ref{THM:VC:DPSA}
gives -DPSA for .
The space usage of PDPSA of Theorem \ref{thm:vc:max:k} matches
the lower bound of Theorem \ref{thm:vc:lower:bound}. On the other hand,
there is a gap between space bound  of DPSA of Theorem \ref{THM:VC:DPSA}
and lower bound  of Theorem \ref{thm:vc:lower:bound}.
We conjecture that the lower bound for the space usage of any
(randomized) DPSA for  problem is indeed .




\subsection{Related Work}

The question of finding maximal and maximum cardinality matchings has
been heavily studied in the model of (insert-only) graph streams.
A greedy algorithm trivially obtains a maximal matching (simply store
every edge that links two currently unmatched nodes); this can also be
shown to be a 0.5-approximation to the maximum cardinality matching~\cite{Feigenbaum:Kannan:McGregor:Suri:Zhang:05}.
By taking multiple passes over the input streams, this can be improved
to a  approximation, by finding augmenting paths with
successive passes~\cite{McGregor:05,McGregor:09}.

Subsequent work has extended to the case of weighted edges (when a
maximum weight matching is sought), and reducing the number of passes
to provide a guaranteed approximation~\cite{Eggert:Kliemann:Srivastav:09,Eggert:Kliemann:Munstermann:Srivastav:12}.
While approximating the size of the vertex cover has been studied in other sublinear models, such as
sampling~\cite{Parnas:Ron:07,Onak:Ron:Rosen:Rubinfeld:12}, we are not
aware of prior work that has addressed the question of finding a
vertex cover over a graph stream.
Likewise, parameterized complexity has not been explicitly studied in
the streaming model, so we initiate it here.

The model of dynamic graph streams has recently received much attention,
due to breakthroughs by Ahn, Guha and
McGregor~\cite{AGM12a,Ahn:Guha:McGregor:12PODS}.
Over two papers, they showed the first results for a number of graph
problems over dynamic streams, including
determining connected components,
testing bipartiteness,
minimum spanning tree weight and building a sparsifier.
They also gave multipass algorithms for maximum weight matchings
and spanner constructions.
This has provoked much interest into what can be computed over dynamic
graph streams.



\paragraph{Outline.}
Section~\ref{sec:prelims} provides background on techniques for
kernelization of graph problems, and on streaming algorithms for
building a sketch to recover a compact set.
Our results on PSA and DPSA are stated in Section~\ref{sec:psa:vc}
and Section~\ref{app:vc:dpsa}, respectively.
Section~\ref{sec:promised:dynamic} is the most involved, as it addresses the most
difficult dynamic case in the promised model.





\section{Preliminaries}
\label{sec:prelims}



In this section, we present the definitions of streaming model and
the graph sketching that we use.







\paragraph{Streaming Model.}
Let  be a stream of insertions (or similarly, insertions and deletions) of edges of an underlying graph
. We assume that vertex set  is fixed and given, and the size of 
is . We assume that the size of stream  is  for some large
enough constant  so that we may assume that
.
Here  when .
Throughout the paper we denote a failure probabilities by , and
approximation parameters by .


We assume that there is a unique numbering for the vertices in
 so that we can treat  as a unique number  for .
We denote an undirected edge in  with two endpoints  by
. The graph  can
have at most  edges.
Thus, each edge can also be thought of as referring to a unique number
between 1 and .



At the start of stream , edge set  is an empty set.
We assume in the course of stream , the maximum size of  is a
number , i.e., . Counter  stores the current number of
edges of stream , i.e., after every insertion we increment  by one and
after every deletion we decrement  by one.


Let  be a maximal matching that we maintain for stream .
Edges in  are called \textit{matched} edges; the other edges are \textit{free}.
If  is a matched edge, then  is the \textit{mate} of  and  is the
\textit{mate} of . Let  be the vertices of  and
. A vertex  which is in  is called
a \textit{matched} vertex, otherwise, i.e., if ,  is called
an \textit{exposed} vertex.

The \textit{neighborhood} of a vertex  is defined as
. Hence the degree of a vertex
 is .
We split the neighborhood of  into the set
of matched neighbors of , , and the set
of exposed neighbors of , i.e.,
.






\paragraph{Oblivious Adversarial Model.}
We work in the \textit{oblivious adversarial model} as is common for analysis of randomized data structures
such as universal hashing \cite{CW77}.
This model has been used in a series of papers on
dynamic maximal matching and dynamic connectivity problems: see for example
\cite{OR10, BGS11, KKM13, NS13}.
The model allows the adversary to know all the edges in the graph
 and their arrival order, as well as the algorithm to be
used.
However, the adversary is not aware of the random bits used by the
algorithm, and so cannot choose updates adaptively in response to the
randomly guided choices of the algorithm.
This effectively means that we can assume that the adversary prepares
the full input (inserts and deletes) before the algorithm runs.





\paragraph{-Sparse Recovery Sketch and Graph Sketching.}
We first define an -Sampler as follows.
\begin{definition}[-Sampler \cite{FIS05, mw10}]
\label{def:l0}
Let  be a parameter.
Let  be a stream of updates of an underlying vector 
where  and . The -th update 
updates  the -th element of  using
. A -sampler algorithm for  returns FAIL with probability at most .
Else, with probability , it returns an element  such that the probability that -th element
is returned is .
\end{definition}

Here,  is the
(so-called) ``-norm'' of  that counts the number of non-zero entries.

\begin{lemma}[\cite{JST11}]
\label{lem:l0:sampling}
Let  be a parameter.
There exists a linear sketch-based algorithm for -sampling
using  bits of space.
\end{lemma}

The concepts behind sketches for -sampling can be generalized
to draw  distinct elements from the support set of :


\begin{definition}[-sample recovery]
\label{def:ksparse}
A -sample recovery algorithm recovers
 elements from  such that sampled index  has
 and is sampled uniformly.
\end{definition}

Constructions of -sample recovery mechanisms are known which
require space  and fail only with probability
polynomially small in ~\cite{Barkay:Porat:Shalem:12}.
We apply this algorithm to the neighborhood of vertices: for each
node , we can maintain an instance of the -sample recovery sketch (or algorithm) to the
vector corresponding to the row of the adjacency matrix for .
Note that as edges are inserted or deleted, we can propagate these to
the appropriate -sample recovery algorithms, without needing
knowledge of the full neighborhood of nodes.



Specifically,
let  be the rows of the adjacency matrix of
, , where
 encodes the neighborhood of a vertex . We define the sketch of  as follows.
Let  be a stream of insertions and deletions of edges to an underlying graph .
We sketch each row  of  using the sketching matrix of
Lemma \ref{lem:l0:sampling}. Let us denote this sketch by .
Since sketch  is linear, the following operations can be done in the sketch space.

\begin{itemize}
\item \textsc{Query}:  This operation queries sketch  to
find a uniformly random neighbor of vertex . Since  is a -sample
recovery sketch, we can query up to  uniformly random neighbors of vertex .
\item \textsc{Update}:
This operation updates the sketch of a vertex .
In particular, operation \textsc{Update}
means that edge  is added to  sketch .
And, operation \textsc{Update}
means that edge  is deleted from  sketch .
\end{itemize}




\section{Parameterized Streaming Algorithm (PSA) for }
\label{sec:psa:vc}

In this section, we give a -PSA for  along with a matching  lower bound for the space complexity of . First, we review the kernelization algorithm of Buss and Goldsmith~\cite{BG93}
since we use it in our PSA for .

\subsection{Kernel for }
\label{sec:kernel:VC}
Let  be the original instance of the problem which is initialized by graph
 and parameter . Let  denote the degree of  in .
While one of the following rules can be applied, we follow it.


\begin{enumerate}
 \item \underline{There exists a vertex  with }: Observe that if we do not include  in the vertex cover, then we must include all of . Since , we must include  in our vertex cover for now. Update  and .

\item \underline{There is an isolated vertex }: Remove  from , since
  cannot cover any edge.


\end{enumerate}

If neither of above rules can be applied, then we look at the number of edges of . Note that the maximum degree of  is now . Hence, if  has a vertex cover of size , then the maximum number of edges in  is . If , then we can safely answer NO. Otherwise we now have a kernel graph  such that . Since  does not have any isolated vertex, we have . Observe that we obtain the kernel graph  in polynomial time.


Now we show how to obtain an FPT algorithm for Parameterized Vertex Cover using the above kernelization: Enumerate all vertex subsets of
 of size , and checks whether any of them forms a vertex cover. The number of such subsets is ,
and checking whether a given subset is a vertex cover can be done in polynomial time.
We answer YES if any of the subsets of size  forms a vertex cover, and NO otherwise. After obtaining the kernel graph in polynomial time, the running time of this algorithm is .

\subsection{-PSA for }
\label{subsec:psa-vc}

We now prove Theorem~\ref{THM:VC:INSERTION}, which is restated below:



\begin{reptheorem}{THM:VC:INSERTION}
Let  be a stream of  insertions of edges of an underlying graph . Then
there exists a \emph{deterministic} -PSA for  problem.
\end{reptheorem}
\begin{proof}
The proof is divided into three parts: first we describe the algorithm, analyze its complexity and then show its correctness.

\paragraph{Algorithm.} Let  be a stream of insertions of edges of an underlying graph .
We maintain a maximal matching  of stream  in a greedy fashion.
Let  be the vertices of matching . For every matched vertex ,
we also store up to  edges incident on  in a set . If at a timestamp  of stream 
we observe that , we report that the size of any vertex cover of
 is more than  and quit.
At the end of stream , we run the kernelization algorithm of Section~\ref{sec:kernel:VC}
on instance .



\paragraph{Complexity of the Algorithm.}
We observe that the space complexity of the algorithm is . In fact,
for each vertex  assuming  we keep at most  edges, thus
we need space of at most . If , as soon as the size
of the matching  goes beyond  we quit the algorithm and so in this case
we also use space of at most .
The query time of this algorithm
is dominated by the time to extract the vertex cover of  (and
also of ) using the brute-force search algorithm (if one exists), which is .



\paragraph{Correctness.}
We argue that
\begin{enumerate}
\item if the kernelization algorithm succeeds on
instance  and finds a vertex cover of size at most  for ,
then that vertex cover is also a vertex cover of size at most  for .
\item On the other hand, if the kernelization algorithm reports that
instance  does not have a vertex cover of size at most ,
then instance  does not have a vertex cover of size at most .
\end{enumerate}

First, note that trivially, any matching provides a lower bound on the
size of the vertex cover, and hence we are correct to reject if
.


Otherwise, i.e., if , we write  and  for the degree of 
in  and , respectively. We follow rules of the kernelization algorithm
on  and  in lockstep. Observe that since every edge  is incident
on at least one matched vertex , when an edge
 is not stored in  it is in one of the following
cases.
\begin{enumerate}
\item  and :
Then, we must have  and  which means that  and .
\item Only :
Then, we must have  which means that .
\item Only :
Then, we must have  which means that .
\end{enumerate}



Now, let us consider a set  (for ) of vertices
that Rule  of the kernelization algorithm for  removes. According to Rule
, for a vertex  (for ) with , we
remove  and all edges incident on  from  and decrease  by one.
Note that  iff . This is due to the fact that,
before we remove vertex  from ,
we have removed only  those neighbors of  that are matched and the number of
such vertices is less than . Thus, Rule  of the kernelization algorithm can be applied
on  and we remove  and all edges
incident on  from  and decrease  by one.




Next we consider Rule . Assume in one step of the kernelization algorithm for ,
we have an isolated vertex . Observe that those neighbors of  that we
have removed using Rule  (before vertex  becomes isolated) are all matched vertices
and the number of such vertices is less than .
Moreover,  never had any neighbor in  otherwise,  is not isolated.
Thus, if  has a neighbor  in the remaining vertices of ,
edge  must be in  as we store up to  edges incident on  in
set  which means  is not isolated in  and that is in contradiction
to our assumption that  is isolated in .
Since we run the kernelization algorithm on  and
on  for the vertices in set , the same thing happens for , i.e.,
 in  is also isolated. So, using Rule ,  is removed from
 iff  is removed from .




 Now assume neither Rule  nor Rule  can be applied for ,
 but the number of edges in  is more than . The same thing must
 happen for . Therefore,  and  do not have a vertex cover of size
 at most .

If none of the above rules can be applied for , we have a kernel 
such that  and . Now observe that after removal
of all vertices of  and their incident edges from , for every remaining vertex 
in , ; otherwise  and ; so we can apply Rule  which is
in contradiction to our assumption that none of the above rules can be applied for .
Therefore,  kernel  is also a kernel for  and this proves the correctness
of our algorithm.
\end{proof}


\subsection{ Lower Bound for }
\label{sec:vc:bound}

Next, we prove Theorem~\ref{thm:vc:lower:bound} which is restated below:

\begin{reptheorem}{thm:vc:lower:bound}
Any (randomized) PSA for the  problem requires  space.
\label{thm:lower-bound-vc}
\end{reptheorem}
\begin{proof}

We will reduce from the \textsc{Index} problem in communication complexity:

\begin{center}\framebox{\begin{minipage}{0.8\columnwidth}
\underline{\textsc{Index}}\\
\emph{Input}: Alice has a string  given by
. Bob has an index \\
\emph{Question}: Bob wants to find , i.e., the  bit of .
\end{minipage}}\end{center}

It is well-known that there is a lower bound of  bits
in the one-way randomized communication model for Bob to compute ~\cite{nisan}.
We assume an instance of the index problem where  is a perfect
square, and let . Fix a canonical mapping from .
Consequently we can interpret the bit string as an adjacency matrix for a
bipartite graph with  vertices on each side.

From the instance of \textsc{Index}, we construct an instance 
of Vertex Cover.
Assume that Alice has an algorithm which solves the  problem
using  bits.
For each , we have vertices,
, and .
First, we insert the edges corresponding to the edge interpretation of
 between nodes  and :
  for each , Alice adds the edge  if
  the corresponding entry in  is 1.
Alice then sends the memory contents of her algorithm to Bob, using
 bits.

Bob has the index , which he interprets as  under
the same canonical remapping to .
He receives the memory contents of the algorithm, and proceeds to add
edges to the instance of vertex cover.
For each , Bob adds the edges  and .
Similarly, for each , Bob adds the edges
 and .




\junk{
It follows that there is a lower bound of  bits for Bob
to compute the matrix entry . The next theorem shows that
if there is a single pass streaming algorithm, say ,
which solves the  problem in  space,
then there is a protocol for the \textsc{Matrix Index} instance with  bits.
Hence the lower bound of  transfers to the  problem as well.

Given an instance  of \textsc{Matrix Index},
we construct an instance  of Vertex Cover as follows:
\begin{itemize}
\item For each  introduce three vertices 
\item For each  introduce three vertices 
\item For each  Alice adds the edge  if and only if 
\item Alice then sends this edge set to Bob using  bits
\item Bob knows the index 
\item For each  Bob adds the edges  and 
\item For each  Bob adds the edges  and 
\end{itemize}
}

The next lemma shows that finding the minimum vertex cover of 
allows us to solve the corresponding instance of \textsc{Index}.


\begin{lemma}
The minimum size of a vertex cover of  is  if and only if
.
\label{thm:lower-bound-vc-insertion}
\end{lemma}
\begin{proof}
Suppose .
Then it is easy to check that the set 
forms a vertex cover of size  for .

Now suppose
,
and let  be a minimum vertex cover for .
For any  the vertices  and  have degree one in .
Hence, without loss of generality, we can assume that .
Similarly,  for each .
This covers all edges except .
To cover this we need to pick one of  or ,
which shows that .
\end{proof}
Thus, by checking whether the output of  on the instance
 of  is  or , Bob can determine the index
.
The total communication between Alice and Bob was  bits,
and hence we can solve the \textsc{Index} problem in  bits. Recall that the
lower bound for the \textsc{Index} problem is , and hence we have .
\end{proof}

\begin{corollary}
Let . Any (randomized) PSA that approximates  within a relative error of
 requires  space.
\end{corollary}

\begin{proof}
Choose . Theorem~\ref{thm:lower-bound-vc-insertion}
shows that the relative error is at most , which is less than .
Hence finding an approximation within  relative error amounts to
finding the exact value of the vertex cover. The lower bound of 
from Theorem~\ref{thm:lower-bound-vc-insertion} translates to  here.
\end{proof}




\section{Promised Dynamic Parameterized Streaming Algorithm (PDPSA) for }
\label{sec:promised:dynamic}
In this section we prove Theorem~\ref{thm:vc:max:k} which is restated here.
We let  be a constant so that for the length of stream  we have .

\begin{reptheorem}{thm:vc:max:k}
Assume that at every timestep
the size of the vertex cover of underlying graph  is at most .
There exists a -PDPSA for  problem with probability at least ,
where  and  is a constant.
\end{reptheorem}



\subsection{Outline}
We develop a streaming algorithm that maintains a maximal matching of underlying graph 
in a streaming fashion.
At the end of stream  we run the kernelization algorithm of Section~\ref{sec:kernel:VC}
on the maintained maximal matching.
Our data structure to maintain a maximal matching  of stream  consists of two parts.

First, for each matched vertex , we maintain an -sample recovery sketch
 of its incident edges, where
 is chosen to be .
Insertions of new edges are relatively easy to handle: we update the
matching with the edge if we can, and update the sketches if the new
edge is incident on matched nodes.
The difficulty arises with deletions of edges: we must try to ``patch
up'' the matching, so that it remains maximal, using only the stored
information, which is constrained to be .
The intuition behind our algorithm is that, given the promise, there
cannot be more than  matched nodes at any time.
Therefore, keeping  information about the
neighborhood of each matched node can be sufficient to identify any
adjacent unmatched nodes with which it can be paired if it becomes
unmatched.
However, this intuition requires significant care and case-analysis to
put into practice.
The reason is that we need some extra book-keeping to record where
information is stored, since nodes are entering and leaving the
matching, and we do not necessarily have access to the full
neighborhood of a node when it is admitted to the matching.
Nevertheless, we show that additional book-keeping information of size
 is sufficient for our purposes, allowing us to meet the
 space bound.

This book-keeping comes in the form of another
data structure , that
stores a set of edges  such that both endpoints are matched
(not necessarily to each other),
and  has been inserted into sketches  and , but not deleted from them.
The size of  is clearly .
To implement , we can adopt any fast dictionary data
structure (AVL-tree, red-black tree, or hash-tables).

The update at a time  is either the insertion or the deletion of
an edge  for  where  is
the length of stream .
We continue our outline of the algorithm by describing the behavior
in each case informally, with the formal details spelled out in
subsequent sections.

\paragraph{Insertion of an Edge  at Time .}
When the update at time  is insertion of an edge 
two cases can occur.
The first case is if at least one of  and  is matched,
we insert edge  to the sketches of those vertices (from  and )
which are matched.
If both  and  are matched, we also insert edge 
to .


The second case occurs if both vertices  and  are exposed.
We add edge  to the current matching and to , and initialize sketches 
and  by insertion of edge  to  and .
However, we also need to perform some additional book-keeping updates
to ensure that the information is up to date.
Fix one of the nodes .
There can be matched vertices, say , which are neighbors of .
If previously an edge  arrived while  was not in the
matching, then 
we inserted  to
sketch , but  was not inserted to sketch 
as  was an exposed vertex at that time.
If at some subsequent point  becomes an exposed vertex
and the matching edge  is deleted
then vertex  must have the option of
choosing an unexposed vertex  to be rematched.
For that, we need to ensure that some information about the edge
 is accessible to the algorithm.




A first attempt to address this is to try interrogating each sketch
 for all edges incident on , say when  is first added to
the matching.
However, this may not work while respecting the space bounds:
 may have a large number of neighbors, much larger than the limit
.
In this case, we can only use  to recover a sample of the
neighbors of , and  may not be among them.


To solve this problem we must wait until 
has low enough degree that we can retrieve its complete neighborhood
from .
At this point, we can use these recovered edges to update the sketches
of other matched nodes.
We use the structure  to track information about edges
on matched vertices that are already represented in sketches, to avoid
duplicate representations of an edge.
This is handled during the deletion of an edge, since this is the only
event that can cause the degree of a node  to drop.






\paragraph{Deletion of an Edge  at Time .}
When
the update at time  is deletion of an edge ,
we have three cases to consider.
The first case is if only one of vertices  and  is matched, we
delete edge  from the sketch of that matched vertex.

The second case is if both  and  are matched vertices,
but .  We want to delete edge  from sketches
 and , but  might not be represented in both these sketches.
We need to find out if  has been inserted to  and ,
or only to one of them.
This can be found from .
If , edge  has been inserted to both  and .
So, we delete  from both sketches safely.
Otherwise, i.e., if ,  has been
inserted to the sketch of only one of  and .
Assume that this is .
To discover this we define {\em timestamps} for matched vertices.
The timestamp  of a matched vertex  is the (most recent) time
when  was matched.
We show that edge  is only in sketch  (not )
if and only if  and .
Therefore, if , we delete  from sketch .
Otherwise, i.e., if , we delete  from sketch .
Observe that if , we have inserted  to  and
 as well as .





The third case
is when .
We delete edge  from sketches  and  as well as matching  and .
To maintain the maximality of matching  we need to see whether
we can rematch  and .
Let us consider  (the case for  is identical).
If  has high degree,
we sample edges  from sketch .
Given the size of the sketch, we argue that there is high probability
of finding an edge to rematch .
Meanwhile, if  is has low degree, then we can recover its full
neighborhood, and test whether any of these can match .
Otherwise,
  is an exposed vertex, and its sketch is deleted.
We also remove all edges incident on  from .




\subsection{Notations, Data Structures and Invariants}

We now describe and prove the properties of this process in full.
We begin with notations, data structures and invariants.

\paragraph{Timestamp of a Vertex and an Edge.}
We define \textit{time } corresponding to the -th update
operation (insert or delete of an edge) in stream .
We define the {\em timestamp of a matched vertex} as follows.
Let  be a matched vertex at time .
Let  be the greatest time such that  was unmatched before
time  and is matched in the interval .
Then we say the timestamp  of vertex  is  and we set
.
If at time , vertex  is exposed we define ,
i.e. a value larger than any timestamp.


We define the {\em timestamp of an edge} as follows.
Let  denote the set of edges present at time , i.e. which have
been inserted without a corresponding deletion.
Let  be the last time in which the edge  is inserted
but not deleted in the interval .
Then we say the timestamp  of edge  is  and we
set .
If at timestamp , edge  is deleted we define .




\paragraph{Low and High Degree Vertices.}
Let , for constant  (where, we assume that
).
At time  we say a vertex  is a \textit{high-degree} vertex if ;
otherwise, if , we say  is a \textit{low-degree} vertex.




\paragraph{Data Structures.}
For every matched vertex , i.e., , we maintain an
-sample recovery sketch  of edges incident on .
We also maintain a dictionary data structure 
of size .
We assume the insertion, deletion and query times of
 are all worst-case .
At every time ,  stores edges  for which
vertices  and  are matched at time  (not necessarily
  to each other); and also
edge  is represented in both sketches  and ,
  i.e.
there is a time  at which we  invoked
\textsc{Update}, but there is no time
in interval  in which we have invoked
\textsc{Update}, and symmetrically for .





\paragraph{Sketched Neighbors of a Vertex.}
Let  be a matched vertex at some time , i.e., .
Recall that  is the full neighborhood of  at time .
Let 
be the set of neighbors of  that up to time 
are inserted to  but not deleted from ,
that is for every vertex  we have invoked
\textsc{Update} at a time  but have not invoked
\textsc{Update} in time interval .
We call the vertices in  the sketched neighbors of vertex .
Note that we can recover  {\em exactly} when
, per Definition~\ref{def:ksparse}.



\junk{
\begin{lemma}
\label{lem:low:degree:recover:set}
Let  be a time of stream .
Let  be a low-degree matched vertex at time  that is  and .
Let  be the set of neighbors of  that up to time 
are inserted to sketch  but not deleted from .
By querying  and with probability at least ,
we can recover .
\end{lemma}

\begin{proof}
From Definition \ref{def:l0}, a -sampler returns an element 
with probability  and returns FAIL with
probability at most . Using Lemma \ref{lem:l0:sampling}, there exists
a linear sketch-based algorithm for -sampling using
 bits of space.


Sketch  is a -sample recovery sketch which means
we can recover  of items (here edges) that are
inserted into sketch .  We can think of 
as  instances of a -sampler. Note the in this way the space
to implement  would be  times the space to implement a -sampler
which is  bits of space.



Each one of these  -samplers
returns FAILS with probability at most . Using a union bound the probability
that  returns FAIL is . We replace  by
 for a constant . Therefore, the probability
that sketch  returns FAIL is . Therefore, the overall space
of  would be

as  and .


Let us condition on the event that  does not return FAIL
which happens with probability .
We query  and it returns  edges.
Since  is a low degree vertex (),
we have .
Therefore, we recover all edges incident on 
which are added to sketch  up to time 
but are not deleted from . Thus, we can recover .
\end{proof}
}



\paragraph{Invariants.}
Recall that at every time  of stream , set  is the set of edges which
are inserted but not deleted up to time .
We develop a dynamic algorithm that at every time  of stream 
maintains the following three invariants.

\begin{center}
\shadowbox{
\parbox{\columnwidth} {
\begin{itemize}
\item \textbf{Invariant 1:} For every edge  at time  we have at least one of  or .
\item Let  be an edge at time  such that .
At time ,
     \begin{itemize}
     \item \textbf{Invariant 2:}   {\bf iff }  and .
     \item \textbf{Invariant 3:}    and  {\bf iff } .
     \end{itemize}
\end{itemize}
}}
\end{center}

Observe that these invariants imply that at any time
.
That is, since  only holds edges such that both ends are
matched, and we assume that the matching has at most  nodes, then
the number of edges can be at most .

\junk{
\begin{lemma}
\label{lem:size:tree}
Suppose Invariant  holds at a time .
Then, at time , we have .
\end{lemma}

\begin{proof}
At every time of stream , for the size of a maximal matching  we have .
Thus .
Using Invariant , every edge  at time  is in  if
 and  are both matched. Therefore, 
for .
\end{proof}
}







\subsection{Adding an Edge to Matching }
The first primitive that we develop is Procedure
{\sf AddEdgeToMatching}.
This procedure first adds edge  to matching  and data structure .
Then it inserts vertex  to , sets timestamp  to the current time
, and initializes sketch  by inserting
edge  to sketch .
It also repeats these steps for .
We invoke this procedure in Procedures {\sf Rematch}
and {\sf Insertion}.


\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\raggedright
\underline{\textbf{Insertion}}
\vspace{-0.2cm}
\begin{enumerate}
\item  If  and , then {\sf AddEdgeToMatching}.
\item  Else {\sf InsertToDS)}.
\end{enumerate}
\vspace{-0.2cm}
}}
\end{center}



\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\underline{\textbf{AddEdgeToMatching}}
\begin{enumerate}
\item Add edge  to  and .
\item For  \begin{enumerate}
     \item 
     \item 
\item Initialize sketch  with \textsc{Update}.
    \end{enumerate}
\end{enumerate}
}}
\end{center}




\begin{lemma}
 \label{lem:timestamps:case:1}
Let  be a time when we invoke Procedure {\sf AddEdgeToMatching}.
Suppose before time , Invariants ,  and  hold.
Then, Invariants ,  and  hold after time .
\end{lemma}

\begin{proof}
Recall that  is the last time  such that  before time  was
unmatched and is matched in the interval .
Similarly,  is the last time  such that  before time  was
unmatched and is matched in the interval .

In Procedure {\sf AddEdgeToMatching} we insert  to sketches
 and/or  if the edge has not been inserted to these sketches.
So, at time , Invariant  for edge  holds.
Since , nothing changes for Invariants  and .
Therefore, if Invariants  and  hold at time ,
they also hold at time .
\end{proof}




\subsection{Maintenance of Data Structure }
To maintain data structure  at every time  of stream ,
we develop two procedures to handle insertions and deletions to the
structure.
If  and  are  matched vertices,
Procedure {\sf InsertToDS()} inserts edge 
to sketches  and  as well as to data structure .
If only one of  and  is matched, we insert 
to the sketch of the matched vertex.
We invoke this procedure upon insertion of an arbitrary edge 
inside Procedure {\sf Insertion}.



\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\underline{\textbf{InsertToDS}}
\begin{enumerate}
\item If  and  then
insert edge  into .
\item If  then \textsc{Update}.
\item If  then \textsc{Update}.
\end{enumerate}
}}
\end{center}





\begin{lemma}
 \label{lem:timestamps:case:2}
Let  be a time of stream  when we invoke Procedure {\sf InsertToDS()}.
Suppose before time , Invariants ,  and  hold.
Then, Invariants ,  and  hold after time .
\end{lemma}

\begin{proof}
First assume at time  when we invoke Procedure {\sf InsertToDS()},
vertices  and  are already matched.
In Procedure {\sf InsertToDS()} we insert  to sketches
 and  using \textsc{Update})
and \textsc{Update}).
So,  and  and  Invariant  holds.
Moreover, we insert  to .
Therefore, Invariant  holds.
Invariant  also holds as
neither condition is true ( and ).

Next assume only vertex  is matched.
We insert  to sketch , but not to  and .
Since , Invariant  is correct.
Invariant  and  are correct as  is not matched at time .
The case when only vertex  is matched is symmetric.
\end{proof}




The second procedure is {\sf DeleteFromDS}
which is invoked in Procedure {\sf Deletion} when .
There are three main cases to consider.
If ,
we delete  from sketches  and 
as well as data structure .
If not, we know that   is only in one of  and .

If  and both   and  are matched, we delete the edge from ,
otherwise, if  and  and  are matched from , we delete the edge from .
If none of these cases occur, then only one of  and  is matched.
If the matched vertex is , we delete  from .
Otherwise, we delete  from .



\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\underline{\textbf{Deletion}}
\vspace{-0.2cm}
\raggedright
\begin{enumerate}
\item  If  then invoke {\sf Rematch}
\item  Else invoke {\sf DeleteFromDS}.
\item  Invoke {\sf AnnounceNeighborhood} and {\sf AnnounceNeighborhood}
\end{enumerate}
\vspace{-0.2cm}
}}
\end{center}





\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\underline{\textbf{DeleteFromDS}}
\raggedright
\begin{enumerate}
\item If  then
\begin{enumerate}
   \item \textsc{Update} and \textsc{Update}.
   \item Remove  from .
\end{enumerate}
\item Else if  and  then \textsc{Update}.
\item Else if  and  then \textsc{Update}.
\item Else if  and  then \textsc{Update}.
\item Else if  and  then \textsc{Update}.
\end{enumerate}
}}
\end{center}





\begin{lemma}
\label{lem:timestamps:case:3}
Assume Invariants ,  and  hold at time  when Procedure
{\sf DeleteFromDS} is invoked.
Then, Procedure {\sf DeleteFromDS()} chooses the correct case.
\end{lemma}

\begin{proof}
First, we consider the case that  both  and  are matched vertices.
Since Invariant  holds, we know that edge 
at time  is in  if and only if
 and .
Procedure {\sf DeleteFromDS()} searches for  in .
If this finds  in , we then know that 
and . So, we can safely delete the edge from
sketches  and  and data structure .

On the other hand, if , we ensure that the edge
is in only one of  and .  Now, we can use the claim of Invariant
 which says  if and only if  and .
We compare  and . If , then .
Recall that since Invariant  holds, we know that at least one of 
and  is correct.
Because , we must have .
So deleting edge  from sketch  is the correct operation.
On the other hand, if , then  and so
edge  is only in sketch . Thus, deleting edge  from
sketch  is the correct operation.

Next we consider the case that only one of  and  is matched.
Let us assume  is the matched vertex.
Since Invariant  holds, we know that at least one of 
and  is correct. Because  is the matched vertex and
we maintain the sketch of matched vertices,  has been inserted to sketch
 that is . Therefore,
deleting edge  from sketch  is the correct operation.
The case when  is the matched vertex is symmetric.
\end{proof}





\subsection{Announcement and Deletion of Neighborhood of a Vertex}
In this section we develop basic primitives for the announcement
and deletion of the neighborhood of a vertex.
Announcement is performed by
Procedure {\sf AnnounceNeighborhood} which is
invoked in Procedure {\sf Deletion}.
Suppose that node  has low degree.
For every matched vertex ,
we search for edge  in . If ,
 is in both  and  and no action is needed.
But if not, we insert edge  into tree  as well as
sketch .







\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\underline{\textbf{AnnounceNeighborhood}}
\vspace{-0.1cm}
\begin{enumerate}
\item If  and , then
     \begin{enumerate}
     \item For every edge  in sketch 
           \begin{enumerate}
            \item Add  to set .
           \end{enumerate}
     \item For every 
\begin{enumerate}
               \item 
If edge , then
insert  to ; \textsc{Update}.
               \end{enumerate}
\end{enumerate}
\end{enumerate}
}}
\end{center}





We also introduce a deletion primitive in the form of Procedure {\sf
  DeleteNeighborhood}.  This is
invoked in {\sf Rematch} when
the matched edge  is removed. 
The {\sf DeleteNeighborhood} procedure is called on a node  when 
all the following three conditions hold.
\begin{enumerate}
 \item The matched edge of matched vertex  is deleted.
 \item Vertex  is a low-degree vertex.
 \item Vertex   does not have any exposed neighbor.
\end{enumerate}

In this case, we need to delete  from  and delete incident edges on
 from data structure  as Invariant  for  is not valid
anymore. More precisely, for a low-degree matched vertex whose neighborhood
are all matched we do as follows.

We recover all edges from the sketch  (i.e. ).
For every edge ,
we check to see if .
If so, we know that 
is represented in both sketches  and .
We also delete  from  as  is not
matched and Invariant  does not hold.
But if ,
since Invariant  holds we know that  is inserted only in  not in .
Observe that since  does not have
any exposed neighbor, vertex  must be a matched vertex, and so
vertex  has an associated sketch .
Therefore, in order to fulfill Invariant , we first insert  to sketch .
Finally, we delete the whole sketch ,
and remove  from .



\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\underline{\textbf{DeleteNeighborhood}}
\begin{enumerate}
\item For every edge  in sketch 
           \begin{enumerate}
           \item If edge , then
Remove  from .
\item Else \textsc{Update}.
\end{enumerate}
\item Delete sketch  and remove  from .
\end{enumerate}
}}
\end{center}



\COMMENTED{

\begin{figure*}[t]
\centering
\fbox{
\parbox{6.0in} {
\begin{tabular}{ l | r }
\parbox{2.9in} {
\underline{\textbf{AnnounceNeighborhood}}
\begin{enumerate}
\item If  and , then
     \begin{enumerate}
     \item For every edge  in sketch 
           \begin{enumerate}
            \item Add  to set .
           \end{enumerate}
     \item For every 
           \begin{enumerate}
            \item If edge , then
               \begin{enumerate}
               \item Insert  to .
               \item \textsc{Update}.
               \end{enumerate}
           \end{enumerate}
     \end{enumerate}
\end{enumerate}
}
\hspace{-0.5cm}
&
\parbox{3.1in} {
\vspace{-0.4cm}
\underline{\textbf{DeleteNeighborhood}}
\begin{enumerate}
\item For every edge  in sketch 
           \begin{enumerate}
           \item If edge , then
                    \begin{enumerate}
                    \item Remove  from .
                    \end{enumerate}
           \item Else \textsc{Update}.
\end{enumerate}
\item Delete sketch  and remove  from .
\end{enumerate}
}
\end{tabular}
}}
\end{figure*}

}


\begin{lemma}
\label{lem:timestamps:case:4}
Let  be a time when we invoke Procedure {\sf AnnounceNeighborhood}.
Suppose  is a low-degree matched vertex at time .
Suppose before time , Invariants ,  and  hold.
Then after time , Invariants ,  and  hold.
\end{lemma}

\begin{proof}
Let  be the set of neighbors of  that up to time 
are inserted into sketch  but not deleted from .
Since  at time  is a low-degree vertex we can use
Definition~\ref{def:ksparse} to recover  in its entirety.
We assume Invariants ,  and  hold before time .
We prove that all three invariants continue to hold after invocation of
{\sf AnnounceNeighborhood}.

Fix a matched neighbor  of  in  that is
. In Procedure
{\sf AnnounceNeighborhood} for  we do the following.
If edge  has not been already inserted in ,
we insert edge  to  and .
So, now  and , and
. Invariants ,  and  hold for ,
and continue to hold for all other edges.
\end{proof}

After processing this deletion, edge  is no longer in ,
and so the invariants trivially hold in regard of this edge.
Meanwhile, for any other edge, if the invariants held before, then
they continue to hold, since the changes only affected edge .



\begin{lemma}
\label{lem:timestamps:case:5}
Suppose before time , Invariants ,  and  hold and we
invoke {\sf DeleteNeighborhood} at time .
Here we assume  is a matched vertex whose neighbors are all matched, i.e.,
.
Then after time , Invariants ,  and  hold.
\end{lemma}

\begin{proof}
Let  be an edge in sketch .
Since we assume Invariant  holds before time ,  must be
inserted into at least one of  and . We know edge  is in .
Since Invariants  and  hold, we have one of the two following cases.

(i) If edge  is also inserted to , this means this edge must be in .
In {\sf DeleteNeighborhood} if edge  is in ,
we delete the edge from  as well as sketch .
As  is still in , Invariant  after time  holds.

(ii) Else, edge  is not in . Using Invariant  this happens
if and only if  and .
We want to delete all edges which are inserted to  and delete sketch .
Observe that since  does not have
any exposed neighbor, vertex  must be a matched vertex and so has
an associated sketch .
We insert  to sketch , and
subsequently  is deleted.
Therefore, Invariant  still holds.

Finally, Invariants  and  hold after time  since 
is not a matched vertex anymore.
\end{proof}




\subsection{Rematching Matched Vertices}
In this section we develop the last (and most involved) primitive, {\sf Rematch}.
We invoke this procedure in Procedure {\sf Deletion} when the matched edge
 is deleted. We first delete edge  from sketches  and  as well as data structure .
We also delete  from current set  of matched edges.
To see if we can rematch  and  to one of their exposed neighbors,
we perform the subsequent steps for  (and then repeat for ).

If  is a low degree vertex, by querying  we recover , i.e.,
the set of neighbors of  that up to time 
are inserted into sketch  but not deleted from .
We then check whether there is an exposed vertex
. If so, we rematch  to .

But if there is no exposed vertex in ,
we announce  as an exposed vertex.
We also remove sketch  as  is not a matched vertex anymore. Moreover,
we remove all incident edges on  from  as our third invariant
does not hold anymore. Lemma \ref{lem:low:rematch} shows that in both cases,
the matching after invoking Procedure {\sf Rematch} is maximal
if the matching before this invocation was maximal.

If  is a high degree vertex, it samples an edge  from sketch .
In Lemma \ref{lem:high:rematch} we show that with high probability 
is an exposed vertex, so we rematch  to .
Therefore, if the matching before the invocation of
Procedure {\sf Rematch} is maximal, the matching after this
invocation would be maximal as well.

\begin{center}
\fbox{
\parbox{0.95\columnwidth} {
\raggedright
\underline{\textbf{Rematch}}
\begin{enumerate}
\item {\sf DeleteFromDS}, remove  from , remove
  ,  from 
\item For 
\begin{enumerate}
\item If  then
      \begin{enumerate}
      \item For every edge  in sketch , add  to set .
      \item If there is an exposed   then invoke {\sf AddEdgeToMatching}.
      \item Else invoke {\sf DeleteNeighborhood}.
      \end{enumerate}
\item If  then
      \begin{enumerate}
      \item Query edges  from sketch  for .
      \item If there is an exposed  then invoke {\sf AddEdgeToMatching}.
      \end{enumerate}
\end{enumerate}
\end{enumerate}
}}
\end{center}



\subsubsection{Analyzing Rematching of a Low-Degree Vertex.}

\begin{lemma}
\label{lem:low:rematch}
Let  be a low-degree matched vertex at time .
Assuming the matching  before time  is maximal,
then, after the invocation of Procedure {\sf Rematch}, the matching  is maximal.
The running time of Procedure {\sf Rematch}
when  is a low-degree vertex is .
\end{lemma}

\begin{proof}
Let  be the set of neighbors of  up to time 
that are inserted into sketch  but not deleted from .
From Definition~\ref{def:ksparse},
by querying  and with probability at least
, we can recover .
Observe that assuming Invariants ,  and  hold, we must have
, that is, those neighbors of
 that are not in  at time  must be matched.
Therefore, all exposed neighbors of  must be in .

Two cases can occur. The first is if there is an exposed vertex  in .
Then, Procedure {\sf Rematch} will rematch  using exposed vertex .
The second is when all neighbors of  are already matched. Since all neighbors of 
are matched, vertex  cannot be matched to one of its neighbors and
so we announce  as an exposed vertex and release its sketch .
Therefore, assuming  before time  is maximal,  after time 
would be maximal as well.

We next discuss the running time of Procedure {\sf Rematch}
when  is a low-degree vertex.
By properties of the sketch data structures,
the time to query  sampled edges from sketch  and construct set
 is .
If the second case happens, since we assume at every time of stream , ,
we then have .

Recall that  is a data structure with at most  edges
whose space is . The insertion, deletion and
search times of  are all worst-case .
In the second case, the main cost is to remove incident edges
on  from .
For every neighbor  we search, in time ,
if edge  has been inserted into ; so overall the deletion of incident
edges on  from  is done in time 
as . Overall, the running time of Procedure {\sf Rematch}
when  is a low-degree vertex is , as we set .
\end{proof}





\subsubsection{Analyzing Rematching of a High-Degree Vertex.}

\begin{lemma}
\label{lem:high:rematch}
Let  and .
Let  be a high degree vertex, i.e., .
Suppose we query edges  from sketch .
The probability that there exists an exposed vertex  is
at least .
Further, the running time of Procedure {\sf Rematch}
when  is a high-degree vertex is .
\end{lemma}

\begin{proof}
From Definition \ref{def:l0}, a -sampler returns an element 
with probability  and returns FAIL with
probability at most . Using Lemma \ref{lem:l0:sampling}, there exists
a linear sketch-based algorithm for -sampling using
 bits of space.

Sketch  is a -sample recovery sketch which means
we can recover  items (here, edges) that are
inserted into sketch .
We can think of  as  instances of a -sampler.
Note that in this way the space
to implement  would be  times the space to implement a -sampler
which is  bits of space.
Each one of these  -samplers
returns FAIL with probability at most .
Using a union bound the probability that  returns FAIL is
.
We rescale the failure probability  to 
for a constant .
Therefore, the probability that sketch  returns FAIL is
, and hence the overall space of  is

as  and .

Let  be the edges queried from sketch  for .
Note that the time to query  edges from sketch  is
.
Let us define event  if  does not return FAIL.
Let us condition on event  which happens with probability
.

Fix a returned edge .
Recall that  is the neighborhood
of  that is, .
The number of matched vertices is at most ,
i.e., . Thus,  and
.
The probability that  is a fixed edge  is
.
Using a union bound and since  we obtain



Therefore the probability that  is an exposed vertex, i.e., 
is .

We define an indicator variable  for queried edge  for 
which is one if  and zero otherwise.
Note that . Let .
Then, since  -samplers of  use independent hash functions
we obtain



Therefore, the probability that there exists an exposed vertex 
is .
Overall, the probability that sketch 
does not return FAIL and there exists an exposed vertex
 is

\smallskip
{}
\end{proof}

\begin{lemma}
\label{lem:timestamps:case:6}
Suppose that we invoke {\sf Rematch}, and
 before time , Invariants ,  and  hold,
and matching  is maximal.
Then after time , Invariants ,  and  hold and
matching  is maximal.
The running time of  {\sf Rematch} is .
\end{lemma}

\begin{proof}
First of all, we invoke {\sf AddEdgeToMatching} to add edge 
to matching . In Procedure {\sf AddEdgeToMatching}, we insert the edge to
 as well as  for some .
We also insert  to the sketch of
whichever vertex ( or ) was exposed before time .
So at the end of {\sf AddEdgeToMatching} edge  is in ,  and .

Once we invoke, Procedure {\sf DeleteFromDS}, it deletes edge  from
,  and . We also delete the edge from . So after invocation of
{\sf DeleteFromDS}, Invariants ,  and  hold.
Let us fix vertex . The following proof is the same for vertex .
We consider two cases for .

(i) First,  is a low-degree vertex, i.e., 
assuming Invariants ,  and  hold. Observe that using
Lemma \ref{lem:low:rematch}, after the invocation of Procedure
{\sf Rematch}, matching  is maximal.
Moreover, the running time of {\sf Rematch} when  is
a low-degree vertex is .
Let  be the set of neighbors of  that up to time 
are inserted into sketch  but not deleted from .
By Definition~\ref{def:ksparse},
by querying  and with probability at least , we can recover .
Observe that assuming Invariants ,  and  hold, we must have
.
That is, those neighbors of
 that are not in  at time  must be matched.
Therefore, all exposed neighbors of  must be in .
We have two sub-cases. First, if there is an exposed
 then we invoke {\sf AddEdgeToMatching}.
Lemma \ref{lem:timestamps:case:1} shows that Invariants ,  and  hold
after invocation of {\sf AddEdgeToMatching}.
The second subcase is if there is no exposed node in , we then invoke
{\sf DeleteNeighborhood}.
Lemma \ref{lem:timestamps:case:5} shows that Invariants ,  and  hold
after invocation of {\sf DeleteNeighborhood}.



(ii) Second,  is a high-degree vertex assuming Invariants ,  and  hold.
Observe that using Lemma \ref{lem:high:rematch}, after the invocation of Procedure
{\sf Rematch}, matching  with probability at least 
is maximal and the running time of Procedure {\sf Rematch}
when  is a high-degree vertex is .
Since with probability at least  there exists an exposed vertex
, with this probability we invoke {\sf AddEdgeToMatching}.
Lemma \ref{lem:timestamps:case:1} then shows that Invariants ,  and  hold
after invocation of {\sf AddEdgeToMatching}.
\end{proof}



\subsection{Completing the Proof of Theorem \ref{thm:vc:max:k}}

First we prove the claim for the space complexity of our algorithm.
We maintain at most  sketches (for matched vertices), each one is
an -sample recovery sketch for .
From Definition~\ref{def:ksparse} and the proof of Lemma
\ref{lem:timestamps:case:6}, the space to maintain an
-sample recovery sketch is . So, we need
 bits of space to maintain the sketches of matched vertices.
The size of data structure , i.e.,
the number of edges stored in  is .
Thus, overall the space complexity of our algorithm is  bits.

Next we prove the update time and query time of our dynamic algorithm
for maximal matching is . In fact, the deletion or the insertion
time of an edge  is dominated by the running time of most expensive
procedures which are {\sf AnnounceNeighborhood}, {\sf DeleteNeighborhood},
and {\sf Rematch}. The running times of these procedures are also
dominated by the time to query at most  edges from sketches  and  plus
the time to search for  edges in data structure .

The time to query at most  edges from sketches  and  using
Lemma \ref{lem:timestamps:case:6} is . The time to search
for  edges in data structure  is  as we assume
the insertion, deletion and query times of  are all worst-case .
Therefore, the update time and query time of our dynamic algorithm
for maximal matching is .

Finally, we give the correctness proof of Theorem \ref{thm:vc:max:k}.
Observe that since after every time  of stream , Invariants  and  hold, and hence the matching 
is maximal. In fact, since Invariant  holds,  for every edge 
we have at least one of  or 
which means  is maximal.
Recall that  is the set of vertices of matched edges in .
Note that for every matched vertex ,
we maintain an -sample recovery sketch .

Next, similar to the algorithm of Theorem~\ref{THM:VC:INSERTION} (Section \ref{sec:psa:vc}) we construct a graph .
For every matched vertex , we extract up to  edges incident on  from
sketch  and store them in set . At the end,
we run the kernelization algorithm of Section~\ref{sec:kernel:VC}
on instance . The rest of proof of correctness of
Theorem~\ref{thm:vc:max:k} requires showing that maintaining a maximal
matching is sufficient to obtain a kernel for vertex cover, which is what was exactly argued in proof of Theorem~\ref{THM:VC:INSERTION}.






\section{Dynamic Parameterized Streaming Algorithm (DPSA for  }
\label{app:vc:dpsa}

In this section we prove Theorem~\ref{THM:VC:DPSA}, which is restated below:

\begin{reptheorem}{THM:VC:DPSA}
Let  be a dynamic parameterized stream of  insertions and deletions of edges of an underlying graph .
There exists a randomized -DPSA for  problem.
\end{reptheorem}
\begin{proof}
Let  be a stream of insertions and deletions of edges to an
underlying graph .
We maintain a
-sample recovery algorithm (Definition~\ref{def:ksparse}), which
processes all the edges seen in the stream; we also keep a counter
to record the degree of the vertex.
At the end of the stream , we recover a graph  by extracting
the at most  edges from the recovery algorithm data structure, or
outputting ``NO'' if there are more than  edges currently in the
graph.
We then run the kernelization algorithm of Section~\ref{sec:kernel:VC}
on instance .

Observe that if a graph has a vertex cover of
size at most , then there can be at most  edges.
Each node in the cover has degree at most , and every node must
either be in the cover, or be adjacent to a node in the cover.
Therefore, if the graph has more than  edges, it cannot have a
vertex cover of size .
We take advantage of this fact to bound the overall cost of the
algorithm in the dynamic case.
We maintain a structure which allows us to recover at most  edges
from the input graph, along with a counter for the current number of
``live'' edges.
This can be implemented using a -sample recovery algorithm
(Definition~\ref{def:ksparse}), or indeed by a deterministic algorithm
(e.g. Reed-Solomon syndromes).

The algorithm now proceeds follows.
To test for a vertex cover of size , we first test whether the
number of edges is above : if so, there can be no such cover, and
we can immediately reject.
Otherwise, we can recover the full graph, and pass the graph to the
standard kernelization algorithm (Section~\ref{sec:kernel:VC}).
The total time for this algorithm is then , and the
space used is that to store the -sample recovery algorithm, which
is .

This assumes that each edge is inserted at most once, i.e. the same
edge is not inserted multiple times without intervening deletion.
This assumption can be removed, if we replace the edge counter with a
data structure which counts the (approximate) number of distinct edges
currently in the data structure.
This can provide a constant factor approximation with polylogarithmic
space.
This is sufficient to determine if the number of edges is greater than
, and if not, to recover the at most (say)  edges in
the graph from the data structure storing the edges, and apply the kernelization algorithm of Section~\ref{sec:kernel:VC}.
\end{proof}

\junk{
\paragraph{[Alternate] Complexity of the algorithm.}
The space complexity of the algorithm is : for each
node, we keep a -sample recovery algorithm, which requires space
.
Tracking the degree of each vertex consumes space  in total.
Updating the -sample algorithm takes time  per
update, be it insertion or deletion of an edge.
Querying each -sample algorithm takes time linear in its size,
 overall.
Thus, the cost of the query operation is dominated by the time to run
the kernel algorithm, which in turn is dominated by the brute-force
search step, which takes .

\paragraph{[Alternate] Correctness.}
First, we argue that each -sample algorithm successfully recovers
(up to)  true edges incident on each node, i.e., no instance fails
to recover, reports fewer than  true nodes adjacent to ,
and none reports false positive neighbors.
For a single instance, the probability of such failures can be made
polynomially small within the  space bounds, so summing
these up, we obtain small failure probability overall via the union
bound.
Thus we assume that these algorithms all perform correctly, except
with this small probability.

Next, we argue that on the recovered graph , the kernelization
algorithm performs correctly: either it finds a vertex cover for  of size
at most , or correctly reports that no such cover exists.
We proceed by arguing that the kernelization algorithm described in
Section~\ref{sec:kernel:VC} has access to the same information from
 as it would for , and therefore can follow a corresponding
execution path.
Consequently, the outcome must be the same.

We consider each of the kernelization rules in turn.
If (1) applies, there is a vertex  with .
This must be a vertex whose degree in  was also more than .
Accordingly, we remove  and all edge incident on  from , and
decrease  by one.
We call this a ``node removal''.
Note that there may well be edges present in  incident on  which
are not present in .
However, their absence in  does not affect the execution of the
algorithm, and they are effectively removed following the selection of
 to the vertex cover.

If there is an isolated vertex  (and hence is removed by rule (2)), then it would also be isolated
in  following the sequence of applications of rule (1).
This is since each application of rule (1) decreases the degree of any
other vertex by at most 1, and we apply rule (1) at most  times.
The only reason  could become isolated in  but not in  after
following the same set of node removals under rule (1) on both graphs
is if  had degree  in  but degree  in , and was
subject to more than  removals of adjacent nodes.
However, observe that if , then , i.e. the
degree of  in  is preserved, since all incident edges are
recovered correctly by the -sample algorithm.
Hence, we only have  if , and so  cannot
reach 0, since there cannot be this many node removals.

If we can apply rule (3), then the modified  has more than 
edges.  Since the edges of  are a subset of the edges of , then
necessarily, there can be no vertex cover of  of size at most
.
This then leaves us with the case that no rules apply, and we have
reached a kernel of   with at most  vertices and at most  edges.
We now observe that we must have the full connectivity information on
this set of at most  vertices.
Let  be the set of removed nodes, and let  be the
graph  with  (and all edges incident on nodes in ) removed.

Suppose that some edge  is present in  but
absent in .
This can only occur if the degree of both  and  in  was high:
if , then the edge  would have been captured by  in its
-sample algorithm; and similarly for .
However, if it was the case that , then we could apply rule
(1) on .
Note that if initially , then it is the case that after each
application of (1), it remains that , since the degree of
 in the modified graph decreases by at most 1, and  decreases
by 1 in each application of (1).
So if  survives (is not covered by another node), then it can still
be picked by rule (1), contradicting the assumption that no rule can
be applied.
Therefore, if we reach the state that no rule can be applied, then we
must have the full adjacency information of the remaining kernel,
i.e. , and so we reach the same
conclusion when applying the exhaustive search.
Thus we correctly determine the solution to the  instance from
.
}



\section{Concluding Remarks}

By combining techniques of kernelization with randomized sketch structures, we have initiated the study of \emph{parameterized streaming algorithms}. We considered the widely-studied Vertex Cover problem, and obtained results in three models: insertion only stream, dynamic stream and promised dynamic stream.
There are several natural directions for further study. We mention some of the below.

\paragraph{Dynamic Algorithms.} Recent work has uncovered connections between streaming algorithms and
dynamic algorithms~\cite{KKM13}.
It is natural to ask whether we can make the algorithms provided
dynamic: that is, ensure that after each step they provide (implicitly
or explicitly) a current answer to the desired problem.
The current algorithm for maximal matching sometimes takes time
polynomial in  to process an update: can this be made sublinear in
?

Our main algorithm in Section~\ref{sec:promised:dynamic} applies in the case where
there is a promise on the size of the maximal matching.
Can this requirement be relaxed?
That is, is there a dynamic algorithm that will succeed in finding a
maximal matching of size  at time , even if some intermediate
maximal matching has exceeded this bound?  Or can the cost be made
proportional to the largest maximal matching encountered, i.e. remove
the requirement for  to be specified at the time, and allow the
algorithm to adapt to the input instance.

\paragraph{Other Problems.} In this paper, we only considered the
Vertex Cover problem. We think it is interesting to consider other
NP-hard problems in the framework of parameterized streaming, and that
kernelization algorithms can be helpful in this endeavour. In some
cases, one might be able to obtain parameterized streaming algorithms
with simple observations. For example, in the Feedback Vertex Set
() problem, we are given a graph  and an integer
. The question is whether there exists a set  such
that  has no cycles.
We can show the following results (proved in the appendix)
for :

\begin{theorem}
\label{thm:fvs:upper}
There is a deterministic PSA for  which uses  space.
\end{theorem}

\begin{theorem}
\label{thm:fvs:lower}
Any (randomized) PSA for  requires  space.
\end{theorem}



\paragraph{Acknowledgments.}
The third author would like to thank Marek Cygan for fruitful discussion
on early  stages of this project in a Dagstuhl workshop.
We thank Catalin Tiseanu for some useful discussions regarding the
Feedback Vertex Set problem.



\newcommand{\Proc}{Proceedings of the~}

\newcommand{\STOC}{Annual ACM Symposium on Theory of Computing (STOC)}
\newcommand{\FOCS}{IEEE Symposium on Foundations of Computer Science (FOCS)}
\newcommand{\SODA}{Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)}
\newcommand{\SOCG}{Annual Symposium on Computational Geometry (SoCG)}
\newcommand{\ICALP}{Annual International Colloquium on Automata, Languages and Programming (ICALP)}
\newcommand{\ESA}{Annual European Symposium on Algorithms (ESA)}
\newcommand{\CCC}{Annual IEEE Conference on Computational Complexity (CCC)}
\newcommand{\RANDOM}{International Workshop on Randomization and Approximation Techniques in Computer Science (RANDOM)}
\newcommand{\PODS}{ACM SIGMOD Symposium on Principles of Database Systems (PODS)}
\newcommand{\SSDBM}{ International Conference on Scientific and Statistical Database Management (SSDBM)}
\newcommand{\ALENEX}{Workshop on Algorithm Engineering and Experiments (ALENEX)}
\newcommand{\BEATCS}{Bulletin of the European Association for Theoretical Computer Science (BEATCS)}
\newcommand{\CCCG}{Canadian Conference on Computational Geometry (CCCG)}
\newcommand{\CIAC}{Italian Conference on Algorithms and Complexity (CIAC)}
\newcommand{\COCOON}{Annual International Computing Combinatorics Conference (COCOON)}
\newcommand{\COLT}{Annual Conference on Learning Theory (COLT)}
\newcommand{\COMPGEOM}{Annual ACM Symposium on Computational Geometry}
\newcommand{\DCGEOM}{Discrete \& Computational Geometry}
\newcommand{\DISC}{International Symposium on Distributed Computing (DISC)}
\newcommand{\ECCC}{Electronic Colloquium on Computational Complexity (ECCC)}
\newcommand{\FSTTCS}{Foundations of Software Technology and Theoretical Computer Science (FSTTCS)}
\newcommand{\ICCCN}{IEEE International Conference on Computer Communications and Networks (ICCCN)}
\newcommand{\ICDCS}{International Conference on Distributed Computing Systems (ICDCS)}
\newcommand{\VLDB}{ International Conference on Very Large Data Bases (VLDB)}
\newcommand{\IJCGA}{International Journal of Computational Geometry and Applications}
\newcommand{\INFOCOM}{IEEE INFOCOM}
\newcommand{\IPCO}{International Integer Programming and Combinatorial Optimization Conference (IPCO)}
\newcommand{\ISAAC}{International Symposium on Algorithms and Computation (ISAAC)}
\newcommand{\ISTCS}{Israel Symposium on Theory of Computing and Systems (ISTCS)}
\newcommand{\JACM}{Journal of the ACM}
\newcommand{\LNCS}{Lecture Notes in Computer Science}
\newcommand{\RSA}{Random Structures and Algorithms}
\newcommand{\SPAA}{Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA)}
\newcommand{\STACS}{Annual Symposium on Theoretical Aspects of Computer Science (STACS)}
\newcommand{\SWAT}{Scandinavian Workshop on Algorithm Theory (SWAT)}
\newcommand{\TALG}{ACM Transactions on Algorithms}
\newcommand{\UAI}{Conference on Uncertainty in Artificial Intelligence (UAI)}
\newcommand{\WADS}{Workshop on Algorithms and Data Structures (WADS)}
\newcommand{\SICOMP}{SIAM Journal on Computing}
\newcommand{\JCSS}{Journal of Computer and System Sciences}
\newcommand{\JASIS}{Journal of the American society for information science}
\newcommand{\PMS}{ Philosophical Magazine Series}
\newcommand{\ML}{Machine Learning}
\newcommand{\DCG}{Discrete and Computational Geometry}
\newcommand{\TODS}{ACM Transactions on Database Systems (TODS)}
\newcommand{\PHREV}{Physical Review E}
\newcommand{\NATS}{National Academy of Sciences}
\newcommand{\MPHy}{Reviews of Modern Physics}
\newcommand{\NRG}{Nature Reviews : Genetics}
\newcommand{\BullAMS}{Bulletin (New Series) of the American Mathematical Society}
\newcommand{\AMSM}{The American Mathematical Monthly}
\newcommand{\JAM}{SIAM Journal on Applied Mathematics}
\newcommand{\JDM}{SIAM Journal of  Discrete Math}
\newcommand{\JASM}{Journal of the American Statistical Association}
\newcommand{\AMS}{Annals of Mathematical Statistics}
\newcommand{\JALG}{Journal of Algorithms}
\newcommand{\TIT}{IEEE Transactions on Information Theory}
\newcommand{\CM}{Contemporary Mathematics}
\newcommand{\JC}{Journal of Complexity}
\newcommand{\TSE}{IEEE Transactions on Software Engineering}
\newcommand{\TNDE}{IEEE Transactions on Knowledge and Data Engineering}
\newcommand{\JIC}{Journal Information and Computation}
\newcommand{\ToC}{Theory of Computing}
\newcommand{\MST}{Mathematical Systems Theory}
\newcommand{\Com}{Combinatorica}
\newcommand{\NC}{Neural Computation}
\newcommand{\TAP}{The Annals of Probability}
\newcommand{\TCS}{Theoretical Computer Science}


\bibliographystyle{plain}
\bibliography{gem}


\appendix
















\section{Feedback Vertex Set}
\label{app:FVS}
In this section, we prove Theorem~\ref{thm:fvs:upper} and Theorem~\ref{thm:fvs:lower}












\subsection{Parameterized Streaming Algorithm (PSA) for }

We restate and prove Theorem~\ref{thm:fvs:upper} below:

\begin{reptheorem}{thm:fvs:upper}
There is a \emph{deterministic} PSA for  which uses  space.
\end{reptheorem}



\begin{proof}
To prove Theorem~\ref{thm:fvs:upper}, we use the following lemma bounds the number of edges of a graph with small feedback vertex set.

\begin{lemma}
Any graph with a feedback vertex set of size at most  can have at most  edges, where  is the number of vertices of the graph.
\end{lemma}
\begin{proof}
Let the graph be  and  be the feedback vertex
set of size at most . Then the graph  is a forest,
and hence has at most  edges. Now each of the vertices in 
is adjacent to at most  vertices in . Hence the total number
of edges of  is at most  since .
\end{proof}

\noindent The PSA algorithm for  runs as follows:
\begin{itemize}
\item Store all the edges that appear in the stream.
\item If the number of edges exceeds , output NO.
\item Otherwise the total number of edges (and hence the space complexity) is . Now that we have stored the entire graph, use any one of the various known FPT algorithms~\cite{DBLP:conf/swat/CaoCL10, DBLP:journals/corr/KociumakaP13} to solve the  problem.
\end{itemize}
This concludes the proof of Theorem~\ref{thm:fvs:upper}.
\end{proof}


\subsection{ Lower Bound for }

We restate and prove Theorem~\ref{thm:fvs:lower} below:

\begin{reptheorem}{thm:fvs:lower}
Any (randomized) PSA for  requires  space.
\end{reptheorem}


\begin{proof}
We show the proof by reduction to the {\textsc Disjointness} problem
in communication complexity.

\begin{center}
\noindent\framebox{\begin{minipage}{0.95\columnwidth}
\textsc{Disjointness}\\
\emph{Input}: Alice has a string  given by
.\\ Bob has a string .\\
\emph{Question}: Bob wants to check if .
\end{minipage}}
\end{center}

There is a lower bound of  bits of communication between
Alice and Bob, even allowing randomization~\cite{nisan}.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{cycle}
\caption{Gadget for reduction to {\textsc Disjointness}}
\label{fig:disj}
\end{figure}

Given an instance of {\textsc Disjointness}, we create a graph on 
nodes as follows.
We create nodes , and insert edges
 for all .
We also create edges  for .
This is illustrated in the first graph in Figure~\ref{fig:disj}.

For each , we add 2 edges corresponding to , and two according
to .
If , we add  and ; else we add  and .
If , we add  and ; else we add  and .

We now observe that the resulting graph is a tree (in fact it is a
path) if the two strings are disjoint, but it has at least one cycle
if there is any  such that .
This can be seen by inspecting Figure~\ref{fig:disj}, which shows the
configuration for each possibility for  and .
Thus, any streaming algorithm that can determine whether a graph
stream is cycle-free or has one (or more) cycles implies a
communication protocol for {\sc Disjointness}, and hence requires
 space.

Since  must, in the extreme case , determine whether 
is acyclic, then  space is required for this problem also.
This generalizes to any constant 
by simply adding  triangles
on  new nodes to the graph: one node from each must be removed,
leaving the question whether the original graph is acyclic.
\junk{
First we give a lower bound for a relative of the {\textsc Index}
problem that we refer to as the \textsc{Index Same} problem.
\begin{center}
\noindent\framebox{\begin{minipage}{0.95\columnwidth}
\textsc{Index Same}\\
\emph{Input}: Alice has a string  given by .\\ Bob has an index \\
\emph{Question}: Bob wants to check if .
\end{minipage}}
\end{center}

\begin{lemma}
Any (randomized) one-way communication protocol between Alice and Bob for the \textsc{Index Same} problem requires  bits.\label{thm:lower-index-same}
\end{lemma}
\begin{proof}
We give a reduction from the \textsc{Index} problem. Consider an instance  of the \textsc{Index} problem, where  and . We now build a string  by setting  and  if  and  otherwise. By the construction of the string , it follows that  if and only if . Hence we can use the \textsc{Index Same} problem to solve the \textsc{Index} problem, and hence the lower bound carries over to the \textsc{Index Same} problem as well.
\end{proof}

The next lemma shows that if there is a single pass streaming
algorithm, say , which solves the problem of checking
whether a given forest on  vertices is a tree
in  space, then there is a protocol for the \textsc{Index Same} instance with  bits. Hence the lower bound of  transfers to the problem of checking whether a given forest is a tree.

\begin{lemma}
Consider a graph  with  edges on  nodes. Then any (randomized) algorithm to determine whether  is a tree requires  space. \label{thm:lower-bound-checking-tree}
\end{lemma}
\begin{proof}
Consider an instance  of the \textsc{Index Same} problem, where  and . We now build a graph as follows:
\begin{itemize}
\item For each , add a vertex 
\item Add two special vertices ZERO and ONE.
\item For each , if  then Alice adds the edge ; otherwise if  then Alice adds the edge .
\end{itemize}
The graph now has  edges and  vertices. Alice sends this edge
set to Bob using  bits.
Bob, holding the desired index , now adds the edge 
Now we have  edges and  vertices. It is easy to see that the
constructed graph is connected (and hence a tree) if and only if
, i.e., Bob can check solve the \textsc{Index Same}
problem by checking whether the output of the algorithm 
indicates if  is a tree.
The total communication between Alice and Bob was  bits, and hence we can solve the \textsc{Matrix Index} problem in  bits. This implies .
\end{proof}

Finally, we are now ready to prove Theorem~\ref{thm:fvs:lower}.
Let  be any PSA for . If a connected graph  is
given as an input to , then deciding if  is acyclic
(in the extreme case when ),
i.e., a tree, needs  space from Lemma~\ref{thm:lower-bound-checking-tree}. Hence, any PSA for  needs  space.
}
\end{proof}





\end{document}
