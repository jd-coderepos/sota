\documentclass[sigconf]{acmart}
\acmSubmissionID{578}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\usepackage{environ}
\ExplSyntaxOn
\seq_new:N \g_appendices_seq\NewEnviron{Appendix}{\seq_gput_right:No \g_appendices_seq \BODY}
\newcommand\AddAppendices{\appendix \seq_map_inline:Nn \g_appendices_seq {##1}
}
\ExplSyntaxOff

\usepackage{etoolbox}
\AtEndDocument{\AddAppendices}

\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\setcopyright{acmcopyright}\acmConference[MM '21]{Proceedings of the 29th ACM International Conference on Multimedia}{October 20--24, 2021}{Virtual Event, China}
\acmBooktitle{Proceedings of the 29th ACM International Conference on Multimedia (MM '21), October 20--24, 2021, Virtual Event, China}
\acmPrice{15.00}
\acmDOI{10.1145/3474085.3475275}
\acmISBN{978-1-4503-8651-7/21/10}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{enumitem}

\newif\ifisfinal
\isfinaltrue

\usepackage{multirow}
\newcommand{\YH}[1]{\unless\ifisfinal\textcolor{violet}{[{\bf YH:} {\uwave{#1}}]}\fi}
\newcommand{\LSS}[1]{\unless\ifisfinal\textcolor{blue}{[{\bf LSS:} {\uwave{#1}}]}\fi}
\newcommand{\YS}[1]{\unless\ifisfinal\textcolor{red}{[{\bf YS:} {\uwave{#1}}]}\fi}
\newcommand{\TODO}[1]{\unless\ifisfinal\textcolor{purple}{[{\bf TODO:} {\dashuline{#1}}]}\fi}

\newcommand\NetName{UniCon}

\settopmatter{printacmref=true}
\begin{document}
\fancyhead{}

\title{\NetName{}: Unified Context Network for Robust Active Speaker Detection}

\author{Yuanhang Zhang}
\ifisfinal
\authornote{Both authors contributed equally to this research.}
\fi
\affiliation{\institution{Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS)}
	\city{Beijing}
	\postcode{100190}
	\country{China}
}
\email{zhangyuanhang15@mails.ucas.ac.cn}
\ifisfinal
\additionalaffiliation{\institution{School of Computer Science and Technology, University of Chinese Academy of Sciences}
	\city{Beijing}
	\postcode{100049}
	\country{China}
}
\fi

\author{Susan Liang}
\ifisfinal
\authornotemark[1]
\authornotemark[2]
\fi
\affiliation{\institution{Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS}
	\city{Beijing}
	\postcode{100190}
	\country{China}
}
\email{liangsusan18@mails.ucas.ac.cn}

\author{Shuang Yang}
\ifisfinal
\authornotemark[2]
\fi
\affiliation{\institution{Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS}
	\city{Beijing}
	\postcode{100190}
	\country{China}
}
\email{shuang.yang@ict.ac.cn}

\author{Xiao Liu}
\affiliation{\institution{Tomorrow Advancing Life}
	\city{Beijing}
	\country{China}
}
\email{ender.liux@gmail.com}

\author{Zhongqin Wu}
\affiliation{\institution{Tomorrow Advancing Life}
	\city{Beijing}
	\country{China}
}
\email{30388514@qq.com}

\author{Shiguang Shan}
\ifisfinal
\authornotemark[2]
\fi
\affiliation{\institution{Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS}
	\city{Beijing}
	\postcode{100190}
	\country{China}
}
\email{sgshan@ict.ac.cn}

\author{Xilin Chen}
\ifisfinal
\authornotemark[2]
\fi
\affiliation{\institution{Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS}
	\city{Beijing}
	\postcode{100190}
	\country{China}
}
\email{xlchen@ict.ac.cn}

\renewcommand{\shortauthors}{Zhang and Liang, et al.}

\begin{abstract}
	We introduce a new efficient framework, the Unified Context Network (\NetName{}), for robust active speaker detection (ASD).
Traditional methods for ASD usually operate on each candidate's pre-cropped face track separately and do not sufficiently consider the relationships among the candidates. This potentially limits performance, especially in challenging scenarios with low-resolution faces, multiple candidates, etc. 
	Our solution is a novel, unified framework that focuses on jointly modeling multiple types of \textit{contextual} information:
spatial context to indicate the position and scale of each candidate's face, relational context to capture the visual relationships among the candidates and contrast audio-visual affinities with each other, and temporal context to aggregate long-term information and smooth out local uncertainties. 
Based on such information, our model optimizes all candidates in a unified process for robust and reliable ASD.
A thorough ablation study is performed on several challenging ASD benchmarks under different settings. In particular, our method outperforms the state-of-the-art by a large margin of about \% mean Average Precision (mAP) absolute on two challenging subsets: one with three candidate speakers, and the other with faces smaller than  pixels. Together, our \NetName{} achieves \% mAP on the AVA-ActiveSpeaker validation set, surpassing \% for the first time on this challenging dataset at the time of submission. Project website: \url{https://unicon-asd.github.io/}.
\end{abstract} 
\begin{CCSXML}
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010147.10010178.10010224.10010225.10010228</concept_id>
	<concept_desc>Computing methodologies~Activity recognition and understanding</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Activity recognition and understanding}

\keywords{active speaker detection, audio-visual, speech, computer vision}

\maketitle

\section{Introduction}
\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figures/main_teaser.pdf}
	\vskip-2ex
	\caption{\textit{What marks an active speaker out from others?} Admittedly, face motion and its synchrony with the audio are the most obvious clues; however, as shown in the figure above, the underlying signals can be highly ambiguous, especially in hard scenarios with poorly lit, low-resolution faces and noisy acoustics, etc. We observe that three additional types of \textit{contextual} clues can help identify the active speaker: (a) higher correspondence between his/her face track and the audio relative to other candidates, (b) a form of contrast with the remaining candidates in terms of visual saliency, the amount of attention he/she receives from others, etc. and (c) temporal continuity. In this paper, we propose a novel Unified Context Network (\NetName{}) which models such context information to jointly optimize all candidates for robust active speaker detection.}
	\label{fig:teaser}
	\vskip-2ex
\end{figure*}
 Active Speaker Detection (ASD) refers to the task of identifying when each visible person is speaking in a video, typically through careful joint analysis of face motion and voices. It has a wide range of modern practical applications, such as video re-targeting~\cite{DBLP:conf/icassp/CutlerMJZKWK20}, speech diarization~\cite{DBLP:conf/interspeech/ChungHNAZ20}, automatic video indexing, human-robot interaction~\cite{7029977}, speech enhancement, and the automatic curation of large-scale audio-visual speech datasets~\cite{DBLP:journals/tog/EphratMLDWHFR18,DBLP:conf/interspeech/ChungNZ18,DBLP:conf/icassp/FanKLLCCZZCW20}. 

Although research in this direction dates back almost two decades \cite{DBLP:conf/icmcs/CutlerD00,DBLP:conf/nips/SlaneyC00}, most early work only experiment with simple, short sequences depicting frontal faces, which is a major simplification of practical settings. Recent developments in deep learning for audio-visual tasks have led to renewed interest in the ASD problem~\cite{DBLP:conf/accv/ChungZ16a,DBLP:conf/icassp/HooverCPSS18}. In 2020, Roth \textit{et al.}~\cite{DBLP:conf/icassp/RothCKMGKRSSXP20} introduced AVA-ActiveSpeaker, the first large-scale, challenging ASD benchmark extracted from diverse YouTube movies, which significantly boosted subsequent research. Several effective methods have been proposed, building upon 2D and 3D convolutional neural networks (CNNs) and recurrent neural networks (RNNs)~\cite{DBLP:conf/icassp/RothCKMGKRSSXP20,DBLP:journals/corr/abs-1906-10555,zhangmulti2019,DBLP:conf/cvpr/AlcazarCMPLAG20}. 
Among them, most closely related to our work is the Active Speaker Context (ASC) model proposed in 2020~\cite{DBLP:conf/cvpr/AlcazarCMPLAG20}, which combines cropped face tracks from all candidates with the audio using a self-attention module~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} to model relationships among the candidates. However, relationships learned in this manner are weak, and lack clear interpretation. In addition, like most existing methods, the ASC model also focuses on optimizing each candidate separately. As a consequence, in complex scenarios with multiple candidates, low-resolution faces, occlusion, noisy acoustics etc., current ASD models often lack robustness and cannot yield satisfactory results. 

In this paper, we present a fundamentally different approach where we jointly model multiple sources of \textit{contextual} information and simultaneously optimize all candidates in the scene in a unified process for robust ASD. 
As shown in Fig. \ref{fig:teaser}, we observe that when face motion and audio signals are ambiguous, three other types of contextual clues often help mark the true active speaker out from others: 
(a) \textit{higher} correspondence between his/her face track and the audio relative to others, e.g. hearing a male's voice helps eliminate female candidates; 
(b) a certain form of contrast with the remaining candidates in terms of his/her visual saliency, the amount of attention he/she received from others, etc., for instance, one person speaks while the rest look at him/her; 
and (c) temporal continuity, \textit{i.e.} by observing each candidate's behavior in a temporal neighborhood, we can smooth out erroneous predictions that arise from instantaneous frame-level decisions.

In light of these observations, we propose a novel framework that seeks to resolve local ambiguities and achieve robust ASD by jointly modeling different kinds of contextual evidence. 
Specifically, we introduce face position and scale information as global \textit{spatial context} to implicitly reflect the visual saliency and the visual focus of attention (VFoA) of each candidate in the scene.
Building upon this information, we then construct a powerful and efficient multi-speaker \textit{relational context} where each candidate is contrasted with others from both visual and audio-visual perspectives. 
Finally, we integrate \textit{temporal context} into the previous relational context to aggregate long-term information and remove noises in frame-level predictions. 
By incorporating spatial, relational, and temporal context into a unified framework, our method can jointly optimize all candidates in the scene end-to-end. In the experiments, we show that our method significantly improves ASD performance, especially in challenging scenarios.

In summary, the main contributions of our work are:
\begin{itemize}[topsep=2pt]
	\item We propose a well-motivated, compact framework, the Unified Context Network (\NetName{}) for robust ASD, which unifies spatial, relational, and temporal context to jointly optimize all candidates in the scene.
	\item We highlight the use of face position and scale in constructing each candidate's global spatial context, and explore the modeling of the relationships among the candidates.
	\item We conduct thorough ablation studies that prove the effectiveness of our method, particularly under challenging multi-speaker and low-resolution settings. Moreover, we outperform the current state-of-the-art by a large margin on challenging ASD benchmarks.
\end{itemize}
\vspace{-2ex} 
\section{Related Work}
\paragraph{Active speaker detection (ASD)} Current approaches for ASD can be summarized into two paradigms: unsupervised and supervised. \textit{Unsupervised} methods usually operate under one of two assumptions: (a) the active speaker's face is the one with the highest correspondence between the audio and its visual information, or (b) the active speaker's face is the one that co-occurs most frequently with the person's idiosyncratic voice. Under assumption (a), one typical practice is to learn an audio-visual two-stream network using a contrastive loss between the two modalities~\cite{DBLP:conf/accv/ChungZ16a}. During training, pairs of temporally aligned audio and visual signals are pulled close, while misaligned ones are pushed apart. During inference, the active speaker is naturally the one with lowest distance between audio and visual features. Recently, the method has been improved by better training objective designs, such as multi-way classification~\cite{DBLP:conf/icassp/ChungCK19} and multinomial loss~\cite{DBLP:conf/icassp/DingXZCW20}. Methods based on assumption (b) require strong face and voice embeddings for robust clustering and matching~\cite{DBLP:conf/mm/BredinG16,DBLP:conf/icassp/HooverCPSS18}. For example, Hoover \textit{et al.} \cite{DBLP:conf/icassp/HooverCPSS18} first detect speech segments and face tracks, and cluster them independently using pre-trained speech embeddings and  FaceNet~\cite{DBLP:conf/cvpr/SchroffKP15}. They then apply bipartite matching to assign each speech cluster to the face track cluster with maximum temporal overlap. However, unsupervised methods are not robust enough when the assumptions are not met or clustering results are noisy. \textit{Supervised} methods usually formulate ASD as a binary classification problem, and directly learn a frame-level binary classifier on fused audio-visual features \cite{DBLP:conf/icassp/RothCKMGKRSSXP20,DBLP:journals/corr/abs-1906-10555}. Some works~\cite{zhangmulti2019,DBLP:conf/cvpr/HuangK20} apply multi-task learning to boost performance, by introducing an additional contrastive loss on audio and visual features. An interesting recent work~\cite{DBLP:conf/cvpr/AlcazarCMPLAG20} constructs an Active Speaker Context ensemble to improve local predictions by querying adjacent time intervals and other speakers in the scene. Our work is a new and fundamentally different attempt in this direction, which proposes a methodical process to better incorporate different types of meaningful context information. We leverage such information in a unified framework to optimize all candidates jointly rather than independently for more robust ASD.
\vspace{-1ex}
\paragraph{Context-aware action recognition and detection} There has been a large body of work that focuses on how to integrate \textit{contextual} information in the field of action recognition and spatio-temporal action detection. For example, \cite{DBLP:conf/cvpr/GirdharCDZ19} uses a modified Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} architecture to aggregate context from other people to recognize the target person's actions. \cite{DBLP:journals/corr/abs-2006-07976} proposes a high-order relation reasoning operator that enhances action recognition by modeling Actor-Context-Actor relations. \cite{DBLP:conf/eccv/WuKWZW20} improves spatio-temporal action detection by expanding actor bounding boxes to include more background context. We draw inspiration from these successful works and explore the use of contextual information in ASD.
\vspace{-1ex}
\paragraph{Gaze and social interaction} Evidence from psychology suggests that gaze direction plays an important role in social interactions~\cite{kendon1967some}, such as conversations. Since the VFoA of conversation participants typically converge on the active speaker, some previous work on speaker turn detection \cite{DBLP:conf/interspeech/JokinenHNY10} and audio-visual diarization of meetings \cite{DBLP:conf/mm/GarauBBO09,DBLP:conf/interspeech/GarauDB10,diaz2021audio} have reported good results by explicitly modeling head pose or gaze information with ad-hoc modules. In this work, we implicitly utilize such information through our spatial and relational contexts, which model the relationships among the candidates in an efficient and unified process.
\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{figures/model.pdf}
	\vskip-2ex
	\caption{\textit{Model overview.} Given a video, we first extract face track features and audio features. Face scale and position information are encoded as 2D Gaussians and embedded with CNN layers, which we refer to as \textit{spatial context}. Next, we construct contextual audio-visual representations for each candidate, through intermediate \textit{visual relational context} and \textit{audio-visual relational context} modules. For the visual relational context, we introduce a permutation-equivariant layer to refine each speaker's visual representation by incorporating pairwise relationships and long-term \textit{temporal context}. For the audio-visual relational context, we model audio-visual affinity over time for each candidate and then suppress non-active speakers by contrasting their affinity features with others. The final refined visual and audio-visual representations are concatenated and passed through a shared prediction layer to estimate a confidence score for each visible candidate.}
	\label{fig:architecture}
	\vskip-2ex
\end{figure*}
 \vspace{-2ex} 
\section{The \NetName{} Model}
In this section, we describe our \NetName{} model, which integrates spatial, relational, and temporal context in a unified framework.
First, for each candidate, the scale and position of all candidates' faces are introduced as global \textit{spatial} context to complement facial information and help learn the relationships among the speakers. Each candidate is then contrasted with others from both a visual and audio-visual perspective in the \textit{relational} context modeling component. To further improve the robustness of the model's predictions, \textit{temporal} context is integrated. Finally, based on the aggregated contextual features, we generate speaker activity predictions for all speakers simultaneously with a shared prediction layer. Fig.~\ref{fig:architecture} provides an overview of our proposed approach.
\vspace{-2ex}
\subsection{Encoders}
Given an input video clip, we first crop face tracks for each candidate speaker and transform them into low-dimensional speaker descriptors for further analysis. Likewise, we encode each audio frame into a low-dimensional audio descriptor.
\vspace{-2ex}
\paragraph{Face Track Encoder:}
To encode short-term temporal dynamics, at each time step , where  is the total number of time steps (i.e. frames), the -th candidate's input ( where  is the number of visible candidates at time step ) is a stack of  consecutive face crops  between step  and . Then an encoder , which is a ResNet-18~\cite{DBLP:conf/cvpr/HeZRS16} is used to produce an average-pooled feature vector  over the  face crops. To keep the computational cost manageable, we choose  for our experiments and reduce the dimensionality of each  to  using a single shared fully-connected layer to obtain each candidate's final face track features:

\paragraph{Audio Encoder:}
At each time step  (), we obtain audio representations from a ms window preceding . A ResNet-18 encoder  takes -dimensional MFCCs of the window as input and outputs a -dimensional average-pooled feature. These features are also dimension-reduced to  using a single fully-connected layer. We denote the final audio features by .
\vspace{-2ex}
\subsection{Spatial Context}
The purpose of modeling spatial context is twofold. First, active speakers usually occupy a central position in the scene and depict higher visual saliency, especially in movies (commonly known as "the language of the lens"). Hence knowing the scale, position, and trajectory of a face in the video can help eliminate unlikely candidates. Second, people tend to look at the active speaker as they listen. Therefore, we wish to reflect such gaze-related information to some degree, by providing the model with both facial features and the relative positions of the candidates in the scene.

Specifically, we encode head positions of all candidates in the scene using  coordinate-normalized maps of 2D Gaussians, which is motivated by \cite{DBLP:conf/cvpr/Marin-JimenezKM19}. The position and radius of each Gaussian represent the relative position and size of each candidate's face. Next, as shown in Fig.~\ref{fig:head-maps}, to further indicate the candidates' relationships, for each candidate  () we construct his/her person-specific head map  by generating a color-coded version of the initial Gaussian map in the following principle: yellow denotes candidate , and blue denotes other candidates in the scene. To further facilitate the subsequent modeling of relationships between candidate  and every other candidate  (, ), we tweak the color coding scheme to construct paired variants  as follows: red denotes candidate , green denotes candidate  and blue denotes the rest. Alternatively,  can be thought of an RGB image: the red channel shows the "subject", i.e. candidate ; the green channel shows the "object", i.e. candidate ; and the blue channel shows other "context" candidates, i.e. those other than  and  ( is identified with ). Then a VGG-M-inspired network with four 2D convolutional layers~\cite{DBLP:conf/cvpr/Marin-JimenezKM19} is used to embed these colored head maps  and  into -dimensional vectors per frame,  and  for each candidate  and candidate pair  with . We term the resulting embeddings  and  candidate 's \textbf{spatial context}.
\vspace{-4.5ex}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\columnwidth]{figures/head_map.pdf}
	\vskip-3ex
	\caption{\textit{Head map construction.} For each candidate , we construct his/her speaker-centric head map  and pairwise head maps  for each candidate pair . Note that the lower-triangular elements with  are not used due to the skew-symmetric implementation (see Sec.~\ref{sec:relational-context}).}
	\label{fig:head-maps}
	\vskip-4.5ex
\end{figure}
 \subsection{Relational Context}\label{sec:relational-context}
After obtaining face track and audio descriptors as well as the spatial context embeddings, we jointly model all candidates in the scene and refine each candidate's representations for robust ASD. Our motivation for modeling relational context is the inevitable presence of local ambiguities which make directly matching face motion (e.g. lip movements) and audio non-trivial. Therefore, we integrate different types of contextual clues and consider all candidates in the scene holistically to facilitate the ASD task. For example, a person who is speaking tends to receive more attention from others and is usually portrayed with higher visual saliency (especially in movies).

Specifically, our relational context component is designed to complete two natural sub-tasks for ASD: learning a contextual visual representation for \textit{visual voice activity detection}, and a contextual audio-visual representation for \textit{audio-visual affinity modeling}. The two resulting representations will then be fused for joint analysis to produce the final prediction. As shown in Fig.~\ref{fig:architecture}, our relational context contains two parts: \textbf{visual} () and \textbf{audio-visual relational context} (). This special decoupled design strengthens the model's robustness to synchronization errors between audio and video. We now elaborate on the design of  and .
\paragraph{Visual Relational Context:}
\iffalse
When jointly optimizing all candidates in the scene, the order in which the candidates are arranged within the context should be immaterial. To simultaneously process all candidates, we introduce a \textit{permutation-equivariant} layer  which produces exactly one output per candidate, while preserving the order they are provided to the network. In other words, given any permutation of the  candidate speakers , where  is the symmetry group in  letters, the following holds:

\fi
Our key idea is to represent each candidate's visual activity by aggregating his/her locally perceived activity and all pairwise interactions with other candidates in the scene. Specifically, we design a \textit{permutation-equivariant} layer  to process all candidates simultaneously, while preserving the order in which they are provided to the network. Denote the input visual feature stack by . For each candidate , the output in the -th position of  is

where  and  are two networks that respectively model visual activity and pairwise visual interactions,  denotes candidate 's visual features,  is the head map embedding for candidate , and  is the head map embedding for the candidate pair .

Here, the parameters of  are shared across all candidates, and the parameters of  are shared across all candidate pairs, so Eq.~\eqref{eq:equivariant-gamma} is directly applicable to any number of co-occurring candidates . This in turn means that our network can analyze a theoretically unlimited number of candidates at test time. On the surface, Eq.~\eqref{eq:equivariant-gamma} appears to suggest that obtaining  requires  evaluations in total. In our implementation, we reduce this number by half to  by parameterizing  as a \textit{skew-symmetric} function:
Our intuition is that  should learn to represent the amount of attention that candidate  directs at candidate , and rank the relative ``activeness" and visual saliency of candidate  against candidate . In this case, it suffices to express the direction of such relationships using a single positive or negative sign for each component.
\vspace{-1.5ex}
\paragraph{Audio-Visual Relational Context:}
We model the affinity between the audio and each speaker's face track in terms of both synchrony and cross-modal biometrics. We fuse per-frame face track features with the audio features via concatenation, and then pass them through a shared network  to compute local A-V affinity features . Considering that local A-V affinity estimation is vulnerable to signal ambiguity (e.g. low-resolution faces, profile faces, noisy audio etc.), we aggregate evidence from all candidates to make more reliable predictions: if one or some candidates display strong A-V agreement, then the network can accordingly lower its affinity predictions for other candidates to mitigate false positives. We therefore design a learnable module to adaptively suppress only the non-active candidates whose A-V affinities are of significantly lower magnitudes, while leaving the active candidates' features "as-is". To this end, we introduce an element-wise max-pooling operation over all A-V affinity features to obtain a global representation. Specifically, the pooled feature is computed as , 
\iffalse

\fi
and then concatenated to each candidate's initial features . The concatenated features are further processed with two fully connected layers to generate each candidate's final contextual audio-visual representation . We name this operation \textit{non-active speaker suppression} after its effect.
\vspace{-2ex}
\subsection{Temporal Context}\label{sec:temporal-context}
As shown in Fig.~\ref{fig:architecture}, we incorporate temporal context in the networks , , and  inside  and . This benefits ASD in two ways: first, it improves the consistency of the relational context modeling process and smoothes out local, instantaneous noises; second, it helps to alleviate synchronization errors between the audio and the video stream, an issue that is ubiquitous with in-the-wild videos.

We choose two basic architectures as our temporal modeling back-end: temporal convolutions (1D CNNs), and bi-directional Gated Recurrent Units (Bi-GRUs). The former is favorable since it has a fixed temporal receptive field, and can be easily adapted for online settings. The latter is more reliable since it has access to both past and future information. Hence, for the rest of the paper, by mentioning \textbf{temporal context} we refer to the usage of a one-layer Bi-GRU backend with  cells. Otherwise, we apply a stack of two 1D convolutional layers of kernel size  interleaved with Batch Normalization~\cite{DBLP:conf/icml/IoffeS15} and ReLU activation by default.

Finally, the refined contextual representations  and  are concatenated and fed to a fully-connected layer that is shared across candidates. The outputs are passed through sigmoid activation, resulting in real values between  and  that indicate each candidate's probability of being the active speaker.
\vspace{-2ex}
\subsection{Losses}\label{sec:losses}
The model is trained end-to-end with a multi-task loss formulation. Since the goal is to predict a binary speaking/not speaking label, for each loss term we apply the standard binary cross-entropy (BCE) loss, averaged over all time steps. The BCE loss is defined as

where  and  are the predictions and ground truth labels, respectively. For a scene with  frames and  candidates, let  and  denote the audio and visual auxiliary prediction layer,  the final audio-visual prediction layer, and  and  the corresponding visual and audio-visual ground truths of the -th candidate. We add two auxiliary losses  and  to learn discriminative features for both visual and audio streams. The \textit{audio} prediction loss is

where  is the audio ground truth indicating whether at least one of the candidates is speaking, and  is the sigmoid function. 

When training on a single candidate (\textit{i.e.} without employing relational context), 
the \textit{visual} prediction loss and main \textit{audio-visual} prediction loss are given by 

where  denotes feature concatenation. When training with relational context on multiple candidates, the losses are slightly different. To compute the visual and audio-visual losses, we replace the inputs to the auxiliary prediction layers with (concatenated) contextual representations, and aggregate losses from all candidates:


Finally, the \textit{total} loss is defined as follows:


Note that in practice,  may vary over time as candidates enter and leave the scene.
\vspace{-1.5ex} 
\section{Experiments}
To validate the effectiveness of our model, we thoroughly evaluate and analyze the performance of \NetName{} from two aspects. First, we perform comprehensive ablation studies and comparison on a popular and challenging ASD benchmark derived from diverse, in-the-wild movies, AVA-ActiveSpeaker. Second, we perform cross-dataset testing on three additional datasets, Columbia, RealVAD, and AVDIAR. Example frames from each dataset are shown in Fig.~\ref{fig:dataset-teaser}.

	\begin{figure}
		\centering
		\subfigure[AVA-ActiveSpeaker]{\includegraphics[width=0.45\columnwidth]{figures/datasets/ava}}
		\hfill
		\subfigure[Columbia]{\includegraphics[width=0.45\columnwidth]{figures/datasets/columbia}}\-6pt]
		\subfigure[RealVAD]{\includegraphics[width=0.45\columnwidth]{figures/datasets/realvad}}
		\hfill
		\subfigure[AVDIAR]{\includegraphics[width=0.4\columnwidth]{figures/datasets/avdiar}}
		\vskip-4ex
		\caption{\textit{Example frames from the datasets used in this paper.} Green boxes denote the active speakers.}
		\label{fig:dataset-teaser}
		\vskip-4ex
	\end{figure}
 \vspace{-2.5ex}
\subsection{Experimental Setup}\label{sec:setup}
\paragraph{Datasets:} The \textbf{AVA-ActiveSpeaker} dataset \cite{DBLP:conf/icassp/RothCKMGKRSSXP20} consists of  YouTube movies from film industries around the world. Each video is annotated from minutes  to . The dataset provides face bounding boxes that are linked into face tracks and labeled for both speech activity and whether the speech is audible. As shown in Fig.~\ref{fig:dataset-teaser}a, the dataset is highly challenging as it contains occlusions, low-resolution faces, low-quality audio, and varied lighting conditions. It has become a mainstream benchmark for the ASD task.

The \textbf{Columbia} dataset \cite{DBLP:conf/eccv/ChakravartyT16} annotates  minutes of a one-hour panel discussion video featuring  unique panelists (Bell, Bollinger, Lieberman, Long, Sick, Abbas). The panelists directly face the camera, but occasionally look elsewhere, or perform spontaneous activities (e.g. drinking water). Upper body bounding boxes and voice activity ground truth labels are provided, as shown in Fig.~\ref{fig:dataset-teaser}b.

The \textbf{RealVAD} dataset~\cite{9133504} (Fig.~\ref{fig:dataset-teaser}c) is also constructed from a panel discussion lasting approximately  minutes. The video is recorded using a static, mounted camera, capturing the  panelists in a full shot. The panelists sit in two rows, and at any given time can be looking anywhere. In addition, the panelists perform natural, spontaneous actions that may result in partial occlusion of the face and mouth (e.g. touching faces, cupping chins). Similar to Columbia, upper body bounding boxes and voice activity labels are provided. These two datasets are often adopted by works on visual voice activity detection (V-VAD) and ASD~\cite{DBLP:conf/iccvw/ShahidBM19,Shahid_2021_WACV,DBLP:conf/accv/ChungZ16a,DBLP:conf/eccv/AfourasOCZ20}.

The \textbf{AVDIAR} dataset \cite{DBLP:journals/pami/GebruBLH18} (Fig.~\ref{fig:dataset-teaser}d) provides  recordings of informal conversations involving one to four participants. The dataset is extremely challenging due to varying levels of speaker movement and speech overlap, occlusions, profile faces, and even back-facing speakers. The dataset was originally used to evaluate speaker diarization and tracking systems. We re-purpose the dataset to evaluate our ASD model by aggregating per-frame ASD results into diarization results.
\vspace{-2ex}
\paragraph{Data Preprocessing:} We preprocess each dataset by cropping the face tracks of each visible candidate. For the \textbf{AVA-ActiveSpeaker} dataset, to group face tracks into ``scenes" for training and testing, each video is first segmented into shots with \texttt{ffmpeg}~\cite{DBLP:journals/ijig/Lienhart01}. Within each shot, disconnected tracks belonging to the same candidate are then merged based on bounding box overlap with an IoU threshold of . All videos are resampled to fps, and ground truth labels are computed according to frame timestamps via nearest-neighbor interpolation. The \textbf{Columbia} and \textbf{RealVAD} datasets do not provide face bounding boxes, so face detection is performed with an off-the-shelf RetinaFace detector~\cite{DBLP:conf/cvpr/DengGVKZ20}. The resulting bounding boxes are expanded by x to mimic AVA-style detections. For the \textbf{AVDIAR} dataset, the provided bounding boxes are used directly.
\vspace{-1.5ex}
\paragraph{Train/test Split and Evaluation Metric:}
For \textbf{AVA-ActiveSpeaker}, the official train/validation/test split is adopted. The test set is held out for the ActivityNet challenge and unavailable, so we perform our analysis on the validation set instead, as several previous methods do~\cite{DBLP:conf/cvpr/AlcazarCMPLAG20,zhangmulti2019,DBLP:journals/corr/abs-1906-10555}. The \textbf{Columbia} and \textbf{AVDIAR} datasets do not provide training sets, so we only perform zero-shot testing. For Columbia, following previous practice \cite{DBLP:conf/eccv/ChakravartyT16,DBLP:conf/accv/ChungZ16a}, we report results on all but one speaker (Abbas) which is usually held out for validation. For \textbf{RealVAD}, in addition to zero-shot results, we also report leave-one-out cross-validation results for each of the  panelists to compare with previous work. We only train on scenes in which the test speaker does not co-occur with the training speakers. 


Finally, we report the metric that was commonly adopted by previous work for each dataset: mean Average Precision (mAP) and Area Under Receiver Operating Characteristic Curve (AUROC) for AVA-ActiveSpeaker, frame-wise -1 score for Columbia and RealVAD, and Diarization Error Rate (DER) for AVDIAR, which is defined as the sum of false alarm rate, missed speech rate, and speaker confusion rate.
\vspace{-2ex}
\paragraph{Implementation Details:} We implement our model with PyTorch and the \texttt{pytorch-lightning} package. All models are trained using the AdamW optimizer~\cite{DBLP:conf/iclr/LoshchilovH19}. The network parameters are initialized using He initialization~\cite{DBLP:conf/iccv/HeZRS15}. During training, we augment the data via random horizontal flipping and uniform corner cropping along the input face tracks, followed by random adjustments to brightness, contrast, and saturation. All cropped face tracks are resized to , and randomly cropped to  for training. We use a central  patch for testing. We only run one trial with a fixed random seed for all experiments to ensure the results are comparable, but find the results to be stable under different network initializations and data augmentations. During inference, occasional long segments that do not entirely fit into GPU memory are split into shorter, fixed-size chunks and the predictions are re-combined later.

To facilitate training, we apply curriculum learning~\cite{DBLP:conf/icml/BengioLCW09} by first training a single-candidate model without relational context according to Eq.~\eqref{eq:total_loss}, and then continuing to train on up to  candidates, \textit{i.e.}  in Eq.~\eqref{eq:equivariant-gamma}, but without the  term in Eq.~\eqref{eq:total_loss}. This is theoretically and empirically enough to learn the relevant parameters since  captures \textit{pairwise} interactions, and we observe diminishing returns by sampling  or more candidates (note that we can still test on  candidates due to parameter sharing). In addition, we follow the sampling strategy in \cite{DBLP:conf/cvpr/AlcazarCMPLAG20} during training, and randomly sample a s segment ( frames at fps)\footnote{The mean speech segment duration is s on the AVA-ActiveSpeaker dataset~\cite{DBLP:conf/icassp/RothCKMGKRSSXP20}.} from every training example in the dataset for each epoch. Therefore, our epoch size correlates with the number of identities or scenes rather than face detections, which prevents over-fitting.
\vspace{-2ex}
\subsection{Ablation Studies}\label{sec:ablations}
In this subsection, we provide a thorough ablation study on the modeling components on the challenging AVA-ActiveSpeaker dataset.
\vspace{-4ex}
\paragraph{Spatial Context:} We compare two different ways to incorporate spatial context. The first approach applies \textit{early fusion} and concatenates speaker-centric spatial embeddings  with the -th candidate's face track features before dimensionality reduction. In contrast, the second approach applies \textit{late fusion} and concatenates  with the dimension-reduced face track features . As shown in Table~\ref{table:head-maps}, both improve upon the simplest baseline that only uses convolutional temporal context (see Sec.~\ref{sec:temporal-context}) on audio and face track features, but early fusion performs \% worse than late fusion, likely because information is lost too early as features pass the -dim bottleneck. Thus, we adopt late fusion hereafter. As shown in Table~\ref{table:ablations}, simply concatenating the spatial context embeddings to the face track features (denoted by \textbf{+S}) already yields stronger visual representations that improve the baseline (\% mAP).
\vspace{-2.5ex}
\begin{table}[h]
	\caption{\textit{Comparison w.r.t. the stage at which spatial context is introduced.} We observe that applying late fusion yields better results since it helps to retain more spatial information for subsequent modeling.}
	\vskip-3ex
	\label{table:head-maps}
	\begin{center}
		\begin{tabular}{l|c|c}
			\toprule
			\textbf{Method} & \textbf{mAP (\%)} & \textbf{AUROC} \\
			\midrule
Baseline &  &  \\
Early fusion &  &  \\
\textbf{Late fusion} &  &  \\
			\bottomrule
		\end{tabular}
	\end{center}
	\vskip-3.5ex
\end{table}
 \vspace{-1ex}
\paragraph{Relational Context:}
We assess the efficacy of our relational context component. As shown in Table~\ref{table:ablations}, incorporating the relational context module yields noticeable improvements of \% absolute over the 1D CNN baseline when used alone (\textbf{+R}), and works in synergy with spatial context (\% mAP for \textbf{+S+R}). We observe that this improvement is especially pronounced in multi-speaker and small face scenarios. This is because \NetName{} can model the relationships among the candidates in the scene, which leads to a better holistic understanding under these challenging settings. Moreover, we introduce weight sharing strategies within a permutation-equivariant formulation, which enables our model to process a theoretically unlimited number of speakers at test time.
\vspace{-1.5ex}
\paragraph{Temporal Context:}
Finally, incorporating temporal context with Bi-GRUs yields the most substantial improvement, which is reasonable because Bi-GRUs have access to each candidate's complete track history, and can thus make more reliable predictions. By leveraging long-term temporal context, our model ultimately achieves \% mAP, improving upon our initial baseline by as much as \%.
\begin{table}
	\centering
	\vskip-3ex
	\caption{\textit{Ablation results on the AVA-ActiveSpeaker validation set.} \textbf{S}: Spatial Context; \textbf{R}: Relational Context; \textbf{T}: Temporal Context. Predictions from models without temporal context are smoothed over -frame windows via Wiener filtering.}
	\vskip-2.5ex
	\label{table:ablations}
	\resizebox{0.85\columnwidth}{!}{
		\begin{tabular}{l|c|c|c}
			\toprule
			\textbf{Method} & \textbf{\#Params} & \textbf{mAP (\%)} & \textbf{AUROC}\\
			\midrule
Baseline & M &  &  \\
+S & M &  &  \\
+R & M &  &  \\
+S+R & M &  &  \\
\midrule
			+T (Bi-GRU) & M &  &  \\
+S+T & M &  &  \\
+R+T & M &  &  \\
\textbf{+S+R+T (\NetName{})} & M &  &  \\
			\bottomrule
		\end{tabular}
	}
	\vskip-2.5ex
\end{table}
 \vspace{-4ex}
\paragraph{Non-active Speaker Suppression:}
In this part, we discuss the role of non-active speaker suppression in ASD. As shown in Table~\ref{table:suppression}, we compare two ways of integrating global information against a baseline that does not apply non-active speaker suppression. One uses mean-pooling to obtain , and the other uses max-pooling. Both improve over the model that only introduces visual relational context, and max-pooling performs slightly better, yielding fewer false negatives. One possibility is that max-pooling back-propagates more gradients to the most salient speaker, while mean-pooling results in weaker gradients for the true active speaker, making the model less confident.

\begin{table}
	\caption{Effect of implementing non-active speaker suppression with different pooling methods.}
	\vskip-3ex
	\label{table:suppression}
	\begin{center}
		\begin{tabular}{l|c|c}
			\toprule
			\textbf{Pooling}             & \textbf{mAP (\%)} & \textbf{AUROC}  \\
			\midrule
None (no suppression) &  &  \\
Mean-pooling &  &  \\
\textbf{Max-pooling} &  &  \\
			\bottomrule
		\end{tabular}
	\end{center}
	\vskip-4ex
\end{table}
 \vspace{-2ex}
\subsection{Comparison with the State-of-the-Art}
As shown in Table~\ref{table:ava-sota}, our full \NetName{} outperforms previous audio-visual ASD methods including the state-of-the-art by a large margin. Remarkably, we achieve \% mAP without any pre-training, surpassing \% for the first time on the AVA-ActiveSpeaker validation set at the time of submission. This overall performance strongly supports the effectiveness and superiority of our \NetName{}. It is also worth noting that the \textbf{+S+T} model in Table~\ref{table:ablations} provides a very strong baseline (\% mAP) that already exceeds all methods available for comparison here, which further confirms the benefits of our model. As a side note, applying ImageNet pre-training to the encoders further boosts performance by \% to \%.
\vspace{-2.5ex}
\begin{table}[h]
	\centering
	\caption{\textit{Comparison with previous work on the AVA-ActiveSpeaker validation set.} For each method, we copy the results from its original paper. mAP is calculated using the official evaluation tool, after interpolating our predictions to the timestamps in the original annotations.}
	\label{table:ava-sota}
	\vskip-2.5ex
	\resizebox{0.9\columnwidth}{!}{
		\begin{tabular}{l|c|c|c}
			\toprule
			\textbf{Method} & \textbf{Pre-training?}  & \textbf{mAP (\%)} & \textbf{AUROC}\\
			\midrule
			AV-GRU \cite{DBLP:conf/icassp/RothCKMGKRSSXP20} & 
			& - & \\ 
			Optical Flow \cite{DBLP:conf/cvpr/HuangK20} & 
			& - & \\ 
			Multi-Task \cite{zhangmulti2019} & \checkmark
			& & -\\
			VGG-LSTM \cite{DBLP:journals/corr/abs-1906-10555} & \checkmark
			&  & -\\ 
			VGG-TempConv \cite{DBLP:journals/corr/abs-1906-10555} & \checkmark
			&  & -\\ 
			ASC \cite{DBLP:conf/cvpr/AlcazarCMPLAG20} & \checkmark
			&  & -\\ 
			\textbf{Ours (\NetName{})} & 
			&  & \\
			\textbf{Ours (\NetName{})} & \checkmark &  &  \\
			\bottomrule
		\end{tabular}
	}
	\vskip-3.5ex
\end{table}
 \vspace{-1ex}
\subsection{Cross-dataset Evaluation}\label{sec:cross-dataset-eval}
Apart from evaluating on the large-scale AVA-ActiveSpeaker dataset, we also conduct cross-dataset evaluation on three other datasets: Columbia, RealVAD and AVDIAR. These datasets contain challenges that are previously rare, if not unseen in AVA-ActiveSpeaker, such as overlapped speech, reverberation, extreme poses, heavy face occlusion, and different spatial distributions of candidates. Our cross-dataset evaluation protocols include zero-shot testing on all three datasets using the model trained on AVA-ActiveSpeaker, and leave-one-out testing after fine-tuning on RealVAD. We report both visual-only and audio-visual performance. The former is obtained using only the auxiliary visual prediction layer  in Sec.~\ref{sec:losses}.
\vspace{-1.5ex}
\paragraph{Zero-shot Testing:}
Overall, our \NetName{} model achieves good zero-shot generalization performance on all three datasets. In particular, it achieves state-of-the-art performance on both Columbia ( average -1 score) and RealVAD ( average -1 score). On AVDIAR, we are the first to report diarization performance using single-channel audio and a single camera viewpoint.

On the \textbf{Columbia} dataset, 
we outperform all previous state-of-the-art under both visual-only and audio-visual settings. It is worth noting that the listed visual-only methods~\cite{DBLP:conf/eccv/ChakravartyT16,DBLP:conf/iccvw/ShahidBM19,Shahid_2021_WACV} use the entire upper body, while we only use cropped face tracks. We also outperform previous audio-visual models~\cite{DBLP:conf/eccv/AfourasOCZ20,DBLP:conf/accv/ChungZ16a} that are pre-trained on datasets a magnitude larger ( and  hours, respectively).
Similarly, on \textbf{RealVAD}, our zero-shot visual-only and audio-visual results outperform the only existing method available for equal comparison by a large margin (\% V, \% AV vs. \%). An interesting observation is that on RealVAD and Columbia, our visual-only model outperforms the audio-visual model in most cases. This is because labels provided with the two datasets are not frame-accurate; our audio-visual model successfully detects short inter-sentence pauses that are not annotated as "not speaking" by the dataset curators. Due to the page limit, please refer to the supplementary video for details.

On \textbf{AVDIAR}, our model provides the first baseline 
that only uses mono microphone input, and a single camera viewpoint -- a highly challenging setup. Nevertheless, our audio-visual model still achieves  DER on average, which is a very promising zero-shot result. Moreover, our full \NetName{} model reduces DER by  to \% over the \textbf{+S+T} model that does not model relationships among the candidates, which once again supports the efficacy of our relational context in multi-speaker scenarios.
\vspace{-2ex}
\paragraph{Fine-tuned Results:} We also evaluate fine-tuned leave-one-out performance on RealVAD (Columbia and AVDIAR do not provide training splits). After 
fine-tuning on homogeneous data, our model quickly adapts to the unseen scenario (panel discussion) and noises (e.g. spontaneous actions near the face). The average audio-visual -1 score is improved by \% from \% to \%, and the average visual -1 score is improved by \% from \% to \%. More detailed results can be found in the appendix.
\vspace{-1.5ex}
\subsection{Performance Breakdown}
In this part, we provide a more in-depth analysis of three known challenging scenarios on AVA-ActiveSpeaker. We provide a comparison with the existing state-of-the-art and highlight some advantages and appealing properties of the \NetName{} model.
\vspace{-1ex}
\paragraph{Low-resolution Faces.} We summarize performance for different face sizes in Fig.~\ref{fig:breakdown}a. Following previous evaluation procedures \cite{DBLP:conf/icassp/RothCKMGKRSSXP20,DBLP:conf/cvpr/AlcazarCMPLAG20}, we partition the dataset into three bins by the widths of the detected faces: small (width  64 pixels), medium (width between  and  pixels), and large (width  pixels). While the \textbf{+S+T} model provides a strong baseline with \%mAP that already outperforms the previous state-of-the-art \cite{DBLP:conf/cvpr/AlcazarCMPLAG20}, our full model which exploits relational context yields further performance gains of around \% mAP absolute on all subsets. In particular, on the ``small" subset, we outperform the previous state-of-the-art (\% mAP) by a large margin of \%.

\begin{figure}
	\vskip2ex
	\subfigure[mAP breakdown by face resolution.]{\includegraphics[width=0.48\columnwidth]{figures/face-size.pdf}}
	\hfill
	\subfigure[mAP breakdown by number of on-screen faces.]{\includegraphics[width=0.48\columnwidth]{figures/num-faces.pdf}}
	\vskip-3.5ex
	\caption{\textit{Performance breakdown.} We evaluate the performance of our model on faces of different sizes and on frames with one, two, and three detected faces (see main text). The performance of the previous state-of-the-art~\cite{DBLP:conf/cvpr/AlcazarCMPLAG20} (ASC) and our best model are annotated for better comparison.}
	\label{fig:breakdown}
	\vskip-3.5ex
\end{figure}
 \vspace{-1ex}
\paragraph{Multiple Candidates.} Fig.~\ref{fig:breakdown}b shows the model performance according to the number of detected faces in a frame. We consistently improve over the previous state-of-the-art for different numbers of detected faces, with the most significant gain being in the subset with  faces (\% mAP). Moreover, we surpass  on the two-faces subset for the first time. The improvements prove the effectiveness of spatial and relational context modeling, which equip the model with the ability to discern background actors from main actors, as well as an enhanced understanding of the relationships between the visible speakers. 
We refer interested readers to the supplemental video for more interesting qualitative results.
\vspace{-1ex}
\paragraph{Out-of-sync Audio and Video.}\label{sec:out-of-sync}
Many real-world videos suffer from poor synchronization between audio and video, caused by transmission or re-encoding. To evaluate our model's performance on out-of-sync data, we assume perfect synchronization in the source videos and artificially shift the audio stream by up to  frames to mimic out-of-sync videos. We then assess the performance of our model on manually de-synchronized videos. As shown in Fig.~\ref{fig:sync}, our model is fairly resilient to A-V sync errors. Remarkably, even on videos that are shifted by  frames (sec), our full \NetName{} model only loses \% mAP, and still outperforms a simple Bi-GRU baseline by \% absolute, which does not model spatial and relational context (\textbf{+T} in Table~\ref{table:ablations}), and receives \textit{synchronized} inputs. This shows that our model is robust to synchronization error.
\begin{figure}[!htb]
	\centering
	\vskip-2ex
	\includegraphics[width=0.62\columnwidth]{figures/out_of_sync_map.pdf}
	\vskip-2.5ex
	\caption{\textit{Results on out-of-sync videos.} Our model is fairly resilient to A-V synchronization error.}
	\label{fig:sync}
	\vskip-4ex
\end{figure}
 \vspace{-1.5ex} 
\section{Conclusion}
We have proposed a novel model named \NetName{} for active speaker detection. Key to our method is a unified modeling framework which efficiently aggregates different types of contextual evidence to make robust decisions, by considering the relationships between each candidate and others. We demonstrate via experiments on the large-scale AVA-ActiveSpeaker dataset and various other real-world datasets that our model successfully tackles challenging cases with multiple candidates and low-resolution faces, and outperforms state-of-the-art methods by a large margin.
\vspace{-1.5ex} 
\section*{Acknowledgments}
This work was partially supported by the National Key R\&D Program of China (No. 2017YFA0700804) and the National Natural Science Foundation of China (No. 61876171).

\begin{Appendix}
	\section{AVA-ActiveSpeaker Dataset Statistics}
	Table~\ref{table:ava-statistics} provides some statistics about the AVA-ActiveSpeaker dataset. Note that the number of entities is a rough estimate of the upper bound, which is the number of merged face tracks that are obtained through the procedure described in Sec.~\ref{sec:setup} of the main paper.
\section{Cross-Dataset Evaluation Results}
	In this section, we provide detailed results for the cross-dataset evaluations in Sec.~\ref{sec:cross-dataset-eval} of the main paper. Results for Columbia, RealVAD and AVDIAR are presented in Table~\ref{table:columbia}, \ref{table:realvad}, and \ref{table:avdiar}, respectively. For an analysis of these results, please refer to Sec.~\ref{sec:cross-dataset-eval} of the main paper. Note that we find if the sampling rate of the videos are changed to 25fps (the value used during training), the results will be different. In these tables, we report the results at the original sampling rate. For fine-tuned results on RealVAD, we report the numbers from the best models on the validation set.
	
	\begin{table*}
	\centering
	\caption{\textit{AVA-ActiveSpeaker dataset statistics}. The number of entities and scenes are obtained after merging annotated face tracks as described in Sec.~4.1, and the last column indicates the total duration of all scenes in each partition. Test set labels are held out for a separate ActivityNet challenge.}
	\label{table:ava-statistics}
	\vskip-1ex
		\begin{tabular}{l|c|c|c|c|c}
			\toprule
			\textbf{Partition} & \textbf{\#Videos} & \textbf{\#Faces} & \textbf{\#Entities} & \textbf{\#Scenes} & \textbf{\#Hours} \\
			\midrule
			Train &  & k &  &  &  \\
			Validation &  & k &  &  &  \\
			Test &  & k &  &  & \\
			\bottomrule
		\end{tabular}
\end{table*}
 	\begin{table*}
	\caption{\textit{Zero-shot results on the Columbia dataset}. We report -1 scores (\%) for each speaker, and the overall average. The methods marked with an asterisk () employ large-scale self-supervised pre-training.}
	\vskip-1ex
		\begin{tabular}{l|c|c|c|l|l|l}
			\toprule
			\multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{Speaker}} \\ \cline{2-7} 
			& \textbf{Bell} & \textbf{Boll}    & \textbf{Lieb}    & \textbf{Long} & \textbf{Sick} & \textbf{Avg.} \\
			\midrule
			\multicolumn{7}{l}{\textit{Visual-only}} \\
			\midrule
			Cross-modal \cite{DBLP:conf/eccv/ChakravartyT16}
			&  &  &  &  &  &  \\
			Upper Body \cite{DBLP:conf/iccvw/ShahidBM19}
			&  &  &  &  &  &  \\
			S-VVAD \cite{Shahid_2021_WACV} &  &  &  &  &  &  \\
			\textbf{Ours}
			&  &  &  &  &  &  \\
			\midrule
			\multicolumn{7}{l}{\textit{Audio-visual}} \\
			\midrule
			SyncNet \cite{DBLP:conf/accv/ChungZ16a}
			&  &  &  &  &  &  \\
			LWTNet \cite{DBLP:conf/eccv/AfourasOCZ20}
			&  &  &  &  &  &  \\
			\textbf{Ours}
			&  &  &  &  &  &  \\
			\bottomrule
		\end{tabular}
	\label{table:columbia}
\end{table*}
 	\begin{table*}
	\caption{\textit{Zero-shot and leave-one-out fine-tuning results on the RealVAD dataset}. We report -1 scores (\%) for each panelist, the overall average, and the overall standard deviation.}
	\vskip-1ex
		\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c}
			\toprule
			\multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{11}{c}{\textbf{Speaker}} \\ \cline{2-12} 
			\multicolumn{1}{c|}{} & \textbf{P1}    & \textbf{P2}    & \textbf{P3}    & \textbf{P4}    & \textbf{P5}    & \textbf{P6}    & \textbf{P7}    & \textbf{P8}    & \textbf{P9}    & \textbf{Avg.}   & \textbf{Std.}
			\\ \midrule
			Beyan~\cite{9133504} (V)                 &  &  &  &   &   &  &  &  &  &  &  \\
			Ours (V, zero-shot)           &  &  &  &  &  &  &  &  &  &  & \\
			\textbf{Ours (V, fine-tuned)} &  &  &  &  &  &  &  &  &  &  & \\
			Ours (AV, zero-shot)            &  &  &  &  &  &  &  &  &  &  &  \\ 
			\textbf{Ours (AV, fine-tuned)} &  &  &  &  &  &  &  &  &  &  & \\
			\bottomrule
		\end{tabular}
	\label{table:realvad}
\end{table*}
 	\begin{table*}
	\caption{\textit{Zero-shot results on the AVDIAR dataset.} The average is computed over all test sequences. Diarization Error Rate (DER) (\%) is reported; the lower the DER is, the better.}
	\label{table:avdiar}
	\vskip-1ex
		\begin{tabular}{l|c|c|c|c}
			\toprule
			\multirow{2}{*}{\textbf{Subset}} & \multicolumn{4}{c}{\textbf{Methods}}                              \\ \cline{2-5} 
		   & \textbf{+S+T (V)} & \textbf{\NetName{} (V)}  & \textbf{+S+T (AV)} & \textbf{\NetName{} (AV)}\\ \midrule
			 participant &  &  &  &  \\
			 participants &  &  &  &  \\
			 participants &  &  &  &  \\
			 participants &  &  &  &  \\
			\midrule
			\textbf{Average} &  &  &  & 
			\\
 			\bottomrule
		\end{tabular}
\end{table*}
 \end{Appendix}

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

\end{document}
