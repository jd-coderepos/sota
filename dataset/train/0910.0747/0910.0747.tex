
\chapter{A Logic for Reasoning About Specifications}
\label{ch:meta-logic}

In this chapter we present the meta-logic \logic. This logic allows
for encoding descriptions of computational systems and for reasoning
over those descriptions. The logic includes traditional reasoning
devices such as case analysis, induction, and co-induction as well as
new devices specifically designed for working with higher-order abstract
syntax.

The relevant history of \logic begins with the meta-logic \FOLDN
developed by McDowell and Miller for the purposes of inductive
reasoning over higher-order abstract syntax descriptions
\cite{mcdowell02tocl, mcdowell00tcs}. This logic contains a definition
mechanism which allows one to specify and reason about closed-world
descriptions, \ie, allows one to form judgments and to perform case
analysis on them. This definition mechanism is based on earlier work
on closed-world reasoning by many others, but most notably by
Schroeder-Heister \cite{schroeder-Heister93lics}, Eriksson
\cite{eriksson91elp}, and Girard \cite{girard92mail}. The primary
contribution of \FOLDN was the recognition that definitions provided a
way of encoding higher-order abstract syntax descriptions in such a
way that does not conflict with inductive reasoning. In particular,
\FOLDN allowed for natural number induction, and so many reasoning
tasks could be naturally encoded. More recently, Tiu \cite{tiu04phd}
developed the meta-logic Linc which extends the mechanism of
definitions to integrate notions of generalized induction and
co-induction over the structure of definitions. These more general
notions are present in \logic as well.

Another central advancement in the development of logics for reasoning
over higher-order abstract syntax descriptions was the recognition
that one needed a way to reflect the binding structure of terms into
the structure of proofs. This was realized in earlier logics by using
universal judgments. However, this kind of correspondence was always
an uneasy one and the mismatch became explicit when it was necessary
to use case analysis arguments over binding structure as must be done,
for example, in bisimilarity proofs associated with $\pi$-calculus
models of concurrent systems. The desire to provide a logically
precise and cleaner treatment led to the development of the
$\nabla$-quantifier and the associated generic judgment by Miller and
Tiu in the meta-logic \foldnb \cite{miller05tocl}. Tiu later refined
this notion in the meta-logic \LG so that $\nabla$-quantifier behaved
well with respect to inductive reasoning \cite{tiu06lfmtp}. This
interpretation of the $\nabla$-quantifier is present in \logic, and in
this context it can be understood as quantifying over fresh names.

The meta-logic \logic is a continuation of the research surrounding
inductive reasoning and higher-order abstract syntax descriptions. In
particular, it extends the notion of equality in the logic to one
which can describe the binding structure of terms relative to the
proof context in which they occur. This turns out to be essential to
describing the structure of terms which are generated during inductive
reasoning over higher-order abstract syntax descriptions. Moreover,
\logic identifies how this extended notion of equality can be
integrated with the definition mechanism to allow a succinct
description of such objects.

The presentation of \logic is divided into three parts. First,
Section~\ref{sec:logic} contains the core of the logic including
generic quantification. Then Section~\ref{sec:nominal-abstraction}
introduces the extended notion of equality known as {\em nominal
  abstraction} and rules for treating this notion within the logic.
Finally, Section~\ref{sec:definitions} presents rules for treating
fixed-points in the logic including mechanisms for induction and
co-induction. Although the logical features of \logic are described in
their entirety in the first three sections, it is sometimes convenient
to use an alternative presentation for fixed-point definitions. This
form, which uses patterns to distinguish different cases in the
structure of the atom being defined, is introduced in
Section~\ref{sec:pattern-form} and is elaborated as an interpretation
of the basic form of definitions that uses nominal abstractions
explicitly. Rules for treating this alternative form of fixed-points
are presented and proven to be admissible. Finally,
Section~\ref{sec:examples} provides some small examples to illustrate
the expressive power of the logic.

\section{A Logic with Generic Quantification}
\label{sec:logic}

In this section we present the core logic underlying \logic. This
logic is obtained by extending an intuitionistic and predicative
subset of Church's Simple Theory of Types with a treatment of generic
judgments. The encoding of generic judgments is based on the
quantifier called $\nabla$ (pronounced nabla) introduced by Miller and
Tiu \cite{miller05tocl} and further includes the structural rules
associated with this quantifier in the logic \LG described by Tiu
\cite{tiu06lfmtp}.


\subsection{The Basic Syntax}

Following Church \cite{church40}, terms are constructed from constants
and variables using abstraction and application. All terms are
assigned types using a monomorphic typing system; these types also
constrain the set of well-formed expressions in the expected way. The
collection of types includes $o$, a type that corresponds to
propositions. Well-formed terms of this type are also called formulas.
Two terms are considered to be equal if one can be obtained from the
other by a sequence of applications of the $\alpha$-, $\beta$- and
$\eta$-conversion rules, \ie, the $\lambda$-conversion rules. This
notion of equality is henceforth assumed implicitly wherever there is
a need to compare terms. Logic is introduced by including special
constants representing the propositional connectives $\top$, $\bot$,
$\land$, $\lor$, $\supset$ and, for every type $\tau$ that does not
contain $o$, the constants $\forall_\tau$ and $\exists_\tau$ of type
$(\tau \rightarrow o) \rightarrow o$. The binary propositional
connectives are written as usual in infix form and the expressions
$\forall_\tau x. B$ and $\exists_\tau x. B$ abbreviate the formulas
$\forall_\tau \lambda x.B$ and $\exists_\tau \lambda x.B$,
respectively. Type subscripts will be omitted from quantified formulas
when they can be inferred from the context or are not important to the
discussion. We also use a shorthand for iterated quantification: if
${\cal Q}$ is a quantifier, we will often abbreviate ${\cal
  Q}x_1\ldots{\cal Q}x_n.P$ to ${\cal Q}x_1,\ldots,x_n.P$ or simply
${\cal Q}\vec{x}.P$. We consider the scope of $\lambda$-binders (and
therefore quantifiers) as extending as far right as possible. We
further assume that $\supset$ is right associative and has lower
precedence than $\land$ and $\lor$. For example, $\forall x. t_1
\supset t_2 \supset t_3 \land t_4$ should be read as $\forall x.(t_1
\supset (t_2 \supset (t_3\land t_4)))$.

The usual inference rules for the universal quantifier can be seen as
equating it to the conjunction of all of its instances: that is, this
quantifier is treated extensionally.
There are several situations  where one
wishes to treat an expression such as ``$B(x)$ holds for all
$x$'' as a statement about the existence of a uniform argument for
every instance rather than the truth of a particular property for each
instance \cite{miller05tocl};
such situations typically arise when one is reasoning about the
binding structure of formal objects represented using the
$\lambda${\em -tree syntax} \cite{miller00cl} version of {\em
higher-order abstract syntax} \cite{pfenning88pldi}.
The $\nabla$-quantifier serves to encode judgments that have this kind
of a ``generic'' property associated with them. Syntactically, this
quantifier corresponds to
including a constant $\nabla_\tau$ of type $(\tau \rightarrow o)
\rightarrow o$ for each type $\tau$ not containing $o$.\footnote{
We may choose to allow $\nabla$-quantification at fewer types in
particular applications; such a restriction may be
  useful in adequacy arguments for reasons we discuss later.}
As with the
other quantifiers, $\nabla_\tau x. B$ abbreviates $\nabla_\tau \lambda
x. B$ and the type subscripts are often suppressed for readability.

\subsection{Generic Judgments and $\nabla$-quantification}

Sequents in intuitionistic logic can be written as
\[\Sigma : B_1, \ldots, B_n \lra B_0 \qquad (n\ge0)
\]
where $\Sigma$ is the ``global signature'' for the sequent that
contains the {\em eigenvariables} ({\em i.e.}, variables associated to
the $\existsL$ and $\forallR$ inference rules) relevant to the sequent
proof. We shall think of $\Sigma$ in this prefix position as an
operator that binds each of the variables it contains and that has the
rest of the sequent as its scope. To treat the $\nabla$-quantifier,
the \foldnb logic \cite{miller05tocl} extends the notion of a judgment
from just a formula to a formula paired with a ``local signature.''
Thus, sequents within this logic are written more elaborately as
\[\Sigma : \sigma_1\triangleright B_1, \ldots,
           \sigma_n\triangleright B_n \lra
           \sigma_0\triangleright B_0,
\]
where each $\sigma_0, \ldots, \sigma_n$ is a list of variables that are
bound locally in the formula adjacent to it.  Such local signatures
correspond to a proof-level encoding of binding that is expressed
within formulas through the $\nabla$-quantifier. In particular, the
judgment $x_1,\ldots,x_n\triangleright B$ and the formula $\nabla
x_1\cdots\nabla x_n.B$ for $n \ge 0$ have the same proof-theoretic
force. In keeping with this observation, we shall refer to a judgment
of the form $\sigma \triangleright B$ as a {\it generic judgment}.

As part of a generalization of sequents that bases them on generic
judgments rather than on formulas, we need to
define when two such judgments are equal: this is necessary
for describing at least the initial and cut inference rules.  The
\foldnb logic \cite{miller05tocl} uses a simple form of equality for
this purpose. It deems two generic judgments of the form
$x_1,\ldots,x_n\triangleright B$ and $y_1,\ldots,y_m\triangleright C$
to be equal exactly when the $\lambda$-terms $\lambda
x_1\ldots\lambda x_n. B$ and $\lambda y_1\ldots\lambda y_m. C$ are
$\lambda$-convertible; notice that this necessarily implies that
$n=m$. An equality notion is also needed in formulating an induction
rule. Unfortunately, the simple form of equality present in \foldnb
leads to a rather weak version of such a rule. To overcome this difficulty,
Tiu proposed the addition to the logic of two natural ``structural''
identities between generic judgments.
These identities are the $\nabla${\em -strengthening rule}
$\nabla x. F = F$, provided $x$ is not free in $F$, and the
$\nabla${\em -exchange rule} $\nabla x\nabla y. F = \nabla y\nabla
x. F$.  In its essence, the \LG proof system \cite{tiu06lfmtp} is
obtained from \foldnb by strengthening its notion of equality based on
$\lambda$-conversion through the addition of these two structural
rules for $\nabla$.


The move from the weaker
logic \foldnb to the stronger logic \LG involves an ontological
commitment and has a proof-theoretic consequence.

At the ontological level, the strengthening rule implies that every
type at which one is
willing to use $\nabla$-quantification is non-empty and, in fact,
contains an unbounded number of members.  For example, the formula
$\exists_\tau x.\top$ is always provable, even if there are no closed
terms of type $\tau$ because this formula is equivalent to
$\nabla_\tau y.\exists_\tau x.\top$, which is provable.
Similarly, for any given $n\geq 1$, the following formula
is provable
\[
\exists_\tau x_1\ldots\exists_\tau x_n.
 \left[\bigwedge_{1\leq i,j\leq n, i\not= j} x_i \not= x_j\right].
\]

At the proof-theoretic level, an acceptance of the strengthening and
exchange rules means
that the length of a local context and the order of variables within
it are unimportant.  For example, a sequent that contains the generic
judgments $x_1,\ldots,x_n\triangleright B$  and
$y_1,\ldots,y_m\triangleright C$ can be rewritten (assuming  $n\ge m$)
using
$\alpha$-conversion and strengthening into the judgments
 $z_1,\ldots,z_n\triangleright B'$  and
$z_1,\ldots,z_n\triangleright C'$ where $B'$ and $C'$ are
equal to $B$ and $C$ modulo variable renamings.  In this fashion, all
local bindings in a sequent can be made to involve the same
variables, and, hence, the local bindings can be seen as a global
binding over a sequent that contains formulas and not generic
judgments.  The resulting sequent-level variable bindings will be represented by
specially designated {\it nominal constants}.
Notice, however, that each of these nominal ``constants'' has as its scope only a single
formula.  Thus, we must distinguish the same nominal constant when it
appears in two different formulas and we should treat judgments
as being equal if they are identical up to permutations of these
constants.


\subsection{A Sequent Calculus Presentation of the Core Logic}

The logic \logic inherits from \LG the shift from a local to a global
scope in the treatment of the $\nabla$-quantifier.  In particular,
we assume that the collection of constants is partitioned into the set
$\mathcal{C}$ of nominal constants and the set $\mathcal{K}$ of
usual, non-nominal constants.
We assume the set $\mathcal{C}$ contains an infinite number of nominal
constants for each type at which $\nabla$ quantification is permitted.
We define the {\it support} of a term (or
formula), written $\supp(t)$, as the set of nominal constants
appearing in it.
A permutation of nominal constants is a type-preserving bijection $\pi$ from
$\mathcal{C}$ to $\mathcal{C}$ such that $\{ x\ |\ \pi(x) \neq x\}$ is
finite.  We denote the application of such a
permutation to a term or formula $t$ by $\pi . t$ and define this as
follows:
\[
\begin{array}{l@{\qquad\qquad }l}
\pi.a = \pi(a), \mbox{ if $a \in \mathcal{C}$} &
\pi.c = c, \mbox{ if $c\notin \mathcal{C}$ is atomic} \\
\pi.(\lambda x.M) = \lambda x.(\pi.M) &
\pi.(M\; N) = (\pi.M)\; (\pi.N)
\end{array}
\]
We extend the notion of equality between terms to encompass also
the application of permutations to nominal constants appearing in
them. Specifically, we write $B \approx B'$ to denote the fact that
there is a permutation $\pi$ such that $B$ $\lambda$-converts to
$\pi.B'$. Using the observations that permutations are invertible and
composable and that $\lambda$-convertibility is an equivalence
relation, it is easy to see that $\approx$ is also an equivalence
relation.

\begin{figure*}[t]
\small
\begin{align*}
\infer[id]{\Sigma : \Gamma, B \lra B'}{B \approx B'} &&
\infer[\cut]{\Sigma : \Gamma, \Delta \lra C} {\Sigma : \Gamma \lra B &
  \Sigma : B, \Delta \lra C} &&
\infer[\cL]{\Sigma : \Gamma, B \lra C} {\Sigma : \Gamma, B, B \lra C}
\end{align*}
\begin{align*}
\infer[\botL]{\Sigma :\Gamma,\bot \lra C}{} &&
\infer[\topR]{\Sigma : \Gamma \lra \top}{}
\end{align*}
\begin{align*}
\infer[\lorL]{\Sigma :\Gamma,B\lor D \lra C} {\Sigma :\Gamma,B\lra C &
  \Sigma:\Gamma,D\lra C} &&
\infer[\lorR,i\in\{1,2\}]{\Sigma : \Gamma \lra B_1 \lor B_2} {\Sigma :
  \Gamma \lra B_i}
\end{align*}
\begin{align*}
\infer[\landL,i\in\{1,2\}]{\Sigma : \Gamma, B_1 \land B_2 \lra
  C}{\Sigma : \Gamma, B_i \lra C} &&
\infer[\landR]{\Sigma : \Gamma \lra B \land C}{\Sigma : \Gamma \lra B
  & \Sigma : \Gamma \lra C}
\end{align*}
\begin{align*}
\infer[\supsetL]{\Sigma : \Gamma, B \supset D \lra C} {\Sigma : \Gamma
  \lra B & \Sigma : \Gamma, D \lra C} &&
\infer[\supsetR]{\Sigma : \Gamma \lra B \supset C} {\Sigma : \Gamma, B
  \lra C}
\end{align*}
\begin{align*}
\infer[\forallL]{\Sigma : \Gamma, \forall_\tau x.B \lra C} {\Sigma,
  \mathcal{K}, \mathcal{C} \vdash t : \tau & \Sigma : \Gamma, B[t/x]
  \lra C} &&
\infer[\forallR,h\notin\Sigma, \supp(B)=\{\vec{c}\}]
{\Sigma : \Gamma \lra \forall x.B} {\Sigma, h : \Gamma \lra B[h\
  \vec{c}/x]}
\end{align*}
\begin{align*}
\infer[\existsL,h\notin\Sigma,\supp(B)=\{\vec{c}\}] {\Sigma : \Gamma,
  \exists x. B \lra C} {\Sigma, h : \Gamma, B[h\; \vec{c}/x] \lra C} &&
\infer[\existsR]{\Sigma : \Gamma \lra \exists_\tau x.B} {\Sigma,
  \mathcal{K}, \mathcal{C} \vdash t:\tau & \Sigma : \Gamma \lra
  B[t/x]}
\end{align*}
\begin{align*}
\infer[\nablaL,a\notin \supp(B)] {\Sigma : \Gamma, \nabla x. B \lra C}
{\Sigma : \Gamma, B[a/x] \lra C} &&
\infer[\nablaR,a\notin \supp(B)]
{\Sigma : \Gamma \lra \nabla x.B} {\Sigma : \Gamma \lra B[a/x]}
\end{align*}
\caption{The core rules of \logic}
\label{fig:core-rules}
\end{figure*}

The rules defining the core of \logic are presented in Figure
\ref{fig:core-rules}. Sequents in this logic have the form $\Sigma :
\Gamma \lra C$ where $\Gamma$ is a multiset and the signature $\Sigma$
contains all the free variables of $\Gamma$ and $C$. We use
expressions of the form $B[t/x]$ in the quantifier rules to denote the
result of substituting the term $t$ for $x$ in the formula $B$. Note
that such a substitution must be done carefully, making sure to rename
bound variables in $B$ to avoid capture of variables appearing in $t$.
In the $\nabla\mathcal{L}$ and $\nabla\mathcal{R}$ rules, $a$ denotes
a nominal constant of an appropriate type. In the $\exists\mathcal{L}$
and $\forall\mathcal{R}$ rule we use raising \cite{miller92jsc} to
encode the dependency of the quantified variable on the support of
$B$; the expression $(h\ \vec{c})$ in which $h$ is a fresh
eigenvariable is used in these two rules to denote the (curried)
application of $h$ to the constants appearing in the sequence
$\vec{c}$. The $\forall\mathcal{L}$ and $\exists\mathcal{R}$ rules
make use of judgments of the form $\Sigma, \mathcal{K}, \mathcal{C}
\vdash t : \tau$. These judgments enforce the requirement that the
expression $t$ instantiating the quantifier in the rule is a
well-formed term of type $\tau$ constructed from the eigenvariables in
$\Sigma$ and the constants in ${\cal K} \cup {\cal C}$. Notice that in
contrast the $\forall\mathcal{R}$ and $\exists\mathcal{L}$ rules seem
to allow for a dependency on only a restricted set of nominal
constants. However, this asymmetry is not significant:
Corollary~\ref{cor:extend} in Section~\ref{sec:meta-theory} will tell
us that the dependency expressed through raising in the latter rules
can be extended to any number of nominal constants that are not in the
relevant support set without affecting the provability of sequents.

Equality modulo $\lambda$-conversion is built into the rules in
Figure~\ref{fig:core-rules}, and also into later extensions of
this logic, in a fundamental way: in particular, proofs are preserved
under the replacement of formulas in sequents by ones to which they
$\lambda$-convert.  A more involved observation
is that we can replace a formula $B$ in a sequent by another formula
$B'$ such that $B\approx B'$ without affecting the provability of the
sequent or even the very structure of the proof. For the core logic,
this observation follows from the form of the $id$ rule and the fact
that permutations distribute over logical structure. We shall prove
this property explicitly for the full logic in
Chapter~\ref{ch:meta-theory}.

\section{Characterizing Occurrences of Nominal Constants}
\label{sec:nominal-abstraction}

We are interested in adding to our logic the capability of
characterizing occurrences of nominal constants within terms and also
of analyzing the structure of terms with respect to such
occurrences. For example, we may want to define a predicate called
{\em name} that holds of a term exactly when that term is a nominal
constant. Similarly, we might need to identify a binary relation
called {\em fresh} that holds between two terms just in the case that
the first term is a nominal constant that does not occur in the second
term. Towards supporting such possibilities, we define in this section
a special binary relation called {\it nominal abstraction} and then
present proof rules that incorporate an understanding of this relation
into the logic. A formalization of these ideas requires a careful
treatment of substitution. In particular, this operation must be
defined to respect the intended formula-level scope of nominal
constants. We begin our discussion with an elaboration of this aspect.

\subsection{Substitutions and their Interaction with Nominal Constants}

The following definition reiterates a common view of substitutions
in logical contexts.

\begin{definition}\label{subst}
A substitution is a type preserving mapping from variables
to terms that is the identity at all but a finite number of variables.
The domain of a substitution is the set of variables that are
not mapped to themselves and its range is the
set of terms resulting from applying it to the variables in its
domain.  We write a substitution as $\{t_1/x_1,\ldots,t_n/x_n\}$
where $x_1,\ldots,x_n$ is a list of variables that contains the
domain of the substitution and $t_1,\ldots,t_n$ is the value of the
map on these variables. The support of a substitution $\theta$,
written as $\supp(\theta)$, is the set of nominal constants that appear in
the range of $\theta$. The restriction of a substitution $\theta$ to
the set of variables $\Sigma$, written as $\theta \restrict
\Sigma$, is a mapping that is like $\theta$ on the variables in
$\Sigma$ and the identity everywhere else.
\end{definition}

A substitution essentially calls for the replacement of
variables by their associated terms in any context to which it is
applied. A complicating factor in our setting is that nominal
constants can appear in the terms that are to replace
particular variables. A substitution may be determined relative to one
formula in a sequent but may then have to be applied to other formulas
in the same sequent. In doing this, we have to take into account the
fact that the scopes of the implicit quantifiers over nominal
constants are restricted to individual formulas. Thus, the logically
correct application of a substitution should be accompanied by a
renaming of these constants in the term being substituted into so as to
ensure that they are not confused with the ones appearing in
the range of the substitution.

\begin{definition}\label{ncasubst}
The ordinary application of a substitution $\theta$ to a term $B$ is
denoted by $B[\theta]$ and corresponds to the replacement of the
variables in $B$ by the terms that $\theta$ maps them to, making sure,
as usual, to avoid accidental binding of the variables appearing in
the range of $\theta$. More precisely, if $\theta = \{t_1/x_1,\ldots,
t_n/x_n\}$, then $B[\theta]$ is the term $(\lambda x_1\ldots\lambda x_n.B)\;
t_1\; \ldots\; t_n$; this term is, of course, considered to be equal
to any other term that it $\lambda$-converts to. By contrast,
the {\em nominal capture avoiding application} of $\theta$ to $B$ is
written as $B\cas{\theta}$ and is defined as follows. Assuming that
$\pi$ is a permutation of nominal constants that maps those appearing
in $supp(B)$ to ones not appearing in $\supp(\theta)$, let $B' =
\pi.B$. Then $B\cas{\theta} = B'[\theta]$.
\end{definition}

The notation $B[\theta]$ generalizes the one
used in the quantifier rules in Figure~\ref{fig:core-rules}.
The definition of the nominal capture avoiding application of a
substitution is ambiguous in that we do not uniquely specify the
permutation to be used.  We resolve this ambiguity by deeming as
acceptable {\it any} permutation that avoids conflicts. As a special
instance of the lemma below, we see that for any given formula $B$ and
substitution $\theta$,
all the possible values for $B\cas{\theta}$ are equivalent modulo the
$\approx$ relation. Moreover, as we show in
Chapter~\ref{ch:meta-theory}, formulas that are equivalent under
$\approx$ are interchangeable in the contexts of proofs.

\begin{lemma}
\label{lem:approx-cas}
If $t \approx t'$ then $t\cas{\theta} \approx t'\cas{\theta}$.
\end{lemma}
\begin{proof}
Let $t$ be $\lambda$-convertible to $\pi_1.t'$, let $t\cas{\theta} =
(\pi_2.t)[\theta]$ where $\supp(\pi_2.t) \cap \supp(\theta) =
\emptyset$, and let $t'\cas{\theta}$ be $\lambda$-convertible to
$(\pi_3.t')[\theta]$ where $\supp(\pi_3.t')\cap \supp(\theta) =
\emptyset$. Then we define a function $\pi$ partially by the following
rules:
\begin{enumerate}
\item $\pi(c) = \pi_2.\pi_1.\pi_3^{-1}(c)$ if $c \in \supp(\pi_3.t')$ and
\item $\pi(c) = c$ if $c \in \supp(\theta)$.
\end{enumerate}
Since $\supp(\pi_3.t') \cap \supp(\theta) = \emptyset$, these rules
are not contradictory, \ie, this (partial) function is well-defined.
The range of the first rule is $\supp(\pi_2.\pi_1.\pi_3^{-1}.\pi_3.t')
= \supp(\pi_2.\pi_1.t') = \supp(\pi_2.t)$ which is disjoint from the
range of the second rule, $\supp(\theta)$. Since the mapping in each
rule is determined by a permutation, these rules together define a
one-to-one partial mapping that can be extended to a bijection on
$\mathcal{C}$. We take any such extension to be the complete
definition of $\pi$ that must therefore be a permutation.

To prove that $t\cas{\theta} \approx t'\cas{\theta}$ it suffices to
show that $(\pi_2.t)[\theta]$ is $\lambda$-convertible to
$\pi.((\pi_3.t')[\theta])$. We do this by induction on the structure
of $t'$ under the further assumption that $t$ $\lambda$-converts to
$\pi_1.t'$. Suppose $t'$ is an abstraction. Then, it is easy to see
that $(\pi_2.t)[\theta]$ $\lambda$-converts to $\lambda x.
((\pi_2.s)[\theta])$ and $\pi.((\pi_3.t')[\theta])$ $\lambda$-converts
to $\lambda x. (\pi.((\pi_3.s')[\theta]))$ for some choice of variable
$x$ and terms $s$ and $s'$ such that $s'$ is structurally less complex
than $t'$ and $s$ $\lambda$-converts to $\pi_1.s'$. But then, by the
induction hypothesis, $(\pi_2.s)[\theta]$ $\lambda$-converts to
$\pi.((\pi_3.s')[\theta])$ and hence $(\pi_2.t)[\theta]$ is
$\lambda$-convertible to $\pi.((\pi_3.t')[\theta])$. A similar and, in
fact, simpler argument can be provided in the case where $t'$ is an
application. If $t'$ is a nominal constant $c$ then
$(\pi_2.t)[\theta]$ must be $\lambda$-convertible to
$(\pi_2.\pi_1.c)[\theta] = \pi_2.\pi_1.c$. Also,
$\pi.((\pi_3.t')[\theta])$ must be $\lambda$-convertible to
$\pi.\pi_3.c$. Further, in this case the first rule for $\pi$ applies
which means $\pi.\pi_3.c = \pi_2.\pi_1.\pi_3^{-1}.\pi_3.c =
\pi_2.\pi_1.c$. Thus $(\pi_2.t)[\theta]$ is again
$\lambda$-convertible to $\pi.((\pi_3.t')[\theta])$. Finally, suppose
$t'$ is a variable $x$. In this case $t$ must be $\lambda$-convertible
to $x$ so that we must show $x[\theta]$ $\lambda$-converts to
$\pi.(x[\theta])$. If $x$ does not have a binding in $\theta$ then
both terms are equal. Alternatively, if $x[\theta] = s$ then $\pi.s =
s$ by the second rule for $\pi$ and so the two terms are again equal.
Thus $(\pi_2.t)[\theta]$ $\lambda$-converts to
$\pi.((\pi_3.t')[\theta])$, as is required.
\end{proof}

The nominal capture avoiding application of substitutions turns out to
be the dominant notion in the analysis of provability.
For this reason, when we speak of the application of a
substitution in an unqualified way, we shall mean the nominal capture
avoiding form of this notion.

We shall need to consider the composition of substitutions later in
this section. The definition of this notion must also pay attention to
the presence of nominal constants.

\begin{definition}\label{nascomp}
Given a substitution $\theta$ and a permutation $\pi$ of nominal
constants, let $\pi.\theta$ denote
the substitution that is obtained by replacing each $t/x$ in $\theta$
with $(\pi.t)/x$. Given any two substitutions $\theta$ and $\rho$, let
$\theta \circ \rho$ denote the substitution that is such that
$B[\theta\circ \rho] = B[\theta][\rho]$. In this context, the {\em
  nominal capture   avoiding composition} of $\theta$ and $\rho$ is
written as $\theta\bullet\rho$ and defined as follows. Let $\pi$ be a
permutation of nominal constants such that
$\supp(\pi.\theta)$ is disjoint from $\supp(\rho)$. Then
$\theta\bullet\rho = (\pi.\theta)\circ \rho$.
\end{definition}\label{substequiv}
The notation $\theta \circ \rho$ in the above definition represents
the usual composition of $\theta$ and $\rho$ and can, in fact, be
given in an explicit form based on these substitutions. Thus, $\theta
\bullet \rho$ can also be presented in an explicit form. Notice that our
definition of nominal capture avoiding composition is, once again,
ambiguous because it does not fix the permutation to be used,
accepting instead any one that satisfies the constraints. However, as
before, this ambiguity is harmless. To understand this, we first
extend the notion of equivalence under permutations to substitutions.
\begin{definition}
Two substitutions $\theta$ and $\rho$ are considered to be permutation
equivalent, written $\theta \approx \rho$, if and only if there is a
permutation of nominal constants $\pi$ such that $\theta =
\pi.\rho$. This notion of equivalence may also be parameterized by a
set of variables $\Sigma$ as follows: $\theta \approx_\Sigma \rho$
just in the case that $\theta \restrict \Sigma \approx \rho \restrict
\Sigma$.
\end{definition}
It is easy to see that all possible choices for $\theta \bullet \rho$
are permutation equivalent and that if $\varphi_1 \approx \varphi_2$
then $B\cas{\varphi_1} \approx B\cas{\varphi_2}$ for any term $B$.
Thus, if our focus is on provability, the ambiguity in
Definition~\ref{nascomp} is inconsequential by a result to be
established in Chapter~\ref{ch:meta-theory}. As a further observation,
note that $B\cas{\theta\bullet\rho} \approx B\cas{\theta}\cas{\rho}$
for any $B$. Hence our notion of nominal capture avoiding composition
of substitutions is sensible.

The composition operation can be used to define an ordering
relation between substitutions:
\begin{definition}\label{nasordering}
Given two substitutions $\rho$ and $\theta$, we say $\rho$ is {\em
  less general than} $\theta$, notated as $\rho \leq \theta$, if and
only if there exists a $\sigma$ such that $\rho \approx
\theta\bullet\sigma$. This relation can also be parameterized by a
set of variables: $\rho$ is less general than $\theta$
relative to $\Sigma$, written as $\rho \leq_\Sigma \theta$, if and
only if $\rho
\restrict \Sigma \leq \theta \restrict \Sigma$.
\end{definition}
The notion of generality between substitutions that is based on
nominal capture avoiding composition has a different flavor from that
based on the traditional form of substitution composition. For
example, if $a$ is a nominal constant, the substitution $\{a/x\}$ is
strictly less general than $\{a/x, y' a/y\}$ relative to $\Sigma$ for
any $\Sigma$ which contains $x$ and $y$. To see this, note that we can
compose the latter substitution with $\{(\lambda z.y)/y'\}$ to obtain
the former, but the naive attempt to compose the former with
$\{y'a/y\}$ yields $\{b/x, y'a/y\}$ where $b$ is a nominal constant
distinct from $a$. In fact, the ``most general'' solution relative to
$\Sigma$ containing $\{a/x\}$ will be $\{a/x\}\cup \{z'a/z \mid
z\in\Sigma\backslash\{x\}\}$.

\subsection{Nominal Abstraction}

The nominal abstraction relation allows implicit formula-level
bindings represented by nominal constants to be moved into explicit
abstractions over terms. The following notation is useful for
defining this relationship.

\begin{notation}
Let $t$ be a term, let $c_1,\ldots,c_n$ be distinct nominal constants that
possibly occur in $t$, and let $y_1,\ldots,y_n$ be distinct variables
not occurring in $t$ and such that, for $1 \leq i \leq n$, $y_i$ and
$c_i$ have the same type. Then we write $\lambda c_1
\ldots\lambda c_n. t$ to denote the term $\lambda y_1 \ldots \lambda
y_n . t'$ where $t'$ is the term obtained from $t$ by replacing
$c_i$ by $y_i$ for $1\leq i\leq n$.
\end{notation}

There is an ambiguity in the notation introduced above in that
the choice of variables $y_1,\ldots,y_n$ is not fixed. However, this
ambiguity is harmless: the terms that are produced by acceptable
choices are all equivalent under a renaming of bound variables.

\begin{definition}\label{nominal-abstraction}
Let $n\ge 0$ and
let $s$ and $t$ be terms of type $\tau_1 \to \cdots \to \tau_n \to
\tau$ and $\tau$, respectively; notice, in particular, that $s$ takes
$n$ arguments to yield a term of the same type as $t$.
Then the expression $s \unrhd t$ is a formula that is referred to as a
nominal abstraction of degree $n$ or simply as a nominal abstraction. The
symbol $\unrhd$ is used here in an overloaded way in that the degree
of the nominal abstraction it participates in can vary.
The nominal abstraction $s \unrhd t$ of degree $n$ is said to hold just in
the case that $s$ $\lambda$-converts to $\lambda c_1\ldots c_n.t$ for
some nominal constants $c_1,\ldots,c_n$.
\end{definition}

Clearly, nominal abstraction of degree $0$ is the same as equality
between terms based on $\lambda$-conversion, and we will therefore use
$=$  to denote this relation in that situation. In the more general
case,
the term on the left of the operator serves as a pattern for isolating
occurrences of nominal constants. For example, the relation $(\lambda
x.x) \unrhd t$ holds exactly when $t$ is a nominal constant.



The symbol $\unrhd$ corresponds, at the moment, to a mathematical
relation that holds between pairs of terms as explicated by
Definition~\ref{nominal-abstraction}. We now overload this symbol by
treating it also as a binary predicate symbol of \logic. In the next
subsection we shall add inference rules to make the mathematical
understanding of $\unrhd$ coincide with its syntactic use as a
predicate in sequents. It is, of course, necessary to be able to
determine when we mean to use $\unrhd$ in the mathematical sense and
when as a logical symbol. When we write an expression such as $s\unrhd
t$ without qualification, this should be read as a logical formula
whereas if we say that ``$s\unrhd t$ holds'' then we are referring to
the abstract relation from Definition~\ref{nominal-abstraction}. We
might also sometimes use an expression such as ``$(s\unrhd
t)\cas{\theta}$ holds.'' In this case, we first treat $s \unrhd t$ as
a formula to which we apply the substitution $\theta$ in a nominal
capture avoiding way to get a (syntactic) expression of the form
$s'\unrhd t'$. We then read $\unrhd$ in the mathematical sense,
interpreting the overall expression as the assertion that ``$s'\unrhd
t'$ holds.'' Note in this context that $s \unrhd t$ constitutes a
single formula when read syntactically and hence the expression
$(s\unrhd t)\cas{\theta}$ is, in general, {\it not} equivalent to the
expression $s\cas{\theta}\unrhd t\cas{\theta}$.

In the proof-theoretic setting, nominal abstraction will be used with
terms that contain free occurrences of variables for which
substitutions can be made. The following definition is relevant to
this situation.

\begin{definition}\label{nasolution}
A substitution $\theta$ is said to be a solution to the nominal
abstraction $s \unrhd t$ just in the case that $(s\unrhd t)\cas{\theta}$ holds.
\end{definition}

Solutions to a nominal abstraction can be used to provide rich
characterizations of the structures of terms. For example, consider
the nominal abstraction
$(\lambda x.\fresh x T) \unrhd S$ in which $T$ and $S$ are
variables and {\sl fresh} is a binary predicate symbol.  Any solution
to this problem requires that $S$ be
substituted for by a term of the form $\fresh a R$ where $a$
is a nominal constant and $R$ is a term in which $a$ does not appear,
\ie, $a$ must be ``fresh'' to $R$.

An important property of solutions to a nominal abstraction is that
these are preserved under permutations to nominal constants. We
establish this fact in the lemma below; this lemma will be used later
in showing the stability of the provability of sequents with
respect to the replacement of formulas by ones they are equivalent to
modulo the $\approx$ relation.

\begin{lemma}
\label{lem:na-approx}
Suppose $(s\unrhd t) \approx (s'\unrhd t')$. Then $s\unrhd t$ and
$s'\unrhd t'$ have exactly the same solutions. In particular,
$s\unrhd t$ holds if and only if $s'\unrhd t'$ holds.
\end{lemma}
\begin{proof}
We prove the particular result first. It suffices to only show it in
the forward direction since $\approx$ is symmetric. Let $\pi$ be the
permutation such that the expression $s'\unrhd t'$ $\lambda$-converts to
$\pi.(s\unrhd t)$. Now suppose $s \unrhd t$ holds since $s$
$\lambda$-converts to $\lambda\vec{c}.t$. Then $s'$ will
$\lambda$-convert to $\lambda(\pi.\vec{c}).t'$ where $\pi.\vec{c}$ is
the result of applying $\pi$ to each element in the sequence $\vec{c}$.
Thus $s' \unrhd t'$ holds.

For the general result it again suffices to show it in one direction,
\ie, that all the solutions of $s\unrhd t$ are solutions to $s'\unrhd
t'$. Let $\theta$ be a substitution such that $(s\unrhd
t)\cas{\theta}$ holds. By Lemma~\ref{lem:approx-cas}, $(s\unrhd
t)\cas{\theta} \approx (s'\unrhd t')\cas{\theta}$. Thus by the
particular result from the first half of this proof, $(s'\unrhd
t')\cas{\theta}$ holds.
\end{proof}

\subsection{Proof Rules for Nominal Abstraction}

\begin{figure}[t]
\small
\begin{align*}
\infer[\unrhdL]{\Sigma : \Gamma, s \unrhd t \lra C}
{\left\{\Sigma\theta : \Gamma\cas{\theta} \lra C\cas{\theta} \;|\;
  \hbox{$\theta$ is a solution to $(s \unrhd t)$}
  \right\}_\theta}
&&
\infer[\unrhdR,\ \hbox{$s \unrhd t$ holds}]
{\Sigma : \Gamma \lra s \unrhd t}
{}
\end{align*}
\caption{Nominal abstraction rules}
\label{fig:na-rules}
\bigskip
\begin{equation*}
\infer[\unrhdL_{\CSNAS}]
       {\Sigma : \Gamma, s \unrhd t \lra C}
       {\left\{\Sigma\theta : \Gamma\cas{\theta} \lra C\cas{\theta}
            \;|\;
          \theta \in \CSNAS(\Sigma, s, t)
        \right\}_\theta}
\end{equation*}
\caption{A variant of $\unrhdL$ based on \CSNAS}
\label{fig:csnas}
\end{figure}

We now add the left and right introduction rules for $\unrhd$ that are
shown in Figure~\ref{fig:na-rules} to link its use as a predicate
symbol to its mathematical interpretation. The expression $\Sigma
\theta$ in the $\unrhdL$ rule denotes the application of a
substitution $\theta=\{t_1/x_1,\ldots,t_n/x_n\}$ to the signature
$\Sigma$ that is defined to be the signature that results from
removing from $\Sigma$ the variables $\{x_1,\ldots,x_n\}$ and then
adding every variable that is free in any term in
$\{t_1,\ldots,t_n\}$. Notice also that in the same inference rule the
operator $\cas{\theta}$ is applied to a multiset of formulas in the
natural way: $\Gamma\cas{\theta}=\{B\cas{\theta}\;|\; B\in\Gamma\}$.
Note that the $\unrhdL$ rule has an {\it a priori} unspecified number
of premises that depends on the number of substitutions that are
solutions to the relevant nominal abstraction. If $s \unrhd t$
expresses an unsatisfiable constraint, meaning that it has no
solutions, then the premise of $\unrhdL$ is empty and the rule
provides an immediate proof of its conclusion.

The $\unrhdL$ and $\unrhdR$ rules capture nicely the intended
interpretation of nominal abstraction. However, there is an obstacle
to using the former rule in derivations: this rule has an infinite
number of premises any time the nominal abstraction $s \unrhd t$ has a
solution. We can overcome this difficulty by describing a rule that
includes only a few of these premises but in such way that their
provability ensures the provability of all the other premises.  Since
the provability of $\Gamma \lra C$ implies the provability of
$\Gamma\cas{\theta} \lra C\cas{\theta}$ for any $\theta$ (a property
established formally in Chapter\ref{ch:meta-theory}), if the first
sequent is a premise of an occurrence of the $\unrhdL$ rule, the
second does not need to be used as a premise of that same rule
occurrence.  Thus, we can limit the set of premises to be considered
if we can identify with any given nominal abstraction a (possibly
finite) set of solutions from which any other solution can be obtained
through composition with a suitable substitution. The following
definition formalizes the idea of such a ``covering set.''

\begin{definition}\label{csnas}
A {\em complete set of nominal abstraction solutions} (\CSNAS) of $s$
and $t$ on $\Sigma$
is a set $S$ of substitutions such
that
\begin{enumerate}
\item each $\theta \in S$ is a solution to $s \unrhd t$, and
\item for every solution $\rho$ to $s \unrhd t$, there exists a
$\theta \in S$ such that $\rho \leq_\Sigma \theta$.
\end{enumerate}
We denote any such set by $\CSNAS(\Sigma, s, t)$.
\end{definition}
Using this definition we present an alternative version of $\unrhdL$
in Figure~\ref{fig:csnas}. Note that if we can find a finite complete
set of nominal abstraction solutions then the number of premises to
this rule will be finite.


\begin{theorem}\label{thm:csnas}
The rules $\unrhdL$ and $\unrhdL_{\CSNAS}$ are inter-admissible.
\end{theorem}
\begin{proof}
Suppose we have the following arbitrary instance of $\unrhdL$ in a
derivation:
\begin{equation*}
\infer
 [\unrhdL]
 {\Sigma : \Gamma, s \unrhd t \lra C}
 {\left\{
\Sigma\theta : \Gamma\cas{\theta} \lra C\cas{\theta} \;|\;
     \hbox{$\theta$ is a solution to $(s \unrhd t)$}\right\}_\theta}
\end{equation*}
This rule can be replaced with a use of $\unrhdL_{\CSNAS}$ instead if
we could be certain that, for each $\rho \in \CSNAS(\Sigma,s,t)$, it is
the case that $\Sigma\rho : \Gamma\cas{\rho} \lra
C\cas{\rho}$ is included in the set of premises of the shown rule
instance. But this must be the case: by the
definition  of $\CSNAS$, each such $\rho$ is a solution to $s \unrhd
t$.

In the other direction, suppose we have the following arbitrary
instance of $\unrhdL_{\CSNAS}$.
\begin{equation*}
\infer
 [\unrhdL_{\CSNAS}]
 {\Sigma : \Gamma, s \unrhd t \lra C}
 {\left\{
\Sigma\theta : \Gamma\cas{\theta} \lra C\cas{\theta}
      \;|\;
    \theta \in \CSNAS(\Sigma, s, t)\right\}_\theta}
\end{equation*}
To replace this rule with a use of the $\unrhdL$ rule
instead, we need to be able to construct a
derivation of $\Sigma\rho : \Gamma\cas{\rho} \lra C\cas{\rho}$ for
each $\rho$ that is a solution to $s \unrhd t$. By the definition of
$\CSNAS$, we know that for any such $\rho$ there exists a $\theta \in
\CSNAS(\Sigma,s,t)$ such that $\rho \leq_\Sigma \theta$, \ie, such
that there
exists a $\sigma$ for which $\rho\restrict\Sigma \approx
(\theta\restrict\Sigma) \bullet \sigma$. Since we are considering the
application of these substitutions to a sequent all of whose
eigenvariables are contained in $\Sigma$, we can drop the restriction
on the substitutions and suppose that $\rho \approx \theta \bullet
\sigma$. Now, we shall show in Chapter~\ref{ch:meta-theory} that if a
sequent has a derivation then the result of applying a substitution to
it in a nominal capture-avoiding way produces a sequent that also has
a derivation. Using this observation, it follows that
$\Sigma\theta\sigma : \Gamma\cas{\theta}\cas{\sigma} \lra
C\cas{\theta}\cas{\sigma}$  has a proof. But this sequent is
permutation equivalent to $\Sigma\rho : \Gamma\cas{\rho} \lra
C\cas{\rho}$ which must, again by a result established explicitly in
Chapter~\ref{ch:meta-theory}, also have a proof.
\end{proof}

Theorem~\ref{thm:csnas} allows us to choose which of the left rules we
wish to consider in any given context. We shall assume the $\unrhdL$
rule in the formal treatment in the rest of this thesis, leaving the
use of the $\unrhdL_{\CSNAS}$ rule to practical applications of the
logic.


\subsection{Computing Complete Sets of Nominal Abstraction
  Solutions}\label{ssec:complete-sets}

For the $\unrhdL_{CSNAS}$ rule to be useful, we need an effective way
to compute restricted complete sets of nominal abstraction
solutions. We show here that the task of finding such complete sets of
solutions can be reduced to that of finding complete sets of unifiers
(\CSU) for higher-order unification problems \cite{huet75tcs}. In the
straightforward approach to finding a solution to a nominal
abstraction $s \unrhd t$, we would first identify a substitution
$\theta$ that we apply to $s \unrhd t$ to get $s' \unrhd t'$ and we
would subsequently look for nominal constants to abstract from $t'$ to
get $s'$.  To relate this problem to the usual notion of unification,
we would like to invert this order: in particular, we would like to
consider all possible ways of abstracting over nominal constants first
and only later think of applying substitutions to make the terms
equal. The difficulty with this second approach is that we do not know
which nominal constants might appear in $t'$ until after the
substitution is applied. However, there is a way around this
problem. Given the nominal abstraction $s \unrhd t$ of degree $n$, we
first consider substitutions for the variables occurring in it that
introduce $n$ new nominal constants in a completely general way.  Then
we consider all possible ways of abstracting over the nominal
constants appearing in the altered form of $t$ and, for each of these
cases, we look for a complete set of unifiers.

The idea described above is formalized in the following definition and
associated theorem. We use the notation $\CSU(s,t)$ in them to denote
an arbitrary but fixed selection of a complete set of unifiers for
the terms $s$ and $t$.

\begin{definition}\label{def:s}
Let $s$ and $t$ be terms of type $\tau_1 \to \ldots \to \tau_n
\to \tau$ and $\tau$, respectively. Let $c_1,\ldots,c_n$ be $n$
distinct nominal constants disjoint from $\supp(s\unrhd t)$ such that,
for $1 \leq i \leq n$, $c_i$ has the type $\tau_i$. Let $\Sigma$ be a
set of variables and for each $h \in \Sigma$ of type $\tau'$, let
$h'$ be a distinct variable not in $\Sigma$ that has type
$\tau_1\to \ldots\to\tau_n\to \tau'$. Let $\sigma = \{h'\ c_1\ \ldots\
c_n/h \mid h \in \Sigma\}$ and let $s' = s[\sigma]$ and $t' =
t[\sigma]$. Let
\begin{equation*}
C = \bigcup_{\vec{a}} \CSU(\lambda \vec{b}.s', \lambda \vec{b}. \lambda\vec{a}.t')
\end{equation*}
where $\vec{a}= a_1,\ldots,a_n$ ranges over all selections of $n$
distinct nominal constants from $\supp(t)\cup \{\vec{c}\}$ such that,
for $1 \leq i \leq n$,
$a_i$ has type $\tau_i$ and $\vec{b}$ is some corresponding listing of
all the nominal constants in $s'$ and $t'$ that are not included in
$\vec{a}$.
Then we define
\begin{equation*}
S(\Sigma, s, t) = \{ \sigma \bullet \rho \mid \rho \in C \}
\end{equation*}
\end{definition}

The use of the substitution $\sigma$ above represents
another instance of the application of the general technique of
raising that allows
certain variables (the $h$ variables in this definition) whose
substitution instances might depend on certain nominal constants
($c_1,\ldots,c_n$ here) to be replaced by new variables of higher type
(the $h'$ variables) whose substitution instances are not allowed to
depend on those nominal constants. This technique was previously used
in the $\existsL$ and $\forallR$ rules presented in
Section~\ref{sec:logic}.

\begin{theorem}
$S(\Sigma, s, t)$ is a complete set of nominal abstraction solutions
for $s\unrhd t$ on $\Sigma$.
\end{theorem}
\begin{proof}
First note that $\supp(\sigma) \cap \supp(s\unrhd t) = \emptyset$ and
thus $(s\unrhd t)\cas{\theta}$ is equal to $(s' \unrhd t')$.
Now we must show that every element of $S(\Sigma, s, t)$ is a
solution to $s \unrhd t$. Let $\sigma\bullet\rho \in S(\Sigma, s, t)$
be an arbitrary element where $\sigma$ is as in Definition~\ref{def:s},
$\rho$ is from $\CSU(\lambda\vec{b}.s',
\lambda\vec{b}.\lambda\vec{a}.t')$, and $s' = s[\sigma]$ and $t' =
t[\sigma]$.  By the definition of $\CSU$ we know $(\lambda\vec{b}.s' =
\lambda\vec{b}.\lambda\vec{a}.t')[\rho]$. This means $(s' =
\lambda\vec{a}.t')\cas{\rho}$ holds and thus $(s' \unrhd
t')\cas{\rho}$ holds. Rewriting $s'$ and $t'$ in terms of $s$ and $t$ this
means $(s \unrhd t)\cas{\sigma}\cas{\rho}$. Thus $\sigma\bullet\rho$
is a solution to $s\unrhd t$.

In the other direction, we must show that if $\theta$ is a solution to $s
\unrhd t$ then there exists $\sigma\bullet\rho \in S(\Sigma, s, t)$
such that $\theta \le_\Sigma \sigma\bullet\rho$. Let $\theta$ be a
solution to $s\unrhd t$. Then we know $(s\unrhd t)\cas{\theta}$ holds.
The substitution $\theta$ may introduce some nominal constants which
are abstracted out of the right-hand side when determining equality, so
let us call these the {\em important} nominal constants. Let $\sigma =
\{h'\ c_1\ \ldots\ c_n/h \mid h \in \Sigma\}$ be as in
Definition~\ref{def:s} and let $\pi'$ be a permutation which maps the
important nominal constants of $\theta$ to nominal constants from
$c_1, \ldots, c_n$. This is possible since $n$ nominal constants are
abstract from the right-hand side and thus there are at most $n$
important nominal constants. Then let $\theta' = \pi'.\theta$, so that
$(s\unrhd t)\cas{\theta'}$ holds and it suffices to show that $\theta'
\le_\Sigma \sigma\bullet\rho$. Note that all we have done at this
point is to rename the important nominal constants of $\theta$ so that
they match those introduced by $\sigma$. Now we define $\rho' = \{
\lambda c_1\ldots\lambda c_n.r / h' \mid r / h \in \theta'\}$ so that
$\theta' = \sigma\bullet\rho'$. Thus $(s\unrhd
t)\cas{\sigma}\cas{\rho'}$ holds. By construction, $\sigma$ shares no
nominal constants with $s$ and $t$, thus we know $(s'\unrhd
t')\cas{\rho'}$ where $s' = s[\sigma]$ and $t' = t[\sigma]$. Also by
construction, $\rho'$ contains no interesting nominal constants and
thus $(s' = \lambda\vec{a}. t')\cas{\rho}$ holds for some nominal
constants $\vec{a}$ taken from $\supp(t) \cup \{\vec{c}\}$. If we let
$\vec{b}$ be a listing of all nominal constants in $s'$ and $t'$ but
not in $\vec{a}$, then $(\lambda\vec{b}. s' =
\lambda\vec{b}.\lambda\vec{a}. t')\cas{\rho}$ holds. At this point the inner
equality has no nominal constants and thus the substitution $\rho$ can
be applied without renaming: $(\lambda\vec{b}. s' =
\lambda\vec{b}.\lambda\vec{a}. t')[\rho']$ holds. By the definition of
$\CSU$, there must be a $\rho \in \CSU(\lambda\vec{b}.s',
\lambda\vec{b}.\lambda\vec{a}.t')$ such that $\rho' \le \rho$. Thus
$\sigma\bullet\rho' \le_\Sigma \sigma\bullet\rho$ as desired.
\end{proof}

\section{Definitions, Induction, and Co-induction}
\label{sec:definitions}

\begin{figure}[t]
\begin{center}
$\infer[\defL]
      {\Sigma : \Gamma, p\ \vec{t} \lra C}
      {\Sigma : \Gamma, B\ p\ \vec{t} \lra C}
\hspace{1in}
\infer[\defR]
      {\Sigma : \Gamma \lra p\ \vec{t}}
      {\Sigma : \Gamma \lra B\ p\ \vec{t}}
$
\end{center}
\caption{Introduction rules for atoms whose predicate is defined as $\forall
  \vec{x}.~p\ \vec{x} \triangleq B\ p\ \vec{x}$}
\label{fig:defrules}
\end{figure}


The sequent calculus rules presented in Figure~\ref{fig:core-rules}
treat atomic judgments as fixed, unanalyzed objects.
We now add the capability of defining such judgments by means of
formulas, possibly involving other predicates. In particular, we shall
assume that we are given a
fixed, finite set of \emph{clauses} of the
form $\forall \vec{x}.~p\ \vec{x} \triangleq B\ p\ \vec{x}$ where $p$
is a predicate constant that takes a number of arguments equal to the
length of $\vec{x}$. Such a clause is said to define $p$ and the
entire collection of clauses is called a {\em
  definition}. The expression $B$, called the {\em body} of the
clause, must be a term that does not contain $p$ or
any of the variables in $\vec{x}$ and must have a type such that
$B\ p\ \vec{x}$ has type $o$.  Definitions are also restricted so that
a predicate is defined by at most one clause.
The intended interpretation of a clause $\forall \vec{x}.~p\ \vec{x}
\triangleq B\ p\ \vec{x}$ is that the atomic
formula $p\ \vec{t}$, where $\vec{t}$ is a list of terms of the same
length and type as the variables in $\vec{x}$, is true if and only if
$B\ p\ \vec{t}$ is true.
This interpretation is realized by adding to the calculus the rules
$\defL$ and $\defR$ shown in Figure~\ref{fig:defrules} for unfolding
predicates on the left and the right of sequents using their defining
clauses.

A definition can have a recursive structure. For example, in the clause
$\forall \vec{x}.~p\ \vec{x} \triangleq B\ p\ \vec{x}$, the predicate
$p$ can appear free in $B\ p\ \vec{x}$.  In this setting, the meanings
of predicates are intended to be given by any one of the fixed points
that can be associated with the definition.  Such an interpretation may
not always be sensible. In particular, without further restrictions,
the resulting proof system may not be consistent.  There are two
constraints that suffice to ensure consistency. First, the body of a
clause must not contain any nominal constants. This restriction
can be justified from another perspective as well: as we see in
Chapter~\ref{ch:meta-theory}, it helps in establishing that $\approx$
is a provability preserving equivalence between formulas. Second, definitions
should be {\em stratified} so that clauses, such as $a\triangleq
(a\supset \bot)$, in which a predicate has a negative dependency on
itself, are forbidden.  While such stratification can be enforced in
different ways, we use a simple approach to doing this in this
thesis. This approach is based on associating with each predicate $p$
a natural number that is called its {\em level} and that is denoted
by $\lvl(p)$.  This measure is then extended to arbitrary formulas by
the following definition.
\begin{definition}
Given an assignment of levels to predicates, the function $\lvl$ is
extended to all formulas in $\lambda$-normal form as follows:
\begin{enumerate}
\item $\lvl(p\ \bar{t}) = \lvl(p)$
\item $\lvl(\bot) = \lvl(\top) = \lvl(s\unrhd t) = 0$
\item $\lvl(B\land C) = \lvl(B\lor C) = \max(\lvl(B),\lvl(C))$
\item $\lvl(B\supset C) = \max(\lvl(B)+1,\lvl(C))$
\item $\lvl(\forall x.B) = \lvl(\nabla x.B) = \lvl(\exists x.B) =
\lvl(B)$
\end{enumerate}
In general, the level of a formula $B$, written as $\lvl(B)$, is the
level of its $\lambda$-normal form.
\end{definition}

A definition is {\em stratified} if we can assign levels to predicates
in such a way that $\lvl(B\ p\ \vec{x}) \leq \lvl(p)$ for each clause
$\forall \vec{x}.~p\ \vec{x} \triangleq B\ p\ \vec{x}$ in that
definition.

\begin{figure}[t]
\begin{center}
$\infer[\IL]
      {\Sigma : \Gamma, p\; \vec{t} \lra C}
      {\vec{x} : B\; S\; \vec{x} \lra S\; \vec{x} \qquad
       \Sigma : \Gamma, S\; \vec{t} \lra C}$\\[5pt]
provided $p$ is defined as $\forall  \vec{x}.~ p\  \vec{x}
\mueq  B\ p\  \vec{x}$ and $S$ is a term that has the same type as $p$
      \\[15pt]
$\infer[\CIR]
      {\Sigma : \Gamma \lra p\; \vec{t}}
      {\Sigma : \Gamma \lra S\; \vec{t} \qquad
       \vec{x} : S\; \vec{x} \lra B\; S\; \vec{x}}
$\\[5pt]
provided $p$ is defined as $\forall  \vec{x}.~ p\  \vec{x}
\nueq  B\ p\  \vec{x}$ and $S$ is a term that has the same type as $p$
\end{center}
\caption{The induction left and co-induction right rules}
\label{fig:indandcoind}
\end{figure}

The $\defL$ and $\defR$ rules do not discriminate between any of the
fixed points of a definition.
We now allow the selection of least and greatest fixed points so as to
support inductive and co-inductive definitions of predicates.
Specifically, we denote an inductive clause by $\forall \vec{x}.~ p\
\vec{x} \mueq B\ p\ \vec{x}$ and a co-inductive one by $\forall
\vec{x}.~ p\ \vec{x} \nueq B\ p\ \vec{x}$. As a refinement of the
earlier restriction on definitions, a predicate may have at most one
defining clause that is designated to be inductive, co-inductive or
neither. The $\defL$ and $\defR$ rules may be used with clauses in any
one of these forms. Clauses that are inductive admit additionally the
left rule $\IL$ shown in Figure~\ref{fig:indandcoind}. This rule is
based on the observation that the least fixed point of a monotone
operator is the intersection of all its pre-fixed points; intuitively,
anything that follows from any pre-fixed point should then also follow
from the least fixed point. In a proof search setting, the term
corresponding to the schema variable $S$ in this rule functions like
the induction hypothesis and is accordingly called the invariant of
the induction. Clauses that are co-inductive, on the other hand, admit
the right rule $\CIR$ also presented in Figure~\ref{fig:indandcoind}.
This rule reflects the fact that the greatest fixed point of a
monotone operator is the union of all the post-fixed points; any
member of such a post-fixed point must therefore also be a member of
the greatest fixed point. The substitution that is used for $S$ in
this rule is called the co-invariant or the simulation of the
co-induction. Just like the restriction on the body of clauses, in
both $\IL$ and $\CIR$, the (co-)invariant $S$ must not contain any
nominal constants.

As a simple illustration of the use of these rules, consider the
clause $p \mueq p$. The desired
inductive reading of this clause
implies that $p$ must be false. In a proof-theoretic setting, we would
therefore expect that the sequent $\cdot : p \lra \bot$ can be
proved. This can, in fact, be done by using $\IL$ with the invariant
$S = \bot$. On the other hand, consider the clause $q
\nueq q$. The co-inductive reading intended here implies that $q$ must
be true. The logic \logic satisfies this expectation: the
sequent $\cdot : \cdot \lra q$ can be proved using $\CIR$ with the
co-invariant $S = \top$.

The addition of inductive and co-inductive forms of clauses and the
mixing of these forms in one setting might be expected to require
stronger conditions than those described earlier in this section to
guarantee consistency. One condition, in addition to the absence of
nominal constants in the bodies of clauses and stratification based on
levels, that suffices and that is also practically acceptable is the
following that is taken from \cite{tiu.momigliano}: in a clause of any
of the forms $\forall \vec{x}.~ p\ \vec{x} \triangleq B\ p\ \vec{x}$,
$\forall \vec{x}.~ p\ \vec{x} \mueq B\ p\ \vec{x}$ or $\forall
\vec{x}.~ p\ \vec{x} \nueq B\ p\ \vec{x}$, it must be that $\lvl(B\
(\lambda\vec{x}.\top)\ \vec{x}) < \lvl(p)$. This disallows any mutual
recursion between clauses, a restriction which can easily be overcome
by merging mutually recursive clauses into a single clause. We
henceforth assume that all definitions satisfy all three conditions
described for them in this section.
Corollary \ref{consistency} in
Chapter~\ref{ch:meta-theory} establishes the consistency of the logic
under these restrictions.

\section{A Pattern-Based Form for Definitions}
\label{sec:pattern-form}

When presenting a definition for a predicate, it is often convenient
to write this as a collection of clauses whose applicability is also
constrained by patterns appearing in the head. For example, in logics
that support equality but not nominal abstraction, list membership
may be defined by the two pattern based clauses shown below.
\begin{equation*}
\member X (X::L) \triangleq \top \hspace{2cm}
\member X (Y::L) \triangleq \member X L
\end{equation*}
These logics also include rules for directly treating definitions
presented in this way. In understanding these rules, use may be made
of the translation of the extended form of definitions to a version
that does not use patterns in the head and in which there is at most
one clause for each predicate. For example, the definition of the list
membership predicate would be translated to the following form:
\begin{equation*}
\member X K \triangleq (\exists L.~ K = (X :: L)) \lor
 (\exists Y \exists L.~ K = (Y :: L) \land \member X L)
\end{equation*}
The treatment of patterns and multiple clauses can now be understood
in terms of the rules for definitions using a single clause and the
rules for equality, disjunction, and existential quantification.

In the logic \logic, the notion of equality has been generalized to
that of nominal abstraction. This allows us also to expand the
pattern-based form of definitions to use nominal abstraction in
determining the selection of clauses. By doing this, we would allow
the head of a clausal definition to describe not only the term
structure of the arguments, but also to place restrictions on the
occurrences of nominal constants in these arguments.
For example, suppose we want to describe the contexts in typing
judgments by lists of the form $\of {c_1} {T_1} :: \of {c_2} {T_2} ::
\ldots :: nil$ with the further proviso that each $c_i$ is a distinct
nominal constant. We will allow this to be done by using the following
pattern-based form of definition for the predicate $\ctx$:
\begin{equation*}
\ctx nil \triangleq \top \hspace{2cm}
(\nabla x. \ctx (\of x T :: L)) \triangleq \ctx L
\end{equation*}
Intuitively, the $\nabla$ quantifier in the head of the second clause
imposes the requirement that, to match it, the argument of $\ctx$
should have the form $\of x T :: L$ where $x$ is a nominal constant
that does not occur in either $T$ or $L$. To understand this
interpretation, we could think of the earlier definition of {\sl ctx}
as corresponding to the following one that does not use patterns or
multiple clauses:
\begin{equation*}
\ctx K \triangleq (K = nil) \lor
(\exists T \exists L.~ (\lambda x . \of x T :: L) \unrhd K \land \ctx
L)
\end{equation*}
Our objective in the rest of this section is to develop machinery
for allowing the extended form of definitions to be used directly. We
do this by presenting its
syntax formally, by describing rules that allow us to work off of such
definitions and, finally, by justifying the new rules by means of a
translation of the kind indicated above.

\begin{definition}
A {\em pattern-based definition} is a finite collection of clauses of
the form
\[\forall \vec{x}.(\nabla \vec{z}. p\ \vec{t}) \triangleq
B\ p\ \vec{x}\] where $\vec{t}$ is a sequence of terms that do not
have occurrences of nominal constants in them, $p$ is a constant such
that $p\ \vec{t}$ is of type $o$ and $B$ is a term devoid of
occurrences of $p$, $\vec{x}$ and nominal constants and such that $B\
p\ \vec{t}$ is of type $o$. Further, we expect such a collection of
clauses to satisfy a stratification condition: there must exist an
assignment of levels to predicate symbols such that for any clause
$\forall \vec{x}.(\nabla \vec{z}. p\ \vec{t}) \triangleq B\ p\
\vec{x}$ occurring in the set, assuming $p$ has arity $n$, it is the
case that $\lvl(B\ (\lambda \vec{x}.\top)\ \vec{x}) <
\lvl(p)$. Notice that we allow the collection to contain more than one
clause for any given predicate symbol.
\end{definition}

\begin{figure}[t]
\begin{center}
$\infer[\defR^p]
      {\Sigma : \Gamma \lra p\; \vec{s}}
      {\Sigma : \Gamma \lra (B\; p\; \vec{x})[\theta]}$\\[5pt]
for any clause $\forall \vec{x}.(\nabla \vec{z}. p\ \vec{t}) \triangleq
B\ p\ \vec{x}$ in $\cal D$ and any $\theta$ such that
$range(\theta)\cap \Sigma = \emptyset$ and $(\lambda \vec{z} . p\ \vec{t})[\theta]
                        \unrhd p\ \vec{s}$ holds\\[20pt]
$\infer[\defL^p]
      {\Sigma : \Gamma, p\; \vec{s} \lra C}
      {\left\{
        \begin{tabular}{l|l}
         $\Sigma\theta : \Gamma\cas{\theta}, (B\; p\;
         \vec{x})\cas{\theta} \lra C\cas{\theta}$ &
           $\forall \vec{x}.(\nabla \vec{z}. p\ \vec{t}) \triangleq
                              B\ p\ \vec{x} \in {\cal D}$ and \\
        &
           $\theta$ is a solution to $((\lambda \vec{z} . p\ \vec{t}) \unrhd p\
                        \vec{s})$
       \end{tabular}
              \right\}
      }
$
\end{center}
\caption{Introduction rules for a pattern-based definition $\cal D$}
\label{fig:patterndefrules}
\end{figure}

The logical rules for treating pattern-based definitions are presented
in Figure~\ref{fig:patterndefrules}. These rules encode the
idea of matching an instance of a predicate with the head of a
particular clause and then replacing the predicate with the
corresponding clause body. The kind of matching involved is made
precise through the construction of a nominal abstraction after
replacing the $\nabla$ quantifiers in the head of the clause by
abstractions. The right rule embodies the fact that it is enough if
an instance of any one clause can be used in this way to yield a
successful proof. In this rule, the substitution $\theta$ that results
from the matching must be applied in a nominal capture avoiding way to
the body. However, since $B$ does not contain nominal constants,
the ordinary application of the substitution also suffices.
To accord with the treatment in the right rule, the left rule
must consider all possible ways in which an instance of an atomic
assumption  $p\ \vec{s}$ can be matched by a clause and must show that
a proof can be constructed in each such case.

The soundness of these rules is the content of the following theorem
whose proof also makes explicit the intended interpretation of the
pattern-based form of definitions.

\begin{theorem}
The pattern-based form of definitions and the associated proof rules
do not add any new power to the logic. In particular, the $\defL^p$
and $\defR^p$ rules are admissible under the intended interpretation
via translation of the pattern-based form of
definitions.
\end{theorem}
\begin{proof}
Let $p$ be a predicate whose clauses in the definition being
considered are given by the following set of clauses.
\begin{equation*}
\{\forall \vec{x}_i.~ (\nabla \vec{z}_i. p\ \vec{t}_i) \triangleq
B_i\ p\ \vec{x}_i\}_{i\in 1..n}
\end{equation*}
Let $p'$ be a new constant symbol with the same argument types as
$p$. Then the intended interpretation of the definition of $p$ in a
setting that does not allow the use of patterns in the head and that
limits the number of clauses defining a predicate to one is given by
the clause
\begin{equation*}
\forall \vec{y} . p\ \vec{y} \triangleq \bigvee_{i\in 1..n} \exists \vec{x}_i
. ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land B_i\
p\ \vec{x}_i
\end{equation*}
in which the variables $\vec{y}$ are chosen such that they do not
appear in the terms $\vec{t}_i$ for $1 \leq i \leq n$. Note also that we are using the term
constructor $p'$ here so as to be able to match the entire
head of a clause at once, thus ensuring that the $\nabla$-bound
variables in the head are assigned a consistent value for all
arguments of the predicate.

Based on this translation, we can replace
an instance of $\defR^p$,
\begin{equation*}
\infer[\defR^p]
      {\Gamma \lra p\; \vec{s}}
      {\Gamma \lra (B_i\; p\; \vec{x}_i)[\theta]}
\end{equation*}
with the following sequence of rules, where a double inference line
indicates that a rule is used multiple times.
\begin{equation*}
\infer[\defR]{\Gamma \lra p'\; \vec{t}}
{\infer=[\lorR]
 {\Gamma \lra \bigvee_{i\in 1..n} \exists \vec{x}_i
  . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
 }
 {\infer=[\existsR]
  {\Gamma \lra \exists \vec{x}_i
   . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
  }
  {\infer[\landR]
   {\Gamma \lra ((\lambda \vec{z}_i . p'\ \vec{t}_i)[\theta] \unrhd
     p'\ \vec{s}) \land (B_i\ p\ \vec{x}_i)[\theta]
   }
   {\infer[\unrhdR]{\Gamma \lra (\lambda \vec{z}_i . p'\
       \vec{t}_i)[\theta] \unrhd p'\ \vec{s}}{}
    &
    \Gamma \lra (B_i\; p\; \vec{x}_i)[\theta]
   }
  }
 }
}
\end{equation*}
Note that we have made use of the fact that $\theta$ instantiates only
the variables $x_i$ and thus has no effect on $\vec{s}$. Further, the
side condition associated with the $\defR^p$ rule ensures that the
$\unrhdR$ rule that appears as a left leaf in this derivation is well
applied.

Similarly, we can replace an instance of $\defL^p$,
\begin{equation*}
\infer[\defL^p]
      {\Sigma : \Gamma, p\; \vec{s} \lra C}
      {\left\{
         \Sigma\theta : \Gamma\cas{\theta}, (B_i\; p\;
         \vec{x}_i)\cas{\theta} \lra C\cas{\theta}\ |\
           \hbox{$\theta$ is a solution to $((\lambda \vec{z} . p\ \vec{t}_i) \unrhd p\
                        \vec{s})$}
              \right\}_{i\in 1..n}
      }
\end{equation*}
with the following sequence of rules
\begin{equation*}
\hspace{-1.8cm}
\infer[\defL]{\Gamma, p\; \vec{s} \lra C}
{
 \infer=[\lorL]
 {\Gamma, \bigvee_{i\in 1..n} \exists \vec{x}_i
  . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
  \lra C
 }
 {\hspace{2.2cm}\left\{\raisebox{-6ex}{
  \infer=[\existsL]
   {\Gamma, \exists \vec{x}_i
     . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
    \lra C
   }
   {\infer[\landL^*]
    {\Gamma, ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
     \lra C
    }
    {\infer[\unrhdL]
     {\Gamma, (\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s},
       B_i\ p\ \vec{x}_i
       \lra C
     }
     {\left\{\hbox{
       $\Gamma\cas{\theta}, (B_i\; p\; \vec{x}_i)\cas{\theta}
         \lra C\cas{\theta}\ |\ \theta$ is a solution to
	   $((\lambda \vec{z} . p'\ \vec{t}_i) \unrhd p'\ \vec{s})$
      }
      \right\}
     }
    }
   }
 }\right\}_{i \in 1..n}\hspace{3cm}
 }
}
\end{equation*}
Here $\landL^*$ is an application of $\cL$ followed by $\landL_1$ and
$\landL_2$ on the contracted formula. It is easy to see that the
solutions to $(\lambda \vec{z}.p\;\vec{t}_i)
\unrhd p\;\vec{s}$ and $(\lambda \vec{z}.p'\;\vec{t}_i)
\unrhd p'\;\vec{s}$ are identical and hence the leaf sequents in this
partial derivation are exactly the same as the upper sequents of the
instance of the $\defL^p$ rule being considered.
\end{proof}

A weak form of a converse to the above theorem also holds. Suppose
that the predicate $p$ is given by the following clauses
\begin{equation*}
\{\forall \vec{x}_i.~ (\nabla \vec{z}_i. p\ \vec{t}_i) \triangleq
B_i\ p\ \vec{x}_i\}_{i\in 1..n}
\end{equation*}
in a setting that uses pattern-based definitions and that has the
$\defL^p$ and $\defR^p$ but not the $\defL$ and $\defR$ rules. In such
a logic, it is easy to see that the following is provable:
\begin{equation*}
\forall \vec{y} . \left[p\ \vec{y} \equiv \bigvee_{i\in 1..n} \exists
  \vec{x}_i . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y})
  \land B_i\ p\ \vec{x}_i\right]
\end{equation*}
Where $B \equiv C$ denotes $(B \supset C) \land (C \supset B)$. Thus,
in the presence of \cut, the $\defL$ and $\defR$ rules can be treated
as derived ones relative to the translation interpretation of
pattern-based definitions.




















We would like also to allow patterns to be used in the heads of
clauses when writing definitions that are intended to pick out the
least and greatest fixed points, respectively. Towards this end we
admit in a definition also clauses of the form $\forall
\vec{x}.(\nabla \vec{z}. p\ \vec{t}) \mueq B\ p\ \vec{x}$ and $\forall
\vec{x}.(\nabla \vec{z}. p\ \vec{t}) \nueq B\ p\ \vec{x}$ with the
earlier provisos on the form of $B$ and $\vec{t}$ and the types of $B$ and
$p$ and with the additional requirement that all the clauses for any
given predicate are un-annotated or annotated uniformly with either $\mu$ or
$\nu$. Further, a definition must satisfy stratification conditions as
before. In reasoning about the least or greatest fixed point forms of
definitions, we may use the translation into the earlier, non-pattern
form together with the rules $\IL$ and $\CIR$. It is possible to
formulate an induction rule that works directly from pattern-based
definitions using the idea that to show $S$ to be an induction
invariant for the predicate $p$, one must show that every clause of
$p$ preserves $S$. A rule that is based on this intuition is presented
in Figure~\ref{fig:pattern-induction-rule}. The soundness of this rule
is shown in the following theorem.

\begin{figure}[t]
\begin{equation*}
\infer[\IL^p]
{\Sigma : \Gamma, p\ \vec{s} \lra C}
{\left\{\vec{x}_i : B_i\ S\ \vec{x}_i \lra \nabla \vec{z}_i.S\
  \vec{t}_i\right\}_{i\in 1..n} \quad
  \Sigma : \Gamma, S\ \vec{s} \lra C}
\end{equation*}
\begin{center}
assuming $p$ is defined by the set of clauses $\{\forall \vec{x}_i.
(\nabla \vec{z}_i. p\ \vec{t}_i) \mueq B_i\ p\ \vec{x}_i\}_{i\in 1..n}$
\end{center}
\caption{Induction rule for pattern-based definitions}
\label{fig:pattern-induction-rule}
\end{figure}

\begin{theorem}
The $\IL^p$ rule is admissible under the intended translation of
pattern-based definitions.
\end{theorem}
\begin{proof}
Let the clauses for $p$ in the pattern-based definition be given by
the set
\[\{\forall \vec{x}_i.
(\nabla \vec{z}_i. p\ \vec{t}_i) \mueq B_i\ p\ \vec{x}_i\}_{i\in
  1..n}\]
in which case the translated form of the definition for $p$ would be
\begin{equation*}
\forall \vec{y} . p\ \vec{y} \mueq \bigvee_{i\in 1..n} \exists \vec{x}_i
. ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land B_i\
p\ \vec{x}_i.
\end{equation*}
In this context, the rightmost upper sequents of the $\IL^p$ and the $\IL$
rules that are needed to derive a sequent of the form $\Sigma :
\Gamma, p\ \vec{s} \lra C$ are identical. Thus, to show
that $\IL^p$ rule is admissible, it suffices to show that the left
upper sequent in the $\IL$ rule can be derived in the original
calculus from all but the rightmost upper sequent in an $\IL^p$
rule. Towards this end, we observe that we can construct the following
derivation:
\begin{equation*}
\small
\hspace{-2.3cm}
\infer=[\lorL]
{\vec{y} : \bigvee_{i\in 1..n} \exists \vec{x}_i
  . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land
  B_i\ S\ \vec{x}_i
  \lra S\ \vec{y}
}
{\hspace{2.8cm}\left\{\raisebox{-6ex}{
    \infer=[\existsL]
    {\vec{y} : \exists \vec{x}_i
      . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land
      B_i\ S\ \vec{x}_i \lra S\ \vec{y}
    }
    {\infer[\landL^*]
      {\vec{y}, \vec{x}_i :
        ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land
        B_i\ p\ \vec{x}_i \lra S\ \vec{y}
      }
      {\infer[\unrhdL]
        {\vec{y}, \vec{x}_i :
          (\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y},
          B_i\ S\ \vec{x}_i
          \lra S\ \vec{y}
        }
        {\left\{\hbox{
            $(\vec{y}, \vec{x}_i)\theta : (B_i\; p\; \vec{x}_i)\cas{\theta}
            \lra (S\ \vec{y})\cas{\theta}\ |\ \theta$ is a solution to
            $((\lambda \vec{z} . p'\ \vec{t}_i) \unrhd p'\ \vec{y})$
          }
          \right\}
        }
      }
    }
  }\right\}_{i \in 1..n}\hspace{3cm}
}
\end{equation*}
Since the variables $\vec{y}$ are distinct and do not occur in
$\vec{t}_i$, the solutions to $(\lambda \vec{z} . p'\ \vec{t}_i)
\unrhd p'\ \vec{y}$ have a simple form. In particular, let $\vec{t}'_i$
be the result of replacing in $\vec{t}_i$ the variables $\vec{z}$ with
distinct nominal constants. Then $\vec{y} = \vec{t}'_i$ will be a most
general solution to the nominal abstraction. Thus the upper sequents
of the invariant derivation above will be
\begin{equation*}
\vec{x}_i : B_i\ p\ \vec{x}_i \lra S\ \vec{t}'_i
\end{equation*}
which are derivable if and only if the sequents
\begin{equation*}
\vec{x}_i : B_i\ p\ \vec{x}_i \lra \nabla\vec{z}_i. S\ \vec{t}_i
\end{equation*}
are derivable.
\end{proof}

We do not introduce a co-induction rule for pattern-based
definitions largely because it seems that there are few interesting
co-inductive definitions that require patterns and multiple clauses.


\section{Examples}
\label{sec:examples}

We now provide some examples to illuminate the properties of nominal
abstraction and its usefulness in both specification and reasoning
tasks; while \logic has many more features, their characteristics and
applications have been exposed in other work (\eg, see
\cite{mcdowell02tocl,momigliano03types,tiu04phd,tiu.tocl}). In the
examples that are shown, use will be made of the pattern-based form of
definitions described in Section~\ref{sec:pattern-form}. We will also
use the convention that tokens given by capital letters denote
variables that are implicitly universally quantified over the entire
clause.

\subsection{Properties of $\nabla$ and Freshness}
\label{sec:nabla-freshness}

We can use nominal abstraction to gain a better insight into the
behavior of the $\nabla$ quantifier. Towards this end, let the {\sl fresh}
predicate be defined by the following clause.
\begin{equation*}
(\nabla x.\fresh x E) \triangleq \top
\end{equation*}
We have elided the type of {\sl fresh} here; it will have to be
defined at each type that it is needed in the examples we consider
below. Alternatively, we can ``inline'' the definition by using nominal
abstraction directly, \ie, by replacing occurrences of of $\fresh
{t_1} {t_2}$ with $\exists E. (\lambda x. \tup{x, E} \unrhd \tup{t_1,
  t_2})$ for a suitably typed pairing construct $\tup{\cdot,\cdot}$.

Now let $B$ be a formula whose free variables are among $z, x_1,
\ldots, x_n$, and let $\vec{x} = x_1 :: \ldots :: x_n :: nil$ where
$::$ and $nil$ are constructors in the logic.\footnote{We are, once
  again, finessing typing issues here in that the $x_i$ variables may
  not all be of the same type. However, this problem can be solved by
  surrounding each of them with a constructor that yields a term with
  a uniform type.} Then the following
formulas logically imply one another in \logic.
\[
\nabla z. B \qquad\quad
\exists z. (\fresh z \vec{x} \land B) \qquad\quad
\forall z. (\fresh z \vec{x} \supset B)
\]
Note that the type of $z$ allows it to be an arbitrary term in the
last two formulas, but its occurrence as the first argument of {\sl
  fresh} will restrict it to being a nominal constant (even when
$\vec{x} = nil$).

In the original presentation of the $\nabla$ quantifier
\cite{miller03lics}, it was shown that one can move a $\nabla$
quantifier inwards over universal and existential quantifiers by using
raising to encode an explicit dependency. To illustrate this, let $B$
be a formula with two variables abstracted out, and let $C \equiv D$
be shorthand for $(C \supset D) \land (D\supset C)$. The the following
formulas are provable in the logic.
\begin{align*}
\nabla z. \forall x. (B\ z\ x) &\equiv \forall h. \nabla z.
(B\ z\ (h\ z)) &
\nabla z. \exists x. (B\ z\ x) &\equiv \exists h. \nabla z.
(B\ z\ (h\ z))
\end{align*}
In order to move a $\nabla$ quantifier outwards over universal and
existential quantifiers, one would need a way to make non-dependency
(\ie, freshness) explicit. This is now possible using nominal
abstraction as shown by the following equivalences.
\begin{align*}
\forall x. \nabla z. (B\ z\ x) &\equiv
\nabla z. \forall x. (\fresh z x \supset B\ z\ x)
&
\exists x. \nabla z. (B\ z\ x) &\equiv
\nabla z. \exists x. (\fresh z x \land B\ z\ x)
\end{align*}
Finally, we note that the two sets of equivalences for moving the
$\nabla$ quantifier interact nicely. Specifically, starting with a
formula like $\nabla z. \forall x. (B\ z\ x)$ we can push the $\nabla$
quantifier inwards and then outwards to obtain $\nabla z. \forall h.
(\fresh z (h\ z) \supset B\ z\ (h\ z))$. Here $\fresh z (h\ z)$ will
only be satisfied if $h$ projects away its first argument, as
expected.

\subsection{Polymorphic Type Generalization}

In addition to reasoning, nominal abstraction can also be useful in
providing declarative specifications of computations. We consider the
context of a type inference algorithm that is also discussed in
\cite{cheney08toplas} to illustrate such an application. In this
setting, we might need a predicate {\sl spec} that relates a
polymorphic type $\sigma$, a list of distinct variables
list of distinct variables $\vec{\alpha}$ (represented by nominal
constants) and a monomorphic type $\tau$ just in the case that $\sigma =
\forall\vec{\alpha}.\tau$. Using nominal abstraction, we can define
this predicate as follows.
\begin{align*}
&\spec {(\monoTy T)} {nil} T \mueq \top \\
(\nabla x. &\spec {(\polyTy P)} {(x::L)} {(T\ x)}) \mueq
\nabla x. \spec {(P\ x)} L {(T\ x)}.
\end{align*}
Note that we use $\nabla$ in the head of the second clause to
associate the variable $x$ at the head of the list $L$ with its
occurrences in the type $(T\ x)$. We then use $\nabla$ in the body of
this clause to allow for the recursive use of {\sl spec}.

\subsection{Arbitrarily Cascading Substitutions}

Many reducibility arguments, such as Tait's proof of normalization for
the simply typed $\lambda$-calculus \cite{tait67jsl}, are based on
judgments over closed terms. During reasoning, however, one has often
to work with open terms. To accommodate this requirement, the closed
term judgment is extended to open terms by considering all possible
closed instantiations of the open terms. When reasoning with \logic,
open terms are denoted by terms with nominal constants representing
free variables. The general form of an open term is thus $M\; c_1\;
\cdots\; c_n$, and we want to consider all possible instantiations
$M\; V_1\; \cdots\; V_n$ where the $V_i$ are closed terms. This type
of arbitrary cascading substitutions is difficult to realize in
reasoning systems where variables are given a simple type since $M$
would have an arbitrary number of abstractions but the type of $M$
would {\em a priori} fix that number of abstractions.

We can define arbitrary cascading substitutions in \logic using
nominal abstraction. In particular, we can define a predicate which
holds on a list of pairs $\tup{c_i,V_i}$, a term with the form $M\; c_1\;
\cdots\; c_n$ and a term of the form $M\; V_1\; \cdots\; V_n$. The
idea is to iterate over the list of pairs and for each pair $\tup{c,V}$
use nominal abstraction to abstract $c$ out of the first term and then
substitute $V$ before continuing. The following definition of the
predicate {\sl subst} is based on this idea.
\begin{align*}
& \subst {nil} T T \mueq \top \\
(\nabla x. &\subst {(\tup{x,V}::L)} {(T\; x)} S) \mueq
\subst L {(T\; V)} S
\end{align*}

Given the definition of {\sl subst} one may then show that arbitrary
cascading substitutions have many of the same properties as normal
higher-order substitutions. For instance, in the domain of the untyped
$\lambda$-calculus, we can show that {\sl subst} acts compositionally via
the following lemmas.
\begin{align*}
&\forall \ell, t, r, s.~
\subst \ell {(\app t r)} s \supset
\exists u, v. (s = \app u v \land \subst \ell t u \land \subst \ell r v)
\\
&\forall \ell, t, r.~
\subst \ell {(\uabs t)} r \supset
\exists s. (r = \uabs s \land \nabla z. \subst \ell {(t\; z)} (s\; z))
\end{align*}
Both of these lemmas have straightforward proofs by induction on {\sl
  subst}.

We use this technique for describing arbitrary cascading substitutions again in
Section~\ref{sec:girards-strong-norm} to formalize Girard's strong
normalization argument for the simply-typed $\lambda$-calculus.



