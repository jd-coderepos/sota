
\chapter{A Logic for Reasoning About Specifications}
\label{ch:meta-logic}

In this chapter we present the meta-logic \logic. This logic allows
for encoding descriptions of computational systems and for reasoning
over those descriptions. The logic includes traditional reasoning
devices such as case analysis, induction, and co-induction as well as
new devices specifically designed for working with higher-order abstract
syntax.

The relevant history of \logic begins with the meta-logic \FOLDN
developed by McDowell and Miller for the purposes of inductive
reasoning over higher-order abstract syntax descriptions
\cite{mcdowell02tocl, mcdowell00tcs}. This logic contains a definition
mechanism which allows one to specify and reason about closed-world
descriptions, \ie, allows one to form judgments and to perform case
analysis on them. This definition mechanism is based on earlier work
on closed-world reasoning by many others, but most notably by
Schroeder-Heister \cite{schroeder-Heister93lics}, Eriksson
\cite{eriksson91elp}, and Girard \cite{girard92mail}. The primary
contribution of \FOLDN was the recognition that definitions provided a
way of encoding higher-order abstract syntax descriptions in such a
way that does not conflict with inductive reasoning. In particular,
\FOLDN allowed for natural number induction, and so many reasoning
tasks could be naturally encoded. More recently, Tiu \cite{tiu04phd}
developed the meta-logic Linc which extends the mechanism of
definitions to integrate notions of generalized induction and
co-induction over the structure of definitions. These more general
notions are present in \logic as well.

Another central advancement in the development of logics for reasoning
over higher-order abstract syntax descriptions was the recognition
that one needed a way to reflect the binding structure of terms into
the structure of proofs. This was realized in earlier logics by using
universal judgments. However, this kind of correspondence was always
an uneasy one and the mismatch became explicit when it was necessary
to use case analysis arguments over binding structure as must be done,
for example, in bisimilarity proofs associated with -calculus
models of concurrent systems. The desire to provide a logically
precise and cleaner treatment led to the development of the
-quantifier and the associated generic judgment by Miller and
Tiu in the meta-logic \foldnb \cite{miller05tocl}. Tiu later refined
this notion in the meta-logic \LG so that -quantifier behaved
well with respect to inductive reasoning \cite{tiu06lfmtp}. This
interpretation of the -quantifier is present in \logic, and in
this context it can be understood as quantifying over fresh names.

The meta-logic \logic is a continuation of the research surrounding
inductive reasoning and higher-order abstract syntax descriptions. In
particular, it extends the notion of equality in the logic to one
which can describe the binding structure of terms relative to the
proof context in which they occur. This turns out to be essential to
describing the structure of terms which are generated during inductive
reasoning over higher-order abstract syntax descriptions. Moreover,
\logic identifies how this extended notion of equality can be
integrated with the definition mechanism to allow a succinct
description of such objects.

The presentation of \logic is divided into three parts. First,
Section~\ref{sec:logic} contains the core of the logic including
generic quantification. Then Section~\ref{sec:nominal-abstraction}
introduces the extended notion of equality known as {\em nominal
  abstraction} and rules for treating this notion within the logic.
Finally, Section~\ref{sec:definitions} presents rules for treating
fixed-points in the logic including mechanisms for induction and
co-induction. Although the logical features of \logic are described in
their entirety in the first three sections, it is sometimes convenient
to use an alternative presentation for fixed-point definitions. This
form, which uses patterns to distinguish different cases in the
structure of the atom being defined, is introduced in
Section~\ref{sec:pattern-form} and is elaborated as an interpretation
of the basic form of definitions that uses nominal abstractions
explicitly. Rules for treating this alternative form of fixed-points
are presented and proven to be admissible. Finally,
Section~\ref{sec:examples} provides some small examples to illustrate
the expressive power of the logic.

\section{A Logic with Generic Quantification}
\label{sec:logic}

In this section we present the core logic underlying \logic. This
logic is obtained by extending an intuitionistic and predicative
subset of Church's Simple Theory of Types with a treatment of generic
judgments. The encoding of generic judgments is based on the
quantifier called  (pronounced nabla) introduced by Miller and
Tiu \cite{miller05tocl} and further includes the structural rules
associated with this quantifier in the logic \LG described by Tiu
\cite{tiu06lfmtp}.


\subsection{The Basic Syntax}

Following Church \cite{church40}, terms are constructed from constants
and variables using abstraction and application. All terms are
assigned types using a monomorphic typing system; these types also
constrain the set of well-formed expressions in the expected way. The
collection of types includes , a type that corresponds to
propositions. Well-formed terms of this type are also called formulas.
Two terms are considered to be equal if one can be obtained from the
other by a sequence of applications of the -, - and
-conversion rules, \ie, the -conversion rules. This
notion of equality is henceforth assumed implicitly wherever there is
a need to compare terms. Logic is introduced by including special
constants representing the propositional connectives , ,
, ,  and, for every type  that does not
contain , the constants  and  of type
. The binary propositional
connectives are written as usual in infix form and the expressions
 and  abbreviate the formulas
 and ,
respectively. Type subscripts will be omitted from quantified formulas
when they can be inferred from the context or are not important to the
discussion. We also use a shorthand for iterated quantification: if
 is a quantifier, we will often abbreviate  to  or simply
. We consider the scope of -binders (and
therefore quantifiers) as extending as far right as possible. We
further assume that  is right associative and has lower
precedence than  and . For example,  should be read as .

The usual inference rules for the universal quantifier can be seen as
equating it to the conjunction of all of its instances: that is, this
quantifier is treated extensionally.
There are several situations  where one
wishes to treat an expression such as `` holds for all
'' as a statement about the existence of a uniform argument for
every instance rather than the truth of a particular property for each
instance \cite{miller05tocl};
such situations typically arise when one is reasoning about the
binding structure of formal objects represented using the
{\em -tree syntax} \cite{miller00cl} version of {\em
higher-order abstract syntax} \cite{pfenning88pldi}.
The -quantifier serves to encode judgments that have this kind
of a ``generic'' property associated with them. Syntactically, this
quantifier corresponds to
including a constant  of type  for each type  not containing .\footnote{
We may choose to allow -quantification at fewer types in
particular applications; such a restriction may be
  useful in adequacy arguments for reasons we discuss later.}
As with the
other quantifiers,  abbreviates  and the type subscripts are often suppressed for readability.

\subsection{Generic Judgments and -quantification}

Sequents in intuitionistic logic can be written as

where  is the ``global signature'' for the sequent that
contains the {\em eigenvariables} ({\em i.e.}, variables associated to
the  and  inference rules) relevant to the sequent
proof. We shall think of  in this prefix position as an
operator that binds each of the variables it contains and that has the
rest of the sequent as its scope. To treat the -quantifier,
the \foldnb logic \cite{miller05tocl} extends the notion of a judgment
from just a formula to a formula paired with a ``local signature.''
Thus, sequents within this logic are written more elaborately as

where each  is a list of variables that are
bound locally in the formula adjacent to it.  Such local signatures
correspond to a proof-level encoding of binding that is expressed
within formulas through the -quantifier. In particular, the
judgment  and the formula  for  have the same proof-theoretic
force. In keeping with this observation, we shall refer to a judgment
of the form  as a {\it generic judgment}.

As part of a generalization of sequents that bases them on generic
judgments rather than on formulas, we need to
define when two such judgments are equal: this is necessary
for describing at least the initial and cut inference rules.  The
\foldnb logic \cite{miller05tocl} uses a simple form of equality for
this purpose. It deems two generic judgments of the form
 and 
to be equal exactly when the -terms  and  are
-convertible; notice that this necessarily implies that
. An equality notion is also needed in formulating an induction
rule. Unfortunately, the simple form of equality present in \foldnb
leads to a rather weak version of such a rule. To overcome this difficulty,
Tiu proposed the addition to the logic of two natural ``structural''
identities between generic judgments.
These identities are the {\em -strengthening rule}
, provided  is not free in , and the
{\em -exchange rule} .  In its essence, the \LG proof system \cite{tiu06lfmtp} is
obtained from \foldnb by strengthening its notion of equality based on
-conversion through the addition of these two structural
rules for .


The move from the weaker
logic \foldnb to the stronger logic \LG involves an ontological
commitment and has a proof-theoretic consequence.

At the ontological level, the strengthening rule implies that every
type at which one is
willing to use -quantification is non-empty and, in fact,
contains an unbounded number of members.  For example, the formula
 is always provable, even if there are no closed
terms of type  because this formula is equivalent to
, which is provable.
Similarly, for any given , the following formula
is provable


At the proof-theoretic level, an acceptance of the strengthening and
exchange rules means
that the length of a local context and the order of variables within
it are unimportant.  For example, a sequent that contains the generic
judgments   and
 can be rewritten (assuming  )
using
-conversion and strengthening into the judgments
   and
 where  and  are
equal to  and  modulo variable renamings.  In this fashion, all
local bindings in a sequent can be made to involve the same
variables, and, hence, the local bindings can be seen as a global
binding over a sequent that contains formulas and not generic
judgments.  The resulting sequent-level variable bindings will be represented by
specially designated {\it nominal constants}.
Notice, however, that each of these nominal ``constants'' has as its scope only a single
formula.  Thus, we must distinguish the same nominal constant when it
appears in two different formulas and we should treat judgments
as being equal if they are identical up to permutations of these
constants.


\subsection{A Sequent Calculus Presentation of the Core Logic}

The logic \logic inherits from \LG the shift from a local to a global
scope in the treatment of the -quantifier.  In particular,
we assume that the collection of constants is partitioned into the set
 of nominal constants and the set  of
usual, non-nominal constants.
We assume the set  contains an infinite number of nominal
constants for each type at which  quantification is permitted.
We define the {\it support} of a term (or
formula), written , as the set of nominal constants
appearing in it.
A permutation of nominal constants is a type-preserving bijection  from
 to  such that  is
finite.  We denote the application of such a
permutation to a term or formula  by  and define this as
follows:

We extend the notion of equality between terms to encompass also
the application of permutations to nominal constants appearing in
them. Specifically, we write  to denote the fact that
there is a permutation  such that  -converts to
. Using the observations that permutations are invertible and
composable and that -convertibility is an equivalence
relation, it is easy to see that  is also an equivalence
relation.

\begin{figure*}[t]
\small








\caption{The core rules of \logic}
\label{fig:core-rules}
\end{figure*}

The rules defining the core of \logic are presented in Figure
\ref{fig:core-rules}. Sequents in this logic have the form  where  is a multiset and the signature 
contains all the free variables of  and . We use
expressions of the form  in the quantifier rules to denote the
result of substituting the term  for  in the formula . Note
that such a substitution must be done carefully, making sure to rename
bound variables in  to avoid capture of variables appearing in .
In the  and  rules,  denotes
a nominal constant of an appropriate type. In the 
and  rule we use raising \cite{miller92jsc} to
encode the dependency of the quantified variable on the support of
; the expression  in which  is a fresh
eigenvariable is used in these two rules to denote the (curried)
application of  to the constants appearing in the sequence
. The  and  rules
make use of judgments of the form . These judgments enforce the requirement that the
expression  instantiating the quantifier in the rule is a
well-formed term of type  constructed from the eigenvariables in
 and the constants in . Notice that in
contrast the  and  rules seem
to allow for a dependency on only a restricted set of nominal
constants. However, this asymmetry is not significant:
Corollary~\ref{cor:extend} in Section~\ref{sec:meta-theory} will tell
us that the dependency expressed through raising in the latter rules
can be extended to any number of nominal constants that are not in the
relevant support set without affecting the provability of sequents.

Equality modulo -conversion is built into the rules in
Figure~\ref{fig:core-rules}, and also into later extensions of
this logic, in a fundamental way: in particular, proofs are preserved
under the replacement of formulas in sequents by ones to which they
-convert.  A more involved observation
is that we can replace a formula  in a sequent by another formula
 such that  without affecting the provability of the
sequent or even the very structure of the proof. For the core logic,
this observation follows from the form of the  rule and the fact
that permutations distribute over logical structure. We shall prove
this property explicitly for the full logic in
Chapter~\ref{ch:meta-theory}.

\section{Characterizing Occurrences of Nominal Constants}
\label{sec:nominal-abstraction}

We are interested in adding to our logic the capability of
characterizing occurrences of nominal constants within terms and also
of analyzing the structure of terms with respect to such
occurrences. For example, we may want to define a predicate called
{\em name} that holds of a term exactly when that term is a nominal
constant. Similarly, we might need to identify a binary relation
called {\em fresh} that holds between two terms just in the case that
the first term is a nominal constant that does not occur in the second
term. Towards supporting such possibilities, we define in this section
a special binary relation called {\it nominal abstraction} and then
present proof rules that incorporate an understanding of this relation
into the logic. A formalization of these ideas requires a careful
treatment of substitution. In particular, this operation must be
defined to respect the intended formula-level scope of nominal
constants. We begin our discussion with an elaboration of this aspect.

\subsection{Substitutions and their Interaction with Nominal Constants}

The following definition reiterates a common view of substitutions
in logical contexts.

\begin{definition}\label{subst}
A substitution is a type preserving mapping from variables
to terms that is the identity at all but a finite number of variables.
The domain of a substitution is the set of variables that are
not mapped to themselves and its range is the
set of terms resulting from applying it to the variables in its
domain.  We write a substitution as 
where  is a list of variables that contains the
domain of the substitution and  is the value of the
map on these variables. The support of a substitution ,
written as , is the set of nominal constants that appear in
the range of . The restriction of a substitution  to
the set of variables , written as , is a mapping that is like  on the variables in
 and the identity everywhere else.
\end{definition}

A substitution essentially calls for the replacement of
variables by their associated terms in any context to which it is
applied. A complicating factor in our setting is that nominal
constants can appear in the terms that are to replace
particular variables. A substitution may be determined relative to one
formula in a sequent but may then have to be applied to other formulas
in the same sequent. In doing this, we have to take into account the
fact that the scopes of the implicit quantifiers over nominal
constants are restricted to individual formulas. Thus, the logically
correct application of a substitution should be accompanied by a
renaming of these constants in the term being substituted into so as to
ensure that they are not confused with the ones appearing in
the range of the substitution.

\begin{definition}\label{ncasubst}
The ordinary application of a substitution  to a term  is
denoted by  and corresponds to the replacement of the
variables in  by the terms that  maps them to, making sure,
as usual, to avoid accidental binding of the variables appearing in
the range of . More precisely, if , then  is the term ; this term is, of course, considered to be equal
to any other term that it -converts to. By contrast,
the {\em nominal capture avoiding application} of  to  is
written as  and is defined as follows. Assuming that
 is a permutation of nominal constants that maps those appearing
in  to ones not appearing in , let . Then .
\end{definition}

The notation  generalizes the one
used in the quantifier rules in Figure~\ref{fig:core-rules}.
The definition of the nominal capture avoiding application of a
substitution is ambiguous in that we do not uniquely specify the
permutation to be used.  We resolve this ambiguity by deeming as
acceptable {\it any} permutation that avoids conflicts. As a special
instance of the lemma below, we see that for any given formula  and
substitution ,
all the possible values for  are equivalent modulo the
 relation. Moreover, as we show in
Chapter~\ref{ch:meta-theory}, formulas that are equivalent under
 are interchangeable in the contexts of proofs.

\begin{lemma}
\label{lem:approx-cas}
If  then .
\end{lemma}
\begin{proof}
Let  be -convertible to , let  where , and let  be -convertible to
 where . Then we define a function  partially by the following
rules:
\begin{enumerate}
\item  if  and
\item  if .
\end{enumerate}
Since , these rules
are not contradictory, \ie, this (partial) function is well-defined.
The range of the first rule is  which is disjoint from the
range of the second rule, . Since the mapping in each
rule is determined by a permutation, these rules together define a
one-to-one partial mapping that can be extended to a bijection on
. We take any such extension to be the complete
definition of  that must therefore be a permutation.

To prove that  it suffices to
show that  is -convertible to
. We do this by induction on the structure
of  under the further assumption that  -converts to
. Suppose  is an abstraction. Then, it is easy to see
that  -converts to  and  -converts
to  for some choice of variable
 and terms  and  such that  is structurally less complex
than  and  -converts to . But then, by the
induction hypothesis,  -converts to
 and hence  is
-convertible to . A similar and, in
fact, simpler argument can be provided in the case where  is an
application. If  is a nominal constant  then
 must be -convertible to
. Also,
 must be -convertible to
. Further, in this case the first rule for  applies
which means . Thus  is again
-convertible to . Finally, suppose
 is a variable . In this case  must be -convertible
to  so that we must show  -converts to
. If  does not have a binding in  then
both terms are equal. Alternatively, if  then  by the second rule for  and so the two terms are again equal.
Thus  -converts to
, as is required.
\end{proof}

The nominal capture avoiding application of substitutions turns out to
be the dominant notion in the analysis of provability.
For this reason, when we speak of the application of a
substitution in an unqualified way, we shall mean the nominal capture
avoiding form of this notion.

We shall need to consider the composition of substitutions later in
this section. The definition of this notion must also pay attention to
the presence of nominal constants.

\begin{definition}\label{nascomp}
Given a substitution  and a permutation  of nominal
constants, let  denote
the substitution that is obtained by replacing each  in 
with . Given any two substitutions  and , let
 denote the substitution that is such that
. In this context, the {\em
  nominal capture   avoiding composition} of  and  is
written as  and defined as follows. Let  be a
permutation of nominal constants such that
 is disjoint from . Then
.
\end{definition}\label{substequiv}
The notation  in the above definition represents
the usual composition of  and  and can, in fact, be
given in an explicit form based on these substitutions. Thus,  can also be presented in an explicit form. Notice that our
definition of nominal capture avoiding composition is, once again,
ambiguous because it does not fix the permutation to be used,
accepting instead any one that satisfies the constraints. However, as
before, this ambiguity is harmless. To understand this, we first
extend the notion of equivalence under permutations to substitutions.
\begin{definition}
Two substitutions  and  are considered to be permutation
equivalent, written , if and only if there is a
permutation of nominal constants  such that . This notion of equivalence may also be parameterized by a
set of variables  as follows: 
just in the case that .
\end{definition}
It is easy to see that all possible choices for 
are permutation equivalent and that if 
then  for any term .
Thus, if our focus is on provability, the ambiguity in
Definition~\ref{nascomp} is inconsequential by a result to be
established in Chapter~\ref{ch:meta-theory}. As a further observation,
note that 
for any . Hence our notion of nominal capture avoiding composition
of substitutions is sensible.

The composition operation can be used to define an ordering
relation between substitutions:
\begin{definition}\label{nasordering}
Given two substitutions  and , we say  is {\em
  less general than} , notated as , if and
only if there exists a  such that . This relation can also be parameterized by a
set of variables:  is less general than 
relative to , written as , if and
only if .
\end{definition}
The notion of generality between substitutions that is based on
nominal capture avoiding composition has a different flavor from that
based on the traditional form of substitution composition. For
example, if  is a nominal constant, the substitution  is
strictly less general than  relative to  for
any  which contains  and . To see this, note that we can
compose the latter substitution with  to obtain
the former, but the naive attempt to compose the former with
 yields  where  is a nominal constant
distinct from . In fact, the ``most general'' solution relative to
 containing  will be .

\subsection{Nominal Abstraction}

The nominal abstraction relation allows implicit formula-level
bindings represented by nominal constants to be moved into explicit
abstractions over terms. The following notation is useful for
defining this relationship.

\begin{notation}
Let  be a term, let  be distinct nominal constants that
possibly occur in , and let  be distinct variables
not occurring in  and such that, for ,  and
 have the same type. Then we write  to denote the term  where  is the term obtained from  by replacing
 by  for .
\end{notation}

There is an ambiguity in the notation introduced above in that
the choice of variables  is not fixed. However, this
ambiguity is harmless: the terms that are produced by acceptable
choices are all equivalent under a renaming of bound variables.

\begin{definition}\label{nominal-abstraction}
Let  and
let  and  be terms of type  and , respectively; notice, in particular, that  takes
 arguments to yield a term of the same type as .
Then the expression  is a formula that is referred to as a
nominal abstraction of degree  or simply as a nominal abstraction. The
symbol  is used here in an overloaded way in that the degree
of the nominal abstraction it participates in can vary.
The nominal abstraction  of degree  is said to hold just in
the case that  -converts to  for
some nominal constants .
\end{definition}

Clearly, nominal abstraction of degree  is the same as equality
between terms based on -conversion, and we will therefore use
  to denote this relation in that situation. In the more general
case,
the term on the left of the operator serves as a pattern for isolating
occurrences of nominal constants. For example, the relation  holds exactly when  is a nominal constant.



The symbol  corresponds, at the moment, to a mathematical
relation that holds between pairs of terms as explicated by
Definition~\ref{nominal-abstraction}. We now overload this symbol by
treating it also as a binary predicate symbol of \logic. In the next
subsection we shall add inference rules to make the mathematical
understanding of  coincide with its syntactic use as a
predicate in sequents. It is, of course, necessary to be able to
determine when we mean to use  in the mathematical sense and
when as a logical symbol. When we write an expression such as  without qualification, this should be read as a logical formula
whereas if we say that `` holds'' then we are referring to
the abstract relation from Definition~\ref{nominal-abstraction}. We
might also sometimes use an expression such as `` holds.'' In this case, we first treat  as
a formula to which we apply the substitution  in a nominal
capture avoiding way to get a (syntactic) expression of the form
. We then read  in the mathematical sense,
interpreting the overall expression as the assertion that `` holds.'' Note in this context that  constitutes a
single formula when read syntactically and hence the expression
 is, in general, {\it not} equivalent to the
expression .

In the proof-theoretic setting, nominal abstraction will be used with
terms that contain free occurrences of variables for which
substitutions can be made. The following definition is relevant to
this situation.

\begin{definition}\label{nasolution}
A substitution  is said to be a solution to the nominal
abstraction  just in the case that  holds.
\end{definition}

Solutions to a nominal abstraction can be used to provide rich
characterizations of the structures of terms. For example, consider
the nominal abstraction
 in which  and  are
variables and {\sl fresh} is a binary predicate symbol.  Any solution
to this problem requires that  be
substituted for by a term of the form  where 
is a nominal constant and  is a term in which  does not appear,
\ie,  must be ``fresh'' to .

An important property of solutions to a nominal abstraction is that
these are preserved under permutations to nominal constants. We
establish this fact in the lemma below; this lemma will be used later
in showing the stability of the provability of sequents with
respect to the replacement of formulas by ones they are equivalent to
modulo the  relation.

\begin{lemma}
\label{lem:na-approx}
Suppose . Then  and
 have exactly the same solutions. In particular,
 holds if and only if  holds.
\end{lemma}
\begin{proof}
We prove the particular result first. It suffices to only show it in
the forward direction since  is symmetric. Let  be the
permutation such that the expression  -converts to
. Now suppose  holds since 
-converts to . Then  will
-convert to  where  is
the result of applying  to each element in the sequence .
Thus  holds.

For the general result it again suffices to show it in one direction,
\ie, that all the solutions of  are solutions to . Let  be a substitution such that  holds. By Lemma~\ref{lem:approx-cas}, . Thus by the
particular result from the first half of this proof,  holds.
\end{proof}

\subsection{Proof Rules for Nominal Abstraction}

\begin{figure}[t]
\small

\caption{Nominal abstraction rules}
\label{fig:na-rules}
\bigskip

\caption{A variant of  based on \CSNAS}
\label{fig:csnas}
\end{figure}

We now add the left and right introduction rules for  that are
shown in Figure~\ref{fig:na-rules} to link its use as a predicate
symbol to its mathematical interpretation. The expression  in the  rule denotes the application of a
substitution  to the signature
 that is defined to be the signature that results from
removing from  the variables  and then
adding every variable that is free in any term in
. Notice also that in the same inference rule the
operator  is applied to a multiset of formulas in the
natural way: .
Note that the  rule has an {\it a priori} unspecified number
of premises that depends on the number of substitutions that are
solutions to the relevant nominal abstraction. If 
expresses an unsatisfiable constraint, meaning that it has no
solutions, then the premise of  is empty and the rule
provides an immediate proof of its conclusion.

The  and  rules capture nicely the intended
interpretation of nominal abstraction. However, there is an obstacle
to using the former rule in derivations: this rule has an infinite
number of premises any time the nominal abstraction  has a
solution. We can overcome this difficulty by describing a rule that
includes only a few of these premises but in such way that their
provability ensures the provability of all the other premises.  Since
the provability of  implies the provability of
 for any  (a property
established formally in Chapter\ref{ch:meta-theory}), if the first
sequent is a premise of an occurrence of the  rule, the
second does not need to be used as a premise of that same rule
occurrence.  Thus, we can limit the set of premises to be considered
if we can identify with any given nominal abstraction a (possibly
finite) set of solutions from which any other solution can be obtained
through composition with a suitable substitution. The following
definition formalizes the idea of such a ``covering set.''

\begin{definition}\label{csnas}
A {\em complete set of nominal abstraction solutions} (\CSNAS) of 
and  on 
is a set  of substitutions such
that
\begin{enumerate}
\item each  is a solution to , and
\item for every solution  to , there exists a
 such that .
\end{enumerate}
We denote any such set by .
\end{definition}
Using this definition we present an alternative version of 
in Figure~\ref{fig:csnas}. Note that if we can find a finite complete
set of nominal abstraction solutions then the number of premises to
this rule will be finite.


\begin{theorem}\label{thm:csnas}
The rules  and  are inter-admissible.
\end{theorem}
\begin{proof}
Suppose we have the following arbitrary instance of  in a
derivation:

This rule can be replaced with a use of  instead if
we could be certain that, for each , it is
the case that  is included in the set of premises of the shown rule
instance. But this must be the case: by the
definition  of , each such  is a solution to .

In the other direction, suppose we have the following arbitrary
instance of .

To replace this rule with a use of the  rule
instead, we need to be able to construct a
derivation of  for
each  that is a solution to . By the definition of
, we know that for any such  there exists a  such that , \ie, such
that there
exists a  for which . Since we are considering the
application of these substitutions to a sequent all of whose
eigenvariables are contained in , we can drop the restriction
on the substitutions and suppose that . Now, we shall show in Chapter~\ref{ch:meta-theory} that if a
sequent has a derivation then the result of applying a substitution to
it in a nominal capture-avoiding way produces a sequent that also has
a derivation. Using this observation, it follows that
  has a proof. But this sequent is
permutation equivalent to  which must, again by a result established explicitly in
Chapter~\ref{ch:meta-theory}, also have a proof.
\end{proof}

Theorem~\ref{thm:csnas} allows us to choose which of the left rules we
wish to consider in any given context. We shall assume the 
rule in the formal treatment in the rest of this thesis, leaving the
use of the  rule to practical applications of the
logic.


\subsection{Computing Complete Sets of Nominal Abstraction
  Solutions}\label{ssec:complete-sets}

For the  rule to be useful, we need an effective way
to compute restricted complete sets of nominal abstraction
solutions. We show here that the task of finding such complete sets of
solutions can be reduced to that of finding complete sets of unifiers
(\CSU) for higher-order unification problems \cite{huet75tcs}. In the
straightforward approach to finding a solution to a nominal
abstraction , we would first identify a substitution
 that we apply to  to get  and we
would subsequently look for nominal constants to abstract from  to
get .  To relate this problem to the usual notion of unification,
we would like to invert this order: in particular, we would like to
consider all possible ways of abstracting over nominal constants first
and only later think of applying substitutions to make the terms
equal. The difficulty with this second approach is that we do not know
which nominal constants might appear in  until after the
substitution is applied. However, there is a way around this
problem. Given the nominal abstraction  of degree , we
first consider substitutions for the variables occurring in it that
introduce  new nominal constants in a completely general way.  Then
we consider all possible ways of abstracting over the nominal
constants appearing in the altered form of  and, for each of these
cases, we look for a complete set of unifiers.

The idea described above is formalized in the following definition and
associated theorem. We use the notation  in them to denote
an arbitrary but fixed selection of a complete set of unifiers for
the terms  and .

\begin{definition}\label{def:s}
Let  and  be terms of type  and , respectively. Let  be 
distinct nominal constants disjoint from  such that,
for ,  has the type . Let  be a
set of variables and for each  of type , let
 be a distinct variable not in  that has type
. Let  and let  and . Let

where  ranges over all selections of 
distinct nominal constants from  such that,
for ,
 has type  and  is some corresponding listing of
all the nominal constants in  and  that are not included in
.
Then we define

\end{definition}

The use of the substitution  above represents
another instance of the application of the general technique of
raising that allows
certain variables (the  variables in this definition) whose
substitution instances might depend on certain nominal constants
( here) to be replaced by new variables of higher type
(the  variables) whose substitution instances are not allowed to
depend on those nominal constants. This technique was previously used
in the  and  rules presented in
Section~\ref{sec:logic}.

\begin{theorem}
 is a complete set of nominal abstraction solutions
for  on .
\end{theorem}
\begin{proof}
First note that  and
thus  is equal to .
Now we must show that every element of  is a
solution to . Let 
be an arbitrary element where  is as in Definition~\ref{def:s},
 is from , and  and .  By the definition of  we know . This means  holds and thus  holds. Rewriting  and  in terms of  and  this
means . Thus 
is a solution to .

In the other direction, we must show that if  is a solution to  then there exists 
such that . Let  be a
solution to . Then we know  holds.
The substitution  may introduce some nominal constants which
are abstracted out of the right-hand side when determining equality, so
let us call these the {\em important} nominal constants. Let  be as in
Definition~\ref{def:s} and let  be a permutation which maps the
important nominal constants of  to nominal constants from
. This is possible since  nominal constants are
abstract from the right-hand side and thus there are at most 
important nominal constants. Then let , so that
 holds and it suffices to show that . Note that all we have done at this
point is to rename the important nominal constants of  so that
they match those introduced by . Now we define  so that
. Thus  holds. By construction,  shares no
nominal constants with  and , thus we know  where  and . Also by
construction,  contains no interesting nominal constants and
thus  holds for some nominal
constants  taken from . If we let
 be a listing of all nominal constants in  and  but
not in , then  holds. At this point the inner
equality has no nominal constants and thus the substitution  can
be applied without renaming:  holds. By the definition of
, there must be a  such that . Thus
 as desired.
\end{proof}

\section{Definitions, Induction, and Co-induction}
\label{sec:definitions}

\begin{figure}[t]
\begin{center}

\end{center}
\caption{Introduction rules for atoms whose predicate is defined as }
\label{fig:defrules}
\end{figure}


The sequent calculus rules presented in Figure~\ref{fig:core-rules}
treat atomic judgments as fixed, unanalyzed objects.
We now add the capability of defining such judgments by means of
formulas, possibly involving other predicates. In particular, we shall
assume that we are given a
fixed, finite set of \emph{clauses} of the
form  where 
is a predicate constant that takes a number of arguments equal to the
length of . Such a clause is said to define  and the
entire collection of clauses is called a {\em
  definition}. The expression , called the {\em body} of the
clause, must be a term that does not contain  or
any of the variables in  and must have a type such that
 has type .  Definitions are also restricted so that
a predicate is defined by at most one clause.
The intended interpretation of a clause  is that the atomic
formula , where  is a list of terms of the same
length and type as the variables in , is true if and only if
 is true.
This interpretation is realized by adding to the calculus the rules
 and  shown in Figure~\ref{fig:defrules} for unfolding
predicates on the left and the right of sequents using their defining
clauses.

A definition can have a recursive structure. For example, in the clause
, the predicate
 can appear free in .  In this setting, the meanings
of predicates are intended to be given by any one of the fixed points
that can be associated with the definition.  Such an interpretation may
not always be sensible. In particular, without further restrictions,
the resulting proof system may not be consistent.  There are two
constraints that suffice to ensure consistency. First, the body of a
clause must not contain any nominal constants. This restriction
can be justified from another perspective as well: as we see in
Chapter~\ref{ch:meta-theory}, it helps in establishing that 
is a provability preserving equivalence between formulas. Second, definitions
should be {\em stratified} so that clauses, such as , in which a predicate has a negative dependency on
itself, are forbidden.  While such stratification can be enforced in
different ways, we use a simple approach to doing this in this
thesis. This approach is based on associating with each predicate 
a natural number that is called its {\em level} and that is denoted
by .  This measure is then extended to arbitrary formulas by
the following definition.
\begin{definition}
Given an assignment of levels to predicates, the function  is
extended to all formulas in -normal form as follows:
\begin{enumerate}
\item 
\item 
\item 
\item 
\item 
\end{enumerate}
In general, the level of a formula , written as , is the
level of its -normal form.
\end{definition}

A definition is {\em stratified} if we can assign levels to predicates
in such a way that  for each clause
 in that
definition.

\begin{figure}[t]
\begin{center}
\15pt]
\
\member X (X::L) \triangleq \top \hspace{2cm}
\member X (Y::L) \triangleq \member X L

\member X K \triangleq (\exists L.~ K = (X :: L)) \lor
 (\exists Y \exists L.~ K = (Y :: L) \land \member X L)

\ctx nil \triangleq \top \hspace{2cm}
(\nabla x. \ctx (\of x T :: L)) \triangleq \ctx L

\ctx K \triangleq (K = nil) \lor
(\exists T \exists L.~ (\lambda x . \of x T :: L) \unrhd K \land \ctx
L)
\forall \vec{x}.(\nabla \vec{z}. p\ \vec{t}) \triangleq
B\ p\ \vec{x}5pt]
for any clause  in  and any  such that
 and  holds\
\{\forall \vec{x}_i.~ (\nabla \vec{z}_i. p\ \vec{t}_i) \triangleq
B_i\ p\ \vec{x}_i\}_{i\in 1..n}

\forall \vec{y} . p\ \vec{y} \triangleq \bigvee_{i\in 1..n} \exists \vec{x}_i
. ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land B_i\
p\ \vec{x}_i

\infer[\defR^p]
      {\Gamma \lra p\; \vec{s}}
      {\Gamma \lra (B_i\; p\; \vec{x}_i)[\theta]}

\infer[\defR]{\Gamma \lra p'\; \vec{t}}
{\infer=[\lorR]
 {\Gamma \lra \bigvee_{i\in 1..n} \exists \vec{x}_i
  . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
 }
 {\infer=[\existsR]
  {\Gamma \lra \exists \vec{x}_i
   . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
  }
  {\infer[\landR]
   {\Gamma \lra ((\lambda \vec{z}_i . p'\ \vec{t}_i)[\theta] \unrhd
     p'\ \vec{s}) \land (B_i\ p\ \vec{x}_i)[\theta]
   }
   {\infer[\unrhdR]{\Gamma \lra (\lambda \vec{z}_i . p'\
       \vec{t}_i)[\theta] \unrhd p'\ \vec{s}}{}
    &
    \Gamma \lra (B_i\; p\; \vec{x}_i)[\theta]
   }
  }
 }
}

\infer[\defL^p]
      {\Sigma : \Gamma, p\; \vec{s} \lra C}
      {\left\{
         \Sigma\theta : \Gamma\cas{\theta}, (B_i\; p\;
         \vec{x}_i)\cas{\theta} \lra C\cas{\theta}\ |\
           \hbox{ is a solution to }
              \right\}_{i\in 1..n}
      }

\hspace{-1.8cm}
\infer[\defL]{\Gamma, p\; \vec{s} \lra C}
{
 \infer=[\lorL]
 {\Gamma, \bigvee_{i\in 1..n} \exists \vec{x}_i
  . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
  \lra C
 }
 {\hspace{2.2cm}\left\{\raisebox{-6ex}{
  \infer=[\existsL]
   {\Gamma, \exists \vec{x}_i
     . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
    \lra C
   }
   {\infer[\landL^*]
    {\Gamma, ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s}) \land
     B_i\ p\ \vec{x}_i
     \lra C
    }
    {\infer[\unrhdL]
     {\Gamma, (\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{s},
       B_i\ p\ \vec{x}_i
       \lra C
     }
     {\left\{\hbox{
        is a solution to
	   
      }
      \right\}
     }
    }
   }
 }\right\}_{i \in 1..n}\hspace{3cm}
 }
}

\{\forall \vec{x}_i.~ (\nabla \vec{z}_i. p\ \vec{t}_i) \triangleq
B_i\ p\ \vec{x}_i\}_{i\in 1..n}

\forall \vec{y} . \left[p\ \vec{y} \equiv \bigvee_{i\in 1..n} \exists
  \vec{x}_i . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y})
  \land B_i\ p\ \vec{x}_i\right]

\infer[\IL^p]
{\Sigma : \Gamma, p\ \vec{s} \lra C}
{\left\{\vec{x}_i : B_i\ S\ \vec{x}_i \lra \nabla \vec{z}_i.S\
  \vec{t}_i\right\}_{i\in 1..n} \quad
  \Sigma : \Gamma, S\ \vec{s} \lra C}
\{\forall \vec{x}_i.
(\nabla \vec{z}_i. p\ \vec{t}_i) \mueq B_i\ p\ \vec{x}_i\}_{i\in
  1..n}
\forall \vec{y} . p\ \vec{y} \mueq \bigvee_{i\in 1..n} \exists \vec{x}_i
. ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land B_i\
p\ \vec{x}_i.

\small
\hspace{-2.3cm}
\infer=[\lorL]
{\vec{y} : \bigvee_{i\in 1..n} \exists \vec{x}_i
  . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land
  B_i\ S\ \vec{x}_i
  \lra S\ \vec{y}
}
{\hspace{2.8cm}\left\{\raisebox{-6ex}{
    \infer=[\existsL]
    {\vec{y} : \exists \vec{x}_i
      . ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land
      B_i\ S\ \vec{x}_i \lra S\ \vec{y}
    }
    {\infer[\landL^*]
      {\vec{y}, \vec{x}_i :
        ((\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y}) \land
        B_i\ p\ \vec{x}_i \lra S\ \vec{y}
      }
      {\infer[\unrhdL]
        {\vec{y}, \vec{x}_i :
          (\lambda \vec{z}_i . p'\ \vec{t}_i) \unrhd p'\ \vec{y},
          B_i\ S\ \vec{x}_i
          \lra S\ \vec{y}
        }
        {\left\{\hbox{
             is a solution to
            
          }
          \right\}
        }
      }
    }
  }\right\}_{i \in 1..n}\hspace{3cm}
}

\vec{x}_i : B_i\ p\ \vec{x}_i \lra S\ \vec{t}'_i

\vec{x}_i : B_i\ p\ \vec{x}_i \lra \nabla\vec{z}_i. S\ \vec{t}_i

(\nabla x.\fresh x E) \triangleq \top

\nabla z. B \qquad\quad
\exists z. (\fresh z \vec{x} \land B) \qquad\quad
\forall z. (\fresh z \vec{x} \supset B)

\nabla z. \forall x. (B\ z\ x) &\equiv \forall h. \nabla z.
(B\ z\ (h\ z)) &
\nabla z. \exists x. (B\ z\ x) &\equiv \exists h. \nabla z.
(B\ z\ (h\ z))

\forall x. \nabla z. (B\ z\ x) &\equiv
\nabla z. \forall x. (\fresh z x \supset B\ z\ x)
&
\exists x. \nabla z. (B\ z\ x) &\equiv
\nabla z. \exists x. (\fresh z x \land B\ z\ x)

&\spec {(\monoTy T)} {nil} T \mueq \top \\
(\nabla x. &\spec {(\polyTy P)} {(x::L)} {(T\ x)}) \mueq
\nabla x. \spec {(P\ x)} L {(T\ x)}.

& \subst {nil} T T \mueq \top \\
(\nabla x. &\subst {(\tup{x,V}::L)} {(T\; x)} S) \mueq
\subst L {(T\; V)} S

&\forall \ell, t, r, s.~
\subst \ell {(\app t r)} s \supset
\exists u, v. (s = \app u v \land \subst \ell t u \land \subst \ell r v)
\\
&\forall \ell, t, r.~
\subst \ell {(\uabs t)} r \supset
\exists s. (r = \uabs s \land \nabla z. \subst \ell {(t\; z)} (s\; z))

Both of these lemmas have straightforward proofs by induction on {\sl
  subst}.

We use this technique for describing arbitrary cascading substitutions again in
Section~\ref{sec:girards-strong-norm} to formalize Girard's strong
normalization argument for the simply-typed -calculus.



