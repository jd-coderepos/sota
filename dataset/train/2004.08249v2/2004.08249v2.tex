

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\usepackage{xspace}
\usepackage{enumitem}
\usepackage{smile}
\usepackage{url}

\usepackage{booktabs} \usepackage{diagbox}

\usepackage{multirow}
\usepackage{balance}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{wrapfig,lipsum}
\usepackage[noend,ruled]{algorithm2e}
\usepackage{makecell}
\usepackage[T1]{fontenc}
\usepackage{soul}
\usepackage{xcolor}
\setul{}{1.6pt}
\newcommand{\soutthick}[1]{\st{#1}}
\usepackage{lipsum}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\usepackage{appendix}
\usepackage{tabularx, booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\arraybackslash}X}


\def\aclpaperid{1552}
\aclfinalcopy  



\newcommand\BibTeX{B{\sc ib}\TeX}

\newcommand{\our}{\mbox{Admin}\xspace}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\TODO}[1]{{\color{red}{TODO: #1}}}
\newcommand{\smallsection}[1]{{\vspace{0.2cm}\noindent\textbf{#1.}}}

\newcommand{\LN}{{f_{\mbox{\scriptsize LN}}}}
\newcommand{\ATT}{{f_{\mbox{\scriptsize ATT}}}}
\newcommand{\SATT}{{f_{\mbox{\scriptsize S-ATT}}}}
\newcommand{\EATT}{{f_{\mbox{\scriptsize E-ATT}}}}
\newcommand{\FFN}{{f_{\mbox{\scriptsize FFN}}}}
\newcommand{\softmax}{{f_{s}}}
\newcommand{\ep}{^{(pe)}}
\newcommand{\pd}{^{(pd)}}
\newcommand{\eo}{^{(oe)}}
\newcommand{\od}{^{(od)}}
\newcommand{\xbteo}{{\xb^T}^{(oe)}}

\newcommand{\wf}{W^{(1)}}
\newcommand{\wff}{W^{(2)}}
\newcommand{\wk}{W^{(K)}}
\newcommand{\wv}{W^{(V_1)}}
\newcommand{\wq}{W^{(Q)}}
\newcommand{\wo}{W^{(V_2)}}

\newcommand{\hxb}{\mathbf{\hat{x}}}
\newcommand{\hzb}{\mathbf{\hat{z}}}
\newcommand{\hbb}{\mathbf{\hat{b}}}
\newcommand{\hab}{\mathbf{\hat{a}}}

\title{Understanding the Difficulty of Training Transformers}

\author{
Liyuan Liu\textsuperscript{}~~
Xiaodong Liu\textsuperscript{}~~
Jianfeng Gao\textsuperscript{}~~
Weizhu Chen\textsuperscript{}~~
Jiawei Han\textsuperscript{}\\
\texttt{\small \{ll2, hanj\}@illinois.edu}
,~~\texttt{\small \{xiaodl,jfgao,wzchen\}@microsoft.com}
\\
\textsuperscript{}{University of Illinois at Urbana-Champaign} \\ \textsuperscript{}Microsoft Research \\ \textsuperscript{} Microsoft Dynamics 365 AI
}

\date{}

\begin{document}
\maketitle



\begin{abstract}
  


Transformers have proved effective in many NLP tasks.
However, their training requires non-trivial efforts regarding  designing cutting-edge optimizers and learning rate schedulers carefully (\eg, conventional SGD fails to train Transformers effectively). 
Our objective here is to understand \emph{what complicates Transformer training} from both empirical and theoretical perspectives. 
Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. 
Instead, we identify an \emph{amplification effect} that influences training substantially -- 
for each layer in a multi-layer Transformer model, 
heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (\eg, parameter updates)
and results in significant disturbances in the model output.
Yet we observe that a light dependency limits the model potential and leads to inferior trained models. 
Inspired by our analysis, we propose \emph{\our} (\textbf{Ad}aptive \textbf{m}odel \textbf{in}itialization) to stabilize stabilize the early stage's training and unleash its full potential in the late stage. 
Extensive experiments show that \our is more stable, converges faster, and leads to better performance\footnote{Implementations are released at: \url{https://github.com/LiyuanLucasLiu/Transforemr-Clinic}}. 
 \end{abstract}



\section{Introduction}

Transformers~\cite{Vaswani2017AttentionIA} have led to a series of breakthroughs in various deep learning tasks ~\cite{Devlin2019BERTPO,Velickovic2017GraphAN}. 
They do not contain recurrent connections and can parallelize all computations in the same layer, thus improving effectiveness, efficiency, and scalability.
Training Transformers, however, requires extra efforts.
For example, although stochastic gradient descent (SGD)
is the standard algorithm for conventional RNNs and CNNs, it converges to bad/suspicious local optima for Transformers~\cite{Zhang2019WhyAB}.
Moreover, comparing to other neural architectures, removing the warmup stage in Transformer training results in more severe consequences such as model divergence~\cite{Popel2018TrainingTF,Liu2019OnTV}.
Here, we conduct comprehensive analyses in empirical and theoretical manners to answer the question: \textit{what complicates Transformer training}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/wmt14-devppl-6l-18l.pdf}
\caption{Lacking enough robustness and stability, the 18-Layer Post-LN Transformer training (\ie the original architecture) diverges and is omitted in the left graph. \our not only stabilizes model training but unleashes the model potential for better performance. 
}
\label{fig:wmt14ende_devppl_6_18}
\end{figure}

Our analysis starts from the observation: the original Transformer (referred to as Post-LN) is less robust than its Pre-LN variant\footnote{As in Figure~\ref{fig:pre-post-diagram}, Post-LN places layer norm outside of residual blocks, and Pre-LN moves them to the inside.}~\cite{Baevski2018AdaptiveIR,Xiong2019OnLN,Nguyen2019TransformersWT}. 
We recognize that gradient vanishing issue is not the direct reason causing such difference, since fixing this issue alone cannot stabilize Post-LN training. 
It implies that, besides unbalanced gradients, there exist other factors influencing model training greatly. 



\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/Pre-Post-new.pdf}
\caption{The Architecture and notations of Pre-LN Transformers (Left) and Post-LN Transformers (Right).}
\label{fig:pre-post-diagram}
\end{figure*}
 
With further analysis, we recognize that for each Transformer residual block, the dependency on its residual branch\footnote{For a residual block , its shortcut output refers to , its residual branch output refers to , and the dependency on its residual branch refers to .} plays an essential role in training stability. 
First, we find that a Post-LN layer has a heavier dependency on its residual branch than a Pre-LN layer.
As in Figure~\ref{fig:layer-dependency}, at initialization, a Pre-LN layer has roughly the same dependency on its residual branch and any previous layer, whereas a Post-LN layer has a stronger dependency on its residual branch 
(more discussions are elaborated in Section~\ref{subsec:impact-ln}). 
We find that strong dependencies of Post-LN amplify fluctuations brought by parameter changes and destabilize the training (as in Theorem~\ref{theo:layer-dependency-and-shift} and Figure~\ref{fig:stability}). 
Besides, the loose reliance on residual branches in Pre-LN generally limits the algorithm's potential and often produces inferior models.  

In light of our analysis, we propose \our, an adaptive initialization method which retains the merits of Pre-LN stability without hurting the performance.
It restricts the layer dependency on its residual branches in the early stage and unleashes the model potential in the late stage.
We conduct experiments on IWSLT'14 De-En,  WMT'14 En-De, and WMT'14 En-Fr;
\our is more stable, converges faster, and achieves better performance. 
For example, without introducing any additional hyper-parameters, \our successfully stabilizes 72-layer Transformer training on WMT'14 En-Fr and achieves a 43.80 BLEU score.
 

\section{Preliminaries}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/annotate_all_postln.pdf}
\caption{Relative gradient norm histogram (on a log scale) of 18-layer Transformers on the WMT'14 En-De dataset, \ie, the gradient norm of sub-layer outputs, scaled by the largest gradient norm in the same network. 
}
\label{fig:gradient-histogram}
\end{figure*}

\smallsection{Transformer Architectures and Notations}
\label{subsec:architecture}
The Transformer architecture contains two types of sub-layers, \ie, Attention sub-layers and Feedforward (FFN) sub-layers. 
They are composed of mainly three basic modules~\cite{Vaswani2017AttentionIA}, \ie, Layer Norm (), Multi-head Attention (), and Feedforward Network ().

As illustrated in Figure~\ref{fig:pre-post-diagram}, the Pre-LN Transformer and the Post-LN Transformer organize these modules differently. 
For example, a Pre-LN encoder organizes the Self-Attention sub-layer as  and a Post-LN encoder as , where  is the input of the -th Transformer layer and  is the output of the -th Self-Attention sub-layer.
Here, we refer   and  as the residual branches and their outputs as the residual outputs, in contrast to layer/sub-layer outputs, which integrates residual outputs and shortcut outputs. 


Notation elaborations are shown in Figure~\ref{fig:pre-post-diagram}.
In particular, we use superscripts to indicate network architectures (\ie, the Pre-LN Encoder), use subscripts to indicate layer indexes (top layers have larger indexes), all inputs and outputs are formulated as .  


\smallsection{Layer Norm}
Layer norm~\cite{Ba2016LayerN} plays a vital role in Transformer architecture. It is defined as , where  and  are the mean and standard deviation of 
.

\smallsection{Feedforward Network}
Transformers use two-layer perceptrons
as feedforward networks,
\ie, , where  is the non-linear function\footnote{Our analysis uses ReLU as the activation function, while \our can be applied to other non-linear functions.}, and  
are parameters.

\smallsection{Multi-head Attention}
Multi-head Attentions allows the network to have multiple focuses in a single layer and plays a crucial role in many tasks~\cite{Chen2018TheBO}.
It is defined as (with  heads):
,
where  is the row-wise softmax function and  are parameters. 
 and  are  matrices,  and  are  matrices, where  is the hidden state dimension. 
Parameters without subscript refer the concatenation of all -head parameters, \eg, . 
In Transformer,
this module is used in two different settings: Encoder-Attention ( and  is the encoder output), and Self-Attention ().

 


\section{Unbalanced Gradients}
\label{sec:gradient-distribution}

In this study, we strive to answer the question: \emph{what complicates Transformer training}.
Our analysis starts from the observation: Pre-LN training is more robust than Post-LN, while Post-LN is more likely to reach a better performance than Pre-LN. 
In a parameter grid search (as in Figure~\ref{fig:grid-search}), Pre-LN converges in all 15 settings, and Post-LN diverges in 7 out of 15 settings; when Post-LN converges, it outperforms Pre-LN in 7 out of 8 settings. 
We seek to reveal the underlying factor that destabilizes Post-LN training and restricts the performance of Pre-LN. 

\begin{figure*}[t]
\begin{minipage}{0.73\linewidth}
\includegraphics[width=\textwidth]{fig/annotated_stability_left.pdf}
\captionof{figure}{
Encoder output changes for parameter changes,
\ie,  where  is random perturbations (left) or gradient updates (right). 
Intuitively, very large  indicates the training to be ill-conditioned.}
\label{fig:stability}
\end{minipage}
\,
\begin{minipage}{0.24\linewidth}
\includegraphics[width=\textwidth]{fig/annotated_stability_right.pdf}
 \captionof{figure}{Histogram of relative norm of gradient and  where  is the checkpoint saved after training for  epochs.}
\label{fig:all_histogram}
\end{minipage}
\end{figure*}

In this section, we focus on the unbalanced gradients (\eg, gradient vanishing). 
We find that, although 
Post-LN suffers from gradient vanishing and Pre-LN does not, 
gradient vanishing is not the direct reason causing the instability of Post-LN. 
Specifically, we first theoretically and empirically establish that only Post-LN decoders suffer from gradient vanishing and Post-LN encoders do not.
We then observe that fixing the gradient vanishing issue alone cannot stabilize training.


\subsection{Gradients at Initialization}
\label{subsec:ini_grad}

As gradient vanishing can hamper convergence from the beginning, it has been regarded as the major issue causing unstable training. 
Also, recent studies show that this issue exists in the Post-LN Transformer, even after using residual connections~\cite{Xiong2019OnLN}. 
Below, we establish that only Post-LN decoders suffer from the gradient vanishing, and neither Post-LN encoders, Pre-LN encoders, nor Pre-LN decoders. 

\begin{table}[t]
\begin{center}
\small
\vspace{0.4cm}
\begin{tabularx}{\linewidth}{ *{4}{Y}}
\toprule
\textbf{Encoder} & \textbf{Decoder} & \textbf{Gradient} & \textbf{Training} \\ \midrule
Post-LN & Post-LN & Varnishing & Diverged \\
Post-LN & Pre-LN & \soutthick{Varnishing} & Diverged \\
Pre-LN & Pre-LN & \soutthick{Varnishing} & Converged \\
\bottomrule
\end{tabularx}
\end{center}
\caption{Changing decoders from Post-LN to Pre-LN fixes gradient vanishing, but does not stabilize model training successfully. Encoder/Decoder have 18 layers.}
\label{tbl:fixing-gradient-vanishing}
\end{table}

We use  to denote gradients, \ie,  where  is the training objective. 
Following previous studies~\cite{Glorot2010UnderstandingTD},
we analyze the gradient distribution at the very beginning of training and find only Encoder-Attention sub-layers in Post-LN suffers from gradient vanishing.  

First, we conduct analysis from a theoretical perspective. 
Similar to \citet{Xiong2019OnLN}, we establish that Pre-LN networks do not suffer from gradient vanishing (as elaborated in Appendix~\ref{subsec:preln-analysis}). 
Unlike \citet{Xiong2019OnLN}, we recognize that not all Post-LN networks suffer from gradient vanishing. 
As in Theorem~\ref{theo:post-ln-encoder-gradient}, we establish that Post-LN Encoder networks do not suffer from gradient vanishing. 
Detailed derivations are elaborated in Appendix~\ref{subsec:postln-encoder-analysis}. 

\begin{theorem}
For Post-LN Encoders, if  and  in the Layer Norm are initialized as  and  respectively; all other parameters are initialized by symmetric distributions with zero mean;  and  are subject to symmetric distributions with zero mean; the variance of  is  (\ie, normalized by Layer Norm);  and the derivatives of modules in -th sub-layer are independent, we have .
\label{theo:post-ln-encoder-gradient}
\end{theorem}

To make sure that the assumptions of Theorem~\ref{theo:layer-dependency-and-shift} match the real-world situation, we further conduct empirical verification. 
At initialization, we calculate  for 18-layer Transformers\footnote{Note if , .} and visualize
 
in Figure~\ref{fig:gradient-histogram}.
It verifies that only Post-LN decoders suffer from the gradient vanishing. 
Besides, we can observe that the dropping of gradient norms mostly happens in the backpropagation from encoder-attention outputs (encoder-attention bars) to its inputs (self-attention bars, since the output of self-attention is the input of encoder-attention). 
This pattern is further explained in Appendix~\ref{subsec:postln_decoder_analysis}. 

\subsection{Impact of the Gradient Vanishing}

Now, we explore whether gradient vanishing is the direct cause of training instability.





First, we design a controlled experiment to show the relationship between gradient vanishing and training stability. 
We construct a hybrid Transformer by combining a Post-LN encoder and a Pre-LN decoder. 
As in Section~\ref{subsec:ini_grad}, only Post-LN decoders suffer from gradient vanishing, but not Post-LN encoders. 
Therefore, this hybrid Transformer does not suffer from gradient vanishing. 
As shown in Table~\ref{tbl:fixing-gradient-vanishing}, fixing gradient vanishing alone (\ie, changing Post-LN decoders to Pre-LN decoders) fails to stabilize model training. 
This observation provides evidence supporting that the gradient vanishing issue is not the direct cause of unstable Post-LN training. 

Moreover, we observe
that gradients of all attention modules are unbalanced, while adaptive optimizers mostly address this issue.
As in Figure~\ref{fig:all_histogram}, adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes even with unbalanced gradients.
It explains why the standard SGD fails in training Transformers (\ie, lacking the ability to handle unbalanced gradients) and necessitates using adaptive optimizers. 
More discussions are included in Appendix~\ref{subsec:attention_gradients}. 

 

\section{Instability from Amplification Effect}
\label{sec:layer-dependency}

We find that unbalanced gradients are not the root cause of the instability of Post-LN, which implies the existence of other factors influencing model training.
Now, we go beyond gradient vanishing and introduce the \emph{amplification effect}.
Specifically, we first examine the difference between Pre-LN and Post-LN, including their early-stage and late-stage training. 
Then, we show that Post-LN's training instability is attributed to layer dependency's amplification effect, which intensifies gradient updates and destabilizes training.



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/weight.pdf}
\caption{
The major difference between Pre-LN and Post-LN is the position of layer norms. 
}
\label{fig:layer-norm-position}
\end{figure}

\subsection{Impact of Layer Norms Positions}
\label{subsec:impact-ln}

As described in Section~\ref{subsec:architecture}, both Pre-LN and Post-LN employ layer norm to regularize inputs and outputs. 
Different residual outputs are aggregated and normalized in residual networks before serving as inputs of other layers (\ie, residual outputs will be scaled to ensure the integrated input to have a consistent variance). 
To some extend, layer norm treats the variance of residual outputs as weights to average them.
For example, for Post-LN Self-Attention, we have

at initialization. 
Larger  not only increases the proportion of  in  but decreases the proportion of other residual outputs.
Intuitively, this is similar to the weight mechanism of the weighted average.  



The position of layer norms is the major difference between Pre-LN and Post-LN and makes them aggregate residual outputs differently (\ie, using different weights). 
As in Figure~\ref{fig:layer-norm-position}, all residual outputs in Pre-LN are only normalized once before feeding into other layers (thus only treating residual output variances as weights); in Post-LN, most residual outputs are normalized more than once, and different residual outputs are normalized for different times. 
For example, if all layers are initialized in the same way, output variances of different Pre-LN residual branches would be similar, and the aggregation would be similar to the simple average. 
Similarly, for Post-LN, nearby residual outputs are normalized by fewer times than others, thus having relatively larger weights. 
We proceed to calculate and analyze these weights to understand the impact of layer norm positions. 



First, we use  to refer  (\ie, normalized outputs of -th residual branch) and  to refer
 (\ie, normalized outputs of -th layer or normalized inputs of (+1)-th residual branch).
Then, we describe their relationships as , where  integrates scaling operations of all layer norms (including ). 
For example, Pre-LN sets . 
Intuitively,  describes the proportion of -th residual branch outputs in -th layer outputs, thus reflects the dependency among layers.

We visualize  in Figure~\ref{fig:layer-dependency}. 
For a Post-LN layer, its outputs rely more on its residual branch from the initialization to the end.
At initialization, Pre-LN layer outputs have roughly the same reliance on all previous residual branches. 
As the training advances, each layer starts to rely more on its own residual outputs. 
However, comparing to Post-LN, Pre-LN layer outputs in the final model still has less reliance on their residual branches. 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/annotate_ld.pdf}
\caption{
 in 6-Layer Post-LN and Pre-LN on the WMT-14 En-De dataset (contains 12 sub-layers).
}
\label{fig:layer-dependency}
\end{figure}


Intuitively, it is harder for Pre-LN layers to depend too much on their own residual branches.
In Pre-LN, layer outputs (\ie, ) are not normalized, and their variances are likely to be larger for higher layers\footnote{If  and  are independent, ; also, in our experiments  increases as  becomes larger}.
Since ,  is likely to be smaller for higher layers, which restricts -th layer outputs from depending too much on its residual branch and inhibits the network from reaching its full potential.
In other words, Pre-LN restricts the network from being too deep 
(\ie, if it is hard to distinguish  and ,
appending one layer would be similar to doubling the width of the last layer), 
while Post-LN gives the network the choice of being wider or deeper.


\subsection{Amplification Effect at Initialization}

Although depending more on residual branches allows the model to have a larger potential, it amplifies the fluctuation brought by parameter changes. 
For a network  where  is the model input and  is the parameter, 
the output change caused by parameter perturbations is , where . 
Its relationship with  is described in Theorem~, and the derivation is elaborated in Appendix~\ref{appendix:theo2}. 
\begin{theorem}
Consider a -layer Transformer  at initialization, where  is the input and  is the parameter. 
If the layer dependency stays the same after a parameter change 
(\ie,  has the same value after changing  to , where  is randomly initialized and  is independent to ), the output change (\ie, ) can be estimated as  where  is a constant.
\label{theo:layer-dependency-and-shift}
\end{theorem}
If  is the same for all layers, Pre-LN sets  as , and Post-LN sets  as a constant.
Thus, we have Corollary~\ref{cor:pre-ln} and \ref{cor:post-ln} as below.
\begin{corollary}
For a -layer Pre-LN , we have .
\label{cor:pre-ln}
\end{corollary}
\begin{corollary}
For a -layer Post-LN , we have .
\label{cor:post-ln}
\end{corollary}
They show that, since Post-LN relies more on residual branches than Pre-LN (\ie, has a larger ), the perturbation is amplified to a larger magnitude. 
To empirically verify these relationships, we calculate  for Pre-LN and Post-LN and visualize the results in Figure~\ref{fig:stability}. 
In Corollary~\ref{cor:post-ln},  is linearly associated with   for Post-LN; and in Corollary~\ref{cor:pre-ln},  is linearly associated with 
 for Pre-LN.
These relationships match the observation in our experiments (as in Figure~\ref{fig:stability}).
For further verification, we measure their correlation magnitudes by  and find  in both cases. 

Moreover, we replace the random noise  with optimization updates (\ie, setting , where  is update calculated by the Adam optimizer) and visualize output shifts. 
This replacement makes the correlation between  and  (for Post-LN) or  (for Pre-LN) to be weaker (\ie, ).
Still, as in Figure~\ref{fig:stability}, the output shift  for Post-LN is larger than Pre-LN by multiple magnitudes. 

Intuitively, large output shifts would destabilize the training~\cite{Li2018VisualizingTL}.
Also, as elaborated in Appendix~\ref{appendix:theo2}, the constant  in Theorem~\ref{theo:layer-dependency-and-shift} is related to network derivatives and would be smaller as training advances, which explains why warmup is also helpful for the standard SGD. 
Therefore, we conjecture it is the large output shift of Post-LN results in unstable training.
We proceed to stabilize Post-LN by controlling the dependency on residual branches in the early stage of training. 



\subsection{\textit{Admin} -- Adaptive Model Initialization}
\label{subsec:admin}

In light of our analysis, we add additional parameters (\ie, ) to control residual dependencies of Post-LN and stabilize training by adaptively initializing  
to ensure an  output change. 

Due to different training configurations and model specificities (\eg, different models may use different activation functions and dropout ratios), it is hard to derive a universal initialization method. 
Instead, we decompose model initialization into two phrases: \textit{Profiling} and \textit{Initialization}. 
Specifically, \our adds new parameters  and constructs its i-th sub-layer as , where ,  is a -dimension vector and  is element-wise product. 
Then the \textit{Profiling} phrase and \textit{Initialization} phrase are:

\smallsection{Profiling}
After initializing the network with a standard method (initializing  as ), conduct forward propagation without parameter updating and record the output variance of residual branches (\ie, calculate ). 
Since all elements in the same parameter/output matrix are independent to each other and are subject to the same distribution, it is sufficient to use a small number of instances in this phrase.
In our experiments, the first batch (no more than 8192 tokens) is used. 

\smallsection{Initialization}
Set  and initialize all other parameters with the same method used in the \textit{Profiling} phrase. 

In the early stage, \our sets  to approximately  and ensures an  output change, thus stabilizing training. 
Model training would become more stable in the late stage (the constant  in Theorem~\ref{theo:layer-dependency-and-shift} is related to parameter gradients), and each layer has the flexibility to adjust  and depends more on its residual branch to calculate the layer outputs. 
After training finishes, \our can be reparameterized as the conventional Post-LN structure (\ie, removing ).  
More implementation details are elaborated in Appendix~\ref{appendix:implement}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/annotate_large_ld.pdf}
\caption{
 of 18-Layer Admin (Post-LN) and Pre-LN on the WMT-14 En-De dataset.
}
\label{fig:large-layer-dependency}
\end{figure}

\begin{table*}[t]
    \centering
    \caption{BLEU on IWSLT'14 De-En and WMT'14 En-Fr/De (AL-BL refers A-layer encoder \& B-layer decoder).}
    \label{tab:wmt14de}
    \begin{tabularx}{0.95\linewidth}{l|c|YY|YYY}
        \toprule
Dataset &
        IWSLT'14 De-En &
        \multicolumn{2}{c|}{WMT'14 En-Fr} &
        \multicolumn{3}{c}{WMT'14 En-De}\\
\midrule
        Enc \#--Dec \#
        & 6L--6L (small) 
        & 6L--6L & 60L--12L
        & 6L--6L & 12L--12L & 18L--18L \\


        \midrule
        Post-LN & 35.640.23
        & 41.29 & failed 
        & 27.80 & failed & failed \\
        Pre-LN & 35.500.04
        & 40.74 & 43.10
        & 27.27 & 28.26 & 28.38 \\ 
        Admin & \textbf{35.670.15} 
        & \textbf{41.47} & \textbf{43.80}
        & \textbf{27.90} & \textbf{28.58} & \textbf{29.03} \\
        \bottomrule
    \end{tabularx}
\end{table*}



To verify our intuition, we calculate the layer dependency of 18-Layer models and visualize the result in Figure~\ref{fig:large-layer-dependency}. 
Figures~\ref{fig:layer-dependency} and \ref{fig:large-layer-dependency}
show that \our avoids over-large dependencies at initialization and unleashes the potential to make the layer outputs depend more on their residual outputs in the final model. 
Moreover, we visualize the output change of \our in Figure~\ref{fig:stability}. 
Benefiting from the adaptive initialization, the output change of \our gets roughly the same increase speed as Pre-LN, even constructed in the Post-LN manner. 
Also, although \our is formulated in a Post-LN manner and suffers from gradient vanishing, 18-layer \our successfully converges and outperforms 18-layer Pre-LN (as in Table~\ref{tab:wmt14de}). 
This evidence supports our intuition that the large dependency on residual branches amplifies the output fluctuation and destabilizes training. 
 

\section{Experiments}

We conduct experiments on 
IWSLT'14 De-En, WMT'14 En-De, and WMT'14 En-Fr. 
More details are elaborated in Appendix~\ref{appendix:exp}. 

\subsection{Performance Comparison}

We use BLEU as the evaluation matric and summarize the model performance in Table~\ref{tab:wmt14de}. 
On the WMT'14 dataset, we use Transformer-base models with 6, 12, or 18 layers. 
\our achieves a better performance than Post-LN and Pre-LN in all three settings. 
Specifically, 12-Layer and 18-Layer Post-LN diverges without the adaptive initialization.
Pre-LN converges in all settings, but it results in sub-optimal performance. 
Admin not only stabilizes the training of deeper models but benefits more from the increased model capacity then Pre-LN, which verifies our intuition that the Pre-LN structure limits the model potential. 
As in Figure~\ref{fig:wmt14ende_devppl_6_18} and Figure~\ref{fig:wmt14-devppl-12l-iwslt}, although the 6-layer Pre-LN converges faster than Post-LN, its final performance is worse than Post-LN. 
In contrast, \our not only achieves the same convergence speed with Pre-LN in the early stage but reaches a good performance in the late stage. 

We use 6-layer Transformer-small (its hidden dimension is smaller than the base model) on the IWSLT'14 dataset, and all methods perform similarly.
Still, as in Figure~\ref{fig:grid-search}, Admin outperforms the other two by a small margin. 
Together with WMT'14 results, it implies the training stability is related to layer number. 
For shallow networks, the stability difference between Post-LN and Pre-LN is not significant (as in Figure~\ref{fig:stability}), and all methods reach reasonable performance. 
It is worth mentioning that attention and activation dropouts have an enormous impact on IWSLT'14, which is smaller than WMT'14 datasets. 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/wmt14-devppl-12l-iwslt.pdf}
\caption{
Development PPL on the WMT'14 En-De dataset and the IWLST'14 De-En dataset. }
\label{fig:wmt14-devppl-12l-iwslt}
\end{figure}

To further explore the potential of \our, we train Transformers with a larger size. 
Specifically, we expand the Transformer-base configuration to have a 60-layer encoder and a 12-layer decoder. 
As in Table~\ref{tab:wmt14de}, our method achieves a BLEU score of 43.8 on the WMT'14 En-Fr dataset, the new state-of-the-art without using additional annotations (\eg, back-translation).
More discussions are conducted in Appendix~\ref{appendix:enfr} to compare this model with the current state of the art. 
Furthermore, in-depth analyses are summarized in \citet{Liu2020VeryDT}, including systematic evaluations on the model performance (with TER, METEOR, and BLEU), comprehensive discussions on model dimensions (\ie, depth, head number, and hidden dimension), and fine-grained error analysis.
It is worth mentioning that the 60L-12L Admin model achieves a 30.1 BLEU score on WMT'14 En-De~\cite{Liu2020VeryDT}. 









\subsection{Connection to Warmup}
\label{subsec:warmup}

Our previous work~\cite{Liu2019OnTV} establishes that the need for warmup comes from the unstable adaptive learning rates in the early stage.
Still, removing the warmup phrase results in more severe consequences for Transformers than other architectures.
Also, warmup has been found to be useful for the vanilla SGD~\cite{Xiong2019OnLN}.

Theorem~\ref{theo:post-ln-encoder-gradient} establishes that

where . 
In the early stage of training, the network has larger parameter gradients and thus larger . 
Therefore, using a small learning rate at initialization helps to alleviate the massive output shift of Post-LN. 
We further conduct experiments to explore whether more prolonged warmups can make up the stability difference between Post-LN and Pre-LN.
We observe that 18-layer Post-LN training still fails after extending the warmup phrase from 8 thousand updates to 16, 24, and 32 thousand.
It shows that learning rate warmup alone cannot neutralize the instability of Post-LN. 
Intuitively, massive output shifts not only require a small learning rate but also unsmoothes the loss surface~\cite{Li2018VisualizingTL} and make the training ill-conditioned.

\our regularizes the model behavior at initialization and stabilizes the training. 
To explore whether \our is able to stabilize the training alone, we remove the warmup phase and conduct a grid search on optimizer hyper-parameters. 
The results are visualized in Figure~\ref{fig:grid-search}. 
It shows that as Post-LN is more sensitive to the choice of hyper-parameters, \our successfully stabilizes the training without hurting its potential.  

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/grid_search.pdf}
\caption{
BLEU score of Post-LN, Pre-LN and Admin on the IWSLT'14 De-En dataset (x-axis is the  for adaptive optimizers and y-axis is the learning rate). Pre-LN converges in all settings while Post-LN diverges in 7 out of 15 settings. 
When Post-LN converges, it outperforms Pre-LN in 7 out of 8 settings. 
Admin stabilizes Post-LN training and outperforms Pre-LN (its best performance is comparable with Post-LN). }
\label{fig:grid-search}
\end{figure}

\subsection{Comparing to Other Initializations}

We compare our methods with three initialization methods, \ie, ReZero~\cite{Bachlechner2020ReZeroIA}, FixUp~\cite{zhang2018residual}, and LookLinear~\cite{balduzzi17b}.
Specifically, we first conduct experiments with 18-layer Transformers on the WMT'14 De-En dataset. 
In our experiments, we observe that all of ReZero (which does not contain layer normalization), FixUp (which also does not contain layer normalization), and LookLinear (which is incorporated with Post-LN) leads to divergent training.
With further analysis, we find that the half-precision training and dropout could destabilize FixUp and ReZero, due to the lack of layer normalization.
Simultaneously, we find that even for shadow networks, having an over small reliance on residual branches hurts the model performance, which also supports our intuition. 
For example, as elaborated in Appendix~\ref{appendix:rezero}, applying ReZero to Transformer-small leads to a 1-2 BLEU score drop on the IWSLT'14 De-En dataset. 
 

\section{Related Work}
\label{sec:related}

\smallsection{Transformer}
Transformer~\cite{Vaswani2017AttentionIA} has led to a series of breakthroughs in various domains~\cite{Devlin2019BERTPO,Velickovic2017GraphAN,Huang2019MusicTG,Parmar2018ImageT,Ramachandran2019StandAloneSI}. 
\citet{Liu2019OnTV} show that compared to other architectures, removing the warmup phase is more damaging for Transformers, especially Post-LN. 
Similarly, it has been found that the original Transformer (referred to as Post-LN) is less robust than its Pre-LN variant~\cite{Baevski2018AdaptiveIR,Nguyen2019TransformersWT,wang-etal-2019-learning}. 
Our studies go beyond the existing literature on gradient vanishing~\cite{Xiong2019OnLN} and identify an essential factor influencing Transformer training greatly. 


\smallsection{Deep Network Initialization} 
It has been observed that deeper networks can lead to better performance. 
For example, \citet{Dong2020TowardsAR} find that the network depth players a similar role with the sample number in numerical ODE solvers, which hinders the system from getting more precise results.
Many attempts have been made to clear obstacles for training deep networks, including various initialization methods.
Based on the independence among initialized parameters, one method is derived and found to be useful to handle the gradient vanishing~\cite{Glorot2010UnderstandingTD}. 
Similar methods are further developed for ReLU networks~\cite{He2015DelvingDI}. 
\citet{He2016DeepRL} find that deep network training is still hard even after addressing the gradient vanishing issue and propose residual networks.
\citet{David2017Shattered} identifies the shattered gradient issue and proposes LookLinear initialization. 


On the other hand, 
although it is observed that scaling residual outputs to smaller values helps to stabilize training~\cite{Hanin2018HowTS,Mishkin2015AllYN,zhang2018residual,Bachlechner2020ReZeroIA,Goyal2017AccurateLM}, there is no systematic analysis on what complicates Transformer training or its underlying connection to the dependency on residual branches.
Here, we identify that unbalanced gradients are not the direct cause of the Post-LN instability, recognize the amplification effect, and propose a novel adaptive initialization method.
 

\section{Conclusion}

In this paper, we study the difficulties of training Transformers in theoretical and empirical manners. 
Our study in Section~\ref{sec:gradient-distribution} suggests that the gradient vanishing problem is not the root cause of unstable Transformer training. 
Also, the unbalanced gradient distribution issue is mostly addressed by adaptive optimizers. 
In Section~\ref{sec:layer-dependency}, we reveal the root cause of the instability to be the strong dependency on residual branches, which amplifies the fluctuation caused by parameter changes and destabilizes model training. 
In light of our analysis, we propose \our, an adaptive initialization method to stabilize Transformers training. 
It controls the dependency at the beginning of training and maintains the flexibility to capture those dependencies once training stabilizes. 
Extensive experiments verify our intuitions and show that, without introducing additional hyper-parameters, \our achieves more stable training, faster convergence, and better performance. 

Our work opens up new possibilities to not only further push the state-of-the-art but understand deep network training better. 
It leads to many interesting future works, including generalizing Theorem~\ref{theo:layer-dependency-and-shift} to other models, designing new algorithms to automatically adapt deep networks to different training configurations, upgrading the Transformer architecture, and applying our proposed \our to conduct training in a larger scale.
 

\section*{Acknowledge}
We thank all reviewers for their constructive comments; Chengyu Dong, Haoming Jiang, Jingbo Shang, Xiaotao Gu, and Zihan Wang for valuable discussions and comments; Jingbo Shang for sharing GPU machines; and Microsoft for setting up GPU machines.
The research was sponsored in part by DARPA No. W911NF-17-C-0099 and No.  FA8750-19-2-1004, National Science Foundation IIS-19-56151, IIS-17-41317, IIS 17-04532, and IIS 16-18481, and DTRA HDTRA11810026.
 
\bibliography{citation.bib}
\bibliographystyle{acl_natbib}

\onecolumn
\appendix
\appendixpage

\setcounter{theorem}{0}    


\section{Gradients at Initialization}
\label{appendix:ini_grad}

Here, we first reveal that Pre-LN does not suffer from the gradient vanishing. 
Then we establish that only the Post-LN decoder suffers from the gradient vanishing, but not the Post-LN encoder. 
For simplicity, we use  to denote gradients, \ie,  where  is the training objective. 
Following the previous study~\cite{Bengio1994LearningLD,Glorot2010UnderstandingTD,He2015DelvingDI,saxe2013exact}, we analyze the gradient distribution at the very beginning of training, assume that the randomly initialized parameters and the partial derivative with regard to module inputs are independent.


\subsection{Pre-LN Analysis}
\label{subsec:preln-analysis}
For Pre-LN encoders, we have 
 and
. 
At initialization, the two terms on the right part are approximately independent and . Therefore we have .
Similarly, we can get  thus . 
Applying the same analysis to Pre-LN decoders, we can get .
Thus, lower layers have larger gradients than higher layers, and gradients do not vanish in the backpropagation. 
\begin{remark}
For Pre-LN, if  and the derivatives of modules in the -th sub-layer are independent, then .
\label{remark: pre-ln-gradient}
\end{remark}


\subsection{Post-LN Encoder Analysis}
\label{subsec:postln-encoder-analysis}
Different from Pre-LN,  and  are associated with not only the residual connection but the layer normalization, which makes it harder to establish the connection on their gradients.
After making assumptions on the model initialization, we find that lower layers in Post-LN encoder also have larger gradients than higher layers, and gradients do not vanish in the backpropagation through the encoder.   

\begin{theorem}
For Post-LN Encoders, if  and  in the Layer Norm are initialized as  and  respectively; all other parameters are initialized by symmetric distributions with zero mean;  and  are subject to symmetric distributions with zero mean; the variance of  is  (\ie, normalized by Layer Norm);  and the derivatives of modules in -th sub-layer are independent, we have .
\end{theorem}
\begin{proof}
We first prove , \ie, the backpropagation through FFN sublayers does not suffer from gradient vanishing. 
In Post-LN encoders, the output of FFN sublayers is calculated as  where .
Since at initialization,  and  are independently randomized by symmetric distributions, we have  and 
 
where . 
Referring to the dimension of  as , \citet{He2015DelvingDI} establishes that

Since in Post-LN,  is the output of layer norm, we have . 
Thus, 

Assuming different terms are also independent in the backpropagation, we have
 
At initialization, \citet{He2015DelvingDI} establishes that

Therefore, we have

Combining Equation~\ref{eqn:ffn-sigma} with Equation~\ref{eqn:ffn-back-propagation}, we have

which shows the backpropagation through FFN sublayers does not suffer from gradient vanishing. 

Now we proceed to prove that, , \ie, the backpropagation through Self-Attention sublayers do not suffer from gradient vanishing. 
In Post-LN encoders, the output of Self-Attention sublayers are calculated as 
 where  and . 
At initialization, since , , , and  are independently randomized by symmetric distributions, we have , thus , where .

Referring  as , we have 

Similar to \citet{He2015DelvingDI}, we have

Since  is the output of layer norm, we have . Thus, 

In the backpropagation, we have

At initialization, we assume  and model parameters are independent~\cite{He2015DelvingDI}, thus

Therefore, we have

Integrating Equation~\ref{eqn:satt-sigma} with Equation~\ref{eqn:satt-back-propagation}, we have

Combining Equation~\ref{eqn:ffn-no-vanish} and Equation~\ref{eqn:satt-no-vanish}, we have .
\end{proof}




\subsection{Post-LN Decoder Analysis}
\label{subsec:postln_decoder_analysis}
In Post-LN, the Encoder-Attention sub-layer suffers from gradient vanishing. 
The Encoder-Attention sub-layer calculates outputs as 
 where  and . 
Here  is encoder outputs and  is the row-wise softmax function. 
In the backpropagation, 
All of the backpropagations from  to  went through the softmax function, we have . 
Thus, those backpropagations suffer from gradient vanishing. 
This observation is further verified in Figure~\ref{fig:gradient-histogram}, as the encoder attention bars (gradients of encoder-attention outputs) are always shorter than self-attention bars (gradients of encoder-attention inputs), while adjacent self-attention bars and fully connected bars usually have the same length. 


\subsection{Distributes of Unbalanced Gradients}
\label{subsec:attention_gradients}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/self-attn-distribution-radam-four.pdf}
\caption{
Relative Norm of Gradient (, where  is the checkpoint of -th epoch) and Update () of Self-Attention Parameters in 12-Layer Pre-LN.
}
\label{fig:update-with-gradient-four}
\end{figure}

As in Figure~\ref{fig:all_histogram} and Figure~\ref{fig:update-with-gradient-four}, the gradient distribution of Attention modules is unbalanced even for Pre-LN. 
Specifically, parameters within the softmax function (\ie,  and ) suffer from gradient vanishing (\ie, ) and have smaller gradients than other parameters.

With further analysis, we find it is hard to neutralize the gradient vanishing of softmax. 
Unlike conventional non-linear functions like ReLU or sigmoid, softmax has a dynamic input length (\ie, for the sentences with different lengths, inputs of softmax have different dimensions). 
Although this setting allows Attention modules to handle sequential inputs, it restricts them from having stable and consistent backpropagation. 
Specifically, let us consider the comparison between softmax and sigmoid. 
For the sigmoid function, although its derivation is smaller than 1, this damping effect is consistent for all inputs. 
Thus, sigmoid can be neutralized by a larger initialization~\cite{Glorot2010UnderstandingTD}. 
For softmax, its damping effect is different for different inputs and cannot be neutralized by a static initialization. 


Also, we observe that adaptive optimizers largely address this issue. 
Specifically, we calculate the norm of parameter change in consequent epochs (\eg,  where  is the checkpoint saved after  epochs) and visualize the relative norm (scaled by the largest value in the same network) in Figure~\ref{fig:update-with-gradient-four}.  
Comparing the relative norm of parameter gradients and parameter updates, we notice that: although the gradient distribution is unbalanced, adaptive optimizers successfully assign different learning rates to different parameters and lead to consistent update magnitudes. 
This result explains why the vanilla SGD fails for training Transformer (\ie, lacking the ability to handle unbalanced gradient distributions). 
Besides, it implies that the unbalanced gradient distribution (\eg, gradient vanishing) has been mostly addressed by adaptive optimizers and may not significantly impact the training instability. 

\section{Proof of Theorem~\ref{theo:layer-dependency-and-shift}}
\label{appendix:theo2}

Here, we elaborate the derivation for Theorem~\ref{theo:layer-dependency-and-shift}, which establishes the relationship between layer number and output fluctuation brought by parameter change. 

\begin{theorem}
Consider a -layer Transformer , where  is the input and  is the parameter. 
If the layer dependency stays the same after a parameter change 
(\ie,  has the same value after changing  to , where  is randomly initialized and  is independent to ), the output change (\ie, ) can be estimated as  where  is a constant.
\end{theorem}

\begin{proof}

We refer the module in  sub-layer as , where  is the normalized residual output and  is the normalized module output. 
The final output is marked as .
To simplify the notation, we use the superscript  to indicate variables related to , \eg,  and .

At initialization, all parameters are initialized independently.
Thus ,  and  are independent and . 
Also, since -layer and -layer share the residual connection to previous layers,  we have .
Thus  and 


Now, we proceed to analyze . Specifically, we have

Since  is randomly initialized,  should have the same value for all layers, thus we use a constant  to refer its value ( and ). 
As to , since the sub-layer of Transformers are mostly using linear weights with ReLU nonlinearity and , we have . 
Thus, we can rewrite Equation~\ref{eqn:a-decompose} and get  

With Equation~\ref{eqn:recurrent}, we have

Therefore, we have .
\end{proof}

\section{\our Implementation Details}
\label{appendix:implement}

As introduced in Section~\ref{subsec:admin}, we introduce a new set of parameters to rescale the module outputs. 
Specifically, we refer these new parameters as  and construct the Post-LN sub-layer as:

where  is the element-wise product. 

After training, \our can be reparameterized as the conventional Post-LN structure (\ie, removing ). 
Specifically, we consider . 
Then, for feedforward sub-layers, we have

It can be reparameterized by changing , ,  to , ,  respectively, \ie, 

For Self-Attention sub-layers, we have 
 
It can be reparameterized by changing , , , ,  to , , ,   respectively, \ie, 

For Encoder-Attention sub-layers, we have 
 
It can be reparameterized by changing , ,  to , ,  respectively, \ie, 

It is easy to find  in all three situations. 

From the previous analysis, it is easy to find that introducing the additional parameter  is equivalent to rescale some model parameters. 
In our experiments on IWSLT14 De-En, we find that directly rescaling initialization parameters can get roughly the same performance with introducing . 
However, it is not very stable when conducting training in a half-precision manner. 
Accordingly, we choose to add new parameters  instead of rescaling parameters. 

\section{Experimental Setup}
\label{appendix:exp}

Our experiments are based on the implementation from the fairseq package~\citep{ott2019fairseq}.
As to pre-processing, we follow the public released script from previous work~\citep{ott2019fairseq,lu2020understanding}. 
For WMT'14 datasets, evaluations are conducted on the provided `newstest14` file, and more details about them can be found in \citet{bojar2014findings}. 
For the IWSLT'14 De-En dataset, more analysis and details can be found in \citet{cettolo2014report}. 

\begin{table}[t]
\centering
\caption{ReZero Performance on IWSLT'14 De-En. Models are Transformer-small w. 6-layer encoder \& decoder.}
\label{tab:rezero}
\begin{tabular}{r|ccccc}
\toprule
Models & Admin & Post-LN & Pre-LN & ReZero & ReZero+Post-LN \\
\midrule
BLEU & 35.670.15 & 35.640.23 & 35.500.04 & 33.670.14 & 34.670.08 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Performance and model size on WMT'14 En-Fr (AL-BL refers A-layer encoder \& B-layer decoder).}
\label{tab:wmt14fr}
\begin{tabularx}{\linewidth}{lccYY}
\toprule
Methods & Param. \# & dim() in FFN & Enc\#-Dec\# & BLEU \\
\midrule
T5-Base~\cite{raffel2019exploring} & 220 M &  & 6L-6L & 41.2 \\
T5-Large~\cite{raffel2019exploring} & 770 M &  & 12L-12L & 41.5 \\
T5-3B~\cite{raffel2019exploring} & 3 B &  & 24L-24L & 42.6 \\
T5-11B~\cite{raffel2019exploring} & 11 B &  & 24L-24L & 43.4 \\
\midrule
Trans.Big-RNMT+~\cite{Chen2018TheBO} & 377 M &   & 6L-6L & 41.12 \\
DynamicConv~\cite{wu2018pay} & 213 M &  & 7L-7L & 43.2 \\
DG-Transformer~\cite{Wu2019DepthGF} & 264 M &  & 8L-8L & 43.27 \\
Prime~\cite{zhao2019muse} & 252 M &  & 6L-6L & 43.48 \\
\midrule
Pre-LN (60L--12L) & 262 M &   & 60L-12L & 43.10 \\
Admin (60L--12L) & 262 M &  & 60L-12L & 43.80 \\
\bottomrule
\end{tabularx}
\end{table}

As to model specifics, we directly adopt Transformer-small configurations on the IWSLT'14 De-En dataset and stacks more layers over the Transformer-base model on the WMT'14 En-De and WMT'14 En-Fr datasets.
Specifically, on the IWSLT'14 De-En dataset, we use word embedding with 512 dimensions and 6-layer encoder/decoder with 4 heads and 1024 feedforward dimensions; on the WMT'14 En-De and WMT'14 En-Fr datasets, we use word embedding with 512 dimension and 8-head encoder/decoder with 2048 hidden dimensions. 
Label smoothed cross entropy is used as the objective function with an uncertainty ~\citep{szegedy2016rethinking}. 

For Model training, we use RAdam as the optimizer~\cite{Liu2019OnTV} and adopt almost all hyper-parameter settings from \citet{lu2020understanding}.
Specifically, for the WMT'14 En-De and WMT'14 En-Fr dataset, all dropout ratios (including (activation dropout and attention dropout) are set to 0.1. 
For the IWSLT'14 De-En dataset, after-layer dropout is set to , and a weight decay of  is used. 
As to optimizer, we set ,  use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps on the WMT'14 En-De/Fr dataset, and 6000 steps on the IWSLT'14 De-En dataset). 
The maximum learning rate is set to  on the WMT'14 En-De dataset and  on the IWSLT'14 De-En and WMT'14 En-Fr datasets. 
We conduct training for  epochs on the WMT'14 En-De dataset,  epochs on the IWSLT'14 De-En dataset and  epochs on the WMT'14 En-Fr dataset, while the last 10 checkpoints are averaged before inference. 

On the IWSLT'14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU and set the maximum batch size to be . 
On the WMT'14 En-De dataset, we conduct training on four NVIDIA Quadro R8000 GPUs and set maximum batch size (per GPU) as . 
On the WMT'14 En-Fr dataset, we conduct training with the Nvidia DGX-2 server (6L-6L uses 4 NVIDIA TESLA V100 GPUs and 60L-16L uses 16 NVIDIA TESLA V100 GPUs) and set the maximum batch size (per GPU) as . 
On the IWSLT'14 De-En dataset, Transformer-small models (w. 37 M Param.) take a few hours to train.
On the WMT'14 En-De dataset, 6L-6L models (w. 63 M Param.) take  day to train, 
12L-12L (w. 107M Param.) models take  days to train, 
and 18L-18L (w. 151M Param.) models take  days to train. 
On the WMT'14 En-Fr dataset, 6L-6L models (w. 67 M Param.) takes  days to train, and 60L-12L models (w. 262M Param.) takes  days to train. 
All training is conducted in half-precision with dynamic scaling (with a 256-update scaling window and a 0.03125 minimal scale).
All our implementations and pre-trained models would be released publicly. 

\section{Comparison to ReZero}
\label{appendix:rezero}

Here, we first conduct comparisons with ReZero~\cite{Bachlechner2020ReZeroIA} under two configurations--the first employs the original ReZero model, and the second adds layer normalizations in a Post-LN manner. 
As summarized in Table~\ref{tab:rezero}, the ReZero initialization leads to a performance drop, no matter layer normalization is used or not. 
It verifies our intuition that over small dependency restricts the model potential. 
At the same time, we find that adding layer normalization to ReZero helps to improve the performance. 
Intuitively, as dropout plays a vital role in regularizing Transformers, layer normalization helps to not only stabilize training but alleviate the impact of turning off dropouts during the inference. 

\section{Performance on the WMT'14 En-Fr}
\label{appendix:enfr}

To explore the potential of \our, we conduct experiments with 72-layer Transformers on the WMT'14 En-Fr dataset (with a 60-layer encoder and 12-layer decoder, we add less layers to decoder to encourage the model to rely more on the source context). 

As in Table~\ref{tab:wmt14fr}, \our (60L--12L) achieves a BLEU score of 43.80, the new state-of-the-art on this long-standing benchmark. 
This model has a 60-layer encoder and a 12-layer decoder, which is significantly deeper than other baselines.
Still, since the number of parameters increases in a quadratic speed with regard to hidden dimensions and a linear speed with regard to layer numbers, our model has roughly the same number of parameters with other baselines. 
It is worth mentioning that \our even achieves better performance than all variants of pre-trained T5 models, which demonstrates the great potential of our proposed method.
Also, \our achieves a better performance than Pre-LN (60L--12L), which further verifies that the Pre-LN architecture restricts deep models' potential. 




 

\end{document}