\documentclass{article}



\usepackage [nonatbib,preprint]{neurips_2021}



\usepackage{mathrsfs}


\usepackage[numbers,sort,compress]{natbib}





\clearpage{}\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \hypersetup{
     colorlinks=true,
     linkcolor=blue,
     filecolor=blue,
     citecolor = red,      
     urlcolor=cyan,
     }
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{sidecap}

\sidecaptionvpos{figure}{t}

\usepackage{mathtools} 

\usepackage{booktabs} \usepackage{tikz} \usetikzlibrary{calc,positioning} 
\usepackage{amsfonts}
\usepackage{cleveref}
\usepackage{bm}
\newcommand\subfigwidth{.24\textwidth}
\usepackage{tabu}
\usepackage[ruled,vlined]{algorithm2e}
\SetKw{KwBy}{by}
\usepackage{caption}
\usepackage{subcaption}

\crefname{equation}{}{}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\dg}[1]{{\bf \color{blue} Dar: #1}}
\usepackage[compact]{titlesec}


\newcommand{\eq}{}
\newcommand{\eqa}{}
\newcommand{\eqn}{}
\newcommand{\be}{}
\newcommand{\eqan}{}
\newcommand{\vp}{\varpropto}
\newcommand{\nn}{\nonumber}
\newcommand{\tab}{\hspace*{1em}}

\newcommand{\J}{ {\bf J} }
\newcommand{\M}{ {\bf M} }
\newcommand{\X}{ {\bf X} }
\newcommand{\V}{ {\bf V} }
\newcommand{\B}{ {\bf B} }
\newcommand{\q}{ {\bf q} }
\newcommand{\s}{ {\bf s} }

\newcommand{\cc}{ {\bf c} }
\newcommand{\bu}{ {\bf u} }
\newcommand{\bfa}{ {\bf a} }
\newcommand{\bb}{ {\bf b} }
\newcommand{\xx}{ {\bf x} }

\newcommand{\E}{{\mathrm{E}}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\eP}{{\mathbb{P}_n}}
\newcommand{\eG}{{\mathbb{G}_n}}
\newcommand{\Fcal}{{\mathcal F}}
\newcommand{\Gcal}{{\mathcal G}}
\newcommand{\Acal}{{\mathcal A}}
\newcommand{\Ccal}{{\mathcal C}}
\newcommand{\Rcal}{{\mathcal R}}
\newcommand{\Xcal}{{\mathcal X}}
\newcommand{\Ical}{{\mathcal I}}
\newcommand{\varphid}{{\dot \varphi}}

\usepackage{dsfont}
\newcommand{\Ind}{\mathds{1}}

\newcommand{\ubar}[1]{\text{\b{}}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{}\mkern2mu{#1#2}}}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}\clearpage{}


\title{Marginalizable Density Models}




\author{
   Dar Gilboa \\
   Harvard University \\
   \texttt{dar\_gilboa@fas.harvard.edu} \\
   \And
   Ari Pakman \\
   Columbia University \\
   \texttt{ari@stat.columbia.edu} \\
   \And
   Thibault Vatter \\
   Columbia University \\
   \texttt{thibault.vatter@columbia.edu} \\
}

\begin{document}
\maketitle
\begin{abstract}
Probability density models based on deep networks have achieved
remarkable success in modeling complex high-dimensional datasets.
However, unlike kernel density estimators, modern neural models do not yield marginals or conditionals in closed form, 
as these quantities require the evaluation of seldom tractable integrals.
In this work, we present the \emph{marginalizable density model approximator} (MDMA),
a novel deep network architecture which provides closed form expressions for the probabilities, marginals and conditionals of any subset of the variables.
The MDMA learns deep
scalar representations for each individual variable
and combines them via learned hierarchical tensor decompositions into a tractable yet expressive CDF,
from which marginals and conditional densities are easily obtained.
We illustrate the advantage of exact marginalizability in several tasks that are out of reach of previous deep network-based density estimation models, such as estimating mutual information between arbitrary subsets of variables, inferring causality by testing for conditional independence, and inference with missing data without the need for data imputation, outperforming state-of-the-art models on these tasks. The model also allows for parallelized sampling with only a logarithmic dependence of the time complexity on the number of variables.  
\end{abstract}




\section{Introduction}\label{sec:intro}

Estimating the joint probability density of a set of random variables is a fundamental task in statistics and machine learning that has witnessed much progress in recent years.
While traditional estimators such as histograms~\cite{scott1979optimal,lugosi1996consistency} and kernel-based methods~\cite{Rosenblatt,parzen1962estimation} have appealing theoretical properties and typically perform well in low dimensions, they become computationally impractical above 5-10 dimensions.
Conversely, recent density models based on neural networks~\cite{kingma2018,huang2018neural,oliva2018transformation,grathwohl2018ffjord,de2020block,bigdeli2020learning} scale efficiently with the number of random variables, but lack a crucial feature available to traditional methods: the ability to compute probabilities, marginalize over subsets of variables, and evaluate conditionals. These tasks require integrals of the estimated density, 
which are intractable for modern neural density models.  
Thus, while such operations are central 
to many applications (e.g., inference with missing data, testing for conditional (in)dependence, or performing do-calculus~\cite{pearl_book}), approaches based on neural networks require estimating separate models 
whenever marginals or conditionals are needed.  

Alternatively, one could model the cumulative distribution function (CDF), making computing probabilities and marginalization straightforward. But evaluating the density requires taking  derivatives of the CDF, which incurs an exponential cost in  for a generic computational graph.
This observation has made direct CDF modeling traditionally challenging~\cite{chilinski2020neural}.

In this work, we present the \emph{marginalizable density model approximator} (MDMA), a novel deep network architecture preserving most of the expressive power of neural models for density estimation, while  providing closed form expressions for the probabilities, marginals and conditionals of any subset of the variables.
In a nutshell, the MDMA learns many deep 
scalar representations for each individual variable 
and combines them using hierarchical tensor decompositions \cite{hackbusch2012tensor,cichocki2016tensor} into a tractable multivariate CDF that can be fitted using stochastic gradient descent.
Additionally, sampling from MDMA can be parallelized along the input dimension, resulting in 
a very low space complexity and a time complexity that scales only logarithmically with the number of variables in the problem (as opposed to linearly in naive autoregressive sampling, see below in \Cref{sec:related}). 


As could be expected, the architectural choices that allow for easy marginalization take  a minor toll  in terms of performance. Indeed, while competitive, our model admittedly does not beat  state-of-the-art models in out-of-sample log-likelihood of 
high-dimensional datasets. On the other hand, it does 
beat those same models in a task for which the latter are ill-prepared: learning densities from data containing missing values, a
common setting in some application areas such as genomics~\cite{li2009genotype, marchini2010genotype}. 
While our model is able to deal optimally with missing values by evaluating, for every data point,  the marginal likelihood over its non-missing values, other models must resort to data
imputation. Consequently, we significantly outperform state-of-the-art neural density estimators trained 
using a number of common data imputation strategies. We also show MDMA can be used to test for conditional independence, which is useful for discovering the causal structure in graphs, a task on which it outperforms existing methods, and that it enables estimation of mutual information between arbitrary subsets of variables after fitting a single model. Additionally, we prove that the model class is a universal approximator over the space of absolutely continuous multivariate distributions. 


The structure of this paper is as follows. In \Cref{sec:related} we 
review related works. In \Cref{sec:marginalizable_models} we
present our new model and its theoretical properties.
We present our experimental results in~\Cref{sec:experiments}, and 
conclude in~\Cref{sec:discussion}. 


 

\section{Related work}
\label{sec:related}

Modern approaches to non-parametric density estimation, 
based on normalizing flows~\cite{kingma2018,huang2018neural,oliva2018transformation,grathwohl2018ffjord,de2020block,bigdeli2020learning} (see~\cite{kobyzev2020normalizing,papamakarios2021normalizing} for recent reviews), model expressive yet invertible functions that transforms a simple (usually uniform or Gaussian) density to the target density.
Nonetheless, as previously mentioned, such architectures 
lead to intractable derived quantities such as probabilities, marginals and/or conditional distributions.

Moreover, many normalizing flow models rely on an autoregressive construction, 
which makes the cost of generating samples scale linearly with the dimension.
This can be circumvented using inverse autoregressive flows~\cite{kingma2016improving}, but in this dual case a linear cost 
is incurred instead in density evaluations and hence in training. Another solution is training a feed-forward network using the outputs of a trained autoregressive model~\cite{oord2018parallel}. With MDMA, fast inference and sampling is achieved without requiring this distillation procedure. 




Tensor decompositions~\cite{hackbusch2012tensor,cichocki2016tensor}, 
which are exploited in this work, have been used in various applications 
of signal processing, machine learning, computer vision, and more~\cite{vasilescu2002multilinear,cichocki2009nonnegative,anandkumar2014tensor,papalexakis2016tensors,sidiropoulos2017tensor}.
Recently, such decompositions have been used to speed-up or reduce the number of parameters in existing deep architectures~\cite{lebedev2014speeding,tai2015convolutional,novikov2015tensorizing,kim2015compression,yang2016deep,chen2018sharing}.
In addition to their practical appeal, 
tensor methods have been widely studied to understand the success of deep neural networks \cite{cohen2016expressive,haeffele2015global,janzamin2015generalization,janzamin2015beating,sharir2017expressive}



 

\section{Marginalizable Density Models}
\label{sec:marginalizable_models}

\subsection{Notations}
In the following, we use a capital and lowercase Roman letter (e.g.,  and ) or Greek letters along with dot above (e.g.,  and ) to denote respectively absolutely continuous CDFs of arbitrary dimensions and the corresponding densities. When dealing with multivariate distributions, the marginal or conditional distribution over a subset of variables will be indicated by the argument names (i.e., ).

For a positive integer , 
let .
Denote the space of absolutely continuous univariate and -dimensional CDFs respectively by  and . 
For any , the density  is .
Similarly, for any , the density  is ,
and  for  is the th marginal distribution.


\subsection{The bivariate case}
For the task of modeling joint distributions of two variables supported on ,
consider a family of univariate CDFs
 with , i.e., the functions 
 satisfy

These functions are our basic building block, 
and we model them using a simple neural architecture proposed in~\cite{balle2018variational} and described in~\Cref{sec:theory}. 
If  is an  matrix of nonnegative elements satisfying ,
we can combine it with the univariate CDFs to obtain

The coefficients  encode the dependencies between the two variables, and the normalization ensures that  is a valid CDF, that is .
Even though in each summand the interaction is modeled by a single scalar parameter, such a model can be used to approximate well complex interactions between  and  if  is sufficiently large, as we show in \Cref{sec:theory}. 
The advantage of this construction is that , the family of densities corresponding to the univariate CDFs, leads immediately to 

It is similarly straightforward to obtain marginal and conditional quantities, e.g.:

and the corresponding densities result from replacing  by .
Deriving these simple expressions 
relies on the fact that \eqref{eq:bivariate_cdf} combines the univariate CDFs linearly. Nonetheless, it is clear that, with  and for judiciously chosen univariate CDFs, such a model is a universal approximator of both CDFs and sufficiently smooth densities.




\subsection{The multivariate case}\label{sec:multivariate}







To generalize the bivariate case, consider a collection of univariate CDFs  with  for each  and , and define the tensor-valued function  by  for .
Furthermore, denote the class of normalized order  tensors with  dimensions in each mode and nonnegative elements by

\begin{definition}[Marginalizable Density Model Approximator]
For  as above and ,
the marginalizable density model approximator (MDMA) is

\end{definition}
If is clear that the conditions on  and  imply that .  
As in the bivariate case, densities or marginalization over  are obtained by replacing each  respectively by  or . 
As for conditioning, considering any disjoint subsets  and  of  such that , we have  



For a completely general tensor 
with  parameters, the expression \eqref{eq:F_general_A} is computationally impractical, hence some structure must be imposed on .
For instance, one simple choice is , which leads to , with  and .
Instead of this top-down approach,  can be tractably constructed bottom-up, as we explain next. 


Assuming 
 for integer~, 
define 
for  recursively by
 
for , , and where  is a non-negative  tensor, normalized as . 
The joint CDF can then be written as

with  satisfying . 
It is easy to verify that the underlying  satisfies  defined in \eqref{eq:define_acal}. A graphical representation of this tensor is provided in \Cref{fig:tensor_decomps} in the supplementary materials.  

For example, for , we first combine  and  into

and then merge them as 

from which we can read off that . 

Note that the number of parameters required to represent  is only . The construction is easily 
generalized to dimensions  not divisible by 2. 
Also, the number of  factors combined at 
each iteration in (\ref{eq:HT_recurs}) (called the \emph{pool size}),
 can be any positive integer.
 This construction is a variant of the hierarchical Tucker decomposition of tensors~\cite{Hackbusch2009-yr}, which has been used in the construction of tensor networks for image classification in \cite{cohen2016expressive}.  

Given a set of training points , we fit
MDMA models by maximizing the log of the density with respect to both the parameters in  and the components of . We present additional details regarding the choice of architectures and initialization in \Cref{app:design_details}. 


\subsection{A non-marginalizable MDMA} \label{sec:nmdma}

We can construct a more expressive variant of MDMA at the price of losing the ability to marginalize and condition on arbitrary subsets of variables. We find that the resulting model leads to state-of-the-art performance on a density estimation benchmark.
We define  where  is an upper-triangular matrix with non-negative entries and  on the main diagonal. Note that .
Given some density , we have 

We refer to this model nMDMA, since it no longer enables efficient marginalization and conditioning. 











\subsection{MDMA sampling}

Given an MDMA as in \cref{eq:F_general_A}, we can sample in the same manner as for autoregressive models:
from  independent  variables, we obtain a sample from  by computing 

where, unlike with autoregressive model, the order of taking the conditionals does not need to be fixed.
The main drawback of this method is that due to the sequential nature of the sampling the computational cost is linear in  and cannot be parallelized.

However, the structure of  can be leveraged to sample far more efficiently.
Define by  a vector-valued categorical random variable taking values in , with distribution 

The fact that  with  from \eqref{eq:define_acal} ensure the validity of this definition.
Consider a vector  where  is distributed as above, and 

for the collection  of univariate CDFs.
Denoting the distribution of this vector by , marginalizing over  gives 

Instead of sampling directly from the distribution , we can thus sample from  and discard the sample of .
To do this, we first sample from the categorical variable . Denoting this sample by , we can sample from  by inverting the univariate CDF .
This can be \textit{parallelized} over . 

The approach outlined above is impractical since  can take  possible values, yet if  can be expressed by an efficient tensor representation this exponential dependence can be avoided. 
Consider the HT decomposition \cref{eq:HT}, which can be written as 

that is a normalized sum of  univariate CDFs. 
\begin{proposition}\label{prop:HT_sampling}
Sampling from \eqref{eq:HT_mixture} can be achieved in  time requiring the storage of only  integers.
\end{proposition}
Note that the time complexity of this sampling procedure depends only logarithmically on .
The reason is that a simple hierarchical structure of Algorithm \ref{algo:ht_sampling}, where  denotes the multinomial distribution.
\begin{figure}[ht]
\vspace{-.2in}
  \centering
  \begin{minipage}{.7\linewidth}
  \begin{algorithm}[H]
    \SetAlgoLined
    \KwResult{ for }
    \;
     \For{ \KwTo  \KwBy }{
      \;
     }
     \;
     \caption{Sampling from the HT MDMA}\label{algo:ht_sampling}
    \end{algorithm}
    \end{minipage}
    \vspace{-.2in}
    \label{alg:HT}
\end{figure}


The logarithmic dependence is only in the sampling from the categorical variables, which is inexpensive to begin with.
We thus avoid the linear dependence of the time complexity on  that is common in sampling from autoregressive models. Furthermore, the additional memory required for sampling scales like  (since storing the categorical samples requires representing integers up to size ), and aside from this each sample requires evaluating a single product of univariate CDFs (which is independent of ). In preliminary experiments, we have found that even for densities with , this sampling scheme is faster by  to  orders of magnitude than autoregressive sampling. The relative speedup should only increase with . 





 

\subsection{Universality of the MDMA}
\label{sec:theory}

To model functions in , we use , a class of constrained feedforward neural networks proposed in \cite{balle2018variational} with  hidden layers, each with  neurons, and  a nonaffine, increasing and continuously differentiable elementwise activation function, defined as

where  is the affine map  for an  weight matrix  with \emph{nonnegative} elements and an  bias vector , with  and  for .
The constraints on the weights and the final sigmoid guarantee that , and for any , the corresponding density  can be obtained with the chain rule.
The universal approximation property of the class  is expressed in the following proposition.
\begin{proposition}\label{prop:universal_1}
 is dense in  with respect to the uniform norm.
\end{proposition}
While the proof in the supplementary assumes that  and , it can be easily modified to cover other activations.
For instance, in our experiments, we use  following \cite{balle2018variational}, and refer to the supplementary material for more details regarding this case.
In the multivariate case, consider the class of order  tensored-valued functions with  dimensions per mode defined as

Combining  with the , the normalized tensors introduced in \Cref{sec:multivariate}, the class of neural network-based MDMAs can then be expressed as

\begin{proposition}\label{prop:universal_d}
The set  is dense in  with respect to the uniform norm.
\end{proposition}
The proof relies on the fact that setting  yields a class that is dense in the space of -dimensional CDFs with independent components. All proofs are provided in \Cref{app:proofs}. 



































%
 \section{Experiments}
Additional experimental details for all experiments are provided in \Cref{app:exp_details}.\footnote{Code for reproducing all experiments is available at \url{https://github.com/dargilboa/mdma}.}
\label{sec:experiments}


\subsection{Toy density estimation}
We start by considering  3D augmentations of three popular 2D toy probability distributions 
introduced in~\cite{grathwohl2018ffjord}: two spirals, a ring of  8 Gaussians and a checkerboard pattern. These distributions allow to explore the ability of density models
to capture challenging multimodalities and discontinuities~\cite{de2020block, huang2018neural}.
The results, presented in~\Cref{fig:toy_de}, show that MDMA captures all marginal densities with high accuracy, and samples from the learned model appear indistinguishable from the training data. 
\begin{figure}[h]
  \centering
  \renewcommand{\tabcolsep}{1pt}
  \begin{tikzpicture}
    \node {\begin{tabular}[c]{cccc}
\begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_spirals_data_2d_1_2.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_spirals_data_2d_1_3.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_spirals_data.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_spirals_model_samples_3d.pdf}
      \label{}
    \end{subfigure}
    \-.1in] \begin{subfigure}[c]{\subfigwidth}
    \vspace{.05in}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_data_title.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_2d_marg_1_2.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_data_title.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_2d_marg_1_2.pdf}
      \label{}
    \end{subfigure}
  \end{tabular}};
\draw[black,thick,] (-7,2.3) rectangle (3.43,5.8);
      \node[text width=3cm] at (-1,6) 
    {Training data};
\end{tikzpicture}
\vspace{-.3in}
  \caption{\textbf{Density estimation with closed-form marginals and conditionals.} 
  \textit{First Row:} Panels~1,2: Empirical histograms of training data. Panel 3: Samples from the training data. Panel 4: Samples from the trained MDMA model.
  \textit{Second Row:} Panels 1,2: The marginal density learned by MDMA plotted on a grid. Panels 3,4: Conditional densities learned by MDMA plotted on a grid. 
   \textit{Third Row:} Results on additional datasets: Panels 1,2: Training data and learned marginal density for a 3D checkerboard dataset. Panels 3,4: Similarly for a 3D mixture of Gaussians.
   }
  \label{fig:toy_de}
\end{figure} 

\subsection{Mutual information estimation}
Given a multivariate probability distribution over some variables , estimating the mutual information 

where  are random vectors defined by disjoint subsets of the , requires evaluating  which are marginal densities of . Typically,  and  must be fixed in advance, yet in some cases it is beneficial to be able to flexibly compute mutual information between any two subsets of variables. Estimating both  and  may be highly inefficient, e.g. if  and  are highly overlapping subsets of . 
Using MDMA however, we can fit a single model for the joint distribution
and easily estimate the mutual information between \textit{any} subset of variables by simply marginalizing over the remaining variables to obtain the required marginal densities. 
Thus a Monte Carlo estimate of (\ref{eq:MI}) can be obtained by evaluating the marginal densities at the points that make up the training set. 
\Cref{fig:MI_est} presents an example of this method, showing the accuracy of the estimates.
\begin{SCfigure}
    \centering
    \includegraphics[height=1.8in]{figs/MI_estimation.pdf}
    \caption{\textbf{Mutual information estimation between subsets of a random vector.}
    We fitted a single MDMA model to samples from a zero-mean  Gaussian, with covariance . 
    Monte Carlo estimates of the mutual information~(\ref{eq:MI}) between 
      and  for any  are easily obtained and match closely the exact values.       For each  we average over 5 repetitions of drawing the dataset and fitting. 
}
    \label{fig:MI_est}
    \vspace{-.5in}
\end{SCfigure}


%
 

\subsection{Density estimation with missing values} \label{sec:demv}


Dealing with missing values in multivariate data is a classical challenge in statistics that has been studied for decades~\cite{little2019}.
The standard solution is the application of a data imputation procedure (i.e., ``filling in the blanks''), 
which requires making structural assumptions. 
In some cases, this is natural, as for the matrix completion problem under a low-rank assumption~\cite{candes2009exact, candes2010matrix}, where the imputed values are the main object of interest.
But the artifacts introduced by data imputation~\cite{beretta2016nearest}
are generally a price that one must 
unwillingly pay in order to perform statistical inference in models that require fully-observed data points.
Two popular, generic techniques for imputation are MICE~\cite{buuren2010mice} and -NN imputation~\cite{troyanskaya2001missing}.
The former imputes missing values by iteratively regressing each missing variable against the remaining variables, while the latter uses averages over the non-missing values at -nearest datapoints.

More formally, let  be distributed according to some density  with parameters ,  let  be the non-missing and missing entries of  respectively, and  a vector indicating the missing entries. 
In the missing-at-random setting (i.e.  is independent of ), 
likelihood-based inference using the full likelihood of the model is equivalent to inference using the marginal likelihood~\cite{little2019} .
Standard neural network-based density estimators 
must resort to data imputation because
of the impossibility of computing this marginal likelihood.
MDMA however can directly maximize the marginal likelihood for any pattern of missing data at the same (actually slightly cheaper) computational cost as maximizing the full likelihood, without introducing any bias or variance due to imputation. 


As a demonstration of this capability, we consider the UCI POWER and GAS datasets, following the same pre-processing as \cite{papamakarios2017masked}. We construct a dataset with missing values by setting each entry in the dataset to be missing independently with a fixed probability . We compare MDMA to BNAF~\cite{de2020block}, a neural density model which achieves state-of-the-art results on a number of density estimation benchmarks including GAS. We train MDMA directly on the log marginal likelihood of the missing data, and BNAF by first performing data imputation using MICE \cite{buuren2010mice} and then training using the full log likelihood with the imputed data. The validation loss is the log marginal likelihood for MDMA and the log likelihood of the imputed validation set for BNAF. The test set is left unchanged for both models and does not contain any missing values. We train BNAF using the settings specified in \cite{de2020block} that led to the best performance ( layers and  hidden units where  is the dimensionality of the dataset). The results are shown in \Cref{fig:missing_data}. 
\begin{figure}
\vspace{-.2in}
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/missing_data_power.pdf}
  \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/missing_data_gas.pdf}
  \end{subfigure}
    \caption{\textbf{Density estimation with missing data} Test NLL on two density estimation benchmarks, varying the proportion of entries in each data matrix that are designated missing and not used for fitting. We compare MDMA which can fit marginal densities directly with BNAF which achieves state-of-the-art results on the POWER dataset, after performing data imputation using MICE. As the proportion of missing data increases, MDMA outperforms BNAF.}
    \label{fig:missing_data}
\end{figure}
We find that, as the probability of missingness increases, MDMA significantly outperforms BNAF on both datasets.
Note that, while the proportion of missing values might seem extreme, it is not uncommon in some applications (e.g., proteomics data).
We also trained BNAF using -NN imputation \cite{troyanskaya2001missing}, finding that performance was worse than MICE imputation for all values of .
A comparison of the two methods is provided in \Cref{app:additional_exps}.  

\subsection{Conditional independence testing and causal discovery}

Randomized control trials \cite{Fisher1936} remain the golden standard for causal discovery.
Nonetheless, experiments or interventions are seldom doable, e.g. due to  financial or ethical considerations.
Alternatively, observational data can help uncovering causal relationships \cite{Spirtes2000, Maathuis2015}.
In this context, a class of popular methods targeted at recovering the full causal graph, like PC or FCI \citep{Spirtes2000,strobl2019}, rely on conditional independence (CI) tests.
Letting ,  and  be random variables, the CI of  and  given , denoted , means that given , no information about  (or ) can be gained by knowing the value of  (or ).
And testing  against  is a problem tackled in econometrics \citep{su2007consistent,su2008nonparametric}, statistics \citep{huang2010testing,shah2020hardness}, and machine learning \citep{zhang2011,petersen2021testing}.

Following \citep{petersen2021testing}, denote  and .
It is clear that  implies , although the converse does not hold \citep[see e.g.,][]{spanhel2016partial}.
Nonetheless,  implies , so a test based on the independence between  and  can still have power.
While the test from \citep{petersen2021testing} is based on estimating the conditional CDFs through quantile regression, we proceed similarly, albeit using the MDMA as a plugin for the conditional distributions.
Our approach is especially appealing in the context of causal discovery, where algorithms require computing many CI tests to create the graph's skeleton.
Instead of having to regress for every test, MDMA estimates the full joint distribution, and its lower dimensional conditionals are then used for the CI tests.

In \Cref{tab:CD}, we present results on inferring the structure of causal graphs using the PC algorithm \cite{Spirtes2000,kalisch2007estimating,sun2007kernel,tillman2009nonlinear,harris2013pc}.
As a benchmark, we use the vanilla (i.e., Gaussian) CI test, and compare it to the PC algorithm obtained with the CI test from \citep{petersen2021testing}, albeit using the MDMA for the conditional distributions.
Synthetic random directed acyclic graphs (DAGs) along with sigmoidal or polynomial mechanisms linking parents to children are sampled using~\cite{JMLR:v21:19-187}.
Each dataset is  dimensional and contains 20,000 observations.
We also compare the two algorithms on data from a protein signaling network with  \cite{sachs2005causal} for which the ground truth causality graph is known.
Performance is assessed based on the structural Hamming distance (SHD)~\cite{Tsamardinos2006}, that is the  norm of the difference between learned adjacency matrices and the truth, as well as a variant of this metric for directed graphs SHD(D) which also accounts for the direction of the edges.
\Cref{tab:CD} shows averages over 8 runs for each setting.
In all cases, MDMA outperforms the vanilla PC in terms of both metrics.
For the synthetic data, we note the large standard deviations, due in part to the fact that we sample randomly from the space of DAG structures, which has cardinality super-exponential in .
An example of the inferred graphs is presented in \Cref{app:additional_exps}.

\begin{table}[]
\caption{\textbf{MDMA for causal discovery.} Conditional densities from a trained MDMA model can be used for causal discovery by allowing to test for conditional independence between variables. Both on synthetic DAG data and real data from a protein signaling network, MDMA infers the graph structure more accurately than a competing method based on quantile regression \cite{petersen2021testing}. The metrics are the structural Hamming distance for the directed (SHD(D)) and undirected (SHD) graph. \label{tab:CD} }
\begin{tabular}{lcccccc}
\toprule
Model & \multicolumn{2}{c}{Sigmoidal DAG, d=10} &  \multicolumn{2}{c}{Polynomial DAG, d=10} & \multicolumn{2}{c}{Sachs \cite{sachs2005causal}, d=11} 
\\ \hline
      & SHD(D)                  & SHD              & SHD(D)                    & SHD                & SHD(D)            & SHD        \\
Gaussian    &         &        &          &          & 32             & 27             \\
MDMA        &         &        &          &          &  & 
\\
\bottomrule
\end{tabular}
\end{table}


%
 

\subsection{Density estimation on real data}

We trained MDMA/nMDMA and the non-marginalizable variant described in \Cref{sec:nmdma} on a number of standard density estimation benchmarks from the UCI repository,\footnote{http://archive.ics.uci.edu/ml/datasets.php} following the pre-processing described in \cite{papamakarios2017masked}. 
\Cref{tab:UCI} compares test log likelihoods of 
MDMA/nMDMA with several other neural density models.  
We find the performance of MDMA on the lower-dimensional datasets  comparable to state-of-the-art models, while for higher-dimensional datasets it appears to overfit. nMDMA achieves state-of-the-art performance on the POWER () dataset, but at the cost of losing the ability to marginalize or condition over subsets of the variables. The width of MDMA was chosen based on a grid search over  for each dataset, and the marginal CDF parameters by a search over . All models were trained using ADAM with learning rate , and results for MDMA and nMDMA are averaged over  runs.  
Additional experimental details are provided in \Cref{app:exp_details}. 

\begin{table}[]
\caption{\textbf{General density estimation.} Test log likelihood for density estimation on UCI datasets. The comparison results are reproduced from \cite{bigdeli2020learning}.
}
\begin{tabular}{lcccc}
\toprule
Model  &   {\small POWER} {\footnotesize [d=6]}  &  {\small GAS} {\footnotesize [d=11]}           & {\small HEPMASS} {\footnotesize [d=21]}   & {\footnotesize MINIBOONE } {\footnotesize [d=43]}        \\ 
\hline
Kingma et al. 2018   \cite{kingma2018}      &  &             &          &           \\
Grathwohl et al. 2019     \cite{grathwohl2018ffjord}      &  &            &          &           \\
Huang et al. 2018         \cite{huang2018neural}          &  &           &           &            \\
Oliva et al. 2018         \cite{oliva2018transformation}  &  &  &          &           \\
De Cao et al. 2019 \cite{de2020block}                                         &  &  &          &            \\
Bigdeli et al. 2020 \cite{bigdeli2020learning}                                                  &  &           &  &  \\ \hline
MDMA       &                                          &                &                                       &                           \\
nMDMA                                            &                    &                          &                          &      
\\

\bottomrule
\end{tabular}
\label{tab:UCI}
\vspace{-.1in}
\end{table} 

\section{Discussion}
\label{sec:discussion}
MDMAs offer the ability to obtain, from a single model, closed form probabilities, marginals and conditionals for any subset of the variables. These properties enable one to straightforwardly use the model to solve a diverse array of problems, of which we have demonstrated only a few: mutual information estimation between arbitrary subsets of variables, inference with missing values, and conditional independence testing targeted at multivariate causal discovery. 
In addition to these, MDMA's marginalization property can be used for anomaly detection with missing values~\cite{dietterich2018anomaly}. We have shown that MDMA can fit data with missing values without requiring imputation, yet if one is interested in data imputation for downstream tasks, the ability to sample from arbitrary conditional distributions means that MDMA can be used for imputation as well. 

Additionally, in some application areas (e.g., financial risk management), powerful models exist for the univariate distributions, and marginal distributions are then glued together using copulas~\cite{mcneil2015quantitative}.
However, popular copula estimators suffer from the same drawbacks as modern neural network density estimators with regard to marginalization and conditioning. Using MDMA for copula estimation (say by replacing the kernel density estimator by MDMA in the formulation of \cite{Geenens2017-tg}), one can then obtain copula estimators that do not suffer from these deficiencies. 

The main shortcoming of MDMA is the linearity in the combination of the products of univariate CDFs which appears to limit the expressivity of the model. The study of tensor decompositions is an active area of research, and novel constructions, ideally adapted specifically for this task, could lead to improvements in this regard despite the linear structure.  
\newpage 

\section*{Acknowledgements}

The work of DG is supported by a Swartz fellowship. 
The work of AP is supported by the Simons Foundation, the DARPA NESD program, NSF NeuroNex Award DBI1707398 and The Gatsby Charitable Foundation. \bibliographystyle{unsrt}  
\bibliography{thebib}

\newpage

\appendix
{\Large \bf{Supplementary Material}}


\section{Proofs} \label{app:proofs}

\subsection{Proof of \Cref{prop:HT_sampling}}

The proof follows directly from Algorithm \ref{algo:ht_sampling}. The distribution  is a mixture model, and thus in order to sample from it we can first draw a single mixture component (which is a product of univariate CDFs) and then sample from this single component. The mixture weights are the elements of the tensor  given by the diagonal HT decomposition \cref{eq:HT_mixture}. In the next section, we add details on the sampling process for the sake of clarity.

\subsubsection{Details on the sampling for the HT model}
Define a collection of independent categorical variables  taking values in , where  and for any , . These variables are distributed according to  

where  are the parameters of the HT decomposition. The fact that the parameters are nonnegative and  ensures the validity of this distribution.

With the convention , define the event

Let  be a random vector such that

which implies that the distribution of  obtained after conditioning on a subset of the  in this way is equal to a single mixture component in .
Thus, based on a sample of , one can sample  by inverting the univariate CDFs  numerically and parallelizing over . 
Numerical inversion is trivial since the functions are increasing and continuously differentiable, and this can be done for instance using the bisection method.
It remains to sample a mixture component.

Assume that a sample  for a sequence of variables as in \cref{eq:sample_HT} is obtained e.g. from Algorithm \ref{algo:ht_sampling}.
With the convention , since 

sampling from the categorical variables in this fashion is equivalent to sampling a mixture component.
It follows that by first sampling a single mixture component and then sampling from this component, one obtains a sample from . 

The main loop in Algorithm \ref{algo:ht_sampling} samples such a mixture component, and there are  layers in the decomposition, so the time complexity of the main loop is , and aside from storing the decomposition itself this sampling procedure requires storing only  integers. This logarithmic dependence is only in sampling from the categorical variables which is computationally cheap. This not only avoids the linear time complexity common in sampling from autoregressive models (without using distillation), but the space complexity is also essentially independent of  since only a single mixture component is evaluated per sample. 





\subsection{Proof of \Cref{prop:universal_1}}





Assume that the activation function  is increasing, continuously differentiable, and such that  and .
\Cref{prop:universal_1} then follows immediately from \Cref{prop:universal_1a} and the fact that .
\begin{remark}
    In practice, we use the activation  for some .
    While it does not satisfy the assumptions, the arguments in the proof of \Cref{prop:universal_1b} can be modified in a straightforward manner to cover this activation (see \Cref{rmk:activation_cdf}).
\end{remark}
\begin{proposition}\label{prop:universal_1a}
 is dense in  with respect to the uniform norm.
\end{proposition}
Letting

the proof of \Cref{prop:universal_1a} relies on the following proposition.
\begin{proposition}\label{prop:universal_1b}
 is dense in  with respect to the uniform norm.
\end{proposition}


\subsubsection{Proof of \Cref{prop:universal_1a}}
This proof is similar to that of \cite[][Theorem 2]{dugas2009incorporating}, which deals with functions with positive outputs.
We want to show that, for any , compact , and , there exists  such that


Denote the sigmoid function by  and define the function  by , so that .
By \Cref{prop:universal_1b},
there exists  such that

Thus, letting , we have

Since , the result follows.

\subsubsection{Proof of \Cref{prop:universal_1b}}
This proof is similar to that of \cite[][Theorem 3.1]{Daniels2010}, which is incomplete and only deals with the sigmoid activation.
First, note that  is the space of strictly increasing and continuously differentiable functions.
Therefore, for any  and interval , we can write, for any ,

where the existence of the inverse  is guaranteed by the fact that  is strictly increasing and continuous.
Thus, for  a partition of  with ,  and

we have , namely the approximation error of the Riemann sum for increasing functions.
Let  and  obtained by setting , as well as ,  and  for , then

By the assumptions on , it is clear that  can be made arbitrarily small.
Thus, taking  large enough so that , we have


\begin{remark}\label{rmk:activation_cdf}
   Let  for some  and  obtained by setting , as well as ,  and  for , then

Because  is arbitrary, one can take it large enough so that   as above.
\end{remark}



\subsection{Proof of \Cref{prop:universal_d}}

Consider the classes of order  tensored-valued functions with  dimensions per mode defined as

as well as the class of neural network-based and -based MDMAs, that is

We can now state the following proposition.
\begin{proposition}\label{prop:universal_da}
 is dense in  with respect to the uniform norm
\end{proposition}
\Cref{prop:universal_d} then follows immediately from the fact that  is the space of multivariate mixture distributions admitting a density, which is dense in  with respect to the uniform norm (see e.g.,  \cite[][Theorem 33.2]{DasGupta2008}, \cite[][Theorem 5]{Cheney2009}, or \cite[][Corollary
11]{Nguyen2019}).

\subsubsection{Proof of \Cref{prop:universal_da}}

With , ,  and  and a compact , we want to prove that there exists
 and , such that .
Assuming that we can show , the result would then follow from setting  and the fact that  implies



With , by \Cref{prop:universal_1}, there exists , , and  with , such that

Thus, we have that

 \section{Additional experimental results} \label{app:additional_exps}
\subsection{Toy density estimation}

Figures \ref{fig:checkerboard} and \ref{fig:8gaussians}
show more results on the popular checkerboard and 8 Gaussians toy datasets
studied in \Cref{fig:toy_de}. 


\begin{figure}[h]
  \centering
  \renewcommand{\tabcolsep}{1pt}
  \begin{tikzpicture}
    \node {\begin{tabular}[c]{cccc}
\begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_data.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_data_2d_1_2.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_data_2d_1_3.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_data_2d_2_3.pdf}
      \label{}
    \end{subfigure}
    \-.1in]
    \begin{subfigure}[c]{\subfigwidth}
    \vspace{.05in}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_1d_marg.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_2d_cond_0.5.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_2d_cond_0.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_checkerboard_2d_cond_-0.5.pdf}
      \label{}
    \end{subfigure}
  \end{tabular}};
    \draw[black,thick,] (-7,2.3) rectangle (7,6.3);
      \node[text width=3cm] at (0.6,6) 
    {Training data};
\end{tikzpicture}
  \caption{\textbf{Density estimation with closed-form marginals and conditionals.}
  \textit{Top Row:} Samples from a 3D density, and 2D marginal histograms. \textit{Middle Row:} Samples from MDMA after fitting the density, and plots of the learned 2D marginals.
  \textit{Bottom Row:} \textit{Left:} learned 1D  marginals compared to 1D marginal histograms of the training data. \textit{Right:} Learned conditional densities.}
  \label{fig:checkerboard}
\end{figure} 

\begin{figure}[h]
  \centering
  \renewcommand{\tabcolsep}{1pt}
  \begin{tikzpicture}
    \node {\begin{tabular}[c]{cccc}
\begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_data.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_data_2d_1_2.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_data_2d_1_3.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_data_2d_2_3.pdf}
      \label{}
    \end{subfigure}
    \-.1in]
    \begin{subfigure}[c]{\subfigwidth}
    \vspace{.05in}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_1d_marg.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_2d_cond_0.5.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_2d_cond_0.pdf}
      \label{}
    \end{subfigure}&
    \begin{subfigure}[c]{\subfigwidth}
      \includegraphics[width=\textwidth]{figs/3d_gaussians_2d_cond_-0.5.pdf}
      \label{}
    \end{subfigure}
  \end{tabular}};
    \draw[black,thick,] (-7,2.3) rectangle (7,6.3);
      \node[text width=3cm] at (0.6,6) 
    {Training data};
\end{tikzpicture}
  \caption{\textbf{Density estimation with closed-form marginals and conditionals.}
  \textit{Top Row:} Samples from a 3D  density, and 2D marginal histograms. \textit{Middle Row:} Samples from MDMA after fitting the density, and plots of the learned 2D  marginal.
  \textit{Bottom Row:} \textit{Left:} learned 1D  marginals compared to 1D marginal histograms of the training data. \textit{Right:} Learned conditional densities.}
  \label{fig:8gaussians}
\end{figure} 
\subsection{Density estimation with missing data}

We compare MICE imputation \cite{buuren2010mice} to -NN imputation (with  neighbours) \cite{troyanskaya2001missing} on the UCI POWER dataset in \Cref{fig:mice_vs_knn}, before performing density estimation with BNAF \cite{de2020block}. Due to the size of the dataset, we were not able to use -NN imputation on the full 
dataset, but instead split it up into  batches and performed the imputation per batch. Similar results were obtained on the UCI GAS dataset, and for this reason we only compare MDMA to MICE imputation in the main text.
\begin{figure}
    \centering
    \includegraphics[width=3in]{figs/mice_vs_knn.pdf}
    \caption{A comparison of data imputation methods on the UCI POWER dataset followed by density estimation with BNAF, showing that MICE imputation outperforms -NN. We subsequently use MICE in the comparison with MDMA in the main text.}
    \label{fig:mice_vs_knn}
\end{figure}

\subsection{Causal discovery}

In \Cref{fig:cd_graphs}, we present examples of completely partially directed acyclical graphs (CPDAGs) learned using the PC algorithm, using either MDMA or the vanilla (Gaussian) method for testing conditional independence used in \cite{petersen2021testing}. See \Cref{app:exp_details_CD} for additional details. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/random_DAG.pdf}
    \includegraphics[width=\textwidth]{figs/sachs.pdf}
    \caption{\textbf{Recovered causal graphs}: \textit{Top:} Synthetic data from a random DAG with sigmoidal causality mechanism. The graph inferred using MDMA PC had directional SHD of 11, compared to 15 for the Gaussian PC. \textit{Bottom:} Protein signaling graph \cite{sachs2005causal}. The graph inferred using MDMA PC had directional SHD of 27, compared to 32 for the Gaussian PC.}
    \label{fig:cd_graphs}
\end{figure}

\subsection{Density estimation on real data}

To demonstrate how MDMA allows one to visualize marginal densities
we show in \Cref{fig:power_marginal} learned bivariate marginals from the 
UCI POWER and HEPMASS datasets. The former is composed of power consumption measurements from different parts of a house, with one of the variables () being the time of day. 
\begin{figure}
    \centering
        \begin{subfigure}[c]{0.45\textwidth}
      \includegraphics[width=\textwidth]{figs/power_marginal.pdf}
      \label{}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
    \vspace{0.13in}
      \includegraphics[width=0.91\textwidth]{figs/hepmass_marginal.pdf}
      \label{}
    \end{subfigure}
\caption{\textbf{Log marginal density on UCI datasets.} The scatter plot is composed of  samples from the dataset. \textit{Left:} The POWER dataset. One variable corresponds to the time of day, and the other to power consuption from the kitchen of a house. Note the small value of the density during night-time. The data is normalized during training, yet the labels on the horizontal axis reflect the value of the unnormalized variable for interpretability. \textit{Right:} The HEPMASS dataset. Despite MDMA not achieving state-of-the-art results on test likelihood for this dataset, the model still captures accurately the non-trivial dependencies between the variables. }
    \label{fig:power_marginal}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3in]{figs/phidots.pdf}
    \caption{\textbf{Feature learning in MDMA.} We plot ten univariate PDFs  parameterized as in \cref{app:univariate_marginal_param} for , both at initialization and after training on the UCI POWER dataset. Overlaid is a histogram of computed from 500 datapoints. We find that the features localize near the datapoints.}
    \label{fig:phidots}
\end{figure}

\section{Experimental details} \label{app:exp_details}

All experiments were performed on Amazon Web Services using Tesla V-100 GPUs. The total compute time was 3,623 hours, with the vast majority devoted to the experiments on density estimation with missing values (\Cref{sec:demv}), where some of the runs of BNAF required over 72 hours to complete. 

\subsection{Mutual information estimation}
 samples from the true density are used for fitting MDMA and for estimating the integral over the log marginals in order to compute the mutual information. The MDMA model used had parameters  and was trained with a batch size  and learning rate  for  epochs. 

\subsection{Causal discovery} \label{app:exp_details_CD}

In many application areas, causal relationships between random variables can be represented by a directed acyclical graph (DAG).
The PC algorithm \cite{Spirtes2000} is an efficient algorithm for recovering sparse DAGs from observations.
In general, this recovery is complicated by the fact that two DAGs can induce the same probability distribution, leading to them being called Markov equivalent.
Hence, observational data can only help infer the Markov equivalence class of a given DAG.
The equivalence class, known as a completely partially directed acyclical graph (CPDAG, also called essential graph)~\cite{chickering2002learning}, encodes all the dependence information in the induced distribution.
The object of the PC algorithm is therefore the recovery of a CPDAG that is consistent with the data.
This is generally a hard problem, since the cardinality of the space of DAGs is super-exponential in the number of variables \cite{robinson1977counting}. 

The PC algorithm requires repeatedly testing for independence between pairs of variables conditioned on subsets of the remaining variables.
As mentioned in the main text, testing for conditional independence can be reduced to an independence test between variables that depend on conditional 
CDFs~\cite{petersen2021testing}, which can be obtained easily after fitting the joint density using MDMA.
In our experiments, the results of using MDMA as part of the PC algorithm for testing conditional independence are compared to the results obtained by using a Gaussian conditional independence test based on partial correlations. 

The synthetic DAGs were generated using the the Causal Discovery Toolbox.\footnote{https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html} When the sigmoidal causal mechanism is used, given a variable  and parents , then , and if a polynomial mechanism is used then , where  are random. MDMA was trained with  for  epochs and learning rate  on all datasets. In all experiments we find that the graphs recovered using MDMA are closer to the truth than those recovered using Gaussian PC, as measured by the structural Hamming distance. Example recovered graphs are shown in \Cref{fig:cd_graphs}. 

\subsection{Density estimation on real data}

\begin{table}[]
\caption{Dimension and size of the UCI datasets, and the hyperparameters used for fitting MDMA on these datasets.  is the width of the MDMA model,  and  are respectively the depth and width of the univariate CDF models described in \Cref{app:univariate_marginal_param}. \label{tab:UCI_details} }
\centering 
\begin{tabular}{ccccc}
\toprule
 & POWER   & GAS & HEPMASS & MINIBOONE\\ \hline
d                                               & 6       & 8   & 21      & 43        \\
Training set                                    & 1659917 & 852174                  & 315123                      & 29556                         \\
Validation set                                  & 184435  & 94685                   & 35013                       & 3284                          \\
Test set                                        & 204928  & 105206                  & 174987                      & 3648   \\    \hline
 & 1000 & 4000 & 1000 & 1000 \\ 
 & 2 & 4 & 2 & 2 
\\ 
 & 3 & 5 & 3 & 3
\\
\bottomrule
\end{tabular}
\end{table}

We train MDMA on four UCI datasets, details of the dataset sizes and hyperparameter choices are presented in \Cref{tab:UCI_details}. In all experiments a batch size of  and learning rate of  were used. We use the same pre-processing as \cite{papamakarios2017masked}, which involves normalizing and adding noise. Details are provided in the attached code.\footnote{The raw datasets are available for download at https://zenodo.org/record/1161203\#.YLUMImZKjuU} The POWER dataset consists of measurements of power consumption from different parts of a house as well as the time of day. The GAS dataset contains measurements of chemical sensors used to discriminate between different gases. The HEPMASS and MINIBOONE datasets are both measurements from high-energy physics experiments, aiming respectively for the discovery of novel particles and to distinguish between different types of fundamental particles (electron and muon neutrinos). 


\section{Tensor decompositions} \label{app:tensor_decomps}


In constructing the MDMA estimator, we are confronted with the problem of combining products of univariate CDFs linearly, in a manner that is both computationally efficient and expressive. The linearity constraint reduces this to a tensor decomposition problem (with additional non-negativity and normalization constraints). There is an extensive literature on such efficient tensor decompositions (see \cite{Cichocki2017-gp} for a review).

The analogy with tensor decompositions becomes clear when we consider discrete rather than continuous variables.
Assume  we wish to model the joint distribution of  discrete variables, each taking one of  possible values.
The distribution is then a function , which can also be viewed as an order  tensor.
A general tensor  will require order of  numbers to represent, and is thus impractical even for moderate .
The continuous analog of such a tensor is a multivariate function , with the value of  corresponding to the discrete index .
We will thus use the same notation for the continuous case. 
A graphical representation of tensors and of the diagonal HT tensor used in MDMA is presented in \Cref{fig:tensor_decomps}. 

\begin{figure}
    \centering
     \begin{tikzpicture}
    \node at (-7, 2) (examples){ \includegraphics[scale=0.6] {figs/tensor_examples.pdf}};
    \node[left = 0cm of examples]{a)};
    \node at (-8.5, 0) (phi){ \includegraphics[scale=0.6] {figs/tensor_phi.pdf}};
    \node[left = 0cm of phi]{b)};
    \node at (-5, 0) (general){ \includegraphics[scale=0.6] {figs/tensor_general}};
    \node[left = 0cm of general]{c)};
\node at (-1, 1) (HT){ \includegraphics[scale=0.6] {figs/tensor_HT}};
    \node[left = 0cm of HT]{d)};
    \end{tikzpicture}
    \caption{\textbf{Tensor decompositions}. a) Tensors of various orders (resp. vectors, matrices, delta tensors). Each edge represents an index, and connecting two edges represents contraction (summation over an index). b) The set of univariate CDFs , which can be viewed as an order  tensor. c) A general unstructured tensor of order 6. 
d) The hierarchical Tucker (HT) decomposition \cref{eq:HT}. After suitable normalization, the tensor in d) can be contracted with the tensor  shown in b) to give a multivariate CDF. }
    \label{fig:tensor_decomps}
\end{figure}



\section{Additional design details} \label{app:design_details}


\subsection{Univariate marginal parameterization} 
\label{app:univariate_marginal_param}

We parameterize the univariate marginal CDF  for some scalar  using a simple feed-forward network following \cite{balle2018variational}. Recall from section \Cref{sec:theory} that we model the univariate CDFs as functions

where  is the affine map  for an  weight matrix  with \emph{nonnegative} elements and an  bias vector , with  and  for . This is a slightly more general form than the one in \Cref{sec:theory} since we allow the nonlinearities to depend on the layer. For the nonlinearities, we use 

for some vector  with elements constrained to lie in  (the lower bound on  is necessary to ensure that  are invertible, but the upper bound is not strictly required). This constraint, as well as the non-negativity constraint on the , is enforced by setting  in terms of some . The softplus function is  and is a smooth, invertible approximation of the ReLU. We typically use small values for  in the experiments (see \Cref{app:exp_details}).  


 \subsection{Adaptive variable coupling}

One degree of freedom in constructing a HT decomposition is the choice of partitions of subsets of the variables at every layer over which the products are taken.
This imposes a form of weight-sharing, and it will be natural to share weights between variables that are highly correlated.
As a simple example, let  and consider two different HT decompositions

obtained by coupling  in the first layer respectively with  and .
The univariate marginals for ,  and  can then be written as

Assume that the variables  and  are identical. 
In , both of their univariate marginals depend in an identical way on the tensor parameters.
In  however, additional parameters are required to represent them.
Hence  is a more parsimonious representation of the join distribution.
If  and  are identical instead, then the converse holds and  is the more parsimonious representation.
This property extends to any higher-dimensional (e.g., bivariate) marginals.

In data with spatial or temporal structure (e.g. if the variables are image pixels) there is a natural way to couple variables based on locality. When this is not present, we can adaptively construct the couplings based on the correlations in the data using a simple greedy algorithm. After constructing an empirical covariance matrix from a minibatch of data, we couple the two variables that are most correlated and have not yet been paired. We repeat this until we couple all the groups of variables. Then we "coarse-grain" by averaging over blocks of the covariance matrix arranged according to the generated coupling and repeat the process, this time coupling subsets of variables. We find that this coupling scheme improves performance compared to naive coupling that does not take correlations into account. 

\subsection{Initialization}

As in the univariate case, the non-negativity constraint of the HT tensor parameters  is enforced by defining  for some . 

As is standard, we initialize independently the elements of the univariate PDF weights  as zero-mean gaussians with variance , the  as standard gaussians and the  as 0. The initialization of the HT parameters is  for . This initialization is chosen so that after applying the softplus the matrix  is close to an identity at initialization, which we have found facilitates training compared to using a random initialization. Benefits of such ``orthogonal'' initialization schemes have also been shown for deep convolutional networks \cite{xiao2018dynamical, Blumenfeld2020-lu}. The final layer  are initialized as zero mean gaussians with variance .  

\subsection{From HT to MERA}

The choice of the diagonal HT decomposition \cref{eq:HT} is convenient, yet there is a wealth of other tensor decompositions that can be explored. Here we highlight one such decomposition that generalizes the diagonal HT and could potentially lead to more expressive models. It is based on \cite{Giovannetti2008-pk}. 

Let  be a matrix-valued function such that .
For , define the matrix-value functions  recursively by 

with  when ,  as in \eqref{eq:HT_recurs}, and  a  tensor with nonegative elements satisfying .
The MERA parametrization of a distribution can then be written as

with Since the conditions on  and  imply , this parametrization clearly results in a valid CDF.
Note that  leads to  having only  free parameters.
For , we have
  
\end{document}