

\section{Results and evaluations}


\subsection{Benchmark}

\paragraph{The Fine-grained Segmentation Benchmark (FineSeg).}
With the advances in deep learning based 3D shape segmentation, a benchmark for instance segmentation of fine-grained parts is called for. A nice benchmark for evaluating fine-grained shape segmentation is recently proposed in a concurrent work in~\cite{mo2018partnet}. In this work, we propose FineSeg. The dataset contains about  3D shapes over six shape categories: chair (), table (), airplanes (), sofa (), helicopter () and bike (). The models are collected from a subset of ShapeNet~\cite{Shapenet} used in the work of Sung et al.~\cite{sung2017}. These models are consistently aligned and uniformly scaled. For those model whose segmentation is not fine-grained enough (e.g., no instance part segmentation), we manually segment the models. We then build a part hierarchy for each shape, using the method proposed in~\cite{wang2011symmetry}.
We point sample each 3D model, thus generating a ground-truth fine-grained segmentation of the corresponding 3D point cloud. The hierarchies can be used for training our recursive part decomposition network. This benchmark can be used to quantitatively evaluate fine-grained segmentation of 3D point clouds, based on \emph{Average Precision (AP)} for part detection (with the IoU against ground-truth greater than a threshold).
The benchmark is publicly available at: \url{www.kevinkaixu.net/projects/partnet.html}.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images//fine_grained}
  \caption{Fine-grained point cloud segmentation by PartNet. For comparison, we show for each shape the fine-grained segmentation result (bottom) and the corresponding ground-truth (top).}
  \label{fine_grained}
\end{figure*}






\subsection{Segmentation results and evaluation}
Our PartNet model is trained with  models of FineSeg, leaving the rest  for testing.
\kx{The discussion of complexity and timing (for both training and testing) can be found in the supplemental material.}

\paragraph{Visual results on FineSeg.}
We first show in Figure~\ref{fine_grained} some visual examples of fine-grained point cloud segmentation obtained by PartNet. For side-by-side comparison, we also show the ground-truth segmentation for each example. Our method produces precise fine-grained segmentation on the noisy point clouds with complicated part structures. Furthermore, once trained, the same model can be used to segment the test (unseen) point clouds into varying number of parts, demonstrating its flexibility and generality.
Figure~\ref{fig:parts} demonstrates how the same model of PartNet can segment different shapes in a category into for an arbitrary number of targeting parts, depending on structure complexity.
\kx{More results can be found in the supplemental material.}
\kx{In the supplemental material, we also show a visual comparison of hierarchical segmentation
with two traditional (non-learned) baseline methods.}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{images//parts}
  \caption{The same PartNet model trained on the Chair set, can be used to segment different chair models into different number of parts, depending on the structure complexity of the input shapes.}
  \label{fig:parts}\vspace{-10pt}
\end{figure}


\paragraph{Quantitative evaluation with ablation study.}\label{ablation study}
In quantitative evaluation on FineSeg, we compare to two baselines which are ablated versions of our method. Specially, we are interested in the effect of the two important node features used in PartNet: recursive context feature (RCF) and part shape feature (PSF).





\begin{table}[!t]\centering\small
\scalebox{0.93}{
\setlength{\tabcolsep}{1.2mm}{
 \begin{tabular}{l|l|c|c|c|c|c|c|c}
\hline
   \multicolumn{2}{c|}{}& mean &aero & bike & chair & heli. & sofa & table \\
\hline
  \multirow{3}*{\makecell[c]{IoU \\ }} & Full & &  & &  & &  &\\
  &w/o RCF &  &  &  &  &  & & \\
  &w/o PSF & &  &  &  & &  & \\
\hline
  \multirow{3}*{\makecell[c]{IoU \\ }} & Full &  & &  &  &  & &\\
  &w/o RCF & &  & &  & & &\\
  &w/o PSF & & & &  &  &  & \\
\hline
\end{tabular}
}}\vspace{5pt}
\caption{Comparing our full model with two baselines (w/o RCF and w/o PSF) on FineSeg. AP() is measured with IoU threshold being  and , respectively.}
 \label{ablation_table}\vspace{-6pt}
\end{table}







In the first baseline (w/o RCF), recursive context feature is removed from both node classification (see Figure~\ref{fig:node_class}) and node segmentation (see Figure~\ref{fig:node_seg}). To compensate the missing of recursive context feature, the D part shape feature is duplicated into a D feature. The ablated network is re-trained using the training set of FineSeg.
In the second baseline (w/o PSF), PSF is removed only from the node classification module.



Table~\ref{ablation_table} reports AP on all six categories of the testing set, with the IoU thresholds being  and , respectively. The consistent superiority of our full model demonstrates the importance of the two features.
Figure~\ref{abalation_figure} plots the training loss over iterations for the three methods, on three shape categories (Bike, Sofa and Airplane). The results show that both features are critical for fast training of the node classification module and the node segmentation module. This evidences the importance of both global context information and local shape geometry on learning hierarchical segmentation.
\kx{More results are in the supplemental material.}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images//abaltion}
  \caption{Training loss over iterations in the ablation study of the two key node features (RCF and PSF), on three shape categories (Bike, Sofa and Airplane). For each category (row), we plot both node segmentation loss (left) and node classification loss (right).}
  \label{abalation_figure}
\end{figure}

\if 0
We can also see that if we remove part shape feature from PointNet1 for node classification, our training loss increase a lot for node classification. On the other hand, the fact that training loss for node point cloud segmentation also increase a few demonstrates that part shape feature on higher level can also improve recursive context feature learning. Thus, part shape feature extracted from the corresponding point cloud at each node can improve node classification and segmentation performance.
\fi





\paragraph{ShapeNet challenge for fine-grained segmentation.}
In addition, we conduct a moderate-scale stress test using ShapeNet~\cite{Shapenet} challenge for fine-grained segmentation. We randomly select a collection of shapes from ShapNet and use PartNet to segment them.
Since we don't have ground-truth fine-grained segmentation for ShapeNet models, we resort to a subjective study to evaluate our segmentation results. We ask the participants to rate the quality of fine-grained segmentation in the range from  to . \kx{The user study shows that our method attains  average ratings for all the categories tested, much higher than the results of the ``w/o RCF'' baseline.} \kx{The details and results of this study are provided in the supplemental material.} In Figure~\ref{shapenet}, we show a few visual examples, from which one can see that our method produces fine-grained segmentation these unseen shapes with complicated structures. Moreover, our method obtains the adjacency and symmetry relations of the decomposed parts, which can be used for many downstream structure-aware applications~\cite{mitra2013structure}.




\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images//shapenet}
  \caption{A few results from the ShapeNet fine-grained segmentation challenge. Besides segmentation, PartNet can also recover the relations (adjacency and symmetry) between the segmented parts. We visualize the recovered symmetry relations with colored arrows (Reflective: Red; Translational: Blue; Rotational: Green).}
  \label{shapenet}\vspace{-8pt}
\end{figure}


\subsection{Comparison of semantic segmentation}
\label{subsec:semantic}
Although PartNet is designed for fine-grained segmentation, the recursive decomposition should work even better for semantic segmentation since the latter is usually a much coarser-level segmentation. We evaluate PartNet for semantic segmentation of 3D point clouds on the ShapeNet part dataset~\cite{Yi16}, through comparing with seven state-of-the-art methods on this task. Similar to PointNet~\cite{qi2016pointnet}, we re-sample the point cloud for each shape into  points. We use the same training/testing split setting as those state-of-the-arts, and compute part-wise average IoU as metric.

Note that PartNet does not produce semantic labels for points, so it cannot perform labeled segmentation. To enable the comparison, we add an extra module to PartNet to predict a semantic label for each part it decomposes. The part label prediction module takes the node feature of the leaf nodes as input and outputs a semantic label for all points included in that leaf node. This module is implemented with three fully-connected layers and is trained with cross-entropy loss.

\begin{table*}[!t]\centering\small
 \scalebox{0.98}{
\setlength{\tabcolsep}{1.2mm}{
 \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
  Method & mean &aero & bag & cap & car & chair & eph. & guitar & knife & lamp & laptop & motor & mug & pistol & rocket & skate. & table\\
\hline
  PointNet~\cite{qi2016pointnet}&&&&&&&&&&&&&&&&&\\
  PointNet++~\cite{qi2017pnpp}&&&&&&&&&&&&&&&&&\\
  O-CNN~\cite{Wang2017ocnn}&&&\textbf{87.1}&&&&\textbf{85.1}&&&&&&&&&&\\
  SSCN~\cite{graham20183d}&&&&&\textbf{80.8}&&&&\textbf{89.1}&&&&&&&&\\
  PCNN~\cite{PCNN18}&&&&&&&&&&&&&&&&&\\
  SPLATNet~\cite{su18splatnet}&&&&&&&&&&&&&&&&&\\
  PointCNN~\cite{NIPS2018_7362}&&&&&\textbf{80.8}&&&\textbf{92.3}&&\textbf{85.3}&&\textbf{77.2}&&\textbf{84.2}&&\textbf{80.0}&\\
\hline
 Ours&\textbf{87.4}&\textbf{87.8}&&\textbf{89.7}&&\textbf{91.9}&&&&&\textbf{97.0}&&\textbf{97.3}&&\textbf{64.6}&&\textbf{85.8}\\
\hline
\end{tabular}}}\vspace{5pt}
\caption{Comparison of semantic segmentation on the ShapeNet part dataset~\cite{Yi16}. Metric is part-wise IoU ().}
\label{shapenet_table}\vspace{-8pt}
\end{table*}

The results are reported in Table~\ref{shapenet_table}. PartNet, augmented with a part label prediction module, achieves better performance in most of the categories, and the highest mean accuracy over all categories. Furthermore, our method works especially well for those categories with complex structures such as chair, table, and aeroplane, etc. We believe that the divide-and-conquer nature of recursive decomposition does help reduce the difficulty of segmentation learning. Another key benefit of recursive decomposition is that the segmentation of higher levels provides contextual cues constraining that of the lower levels.
\kx{Similar results can also be observed in testing our trained model on
the Princeton Segmentation Benchmark~\cite{Chen:2009:ABF} (see supplemental material).}

\kx{
For semantic segmentation, PartNet can be trained with a consistent hierarchy for all shapes in a category.
The training is can be done with \emph{any} hierarchy that is consistent across all training shapes.
Therefore, we do \emph{not} need an extra process (such as the one~\cite{wang2011symmetry} used in fine-grained segmentation) for hierarchy construction. Taking \emph{any random hierarchy} of one training shape as a ``template'', we unify the hierarchies of all the other shapes based on the semantic part labels.
Therefore, PartNet does \emph{not} require an extra supervision of part hierarchy for training for semantic segmentation. Consequently, the comparison reported in Table~\ref{shapenet_table} of the main paper is a fair one.
}

\subsection{Comparison of instance segmentation}
SGPN~\cite{Wang2017SGPN} is the first deep learning model that learns instance segmentation on 3D point clouds. It can segment object instances and predict a class label for each instance, which is very similar to our method (augmented the label prediction module), except that SGPN cannot obtain part relations as our method does. We make a comparison to SGPN on our FineSeg dataset, using again AP with IoU thresholds of  and .

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images//SGPN}
  \caption{Visual comparison of fine-grained part instance segmentation with SGPN~\cite{Wang2017SGPN}. Left: Ground-truths. Middle: Segmentation results by PartNet. Right: Results by SGPN. Incorrect segmentations (w.r.t. ground-truth) are marked with red circles.}
  \label{sgpn_figure}\vspace{-5pt}
\end{figure}

\begin{table}[!t]\centering\small
 \scalebox{0.98}{
\setlength{\tabcolsep}{1.0mm}{
 \begin{tabular}{c|l|c|c|c|c|c|c|c}
\hline
   \multicolumn{2}{c|}{}& mean &aero & bike & chair & heli. & sofa & table \\
\hline
  \multirow{2}*{\makecell[c]{IoU \\ }} & SGPN~\cite{li2017grass} &  &  &  &  &  &  & \\
  & Ours &  &  &  &  &  &  & \\
\hline
  \multirow{2}*{\makecell[c]{IoU \\ }} & SGPN~\cite{li2017grass} &  &  &  &  &  &  & \\
  & Ours &  &  &  &  &  &  & \\
\hline
\end{tabular}}}\vspace{5pt}
\caption{Comparison with SGPN~\cite{Wang2017SGPN} on fine-grained instance segmentation over the FineSeg dataset. The metric is AP () with IoU threshold being  and , respectively.}
\label{sgpn_table}\vspace{-8pt}
\end{table}

Figure~\ref{sgpn_figure} shows a few visual comparisons, where incorrectly segmented regions are marked out. Table~\ref{sgpn_table} shows the quantitative comparison on our datasets. We attribute the consistent improvement over SGPN to two factors. First, the instance group learning of SGPN is based on point clustering. Such a one-shot point grouping over the entire shape is hard to learn. Our method, on the other hand, performs top-down recursive decomposition which breaks the full shape segmentation into a cascade of partial shape segmentations. Second, the point features used by SGPN are solely point convolutional features~\cite{qi2016pointnet} while our features accounts for both local part shape and global context.
