\section{Experiments}
\subsection{Experimental Setup}


\paragraph{Tasks.} We conduct experiments on four tasks, which could be divided into two categories: 1) Cross-modal generation tasks: dense video captioning (DVC) and video paragraph captioning (PC); 2) Cross-modal understanding tasks: single-/multi-sentence video grounding (SSVG/MSVG). For DVC and PC, we utilize the reconstructed sentences from the text generator and their time boundaries from the localizer as the prediction results. For SSVG/MSVG, we use the similarity matrix $\boldsymbol{\omega}$ to select the most similar event features for each sentence query, which is further decoded as a temporal boundary. Note that for SSVG, we use $\mathbf{Q}_{\rm sent}$ to calculate $\boldsymbol{\omega}$ since cross-sentence contexts are inaccessible by each sentence query.

\vspace{-1.0em}
\paragraph{Datasets.} The proposed framework is evaluated on four public datasets: ActivityNet Captions~\cite{krishna2017dense}, TACoS~\cite{regneri2013grounding}, YouMakeup~\cite{wang2019youmakeup} and YouCook2~\cite{zhou2018towards}. We follow the standard split utilized in previous work. More details about the datasets can be found in Sec.~\ref{supp_dataset}.



\vspace{-1.0em}
\paragraph{Implementation Details.}
We adopt the C3D~\cite{tran2015learning} pre-trained on Sports1M~\cite{karpathy2014large} to extract frame features for ActivityNet Captions and TACoS. For a fair comparison with state-of-the-art methods on ActivityNet Captions, we also evaluate our model based on TSP~\cite{wang2018temporal} features. For YouCook2, we adopt the TSN features provided by~\cite{zhou2018end}. For YouMakeup, we use the I3D features provided by~\cite{wang2019youmakeup}.

The transformer for event encoding is implemented as a Deformable Transformer~\cite{zhu2020deformable} with 2 encoder-decoder layers. We follow PDVC~\cite{wang2021end} to use LSTM-DSA as the text generator. The number of event queries $N$ is set to 30 for ActivityNet Captions and 100 for other datasets. For model training, we use AdamW optimizer with batch size 1 and learning rate of $5e^{-5}$ for ActivityNet Captions/YouCook2, and batch size 4 and learning rate of $1e^{-4}$ for YouMakeup/TACoS. The RoBERTa encoder is frozen for ActivityNet Captions/YouCook2 and trained with a learning rate of $1e^{-5}$ for TACoS/YouMakeup, respectively. The weight decay is set to $1e^{-4}$ for all datasets. The trade-off factor $\alpha$ is set to 0.05. The temperature $\tau$ is set to 0.1. The ratio of semantic cost $\lambda$ is set to 1.0. More details can be found in Sec.~\ref{supp_details}.  

\subsection{Comparison with State-of-the-art Methods}

\paragraph{\textbf{Dense Video Captioning.}} In Table ~\ref{table:SotaANET}, our method could surpass the state-of-the-art methods on most metrics on ActivityNet Captions, both with cross-entropy training and reinforcement training. Adopting TSP features could further boost the performance and establish the new state-of-the-art under these two training scenarios. Compared with PDVC, our framework exploits the bidirectional information flow between visual features and text, which increases the expressive ability of the event encoding network in two aspects: 1) TEG guides the event features towards a better coverage of salient video segments, advancing the generated captions to reveal a full video story; 2) label assignment strategy is enhanced by semantic cost, which could achieve a better assignment to link results than PDVC, especially for videos with noisy boundary annotations. In Table~\ref{table:SotaYC} and Table~\ref{tab:ym}, performance comparisons on YouCook2 and YouMakeup further verify the effectiveness of our method, which surpasses previous methods with a clear margin. We achieved 1st place in the MDVC@PIC Challenge 2022\footnote{Leaderboard at \url{https://codalab.lisn.upsaclay.fr/competitions/5102}}. 
\vspace{-1.0em}
\paragraph{\textbf{Video Paragraph Captioning.}} Generating a coherent paragraph requires a global perception of the foreground events and getting rid of the noisy segments. Table~\ref{table:SotaParaCap} shows the PC performance comparison on ActivityNet Captions.  Our method with  TSP features (RGB modality) could achieve similar performance with TDPC, the state-of-the-art paragraph captioner based on Transformer. We note that TDPC utilizes the combination of three feature extractors (ResNet-200~\cite{he2016deep} + I3D RGB + I3D Flow~\cite{carreira2017quo}), which can not be fairly compared with our method. Compared with the method relying on localization annotations during inference, our method could largely reduce the cost of manpower, meanwhile achieving comparable or better performance.

\begin{table}[t]
\caption{Dense captioning performance on ActivityNet Captions. 
}

\small
\vspace{-0.6em}
\renewcommand\arraystretch{0.9}
  \centering
     \makeatletter\def\@captype{table}\makeatother
     \setlength{\tabcolsep}{0.75 mm}{
\begin{tabular}{l|c|cccc}
    \toprule
    Method & Features & BLEU@4 & METEOR  & CIDEr & SODA(c) \\
    \midrule
    \multicolumn{6}{l}{\textbf{with cross-entropy training}} \\
    DCE~\cite{krishna2017dense} & C3D & 0.17 & 5.69 & 12.43& - \\
    TDA-CG~\cite{wang2018bidirectional} & C3D & 1.31 & 5.86 & 7.99 & - \\
    DVC~\cite{li2018jointly} & C3D & 0.73 & 6.93 & 12.61 & - \\
    Efficient~\cite{Suin2020efficient} & C3D & 1.35 & 6.21 & 13.82 & -  \\
    SDVC~\cite{Mun2019stream} &C3D & - & 6.92 & - & -\\
    ECHR~\cite{wang2020event} & C3D & 1.29 & 7.19 & 14.71 & 3.22 \\
    {PDVC}~\cite{wang2021end} & C3D & 1.65 & \textbf{7.50} & {25.87} & {5.26} \\
    UEDVC~\cite{zhang2022unifying} & C3D & 1.45 & 7.33 & \textbf{26.92} & 5.29 \\
    \rowcolor{Gray}
    {Ours} & C3D & \textbf{1.69} & {7.46} & {26.23} & \textbf{5.44} \\
    \midrule
    MT~\cite{zhou2018end} & TSN & 1.15 & 4.98 & 9.25 & - \\
    BMT~\cite{Iashin2020better} & I3D & 1.88 & 7.43 & 11.94& - \\
    {PDVC}~\cite{wang2021end}& TSP &{2.17} & {8.37} & {31.14} & {6.05} \\
    \rowcolor{Gray}
    {Ours} & TSP & \textbf{2.18} & \textbf{8.50} & \textbf{32.76} & \textbf{6.22} \\
    \midrule
    \multicolumn{6}{l}{\textbf{with reinforcement training}} \\
    SDVC~\cite{Mun2019stream} & C3D & 0.93 & 8.82 & \textbf{30.68}& -\\
    SGR~\cite{deng2021sketch} & C3D &\textbf{1.67} & {9.07} & 22.12 & - \\
    \rowcolor{Gray}
    {Ours} & C3D & 0.81 & {\textbf{9.13}} & 26.00 & \textbf{5.91} \\
    \midrule
    MFT~\cite{xiong2018move} & TSN & \textbf{1.24}& 7.08 & 21.00 & - \\
    SGR~\cite{deng2021sketch} & TSN & - & 9.37 & - & 5.29 \\
    \rowcolor{Gray}
    {Ours} & TSP & 1.11 & \textbf{10.03} & \textbf{33.33} & \textbf{7.11} \\
    \bottomrule
\end{tabular}}
\label{table:SotaANET}
\vspace{-1.0 em}
\end{table}

\begin{table}[t]
\caption{Dense captioning on YouCook2 with TSN features.}
\vspace{-0.6em}
\small
\renewcommand\arraystretch{0.9}
       \centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{2.2 mm}
        {
        \begin{tabular}{lcccc}
            \toprule
            \multirow{1}{*}{Method}  & BLEU@4 & \multirow{1}{*}{METEOR} & CIDEr & \multirow{1}{*}{SODA(c)} \\
            \midrule
            MT~\cite{zhou2018end} & 0.30 & 3.18  & 6.10 & - \\
            ECHR~\cite{wang2020event} & - & 3.82 & - & -\\
            SGR~\cite{deng2021sketch} & - & 4.35 & - & - \\
            {PDVC~\cite{wang2021end}} & 0.80 & {4.74}  & 22.71 & {4.42} \\
            \rowcolor{Gray}          
            {{Ours}} & \textbf{1.04} & \textbf{5.01} & \textbf{26.52} & \textbf{4.91} \\            
            \bottomrule
        \end{tabular}}
        \label{table:SotaYC}
        \vspace{-1.0em}
\end{table}

\begin{table}[]
\caption{Paragraph captioning on ActivityNet Captions. Anno. means needing boundary annotations during inference. $*$ means extra input modalities (\eg, flow, object detection) except RGB. 
}

\renewcommand\arraystretch{0.9}
\small
\vspace{-0.6em}
\centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{1.35 mm}{
\begin{tabular}{lcccc}
    \toprule
    \multirow{1}{*}{Method}  & \multirow{1}{*}{Anno.} & \multirow{1}{*}{BLEU@4} & \multirow{1}{*}{METEOR}  & \multirow{1}{*}{CIDEr} \\
    \midrule
    MFT$^{*}$~\cite{xiong2018move} & $\times$ & {10.29} & 14.73  & 19.12 \\
    {PDVC}~\cite{wang2021end} & $\times$ & 10.46 & \textbf{16.42} & 20.91 \\
    {TDPC$^{*}$ (w/o RL)}~\cite{song2021towards} & $\times$ & \textbf{11.74} & 15.64 & {\textbf{26.55}} \\
    \rowcolor{Gray}
    {Ours} & $\times$ & \underline{11.70} & \underline{16.35} & \underline{26.03} \\
    \midrule
    {\color{gray}HSE~\cite{zhang2018cross}} & {\color{gray}\checkmark} &{\color{gray}9.84} & {\color{gray}13.78} & {\color{gray}18.78} \\
    {\color{gray}MART$^{*}$~\cite{lei2020mart}}  & {\color{gray}\checkmark} & {\color{gray}10.33} & {\color{gray}15.68} & {\color{gray}23.42} \\
    {\color{gray}GVD$^{*}$~\cite{zhou2019grounded}} & {\color{gray}\checkmark} & {\color{gray}11.04} & {\color{gray}15.71} & {\color{gray}21.95} \\
    {\color{gray}AdvInf$^{*}$~\cite{park2019adversarial}} & {\color{gray}\checkmark} & {\color{gray}10.04} & {\color{gray}16.60} & {\color{gray}20.97} \\
    {\color{gray}COOT$^{*}$~\cite{ging2020coot}} & {\color{gray}\checkmark} & {\color{gray}10.85} & {\color{gray}15.99} & {\color{gray}{28.19}} \\
    \bottomrule
\end{tabular}}
\label{table:SotaParaCap}
\vspace{-1.0em}
\end{table}

\begin{table}[]
\caption{Dense captioning and multi-sentence video grounding performance on YouMakeup. For the validation set, all approaches utilize I3D features, while for the test set, $\ddagger$ means using 2D and 3D features, and ${\star}$ means using model ensembling. }
\small
\vspace{-0.6em}
\renewcommand\arraystretch{0.9}
\centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{1.0 mm}{
\begin{tabular}{l|cc|ccc}
\toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c|}{{Dense Captioning}} &\multicolumn{3}{c}{{Grounding}} \\
 & METEOR & SODA(c) & IoU0.5   & IoU0.7   & mIoU \\ 
 \midrule
  \multicolumn{6}{l}{\textbf{Validation Set}} \\
PDVC~\cite{wang2021end} & 21.86 & 17.12 & - & - & -  \\
MMN~\cite{wang2021negative} & - & - & 35.18 & 20.08 & -  \\
\rowcolor{Gray}
{Ours} & \textbf{22.28} & \textbf{18.93} & \textbf{55.70} & \textbf{38.24} & \textbf{51.02}  \\
\midrule
    \multicolumn{6}{l}{\textbf{Leaderboard on Evaluation Server (Test set)}} \\
Lu et.al$^{\ddagger}$~\cite{mdvc2nd} & 23.17 & - & - & - & - \\    
Shu et.al$^{\ddagger \star}$~\cite{mtvg3rd} & - & - & 56.01 & 38.44 & - \\
Zhang et.al$^{\ddagger \star}$~\cite{mtvg2nd} & - & - & \textbf{62.50} & 42.12 & - \\
\rowcolor{Gray}
{Ours$^{\ddagger \star}$} & \textbf{37.58} & - & 61.56 & \textbf{46.18} & - \\    
\bottomrule
\end{tabular}
}
\label{tab:ym}
\vspace{-0.2em}
\end{table}

\vspace{-1.0em}
\paragraph{\textbf{Single-Sentence Video Grounding.}}


Table~\ref{tab:ssvg} shows the SSVG performance comparison on ActivityNet Captions and TACoS. The proposed method achieves state-of-the-art performance among metric learning methods and competitive performance with non-metric learning
methods. This verifies that the learned event representations are semantically rich enough to make them distinguishable from each other and are also temporally sensitive to differentiate themselves from many background segments. Compared with non-metric learning methods, our method could reuse the visual feature when querying the same video with different sentences, which is more efficient during training and inference. MMN is the state-of-the-art metric learning grounding method that adopts a contrastive learning loss similar to ours. However, its label assignment is merely based on localization cost, which lacks semantic similarity and thus may suffer from incorrect boundary annotations. We surpass MMN on TACoS  with a clear margin.



\begin{table}[]
\caption{Single-sentence video grounding on ActivityNet Captions and TACoS.  All methods are based on C3D features without special indication.}
\vspace{-0.5em}
\renewcommand\arraystretch{1.0}
\centering
\small
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{0.8 mm}{
\begin{tabular}{l|ccc|ccc}
\toprule
    \multirow{2}{*}{Method} &\multicolumn{3}{c|}{{ActivityNet Captions}} & \multicolumn{3}{c}{{TACoS}} \\ & IoU0.5   & IoU0.7   & mIoU & IoU0.3   & IoU0.5 & mIoU  \\
\midrule
\multicolumn{7}{l}{\textbf{Non-metric learning based}} \\


CTRL \cite{gao2017tall} & 29.01 & 10.34 & 20.54  & 18.32 & 13.30 & 11.98\\
TGN \cite{chen2018temporally} & 27.93 & 11.86 & 29.17 & 21.77 & 18.90 & 17.93\\
QSPN \cite{xu2019multilevel} & 33.26 & 13.43 & - & 20.15 & 15.23 & -  \\
ABLR \cite{yuan2019find} & 36.79 & - & 36.99 & 19.50 & 9.40  & 13.40 \\
DRN \cite{zeng2020dense} & 45.45 & 24.36 & -  & - & 23.17 & -   \\
2D-TAN \cite{zhang2020learning}& 44.51 & 26.54 & - & 37.29 & 25.32 & - \\
VSLNet \cite{zhang2020span} & 43.22 & 26.16 & \textbf{43.19} & 29.61 & 24.27 & 24.11 \\
CPNet \cite{li2021proposal} & 40.56 & 21.63 & 40.65 & 42.61 & 28.29 & \textbf{28.69} \\
BPNet\cite{xiao2021boundary} & 42.07 & 24.69 & 42.11 & 25.96 & 20.96 & 19.53 \\
CBLN\cite{liu2021context} & 48.12 & 27.60 & - & 38.98 & 27.65 & -   \\
MATN\cite{zhang2021multi} & 48.02 & \underline{31.78} & - & \textbf{48.79} & \textbf{37.57} & - \\
MGSL-Net\cite{liu2022memory} & \underline{51.87} & 31.42 & - & 42.54 & 32.27 & - \\
D-TSG~\cite{liu2022reducing} & \textbf{54.29} & \textbf{33.64} & - & \underline{46.32} & \underline{35.91} & - \\
\midrule
\multicolumn{7}{l}{\textbf{Metric learning based}} \\
MCN \cite{anne2017localizing}   & 21.36 & 6.43  & 15.83  & -     & 5.58  & -  \\
MMN\cite{wang2021negative} & 48.59 & \underline{29.26} & - & \underline{39.24} & \underline{26.17} & - \\
\rowcolor{Gray}
{Ours (C3D) } & \underline{48.93} & 27.16 & \underline{46.38} & \textbf{45.92} & \
\textbf{34.57} & \textbf{32.48}\\
\rowcolor{Gray}
{\color{black}
{Ours (TSP)}} & \textbf{49.18} & \textbf{29.69} & \textbf{46.83} & - & - & - \\

\bottomrule
\end{tabular}}
\label{tab:ssvg}
\vspace{-1.0em}
\end{table}


\vspace{-1.0em}
\paragraph{\textbf{Multi-Sentence Video Grounding.}}
MSVG performs a set-level prediction, which takes all text queries as input and localizes their target events simultaneously. 
As shown in Table~\ref{tab:msvg} and Table~\ref{tab:ym}, 
we establish a new state-of-the-art on TACoS and Youmakeup, meanwhile achieve competitive performance on ActivityNet Captions. We also achieved 1st place in the MTVG@PIC Challenge 2022\footnote{Leaderboard at \url{https://codalab.lisn.upsaclay.fr/competitions/5244}}. We conjecture the advantage of our method comes from 
the one-to-one matching strategy with a semantic-aware cost at the set level. Our method guarantees the coherence of the matching results, which could suppress the hard negative segments that have a high IoU but low semantic similarity.

\begin{table}[]
\caption{Multi-sentence video grounding performance on ActivityNet Captions and TACoS. All methods are based on C3D features without special indication.}
\vspace{-0.5em}
\renewcommand\arraystretch{0.9}
\centering
\small
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{1.0 mm}{
\begin{tabular}{l|ccc|ccc}
\toprule
    \multirow{2}{*}{Method} &\multicolumn{3}{c|}{{ActivityNet Captions}} & \multicolumn{3}{c}{{TACoS}} \\
 & IoU0.5   & IoU0.7   & mIoU & IoU0.3   & IoU0.5   & mIoU  \\ \midrule

BS \cite{bao2021dense} & 46.43 & 27.12 & - & 38.14 & 25.72 & -   \\
3D-TPN \cite{zhang2020learning} & 51.49 & 30.92 & - & 40.31 & 26.54 & -   \\
DepNet \cite{bao2021dense} & 55.91 & 33.46 & - & 41.34 & 27.16 & -   \\
SVPTR \cite{jiang2022semi} & \textbf{61.70} & \underline{38.36} & \textbf{55.91} & \underline{47.89} & \underline{28.22} & \underline{31.42} \\
\rowcolor{Gray}
{Ours (C3D)} & 59.08 & 37.47 & 54.56 & \textbf{48.29} & \textbf{36.07} & \textbf{34.29} \\
\rowcolor{Gray}

{\color{black}
{Ours (TSP)}} & \underline{60.67} & \textbf{38.55} & \underline{55.40} & - & - & - \\

\bottomrule
\end{tabular}}
\label{tab:msvg}
\vspace{-0.2em}
\end{table}
 

\subsection{Ablation Studies} \label{sec-abl}
We study the key design choices of the proposed framework on ActivityNet Captions, including the bidirectional pretext tasks, matching cost in label assignment, and the choices of text encoding. 
\vspace{-1.0 em}
 \paragraph{\textbf{Bidirectional Pretext Tasks.}} After training, we study the expressive ability of the event-level features learned by bidirectional pretext tasks.  Specifically, a caption head (vanilla LSTM) and an FC layer are attached to the event features to evaluate the caption prediction ability and classification ability, respectively. By the linear probing performance of caption prediction and temporal action localization, we investigate whether the bidirectional prediction learns better representations than unidirectional prediction. 
As shown in Table~\ref{tab:bp}, our final model with bidirectional prediction performs better than models trained with merely ETG or TEG tasks in terms of several captioning metrics, and the average mAP score of temporal action localization also shows the same trend, which verifies the effectiveness of the proposed bidirectional pretext tasks.

\begin{table}[]
\caption{Ablation studies of bidirectional pretext tasks. The performance of temporal action localization is evaluated on ActivityNet 1.3~\cite{caba2015activitynet}, which shares the same videos with ActivityNet Captions. For ``w/o ETG", we remove the caption generator, and for ``w/o TEG", we remove the text encoding branch. }
\vspace{-0.5em}
\renewcommand\arraystretch{0.9}
\centering
    \small
    \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{1.15 mm}{
\begin{tabular}{l|ccc|c}
\toprule
    \multirow{2}{*}{Method} & \multicolumn{3}{c|}{{Captioning}} & Localization  \\
  & METEOR(PC) & METEOR & SODA(c) & Avg mAP \\ 
 \midrule
w/o TEG & 14.18 & 7.50 & 4.65 & 18.83 \\
w/o ETG & 13.66 & 6.84 & 4.80 & 12.40  \\
\rowcolor{Gray}
{Full} & \textbf{15.73} & \textbf{8.03} & \textbf{5.68} & \textbf{22.28} \\
\bottomrule

\end{tabular}}
\label{tab:bp}
\vspace{-1.0em}
\end{table}

 \begin{table}[]
\caption{Comparison of different semantic-aware costs.}
\small
\vspace{-0.5em}
\renewcommand\arraystretch{0.9}
\centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{0.75 mm}{
\begin{tabular}{l|ccc|cc}
\toprule
    \multirow{2}{*}{Method} &\multicolumn{3}{c|}{{Grounding}} & \multicolumn{2}{c}{{Dense Captioning}} \\
 & IoU0.5   & IoU0.7   & mIoU & METEOR & SODA(c) \\ 
 \midrule
w/o semantic cost & 58.30 & 36.82 & 53.96 & 8.70 & 5.63  \\
caption cost & 58.14 & 37.19 & 54.12 & \textbf{8.84} & 5.63  \\
\rowcolor{Gray}
{contrastive cost} & \textbf{60.67} & \textbf{38.55} & \textbf{55.40}  & 8.50 & \textbf{6.22} \\
\bottomrule
\end{tabular}
}
\label{tab:labelassign}
\vspace{-1.0em}
\end{table}

\begin{figure*}[]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/fig3_v11.pdf}
         \vspace{-0.7 em}
    \caption{The performance on three tasks by varying contrastive cost ratios.}
    \label{fig:clratio}
    \vspace{-1.0em}
\end{figure*}

\vspace{-1.0em}
\paragraph{\textbf{Label Assignment.}} We test two types of cost to measure the semantic distance between an event and a sentence: 1) Contrastive cost; 2) Caption cost, i.e., the negative log probability of the target sentence produced by the caption head. Table~\ref{tab:labelassign} shows the ablation of semantic costs in label assignment. Only adopting localization cost can not capture the high-level similarity between events and sentences, resulting in inferior performance. By using caption cost, the matching algorithm focus on event proposals that could generate the target sentence with a high probability. However, we found that the joint probability of generated sentences is affected by the sentence length and the portion of common words. For example, short sentences containing common words usually have a lower matching cost with all event segments. We can see from the table that incorporating caption cost does not show a clear improvement. Our model with contrastive cost could achieve a clear performance gain on most metrics. Although there is a performance drop on METEOR, we achieve a substantial improvement gain on SODA(c). Considering that SODA(c) measure a set of captions together by considering their temporal structure, we conclude that the proposed contrastive cost could increase the coherence of generated captions and the accuracy of grounding results. 

Fig.~\ref{fig:clratio} shows the performance comparison by varying semantic cost ratio ${\lambda}$. From the sub-figures, we can see that after modulating the cost ratio, the listed metrics across four tasks could achieve performance gain over ${\lambda}=0$. As ${\lambda}$ becomes larger than 1.5, the performance on most tasks drops. It is reasonable since too large ${\lambda}$ will make the total matching cost overwhelmed by semantic similarity and neglects the localization cost, leading to wrong matching when semantically-similar events occur in different positions of the video.  


\begin{table}[]
\caption{Ablation studies of text encoding.}
\vspace{-0.5em}
\small
\renewcommand\arraystretch{1.0}
\centering
        \makeatletter\def\@captype{table}\makeatother
        \setlength{\tabcolsep}{0.65 mm}{
\begin{tabular}{l|cc|cc}
\toprule
     \multirow{2}{*}{Settings} &\multicolumn{2}{c|}{{Grounding}} & \multicolumn{2}{c}{{Dense Captioning}} \\
& IoU0.7   & mIoU & METEOR & SODA(c) \\ 
\midrule
CLS& 22.17 & 42.14 & 8.27 & 6.20 \\
Mean& 28.02 & 46.74 & 8.30 & \textbf{6.24} \\
Att & 29.69 & 46.83 & 8.42 & 6.13 \\
Att + Cross-Sent Att& 32.11 & 48.88 & 8.34 & 6.17 \\
\rowcolor{Gray}
Att + Cross-Sent Att + PE& \textbf{38.55} & \textbf{55.40} & \textbf{8.50} & 6.22\\ 
\bottomrule
\end{tabular}}
\label{tab:te}
\vspace{-1.0em}
\end{table}

\vspace{-1.0em}
\paragraph{\textbf{Text Encoding}}

Table~\ref{tab:te} shows the ablation of text encoding. We first study the ablations of sentence encoding. ``CLS" considers the [CLS] tokens produced by RoBERTa as the sentence representation. ``Mean" means we obtain the sentence representation by calculating the mean-pooled features of all output tokens of RoBERTa. Our model with attention aggregation (``Att") could achieve superior performance over these two variants on grounding and comparable performance on dense captioning. Additionally, we found that position embedding (PE) in context modeling plays an essential role in boosting the grounding performance. We conclude that the order of sequence is crucial for localization since essential sentence orders are typically consistent with the temporal order of the target events.

\begin{figure}[]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/fig5_v3.pdf}
         \vspace{-2.2em}
    \caption{Visualization of video examples.}
    \label{fig:qa}
    \vspace{-1.5em}
\end{figure}

\subsection{Qualitative Analysis}
To illustrate the quality of learned event-level features, we visualize two examples of different tasks and compare them with previous state-of-the-art methods. As shown in Fig.~\ref{fig:qa}, the proposed method could surpass previous methods in both the quality of decoded sentences and the accuracy of predicted timestamps. Note that distinguishing semantically similar events is a challenging problem for untrimmed video learning. In Fig.~\ref{fig:qa}(a), PDVC mixes up multiple events and recognizes the wrong subject (``woman'' instead of ``man''). Compared with PDVC, our method benefits from the text supervision in TEG so that the model can understand fine-grained differences between events and preserve more accurate semantic information in event features.

