\documentclass[11pt,a4paper]{article}

\usepackage{graphicx}
\usepackage[OT1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{fullpage}

\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{prop}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\usepackage{float}
\usepackage{color}

\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\bf{ \color{red} TODO: {#1}}}}

\newcommand{\vecteur}[1]{\ensuremath{\mathbf{#1}}}


\newcommand{\precedent}{\texttt{prec}}


\setlength{\unitlength}{0.5cm}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algorithm,algorithmic}


\usepackage[numbers]{natbib}

\tolerance=1000         

\emergencystretch=1cm   


\begin{document}

\title{Throughput Maximization in Multiprocessor Speed-Scaling}



\author{
   Eric Angel\footnote{IBISC, Universit\'e d'Evry Val d'Essonne, France.}\\
   \and
   Evripidis Bampis
   \footnote{Sorbonne Universit\'es, UPMC Univ Paris 06, UMR 7606, LIP6, F-75005, Paris, France.}\\
   \and
   Vincent Chau\\
   \and
   Nguyen Kim Thang
}

\date{}
\maketitle
\begin{abstract}
We are given a set of  jobs that have to be executed on a set of  speed-scalable machines that can vary their speeds dynamically using the energy model introduced in [Yao et al., FOCS'95]. Every job  is characterized by its release date , its deadline , its processing volume  if  is executed on machine  and its weight . We are also given a budget of energy  and our objective is to maximize the weighted throughput, i.e. the total weight of jobs that are completed between their respective release dates and deadlines. We propose a polynomial-time approximation algorithm where the preemption of the jobs is allowed but not their migration. Our algorithm uses a primal-dual approach on a linearized version of a convex program with linear constraints. Furthermore, we present two optimal algorithms for the non-preemptive case where the number of machines is bounded by a fixed constant. More specifically, we consider: {\em (a)} the case of identical processing volumes, i.e.  for every  and , for which we present a polynomial-time algorithm for the unweighted version, which becomes a pseudopolynomial-time algorithm for the weighted throughput version, and {\em (b)} the case of agreeable instances, i.e. for which  if and only if , for which we present a pseudopolynomial-time algorithm. Both algorithms are based on a discretization of the problem and the use of dynamic programming.
\end{abstract}



\section{Introduction}

Power management has become a major issue in our days. One of the mechanisms  used for saving energy in computing systems is speed-scaling where the speed of the machines can dynamically change over time. We adopt the model first introduced by 
Yao et al. \cite{YDS95} and we study the multiprocessor scheduling problem of maximizing the throughput of jobs for a given budget of energy. 
Maximizing throughput, i.e. the number of jobs or the total weight of jobs executed on time for a given budget of energy is a very natural objective in this setting. Indeed mobile devices, such as mobile phones or computers, have a limited energy capacity depending on the quality of their battery, and throughput is one of the most popular objectives in scheduling literature for evaluating the performance of scheduling algorithms for problems involving jobs that are subject to release dates and deadlines \cite{Brucker:2010:SA:1951614, Lawler90, DBLP:journals/orl/Baptiste99}. Different variants of the throughput maximization problem in the online speed-scaling setting have been
studied in the literature \cite{CCLLMW07,Li11,BCLL08,CLL10}. However, in the off-line context, only recently, an optimal pseudopolynomial-time algorithm has been proposed for the {\em preemptive}\footnote{The execution of a job may be interrupted and resumed later.} single-machine case \cite{ABC14}. Up to our knowledge no results are known for the throughput maximization problem in the multiprocessor case. In this paper, we address this issue. More specifically, we first consider the case of a set of unrelated machines and we propose a polynomial-time constant-approximation algorithm for the problem of maximizing the weighted throughput in the {\em preemptive non-migratory}\footnote{This means that the execution of a job may be interrupted and resumed later, but only on the same machine on which it has been started.} case. Our algorithm is based on the primal-dual scheme and it is inspired by the approach used in \cite{DevanurJain12:Online-matching} for the online matching problem. In the second part of the paper, we propose exact algorithms for a fixed number of identical parallel machines for instances where the  processing volumes of the jobs are all equal, or agreeable instances. Much attention has been paid to these types of instances in the speed-scaling literature (witness for instance \cite{DBLP:conf/spaa/AlbersMS07}). Our algorithms, in this part, are for the non-preemptive case and they are based on a discretization of the problem and the use of dynamic programming. 

\paragraph{Problem Definition and Notations} In the first part of the paper, we consider the problem for a set of unrelated parallel machines.
Formally, there are  unrelated machines and  jobs. Each job  has its release date , deadline ,
weight  and its processing volume  if  is assigned to machine . If a job is executed on 
machine  then it must be entirely processed during time interval  on that machine without migration. 
The \emph{weighted throughput} of a schedule is the total weight of completed jobs.  
At any time, a machine can choose a speed to process a job. If the speed of machine  at time  is 
 then the energy power at  is  where  is a given convex function.
Typically, one has  where . The \emph{consumed energy} 
on machine  is . 
Our objective is to maximize the weighted throughput for a given budget of energy .
Hence, the scheduler has to decide the set of jobs which will be executed, assign the 
jobs to machines and choose appropriate speeds to schedule such jobs without exceeding 
the energy budget.  
In the second part of the paper we consider identical parallel machines (where the processing volume is not machine-dependent) and two  families of instances: {\em (a)} instances with identical processing volumes, i.e.  for every  and , and {\em (b)} agreeable instances, i.e. for which  if and only if .

In the sequel, we need the following definition: Given an arbitrary convex function  as the energy power function, define 
. As said before, for the most studied case in the literature one has , and therefore 
. 




\subsection{Our approach and contributions}
In this paper, we propose an approximation algorithm for the preemptive non-migratory
weighted throughput problem
on a set of unrelated speed-scalable machines in Section~\ref{sec:approx}. 
Instead of studying the problem directly, 
we study the related problem of minimizing the consumed energy 
under the constraint that the total weighted throughput must be at least some given 
throughput demand .
 


For the problem of minimizing the energy's consumption under throughput constraint,
we present a polynomial time algorithm which has the following property: 
the consumed energy
of the algorithm given a throughput demand  is at most that 
of an optimal schedule with throughput demand .  
The algorithm is based on a primal-dual scheme for mathematical programs with 
linear constraints and a convex objective function. Specifically, our approach consists in 
considering a relaxation with convex objective and linear constraints. Then, we
linearize the convex objective function and construct a dual program. Using this procedure,
the strong duality is not necessarily ensured but the weak duality always holds and 
that is indeed the property that we need for our approximation algorithm. The linearization and 
the dual construction follow the scheme introduced in \cite{DevanurJain12:Online-matching}
for online matching.  

For the problem of maximizing 
the throughput under a given budget of energy, we apply a dichotomy search 
using as subroutine the algorithm for the problem of minimizing the energy's consumption for a given weighted throughput demand.
Our algorithm is a -approximation for the weighted throughput  
and the consumed energy is at most  factor of the given energy budget
where  is an arbitrarily small constant. The violation of  the 
energy budget by a factor of  is due to the arithmetic precision in computation.
The energy budget is rational as the input size is finite while the consumed 
energy for a given throughput demand could be an irrational number. For that reason,
the algorithm's running time is polynomial in the input size of the problem 
and  . Clearly, one may be interested in finding a 
tradeoff between the precision and the running time of the algorithm.   
 
 
In Section~\ref{sec:exact}, we propose exact algorithms for the non-preemptive scheduling on 
a fixed number of speed-scalable identical machines. By identical machines, we mean that , 
i.e. the processing volume of every job is independent of the machine on which it will be executed.
We show that for the special case of the problem in which there is a single machine 
and the release dates and deadlines of the jobs are \emph{agreeable} (for every jobs  and , if  then ) the weighted throughput problem is already weakly 
-hard when all the processing volumes are equal. 
We consider the following two cases
 (1) jobs have the same 
processing volume but have arbitrary release dates and deadlines; 
and (2) jobs have arbitrary processing volumes, but their release dates and deadlines are agreeable. We present pseudo-polynomial time algorithms 
based on dynamic programming for these variants. Specifically, when all jobs have the same 
processing volume, our algorithm has running time  where .  
Note that when jobs have unit weight, the algorithm has polynomial running time.
When jobs are agreeable, our algorithm has running time 
where . Using standard techniques, these algorithms may lead to 
approximation schemes. 





\subsection{Related work}
A series of papers appeared for some online variants of throughput maximization: the first work that considered throughput maximization and speed scaling in the online setting  has been presented by Chan et al.
\cite{CCLLMW07}. They considered the single machine case with release dates and deadlines and they assumed that there is an upper bound on the machine's speed.
They are interested in maximizing the throughput, and minimizing the energy among
all the schedules of maximum throughput.
They presented an algorithm which is -competitive with respect to both objectives.
Li \cite{Li11} has also considered the maximum throughput when
there is an upper bound in the machine's speed and he proposed
a 3-approximation greedy algorithm for the throughput and a 
constant approximation ratio for the energy consumption.
In \cite{BCLL08}, Bansal et al. improved the results of \cite{CCLLMW07}, while in \cite{LLTW07}, Lam et al. studied the 2-machines
environment.
In \cite{CLMW07}, Chan et al.  defined the energy efficiency of a schedule 
to be the total amount of work completed in time divided by the total energy usage.
Given an efficiency threshold, they considered the problem of finding a schedule of maximum throughput.
They showed that no deterministic algorithm can have competitive ratio less than the ratio of the maximum to 
the minimum jobs' processing volume.
However, by decreasing the energy efficiency of the online algorithm the competitive ratio
of the problem becomes constant.
 Finally,
in \cite{CLL10}, Chan et al. studied the problem of minimizing the energy plus a rejection penalty. The rejection penalty is a cost incurred for each job which is not completed on time and each job is associated with a value which is its importance. The authors proposed an -competitive algorithm for the case where the speed is unbounded and they showed that no -competitive algorithm exists for the case where the speed is bounded.
In what follows, we focus on the offline case.
Angel et al. \cite{ABCL13} were the first to consider the
throughput maximization problem in this setting. 
They provided
a polynomial time algorithm to solve optimally the single-machine problem for agreeable instances. More recently in \cite{ABC14}, they proved that there is a pseudo-polynomial time algorithm for solving optimally the preemptive single-machine problem with arbitrary release dates and deadlines and arbitrary processing volume. For the weighted version, the problem is -hard even for instances in which all the jobs have common release dates and deadlines. Angel et al. \cite{ABCL13} showed that the problem admits a pseudo-polynomial time algorithm for agreeable instances. 
Furthermore, Antoniadis et al. \cite{AHOV13} considered a generalization of the classical knapsack problem where the objective is to maximize the total profit of the chosen items minus the cost incurred by their total weight. The case where the cost functions are convex can be translated in terms of a weighted throughput problem where the objective is to select the most profitable set of jobs taking into account the energy costs. Antoniadis et al. presented a FPTAS and a fast 2-approximation algorithm for the non-preemptive problem where the jobs have no release dates or deadlines.

Up to the best of our knowledge, no works are known for the offline throughput maximization problem in the case of multiple machines. However, many papers consider the closely related problem of minimizing the consumed energy.

For the preemptive single-machine case, Yao et al.\cite{YDS95} in their seminal paper proposed an optimal polynomial-time algorithm. Since then, a lot of papers appears in the literature (see \cite{Albers10}).
Antoniadis and Huang \cite{AH13} have considered the non-preemptive energy minimization problem. They proved that the non-preemptive single-machine case
is strongly NP-hard even for laminar instances \footnote{In a laminar instance  for any
pair of jobs  and , either , 
, or .} and they proposed a 
-approximation algorithm. This result has been improved recently in \cite{DBLP:conf/fsttcs/BKLLS13} where the authors proposed a -approximation algorithm, where  is the generalized Bell number.
For instances in which all the jobs have the same processing volume,
Bampis et al. \cite{BKLLN13} gave a -approximation for the single-machine case.
However the complexity status of this problem remained open.
In this paper, we settle this question even for the identical machine case where the number of the machine is a fixed constant. Notice that independently, Huang et al. in \cite{HO13} proposed a polynomial-time algorithm for the single machine case.

The multiple machine case where the preemption and the migration of jobs are allowed can be solved in polynomial time in
\cite{DBLP:conf/spaa/AlbersAG11}, \cite{DBLP:conf/europar/AngelBKL12} and \cite{DBLP:conf/isaac/BampisLL12}.
Albers et al. \cite{DBLP:conf/spaa/AlbersMS07} considered the multiple machine problem
where the preemption of jobs is allowed but not their migration. They showed that the problem is polynomial-time solvable for agreeable instances when the jobs
have the same processing volumes. They have also showed that it becomes strongly
NP-hard for general instances even for jobs with equal processing volumes and for this case
they proposed an -approximation algorithm. For the case
where the jobs have arbitrary processing volumes, they showed that  the problem is NP-hard
even for instances with common release dates and common deadlines. Albers et
al. proposed a -approximation algorithm for instances with common
release dates, or common deadlines, and an
-approximation algorithm for
instances with agreeable deadlines. 
Greiner et al.  \cite{DBLP:conf/spaa/GreinerNS09} proposed a -approximation algorithm for general instances, 
where  is the -th Bell number.
Recently, the approximation ratio for agreeable instances has been improved to
 in \cite{BKLLN13}. For the non-preemptive multiple machine energy minimization problem, the only known result is a non-constant approximation algorithm presented in \cite{BKLLN13}.



\section{Approximation Algorithms for Preemptive Scheduling}		\label{sec:approx}

In Section \ref{sec:approx-energy}, we first study a related problem in which we look for an algorithm
that minimizes the consumed energy under the constraint of throughput demand. 
Then in Section \ref{sec:approx-throughput} we use that algorithm as a sub-routine to derive an algorithm for 
the problem of maximizing throughput under the energy constraint. 

\subsection{Energy Minimization with Throughput Demand Constraint}	\label{sec:approx-energy}

In the problem, there are  jobs and  unrelated machines. 
A job  has release date , deadline ,
weight  and processing volume  if it is scheduled in machine . 
Given throughput demand , the scheduler needs to choose a 
subset of jobs, assign them to the machines and decide the speed 
to process these job in such a way that the total weight (throughput) of completed 
jobs is at least  and the consumed energy is minimized. 
Jobs are allowed to be processed preemptively but without migration.   



Let 's be variables indicating whether job  
is scheduled in machine . 
Let 's be the variable representing the speed
that the machine  processes job  at time .  
The problem can be formulated as the following primal convex relaxation .

In the relaxation, constraints (\ref{constr:box}) ensures that a job can be chosen at most once.
Constraints (\ref{constr:completed}) guarantee that job  must be completed if it is assigned to 
machine . To satisfy the throughput demand constraint, we use the knapsack inequalities (\ref{constr:knapsack}) 
introduced in \cite{CarrFleischer00:Strengthening-integrality}. 
Note that in the constraints,   is a subset of jobs and .
Those constraints reduce significantly the integrality gap of the relaxation 
compared to the natural constraint .

Define function . 
Consider the following a dual program .

The construction of the dual  is inspired by \cite{DevanurJain12:Online-matching} 
and is obtained by linearizing the convex objective of the primal. 
By that procedure the strong duality is not necessarily guaranteed but the weak duality always holds. 
Indeed we only need the weak duality for approximation algorithms. 
In fact, the dual  gives a meaningful lower bound 
that we will exploit to design our approximation algorithm. 

\begin{lemma}[Weak Duality] 	\label{lem:formulation-PD}
The optimal value of the dual program  is at most the optimal value of the primal program
.
\end{lemma}
\begin{proof}
As  is convex, for every  and functions  and , we have


Notice that if  is fixed then  has a lower bound in linear form
(since in that case   and  are constants).
We use that lower bound to derive the dual.  
Fix functions  for every . 
Consider the following linear program and its dual
in the usual sense of linear programming.


\begin{figure}[ht]	
\centering{
\begin{minipage}[t]{0.3\linewidth}
\begin{center}

  
\end{center}
 \end{minipage}

\begin{minipage}[t]{0.6\linewidth}
\begin{center}
    
\end{center}
\end{minipage}
}
\caption{Strong duality for LP}
\label{fig:weak-duality}
\end{figure}
By strong LP duality, the optimal value of theses primal and dual programs are equal. Denote that value with .  

Let  be the optimal value of the primal program .
Hence, for every choice of , we have a lower bound on , i.e., 
 
by (\ref{ineq:convex-energy+values}). 
So 
where 's are feasible solutions for . 
The latter is the optimal value of the dual program . Hence, 
the lemma follows.  
\end{proof}

The primal/dual programs  and  
highlights main ideas for the algorithm. 
Intuitively, if a job  is assigned to machine  then
one must increase the speed of job  in machine  at 
 in order to always 
satisfy the constraint (\ref{constr:dual-1}). 
Moreover, when constraint (\ref{constr:dual-2}) becomes tight for some job 
and machine , one could assign  to  in order to continue to raise some 
and increase the dual objective. The formal algorithm is given as follows.


\begin{algorithm}[H]
\begin{algorithmic}[1] 
\STATE Initially, set  and  equal to 0
	for every job , machine  and time .
\STATE Initially, .
\WHILE{}
	\FOR{every job  and every machine }
		\STATE Continuously increase  at   for 
			and simultaneously update 
			until .
		\STATE Set .
		\STATE Reset  as before, i.e.,  for every .
	\ENDFOR 
	\STATE Continuously increase  until
		 for some job  
		and machine .
	\STATE Assign job  to machine . Set  and 
		for every .
	\STATE Set . Moreover, set 
	\STATE Reset  and  for every .
\ENDWHILE
\end{algorithmic}
\caption{Minimizing the consumed energy under the throughput constraint}
\label{algo:energy}
\end{algorithm}

In the algorithm  for  is defined as , this is usually a
set of intervals, and thus the speed  is increased simultaneously on a set of intervals.
Notice also that since  is a convex function,   is non decreasing. Hence, in line 5 of the algorithm,
 can be replaced by ; so we can avoid the computation of 
the derivative P'(z).
Given the assignment of jobs and the speed function  of each machine  returned by the algorithm, in order to obtain a feasible schedule it is sufficient to schedule on each machine the jobs with the earliest deadline first order.
Note that in the end of the algorithm variables  is indeed equal to 
 --- the speed of machine  for every . 

The algorithm is illustrated by an exemple given in the appendix. 

\begin{lemma}		\label{lem:approx-feasible}
The solution  and  for every  constructed 
by Algorithm~\ref{algo:energy} is feasible for the dual . 
\end{lemma}
\begin{proof}
By the algorithm, variables 's and variables 's are maintained 
in such a way that the constraints (\ref{constr:dual-1}) are always satisfied. 
Moreover, by the construction of variables 
's, 's and 's, the constraints
(\ref{constr:dual-2}) are ensured (for every machine and every job).  
\end{proof}

\begin{theorem}		\label{thm:approx-main}
The consumed energy of the schedule returned by the algorithm with a throughput demand of
 is at most the energy of the optimal schedule with a throughput 
demand .
\end{theorem}
\begin{proof}
Let  be the energy consumed by the optimal schedule
with the throughput demand . By Lemma~\ref{lem:formulation-PD}, 
we have that 

where the variables  satisfy the same constraints in the dual
. Therefore, it is sufficient to prove that latter quantity is larger than
the consumed energy of the schedule returned by the algorithm with the throughput demand , denoted by .
Specifically, we will prove a stronger claim. For  and  (which is equal to ) in the 
feasible dual solution constructed by Algorithm~\ref{algo:energy} with the throughput demand , it always holds that


By the algorithm, we have that 
By the algorithm in the first sum , each term  iff
 and  is assigned to . In the third sum,  iff  equals
 at some step during the execution of the algorithm. Thus, we consider only such sets 
in that sum. Let  be the last element 
added to . For 
and , by the while loop condition 
. Moreover,
.
Hence,  and the inequality 
(\ref{eq:approx-1}) follows.

Fix a machine  and let  be the set of jobs assigned to machine  (renaming jobs if 
necessary).
Let  be the speed of machine  at time  after assigning 
jobs , respectively. In other words,  for every
. By the algorithm, we have 
 for every .
As every job  is completed in machine , .
Note that  only at  in .
Thus,

where in the second equality, note that  for ;
the inequality is due to the convexity of .

As inequality (\ref{eq:approx-2}) holds for every machines , summing over all machines we get

Together with (\ref{eq:approx-1}), we deduce that

where the last inequality is due to the definition of  
(recall that  for every  such that ) and 
 for every job  (by the algorithm).
\end{proof}

\begin{corollary}	\label{cor:single-machine}
For single machine setting, 
the consumed energy of the schedule returned by the algorithm with a throughput demand of
 is at most that of the optimal schedule with a throughput 
demand .
\end{corollary}
\begin{proof}
For single machine setting, we can consider a relaxation similar to 
without constraints (\ref{constr:box}) and without machine index  for all variables. 
The dual construction, the algorithm and the analysis remain the same. Observe that now 
there is no dual variable . By that point we can improve the factor 
from  to .
\end{proof}

Note that a special case of the single machine setting is the minimum knapsack problem.
In the latter, we are given a set of  items, item  has size  and value .
Moreover, given a demand , the goal is to find a subset of items having minimum total 
size such that the total value is at least . The problem corresponds to the single machine 
setting where all jobs have the same span, i.e.,  for all jobs
; item size and value correspond to job processing volume and weight, respectively;
and the energy power . Carnes and Shmoys \cite{CarnesShmoys08:Primal-Dual-Schema} 
gave a 2-approximation primal-dual
algorithm for the minimum knapsack problem. That result is a special case of 
Corollary~\ref{cor:single-machine} where  for linear function . 

\subsection{Throughput Maximization with Energy Constraint} 	\label{sec:approx-throughput}
We use the algorithm in the previous section as a sub-routine and 
make a dichotomy search in the feasible domain of the total throughput. 
The formal algorithm is given as follows.

\begin{algorithm}[htbp]
\begin{algorithmic}[1] 
\STATE Given a throughput demand , denote  the 
	consumed energy due to Algorithm~\ref{algo:energy}. 
\STATE Initially, set  and  where the sum is taken over all jobs .
\STATE Set .
\WHILE{ or }
	\IF{} 
		\STATE 
	\ENDIF
	\IF{} 
		\STATE 
	\ENDIF
	\STATE 
\ENDWHILE
\RETURN the schedule which is the solution of Algorithm~\ref{algo:energy} with throughput demand .
\end{algorithmic}
\caption{Maximizing throughput under the energy constraint}
\label{algo:throughput}
\end{algorithm}

\begin{theorem}
Given an arbitrary constant , Algorithm~\ref{algo:throughput} is
-approximation in throughput with the consumed energy at most 
. The running time of the algorithm is 
polynomial in the size of input and . 
\end{theorem}
\begin{proof}
Let  be the optimal throughput with the energy budget . 
Suppose that . By Theorem~\ref{thm:approx-main},
the consumed energy of the optimal schedule must be strictly larger than . However, the latter is 
at least . So the consumed energy constraint is violated in the optimal schedule
(contradiction). Hence, . By the algorithm, the consumed energy
of the algorithm is at most . In Algorithm~\ref{algo:throughput}, 
the number of iterations in the while loop is proportional to the size of the input and .
As Algorithm~\ref{algo:energy} is polynomial, the running time of Algorithm~\ref{algo:throughput}
is polynomial in the size of input and . 
\end{proof}


\section{Exact Algorithms for Non-Pre\-emptive Scheduling}	\label{sec:exact}

\subsection{Preliminaries}

\paragraph{Notations} In this section, we consider schedules without preemption with 
a fixed number  of identical machines. So the processing volume of a job 
is the same on every machine and is equal to . 
Without loss of generality, we assume that all parameters of the problem such as 
release dates, deadlines and processing volumes of jobs are \emph{integer}.
We rename jobs in non-decreasing order of their deadlines, i.e. 
.
We denote by  
the minimum release date.
Define  as the set of release dates and deadlines (\textsc{edf}), 
i.e., .
Let  
be the set of jobs among the  first ones w.r.t. the \textsc{edf} order,
whose release dates are within  and .
We consider \emph{time vectors}  
where each component  is a time associated to the machines  for .
We say that  if  for every .
Moreover,  if  and 
. The relation  is a partial order over 
the time vectors. Given a vector , we denote by 
.

\paragraph{Observations} We give some simple observations on non-preemptive scheduling with the objective 
of maximizing
throughput under the energy constraint. First, it is well known that
due to the convexity of the power function , 
each job runs at a constant speed during its whole
execution in an optimal schedule. This follows from Jensen's Inequality.
Second, for a restricted version of the problem in which there is a single machine, jobs have the same 
processing volume and are agreeable, the problem is already -hard.
That is proved by a simple reduction from {\sc Knapsack}.

\begin{prop}	\label{prop_np_hard}
The problem of maximizing the weighted throughput on the case where jobs have agreeable deadline and have the same processing volume is weakly -hard.
\end{prop}
\begin{proof}
Let  be the the weighted throughput problem on the case where jobs have agreeable deadline and have the same processing volume.
In an instance of the {\sc Knapsack} problem we are given a set of  items, 
each item  has a value  and a size .
Given a capacity  and a value , we are asked for a subset of items with total 
value at least  and total size at most .


Given an instance of the {\sc Knapsack} problem, 
construct an instance of problem  as follows.
For each item , create a job  with 
,  ,  and .
Moreover, we set , i.e. the budget of energy is equal to .

We claim that the instance of the {\sc Knapsack} problem is feasible 
if and only if there is a feasible schedule for problem 
of total weighted throughput at least .

Assume that the instance of the {\sc Knapsack} is feasible.
Therefore, there exists  a subset of items  such that  and .
Then we can schedule all jobs corresponding to item in  
with constant speed equal to 1. That gives a feasible schedule
with total energy consumption at most  and the total weight at least . 

For the opposite direction of our claim, assume there is a feasible schedule for
problem  of total weighted throughput at least .
Let  be the jobs which are completed on time in this schedule.
Clearly, due to the convexity of the speed-to-power function, the schedule that 
executes the jobs in  with constant speed is also feasible.
Since the latter schedule is feasible, we have that .
Moreover, .
Therefore, the items which correspond to the jobs in  form a feasible solution for the {\sc Knapsack}. 
\end{proof}

The hardness result rules out the possibility of polynomial-time exact algorithms for the problem. 
However, as the problem is weakly -hard, there is still possibility for approximation schemes. 
In the following sections, we show pseudo-polynomial-time exact algorithms for instances with 
equal processing volume jobs 
and agreeable jobs.  



\subsection{Equal Processing Volume}


In this section, we assume that  for every job .

\begin{definition}\label{def:Theta_x_y}
Let  
. Moreover, .
\end{definition}


The following lemma gives an observation on the structure of an optimal 
schedule.

\begin{lemma}\label{theta}
There exists an optimal schedule in which the starting time and completion time
of each job belong to the set .
\end{lemma}
\begin{proof}
Let  be an optimal schedule and 
be the corresponding schedule  on machine .
 can be partitioned into successive blocks
of jobs where the blocks are separated by idle-time periods. 
Consider a block  and decompose
 into maximal sub-blocks  such 
that all the jobs executed inside a sub-block  are scheduled with
the same common speed  for . 
Let  and  be two consecutive jobs such
that  and  belong to two consecutive sub-blocks, let's say  and 
. Then either  or .
In the first case, the completion time of job  (which is also the starting time
of job ) is necessarily , otherwise we could obtain a better schedule
by decreasing (resp. increasing) the speed of job  (resp. ).
For the second case, a similar argument shows that the completion time of job 
is necessarily . Hence, each sub-block begins and finishes at a date which belong to . 

Consider a sub-block  and let  be its starting and completion times. 
As jobs have the same volume and 
the jobs scheduled in  are processed non-preemptively by the same speed, 
their starting and completion times must belong to .
\end{proof}

Using Lemma~\ref{theta} we can assume that each job is processed at some speed which belong to the following set.

\begin{definition}	\label{set_of_speed}
Let 
 be the set of different speeds.
\end{definition}


\begin{definition}\label{def:Eksxtu}
For , define  as the minimum energy consumption 
of a non-preemptive (non-migration) schedule  such that 
\begin{itemize}
\item  
	and   where  is the set of jobs scheduled in , 
\item if  is assigned to machine  then it is entirely processed in 
	  for every ,
\item ,
\item for some machine , it is idle during interval ,
\item for arbitrary machines ,  is at least the last 
	starting time of a job in machine . 
\end{itemize}
\end{definition}
Note that  if no such schedule 
exists. 





\begin{prop}\label{prop_E}
One has

where

\end{prop}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.7\textwidth]{fig_decompo_E.pdf}
\end{center}
\caption{Illustration of Proposition~\ref{prop_E}}
\label{fig_E}
\end{figure}


\begin{proof}
The base case for  is straightforward. We will prove the recursive formula 
for . There are two cases: (1) either in the schedule
that realizes , job  is not chosen, so 
; 
(2) or  is chosen in that schedule. In the following, we are interested 
by that second case.

\paragraph{We first prove that }
Fix some arbitrary time vector 
and weight  and time  such that 
for some  and some machine . 
Consider a schedule  
that realizes  and  
a schedule that realizes .
We build a schedule with  from  to  and with 
 from  to  and job  scheduled within 
during  on machine . 
Recall that by definition of , 
machine  does not execute any job during .
Obviously, the subsets  and  do not intersect, so
this is a feasible schedule which costs at most 

As that holds for every time vector 
and weight  and time  such that 
for some  and some machine , we deduce that 
.



\paragraph{We now prove that }
Let  be the schedule that realizes  in which
the starting time of job  is maximal. Suppose that job  is scheduled on machine  and 
its starting time is denoted as . For every machine , define  be the earliest 
completion time of a job which is completed after  on machine  by schedule . 
If no job is completed after  on machine  then define . Hence, we have a time vector 
.

We split  into two sub-schedules  
and 
such that  if it is started and completed in  for 
some machine . Note that such job  has release date .

We claim that for every job ,  where 
by the definition of vector . 
By contradiction, suppose that some job 
has , meaning that job  is available at the starting time of 
job . By definition of  and , 
job  is started after the starting time of job . Moreover,  means that .
Thus, we can swap jobs  and  (without modifying the machine speeds). 
Since all jobs have the same volume, this operation 
is feasible. The new schedule has the same energy cost while the starting time of job  is
strictly larger. That contradicts the definition of .


\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.7\textwidth]{swap_E_multi_proc.pdf}
\end{center}
\caption{Illustration of the swap argument}
\label{fig_swap}
\end{figure}

Therefore, all jobs in  have release dates in 
and all jobs in  have release dates in .
Moreover, with the definition of vector , the schedules
 and  are valid (according to Definition~\ref{def:Eksxtu}).
Let  be the speed that machine  processes job
 in .
Hence, the consumed energies by schedules  and 
 are at least 
and  where 
 is the total weight of jobs in . 
We have

Therefore, we deduce that  in case 
job  is chosen in the schedule that realizes .
The proposition follows.
\end{proof}





\begin{theorem}\label{prop_complexity_non_preempt}
The dynamic program in Proposition~\ref{prop_E} has a running time of .
\end{theorem}


\begin{proof}
Denote .
Given an energy budget , the objective function is
.
The values of  are stored in a multi-dimensional array of
size .
Each value need   time to be computed thanks to Proposition~\ref{prop_E}.
Thus we have a total running time of .
This leads to an overall time complexity .
\end{proof}




\subsection{Agreeable Jobs}

In this section, we focus on another important family of instances. More precisely,
we assume that the jobs have {\em agreeable} deadlines,
i.e.  for any pair of jobs  and , one has  if and only if
.

Based on Definition~\ref{def:Theta_x_y}, we can extend the set of starting and completion times
for each job into the set .
\begin{definition}\label{def:Phi}
Let 
 with , and .
\end{definition}


The following lemmas show the structure of an optimal schedule that we will use in order to design our algorithm.


\begin{lemma}\label{agreeable}
There exists an optimal solution in which all jobs in each machine are
scheduled according to the Earliest Deadline First ({\sc edf}) order without preemption.
\end{lemma}

\begin{proof}
Let  be an optimal schedule.
Let  and  be two consecutive jobs that are scheduled on the same machine  in .
We suppose that job  is scheduled before job  with .
Let  (resp. ) be the starting time (resp. completion time)
of job  (resp. job ) in . Then, we have necessarily . The execution of jobs  and  can be swapped in the time interval 
.
Thus we obtain a feasible schedule  in which job  is scheduled before job
 with the same energy consumption.
\end{proof}



\begin{lemma}\label{Phi}
There exists an optimal {\sc edf} schedule  in which each job in 
has its starting time and its completion time that belong to the set .
\end{lemma}
\begin{proof}
We proceed as in Lemma~\ref{theta}. We partition an optimal schedule
 into blocks and sub-blocks where the starting and completion times of 
every sub-blocks belong to the set . Consider an arbitrary sub-block. As all the 
parameters are integer, the total volume of the sub-block is also an integer in  and the 
total number of jobs processed in the sub-block is bounded by the total volume. 
Thus the starting and completion times of any job in the sub-block must belong to the set . 
\end{proof}

By Lemma~\ref{Phi}, we can assume that each job is processed with a speed that 
belongs to the following set.

\begin{definition}\label{def:Delta}
Let  
be the set of different speeds.
\end{definition}





\begin{definition}\label{def:Fktw}
For , define  as the minimum energy consumption 
of an non-preemptive (and a non-migratory) schedule  such that:
\begin{itemize}
\item  and 
	where  is the set of jobs scheduled in 
\item if  is assigned to machine  then it is entirely processed in 
	  for every .
\end{itemize}
\end{definition}
Note that  if no such schedule 
exists. 







For a vector  and a speed ,
let  be the set of vectors
 such that there always exists some machine 
 with the following properties:


\begin{prop}\label{prop_F}
One has

where 

\end{prop}


\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.7\textwidth]{fig_decompo_agreeable.pdf}
\end{center}
\caption{Illustration of Proposition~\ref{prop_F}}
\label{fig_F}
\end{figure}


\begin{proof}

The base case for  is straightforward. We will prove the recursive formula 
for . There are two cases: (1) either in the schedule
that realizes , job  is not chosen, so 
; 
(2) or  is chosen in that schedule. In the following, we are interested 
in the case when  is chosen.

\paragraph{We first prove that }
Fix some arbitrary time vector   and  such that
 for some 
speed  and some machine .
Then we have .
Consider a schedule  that realizes .
We build a schedule with  from  to  
and job  is scheduled on machine 
during  
and an idle period during . 
So this is a feasible schedule which costs at most 

As that holds for every time vector 
and some speed  and some machine , we deduce that 
.


\paragraph{We now prove that }
Let  be the schedule that realizes  in which
the starting time of job  is maximal.
We consider the sub-schedule .
We claim that all the jobs of  
are completed before  which
is the vector obtained from  after removing job .

Hence the cost of the schedule 
 is at least . Thus,

Therefore, we deduce that  in case job  is chosen in the schedule that realizes . The proposition follows.
\end{proof}



\begin{theorem}\label{prop_complexity_agreeable}
The dynamic programming in Proposition~\ref{prop_F} has a total running time of .
\end{theorem}


\begin{proof}
Given an energy budget , the objective function is
.
The values of  are stored in a multi-dimensional array of
size .
Each value need  time to be computed thanks to Proposition~\ref{prop_F}.
Thus we have a total running time of .
This leads to an overall time complexity  .
\end{proof}





\bibliographystyle{abbrv}
\bibliography{biblio} 
\appendix
\section{Execution of Algorithm 1}

In this example, we have  unrelated machines,  jobs and each job have the same weight, i.e.
. We want to compute the energy's consumption when we have to choose 
 jobs according to our algorithm.

Let  with  be the power function of the machines.
And let the derivative function .

The processing volume of each job is given in the following table.
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & 1 & 2 & 3 & 4 \\ 
\hline 
1 & 1 & 3 & 4 & 2 \\ 
\hline 
2 & 2 & 5 & 3 & 1 \\ 
\hline 
\end{tabular} 
\end{center}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{fig_instance_exemple.pdf}
\caption{Instance of 4 jobs with the respective release date and deadline}
\label{exemple_appendix}
\end{center}
\end{figure}


\paragraph{Step 1}
At this step, the set of chosen jobs is \\

We continuously increase the speed  for each job  and each machine  with .
Then we obtain the value of .


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & 1 & 2 & 3 & 4 \\ 
\hline 
1 &  &  &  &  \\ 
\hline 
2 &  &  &  &  \\ 
\hline 
\end{tabular} 
\caption{Table of  at Step 1}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline 
 & 1 & 2 & 3 & 4 \\ 
\hline 
1 & 3/4 & 81/4 & 192/25 & 6 \\ 
\hline 
2 & 6 & 625/4 & 81/25 & 3/4 \\ 
\hline 
\end{tabular} 
\caption{Table of  at Step 1}
\end{table}

We continuously increase  until
		 for some job  
		and machine .

Since  at this step and we can only modify 
the value of , then we have to find the maximum 
value of  such that one of the constraint becomes tight.

 and 


Thus Job 1 is affected to machine 1 and 


\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{fig_step1.pdf} 
\end{center}
\caption{Speed profile  at the end of Step 1}
\label{speed_profile_step1}
\end{figure}



\paragraph{Step 2}

At this step, the set of chosen jobs is  and the speed profile  can be found in Figure~\ref{speed_profile_step1}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline 
  & 2 & 3 & 4 \\ 
\hline 
1  &  &  &  \\ 
\hline 
2 &  &  &  \\ 
\hline 
\end{tabular} 
\caption{Table of  at Step 2}
\end{table}




\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline 
 & 2 & 3 & 4 \\ 
\hline 
1 & 441/16 & 12 & 150/16 \\ 
\hline 
2  & 625/4 & 81/25 & 3/4 \\ 
\hline 
\end{tabular} 
\caption{Table of  at Step 2}
\end{table}

At this step we have only  which is positive.
Then we have 




Job 4 is affected to machine 2,  and .

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{fig_step2.pdf} 
\end{center}
\caption{Speed profile  at the end of Step 2}
\label{speed_profile_step2}
\end{figure}



\paragraph{Step 3}

\\

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline 
  & 2 & 3  \\ 
\hline 
1  &  &   \\ 
\hline 
2 &  &   \\ 
\hline 
\end{tabular} 
\caption{Table of  at Step 3}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline 
 & 2 & 3 \\ 
\hline 
1 & 441/16 & 12  \\ 
\hline 
2  & 625/4 & 144/25 \\ 
\hline 
\end{tabular} 
\caption{Table of  at Step 3}
\end{table}






Job 3 is affected to machine 2,  and .


\begin{figure}[H]
\begin{center}
\includegraphics[width=0.7\textwidth]{fig_step3.pdf} 
\end{center}
\caption{Speed profile  at the end of Step 3}
\label{speed_profile_step3}
\end{figure}




\end{document}
