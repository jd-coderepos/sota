\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs,subcaption,amsfonts,dcolumn}
\usepackage{dsfont}
\usepackage{times}
\usepackage{epsfig}
\usepackage{color, colortbl}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{hhline}
\usepackage{cellspace}
\usepackage{bbding}
\usepackage{bm}
\usepackage{nicefrac}     
\usepackage{microtype}     
\usepackage{verbatim}
\usepackage{color}
\usepackage{float}
\usepackage{enumitem}
\usepackage{tabulary,overpic,xcolor}
\definecolor{mygray}{gray}{.92}
\definecolor{myred}{rgb}{1,0.5,0.5}
\definecolor{myred2}{rgb}{0.75,0,0}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}
\newcommand{\myparagraph}[1]{{\vspace{0.5em} \noindent \bf #1}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\bd}[1]{\textbf{#1}}
\usepackage[utf8]{inputenc}
\newcommand{\authorskip}{\hspace{12mm}}
\usepackage{caption}
\usepackage{cuted}
\newcommand\blfootnote[1]{
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\begin{document}

\title{GRiT: A Generative Region-to-text Transformer for Object Understanding}

\author{
 Jialian Wu \authorskip Jianfeng Wang \authorskip  Zhengyuan Yang \authorskip
 Zhe Gan \\ Zicheng Liu \authorskip Junsong Yuan \authorskip Lijuan Wang\
    L_{t} = \frac{1}{N+1}\sum_{i=1}^{N+1}\text{CE}(y_i, p(y_i|o,y_{0,...,i-1})),
Image-text pairs)}}&\multirow{3}{*}{\makecell{backbone,\\
			text decoder}}&ViT-B&52.0\\
			&&&ViT-L&52.7\\
			&&&Coswin-H&54.8\\
			\hline
			\multirow{3}{*}{MAE~\cite{he2022masked}}&\multirow{3}{*}{\makecell{Image\\Reconstruction\ImageNet-1K)}}&\multirow{3}{*}{\makecell{backbone}}&ViT-B&53.6\\
			&&&ViT-L&56.3\\
			&&&ViT-H&58.8\\
			\hline
		\end{tabular}
	\end{center}
	\vspace{-5mm}
	\caption{\textbf{GRiT pre-training.} MAE pre-training outperforms GIT pre-training, especially in the huge size. GIT adopts Coswin-H~\cite{yuan2021florence} as the visual backbone, so we adjust our backbone accordingly. Results are evaluated on COCO 2017 val.}
	\label{tab:pretrain}
	\vspace{-4mm}
\end{table}

\subsection{Ablation Studies}
In this section, we perform ablation experiments on object detection task to study GRiT's properties.

\myparagraph{GRiT \emph{vs.} Closed-set Object Detector:} GRiT achieves object detection in an \emph{open-set} way, which is more difficult than the closed-set way of standard object detectors. To measure the performance gap between these two strategies, we build a closed-set object detector by replacing GRiT's text decoder with the closed-set multi-category classifier as used in standard object detectors. The rest of the model settings are exactly the same as GRiT's settings. As shown in Table~\ref{tab:ablation1}, GRiT is comparable to the closed-set object detector with a 0.8 AP gap. This result demonstrates GRiT's open-set framework can serve as a new promising formulation for object detection.

\begin{table*}[t!]
	\vspace{-2mm}
	\begin{center}	
	\setlength{\tabcolsep}{4pt}
		\begin{tabular}{l|c|c|c|ccc|ccc}
			\rowcolor{mygray}
			&&&& \multicolumn{3}{c|}{{ 2017 val}} & \multicolumn{3}{c}{{2017 test-dev}}\\
			\rowcolor{mygray}
        \multirow{-2}{*}{Model} & \multirow{-2}{*}{Backbone}  &
        \multirow{-2}{*}{\makecell{Backbone\\Pre-training}}  &\multirow{-2}{*}{\makecell{Full model\\Pre-training}} & AP & AP &AP& AP & AP &AP\\
            \shline
            \multicolumn{3}{@{}l}{\emph{Closed-Set framework:}}\\
            \hline
           Deformable DETR~\cite{zhu2020deformable}&ResNeXt-101&IN-1K&-&-&-&-&50.1&69.7&54.6\\
           EfficientDet-D7x~\cite{tan2020efficientdet}&EfficientNet-B7&IN-1K&-&54.4&-&-&55.1&74.3&59.9\\ 
           CenterNet2~\cite{zhou2021probabilistic}& Res2Net&IN-1K& Unlabeled COCO&-&-&-&56.4&74.0&61.6\\
            HTC++~\cite{liu2021swin,chen2019hybrid}&Swin-L&IN-22K&-&57.1&-&-&57.7&-&-\\
            DyHead~\cite{dai2021dynamic}&Swin-L&IN-22K&-&58.4&-&-&58.7&77.1&64.5\\
            ViT-Adapter-L~\cite{chen2022vision}&ViT-L&IN-22K&-&58.4&-&-&58.9&-&-\\
             Soft Teacher~\cite{xu2021end}&Swin-L&IN-22K&Object365&60.1&-&-&-&-&-\\
            ViTDet~\cite{li2022exploring}&ViT-H&IN-1K&-&60.4&-&-&-&-&-\\
            DINO~\cite{zhang2022dino}&Swin-L&IN-22K&Object365&63.1&-&-&63.2&-&-\\
            GLIPv2~\cite{zhang2022glipv2}&Swin-H&\makecell{CC-15M+\\IN-22K}&\makecell{Object365, FourODs,\\GoldG, CC15M+SBU}&-&-&-&60.6&-&-\\
			\shline
			\multicolumn{3}{@{}l}{\emph{Open-Set framework:}}\\
			\hline
			GRiT (Ours)&ViT-B&IN-1K&-&53.6&71.6&58.2&53.8&71.8&58.7\\
			GRiT (Ours)&ViT-L&IN-1K&-&56.3&73.8&61.4&56.6&74.5&61.8\\
			GRiT (Ours)&ViT-H&IN-1K&-&58.8&76.6&64.4&59.0&76.9&64.5\\
			GRiT (Ours)&ViT-H&IN-1K&Object365&60.3&78.1&65.7&60.4&78.1&66.0\\
			\shline
		\end{tabular}
	\end{center}
	\vspace{-5mm}
	\caption{\textbf{Comparison with state-of-the-art object detectors on COCO.} All results are reported under single-scale testing. The reference results are cited from the best-performing models in their papers. }
	\label{tab:sota_od}
	\vspace{-1mm}
\end{table*}

\myparagraph{Object Feature Size:} We experiment with different sizes of object features input to the text decoder. As shown in Table~\ref{tab:ablation2}, 49 feature vectors achieve similar performance to 196 feature vectors, which indicates the text decoder is robust to the number of input object features.

\myparagraph{Beam Search:} Standard object detector outputs multiple class labels for one box by its multi-category classifier. Similarly, we use beam search to output multiple class name texts for each box as introduced in the method section. As shown in Table~\ref{tab:ablation3}, beam search improves object detection metric, especially for recall, and beam size=3 is a good trade-off between accuracy and inference time.

\myparagraph{Object Scoring:} To rate object predictions, we combine both objectness score from the foreground object extractor and object description score from the text decoder. GRiT is always equipped with objectness score due to its function of removing background boxes. As shown in Table~\ref{tab:ablation4}, description score improves 0.9 AP when beam size=1. However, the model without description score fails when beam size=3, \ie, outputting three class names each box. The reason is that all the three classes share the same confidence score though at least two of them are false positives. This has a mild impact on recall but leads to significantly worse precision and AP.

\myparagraph{Incremental Training:} GRiT is open-set and capable of generating unlimited number of words. As data evolve, one can add new object classes or concepts in the middle of training without adapting any architecture. We simulate this use case on COCO in Table~\ref{tab:ablation5}, where we start training with 60 classes and add the remaining 20 classes in the middle of training. Compared to the model that is trained on all classes throughout, we achieve similar results when adding the rest of the classes in the last one-third of training. GRiT performs reasonably even in the case where we supplement the remaining classes in the last one-ninth of training.

\begin{figure*}[t!]
	\vspace{-1mm}
	\centering
	\includegraphics[width=1\linewidth]{figs/fig6.pdf}
	\vspace{-5mm}
	\caption{\textbf{Zero-shot object understanding predictions.} Zoom in for the best viewing.}
	\vspace{-2mm}
	\label{fig:fig6}
\end{figure*}

\myparagraph{Pre-training:} We study GRiT with different pre-training schemes. As shown in Table~\ref{tab:pretrain}, the performance improves notably as the model size increases, and MAE pre-training shows better performance than GIT pre-training. MAE pre-training is to recover the masked patches in the image, which may exhibit a stronger localization capability helping object detection task. GIT pre-training focuses more on the image-level representation and language modeling, which shows slightly better performance on dense captioning task, as in Table~\ref{tab:sota_densecap}.

\subsection{Comparison to State-of-the-Arts on COCO}
As shown in Table~\ref{tab:sota_od}, we compare GRiT with the state-of-the-art object detectors on COCO. Generally, all the methods use the visual backbone pre-trained on ImageNet (IN). To deliver the best performance, some models are also finetuned on extra object detection datasets, \eg, Object365~\cite{shao2019objects365}, before finetuning on COCO. The results of the state-of-the-art object detectors are cited from their best model settings. Therefore, some listed models may be achieved in different training and testing configurations than others. For example, GLIPv2~\cite{zhang2022glipv2} makes use of Object365 plus four object detection datasets~\cite{zhang2022glipv2}, image-text datasets CC~\cite{sharma2018conceptual} and SBU~\cite{ordonez2011im2text}, and grounding datasets GoldG~\cite{zhang2022glipv2}. DyHead~\cite{dai2021dynamic} utilizes a larger input image size with 2000 pixels at maximum. CenterNet2~\cite{zhou2021probabilistic} adopts BiFPN~\cite{tan2020efficientdet}, DCN~\cite{dai2017deformable}, and a larger input image size of 15601560 pixels.  Soft Teacher~\cite{xu2021end} is constructed on HTC++~\cite{liu2021swin}. Despite the challenge of the open-set framework, GRiT performs comparably with the state-of-the-art closed-set object detectors. This once again demonstrates our generative region-to-text framework is a promising formulation for excelling in object detection. 

\subsection{Comparison to State-of-the-Arts on VG}
As shown in Table~\ref{tab:sota_densecap}, we compare GRiT with the state-of-the-art dense captioning models. Without object relation modeling as used in previous dense captioning methods, GRiT outperforms the current best model by 4 mAP. We notice that the absolute values are low on the dense captioning metric. We hypothesize the cause is the very dense region annotations and the strictness of language metric.

\begin{table}[t]
\vspace{-3mm}
	\begin{center}	
		\setlength{\tabcolsep}{10pt}
		\begin{tabular}{l|c}
			\rowcolor{mygray}
			Method&mAP\\ 
			\shline
            FCLN~\cite{johnson2016densecap}&5.39\\
            JIVC~\cite{yang2017dense}&9.31\\
            ImgG~\cite{li2019learning}&9.25\\
            COCD~\cite{li2019learning}&9.36\\
            COCG~\cite{li2019learning}&9.82\\
            CAG-Net~\cite{yin2019context}&10.51\\
            TDC+ROCSU~\cite{shao2022region}&11.49\\
            \hline
            GRiT (Ours)&15.48\\
			GRiT (Ours)&15.52\\
			\hline
		\end{tabular}
	\end{center}
    \vspace{-4mm}
	\caption{\textbf{Comparison with state-of-the-art dense captioning models on Visual Genome.} GRiT refers to the model initialized by MAE pre-training scheme. GRiT is initialized from the GIT model that is re-pretrained on CC3M and CC12M~\cite{sharma2018conceptual} datasets removing the VG dataset. Our results are reported based on ViT-B.}
	\vspace{-4mm}
	\label{tab:sota_densecap}
\end{table}

\subsection{Zero-shot Object Understanding}
Inspired by the success of image-to-text transformers on image understanding, GRiT adopts the same network structure for the visual encoder backbone and text decoder as GIT~\cite{wang2022git}. We explore whether GRiT can achieve zero-shot object understanding by simply using GIT's trained parameters without finetuning on object description annotations. To this end, we initialize GRiT by the GIT model fine-tuned on COCO image captioning task.
Then, we finetune our feature pyramid and foreground object extractor on COCO detection data (no use of the class names) while keeping GIT's parameters fixed, such that GRiT is able to generate object boxes. To align with GIT's parameters, object features are cropped from the last feature map of the visual backbone rather than the feature pyramid. This zero-shot object understanding result is shown in Fig.~\ref{fig:fig6}. We see that the model generates various object-level descriptions for different regions in the same image though GIT is trained on the image-level description task. While, we also notice in experiments that there are images for which this zero-shot scheme does not work well. We hypothesize the issue is that GIT's text decoder parameters do not well recognize the cropped features as GIT's text decoder is trained with the entire image's features as input. 

\section{Conclusion}
This work presents a general and open-set object understanding framework, GRiT. GRiT formulates object understanding as region-text pairs, which is capable of unifying various region/object-level tasks in a single paradigm. GRiT is end-to-end from image feature extraction to foreground object detection to object description generation. Extensive experiments on object detection and dense captioning demonstrate the effectiveness and generality of GRiT. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{GRiT}
}
\end{document}