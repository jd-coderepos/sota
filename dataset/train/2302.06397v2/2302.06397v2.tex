\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{amsmath, bm}
\usepackage{longtable}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{enumerate}
\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{longtable}
\usepackage{xspace}
\usepackage{comment}



\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}





\newcommand{\ours}{{TadNER}\xspace}

\newcommand{\nonetoken}{\textsc{O}\xspace}

\title{Type-Aware Decomposed Framework \\for Few-Shot Named Entity Recognition}


\author{Yongqi Li, Yu Yu, Tieyun Qian\thanks{{ }{ }Corresponding author.}\\
         School of Computer Science, Wuhan University, China \\ 
         Intellectual Computing Laboratory for Cultural Heritage, Wuhan University, China\\
        \texttt{\{liyongqi,Yu.Yu1024,qty\}@whu.edu.cn}}


\begin{document}
\maketitle

\begin{abstract}
Despite the recent success achieved by several two-stage prototypical networks in few-shot named entity recognition (NER) task, \emph{the over-detected false spans} at the span detection stage and \emph{the inaccurate and unstable prototypes} at the type classification stage remain to be challenging problems.
In this paper, we propose a novel \textbf{T}ype-\textbf{A}ware \textbf{D}ecomposed framework, namely \ours, to solve these problems.
We first present \emph{a type-aware span filtering strategy} to filter out false spans by removing those semantically far away from type names. We then present \emph{a type-aware contrastive learning strategy} to construct more accurate and stable prototypes by jointly exploiting support samples and type names as references.
Extensive experiments on various benchmarks prove that our proposed \ours framework yields a new state-of-the-art performance.
\footnote{Our code and data will be available at \url{https://github.com/NLPWM-WHU/TadNER}.}
\end{abstract}




\section{Introduction}
Named entity recognition (NER) aims to detect entity spans and classify them into pre-defined categories (entity types). When there are sufficient labeled data, deep learning-based methods~\cite{huang2015bidirectional,ma2016end,lample2016neural,chiu2016named} can get impressive performance. In real applications, it is desirable to recognize new categories which are unseen in training/source domain. However, collecting extra labeled data for these new types will be surely time-consuming and labour-expensive. Consequently, few-shot NER~\cite{fritzler-2019,yang-katiyar-2020-simple}, which involves identifying unseen entity types based on a few labeled samples for each class (i.e., \emph{support samples}) in test domain, has attracted great research interests in recent years.


End-to-end metric learning based methods~\cite{yang-katiyar-2020-simple,das-etal-2022-container} are the mainstream in few-shot NER.
These methods need to simultaneously learn the complex structure consisting of entity boundary and  entity type. When the domain gap is large, their performance will drop dramatically because it is extremely hard to capture such complicated structure information with  only a few support examples for domain adaptation.  This leads to the insufficient learning of boundary information, resulting that these methods often misclassify entity boundaries and cannot obtain very satisfying performance.

Recently, there is an emerging trend in adopting two-stage prototypical networks~\cite{wang-2022-enhanced,ma-etal-2022-decomposed} for few-shot NER, which decompose NER into two separate \emph{span extraction} and \emph{type classification} tasks and perform one task at each stage.
Since decomposed methods only need to handle one single boundary detection task at the first stage, they can find more accurate boundaries and obtain better performance than end-to-end approaches.\looseness-1

\begin{figure}[t!]
\centering
\includegraphics[width=0.48\textwidth]{figures/introduction_comparision.pdf}
\caption{(a) shows  over-detected false spans, (b) shows  spans got by adopting our type-aware span filtering  strategy.
(c) shows  inaccurate and unstable prototypes, (d) shows  prototypes got by adopting our  type-aware contrastive learning strategy.
}
\label{fig_introduction_comparision}
\vspace{-3mm}
\end{figure}

While making good progress, these two-stage  prototypical networks still face two challenging problems, i.e., \emph{the over-detected false spans}  and \emph{the inaccurate and unstable prototypes} in corresponding stages.
(1) At the span extraction stage in test phase, the decomposed approaches usually recall many over-detected false spans whose types only exist in the source domain. For example, ``1976'' in Figure~\ref{fig_introduction_comparision} (a) belongs to a DATE type in the source domain since there are many samples like ``Obama was born in 1961'' in training, and thus it is easily recognized as a span by the span detector. However, there is no such label in the test domain and ``1976'' is thus assigned a false LOC type.
(2) The  prototypical networks in decomposed methods target at learning a type-agnostic metric similarity function to classify entities in test samples (\emph{i.e., query samples}) via their distance to prototypes.
Since the prototypes are constructed using very few support samples in the type-agnostic feature space, they might be inaccurate and unstable. For example, in Figure~\ref{fig_introduction_comparision} (c), a prototype is just the support sample in one-shot NER and thus deviates far away from the real class center.




Based on the above observations, we propose a \textbf{T}ype-\textbf{A}ware \textbf{D}ecomposed framework, namely \ours,  for few-shot \textbf{NER}. Our method follows the span detection and type classification learning scheme in the decomposed framework but moves two steps further to overcome the aforementioned issues.

Firstly, we present \emph{a type-aware span filtering strategy} to filter out false spans by removing those semantically far away from type names~\footnote{Note that though  type assignments are unknown in few-shot NER, the type names (labels) in test domain are provided.}. By this means, the over-detected spans like ``1976'' whose types do not exist in test domain can be removed due to the long semantic distance to type names, as shown in Figure~\ref{fig_introduction_comparision} (b).

Secondly, we  present \emph{a type-aware contrastive learning strategy} to construct  more accurate and stable prototypes by jointly leveraging  type names and support  samples as references. Through this way,  the type names can serve as the guidance for prototypes and make them  not deviate  too far away from the class centers even in some extreme outlier cases, as shown in Figure~\ref{fig_introduction_comparision} (d).

Extensive experimental results  on 5 benchmark datasets demonstrate the superiority of our \ours over the state-of-the-art decomposed methods. In particular, in the hard intra Few-NERD and 1-shot Domain Transfer settings, \ours achieves a 8\% and 9\% absolute F1 increase, respectively.


\begin{figure*}[tbh!]
\centering
\includegraphics[width=1.0\textwidth]{figures/framework.pdf}
\caption{The overall structure of our proposed \ours framework. (a) Training in the source domain. (b) Inference on the query set by utilizing the support samples in the target domain. Note that the source and target domains have different entity type sets.
}
\label{fig_framework}
\vspace{-0mm}
\end{figure*}

\section{Method}




In this section, we formally present our proposed \ours. The overall structure of our \ours is shown in Figure~\ref{fig_framework}.
Note that the type-aware contrastive learning and type-aware span filtering strategies take effect at the type classification stage in the training and test domain, respectively.

\paragraph{Task Formulation}
Given a sequence  with  tokens, NER aims to assign each token  a corresponding label , where  is the entity type set and \nonetoken denotes the non-entity label.
For few-shot NER, a model is trained in a source domain dataset  with the entity type set .
The model is then fine-tuned in a test/target domain dataset  with the entity type set  using a given support set . The entity token set and corresponding label set in  are denoted as  and ,
where  is the label  and  is the number of entity tokens.
The model is supposed to recognize entities in the query set  of the target domain.
Besides,  and  have no or very little overlap, making few-shot NER very challenging.
More specifically, in the -way -shot setting, there are  labels in  and  examples associated with each label in the support set .



\subsection{Source Domain Training}
The source domain training consists of span detection and type classification stages. The procedure is shown in Figure~\ref{fig_framework} (a).
\subsubsection{Span Detection}\label{section_span_detetor}
The span detection stage is formulated as a sequence labeling task, similar to an existing decomposed NER model~\cite{ma-etal-2022-decomposed}.
We adopt BERT~\cite{devlin-etal-2019-bert} with parameters  as the PLM encoder .
Given an input sentence , the encoder produces contextualized representations for each token as:

where ~\footnote{In this paper,  denotes the hidden size of the PLM.}.
 is then fed into a classification layer consisting of a dropout layer~\cite{srivastava2014dropout} and a linear layer to get the probability distribution  (, )~\footnote{In Appendix~\ref{appendix_results_span_detection}, we perform a detailed analysis using the IO, BIO, and BIOES tagging schemes.} using a softmax function:

where  and 
are the weight matrix and bias.

After that, the training loss is formulated by the averaged cross-entropy of the probability distribution and the ground-truth labels:
\vspace{-0mm}

\vspace{-0mm}
where = when the -th token is \nonetoken -token, = otherwise. Specifically, we denote the training loss of span detection stage as . During the training procedure, the parameters  are updated to minimize .

\subsubsection{Type Classification}
\paragraph{Representation}
Given an input sentence , we only select entity-tokens  () with ground-truth labels  for the training of this stage.
For the entity type set  of the source domain , we manually convert them into their corresponding type names \footnote{ \texttt{Map()} is the function used to convert a label to a type name, e.g. “PER” to “person”. Please refer to Appendix \ref{appendix_type_names} for type names of all datasets.}.

After that, to obtain tokens with type name information, which are further used for calculating contrastive loss, we concatenate entity tokens with their corresponding labels in two orders, i.e., entity-label order and label-entity order.
Here we use another encoder  with parameters  to obtain contextual representations:

\vspace{-0mm}

where  is the concatenation operator, and  and  denote two kinds of type-aware representations of the entity-token , which are obtained in entity-label order and label-entity order, respectively.

\paragraph{Type-Aware Contrastive Learning}
To learn a generalized and type-aware feature space, which can further be used for constructing more accurate and stable prototypes, we borrow the idea of contrastive learning~\cite{NEURIPS2020_d89a66c7_supervised_contrastive_learning} and use  two kinds of type-aware token representations mentioned above to construct positive and negative pairs as shown in Figure~\ref{fig_framework} (a), i.e., those with the same label in different orders as positive pairs and those with different labels as negative pairs.
The type-aware contrastive loss is calculated as:\looseness=-1

\vspace{-0mm}

\vspace{-0mm}

where  is the number of entity tokens in a batch and  is the set of positive samples with the same label type .
Here we adopt the dot product with a normalization factor as the similarity function .
We also add a temperature hyper-parameter  for focusing more on difficult pairs~\cite{pmlr-v119-chen20j}.
During the source domain training, the parameters  are updated to minimize .


\subsection{Target Domain Inference}
As illustrated in Figure~\ref{fig_framework} (b), during the target domain inference, we first extract candidate spans in query sentences and then remove over-detected false spans via the type-aware span filtering strategy. 
Finally, we classify remaining candidate spans into certain entity types to get the final results.


\subsubsection{Span Detection}\label{early_stopping_span}
The span detector with its parameters  trained in the source domain is further fine-tuned with samples in the support set  in the target domain  to minimize  in Eq.(\ref{loss_span_detector}).
To alleviate the risk of over-fitting, we adopt a loss-based early stopping strategy, i.e., stopping the fine-tuning procedure once the loss rises  times continuously, where  is a hyper-parameter.

After fine-tuning the span detector, we use it to detect entity words of query sentences in  and then consider continuous entity words as a candidate span, e.g., ``Barack Obama''.
Finally, we obtain the candidate span set  containing all candidate spans, which will be assigned  entity types at the type classification stage.




\subsubsection{Type Classification}


\paragraph{Domain Adaption}
Benefiting from the generalized and type-aware feature space trained in the source domain, we can further get a domain-specific encoder  via fine-tuning with the following loss:

\vspace{-0mm}


\paragraph{Type-Aware Span Filtering}
As we illustrate in the introduction, the span detector may generate some over-detected false spans whose type names only belong the source domain, since the semantics of entity type names are not considered at the span detection stage.
To solve this problem, we propose a type-aware span filtering strategy during the inference phase to remove these false spans.
Intuitively, the semantic distance of these false spans is far from all the golden type names.
Based on this assumption, we calculate a threshold  with the fine-tuned encoder  using entity tokens and corresponding type names in the support set:

This threshold  is used to remove the over-detected false spans. 
And the remaining candidate spans will be assigned corresponding labels.

\paragraph{Type-Aware Prototype Construction}
We can construct a type-aware prototype for each entity type ,  which is more accurate and stable owing to the generalized and type-aware feature space learned in the source domain:

\vspace{-0mm}

where  is the concatenation operator and  denotes the set of entity words with the label type  in the support set.

\paragraph{Inference}
For each remaining candidate span , we assign it a label type  with the highest similarity:

\vspace{-0mm}

where  is the type-aware prototype representation corresponding to the label type , and  is the predicted  label type of the candidate span .  is the self-concatenated representation of  for consistency with the dimension of the prototype .
The entire procedure of inference in the target domain is presented in Appendix~\ref{app:algorithm_1}.


\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1.2mm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l l  ccccc  ccccc}
    \toprule
        \multirow{3}{*}{\textbf{Paradigms}} &
        \multirow{3}{*}{\textbf{Models}} &
        \multicolumn{5}{c}{\textbf{Intra}} & \multicolumn{5}{c}{\textbf{Inter}}\\
        \cmidrule(lr){3-7} \cmidrule(lr){8-12}
        & & \multicolumn{2}{c}{\textbf{12-shot}} & \multicolumn{2}{c}{\textbf{510-shot}} & \multirow{2}{*}{\textbf{Avg.}} & \multicolumn{2}{c}{\textbf{12-shot}} & \multicolumn{2}{c}{\textbf{510-shot}} & \multirow{2}{*}{\textbf{Avg.}}\\

        \cmidrule(lr){3-4}\cmidrule(lr){5-6} \cmidrule(lr){8-9}\cmidrule(lr){10-11}

         & & 5 way & 10 way & 5 way & 10 way & & 5 way & 10 way & 5 way & 10 way & \\

         \cmidrule(lr){1-2}\cmidrule(lr){3-7} \cmidrule(lr){8-12}

        \multirow{5}{*}{\emph{One-stage}} &
         ProtoBERT & 20.76{\small{\textpm0.84}} & 15.05{\small{\textpm0.44}} & 42.54{\small{\textpm0.94}} & 35.40{\small{\textpm0.13}} & 28.44
         & 38.83{\small{\textpm1.49}} & 32.45{\small{\textpm0.79}} & 58.79{\small{\textpm0.44}} & 52.92{\small{\textpm0.37}} & 45.75\\

         & NNShot & 25.78{\small{\textpm0.91}} & 18.27{\small{\textpm0.41}} & 36.18{\small{\textpm0.79}} & 27.38{\small{\textpm0.53}} & 26.90
         & 47.24{\small{\textpm1.00}} & 38.87{\small{\textpm0.21}} & 55.64{\small{\textpm0.63}} & 49.57{\small{\textpm2.73}} & 47.83\\

         & StructShot & 30.21{\small{\textpm0.90}} & 	21.03{\small{\textpm1.13}} & 38.00{\small{\textpm1.29}} & 26.42{\small{\textpm0.60}} & 28.92
         & 51.88{\small{\textpm0.69}} & 43.34{\small{\textpm0.10}} & 57.32{\small{\textpm0.63}} & 49.57{\small{\textpm3.08}} & 50.53\\

        & FSLS & 30.38{\small{\textpm2.85}} & 28.31{\small{\textpm4.03}} & 46.85{\small{\textpm3.49}} & 40.76{\small{\textpm3.18}} & 36.58  
         & 44.52{\small{\textpm4.59}} & 44.01{\small{\textpm3.35}} & 59.74{\small{\textpm2.51}} & 56.67{\small{\textpm1.75}} &  51.24\\

        & CONTaiNER & 41.51{\small{\textpm0.07}} & 	36.62{\small{\textpm0.04}} & 57.83{\small{\textpm0.01}} & 51.04{\small{\textpm0.24}} & 46.75
         & 50.92{\small{\textpm0.29}} & 47.02{\small{\textpm0.24}} & 63.35{\small{\textpm0.07}} & 60.14{\small{\textpm0.16}} & 55.36\\	

         \cmidrule(lr){1-2}\cmidrule(lr){3-7} \cmidrule(lr){8-12}
         \multirow{3}{*}{\emph{Two-stage}} &

        ESD & 36.08{\small{\textpm1.60}} & 	30.00{\small{\textpm0.70}} & 52.14{\small{\textpm1.50}} & 42.15{\small{\textpm2.60}} & 40.09
         & 59.29{\small{\textpm1.25}} & 52.16{\small{\textpm0.79}} & {69.06{\small{\textpm0.80}}} & 	64.00{\small{\textpm0.43}} & 61.13\\

        & DecomposedMetaNER & \underline{49.48{\small{\textpm0.85}}} & \underline{42.84{\small{\textpm0.46}}} & \underline{62.92{\small{\textpm0.57}}} & \underline{57.31{\small{\textpm0.25}}} & \underline{53.14}
         &\underline{64.75{\small{\textpm0.35}}} & \underline{58.65{\small{\textpm0.43}}} & \underline{71.49{\small{\textpm0.47}}} & \underline{68.11{\small{\textpm0.05}}} & \underline{65.75}\\

        & \textbf{\ours} & \textbf{60.78{\small{\textpm}0.32}} & \textbf{55.44{\small{\textpm}0.08}} & \textbf{67.94{\small{\textpm}0.17}} & \textbf{60.87{\small{\textpm}0.22}} & \textbf{61.26}
         & \textbf{64.83{\small{\textpm}0.14}} & \textbf{64.06{\small{\textpm}0.19}} & \textbf{72.12{\small{\textpm}0.12}} & \textbf{69.94{\small{\textpm}0.15}} & \textbf{67.74}\\
         
        \bottomrule
    \end{tabular}
    }
    \caption{F1 scores with standard deviations for Few-NERD.  denotes the results reported by \citet{ma-etal-2022-decomposed}.  denotes the results reported by our replication using data of the same version. The best results are in \textbf{bold} and the second best ones are \underline{underlined}.}
    \label{tab:performance_comparison_fewnerd}
\end{table*}



\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{2mm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ll  ccccc  ccccc}
    \toprule
        \multirow{2}{*}{\textbf{Paradigms}} &
        \multirow{2}{*}{\textbf{Models}} &
        \multicolumn{5}{c}{\textbf{1-shot}} & \multicolumn{5}{c}{\textbf{5-shot}}\\
        \cmidrule(lr){3-7} \cmidrule(lr){8-12}

        & & \textbf{I2B2} & \textbf{CoNLL} & \textbf{WNUT} & \textbf{GUM} & \textbf{Avg.} & \textbf{I2B2} & \textbf{CoNLL} & \textbf{WNUT} & \textbf{GUM} & \textbf{Avg.} \\

        \cmidrule(lr){1-2}\cmidrule(lr){3-7} \cmidrule(lr){8-12}

        \multirow{3}{*}{\emph{One-stage}} &
        ProtoBERT & 13.4\small{\textpm 3.0} & 49.9\small{\textpm 8.6} & 17.4\small{\textpm 4.9} & 17.8\small{\textpm 3.5} & 24.6 
        & 17.9\small{\textpm 1.8} & 61.3\small{\textpm 9.1} & 22.8\small{\textpm 4.5} & 19.5\small{\textpm 3.4} & 30.4\\


        & NNShot & 15.3\small{\textpm 1.6} & 61.2\small{\textpm 10.4} & 22.7\small{\textpm 7.4} & 10.5\small{\textpm 2.9} & 27.4 
        & 22.0\small{\textpm 1.5} & 74.1\small{\textpm 2.3} & 27.3\small{\textpm 5.4} & 15.9\small{\textpm 1.8} & 34.8\\


        & StructShot & 21.4\small{\textpm 3.8} & \underline{62.4\small{\textpm 10.5}} & 24.2\small{\textpm 8.0} & 7.8\small{\textpm 2.1} & 29.0  
        & 30.3\small{\textpm 2.1} & 74.8\small{\textpm 2.4} & 30.4\small{\textpm 6.5} & 13.3\small{\textpm 1.3} & 37.2 \\

        & FSLS & 18.3\small{\textpm 3.5} & {50.9\small{\textpm 6.5}} & 14.3\small{\textpm 5.5} & 12.6\small{\textpm 2.8} &  24.0
        & 25.4\small{\textpm 2.7} & 63.9\small{\textpm 3.3} & 24.0\small{\textpm 3.2} & 18.8	\small{\textpm 2.2} &   33.1\\


        & CONTaiNER & \underline{21.5\small{\textpm 1.7}} & {61.2\small{\textpm 10.7}} & {27.5\small{\textpm 1.9}} & {18.5\small{\textpm 4.9}} & \underline{32.2}
        & \underline{36.7\small{\textpm 2.1}} & \underline{75.8\small{\textpm 2.7}} & \underline{32.5\small{\textpm 3.8}} & {25.2\small{\textpm 2.7}} & \underline{42.6}\\

        \cmidrule(lr){1-2}\cmidrule(lr){3-7} \cmidrule(lr){8-12}

        \multirow{2}{*}{\emph{Two-stage}} &
        DecomposedMetaNER & {15.5\small{\textpm 3.0}} & {61.2\small{\textpm 9.2}} & \underline{27.7\small{\textpm 5.3}} & \underline{20.3\small{\textpm 4.2}} & 31.2
        & 19.8\small{\textpm 2.6} & {75.2\small{\textpm 5.8}} & { 29.8\small{\textpm 3.9 }} & \underline{33.5\small{\textpm 2.4}} & 39.6\\

        & \textbf{\ours} & \textbf{39.3\small{\textpm 3.8}} & \textbf{70.4\small{\textpm 10.6}}  & \textbf{32.8\small{\textpm 4.8}} & \textbf{24.2\small{\textpm 4.1}} & \textbf{41.7}
        & \textbf{45.2\small{\textpm 2.3}} & \textbf{80.5\small{\textpm 3.6}} & \textbf{34.5\small{\textpm 4.6}}  & \textbf{35.1\small{\textpm 2.2}} & \textbf{48.8}\\
        \bottomrule
    \end{tabular}
    }
    \caption{F1 scores with standard deviations for Domain Transfer.  denotes the results reported by \citet{das-etal-2022-container}.  denotes the results reported by our replication.
    Since no previous two-stage methods have conducted experiments under this setting, we choose the strong DecomposedMetaNER for reproduction experiments, and  denotes the results reported by our replication.
    The best results are in \textbf{bold} and the second best ones are \underline{underlined}.}
    \label{tab:domaintransfer}
    \vspace{-0mm}
\end{table*}





\section{Experiments}
\subsection{Evaluation Protocal}
\paragraph{Datasets}
\citet{ding2021few} propose a large scale dataset \textbf{Few-NERD} for few-shot NER, which contains 66 fine-grained entity types across 8 coarse-grained entity types. 
It contains {intra} and {inter} tasks where the train/dev/test sets are divided according to the coarse-grained and fine-grained types, respectively. 
Besides, following \citet{das-etal-2022-container}, we also conduct \textbf{Domain Transfer} experiments, where data are from different text domains (e.g., Wiki, News).
We take OntoNotes (General)~\cite{weischedel2013ontonotes} as our source domain, and evaluate few-shot performances on I2B2 (Medical)~\cite{stubbs-2015-i2b2-2014}, CoNLL (News)~\cite{sang2003conll2003}, WNUT (Social)~\cite{derczynski2017wnut17} and GUM~\cite{zeldes2017gum} datasets.
\paragraph{Baselines}
We compare our proposed \ours with many strong baselines, including \emph{one-stage} and \emph{two-stage} types.
The \emph{one-stage} baselines include ProtoBERT~\cite{snell_prototypical_2017}, NNShot~\cite{yang-katiyar-2020-simple}, StructShot~\cite{yang-katiyar-2020-simple}, FSLS~\cite{ma2022label} and CONTaiNER~\cite{das-etal-2022-container}. 
Note that FSLS also adopts type names.
The \emph{two-stage} baselines include ESD~\cite{wang-2022-enhanced} and the DecomposedMetaNER~\cite{ma-etal-2022-decomposed}
\footnote{Please refer to Appendix \ref{appendix_datasets}-\ref{appendix_implementation_details} for more descriptions about datasets, evaluation methods, baselines and implementation details.}.


\subsection{Main Results}

Table \ref{tab:performance_comparison_fewnerd} and \ref{tab:domaintransfer} report the comparison results between our method and baselines under {Few-NERD}~\footnote{Results are tested with the latest version of data from \url{https://ningding97.github.io/fewnerd/}, which is corresponding with \url{https://github.com/microsoft/vert-papers/tree/master/papers/DecomposedMetaNER\#few-nerd-arxiv-v6-version}.} and {Domain Transfer}, respectively.
We have the following important observations: 
1) Our model demonstrates superiority under {Few-NERD} settings. Notably, in the more challenging {intra} task, our \ours achieves an average 8.2\% increase in F1 score. 
Besides, our model outperforms baselines by 10.5\% and 9.2\% under 1-shot and 5-shot Domain Transfer settings, respectively.
2) Particularly, when provided with very few samples (e.g., 1-shot), the improvements become even more significant, which is a very attractive property.
3) The performance of DecomposedMetaNER, a competing model, severely deteriorates under certain settings, such as I2B2. 
This is primarily due to the presence of numerous sentences without entities, leading to multiple false detected spans.
In contrast, our \ours effectively mitigates this issue through the type-aware span filtering strategy, successfully removing false spans and achieving promising results.\looseness-1

\begin{figure*}[t]
\vspace{-0mm}
\centering
\includegraphics[width=1.0\textwidth]{figures/case_study.pdf}
\caption{Case study.
 C1 and C2 are from Few-NERD intra and CoNLL2003 in Cross datasets, respectively, and \textcolor[RGB]{0,176,80}{organization-sportsteam}, \textcolor[RGB]{197,90,17}{organization-sportsleague}, \textcolor[RGB]{114,172,225}{ORG} and \textcolor[RGB]{239,116,33}{LOC} are entity types.
}
\label{fig_case_study}
\vspace{-0mm}
\end{figure*}


\subsection{Ablation Study}
To validate the effectiveness of the main components in \ours, we introduce the following variant baselines for the ablation study:
1) \ours \textit{w/o} Type-Aware Span Filtering (TASF) removes the type-aware span filtering strategy and directly feeds all spans detected at {span detection} stage to {type classification}.
2) \ours \textit{w/o} Type Names (TN) further replaces type names with random vectors when calculating contrastive loss and constructs class prototypes using only the support samples.
3) \ours \textit{w/o} Span-Finetune skips the target domain adaptation of the span detection stage.
4) \ours \textit{w/o} Type-Finetune skips the target domain adaptation of the type classification stage.

\begin{table}[htb]
    \centering
    \setlength{\tabcolsep}{0.8mm}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccccccc}
    \toprule
        \multirow{2}{*}{\textbf{Models}}  & \multicolumn{4}{c}{\textbf{1-shot}} & \multicolumn{4}{c}{\textbf{5-shot}} & \multirow{2}{*}{\textbf{Avg.}}\\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
        & {\small{I2B2}} & {\small{CoNLL}}  & {\small{WNUT}} & {\small{GUM}} & {\small{I2B2}} & {\small{CoNLL}}  & {\small{WNUT}} & {\small{GUM}}\\
        \cmidrule(lr){1-1}   \cmidrule(lr){2-5} \cmidrule(lr){6-9}\cmidrule(lr){10-10}
        \textbf{\ours} & \textbf{39.3} & \textbf{70.4} & \textbf{32.8} & \textbf{24.2} & \textbf{45.2} & \textbf{80.5} & \textbf{34.5} & \textbf{35.1} & \textbf{45.3} \\
        \cmidrule(lr){1-1}    \cmidrule(lr){2-5} \cmidrule(lr){6-9}\cmidrule(lr){10-10}
        \textit{w/o} TASF & {21.2} & {68.5} & {31.6} & \textbf{24.2} & {27.4} & {80.1} & {34.3} & \textbf{35.1} & {40.3}\\
        \textit{w/o} TN & {20.0} & {65.6} & {28.3} & {20.3} & {26.2} & {76.3} & {33.8} & {33.2} & {38.0}\\
        \textit{w/o} Span-Finetune & 37.0 & 52.5 & 30.7 & 15.0 & {40.1} & {50.8} & {31.7} & {16.2}& {34.3}\\
        \textit{w/o} Type-Finetune & 37.6 & 68.3 & 32.3 & 20.3 & {45.2} & {76.3} & {33.6} & {27.9}& {42.7}\\
        
    \bottomrule
    \end{tabular}
    }
    \caption{Results (F1 scores) for ablation study under Domain Transfer settings. The best results are in \textbf{bold}.}
    \label{tab:ablation}
    \vspace{-0mm}
\end{table}
From Table \ref{tab:ablation}, we can observe that:
1) The removal of the type-aware span filtering strategy leads to a drop in performance across most cases, particularly in entity-sparse datasets like I2B2, where a large number of false positive spans are detected. 
Besides, for entity-dense datasets like GUM, the performance is not harmed by the span filtering strategy, which proves the robustness and effectiveness of our model in various real-world applications.
2) The omission of type names also results in a significant decrease in performance, indicating that our model indeed learns a type-aware feature space, which plays a crucial role in few-shot scenarios.
3) The elimination of finetuning in the span detection and type classification stages exhibits a substantial performance drop. 
This demonstrates that the training objective in the source domain training phase aligns well with the target domain finetuning phase via task decomposition and contrastive learning strategy, despite having different entity classes. 
As a result, the model can effectively utilize the provided support samples from the target domain, enhancing its performance in few-shot scenarios.


\subsection{Case Study}
To examine how our model accurately constructs prototypes and filters out over-detected false spans  with the help of type names, we randomly select one query sentence from Few-NERD intra and CoNLL2003 for case study.
We compare \ours with DecomposedMetaNER~\cite{ma-etal-2022-decomposed}, which also belongs to the two-stage methods.\looseness=-1



As shown in Figure~\ref{fig_case_study}, in the first case, our model correctly predicts ``turkish tff third league'' as ``organization-sportsleague'' type, while DecomposedMetaNER identifies it as a wrong ``organization-sportsteam'' type.
Since the type name and the entity span have an overlapping word ``league'', incorporating the type name into the construction of the prototype will make the identification much easier. Conversely, without the type name, it would be hard to distinguish two categories of entities because they both represent ``sports-related organizations''.

In the second case, DecomposedMetaNER incorrectly identifies ``two'' as an entity span and then assigns it a wrong entity type ``LOC'', since there are many samples like ``The two sides had not met since Oct. 18'' in the source domain Ontonotes, where ``two'' is an entity of ``CARDINAL'' type.
In contrast, our \ours successfully removes this false span via the type-aware span filtering strategy.


\subsection{Impact of Type Names}\label{sec:impact_type_names}
To further explore the impact of incorporating the semantics of type names and whether model performance is sensitive to these converted type names.
We perform experiments with the following variants of type names: 
1) Original type names, which are used in our main comparision experiments.
2) Synonymous type names. We generate three synonyms for each original type name as variants using ChatGPT. These synonyms were automatically generated to explore the effect of different but related type names on model performance.
3) Meaningless type names, e.g., ``label 1'' and ``label 2''.
4) Misleading type names, e.g., ``person'' for ``LOC'' and ``location' for ``PER'' in the CoNLL dataset.
Please refer to Appendix~\ref{appendix_type_names} for details.

\begin{figure}[htbp]
\vspace{-0mm}
\centering
\includegraphics[width=1.0\linewidth]{figures/impact_type_name.pdf}
\caption{F1 Scores on Few-NERD Intra and CoNLL 2003 with different variants of type names.}
\label{compare_type_name}
\vspace{-0mm}
\end{figure}

As shown from the Figure~\ref{compare_type_name}, we can make the following observations: 
1) All three variants of synonym type names have comparable performance, indicating that our method is robust to different ways of transforming type names.
However, the best way is still the direct transformation method, such as ``person'' for ``PER'', which is how we obtain the original type names.
2) Irrelevant or incorrect information in meaningless and misleading type names leads to a significant degradation in model performance, indicating that the semantics associated with entity classes are more suitable as anchor points for contrastive learning.

\subsection{Impact of Type-Aware Prototypes}


In order to investigate the effectiveness of our proposed strategy for solving the problem of inaccurate and unstable prototypes in the type classification stage, we further perform an analysis of the impact of stability and quality of prototypes.
We select three baselines as our compared methods: 
1) \ours w/o Type Names (TN) (the second variant baseline in the ablation study). 
2) DecomposedMetaNER~\cite{ma-etal-2022-decomposed}. 
3) Vanilla Contrastive Learning (CL), which adopts token-token contrastive loss and was proposed by ~\citet{das-etal-2022-container}.
We use it to train the type classification module in a decomposed NER framework, in order to explore whether it can address the issue of unstable and inaccurate prototypes.
Here we adopt the same 10 samplings used in the 1-shot {Domain Transfer} experiments.

\begin{figure}[htbp]
\vspace{-0mm}
\centering
\includegraphics[width=1.0\linewidth]{figures/proto_comparison.pdf}
\caption{Impacts of prototypes by different methods under 1-shot {Domain Transfer} setting. 
The horizontal and vertical coordinates indicate the n-th sampling and the accuracy of type classification, respectively.
}
\label{compare_tc_stage}
\vspace{-0mm}
\end{figure}

As shown in Figure~\ref{compare_tc_stage}, our proposed \ours achieves a significant improvement over DecomposedMetaNER on each dataset and is more stable across different samplings.
Besides, removing type names causes a sharp performance drop in some cases for \ours w/o TN, indicating that the incorporation of type names indeed helps construct more stable and accurate prototypes.
Moreover, Vanilla CL performs extremely poorly due to the introduction of an additional projection layer, which is a crucial component employed in various contrastive learning methods~\cite{pmlr-v119-chen20j,das-etal-2022-container}.
However, the inclusion of this layer hampers the model's capacity to acquire adequate semantics related to entity classification.


\subsection{Error Analysis}

We conduct an error analysis to examine the detailed types of errors made by different models.
The error statistics are shown in Table~\ref{tab:error_analysis_1}.


\begin{table}[htb]
    \centering
    \setlength{\tabcolsep}{0.8mm}
    \resizebox{\linewidth}{!}{
            \begin{tabular}{lccccccccc}
            \toprule
            \multirow{3}{*}{\textbf{Models}} & \multicolumn{4}{c}{\textbf{False   Positive}}       & \multicolumn{4}{c}{\textbf{False   Negative}}       & \multirow{3}{*}{\textbf{False}} \\
            \cmidrule(lr){2-5} \cmidrule(lr){6-9}
                                             & \multicolumn{2}{c}{Span} & \multicolumn{2}{c}{Type} & \multicolumn{2}{c}{Span} & \multicolumn{2}{c}{Type} &                                 \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                                             & Num.       & Ratio       & Num.       & Ratio       & Num.       & Ratio       & Num.       & Ratio       &                                 \\
            \cmidrule(lr){1-1} \cmidrule(lr){2-9} \cmidrule(lr){10-10}
            FSLS                            & 990        & 85.4\%      & 169        & 14.6\%      & 1178       & 87.5\%      & 169        & 12.5\%       & 2506                            \\
            CONTaiNER                        & 881        & 63.3\%      & 511        & 36.7\%      & 628        & 55.1\%      & 511        & 44.9\%      & 2531                            \\
            ESD                              & 562        & 56.5\%      & 433        & 43.5\%      & 932        & 68.3\%      & 433        & 31.7\%      & 2360                            \\
            DecomposedMetaNER                           & 622        & 56.2\%      & 485        & 43.8\%      & 639        & 56.9\%      & 485        & 43.1\%      & 2231                            \\
            \textbf{\ours}                   & 786        & 81.2\%      & 182        & 18.8\%      & 450        & 71.2\%      & 182        & 28.8\%      & 1600                             \\
            \bottomrule
            \end{tabular}
    }
    \caption{Error analysis for different methods under the Few-NERD Intra 5-way 12-shot setting. 
    We select the first 300 episodes for analysis.
    ``False Positive'' and ``False Negative'' denote the incorrectly extracted entities and unrecalled entities, respectively. 
    ``Span'' and ``Type'' denote the error is due to incorrect span/type.
}
    \label{tab:error_analysis_1}
    \vspace{-0mm}
\end{table}







We can observe that: 
1) Our \ours makes fewer errors than baselines overall. Notably, it significantly reduces false negatives, indicating its ability to accurately recall more correct entities. 
2) Both \ours and FSLS can effectively reduce  ``Type'' errors by incorporating type names. 
However, though FSLS has less ``Type'' errors than our \ours, it produces a much larger number of unrecalled samples, i.e.,  false negatives.
3) Our \ours still suffers from inaccurate span prediction, which inspires our future work.

\subsection{Model Efficiency}
Compared to one-stage approaches, e.g., CONTaiNER, two-stage models require more parameters, longer training and inference times. 
To have a close look at the time cost induced by two-stage models, we perform a model efficiency analysis and show the results in Table~\ref{tab:model_efficiency}.

\begin{table}[htb]
    \centering
    \setlength{\tabcolsep}{0.8mm}
    \resizebox{\linewidth}{!}{
            \begin{tabular}{clccccc}
            \toprule
            \multicolumn{1}{c}{\textbf{Paradigms}} &  \multicolumn{1}{c}{\textbf{Models}}   & \textbf{\#Para.} & \textbf{Train} & \textbf{Adapt} & \textbf{Inference} & \textbf{F1} \\
            \cmidrule(lr){1-2} \cmidrule(lr){3-7}
            \multirow{2}{*}{One-stage} & FSLS           & 222M    & 1871s    & 10s     & 14ms        & 30.38       \\
                                       & CONTaiNER      & 112M    & 980s     & 1s      & 17ms         & 41.51       \\
            \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-7}
            \multirow{3}{*}{Two-stage} & ESD           & 112M    & 2601s    & 0s      & 35ms          & 36.08       \\
                                       & DecomposedMetaNER         & 222M    & 35495s  & 2s      & 37ms          & 49.48       \\
                                       & \textbf{\ours}         & 222M    & 3796s      & 1.5s    & 32ms          & 60.29        \\
            \bottomrule
            \end{tabular}
    }
    \caption{Model efficiency analysis for different methods under the Few-NERD Intra 5-way 12-shot setting. 
}
    \label{tab:model_efficiency}
    \vspace{-0mm}
\end{table}

From Table~\ref{tab:model_efficiency}, it can be seen that two-stage models indeed require longer training and inference time than one-stage models. 
However, two-stage models often get better performance. 
In particular, our \ours is the most effective one among both one-stage and two-stage models, and it achieves a F1 improvement of 45\% and 67\% over CONTaiNER and ESD.
It is also the most efficient one among three two-stage models in terms of the inference time.


\subsection{Zero-Shot Performance}\label{appendix_zero}
Since there is no domain-specific support set under zero-shot NER settings, it is extremely challenging and rarely explored.
While we believe our proposed \ours can obtain certain zero-shot ability after training in the source domain for the following two reasons: 1) the model can extract entity spans in the span detection stage before fine-tuning with support samples, 2) since the feature space learnt in the type classification stage is well generalized and type-aware, we can directly adopt the representations of type names as prototypes of novel entity types.
To demonstrate the promising performance of our model under zero-shot settings, we select SpanNER~\cite{wang2021learning-from-description} as a strong baseline, which is a decomposed-based method and good at solving zero-shot NER problem.

\begin{table}[htb]
    \small
    \centering
    \setlength{\tabcolsep}{1mm}  
    {
    \begin{tabular}{lcccccccc}
    \toprule
        \multirow{2}{*}{\textbf{Model}}  & \multicolumn{5}{c}{\textbf{Domain Transfer}}\\
        \cmidrule(lr){2-6} 
        & {I2B2} & {CoNLL}  & {WNUT} & {GUM} & {Avg.}\\
         \cmidrule(lr){1-1}\cmidrule(lr){2-6} 


        SpanNER (0-shot)  & 8.02 & 23.63  & 24.82 & 6.57 & 15.76\\
        
        \textbf{\ours (0-shot)}  & \textbf{17.13}& \textbf{43.14} & \textbf{25.06}& \textbf{7.62} & \textbf{23.24}\\
        \bottomrule
    \end{tabular}
    }
    \caption{F1 scores under Domain Transfer zero-shot settings.}
    \label{tab:performance_zero_shot}
\end{table}

As shown in Table \ref{tab:performance_zero_shot}, our proposed \ours performs better than SpanNER~\cite{wang2021learning-from-description} under every case.
The reason for this may be that the type classification of SpanNER is based on a traditional supervised classification model, which performs worse generalization in cross-domain scenarios.
Besides, compared with previous metric-based methods~\cite{das-etal-2022-container,ma-etal-2022-decomposed} for few-shot NER, which heavily rely on support sets and had \textbf{no} zero-shot capability, our method is more inspirational for future zero-shot NER works.


\section{Related Work}
\paragraph{Few-Shot NER}
Few-shot NER methods can be categorized into two types: prompt-based and metric-based.
Prompt-based methods focus on leveraging pre-trained language model knowledge for NER through prompt learning~\cite{cui-etal-2021-template,ma-etal-2022-template,huang-etal-2022-copner,lee-etal-2022-good}. 
They rely on templates, prompts, or good examples to utilize the pre-trained knowledge effectively.
Metric-based methods aim to learn a feature space with good generalizability and classify test samples using nearest class prototypes~\cite{snell_prototypical_2017,fritzler-2019,ji-etal-2022-shot,ma-etal-2022-decomposed} or neighbor samples~\cite{yang-katiyar-2020-simple,das-etal-2022-container}. 

There are also some efforts to improve few-shot NER by incorporating type name (label) semantics~\cite{hou-etal-2020-shot,ma2022label}. 
These methods usually treat labels as class representatives and align tokens with them, yet neglecting the joint training of entity words and label representations. 
Hence they can only use either support sets or labels as class references.
Instead, our method exploits support samples and type names simultaneously, which helps construct more accurate and stable prototypes in the target domain.

\paragraph{Task Decomposition and Contrastive Learning}
Recently, decomposed-based methods have emerged as effective solutions for the NER problem~\cite{shen-etal-2021-locate,wang2021learning-from-description,zhang2022exploring,wang-2022-enhanced,ma-etal-2022-decomposed}. 
These methods can learn entity boundary information well in data-limited scenarios and often get better results.
However, the widely used prototypical networks in these methods may encounter inaccurate and unstable prototypes given limited support samples at the type classification stage. 
Besides, they may face the problem of over-detected false spans produced at the span detection stage. 
Our method can address these two issues via the proposed type-aware contrastive learning and type-aware span filtering strategies.

Our method is also inspired by contrastive learning~\cite{pmlr-v119-chen20j,NEURIPS2020_d89a66c7_supervised_contrastive_learning}.
Due to its good generalization performance, two recent methods~\cite{das-etal-2022-container,huang-etal-2022-copner} borrow this idea for few-shot NER, which construct contrastive loss between tokens or between the token and the prompt.
However, they are both the end-to-end approach and thus have the inherent drawback that cannot learn good entity boundary information.
In contrast, our method is a decomposed one and our contrastive loss is constructed between tokens with additional type name information, which can find accurate boundary and learn a type-aware feature space.





\section{Conclusion}
In this paper, we propose a novel \ours framework  for few-shot NER, which handles the span detection and type classification sub-tasks at two stages.
For type classification, we present a type-aware contrastive learning  strategy to learn a type-aware and generalized feature space, enabling the model to construct more accurate and stable prototypes with the help of type names.
Based on it, we introduce a type-aware span filtering strategy for removing over-detected false spans produced at the span detection stage.
Extensive experiments demonstrate that our method achieves superior performance over previous SOTA methods, especially in the challenging scenarios.
In the future, we will try to extend \ours to other NLP tasks.





\section*{Limitations}
Our proposed \ours mainly focuses on the type classification stage of few-shot NER and simply adopt token classification for detecting entity spans.
There might be better solutions, e.g., using global boundary matrix. 
However, due to its high GPU memory requirements, we do not include it in our current framework. 
This drives us to find more efficient and powerful span detector for better few-shot NER performance in the future.

\section*{Ethics Statement}
Our work is entirely at the methodological level and therefore there will not be any negative social impacts. 
In addition, since the performance of the model is not yet at a practical level, it cannot be applied in certain high-risk scenarios (such as the I2B2 dataset used in our paper) yet, leaving room for further improvements in the future.

\section*{Acknowledgments}


This work was supported by the grant from the National Natural Science Foundation of China (NSFC) project (No. 62276193). It was also supported by the Joint Laboratory on Credit Science and Technology of CSCI-Wuhan University.

\bibliography{custom,few-shot-NER,dataset}
\bibliographystyle{acl_natbib}

\clearpage

\appendix
\section{Appendix}

\subsection{Target Domain Inference Algorithm}\label{app:algorithm_1}

Algorithm~\ref{algorithm_1} describes the process of domain adaptation using support set in the target domain and prediction on the query set.
Lines 1-7 describe the target domain adaptation process for the span detection stage.
Lines 8-14 describe the target domain adaptation process for the type classification stage.
Lines 15-19 describe the extraction of candidate entity spans in the query set using the fine-tuned span detector.
Lines 20-31 describe the candidate entity span filtering and entity type classification using type-aware prototypes.


\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}[t]
\begin{footnotesize}
\caption{Procedure of target domain inference in \ours .}
\begin{algorithmic}[1]\label{algorithm_1}
 \REQUIRE{Support set ; Query set ; Class set ; Encoders , ;}
 \ENSURE{Query set predictions }

 \STATE {;  (Any large positive value);}\\
 
 \STATE {;}
 
 \WHILE{}
     \STATE {;}\\
     \STATE {Compute loss  using Eq. (\ref{loss_span_detector});}\\
     \STATE {Update  to reduce ;}\\
     
 \ENDWHILE \\
 
 \STATE {;  (Any large positive value);}
 \STATE {;}
 \WHILE{}
     \STATE {;}
     \STATE {Compute loss  using Eq. (\ref{eq_entity_label_loss});}
     \STATE {Update parameters  to reduce ;}
 \ENDWHILE \\
 
\STATE {;}
\FOR{ in }
    \STATE {Extract candidate entity spans  from sentence  according to Section~\ref{early_stopping_span};}\\
    \STATE {;}\\
\ENDFOR

\STATE {Calculate threshold  for span filtering using Eq. (\ref{gamma_t});}\\
\STATE {Calculate all prototypes in  using Eq. (\ref{inferece_prototype});}\\
\STATE {The prototype of class  is denoted as };\\

{;}
\FOR{ in }
    \STATE {}
    \IF{}
        \STATE {Assign the label  to  using Eq. (\ref{eq_final_inference});}\\
        \STATE { =   \{\};}\\
    \ELSE
        \STATE {Remove this candidate span ;}\\
    \ENDIF
\ENDFOR

\end{algorithmic}
\end{footnotesize}
\end{algorithm}




\subsection{Statistics of Datasets}\label{appendix_datasets}
Table \ref{tab:dataset-statistic} shows statistics of various datasets used in our experiments.

\begin{table}[htb]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ccccc}
        \toprule
           \textbf{Dataset}  & \textbf{Domain} & \textbf{\# Classes}& \textbf{\# Sentences}  & \textbf{\# Entities} \\
           \cmidrule(r){1-1} \cmidrule(r){2-5} 
Few-NERD  & Wikipedia & 66& 188.2k  &491.7k\\
           I2B2'14 & Medical & 23& 140.8k  &29.2k\\
           CoNLL'03 & News & 4& 20.7k  &35.1k\\
           GUM & Wiki & 11& 3.5k  &6.1k\\
           WNUT'17 & Social  & 6 & 5.7k &3.9k\\
           OntoNotes & General & 18 & 76.7k &104.2k\\
         \bottomrule
        \end{tabular}
    }
    \caption{Dataset statistics}
    \vspace{-3mm}
    \label{tab:dataset-statistic}
\end{table}

\subsection{Details of Evaluation Methods}\label{appendix_eval_methods}



\paragraph{Episode-level Evaluation}
Following~\citet{ma-etal-2022-decomposed}, we adopt the episode-level evaluation method for the \textbf{Few-NERD} dataset. 
Each episode consists of a support set and a query set, both given in the n-way k-shot form. 
In each episode, the model trained in the source domain is tested on the query set by utilizing the support set. 
To make fair comparisons, we obtain the Micro F1 score with the episode-data processed by~\citet{ding2021few}. 
We report the mean F1 score with standard deviation using 3 different seeds.

\paragraph{Dataset-level Evaluation}
\citet{yang-katiyar-2020-simple} point that sampling test episodes may not reflect the real-world performance due to various data distributions, and they propose to sample support sets and then test the model in the original test set.
Each support set consists of  examples corresponding to each label.
The final Micro F1 scores and standard deviations are obtained using different sampled support sets.
Thus, following~\citet{yang-katiyar-2020-simple} and~\citet{das-etal-2022-container}, we also adopt this evaluation schema for \textbf{Domain Transfer} settings.
For fair comparisons, we use the support sets sampled by \citet{das-etal-2022-container}~\footnote{\url{https://github.com/psunlpgroup/CONTaiNER}.}.


\subsection{Baselines}\label{appendix_baselines}
{\textbf{ProtoBERT}~\cite{fritzler-2019}} adopts a token-level prototypical network, where the prototype of each class is obtained by averaging token samples of the same label, and the label of each unlabeled token in the query set is determined by its nearest class prototype.\\
{\textbf{NNShot}~\cite{yang-katiyar-2020-simple}} pre-trains BERT by traditional classification methods in the source domain training phase, and decides the class of each unlabeled token by the nearest neighbor at the token level in the target domain inference phase.\\
{\textbf{StructShot}~\cite{yang-katiyar-2020-simple}} is based on NNshot and uses an abstract transition probability for Viterbi decoding during testing.\\
{\textbf{ESD}~\cite{wang-2022-enhanced}} uses a span-level prototypical network, which designs multiple prototypes for \nonetoken-tokens and uses inter- and cross-span attention for better span representation.\\
{\textbf{FSLS}~\cite{ma2022label}} adopts two encoders, one for obtaining type names representations and the other for token representations.  
During the training procedure, the Euclidean distance between tokens and their corresponding type name semantics are minimized.
During prediction, the label for a token is determined based on the closest type name semantics.
We chose this baseline to demonstrate the superiority of our approach over existing approaches using the semantics of type names.\\
{\textbf{CONTaiNER}~\cite{das-etal-2022-container}} first trains BERT in the source domain using token-level contrastive learning loss function, then fine-tunes the trained model on the support set, and finally use the nearest neighbor method proposed in NNShot~\cite{yang-katiyar-2020-simple} for target domain inference phase.\\
{\textbf{DecomposedMetaNER}~\cite{ma-etal-2022-decomposed}} is a decomposed approach that incorporates model-agnostic meta-learning strategy into traditional prototypical network to learn a model-agnostic model and more fully exploits the support set.\\


\subsection{Implementation Details}\label{appendix_implementation_details}
Following previous methods~\cite{ding2021few,das-etal-2022-container,ma-etal-2022-decomposed}, we use {\texttt{bert-base-uncased}} model~\cite{devlin-etal-2019-bert} from HuggingFace~\cite{wolf-etal-2020-transformers}\footnote{\url{https://huggingface.co/bert-base-uncased}} as our encoder  and .

During the source domain training procedure, we use AdamW~\cite{loshchilov2018decoupled} as the optimizer with a learning rate of 3e-5 and 1\% linear warmup steps, and the batch size is set to 64. We set the temperature hyper-parameter  = 0.05 in Eq.(\ref{loss_type_contrastive_loss}) and keep dropout rate as 0.2 in the classification layer of the span detection.

As for the early stopping strategy in \ref{early_stopping_span}, we found that the fewer samples face a higher risk of over-fitting, and a lower  threshold is required. So we set  = 2 in all 1-shot settings and  = 6 in all other cases.
Table \ref{tab:hyper-parameters} shows the searching space of each hyper-parameter. Besides, we implement our framework with Pytorch 1.12\footnote{\url{https://pytorch.org/}} and train it with a V100-16G GPU.

\begin{table}[htb]
    \centering
    \small
    {
        \begin{tabular}{lc}
        \toprule
            Learning rate & \{1e-5, 3e-5, 1e-4\} \\
            Batch size & \{ 32, 64, 128\} \\ 
            Dropout rate & \{0.1, 0.2, 0.5\} \\
            temperature  & \{0.01, 0.05, 0.1\} \\
            Early stopping threshold  & \{1, 2, 4, 6, 8\} \\
         \bottomrule
        \end{tabular}
    }
    \caption{Hyper-parameters search space in our experiments.}
    \vspace{-3mm}
    \label{tab:hyper-parameters}
\end{table}










\subsection{Analysis of Tagging Schemes in the Span Detection Stage}\label{appendix_results_span_detection}


Table~\ref{tab:app_span_detection_1} and Table~\ref{tab:app_span_detection_2} show the span detection and overall performance under the Domain Transfer settings.
We observe that: 
1) The three tagging schemes have their own advantages and disadvantages. IO and BIO schemes can achieve higher recall, BIOES can achieve higher precision. 
2) The IO tagging scheme can achieve the best overall performance in most settings, except for the GUM dataset. 
Therefore, the IO scheme is selected for the span detection stage in this paper.	
3) The type-aware span filtering strategy proposed in this paper shown robust and positive effects across different tagging schemes.
Even when dealing with entity-dense datasets, where incorrect entity spans are minimal, this strategy does not significantly impair performance.
In future work, we can try to combine the advantages and disadvantages of different tagging schemes to further improve the performance of the span detection stage.





\subsection{Detailed Type Names}\label{appendix_type_names}

Tables~\ref{tab:dataset_labels_nlf_1} and~\ref{tab:dataset_labels_nlf_2} show the type names used in our \ours framework.
Tables~\ref{app:tab:variant_types_fewnerd} and~\ref{app:tab:variant_types_transfer} show the variant type names used in the analysis experiments on the impact of type names in Section~\ref{sec:impact_type_names}.

\clearpage


\begin{table*}[htb]
    \centering
    \small
    \setlength{\tabcolsep}{1.2mm}
    \resizebox{\textwidth}{!}
    {
        \begin{tabular}{ccccccccccccccc}
        \toprule
        \multirow{3}{*}{\textbf{Stage}} & \multirow{3}{*}{\textbf{Filtered}}             & \multirow{3}{*}{\textbf{Schema}} & \multicolumn{6}{c}{\textbf{I2B2}}                             & \multicolumn{6}{c}{\textbf{CoNLL}}                            \\
        \cmidrule(lr){4-9} \cmidrule(lr){10-15} 
                                        &                                                &                                  & \multicolumn{3}{c}{1-shot}    & \multicolumn{3}{c}{5-shot}    & \multicolumn{3}{c}{1-shot}    & \multicolumn{3}{c}{5-shot}    \\
        \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12} \cmidrule(lr){13-15} 
                                        &                                                &                                  & Precision & Recall  & F1      & Precision & Recall  & F1      & Precision & Recall  & F1      & Precision & Recall  & F1      \\
        \cmidrule(lr){1-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15} 
        \multirow{6}{*}{Span}           & \multirow{3}{*}{\textit{No}}  & IO                                                & 19.62   & 70.59 & 30.46 & 25.12   & 77.71 & 37.86 & 75.05   & 84.28 & 78.96 & 87.48   & 90.68 & 89.01 \\
                                        &                                                & BIO                              & 19.84   & 67.89 & 30.49 & 22.36   & 75.76 & 34.40 & 72.01   & 84.27 & 77.15 & 85.87   & 88.78 & 87.24 \\
                                        &                                                & BIOES                            & 19.71   & 60.46 & 29.47 & 23.89   & 70.19 & 35.53 & 70.38   & 80.93 & 74.89 & 84.02   & 87.77 & 85.72 \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15}
                                        & \multirow{3}{*}{\textit{Yes}} & IO                                                & 53.79   & 41.54 & 45.33 & 55.78   & 52.84 & 53.82 & 78.65   & 83.25 & 80.47 & 87.86   & 89.56 & 88.67 \\
                                        &                                                & BIO                              & 54.20   & 40.83 & 45.63 & 53.24   & 55.64 & 53.63 & 77.22   & 84.29 & 80.18 & 87.11   & 88.78 & 87.90 \\
                                        &                                                & BIOES                            & 52.77   & 34.04 & 39.80 & 57.46   & 50.97 & 53.32 & 74.39   & 80.72 & 77.00 & 84.65   & 87.65 & 86.00 \\
        \cmidrule(lr){1-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15} 
        \multirow{6}{*}{Span+Type}      & \multirow{3}{*}{\textit{No}}  & IO                                                & 14.14   & 47.18 & 21.57 & 17.83   & 51.11 & 26.35 & 65.37   & 72.73 & 68.47 & 79.06   & 81.32 & 80.14 \\
                                        &                                                & BIO                              & 14.92   & 49.32 & 22.74 & 16.65   & 54.40 & 25.40 & 63.66   & 74.08 & 68.01 & 77.88   & 80.27 & 79.00 \\
                                        &                                                & BIOES                            & 14.18   & 42.01 & 21.02 & 17.44   & 49.36 & 25.69 & 61.84   & 70.69 & 65.62 & 76.36   & 79.46 & 77.75 \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15}
                                        & \multirow{3}{*}{\textit{Yes}} & IO                                                & 47.24   & 35.77 & 39.32 & 46.92   & 44.33 & 45.20 & 68.89   & 72.70 & 70.38 & 79.81   & 81.31 & 80.53 \\
                                        &                                                & BIO                              & 47.83   & 35.42 & 39.87 & 45.18   & 47.00 & 45.39 & 67.98   & 74.07 & 70.52 & 78.75   & 80.26 & 79.47 \\
                                        &                                                & BIOES                            & 45.47   & 28.69 & 33.80 & 47.54   & 42.15 & 44.10 & 65.26   & 70.62 & 67.46 & 76.80   & 79.46 & 77.99 \\
        \bottomrule
        \end{tabular}
    }
    \caption{span detection.}
    \label{tab:app_span_detection_1}
\end{table*}

\begin{table*}[htb]
    \centering
    \small
    \setlength{\tabcolsep}{1.2mm}
    \resizebox{\textwidth}{!}
    {
        \begin{tabular}{ccccccccccccccc}
        \toprule
        \multirow{3}{*}{\textbf{Stage}} & \multirow{3}{*}{\textbf{Filtered}}             & \multirow{3}{*}{\textbf{Schema}} & \multicolumn{6}{c}{\textbf{WNUT}}                             & \multicolumn{6}{c}{\textbf{GUM}}                            \\
        \cmidrule(lr){4-9} \cmidrule(lr){10-15} 
                                        &                                                &                                  & \multicolumn{3}{c}{1-shot}    & \multicolumn{3}{c}{5-shot}    & \multicolumn{3}{c}{1-shot}    & \multicolumn{3}{c}{5-shot}    \\
        \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12} \cmidrule(lr){13-15} 
                                        &                                                &                                  & Precision & Recall  & F1      & Precision & Recall  & F1      & Precision & Recall  & F1      & Precision & Recall  & F1      \\
        \cmidrule(lr){1-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15} 
        \multirow{6}{*}{Span}           & \multirow{3}{*}{\textit{No}}  & IO                                                & 38.42              & 65.42              & 47.37              & 40.70              & 65.64              & 49.13              & 45.93              & 45.70              & 45.72              & 56.41              & 64.25              & 60.04              \\
                                        &                                                & BIO                              & 40.89              & 63.85              & 48.82              & 38.28              & 68.60              & 48.92              & 44.86              & 45.67              & 45.05              & 53.99              & 64.34              & 58.64              \\
                                        &                                                & BIOES                            & 42.67              & 56.14              & 47.30              & 41.90              & 65.24              & 50.59              & 54.28              & 48.32              & 50.97              & 60.57              & 64.07              & 62.22              \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15}
                                        & \multirow{3}{*}{\textit{Yes}} & IO                                                & 40.86              & 63.50              & 48.49              & 41.13              & 65.27              & 49.34              & 46.14              & 44.61              & 45.26              & 55.98              & 62.09              & 58.84              \\
                                        &                                                & BIO                              & 43.61              & 61.33              & 49.41              & 38.74              & 68.15              & 49.17              & 45.55              & 45.74              & 45.44              & 54.74              & 64.40              & 59.11              \\
                                        &                                                & BIOES                            & 45.78              & 54.39              & 48.14              & 42.45              & 65.06              & 50.92              & 54.58              & 47.92              & 50.88              & 60.88              & 63.56              & 62.15              \\
        \cmidrule(lr){1-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15} 
        \multirow{6}{*}{Span+Type}      & \multirow{3}{*}{\textit{No}}  & IO                                                & 25.86              & 43.19              & 31.62              & 28.59              & 45.35              & 34.26              & 24.64              & 23.90              & 24.21              & 33.39              & 37.02              & 35.09              \\
                                        &                                                & BIO                              & 27.34              & 42.11              & 32.52              & 25.69              & 45.83              & 32.77              & 24.97              & 25.24              & 24.99              & 33.14              & 39.04              & 35.81              \\
                                        &                                                & BIOES                            & 28.94              & 36.92              & 31.74              & 28.60              & 44.37              & 34.48              & 30.20              & 26.65              & 28.23              & 37.38              & 39.02              & 38.15              \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-9} \cmidrule(lr){10-15}
                                        & \multirow{3}{*}{\textit{Yes}} & IO                                                & 27.95              & 42.60              & 32.84              & 28.95              & 45.34              & 34.51              & 24.65              & 23.87              & 24.20              & 33.39              & 37.02              & 35.09              \\
                                        &                                                & BIO                              & 29.79              & 41.22              & 33.56              & 26.06              & 45.80              & 33.06              & 24.98              & 25.23              & 24.99              & 33.14              & 39.04              & 35.81              \\
                                        &                                                & BIOES                            & 31.72              & 36.31              & 32.85              & 28.95              & 44.37              & 34.72              & 30.21              & 26.64              & 28.22              & 37.38              & 39.02              & 38.15              \\
        \bottomrule
        \end{tabular}
    }
    \caption{span detection.}
    \label{tab:app_span_detection_2}
\end{table*}



\begin{table}[htb]
\small
\begin{center}
\resizebox{\columnwidth}{!}  {
\begin{tabular}{lcl}
\toprule
\bf Dataset & \makecell[c]{\textbf{Labels}} & \makecell[c]{\textbf{Type names}} \\

\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\multirow{66}{*}{\textbf{Few-NERD} } 
    &art-broadcastprogram &  broadcast program \\
    &art-film & film \\
    &art-music & music \\
    &art-other & other art\\
    &art-painting & painting \\
    &art-writtenart & written art \\
    &person-actor & actor \\
    &person-artist/author & artist author \\
    &person-athlete & athlete \\
    &person-director & director \\
    &person-other & other person \\
    &person-politician & politician \\
    &person-scholar & scholar \\
    &person-soldier &  soldier \\ 
    &product-airplane & airplane \\
    &product-car &  car \\
    &product-food &  food \\ 
    &product-game & game \\
    &product-other &  other product \\ 
    &product-ship & ship \\
    &product-software &  software \\
    &product-train &  train \\
    &product-weapon &  weapon \\
    &other-astronomything & astronomy thing \\
    &other-award &  award \\
    &other-biologything &  biology thing \\ 
    &other-chemicalthing & chemical thing \\ 
    &other-currency & currency \\
    &other-disease &  disease \\
    &other-educationaldegree &  educational degree \\ 
    &other-god & god \\ 
    &other-language & language \\
    &other-law &  law \\
    &other-livingthing & living thing \\
    &other-medical & medical \\

    &building-airport & airport \\
    &building-hospital & hospital \\
    &building-hotel & hotel \\
    &building-library & library \\
    &building-other & other building \\
    &building-restaurant & restaurant \\
    &building-sportsfacility & sports facility \\
    &building-theater & theater \\
    &\makecell[c]{event-attack/battle\\/war/militaryconflict} & \makecell[l]{attack battle \\war military conflict} \\
    &event-disaster & disaster \\
    &event-election & election \\
    &event-other & other event \\
    &event-protest & protest \\
    &event-sportsevent & sports event \\
    &location-bodiesofwater & bodies of water \\
    &location-GPE & \makecell[l]{geographical social \\political entity} \\
    &location-island & island \\
    &location-mountain & mountain \\
    &location-other & other location \\
    &location-park & park \\
    &\makecell[c]{location-road/railway\\/highway/transit} & \makecell[l]{road railway \\highway transit} \\
    &organization-company & company \\
    &organization-education & education \\
    &\makecell[c]{organization-government\\/governmentagency} & government agency \\
    &organization-media/newspaper & media newspaper \\
    &organization-other & other organization \\
    &organization-politicalparty & political party \\
    &organization-religion & religion \\
    &organization-showorganization & show organization \\
    &organization-sportsleague & sports league \\
    &organization-sportsteam & sports team \\



\bottomrule
\end{tabular}

}

\end{center}
\caption{Original labels and their corresponding natural-language-form type names of {Few-NERD}.}
\label{tab:dataset_labels_nlf_1}
\end{table}

\begin{table}[htb]
\small
\begin{center}
\resizebox{\columnwidth}{!} {
\begin{tabular}{lcl}
\toprule
\bf Dataset & \makecell[c]{\textbf{Labels}} & \makecell[c]{\textbf{Type names}} \\

\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\multirow{23}{*}{\textbf{I2B2'14} } 
& AGE & age \\
& BIOID & biometric ID \\
& CITY & city \\
& COUNTRY & country \\
& DATE & date \\
& DEVICE & device \\
& DOCTOR & doctor \\
& EMAIL & email \\
& FAX & fax \\
& HEALTHPLAN & health plan number \\
& HOSPITAL & hospital \\
& IDNUM & ID number \\
& LOCATION\_OTHER & location \\
& MEDICALRECORD & medical record \\
& ORGANIZATION & organization \\
& PATIENT & patient \\
& PHONE & phone number \\
& PROFESSION & profession \\
& STATE & state \\
& STREET & street \\
& URL & url \\
& USERNAME & username \\
& ZIP & zip code \\


\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\multirow{4}{*}{\textbf{CoNLL'03} } 
& PER & person \\
& LOC & location \\
& ORG & organization \\
& MISC & miscellaneous \\

\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\multirow{11}{*}{\textbf{GUM} } 
& abstract & abstract \\
& animal & animal \\
& event & event \\
& object & object \\
& organization & organization \\
& person & person \\
& place & place \\
& plant & plant \\
& quantity & quantity \\
& substance & substance \\
& time & time \\


\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\multirow{6}{*}{\textbf{WNUT'17} } 
& corporation & corporation \\
& creative-work & creative work \\
& group & group \\
& location & location \\
& person & person \\
& product & product \\


\cmidrule(lr){1-1} \cmidrule(lr){2-3}
\multirow{18}{*}{\textbf{Ontonotes} } 
& CARDINAL & cardinal \\
& DATE & date \\
& EVENT & event \\
& FAC & fac \\
& GPE & \makecell[l]{geographical social \\political entity} \\
& LANGUAGE & language \\
& LAW & law \\
& LOC & location \\
& MONEY & money \\
& NORP & nationality religion \\
& ORDINAL & ordinal \\
& ORG & organization \\
& PERCENT & percent \\
& PERSON & person \\
& PRODUCT & product \\
& QUANTITY & quantity \\
& TIME & time \\
& WORK\_OF\_ART & work of art \\


\bottomrule
\end{tabular}

}

\end{center}
\caption{Original labels and their corresponding natural-language-form type names of datasets under {Domain Transfer} settings.}
\label{tab:dataset_labels_nlf_2}
\end{table}



\begin{table*}[htb]
\small
\setlength{\tabcolsep}{2.6mm}
\begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{llll}
    \toprule
    \multicolumn{1}{l}{\textbf{Original Type Names}} & \multicolumn{1}{l}{\textbf{Synonym 1}} & \multicolumn{1}{l}{\textbf{Synonym 2}} & \multicolumn{1}{l}{\textbf{Synonym 3}} \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-4}
    broadcast program                                & television show                        & TV program                             & telecast                               \\
    film                                             & movie                                  & motion picture                         & cinema                                 \\
    music                                            & melody                                 & tunes                                  & songs                                  \\
    other art                                        & different art                          & alternative art                        & diverse art                            \\
    painting                                         & artwork                                & canvas                                 & picture                                \\
    written art                                      & literature                             & written work                           & prose                                  \\
    actor                                            & performer                              & thespian                               & artist                                 \\
    artist author                                    & creative writer                        & author                                 & wordsmith                              \\
    athlete                                          & sportsman/woman                        & player                                 & competitor                             \\
    director                                         & filmmaker                              & supervisor                             & manager                                \\
    other person                                     & someone else                           & another person                         & another individual                     \\
    politician                                       & statesman/woman                        & lawmaker                               & public servant                         \\
    scholar                                          & academic                               & intellectual                           & researcher                             \\
    soldier                                          & military personnel                     & serviceman/woman                       & trooper                                \\
    airplane                                         & aircraft                               & plane                                  & jet                                    \\
    car                                              & automobile                             & nourishment                            & fare                                   \\
    food                                             & cuisine                                & nourishment                            & fare                                   \\
    game                                             & sport                                  & competition                            & match                                  \\
    other product                                    & different product                      & alternative item                       & various commodity                      \\
    ship                                             & vessel                                 & boat                                   & craft                                  \\
    software                                         & program                                & application                            & computer program                       \\
    train                                            & locomotive                             & railway vehicle                        & railcar                                \\
    weapon                                           & armament                               & firearm                                & arm                                    \\
    astronomy thing                                  & celestial object                       & astronomical entity                    & heavenly body                          \\
    award                                            & accolade                               & prize                                  & recognition                            \\
    biology-thing                                    & biological entity                      & living organism                        & life form                              \\
    chemical thing                                   & chemical substance                     & compound                               & element                                \\
    currency                                         & money                                  & cash                                   & legal tender                           \\
    disease                                          & illness                                & sickness                               & disorder                               \\
    educational degree                               & academic qualification                 & diploma                                & certification                          \\
    god                                              & deity                                  & divine being                           & higher power                           \\
    language                                         & tongue                                 & speech                                 & communication                          \\
    law                                              & legislation                            & legal system                           & jurisprudence                          \\
    living thing                                     & organism                               & creature                               & being                                  \\
    medical                                          & healthcare                             & medicinal                              & therapeutic                            \\
bodies of water                                  & Waterways                              & aquatic features                       & lakes and rivers                       \\
    geographical social political entity             & Territory                              & region                                 & jurisdiction                           \\
    island                                           & Isle                                   & islet                                  & key                                    \\
    mountain                                         & Peak                                   & summit                                 & range                                  \\
    other location                                   & Site                                   & spot                                   & place                                  \\
    park                                             & Garden                                 & reserve                                & recreational area                      \\
    road railway highway transit                     & Route                                  & thoroughfare                           & transportation network                 \\
    company                                          & Corporation                            & firm                                   & enterprise                             \\
    education                                        & Learning                               & schooling                              & instruction                            \\
    government agency                                & Public body                            & administrative department              & authority                              \\
    media newspaper                                  & Press                                  & journalism                             & news organization                      \\
    other organization                               & Institution                            & establishment                          & association                            \\
    political party                                  & Political group                        & faction                                & party organization                     \\
    religion                                         & Faith                                  & belief system                          & spirituality                           \\
    show organization                                & Production company                     & entertainment group                    & performance troupe                     \\
    sports league                                    & Athletic association                   & sporting federation                    & league organization                    \\
    sports team                                      & Athletic club                          & competitive squad                      & sporting roster                        \\
    \bottomrule
    \end{tabular}
    }
\end{center}
\caption{Variant type names for Few-NERD Intra setting.}
\label{app:tab:variant_types_fewnerd}
\end{table*}



\begin{table*}[htb]
\small
\begin{center}
    {
    \begin{tabular}{llll}
    \toprule
    \multicolumn{1}{l}{\textbf{Original Type Names}} & \multicolumn{1}{l}{\textbf{Synonym 1}} & \multicolumn{1}{l}{\textbf{Synonym 2}} & \multicolumn{1}{l}{\textbf{Synonym 3}} \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-4}
    cardinal                                         & Primary                                & fundamental                            & principal                              \\
    date                                             & Day                                    & time                                   & appointment                            \\
    event                                            & Occasion                               & happening                              & function                               \\
    fac                                              & Facility                               & building                               & structure                              \\
    geographical social political entity             & Territory                              & region                                 & jurisdiction                           \\
    language                                         & Tongue                                 & speech                                 & communication                          \\
    law                                              & Regulation                             & rule                                   & statute                                \\
    location                                         & Place                                  & site                                   & spot                                   \\
    money                                            & Currency                               & funds                                  & finances                               \\
    nationality religion                             & Citizenship                            & faith                                  & belief system                          \\
    ordinal                                          & Sequential                             & numbered                               & ordered                                \\
    organization                                     & Institution                            & establishment                          & association                            \\
    percent                                          & Percentage                             & proportion                             & rate                                   \\
    person                                           & Individual                             & human                                  & character                              \\
    product                                          & Item                                   & good                                   & merchandise                            \\
    quantity                                         & Amount                                 & volume                                 & measure                                \\
    time                                             & Duration                               & period                                 & interval                               \\
    work of art                                      & Artwork                                & creation                               & masterpiece                            \\
    \midrule
    \midrule
    person                                           & Individual                             & human being                            & somebody                               \\
    location                                         & Place                                  & site                                   & spot                                   \\
    organization                                     & Institution                            & establishment                          & company                                \\
    miscellaneous                                    & Various                                & diverse                                & mixed                                  \\
    \bottomrule
    \end{tabular}
    }
\end{center}
\caption{Variant type names for Domain Transfer setting. Here we show the type names of the OntoNotes dataset and the CoNLL2003 dataset.}
\label{app:tab:variant_types_transfer}
\end{table*}

































\end{document}
