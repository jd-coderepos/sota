\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stfloats}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{ Dance with Flow: \\Two in One Stream for Action Detection }

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle




\begin{abstract}
   
\end{abstract}


\begin{figure*}[ht]
\centering
\includegraphics[scale=0.62]{CVPR2019/F1.pdf}
\caption{(a) If only considering single-frame (frame-level), it is difficult to distinguish the dancer is standing up or sitting down. With flow information, a "momentum" is given to the dancer, which helps to recognize the action. (b) Multiple-frame (tubelet-level) network has less of the issue in (a). However, it still suffers from another pitfall that the model may focus on background more than on actions. Cliff diving, soccer juggling and skateboarding are respectively wrongly recognized as surfing, tennis swing and skiing as the similar backgrounds using the two-stream method in ~\cite{Singh2017}. Applying flow information to appearance images is favourable for the model to pay more attention on moving objects and weaken the weights of backgrounds.}
\label{Fig:motivation}
\end{figure*}


\section{Introduction}

The merits of our method are three-fold as follows:
\begin{itemize}
    \item We propose to embed RGB and optical-flow into a single stream for spatio-temporal action detection. It reduces the computational costs of conventional two-stream networks by half while maintaining its high performance.
    \item We introduce the Motion-Condition Network with motion modulation layers, which learns video representations of appearance-stream features conditioned on motion information. 
    \item The method is easily embedded in the state-of-the-art appearance-stream or two-stream networks, and trained end-to-end together with the existing network, leading to new state-of-the-art on UCF101, UCFSports and JHMDB. 
\end{itemize}

The code of our method is available at .......





\section{Related Work}

\subsection{Two-stream Network}

\subsection{Feature Transformation}
\section{Motivation}
Appearance-stream learns semantic information including the objects and the contexts but without motion clues. Flow-stream understands the movements from the motion images which are very sparse and noisy. Lacking of semantic information leads to poor detection. Currently, two-stream pipeline for spatio-temporal action detection is most popular as it takes advantages of appearance-stream and flow-stream, which obviously boosts the detection results. However, it also holds up the disadvantages of each single stream due to that the two streams are usually separately trained and then simply fused. Moreover, the improved performance gains at the cost of double computation and parameters. Based on these analysis, we propose to leverage two domain (RGB and optical-flow) information in one stream for a light and efficient model. We make an effort to address two issues as below.

{\bf{Frame-level Issue}} Most of the state-of-the-art action location methods are based on linking the frame-level detection. Considering the appearance-stream, it is not optimal to independently process single frame which does not reveal continuous information ~\cite{Kalogeithon2017}. In Figure~\ref{Fig:motivation} (a), one cannot recognize the dancer in the current frame is standing up or sitting down without seeing the future frames. Utilizing additional information from flow images, the dancer is given a moving direction, up $\uparrow$ or down $\downarrow$, which indicate what the action is. Or like in ~\cite{Kalogeithon2017}, taking multiple frames as input and detecting a tubelet with temporal information is also able to surpass the limitation. 

{\bf{Background-dominant Issue}} However, the tubelet-level detector still surfers from another issue that the model may focus more on background than on actions. We show some results of UCF101 dataset generated from the two-stream method in ~\cite{Singh2017} in Figure~\ref{Fig:motivation} (b). When the actor performing cliff diving reaches the surface of water, the action is mistaken as surfing due to the sea circumstance. Performing soccer juggling on a tennis court is recognized as tennis swing. And skateboarding on snow is misunderstood as skiing. In order to decrease the influence of background, flow-images are taken as ``attention maps" to the appearance-stream, which impels the model to learn more from the moving objects.

Driven by the three points above, we propose a simple but effective and efficient idea. We learn an appearance-stream with flow-images as condition. With this kind of motion condition, even only given a single frame, the model is suppose to see the near future of the `dance'. Furthermore, the motion condition will guide the model to pay much attention on related objects rather than background. Mediating RGB and optical flow information in one network is capable to promote the virtues and avoid the shortages compared with training two stream independently. Moreover, motion condition is flexible to leveraged to state-of-the-art appearance networks.     

\section{Methodology}
Inspired by Conditional Normalization which utilizes a learned function of some conditions for feature-wise affine transformation in Batch Normalization~\cite{Wang2018, Ioffe2015, Perez2017}, we design a motion-modulation layer conditioned on flow images to alter the behavior of an appearance-stream network for the purposes we mentioned in Motivation Section. 

\subsection{Motion Condition-Modulation Network}
Specifically, considering an appearance-stream network $\mathcal{D}^{rgb}_{\theta}$ trained on frame-level for spatio-temporal action detection, it can be formulated as:
\begin{equation}
(L^{rgb}, S^{rgb}) = \mathcal{D}^{rgb}_{\theta}(I^{rgb})
\end{equation}
where $I^{rgb}\in\mathbb{R}^{H \times W \times 3}$ is a single RGB frame of $H$ height and $W$ width input to the network $\mathcal{D}^{rgb}_{\theta}$. $L^{rgb}\in\mathbb{R}^{Q \times 4}$ and $S^{rgb}\in\mathbb{R}^{Q \times (P+1)}$ are $Q$ boxes' locations and corresponding boxes' classification scores for $P$ action classes and background. $\theta$ represents parameters of the learned network. Similarly, we define a flow-stream network on frame-level for spatio-temporal action detection as:
\begin{equation}
(L^{of}, S^{of}) = \mathcal{D}^{of}_{\theta}(I^{of})
\end{equation}
$I^{of}\in\mathbb{R}^{H \times W \times 2}$ is a single optical flow image with $x$ and $y$ components of the velocity respectively in two channels. The conventional two-stream method includes training the two networks $\mathcal{D}^{rgb}_{\theta}$ and $\mathcal{D}^{of}_{\theta}$ independently, and fusing the results $(L^{rgb}, S^{rgb})$ and  $(L^{of}, S^{of})$. We already illustrated the shortcomings of it in last section. 

In our method, $I^{of}$ is regarded as a motion map with the same resolution of corresponding RGB image $I^{rgb}$. The pixel value equaling 0 means there is no movement, others indicating the amplitude and direction of the movement. We take $I^{of}$ as prior information $\Psi$ when applying an appearance-stream network $\mathcal{D}^{rgb}_{\theta}$ to estimate where and what actions may occur. Then we formulate our Motion Condition-Modulation (Mcm) Network as:
\begin{equation}
\Psi = \mathcal{MC}(I^{of}) = \mathcal{MC}((I^{of_x},I^{of_y}))
\end{equation}
\begin{equation}
\begin{aligned}
(L^{\mathrm{Mcm}}, S^{\mathrm{Mcm}}) &= \mathcal{D}^{\mathrm{Mcm}}_{\theta}(I^{rgb}|\Psi)\\
&=\mathcal{D}^{\mathrm{Mcm}}_{\theta}(I^{rgb}|\mathcal{MC}(I^{of}))
\end{aligned}
\end{equation}
$\mathcal{MC}(\bullet)$ is a mapping function to generate simple features from the flow images. In next part, we will introduce how we use the flow condition for guiding an appearance-stream network to learn better video representation for action detection.

\subsection{Motion Modulation Layer}
We develop a Motion-Modulation ($\mathrm{M}^2$) layer to modify the features learned from RGB images. A $\mathrm{M}^2$ layer is able to influence the appearance network by incorporating temporal information and weighting the action area. Referring to ~\cite{Wang2018}, we firstly learn a pair of affine transformation parameters ($\beta, \gamma$) from the prior flow condition $\Psi$ by a function $\mathcal{F}: \Psi\longmapsto(\beta, \gamma)$ . Concretely, the Motion Condition-Modulation Network is further expressed as:
\begin{equation}
\begin{aligned}
(\beta, \gamma) &= \mathcal{F}(\Psi),\\
(L^{\mathrm{Mcm}}, S^{\mathrm{Mcm}}) &= \mathcal{D}^{\mathrm{Mcm}}_{\theta}(I^{rgb}|\beta, \gamma)   
\end{aligned}
\end{equation}
In order to modulate the appearance network, we then apply a transform function $\mathcal{M}^2(\bullet)$  with the learned transformation parameters ($\beta, \gamma$) to the RGB features $F^{rgb}$.
\begin{equation}
\mathcal{M}^2(F^{rgb}) = \beta\odot{F^{rgb}}+\gamma
\end{equation}
$\odot$ is an element-wise multiplication operation. The RGB feature maps $F^{rgb}$ has the same dimensions with parameters $\beta$ and $\gamma$. So the flow information represented by ($\beta, \gamma$) influences the appearance network by both feature-wise and spatial-wise manipulations. The specific network with motion condition layer and modulation layer is shown in Figure~\ref{Fig:network}.

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.62]{CVPR2019/F2.pdf}
\caption{The structure of Motion Condition-Modulation Network based on SSD (Mcm-SSD). Motion Condition Layer maps flow images to prior condition information. The condition inputs to Motion Modulation Layer for generating transformation parameters which are used to modulate RGB features. Motion Modulation Layer is able to applied to any low-level convolutional features. }
\label{Fig:network}
\end{figure*}



\subsection{Network Training and Test}
{\bf{Network Architecture}} The Motion Condition Layer and Motion Modulation Layer are capable to embedded in existing appearance-stream network. In this paper, we adopt a real-time Single Shot Multibox Detector (SSD)~\cite{Singh2017, Liu2016} network as the backbone. We simply call it Mcm-SSD. Due to sparsity of flow images, we adopt simple convolutional layers to extract low-level motion condition information. Three 1x1 convolutional layers attempt to keep the spatial pixel-wise moveability. The forth layer with 3x3 kernel considers some movement context. The motion condition then inputs to a Motion Modulation ($\mathrm{M}^2$) layer in which it is separately mapped to a pair of transformation parameters $\beta$ and $\gamma$. Two groups of 1x1 convolutional layers are independently adopted for generating each of the parameters. The low-level RGB features from the appearance network are adjusted by the parameters. Motion Modulation Layer is capable to introduced to any bottom layers including conv1, conv2, conv3 and conv4 of appearance network. All of them share the Motion Condition Layer. The whole network is able to train in end-to-end fashion.

{\bf{Training Loss}} In order to demonstrate the generalization and flexibility of the proposed method, we embed Motion Condition layer and Motion Modulation layer in two start-of-the-art network, SSD based on frame-level and SSD based on tubelet-level, both of which are trained on RGB images. The basic loss function used in the paper is derived from SSD ~\cite{Liu2016} for object detection. Defining ${x^p}_{ij}=\{1,0\}$ as an indicator for matching the $i$-th default box to the $j$-th ground truth box of action category $p$. The overall loss function contains the localization ($loc$) loss and the confidence ($conf$) loss:
\begin{equation}
\mathcal{L}(x,c,l,g) = \frac{1}{N}(\mathcal{L}_{loc}(x,l,g)+\mathcal{L}_{conf}(x,c))
\end{equation}

with $N$ is the number of matched default boxes. $c$ represents multiple classes confidences. $l$ and $g$ are the predicted box and the ground truth box.

The confidence loss applies the softmax loss as below:
\begin{equation}
\begin{aligned}
\mathcal{L}_{conf}(x,c) &= -\sum_{i\in{Pos}}^{N}x_{ij}^{p}log(\hat{c}_{i}^{p})-\sum_{i\in{Neg}}log(\hat{c}_{i}^{0}) \\
\hat{c}_{i}^{p} &=\frac{exp(c_{i}^{p})}{\sum_{p}exp(c_{i}^{p})}
\end{aligned}
\end{equation}
The localization loss applies a Smooth L1 loss~\cite{Girshick2015} between the predicted box and the ground truth box. The network regresses to offsets for the center ($cx,cy$) of the default box($d$) and for its width ($w$) and height ($h$).
\begin{equation}
\begin{aligned}
\mathcal{L}_{loc}(x,l,g) &=\sum_{i\in{Pos}}^{N}\sum_{m\in\{cx,cy,w,h\}}x_{ij}^{k}smooth_{\mathrm{L}1}(l_{i}^{m}-\hat{g}_{j}^{m}) \\
\hat{g}_{j}^{cx} &= (g_{j}^{cx}-d_{i}^{cx})/d_{i}^{w} \qquad
\hat{g}_{j}^{cy} = (g_{j}^{cy}-d_{i}^{cy})/d_{i}^{h} \\
\hat{g}_{j}^{w} &= log (\frac{{g}_{j}^{w}}{d_{i}^{w}}) \qquad \hat{g}_{j}^{h} = log (\frac{{g}_{j}^{h}}{d_{i}^{h}})
\end{aligned}
\end{equation}
For the tubelet-level SSD, we follow ~\cite{Kalogeithon2017} to train the network.

{\bf{Linking}} Once the frame-level detections or tubelet detections are achieved, we should link them to build action tubes. We adopt the linking method described in ~\cite{Singh2017} for frame-level detections and method in ~\cite{Kalogeithon2017} for tubelet detections. Traditionally, two-stream method performs based on average scores of appearance-stream and flow-stream. Using Motion Condition-Modulation Network, we generate the tubes only from one stream, which reduces the computation cost and parameters by half.  

\subsection{Network Analysis}
We claim that Motion Condition Layer and Motion Modulation Layer are beneficial to generate better video representations for spatio-temporal action detection. Specifically, how do the layers make a difference to the appearance network? To understand this point, we disentangle the proposed Mcm-SSD network and visualize the feature maps and results. 

We firstly show the generated feature maps from appearance network before and after modulated by motion condition in Figure ~\ref{Fig:maps}. 

Furthermore, in Figure~\ref{Fig:heatmap}, we directly compare the detection results of appearance-SSD network and Mcm-SSD network. Also, we visualize the gradient-weighted class activation heatmaps on image~\cite{Zhou2015,Utk} for better understanding how the motion condition influences the behavior of the appearance network. We choose a very challenging case of cliff diving here. The image resolution is much low and the actor is quite tiny. The mussy background obviously increases the difficulty to detect actions. We use red dot-line boxes to label the actor just for that one is able to see where the action happening. First row shows that appearance-SSD network fails to detect any actions. From the corresponding heatmaps, it is apparent that the appearance network pays more attention on the background than on the actions. There are weak responses on the action positions. From heatmaps for Mcm-SSD network in the last row, we clearly see that Mcm-SSD network is capable to balance the activation on actions and background. The response on action positions are strengthened. Thus, as expected, Mcm-SSD performs better than appearance-SSD. It outputs correct detections of cliff diving for all the frames (seen in the third row ).     

\begin{figure*}[ht]
\centering
\includegraphics[scale=0.62]{CVPR2019/F3.pdf}
\caption{Visualization of the RGB feature maps from conv2 layer of appearance SSD network in the first row; Motion condition maps from MC Layer in the second rwo; Modulated features from conv2 layer of Mcm-SSD network in the last row. }
\label{Fig:maps}
\end{figure*}
\begin{figure*}[ht]
\centering
\includegraphics[scale=0.62]{CVPR2019/F4.pdf}
\caption{Visualization of detection results and heatmaps on conv4 layers from Appearance-SSD network (first two rows) and Mcm-SSD network (last two rows). }
\label{Fig:heatmap}
\end{figure*}

\section{Experiments}
 
\subsection{Datasets and Evaluation Metrics}
{\bf{Datasets}} The {\it{UCF101-24}} ~\cite{Soomro2012} is a subset of UCF101. It contains 24 sport classes in 3207 untrimmed videos. Each video only contains a single action category. Multiple action instances with the same class but different spatial and temporal boundaries may occur. We only report the results on first split following ~\cite{}. 

The {\it{UCF-Sports}} dataset ~\cite{Rodrigue2008} contains 10 sport classes in 150 trimmed videos. We follow ~\cite{} to divide the training and test splits. 

The {\it{J-HMDB}} dataset contains 21 action categories such as brush hair and throw in 928 trimmed videos. We report the average results on three splits.

{\bf{Metrics}} Following ~\cite{}, we utilize video mean Average Precision ($mAP$) for evaluating the performance. We calculate an average across time of per-frame Intersection-over-Union (IoU) between tubes. A detection is correct if its IoU with the ground truth tube is greater than 0.5 and its action label is correctly assigned. We compute the average precision ($AP$) for each class and report the average over all classes ($mAP$).  

\subsection{Implementation Details}
We insert the developed motion layers to two state-of-the-art appearance network, SSD based on single frame and SSD based on multiple frames. We use VGG-16 pre-trained weights on ImageNet dataset as model initialization. The input size is 300x300 for both of them. We use 6 continuous frames input to the tubelet-level SSD. The initial learning rate is set to 0.0001 for both network on all three datasets and will be changed by applying step decay strategy. All the ablation study are performed using frame-level SSD on {\it{UCF101-24}}. We execute all the experiments on  one NVIDIA GTX 1080 GPU.

\subsection{Ablation Study}

\subsection{Performance on Different Datasets}

\subsection{Compare with the State-of-the-art}
           ~\cite{Weinzaepfel2015}  & R-CNN & & & $\surd$ & 46.80 & -- & -- & -- & -- & -- & -- & --  \\
~\cite{Saha16} & Faster-RCNN & & & $\surd$ & 66.70 & 35.90 & 7.90 & 14.40 & -- & -- & 43.30 & 40.00  \\
            \specialrule{0em}{0pt}{0pt}
           ~\cite{Peng16} & Faster-RCNN & & $\surd$ & $\surd$ & 71.80 & 35.90 & 1.60 & 8.80 & 47.30 & 51.00 & 48.20 & 42.20  \\
            \specialrule{0em}{0pt}{0pt}
           ~\cite{Singh18}  & SSD & & & $\surd$ & 71.53 & 40.07 & 13.91 & 17.90 & -- & -- & -- & --  \\
            \specialrule{0em}{0pt}{0pt}
           ~\cite{Singh17} & SSD & & & $\surd$ & 73.50 & 46.30 & 15.00 & 20.40 & -- & -- & 44.5 & 41.6  \\
           	\specialrule{0em}{0pt}{0pt}
			
           \hline
            {\bf{TiO}} & SSD &  &  &  & {\bf{75.13}} & {\bf{47.47}} & {\bf{17.21}} & {\bf{21.51}} & {\bf{57.81}} & {\bf{51.69}} & 47.23 & 38.72  \\
            \specialrule{0em}{0pt}{0pt}
            \hline
            \tabincell{l}{{\bf{TiO-}}\\{\bf{TowStream}}} & SSD &  &  & $\surd$ & \bf{77.49} & \bf{49.54} & \bf{17.62} & \bf{22.02} & \bf{62.67} & \bf{52.32} & \bf{52.00} & \bf{43.20}  \\
            \specialrule{0.1em}{0pt}{0pt}
			\hline\hline
			~\cite{Hou17} & R-CNN 3D & $\surd$ &  & & 47.10 & -- & -- & -- & -- & -- & -- & --  \\
             \specialrule{0em}{0pt}{0pt}
             ~\cite{He} & Faster-RCNN + LSTM & $\surd$ &  & $\surd$ & 71.69 & -- & -- & -- & -- & -- & -- & --  \\
			\specialrule{0em}{0pt}{0pt}
			~\cite{Yang} & Faster-RCNN & $\surd$ & &  & 73.54 & 37.80 & -- & -- & -- & -- & -- & --  \\
			\specialrule{0em}{0pt}{0pt}
			
            ~\cite{Li}Vgg16 & Faster-RCNN + LSTM & $\surd$ &  & $\surd$ & 76.30 & -- & -- & -- & -- & -- & -- & --  \\
			\specialrule{0em}{0pt}{0pt}
			~\cite{Li}ResNet101 & Faster-RCNN + LSTM & $\surd$ &  & $\surd$ & \bf{77.90} & -- & -- & -- & -- & -- & -- & --  \\
			\specialrule{0em}{0pt}{0pt}
			~\cite{Kalogeiton2017}  & SSD & $\surd$ & $\surd$ & $\surd$ & 76.50 & 49.20 & 19.70 & 23.40 & 78.40 & 58.80 & 52.10 & 44.80  \\
			\specialrule{0em}{0pt}{0pt}
			\hline
			{\bf{TiO-Multi}} & SSD & $\surd$ &  &  & 74.66 & 47.70 & \bf{20.86} & \bf{24.81} & \bf{83.60} & \bf{59.60} &  &   \\
			\specialrule{0.2em}{0pt}{0pt}
			







\section{Conclusion}



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
