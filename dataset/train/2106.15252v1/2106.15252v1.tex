\section{Experiments}\label{sec:exp}

\subsection{Data and implementation details}\label{sec:exp:imp}

We evaluate AutoNovel on a variety of standard benchmark datasets:
CIFAR10~\cite{Krizhevsky09cifar}, CIFAR100~\cite{Krizhevsky09cifar}, SVHN~\cite{Netzer2011svhn}, OmniGlot~\cite{Lake15omnniglot}, and ImageNet~\cite{deng09imagnet}.  Following~\cite{han2019learning}, we split these to have
 5/20/5/654/30 classes respectively in the unlabelled set. The splits are summarized in~\cref{tab:datasplit}.
In addition, for OmniGlot and ImageNet we use 20 and 3 different splits respectively, as in~\cite{han2019learning}, and report average clustering accuracy (as defined in~\cref{e:acc}) on the unlabelled data.
\rev{While we follow standard practice to split the datasets we note here that most of the time, the number of unlabelled classes is under a hundred. This is a potential limitation of clustering which still proves to be very difficult for classification over thousands of categories \cite{vangansbeke2020scan}.}

\begin{table}[ht]
\centering
\footnotesize
\caption{Data splits in the experiments.}\label{tab:datasplit}
\begin{tabular}{lcc}
\toprule
& labelled classes & unlabelled classes \\
\midrule
CIFAR10 & 5 & 5 \\
CIFAR100 & 80 &  20 \\
SVHN & 5  & 5 \\
OmniGlot & 964  & 659 \\
ImageNet & 882 & 118 \\
\bottomrule
\end{tabular}
\end{table}




We use the ResNet-18~\cite{he2016deep} architecture, except for OmniGlot for which we use a VGG-like network~\cite{simonyan15vgg} with six layers to make our setting directly comparable to prior work.
We use SGD with momentum~\cite{sutskever2013importance} as the optimizer for all but the OmniGlot dataset, for which we use Adam~\cite{kingma2014adam}.
For all experiments we use a batch size of 128 and $k=5$ which we found work consistently well across datasets.

In the first self-supervised training step, \rev{unless} otherwise mentioned, we train our model with the pretext task of rotation predictions (i.e., a four-class classification: $0^{\circ}$, $90^{\circ}$, $180^{\circ}$, and $270^{\circ}$) for 200 epochs and a step-wise decaying learning rate starting from 0.1 and divided by 5 at epochs 60, 120, and 160.

In the second step of our framework (i.e., supervised training using labelled data), we fine-tune our model on the labelled set for 100 epochs and a step-wise decaying learning rate starting from 0.1 and halved every 10 epochs.
From this step onward we fix the first three convolutional blocks of the model, and fine-tune the last convolutional block together with the linear classifier.

Finally, in the last joint training step, we fine-tune our model for 200/100/90 epochs for \{CIFAR10, CIFAR100, SVHN\}/OmniGlot/ImageNet, which is randomly sampled from the merged set of both labelled and unlabelled data.
The initial learning rate is set to 0.1 for all datasets, and is decayed with a factor of 10 at the 170th/\{30th, 60th\} epoch for \{CIFAR10, CIFAR100, SVHN\}/ImageNet.
The learning rate of 0.01 is kept fixed for OmniGlot.
For the consistency regularization term, we use the ramp-up function as described in~\cref{s:consistency} with $\lambda = \{5.0, 50.0, 50.0, 100.0, 10.0\}$, and $T = \{50, 150, 80, 1, 50\}$ for CIFAR10, CIFAR100, SVHN, OmniGlot, and ImageNet respectively.

In the incremental learning setting, all previous hyper parameters remain the same for our method.
We only add a ramp-up on the cross entropy loss on unlabelled data.
The ramp-up length is the same as the one used for eq.~(4) and we use for all experiments a coefficient of 0.05.
For all other methods we train the classifier for 150 epochs with SGD with momentum and learning rate of 0.1 divided by 10 at epoch 50.

\rev{For hyper-parameter tuning, we create a probe validation set from the labelled data by dropping the labels of a few classes. We then tune the hyper-parameters based on the ACC on this probe validation set. We construct the probe validation set to have the same number of classes as the actual unlabelled set. For example, for CIFAR100, we split the 80 labelled classes into a 60-class labelled subset and a 20-class probe validation set. We then tune the hyper-parameters based on the novel category discovery performance on the probe validation set. For CIFAR10 and SVHN, due to the small number of labelled classes, we only take 2 classes from the labelled data to construct the probe validation set.}

We implement our method using PyTorch 1.1.0 and run experiments on NVIDIA Tesla M40 GPUs. Following \cite{han2019learning}, our results are averaged over 10 runs for all datasets, except ImageNet, for which the results are averaged over the three 30-class subsets.  In general, we found the results are stable. Our code is publicly available at \url{http://www.robots.ox.ac.uk/~vgg/research/auto_novel}.

\begin{table}[t]
\centering
\caption{Ablation study of AutoNovel. ``MSE'' means MSE consistency constraint; ``CE'' means cross entropy loss for training on labeled data; ``BCE'' means binary cross entropy loss for training on unlabeled data; ``S.S.'' means self-supervision; \rev{``I.L.'' means incremental learning.} \rev{The evaluation metric is the ACC.}}\label{tab:ablation_unlabelled}
\begin{tabular}[c]{lccc}
\toprule
          & CIFAR-10     & CIFAR-100     & SVHN \\
\midrule
Ours w/o MSE & 82.6$\pm$12.0\% & 61.8$\pm$3.6\% & 61.3$\pm$1.9\%   \\
Ours w/o CE & 84.7$\pm$4.4\%  & 58.4$\pm$2.7\% & 59.7$\pm$6.6\%  \\
Ours w/o BCE & 26.2$\pm$2.0\%  & 6.6$\pm$0.7\% & 24.5$\pm$0.5\% \\
Ours w/o S.S.& 89.4$\pm$1.4\%  & 67.4$\pm$2.0\% & 72.9$\pm$5.0\% \\\midrule
Ours full   & \textbf{90.4$\pm$0.5}\% & \textbf{73.2$\pm$2.1}\%  & \textbf{95.0$\pm$0.2}\% \\
Ours w/ I.L.  & \textbf{91.7$\pm$0.9}\% & \textbf{75.2$\pm$4.2}\% & \textbf{95.2$\pm$0.3}\% \\
\bottomrule
\end{tabular}\hfill 

\end{table} 
\subsection{Ablation study}

We validate the effectiveness of the components of AutoNovel by ablating them and measuring the resulting ACC on the unlabelled data.
Note that, since the evaluation is restricted to the unlabelled data, we are solving a clustering problem. The same unlabelled data points are used for both training and testing, except that data augmentation (i.e.~image transformations) is not applied when computing the cluster assignments.
As can be seen in~\cref{tab:ablation_unlabelled}, all components have a significant effect as removing any of them causes the performance to drop substantially.
Among them, the BCE loss is by far the most important one, since removing it results in a dramatic drop of 40--60\% absolute ACC points.
For example, the full method has ACC $90.4\%$ on CIFAR10, while removing BCE causes the ACC to drop to $26.2\%$.
This shows that that our rank-based embedding comparison can indeed generate reliable pairwise pseudo labels for the BCE loss.
Without consistency, cross entropy, or self-supervision, the performance drops by a more modest but still significant $7.8\%$, $5.7\%$ and $1.0\%$ absolute ACC points, respectively, for CIFAR10.
It means that the consistency term plays a role as important as the cross-entropy term by preventing the ``moving target'' phenomenon described in~\cref{s:consistency}.
Finally, by incorporating the discovered classes in the classification task, we get a further boost of 1.3\%, 2.0\% and 0.2\% points on CIFAR10, CIFAR100 and SVHN respectively.

We also evaluate the evolution of performances of our method with respect to $k$ for ranking statistics. The results on SVHN/CIFAR10/CIFAR100 are shown in \cref{fig:k}. We found that $k=\{5,7\}$ give the best results overall. We also found that for all values of $k$ except 1 results are in general stable.
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/k.pdf}
    \caption{Performance evolution w.r.t. $k$ for ranking statistics. We report results for $k=\{1,2,3,5,7,10,15,20,50\}$.}
    \label{fig:k}
\end{figure}

\subsection{Novel category discovery}

\begin{table}[tb]
 \footnotesize
  \caption{Novel category discovery results on CIFAR10, CIFAR100, and SVHN. ACC on the unlabelled set.
``S.S.'' means self-supervision; \rev{``I.L.'' means incremental learning.}}\label{tab:comparison}
  \centering
  \resizebox{0.48\textwidth}{!}{
  \begin{tabular}{clccc}
    \toprule
    No   &                                      & CIFAR10        & CIFAR100       & SVHN \\
    \midrule
    (1)  & $k$-means~\cite{MackQueen67_Kmeans} & 65.5$\pm$0.0 \%          & 56.6$\pm$1.6\%          & 42.6\%$\pm$0.0\\
    (2)  & KCL~\cite{Hsu18_L2C}                & 66.5$\pm$3.9\%          & 14.3$\pm$1.3\%          & 21.4\%$\pm$0.6 \\
    (3)  & MCL~\cite{Hsu19_MCL}                & 64.2$\pm$0.1\%          & 21.3$\pm$3.4\%          & 38.6\%$\pm$10.8 \\
    (4)  & DTC~\cite{han2019learning}          & 87.5$\pm$0.3\%          & 56.7$\pm$1.2\%          & 60.9\%$\pm$1.6\\
    \midrule
    (5)  & $k$-means~\cite{MackQueen67_Kmeans} w/ S.S.
         & 72.5$\pm$0.0\%                               & 56.3$\pm$1.7\%          & 46.7$\pm$0.0\% \\
    (6)  & KCL~\cite{Hsu18_L2C} w/ S.S.
         & 72.3$\pm$0.2\%                               & 42.1$\pm$1.8\%          & 65.6$\pm$4.9\%\\
    (7)  & MCL~\cite{Hsu19_MCL} w/ S.S.
         & 70.9$\pm$0.1\%                               & 21.5$\pm$2.3\%          & 53.1$\pm$0.3\% \\
    (8)  & DTC~\cite{han2019learning} w/ S.S.
         &88.7$\pm$0.3\%  & 67.3$\pm$1.2\%      & 75.7$\pm$0.4\% \\
\midrule
    (9)  & Ours                                 & \textbf{90.4$\pm$0.5}\% & \textbf{73.2$\pm$2.1}\% & \textbf{95.0$\pm$0.2}\% \\
    (10) & Ours w/ I.L.
         & \textbf{91.7$\pm$0.9}\%                      & \textbf{75.2$\pm$4.2}\% & \textbf{95.2$\pm$0.2}\% \\
    \bottomrule
  \end{tabular}
}
\end{table}

We compare AutoNovel to baselines and state-of-the-art methods for new class discovery, starting from CIFAR10, CIFAR100, and SVHN in~\cref{tab:comparison}.
The first baseline (row 5 in~\cref{tab:comparison}) amounts to applying $k$-means~\cite{MackQueen67_Kmeans} to the features extracted by the fine-tuned model (the second step in~\cref{s:slefsup}), for which we use the $k$-means++ \cite{Arthur2008kmeanspp} initialization.
The second baseline (row 1 in~\cref{tab:comparison}) is similar, but uses as feature extractor a model  trained from scratch using only the labelled images, which corresponds to a standard transfer learning setting.
By comparing rows 1, 5 and 9 in~\cref{tab:comparison}, we can see that our method substantially outperforms $k$-means.
Next, we compare with the KCL~\cite{Hsu18_L2C}, MCL~\cite{Hsu19_MCL} and DTC~\cite{han2019learning} methods.
By comparing rows 2--4 to 9, we see that our method outperforms these by a large margin.
We also try to improve KCL, MCL and DTC by using the same self-supervised initialization we adopt (\cref{s:slefsup}), which indeed results in an improvement (rows 2--4 vs 6--8).
However, their overall performance still lags behind ours by a large margin.
For example, our method of~\cref{s:consistency} achieves $95.0\%$ ACC on SVHN, while ``KCL w/ S.S.'', ``MCL w/ S.S.'' and ``DTC w/ S.S.'' achieve only $65.6\%$, $53.1\%$ and $75.7\%$ ACC, respectively.
Similar trends hold for CIFAR10 and CIFAR100.
Finally, the incremental learning scheme of~\cref{s:incremental} results in further improvements, as can be seen by comparing rows 9 and 10 of~\cref{tab:comparison}.

In~\cref{fig:tsne}, we show the evolution of the learned representation on the unlabelled data from CIFAR10 using t-SNE~\cite{Maaten2008visualizing}.
As can be seen, while the clusters overlap in the beginning, they become more and more separated as the training progresses, showing that our model can effectively discover novel visual categories without labels and learn meaningful embeddings for them.

\begin{figure}[t]
\centering
\tabcolsep=0.02cm
\renewcommand{\arraystretch}{0.25}
\begin{tabular}[b]{ccc}
{\includegraphics[height=6.5em,clip,trim=4cm 0 6cm 1cm]{images/tsne/tsne_epoch_0.png}} &
{\includegraphics[height=6.5em,clip,trim=2cm 0 5cm 1cm]{images/tsne/tsne_epoch_30.png}}&
{\includegraphics[height=6.5em,clip,trim=3cm 0 0cm 1cm]{images/tsne/tsne_epoch_90.png}} \\[-0.5em]
(a) init & (b) epoch 30 & (c) epoch 90
\end{tabular}\hfill \caption{Evolution of the t-SNE during the training of CIFAR-10. Performed on unlabelled data (i.e., instances of dog, frog, horse, ship, truck).
Colors of data points denote their ground-truth labels.}\label{fig:tsne}
\end{figure}
 
\begin{table}[t]
 \centering
\caption{Novel category discovery results on OmniGlot and ImageNet. ACC on the unlabelled set.
}\label{tab:comparison_omniglot_imagenet}
  \begin{tabular}[c]{clccc}
    \toprule
    No  &                                      & OmniGlot        & ImageNet \\
    \midrule
    (1) & $k$-means~\cite{MackQueen67_Kmeans} & 77.2\%          & 71.9\%   \\
    (2) & KCL~\cite{Hsu18_L2C}                & 82.4\%          & 73.8\%  \\
    (3) & MCL~\cite{Hsu19_MCL}                & 83.3\%          & 74.4\%  \\
    (4) & DTC~\cite{han2019learning}          & 89.0\%          & 78.3\% \\\midrule
    (5) & Ours                                 & \textbf{89.1}\% & \textbf{82.5}\% \\
    \bottomrule
  \end{tabular}
  \hfill

\end{table}

We further compare AutoNovel to others on two more challenging datasets, OmniGlot and ImageNet, in~\cref{tab:comparison_omniglot_imagenet}.
For OmniGlot, results are averaged over the 20 alphabets in the \emph{evaluation} set; for ImageNet, results are averaged over the three 30-class unlabelled sets used in~\cite{Hsu18_L2C,Hsu19_MCL}.
Since we have a relatively larger number of labelled classes in these two datasets, we follow~\cite{han2019learning} and use metric learning on the labelled classes to pre-train the feature extractor, instead of the self-supervised learning.
We empirically found that self-supervision does not provide obvious gains for these two datasets.
This is reasonable since the data in the labelled sets of these two datasets are rather diverse and abundant, so metric learning can provide good feature initialization as there is less class-specific bias due to the large number of pre-training classes.
However, by comparing rows 1 and 5 in~\cref{tab:comparison_omniglot_imagenet}, it is clear that metric learning alone is not sufficient for the task of novel category discovery.
Our method substantially outperforms the $k$-means results obtained using the features from metric learning --- by $11.9\%$ and $10.6\%$ on OmniGlot and ImageNet respectively.
Our method also substantially outperforms the current state-of-the-art, achieving $89.1\%$ and $82.5\%$ ACC on OmniGlot and ImageNet respectively, compared with $89.0\%$ and $78.3\%$ of~\cite{han2019learning}, thus setting the new state-of-the-art.
\rev{By comparing~\cref{tab:comparison} and~\cref{tab:comparison_omniglot_imagenet}, we observe that KCL and MCL perform better on the more challenging ImageNet than the smaller datasets CIAR10, CIFAR100 and SVHN. This can be explained by the fact that the pairwise psuedo labels are provided by a similarity prediction network (SPN) which is pretrained on the labelled data. As there are much less labelled data in CIFAR10, CIFAR100 and SVHN than ImageNet, the learned SPN is less reliable, thus resulting in relatively poor performance for novel category discovery on unlabelled data from new classes.}




\begin{table*}[htb]
\footnotesize
\centering
\caption{Incremental Learning with the novel categories. ``old'' refers to the ACC on the labelled classes while ``new'' refers to the unlabelled classes in the \emph{testing set}.
``all'' indicates the whole testing set.
It should be noted that the predictions are not restricted to their respective subset. ``S.S.'' means self-supervision; \rev{``I.L.'' means incremental learning.}}\label{tab:increment}
\begin{tabular}{lccccccccc}
\toprule
& \multicolumn{3}{c}{CIFAR10} & \multicolumn{3}{c}{CIFAR100} & \multicolumn{3}{c}{SVHN}\\
\cmidrule(rl){2-4}
\cmidrule(rl){5-7}
\cmidrule(rl){8-10}
Classes       & old      & new     & all        & old       & new       & all       & old       & new       & all \\\midrule
KCL w/ S.S.
  & 79.4$\pm$0.6\%   & 60.1$\pm$0.6\%  & 69.8$\pm$0.1\%     & 23.4$\pm$0.3\%    & 29.4$\pm$0.3\%    & 24.6$\pm$0.2\%    & 90.3$\pm$0.3\%    & 65.0$\pm$0.5\%    & 81.0$\pm$0.1\% \\
MCL w/ S.S.
  & 81.4$\pm$0.4\%   & 64.8$\pm$0.4\%  & 73.1$\pm$0.1\%     & 18.2$\pm$0.3\%    & 18.0$\pm$0.1\%    & 18.2$\pm$0.2\%    & 94.0$\pm$0.2\%    & 48.6$\pm$0.3\%    & 77.2$\pm$0.1\% \\
DTC w/ S.S.
  & 58.7$\pm$0.6\%   & 78.6$\pm$0.2\%  & 68.7$\pm$0.3\%     & 47.6$\pm$0.2\%    & 49.1$\pm$0.2\%    & 47.9$\pm$0.2\%    & 90.5$\pm$0.3\%    & 72.8$\pm$0.2\%    & 84.0$\pm$0.1\% \\
\midrule
Ours w/ I.L.
 & \textbf{90.6$\pm$0.2}\% & \textbf{88.8$\pm$0.2}\%  & \textbf{89.7$\pm$0.1}\% & \textbf{71.2$\pm$0.1}\% & \textbf{56.8$\pm$0.3}\% & \textbf{68.3$\pm$0.1}\% & \textbf{96.3$\pm$0.1}\% & \textbf{96.1$\pm$0.0}\% & \textbf{96.2$\pm$0.1}\% \\
\bottomrule
\end{tabular}
\end{table*}


\begin{figure}[t]
\centering
\tabcolsep=0.2em
\renewcommand{\arraystretch}{0.25}
\begin{tabular}[b]{cc}
{\includegraphics[height=7em,clip,trim=0pt 0pt 5cm 0cm]{images/tsne/tsne_ours_Both.png}} &
{\includegraphics[height=7em,clip,trim=0pt 0pt 0cm 0cm]{images/tsne/tsne_all_Both.png}} \\
(a) Ours & (b) +~incr.~learning
\end{tabular}\hfill
\caption{t-SNE on CIFAR10: impact of incremental Learning.
Colors of data points denote their ground-truth labels (``old'' classes 0-4; ``new'' classes 5-9). We observe a bigger overlap in (a) between the ``old'' class 3 and the ``new'' class 5 when not incorporating Incremental Learning.}\label{fig:tsne_both}
\end{figure}

\subsection{Incremental learning scheme}
Here, we further evaluate our incremental scheme for novel category discovery as described in~\cref{s:incremental}.
Methods for novel category discovery such as~\cite{han2019learning, Hsu19_MCL, Hsu18_L2C} focus on obtaining the highest clustering accuracy for the new unlabelled classes, but may forget the existing labelled classes in the process.
In practice, forgetting is not desirable as the model should be able to recognize both old and new classes.
Thus, we argue that the classification accuracy on the labelled classes should be assessed as well, as for any incremental learning setting.
Note however that our setup differs substantially from standard incremental learning~\cite{rebuffi2017icarl,lopez2017gradient,shmelkov2017incremental,aljundi2018memory} where every class is labelled and the focus is on using limited memory.
In our case, we can store and access the original data without memory constraints, but the new classes are unlabelled, which is often encountered in practical applications.

By construction (\cref{s:incremental}), our method learns the new classes on top of the old ones incrementally, out of the box.
In order to compare AutoNovel to methods such as KCL, MCL and DTC that do not have this property, we proceed as follows.
First, the method runs as usual to cluster the unlabelled portion of the data, thus obtaining pseudo-labels for it, and learning a feature extractor as a byproduct.
Then, the feature extractor is used to compute features for both the labelled and unlabelled training data, and a linear classifier is trained using labels and pseudo-labels, jointly on all the classes, old and new.



We report in~\cref{tab:increment} the performance of the resulting joint classifier networks \emph{on the testing set} of each dataset (this is now entirely disjoint from the training set).
Our method has similar performances on the old and new classes for CIFAR10 and SVHN, as might be expected as the split between old and new classes is balanced.
In comparison, the feature extractor learned by KCL and MCL works much better for the old classes (e.g., the accuracy discrepancy between old and new classes is $25.3\%$ for KCL on SVHN).
Conversely, DTC learns features that work better for the new classes, as shown by the poor performance for the old classes on CIFAR10.
Thus, KCL, MCL and DTC learn representations that are biased to either the old or new classes, resulting overall in suboptimal performance.
In contrast, our method works well on both old and new classes; furthermore, it drastically outperforms existing methods on both.
In~\cref{fig:tsne_both}, we show the t-SNE projection of the learned feature representation on both old and new classes. It can be seen, with incremental learning, the embedding becomes more discriminative between old and new classes. Similarly, in~\cref{fig:conf_cifar5} we compare the confusion matrices w/ and w/o the incremental learning scheme. It can be seen that, with the incremental learning scheme, the clusters for new classes turn out to be more accurate. We notice that the errors are mainly due to the confusion between dog and horse. By looking into the images, we found that images of dogs and horses are confused because of having similar colors or poses.


\begin{figure}
\centering
\tabcolsep=0.02cm
\renewcommand{\arraystretch}{0.25}
\begin{tabular}[b]{cc}
    {\includegraphics[width=0.5\linewidth]{images/confusion_autonovel_cifar10.pdf}} &
    {\includegraphics[width=0.5\linewidth]{images/confusion_autonovel_cifar10_IL.pdf}} \\
\end{tabular}\hfill \caption{Confusion matrix on unlabelled classes of CIFAR10. Left: our method; Right: our method w/ I.L.}
\label{fig:conf_cifar5}
\end{figure}


\subsection{Finding the number of novel categories}
\label{s:results_unknown_k}
We now experiment under the more challenging (and realistic) scenario where the number of categories in the unlabelled data is unknown.
KCL and MCL assume the number of categories to be a large value (i.e., 100) instead of estimating the number of categories explicitly.
By contrast, we choose to estimate the number of categories as described in method summary~\ref{alg:unknown_k} (with $C^u_\text{max} = 100$ for all our experiments), before running the transfer clustering algorithm, and only then apply our ranking based method to learn the representation and find the cluster assignment.
Results for novel category number estimation are reported in~\cref{tab:omniglot_k_est} and ~\cref{tab:imagenet_k_est} on OmniGlot and ImageNet respectively. The average errors are 4.6 on OmniGlot and 2.33 on ImageNet, which validates the effectiveness of our approach.
In~\cref{tab:comparison_unknown}, we show the clustering results on OmniGlot and ImageNet, by substituting these estimates to our ranking based method for novel category discovery, and also compare with other methods.
The results of traditional methods are those reported in~\cite{Hsu19_MCL} using raw images for OmniGlot and pretrained features for ImageNet.
AutoNovel outperforms the previous state-of-the-art MCL by 5.2\% and  9.0\% ACC on OmniGlot and ImageNet respectively.
We also experiment on KCL, MCL and DTC by using our estimated number of clusters.
With this augmentation, both KCL amd MCL improve significantly, indicating that our category number estimation method can also be beneficial for other methods. DTC slightly outperforms our ranking based method by $1.6\%$ on OmniGlot with our estimated category number, while our method outperforms DTC by $2.9\%$ on ImageNet.
\rev{In addition, we also validate the sensitivity of different methods to the choice of cluster number on the 30-class ImageNet$_\text{A}$. We vary the cluster number from 20 to 100.  The results are shown in \cref{fig:cluster_number}. It can be seen that the sensitivity to the cluster number is similar for all methods. All methods achieve the best performance when the cluster number equals the ground truth, while the performance drops when the cluster number is off the ground truth. Our method consistently outperforms all others for cluster numbers 25 to 40 (note that our estimated cluster number is 34). For the extreme case with the cluster number of 100, MCL performs the best.}


\begin{table}[ht]
\footnotesize
\centering
\caption{Category number estimation on OmniGlot.}\label{tab:omniglot_k_est}
\setlength\tabcolsep{1pt}
\begin{tabular}{lccccc}
\toprule
Alphabet      & GT & SKMS~\cite{Anand14_SKMS} & KCL~\cite{Hsu18_L2C} & MCL~\cite{Hsu19_MCL}  & Ours \\
\midrule
Angelic & 20 & 16 & 26 & 22 & 23\\
Atemayar Q. & 26 & 17 & 34 & 26 & 25\\
Atlantean & 26 & 21 & 41 & 25 & 34\\
Aurek\_Besh & 26 & 14 & 28 & 22 & 34\\
Avesta & 26 & 8 & 32 & 23 & 31\\
Ge\_ez & 26 & 18 & 32 & 25 & 31\\
Glagolitic & 45 & 18 & 45 & 36 & 46\\
Gurmukhi & 45 & 12 & 43 & 31 & 34\\
Kannada & 41 & 19 & 44 & 30 & 40\\
Keble & 26 & 16 & 28 & 23 & 25\\
Malayalam & 47 & 12 & 47 & 35 & 42\\
Manipuri & 40 & 17 & 41 & 33 & 39\\
Mongolian & 30 & 28 & 36 & 29 & 33\\
Old Church S. & 45 & 23 & 45 & 38 & 51\\
Oriya & 46 & 22 & 49 & 32 & 33\\
Sylheti & 28 & 11 & 50 & 30 & 22\\
Syriac\_Serto & 23 & 19 & 38 & 24 & 26\\
Tengwar & 25 & 12 & 41 & 26 & 28\\
Tibetan & 42 & 15 & 42 & 34 & 43\\
ULOG & 26 & 15 & 40 & 27 & 33\\
\midrule
Avg$_{error}$ & - & 16.3 & 6.35 & 5.10 & \textbf{4.60} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\footnotesize
\caption{Category number estimation results.}\label{tab:imagenet_k_est}
\begin{tabular}{lccc}
\toprule
Data & GT  & Ours & Error \\
\midrule
ImageNet$_\text{A}$ & 30 & 34 & 4\\
ImageNet$_\text{B}$ & 30 & 31 & 1\\
ImageNet$_\text{C}$ & 30 & 32 & 2\\
\midrule
Avg$_{error}$ & - & - & 2.33\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\footnotesize
\centering
\caption{Novel category discovery with an unknown class number $C^u$.}\label{tab:comparison_unknown}
\begin{tabular}{lcccc}
 \toprule
    & OmniGlot
    & ImageNet \\
\midrule
Method & ACC  & ACC  \\
\midrule
$k$-means~\cite{MackQueen67_Kmeans} & 18.9\% & 34.5\%\\
LPNMF~\cite{Cai09_LPNMF}  & 16.3\%  & 21.8\%\\
LSC~\cite{Chen11_LSC}  & 18.0\% & 33.5\% \\
\midrule
KCL~\cite{Hsu18_L2C} & 78.1\% & 65.2\% \\
MCL~\cite{Hsu19_MCL} & 80.2\% & 71.5\% \\
\midrule
KCL~\cite{Hsu18_L2C} w/our $C^u$               & 80.3\%          & 71.4\%  \\
MCL~\cite{Hsu19_MCL} w/our $C^u$               & 80.5\%          & 72.9\%  \\
DTC~\cite{han2019learning} w/our $C^u$        & \textbf{87.0}\%          & 77.6\% \\
\midrule
Ours           & 85.4\% & \textbf{80.5}\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{images/cluster_number.pdf}
  \caption{Performance of different methods with different cluster number on ImageNet$_\text{A}$. The ground truth is 30. We vary the cluster number from 20 to 100.}
  \label{fig:cluster_number}
\end{figure}


\subsection{Transferring from ImageNet pretrained model}
\label{s:results_transfer_imagenet}
Rather than pretraining the model with self-supervised learning, one may also think to transfer representation learned from other datasets. The most common way of transfer learning with modern deep convolutional neural networks is to use ImageNet pretrained models. Here, we explore the potential of leveraging the ImageNet pretrained model to transfer features for novel category discovery. In particular, we take the ImageNet pretrained model as our feature extractor, and finetune the last macro-block and the linear heads of the model using our ranking based method. We experiment on CIFAR10, CIFAR100, and SVHN. The results are shown in \cref{tab:imagenet_transfer}. As can be seen, with the ImageNet pretrained representation, the performance of our method on CIFAR10 and CIFAR100 are further improved w.r.t to the results in~\cref{tab:comparison}. The incremental learning scheme succesfully boosts the performance by $0.7\%$ and $3.8\%$ on CIFAR10 and CIFAR100 respectively. 
\rev{For the performance on SVHN, we notice a significant drop between~\cref{tab:comparison} and~\cref{tab:imagenet_transfer}. }
This is likely due to the small correlation between ImageNet and SVHN, as also noted in other works that try to transfer features from ImageNet to other datasets~\cite{han2019learning,oliver2018realistic}.

\begin{table}[h!]
\centering
\footnotesize
\caption{Transferring from ImageNet to CIFAR10/CIFAR100/SVHN.}\label{tab:imagenet_transfer}
\begin{tabular}{lccc}
\toprule
& CIFAR10 & CIFAR100 & SVHN  \\
\midrule
$k$-means~\cite{MackQueen67_Kmeans} &92.4\%  & 78.8\% & 23.4\% \\
Ours &95.4\% &87.1\% &40.2\% \\
Ours w/I.L. & 96.1\%  & 90.9\% & 38.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Alternatives to ranking statistics}

The ranking statistics is the key to transfer knowledge from old classes to new classes in our model.
As discussed in~\cref{sec:method:ranking}, other methods like $k$-means, cosine similarity, and nearest neighbor can potentially be used as alternatives to ranking statistics to generate pairwise pseudo labels in AutoNovel.
We experiment with such alternatives and the results are shown in~\cref{tab:diff_pairwise}.
\rev{We experiment on two cases for $k$-means, one by running $k$-means on the mini-batch (denoted as $k$-means (batch)) and the other by running $k$-means (denoted as $k$-means (all)) on the whole unlabelled set.}
As it can be seen, ranking statistics, nearest neighbor and cosine similarity work significantly better than $k$-means on CIFAR10 and SVHN, while ranking statistics and cosine similarity work notably better than nearest neighbor on CIFAR100 and SVHN.
Note that the performance of cosine similarity depends on the choice of a proper threshold $\tau$.
Here, we report the results using the best thresholds on each dataset (0.85/0.8/0.9 for CIFAR10/CIFAR100/SVHN).
The effect of different thresholds is shown in~\cref{fig:cosine}.
It can be seen that, with a carefully chosen threshold, cosine similarity can also be a good measure to generate pairwise pseudo labels in our method, though the results turn to be relatively sensitive to $\tau$.
When $\tau < 0.6$, the cosine similarity fails to provide reliable pairwise pseudo labels.
Meanwhile, as $\tau$ lies in the continues space while $k$ in our ranking based method lies in the discrete integer space, it is easier to set a proper $k$ than $\tau$.
\rev{Overall, while it can be seen that ranking statistics and cosine similarity exhibit a similar behaviour when grid-searching with a relatively low sensitivity to the best value, we still find that ranking statistics is an interesting alternative to cosine similarity and is relatively unexplored in the context of deep learning. Throughout all of our experiments we demonstrate that ranking statistics performs consistently well and could open the way for more applications. Therefore, unless stated otherwise, ranking statistics is our default choice for all experiments.}

\begin{table}[h!]
\centering
\footnotesize
\caption{Different methods for pairwise pseudo labels.}\label{tab:diff_pairwise}
\begin{tabular}{lccc}
\toprule{}
& CIFAR10 & CIFAR100 & SVHN  \\
\midrule
$k$-means (batch) &42.9\%  &\textbf{74.3}\% &45.3\% \\
$k$-means (all) &62.2\%  &55.5\% &61.5\% \\
cosine & 90.1\% &73.3\% & 95.0\% \\
nearest neighbor & 90.2\% &69.7\% & 78.2\% \\
ranking statistics & \textbf{90.4}\%  &73.2\% & 95.0\% \\
\midrule
soft ranking statistics ($k=5$) & 62.2\%  &65.2\% & 72.5\% \\
soft ranking statistics ($k=15$)& 89.7\%  &71.1\% & \textbf{95.2}\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{images/cosine.pdf}
    \caption{Performance evolution w.r.t. the threshold $\tau$  for cosine similarity. We report results for $\tau=\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,0.97\}$.}
    \label{fig:cosine}
\end{figure}

\rev{Through ranking statistics, instead of generating hard (binary) pseudo targets, we can also encode soft rank similarities. To do so, we calculate the shared elements in the top-$k$ rank between two images. Let $c$ be the number of shared elements. The soft similarity is then defined as $\frac{c}{k}$, which can be used to replace the $s_{ij}$ in~\cref{e:bce}. The results are shown in~\cref{tab:diff_pairwise}. We find that $k=5$ is not optimal for the soft rank similarity, as the results largely lag behind the hard counterpart. We adopt the validation method introduced in~\cref{sec:exp:imp} to get $k=15$ as a better choice for the soft rank similarity. The results are in general on par with the hard (binary) rank similarity.}

\subsection{Other self-supervised learning methods}

We adopt the RotNet~\cite{gidaris2018unsupervised} for the first stage of AutoNovel. However, any other self-supervised representation methods can be applied. Here, we further experiment with the latest self-supervised representation learning methods including SimCLR~\cite{chen2020simple}, MoCo~\cite{he2020moco} and MoCo v2~\cite{chen2020mocov2}, which are the state-of-the-art representation learning methods for the tasks of object recognition and detection.
In this experiment, we replace RotNet by the latest self-supervised learning methods in our pipeline, and the other two steps remain the same as before.
In~\cref{tab:diff_selfsup}, we first directly compare the learned feature representations of different methods by running $k$-means on the output of the global average pooling layer on the unlabelled data.
All the three alternatives work better than RotNet when comparing the raw features learned with self-supervised learning on CIFAR10 and CIFAR100, while RotNet performs slightly better on SVHN\@.
This is likely due to nature of the pretext tasks used in these self-supervised learning methods.
RotNet uses the rotation prediction task, which is less relevant to the down stream task of partitioning the unlabelled data based on their semantic meaning.
Differently, the other three methods are contrastive learning based methods, which encourage the images of the same instance to be close in the feature space while the images of different instances to be further away.
Interestingly, we find that RotNet consistently outperforms the other three methods for AutoNovel.
This reveals that better feature initialization does not necessarily mean better representation fine-tuned on downstream tasks like novel category discovery.
Overall, by taking any of these self-supervised learning methods to pre-train our model, the performance can be significantly boosted on novel category discovery by our method.

\begin{table}[htb]
\centering
\footnotesize
\caption{Different self-supervised learning methods.}\label{tab:diff_selfsup}
\begin{tabular}{llccc}
\toprule
& & CIFAR10 & CIFAR100 & SVHN  \\
\midrule
\multirow{4}{*}{$k$-means~\cite{MackQueen67_Kmeans}}
 &SimCLR~\cite{chen2020simple} & \textbf{84.7}\% & \textbf{41.2}\% & 30.6\% \\
 &MoCo~\cite{he2020moco} & 58.7\% & 34.5\% & 21.3\% \\
 &MoCo v2~\cite{chen2020mocov2} & 61.8\% & 39.3\% & 28.5\% \\
 &RotNet~\cite{gidaris2018unsupervised} & 25.5\% &10.3\% & \textbf{31.7}\% \\
\midrule
\multirow{4}{*}{Ours}
 &SimCLR~\cite{chen2020simple} & 89.0\% & 54.6\% & 67.8\% \\
 &MoCo~\cite{he2020moco} & 87.6\% & 61.1\% & 74.6\% \\
 &MoCo v2~\cite{chen2020mocov2} & 89.0\% & 62.5\% & 76.6\% \\
 &RotNet~\cite{gidaris2018unsupervised} & \textbf{90.4}\% & \textbf{73.2}\% & \textbf{95.0}\% \\
\bottomrule
\end{tabular}
\end{table}

\rev{To investigate why RotNet appears to be more effective in our experiments, we carry out experiments by freezing different layers of the network and finetuning the rest of the layers employing different self-supervised learning approaches. ResNet18 is composed of four macro-blocks, denoted as $layer_{\{1, 2, 3, 4\}}$. In~\cref{tab:different_layer}, for each column, we freeze all the parameters before $layer_i$ and finetune $layer_i$ together with the subsequent layers. $head$ denotes the case where we only finetune the two linear heads while $layer_0$ denotes the case where we finetune all the parameters. We measure the novel category performance on CIFAR10 for all methods and report the ACC on the unlabelled data. It can be seen that, if we only finetune the linear heads, SimCLR, MoCo and MoCoV2 significantly outperform RotNet, which is consistent with the conclusion in the literature that the contrastive learning based methods can learn more meaningful higher level feature representation. The higher level features for RotNet is focusing on the task of rotation prediction, which is loosely related to the target task of novel category discovery, thus the performance is poor. However, if we finetune more layers, we can see that the performance are similar, while RotNet appears to be more effective for $layer_4$ and $layer_3$. The strong augmentation is essential for the performance of contrastive learning based self-supervision. However, for SVHN, where multiple digits appear in the same image and only the center digit is to be recognized, strong augmentations like cropping is not suitable for training, because random cropping will change the location of the center digits, which is harmful for training. Therefore, the performance of contrastive learning based methods lags behind RotNet on SVHN as in~\cref{tab:diff_selfsup}.}


\begin{table}[htb]
   \centering
  \caption{Performance on fine-tuning different layers on CIFAR10. ACC on the unlabelled set.
  }\label{tab:different_layer}
  \resizebox{0.48\textwidth}{!}{
    \begin{tabular}[c]{lcccccc}
      \toprule
      Method  & $head$ & $layer_4$ & $layer_3$ & $layer_2$ & $layer_1$ & $layer_0$ \\
      \midrule
      RotNet & 39.9\% & 90.4\% & 90.8\% & 88.4\% & 89.3\% & 88.9\% \\
      SimCLR & 73.1\% & 89.0\% & 89.1\% & 89.5\% & 90.4\% & 88.6\% \\
      MoCo   & 80.8\% & 87.6\% & 88.8\% & 89.3\% & 90.4\% & 89.5\% \\
      MoCoV2 & 84.6\% & 89.0\% & 89.5\% & 89.0\% & 90.3\% & 89.2\% \\
      \bottomrule
    \end{tabular}
}
\end{table}

\subsection{Unsupervised image clustering}
As discussed in~\cref{s:clustering}, by removing the requirement of labelled data, AutoNovel turns to an unsupervised clustering method that can learn both feature representation and clustering assignment.
Here, we compare our method on the clustering problem with the state-of-the-art methods on three popular benchmarks CIFAR10, CIFAR100 and STL10~\cite{Coates11STL10}.
We follow the common practice to use all 10 classes in CIFAR10 and STL10, and the 20 meta classes in CIFAR100 (denoted as CIFAR100-20) in our experiment for fair comparison.
The results are presented in~\cref{tab:clustering}.
Our method performs on par with the state-of-the-art method IIC~\cite{ji2019invariant} on CIFAR10 and and STL10, while significantly outperforms IIC on CIFAR100-20 by $9.3\%$.
Compared with IIC, which requires extra Sobel filtering on the input data and large batch sizes (660/1000/700 on CIFAR10/CIFAR100-20/STL10), our method only needs the conventional data augmentation (random cropping and horizontal flipping) and a small batch size of 128 for all three datasets.
Therefore, our method is a good alternative to state-of-the-art methods for the task of unsupervised image clustering, though this is not the main objective of this work.
\rev{Moreover, we report the $k$-means results on the feature representation of the base self-supervised model (i.e., RotNet) on each dataset. Unsurprisingly, the results are not satisfactory, because the high level features of RotNet are learned for the task of rotation prediction, making it less effective in capturing useful semantic information for downstream tasks like clustering.}
Meanwhile, we also validate the effectiveness of self-supervised pretraining and consistency regularization in~\cref{eq:cluster_loss}.
We can see that by dropping each of them in our method, the performance drops.
Without the self-supervised pretraining, the performance drops significantly. This suggests that self-supervised learning captures discriminative low level features for the task of image clustering. \rev{Similar to the task of novel category discovery, when applying our approach to unsupervised clustering, the MSE consistency loss is also effective in preventing the ``moving target'' phenomenon described in \cref{s:consistency} during training.}
We show the confusion matrix on CIFAR10 by our full method in~\cref{fig:cm_cluster}.
As can be seen from the diagonal of the matrix, our method can properly cluster objects into proper clusters.
We found airplane and bird are confused with ship because of the shared blue background; cat and dog are confused because of similar poses and colors.

\begin{table}[htb]
\centering
\footnotesize
\caption{Unsupervised image clustering. ``$k$-means on S.S.'' refers to the $k$-means results on the representation of the self-supervised model.}\label{tab:clustering}
\begin{tabular}{lccc}
\toprule{}
& CIFAR10 & CIFAR100-20 & STL10  \\
\midrule
$k$-means~\cite{MackQueen67_Kmeans} &22.9\%  &13.0\% &19.2\% \\
JULE~\cite{yang2016joint} & 27.2\% &13.7\% & 27.7\% \\
DEC~\cite{Xie16_DEC} & 30.1\% &18.5\% & 35.9\% \\
DAC~\cite{Chang_2018_DAC} & 52.2\%  &23.8\% & 47.0\% \\
IIC~\cite{ji2019invariant} &  \textbf{61.7}\%  &25.7\% & \textbf{59.6}\% \\
\midrule
$k$-means on S.S. & 14.3\% & 8.8\% & 15.7\% \\
\midrule
Ours w/o S.S. & 18.8\%  & 13.0\% &  22.7\% \\
Ours w/o MSE &  57.7\%  & 31.6\% &  48.6\% \\
Ours & \textbf{61.7}\%  &\textbf{35.0}\% & 56.4\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{images/confusion_cluster_cifar10.pdf}
\caption{Confusion matrix of clustering on CIFAR10.}\label{fig:cm_cluster}
\end{figure}

