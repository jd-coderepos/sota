\newpage
\appendix
\section*{Appendix}

\section{Experiment Setting Details}
\label{sec:training-details}
\begin{table}[!ht]
\centering
\small
\begin{tabular}{l|cc}
\toprule
Name                 & MathQA             & GSM8K.                       \\\midrule
\# Training Steps    & 50K                & 25K                          \\
Learning Rate (LR)   & \multicolumn{2}{c}{1.0e-4}                        \\
Optimizer            & \multicolumn{2}{c}{AdamW}                         \\
Adam Betas           & \multicolumn{2}{c}{(0.9, 0.999)}                  \\
Adam Eps             & \multicolumn{2}{c}{1.0e-8}                        \\
Weight Decay         & \multicolumn{2}{c}{0.1}                           \\
LR Scheduler         & \multicolumn{2}{c}{Linear w/ Warmup}              \\
\# LR Warm-up Steps  & \multicolumn{2}{c}{100}                           \\
Effective Batch Size & \multicolumn{2}{c}{32}                            \\
FP Precision         & \multicolumn{2}{c}{FP 32 for 125M, FP16 for 2.7B} \\
Gradient Clipping    & \multicolumn{2}{c}{1.0}                           \\\bottomrule
\end{tabular}
\caption{The hyperparameters used for model training on two different types of datasets.}
\label{tab:hyperparams}
\end{table} \paragraph{Hyperparameters.} All hyperparameters for training is shown in \autoref{tab:hyperparams}. 
We use  in the experiments with -MML, as a result of enumeration search among the values of . 
We use the default AdamW optimizer settings and slightly tuned the learning rate by trying out several values between 1.0e-3 and 1.0e-5. The difference in floating point precision is to fit the GPT-Neo 2.7B model into the memory of the GPUs. All experiments are conducted on V100-32GB GPUs. 

\paragraph{\patk evaluation.}
We use temperature sampling and sample  solutions with , where  for MathQA and  for GSM to evaluate \textsc{pass}@, to be maximally consistent with previous work \citep{austin2021program, cobbe2021training, chowdhery2022palm}. We also report \textsc{pass}@ using the  samples and the unbiased estimator proposed in \cite{chen2021evaluating}. We use  to sample 1 solution per specification and evaluate \textsc{pass}@1. 

\paragraph{Codex few-shot settings.} 
We estimate the Codex~\citep{chen2021evaluating} performance under the few-shot settings. More specifically, the prompt consists of a natural language task description "\texttt{\# Generate Python code to solve the following math word problems:}" and four examples, following previous work \citep{chowdhery2022palm}. Each example consists of the NL specification as a one-line comment and the gold program solutions. We evaluate \patk for Codex using the same sampling methods as above.

\paragraph{Details for self-sampling.} \an{Updated this paragraph to make it clearer} During a training step, we sample one solution\footnote{We also experiment with higher sampling budgets but do not observe significant improvements.} for each task (\ie natural language problem) in the batch, \ie  in \autoref{alg:main-algo} and \autoref{alg:pcp-sampling}. 
Thus for each gradient update, we first compute the loss for each task based on the saved solutions in the buffer and loss functions described in \autoref{tab:loss-function}, then it is averaged across the 32 tasks in the batch. 
Note that the total number of samples we generate per task throughout training is also scaled up by the number of training epochs, which is 235 for MathQA-Python-Filtered, 83 for MathQA-Python and 145 for GSM5.5K-Python. For sampling temperature, we use the same setting as inference time, with . 


\section{Additional Experiment Results}
\label{sec:additional-results}
\paragraph{Comparing GSM performance with previous work. }
Here we compare our method with previous work on the original test sets of GSM8K. The results are shown as \autoref{tab:gsm-test}. On GSM8K, some of the prior works are evaluated on a different format of NL inputs than ours, so they are not directly comparable, but we still include them to help better position the performance of our methods. We test Codex using the same input in a few-shot setting, and we find that similar with the result on MathQA in \autoref{tab:mathqa-full-test}, our method achieves better \patks{1} while being significantly worse in \patks{100} compared with Codex. We hypothesize that as Codex model is used tested few-shot setting and not finetuned, it does not suffer from the overfitting issue we mentioned. This leads to great diversity but poor accuracy during generation. However, due to the little information we have about Codex (\eg model size, training data), it is hard to derive any further conclusion. 
\begin{table}
    \small
\centering
    \begin{tabular}{lcc}
    \toprule
    Models      & \patks{1}           & \patks{100} \\\midrule
    \multicolumn{3}{l}{\textit{Previous work}:}           \\
    \quad OpenAI 6B~\citep{cobbe2021training}             & 21.8  & 70.9   \\
    \quad PaLM-Coder 540B~\citep{chowdhery2022palm}              & \textbf{50.9}   & -    \\
    \quad LaMDA 137B~\citep{chowdhery2022palm}              & 7.6   & -    \\
    \quad Codex Cushman\citep{chen2021evaluating}    & 5.0    & 58.0    \\
    \quad Codex Davinci~\citep{chen2021evaluating}    & 17.0   & \textbf{71.0}    \\\hdashline
    \textit{Ours}: \\
    \quad GPT-Neo 2.7B w/ self-sampling FCS + PCS     & 19.5   & 41.4    \\\bottomrule
    \end{tabular}
    \captionsetup[]{type=table}\caption[]{Compare with previous methods on the original test set of GSM8K dataset. : model not pretrained on code. : few-shot learning results. : different setting from ours\footnotemark.\label{tab:gsm-test}}
\end{table}
\footnotetext{Natural language explanations of the solutions are used as input and the few-shot exemplars are not in the same format as ours.} \begin{table}[t]
\small
\begin{tabular}{clcclcccccc}
\toprule
                           &            & \multicolumn{2}{c}{\textbf{\# Sols. in} } &  & \multicolumn{6}{c}{{\textbf{\textsc{pass}@}}\textbf{(\%)}}                                                                   \\ \cline{3-4} \cline{6-11} 
Self-Sampling    & Loss Func. & FCS           & PCS           &  & =1          & =5           & =10          & =20          & =50          & =100         \\\midrule
-            & MLE        & -             & -             &  & 7.4          & 10.6          & 12.7          & 15.3          & 19.2          & 22.7          \\\midrule
\multirow{3}{*}{FCS only}  & MML        & 1.48          & -             &  & {\ul 6.9}    & 11.0          & 13.3          & 16.0          & 20.1          & 23.7          \\
                           & MLE-Aug   & \textbf{1.76} & \textbf{-}    &  & \textbf{7.6} & \textbf{13.1} & \textbf{16.5} & \textbf{20.5} & \textbf{26.8} & \textbf{32.3} \\
                           & -MML  & 1.57          & -             &  & 7.5          & 11.7          & 14.5          & 17.9          & 23.1          & 27.3          \\\midrule
\multirow{3}{*}{FCS + PCS} & MML        & 1.40          & 1.10          &  & {\ul 5.5}    & {\ul 9.0}     & {\ul 11.0}    & {\ul 13.1}    & {\ul 16.2}    & {\ul 18.7}    \\
                           & MLE-Aug   & \textbf{2.00} & \textbf{1.36} &  & \textbf{7.5} & \textbf{13.6} & \textbf{17.5} & \textbf{22.1} & \textbf{29.2} & \textbf{35.0} \\
                           & -MML  & 1.62          & 1.14          &  & {\ul 7.2}    & 12.0          & 14.9          & 18.4          & 23.6          & 27.9         \\\bottomrule
\end{tabular}
\caption{Full comparison of various loss functions (\autoref{sec:multiple-programs}) with different self-sampling strategies. Results are on the dev set of GSM5.5K-Python with GPT-Neo 125M as the base model. Best performance within the same category is in \textbf{bold} and ones \textit{worse than MLE} is \underline{underlined}.  for -MML.}
\label{tab:loss-func-perf}
\end{table} 


\paragraph{Ablation results on loss functions.}
Here we show the full results on the ablation of loss functions in \autoref{tab:loss-func-perf}. We can see that trends observed from \patks{100} in \autoref{fig:loss-perf-compare} are consistent with other \patk results, as MLE-Aug loss beats other two loss functions on all \patk. And using MML loss when adding PCSs for learning results in worse performance than MLE for \patks{1} as well. Moreover, from the number of FCSs and PCSs saved in the buffer , we can also observe that using MLE-Aug loss results in more FCSs and PCSs being saved, thus further encourages diversity in generation.

\section{Full Learning Algorithm with Partial Correctness} 
\label{sec:full-algo-pcp}
\begin{algorithm}[t]
    \small
    \caption{Training Update with Partially Correctness
    }
    \label{alg:pcp-algo}
\begin{algorithmic}
    \STATE \textbf{Initialize: (only once before training starts)} 
    \STATE \quad Solutions buffer  with an empty and the reference solution 
    \STATE \quad Reference solution states  where 
    \STATE \quad State-prefixes mapping 
    \STATE {\bfseries Input:} 
    \STATE \quad Parameterized model  
    \STATE \quad A training example  
    \STATE \quad Tracing function  to obtain intermediate states
\end{algorithmic}
\begin{algorithmic}[1]
    \STATE  \bt{/* call \autoref{alg:pcp-sampling} */}
        \FOR{ \textbf{in} } 
            \FOR{; ; }
                \STATE  \bt{/* get intermediate state for each solution prefix  */}
                \IF{} 
                    \STATE   \bt{/* get existing prefixes that executes to state  */}
                    \IF{\textbf{not} }
                        \STATE 
                        \STATE 
                    \ENDIF
                    \STATE \textbf{continue} \bt{/* we only need the longest matching prefix */}
                \ENDIF
            \ENDFOR
        \ENDFOR
    \STATE 
\end{algorithmic}
\end{algorithm}
 Our general learning framework in shown as \autoref{alg:main-algo} and it is further extended in \autoref{sec:pcp}. Here we show a complete version of the algorithm with using partially-correct solutions in \autoref{alg:pcp-algo}.
Additionally, here are the detailed explanation of the data structure and functions used in it: \\
 \textbf{Mapping }: This is a data structure that maps an intermediate state to a set of solution (prefixes) that execute to that state, \ie . In this mapping, we save \emph{all} PCSs and their intermediate states, \textit{including all prefixes} of any PCS. We use this to significantly speed up the lookup process as mentioned in \autoref{sec:learning-pcp}; \\
 \textbf{Function }: Since all states for all known PCSs are saved in , to know whether a prefix  is partially-correct, we only need to check if its state matches any of the known states for a PCS, \ie if ; \\
 \textbf{Function }: As mentioned in \autoref{sec:learning-pcp}, we use AST and length constraint to rule out "trivial variants" and identify new PCSs to save in the buffer . Here the solutions to compare are the set of solutions  that reaches the same intermediate state, \ie being state-based equivalent;\\
 \textbf{Function }: Here we not only need to add the new PCS into the buffer , but also need to prune out the saved solutions that are prefix of ; \\
 \textbf{Function }: Here we need to save the states of all prefixes of an identified partially-correct solution, thus we will loop through all prefixes of  and obtain its execution state, then update  accordingly. As mentioned above, existing PCSs may be a prefix of the new PCS, so we also need to prune out such existing PCSs from mapping .

\section{Qualitative Analysis}
\label{sec:qualitative}
\begin{table}[t]
\centering
\small
\begin{tabular}{p{0.35\linewidth}p{0.18\linewidth}p{0.18\linewidth}p{0.18\linewidth}}
\toprule
\textbf{NL Problem Descriptions} & \textbf{Ref. Solution} & \textbf{Self-Sampled FCS} & \textbf{Self-Sampled PCS} \\\midrule
\textit{(MathQA-Example-1):} \newline
The charge for a single room at hotel P is 70 percent less than the charge for a single room at hotel R and 10 percent less than the charge for a single room at hotel G. The charge for a single room at hotel R is what percent greater than the charge for a single room at hotel G? & 
\texttt{n0=70.0\newline
n1=10.0\newline
t0=100.0-n0\newline
t1=100.0-n1\newline
t2=t0/t1\newline
t3=t2*100.0\newline
t4=100.0-t3\newline
t5=t4/t3\newline
answer=t5*100.0} &
\texttt{n0=70.0\newline
n1=10.0\newline
t0=100.0-n1\newline
t1=100.0-n0\newline
t2=t0/t1\newline
t3=t2*100.0\newline
answer=t3-100.0
}
& \multicolumn{1}{c}{-} \\\midrule
\textit{(MathQA-Example-2):} \newline
If john runs in the speed of 9 km/hr from his house, in what time will he reach the park which is 300m long from his house? &
\texttt{n0=9.0\newline
n1=300.0\newline
t0=n0*1000.0\newline
t1=n1/t0\newline
answer=t1*60.0
} &
\multicolumn{1}{c}{-} &
\multicolumn{1}{c}{-} \\\midrule
\textit{(MathQA-Example-3):} \newline
A class consists of 15 biology students and 10 chemistry students. If you pick two students at the same time, what's the probability that one is maths and one is chemistry? &
\texttt{n0=15.0\newline
n1=10.0\newline
t0=n0+n1\newline
t1=n0/t0\newline
t2=n1/t0\newline
t3=t0-1.0\newline
t4=n1/t3\newline
t5=n0/t3\newline
t6=t1*t4\newline
t7=t5*t2\newline
answer=t6+t7} &
\texttt{n0=15.0\newline
n1=10.0\newline
t0=n0+n1\newline
t1=n0/t0\newline
t2=n1/t0\newline
t3=t0-1.0\newline
t4=n1/t3\newline
t5=n0/t3\newline
t6=t1*t4\newline
t7=t5*t2\newline
answer=t7+t6} &
\texttt{n0=15.0\newline
n1=10.0\newline
t0=n0+n1\newline
t1=n0/t0\newline
t2=n1/t0\newline
t3=t0-1.0\newline
t4=n0/t3\newline
t5=n1/t3} \\\midrule
\textit{(GSM-Example-1):} \newline
Ellie has found an old bicycle in a field and thinks it just needs some oil to work well again.  She needs 10ml of oil to fix each wheel and will need another 5ml of oil to fix the rest of the bike. How much oil does she need in total to fix the bike? & 
\texttt{n0=2\newline
n1=10\newline
n2=5\newline
t0=n0*n1\newline
answer=t0+n2} &
\texttt{n0=10\newline
n1=5\newline
t0=n0+n1\newline
answer=n0+t0} &
\texttt{n0=10\newline
n1=5\newline
n2=2} \\\midrule
\textit{(GSM-Example-2):} \newline
There is very little car traffic on Happy Street. During the week, most cars pass it on Tuesday - 25. On Monday, 20\% less than on Tuesday, and on Wednesday, 2 more cars than on Monday. On Thursday and Friday, it is about 10 cars each day. On the weekend, traffic drops to 5 cars per day. How many cars travel down Happy Street from Monday through Sunday? &
\texttt{n0=20\newline
n1=100\newline
n2=25\newline
n3=2\newline
n4=10\newline
t0=n0/n1*n2\newline
t1=n2-t0\newline
t2=t1+n3\newline
t3=n4*n3\newline
t4=t0*n3\newline
answer=t3+n2 \textbackslash \newline\quad+t2+t3+t4} &
\texttt{n0=25\newline
n1=2\newline
n2=20\newline
n3=100\newline
n4=10\newline
t0=n0-n1\newline
t1=n2/n3*n0\newline
t2=t0-t1\newline
t3=t2+n4\newline
t4=n0-t3\newline
answer=t4+n3} &
\texttt{n0=2\newline
n1=25\newline
n2=20\newline
n3=100\newline
n4=10}
\\\bottomrule
\end{tabular}
\vspace{2pt}
\caption{More examples of self-sampled fully-correct (FCS) and partially-correct solutions (PCSs). "MathQA" denotes the MathQA-Python-Filtered dataset and "GSM" denotes the GSM5.5K-Python dataset. All solutions are from the \textit{final buffer after training} a GPT-Neo 2.7B model, while learning from self-sampled FCS+PCS with the MLE-Aug loss.}
\label{tab:case-study}
\end{table} In \autoref{tab:case-study}, we show more examples of the fully-correct and partially-correct solutions that the models found during self-sampling, from both the MathQA and GSM datasets. First, we can see that for some NL problems, it is possible that no FCS or PCS can be found with self-sampling, as in \textit{MathQA-Example-1} and \textit{MathQA-Example-1}. 
Take \textit{MathQA-Example-2} as an example, the question is quite straightforward thus it leaves very little room for the existence of other correct solutions, as the reference solution is already very short.
Moreover, we can also observe that the ways self-sampled FCS and PCS differ from the reference solution vary a lot. In \textit{MathQA-Example-2}, \textit{GSM-Example-1} and \textit{GSM-Example-2} the sampled FCSs complete the task with very different paths compared with the reference solution, and actually result in using fewer lines of code. Another way of getting FCS or PCS is to perform small and local perturbations, \eg switch the two sides of a addition or re-order the two non-dependent statements, as shown in other examples. We find that these local perturbations are more common in general in both datasets, as such patterns are easier for the model to learn.

\section{Tracking Training Progress}
\label{sec:training-progress}
\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{imgs/gsm-pat1.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{imgs/gsm-pat5.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{imgs/gsm-pat10.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{imgs/gsm-pat20.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{imgs/gsm-pat50.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{imgs/gsm-pat100.png}
     \end{subfigure}
        \caption{How \patk on the dev set evolve during training. Results shown on GSM5.5K-Python dataset with GPT-Neo 125M model. Exponential moving average smoothing is applied for more clarity, but original curve is shown in shade.}
        \label{fig:training-progress}
\end{figure} \paragraph{Learning from self-sampled solutions mitigates overfitting.}
Here we shown the \patk performance curve with respect to the training process in \autoref{fig:training-progress}. 
From the curves, we can observe that for MLE, while \patks{1} and \patks{5} generally improves during training, other \patk for higher  actually decreases after reaching the peak performance in early epochs, which is consistent with previous findings \citep{cobbe2021training}. This is due to overfitting: in the early stage of training, the model is less confident about its predictions thus the sampled  solutions are very diverse, and while training continues, it overfits to the one reference solution provided for learning thus leads to poor generalization when evaluated by \patk with high  values. \autoref{fig:training-progress} also shows how our proposed self-sampling method can mitigate the overfitting problem, as it keeps improving or maintaining \textsc{pass}@ while such performances start decreasing for MLE. Though it also shows improvements for \textsc{pass}@, but the performance still decreases in later training stages. Here we can also see the importance of suitable learning objective, as MML has almost no effect in mitigating such overfitting issue.

\paragraph{Early stopping is needed when prioritizing high  value for \patk.}
In our experiments, we select the model checkpoint with the best \patks{1} performance to evaluate all \patk. This setup aims to choose the best model that can solve the task with a small number of attempts (which corresponds to smaller  value), as studied in \citep{austin2021program}.
We can also observe that with our methods, the best \patks{1} checkpoint also yields the best or close to the best \textsc{pass}@ performances.
However, in certain applications where large number of attempts are allowed, \patk with high  values should be prioritized. An example is to generate candidate solutions before reranking \citep{cobbe2021training}. In this case, an earlier checkpoint (\eg one with best \patks{100}) should be used instead, which is not the best checkpoint for \patk where  is small. Also note that our proposed method are not suitable for these applications, as we observe no improvement on the peak \textsc{pass}@ performances. We think this because when such peak performance is reached, it is still in the early stage of training thus not many FCSs or PCSs have been saved in the buffer yet.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/buffer_size.png}
        \caption{Growth of the number of saved FCS and PCS during training. \# FCS includes the gold solutions.}
        \label{fig:training-sampling-buffer}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/sample_outcome.png}
        \caption{Distribution of the characterization of self-sampled solutions during training.}
        \label{fig:training-sampling-outcome}
    \end{subfigure}
    \caption{How self-sampling evolves throughout the training process. Results shown as training the GPT-Neo 125M model on the GSM5.5K-Python dataset with MLE-Aug loss.}
    \label{fig:training-sampling}
\end{figure} \paragraph{Partially-correct solutions help in early training stages.}
To show how self-sampling effects training, in \autoref{fig:training-sampling-buffer} we show how the size of the buffer progresses during training. From the curves, we can see that in the early training stages (\ie first 5k steps), the number of saved PCSs rapidly grows while the number of FCSs only slightly increases. In later stages of training, the growth of buffer size is mainly contributed by more FCSs being sampled and saved while the number of PCSs stays steady. Also when compared to learning only with FCSs, learning with FCSs + PCSs eventually accumulates more FCSs in the buffer (green dotted line vs yellow dotted line). 
In addition, we show how the distribution of the outcomes of self-sampled solutions changes throughout training in \autoref{fig:training-sampling-outcome}. We can see that in the early training stages, the ratio of not executable/incorrect solutions quickly drops to almost zero. At the same time, the ratio of new FCS or PCS being saved reaches the peak. As training proceeds, the models are mostly sampling known FCS or PCS as the size of the buffer converges as well. But the number of self-sampled fully-correct solutions gradually overtakes the partially-correct ones. 
