[{'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '60.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '83.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '69.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '28.8'}}, {'LEADERBOARD': {'Task': 'Math Word Problem Solving', 'Dataset': 'MATH', 'Metric': 'Accuracy', 'Score': '13.1'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'MBPP', 'Metric': 'Accuracy', 'Score': '47.5'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '30.5'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '55.5'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Easy)', 'Metric': 'Accuracy', 'Score': '80.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '75.3'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '81.3'}}, {'LEADERBOARD': {'Task': 'Arithmetic Reasoning', 'Dataset': 'GSM8K', 'Metric': 'Accuracy', 'Score': '52.2'}}]
