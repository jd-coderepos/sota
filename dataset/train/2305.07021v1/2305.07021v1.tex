\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[usenames,dvipsnames]{xcolor}

\usepackage{floatrow}
\usepackage{comment}
\usepackage{amsthm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{nicefrac}
\usepackage{url}
\usepackage{makecell}
\usepackage{hhline}
\usepackage{mathtools}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}

\iccvfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi


\newcommand{\figref}[1]{Fig.\xspace~\ref{#1}}
\newcommand{\tabref}[1]{Tab.\xspace~\ref{#1}}
\newcommand{\secref}[1]{Sec.\xspace~\ref{#1}}
\newcommand{\appref}[1]{Appendix\xspace~\ref{#1}}
\newcommand{\eqnref}[1]{Eq.\xspace~\ref{#1}}


\newcommand{\minisection}[1]{\noindent{\textbf{#1.}}}
\newcommand{\ApproachName}{TLC\xspace}
\newcommand{\ApproachNameLong}{\underline{T}oken-\underline{L}evel \underline{C}onfidence\xspace}

\newcommand{\OFALarge}{OFA\xspace}
\newcommand{\OFABase}{OFA\xspace}
\newcommand{\OFATiny}{OFA\xspace}


\begin{document}

\title{Simple Token-Level Confidence Improves Caption Correctness}

 \author{Suzanne Petryk \qquad
Spencer Whitehead \qquad
Joseph E. Gonzalez \qquad \\
Trevor Darrell \qquad
Anna Rohrbach \qquad
Marcus Rohrbach \\ \\
\vspace{-.5em}
 UC Berkeley \qquad  Meta AI
\vspace{-.8em}
}


\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}


The ability to judge whether a caption correctly describes an image is a critical part of vision-language understanding. However, state-of-the-art models often misinterpret the correctness of fine-grained details, leading to errors in outputs such as hallucinating objects in generated captions or poor compositional reasoning. In this work, we explore \ApproachNameLong, or TLC, as a simple yet surprisingly effective method to assess caption correctness. Specifically, we fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency. Compared to sequence-level scores from pretrained models, TLC with algebraic confidence measures achieves a relative improvement in accuracy by 10\% on verb understanding in SVO-Probes and outperforms prior state-of-the-art in image and group scores for compositional reasoning in Winoground by a relative 37\% and 9\%, respectively. When training data are available, a learned confidence estimator provides further improved performance, reducing object hallucination rates in MS~COCO Captions by a relative 30\% over the original model and setting a new state-of-the-art. 

\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{figures/arxiv-teaser.pdf}
   \caption{Judging caption correctness is still a challenge for large-scale models that operate at a sequence-level. We show that both algebraic and learned confidences at a token-level from a finetuned image captioning model improve fine-grained estimates of caption correctness.
   }
   \label{fig:teaser}
\end{figure} 

For vision-and-language models, grounding and the ability to assess the correctness of a caption with respect to an image is critical for vision-language understanding.
When models have difficulties with these, the outputs can be error prone~\cite{rohrbach2018emnlp} or rely on biases~\cite{agrawal2018vqacp,hendricks2018women}.
State-of-the-art models, like CLIP~\cite{radford2021learning} or OFA~\cite{wang2022unifying}, demonstrate impressive capabilities in a variety of settings, in part, thanks to these properties.
While these models have had much success, recent efforts for probing state-of-the-art models have revealed some weaknesses in these areas.
For instance, the recent Winoground task~\cite{thrush_and_ross2022winoground} illustrates that these models, including large-scale pre-trained ones, can struggle to correctly associate image-caption pairs when the captions have differences in word order.
Similarly, SVO-Probes~\cite{hendricks2021probing} has shown that models can fail in situations that require understanding verbs compared to other parts of speech.
The observations from these probing tasks suggest that existing models have difficulties discerning fine-grained details that can appear in multimodal data.
This may hinder their accuracy and reliability when used in real settings, which presents significant issues in scenarios that require highly correct outputs, such as assisting people with visual impairments~\cite{gurari2018vizwiz,whitehead2022reliable}.


We conjecture that these weaknesses may be related to the granularity with which models perform image-text matching (ITM).
As shown in \cref{fig:teaser}, many existing models often operate at a sequence-level, pooling the representations of the image and caption to assess whether the text correctly describes the image.
This pretext task relies on sequence-level supervision and data with sufficient scale to learn finer-grained concepts, such as the difference between ``a cat jumping over a box'' and ``a box with a cat inside''.
Typical generative image captioning methods, on the other hand, generate words token-by-token and produce confidences for each one.
They are supervised at a token-level rather than sequence-level, which may emphasize the consistency of each token in a sequence more explicitly.



\begin{table}
  \centering
  \resizebox{1.\columnwidth}{!}{
  \begin{tabular}{l@{}ccc}
    \toprule
    \multirow{2}{*}{Dataset \& Task} & Winoground  & SVO-Probes & Hallucination in \\
                    & \cite{thrush_and_ross2022winoground} & \cite{hendricks2021probing} & Captioning \cite{rohrbach2018emnlp}\
\mathbb{P}(t_{1:n}) = \sum_{k=1}^{n} \text{log}\, p(t_k \mid t_{1:k-1}, x)
    \label{eq:beam-ranks}

\text{CHAIR}_s = \frac{\text{\# captions with  hallucination}}{\text{\# captions}}
\label{eq:chairs}

    \text{CHAIR}_i = \frac{\text{\# objects hallucinated}}{\text{ \# objects mentioned}}
    \label{eq:chairi}



We report standard captioning metrics~\cite{anderson2016spice,vedantam2015cider} as well as CHAIRs and CHAIRi (or CHs and CHi). We also report several caption diversity measures~\cite{shetty2017speaking,xiong2018move} to examine whether captions with lower hallucination rates reduce caption diversity: \textit{Vocab Size} measures unique unigrams across predictions, \textit{\% Novel} measures the percentage of generated captions which do not appear in the training set annotations,
\textit{Div-2} measures the ratio of unique bigrams to the number of generated words,
and \textit{Re-4} measures the repetition of four-grams.

For both \ApproachName-A and \ApproachName-L, we choose a threshold  on the validation set. This threshold is used at test time to make binary decisions on the correctness of a given object in a predicted caption. We extract all objects from the validation set predictions, as well as corresponding token confidences and ground-truth hallucination scores. Then, we choose a confidence level  that reaches at least 99\% precision when separating correct vs. hallucinated objects. This precision is intentionally very high; the OFA captioning models have fairly low rates of hallucination on MS~COCO already (as seen in~\tabref{tab:hallucination-main}), yet we are interested in pushing the caption reliability as far as possible. When aggregating token confidences over object words, we select the minimum value for \ApproachName-A and the average value for \ApproachName-L based on the validation set recall. We use a large beam size of  to observe the behavior of our caption selection method when given many possible candidates.

We show results from the following methods. \textbf{Standard} uses the original top caption, that is, the caption from the beam ranked highest by . \textbf{Standard-Aug} uses the top caption from a captioning model , where its training set is augmented by the training set for . This tests whether the improvements from \ApproachName-L result from using token confidence itself or from additional training data. More details on Standard-Aug are in Appendix \ref{sec:supp-model}. \textbf{ITM} uses  to re-rank the  candidate captions from Standard based on their image-text matching score, and selects the highest-ranked caption as output. \textbf{\ApproachName-A} and \textbf{\ApproachName-L} use the respective algebraic or learned confidences over the MS~COCO object words to re-rank captions as described in \secref{sec:method-reducing-hallucinations}.

\minisection{Learned confidences lead to the least hallucinations} From~\tabref{tab:hallucination-main}, we can see that both \ApproachName-A and \ApproachName-L lower the CHs and CHi hallucination rates across all model sizes compared to the original (Standard) captions. \ApproachName-L reaches the lowest rates in each case; for example, it lowers CHs and CHi for \OFALarge by a relative 37.6\% and 34.3\% respectively. Additionally, \ApproachName-L lowers hallucination rates compared to Standard-Aug as well (\eg, a relative 20.9\% lower CHs for \OFALarge). This indicates that reserving a portion of data to train  can have a bigger impact on reducing hallucinations than does using the data for augmentation. Using ITM scores slightly lessens hallucination rates over Standard, yet at the cost of large degradation in CIDEr and SPICE, and underperforms \ApproachName in all metrics. In~\tabref{tab:hallucination-beam-breakdown}, we further evaluate hallucination rates on the subset of images where the top beam from Standard was \textit{not} selected by \ApproachName-L with \OFALarge -- in other words, samples where using \ApproachName-L made a difference. This occurred in almost a quarter of the captions. Standard hallucination rates are much higher on this subset (\eg, 6.78\% CHs), whereas \ApproachName-L reduces this by at least half.

\minisection{Captioning metrics do not capture hallucinations} CIDEr and SPICE decrease across all \ApproachName-based approaches, despite having dramatic reductions in hallucination rates. This effect was also observed by~\cite{rohrbach2018emnlp}, which described how standard metrics can often fail to penalize hallucinations. For instance, the majority of a sentence might overlap with a reference caption, yet still, misclassify an object. \cite{macleod2017understanding} nevertheless find that some visually-impaired users of captioning systems prefer correctness above possibly-wrong detail, motivating the drive for low hallucination rates.


\definecolor{Gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{Gray}}r}
\begin{table}[t]
  \centering
  \resizebox{1.\columnwidth}{!}{
  \begin{tabular}{@{\extracolsep{6pt}}llggrrrr}
    \toprule
     \multirow{2}{*}{Model} & \multirow{2}{*}{Confidence} & \multicolumn{2}{c}{Hallucination} & \multicolumn{2}{c}{Quality} \\ \cline{3-4} \cline{5-6} 
     
    \rule{0pt}{2.5ex} & & \multicolumn{1}{c}{CHs  }  & \multicolumn{1}{c}{CHi } & \multicolumn{1}{c}{CIDEr } & \multicolumn{1}{c}{SPICE } \\
    \midrule
 \multirow{5}{*}{\OFALarge} & Standard-Aug & 2.20 & 1.38 & 153.3 & 26.7 \\ \cmidrule{2-6}
     \rule{0pt}{1.2ex}  & Standard & 2.79 & 1.78 & \textbf{144.4} & \textbf{25.8} \\
    & ITM & 2.57 & 1.76 & 126.5 & 24.4 \\
    & \ApproachName-A & 1.81 & 1.24 & 140.7 & 25.5 \\
    & \ApproachName-L & \textbf{1.74} & \textbf{1.17} & 141.8 & 25.4  \\
    \midrule
     \multirow{5}{*}{\OFABase}  & Standard-Aug & 3.00 & 1.89 & 148.8 & 26.1  \\ \cmidrule{2-6}
     \rule{0pt}{1.2ex} & Standard & 3.78 & 2.39 & \textbf{142.9} & \textbf{25.6} \\
     & ITM & 3.22  &  2.15  & 127.1 & 24.3  \\
    & \ApproachName-A & 2.47 & 1.75 & 137.5 & 25.2 \\
     & \ApproachName-L & \textbf{2.05} &\textbf{1.48} & 137.5 & 24.9  \\
     \midrule
      \multirow{5}{*}{\OFATiny} & Standard-Aug & 10.58 & 6.83 & 119.8 & 22.1 \\ \cmidrule{2-6}
     \rule{0pt}{1.2ex} &  Standard & 11.01  & 7.23 & \textbf{117.4} & \textbf{21.7}   \\
      & ITM & 9.42 & 6.51 & 106.6 & 20.6  \\
    & \ApproachName-A & 9.87 & 6.86 & 115.8 & 21.5 \\
      & \ApproachName-L & \textbf{8.79} & \textbf{6.43} & 113.9 & 21.3 \\
    \bottomrule
  \end{tabular}
  \caption{Hallucination rates and captioning metrics on our test set when generating captions with a beam size of 25.
  }
  \label{tab:hallucination-main}}
\end{table}


\definecolor{Gray}{gray}{0.9}
\begin{table}[t]
  \centering
  \resizebox{1.\columnwidth}{!}{
  \begin{tabular}{@{}llrrrr}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Conf.} & \multicolumn{1}{c}{Vocab} &\multicolumn{1}{c}{\% Novel} & \multicolumn{1}{c}{Div-2} & \multicolumn{1}{c}{Re-4} \\
     & &  \multicolumn{1}{c}{Size } & \multicolumn{1}{c}{} &  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
    \midrule
 \multirow{3}{*}{\OFALarge} & Std. & 2822 & 77.07  & 6.97 & 66.34 \\
    & \ApproachName-A &  \textbf{2980} & \textbf{78.97} & \textbf{7.37} & \textbf{64.74} \\
    & \ApproachName-L & \underline{2915} & \underline{77.70}  & \underline{7.13} & \underline{65.54} \\
    \midrule
     \multirow{3}{*}{\OFABase}  & Std. & 2272 & 75.43  & 5.68 & 71.14 \\
    & \ApproachName-A & \textbf{2453} & \textbf{78.49} & \textbf{6.13} & \underline{69.28} \\
     & \ApproachName-L & \underline{2452} & \underline{77.53}  & \underline{6.03} & \textbf{69.76} \\
     \midrule
      \multirow{3}{*}{\OFATiny} & Std. &  1130 & 74.80  & 2.73 & 83.29  \\
    & \ApproachName-A & \underline{1211} & \underline{75.71}  & \underline{2.91} & \underline{82.68} \\
      & \ApproachName-L & \textbf{1243} & \textbf{77.05} & \textbf{3.01} & \textbf{82.12}  \\
    \bottomrule
  \end{tabular}
  \caption{Caption diversity metrics, evaluated on our test set.}
  \label{tab:hallucination-diversity}}
\end{table}


 

\begin{table}
    \centering
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{@{}lrrrr}
    \toprule
        Subset  &  I & Method & CHs () & CHi ()   \\
        \midrule
         \multirow{2}{*}{Full test set} &  \multirow{2}{*}{20,252} & Standard & 2.79 & 1.78  \\
         & & \ApproachName-L & \textbf{1.74} & \textbf{1.17}  \\
       \midrule
    \multirow{2}{*}{\ApproachName-L, } &  \multirow{2}{*}{5,401} & Standard & 6.78  & 3.22  \\
    & & \ApproachName-L & \textbf{2.81} & \textbf{1.61} \\
         \bottomrule
    \end{tabular}
    }
    \caption{Top: Results on the full test set reported in~\tabref{tab:hallucination-main}. Bottom: Hallucination rates on a subset of images where \ApproachName-L did not choose the top beam.  I denotes the number of images in each set. Results are shown for \OFALarge.
 }
    \label{tab:hallucination-beam-breakdown}
\end{table} 
\minisection{\ApproachName improves caption diversity} From~\tabref{tab:hallucination-diversity}, our method achieves higher performance on diversity metrics across all model sizes. For instance, \ApproachName-A consistently increases bigram uniqueness score \textit{Div-2}, and decreases the repetition measure \textit{Re-4}. Incorporating confidence into caption selection may help overcome language priors, where co-occurrence statistics from training influence token likelihoods. Diversity can improve as a result, where captions are driven more by consistency with the image rather than language. For example, the top center sample in~\figref{fig:qual} shows the baseline hallucinating a ``metal chair'', compared to the correct yet uncommon words ``wrought iron fence'' described by \ApproachName-L.

\definecolor{Gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{Gray}}r}
\begin{table*}[t!]
     \resizebox{\textwidth}{!}{
    \begin{tabular}{@{}ll|c|rrrr@{}r@{\ \ }r|rrrr@{\ }r@{\ \ }r@{}}
    \toprule
Reported in  & Method & Beam & \multicolumn{6}{c|}{XE Loss} & \multicolumn{6}{c}{SC Loss} \\
      &        & Size & \multicolumn{1}{c}{B@4} & \multicolumn{1}{c}{S} & \multicolumn{1}{c}{M} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{CHs ()} & CHi () & \multicolumn{1}{c}{B@4} & \multicolumn{1}{c}{S} & \multicolumn{1}{c}{M} & \multicolumn{1}{c}{C} & CHs () & CHi () \\
        \midrule
\cite{rohrbach2018emnlp} EMNLP 2018 & NBT \cite{lu2018neural} & 5 & - & 19.4 & 26.2 & 105.1 & 7.4 & 5.4 & - & - & - & - & - & - \\
\cite{rohrbach2018emnlp} EMNLP 2018 & TopDown \cite{anderson2018bottom} (no Boxes) & 5 & - & 19.9 & 26.7 & 107.6 & 8.4 & 6.1 & - & 20.4 & 27.0 & 117.2 & 13.6 & 8.8 \\
\cite{rohrbach2018emnlp} EMNLP 2018 & TopDown \cite{anderson2018bottom} & 5 & - & 20.4 & 27.1 & 113.7 & 8.3 & 5.9 & - & 21.4 & 27.7 & 120.6 & 10.4 & 6.9 \\
\cite{yang2021causal} CVPR 2021 & Transformer & unk & - & - & - & - & - & - & 38.6 & 22.0 & 28.5 & 128.5 & 12.1 & 8.1 \\
\cite{yang2021causal} CVPR 2021 & Transformer+CATT & unk & - & - & - & - & - & - & 39.4 & 22.8 & 29.3 & 131.7 & 9.7 & 6.5 \\
\cite{yang2021deconfounded} PAMI 2021 & UD-DICv1.0 & 5 & - & - & - & - & - & - & 38.7 & 21.9 & 28.4 & 128.2 & 10.2 & 6.7 \\
\cite{biten2022let} WACV 2022 & UD-L & no & 34.4 & 20.7 & 27.3 & 112.7 & 6.4 & 4.1 & 37.7 & 22.1 & 28.6 & 124.7 & 5.9 & 3.7 \\
\cite{biten2022let} WACV 2022 & UD-L + Occ & no & 33.9 & 20.3 & 27.0 & 110.7 & 5.9 & 3.8 & 37.7 & 22.2 & 28.7 & 125.2 & 5.8 & 3.7 \\
\cite{liu2022show} CVPR 2022 & CIIC & 3 & 37.3 & 21.5 & 28.5 & 119.0 & 5.3 & 3.6 & 40.2 & 23.2 & 29.5 & 133.1 & 7.7 & 4.5 \\
\cite{li2022comprehending} CVPR 2022 & COS-Net & 3 & 39.1 & 22.7 & 29.7 & 127.4 & 4.7 & 3.2 & 42.0 & 24.6 & 30.6 & 141.1 & 6.8 & 4.2 \\

This work & OFA \cite{ofa} & 5 & \textbf{41.8} &\textbf{ 24.4} & \textbf{31.3} & \textbf{140.7} & 3.1 & 2.0 & \textbf{42.3 }& \textbf{25.5} &\textbf{ 31.6} & \textbf{145.0 }& 3.1 & 2.0 \\
This work   & OFA  + \ApproachName-L & 5 & 41.2 & 24.1 & 30.9 & 138.4 & \textbf{*2.0}& \textbf{*1.4} & 42.0 & 25.2 & 31.4 & 143.8 & \textbf{2.3} & \textbf{1.5} \\
         
         \bottomrule
    \end{tabular}
    }
    \caption{Comparison to prior work for hallucination in image captioning on the MS~COCO Karpathy test split. Although we add a noun parser for our results in Tables \ref{tab:hallucination-main}, \ref{tab:hallucination-diversity}, and \ref{tab:hallucination-beam-breakdown}, we remove this step here and use the original evaluation provided by~\cite{rohrbach2018emnlp} to be consistent with prior work. We show captioning metrics B@4 (BLEU~\cite{papineni2002bleu}), S (SPICE~\cite{anderson2016spice}), M (METEOR~\cite{lavie2007meteor}), and C (CIDEr~\cite{vedantam2015cider}). \textbf{*} indicates state-of-the-art for hallucination rates. }
    \label{tab:hallucination-prior}
\end{table*} 
\minisection{Qualitative analysis} We show several qualitative examples in~\figref{fig:qual}.
In the left column, we see two examples where \ApproachName-L ``backed-off'' to a more general concept, whereas the baseline was specific, yet the image did not contain enough information to determine whether the specificity was indeed correct (\eg, ``car'' vs. ``vehicle'' and ``apples'' vs. ``fruit'').
A prior work~\cite{guadarrama2013youtube2text} explicitly optimized for this hierarchical generalization of unknown concepts, whereas here it emerges when considering confidence.
\ApproachName-L also avoids misclassification errors, such as ``person'' or ``scissors'' in the middle column. On the right column, we show examples influenced by incomplete object annotations. For example, the reference segmentations and captions might overlook the object ``table''. \ApproachName-L rejects captions that mention ``table'' in some of these cases, reflecting its training objective where correctness was judged based on faithfulness to the reference distribution. We include additional examples, including several failure cases, in Appendix~\ref{sec:supp-qual}.


\minisection{\ApproachName-L with \OFALarge sets a new state-of-the-art} We compare to previous results on MS~COCO object hallucination in~\tabref{tab:hallucination-prior}. We re-train our captioning models and confidence estimators on a dataset split that does not overlap with the Karpathy test split used for evaluation~\cite{karpathy2015deep}. \cite{rohrbach2018emnlp} show that training with a self-critical (SC) loss after training with cross-entropy (XE)~\cite{rennie2017self} can improve captioning metrics, yet worsen hallucination rates compared to training with XE alone.
We find that the baseline \OFALarge has similar hallucination rates for XE and SC, yet \ApproachName-L indeed produces the least hallucinations on top of the XE model.
This leads to a new state-of-the-art of 2.0\% and 1.4\% for CHs and CHi respectively. Notably, \ApproachName-L reduces hallucination without requiring any architecture changes to its captioning model, in contrast to the prior SOTA of COS-Net, where specific modules were introduced to capture image semantics.


\section{Discussion and Limitations}
\label{sec:limitations}

While \ApproachName-L provides effective confidence estimates for caption generation, it requires domain-specific training data for learning a confidence estimator from scratch on top of captioning model features. \ApproachName-A, on the other hand, uses the captioning model outputs directly, which leverages generalization ability from large-scale pretraining. Thus, \ApproachName-A can be effectively applied in settings where in-domain training data for captioning is not available. To combine these advantages, future research could explore unsupervised methods for learning correctness. Additionally, we use algebraic confidence estimates from uncalibrated output distributions, where output probabilities do not necessarily match actual probabilities of correctness. Potential future work may apply calibration methods to token-level confidence for improving caption correctness. Finally, learned confidences may also be incorporated into decoding methods that are not autoregressive.


\section{Conclusion}
\label{sec:conclusion}

In this work, we have explored a simple method using \ApproachNameLong (\ApproachName) for determining whether a caption correctly describes an image, a critical part of vision-language understanding. 
We find that judging caption correctness at a finer granularity than existing approaches leads to improvements in several settings, such as evaluating compositional reasoning with image-caption pairs or reducing object hallucinations in generated captions. To do so, \ApproachName uses a vision-language model fine-tuned on image captioning to produce token confidences, and then aggregates either algebraic (\ApproachName-A) or learned token confidences (\ApproachName-L) over words or sequences to estimate image-caption consistency. Increasing the confidence granularity with \ApproachName-A improves over prior state-of-the-art image and group scores on Winoground~\cite{thrush_and_ross2022winoground} by a relative 37\% and 9\%, respectively, and improves accuracy in verb understanding on SVO-Probes~\cite{hendricks2021probing} by a relative 10\%. When training data are available to learn and calibrate confidences with \ApproachName-L, we reduce object hallucination rates on COCO Captions by a relative 30\%, setting a new state-of-the-art. Overall, our results demonstrate that token-level confidence, whether algebraic or learned, can be a powerful yet simple resource for reducing errors in captioning output and assessing image-caption consistency.

\minisection{Acknowledgements} We thank David Chan, Kate Saenko, and Anastasios Angelopoulos for helpful discussions and feedback.  Authors, as part of their affiliation with UC Berkeley, were supported in part by the NSF CISE Expeditions Award CCF-1730628, DoD, including DARPAâ€™s LwLL, PTG, and/or SemaFor programs, the Berkeley Artificial Intelligence Research (BAIR) industrial alliance program, as well as gifts from Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Nexla, Samsung SDS, Uber, and VMware.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\appendix

\section*{Appendix}

\section{Overview}
\label{sec:supp-overview}

\cref{sec:supp-precision-recall} presents an ablation showing several alternative algebraic confidence estimates, and compares the precision-recall curve for the learned \ApproachName-L to that of algebraic confidences when separating correct and hallucinated objects.
\cref{sec:supp-qual} presents additional qualitative examples of both success and failure cases, comparing \ApproachName-L to the Baseline model. \cref{sec:supp-dataset} and \cref{sec:supp-model} provide further details on datasets and models respectively.


\section{Alternative Confidence Estimates}
\label{sec:supp-precision-recall}

\begin{figure*}[h]
  \centering
   \includegraphics[width=\linewidth]{figures/pr-curve.pdf}
   \caption{Precision-recall curve (left) and AUC (right) with different confidence estimates for separating correct and hallucinated objects. Results are shown on our validation set using \OFALarge.
   }
   \label{fig:supp-pr}
\end{figure*} 

We compare several other choices of algebraic confidence estimates for \ApproachName-A besides softmax score used in the main paper. All are derived from the likelihood (logit) distribution , as mentioned in \secref{sec:method-algebraic}. \textbf{Logit} is the logit value for the selected token directly from , whereas \textbf{Softmax} is the corresponding value after a softmax function. Again, in our main paper, \ApproachName-A is based on this softmax score confidence.
\textbf{Entropy} is the negative entropy of the log-softmax distribution, as a higher entropy should indicate higher uncertainty. Entropy has been previously used as a direct estimate of model uncertainty~\cite{wang2020tent} as well as a penalty in image caption decoding~\cite{xiao2021hallucination}. Finally, we consider the \textbf{Energy} score~\cite{liu2020energy}, originally proposed as a measure for OOD detection that theoretically correlates with the probability density of the in-domain samples. We use a temperature of 1, and negate the energy score so positive values indicate confident samples. 

In \figref{fig:supp-pr}, we show the precision-recall curve for various confidence estimates to separate correct and hallucinated objects. We compute these results on our MSC-Main validation set for  (see \tabref{tab:supp-datasets}). Specifically, we are not interested in the exact values of confidence estimates themselves, but rather how well they can \textit{rank} correct objects over those that are hallucinated. When using confidence estimates in practice, we need a threshold to make a binary decision about whether an object in a caption is considered hallucinated or not (\secref{sec:method-reducing-hallucinations}). We choose this threshold for a specific precision level, above the accuracy that the model achieves on its own. For instance, on the validation set for , about 98.3\% of the captioning model's predicted objects are correct (and the rest hallucinated). To push reliability further, we choose a threshold  for each method that achieves a precision of 99\%. In \figref{fig:supp-pr} (left), we therefore only show recall rates above 98\% precision, yet show the overall area-under-the-curve (AUC) in \figref{fig:supp-pr} (right).

From \figref{fig:supp-pr}, we can see that \ApproachName-Learned (\ie, \ApproachName-L) achieves the highest AUC of 99.48\%, and \ApproachName-Softmax achieves the second-highest of 99.07\%. The precision-recall plot shows that all algebraic confidences reach 0\% recall before 99.5\% precision, whereas \ApproachName-L still retains about 60\% recall at this high precision rate. In our main paper, we use \ApproachName-A to denote \ApproachName-Softmax, as it performed the best among the algebraic confidences.

\section{Additional Qualitative Examples}
\label{sec:supp-qual}

\begin{figure*}
  \centering
     \includegraphics[width=\linewidth]{figures/supp-qual-success-tlc.pdf}
  \caption{Additional qualitative examples on our test set for \ApproachName-L on \OFALarge, where the Baseline model caption contained a hallucination, yet the caption selected by \ApproachName-L did not.
  }
  \label{fig:supp-qual-success}
\end{figure*} \begin{figure*}
  \centering
     \includegraphics[width=\linewidth]{figures/supp-qual-failure-tlc.pdf}
  \caption{Failure cases on our test set for \ApproachName-L on \OFALarge, where \ApproachName-L selected a caption with a hallucination, yet the Baseline did not.
  }
  \label{fig:supp-qual-failure}
\end{figure*} 
In \figref{fig:supp-qual-success}, we present qualitative examples (in addition to those in \figref{fig:qual}) where the Baseline model caption contained a hallucination, yet the caption selected by \ApproachName-L did not. Note that ``Baseline'' refers to ``Standard'' as in \tabref{tab:hallucination-main}. In \figref{fig:supp-qual-failure}, we show several failure cases of \ApproachName-L. On the left is a case where the Baseline model selects a more general caption, whereas \ApproachName-L erroneously rejects it for one with a hallucinated ``carrot''. On the middle and right, \ApproachName-L selects captions that include other hallucinations of objects. Nevertheless, \ApproachName-L corrected 44.5\% () of captions that contained a hallucination from the Baseline model, whereas \ApproachName-L introduced a hallucination in only 0.2\% () of captions that did not contain a hallucination from the Baseline model.


\section{Dataset details}
\label{sec:supp-dataset}

\minisection{MS~COCO Captions} We use the same dataset splits as~\cite{whitehead2022reliable} for training and validating the captioning model  and confidence estimator , as~\cite{whitehead2022reliable} similarly reserves validation data in MS~COCO for training a confidence estimator (yet for the visual question answering task, rather than image captioning). For the Standard-Aug model  in \tabref{tab:hallucination-main}, we include the training set for  as part of the training set for .  In~\tabref{tab:supp-datasets}, we refer to these splits as MSC-Main (for MS~COCO Main), and use them for results in Tabs. \ref{tab:hallucination-main}, \ref{tab:hallucination-diversity}, and \ref{tab:hallucination-beam-breakdown}, and Figs. \ref{fig:qual}, \ref{fig:supp-pr}, \ref{fig:supp-qual-success}, and \ref{fig:supp-qual-failure}. For comparison to prior work that uses the Karpathy test split (\tabref{tab:hallucination-prior}), we re-split the validation set to prevent overlap. These details are presented as MSC-Prior in~\tabref{tab:supp-datasets}.


\begin{table}
  \centering
  \resizebox{1.\columnwidth}{!}{
  \begin{tabular}{l@{\hskip12pt}lrr}
    \toprule
    Dataset & Use Case & \# Images & \# Captions \\

\midrule

\multirow{4}{*}{MSC - Main } & Train  and  & 82,783 & 414,113 \\
&  Validate , Train  and   & 16,202 & 81,065 \\
& Validate  and , Select  thresholds & 4,050 & 20,268  \\
& Evaluation  & 20,252 & 101,321 \\ 
\midrule

\multirow{4}{*}{MSC - Prior} & Train  & 82,783 & 414,113  \\
& Validate , Train  & 28,403 & 142,120  \\
& Validate , Select  thresholds &  7,101 & 35,524  \\
& Evaluation  & 5,000 & 25,010  \\ 
\midrule

Winoground & Evaluation &  800 & 800 \\
\midrule
SVO-Probes & Evaluation & 12,958 & 6,479 \\

\bottomrule
  \end{tabular}
  \caption{Overview of datasets used in our work. MSC indicates MS~COCO Captions~\cite{chen2015microsoft}. }
  \label{tab:supp-datasets}
  }

\end{table}
 
\minisection{Winoground} We use the original data and evaluation setup for Winoground as in the original paper~\cite{thrush_and_ross2022winoground}, which consisted of 800 unique images and captions. This leads to 400 examples, each consisting of two image-caption pairs, where the captions contain the same words and/or morphemes yet a different word order.

\minisection{SVO-Probes} For SVO-Probes~\cite{hendricks2021probing}, we use the authors' public code to access a subset of data where the images were available. As discussed in \secref{sec:experiment-svo}, each image is annotated with a  subject, verb, object relation, \eg,  girl, sit, shore relation. We take the available data that contrasts two verbs, \eg, a ``positive'' or image-consistent relation girl, sit, shore and a ``negative'' or inconsistent relation girl, walk, shore. For each image, we take the provided ``positive'' caption (\eg, ``A girl sits on the shore''), and use a part-of-speech tagger~\cite{spacy} to localize the verb (\eg, ``sit'') in the sentence. We do not use images where the tagger failed to identify the verb, often in cases where the verb did not appear in the caption itself (\eg,  a triplet of person, wear, glasses with a caption of ``The glasses fogged up''). The final split contains about 6,500 image-caption pairs (\tabref{tab:supp-datasets}), half of which are correct pairs. This evaluation is not directly comparable to prior work~\cite{hendricks2021probing}, which used the full set of data, chose a threshold of 0.5 to indicate whether or not an individual sample matched an image, and was performed at a sequence-level rather than word-level. In our work, we contrast a positive and negative image for a given caption, and label a sample as correct if the confidence for the positive pair is larger than the confidence for the negative pair, similar to Winoground.

\minisection{Overlap with training data} All OFA models were not exposed to any MS~COCO validation or test data during pretraining~\cite{ofa}. Winoground was hand-curated from the Getty Images API~\cite{getty,thrush_and_ross2022winoground}, which is not used by OFA pretraining. Data from SVO-Probes was collected via the Google Image Search API and de-duplicated against Conceptual Captions~\cite{hendricks2021probing,sharma2018conceptual}. As OFA models used Conceptual Captions during pretraining, we assume there is no further overlap. 

\section{Model details}
\label{sec:supp-model}

\minisection{Captioning} To complement the details in \secref{sec:experiment-setup}, we provide additional experimental details for the captioning models. We use publicly available checkpoints for pretrained models provided by~\cite{ofa}. Parameter counts are 930M for \OFALarge, 180M for \OFABase, and 33M for \OFATiny~\cite{ofa}. To finetune the pretrained models on MS~COCO Captions, we follow the same settings from~\cite{ofa}, where we train with cross entropy loss for 2 epochs for \OFALarge, and 5 epochs for \OFABase and \OFATiny. We then train with CIDEr optimization for 3 epochs.

\minisection{\ApproachName-L} In addition to details in \secref{sec:experiment-setup}, we provide further information on the learned confidence estimator . We use a 4-layer Transformer encoder~\cite{vaswani2017attention} with 4 attention heads each. The embedded output corresponding to the token of interest  (\secref{sec:method-learning}) is passed to a 2-layer MLP, with hidden dimensions of size 512. The embedding dimension is 1024 for \OFALarge, 768 for \OFABase, and 512 for \OFATiny.  We train  for 200 epochs, with a batch size of 256, starting learning rate of 0.001, warm up ratio of 0.06 and polynomial learning rate decay to 2-7. We use the Adam optimizer~\cite{kingma2014adam} and clip gradients over 1.0. For aggregating tokens over objects for caption generation (\secref{sec:method-reducing-hallucinations}), we use the minimum score for softmax and average for \ApproachName-L, found on our validation set.


\end{document}