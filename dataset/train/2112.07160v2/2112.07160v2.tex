

\documentclass[nohyperref]{article}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}


\usepackage{placeins}











\usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{xcolor}         \usepackage{subfigure}
\usepackage{multirow}
\usepackage{mathabx}
\usepackage{enumerate}
\usepackage{pifont}
\usepackage{threeparttable}

\newtheorem{claim}{Claim}
\newcommand{\yes}{\ding{51}}












\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{A New Perspective on the Effects of Spectrum in Graph Neural Networks}

\begin{document}
	
	\twocolumn[
	\icmltitle{A New Perspective on the Effects of Spectrum in Graph Neural Networks}
	






	\begin{icmlauthorlist}
		\icmlauthor{Mingqi Yang}{dlut}
		\icmlauthor{Yanming Shen}{dlut}
		\icmlauthor{Rui Li}{dlut}
		\icmlauthor{Heng Qi}{dlut}
		\icmlauthor{Qiang Zhang}{dlut}
		\icmlauthor{Baocai Yin}{dlut,pcl}
	\end{icmlauthorlist}
	
	\icmlaffiliation{dlut}{Dalian University of Technology, China}
	\icmlaffiliation{pcl}{Peng Cheng Laboratory, China}
	
	\icmlcorrespondingauthor{Yanming Shen}{shen@dlut.edu.cn}
	
\icmlkeywords{Machine Learning, ICML}
	
	\vskip 0.3in
	]
	




	\printAffiliationsAndNotice{}  







\begin{abstract}
	Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the  spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs' performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at \url{https://github.com/qslim/gnn-spectrum}.
\end{abstract}

\section{Introduction}
Although graph neural network (GNN) communities are in a rapid development of both theories and applications, there is still a lack of a generalized understanding of the effects of the graph's spectrum in GNNs. As we can see, many improvements can finally be unified into different operations on the spectrum of the underlying graph, while their effectiveness is interpreted by several well-accepted isolated concepts: \cite{pmlr-v97-wu19e,zhu2021interpreting,klicpera_predict_2019,klicpera2019diffusion,chien2021adaptive,balcilar2021analyzing} explain it in the perspective of simulating low/high pass filters; \cite{chenWHDL2020gcnii,xu2018representation,liu2020towards,li2018deeper} interpret it as ways of alleviating oversmoothing phenomenon in deep architectures; \cite{cai2020graphnorm} adopts the conception of normalization operation in neural networks and applies it to graph data.
Since these improvements all indirectly operate on the spectrum,
it motivates us to study the potential connections between the GNN performance and the characteristics of the graph's spectrum. If we can find such a connection, it would provide a deeper and generalized insight into these seemingly unrelated improvements associated with the graph's spectrum (low/high pass filter, oversmoothing, graph normalization, etc), and further identify potential issues in existing architectures.
To this end, we first consider the simple correlation metric: cosine similarity among signals, and study the relations between it and the graph's spectrum in the graph convolution operation. It provides a new perspective that in existing GNN architectures, the distribution of eigenvalues of the underlying graph matrix controls the cosine similarity among signals. An ill-posed  spectrum would easily make signals over-correlated which is evidence of information loss.

Compared with oversmoothing studies~\cite{li2018deeper,oono2020graph,rong2019dropedge,huang2020tackling}, the correlation analysis associated with the graph's spectrum further indicates that the correlation issue is essentially caused by the graph's spectrum. In other words, for graph topologies with an unsmooth spectrum, the issue can appear even with a shallow architecture, and a deep model further makes the spectrum less smooth and eventually exacerbates this issue. Meanwhile, the correlation analysis also provides a unified interpretation of the effectiveness of various existing improvements associated with the graph's spectrum since they all implicitly impose some constraints on the spectrum to alleviate the correlation issue. However, these improvements are trade-offs between alleviating the correlation issue and applying more powerful graph filters: since a filter implementation directly reflects on the spectrum, a more appropriate filter for relevant signal patterns may correspond to an ill-posed spectrum, which in return will not gain performance improvements. Hence, in general GNN architectures, the correlation issue becomes the obstacle to applying more powerful filters.
As we can see, although one can approximate more sophisticated graph filters by increasing the order  of the polynomial theoretically~\cite{shuman2013emerging}, in the popular models, simple filters, e.g. low-pass filter~\cite{kipf2017semi,pmlr-v97-wu19e}, or the fixed filter coefficients~\cite{klicpera_predict_2019,klicpera2019diffusion} serve as the practical applicable choice.

With all the above understandings, the key solution is to decouple the correlation issue from the filter design, which results in our correlation-free architecture. In contrast to existing approaches, it allows to focus on exploring more sophisticated filters without the concern of the correlation issue. With this guarantee, we can improve the approximation abilities of polynomial filters to better approximate the desired more complex filters~\cite{hammond2011wavelets,defferrard2016convolutional}.
However, we also find that it cannot be achieved by simply increasing the number of polynomial bases as the basis characteristics implicitly restrict the number of available bases in the resulting polynomial filter. For this reason, commonly used (normalized) adjacency or Laplacian matrix where its spectrum serves as the basis cannot effectively utilize high-order bases. To address this issue, we propose new graph matrix representations, which are capable of leveraging more bases and learnable filter coefficients to better respond to more complex signal patterns. The resulting model significantly boosts performance on learning graph representations. Although there are extensive studies on the polynomial filters including the fixed coefficients and learnable coefficients~\cite{defferrard2016convolutional,8521593,chien2021adaptive,he2021bernnet}, to the best of our knowledge, they all focus on the coefficients design and use the (normalized) adjacency or Laplacian matrix as a basis. Therefore, our work is well distinguished from them.
Our contributions are summarized as follows:
\begin{itemize}
	\item 
	\vspace{-5pt}
	We show that general GNN architectures suffer from the correlation issue and also quantify this issue with spectral smoothness;
	\item
	\vspace{-5pt}
	We propose the correlation-free architecture that decouples the correlation issue from graph convolution;
	\item
	\vspace{-5pt}
	We show that the spectral characteristics also hinder the approximation abilities of polynomial filters and address it by altering the graph's spectrum.
\end{itemize}

\section{Preliminaries}

Let  be an undirected graph with node set  and edge set .
We denote  the number of nodes,  the adjacency matrix and  the node feature matrix where  is the feature dimensionality.
 is a graph signal that corresponds to one dimension of .

\textbf{Spectral Graph Convolution~\cite{hammond2011wavelets,defferrard2016convolutional}.}
The definition of spectral graph convolution relies on Fourier transform on the graph domain.
For a signal  and graph Laplacian , we have Fourier transform  and inverse transform .
Then, the graph convolution of a signal  with a filter  is

where  denotes a diagonal matrix in which the diagonal corresponds to spectral filter coefficients.
To avoid eigendecomposition and ensure scalability,  is approximated by a truncated expansion in terms of Chebyshev polynomials  up to the -th order~\cite{hammond2011wavelets}, which is also the polynomials of ,

where .
Now the convolution in Eq.~\ref{equ:graph_conv} is

Note that this expression is -localized since it is a -order polynomial in the Laplacian, i.e., it depends only on nodes that are at most  hops away from the central node.

\textbf{Graph Convolutional Network (GCN)~\cite{kipf2017semi}.}
GCN is derived from -order Chebyshev polynomials with several approximations.
The authors further introduce the renormalization trick  with  and .
Also, GCN can be generalized to multiple input channels and a layer-wise model:

where  is learnable matrix and  is nonlinear function.

\textbf{Graph Diffusion Convolution (GDC)~\cite{klicpera2019diffusion}.}
A generalized graph diffusion is given by the diffusion matrix:

with the weight coefficients  and the generalized transition matrix .
 can be ,  or others as long as they are convergent.
GDC can be viewed as a generalization of the original definition of spectral graph convolution, which also applies polynomial filters but not necessarily the Laplacian.


\section{Revisiting Existing GNN Architectures}

\begin{table*}[t]
	\small
	\centering
	\caption{A summary of  in Eq.~\ref{equ:gnn_generalization} in general graph convolutions.}
	\label{tab:gnn_summary}
	\vspace{5pt}
	\resizebox{1.0\textwidth}{!}{\begin{tabular}{c|cccccccccc}
			\toprule
			&GCN  &SGC  &APPNP  &GCNII  &GDC  &SSGC  &GPR  &ChebyNet  &CayleNet  &BernNet   	         \\
			\midrule
			Poly-basis			&General &General &Residual &Residual &General &General &General &Chebyshev & Cayle &Bernstein    \\
			\midrule
			Poly-coefficient	&Fixed &Fixed &Fixed &Fixed &Fixed &Fixed &Learnable &Fixed &Learnable &Learnable     \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-10pt}
\end{table*}
We first generalize existing spectral graph convolution as follows

where  is the graph matrix, e.g. adjacency or Laplacian matrix and their normalized forms.
 is the polynomial of graph matrices with coefficients  for a -order polynomial.
 is the feature transformation neural network with the learnable parameters .
In SGC~\cite{pmlr-v97-wu19e}, GDC~\cite{klicpera2019diffusion}, SSGC~\cite{zhu2020simple}, and GPR~\cite{chien2021adaptive},  is implemented as the general polynomial, i.e. .
Their differences are identified by the coefficients .
For example, SGC corresponds to a very simple form with  and .
By removing the nonlinear layer in GCNII~\cite{chenWHDL2020gcnii}, APPNP~\cite{klicpera_predict_2019} and GCNII share the similar graph convolution layer as

where  and  is the input node features.
By deriving its closed-form, we reformulate it with Eq.~\ref{equ:gnn_generalization} as .
In ChebyNet~\cite{defferrard2016convolutional}, CayleNet~\cite{8521593} and BernNet~\cite{he2021bernnet},  corresponds to Chebyshev, Cayle and Bernstein polynomials respectively.
GPR, CayleNet and BernNet apply learnable coefficient , where  is learned as the coefficients of general, Cayle and Bernstein basis respectively.
Therefore, with our formulation in Eq.~\ref{equ:gnn_generalization}, general graph convolutions are mainly different from  as summarized in Tab.~\ref{tab:gnn_summary}\footnote{Here, we follow the naming convention in GCNII called initial residual connection. GCN and GCNII interlace nonlinear computations over layers, making them difficult to reformulate all layers with Eq.~\ref{equ:gnn_generalization}. But one can represent them with the recursive form as . For example, in GCN, we have  and  with .}.

\subsection{Correlation Analysis in the Lens of Graph's Spectrum}
\label{sec:correlation_analysis}

Based on the generalized formulation of Eq.~\ref{equ:gnn_generalization}, we conduct correlation analysis on existing graph convolution in the perspective of the graph's spectrum.
We denote   for simplicity.
 denotes one channel in .
Then the convolution on  is represented as .
The cosine similarity between  and the -th eigenvector  of  is

 is the weight of  on  when representing  with the set of orthonormal bases .
The cosine similarity between  and  is

The detailed derivations of Eq.~\ref{equ:cos_signal} and Eq.~\ref{equ:cos_signal_eigenvalue} are given in Appendix~\ref{deriv:equ:cos_signal_eigenvalue}.

Eq.~\ref{equ:cos_signal_eigenvalue} builds the connection between the cosine similarity and the spectrum of the underlying graph matrix.
We say the spectrum is  if all eigenvalues have similar magnitudes.
By comparing Eq.~\ref{equ:cos_signal} and Eq.~\ref{equ:cos_signal_eigenvalue}, it shows that the graph convolution operation with the unsmooth spectrum, i.e., dissimilar eigenvalues, results in signals correlated (a higher cosine similarity) to the eigenvectors corresponding to larger magnitude eigenvalues and orthogonal (a lower cosine similarity) to the eigenvectors corresponding to smaller magnitude eigenvalues.
In the case where 0 eigenvalue is involved in the spectrum, signals would lose information in the direction of the corresponding eigenvectors.
In the deep architecture, this problem would further be exacerbated:
\begin{proposition}
	\label{prop:cos_convergence}
	Assume  is a symmetric matrix with real-valued entries.  are  real eigenvalues, and  are corresponding eigenvectors. Then, for any given , we have\\
	(i)  and  for ;\\
	(ii) If , , and the convergence speed is decided by .
\end{proposition}
We prove Proposition~\ref{prop:cos_convergence} in Appendix~\ref{proof:prop:cos_convergence}.
Proposition~\ref{prop:cos_convergence} shows that a deeper architecture violates the spectrum's smoothness, which therefore makes the input signals more correlated to each other.~\footnote{
	Here, nonlinearity is not involved in the propagation step.
	This meets the case of the decoupling structure where a multi-layer GNN is split into independent propagation and prediction steps~\cite{liu2020towards,pmlr-v97-wu19e,klicpera_predict_2019,zhu2020simple,zhang2021litegem}.
	The propagation involving nonlinearity remains unexplored due to its high complexity, except for one case of ReLU as nonlinearity~\cite{oono2020graph}.
	Most convergence analyses (such as over-smoothing) only study the simplified linear case~\cite{cai2020graphnorm,liu2020towards,pmlr-v97-wu19e,klicpera_predict_2019,zhao2020pairnorm,xu2018representation,chenWHDL2020gcnii,zhu2020simple,klicpera2019diffusion,chien2021adaptive}.
}
Finally, , and the information within signals would be washed out.
Note that all the above analysis does not impose any constraint to the underlying graph such as connectivity.

\textbf{Revisiting oversmoothing via the lens of correlation issue.}
In the well-known oversmoothing analysis, the convergence is considered as  where each row of  only depends on the
degree of the corresponding node, provided that the graph is  and ~\cite{xu2018representation,liu2020towards,zhao2020pairnorm,chien2021adaptive}.
Our analysis generalizes this result.
In our analysis, the convergence of the cosine similarity among signals does not limit a graph to be  or  that is required in the oversmoothing analysis analogical to the stationary distribution of the Markov chain, and even does not require a model to be necessarily  :
it is essentially caused by the bad distributions of eigenvalues, while the deep architecture exacerbates it.
Interestingly, inspired by this perspective, the correlation problem actually relates to the specific topologies since different topologies correspond to different spectrum.
There exists topologies inherently with bad distributions of eigenvalues, and they will suffer from the problem even with a shallow architecture.
Also, by taking the symmetry into consideration, Proposition~\ref{prop:cos_convergence}(i) shows that the convergence of cosine similarity with respect to  is also .
In contrast that existing results only discuss the theoretical infinite depth case, this provides more concrete evidence in the practical finite depth case that a deeper architecture can be more harmful than a shallow one.

\textbf{Revisiting graph filters via the lens of correlation issue.}
The graph filter is approximated by a polynomial in the theory of spectral graph convolution~\cite{hammond2011wavelets,defferrard2016convolutional}. Although theoretically, one can approximate any desired graph filter by increasing the order  of the polynomial~\cite{shuman2013emerging}, most GNNs cannot gain improvements by enlarging . Instead, the simple low-pass filter studied by many improvements on spectral graph convolution acts as the practical effective choice~\cite{shuman2013emerging,pmlr-v97-wu19e,1905.09550,muhammet2020spectral,klicpera2019diffusion}. Although there are studies involving high-pass filters to better process high-frequency signals recently, the low-pass is always required in graph convolution~\cite{zhu2020simple,zhu2021interpreting,balcilar2021analyzing,fagcn2021,gao2021message}. This can be explained in the perspective of correlation analysis. As we have shown, the graph convolution is sensitive to the spectrum. A more proper filter to better respond to relevant signal patterns may result in an unsmooth spectrum, making different channels correlated to each other after convolution. In contrast, although a low-pass filter has limited expressiveness, it corresponds to a smoother spectrum, which alleviates the correlation issue.


\section{Correlation-free Architecture}
\label{sec:channel_wise_architecture}

The correlation analysis via the lens of graph's spectrum shows that in general GNN architectures, the  spectrum leads to correlation issue and therefore acts as the obstacle to developing deep architectures as well as leveraging more expressive graph filters.
To overcome this issue, a natural idea is to assign the graph convolution in different channels of  with different spectrums, which can be viewed as a generalization of Eq.~\ref{equ:gnn_generalization} as follows

Both  and  are the feature transformation neural networks with the learnable parameters  and  respectively.
 is the -th polynomial with the learnable coefficients .
 is the -th channel of .
We denote  for simplicity.
Then the convolution operation on  in Eq.~\ref{equ:channel_wise_graph_conv} is

with the filter .
We denote .
Then,

where .
If  for any , i.e., the algebraic multiplicity of all eigenvalues is 1,  is a Vandermonde matrix with .
 serve as a set of  bases, where each filter  is a linear combination of .
Hence, a larger  helps to better approximate the desired filter.
When ,  is a full-rank matrix and  is sufficient to represent any desired filter with proper assignments of .
Note that  is much smaller in real-world graph-level tasks than that in node-level tasks, making  more tractable.

By considering the columns of a Vandermonde matrix, i.e.  as bases, we can see that when increasing  (aka applying more bases),  with  goes diminishing and  with  goes divergent.
To balance the diminishing and divergence problems when applying a larger , we need to carefully control the range of the spectrum close to  or .
General approaches have ~\footnote{General approaches use the (symmetry) normalized , i.e. ,  to guarantee its spectrum is bounded by ~\cite{kipf2017semi,klicpera2019diffusion} or the (symmetry) normalized , i.e.  to ensure the boundary [0, 2] and then rescale it to ~\cite{he2021bernnet}.}.
Although there is no concern of divergence problems, , especially for a small , inclines to 0 when increasing ,
making the higher-order basis ineffective in the practical limited precision condition.

On the other hand, general approaches are less likely to learn the coefficients of polynomial filters in a completely free manner~\cite{klicpera2019diffusion,he2021bernnet}.
The specially designed coefficients to explicit modify spectrum, i.e. Personalized PageRank (PPR), heat kernel~\cite{klicpera2019diffusion}, etc or the coefficients learned under the constrained condition, i.e. Chebyshev~\cite{defferrard2016convolutional}, Cayley~\cite{8521593}, Bernstein~\cite{he2021bernnet} polynomial, etc act as the practical applicable filters.
This is probably because the polynomial filter relies on sophisticated coefficients to maintain spectral properties.
Learning them from scratch would easily fall into an ill-posed filter~\cite{he2021bernnet}.
However, by modifying the filter bases, it would relax the requirement on the coefficients, making it more suitable for learning coefficients from scratch.

Finally, although the new architecture in Eq.~\ref{equ:channel_wise_graph_conv} decouples the correlation issue from developing more powerful filters, general filter bases are less qualified for approximating more complex filters. Hence, we still need to explore more effective filter bases to replace existing ones. To this end, we will introduce two different improvements on filter bases in the following sections whose effectiveness will serve as a verification of our analysis.


\subsection{Spectral Optimization on Filter Basis}
\label{sec:basis_optim}

One can directly apply a smoothing function on the spectrum of , which helps to narrow the range of eigenvalues close to 1 or -1.
There can be various approaches to this end, and in this paper, we propose the following eigendecomposition-based method for a symmetric matrix ~\footnote{Although the computation of  requires eigendecomposition,  is always a symmetric matrix and the eigendecomposition on it is much faster than a general matrix.}

where .
, .
 serves as the polynomial bases in Eq.~\ref{equ:channel_wise_filter}.
Unlike general spectral approaches,  is not required to be a bounded spectrum.
It can leverage more bases while alleviating both the diminishing and divergence problems by controlling  in a small range.
Therefore,  can be considered as a basis-augmentation technique as shown in Fig.~\ref{fig:basis_optim}.
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.95\columnwidth]{figure/polynomial_bases}
	\vspace{-5pt}
	\caption{Assume  and .}
	\label{fig:basis_optim}
	\vspace{-10pt}
\end{figure}

There can be other transformations on the spectrum, e.g., , which have a similar effect to .
Note that the injectivity of  also influences the approximation ability, which is discussed in more details in Appendix~\ref{more_filter_basis}.


\subsection{Generalized Normalization on Filter Basis}

Eq.~\ref{equ:basis_optim} directly operates on the spectrum, which can achieve an accurate control on the range of the spectrum but requires eigendecomposition.
To avoid eigendecomposition, we alternatively study the effects of graph normalization on the spectrum.
We generalize the normalized adjacency matrix as follows

where  is the normalization coefficient and  is the shift coefficient.
Widely-used  corresponds to  and .

\begin{proposition}
	\label{prop:basis_norm}
	Let  be the spectrum of  and  be the spectrum of , then for any , we have
	
	where  and  are the minimum and maximum degrees of nodes in the graph.
\end{proposition}
We prove Proposition~\ref{prop:basis_norm} in Appendix~\ref{proof:prop:basis_norm}.
Proposition~\ref{prop:basis_norm} extends the results in \cite{4389477}, showing that the normalization has a scaling effect on the spectrum: a smaller  is likely to lead to a smaller ,
while a larger  is likely to lead to a larger .
When , the upper and lower bounds coincide with .

\begin{figure*}[th]
	\centering
	\includegraphics[width=\textwidth]{figure/eig_visual}
	\vspace{-20pt}
	\caption{We use the metric  to evaluate the shrinking effects of  on the spectrum. We randomly sample 5 graphs in each of three datasets ZINC, MolPCBA and NCI1 respectively. In the first three figures, we use the fixed  on all 5 graphs. In the fourth figure, we use  respectively on one graph, which corresponds to the 5 lines from top to bottom.
	More visualization results on other datasets can be found in Appendix~\ref{spectrum_visualizations}.}
	\label{fig:basis_norm}
	\vspace{-10pt}
\end{figure*}
To further investigate the effects of the normalization on the  spectrum, we fix  and empirically evaluate  as shown in Fig.~\ref{fig:basis_norm}.
When fixing ,  shrinks the spectrum of  with different degrees on different eigenvalues.
For eigenvalues with small magnitudes (in the middle area of the spectrum), it has a small shrinking effect, while for eigenvalues with large magnitudes, it has a relatively large shrinking effect.
Hence,  can be used as a spectral smoothing method.
Also, different  results in different shrinking effects, which is consistent with the results in Proposition~\ref{prop:basis_norm}.
Widely-used  with the spectrum bounded by  may not be a good choice since the diminishing problem.
Intuitively, to utilize more bases, we should narrow the range of the spectrum close to 1 (or -1) to avoid both the diminishing and divergence problems in higher-order bases.
This can vary from different datasets and we should carefully balance  and .

\section{Related Work}

Many improvements on GNNs can be unified into the spectral smoothing operations, e.g. low-pass filter~\cite{pmlr-v97-wu19e,zhu2021interpreting,klicpera_predict_2019,klicpera2019diffusion,chien2021adaptive,balcilar2021analyzing}, alleviating oversmoothing~\cite{chenWHDL2020gcnii,xu2018representation,liu2020towards,li2018deeper}, graph normalization\cite{cai2020graphnorm}, etc, our analysis on the relations of the correlation issue and the spectrum of underlying graph's matrix provides a unified interpretation on their effectiveness.

ChebyNet~\cite{defferrard2016convolutional}, CayleNet~\cite{8521593}, APPNP~\cite{klicpera_predict_2019}, SSGC~\cite{zhu2020simple}, GPR~\cite{chien2021adaptive}, BernNet~\cite{he2021bernnet}, etc explore various polynomial filters and use the normalized adjacency or Laplacian matrix as basis.
We improve the approximation ability of polynomial filters by altering the spectrum of filter bases.
The resulting bases allow leveraging more bases to approximate more sophisticated filters and are more suitable for learning coefficients from scratch.

We note that the concurrent work \cite{jin2022towards} has also pointed out the overcorrelation issue in the infinite depth case, without further discussion on the reason (e.g. graph's spectrum) behind this phenomenon.
In contrast, we show that correlation is inherently caused by the  spectrum of the underlying graph filter, and also quantify this effect with spectral smoothness.
It allows to analyze the correlation across all layers instead of only the theoretical infinite depth.


\section{Experiments}

We conduct experiments on TUDatasets~\cite{yanardag2015deep,KKMMN2016}, OGB~\cite{hu2020open} which involve graph classification tasks and ZINC~\cite{dwivedi2020benchmarking} which involves graph regression tasks.
Then, we evaluate the effects of our proposed new graph convolution architecture and two filter bases.

\subsection{Results}

\begin{table}[h]
	\centering
	\caption{Results on TUDatasets. Higher is better.}
	\label{tab:tu_results}
	\vspace{5pt}
	\resizebox{1.0\columnwidth}{!}{\begin{tabular}{ccccc}
			\toprule
			dataset															  & NCI1                            & NCI109                 & ENZYMES                 & PTC\_MR                 	         \\
			\midrule
			GK			 & 62.490.27       & 62.350.3   & 32.701.20      & 55.650.5                  \\
			RW				&-                               &-                         & 24.161.64     & 55.910.3                             			  \\
			PK           & 82.540.5          &-                         &-                           & 59.52.4                             			 \\
			FGSD						 & 79.80                           & 78.84                    &-                            & 62.8                          			 \\
			AWE                  &-                               &-                         & 35.775.93    &-						                  \\
			DGCNN                      & 74.440.47       &-                  	    & 51.07.29       & 58.592.5                  \\
			PSCN                 & 74.440.5         &-                   		 &-                            & 62.295.7                  \\
			\midrule
			DCNN               & 56.611.04        &-                 		   &-                           &-						                   \\
			ECC            & 76.82                          & 75.03                    & 45.67                      &-                             			  \\
			DGK                     & 80.310.46        & 80.320.3   & 53.430.91   & 60.082.6                  \\
			GraphSag                & 76.01.8        &-           & 58.26.0   &-                  \\
			CapsGNN                & 78.351.55        &-           & 54.675.67   &-                  \\
			DiffPool         &76.91.9                               &-                         & 62.53                      &-                              			\\
			GIN                      			    & 82.71.7             &-                    		&-                           & 64.67.0  \\
			-GNN          & 76.2                            &-                         &-                           & 60.9                              			\\
			\midrule
			Spec-GN         &84.791.63 &\textbf{83.62}\textbf{0.75} &72.505.79  &\textbf{68.05}\textbf{6.41}\\
			Norm-GN          &\textbf{84.87}\textbf{1.68} &83.501.27 &\textbf{73.33}\textbf{7.96} &67.764.52\\
			\bottomrule
		\end{tabular}
	}
	\vspace{-10pt}
\end{table}

\begin{figure*}[th]
	\centering
	\includegraphics[width=0.95\textwidth]{figure/filter_visual_zinc}
	\vspace{-10pt}
	\caption{A visualization of the learned filters on ZINC. We tested on three bases with each basis randomly sampling 9 filters. Dots represent the eigenvalues of each basis. More visualization results on other datasets can be found in Appendix~\ref{filter_visualizations}.}
	\label{fig:filter_visual}
	\vspace{-10pt}
\end{figure*}

\textbf{Settings.}
We use the default dataset splits for OGB and ZINC.
For TUDatasets, we follow the standard 10-fold cross-validation protocol and splits from \cite{zhang2018end} and report our results following the protocol described in \cite{xu2018how,ying2018hierarchical}.
Following all baselines on the leaderboard of ZINC, we control the number of parameters around 500K.
The baseline models include: 
GK~\cite{shervashidze2009efficient},
RW~\cite{vishwanathan2010graph},
PK~\cite{neumann2016propagation},
FGSD~\cite{verma2017hunt},
AWE~\cite{pmlr-v80-ivanov18a},
DGCNN~\cite{zhang2018end},
PSCN~\cite{niepert2016learning},
DCNN~\cite{atwood2016diffusion},
ECC~\cite{simonovsky2017dynamic},
DGK~\cite{yanardag2015deep},
CapsGNN~\cite{xinyi2018capsule},
DiffPool~\cite{ying2018hierarchical},
GIN~\cite{xu2018how},
-GNN~\cite{morris2019weisfeiler},
GraphSage~\cite{hamilton2017inductive},
GAT~\cite{velickovic2018graph},
GatedGCN-PE~\cite{bresson2017residual},
MPNN (sum)~\cite{gilmer2017neural},
DeeperG~\cite{li2020deepergcn},
PNA~\cite{corso2020pna},
DGN~\cite{beani2021directional},
GSN~\cite{bouritsas2020improving},
GINE-{\scriptsize VN}~\cite{brossard2020graph},
GINE-{\scriptsize APPNP}~\cite{brossard2020graph},
PHC-GNN~\cite{le2021parameterized},
SAN~\cite{Kreuzer2021rethinking},
Graphormer~\cite{ying2021transformers}.
Spec-GN denotes the proposed graph convolution in Eq.~\ref{equ:channel_wise_graph_conv} with the smoothed filter basis by spectral transformation in Eq.~\ref{equ:basis_optim}.
Norm-GN denotes the proposed graph convolution in Eq.~\ref{equ:channel_wise_graph_conv} with the smoothed filter basis by graph normalization in Eq.~\ref{equ:basis_norm}.

\begin{table}[h]
	\centering
	\caption{Results on ZINC (Lower is better) and MolPCBA (Higher is better).}
	\label{tab:zinc_pcba_results}
	\vspace{5pt}
	\resizebox{1.0\columnwidth}{!}{\begin{tabular}{c|cc}
			\toprule
			method                & ZINC   & MolPCBA \\
			\midrule
			GCN &0.3670.011  &24.240.34  \\
			GIN &0.5260.051  &27.030.23  \\
			GAT &0.3840.007  &- \\
			GraphSage &0.3980.002  &- \\
			GatedGCN-PE &0.2140.006  &- \\
			MPNN &0.1450.007  &- \\
			DeeperG &- &28.420.43  \\
			PNA &0.1420.010  &28.380.35  \\
			DGN &0.1680.003  &28.850.30  \\
			GSN &0.1010.010  &- \\
			GINE-{\scriptsize VN} &- &29.170.15  \\
			GINE-{\scriptsize APPNP} &- &\textbf{29.79}\textbf{0.30}  \\
			PHC-GNN &- &29.470.26  \\
			SAN &0.1390.006  &- \\
			Graphormer &0.1220.006  &- \\		
			\midrule
			Spec-GN &\textbf{0.0698}\textbf{0.002}  &29.650.28  \\
			Norm-GN  &0.07090.002  &29.510.33  \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-10pt}
\end{table}

\textbf{Results.}
Tab.~\ref{tab:tu_results} and~\ref{tab:zinc_pcba_results} summarize performance of our approaches comparing with baselines on TUDatasets, ZINC and MolPCBA.
For TUDatasets, we report the results of each model in its original paper by default. When the results are not given in the original paper, we report the best testing results given in \cite{zhang2018end,pmlr-v80-ivanov18a,xinyi2018capsule}.
For ZINC and MolPCBA, we report the results of their public leaderboards.
TUDatasets involves small-scale datasets.
NCI1 and NCI109 are around 4K graphs.
ENZYMES and PTC\_MR are under 1K graphs.
General GNNs easily suffer from overfitting on these small-scale data, and therefore we can see that some traditional kernel-based methods even get better performance.
However, Spec-GN and Norm-GN achieve higher classification accuracies by a large margin on these datasets.
The results on TUDatasets show that although Spec-GN and Norm-GN achieve more expressive filters, it does not lead to overfitting on learning graph representations.
Recently, Transformer-based models are quite popular in learning graph representations, and they significantly improve the results on large-scale molecular datasets.
On ZINC, Spec-GN and Norm-GN outperform these Transformer-based models by a large margin.
And on MolPCBA, they are also competitive compared with SOTA results.

\begin{table*}[th]
	\small
	\centering
	\caption{Ablation study results on ZINC with different settings. }
	\label{tab:ablation}
	\vspace{5pt}
	\begin{tabular}{cccccccc}
		\toprule
		\multicolumn{2}{c}{Architecture} & \multicolumn{4}{c}{Basis}  & \multirow{1}{*}{test MAE} & \multirow{1}{*}{valid MAE}\\ \cline{1-2} \cline{3-6}
		shd&idp&&&&& &\\ \hline
		\yes& &\yes& & & & 0.14150.00748 & 0.15680.00729 \\ \hline
		\yes& & &\yes& & & 0.14390.00900 & 0.15690.00739 \\ \hline
		\yes& & & &\yes& & 0.10610.01018 & 0.12940.01454 \\ \hline
		\yes& & & & &\yes& 0.11330.01711 & 0.13160.02057 \\ \hline
		&\yes&\yes& & & & 0.09440.00379 & 0.11000.00787 \\ \hline
		&\yes& &\yes& & & 0.09820.00417 & 0.11720.00666 \\ \hline
		&\yes& & &\yes& & \textbf{0.0698}\textbf{0.00200} & \textbf{0.0884}\textbf{0.00319} \\ \hline
		&\yes& & & &\yes & 0.07090.00176 & 0.09290.00445 \\
		\bottomrule
	\end{tabular}
	\vspace{-10pt}
\end{table*}

\subsection{Ablation Studies}
\label{sec:ablation_studies}

We perform ablation studies on the proposed architecture and the filter bases  (by setting  in Eq.~\ref{equ:basis_optim}) and  on ZINC.
We use ``idp'' and ``shd'' to respectively represent the correlation-free architecture (also known as independent filter architecture) in Eq.~\ref{equ:channel_wise_graph_conv} and the general shared filter architecture in Eq.~\ref{equ:gnn_generalization}.
Both architectures learn the filter coefficients from scratch.

\textbf{Correlation-free architecture and different filter bases.}
In Fig.~\ref{fig:filter_visual}, we visualize the learned filters in the correlation-free on three bases, i.e. ,  and .
The visualizations show that each channel indeed learns a different filter on all three bases.
 has the bounded spectrum  that is slightly close to  due to the involvement of self-loop.
The filters learn a similar response on all range which corresponds to different frequencies in frequency domain.
 and  have the spectrum close to  or  while the filters learn diverse responses on these areas, which corresponds to more complex patterns on different frequencies.
Tab.~\ref{tab:ablation} shows that the correlation-free always outperforms the shared filter by a large margin on all tested bases.
Both  and  have the bounded spectrum  and they have similar performance.
 and  narrow the range of the spectrum close to  or  through completely different strategies, but they have similar performance that is much better than  and .
This validates our analysis on the filter basis.
Meanwhile,  achieves more accurate control on the spectrum, and correspondingly, it slightly outperforms .

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{figure/ablation_zinc}
	\vspace{-10pt}
	\caption{Ablation study results on ZINC with different number of bases .}
	\label{fig:ablation_plot}
	\vspace{-10pt}
\end{figure*}

\textbf{Do more bases gain improvements?}
In Fig.~\ref{fig:ablation_plot}, we systematically evaluate the effects of the number of bases on learning graph representations, including  and our  with .
The shared filter case, i.e. shd cannot well leverage more bases (a larger ) as the MAE stops decreasing at  which is also reported by several baselines in Tab.~\ref{tab:zinc_pcba_results}.
In contrast, both correlation-free cases idp and idp outperform the shared filter case by a large margin and they continuously gain improvements when increasing .
The MAE of idp stops decreasing at the test MAE close to 0.09 and the valid MAE close to 0.11.
By replacing  with , the best test MAE is below 0.07, and the best valid MAE is close to 0.088.
The bases in  are controlled by both  and .
We use the tuple  to denote a combination of  and .
By fixing , the curves corresponding to  and  show that increasing  gains improvements.
By fixing the upper bound of  to be 1,  involves 3 more bases than  and outperforms .
The same results are also reflected in the comparison of  and .
For the comparison of  and , both settings achieve the lowest MAE and the difference is less obvious.

\begin{figure*}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{figure/ablation_deep}
	\vspace{-10pt}
	\caption{Ablation study results on ZINC with different number of layers.}
	\label{fig:zinc_depth}
	\vspace{-10pt}
\end{figure*}

\textbf{The effects of model depth.}
Fig.\ref{fig:zinc_depth} shows the performance comparisons between correlation-free and shared filter as depth increases.
Each architecture is tested with the default basis  and our proposed .
We set the same number of bases in all resulting models, and each model is tested with the number of layers (depth) equal to .
The results show that the correlation-free can preserve the performance as depth increases.
The shared filter cases perform quite unstable and drop dramatically when depth . Also, across all depths, the correlation-free almost always outperforms the shared filter and has low variance among different runs.
In Appendix~\ref{ablate_deep}, we also test cosine similarities of different layers in a deep model.

\textbf{Stability.}
We also found that the correlation-free is more stable in different runs than the shared filter case as reflected in the standard deviation in Tab.~\ref{tab:ablation}.
This is probably because different channels may pose different patterns, which causes interference among each other in the shared filter case. While the correlation-free well avoids this problem.
Also, the results of  and  are more stable than  and  in different runs.
For  and , the difference between the best and the worst runs can be more than 0.02.
While for  and , this difference is less than 0.01.
More results are given in Appendix~\ref{more_results}.
The instability of  and  is probably because learning filter coefficients from scratch without any constraints is difficult to maintain spectrum properties and therefore easily falls into an ill-posed filter~\cite{he2021bernnet}.
In contrast,  and  inherently with smoother spectrum alleviate this problem and make them more appropriate in the scenario of learning coefficients from scratch.


\section{Conclusion}

We study the effects of spectrum in GNNs. It shows that in existing architectures, the unsmooth spectrum results in the correlation issue, which acts as the obstacle to developing deep models as well as applying more powerful graph filters. Based on this observation, we propose the correlation-free architecture which decouples the correlation issue from filter design. Then, we show that the spectral characteristics also hinder the approximation abilities of polynomial filters and address it by altering the graph's spectrum.
Our extensive experiments show the significant performance gain of correlation-free architecture with powerful filters.

\section*{Acknowledgments}

This work is supported in part by the National Key Research and Development Program of China (no. 2021ZD0112400), and also in part by the National Natural Science Foundation of China under grants U1811463 and 62072069.


\nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2022}



\newpage
\appendix
\onecolumn



\section{Derivations of Eq.~\ref{equ:cos_signal} and Eq.~\ref{equ:cos_signal_eigenvalue}}
\label{deriv:equ:cos_signal_eigenvalue}

Since  is a symmetric matrix, assume the eigendecomposition  with  and .





\section{Proof of Proposition~\ref{prop:cos_convergence}}
\label{proof:prop:cos_convergence}

\begin{proof}
	(i)
	As  and Eq.~\ref{equ:cos_signal_eigenvalue}, for , we have
	
	Similarly, we can prove that .
	
	(ii)
	Since  monotonously increases with respect to  and has the upper bound 1,  must be convergent.
	
	As , we have  and the convergence speed is decided by .
	Therefore .
	
	
	Then, 
	
\end{proof}


\section{More Discussions of Spectral Optimization on Filter Basis}
\label{more_filter_basis}

We use  to denote the eigenspace of  associated with  such that .

\begin{proposition}
	\label{prop:eig_mapping}
	Given a symmetric matrix  with  where , and  can be any eigenbasis of ,
	let , where  is an entry-wise function applied on . Then we have \\
	(i) ; \\
	(ii) Meanwhile, if  is injective,  and  is injective.
\end{proposition}

\begin{proof}
	
	Let .
	 is equivalent to .
	For any , the geometric multiplicity of any  is equal to its algebraic multiplicity, and .
	 and .
	Similarly, for any , .
	Note that  for any .
	Hence .
	As a result,  for any .
	
	If  is injective,  for any .
	Thus .
	
	We use  to denote the generalisation of the set of all eigenvalues of  (Slso known as the spectrum of ).
	Let  and .
	Suppose ,
	to prove , we discuss two cases respectively.
	
	\textbf{Case 1:} 
	
	Then .
	The characteristic polynomials of  and  are different.
	Therefore, .
	
	\textbf{Case 2:} 
	
	Then .
	We prove the equivalent proposition "".
	If , .
	For any  with geometric multiplicity , we can find the corresponding eigenvectors  according to .
	Similarly, we can find the corresponding eigenvectors  according to .
	Note that the eigen-decomposition is unique in terms of eigenspaces.
	Thus, .
	Therefore, for any ,  (As given in Proposition~\ref{prop:eig_mapping}).
	Correspondingly, .
	
\end{proof}

Proposition~\ref{prop:eig_mapping} shows that the eigenspace of  involves the eigenspace of .
Therefore,  is invariant to the choice of eigenbasis, i.e.,  for any eigenbases  and  of .
Hence,  is unique to  for a given .
Consistently, we denote the mapping .

When  is injective,  and  share the same algebraic multiplicity.
Otherwise,  has a larger algebraic multiplicity on the corresponding eigenvalues, which may weaken the approximation ability based on the understanding of Vandermonde matrix.
Also, the injectivity of  serves as a guarantee that the transformation is reversible with no information loss.

 is also equivariant to graph isomorphism.
For any two graphs  and  with matrix representations  and  (e.g., adjacency matrix, Laplacian matrix, etc.),  and  are isomorphic if and only if there exists a permutation matrix  such that .
We denote .
Then
\begin{claim}
	 is equivariant to graph isomorphism, i.e. .
\end{claim}
\begin{proof}
	
\end{proof}
Hence, for a specific GNN model , .
The learned representation is invariant to graph isomorphism (also known as permutation invariance~\cite{NIPS2017_f22e4747,murphy2018janossy}) when introducing .

\section{Proof of Proposition~\ref{prop:basis_norm}}
\label{proof:prop:basis_norm}

\begin{proof}
	Let .
	According to Courant-Fischer theorem,
	
	Let . As the change of variables  is non-singular, this is equivalent to
	
	Therefore,
	
	Similarly, we can prove .
\end{proof}

\FloatBarrier
\section{Visualizations of the Effects of the Normalization  on the Spectrum}
\label{spectrum_visualizations}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{figure/eig_visual_appendix}
	\vspace{-10pt}
	\caption{We randomly sample 5 graphs in each of three datasets NCI109, ENZYMES and PTC\_MR respectively. And we use the fixed  to see the effects of the normalization on all graphs.}
	\label{fig:spectrum_visualizations}
\end{figure}

\FloatBarrier
\section{Visualizations of the Learned Filters}
\label{filter_visualizations}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{figure/filter_visual_molpcba}
	\includegraphics[width=0.95\textwidth]{figure/filter_visual_nci1}
\end{figure}
\begin{figure}[h]
	\includegraphics[width=0.95\textwidth]{figure/filter_visual_nci109}
	\includegraphics[width=0.95\textwidth]{figure/filter_visual_enzymes}
	\includegraphics[width=0.95\textwidth]{figure/filter_visual_ptc_mr}
	\caption{Visualizations of the learned filters on MolPCBA, NCI1, NCI109, ENZYMES and PTC\_MR.}
	\label{fig:filter_visualizations1}
\end{figure}

\FloatBarrier
\section{The Correlation Issue of Deep Models}
\label{ablate_deep}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figure/ablation_cosine}
	\vspace{-10pt}
	\caption{Cosine similarities on ZINC.}
	\label{fig:zinc_cosine}
\end{figure}
We test the absolute value of cosine similarities in different layers for a depth=25 model. For each graph, we compute the mean of all hidden signal pairs. The final visualized results in Fig.\ref{fig:zinc_cosine} are the mean of all graphs within a randomly selected batch. To be consistent with the definition of spectral graph convolution as well as our correlation analysis, the test runs do not utilize edge features of ZINC.

The results show that on both bases, the cosine of the shared filter case converges to 1, while the correlation-free converges to  for  and  for .
(We also found that it easily leads to a large cosine similarity on ZINC, which is mainly because graphs are small such that , where  is the number of nodes and  is the number of hidden features.)
These results do show that general GNNs suffer from the correlation issue as depth increases, while our correlation-free architecture enjoys a relatively stable performance.

\FloatBarrier
\section{More Results}
\label{more_results}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{figure/zinc_basis9}
	\includegraphics[width=0.95\textwidth]{figure/zinc_basis18}
\end{figure}
\begin{figure}[h]
	\includegraphics[width=0.95\textwidth]{figure/zinc_basis21}
	\includegraphics[width=0.95\textwidth]{figure/zinc_basis24}
	\caption{The curves of 5 runs on ZINC with the number of basis .}
	\label{fig:more_results1}
\end{figure}


\end{document}
