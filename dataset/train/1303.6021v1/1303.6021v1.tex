\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1

\usepackage{wacv}
\usepackage{times}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{float}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{tabularx}
\usepackage{url}
\usepackage{xspace}
\usepackage{balance}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false,urlcolor=blue,linkcolor=blue,citecolor=blue,pdftitle={Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition},pdfauthor={Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell}]{hyperref}


\newcommand{\alg}[1]{\mbox{Algorithm~\ref{#1}}}
\newcommand{\fig}[1]{\mbox{Fig.~\ref{#1}}}
\newcommand{\tab}[1]{\mbox{Table~\ref{#1}}}
\newcommand{\todo}[1]{{\color{red}{\bf [TODO:} {\it{#1}}{\bf ]}}}

\newcommand{\rh}[1]{\rotatebox{66}{\bf #1}}
\newcolumntype{x}{>{\centering}X}

\newcommand{\eqsize}{\footnotesize}
\newcommand{\cov}{{Cov3D}}
\newcommand{\wrlpp}{WRLPP}
\newcommand{\vect}[1]{{\boldsymbol{#1}}}
\newcommand{\mat}[1]{{\boldsymbol{#1}}}
\newcommand{\tens}[1]{#1}

\floatstyle{ruled}
\newfloat{algorithm}{!tb}{loa}
\floatname{algorithm}{Algorithm}


\wacvfinalcopy \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\def\TODO#1{ \textcolor{red}{{\bf [TODO:} {\it #1}{\bf ]}}}
\def\FIX#1{ \textcolor{blue}{{\bf [FIX:} {\it #1}{\bf ]}}}

\pagestyle{empty}
\begin{document}

\title{Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition}

\author
  {
  {\it Andres~Sanin, Conrad~Sanderson, Mehrtash~T.~Harandi, Brian~C.~Lovell}\\
  ~\\
  NICTA, PO Box 6020, St Lucia, QLD 4067, Australia~\thanks
    {{\bf Published~in:} IEEE Workshop on Applications of Computer \mbox{Vision}, pp.~103--110, 2013.
    \href{\bf http://dx.doi.org/10.1109/WACV.2013.6475006}{http://dx.doi.org/10.1109/WACV.2013.6475006}.
    {\tiny \mbox{\bf Acknowledgements:} NICTA is funded by the Australian Government via the {\it Department of Broadband, Communications and the Digital Economy}, and the Australian Research Council through the {\it ICT Centre of Excellence} program.}}
  \\
  University of Queensland, School of ITEE, QLD 4072, Australia}

\maketitle
\thispagestyle{empty}

\begin{abstract}
\vspace{-2ex}

\noindent
We propose a new action and gesture recognition method based on spatio-temporal covariance descriptors
and a weighted Riemannian locality preserving projection approach
that takes into account the curved space formed by the descriptors.
The weighted projection is then exploited during boosting to create
a final multiclass classification algorithm that employs the most useful spatio-temporal regions.
We also show how the descriptors can be computed quickly through the use of integral video representations.
Experiments on the UCF sport, CK+ facial expression and Cambridge hand gesture datasets
indicate superior performance of the proposed method
compared to several recent state-of-the-art techniques.
The proposed method is robust and does not require additional processing of the videos,
such as foreground detection, interest-point detection or tracking. 







\end{abstract}
\vspace{-1ex}
\section{Introduction}
\label{sec:introduction}

Video-based classification plays a key role in human motion analysis fields such as action and gesture recognition.
Both fields have shown promising applications in many areas,
including security and surveillance, content-based video analysis,
human-computer interaction and animation.
According to a recent survey on recognition of human activities~\cite{TuragaEtAl2008},
the focus has shifted to methods that do not rely on human body models,
where the information is extracted directly from the images
and hence being less dependent on reliable segmentation and tracking algorithms.
Such image representation methods can be categorised into 
global and local based approaches~\cite{Poppe2010}.

Methods with global image representation encode visual information as a whole.
Ali and Shah~\cite{AliAndShah2010} extract a series of kinematic features based on optical flow.
A group of kinematic modes is found using principal component analysis.
Guo \etal~\cite{GuoEtAl2010} encode the same kinematic features using sparse representation of covariance matrices.
Several methods first divide the region of interest into a fixed spatial or temporal grid,
extract features inside each cell and then combine them into a global representation.
For example, this can be achieved using local binary patterns (LBP)~\cite{KellokumpuEtAl2008},
or histograms of oriented gradients (HOG)~\cite{ThurauAndHlavac2008}.
Global representations are sensitive to viewpoint, noise and occlusions which may lead to unreliable classification.
Furthermore, global representations depend on reliable localisation of the region of interest~\cite{Poppe2010}.

Local representations are designed to deal with the abovementioned issues by describing the visual information as a collection of patches,
usually at the cost of increased computation.
Laptev and Lindeberg~\cite{LaptevAndLindeberg2003} extract interest points using a 3D Harris corner detector
and use the points for modelling the actions.
One of the major drawbacks is the low number of interest points that are able to remain stable across an image sequence.
A common solution is to work with windowed data,
extracting salient regions which can be represented using Gabor filtering~\cite{DollarEtAl2005}.

Wang \etal~\cite{WangEtAl2009} showed that dense sampling approaches tend to perform better compared to interest point based approaches. Dense
sampling is typically done for a set of patches inside the region of interest. Features are extracted from each patch to form a descriptor. These
descriptor representations differ from grid-based global representations in that they can have an arbitrary position and size, and that the patches
are not combined to form a single representation but form a set of multiple representations.
Examples are HOG and HOF (histogram of oriented flow) descriptors~\cite{LaptevEtAl2008},
SIFT descriptors~\cite{Lowe2004},
and their respective spatio-temporal versions, HOG3D~\cite{WangEtAl2009} and 3D SIFT~\cite{ScovannerEtAl2007}.
Because of the likely large number of descriptors and/or their high dimensionality, comparing sets of
descriptors is often not straightforward.
This has led to compressed representations such as formulating sets of descriptors as bags-of-words~\cite{NieblesEtAl2008}.

In this paper we propose the use of spatio-temporal covariance descriptors for action and gesture recognition tasks.
Flat region covariance descriptors were first proposed for the task of object detection and classification in images~\cite{TuzelEtAl2008}.
Each covariance descriptor represents the features inside an image region as a normalised covariance matrix.
They have led to improved results over related descriptors such as HOG,
in terms of detection performance as well as robustness to translation and scale~\cite{TuzelEtAl2008}.
Furthermore, covariance matrices provide a low dimensional representation
which enables efficient comparison between sets of covariance descriptors.

The proposed spatio-temporal descriptors, which we name \cov,
belong to the group of symmetric positive definite matrices which do not form a vector space.
They can be formulated as a connected Riemannian manifold,
and taking into account the non-linear nature of the space of the descriptors may lead to improved classification results.
The most common approach for classification on manifolds is to first map the points
into an appropriate Euclidean representation~\cite{LinAndZha2008}
and then use traditional machine learning methods. 
A recent example of mapping is the Riemannian locality preserving projection (RLPP) technique~\cite{HarandiEtAl2012}.

The \cov~descriptors are extracted from spatio-temporal windows inside sample videos,
with the number of possible windows being very large.
As such, we use a boosting approach to search the windows to find a subset which is the most useful for classification.
We propose to extend RLPP by weighting (WRLPP), in order to take into account the weights of the training samples.
This weighted projection leads to a better representation of the neighbourhoods around the most critical training samples during each boosting iteration.
The proposed \cov~descriptors, in conjunction with the classification approach based on WRLPP boosting,
lead to a state-of-the-art method for action and gesture recognition.

We continue the paper as follows.
In Section~\ref{sec:cov3d} we describe the spatio-temporal covariance descriptors,
and use the concept of integral video to enable fast calculation inside any spatio-temporal window.
In Section~\ref{sec:method}, we first overview the concept of Riemannian manifolds formulated in the context of positive definite symmetric matrices,
and then detail the proposed boosting classification approach based on weighted Riemannian locality preserving projection.
In Section~\ref{sec:experiments}, we compare the performance of the proposed method against several recent state-of-the-art methods
on three benchmark datasets.
Concluding remarks and possible future directions are given in Section~\ref{sec:discussion}.
\begin{figure}[!b]
  \centering
  \includegraphics[width=1\columnwidth]{figures/cov3d}
  
  ~
  
  \caption
    {
    \small
    Conceptual demonstration for obtaining a {\cov} spatio-temporal covariance descriptor.
    A~spatio-temporal window {\eqsize } is defined inside the input video.
    For each pixel in {\eqsize } a feature vector {\eqsize } is calculated.
    The feature vectors are then used to compute the covariance matrix {\eqsize }.
    }
  \label{fig:cov3d}
\end{figure}


\section{\cov~Descriptors}
\label{sec:cov3d}

In this section we first present the general form of the proposed spatio-temporal covariance descriptors (\cov),
an algorithm for their fast calculation,
and finally how they can be specialised for action and gesture recognition.
For convenience, we follow the notation in~\cite{TuzelEtAl2008}.

Let {\eqsize } be the sequence of images {\eqsize }
and {\eqsize } be the {\eqsize } dimensional feature video extracted from {\eqsize }:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where the function {\eqsize } can be any mapping such as intensity, colour, gradients, or optical flow.
For a given spatio-temporal window {\eqsize },
let {\eqsize } be the {\eqsize }-dimensional feature vectors inside {\eqsize }.
The region {\eqsize } is represented with the
{\eqsize } covariance matrix of the feature vectors:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where {\eqsize } is the mean of the points.
\fig{fig:cov3d} shows the construction of a covariance descriptor inside a spatio-temporal window.
Examples of feature vectors specific for action and gesture recognition are given in Section~\ref{sec:features}.

Representing a spatio-temporal window with a covariance matrix has several advantages:
{\bf (i)}~it is a low-dimensional representation which is independent on the size of the window,
{\bf (ii)}~the impact of noisy samples is reduced through the averaging during covariance computation,
{\bf (iii)}~it is a straightforward method of fusing correlated features.




\subsection{Fast computation}
\label{sec:integral}

Integral images are an intermediate image representation used for the fast calculation of region sums~\cite{ViolaAndJones2001}.
The concept has been extended to image sequences~\cite{KeEtAl2005},
where the integral images are stacked to form an integral video,
and can be used to compute spatio-temporal region sums in constant time.
For a video {\eqsize }, its integral video {\eqsize } is defined as:

\vspace{-1ex}
\eqsize

\normalsize

Tuzel \etal~\cite{TuzelEtAl2008} used the integral image representations for fast calculation of flat region covariances.
Here we extend the idea for fast calculation of covariance matrices inside a spatio-temporal window using the integral video representation.
The {\eqsize }-th element of the covariance matrix defined in \eqref{eq:cov} can be expressed as:

\noindent
\eqsize

\normalsize

\noindent
where {\small } refers to the {\small }-th element of the {\small }-th vector.
To find the covariance in a given spatio-temporal window {\eqsize }, we have to compute the sum of each feature dimension, {\eqsize
}, as well as the sum of the multiplication of any two feature dimensions, {\eqsize }.
With {\eqsize } representing the number of dimensions,
the covariance of any spatio-temporal window can be computed in {\eqsize } time, as follows.

We need to compute a total of {\eqsize } integral videos.
Let {\eqsize } be the {\eqsize } tensor of the integral videos:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where {\small } is the {\small }-th element of vector {\small }.
Furthermore, let {\small } be the
 {\small } tensor of the second-order integral videos:

\vspace{-2ex}
\eqsize

\normalsize

\noindent
for {\small }.
The complexity of calculating the tensors is {\small }.
The {\small }-dimensional feature vector {\small }
and the {\small } dimensional matrix {\small }
can be obtained from the above tensors using:

\noindent
\eqsize

\normalsize

Let {\eqsize }
be the spatio-temporal window of points
{\eqsize },
as shown in \fig{fig:integral}.
The covariance of the spatio-temporal window bounded by {\eqsize } and {\eqsize } is:

\vspace{-2ex}
\eqsize

\normalsize

\noindent
where {\eqsize }.
Similarly, after a few rearrangements,
the covariance of the region {\eqsize } can be computed as:

\noindent
\eqsize

\normalsize

\noindent
where
\vspace{-3ex}

\noindent
\eqsize

\normalsize

\noindent
and {\eqsize }.

\begin{figure}[!b]
  \vspace{-3ex}
  \centering
  \includegraphics[width=0.85\columnwidth]{figures/integral}
\caption
    {
    \small
    Integral feature video.
    The spatio-temporal window {\eqsize } is bounded by {\eqsize } and {\eqsize }.
    Each point in {\eqsize } is a {\eqsize } dimensional vector,
    where {\eqsize } is the number of features.
    }
  \label{fig:integral}
\end{figure}







\subsection{Features and regions}
\label{sec:features}
\vspace{-1ex}

Commonly used features for action and gesture recognition include intensity gradients and optical flow.
Previous studies have shown the benefit of combining both types of features~\cite{DollarEtAl2005,WangEtAl2009}.
We define the feature mapping {\eqsize },
present in \eqref{eq:feature_mapping},
as the following combination of gradient and optical-flow based features,
extracted from pixel location {\eqsize }:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where
\vspace{-2ex}

\noindent
\eqsize

\normalsize

The first four gradient based features in \eqref{eq:gradients} represent the first and second order intensity gradients at pixel location {\eqsize
}. The last two gradient based features correspond to the gradient magnitude and gradient orientation. The optical-flow based features in
\eqref{eq:flow} represent, in order: the horizontal and vertical components of the flow vector,
the first order derivatives of the flow components with respect to {\eqsize },
and the spatial divergence and vorticity of the flow field as defined in~\cite{AliAndShah2010}.
Each descriptor is hence a {\eqsize } matrix, as {\eqsize } has {\eqsize } dimensions.

For reliable recognition, several regions (and hence several descriptors) are typically used.
\fig{fig:stwins} shows the spatio-temporal windows of two descriptors which can be used for recognition of face expressions.
With the defined mapping, the input video {\small } is mapped to {\small }, a {\small }-dimensional feature video.
Since the cardinality of the set of spatio-temporal windows {\eqsize } is very large,
we only consider windows of a minimum size and increment their location and size by a minimum interval value.
Further specifics on the windows used in the experiments are given in Section~\ref{sec:experiments}.

\begin{figure}[!b]
  \centering
  \begin{minipage}{1\columnwidth}
    \begin{minipage}{0.4\columnwidth}
      \includegraphics[width=\columnwidth]{figures/stwins}
    \end{minipage}
    \hfill
    \begin{minipage}{0.55\columnwidth}
      \caption
        {
        \small
        Two examples of \cov~windows that, together, can be useful for the recognition of face expressions.
        }
      \label{fig:stwins}
      ~
    \end{minipage}
  \end{minipage}
\end{figure}

Following~\cite{TuzelEtAl2008}, each covariance descriptor {\eqsize },
is normalised with respect to the covariance descriptor of the region containing the full feature video, {\eqsize },
to improve the robustness against illumination variations:


\noindent
\eqsize

\normalsize

\noindent
where {\small } is equal to {\small } at the diagonal entries and the rest is set to zero.
\section{Classification of Actions and Gestures}
\label{sec:method}

The \cov~descriptors are symmetric positive definite matrices of size \mbox{\eqsize },
which can be formulated as a connected Riemannian manifold ({\eqsize })~\cite{harandi_eccv_2012}.
In this section we first briefly overview Riemannian manifolds,
followed by describing the proposed weighted Riemannian locality preserving projection (WRLPP)
that allows mapping from Riemannian manifolds to Euclidean spaces.
We then describe a classification algorithm that uses WRLPP.



\subsection{Riemannian manifolds}
\label{sec:manifolds}

A manifold can be considered as a continuous surface lying in a higher dimensional Euclidean space.
Formally, a manifold is a topological space which is locally similar to an Euclidean space~\cite{TuzelEtAl2008}.
Intuitively, the tangent space {\eqsize } is the plane tangent to the surface of the manifold at point~{\eqsize }.

A point {\eqsize } on the manifold can be mapped to a vector in the tangent space {\eqsize }
using the logarithm map operator {\eqsize }.
For~{\eqsize } the logarithm map is defined as:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where {\eqsize } is the matrix logarithm operator.
Given the eigenvalue decomposition of a symmetric matrix,
\mbox{\eqsize },
the matrix logarithm can be computed via:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where {\small } is a diagonal matrix,
with each diagonal element equal to the logarithm of the corresponding element in {\small }.

The minimum length curve connecting two points on the manifold is called the geodesic,
and the distance between two points is given by the length of this curve.
Geodesics are related to the tangents in the tangent space.
For {\eqsize }, the distance between two points on the manifold can be found via:

\noindent
\eqsize

\normalsize



\subsection{Weighted RLPP}
\label{sec:wrlpp}





The usual approach for classification on manifolds
is to first map the points into an appropriate Euclidean representation~\cite{TuzelEtAl2008}
and then use traditional machine learning methods.
Points in the manifold can be mapped into a fixed tangent space (such as  where  is the identity matrix) \cite{GuoEtAl2010}.
Since distances in the manifold are only locally preserved in the tangent space,
better results can be achieved by considering the tangent space at the Karcher mean,
the point which minimises the distances among the samples, as shown in~\cite{TuzelEtAl2008}.
Improved results have been obtained by considering multiple tangent spaces~\cite{Lui2010,SaninEtAl2012}.
A more complex approach involves using training data to create a mapping that tries to preserve
the relations between points, such as the RLPP approach~\cite{HarandiEtAl2012}.



RLPP is based on Laplacian eigenmaps~\cite{BelkinAndNiyogi2003}.
Given {\eqsize } training points
\mbox{\eqsize }
from the underlying Riemannian manifold~{\eqsize },
the local geometrical structure of~{\eqsize } can be modelled by building an adjacency graph~{\eqsize }.
The simplest form of {\eqsize } is a binary graph obtained based on the nearest neighbour properties of Riemannian points:
two nodes are connected by an edge if one node is among the  nearest neighbours of the other node.
From the adjacency graph {\eqsize } we can find the degree and Laplacian matrices, respectively:

\vspace{-1ex}
\noindent
\eqsize

\normalsize

\noindent
where the degree
matrix {\eqsize } is a diagonal matrix of size \mbox{\eqsize },
with diagonal entries indicating the the number of edges of each node in the adjacency graph.

RLPP also uses a heat pseudo-kernel matrix {\eqsize },
with the {\eqsize }-th element constructed via:

\noindent
\eqsize

\normalsize

\noindent
where {\eqsize } is the geodesic distance defined in~\eqref{eq:geodesic}.

The final mapping can be found through the following generalised eigenvalue problem~\cite{HarandiEtAl2012}:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where the eigenvectors with the {\eqsize } smallest eigenvalues form the projection matrix {\eqsize }.

The number of possible \cov~descriptors inside a sample video is very large.
As such, we elected to use boosting to search for a subset of the best descriptors for classification.
We could use the original RLPP mapping approach to map the matrices as vectors at each boosting iteration.
However, as shown in~\cite{TuzelEtAl2008}, the sample weights can be used to generate a mapping
which is more appropriate for the critical training samples.
Therefore, we propose a modified projection, specifically designed to be used during boosting,
which uses sample weights to generate the final mapping.
We refer to this approach as weighted Riemannian locality preserving projection (\wrlpp).

In the modified projection,
the adjacency graph~{\eqsize } is replaced 
with a weighted adjacency graph~{\eqsize },
defined as:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where {\eqsize } is a diagonal matrix with diagonal values
that correspond to the vector of sample weights {\eqsize }.
Using the weighted adjacency graph, edges involving critical samples
(ie.~samples with higher weights) become more important and their geometrical structure is better preserved.
The modified projection approach is detailed in~\alg{alg:wrlpp}.

\begin{algorithm}
  \footnotesize
  \raggedright
  \caption{{\bf :} {\footnotesize obtaining weighted RLPP}}
  \label{alg:wrlpp}
  
  \textbf{Input:} Training samples (covariance matrices), labels and weights\\
                  ~~~~~~~~~~~ , \\
  
  \begin{itemize} \leftskip-1em
    \item
      Create Riemannian pseudo-kernel matrix:\\
       ~~~ using \eqref{eq:geodesic} as 
               
    \item
      Construct weighted adjacency graph:\\
      
    \item Obtain the weighted degree  diagonal matrix:\\
               
    \item Calculate the weighted Laplacian matrix:\\
          
    \item The eigenvectors with the  smallest eigenvalues of the Rayleigh quotient  form the projection matrix .
  \end{itemize}
  
  \textbf{Output:} Projection model 
\end{algorithm}

Once the the projection matrix {\eqsize } has been obtained,
a given point {\eqsize } (a {\cov} matrix) on the manifold can then be mapped to Euclidean space via:

\vspace{-1ex}
\eqsize

\normalsize


\noindent
where \mbox{\eqsize },
with {\eqsize } defined in \eqref{eq:kernel_matrix},
and {\eqsize } representing the training points.








\subsection{Classification}
\label{sec:classifier}

As mentioned in the preceding section,
we have chosen to use boosting to find a subset of the best descriptors for classification,
as the number of possible \cov~descriptors inside a sample video is large.
For simplicity, we used a combination of one-vs-one LogitBoost classifiers~\cite{FriedmanEtAl2000}
to achieve multiclass classification.

We start with a brief description of binary LogitBoost classification, with class labels {\eqsize }.
The probability of sample {\eqsize } belonging to class {\eqsize } is represented by:

\vspace{-1ex}
\eqsize

\normalsize

\noindent
where {\eqsize },
with {\eqsize } representing a weak learner.

The LogitBoost algorithm learns a set of {\eqsize } weak learners
by minimising the negative binomial log likelihood of the data.
A weighted least squares regression {\eqsize } of training points {\eqsize } is fitted to response values {\eqsize }, with weights {\eqsize },
where

\noindent
\eqsize

\normalsize

As we are using \cov~descriptors (covariance matrices) as input data,
we adapt the weak learners {\eqsize } to use the projected descriptors.
In other words, {\eqsize } is replaced with {\eqsize },
with {\eqsize } representing a covariance matrix.


For every unique pair of classes, we train a one-vs-one LogitBoost classifier as follows.
Only the samples belonging to the pair of classes are used for training the binary classifier.
One class is selected to be the positive class and the other as the negative class.
For each boosting iteration, we search for the region whose {\cov} descriptor best separates positive from negative samples. The descriptor is
calculated for all the training samples and mapped to vector space with \wrlpp, using the sample weights calculated for the current boosting
iteration. Once in vector space, we fit a linear regression and use it as the weak LogitBoost classifier.

To prevent overfitting, the number of weak classifiers on each one-vs-one classifier
is controlled by a probability margin between the last accepted positive and the last rejected negative.
Both margin samples are determined by the target detection rate ({\eqsize }) and the target false positive rejection rate ({\eqsize }).
The final multiclass classifier is a set of one-vs-one classifiers.
Each one-vs-one classifier {\eqsize }, where {\eqsize } and {\eqsize } are the labels of its two classes, has a positive class
{\eqsize } and a threshold {\eqsize }. The positive class is the label of the class deemed to be positive and the threshold
is found via boosting.
\alg{alg:logitboost} summarises the training process.

\begin{algorithm}
  \footnotesize
  \raggedright
  \caption{{\bf :} {\footnotesize Boosting with WRLPP}}
  \label{alg:logitboost}
  
  \textbf{Input:} Training videos with labels  belonging to  classes\\
  
  \begin{itemize} \leftskip-1em
    \item For each unique pair of class labels  train the one-vs-one classifier 
      \begin{itemize} \leftskip-1em
        \item Let  be the positive class label and restrict the training set to
              
        \item Let either  or  be the positive label 
        \item Create binary labels 
        \item Start with , , , 
        \item Repeat while 
          \begin{itemize} \leftskip-1em
            \item Compute the response values and weights , 
            \item For each spatio-temporal window 
              \begin{itemize} \leftskip-2em
                \item Construct the descriptors 
                \item From  obtain the projection model  using \alg{alg:wrlpp}
                \item Map the data points  using~\eqref{eq:wrlpp}
                \item Fit function  by weighted least-squares regression of  to  using weights 
              \end{itemize}
            \item Update , where  is the best classifier among  which
                  minimises the negative binomial log-likelihood 
            \item Update 
            \item Sort positive and negative samples according to descending probabilities and find samples at the decision boundaries
                  -th , -th , where  and  are the desired detection and false positive
                  rejection rates
            \item 
          \end{itemize}
        \item Store , threshold  and positive label 
      \end{itemize}
  \end{itemize}
  
  \textbf{Output:} A set of  one-vs-one classifiers
\end{algorithm}

A sample video {\eqsize } is classified as follows.
Given a one-vs-one classifier {\eqsize },
the probability of a sample video {\eqsize } belonging to the positive class {\eqsize } is evaluated using:

\vspace{-2ex}
\eqsize

\normalsize

After evaluating {\eqsize } with all the one-vs-one classifiers in the set,
the sample is labelled as the class {\small } which maximises:

\vspace{-2ex}
\eqsize

\normalsize

\noindent
where {\eqsize } is {\eqsize }
if {\small } is the positive class {\small },
or {\small } otherwise.
In other words, {\eqsize } is labelled as the class with greater probability sum,
selecting all the one-vs-one classifiers that evaluate to that class.
\section{Experiments}
\label{sec:experiments}


We compared the performance of the proposed algorithm against 
baseline approaches as well as several state-of-the-art methods.
We used three benchmark datasets, with an overview of the datasets shown in \tab{tab:datasets}.

In the following subsections,
we first present an evaluation of several Riemannian to Euclidean space mapping approaches,
justifying the use of the weighted RLPP.
We then follow with experiments showing the performance on sport actions,
facial expressions and hand gestures.

Unless otherwise stated, no pre-processing was performed in the input sequences
and all the recognition results were obtained using 5-fold cross validation to divide the samples into training and testing sets.


In all cases we used the following parameters:
0.95 detection rate, 0.95 false positive rejection rate, 0.5 margin.
Furthermore, since the search space of spatio-temporal windows is very large,
we restricted the minimum size of the windows,
as well as the minimum increment on location and size of the windows, to {\small } of the frame size.



\begin{table}[!tb]
  \centering
  \footnotesize
  \begin{tabular}{lccc}
    \toprule
    \hspace{-2ex}
    {\bf Dataset}
    &\begin{minipage}{14ex}
      \centering
\includegraphics[width=\columnwidth]{figures/dataset_ucf_subset.png}
      {\bf UCF~\cite{RodriguezEtAl2008}}
    \end{minipage}
    &\begin{minipage}{14ex}
      \centering
\includegraphics[width=\columnwidth]{figures/dataset_ck_subset.png}
      {\bf CK+~\cite{LuceyEtAl2010}}
    \end{minipage}
    &\begin{minipage}{14ex}
      \centering
\includegraphics[width=\columnwidth]{figures/dataset_hands_subset.png}
      \mbox{\bf Cambridge~\cite{KimAndCipolla2009}}
    \end{minipage}\\
    \midrule[\heavyrulewidth]
\hspace{-1ex}\bf{Type}          &sports           &facial expressions &hand gestures \\
    \hspace{-1ex}\bf{Classes}       &10               &7                  &9\\
    \hspace{-1ex}\bf{Subjects}      &---              &123                &2\\
    \hspace{-1ex}\bf{Scenarios}     &---              &---                &5\\
    \hspace{-1ex}\bf{Video samples}\hspace{-10ex}
                       &150              &593                &900\\
    \hspace{-1ex}\bf{Resolution}    &variable         &   &\\
    \bottomrule
  \end{tabular}
  
  ~
  
  \caption
    {
    \small
    Overview of the datasets used in the experiments.
    }
  \label{tab:datasets}
\end{table}



\subsection{Comparison of mapping approaches}
\vspace{-0.5ex}

In \fig{fig:mappings}, we compare the following six Riemannian to Euclidean space mapping ({\eqsize })
approaches which can be used during boosting:
{\bf (i)}~no mapping (ie., using a vectorised representation of the upper-triangle of the covariance matrix),
{\bf (ii)}~projection to a fixed tangent space~\cite{GuoEtAl2010},
{\bf (iii)}~projection to the weighted Karcher mean of the samples~\cite{TuzelEtAl2008},
{\bf (iv)}~projection using k-tangent spaces~\cite{SaninEtAl2012},
{\bf (v)}~mapping the points with the original RLPP method~\cite{HarandiEtAl2012},
and
{\bf (vi)}~mapping the points with the proposed \wrlpp~approach.

Since the mapping approach affects individual binary classifiers,
we show results per classifier with detection error trade-off curves.
We chose the one-vs-one classifiers between conflicting class pairs
(where samples of one class are misclassified as the other class)
on the Cambridge hand gesture recognition dataset
(which is described in Section~\ref{sec:camb_hand_gesture}). Each point on the curve represents the average of all the chosen classifiers.
The curves were obtained by varying the classification threshold {\eqsize } in \alg{alg:logitboost}.

With the exception of the original RLPP method, incrementally better results are obtained by using the mapping approaches in the mentioned order,
as they provide increasingly better vector representations of the manifold space.
Although RLPP is designed to provide a better representation compared to tangent-based approaches,
it appears not to be appropriate for boosting as it does not take into account the sample weights of critical training points.
The proposed WRLPP method addresses this problem, resulting in the best overall performance.

\begin{figure}[!tb]
  \centering
  \includegraphics[width=0.9\columnwidth]{figures/mappings}
  \caption
    {
    \small
    Performance comparison of various {\eqsize } mapping approaches,
    used within the classifier framework described in Section~\ref{sec:classifier}.
    }
  \label{fig:mappings}
\end{figure}





\subsection{UCF sport dataset}
\vspace{-0.5ex}

The UCF sport action dataset~\cite{RodriguezEtAl2008} consists of ten categories of human actions,
containing videos with non-uniform backgrounds where both the camera and the subject might be moving.
We use the regions of interest provided with the dataset.

We compared the \cov~approach against the following methods:
HOG3D~\cite{WangEtAl2009},
hierarchy of discriminative space-time neighbourhood features (HDN)~\cite{KovashkaAndGrauman2010},
and augmented features in conjunction with multiple kernel learning (AFMKL)~\cite{WuEtAl2011}.
HOG3D is the extension of histogram of oriented gradient descriptor~\cite{LaptevEtAl2008} to the spatio-temporal case.
HDN learns shapes of space-time feature neighbourhoods that are most discriminative for a given action category.
The idea is to form new features composed of the neighbourhoods around the interest points in a video.
AFMKL exploits appearance distribution features and spatio-temporal context features in a learning scheme for action recognition.
As shown in Table~\ref{tab:ucf_rates}, the proposed \cov-based approach achieves the highest accuracy.

\begin{table}[!tb]
  \centering
  \footnotesize
  \begin{tabular}{lc}
    \toprule
    \bf{Method}                       &\bf{Performance}\\
    \midrule[\heavyrulewidth]
    HOG3D~\cite{WangEtAl2009}         &85.60\%\\
    HDN~\cite{KovashkaAndGrauman2010} &87.27\%\\
    AFMKL~\cite{WuEtAl2011}           &91.30\%\\
    \bf{\cov}                         &\bf{93.91\%}\\
    \bottomrule
  \end{tabular}
  
  ~
  
  \caption
    {
    \small
    Average recognition rate on the UCF dataset~\cite{RodriguezEtAl2008}.
    }
  \label{tab:ucf_rates}
\end{table}



\subsection{CK+ facial expression dataset}


The extended Cohn-Kanade (CK+) facial expression database~\cite{LuceyEtAl2010} contains 593 sequences from 123 subjects.
We used the sequences with validated emotion labels, among 7 possible emotions.
The image sequences vary in duration (i.e. 10 to 60 frames)
and incorporate the onset (which is also the neutral frame) to peak formation of the facial expressions.

We compared the \cov~approach against
active appearance models (AAM),
constrained local models (CLM)~\cite{ChewEtAl2011},
and temporal modelling of shapes (TMS)~\cite{JainEtAl2011}.
AAM is the baseline approach included with the dataset.
It uses active appearance models to track the faces and extract the features,
and then uses support vector machines (SVM) to classify the facial expressions.
The CLM approach is an improvement on AAM, designed for better generalisation to unseen objects.
The TMS approach uses latent-dynamic conditional random fields to model temporal variations within shapes.

We show the performance per emotion in \tab{tab:ck_rates},
in line with existing literature.
The proposed \cov~approach achieves the highest average recognition accuracy of  (averaged over the 7 classes).
The next best method (TMS) obtained an average accuracy of .

\begin{table}[!tb]
  \centering
  \footnotesize
  \begin{tabular}{lccccccc}
    \toprule
    \bf{Method}              &\rh{angry} &\rh{contempt} &\rh{disgust} &\rh{fear} &\rh{happy} &\rh{sadness} &\rh{surprise}\\
    \midrule[\heavyrulewidth]
    AAM~\cite{LuceyEtAl2010} &75.0       &84.4          &94.7         &65.2      &\bf{100}   &68.0      &96.0\\
    CLM~\cite{ChewEtAl2011}  &70.1       &52.4          &92.5         &72.1      &94.2       &45.9      &93.6\\
    TMS~\cite{JainEtAl2011}  &76.7       &---           &81.5         &94.4      &98.6       &\bf{77.2} &99.1\\
    \bf{\cov}                &\bf{94.4}  &\bf{100}      &\bf{95.5}    &\bf{90.0} &96.2       &70.0      &\bf{100}\\
    \bottomrule
  \end{tabular}
  
  ~
  
  \caption
    {
    \small
    Recognition rate (in \%) on the CK+ dataset~\cite{LuceyEtAl2010}.
    }
  \label{tab:ck_rates}
\end{table}



\subsection{Cambridge hand gesture dataset}
\label{sec:camb_hand_gesture}


The Cambridge hand-gesture dataset~\cite{KimAndCipolla2009} consists of 900 image sequences of 9 gesture classes.
Each class has 100 image sequences performed by 2 subjects,
captured under 5 illuminations and 10 arbitrary motions.
The 9 classes are defined by three  primitive hand shapes and three primitive motions.
Each sequence was recorded with a fixed camera having roughly isolated gestures in space and time.
We followed the test protocol defined in~\cite{KimAndCipolla2009}.
Sequences with normal illumination were considered for training while tests were performed on the remaining sequences.

The proposed method was compared against tensor canonical correlation analysis (TCCA)~\cite{KimAndCipolla2009}, product manifolds
(PM)~\cite{LuiEtAl2010} and tangent bundles (TB)~\cite{Lui2010}. TCCA is the extension of canonical correlation analysis to multiway data arrays or
tensors. Canonical correlation analysis and principal angles are standard methods for measuring the similarity between subspaces.
In the PM method a tensor is characterised as a point on a product manifold and  classification is performed on this space. The product manifold is
created by applying a modified high order singular value decomposition on the tensors and interpreting each factorised space as a Grassmann manifold.
In the TB method, video data is represented as a third order tensor and factorised using high order singular value decomposition, where each factor is
projected onto a tangent space and the intrinsic distance is computed from a tangent bundle for action classification.

We report the recognition rates for the four test sets in Table~\ref{tab:hands_rates},
where the proposed \cov-based approach obtains the highest performance.

\begin{table}[!tb]
  \centering
  \footnotesize
  \begin{tabular}{lccccc}
    \toprule
    \bf{Method}                   &\bf{Set1} &\bf{Set2} &\bf{Set3} &\bf{Set4} &\bf{Overall}\\
    \midrule[\heavyrulewidth]
    TCCA~\cite{KimAndCipolla2009} &81\%      &81\%      &78\%      &86\%      &82\% (3.5)\\
    PM~\cite{LuiEtAl2010}         &89\%      &86\%      &89\%      &87\%      &88\% (2.1)\\
    TB~\cite{Lui2010}             &\bf{93\%} &88\%      &90\%      &91\%      &91\% (2.4)\\
    \bf{\cov}                     &92\%      &\bf{94\%} &\bf{94\%} &\bf{93\%} &\bf{93\% (1.1)}\\
    \bottomrule
  \end{tabular}
  
  ~
  
  \caption
    {
    \small
    Average recognition rate on the Cambridge dataset~\cite{KimAndCipolla2009}.
    }
  \label{tab:hands_rates}
\end{table}

\section{Conclusion}
\label{sec:discussion}


In this paper, we first extended the flat covariance descriptors proposed in~\cite{TuzelEtAl2008}
to spatio-temporal covariance descriptors termed \cov,
and then showed how they can be computed quickly through the use of integral video representations. 

The proposed \cov~descriptors belong to the group of symmetric positive definite matrices,
which can be formulated as a connected Riemannian manifold.
Prior to classification, points on a manifold are generally mapped to an Euclidean space,
through a technique such as Riemannian locality preserving projection (RLPP)~\cite{HarandiEtAl2012}.

The \cov~descriptors are extracted from spatio-temporal windows inside sample videos,
with the number of possible windows being very large.
We used a boosting approach to find a subset which is the most useful for classification.
In order to take into account the weights of the training samples,
we further proposed to extend RLPP by incorporating weighting during the projection.
The weighted projection (termed WRLPP) leads to a better representation of the
neighbourhoods around the most critical training samples during each boosting iteration.

Combining the proposed \cov~descriptors with the classification approach based on WRLPP boosting
leads to a state-of-the-art method for action and gesture recognition.
The proposed \cov-based method performs better than several recent approaches 
on three benchmark datasets for action and gesture recognition.
The method is robust and does not require additional processing of the videos,
such as foreground detection, interest-point detection or tracking.
To our knowledge, this is the first approach proving to be equally suitable (ie.,  recognition accuracy)
for both action and gesture recognition.


Further avenues of research include adapting the method for related tasks,
such as anomaly detection in surveillance videos~\cite{reddy_cvprw_2011},
where there is often a shortage of positive examples.




\balance
\renewcommand{\baselinestretch}{0.983}\small\footnotesize
\footnotesize
\bibliographystyle{ieee}
\bibliography{references}

\end{document}
