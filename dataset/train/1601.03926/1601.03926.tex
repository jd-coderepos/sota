\documentclass[10pt, conference, letterpaper]{IEEEtran}
\setlength{\evensidemargin}{-0.375in}
\setlength{\oddsidemargin}{-0.375in}
\setlength{\textwidth}{7.25in}
\setlength{\textheight}{9.25in}
\usepackage{color,overpic,cite}
\usepackage{pdfsync}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm,bm,comment}
\newcommand{\ABT}{\textsf{ABT}}
\newcommand{\gp}[1]{{\color{red} #1}}
\newcommand{\spyros}[1]{{#1}}
\newcommand{\eat}[1]{{}}
\newcommand{\mean}[1]{\mathbb{E}\!\left[#1\right]}
\newcommand{\prob}[1]{\mathbb{P}\!\left(#1\right)}
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\mat}[1]{\mathbf{#1}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{remark}{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{problem}{Problem}
\title{Placing Dynamic Content in Caches \\with Small Population}
\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
\def\ind{\mathbbm{1}}
\def\Pcal{\mathcal{P}}
\def\Bcal{\mathcal{B}}
\def\Mcal{\mathcal{M}}
\def\Ucal{\mathcal{U}}
\def\Ccal{\mathcal{C}}
\def\Lcal{\mathcal{L}}
\def\Hcal{\mathcal{H}}
\def\Poi{\text{Poi}}
\def\PP{\mathbb{P}}
\def\EE{\mathbb{E}}

\author{
\IEEEauthorblockN{Mathieu Leconte, Georgios Paschos, Lazaros Gkatzikis, Moez Draief, Spyridon Vassilaras, Symeon Chouvardas}
\IEEEauthorblockA{Mathematical and Algorithmic Sciences Lab, France Research Center, Huawei Technologies Co., Ltd.\\
Email: firstname.lastname@huawei.com}
}

\begin{document}
\maketitle
\begin{abstract}
This paper addresses a fundamental limitation for the adoption of caching for wireless access networks due to small population sizes. This shortcoming is due to two main challenges: (i) making timely estimates of varying content popularity and (ii) inferring popular content from small samples. We propose a framework which alleviates such limitations. 

To timely estimate varying popularity in a context of a single cache we propose an Age-Based Threshold () policy which caches all contents requested more times than a threshold , where  is the content age.
We show that  is asymptotically hit rate optimal in the many contents regime, which allows us to obtain the first characterization of the optimal performance of a caching system in a dynamic context. 
We then address small sample sizes focusing on  local caches and one global cache. On the one hand we show that the global cache learns  times faster by aggregating all requests from local caches, which improves hit rates. On the other hand, aggregation washes out local characteristics of correlated traffic which penalizes hit rate.
This motivates coordination mechanisms which combine global learning of popularity scores in clusters and LRU with prefetching.  
\end{abstract}
\vspace{-0.2in}
\section{Introduction}
Content Delivery Networks (CDNs) transformed the way data is replicated to respond to the ever increasing demand for popular content. The underlying technology uses large network caches to cover densely populated areas of millions of users. This paradigm yields to a number of benefits for the performance of the network, namely reducing latency and saving bandwidth. To further gain on such metrics in a wireless network, content can be stored closer yet to the user, e.g.,~at a base station or at the mobile. The main limitation to the adoption of such an appealing approach is that small local caches will only have a partial view of the content dynamics and each cache will only see populations of small sizes around it. There is indeed a general consensus that small population sizes result in poor hit rates. 
This paper contributes to answering the following fundamental question: \emph{How can one achieve good hit rates in caches which cover small populations?}

Fresh content such as news, music or TV series is produced on a regular basis. One of the characteristics of such content is that it is ephemeral, i.e., it is highly demanded for a certain duration and then the demand fades. Tracking an ever changing popularity profile of content is challenging as effective caching crucially relies on the knowledge of content popularity. 
This aspect is deemed one of the main hurdles to deploying caches closer to the user. At first sight, tracking popularity for local caches may seem hopeless due to the small sample size. \eat{Although content popularity evolution in time (a phenomenon known as temporal locality) and popularity distribution correlations among different locations (known as geographical locality) have been well documented in the literature, a large percentage of the caching literature uses the (time and location) Independent Request Model (IRM) as a basis for algorithmic design and performance analysis.}   
In this paper, we study caches with small population under the assumption of time-varying and unknown content popularity. In what follows we describe an important application of our research.


\begin{figure}[t]
\begin{center}
   \hspace{-0.6in}\begin{overpic}[scale=0.22]{hierarchy.png}
        \put(26,7){\small small user population}
            \put(97,30){\small  local caches}
            \put(60,70){\small global cache}
      \end{overpic}\vspace{-0.2in} 
      \caption{A hierarchy of  local caches, each one receiving requests from a small population, and a global cache which receives requests from the aggregate large population.}
            \label{fig:distr}
						\vspace{-0.4in}
            \end{center}
\end{figure}


\subsection{Reducing Content Latency with Base Station Caches}
\label{sec:motivation}

The upcoming 5G wireless architectures pose stringent requirements in terms of latency \cite{5G} and motivate the placement of content near the user \cite{5Gcaching}.
Introducing caches at the network edge is an appealing solution since the cost of network equipment (base station or user equipment) substantially exceeds the cost of installing a cache \cite{Roberts13}.
There has recently been a large body of work on cache optimization for wireless systems, cf.~\cite{FemtoCaching, J_gitzenis_13, JiMolisch13, Niesen13}. However, all these ideas suffer from two main unrealistic assumptions, (i) the cache size is of the order of the catalog size, and (ii) the content popularities are known (or static). In this paper we study caching at wireless access by relaxing these two assumptions. Below we discuss the context of our paper.

Regarding the cache size, we remark that 
a determining factor for the caching performance is the ratio , where  is the size of the content catalog and  is the cache size.\footnote{In this paper we will make the common simplifying assumption that all files have the same size\spyros {, which is well justified by the fact that we can break large files into equal size chunks and consider the chunks as the cacheable contents}. Hence,  denotes the number of the contents which can be cached.} 
Prior studies of caching performance have shown that when  is small, the probability to find a content in the cache becomes negligible~\cite{Roberts13}.
Since the base station (or mobile) cache is physically constrained, caches must be relatively small in storage size, and hence ineffective. 

 
\spyros {There is one important case, however, where the effect of small cache size is counterbalanced by decreasing the size of cached contents: When content access latency reduction is the primary objective of caching, \emph{contents can be split into small chunks and only a small fraction of these chunks need to be cached}} \cite{Sen99}. 
See Figure~\ref{fig:request} for an illustration of this technique.
An interesting observation is that the chunk hit probability (i.e., the probability of finding the first chunk in the cache) for cache size   will be equal to the hit probability for cache size , where  is the inverse of the file fraction which is required for smooth content display. Hence when using caching for latency minimization, the cache sizes are virtually scaled by , which gives a solution to the challenge of small caches. In the remainder of the paper we will study  hit probabilities with large caches, with the understanding that this directly corresponds to  \emph{chunk} hit probabilities in small caches.

\begin{figure}[t]
      \centering
            \includegraphics[width=0.86\linewidth]{request.png}
						\vspace{-0.15in} 
      \caption{Latency reduction by means of caching the first content chunk at the base station.}\vspace{-0.4in}
			\label{fig:request}
\end{figure}


Having dealt with the small cache size issue, we shift our attention to the small population issue.
A base station cache receives requests from a small population of users, hence the number of requests per unit time is also very small. This in turn makes popularity estimation very challenging \cite{5Gcaching}. 
To make caching efficient for dynamic content popularity, 
the remainder of the paper 
focuses on the study of  
caches with small population.
Below we survey related work on the topic and then explain our contribution.





\subsection{Related Work}

There is an increasing demand to use caching to combat the growth of mobile information \cite{IdealvsReality}, though the adaptation of caching techniques in the wireless domain is challenging \cite{5Gcaching}.
We deal with the problem of \emph{small cache size} using partial caching \cite{Ahle14} to improve the hit rate performance of the content header.
Storing contents partially has been previously proposed in the literature of proxy caching for streaming media applications \cite{Liu04,Sen99}.

The problem of \emph{small user population} on the other hand is relatively underexplored, perhaps because web caching is applied mostly in densely populated areas. It is noted though that hit rate performance has a sharp cutoff point as the user population becomes small \cite{Wolman}, mainly because there is insufficient room for correctly estimating content popularity. In this work we analyze the issue of small user population by formulating a problem on the interface of caching and learning.

Although the use of learning in the caching domain goes back to the days of prefetching for web caching, cf.~\cite{Pallis}, very recently it has been revived in the context of wireless networks. The use of transfer learning to tap social network side-information in order to alleviate data sparsity has been proposed in \cite{bastug2014b}, while \cite{Blasco} models the popularity learning as a multi-armed bandit problem. 
Most prior techniques are limited to the environment of the static popularity.
In practice, not only content popularity is dynamic, but moreover recent works argue that correct modeling has a significant impact on the performance analysis of caching schemes \cite{traverso2015,shen}. 
Learning time-varying popularity is actually an interesting problem on its own \cite{Szabo10,Ahmed13}.
However, any scheme which learns popularity separately from performing content placement is suboptimal \cite{moharir}, which  motivates a joint approach.


\subsection{Our Contribution}



For a single cache we study the joint problem of caching and learning time-varying popularity. We propose  a simple threshold policy called \spyros {Age-Based Threshold} (): a content is stored if it has been requested more than  times, where  is the age of the content\spyros {, i.e., the elapsed time since this content was first inserted in the contents catalog}, and  is a selected threshold. 
We show that  is asymptotically optimal when we increase the number of contents, which provides a first characterization of the joint problem of caching and learning under time-varying popularity.



We then study an architecture where the popularity is estimated (or learned) at a global point which has access to all the requests arriving in  local caches. 
We prove that \emph{global learns faster}; by aggregating requests from all  caches it is able to \spyros {track popularity changes}  times faster.
If contents exhibit correlations in locations however, we show that the distribution of the local popularity of contents is more skewed, which means that local learning yields better performance  provided that these local popularities can be well estimated, i.e., \emph{local is more accurate}. \spyros {Combining the two last observations, we propose learning content popularities in clusters which are both able to retain local characteristics and to accumulate enough many request samples.}


Our goal is to learn a good estimate at the global point and then feed it back to the local caches in the form of content scores. In fact, we propose the modification of the threshold  as a score which  
takes into account both the frequency of requests, as well as the content age. 
Using these scores we propose two globally coordinated mechanisms for \spyros {managing} the local caches: (i) a score-gated LRU\footnote{The Least Recently Used (LRU) cache replacement rule dictates that every requested content is cached, and if the cache is full then the least recently requested content is evicted. The score-gated counterpart avoids caching (gates) certain contents based on popularity scores. } and (ii) a score-based prefetching scheme.  
Here the term prefetching refers to \emph{the act of populating a cache with content which is not currently being requested at that cache}. 
We exhibit, using simulations, that prefetching is crucial for small population caches. 

Although global learning resolves the popularity estimation issue, there is a hidden outstanding issue in our architecture: the extra traffic incurred to prefetch content in the caches.
The latency minimization with caching comes at a cost of increased traffic in the core network due to prefetching. Using our proposed methodology, we evaluate this tradeoff by means of simulations and show that the incurred traffic can be kept significantly small--our simulations show 3\% of increase in total bandwidth in the worst case.














\section{Requests With Time-Varying Popularity}\label{sec:snm}

For our analysis we use a dynamic request model with time-varying popularities, the recently proposed Poisson \emph{Shot Noise Model} (SNM) \cite{traverso2015}.
This model introduces dynamicity in a simple manner while retaining the power law characteristics of instantaneous popularity observed from past works \cite{breslau99,adamic02,newman05}. In fact \cite{traverso2015} shows that SNM fits well real  data \spyros {of content requests in cellular networks}.

The lifetime of each content is associated with a shot, which is characterized by (i) a shape, (ii) a duration, (iii) an arrival instance, and (iv) a volume.
It is reported that the choices of (i)-(ii) have a smaller impact to the hit probability under the LRU cache management policy \cite{traverso2015}. Thus, in the following, we will consider rectangular pulses of fixed durations  for all contents, see Figure~\ref{fig:SNM}, in order to develop an optimal cache management policy amenable to analytic expressions for the parameters of the policy and the resulting hit probability. For different shaped shots the details of our analysis must be revisited, but the main insights can be used to derive heuristic policies for the generic SNM. Also, one could perform the same analysis using a joint distribution of lifespan and shot volumes, as in \cite{olmos2014catalog}. 


The shot arrival times (iii) are points of a Poisson  process with constant rate . 
Denote with  the arrival time of shot .
At time  the alive content catalog is given by the set


The shot volumes (iv) are determined by a power-law distribution, commonly known to fit well the instantaneous content popularity.
More specifically, we set the request rate of content  while it is alive to the random variable  constructed in the following way. First, for any , let  be an i.i.d. random variable drawn uniformly at random in . Then

where  is the mean popularity, and  is the power law exponent. 
{We let  denote the density of the power-law distribution at ;  is in fact a Pareto distribution \spyros {with parameters  and } \cite{newman05}, which is the limit of Zipf distributions for large catalogs.\footnote{We choose to generate the power-law popularity in this particular way to facilitate the modeling in Section~\ref{sec:correlated_popularities}.}}
 Finally, we generate requests independently for each content  using an independent Poisson process with rate .





\begin{figure}[t!]
\begin{center}
   \begin{overpic}[scale=0.22]{psn_2.png}
    \put(22,-4){}
		\put(34,-4){}
		\put(60,-4){}
		\put(80,-4){}
		\put(-4,17){}
		\put(-4,6){}
   \end{overpic}
	\caption{Poisson Shot Noise Model (SNM). A realization showing two shots of different volumes and arrival time instances. In our model we keep the shot duration  and the shot shape constant for all contents.}\label{fig:SNM}
	\vspace{-0.35in}
	\end{center}
\end{figure}






\section{Cache Hit Rate With Estimated Popularities}\label{sec:onecache}

In this section we focus on one cache receiving requests with dynamic and unknown popularities and we study the optimal hit rate performance.

\subsection{Hit Rate Optimization With Estimated Popularities}

In what follows we restrict attention to a caching controller which is not aware of the actual content popularities .
 Instead we assume that the controller 
estimates the content popularities via observations of past requests. To proceed with the analysis we additionally make the following simplifying assumptions:
\begin{itemize}
\item The controller knows the exact arrival times of shots .
\item There is no cost for replacing a content in the cache (i.e., there is no traffic cost for transmitting a content from the origin server to the cache or between caches). We will relax this assumption in section \ref{sec:prefetch}.
\end{itemize}

Our overarching goal is to maximize the hit rate  over the time horizon. However, given that the cache updates induce zero cost, we can decouple the time horizon hit rate optimization to individual problems of maximizing the  hit probability at each time instance. 



We may characterize the alive content  by its \emph{shot age} and the number of observed requests.
The shot age of content  at time  is the elapsed time since the content appeared in the system, denoted by . 
	We denote with  the number of requests observed for content  by time .
We represent the caching decision at time  by a binary \emph{caching vector}  of dimension , where  if content  is stored in the cache, and  otherwise. 
		Hereinafter we will exchangeably use the notations  and  to denote the vector , and 
	drop the index whenever it is directly inferred from the context.
		The cache size  dictates that the constraint  must be satisfied at each time instance.
	
By pointwise ergodicity of the SNM model  we may study any one time instance; we choose to study .
	Since the caching performance at time  depends only on alive shots , we only need to focus on random events in the time interval .
	To simplify notations, hereinafter we will omit the mention of the time index  and write , , , and .



Next we would like to choose the caching vector  to maximize the instantaneous hit probability at the origin. 
If the popularities were known the controller would employ the policy \emph{store the most popular}. However, the challenge here lies on the fact that the popularities  are unknown. 
In fact, the instantaneous hit rate at the origin is given by . However, the popularities  are not observed. Therefore, we need to consider instead the expected hit rate conditionally on the available information :

where the popularity estimates are computed using the prior model of :

where  is the power-law density. To evaluate numerically \eqref{eq:condexp} observe that the popularity of content  is equal to  and constant  over the period , and the request process is Poisson, hence the term  is equal to the probability that a Poisson random variable with mean  is equal to , i.e.,

For every instance , we want to find the best contents to store to maximize the conditional expected hit rate:

\vspace{0.1in}

\noindent \underline{\emph{Max instantaneous hit probability with estimated popularities:}}

The optimization \eqref{eq:opt1} can be solved by storing the  items with the highest estimate . 
Given values for  we may compute numerically the instantaneous hit rate. However, the above are random. We define the maximum expected hit probability  for shot arrival rate  and shot duration , which will be our main performance metric:

where  all depend on  and .
 Computing \eqref{eq:avhr} is complicated mainly because the set of active contents  is itself random and the caching decisions are correlated across all the active contents. Below, we characterize a simple caching policy which is asymptotically optimal for large catalogs, which allows us to obtain an asymptotic expression for \eqref{eq:avhr}.



\subsection{Age-Based Threshold Policy}


\begin{figure}[t!]
\begin{center}
   \begin{overpic}[scale=0.44]{abt_1.pdf}
        \put(35.5,-5.5){\footnotesize  (shot age)}
				\put(32,88.5){\small  Threshold}
				\put(27,46){\scriptsize }
				\put(35,14){\scriptsize }
				\put(21,21.5){\vector(1,3){7}}
				\put(22,17.5){\vector(4,-1){12}}
   \end{overpic}
	 \begin{overpic}[scale=0.44]{abt_2.pdf}
        \put(35.5,-5.5){\footnotesize  (shot age)}
				\put(11,89){\scriptsize Est.~popularity of marginally cached}
				\put(12.3,55.4){\vector(1,1){12}}
				\put(25,70){\tiny }
				\put(11,35){\vector(1,-1){12}}
				\put(25,20){\scriptsize }
   \end{overpic}
   \caption{ behavior in the many contents regime. \textbf{Parameters:}  (reported to be a typical value \cite{breslau99,adamic02}), , , .\vspace{-0.6in}}
   \label{fig: optimal caching plot}
\end{center}
\end{figure}


Our plan is to design a simplified caching policy  which caches highly requested content without having to calculate the estimates  and to solve the optimization \eqref{eq:opt1} at every time instance. 
A complication comes from the fact that the shot age  affects the estimate . Intuitively, on average, to maximize hit probability we need to store more content with larger age , because the uncertainty in the estimate  is lower for them, which results in turn in fewer caching mistakes and a higher efficiency for older contents. We introduce a deterministic threshold  which depends on the age  and is used to allocate cache capacity differently for each . Under our policy, content  is stored if it satisfies \spyros { }.






\noindent\rule{3.55in}{0.02in}

\noindent\textbf{Age-Based Threshold (\ABT) Policy.}

\vspace{-0.05in}
\noindent\rule{3.55in}{0.02in}

\noindent\textbf{Parameter Selection.} 
 is the cache size (in contents),  is the shot rate,  the shot duration, and hence  is the average number of alive shots at any time instance.
Define , which is roughly the fraction of the content catalog which can be cached.
Denote with  the \spyros {cumulative distribution function} of  . 
Choose  to be the -th upper-percentile of , since  has a density  is invertible, hence




\noindent\textbf{Age-Based Threshold.}
Choose the threshold  




\noindent\textbf{Caching Vector.}
  For each content  observe  and choose: 
	\vspace{-0.1in}

\noindent\textbf{Ensuring Cache Size Constraint.} If , then choose arbitrarily  contents and set .

\noindent\rule{3.55in}{0.02in}

For a given content ,  is known, hence under our policy the caching decision depends only on , which simplifies greatly caching decisions. The  complicated part of the policy is to compute the threshold function . However, this can be done in an offline manner: for any given parameters  and power law parameters  we can numerically compute the threshold using \eqref{eq:condexp} and \eqref{eq:threshold}. Another approach is to compute   by iteratively filling cache capacity so that the marginal hit rate improvements  for each  are approximately equal.\footnote{
This process involves splitting  to small intervals and increasing the threshold at each interval one-by-one inspecting the marginal hit rate improvements.
Since the possible values of the threshold  are discrete, we remark that a perfect equality cannot be achieved.}







Figure~\ref{fig: optimal caching plot} (left)  shows the   threshold  for different content age ; 
the dotted line corresponds to expected number of requests  for the content  which is the  most popular content in the active catalog.
Note that the optimal threshold roughly follows this line, although it is a bit higher for contents with small age. This indicates that the  policy differs from a simple frequency estimate since it is more conservative with recent contents, as their popularity estimates are less accurate. 
Figure~\ref{fig: optimal caching plot} (right) shows the density of the marginal hit rate improvement  for each age ; the dotted line is the threshold  which is the minimum conditional expected popularity of contents optimally stored in the cache and also the marginal hit rate improvement at which the iterative filling algorithm would stop.





There is an intuitive connection between  and the policy which solves optimally \eqref{eq:opt1} by caching the highest  values, let us call it . Similar to , we may think of  as a threshold policy, only with a threshold 
which results from considering the -th upper-quantile of the empirical distribution of , which is random and dependent on . 
Due to the differences between the two thresholds,   decisions result in a few caching mistakes and thus in suboptimal hit rate performance. 
However, as the number of contents increases , the random threshold of the optimal policy converges to that of . We establish this fact in the following Theorem:




\begin{theorem}[ Optimality in  Many Contents Regime]\label{th:ABT}
For shot rate , consider two caching systems, one running the optimal policy , and one with . Denote their average hit probabilities by  and  respectively.
We let  and  go to infinity together such that . 
Then we have almost surely: \vspace{-0.09in}

in the sense that they asymptotically have the same threshold function, and thus they cache the same contents. 

Moreover,  is almost surely asymptotically optimal:

where \vspace{-0.08in}

\end{theorem}
\begin{proof}
The proof is in appendix~\ref{app:a}.
\end{proof}



We call the regime  the ``many contents'' regime. 
This is generally a reasonable regime in the caching context, where catalogs of contents typically contain millions, if not billions, of contents, and the caches are dimensioned so that they can store a fraction of the catalog of active contents.
A corollary of Theorem \ref{th:ABT} is that the threshold \eqref{eq:threshold} separates the seemingly most popular contents from the less popular ones, such that the fraction of contents deemed popular is exactly , which captures the relative cache size () in our model. Hence, we can think of this threshold as a way of separating the  seemingly most popular contents from the rest. For the rest of the paper, we consider the many content regime and focus on the influence of the shot duration ; from now on, we omit the mention of  in .





\section{Aggregating Estimates From  Caches}

We consider a hierarchy of  caches connected to a central cache, as in  Figure~\ref{fig:distr}. 
Each content request arrives first at one of the  local caches, and then it is observed by the global cache. 
In this section we explain that the hit rate performance of the local caches can be improved if popularities are estimated at the global cache.





 







\subsection{Local vs Global Estimation}

\subsubsection{Request Model for Uncorrelated Requests}\label{sec:model2}

We clarify how the SNM model is used in the  cache system. Let  be an inhomogeneous Poisson process describing the requests for content  which reach the global cache, and assume that  is built using our SNM model from Section~\ref{sec:snm}, where the mean popularity of contents at the global cache is equal to . 
When a request is made, we randomly select one local cache uniformly with probability  and assume that the specific local cache \spyros {was the one this request came from}. We denote by  the thinned inhomogeneous Poisson process observed at local cache .

\subsubsection{Global is Faster}
As before we will fix a particular time instance () and study the behavior of our system at this instance. For this section, we will omit reference to absolute time, and track different times using the shot age  with respect to the observation instance .

We let  be the set of local caches, with . Denote by , the number of requests for content  arrived at cache  in the time interval .\footnote{Here we slightly abuse the notation  to count requests in the interval  instead of .} Observe that  is the number of all requests for this content at cache  so far, and  corresponds to the entire history of requests for item  and cache . 
Note that  for  are all independent Poisson processes with time-varying rate . In this section, we consider the case where the  are equal for all ; however, we keep the index  to stress that it refers to a quantity at local cache .

By the properties of thinning Poisson processes, the rate of the aggregate request process satisfies


To compare local versus global estimation we  define two ways in which a local cache  can decide its caching vector .

\noindent\textbf{Local estimation.} The caching policy  takes as input the local request information only, i.e., it is a causal mapping of past local observations:

Let  be the average hit probability of policy  using local estimation when the SNM shots have duration ; here
 has a profound impact on the quality of caching decisions since it determines the dynamicity of the model. For example, for a very small , many contents are requested only for a very few times in their lifetime. The best local cache hit probability performance with local estimation is then 




\noindent\textbf{Global estimation.} The caching policy can take as input the collection of local request information, in this case we pass all requests as arguments and we write  instead of 

where the index  on  points out that the histories of all local caches are available to the policy.
Similarly as above, let  be the average hit probability of policy  using aggregate estimation.
The best hit probability performance with global estimation is 

The global estimation may use the observations from all locations  to better detect changing popularities. This directly translates to a hit rate benefit, which we capture with the following result.

\begin{theorem}[Global is Faster]\label{th:agg}
Consider the SNM model of Section~\ref{sec:model2} whose  requests  are observed by the global system, and a thinned version of them  are observed by local cache . 
The maximum hit probability performance of global system  compares to the performance of any local cache  in the following manner

\end{theorem}
\begin{proof}
The proof is in appendix~\ref{app:b}. The proof shows more generally that \emph{for any} policy using local estimation for a system with shot duration , we can define a policy using global estimation having the same performance for shot duration , and conversely.
\end{proof}
According to Theorem \ref{th:agg} the global system aggregates more samples and its performance can be understood as virtually slowing down the popularity dynamics. 
Since faster dynamics have  a detrimental effect on hit rate, this virtual slowing down helps the global system to improve hit rate performance. 
Below we provide numerical performance comparison between local and global learning. We use the  policy in the many contents regime, where its optimality allows us to compute (in numerical terms) the exact benefit we have from aggregation. We define the hit probability gain as

Figure~\ref{fig:lvg} plots  and shows that gains reach  of absolute hit rate improvement for a specific . We observe that there is a wide range of values of  for which the system greatly benefits from aggregating requests and learning faster.

\begin{figure}[t]
\begin{center}
   \begin{overpic}[scale=0.38]{Agg_1.pdf}
        \put(32,-4){\scriptsize  (shot duration)}
				\put(12,84){\scriptsize Average Hit Probability}
				\put(25,58){\tiny global }
				\put(23,52){\tiny }
				\put(57.5,66){\tiny local}	
				\put(54.5,60){\tiny }	
				\put(75,28.5){\tiny whole files}	
      \end{overpic}
			\begin{overpic}[scale=0.38]{Agg_2.pdf}
        \put(32,-4){\scriptsize  (shot duration)}
				\put(12,84){\scriptsize  Absolute Gain}
				\put(43,43){\tiny global vs local}
				\put(45,37){\tiny }
				\put(59,82){\tiny global vs whole files}
      \end{overpic}
      \caption{\textbf{Global is faster for uncorrelated caches.} (left) Optimal  hit probability under time-varying popularity using (i) global learning,  (ii) local learning, or (iii) storing entire files (cache size reduced to ). (right) Absolute hit probability gain. 
			\textbf{Parameters:} , , , .
			\vspace{-0.3in}}
            \label{fig:lvg}
            \end{center}
\end{figure}

\subsection{Correlated Popularities}\label{sec:correlated_popularities}



We might expect that some popularities may vary from region to region; this could be attributed to different sociological and cultural backgrounds of users or different types of activities associated with these locations. 
For example, job commuters might pursue similar requests for contents
and hence office areas might ``see'' a particular request pattern. 
The caching benefit  from geographical locality of content has been recently pointed out \cite{kurose,scellato2011,huguenin2012}. 

In this section we assume that the contents exhibit geographical correlations and hence there exist groups of contents which are very popular in a subset of local caches. 
Although for identical local caches the best approach was to learn from the aggregation of  all caches, 
here it may be more efficient to restrain the aggregation to subsets of caches which witness similar traffic patterns.



\subsubsection{Request Model for Correlated Locations}

We propose here a model for correlated local popularities . 
As before, aggregate popularities  are described by the SNM model of Section~\ref{sec:snm}, with global mean popularity .
To model correlations between the local cache popularities , we draw inspiration from the field of community detection \cite{lelarge2013reconstruction} and inhomogeneous random graphs \cite{bollobas2007phase}:
\begin{itemize}
\item Each content  is associated with a feature vector . To simplify the model we let  be independent uniform random variables taking values in . 

\item Each location  is associated with feature vector , which are again chosen independently and uniformly in .

\item We define a kernel , where  is continuous, strictly decreasing on , symmetric and -periodic, with  for all . For such a correlation kernel , we can think of the feature vectors  as lying on the torus  rather than the interval.
\end{itemize}
The local popularity of content  at cache  is defined as 

As the number of caches  increases and provided the kernel function  satisfies some basic conditions, the normalization constant almost surely becomes deterministic: 
 
so that 

















This basic model can easily be extended to multi-dimensional features, to capture more complex correlation structures.

\subsubsection{Local is More Accurate}
With correlated popularities, the popularity distribution is more skewed if observed on a subset of caches, and less skewed if aggregated over all caches. Since popularity skewness is advantageous to caching, it should not be surprising that learning in clusters can outperform learning globally.
Recall that 
 denotes the maximum average hit probability achieved by observing the aggregated request, and  the corresponding maximum average hit probability when observing local requests at cache . 
Due to convexity, we have the following result.


\begin{theorem}[Local is More Accurate - Known Popularities]\label{th:corr}
In the limit of a static system (i.e., assuming ), the hit probability performance of local learning is higher than that of aggregate global learning, i.e., for any global popularity distribution  it holds that


Furthermore, 
as the number of edge caches  tends to infinity (such that \eqref{eqn: limit local pop} holds), the maximum expected hit probability  is almost surely:

where  is the inverse of  and  is the unique value satisfying

\end{theorem}
\begin{proof}
The proof is in appendix~\ref{app:c}.
\end{proof}




The above theorem characterizes the performance of correlated caches for nearly-static popularities.
The benefit in this case is due to the skewness of the local popularity distribution. 
However, when the popularities are unknown, we saw that aggregation is beneficial. In fact we observe a tradeoff; (i) aggregating all observations improves performance by collecting more samples and having more accurate estimates, but (ii) aggregating in subsets allows for more accurate popularity models with higher skewness value. 
\subsubsection{Clustering}
In order to retain the benefit from the increased skewness of the local popularities and at the same time  capture faster dynamics than local estimation would allow, we need to estimate the local popularities of contents based on the global information. One way to achieve this goal is to identify locations with similar local popularity profiles and to aggregate samples from these locations only. Leveraging our correlated local popularities model and the understanding of optimal policies gained from the previous sections, we can explore the tradeoff between capturing local popularities and detecting faster dynamics.

For this paper, we leave aside the problem of determining which local caches should be clustered together from the requests history. Instead, we assume that we know the embeddings of local caches . When the kernel  has the simple form assumed here, the local caches which should be aggregated are those with similar feature vectors . 
We will consider a cluster  covering the subset  of the feature space, and study feature vectors .
For , , this is equivalent to considering  disjoint clusters covering equal portions of the feature space.
 A meaningful regime here is to let the total number of local caches  increase, i.e., , which means that we are looking at smaller and smaller local user populations, while keeping the global popularity distribution fixed, i.e.,  and  are fixed. As , we have , and the aggregated popularity of items  within  equals

As discussed in previous sections, the aggregated popularity  is not known and has to be estimated from the aggregate requests . We denote by  the limit as  of the optimal hit probability averaged over local caches in the cluster  for shot duration , when popularities are estimated from ; we call this quantity the clustered hit probability. Leveraging the techniques from the previous sections, the clustered hit probability is obtained by appropriately defining an aging-based threshold  to (approximately) equalize the marginal improvements  for all . This yields

where the thresholds  also ensure the correct fraction of contents is stored:

Figure~\ref{fig: performance curves}-(left) shows the clustered hit probability  as a function of the shot duration  for different sizes  of the cluster and for a particular kernel .
As , smaller values of  (i.e. smaller clusters) yield better hit rates, as stated in Theorem~\ref{th:corr}; however, as the system becomes more dynamic (i.e., for smaller ) small cluster sizes fail to estimate the aggregated popularities , which results in poor clustered hit rate.
Cluster size  corresponds to learning by aggregation of all requests in one big cluster, which is advantageous when  is small. 
 The right panel shows the kernel , along with smoothed versions of the kernel  corresponding to using a cluster size . 
When  (global learning) the corresponding smoothed kernel is flat, resulting in loss of location information. 
However, smaller values of  yield smoothed kernels which approximate better and better the true correlation kernel, which allows to capture the local popularity characteristics. 







\begin{figure}[t]
\begin{center}
   \begin{overpic}[scale=0.415]{Clu_1}
        \put(32,-4){\scriptsize  (shot duration)}
				\put(12,89){\scriptsize Average Hit Probability}
				\put(78,31){\tiny =1}
			  \put(78,26.4){\tiny =.5}
				\put(78,22){\tiny =.1}
				\put(78,17.2){\tiny =.01}
				\put(78,12.4){\tiny =.001}
      \end{overpic}
			\begin{overpic}[scale=0.417]{Clu_2}
        \put(2,-4){\scriptsize Distance between cluster center and }
				\put(12,90.5){\scriptsize  Virtual smoothing from clustering}
				\put(70,81.2){\tiny true kernel}
				\put(70,76.4){\scriptsize =1}
				\put(70,71.6){\scriptsize =.5}
				\put(70,66.8){\scriptsize =.1}
				\put(70,62){\scriptsize =.01}
				\put(70,57.2){\scriptsize =.001}
      \end{overpic}
      \caption{\textbf{Learning in clusters for correlated caches.} (left) Optimal hit probability under time-varying popularity using clustering for different relative size  of cluster. (right) True kernel vs. smoothed kernels resulting from clustering. 
			\textbf{Parameters:} , , .
			\vspace{-0.25in}}
            \label{fig: performance curves}
            \end{center}
\end{figure}





\section{To Prefetch Or Not}\label{sec:prefetch}

So far our analysis did not consider the traffic required for placing the content in the local caches. In this section we
 focus on practical policies which either perform adaptive caching (without prefetching) or explicitly prefetch content which is not yet requested. 
We first introduce popularity scores which are calculated by the global cache and then made available at the local caches. 
These scores can be used both for performing score-gated LRU as well as for determining which contents to prefetch. 
We present simulations of the proposed techniques and showcase that prefetching is of fundamental significance to small population caches.

\subsection{Age-based Popularity Scores}

As explained in the previous section, it is advantageous to estimate popularities at the global cache and then use the estimates at the local caches. One standard methodology to coordinate this mechanism is to use \emph{content scores}. A score is simply a value per content which can be used to perform caching. For example, we may give value 1 to very popular contents and value 0 to the rest. 

We exploit intuition from the one-cache analysis in section~\ref{sec:onecache} to propose the use of threshold functions  as scores. 
Recall the definition of the threshold 
of  policy

where .
Then, the contents which satisfy  are the  with highest popularity estimates and should be cached; 
here  is the equivalent cache size.
We may produce new thresholds by choosing different values for . In particular let us pick ,  such that

Replacing  with  or  is equivalent to considering the  policy on a virtual cache with larger or smaller cache size respectively. In particular, if we use  the virtual cache is larger, hence the threshold  smaller: almost all contents will pass the threshold. Clearly we cannot store all these in our cache (which is of size ), but if a content with  does not satisfy  we may infer that it is ``super unpopular'', see Figure~\ref{fig:multifig}-(a). Similarly, by using  hence a small virtual cache, only the ``super popular'' contents will satisfy the threshold.
The idea is to use  the function  as a generalized score for the popularity of content . 
What is convenient in this definition is that we take into account the age of content without complicating the design of scores.




\begin{figure*}[t]
	\centering
\hspace{0.03in}
	\begin{overpic}[scale=0.15]{pref_11}
	\put(5,-6){\small (a)}
\put(75,12){\footnotesize gate}
\put(25,54){\footnotesize prefetch}
		\put(41,-6){\footnotesize Age }
		\put(-5,15){\footnotesize \rotatebox{90}{Number of Requests}}
		\put(60,41.5){\scriptsize }
		\put(60,22){\scriptsize  }
		\put(60,65){\scriptsize  }
	\end{overpic}
\hspace{0.16in}
	\begin{overpic}[scale=0.17]{pref_3.png}
	\put(5,-6){\small (b)}
		\put(-6,35){\footnotesize \rotatebox{90}{Hit Probability}}
				\put(25,-6){\footnotesize Shot duration }
	\end{overpic}
	\hspace{0.16in}
\begin{overpic}[scale=0.17]{pref_4.png}
	\put(5,-6){\small (c)}
		\put(-6,7){\footnotesize \rotatebox{90}{Transmissions per request}}
		\put(25,-6){\footnotesize Shot duration }
	\end{overpic}
\hspace{0.16in}
	\begin{overpic}[scale=0.23]{pref_21}
	\put(5,-6){\small (d)}
		\put(31,-6){\footnotesize Threshold }
		\put(-6,35){\footnotesize \rotatebox{90}{Hit Probability}}
		\put(98,94){\footnotesize \rotatebox{-90}{Transmissions per request}}
\end{overpic}
	\caption{Simulation of LRU policies with scores. (a) Thresholds for age-based scores. 
	(b) Hit probability performance comparison. (c) Traffic footprint performance comparison. (d) Performance of LRU with prefetching vs threshold parameter , hit probability (read left) and transmissions per request (read right). \textbf{Simulation Parameters:} , , ,  , , , .}\vspace{-0.2in}
	\label{fig:multifig}
\end{figure*}


\subsection{Score-gated LRU}

The Least Recently Used (LRU) replacement rule is one of the most widely used caching policies. 
An intuitive way to implement LRU is to maintain a linked-list where the contents are always stored from the most recently used (head) to the least recently used (tail). A new request puts the new content at the head and pushes all contents by one position in the list causing the eviction of the content from the tail. Requests for existing contents simply bring the content to the head.

LRU is desirable in practice because it is purely adaptive
and simple to implement. 
Nevertheless it performs quite poorly in our setup. A traditional improvement over LRU is the so-called \emph{score-gated LRU}, whereby the content requests are filtered using a threshold on the content score. The high-score requests follow the LRU rule, while the low-score ones are never cached. 
In the context of our model we may use the age-based scores   to  
perform score-gated LRU using the function . 

In Figure~\ref{fig:multifig}-(b),(c) we compare LRU and gated-LRU for different shot durations , in a hierarchy of  caches. 
We use , which means that only  of highly unpopular content is not cached.
In particular, Figure~\ref{fig:multifig}-(c) shows that the number of transmissions from the origin server are roughly the same for the two policies, since for adaptive policies this is equal to one minus the hit probability.
Figure~\ref{fig:multifig}-(b) shows that gated-LRU benefits from predictions only at high values of  where the popularity is semi-static. In the more dynamic scenarios each one of the 1000 local caches only receives a handful of requests per content and hence no adaptive caching policy can be effective.

\subsection{LRU with Prefetching}

With adaptive policies the first local request for a content is always a miss, a fact which may hurt hit rates in dynamic settings. 
To amend this situation we propose prefetching content to local caches whenever it is deemed ``super popular'' by the global cache.
There is a simple coordination mechanism for this.
In a periodic fashion (here we choose a period ), the controller sends a fake request for content  if . If a local cache does not have content , it performs a prefetching operation, else it simply brings the content to the head.
Figure~\ref{fig:multifig}-(b) shows that the performance of this proactive mechanism successfully improves the hit rate.
	
	Although prefetching is clearly improving hit rates, 
	we need to measure the amount of traffic it induces.
Figure~\ref{fig:multifig}-(c) shows how many times on average each content is transmitted over the backhaul for each request.
In very dynamic settings we may need to make 30 transmissions in total to deliver one request. Although this might seem expensive, if we factor in the considerations of partial caching and the coefficient , we conclude that this corresponds roughly to  increase in total used bandwidth, which in this example is equal to , which is extremely low for a system with  cache. 
Figure~\ref{fig:multifig}-(d) shows how hit rate and traffic footprint tradeoff for  when we vary threshold ; the diminishing returns in hit rate may motivate the use of smaller values of  in a joint consideration of hit rate and traffic footprint performance.
 




\section{Conclusions}
Our work focuses on learning time-varying popularities at wireless access caching.
An architecture which combines global learning and local caches with small population is proposed to improve the latency of accessing wireless content.
It is shown that age-based thresholds can timely exploit time-varying popularities to improve caching performance. Moreover, the caching efficiency is maximized by a combination of global learning and clustering of access locations. Score mechanisms are then proposed to help with practical considerations at local caches.



\bibliography{IEEEabrv,Caching}
\bibliographystyle{IEEEtran}














\appendices

\section{Proof of Theorem \ref{th:ABT}}\label{app:a}

\begin{proof}First we reformulate the  optimal policy for a finite  as a threshold policy. Then, we show that in the limit as  the threshold function of the optimal policy becomes deterministic and equal to that of the , which also implies that the  policy is asymptotically optimal.

Recall that the optimal solution of the optimization problem~\eqref{eq:opt1} is to store the  items with the highest values of . For each item ,  is independent of the other items; let  be the distribution of  and let  be the empirical distribution of the 's, where  is a Dirac function at . The distribution  has a density, because, given any value of ,  is a smooth decreasing function of the continuous random variable . As a consequence, for finite , all the values  are almost surely distinct. Thus, a more intricate but equivalent way to define the optimal policy is that it stores all the items  which have  larger or equal to a threshold , where we set  equal to the -th upper-quantile of the empirical distribution , i.e.,  is the largest value such that . Note that, if  did not have a density, it would not always be possible to find such a value , which exactly separates the contents with the top  estimates  from the rest. Indeed, if  had atoms, many contents could have the same value of ; there would then be a need for a tie-breaking rule to decide between content with the same estimate .

We now let the shot arrival rate and the cache size tend to infinity together, i.e.,  with . The size  of the set of active contents is a Poisson random variable with mean , so . Also, the 's are independent samples from , so their empirical distribution tends to , i.e.,  almost surely. Then, the -th upper-quantile  of  tends to the -th upper-quantile  of , which is well-defined because  has a density. For each value of , we let  be the smallest integer  such that ; this is the age-dependent threshold of the  policy.
Then, in the many-content regime, we have almost surely that  if and only if , which means the  policy is the limit as  of the finite-system optimal policy . Finally, , which means the fraction of contents initially stored by the  policy tends to , and thus the last stage of  which sets arbitrary 's to 0 to ensure the cache size constraint affects a negligible number of contents. This implies ; hence, the  policy is asymptotically optimal. The expression for the asymptotic optimal expected hit probability  follows directly from the expression of the  policy.
\end{proof}

\section{Proof of Theorem \ref{th:agg}}\label{app:b}
\begin{proof}
 To prove the result we will make a connection between three arrival processes, (i) the observations at a local cache, (ii) the observations at the global cache, and (iii) the observations at the global cache about a system with accelerated time. In particular we will show that (i) and (iii) are the same in distribution. Then, we will show that the corresponding caching mappings are the same, from which the hit rate comparison will follow.

We will need an intermediate step. Define the \emph{-speedup dynamics} , for which time evolves -times faster than in the original system.
Under an -speedup, a SNM process with rate  becomes a SNM with rate  and the shot duration is shrunk to . The associated Poisson process  of requests for content , with rate , is called the -speedup aggregate requests process. As the next lemma states, the -speedup aggregate requests process is statistically identical to the request process which an individual local cache receives in the original system.
\begin{lemma}[Speedup Statistics]\label{lem:agg}

\end{lemma}
\begin{proof}
Let us consider any interval  in . We can define  and ; this simply means we consider the number of requests occurring at cache  during the time interval  rather than from a fixed time until time .  is a Poisson random variable with mean . Similarly,  is a Poisson random variable with mean . In addition, we have  so that  for all intervals , which shows the two process are statistically identical.
\end{proof}
Using Lemma~\ref{lem:agg} we can establish a one-to-one mapping between policies  using local information and policies  using global information for the -speedup dynamics. Let  be the vector indicating which contents are stored at time  for the -speedup dynamics under policy , i.e., . For any policy  using local information and for any realization of the requests processes, we can define  as  for all . Using Lemma~\ref{lem:agg}, the hit probabilities are the equal in distribution under  and for the -speedup dynamics under . Hence, . The same reasoning can be made starting from any policy  using aggregate information to define a policy  using only the local information of cache  for the original system such that the two policies have identical performance in distribution. The theorem then follows immediately by considering optimal policies in both directions.
\end{proof}

\section{Proof of Theorem \ref{th:corr}}\label{app:c}
\begin{proof}
By definition, we have  for all . The local popularity distributions  are identically distributed for each edge cache , so that 
where the expectation is over the profiles  and . This shows the local popularity are larger for the convex stochastic order than the global popularities (after re-scaling by the constant factor , which will not impact hit probabilities). Also, the optimal expected hit probability under a given popularity distribution  is a convex function of . Indeed, in a finite system , we have 

where the popularity of the items are independently drawn from . Convexity follows from maximum being a convex function. Then, by definition of the convex stochastic order, we have . Taking the limit as  and  proves the first statement.

To compute the almost sure limit of  as , the reasoning is very similar as that to compute the limit of  in Theorem~\ref{th:ABT}, except that we do not deal with how to estimate the probabilities. Therefore, we only explain the main steps. Instead of defining a unique threshold , we now need to define a threshold  at each edge cache (we will omit to write the dependency on  from now on). Again, for finite values of , this threshold is defined by the -th upper quantile of the distribution of  (where the dependence in  is omitted from the notation) for each . In other words, the local threshold  is characterized by 

where the limit follows from the limiting expression of equation~\eqref{eqn: limit local pop}. Intuitively, the local thresholds  become independent of the particular location  and of the value of  and converge to a same limiting value  as . The inner probability in the expression above can be computed explicitly, due to the particular form of  assumed here, which implies the inverse function  is well-defined (with  if  for all ) and onto:

It remains only to compute the limit of the optimal expected hit probability:

Again, the inner expectation can be computed explicitly:

which yields the claimed expression.
\end{proof}


\end{document}
