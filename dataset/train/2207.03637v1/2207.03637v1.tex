\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xspace}
\usepackage[]{algorithm2e}
\usepackage{bm}
\usepackage{pifont}
\usepackage{multirow}

\urlstyle{same}

\newcommand{\zj}[1]{\textcolor{blue}{\bf\small [#1 --ZJ]}}
\newcommand{\yi}[1]{\textcolor{green}{\bf\small [#1 -- YI]}}
\newcommand{\pc}[1]{\textcolor{yellow}{\bf\small [#1 -- PC]}}
\newcommand{\wc}[1]{\textcolor{red}{\bf\small [#1 -- WC]}}
\newcommand{\gn}[1]{\textcolor{magenta}{\bf\small [#1 -- GN]}}







\def\cls{\text{[CLS]}\xspace}
\def\sep{\text{[SEP]}\xspace}

\def\rowsep{\text{[SEP]}\xspace}
\def\cellsep{\text{|}\xspace}
\def\insep{\text{:}\xspace}

\def\smallcol{\hskip 6pt}
\def\tinycol{\hskip 2pt}

\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{cerulean}{rgb}{0.0, 0.48, 0.65}
\definecolor{cadmiumorange}{rgb}{0.93, 0.53, 0.18}

\def\phrase{\text{phrase}}
\def\cell{\text{cell}}
\def\ours{OmniTab\xspace}
\def\wtq{WikiTableQuestions\xspace}
\def\squall{SQUALL\xspace}
\def\wikisql{WikiSQL\xspace}

\def\argtopk{\arg\text{topk}}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\textcolor{lightgray}{\ding{55}}}
\newcommand{\toright}[1]{{\hfilll #1}}
\newcommand{\hlcell}{\cellcolor{gray!20}}

\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\renewcommand{\subsubsectionautorefname}{\S}
\renewcommand*{\figureautorefname}{Fig.}
\renewcommand*{\tableautorefname}{Tab.}
\renewcommand*{\algorithmautorefname}{Alg.}
\renewcommand{\equationautorefname}{Eq.}



\title{\ours: Pretraining with Natural and Synthetic Data \\ for Few-shot Table-based Question Answering}



\author{Zhengbao Jiang\thanks{~~Work was done when interning at Microsoft Azure AI.}, Yi Mao, Pengcheng He, Graham Neubig, Weizhu Chen \\
Language Technologies Institute, Carnegie Mellon University \quadMicrosoft Azure AI \\
\texttt{\{zhengbaj,gneubig\}@cs.cmu.edu} \\
\texttt{\{maoyi,penhe,wzchen\}@microsoft.com}}

\begin{document}
\maketitle
\begin{abstract}
The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value.
The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation.
In this paper, we aim to develop a simple table-based QA model with minimal annotation effort.
Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both \emph{natural} and \emph{synthetic} data to endow models with these respective abilities.
Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss.
We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model \ours, with the best multitasking approach achieving an absolute gain of 16.2\% and 2.7\% in 128-shot and full settings respectively, also establishing a new state-of-the-art on \wtq.
Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.\footnote{Code, pretraining data, and pretrained models are available at \url{https://github.com/jzbjyb/OmniTab}.}
\end{abstract}

\section{Introduction}\label{sec:intro}
\begin{figure*}[tb]
\begin{minipage}{\textwidth}
\begin{minipage}[b]{0.61\textwidth}
\centering
\includegraphics[width=\columnwidth, clip, keepaspectratio]{example.pdf}
\captionof{figure}{Example of natural and synthetic pretraining data and a manually annotated finetuning question. Phrases aligned with table elements and reasoning operations are marked with \textcolor{violet}{violet} and \textcolor{red}{red} respectively. [Table] denotes the linearized table.}
\label{fig:example}
\end{minipage}
\hfill
\begin{minipage}[b]{0.37\textwidth}
\centering
\includegraphics[width=\columnwidth, clip, keepaspectratio]{framework.pdf}
\captionof{figure}{The overall framework of generating and pretraining with natural data (\textcolor{cerulean}{blue} pipeline), synthetic data (\textcolor{cadmiumgreen}{green} pipeline), and finetuning with limited annotated questions (\textcolor{cadmiumorange}{orange} pipeline) for our \ours model.}
\label{fig:framework}
\end{minipage}
\end{minipage}
\end{figure*}

Humans are voracious information omnivores, consuming information in a number of forms.
Unstructured text is the form covered in most work in NLP, but another form widely used on the web, academic papers, and reports is the \emph{table}, where elements are organized in rows and columns and presented in a structured and succinct way.
Because of this, systems to aid information access over tables, such as table-based question answering (QA) \cite{wtq-2015-pasupat,sqa-2017-iyyer,wikisql-2017-zhong}, hold significant promise.
However, they also require understanding of the table structure and sophisticated reasoning across multiple elements to get the final answer.
This intrinsic complexity not only often requires special-purpose model designs such as pipeline systems that generate structured queries as intermediate steps \cite{liang-2018-mapo,wang-2019-latent,tabert-2020-yin,yu-grappa-2021}, but also adds an extra burden to the process of data annotation \cite{wtq-2015-pasupat,shi-2020-squall}.

Given the challenges above, we ask the question: ``can we create a \emph{simple} model that is able to answer complex table-based questions with \emph{minimal annotation effort}?''
Both modeling simplicity and limited assumptions regarding data availability would make it easier to apply table-based QA systems in practical scenarios.
To this end, we focus on developing a simple and generic end2end table-based QA model where only a limited number of annotated natural language (NL) questions are available; the first attempt to address this problem under few-shot settings.
In order to answer table-related questions, an end2end model needs to understand both the NL question and the table, build connections between the two formats, then perform complex reasoning.
Taking the manually annotated question in \autoref{fig:example} as an example, models need to align entities (``Collateral Damage'') and concepts (``film'', ``air'') in the question to elements in the table (the ``Collateral Damage'' cell and the ``Film'' and ``Date'' columns) and perform comparative reasoning based on chronological order (indicated by ``previous'' and ``before'') to find the final answer.
Motivated by this, we propose \textbf{\ours}, an omnivorous pretraining approach that consumes \emph{natural} data to endow models with the ability to understand and align NL with tables, and \emph{synthetic} data to train models to perform reasoning.

To obtain natural NL-table pairs, we propose a novel approach that leverages the multitude of tables freely available from the web, and uses \emph{retrieval} to pair them with relevant NL sentences.
Compared with manually defined heuristics used in previous work \cite{tapas-2020-herzig,tabert-2020-yin}, retrieval-based methods have the potential to discover better-aligned sentences.
We explore different retrieval methods including string-based matching, sparse retrieval, and dense retrieval \cite{dpr-2020-karpukhin,khattab-2020-colbert,gao-2021-coil}.
Given these retrieved pairs, phrases in the sentence that align with table elements are then masked and the model takes both the masked sentence and the linearized table as input to predict masked mentions.
For example, in \autoref{fig:example} the retrieved sentence describes a particular row and contains two phrases matching cells in the table (i.e., ``Spider-Man'' and ``\\bm{q}T\bm{a}N\{r_i\}_{i=1}^{N}Mij\bm{c}_{i,j}r_1\bm{a}\bm{q}TiiTr_1r_2Nr_Nr_i\bm{c}_{i,1}\bm{c}_{i,2}\bm{c}_{i,M}\bm{q}\bm{q}TP(\bm{a}|\bm{q}, T)\bm{c}_{i,j}\bm{s}114,844,116'' and ``\\bm{s}T\{\bm{p}_l\}_{l=1}^{L}\bm{e}_{\bm{p}_l}\bm{e}_{\bm{c}_{i,j}}\bm{e}_{\bm{c}_{i,j}} \cdot \bm{e}_{\bm{p}_l}A \in \mathbb{R}^{NM \times L}A\text{max-sp}(A, \text{dim}=i)iA\text{dim}=1\text{dim}=0\bm{p}_2>\bm{p}_3>\bm{p}_1\tau\bm{s}T\bm{s}^*T^*P_{\text{mask}}(\cdot|\cdot)\bm{o}\bm{q}\hat{\bm{q}}\text{score}_{\text{gen.}}(\hat{\bm{q}})=P^{\text{SQL2NL}}(\hat{\bm{q}}|\bm{o})\text{score}_{\text{ver.}}(\hat{\bm{q}})=P(\bm{a}|\hat{\bm{q}},T)K\mathcal{L}_{\text{sql}}=-\log P(\bm{a}|\bm{o},T)\mathcal{L}_{\text{mask}}+\mathcal{L}_{\text{QA}}+\mathcal{L}_{\text{sql}}\approxfffffff^*\in [-1, 1]\tau=0.6fff^*f^*\tau=0.6f\tau=0.5\tau=0.7\tau=0.6\tau=0.6$) \\ \end{tabular}} & w/o salient mask & 17.5 & 33.6 & 47.1 \\
 & w/o random mask & 23.1 & 37.8 & 48.5 \\
\bottomrule
\end{tabular}
\caption{WTQ test accuracy when pretraining on natural data obtained by different retrieval methods, and using two masking strategies separately. Design choices used in our final model are \underline{underlined}.}
\label{tab:exp_retrieval}
\end{table}

\paragraph{Random Masking vs. Salient Masking}
We use both salient mention masking that only masks mentions of cells in the sentence and random masking in our final model.
To examine the contribution of each masking strategy, we remove one masking strategy from the underlined model at the bottom of \autoref{tab:exp_retrieval}.
It is clear that both maskings help, with salient masking being the major contributor, which indicates that masking tokens indicative of alignment is more effective than aimless masking.

\paragraph{Comparison of Different Self-training Methods}
To study which element is crucial in self-training, we perform ablations to study various aspects of self-training including (1) selection criterion for questions (generation- vs verification-based) and (2) models used for verification (BART vs \ours) by comparing all variants under the same setting of 128 annotated SQL-NL. 
As summarized in \autoref{tab:exp_selftraining}, self-training on selected questions with the highest generation probabilities given by the SQL2NL model does not improve over the baseline without self-training, which is mainly because the SQL2NL model is too weak to output reliable generation probabilities.
However, our method that selects questions with the highest probabilities to elicit answers from \ours (last line) improve over no self-training by a large margin (4.3\%, 2.5\%, and 1.8\%), validating the idea of leveraging the strong QA capacity of \ours to assess the quality of generated questions.
To confirm the source of success, we perform a sanity check that selects sentences most \emph{unlikely} to elicit answers (min), and the performance indeed becomes much lower.
We also replace \ours with BART that is only finetuned with 128 annotations, and the performance is significantly lower, confirming that stronger QA models can provide a better assessment.

\begin{table}
\small
\centering
\begin{tabular}{l@{\smallcol}l@{\smallcol}lr@{\smallcol}r@{\smallcol}r}
\toprule
\multicolumn{3}{l}{\textbf{Model} \hfill{\textbf{WTQ-}}} & \textbf{16} & \textbf{128} & \textbf{1024} \\
\midrule
\multicolumn{3}{l}{TAPEX \cite{tapex-2021-liu}} & 10.4 & 23.1 & 45.5 \\
\midrule
\multicolumn{6}{c}{\emph{\ours w/ synthetic data from SQL2NL trained}} \\
\emph{criteria} & \emph{op.} & \emph{verify}  \\
w/o self-training & - & - & 26.5 & 35.0 & 45.5 \\
w/ generation-based & max & - & 24.0 & 35.8 & 44.3 \\
\multirow{3}{*}{\underline{w/ verification-based}} & min & \ours & 15.5 & 27.4 & 41.9 \\
 & max & BART & 28.9 & 36.4 & 45.4 \\
 & \underline{max} & \underline{\ours} & \textbf{30.8} & \textbf{37.5} & \textbf{47.3} \\
\bottomrule
\end{tabular}
\caption{WTQ test accuracy when pretraining on synthetic data generated from an SQL2NL model trained with 128 annotations and various self-training methods. Design choices used in our final model are \underline{underlined}.}
\label{tab:exp_selftraining}
\end{table}

\begin{figure}[tb]
\centering
\includegraphics[width=\columnwidth, clip, keepaspectratio]{sql2nl_line.pdf}
\caption{WTQ test accuracy (128-shot) using different numbers of annotated and self-training SQL-NL pairs.}
\label{fig:exp_sql2nl}
\end{figure}

\paragraph{Performance w.r.t.~Number of Annotated and Self-training Pairs}
Here we study the influence of increasing either annotated or self-training SQL-NL pairs.
We use the SQL2NL model trained with 128 annotated pairs as a starting point, and additionally using more annotated or self-training pairs.
As shown in \autoref{fig:exp_sql2nl}, using more annotated or self-training pairs both improves over the initial performance of 35.0\%.
However, the improvement due to self-training still falls far behind the supervised approach, demonstrating the challenge of learning a robust SQL2NL model with very few annotations.
The increasing trend of self-training suggests that further improvements may be provided by using more pairs in self-training.

\begin{table}[tb]
\small
\centering
\begin{tabular}{@{}l@{\tinycol}c@{\smallcol}c@{}}
\toprule
\multicolumn{1}{r}{\textbf{Cases favoring:}} & \textbf{Natural} & \textbf{Synthetic} \\
\midrule
Avg \#tok in questions / SQL & 10.6 / 11.4 & 10.8 / \textbf{12.9} \\
Avg \#aligned question tok with tables & \textbf{1.8} & 1.0 \\
\bottomrule
\end{tabular}
\caption{Statistics of cases favoring natural vs synthetic data. Numbers indicating advantages are in \textbf{bold}.}
\label{tab:case_stat}
\end{table}

\begin{figure}[tb]
\centering
\includegraphics[width=\columnwidth, clip, keepaspectratio]{sqlkw.pdf}
\caption{Frequent SQL keywords in cases favoring natural vs synthetic data. Keywords with a large frequency difference are annotated with frequencies.}
\label{fig:case_sqlkw}
\end{figure}

\subsection{Analysis}\label{sec:exp_analysis}
\paragraph{Roles of Natural and Synthetic Data}
We quantitatively verified using the multitasking experiment that natural and synthetic are complementary to each other, with the hypothetical reason that natural data excels at enhancing alignment while synthetic data is more targeted on endowing reasoning capabilities.
Our analysis on cases where natural pretraining succeeds but synthetic fails and the opposite cases confirms that this is \emph{indeed} the case.
Enabled by the fine-grained annotation in \squall \cite{shi-2020-squall}, we compare \ours trained on natural or synthetic data separately in the 128-shot setting, and study on the two groups of cases in the development set of \wtq.
Based on 309/315 cases favoring natural/synthetic pretraining, we witness a clear distinction on the average number of question tokens aligned with tables between the two groups in \autoref{tab:case_stat}, indicating that natural data is indeed more targeted at addressing the alignment across formats.
We also compute the frequency of each SQL keyword for the two contrasting groups of cases. As shown in \autoref{fig:case_sqlkw}, cases favoring synthetic data indeed involves reasoning-rich keywords more frequently, such as ``where = ( )'' which are often used in nested queries, and ``count * id'' which are often used in aggregation.

\paragraph{Performance under Topical Distributional Shift}
Last, we analyze the robustness of \ours under topical distributional shift on WTQ-TS, which splits WTQ into five topics.
We finetune \ours on one topic (128-shot) and test the resulting model on all five topics.
As indicated by \autoref{tab:exp_topic}, \ours outperforms TAPEX by a large margin across all topics, validating the robustness of our methods under topical shift.

\begin{table}
\small
\centering
\begin{tabular}{lr@{\smallcol}r@{\smallcol}r@{\smallcol}r@{\smallcol}r}
\toprule
\textbf{Train/test} & \textbf{Sports} & \textbf{Culture} & \textbf{People} & \textbf{Politics} & \textbf{Misc} \\
\midrule
\textbf{Sports} & +16.3 & +14.1 & +15.8 & +14.4 & +18.8 \\
\textbf{Culture} & +13.9 & +12.7 & +14.3 & +13.0 & +10.3 \\
\textbf{People} & +21.5 & +14.6 & +20.6 & +14.6 & +17.5 \\
\textbf{Politics} & +18.5 & +14.3 & +17.8 & +16.0 & +13.5 \\
\textbf{Misc} & +18.3 & +14.7 & +17.2 & +14.6 & +14.9 \\
\bottomrule
\end{tabular}
\caption{Accuracy gain (128-shot) of \ours over TAPEX when finetuned on one topic and tested on all.}
\label{tab:exp_topic}
\end{table}

\section{Related Work}\label{sec:related}
Table-based QA is a well-studied area from early systems using structured queries as intermediate steps \cite{krishnamurthy-2017-typeparser,liang-2018-mapo,wang-2019-latent,tabert-2020-yin,yu-grappa-2021} to recent advances that generate answers in an end2end fashion \cite{tapas-2020-herzig,tapex-2021-liu}.
Our methods follow the end2end approach because of its modeling simplicity and higher flexibility.
Given large amounts of table and text on the web, table-based QA and other table-related tasks such as semantic parsing \cite{strug-2021-deng,shi-2021-gap} and table understanding \cite{turl-2020-deng,tuta-2021-wang} start witnessing efforts invested in pretraining on both structured and unstructured information.
Most works leveraging retrieval to find relevant information to assist pretraining are designed for text format only \cite{guu-2020-realm,lewis-2020-para}, while the majority of table-based pretraining still use alignment heuristics \cite{tapas-2020-herzig,tabert-2020-yin}.
There are some initial attempts to perform retrieval over tables \cite{ouz-2020-unik,herzig-2021-dtr,ma-2021-uni}, but they mainly use tables as an additional information source while we focus on pairing tables with text for pretraining.

\section{Conclusion}
We propose an omnivorous pretraining approach that consumes both natural and synthetic data to enhance the ability to understand and align text and tables and the ability to perform reasoning.
Our extensive results demonstrate the effectiveness of both data and verify their complementary value.
Our empirical results together with the case analysis indicate that omnivorous pretraining can indeed benefit from the merits of both data, encouraging future advances in retrieval and synthesis to obtain higher-quality data and better pretraining strategies to combine heterogeneous data.

\section*{Acknowledgments}
We thank Qian Liu and Pengcheng Yin for their insightful comments and suggestions.

\bibliography{custom}
\bibliographystyle{acl_natbib}

\end{document}
