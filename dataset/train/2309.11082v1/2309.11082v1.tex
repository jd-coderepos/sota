





\documentclass[sigconf]{acmart}
\usepackage{amsthm}
\let\Bbbk\relax
\usepackage{amsmath,amssymb}

\usepackage{mathrsfs}
\usepackage{soul}
\usepackage{algorithm}  
\usepackage{algpseudocode}  
\usepackage{balance} 
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  \renewcommand{\algorithmicensure}{\textbf{Output:}} 



\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{acmcopyright}


\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[MM '23]{Proceedings of the 31st ACM International Conference on Multimedia}{October 29-November 3, 2023}{Ottawa, ON, Canada}
\acmBooktitle{Proceedings of the 31st ACM International Conference on Multimedia (MM '23), October 29-November 3, 2023, Ottawa, ON, Canada}
\acmPrice{15.00}
\acmDOI{10.1145/3581783.3612006}
\acmISBN{979-8-4007-0108-5/23/10}





\acmSubmissionID{1377}





\setlength{\abovecaptionskip}{0.3cm} \setlength{\belowcaptionskip}{-0.3cm} \settopmatter{printacmref=true}
\begin{document}


\title{Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning}




\author{Chen Jiang}
\authornote{Both authors contributed equally to this research.}
\email{qichen.jc@antgroup.com}
\affiliation{\institution{Artificial Intelligence Innovation and Incubation Institute, Fudan University}
  \institution{Ant Group}
  \country{} 
}


\author{Hong Liu}
\authornotemark[1]
\email{yizhou.lh@antgroup.com}

\author{Xuzheng Yu}
\email{yuxuzheng.yxz@antgroup.com}
\author{Qing Wang}
\email{wq176625@antgroup.com}
\affiliation{\institution{Ant Group}
  \country{} 
}

\author{Yuan Cheng}
\authornote{Corresponding author.}
\email{cheng_yuan@fudan.edu.cn}
\affiliation{\institution{Artificial Intelligence Innovation and Incubation Institute, Fudan University}
  \country{} 
}

\author{Jia Xu}
\email{steve.xuj@antgroup.com}
\author{Zhongyi Liu}
\email{zhongyi.lzy@antgroup.com}
\affiliation{\institution{Ant Group}
  \country{} 
}

\author{Qingpei Guo}
\authornotemark[2]
\email{qingpei.gqp@antgroup.com}
\author{Wei Chu}
\email{weichu.cw@antgroup.com}

\author{Ming Yang}
\email{m.yang@antgroup.com}
\affiliation{\institution{Ant Group}
  \country{} 
}

\author{Yuan Qi}
\email{qiyuan@fudan.edu.cn}
\affiliation{\institution{Artificial Intelligence Innovation and Incubation Institute, Fudan University}
  \country{} 
}

\renewcommand{\shortauthors}{Chen Jiang et al.}
\begin{abstract}
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search.
Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos.
Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. 
Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity.
To address these two issues, this paper improves contrastive learning using two novel techniques. 
First, to exploit hard examples for robust discriminative power, we propose a novel \textit{Dual-Modal Attention-Enhanced Module (DMAE)} to mine hard negative pairs from textual and visual clues. 
By further introducing a \textit{Negative-aware InfoNCE (NegNCE)} loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss.
Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. 
We thereby present a new \textit{Triplet Partial Margin Contrastive Learning (TPM-CL)} module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs.
The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction
to model subtle semantic differences.
Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
\end{abstract}



\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003338.10010403</concept_id>
<concept_desc>Information systems~Novelty in information retrieval</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Novelty in information retrieval}

\keywords{Text-Video Retrieval, Dual-Modal Attention-Enhanced, Negative-aware InfoNCE, Triplet Partial Margin Contrastive Learning}





\maketitle

\section{Introduction}
With the explosive growth of videos in recent years, the task of text-video retrieval has become increasingly essential and popular.
The goal of text-video retrieval is to retrieve videos that are most semantically relevant to the given text query.
A typical paradigm tends to first embed texts and videos into a joint latent space and then employ a distance metric to measure cross-modal similarity~\cite{HERO,univl2020,MMT,clip4clip2021,clip2video2021}. 
A critical challenge is to learn precise semantic similarities between texts and videos. The recent trend towards large-scale contrastive image-language pre-training like CLIP~\cite{clip2021} mitigates this issue to some extent~\cite{clip4clip2021,ts2net2022,xclip2022},
yet they tend to neglect the distinct role of hard examples, 
leading to confusion with hard positives/negatives and noisy correspondence. 
Moreover, most existing contrastive learning works focus on the pairwise semantic relation, which lacks the ability to measure different levels of semantic similarity~\cite{zhang2022ArcCSE}.



\begin{figure}[ht!] \centering
  \includegraphics[width=1.0\linewidth]{images/dmae2.png}
  \caption{Illustration of embedding distributions constrained by NegNCE and DMAE. 
  (a) Embedding distribution constrained by InfoNCE loss. Hard negative pairs that lie inside the elastic boundary are ignored. (b) Hard negative pairs inside the elastic boundary are adaptively incorporated into the NegNCE loss, and are pushed away from the anchor. (c) DMAE enlarges the elastic boundary and adds more hard negatives. (d) After training with DMAE, positive and negative pairs are further pushed away from each other. }
\label{fig:dmae}
\end{figure}

Contrastive learning becomes a popular representation learning paradigm for text-video retrieval recently~\cite{taco2021,univl2020,frozen2021,Li2021AlignAP,clip4clip2021,videoclip2022,xpool2022,xclip2022,ts2net2022}. 
Among them, the majority~\cite{videobert2019,univl2020,MMT,hanet2021,clip4clip2021,xpool2022,xclip2022,ts2net2022} rely on conventional pairwise contrastive losses (\textit{e.g.}, NCE~\cite{nce2010}, BCE~\cite{hanet2021}, infoNCE~\cite{infonce2018}) to learn a cross-modal embedding space, which minimizes the distance of matched pairs while maximizing the distance of all other negative pairs in a batch.
As shown in Fig.~\ref{fig:dmae}(a), the elastic boundary is defined by the farthest positives, and hard negatives refer to negatives that lie inside the elastic boundary and are closer to the anchor than the farthest positives. 
Usually, these conventional contrastive losses focus on positives and treat all negatives in the batch equally, without distinguishing between hard and easy ones.
However, hard negatives should make a greater impact on the discrimination between matched and mismatched pairs. 
Most of the current approaches adopt random sampling strategies~\cite{actbert2020,univl2020} or a specific sampling strategy~\cite{taco2021,hanet2021,acp2022} to cut the number of negatives to a fixed number. 
These strategies may result in sub-optimal learning or overlooking some hard negatives. This is because the limited number of selected negatives may not accurately reflect the true distribution of negative pairs.
To address these issues, we introduce a \textit{Negative-aware InfoNCE (NegNCE)} loss to adaptively find out all hard negative pairs inside the elastic boundary and incorporate them into the training objective as in Fig.~\ref{fig:dmae}(b). 

Nevertheless, the fundamental challenge is: "How to select as many hard negatives as possible?". Selecting hard negatives is not as straightforward as selecting positives.
As in Fig.~\ref{fig:dmae}(b), it is easy to miss the hard negative pairs near the elastic boundary as they are more challenging to differentiate from the positives.
Previous work~\cite{tupleinfonce2021} has observed that \textit{"strong variations between the positive and anchor samples usually result in smaller shared information but a greater degree of invariance against nuisance variables"}.
Therefore, 
we need to enhance text-video pairs so that contrastive learning can keep the shared information between positives and anchors while mining hard negatives.
Here, we propose a novel \textit{Dual-Modal Attention-Enhanced Module (DMAE)} to enlarge variations between easy and hard positives so that similar hard negatives that lie near the elastic boundary can be extracted  as in Fig.~\ref{fig:dmae}(c).
As the cases shown in Fig.~\ref{fig:dmae}, 
when matching visual content to a text query, we categorize text-video pairs accordingly. Those pairs with multiple frames matching the query are considered as \textit{easy positives}, while those with only a single frame match are classified as \textit{hard positives}.
By enlarging the discrimination between easy positives and hard positives, we can find those \textit{hard negatives} with single-frame visual content that only partly matches the text query.
Specifically, DMAE enhances text-video pairs through two components named \textit{Textual Attention} and \textit{Visual Attention} to find out more challenging hard negatives while filtering out easy negatives. 
In this way, we expect that positives are pulled closer to each other while negatives are pushed away after training as in Fig.~\ref{fig:dmae}(d).

As mentioned above, most existing works~\cite{clip4clip2021,ts2net2022,xclip2022,xpool2022,drl2022} focus on pairwise contrastive losses. Yet using pairwise losses essentially applies a binary quantization on the semantic similarity among text-video pairs, \emph{i.e.}, to either positive or negative pairs, which is a very coarse way to measure their relations. In contrast, we prefer to have a finer measurement on the semantic similarity among text-video pairs so as to take advantage of different levels of semantic similarity in contrastive learning. As the case in the right part of Fig.~\ref{fig:framework}, the original text query with more details of the video should be more similar than the masked one \textit{"the woman talk as the shelves are behind her"} or \textit{"the woman talk"} to the video. 
Therefore, we propose the \textit{Triplet Partial Margin Contrastive Learning (TPM-CL)} module to model the subtle difference in semantic similarity by leveraging partial order triplet samples. {Unlike previous work~\cite{falcon_relevance-based_2022} which adopts a relevance-based margin in the triplet loss to impart subtle semantic differences to the model, our focus is on the automatic generation of partial order triplet samples.}
Previous works construct triplet samples by offline text token masking for text matching~\cite{zhang2022ArcCSE} or in-batch hard negative mining for face recognition~\cite{facenet2015,triplet4reid2020}. Instead, we design an automatic scheme to generate partial text-video triplets by cross-modal interaction. 
Then an auxiliary target based on triplet ranking loss is adopted to consume the fine-grained semantic similarity among the triplet samples.

Extensive experiments on four text-video retrieval benchmarks show that the proposed method achieves the state-of-the-art performance, including MSR-VTT (212.2 rsum), MSVD (209.3 rsum), DiDeMo (206.3 rsum) and ActivityNet (207.5 rsum). Our approach outperforms the previous SOTA methods by +2.2\%, +0.7\%, +2.4\%, +2.7\% absolute improvements on these benchmarks. The ablation experiments demonstrate that the proposed DMAE and TPM-CL modules both improve the text-video retrieval performance. 

Our main contributions can be summarized as follows:
\begin{itemize}
    \item We propose a novel \textit{Dual-Modal Attention-Enhanced Module (DMAE)} to leverage hard negatives from textual and visual clues, and introduce a \textit{Negative-aware InfoNCE (NegNCE)} loss to explicitly incorporate these hard negatives into the training objective.
    \item We present a new \textit{Triplet Partial Margin Contrastive Learning (TPM-CL)} module to automatically generate partial order triplet samples by an adaptive token masking strategy with cross-modal interaction and model different levels of semantic similarity among them.
    \item We report top performance of retrieval performance on four text-video retrieval benchmarks and conduct extensive ablation studies to demonstrate the merits of our approach.
\end{itemize}


\section{Related Work}
\subsection{Text-Video Retrieval}
Most of the existing works directly apply the pre-trained backbone to obtain textual and visual representations, followed by interaction modules to measure the cross-modal similarity. 
With the success in many downstream tasks~\cite{COOP,CLIPCAP,STYLECLIP,DALL_E2,ActionCLIP}, CLIP~\cite{clip2021} has injected new impetus into the improvement of text-video retrieval and quickly becomes one of the mainstream backbones~\cite{clip4clip2021,xpool2022,clip2tv2021,camoe2021,clip2video2021,drl2022,xclip2022}.
For example, CLIP4CLIP~\cite{clip4clip2021} and CLIP2TV~\cite{clip2tv2021} transfer image knowledge to text-video retrieval to learn better representations. 
TS2-Net~\cite{ts2net2022} and CenterCLIP~\cite{centerclip2022} introduce a token selection or token clustering module to find the most informative tokens. 
XCLIP~\cite{xclip2022} first applies multi-grained contrastive learning to reduce the negative effects of unnecessary information. DRL~\cite{drl2022} proposes an effective interaction method to solve the sequential matching problem, and an auxiliary loss to reduce feature redundancy.
Yet these works do not pay enough attention to either the entailment relation among hard examples or triplet samples.
Our work also applies contrastive learning under the aforementioned typical paradigm. 
Differently, we are the first to improve the discriminative power from pairwise and triple-wise perspectives by hard negative mining and automatic partial order triplet generating.






\subsection{Negative Mining in Contrastive Learning}


Most of the negative mining methods can be divided into two categories: negative sampling and negative generating. The former focuses more on selecting hard negatives from a given corpus, while the latter aims to generate hard negatives in certain ways.

In terms of negative sampling, the majority of current methods~\cite{actbert2020,univl2020,frozen2021,clip4clip2021,videoclip2022,xclip2022,ts2net2022} use random sampling strategies.
TACo~\cite{taco2021} utilizes a token-aware cascade hard negative sampling strategy to select a fixed number of hard negatives within a batch. 
Moreover, triplet ranking loss with online triplet mining often acts as an auxiliary target to guide the text-video alignment, which usually selects the hardest negative sample to construct triplet samples~\cite{Dong2021DualEF,acp2022,hanet2021}.
Nevertheless, these strategies may result in sub-optimal learning or missing some hard negatives
as the distribution of hard pairs may be either scarce or dense, depending on the batch size.
Some other works~\cite{hit2021,lgdn2022, sstvlmss2022} introduce a momentum mechanism (like MOCO~\cite{moco2019}) to maintain a large negative queue as the corpus of negatives. Although MOCO-based methods can decouple the number of negative samples from the batch size, they require keeping a large memory up-to-date for negatives.
Different from them, we focus on adaptively finding out hard negative pairs according to the distribution of pairs and taking them into consideration in the pairwise contrastive loss.

As for negative generating, ~\cite{Wang2022LearnTU} proposes an offline strategy to generate negated text-video pairs by partially negating its original caption, which is unable to model the negation from visual clues. 
Authors in~\cite{MoCHi2020} adopt a feature mix-up strategy to generate hard negatives, which may lead to false negatives. Yet the major drawback of these methods is the lack of cross-modal interaction.  
This work generates fine-grained hard negatives by an adaptive token masking strategy with cross-modal interaction to construct triplet samples.
Coupling with the triplet ranking loss, it is able to model different levels of semantic similarity among them.

\section{Method}
This section presents each component of the proposed method (Fig.~\ref{fig:framework}). Starting with an introduction of feature representation in Sec.~\ref{subsec:feats}, we then elaborate on the details of our two core modules:(i) \textit{Dual-Modal Attention-Enhanced Module (DMAE)}, (ii) \textit{Triplet Partial Margin Contrastive Learning (TPM-CL)}, in Sec.~\ref{subsec:dmae} and~\ref{subsec:TPM-CL}, respectively, followed by the total objective function in Sec.~\ref{subsec:obj}.


\begin{figure}[ht!] \centering
  \includegraphics[width=0.9\linewidth]{images/simple_framework2.png}
  \caption{Overview of our approach, containing two major modules: (1)\textit{Dual-Modal Attention-Enhanced Module
(DMAE)}, which aims to mine hard negatives and is coupled with a \textit{Negative-aware InfoNCE(NegNCE)} loss to incorporate these hard negatives into training objective, and (2)\textit{Triplet Partial Margin
Contrastive Learning (TPM-CL)}, which aims to model the partial order of semantics among triplet samples.}
  \label{fig:framework}
\end{figure}


\subsection{Feature Representation}\label{subsec:feats}
Given a text set  and a video set , our target is to learn a similarity function , which calculates the similarity score between a text  and a video .
Following the typical text-video retrieval framework~\cite{clip4clip2021, clip2tv2021,clip2video2021}, our model is composed of a text encoder  and a video encoder , which leverages CLIP~\cite{clip2021} as a backbone. The text encoder  produces the sentence-level textual feature  and the word-level textual feature , where  is the length of  and  is the dimension of features. The video encoder  produces the frame-level visual feature  and the patch-level visual feature , where  is the number of frames and  is the length of the patch sequence.
Note that  and  are extracted from separate frames and the interaction among frames is ignored. Thus, we further use a temporal encoder to aggregate the features of all frames as in previous works ~\cite{clip4clip2021,ts2net2022,xclip2022,xpool2022}. Then, we obtain the aggregated frame-level visual feature .


\subsection{Dual-Modal Attention-Enhanced Module}\label{subsec:dmae}

To alleviate the limitations of InfoNCE loss for overlooking hard negatives, we present a modified \textit{Negative-aware InfoNCE (NegNCE)} loss. 
In addition, we introduce a novel \textit{Dual-modal Attention-Enhanced Module (DMAE)} to optimize representations of text-video pairs, aiming to find out more challenging hard negatives. 
As shown in Fig.~\ref{fig:dmae_diagram}, DMAE consists of two components, which are 1) \textit{Textual Attention}, aiming to mine crucial textual clues; and 2) \textit{Visual Attention}, aiming to explore the intrinsic characteristics from visual clues. 
Then we obtain the \textit{Textual Attention Matrix  and Visual Attention Matrix }, which are applied to incorporate the crucial textual and visual clues into the final similarity calculation. After that, we get the attention-enhanced similarity matrix  and employ the NegNCE loss to train our model.


\begin{figure}[ht!] \centering
\includegraphics[width=0.9\linewidth]{images/dmae_diagram.png}
  \caption{Illustration of DMAE, which mines hard negatives from textual and visual clues}.
\label{fig:dmae_diagram}
\end{figure}


. \label{subsec:dmae_text}
Some works~\cite{finegrained2019,Chen2020FineGrainedVR,taco2021} have observed that content words with specific PoS tags, such as nouns and verbs, are more likely than function words to be aligned with visual content in the video. Moreover, words with a high frequency in a paragraph also tend to show higher relevance to videos. Hence, our idea is to obtain two weight vectors  and  from these two aspects for modeling the crucial textual clues.
The algorithm is shown in Algorithm~\ref{alg:algo_ta}. Next, as shown in the left part of Fig.~\ref{fig:dmae_diagram}, we construct the \textit{Textual Attention Matrix } as follows:
   
where  denotes element-wise multiplication.

\begin{algorithm}[htb]  
  \caption{Textual Attention.}  
  \label{alg:algo_ta}  
  \begin{algorithmic}[1]  
    \Require  
      A text query  with  words of the video , ;  
      All description sentences of the video ;
    \Ensure  
      A weight vector of PoS, ; 
      A weight vector of word frequency, ; 
    \State Extracting the PoS of each word with the \textit{Spacy}~\cite{spacy2} toolkit; \State Defining the significant PoS set:  = ['NOUN','VERB','ADJ'];
    \State Concatenating all sentences into a paragraph ;
    \State Calculating the word frequency with the \textit{TF-IDF} method~\cite{SprckJones2021ASI,Aizawa2003AnIP};
    \State Selecting the irrelevant word set : the  words with the lowest tf-idf score in ;
    \For{ to }
        \If{ PoS of word }  \State ; \Comment{ by default}
        \Else \State ;
        \EndIf
        \If{word } \State ;
        \Else \State ;
        \EndIf
    \EndFor 
    \label{code:fram:select} \\  
    \Return , ;  
  \end{algorithmic}  
\end{algorithm}  


. \label{subsec:dmae_vis}
Due to the redundancy nature in continuously changing visual frames, there often exists more than one critical frame. Some recent works \cite{centerclip2022,ts2net2022} apply a token selection algorithm to reduce the redundant visual tokens, which may abandon informative tokens due to the limited number of selected tokens. Differently, our work argues that shared information of critical frames can facilitate representations as well. 
Towards this end, we aim to enhance samples by aggregating the shared information of critical frames.

As shown in the right part of Fig.~\ref{fig:dmae_diagram}, we first utilize cosine similarity to compute the similarities between frames based on . Then, we obtain the self-frame similarity matrix  to capture the intrinsic similarity relations among frames. 
Next, we build the \textit{Visual Attention Matrix } as follows:

where topK is set to top-2 by default.  preserves similarities between the two most similar frames and erases the others to 0.


After obtaining  and , we construct the attention-enhanced similarity matrix  for each text-video pair  as follows:

where   means dot-product,  is the original cross similarity matrix calculated based on the input textual features and visual features.

Finally, we apply the \textbf{T}oken-wise \textbf{I}nteraction(\textbf{TI}) or the \textbf{W}eighted \textbf{T}oken-wise \textbf{I}nteraction(\textbf{WTI}) method \cite{drl2022} on  to get the final similarity score  of each pair .



. 
Most prior works~\cite{clip2021,ts2net2022,xclip2022,drl2022} adopt the symmetric InfoNCE loss to optimize the retrieval model, which only considers the positive pairs () with little attention to the hard negative pairs (. The InfoNCE loss can be formulated as:  


Different from the above InfoNCE loss, we propose a modified \textit{Negative-aware InfoNCE (NegNCE)} loss, {which identifies all hard negatives within a batch and penalizes them more heavily in the training loss.}




In order to adaptively find out the hard negative pairs, we additionally compute a marginal similarity score  for all pairs  in a batch of  pairs, which is expected to measure the distances between the hard negative and positive pairs. Concretely, the marginal similarity score is calculated by: 

where  is the margin and is set to 0 by default.  denotes that when the similarity of the negative pair  is larger than the similarity of the positive one , it equals the difference between them, otherwise it will be set to zero.
Therefore, if , we set the pair , where  is the set of hard negative pairs and represents all negatives inside the elastic boundary in Fig.\ref{fig:dmae}(c).

Next, the effect of hard negative pairs can be measured as:

where  is the number of negative pairs in . The symmetric probabilities  and  of each pair  are computed by: 




Finally, we compute the symmetric weighted loss based on the corresponding positive and negative pairs as follows:



where  and  are the weighting parameters.


\subsection{Triplet Partial Margin Contrastive Learning}\label{subsec:TPM-CL}
In this section, we elaborate on the details of the proposed \textit{Triplet Partial Margin Contrastive Learning (TPM-CL)} module. In order to capture different levels of semantic similarity, the TPM-CL module automatically generates partial order triplet samples for matched text-video pairs and optimizes an auxiliary \textit{Triplet Partial Margin (TPM)} loss. As shown in Fig.~\ref{fig:TPM-CL_diagram}, TPM-CL is formed by two key components, namely, 1)\textit{Cross-Modal Token Weight Predictor} and 2)\textit{Adaptive Token Selector}. The former aims to utilize the cross-modal interaction to predict token weights. The latter generates partial triplets by masking informative textual and visual tokens according to their weights. At last, we design an auxiliary target based on triplet ranking loss to learn the similarity levels.

\begin{figure}[ht!] \centering
  \includegraphics[height=50mm,width=0.9\linewidth]{images/tpmc.png}
  \caption{Illustration of \textit{TPM-CL}, which generates partial order triplet samples with cross-modal interaction and models the subtle difference in semantics among them.}
  \label{fig:TPM-CL_diagram}
\end{figure}

. Intuitively, the importance of the visual patches and the text words differs depending on the given context. Thus, we use coarse-grained sentence-level and frame-level features to select the most informative tokens. 

Given the word-level textual feature  and the frame-level visual feature , we first apply a linear projection layer (an MLP) over  for dimension alignment and output . 
We then concatenate  with . Finally, we feed the concatenated feature to an adaptive module  to calculate the weight of each textual token:

where  is composed of another MLP and a Softmax layer.


In the same manner, we use the sentence-level textual feature  and the patch-level visual feature  to calculate the element-wise weight of each visual token, which can be formulated as:

where  has a similar structure to .


. After obtaining the weight of each textual and visual token, we adopt an adaptive approach to generate a triplet with entailment relation. We first mask the original textual and visual features according to the binary masks as follows:


where  denotes element-wise multiplication with broadcasting,  and  are the binary masks for the textual and visual tokens, respectively. The element  in  or  is defined as:

where  means the cumulative weights till the element  which ranks th in descending order. And  is a fixed threshold that indicates the ratio of features to be masked.

We then derive the weighted global textual feature:

and the corresponding masked textual feature:

where operation  in Eq.~\ref{eq:tg}-\ref{eq:tgh} means dot-product. 

Finally, we apply a temporal encoder like in Sec.~\ref{subsec:feats} to aggregate  to obtain the masked visual feature .



. In order to model cross-modal partial order of semantics, we formulate the margin losses as:

where  is the margin constant. 

Finally, the auxiliary \textit{Triplet Partial Margin (TPM)} loss is formulated as:




\subsection{Objective Function}\label{subsec:obj}
Given a batch of  video-text pairs, the model generates a  similarity matrix. We employ the \textit{Negative-aware InfoNCE} loss  to jointly incorporate the effects of positive and hard negative pairs. Moreover, we also utilize the \textit{Triplet Partial Margin} loss  to model different levels of semantic similarity among triplet samples. Hence, the total training loss  is defined as:

\begin{table*}[]
    \centering
    \caption{Retrieval results on MSR-VTT-1kA.  denotes that results are obtained by our re-training.}
    \label{tab:res_msrvtt}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\scalebox{1.0}{
    \begin{tabular}{l| c c c c c c| c c c c c c}
        \toprule
       \multicolumn{1}{c}{} & \multicolumn{6}{|c}{Text-to-Video Retrieval} & \multicolumn{6}{|c}{Video-to-Text Retrieval} \\ \toprule
        {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
       \multicolumn{13}{l}{\textit{CLIP-ViT-B/32}} \\ 
        {CLIP4Clip~\cite{clip4clip2021}} &44.5	&71.4	&81.6	&2.0	&15.3	&197.5 &- &- &- &-&- &- \\ 
        {CenterCLIP~\cite{centerclip2022}} & 44.2	&71.6	&82.1	&2.0	&15.1	&197.9 &42.8	&71.7	&82.2	&2.0	&10.9	&196.7 \\ 
        {CLIP2TV~\cite{clip2tv2021}} & 46.1	&72.5	&82.9	&2.0	&15.2	&201.5	&43.9	&73	&82.8	&2.0	&11.1	&199.7 \\ 
        {XPool~\cite{xpool2022}} &46.9	&72.8	&82.2	&2.0	&14.3	&201.9 &- &- &- &- &- &- \\  
{XCLIP~\cite{xclip2022}} & 47.4& 73.4& 83.1& 2.0& 13.7& 203.9 & 46.7&  72.7&  83.0&  2.0&  10.0&  202.4 \\   
{DRL~\cite{drl2022}} &47.5& 73.8& 83.6& 2.0& 13.3& 204.9 &46.3& 72.7& 82.5& 2.0& 9.5& 201.5\\ 
{TS2-Net~\cite{ts2net2022}} &47.2	&73.7	&83.1	&2.0	&13.1	&204.0 &44.8	&74.3	&84.0	&2.0	&9.3	&203.1  \\ 
{Baseline} &45.3	&74.2	&83.5	&2.0	&13.0	&203.0	&45.5	&73.1	&83.9	&2.0	&9.6	&202.5  \\ 
        {Ours} & 46.6& \textbf{75.0}& \textbf{84.1}& \textbf{2.0}&13.3& \textbf{205.7} & 46.0& \textbf{74.7}& 83.0& \textbf{2.0}& 9.5& \textbf{203.7}\\
        {Ours} &46.9& \textbf{74.6}& \textbf{84.2}& \textbf{2.0}& \textbf{12.8}& \textbf{205.7}	&46.2& 73.7& \textbf{84.2}& \textbf{2.0}& \textbf{8.8}& \textbf{204.1}  \\ 

\midrule
        \multicolumn{12}{l}{\textit{CLIP-ViT-B/16}} \\  
         {CenterCLIP~\cite{centerclip2022}} & 48.4	& 73.8	& 82.0	& 2.0	& 13.8	& 204.2	& 47.7	& 75.0	& 83.3	& 2.0	& 10.2	& 206.0  \\
         {CLIP2TV~\cite{clip2tv2021}} &49.3	&74.7	&83.6	&2.0	&13.5	&207.6	&46.9	&75.0	&85.1	&2.0	&10.0	&207.0  \\
{DRL~\cite{drl2022}} &49.4& 76.4& 84.2& 2.0& 13.2& 210.0 &47.0& 77.1& 84.4& 2.0& 9.2& 208.5 \\
{XCLIP~\cite{xclip2022}} &49.0& 76.9& 83.7& 2.0& 13.6& 209.6 &47.9& 75.0& 83.2& 2.0& 9.8& 206.1\\
{TS2-Net~\cite{ts2net2022}} &47.8& 76.8& 85.2& 2.0& 13.7& 209.8 & 47.8	& 76.0	& 84.6	& 2.0	& 8.5	& 208.4\\
       {Baseline} &48.6 &74.8	&84.4	&2.0 &13.6	&207.8	&\textbf{48.0}	&75.9	&83.1	&2.0	&9.6 &207.0 \\ 
       {Ours} &  \textbf{49.3}  &\textbf{77.0}  &\textbf{85.9}  &\textbf{2.0}	 &\textbf{12.7}	 &\textbf{212.2}  &\textbf{47.9}	 &76.0	 &\textbf{85.1}	 &\textbf{2.0}	 &9.1	 &\textbf{209.0} \\ 
{Ours} & \textbf{49.9}& 75.8& \textbf{85.5}& \textbf{2.0} & \textbf{12.5}& \textbf{211.2} &\textbf{49.6}& \textbf{76.3}& \textbf{85.0}&\textbf{2.0}& \textbf{8.5}& \textbf{210.9} \\ \bottomrule
     \end{tabular} }
\end{table*}

\begin{table}[]
\centering
    \caption{Retrieval results on MSVD.  denotes re-training.}\label{tab:res_msvd}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
        \toprule
         {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
        {CLIP4Clip~\cite{clip4clip2021}} & 45.2	&75.5	&84.3	&2.0	&10.3	&205.0 \\ 
        {CLIP2TV~\cite{clip2tv2021}} & 47.0	&76.5	&85.1	&2.0	&10.1	&208.6 \\ 
{DRL~\cite{drl2022}} & 46.5&  76.3&  85.0&  2.0& 10.7& 207.8 \\ 
{TS2-Net~\cite{ts2net2022}} & 44.0	&75.5	&84.6	&2.0	&10.4	&204.1 \\
{Baseline} & 44.0	&75.2	&84.2	&2.0	&10.9	&203.4 \\
{Ours} & 46.1 	& \textbf{76.4} & 	\textbf{85.0} & 	\textbf{2.0} & 	\textbf{10.1} & 207.5 \\

{Ours} & \textbf{46.9}&  \textbf{76.8}&  \textbf{85.6}&  \textbf{2.0}&  \textbf{9.7}&  \textbf{209.3} \\ \bottomrule
    \end{tabular}
\end{table}


\begin{table}[]
\centering
    \caption{Retrieval results on DiDeMo.  denotes re-training.}\label{tab:res_didemo}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
        \toprule
         {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
        {CLIP4Clip~\cite{clip4clip2021}} & 42.5	&70.2	&80.6	&2.0	&17.5	&193.3 \\ 
        {CLIP2TV~\cite{clip2tv2021}} & 45.5	&69.7	&80.6	&2.0	&17.1	&195.8 \\ 
{TS2-Net~\cite{ts2net2022}} & 41.5& 70.9& 80.6& 2.0& 13.9& 193.0 \\ 
{DRL~\cite{drl2022}} & 46.5& 73.9& 83.5& 2.0& 13.3& 203.9 \\ 
{Baseline} &  44.4	&73.3	&82.6	&2.0	&13.1	&200.3 \\ {Ours} & 45.2	&\textbf{74.1}	&\textbf{84.3}	&\textbf{2.0}	&\textbf{12.7}	&\textbf{203.6} \\{Ours} &\textbf{46.7}	&\textbf{75.6}	&\textbf{84.0}	&\textbf{2.0}	&\textbf{11.7}	&\textbf{206.3}  \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[]
\centering
    \caption{Retrieval results on ActivityNet.  denotes re-training.}
    \label{tab:res_activity}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
        \toprule
         {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
        {CLIP4Clip~\cite{clip4clip2021}} &40.5	&72.4	&-	&2.0	&7.5	&-  \\  {CenterCLIP~\cite{centerclip2022}} &43.9 	&75.3 	&85.2 	&2.0 	&7.0 	&204.4 \\
{TS2-Net~\cite{ts2net2022}} & 39.9&  72.3&  84.3&  2.0&  8.5&  196.5 \\ 
        {DRL~\cite{drl2022}} & 44.2	& 74.5	& 86.1	& 2.0	& - & 204.8 \\     {Baseline} &41.1& 72.3&84.1& 2.0& 8.2& 197.5 \\  {Ours} &\textbf{44.8}& 74.4& 85.1& \textbf{2.0}& 7.4& \textbf{204.3}   \\  {Ours} &\textbf{44.9}& \textbf{76.1}& \textbf{86.5}& \textbf{2.0}& \textbf{6.6}& \textbf{207.5}  \\ \bottomrule
    \end{tabular}
  \vspace{-0.2cm}
\end{table}

\section{Experiments}\label{sec:exp}
\subsection{Experimental Setting}\label{subsec:exp_setting}
\textbf{Datasets}
To validate the effectiveness, we conduct experiments on four popular text-video retrieval datasets, including MSR-VTT~\cite{msrvtt2016}, MSVD~\cite{msvd2011}, DiDeMo~\cite{didemo2017} and ActivityNet~\cite{activitynet2015}.
\textbf{MSR-VTT}~\cite{msrvtt2016} is a general video dataset collected from YouTube and contains 10k videos and 200k captions. The videos range in length from 10 to 32 seconds. We train models on 9K videos, and report results on the 1K-A test set like~\cite{ts2net2022,xclip2022,drl2022}. 
\textbf{MSVD}~\cite{msvd2011} contains 1,970 videos and the duration of videos varies from 1 to 62 seconds. There are 40 English captions annotated for each video. The number of videos in the train/validation/test split is 1,200/100/670, respectively.
\textbf{DiDeMo}~\cite{didemo2017} is one of the largest and most diverse datasets for the temporal localization of events in videos given natural language descriptions and contains 10k Flickr videos annotated with 40k sentences. Following earlier studies~\cite{clip4clip2021,ts2net2022,xclip2022}, all captions from a video are concatenated together for video-paragraph retrieval.
\textbf{ActivityNet}~\cite{activitynet2015} contains 20k YouTube videos with 100k caption annotations. 
The videos are 120 seconds long on average. 
We concatenate all of a video's descriptions into one paragraph and evaluate the model with video-paragraph retrieval on the \textit{val1} split.





\textbf{Evaluation Metrics}
For a fair comparison, we evaluate the experimental results using standard text-video retrieval metrics: Recall at Rank K (R@K, higher is better), Median Rank (MdR, lower is better), Mean Rank (MeanR, lower is better) and rsum (higher is better). R@K calculates the percentage of correct samples in the top-K retrieved points to the query sample. 
Following previous works~\cite{clip2tv2021,ts2net2022,xclip2022}, we report results for R@1, R@5, R@10. In order to reflect the overall retrieval performance, we also sum together all the R@K results as rsum like in~\cite{Chen2020FineGrainedVR,Dong2021DualEF,hanet2021,ts2net2022}, which is the main concern in our experiments.
MdR measures the median rank of correct items in the retrieved ranking list and MeanR calculates the mean rank of correct items in the retrieved ranking list. 


\textbf{Implementation Details}
Our experiments are conducted on 8 NVIDIA Tesla V100 32GB GPUs using PyTorch. We initialize the text and video encoder with pre-trained weights from CLIP~\cite{clip2021}, while other modules are initialized randomly. We adopt the Adam optimizer~\cite{Kingma2014AdamAM} to train our model and decay the learning rate using a cosine schedule strategy~\cite{Loshchilov2016SGDRSG}. We set the learning rate  1e-7 and 1e-4 for text/video encoder and other modules, respectively. For MSR-VTT and MSVD, we set the max query text length, max video frame length and batch size to 32, 12 and 128, and apply  for the temporal encoder. We set the max query text length and max video frame length as 64 in ActivityNet and DiDeMo. Because of GPU memory limitations, we reduce the batch size of DiDeMo and ActivityNet to 64 and adopt  for the temporal encoder.
We perform ablation experiments on the MSR-VTT dataset and the base model is ViT-B/16, while for the other datasets, the base model is ViT-B/32.
During training, we set the NegNCE loss weight  and  (in Eq.~\ref{eq:pos_neg}), and the TPM-CL parameters  and (in Eq.~\ref{eq:triplet_mask} and Eq.~\ref{eq:triplet_loss}).

\vspace{-0.2cm}
\subsection{Comparison with State-of-the-Art Methods}
We compare our approach against recent works (CLIP4CLIP, TS2-Net, DRL, \textit{etc}.) on MSR-VTT, MSVD, DiDeMo and ActivityNet datasets. 
Note that the performance may be affected by many factors, such as environment and algorithm module settings.
To mitigate the influence of the environment (\textit{e.g.}, GPU memory and version), we re-trained some experiments of previous methods in a unified environment setting. 
For a fair comparison with different methods, we show the results of our approach with two similarity calculation methods, \emph{i.e.}, \textbf{TI} and \textbf{WTI} \cite{drl2022}(denoted as  and  in Tab.~\ref{tab:res_msrvtt}-\ref{tab:res_activity}).
We set our baseline model as the degraded model, which removes the two core modules (DMAE and TPM-CL) and applies TI for similarity calculation.


We can see that our approach notably outperforms the baseline model in terms of all evaluation metrics and achieves the state-of-the-art performance. For the MSR-VTT dataset in Tab.\ref{tab:res_msrvtt}, our approach outperforms existing methods by a large margin on both ViT-B/32 and ViT-B/16 at two retrieval directions. Specifically,  outperforms DRL by nearly 1\% and over 2\% improvements on rsum of ViT-B/32 at two directions, respectively.
When compared with the baseline model using ViT-B/16,  obtains \textbf{4.4\%} and \textbf{2.0\%} improvements at two directions, respectively, while  largely improves rsum by \textbf{3.4\%} and \textbf{3.9\%}, where the R@1 gains 1.3\% and 1.6\% improvement. Moreover, compared to the previous SOTA methods (\emph{i.e.}, DRL and TS2-Net) using ViT-B/16, we have also over 2\% improvements on rsum at two directions.


We also further verify the generalization and robustness of our approach on MSVD, DiDeMo and ActivityNet.
Precisely, on the MSVD dataset as shown in Tab.~\ref{tab:res_msvd}, we observe that  outperforms the baseline model by \textbf{4.1\%} improvement on rsum, while  achieves 1.5\% gains compared to DRL.
For the DiDeMo dataset in Tab.~\ref{tab:res_didemo}, compared with the baseline and DRL,  surpasses all their evaluation performance and gains improvements of \textbf{6.0\%} and 2.4\% on rsum, respectively.
In the case of the ActivityNet dataset in Tab.~\ref{tab:res_activity}, our approach outperforms other existing methods by a large margin and achieves SOTA results on all evaluation metrics.
In general, the steady progress on several benchmarks is a solid indication of the effectiveness of our approach.





\vspace{-0.1cm}
\subsection{Ablation Study}\label{subsec:ablation}
In this section, we conduct ablation experiments on the MSR-VTT to verify the effectiveness of each module in our approach. 
\begin{table}[]
    \centering
    \caption{Retrieval performance with different settings of DMAE on the MSR-VTT.}\label{tab:res_c1}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{1pt}\begin{tabular}{l| c c c c c c}
        \toprule
        {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
{Baseline} &48.6 &74.8	&84.4	&2.0 &13.6	&207.8 \\ \midrule
{Exp1(+NegNCE)} & 49.3	& 75.9& 83.8& 2.0	&12.8	& 209.0 \\ 
        {Exp2(+NegNCE+TA)} & 48.7 &76.5 &84.3 &2.0 &\textbf{12.7}	& \textbf{209.5} \\
        {Exp3(+NegNCE+VA)} & \textbf{50.0} &75.7 &84.5 &\textbf{1.5} &\textbf{12.7}	& \textbf{210.2} \\ {Exp4(+All)} &49.5 & \textbf{76.7} & \textbf{84.7} &2.0	&12.8	&\textbf{210.9}  \\ \bottomrule
    \end{tabular}
\end{table}

\begin{table}[]
    \centering
    \caption{Ablation studies about the weighting parameters of NegNCE loss (in Eq.~\ref{eq:pos_neg}) on the MSR-VTT. The experiment setting is the same as Exp4 in Tab.\ref{tab:res_c1}.}.
    \label{tab:res_c1_weight}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
        \toprule
        {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
        {Baseline} &48.6 &74.8	&84.4	&\textbf{2.0} &13.6	&207.8 \\ \midrule
        \multicolumn{7}{l}{\textit{with  by default}} \\  
        {} & 49.2	&76.0	&84.4	&\textbf{2.0}	&13.0	&209.6	\\
        {} & \textbf{50.1}	&75.4	&84.4	&\textbf{2.0}	&13.0	&209.9	\\
        {} &49.5 &\textbf{76.7} & 84.7 &\textbf{2.0}	&\textbf{12.8}	&\textbf{210.9} \\ 
        {} & 48.4 &75.7 &\textbf{85.5} &\textbf{2.0} &13.7 &209.6 \\ \bottomrule
    \end{tabular}
  \vspace{-0.3cm}
\end{table}



\subsubsection{Effectiveness of Dual-Modal Attention-Enhanced Module}
We first investigate the impact of DMAE and conduct an ablation study to compare different variants of each component. As shown in Tab.~\ref{tab:res_c1}, all variants see a big boost in terms of retrieval performance. 
Specifically, compared with the baseline model, 
DMAE with only NegNCE (\emph{i.e.}, Exp1) achieves merely 1.2\% gains on rsum. When DMAE is equipped with the components of \textit{Textual Attention} and \textit{Visual Attention} (\emph{i.e.}, Exp2 and Exp3), the performance further improves by 1.7\% and 2.4\%, respectively. At last, DMAE with all components (\emph{i.e.}, Exp4) obtains a notable improvement of \textbf{3.1\%} on rsum, where the R@1, R@5, R@10 gain 0.9\%, 1.9\%, 0.3\% improvements, respectively. Therefore, we conclude that all components in DAME contribute to the retrieval task and different components can promote each other to achieve better results.






\subsubsection{The Impact of weight in NegNCE loss}
To explore the impact of different weights in the NegNCE loss, we also design a group of experiments by setting different weighting parameters  with a fixed setting of  (\emph{i.e.}, the original InfoNCE loss is a special case of the NegNCE loss if ). From Tab.~\ref{tab:res_c1_weight}, it can be seen that the overall retrieval performance initially increases before reaching saturation (\emph{i.e.}, ), and then declines slightly. The main reason may be that when  is large, the model weights too much on the hard negative pairs. Conversely, if the  is small, the effect of hard negative pairs may be underestimated. 


\begin{table}[!htb]
    \centering
    \caption{Retrieval performance with TPM-CL on the MSR-VTT.}\label{tab:res_c1c2}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
        \toprule
        {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ \toprule
        {Baseline} &48.6 &74.8	&84.4	&\textbf{2.0} &13.6	&207.8 \\ {+TPM-CL} & \textbf{49.4} &76.1 &{85.1} &\textbf{2.0}	&13.2	&\textbf{210.6} \\ {+DMAE+TPM-CL} & 49.3 &\textbf{77.0} &\textbf{85.9} &\textbf{2.0}	&\textbf{12.7}	&\textbf{212.2} \\ \bottomrule
    \end{tabular}
  \vspace{-0.2cm}
\end{table}

\begin{table}[]
    \centering
    \caption{Ablation studies about the hyper parameters of TPM-CL (in Eq.~\ref{eq:triplet_mask} and Eq.~\ref{eq:triplet_loss}) on the MSR-VTT.}.
    \label{tab:hyper_TPM-CL}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
        \toprule
        {Method} & {R1} & {R5} & {R10} & {MdR} & {MeanR} & {rsum} \\ 
        \toprule
        {Baseline} &48.6 &74.8	&84.4	&\textbf{2.0} &13.6	&207.8 \\ 
        \midrule
        \multicolumn{7}{l}{\textit{with triplet ranking loss margin }} \\  
        {} &47.9	&76.1 &84.6	&\textbf{2.0}	&13.0	&208.6 \\
{} &48.8  &\textbf{76.3} &\textbf{85.0} &\textbf{2.0}    &12.8   &\textbf{210.1} \\  
{} &\textbf{49.2}  &75.6 &84.5 &\textbf{2.0}    &13.6   &209.3 \\  
        \midrule
        \multicolumn{7}{l}{\textit{with masked feature ratio }} \\  
        {} &48.8	&76.3	&85.0	&\textbf{2.0}	&12.8	&210.1 \\
{} &\textbf{49.3} &\textbf{77.0} &\textbf{85.9} &\textbf{2.0} &\textbf{12.7}	&\textbf{212.2} \\ 
{} &48.4    &76.6   &84.6   &\textbf{2.0}   &12.9   &209.6 \\ 
        \bottomrule
    \end{tabular}
\end{table}



\subsubsection{Effectiveness of Triplet Partial Margin Contrastive Module}
 Similarly, we also perform experiments to validate the effect of TPM-CL. The results in Tab.\ref{tab:res_c1c2} clearly demonstrate that the model with only TPM-CL outperforms the baseline model by 2.8\% on rsum, while the full model equipped with DMAE and TPM-CL further obtains a large margin of \textbf{4.4\%}, indicating that our two core modules are both beneficial to improve the retrieval performance.







\subsubsection{The Impact of hyper parameters in TPM-CL}
We conduct a group of experiments with different values of the triplet ranking loss margin  and the masked feature ratio . From Tab.~\ref{tab:hyper_TPM-CL}, we get the best rsum performance when  for a fixed setting of .  
Furthermore, with  varying, the overall performance first improves from 210.1 to 212.2 and then declines if  is fixed to 0.6. 
The main reason may be that a large  makes the masked sample quite different from the original one and a large  means the difference between them should be large enough to make sense. 
Thus, a moderate setting of both  and  can encourage the learning to examine the fine-grained semantic similarity among triplets.






\subsection{Qualitative Analysis}

\begin{figure}[] \centering
\includegraphics[width=0.9\linewidth]{images/examples.png}
  \caption{Visualization of text-to-video retrieval results on MSR-VTT. For each query, the top-3 results are displayed and sorted based on their similarity scores. The upper half of the two retrieval groups are the results with our full model, while the lower half are the retrieval results with the baseline model. Green box: ground truth.}
  \label{fig:examples}
  \vspace{-0.2cm}
\end{figure}

To qualitatively validate the effectiveness of our approach, we visualize some text-to-video retrieval examples from the MSR-VTT in Fig.~\ref{fig:examples}. Specifically, as in Fig.~\ref{fig:examples}(a) and (b), our model retrieves the correct videos that contain all matched fragments described in the text query (\emph{i.e.}, \textit{"child in pink"}, \textit{"a white bird"} and \textit{"an open box"} in (a), \textit{"woman in the purple blouse"} and \textit{"shelves"} in (b), respectively). 
Our model can successfully differentiate positives from those hard negatives with partly matched fragments. 
The examples in Fig.~\ref{fig:examples}(c) show that our model is capable of accurately capturing the corresponding hard positive video, even if the matched fragment is only present in a small segment of the video.
Meanwhile, we find that our model can focus on the relevant videos with fully or partly matched fragments while eliminating the false positives without matched fragments (\textit{e.g.}, the 2nd example in the lower part of Fig.~\ref{fig:examples}(c)).
To summarize, our model can accurately capture the correct videos and retrieve more related videos compared to the baseline model, demonstrating the merits of our approach.

\section{Conclusion}
This paper proposed a novel \textit{Dual-Modal Attention-Enhanced module (DMAE)} to mine hard negatives from textual and visual clues, and introduced a \textit{Negative-aware InfoNCE (NegNCE)} loss to adaptively incorporate them into the training objective.
Then we presented a new \textit{Triplet Partial Margin Contrastive Learning (TPM-CL)} module, which aims to focus on the automatic constitution of triplet samples and capture the fine-grained semantic similarity among them.
The effectiveness and superiority of our proposed method have been clearly demonstrated in comprehensive experiments on four text-video retrieval benchmarks.













\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{sample_t2v}

\appendix


























\section{Methods}
\subsection{More Details of Feature Representation}
\textbf{Text Encoder.} We adopt the text encoder of CLIP to generate the textual representation, which is a transformer encoder and typically consists of multi-head self-attention (MHSA) and feed-forward (FFN) networks.
Specifically, there are 12 layers and 8 attention heads in the transformer and the query, key, and value features have a 512-dimensional size. The text tokenizer employed in our experiment is a lower-cased byte pair encoding (BPE)~\cite{Sennrich2015NeuralMT} with a 49,152 vocab size. 
After adding a special token  and  at the beginning and end of the textual token sequence, respectively, we feed the token sequence into the text encoder to obtain the sentence-level textual feature  and the word-level textual feature , where  is the length of  and  is the dimension of features. 
The text representations  and  are outputs of the  token and corresponding word tokens from the last layer of the text encoder.  

\textbf{Video Encoder.} 
In this work, the video encoder is a standard vision transformer (ViT) with 12 layers, whose architecture is the same as the transformer in natural language processing. The difference is the additional visual tokenization process that turns frames into discrete token sequences. We first sample the given video  into  frames with the sampling rate of 1 frame per second (FPS) and convert each frame into  non-overlapped patches. After adding a token  at the beginning of each token sequence, we feed the token sequence into the video encoder to obtain the frame-level visual feature  and the patch-level visual feature , where  is the length of the patch sequence. 
The visual representations  and  are outputs of the  token and corresponding patch tokens from the last layer of the video encoder. 
Specifically, we use a ViT-B/32 model~\cite{Dosovitskiy2020AnII} with 12 layers and 8 attention heads following the previous work~\cite{clip4clip2021, ts2net2022, xclip2022}.

\subsection{More Details of the Cross-Modal Token Weight Predictor in TPM-CL}
Here we include some further elaboration on the details of the concatenation process described in Eq.~\ref {eq:text_token_weights} in the TPM-CL module.

As discussed in Sec.~\ref {subsec:TPM-CL}, we use a cross-modal feature interaction module to get the weight of each textual token. For a given text  and its word-level textual feature , a video  and its frame-level visual feature  , the detailed explanation of the cross-modal token weight predictor is as follows:
\begin{itemize}
\label{itm:xwp_detaila}
    \item First, we apply a dense layer with a trainable weight matrix  over the frame-level visual feature  to align its dimension with the word-level textual feature :
    
    where  in Eq.10. The dense layer is also a lightweight aggregator for the interaction among frames and its weight matrix  is updated during the training phase. 
    
    \item We then concatenate  with  to get the concatenated word-leval textual feature:
    
    where  is the concatenated feature in Eq.~\ref{eq:text_token_weights} and .
    
    \item Finally, we feed the concatenated feature  to an adaptive module  to calculate the weight of each textual token.
\end{itemize}

\section{Experiments}
\subsection{More Results with post-processing operations}

The hubness problem~\cite{radovanovic2010hubs,liu2019strong} has been shown to be particularly prevalent in high-dimensional embedding spaces. Qualitatively, the hubness problem means that a small proportion of samples occur disproportionately frequently among the set of k-nearest neighbours of all embeddings~\cite{querybank2022}, which can harm the model's performance. To mitigate the hubness problem observed among cross-modal embeddings for text-video retrieval, some methods adopted Inverted Softmax (IS) to improve the text-video matching. Among them, Dual Softmax loss (DSL)~\cite{camoe2021} and QueryBank Normalization (QB-Norm)~\cite{querybank2022} are two commonly used and effective post-processing operations(such as DRL~\cite{drl2022} uses QB-Norm, and CAMoE~\cite{camoe2021}, TS2-Net~\cite{ts2net2022} and STAN~\cite{liu2023revisiting} use DSL). They can bring significant advancements in performance. 

As shown in Tab.~\ref{tab:res_dsl}, we add the results of our approach with DSL~\cite{camoe2021} to make a fair comparison with previous methods using post-processing operations, \emph{e.g.}, DSL or QB-Norm. Note that our results with DSL still surpass all other methods with significant improvements and achieve SOTA performance on all four datasets. Overall, the good results on different datasets also demonstrate the effectiveness and generalization of our approach.

\begin{table}[]
    \centering
    \caption{Retrieval results with post-processing.  means  DSL~\cite{camoe2021} are utilized during inference.  denotes re-training.}\label{tab:res_dsl}
    \renewcommand{\arraystretch}{0.8}\setlength\tabcolsep{2pt}\begin{tabular}{l| c c c c c c}
     \toprule
     \multicolumn{7}{c}{MSR-VTT-1kA}  \\
        \toprule
         {Method} & {R1} & {R5} & {R10} & {MdR} & 
         {MeanR} & {rsum} \\ \toprule   
                \multicolumn{7}{l}{\textit{CLIP-ViT-B/32}} \\ 
       {QB-Norm~\cite{querybank2022}} &47.2 &73.0 &83.0 &2.0 &- &203.2 \\
       {CAMoE~\cite{camoe2021}} &47.3 &74.2 &84.5 &2.0 &11.9 &206.0 \\
        {TS2-Net\cite{ts2net2022}} &50.5	&76.7	&85.9	&1.0	&11.8	&213.1	\\ 
{STAN~\cite{liu2023revisiting}} &49.0 &74.8 &83.5 &2.0 &- &207.3 \\
        {Baseline} &49.8 &{76.9} &85.8 &2.0 & \textbf{11.3} &212.5 \\
        {Ours} &\textbf{51.7} &\textbf{77.6}  &\textbf{86.2} 	 &\textbf{1.0} 	 &\textbf{11.4} 	 &\textbf{215.5}  \\ 
        \midrule
        \multicolumn{7}{l}{\textit{CLIP-ViT-B/16}} \\  
       {TS2-Net\cite{ts2net2022}} &52.8	&79.0	&87.4	&1.0	&11.4	&219.2\\
{{DRL~\cite{drl2022}}} &53.3	&80.3	&87.6	&1.0	&-	&221.2	\\
       {STAN~\cite{liu2023revisiting}} &54.1 &79.5 &87.8 &1.0 &- &221.4\\
       {Baseline} &53.3 &78.8&87.1 &1.0& 11.0&219.2 \\
      {Ours} &\textbf{55.5} &\textbf{79.4}& \textbf{87.1} &\textbf{1.0}& \textbf{10.0}& \textbf{222.0} \\ \midrule
      
      \multicolumn{7}{c}{MSVD}  \\
        \midrule
         {Method} & {R1} & {R5} & {R10} & {MdR} & 
         {MeanR} & {rsum} \\ \midrule            
         \multicolumn{7}{l}{\textit{CLIP-ViT-B/32}} \\ 
         {QB-Norm~\cite{querybank2022}} &47.6 &77.6 &86.1 &2.0 &- &211.3\\
{TS2-Net~\cite{ts2net2022}} & 46.9	&77.3	&85.4	&2.0	&10.5	&209.6 \\
        {{Baseline}} & 46.4	&76.8	&84.6	&2.0	&11.3	&207.8 \\{Ours} &\textbf{48.7}	&\textbf{78.4}	&\textbf{86.3}	&\textbf{2.0}	&\textbf{9.8}	&\textbf{213.4}  \\ \midrule 

         \multicolumn{7}{c}{DiDeMo}  \\
         \midrule
          {Method} & {R1} & {R5} & {R10} & {MdR} & 
         {MeanR} & {rsum} \\ \midrule    
         \multicolumn{7}{l}{\textit{CLIP-ViT-B/32}} \\ 
         {QB-Norm~\cite{querybank2022}} &43.5 &71.4 &80.9 &2.0 &-&195.8\\
         {CAMoE~\cite{camoe2021}} &43.8 &71.4 &79.9 &2.0 &16.3 &195.1\\
         {TS2-Net~\cite{ts2net2022}} & 47.1	&73.9	&82.9	&2.0	&12.6	&203.9 \\
{STAN~\cite{liu2023revisiting}} &51.3 &75.1 &83.4 &1.0 &- &209.8\\
         {{Baseline}} & 49.1	&76.9	&85.1	&2.0	&10.5	&211.1 \\{Ours} &\textbf{52.7} &\textbf{79.3} &\textbf{86.6} &\textbf{1.0} &\textbf{10.5} &\textbf{218.6}  \\ \midrule
        
         \multicolumn{7}{c}{ActivityNet}  \\
         \midrule
          {Method} & {R1} & {R5} & {R10} & {MdR} & 
         {MeanR} & {rsum} \\ \midrule    
         \multicolumn{7}{l}{\textit{CLIP-ViT-B/32}} \\ 
         {CAMoE~\cite{camoe2021}} &51.0 &77.7 &- &- &- &-\\
         {TS2-Net~\cite{ts2net2022}} & 48.3 	&78.0 	&86.8 	&2.0 	&7.7 	&213.1  \\
        {{Baseline}} & 48.3	&76.3	&86.5	&2.0	&7.6	&211.1 \\{Ours} &\textbf{53.4}& \textbf{80.7}& \textbf{89.2}& \textbf{1.0}& \textbf{5.3}& \textbf{223.3} \\ 
         \bottomrule
    \end{tabular}
\end{table}




\subsection{More Qualitative Results with TPM-CL}

\begin{figure}[b] \centering
\includegraphics[width=0.9
\linewidth]{images/triplet_visual.png}
  \caption{Visualization of the internal mechanism of generating triplet samples in TPM-CL. Masked textual and visual tokens are marked with an orange strikethrough, whose cumulative weights  are less than . Note that, we give the heatmap visualization of the visual token aggregate weights, with bright yellow colors representing areas of large weights and dark blue colors representing areas of small weights.}
  \label{fig:examples_TPM-CL}
\end{figure}

Fig.~\ref{fig:examples_TPM-CL} shows the internal mechanism of generating the triplet samples. For the given example, the cross-modal interaction masks the informative textual tokens(\emph{i.e.}, \textit{"woman"} and the \textit{[CLS]} token, in the lower half of Fig.~\ref{fig:examples_TPM-CL}) and visual tokens(\emph{i.e.}, the \textit{[CLS]} token for each frame, and the visual tokens  and  in , in the upper half of Fig.~\ref{fig:examples_TPM-CL}). From the heatmap of the visual token aggregate weights, we can tell that the masked visual tokens  and  are primarily concentrated on the middle and bottom areas of , which correspond to the female subject.
This indicates that our model has effectively captured the informative tokens and produced accurate, fine-grained hard negatives to represent the subtle semantic differences.



\end{document}
