\documentclass[preprint,11pt]{elsarticle}
\usepackage{graphicx}
\usepackage{latexsym}

\begin{document}

\title{Matrix Code}

\author{M.H. van Emden}

\maketitle
\begin{abstract}
Matrix Code gives imperative programming a mathematical semantics
and heuristic power comparable in quality to functional and logic
programming.  A program in Matrix Code is developed incrementally
from a specification in pre/post-condition form.  The computations
of a code matrix are characterized by powers of the matrix when it
is interpreted as a transformation in a space of vectors of logical
conditions.  Correctness of a code matrix is expressed in terms of
a fixpoint of the transformation.  The abstract machine for Matrix
Code is the dual-state machine, which we present as a variant of
the classical finite-state machine.

\end{abstract}

\hyphenation{Dijk-stra}

\newtheorem{theorem}{Theorem}{}
\newtheorem{definition}{Definition}{}
\newtheorem{lemma}{Lemma}{}

\newcommand{\emln}{\\} \newcommand{\lmnt}[1]{\parbox{1.0in}{
   {\footnotesize {\emln \tt #1 }} \emln}}
\newcommand{\lmntWdth}[2]{\parbox{#1in}{
   {\footnotesize {\emln \tt #2 }} \emln}}
\newcommand{\nc}[1]{\lmntWdth{0.5}{#1}} \newcommand{\emp}{\lmnt{}} 

\newcommand{\set}[2]{\{#1 \mid #2\}}
\newcommand{\bk}[3]
  {{\tt \{#1\}#2\{#3\}}}
\newcommand{\brr}[2]{{\tt \{#1\}#2}}
\newcommand{\ktt}[2]{#1\{#2\}}

\newcommand{\vc}[3]{}
\newcommand{\trpl}[3]{{\tt \{#1\}#2\{#3\}}}
\newcommand{\mpr}[2]{{\tt #1:#2}} \newcommand{\mst}[2]{{\tt (#1,#2)}} 

\newcommand{\pr}[2]{\ensuremath{\langle #1,#2 \rangle}}

\newcommand{\ccc}{{\tt C}}
\newcommand{\cpp}{{\tt C++}}

\newcommand{\mc}[1]{\mathcal{#1}}

\section{Introduction}
By \emph{imperative programming} we will understand
the writing of code in which the state of the computation
is directly tested
and explicitly manipulated by assignment statements.
As a programming paradigm, imperative programming should be
compared with functional and logic programming.
Compared to these latter paradigms,
imperative programming is in an unsatisfactory state.
At least as a first approximation,
a definition in functional or logic programming
is both a specification and is executable.
In imperative programming proving that a function body
meets its specification is such a challenge
that it is not considered part of a programmer's task.
Another difference, probably related,
is that functional and logic programming
have an elegant mathematical semantics
in which the behaviour of an executable definition
is characterized as a fixpoint of the transformation
associated with the definition.

C is the programming language \emph{par excellence}
for imperative programming.
But in C one can fake functional programming to a certain extent
by doing as much as possible with function definitions,
function calls, and function parameters.
In this paper we will be concerned with what may be called
\emph{hard-core} imperative code:
code in the form of the body of a procedure
({\tt void} function in C) that contains no global variables
and that interacts only with its environment
by reading, testing, and modifying the actual parameters in the call.
These parameters,
together with any local variables that may be present,
comprise the state that is changed by assignment statements.
Surprisingly perhaps, hard-core imperative code does not
exclude function or procedure calls.

In imperative programming verification is a serious problem.
The problem is more serious than in functional or logic
programming because there the executable code can itself be 
the definition of the function or predicate to be executed.
Of course this ideal is rarely reached completely.
But it is a clear ideal for the programmer to strive after.
In imperative programming such an ideal does not exist.
Here correctness has to be proved independently of the code.
Although a powerful verification method was developed
by Floyd and by Hoare,
the experience is that it is hard to produce
a correctness proof for \emph{existing} code.
Dijkstra observed \cite{djk68a,djkInfotech71}
that code has to be \emph{designed
for} correctness proof.
He did not make this suggestion more concrete than
to call for the \emph{parallel development of proof and code}.

This paper is a contribution to the parallel development
of proof and code for imperative programming.
It takes the form of a new language,
called \emph{Matrix Code},
in which programs take the form of a matrix
of which the elements are binary relations among data states.
Matrix Code is distinguished by a development process
that begins with a
null code matrix, progresses with small, obvious steps, and
ends with a matrix that is of a special form that
is trivially translatable to a conventional language
like Java or {\tt C}.
The result of the translation has the same behaviour
as the one determined by the mathematical semantics of the code matrix.
Therefore the latter can be said to be executable.
As every stage in the development process is partially correct
with respect to the specification
(the correctness of the initial null code matrix
is \emph{very} partial).
Matrix Code comes close to the ideal in which
the code is itself a proof of partial correctness.
Matrix Code comes with an abstract machine,
which we call a \emph{dual-state machine} (DSM).
The DSM has easily identifiable special cases that are
trivially translatable to conventional languages like {\tt C} or Java.

\paragraph{Plan of the paper}

Because we derive DSMs from finite-state machines
we first review conventional automata theory
and regular expressions.
The step to DSMs is made by exploiting
the fact that formal languages
are mathematically similar to binary relations
and that both are best regarded as interpretations of
regular expressions.
Accordingly, in Section~\ref{sec:prelim} we establish
our notation and terminology for formal languages, binary
relations, and regular expressions. 

In Section~\ref{sec:FSM} 
we present the main definitions concerning FSMs.
This presentation is necessary because of slight,
yet essential variations in the usual definitions.
One such variation is that the transition is given as a matrix. 
In Section~\ref{sec:DSM} dual-state machines are
introduced as a close variant of FSMs.
The versatility of DSMs is demonstrated
by examples including an FSM, a Turing machine,
and DSMs that translate to {\tt C} programs for generating
prime numbers and for merging files.
As the latter type of DSM is the motivation for the
entire enterprise we devote Section~\ref{sec:MC}
to it.

In Section~\ref{sec:floyd} we
adapt the verification method of Floyd and Hoare to
Matrix Code.
In Section~\ref{sec:sys}
we solve as example problem the generation of prime numbers
in the systematic manner that is specific to Matrix Code.
This is the same problem as one of those treated by Dijkstra
in \cite{djkddh72},
so that Matrix Code can be compared with structured programming.
Although the derivation method for the prime-number
algorithm is original,
the computations of the resulting code are the same
as those of the conventionally produced version.
But Matrix Code is not only valuable as a method
for developing proof and code in parallel,
but, as we show in the derivation of the merging algorithm
in Section~\ref{sec:expressiveness},
it is valuable also
for finding algorithms that are more efficient
than those obtained in the conventional manner.
The final two sections
draw conclusions and survey related work
in widely scattered areas of computer science.

\section{Preliminaries}
\label{sec:prelim}

The dual-state machines to be introduced in this paper
are a variant of the classical finite-state machines.
Just as finite-state machines define formal languages,
dual-state machines define binary relations.
The similarity between the two types of machine
has to do with the similarity between formal languages
and binary relations.
One of the ways this similarity manifests itself
is the fact that formal languages and binary relations
have a natural notation in common: regular expressions.

\subsection{Formal languages}
Given a set , we denote the set of finite sequences
of its elements as .
We often think of  as an ``alphabet'',
of its elements as ``symbols'',
of the sequences of symbols as ``words'',
and of sets of words as a (formal) ``language''.

 includes the empty word,
the sequence of length 0, which is denoted .
The null language is the empty set.
This is not to be confused with the unit language,
which contains the empty word as its only word.

The concatenation of words  and  is denoted 
.
We have

for all words .
Concatenation of words is extended elementwise to
concatenation of languages:

Concatenation of a language  with itself
gives rise to the powers of :
 is the unit language
and  for all .
The closure  of  is defined as
.

The partial order  on formal languages is defined
to be set inclusion among the subsets of .

\subsection{Binary relations}
A binary relation on a set  is a subset of
the Cartesian product .
If  is in a binary relation,
then we say that  is an \emph{input};
 is a corresponding \emph{output} of the relation.

The null relation is the empty subset of .
The identity relation  on 
is . 
The union  of binary relations
 and   is defined to be
their union as subsets of .
The composition  of binary relations
 and 
is . 

Powers of a relation  are defined by
 for 
and  for .
We write  for .

The partial order  on binary relations is defined
to be set inclusion among the subsets of .

\subsection{Regular expressions}

The syntax of regular expressions \cite{prr90}
over a given set of constants is
defined as follows.
\begin{enumerate}
\item
The constants, , and  are regular expressions.
\item
If  and  are regular expressions
then so are  and
.

 and  are shorthand for

 is  and  is .
\item
If  is a finite set of regular expressions
and  is a regular expression,
then  is defined as
 if 
and
.
\item
If  is a regular expression,
then so is its closure .
\end{enumerate}

In practice a different syntax is used for regular expressions.
We see {\tt EF} for ,
{\tt E|F} for ,
{\tt E?} for ,
and
{\tt E+} for .

The syntax of regular expressions has several semantics:
algebras of which the elements and operations
can serve as interpretations of regular expressions.
Here these algebras are formal languages and binary relations
and serve as semantics for regular expressions. 
The way we intend these algebras to be semantics for regular
expressions is shown in Figure~\ref{fig:REsem}.

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{4in}
\begin{tabular}{c|c|c|c|c|c}
\parbox{20mm}{regular\\ expressions\0mm]}
  & \parbox{20mm}{empty\\ language}
    & \parbox{5mm}{}
      & \parbox{5mm}{}
        & \parbox{5mm}{}
          & \parbox{5mm}{}
 \\
\hline
\parbox{20mm}{\vspace{2mm}binary\\ relations\
&& E+F = F+E,\; E+E = E,\;
   E\cdot(F+G) = E\cdot F + E\cdot G,\; \\
&& (F+G)\cdot E = F\cdot E + G\cdot E,\; 
   E\cdot(F\cdot G) = (E\cdot F)\cdot G,\;\\
&& 0+E = E+0 = E,\;
   1\cdot E = E \cdot 1 = E,\;
   0\cdot E = E \cdot 0 = 0,\\
&& (E+F)^* = (E^*\cdot F)\cdot E^*,\;
   (E\cdot F)^* = 1+E\cdot(F\cdot E)^*\cdot E,\;\\
&& (E^*)^* = E^*,\;
   E^* = (E^n)^*\cdot E^{<n}
-9mm]
\begin{flushright}\end{flushright}

\begin{lemma}
An FSM 
has a computation \verb"(x,v),...,(y,v.u)"
of length  iff ,
for all 
\end{lemma}
\emph{Proof.} Straightforward induction on .\-0.9cm]
\begin{flushright}\end{flushright}

\begin{figure}[htbp]
\hrule
\vspace{0.3cm}
\begin{center}
\begin{minipage}[b]{2.1in}
\includegraphics[scale = 0.6]{FSM.pdf}
\vspace{-2cm}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}{1.5in}
{\footnotesize
\begin{tabular}{|l|l|l||l}
\lmntWdth{0.25}{B} & \lmntWdth{0.25}{A} & \lmntWdth{0.25}{S} & \\
\hline \hline
{\tt nil} & & & \lmntWdth{0.21}{H}  \\
\hline
& & {\tt sign}  {\tt nil} & \lmntWdth{0.25}{A}  \\
\hline
{\tt digit} &
 {\tt digit} & & \lmntWdth{0.25}{B}  \\
\hline
\end{tabular}
} \end{minipage}
\end{center}
\caption{\label{fig:FSM}
On the left,
transition matrix in graph form of an FSM that accepts decimal
numerals.
There is no arc from state  to state 
where .
The labels on the arcs are
{\tt nil} , where  is the empty word,
{\tt sign} 
and
{\tt digit} 
Here  is the word of length 1
containing the symbol .
On the right,
matrix version of Figure~\ref{fig:FSM}.
The rows and columns are identified by the states as labels.
As the row labeled by {\tt S} is by definition empty,
it is omitted.
Similarly, the column labeled by {\tt H} is omitted.
Empty cells are understood to contain .
}
\vspace{0.2cm}
\hrule
\end{figure}

According to Definition~\ref{def:FSMsynt}
a configuration  can occur only 
as the first configuration of a computation.
Similarly, a configuration  can occur only 
as the second of the last configuration of a computation.

The purpose of the computations of an FSM
is to define languages in the form of sets of words
over the alphabet .

An FSM is usually presented as a directed
graph with the states as nodes
and with language 
labeling the arc from state  to state .
We prefer the presentation in Figure~\ref{fig:FSM}.

\section{Dual-state machines}
\label{sec:DSM}
The operation of an FSM consists of two
kinds of changes: a change of state and an advance of the
input when a symbol is accepted.
Imagine giving the machine a data memory in the form
of a file that can be accessed only sequentially.
This memory can take the place of the input.
The accepting of a symbol becomes a change
in the state of the data in memory.
But the elements of ,
although not data, are also a kind of memory.
To avoid confusion between these different kinds of memory,
we rename elements of  to \emph{control states};
the states of data memory can then be called \emph{data states}.
A machine with these two kinds of memory
we call a \emph{dual-state machine},
a machine that is very like an FSM:
note the similarity between
Definitions~\ref{def:FSMsynt}
and \ref{def:DSMsyntax};
between
Definitions~\ref{def:FSMbeh}
and \ref{def:DSMbeh}.

The syntax of dual-state machines is defined as follows.
\begin{definition}\label{def:DSMsyntax}
The syntax of a \emph{dual-state machine} (DSM)
is given by a tuple 
where  is a nonempty finite set,
 is a nonempty set,
 is a function of type
,
, and
.
 consists of \emph{control states};
 consists of \emph{data states};
 is the \emph{transition matrix},
a matrix with rows and columns labeled by the elements of 
with binary relations over  as elements.
 is the \emph{start state};
 is the \emph{halt state}.
For all  it is the case that 
must satisfy 
and .
\end{definition}

\subsection{The computations of a DSM}
\label{sec:exec}

The semantics of DSMs is defined as follows.\\delta^n[k_0,k_{n-1}];\delta[k_{n-1},k_n]
\subseteq  \bigcup_{j \in K} \delta^n[k_0,j];\delta[j,k_n]
= (\delta^n;\delta)[k',k''] =  \delta^{n+1}[k',k'']
-0.9cm]
\begin{flushright}\end{flushright}

\paragraph{Example: Turing Machine as DSM}
We saw that an FSM is a DSM with a certain type of memory
defined by the admissible operations.
A Turing machine is a DSM with a different type of memory
defined by a different set of admissible operations.
Among the several variants of Turing machine
we choose the one where the memory takes the form of
a sequence of squares (a ``tape'') that is unbounded
in both directions.
Each square contains one symbol from the finite alphabet
.
In addition to the contents of the tape,
the state of the memory is determined by a pair 
where  is a square of the tape (the ``scanned'' square) and 
is a direction on the tape, being  (left),  (right),
or  (don't care).
The operations on the memory include reading,
a function with no argument having as value
the symbol on the scanned square and writing,
a function with a symbol as argument
causing the scanned square to contain that symbol.
Writing has an additional effect:
to ``move the tape'',
meaning that it causes the scanned square to become
the one on the left or on the right of the currently scanned square,
depending on whether  (``direction'') is  or .

The operation of a Turing machine is determined by a
set of rules, each in the form of quintuple .
The rule specifies that, if the state is 
and the scanned square contains ,
then  is written, the tape moves in the current
direction , and the state and direction become
 and , respectively.

For this example we selected a simple Turing machine
(\cite{mnsk67}, page 122).

The conventional presentation of the Turing machine
as set of quintuples
is in Figure~\ref{fig:TM}.
The matrix version is in Figure~\ref{fig:TMmtrx}.
The Matrix Code version is in Figure~\ref{fig:TurCode}.
\begin{figure}[htbp]
\begin{center}
\begin{minipage}{4.5in}
\begin{tabular}{l||l|l|l|l|l|l|l|l|l|l|l|l|}
 &&&&&&&&&&&&\\
\hline
 &&&&&&&&&&&&\\
\hline
 &&&&&&&&&&&& \\
\hline
 &&&&&&&&&&&&\\
\hline
 &&&&&&&&&&&&
\end{tabular}
\end{minipage}
\end{center}
\caption{\label{fig:TM}
A Turing machine.
The leftmost column shows the generic quintuple
.
The other twelve columns contain the actual twelve
quintuples that define the Turing machine.
The states are
, , , and .
The tape symbols are ), (, , , , and .
The dashes indicate that in state  the symbol `)'
is never encountered.
The 's in the last row stand for ``don't care''.
}
\end{figure}
As a first step for its simulation by a DSM
we rewrite the conventional Turing machine presentation
to matrix format, which we then find \emph{is} a DSM.
Subsequently we rewrite the code matrix to {\tt C} or {\tt C++}.

\vspace{3mm}

The Turing machine of Figure~\ref{fig:TM}
is designed to start operation with
a tape containing a sequence of parentheses bounded on either
side by the symbol .
Initially the scanned square is
the square containing the leftmost parenthesis,
that is, the square to the right of the leftmost .
When the machine halts,
all matching parentheses have been removed.
In this way one can tell whether the tape initially contained
a well-formed sequence.
Thus, for example,

is replaced by

indicating that the input sequence was unbalanced because
of an unmatched open parenthesis, whereas

is replaced by


The conventional presentation of Turing machines
as a set of quintuples hides their essence,
which is a matrix.
Just as FSMs centre around transitions from state to state,
so do Turing machines.
Whatever the nature of this transition,
its natural presentation is as an element of a matrix
of which the rows and columns are indexed by the states.
Figure~\ref{fig:FSM} gives this matrix for an FSM;
Figure~\ref{fig:TMmtrx} gives this matrix for the Turing
machine in Figure~\ref{fig:TM}.

To familiarize ourselves with the matrix format,
let us find in Figure~\ref{fig:TMmtrx}
the equivalent of the quintuple 
of Figure~\ref{fig:TM}.
In this quintuple we see that it specifies
a transition from  to .
The attributes of this transition are in column ,
row .
The machine makes this particular transition
if the scanned square contains `)'.
As a result of the transition an  is written on
the scanned square and the tape moves left.
Given that the transition is from  to ,
the further particulars can be given
in the form of a condition/action rule:

which is in the matrix cell of column , row .
Some transitions, for example the one from  to ,
contain more than one such rule,
one for each of the possible contents of the scanned square.
See Figure~\ref{fig:TMmtrx}.

\begin{flushright}\end{flushright}

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{4in}
\begin{tabular}{|c|c|c||c}
\lmnt{Q2}& \lmnt{Q1} & \nc{Q0} & \\
\hline \hline
\lmnt{( -> 0;d \\ A -> 1;d}
     &\lmnt{A -> 0;d} & \lmnt{}& \nc{H}  \\
\hline
& \lmnt{( -> X;R}  &\lmnt{( -> (;R \\ X -> X;R} & \nc{Q0}\\
\hline
& \lmnt{) -> );L \\ X -> X;L}
      &\lmnt{) -> X;L}  & \nc{Q1}\\
\hline
\lmnt{X -> X;L}
    & & \lmnt{A -> A;L} 
        & \nc{Q2}\\
\hline
\end{tabular}
\end{minipage}
\end{center}
\caption{\label{fig:TMmtrx}
The Turing machine of Figure~\ref{fig:TM} in matrix form;
the initial state is {\tt Q0}.
We propose this as a more readable alternative for the
standard set of quintuples.
}
\end{figure}

\vfill\eject
\section{Matrix Code}
\label{sec:MC}
The main application of DSMs is what we call ``Matrix Code''.
This is a DSM with components  where 
is defined by declarations in a conventional programming language,
say , and where the binary relations in the transition matrix
 are specified by  with reference to the declarations
for .

To use DSMs to best effect,  should be
equal to \ccc\ or \cpp\
in speed and compactness of compiled code.
Although the binary relations constituting 
are defined by pieces of code of ,
 itself is not a construct of  ---
after all,  is a \emph{matrix}.
Thus DSMs of this kind constitute a different programming language,
and it is this programming language that we call
\emph{Matrix Code}.
A specific  will be referred to as a \emph{code matrix}.

A code matrix is a hybrid object
composed of two programming languages:
Matrix Code and a conventional programming language .
The primitive binary relations of the matrix elements
are written in .
The way they are composed into composite matrix elements
as well as the matrix as a whole are written in Matrix Code.

As we shall show, Matrix Code has two advantages over conventional
languages:
its programs can be their own proof of partial correctness
and it supports
the parallel development of correctness proof and code.
At the same time, a code matrix can be written in such a way as to be
trivially translatable to .
One can say that suitably written code matrices
are ``almost executable''.
For the examples in this paper
we use the following translation method.

The control state is represented by a variable.
Each column is translated to a switch on this variable.
Each cell in the column is then translated
to one of the cases of the switch.
The matrix as a whole is translated
to the body of a {\tt void} function.
The data state of the code matrix becomes
the parameter(s) of the function.

The elements of a code matrix are binary relations
over the data states, as in all DSMs.
In the case of Matrix Code these binary relations are often
composed of primitive relations, which are of two kinds:
guards and statements.
Guards are boolean expressions;
semantically they are subsets of the identity relation.
That is, if  is a boolean expression,
then its meaning is
.

The primitive relations that are not guards
are statements of .
If  is a statement,
then its meaning is the set of pairs  such that
 is a possible state of termination of 
if  starts execution in data state .
Because guards and statements both denote
binary relations over ,
they are freely intercomposable:
\begin{verbatim}
                 guard ; guard
             statement ; guard
             statement ; statement
                 guard ; statement
\end{verbatim}
are all defined,
and are binary relations over .
As far as Matrix Code is concerned,
\verb"x-- ; x >= 0" and
\verb"x > 0 ; x-- " are equally valid expressions
for binary relations.
The latter form is preferred for reasons
of translatability to a conventional programming language.

\paragraph{Example: code matrix for computing prime numbers}
Consider a DSM with components 
with 
and  the set of tuples with as components
an integer {\tt N}, an array {\tt p} of length {\tt N},
and integers
{\tt j},
{\tt k}, and
{\tt n}.
 is the code matrix shown in Figure~\ref{fig:primes3}.
For example,  is the composition of three binary
relations:\
x \in c \leftrightarrow (x,x) \in I_c.

(k,d),
(k_1,d_1),
\ldots,
(k_{n-1},d_{n-1}),
(k',d').
\{V[k_{n-1}]\}M[k_{n-1},k']\{V[k']\}.
It follows that , which establishes
the theorem for the computation of length .

\section{Parallel development of proof and code}
\label{sec:sys}

Floyd's method is difficult to apply
because it is difficult to find the required conditions.
Because of this Dijkstra \cite{djk68a,djkInfotech71}
advocated parallel development of code and proof.
In this section we demonstrate parallel development
of a code matrix for the sample problem
solved in Figure~\ref{fig:floyd}:
to fill an array with the first  prime numbers
in increasing order.

\paragraph{Background on prime numbers}
Before we start,
let us review what we need to know about prime numbers.
The following list of facts is not intended as a
complete or nonredundant set of axioms;
they are a selection to guide us in the choice
of conditions and transitions.
\begin{enumerate}
\item
\emph{A prime is a positive integer that has no divisors.}
(We do not count 1 or the integer itself as divisors.
Moreover, 1 is not a prime.)
\item \label{axiom:infinity}
\emph{There are infinitely many primes},
so the problem can be solved for any .
\item \emph{2 and 3 are the first two primes}.
So a way to get started is to accept these as given
and place them in the beginning of the table.
This has the advantage
that we always have the situation
where the last prime in the table is odd
and the next odd number is the first candidate to be tested
for the next prime.
\item \label{axiom:suff}
\emph{If a number has a divisor,
then it has a prime divisor.}
This can be used to save effort:
we have to test only for divisibility by smaller primes,
and these are already in the table.
\item \label{axiom:limit}
\emph{If a number has a divisor,
then it has a prime divisor
less than or equal to its square root.}
This implies that we do not have to test
the candidate for the next prime for divisibility
by all primes already in the table.
\item \label{axiom:square}
\emph{The square of every prime is greater than the next prime.}
The significance of this fact will become apparent as we proceed.
\end{enumerate}

\paragraph{Deriving the code matrix}
The distinctive advantage of Matrix Code is
that a matrix can be expanded from the specification
in small steps using only the \emph{logic}
of the application without needing to attend
to the \emph{control} component of the algorithm.
Thus Matrix Code is an example of Kowalski's principle
``Algorithm = Logic + Control'' \cite{kwl79a}.

We assume that the specification exists
in the form of a precondition and a postcondition.
This gives rise to code matrix with one row and one column;
the one in Figure~\ref{fig:primes0}.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{|l||l}
\lmnt{S: p[0..N-1] exists \& N>1} & \\
\hline \hline
        \lmnt{/*which T?*/}
        & \lmnt{H: p[0..N-1] contains the first N primes}  \\
\hline

\end{tabular}
\end{center}
\caption{\label{fig:primes0}
There is only an empty transition  such that .
}
\end{figure}

The one element of this matrix is the transition \verb"T"
such that \bk{S}{T}{H} is true.
That is, \verb"T" has to be a simple combination
of guards and assignment statements that places
the \verb"N" first primes in \verb"p",
whatever \verb"N" is.
Absent such a \verb"T", we leave the matrix cell empty.
The resulting code matrix
satisfies \bk{S}{T}{H}, which makes it partially correct,
but \emph{very} partially so:
it has no successful computations.
Although Figure~\ref{fig:primes0} is the correct start
of the development process, it is not the last step.

As it is too ambitious to place all primes in the array
with a single transition,
a reasonable thing to try is to fill it with the first \verb"k"
primes and then try to add the next prime after \verb"p[k-1]".

We need a condition {\tt A} that is intermediate in the sense
that \bk{S}{T1}{A} and \bk{A}{T2}{H}
for simple {\tt T1} and {\tt T2}.
Such a condition is:
the first {\tt k} primes in increasing order are in
{\tt p[0..k-1]} with {\tt 1 < k <= N}.

Condition \verb"A" is promising because it is easy to think
of such a
\verb"T1"
and such a
\verb"T2".
The result is in Figure~\ref{fig:primes1}.

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{3in}
\begin{tabular}{|l|l||l}
\lmnt{A:} & \lmnt{S: p[0..N-1] exists \& N>1} & \\
\hline \hline
\lmnt{k >= N} & & \lmnt{H: p[0..N-1] contains the first N primes}  \\
\hline
& \lmnt{p[0] = 2; p[1] = 3; k = 2}
   & \lmnt{A: p[0..k-1] contains the first k primes \&
k <= N}  \\
\hline
\end{tabular}
\end{minipage}
\end{center}
\caption{\label{fig:primes1}
In column  the case {\tt k < N} is missing.
}
\end{figure}

This again is a partially correct code matrix.
It is a slight improvement in that it solves the problem
if  happens to be one or two.
In all other cases it leads to failed computations. 
The difficulty is that in column {\tt A}
we may have that {\tt k < N},
so that we cannot make the transition to {\tt H}.
We need to find the next prime after {\tt p[k-1]}.
Let {\tt j} be the current candidate for this next
prime.
That suggests for condition {\tt B:}
{\tt A} is true and {\tt j} is such that
there is no prime greater than {\tt p[k-1]}
and less than {\tt j}.

This is always true when {\tt j} is the next odd
number after {\tt p[k-1]}.
Another way of saying this is that {\tt j} is not
divisible by any of the primes in {\tt p[0..n]} with {\tt n}
set to 0.
We are interested more generally in
\begin{quote}
There are no primes between {\tt p[k-1]} and {\tt j} (with
{\tt j} is not divisible by any of the primes in {\tt p[0..n]})
and {\tt n<k}.
\end{quote}
We abbreviate this condition to \verb"relB(p,k,n,j)".

The largest prime factor of a number is less than the square root
of the number.
Hence, if we find that the square of {\tt p[n+1]} is greater
than {\tt j}, then we can conclude that {\tt j}
is the next prime after \verb"p[k-1]".
Hence, in the new column \verb"B", it is easy to detect
whether \verb"n" is large enough to conclude that \verb"j"
is the next prime after \verb"p[k-1]".
We place the corresponding transition in column \verb"B"
and we have Figure~\ref{fig:primes2}.

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{5in}
\begin{tabular}{|l|l|l||l}
\lmnt{B:} & \lmnt{A:} & \lmnt{S: p[0..N-1] exists \& N>1} & \\
\hline \hline
& \lmnt{k >= N} & & \lmnt{H: p[0..N-1] contains the first N primes}  \\
\hline
\lmnt{p[n]*p[n]>j; p[k++]=j} &
  & \lmnt{p[0] = 2; p[1] = 3; k = 2}
   & \lmnt{A: p[0..k-1] contains the first k primes \&
k <= N}  \\
\hline
        &\lmnt{k<N; j = p[k-1]+2; n=0}  &
           & \lmnt{B: A \& k<N \& relB(p,k,n,j)}                       \\
\hline
\end{tabular}
\end{minipage}
\end{center}
\caption{\label{fig:primes2}
In column  we have added a transition in column 
for the case that {\tt k < N}.
In that case we can start finding the next prime after
{\tt p[k-1]} because we know that there is enough space
in {\tt p} to store it.
{\tt relB(p,k,n,j)} means that
there is no prime between the last prime found and {\tt j}
and that {\tt n<k}, and that {\tt j} is not divided
by any prime in {\tt p[0..n]}.
}
\end{figure}

There are still failed computations.
(In fact, there is still no way to get beyond .)
The way ahead is clear:
a transition is missing in column \verb"B",
for the situation where \verb"n" is too small to
conclude that \verb"j" is the next prime.
That in itself produces condition \verb"C"
and, with it, a new row and column.

In column \verb"C" the missing information
is whether \verb"j",
the candidate for the next prime,
is divisible by \verb"p[n+1]".
If not, then \verb"n" can be incremented,
and condition \verb"B" is verified.
If so, then \verb"j" is not a prime
and the search for the next prime
must be restarted with \verb"j+2".
This determines a transition in column \verb"C"
that verifies condition \verb"C",
so is placed in that row.
See Figure~\ref{fig:primes3}.

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{5.5in}
\begin{tabular}{|l|l|l|l||l}
\lmnt{C:}& \lmnt{B:} & \lmnt{A:}
     & \lmnt{S: p[0..N-1] exists \& N>1} & \\
\hline \hline
& & \lmnt{k >= N} & & \lmnt{H: p[0..N-1] contains the first N primes}  \\
\hline
& \lmnt{p[n]*p[n]>j; p[k++]=j} &
  & \lmnt{p[0] = 2; p[1] = 3; k = 2}
   & \lmnt{A: p[0..k-1] contains the first k primes \&
k <= N}  \\
\hline
\lmnt{j\%p[n+1]!=0; n++} & &\lmnt{k<N; j = p[k-1]+2; n=0}  &
           & \lmnt{B: A \& k<N \& relB(p,k,n,j)}                       \\
\hline
\lmnt{j\%p[n+1]==0; j += 2; n=0}
    & \lmnt{p[n]*p[n]<= j} &
       & & \lmnt{C: B \& \\ p[n]*p[n] <= j}\\
\hline
\end{tabular}
\end{minipage}
\end{center}
\caption{\label{fig:primes3}
This figure is both a general example of a code matrix
and the final stage of the development
consisting of the sequence of Figures
\ref{fig:primes0},
\ref{fig:primes1}, and
\ref{fig:primes2}.
Change from Figure~\ref{fig:primes2}:
row and column with label  are added.
There are no incomplete columns.
This, as well as each of the previous versions is
partially correct,
as implied by the validity of
the verification condition for each
of the null matrix elements.
The absence of incomplete columns opens the possibility
of total correctness, but does not prove it.
}
\end{figure}

Up till now we detected with every additional
row and column that the new column lacked a transition.
Not this time: none of the columns has a missing transition.
The code matrix has no failed computations.
So it gives the correct answer by exiting in row \verb"H",
or it continues in an infinite computation.
As we have proved only partial correctness,
this latter alternative remains a possibility.

\paragraph{Termination}
For an infinite computation to arise,
there must be at least one condition
that is revisited an infinite number of times.
For each condition we give a reason why
it can be revisited only a finite number of times.

\begin{enumerate}
\item
Condition {\tt A.}
For this condition to be returned to,
{\tt k} has to have increased.
{\tt k} is never decreased and is bounded by {\tt N}.

\item
Condition {\tt B.}
For this condition to be returned to,
{\tt n} or {\tt j} has to have increased.
{\tt n} is bounded by the square root of 
{\tt p[N-1]}.
The number of times it is reset to zero is bounded by
{\tt p[N-1]}.
{\tt j} is never decreased and is bounded by {\tt p[N-1]}.
\item
Condition {\tt C.}
For this condition to be returned to,
{\tt n} has to have increased and is bounded as noted above.
\end{enumerate}

The transitions have been chosen
so that the corresponding revisiting condition
is satisfied.
As none of these conditions can be satisfied
an infinite number of times,
the code matrix has no infinite computation.

\paragraph{Running Matrix Code}
Running a code matrix in current practice
requires translation to a currently available language.
Our examples of Matrix Code have been constructed
for ease of translation to languages like Java or {\tt C}.
This entails a drastic reduction in expressivity.
Let us now demonstrate translation
using Figure~\ref{fig:primes3} as example.

As there is a similarity between the control states
and the states of a finite-state machine (FSM),
a good starting point for systematic translation
of a code matrix is the pattern according to which
an FSM is implemented.
This is usually done by introducing a constant for every
state and to let a variable, say, \verb"state"
assume these constants as values.
An infinite loop containing a \verb"switch" controlled by
\verb"state" then contains a \verb"case" statement
for every control state.


Each column of a code matrix translates
to a \verb"case" statement.
The order in which the translations of the columns
occur does not matter as long as \verb"state"
is initialized at \verb"S".
Here we have arbitrarily chosen alphabetic order.
In this way Figure~\ref{fig:primes3} translates to
the following.

\begin{figure}[htbp]
\hrule \vspace{0.1in}
\begin{center}
\begin{minipage}{3in}
{\footnotesize
\begin{verbatim}
void prTable(int p[], int N) {
  typedef enum{A,B,C,H,S} State;
  State state(S); // control state
  int j,k,n;      // part of data state
  while (true) {
    switch(state) {
      case A:
        if(k >= N) state = H;
        else {j = p[k-1]+2; n = 0; state = B;}
        break;
      case B: if (p[n]*p[n] > j) {
                p[k++] = j; state = A;
              } else state = C;
              break;
      case C:
        if (jelse {j += 2; n = 0; state = C;}
        break;
      case H: return;
      case S: p[0] = 2; p[1] = 3; k = 2; state = A;
    }
  }
}
\end{verbatim}
} \end{minipage}
\end{center}
\caption{\label{fig:primesCM}
Translation of the code matrix
in Figure~\ref{fig:primes3} to \cpp.
}
\vspace{0.1in}
\hrule
\end{figure}
A transition {\tt b0;S0} in column  and row  and
transition {\tt !b0;S1} in column  and row 
translate to
{\tt case X: if (b0) \{S0; state = R0;\}
else \{S1; state = R1\} break;} in the above code.

\section{Expressiveness of Matrix Code}
\label{sec:expressiveness}

The code obtained by translating a code matrix
is quite different from what one conventionally would write:
compare Figure~\ref{fig:floyd} with
Figure~\ref{fig:primesCM}.
In this example Matrix Code has the advantage of being
a verification and of being easy to discover.
But in the prime-number problem
Matrix Code does not lead to a more efficient program:
it has the same set of computations as the conventional one.

In this section we present an example where Matrix Code
makes it easy to discover an algorithm that is
more efficient than what is obtained via the conventional
programming style.
Consider the merging of two
monotonically nondecreasing input streams
into a single output stream.
We have available the following \cpp\ functions.

\begin{verbatim}
bool getL(int& x);  // output parameter x
bool getR(int& x);  // output parameter x
void putL();
void putR();
\end{verbatim}
where {\tt getL} ({\tt getR})
tests the left (right) input stream for emptiness.
In case of nonemptiness the output parameter
{\tt x} gets the value of the first element
of the stream.
Neither {\tt getL} nor {\tt getR} change
any of the streams.
This is done only by the functions
{\tt putL()} and
{\tt putR()}
which transfer the first element of a nonempty
left or right input stream to the output stream.

Figure~\ref{fig:eMerge}
is a typical program for this situation.
It typically acts in two stages.
In the first stage both input streams are nonempty.
In the second stage one of the input streams is empty
so that all that remains to be done
is to copy the other stream to the output.

\begin{figure}[htbp]
\hrule \vspace{0.1in}
\begin{center}
\begin{minipage}[t]{2.5in}
\begin{verbatim}
void eMerge() {
  int u,v;
  while (getL(u) && getR(v))
    if (u <= v) putL();
    else        putR();
  while (getL(u)) putL();
  while (getR(v)) putR();
}
\end{verbatim}
\end{minipage}
\end{center}
\caption{\label{fig:eMerge}
A structured program for merging two streams.
}
\vspace{0.1in}
\hrule
\end{figure}

This algorithm performs unnecessary tests:
in the first stage only one of the
input streams is changed, so that only that
one needs to be tested for emptiness;
here both are tested\footnote{
With the one exception when the left input
stream runs out at the same time as, or before,
the right input stream.
}.
It is superfluous tests like this
that allow the algorithm to be as simple as it is.

Of course it is unlikely that it is important
to save the kind of test just mentioned.
But there are many types of merging situations
and there may be some in which it does matter.
An advantage of Matrix Code is that 
it does not bias the programmer
towards including superfluous tests.

We proceed to develop a code matrix for merging.
The assertions need to indicate whether
it is known that an input stream is empty
and, if not, what its first element is.
If an input stream is possibly empty
then we represent it by ``{\tt ?}''.
We write ``{\tt e}'' if an input stream is empty.
Nonemptiness is indicated by writing ``\mpr{x}{?}'',
where {\tt x} is the first element.
We have to do this for each of the input streams;
we write e.g. the assertion \mst{\mpr{u}{?}}{\mpr{v}{?}}
to mean that both input streams are nonempty
and have first elements {\tt u} and {\tt v}, respectively.

We write all conditions in the form {\tt (left,right)},
where {\tt left} and {\tt right}
indicate the state of the input concerned,
in conjunction with the statement
that the result of appending the output
to the result of merging the remaining input streams
is equal to the result of merging the input streams
before the beginning of the execution of the program.
As this conjunct is part of every condition,
it need not be stated explicitly.
Of course its validity needs to be verified for every matrix entry.

With these conventions
we can state the program's specification
as obtaining a transition from
the state {\tt S}, which is {\tt (?,?)}
to
the state {\tt H}, which is {\tt (e,e)}.
Accordingly, the development starts with Figure~\ref{fig:mrg0}.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{|l||l}
          \lmnt{S:\mst{?}{?}} & \\
\hline \hline
        \lmnt{/*which T?*/}
        & \lmnt{H:\mst{e}{e}}  \\
\hline \\

\end{tabular}
\end{center}
\caption{\label{fig:mrg0}
Matrix Code corresponding to specification of the merging program.
But there is no {\tt T} such that \bk{S}{T}{H}.
The conditions in this figure,
as well as those in Figures
\ref{fig:mrg1}
and
\ref{fig:mrg2}
include the unstated
conjunct that the result of appending the output stream to
the merge of the input streams is equal to the merge of the input
streams in the initial state.
}
\end{figure}
As always with Matrix Code,
we start with the conditions.
Which do we need, in addition to the
\mst{?}{?}
and
\mst{e}{e}
given by the specification?
For each of the input streams
there are three states of information:

\begin{itemize}
\item
{\tt ?}
\item
{\tt e}
\item
\mpr{x}{?} for some first element {\tt x}
\end{itemize}
It is to be expected that the two input streams
can assume each of the three information states
independently, for a total of nine conditions. 

It is desirable that the initial condition \mst{?}{?} of 
minimal information does not arise during a computation
of the code matrix.
Under the assumption that we can avoid this
there will be only rows for the eight other conditions. 
By the time we will have populated the columns for these eight
conditions we will see whether this assumption was justified.

This problem is easy because the conditions are determined
by the nature of the problem.
For each condition there is an obvious and easy-to-realize
revisiting condition.
If there is at least one unknown input stream
at least one of them has to become known before revisiting.
If both input streams are known,
then at least one of them has to have its first element
transferred to output before revisiting.
See Figures~\ref{fig:mrg1} and \ref{fig:mrg2},
where the transitions have been chosen to conform
to the revisiting requirements.
As each column either has no guard or two complementary guards,
no additional rows are needed.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{|l|l||l}
\lmnt{{\tt A}}  &
          \lmnt{S:\mst{?}{?}} & \\
\hline \hline
     &      & \lmnt{H:\mst{e}{e}}  \\
\hline
     &\lmnt{{\tt getL(u)}}&\lmnt{{\tt A:(u:?,?)} }
\\
\hline
     &\lmnt{{\tt !getL(u)}}&\lmnt{{\tt B:(e,?)}}
\\
\hline
     \lmnt{{\tt getR(v)}}&&\lmnt{{\tt C:(u:?,v:?)}}
\\
\hline
     \lmnt{{\tt !getR(v)}}&&\lmnt{{\tt D:(u:?,e)}}
\\
\hline

\end{tabular}
\end{center}
\caption{\label{fig:mrg1}
See Figure~\ref{fig:mrg0}.
An input stream needs to be tested;
the left one is chosen arbitrarily.
This gives rise to new conditions.
Columns for these will cause addition of yet more conditions.
See Figure~\ref{fig:mrg2}.
}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{6.5in}
\begin{tabular}{|l|l|l|l|l|l|l|l||l}
\nc{G} & \nc{F} & \nc{E} & \nc{D} & \nc{C} &
\nc{B} & \nc{A} & \nc{S:(?,?)} & \\
\hline \hline
& \nc{!getL(u)} &  &  &  &
\nc{!getR(v)} &  &  & \nc{H:(e,e)} \\
\hline
& & & & \nc{u>v;\\ putR()} &
&  & \nc{getL(u)} & \nc{A:(u:?,?)}\\
\hline
\nc{putR()} &  &  &  & &
\nc{getR(v);\\putR()} & & \nc{!getL(u)} & \nc{B:(e,?)}\\
\hline
& & \nc{getL(u)} & & & & \nc{getR(v)} &  & \nc{C:(u:?,v:?)}\\
\hline
& \nc{getL(u)} & & &  & & \nc{!getR(v)} &  & \nc{D:(u:?,e)}\\
\hline
 &  &  &  & \nc{u <= v;\\ putL()} & &  &  & \nc{E:(?,v:?)}\\
\hline
 &  &  & \nc{putL()} &  & &  &  & \nc{F:(?,e)}\\
\hline
 &  & \nc{!getL(u)} &  &  & &  &  & \nc{G:(e,v:?)}\\
\hline
\end{tabular}
\end{minipage}
\end{center}
\caption{\label{fig:mrg2}
The complete code matrix for the merging problem,
continuing Figures \ref{fig:mrg0} and \ref{fig:mrg1}.
}
\end{figure}

The translation of the code matrix in
Figure~\ref{fig:mrg2}
is given in
Figure~\ref{fig:mMerge}.
As the order of the translations of the columns
is immaterial,
we have placed them in alphabetic order by label.

\begin{figure}[htbp]
\hrule \vspace{0.1in}
\begin{center}
\begin{minipage}{4in}
{\footnotesize
\begin{verbatim}
void mMerge(Trinity& tri) {
  int u,v;
  typedef enum{S,A,B,C,D,E,F,G,H} State;
  State state = S; // control state
  while(true) {
    switch(state) {
      case A: state = (tri.getR(v))?C:D; break;
      case B: if (tri.getR(v)) {tri.putR(); state = B;}
         else state = H; break;
      case C: if (u <= v) {tri.putL(); state = E;}
         else {tri.putR(); state = A;} break;
      case D: tri.putL(); state = F; break;
      case E: state = tri.getL(u)?C:G; break;
      case F: state = tri.getL(u)?D:H; break;
      case G: tri.putR(); state = B; break;
      case H: return;
      case S: state = tri.getL(u)?A:B; break;
    }
  }
}
\end{verbatim}
} \end{minipage}
\end{center}
\caption{\label{fig:mMerge}
A \cpp\ function for merging two streams
translated from Figure~\ref{fig:mrg2}.
{\tt tri} is an object of class {\tt Trinity}.
It contains three components: two input streams and an output stream. 
The admissible operations on these components are
{\tt getL(u)}, which is to determine the value
of the first element from the
left input stream (if there is one) and to make the argument
{\tt u} equal to it. The input stream is left unchanged.
{\tt putL()} removes the first element from the left input
stream and makes it the next element of the output stream.
Similarly for {\tt getR(v)} and {\tt putR()}
for the right input stream.
}
\vspace{0.1in}
\hrule
\end{figure}

The reason for developing a code matrix for the merge problem
was the desire to avoid the superfluous tests of a function
like the {\tt eMerge} listed in Figure~\ref{fig:eMerge}.
To see in how far {\tt mMerge} improves in this respect
we have run both functions on the same set of pairs of input
streams and counted the calls executed in both merge functions.

Such comparisons are of course dependent on the nature of the
input streams.
For example, the more equal in length the input streams are,
the more favourable for {\tt mMerge}.
Accordingly we have used a random-number generator
to determine the lengths of the input streams.
The input streams themselves are monotonically
increasing with random increments.

\begin{center}
\begin{tabular}{l||r|r|r|r}
    & getL & getR & putL & putR \\
\hline \hline
eMerge  & 1756 & 2691 & 871 & 1819 \\
\hline
mMerge  & 872 & 1821 & 871 & 1819 \\
\hline \hline
eMerge  & 1067 & 830 & 655 & 410 \\
\hline
mMerge  & 656 & 411 & 655 & 410 \\
\hline \hline
eMerge  & 3261 & 735 & 2894 & 365 \\
\hline
mMerge  & 2895 & 366 & 2894 & 365 \\
\hline \hline
eMerge  & 1355 & 1024 & 844 & 509 \\
\hline
mMerge  & 845 & 510 & 844 & 509 \\
\hline \hline
\end{tabular}
\end{center}

Each pair of successive lines gives the result of
running {\tt eMerge} and {\tt mMerge}
on the same pair of input streams.
The lengths of the streams are not listed separately,
as they are equal to the number of calls to {\tt putL}
and {\tt putR} shown in the table.

A merge function needs to make at least one call
to {\tt getL} ({\tt getR}) for every element of the left (right)
input stream.
It can be seen that {\tt mMerge} remains close to this minimum,
while {\tt eMerge} does not.

This example is notable
in that Matrix Code yields
an unfamiliar, test-optimal algorithm
by \emph{default}.
Structured programming tends to reduce
the number of control states.
Matrix Code lacks this bias:
in its use it is natural to introduce control states
as needed to serve as memory for test outcomes.




\section{Related work}

We organize related work in the form of seven ways
to discover Matrix Code:
flowcharts,
automata theory,
abstract state machines,
augmented transition networks,
logic programming,
tail-recursion optimization,
and
recursive program schemes.

\paragraph{Flowcharts}
The following comment has been made on Matrix Code:
``\emph{Although it reeks of flowcharts,
the proposal has some merit}.''
The comment has some merit:
flowcharts are indeed closely related to Matrix Code.
Flowcharts were widely used
as an informal programming notation
from the early 1950s to 1970.
Floyd \cite{fld67} showed
how assertions and verification conditions
can prove a flowchart partially correct.
Hoare \cite{hr69} introduced the notation of triples
for the verification conditions
and cast Floyd's method in the form of inference rules
for control structures such as \\
\verb"      while ... do ...    "
and 
\verb"      if ... then ... else ..."

Dijkstra observed that verifying assertions
are difficult to find for existing code,
so that an attempt at verification is a costly undertaking
with an uncertain outcome. 
He argued \cite{djk68a,djkInfotech71}
that code and correctness should be ``developed in parallel''.
The proposal seems to have found no response,
if only for the lack of specifics in the proposal.

Given the fact that Dijkstra's proposal was considered
unrealistically utopian, and still is,
it is interesting to read
what seems to be the first treatise \cite{gldNmn46}
on programming in the modern sense, published in 1946.
Here programs are expressed in the form of \emph{flow diagrams}.
At first sight one might think
that these are flowcharts under another name.
This is not the case:
flow diagrams consist of executable code
integrated with assertions, with the understanding
that a consistent flow diagram proves the correctness
of the computations performed by it.

The imperative part of a flow diagram was translated
to machine code
(this was before the appearance of assemblers).
I found no indication in \cite{gldNmn46} 
that it was even contemplated to split off
the imperative part of the flow diagram.
Thus we see that what was a vague proposal \cite{djk68a,djkInfotech71},
and regarded as unrealistically utopian in 1970,
was fully worked out in 1946
and may have become a practical reality in 1951
when the IAS machine became operational.

By the time flowcharts appeared,
the proof part of flow diagrams had been dropped.
And apparently forgotten,
for Floyd's discovery was published in 1967
and universally acknowledged as such.
Floyd's format is rather different,
and, in our opinion,
preferable to the flow diagrams of \cite{gldNmn46}.
Matrix Code can be regarded as a simplification
of Floyd's flowchart annotated with assertions,
a simplification made possible by the use of binary relations
that provide a common generalization of statements and tests.
Apt and Schaerf unify statements and tests
in their nondeterministic control structures \cite{ptSchrf97}.

\paragraph{Automata theory}
DSMs can be regarded as a realization
of Dana Scott's idea
\cite{sctt67} to put an end to the proliferation
of new variations of FSM by replacing them by programs
defined to run on suitably defined computers.
DSMs are very different from the programs proposed by
Scott. Scott's programs are unlike FSMs; DSMs closely
resemble FSMs. Paradoxically, DSMs, in the form
of Matrix Code, are of practical use;
Scott's programs are not.

\paragraph{Abstract State Machines}
DSMs can be obtained as a drastic simplification of
ASMs \cite{brgr03} where evolving algebras are
replaced by binary relations over data states and
formulas of logic are replaced by guards.
One might think that guards are a special case
of the formulas of the ASMs.
There is however a fundamental difference: regarded
as logic formulas, guards have free variables;
the formulas of ASMs do not.

\paragraph{Augmented Transition Networks}
In spite of Scott's plea \cite{sctt67}, variants of
FSM continued to appear.
Of special interest in this context are
\emph{labeled transition systems} which are used
to model and verify reactive systems \cite{brktn08}.
Here the set of states is often infinite
and there is typically no halt state.
Such systems are specified by rules of the
form 
to indicate the possibility of a transition from
state  to state  accompanied by action .
Mathematically the rules are viewed as a ternary relation
containing triples consisting of , , and .
This is of course unobjectionable,
but the alternative view of the rules as constituting
a matrix indexed by states,
containing in this instance  as element indexed
by  and  has the advantage of connecting
the theory to that of semilinear programming
in the sense of Parker \cite{prkr87}.
Another variant of FSM are the \emph{augmented transition networks}
used in linguistics \cite{wds70}.
The modification of flowcharts by means of binary relations
was introduced in \cite{vnmdn79}.
These can be viewed as augmented transition networks
with binary relations as labels on the transition arrows. 

\paragraph{Logic Programming}
The property that a code matrix is both a set of logical formulas
and an executable program is reminiscent of logic programming,
especially its aspect of separating logic from control
\cite{kwl79a}.
A special form of logic program corresponding to imperative
programs was investigated in \cite{cvn81}.

\paragraph{Recursive program schemes}
De Bakker and de Roever \cite{ddr73}
modeled programming constructs such as if-then-else and while-do.
For both guards and assignments they used binary relations
among what we call data states.

\paragraph{Tail-recursion optimization}
An attractive way of deriving efficient imperative code
is to use a recursive
definition of the function to be computed as starting point.
These can sometimes be transformed to a form
in which there is a single recursive call
and where this call occurs as the last statement
of the function. A further transformation replaces this call
by the more efficient {\tt goto} statement.
The result is similar to the result of translating
a code matrix to executable code.
The definition of the function can then be used to obtain
an assertion verifying the transformed program.
This is used in logic programming \cite{cvn81}.

\section{Conclusions}

In this paper we write programs as matrices with
binary relations as elements.
These matrices can be regarded as transformations
in a generalized vector space,
where vectors have assertions about data states as elements.
Computations of the programs are characterized by
powers of the matrix and verified assertions
show up as generalized eigenvectors of the matrix.
Such results may be dismissed as frivolous theorizing.
It seems to us that they are related to the following practical
benefits.

Our motivation was to address
the fact that imperative programming is in an unsatisfactory
state compared to functional and logic programming.
In the latter paradigms,
implementation is, or is close to, specification.
In imperative programming the relation between implementation
and specification is the verification problem,
a problem considered too hard for the practising programmer.
We proposed Matrix Code as an imperative programming language
where the same construct can be read as logical formula
and can serve as basis for a routine translation to Java,
\verb"C", or \verb"C++".

Matrix Code is only applicable to small algorithms.
Take it as a warning sign when it no longer fits on the back
of an envelope.
Yet it can play a useful role in large programs.
Even the largest software system is ultimately subdivided into
functions or methods.
Software engineering wisdom is unanimous in declaring
any function that is not small as a ``code smell''
and hence a candidate for refactoring.
Everyone of these many small functions
is a candidate for derivation by Matrix Code.

Experience so far suggests that it is possible
to develop algorithms incrementally
by small, obvious steps from the specification.
In this paper we go through such steps for
an algorithm to fill a table with prime numbers
using the method of trial division.
Whether or not this success is an exceptional case,
it seems certain that progress has been made in the direction
of the old dream according to which the production
of verified code is facilitated by developing
proof and code in parallel.  

\section*{Acknowledgments}
Thanks to Paul McJones and Mantis Cheng for their help.
I am grateful to the reviewers for PPDP 2012
and to the ones for \emph{Science of Computer Programming}
for their careful reading
and for their suggestions for improvement.
This research benefited from facilities provided
by the University of Victoria and by the Natural Science
and Engineering Research Council of Canada.

\begin{thebibliography}{10}

\bibitem{ptSchrf97}
K.R. Apt and A. Schaerf.
\newblock Search and imperative programming.
\newblock {\em POPL '97},
pages 67--79.

\bibitem{brktn08}
C. Baier and J.P. Katoen.
\newblock {\em Principles of Model Checking}.
\newblock MIT Press, 2008.

\bibitem{brgr03}
Egon B\"orger and Robert St\"ark.
\newblock {\em Abstract state machines:
   a method for high-level system design and analysis}.
\newblock Springer, 2003.

\bibitem{cvn81}
Keith L. Clark and M.H. van Emden.
\newblock Consequence Verification of Flowcharts.
\newblock {\em IEEE Transactions on Software Engineering},
SE-7:52--60, January 1981.

\bibitem{cnw71}
J.H. Conway.
\newblock {\em Regular Algebra and Finite Machines}.
\newblock Chapman and Hall, 1971.

\bibitem{ddr73}
J.W. de Bakker and W.P. de Roever.
\newblock A Calculus for Recursive Program Schemes.
\newblock {\em Automata, Languages, and Programming},
M. Nivat (ed.), 1973.

\bibitem{djk68a}
E.W. Dijkstra.
\newblock A constructive approach
          to the problem of program correctness.
\newblock {\em BIT}, 8:174--186, 1968.

\bibitem{djkInfotech71}
Edsger~W. Dijkstra.
\newblock Concern for correctness as a guiding principle for program
  composition.
\newblock In J.S.J. Hugo, editor, {\em The Fourth Generation}, pages 359--367.
  Infotech, Ltd, 1971.

\bibitem{djkddh72}
Edsger~W. Dijkstra.
\newblock Notes on structured programming.
\newblock In O.-J. Dahl, E.W. Dijkstra, and C.A.R. Hoare, editors, {\em
  Structured Programming}, pages 1--72. Academic Press, 1972.



\bibitem{fld67}
Robert~W. Floyd.
\newblock Assigning meanings to programs.
\newblock In J.T. Schwartz, editor, {\em Proceedings Symposium
  in Applied Mathematics}, pages 19--32.
  American Mathematical Society, 1967.

\bibitem{gldNmn46}
H.H. Goldstine and J. von Neumann.
\newblock Planning and coding of problems
for an electronic computing instrument. Part II, volume 1, 1946.
\newblock Reprinted in: \emph{John von Neumann: Collected Works},
Pages 80 -- 151, volume V.
\newblock A.H. Taub, editor. Pergamon Press, 1963. 

\bibitem{hr69}
C.A.R. Hoare.
\newblock An axiomatic basis for computer programming.
\newblock {\em Communications of the ACM}, 12(10):576--583, 1969.

\bibitem{kwl79a}
R.A. Kowalski.
\newblock Algorithm = Logic + Control. 
\newblock {\em Comm. ACM}, 22:424--436, 1979.



\bibitem{mnsk67}
Marvin L. Minsky.
\newblock \emph{Computation: Finite and Infinite Machines}.
\newblock Prentice Hall, 1967.

\bibitem{prkr87}
D.~Stott Parker.
\newblock Partial order programming.
\newblock Technical Report CSD-870067, Computer Science Department,
  University of California at Los Angeles, 1987.

\bibitem{prr90}
Dominique Perrin.
\newblock Finite Automata.
\newblock Jan van Leeuwen, editor,
{\em Handbook of Theoretical Computer Science, volume B}, pages 1--57.
  Elsevier, 1990.

\bibitem{sctt67}
Dana Scott.
\newblock Some definitional suggestions for automata theory.
\newblock {\em Journal of Computer and Systems Sciences},
1:187--212, 1967.

\bibitem{vnmdn79}
M.H. van Emden.
\newblock Programming with verification conditions.
\newblock {\em IEEE Transactions on Software Engineering},
vol. 3(1979), pp 148--159.

\bibitem{wds70}
W.A. Woods.
\newblock Transition network grammars.
\newblock {\em Comm. ACM}, 13:591--606, 1970.

\end{thebibliography}

\end{document}
