
In this document, we provide detailed implementation details, more quantitative results on semantic correspondence benchmarks, including SPair-71k~\cite{min2019spair}, PF-PASCAL~\cite{ham2017proposal}, and PF-WILLOW~\cite{ham2016proposal}, and more qualitative results on all the benchmarks we used.

\section*{Appendix A. Implementation details}
\paragraph{Implementation Details.}
We first extract backbone features using ResNet50~\cite{he2016deep} and ResNet101~\cite{he2016deep} from conv3x, conv4x and conv5x. This feature extracting scheme results in 3 pyramidal layers. To compose VEM, we stack a series of  convolutions followed by ReLU and Group Normalization. The coarsest level correaltion map has a size of  after undergoing VEM. Finer levels,  and , have volume embeddings of size  and , respectively. We set the number of heads in VTM as 4. At  level, we set the depth of VTM as 4 and rest are set to 2. Window size  is set to 4 at all levels of VTM. We project the feature maps of query at last layer of level  and  to have channels 64, 32 and 16 from 1024, 512 and 256, respectively. We implemented our work with PyTorch~\cite{paszke2017automatic}. We used learning rate of 5e-4. We used augmentation introduced in~\cite{buslaev2020albumentations} for training as shown in Table~\ref{tab:augmentation}. We adopted early-stopping for all the training, which we picked the model with best validation score. We set the batch size as 16.
\vspace{-10pt}

\paragraph{Components of VAT.}
For the ablation study of components of VAT, we defined the baseline model which substitutes VEM with 4D max-pooling. As VTM is a transformer module that takes an input and outputs the same shape to input, it can simply be skipped. Residual connections are simply removed to test its efficiency, and appearance affinity is excluded at the decoder. This experiment is conducted in a sequential manner, where each component is added progressively. 
\vspace{-10pt}

\paragraph{Different Cost Aggregators.}
For the ablation study of cost aggregators, we used codes released by authors of works, including standard transformer~\cite{vaswani2017attention}, center-pivot 4D convolutions~\cite{min2021hypercorrelation}, linear transformer~\cite{katharopoulos2020transformers}, and fastformer~\cite{wu2021fastformer}. We simply replaced VTM with those in order to make a fair comparison.\vspace{-10pt}

\paragraph{Different backbone networks.} Ablation study on different feature backbone follows similar procedure, as PVT~\cite{wang2021pyramid} and swin transformer~\cite{liu2021swin} have pyramidal architecture that outputs features of size and dimensions similar to ResNet~\cite{he2016deep}. 
\vspace{-10pt}

\begin{table*}[]
    \begin{center}
        \scalebox{0.75}{
        \begin{tabular}{l|cccccccccccccccccc|c}
        \toprule
         Methods & aero. & bike & bird & boat & bott. & bus & car & cat & chai. & cow & dog & hors. & mbik. & pers. & plan. & shee. & trai. & tv & all\\
        \midrule
        
       
        CNNGeo~\cite{rocco2017convolutional} &  23.4 & 16.7 & 40.2 & 14.3 & 36.4 & 27.7 & 26.0 & 32.7 & 12.7 & 27.4 & 22.8 & 13.7 & 20.9 & 21.0 & 17.5 & 10.2 & 30.8 & 34.1 & 20.6  \\
      
       WeakAlign~\cite{rocco2018end} &  22.2 & 17.6 & 41.9 & {15.1} & {38.1} & {27.4} & {27.2} & 31.8 & 12.8 & 26.8 & 22.6 & 14.2 & 20.0 & 22.2 & 17.9 & 10.4 & {32.2} & 35.1 & 20.9 \\
    
       NC-Net~\cite{Rocco18b} & 17.9 & 12.2 & 32.1 & 11.7 & 29.0 & 19.9 & 16.1 & 39.2 & 9.9 & 23.9 & 18.8 & 15.7 & 17.4 & 15.9 & 14.8 & 9.6 & 24.2 & 31.1 & 20.1   \\\-2.3ex]
        
        {SCOT}~\cite{liu2020semantic} & {34.9} & {20.7} & {63.8} & {21.1} & {43.5} & {27.3} & {21.3} & {63.1} & {20.0} & {42.9} & {42.5} & {31.1} & {29.8} & {35.0} & {27.7} & {24.4} & {48.4} & {40.8} & {35.6} \\
       
       {DHPF}~\cite{min2020learning} & {38.4} & {23.8} & {68.3} & {18.9} & {42.6} & {27.9} & {20.1} & {61.6} & {22.0} & {46.9} & {46.1} & {33.5} & {27.6} & {40.1} & {27.6} & {28.1} & {49.5} & {46.5} & {37.3} \\
        
        
        {CHM}~\cite{min2021convolutional} & {49.6} & {29.3} & {68.7} & {29.7} & {45.3} & {48.4} & {39.5} & {64.9} & {20.3} & {60.5} & {56.1} & {46.0} & {33.8} & {44.3} & {38.9} & {31.4} & {72.2} & {55.5} & {46.3}  \\
        MMNet~\cite{zhao2021multi}  &\underline{55.9}  &\underline{37.0}  &65.0  &35.4 &\underline{50.0}  &\textbf{63.9}  &\textbf{45.7} &62.8  &\textbf{28.7}  &65.0  &54.7  &51.6  &38.5  &34.6  &41.7  &\underline{36.3}  &\textbf{77.7}  &\underline{62.5}  &\underline{50.4}  \\
        PMNC~\cite{Lee_2021_CVPR}  &54.1  &35.9  &\textbf{74.9}  &\underline{36.5} &42.1  &48.8  &40.0 &\textbf{72.6}  &21.1  &\underline{67.6}  &\underline{58.1}  &50.5  &40.1  &\textbf{54.1}  &\underline{43.3}  &35.7  &\underline{74.5}  &59.9  &\underline{50.4}  \\
        CATs~\cite{cho2021semantic}  & {52.0} & {34.7} & {72.2} & {34.3} & {49.9} & {57.5} & \underline{43.6} & {66.5} & \underline{24.4} & {63.2} & {56.5} & \underline{52.0} & \underline{42.6} & {41.7} & {43.0} & {33.6} & {72.6} & {58.0} & {49.9} \\\midrule
        VAT (ours)  &\textbf{56.5}& \textbf{37.8}& \underline{73.0}& \textbf{38.7} &\textbf{50.9} &\underline{58.2}  &40.8  &\underline{70.5} &20.4  &\textbf{72.6}  &\textbf{61.1}  &\textbf{57.8}  &\textbf{45.6}  &\underline{48.1}  &\textbf{52.4}  &\textbf{39.7}  &\textbf{77.7}  &\textbf{71.4}  &\textbf{54.2}  \\
       \bottomrule
        \end{tabular}} 
    \caption{\label{tab:spair} \textbf{Per-class quantitative evaluation on SPair-71k~\cite{min2019spair} benchmark.}}\vspace{-10pt}
    
    \end{center}\vspace{-10pt}
\end{table*}
\begin{table}[]
    
    
    \begin{center}
    
    \scalebox{0.8}{
    \begin{tabular}{l|cc|cc}
            \toprule
             \multirow{3}{*}{Methods}& \multicolumn{2}{c|}{PF-PASCAL~\cite{ham2017proposal}} & \multicolumn{2}{c}{PF-WILLOW~\cite{ham2016proposal}}  \\
          
              & \multicolumn{2}{c|}{PCK @ }  & \multicolumn{2}{c}{PCK @ }   \\ 
        
             & 0.1 & 0.15  & 0.1 & 0.15  \\ 
             \midrule
        
             CNNGeo~\cite{rocco2017convolutional}  &69.5 &80.4 &69.2 &77.8\\
             
              WeakAlign~\cite{rocco2018end}  &74.8 &84.0 &70.2 &79.9\\
              
              SFNet~\cite{lee2019sfnet} &81.9 &90.6  &74.0 &84.2\\
            
         
             NC-Net~\cite{Rocco18b} &78.9 &86.0  &67.0 &83.7\\


             DCC-Net~\cite{huang2019dynamic}  &82.3 &90.5  &73.8 &86.5\\
             
             HPF~\cite{min2019hyperpixel}  &84.8 &92.7 &74.4 &85.6\\
             GSF~\cite{jeon2020guided} &87.8 &95.9  &78.7 &90.2\\
           
             DHPF~\cite{min2020learning}   &90.7 &95.0  &77.6 &89.1 \\
             SCOT~\cite{liu2020semantic}   &85.4 &92.7 &76.0 &87.1\\
             CHM~\cite{min2021convolutional}   &{91.6} &94.9  &\underline{79.4} &87.5\\
             MMNet~\cite{zhao2021multi}  &{91.6} &95.9 &-&-\\
             PMNC~\cite{Lee_2021_CVPR}  &90.6 &- &-&-\\
             CATs~\cite{cho2021semantic}  &\textbf{92.6} &\textbf{96.4} &{79.2} &\underline{90.3}\\\midrule
             VAT (ours) &\underline{92.3} &\underline{96.1} &\textbf{81.0}&\textbf{91.4}\\\bottomrule
        
        
    \end{tabular}
    }\caption{\textbf{Quantitative evaluation on standard benchmarks~\cite{min2019spair,ham2016proposal,ham2017proposal}.} 
    }\label{tab:pascal}\vspace{-5pt}
    \end{center}
\end{table}
\begin{table}[]
    \centering
        \scalebox{0.75}{
        \begin{tabular}{l|cc}
    \toprule
    \multirow{2}{*}{Different aggregators}& Memory &Run-time\\
    &(GB) &(ms)\\\midrule
    
       Standard transformer~\cite{vaswani2017attention}&OOM&N/A \\
Center-pivot 4D convolutions~\cite{min2021hypercorrelation}& \textbf{3.5}
       &\textbf{52.7}\\
         Linear transformer~\cite{katharopoulos2020transformers}& \textbf{3.5}&\underline{56.8} \\
         Fastformer~\cite{wu2021fastformer}&\textbf{3.5} &122.9 \\
         \hline
         Volumetric transformer (ours) &\underline{3.8}&57.3 \\
         \bottomrule
    \end{tabular}
        }
    \caption{\textbf{Memory and Run-time comparison between different aggregators.} The memory and run-time are measured by simply replacing the aggregator module within VAT.}
    \label{tab:memory}
\end{table}

\begin{table}[]
    \centering
        \scalebox{0.75}{
        \begin{tabular}{l|cc}
    \toprule
    \multirow{2}{*}{}& Memory &Run-time\\
    &(GB) &(ms)\\\midrule
    
       HSNet~\cite{min2021hypercorrelation}&\textbf{2.2}&\textbf{51.7} \\
         \hline
         VAT (ours) &\underline{3.8}&\underline{57.3}\\
         \bottomrule
    \end{tabular}
        }
    \caption{\textbf{Memory and Run-time comparison to HSNet~\cite{min2021hypercorrelation}.}}
    \label{tab:overallmemory}
\end{table}
\paragraph{VAT in Semantic Correspondence.}

For the experiments to validate effectiveness of VAT on semantic correspondence, we used the three most widely used datasets in this task, which include SPair-71k~\cite{min2019spair}, PF-PASCAL~\cite{ham2017proposal} and PF-WILLOW~\cite{ham2016proposal}. We made minor modifications to our model: we excluded bilinear upsampling in the affinity-aware decoder, which the original output shape is  but with the modification, now the output shape is  as two x2 bilinear upsampling is excluded. We used embedded features from conv4 and conv5x, which is different from the training setting for the experiments on few-shot segmentation datasets. Note that we do not utilize segmentation mask for this task. We only use keypoints provided in the datasets for training similarly to~\cite{cho2021semantic}. We used the same data augmentation scheme~\cite{cho2021semantic, buslaev2020albumentations} as Table~\ref{tab:augmentation}. We used learning rate of 3e-6 for the feature backbone and 3e-4 for VAT. We used batch size of 8. For the evaluation, we followed the standard protocol~\cite{min2019hyperpixel,min2020learning,liu2020semantic,jeon2020guided,cho2021semantic,min2021convolutional}.
\begin{table}[]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{cl|c}
       \toprule
        &Augmentation type &Probability \\
        \midrule
        \textbf{(I)} &ToGray  &0.2 \\
        \textbf{(II)} &Posterize &0.2 \\
        \textbf{(III)} &Equalize &0.2 \\
        \textbf{(IV)} &Sharpen &0.2 \\
        \textbf{(V)} &RandomBrightnessContrast &0.2 \\
        \textbf{(VI)} &Solarize &0.2 \\
        \textbf{(VII)} &ColorJitter &0.2 \\\bottomrule
        
 
\end{tabular}}\caption{\textbf{Augmentation type}}
    \label{tab:augmentation}
\end{table}


\section*{Appendix B. Failure Cases}
Overall, we observe that our method may lack an ability to preserve fine-details and address multi-correspondence as shown in Figure~\ref{Failure}. Given an object in the support image and multiple corresponding objects in query image, VAT sometimes seem to fail finding correspondence. From these, perhaps, it could be argued that the proposed method is better at encoding matching information and effectively aggregating them than others, but it might lack an ability to better find multi-objects or preserve the fine-details. Better means to consider both factors either by introducing some confidence module~\cite{truong2021learning} or preserve fine-details when increasing the resolutions of the predicted mask during the process in decoder can be a promising direction future work. 

\section*{Appendix C. Memory and Run-time}
We additionally provide memory and run-time comparison to other aggregators and HSNet~\cite{min2021hypercorrelation}. The results are obtained using a single NVIDIA GeForce RTX 3090 GPU and Intel Core i7-10700 CPU. As shown in Table~\ref{tab:memory}, we observe that VAT is relatively slower and comsumes more memory than other aggregators. About 0.3 GB of more memory consumption and 5 ms slower run-time occur  with the proposed VAT in return for the performance. From the perspective of aggregators, 0.3 GB and 5 ms gap seem quite trivial to us.

We also provide direct comparison to the current state-of-the-art method, HSNet~\cite{min2021hypercorrelation}, in Table~\ref{tab:overallmemory}. We observe that memory consumption gap is quite large, which is an apparent limitation to the proposed method. Run-time gap is quite trivial, enabling real-time inference. Note that standard transformer can not be used due to OOM. In light of this, although  we managed to propose a method to aggregate a raw correlation map without arbitrarily changing its resolutions prior to feeding into networks, we observe relatively large memory consumption gap to HSNet~\cite{min2021hypercorrelation}, which suggests a promising direction for future work, {\it i.e.,} even more efficient method while enabling to process at higher spatial resolutions. 



\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{latex/fig_supp/failure_final.pdf}
\caption{\textbf{Failure cases.}   }
\label{Failure}
\end{figure}



\section*{Appendix D. More Results}
For few-shot segmentation, VAT clearly sets new state-of-the-art for all the benchmarks, demonstrating its effectiveness. Also as VAT outperforms other SOTA methods by large margin for semantic correspondence, we argue that the cost aggregation is indeed a prime importance, even for few-shot segmentation as well. We argue that this performance boost in few-shot segmentation can be attributed to VAT's ability to find accurate semantic correspondences when tackling the segmentation task. In light of this, we are suggesting a paradigm shift for the few-shot segmentation task.
\vspace{-10pt}
\paragraph{Quantitative Results on Semantic Correspondence.}
As shown in Table~\ref{tab:spair} and Table~\ref{tab:pascal}, we provide more quantitative results on  SPair-71k~\cite{min2019spair}, PF-PASCAL~\cite{ham2017proposal}, and PF-WILLOW~\cite{ham2016proposal} in comparison to other semantic correspondence methods, including CNNGeo~\cite{rocco2017convolutional}, WeakAlign~\cite{rocco2018end}, NC-Net~\cite{Rocco18b}, HPF~\cite{min2019hyperpixel}, SFNet~\cite{lee2019sfnet}, DCC-Net~\cite{huang2019dynamic}, GSF~\cite{jeon2020guided}, SCOT~\cite{liu2020semantic}, DHPF~\cite{min2020learning}, CHM~\cite{min2021convolutional}, MMNet~\cite{zhao2021multi}, PMNC~\cite{Lee_2021_CVPR} and CATs~\cite{cho2021semantic}. \vspace{-10pt}

\paragraph{Qualitative Results.}
As shown in Figure~\ref{PASCAL}, Figure~\ref{COCO}, Figure~\ref{FSS}, Figure~\ref{WILLOW} and Figure~\ref{SPAIR}, we provide qualitative results on all the benchmarks, which includes PASCAL-5~\cite{shaban2017one}, COCO-20~\cite{lin2014microsoft}, FSS-1000~\cite{li2020fss}, PF-PASCAL~\cite{ham2017proposal}, PF-WILLOW~\cite{ham2016proposal} and SPair-71k~\cite{min2019spair}.
\newpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig_supp/pascal_final.pdf}
\caption{\textbf{Qualitative results on PASCAL-5~\cite{shaban2017one}.}   }
\label{PASCAL}
\end{figure*}
\newpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig_supp/coco_final.pdf}
\caption{\textbf{Qualitative results on COCO-20~\cite{lin2014microsoft}.}   }
\label{COCO}
\end{figure*}
\newpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig_supp/fss_final.pdf}
\caption{\textbf{Qualitative results on FSS-1000~\cite{li2020fss}.}   }
\label{FSS}
\end{figure*}
\newpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.99\textwidth]{latex/fig_supp/pf_final.pdf}
\caption{\textbf{Qualitative results on PF-PASCAL~\cite{ham2017proposal} (left) and PF-WILLOW~\cite{ham2016proposal} (right). }   }
\label{WILLOW}
\end{figure*}
\newpage
\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{latex/fig_supp/spair_final.pdf}
\caption{\textbf{Qualitative results on SPair-71k~\cite{min2019spair}.}   }
\label{SPAIR}
\end{figure*}
\clearpage
\newpage
