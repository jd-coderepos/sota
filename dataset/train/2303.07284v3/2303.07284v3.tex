\appendix
\section*{Appendix}


Sec.~\ref{sec:bliss} elaborates more details about the collection process of BLiSS dataset.
Sec.~\ref{sec:experiment_details} provides more dataset-specific implementation details and hyper-parameters for training and testing.
We also present more qualitative results in Sec.~\ref{sec:qualitative_more}.
Finally, we discuss the limitation and some future work of our paper in Sec.~\ref{sec:limitation}.


\section{BLiSS Dataset}
\label{sec:bliss}

\noindent\textbf{Data Collection}

We collected 628 livestream videos from behance.net, including the corresponding English transcripts and meta data.
Audio tracks are also available in the videos; however, we don't use them in our study since most information from audio modality can be captured by transcripts. Meta data from the website are annotated by creators which include title, overall text description, creative fields, creative tools, streamer, cover image and animation.

The transcripts are first segmented by sentence, each with corresponding time stamps. We remove transcripts with very short duration which are likely caused by broken words and speech recognition failures. Based on the transcript segments, we further divide each video into 5-minute long clips, so that each clip has its corresponding frames and aligned transcript sentences.

Annotators were instructed to watch the entire clip, read the transcript sentences, and select keywords from the sentences representing the important content in the clip. Each clip has about 5 to 10 keywords. The sentences containing keywords are regarded as key sentences for extractive text summarization. The annotators were also asked to write a summary of the whole clip in their own words, which can be used for abstractive text summarization.

For each video, we extract all the frames from its thumbnail animation. 
For each frame $g$ in the thumbnail, we select the most similar frame $f$ in the video as the key-frame.


\vspace{0.05in} 
\noindent\textbf{Corpus Statistics of BLiSS Dataset}
In Table~\ref{tab:statistic}, we compare the statistics of our collected BLiSS dataset with other datasets including standard video summarization datasets (SumMe~\cite{gygli2014creating} and TVSum~\cite{song2015tvsum}), multimodal datasets (CNN~\cite{fu2021mm} and Daily Mail~\cite{fu2021mm}) and the transcript summarization dataset (~\cite{cho2021streamhover}).
We can see that our BLiSS dataset has a much larger scale than all the other datasets. 
Specifically, the BLiSS dataset has 1,109 hours of total video duration. 
The total number of text tokens of the BLiSS dataset is 5.5M, much larger than the Daily Mail and StreamHover datasets.

\begin{table*}[t]
\centering
\caption{Statistics comparison of BLiSS dataset with other datasets.}
\resizebox{0.85\linewidth}{!}{
\renewcommand{\arraystretch}{1.2}
    \begin{tabular*}{0.85\linewidth}{@{\extracolsep{\fill}\;}l|cccccc} 
    \toprule
     & SumMe & TVSum & CNN & Daily Mail & StreamHover & BLiSS  \\
    \midrule
    Number of Data &  25 & 50 & 203 & 1970 & 5421 & 13303 \\
    Total Video Duration (Hours) & 1.0 & 3.5 & 7.1 & 44.2 & 452 & 1109 \\
    Total Number of Text Tokens & -- & -- & 0.2M & 1.3M & 3.1M & 5.5M \\
    Avg. Video Summary Length & 44 & 70 & -- & 2.9 & -- & 10.1 \\
    Avg. Text Summary Length & -- & -- & 29.7 & 59.6 & 79 & 49 \\
    \bottomrule
    \end{tabular*}
}
\label{tab:statistic}
\end{table*}



\vspace{0.05in} 
\noindent\textbf{Example}
We show one example of the annotated sample in the BLiSS dataset in Figure~\ref{fig:example_bliss}. We visualize the uniformly sampled video frames, annotated keyframes, sentence-level transcripts, and the abstractive text summary. Note that the extractive text summary is formed by the key sentences, where the ground-truth keywords in the key sentences are marked in blue color.
\begin{figure*}[t]
\vspace{-0.1in}
\centering
    \adjincludegraphics[width=\linewidth, trim={{0.03\width} {0.15\height} {0.03\width} {0.15\height}},clip]{fig/behance_example.pdf}
    \vspace{-0.25in}
    \caption{Example of one data sample from the BLiSS dataset. Here, we visualize the uniformly sampled video frames, annotated keyframes, sentence-level transcript, and abstractive text summary. Note that the extractive text summary is formed by the key sentences, where the ground-truth keywords are marked with \textcolor{-red!70!green}{blue} color. Best viewed in color.}
\vspace{-0.15in}
\label{fig:example_bliss}
\end{figure*}



\section{Experiment Details}
\vspace{-0.05in}
\label{sec:experiment_details}
On multimodal summarization datasets (Daily Mail and CNN), we train our \system with a batch size of 4, a learning rate of 2e-4, weight decay of 1e-7 and 1e-5, training epochs of 100,  $L$ = 2, the ratio controlling hard-negative samples $r=8$, the balancing weights for dual contrastive losses $\beta$ of 0.001 and 0, $\lambda$ of 0.001 and 0 for the Daily Mail and CNN datasets, respectively. 

On standard video summarization datasets (SumMe and TVSum), we train our \system with a batch size of 4, a learning rate of 1e-3, weight decay of 1e-3 and 1e-5, training epochs of 300, number of transformer layers $L$ = 2, the ratio controlling hard-negative samples $r=16$ the balancing weights for dual contrastive losses $\beta$ of 0.1, $\lambda$ of 3 and 1 for the SumMe and TVSum datasets, respectively.

On the BLiSS dataset, we set a batch size of 64, a learning rate of 1e-3, weight decay of 1e-7, training epochs of 50, transformer layers $L$ of 6, the ratio controlling hard-negative samples $r=4$, the balancing weights for dual contrastive losses $\beta$ of 0.01 and $\lambda$ of 0.001.

We set the expansion size for both sides of key-frames and key-sentences in the contrastive pair selection procedure as 4 on all the datasets.


\begin{figure*}[t]
\vspace{-0.1in}
\centering
    \adjincludegraphics[width=\linewidth, trim={{0.03\width} {0.12\height} {0.065\width} {0.07\height}},clip]{fig/behance_visualization.pdf}
    \caption{Visualization of multimodal summarization results for the BLiSS dataset. The ground-truth text summary, predictions from the baseline model and our \system are shown for each video. ``Baseline'' denotes our \system without the proposed alignment module and dual contrastive losses. The ground-truth keywords from key sentences are marked with \textcolor{-red!70!green}{blue} color. We also show the corresponding video frames for each transcribed sentence where the frames with \textcolor{red}{red} boxes represent some of the predicted key-frames from our \system. The title for each video clip is the annotated abstractive summary. }
\vspace{-0.15in}
\label{fig:visualization_behance}
\end{figure*}

\section{More Qualitative Results}
\label{sec:qualitative_more}
\vspace{-0.05in}

In Figure~\ref{fig:visualization_behance}, we show three different examples of multimodal summarization results on the BLiSS dataset.
We can see that, compared to the baseline method, our \system can predict the key sentences more accurately and faithfully for the extractive text summarization task. 
It proves the effectiveness of the proposed alignment module and dual contrastive losses for the text modality.
For the video summarization task, because livestream videos change slowly over time, their video frames generally share similar visual content. However, our predicted key-frames can still capture the important scenes from the input video qualitatively.

\section{Limitation and Future Work}
\label{sec:limitation}
\vspace{-0.05in}

The main limitation is that our \system is based on the Transformer~\cite{vaswani2017attention} architecture with the self-attention operation, which suffers from heavy computation cost due to the quadratic computation complexity with respect to the input sequence length.
Although there are a series of works~\cite{beltagy2020longformer,kitaev2020reformer,correia2019adaptively,vyas2020fast} trying to design computation efficient transformer models to handle long sequences, it is out of the scope of our paper and we still follow the basic transformer design.
In addition, the data annotation process for the video and text summaries is laborious. More research on unsupervised or self-supervised multimodal summarization tasks would be a good direction for future work.


