\documentclass[10pt,twocolumn,letterpaper]{article}



\usepackage{amsmath,amsfonts,bm,amssymb,xspace}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{(a)\xspace}}
\newcommand{\captionb}{{(b)\xspace}}
\newcommand{\captionc}{{(c)\xspace}}
\newcommand{\captiond}{{(d)\xspace}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\valpha{{\bm \alpha}}
\def\vbeta{{\bm \beta}}
\def\vgamma{{\bm \gamma}}
\def\vdelta{{\bm \delta}}
\def\vepsilon{{\bm \epsilon}}
\def\vzeta{{\bm \zeta}}
\def\veta{{\bm \eta}}
\def\vtheta{{\bm \theta}}
\def\viota{{\bm \iota}}
\def\vkappa{{\bm \kappa}}
\def\vlambda{{\bm \lambda}}
\def\vmu{{\bm \mu}}
\def\vnu{{\bm \nu}}
\def\vxi{{\bm \xi}}
\def\vomicron{{\bm \omicron}}
\def\vpi{{\bm \pi}}
\def\vrho{{\bm \rho}}
\def\vsigma{{\bm \sigma}}
\def\vtau{{\bm \tau}}
\def\vupsilon{{\bm \upsilon}}
\def\vphi{{\bm \phi}}
\def\vchi{{\bm \chi}}
\def\vpsi{{\bm \psi}}
\def\vomega{{\bm \omega}}

\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\KL}{D_{\mathrm{KL}}}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\newcommand{\Tabref}[1]{Table \ref{#1}}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\imagesize}[2]{}
\newif\ifdraft
\ifdraft
    \usepackage[colorinlistoftodos]{todonotes}
    \newcommand{\todoline}[1]{\todo[inline]{#1}}
\else
    \newcommand{\todoline}[1]{}
\fi
\DeclareMathOperator{\diag}{diag}
 

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{xspace,multirow}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\algorithmicdo{:}


\newcommand{\autoaugment}{AutoAugment\xspace}
\newcommand{\faster}{Faster \autoaugment}
\newcommand{\fast}{Fast \autoaugment}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{\faster: Learning Augmentation Strategies using Backpropagation}

\author{Ryuichiro Hataya \and Jan Zdenek \and Kazuki Yoshizoe \and Hideki Nakayama \\
 Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan\\
 RIKEN Center for Advanced Intelligence Project, Tokyo, Japan\\
}

\maketitle


\begin{abstract}

Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper,  we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, \faster, achieves significantly faster searching than prior work without a performance drop.

\end{abstract}

\section{Introduction}\label{sec:intorduction}

Data augmentation is a powerful technique for machine learning to virtually increase the amount and diversity of data, which improves the performance especially in image recognition tasks. Conventional data augmentation methods include geometric transformations such as rotation and color enhancing such as auto-contrast. 
Similarly to other hyper-parameters, the designers of data augmentation strategies usually select transformation operations based on their prior knowledge (e.g., required invariance). For example, horizontal flipping is expected to be effective for general object recognition but probably not for digit recognition. In addition to the selection, the designers need to combine several operations and set their magnitudes (e.g., degree of rotation). Therefore, designing of data augmentation strategies is a complex combinatorial problem.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.7\linewidth]{catch}
    \caption{Overview of our proposed model. We propose to use a \textbf{differentiable data augmentation pipeline} to achieve faster policy search by using adversarial learning.}
    \label{fig:catch}
\end{figure}

\begin{table}[tb]
    \centering
    \begin{tabular}{c|c|c|c|c}
        Dataset & AA  & PBA  & Fast AA  & Faster AA (ours) \\ \hline
        CIFAR-10 & 5,000 & 5.0 & 3.5 & \textbf{0.23} \\ \hline
        SVHN & 1,000 & 1.0 & 1.5 & \textbf{0.061} \\ \hline
        ImageNet & 15,000 & - & 450 & \textbf{2.3}
    \end{tabular}
    \vspace{5pt}
    \caption{\textbf{\faster is faster than others, without a significant performance drop}  (see section \ref{sec:experiments}). GPU hours comparison of \faster (Faster AA), \autoaugment (AA) \cite{Cubuk2018}, PBA \cite{Ho2019} and \fast (Fast AA) \cite{Lim2019}.}
    \label{tab:gpuhours}
    \vspace{-5pt}
\end{table}

When designing data augmentation strategies in a data-driven manner, one can regard the problem as searching for optimal hyper-parameters in a search space, which becomes prohibitively large as the combinations get complex. Therefore, efficient methods are required to find optimal strategies. If gradient information of these hyper-parameters is available, they can be efficiently optimized by gradient descent \cite{pmlr-v37-maclaurin15}. However, the gradient information is usually difficult to obtain because some magnitude parameters are discrete, and the selection process of operations is non-differentiable. Therefore, previous research to automatically design data augmentation policies has used black-box optimization methods that require no gradient information. For example, \autoaugment \cite{Cubuk2018} used reinforcement learning.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.95\linewidth]{filling}
    \caption{We regard data augmentation as a process that \textbf{fills missing data points of the original training data}; therefore, our objective is to minimize the distance between the distributions of augmented data and the original data using adversarial learning.}
    \label{fig:filling}
\end{figure}

In this paper, we propose to solve the problem by approximating gradient information and thus enabling gradient-based optimization for data augmentation policies. To this end, we approximate the gradients of discrete image operations using straight-through estimator \cite{Bengio2013} and make the selection process of operations differentiable by incorporating a recent differentiable neural architecture search method \cite{Liu2018c}. As the objective, we minimize the distance between the distributions of the original images and augmented images, because we want the data augmentation pipeline to transform images so that it fills missing points in the training data \cite{Lim2019} (see \Figref{fig:filling}). To make the transformed images match the distribution of original images, we use adversarial learning (see \Figref{fig:catch}). As a result, the searching process becomes end-to-end differentiable and significantly \textbf{faster} than prior work such as \autoaugment, PBA and \fast ~(see \Tabref{tab:gpuhours} \footnote{Note that \cite{Lim2019} and we estimate the GPU hours with an NVIDIA V100 GPU while \cite{Cubuk2018} did with an NVIDIA P100 GPU.}).

We empirically show that our method, which we call \faster, enables much faster policy search while achieving comparable performance with that of prior work on standard benchmarks: CIFAR-10, CIFAR-100 \cite{Krizhevsky2009}, SVHN \cite{Netzer2011} and ImageNet \cite{Russakovsky2015}.

In summary, our contributions are following three points:

\begin{enumerate}
    \item We introduce gradient approximations for several non-differentiable data augmentation operations.
    \item We make the searching of data augmentation policies end-to-end differentiable by gradient approximations, differentiable selection of operations and a differentiable objective that measures the distance between the original and augmented image distributions.
    \item We show that our proposed method, \faster, significantly reduces the searching time compared to prior methods without a performance drop.
\end{enumerate}


\section{Related Work}
\subsection*{Neural Architecture Search}

Neural Architecture Search (NAS) aims to automatically design architectures of neural networks to achieve higher performance than manually designed ones. To this end, NAS algorithms are required to select better combinations of components (e.g., convolution with a 3x3 kernel) from discrete search spaces using searching algorithms such as reinforcement learning  \cite{Zoph2016b} and evolution strategy \cite{Real2019}. Recently, DARTS \cite{Liu2018c} achieved faster search by relaxing the discrete search space to a continuous one which allowed them to use gradient-based optimization. While \autoaugment \cite{Cubuk2018} was inspired by \cite{Zoph2016b}, our method is influenced by DARTS \cite{Liu2018c}.

\subsection*{Data Augmentation}

Data augmentation methods improve the performance of learnable models by increasing the virtual size and diversity of training data without collecting additional data samples. Traditionally, geometric transformations and color enhancing transformations have been used in image recognition tasks. For example, \cite{Krizhevsky2012,He2016b}  randomly apply horizontal flipping and cropping as well as alternation of image hues. In recent years, other image manipulation methods have been shown to be effective. \cite{zhong2017random, DeVries2017} cut out a random patch from the image and replace it with random noise or a constant value. Another strategy is to mix multiple images of different classes either by convex combinations \cite{Zhang2017c,Tokozume2017b} or by creating a patchwork from them \cite{yun2019cutmix}. In these studies, the selection of operations, their magnitudes and the probabilities to be applied are carefully hand-designed.


\subsection*{Automating Data Augmentation}

Similar to NAS, it is a natural direction to aim to automate data augmentation. One direction is to search for better combinations of symbolic operations using black-box optimization techniques: reinforcement learning \cite{Cubuk2018,Ratner2017a}, evolution strategy \cite{Volpi2018}, Bayesian optimization \cite{Lim2019} and Population Based Training \cite{Ho2019}. As the objective, \cite{Cubuk2018,Volpi2018,Ho2019} directly aim to minimize error rate, or equivalently to maximize accuracy, while \cite{Ratner2017a,Lim2019} try to match the densities of augmented and original images.

Another direction is to use generative adversarial networks (GANs) \cite{Goodfellow2014b}. \cite{Tran2017a,Antoniou2019a} use conditional GANs to generate images that promote the performance of image classifiers. \cite{Shrivastava2017a,Sixt2018} use GANs to modify the outputs of simulators to look like real objects. 

Automating data augmentation can also be applied to representation learning such as semi-supervised learning \cite{Berthelot2019,Xie2019} and domain generalization \cite{Volpi2018}.


\section{Preliminaries}


In this section, we describe the common basis of \autoaugment \cite{Cubuk2018}, PBA \cite{Ho2019} and \fast \cite{Lim2019} (see also \Figref{fig:overview}). \faster also follows this problem setting.

In these works, input images are augmented by a policy which consists of  different sub-policies . A randomly selected sub-policy transforms each image . A single sub-policy consists of  consecutive image processing operations  which are applied to the image one by one. We refer to the number of consecutive operations  as operation count. In the rest of this paper, we focus on sub-policies; therefore, we omit the superscripts .

Each method first searches for better policies. After the searching phase, the obtained policy is used as a data augmentation pipeline to train neural networks.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{overview}
    \caption{Schematic view of the problem setting. Each image is augmented by a \textbf{sub-policy} randomly selected from the \textbf{policy}. A single sub-policy is composed of  consecutive \textbf{operations} , such as \texttt{shear\_x} and \texttt{solarize}. An operation  operates a given image with probability  and magnitude .}
    \label{fig:overview}
\end{figure}

\subsection{Operations}\label{sub:operations}

\begin{table}[tb]
    \centering
    \begin{tabular}{l|l|l}
           & Operation & Magnitude  \\ \hline
       \multirow{5}{*}{\shortstack[l]{Affine\\transformation}}
             & \texttt{shear\_x} & continuous \\
             & \texttt{shear\_y} & continuous \\
             & \texttt{translate\_x} & continuous \\
             & \texttt{translate\_y} & continuous \\
             & \texttt{rotate} & continuous \\
             
             & \texttt{flip}   & none \\ \hline
       \multirow{9}{*}{\shortstack[l]{Color\\enhancing\\operations}}
             & \texttt{solarize} & discrete \\
             & \texttt{posterize} & discrete \\
             & \texttt{invert} & none \\
             & \texttt{contrast} & continuous \\
             & \texttt{color} & continuous \\
             & \texttt{brightness} & continuous \\
             & \texttt{sharpness} & none \\
             & \texttt{auto\_contrast} & none \\
             & \texttt{equalize}   & none \\ \hline
       \multirow{2}{*}{Other operations}
             & \texttt{cutout} & discrete \\
             & \texttt{sample\_pairing} & continuous
    \end{tabular}
    \vspace{5pt}
    \caption{Operations used in \autoaugment, PBA, \fast and \faster. Some operations have discrete magnitude parameters , while others have no or continuous magnitude parameters. Different from previous works, we approximate gradients of operations w.r.t. discrete magnitude , which we describe in section \ref{subsub:magnitude}.}
    \label{tab:operations}
\end{table}

Operations used in each sub-policy include affine transformations such as \texttt{shear\_x} and color enhancing operations such as \texttt{solarize}. In addition, we use \texttt{cutout} \cite{DeVries2017} and \texttt{sample\_pairing} \cite{Inoue} following \cite{Cubuk2018,Ho2019,Lim2019}. We show all 16 operations used in these works in \Tabref{tab:operations}. We denote the set of operations as .

Some operations have magnitude parameters that are free variables, e.g., the angle in \texttt{rotate}. On the other hand, some operations, such as \texttt{invert}, have no magnitude parameter. For simplicity, we use the following expressions as if every operation had its magnitude parameter . Each operation is applied with probability of . Therefore, each image  is augmented as



Rewriting this mapping as , each sub-policy  consisting of operations  can be written as



\noindent where  and . In the rest of this paper, we represent an image operation ,  and  interchangeably according to the context.

\subsection{Search Space}\label{sub:search_space}

The goal of searching is to find the best operation combination  and parameter sets  for  sub-policies. Therefore, the size of the total search space is roughly . Using multiple sub-policies results in a prohibitively large search space for brute-force searching. \cite{Lim2019} uses Bayesian optimization in this search space. \cite{Cubuk2018,Ho2019} discretize the continuous part  into  or  values and search the space using reinforcement learning and population based training. Nevertheless, the problem is still difficult to solve naively even after discretizing the search space. For instance, if the number of sub-policies  is  with  consecutive operations, the discretized space size becomes . 

Previous methods \cite{Cubuk2018,Ho2019,Lim2019} use black-box optimization. Therefore, they need to train CNNs with candidate policies and obtain their validation accuracy. The repetition of this process requires a lot of time. In contrast, \faster achieves faster searching with gradient-based optimization to avoid repetitive evaluations, even though the search space is the same as in \fast. We describe the details in the next section.


\section{\faster}\label{sec:faster_auto_augment}


\faster explores the search space to find better policies in a gradient-based manner, which distinguishes our method. In section \ref{sub:differentiable_da}, we describe the details of gradient approximation for policy searching. To accomplish gradient-based training, we adopt distance minimization between the distributions of the augmented and the original images as the learning objective, which we present in section \ref{sub:density_matching}.



\subsection{Differentiable Data Augmentation Pipeline}\label{sub:differentiable_da}

Previous searching methods \cite{Cubuk2018,Ho2019,Lim2019} have used image processing libraries (e.g., Pillow) which do not support backpropagation through the operations in \Tabref{tab:operations}. Contrary to previous work, we modify these operations to be differentiable --- each of which can be differentiated with respect to the probability  and the magnitude . Thanks to this modification, the searching problem becomes an optimization problem. The sequence of operations in each sub-policy also needs to be optimized in the same fashion.

\subsubsection*{On the probability parameter }

First, we regard \eqref{eq:operation} as 



\noindent where  is sampled from Bernoulli distribution , i.e.  with probability of . Since this distribution is non-differentiable, we instead use Relaxed Bernoulli distribution \cite{Jang2016}



Here,  is a sigmoid function that keeps the range of function in  and  is a value sampled from a uniform distribution on . With low temperature of , this relaxed distribution behaves like Bernoulli distribution. Using this reparameterization, each operation  can be differentiable w.r.t. its probability parameter .

\subsubsection*{On the magnitude parameter }\label{subsub:magnitude}

For some operations, such as \texttt{rotate} or \texttt{translate\_x}, their gradients w.r.t. their magnitude parameters  can be obtained easily. However, some operations such as \texttt{posterize} and \texttt{solarize} discretize magnitude values. In such cases, gradients w.r.t.  cannot backpropagate through these operations. Thus, we approximate their gradient in a similar manner to the straight-through estimator \cite{Bengio2013,Oord2017}. More precisely, we approximate the th element of an augmented image by an operator  as



\noindent where  is a stop gradient operation which treats its operand as a constant. During the forward computation, the augmentation is exactly operated: . However, during the backward computation, the first term of the right-hand side of equation \ref{eq:straight_through} is ignored because it is constant, and then we obtain an approximated gradient:



Despite its simplicity, we find that this method works well in our experiments. Using this approximation, each operation  can be differentiable w.r.t. its magnitude parameter .

\subsubsection*{Searching for operations in sub-policies}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{schematic}
    \caption{Schematic view of the selection of operations in a single sub-policy when . During searching, we apply all operations to an image and take weighted sum of the results as an augmented image. The weights,  and , are also updated as other parameters. After searching, we sample operations according to the trained weights.}
    \label{fig:soft_path}
\end{figure}
                        
Each sub-policy  consists of  operations. To select the appropriate operation  where , we use a strategy similar to the one used in neural architecture search \cite{Liu2018c} (see also Algorithm \ref{alg:combination} and \Figref{fig:soft_path} for details). To be specific, we approximate the output of a single selected th operation  by weighted sum of the outputs of all operations as 



\noindent where,  is an operation in , and  and  are different operations if .  is a learnable parameter and  is a softmax function  with a temperature parameter . With a low temperature ,  becomes a onehot-like vector. During inference, we sample the th operation according to categorical distribution .


\begin{algorithm}
\caption{Selection of operations in a single sub-policy during searching. Refer to \Figref{fig:soft_path} for the   case.}
\label{alg:combination}

\begin{algorithmic}
    \Statex : input image, 
    \Statex : learnable weights, 
    \Statex : softmax function with temperature 
    \For{ in }
        \State Augment  by the th stage operations: 
        
        
    \EndFor
    \Return 
\end{algorithmic}
\end{algorithm}



\subsection{Data Augmentation as Density Matching}\label{sub:density_matching}

Using the techniques described above, we can back-propagate through the data augmentation process. In this section, we describe the objective of policy learning.

One possible candidate for the objective is the minimization of the validation loss as in DARTS \cite{Liu2018c}. However, this bi-level formulation takes a lot of time and costs a large memory footprint \cite{Finn2017b}. To avoid this problem, we adopt a different approach.

Data augmentation can be seen as a process that fills missing data points in training data \cite{Lim2019,Ratner2017a,Tran2017a}. Therefore, we minimize the distance between distributions of the original images and the augmented images. This goal can be achieved by minimizing the Wasserstein distance between these distributions  using Wasserstein GAN \cite{Arjovsky2017b} with gradient penalty \cite{Gulrajani2017}. Here,  is the parameters of its critic, or almost equivalently, discriminator. 
Unlike usual GANs for image modification, our model does not have a typical generator that learns to transform images using conventional neural network layers. Instead, a policy --- explained in previous sections --- is trained, and it transforms images using predefined operations.
Following prior work \cite{Cubuk2018,Ho2019,Lim2019}, we use WideResNet-40-2 \cite{Zagoruyko} (for CIFAR-10, CIFAR-100 and SVHN) or ResNet-50 \cite{He2016b} (for ImageNet) and replace their classifier heads with a two-layer perceptron  that serves as a critic. Besides, we add a classification loss to prevent images of a certain class to be transformed into images of another class (see Algorithm \ref{alg:faster_autoaugment}).

\begin{algorithm}
\caption{Training of \faster}
\label{alg:faster_autoaugment}

\begin{algorithmic}
    \Statex : learnable parameters of a sub-policy
    \Statex : distance between two densities with learnable parameters  
    \Statex : image classifier
    \Statex : cross entropy loss
    \Statex : coefficient of classification loss
    \Statex : training set
    \While{not converge}
        \State Sample a pair of batches  from 
        \State Augment data 
        \State Measure distance 
        \State Classification loss \\ 
        \State Update parameters  to minimize  using stochastic gradient descent (e.g., Adam)
    \EndWhile
\end{algorithmic}
\end{algorithm}

\section{Experiments and Results}\label{sec:experiments}

In this section, we show the empirical results of our approach on CIFAR-10, CIFAR-100 \cite{Krizhevsky2009}, SVHN \cite{Netzer2011} and ImageNet \cite{Russakovsky2015} datasets and compare the results with \autoaugment \cite{Cubuk2018}, PBA \cite{Ho2019} and \fast \cite{Lim2019}. Except for ImageNet, we run all experiments three times and report the average results. The details of datasets are presented in \Tabref{tab:dataset_summary}.

\subsection{Implementation Details}

Prior methods \cite{Cubuk2018,Ho2019,Lim2019} employed Python's Pillow \footnote{\url{https://python-pillow.org/}} as the image processing library. We transplanted the operations described in section \ref{sub:operations} to PyTorch \cite{Paszke2017}, a tensor computation library with automatic differentiation. For geometric operations, we extend functions in kornia \cite{eriba2019kornia}. For color-enhancing operations, sample pairing \cite{Inoue} and cutout \cite{DeVries2017}, we implement them using PyTorch. Operations with discrete magnitude parameters are implemented as described in section \ref{sub:differentiable_da} with additional CUDA kernels.

We use CNN models and baseline preprocessing procedures available from the \fast's repository \footnote{\url{https://github.com/kakaobrain/fast-autoaugment/tree/master/FastAutoAugment/networks}} and follow their settings and hyper-parameters for CNN training such as the initial learning rate and learning rate scheduling.


\subsection{Experimental Settings}\label{sub:experimental_settings}

To compare our results with previous studies \cite{Cubuk2018,Lim2019,Ho2019}, we follow their experimental settings on each dataset. We train the policy on randomly selected subsets of each dataset presented in \Tabref{tab:dataset_summary}. In the evaluation phase, we train CNN models from scratch on each dataset with learned \faster policies. For SVHN, we use both training and additional datasets.


Similar to \fast \cite{Lim2019}, our policies are composed of 10 sub-policies each of which has operation count  as described in section \ref{sub:search_space}. We train the policies for 20 epochs using ResNet-50 for ImageNet and WideResNet-40-2 for other datasets. In all experiments, we set temperature parameters  and  to . We use Adam optimizer \cite{Kingma2015} with a learning rate of , coefficients for running averages (betas) of , the coefficient for the classification loss  of , and the coefficient for gradient penalty of . Because GPUs are optimized for batched tensor computation, we apply sub-policies to chunks of images. The number of chunks determines the balance between speed and diversity. We set the chunk size to  for ImageNet and  for other datasets during searching. For evaluation, we use the chunk size of  for ImageNet and  for other datasets.


\begin{table}[tb]
    \centering
    
    \resizebox{\columnwidth}{!}{\begin{tabular}{l|r|r}
       Dataset  & Training set size & Subset size for policy training  \\ \hline
       CIFAR-10 \cite{Krizhevsky2009} & 50,000 & 4,000 \\
       CIFAR-100 \cite{Krizhevsky2009} & 50,000 & 4,000 \\
       SVHN \cite{Netzer2011} & 603,000 & 1,000 \\
       ImageNet \cite{Russakovsky2015} &  1,200,000 & 6,000
    \end{tabular}
    }
    \vspace{5pt}
    \caption{Summary of datasets used in the experiments. For the policy training on ImageNet, we use only 6000 images from the 120 selected classes following \cite{Cubuk2018,Lim2019}.}
    \label{tab:dataset_summary}
\end{table}

\subsection{Results}\label{sub:qualititative_results}

\subsubsection*{CIFAR-10 and CIFAR-100}

In \Tabref{tab:misc_results}, we show test error rates on CIFAR-10 and CIFAR-100 with various CNN models: WideResNet-40-2, WideResNet-28-10 \cite{Zagoruyko}, Shake-Shake (d) \cite{Gastaldi}. We train WideResNets for 200 epochs and Shake-Shakes for 1,800 epochs as \cite{Cubuk2018} and report averaged values over three runs for \faster. The results of baseline and Cutout are from \cite{Cubuk2018,Lim2019}.  \faster not only shows competitive results with prior work, but this method is significantly faster to train than others (See \Tabref{tab:gpuhours}). For CIFAR-100, we report results with policies trained on reduced CIFAR-10 following \cite{Cubuk2018} as well as policies trained on reduced CIFAR-100. The latter results are better than the former ones, which suggests the importance of training policy on the target dataset.

We also show several examples of augmented images in \Figref{fig:augmented_images}. The policy seems to prefer color enhancing operations as reported in \autoaugment \cite{Cubuk2018}.

In \Tabref{tab:reduced_cifar10_results}\footnote{\cite{Cubuk2018} reports better baseline and Cutout performance than us  (18.8 \% and 16.5 \% respectively), but we could not reproduce the results.}, we report error rates on Reduced CIFAR-10 to show the effect of \faster in the low-resource scenario. In this experiment, we randomly sample 4,000 images from the training dataset. We train the policy using the subset and evaluate the policy with WideResNet-28-10 on the same subset for 200 epochs. As can be seen, \faster improves the performance 7.7 \% over Cutout and achieves a close error rate to \autoaugment. This result implies that data augmentation can moderately unburden the difficulty of learning from small data.


\begin{figure*}
    \centering
    \includegraphics[width=0.68\linewidth]{augmented}
    \caption{Original and augmented images of CIFAR-10 (upper) and SVHN (lower). As can been seen, \faster can transform original images into diverse augmented images with sub-policies at the right-hand side.}
    \label{fig:augmented_images}
\end{figure*}

\begin{table*}[tb]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{l|l|ccccc|c}
       Dataset & Model & Baseline & Cutout \cite{DeVries2017} & AA \cite{Cubuk2018} & PBA \cite{Ho2019} & Fast AA \cite{Lim2019} & Faster AA (ours) \\ \hline
       \multirow{5}{*}{CIFAR-10} &
        WideResNet-40-2 \cite{Zagoruyko}  &              5.3 & 4.1 & 3.7 & -   & 3.6 & 3.7 \\
       &WideResNet-28-10 \cite{Zagoruyko} &              3.9 & 3.1 & 2.6 & 2.6 & 2.7 & 2.6 \\
       &Shake-Shake (26 d) \cite{Gastaldi} & 3.6 & 3.0 & 2.5 & 2.5 & 2.7 & 2.7 \\
       &Shake-Shake (26 d) \cite{Gastaldi} & 2.9 & 2.6 & 1.9 & 2.0 & 2.0 & 2.0 \\
       &Shake-Shake (26 d) \cite{Gastaldi} & 2.8 & 2.6 & 1.9 & 2.0 & 2.0 & 2.0 \\ \hline
       
       \multirow{3}{*}{CIFAR-100} &
        WideResNet-40-2  \cite{Zagoruyko} &              26.0 & 25.2 & 20.7 & -    & 20.7 & 22.1 / 21.4 \\
       &WideResNet-28-10 \cite{Zagoruyko} &              18.8 & 18.4 & 17.1 & 16.7 & 17.3 & 17.8 / 17.3 \\
       &Shake-Shake (26 d) \cite{Gastaldi}  &17.1 & 16.0 & 14.3 & 15.3 & 14.9 & 15.6 / 15.0 \\ \hline
       
      SVHN & WideResNet-28-10 \cite{Zagoruyko} &   1.5 & 1.3 & 1.1 & 1.2 & 1.1 & 1.2 \\
    \end{tabular}
    }
    \vspace{5pt}
    \caption{\textbf{\faster yields comparable performance with prior work}. Test error rates on CIFAR-10, CIFAR-100 and SVHN. We report average rates over three runs. For CIFAR-100, we report results obtained with policies trained on CIFAR-10 / CIFAR-100.}
    \label{tab:misc_results}

\end{table*}


\begin{table}
    \centering

    \begin{tabular}{ccc|c}
       Baseline & Cutout \cite{DeVries2017} & AA \cite{Cubuk2018}    & Faster AA  (ours)  \\ \hline
       24.3 & 22.5 & 14.1 & 14.8
    \end{tabular}
    \vspace{5pt}
    \caption{Test error rates with models trained on Reduced CIFAR-10, which consists of 4,000 images randomly sampled from the training set. We show that the obtained policy by \faster is useful for the low-resource scenario.}
    \label{tab:reduced_cifar10_results}
    
\end{table}

\subsubsection*{SVHN}

In \Tabref{tab:misc_results}, we show test error rates on SVHN with WideResNet-28-10 trained for 200 epochs. For \faster, we report the average value of three runs. \faster achieves the error rate of , which is  improvement over Cutout and on par with PBA. The augmented images are seen in \Figref{fig:augmented_images}. Besides, we show the augmented images in \Figref{fig:augmented_images} with used sub-policies, which seem to select more geometric transformations than CIFAR-10's policy as reported in \autoaugment \cite{Cubuk2018}.


\subsubsection*{ImageNet}

In \Tabref{tab:imagenet_results}, we compare the top-1 and top-5 validation error rates on ImageNet with \cite{Cubuk2018,Lim2019}. To align our results with \cite{Cubuk2018}, we also train ResNet-50 for 200 epochs. \cite{Cubuk2018,Lim2019} report top-1 / top-5 error rates of \% / \%; however, despite efforts to reproduce the results, we could not reach the same baseline performance. \faster achieves a \% improvement over the baseline on top-1 error rate. This gain is close to that of \autoaugment and \fast, which verifies that \faster has an effect comparable to prior work on a large and complex dataset.


\begin{table}
    \centering
    \begin{tabular}{lccc}
                           & Baseline & with Policy & Gain  \\ \hline
       AA \cite{Cubuk2018} &  23.7/6.9      &   22.4/6.2      &     1.3/0.7            \\
       Fast AA \cite{Lim2019} &       & 22.4/6.3      &       1.3/0.6          \\ \hline
       Faster AA (ours)  &   24.1/7.2        &  23.5/6.8      &     0.6/0.4             
    \end{tabular}
    \vspace{5pt}
    \caption{Top-1/Top-5 validation error rates on ImageNet \cite{Russakovsky2015} with ResNet-50 \cite{He2016b}. \faster achieves comparable performance gain to AA and Fast AA.}
    \label{tab:imagenet_results}
\end{table}


\section{Analysis}

\subsection{Changing the Number of Sub-policies}\label{sub:num_sub_policies}

The number of sub-policies  is arbitrary. \Figref{fig:num_sub_policies} shows the relationship between the number of sub-policies and the final test error on CIFAR-10 dataset with WideResNet-40-2. As can be seen, the more sub-policies we have, the lower the error rate is. This phenomenon is straight-forward because the number of sub-policies determines the diversity of augmented images; however, an increase in the number of sub-policies results in exponential growth of the search space, which is prohibitive for standard searching methods.


\begin{figure*}[tb]
    \centering
    \begin{tabular}{c}
        \hspace{-0.02\linewidth}
        \begin{minipage}{0.48\linewidth}
            \includegraphics[width=\linewidth]{num_policies_vs_error}
            \caption{\textbf{As the number of sub-policies grows, performance increases.} The relationship between the number of sub-policies and the test error rate (CIFAR-10 with WideResNet-40-2). We plot test error rates and their standard deviations averaged over three runs.}
            \label{fig:num_sub_policies}
        \end{minipage}
        
        \hspace{0.02\linewidth}
        
        \begin{minipage}{0.48\linewidth}
            \vspace{-10pt}
            \includegraphics[width=\linewidth]{depth_vs_error}
            \caption{\textbf{As the operation count grows, performance increases.} The relationship between the operation count of each sub-policy and the average test error rate of three runs (CIFAR-10 with WideResNet-40-2).}
            \label{fig:num_operations}
        \end{minipage}
    \end{tabular}
    

\end{figure*}

\subsection{Changing Operation Count}\label{sub:operation_count}

The operation count  of each sub-policy is also arbitrary. Like the number of sub-policies , the operation count of a sub-policy  also exponentially increases the search space. We change  from  to  on CIFAR-10 dataset with WideResNet-40-2. We present the resulted error rates in \Figref{fig:num_operations}. As can be seen, as the operation count in each sub-policy grows, the performance increases, i.e., the error rates decrease. Results of section \ref{sub:num_sub_policies} and section \ref{sub:operation_count} show that \faster is scalable to a large search space.


\subsection{Changing Data Size}

In the main experiments in section \ref{sec:experiments}, we use a subset of CIFAR-10 of 4,000 images for policy training. To validate the effect of this sampling, we train a policy on the full CIFAR-10 of 50,000 images as \cite{Lim2019} and evaluate the obtained policy with WideResNet-40-2. We find that the increase of data size causes a significant performance drop (from \% to \%) with the number of sub-policies . We hypothesize that this drop is because of lower capability of the policy when . Therefore, we train a policy with  sub-policies and randomly sample  sub-policies to evaluate the policy, which results in comparable error rates (\%). We present the results in \Tabref{tab:full_cifar10}, comparing with \fast \cite{Lim2019}, which shows the effectiveness of using subsets for \fast and \faster.

\begin{table}
    \centering
    \begin{tabular}{c|c|c}
    Data size & Fast AA \cite{Lim2019} & Faster AA (ours) \\ \hline
    4,000  & 3.6     & 3.7 \\
    50,000 & 3.7     &  3.8
    \end{tabular}
    \vspace{5pt}
    \caption{Test error rates on CIFAR-10 using policies trained on the reduced CIFAR-10 (4,000 images) and the full CIFAR-10 (50,000 images) with WideResNet-40-2.}
    \label{tab:full_cifar10}
\end{table}

\subsection{The Effect of Policy Training}

To confirm that trained policies are more effective than randomly initialized policies, we compare test error rates on CIFAR-10 with and without policy training, as performed in \autoaugment \cite{Cubuk2018}. Using WideResNet-28-10, trained policies achieve error rate of \% while randomly initialized policies have a slightly worse error rate of \% (both error rates are an average of three runs). These results imply that data augmentation policy searching is a meaningful research direction, but still has much room to improve.



\section{Conclusion}

In this paper, we have proposed \faster, which achieves faster policy searching for data augmentation than previous methods \cite{Cubuk2018,Ho2019,Lim2019}. To achieve this, we have introduced gradient approximation for several non-differentiable image operations and made the policy searching process end-to-end differentiable. We have verified our method on several standard benchmarks and showed that \faster could achieve competitive performance with other methods for automatic data augmentation. Besides, our additional experiments suggest that gradient-based policy optimization can scale to more complex scenarios.

We believe that faster policy searching will be beneficial for research on representation learning such as semi-supervised learning \cite{Berthelot2019,Xie2019} and domain generalization \cite{Volpi2018}. Additionally, learning from small data using learnable policies might be an interesting future direction.

\clearpage
{\small
\bibliographystyle{ieee}
\bibliography{ms}
}

\end{document}
