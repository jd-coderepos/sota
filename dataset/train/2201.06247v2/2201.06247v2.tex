\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{xcolor}
\newtheorem{theorem}{Theorem}
\usepackage{amsthm}
\usepackage{verbatim}

\DeclareMathOperator*{\argmax}{\arg\max}

\usepackage{newfloat}
\usepackage{listings}
\usepackage{wrapfig}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}


\pdfinfo{
/Title (Contrastive Regularization for Semi-Supervised Learning)
/Author (Doyup Lee, Ildoo Kim, Sungwoong Kim, Yeongjae Cheon, Minsu Cho, and Wook-Shin Han)
/TemplateVersion (2022.1)
} 

\setcounter{secnumdepth}{0} 




\title{Contrastive Regularization for Semi-Supervised Learning}




\author{
    \Large \textbf{Doyup Lee\textsuperscript{\rm 1}, 
                    Sungwoong Kim\textsuperscript{\rm 2}, 
                    Ildoo Kim\textsuperscript{\rm 2}, 
                    Yeongjae Cheon\textsuperscript{\rm 3}, 
                    Minsu Cho\textsuperscript{\rm 1}, 
                    Wook-Shin Han\textsuperscript{\rm 1}\thanks{Corresponding Author}} \\
}

\affiliations{
    POSTECH\textsuperscript{\rm 1}, Kakao Brain\textsuperscript{\rm 2}, DCML Inc.\textsuperscript{\rm 3} \\
    South Korea \\
    \{doyup.lee, mscho, wshan\}@postech.ac.kr\textsuperscript{\rm 1}, \{swkim, ildoo.kim\}@kakaobrain.com\textsuperscript{\rm 2}, yj.star@dcml.co.kr\textsuperscript{\rm 3}\\
}

\usepackage{bibentry}


\begin{document}
\maketitle


\begin{abstract}
  Consistency regularization on label predictions becomes a fundamental technique in semi-supervised learning, but it still requires a large number of training iterations for high performance.
  In this study, we analyze that the consistency regularization restricts the propagation of labeling information due to the exclusion of samples with unconfident pseudo-labels in the model updates.
  Then, we propose \textit{contrastive regularization} to improve both efficiency and accuracy of the consistency regularization by well-clustered features of unlabeled data.
  In specific, after strongly augmented samples are assigned to clusters by their pseudo-labels, our contrastive regularization updates the model so that the features with confident pseudo-labels aggregate the features in the same cluster, while pushing away features in different clusters.
  As a result, the information of confident pseudo-labels can be effectively propagated into more unlabeled samples during training by the well-clustered features.
  On benchmarks of semi-supervised learning tasks, our contrastive regularization improves the previous consistency-based methods and achieves state-of-the-art results, especially with fewer training iterations.
  Our method also shows robust performance on open-set semi-supervised learning where unlabeled data includes out-of-distribution samples.
  
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Recent semi-supervised learning (SSL) methods mostly make use of the consistency regularization to learn a specific task with sparse labels, showing competitive results to the fully supervised learning \cite{berthelot2019remixmatch,sohn2020fixmatch,li2020comatch}.
The consistency regularization enforces a model to produce consistent predictions on various augmented views of input with pseudo-labeling \cite{lee2013pseudo}. 
Moreover, in order to avoid a confirmation bias \cite{arazo2020pseudo} and increase the reliability of pseudo-labeling, a selection mask is typically used in this consistency regularization to exclude unconfident label predictions during SSL training. 
Consequently, the consistency regularization can propagate the labeling information into unlabeled samples around the augmented views of confident pseudo-labels \cite{ghosh2021data}.

Despite its promising results, the existing consistency regularization requires an expensive training cost to achieve high performance.
For example, although FixMatch \cite{sohn2020fixmatch} can achieve high SSL performance without pretraining on a large scale unlabeled data \cite{chen2020big}, it needs over 10,000 epochs to obtain the best performance even on small-scale datasets such as SVHN, CIFAR-10, or CIFAR-100.
Thus, we first analyze the inefficiency of the consistency regularization for SSL, both theoretically and empirically, and then verify that this inefficiency is originated from the exclusion of samples with unconfident pseudo-labels when updating a model.
Namely, it restricts the active propagation of confident labeling information into unlabeled samples, especially in the early stage of training.

Based on the above analysis, we propose \textit{contrastive regularization} to improve the performance of SSL based on consistency regularization.
The main idea is described in Figure~\ref{fig:concept}.
The consistency regularization moves the features of strongly augmented samples having only confident pseudo-labels toward their corresponding class centers of the confident features by pseudo-labels. 
In contrast, the proposed contrastive regularization forms class clusters based on both confident and unconfident pseudo-labels.
Then, it moves the features having confident pseudo-labels toward the center positions of their clusters, while pulling the features of samples with both confident and unconfident pseudo-labels in the same cluster and pushing the features in different clusters.
Thus, a model can learn well-clustered features of unlabeled data, enabling the confident labeling information to be propagated into more unlabeled samples during training.

In the experiments, we show that our contrastive regularization improves the performance of consistency regularization methods on various SSL benchmarks, including SVHN, CIFAR-10, CIFAR-100, STL-10, and ImageNet with limited labels.
Especially, different from the previous methods, we show that our method leverages unlabeled samples in the early stage of training and requires much fewer iterations for the outperformance.
We also demonstrate that the contrastive regularization achieves the robust performance on the task of open-set SSL, which is more realistic in that unlabeled data contains out-of-distribution samples \cite{oliver2018realistic,yu2020multi}.
Finally, we conduct an extensive ablation study to show that the contrastive regularization is valid and not highly sensitive to the selection of hyper-parameters.

Our main contributions can be summarized as follows. 
1) It is the first study to analyze the limitation of the consistency regularization with respect to the efficiency of SSL training.
2) We propose a simple yet powerful solution, the contrastive regularization, which consistently improves the SSL performance on different SSL benchmarks with fewer training iterations than the previous consistency regularization.
3) Contrastive regularization shows the robustness on the more realistic benchmark that includes out-of-distribution samples in the unlabeled dataset.

\begin{figure}
\centering
\includegraphics[width=3.3in]{ssc_concept5.pdf}
\caption{The feature update of (a) consistency and (b) contrastive regularization. Different colors represent pseudo-labels. The circles with solid and dashed line are penultimate features having confident and unconfident pseudo-labels, respectively. The black dashed line is the decision boundary. The symbol  represents a cluster center that is estimated by confident samples only, and * represents a cluster center that is estimated by all samples in the same cluster. The length of arrows represents the magnitude of gradient vectors. The cluster centers are computed by the class weight vectors.}
\label{fig:concept}
\vspace{-0.1in}
\end{figure}

\section{Related Work}
\label{sec:RW}

\paragraph{Consistency Regularization for SSL.} Recent SSL methods use the consistency regularization \cite{laine2016temporal} and focus on the policies of stochastic data augmentations such as adversarial perturbations \cite{miyato2018virtual} or mixup \cite{berthelot2019remixmatch,berthelot2019mixmatch,zhang2018mixup}.
As the most simplified yet powerful framework of SSL, UDA and FixMatch \cite{xie2020unsupervised,sohn2020fixmatch} show that the simple combination of strong data augmentation such as RandAugment \cite{cubuk2020randaugment} and pseudo-labeling \cite{lee2013pseudo} can obtain high performance.
Thus, we focus on improving the consistency regularization, because they have shown state-of-the-art results compared with other SSL approaches \cite{shi2018transductive,iscen2019label}.

\paragraph{Semi-Supervised Learning with Self-Supervision.} The SSL performance can be improved when self-supervised learning is used with an auxiliary task for representation learning, and our contrastive regularization can be viewed as an auxiliary task for SSL.
S4L \cite{zhai2019s4l} demonstrates that auxiliary tasks such as rotation or exemplar self-supervision can improve the SSL performance.
For time-series classification, forecasting of the next-step value is used as an auxiliary task \cite{jawed2020self}. 
CoMatch \cite{li2020comatch} unifies pseudo-labeling, self-supervised learning, and graph-based SSL, using the graph contrastive learning and the pseudo-label smoothing with a large size of memory bank \cite{he2020momentum}.
Although both CoMatch and our method use a contrastive loss to regularize the unlabeled features, our method can be easily in tandem with the consistency regularization with minimal change for the contrastive loss.


\paragraph{Pretraining and Finetuning.}
Finetuning after pretraining on an upstream task is a solution for learning a task with scarce labels, when large-scale labeled or unlabeled datasets are available for the upstream task.
For instance, a model, which is pretrained on a large-scale labeled dataset, can be well transferred to downstream tasks \cite{kolesnikov2019large}.
but a negative transfer occurs when the target task is unrelated to the upstream domain or task \cite{zamir2018taskonomy}.
When a large-scale unlabeled data is available, a framework using both \textit{task-agnostic} pretraining and task-specific finetuning can become a strong SSL approach \cite{vincent2010stacked,he2020momentum,chen2020big}.
However, utilizing unlabeled samples in a task-specific way can outperform a task-agnostic approach without a large number of unlabeled samples.
Thus, we emphasize that \textit{task-specific} SSL methods are important because it is hard to collect a large number of unlabeled samples in the real world.



\section{Contrastive Regularization for Semi-Supervised Learning}
\label{sec:method}
In this section, we introduce our \textit{contrastive regularization} to improve the SSL performance of the consistency regularization.
We first formulate SSL and the consistency regularization, which is the most common approach and shows remarkable results with deep neural networks (DNNs).


\subsection{Problem Formulation}
We assume that a labeled dataset  and an unlabeled dataset  are given to train a model parameterized by .
A mini-batch  consists of  labeled samples  and  unlabeled samples , where  is the ratio of unlabeled samples to the labeled samples in a mini-batch.
The total loss  is minimized at each training iteration

where  is a supervised loss,  is an unsupervised loss, and  is an unlabeled loss ratio.
Cross entropy is used for a supervised loss, and the type of  determines how to leverage the unlabeled samples.
For example, entropy regularization \cite{grandvalet2005semi} and pseudo-labeling enforce the predictions on unlabeled samples to have a low entropy, so that the decision boundary is located in the low-density area \cite{sajjadi2016regularization}.

For an unlabeled sample , the label prediction  is given by the model with  comprising -class weight matrix , where  denotes the penultimate features.
We define a stochastic function of strong augmentation as , and the set of strongly augmented samples for an unlabeled mini-batch as , where  is the number of augmented view per an unlabeled sample in the mini-batch.
Then, the consistency regularization, , is defined as 

where  is the augmented sample of ,  is a confidence threshold, and  is the cross entropy loss.
\textcolor{black}{In the remaining parts of this paper, a strongly augmented sample of  is represented as  for brevity.}
 is the pseudo-label of  and defined as , where  and sg is the stop gradient.
Note that the pseudo-label of  is determined by the label prediction on the sample without strong augmentation, , for the reliability of pseudo-labeling.

The performance of consistency regularization highly depends on the choices of  and .
Data augmentation encourages DNNs to learn the generalized representations with the local geometry of the data-manifold, assuming that the learned manifolds of different classes are well-separated \cite{verma2019interpolation,ghosh2021data}. 
Therefore, the features having different pseudo-labels become well-separated, propagating the confident (pseudo-)labeling information into their neighbors on the data manifold.
Determining the threshold  is an inherent trade-off for the SSL performance, because the  controls the balance of the reliability and the number of unlabeled samples leveraged.
A higher value of  is commonly used to avoid a confirmation bias, but it restricts unlabeled samples to be included in SSL training and can preclude the model from learning the transformation-invariant representations on the excluded samples \cite{arazo2020pseudo}.
It minimizes the entropy of only a sample using a confident pseudo-labeling for SSL training.
\vspace{-0.15in}

\subsection{Training Inefficiency of Consistency Regularization} \label{sec:grad_cs}
As the consistency regularization achieves high SSL performance competitive with the fully supervised setting, it requires a large number of training iterations even on small-scale datasets.
For example, FixMatch \cite{sohn2020fixmatch} requires over 10,000 epochs to train WRN-28-2 \cite{zagoruyko2016wide} on the CIFAR-10 dataset.
However, when the labels are fully provided, about 100 epochs are enough to learn the dataset under supervision.

Here, we analyze the consistency regularization to show its training inefficiency.
Assume that  is a set of strongly augmented samples assigned to the -th class by the pseudo-label, .
The minus gradients of  with respect to the features  and to the -th class weight vector  are as follows:


By this gradient analysis, we postulate that the inefficiency of the consistency regularization results from the exclusion of samples with unconfident pseudo-labels and the training bias on the confident pseudo-labels by the masking .
Figure~\ref{fig:concept}(a) contains the interpretation of the gradient analysis.
Here, we assume that the features with unconfident pseudo-labels are close to the decision boundary, considering the linearity of softmax classifier \cite{bishop2006pattern}.
The class weight vector  is updated to the weighted sum of only confident features in Eq.~(\ref{eq:cs_class}).
Then, the confident features in Eq.~(\ref{eq:cs_hidden}) are updated by the class weight vectors, so the features only having confident pseudo-labels become close together.
However, the unconfident samples are excluded in the gradients computations, and the labeling information of confident samples cannot be effectively propagated into the unlabeled samples.
In addition, the class weight vector is slowly changed due to the exclusion of unconfident samples, because the gradient in Eq.~\ref{eq:cs_class} is bounded by the confidence threshold, , where  is typically selected as a high value such as 0.95.
Thus, the model cannot leverage lots of unlabeled samples over the SSL training and requires a large number of training iterations to gradually increase the number of confident samples. 


\subsection{Contrastive Regularization for SSL} \label{sec:grad_cr}
We propose \textit{contrastive regularization} in Figure~\ref{fig:concept}(b) to effectively leverage unlabeled samples for SSL.
Even though Figure~\ref{fig:concept}(b) describes the two-class classification, the concept can be generalized to the setting of multiple classes, and all experiments in this study are also conducted on multi-class tasks.
Different from the consistency regularization, the class clusters are formed by the features with both confident and unconfident pseudo-labels.
Then, our method regularizes the hidden features of confident unlabeled samples to be moved toward the samples with unconfident pseudo-labels in the same cluster, and propagates the labeling information.
At the same time, to leverage the unconfident samples without decreasing the confident threshold , the features having confident pseudo-labels pull the features of unconfident samples in the same cluster, while pushing the features in different clusters.
It can achieve the entropy minimization for SSL, and unlabeled samples are beneficial with a small overlap of classes, since the contrastive regularization can learn well-clustered features that reduce the overlaps.

For this, we modify SupContrast \cite{khosla2020supervised}, which is used for a supervised pretraining on large-scale labeled data, into SSL setting by adding a projection head after the penultimate features.
We define the set of \textit{pseudo}-positive pairs of  as , where  and  are the pseudo-label of  and , respectively.
Note that a pseudo-label of a strongly augmented sample  is defined by the label prediction on the unlabeled sample  before strong augmentation.
\textcolor{black}{The positive sample pairs represent the samples whose pseudo-labels are the same, and the augmented samples in  have the same pseudo-label with .}
Then, the contrastive regularization, , is defined as follows:


where  is a confidence threshold,  is a temperature scaling parameter, and  is a \textit{normalized} vector of the projection head.
Our total loss is 
, where  and  is the loss ratio of consistency and contrastive regularization, respectively.

The features of confident samples move toward the centroid of its feature cluster, which consists of features having the same pseudo-labels, and pull the unconfident features in the same cluster by our contrastive regularization.
Without the loss of generalizability, we notate the softmax score of  with  as , and assume the normalized vector , and .
For  and , the minus gradients of  with respect to  are as follows: 

where  is a remainder term and small enough.
We attach the detailed derivation of Eq.~(\ref{eq:gradient_cr}) and~(\ref{eq:gradient_cr_unconf}) in Appendix A.
If the  has a confident pseudo-label as Eq.~(\ref{eq:gradient_cr}), the contrastive regularization updates its feature vector  toward the weighted sum of positive features both with confident and unconfident pseudo-labels.
Different from the consistency regularization, the feature update of confident samples also considers the features with unconfident pseudo-labels in the same cluster.
At the same time, in Eq.~(\ref{eq:gradient_cr_unconf}), the confident features pull the features of both confident and unconfident samples in the same cluster , while pushing the features in different clusters.
Although our contrastive regularization of a confident feature  learns to aggregate positive samples with , the  can push a positive sample  of  with a negative value in Eq.~(\ref{eq:gradient_cr_unconf}) \textit{during training}, because all positive samples in a mini-batch are included in the denominator of the long term in Eq.~(\ref{eq:sample_CR}).
However, note that other negative samples in the different clusters still push , avoiding a wrong cluster assignment by the negative values of Eq.~(\ref{eq:gradient_cr_unconf}) during training.
Thus, the model can propagate the confident labeling information into the unlabeled samples, while learning well-clustered features for SSL \cite{castelli1996relative}.

Although our contrastive regularization utilizes the information of unconfident pseudo-labeling, the confirmation bias does not more increase than previous consistency regularization methods.
According to Appendix C, the performance degradation by the memorization of wrong pseudo-labels occurs in the later stage of SSL training.
In the early stage of training, our method learns well-clustered representations of unlabeled samples to effectively propagate labeling information of labeled samples and unlabeled samples with confident pseudo-labeling.
Thus, the contrastive regularization can improve the SSL performance before the SSL model starts to memorize wrong pseudo-labels~\cite{arpit2017closer}.
In addition, different from the consistency regularization, our method is performed on features of unlabeled samples, not directly on class predictions, to avoid the memorization of wrong labels by the contrastive regularization.




\begin{table*}[]
\centering
\footnotesize
\caption{Test accuracies (\%) for SVHN and CIFAR-10 on five different runs with randomly selected labeled samples. The Asterisks mean that the results are from the previous studies \cite{sohn2020fixmatch,li2020comatch,kim2021selfmatch}.}
\label{tab:cifar_acc}
\begin{tabular}{l|cccc|cccc}
\toprule
 & \multicolumn{4}{c|}{SVHN} & \multicolumn{4}{c}{CIFAR-10} \\ \hline
 Method & 20 labels & 40 labels & 250 labels & 1000 labels&  20 labels & 40 labels & 250 labels & 4000 labels       \\ \hline
 MixMatch*           &   - & 57.45\scriptsize{14.53} & 96.02\scriptsize{0.23} & 96.50\scriptsize{0.28} & -  &  52.46\scriptsize{11.50} & 88.95\scriptsize{0.86} & 93.58\scriptsize{0.10}  \\
 UDA*           &   - & 43.75\scriptsize{20.51} & 94.31\scriptsize{2.76} & 97.54\scriptsize{0.24} & -  &  70.95\scriptsize{5.93} & 91.18\scriptsize{1.08}  & 95.12\scriptsize{0.18} \\
 ReMixMatch*    &   -  & \textbf{96.66\scriptsize{0.20}} & 97.08\scriptsize{0.48} & 97.35\scriptsize{0.08} & - & 81.90\scriptsize{9.64} & 94.46\scriptsize{0.05} & 95.28\scriptsize{0.13}  \\ 
 CoMatch*    &   -  & - & - & - & 81.85\scriptsize{5.56} & 91.51\scriptsize{2.15} & -  & -  \\ \hline
 FixMatch    & 90.05\scriptsize{8.01} & 94.83\scriptsize{2.24} & 97.28\scriptsize{0.66} & 97.46\scriptsize{0.09} & 74.98\scriptsize{11.38}  & 91.24\scriptsize{3.72} & 94.67\scriptsize{0.28} &  95.57\scriptsize{0.05}\\
 FixMatch+CR   & \textbf{94.96\scriptsize{4.77}} & 96.33\scriptsize{1.84} & \textbf{97.55\scriptsize{0.08}} & \textbf{97.61}\scriptsize{0.06} & \textbf{88.26}\scriptsize{1.38} & \textbf{94.31}\scriptsize{0.90} & \textbf{94.96}\scriptsize{0.30} & \textbf{95.84}\scriptsize{0.13}\\ \hline
 SelfMatch*    &   -  & 96.58\scriptsize{1.02} & 97.37\scriptsize{0.43} & 97.49\scriptsize{0.07} & - & 93.19\scriptsize{1.08} & 95.13\scriptsize{0.26} & 95.94\scriptsize{0.08}  \\ 
 FixMatch+CR++   & \textbf{96.88}\scriptsize{0.60} & \textbf{97.05}\scriptsize{0.28} & \textbf{97.95}\scriptsize{0.09} & \textbf{98.11}\scriptsize{0.05}& \textbf{94.24}\scriptsize{3.48} & \textbf{95.26}\scriptsize{0.70} & \textbf{96.00}\scriptsize{0.31} & \textbf{96.68}\scriptsize{0.18}\\
\bottomrule
\end{tabular}
\end{table*}


\section{Experiments} \label{sec:EXP}
We empirically validate that the contrastive regularization consistently improves the performance on standard SSL benchmarks such as SVHN, CIFAR-10, CIFAR-100, STL-10, and ImageNet with limited labels.
We also show that the contrastive regularization is also robust to the open-set SSL setting, and an extensive ablation study is conducted in this section.
For experiments, we use an exponential moving average (EMA) of model parameters \cite{tarvainen2017mean} with 0.999 momentum and cosine learning rate scheduling in \cite{sohn2020fixmatch} for all experiments.
The training epochs are computed based on the batch size of unlabeled samples.
For a fair comparison, we follow the experimental setting in the previous study \cite{sohn2020fixmatch}, and attach the implementation details in Appendix B.

\subsection{Classification of SVHN, CIFAR-10, CIFAR-100}
To analyze the effect of contrastive regularization (CR), we reproduce FixMatch and UDA using Pytorch 1.6.0 \cite{NEURIPS2019_9015}.
For a fair comparison with previous studies, we use the encoder of WRN-28-2 (1.5M parameters) for SVHN and CIFAR-10, and a WRN-28-8 (23.4M parameters) for CIFAR-100.
For SVHN and CIFAR-10, we also use WRN-28-8 (FixMatch+CR++) for comparison with SelfMatch \cite{kim2021selfmatch} which uses over 21M parameters.

For the projection embedding , we add a 2-layer MLP after the feature extractor , and its dimension sizes are 64 for WRN-28-2 and 256 for WRN-28-8.
We use RandAugment \cite{cubuk2020randaugment} for strong data augmentation, and set .
We use  for SVHN and CIFAR-10, and  for CIFAR-100.
Following \cite{sohn2020fixmatch}, FixMatch and UDA use 10,500 epochs of unlabeled data.
FixMatch+CR uses 6,500 epochs for CIFAR-10 and SVHN, and 2,500 epochs for CIFAR-100 to achieve state-of-the-art results.
Nevertheless, note that much less time needs to outperform FixMatch in the next section.

For SVHN and CIFAR-10, Table~\ref{tab:cifar_acc} shows that our method consistently improves the SSL performance of FixMatch on the same codebase.
Consequently, the proposed FixMatch+CR achieves the state-of-the-art performance of WRN-28-2 except SVHN with 40 labels.
Although FixMatch+CR cannot outperform the reported result of ReMixMatch \cite{berthelot2019remixmatch} on SVHN with 40 labels, our method improves the test accuracy of FixMatch by 1.50\%.

\begin{table} \footnotesize
\centering
\caption{Test accuracy (\%) of WRN-28-8 on the CIFAR-100 dataset with 400, 2500, and 10000 labels.}
\begin{tabular}{l|ccc}
\hline
 & \multicolumn{3}{c}{CIFAR-100} \\ \hline
Medthod     & 400 labels & 2500 labels & 10000 labels \\ \hline
UDA & 48.02\scriptsize{2.66} & 70.50\scriptsize{0.53} & 77.07\scriptsize{0.33} \\
UDA+CR & \textbf{49.91}\scriptsize{0.79} & \textbf{72.12}\scriptsize{0.28} & \textbf{78.58}\scriptsize{0.11} \\ \hline
FixMatch & 48.48\scriptsize{0.55} & 71.53\scriptsize{0.29} & 78.03\scriptsize{0.26} \\
FixMatch+CR & \textbf{50.77}\scriptsize{0.79} & \textbf{72.42}\scriptsize{0.37} & \textbf{78.97}\scriptsize{0.23} \\ \hline
\end{tabular}
\label{tab:ssl_cifar100}
\vspace{-0.1in}
\end{table}

We emphasize that the contrastive regularization has remarkable performance gains.
For the setting of 20 labels (2 labels per class), FixMatch+CR significantly improves the accuracy and the robustness to the selection of labeled samples.
FixMatch+CR outperforms CoMatch \cite{li2020comatch}, which has firstly reported the results on CIFAR-10 with 20 and 40 labels.
In addition, WRN-28-2 with FixMatch+CR are competitive with SelfMatch, although the number of parameters is about 15 times smaller.
When we increase the number of parameters into 23.4M, FixMatch+CR++ outperforms SelfMatch in all experimental settings of SVHN and CIFAR-10.

Our method is also effective on the CIFAR-100 dataset with 400, 2,500, and 10,000 labels (Table~\ref{tab:ssl_cifar100}).
When the contrastive regularization is used along with UDA and FixMatch, it improves the performance and outperforms the previous methods.
Note that the performance gains are significant and consistent regardless of the number of labels.

\subsection{Classification of STL-10 and ImageNet}

We evaluate our contrastive regularization on a larger scale of datasets such as STL-10 and ImageNet.
We set  for SVHN and  for ImageNet, and  for the two.
The STL-10 dataset includes 5,000 labeled and 100,000 unlabeled 9696 images in 10 classes.
We train WRN-37-2 (5.9M parameters) on STL-10 with five folds of 1,000 and 5,000 labels.
10,500 and 5,000 epochs are used for FixMatch and FixMatch+CR, respectively.
The projection head uses 2-layer MLP with 256 dimensions.
For 1,000 labels, FixMatch+CR improves the results of FixMatch in Table~\ref{tab:ssl_stl_imagenet}.
For 5,000 labels, FixMatch+CR achieves 95.40\%, improving 95.18\% of FixMatch.

\begin{table}
\footnotesize
\centering
\caption{Test accuracy (\%) on the STL-10 and ImageNet datasets. Top-1 (top-5) accuracies are reported for ImageNet}
\begin{tabular}{l|c|cc}
\hline
 & STL-10 & \multicolumn{2}{c}{ImageNet} \\ \hline
Method     & 1,000 labels & 1\% labels & 10\% labels \\ \hline
FixMatch & 89.34\scriptsize{1.79} & 51.29 (72.48) & 72.18 (89.98) \\
FixMatch+CR & \textbf{93.04}\scriptsize{0.42} & \textbf{57.77 (78.12)} & \textbf{72.77 (90.15)} \\ \hline
\end{tabular}
\label{tab:ssl_stl_imagenet}
\vspace{-0.15in}
\end{table}

We also evaluate our method on the ImageNet dataset that includes about 1.3M training images in 1,000 object classes.
We use a self-supervised and pretrained ResNet-50 model by MoCo v2 \cite{he2020momentum,chen2020improved}, since reproducing FixMatch on ImageNet from scratch requires expensive cost such as about three days using 32 cores of TPU.
We use 1,024 labeled and 5,120 unlabeled images in each mini-batch, and train a model in 300 epochs of unlabeled data.
2-layer MLP with 512 dimensions is used for the projection embedding.
For 1\% and 10\% of labels, our contrastive regularization improves the accuracy of FixMatch in Table~\ref{tab:ssl_stl_imagenet}, and our method significantly improves the performance on the fewer labels.
Thus, we conclude that our contrastive regularization is also effective on large-scale datasets.

\begin{figure}
\centering
\includegraphics[width=3.315in]{c100_10000_time_2.pdf}
\caption{Results of FixMatch and FixMatch+CR with WRN-28-4 trained on the CIFAR-100 with 10000 labels. (a) Test accuracy over training time, (b) Silhouette score of penultimate features based on pseudo-labels.}
\label{fig:emp_c100_w4}
\vspace{-0.1in}
\end{figure}


\subsection{Cost Efficiency of the Contrastive Regularization} \label{sec:efficiency}

The contrastive regularization not only improves the accuracy, but also enhances the training efficiency of SSL.
For a fair comparison of training time, four NVIDIA V100 GPUs are used to train both FixMatch and FixMatch+CR.
In Figure~\ref{fig:emp_c100_w4}(a), the accuracy of FixMatch gradually increases over the entire training time of 10,500 epochs.
Although one iteration of FixMatch+CR takes about 1.5 more time than FixMatch due to the use of two strongly augmented views, FixMatch+CR only takes 31\% of the total training time of FixMatch to achieve the best performance.
Also, 7\% of the training time of FixMatch is enough for FixMatch+CR to achieve the best performance of FixMatch (dashed line).
For other datasets, 2,500 epochs for SVHN and CIFAR-10, 1,000 epochs for CIFAR-100, and 1,500 epochs for STL-10 are enough to outperform FixMatch of 10,500 epochs, as shown in Appendix C.
Consequently, our method can save the training cost, reducing the training time and iterations.

We conjecture that the improved efficiency comes from the well-clustered representations by the contrastive regularization in the early stage of training.
Figure~\ref{fig:emp_c100_w4}(b) shows how features are well-clustered according to their pseudo-labels in terms of Silhouette score \cite{rousseeuw1987silhouettes}. 
If the decision boundary lies in the low-density regions and the features are well-clustered, the score is closed to +1, otherwise it is closed to -1.
For the features of strongly augmented samples, the clustering scores of FixMatch are near zero and it increases slowly after 40K iterations.
However, the clustering score of FixMatch+CR increases fast in the early stage of training. 
In addition, the scores are much higher than those of FixMatch during the entire training. 
This means that our contrastive regularization is effective in feature clustering, especially in the early training stage, and eventually improves both the training efficiency and final performance. 

\begin{table} \centering
\footnotesize
\caption{Test accuracy (\%) with different sizes of the widen factor on the same random seed. 28 layers of WRN is trained on CIFAR-100 with 2500 labels.}
\label{tab:abl_wf}
\begin{tabular}{l|cccc}
\toprule
 Widen Factor & 1 & 2 & 4 & 8 \\ \hline
 \# of Params & 0.38M & 1.48M & 5.87M & 23.40M \\ \hline
FixMatch & 55.86 & 64.74 & 69.75 & 72.02 \\
FixMatch+CR & \textbf{59.94} & \textbf{69.03} & \textbf{72.00} & \textbf{72.83} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}


Table~\ref{tab:abl_wf} shows that the contrastive regularization is especially effective to a smaller model for SSL.
When the contrastive regularization is applied to WRN-28-4 and WRN-28-2 with 2500 labels of the CIFAR-100, the accuracies are improved by 2.25\% and 4.29\%, respectively.
Thus, the obtained accuracies of WRN-28-2 and WRN-28-4 with the contrastive regularization are comparable with those of WRN-28-4 and WRN-28-8 without it, respectively.
Note that increasing the widen factor by two times leads to a four times larger number of trainable parameters.

\subsection{Open-Set Semi-Supervised Learning}
For a realistic evaluation, open-set SSL \cite{oliver2018realistic,yu2020multi} assumes that an unlabeled dataset includes out-of-distribution (OOD) samples, which are totally different from the training and test samples.
Considering SVHN and CIFAR-10 as OOD of CIFAR-100, we add the OOD samples to the unlabeled data of CIFAR-100, and train WRN-28-4 on CIFAR-100 with 2500 and 10000 labels. Then, we evaluate the accuracy on the test data of CIFAR-100 according to the number of added OOD samples such as 10K, 20K, 30K, and 40K.

As shown in Figure~\ref{fig:opensetSSL}, the contrastive regularization enhances the robustness of SSL to the OOD samples.
FixMatch has severe degradation of accuracy as OOD samples are added into unlabeled data.
However, FixMatch+CR avoids the accuracy degradation and always outperforms FixMatch regardless of the number of OOD samples, because FixMatch+CR effectively leverages unlabeled samples from in-distribution, when the number of labels is limited. (Table~\ref{tab:cifar_acc}).
Note that FixMatch+CR is more robust to the OOD samples from SVHN than those from CIFAR-10, since SVHN has a totally different class distribution from CIFAR-100 and less affects the discrimination of the CIFAR-100 classes.

\begin{figure}
\centering
\includegraphics[width=3.315in]{openset_two_figure.pdf}
\caption{Open-set SSL results of WRN-28-4 on CIFAR-100 with (a) 2500 and (b) 10000 labels. OOD samples (SVHN, CIFAR-10) are added into the unlabeld samples.}
\label{fig:opensetSSL}
\vspace{-0.125in}
\end{figure}


\subsection{Ablation Study}


\begin{figure*}
\centering
\includegraphics[width=6.5in]{IJCAI_Ablation.pdf}
\caption{The ablation study on the hyper-parameters (WRN-28-4, CIFAR-100 with 2500 labels): (a) contrastive regularization loss ratio, \textcolor{black}{(b) consistency regularization loss ratio, (c) unlabeled sample ratio in a mini-batch,} and (d) confidence thresholds.}
\label{fig:hyper_abl}
\end{figure*}

\textbf{Effects of Hyper-parameter Settings.}
We conduct an extensive ablation study to understand the effects of the different components in our method.
In Figure~\ref{fig:hyper_abl}(a), the contrastive regularization improves the accuracy when the weight of the contrastive loss  is large enough ( is fixed to ). 
Although an excessive large value of  deteriorates the test accuracy by increased confirmation bias, the performance is robust to the selection of the .

\textcolor{black}{
Figure~\ref{fig:hyper_abl}(b) shows the effect of the consistency regularization on the SSL performance, where the weight of contrastive loss  is fixed to . 
The test accuracy decreases when the weight of the consistency regularization  increases, since the relative effect of our contrastive regularization decreases.
When the  becomes smaller than 1.0, the test accuracy is competitive with  and shows the effectiveness of our method.
}

\textcolor{black}{
Although the consistency regularization can completely be replaced with our contrastive regularization, we use the two regularizations to consistently achieve the state-of-the-art performance on different settings of the number of labeled samples.
In Table~\ref{tab:cs_cr}, we remove the consistency regularization and evaluate our contrastive regularization alone (CR-only, ) on CIFAR-10 with WRN-28-2.
When 4000 labels are available, CR-only still outperforms FixMatch, but the performance of CR-only is degraded when 40 and 250 labels are used.
Without the consistency regularization, the task-specific classification head is trained only by a supervised loss on labeled data  in Eq.~(\ref{eq:ssl_form}), and it can be easily overfitted when the number of labeled samples is few.
Thus, our method is complementary to the consistency regularization to maximize the SSL performance.
}

In Figure~\ref{fig:hyper_abl}(c), the accuracies of both FixMatch and FixMatch+CR decrease when the ratio of unlabeled samples is low.
This observation is consistent with the findings in UDA and FixMatch.
It indicates that both consistency and contrastive approaches require a sufficiently large number of unlabeled samples in a mini-batch for high SSL performance.



Figure~\ref{fig:hyper_abl}(d) shows that the confident threshold is related to the trade-off between the reliability of pseudo-labeling and the number of unlabeled samples leveraged.
The confidence thresholds of the consistency and contrastive regularizations are denoted as  in Eq.~(\ref{eq:consistency_regularization}) and  in Eq.~(\ref{eq:batch_CR}), respectively.
The low value of  worsens the performance of both FixMatch and FixMatch+CR because of the low reliability of pseudo-labeling.
Although the test accuracy of FixMatch with  drops to 68.74\%, only half of the training epochs are needed to achieve the performance, since it leverages all unlabeled samples regardless of the pseudo-labeling quality.
When  becomes low, FixMatch+CR suffers from the confirmation bias to some degree, but the test accuracy of FixMatch+CR with  still outperforms FixMatch, since our contrastive regularization effectively leverages unlabeled samples to improve the SSL performance.
When  keeps high value (0.95) and  becomes low, the performance of FixMatch+CR decreases, but the results with  still significantly outperforms FixMatch with .
The results imply that our contrastive regularization at the feature-level does not explicitly update the weights of the classifier, and therefore it shows robust results to the noisy pseudo-labels. 
However, as generating reliable pseudo-labels is still important to improve the SSL performance, we apply the selection mask with  in our method.
\textcolor{black}{Note that, different from the consistency regularization, our contrastive regularization in Eq.~(\ref{eq:gradient_cr_unconf}) can still leverage unlabeled samples with both confident and unconfident pseudo-labels, while keeping high reliability of pseudo-labeling by the confidence threshold.}

\begin{table} \centering
\footnotesize
\caption{Test accuracies (\%) on the numbers of views () per sample in  (CIFAR-100 with 10000 labels).}
\label{tab:abl_view}
\begin{tabular}{l|ccc}
\toprule
\# of Views  &  &  &  \\ \hline
FixMatch & 76.01 & 76.03 & 76.67 \\
FixMatch+CR & 76.08 & \textbf{78.27} & 77.83 \\
\bottomrule
\end{tabular}
\end{table}


\begin{table} \centering
\caption{Test accuracies (\%) of contrastive regularization without consistency regularization on CIFAR-10}
\label{tab:cs_cr}
\footnotesize
\begin{tabular}{l|ccc}
\toprule
CIFAR-10  & 40 labels & 250 labels & 4000 labels \\ \hline
FixMatch & 94.81 & 95.11 & 95.6 \\
CR  & 91.51 & 94.41 & 95.89 \\
FixMatch+CR & \textbf{95.32} & \textbf{95.39} & \textbf{95.92} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
\end{table}



\textbf{Effect of the Number of Views.} \label{sec:num_views} 
FixMatch and FixMatch+CR are compared with different numbers of augmented views  of .
Note that at least two views of each sample are required for FixMatch+CR to assure the existence of a positive sample in each mini-batch \cite{chen2020simple}.
Table~\ref{tab:abl_view} shows that the performance gain does not come from the solely increased number of views in our contrastive regularization.
The accuracy of FixMatch is not improved by two augmented views and does not outperform FixMatch+CR although three augmented views are used.
The results imply that the performance gain by our method does not depend on the increased number of views, but is from effectively leveraging more unlabeled data in SSL training shown in Figure~\ref{fig:emp_c100_w4}.

\textbf{Comparison with Unsupervised Contrastive Loss.}
We compare our method, which uses a contrastive loss with pseudo-labels for regularization, with the unsupervised contrastive loss (NT-Xent \cite{chen2020simple}).
As an auxiliary task, a self-supervised pretext task is known to improve the performance of semi-supervised learning.
Thus, we assume that unsupervised NT-Xent also improves the performance of semi-supervised learning, if the improvement by the contrastive regularization depends only on an auxiliary representation learning.
Table~\ref{tab:nt-xent} shows that the NT-Xent highly decreases the SSL performance. 
This might be due to that the task-agnostic instance discrimination tends to push away semantically similar instances in the same class.

\begin{table} \centering
\footnotesize
\caption{SSL performance of unsupervised contrastive learning as the SSL regularization.}
\label{tab:nt-xent}
\begin{tabular}{l|ccc}
\toprule
  & FixMatch & +NT-Xent & +CR \\ \hline
400 labels & 49.42 & 32.27 & \textbf{50.15} \\
2500 labels & 69.75&  54.44 & \textbf{72.00} \\
10000 labels & 76.15&  67.02 & \textbf{78.27} \\
\bottomrule
\end{tabular}
\vspace{-0.05in}
\end{table}

\section{Conclusion} \label{sec:conclusion}

Semi-supervised learning is important to learn a task with limited labels, while effectively leveraging unlabeled data in a task-specific way.
In this work, we show that the SSL training of the previous consistency regularization is biased on unlabeled samples only with confident pseudo-labeling by a selection mask.
Thus, we propose contrastive regularization that significantly improves SSL performance and can be used along with the consistency regularization by minimal change of implementation.
On various SSL benchmarks, the contrastive regularization not only improves the test accuracy but also significantly reduces the number of training iterations to achieve high performance.
Especially, our method shows more effective results on a dataset containing fewer labels or out-of-distribution samples.

In future work, 
\textcolor{black}{our approach can be applied to other SSL methods based on pseudo-labels~\cite{xie2020self,chen2020big} on large-scale datasets. However, our method still has a limitation on the memory efficiency for large-scale datasets due to large batch size, although the contrastive regularization improves training efficiency and the SSL performance.}
Thus, it is also worth exploration for leveraging large-scale unlabeled datasets in a task-specific way.
{


\section{Acknowledgements}
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation(IITP) grant funded by the Korea government(MSIT) (No.2018-0-01398, Development of a Conversational, Self-tuning DBMS).
\bibliography{aaai22}
}

\clearpage
\appendix
\onecolumn



\section{A. Contrastive Regularization for Semi-Supervised Learning}
In this section, we introduce the detailed derivation of the gradients of contrastive regularization.
For a model parameterized by , the label prediction of an unlabeled sample  is , where a mini-batch of unlabeled sample , -class weight matrix   and the penultimate features of  .
We assume that the model parameter  comprises the class weight matrix .
For a stochastic strong augmentation , we define the set of  strongly augmented samples in an unlabeled mini-batch as .
 is the pseudo-label of  and defined as , where  and sg is the stop gradient.
We define the set of \textit{pseudo}-positive pairs of  as , where  and  are the pseudo-label of  and , respectively.
Note that pseudo-labels of strongly augmented samples are defined by the label predictions on the samples \textit{before} the strong data augmentation to improve the reliability of pseudo-labeling.
Then, for an unlabeled sample  in an unlabeled mini-batch , contrastive regularization, , is defined as follows:


where a confidence threshold , a temperature scaling parameter, and a normalized vector of the projection head output .

Assuming that an augmented sample  has a confident pseudo-label, we first show that the contrastive regularization moves the features of  toward the centroid of the feature cluster having the same pseudo-label, while pushing away the features in different clusters.
Without the loss of generalizability, we assume  and .
In addition, we omit  for the notation brevity.
The first-order derivative of  with respect to a feature vector of  is as follows:

where  is the softmax score of the pair of  and , ,  is the original sample of strongly augmented .
Since the sum of minus log-sum-exp terms is the convex function, the first-order optimality condition holds when the  if  and  otherwise.
Thus, the contrastive regularization on an unlabeled sample pushes the features of different pseudo-labels and pulls those of the same pseudo-label.
Assuming that the softmax scores of the negative pairs are small enough, Eq.~(\ref{eq:supp_pos_neg}) is summarized as follows:

where  is a remainder term.
The feature vector of  is updated toward the centroid, which is the weighted sum of positive features regardless of the confidence of the pseudo-labels.

For another unlabeled sample  and , the contrastive regularization on features with confident pseudo-labels pushes the features of  if  has a different pseudo-label, and pulls them otherwise.
By the same process of the derivation of above , we can derive  as follows:




\section{B. Implementation Details}
For the implementation, we follow the setting of the original FixMatch paper \cite{sohn2020fixmatch} except the hyper-parameters related to the contrastive regularization.
We use Pytorch 1.6.0 to reproduce FixMatch and implement the contrastive regularization on the same codebase.
We conduct all experiments using four Tesla V100 32GB GPUs, except the ImageNet dataset.
For ImageNet, we use 32 V100 GPUs for FixMatch and 64 GPUs for FixMatch with the contrastive regularization.
For all datasets, we use the stochastic gradient descent (SGD) optimizer with Nesterov momentum , and temperature parameter .
We use an exponential moving average (EMA) of model parameters with 0.999 momentum and cosine learning scheduling used in \cite{sohn2020fixmatch}.
The batch size of labeled data () is 64 for SVHN, CIFAR-10, CIFAR-100, and STL-10 except SVHN with 20 and 40 labels, and CIFAR-10 with 20 labels.
Considering the small number of labeled samples, we use 16 labeled samples for training SVHN with 20 and 40 labels, and 32 labeled samples for CIFAR-10 with 20 labels per training iteration of WRN-28-2.
For WRN-28-8, we use 16 labeled samples for training SVHN with 20 and 40 labels, and 32 labeled samples for CIFAR-10 with 20 and 40 labels.

In the ablation study for different hyper-parameters, we use WRN-28-4 for CIFAR-100 with 2500 labels, because WRN-28-8 requires over two times more training time than WRN-28-4 but the accuracy gain is marginal (+0.83\% in Table~\ref{tab:abl_wf}).
We use random horizontal flipping and the random crop for both weak and strong augmentations of training datasets.
For the SVHN dataset, we do not use horizontal flipping, considering that a classifier can be easily confused to discriminate some classes such as eight and three.
For strong augmentations, we use RandAugment \cite{sohn2020fixmatch} following the original paper of FixMatch.
Please refer to the original paper of FixMatch and our source codes to check the details of augmentation policies according to the datasets.
The other training details are available in Table~\ref{tab:abl_setting}.


\begin{table}[]
\centering
\small
\caption{The details of hyper-parameters on training datasets. LR and WD describe initial learning rate and weight decay, respectively.}
\begin{tabular}{c|cccccccccc} \toprule
Dataset   & Model     &   &  & Epochs &  &  & LR & WD &  &  \\ \hline
SVHN      & WRN-28-2  & 16, 64   & 7     & 6500 & 1.0                 & 1.0                 & 0.03          & 0.0005       & 0.95     & 0.95      \\
CIFAR-10  & WRN-28-2  & 32, 64   & 7  & 6500   & 1.0                 & 1.0                 & 0.03          & 0.0005       & 0.95     & 0.95      \\
CIFAR-100 & WRN-28-8  & 64   & 7    & 2500  & 1.0                 & 10.0                & 0.03          & 0.001        & 0.95     & 0.95      \\
STL-10    & WRN-37-2  & 64   & 7 & 5000     & 1.0                 & 10.0                & 0.03          & 0.0005       & 0.95     & 0.95      \\
ImageNet  & ResNet-50 & 1024 & 5  & 300   & 10.0                & 1.0                 & 0.05          & 0.003       & 0.7      & 0.7      \\ \bottomrule
\end{tabular}
\label{tab:abl_setting}
\end{table}

\section{C. Additional Experimental Results}

\begin{wrapfigure}{r} {0.3\textwidth}
\centering
\includegraphics[width=0.3\textwidth]{FixMatch_treshold_c100_2500.pdf}
\caption{Test accuracy of FixMatch with different confident thresholds views over training.}
\label{fig:supp_thres}
\end{wrapfigure}


In this study, we have claimed that the previous consistency regularization suffers from the training inefficiency by the exclusion of unconfident pseudo-labels to ensure the reliability of pseudo-labeling.
Figure~\ref{fig:supp_thres} shows the relationship between the confident threshold  and the convergence speed of SSL training in FixMatch.
When low confidence thresholds are used such as 0.0 or 0.7, FixMatch can leverage more unlabeled samples than FixMatch with .
Thus, SSL training with low confidence thresholds can achieve the best performance much faster and increase the training efficiency with respect to the training iterations and time.
However, the low s lower the best test accuracies, because the unreliable pseudo-labeling propagates wrong labeling information to other unlabeled samples.
Meanwhile, when a high confidence threshold is used, the accuracy increases slower especially in the early stage of training, because it cannot leverage many unlabeled samples.
Despite the low training efficiency, FixMatch with  can achieve high performance, while leveraging only its confident labeling information.
The results imply that the training inefficiency of the previous consistency regularization comes from the trade-off between the reliability of pseudo-labeling and the number of used unlabeled samples in training.

\begin{wrapfigure}{r} {0.3\textwidth}
\centering
\includegraphics[width=0.3\textwidth]{view.pdf}
\caption{Test accuracy of FixMatch with different views over training.}
\label{fig:supp_view}
\end{wrapfigure}

In the ablation study, we have shown that the increased views () of unlabeled samples also increase the accuracy of FixMatch.
However, we show that more numbers of views cannot improve the training efficiency, and the accuracy over training still increases gradually in the training.
The results imply that the effect of our contrastive regularization does not result from the increased views of unlabeled samples, but well-clustered representations for SSL.

We show the additional results to show the more efficient training of contrastive regularization than consistency regularization.
For better visualization, we try to train FixMatch+CR in the same epochs of FixMatch, 10,500 epochs, until FixMatch+CR starts to be overfitted.
First, we show that the results described in Section 4.3 are also consistent with other datsets such as CIFAR-10 (Figure~\ref{fig:supp_c100}) and STL-10 (Figure~\ref{fig:supp_stl}).
Figure~\ref{fig:supp_c10_250} and ~\ref{fig:supp_c10_4000} show the test accuracy of EMA models, the average ratio of selection mask in a training mini-batch, and clustering scores of features in training on the CIFAR-10 dataset.
The results imply that our contrastive regularization learns well-clustered features for SSL and requires fewer iterations for high performance.
The accuracy of FixMatch gradually increases over the entire training iterations, but the accuracy of FixMatch+CR increases much faster than FixMatch, especially in the early stage of training.
Moreover, we find that about 10\% of iterations for FixMatch+CR are enough to achieve the performance of FixMatch.





\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{stl1000_5000.pdf}
\caption{Test accuracy of FixMatch and FixMatch+CR of WRN-37-2 trained on STL-10 with (a) 1000 and (b) 5000 labels.}
\label{fig:supp_stl}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{c10_250.pdf}
\caption{Empirical Results of FixMatch and FixMatch+CR with WRN-28-2 trained on the CIFAR-10 with 250 labels. (a) Test accuracy of EMA models, (b) the ratio of selection mask, and (c) Silhouette score of penultimate features of unlabeled samples based on pseudo-labels.}
\label{fig:supp_c10_250}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{c10_4000.pdf}
\caption{Empirical Results of FixMatch and FixMatch+CR with WRN-28-2 trained on the CIFAR-10 with 4000 labels. (a) Test accuracy of EMA models, (b) the ratio of selection mask, and (c) Silhouette score of penultimate features of unlabeled samples based on pseudo-labels.}
\label{fig:supp_c10_4000}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{c100_10000.pdf}
\caption{Empirical Results of FixMatch and FixMatch+CR with WRN-28-4 trained on the CIFAR-100 with 1000 labels. (a) Test accuracy of EMA models, (b) the ratio of selection mask, and (c) Silhouette score of penultimate features of unlabeled samples based on pseudo-labels.}
\label{fig:supp_c100}
\end{figure}

\end{document}
