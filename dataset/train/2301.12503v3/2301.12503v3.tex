

\documentclass{article}

\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{enumitem}
\usepackage{bbm}
\usepackage{subfigure}
\usepackage{float}
\usepackage{amsmath, amssymb, mathtools,amsthm}
\usepackage{enumitem}
\usepackage{url}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\vequalsignnospace}{\texttt{=}}
\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}
\def\arxiv{1}



\begin{document}

\twocolumn[


\icmltitle{AudioLDM: Text-to-Audio Generation with Latent Diffusion Models}







\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Haohe Liu}{equal,su}
\icmlauthor{Zehua Chen}{equal,im}
\icmlauthor{Yi Yuan}{su}
\icmlauthor{Xinhao Mei}{su} 
\icmlauthor{Xubo Liu}{su}
\icmlauthor{Danilo Mandic}{im} \\
\icmlauthor{Wenwu Wang}{su} 
\icmlauthor{Mark D. Plumbley}{su}

\end{icmlauthorlist}

\icmlaffiliation{su}{Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, UK}
\icmlaffiliation{im}{Department of Electrical and Electronic Engineering, Imperial College London, London, UK}
\icmlcorrespondingauthor{Haohe Liu}{haohe.liu@surrey.ac.uk}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 



\begin{abstract}



Text-to-audio (TTA) systems have recently gained attention for their ability to synthesize general audio based on text descriptions.
However, previous studies in TTA have limited generation quality with high computational costs.
In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn continuous audio representations from contrastive language-audio pretraining (CLAP) embeddings.
The pretrained CLAP models enable us to train LDMs with audio embeddings while providing text embeddings as the condition during sampling. 
By learning the latent representations of audio signals without modelling the cross-modal relationship, AudioLDM improves both generation quality and computational efficiency. 
Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance compared to other open-sourced systems, measured by both objective and subjective metrics. AudioLDM is also the first TTA system that enables various text-guided audio manipulations~(e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at \url{https://audioldm.github.io}.













\end{abstract}



\section{Introduction}
\label{Introduction}



Generating sound effects, music, or speech according to personalized requirements is important for applications such as augmented and virtual reality, game development, and video editing. Traditionally, audio generation has been achieved through signal processing techniques~\cite{andresen1979new, karplus1983digital}. In recent years, generative models~\cite{oord2016wavenet, DDPM, SGM, tan2022naturalspeech}, either unconditional or conditioned on other modalities~\cite{kreuk2022audiogen, zelaszczyk2022audio}, have revolutionized this task. Previous studies primarily worked on the label-to-sound setting with a small set of labels~\cite{liu2021conditional,pascual2022full} such as the ten sound classes in the UrbanSound8K dataset~\cite{salamon2014dataset}.
In comparison, natural language is considerably more flexible than labels as they can include fine-grained descriptions of audio signals, such as pitch, acoustic environment, and temporal order. 
The task of generating audio prompted with natural language descriptions is known as \textit{text-to-audio}~(TTA) generation.






TTA systems are designed to generate a wide range of high-dimensional audio signals. To efficiently model the data, we adopt a similar approach as DiffSound~\cite{yang2022diffsound} by employing a learned discrete representation to efficiently model high-dimensional audio signals. We also draw inspiration from the recent advancements in autoregressive modelling of discrete representation learnt on the waveform, such as AudioGen~\cite{kreuk2022audiogen}, which has surpassed the capabilities of DiffSound. Building on the success of StableDiffusion~\cite{rombach2022high}, which uses latent diffusion models (LDMs) for high-quality image generation, we extend previous TTA approaches to continuous latent representations, instead of learning discrete representations. Additionally, as audio manipulations, such as style transfer~\cite{engel2020ddsp, pascual2022full}, are desired for some applications such as games, 
we explore and achieve various zero-shot text-guided audio manipulations with LDMs, which have not been demonstrated before.







\begin{figure*}
    \centerline{
    \includegraphics[width=\linewidth]{pics/main_graph_v2.pdf}}
    \caption{Overview of the AudioLDM system for text-to-audio generation~(a). During training, latent diffusion models~(LDMs) are conditioned on an audio embedding  and trained in a continuous space  learned by VAE. The sampling process uses text embedding  as the condition. Given a pretrained LDM, zero-shot audio inpainting~(b) and style transfer~(c) are realized in the reverse diffusion process of LDM. The block \textit{Forward Diffusion} denotes the process that corrupt data with gaussian noise~(see Equation~\ref{forwardprocess}).}
    \label{fig:overalldesign}
\end{figure*}

For previous TTA works, a potential limitation for generation quality is the requirement of large-scale high-quality audio-text data pairs, which are usually not readily available, and where they are available, are  
of limited quality and quantity~\cite{liu2022separate}. To better utilize the low-quality data, several methods for text preprocessing have been proposed~\cite{kreuk2022audiogen, yang2022diffsound}. However, these preprocessing steps limit generation performances by overlooking the relations of sound events~
(e.g., \textit{a dog is barking at the bark} is transformed into \textit{dog bark park}).
By comparison, our proposed method only requires audio data for generative model training, circumvents the challenge of text preprocessing, and performs better than using audio-text paired data, as we will discuss later.  


















In this work, we present a TTA system, AudioLDM, which achieves high generation quality with continuous LDMs, with good computational efficiency and enables text-conditional audio manipulations.
The overview of AudioLDM design for TTA generation and text-guided audio manipulation is shown in Figure~\ref{fig:overalldesign}. 
Specifically, AudioLDM learns to generate the representation in a latent space encoded by a mel-spectrogram-based variational auto-encoder~(VAE). An LDM conditioned on a contrastive language-audio pretraining~(CLAP) embedding is developed for VAE latent generation. By leveraging the audio-text-aligned embedding space in CLAP, we remove the requirement for paired audio-text data during training LDM, as the condition for VAE latent generation can directly come from the audio itself. We demonstrate that training an LDM with audio only can be even better than training with audio-text data pairs. The proposed AudioLDM achieves leading TTA performance on the AudioCaps dataset with a Freshet distance~(FD) of , outperforming the DiffSound baseline~(FD of ) by a large margin. Our system also enables zero-shot audio manipulations in the sampling process. In summary, our contributions are as follows:
\begin{list}{\labelitemi}{\leftmargin=1em}
    \setlength{\itemsep}{1pt}
    \setlength{\parskip}{0pt}
    \item We demonstrate the first attempt to develop a continuous LDM for TTA generation. Our AudioLDM method outperforms existing methods in both subjective evaluation and objective metrics.
    \item We utilize CLAP embeddings to enable TTA generation without using language-audio pairs to train LDMs. 
    \item We experimentally show that using audio only data in LDM training can obtain a high-quality and computationally efficient TTA system. 
    \item We show that our proposed TTA system can perform text-guided audio styles manipulation, such as audio style transfer, super-resolution, and inpainting, without fine-tuning the model on a specific task.
\end{list}










\section{Related Work}
\label{Background}

\label{TTA}








\textbf{Text-to-Audio Generation}~has gained a lot of attention recently. Two works~\cite{yang2022diffsound,kreuk2022audiogen} explore how to learn audio representations in a discrete space given a natural language description, and then decode the representations to the audio waveform. Since both works require audio-text paired data for training the latent generation model, they have both proposed methods to address the issues of low quality and scarcity of paired data.

DiffSound~\cite{yang2022diffsound} consists of a text encoder, a decoder, a vector-quantized variational autoencoder~(VQ-VAE), and a vocoder. To alleviate the scarcity of audio-text paired data, they propose a mask-based text generation strategy~(MBTG) for generating text descriptions from audio labels. For example, the label \textit{dog bark, a man speaking} will be represented as \textit{[M] [M] dog bark [M] man speaking [M]}, where \textit{[M]} represent the mask token. However, the text generated by MBTG still only includes the label information, which might potentially limit model performance. 


AudioGen~\cite{kreuk2022audiogen} uses a Transformer-based decoder to learn to generate the target discrete tokens that are directly compressed from the waveform. AudioGen is trained on  datasets and proposes data augmentation methods to enhance the diversity of training data. When creating the language-audio pairs, they pre-process the language descriptions to labels to better match the class-label annotation distribution and simplify the task. For example, 
the text description \textit{a dog is barking at the park} is transformed to \textit{dog bark park}. For data augmentation, they mix audio samples according to various signal-to-noise ratios and concatenate the transformed language descriptions. This means that the detailed text descriptions showing the spatial and temporal relationships are discarded.



\textbf{Diffusion Models}~\cite{DDPM, SGM} have achieved state-of-the-art sample quality in tasks such as image generation~\cite{DiffusionBeatsGANs, DALLE2, Imagen}, image restoration~\cite{ISRIR}, speech generation~\cite{WaveGrad, DiffWave, leng2022binauralgrad}, and video generation~\cite{MakeAVideo, ImagenVideo}. For speech or audio synthesis, diffusion models have been studied for both mel-spectrogram generation~\cite{Grad-TTS, ResGrad} and waveform generation~\cite{BDDM, PriorGrad, InferGrad}. 

A major concern with diffusion models is that the iterative generation process in a high-dimensional data space will result in a low inference speed. One of the solutions is to employ diffusion models in a small latent space, an approach used, for example, in image generation~\cite{LSGM, D2C, rombach2022high}. For TTA generation, the audio waveform has redundant information~\cite{liu2022simple, liu2022learning} that increases modeling complexity and decreases inference speed. To overcome this, DiffSound~\cite{yang2022diffsound} uses text-conditional discrete diffusion models to generate discrete tokens as a compressed representation of mel-spectrograms. However, the quality of the sound generated by their method is limited. In addition, audio manipulation methods are not explored. 




\section{Text-Conditional Audio Generation}
\label{AudioLDM}





\subsection{Contrastive Language-Audio Pretraining}
\label{CLAP}
Text-to-image generation models have shown stunning sample quality by utilizing Contrastive Language-Image Pretraining (CLIP)~\cite{CLIP} for generating the image prior. Inspired by this, we leverage Contrastive Language-Audio Pretraining~(CLAP)~\cite{wu2022large} to facilitate TTA generation.   

We denote audio samples as  and the text description as . A text encoder  and an audio encoder  are used to extract a text embedding  and an audio embedding  respectively, where  is the dimension of CLAP embedding. A recent study \cite{wu2022large} has explored different architectures for both the text encoder and the audio encoder when training the CLAP model. We follow their result to build an audio encoder based on HTSAT~\cite{HTSAT}, and built a text encoder based on RoBERTa~\cite{RoBERTa}. We use a symmetric cross-entropy loss as the training objective~\cite{CLIP,wu2022large}. For details of the training process and the language-audio datasets see Appendix~\ref{app:CLAP}. 

After training the CLAP model, an audio sample  can be transformed into an embedding  within an aligned audio and text embedding space. The generalization ability of CLAP model has been demonstrated by various downstream tasks such as the zero-shot audio classification~\cite{wu2022large}. Then, for unseen language or audio samples, CLAP embeddings also provide cross-modal information.

\subsection{Conditional Latent Diffusion Models}
\label{CLDMs}

The TTA system can generate an audio sample  given text description . With probabilistic generative model LDMs, we estimate the true conditional data distribution  with a model distribution , where  is the prior of an audio sample  in the space formed from the compressed representation of the mel-spectrogram , and  is the text embedding obtained by pretrained text encoder  in CLAP. Here,  denotes the compression level,  denotes the channel of the compressed representation,  and  denote the time-frequency dimensions in the mel-spectrogram . With pretrained CLAP to jointly embed the audio and text information, the audio embedding  and the text embedding  share a joint cross-modal space. This allows us to provide  for training the LDMs, while providing  for TTA generation.

Diffusion models~\cite{DDPM, SGM} consist of two processes: i) a forward process to transform the data distribution into a standard Gaussian distribution with a predefined noise schedule , and ii) a reverse process to gradually generate data samples from the noise according to an inference noise schedule. 

In the forward process, at each time step , the transition probability is given by:
 
where  denotes injected noise,  is a reparameterization of  and  represents the noise level at each step. 
At the final time step ,  has a standard isotropic Gaussian distribution.
For model optimization, we employ the reweighted noise estimation training objective \cite{DDPM,DiffWave,rombach2022high}:

where  is the embedding of the audio waveform  produced by the pretrained audio encoder  in CLAP. In the reverse process, starting from Gaussian noise distribution  and the text embedding , a denoising process conditioned on  gradually generates the audio prior  by the following process:

The mean and variance are parameterized as~\cite{DDPM}: 

where  is the predicted generation noise, and . In the training stage, we learn the generation of an audio prior  given the cross-modal representation  of an audio sample . Then, in TTA generation, we provide the text embeddings  to predict the noise . Built on the CLAP embeddings, our LDM realizes TTA generation without text supervision in the training stage. We provide the details of network architecture in Appendix~\ref{app:LDMArchitecture}.

\subsection{Conditioning Augmentation}
\label{CA}

In text-to-image generation, diffusion-based models have demonstrated an ability to capture the fine-grained details between objects and backgrounds~\cite{DALLE2,Imagen,CompositionalDDPM}. One of the reasons for this success is the large-scale language-image training pairs, such as  million image-text pairs in the LAION dataset~\cite{schuhmann2021laion}. For TTA generation, it is also desired to generate compositional audio signals whose relationships are consistent with natural language descriptions. However, the scale of available language-audio datasets is not comparable to that of language-image datasets. For data augmentation, AudioGen~\cite{kreuk2022audiogen} use a mixup strategy which mixes pairs of audio samples and concatenates their respective processed text captions to form new paired data. In our work, as shown in Equation~\ref{trainingobjective}, we provide the audio only embedding  as conditioning information when training LDMs, we can implement data augmentation on audio only signals instead of needing to augment language-audio pairs. Specifically, we perform mixup augmentation on audio  and  by:  

where  is a scaling factor varying between  sampled from a Beta distribution ~\cite{gong2021psla}. Here we do not need to consider the corresponding text description , since text information is not needed during LDM training. By mixing audio pairs, we increase the number of training data pairs  for LDMs, which makes LDMs robust to CLAP embeddings. In the sampling process, given the text embedding  from unseen language descriptions, LDMs are expected to generate the corresponding audio prior .

\subsection{Classifier-free Guidance}
\label{CFG}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/guidance_scale_compare.pdf}
    \caption{The samples generated with different scales of the classifier-free guidance. The text prompt is \textit{``A cat is meowing"}.}
    \label{fig:my_label}
\end{figure}

For diffusion models, controllable generation can be achieved by introducing guidance at each sampling step. After classifier guidance~\cite{SGM,ImprovedDDPM}, classifier-free guidance~\cite{CFG,Glide} (CFG) has been the state-of-the-art technique for guiding diffusion models. During training, we randomly discard our condition  with a fixed probability, e.g.,  to train both the conditional LDMs  and the unconditional LDMs . In generation, we use text embedding  as condition and perform sampling with a modified noise estimation :

where  determines the guidance scale.
Compared with AudioGen~\cite{kreuk2022audiogen}, we have two differences. First, they leverage CFG on a transformer-based auto-regressive model, while our LDMs retain the theoretical formulation behind the CFG~\cite{CFG}. Second, our text embedding  is extracted from unprocessed natural language and therefore enables CFG to make use of the detailed text descriptions as guidance for audio generation. However, AudioGen removed the text details showing spatial or temporal relationships with text preprocessing methods. 

\subsection{Decoder}
\label{Decoder}

We use VAE to compress the mel-spectrogram  into a small latent space , where  is the compression level of the latent space. Our VAE is composed of an encoder and a decoder with stacked convolutional modules. In the training objective, we adopt a reconstruction loss, an adversarial loss, and a Gaussian constraint loss. We provide the detailed architecture and training methods in Appendix~\ref{app:VAE}. In the sampling process, the decoder is used to reconstruct the mel-spectrogram  from the audio prior  generated from LDMs. To explore a compression level  that achieves a small latent space for LDMs without sacrificing sample quality, we test a group of values , and take  as our default setting because of its high computational efficiency and generation quality. Moreover, as we conduct conditioning augmentation for LDMs, we implement data augmentation with Equation~\ref{mixup} for VAE as well in order to guarantee the reconstruction quality of generated compositional samples. For vocoder, we employ HiFi-GAN~\cite{kong2020hifi} to generate the audio sample  from the reconstructed mel-spectrogram . The training details are shown in Appendix~\ref{app:HiFi-GAN}.




\section{Text-Guided Audio Manipulation}
\label{AudioLDM}



\textbf{Style Transfer}
\label{AST}
Given a source audio sample , we can calculate its noisy latent representation  with a predefined time step  according to the forward process shown in Equation~\ref{forwardprocess}. By utilizing  as the starting point of the reverse process of a pretrained AudioLDM model, we enable the manipulation of audio  with text input  with a shallow reverse process :

where  controls the manipulation results. If we define a , the information provided by source audio will not be retained and the manipulation would be similar to TTA generation. We show the effect of  in Figure~\ref{fig:style-transfer-demo}, where larger manipulations can be seen in the setting of .

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{pics/Figure3_23Jan_1.pdf}
    \caption{The manipulation result with different starting points  of the shallow reverse process. The original signal is \textit{Trumpet}, and the text prompt for style transfer is \textit{Children Singing}.}
    \label{fig:style-transfer-demo}
\end{figure}

\textbf{Inpainting and Super-Resolution}
\label{AI}
Both audio inpainting and audio super-resolution refer to generating the missing part given the observed part . We explore these tasks by incorporating the observed part in latent representation  into the generated latent representation . Specifically, in reverse process, starting from , after each inference step shown in Equation~\ref{singlereversestep}, we modify the generated  with:

where  is the modified latent representation,  denotes an observation mask in latent space,  is obtained by adding noise on  with the forward process shown by Equation~\ref{forwardprocess}. 

The values of observation mask  depend on the observed part of a mel-spectrogram . As we adopt a convolutional structure in VAE to learn the latent representation , we can roughly retain the spatial correspondency in mel-spectrogram, as it is shown in Figure~\ref{fig:demo-spatial-correspondancy} in Appendix~\ref{app:VAE}. Therefore, if a time-frequency bin  is observed, we set the observation mask  in latent space as .
By using  to denote the generation part and observation part in , according to Equation~\ref{restoration}, we can generate the missing information conditioned on the text prompt with TTA models, while retaining the ground-truth observation .



\begin{table*}[tbp]
\centering
\scriptsize
\begin{tabular}{ccccc|cccc|cc}
\toprule
    Model    & Text Data & Use CLAP & Params & Duration~(h) & FD~  & IS~   & KL~ & FAD~ & OVL~ & REL~   \\
\midrule
Ground truth & - & - & - & - & - & - & - & - &  &  \\
DiffSound~\cite{yang2022diffsound}   & \cmark & \xmark           & M  &  &  &  &  &  &  &  \\
AudioGen~\cite{kreuk2022audiogen}      & \cmark & \xmark &M &   & -    &  -    &   &  & - & - \\
\midrule
AudioLDM-S-Full-RoBERTa   & \cmark   & 
\xmark      & M &   &   &  &  &  & - & - \\
AudioLDM-S    & \xmark  & 
\cmark       & M &   &   &  &  &  &  &  \\
AudioLDM-L    & \xmark   & 
\cmark     & M &  &   &  &  &  
 &  & \\
AudioLDM-S-Full & \xmark & 
\cmark   & M &  &   &  &  &  & - & - \\
AudioLDM-L-Full & \xmark & 
\cmark  & M &  &   &  &  &  &  &  \\
\bottomrule
\end{tabular}
\caption{The comparison between AudioLDM and baseline TTA generation models. Evaluation is conducted on AudioCaps test set. The symbol  marks industry-level computation. DiffSound is trained on  V GPUs and AudioGen is trained on  A GPUs, while AudioLDM models are trained on a single GPU, RTX  or A. The AS and AC stand for AudioSet and AudioCaps datasets respectively. The results of AudioGen are employed from ~\citep{kreuk2022audiogen} since their implementation has been not publicly available.}
\label{tab: AudioCapResults}
\end{table*}

\section{Experiments}
\label{Experiments}


\textbf{Training dataset} The datasets we used in this paper includes AudioSet~(AS)~\cite{gemmeke2017audio}, AudioCaps~(AC)~\cite{kim2019audiocaps}, Freesound~(FS)\footnote{\url{https://freesound.org/}}, and BBC Sound Effect library~(SFX)\footnote{\url{https://sound-effects.bbcrewind.co.uk/search}}. AS is currently the largest audio dataset, with  labels and over  hours of audio data. AC is a much smaller dataset with around  audio clips and text descriptions. Most of the data in AudioSet and AudioCaps are in-the-wild audio from YouTube, so the quality of the audio is not guaranteed. To expand the dataset, especially with high-quality audio data, we crawl the data from the FreeSound and BBC SFX datasets, which have a wide range of categories such as music, speech, and sound effects. We show our detailed data processing methods and training configuration in Appendix~\ref{app:TrainingDetails}.

\textbf{Evaluation dataset} We evaluate the model on both AC and AS. Each audio clip in AC has  text captions. We generate the evaluation set by randomly selecting one of them as text condition. Because the authors of AC intentionally remove the audio with the label related to music~\cite{kim2019audiocaps}, to evaluate model performance with a wider range of sound, we randomly select  audio samples from AS as another evaluation set. Since AS does not contain text descriptions, we use the concatenation of labels as text descriptions, such as \textit{Speech, hip hop music, and crowd cheering}. 





\textbf{Evaluation methods} We perform both objective evaluation and human subjective evaluation. The main metrics we use for objective evaluation include \textbf{frechet distance~(FD)}, \textbf{inception score~(IS)}, and \textbf{kullback–leibler~(KL) divergence}. Similar to the frechet inception distance in image generation, the FD in audio indicates the similarity between generated samples and target samples. IS is effective in evaluating both sample quality and diversity. KL is measured at a paired sample level and averaged as the final result. All of these three metrics are built upon a state-of-the-art audio classifier PANNs~\cite{kong2020panns}. To compare with~\cite{kreuk2022audiogen}, we also adopt the frechet audio distance~(FAD)~\cite{kilgour2019frechet}. FAD has a similar idea to FD but it uses VGGish~\cite{vggish_hershey2017cnn} as a classifier which may have inferior performance than PANNs. To better measure the generation quality, we choose FD as the main evaluation metric in this paper. For subjective evaluation, we recruit six audio professionals to carry on a rating process following~\cite{kreuk2022audiogen,yang2022diffsound}. Specifically, the generated samples are rated based on i) \textbf{overall quality~(OVL)}; and ii) \textbf{relevance to the input text~(REL)} between a scale of  to . We include the details of human evaluation in Appendix~\ref{app:TrainingDetails}. We open-source our evaluation pipeline to facilitate reproducibility\footnote{\url{https://github.com/haoheliu/audioldm_eval}}.








\textbf{Models} We employ two recently proposed TTA systems, DiffSound~\cite{yang2022diffsound} and AudioGen~\cite{kreuk2022audiogen} as our baseline models. DiffSound is trained on AS and AC datasets with around M parameters. AudioGen is trained on AS, AC, and eight other datasets with around M parameters. Since AudioGen has not released publicly available implementation, we reuse the KL and FAD results reported in their paper. We train two AudioLDM models. One is a small model named AudioLDM-S, which has M parameters, and the other is a large model named AudioLDM-L with M parameters. We describe the details of UNet architecture in Appendix~\ref{app:LDMArchitecture}. To demonstrate the advantage of our method, we simply train these two models only with the AC dataset. Moreover, to explore the effect of the scale of training data, we develop an AudioLDM-L-Full model which is trained on AC, AS, FreeSound, and BBC SFX datasets. 





\subsection{Results}

We show the main evaluation results on the AC test set in Table~\ref{tab: AudioCapResults}. Given the single training dataset AC, AudioLDM-S can achieve better generation results than the baseline models on both objective and subjective evaluations, even with smaller model size. By expanding model capacity with AudioLDM-L, we further improve the overall results. Then, by incorporating AS and the two other datasets into training, our model AudioLDM-L-Full achieves the best quality, with an FD of . 
Although RoBERTa and CLAP have the same text encoder structure, CLAP has an advantage in that it decouples audio-text relationship learning from generative model training. This decoupling is intuitive as CLAP has already modelled the relationship between audio and text by aligning their embedding spaces. On the other hand, AudioLDM-S-Full-RoBERTa, in which the text encoder only represents textual information, requires the model to learn the text-audio relationships while simultaneously learning the audio generation process. Additionally, our CLAP-based method allows for model training using audio-only data. Therefore, using Roberta without pretraining with CLAP may increase the difficulty of training.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/hist.png}
\caption{The histogram of the human evaluation result. The horizontal axis and vertical axis represent the rating score and frequency, respectively. \textit{OVL} denotes the overall quality of audio files and \textit{REL} denotes the relation between text and generated audio. Both OVL and REL are scored on a scale of  to . Scores on each evaluation file are averaged among all the raters.}
    \label{fig:hist-human-evaluation}
\end{figure*}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/training_curve.pdf}
    \caption{The comparison of various evaluation metrics evaluated in the training process of i) AudioLDM-S trained with text embedding (S-Text+Audio) ii) AudioLDM-S (S-Audio), and iii) AudioLDM-L (L-Audio).}
    \label{fig:training-steps-fd-is-kl}
\end{figure}

Our human evaluation shows a similar trend as other evaluation metrics. Our proposed methods have OVL and REL of around , outperforming DiffSound with OVL of  and REL of  by a large margin. On the AudioLDM model size, we notice that the larger model is advantageous for the overall audio qualities. After scaling up the training data, both OVL and REL show significant improvements.
Figure~\ref{fig:hist-human-evaluation} shows the score statistic of different models averaged between all the raters. We notice our model is more concentrated on the higher scores compared with DiffSound. Our spam cases, which are randomly selected real recordings, show high scores, indicating the rating result is reliable.



To perform the evaluation on audio data that could include music, we further evaluate our model on the AS evaluation set. We compare our method with DiffSound and show the results in Table~\ref{tab: AudioSetResults}. Our three AudioLDM models show a similar trend as they perform on the AC test set. We can outperform the DiffSound baseline by a large margin on all the metrics. 

\begin{table}[htbp]
\small
\centering
\begin{tabular}{ccccc}
\toprule
       Model            & FD~   & IS~   & KL~  \\
\midrule
DiffSound          &  &   &  \\
AudioLDM-S      &  &  &   \\
AudioLDM-L      &  &  &  \\
AudioLDM-L-Full &  &  &  \\
\bottomrule
\end{tabular}
\caption{The evaluation results on the AudioSet evaluation set.}
\label{tab: AudioSetResults}
\end{table}



\textbf{Conditioning Information}
As we train LDMs conditioned on the audio embedding  but provide the text embedding  to LDMs in TTA generation, a natural concern is that if stronger results could be achieved by directly using the text embedding as training condition. We conduct experiments and show the results in Table~\ref{tab: conditioning-text-audio}. For a fair comparison, we also conduct data augmentation and we adopt the strategy from AudioGen. Specifically, we use the same mixing method for audio pairs shown in Section~\ref{CA}, and concatenate two text captions as conditioning information. Table~\ref{tab: conditioning-text-audio} shows by training LDMs on , we can achieve better results than training with . 

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{cccccc}
\toprule
    Model     & Text & Audio  & FD~   & IS~   & KL~  \\
\midrule
AudioLDM-S & \cmark & \cmark &  &  &  \\
AudioLDM-S & \xmark & \cmark &   &  &  \\
AudioLDM-S-Full & \cmark & \cmark &  &  &  \\
AudioLDM-S-Full & \xmark & \cmark &  &  &  \\
AudioLDM-L-Full & \cmark & \cmark &  &  &  \\
AudioLDM-L-Full & \xmark & \cmark &  &  &  \\
\bottomrule
\end{tabular}
\caption{The comparison between text embedding and audio embedding as conditioning information on the training of LDMs.}
\label{tab: conditioning-text-audio}
\end{table}


We believe the primary reason for the result in Table~\ref{tab: conditioning-text-audio} is that text embedding cannot represent the generation target as good as audio embedding. 
Firstly, due to the ambiguity and complexity of sound, the text caption is difficult to be accurate and comprehensive. Different human annotators may have different perceptions and descriptions over the same audio, which make training with text-audio pair less stable than with audio only.
Moreover, some of the captions are at a highly-abstracted level and cannot correctly describe the audio content. For example, there is an audio in the BBC SFX dataset with caption \textit{Boats: Battleships-5.25 conveyor space}, which is even difficult for humans to imagine how it sounds. This quality of language-audio pairs may hinder model optimization. 
By comparison, if we use  from CLAP latents as a condition, it is extracted directly from the audio signal and is aligned with ideally the best text caption, which enables us to provide strong conditioning information to LDMs without considering the noisy labeled text description.
Figure~\ref{fig:training-steps-fd-is-kl} shows sample quality as a function of training progress. We notice that i) training with audio embedding can lead to significantly better results than text embedding throughout the entire training process; and ii) larger models may converge more slowly but can achieve better final performance.



\textbf{Compression Level} We study the effect of compression level  on generation quality. Table~\ref{tab: compressionratio} shows the performance comparison with . We observe a decreasing trend with the increase of compression levels. Nevertheless, in the setting of  where we compress the -band mel-spectrogram into only  dimensions in the frequency axis, our performance is still on par with AudioGen on KL, and better than DiffSound on all the metrics.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{ccccc}
\toprule
Model         &  & FD~   & IS~   & KL~   \\
\midrule
AudioLDM-S &           &  &  &  \\
AudioLDM-S &           &   &  &  \\
AudioLDM-S &          &   &  &  \\
\bottomrule
\end{tabular}
\caption{The effect of the compression level on AudioLDM.}
\label{tab: compressionratio}
\end{table}

If we set the compression level as , which means we directly generate mel-spectrogram from CLAP embeddings, the training process is difficult to implement on a single RTX  GPU. Similar results happen on . Moreover, the inference speed will be low with . In our studies,  achieves high generation quality while reducing the computational load to a reasonable level. Hence, we use it as the default setting in our experiments.

\textbf{Text-Guided Audio Manipulation} We show the performance of our text-guided audio manipulation methods on two tasks: super-resolution and inpainting. Specifically, for super-resolution, we upsample the audio signal from ~kHz to ~kHz sampling rate. For the inpainting task, we remove the audio signal between  and  seconds and refill this part by inpainting. Since most studies on audio super-resolution work on speech signal~\cite{liu2021voicefixer, liu2022neural}, we demonstrate our results on both AudioCaps, and a speech dataset VCTK~\cite{vctk-yamagishi2019cstr}, which is a multi-speaker speech dataset. For super-resolution, we employ two models AudioUNet~\cite{kuleshov2017audio} and NVSR~\cite{liu2022neural} as baseline models, and employ log-spectral distance (LSD)~\cite{heming-towards-sr-wang2021towards} as the evaluation metric for comparison. For the inpainting task, we use FAD as a metric and establish a baseline for this task. 
\begin{table}[tbp]
\centering
\small
\begin{tabular}{cccc}
\toprule
Task          & \multicolumn{2}{c}{Super-resolution}  &  Inpainting     \\
\midrule
Dataset       & AudioCaps            & VCTK                 & AudioCaps            \\
\midrule
Unprocessed   &                      &                      &                      \\
\citet{kuleshov2017audio}     & -                    &                  & -                    \\
\citet{liu2022neural}          & -                    &                  & -                    \\
AudioLDM-S &                      &                      &                      \\
AudioLDM-L &  &  &  \\
\bottomrule
\end{tabular}
\caption{Performance comparisons on zero-shot super-resolution and inpainting, which are evaluated by LSD and FAD, respectively.}
\label{tab: audiomanipulation}
\end{table}
Table~\ref{tab: audiomanipulation} shows that AudioLDM can outperform the strong AudioUNet baseline, but the result is not as good as NVSR~\cite{liu2022neural}. 
Recall that AudioLDM is a model trained on a diverse set of audio signals, including those with heavy background noise. This can lead to the presence of white noise or other non-speech sound events in the output of our super-resolution process, potentially reducing performance. 
Nevertheless, our contribution could open the door to achieving text-guided audio manipulation with the TTA system in a zero-shot way. Further improvements could be expected based on our benchmark results. We provide several samples of our results in Appendix~\ref{app:demos}. 

\subsection{Ablation Study}

Table~\ref{tab: ablationstudy} shows the result of our ablation study on AudioLDM-S. By simplifying the attention mechanism in UNet into a one-layer multi-head self-attention~(\textit{w.~Simple attn}), the performance in each metric will have a notable decrease, which indicates complex attention mechanism is preferred. Also, we notice the widely used balanced sampling strategy~\cite{gong2021psla, liu2022ontology} in audio classification does not show improvement in TTA~(\textit{w.~Balance samp}). Conditional augmentation~(see Section~\ref{CA}) shows improvement in the subjective evaluation, but it does not show improvement in the objective evaluation metrics~(\textit{w.~Cond aug}). 
The reason could be that conditioning augmentation generates training data that is not representative of the AudioCaps dataset, resulting in model outputs that are not well-aligned with the evaluation data, ultimately leading to lower metric scores. Nevertheless, conditioning augmentation can improve two subjective metrics and we still recommend using it as a data augmentation technique.

\begin{table}[htbp]
\small
\centering
\begin{tabular}{lccc|cc}
\toprule
       Setting            & FD   & IS   & KL  & OVL~ & REL~ \\
\midrule
AudioLDM-S      &  &  &  &  &  \\
\textit{w.~Simple attn}             &  &  &  & - & - \\
\textit{w.~Balance samp} &       &      &     & - & - \\
\textit{w.~Cond aug}           &  &  &  &  &   \\
\bottomrule
\end{tabular}
\caption{The ablation study on the attention mechanism, the balance sampling technique for training data, and the conditioning augmentation algorithm.}
\label{tab: ablationstudy}
\end{table}

\textbf{DDIM Sampling Step} The number of inference steps in the reverse process of DDPMs can directly affect the generation quality \cite{DDPM,SGM}. Generally, the sample quality can be improved with an increase in the number of sampling steps and computational load at the same time. We explore the effect of the DDIM~\cite{song2020denoising} sampling steps on our latent diffusion model. Table~\ref{tab: DDIMsampling} shows that more sampling steps lead to better quality. With enough sampling steps such as , the gain of adding sampling steps becomes less significant. The result of  steps is only slightly better than that of  steps.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{pics/guidance_scale.png}
    \caption{The comparison between different classifier-free guidance scales (on the horizontal axis) for the AudioLDM-S model trained on AudioCaps.}
    \label{fig:classifier-free-guidance}
    \vspace{-2mm}
\end{figure}

\begin{table}[tbp]
\centering
\small
\begin{tabular}{cccccc}
\toprule
DDIM steps    &     &       &      &      &    \\
\midrule
FD    &       &         &        &         &       \\
IS    &       &         &        &         & 
       \\ KL    &       &         &        &         &       \\
\bottomrule
\end{tabular}
\caption{Effect of sampling steps of LDMs with a DDIM sampler.}
\label{tab: DDIMsampling}
\vspace{-6mm}
\end{table}

\textbf{Guidance Scale} 
represents a trade-off between conditional generation quality and sample diversity. A suitable guidance scale can improve the consistency between generated samples and conditioning information at an acceptable cost of generation diversity. We show the effect of guidance scale  on TTA in Figure~\ref{fig:classifier-free-guidance}. When , we achieve the best results in both FD and KL, but not in FAD. We suppose the reason is the audio classifier in FAD is not as good as FD, as mentioned in Section~\ref{Experiments}. In this case, the improvement in the adherence to detailed language description may become misleading information to the classifier in FAD. Considering previous studies report FAD results instead of FD, we set  for comparison, but also provide detailed effects of  on FAD, FD, IS, and KL, respectively. 

\textbf{Case Study}~We conduct case study and show the generated results in Appendix~\ref{app:demos}, including style transfer~(see Figure~\ref{fig:demo-style-transfer-1}-\ref{fig:demo-style-transfer-3}), super-resolution~(see Figure~\ref{fig:demo-super-resolution}), inpainting~(see Figure~\ref{fig:demo-inpainting}-\ref{fig:demo-inpainting-prompt}), and text-to-audio generation~(see Figure~\ref{fig:demo-control-speech}-\ref{fig:demo-audioset-music}). Specifically, for text-to-audio, we demonstrate the controllability of AudioLDM, including the control of the acoustic environment, material, sound event, pitch, musical genres, and temporal orders.

\section{Conclusions}
\label{Conclusion}
We have presented a new method AudioLDM for text-to-audio~(TTA) generation, with contrastive language-audio pretraining~(CLAP) models and latent diffusion models~(LDMs). Our method is advantageous in generation quality, computational efficiency, and audio manipulations. With a single training dataset AudioCaps and a single GPU, AudioLDM achieves SOTA generation quality evaluated by both subjective and objective metrics. Moreover, AudioLDM enables zero-shot text-guided audio style transfer, super-resolution, and inpainting.

\section{Acknowledgement}
\label{sec:ack}
We would like to thank James King and Jinhua Liang for the useful discussion on the latent diffusion model. This research was partly supported by the British Broadcasting Corporation Research and Development~(BBC R\&D), Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 ``AI for Sound'', and a PhD scholarship from the Centre for Vision, Speech and Signal Processing (CVSSP), Faculty of Engineering and Physical Science (FEPS), University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising.

\bibliography{maincontent}
\bibliographystyle{icml2021}

\newpage
\onecolumn
\section*{Appendix}
\label{sec:appendix}



\renewcommand{\thesubsection}{\Alph{subsection}}




\subsection{Contrastive Language-Audio Pretraining}
\label{app:CLAP}



We follow the pipeline of the contrastive language-audio pretraining (CLAP) models proposed by \cite{wu2022large} to capture the similarity between text and audio, and project them into joint latent space. The training dataset includes the currently largest public dataset LAION-Audio-K, the AudioSet dataset whose text caption is augmented with keyword-to-caption\footnote{\url{https://github.com/gagan3012/keytotext}} by T model~\cite{T5}, the AudioCaps dataset and the Clotho dataset~\cite{Clotho}. The LAION-Audio-K dataset contains  language-audio pairs and  hours of audio samples. The AudioSet dataset contains  pairs and  hours of audio samples. The AudioCaps dataset contains  pairs and  hours of audio samples. The Clotho dataset contains  pairs and  hours of audio samples. These datasets contain various natural sounds, audio effects, music and human activity.

Given the audio sample  and the text data , we use an audio encoder and a text encoder to extract their embedding  and  respectively, where  is set as . We build the audio encoder based on HTSAT~\cite{HTSAT} and the text encoder based on RoBERTa~\cite{RoBERTa}. The symmetric cross-entropy loss used to train these contrastive encoders is:

where  is a learnable temperature parameter and  is the batch size. 

\subsection{Latent Diffusion Model}
\label{app:LDMArchitecture}

We adopt the UNet backbone of StableDiffusion~\cite{rombach2022high} as the basic architecture of LDM for AudioLDM. As shown in Equation~\ref{singlereversestep}, the UNet model is conditioned on both the time step  and the CLAP embedding . We map the time step into a one-dimensional embedding and then concatenate it with  as conditioning information. Since our condition vector is only one-dimensional, we do not use the cross-attention mechanism in StableDiffusion for conditioning. Instead, we directly use the feature-wise linear modulation layer~\cite{perez2018film} to merge conditioning information with the feature map of the UNet convolution block. The UNet backbone we use has four encoder blocks, a middle block, and four decoder blocks. With a basic channel number of , the channel dimensions of encoder blocks are . The channel dimensions of decoder blocks are the reverse of encoder blocks, and the channel of the middle block has  dimensions. We add an attention block in the last three encoder blocks and the first three decoder blocks. Specifically, we add two multi-head self-attention layers with a fully-connected layer in the middle as the attention block. The number of heads is determined by dividing the embedding dimension of the attention block with a parameter . We set AudioLDM-S and AudioLDM-L with , and , respectively. In the forward process, we use  steps. A linear noise schedule from  to  is used. In sampling, we employ the DDIM~\cite{song2020denoising} sampler with  sampling steps. For classifier-free guidance, a guidance scale  of  is used in Equation \ref{mixup}.


\subsection{Variational Autoencoder}
\label{app:VAE}

We compress the mel-spectrogram  of  into a small continuous space  with a convolutional VAE, where  and  is the time and frequency dimension size respectively,  is the channel number of the latent encoding, and  is the compression level (downsampling ratio) of latent space. Both the encoder  and the decoder  are composed of stacked convolutional modules. In this way, VAE encoder could preserve the spatial correspondancy between mel-spectrogram and latent space, as it is shown in Figure~\ref{fig:demo-spatial-correspondancy}. Each module is formed by ResNet blocks~\cite{kong2021decoupling} which are made up of convolutional layers and residual connections. The encoding  will be evenly split into two parts,  and , with shape , representing the mean and variance of the VAE latent space. The input of the decoder is a stochastic encoding . During generation, the decoder will be used to reconstruct the mel-spectrogram given the generated latent representations.


We employ three loss functions in our training objective: the mel-spectrogram reconstruction loss, adversarial losses, and a gaussian constraint loss. The reconstruction loss calculates the mean absolute error between the input sample  and the reconstructed mel-spectrogram . The adversarial losses are employed to enhance the reconstruction quality. Specifically, we adopt the PatchGAN~\citep{isola2017image} as our discriminator, which will divide the input image into small patches and predict whether each patch is real or fake by outputting a matrix of logits. 
The PatchGAN discriminator is trained to maximize the logits of correctly identifying real patches while minimizing the logits of incorrectly identifying fake patches. We also apply the gaussian constraint on the latent space of VAE. By enforcing a gaussian constraint on the latent space, the VAE is encouraged to learn a continuous, structured latent space, rather than a disorganized one. This can help the VAE to better capture the underlying structure of the data, which can result in more stabilized and accurate reconstructions~\cite{kingma2013auto}. 


We train our VAE using the Adam optimizer~\cite{kingma2014adam} with a learning rate of  and a batch size of six. The audio data we use includes AudioSet, AudioCaps, Freesound, and BBC SFX. We perform experiments with three compression-level settings , for which the latent channels are , respectively.  VAEs in all three settings are trained with at least M steps on a single NVIDIA RTX 3090 GPU. To stabilize training, we do not apply the adversarial loss in the first K training steps. We apply the mixup~\cite{kong2020panns} strategy for data augmentation. 












Table~\ref{tab: vae-reconstruct} shows the reconstruction performance of our VAE model with different values of . All three settings achieve comparable metrics score with the \textit{GT Mel + Vocoder} setting, indicating the autoencoder can perform reliable mel-spectrogram encoding and decoding.



\begin{table}[htbp]
\small
\centering
\begin{tabular}{cccccc}
\toprule
        Setting        & PSNR  & SSIM & FD   & IS   & KL   \\
\midrule
GT Mel + Vocoder &  &  &  &  &  \\
\midrule
Compression     &  &  &  &  &  \\
Compression     &  &  &  &  &  \\
Compression    &  &  &  &  &  \\
\bottomrule
\end{tabular}
\caption{The objective metrics of VAE reconstruction performance with different compression level  on the AudioSet evaluation set.}
\label{tab: vae-reconstruct}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{pics/spatial_correspondancy.pdf}
    \caption{Visualization of the time-frequency-masked spectrogram and their corresponding VAE latent. This figure shows VAE encoder roughly preserves the spatial correspondancy between the spectrogram and the latent.}
    \label{fig:demo-spatial-correspondancy}
\end{figure}

\subsection{Vocoder}
\label{app:HiFi-GAN}

In this work, we employ HiFi-GAN~\cite{kong2020hifi} as a vocoder, which is widely used for speech waveform generation. It contains two sets of discriminators, a multi-period discriminator, and a multi-scale discriminator, to enhance the perceptual quality. To synthesize the audio waveform, we train it on the AudioSet dataset. For the input samples at the sampling rate of Hz, we extract  bands mel-spectrogram. Then we follow the default settings of HiFi-GAN V. The window, FFT, and hop size are set to , , and . The  and  are set as  and . We use the AdamW optimizer with  and . The learning rate starts from  and a learning rate decay of  is used. We use a batch size of  and train the model with  NVIDIA  GPUs. We release this pretrained vocoder in our open-source implementation.

\subsection{Experiment Details}
\label{app:TrainingDetails}

\textbf{Data Processing} The duration of the audio samples in AudioSet and AudioCaps is  seconds, while it is much longer in FreeSound and BBC SFX datasets. To avoid overusing the data from long audio, which usually have repeated sound, we only use the first thirty seconds of the audio in both the FreeSound and BBC SFX datasets and segment them into ten-second long audio files. Finally, we have in total  ten-seconds audio samples for model training. It should be noted that even if some datasets, e.g., AudioCaps and BBC SFX, have text captions for the audio, we do not utilize them during the training of LDMs. We only use the audio samples for training. We resample all the datasets into kHz sampling rate and mono format, and all samples are padded to  seconds. 

\textbf{Configuration} For each LDM model, we use the compression level  as the default setting. Then, we train AudioLDM-S and AudioLDM-L for M steps on a \textbf{single GPU}, NVIDIA RTX , with the batch size of  and , respectively. The learning rate is set as . The AudioLDM-L-Full is trained for M steps on one NVIDIA A with a batch size of . The learning rate is . For better performance on AudioCaps, we further fine-tune AudioLDM-L-Full on AudioCaps for M steps before evaluation. It should be noted that we limit our batch size because of the scarcity of GPU. However, this potentially restricts the performance of AudioLDM models. In comparison, DiffSound uses  NVIDIA V GPUs for model training with a batch size of  on each GPU. AudioGen utilizes  A GPUs with a batch size of . 

\textbf{Human evaluation} We construct the  dataset for human subjective evaluation with  randomly selected samples where  audios are from AudioCaps,  audios are from AudioSet, and  randomly selected real recordings, which we will refer to as spam cases. Therefore, each model should generate  audio samples given the corresponding text descriptions. We gather the output from models in one folder and anonymize them with random identifiers. An example questionnaire is shown in Table~\ref{tab:questionniare}. The participant will need to fill in the last two columns for each audio file given the text description. Our final result shows that all the human raters have an average score above  on the spam cases. Hence, their evaluation result is considered reliable.

\begin{table}[htbp]
\scriptsize
\centering
\begin{tabular}{cccc}
\toprule
File name &
  Text description &
  Overall impression (1-100) &
  Relation to the text description   (1-100) \\
\midrule
random\_name\_108029.wav &
  A man talking followed by lights scrapping on a wooden surface &
  80 &
  90 \\
\midrule
random\_name\_108436.wav & Bicycle Music Skateboard   Vehicle          & 70 & 80 \\
\midrule
random\_name\_116883.wav & A power tool drilling as rock   music plays & 90 & 95 \\
\midrule
... & ... & ... & ... \\
\bottomrule
\end{tabular}
\caption{Example questionnaire for human evaluation. The participant will need to fill in the last two columns.}
\label{tab:questionniare}
\end{table}

\subsection{The Effect of Finetuning}

\begin{table}[tbp]
\centering
\small
\begin{tabular}{cccc|cccc}
\toprule
    Model    & Text Data & Use CLAP  & Finetuned & FD~  & IS~   & KL~ & FAD~   \\
\midrule
AudioLDM-S-Full-Roberta   & \cmark   & \xmark      & \xmark  &   &  &  &  \\
AudioLDM-S-Full-Roberta   & \cmark   & \xmark      & \cmark  &   &  &  &  \\
AudioLDM-S-Full & \xmark & \cmark  & \xmark &   &  &  &  \\
AudioLDM-S-Full & \xmark & \cmark  & \cmark &   &  &  &  \\
AudioLDM-L-Full & \xmark & \cmark  & \xmark &   &  &  &   \\
AudioLDM-L-Full & \xmark & \cmark  & \cmark &   &  &  &   \\
\bottomrule
\end{tabular}
\caption{The comparison between fine-tuned and non-finetuned models on the AudioCaps evaluation set.}
\label{tab: finetune-non-finetune-comparison}
\end{table}

Table~\ref{tab: finetune-non-finetune-comparison} compares the results obtained with and without fine-tuning on the evaluation set. We observe an improvement in various evaluation metrics, which is expected since the training set of AudioCaps has a distribution that is similar to the evaluation set. However, it is important to note that higher performance on the limited distribution of the evaluation set may not necessarily indicate better performance overall. A model that can generate broader distributions of audio may perform worse on the evaluation set, even though it may have better generalization capabilities. Future work in audio generation can focus on building an evaluation protocal that is more aligned with human perceptions.

\subsection{Computation Efficiency Comparison}

\begin{figure}[htbp]
  \centering
  \subfigure[Different batch sizes]{\includegraphics[width=0.3\textwidth]{pics/AudioLDM-S.png}\label{fig:sub-audioldm-s}}
  \subfigure[Different sampling steps]{\includegraphics[width=0.3\textwidth]{pics/AudioLDM-L.png}\label{fig:sub-audioldm-l}}
  \subfigure[With/Without classifier-free guidance]{\includegraphics[width=0.3\textwidth]{pics/timing.png}\label{sub-audioldm-timing}}
  \caption{These three figures show the time cost when generating ten seconds of audio, measured on a single A100 GPU.}
  \label{fig:speed-comparison}
\end{figure}

As shown in Figure~\ref{fig:sub-audioldm-s}, AudioLDM-S can generate eight ten-second-long audios within ten seconds without classifier-free guidance. With classifier-free guidance, AudioLDM-Small can generate eight ten-second-long audios with  DDIM steps. Figure~\ref{sub-audioldm-timing} shows our model is faster than the DiffSound on different batch sizes. Our model can generate eight ten-second-long audios with  seconds while DiffSound needs more than  seconds. Since AudioGen has not been open-sourced yet, we did not perform a speed comparison with AudioGen.

\subsection{Limitations}

There are several limitations to our study that warrant further investigation in future work. For example, the sampling rate of our model is still insufficient, especially for the generation of music. Exploring higher-fidelity sampling rates such as 32 kHz or 48 kHz could improve the quality of the generated audio. Also, all the modules in AudioLDM are trained separately, which may result in misalignment between different modules. For instance, the latent space learned by VAE may not be optimal for the latent diffusion model. Future work can explore approaches to better align the different modules, such as end-to-end fine-tuning.

The possible negative impact of our method might be the abuse of our technology or released models, e.g., generating fake audio effects to provide misleading information. Moreover, sensitive text content should be restricted in future work to prevent the creation of harmful audio content.

\newpage
\subsection{Demos}
\label{app:demos}

\textbf{Audio Style Transfer}

We show three examples of zero-shot audio style transfer with AudioLDM-S, using the developed shallow reverse process~(see Equation~\ref{shallowreverse}). In Figure~\ref{fig:demo-style-transfer-1}, we show the transfer from \textbf{drum beats} to \textbf{ambient music}. From left to right, we show the source audio sample drum beats, and the six generated samples guided by text prompt \textbf{ambient music} with different starting points . Given a smaller ~(i.e., the left part of the figure), the generated sample is similar to drum beats, while when we set  for the last sample, the generated sample will be aligned with the text input \textbf{ambient music}. Similarly, we show the source audio \textbf{trumpt}, and the seven generated samples guided by text prompt \textbf{children singing} in Figure~\ref{fig:demo-style-transfer-2}. We show the source audio \textbf{sheep vocalization}, and the five generated samples guided by text prompt \textbf{narration, monologue} in Figure~\ref{fig:demo-style-transfer-3}.

\vspace{0.5cm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{pics/drum_beat_to_ambient_music.png}
    \caption{Audio style transfer from \textbf{drum beats} to \textbf{ambient music}.}
    \label{fig:demo-style-transfer-1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{pics/trumpet-to-children-singing.png}
    \caption{Audio style transfer from \textbf{trumpet} to \textbf{children singing}.}
    \label{fig:demo-style-transfer-2}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{pics/sheep_to_human.000.png}
    \caption{Audio style transfer from \textbf{sheep vocalization} to \textbf{narration, monologue}.}
    \label{fig:demo-style-transfer-3}
\end{figure}

\newpage

\textbf{Audio Super-Resolution}

In Figure~\ref{fig:demo-super-resolution}, we show four cases of zero-shot audio super-resolution with AudioLDM-S: ) \textbf{violin}, ) \textbf{sneezing sound from a woman}, ) \textbf{baby crying}, and ) \textbf{female speech}. The sampling rate of input samples (left) is ~kHz, and that of generated samples (middle) and ground-truth samples (right) is ~kHz. Our visualization shows we can retain the ground-truth observation in the low-frequency part (below ~kHz), while generating the high-frequency missing part (from ~kHz to ~kHz) with pretrained AudioLDM-S. The generated high-frequency information is consistent with the low-frequency observation.  

\vspace{0.5cm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{pics/super_resolution_2.pdf}
    \caption{The examples of zero-shot audio super-resolution with AudioLDM-S.}
    \label{fig:demo-super-resolution}
\end{figure}

\newpage

\textbf{Audio Inpainting}

In Figure~\ref{fig:demo-inpainting}, we show four samples of zero-shot audio inpainting with AudioLDM-S. The time length of each audio sample is  seconds. In the \textbf{unprocessed} part, we remove the content between  and  seconds from the ground-truth sample as the input of inpainting. In the \textbf{inpainting result} part, we show the generated samples guided by the same text prompt of the ground-truth sample. In the \textbf{ground truth} part, we show the ground-truth sample for comparison.

\vspace{0.5cm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/inpainting.pdf}
    \caption{The examples of zero-shot audio inpainting with AudioLDM-S.}
    \label{fig:demo-inpainting}
\vspace{0.5cm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/inpaint_prompt.pdf}
    \caption{The example of zero-shot audio inpainting with AudioLDM-S given different text prompts.}
    \label{fig:demo-inpainting-prompt}
\vspace{0.5cm}
\end{figure}


In Figure~\ref{fig:demo-inpainting-prompt}, we use one sample to demonstrate the audio inpainting guided by different text prompts. Given the observed audio signal shown in the top row, we guide the inpainting process with four different text prompts: ) \textbf{ambient music}; ) \textbf{a man is speaking with bird calls in the background}; ) \textbf{a cat is meowing}; ) \textbf{raining with wind blowing}. As can be seen, the observed audio signal is preserved in each generated sample, while the generated content can be controlled by text input.   

\newpage

\vspace{0.5cm}

\textbf{Environment Control}

In Figure~\ref{fig:demo-control-speech}, we demonstrate that AudioLDM can control the acoustic environment of generated samples with a text description. The four samples are generated with the same random seed, but with different text prompts. Their common text information is \textbf{``A man is speaking in''}, while the specific text information describes the acoustic environment as \textbf{``a small room''}, \textbf{``a huge room''}, \textbf{``a huge room without background noise''}, and \textbf{``a studio''}. These samples show the ability of AudioLDM to capture the fine-grained text description about the acoustic environment, and control the corresponding effects on audio samples, such as reverberation or background noise.   
\vspace{0.5cm}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/control_speech_2.pdf}
    \caption{The examples of controlling acoustic environment with AudioLDM-S.}
    \label{fig:demo-control-speech}
\end{figure}


\newpage

\textbf{Music Control}

In Figure~\ref{fig:demo-control-music}, we show the generated music samples when we control the music characteristics with text input. The first sample is generated by \textbf{``Theme music with bass drum''}. Then, we add specific text information \textbf{``flute''}, \textbf{``fast, flute''}, or \textbf{``flute in the background''}, to change the text input. The corresponding variations can be seen in generated mel-spectrograms. We use these samples to demonstrate the ability of AudioLDM to add new musical instruments to music samples, tune the speed of music, and control the foreground-background relations. 

\vspace{0.8cm}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{pics/control_music_2.pdf}
    \caption{The examples of controlling music characteristics with AudioLDM-S.}
    \label{fig:demo-control-music}
\vspace{0.5cm}
\end{figure}

\newpage    

\textbf{Pitch Control}

In Figure~\ref{fig:pitch}, we show the ability of AudioLDM to control the pitch of generated samples. Pitch is an important characteristic of sound effects, music and speech. Here, we set the common text information as \textbf{``Sine wave with  pitch''}, and input the specific text information \textbf{``low''}, \textbf{``medium''}, and \textbf{``high''}. The text-controlled pitch variation can be seen from the three generated samples.

\vspace{0.5cm}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{pics/pitch_control.pdf}
    \caption{The examples of pitch controlling on generating samples with AudioLDM-S.}
    \label{fig:pitch}
\end{figure}


\newpage

\textbf{Material Control}

In Figure~\ref{fig:material}, we show the ability of AudioLDM to control the materials which generate audio samples. We show four samples generated by the common action \textbf{``hit''} between different materials, e.g., \textbf{wooden object} and \textbf{wooden environment}, or \textbf{metal object} and \textbf{wooden environment}. 

\vspace{0.5cm}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/material_control.pdf}
    \caption{The examples of controlling the materials of generated audio samples with AudioLDM-S.}
    \label{fig:material}
\vspace{0.5cm}
\end{figure}

\vspace{0.5cm}
\textbf{Temporal Order Control}

In Figure~\ref{fig:temporalorder}, we show the ability of AudioLDM to control the temporal order between generated compositional audio signals. When the text description includes multiple sound effects, AudioLDM can generate the audio signals, and the temporal order between them is consistent with the text input.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/order_control.pdf}
    \caption{The examples of controlling the temporal order between generated compositional audio samples with AudioLDM-S.}
    \label{fig:temporalorder}
\vspace{0.5cm}
\end{figure}


\textbf{Text-to-Audio Generation}

In Figure~\ref{fig:text-to-audio}, we show four text-to-audio generation results with AudioLDM-S. They include sound effects in natural environment, human speech, human activity, and sound from objects interaction.

\vspace{0.3cm}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/text2sound.pdf}
    \caption{The examples of text-to-audio generation with AudioLDM-S.}
    \label{fig:text-to-audio}
\vspace{0.6cm}
\end{figure}


\textbf{Novel Audio Generation}

In Figure~\ref{fig:novel-audio}, we show four novel audio samples generated with AudioLDM-S. Their text description is rarely seen, e.g., \textbf{``A wolf is singing a beautiful song.''}. We use them to exhibit the generalization ability of AudioLDM.
\vspace{0.3cm}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/novel_sound.pdf}
    \caption{The examples of novel audio generation with AudioLDM-S.}
    \label{fig:novel-audio}
\end{figure}

\newpage

\textbf{Music Generation}

In Figure~\ref{fig:demo-audioset-music}, we show four music samples generated with AudioLDM-S. Here, we are using the labels of AudioSet as text description for music generation, and we are able to specify the music genres of generated samples such as \textbf{Classical music}.


\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{pics/label2music_generation.pdf}
    \caption{The examples of music generation with AudioLDM-S.}
    \label{fig:demo-audioset-music}
\end{figure}










\newpage













































































































































































































































































\end{document}
