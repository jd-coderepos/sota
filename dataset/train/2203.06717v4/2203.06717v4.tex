

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 



\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{lipsum}
\usepackage{cuted}
\usepackage{siunitx}
\usepackage{booktabs}

\usepackage{subcaption}
\usepackage{multirow}


\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{07053} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}
	


	\title{Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs}
	
	\author{Xiaohan Ding \textsuperscript{1}\thanks{This work is supported by the National Natural Science
			Foundation of China (Nos.61925107, U1936202, 62021002) and the Beijing Academy of Artificial Intelligence (BAAI). This work is done during Xiaohan Ding's internship at MEGVII Technology.} 
		\quad Xiangyu Zhang \textsuperscript{2}\thanks{Project leader and corresponding author.}  
		\quad Yizhuang Zhou \textsuperscript{2} \\
		Jungong Han \textsuperscript{3} 
		\quad Guiguang Ding \textsuperscript{1}
		\quad Jian Sun \textsuperscript{2} \\
		\textsuperscript{1} Beijing National Research Center for Information Science and Technology (BNRist); \\School of Software, Tsinghua University, Beijing, China \\
		\textsuperscript{2} MEGVII Technology \\
		\textsuperscript{3} Computer Science Department, Aberystwyth University, SY23 3FL, UK \\
		\tt\small dxh17@mails.tsinghua.edu.cn \quad zhangxiangyu@megvii.com \quad zhouyizhuang@megvii.com\\
		\tt\small jungonghan77@gmail.com \quad dinggg@tsinghua.edu.cn \quad sunjian@megvii.com \\
	}
	
	\maketitle
	
	
	
\begin{abstract}
	    We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, \eg, applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a \textbf{pure} CNN architecture whose kernel size is as large as 3131, in contrast to commonly used 33. RepLKNet greatly closes the performance gap between CNNs and ViTs, \eg, achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining \textbf{87.8\%} top-1 accuracy on ImageNet and \textbf{56.0\%} mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code \& models at \url{https://github.com/megvii-research/RepLKNet}.
	\end{abstract}
	
\section{Introduction}
	\label{sec:intro}
	
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\linewidth]{erfs.pdf}
			\vspace{-0.25in}
			\caption{The \emph{Effective Receptive Field (ERF)} of ResNet-101/152 and RepLKNet-13/31 respectively. A more widely distributed dark area indicates a larger ERF. More layers (\eg, from ResNet-101 to ResNet-152) help little in enlarging ERFs. Instead, our large kernel model \emph{RepLKNet} effectively obtains large ERFs.}
			\label{fig-rf}
			\vspace{-0.35in}
		\end{center}
	\end{figure}	
	
	\emph{Convolutional neural networks (CNNs)} \cite{krizhevsky2012imagenet,he2016deep} used to be a common choice of visual encoders in modern computer vision systems. However, recently, CNNs \cite{krizhevsky2012imagenet,he2016deep} have been greatly challenged by \emph{Vision Transformers (ViTs)}~\cite{vit,swin,deit,pvt}, which have shown leading performances on many visual tasks -- not only image classification~\cite{vit,yuan2021volo} and representation learning~\cite{mocov3,dino,swinself,bao2021beit}, but also many downstream tasks such as object detection~\cite{swin,dai2021dynamic}, semantic segmentation~\cite{pvt,xie2021segformer} and image restoration~\cite{ipt,liang2021swinir}. 
	Why are ViTs super powerful? Some works believed that \emph{multi-head self-attention (MHSA)} mechanism in ViTs plays a key role. They provided empirical results to demonstrate that, MHSA is more flexible~\cite{clip}, capable (less inductive bias)~\cite{cordonnier2019relationship}, more robust to distortions~\cite{paul2021vision,xie2021segformer}, or able to model long-range dependencies~\cite{vaswani2017attention,raghu2021vision}. But some works challenge the necessity of MHSA \cite{zhu2019empirical}, attributing the high performance of ViTs to the proper building blocks~\cite{dong2021attention}, and/or dynamic sparse weights~\cite{han2021demystifying,zhao2021battle}. More works~\cite{hinton2021represent,zhu2019empirical,han2021demystifying,wu2019pay,cordonnier2019relationship} explained the superiority of ViTs from different point of views.
	
	In this work, we focus on one view: \emph{the way of building up large receptive fields}. In ViTs, \emph{MHSA} is usually designed to be either global~\cite{vit,pvt,bot} or local but with large kernels~\cite{swin,halonet,sasa}, thus each output from a \emph{single} MHSA layer is able to gather information from a large region. However, large kernels are not popularly employed in CNNs (except for the first layer~\cite{he2016deep}). Instead, a typical fashion is to use a stack of many small \emph{spatial convolutions}\footnote{Convolutional kernels (including the variants such as depth-wise/group convolutions) whose spatial size is larger than 11.} \cite{simonyan2014very,he2016deep,zhang2018shufflenet,mbv1,huang2017densely,efficientnet,regnet} (\eg, 33) to enlarge the receptive fields in state-of-the-art CNNs. Only some old-fashioned networks such as \emph{AlexNet}~\cite{krizhevsky2012imagenet}, \emph{Inceptions}~\cite{szegedy2015going,szegedy2016rethinking,szegedy2017inception} and a few architectures derived from \emph{neural architecture search}~\cite{mbv3,liu2018darts,zoph2016neural,guo2020single} adopt large spatial convolutions (whose size is greater than 5) as the main part. 
	The above view naturally lead to a question: what if we use \emph{a few large} instead of \emph{many small} kernels to conventional CNNs? Is large kernel or the way of building large receptive fields the key to close the performance gap between CNNs and ViTs?
	
	
	To answer this question, we systematically explore the large kernel design of CNNs. We follow a very simple ``philosophy'': just introducing large \emph{depth-wise} convolutions into conventional networks, whose sizes range from 33 to 3131, although there exist other alternatives to introduce large receptive fields via a single or a few layers, \eg feature pyramids~\cite{wang2020deep}, dilated convolutions~\cite{yu2015multi,yu2017dilated,chen2017deeplab} and deformable convolutions~\cite{dai2017deformable}. Through a series of experiments, we summarize five empirical guidelines to effectively employ large convolutions: \textbf{1)} very large kernels can still be efficient in practice; \textbf{2)} identity shortcut is vital especially for networks with very large kernels; \textbf{3)} re-parameterizing~\cite{ding2021repvgg} with small kernels helps to make up the optimization issue; \textbf{4)} large convolutions boost downstream tasks much more than \emph{ImageNet}; \textbf{5)} large kernel is useful even on small feature maps.
	
	
	Based on the above guidelines, we propose a new architecture named \emph{RepLKNet}, a \emph{pure}\footnote{Namely CNNs free of any attention or dynamic mechanism, \eg, \emph{squeeze-and-excitation}~\cite{hu2018squeeze}, \emph{multi-head self-attention}, \emph{dynamic weights}~\cite{han2021demystifying,wu2019pay}, and \etc.} CNN where re-parameterized large convolutions are employed to build up large receptive fields. Our network in general follows the macro architecture of \emph{Swin Transformer}~\cite{swin} with a few modifications, while replacing the~\emph{multi-head self-attentions} with large depth-wise convolutions. We mainly benchmark middle-size and large-size models, since ViTs used to be believed to surpass CNNs on large data and models. On ImageNet classification, our baseline (similar model size with \emph{Swin-B}), whose kernel size is as large as 3131, achieves \textbf{84.8\%} top-1 accuracy trained only on ImageNet-1K dataset, which is 0.3\% better than Swin-B but much more efficient in latency. 
	
	
	More importantly, we find that the large kernel design is particularly powerful on \emph{downstream} tasks. For example, our networks outperform \emph{ResNeXt}-101~\cite{xie2017aggregated} or \emph{ResNet}-101~\cite{he2016deep} backbones by \textbf{4.4\%} on \emph{COCO} detection~\cite{lin2014microsoft} and \textbf{6.1\%} on \emph{ADE20K} segmentation~\cite{zhou2019semantic} under the similar complexity and parameter budget, which is also on par with or even better than the counterpart \emph{Swin Transformers} but with higher inference speed. Given more pretraining data (\eg, 73M images) and more computational budget, our best model obtains very competitive results among the state-of-the-arts with similar model sizes, \eg. \textbf{87.8\%} top-1 accuracy on ImageNet and \textbf{56.0\%} on ADE20K, which shows excellent scalability towards large-scale applications. 
	
	
	We believe the high performance of RepLKNet is mainly because of the large \emph{effective receptive fields (ERFs)}~\cite{erf} built via large kernels, as compared in Fig.~\ref{fig-rf}. Moreover, RepLKNet is shown to leverage more shape information than conventional CNNs, which partially agrees with human's cognition. We hope our findings can help to understand the intrinsic mechanism of both CNNs and ViTs.

	
	\begin{table*}[t]
	\caption{Inference speed of a stack of 24-layer depth-wise convolutions with various kernel sizes and resolutions on a single GTX 2080Ti GPU. The input shape is (64, 384, , ). Baselines are evaluated with Pytorch 1.9.0 + cuDNN 7.6.5, in FP32 precision.}
	\label{table-speed-kernelsize}
	\small
	\begin{center}
	\vspace{-0.2in}
    \begin{tabular}{llccccccccccccc}
    \hline
    \multirow{2}{*}{Resolution } & \multirow{2}{*}{Impl} & \multicolumn{10}{c}{Latency (ms) @ Kernel size} \\
                                &                           
                                & 3     & 5     & 7     & 9     & 13   & 17     & 21    & 27    & 29    & 31        \\ \hline
    \multirow{2}{*}{}         & Pytorch        
                                & 5.6   & 11.0  & 14.4  & 17.6  & 36.0 & 57.2   & 83.4  & 133.5 & 150.7 & 171.4       \\
                                & Ours                      
                                & 5.6   & 6.5   & 6.4   & 6.9   & 7.5  & 8.4    & 8.4   & 8.4   & 8.3   & 8.4       \\ \hline
    \multirow{2}{*}{}         & Pytorch        
                                & 21.9  & 34.1  & 54.8  & 76.1  & 141.2 & 230.5 & 342.3 & 557.8 & 638.6 & 734.8       \\
                                & Ours                      
                                & 21.9  & 28.7  & 34.6  & 40.6  & 52.5  & 64.5  & 73.9  & 87.9  & 92.7  & 96.7       \\ \hline
    \multirow{2}{*}{}         & Pytorch                   
                                & 69.6  & 141.2 & 228.6 & 319.8 & 600.0 & 977.7 & 1454.4 & 2371.1 & 2698.4 & 3090.4      \\
                                & Ours  
                                & 69.6  & 112.6 & 130.7 & 152.6 & 199.7 & 251.5 & 301.0 & 378.2 & 406.0 & 431.7       \\ \hline
    \end{tabular}
    \end{center}
    \vspace{-0.3in}
    \end{table*}
	
	
	\section{Related Work}
	
	\subsection{Models with Large Kernels}
	
	As mentioned in the introduction, apart from a few old-fashioned models like \emph{Inceptions}~\cite{szegedy2015going,szegedy2016rethinking,szegedy2017inception}, large-kernel models became not popular after \emph{VGG-Net}~\cite{simonyan2014very}. One representative work is \emph{Global Convolution Networks (GCNs)}~\cite{peng2017large}, which uses very large convolutions of 1K followed by K1 to improve semantic segmentation task. However, large kernels are reported to harm the performance on ImageNet. \emph{Local Relation Networks (LR-Net)}~\cite{hu2019local} proposes a spatial aggregation operator (LR-Layer) to replace standard convolutions, which can be viewed as a dynamic convolution. LR-Net could benefit from a kernel size of 77, but the performance decreases with 99. With a kernel size as large as the feature map, the top-1 accuracy significantly reduced from 75.7\% to 68.4\%. 
	
	Recently, \emph{Swin Transformers}~\cite{swin} propose to capture the spatial patterns with shifted window attention, whose window sizes range from 7 to 12, which can also be viewed as a variant of large kernel. The follow-ups \cite{dong2021cswin,liu2021swin} employ even larger window sizes. Inspired by the success of those local transformers, a recent work~\cite{han2021demystifying} replaces \emph{MHSA} layers with static or dynamic 77 depth-wise convolutions in \cite{swin} while still maintains comparable results. Though the network proposed by \cite{han2021demystifying} shares similar design pattern with ours, the motivations are different: \cite{han2021demystifying} does not investigate the relationship between \emph{ERFs}, large kernels and performances; instead, it attributes the superior performances of vision transformers to sparse connections, shared parameters and dynamic mechanisms. Another three representative works are \emph{Global Filter Networks (GFNets)}~\cite{rao2021global}, \emph{CKConv}~\cite{romero2021ckconv} and \emph{FlexConv}~\cite{romero2021flexconv}. GFNet optimizes the spatial connection weights in the Fourier domain, which is equivalent to circular global convolutions in the spatial domain. CKConv formulates kernels as continuous functions to process sequential data, which can construct arbitrarily large kernels. FlexConv learns different kernel sizes for different layers, which can be as large as the feature maps. Although they use very large kernels, they do not intend to answer the key questions we desire: why do traditional CNNs underperform ViTs, and how to apply large kernels in \emph{common CNNs}.	Besides, both \cite{han2021demystifying} and \cite{rao2021global} do not evaluate their models on strong baselines, \eg, models larger than \emph{Swin-L}. Hence it is still unclear whether large-kernel CNNs can scale up well as transformers. 

	
	\noindent \textbf{Concurrent works}.  
	
	\emph{ConvMixer} \cite{trockman2022patches} uses up to 99 convolutions to replace the ``mixer'' component of \emph{ViTs}~\cite{vit} or \emph{MLPs}~\cite{tolstikhin2021mlp,touvron2021resmlp}. \emph{MetaFormer}~\cite{yu2021metaformer} suggests \emph{pooling} layer is an alternate to self-attention. \emph{ConvNeXt}~\cite{liu2022convnet} employs 77 depth-wise convolutions to design strong architectures, pushing the limit of CNN performances. Although those works show excellent performances, they do not show benefits from much larger convolutions (\eg, 3131). 
	
	
	\subsection{Model Scaling Techniques}
	
	Given a small model, it is a common practice to scale it up for better performance, thus scaling strategy plays a vital role in the resultant accuracy-efficiency trade-offs. For CNNs, existing scaling approaches usually focus on model depth, width, input resolution~\cite{efficientnet,regnet,dollar2021fast}, bottleneck ratio and group width~\cite{regnet,dollar2021fast}. Kernel size, however, is often neglected. In Sec.~\ref{sect-3}, we will show that the kernel size is also an important scaling dimension in CNNs, especially for \emph{downstream} tasks. 
	

	
	\subsection{Structural Re-parameterization}
	
	Structural Re-parameterization~\cite{ding2021repvgg,ding2021diverse,ding2021resrep,ding2021repmlpnet,ding2019acnet} is a methodology of equivalently converting model structures via transforming the parameters. For example, RepVGG targeted at a deep inference-time VGG-like (\eg, branch-free) model, and constructed extra ResNet-style shortcuts parallel to the 33 layers during training. In contrast to a real VGG-like model that is difficult to train~\cite{he2016deep}, such shortcuts helped the model reach a satisfactory performance. After training, the shortcuts are absorbed into the parallel 33 kernels via a series of linear transformations, so that the resultant model becomes a VGG-like model. In this paper, we use this methodology to add a relatively small (\eg, 33 or 55) kernel into a very large kernel. In this way, we make the very large kernel capable of capturing small-scale patterns, hence improve the performance of the model.
	
	
	\section{Guidelines of Applying Large Convolutions}\label{sect-3}
	
	Trivially applying large convolutions to CNNs usually leads to inferior performance and speed. In this section, we summarize 5 guidelines for effectively using large kernels.
	
	\vspace{-0.15in}\paragraph{Guideline 1: large depth-wise convolutions can be efficient in practice.} 
	It is believed that large-kernel convolutions are computationally expensive because the kernel size quadratically increases the number of parameters and FLOPs. The drawback can be greatly overcome by applying depth-wise (DW) convolutions \cite{mbv1,chollet2017xception}. For example, in our proposed \emph{RepLKNet} (see Table~\ref{table-replknet-224} for details), increasing the kernel sizes in different stages from  to  only increases the FLOPs and number of parameters by 18.6\% and 10.4\% respectively, which is acceptable. The remaining 11 convolutions actually dominate most of the complexity. 
	
	One may concern that DW convolutions could be very inefficient on modern parallel computing devices like GPUs. It is true for conventional DW 33 kernels~\cite{mbv1,mbv2,zhang2018shufflenet}, because DW operations introduce low ratio of computation \vs memory access cost~\cite{ma2018shufflenet}, which is not friendly to modern computing architecture. However, we find when kernel size becomes large, the computational density increases: for example, in a DW 1111 kernel, each time we load a value from the feature map, it can attend at most 121 multiplications, while in a 33 kernel the number is only 9. Therefore, according to the roofline model, the actual latency should not increase as much as the increasing of FLOPs when kernel size becomes larger. 
	
	\noindent \textbf{Remark 1.} Unfortunately, we find off-the-shelf deep learning tools (such as Pytorch) support large DW convolutions poorly, as shown in Table~\ref{table-speed-kernelsize}. Hence we try several approaches to optimize the CUDA kernels. FFT-based approach~\cite{mathieu2013fast} appears reasonable to implement large convolutions. However, in practice we find block-wise (inverse) \emph{implicit gemm} algorithm is a better choice. The implementation has been integrated into the open-sourced framework MegEngine~\cite{MegEngine} and we omit the details here. We have also released an efficient implementation~\cite{RepLKNet-pytorch} for PyTorch. Table~\ref{table-speed-kernelsize} shows that our implementation is far more efficient, compared with the Pytorch baseline. With our optimization, the latency contribution of DW convolutions in RepLKNet reduces from 49.5\% to 12.3\%, which is roughly in proportion to the FLOPs occupation. 
	
	\vspace{-0.15in}
	\paragraph{Guideline 2: identity shortcut is vital especially for networks with very large kernels.} 
To demonstrate this, we use \emph{MobileNet V2}~\cite{mbv2} to benchmark, since it heavily uses DW layers and has two published variants (with or without shortcuts). For the large-kernel counterparts, we simply replace all the DW 33 layers with 1313. All the models are trained on ImageNet with the identical training configurations for 100 epochs (see Appendix A for details). Table~\ref{table-mob2-shortcut} shows large kernels improve the accuracy of MobileNet V2 with shortcuts by 0.77\%. However, without shortcuts, large kernels reduce the accuracy to only 53.98\%. 


\noindent \textbf{Remark 2.} The guideline also works for \emph{ViTs}. A recent work~\cite{dong2021attention} finds that without identity shortcut, attention loses rank doubly exponentially with depth, leading to over-smoothing issue. Although large-kernel CNNs may degenerate in a different mechanism from ViT's, we also observed without shortcut, it is difficult for the network to capture local details. From a similar perspective as \cite{veit2016residual}, shortcuts make the model an implicit ensemble composed of numerous models with different receptive fields (RFs), so it can benefit from a much larger maximum RF while not losing the ability to capture small-scale patterns.

	
	\begin{table}
		\caption{Results of different kernel sizes in normal/shortcut-free MobileNet V2.}
		\label{table-mob2-shortcut}
		\vspace{-0.2in}
		\begin{center}
			\small
			\begin{tabular}{lcccc}
				\hline
				Shortcut 		& Kernel size		& ImageNet top-1 accuracy (\%)	\\
				\hline
				\checkmark		&	33		&	71.76		\\
				\checkmark		&	1313	&	\textbf{72.53}		\\
		        \hline
				&	3		&	\textbf{68.67}		\\
				&	1313	&	53.98		\\		
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.25in}
	\end{table}
	
	\begin{figure*}
		\begin{center}
			\includegraphics[width=\linewidth]{reparam.pdf}
			\vspace{-0.25in}
			\caption{An example of re-parameterizing a small kernel (\eg, 33) into a large one (\eg, 77). See \cite{ding2019acnet,ding2021repvgg} for details. }
			\label{fig-reparam}
			\vspace{-0.25in}
		\end{center}
	\end{figure*}
	
	\vspace{-0.15in} \paragraph{Guideline 3: re-parameterizing \cite{ding2021repvgg} with small kernels helps to make up the optimization issue.} We replace the 33 layers of \emph{MobileNet V2} by 99 and 1313 respectively, and optionally adopt \emph{Structural Re-parameterization}~\cite{ding2021repvgg,ding2021repmlpnet,ding2019acnet} methodology. Specifically, we construct a 33 layer parallel to the large one, then add up their outputs after \emph{Batch normalization (BN)}~\cite{ioffe2015batch} layers (Fig.~\ref{fig-reparam}). After training, we merge the small kernel as well as BN parameters into the large kernel, so the resultant model is equivalent to the model for training but no longer has small kernels. Table~\ref{table-mob2-reparam} shows directly increasing the kernel size from 9 to 13 reduces the accuracy, while re-parameterization addresses the issue. 
	
	We then transfer the ImageNet-trained models to semantic segmentation with DeepLabv3+~\cite{chen2018encoder} on Cityscapes~\cite{cityscapes}. We only replace the backbone and keep all the default training settings provided by MMSegmentation~\cite{mmseg2020}. The observation is similar to that on ImageNet: 33 re-param improves the mIoU of the 99 model by 0.19 and the 1313 model by 0.93. With such simple re-parameterization, increasing kernel size from 9 to 13 no longer degrades the performance on both ImageNet and Cityscapes. 
	
	\noindent\textbf{Remark 3.} 
	It is known that \emph{ViTs} have optimization problem especially on small datasets~\cite{vit,liu2020understanding}. A common workaround is to introduce \emph{convolutional prior}, \eg, add a DW 33 convolution to each self-attention block~\cite{wu2021cvt,chu2021conditional}, which is analogous to ours. Those strategies introduce additional \emph{translational equivariance} and \emph{locality} prior to the network, making it easier to optimize on small dataset without loss of generality. Similar to what ViT behaves~\cite{vit}, we also find when the pretraining dataset increases to 73 million images (refer to \emph{RepLKNet-XL} in the next section), re-parameterization can be omitted without degradation. 
	
	\begin{table}
		\caption{Results of 33 re-parameterization on MobileNet V2 with various kernel sizes.}
		\label{table-mob2-reparam}
		\vspace{-0.2in}
		\begin{center}
			\small
			\begin{tabular}{lcccc}
				\hline
				Kernel 					&	33 re-param		& \makecell{ImageNet \\ top-1 acc (\%)}	&	\makecell{Cityscapes \\ val mIoU (\%)}	\\
				\hline
				33				&  N/A						&	71.76				&	72.31	\\
\hline
				99			&						&	72.67				&	76.11	\\
				99			&	\checkmark			&	\textbf{73.09}				&	\textbf{76.30}	\\
				\hline
				1313		&						&	72.53				&	75.67	\\
				1313		&	\checkmark			&	\textbf{73.24}				&	\textbf{76.60}	\\			
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.25in}
	\end{table}
	
	
	
	\vspace{-0.15in}\paragraph{Guideline 4: large convolutions boost downstream tasks much more
than ImageNet classification.} 
Table~\ref{table-mob2-reparam} (after \emph{re-param}) shows increasing the kernel size of MobileNet V2 from 33 to 99 improves the \emph{ImageNet} accuracy by 1.33\% but the \emph{Cityscapes} mIoU by 3.99\%. Table~\ref{table-replknet-224} shows a similar trend: as the kernel sizes increase from  to , the ImageNet accuracy improves by only 0.96\%, while the mIoU on \emph{ADE20K}~\cite{zhou2019semantic} improves by 3.12\%. Such phenomenon indicates that models of similar ImageNet scores could have very different capability in downstream tasks (just as the bottom 3 models in Table~\ref{table-replknet-224}).  

\noindent \textbf{Remark 4.} 
What causes the phenomenon? First, large kernel design significantly increases the \emph{Effective Receptive Fields (ERFs)} \cite{erf}. Numerous works have demonstrated ``contextual'' information, which implies large ERFs, is crucial in many downstream tasks like object detection and semantic segmentation \cite{peng2017large,long2015fully,yu2017dilated,wang2020deep,yu2015multi}. We will discuss the topic in Sec.~\ref{sec:largeerf}. 
Second, We deem another reason might be that large kernel design contributes more shape biases to the network. Briefly speaking, ImageNet pictures can be correctly classified according to either texture or shape, as proposed in \cite{geirhos2018imagenet,brendel2019approximating}. However, humans recognize objects mainly based on shape cue rather than texture, therefore a model with stronger shape bias may transfer better to downstream tasks. A recent study \cite{tuli2021convolutional} points out ViTs are strong in shape bias, which partially explains why ViTs are super powerful in transfer tasks. In contrast, conventional CNNs trained on ImageNet tend to bias towards texture \cite{geirhos2018imagenet,brendel2019approximating}. Fortunately, we find simply enlarging the kernel size in CNNs can effectively improve the shape bias. Please refer to Appendix C for details.
	
	\begin{figure}
		\begin{center}
			\includegraphics[width=\linewidth]{large-kernel-small-feature.pdf}
			\vspace{-0.25in}
			\caption{Illustration to convolution with small feature map and large kernel. Two outputs at adjacent locations only share a part of kernel weights. Translational equivariance does not strictly hold.}
			\label{fig-large-kernel-small-feature}
			\vspace{-0.15in}
		\end{center}
	\end{figure}
	
	\vspace{-0.15in}\paragraph{Guideline 5: large kernel (\eg, 13\bm{}13) is useful even on small feature maps (\eg, 7\bm{}7).} 
	To validate it, We enlarge the DW convolutions in the \emph{last stage} of \emph{MobileNet V2} to 77 or 1313, hence the kernel size is on par with or even larger than feature map size (77 by default). We apply re-parameterization to the large kernels as suggested by Guideline 3. Table~\ref{table-mob2-smallfeature} shows although convolutions in the last stage already involve very large receptive field, further increasing the kernel sizes still leads to performance improvements, especially on downstream tasks such as \emph{Cityscapes}. 
	
	\noindent \textbf{Remark 5.} 
	When kernel size becomes large, notice that translational equivariance of CNNs does not strictly hold. As illustrated in Fig.~\ref{fig-large-kernel-small-feature}, two outputs at adjacent spatial locations share only a fraction of the kernel weights, \ie, are transformed by different mappings. The property also agrees with the ``philosophy'' of \emph{ViTs} -- relaxing the symmetric prior to obtain more capacity. Interestingly, we find 2D \emph{Relative Position Embedding (RPE)}~\cite{shaw2018self,bello2019attention}, which is widely used in the transformer community, can also be viewed as a large depth-wise kernel of size , where  and  are feature map height and width respectively. Large kernels not only help to learn the relative positions between concepts, but also encode the \emph{absolute position} information due to \emph{padding effect}~\cite{kayhan2020translation}. 
	
	\begin{table}
		\caption{Results of various kernel sizes in the \emph{last stage} of MobileNet V2. Kernel sizes in previous stages remain to be .}
		\label{table-mob2-smallfeature}
		\vspace{-0.2in}
		\begin{center}
			\small
			\begin{tabular}{lcccc}
				\hline
				Kernel size		& ImageNet acc (\%)	&	Cityscapes mIoU (\%)	\\
				\hline
				33		&	71.76				&	72.31	\\
				77		&	\textbf{72.00}				&	74.30	\\
				1313	&	71.97				&	\textbf{74.62}	\\		
				\hline
			\end{tabular}
			
		\end{center}
		\vspace{-0.2in}
	\end{table}
	
	
	\section{RepLKNet: a Large-Kernel Architecture}
	\label{sec:arch}
	Following the above guidelines, in this section we propose RepLKNet, a \emph{pure} CNN architecture with large kernel design. To our knowledge, up to now CNNs still dominate small models~\cite{zhang2021collaboration,zhang2021basisnet}, while \emph{vision transformers} are believed to be better than CNNs under more complexity budget. Therefore, in the paper we mainly focus on relatively large models (whose complexity is on par with or larger than \emph{ResNet-152}~\cite{he2016deep} or \emph{Swin-B}~\cite{swin}), in order to verify whether large kernel design could eliminate the performance gap between CNNs and ViTs. 
	

	
	\subsection{Architecture Specification}
	
	We sketch the architecture of RepLKNet in Fig.~\ref{fig-lknet-arch}:
	
	\textbf{Stem} refers to the beginning layers. Since we target at high performance on downstream dense-prediction tasks, we desire to capture more details by several conv layers at the beginning. After the first 33 with 2 downsampling, we arrange a DW 33 layer to capture low-level patterns, a 11 conv, and another DW 33 layer for downsampling. 
	
\textbf{Stages} 1-4 each contains several RepLK Blocks, which use shortcuts (Guideline 2) and DW large kernels (Guideline 1). We use 11 conv before and after DW conv as a common practice. Note that each DW large conv uses a 55 kernel for re-parameterization (Guideline 3), which is not shown in Fig.~\ref{fig-lknet-arch}. Except for the large conv layers which provide sufficient receptive field and the ability to aggregate spatial information, the model's representational capacity is also closely related to the depth. To provide more nonlinearities and information communications across channels, we desire to use 11 layers to increase the depth. Inspired by the Feed-Forward Network (FFN) which has been widely used in transformers~\cite{vit,swin} and MLPs~\cite{tolstikhin2021mlp,touvron2021resmlp,ding2021repmlpnet}, we use a similar CNN-style block composed of shortcut, BN, two 11 layers and GELU~\cite{hendrycks2016gaussian}, so it is referred to as ConvFFN Block. Compared to the classic FFN which uses Layer Normalization~\cite{ba2016layer} before the fully-connected layers, BN has an advantage that it can be fused into conv for efficient inference. As a common practice, the number of internal channels of the ConvFFN Block is 4 as the input. Simply following ViT and Swin, which interleave attention and FFN blocks, we place a ConvFFN after each RepLK Block.
	
	\textbf{Transition Blocks} are placed between stages, which first increase the channel dimension via 11 conv and then conduct 2 downsampling with DW 33 conv.
	
	In summary, each stage has three architectural hyper-parameters: the number of RepLK Blocks , the channel dimension , and the kernel size . So that a RepLKNet architecture is defined by ,,.
	
		\begin{figure}
		\begin{center}
			\includegraphics[width=\linewidth]{lknet-arch.pdf}
			\vspace{-0.3in}
			\caption{RepLKNet comprises Stem, Stages and Transitions. Except for depth-wise (DW) large kernel, the other components include DW 33, dense 11 conv, and batch normalization~\cite{ioffe2015batch} (BN). Note that every conv layer has a following BN, which are not depicted. Such conv-BN sequences use ReLU as the activation function, except those before the shortcut-addition (as a common practice \cite{he2016deep,mbv2}) and those preceding GELU~\cite{hendrycks2016gaussian}.}
			\label{fig-lknet-arch}
			\vspace{-0.2in}
		\end{center}
	\end{figure}
	
	
	\subsection{Making Large Kernels Even Larger}\label{sect-replknet-120}
	
	\setlength{\tabcolsep}{3pt}
	\begin{table}
		\caption{RepLKNet with different kernel sizes. The models are pretrained on ImageNet-1K in 120 epochs with 224224 input and finetuned on ADE20K with UperNet in 80K iterations. On ADE20K, we test the \emph{single-scale} mIoU, and compute the FLOPs with input of 2048512, following Swin.}
		\label{table-replknet-224}
		\vspace{-0.25in}
		\begin{center}		
			\small
			\begin{tabular}{l|ccc|ccc}
				\hline
				& \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c}{ADE20K} \\
				Kernel size		&	Top-1		& \makecell{Params}		&\makecell{FLOPs}	&	mIoU	& Params	& FLOPs	\\
				\hline
				3-3-3-3         &   82.11           &   71.8M       &   12.9G   &   46.05   &   104.1M  &   1119G\\
				7-7-7-7         &   82.73           &   72.2M       &   13.1G   &   48.05   &   104.6M  &   1123G\\
				13-13-13-13		&	83.02			&	73.7M		&	13.4G	&   48.35   &   106.0M  &   1130G\\ 
				25-25-25-13		&	83.00			&	78.2M       &   14.8G	&   48.68   &   110.6M  &   1159G\\
				31-29-27-13     &   83.07           &   79.3M       &   15.3G   &   49.17   &   111.7M  &   1170G\\
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.3in}
	\end{table}
	\setlength{\tabcolsep}{1.4pt}
	
	We continue to evaluate large kernels on RepLKNet via fixing =, =, varying  and observing the performance of both classification and semantic segmentation. Without careful tuning of the hyper-parameters, we casually set the kernel sizes as , , , respectively, and refer to the models as RepLKNet-13/25/31. We also construct two small-kernel baselines where the kernel sizes are all 3 or 7 (RepLKNet-3/7). 
	
	On ImageNet, we train for 120 epochs with AdamW~\cite{loshchilov2017decoupled} optimizer, RandAugment~\cite{cubuk2020randaugment}, mixup~\cite{zhang2017mixup}, CutMix~\cite{yun2019cutmix}, Rand Erasing~\cite{zhong2020random} and Stochastic Depth~\cite{stochastic}, following the recent works~\cite{swin,deit,bao2021beit,liu2022convnet}. The detailed training configurations are presented in Appendix A.
	
	For semantic segmentation, we use ADE20K~\cite{zhou2019semantic}, which is a widely-used large-scale semantic segmentation dataset containing 20K images of 150 categories for training and 2K for validation. We use the ImageNet-trained models as backbones and adopt UperNet~\cite{xiao2018unified} implemented by MMSegmentation~\cite{mmseg2020} with the 80K-iteration training setting and test the \emph{single-scale} mIoU.
	
	Table~\ref{table-replknet-224} shows our results with different kernel sizes. On ImageNet, though increasing the kernel sizes from 3 to 13 improves the accuracy, making them even larger brings no further improvements. However, on ADE20K, scaling up the kernels from  to  brings 0.82 higher mIoU with only 5.3\% more parameters and 3.5\% higher FLOPs, which highlights the significance of large kernels for downstream tasks.
	
	In the following subsections, we use RepLKNet-31 with stronger training configurations to compare with the state-of-the-arts on ImageNet classification, Cityscapes/ADE20K semantic segmentation and COCO~\cite{lin2014microsoft} object detection. We refer to the aforementioned model as RepLKNet-31B (B for Base) and a wider model with  as RepLKNet-31L (Large). We construct another RepLKNet-XL with  and 1.5 inverted bottleneck design in the RepLK Blocks (\ie, the channels of the DW large conv layers are 1.5 as the inputs).
	
	\setlength{\tabcolsep}{2pt}
	\begin{table}
		\caption{ImageNet results. The throughput is tested with FP32 and a batch size of 64 on 2080Ti.  indicates ImageNet-22K pretraining.  indicates pretrained with extra data.}
		\label{table-to-swin}
		\vspace{-0.25in}
		\begin{center}		
			\small
			\begin{tabular}{llcccccc}
				\hline
				Model			& \makecell{Input \\ resolution}& \makecell{Top-1 \\ acc}	&	\makecell{Params \G)} & \makecell{Throughput\\examples/s}\\
				\hline
				\textbf{RepLKNet-31B}     &	224224 & 	83.5            &   79   & 15.3 & 295.5\\
				Swin-B     	&	224224&   	83.5    		&	88		& 15.4	    &  226.2 \\
				\hline
				\textbf{RepLKNet-31B}     &	384384& 	84.8            &   79   	& 45.1 & 97.0   \\
				Swin-B     	&	384384&   	84.5    		&	88		& 47.0	    & 67.9\\
				\hline
				\textbf{RepLKNet-31B}~     &	224224 & 	85.2            &   -  & - & - \\
				Swin-B~     	&	224224&   	85.2    		&	-		& -	    &  - \\
				\hline
				\textbf{RepLKNet-31B}~     &	384384& 	86.0            &   -   	& - & -   \\
				Swin-B~     	&	384384&   	86.4    		&	-		& -	    & -\\
				\hline
				\textbf{RepLKNet-31L}~   &	384384& 	86.6  &   172 &   96.0 &   50.2               \\
				Swin-L~ 	       &	384384&    87.3    &    197 &103.9 &   36.2 	                \\
				
				\hline
				\textbf{RepLKNet-XL}~ & 320320 & 87.8 & 335 & 128.7 & 39.1 \\
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.25in}
	\end{table}

	
	
	\setlength{\tabcolsep}{2pt}
	\begin{table}
		\caption{Cityscapes results. The FLOPs is computed with 10242048 inputs. The mIoU is tested with single-scale (ss) and multi-scale (ms). The results with Swin are implemented by~\cite{gong2021improve}.  indicates ImageNet-22K pretraining.}
		\label{table-compare-cityscapes}
		\vspace{-0.25in}
		\begin{center}		
			\small
			\begin{tabular}{lllccccc}
				\hline
				Backbone			&	Method		&		\makecell{mIoU \\ (ss)}	&	\makecell{mIoU \\ (ms)}	& 	\makecell{Param \\ (M)}	& \makecell{FLOPs \\ (G)}\\
				\hline
				\textbf{RepLKNet-31B}     	&	UperNet \cite{xiao2018unified}		&		\textbf{83.1}		&	\textbf{83.5}			&	110	&	2315\\			
				ResNeSt-200~\cite{resnest}         &   DeepLabv3~\cite{chen2017rethinking}   &       -           &   82.7            &   -       &   -   \\
				Axial-Res-XL	&Axial-DL \cite{wang2020axial}		&	80.6		&	81.1		&	173	&	2446	\\
				Swin-B					&	UperNet		&		80.4		&	81.5		&	121		&	2613	\\	Swin-B &	UperNet + \cite{gong2021improve}		&		80.8		&	81.8		&	121		&	-	\\
				\hline
				ViT-L~	&	SETR-PUP \cite{zheng2021rethinking}	&		79.3		&	82.1		&	318	&	-	\\
				ViT-L~	&	SETR-MLA &		77.2		&	-			&	310	&	-		\\
				Swin-L~	&	UperNet		&		82.3		&	83.1		&	234		&	3771	\\
				Swin-L~     &	UperNet + \cite{gong2021improve}		&		82.7		&	83.6		&	234		&	-	\\
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.3in}
	\end{table}
	
	





	
	\setlength{\tabcolsep}{2pt}
	\begin{table}
		\caption{ADE20K results. The mIoU is tested with single-scale (ss) and multi-scale (ms). The results with 1K-pretrained Swin are cited from the official GitHub repository.  indicates ImageNet-22K pretraining and 640640 finetuning on ADE20K.  indicates pretrained with extra data. The FLOPs is computed with 2048512 for the ImageNet-1K pretrained models and 2560640 for the ImageNet-22K and larger, following Swin.}
		\label{table-compare-ade20K}
		\vspace{-0.25in}
		\begin{center}		
			\small
			\begin{tabular}{lllcccc}
				\hline
					Backbone			&	Method		&		\makecell{mIoU \\ (ss)}	&	\makecell{mIoU \\ (ms)}	& 	\makecell{Param \\ (M)}	& \makecell{FLOPs \\ (G)}\\
				\hline
				\textbf{RepLKNet-31B}   	&	UperNet		&		\textbf{49.9}		&	\textbf{50.6}			&	112	&	1170\\   
				ResNet-101				&	UperNet \cite{xiao2018unified}			&	43.8	&	44.9		&	86		&	1029	\\
				ResNeSt-200~\cite{resnest}     &	DeepLabv3~\cite{chen2017rethinking}	&	-	&	48.4		&	113		&	1752	\\
				Swin-B	&	UperNet			&		48.1		&	49.7		&	121	&	1188	\\
				Swin-B           &	UperNet + \cite{gong2021improve}		&	48.4	&	50.1	&	121	&	-	\\
				ViT-Hybrid			&DPT-Hybrid~\cite{ranftl2021vision}				&		-			&	49.0		&	90		&	-	\\
				ViT-L				&	DPT-Large			&		-			&	47.6		&	307	&	-	\\
				ViT-B					&	SETR-PUP \cite{zheng2021rethinking}	&		46.3		&	47.3		&	97		&	-	\\
				ViT-B						&	SETR-MLA \cite{zheng2021rethinking}	&		46.2		&	47.7		&	92		&	-		\\
				\hline
				
				\textbf{RepLKNet-31B}~   	&	UperNet		&		\textbf{51.5}		&	\textbf{52.3}			&	112	&	1829\\
				Swin-B~	&	UperNet			&		50.0		&	51.6		&	121	&	1841	\\

				\textbf{RepLKNet-31L}~   	&	UperNet		&		\textbf{52.4}		&	52.7			&	207	&	2404\\	
				Swin-L~	&	UperNet			&		52.1		&	\textbf{53.5}		&	234	    &	2468	\\
				ViT-L~		&	SETR-PUP	&		48.6		&	50.1		&	318		&	-	\\
				ViT-L~		&	SETR-MLA	&		48.6		&	50.3		&	310		&	-		\\
				\hline
				\textbf{RepLKNet-XL}~ & UperNet & \textbf{55.2} & \textbf{56.0} & 374 & 3431 \\
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.3in}
	\end{table}
	
	
	\subsection{ImageNet Classification}\label{sect-imgnet}
	
	Since the overall architecture of RepLKNet is akin to Swin, we desire to make a comparison at first. For RepLKNet-31B on ImageNet-1K, we extend the aforementioned training schedule to 300 epochs for a fair comparison. Then we finetune for 30 epochs with input resolution of 384384, so that the total training cost is much lower than the Swin-B model, which was trained with 384384 from scratch. Then we pretrain RepLKNet-B/L models on ImageNet-22K and finetune on ImageNet-1K. RepLKNet-XL is pretrained on our private semi-supervised dataset named \emph{MegData73M}, which is introduced in the Appendix. We also present the throughput tested with a batch size of 64 on the same 2080Ti GPU. The training configurations are presented in the Appendix.
	
	Table~\ref{table-to-swin} shows that though very large kernels are not intended for ImageNet classification, our RepLKNet models show a a favorable trade-off between accuracy and efficiency. Notably, with only ImageNet-1K training, RepLKNet-31B reaches 84.8\% accuracy, which is 0.3\% higher than Swin-B, and runs 43\% faster. And even though RepLKNet-XL has higher FLOPs than Swin-L, it runs faster, which highlights the efficiency of very large kernels.


	
	\subsection{Semantic Segmentation}
	
	We then use the pretrained models as the backbones on Cityscapes (Table~\ref{table-compare-cityscapes}) and ADE20K (Table~\ref{table-compare-ade20K}). Specifically, we use the UperNet~\cite{xiao2018unified} implemented by MMSegmentation~\cite{mmseg2020} with the 80K-iteration training schedule for Cityscapes and 160K for ADE20K. Since we desire to evaluate the backbone only, we do not use any advanced techniques, tricks, nor custom algorithms. 
	
	On Cityscapes, ImageNet-1K-pretrained RepLKNet-31B outperforms Swin-B by a significant margin (single-scale mIoU of 2.7), and even {outperforms the ImageNet-22K-pretrained Swin-L}. Even equipped with DiversePatch~\cite{gong2021improve}, a technique customized for vision transformers, the single-scale mIoU of the 22K-pretrained Swin-L is still lower than our 1K-pretrained RepLKNet-31B, though the former has 2 parameters.
	
	On ADE20K, RepLKNet-31B outperforms Swin-B with both 1K and 22K pretraining, and the margins of single-scale mIoU are particularly significant. Pretrained with our semi-supervised dataset \emph{MegData73M}, RepLKNet-XL achieves an mIoU of 56.0, which shows feasible scalability towards large-scale vision applications.

	
	\subsection{Object Detection}
	
	For object detection, we use RepLKNets as the backbone of FCOS~\cite{fcos} and Cascade Mask R-CNN~\cite{he2017mask,cai2019cascade}, which are representatives of one-stage and two-stage detection methods, and the default configurations in MMDetection~\cite{mmdetection}. The FCOS model is trained with the 2x (24-epoch) training schedule for a fair comparison with the X101 (short for ResNeXt-101~\cite{xie2017aggregated}) baseline from the same code base~\cite{mmseg2020}, and the other results with Cascade Mask R-CNN all use 3x (36-epoch). Again, we simply replace the backbone and do not use any advanced techniques. Table~\ref{table-coco} shows RepLKNets outperform ResNeXt-101-64x4d by up to 4.4 mAP while have fewer parameters and lower FLOPs. Note that the results may be further improved with the advanced techniques like HTC~\cite{chen2019hybrid}, HTC++~\cite{swin}, Soft-NMS~\cite{bodla2017soft} or a 6x (72-epoch) schedule. Compared to Swin, RepLKNets achieve higher or comparable mAP with fewer parameters and lower FLOPs. Notably, RepLKNet-XL achieves an mAP of 55.5, which demonstrates the scalability again.
	
	\setlength{\tabcolsep}{2pt}
	\begin{table}
		\caption{Object detection on COCO. The FLOPs is computed with 1280800 inputs. The results of ResNeXt-101-64x4d + Cas Mask are reported by \cite{swin}. The results of 22K-pretrained Swin (without HTC++~\cite{swin}) are reported by \cite{liu2022convnet}.  indicates ImageNet-22K pretraining.  indicates pretrained with extra data.}
		\label{table-coco}
		\vspace{-0.25in}
		\begin{center}		
			\small
			\begin{tabular}{llcccccc}
				\hline
				Backbone				&	Method	&	 &	   &\makecell{Param \\ (M)}	& \makecell{FLOPs \\ (G)}		\\
				\hline
				\textbf{RepLKNet-31B}    &   FCOS    &   \textbf{47.0}   &   -   &   87  &   437\\
				X101-64x4d                  &   FCOS    &    42.6   &   -   &   90  &   439 \\
				\hline
				\textbf{RepLKNet-31B}	&	Cas Mask 	&	\textbf{52.2}    &   \textbf{45.2}   &   137    &   965    \\
				X101-64x4d		&	Cas Mask 	&	48.3    &   41.7   &  140		&	972\\
				ResNeSt-200		&	Cas R-CNN \cite{cai2019cascade}	&	49.0    &   -	&	- & -	\\				
				Swin-B      	&	Cas Mask 	&	51.9    &   45.0    &   145    &   982    \\
				\hline
				\textbf{RepLKNet-31B}~	&	Cas Mask 	&	\textbf{53.0}    &   \textbf{46.0}   &   137    &   965       \\
				Swin-B~	                &	Cas Mask 	&	\textbf{53.0}    &   45.8            &   145    &   982\\
				\hline
				\textbf{RepLKNet-31L}~ & Cas Mask  & \textbf{53.9}    & 46.5  &   229 &   1321\\
				Swin-L~     & Cas Mask  & \textbf{53.9}  &   \textbf{46.7}    &   254   &   1382  \\
				\hline
				\textbf{RepLKNet-XL}~ & Cas Mask & \textbf{55.5} & \textbf{48.0} &   392 &   1958 \\ 
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.35in}
	\end{table}
	
	

	
	\section{Discussions}
	\label{sec:largeerf}
	
	\subsection{Large-Kernel CNNs have Larger ERF than Deep Small-Kernel Models}
	
	We have demonstrated large kernel design can significantly boost \emph{CNNs} (especially on \emph{downstream} tasks). However, it is worth noting that large kernel can be expressed by a series of small convolutions \cite{simonyan2014very}, \eg, a 77 convolution can be decomposed into a stack of three 33 kernels without information loss (more channels are required after the decomposition to maintain the degree of freedom). Given that fact, a question naturally comes up: why do conventional CNNs, which may contain tens or hundreds of small convolutions (\eg, \emph{ResNets}~\cite{he2016deep}), still behave inferior to large-kernel networks?
	
	We argue that in terms of obtaining large receptive field, a single large kernel is much more \emph{effective} than many small kernels. \textbf{First}, according to the theory of \emph{Effective Receptive Field (ERF)}~\cite{erf}, ERF is proportion to , where  is the kernel size and  is the depth, \ie, number of layers. In other words, ERF grows linearly with the kernel size while sub-linearly with the depth. \textbf{Second}, the increasing depth introduces optimization difficulty~\cite{he2016deep}. Although ResNets seem to overcome the dilemma, managing to train a network with hundreds of layers, some works~\cite{veit2016residual,de2020batch} indicate ResNets might not be as deep as they appear to be. For example, \cite{veit2016residual} suggests ResNets behave like ensembles of shallow networks, which implies the ERFs of ResNets could still be very limited even if the depth dramatically increases. Such phenomenon is also empirically observed in previous works~\cite{kim2021dead}. To summarize, large kernels design requires fewer layers to obtain large ERFs and avoids the optimization issue brought by the increasing depth. 

	To support our viewpoint, we choose ResNet-101/152 and the aforementioned RepLKNet-13/31 as the representatives of small-kernel and large-kernel models, which are all well-trained on ImageNet, and test with 50 images from the ImageNet validation set resized to 10241024. To visualize the ERF, we use a simple yet effective method (code released at \cite{RepLKNet-pytorch}) as introduced in Appendix B, following \cite{kim2021dead}. Briefly, we produce an \emph{aggregated contribution score matrix}  (10241024), where each entry  () measures the contribution of the corresponding pixel on the input image to the central point of the feature map produced by the last layer. Fig.~\ref{fig-rf} shows the high-contribution pixels of ResNet-101 gather around the central point, but the outer points have very low contributions, indicating a limited ERF. ResNet-152 shows a similar pattern, suggesting the more 33 layers do not significantly increase the ERF. On the other hand, the high-contribution pixels in Fig.~\ref{fig-rf} (C) are more evenly distributed, suggesting RepLKNet-13 attends to more outer pixels. With larger kernels, RepLKNet-31 makes the high-contribution pixels spread more uniformly, indicating an even larger ERF. 
	
		\setlength{\tabcolsep}{4pt}
	\begin{table}[t]
		\caption{Quantitative analysis on the ERF with the high-contribution area ratio . A larger  suggests a smoother distribution of high-contribution pixels, hence larger ERF.}
		\label{table-rf}
		\vspace{-0.2in}
		\begin{center}
			\small
			\begin{tabular}{lcccc}
				\hline
				& 		&				&		&	\\
				\hline
				ResNet-101		&	0.9\%		&	1.5\%	&	3.2\%	& 22.4\%	\\
				ResNet-152		&	1.1\%		&	1.8\%	&	3.9\%	& 34.4\%\\
				RepLKNet-13		&	11.2\%		&	17.1\%	&	30.2\%	&	96.3\%\\		
				RepLKNet-31		&	16.3\%		&	24.7\%	&   43.2\%	&	98.6\%	\\
				\hline
			\end{tabular}
			
		\end{center}
		\vspace{-0.2in}
	\end{table}
	
	Table~\ref{table-rf} presents a quantitative analysis, where we report the high-contribution area ratio  of a minimum rectangle that covers the contribution scores over a given threshold . For examples, 20\% of the pixel contributions ( values) of ResNet-101 reside within a 103103 area at the center, so that the area ratio is  with . We make several intriguing observations. \textbf{1)} While being significantly deeper, ResNets have much smaller ERFs than RepLKNets. For example, over 99\% of the contribution scores of ResNet-101 reside within a small area which takes up only 23.4\% of the total area, while such area ratio of RepLKNet-31 is 98.6\%, which means most of pixels considerably contribute to the final predictions. \textbf{2)} Adding more layers to ResNet-101 does not effectively enlarge the ERF, while scaling up the kernels improves the ERF with marginal computational costs.
	

	
	\subsection{Large-Kernel Models are More Similar to Human in Shape Bias}
	
	We have found out that RepLKNet-31B has much higher shape bias than Swin Transformer and small-kernel CNNs.
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\linewidth]{shapebias.pdf}
			\vspace{-0.25in}
			\caption{Shape bias of RepLKNet, Swin, and ResNet-152 pretrained on ImageNet-1K or 22K. The scatters represent the shape bias of 16 categories, and the vertical lines are the averages across categories (note RepLKNet-3 and ResNet-152 are very close).}
			\label{fig-shapebias}
			\vspace{-0.25in}
		\end{center}
	\end{figure}
	
	A recent work~\cite{tuli2021convolutional} reported that vision transformers are more similar to the human vision systems in that they make predictions more based on the overall shapes of objects, while CNNs focus more on the local textures. We follow its methodology and use its toolbox~\cite{modelvshuman} to obtain the shape bias (\eg, the fraction of predictions made based on the shapes, rather than the textures) of RepLKNet-31B and Swin-B pretrained on ImageNet-1K or 22K, together with two small-kernel baselines, RepLKNet-3 and ResNet-152. Fig.~\ref{fig-shapebias} shows that RepLKNet has higher shape bias than Swin. Considering RepLKNet and Swin have similar overall architectures, we reckon shape bias is closely related to the \emph{Effective Receptive Field} rather than the concrete formulation of self-attention (\ie, the \emph{query-key-value} design). This also explains \textbf{1)} the high shape bias of \emph{ViTs}~\cite{vit} reported by \cite{tuli2021convolutional} (since ViTs employ global attention), \textbf{2)} the low shape bias of 1K-pretrained Swin (attention within local windows), and \textbf{3)} the shape bias of the small-kernel baseline RepLKNet-3, which is very close to ResNet-152 (both models are composed of  convolutions). 
	
	

	\subsection{Large Kernel Design is a Generic Design Element that Works with ConvNeXt} Replacing the 77 convolutions in ConvNeXt~\cite{liu2022convnet} by kernels as large as 3131 brings significant improvements, \eg, ConNeXt-Tiny + large kernel \textgreater ConNeXt-Small , and ConNeXt-Small + large kernel \textgreater ConNeXt-Base. 
	
		\setlength{\tabcolsep}{3pt}
	\begin{table*}
		\caption{ConvNeXt with different kernel sizes. The models are pretrained on ImageNet-1K in 120 epochs with 224224 input and finetuned on ADE20K with UperNet in 80K iterations. On ADE20K, we test the \emph{single-scale} mIoU, and compute the FLOPs with input of 2048512, following Swin.}
		\label{table-convnext}
		\vspace{-0.25in}
		\begin{center}		
			\small
			\begin{tabular}{ll|ccc|ccc}
				\hline
				& & \multicolumn{3}{c|}{ImageNet} & \multicolumn{3}{c}{ADE20K} \\
				Kernel size 	&		Architecture		&	Top-1		& \makecell{Params}		&\makecell{FLOPs}	&	mIoU	& Params	& FLOPs	\\
				\hline
				7-7-7-7			&	ConvNeXt-Tiny	&	     81.0           &   29M       &   4.5G   &   44.6  	 	&   60M		&   939G\\
				7-7-7-7			&	ConvNeXt-Small	&	     82.1           &   50M       &   8.7G   &   45.9  	 	&   82M  	&   1027G\\
				7-7-7-7			&	ConvNeXt-Base	&	     82.8           &   89M       &   15.4G  &   47.2  	 	&   122M  	&   1170G\\
				\hline
				31-29-27-13		&	ConvNeXt-Tiny	&	     81.6           &   32M       &   6.1G   &   \textbf{46.2}  	 	&   64M  &   973G\\
				31-29-27-13		&	ConvNeXt-Small	&	     82.5           &   58M       &   11.3G  &   \textbf{48.2}  	 	&   90M  &   1081G\\
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.3in}
	\end{table*}
	
	We use the recently proposed ConvNeXt~\cite{liu2022convnet} as the benchmark architecture to evaluate large kernel as a \emph{generic design element}. We simply replace the 77 convolutions in ConvNeXt~\cite{liu2022convnet} by kernels as large as 3131. The training configurations on ImageNet (120 epochs) and ADE20K (80K iterations) are identical to the results shown in Sec.~\ref{sect-replknet-120}. Table~\ref{table-convnext} shows that though the original kernels are already 77, further increasing the kernel sizes still brings significant improvements, especially on the downstream task: with kernels as large as 3131, ConvNeXt-Tiny outperforms the original ConvNeXt-Small, and the large-kernel ConvNeXt-Small outperforms the original ConvNeXt-Base. Again, such phenomena demonstrate that kernel size is an important scaling dimension.
	
	
	\subsection{Large Kernels Outperform Small Kernels with High Dilation Rates} 
	
	Please refer to Appendix C for details.
	


	
	\section{Limitations}
	
	Although large kernel design greatly improves CNNs on both ImageNet and downstream tasks, however, according to Table~\ref{table-to-swin}, as the scale of data and model increases, \emph{RepLKNets} start to fall behind \emph{Swin Transformers}, \eg, the ImageNet top-1 accuracy of RepLKNet-31L is 0.7\% lower than Swin-L with ImageNet-22K pretraining (while the downstream scores are still comparable). It is not clear whether the gap is resulted from suboptimal hyper-parameter tuning or some other fundamental drawback of CNNs which emerges when data/model scales up. We are working in progress on the problem. 
	
	\section{Conclusion}
	
	This paper revisits large convolutional kernels, which have long been neglected in designing \emph{CNN} architectures. We demonstrate that using a few large kernels instead of many small kernels results in larger \emph{effective receptive field} more efficiently, boosting CNN's performances especially on downstream tasks by a large margin, and greatly closing the performance gap between CNNs and ViTs when data and models scale up. We hope our work could advance both studies of CNNs and ViTs. On one hand, for CNN community, our findings suggest that we should pay special attention to \emph{ERFs}, which may be the key to high performances. On the other hand, for ViT community, since large convolutions act as an alternative to \emph{multi-head self-attentions} with similar behaviors, it may help to understand the intrinsic mechanism of self-attentions.   
	
	
{\small
		\bibliographystyle{ieee_fullname}
		\bibliography{replknetbib}
	}
	
	\newpage
	
	
	\section*{Appendix A: Training Configurations}
	
	\subsection*{ImageNet-1K}
	
	For training MobileNet V2 models (Sec.~\ref{sect-3}), we use 8 GPUs, an SGD optimizer with momentum of 0.9, a batch size of 32 per GPU, input resolution of 224224, weight decay of , learning rate schedule with 5-epoch warmup, initial value of 0.1 and cosine annealing for 100 epochs. For the data augmentation, we only use random cropping and left-right flipping, as a common practice.
	
	For training RepLKNet models (Sec.~\ref{sect-replknet-120}),we use 32 GPUs and a batch size of 64 per GPU to train for 120 epochs. The optimizer is AdamW~\cite{loshchilov2017decoupled} with momentum of 0.9 and weight decay of 0.05. The learning rate setting includes an initial value of , cosine annealing and 10-epoch warm-up. For the data augmentation and regularization, we use RandAugment~\cite{cubuk2020randaugment} (``rand-m9-mstd0.5-inc1'' as implemented by timm~\cite{timm-aa}), label smoothing coefficient of 0.1, mixup~\cite{zhang2017mixup} with , CutMix with , Rand Erasing~\cite{zhong2020random} with probability of 25\% and Stochastic Depth with a drop-path rate of 30\%, following the recent works~\cite{swin,deit,bao2021beit,liu2022convnet}. The RepLKNet-31B reported in Sec.~\ref{sect-imgnet} is trained with the same configurations except the epoch number of 300 and drop-path rate of 50\%.
	
	For finetuning the 224224-trained RepLKNet-31B with 384384, we use 32 GPUs, a batch size of 32 per GPU, initial learning rate of , cosine annealing, 1-epoch warm-up, 30 epochs, model EMA (Exponential Moving Average) with momentum of , the same RandAugment as above but \emph{no CutMix nor mixup}.
	
	
	\subsection*{ImageNet-22K Pretraining and 1K Finetuning}
	
	For pretraining RepLKNet-31B/L on ImageNet-22K, we use 128 GPUs and a batch size of 32 per GPU to train for 90 epochs with a drop-path rate of 10\%. The other configurations are the same as the aforementioned ImageNet-1K pretraining. 
	
	Then for finetuning RepLKNet-31B with 224224, we use 16 GPUs, a batch size of 32 per GPU, drop-path rate of 20\%, initial learning rate of , cosine annealing, model EMA with momentum of  to finetune for 30 epochs. Note again that we use the same RandAugment as above but \emph{no CutMix nor mixup}.
	
	For finetuning RepLKNet-31B/L with 384384, we use 32 GPUs and a batch size of 16 per GPU, and the drop-path rate is raised to 30\%.
	
	
	\subsection*{RepLKNet-XL and Semi-supervised Pretraining}
	
	We continue to scale up our architecture and train a ViT-L~\cite{vit} level model named RepLKNet-XL. We use , , , and introduce inverted bottleneck with expansion ratio of 1.5 to each RepLK Block.
	During pretraining, we use a private semi-supervised dataset named \emph{MegData73M}, which contains 38 million labeled images and 35 million unlabeled ones. Labeled images come from public and private classification datasets such as ImageNet-1K, ImageNet-22K and Places365~\cite{zhou2017places}. Unlabeled images are selected from YFCC100M~\cite{thomee2016yfcc100m}. We design a multi-task label system according to \cite{ghiasi2021multi}, and utilize soft \textit{pseudo} labels which are offline generated by multiple task-specific ViT-Ls wherever human annotations are unavailable. We pretrain our model for up to 15 epochs with similar configurations as ImageNet-1K pretraining. We do \emph{not use CutMix or mixup}, decrease drop-path rate to 20\%, and use a lower initial learning rate of  and a total batch size of 2048. Structural Re-parameterization is omitted because it only brings less than 0.1\% performance gain on such a large-scale dataset. In other words, we observe that the inductive bias (re-parameterization with small kernels) becomes less important as the data become bigger, which is similar to the discoveries reported by ViT~\cite{vit}.
	
	We finetune on ImageNet-1K with input resolution of 320320 for 30 epochs following BeiT~\cite{bao2021beit}, except for a higher learning rate of  and stage-wise learning rate decay of 0.4. Finetuning with a higher resolution of 384384 brings no further improvements. For downstream tasks, we use the default training setting except for a drop-path rate of 50\% and stage-wise learning rate decay.
	
	\section*{Appendix B: Visualizing the ERF}
	
	Formally, let  be the input image,  be the final output feature map, we desire to measure the contributions of every pixel on  to the central points of every channel on , \ie, , which can be simply implemented via taking the derivatives of  to  with the auto-grad mechanism. Concretely, we sum up the central points, take the derivatives to the input as the pixel-wise contribution scores and remove the negative parts (denoted by ). Then we aggregate the entries across all the examples and the three input channels, and take the logarithm for better visualization. Formally, the aggregated contribution score matrix  is given by
	
	\vskip -0.2in
	
	
	Then we respectively rescale  of each model to  via dividing the maximum entry for the comparability across models.
	
 	
	\section*{Appendix C: Dense Convolutions \vs Dilated Convolutions}
	\label{sec:comp_dilated}
	
	As another alternative to implement large convolutions, \emph{dilated convolution} \cite{yu2015multi,chen2017deeplab} is a common component to increase the \emph{receptive field (RF)}. However, Table~\ref{table-mob2-dilation} shows though a depth-wise dilated convolution may have the same maximum RF as a depth-wise dense convolution, its representational capacity is much lower, which is expected because it is mathematically equivalent to a \emph{sparse} large convolution. Literature (\eg, \cite{wang2018understanding,xie2021segformer}) further suggests that dilated convolutions may suffer from \emph{gridding problem}. We reckon the drawbacks of dilated convolutions could be overcome by mixture of convolutions with different dilations, which will be investigated in the future. 
	
	\begin{table}
		\caption{MobileNet V2 with all regular DW 33 layers replaced by 33 dilated layers.}
		\label{table-mob2-dilation}
		\vspace{-0.2in}
		\begin{center}
			\small
			\begin{tabular}{lcccccc}
				\hline
				Max RF &	Kernel size		& Dilation			&	\makecell{ImageNet acc}     &   Params   & FLOPs	\\
				\hline
				9					&	99		&	-				&	72.67	    & 4.0M  &   319M  \\
				9					&	33		&	4				&	57.23	    & 3.5M  &   300M  \\
				\hline
				13					&	1313	&	-				&	72.53	    & 4.6M  &   361M  \\
				13					&	33		&	6				&	51.21	    & 3.5M  &   300M  \\
				\hline
			\end{tabular}
		\end{center}
		\vspace{-0.2in}
	\end{table}
	
	
	
	\section*{Appendix D: Visualizing the Kernel Weights with Small-Kernel Re-parameterization}
	
	\begin{figure}[t]
		\centering
		\begin{subfigure}{0.66\linewidth}
			\includegraphics[width=\linewidth]{mbv2_kernel_before.pdf}
			\label{fig-kernel-before} 
			\vskip -0.3in
			\caption{Before re-param.}
		\end{subfigure}
		\begin{subfigure}{0.66\linewidth}
			\includegraphics[width=\linewidth]{mbv2_kernel_after.pdf}
			\label{fig-kernel-after}
			\vskip -0.3in
			\caption{After re-param.}
		\end{subfigure}
		\begin{subfigure}{0.66\linewidth}
			\includegraphics[width=\linewidth]{mbv2_kernel_noreparam.pdf}
			\label{fig-kernel-noreparam}
			\vskip -0.3in
			\caption{Without re-param.}
		\end{subfigure}
		\caption{Parameters of 1313 kernels in MobileNet V2 aggregated into 1313 matrices.}
		\label{fig-tradeoff}
		\vskip -0.15in
	\end{figure}
	
	We visualize the weights of the re-parameterized 1313 kernels. Specifically, we investigate into the MobileNet V2 models both with and without 33 re-parameterization. As Shown in Sec.~\ref{sect-3} (Guideline 3) , the ImageNet scores are 73.24\% and 72.53\%, respectively. We use the first stride-1 1313 conv in the last stage (\ie, the stage with input resolution of 77) as the representative, and aggregate (take the absolute value and sum up across channels) the resultant kernel into a 1313 matrix, and respectively rescale to  for the comparability. For the model with 33 re-param, we show both the original 1313 kernel (only after BN fusion) and the result after re-param (\ie, adding the 33 kernel onto the central part of 1313). For the model without re-param, we also fuse the BN for the fair comparison.
	
	We observe that every aggregated kernel shows a similar pattern: the central point has the largest magnitude; generally, points closer to the center have larger values; and the ``skeleton'' parameters (the 131 and 113 criss-cross parts) are relatively larger, which is consistent with the discovery reported by \emph{ACNet}~\cite{ding2019acnet}. But the kernel with 33 re-param differs in that the central 33 part of the resultant kernel is further enhanced, which is found to improve the performance.
	

	
	
	
\end{document}
