\documentclass{LMCS}

\def\doi{8 (3:12) 2012}
\lmcsheading {\doi}
{1--27}
{}
{}
{Nov.~\phantom06, 2009}
{Aug.~13, 2012}
{}

\usepackage{hyperref,enumerate}



\usepackage{proof}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{epsf}
\usepackage{graphics}
\usepackage{wrapfig}
\usepackage{mathrsfs}
\usepackage{url}
\usepackage{calc}
\usepackage{amsmath,amssymb,amsfonts,mathrsfs,latexsym,stmaryrd}
\usepackage[all]{xy}



\newcommand{\funone}{\mathbf{f}}
\newcommand{\funtwo}{\mathbf{g}}
\newcommand{\funthree}{\mathbf{h}}
\newcommand{\funfour}{\mathbf{p}}
\newcommand{\conone}{\mathbf{c}}
\newcommand{\contwo}{\mathbf{d}}
\newcommand{\conthree}{\mathbf{e}}
\newcommand{\patone}{s}
\newcommand{\pattwo}{r}
\newcommand{\patthree}{q}
\newcommand{\seqpone}{\alpha}
\newcommand{\seqptwo}{\beta}
\newcommand{\varone}{x}
\newcommand{\vartwo}{y}
\newcommand{\varthree}{z}
\newcommand{\varfour}{w}
\newcommand{\varfive}{f}
\newcommand{\varsix}{g}
\newcommand{\varseven}{h}
\newcommand{\lambdaone}{M}
\newcommand{\lambdatwo}{N}
\newcommand{\lambdathree}{L}
\newcommand{\lambdafour}{P}
\newcommand{\lambdafive}{Q}
\newcommand{\valueone}{V}
\newcommand{\valuetwo}{W}
\newcommand{\valuethree}{Z}
\newcommand{\termone}{t}
\newcommand{\termtwo}{u}
\newcommand{\termthree}{v}
\newcommand{\termfour}{w}
\newcommand{\termfive}{d}
\newcommand{\termsix}{e}
\newcommand{\termseven}{f}
\newcommand{\cltermone}{X}
\newcommand{\cltermtwo}{Y}
\newcommand{\sigone}{\Sigma}
\newcommand{\sigtwo}{\Theta}
\newcommand{\vsone}{V}
\newcommand{\vstwo}{W}
\newcommand{\ordone}{\alpha}
\newcommand{\ordtwo}{\beta}
\newcommand{\labelone}{\delta}
\newcommand{\labeltwo}{\sigma}
\newcommand{\rootone}{r}
\newcommand{\roottwo}{s}
\newcommand{\verone}{v}
\newcommand{\vertwo}{w}
\newcommand{\verthree}{z}
\newcommand{\tgone}{G}
\newcommand{\tgtwo}{H}
\newcommand{\tgthree}{J}
\newcommand{\tgfour}{K}
\newcommand{\tgfive}{I}
\newcommand{\rrone}{\rho}
\newcommand{\rrtwo}{\sigma}
\newcommand{\homone}{\varphi}
\newcommand{\homtwo}{\psi}
\newcommand{\nat}[1]{\lceil #1\rceil}
\newcommand{\rewrite}[1]{\stackrel{#1}{\longrightarrow}}

\newcommand{\appTRS}{\mathbf{app}}
\newcommand{\cappTRSW}{\mathbf{capp}}
\newcommand{\constr}[2]{\mathbf{c}_{#1,#2}}
\newcommand{\LambdatoTRS}[1]{[#1]_{\Phi}}
\newcommand{\TRStolambda}[1]{\langle{#1}\rangle_{\Lambdaterms}}
\newcommand{\TRStolambdaI}[1]{\langle\!\langle #1\rangle\!\rangle_{\Lambdaterms}}
\newcommand{\TRStolambdaII}[1]{[#1]_{\Lambdaterms}}
\newcommand{\TRSonetolambdaI}[1]{\langle\!\langle #1\rangle\!\rangle_{\Lambdaterms}}
\newcommand{\TRSonetolambdaII}[1]{[#1]_{\Lambdaterms}}
\newcommand{\TRSonetolambdaIII}[1]{\langle\!|#1|\!\rangle_{\Lambdaterms}}
\newcommand{\TRSWtolambda}[1]{\langle #1\rangle_{\Lambdaterms}}
\newcommand{\TRStoGRS}[1]{[#1]}
\newcommand{\TRStoGRSW}[1]{[#1]_{\Xi}}
\newcommand{\GRStoTRS}[1]{\langle #1\rangle}
\newcommand{\LambdatoTRSW}[1]{[#1]_{\Psi}}
\newcommand{\LambdatoTRSWaux}[1]{\{#1\}_{\Psi}}
\newcommand{\errorterm}{\bot}
\newcommand{\arity}[1]{\mathit{ar}(#1)}
\newcommand{\dsize}[1]{||#1||}

\newcommand{\Variables}{\Upsilon}
\newcommand{\Lambdaterms}{\Lambda}
\newcommand{\Values}{\Xi}
\newcommand{\Functions}[1]{\Sigma_{#1}}
\newcommand{\Rules}[1]{\mathcal{R}_{#1}}
\newcommand{\Graphs}[1]{\mathcal{G}_{#1}}
\newcommand{\TRS}{\Phi}
\newcommand{\TRSterms}{\mathcal{T}(\Phi)}
\newcommand{\TRSvarterms}{\mathcal{V}(\Phi,\Variables)}
\newcommand{\TRSconterms}{\mathcal{C}(\Phi)}
\newcommand{\TRSone}{\Xi}
\newcommand{\TRStwo}{\Theta}
\newcommand{\TRStermsp}[1]{\mathcal{T}(#1)}
\newcommand{\TRSpatsp}[1]{\mathcal{P}(#1,\Variables)}
\newcommand{\TRSvartermsp}[1]{\mathcal{V}(#1,\Variables)}
\newcommand{\TRScontermsp}[1]{\mathcal{C}(#1)}
\newcommand{\GRS}{\Theta}
\newcommand{\GRSW}{\Xi}
\newcommand{\CtoCG}[1]{[#1]}
\newcommand{\CGtoC}[1]{\langle #1\rangle}
\newcommand{\TRSW}{\Psi}
\newcommand{\TRSWterms}{\mathcal{T}(\Psi)}
\newcommand{\TRSWvarterms}{\mathcal{V}(\Psi,\Variables)}
\newcommand{\TRSWconterms}{\mathcal{C}(\Psi)}
\newcommand{\domain}[1]{\mathit{dom}(#1)}
\newcommand{\srrone}{\Rules{}}
\newcommand{\srrtwo}{\mathcal{S}}
\newcommand{\sgrone}{\Graphs{}}
\newcommand{\Time}[1]{\mathit{Time}_v(#1)}
\newcommand{\Timew}[1]{\mathit{Time}_h(#1)}

\newcommand{\TMone}{\mathcal{M}}

\newcommand{\rewrTRS}{\rightarrow}
\newcommand{\rewrTRSW}{\rightarrow}
\newcommand{\rewr}[1]{\rightarrow_{#1}}
\newcommand{\rewrlambdav}{\rightarrow_v}
\newcommand{\rewrlambdah}{\rightarrow_h}
\newcommand{\rewrgraph}{\rightarrow}

\newcommand{\subgr}[2]{#1\downarrow #2}

\newcommand{\cod}[1]{\ulcorner #1\urcorner}


\newcommand{\A}{\mathbb{A}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\U}{\mathbb{U}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\bn}{\rightarrow_{n} }
\newcommand{\bv}{\rightarrow_{v} }
\newcommand{\canonic}{\ensuremath{\cors{C}\!\mathit{an}}}
\newcommand{\cors}[1]{\ensuremath{\mathscr{#1}}}
\newcommand{\cpromote}[5]{\ensuremath{\nabla\left(#1\right)
    \left[{}^{#2}/#3,\ldots,{}^{#4}/#5\right]}}
\newcommand{\FV}[1]{\mathtt{FV}(#1)}
\newcommand{\length}[1]{|#1|}
\newcommand{\plength}[2]{|#1|_{#2}}



\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}




\newenvironment{varitemize}
{
\begin{list}{\labelitemi}
{\setlength{\itemsep}{0.0mm}
 \setlength{\topsep}{0.0mm}
 \setlength{\parindent}{0.0mm}
 \setlength{\parskip}{0.0mm}
 \setlength{\parsep}{0.0mm}
 \setlength{\partopsep}{0.0mm}
 \setlength{\leftmargin}{15pt}
 \setlength{\labelsep}{5pt}
 \setlength{\labelwidth}{10pt}}}
{
 \end{list} 
}

\newenvironment{varitemizeii}
{
\begin{list}{\labelitemiv}
{\setlength{\itemsep}{0.0mm}
 \setlength{\topsep}{0.0mm}
 \setlength{\parindent}{0.0mm}
 \setlength{\parskip}{0.0mm}
 \setlength{\parsep}{0.0mm}
 \setlength{\partopsep}{0.0mm}
 \setlength{\leftmargin}{15pt}
 \setlength{\labelsep}{5pt}
 \setlength{\labelwidth}{10pt}}}
{
 \end{list} 
}

\newcounter{number}

\newenvironment{varenumerate}
{\begin{list}{\arabic{number}.}
  {
   \usecounter{number}
   \setlength{\labelwidth}{4.0mm}
   \setlength{\labelsep}{2.0mm}
\setlength{\itemindent}{0.0mm}
\setlength{\itemsep}{0.0mm}
   \setlength{\topsep}{0.0mm}
   \setlength{\parskip}{0.0mm}
   \setlength{\parsep}{0.0mm}
   \setlength{\partopsep}{0.0mm}
  }
}
{\end{list}}

 \begin{document}

\title[On Constructor Rewrite Systems and the Lambda-Calculus]{On Constructor Rewrite Systems\\ and the Lambda-Calculus\rsuper*}
\author[U.~Dal Lago]{Ugo Dal Lago}
\author[S.~Martini]{Simone Martini}
\address{Universit\`a di Bologna, and INRIA Sophia Antipolis}
\email{\{dallago, martini\}@cs.unibo.it}

\keywords{lambda calculus, term rewriting, implicit computational
  complexity} 
\subjclass{F.4.1}  \titlecomment{{\lsuper*}This paper is an extended
  version of~\cite{DLMicalp}, appeared in the proceedings of ICALP
  2009.}

\begin{abstract}\noindent
We prove that orthogonal constructor term rewrite systems and -calculus with weak 
(i.e., no reduction is allowed under  the scope of a -abstraction)
call-by-value reduction can simulate each other with a linear overhead.
In particular, weak call-by-value beta-reduction can be simulated by an orthogonal 
constructor term rewrite system in the same number of reduction steps. 
Conversely, each reduction in a term rewrite system can be simulated by a constant number 
of beta-reduction steps. This is relevant to implicit computational complexity, because the number of beta steps 
to normal form is polynomially related to the actual cost (that is, as performed on a Turing 
machine) of normalization, under weak call-by-value reduction. Orthogonal constructor term rewrite systems 
and -calculus are thus both polynomially related to Turing machines, taking as notion 
of cost their natural parameters.
\end{abstract}

\maketitle
\section*{Introduction}
\par
Implicit computational complexity is a young research area, whose main aim is the description of complexity phenomena 
based on language restrictions, and not on external 
measure conditions or on explicit machine models. 
It borrows techniques and results from mathematical logic (model theory, recursion theory, and proof theory)
and in doing so it has allowed the
incorporation of aspects of computational complexity into areas such as formal methods in software development
and programming language design. The most developed area of implicit computational complexity is probably the
model theoretic one --- finite model theory being a very successful way to describe
complexity classes. In the design of  programming language tools (e.g., type systems), however, syntactical
techniques prove more useful. In the last years we have seen much work restricting
recursive schemata and developing general proof theoretical techniques to enforce
resource bounds on programs. 
Important achievements have been the characterizations of several complexity classes
by means of limitations of recursive definitions (e.g.,~\cite{Bellantoni92CC,Leivant95RRI}) and, more recently, by 
using the ``\emph{light}'' fragments of 
linear logic~\cite{Girard98ic}. 
Moreover, rewriting techniques such as recursive path orderings and
the interpretation method have been proved useful in the field~\cite{Marion00}.
By borrowing the terminology from software design technology, we may dub this 
area as implicit computational complexity \emph{in the large}, aiming at a broad, global view on complexity classes.
We may have also an implicit computational complexity \emph{in the small} ---
using logic to study single machine-free models of computation. Indeed, many models of computations do not come
with a natural cost model --- a definition of cost which is both intrinsically rooted in the model of 
computation, and, at the same time, it is polynomially related to the cost of implementing that
model of computation on a standard Turing machine. The main example is the -calculus: the most natural intrinsic
parameter of a computation is its number of beta-reductions, but this very parameter bears no
relation, in general, with the actual cost of performing that computation, since a beta-reduction may involve the duplication of arbitrarily big subterms\footnote{
In full beta-reduction, the size of the duplicated term is indeed arbitrary and does not depend on
the size of the original term the reduction started from. The situation is much different with weak 
reduction, as we will see.}.
What we call implicit computational complexity in the small, therefore, gives complexity significance to
notions and results for computation models where such natural cost measures do not exist, or are
not obvious. In particular, it looks for cost-explicit simulations between such computational models.

The present paper applies this viewpoint to the relation between -calculus and orthogonal constructor 
term rewrite systems (OCRSs in the following). We will prove that these two computational
models simulate each other with a linear overhead. That each OCRS
could be simulated by -terms and beta-reduction is
well known, in view of the availability, in -calculus, of
fixed-point operators, which may be used to solve the mutual recursion expressed by 
first-order rewrite rules. 
Here (Section~\ref{Sect:CTR2L})
we make explicit the complexity content of this simulation, by showing that
any first-order rewriting of  steps can be simulated by  beta steps, where
 depends on the specific rewrite system but \emph{not} on the size of the involved terms.
Crucial to this result is the encoding of constructor terms using Scott's schema for numerals~\cite{Wadsworth80}.
Indeed, Parigot~\cite{Parigot89CSL} (see also~\cite{ParigotRoziere93}) shows that in the pure -calculus
Church numerals do not admit a predecessor working in a constant number of beta steps.
Moreover, Splawski and Urzyczyn~\cite{SplawskiU99} show that it is unlikely that our encoding 
could work in the typed context of System .

Section~\ref{sect:CRS} studies the converse -- the simulation of (weak) -calculus reduction by means of
OCRSs. We give an encoding of -terms into a (first-order) constructor term rewrite 
system. We write    for the map returning a first-order term, given a -term; 
 is, in a sense, a complete defunctionalization of the -term , 
where any -abstraction is represented by an atomic constructor. This is similar, 
although not technically the same, to the use of supercombinators (e.g.,~\cite{PJ87}).
We show that -reduction is simulated step by step by first-order rewriting 
(Theorem~\ref{theo:termreducible}).

As a consequence, taking the number of beta steps as a cost model for weak -calculus
is equivalent (up to a linear function) to taking the number of rewritings in OCRSs
systems. This is relevant to implicit computational complexity ``in the small'', because
the number of beta steps to normal form is polynomially related to the
actual cost (that is, as performed on a Turing machine) of normalization,
under weak call-by-value reduction. This has been established by 
Sands, Gustavsson, and Moran~\cite{Sands:Lambda02}, by a fine analysis of a -calculus implementation based on
a stack machine. OCRSs and -calculus
are thus both \emph{reasonable} machines (see the ``invariance thesis'' in~\cite{vanEmdeBoas90}),
taking as notion of cost their natural, intrinsic parameters.

As a byproduct, in Section~\ref{Sect:GraphRep} we sketch a different proof
of the cited result in~\cite{Sands:Lambda02}. Instead of using a stack machine, 
we show how we could implement
constructor term rewriting via term graph rewriting. In term graph rewriting we
avoid the explicit duplication and substitution inherent to
rewriting (and thus also to beta-reduction) and, moreover, we exploit the possible sharing of subterms. 
A more in-depth study of the complexity of 
(constructor) graph rewriting and its relations with (constructor) term rewriting can be found in another
paper by the authors~\cite{DLM09}.

In Section~\ref{sect:HeadReduction}, we show how to obtain the same results of the previous sections when 
call-by-name replaces call-by-value as the underlying strategy in the -calculus.



\section{Preliminaries}\label{sect:prelim}
The language we study is the pure untyped
-calculus endowed with weak (that is, we never reduce
under an abstraction)
call-by-value reduction. 
\begin{defi}
The following definitions are standard:
\begin{varitemize}
  \item
    \emph{Terms} are defined as follows:
    
    where  ranges a denumerable set .
     denotes the set of all -terms.
    We assume the existence of a fixed, total, order on ; this
    way  will be a sequence (without repetitions) of variables, not a set. A term
     is said to be \emph{closed} if ,
    where  is the empty sequence.
  \item
    Values are defined as follows:
    
\item
    Weak call-by-value reduction is denoted by 
    and is obtained by closing call-by-value reduction under
    any applicative context:
    
    Here  range over terms, while  ranges over values.
  \item
    The length  of  is defined as follows, by induction
    on : ,  
    and .
\end{varitemize}
\end{defi}

\noindent Weak call-by-value reduction enjoys many nice properties. In
particular, the one-step diamond property holds and, as a consequence,
the number of beta steps to normal form (if any) is invariant on the
reduction order~\cite{CIE2006} (this justifies the way we defined
reduction, which is slightly more general than Plotkin's
one~\cite{Plotkin75tcs}). It is then meaningful to define
 as \emph{the} number of beta steps to normal form
(or  if such a normal form does not exist). This cost model
will be referred to as the \emph{unitary} cost model, since each beta
(weak call-by-value) reduction step counts for  in the global cost
of normalization. Moreover, notice that -conversion is not
needed during reduction of closed terms: if
 and  is closed, then
the reduced redex will be in the form
, where  is a
\emph{closed} value. As a consequence, arguments are always closed and
open variables cannot be captured. Suppose  has  free
variables , and that
 are lambda-terms. The term

is sometimes denoted simply with
, taking advantage of
the implicit order between the variables.

The following lemma gives us a generalization of the fixed-point (call-by-value)
combinator (but observe the explicit limit  on the reduction length, 
in the spirit of implicit computational complexity in the small):
\begin{lem}[call-by-value fixpoint combinator]\label{lemma:mfpc}
For every natural number , there are terms  and a natural number  such
that for any sequence of values  and for any :

where .
\end{lem}
\proof
The terms we are looking for are simply the following:

where, for every ,

The natural number  is simply .
\qed

We only consider orthogonal and constructor rewriting in this paper.
A \emph{constructor term rewrite system} is a pair 
 where:
\begin{varitemize}
\item
  Symbols in the signature  can be either
  \emph{constructors} or \emph{function symbols}, each with its arity.
  \begin{varitemizeii} 
    \item
      Terms in  are those built
      from constructors and are called \emph{constructor terms}.
    \item
      Terms in  are those built
      from constructors and variables and are called \emph{patterns}.
    \item
      Terms in  are those built
      from constructor and function symbols and are called \emph{closed terms}.
    \item
      Terms in  are those built
      from constructors, functions symbols and variables in  and are dubbed
      \emph{terms}.
    \end{varitemizeii}
\item
  Rules in  are in the form 
  where  is a function symbol, 
  and .
  We here consider orthogonal rewrite systems only, i.e. we assume that no distinct two
  rules in  are overlapping and that every variable appears at most
  once in the lhs of any rule in . Moreover, we assume that reduction is
  call-by-value, i.e. the substitution triggering any reduction must assign
  \emph{constructor} terms to variables. This restriction is anyway natural in
  constructor rewriting.
\end{varitemize}
For any term  in a OCRS,  denotes 
the number of symbol occurrences, while  denotes
the number of occurrences of the symbol  in .
Similarly to -terms, if  contains instances of  variables
, the term 

is sometimes denoted simply with .
\section{From -Calculus to Constructor Term Rewriting}\label{sect:CRS}
In this section, we will prove that the -calculus, in the form introduced 
in Section~\ref{sect:prelim}, can be seen as a OCRS. This result will be spelled
out as follows.
\begin{varitemize}
\item
  An OCRS  on a signature  will be defined, together with two maps
   and
  .
  These two maps are \emph{not} bijections. However, 
  is injective, and  is the identity.
\item
  The concept of canonicity for terms in  will be defined. Moreover,
  the set of canonical terms will be shown to include 
  and to be closed by reduction.
\item
  Reduction of canonical terms will be shown to simulate weak call-by-value reduction
  on -terms, via . Conversely, the dynamics of
  -terms is proved to simulate rewriting of constructor terms again through
  .
\end{varitemize}
Altogether, the three ingredients above implies that  is a sound and complete
way of implementing call-by-value -reduction.

Let us start by defining  and the two functions allowing to translate terms
 into -terms and, conversely, -terms back into terms of . 
Canonicity can already be defined.
\begin{defi}[The OCRS , Canonicity]
The OCRS  is defined as a set of rules  over
an infinite signature . In particular:
\begin{varitemize} 
  \item
    The signature  includes the binary function symbol  and
    constructor symbols  for every  and
    every . The arity of
     is the length of . 
    To every term  we can associate
    a term  as follows:
    
    Observe that if  is closed, then . 
  \item
    The rewrite rules in  are all the rules in the following form:
    
    where .
  \item
    To every term  we can associate
    a term  as follows:
    
    where .
  \item
    A term  is \emph{canonical} if either  or
     where  and  are
    themselves canonical.
  \end{varitemize}
\end{defi}

\noindent Notice that the signature  contains an
infinite number of constructors.
\begin{exa}
Consider the -term .
 is
.
Moreover, , 
as expected. We have . Both  and  are
canonical. Finally, .
\end{exa}
The map  is injective, but not surjective. However:
\begin{lem}\label{lemma:invert}
For every -term , .
\end{lem}
\proof
By induction on :
\begin{varitemize}
\item
  If , then 
  
\item
  If , then
  
\item
  If , then
  
\end{varitemize}
This concludes the proof.
\qed
Canonicity holds for terms in  obtained as images of (closed) 
-terms via . Moreover, canonicity
is preserved by reduction in :
\begin{lem}\label{lemma:canonicity}
For every closed ,  is canonical.
Moreover, if  is canonical and , 
then  is canonical. 
\end{lem}
\proof
 is canonical for any  by
induction on the structure of  (which, by hypothesis, is either an
abstraction or an application  where both
 and  are closed). We can further prove that

is canonical whenever  and
 includes all the variables in :
\begin{varitemize}
\item
  If , then , which is
  clearly canonical.
\item
  If , then
  
  which is canonical, by IH.
\item
  If , then
  
  which is canonical, because each  (and hence also ) is in .
\end{varitemize}
This implies the rhs of any instance of a rule in  is canonical. As a consequence,
 is canonical whenever  and  is canonical.
This concludes the proof. 
\qed
For canonical terms, being a normal form is equivalent to being mapped to a normal
form via . This is not true, in general: take as a counterexample
,
which corresponds to  via .
\begin{lem}\label{lemma:NFcanonical}
A canonical term  is a normal form iff 
 is a normal form.
\end{lem}
\proof
If a canonical  is a normal form, then  does not contain the function symbol  and,
as a consequence,  is an abstraction, which is always a normal form.
Conversely, if  is a normal form, then
 is not in the form , because otherwise
 will be a (closed) application, which cannot be a normal form.
But since  is canonical, , which only contains terms
in normal form.
\qed
The following substitution lemma will be useful later.
\begin{lem}[Substitution]
For every term  and every ,

whenever  includes all the variables in .
\end{lem}
\proof
By induction on :
\begin{varitemize}
  \item
    If , then 
    
  \item
    If , then
    
  \item
    If , then
    
\end{varitemize}
This concludes the proof.
\qed
Two of the previous lemmas imply that if , 
 and  
includes all the variables in , then:

Reduction in  can be simulated by reduction in the -calculus,
provided the starting term is canonical.  
\begin{lem}\label{lemma:TRStolam}
If  is canonical and , then
.
\end{lem}
\proof
Consider the (instance of the) rewrite rule which
turns  into . Let it be

Clearly,

while, by~(\ref{equat:commute}):

which implies the thesis.
\qed
Conversely, call-by-value reduction in the -calculus can be simulated in :
\begin{lem}\label{lemma:lamtoTRS}
If ,  is canonical and , then
, where .
\end{lem}
\proof
Let  be the redex fired in  when rewriting
it to . There must be a corresponding subterm  of  such
that . Then

where .
and . Observe that, by definition,

where . Since  is canonical, 
. Moreover, since  is a value,
 itself is in . This implies
   
By~(\ref{equat:commute}):

This concludes the proof.
\qed
The previous lemmas together imply the following theorem, by which -calculus
normalization can be mimicked (step-by-step) by reduction in :
\begin{thm}[Term Reducibility]\label{theo:termreducible}
Let  be a closed term. The following
two conditions are equivalent:
\begin{enumerate}[\em 1.]
\item
   where  is in normal form;
\item
   where
   and  is in normal form.
\end{enumerate}
\end{thm}
\proof
Suppose , where  is in normal form. 
Then, by applying Lemma~\ref{lemma:lamtoTRS}, we obtain a term  such that
 and .
By Lemma~\ref{lemma:canonicity},  is canonical and, by Lemma~\ref{lemma:NFcanonical}, 
it is in normal form. Now, suppose  where
 and  is in normal form. By
applying  times Lemma~\ref{lemma:TRStolam}, we obtain
.
But  by Lemma~\ref{lemma:invert}
and  is a normal form by Lemma~\ref{lemma:NFcanonical}, since  and  are
canonical by Lemma~\ref{lemma:canonicity}.
\qed
There is another nice property of , that will be crucial in proving the main
result of this paper:
\begin{proposition}[Subterm Property]\label{prop:constred}
For every , for every  with 
and for every occurrence of a constructor  in ,  is
a subterm of .
\end{proposition}
\proof
Assume  and proceed
by induction on .
\qed
\begin{exa}
Let us consider the -term . Notice that 

Clearly . Moreover:

For every constructor  occurring in any term in the previous
reduction sequence,  is a subterm of . 
\end{exa}

A remark on  is now in order.  is an infinite OCRS, since  contains
an infinite amount of constructor symbols and, moreover, there are infinitely many
rules in . As a consequence, what we have presented here is an embedding
of the (weak, call-by-value) -calculus into an infinite OCRS. Consider, now,
the following scenario: suppose the -calculus is used to write a \emph{program} ,
and suppose that inputs to  form an infinite set of -terms  which can 
anyway be represented by a finite set of constructors in . In this scenario,
Proposition~\ref{prop:constred} allows to conclude the existence of finite
subsets of  and  such that \emph{every} 
(where ) can be reduced via  by using only
constructors and rules in those \emph{finite} subsets. As a consequence, we can see the
above schema as one that puts any program  
in correspondence to a \emph{finite} OCRS. Finally, observe that assuming \emph{data} to
be representable by a finite number of constructors in  is reasonable.
Scott's scheme~\cite{Wadsworth80}, for example, allows to represent any term in a given
free algebra in a finitary way, e.g. the natural number  becomes
 while
 becomes .
Church's scheme, on the other hand, does not have this property.
\subsection{An Example}
Consider the lambda terms 
and . It is easy to
verify that:

Therefore, both  and  diverge. Now:

Similarly, . Observe
that along the computation we reach the term , which is not the image of any -term. However, all constructor terms in the reduction are canonical and, moreover,
 is , 
the lambda term found along the reduction from  to .
\section{From Constructor Term Rewriting to the -Calculus}\label{Sect:CTR2L}
In this section, we will show that one rewriting step of any constructor rewrite
system can be simulated by a fixed number of weak call-by-value beta-reductions. As 
an easy consequence, -calculus will be shown to efficiently simulate any
OCRS. During this section we will assume fixed an  OCRS   over a finite signature . Let
 be the constructors of 
and let  be the function symbols of
. We will describe several constructions, which work independently of  (they only depends on the arity of the symbols).
\begin{varitemize}
\item
  A map  can
  be defined by recursion on the structure of the input. The map can be
  extended to constructors of  (which are \emph{not} terms by themselves),
  in such a way that for every , the lambda term 
  ``computes'' 
  when fed with ,
  for any . (See Definition~\ref{Def-TRSonetolambdaI}.)
\item
  Defining a map analogous to , but acting on closed terms (and not only on 
  constructor terms) is more delicate. Indeed, a term 
   
  does not necessarily rewrite to a constructor term,
  even if it does \emph{not} diverge ---
  the rewrite rules of  are not necessarily exhaustive and a deadlock
  can be reached. To handle this case we define a lambda term , which 
  will represent any deadlocked term.
\item
  Now a map 
  can be defined, in such a way that  reduces
  to  (where ) if 
   has normal form , but  reduces to 
  if  rewrites to a deadlock. The map 
  is defined compositionally, that is to say:
  
  In other words,  is completely specified by its
  behavior on constructors and function symbols.
\item
  While defining  is relatively easy 
  (Definition~\ref{Def-TRSonetolambdaII} and Lemma~\ref{Lemma-TRSonetolambdaII}),
   requires a form of pattern matching
  to be implemented in the -calculus (Lemma~\ref{lemma:pm} and Definition~\ref{Def-TRSonetolambdaII-fun}).
\item The complete simulation is stated in Theorem~\ref{theo:simulcl}. The example in~\ref{sect-EsempioTRStoLam} may be used along the section to clarify the definitions.
\end{varitemize}
We will first concentrate on constructor terms, encoding them as
-terms using Scott's schema~\cite{Wadsworth80}. 
\begin{defi}\label{Def-TRSonetolambdaI}
\begin{varitemize}
\item
  Constructor terms can be easily put in correspondence with -terms
  by way of a map  defined by induction as follows:
  
\item
  The function  can be extended to a map on constructors:
  
  Trivially, if   are in ,
  
  rewrites to  in
   steps. 
\item
  To represent an error value, we use the -term
  . A -term which is either  or in
  the form  is denoted with metavariables like
   or .
\end{varitemize}
\end{defi}

\noindent The map  defines encodings of constructor terms.
For function symbols our goal is defining another map  returning
a -term given any term  in , in such
a way that  and  implies
. Moreover,
 should rewrite to  whenever the rewriting of 
causes an error (i.e.  has a normal form containing
a function symbol).
First of all, we define the -term  corresponding
to a constructor .
\newcommand{\conslambda}{\mathit{CON}}
\newcommand{\patlambda}{\mathit{PAT}}
\begin{defi}\label{Def-TRSonetolambdaII}
  \begin{varitemize}
  \item
    For every , for every , and for every
    sequence of variables , 
    define the -term  
    by induction on :
    
    where:
    
  \item
    For every , the -term  is .
  \end{varitemize}
\end{defi}
We need to prove that  does what it is supposed to do. We show something slightly stronger:
\begin{lem}\label{Lemma-TRSonetolambdaII}
There is a constant  such that for any , for any , and for any
 in :

where , and

where , whenever  is either  or 
but at least one among  is .
\end{lem}
\proof
We proceed by induction on :
\begin{varitemize}
\item
  If , then
  
\item
  If , we
  use the following abbreviations:
  
  Let's distinguish two cases:
  \begin{varitemizeii}
  \item
    If , then:
    
  \item
    Let  be ,
    where . 
    Then:
    
    and, by the inductive hypothesis, the last term in the reduction sequence reduces to the correct
    normal form. The existence of a natural number  with the prescribed properties is clear
    by observing that none of the reductions above have a length which depends on the parameters
     and .
  \end{varitemizeii}
\end{varitemize}
This concludes the proof.
\qed
Interpreting function symbols is more difficult, since we have to ``embed'' the reduction rules
into the -term interpreting the function symbol. To do that, we need a preliminary
result to encode pattern matching. More specifically, suppose  are
non-overlapping sequences of patterns of the same length , i.e. that for every
sequence of constructor terms  there is at most one  with
 such that  unifies with the patterns
in . Then, we need to build a -term 
 which, when fed with  (encodings of) constructor
terms and  values, perform pattern matching and select the ``right'' value, or returns
 if none of  unifies with the constructor terms
in input. 
\begin{lem}[Pattern matching]\label{lemma:pm}
Let  be non-overlapping sequences of patterns of the same length .
Then there are a term  and an integer  such that
for every sequence of values ,
if  then

where , whenever the  are constructor terms. Moreover,

where ,
whenever  do not unify with
any of the sequences  or any of 
the  is itself .
\end{lem}
\proof
We go by induction on , where
 is the number of constructors occurrences in patterns
inside :
\begin{varitemize}
\item
  If  and , then we should always return : 
  
\item
  If  and , then  and  is simply a sequence of variables 
  , because the  are assumed to be non-overlapping.
  Then  is a term defined by induction on 
  which returns  only if one of its first  arguments is  and otherwise
  returns its -th argument applied to its first  arguments.
\item
  If , then there must be integers  and  with 
   and  such that
  
  for a constructor  and for some patterns  and some .
  Now, for every  
  and for every  
  we define sequences of patterns  and values  as follows:
  \begin{varitemizeii}
  \item
    If 
    
    then  is defined to be the sequence
    
    Moreover,  is simply the indentity .
  \item
    If 
    
    where  then  and  are both undefined.
  \item
    Finally, if
    
    then  is defined to be the sequence
    
    and  is the following -term
    
    where  is the number of variables in  and  is the
    number of variables in .
  \end{varitemizeii}
  As a consequence, for every , we can find a natural number  and 
  a sequence of pairwise distinct natural numbers  such that 
   are exactly the sequences which can
  be defined by the above construction. We are now able to formally define
  ; it is the term
  
  where
  
  Notice that, for every ,
  . Moreover, for every  any
   has the same length .
  This justifies the application of the
  induction hypothesis above. Informally,  first do some case
  analysis based on the shape of its -th argument. Based on the topmost constructor in it,
  one between  is selected which itself
  do the rest of the pattern matching by way of .
\end{varitemize}
This concludes the proof.
\qed
Once a general form of pattern matching is available in the  calculus, we may define
the -term  interpreting the function symbol .
\begin{defi}\label{Def-TRSonetolambdaII-fun}
For every function symbol , let

be the rules for . Moreover, suppose that
the variables appearing in the patterns in  are
. Observe that the
sequences  all have
the same length . Recall that we have a signature with function symbols . For any  
the -term 
interpreting  is defined to be:

where

whenever  and  and
 is defined by induction as follows:

\end{defi}
We have now implicitly defined how the map  behaves on
any term in :


\begin{thm}\label{theo:simulcl}
There is a natural number  such that for every
function symbol  and for every 
,
the following three implications hold, where
 stands for 
and  stands for 
:
\begin{varitemize}
\item
If  rewrites to
 in  steps, then
 rewrites to  in
at most  steps.
\item
If  rewrites to a normal form
, then
 rewrites to .
\item
If  diverges, then  diverges.
\end{varitemize}
\end{thm}
\proof
By an easy combinatorial argument following from the definition
of . Actually, a slightly stronger statement
should be proved to make the proof formal:
there is a natural number  such that for every ,
for every  (where  is 
the number of distinct variables in ),
the following three implications hold, where 
 stands for 
. 
\begin{varitemize}
\item
If  rewrites to
 in  steps, then
 rewrites to 
 in
at most  steps.
\item
If  rewrites to a normal form
, then
 
rewrites to .
\item
If  diverges, then 
 diverges.
\end{varitemize}
The first statement can be proved by induction on . The second and third one
are quite easy.
\qed
Clearly, the constant  in
Theorem~\ref{theo:simulcl} depends on , but is independent on the particular
term .
\subsection{An Example}\label{sect-EsempioTRStoLam}
\newcommand{\TRSadd}{\mathcal{ADD}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\suc}{\mathbf{s}}
\newcommand{\add}{\mathbf{add}}
In this section, we will describe the encoding of a concrete OCRS called  as a set of 
-terms. The signature  contains two constructor symbols
 and , with arity  and  (respectively), and a single function
symbol  of arity . The only two rules in  are the following:

Let us construct first some -terms in the image of :

We now take a look at . By definition:

This -term indeed ``simulates'' the successor constructor, when fed with an input. Suppose , then:

Finally, consider , the only function symbol of . By definition:

It is easy to verify that, by Lemma~\ref{lemma:pm},

\section{Graph Representation}
\label{Sect:GraphRep}
The previous two sections proved the main simulation result of the paper.
To complete the picture, we show in this section that the unitary cost model
for the (weak call-by-value) -calculus (and hence the number of
rewriting in a OCRSs) is polynomially related to the actual cost of implementing those 
reductions\footnote{As mentioned in the introduction, see~\cite{Sands:Lambda02} for another 
proof of this with other means.}. We do so by introducing term graph rewriting, following~\cite{TGRbarendregt} but
adapting the framework to call-by-value constructor rewriting. Contrarily to what
we did in Section~\ref{sect:CRS}, we will stay abstract here: our attention will
not be restricted to the particular graph rewrite system that is needed to implement 
reduction in the -calculus. 

We refer the reader to our~\cite{DLM09} for more details on efficient simulations between 
term graph rewriting and constructor term rewriting, both under innermost 
(i.e., call-by-value) and outermost (i.e., call-by-name) reduction strategies.

\begin{defi}[Labelled Graph]
Given a signature , a \emph{labelled graph over } consists of a directed
acyclic graph together with an ordering on the outgoing edges of each node and a (partial)
labelling of nodes with symbols from  such that the out-degree of each node
matches the arity of the corresponding symbols (and is  if the labelling is undefined).
Formally, a labelled graph is a triple 
where: 
\begin{varitemize}
\item
   is a set of \emph{vertices}.
\item
   is a (total) \emph{ordering function}.
\item
   is a (partial) \emph{labelling function} such
  that the length of  is the arity of  if
   is defined and is  otherwise.
\end{varitemize}
A labelled graph  is \emph{closed} iff  is a 
total function. 
\end{defi}
Consider the signature , where
arities of  are , , ,  respectively, and
, ,  are constructors. Examples of labelled graphs over 
the signature  are the following ones:

The symbol  denotes vertices where the underlying labelling function
is undefined (and, as a consequence, no edge departs from such vertices).
Their role is similar to the one of variables in terms.

If one of the vertices of a labelled graph is selected as the \emph{root}, we obtain
a term graph:
\begin{defi}[Term Graph]
A \emph{term graph}, is a quadruple 
, where 
 is a labelled graph and
 is the \emph{root} of the term graph.
\end{defi}
The following are graphic representations of some term graphs.

The root is the only vertex drawn inside a circle.

There are some classes of paths which are particularly relevant for our purposes.
\begin{defi}[Path]
A path  in a labelled graph
 is said to be:
\begin{varitemize}
\item
  A \emph{constructor path} iff for every , the symbol
   is a constructor;
\item
  A \emph{pattern path} iff for every , 
   is either a constructor symbol or is undefined;
\item
  A \emph{left path} iff , the symbol  is 
  a function symbol and 
  is a pattern path.
\end{varitemize}
\end{defi}

\begin{defi}[Homomorphism]
A \emph{homomorphism} between two labelled graphs 
 and 
 over the same 
signature  is a function  from  to  
preserving the term graph structure. In particular

for any , where  is the obvious
generalization of  to sequences of vertices. 
An \emph{homomorphism} between two term graphs 
 and 
 is
a homomorphism between  and 
 such that
. Two labelled graphs  and  are 
isomorphic iff there is a bijective homomorphism from
 to ; in this case, we write . Similarly
for term graphs.
\end{defi}
In the following, we will consider term graphs modulo isomorphism, i.e., 
iff . Observe that two isomorphic term graphs have the same graphical
representation.
\begin{defi}[Graph Rewrite Rule]
A \emph{graph rewrite rule} over a signature  
is a triple  such that:
\begin{varitemize}
\item
   is a labelled graph;
\item
   are vertices of , called 
  the \emph{left root} and the \emph{right root} of ,
  respectively.
\item
  Any path starting in  is a
  left path.
\end{varitemize}
\end{defi}
The following are examples of graph rewrite rules, assuming  to be a function
symbol and  to be constructors:

\begin{defi}[Subgraph]
Given a labelled graph  
and any vertex , the \emph{subgraph of  rooted
at }, denoted , is the
term graph  where
\begin{varitemize}
\item
   is the subset of  whose elements
  are vertices which are reachable from  in .
\item
   and  are
  the appropriate restrictions of  and 
  to .
\item
   is .
\end{varitemize}
\end{defi}
\begin{defi}[Redex]
Given a labelled graph , a \emph{redex} for  is
a pair , where  is a rewrite rule 
 and  is a homomorphism
between  and 
such that for any vertex 
with , 
any path starting in  is a constructor path.
\end{defi}
The last condition in the definition of a redex is needed to
capture the call-by-value nature of the rewriting process.

Given a term graph  and a redex ,
the result of firing the redex is another term graph obtained by
successively applying the following three steps to :
\begin{enumerate}[1.]
\item
  The \emph{build phase}: create an isomorphic copy of the portion of
   not contained in
  , and add it to , obtaining .
  The underlying ordering and labelling functions are defined in the natural
  way.
\item
  The \emph{redirection phase}: all edges in  pointing to 
  are replaced by edges pointing to the copy of . If 
  is the root of , then the root of the newly created graph will be
  the newly created copy of . The graph  is obtained.
\item
  The \emph{garbage collection phase}: all vertices which are not accessible
  from the root of  are removed. The graph  is obtained.
\end{enumerate}
We will write  (or simply
, if this does not cause ambiguity) in this case.

As an example, consider the term graph  and the rewrite rule 
:

There is a homomorphism  from
 to
. In particular,  maps  to
the rightmost vertex in .
Applying the build phase and the redirection phase we get 
and  as follows:

Finally, applying the garbage collection phase, we get the
result of firing the redex :


\begin{defi}
A constructor graph rewrite system (CGRS) over a signature  consists of
a set  of graph rewrite rules  on .
\end{defi}
\subsection{From Term Rewriting to Graph Rewriting}\label{Sect-TRtoGR}
Any term  over a signature  can be turned into a
graph  in the obvious way: take as  the abstract
syntax tree of , where vertices are in one-to-one correspondence
with symbol occurrences in . Conversely, any term graph
 over  can be turned into a term 
over  by simply unfolding the graph, that is applying (the label of) any vertex
to (the terms obtained as unfolding of) its sons (remember: we only consider acyclic graphs here).
We omit the boring formal definitions of both  and ; it is clear that 
for any term , , while in general 
 is not equal to , since the sharing present in  is lost during the unfolding. 

\begin{defi}
Given a constructor rewriting system  over , the 
corresponding constructor graph rewriting system  is defined 
by translating the terms with  and by translating any term 
rewrite rule  
over  into a graph rewrite rule 
as follows:
\begin{varitemize}
\item
  Take the graphs  and  (which are
  trees, in fact). 
\item
  From the union of these two trees, share those
  nodes representing the same variable in  and .
  This is .
\item
  Take  to be the root of  in  and
   to be the root of  in .
\end{varitemize}
\end{defi}

As an example, consider the rewrite rule

Its translation as a graph rewrite rule is the following:


Given a constructor rewriting system , it is easy to realize that
the following invariant is preserved while performing
rewriting in : whenever any vertex  can
be reached by two distinct paths starting at the root
(i.e.,  is \emph{shared}), any path starting at
 is a constructor path. A term graph
satisfying this invariant is said to be 
\emph{constructor-shared}.

Constructor-sharedness holds for term graphs coming from terms and
is preserved by graph rewriting:
\begin{lem}\label{lemma:constructorsharedness}
For every closed term ,  is constructor-shared.
Moreover, if  is closed and constructor-shared and  in , 
then  is constructor-shared.
\end{lem}
\proof
The fact that  is constructor-shared for every  follows
from the way the  map is defined: it does not introduce any
sharing. Now, suppose  is constructor-shared and

where  corresponds to a term rewrite
rule . The term graph  obtained from
 by the build phase is itself constructor-shared: it is obtained from
 by adding some new nodes, namely an isomorphic copy of the portion
of  not contained in . Notice
that  is constructor-shared in a stronger sense: any vertex which
can be reached from the newly created copy of  by two distinct paths
must be a constructor path. This is a consequence of  
being a graph rewrite rule corresponding to a term rewrite
rule , where the only shared vertices are those
where the labelling function is undefined. The redirection phase preserves
itself constructor-sharedness, because only one pointer is redirected
(the vertex is labelled by a function symbol) and the destination 
of this redirection is a vertex (the newly created
copy of ) which had no edge incident to it. Clearly, the
garbage collection phase preserves constructor-sharedness.
\qed
\begin{lem}\label{lemma:NFconstructorshared}
A closed term graph  in   is a 
normal form iff  is a normal form.
\end{lem}
\proof
Clearly, if a closed term graph  is in normal form,
then  is a term in normal form, because each
redex in  translates to a redex in .
On the other hand, if  is in normal form,
then  is in normal form: each redex in 
translates back to a redex in .
\qed
Reduction at the level of graphs correctly simulates reduction
at the level of terms, but only if the underlying graphs
are constructor shared:
\begin{lem}\label{lemma:CGtoC}
If  is closed and constructor-shared, and 
 in  , then
 in .
\end{lem}
\proof
The fact that each reduction step starting in  can be
mimicked in  is known
from the literature. If  is constructor-shared,
then the simulation is done in exactly one reduction step,
because any redex in a constructor-shared
term graph cannot be shared.
\qed
When  in not constructor-shared, a counterexample
can be easily built. Consider the term rewrite rule
 and the following term graph:

It corresponds to , and it is not
constructor-shared, since the shared vertex  is not a constructor. 
It rewrites in \emph{one} step to 

while the term  rewrites to  in
\emph{two} steps.

As can be expected, graph reduction is also complete with respect to
term reduction, with the only \emph{proviso} that term graphs must
be constructor-shared:
\begin{lem}\label{lemma:CtoCG}
If  in ,  is constructor-shared 
and , then
 in , where . \qed
\end{lem}

\begin{thm}[Graph Reducibility]\label{theo:graphreducible}
For every constructor rewrite system  over  and
for every term  over , the following two conditions are
equivalent:
\begin{enumerate}[\em 1.]
\item
   in , where  is in normal form;
\item
   in , where  is in normal
  form and .
\end{enumerate}
\end{thm}
\proof
Suppose , where  is in normal form. 
Then, by applying Lemma~\ref{lemma:CtoCG}, we obtain a term graph  such that
 and .
By Lemma~\ref{lemma:constructorsharedness},  is constructor-shared and, by 
Lemma~\ref{lemma:NFconstructorshared}, 
it is in normal form. Now, suppose  where
 and  is in normal form. By
applying  times Lemma~\ref{lemma:CGtoC}, we obtain
that .
But  and  is a normal form 
by Lemma~\ref{lemma:NFconstructorshared}, since 
 and  are
constructor shared due to Lemma~\ref{lemma:constructorsharedness}.
\qed

There are \emph{term} rewrite systems which are not graph reducible, i.e.
for which the two conditions of Theorem~\ref{theo:graphreducible} are
not equivalent (see~\cite{TGRbarendregt}). However, any 
\emph{orthogonal constructor} rewrite system is graph reducible, due to the 
strict constraints on the shape of rewrite rules~\cite{Plump90ggacs}.
This result can be considered as a by-product of our analysis, for which graph rewriting
is only instrumental.

\subsection{Lambda-Terms Can Be Efficiently Reduced by Graph Rewriting}
\label{sect:MainResult}
As a corollary of Theorems~\ref{theo:graphreducible} and~\ref{theo:termreducible},
we may reduce -terms using term graphs.
To this purpose, we apply the construction of the previous section
to the OCRS  that we defined in Section~\ref{sect:CRS}. Let 
then :
\begin{cor}
Let  be a closed -term. The following
two conditions are equivalent:
\begin{enumerate}[\em 1.]
\item
   where  is in normal form;
\item
   where
   and  is in normal form. \qed
\end{enumerate}
\end{cor} 
Let us now analyze more closely the combinatorics 
of graph rewriting in , so that we can obtain information on the efficiency of this simulation. 
\begin{varitemize}
\item
  Consider a closed -term  and a term graph  such
  that . By Proposition~\ref{prop:constred}
  and Lemma~\ref{lemma:CGtoC}, for every constructor  appearing
  as a label of a vertex in ,  is a subterm of . 
\item
  As a consequence, if ,
  then the difference  cannot be too big: at most
  . Therefore, if 
   then .
  Here, we exploit in an essential way the possibility of sharing constructors.
\item
  Whenever , computing
  a graph  such that  takes polynomial time in
  , which is itself polynomially bounded by  and .
\end{varitemize}
Hence (recall that  is the number of weak call-by-value beta steps to normal form): 
\begin{thm}\label{thm:main}
There is a polynomial  such that for every -term ,
the normal form of  can be computed in time at most 
. \qed
\end{thm}
This cannot be achieved when using explicit representations
of -terms. Moreover, reading back a -term from a term graph can take exponential
time.

We can complement Theorem~\ref{thm:main} with a completeness statement --- any universal computational model 
with an invariant cost model can be embedded in the -calculus with a polynomial
overhead. We can exploit for this the analogous result we proved in~\cite{CIE2006} (Section 4, Theorem 1) --- 
the unitary cost model is easily proved to be more parsimonious than 
the difference cost model considered in~\cite{CIE2006}.

\begin{thm}
Let  be computed by a Turing machine
 in time . Then, there are a -term 
and a suitable encoding  
such that  normalizes to  in 
 beta steps. \qed
\end{thm}

The encoding  mentioned in the theorem depends only on (the cardinality of) 
 (but not on the Turing machine). 
Interestingly enough it exploits once again the scheme that we used in Definition~\ref{Def-TRSonetolambdaI}: encode the empty string  as a zero-ary constructor,
and any symbol in  as a unary constructor (see~\cite{CIE2006} for details).
\section{Variations: Call-by-Name Reduction}
\label{sect:HeadReduction}
In the previous sections, -calculus was endowed with weak call-by-value reduction.
The same technique, however, can be applied to weak call-by-name reduction, as we will sketch in
this section.  is now endowed with a relation  defined
as follows:

Similarly to the call-by-value case,  stands for the number of
reduction steps to the normal form of  (if any). Since the relation
 is deterministic (i.e., functional),  is well-defined.

We need another OCRS, called , which is similar to  but designed
to simulate weak call-by-name reduction:
\begin{varitemize}
\item
  The signature  includes the binary function symbol  and
  constructor symbols  for every  and
  every , exactly as . Moreover, there is another
  binary constructor symbol . 
  To every term  we can associate
  terms  as follows:
  
  Notice that  maps -terms to \emph{constructor} terms, while
  terms obtained via  can contain function symbols. The ``official'' translation of a term  is thus , where only the applications ``on the spine'' of  are  encoded with . All other applications are frozen by the constructor .
\item
    The rewrite rules in  are all the rules in the following form:
    
    where  ranges over -terms,  ranges over abstractions and
    applications,
    
    and . These rewrite rules
    are said to be \emph{ordinary rules}. We also need the following \emph{administrative} rule:
    
\end{varitemize}
The CTRS  is more complicated than , because we need to force reduction to happen only in head position. The applications  (on the spine) may be fired immediately.
Observe, however, that the main rewriting rule (the last of the ordinary ones) is restricted to those 

where  is \emph{not} a variable. When  is a single variable, 
the corresponding beta redex would be either  or 
, with  free. In the former case, an application at the top
level of  (encoded as a  at this point) would become the top level
application of the spine of the reduct: the first ordinary reduction rule handles this case, unfreezing
 into  . When, on the other hand, the encoded redex is 
, we do not need to worry for , which will be discarded,
but in the term-reduction we must take care of the eventual substitution that may occur for
: the term substituted for  may have a top level frozen application  that must be
converted into an  --- this is the role of the third ordinary reduction rule. 
The second and fourth reduction rules just handle the remaining cases (they would be
instances of the last ordinary rule if this was not restricted to the non-variable cases).
A last remark on the administrative rule. There are never administrative redexes in the translation  of a term. During reduction, however, by the effect of the other rules a frozen application (a )
    may appear on the spine. The administrative rule recognizes this situation and unfreezes the application.

As usual, to every term  we can associate a term :

where .
A term  is \emph{canonical} if either  or
 where  is canonical and .
\begin{lem}\label{lemma:closedcanonical}
For every closed ,  is canonical.
\end{lem}
\proof
By a straightforward induction on .
\qed
The obvious variation on Equation~(\ref{equat:commute}) holds here:

 mimics call-by-name reduction in much the same way  mimics call-by-value
reduction. However, one reduction step in the -calculus corresponds to  steps
in , although  is kept under control:
\begin{lem}\label{lemma:simul}
Suppose that  is canonical and that . Then
there is a natural number  such that:
\begin{enumerate}[\em 1.]
\item
  ;
\item
  There is a canonical term  such that ;
\item
   whenever 
  and ;
\item
   whenever 
  and .
\end{enumerate}
\end{lem}
\proof
A term  is said to be \emph{semi-canonical} iff , where
 and  is either semi-canonical or is itself an element
of . We now prove that if  is semi-canonical, there there are a natural
number  and a canonical term  such that:
\begin{varitemize}
\item
  ;
\item
   whenever 
  and ;
\item
   whenever 
  and .
\end{varitemize}
We proceed by induction on . By definition  is 
in the form ; we have three cases:
\begin{varitemize}
\item
   is semi-canonical. Then, we get what we want by induction hypothesis.
\item
   is in  and has the form .
  Then,  and  is itself canonical.
\item
   is in  and has the form . Then
  
  Apply now the induction hypothesis to  (since its length is
  strictly smaller than ).
\end{varitemize}
We can now proceed as in Lemma~\ref{lemma:TRStolam}, since whenever
 rewrites to  by one of the ordinary rules,  is 
semi-canonical.
\qed
\begin{lem}\label{lemma:TRSWnormalform}
A canonical term  is in normal form iff  is in
normal form.
\end{lem}
\proof
We first prove that any canonical normal form  can be written
as , where
. We proceed by induction
on :
\begin{varitemize}
\item
  If ,
  then the thesis holds.
\item
  If , then  is canonical
  and in normal form, hence in the form 
  by induction hypothesis. As a consequence,  is not a normal
  form, which is a contradiction.
\end{varitemize}
We can now prove the statement of the lemma, by distinguishing two cases:
\begin{varitemize}
\item
  If ,
  where , then
   is in normal form and  is an
  abstraction, hence a normal form.
\item
  If , then  cannot be a normal form,
  since  is canonical and in normal form and, as a consequence,
  it can be written as .
\end{varitemize}
This concludes the proof.
\qed
Observe that this property holds only if  is canonical: a non-canonical term
can reduce to another one (canonical or not) even if the underlying -term
is a normal form.
\begin{lem}\label{lemma:lamtoTRSW}
If ,  is canonical and ,
then , where  and 
.
\end{lem}
\proof
Analogous to the one of Lemma~\ref{lemma:lamtoTRS} for the first part of the statement.
For the bound on the number of , argument similarly to the proof
of Lemma~\ref{lemma:simul}.
\qed
The slight mismatch between call-by-name reduction in  and
reduction in  is anyway harmless globally. As we now show, the total number of reduction
steps in  is at most two times as large as the total number of call-by-name reduction
steps in .
\begin{thm}[Term Reducibility]
Let  be a closed term. The following
two conditions are equivalent:
\begin{enumerate}[\em 1.]
\item
   where  is in normal form;
\item
   where
   and  is in normal form.
\end{enumerate}
Moreover .
\end{thm}
\proof
Suppose , where  is in normal form.
 is closed and, by Lemma~\ref{lemma:closedcanonical}, 
is canonical. By iterating over Lemma~\ref{lemma:simul} and Lemma~\ref{lemma:lamtoTRSW},
we obtain the existence of a term  such that ,
 is in normal form and , where  and

Since  ( is in normal form), .
If  where
 and  is in normal form, then
by iterating over Lemma~\ref{lemma:simul} we obtain that 
where  and  is in normal form.
\qed
 is the graph rewrite system corresponding to , in the sense of
Section~\ref{Sect:GraphRep}. Exactly as for the call-by-value case, computing the normal
form of (the graph representation of) any term takes time polynomial in the
number of reduction steps to normal form:
\begin{thm}\label{thm:mainw}
There is a polynomial  such that for every -term ,
the normal form of  can be computed in time at most 
.
\end{thm}
On the other hand, we cannot hope to \emph{directly} reuse the results in Section~\ref{Sect:CTR2L}
when proving the existence of an embedding of OCRSs into weak call-by-name 
-calculus: the same -term can have distinct normal forms in the two cases. 
It is widely known, however, that a continuation-passing translation
can be used to simulate call-by-value reduction by call-by-name reduction~\cite{Plotkin75tcs}.
The only missing tale is about the relative performances: do terms obtained via the CPS 
translation reduce (in call-by-name) to their normal forms in a number of
steps which is \emph{comparable} to the number of (call-by-value) steps to normal form
for the original terms? We conjecture the answer is ``yes'', but we leave the task of proving that
to a future work.
\section{Conclusions}
We have shown that the most na\"\i{}ve cost models for weak call-by-value 
and call-by-name
-calculus
(each beta-reduction step has unitary cost)
and orthogonal constructor term rewriting (each rule application has unitary cost)
are linearly related. Since, in turn, this cost model for
-calculus is polynomially related to the actual cost of reducing
a -term on a Turing machine, 
the two machine models we considered are both \emph{reasonable} machines, when
endowed with their natural, intrinsic cost models (see also Gurevich's opus on Abstract State Machine
simulation ``at the same level of abstraction'', e.g.~\cite{Gurevich}).
This strong (the embeddings we consider are compositional), complexity-preserving
equivalence between a first-order and a higher-order model is the most important technical
result of the paper. 

Ongoing and future work includes the investigation of how much of this simulation could be
recovered either in a typed setting (see~\cite{SplawskiU99} for some of the difficulties),
or in the case of -calculus with strong reduction, where we reduce under an abstraction. 
Novel techniques have to be developed, since the analysis of the present 
paper cannot be easily extended to these cases.

\bibliographystyle{alpha}
\bibliography{wie}
\end{document}
