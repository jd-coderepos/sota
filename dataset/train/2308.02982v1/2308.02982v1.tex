\documentclass[sigconf]{acmart}

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{balance}

\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}


\AtBeginDocument{\providecommand\BibTeX{{Bib\TeX}}}

\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[MM '23]{Proceedings of the 31st
ACM International Conference on Multimedia}{October 29-November 3,
2023}{Ottawa, ON, Canada}
\acmBooktitle{Proceedings of the 31st ACM International Conference on
Multimedia (MM '23), October 29-November 3, 2023, Ottawa, ON, Canada}
\acmPrice{15.00}
\acmDOI{10.1145/3581783.3611767}
\acmISBN{979-8-4007-0108-5/23/10}










\begin{document}

\title{Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation}

\author{Haowei Wang}\authornote{Work is done during internship at Fuxi AI Lab, Netease Inc.}\authornote{Equal Contributions.}
\email{wanghaowei@stu.xmu.edu.cn}
\affiliation{
  \institution{Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  Ministry of Education of China,
  Xiamen University}
  \city{}
  \state{}
  \country{}
}

\author{Jiji Tang}\authornotemark[2]
\email{tangjiji_bupt@163.com}
\affiliation{\institution{Fuxi AI Lab, Netease Inc.}
  \state{}
  \country{}
}


\author{Jiayi Ji}\authornotemark[2]
\email{jjyxmu@gmail.com}
\affiliation{
  \institution{Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  Ministry of Education of China,
  Xiamen University}
  \city{}
  \state{}
  \country{}
}

\author{Xiaoshuai Sun}\authornote{Corresponding author.}
\email{xssun@xmu.edu.cn}
\affiliation{
  \institution{Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  Ministry of Education of China,
  Xiamen University}
  \city{}
  \state{}
  \country{}
}

\author{Rongsheng Zhang}
\email{zhangrongsheng@corp.netease.com}
\affiliation{\institution{Fuxi AI Lab, Netease Inc.}
  \state{}
  \country{}
}


\author{Yiwei Ma}
\email{yiweima@stu.xmu.edu.cn}
\affiliation{
  \institution{Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  Ministry of Education of China,
  Xiamen University}
  \city{}
  \state{}
  \country{}
}

\author{Minda Zhao}
\email{zhaominda01@corp.netease.com}
\affiliation{\institution{Fuxi AI Lab, Netease Inc.}
  \state{}
  \country{}
}



\author{Lincheng Li}
\email{lilincheng@corp.netease.com}
\affiliation{\institution{Fuxi AI Lab, Netease Inc.}
  \state{}
  \country{}
}


\author{Zeng Zhao}
\email{zengzhao_wl@163.com}
\affiliation{\institution{Fuxi AI Lab, Netease Inc.}
  \state{}
  \country{}
}


\author{Tangjie Lv}
\email{hzlvtangjie@corp.netease.com}
\affiliation{\institution{Fuxi AI Lab, Netease Inc.}
  \state{}
  \country{}
}


\author{Rongrong Ji}
\email{rrji@xmu.edu.cn}
\affiliation{
  \institution{Key Laboratory of Multimedia Trusted Perception and Efficient Computing,
  Ministry of Education of China, Xiamen University}
  \city{}
  \state{}
  \country{}
}




\begin{abstract}
In recent years, 3D representation learning has turned to 2D vision-language pre-trained models to overcome data scarcity challenges. However, existing methods simply transfer 2D alignment strategies, aligning 3D representations with single-view 2D images and coarse-grained parent category text. These approaches introduce information degradation and insufficient synergy issues, leading to performance loss. Information degradation arises from overlooking the fact that a 3D representation should be equivalent to a series of multi-view images and more fine-grained subcategory text. Insufficient synergy neglects the idea that a robust 3D representation should align with the joint vision-language space, rather than independently aligning with each modality.
In this paper, we propose a multi-view joint modality modeling approach, termed JM3D, to obtain a unified representation for point cloud, text, and image. Specifically, a novel Structured Multimodal Organizer (SMO) is proposed to address the information degradation issue, which introduces contiguous multi-view images and hierarchical text to enrich the representation of vision and language modalities. A Joint Multi-modal Alignment (JMA) is designed to tackle the insufficient synergy problem, which models the joint modality by incorporating language knowledge into the visual modality.
Extensive experiments on ModelNet40 and ScanObjectNN demonstrate the effectiveness of our proposed method, JM3D, which achieves state-of-the-art performance in zero-shot 3D classification. JM3D outperforms ULIP by approximately 4.3\% on PointMLP and achieves an improvement of up to 6.5\% accuracy on PointNet++ in top-1 accuracy for zero-shot 3D classification on ModelNet40. The source code and trained models for all our experiments are publicly available at \url{https://github.com/Mr-Neko/JM3D}.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010224.10010240.10010241</concept_id>
       <concept_desc>Computing methodologies~Image representations</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010226.10010239</concept_id>
       <concept_desc>Computing methodologies~3D imaging</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Image representations}
\ccsdesc[500]{Computing methodologies~3D imaging}

\renewcommand{\shortauthors}{Haowei Wang, et al.}

\keywords{3D Representation, Pretrain, Multi-modal Contrastive Learning}




\maketitle

\section{Introduction \label{sec:Introduction}}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{fig1.pdf}

\caption{The visualization of JM3D. JM3D aligns 3D modality with the pre-aligned vision and language modalities, constructing a unified representation of the three modalities. Continuous Image Sequence (CIS, left) and Hierarchical Text Tree (HTT, right) organize structured images and texts to enhance the information from vision and language modalities. The joint alignment and modeling {\color{green}(green line)} correct the inappropriate way of independent alignment {\color{red}(red line)} used in previous methods.}
\label{fig1}
\end{figure}



The representation learning of 3D models~\cite{achlioptas2018learning, liu2019densepoint, liu2020closer, ran2022surface, xie2020grnet, xu2021paconv} has become increasingly significant in various real-world applications such as augmented/virtual reality~\cite{liu2021group, vu2022softgroup, armeni20163d} and autonomous driving~\cite{li2022deepfusion, yin2021center}. Nevertheless, the development of 3D understanding is hindered by the scarcity of data and the inadequacy of category representation, contrasting sharply with the vast availability of image-text pairs.

To tackle the issue of insufficient 3D data, existing works~\cite{hegde2023clip, xue2022ulip, zhang2023clip} aim to harness the wealth of other modalities by employing large-scale vision-language models, such as CLIP~\cite{radford2021learning}, to improve 3D representation. The underlying principle of these approaches is to align the 3D features with the unified space of vision and language, thus benefiting from the powerful zero-shot capabilities of foundation models. These methods typically involve rendering an image of a 3D model from a specific angle, accompanied by a simple category label, and feeding them into CLIP. The 3D features are then aligned with the visual-language space via a contrastive approach. This strategy, which incorporates abundant external information, has been demonstrated to effectively enhance 3D understanding capabilities and exhibit good transferability, as evidenced by works such as ULIP~\cite{xue2022ulip} and CG3D~\cite{hegde2023clip}.

However, these methods predominantly adopt 2D alignment strategies for 3D representation learning, failing to account for the unique characteristics of 3D models. Consequently, they face two significant limitations: (1) \textbf{Information degradation}: the alignment of 3D representations with single-view images and coarse text leads to a loss of crucial spatial and depth information. For instance, when examining single-view images, the front render of an airplane in Fig.~\ref{fig1} lacks wing details, and so is the back render. Additionally, from a textual standpoint, the generic term ``airplane'' is insufficient to differentiate an airliner from other types of aircraft, such as jets or bombers. (2) \textbf{Insufficient synergy}:  these methods align 3D representations with image features and text features separately, neglecting the joint modeling of vision and language modalities. This issue complicates the optimization process for 3D representations, making it difficult to determine whether to move closer to image features or text features, and to what extent, ultimately leading to incomplete information utilization.


In response to the aforementioned limitations, we propose a more synergistic multi-modal approach for comprehensive 3D representation learning, named JM3D, as illustrated in Fig.~\ref{fig1}. The approach consists of two key components: the Structured Multi-modal Organizer (SMO) and the Joint Multi-modal Alignment (JMA). 
First, to mitigate information degradation, we design the SMO to enhance both vision and language modalities individually. For the vision, we believe that a 3D model should align with a continuous sequence of images from various viewpoints. Based on this premise, we introduce a Continuous Image Sequence (CIS) that jointly models a series of images. To further reduce information loss, we embed angle, color, and depth information into the images through encoding. For the language, we implement a Hierarchical Text Tree (HTT). By introducing sub-categories such as``jet'', ``airliner'', or ``bomber'', the model is encouraged to learn finer-grained information, thus enhancing generalizability. The inclusion of parent categories like "airplane" ensures that semantically similar sub-categories have similar features, thereby increasing the robustness of the representation.
Second, to address insufficient synergy, we propose the JMA method that jointly models visual and language modalities, considering both modalities simultaneously during optimization rather than in isolation. This approach allows the 3D representation to effectively capture comprehensive information from both vision and language modalities. More importantly, we systematically derive a theoretical framework to validate the effectiveness of this modeling approach, which can serve as a foundation for future related research. Extensive experiments demonstrate that our approach significantly improves 3D representation in the zero-shot 3D classification of ModelNet40 and ScanObjectNN, with respective performance gains of {\color{black}4.3}\% and {\color{black}6.5}\% for PointMLP and PointNet++ on the top-1 accuracy of the ModelNet40.

In summary, this paper presents three key contributions:
\begin{itemize}
    \item To address the information degradation issue, we propose the Structured Multimodal Organizer (SMO) that constructs a continuous multi-view sequence of rendered images and a hierarchical text tree. By enhancing visual and textual features, SMO compensates for the loss of 3D visual features, ensuring more comprehensive representations. 

    \item To tackle the insufficient synergy problem, we design the Joint Multi-modal Alignment (JMA), which incorporates both textual and visual modalities to obtain a joint representation. This approach significantly avoids suboptimal performance and facilitates a more cohesive understanding of the image-text pairs. 

    \item Our proposed approach, JM3D, achieves state-of-the-art performance on various downstream tasks, particularly in zero-shot 3D classification. On the ModelNet40 dataset, JM3D delivers a 4.3\% improvement for PointMLP and a 6.5\% improvement for PointNet++ on the top-1 accuracy of the ``All'' set of ModelNet40 compared to existing methods. \end{itemize}

\begin{figure*}[]
\centering
\includegraphics[width=1.0\textwidth]{fig2.pdf}

\vspace{-0.25cm}
\caption{The framework of JM3D. Continuous Image Sequence {\color{green}(CIS)} and Hierarchical Text Tree {\color{orange}(HTT)} organized continuous multi-view images and hierarchical texts respectively, which are fed into a pre-training model (frozen) to extract features on the left. Then, Joint Multi-modal Alignment {\color{gray}(JMA)} incorporates the features from two modalities to generate the joint modeling features. On the last, contrastive learning is applied to align 3D features (training) with joint features and subcategory texts, while 3D features are aggregated with the assistance of the parent category.}
\label{fig2}
\end{figure*}

\section{Related work}
\subsection{Representation Learning in 3D Space}
The representation learning of 3D space is aimed to obtain semantic features of a 3D model. 3D models have different representation methods, among which point cloud has become one of the most suitable input formats in deep learning due to its sparsity and discreteness~\cite{aubry2011wave, bronstein2010scale, sun2009concise, wu20153d, maturana2015voxnet, zhao-etal-2023-generating-visual}. Early methods~\cite{maturana2015voxnet, shi2020pv} often extracted point cloud data from voxels and used some convolutions to extract global features. Subsequent methods attempted to design specific structures directly for point cloud, \emph{e.g.}, PointNet~\cite{qi2017pointnet}, PointNext~\cite{qian2022pointnext} and PointMLP~\cite{marethinking}. PointNet directly extracts permutation-invariant features from a point cloud, greatly influencing subsequent method designs. Recent PointMLP takes two MLP blocks and a geometrical transformer to achieve impressive results without integrating sophisticated local geometrical extractors.

With the rise of transformers~\cite{vaswani2017attention}, some self-supervised learning methods~\cite{guo2021pct, liu2022masked, xiao2023unsupervised} attempt to generate more point clouds and increase the difficulty of training tasks by using encoder-decoder structures to reconstruct point clouds, which is considered effective in such PointBert~\cite{yu2022point}, Point-MAE~\cite{pang2022masked}, and Point-M2AE~\cite{zhangpoint}. 

Even so, regardless of the structural design, the biggest challenge in 3D representation learning is the limited-scale dataset. Those methods are constrained by simple category annotations and insufficient data volume, resulting in mediocre robustness in the real world.

\subsection{Representation Learning in Multi-modal Space}
Representation learning in multi-modal space mainly aims at aligning semantics features from different modalities. Current methods primarily achieve modality alignment through two lines. One of them fused features from different modalities by delicately designed architectures~\cite{li2019visualbert, li2020oscar, chen2020uniter, hu2023you, fei-etal-2023-scene} and trained alignment through tasks such as retrieval and comprehension. These methods~\cite{li2019visualbert, li2020oscar, lu2019vilbert, tan2019lxmert, wang2023towards, jin2023refclip} often focus on learning the interaction between image regions and their corresponding descriptions.

Another line, such as CLIP~\cite{radford2021learning}, first applies contrastive learning on vision and language modalities, directly aligning features across different modalities with massive positive and negative samples. Some subsequent methods optimized the alignment of CLIP from both data scale and granularity perspectives. The series of GLIP~\cite{li2022grounded, zhang2022glipv2} attempted to achieve finer alignment by using detection tasks, while Flamingo~\cite{alayrac2022flamingo} introduced a scaled dataset. At the same time, approachs~\cite{luo2022clip4clip, li2022align, xu2021videoclip, ju2022prompting, FeiMatchStruICML22, ma2022xclip} have shown that the CLIP-paradigm remains effective even when extended to other modalities like video and text.
 
\subsection{Learning 3D Representation with Multi-modal}
Representation learning in multi-modal domains has shown that learning from different modalities brings better performance. Therefore, utilizing the rich knowledge of image-text pre-trained models to enhance the feature representation of 3D models is a promising way~\cite{chen2021multimodal, yanlet, ma2023xmesh}. PointCLIP~\cite{zhang2022pointclip} is the first attempt to utilize a visual-language pre-training model for point cloud models. It renders point cloud models as a series of depth images and directly feeds them into CLIP for zero-shot classification. However, it did not improve the expressive power of point clouds. ULIP~\cite{xue2022ulip} and CG3D~\cite{hegde2023clip} attempt to directly transplant the paradigm of CLIP to learn a unified representation space for point clouds, language, and images. Contrastive learning paradigm is operated between point cloud modality and visual modality as well as language modality, separately. These methods overlook the fact that the information contained in a single image or a piece of text cannot be equivalent to that of the entire 3D model. At the same time, aligning them separately ignores the joint distribution of image-text modalities. Our methods, on the one hand, with SMO reorganized the visual and language data, and the representations of the visual and language modality are strengthened. On the other hand, JMA modeled the joint features of the visual-language modality, optimizing the process of learning a unified semantic space.

\section{Method}

In this section, we will first review the contrastive learning framework for 3D representations in Sec.~\ref{sec:ulip}.  Next, the details of our proposed SMO module will be introduced in Sec.~\ref{sec:SMO}. Then we present the
JMA module in Sec.~\ref{sec:JMA}.  Finally, we present the objectives of the construction of unified triplet modality modeling in Sec.~\ref{sec:loss}. The overall framework is illustrated in Fig.~\ref{fig2}.

\subsection{Preliminary \label{sec:ulip}}

To acquire a unified semantic space across vision, language, and point cloud, ULIP~\cite{xue2022ulip} investigates the feasibility of directly transferring 2D contrastive learning to 3D. A dataset comprising point clouds, images, and textual descriptions is constructed from the ShapeNet55~\cite{chang2015shapenet}, which consists of publicly available 3D CAD models and their associated textual descriptions.  For the -th CAD model, a triplet sample  is created, consisting of an image , a text description , and a sampled point cloud , where , , and  represent the height, width of the image, and the number of sampled points, respectively.
The image  is rendered from the CAD model with a random angle, while the text description  is created by concatenating a fixed prompt with a coarse pre-defined category, such as "a point cloud of [CLASS]". Meanwhile, the point cloud  is uniformly sampled from the original models to accommodate different point cloud backbones. With these triplet samples, the representations of the three modalities are expected to align within a unified semantic space. The formulation of this alignment can be expressed as:

Prior work~\cite{xue2022ulip, hegde2023clip} simplified Eq.~\ref{eq:definition} based on an approximate assumption, which states that the \textit{\textbf{2D image  and text  are conditionally independent}}. Consequently, ULIP does not consider the joint modality conditional probability , and instead simplifies it by aligning individual modalities as follows:

Inspired by Eq.\ref{eq:ULIP}, ULIP learns the joint distribution contrastively, aligning the 3D features with language and vision features separately. Specifically,  can be represented by a pre-trained vision-language model, such as CLIP\cite{radford2021learning}. This pre-aligned model is employed to extract language and vision features, while 3D features can be extracted using various backbones~\cite{marethinking, qi2017pointnet++, yu2022point}. These operations can be formulated as:

where  is a 3D representation network with various backbones.  and  are the pre-aligned vision encoder and language encoder from CLIP, which utilize vanilla transformer~\cite{vaswani2017attention} structures. The vectors , , and  denote 3D, language, and vision features, respectively, and  represents the dimensions of the final representation vectors.

Next, a contrastive method~\cite{radford2021learning} is chosen to model the conditional distribution between any two modalities in Eq.~\ref{eq:ULIP}, aligning , , and  by:

where  represents the combination of pairwise modalities. 

\subsection{Structured Multi-modal Organizer \label{sec:SMO}}

To address the issue of information loss resulting from naively aligning 3D models  with 2D vision and coarse-grained categories, we propose a more refined approach. For example, a car exists in 3D space, but a single frontal render lacks information about the rear end, as do other individual view renders. Similarly, the term "bottle" does not accurately represent specific models like ``jug'', ``beer'', ``bottle'' or ``flask''. To mitigate this information loss, we organize data in the triplet sample  using a multi-view approach, constructing a new triplet data and redefining it as:

This refined representation leverages the  images from Continuous Image Sequence (CIS) and structured texts with parent category and sub-category from Hierarchical Text Tree (HTT) to ensure a more accurate and detailed alignment between the 3D models and their corresponding 2D vision and textual descriptions.







\subsubsection{\textbf{Continuous Image Sequence}}
For the visual modality, single synthetic images from random angles only capture partial features of a 3D model. To enhance vision semantics, it is logical to introduce multi-view rendered images.
Building upon ULIP, we synthesize RGB and depth images at 12-degree intervals, creating a candidate image set , where  denotes the number of rendered images. Since the sampled images are discrete, we observed that large angular deviations between images in the sequence may cause training instability. To address this, we sample  images within a specific angular range during each training process, as follows:

where  signifies sampling within a specific angular range,  denotes the render degree of the -th image, and  is a hyperparameter set to  based on our experiments.

The image encoder from pre-trained CLIP embeds multi-view images into the feature space. Furthermore, given the importance of angle and depth as positional information in images, we design angle and depth encodings, akin to \cite{vaswani2017attention}, to capture the 3D spatial information of different images. As illustrated in Fig.\ref{fig2}, these embeddings are added to the visual features, resulting in the following formulation:

where , introduced in~\cite{1607.06450}, controls the range of vision vectors , consequently expediting convergence.

\subsubsection{\textbf{Hierarchical Text Tree}}
For the language modality, existing methods like ULIP use only simple parent categories (55 categories) for 3D representation learning. This approach reduces the number of categories to be learned, making the model easier to fit but also lowering its generalization performance. The ShapeNet55 dataset~\cite{chang2015shapenet} also provides subcategories (205 categories). Introducing subcategories forces the model to focus on finer-grained representations, enabling it to correctly distinguish between visually similar subcategories. However, it is important to note that when the dataset size is limited, independently introducing subcategories may cause the model to overlook the modeling of family relationships, where subcategories belonging to the same parent category should have more similar features. Incorporating these relationships greatly reduces the model's dependency on data size and improves convergence speed.

To address these issues, HTT constructs a novel hierarchical category tree for each point cloud model, comprising coarse semantic parent categories and more specific subcategories (e.g., ``bed''  ``bunk''). Specifically, when subcategory annotations are partially missing for a model, we use its parent category as a replacement. Through this approach, HTT assigns structured category information to each model as , where  represents the parent category with primary semantics, and  denotes the subcategory with fine-grained details.
With hierarchical-grained categories, we design specific tasks for both parent and subcategories. The subcategories  follow Eq.~\ref{eq:extract} to generate text features using the textual encoder of CLIP, which will be learned in a contrastive manner. Parent categories are used to guide the aggregation of point cloud features, as expressed in the following formula:

where  is an MLP~\cite{tolstikhin2021mlp} network, and  maps the parent category of -th sample to a fixed label. By performing this operation, the model can learn fine-grained features within limited samples while also possessing the ability to abstract family features, thus avoiding underfitting.









\subsection{Joint Multi-modal Alignment \label{sec:JMA}}
In previous methods, vision and language modalities were assumed to be conditionally independent, as demonstrated in Eq.\ref{eq:ULIP}. This approximation results in an \emph{insufficient synergy} issue when compared to Eq.\ref{eq:definition}, leading to suboptimal performance. To tackle this problem, we propose a more general approach by directly modeling the relationships between vision and language modalities, denoted as . This relationship can be reformulated as:

which means that for any sample  in Eq.~\ref{eq:sample}, we use both textual and multi-view visual information interactively for joint modeling. In practice, JMA generates multiple image features  and fine-grained text feature . Using these features, attention between text and all views is calculated to help reconstruct image features into joint modality features, as follows:

where  denotes matrix multiplication, and  represents element-wise multiplication.

By leveraging JMA, we jointly map 2D image and textual information into a space that is more conducive to 3D features, obtaining a robust joint image-text representation feature. This enhanced feature further aids in aligning the 3D modality with the other two modalities using the contrastive learning paradigm, ultimately resulting in improved performance.

\subsection{Training Objective \label{sec:loss}}

For the training process, we set two tasks for the unified 3D representation learning. First, a contrastive way is set between the point cloud representation, language features like Eq.~\ref{eq:ULIP contrastive}, which is formulated as:

where the , , and  are the hyper parameters. The contrastive learning draws the features across three modalities to a unified semantic space, while the  from the subcategory provides guidance in fine-grained. 

At the same time, another task is the classification task constructed for the parent category as Eq.~\ref{eq:parent classed}, which is formulated as:


where the  is the parameters of a MLP, and  is the probability of -th parent category for the -th sample. In this manner, point cloud features are aggregated under the guidance of the parent category, providing a softer constraint compared to the strict contrastive learning approach. This simple method, through the introduction of parent categories, significantly reduces the difficulty in fitting subcategories. The final loss is obtained by summing the two aforementioned loss. 

\begin{table*}[htb]
    \caption{The results of zero-shot 3D classification on ModelNet40 and ScanObjectNN datasets. PointMLP + JM3D outperforms the previous state-of-the-art methods by a large margin in various evaluation settings, especially achieving a 12.3\% and 13.7\% improvement of the ``Medium'' and ``Hard'' mode on ModelNet40, which is the SOTA.}
    \centering
    \begin{tabular}{lcccccccc}
         \toprule
         \multirow{3}*{Model} & \multicolumn{6}{c}{ModelNet40} & \multicolumn{2}{c}{ScanObjectNN} \\
         
         \cmidrule(lr){2-7}\cmidrule(lr){8-9}
         
         ~ & \multicolumn{2}{c}{ All } & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard} & \multicolumn{2}{c}{All}
         \\
         \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
         ~  & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5
         \\
         \midrule
PointCLIP~\cite{zhang2022pointclip}  & 20.2 & -- & 10.4 & --& 8.3 &-- & 15.4 & --\\
         PointMLP~\cite{marethinking} + CG3D~\cite{hegde2023clip} & 50.4 & -- & -- & -- & -- & -- & 25.0 & --\\
         PointTransformer~\cite{zhao2021point} + CG3D~\cite{hegde2023clip} & 50.6 & -- & -- & -- & -- & -- & 25.6 & --\\
         PointNet++(ssg)~\cite{qi2017pointnet++} + ULIP~\cite{xue2022ulip}& 55.7 & 75.7 & 35.6 & 64.4 & 33.7 & 55.8 & 45.6 & 73.8\\
         PointBERT~\cite{yu2022point} + ULIP~\cite{xue2022ulip}& 60.4 & \textbf{84.0} & 40.4 & 72.1 & 37.1 & 66.3 & 48.5 & 79.9\\
         PointMLP~\cite{marethinking} + ULIP~\cite{xue2022ulip} & 61.5 & 80.7 & 43.2 & 72.0 & 36.3 & 65.0 & 44.6 & 82.3\\
\midrule
         PointNet++(ssg)~\cite{qi2017pointnet++} + JM3D & 62.2 \textcolor{darkgreen}{\small ()} & 79.3 & 47.6 \textcolor{darkgreen}{\small ()} & 75.9 & 43.3 \textcolor{darkgreen}{\small ()} & 74.7 & 46.0 \textcolor{darkgreen}{\small ()} & 78.1\\
         
         PointBERT~\cite{yu2022point} + JM3D & 61.8 \textcolor{darkgreen}{\small ()} & 81.7 & 52.9 \textcolor{darkgreen}{\small ()} & 73.6 & 48.4 \textcolor{darkgreen}{\small ()} & 71.1 & \textbf{48.9} \textcolor{darkgreen}{\small ()} & 82.8\\
         
         PointMLP~\cite{marethinking} + JM3D & \textbf{65.8} \textcolor{darkgreen}{\small ()} & 82.1 & \textbf{55.5} \textcolor{darkgreen}{\small ()} & \textbf{77.1} & \textbf{55.0} \textcolor{darkgreen}{\small ()} & \textbf{75.0} & 47.5 \textcolor{darkgreen}{\small ()} & \textbf{83.3}\\


         \bottomrule
    \end{tabular}
    \label{tab:zero-shot-modelnet}
\end{table*}

\begin{table*}[htb]
    \caption{The ablation study of CIS. Multi-views will lead to point cloud features being biased towards vision modality, which decreases the performance on zero-shot 3D classification. However, CIS effectively improves the performance with the embeddings and within-view sample.}
    \centering
    \begin{tabular}{ccc|cccccccc}
         \toprule
         \multirow{3}*{Views} & \multirow{3}*{Sample} & \multirow{3}*{Embedding}  & \multicolumn{6}{c}{ModelNet40} & \multicolumn{2}{c}{ScanObjectNN} \\
         
         \cmidrule(lr){4-9}\cmidrule(lr){10-11}
         
         ~ & ~ & ~ & \multicolumn{2}{c}{ All } & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard} & \multicolumn{2}{c}{All}
         \\
         \cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
         ~ & ~ & ~ & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 \\
         \midrule
1 & random & \ding{55} & 60.0 & 79.4 & 43.2 & 72.0 & 36.3 & 65.0 & 44.6 & 82.3\\
         4 & random & \ding{55} & 56.5 & 76.6 & 42.3 & 70.4 & 38.2 & 66.3 & 44.5 & 75.3\\
         4 & random & \ding{51} & 58.8 & 79.9 & 47.6 & \textbf{75.1} & 47.5 & 71.2 & 44.9 & 81.2\\
         4 & within-view & \ding{51} & 60.7 & 80.2 & 48.7 & 74.7 & 50.0 & 71.2 & \textbf{45.4} & \textbf{82.8}\\
         2 & within-view & \ding{51} & \textbf{61.2} & \textbf{80.7} & \textbf{49.5} & 74.5 & \textbf{51.4} & \textbf{71.7} & \textbf{45.4} & 82.2\\

\bottomrule
    \end{tabular}\label{tab:ablation-image}
\end{table*}

\begin{table*}[htb]
    \caption{The ablation study of HTT. Results show that structured text is more effective than fine-grained text. Even if the language modality is enhanced, the independent alignment method still makes the improvements unstable.}
    \centering
    \begin{tabular}{cc|cccccccc}
         \toprule
         \multirow{3}*{Contrastive Text} & \multirow{3}*{Classed Text}  & \multicolumn{6}{c}{ModelNet40} & \multicolumn{2}{c}{ScanObjectNN} \\
         
         \cmidrule(lr){3-8}\cmidrule(lr){9-10}
         
         ~ & ~ & \multicolumn{2}{c}{ All } & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard} & \multicolumn{2}{c}{All}
         \\
         \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
         ~ & ~ & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 \\
         \midrule
Parent category & \ding{55} & 61.2& 80.7 & 49.5 & 74.5 & \textbf{51.4} & 71.7 & 45.4 & \textbf{82.2}\\
         Subcategory & \ding{55} & 61.8 & 81.0 & 48.5 & \textbf{78.6} & 43.5 & \textbf{75.2} & 46.4 & 79.5\\
         Subcategory & Parent category & \textbf{63.1} & \textbf{81.4} & \textbf{49.7} & 76.7 & 50.4 & 74.4 & \textbf{46.9} & 80.4\\

\bottomrule
    \end{tabular}\label{tab:ablation-text}
    \vspace{-0.1cm}
\end{table*}

\begin{table*}[htb]
    \caption{The ablation study for JMA. Independent alignment wastes the rich semantics brought by SMO, while JMA achieves significant improvements on all settings of different datasets.}
    \centering
    \begin{tabular}{ccc|cccccccc}
         \toprule
         \multirow{3}*{CIS} & \multirow{3}*{HTT} & \multirow{3}*{JMA}  & \multicolumn{6}{c}{ModelNet40} & \multicolumn{2}{c}{ScanObjectNN} \\
         
         \cmidrule(lr){4-9}\cmidrule(lr){10-11}
         
         ~ & ~ & ~ & \multicolumn{2}{c}{ All } & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard} & \multicolumn{2}{c}{All}
         \\
         \cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
         ~ & ~ & ~ & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 \\
         \midrule
\ding{55} & \ding{55} & \ding{55} & 60.0 & 79.4 & 43.2 & 72.0 & 36.3 & 65.0 & 44.6 & 82.3\\
         \ding{51} & \ding{51} & \ding{55} & 63.1 & 81.4 & 49.7 & 76.7 & 50.4 & 74.4 & 46.9 & 80.4\\
         \ding{51} & \ding{51} & \ding{51} & \textbf{65.8} & \textbf{82.1} & \textbf{55.5} & \textbf{77.1} & \textbf{55.0} & \textbf{75.0} & \textbf{47.5} & \textbf{83.3}\\

\bottomrule
    \end{tabular}
\label{tab:ablation-joint}
\end{table*}

\section{Experiment}
\subsection{Datasets}
\subsubsection{\textbf{Pretrain Datasets}}
We use \textbf{ShapeNet55}~\cite{chang2015shapenet} as the pre-training dataset, which is the publicly-available subset of ShapeNet. ShapeNet consists of 52.5K CAD models with multiple texture maps and corresponding category annotations. The annotations have a total of 55 basic categories and 205 fine-grained subcategories, with a small number of models missing subcategories. During training, we randomly sample different numbers of points from the CAD models to adapt different networks.
\subsubsection{\textbf{Downstream Datasets}}
We conducted downstream task experiments primarily on the following two datasets.

\textbf{ModelNet40}~\cite{wu20153d} is composed of synthetic 3D CAD models, consisting of 9,843 training samples and 2,468 testing samples, spanning across 40 categories. For the test, we follow~\cite{marethinking} to downsample the point cloud data to 1024 points.

\textbf{ScanObjectNN}~\cite{uy2019revisiting} is different from ModelNet40. It is a dataset of scanned 3D objects from real scenes, which consists of 2,902 samples among 15 categories. It can be divided into two categories based on whether or not it includes background: \emph{OBJ\_ONLY} and \emph{OBJ\_BJ}. The former refers to a clean mesh, while the latter includes background noise. Here we follow ULIP to use the pre-processed data~\cite{uy2019revisiting} from~\cite{yu2022point}, which is normalized and downsampled to 1024 points.

\subsection{3D Backbone Networks}
To verify the effectiveness of the proposed JM3D, we conducted experiments on ModelNet40 and ScanObjectNN with different 3D backbones, which are:

\textbf{PointNet++}~\cite{qi2017pointnet++} is the sequel of PointNet~\cite{qi2017pointnet}, which is an encoder-decoder formulation to extract the deep hierarchical features on point sets. The encoder is composed of many set abstraction modules, and the farthest point sample is between them to reduce the scale of point sets. For the decoder, the outputs of all the encoder layers will be sent to different head networks to adapt to diverse tasks.

\textbf{PointMLP}~\cite{marethinking} is a lite network with residual MLP modules to better extract features, avoiding the complicated architecture to utilize the local geometric features. PointMLP introduces Geometric Affine Module to transfer points to the normal distribution, and two MLP blocks are used to extract the representation and position information respectively. 

\textbf{PointBert}~\cite{yu2022point} is a transformer-based model to introduce self-supervised learning to the the point cloud representation learning. By reconstructing the masked point cloud, it achieves impressive performances in unlabeled point cloud datasets. PointBert randomly masks a number of points and learns stability features by reconstructing the masked points. 

\subsection{Implementation Details}
\subsubsection{\textbf{Pre-training}}

We uniformly sample the point cloud to 1024, 2048, and 8192 points to match the recommended setting of different backbones. Rendered images and texts are pre-processed as the pre-trained image-text encoder requirements. We use SLIP~\cite{mu2022slip} instead of the original CLIP model to get better performance, while the image and text encoder in our experiment is frozen, just like ULIP. During training, only the parameters in point cloud backbone is trainable. We train JM3D for 250 epochs. The batch size is 128 with a learning rate of . And AdamW is the optimizer, while the Cosine LR schedule is utilized.

\subsubsection{\textbf{Zero-shot 3D Classification}}

JM3D measures the distance between the point cloud features and the text features from new datasets, while the category with the smallest distance is selected. The pre-processing of text and point cloud is the same as pre-training, and there is no finetuning stage involved. We conduct zero-shot evaluations on both ModelNet40 and ScanObjectNN. The former dataset consists of a synthetic model and unseen categories, the results of which can indicate the alignment effects of multi-modal features. ScanObjectNN is a challenged dataset with real-world scanned data, which challenges the robustness of 3D pre-trained models.





\subsection{Zero-shot 3D Classification}



JM3D achieves zero-shot 3D recognition by computing the similarity between point cloud features and textural features. In this section, we present experimental results that demonstrate the cross-modal understanding capabilities of JM3D by conducting zero-shot experiments on two different evaluation sets. We use the top-1 accuracy as the main metric, which indicates the ability of aligned features in the sense of practical application. 

\subsubsection{\textbf{Evaluation Sets.}} 
To be fair with the previous methods~\cite{xue2022ulip, zhang2022pointclip, hegde2023clip}, we conduct experiments on the ModelNet40 and ScanObjctNN datasets by a single model. The ``All'' column in Tab.~\ref{tab:zero-shot-modelnet} represents results on all test samples. Furthermore, in order to distinguish similar classes that appear in the ModelNet and pre-trained datasets, which clearly affect the fairness of the zero-shot evaluation, we separately create a ``Medium'' set and a ``Hard'' set for ModelNet40. The ``Medium'' set excludes the common classes between ModelNet40 and ShapeNet55, while the ``Hard'' set further excludes semantically similar classes, \emph{e.g.}, ``chair'' vs ``stool'', to ensure that all categories in this set have not been leaked. For each set, we compute top-1 accuracy and top-5 accuracy as evaluation metrics. Top-1 refers to the model finding the text with the smallest distance to the point cloud feature, and we believe that the top-1 metric is more intuitive in demonstrating the model's zero-shot ability.

\subsubsection{\textbf{Experiment Results.}}
We present the zero-shot results on ModelNet40 and ScanObjctNN in Tab.~\ref{tab:zero-shot-modelnet}. Firstly, our method outperforms previous SOTA ULIP~\cite{xue2022ulip} across all 3D backbones on the top-1 accuracy on all sets with varying degrees of improvement. Specifically, compared to the ULIP, our best method, JM3D + PointMLP, in the case of the same backbone, still improves the top-1 accuracy on the ``All'' set by 4.3\%, on ``Medium'' set by 12.3\%, and on ``Hard'' set by 13.7\%, which shows that superiority of JM3D. Then, we also demonstrate the effectiveness of JM3D on ScanObjectNN. It can be seen that our JM3D + PointMLP approach outperforms ULIP in terms of top-1 accuracy by 2.9\%. All in all, we take impressive progress beyond the previous SOTA method ULIP~\cite{xue2022ulip}. This indicates that JM3D has good generalization and performs better in actual scanning scenarios.







\subsection{Ablation Study}

To investigate the exact contributions of SMO and JMA to the pre-training process, we conduct independent ablation studies of the two modules on PointMLP. Considering that the goal of validation is the alignment of point cloud features and image-text features, we respectively use the zero-shot metrics on both ModelNet40 and ScanObjectNN and cross-modal retrieval as qualitative evaluation criteria.





\subsubsection{\textbf{Continuous Multi Views vs. Random One Look}}

The comparison result is illustrated in Tab.~\ref{tab:ablation-image}. First, we conduct an ablation study on the number of images to investigate the impact of different numbers of viewpoint images. We found that directly introducing multiple viewpoints leads to a falling on the performance. This is because it disrupts the semantic continuity of different viewpoint features in a crude manner, causing the model to be confused with the different information from multiple viewpoints. It can be seen that with the addition of embeddings, the model's ability to distinguish viewpoints is enhanced, resulting in a 2.3\% improvement. After adding within-view sampling, the semantic continuity of viewpoints is further ensured with a 1.9\% improvement. However, too many images push the model more likely to align with image features, which results in a performance drop when the number of images increases from 2 to 4. At the same time, the image retrieval ability improves as shown in Fig~\ref{fig3}, which will be further explained in Appendix.\ref{sec:cross-retrival}.



\subsubsection{\textbf{Hierarchical Text vs. Pre-difined Text}}

After demonstrating the effectiveness of CIS, we show the role of the text tree in Tab.~\ref{tab:ablation-text}. We can see that simply introducing subcategory text does not bring significant improvement, only leads to a increase of 0.6\% on top-1 accuracy. This indicates that the granularity of the text is not the key issue. However, after introducing the structured category tree, {i.e.}, HTT, a 1.3\% improvement is achieved. Meanwhile, we notice that the top-5 accuracy with subcategories decreases, which is because more categories increase the difficulty of aligning language features. In general, through HTT, it reduces the distance between samples within each parent category cluster. HTT brings structured semantic information, which improves the alignments between point cloud features and text features.



\subsubsection{\textbf{Collaborative Alignment vs. Independent Alignment}}

As shown on Tab.~\ref{tab:ablation-joint}, JMA brings a 2.7\% improvement in the top-1 accuracy, which demonstrates the effectiveness of the JMA in experience. Even with the high-quality data organization brought by SMO, the improvement in the model's performance is not significant. Due to the incorrect alignment methods, the alignment of point cloud features and the alignment of text-image features is unstable. Adding JMA allows us to extend the strong assumption of independent alignment with text-image modality to the joint modeling scenario, significantly improving the alignment of point clouds.

\section{Conclusion}

We propose JM3D, a comprehensive pre-training framework with SMO and JMA that seamlessly integrates language, image, and point cloud features into a unified semantic space without any specialized design. By meticulously organizing data, the SMO module fully leverages information from each modality and the JMA module pioneers joint modeling to optimize modality alignment. Ablation studies validate the efficacy of the proposed SMO and JMA. Additionally, JM3D's superior performance in zero-shot 3D classification and image retrieval tasks, setting a new state-of-the-art, highlights its exceptional cross-modal capabilities. In the future, we will explore diverse data and alternative joint modeling methods to further expand the unified representation learning in 3D.


\begin{acks}

This work was supported by National Key R\&D Program of China (No.2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), China Postdoctoral Science Foundation (No.2023M732948), and the Natural Science Foundation of Fujian Province of China (No.2021J01002,  No.2022J06001).
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{acmart}


\clearpage


\appendix

\begin{table}[htb]
    \small
    \centering
        \caption{The semantic segmentation on S3DIS.}
    \begin{tabular}{lcc}
    \toprule
         Model& Overall Acc & Class avg IoU \\
         \midrule
PointNet++ (ssg) \cite{qi2017pointnet++} &  83.0 & 53.5 \\
         PointNet++ (ssg) \cite{qi2017pointnet++} + JM3D &  \textbf{83.6} & 
 \textbf{57.1}\\
\bottomrule
    \end{tabular}

    \label{tab:seg-semantic}
\end{table}

\begin{table}[htb]
    \small
    \centering
    \caption{The Part segmentation on ShapeNet.}
    \begin{tabular}{lcc}
    \toprule
         Model& Instance avg IoU & Class avg IoU \\
         \midrule
SageMix \cite{lee2022sagemix} &  85.4 & - \\
         PointNet++ (ssg) \cite{qi2017pointnet++} &  84.9 & 81.8 \\
         PointNet++ (ssg) \cite{qi2017pointnet++} + JM3D &  \textbf{85.5} & 
 \textbf{82.1}\\
\bottomrule
    \end{tabular}
    \label{tab:seg-part}
\end{table}

\begin{table}[htb]
    \caption{3D classification results on ScanObjectNN. We follow the default settings of the original method to train on the hardest set. JM3D achieves a significant improvement compared to the previous method, helping to improve the original backbone by 3.5\%.}
    \centering
    \begin{tabular}{lcc}
    \toprule
         Model& Overall Acc & Class-mean Acc \\
         \midrule
         PointNet \cite{qi2017pointnet} &  68.2 & 63.4 \\
PointNet++ \cite{qi2017pointnet++} &  77.9 & 75.4 \\
DGCNN \cite{wu2018dgcnn} &  78.1 & 73.6 \\
MVTN \cite{hamdi2021mvtn} &  82.8 &  --\\
PointBERT \cite{yu2022point} &  83.1 &  --\\
RepSurf-U \cite{ran2022surface} & 84.6 &  --\\
PointMAE \cite{pang2022masked} & 85.2 &  --\\
RepSurf-U (2x) \cite{ran2022surface} &  86.0 &  --\\

         P2P \cite{wang2022p2p} & 89.3 & --\\
         \midrule
PointMLP \cite{marethinking} &  85.7 & 84.4 \\
PointMLP+ ULIP &  88.8 & 87.8 \\
         PointMLP + JM3D & 89.2 & 88.4 \\
         PointMLP + JM3D & \textbf{89.5} & \textbf{88.7} \\
\bottomrule
    \end{tabular}
    \label{tab:fintune-scan}
\end{table}

\section{3D classification Fine-tuning}
In order to show the potential of JM3D, we conduct finr-tuning experiments on ScanObjectNN based on the one of the SOTA frameworks, PointMLP~\cite{marethinking}.

During fine-tune stage, we only train the 3D encoder of JM3D on the \emph{PB\_T50\_RS} set of ScanObjectNN. This is because that the \emph{PB\_T50\_RS} set is a tough split of real-world scanned object with the background noise. We fine-tune PointMLP with the learing rate of 0.03, and weight decay is 3e-4 for 350 epochs.We directly hot start the pre-trained parameters and conduct fine-tuning entirely under the original model's conditions. We adhere to the standard practice in the research community by utilizing OA (Overall Accuracy) and mAcc (Class Average Accuracy) as our evaluation metrics. 

As shown on Tab.~\ref{tab:fintune-scan}, The performance of the backbone model is significantly improved with the assistance of JM3D. Specifically, JM3D takes 3.5\% improvement on PointMLP on OA and 4.0\% on mAcc. In addition, PointMLP + JM3D, outperforms the previous SOTA method RepSurf-U () by 3.2\%. With the voting stategy, PointMLP + JM3D sets a new SOTA. Compared to the training method used in ULIP, JM3D outperformed it, indicating that JM3D has remarkable capabilities in directly enhancing existing models without any specially designed structures. 

\section{Details of Sets on ModelNet40 in zero-shot Classification}

As mentioned in the main text, we notice the shared categories between Shapenet55 and ModelNet40. To ensure a fairer comparison, we exclude similar categories and construct different validate sets for ModelNet40.

\noindent\textbf{All set}: all the categroyies in ModelNet40, which is shown in Tab.~\ref{tab:ModelNet40-All-Set}

\begin{table}[htb]
    \small
    \caption{ModelNet40 All Set.}
    \begin{tabular}{ccccc}
        \toprule
        airplane & bathtub & bed & bench & bookshelf \\
        \midrule
        bottle & bowl & car & chair & cone \\
        \midrule
        cup& curtain& desk& door& dresser \\
        \midrule
        flower\_pot& glass\_box& guitar& keyboard& lamp \\
        \midrule
        laptop& mantel& monitor& night\_stand& person \\
        \midrule
        piano& plant& radio& range\_hood& sink \\
        \midrule
        sofa& stairs& stool& table& tent \\
        \midrule
        toilet& tv\_stand& vase& wardrobe& xbox \\
        \bottomrule
    \end{tabular}
    \label{tab:ModelNet40-All-Set}
\end{table}

\noindent\textbf{Medium Set}: We exclud categories that have exactly the same names as those in our pre-training dataset ShapeNet55. The categories remaining in this set are displayed in Tab.~\ref{tab:ModelNet40-Medium-Set}.

\begin{table}[htb]
    \small
    \caption{ModelNet40 Medium Set.}
    \begin{tabular}{ccccc}
        \toprule
        cone& cup& curtain& door& dresser \\
        \midrule
        glass\_box& mantel& monitor& night\_stand& person \\
        \midrule
        plant& radio& range\_hood& sink& stairs \\
        \midrule
        stool& tent& toilet& tv\_stand& vase \\
        \midrule
        wardrobe& xbox \\
        \bottomrule
    \end{tabular}

    \label{tab:ModelNet40-Medium-Set}
\end{table}

\noindent\textbf{Hard Set}: Both the exact category names and their synonyms present in ShapeNet55 have been removed. The resulting set, known as the \emph{Hard Set}, is presented in Tab.~\ref{tab:ModelNet40-Hard-Set}.

\begin{table}[htb]
    \small
        \caption{ModelNet40 Hard Set.}
    \begin{tabular}{ccccc}
        \toprule
        cone& curtain& door& dresser& glass\_box \\
        \midrule
        mantel& night\_stand& person& plant& radio \\
        \midrule
        range\_hood& sink& stairs& tent& toilet \\
        \midrule
        tv\_stand& xbox \\
        \bottomrule
    \end{tabular}

    \label{tab:ModelNet40-Hard-Set}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{fig3.pdf}
\caption{The qualitative results of the real image to point cloud retrieval. Giving an image, We show the top-3 point cloud retrieval results from ModelNet40. All models perform well on the simple samples (the 1st row and the 3rd row). However, when it comes to the challenging samples (the 2nd row and the 4th row), JM3D demonstrates a more accurate retrieval ability compared to the previous state-of-the-art (ULIP). The JM3D trained with 4 view images shows better performance compared with the 2 view images, benefiting the more solid bias of vision modality.}
\label{fig3}
\end{figure*}

\section{Cross-modal Retrieval}\label{sec:cross-retrival}

Another noteworthy aspect is that JM3D confers enhanced cross-modal capabilities to the foundational point cloud model. In addition to the conducted quantitative analysis aligned with the language modality above, this section primarily showcases the qualitative results of image interaction ability enabled by JM3D."

We collect some images from a real-world dataset, \emph{i.e.}, Caltech101~\cite{fei2004learning} to retrieve 3D models from the ModelNet40 test set, which is a medium-scale dataset with more than 2.5K models across 40 categories. Meanwhile, some more challenging samples are constructed, which often possess unique perspectives, making them difficult to be recognized by conventional models. The top-3 retrieval models are presented in Fig.~\ref{fig3}.

In Fig.~\ref{fig3}, samples are belonging to the categories of "airplane" and "laptop", with each category further divided into two levels: a challenging level (top) and a simple level (bottom). It is apparent that for simplistic samples, all models exhibit a significant level of retrieving. Nonetheless, when the image is captured from an uncommon perspective, ULIP is unable to identify the appropriate point cloud. In contrast, JM3D trained with two views can identify some of the correct outcomes, whereas, with an increase in the number of images to 4 in the CIS, JM3D can successfully locate almost all of the appropriate models. The visualizations show an excellent sign that our model has learned meaningful features across visual and 3D modalities. Furthermore, this suggests that although an increase in the number of views in the CIS may have a minor impact on the text-based performance as Tab.~\ref{tab:ablation-image}, it can greatly enhance the alignment capability of the model in the image domain.

\section{Semantic Segmentation and Part Segmentation}

we have conducted semantic segmentation experiments on the S3DIS~\cite{armeni20163d} dataset, and partial segmentation experiments on the ShapeNet~\cite{chang2015shapenet}, the results of which are recorded in Tab.~\ref{tab:seg-semantic} and Tab.~\ref{tab:seg-part}. Our proposed method has continued to demonstrate great performance, achieving scores of 57.1 and 82.1 on the S3DIS and ShapeNet datasets respectively, and correspondingly increasing by 3.6 and 0.3 points. These scene understanding tasks indisputably demonstrate the effectiveness of our approach.

\end{document}
