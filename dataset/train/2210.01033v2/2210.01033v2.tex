
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{caption}
\usepackage[hidelinks]{hyperref}
\usepackage{url}
\usepackage{color}
\usepackage[pdftex]{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{bbding}
\newcommand{\pz}[1]{{\color{blue}{[#1]}}}
\newcommand{\et}{\emph{et al.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\ie}{\emph{i.e.}}


\title{LPT: Long-tailed Prompt Tuning  for Image Classification}



\author{Bowen Dong \quad
Pan Zhou \quad
Shuicheng Yan \quad
Wangmeng Zuo\textsuperscript{\Envelope} \\
Harbin Institute of Technology  \quad National University of Singapore \quad  Peng Cheng Laboratory \\
\texttt{\{cndongsky,panzhou3,shuicheng.yan\}@gmail.com, wmzuo@hit.edu.cn} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
For long-tailed classification tasks, most works often pretrain a big model on a large-scale (unlabeled) dataset, and then fine-tune the whole pretrained  model for  adapting to long-tailed data. Though promising, fine-tuning the whole pretrained model tends to suffer from high cost in computation and deployment of different models for different tasks, as well as weakened generalization capability for overfitting to certain features of long-tailed data. 
To alleviate these issues, we propose an effective Long-tailed Prompt Tuning (LPT) method for long-tailed classification tasks. LPT introduces several trainable prompts into a frozen pretrained model to adapt it to long-tailed data. For better effectiveness, we divide prompts into two groups: 1) a shared prompt for the whole long-tailed dataset to learn general features and to adapt a pretrained model into the target long-tailed domain; and 2) group-specific prompts to gather group-specific features for the samples which have similar features and also to empower the pretrained model with fine-grained discrimination ability. Then we design a two-phase training paradigm to learn these prompts. In the first phase, we train the shared prompt via conventional supervised prompt tuning to adapt a pretrained model to the desired long-tailed domain. In the second phase, we use the learnt shared prompt as query to select a small best matched set for a group of similar samples from the group-specific prompt set to dig the common features of these similar samples, and then optimize these prompts with a dual sampling strategy and the asymmetric Gaussian Clouded Logit loss. By only fine-tuning a few prompts while fixing the pretrained model, LPT can reduce training cost and deployment cost by storing a few prompts, and enjoys a strong generalization ability of the pretrained model. Experiments show that on various long-tailed benchmarks, with only 1.1\% extra trainable parameters, LPT achieves comparable or higher performance than previous whole model fine-tuning methods, and is more robust to domain-shift. Code is publicly available at \url{https://github.com/DongSky/LPT}. 


\end{abstract}

\vspace{-1.3em}
\section{Introduction}\label{sec:intro}
\vspace{-0.7em}
Learning from  long-tailed data~\citep{cui2019cbloss,Kang2020Decoupling,zhang2021deep} is very challenging in the deep learning era, since networks often excessively overfit to majority classes while ignoring the minority classes  due to the  overwhelming training sample number of majority classes. To eliminate this negative effect, previous methods focus on three individual aspects: \textbf{1)} re-sampling the long-tailed data distribution~\citep{Kang2020Decoupling,Li2022Long,li2021metasaug,ren2020metasoftmax} to achieve balance among all classes in each minibatch data, \textbf{2)} re-weighting the training loss~\citep{cui2019cbloss,Li2022Long,menon2021longtail} to give heavier weights to minority classes, and \textbf{3)} specially-designed decoupled training~\citep{Kang2020Decoupling}, knowledge distillation~\citep{li2021self} or ensemble learning~\citep{zhou2020bbn,wang2020long}. 



Although alleviating the negative effect in long-tailed learning in some sense and achieving  better overall    performance, these methods generally need to train both feature extractors and linear classifiers from scratch or from  pretrained models on  large-scale datasets, \eg~ImageNet~\citep{deng2009imagenet}, thus suffering from three  issues. Firstly,  to  adapt  to long-tailed data,  this whole model fine-tuning  requires much higher extra training cost. Secondly, fine-tuning whole model also impairs the generalization ability of the pretrained model, since the  pretrained model trained on a large-scale dataset often sees abundant data and enjoys  strong discriminative ability to various kinds of features, while  fine-tuning often weaken  this generalization ability caused by the overfitting to certain features of long-tailed data and hardly handle domain-shift or out-of-distributed data which occur frequently in long-tailed learning.   Finally, fine-tuning also results in  very  different models for different learning tasks, which  destroys model compatibility and increases  practical deployment cost.



\begin{figure}[tb]
\vspace{-1em}
	\centering
	\subfigure[{Places-LT}]{
		\includegraphics[width=2in]{figures/placeslt.pdf}
		\label{fig:per_placeslt}
	} \subfigure[{iNaturalist 2018}]{
		\includegraphics[width=2in]{figures/inat18.pdf}
		\label{fig:per_inat18}
	} \\
	\vspace{-1.1em}
	\caption{Comparison among  SoTA long-tailed  methods on the Places-LT dataset and iNaturalist 2018 dataset, where the size of each spot indicates the model size of the overall network, including model backbone, classier and prompts. Our LPT only needs 1.1\% additional trainable parameters while achieving comparable or higher  accuracy on two highly  long-tailed datasets. }
   \label{fig:param_acc_comp}
	\vspace{-1.4em}
\end{figure}



\textbf{Contributions.} To  alleviate the above issues, we propose a novel and effective Long-tailed Prompt Tuning (LPT) approach.  Specifically, LPT builds on a pretrained model, \eg~vision transformer (ViT)~\citep{dosovitskiy2021an}, and introduces extra trainable  prompts into this pretrained model, and finally only fine-tunes these prompts  for adapting the pretrained model to long-tailed data at hand.  For prompts, there are two kinds, \textbf{1)} shared prompt for all classes to learn  general features (knowledge) and to adapt a pretrained model into the target domain; and \textbf{2)} group-specific prompts to  gather group-specific features for those samples which have similar features and also to empower the pretrained model with fine-grained distinguishing ability. For effective training, we design a two-phase training framework to learn these two kinds of prompts. In the first phase, LPT optimizes the shared prompt and a classifier on a long-tailed training dataset of interest. For this phase, its target is to \textbf{1)} adapt the pretrained model to the target domain of  interest via prompt tuning, and \textbf{2)} to empower the pretained model with the trained classifier the discriminative ability to the training data which is a basic to learn the group-specific prompts.  During the second phase, we learn the newly added group-specific prompt set and further fine-tune classifier used in the first phase.  Specifically, given an input, LPT feeds it into the pretrained model with  the learnt  shared prompt, and views  the output class token as the query to select  a small set of matched prompts via computing the cosine similarity  between query and the corresponding keys from group-specific prompt set. Next, the trainable matched group-specific prompts 
 are introduced into the pretrained model with shared prompts to help learn class-specific attributes, and is trained by asymmetric Gaussian Clouded Logit (A-GCL) loss~\citep{Li2022Long} with a dual sampling strategy.








This LPT can well alleviate the above three issues in existing methods as aforementioned. For training cost, LPT only needs to fine-tune a few prompts whose size  is much smaller than the pretrained model, and thus uses much less training cost than fine-tuning whole pretrained model for adaptation. As for generalization ability, LPT only fine-tunes prompt while fixing the pretraining model, and thus enjoys the strong generalization capacity of the pretrained model. On compatibility, LPT shares a pretrained model for different learning tasks, and only needs to store the small-sized prompts, largely increasing  the model compatibility and reducing practical deployment cost. 










As shown in Fig.~\ref{fig:param_acc_comp},  on various long-tailed classification benchmarks, with only 1.1\% additional  parameters of prompts, LPT achieves comparable or higher performance than the previous methods which fine-tunes whole pretrained model.  
Especially, with only vision-based data for training and testing, LPT achieves 50.1\% overall classification accuracy and 46.9\% few-shot accuracy on Places-LT dataset~\citep{zhou2017places}, and makes 8.9\% and 11.6\% improvement over the  previous  methods trained on vision-only data.
Besides, more experimental results shows the superiority of LPT and also its generalization and robustness on long-tailed data and also  domain shifted data.


\vspace{-0.8em}
\section{Related Work}\label{sec:related}
\vspace{-0.6em}
\textbf{Long-tailed Image Classification. }To tackle negative effect from the highly imbalanced data distribution, previous works mainly focus on three different aspects, \ie: 
data re-sampling~\citep{Kang2020Decoupling,li2021metasaug,ren2020metasoftmax}, which utilizes hand-crafted samplers~\citep{Kang2020Decoupling}, data augmentation~\citep{li2021metasaug} or meta-learning-based sampler~\citep{ren2020metasoftmax} to balance the training data among head and tail classes; 
loss re-weighting~\citep{cui2019cbloss,menon2021longtail,Li2022Long,Jamal_2020_CVPR,tan2020equalization}, which focuses on adding hand-crafted bias into the confidence scores~\citep{menon2021longtail,Li2022Long}, re-scaling logits by hand-crafted weights~\citep{cui2019cbloss,tan2020equalization}, or meta-learning-based methods~\citep{Jamal_2020_CVPR}; 
and decoupled training strategies~\citep{Kang2020Decoupling,li2021self} as well as ensemble learning methods~\citep{zhou2020bbn,wang2020long}. Recently, some vision-language-based methods~\citep{ma2021simple,tian2021vl,Long2022} have been proposed, which introduce extra language data~\citep{ma2021simple,tian2021vl} or external database~\citep{Long2022} to generate auxiliary confidence scores during training and testing, finally fine-tuning the whole CLIP-based model on long-tailed data. 
Different from above methods which fully fine-tune all parameters,
we aim to leverage the powerful unbiased feature representation ability of pretrained models, and construct a prompt tuning method to obtain a flexible yet accurate classifier from long-tailed data.

\textbf{Efficient Tuning. }Efficient Tuning methods (including prompt~\citep{lester2021power,jia2022vpt}, adapter~\citep{pmlr-v97-houlsby19a,he2022towards,nie2022protuning,chen2022convadapter}, LoRA~\citep{hu2022lora} and others~\citep{frankle2021training,touvron2022things}) are designed to utilize representation ability from pretrained models, and fine-tune only a few trainable parameters to achieve better performance on downstream tasks~\citep{zhai2019largescale,lin2014microsoft,zhou2017scene}. 
In this paper we focus on prompt tuning~\citep{zhou2022learning,jia2022vpt,bahng2022exploring}. Specifically, \citet{jia2022vpt} introduced prompt into ImageNet~\citep{deng2009imagenet} pretrained ViT~\citep{dosovitskiy2021an};
while \citet{bahng2022exploring} inserted the prompt on the edges of images and optimize prompts. \citet{wang2022learning} also introduced prompt tuning method into continue learning framework, which used multiple learnable prompts to handle corresponding tasks. 
Different from above works, LPT focuses on exploring the transfer ability of prompt tuning with large-scale while highly imbalanced training data, thus achieving comparable and accuracy. 

\vspace{-0.4em}
\section{Preliminary Study}\label{sec:preliminary}
\vspace{-0.2em}
\subsection{Performance Investigation of VPT}\label{sec:pre_exp}
\vspace{-0.4em}
Prompt tuning in previous study~\citep{zhou2022learning,jia2022vpt} focuses on fine-tuning with limited data from balanced distribution~\citep{zhai2019largescale}, while its transfer learning ability on large-scale long-tailed data~\citep{zhou2017places,van2018inaturalist} is not explored. To start our method, we first quantitatively evaluate whether prompt tuning benefits long-tailed learning or not. To this end,  we investigate ViT-B~\citep{dosovitskiy2021an} pretrained on ImageNet-21k~\citep{deng2009imagenet} by  comparing the performance of linear probing and a  prompt tuning method, \ie~VPT~\citep{jia2022vpt} because of its effectiveness,  on the large-scale Places-LT dataset~\citep{zhou2017places}. 
Specifically, linear probing aims to fine-tune a linear classifier at the top of a pretrained and fixed feature extractor (\eg, ViT~\citep{dosovitskiy2021an}); while VPT often concatenates input tokens with  learnable  prompts (tokens) and a  linear classifier at top of a pretrained model. During training,    we use these two methods to independently optimize their learnable parameters for 20 epochs with well-tuned hyper-parameters, \eg, SGD with learning rate of 0.02 and weight decay of 1e-4.  



Table~\ref{table:preliminary} summarizes the  quantitative results of linear probing and VPT. Without class-balanced sampling, VPT achieves 37.52\% overall accuracy and surpasses linear probing by 3.94\%, 3.33\%, 4.52\% in terms of many/medium/few-shot accuracy, respectively. 
Especially, after introducing class-balanced sampling~\citep{Kang2020Decoupling} which first randomly sample classes from training set then randomly sample inputs with equal numbers in each iteration,
VPT achieves 44.17\% overall accuracy and even surpasses the counterpart by 8.67\% in terms of few-shot accuracy. 
Based on the observation, we conclude that: \textbf{a)} prompt tuning can consistently improve the overall performance in long-tailed classification; and \textbf{b)} prompt tuning are more robust to long-tailed distribution and benefits more to tail categories.  However, from Table~\ref{table:preliminary}, one can also observe that the performance of prompt tuning on long-tailed problem is not sufficient and yet far behind  state-of-the-arts.  

\begin{table}[t]
   \caption{Prompt tuning results on Places-LT~\citep{zhou2017places}. Prompt tuning achieves better accuracy on all classes  and  tail classes (\ie~``Few'' in the table)  with different training settings.}
   \vspace{-1em}
\begin{center}
	  		\setlength{\tabcolsep}{10.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|cccc}
   \toprule
   {\bf Method}  &{\makecell{\bf Balanced \\ \bf Sampling}} &{\makecell{\bf Tuned Params\\ \bf (w/o classifier)}} & {\bf Overall} & {\bf Many} & {\bf Medium} & {\bf Few}\\ 
   \midrule
   Linear & - & 0 & 33.29\% & 46.48\% & 29.45\% & 18.77\% \\
   VPT & - & 92K & \textbf{37.52\%} & \textbf{50.42\%} & \textbf{32.78\%} & \textbf{23.29\%} \\
   \midrule
   Linear & \checkmark & 0 & 41.33\% & \textbf{49.47\%} & 41.31\% & 27.51\% \\
   VPT & \checkmark & 92K & \textbf{44.17\%} & 45.79\% & \textbf{46.73\%} & \textbf{36.18\%} \\
   \bottomrule
   \end{tabular}}}
   \end{center}
   \label{table:preliminary}
   \vspace{-1em}
   \end{table}


\begin{figure}[tp]
   \centering
   \begin{minipage}{0.46\textwidth}
\centering
            
      \includegraphics[width=0.8\textwidth]{figures/pre-domain-shift.pdf}
      \caption{LDA visualization of VPT.\vspace{-0.6em} } 
      \label{fig:pre_domain_shift}
   \end{minipage}
\hspace{3mm}
   \begin{minipage}{0.5\textwidth}
      \renewcommand\arraystretch{1.3}
\centering
      \captionsetup{type=table}
      \caption{Quantitative analysis of features learned by pretrained ViT-B and VPT. Features from VPT obtain better discriminative ability in terms of cluster compactness and also KNN accuracy.\vspace{-0.8em}}
\setlength{\tabcolsep}{6.6pt} \renewcommand{\arraystretch}{3.5}{ \fontsize{8.3}{3}\selectfont{
      \begin{tabular}{l|cc}
         \toprule
         \textbf{Method} & {ViT-B}  & {VPT} \\
         \midrule
         \bf Pretrain Data & IN21k & IN21k \\
         \hline
         \bf Fine-tuned & - & \checkmark \\
         \hline
         \bf Inner-class distance  & 2.360.52 & \textbf{1.820.43} \\
         \bf Inner-class / inter-class  & 0.171 & \textbf{0.128} \\
\bf K-NN Acc & {30.80\%} & \textbf{31.90\%} \\
         \bottomrule
      \end{tabular}}}
      \label{table:pre_discriminative}
   \end{minipage}
   \vspace{-1.5em}
\end{figure}

\vspace{-0.3em}
\subsection{Analysis of Prompt Tuning}\label{sec:pre_analysis}
\vspace{-0.4em}
Nevertheless, the reason why prompt tuning improves the performance on long-tailed learning tasks is still unclear. 
To quantitatively and qualitatively analyze prompt tuning, we conduct a series of experiments on Places-LT~\citep{zhou2017places}. 
We first adopt Linear Discriminant Analysis (LDA) to investigate the learned prompt from domain adaptation  perspective. 
Specifically, we use the pretrained ViT-B and the  ViT-B fine-tuned by VPT on Places-LT in Sec.~\ref{sec:pre_exp} to extract features of ImageNet \textit{val} set and Places-LT \textit{val} set, and then employ the above features to obtain corresponding LDA vectors for visualization. 
From the qualitative result in Fig.~\ref{fig:pre_domain_shift}, one can easily find that \textbf{a)} for pretrained ViT-B, its extracted features from ImageNet (red cluster) are far from its features  from Places-LT (green cluster); \textbf{b)} for VPT  fine-tuned ViT-B, its extracted features from ImageNet (yellow cluster) align with its features  from Places-LT (blue cluster) and are close to each other. So these observations indicate that \textbf{1)} \textit{the learned prompts in VPT could help align the fine-tuned data distribution (Places-LT) with the pretrained data distribution (ImageNet), and thus can make the pretrained model adapt to the target domain for the long-tailed learning tasks}.




Next we investigate the learned prompt from group-specific perspective.   
Specifically, for each class in Places-LT, we treat samples  in this class as a group (cluster); then for each group  ( with total  classes in dataset), we calculate average  distance between each sample and its corresponding group center, and views this average  distance as inner-class distance  of each group. 
Furthermore, we also define the inter-class distance  as the average distance between any two group centers, and then calculate the ratio  between the average of inner-class distance   and the inter-class distance , namely,  .  
Intuitively,  for a group, the smaller   inner-class distance , the more compact of the group. So if  is smaller, then the groups are more discriminable.   Thus, we use  as a metric to measure whether the learnt features are distinguishable, and report the  statistic results  in Table~\ref{table:pre_discriminative}.  One can observe that features from VPT fine-tuned pretrained model achieves smaller  average  inner-class distance  and also smaller ratio  than those in the vanilla pretrained model,   indicating  that features of different classes in VPT  are easier to be distinguished. 
Moreover, we also conduct K-NN evaluation between the pretrained  ViT-B and VPT fine-tuned pretrained ViT-B. Table~\ref{table:pre_discriminative} shows that VPT surpasses vanilla pretrained ViT-B by 1.1\% in terms of K-NN accuracy, indicating the higher  discriminative ability of a  VPT  fine-tuned  model.  Therefore, one can conclude that \textbf{2)} \textit{the learned prompt can further improve the discriminative ability of pretrained models, thus benefiting to long-tailed classification problems}. 

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/lpt-pipeline.pdf}
\end{center}
\vspace{-0.5em}
\caption{Pipeline of Long-tailed Prompt Tuning, where \textbf{\textcolor{blue}{snow}} means freezed parameters and \textbf{\textcolor{orange}{fire}} means trainable parameters. 
For Phase 1, LPT learns shared prompt to capture general knowledge for all classes. For Phase 2, LPT uses fixed shared prompt with ViT to generate query, then select best matched prompt from group-specific prompts, finally adopts prompt in the last  blocks. 
}
\vspace{-1em}
\label{fig:pipeline}
\end{figure}

\vspace{-0.5em}
\section{Long-Tailed Prompt Tuning}\label{sec:method}
\vspace{-0.5em}
The  observations in Sec.~\ref{sec:preliminary} inspire us to design an efficient yet effective long-tailed learning method based on prompt tuning~\citep{jia2022vpt}. 
Nevertheless, vanilla VPT in long-tailed learning still lags behind the state-of-the-art methods~\citep{tian2021vl,Long2022}. 
To further improve the overall performance of prompt tuning in long-tailed learning, we propose an effective \textit{Long-tailed Prompt Tuning (LPT)} method, whose framework and training procedure are illustrated  in Fig.~\ref{fig:pipeline}. 
Generally, LPT includes a shared prompt for all classes to learn general features or knowledge and to adapt a pretrained model into the target domain meanwhile empowering the discriminative ability to the training data; and group-specific prompts to gather group-specific features and to  further fine-tune classifier used in the first phase for higher performance.  
Two sets of prompts are optimized by shared prompt tuning and group prompt tuning, respectively. 
We introduce our  LPT as follows.

\vspace{-0.3em}
\subsection{Phase 1: Shared Prompt Tuning}\label{sec:method_phase1}
\vspace{-0.4em}
For Shared Prompt Tuning phase in Fig.~\ref{fig:pipeline}, with given pretrained ViT~\citep{dosovitskiy2021an} with  layers, we aim to optimize the shared prompt  and cosine classifier , where  follows VPT-Deep~\citep{jia2022vpt} and consists of  individual learnable token sequences. 
Specifically, with given input image , LPT obtains the initial patch tokens  via the pretrained patch embedding layer. 
Then, with given class token (\text{[CLS]})  and pretrained transformer encoder, for the i-th layer in ViT, where , we define the query used in i-th block as , and corresponding key and value , then update  with  by, 

where  denotes a token concatenation operation along the token number direction,   and  are the self-attention layer and feed-forward network in the i-th pretrained ViT block~\citep{vaswani2017attention}. 
Then, the final class token  are fed into the cosine classifier  to calculate per-class confidence scores  . 
Finally, with given ground-truth  of corresponding input , we minimize  during the training of phase 1 to optimize  and , 
where  is the classification loss used in both phases and will be discussed in Sec.~\ref{sec:method_loss}.

\vspace{-0.3em}
\subsection{Phase 2: Group Prompts Tuning}\label{sec:method_phase2}
\vspace{-0.4em}
A straightforward solution to reduce the difficulty of long-tailed learning is dividing the training data into multiple groups via the similarity of features, thus sharing group-specific knowledge in each group and reducing the recognition difficulty.
Based on this motivation, to gather group-specific features for those samples which have similar features and also to empower the pretrained model with fine-grained discriminative ability,
we aim to use different group prompts to handle samples from different classes, thus gathering group-specific features via each group prompt,  benefiting to long-tailed classification. 
Therefore, we introduce group-specific prompts with  individual learnable prompts
, where  is the key of the corresponding i-th group prompt  and each  has  trainable token sequences. 
To reduce the computational cost and number of additional parameters, we keep using only shared prompt in the first  blocks and introduce group-specific prompt set  into the last  blocks. In this subsection,  we mainly discuss the training procedure of group prompts tuning.
Specifically, based on our observation
\text{(2)} in Sec.~\ref{sec:pre_analysis}, we select the query  from Phase 1 rather than using output class token from pretrained ViT like~\citet{wang2022learning},
since the class token  often enjoys stronger discriminative ability. Given the query , we select the best-matched prompts adaptively from  by:

where  returns the indices of prompts  with the largest  cosine similarities, and  means the cosine similarity operator. 
Here we discuss the optimization of keys. Intuitively, one straightforward method to optimize keys is enforcing queries from the same class to match  certain keys. However, this method is infeasible since it is difficult to interpret which classes could be matched into some certain prompts exactly. Instead, we prefer to simply minimize the distance between the matched queries and keys, thus optimizing these keys adaptively. 
We design such query function from this perspective. 
As observed in Sec.~\ref{sec:pre_analysis}, the feature cluster of each class generated by the fine-tuned phase 1 is compact. 
Therefore, for queries from the same class, if we randomly select a query  and a key  then minimize , distance between  and other queries are naturally minimized since these queries are fixed and compact enough. 
Thus during training, each key are learnt to close to one or multiple nearby clusters, finally guiding corresponding group prompt to gather group-specific feature. 
Moreover, since \textbf{1)} VPT~\citep{jia2022vpt} benefits from prompt ensembling, and \textbf{2)} introducing more group-specific knowledge may benefit to recognize samples for tail classes. Instead of using only one matched group prompt from , LPT also conduct prompt ensembling with multiple selected prompts, which is shown as:

thus resulting an ensembled group prompt . 
With given , LPT reuses the feature  from Phase 1 as  to save computational cost, then define the query used in -th block as , and key with value ,
finally update  as:

where  indicates the index of the last  pretrained blocks in ViT.
Next, the output class token  are fed into the cosine classifier  and calculate per-class confidence scores by . 
Finally, with given ground-truth  of corresponding input , we minimize  including both classification loss  as well as the cosine similarity between query  and corresponding matched keys , which is shown as Eqn.~\ref{eqn:phase2}:

where  is scale factor of  and will be discussed in the following. 

Note that naively using class-balanced sampling~\citep{Kang2020Decoupling} or instance-balanced sampling~\citep{Kang2020Decoupling} may lead to severe overfitting on tail classes or head classes~\citep{zhang2021deep} respectively.
To balance the performance between head classes and tail classes and avoid overfitting, we introduce dual sampling strategy. 
Specifically, for each training iteration in Phase 2, LPT randomly samples a mini-batch  from instance-balanced sampler as well as another mini-batch  from class-balanced sampler. For samples in , we simply set  to calculate ; and for samples in , we set , where  is the initialized weight for ,  denotes the maximum number of epochs, and  is the current epoch number.  

\vspace{-0.5em}
\subsection{Loss Function}\label{sec:method_loss}
\vspace{-0.4em}
Finally, we introduce the classification loss   used in our two-phase training. Though LPT  can use multiple classification losses to further improve the performance of LPT, we adopt asymmetric GCL loss  for both adjusting logits based on statistic label frequency from training data and re-weighting gradient between positve and negative classes. 
Without loss of generality, we use  calculated in the Phase 2 of LPT as example to demonstrate . Following~\citep{Li2022Long}, we re-scale the confidence score of i-th class by:

where  is the scaling factor,  is the random variable from gaussian distribution,  and  mean the label frequency of i-th class and the maximum label frequency in the training set, respectively. 
Then, we calculate per-class probability  by:

\begin{table}[t]
\caption{Comparison with state-of-the-art long-tailed classification methods on Places-LT dataset~\citep{zhou2017places}. Our LPT achieves state-of-the-art performance among vision-only pretrained methods and achieves the same performance with state-of-the-art VL-based methods.}
\vspace{-1em}
\label{table:placeslt}
\begin{center}
   \setlength{\tabcolsep}{3.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
\begin{tabular}{l|c|c|c|c|cccc}
\toprule
{\bf Method} & {\bf Backbone} &{\makecell{\bf Tuned \\ \bf Params}}&{\makecell{\bf Total \\ \bf Params}} &{\makecell{\bf Extra Data \\ \bf (Inference)}} & {\bf Overall} & {\bf Many} & {\bf Medium} & {\bf Few}\\ 
\midrule
\multicolumn{6}{l}{\textbf{Vision-only Pretrained}} \\
\midrule
OLTR~\citep{openlongtailrecognition} & Res152 & 60.34M & 60.34M & - & 35.9 & 44.7 & 37.0 & 25.3 \\
TADE~\citep{zhang2021test} & Res152 & 60.34M & 60.34M & - & 38.8 & 42.8 & 39.0 & 31.2 \\
LWS~\citep{Kang2020Decoupling} & Res152 & 60.34M & 60.34M & - & 37.6 & 40.6 & 39.1 & 28.6 \\
MisLAS~\citep{zhong2021mislas} & Res152 & 60.34M & 60.34M & - & 40.4 & 39.6 & 43.3 & 36.1 \\
ALA~\citep{zhao2022adaptive} & Res152 & 60.34M & 60.34M & - & 40.1 & 43.9 & 40.1 & 32.9 \\
PaCo~\citep{cui2021parametric} & Res152 & 60.34M & 60.34M & - & 41.2 & 36.1 & 47.9 & 35.3  \\
VPT~\citep{jia2022vpt} & ViT-B &\textbf{0.09M} & 86.66M & - &37.5 &\textbf{50.4} &33.8 &23.3 \\
LPT (Ours) & ViT-B &\textbf{1.01M} & 87.58M & - &\textbf{50.1}& 49.3& \textbf{52.3}& \textbf{46.9} \\
\midrule
\multicolumn{7}{l}{\textbf{Vision-Languge Pretrained with Extra Data}} \\
\midrule
RAC~\citep{Long2022} & ViT-B & 86.57M & 236.19M & IN21k Feat & 47.2 & 48.7 & 48.3 & 41.8 \\
BALLAD~\citep{ma2021simple} & ViT-B & 149.62M & 149.62M & - & 49.5 & 49.3 & \textbf{50.2} & \textbf{48.4} \\
VL-LTR~\citep{tian2021vl} & ViT-B &149.62M & 149.62M & Wiki Text & \textbf{50.1}& \textbf{54.2}& 48.5& 42.0 \\
\bottomrule
\end{tabular}}}
\vspace{-1em}
\end{center}
\end{table}
\begin{table}[h]
\centering
\begin{minipage}[t]{0.5\textwidth}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{Comparison with state-of-the-art methods on CIFAR100-LT with various imbalanced ratio . LPT performs best among all methods.}
    \vspace{-0.5em}
   \setlength{\tabcolsep}{8.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cccc}
      \toprule
      \bf Imb Ratio  & \bf 200 & \bf 100 & \bf 50 & \bf 10 \\
      \midrule
      \multicolumn{5}{l}{\textbf{Training from Scratch}} \\
      \midrule
PaCo & - & 52.0 & 56.0 & 64.2 \\
\citet{Zhu_2022_CVPR} & - & 51.9 & 56.6 & 64.9 \\
      \citet{Li2022Long} & 44.9 & 48.7 & 53.6 & - \\
      \midrule
      \multicolumn{5}{l}{\textbf{Vision-only Pretrained}} \\
      \midrule
      VPT & 72.8 & 81.0 & 84.8 & 89.6 \\
LPT (Ours) & \textbf{87.9} & \textbf{89.1} & \textbf{90.0} & \textbf{91.0} \\
      \midrule
      \multicolumn{5}{l}{\textbf{VL Pretrained with Extra Data}} \\
      \midrule
      \citet{ma2021simple} & - & 77.8 & - & - \\
      \bottomrule
   \end{tabular}}}
   \label{table:comp_cifar100lt}
\end{minipage}\hspace{4mm}
\begin{minipage}[t]{0.44\textwidth}
\centering
   \caption{Comparison with state-of-the-art methods on iNaturalist 2018. LPT performs best among vision-only pretrained methods. }
    \vspace{-0.5em}
   \setlength{\tabcolsep}{8.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cc}
      \toprule
      \textbf{Method} & \textbf{Overall} & \textbf{Few-shot} \\
      \midrule
      \multicolumn{3}{l}{\textbf{Vision-only Pretrained}} \\
      \midrule
TADE & 72.9 & - \\
      PaCo & 75.2 & 74.7 \\
      ViT-B/16 & 73.2 & - \\
      \citet{iscen2021class} & 75.3 & 73.2 \\
      ViT-L/16 & 75.9 & - \\
      LPT (Ours) & \textbf{76.1} & \textbf{79.3} \\
      \midrule
      \multicolumn{3}{l}{\textbf{VL Pretrained with Extra Data}} \\
      \midrule
      VL-LTR & 76.8 & - \\
      RAC & \textbf{80.2} & \textbf{81.0} \\
      \bottomrule
   \end{tabular}}}
   \label{table:comp_inat18}
\end{minipage}
\vspace{-1em}
\end{table}
Next, we use asymmetric re-weighting~\citep{ridnik2021asymmetric} to eliminate the effect from negative gradient in long-tailed learning.
Suppose  is ground-truth class of , we calculate  as:

where  and  is the focusing parameter~\citep{lin2017focal} for ground-truth class and negative classes respectively.
Finally, we choose  during two-phase training of LPT.

\section{Experiments}\label{sec:exp}
\vspace{-0.5em}


\subsection{Comparison with State-of-The-Art Methods}\label{sec:comp_sota}
\vspace{-0.5em}
Here we show essential comparison and analysis. More details are shown in Appendix~\ref{sec:impl_detail} and \ref{sec:more_ablation}.

\textbf{Comparison on Places-LT. }
Generally, these methods can be roughly divided into two groups, \textit{i.e.}, vision-only pretrained methods and vision-language (VL) pretrained methods. Note that VL-based methods~\citep{tian2021vl,Long2022,ma2021simple} may introduce extra data (\textit{i.e.}, Wiki text data or external ImageNet-21k database) during training and testing. Our LPT belongs to the first group and does not rely on extra data.
Table~\ref{table:placeslt} lists the evaluation results of competing methods. Compared to other vision-only pretrained methods, with \textbf{only 1.01M} (1.1\%) additional trainable parameters, LPT achieves 50.1\% and 46.9\% in terms of overall accuracy and few-shot accuracy, and respectively surpasses the state-of-the-art PaCo~\citep{cui2021parametric} by 8.9\% and 11.6\%. Even compared with VL-LTR~\citep{tian2021vl} which is a VL-based method with extra data in training and testing, our LPT achieves the same overall accuracy while  obtaining higher few-shot accuracy.  



\textbf{Comparison on CIFAR100-LT. }
Then we evaluate LPT on CIFAR100-LT~\citep{Krizhevsky2009LearningML,cui2019cbloss} with imbalanced ratio . Evaluation results are given in Table~\ref{table:comp_cifar100lt}. Our LPT outperforms all the competing methods on four different imbalanced ratios. 
Especially, LPT surpasses the CLIP-pretrained BALLAD by 11.3\% in terms of accuracy with . These results indicate the effectiveness of LPT in common object-centric data with long-tailed distribution.


\textbf{Comparison on iNaturalist 2018. }
Finally, we explore LPT on large-scale and fine-graind iNaturalist 2018~\citep{van2018inaturalist}.
Quantitative results are given in Table~\ref{table:comp_inat18}. LPT achieves 76.1\% overall and 79.3\% few-shot accuracy and surpasses all other state-of-the-art methods with vision-only pretrained models. 
\begin{table}[h]
\centering
\begin{minipage}[t]{0.48\textwidth}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{ImageNet-Sketch evaluation results from different fine-tuning methods. 
}
    \vspace{-0.5em}
   \setlength{\tabcolsep}{10.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cc}
      \toprule
      \bf Method & \bf Backbone & \bf Overall \\
      \midrule
      Linear Probe & ViT-B & 31.55\%  \\
      Full fine-tune & ViT-B & 32.25\% \\
      LPT (Ours) & ViT-B & \textbf{36.22\%} \\
      \bottomrule
   \end{tabular}}}
   \label{table:imgnet_sketch}
\end{minipage}\hspace{4mm}
\begin{minipage}[t]{0.48\textwidth}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{Ablation study of LPT with different pretrained model sizes. 
}
    \vspace{-0.5em}
   \setlength{\tabcolsep}{10.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cc}
      \toprule
      \textbf{Backbone} & \textbf{Phase 1 Acc}  & \textbf{LPT Acc} \\
      \midrule
      ViT-T & 32.55\% & \textbf{37.40\%} \\
      ViT-S & 40.50\% & \textbf{44.66\%} \\
      ViT-B & 49.41\% & \textbf{50.07\%} \\
      \bottomrule
   \end{tabular}}}
   \label{table:ablation_scale}
\end{minipage}
\vspace{-0.5em}
\end{table}
Especially, LPT also surpasses fully fine-tuned ViT-L/16~\citep{touvron2022things} by 0.2\%. These results demonstrate that LPT can also handle large-scale long-tailed data with only prompt tuning and achieve comparable accuracy. 
Note that since VL-based methods~\citep{tian2021vl,Long2022} leverage both CLIP pretrained models and extra testing data, meanwhile the extra data benefits more to object-centric long-tailed scenarios, such that LPT still has small gap with these methods. How to eliminate this gap is worth to be further explored. 


\vspace{-0.5em}
\subsection{Robustness with Domain Shift}\label{sec:gene_domainshift}
To evaluate the robustness of our LPT with domain shift or out-of-distributed data, we optimize LPT on ImageNet-LT~\citep{deng2009imagenet,openlongtailrecognition} \textit{train} set, and then evaluate the fine-tuned LPT on ImageNet-Sketch~\citep{wang2019learning} \textit{val} set which includes multiple sketches from 1,000 classes and far different from natural images. Here we select linear probing and fully fine-tuning from the same pretrained ViT-B as baseline methods. Evaluation results are shown as Table~\ref{table:imgnet_sketch}. LPT achieves 36.22\% accuracy on ImageNet-Sketch dataset, surpassing linear probing as well as fully fine-tuning by 4.67\% and 3.97\%.
One possible explanation is that our LPT gather the domain-specific knowledge during training and adapt the pretrained model to the target domain of interest via prompt tuning, thus with given input images from other domains, LPT can transfer the feature into the domain learned in LPT, then reducing the effect from domain shifting in long-tailed learning.




\vspace{-0.5em}
\subsection{Ablation Study}\label{sec:ablation}
\vspace{-0.4em}



\textbf{Effect of Each Phase. }
First we analyze the effect of each training phase and component in LPT. Table~\ref{table:ablation} demonstrates the evaluation results of per-phase ablation study. Here we select linear probing~\citep{dosovitskiy2021an} and VPT~\citep{jia2022vpt} as baseline methods. 
After training with phase 1, type (a) and type (b) surpass their corresponding baselines by 8.04\% and 11.58\% in terms of overall accuracy. Meanwhile, compared to type (a), after introducing prompt for fine-tuning, type (b) surpasses type (a) by 7.77\% and 15.74\% in terms of overall accuracy and few-shot accuracy. These results indicate that: \textbf{1)} introducing prompt for fine-tuning benefits to the overall performance and tail classes accuracy in long-tailed learning; and \textbf{2)} the proposed phase 1 of LPT can fully release the representation ability of learnable prompt, thus leading to better classification results. Furthermore, when replacing cross entropy loss in type (b) by , type (c) achieves 49.41\% overall accuracy and obtains 4.07\% improvement in terms of few-shot accuracy. 
\begin{table}[t]
   \caption{Ablation study of each phase in LPT on Places-LT benchmark~\citep{zhou2017places}.}
    \vspace{-1em}
   \label{table:ablation}
   \begin{center}
      \setlength{\tabcolsep}{7.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cccc|cccc}
   \toprule
   {\bf Method}  & \bf Prompt & \textbf{Phase 1} &  & \textbf{Phase 2} & \bf Overall & \bf Many & {\bf Medium} & {\bf Few}\\ 
   \midrule
   Linear & - & - & - & - & 33.29\% & 46.48\% & 29.45\% & 18.77\% \\
   VPT & \checkmark & - & - & - & 37.52\% & \textbf{50.42\%} & 33.78\% & 23.29\% \\
   \midrule
   \text{(a)} & - & \checkmark & - & - & 41.33\% & 49.47\% & 41.31\% & 27.51\%  \\
   \text{(b)} & \checkmark & \checkmark & - & - & 49.10\% & \textbf{49.62\%} & 51.53\% & 43.25\% \\
   \text{(c)} & \checkmark & \checkmark & \checkmark & - & \textbf{49.41\%} & 46.89\% & \textbf{52.54\%} & \textbf{47.32\%}  \\
   \text{(d)} & \checkmark & \checkmark & \checkmark & \checkmark &\textbf{50.07\%}& 49.27\%& \textbf{52.31\%}& \textbf{46.88\%} \\
   \bottomrule
   \end{tabular}}}
   \vspace{-1.5em}
\end{center}
\end{table}
Finally, after introducing the group-specific prompts as well as phase 2 in LPT, type (d) achieves 50.07\% overall accuracy on Places-LT, which indicate that using different group prompts to handle different input samples can reduce the difficulty of long-tailed learning and further improve the classification performance. 


\textbf{Different Model Size and Pretrained Models. }
To verify the compatibility of LPT, we evaluate LPT with ViT-Tiny/Small/Base~\citep{dosovitskiy2021an}, and all ViTs are pretrained with ImageNet-21k~\citep{deng2009imagenet}. Quantitative results are shown as Table~\ref{table:ablation_scale}. LPT achieves 37.40\%, 44.66\% and 50.07\% accuracy with three pretrained models, which surpasses models with only shared prompt by 4.85\%, 4.16\% and 0.66\%, respectively. 
These results demonstrate the compatibility of LPT. Besides, for ViT-T and ViT-S which has less parameters than ViT-B, the combination of shared prompt and group-specific prompts can supply abundant domain-specific knowledge and group-specific features, thus these models benefit more from LPT and obtain larger accuracy improvement. 
\begin{table}[h]
\centering
\begin{minipage}[t]{0.48\textwidth}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{Ablation Study of Decoupled or Joint Training. Decoupled Training is better. }
   \vspace{-0.5em}
   \setlength{\tabcolsep}{12.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cc}
      \toprule
      \bf Method & \bf Epochs & \bf Overall \\
      \midrule
      Joint 1\&2 & 80 & 47.48\%  \\
      Decoupled 1\&2 & 40+40 & \textbf{50.07\%} \\
      \bottomrule
   \end{tabular}}}
   \label{table:ablation_decoupled}
\end{minipage}\hspace{4mm}
\begin{minipage}[t]{0.48\textwidth}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{Ablation study of query function in phase 2. Using phase 1 as query is better. }
   \vspace{-0.5em}
   \setlength{\tabcolsep}{10.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cc}
      \toprule
      \textbf{Query Func} & \textbf{K-NN Acc}  & \textbf{Phase 2 Acc} \\
      \midrule
      ViT-B & 32.11\% & 49.81\% \\
      Phase 1 & \textbf{36.16\%} & \textbf{50.07\%} \\
      \bottomrule
   \end{tabular}}}
   \label{table:ablation_query}
\end{minipage}
\vspace{-1em}
\end{table}
\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/prompt-stats.pdf}
\end{center}
\vspace{-0.7em}
\caption{Statistic results visualization of prompt matching proportion for classes in Places-LT. 
}
\vspace{-1em}
\label{fig:prompt_stats}
\end{figure}
Furthermore, we also keep using ViT-B and analyze the effect of LPT on various pretrained models~\citep{pmlr-v139-touvron21a,caron2021emerging,zhou2022mugs,dosovitskiy2021an}.
Evaluation results indicate that better self-supervised learning methods and more pretraining data lead to better accuracy, \textit{i.e.}, 50.07\% after LPT. More details are shown in Appendix~\ref{sec:pretrained_models}.

\textbf{Decoupled Training. }
To verify the effect of decoupled training, we conduct ablation study which jointly optimizes shared prompt and group-specific prompts. For fairness, we optimize the joint-trained model with 80 epochs. The results are shown as Table~\ref{table:ablation_decoupled}, LPT with decoupled training achieves 50.07\% overall accuracy and surpasses that with joint training by 2.59\%. 
These results indicate that, during joint training, the shared prompt is still updated simultaneously, thus the query function is sub-optimal during training, resulting in worse matching results. Nevertheless, decoupled training leverages a fixed yet optimal shared prompt as query function, thus obtaining better results.

\textbf{Query Function and Group Size . }
We further analyze query function and group size .
Since \citet{wang2022learning} used pretrained ViT as query function, to validate whether using our Phase 1 of LPT as query function is better or not, we conduct ablation study about query function. Specifically, we follow the design of LPT to use cosine similarity as distance metric and evaluate the K-NN accuracy of two query functions, then evaluate Phase 2 accuracy with different query functions, which is given in Table~\ref{table:ablation_query}. Phase 1 achieves 36.16\% K-NN accuracy and surpasses the pretrained ViT-B by 4.05\%, which indicates that queries from Phase 1 obtain higher quality for matching prompts. Meanwhile, compared to LPT with ViT-B query, LPT with phase 1 query achieves 50.07\% overall accuracy, which also demonstrates the effectiveness of using phase 1 as query function. Besides, we also vary the size of   from 5 to 40, and find that LPT is robust to  and achieves the best accuracy 50.07\% when . 
More details are presented in Appendix~\ref{sec:ablation_setsize}.


\textbf{Statistic of Prompt Matching. }
To verify that keys in group-specific prompts can adaptively learn to match samples from the same class, we count the matching results for samples in each class. And for better visualization, we randomly select two classes from many/medium/few-shot classes respectively, and then demonstrate the proportion of best-matched prompt as well as the second best-matched prompt, which is shown as Fig.~\ref{fig:prompt_stats}.
We notice that, for each class, samples matched by prompts with top-2 cosine similarity consists of the majority of proportion. This result is consistent with the adaptive prompt matching and prompt ensembling with  mentioned in Sec.~\ref{sec:method_phase2}, and demonstrate the effectiveness of group-specific prompts. 

\vspace{-0.5em}
\section{Conclusion}\label{sec:conclusion}
\vspace{-0.5em}
We proposed Long-tailed Prompt Tuning (LPT) to tackle long-tailed learning, 
which consists of \textbf{1)} shared prompt for all classes to learn general features or knowledge and to adapt a pretrained model into the target domain; and \textbf{2)} group-specific prompts to gather group-specific features for those samples which have similar features and also to empower the pretrained model with fine-grained discriminative ability.
For effective training, we propose a two-phase training framework, in which the first phase optimizes shared prompt to adapt the pretrained model to the target domain of interest, and the second phase optimizes group-specific prompts with dual-sampling strategy and asymmetric GCL loss to dig the common features of these similar samples.
Experimental results demonstrate the effectiveness and efficiency of our LPT with 1.1\% extra trainable parameters,
meanwhile illustrating its robustness against domain shift.


\vspace{-0.5em}
\section*{Acknowledgement}\label{sec:acknowledgement}
\vspace{-0.5em}
This work was supported  by the National Key R\&D Program of China under Grant No. 2021ZD0112100.




\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\clearpage
\appendix
The content of the Appendix is summarized as follows: 1) in Sec.~\ref{app:datasets}, we demonstrate the details of datasets we use in experiments of LPT; 2) in Sec.~\ref{sec:impl_detail}, we demonstrate the implementation and training detail of LPT; in Sec.~\ref{sec:more_ablation}, we illustrate more detailed ablation studies of LPT, including effect of shared and group prompts in \ref{sec:ablation_different_prompts_more}, pretrained models~\ref{sec:pretrained_models}, group sizes~\ref{sec:ablation_setsize}, effect of ~\ref{sec:ablatioin_k}, prompt ensembling~\ref{sec:ablation_t}, dual sampling strategy~\ref{sec:ablation_dualsample} and asymmetric GCL loss~\ref{sec:ablation_agcl}. Meanwhile, we also illustrate more comparison results, including comparison with state-of-the-art methods with same backbone in \ref{sec:fair_comp_sota}, comparison with different efficient tuning methods in \ref{sec:comp_efficient_tuning}, comparison with multi-task learning based method in \ref{sec:ablation_multitask} and more detailed robustness evaluation in \ref{sec:full_domain_shift}.
\section{Dataset Details}\label{app:datasets}
\paragraph{CIFAR100-LT} is a subset of the original CIFAR-100~\citep{Krizhevsky2009LearningML}, which includes 100 different categories and 10,000 test images. With given imbalanced ratio , where  is the class index of CIFAR-100, we follow the exponential down-sampling setting from~\citep{cao2019learning,cui2019cbloss} to generate corresponding long-tailed training data, and utilize CIFAR100-LT with  to evaluate our LPT.

\paragraph{Places-LT}~\citep{openlongtailrecognition} is the subset of Places-365 dataset~\cite{zhou2017places}, which includes 62500 training images from 365 individual scene categories with per-class samples from 5 to 4980. Following previous works~\citep{openlongtailrecognition,Kang2020Decoupling}, we optimize our LPT on \textit{train} set and then select the best model on \textit{val} set, finally report the test accuracy on \textit{test} set. 

\paragraph{iNaturalist 2018}~\citep{van2018inaturalist} is a large-scale fine-grained classification dataset, which exists extremely high imbalanced distribution with . This dataset includes 437K training images as well as 24.4K validation images. Following previous works~\citep{Kang2020Decoupling,cui2019cbloss}, we use \textit{train} set to optimize our LPT and evaluate LPT on \textit{val} set. 

\paragraph{ImageNet-Sketch}~\citep{wang2022learning} includes 50,000 individual sketch images from the same 1,000 ImageNet categories. Since the backbone of our LPT is pretrained on ImageNet~\citep{deng2009imagenet}, different from previous methods, we only use ImageNet-LT~\citep{openlongtailrecognition} \textit{train} set to fine-tune the prompt and classifier. Instead, we evaluate LPT on ImageNet-Sketch \textit{val} set to verify the robustness of our LPT with domain shift scenarios. 

\subsection{Evaluation Protocol}
Following~\citet{cui2021parametric,tian2021vl,Long2022}, for both Places-LT~\citep{zhou2017places} and iNaturalist 2018~\cite{van2018inaturalist}, we first divide the whole dataset by the number of training images in each class, \textit{i.e.}, many-shot (100 images), medium-shot (20100 images) and few-shot (20 images);
then we report the overall accuracy as well as many/medium/few-shot accuracy, respectively. 
Meanwhile, for CIFAR100-LT dataset, following~\citet{cui2021parametric,Li2022Long}, we directly report the overall accuracy with .
And for ImageNet-Sketch dataset, we also directly report the overall accuracy for simplicity. 

\section{Implementation Details}\label{sec:impl_detail}
Following~\citet{jia2022vpt,wang2022learning}, we use ViT-B/16~\citep{dosovitskiy2021an} with ImageNet-21k pretrained model as the backbone of LPT.
For shared prompt, we simply set the default length of prompt as 10 and adopt prompts on all transformer blocks in the ViT. 
And for group-specific prompts, we set shared layer number  and the size of prompt size ; for each prompt in the set, the prompt length is also set as 10. 
Note that setting  may lead to 1.5x inference cost (\ie, use ViT with shared prompt to generate output class token as query, then reuse features from the 6-th block and inference with both shared prompt and group prompt for the last 6 blocks) compared to VPT~\citep{jia2022vpt}, but can achieve better accuracy, and is more efficient than inference twice (\ie, inference once with pretrained ViT-B to generate the output class token as query, and then inference the second time with prompt to calculate the final confidence scores), \eg, \citet{wang2022learning}.
During training and testing, we set the prompt ensemble number . 
Finally, the number of additional parameters (linear classifiers are omitted for simplicity) is 1.01M.  
Both prompts are initialized with truncated normal distribution. 
During training in both phase one and phase two, we use SGD optimizer with momentum of 0.9 to optimize the shared prompt and group-specific prompts, respectively. 
During training, the initial learning rate is set to be , where  indicates batch size. Different from~\citet{Alshammari_2022_CVPR}, we set the weight decay as 1e-4 since large weight decay does not lead to significant performance improvement in LPT. During training, we use cosine learning rate scheduler to control the learning rate with 5 warmup epochs. 
For Places-LT, CIFAR100-LT and ImageNet-LT, we optimize phase 1 and phase 2 of LPT for  epochs, respectively; and for iNaturalist 2018, since this dataset includes much more training data, we set epoch number  for phase 1 and phase 2, respectively. 
For asymmetric GCL loss, we set  and  as 0 and 4, respectively. 
And for phase 2, we set the initialized weight  used in  as 0.5. 
In all experiments, the training and testing images are resized to 224224.
During both two training phases of LPT, we only introduce random crop and resize operation and Mixup technique as data augmentation; and during evaluation, we 
All programs are implemented by PyTorch toolkit~\citep{NEURIPS2019_9015}, and all experiments are
conducted on a single RTX A6000 GPU.

\begin{table}[h]
   \centering
   \begin{minipage}[t]{0.48\textwidth}
      \renewcommand\arraystretch{1.3}
\centering
      \caption{Ablation Study of different pretrained models. Both more pretraining data and better pretraining algorithms benefit to LPT.}
\setlength{\tabcolsep}{9.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
      \begin{tabular}{l|c|cc}
         \toprule
         \bf Method & \bf Data & \bf Phase 1 & \bf LPT \\
         \midrule
         DINO & IN1k &40.65\% &42.18\% \\
         Mugs & IN1k &41.92\% &43.66\% \\
         Supervised & IN1k & 41.33\% & 42.71\% \\
         Supervised & IN21k & \textbf{49.41\%} & \textbf{50.07\%} \\
         \bottomrule
      \end{tabular}}}
      \label{table:ablation_pretrain}
   \end{minipage}\hspace{4mm}
   \begin{minipage}[t]{0.48\textwidth}
      \renewcommand\arraystretch{1.3}
\centering
      \caption{Ablation Study of the size of group-specific prompts. Prompt set with 20 individual group prompts performs better. }
\setlength{\tabcolsep}{8.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
      \begin{tabular}{l|c|c}
         \toprule
         \textbf{Group Size} & \textbf{Parameters}  & \textbf{Phase 2 Acc} \\
         \midrule
         5 & 0.23M & 49.81\% \\
         10 & 0.46M & 49.90\% \\
         20 & 0.92M & \textbf{50.07\%} \\
         40 & 1.84M & 49.87\% \\
         \bottomrule
      \end{tabular}}}
      \label{table:ablation_setsize}
   \end{minipage}
\end{table}

\section{More Ablation Study and Discussion}\label{sec:more_ablation}

\begin{figure}[tp]
   \centering
   \begin{minipage}{0.46\textwidth}
\centering
            
      \includegraphics[width=\textwidth]{figures/pre-domain-shift-lpt.pdf}
      \vspace{-2em}
      \caption{LDA visualization of LPT.\vspace{-0.6em} } 
      \label{fig:ablation_domain_shift}
   \end{minipage}
\hspace{-1mm}
   \begin{minipage}{0.52\textwidth}
      \renewcommand\arraystretch{1.3}
\centering
      \captionsetup{type=table}
      \caption{Quantitative analysis of features learned by Phase 1 of LPT and Phase 2 of LPT. Features from phase 2 of LPT obtains the similar  value while obtaining larger inter-class distance. }
\setlength{\tabcolsep}{4.6pt} \renewcommand{\arraystretch}{4.5}{ \fontsize{8.3}{3}\selectfont{
      \begin{tabular}{l|cc}
         \toprule
         \textbf{Method} & {LPT Phase 1}  & {LPT Phase 2} \\
         \midrule
\bf Inter-class distance & 15.8 & \textbf{16.1} \\
         \bf Inner-class / inter-class  & \textbf{0.237} & {0.242} \\
\bottomrule
      \end{tabular}}}
      \label{table:ablation_discriminative}
   \end{minipage}
\end{figure}

\subsection{The Effect of Shared Prompt and Group Prompts}\label{sec:ablation_different_prompts_more}
To further investigating the effect of shared prompt and group prompts, in addition to the ablation study in Sec.~\ref{sec:ablation}, we also conduct more quantitative and qualitative analysis on Places-LT dataset. As discussed in Sec.~\ref{sec:method}, shared prompt aims to learn the domain-specific knowledge from training data while group prompts aim to gather group-specific features from training data in corresponding groups. To verify this point, we also adopt LDA among features extracted from LPT (Phase 1) and LPT (Phase 2), as well as features from pretrained ViT without any prompts. The visualization results are shown in Fig.~\ref{fig:ablation_domain_shift}. According to the visualization, we find that the distribution of feature from LPT (Phase 1) and LPT (Phase 2) are highly overlapped, this observation proves that shared prompts in phase 1 obtains the domain-specific knowledge. Moreover, we also investigate the  value and inter-class distance of features from LPT (Phase 1) and LPT (Phase 2), which are shown in Table~\ref{table:ablation_discriminative}. We find that, both of them have the similar , while LPT (Phase 2) has larger inter-class distance, which indicates that group prompts introduce more class discriminative ability.

\begin{table}
\centering
   \caption{Fair comparison with state-of-the-art methods on Places-LT dataset. All methods start from the same IN21K pretrained ViT-B feature extractor to conduct fully fine-tuning or prompt tuning. Quantitative results show that LPT still largely surpasses previous methods by 7.25\%.}
\setlength{\tabcolsep}{13.6pt} \renewcommand{\arraystretch}{3.5}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|c}
      \toprule
      \textbf{Method} & Backbone & Pretrained Data & Overall Acc \\
      \midrule
      Kang \textit{et al.}~\citep{Kang2020Decoupling} & ViT-B & IN21K & 40.45\% \\
      PaCo~\citep{cui2021parametric} & ViT-B & IN21K & 37.00\% \\
      GCL~\citep{Li2022Long} & ViT-B & IN21K & 42.82\% \\
      LPT & ViT-B & IN21K & 50.07\% \\
      \bottomrule
   \end{tabular}}}
   \label{table:comp_sota_same_vitb}
\end{table}

\subsection{Fair Comparison with State-of-The-Art Methods}\label{sec:fair_comp_sota}
Since previous methods mainly leverage IN1k pretrained ResNet to conduct experiments on long-tailed datasets, 
to fairly compare LPT with previous methods via using the same backbone, we reimplement three state-of-the-art long-tailed methods~\citep{Kang2020Decoupling,cui2021parametric,Li2022Long} with the same IN21K pretrained ViT-B as feature extractor to conduct fully fine-tuning on Places-LT. Experimental results are shown in Table~\ref{table:comp_sota_same_vitb}. The state-of-the-art GCL~\citep{Li2022Long} with IN21K pretrained ViT-B backbone achieves 42.82\% overall accuracy on Places-LT, while still lower than LPT by 7.25\%. These results also indicate that LPT surpasses previous state-of-the-art methods under the fair comparison settings.

\begin{table}
\centering
   \caption{Fair comparison with different efficient tuning methods on Places-LT dataset. All methods start from the same IN21K pretrained ViT-B feature extractor. Quantitative results show that LPT achieves the best accuracy.}
\setlength{\tabcolsep}{13.6pt} \renewcommand{\arraystretch}{3.5}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|c}
      \toprule
      \textbf{Method} & Backbone & Pretrained Data & Overall Acc \\
      \midrule
      VPT~\citep{jia2022vpt} & ViT-B & IN21K & 44.2\% \\
      Visual Prompting~\citep{bahng2022exploring} & ViT-B & IN21K & 13.8\% \\
      Pro-Tuning~\citep{nie2022protuning} & ViT-B & IN21K & 48.3\% \\
      \hline
      LPT & ViT-B & IN21K & \textbf{50.1\%} \\
      LPT+FT-Norm~\citep{frankle2021training} & ViT-B & IN21K & 49.9\% \\
      LPT+Adapter~\citep{nie2022protuning} & ViT-B & IN21K & 49.9\% \\
      \bottomrule
   \end{tabular}}}
   \label{table:comp_efficient_tuning_same_vitb}
\end{table}

\subsection{Comparison with Different Efficient Tuning Methods}\label{sec:comp_efficient_tuning}
Without loss of generality, we conduct the comparison between LPT and more efficient tuning methods on Places-LT dataset, including different prompt tuning methods (Visual Prompting~\citep{bahng2022exploring} and VPT~\citep{jia2022vpt}) and adapter tuning methods (e.g., Pro-Tuning~\citep{nie2022protuning}). Furthermore, we also additionally adopt two efficient tuning methods, i.e., normalization tuning and adapter tuning into our LPT (i.e., LPT+FT-Norm~\citep{frankle2021training} and LPT+Adapter~\citep{nie2022protuning}). 
Note that for previous efficient tuning methods, we adopt balanced sampling and use grid search to find the best training parameters to optimize the models. 
Without loss of generality, we keep using the same IN21K pretrained ViT as backbone for all methods and evaluate all methods on Places-LT dataset. 
The corresponding evaluation results are shown in Table~\ref{table:comp_efficient_tuning_same_vitb}. Compared to other efficient tuning methods, our LPT surpasses the state-of-the-art VPT and Pro-Tuning by 1.8\% and 5.9\%, respectively. These results demonstrate the effectiveness of LPT in handling long-tailed classification. And note that \citet{bahng2022exploring} only inserts learnable prompt on the edges of input images, and fixes all the other pretrained parameters (including the linear classifier), which may explain its relatively lower performance. And meanwhile, after adopting other efficient tuning methods into LPT, the final performance has no significant improvement. Thus we choose the current LPT structure as the final framework.

\begin{table}
\centering
   \caption{Full comparison with different fine-tuning methods on six different OOD dataset. All methods start from the same IN21K pretrained ViT-B feature extractor. Quantitative results show that LPT achieves the best accuracy.}
\setlength{\tabcolsep}{5pt} \renewcommand{\arraystretch}{3.5}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|c|c|c|c}
      \toprule
      \textbf{Method} & ImageNet-Sketch & ImageNet-ReaL & ImageNet-V2 & ImageNet-A & ImageNet-R & ObjectNet \\
      \midrule
      Linear Probe & 31.55\% & 81.43\% & 63.54\% & 29.20\% & 45.72\% & 6.61\% \\
      Fully Fine-tune & 32.25\% & 80.10\% & 62.31\% & 30.12\% & 43.14\% & 7.13\% \\
      VPT & 34.64\% & 85.82\% & 68.51\% & 35.17\% & 47.06\% & 8.03\% \\
      WISE-FT & 34.79\% & 82.20\% & 65.76\% & 36.75\% & 47.32\% & 8.00\% \\
      \hline
      LPT & \textbf{36.22\%} & \textbf{87.22\%} & \textbf{70.71\%} & \textbf{39.65\%} & \textbf{50.47\%} & \textbf{8.22\%} \\
      \bottomrule
   \end{tabular}}}
   \label{table:comp_robustness_full}
\end{table}

\subsection{More Detailed Evaluation for Robustness with Domain Shift}\label{sec:full_domain_shift}
To further compare our LPT with the baseline methods (linear probe, fully fine-tune, VPT~\citep{jia2022vpt}, WISE-FT~\citep{wortsman2022robust}), we evaluate all these models on six different OOD datasets (ImageNet-Sketch~\citep{wang2019learning}, ImageNet-ReaL~\citep{imagenetreal}, ImageNet-V2~\citep{imagenetv2}, ImageNet-A~\citep{imageneta}, ImageNet-R~\citep{imagenetr}, ObjectNet~\citep{objectnet}). For fairness, all these methods start from the same IN21K pretrained ViT-B and fine-tune on the same ImageNet-LT training set. The evaluation results are shown in Table~\ref{table:comp_robustness_full}. Our LPT surpasses all baselines on six OOD datasets. Note that \citet{herrmann2022pyramid} mentioned that the pretrained ViT~\citep{dosovitskiy2021an} we used in this experiment performs bad on ObjectNet benchmark (i.e., 17.36\% accuracy after ImageNet-1k training), all corresponding results on ObjectNet in the table are relatively low because the training is performed on the subset of ImageNet-1k (i.e., ImageNet-LT). 

\subsection{Pretrained Models}\label{sec:pretrained_models}
We also evaluate our LPT on various pretrained models from different pretraining algorithms and different pretraining data scale. Specifically, we keep using ViT-B/16 structure and select DeiT~\citep{pmlr-v139-touvron21a}, DINO~\citep{caron2021emerging} and Mugs~\citep{zhou2022mugs} pretrained on ImageNet-1k~\citep{deng2009imagenet}, meanwhile using ViT-B~\citep{dosovitskiy2021an} pretrained on ImageNet-21k as baseline model. Evaluation results are shown as Table~\ref{table:ablation_pretrain}. With the same ImageNet-1k pretraining data, LPT with Mugs achieves 41.92\% accuracy after phase 1 and 43.66\% accuracy after phase 2, which surpasses other two pretrained models. Meanwhile, VPT with ImageNet-21k pretrained model achieves 50.07\% accuracy and largely surpasses LPT with DeiT-B.
These results indicate that pretrained models from better algorithms or larger pretraining data lead to better efficient tuning results with long-tailed target data.

\subsection{Size of Group-Specific Prompts}\label{sec:ablation_setsize}
We also conduct ablation study about the size of group-specific prompts. Generally, we start each phase 2 training from the same phase 1 model, and only change the group size of the group-specific prompts. 
The corresponding evaluation results are shown as Table~\ref{table:ablation_setsize}. When the size of group-specific prompts increasing to 20, the accuracy of LPT increases from 49.81\% to 50.07\%. However, when we further increase the size to 40, the final accuracy declines to 49.87\%. 
A possible reason is that, some classes in the dataset may share some similar group-specific feature or knowledge, such that features from instances in corresponding classes may be similar. Thus instances from these classes can be seen as a cluster and can be matched into the same prompt. Since the number of different attributes is limited, we only need a fixed number (\textit{e.g.}, 20) of group prompts to handle the whole dataset and achieve better accuracy. Besides, using too many group prompts may increase the difficulty of clustering and optimization, which affects the final accuracy. 

\begin{table}
\centering
   \caption{Ablation Study of the number of blocks with only shared blocks, \textit{i.e.}, . Quantitative results show that  performs better for phase 2 of LPT. }
\setlength{\tabcolsep}{13.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|c}
      \toprule
      \textbf{} & 8  & 6 & 4 \\
      \midrule
      \textbf{Param Num} & 0.61M & 0.92M & 1.23M \\
      \textbf{Overall Acc} & 49.77\% & \textbf{50.07\%} & 49.92\% \\
      \bottomrule
   \end{tabular}}}
   \label{table:ablation_k}
\end{table}

\begin{table}
\centering
   \caption{Ablation Study of ensemble prompt number . Quantitative results show that  and  achieve better results, thus we select  in LPT. }
\setlength{\tabcolsep}{13.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|c|c}
      \toprule
      \textbf{Ensemble Num } & 1  & 2 & 3 & 4 \\
      \midrule
      \textbf{Overall Acc} & 49.93\% & \textbf{50.07\%} & 50.00\% & 49.93\% \\
      \textbf{Few-shot Acc} & 46.32\% & \textbf{46.88\%} & 46.87\% & 46.84\% \\
      \bottomrule
   \end{tabular}}}
   \label{table:ablation_t}
\end{table}

\begin{table}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{
   Ablation Study of dual sampling in phase 2. Adding dual sampling strategy with proper small  (\textit{e.g.}, 0.5).
   }
\setlength{\tabcolsep}{13.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cc|cc}
      \toprule
      \textbf{Method} & \textbf{Dual} & \textbf{} & \textbf{Overall} & \textbf{Many-shot} \\
      \midrule
      (a) balanced sampling & - & - & 49.90\% & 47.67\% \\
      (b) dual w/ small  & \checkmark & 0.5 & \textbf{50.07\%} & 49.27\% \\
      (c) dual w/ large  & \checkmark & 1.0 & 49.62\% & \textbf{50.06\%} \\
      \bottomrule
   \end{tabular}}}
   \label{table:ablation_dual_sampling}
\end{table}

\begin{table}
   \renewcommand\arraystretch{1.3}
\centering
   \caption{Ablation Study of asymmetric GCL Loss. Introducing gradient re-weighting into GCL Loss can further improve overall accuracy in long-tailed classification. }
\setlength{\tabcolsep}{13.6pt} \renewcommand{\arraystretch}{2.2}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|cccc}
      \toprule
      \textbf{Loss Function} & \textbf{Overall}  & \textbf{Many} & \textbf{Medium} & \textbf{Few} \\
      \midrule
      GCL~\cite{Li2022Long} & 49.58\% & 48.19\% & 52.62\% & 45.75\% \\
       & 50.07\% & 49.27\% & 52.31\% & 46.88\% \\
      \bottomrule
   \end{tabular}}}
   \label{table:asym_gcl}
\end{table}

\subsection{Effect of K}\label{sec:ablatioin_k}
Next we further analyze the effect of , which stands for the number of blocks with only the shared prompt . Intuitively, the less  means more parameters are inserted into more transformer blocks to conduct group prompts tuning and should lead to better performance. To verify this hypothesis, we set  and conduct corresponding ablation study. Evaluation results are shown in Table~\ref{table:ablation_k}. LPT with  achieves the best performance and surpasses LPT with  by 0.3\% in terms of overall accuracy. Meanwhile, LPT with  achieves similar performance with  counterpart, but does not lead to significant performance improvement with more parameters. The possible reason is that: \textbf{1)} too large  could restrict the group-specific knowledge gathering ability of group-specific prompts, since only a few layers are utilized to extract group-specific features from long-tailed data; and \textbf{2)} compared to , LPT with  fully leverages the adapted feature representation from phase 1, thus reducing the difficulty of optimization and achieving better accuracy. Therefore, we choose  for final LPT during experiments.

\subsection{Effect of Ensemble Number \textit{k}}\label{sec:ablation_t}
We also analyze the effect of ensemble token number  in phase 2. Intuitively, introducing can lead to better accuracy for tail classes since more class-specific knowledge are utilized for recognition. Therefore we set  and conduct corresponding ablation study. Evaluation results are shown as Table~\ref{table:ablation_t}. Based on the results, we find that: \textbf{1)} the overall accuracy are robust with different  values, and \textbf{2)} introducing prompt ensembling benefits to tail classes (+0.56\% in terms of few-shot accuracy), meanwhile using top-2 best matched prompts for ensembling achieves the best results. Therefore, we choose  during training and testing.  

\subsection{Effect of Dual Sampling in Phase 2}\label{sec:ablation_dualsample}
To evaluate the effect of dual sampling in phase 2, we conduct a series of experiments, which is shown as Table~\ref{table:ablation_dual_sampling}. Compared to type (a), type (b) achieves 0.2\% accuracy improvement in terms of overall accuracy, meanwhile surpasses 1.6\% improvement in terms of many-shot accuracy.
These results indicate that: introducing dual sampling strategy with proper  can lead to better overall performance and reduce the overfitting from balanced sampling only, but dual sampling with too large weight may lead to negative effect on overall accuracy. 

\subsection{Effect of Asymmetric GCL Loss}\label{sec:ablation_agcl}
To evaluate the effect of adding asymmetric gradient re-weighting design into GCL loss, we conduct ablation study between  and standard GCL loss~\cite{Li2022Long}. Without loss of generality, we conduct both experiments on phase 2. The quantitative results are shown as Table~\ref{table:asym_gcl}, LPT with  surpasses the counterpart with GCL loss by 0.49\% and 1.03\% in terms of overall accuracy and few-shot accuracy. These results further demonstrate the effect of .

\begin{table}
\centering
   \caption{Fair comparison with multi-task learning methods on Places-LT dataset. All methods start from the same IN21K pretrained ViT-B feature extractor to conduct fully fine-tuning or prompt tuning. Quantitative results show that LPT still surpasses multi-task learning method by a large margin.}
\setlength{\tabcolsep}{5.6pt} \renewcommand{\arraystretch}{3.5}{ \fontsize{8.3}{3}\selectfont{
   \begin{tabular}{l|c|c|c|c}
      \toprule
      \textbf{Method} & Backbone & Pretrained Data & Places-LT Acc & CIFAR100-LT (IF=200) Acc \\
      \midrule
      Multi-task & ViT-B & IN21K & 40.45\% & 73.1\% \\
      LPT (Places-LT) & ViT-B & IN21K & 50.1\% & - \\
      LPT (CIFAR100-LT, IF=200) & ViT-B & IN21K & - & 87.9\% \\
      \bottomrule
   \end{tabular}}}
   \label{table:comp_multitask_same_vitb}
\end{table}

\subsection{Comparison with Multi-task Learning Method}\label{sec:ablation_multitask}
Without loss of generality, we compare our LPT with a multi-task training based method from IN21K pretrained ViT-B to optimize both Places-LT and CIFAR100-LT with imbalanced factor of 200 (IF=200). Generally, in the multi-task training method, for each task (corresponding to a specific dataset), we initialize a linear classifier, and then optimizing all linear classifiers and the pretrained backbone by end-to-end fine-tuning. The corresponding results are shown in Table~\ref{table:comp_multitask_same_vitb}. LPT surpasses multi-task training method by 10.6\% on Places-LT and 14.8\% on CIFAR100-LT (IF=200). Benefiting from the two merits mentioned in the introduction, LPT can achieve high performance meanwhile easy to deployment with different scenarios.

\subsection{Broader Impact}
LPT is based on previous large-scale pretrained models, and fine-tunes only as few extra trainable parameters to adapt to real-world long-tailed scenarios. 
Compared to previou methods, LPT is more efficient for saving training cost and storing additional parameters, which is economic for real-world application.
However, LPT is still fully data-driven, and should be cautious with potential negative impact
from biased data.

\subsection{Limitation}
The performance of head classes from LPT is still lower than that from the VL-based state-of-the-art method~\citep{tian2021vl}. A possible solution is proposing a novel head-tail separation algorithm~\citep{xu2022constructing} to further reduce the difficulty of prompt tuning with divide-and-conquer strategy, thus improving the accuracy for both head and tail classes.
This part leaves for further exploration in the future. 

\section{More Statistic of Prompt Matching}
To verify that keys in group-specific prompts can adaptively learn to match samples from the same class, we count the matching results for samples in each class. And for better visualization, we provide more results from many/medium/few-shot classes, and then demonstrate the proportion of best-matched prompt as well as the second best-matched prompt, which is shown as Fig.~\ref{fig:prompt_stats_more}.
We notice that, for each class, samples matched by prompts with top-2 cosine similarity consists of the majority of proportion. This result is consistent with the adaptive prompt matching and prompt ensembling with  mentioned in Sec.~\ref{sec:method_phase2}, and demonstrate the effectiveness of group-specific prompts. 

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{figures/prompt-stats-more.pdf}
\end{center}
\caption{More statistic results visualization of prompt matching proportion for classes in Places-LT. 
}
\label{fig:prompt_stats_more}
\end{figure}


\end{document}
