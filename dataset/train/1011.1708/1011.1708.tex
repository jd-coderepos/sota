\documentclass{llncs}

\usepackage{epsfig}







\newcommand{\mind}{{m_d}}
\newcommand{\maxd}{{M_d}}
\newcommand{\countd}{{n_d}}
\newcommand{\minh}{{m_b}}
\newcommand{\maxh}{{M_b}}
\newcommand{\counth}{{n_b}}
\newcommand{\In}{{In}}
\newcommand{\Ex}{{Ex}}
\newcommand{\XY}{{X \cup Y}}
\newcommand{\DBlabel}{{\textit{DB}-label\ }}
\newcommand{\DBlabeling}{{\textit{DB}-labeling\ }}

\newcommand{\U}{{\mathcal{U}}}
\newcommand{\T}{{\mathcal{T}}}
\newcommand{\tri}{{tri}}
\newcommand{\comment}[1]{}
\newcommand{\Order}{\mathcal{O}}
\newcommand{\order}{o}



\title{CRAM: Compressed Random Access Memory}

\author{
  Jesper Jansson\inst{1}
\and
  Kunihiko Sadakane\inst{2}
\and
  Wing-Kin Sung\inst{3}
}

\institute{
  Ochanomizu University, 2-1-1 Otsuka, Bunkyo-ku, Tokyo 112-8610, Japan. \\
  ~E-mail: \texttt{jesper.jansson@ocha.ac.jp}
\and
  National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku,
  Tokyo 101-8430, Japan.
  ~E-mail: \texttt{sada@nii.ac.jp}
\and
  National University of Singapore, 13~Computing Drive, Singapore~117417. \\
  ~E-mail: \texttt{ksung@comp.nus.edu.sg}
}

\date{}



\begin{document}

\maketitle

\begin{abstract}
We present a new data structure called the
\emph{Compressed Random Access Memory}~(CRAM)
that can store a dynamic string~ of characters, e.g., representing the
memory of a computer, in compressed form while achieving asymptotically
almost-optimal bounds (in terms of empirical entropy) on the compression
ratio.
It allows short substrings of~ to be decompressed and retrieved efficiently
and, significantly, characters at arbitrary positions of~ to be modified
quickly during execution \emph{without decompressing the entire string}.
This can be regarded as a new type of data compression that can update
a compressed file directly.
Moreover, at the cost of slightly increasing the time spent per operation,
the CRAM
can be extended to also support insertions and deletions.
Our key observation that the empirical entropy of a string does not change
much after a small change to the string, as well as our simple yet efficient
method for maintaining an array of variable-length blocks under length
modifications, may be useful for many other applications as well.
\end{abstract}




\section{Introduction}

Certain modern-day information technology-based applications require
random access to very large data structures.
For example, to do genome assembly in bioinformatics, one needs to maintain
a huge graph~\cite{SimpsonWongJackmanScheinJonesBirol2009}.
Other examples include dynamic programming-based problems, such as optimal
sequence alignment or finding maximum bipartite matchings, which need to
create large tables (often containing a lot of redundancy).
Yet another example is in image processing, where one sometimes needs to
edit a high-resolution image which is too big to load into the main memory
of a computer all at once.
Additionally, a current trend in the mass consumer electronics market is
cheap mobile devices with limited processing power and relatively small
memories; although these are not designed to process massive amounts of data,
it could be economical to store non-permanent data and software on them more
compactly,~if~possible.

The standard solution to the above problem is to employ secondary memory
(disk storage, etc.) as an extension of the main memory of a computer.
This technique is called \emph{virtual memory}.
The drawback of virtual memory is that the processing time will be slowed
down since accessing the secondary memory is an order of magnitude slower
than accessing the main memory.
An alternative approach is to compress the data~ and store it in the main
memory.
By using existing data compression methods, ~can be stored in
-bits space~\cite{FerMan05,GGV03}
for every ,
where  is the length of~,  is the size of the alphabet,
and  denotes the -th order empirical entropy of~.
Although greatly reducing the amount of storage needed, it does not work well
because it becomes computationally expensive to access and update~.

Motivated by applications that would benefit from having a large virtual
memory that supports fast access- and update-operations, we consider
the following task:
Given a memory/text  over an alphabet of size , maintain
a data structure that stores  compactly while supporting the following
operations.
(We assume that  is the length of one machine
word.)
\begin{itemize}
\vspace*{-1mm}
\item[{\raise0.3pt\hbox{}}]
  \textnormal{\texttt{access}}:
  Return the substring .
\item[{\raise0.3pt\hbox{}}]
  \textnormal{\texttt{replace}}:
  Replace  by a character .
  \footnote{The notation  stands for the
  set .}
\item[{\raise0.3pt\hbox{}}]
  \textnormal{\texttt{delete}}:
  Delete ,
  i.e., make  one character shorter.
\item[{\raise0.3pt\hbox{}}]
  \textnormal{\texttt{insert}}:
  Insert a character  into  between
positions  and ,
  i.e., make  one character longer.
\end{itemize}

\noindent
\textbf{Compressed Read Only Memory:}
When only the \texttt{access} operation is supported,
we call the data structure \emph{Compressed Read Only Memory}.
Sadakane and Grossi \cite{SG_SODA2006},
Gonz\'alez and Navarro \cite{GonzalezN06}, and
Ferragina and Venturini \cite{FerVen07b}
developed storage schemes for storing a text succinctly that
allow constant-time access to any word in the text.
More precisely, these schemes store  in
 bits\footnote{Reference~\cite{SG_SODA2006} has
a slightly worse space complexity.}
and \texttt{access} takes  time,
and both the space and access time are optimal for this task.
Note, however, that none of these schemes allow  to be modified.

\medskip

\noindent
\textbf{Compressed Random Access Memory (CRAM):}
When the operations \texttt{access} and \texttt{replace} are supported,
we call the data structure \emph{Compressed Random Access Memory} (CRAM).
As far as we know, it has not been considered previously in the literature,
even though it appears to be a fundamental and important data structure.

\medskip

\noindent
\textbf{Extended CRAM:}
When all four operations are supported,
we call the data structure \emph{extended CRAM}.
It is equivalent to \emph{the dynamic array}~\cite{RRR_WADS2001}
and also solves
\emph{the list representation problem}~\cite{FredmanS_STOC1989}.
Fredman and Saks \cite{FredmanS_STOC1989} proved
a cell probe lower bound of  time 
for the latter, and also showed that  update time is needed
to support constant-time \texttt{access}.
Raman \emph{et al.}~\cite{RRR_WADS2001}
presented an -bit data structure
which supports \texttt{access}, \texttt{replace}, \texttt{delete}, and
\texttt{insert} in  time.
Navarro and Sadakane \cite{NavSad10} recently gave a data structure using

bits that supports \texttt{access}, \texttt{delete}, and \texttt{insert}
in  time.


\subsection{Our contributions}

This paper studies the complexity of maintaining the CRAM and
extended CRAM data structures.
We assume the uniform-cost word RAM model with~word size
 bits,
i.e., 
standard arithmetic and bitwise boolean operations
on -bit word-sized operands can be performed in constant time~\cite{Hag98}.
Also, we assume that the memory consists of a sequence of
bits, and each bit is identified with an address in .
Furthermore, any consecutive  bits can be accessed in constant time.
(Note that this memory model is equivalent under the word RAM model to
a standard memory model consisting of a sequence of words of some fixed
length.)
At any time, if the highest address of the memory used by the algorithm
is~, the space used by the algorithm is said to be
~bits~\cite{HagerupR02}.

\smallskip

Our main results for the CRAM are summarized in:
\begin{theorem}\label{th:main1}
Given a text  over an alphabet of size~
and any fixed ,
after  time preprocessing,
the CRAM data structure for~ can be stored in
 bits
for every  simultaneously,
where  denotes the -th order empirical entropy of~,
while supporting \textnormal{\texttt{access}} in  time
and \textnormal{\texttt{replace}} for any character~
in  time.
\end{theorem}

Theorem~\ref{th:main1} is proved in Section~\ref{sec:replace} below.

Next, by setting
,
we obtain:
\begin{corollary}
Given a text  over an alphabet of size~
and any fixed ,
after  time preprocessing,
the CRAM data structure for~ can be stored in
 bits
while supporting \textnormal{\texttt{access}} in  time and
\textnormal{\texttt{replace}} for any character~
in  time.
\end{corollary}


\smallskip

For the extended CRAM, we have:
\begin{theorem}\label{th:main2}
Given a text  over an alphabet of size~,
after  time preprocessing,
the extended CRAM data structure for~ can be stored in
 bits
for every  simultaneously,
where  denotes the -th order empirical entropy of~,
while supporting all four operations in  time.
\end{theorem}

Due to lack of space, the proof of
Theorem~\ref{th:main2} is given in Appendix~\ref{sec:indel}.


\medskip

Table~\ref{tbl:comparison} shows a comparison with existing data structures.
Many existing dynamic data structures for storing compressed
strings~\cite{GNtcs09,HeMun10,MNtalg08,NavSad10}
use the fact  where 
is the number of occurrences of character  in the string .
However, this approach is helpful for small alphabets only because of the size
of the auxiliary data.
For large alphabets, generalized wavelet trees~\cite{FMMNtalg07} can be used
to decompose a large alphabet into smaller ones, but this slows down the
access and update times.
For example, if , the time complexity of those data
structures is , 
while ours is , or even constant.
Also, a technical issue when using large alphabets is how to update the code
tables for encoding characters to achieve the entropy bound.
Code tables that achieve the entropy bound will change when the string
changes, and updating the entire data structure with the new code table is
too time-consuming.

\begin{table}[tb]
  \small
  \caption{Comparison of existing data structures and the new ones from this
  paper.
  For simplicity, we assume .
  The upper table lists results for the Compressed Read Only Memory (the
  first line) and the CRAM (the second and third lines), and the lower table
  lists results for the extended CRAM.}
  \label{tbl:comparison}

\smallskip
\footnotesize

  \begin{tabular}{ccll}
  \texttt{access} & \texttt{replace} & Space (bits) & Ref. \\ \hline
   & --- &  & \cite{FerVen07b,GonzalezN06} \\
   &  &  & New \\
   &  &  & New \\ \hline
  \end{tabular}
\bigskip

  \begin{tabular}{cll}
  \texttt{access}/\texttt{replace}/\texttt{insert}/\texttt{delete}\,\,\, & Space (bits) & Ref. \\ \hline
  {} &  & \cite{NavSad10} \\
  {} &  & \cite{NavSad10} \\
  {} &  & New \\
   &  & New \\
  \hline
  \end{tabular}
\end{table}


Our results depend on a new analysis of the empirical entropies of
\emph{similar} strings in Section~\ref{sec:entropysimilar}.
We prove that \emph{the empirical entropy of a string does not change a lot
after a small change to the string} (Theorem~\ref{th:entropysimilar}).
By using this fact, we can delay updating the entire code table.
Thus, after each update operation to the string, we just change a part of
the data structure according to the new code table.
In Section~\ref{sec:replace}, we show that the redundancy in space usage by
this method is negligible, and we obtain Theorem~\ref{th:main1}.

Looking at Table~\ref{tbl:comparison}, we observe that Theorem~\ref{th:main1}
can be interpreted as saying that for arbitrarily small, fixed
, by spending  bits
space more than the best existing data structures for
Compressed Read Only Memory, we can also get
 (i.e., constant) time \texttt{replace} operations.

\subsection{Organization of the paper}

Section~\ref{sec:preliminaries} reviews the definition of the empirical
entropy of a string and the data structure of Ferragina and
Venturini~\cite{FerVen07b}.
In Section~\ref{sec:entropysimilar}, we prove an important result on
the empirical entropies of similar strings.
In Section~\ref{sec:memory} and Appendix~\ref{sec:proof1}, we describe
a technique for maintaining an array of variable-length blocks.
Section~\ref{sec:replace} and Appendix~\ref{sec:indel} explain how to
implement the CRAM and the extended CRAM data structures to achieve
the bounds stated in Theorems~\ref{th:main1} and~\ref{th:main2} above.
Finally, Section~\ref{sec:conclusion} gives some concluding remarks.
Experimental results that demonstrate the good performance of the CRAM
in practice can be found in Appendix~\ref{sec:experiments}.




\section{Preliminaries}\label{sec:preliminaries}

\subsection{Empirical entropy}\label{sec:entropydef}

The compression ratio of a data compression method is often expressed in
terms of the \emph{empirical entropy} of the input strings~\cite{KosMan99}.
We first recall the definition of this concept.
Let  be a string of length  over an alphabet
.
Let  be the number of occurrences of  in .  
Let  be the empirical probability distribution for the string .
The -th order empirical entropy of  is defined as

We also use  to denote the -th order empirical entropy of a string
whose empirical probability distribution is .


Next, let  be any non-negative integer.
If a string  precedes a symbol  in ,
 is called the \emph{context of }.
We denote by  the string that is the concatenation of 
all symbols, each of whose context in  is .
The -th order empirical entropy of  is defined as

It was shown~\cite{Man01} that for any  we have 
and  is a lower bound to the output size of any compressor that encodes each symbol
of  with a code that only depends on the symbol and its context of length .

The technique of \emph{blocking}, i.e., to conceptually merge consecutive
symbols to form new symbols over a larger alphabet, 
is used to reduce the redundancy of Huffman encoding
for compressing a string.  The string  of length  is partitioned into  blocks
of length  each, then Huffman or other entropy codings are applied to compress a new string
 of those blocks.  We call this operation \emph{blocking of length }.


To prove our new results, we shall use the following theorem in
Section~\ref{sec:entropysimilar}:
\begin{theorem}[{\cite[Theorem 16.3.2]{CovTho91}}]\label{th:l1bound}
Let  and  be two probability mass functions on  such that

Then

\end{theorem}


\subsection{Review of Ferragina and Venturini's data structure}\label{sec:FV}

Here, we briefly review the data structure of Ferragina and
Venturini from~\cite{FerVen07b}.
It uses the same basic idea as Huffman coding:
replace every fixed-length block of symbols by a variable-length code
in such a way that frequently occurring blocks get shorter codes than
rarely occurring blocks.

To be more precise, consider a text  over an alphabet  
where  and .
Let  and .
Partition  into  super-blocks,
each contains  characters.
Each super-block is further partitioned into  blocks,
each contains  characters.
Denote the  blocks
by  for .

Since each block is of length , there are 
at most  distinct blocks.
For each block ,
let  be
the frequency of  in .
Let  be the rank of  according to the
decreasing frequency, i.e., the number of distinct blocks  such that ,
and  be its inverse function.
Let  be the rank -th binary string in
.


The data structure of Ferragina and Venturini consists of four arrays:
\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
.

\item[{\raise0.3pt\hbox{}}]
 for .

\item[{\raise0.3pt\hbox{}}]
Table  stores 
the starting position in  of the encoding of every super-block.

\item[{\raise0.3pt\hbox{}}]
Table  stores
the starting position in  of the encoding of every block relative to 
the beginning of its enclosing super-block.
\end{itemize}


The algorithm for \textnormal{\texttt{access}} is simple:
Given , compute the address where the block for  is encoded by
using  and  and obtain the code which encodes the rank
of the block.
Then, from , obtain the substring.
In total, this takes  time.
This yields:

\begin{lemma}[\cite{FerVen07b}]
Any substring  can be retrieved in
 time.
\end{lemma}

Using the data structure of Ferragina and Venturini,
 can be encoded using  bits according to the next lemma.


\begin{lemma}[\cite{FerVen07b}]\label{lem:FV}
The space needed by , and  is as follows:
\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
 is of length  bits, simultaneously for all .
\item[{\raise0.3pt\hbox{}}]
 for  can be stored in
 bits.
\item[{\raise0.3pt\hbox{}}]
 can be stored in  bits.
\item[{\raise0.3pt\hbox{}}]
 can be stored in  bits.
\end{itemize}
\end{lemma}





\section{Entropies of similar strings}\label{sec:entropysimilar}

In this section, we prove that the empirical entropy of a string does not
change much after a small change to it.
This result will be used to bound the space complexity of our main
data structure in Section~\ref{sec:space}.
Consider two strings  and  of length  and ,
respectively, such that the edit distance between  and  is one.
That is,  can be obtained from  by replacement, insertion, or deletion of one character.
We show that the empirical entropies of the two strings
do not differ so much.
\begin{theorem}\label{th:entropysimilar}
For two strings  and  on alphabet 
of length  and  respectively,
such that the edit distance between  and  is one, and for any integer ,
.
\end{theorem}

To prove Theorem~\ref{th:entropysimilar}, we first prove the following:

\begin{lemma}\label{lemma:1}
Let  be a string of length  over an alphabet , 
 be a string made by deleting a character from 
at any position,  be a string made by inserting a character into 
at any position,
and  be a string by replacing a character of  into another one
at any position.
Then the following relations hold:

\end{lemma}

\begin{proof}
Let , , , and  denote the empirical probability of a character 
in , , , and , respectively, and let  denote the number of occurrences of  in .
It holds that  for any .

If a character  is removed from , it holds that
, and 
for any other .  Then 
.
If , it holds , and therefore  and the claim holds.
If , which means that all characters in  are , it holds  and the claim holds.
Otherwise,  holds.
If , from Theorem~\ref{th:l1bound}, 
.
Then .
If , which implies , .
This proves the claim for .

If a character  is inserted into , it holds that
, and 
for any other .  Then .
If ,  and the claim holds.
If , which means that  consists of only the character ,  and the claim holds.
Otherwise,  holds.
If , .
If , which implies , .
This proves the claim for .

If a character  of  is replaced with another character  (),
.
If ,
.
If , which implies , 
.
If ,  and .
This completes the proof.
\qed
\end{proof}

By using this lemma, we prove the theorem.

\begin{proof}(of Theorem~\ref{th:entropysimilar})
From the definition of the empirical entropy, .
Therefore for each context , we estimate the change of -th order entropy.

Because the edit distance between  and  is one,
these are expressed as  and 
by two possibly empty strings  and , and possibly empty characters
 and .
For the context  (), denoted by ,
the character  in the string  will change to .
The character  () has the context , denoted by ,
in , but the context will change to , in .
Therefore a character  is removed from the string , and it is inserted to .
Therefore in at most  strings (),
the entropies will change.  From Lemma~\ref{lemma:1}, each one will change only .
This proves the claim.
\qed
\end{proof}





\section{Memory management}\label{sec:memory}

This section presents a data structure for storing a set~ of
~variable-length strings over the alphabet~,
which is an extension of the one
in~\cite{NavSad10}.
The data structure allows the contents of the strings and their lengths to
change, but the value of~ must remain constant.
We assume a unit-cost word RAM model with word size~ bits.
The memory consists of consecutively ordered bits, and any consecutive  bits
can be accessed in constant time, as stated above.
A string over  of length at most~ is called
a \emph{-block}.
Our data structure stores a set~ of  such -blocks,
while supporting the following operations:
\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
  \texttt{address}():
  Return a pointer to where in the memory the -th -block
  is stored ().
\item[{\raise0.3pt\hbox{}}]
  \texttt{realloc}():
  Change the length of the -th -block to ~bits
  ().
  The physical address for storing the block (\texttt{address}()) may change.
\end{itemize}

\begin{theorem}
\label{theorem: Memory_management}
Given that  and ,
consider the unit-cost word RAM model with word size .
Let   be a set of -blocks
and let  be the total number of bits of all -blocks in~.
We can store  in  bits while supporting
\textnormal{\texttt{address}} in  time and
\textnormal{\texttt{realloc}} in  time.
\end{theorem}

\begin{theorem}
\label{theorem: Memory_management2}
Given a parameter ,
consider the unit-cost word RAM model with word size .
Let   be a set of -blocks,
and let
 be the total number of bits of all -blocks in~.
We can store  in  bits while supporting
\textnormal{\texttt{address}} and \textnormal{\texttt{realloc}}
in worst-case  time.
\end{theorem}

\noindent
(Due to lack of space, the proofs of
Theorems~\ref{theorem: Memory_management}
and~\ref{theorem: Memory_management2}
are given in Appendix~\ref{sec:proof1}.)
From here on, we say that the data structure has parameters .




\section{A data structure for maintaining the CRAM}
\label{sec:replace}

This section is devoted to proving Theorem~\ref{th:main1}.
Our aim is to dynamize Ferragina and Venturini's
data structure~\cite{FerVen07b} by allowing \texttt{replace} operations.
(Our data structure for the extended CRAM which also supports
\texttt{insert} and \texttt{delete} is described in Appendix~\ref{sec:indel}.)
Ferragina and Venturini's data structure uses a code table for encoding
the string, while our data structure uses two code tables, which will
change during update operations.

Given a string  defined over an alphabet  (),
we support two operations. 
(1) : which returns ; and 
(2) : which replaces  with a character .

We use blocking of length  of .
Let  be a string of length  on an alphabet 

made by blocking of .
The alphabet size is .
Each character  corresponds to the string .
A super-block consists of  consecutive blocks in 
( consecutive characters in ),
where  is a predefined constant.

Our algorithm runs in phases.
Let .
For every , we refer to the sequence of the -th to
-th replacements as \emph{phase~}.
The preprocessing stage corresponds to phase .
Let  denote the string just before phase .  (Hence,  is the input string .)
Let  denote the frequency table of blocks  
in , 
and  and  a code table and a decode table defined below.
The algorithm also uses a bit-vector ,
where  means that the -th super-block in 
is encoded by code table ; otherwise, it is encoded
by code table .

During the execution of the algorithm, we maintain the following invariant:
\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
At the beginning of phase , the string  is encoded with
code table  (we assume ), and
the table  stores the frequencies of blocks in .
\item[{\raise0.3pt\hbox{}}]
During phase , the -th super-block is encoded with
code table  if , or  if .
The code tables  and  do not change.\item[{\raise0.3pt\hbox{}}]
During phase ,  stores the correct frequency of blocks
of the current~.
\end{itemize}

\subsection{Phase : Preprocessing}

First, for each block ,
we count the numbers of its occurrences in 
and store it in an array .
Then we sort the blocks  in decreasing order of
the frequencies , and assign a code  to encode them.
The code for a block  is defined as follows.
If the length of the code , defined in Section~\ref{sec:FV},
is at most  bits, then  consists of a bit `0',
followed by .  Otherwise, it consists of a bit `1',
followed by the binary encoding of , that is, the block is stored without
compression.
The code length for any block  is upper bounded 
by  bits.
Then we construct a table  for decoding a block.
The table has  entries
and  for all binary patterns  of length 
such that a prefix of  is equal to .  Note that this decode table
is similar to  defined in Section~\ref{sec:FV}.

Next, for each block  (), 
compute its length using ,
allocate space for storing it using the data structure of Theorem~\ref{theorem: Memory_management2}
with parameters 
,
and .
From Lemma~\ref{lem:FV} and Theorem~\ref{theorem: Memory_management2},
if follows that
the size of the initial data structure is 
bits.
Finally, for later use, copy the contents of  to ,
and initialize  by .
By sorting the blocks by a radix sort, the preprocessing time becomes
.


\subsection{Algorithm for \texttt{access}}
The algorithm for \texttt{access} is:
Given the index ,
compute the block number 
and the super-block number  containing .
Obtain the pointer to the block and the length of the code
by \texttt{address}().
Decode the block using the decode table  if ,
or  if .
This takes constant time.


\subsection{Algorithm for \texttt{replace}}
We first explain a naive, inefficient algorithm.
If  is replaced with , we change the frequency
table  
so that  is decremented by one and  is
incremented by one.  Then new code table  and decode table 
are computed from updated ,
and all blocks  () are re-encoded by 
using the new code table.
Obviously, this algorithm is too slow.

To get a faster algorithm, we can delay updating code tables for the
blocks and re-writing the blocks using new code tables
because of Theorem~\ref{th:entropysimilar}.  Because the amount of change
in entropy is small after a small change in the string,  we can show that
the redundancy of using code tables defined according to an old string can be
negligible.  
For each single character change in , we re-encode
a super-block ( characters in ).
After  changes, the whole string will be re-encoded.
To specify which super-block to be re-encoded,
we use an integer array .
It stores a permutation of
 and indicates
that at the -th replace operation in phase 
we rewrite the -th super-block.
The bit  indicates 
if the super-block has been already rewritten or not.
The array  is defined by sorting super-blocks
in increasing order of lengths of codes for encoding super-blocks.

We implement  as follows.
In the -th update in phase~,
\begin{enumerate}
\item
If , i.e., 
if the -th super-block is encoded with ,
decode it and re-encode it with , and
set .
\item
Let  be the super-block number containing ,
 that is, .
\item
Decode the -th super-block, which is encoded with  or
 depending on .
Let  denote the block containing .
Make a new block  from  by applying the \texttt{replace} operation.
\item
Decrement the frequency  
and increment the frequency .
\item
Compute the code for encoding  using  if the -th super-block
is already re-encoded (), or  otherwise ().
\item
Compute the lengths of the blocks in -th super-block and apply
\texttt{realloc} for those blocks.
\item
Rewrite the blocks in the -th super-block.
\item
Construct a part of tables , , , and 
(see below).
\end{enumerate}

To prove that the algorithm above maintains the invariant, we need only to prove
that the tables , , and 
are ready at the beginning of phase .
In phase , we create  based on .  
This is done by just radix-sorting
the frequencies of blocks, and therefore the total time complexity is
.
Because phase  consists of 
 \texttt{replace} operations, the work for creating 
can be distributed in the phase.
We represent the array  implicitly by 
 doubly-linked lists ; 
 stores super-blocks of length .  By retrieving the lists 
in decreasing order of  we can
enumerate the elements of .  If all the elements of a list have been
retrieved, we move to the next non-empty list.  This can be done in 
 time if we use a bit-vector of
 bits indicating which lists are non-empty.
We copy  to  in constant time
by changing pointers to  and .
For each \texttt{replace} in phase , 
we re-encode a super-block, which consists of
 blocks.  This takes  time.
Therefore the time complexity for \texttt{replace} is  time.

Note that during phase , only the tables
, , 
, , , 
, , , 
, , 
, and 
are stored.
The other tables are discarded.


\subsection{Space analysis}\label{sec:space}

Let  denote the size of the encoding of  by our dynamic data structure.
At the beginning of phase , the string  is encoded with
code table , which is based on the string .
Let  and .

After the preprocessing,
.
If we do not re-encode the string, for each \texttt{replace} operation we write at most
 bits.  Therefore  holds.
Because  is made by  character changes to ,
from Theorem~\ref{th:entropysimilar}, we have
.
Therefore we obtain
.
The space for storing the tables , , , ,
, and  is
, , ,
,
,
 bits, respectively.

Next we analyze the space redundancy caused by the re-encoding of super-blocks.
We re-encode the super-blocks with the new code table in increasing order of their lengths,
that is, the shortest one is re-encoded first,
This guarantees that
at any time the space does not exceed .
This completes the proof of Theorem~\ref{th:main1}.




\section{Concluding remarks}\label{sec:conclusion}

We have presented a data structure called Compressed Random Access Memory
(CRAM), which compresses a string  of length  into its -th order
empirical entropy in such a way that any consecutive  bits
can be obtained in constant time (the \texttt{access} operation), and
replacing a character (the \texttt{replace} operation) takes
 time.
The time for \texttt{replace} can be reduced to constant
() time
by allowing an additional  bits redundancy.
The extended CRAM data structure also supports the \texttt{insert} and
\texttt{delete} operations, at the cost of increasing the time for
\texttt{access} to  time, which is optimal under
this stronger requirement, and the time for each update operation also
becomes .

An open problem is how to improve the running time of \texttt{replace}
for the CRAM data structure to  without using the
 extra bits.




\subsection*{Acknowledgments}
JJ~was funded by the Special Coordination Funds for Promoting Science and
Technology, Japan.
KS~was supported in part by KAKENHI 23240002.
WKS~was supported in part by the MOE's AcRF Tier 2 funding
R-252-000-444-112.



\bibliographystyle{plain}
\bibliography{Bibl_compressed_RAM}



\newpage

\appendix

\section*{Appendix}

\section{Memory management}\label{sec2:memory}\label{sec:proof1}

This appendix proves Theorems~\ref{theorem: Memory_management}
and~\ref{theorem: Memory_management2}
from Section~\ref{sec:memory}.

\medskip

We first describe some technical details.
(Recall the definitions from Section~\ref{sec:memory}.)
Let .
This is the number of bits needed to represent a memory address
relative to the head of a memory region storing .
For any -block~ in~, we will refer to the current contents
of~ by~.
To store  for all -blocks compactly while allowing efficient
updates, we allocate memory in such a way that  for any given
-block~ may be spread out over at most two non-consecutive regions in the
memory.
For this purpose, define a \emph{segment} to be ~consecutive bits
of memory.
The data structure will always allocate and deallocate memory in terms of
segments, and keep a pointer to the start of the segment currently at the
highest address in the memory.

The core of our data structure is  doubly-linked lists
, where
each list  is a doubly-linked list of a set of segments,
each is of length  bits.
The main idea is to use the segments in list~ to store all
-blocks of length~.
Moreover, we do so in such a way that when the length of a
-block changes from~ to~, we only need to update a
few segments in the two lists~ and~.

Every segment belonging to a list~ is used as follows:
\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
  \emph{pred}:
  ~bits to store the memory address of its predecessor segment in~.
\item[{\raise0.3pt\hbox{}}]
  \emph{succ}:
  ~bits to store the memory address of its successor segment in~.
\item[{\raise0.3pt\hbox{}}]
  \emph{block\_data}:
  ~bits for storing information associated with one or more -blocks
  of length~.
  More precisely, for , let  denote the binary
  encoding of the integer~ in ~bits.
  For every , form a string~ by
  concatenating the pairs~ for all -blocks~ of
  length~ in some arbitrary order.
  Divide  into substrings of length~ and store them in the
  \emph{block\_data} region of consecutive segments in~.
  Only the first segment in each list~ is allowed to have some unused
  bits in its \emph{block\_data} region.
\item[{\raise0.3pt\hbox{}}]
  \emph{offset}:
  ~bits to store the relative starting position within
  the \emph{block\_data}-region of the first -block.
  Observe that the head \emph{offset} bits of the segment are used
  to store another -block, whose starting position belongs to another
  segment, except the first segment in .
\end{itemize}

See Fig.~\ref{figure2: fig_memory_management_B} for an illustration.
Note that a -block~ may stretch across two segments
in~, and that these two segments might not be located in
a consecutive region of the memory.

\begin{figure}[h!]
\begin{center}
  \includegraphics[scale=0.35]{fig_memory_management_B2.eps}
\caption{Each doubly-linked list~ consists of segments which together
store all -blocks of length~ in their \emph{block\_data}-regions
(shown here).
The data for a single -block may be split across two segments
and the first segment in~ may contain some unused bits.}
\label{figure2: fig_memory_management_B}
\vspace*{-5mm}
\end{center}
\end{figure}


In addition to the doubly-linked lists, we use four arrays
, , , and  to
remember the locations of the -blocks.
To be precise, for each , we have
\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
 contains a pointer  to .
\item[{\raise0.3pt\hbox{}}]
 contains the relative location of 
from the starting position of the segment.
\item[{\raise0.3pt\hbox{}}]
 stores the length of .
\item[{\raise0.3pt\hbox{}}]
 stores the memory address of the segment which stores blocks
 with .
\end{itemize}
The array  is used for indirect addressing.
Consider the case that we do not use the array  and store memory addresses 
in  directly.
When a segment has moved to another address, we have to rewrite non-constant
number of 's for 's such that the -th block is stored in the segment.
On the other hand, if  is used,  we need not to rewrite all 's and
it is enough to rewrite only an entry  ( for those 's).

\medskip

\begin{proof} (of Theorem~\ref{theorem: Memory_management})
To implement \texttt{address} in  time, 
we simply return three values , , and .
Observe that  is stored at position  of the segment .
Moreover, when , the -block  spans
across two segments (i.e. segments  and ).
In this case, the first  bits of  are
stored at the end of segment  while
the remaining bits of  are stored
at the beginning of segment .

To implement \texttt{realloc} in  time, we first find the
location~ of  by \texttt{address}().
Store  in a temporary space~.
Obtain the index  of the first -block~ in~
from the \emph{block\_data} of the first segment in~.
Copy the first -block~ in~ to position~ and
update~ and  accordingly.
If the first segment in~ becomes empty, move the segment
with the largest address to the empty location.
Let  and  be the addresses of the segment before and after the movement,
respectively.
We have to change pointers to all -blocks 
stored in the moved segment.  Though the moved segment may contain 
-blocks, this can be done in constant time as follows.
For any -block  in the segment, it holds  for some
, and  before the movement.
Therefore it holds  for all the blocks.
After the movement, it must hold that  for all the blocks.
This is done in constant time by simply setting  
and we need not change  for each .
Each value  is an integer in the range  and corresponds to
a segment.  Precisely,  if the data structure currently has  segments, and
a new segment is allocated for , we set  and 
is set to the address of the new segment.  If the blocks in the -st segment
have been moved, we change .  

Next, copy the ~bits stored in~ to immediately before the first
-block in the head of the list~.
In case the first segment of~ does not have enough space to store
it, we allocate a new segment.
Finally, update , , , and .
The time complexity is .  This is necessary to copy
segments of  bits.

\medskip

To analyze the space complexity, note that the total length of  for
all  is~.
Each list  has at most one segment which has an empty slot, and therefore
there exist at most  segments with an empty slot.
Then the number of segments to store  bits is at most  and
the space to store the segments is 
.
This is  because 
and .
We also need  bit space 
for storing , , , and .
In total, the total space is
 bits.
This proves Theorem~\ref{theorem: Memory_management}.
\qed
\end{proof}

The data structure of Theorem~\ref{theorem: Memory_management} uses
 bits additional space, which is too large if we want
to store many short blocks.
Because of this, we give an alternative data structure in
Theorem~\ref{theorem: Memory_management2}.
We say the data structure has parameters .

\begin{proof}  (of Theorem~\ref{theorem: Memory_management2})
We use Theorem~\ref{theorem: Memory_management} to store a set of -blocks
 for  and  for some constant .
We say the data structure has parameters .
For general , we split  into  sub-arrays
 and to store each sub-array 
we use Theorem~\ref{theorem: Memory_management} with parameters .
Then \texttt{address}() and \texttt{realloc}() are done in constant time.
However a problem is that the memory space to store the data structure 
for each sub-array will change, and we cannot store it in a consecutive memory region.
To overcome this, we use a two-level data structure.
Let .
The higher level consists of  data structures of 
Theorem~\ref{theorem: Memory_management} with parameters .
We call each one  
().  Each  uses a consecutive
memory region, which is impossible.  Therefore we use a kind of \emph{virtual memory}.
The memory to store segments is divided into pages.
Each page uses a physically consecutive memory region, while the pages are
located in non-consecutive regions.
Each data structure  has  pages, and each page contains either  segments
or no segments, depending on how many bits are necessary to store the blocks.
Precisely, if  segments are necessary, the first  pages
have  segments each, and the rest have no segments.
The total number of pages for all  () is 
.

The whole pages of all the data structures  ()
are managed by a single data structure of Theorem~\ref{theorem: Memory_management}
with parameters .
We call the data structure .  
The algorithm for \texttt{address}() becomes as follows.
Let  and .
The -th block is stored as the -th block of .
Therefore we compute \texttt{address}() in , and obtain the logical (virtual)
address  of the segment containing the block.
To convert it into the physical (real) address  of the memory,
we first compute the page number  for the segment, then compute
\texttt{address}() in .  It is straightforward to compute the address
for the block inside the page because the segments in the page are of
the same length.
These operations are done in constant time.

The function \texttt{realloc}() is implemented as follows.
First we find the data structure  that contains the -th block,
and execute \texttt{realloc} in .  Note that  uses virtual memory
which is managed by .  Therefore we have to convert a logical address to
a physical one for any memory access.
During the execution of \texttt{realloc} in , we have to move a constant
number of pages of  bits in .
If we naively use the result of Theorem~\ref{theorem: Memory_management},
it takes  time.  It is easy to obtain an amortized  time
algorithm because each block is of  bits and movement of pages in 
occurs every  operations of \texttt{realloc} in .
It is also easy to obtain a worst-case  time algorithm at the cost
of  bit redundant space for each .  We can spread the movement
of pages over the next  execution of \texttt{realloc} in .
Then a constant number of pages with empty slots will exist, resulting in 
bit redundancy in space.  This redundancy sums up to 
 bits for all .

We analyze the space complexity.  Each  has -bit
auxiliary data structures, and they sum up to  bits.
The data structure  uses  bits.
Therefore the total space is  bits.
\qed
\end{proof}




\section{A data structure for maintaining the extended CRAM}
\label{sec:indel}

This section proves Theorem~\ref{th:main2}.
To support \texttt{insert} and \texttt{delete} efficiently,
we use variable length super-blocks.  
Namely, let  and .
Each super-block consists of  to  blocks
( to  consecutive characters in ).
These super-blocks are stored using the data structure of Theorem~\ref{theorem: Memory_management} with parameters .
To represent super-block boundaries, we use a bit-vector 
such that  means that  is the first character in a super-block.
Therefore  has  ones.  
This bit-vector is stored using the following data structure.
\begin{lemma}[Lemma 17~\cite{NavSad10}]\label{lem2:GS}
We can maintain any bit-vector  within   bits of space,
while supporting the operations , ,
 , and ,
all in time .
\end{lemma}

Because , the bit-vector can be stored
in  bits,
and  can be computed in  time,
where  is the number of ones in .
By using this data structure, we can compute the super-block number containing  by .

The algorithm for \texttt{insert} and \texttt{delete} in
the extended CRAM works as follows.
First, we re-write the super-block in which \texttt{insert} or \texttt{delete} occurs.
If it contains less than  or more than  blocks, we merge two consecutive super-blocks
or split it into two to maintain the invariant that every super-block consists of  to  blocks.  If the lengths of super-blocks change, we update the bit-vector  accordingly.
Because only a constant number of super-blocks change, the time complexity is 
.

However, in the extended CRAM, the time complexity of 
increases from  to :
We first compute the super-block number for  by using
 in  time.
Then, we scan all the blocks in the super-block
to find the location storing .
Furthermore, for each  operation, we re-encode
a super-block.
This means that we set  in Theorem~\ref{th:main1}.
The time complexity becomes .

We also have to consider ``the change of  problem''.
The sizes of blocks and super-blocks
depend on , the length of the string.  If  changes a lot,  also changes, and
we have to reconstruct the data structure for the new value of .  To avoid this,
we use the same technique as M{\"a}kinen and Navarro~\cite{MNtalg08}.  We partition the string  into three parts
, and encode them using  as their ``'' values,
respectively.  
We maintain the following invariant that
if  is zero or a power of two,  and  are empty and  is
equal to , and
if  increases by one, the length of  grows by two and that of 
shrinks by one.  To accomplish this,
depending on where an insert occurs, we move the rightmost one or two characters
of  to the beginning of , and the rightmost one or two characters of  to the beginning of .
A deletion is done similarly.
We can guarantee that if the string length is doubled,
``'' increases by one.
For example, if  is a power of two then all the characters belong to .
Then after  insertions, the length becomes  and
all the characters move to , and now  becomes the new .
The strings , , and  are stored using
the data structure of Theorem~\ref{theorem: Memory_management} with parameters
,
,
,
respectively.
The asymptotic space and time complexities do not change.




\section{Experimental results}
\label{sec:experiments}

To test the performance of the CRAM data structure in practice, we
implemented a prototype named \texttt{cram} and compared it to
a \texttt{gzip}-based alternative method named \texttt{cramgz}.
For the implementations, we used the C~programming language.
In this appendix, we describe the details of the experiments and the outcome.


\begin{enumerate}
\item
\texttt{cram} is the data structure that we introduced in this paper, but
a few changes were made to make it easier to implement.
(These simplifications may reduce the performance of the method slightly;
however, as shown below, it is still excellent.)
To be precise, the \texttt{cram}-prototype works as described in the paper
but with the following modifications:

\smallskip

\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
The memory is partitioned into \emph{large blocks}, and the size of each
large block is denoted by~.
Each large block is further partitioned into \emph{middle blocks} of
length~.
Finally, each middle block stores \emph{blocks} of length
, instead of , 
which correspond to the ``blocks'' described in the paper.

\smallskip
\item[{\raise0.3pt\hbox{}}]
Pointers to large blocks and middle blocks are stored.

\smallskip
\item[{\raise0.3pt\hbox{}}]
Each block is encoded by a Huffman code.
We assign codes to all characters in the alphabet because otherwise we
cannot encode characters that are missing from the initial string but later
appear due to \texttt{replace} operations.

\smallskip
\item[{\raise0.3pt\hbox{}}]
An additional parameter~ is employed as follows.
If  bytes of memory change occur, then  bytes are re-encoded
with new codes.
Furthermore, if , a large block is re-encoded.
(This means that the worst-case time complexity of \texttt{replace}
becomes amortized, so it is no longer constant.)
\end{itemize}

\medskip

\item
Next, \texttt{cramgz} is an original data structure based on \texttt{gzip}:

\smallskip

\begin{itemize}
\item[{\raise0.3pt\hbox{}}]
The memory is partitioned into \emph{large blocks} of length~.

\smallskip
\item[{\raise0.3pt\hbox{}}]
Each large block is compressed by \texttt{gzip} and stored using our
dynamic memory management algorithm.
We employed the zlib library\footnote{\texttt{http://zlib.net}}, with
compression level~ (i.e., the fastest compression).
\end{itemize}
\end{enumerate}


\noindent
For the experiments, we used the two text files \emph{English} and \emph{DNA}
from the Pizza \& Chili
corpus~\footnote{\texttt{http://pizzachili.dcc.uchile.cl}}.
The lengths of these texts are limited to 200MB, so 
and the alphabet size .

\bigskip

\noindent
\textbf{Experiment~1}:
The first experiment only involved \texttt{cram} and was done to determine
a suitable value for the parameter~.
We first built the CRAM data structure for the English text using the
parameters , .
Then, we overwrote the DNA text onto the (compressed) English text one
character at a time, from left to right, by using \texttt{replace}
operations.
The -st order entropy of the initial text was about ~bits per
character (bpc), and including the auxiliary data structure, the size was
about ~bpc.
On the other hand, since the DNA text only consists of four distinct
characters a, c, g, t and the text is almost random,
its -st order entropy is ~bpc and
about ~bpc including the auxiliary data structure.

Fig.~\ref{fig:size} shows the outcome.
As expected, when the text is modified, its -st order entropy (shown in
the curve `entropy') changes.
The other lines in the diagram show the data structure's change in size for
various values of the parameter~.
If , the code never changes because we write the whole DNA text
before the end of Phase~ (see Section~\ref{sec:replace} for the
explanation of ``phase'').
In other words, all the characters in the DNA text are encoded by the optimal
code for the English text, and the resulting size is bad.
If , the code is updated while the text is modified, and the size
converges to the entropy (plus the size of the auxiliary data structure).
We can see that as the value of~ increases, the size converges towards
the entropy more quickly but the time needed to update the data structure
also increases.
We select  as a good trade-off between the convergence speed and the
update time.


\begin{figure}[t!]
\begin{center}
  \includegraphics[scale=0.90]{size.eps}
\caption{Results for Experiment~1.
The -axis represents the ratio of overwritten text;
~is the initial situation where the entire text is from the English
text, ~means that the left half of the text has been changed to the DNA
text while the right half is still from the English text, and ~is
when the whole text has become the DNA text.}
\label{fig:size}
\end{center}
\end{figure}


\medskip

\noindent
\textbf{Experiment~2}:
The second experiment compared the \texttt{access} and \texttt{replace} times
for \texttt{cram} and \texttt{cramgz}.
We set the parameters of the data structures so that both have the same size.
For \texttt{cram}, we set , , .
Then, the English text has size  bpc.
To achieve the same compression ratio ( bpc) for \texttt{cramgz}, we
had to select .

We performed two types of experiments:
one to evaluate \texttt{access} by measuring the time needed to read the
entire compressed English text, and one to evaluate \texttt{replace} by
measuring the time needed to overwrite the DNA text onto the compressed
English text.
We combined series of consecutive operations into single read/write unit
operations, and tried various sizes of read/write units smaller than the
block size.
Fig.~\ref{fig:time} shows the size of each read/write unit and the resulting
time for \texttt{access} and \texttt{replace}.
In \texttt{cramgz}, large blocks of length  bytes are directly
compressed by \texttt{gzip}, and reading any unit shorter than ~bytes
still requires decoding the whole large block.
Therefore, \texttt{access} is not very efficient when using units shorter
than ~bytes.
Similarly, writing a short unit requires decoding a large block, rewriting
a part of it, and then encoding the whole large block again.
On the other hand, in \texttt{cram}, the base is the middle block (i.e.,
~bytes) and therefore it is more efficient than \texttt{cramgz}.
Fig.~\ref{fig:time} shows that \texttt{cram} is faster than \texttt{cramgz}
for all read unit sizes, and faster for  to  bytes write unit sizes.


\begin{figure}[t!]
\begin{center}
  \includegraphics[scale=0.90]{time3.eps}
\caption{Results for Experiment~2.
The -axis shows the size of the unit for read/write, and the -axis
shows the resulting time needed to read the whole text by \texttt{access}
operations or to overwrite the whole text by \texttt{replace} operations,
respectively.}
\label{fig:time}
\end{center}
\end{figure}




\end{document}
