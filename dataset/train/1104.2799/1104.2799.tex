\documentclass[letterpaper,11pt]{article}

\usepackage{verbatim}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{fullpage}
\usepackage{epsfig,ifpdf,graphics}
\usepackage{times}
\usepackage{color}


\let\myPushQED=\pushQED
\let\myPopQED=\popQED
\newcommand{\myignore}[1]{}
\newenvironment{proof*}
  {\let\pushQED=\myignore\begin{proof}\let\pushQED=\myPushQED}
  {\def\popQED{}\end{proof}\let\popQED=\myPopQED}

\newenvironment{description*}{\vspace{-1ex}\begin{description}\setlength{\itemsep}{-0.5ex}\setlength{\parsep}{0pt}}{\end{description}}
\newenvironment{itemize*}{\vspace{-1ex}\begin{itemize}\setlength{\itemsep}{-0.5ex}\setlength{\parsep}{0pt}}{\end{itemize}}
\newenvironment{enumerate*}{\vspace{-1ex}\begin{enumerate}\setlength{\itemsep}{-0.5ex}\setlength{\parsep}{0pt}}{\end{enumerate}}

{\makeatletter
 \gdef\xxxmark{\expandafter\ifx\csname @mpargs\endcsname\relax \expandafter\ifx\csname @captype\endcsname\relax \marginpar{xxx}\else
       xxx \fi
   \else
     xxx \fi}
 \gdef\xxx{\@ifnextchar[\xxx@lab\xxx@nolab}
 \long\gdef\xxx@lab[#1]#2{{\bf [\xxxmark #2 ---{\sc #1}]}}
 \long\gdef\xxx@nolab#1{{\bf [\xxxmark #1]}}
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{invariant}[theorem]{Invariant}


\newcommand{\poly}{\mathrm{poly}}
\newcommand{\polylog}{\mathrm{polylog}}
\newcommand{\tO}{\widetilde{O}}
\newcommand{\tOmega}{\widetilde{\Omega}}
\newcommand{\eps}{\varepsilon}
\newcommand{\twodots}{\mathinner{\ldotp\ldotp}}
\newcommand{\proc}[1]{\textnormal{\scshape#1}}

\newcommand{\calD}{\mathcal{D}}
\newcommand{\Dyes}{\calD_{\mathrm{YES}}}
\newcommand{\Dno}{\calD_{\mathrm{NO}}}

\newcommand{\imax}{{i_{\max}}}
\newcommand{\imin}{{i_{\min}}}

\newcommand{\tmin}{t^{\min}}

\let\phi=\varphi
\newcommand{\E}{\mathbf{E}}

\newcommand{\foot}{\mathrm{Foot}}
\newcommand{\occ}{\mathrm{occ}}

\newcommand{\john}[1]{{\color{red}(JI: #1)}}

\newcommand{\MSB}{\proc{high}}
\newcommand{\LSB}{\proc{low}}

\title{Using Hashing to Solve the Dictionary Problem \
t_u = O(\tfrac{\lambda}{B} \lg n) &\qquad&
  t_q = O(\log_\lambda n), \qquad \textrm{for } 2 \le \lambda \le B 
\label{eq:buff-low}  \\
t_u = O(\tfrac{1}{B} \log_\lambda n) & &
  t_q = O(\lambda \lg n), \qquad \textrm{for } 2 \le \lambda \le
  \tfrac{M}{B}.  \label{eq:buff-hi}
 U(t) = O(\tfrac{\lg t}{b}) + 2 \cdot U(\sqrt{t});
\qquad\qquad 
U(t \leq \tmin) = O(\tfrac{t\lg t}{b})
 Q(t) = 1 + 2\cdot Q(\sqrt{t});
\qquad \qquad Q(\tmin) = 1  \sum_i O\Big( \frac{\lg n}{\lg(2^i \lg tmin)} \cdot
                  \lg\frac{\lg(2^i \lg\tmin)}{\lg \tmin} \Big) 
= O\Big(\frac{\lg n}{\lg \tmin} \Big) \sum i 2^{-i} 
= O\Big( \frac{\lg n}{\lg \tmin} \Big)
= O(Q(t)). O(\tfrac{1}{B} \log_M n) + U(\tfrac{M}{b}) \cdot \log_M n
=  O(\tfrac{\log_M n}{B}) + O(\log_M n) \cdot
  \tfrac{\lg M}{B\lg n} \cdot (\lg\lg M + \tmin \lg \tmin)

Thus .  Note that .  For any , we can achieve
 by setting .

The cost of querying a key is .

The total space of our data structure is linear. Each key is stored in
only one top-level gadget at a time, and, as we argued
in~\ref{s:analysis}, these are have linear size (in words).

\bibliographystyle{alpha} \bibliography{general}

\appendix

\section{Lower Bounds}



Formally, our model of computation is defined as follows. The update
and query algorithms execute on a processor with  bits of state,
which is preserved from one operation to the next. The memory is an
array of cells of  bits each, and allows random access in
constant time. The processor is nonuniform: at each step, it decides
on a memory cell as an arbitrary function of its state. It can either
write the cell (with a value chosen as an arbitrary function of the
state), or read the cell and update the state as an arbitrary function
of the cell contents.

We first define our hard distribution. Choose  to be a random set
of  keys from the universe , and insert the keys of  in
random order. The sequence contains a single query, which appears at a
random position in the second half of the stream of operations. In
other words, we pick  and place the query
after the -th update (say, at time ). 

Looking back in time from the query, we group the updates into epochs
of  updates, where  is a parameter to be
determined.  Let  be the keys inserted in epoch ;
. Let  be the largest value such that
 (remember that there are
at least  updates before the query). We always construct 
epochs. Let  be the smallest value such that ; remember that  was the cache size in bits. Our
lower bound will generally ignore epochs below , since most of
those elements may be present in the cache. Note .

We have not yet specified the distribution of the query. Let  be
the distribution in which the query is chosen uniformly at random from
.  Let  be the distribution in which the
query is uniformly chosen among . Then, . Observe that
elements in smaller epochs have a much higher probability of being the
query. We will prove a lower bound on the mixture . Since we are working under a distribution, we may assume the
data structure is deterministic by fixing its random coins
(nonuniformly).

\begin{observation}
Assume the data structure has error  on . Then the error on  is at most . At least half
the choices of  are \emph{good} in the
sense that the error on  is at most .
\end{observation}

We will now present the high-level structure of our proof. We focus on
some epoch  and try to prove that a random query in  reads
 cells that are somehow ``related'' to epoch .  To
achieve this, we consider the following encoding problem: for , the problem is to send a random bit vector  with exactly  ones. The entropy of  is  bits.

The encoder constructs a membership instance based on its array 
and public coins (which the decoder can also see). It then runs the
data structure on this instance, and sends a message whose size
depends on the efficiency of the data structure. Comparing the message
size to the entropy lower bound, we get an average case cell-probe
lower bound.

The encoder constructs a data structure instance as follows:
\begin{itemize*}
\item Pick the position of the query by public coins. Also pick
   by public coins.

\item Pick  a random set of  keys
  from , also by public coins.

\item Let , i.e.~the message is encoded
  in the choice of  out of .

\item Run every query in  on the memory snapshot just before the
  position chosen in step 1.
\end{itemize*}

The idea is to send a message, depending on the actions of the data
structure, that will also allow the decoder to simulate the
queries. Note that, up to the small probability of a query error, this
allows the decoder to recover :  iff .

It is crucial to note that the above process generates an  that is
uniform inside . Furthermore, each  with  is
uniformly distributed over . That means that for
each negative query, the data structure is being simulated on
. For each positive query, the data structure is being simulated
on .

Let  be the cells read during epoch , and  be the cells
written during epoch  (these are random variables). We will use the
convenient notation . 

The encoding algorithm will require different ideas for 
and .


\subsection{Constant Update Time}

We now give a much simpler proof for the lower bound of
\cite{verbin10buffer}: for any , the query time is
. 

We first note that the decoder can compute the snapshot of the memory
before the beginning of epoch , by simply simulating the data
structure ( was chosen by public coins). The message of the
encoder will contain the cache right before the query, and the address
and contents of . To bound  we use:

\begin{observation}  \label{obs:wi}
For any epoch , .
\end{observation}

\begin{proof}
We have a sequence of  updates, whose average expected cost (by
amortization) is  cell probes. Epochs  is an interval of
 updates. Since the query is placed randomly, this interval
is shifted randomly, so its expected cost is .
\end{proof}

We start by defining a crucial notion: the footprint  of
query , relative to epoch . Informally, these are the cells that
query  would read if the decoder simulated it ``to the best of its
knowledge.'' Formally, we simulate , but whenever it reads a cell
from , we feed into the simulation the value of
the cell from before epoch . Of course, the simulation may be
incorrect if at least one cell from  is
read. However, we insist on a worst-case time of  (remember that
we allow Monte Carlo randomization, so we can put a worst-case bound
on the query time). Let  be the set of cells that the
query algorithm reads in this, possibly bogus, simulation.

Note that the decoder can compute  if it knows the cache
contents at query time and : it need only simulate  and
assume optimistically that no cell from  is
read.

For some set , let .  Now
define the footprint of epoch : . In other words, the footprint of an epoch
contains the cells written in the epoch and the cells that the
positive queries to that epoch would read in the decoder's optimistic
simulation, but excludes the footprints of smaller epochs. Note that
.


We are now ready for the main part of our proof. Let  be the
binary entropy.

\begin{lemma}  \label{lem:const-tu}
Let  be good and . Let
 be the probability over  that the query reads a cell from
. We can encode a vector of  bits with  ones by a message
of expected size: .
\end{lemma}

Before we prove the lemma, we show it implies the desired lower
bound. Choose , which implies . Note that 
is a constant bounded below  (depending on ). On the other
hand . Thus, there
exist a small enough  depending on  such that
. Since the entropy of the message is
, we must have . Thus , so .

We have shown that a query over  reads a cell from  with
constant probability, for any good . Since half the 's are good
and the 's are disjoint by construction, the expected query time
must be .

\paragraph{Proof of Lemma \ref{lem:const-tu}.}
The encoding will consist of:
\begin{enumerate*}
\item The cache contents right before the query. 

\item The address and contents of cells in . In particular
  this includes , so the decoder can compute  for
  any query.

\item Let  be the set of negative queries
  that read at least one cell from . We encode  as a subset of
  , taking  bits. 

\item For each cell , mark some
  \emph{positive} query  such that . Some
  cells may not mark any query (if they are not in the footprint of
  any positive query), and multiple cells may mark the same
  query. Let  be the set of marked queries. We encode  as a
  subset of , taking  bits.

\item The set of queries of  that return incorrect results (Monte
  Carlo error).
\end{enumerate*}

\noindent
The decoder immediately knows that queries from  are negative, and
queries from  are positive. Now consider what happens if the
decoder simulates some query . If 
reads some cell from , we claim it must
be a positive query. Indeed, , so . But any negative query that reads from 
was identified in .

\begin{claim}
If a query  does not read any cell from
, the decoder can simulate it correctly.
\end{claim}

\begin{proof}
We will prove that such a query does not read any cell from , which means that the decoder knows all necessary
cells. First consider positive . If  reads a cell from  and  is not marked, it means this cell marked
some other . But that means the cell is in , contradiction.

Now consider negative . Note that , so if  had read a cell from this set, it would have been
placed in the set .
\end{proof}

Of course, some queries may give wrong answers when simulated
correctly (Monte Carlo error). The decoder can fix these using
component 5 of the encoding.

It remains to analyze the expected size of the encoding. To bound the
binomial coefficients, we use the inequality: . We will also use convexity of 
repeatedly.
\begin{enumerate*}
\item The cache size is  bits, since . 

\item We have . So this component takes  bits on average.

\item Since , this takes 
  bits on average.

\item Since , this takes  bits on average.

\item The probability of an error is at most  on  and
   on . So we expect at most 
  wrong answers. This component takes  bits on
  average.
\end{enumerate*}
\noindent
This completes the proof of Lemma~\ref{lem:const-tu}, and our analysis
of update time .



\subsection{Subconstant Update Time}

The main challenge in the constant regime was that we couldn't
identify which cells were the ones in ; rather we could only
identify the queries that read them. Since  here, we will
be able to identify those cells among the cells read by queries
, so the footprints are no longer a bottleneck.

The main bottleneck becomes writing  to the encoding.
Following the trick of \cite{patrascu07bit}, our message will instead
contain  and the cache contents at the end of epoch
. We note that this allows the decoder to recover . Indeed,
the keys  are chosen by public coins, so the decoder can
simulate the data structure after the end of epoch . Whenever the
update algorithm wants to read a cell, the decoder checks the message
to see if the cell was written in epoch  (whether it is in ), and retrieves the contents if so.

We bound  by the following, which can be seen as a
strengthening of Observation~\ref{obs:wi}:
\begin{lemma}   \label{lem:riwi}
At least a quarter of the choices of 
are good and satisfy .
\end{lemma}

\begin{proof}
We will now choose  randomly, and calculate , where the expectation is over the random distribution
and random . We will show , from which the lemma follows by a
Markov bound and union bound (ruling out the 's that are not good).

A cell is included in  if it is read at some time 
that falls in epochs , and the last time it was
written is some  that falls in epoch . For fixed , let us
analyze the probability that this happens, over the random choice of
the query's position. There are two necessary and sufficient
conditions for the event to happen:
\begin{itemize*}
\item the boundary between epochs  and  must occur before ,
  so the query must appear before . Since the query must also appear after , the event
  is impossible unless .
  
\item the boundary between epochs  and  must occur in ,
  so there are at most  favorable choices for the query. Note
  also that the query must occur before , so
  there are at most  favorable choices.
\end{itemize*}
Let  be the smallest value such that .  Let us analyze the contribution of this
operation to  for various .  If
, the contribution is zero. For , we use
the bound  for the number of favorable choices. Thus,
the contribution is . For , we use the bound  for
the number of favorable choices. Thus, the contribution is
.

We conclude that the contribution is a geometric sum dominated by
. Overall, there are at most 
memory reads in expectation, so averaging over the choice of , we
obtain .
\end{proof}

Our main claim is:

\begin{lemma}   \label{lem:subconst-tu}
Let  be chosen as in
Lemma~\ref{lem:riwi} and . Let  be the probability
over  that the query reads a cell from . We can encode a vector of  bits with  ones by a
message of expected size: .
\end{lemma}

We first show how the lemma implies the desired lower bound. Let
 be such that the first term in the message size is . This requires setting .

Note that the lower bound is the same for  and, say, . Thus, we may assume that . Therefore .

Since the entropy is , we must have . For a small enough
, we obtain , so .

We have shown that a query over  reads a cell from  with constant probability, for at least a quarter of
the choices of . By linearity of expectation .


\paragraph{Proof of Lemma \ref{lem:subconst-tu}.}
The encoding will contain:
\begin{enumerate*}
\item The cache contents at the end of epoch , and right before the
  query.

\item The address and contents of cells in . The
  decoder can recover  by simulating the updates after epoch
  .

\item Let  be the set of negative queries
  that read at least one cell from . We encode
   as a subset of . 

\item For each cell , mark some positive
  query  such that  is the first cell from  that  reads. Note that distinct cells can only mark
  distinct queries, but some cells may not mark any query if no
  positive query reads them first among the set.  Let  be the set
  of marked queries, and encode  as a subset of .

\item For each , encode the number of cell probes before the
  first probed cell from , using 
  bits.

\item The subset of queries from  that return a wrong answer (Monte
  Carlo error).
\end{enumerate*}

\noindent
The decoder starts by simulating the queries , and stops at the
first cell from  that they read (this is
identified in part 5 of the encoding). The simulation cannot continue
because the decoder doesn't know these cells. Let  be the cells where this simulation stops.

The decoder knows that queries from  are positive and queries from
 are negative. It will simulate the other queries from . If a
query tries to read a cell from , the simulation is stopped
and the query is declared to be positive. Otherwise, the simulation is
run to completion.

We claim this simulation is correct. If a query is negative and it is
not in , it cannot read anything from , so
the simulation only uses known cells. If a query is positive but it is
not in , it means either: (1) it doesn't read any cell from  (so the simulation is correct); or (2) the first
cell from  that it reads is in ,
because it marked some other query in . By looking at component 6,
the decoder can correct wrong answers from simulated queries.

It remain to analyze the size of the encoding:
\begin{enumerate*}
\item The cache size is  bits.

\item By Lemma~\ref{lem:riwi}, , so this component
  takes  bits.

\item Since , this component takes  bits.

\item Since , this takes  bits.

\item This takes  bits.

\item We expect  wrong negative queries and  wrong positive queries, so this component takes  bits on average.
\end{enumerate*}

\noindent
This completes the proof of Lemma~\ref{lem:subconst-tu}, and our
lower bound.



\end{document}
