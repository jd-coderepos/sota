[{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Quora Question Pairs', 'Metric': 'Accuracy', 'Score': '90.1%'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Quora Question Pairs', 'Metric': 'Accuracy', 'Score': '89.8%'}}, {'LEADERBOARD': {'Task': 'Open-Domain Question Answering', 'Dataset': 'DuReader', 'Metric': 'EM', 'Score': '64.2'}}, {'LEADERBOARD': {'Task': 'Open-Domain Question Answering', 'Dataset': 'DuReader', 'Metric': 'EM', 'Score': '61.3'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'XNLI Chinese Dev', 'Metric': 'Accuracy', 'Score': '82.6'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'XNLI Chinese Dev', 'Metric': 'Accuracy', 'Score': '81.2'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'QNLI', 'Metric': 'Accuracy', 'Score': '94.6%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'QNLI', 'Metric': 'Accuracy', 'Score': '92.9%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'WNLI', 'Metric': 'Accuracy', 'Score': '67.8%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '80.2%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '74.8%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Matched', 'Score': '88.7'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Mismatched', 'Score': '88.8'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Matched', 'Score': '86.1'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Mismatched', 'Score': '85.5'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'Accuracy', 'Score': '87.4%'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'Accuracy', 'Score': '86.1%'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'STS Benchmark', 'Metric': 'Pearson Correlation', 'Score': '0.912'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'STS Benchmark', 'Metric': 'Pearson Correlation', 'Score': '0.876'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'SST-2 Binary classification', 'Metric': 'Accuracy', 'Score': '95'}}, {'LEADERBOARD': {'Task': 'Chinese Named Entity Recognition', 'Dataset': 'MSRA Dev', 'Metric': 'F1', 'Score': '96.3'}}, {'LEADERBOARD': {'Task': 'Chinese Named Entity Recognition', 'Dataset': 'MSRA Dev', 'Metric': 'F1', 'Score': '95.2'}}, {'LEADERBOARD': {'Task': 'Chinese Named Entity Recognition', 'Dataset': 'MSRA', 'Metric': 'F1', 'Score': '95'}}, {'LEADERBOARD': {'Task': 'Chinese Named Entity Recognition', 'Dataset': 'MSRA', 'Metric': 'F1', 'Score': '93.8'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA', 'Metric': 'Accuracy', 'Score': '63.5%'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA', 'Metric': 'Accuracy', 'Score': '55.2%'}}]
