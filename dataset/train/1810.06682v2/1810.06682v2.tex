\documentclass{article} \usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{assoccnt}
\usepackage{bbding}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{arydshln}
\usepackage{verbatim}
\usepackage{chngcntr}

\newcommand\mypara[1]{\vspace{0mm}\noindent\textbf{#1}}

\title{Trellis Networks for Sequence Modeling}


\author{Shaojie Bai \\
Carnegie Mellon University\\
\And
J. Zico Kolter \\
Carnegie Mellon University and \\
Bosch Center for AI\\
\And
Vladlen Koltun \\
Intel Labs \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\model}{TrellisNet}

\newcommand{\wikitextres}{29.19}
\newcommand{\charptbres}{1.158}
\newcommand{\wordptbressmall}{56.97}
\newcommand{\wordptbres}{56.80}
\newcommand{\wordptbmosressmall}{54.67}
\newcommand{\wordptbmosres}{54.19}
\newcommand\myeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily (trivially)}}}{=}}}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newenvironment{proofidx}[1]{\renewcommand{\proofname}{Proof of Theorem #1}\proof}{\endproof}
\newenvironment{claimproofidx}[1]{\renewcommand{\proofname}{Proof of Claim #1}\proof}{\endproof}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available \href{https://github.com/locuslab/trellisnet}{\color{violet}{here}}\footnote{\texttt{https://github.com/locuslab/trellisnet}}.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

What is the best architecture for sequence modeling? Recent research has produced significant progress on multiple fronts. Recurrent networks, such as LSTMs, continue to be optimized and extended~\citep{merityRegOpt,Melis2018,yang2018breaking,trinh2018learning}. Temporal convolutional networks have demonstrated impressive performance, particularly in modeling long-range context~\citep{waveNet,dauphinGatedConv,bai2018empirical}. And architectures based on self-attention are gaining ground~\citep{vaswani2017attention,santoro2018relational}.

In this paper, we introduce a new architecture for sequence modeling, the Trellis Network. We aim to both improve empirical performance on sequence modeling benchmarks and shed light on the relationship between two existing model families: recurrent and convolutional networks.

On the one hand, a trellis network is a special temporal convolutional network, distinguished by two unusual characteristics. First, the weights are tied across layers. That is, weights are shared not only by all time steps but also by all network layers, tying them into a regular trellis pattern. Second, the input is injected into all network layers. That is, the input at a given time-step is provided not only to the first layer, but directly to all layers in the network. So far, this may seem merely as a peculiar convolutional network for processing sequences, and not one that would be expected to perform particularly well.

Yet on the other hand, we show that trellis networks generalize truncated recurrent networks (recurrent networks with bounded memory horizon). The precise derivation of this connection is one of the key contributions of our work. It allows trellis networks to serve as bridge between recurrent and convolutional architectures, benefitting from algorithmic and architectural techniques developed in either context. We leverage these relationships to design high-performing trellis networks that absorb ideas from both architectural families. Beyond immediate empirical gains, these connections may serve as a step towards unification in sequence modeling.

We evaluate trellis networks on challenging benchmarks, including word-level language modeling on the standard Penn Treebank (PTB) and the much larger WikiText-103 (WT103) datasets; character-level language modeling on Penn Treebank; and standard stress tests (e.g.\ sequential MNIST, permuted MNIST, etc.) designed to evaluate long-term memory retention. On word-level Penn Treebank, a trellis network outperforms by more than a unit of perplexity the recent architecture search work of \cite{pham2018efficient}, as well as the recent results of \cite{Melis2018}, which leveraged the Google Vizier service for exhaustive hyperparameter search. On character-level Penn Treebank, a trellis network outperforms the thorough optimization work of \cite{merity2018analysis}. On word-level WikiText-103, a trellis network outperforms by 7.6\% in perplexity the contemporaneous self-attention-based Relational Memory Core \citep{santoro2018relational}, and by 11.5\% the work of \cite{merity2018analysis}. (Concurrently with our work, \cite{dai2018transformer} employ a transformer and achieve even better results on WikiText-103.) On stress tests, trellis networks outperform recent results achieved by recurrent networks and self-attention \citep{trinh2018learning}. It is notable that the prior state of the art across these benchmarks was held by models with sometimes dramatic mutual differences.



\section{Background}
\label{sec:related-works}

Recurrent networks~\citep{Elman90findstructure,Werbos1990,Graves2012}, particularly with gated cells such as LSTMs~\citep{hochreiterLSTM} and GRUs~\citep{choGRU}, are perhaps the most popular architecture for modeling temporal sequences. Recurrent architectures have been used to achieve breakthrough results in natural language processing and other domains \citep{Sutskever2011,graves2013generating,sutskeverSeqToSeq,Bahdanau2015,Vinyals2015,Karpathy2015}.
Convolutional networks have also been widely used for sequence processing \citep{waibel,Collobert2011}. Recent work indicates that convolutional networks are effective on a variety of sequence modeling tasks, particularly ones that demand long-range information propagation \citep{waveNet,kalchbrenner2016neural,dauphinGatedConv,gehring2017convolutional,bai2018empirical}.
A third notable approach to sequence processing that has recently gained ground is based on self-attention \citep{vaswani2017attention,santoro2018relational,chen2018best}. Our work is most closely related to the first two approaches. In particular, we establish a strong connection between recurrent and convolutional networks and introduce a model that serves as a bridge between the two. A related recent theoretical investigation showed that under a certain stability condition, recurrent networks can be well-approximated by feed-forward models \citep{miller2018recurrent}.

There have been many combinations of convolutional and recurrent networks \citep{sainath2015convolutional}. For example, convolutional LSTMs combine convolutional and recurrent units \citep{Donahue2015,Venugopalan2015,xingjian2015convolutional}. Quasi-recurrent neural networks interleave convolutional and recurrent layers \citep{bradbury2016quasi}. Techniques introduced for convolutional networks, such as dilation, have been applied to RNNs \citep{chang2017dilated}. Our work establishes a deeper connection, deriving a direct mapping across the two architectural families and providing a structural bridge that can incorporate techniques from both sides.


\section{Sequence Modeling and Trellis Networks}
\label{sec:trellisnet}

\mypara{Sequence modeling.}
Given an input $x_{1:T} = x_1, \dots, x_T$ with sequence length $T$, a sequence model is any function ${G: \mathcal{X}^T \rightarrow \mathcal{Y}^T}$ such that
\begin{equation}
y_{1:T} = y_1, \dots, y_T = G(x_1, \dots, x_T),
\end{equation}
where $y_t$ should only depend on $x_{1:t}$ and not on $x_{t+1:T}$ (i.e.\ no leakage of information from the future). This causality constraint is essential for autoregressive modeling.

In this section, we describe a new architecture for sequence modeling, referred to as a trellis network or TrellisNet. In particular, we provide an atomic view of \model, present its fundamental features, and highlight the relationship to convolutional networks. Section \ref{sec:rnn-tcn-relationship} will then elaborate on the relationship of trellis networks to convolutional and recurrent models.

\mypara{Notation.}
We use $x_{1:T}=(x_1, \dots, x_T)$ to denote a length-$T$ input sequence, where vector $x_t \in \mathbb{R}^p$ is the input at time step $t$. Thus $x_{1:T} \in \mathbb{R}^{T \times p}$. We use \small$z_t^{(i)} \in \mathbb{R}^q$\normalsize\ to represent the hidden unit at time $t$ in layer $i$ of the network. We use $\text{Conv1D}(x; W)$ to denote a 1D convolution with a kernel $W$ applied to input $x=x_{1:T}$.


\begin{figure}[t]
    \vspace{-.2in}
    \centering
    \begin{subfigure}[b]{.33\textwidth}
        \centering
        \includegraphics[width=.98\textwidth]{images/architecture/trellis-atom}
        \caption{\model~at an atomic level}
        \label{fig:trellis-atom}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.59\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/architecture/trellis-general}
        \caption{\model~on a sequence of units}
        \label{fig:trellis-general}
    \end{subfigure}
    \vspace{-1mm}
    \caption{The interlayer transformation of \model, at an atomic level (time steps $t$ and $t+1$, layers $i$ and $i+1$) and on a longer sequence (time steps $1$ to $8$, layers $i$ and $i+1$).}
    \label{fig:generic-trellis-network}
    \vspace{-3mm}
\end{figure}

\mypara{A basic trellis network.}
At the most basic level, a feature vector $z_{t+1}^{(i+1)}$ at time step $t+1$ and level $i+1$ of TrellisNet is computed via three steps, illustrated in Figure \ref{fig:trellis-atom}:
\vspace{-3mm}
\begin{enumerate}[labelsep=*,leftmargin=1pc]
    \item
    The hidden input comprises the hidden outputs $z_t^{(i)}, z_{t+1}^{(i)} \in \mathbb{R}^q$ from the previous layer $i$, as well as an injection of the input vectors $x_t, x_{t+1}$.
    At level $0$, we initialize to $z_t^{(0)} = \mathbf{0}$.
    \item
    A pre-activation output $\hat{z}_{t+1}^{(i+1)} \in \mathbb{R}^r$ is produced by a feed-forward linear transformation:
    \begin{equation}
    \label{eq:trellis-atom}
    \hat{z}_{t+1}^{(i+1)} = W_1 \begin{bmatrix}x_t \\ z_t^{(i)}\end{bmatrix} + W_2 \begin{bmatrix}x_{t+1} \\ z_{t+1}^{(i)}\end{bmatrix},
    \end{equation}
    where $W_1, W_2 \in \mathbb{R}^{r \times (p+q)}$ are weights, and $r$ is the size of the pre-activation output $\hat{z}_{t+1}^{(i+1)}$. (Here and throughout the paper, all linear transformations can include additive biases. We omit these for clarity.)
    \item
    The output \small$z_{t+1}^{(i+1)}$\normalsize~is produced by a nonlinear activation function $f: \mathbb{R}^{r} \times \mathbb{R}^q \rightarrow \mathbb{R}^q$ applied to the pre-activation output \small$\hat{z}_{t+1}^{(i+1)}$\normalsize~and the output \small$z_t^{(i)}$\normalsize~from the previous layer. More formally, \small$z_{t+1}^{(i+1)} = f\left(\hat{z}_{t+1}^{(i+1)}, z_t^{(i)}\right)$\normalsize.
\end{enumerate}

A full trellis network can be built by tiling this elementary procedure across time and depth. Given an input sequence $x_{1:T}$, we apply the same production procedure across all time steps and all layers, using the same weights. The transformation is the same for all elements in the temporal dimension and in the depth dimension. This is illustrated in Figure~\ref{fig:trellis-general}. Note that since we inject the same input sequence at every layer of the \model, we can precompute the linear transformation \small${\tilde{x}_{t+1} = W_1^xx_t + W_2^xx_{t+1}}$\normalsize~for all layers $i$. This identical linear combination of the input can then be added in each layer $i$ to the appropriate linear combination of the hidden units, \small$W_1^zz_t^{(i)} + W_2^zz_{t+1}^{(i)}$\normalsize, where \small$W_j^x \in \mathbb{R}^{r \times p}, W_j^z \in \mathbb{R}^{r \times q}$\normalsize.

Now observe that in each level of the network, we are in effect performing a 1D convolution over the hidden units $z_{1:T}^{(i)}$. The output of this convolution is then passed through the activation function $f$. Formally, with $W \in \mathbb{R}^{r \times q}$ as the kernel weight matrix, the computation in layer $i$ can be summarized as follows (Figure \ref{fig:trellis-general}):
\begin{equation}
\label{eq:trellis-general}
\hat{z}_{1:T}^{(i+1)} = \text{Conv1D}\left(z_{1:T}^{(i)}; W\right) + \tilde{x}_{1:T}, \qquad
z_{1:T}^{(i+1)} = f\left(\hat{z}_{1:T}^{(i+1)}, z_{1:T-1}^{(i)}\right).
\end{equation}
The resulting network operates in feed-forward fashion, with deeper elements having progressively larger receptive fields. There are, however, important differences from typical (temporal) convolutional networks. Notably, the filter matrix is shared across all layers. That is, the weights are tied not only across time but also across depth. (\cite{vogel2017primal} have previously tied weights across depth in image processing.) Another difference is that the transformed input sequence $\tilde{x}_{1:T}$ is directly injected into each hidden layer. These differences and their importance will be analyzed further in Section \ref{sec:rnn-tcn-relationship}.

The activation function $f$ in Equation (\ref{eq:trellis-general}) can be any nonlinearity that processes the pre-activation output \small$\hat{z}_{1:T}^{(i+1)}$\normalsize and the output from the previous layer \small$z_{1:T-1}^{(i)}$\normalsize. We will later describe an activation function based on the LSTM cell. The rationale for its use will become clearer in light of the analysis presented in the next section.

\section{TrellisNet, TCN, and RNN}
\label{sec:rnn-tcn-relationship}

In this section, we analyze the relationships between trellis networks, convolutional networks, and recurrent networks.
In particular, we show that trellis networks can serve as a bridge between convolutional and recurrent networks. On the one hand, TrellisNet is a special form of temporal convolutional networks (TCN); this has already been clear in Section~\ref{sec:trellisnet} and will be discussed further in Section~\ref{subsec:trellis-tcn}. On the other hand, any truncated RNN can be represented as a TrellisNet with special structure in the interlayer transformations; this will be the subject of Section~\ref{subsec:trellis-rnn}. These connections allow TrellisNet to harness architectural elements and regularization techniques from both TCNs and RNNs; this will be summarized in Section~\ref{subsec:optimize-trellis}.

\subsection{\model~and TCN}
\label{subsec:trellis-tcn}

We briefly introduce TCNs here, and refer the readers to~\cite{bai2018empirical} for a more thorough discussion. Briefly, a temporal convolutional network (TCN) is a ConvNet that uses one-dimensional convolutions over the sequence. The convolutions are \emph{causal}, meaning that, at each layer, the transformation at time $t$ can only depend on previous layer units at times $t$ or earlier, not from later points in time. Such approaches were used going back to the late 1980s, under the name of ``time-delay neural networks"~\citep{waibel}, and have received significant interest in recent years due to their application in architectures such as WaveNet~\citep{waveNet}.

In essence, \model~is a special kind of temporal convolutional network. TCNs have two distinctive characteristics: 1) causal convolution in each layer to satisfy the causality constraint and 2) deep stacking of layers to increase the effective history length (i.e.\ receptive field). Trellis networks have both of these characteristics. The basic model presented in Section~\ref{sec:trellisnet} can easily be elaborated with larger kernel sizes, dilated convolutions, and other architectural elements used in TCNs; some of these are reviewed further in Section~\ref{subsec:optimize-trellis}.

However, TrellisNet is not a general TCN. As mentioned in Section~\ref{sec:trellisnet}, two important differences are: 1) the weights are tied across layers and 2) the linearly transformed input $\tilde{x}_{1:T}$ is injected into each layer. Weight tying can be viewed as a form of regularization that can stabilize training, support generalization, and significantly reduce the size of the model. Input injection mixes deep features with the original sequence. These structural characteristics will be further illuminated by the connection between trellis networks and recurrent networks, presented next.

\subsection{\model~and RNN}
\label{subsec:trellis-rnn}

Recurrent networks appear fundamentally different from convolutional networks. Instead of operating on all elements of a sequence in parallel in each layer, an RNN processes one input element at a time and unrolls in the time dimension. Given a non-linearity $g$ (which could be a sigmoid or a more elaborate cell), we can summarize the transformations in an $L$-layer RNN at time-step $t$ as follows:
\begin{equation}
\label{eq:rnn-transformation}
h_t^{(i)} = g\left(W_{hx}^{(i)} h_t^{(i-1)} + W_{hh}^{(i)} h_{t-1}^{(i)}\right) \quad \text{for } 1 \leq i \leq L, \qquad h_t^{(0)} = x_t.
\end{equation}
Despite the apparent differences, we will now show that any RNN unrolled to a finite length is equivalent to a TrellisNet with special sparsity structure in the kernel matrix $W$.
We begin by formally defining the notion of a truncated (i.e.\ finite-horizon) RNN.

\begin{definition}
\label{dfn:unrolled-rnn}
Given an RNN $\rho$, a corresponding \emph{\textbf{$\mathbf{M}$-truncated RNN}} $\rho^M$, applied to the sequence $x_{1:T}$, produces at time step $t$ the output $y_t$ by applying $\rho$ to the sequence $x_{t-M+1:t}$ (here $x_{<0} = 0$).
\end{definition}
\vspace{.05in}

\begin{theorem}
\label{thm:trellisnet-rnn}
Let $\rho^M$ be an $M$-truncated RNN with $L$ layers and hidden unit dimensionality $d$. Then there exists an equivalent TrellisNet $\tau$ with depth \small$(M+L-1)$\normalsize\ and layer width (i.e.\ number of channels in each hidden layer) $Ld$. Specifically, for any $x_{1:T}$, $\rho^M(x_{1:T}) = \tau_{L(d-1)+1: Ld}(x_{1:T})$ (i.e.\ the TrellisNet outputs contain the RNN outputs).
\end{theorem}

Theorem~\ref{thm:trellisnet-rnn} states that any $M$-truncated RNN can be represented as a TrellisNet. How severe of a restriction is $M$-truncation? Note that $M$-truncation is intimately related to truncated backpropagation-through-time (BPTT), used pervasively in training recurrent networks on long sequences. While RNNs can in principle retain unlimited history, there is both empirical and theoretical evidence that the memory horizon of RNNs is bounded~\citep{bai2018empirical,khandelwal2018sharp,miller2018recurrent}.
Furthermore, if desired, TrellisNets can recover exactly a common method of applying RNNs to long sequences~-- hidden state repackaging, i.e.\ copying the hidden state across subsequences. This is accomplished using an analogous form of hidden state repackaging, detailed in Appendix~\ref{appendix:optimize-trellis}.

\vspace{-.1in}
\begin{proofidx}{\ref{thm:trellisnet-rnn}}
Let $h_{t,t'}^{(i)} \in \mathbb{R}^d$ be the hidden state at time $t$ and layer $i$ of the truncated RNN $\rho^{t-t'+1}$ (i.e., the RNN begun at time $t'$ and run until time $t$). Note that without truncation, history starts at time $t'=1$, so the hidden state $h_t^{(i)}$ of $\rho$ can be equivalently expressed as $h_{t,1}^{(i)}$. When $t' > t$, we define \small$h_{t,t'}=0$\normalsize~(i.e.\ no history information if the clock starts in the future).

By assumption, $\rho^M$ is an RNN defined by the following parameters: $\{W_{hx}^{(i)}, W_{hh}^{(i)}, g, M\}$, where $W_{hh}^{(i)} \in \mathbb{R}^{w \times d}$ for all $i$, $W_{hx}^{(1)} \in \mathbb{R}^{w \times p}$, and $W_{hx}^{(i)} \in \mathbb{R}^{w \times d}$ for all $i=2,\dots,L$ are the weight matrices at each layer ($w$ is the dimension of pre-activation output). We now construct a \model~$\tau$ according to the exact definition in Section \ref{sec:trellisnet}, with parameters $\{W_1, W_2, f\}$, where
\begin{equation}
\label{eq:sparse-kernel}
\resizebox{.8\hsize}{!}{
$
W_1 = \left[
\begin{array}{ccccc}
0 & W_{hh}^{(1)} & 0 & \ldots & 0 \\
0 & 0 & W_{hh}^{(2)} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & W_{hh}^{(L)}
\end{array}
\right],
W_2 = \left[
\begin{array}{ccccc}
W_{hx}^{(1)} & 0 & \ldots & 0 & 0 \\
0 & W_{hx}^{(2)} & \ldots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & W_{hx}^{(L)} & 0
\end{array}
\right],
$}
\end{equation}
such that $W_1, W_2 \in \mathbb{R}^{Lw \times (p+Ld)}$. We define a nonlinearity $f$ by $f(\alpha, \beta) = g(\alpha)$ (i.e.\ applying $g$ only on the first entry).

Let $t \in [T]$ , $j \geq 0$ be arbitrary and fixed. We now claim that the hidden unit at time $t$ and layer $j$ of \model~$\tau$ can be expressed in terms of hidden units at time $t$ in truncated forms of $\rho$:
\begin{equation}
\label{pf-eq:rnn-tcn-mix}
z_t^{(j)} = \begin{bmatrix}
h_{t,t-j+1}^{(1)} & h_{t,t-j+2}^{(2)} & \dots & h_{t,t-j+L}^{(L)}
\end{bmatrix}^\top \in \mathbb{R}^{Ld},
\end{equation}
where $z_t^{(j)}$ is the time-$t$ hidden state at layer $j$ of $\tau$ and $h_{t,t'}^{(i)}$ is the time-$t$ hidden state at layer $i$ of $\rho^{t-t'+1}$.

We prove Eq.~(\ref{pf-eq:rnn-tcn-mix}) by induction on $j$. As a base case, consider $j=0$; i.e.\ the input layer of $\tau$. Since $h_{t,t'}=0$ when $t'>t$, we have that \small$z_j^{(0)} = [0 \ \ 0 \ \ \dots \ \ 0]^\top$\normalsize. (Recall that in the input layer of \model~we initialize \small$z_t^{(0)}=\mathbf{0}$\normalsize.) For the inductive step, suppose Eq.~(\ref{pf-eq:rnn-tcn-mix}) holds for layer $j$, and consider layer $j+1$. By the feed-forward transformation of \model~defined in Eq.~(\ref{eq:trellis-atom}) and the nonlinearity $f$ we defined above, we have:
\begin{align}
\hat{z}_t^{(j+1)} &= W_1 \begin{bmatrix}x_{t-1} \\ z_{t-1}^{(j)}\end{bmatrix} + W_2 \begin{bmatrix}x_t \\ z_t^{(j)}\end{bmatrix} \\
&=
\resizebox{.85\hsize}{!}{
$
\left[
\begin{array}{ccccc}
0 & W_{hh}^{(1)} & 0 & \ldots & 0 \\
0 & 0 & W_{hh}^{(2)} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & W_{hh}^{(L)}
\end{array}
\right] \begin{bmatrix} x_{t-1} \\ h_{t-1, t-j}^{(1)} \\ \vdots \\ h_{t-1, t-j+L-1}^{(L)} \end{bmatrix} +
\left[
\begin{array}{ccccc}
W_{hx}^{(1)} & 0 & \ldots & 0 & 0 \\
0 & W_{hx}^{(2)} & \ldots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & W_{hx}^{(L)} & 0
\end{array}
\right] \begin{bmatrix} x_t \\ h_{t, t-j+1}^{(1)} \\ \vdots \\ h_{t, t-j+L}^{(L)} \end{bmatrix}
$} \\
&=
\begin{bmatrix} W_{hh}^{(1)} h_{t-1,t-j}^{(1)} + W_{hx}^{(1)}x_t \\ \vdots \\ W_{hh}^{(L)} h_{t-1,t-j+L-1}^{(L)} + W_{hx}^{(L)} h_{t, t-j+L-1}^{(L-1)} \end{bmatrix}
\\
z_t^{(j+1)} &= f(\hat{z}_t^{(j+1)}, z_{t-1}^{(j)}) = g(\hat{z}_t^{(j+1)}) = \begin{bmatrix} h_{t, t-j}^{(1)} & h_{t, t-j+1}^{(2)} & \dots & h_{t, t-j+L-1}^{(L)} \end{bmatrix}^\top \label{pf-eq:apply-rnn-defn}
\end{align}
where in Eq. (\ref{pf-eq:apply-rnn-defn}) we apply the RNN non-linearity $g$ following Eq.~(\ref{eq:rnn-transformation}). Therefore, by induction, we have shown that Eq.~(\ref{pf-eq:rnn-tcn-mix}) holds for all $j \geq 0$.

If \model~$\tau$ has $M+L-1$ layers, then at the final layer we have \small$z_t^{(M+L-1)} = [\dots \ \ \dots \ \ h_{t, t+1-M}^{(L)}]^\top$\normalsize. Since $\rho^M$ is an $L$-layer $M$-truncated RNN, this (taking the last $d$ channels of $z_t^{(M+L-1)}$) is exactly the output of $\rho^M$ at time $t$.

In other words, we have shown that $\rho^M$ is equivalent to a \model~with sparse kernel matrices $W_1, W_2$. This completes the proof.
\end{proofidx}

Note that the convolutions in the TrellisNet $\tau$ constructed in Theorem~\ref{thm:trellisnet-rnn} are sparse, as shown in Eq.~(\ref{eq:sparse-kernel}). They are related to group convolutions~\citep{krizhevsky2012imagenet}, but have an unusual form because group $k$ at time $t$ is convolved with group $k-1$ at time $t+1$. We refer to these as mixed group convolutions. Moreover, while Theorem~\ref{thm:trellisnet-rnn} assumes that all layers of $\rho^M$ have the same dimensionality $d$ for clarity, the proof easily generalizes to cases where each layer has different widths.

For didactic purposes, we recap and illustrate the construction in the case of a 2-layer RNN. The key challenge is that a na\"ive unrolling of the RNN into a feed-forward network does not produce a convolutional network, since the linear transformation weights are not constant across a layer. The solution, illustrated in Figure \ref{fig:unrolled-RNN-groups}, is to organize each hidden unit into groups of channels, such that each TrellisNet unit represents 3 RNN units simultaneously (for \small$x_t, h_t^{(1)}, h_t^{(2)}$\normalsize). Each TrellisNet unit thus has $(p+2d)$ channels. The interlayer transformation can then be expressed as a mixed group convolution, illustrated in Figure~\ref{fig:rnn-tcn-mixed-group}. This can be represented as a sparse convolution with the structure given in Eq.~(\ref{eq:sparse-kernel}) (with $L=2$). Applying the nonlinearity $g$ on the pre-activation output, this exactly reproduces the transformations in the original 2-layer RNN.

The TrellisNet that emerges from this construction has special sparsity structure in the weight matrix. It stands to reason that a general TrellisNet with an unconstrained (dense) weight matrix $W$ may have greater expressive power: it can model a broader class of transformations than the original RNN $\rho^M$. Note that while the hidden channels of the TrellisNet $\tau$ constructed in the proof of Theorem~\ref{thm:trellisnet-rnn} are naturally arranged into groups that represent different layers of the RNN $\rho^M$ (Eq. (\ref{pf-eq:rnn-tcn-mix})), an unconstrained dense weight matrix $W$ no longer admits such an interpretation. A model defined by a dense weight matrix is fundamentally distinct from the RNN $\rho^M$ that served as our point of departure. We take advantage of this expressivity and use general weight matrices $W$, as presented in Section~\ref{sec:trellisnet}, in our experiments. Our ablation analysis will show that such generalized dense transformations are beneficial, even when model capacity is controlled for.

The proof of Theorem~\ref{thm:trellisnet-rnn} did not delve into the inner structure of the nonlinear transformation $g$ in RNN (or $f$ in the constructed TrellisNet). For a vanilla RNN, for instance, $f$ is usually an elementwise sigmoid or $\tanh$ function. But the construction in Theorem \ref{thm:trellisnet-rnn} applies just as well to RNNs with structured cells, such as LSTMs and GRUs. We adopt LSTM cells for the TrellisNets in our experiments and provide a detailed treatment of this nonlinearity in Section~\ref{subsec:lstm-trellis} and Appendix~\ref{appendix:trellisnet-gated-activation}.


\begin{figure}[t]
    \vspace{-.4in}
    \centering
    \begin{subfigure}[b]{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/architecture/group-unroll-vanilla-rnn}
        \vspace{-.23in}
        \caption{Representing RNN units as channel groups}
        \label{fig:unrolled-RNN-groups}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.42\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/architecture/rnn-tcn-mixed-group}
        \vspace{-.21in}
        \caption{Mixed group convolution}
        \label{fig:rnn-tcn-mixed-group}
    \end{subfigure}
    \vspace{-1.5mm}
    \caption{Representing a truncated 2-layer RNN $\rho^M$ as a trellis network $\tau$. {\bf (a)} Each unit of $\tau$ has three groups, which house the input, first-layer hidden vector, and second-layer hidden vector of $\rho^M$, respectively. {\bf (b)} Each group in the hidden unit of $\tau$ in level $i+1$ at time step $t+1$ is computed by a linear combination of appropriate groups of hidden units in level $i$ at time steps $t$ and $t+1$. The linear transformations form a mixed group convolution that reproduces computation in $\rho^M$. (Nonlinearities not shown for clarity.)}
    \vspace{-5mm}
\end{figure}


\subsection{TrellisNet as a Bridge Between Recurrent and Convolutional Models}
\label{subsec:optimize-trellis}

In Section~\ref{subsec:trellis-tcn} we concluded that TrellisNet is a special kind of TCN, characterized by weight tying and input injection. In Section~\ref{subsec:trellis-rnn} we established that TrellisNet is a generalization of truncated RNNs. These connections along with the construction in our proof of Theorem~\ref{thm:trellisnet-rnn} allow TrellisNets to benefit significantly from techniques developed originally for RNNs, while also incorporating architectural and algorithmic motifs developed for convolutional networks.
We summarize a number of techniques here. From recurrent networks, we can integrate
1) structured nonlinear activations (e.g.\ LSTM and GRU gates);
2) variational RNN dropout~\citep{gal2016dropout};
3) recurrent DropConnect~\citep{merityRegOpt}; and
4) history compression and repackaging.
From convolutional networks, we can adapt
1) larger kernels and dilated convolutions \citep{dilatedConv};
2) auxiliary losses at intermediate layers \citep{lee2015deeply,xie2015holistically};
3) weight normalization~\citep{Salimans2016}; and
4) parallel convolutional processing.
Being able to directly incorporate techniques from both streams of research is one of the benefits of trellis networks. We leverage this in our experiments and provide a more comprehensive treatment of these adaptations in Appendix~\ref{appendix:optimize-trellis}.




\section{Experiments}
\label{sec:experiments}
\vspace{-.05in}

\subsection{A \model~with Gated Activation}
\label{subsec:lstm-trellis}

\begin{wrapfigure}[14]{l}{0.32\textwidth}
\vspace{-3mm}
\centering
    \includegraphics[width=.29\textwidth]{images/architecture/lstm-trellis-atom}
\vspace{-3mm}
    \caption{A gated activation based on the LSTM cell.}
    \label{fig:lstm-trellis-atom}
    \vspace{-3mm}
\end{wrapfigure}
In our description of generic trellis networks in Section~\ref{sec:trellisnet}, the activation function $f$ can be any nonlinearity that computes \small$z_{1:T}^{(i+1)}$\normalsize~based on \small$\hat{z}_{1:T}^{(i+1)}$\normalsize~and \small$z_{1:T-1}^{(i)}$\normalsize. In experiments, we use a gated activation based on the LSTM cell.
Gated activations have been used before in convolutional networks for sequence modeling~\citep{waveNet,dauphinGatedConv}.
Our choice is inspired directly by Theorem~\ref{thm:trellisnet-rnn}, which suggests incorporating an existing RNN cell into TrellisNet. We use the LSTM cell due to its effectiveness in recurrent networks~\citep{jozefowicz2015empirical,greffOdyssey,Melis2018}. We summarize the construction here; a more detailed treatment can be found in Appendix~\ref{appendix:trellisnet-gated-activation}.

In an LSTM cell, three information-controlling gates are computed at time $t$. Moreover, there is a cell state that does not participate in the hidden-to-hidden transformations but is updated in every step using the result from the gated activations. We integrate the LSTM cell into the TrellisNet as follows (Figure~\ref{fig:lstm-trellis-atom}):
\begin{align}
\hat{z}_{t+1}^{(i+1)} &= W_1 \begin{bmatrix}x_t \\ z_{t,2}^{(i)}\end{bmatrix} + W_2 \begin{bmatrix}x_{t+1} \\ z_{t+1,2}^{(i)}\end{bmatrix} = \begin{bmatrix}\hat{z}_{t+1,1} & \hat{z}_{t+1,2} & \hat{z}_{t+1,3} & \hat{z}_{t+1,4} \end{bmatrix}^\top \\
\begin{split}
z_{t+1, 1}^{(i+1)} &= \sigma(\hat{z}_{t+1,1}) \circ z_{t, 1}^{(i)} + \sigma(\hat{z}_{t+1, 2}) \circ \tanh(\hat{z}_{t+1, 3}) \\
z_{t+1, 2}^{(i+1)} &= \sigma(\hat{z}_{t+1,4}) \circ \tanh(z_{t+1, 1}^{(i+1)})
\end{split} \label{eq:lstm-gates} \tag{\number\value{equation}; Gated activation $f$}
\stepcounter{equation}
\end{align}
Thus the linear transformation in each layer of the \model~produces a pre-activation feature $\hat{z}_{t+1}$ with $r=4q$ feature channels, which are then processed by elementwise transformations and Hadamard products to yield the final output \small$z_{t+1}^{(i+1)} = \left(z_{t+1,1}^{(i+1)}, z_{t+1,2}^{(i+1)}\right)$\normalsize~of the layer.
\subsection{Results}

We evaluate trellis networks on word-level and character-level language modeling on the standard Penn Treebank (PTB) dataset \citep{Marcus93buildinga,Mikolov2010PTB}, large-scale word-level modeling on WikiText-103 (WT103) \citep{merity2016pointer}, and standard stress tests used to study long-range information propagation in sequence models: sequential MNIST, permuted MNIST (PMNIST), and sequential CIFAR-10 \citep{chang2017dilated,bai2018empirical,trinh2018learning}. Note that these tasks are on very different scales, with unique properties that challenge sequence models in different ways. For example, word-level PTB is a small dataset that a typical model easily overfits, so judicious regularization is essential. WT103 is a hundred times larger, with less danger of overfitting, but with a vocabulary size of 268K that makes training more challenging (and precludes the application of techniques such as mixture of softmaxes~\citep{yang2018breaking}). A more complete description of these tasks and their characteristics can be found in Appendix~\ref{appendix:task-description}.

The prior state of the art on these tasks was set by completely different models, such as AWD-LSTM on character-level PTB~\citep{merity2018analysis}, neural architecture search on word-level PTB~\citep{pham2018efficient}, and the self-attention-based Relational Memory Core on \mbox{WikiText-103}~\citep{santoro2018relational}. We use trellis networks on all tasks and outperform the respective state-of-the-art models on each. For example, on word-level Penn Treebank, TrellisNet outperforms by a good margin the recent results of \cite{Melis2018}, which used the Google Vizier service for exhaustive hyperparameter tuning, as well as the recent neural architecture search work of \cite{pham2018efficient}. On WikiText-103, a trellis network outperforms by 7.6\% the Relational Memory Core \citep{santoro2018relational} and by 11.5\% the thorough optimization work of \cite{merity2018analysis}.


Many hyperparameters we use are adapted directly from prior work on recurrent networks. (As highlighted in Section~\ref{subsec:optimize-trellis}, many techniques can be carried over directly from RNNs.) For others, we perform a basic grid search. We decay the learning rate by a fixed factor once validation error plateaus. All hyperparameters are reported in Appendix~\ref{appendix:hyperparameters}, along with an ablation study.


\begin{table*}
\caption{Test perplexities (ppl) on word-level language modeling with the PTB corpus. ${}^\ell$ means lower is better.}
\label{table:word-ptb}
\centering
\def\arraystretch{1.1}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{Word-level Penn Treebank (PTB)} \\
\cline{1-3}
Model & Size & Test perplexity${}^\ell$ \\
\midrule
Generic TCN~\citep{bai2018empirical}        & 13M & 88.68 \\
Variational LSTM~\citep{gal2016dropout}     & 66M & 73.4 \\
NAS Cell~\citep{zoph2017neural}             & 54M & 62.4 \\
AWD-LSTM~\citep{merityRegOpt}               & 24M & 58.8 \\
(Black-box tuned) NAS~\citep{Melis2018}     & 24M & 59.7 \\
(Black-box tuned) LSTM + skip conn.~\citep{Melis2018} & 24M & 58.3 \\
AWD-LSTM-MoC~\citep{yang2018breaking}       & 22M & 57.55 \\
DARTS~\citep{liu2018darts}                  & 23M & 56.10 \\
AWD-LSTM-MoS~\citep{yang2018breaking}       & 24M & 55.97 \\
ENAS~\citep{pham2018efficient}              & 24M & 55.80 \\
\midrule
Ours - \model                               & 24M & \wordptbressmall \\
Ours - \model~(1.4x larger)                 & 33M & \wordptbres \\
Ours - \model-MoS                          & 25M & \wordptbmosressmall \\
\textbf{Ours - \model-MoS (1.4x larger)}    & 34M & \textbf{\wordptbmosres} \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}
\caption{Test perplexities (ppl) on word-level language modeling with the WT103 corpus.}
\label{table:word-wt103}
\centering
\def\arraystretch{1.1}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{Word-level WikiText-103 (WT103)} \\
\cline{1-3}
Model & Size & Test perplexity${}^\ell$ \\
\midrule
LSTM~\citep{grave2016improving}               & - & 48.7 \\
LSTM+continuous cache~\citep{grave2016improving}   & - & 40.8 \\
Generic TCN~\citep{bai2018empirical}          & 150M & 45.2 \\
Gated Linear ConvNet~\citep{dauphinGatedConv} & 230M & 37.2 \\
AWD-QRNN~\citep{merity2018analysis}           & 159M & 33.0 \\
Relational Memory Core~\citep{santoro2018relational} & 195M & 31.6 \\
\textbf{Ours - \model}                        & 180M & \textbf{\wikitextres}  \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table*}

\begin{table*}[t]
\caption{Test bits-per-character (bpc) on character-level language modeling with the PTB corpus.}
\label{table:char-ptb}
\centering
\def\arraystretch{1.1}
\resizebox{0.61\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{Char-level PTB} \\
\cline{1-3}
Model & Size & Test bpc${}^\ell$ \\
\midrule
Generic TCN~\citep{bai2018empirical}        & 3.0M & 1.31 \\
Independently RNN~\citep{li2018independently}          & 12.0M & 1.23 \\
Hyper LSTM~\citep{ha2016hypernetworks}      & 14.4M & 1.219 \\
NAS Cell~\citep{zoph2017neural}             & 16.3M & 1.214 \\
Fast-Slow-LSTM-2~\citep{mujika2017fast}            & 7.2M  & 1.19 \\
Quasi-RNN~\citep{merity2018analysis}        & 13.8M & 1.187 \\
AWD-LSTM~\citep{merity2018analysis}         & 13.8M & 1.175 \\
\textbf{Ours - \model}                      & 13.4M & \textbf{\charptbres} \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[t]
\def\arraystretch{1.15}
\small
\centering
\caption{Test accuracies on long-range modeling benchmarks. ${}^h$ means higher is better.}
\label{table:longer-dependency}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{Model} & Seq. MNIST & Permuted MNIST & Seq. CIFAR-10 \\
 & Test acc.${}^h$ & Test acc.${}^h$ & Test acc.${}^h$ \\
\midrule
Dilated GRU~\citep{chang2017dilated}              &  99.0  &  94.6  &  -   \\
IndRNN~\citep{li2018independently}                &  99.0  &  96.0  &  -   \\
Generic TCN~\citep{bai2018empirical}			  &  99.0  &  97.2  &  -   \\
$r$-LSTM w/ Aux. Loss~\citep{trinh2018learning}   &  98.4  &  95.2  & 72.2 \\
Transformer (self-attention)~\citep{trinh2018learning}             &  98.9  &  97.9  & 62.2 \\
\textbf{Ours - \model}                            & \textbf{99.20} & \textbf{98.13} & \textbf{73.42} \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table*}


\mypara{Word-level language modeling.}
For word-level language modeling, we use PTB and WT103. The results on PTB are listed in Table~\ref{table:word-ptb}. TrellisNet sets a new state of the art on PTB, both with and without mixture of softmaxes \citep{yang2018breaking}, outperforming all previously published results by more than one unit of perplexity.

WT103 is 110 times larger than PTB, with vocabulary size 268K. We follow prior work and use the adaptive softmax~\citep{grave2016efficient}, which improves memory efficiency by assigning higher capacity to more frequent words. The results are listed in Table~\ref{table:word-wt103}. TrellisNet sets a new state of the art on this dataset as well, with perplexity \wikitextres: about 7.6\% better than the contemporaneous self-attention-based Relational Memory Core (RMC) \citep{santoro2018relational}. TrellisNet achieves this better accuracy with much faster convergence: 25 epochs, versus 90 for RMC.


\mypara{Character-level language modeling.}
When used for character-level modeling, PTB is a medium-scale dataset with stronger long-term dependencies between characters. We thus use a deeper network as well as techniques such as weight normalization~\citep{Salimans2016} and deep supervision \citep{lee2015deeply,xie2015holistically}. The results are listed in Table~\ref{table:char-ptb}. TrellisNet sets a new state of the art with \charptbres~bpc, outperforming the recent results of \cite{merity2018analysis} by a comfortable margin.

\mypara{Long-range modeling with Sequential MNIST, PMNIST, and CIFAR-10.}
We also evaluate the \model~for ability to model long-term dependencies. In the Sequential MNIST, PMNIST, and CIFAR-10 tasks, images are processed as long sequences, one pixel at a time \citep{chang2017dilated,bai2018empirical,trinh2018learning}. Our model has 8M parameters, in alignment with prior work. To cover the larger context, we use dilated convolutions in intermediate layers, adopting a common architectural element from TCNs \citep{dilatedConv,waveNet,bai2018empirical}.
The results are listed in Table~\ref{table:longer-dependency}. Note that the performance of prior models is inconsistent. The Transformer works well on MNIST but fairs poorly on CIFAR-10, while $r$-LSTM with unsupervised auxiliary losses achieves good results on CIFAR-10 but underperforms on Permuted MNIST. TrellisNet outperforms all these models on all three tasks.


\section{Discussion}
\label{sec:discussion}

We presented trellis networks, a new architecture for sequence modeling. Trellis networks form a structural bridge between convolutional and recurrent models. This enables direct assimilation of many techniques designed for either of these two architectural families. We leverage these connections to train high-performing trellis networks that set a new state of the art on highly competitive language modeling benchmarks. Beyond the empirical gains, we hope that trellis networks will serve as a step towards deeper and more unified understanding of sequence modeling.

There are many exciting opportunities for future work. First, we have not conducted thorough performance optimizations on trellis networks. For example, architecture search on the structure of the gated activation $f$ may yield a higher-performing activation function than the classic LSTM cell we used~\citep{zoph2017neural,pham2018efficient}. Likewise, principled hyperparameter tuning will likely improve modeling accuracy beyond the levels we have observed~\citep{Melis2018}. Future work can also explore acceleration schemes that speed up training and inference.

Another significant opportunity is to establish connections between trellis networks and self-attention-based architectures (Transformers)~\citep{vaswani2017attention,santoro2018relational,chen2018best}, thus unifying all three major contemporary approaches to sequence modeling. Finally, we look forward to seeing applications of trellis networks to industrial-scale challenges such as machine translation.


\bibliography{trellisnet}
\bibliographystyle{iclr2019_conference}

\newpage

\appendix

\section{Expressing an LSTM as a TrellisNet}
\label{appendix:trellisnet-gated-activation}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{.34\textwidth}
        \centering
        \includegraphics[width=.88\textwidth]{images/architecture/lstm-trellis-atom}
        \caption{An atomic view}
        \label{fig:lstm-trellis-atom-appendix}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.64\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{images/architecture/lstm-trellis-general}
        \caption{A sequence view}
        \label{fig:lstm-trellis-general}
    \end{subfigure}
    \caption{A TrellisNet with an LSTM nonlinearity, at an atomic level and on a longer sequence.}
    \label{fig:lstm-trellis-network}
\end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=.48\textwidth]{images/architecture/lstm-tcn-interlayer}
    \caption{A 2-layer LSTM is expressed as a trellis network with mixed group convolutions on four groups of feature channels. (Partial view.)}
    \label{fig:lstm-tcn-interlayer}
\end{figure}

Here we trace in more detail the transformation of an LSTM into a TrellisNet. This is an application of Theorem \ref{thm:trellisnet-rnn}. The nonlinear activation has been examined in Section \ref{subsec:lstm-trellis}. We will walk through the construction again here.

In each time step, an LSTM cell computes the following:
\begin{equation}
\label{eq:full-kernel-rewrite}
\resizebox{.92\hsize}{!}{
$
\begin{aligned}
&f_t^{(\ell)} = \sigma(W_f h_t^{(\ell-1)} + U_f h_{t-1}^{(\ell)}) &i_t^{(\ell)} = \sigma(W_i h_t^{(\ell-1)} + U_i h_{t-1}^{(\ell)}) \quad &g_t^{(\ell)} = \tanh(W_g h_t^{(\ell-1)} + U_g h_{t-1}^{(\ell)})\\
&o_t^{(\ell)} = \sigma(W_o h_t^{(\ell-1)} + U_o h_{t-1}^{(\ell)}) &c_t^{(\ell)} = f_t^{(\ell)} \circ c_{t-1}^{(\ell)} + i_t^{(\ell)} \circ g_t^{(\ell)} \quad &h_t^{(\ell)} = o_t^{(\ell)} \circ \tanh(c_t^{(\ell)})
\end{aligned}
$
}
\end{equation}
where $h_t^{(0)}=x_t$, and $f_t, i_t, o_t$ are typically called the \emph{forget}, \emph{input}, and \emph{output} gates. By a similar construction to how we defined $\tau$ in Theorem \ref{thm:trellisnet-rnn}, to recover an LSTM the mixed group convolution needs to produce $3q$ more channels for these gated outputs, which have the form $f_{t,t'}, i_{t,t'}$ and $g_{t,t'}$ (see Figure \ref{fig:lstm-tcn-interlayer} for an example). In addition, at each layer of the mixed group convolution, the network also needs to maintain a group of channels for cell states $c_{t,t'}$. Note that in an LSTM network, $c_t$ is updated ``synchronously'' with $h_t$, so we can similarly write
\begin{equation}
\label{eq:lstm-gated-activation}
c_{t,t'}^{(1)} = f_{t,t'}^{(1)} \circ c_{t-1,t'}^{(1)} + i_{t,t'}^{(1)} \circ g_{t,t'}^{(1)} \qquad h_{t,t'}^{(1)} = o_{t,t'}^{(1)} \circ \tanh(c_{t,t'}^{(1)})
\end{equation}
Based on these changes, we show in Figure \ref{fig:lstm-trellis-network} an atomic and a sequence view of TrellisNet with the LSTM activation. The hidden units $z_{1:T}$ consist of two parts: $z_{1:T,1}$, which gets updated directly via the gated activations (akin to LSTM cell states), and $z_{1:T,2}$, which is processed by parameterized convolutions (akin to LSTM hidden states). Formally, in layer $i$:
\begin{equation*}
\resizebox{0.8\hsize}{!}{
$
\begin{aligned}
\hat{z}_{1:T}^{(i+1)} &= \text{Conv1D}(z_{1:T,2}^{(i)}; W) + \tilde{x}_{1:T} = \begin{bmatrix}\hat{z}_{1:T,1} & \hat{z}_{1:T,2} & \hat{z}_{1:T,3} & \hat{z}_{1:T,4} \end{bmatrix}^\top \\
z_{1:T, 1}^{(i+1)} &= \sigma(\hat{z}_{1:T,1}) \circ z_{0:T-1, 1}^{(i)} + \sigma(\hat{z}_{1:T, 2}) \circ \tanh(\hat{z}_{1:T, 3}) \\
z_{1:T, 2}^{(i+1)} &= \sigma(\hat{z}_{1:T,4}) \circ \tanh(z_{1:T, 1}^{(i+1)})
\end{aligned}
$
}
\end{equation*}
\vspace{-.1in}

\newpage

\section{Optimizing and Regularizing \model~with RNN and TCN Methods}
\label{appendix:optimize-trellis}

\begin{figure}[h]
    \centering
    \vspace{-.3in}
    \begin{subfigure}[b]{.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/architecture/rnn-repackaging}
        \caption{History repackaging between truncated sequences in recurrent networks.}
        \label{fig:rnn-repackaging}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.54\textwidth}
        \centering
        \includegraphics[width=.75\textwidth]{images/architecture/tcn-repackaging}
        \caption{History repackaging in mixed group convolutions, where we write out $z_t$ explicitly by Eq. (\ref{pf-eq:rnn-tcn-mix}).}
        \label{fig:tcn-repackaging}
    \end{subfigure}
    \vspace{-.15in}
    \caption{Using the equivalence established by Theorem \ref{thm:trellisnet-rnn}, we can transfer the notion of history repackaging in recurrent networks to trellis networks.}
    \label{fig:repackaging}
\end{figure}

In Section \ref{sec:rnn-tcn-relationship}, we formally described the relationship between TrellisNets, RNNs, and temporal convolutional networks (TCN). On the one hand, \model~is a special TCN (with weight-tying and input injection), while on the other hand it can also express any structured RNN via a sparse convolutional kernel. These relationships open clear paths for applying techniques developed for either recurrent or convolutional networks. We summarize below some of the techniques that can be applied in this way to TrellisNet, categorizing them as either inspired by RNNs or TCNs.

\subsection{From Recurrent Networks}
\label{appendix:from-rnns}

\mypara{History repackaging.}
One theoretical advantage of RNNs is their ability to represent a history of infinite length. However, in many applications, sequence lengths are too long for infinite backpropagation during training. A typical solution is to partition the sequence into smaller subsequences and perform truncated backpropagation through time (BPTT) on each. At sequence boundaries, the hidden state $h_t$ is ``repackaged'' and passed onto the next RNN sequence. Thus gradient flow stops at sequence boundaries (see Figure \ref{fig:rnn-repackaging}). Such repackaging is also sometimes used at test time.

We can now map this repackaging procedure to trellis networks. As shown in Figure \ref{fig:repackaging}, the notion of passing the compressed history vector $h_t$ in an RNN corresponds to specific non-zero padding in the mixed group convolution of the corresponding TrellisNet.
The padding is simply the channels from the last step of the final layer applied on the previous sequence (see Figure \ref{fig:tcn-repackaging}, where without the repackaging padding, at layer 2 we will have \small$h_{T+1, T+1}^{(1)}$\normalsize~instead of \small$h_{T+1, 1}^{(1)}$\normalsize). We illustrate this in Figure \ref{fig:tcn-repackaging}, where we have written out $z_t^{(i)}$ in \model~explicitly in the form of $h_{t,t'}$ according to Eq.~(\ref{pf-eq:rnn-tcn-mix}). This suggests that instead of storing all effective history in memory, we can compress history in a feed-forward network to extend its history as well. For a general \model~that employs a dense kernel, similarly, we can pass the hidden channels of the last step of the final layer in the previous sequence as the ``history'' padding for the next \model~sequence (this works in both training and testing).


\mypara{Gated activations.} In general, the structured gates in RNN cells can be translated to gated activations in temporal convolutions, as we did in Appendix \ref{appendix:trellisnet-gated-activation} in the case of an LSTM. While in the experiments we adopted the LSTM gating, other activations (e.g.\ GRUs~\citep{choGRU} or activations found via architecture search~\citep{zoph2017neural}) can also be applied in trellis networks via the equivalence established in Theorem~\ref{thm:trellisnet-rnn}.


\mypara{RNN variational dropout.}
Variational dropout (VD) for RNNs~\citep{gal2016dropout} is a useful regularization scheme that applies the same mask at every time step within a layer (see Figure~\ref{fig:variational-dropout}). A direct translation of this technique from RNN to the group temporal convolution implies that we need to create a different mask for each diagonal of the network (i.e.\ each history starting point), as well as for each group of the mixed group convolution. We propose an alternative (and extremely simple) dropout scheme for TrellisNet, which is inspired by VD in RNNs as well as Theorem~\ref{thm:trellisnet-rnn}. In each iteration, we apply the \emph{same mask} on the post-activation outputs, at every time step in both the temporal dimension and depth dimension. That is, based on Eq. (\ref{pf-eq:rnn-tcn-mix}) in Theorem \ref{thm:trellisnet-rnn}, we adapt VD to the \model~setting by assuming $h_{t, t' \pm \delta} \approx h_{t,t'}$; see Figure \ref{fig:variational-dropout}. Empirically, we found this dropout to work significantly better than other dropout schemes (e.g.\ drop certain channels entirely).

\begin{figure}
\centering
\vspace{-.3in}
    \begin{subfigure}[b]{.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/architecture/variational-drop-rnn}
        \caption{Left: variational dropout (VD) in an RNN. Right: VD in a TrellisNet. Each color indicates a different dropout mask.}
        \label{fig:variational-dropout}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.38\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/architecture/auxiliary-loss-net}
        \caption{Auxiliary loss on intermediate layers in a \model.}
        \label{fig:auxiliary-loss}
    \end{subfigure}
\caption{(a) RNN-inspired variational dropout. (b) ConvNet-inspired auxiliary losses.}
\label{fig:vd-auxiliary}
\vspace{-.15in}
\end{figure}

\mypara{Recurrent weight dropout/DropConnect.} We apply DropConnect on the TrellisNet kernel. \cite{merityRegOpt} showed that regularizing hidden-to-hidden weights $W_{hh}$ can be useful in optimizing LSTM language models, and we carry this scheme over to trellis networks.

\subsection{From Convolutional Networks}
\label{appendix:from-tcns}

\mypara{Dense convolutional kernel.} Generalizing the convolution from a mixed group (sparse) convolution to a general (dense) one means the connections are no longer recurrent and we are computing directly on the hidden units with a large kernel, just like any temporal ConvNet.

\mypara{Deep supervision.} Recall that for sparse \model~to recover truncated RNN, at each level the hidden units are of the form $h_{t,t'}$, representing the state at time $t$ \emph{if we assume that history started at time $t'$} (Eq. (\ref{pf-eq:rnn-tcn-mix})). We propose to inject the loss function at intermediate layers of the convolutional network (e.g.\ after every $\ell$ layers of transformations, where we call $\ell$ the auxiliary loss frequency). For example, during training, to predict an output at time $t$ with a $L$-layer \model, besides \small$z_t^{(L)}$\normalsize~in the last layer, we can also apply the loss function on \small$z_t^{(L-\ell)}$, $z_t^{(L-2\ell)}$\normalsize, etc.~-- where hidden units will predict with a shorter history because they are at lower levels of the network. This had been introduced for convolutional models in computer vision~\citep{lee2015deeply,xie2015holistically}. The eventual loss of the network will be
\begin{equation}
\mathcal{L}_\text{total} = \mathcal{L}_\text{orig} + \lambda \cdot \mathcal{L}_\text{aux},
\end{equation}
where $\lambda$ is a fixed scaling factor that controls the weight of the auxiliary loss.

Note that this technique is not directly transferable (or applicable) to RNNs.

\mypara{Larger kernel and dilations~\citep{dilatedConv}.} These techniques have been used in convolutional networks to more quickly increase the receptive field. They can be immediately applied to trellis networks. Note that the activation function $f$ of \model~may need to change if we change the kernel size or dilation settings (e.g.\ with dilation $d$ and kernel size 2, the activation will be \small$f(\hat{z}_{1:T}^{(i)}, z_{1:T-d}^{(i)})$\normalsize).

\mypara{Weight normalization~\citep{Salimans2016}.} Weight normalization (WN) is a technique that learns the direction and the magnitude of the weight matrix independently. Applying WN on the convolutional kernel was used in some prior works on temporal convolutional architectures~\citep{dauphinGatedConv,bai2018empirical}, and have been found useful in regularizing the convolutional filters and boosting convergence.

\mypara{Parallelism.} Because \model~is convolutional in nature, it can easily leverage the parallel processing in the convolution operation (which slides the kernel across the input features). We note that when the input sequence is relatively long, the predictions of the first few time steps will have insufficient history context compared to the predictions later in the sequence. This can be addressed by either history padding (mentioned in Appendix~\ref{appendix:from-rnns}) or chopping off the loss incurred by the first few time steps.


\section{Benchmark Tasks}
\label{appendix:task-description}

\mypara{Word-level language modeling on Penn Treebank (PTB).} The original Penn Treebank (PTB) dataset selected 2,499 stories from a collection of almost 100K stories published in Wall Street Journal (WSJ)~\citep{Marcus93buildinga}. After \cite{Mikolov2010PTB}~processed the corpus, the PTB dataset contains 888K words for training, 70K for validation and 79K for testing, where each sentence is marked with an \texttt{<eos>} tag at its end. All of the numbers (e.g.\ in financial news) were replaced with a \texttt{?} symbol with many punctuations removed. Though small, PTB has been a highly studied dataset in the domain of language modeling~\citep{miyamoto2016gated,zilly2017recurrent,merityRegOpt,Melis2018,yang2018breaking}.
Due to its relatively small size, many computational models can easily overfit on word-level PTB. Therefore, good regularization methods and optimization techniques designed for sequence models are especially important on this benchmark task \citep{merityRegOpt}.

\mypara{Word-level language modeling on WikiText-103.}
WikiText-103 (WT103) is 110 times larger than PTB, containing a training corpus from 28K lightly processed Wikipedia articles~\citep{merity2016pointer}. In total, WT103 features a vocabulary size of about 268K\footnote{As a reference, Oxford English Dictionary only contains less than \href{https://en.oxforddictionaries.com/explore/how-many-words-are-there-in-the-english-language/}{220K} unique English words.}, with 103M words for training, 218K words for validation, and 246K words for testing/evaluation. The WT103 corpus also retains the original case, punctuation and numbers in the raw data, all of which were removed from the PTB corpus. Moreover, since WT103 is composed of full articles (whereas PTB is sentence-based), it is better suited for testing long-term context retention. For these reasons, WT103 is typically considered much more representative and realistic than PTB~\citep{merity2018analysis}.

\mypara{Character-level language modeling on Penn Treebank (PTB).}
When used for character-level language modeling, PTB is a medium size dataset that contains 5M chracters for training, 396K for validation, and 446K for testing, with an alphabet size of 50 (note: the \texttt{<eos>} tag that marks the end of a sentence in word-level tasks is now considered one character). While the alphabet size of char-level PTB is much smaller compared to the word-level vocabulary size (10K), there is much longer sequential token dependency because a sentence contains many more characters than words.

\mypara{Sequential and permuted MNIST classification.}
The MNIST handwritten digits dataset~\citep{LeCun1989} contains 60K normalized training images and 10K testing images, all of size $28 \times 28$. In the sequential MNIST task, MNIST images are presented to the sequence model as a flattened $784 \times 1$ sequence for digit classification. Accurate predictions therefore require good long-term memory of the flattened pixels~-- longer than in most language modeling tasks. In the setting of permuted MNIST (PMNIST), the order of the sequence is permuted at random, so the network can no longer rely on local pixel features for classification.

\mypara{Sequential CIFAR-10 classification.}
The CIFAR-10 dataset~\citep{krizhevsky2009learning} contains 50K images for training and 10K for testing, all of size $32 \times 32$. In the sequential CIFAR-10 task, these images are passed into the model one at each time step, flattended as in the MNIST tasks. Compared to sequential MNIST, this task is more challenging. For instance, CIFAR-10 contains more complex image structures and intra-class variations, and there are 3 channels to the input. Moreover, as the images are larger, a sequence model needs to have even longer memory than in sequential MNIST or PMNIST~\citep{trinh2018learning}.


\section{Hyperparameters and Ablation Study}
\label{appendix:hyperparameters}

Table \ref{table:hyperparameters} specifies the trellis networks used for the various tasks. There are a few things to note while reading the table. First, in training, we decay the learning rate once the validation error plateaus for a while (or according to some fixed schedule, such as after 100 epochs). Second, for auxiliary loss (see Appendix \ref{appendix:optimize-trellis} for more details), we insert the loss function after every fixed number of layers in the network. This ``frequency'' is included below under the ``Auxiliary Frequency'' entry. Finally, the hidden dropout in the Table refers to the variational dropout we translated from RNNs (see Appendix \ref{appendix:optimize-trellis}), which is applied at all hidden layers of the \model. Due to the insight from Theorem \ref{thm:trellisnet-rnn}, many techniques in \model~were translated directly from RNNs or TCNs. Thus, most of the hyperparameters were based on the numbers reported in prior works (e.g.\ embedding size, embedding dropout, hidden dropout, output dropout, optimizer, weight-decay, etc.)  with minor adjustments~\citep{merityRegOpt,yang2018breaking,bradbury2016quasi,merity2018analysis,trinh2018learning,bai2018empirical,santoro2018relational}. For factors such as auxiliary loss weight and frequency, we perform a basic grid search.
\begin{table*}[h]
\def\arraystretch{1.4}
\small
\centering
\caption{Models and hyperparameters used in experiments. ``--'' means not applicable/used.}
\label{table:hyperparameters}
\vspace{-.05in}
\resizebox{1.01\textwidth}{!}{
\begin{tabular}{l|ccccc}
\toprule
 & Word-PTB (w/o MoS) & Word-PTB (w/ MoS) & Word-WT103 & Char-PTB & (P)MNIST/CIFAR-10 \\
\midrule
Optimizer                       & SGD  & SGD  & Adam & Adam & Adam \\
Initial Learning Rate           & 20   & 20   & 1e-3 & 2e-3 & 2e-3 \\
Hidden Size (i.e. $h_t$)        & 1000 & 1000 & 2000 & 1000 & 100 \\
Output Size (only for MoS)      &  --  & 480  &  --   &  --   &  --  \\
\# of Experts (only for MoS)    &  --   & 15   &  --   &  --   &  --  \\
Embedding Size                  & 400  & 280  & 512  & 200  &  --  \\
Embedding Dropout               & 0.1  & 0.05 & 0.0  & 0.0  &  --  \\
Hidden (VD-based) Dropout       & 0.28 & 0.28 & 0.1  & 0.3  & 0.2  \\
Output Dropout                  & 0.45 & 0.4  & 0.1  & 0.1 & 0.2 \\
Weight Dropout                  & 0.5  & 0.45 & 0.1  & 0.25 & 0.1 \\
\# of Layers                    & 55   & 55   & 70   & 125  & 16  \\
Auxiliary Loss $\lambda$        & 0.05 & 0.05 & 0.08 & 0.3  &  --  \\
Auxiliary Frequency             & 16   & 16   & 25   & 70   &  --  \\
Weight Normalization            &  --   &  --   & \Checkmark & \Checkmark & \Checkmark \\
Gradient Clip                   & 0.225 & 0.2 & 0.1 & 0.2 & 0.5 \\
Weight Decay                    & 1e-6 & 1e-6 & 0.0  & 1e-6 & 1e-6 \\
\hline
\textbf{Model Size}             & 24M  & 25M  & 180M & 13.4M & 8M \\
\bottomrule
\end{tabular}}
\vspace{-.1in}
\end{table*}

We have also performed an ablation study on \model~to study the influence of various ingredients and techniques on performance. The results are reported in Table~\ref{table:ablation}. We conduct the study on word-level PTB using a \model~with 24M parameters. When we study one factor (e.g.\ removing hidden dropout), all hyperparameters and settings remain the same as in column 1 of Table \ref{table:hyperparameters} (except for ``Dense Kernel'', where we adjust the number of hidden units so that the model size remains the same).

\begin{table*}[h]
\def\arraystretch{1.4}
\small
\centering
\caption{Ablation study on word-level PTB (w/o MoS)}
\label{table:ablation}
\vspace{-.05in}
\resizebox{.8\hsize}{!}{
\begin{tabular}{l|c|c|c}
\toprule
 & Model Size & Test ppl & $\Delta$ SOTA \\
\midrule
\model~ & 24.1M & \wordptbressmall & -- \\
$\ \ -$ Hidden (VD-based) Dropout & 24.1M & 64.69 & $\downarrow$ 7.72\\
$\ \ -$ Weight Dropout & 24.1M & 63.82 & $\downarrow$ 6.85\\
$\ \ -$ Auxiliary Losses & 24.1M & 57.99 & $\downarrow$ 1.02 \\
$\ \ -$ Long Seq. Parallelism & 24.1M & 57.35  & $\downarrow$ 0.38 \\
$\ \ -$ Dense Kernel (i.e.\ mixed group conv) & 24.1M & 59.18  & $\downarrow$ 2.21 \\
$\ \ -$ Injected Input (every 2 layers instead) & 24.1M & 57.44 & $\downarrow$ 0.47 \\
$\ \ -$ Injected Input (every 5 layers instead) & 24.1M & 59.75 & $\downarrow$ 2.78 \\
$\ \ -$ Injected Input (every 10 layers instead) & 24.1M & 60.70 & $\downarrow$ 3.73 \\
$\ \ -$ Injected Input (every 20 layers instead) & 24.1M & 74.91 & $\downarrow$ 17.94 \\
\bottomrule
\end{tabular}
}
\vspace{-.1in}
\end{table*}


\end{document}
