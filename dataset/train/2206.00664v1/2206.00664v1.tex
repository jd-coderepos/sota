\documentclass{article}

\usepackage{arxiv_ml_institute}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{graphicx}
\usepackage{subfigure}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{pifont}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{array}
\usepackage{makecell}
\newcolumntype{R}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedleft\arraybackslash}p{#1}}
\sisetup{group-separator={,},group-minimum-digits={3}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand\Ba{\bm{a}}
\newcommand\Bb{\bm{b}}
\newcommand\Bc{\bm{c}}
\newcommand\Bd{\bm{d}}
\newcommand\Be{\bm{e}}
\newcommand\Bf{\bm{f}}
\newcommand\Bg{\bm{g}}
\newcommand\Bh{\bm{h}}
\newcommand\Bi{\bm{i}}
\newcommand\Bj{\bm{j}}
\newcommand\Bk{\bm{k}}
\newcommand\Bl{\bm{l}}
\newcommand\Bm{\bm{m}}
\newcommand\Bn{\bm{n}}
\newcommand\Bo{\bm{o}}
\newcommand\Bp{\bm{p}}
\newcommand\Bq{\bm{q}}
\newcommand\Br{\bm{r}}
\newcommand\Bs{\bm{s}}
\newcommand\Bt{\bm{t}}
\newcommand\Bu{\bm{u}}
\newcommand\Bv{\bm{v}}
\newcommand\Bw{\bm{w}}
\newcommand\Bx{\bm{x}}
\newcommand\By{\bm{y}}
\newcommand\Bz{\bm{z}}

\newcommand\BA{\bm{A}}
\newcommand\BB{\bm{B}}
\newcommand\BC{\bm{C}}
\newcommand\BD{\bm{D}}
\newcommand\BE{\bm{E}}
\newcommand\BF{\bm{F}}
\newcommand\BG{\bm{G}}
\newcommand\BH{\bm{H}}
\newcommand\BI{\bm{I}}
\newcommand\BJ{\bm{J}}
\newcommand\BK{\bm{K}}
\newcommand\BL{\bm{L}}
\newcommand\BM{\bm{M}}
\newcommand\BN{\bm{N}}
\newcommand\BO{\bm{O}}
\newcommand\BP{\bm{P}}
\newcommand\BQ{\bm{Q}}
\newcommand\BR{\bm{R}}
\newcommand\BS{\bm{S}}
\newcommand\BT{\bm{T}}
\newcommand\BU{\bm{U}}
\newcommand\BV{\bm{V}}
\newcommand\BW{\bm{W}}
\newcommand\BX{\bm{X}}
\newcommand\BY{\bm{Y}}
\newcommand\BZ{\bm{Z}}

\newcommand\Bal{\bm{\alpha}}
\newcommand\Bbe{\bm{\beta}}
\newcommand\Bla{\bm{\lambda}}
\newcommand\Bep{\bm{\epsilon}}
\newcommand\Bga{\bm{\gamma}}
\newcommand\Bmu{\bm{\mu}}
\newcommand\Bnu{\bm{\nu}}
\newcommand\Brh{\bm{\rho}}
\newcommand\Bth{\bm{\theta}}
\newcommand\Bxi{\bm{\xi}}
\newcommand\Bka{\bm{\kappa}}
\newcommand\Bsi{\bm{\sigma}}
\newcommand\Bta{\bm{\tau}}
\newcommand\Bph{\bm{\phi}}
\newcommand\Bom{\bm{\omega}}
\newcommand\Bze{\bm{\zeta}}

\newcommand\BDe{\bm{\Delta}}
\newcommand\BLa{\bm{\Lambda}}
\newcommand\BPh{\bm{\Phi}}
\newcommand\BPs{\bm{\Psi}}
\newcommand\BSi{\bm{\Sigma}}
\newcommand\BUp{\bm{\Upsilon}}
\newcommand\BXi{\bm{\Xi}}
\newcommand\BGa{\bm{\Gamma}}
\newcommand\BTh{\bm{\Theta}}
\newcommand\BOm{\bm{\Omega}}
\newcommand\BZe{\bm{\Zeta}}

\newcommand\BOn{\bm{1}}
\newcommand\BZo{\bm{0}}
\newcommand\BLOn{\mathlarger{\mathlarger{\bm{1}}}}
\newcommand\BLZe{\mathlarger{\mathlarger{\bm{0}}}}

\newcommand{\dA}{\mathbb{A}} \newcommand{\dB}{\mathbb{B}} 
\newcommand{\dC}{\mathbb{C}} \newcommand{\dD}{\mathbb{D}} 
\newcommand{\dE}{\mathbb{E}} \newcommand{\dF}{\mathbb{F}}
\newcommand{\dG}{\mathbb{G}} \newcommand{\dH}{\mathbb{H}}
\newcommand{\dI}{\mathbb{I}} \newcommand{\dJ}{\mathbb{J}} 
\newcommand{\dK}{\mathbb{K}} \newcommand{\dL}{\mathbb{L}}
\newcommand{\dM}{\mathbb{M}} \newcommand{\dN}{\mathbb{N}}
\newcommand{\dO}{\mathbb{O}} \newcommand{\dP}{\mathbb{P}} 
\newcommand{\dQ}{\mathbb{Q}} \newcommand{\dR}{\mathbb{R}}
\newcommand{\dS}{\mathbb{S}} \newcommand{\dT}{\mathbb{T}} 
\newcommand{\dU}{\mathbb{U}} \newcommand{\dV}{\mathbb{V}} 
\newcommand{\dW}{\mathbb{W}} \newcommand{\dX}{\mathbb{X}}
\newcommand{\dY}{\mathbb{Y}} \newcommand{\dZ}{\mathbb{Z}}

\newcommand{\rA}{\mathrm{A}} \newcommand{\rB}{\mathrm{B}} 
\newcommand{\rC}{\mathrm{C}} \newcommand{\rD}{\mathrm{D}} 
\newcommand{\rE}{\mathrm{E}} \newcommand{\rF}{\mathrm{F}}
\newcommand{\rG}{\mathrm{G}} \newcommand{\rH}{\mathrm{H}}
\newcommand{\rI}{\mathrm{I}} \newcommand{\rJ}{\mathrm{J}} 
\newcommand{\rK}{\mathrm{K}} \newcommand{\rL}{\mathrm{L}}
\newcommand{\rM}{\mathrm{M}} \newcommand{\rN}{\mathrm{N}}
\newcommand{\rO}{\mathrm{O}} \newcommand{\rP}{\mathrm{P}} 
\newcommand{\rQ}{\mathrm{Q}} \newcommand{\rR}{\mathrm{R}}
\newcommand{\rS}{\mathrm{S}} \newcommand{\rT}{\mathrm{T}}
\newcommand{\rU}{\mathrm{U}} \newcommand{\rV}{\mathrm{V}} 
\newcommand{\rW}{\mathrm{W}} \newcommand{\rX}{\mathrm{X}}
\newcommand{\rY}{\mathrm{Y}} \newcommand{\rZ}{\mathrm{Z}}

\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand{\sA}{\mathscr{A}} \newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}} \newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}} \newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}} \newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}} \newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}} \newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}} \newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}} \newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}} \newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}} \newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}} \newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}} \newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}} \newcommand{\sZ}{\mathscr{Z}}

\newcommand{\Ra}{\mathrm{a}} \newcommand{\Rb}{\mathrm{b}} 
\newcommand{\Rc}{\mathrm{c}} \newcommand{\Rd}{\mathrm{d}} 
\newcommand{\Rre}{\mathrm{e}} \newcommand{\Rf}{\mathrm{f}}
\newcommand{\Rg}{\mathrm{g}} \newcommand{\Rh}{\mathrm{h}}
\newcommand{\Ri}{\mathrm{i}} \newcommand{\Rj}{\mathrm{j}} 
\newcommand{\Rk}{\mathrm{k}} \newcommand{\Rl}{\mathrm{l}}
\newcommand{\Rm}{\mathrm{m}} \newcommand{\Rn}{\mathrm{n}}
\newcommand{\Ro}{\mathrm{o}} \newcommand{\Rp}{\mathrm{p}} 
\newcommand{\Rq}{\mathrm{q}} \newcommand{\Rr}{\mathrm{r}}
\newcommand{\Rs}{\mathrm{s}} \newcommand{\Rt}{\mathrm{t}}
\newcommand{\Ru}{\mathrm{u}} \newcommand{\Rv}{\mathrm{v}} 
\newcommand{\Rw}{\mathrm{w}} \newcommand{\Rx}{\mathrm{x}}
\newcommand{\Ry}{\mathrm{y}} \newcommand{\Rz}{\mathrm{z}}

\newcommand\xs{x}
\newcommand\zs{z}
\newcommand\xsp{\left(x_{\phi}\right)}
\newcommand\xp{\bm{x}_{\phi}}
\newcommand\zp{\bm{z}_{\omega}}
\newcommand\zps{\left(z_{\omega}\right)}
\newcommand\sign{\mbox{sign}}
\newcommand\EXP{\mathbf{\mathrm{E}}}
\newcommand\PR{\mathbf{\mathrm{Pr}}}
\newcommand\VAR{\mathbf{\mathrm{Var}}}
\newcommand\COV{\mathbf{\mathrm{Cov}}}
\newcommand\TR{\mathbf{\mathrm{Tr}}}
\newcommand\ALs{{\mbox{\boldmath }}}
\newcommand\XIs{{\mbox{\boldmath }}}
\newcommand\sgn{\mathop{\mathrm{sgn}\,}}
\newcommand\argmax{\mathop{\mathrm{argmax}\,}}
\newcommand\oo{\mathrm{old}}
\newcommand\nn{\mathrm{new}}
\newcommand\rank{\mathrm{rank}}
\newcommand\net{\mathrm{net}}
\newcommand\emp{\mathrm{emp}}
\newcommand{\mse}{\mathop{\bf mse}}
\newcommand{\bias}{\mathop{\bf bias}}
\newcommand{\var}{\mathop{\bf var}}
\newcommand\att{\mathrm{att}}

\newcommand{\ABS}[1]{{{\left| #1 \right|}}} 
\newcommand{\BRA}[1]{{{\left\{#1\right\}}}} 
\newcommand{\NRM}[1]{{{\left\| #1\right\|}}} 
\newcommand{\PAR}[1]{{{\left(#1\right)}}} 
\newcommand{\SBRA}[1]{{{\left[#1\right]}}}

\newcommand{\soft}{\mathrm{softmax}}
\newcommand{\diag}{\mathrm{diag}}

\makeatletter
\newcommand{\dlmf}[1]{\citep[\def\nextitem{\def\nextitem{, }}\@for \el:=#1\do{\nextitem\href{http://dlmf.nist.gov/\el}{(\el)}}]{Olver:10}}
\makeatother

\definecolor{YColour}{RGB}{212, 101, 209}
\definecolor{RColour}{RGB}{140, 195, 98}
\definecolor{ZColour}{RGB}{46, 91, 207}
\definecolor{SColour}{RGB}{213, 228, 173}
\definecolor{IOColour}{RGB}{189, 46, 98}
\definecolor{RSColour}{RGB}{247, 206, 70}
\definecolor{REColour}{RGB}{243, 176, 138}
\definecolor{HColour}{RGB}{150, 204, 220}

\title{Hopular: Modern Hopfield Networks for \mbox{Tabular Data}}

\author{
    Bernhard Sch\"{a}fl\footnotemark[2]~\thanks{Corresponding author: Bernhard Sch\"{a}fl <\href{mailto:schaefl@ml.jku.at}{schaefl@ml.jku.at}>} \quad
    Lukas Gruber\footnotemark[2] \quad
    Angela Bitto-Nemling\footnotemark[2]~\footnotemark[3] \quad
    Sepp Hochreiter\footnotemark[2]~\footnotemark[3]\\ \\
  \footnotemark[2]~~ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning,\\
                  ~~Johannes Kepler University Linz, Austria\\
  \footnotemark[3]~~Institute of Advanced Research in 
  Artificial Intelligence (IARAI)
}

\begin{document}

\maketitle

\begin{abstract}
    While Deep Learning excels in structured data as encountered in vision and
    natural language processing, it failed to meet its expectations on tabular data.
    For tabular data, Support Vector Machines (SVMs), Random Forests,
    and Gradient Boosting are the best performing techniques with Gradient Boosting in the lead.
    Recently, we saw a surge of Deep Learning methods that were tailored to tabular data
    but still underperform compared to Gradient Boosting on small-sized datasets.
    We suggest ``Hopular'', a novel Deep Learning architecture for medium- and
    small-sized datasets,
    where each layer is
    equipped with continuous modern Hopfield networks. The modern Hopfield networks
    use stored data to identify feature-feature, 
    feature-target, and sample-sample dependencies.
    Hopular's novelty is that every layer can directly access 
    the original input as well as the whole training set 
    via stored data in the Hopfield networks.
    Therefore, 
    Hopular can step-wise update its current model and the resulting 
    prediction at every layer 
    like standard iterative learning algorithms.
    In experiments on small-sized tabular datasets with less than 1,000 samples,
    Hopular surpasses Gradient Boosting, Random Forests, SVMs,
    and in particular several Deep Learning methods.
    In experiments on medium-sized tabular data with about 10,000 samples,
    Hopular outperforms XGBoost, CatBoost, LightGBM and 
    a state-of-the art Deep Learning method designed for tabular data.
    Thus, Hopular is a strong alternative to these methods on tabular data.
\end{abstract}


\section{Introduction}

Deep Learning has led to tremendous success
in vision and natural language processing, where it 
excelled on large image and text corpora \citep{LeCun:15,Schmidhuber:15}.
While it yielded competitive results on large tabular datasets \cite{Avati:18,Simm:18,Zhang:19,Mayr:18},
so far it could not convince on small tabular data.
However, in real-world settings, 
small tabular datasets with less than 10,000 samples are ubiquitous.
They are found in life sciences, when building a model for a certain disease
with a limited number of patients,
for bio-assays in drug design, or for the effect of environmental soil contamination.  
The same situation appears in most industrial applications, when
a company wants to predict customer behavior, to control processes, 
to optimize its logistics, to market new products, or to employ predictive maintenance.
The omnipresence of small tabular datasets can also be witnessed at Kaggle challenges.
On small-sized and medium-sized tabular datasets with less than 10,000 samples,
Support Vector Machines (SVMs) \citep{Boser:92,Cortes:95,Scholkopf:02},
Random Forests \citep{Ho:95,Breiman:01} and, in particular,
Gradient Boosting \citep{Friedman:01}
typically outperform Deep Learning methods with 
Gradient Boosting having the edge. 
In real world applications, 
the best performing and most prevalent Gradient Boosting variants are
XGBoost \citep{Chen:16},
CatBoost \citep{Dorogush:17,Prokhorenkova:18}, and
LightGBM \citep{Ke:17}.



Recently, research on extending Deep Learning
methods to tabular data has been intensified.
Some approaches to tabular data
are only remotely related to Deep Learning.
AutoGluon-Tabular stacks small neural networks for tabular
data \citep{Erickson:20}.
Neural Oblivious Decision Ensembles (NODE) generalizes
ensembles of oblivious decision trees by hierarchical representation
learning \citep{Popov:19}.
NODE is a hybrid of 
differentiable decision trees and neural networks.
DNF-Net builds neural structures corresponding to
logical Boolean formulas in disjunctive normal forms,
which enable localized decisions using small subsets of the features \citep{Abutbul:20}.


However, most research focused on adapting established Deep Learning techniques to
tabular data.
Modifications to deep neural networks 
like introducing leaky gates or skip connections
can improve their performance on tabular data \citep{Fiedler:21}. 
Even plain MLPs that are well-regularized work well on tabular data \citep{Kadra:21}.
Different regularization coefficients to each weight improve
the performance of Deep Learning architectures on tabular data \citep{Shavitt:18}.
TabularNet consists of three modules \citep{Du:21}.
First, it uses handcrafted cell-level feature extraction
with a language model for textual data.
Secondly, it uses both row and column-wise
pooling via bidirectional gated recurrent units.
Thirdly, a graph convolutional network captures
dependencies between cells of the table.

Many approaches that adapt Deep Learning methods to tabular data
use attention mechanisms from transformers \citep{Vaswani:17} and BERT \citep{Devlin:19}.
The TabTransformer learns contextual embeddings of categorical
features \citep{Huang:20}.
However, continuous features are not covered, therefore the feature-feature
interaction is limited.
The FT-Transformer maps
features to tokens that are fed into a transformer \citep{Gorishniy:21}.
The FT-Transformer performs well on tabular data but
all considered datasets have more than 10,000 samples.
TabNet uses an attentive transformer for sequential
attention to predict masked features \citep{Arik:21}.
Therefore, TabNet does instance-wise feature selection, that is,
can select the relevant features for each input differently.
TabNet also utilizes feature masking for pre-training, which was very successful
in natural language processing when pre-training the BERT model.
Also semi-supervised learning has been proposed for tabular
data using projections of the
features and contrastive learning \citep{Darabi:21}.
The contrastive loss is low if pairs of the same class have
high similarity.
Value Imputation and Mask Estimation (VIME) uses 
self- and semi-supervised learning of deep architectures for tabular
data \citep{Yoon:20}.
Like BERT, the network has 
to predict the values of the masked feature vectors, where the
target is always masked.
The success of BERT feature masking confirms that Deep Learning techniques
must employ strong regularization to be
successful on tabular data \citep{Kadra:21}.
A multi-head self-attentive neural network for modeling feature-feature interactions
was also used in AutoInt \citep{Song:19}.
So far we mentioned work, where attention mechanisms extract
feature-feature and feature-target relations. 
However, also inter-sample attention can be implemented, if the whole training
set is given at the input.
TabGNN uses a graph neural network for tabular data
to model inter-sample relations \citep{Guo:21}.
However, the authors focus on large tabular datasets with more than
40,000 samples.
SAINT contains both self-attention and inter-sample attention 
and embeds both categorical and continuous features
before feeding them into transformer modules \citep{Somepalli:21}.
SAINT uses self-supervised pre-training with a contrastive loss to
minimize the difference between original and mixed samples.
Non-Parametric Transformers (NPTs) also use feature self-attention 
and inter-sample attention \citep{Kossen:21}.
The feature self-attention identifies dependencies between
features, while inter-sample attention detects relations
between samples. 
As in previous approaches, BERT masking is used
during training, where
the masked feature values and the target have to be predicted.



We suggest \textbf{Hopular} to learn with modern \textbf{Hop}field networks from tab\textbf{ular} data.
Hopular is a Deep Learning architecture, where each layer is
equipped with continuous modern Hopfield networks
\citep{Ramsauer:21,Widrich:20nips}.
Continuous modern Hopfield networks can store two types of data:
(i) the whole training set or
(ii) the feature embedding vectors of the original input.
Like SAINT and NPT, Hopular can detect feature-feature, feature-target, 
sample-sample, and sample-target dependencies via modern Hopfield networks.
Hopular's novelty is that every layer can directly access 
the original input as well as the whole training set 
via stored data in the Hopfield networks. 
In each layer, the stored training set enables
similarity-, prototype-, or quantization-based learning methods like
nearest neighbor.
In each layer, the stored original input enables
the identification of dependencies between the features and the target.
Consequently, the current model and its prediction 
can be step-wise improved at every
layer via direct access to both the training set and the original input.
Therefore, a pass through a Hopular model is similar to standard
learning algorithms, which iteratively improve the current model and its prediction
by re-accessing the training set. The number of iterations
is fixed by the number of layers in the Hopular architecture.
As previous methods, Hopular uses a feature embedding and
BERT masking, where masked features have to be predicted.
Hopular is most closely related to SAINT \citep{Somepalli:21}
and Non-Parametric Transformers (NPTs) \citep{Kossen:21},
but in contrast to SAINT and NPTs, the whole training set and the original input
are provided via Hopfield networks at every layer and
not only at the input.

Recently, it was reported that Random Forests 
still outperform standard Deep Learning techniques on tabular datasets with up to 10,000
samples \citep{Xu:21}.
In \citep{ShwartzZiv:21}, the authors show that XGBoost
outperforms various Deep Learning methods that are designed for tabular data on
datasets that did not appear in the original papers.
Therefore, we test Hopular on exactly those datasets to see whether
it performs as well as XGBoost.
Furthermore, we test Hopular on
UCI datasets \citep{Ramsauer:21,Klambauer:17,Wainberg:16,Fernandez:14}. 
Hopular surpasses Gradient Boosting, Random Forests,
and SVMs but also state-of-the-art Deep Learning approaches
to tabular data~like~NPTs.

\section{Brief Review of Modern Hopfield Networks}\label{sec:MHN}

We briefly review  
continuous modern Hopfield networks.
Their main properties are that they retrieve 
stored patterns with only one update 
and that they have exponential storage capacity
\citep{Ramsauer:21}.

We assume a set of patterns 
that are stacked as columns to 
the matrix  and a 
state pattern (query)  that represents the current state. 
The largest norm of a stored pattern is
.
Continuous modern Hopfield networks with state 
have the energy

For energy  and state , the update rule 

has been proven to converge globally  
to stationary points of the energy , which are almost always local minima 
\citep{Ramsauer:21}.
The update rule Eq.~\eqref{eq:main_iterate}
is also the formula of the well-known transformer attention mechanism
\citep{Vaswani:17,Ramsauer:21}, therefore Hopfield retrieval and
transformer attention coincide.

The {\em separation}   of a 
pattern  is defined as its minimal dot product difference to any of the other 
patterns:
. 
A pattern is {\em well-separated} from the data if .
If the patterns  are well separated, the iterate Eq.~\eqref{eq:main_iterate}
converges to a fixed point close to a stored pattern.
If some patterns are similar to one another and, therefore, not well separated, 
the update rule Eq.~\eqref{eq:main_iterate} converges to 
a fixed point close to the mean of the similar patterns. 
This fixed point is a {\em metastable state} of the energy function
and averages over similar patterns.

The next theorem states that the update rule Eq.~\eqref{eq:main_iterate} typically converges after
one update if the patterns are well separated. Furthermore, it states
that the retrieval error is 
exponentially small in the separation  (for the proof see~\mbox{\citep{Ramsauer:21}}):
\begin{theorem}
\label{th:oneUpdate}
With query , after one update the distance of the new point 
to the fixed point  is exponentially small in the separation .
The precise bounds using the Jacobian  and its value  in the mean value
theorem are:


For given  and 
sufficiently large , we have ,
that is, retrieval with one update.
The retrieval error  of pattern 
is bounded by

\end{theorem}

The main requirement to modern Hopfield networks to
be suited for tabular data is that they can store and retrieve enough patterns.
We want to store a potentially large training set in every layer
of a Deep Learning architecture.
We first define what we mean by storing and retrieving patterns
from a modern Hopfield network.
\begin{definition}[Pattern Stored and Retrieved]
We assume that around every pattern  a sphere  is given.
We say  {\em is stored} if there is a single fixed point  to
which all points  converge,
and   for .
We say  {\em is retrieved} for a given  if 
iteration (update rule) Eq.~\eqref{eq:main_iterate} gives
a point  that is at least 
-close to the single fixed point . 
The retrieval error is .
\end{definition}

\begin{figure}[b]
    \centering
    \includegraphics[width=\textwidth]{figures/hopular_overview.pdf}
    \caption{Architecture overview of Hopular. Hopular consists of three different types of layers or blocks. \textbf{(I) Embedding Layer}---each attribute of an original input sample is represented in an -dimensional space. The original input sample itself is then represented by the concatenation of all of its attribute representations. \textbf{(II) Hopular Block}---the input representation is then refined by  consecutive Hopular blocks. This is achieved by applying the two Hopfield modules  and  in an alternating way. \textbf{(III) Summarization Layer}---lastly, this refined current prediction is summarized by an attribute-wise mapping, leading to the final prediction.}
    \label{fig:hopular_overview}
\end{figure}

As with classical Hopfield networks, we consider patterns on the sphere, 
i.e.\ patterns with a fixed norm. 
For randomly chosen patterns, the number of patterns that can be stored
is exponential in the dimension  of the space of the patterns 
(for the proof see \citep{Ramsauer:21}):
\begin{theorem}
\label{th:storage}
We assume a failure probability  and randomly chosen patterns 
on the sphere with radius . 
We define , 
,
and ,
where  is the upper branch of the Lambert  function \dlmf{4.13},
and ensure .
Then with probability , the number of random patterns 
that can be stored is: 

Therefore it is proven for  with
, ,  and  ()
and proven for  with , , , and 
().
\end{theorem}

This theorem motivates to use continuous modern Hopfield networks
for tabular data, where we want to store the training set in each
layer of a Deep Learning architecture.
Even for hundreds of thousands of training samples, the 
continuous modern Hopfield network is able to store the training set
if the dimension of the pattern is large enough.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/hopular_block.pdf}
    \caption{A Hopular Block. The first Hopfield module stores the whole training set and identifies sample-sample relations. The second Hopfield module stores the embedded input features and extracts feature-feature and feature-target relations. The Hopfield
    modules refine the current prediction by combining the aggregated retrievals
    of the  Hopfield networks with their respective input.
    \label{fig:architecture_overview}}
\end{figure}


\section{Hopular: Modern Hopfield Networks for Tabular Data} \label{sec:mhn_tabular}

{\bf Hopular architecture.}
The Hopular architecture consists of an Embedding layer, several stacked 
Hopular blocks, and a Summarization layer as depicted in Figure~\ref{fig:hopular_overview}. As Hopular operates on features as well as on targets, we more generally refer to them as \emph{attributes}.

(i) The input to the Embedding Layer is an original input sample with  attributes, 
including a masked target.
Categorical attributes are encoded as one-hot vectors, 
whereas continuous attributes are normalized to zero mean 
and unit variance.
Then a mapping to an \mbox{-dimensional} embedding space is applied. 
The index of an attribute w.r.t.\ the position inside 
the sample as well as the attribute type are conserved 
by separate -dimensional learnable embeddings. 
All three embedding vectors are element-wise summed and
serve as the final representation of an input attribute. 
The original input sample is then represented by the concatenation of all
attribute representations. This concatenation also initializes the current 
prediction vector ~\---{}~see
Figure~\ref{fig:hopular_embedding} of the Appendix.

(ii) The current prediction vector serves as input to a Hopular Block.
A Hopular block consecutively applies two different Hopfield modules.
Each of these Hopfield modules refines the current prediction vector by updating
the current predictions for all attributes and combining it with its
input via a residual connection.
Thus, in addition to the target, also the features of the original input sample must be predicted during training.
\figurename~\ref{fig:architecture_overview} illustrates the forward-pass 
of a single original input sample with the masked target  
indicated by the question mark (\textbf{?}).
All current attribute predictions are refined.
The masked target is transformed by the Hopular block 
to a corresponding prediction as indicated by a check mark (\ding{51}).
Also feature representations can be masked as with
BERT pre-training.

(iii) The Summarization Layer summarizes the refined current prediction vector resulting from the stacked Hopular blocks.
The current prediction vector is mapped to the final prediction vector by separately mapping each current feature prediction to the corresponding final prediction as well as mapping the current target prediction to the final target prediction~\---{}~see Figure~\ref{fig:hopular_summarization} of the Appendix. In the following we describe the components (I)--(II) of a Hopular Block.

{\bf (I) Hopfield Module .} 
The first Hopfield module  implements a modern Hopfield network for Deep Learning architectures
similar to {\tt HopfieldLayer} \citep{Ramsauer:21,Ramsauer:20} 
with the training set as fixed stored patterns.
The current input  (which is also the current prediction from the previous
layer) to Hopfield module 
is interacting with the whole training data 
as described in Eq.~\eqref{eq:Hs}.
This is the update rule of 
continuous modern Hopfield networks as given in Eq.~\eqref{eq:main_iterate}.
Hence, the Hopfield module  identifies sample-sample relations
and can perform similarity searches like a nearest-neighbor search 
in the whole training data.
 can also average over training data that 
are similar to a mapping of the current prediction vector .

Next, we describe Hopfield Module  in more detail.
Let  be the number of attributes, 
 the embedding dimension of each single attribute, 
 the dimension of the Hopfield embedding space, and  the number of samples in the training set. 
The forward-pass for module  with one Hopfield network and
current prediction vector , 
learned weight matrices , \ , the stored training set , and a fixed scaling parameter 
is given~as

The hyperparameter  allows to steer the type of fixed point 
the update rule Eq.~\eqref{eq:main_iterate} converges to, 
hence it may further amplify the nearest-neighbor-lookup of the sample-sample Hopfield module .  may contain more than one 
continuous modern Hopfield network.
In this case, the respective results are combined and projected, 
serving as the modules final output. 
We have  separate Hopfield networks , where the module output is defined as

with vector 
and a learnable weight matrix .

{\bf (II) Hopfield Module .} 
The second Hopfield module  implements a modern Hopfield network for Deep Learning architectures
via the layer {\tt Hopfield} \citep{Ramsauer:21,Ramsauer:20} 
with the embedded features of the original input sample as stored patterns.
The refined prediction vector from the previous layer
is reshaped and transposed
to the matrix , which serves as input to
the Hopfield module .
 interacts with the embedded features
of the original input sample
as described in Eq.~\eqref{eq:Hf}.
Again, this is the update rule of 
continuous modern Hopfield networks as given in Eq.~\eqref{eq:main_iterate}.
Therefore, the Hopfield module  extracts and models
feature-feature and feature-target relations.
Current feature and target predictions are adjusted and refined after they are associated with the original input sample feature representations.

Next, we describe Hopfield Module  in more detail.
The matrix  is a transposed and reshaped version 
of current prediction vector  with respect to the embedding dimension .
Using the learned weight matrices ,  \
, 
the embedded original input sample , and a fixed scaling parameter 
the forward-pass is

 may contain more than one continuous modern Hopfield network,
which leads to an analog equation as~Eq.~\eqref{eq:Hs_combined} for~.

{\bf Hopular architecture and Modern Hopfield Networks.}
Deep Learning could not convince so far on small tabular datasets, 
on the other hand iterative learning algorithms, 
like Gradient Boosting methods, are the best-performing methods in this domain. 
Therefore, we introduce a DL architecture that is able to mimic and 
extend these iterative algorithms by reaccessing the whole training set and 
refining the current prediction in each layer. 
Modern Hopfield Networks directly access an external memory
in a content-based fashion as depicted in Eq.~\eqref{eq:main_iterate}.
Hopular populates this external memory in two different ways: (a) Hopular uses the training set as an external memory, and (b) Hopular uses the embedded feature representations of the original input sample as external memory.
During training, retrieval from the respective memory is learned whereas
the type of fixed point of the modern Hopfield network, 
as described in Section~\ref{sec:MHN}, specifies the type of retrieved pattern.
Additionally, modern Hopfield networks
can retrieve patterns with only one update~\---{}~see Theorem~\ref{th:oneUpdate}.

Furthermore, their exponential storage capacity (Theorem~\ref{th:storage}) makes it possible to
retrieve patterns from external memories with even hundreds of thousands
instances.
Because of these properties Hopular can mimic iterative learning algorithms e.g.\ such based on gradient descent, boosting, or feature selection that refine the current prediction by re-accessing the training set in contrast to other Deep Learning methods for tabular data. Both NPTs and SAINT consider feature-feature and sample-sample interactions via their respective attention mechanisms which solely use the result of the previous layer. In contrast, Hopular not only uses the result of the previous layer but also the original input sample and the whole training set.
For example, our method can implement gradient boosting with a boosting step at each layer.
The ability to mimic iterative learning algorithms that are known to perform specifically well on tabular data makes modern Hopfield networks a promising
approach for processing tabular data.
For the instantiation variant that we use for our experiments
the Hopfield module  identifies sample-sample relations
and can perform similarity searches like a nearest-neighbor search 
in the whole training data.
In the Appendix in Section~\ref{sec:iter_learn}
we give further intuition of how Hopular
can mimic iterative learning algorithms on the basis of two examples.

{\bf Hopular's Objective and Training Method.}
Hopular's objective is a weighted sum of 
the self-supervised loss for predicting masked features and
the standard supervised target loss.
In the following we explain the feature masking as well as the objective in more detail.

{\em Feature Masking.} 
We follow state-of-the-art Deep Learning methods 
like SAINT \citep{Somepalli:21}
and Non-Parametric Transformers (NPTs) \citep{Kossen:21}
that are tailored to tabular data and 
use BERT masking \citep{Devlin:19} of the input features.
Masked input features must be predicted during training.
Feature masking is an especially beneficial self-supervised approach when 
handling small datasets as it exerts a 
strong regularizing effect on the training procedure. 
The amount of masked features during training is 
determined by the masking probability, which is a hyperparameter of the model.
In Hopular, both features and targets can be masked during training,
while for inference only the target is masked.

{\em Objective.}
Hopular's objective is a weighted sum of the masked 
feature loss  
and the supervised target loss .
The overall loss  is

where  and  are the negative logloss 
in case of discrete attributes and the mean squared error 
in case of continuous attributes with  as a hyperparameter. 
In our default hyperparameter setting  is
annealed using a cosine scheduler starting at  with a final value of .
Another essential hyperparameter for Hopular is  
in Eq.~\eqref{eq:Hs} and Eq.~\eqref{eq:Hf}.
A small  retrieves a pattern close to the mean of
the stored patterns, while a large  
retrieves the stored pattern 
that is closest to the initial state pattern \citep{Ramsauer:21}.
For module  a large  value 
emphasizes a nearest-neighbor lookup mechanics.
For module  a large  value leads to less 
diluted features.
Thus, large  values seem to be beneficial for Hopular.
Experiments confirm this assumption 
(see Section~\ref{sec:experiments}).

{\bf Hopular Pseudocode}. Algorithm~\ref{alg:Hopular} shows the forward pass of Hopular for
an original input sample~.

\begin{algorithm}[h]
   \caption{Forward pass of Hopular}
   \label{alg:Hopular}
\begin{algorithmic}[1]
   \Require Hopfield modules  and , embedding layer ,
   summarization layer , number of features , 
    number of Hopular blocks 
   and original input sample 
   \State 
   \State 
   \For{ {\bfseries to} }
   \State 
   \State 
   \State 
   \State 
   \EndFor
   \State 
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\label{sec:experiments}

Since Deep Learning methods have already been successfully applied to larger tabular datasets \citep{Avati:18,Simm:18,Zhang:19,Mayr:18}
we want to know whether Hopular 
is competitive on small tabular datasets. 
In particular, we compare Hopular to XGBoost, CatBoost, LightGBM, and NPTs \citep{Kossen:21}.
Gradient Boosting has the lead on tabular data when excluding Deep Learning methods.
NPTs represent state-of-the-art Deep Learning methods
for tabular data,
as NPTs yielded very good results on small tabular datasets.

\subsection{Small-Sized Tabular Datasets}
\label{sec:experiments_uci}

In these experiments, we compare Hopular to other Deep Learning methods, XGBoost, CatBoost, and LightGBM on small-sized tabular datasets.

{\bf Methods Compared.}
We compare Hopular, XGBoost, CatBoost, LightGBM, NPTs, and other 24 machine learning
methods as described in \citep{Klambauer:17}.
The compared methods include 10 Deep Learning (DL) approaches.
Following \citep{Klambauer:17,Wainberg:16},
17 methods are selected from their respective method group 
as the model with the median performance over all datasets within each method group.
NPTs are used in a non-transductive setting for a fair comparison. 

{\bf Hyperparameter Selection.} 
All hyperparameters are selected on seperate validation sets. For NPTs we perform hyperparameter search as in Table~\ref{tab:hyp_npt}. This includes the hyperparameters that have already been successfully used in \citep{Kossen:21} on small- and medium-sized tabular datasets. 
This selection also serves as a constraint on the computational resources invested for Hopular.
For XGBoost, CatBoost, and LightGBM, we apply the same Bayesian hyperparameter optimization 
procedure as described in~\citep{ShwartzZiv:21}. For LightGBM we use the default hyperparameter ranges as specified by \texttt{hyperopt-sklearn}~\citep{Komer:14}.
Section~\ref{sec:hyperparameter_selection} of the Appendix describes the hyperparameter selection in more detail.

{\bf Datasets.} 
Following \citep{Klambauer:17}, 
we consider UCI machine learning repository datasets 
with less than or equal to 1,000 samples as being {\em small}.
We select 21 of these datasets and give an overview in Table~\ref{tab:small-sized datasets}.
The datasets themselves as well as the train/test splits are taken from~\citep{Fernandez:14}.
A detailed explanation of the dataset selection process as well as a description of the datasets can be found in Section~\ref{sec:dataset_explanation} of the Appendix.

\begin{table}[ht]
    \caption{Median rank of compared methods across 
    the datasets of the UCI machine learning repository. 
    Methods are ranked for each dataset according to the accuracy on the respective test set.
    Hopular achieves the lowest median rank of , therefore is the best
    performing method across the considered UCI datasets. The complete list can be seen in Table~\ref{tab:uci_experiments_complete} of the Appendix.\label{tab:uci_experiments}}
    \begin{center}
        \begin{tabular}{lS[table-format=2.1]lS[table-format=2.1]}
            {\bf Method} & {\bf Rank} & {\bf Method} & {\bf Rank} \\
            \toprule
            Hopular (DL)                     &  7.5                             & CatBoost                            & 14.0 \\
            {\multirow{2}{*}{\quad }} & {\multirow{2}{*}{}} & LightGBM                            & 14.5 \\
                                          &                                     & {\multirow{2}{*}{\quad }} & {\multirow{2}{*}{}}\\
            Non-Parametric Transformers (DL) & 11.0                             &                                     &  \\
            XGBoost                          & 12.0                             & Stacking (Wolpert)                  & 28.0
        \end{tabular}
    \end{center}
\end{table}

{\bf Results.} Table~\ref{tab:uci_experiments} shows the median rank of all compared methods across 
the datasets of the UCI machine learning repository
(see Table~\ref{tab:uci_experiments_complete} of the Appendix for the complete list).
Methods are ranked for each dataset according to the accuracy on the respective test set.
17 method groups have been compared previously \citep{Wainberg:16}, to which
we add XGBoost \citep{Chen:16}, CatBoost \citep{Dorogush:17,Prokhorenkova:18}, LightGBM \citep{Ke:17},
NPTs \citep{Kossen:21}, Self-Normalizing Networks \citep{Klambauer:17}, and our Hopular.
Deep Learning methods are indicated by ``(DL)'' and are not grouped.
Hopular has a median rank of , followed by Support Vector Machines with , 
while NPTs, XGBoost, CatBoost, and LightGBM
have a median rank of , , , and  respectively.
Hopular with modern Hopfield networks as memory performs better than
other Deep Learning methods 
and in particular better than the closely-related NPTs.
{\bf Across the considered UCI datasets,
Hopular is the best performing method.} 

\subsection{Medium-Sized Tabular Datasets}
\label{sec:experiments_tabular}

In these experiments, we compare Hopular to other Deep Learning methods,
XGBoost, CatBoost, and LightGBM on medium-sized tabular datasets.
In \citep{ShwartzZiv:21}, the authors show that XGBoost outperforms various
Deep Learning methods that are designed for tabular data on
datasets that did not appear in the original papers.
We want to know whether XGBoost still has the lead on
these medium-sized datasets.

{\bf Methods Compared.}
We compare Hopular, NPTs, XGBoost, CatBoost, and LightGBM.
NPTs are used in a non-transductive setting for a fair comparison.

{\bf Hyperparameter Selection.} 
All hyperparameters are selected on seperate validation sets. For NPTs we perform hyperparameter search as in Table~\ref{tab:hyp_npt}. This includes the hyperparameters that have already been successfully used in \citep{Kossen:21} on small- and medium-sized tabular datasets.
This selection also serves as a constraint on the computational resources invested for Hopular.
For XGBoost, CatBoost, and LightGBM, we apply the same Bayesian hyperparameter optimization 
procedure as described in~\citep{ShwartzZiv:21}. For LightGBM we use the default hyperparameter ranges as specified by \texttt{hyperopt-sklearn}~\citep{Komer:14}.
Section~\ref{sec:hyperparameter_selection} of the Appendix describes the hyperparameter selection in more detail.

{\bf Datasets.}
We select the datasets and dataset splits of \citep{ShwartzZiv:21}, 
where XGBoost performs better than Deep Learning methods that have been
designed for tabular data.
We extend this selection by two datasets for regression: (a) \textit{colleges} was already
used for other Deep Learning methods for tabular data~\citep{Somepalli:21}, and
(b) \textit{sulfur} is publicly available and fits with its 10,082 instances well into
the existing collection of medium-sized datasets.
Table~\ref{tab:medium-sized datasets} gives an overview of the medium-sized datasets.
A detailed description of the datasets can be found in Section~\ref{sec:dataset_explanation} of the Appendix.

\begin{table}[h]
\centering
    \caption{Results of all compared methods on the subset of medium-sized tabular datasets~\citep{ShwartzZiv:21}. For classification tasks (\texttt{C}), the {\em accuracy} is reported. For regression tasks (\texttt{R}), the {\em mean squared error} multiplied by a factor of  is reported. The reported deviations are the corresponding {\em standard error of the mean}. All values are computed on the respective test sets, averaged over {\em three}~replicates.\label{tab:intel_experiments_results}}
    \begin{center}
        \begin{tabular}{
        l
        S[table-format=2.2(1),separate-uncertainty]
        S[table-format=2.2(1),separate-uncertainty]
        S[table-format=2.2(1),separate-uncertainty]
        S[table-format=2.2(1),separate-uncertainty]
        S[table-format=2.2(1),separate-uncertainty]}
            {\bf Dataset} & {\bf Hopular} & {\bf NPTs} & {\bf XGBoost} & {\bf CatBoost} & {\bf LightGBM} \\
            \toprule
            sulfur (\texttt{R})    &  1.04(02) &  1.24(02) &  1.23(00) &  1.06(01) &  1.16(01) \\
            colleges (\texttt{R})  & 21.18(09) & 25.67(23) & 30.47(00) & 26.40(09) & 25.64(09) \\
            eye (\texttt{C})       & 53.56(48) & 53.21(12) & 57.43(00) & 56.35(05) & 57.34(28) \\
            gesture (\texttt{C})   & 71.20(19) & 67.83(06) & 68.05(00) & 68.86(21) & 69.01(09) \\
            blastchar (\texttt{C}) & 80.05(11) & 79.98(11) & 76.78(00) & 80.13(12) & 79.92(21) \\
            shrutime (\texttt{C})  & 86.12(09) & 85.62(07) & 84.58(00) & 86.39(04) & 86.18(02)
        \end{tabular}
    \end{center}
\end{table}

\textbf{Results.} Table~\ref{tab:intel_experiments_results} reports the results
of Hopular, NPTs, XGBoost, CatBoost, and LightGBM on the medium-sized datasets.
The evaluation procedure is from \citep{ShwartzZiv:21}.
Hopular is the best performing method on 3 out of the 6 datasets.
The runner-up method, CatBoost, is twice the best method, whereas XGBoost once.
The biggest performance difference is achieved by Hopular on the two regression datasets,
where the capabilities of an external memory really shine.
Directly deriving the underlying function for regression datasets may be a difficult task,
especially in absence of abundant data.
Hopular is able to mitigate this shortcoming
by incorporating local neighbourhood information and
iteratively refining its current prediction by memory lookups.
Over the 6 datasets, NPTs and XGBoost have a median rank of 4.5,
CatBoost and LightGBM of 2.5 and 2, respectively,
and Hopular has a median rank of 1.5.
{\bf On average over all 6 datasets, Hopular performs better than 
NPTs, XGBoost, CatBoost, and LightGBM.}
We also found that our method needs only a fraction of the memory compared to NPTs which can be seen in Table~\ref{tab:memory_footprint}. We also added runtime estimates in Table~\ref{tab:training_time}.

\section{Conclusion}
Hopular is a novel Deep Learning architecture where every layer is equipped
with an external memory. This enables Hopular to mimic standard iterative learning
algorithms that refine the current prediction by re-accessing the training set.
We validated the usefulness of this property both on small- and
medium-sized tabular datasets. Hopular is the best performing method
across a broad selection of specifically challenging small-sized UCI
datasets. Additionally, Hopular is the best-performing method on
medium-sized tabular datasets among which CatBoost and LightGBM achieved very competitive
results.
This makes Hopular a strong contender to current state-of-the-art methods like Gradient Boosting and other Deep Learning methods specialized in small- and medium-sized datasets.

\begin{ack}
The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. IARAI is supported by Here Technologies. We thank the projects AI-MOTION (LIT-2018-6-YOU-212), AI-SNN (LIT-2018-6-YOU-214), DeepFlood (LIT-2019-8-YOU-213), Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), AIRI FG 9-N (FWF-36284, FWF-36235), ELISE (H2020-ICT-2019-3 ID: 951847). We thank Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, Software Competence Center Hagenberg GmbH, T\"{U}V Austria, Frauscher Sensonic and the NVIDIA Corporation.
\end{ack}

\bibliography{memory}
\bibliographystyle{ml_institute}

\newpage{}
\appendix

\section{Appendix}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}

\subsection{Architecture} \label{sec:architecture_appendix}

\begin{figure}[h]
    \centering
    \includegraphics[scale=1.12]{figures/hopular_embedding.pdf}
    \caption{Embedding Layer. All attributes of an original input sample
    are mapped to an -dimensional embedding space. The position of an
    attribute within a sample and the attribute type are conserved by separate -dimensional embeddings. All
    three embedding vectors are summed and serve as the final
    representation of an input attribute. The input sample is represented
    by the concatenation of all its attribute representations.}
    \label{fig:hopular_embedding}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=1.12]{figures/hopular_summarization.pdf}
    \caption{Summarization Layer. The current prediction vector on the right is mapped to the final prediction vector on the left by separately mapping each current attribute
    prediction to its respective final prediction. This final prediction vector lives
    in the same space as the original input sample and is used for the 
    computation of the respective losses.}
    \label{fig:hopular_summarization}
\end{figure}

\subsection{Datasets} \label{sec:dataset_explanation}
\subsubsection{UCI Dataset Selection}
To assess the performance of Hopular and other Deep Learning methods 
on small datasets, 
we select a subset of 21 datasets from \citep{Klambauer:17}. 
The sizes of these datasets range from 200 to 1,000 samples.
We put the focus on smaller sizes, therefore we select
13 datasets with 500 samples or less.
Additionally, we select four datasets with 500 to 750 samples and 
four dataset with 750 to 1,000 samples. 
Small datasets typically have small test sets, 
which introduce a high variance in their evaluations.
This is especially true if they are overly small or unbalanced.
Furthermore, some test sets seem to be not sampled iid from the whole population. 
Thus, the method evaluation may be highly dependent on the chosen train/test split and
performance estimates may be skewed.
Problematic datasets in \citep{Klambauer:17} are characterized
by having a range of accuracy values across well established methods of greater or equal 
We exclude the problematic datasets 
{\em seeds}, {\em spectf}, {\em libras}, {\em dermatology}, {\em arrythmia}, 
and {\em conn-bench-vowel-deterding}.
The dataset {\em spect} is excluded as its description in \citep{Fernandez:14} 
is in conflict with the available UCI version regarding the number of attributes and samples.
The dataset
{\em heart-hungarian} is excluded as the dataset description 
is insufficient to distinguish between categorical and continuous attributes,
which is required by some methods.
Since {\em breast-cancer-wisc} is practically solved ( accuracy), it is excluded as
it does not allow to distinguish the performances of the compared methods.
We drop {\em heart-va}, 
since the best reported method has only a low accuracy of .

\pagebreak{}
\subsubsection{Small-Sized Dataset Description}

\begin{table}[ht]
    \caption{Overview of small-sized datasets with their number of instances, number
    of continuous features, and number of categorical features. All small-sized datasets are classification~tasks.\label{tab:small-sized datasets}}
    \begin{center}
        \begin{tabular}{lS[table-format=4.0]S[table-format=2.0]S[table-format=2.0]}
               {\bf Dataset} & {\makecell{\bf Size\N0.0010.1128\beta_L = (0.9, 0.999)\epsilon = 1e{-6}\alpha = 0.5k = 60.001p_{i}p_{h}p_{o}\betap_{i}p_{h}p_{o}10^{\left\{0,2,3\right\}}10^{\left\{0,2,3\right\}}10^{\left\{0,2,3\right\}}10^{\left\{0,2,3\right\}}10^{\left\{0\right\}}\beta1001000\gamma10\gamma0.5H_shedMh = d \cdot e / M\beta_L = (0.9, 0.999)\epsilon = 1e{-6}\alpha = 0.005k = 1[-7, 0][1, 10][0.2, 1][0.2, 1][0.2, 1][-16, 2]\{0, \text{Log-Uniform }[-16, 2] \}\{0, \text{Log-Uniform }[-16, 2] \}\{0, \text{Log-Uniform }[-16, 2] \}1000\text{Log-Uniform}[-7, 0][e^{-7}, e^0][-5, 0][1, 20][0, 25][ \log1, \log10 ][0, 1][1, 20]1000[\log 0.0001, \log 0.5] - 0.0001[1, 11][2, 121][\log 0.001, \log 5] - 0.0001[\log 1, \log 100][0.5, 1][0.5, 1][0.5, 1][\log 0.0001, \log 1][\log 1, \log 4]\{ \text{gbdt, dart, goss}\}10007.5H_s80.00\text{{\raisebox{.2ex}+}}\{(\Bz_1,\By_1),\ldots,(\Bz_N,\By_N)\}\Bz_i\BZ = (\Bz_1,\ldots,\Bz_N)\By_i\BY=(\By_1,\ldots,\By_N)k(\Bz_i,\Bz)\Bg\By\Bz\Bz_i \Bz_i^T \Bz_i = \NRM{\Bz_i}^2
=1k\BW_{\BX}\BW_{\Bxi}\BYy \in \{ -1, +1 \}\beta^{-1}\BY\{ \By_1, \cdots , \By_N \}\mathrm{lse}\Bxi$ comes from the previous layer.

These are two additional examples among the standard iterative learning
algorithms which Hopular can mimic.

\subsection{Source code}
Source code is available at: \url{https://github.com/ml-jku/hopular}

\end{document}
