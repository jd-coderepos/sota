\def\year{2020}\relax
\documentclass[letterpaper]{article} 
\usepackage{aaai20}
\usepackage{times} 
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{graphicx}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\usepackage{multirow}
\usepackage{colortbl} 
\usepackage{arydshln}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{subcaption}
\usepackage{booktabs}
\newcommand {\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand {\citep}{\cite}
 \pdfinfo{
/Title (Gated Convolutional Networks with Hybrid Connectivity for Image Classification)
/Author (Chuanguang Yang, Zhulin An, Hui Zhu, Xiaolong Hu,  Kun Zhang, Kaiqiang Xu, Chao Li, Yongjun Xu)
}

\setcounter{secnumdepth}{0}

\setlength\titlebox{2.5in}
\title{Gated Convolutional Networks with Hybrid Connectivity for Image Classification}

\author{Chuanguang Yang,\textsuperscript{\rm 1,2} Zhulin An,\textsuperscript{\rm 1}\thanks{Corresponding author} Hui Zhu,\textsuperscript{\rm 1,2} Xiaolong Hu,\textsuperscript{\rm 1,2}  \\ \bf \Large{Kun Zhang,\textsuperscript{\rm 1,2} Kaiqiang Xu,\textsuperscript{\rm 1,2} Chao Li,\textsuperscript{\rm 1} Yongjun Xu\textsuperscript{\rm 1}}\\ 
\textsuperscript{\rm 1}Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\\ 
\textsuperscript{\rm 2}University of Chinese Academy of Sciences, Beijing, China\\
\{yangchuanguang, anzhulin, zhuhui, huxiaolong18g, zhangkun17g, xukaiqiang, lichao, xyj\}@ict.ac.cn 
}

  \begin{document}
 	
 	\maketitle
 	
 	\begin{abstract}
 		We propose a simple yet effective method to reduce the redundancy of DenseNet by substantially decreasing the number of stacked modules by replacing the original bottleneck by our SMG module, which is augmented by local residual. Furthermore, SMG module is equipped with an efficient two-stage pipeline, which aims to DenseNet-like architectures that need to integrate all previous outputs, i.e., squeezing the incoming informative but redundant features gradually by hierarchical convolutions as a hourglass shape and then exciting it by multi-kernel depthwise convolutions, the output of which would be compact and hold more informative multi-scale features. We further develop a forget and an update gate by introducing the popular attention modules to implement the effective fusion instead of a simple addition between reused and new features.  Due to the \textbf{H}ybrid \textbf{C}onnectivity (nested combination of global dense and local residual) and \textbf{G}ated mechanisms, we called our network as the \textbf{HCGNet}. Experimental results on CIFAR and ImageNet datasets show that HCGNet is more prominently efficient than DenseNet, and can also significantly outperform state-of-the-art networks with less complexity. Moreover, HCGNet also shows the remarkable interpretability and robustness by network dissection and adversarial defense, respectively. On MS-COCO, HCGNet can consistently learn better features than popular backbones.
 	\end{abstract}
 	\section{Introduction}
 	Deep convolutional neural networks (CNNs) are becoming more and more efficient in parameter and computation without sacrificing the performance owing to novel architectures design. ResNet \cite{he2016deep} introduces the residual connectivity to implement the addition of the input and output features for each micro-block. DenseNet \cite{huang2017densely} holds the dense connectivity by changing skip connections from addition to concatenation. Both of their feature aggregation connectivities can not only encourage feature reuse, but also ease the training problems. For a detailed comparison, dense connectivity is more effect for feature exploitation and exploration but exists a certain redundancy, while residual connectivity contributes to efficient feature reuse by parameter sharing mechanism and thus leads to low redundancy, but lacks the capability of feature preservation and exploration. To enjoy their advantages and avoid inherent limitations, many networks combine them to build a more effective aggregation topology, such as DPN \cite{chen2017dual}, MixNet \cite{wang2018mixed} and AOGNet \cite{AOGNets}. Different from them, we develop a hybrid connectivity (Figure \ref{fig:figure1}) with nested aggregation that facilitates feature flow by dense connectivity for global channel-wise concatenation of outputs produced by all precedent modules (blue links in Figure \ref{fig:figure1}) and residual connectivity for local element-wise addition within the module (red links in Figure \ref{fig:figure1}).
 	\begin{figure}[tbp]  
 		\centering  
 		\includegraphics[width=.95\columnwidth]{{5011.figure-hy}.pdf}
 		\caption{The diagram of a hybrid block including  modules, where . The symbol "" and "" denote element-wise addition and channel-wise concatenation among multiple feature maps, respectively.}  
 		\label{fig:figure1}
 	\end{figure}
 	
 Our main motivation for this pattern design originates from reducing the redundancy of dense connectivity. As the depth of network linearly increases, the number of skip connections and required parameters grow by a rate of , where  denotes the number of stacked modules under dense connectivity. Meanwhile, early superfluous features which have few contributions are transferred quadratically to subsequent modules. So one simple method to reduce redundancy is to decrease the number of modules directly, but it can attenuate the representational power of features and then deteriorate the performance. Thus we develop a novel module by embedding the residual connectivity to assist feature learning within the local module. Experimentally, the number of our proposed modules under dense connectivity can be quite fewer than that of classical modules in the dense block but without sacrificing the performance. 
 
 For further adaptation with hybrid connectivity, we instantiate the basic module that includes a \textbf{squeeze cell (cell 1 in Figure \ref{fig:figure1})} for transforming the input to a compact feature map, and a \textbf{multi-scale excitation cell (cell 2 in Figure \ref{fig:figure1})} to further extract multi-scale features by multi-kernel convolutions. It is widely known that convolution builds pixel relationship in a local neighborhood, which leads to ineffective modeling of long-range dependency. To fully address this issue, we develop an update gate to model the global context features from more informative multi-scale features. Moreover, we locate a forget gate on the residual connection to capture channel-wise dependency for decaying the reused features produced by cell 1. Finally, global context features are added to the reused feature map of each spatial position to form the output, which can not only promote effective feature exploration but also retain the capability of feature re-exploitation to some extent. Moreover, both forget gate and update gate are lightweight and general plug-ins, which can be integrated into any CNNs with negligible overheads.
 
 We perform extensive experiments across the three highly competitive image classification datasets: CIFAR-10/100 \cite{krizhevsky2009learning}, and ImageNet (ILSVRC 2012) \cite{deng2009imagenet}. On CIFAR datasets, HCGNets outperform state-of-the-art both human-designed and auto-searched networks but only requiring extremely fewer parameters, e.g., HCGNet-A3 obtains the better result than the most competitive NASNet-A \cite{zoph2018learning} with  fewer parameters. On ImageNet datasets, it also consistently obtains the best accuracy, interpretability, robustness based on classification and transferability to object detection as well as segmentation among the widely used networks with least complexity, e.g., HCGNet-B outperforms previous SOTA AOGNet across a broad range of tasks with similar complexity. 
 	
 	
 	\section{Related Work}
 	
 	\subsubsection{Improvements of ResNet and DenseNet.}ResNeXt \cite{xie2017aggregated} outperforms ResNet with less overheads since it adopts 33 group convolutions in residual blocks. Afterwards, group convolutions become popular in efficient CNNs design due to the properties of lower parameter and computational cost, including our HCGNets. Wide ResNet \cite{Wide} show that increasing width while decreasing depth of residual networks can surpass very deep counterparts, meanwhile tackling the problems of slow training and weakened feature reuse. By representing the multi-scale features and widening the receptive fields (RF) within the residual block, Res2Net \cite{gao2019res2net} outperforms the other backbones across a broad range of tasks. Multi-scale information has been widely demonstrated a effective way to improve the performance, our HCGNet also constructs the multi-scale features by multi-branch convolutions.  
 	
 	It is widely known that DenseNet has a certain redundancy, thus a typical practice is sparsification. Log-DenseNet \cite{hu2017log} and SparseNet \cite{zhu2018sparsely} regularly conduct a sparse rather than full aggregation of all previous outputs, which change the number of connections from linear to be logarithmic in the overall topology. Learned group convolutions are adopted in CondenseNet \cite{huang2018condensenet} to automatically prune unimportant channels for the incoming feature map based on channel-wise L1-norm. However, excessive sparsification affects the superiority of collective learning. Thus we only decrease the number of modules under dense connectivity to reduce redundancy, which is empirically more effective than sparsification.
 	
 	\subsubsection{Combinations of ResNet and DenseNet.} To enjoy the advantages and avoid drawbacks of both two connectivities, many combinations have proposed. DPN adopts dual path architectures, which can facilitate effective feature reuse by residual path and  feature exploration by dense path in parallel. MixNet blends two connectivities to implement feature aggregation with more flexible positions and sizes, further ResNet, DenseNet and DPN can be treated as particular cases of MixNet. Recently proposed AOGNet utilizes AND-OR Grammar to generate CNNs by parsing feature map as a sentence, where AND-node denotes channel-wise concatenation and OR-node denotes element-wise addition. It demonstrates that the compositional and hierarchical aggregation in AOGNet is more effective than cascade-based way in DPN. Moreover, addition and concatenation as the meta-operations are also widely applied in the field of neural architecture search, such as NASNet, PNASNet \cite{liu2018progressive} and AmoebaNet \cite{real2019regularized}. Extensive experiments indicate that the nested way for feature aggregation in our HCGNets perform the best. 
 	
 	\subsubsection{Attention Mechanisms}Attention has been widely applied in computer vision, e.g., image classification \cite{wang2017residual}. SENet \cite{hu2018squeeze} introduces a lightweight gate to capture channel-wise dependencies for rescaling channel features. SKNet \cite{li2019selective} further employs a dynamic kernel selection attention for weighted multi-scale features fusion, which is inspired by InceptionNets \cite{szegedy2017inception}. Beyond channel, CBAM \cite{woo2018cbam} also constructs a spatial attention map to recalibrate spatial features. To capture long-range dependency, GCNet \cite{cao2019gcnet} simplifies non-local block \cite{wang2018non} to implement query-independent context modeling based on single branch information. Different from them in roles or mechanisms, we build a forget gate to capture channel-wise dependency for decaying the reused features, while an update gate fully models the global context features from multi-scale information.
 	\section{Revisiting ResNet and DenseNet}
 	We revisit the classical ResNet and DenseNet with their individual residual connectivity and dense connectivity, and further investigate their mechanisms of parameter sharing and feature learning. Finally, we analyse the overall efficiency of ResNet and DenseNet.    
 	
 	\subsection{Parameter Sharing}
 	Intuitively, residual connectivity implicitly accompanies a parameter sharing mechanism between the reused and newly extracted features by processing their mixed features. We now formally describe why the parameter sharing mechanism can take place in residual connectivity but not in dense connectivity. Concretely, we use  to denote the bottleneck unit. Consider the input feature map  to the -th residual block, corresponding formula is as follows:
 	
 	Where  can be considered as the reused feature map,  and  refer to convolutional weights and newly extracted feature map, respectively.  represents  for simplicity. Afterwards,  becomes a new input for the next residual block to proceed the transformation:
 	
 	In the -th residual block,  and  are shared with the same  and operations. Similar analysis about dense connectivity is exhibited as follows. Output of the -th module under dense connectivity can be regarded as the concatenation of input  and newly extracted feature map  along the channels:
 	
 	Then, the next module receives  and conducts the following transformation:
 	
 	
 	In the -th dense block,  and  are not shared with the same  because of the different locations of feature space between reused and newly extracted feature maps.
 	
 	\subsection{Feature Learning}
 	The final output of residual block is the element-wise addition of input  and newly extracted feature maps. This addition pattern facilitates efficient feature reuse without increasing the size of feature map thus reducing parameter redundancy. But one potential fact is that too many aggregations by addition may collapse the feature representation and thus impede the information flow, hence some early informative features may be lost inevitably. Moreover, parameter sharing mechanism may damage the capability of exploring new features.
 	
 	Subsequently proposed DenseNet develops a global dense connectivity, where the output feature map of each preceding module flows to the all subsequent modules directly. Different from the element-wise addition, input and newly extracted feature maps are combined by concatenation along the channels. Thus dense connectivity can transfer the early feature maps to later modules, which preserves the all preceding feature information and facilitate the full exploitation of existing features. Moreover, various modules with different weights conduct a collective learning for the same features, which can promote effective feature exploration.
 	\subsection{Overall Efficiency} 	
 	It is widely known that DenseNet-100 with 0.8M parameters slightly outperforms ResNet-1001 with 10.2M parameters on CIFAR10 dataset. The explicit parameter gap is that DenseNet-100 is quite shallower than ResNet-1001 due to the more effective feature exploitation and exploration capabilities produced by collective learning, while ResNet mainly depends on increasing depth to improve the representational power of features. Empirically, DenseNet can also have extremely few number of filters in each convolutional layer due to the collective learning mechanism that further improve the efficiency. However, one potential weakness of dense connectivity is the redundancy of repeated extraction with the same features. In this case, early features flow to all subsequent layers, even if they have few contributions. By contrast, residual connectivity has a relatively low redundancy due to the parameter sharing mechanism.
 	
 	\section{Networks Architecture}
 	\subsection{Hybrid Connectivity Pattern}
 	We develop a hybrid connectivity pattern, which can enjoy the effective feature learning and few filters of each module from global dense connectivity as well as efficient feature reuse by parameter sharing from local residual connectivity. Figure \ref{fig:figure1} illustrates this pattern within the hybrid block schematically. Note that hybrid connectivity pattern exists in the hybrid block which consists of  () modules. To match the definition of growth rate in DenseNet, each module produces one feature map with  channels. The basic module consists of successive two cells, which we call them as cell 1 and cell 2, respectively. Globally, input of each module is a concatenation of all feature-maps produced by preceding modules and transferred by dense connectivity. Locally, residual connectivity provides a shortcut that allows the output of cell 1 bypassing cell 2 and then being added to the new features generated by cell 2 to form the output.
 	
 	\subsection{Instantiation of Basic Module}
 	To orchestrate our hybrid connectivity, we design a basic SMG module which includes a \textbf{Squeeze cell} (cell 1), a \textbf{Multi-scale excitation cell} (cell 2) and \textbf{Gate mechanisms}. Unless specified otherwise, each convolution is bound a pre-activation, which refers to the three consecutive operations: batch normalization (BN)-rectified linear unit (ReLU)-Conv.
 
 	\begin{figure*}
 		\centering 
 		
 		\begin{subfigure}[t]{0.30\textwidth}
 			\centering
 			\includegraphics[width=\textwidth]{{5011.figure-meta}.pdf}
 			\caption{SMG Module}
 			\label{SMG}
 		\end{subfigure}
 		\begin{subfigure}[t]{0.45\textwidth}
 			\centering
 			\includegraphics[width=\textwidth]{{5011.figure-ugate}.pdf}
 			\caption{Update Gate}
 			\label{Update}
 		\end{subfigure}
 	\begin{subfigure}[t]{0.16\textwidth}
 		\centering
 		\includegraphics[width=\textwidth]{{5011.figure-fgate}.pdf}
 		\caption{Forget Gate}
 		\label{Forget}
 	\end{subfigure}

 		\label{Gate} 
 		\caption{Illustrations of SMG module, update gate and forget gate. In all figures,  and  denote broadcast element-wise addition and multiplication, respectively. We employ feature dimensions to describe the flow of feature maps for better understanding. Note that spatial size  when default stride  of 33 GConv in Figure \ref{SMG}.} 
 	\end{figure*} 
 	
 	\subsubsection{Squeeze Cell.} This cell which locates at the beginning of SMG module is responsible for generating the compact feature map from input to improve parameter and computational efficiency for subsequent processing. 11 convolution is firstly adopted for changing the number of input channels  to , where  can be reckoned as a width multiplier which is mostly used to reduce the number of channels, i.e.  , and  denotes the number of final output channels of squeeze cell. Then, 33 group convolution (GConv) with  groups proceeds to squeeze the features by reducing the number of channels from  to , where  needs to be divisible by . Moreover, it can also play a down-sampling by 33 kernel with stride .
 	\subsubsection{Multi-scale Excitation Cell.} Squeezed feature map enters this cell for multi-scale excitation by multi-branch convolutions with different kernel sizes. Note that the costs of parameter and computation are extremely cheap because of the few input channels, and the size of feature map throughout this cell is unchanged. To further improve efficiency, we adopt 33 and 55 depthwise convolutions (DWConv) with 1 and 2 paddings, respectively. Moreover, dilation convolution \cite{DBLP} with a kernel size of 33 and a dilation size of 2 is used to approximate 55 kernel for better trade-off between efficiency and performance. The output of this cell is two-branch feature maps produced by 33 and 55 DWConvs, respectively. 
 	
 	
 	\subsubsection{Update Gate.}To capture long-range dependency , we utilize update gate to model the global context features from multi-scale information. Figure \ref{Update} shows the overall details about the update gate, which can be sequentially summarized for 3 stages: spatial attention, pooling and channel attention. 
 	
 	\textbf{\emph{spatial attention and pooling:}} We perform a global context modeling for calculating spatial-wise weights of each position. For the given feature map , a 11 convolutional filter shrinks it along channel dimensions to a spatial attention map , an then a softmax function normalizes it to obtain the final spatial attention map , each element of which is as follows:
 	
 	We employ global attention pooling via weighted averaging with  to shrink the global spatial information and generate the global context feature map . The -th channel of  is as follows:  
 	
 	Here,  denotes element multiplication. Based on the above framework,  can also be obtained by input feature map .
 	
 	\textbf{\emph{channel attention:}} To maintain the integrity of information, we concatenate  and  as the input. Then it is transformed to a hidden representation , which is always a compact feature map by setting a reduction ratio  for better efficiency. This is achieved by a fully connected (FC) layer with non-linearity:  
 	
 	Where  is the batch normalization,  and  denotes the weights and biases of FC layer.
 	
 	It is noteworthy that we adopt  rather than ReLU as our non-linearity function. For the one side, ReLU inevitably destroys feature representational power especially in low-dimensional space to a great extent, while  preserves information by a smoother way. For the other side, although it is widely known that  is more prone to cause gradient vanish as the increasing depth of CNN, this problem could not occur in our HCGNets because of the hybrid connectivity that can significantly strength the gradient back-propagation. Experimental evidence also proves that  is more effective than ReLU in our HCGNets. 
 	
 	Two-branch FC layers act on fusion representation  to generate two intermediate channel attention maps  and :
 	
 	Where , and , denotes the weights and biases of two FC layers. Then a simple softmax function conducts a normalization between  and  to produce the two final channel attention maps  and :
 	
 	
 	 and  can be regarded as the proportions of aggregating multi-scale global context features. Weighted fusion of  and  is the output of update gate:
 	
 	Where  is the -th channel of the output .
 	
 	\subsubsection{Forget Gate.}
 	To decay the reused feature map by channel-wise weights, we locate a forget gate (see Figure \ref{Forget}) on the residual connection before information fusion. It can also be sequentially summarized for 3 stages: spatial attention, pooling and channel attention. 
 	
 	\textbf{\emph{spatial attention and pooling:}} For the given feature map , we perform the global attention pooling as same as update gate, thus a channel descriptor  can be obtained. 
 	
 	\textbf{\emph{channel attention:}} To meet the requirement of weighted decay for each channel, the final output of each channel weight should be within , thus we refer SE block, which stacks two continuous FC layers as a bottleneck and is ended by sigmoid function. Different from SE block, we insert a batch normalization layer for easing optimization and replace ReLU with  as our non-linearity. In short, the sequent transformations are as follows for the input :
 	
 	Where  is the sigmoid function, , ,  and .  is the bottleneck ratio and  is the final channel attention map.
 	
 	\subsubsection{Information Fusion.} For any given feature map entering SMG module, squeeze cell firstly condenses it to a compact feature map denoted by . Then  enters multi-scale excitation cell and generate two-branch outputs  and  by 33 and 55 DWConvs, respectively. Since then,  can be regarded as the reused features, while  and  are the newly extracted features. An update gate integrates  and  to model a global context feature map  and we aggregate it to the decayed  of each spatial position by addition to build the final output . It can be observed that we maintain the magnitude of new features unchanged while decaying reused features, which can facilitate the effective feature exploration and retain the capability of feature re-exploitation to some extent.
 	\begin{figure}[tbp]  
 		\centering  
 		\includegraphics[width=.95\columnwidth]{{5011.figure-macro}.pdf}
 		\caption{A HCGNet with three hybrid blocks, where each green box denotes SMG module.}  
 		\label{macro}
 	\end{figure}
 
 	\subsection{Macro-architecture.} As shown in Figure \ref{macro}, at the beginning of HCGNet is a stem, which is a composite function to process the initial input images. Then multiple hybrid blocks are stacked with various spatial stage. Between two adjacent hybrid blocks, we adopt a transition layer to perform down-sampling and connectivity truncation. After the final hybrid block, a global average pooling attached with a softmax classifier calculates the probabilities of various categories.
 	
 	\begin{table}[tbp]
 		\centering
 		\caption{HCGNet-B network architecture for ImageNet classification. Each row describes the stage, modules information and input resolution (IR).} 
 		
 		\begin{tabular}{ccc}  
 			
 			\toprule
 			Stage&Module&IR \\
 			\midrule
 			\multirow{2}{*}{Stem}&[33 Conv-BN-ReLU]3&224224\\
 			&33 max pool&112112\\
 			\midrule
 			Hybrid Block&SMG3 ()&5656\\
 			Transition&SMG1&5656\\
 			Hybrid Block&SMG6 ()&2828\\
 			Transition&SMG1&2828\\
 			Hybrid Block&SMG12 ()&1414\\
 			Transition&SMG1&1414\\
 			Hybrid Block&SMG8 ()&77\\
 			\midrule
 			\multirow{2}{*}{Classification}&global average pool&11\\
 			&1000D FC, softmax&-\\
 			\bottomrule
 		\end{tabular}
 		\label{arch}
 	\end{table}
 	
 	Both hybrid block and transition layer adopt SMG modules but with different hyperparameter settings. We only stack one SMG module to build each transition layer and a compression factor  is utilized to reduce the number of channels, i.e, . For each SMG module, we set ,  and  in hybrid blocks, as well as set , ,  and  in transition layers. Note that we apply the standard convolutions in transition layers for best capability of feature extraction and group convolutions in hybrid blocks for better trade-off between efficiency and performance. Compared with the hybrid block, we set less multiplier  and larger reduction ratios  and  for better efficiency due to the more channels of feature maps in transition layers. 
 	
 	Specifically, we construct several networks to act on the image classification across the CIFAR and ImageNet datasets. For CIFAR, we adopt a 33 standard convolution with stride 1 as the stem that the number of output channels is twice the growth rate of the first hybrid block. And we build three networks with various model specifications: HCGNet-(8,8,8)-(=12,24,36)(A1), HCGNet-(8,8,8)-(=24,36,64)(A2) and HCGNet-(12,12,12)-(=36,48,80)(A3). Formally, the first -tuple indicates that there are  hybrid blocks, where each figure denotes the number of SMG modules in the corresponding hybrid block. The second -tuple denotes  growth rates of  hybrid blocks, respectively. For ImageNet, the stem consists of three contiguous 33 Conv-BN-ReLU layers (stride 2 for the first layer) with 32, 32, 64 output channels, and attached by a 33 max pooling with stride 2. We construct two networks: HCGNet-(3,6,12,8)-(=32,48,64,96)(B, as Table \ref{arch}) and HCGNet-(6,12,18,14)-(=48,56,72,112)(C).
 	
 	\section{Experiments}
 	\subsection{Experiments on CIFAR}
 	\begin{table}[tbp]
 		\centering 
 		\caption{Comparisons of our HCGNets against state-of-the-art  networks about test error rates (\%) across CIFAR-10 and CIFAR-100 datasets. Note that the first and second blocks contain human-designed and auto-searched architectures, respectively.}
 		\label{table1}  
 		\begin{tabular}{llccc}  
 			\toprule
 			Model&Params&FLOPs&C-10&C-100 \\  
 			\midrule
 			CondenseNet-182&4.2M&0.5G&3.76&18.47 \\
 			SparseNet-BC&16.7M&-&4.10&18.22\\
 		AOGNet&24.8M&3.7G&3.27&16.63\\
 			LogDenseNetV2 &19.0M&11.1G&3.75&18.80 \\
 			Wide ResNet-28&36.5M&5.2G&4.17&20.50\\
 			ResNeXt-29+SK &27.7M&-&3.47&17.33 \\
 			Res2NeXt-29 &36.9M&-&-&16.56 \\
 			DenseNet-BC-190 &25.6M&9.4G&3.46&17.18 \\
 			DPN-28-10&47.8M&-&3.65&20.23 \\
 			MixNet-190&48.5M&17.3G&3.13&16.96 \\
 			
 			\midrule
 			PNASNet&3.2M&-&3.41&19.53 \\
 			NASNet-A&3.3M&-&3.41&19.70\\
 			ENASNet&4.6M&-&3.54&19.43\\
 			AmoebaNet-A&4.6M&-&3.34&-\\
 			AmoebaNet-B&34.9M&-&2.98&17.66\\
 			NASNet-A &50.9M&-&-&16.03\\
 			ENASNet&52.7M&-&-&16.44\\
 			PNASNet&53.0M&-&-&16.70\\
 			\midrule
 			
 			HCGNet-A1&1.1M&0.2G&3.15&18.13\\
 			HCGNet-A2&3.1M&0.5G&2.29&16.54\\
 			HCGNet-A3&11.4M&2.0G&\textbf{2.14}&\textbf{15.96} \\
 			\bottomrule
 		\end{tabular}
 		
 	\end{table}
 	
 	\subsubsection{Dataset and training details.} Both CIFAR-10 and CIFAR-100 datasets comprise 50k training images and 10k test images corresponding to 10 and 100 classes, respectively. We apply a standard data augmentation following \citet{huang2017densely}. We employ a stochastic gradient descent (SGD) optimizer with momentum 0.9 and batch size 128. Training is regularized by weight decay  and mixup with  \cite{zhang2017mixup}. For HCGNet-A1, we train it for 1270 epochs by SGDR \cite{loshchilov2016sgdr} learning rate curve with initial learning rate 0.1, , . For HCGNet-A2 and A3, we train them for 1260 epochs including two continuous 630 epochs, each of them is a SGDR learning rate curve with initial learning rate 0.1, , .
 	

 \subsubsection{Comparisons with Human-designed Networks.} Quantitatively in Table \ref{table1}, DenseNet-190 has 31 modules in each dense block, while HCGNet-A2 only has 8 modules in each hybrid block thus reduces 93\% redundancy but with substantial accuracy gains. Moreover, HCGNet-A2 significantly outperforms other sparsification variants, such as LogDenseNet, SparseNet and CondenseNet, which indicate that our optimization of DenseNet is more effective than sparsification method. HCGNet-A2 using  fewer parameters surpasses MixNet-190, which represents the most general form of ResNet and DenseNet. It also uses  fewer parameters but obtains better results than concurrent AOGNet, which is the state-of-the-art human network by hierarchical and compositional feature aggregation. Consequently, our nested aggregation is the best method among other combinations and variants of ResNet and DenseNet. 

\begin{table}[tbp]
	\centering
	\caption{Comparisons of our HCGNets against SOTA networks about Top-1 and Top-5 error rates (\%) on ImageNet.}
	\label{imagenet}  
	\begin{tabular}{lcccc}  
		\toprule
		Model&Params&FLOPs&T-1&T-5 \\  
		\midrule
		MixNet-105&11.2M&5.0G&23.3&6.7\\
		MixNet-121&21.9M&8.3G&21.9&5.9\\
		MixNet-141&41.1M&13.1G&20.4&5.3\\	 
		\midrule
		DPN-68 &12.8M&2.5G&23.6&6.9 \\
		DPN-92 &38.0M&6.5G&20.7&5.4 \\
		DPN-98 &61.6M&11.7G&20.2&5.2 \\
		\midrule
		DenseNet-169 &14.2M&3.5G&23.8&6.9 \\
		DenseNet-201 &20.0M&4.4G&22.6&6.3 \\
		DenseNet-264 &33.4M&6.0G&22.2&6.1 \\
		\midrule
		SparseNet-201 &14.9M&9.2G&22.7&- \\
		\midrule
		ResNet-50&25.6M&3.9G&24.6&7.5 \\
		ResNet-50+SE&28.1M&3.9G&23.1&6.7 \\
		ResNet-50+CBAM&28.1M&3.9G&22.7&6.3 \\
		ResNet-101&44.6M&7.6G&23.4&6.9 \\
		ResNet-101+SE&49.3M&7.6G&22.4&6.2 \\
		ResNet-101+CBAM&49.3M&7.6G&21.5&5.7 \\
		\midrule
		ResNeXt-50&25.0M&3.8G&22.9&6.5 \\
		ResNeXt-50+SE&27.6M&3.8G&21.9&6.0 \\
		ResNeXt-101&44.2M&7.5G&21.5&5.8 \\
		ResNeXt-101+SE&49.0M&7.5G&21.2&5.7 \\
		ResNeXt-101+SK&48.9M&8.5G&20.2&- \\
		\midrule
		WideResNet-18&45.6M&6.7G&25.6&8.2 \\
		WideResNet-18+SE&46.0M&6.7G&24.9&7.7 \\
		\midrule
		AOGNet-12M&11.9M&2.4G&22.3&6.1 \\
		AOGNet-40M&40.3M&8.9G&19.8&4.9 \\
		\midrule
		HCGNet-B&12.9M&2.0G&21.5&5.8 \\
		HCGNet-C&42.2M&7.1G&\textbf{19.5}&\textbf{4.8} \\
		\bottomrule
	\end{tabular}
\end{table}

 \begin{table*}[t]
	\centering 
	\caption{Comparisons of HCGNet-B against other backbones on the Mask-RCNN system \cite{he2017mask}. } 
	\label{gty}
	\begin{tabular}{lcccccccc}  
		\toprule
		Backbone &Params&FLOPs&AP&AP&AP&AP&AP&AP \\
	\midrule
		ResNet-50-FPN&44.2M&275.6G&37.3&59.9&40.2&34.2&55.9&36.2 \\ 
		AOGNet-12M-FPN&31.2M&-&38.0&59.8&\textbf{41.3}&34.6&56.6&36.4 \\ 
	\midrule
		HCGNet-B-FPN&32.1M&230.4G&\textbf{38.3}&\textbf{60.6}&\textbf{41.3}&\textbf{35.2}&\textbf{57.5}&\textbf{37.1} \\
	\bottomrule
	\end{tabular}
\end{table*}
 \subsubsection{Comparisons with auto-searched Networks.} Notably, Our HCGNets are also more efficient than auto-searched networks. Compared with other networks with small setting, HCGNet-A2 achieves around 1\% and 3\% reductions on CIFAR-10 and CIFAR-100 error rates, respectively. Moreover, it is noteworthy that HCGNet-A1 can also obtain superior performance with unprecedent efficiency. For large setting, HCGNet-A3 achieves the best results with least complexity. Somewhat surprisingly, HCGNet-A3 can outperform the most competitive NASNet-A with only 22\% parameters.
 	 
 
 
 	\subsection{Experiments on ImageNet 2012}
 		
 	\begin{figure}[tbp]
 		\centering  
 		\includegraphics[width=.95\columnwidth]{{5011.figure-dissect}.pdf}
 		\caption{Comparisons of interpretability by network dissection \cite{bau2017network} among popular models based on ImageNet pretrained models.}  
 		\label{interpretability}
 	\end{figure}
 	\subsubsection{Dataset and Training Details.} ImageNet 2012 dataset comprises 1.2 million training images and 50k validation images corresponding to 1000 classes. We employ the data augmentation following \citet{huang2017densely}. Final error rates are reported by single-crop with size  at test time on the validation set. We employ synchronous SGD with momentum 0.9 and batch size 256. Training is regularized by weight decay , label smoothing with  \cite{szegedy2016rethinking}, mixup with  and dropout \cite{srivastava2014dropout} with rate 0.1 before the final FC layer. All networks are trained for 630 epochs by SGDR learning rate curve with initial learning rate 0.1, , .
 	\subsubsection{Comparisons with popular networks.} As shown in Table \ref{imagenet}, our HCGNets perform the best among all other models with less or comparable complexity in terms of top-1 and top-5 error rates. It is noteworthy that DenseNet-169 stacks 4 dense blocks with 6,12,32,32 modules, while HCGNet-B utilizes shallower design with 3,6,12,8 modules for 4 hybrid blocks, thus reducing 88\% redundancy but obtaining above absolute 2.3\% gain of performance. Furthermore, HCGNets yield significantly better results than the families of DenseNet, MixNet and DPN under comparable complexity. Remarkably, using considerable  fewer FLOPs, HCGNet-B can also surpass SparseNet-201, which is the state-of-the-art variant of DenseNet. The family of HCGNet can consistently obtain better performance than the families of ResNet, ResNeXt, WideResNet and their attention-based variants, which represent the widely applied
 	models in practice. Predominately, HCGNets outperform previous
 	SOTA AOGNets across various model specifications, which show
 	the superiority of our design.
 

 	\subsubsection{Model Interpretability} We quantify the interpretability by network dissection, which compares the number of unique detectors in the final convolutional layer. Figure \ref{interpretability} shows that HCGNet-B obtains the overall highest score with least complexity, which shows that the designs of hybrid connectivity and SMG module can not only achieve the best accuracy, but also generate the best latent representations.
 	\begin{figure}[tbp]
 		\centering  
 		\includegraphics[width=.95\columnwidth]{{5011.figure-attack}.pdf}
 		\caption{Comparisons of adversarial robustness by FGSM \cite{attack} attack.}  
 		\label{attack}
 	\end{figure}
 	\subsubsection{Adversarial Robustness}
 	We attack HCGNet-B by popular FGSM across various perturbation energies  to test the adversarial robustness against widely applied models, results of which are shown in Figure \ref{attack}. HCGNet-B has a more remarkable robustness than other models in adversarial defense, especially the perturbation is relatively high.
 	
 	\subsection{Object Detection and Instance Segmentation}
 	To show the transferability, we experiment HCGNet-B pretrained on ImageNet as a backbone on the Mask-RCNN system to implement object detection and instance segmentation tasks. We use COCO train2017 set to finetune the HCGNet-B by the 1x training schedule, and evaluate the performance on COCO val2017 set. We report the results by standard COCO metrics of Average Precision (AP), i.e, AP, AP, and AP for bounding box detection (AP) and instance segmentation (AP)  in Table \ref{gty}. The results show that HCGNet-B can learn better features than SOTA ResNet and AOGNet backbones.
 	
 	\section{Conclusion.}
 	This paper develops an efficient architecture with the innovative designs of hybrid connectivity, micro-module and attention-based forget and update gates. On CIFAR and ImageNet datasets, HCGNets outperform state-of-the-art networks with less or comparable complexity. Extensive experiments based on the ImageNet pretrained model further show the remarkable interpretability, robustness for recognition and transferability for detection. We hope our HCGNets may inspire the future study of architectural design and search.
 	
 	\bibliographystyle{aaai}
 	\bibliography{5011-aaai.bib}
 	
 \end{document}
