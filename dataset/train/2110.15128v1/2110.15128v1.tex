\vspace{-2mm}
\section{Experiments}
\label{sec:experiments}
\vspace{-1mm}

\textbf{Datasets.} We evaluate the performance of our approach using several publicly available benchmark datasets for video domain adaptation, namely UCF-HMDB~\cite{chen2019domain}, Jester~\cite{pan2020adversarial}, and Epic-Kitchens~\cite{munro2020multi}. UCF-HMDB (assembled by authors in~\cite{chen2019domain}) is an overlapped subset of the original UCF~\cite{soomro2012ucf101} and HMDB datasets~\cite{kuehne2011hmdb}, containing  videos across  classes. 
Jester (assembled by authors in~\cite{pan2020adversarial}) is a large-scale cross-domain dataset that contains videos of humans performing hand gestures~\cite{materzynska2019jester} from two domains, namely Source and Target that contain  and  video clips respectively across  classes.
Epic-Kitchens (assembled by authors in~\cite{munro2020multi}) is a challenging egocentric dataset that consists of videos across  largest action classes from three domains, namely D, D and D, corresponding to P, P and P kitchens on the full Epic-Kitchens dataset~\cite{damen2018scaling}.
We use the standard training and testing splits provided by the authors in~\cite{chen2019domain,pan2020adversarial,munro2020multi} to conduct our experiments on each dataset.
More details about the datasets can be found in the appendix.

\textbf{Baselines.} We compare our approach with the following baselines. (1) source only (a lower bound) and supervised target only (an upper bound) baselines that trains the network using labeled source data and labeled target data respectively, (2) popular UDA methods based on adversarial learning (e.g., DANN~\cite{ganin2016domain}, and ADDA~\cite{tzeng2017adversarial}), (3) existing video domain adaptation methods, including SAVA~\cite{choi2020shuffle}, TA\textsuperscript{}N~\cite{chen2019temporal}, ABG~\cite{luo2020adversarial} and TCoN~\cite{pan2020adversarial}. We also compare with Source + Target (which simply uses all labelled data available to it to train the network) and ENT~\cite{saito2019semi} in semi-supervised domain adaptation experiments.
We directly quote the numbers reported in published papers when possible and use source code made publicly available by the authors of TA\textsuperscript{}N~\cite{chen2019temporal} on both Jester and Epic-Kitchens.


\textbf{Implementation Details.}
Following~\cite{choi2020shuffle}, we use ID~\cite{carreira2017quo} as the backbone feature encoder network, initialized with Kinetics pre-trained weights. For the temporal graph encoder, we use a -layer GCN similar to~\cite{wang2018videos}. We follow the standard `pre-train then adapt' procedure used in prior works~\cite{tzeng2017adversarial,choi2020shuffle} and train the model with only source data to provide a warmstart before the proposed approach is employed. The dimension of the features extracted from the ID encoder is  which is the same as the node-feature dimension of the initial layer of the GCN. The final layer of the GCN has its node-feature dimension same as the number of action classes in a dataset and uses a mean aggregation strategy to output the logits. We use a clip-length of -frames and train all the models end-to-end using SGD with a momentum of  and a weight decay of e-. We use an initial learning rate of  for the ID and  for the GCN in all our experiments. We use a batch size of  equally split over the two domains, where each batch consists of  clips from the same video, where  is  for the fast version () and , , or  for the slow version (). For inference, we use  uniformly sampled clips per video and use the base branch of the model to recognize the action.
The temperature parameter is set to . We extract backgrounds from videos using temporal median filtering~\cite{piccardi2004background} and empirically set  for background mixing. We use a pseudo-label threshold of  in all our experiments and smooth the cross-entropy loss with , following~\cite{szegedy2016rethinking, muller2019does}. We set  and  from  depending on the dataset. We report the average action recognition accuracy over  random trials. 
We use  NVIDIA Tesla V GPUs for training all our models.


\setlength\intextsep{3pt}
\begin{wraptable}{r}{0.6\linewidth}
 \centering
\caption{\small \textbf{Results on UCF-HMDB Dataset.} \ours establishes new state-of-the-art for unsupervised video domain adaptation on UCF-HMDB, by significantly outperforming existing methods.} \vspace{-1mm}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{ l | c |  c | c | c }
\Xhline{2\arrayrulewidth} 
\textbf{Method} & \textbf{Backbone} & \textbf{UCFHMDB} & \textbf{HMDBUCF} & \textbf{Average} \\
\Xhline{1\arrayrulewidth} 
DANN~\cite{ganin2016domain} & ResNet-101 &  &  &  \\
JAN~\cite{long2017deep} & ResNet-101 &  &  &  \\
AdaBN~\cite{li2018adaptive} & ResNet-101 &  &  &   \\
MCD~\cite{saito2018maximum} & ResNet-101 &   &  &   \\
TA\textsuperscript{}N~\cite{chen2019temporal} & ResNet-101 &  &  &  \\ 
ABG~\cite{luo2020adversarial} & ResNet-101 &  &  &  \\
TCoN~\cite{pan2020adversarial} & ResNet-101 &  &  &  \\
\Xhline{2\arrayrulewidth} 
Source Only  & I3D &  &  &  \\

DANN~\cite{ganin2016domain} & I3D &  &  &  \\

ADDA~\cite{tzeng2017adversarial} & I3D &  &  &  \\

TA\textsuperscript{}N~\cite{chen2019temporal} & I3D &  &  &  \\

SAVA~\cite{choi2020shuffle} & I3D &  &  &  \\
\Xhline{1\arrayrulewidth} 
\textbf{\ours} & I3D &  &  &  \\
\Xhline{1\arrayrulewidth} 
\rowcolor{Gray}
Supervised Target & I3D &  &  &  \\
\Xhline{2\arrayrulewidth} 
\end{tabular}
\label{table:ucf_hmdb}
\end{adjustbox}
\end{wraptable}
 \textbf{Results on UCF-HMDB.} Table~\ref{table:ucf_hmdb} shows results of our method and other competing approaches on UCF-HMDB dataset. Our \ours framework achieves the best average performance of \textbf{\%}, which is about \textbf{\%} more than the previous state-of-the-art performance on this dataset. While comparing with the recent method, SAVA~\cite{choi2020shuffle} using the same I3D backbone, \ours obtains \textbf{\%} and \textbf{\%} improvement on UCFHMDB and HMDBUCF task respectively, without relying on frame attention or adversarial learning. These improvements clearly show that our temporal graph contrastive learning with background mixing is not only able to better leverage the temporal information but also shared action semantics, essential for effective video domain adaptation. 
In summary, \ours outperforms all the existing video DA methods on UCF-HMDB, showing the efficacy of our approach in learning more transferable features for cross-domain action recognition without using any target labels. 

\textbf{Results on Jester and Epic-Kitchens.} On the large-scale Jester dataset, our proposed approach, \ours also outperforms other DA approaches by increasing the Source Only (no adaptation) accuracy from \textbf{\%} to \textbf{\%}, as shown in Table~\ref{tab:jester_epic} (left). In particular, our approach achieves an absolute improvement of \textbf{\%} over TA\textsuperscript{}N~\cite{chen2019temporal}, which corroborates the fact that \ours can well handle not only the appearance gap but also the action gap present on this dataset (e.g., for the action class \enquote{rolling hand}, source domain contains videos of \enquote{rolling hand forward}, while the target domain only consists of videos of \enquote{rolling hand backward}).
Table~\ref{tab:jester_epic} (right) summarizes the results on Epic-Kitchens, which is another challenging dataset consisting of total  transfer tasks with a large imbalance across different action classes. Overall, \ours obtains the best on  tasks including the best average performance of \textbf{\%}, compared to only \textbf{\%} and \textbf{\%} achieved by the source only and TA\textsuperscript{}N~\cite{chen2019temporal} respectively. While the improvements achieved by our approach are encouraging on both Jester and Epic-Kitchens, the accuracy gap between \ours and supervised target is still significant (\textbf{\%} on Jester and \textbf{\%} on Epic-Kitchens), which highlights the great potential for improvement in future for unsupervised video domain adaptation. 

\textbf{Comparison with MM-SADA~\cite{munro2020multi}.} MM-SADA\cite{munro2020multi} is another state-of-the-art approach for video domain adaptation that leverages the idea of using multi-modal (RGB and Optical flow) data to learn better domain invariant representations. The approach has two main components: adversarial learning and multi-modal supervision. While \ours does not use optical flow features anywhere, the RGB-only version of MM-SADA still uses optical flow features for the multi-modal self-supervision. Interestingly, \ours () shows very competitive performance using only RGB features when compared to the above () on the Epic-Kitchens dataset. Additionally, we train MM-SADA (RGB-only) (but perform multimodal supervision using both RGB and flow following the original paper~\cite{munro2020multi}) on UCF-HMDB dataset and notice that \ours outperforms it by a margin of \% on an average (UCF  HMDB: \% vs \%, HMDB  UCF: \% vs \%, Avg: \% vs \%), showing its effectiveness in unsupervised video domain adaptation.

\setlength{\tabcolsep}{2pt}
\begin{table*}[!t]
\scriptsize
\begin{center}
\caption{\small \textbf{Results on Jester and Epic-Kitchens Datasets.} \ours outperforms TA\textsuperscript{}N~\cite{chen2019temporal} by \% on the challenging Jester dataset. On Epic-Kitchens, \ours achieves the best performance on  out of  transfer tasks including the best average performance among all compared methods.} \vspace{-2mm}
\resizebox{\linewidth}{!}{
\begin{tabular}{l | c | c || cccccc | c}
\Xhline{2\arrayrulewidth}  
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Backbone}}  & \textbf{Jester} & \multicolumn{6}{c|}{\textbf{Epic-Kitchens}} & \multirow{2}{*}{\textbf{Average}}\\ 
\cline{3-9}
&  & \textbf{SourceTarget} & \textbf{D2D1} & \textbf{D3D1} & \textbf{D1D2} & \textbf{D3D2} & \textbf{D1D3} & \textbf{D2D3} & \\
\Xhline{1\arrayrulewidth}  
Source Only        & I3D       &     &     &         &         &         &         &         &    \\
DANN~\cite{ganin2016domain}                  & I3D       &     &     &         &         &         &         &         &   \\
ADDA~\cite{tzeng2017adversarial}                 & I3D       &     &     &         &         &         &         &         &    \\
TA\textsuperscript{}N~\cite{chen2019temporal}    & I3D       &    &     &         &         &         &         &         &    \\
\Xhline{1\arrayrulewidth}  
\textbf{\ours} & I3D       &       &  &  &  &  &  &  &  \\ 
\Xhline{1\arrayrulewidth} \rowcolor{Gray}
Supervised Target        & I3D       &     &     &         &    &         &        &         &    \\
\Xhline{2\arrayrulewidth}  
\end{tabular}}
\label{tab:jester_epic} 
\vspace{-7mm}
\end{center}
\end{table*}
 \begin{wraptable} {r}{0.6\linewidth}
\centering
\caption{\small \textbf{Semi-Supervised Domain Adaptation on UCF-HMDB and Jester Datasets.} \ours significantly outperforms all the compared methods in both one-shot and three-shot settings.} \vspace{-1mm}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{l | cc | cc || cc}
\Xhline{2\arrayrulewidth}  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{UCFHMDB}} & \multicolumn{2}{c||}{\textbf{HMDBUCF}}  & \multicolumn{2}{c}{\textbf{Jester(S)  Jester(T)}}  \\ 
\cline{2-7}
&  1-shot & 3-shot & 1-shot & 3-shot  & 1-shot & 3-shot \\
\Xhline{1\arrayrulewidth}  
Source + Target &   &    &   &  &  &  \\
DANN~\cite{ganin2016domain}  &     &    &   &  &  &  \\
ADDA~\cite{tzeng2017adversarial}  &     &    &    &   &   &  \\
ENT~\cite{saito2019semi} &     &      &    &   &  & \\
\Xhline{1\arrayrulewidth}  
\textbf{\ours} &   &  &  &  &  &  \\
\Xhline{2\arrayrulewidth}  
\end{tabular}
\label{table:semi-supervised}
\end{adjustbox}
\end{wraptable}

   

 
\textbf{Semi-supervised Domain Adaptation.}
To further study the robustness of our proposed approach, we extend the unsupervised domain adaptation to a semi-supervised setting, where one (-shot) and three target labels (-shot) per class are available for training.
Table~\ref{table:semi-supervised} shows that our simple approach consistently outperforms the adversarial DA methods (DANN~\cite{ganin2016domain}, and ADDA~\cite{tzeng2017adversarial}) including the semi-supervised method, ENT~\cite{saito2019semi}, on both UCF-HMDB and Jester datasets. Remarkably, \ours with three target labels per class improves the performance of Source + Target baseline from \textbf{\%} to \textbf{\%}, which is only \textbf{\%} lower than the supervised target upper bound (in Table~\ref{table:ucf_hmdb}) on HMDBUCF task (\textbf{\%} vs \textbf{\%}). These results well demonstrate the utility of our proposed approach in many practical applications where annotating \textit{a few} videos per class is typically possible and therefore worth doing given the boost it provides.  

\textbf{Effectiveness of Individual Components.} As seen from Table~\ref{table:ablation_component}, the vanilla temporal contrastive learning (TCL) achieves an average accuracy of  on UCF-HMDB while \% on Jester (1\textsuperscript{st} row), which is already better than DANN~\cite{ganin2016domain}, and ADDA~\cite{tzeng2017adversarial} (ref. Table~\ref{table:ucf_hmdb},\ref{tab:jester_epic}), showing its effectiveness over adversarial learning in aligning features.
While both background mixing (BGM) and incorporation of target pseudo-labels (TPL) individually improves the performance over TCL (\textbf{\%}, \textbf{\%} using BGM and \textbf{\%}, \textbf{\%} using TPL, respectively), addition of both of them leads to the best average performance of \% on UCF-HMDB dataset and \% on the Jester dataset. This corroborates the fact that both cross-domain action semantics (through BGM) and discriminabilty (through TPL) of the latent space play crucial roles in video domain adaptation in addition to the vanilla contrastive learning for aligning features. 

\begin{table} [h]
\vspace{-1mm}
\parbox[t]{.48\linewidth}{
  \begin{center}
  \caption{\small \textbf{Ablation Study on UCF-HMDB and Jester.} TCL: Temporal Contrastive Learning, BGM: Background Mixing, TPL: Target Pseudo-Labels.}   \vspace{1mm}
        \resizebox{0.9\linewidth}{!}{
       \begin{tabular}{ccc|c|c|c|c}
             \Xhline{2\arrayrulewidth} 
              \textbf{TCL} & \textbf{BGM} & \textbf{TPL} & \textbf{UH} & \textbf{HU} & \textbf{Average} & \textbf{Jester(S)Jester(R)}   \\
             \Xhline{1\arrayrulewidth}  
           \cmark & \xmark &  \xmark   &  &  &  &    \\
            \cmark & \cmark &  \xmark   &  &  &  &    \\
            \cmark & \xmark &  \cmark   &  &  &  &    \\
           \cmark & \cmark &  \cmark   &  &  &  &    \\
           \Xhline{2\arrayrulewidth}  
        \end{tabular}} 
    \label{table:ablation_component}
    \end{center}}
 \hfill
\parbox[t]{.48\linewidth}{
\begin{center}
\caption{\small \textbf{Comparison with MixUp Strategies}. Background mixing outperforms other alternatives in leveraging shared action semantics on UCF-HMDB.} \vspace{0.5mm}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}{l|c|c|c|c}
\Xhline{2\arrayrulewidth} 
\textbf{Method} & \textbf{UH} & \textbf{HU} & \textbf{Average} & \textbf{Jester(S)Jester(R)}\\ 
\Xhline{1\arrayrulewidth} 
Gaussian Noise &  &  &  & \\
Video MixUp &  &  &  & \\
Video CutMix &  &  &  & \\
\Xhline{1\arrayrulewidth} 
Background Mixing &  &  &  & \\ 
\Xhline{2\arrayrulewidth} 
\end{tabular}}
\label{table:ablation_mixing}
\end{center}
} 
\end{table} 

\textbf{Comparison with Different MixUp Strategies.} We explore the effectiveness of background mixing by comparing with different MixUp strategies (Table~\ref{table:ablation_mixing}): (a) Gaussian Noise: adding White Gaussian Noise to videos in both domains; (b) Video MixUp~\cite{zhang2017mixup}: directly mixing one video with another from a different domain, as in images; (c) Video CutMix~\cite{yun2019cutmix}: randomly replacing a region of a video with another region from the other domain.
The proposed way of generating synthetic videos by mixing background of a video from one domain to a video from another domain, outperforms all three alternatives on UCF-HMDB as well as on the more challenging Jester dataset. Note that while both MixUp and CutMix destroy motion pattern of original video, background mixing keeps semantic consistency without changing the temporal dynamics. 


\textbf{Effect of Background Pseudo-labels.} We investigate the effect of pseudo-labels on background mixed videos (i.e., both videos considered to be of same action class while creating positives) by simply adding them as unlabeled videos without any modification to the contrastive objective in Eq.~\ref{eq:tcl_loss}. \ours without background pseudo-labels decreases the performance from \% to \% (\textbf{\%}: Table~\ref{table:ablation_contrastive}), showing its effectiveness in leveraging action semantics shared across both domains. 

\textbf{Effect of Source Contrastive Learning.} \ours adopts contrastive learning on both source and target domains, although we already have supervised cross-entropy loss on source videos. We observe that applying contrastive learning on target domain only, by removing source contrastive objective  from Eq.~\ref{eq:total_loss}, lowers down the performance from \% to \% (\textbf{\%}) on UCF-HMDB (Table~\ref{table:ablation_contrastive}). This shows the importance of training the model using the same temporal invariance objective on both domains simultaneously to achieve effective alignment across domains.

\begin{wraptable} {r}{0.5\linewidth}
\centering
\caption{\small \textbf{Ablation Study on Contrastive Learning.}} \vspace{-2mm}
\begin{adjustbox}{max width=\linewidth}
\begin{tabular}{l|c|c|c}
\Xhline{2\arrayrulewidth}
\textbf{Method}        & \textbf{UH} & \textbf{HU} & \textbf{Average} \\ 
\Xhline{1\arrayrulewidth}
\ours         &       &       &    \\ 
\Xhline{1\arrayrulewidth}
-- w/o Background Pseudo-labels  &   &  &   \\
-- w/o Source Contrastive Learning &   &  &  \\
-- w/o Random Speed Invariance &   &  &  \\ 
\Xhline{2\arrayrulewidth}
\end{tabular}
\end{adjustbox}
\label{table:ablation_contrastive}
\end{wraptable} 
\textbf{Effect of Random Speed Invariance.} We remove randomness in video speed from the auxiliary branch of our temporal contrastive learning framework and observe that \ours (with  clips in the base branch and only  clips in the auxiliary branch) leads to an average top-1 accuracy of \% compared to \% (\textbf{\%}: Table~\ref{table:ablation_contrastive}), showing the importance of random speed invariance in learning robust features.

\textbf{Self-Training vs Supervised Contrastive Learning.} We directly use self-training that uses cross-entropy loss on target pseudo labels instead of  and find that the average performance drops to \% on UCF-HMDB, indicating the advantage of supervised contrastive objective in enhancing discriminability of the latent space by successfully leveraging label information from target domain.



\begin{wraptable}{r}{0.5\linewidth}
\centering
\caption{\small \textbf{Baseline Comparisons w/ GCN Representations on UCF-HMDB and Jester Datasets.}} \vspace{-2mm}
\begin{adjustbox}{width=0.95\linewidth}
\begin{tabular}{l|c|c|c|c}
\Xhline{2\arrayrulewidth} 
\textbf{Method (w/ GCN)} & \textbf{UH} & \textbf{HU} & \textbf{Average} & \textbf{Jester(S)Jester(R)}\\ 
\Xhline{1\arrayrulewidth} 
Source Only &  &  &  & \\
DANN~\cite{ganin2016domain} &  &  &  & \\
TA\textsuperscript{}N~\cite{chen2019temporal} &  &  &  & \\
\Xhline{1\arrayrulewidth}
\ours &  &  &  & \\ 
\Xhline{2\arrayrulewidth} 
\end{tabular}
\end{adjustbox}
\label{tab:gcn_baseline}
\end{wraptable} 
\textbf{Effect of Graph Representation.} (a) \textit{Removal of Graph Representation from} \ours: We examine the effect of graph representation for videos and find that by removing GCN from our framework lowers down the performance from \% to \% on UCF-HMDB dataset, which shows that graph contrastive learning is more useful in capturing the temporal dependencies, essential for video domain adaptation. (b) \textit{Effect of Graph Representation on Baseline Methods}: Additionally, in Table~\ref{tab:gcn_baseline} we compare with domain adversarial adaptation methods DANN~\cite{ganin2016domain} and TA3N~\cite{chen2019temporal} including the Source only baseline with GCN feature representation on both UCF-HMDB and Jester datasets. \ours improves the Source only accuracy by \% and \% respectively on UCF-HMDB and Jester datasets. Furthermore, \ours outperforms DANN~\cite{ganin2016domain} with the same GCN equipped as ours, on both datasets (+\%, +\%, respectively) showing its effectiveness over adversarial learning in aligning features for video domain adaptation. TA3N~\cite{chen2019temporal} performs very poorly (\% and \%) when equipped additionally with graph representations. We believe this is because TA3N already utilizes Temporal Relational Network~\cite{zhou2018temporal} for modeling temporal relations, which probably hinders in learning GCN features for successful domain adaptation in videos. (c) \textit{Alternatives for Graph Representation}: We replace GCN using MLP/LSTM of similar complexity and notice that both alternatives are inferior to GCN on UCF-HMDB (MLP: , LSTM: , GCN: ), which shows the effectiveness of GCN in our contrastive learning framework for capturing the temporal dependencies, essential for video domain adaptation.

\textbf{Effect of Background Extraction Method.} We experiment with a different background extraction strategy~\cite{zivkovic2004improved} that uses Gaussian Mixture Models (GMM) to extract the backgrounds and observe that the very simple and fast strategy based on temporal median filtering~\cite{piccardi2004background} outperforms GMM by \% on average on UCF-HMDB (UCFHMDB: \% vs \%, HMDBUCF: \% vs \%, Avg: \% vs \%). Note that our \ours framework is agnostic to the method used for background extraction and can be incorporated with any other background extraction techniques for videos, \textit{e.g.}, learnable background segmentation strategies such as~\cite{wang2018background,patil2021multi}. 



\begin{figure*}[t]
\captionsetup[subfigure]{labelformat=empty}
\centering
\subfloat[\tiny\textbf{Source Only}]{
\label{fig:tsne_sourceonly}
\includegraphics[width=0.225\linewidth]{figures/tsne_sourceonly.pdf}}
\subfloat[\tiny\textbf{TCL}]{
\label{fig:tsne_tcl}
\includegraphics[width=0.225\linewidth]{figures/tsne_tcl.pdf}}
\subfloat[\tiny\textbf{TCL w/ BGM}]{
\label{fig:tsne_tclbg}
\includegraphics[width=0.225\linewidth]{figures/tsne_tclbg.pdf}}
\subfloat[\tiny\textbf{TCL w/ BGM \& TPL (CoMix)}]{
\label{fig:tsne_comix}
\includegraphics[width=0.225\linewidth]{figures/tsne_comix.pdf}}
\vspace{-2mm}
\caption{\small \textbf{Feature Visualizations using t-SNE.} Plots show visualization of our approach with different components on UCFHMDB task. \textcolor{blue}{Blue} and \textcolor{red}{red} dots represent source and target data respectively.  
Features for both target and source domain become progressively discriminative and improve from left to right by adoption of our novel components within a temporal contrastive learning framework. Best viewed in color.
} 
\label{fig:tsne} \vspace{-5mm}
\end{figure*} 
\textbf{Feature Visualizations.} We use t-SNE~\cite{maaten2008visualizing} to visualize the features learned using different components of our \ours framework. 
As seen from Figure~\ref{fig:tsne}, alignment of domains including discriminability improves as we adopt \enquote{TCL} and \enquote{BGM} to the vanilla Source only model. The best results are obtained when all the three components \enquote{TCL}, \enquote{BGM} and \enquote{TPL} i.e.,~\ours are added and trained using an unified framework (Eq.~\ref{eq:total_loss}) for unsupervised video domain adaptation.
Additional results and analysis including more qualitative examples are included in the appendix.


