\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{emnlp2021}

\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{color}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{plotmarks}
\newcommand\marksymbol[2]{\tikz[#2,scale=1.8]\pgfuseplotmark{#1};}
\usepackage{xcolor}
\usepackage{multirow}

\usepackage{cleveref}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefrangeformat{section}{\S\S#3#1#4 to~#5#2#6}
\crefmultiformat{section}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{ and~#2#1#3}
\usepackage{refstyle}
\Crefformat{figure}{#2Fig.~#1#3}
\Crefmultiformat{figure}{Figs.~#2#1#3}{ and~#2#1#3}{, #2#1#3}{ and~#2#1#3}
\Crefformat{table}{#2Tab.~#1#3}
\Crefmultiformat{table}{Tabs.~#2#1#3}{ and~#2#1#3}{, #2#1#3}{ and~#2#1#3}
\Crefformat{appendix}{\S#2#1#3}
\crefformat{algorithm}{Alg.~#2#1#3}

\newcommand{\muhao}[1]{{\color{blue}[{MC:} #1]}}
\newcommand{\todo}[1]{{\color{red}[{TD:} #1]}}
\newcommand{\fei}[1]{{\color{brown}[{FW:} #1]}}

\newcommand{\stitle}[1]{\vspace{0.3em} \noindent{\bf #1.}}



\title{Table-based Fact Verification with Salience-aware Learning}

\author{Fei Wang,\;~Kexuan Sun,\;~Jay Pujara,\;~Pedro Szekely \and Muhao Chen \\
  Department of Computer Science \& Information Sciences Institute\\
  University of Southern California \\
\texttt{\{fwang598,kexuansu,jpujara,szekely,muhaoche\}@usc.edu} 
  }

\date{} 
\begin{document}
\maketitle

\begin{abstract}

Tables provide valuable knowledge that can be used to verify textual statements.
While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available.
Moreover, training a generalized fact verification model requires abundant labeled training data.
In this paper, we propose a novel system to address these problems.
Inspired by counterfactual causality, our system identifies token-level salience in the statement with \textit{probing-based salience estimation}. 
Salience estimation allows enhanced learning of fact verification from two perspectives.
From one perspective, our system conducts \textit{masked salient token prediction} to enhance the model for alignment and reasoning between the table and the statement.
From the other perspective, our system applies \textit{salience-aware data augmentation} to generate a more diverse set of training instances by replacing non-salient terms. 
Experimental results on TabFact show the effective improvement by the proposed salience-aware learning techniques, 
leading to the new SOTA performance on the benchmark.
\footnote{Our code is publicly available at \url{https://github.com/luka-group/Salience-aware-Learning}}

\end{abstract} \section{Introduction}




Fact verification, the problem of determining whether a statement is entailed or refuted by evidence, has quickly become a critical problem in NLP to combat information pollution~\cite{rashkin2017truth,thorne2018fever,zhang2019evidence, zellers2019defending, wadden2020fact}. 
Successful fact verification enables downstream tasks such as misinformation detection, fake news identification, factual error correction, and deceptive opinion detection \cite{ott2011finding, shu2017fake, yoon2019detecting, cao2020factual}.




\begin{figure}[t]
\centering
\includegraphics[scale=0.42]{figures/intro2.png}
\caption{
An example of table-based fact verification, 
with green for entailed statements and red for refuted statements.
Alignment and reasoning are essential for both table-based fact verification and masked salient token prediction (e.g. \texttt{''Eagles''}).
Token replacement may lead to similar (e.g. \texttt{'''s''} to \texttt{''team''}) or different (e.g. \texttt{''Eagles''} to \texttt{''Bearcats''}) statements.
}
\label{fig:intro}
\end{figure}



Recently, table-based fact verification \cite{chen2019tabfact, zhong2020logicalfactchecker, yang2020program} has garnered attention. 
As a ubiquitous and clean format of semi-structured knowledge, tables are regarded as reliable sources of evidence to verify the textual statements \cite{chen2019tabfact}.
Leveraging tabular data for fact verification requires identifying relevant evidence in tables,
and conducting logical reasoning according to the selected evidence.
Prior studies have attempted to 
generate logical programs to capture logical operations and relations between the statement and the table \cite{zhong2020logicalfactchecker,yang2020program, shi2020learn}.
More recent work shows that Transformer-based language models with general and task-specific pre-training over textual and tabular data can achieve SOTA performance without counting on explicit logical programs \cite{eisenschlos2020understanding, dong2021structural}.







However, to provide a reliable solution to the table-based fact verification task, several critical challenges are still overlooked by prior studies.
One challenge is to effectively provide connections among components of the statement and substructures of the table, and accordingly conduct the inference.
Being unaware of such fine-grained connections and logical relations could raise the risk of misalignment, incorrect reasoning and ignoring salient components of a statement, and therefore leads to incorrect verification results.
For example, to verify the statement in \Cref{fig:intro}, the model should implicitly or explicitly infer all the five arrows accurately.
Although some works have tried to perform token-level interactions and generate logical programs to connect statements and tables and conduct logical reasoning \cite{zhong2020logicalfactchecker,yang2020program}, the supervision signals to guide the learning process are typically sparse. Another challenge is that training a well-generalized fact verification model non-trivially requires abundant labeled training data.
Limited training data can only cover limited statement patterns and hinder robustness and generalizability of model inference.
Previous works either trained on limited data \cite{zhong2020logicalfactchecker,yang2020program} or augment training data with specific statement generation templates \cite{eisenschlos2020understanding}.
Yet, in real-world scenarios, statements and evidences can be presented in very diverse ways, and such diversity is difficult to be comprehensively captured by specific templates.





To this end, we propose a novel salience-aware learning system for table-based fact verification.
Starting from a TAPAS \cite{herzig2020tapas} language model fine-tuned on the TabFact dataset, 
our system identifies salient and non-salient tokens in statements with a \emph{probing-based salience estimation} method inspired by counterfactual causality \cite{pearl2009causality} (\Cref{sec:salient}).
Then, the system leverages the estimated salience information from two perspectives.
From one perspective, to enhance the model for capturing fine-grained connections and supporting the reasoning between statements and tables, the system conducts \emph{masked salient token prediction} as an auxiliary task (\Cref{sec:token_prediction}). 
More specifically, this task is to predict the masked salient token in an entailed statement given the corresponding table by reusing the embedding layer of TAPAS as a language model head.
The fact verification task can receive indirect supervision from the auxiliary task, as both of them requires table-text alignment and logical reasoning. 
From the other perspective,
to improve the model robustness, instead of using templates for statement augmentation like prior work \cite{eisenschlos2020understanding},
we develop a \emph{salience-aware data augmentation} technique (\Cref{sec:aug}). 
Intuitively, replacing non-salient tokens provides unseen statements while preserving the meaning and correctness of the original statement.
This strategy enhances the size and comprehensiveness of the training data and further complements training with more supervision signals.



The main contributions of this paper are three-fold.
First, we propose a probing-based salience estimation method to evaluate the importance of each token in a statement according to the counterfactual causality theory. 
Second, we propose a novel salience-aware learning system that helps the fact verification model to find the connections between the table and the statement, and enhance the inference ability of the model with the auxiliary task of masked salient token prediction. Third, to complement with insufficient training signals and improve the model robustness on heterogeneous statements, we incorporate a probabilistic data augmentation method driven by non-salient tokens.
We evaluate our system based on the TabFact benchmark, which shows promising performance on this task and drastically outperforms prior methods.
Detailed analysis demonstrates the effectiveness and essentially of both masked salient token prediction and salience-aware data augmentation techniques for the improved performance.


















\begin{figure*}[t]
\centering
\includegraphics[clip, trim=0cm 10cm 10cm 0.5cm, width=1.00\textwidth, scale=0.38]{figures/system.pdf}
\caption{
Workflow of the proposed system. The system is composed of three parts. The arrows illustrate how information is transferred. For tokens, a lighter background color indicates a lower salience score. For augmented statements, a lighter background color indicates a smaller probability.
}
\label{fig:system}
\end{figure*}
 \section{Related Work}

In this section, we provide a selected summary for two related research topics.

\subsection{Fact Verification}
Fact verification have become an essential research topic in recent years with the rising concerns of misinformation \cite{vlachos2014fact, wang2017liar, thorne2018fever, khattar2019mvae, zellers2019defending, chen2019tabfact}. 
Early works on fact verification are mainly based on unstructured textual evidence \cite{yin2018twowingos, nie2019combining, zhou2019gear}. 


Recently, much attention has been paid to table-based fact verification \cite{chen2019tabfact, zhong2020logicalfactchecker, yang2020program, eisenschlos2020understanding, shi2020learn, dong2021structural}. \citet{chen2019tabfact} released the TabFact benchmark, and motivated two lines of research. 
Considering the importance of logical operations in this task, some works introduce such inductive bias by explicitly generating and capturing logical programs.
Latent Program Algorithm (LPA) \cite{chen2019tabfact} collected potential program candidates and execution results according to a search algorithm, and then trained a Transformer-based \cite{vaswani2017attention} model to assign a confidence score to each program based on matching to the statement.
Through this line, later works have explored improved ways to generate and capture logical programs \cite{zhong2020logicalfactchecker, yang2020program}.
LogicalFactChecker \cite{zhong2020logicalfactchecker} generated logical programs using a sequence-to-action generation approach, where it applied neural module networks \cite{andreas2016learning} to capture the logical structure of programs.
HeterTFV \cite{shi2020learn} learned to combine linguistic information and symbolic information with a heterogeneous graph attention network.
ProgVGAT \cite{yang2020program} verbalized the execution processes of the generated programs, and applied graph attention networks \cite{velivckovic2017graph} to capture each execution tree.
Beside logical programs, other studies applied pre-trained language models to linearized tables and perform fact verification as natural language inference (NLI) \cite{chen2019tabfact, eisenschlos2020understanding, dong2021structural}.
Table-BERT \cite{chen2019tabfact} applied BERT \cite{devlin2019bert} as an NLI model. 
\citet{eisenschlos2020understanding} and \citet{dong2021structural} improve this strategy by conducting task-specific pre-training to TAPAS \cite{herzig2020tapas}, a Transformer-based language model pre-trained on both textual and tabular data.


Our work takes advantages of both lines of research on table-based fact verification, introducing cross-structural alignment bias and logical reasoning bias to pre-trained language models. 
Besides, previous works focus on significant words in statements, while we apply data augmentation to improve model robustness to insignificant words. 


\subsection{Counterfactual Causality in NLP}
Counterfactual thinking and causal inference have inspired several studies in natural language processing, including
counterfactual story rewriting \cite{qin2019counterfactual},
paraphrasing diversification \cite{park2019paraphrase},
measuring fairness in text classification \cite{garg2019counterfactual},
debiasing in machine translation \cite{saunders2020reducing} and visual question answering \cite{niu2020counterfactual}.
This direction has also developed data augmentation strategies in various NLP tasks \cite{zmigrod2019counterfactual, kaushik2019learning, fu2020counterfactual, zeng2020counterfactual}.
Especially, counterfactual causality has been used to measure the causal effects of specific inputs in visual question answering \cite{niu2020counterfactual}.

Inspired by these applications, we apply the thought of counterfactual causality on table-based fact verification, and detect token-level salience in statements in a probing manner.








 


\section{Method}
In this section, we describe the technical details of the proposed system. 
Our system extends the NLI formulation of table-based fact verification \cite{eisenschlos2020understanding} with the pretrained language model TAPAS as the backbone (\Cref{sec:basic_model}).
As a preliminary step, our system estimates token-level salience in a probing manner for each statement (\Cref{sec:salient}). 
The proposed salience-aware learning leverages the estimated salience information from two perspectives.
From one perspective, it enhances the main task learning with an auxiliary task of masked salient token prediction (\Cref{sec:token_prediction}). 
In this auxiliary task, our system masks salient tokens in entailed statements and requires the model to jointly solve the cloze task along with the main task of fact verification.
From the other perspective, our system incorporates a probablistic data augmentation technique (\Cref{sec:aug}) by replacing non-salient tokens in statements according to a pretrained masked language model (MLM). 
This is followed by the technical details of training and inference processes (\Cref{sec:training}).
The overall architecture of our system is shown in \Cref{fig:system}.


\subsection{Base Model for Fact Verification}\label{sec:basic_model}


Our system adopts the TAPAS \cite{herzig2020tapas} model from the previous SOTA method as the base model.
In this way, we also formulate the main task of table-based fact verification as an NLI task following \citet{eisenschlos2020understanding}.


For a brief description of TAPAS,
it extends BERT’s architecture \cite{devlin2019bert} with additional positional embeddings to represent tabular structure.
Specifically, in addition to the embeddings used by BERT, the model applies column and row embeddings to represent the column index and row index of the cell enclosing the token, and rank embeddings to represent the numeric rank of the cell referring to the token if the column is sortable.
It flattens the table into a sequence of words and concatenates them with textual sequence if any as input.
The model is pre-trained using an MLM objective.
\citet{eisenschlos2020understanding} designed task-specific intermediate pretraining tasks to improve the model performance on table-based fact verification.
We use the model released by them as our basic model.
Following their setting, we add a \texttt{[CLS]} token at the beginning of the input sequence, and separate the statement and the linearized table with a \texttt{[SEP]} token.
Then, our system adopts the TAPAS model to encode the input sequence and model the probability of entailment with a task-specific prediction head taking the final representation of the \texttt{[CLS]} token as input.
Specifically, the task-specific prediction head is implemented as an MLP with the sigmoid activation fuction for binary classification, which is consistent with \citet{eisenschlos2020understanding}.






\subsection{Probing-based Salience Estimation}\label{sec:salient}
Lexical tokens usually have different levels of importance with regard to the overall content or purpose of a description \cite{chiarcos2011introduction, liu2018automatic, xiong2018towards}.
For example, in the sentence ``\texttt{Post University
has used the Eagles as its nickname}'', the tokens like ``\texttt{Eagles}'' and ``\texttt{nickname}'' are more important than others such as ``\texttt{has used}'' and ``\texttt{as}'' for determining if the sentence is refuted or entailed. 
We refer to such highly important tokens as \textit{salient} tokens, and less important ones as \textit{non-salient} tokens. 
To make use of token-level salience in the table-based fact verification task, the immediate challenge is to \textit{estimate the salience of each token in a statement}.

Inspired by the counterfactual theories of causation \cite{pearl2009causality, lewis2013counterfactuals}, we address the challenge with a probing-based salience estimation method.
Counterfactual causality has been widely used in social science for measuring the causal effects of specific factors \cite{tetlock1997counterfactual, bradycausation, morgan2015counterfactuals}, and has also been introduced to deep learning \cite{tang2020unbiased, niu2020counterfactual}.
In our context of fact verification, the intuition of counterfactual causation is to testify that: 
\textit{If the model has not seen the token, will it still make the same prediction?}
The counterfactual lies between the fact that the token is seen and the imagination that the token is masked.
The comparison between them naturally reflects the effect of the token, because the token is the only thing changed between the two situations.


Technically, to estimate the salience of a token in a statement, 
we compare the confidence score to the gold fact verification label between the statements with that token unmasked and masked.
Formally, given the table , original statement  and its counterfactual version  with the target token  masked, the salience score of  in this statement is
 
where  indicates the gold label for fact verification and  is given by the TAPAS model finetuned on TabFact. 
Larger difference between the predictions for  and  indicates the token is more salient.


\subsection{Masked Salient Token Prediction}\label{sec:token_prediction}




Salient tokens in statements, such as lexemes that appear in table cells, and those referring to aggregations and their results, directly contribute to table-text alignment and reasoning. 
Hence, they are critical to table-based fact verification as shown in \Cref{fig:intro}.
Considering the supervision signals for the verification task are sparse and not necessarily sufficient to capture fine-grained table-text alignment
and the logical relation, we introduce masked salient token prediction as an auxiliary task.

This task is to predict a masked salient token in an entailed statement given the masked statement and the respective table.
We mask the most salient token in each statement according to the salience score estimated in \Cref{sec:salient}.
The reason to do so is that it is hard to find a general threshold to split tokens in different statements into salient and non-salient groups.
The effectiveness of the salience-aware masking will be further evaluated in \Cref{sec:result}.

Both of table-based fact verification and masked salient token prediction share the same TAPAS encoder and the latter reuses the embedding layer as the language modeling head (i.e. linear layer with weights tied to the input embeddings). 
In this way, all parameters that are updated for the auxiliary task are shared with those in the main task.
Both tasks are jointly learned, so that the auxiliary task seeks to provide indirect supervision signals to improve the main task. 
The objective function and training details are described in \Cref{sec:training}.










\subsection{Salience-aware Data Augmentation}
\label{sec:aug}



To effectively learn a robust and generalized NLI model to verify statements based on tables, one requirement is sufficient training data.
Previous work has explored augmenting data 
by filling in specific statement generation templates with entities or values from the table \cite{eisenschlos2020understanding}.
These selected tokens are always detected as salient tokens by the method described in \Cref{sec:salient} as they are important to fact verification.
However, previous works ignored the fact that the statement can be presented in heterogeneous ways, and a reliable table-based fact verification model should also be adaptive and robust to heterogeneous statements.
In this context, it is intuitive to consider that the 
non-salient tokens
should not interfere the meaning and evidential support of a statement.
Accordingly, we introduce an efficient probabilistic data augmentation technique that leverages the salience of tokens from the other perspective.

We augment training data by replacing the least salient token in each statement with reasonable alternatives.
Since we expect non-salient token substitution to cause inconsequential meaning change to the original statement, such automatically generated instances will be augmented into the training data along with the original labels.
Similar to \Cref{sec:token_prediction}, we select the least salient token to augment, because it is hard to find a fixed threshold that works for all statements to justify whether each of their tokens is important enough or not.






In detail,
for each human-annotated statement, we mask the least salient token and request a BERT model to provide the top  tokens to fill in the blank.
Each a filled token gives an augmented instance of statement.
BERT is pretrained on large textual corpora with the MLM objective, so its predictions can reflect the real-world language expressions\footnote{We do not use TAPAS for data augmentation because the table is not used as input for masked sentence completion.}.
Considering the top  token substitutions are not equally confident according to the BERT predictions and potential noise in data augmentation, 
we down-weight each augmented data instance in training according to the token prediction probabilities (denoted by  for the -th augmented instance derived from the -th original instance). 
Related details are presented shortly in \Cref{sec:training}.




\subsection{Training and Inference}\label{sec:training}
We train the model to jointly conduct the main table-based fact verification task (\Cref{sec:basic_model}) using augmented data described in \Cref{sec:aug} along with the auxiliary task of masked salient token prediction (\Cref{sec:token_prediction}). 

In detail, there are two learning objectives: the binary classification objective  for the main task and the MLM objective  for the auxiliary task. 
For fact verification, we denote the gold label of the -th instance in the original dataset as  (1 for entailed and 0 for refuted).
With salience-aware data augmentation,
each original instance in the dataset is augmented to  instances (including itself).
The training instances are also assigned with the probability-based training weight  as described in \Cref{sec:aug} ( for the original instance).
Then, given the model prediction  on each instance,
the loss function is defined as the following weighted cross-entropy, where  is the number of instances in the original dataset:

For the auxiliary task, given the gold label  (1 for the target token, 0 for other tokens) and model outputs  of each candidate token  for the -th instance, the loss function is defined as below, where  is the number of all entailed statements in the dataset:
 
The overall learning objective is to optimize the following joint loss, where  is a coefficient to balance between the two task objectives:
 
In inference, given a statement and a table, we use the prediction head of fact verification independently and perform the verification without augmenting the test data, 
following the details in \Cref{sec:basic_model}. 








 








\section{Experiment}
In this section, we conduct experiments on the TabFact dataset. We first introduce the dataset, a series of recent baselines and details of our method (\Cref{sec:exp_set}). Then we show the overall performance and ablation results (\Cref{sec:result}). We also provide case studies for in-depth analysis (\Cref{sec:case}).


\subsection{Experimental Settings}\label{sec:exp_set}

\stitle{Dataset and Evaluation}
We evaluate our model on the TabFact benchmark \cite{chen2019tabfact} that is widely used by studies on this task\footnote{https://tabfact.github.io/}.
The dataset contains  statements and   tables.
Each table thereof comes along with 2 to 20 statements, and consists of 14 rows and 5 columns in average.
Each statement is paired with a table and is labeled as entailed or refuted by information in the table.
We use the originally released train, validation and test splits for evaluation, for which the statistics are listed in \Cref{table:dataset}.
Tables in these splits do not have overlaps.
Specifically, statements in the test split are further labeled into simple or complex categories according to their verification difficulty.
Additionally, a small subset of the test split is used to compare machine performance and human performance.
Being consistent with previous studies \cite{chen2019tabfact, zhong2020logicalfactchecker, yang2020program, eisenschlos2020understanding},
we report the model performance on the validation and test splits, two of the difficulty-specific subsets, as well as the small subset with human performance, and use accuracy as the evaluation metric.


\begin{table}[t]
\centering
\small
\begin{tabular}{ccc}
\toprule Split & \#Statement & \# Table  \\ \midrule
Train & 92,283 & 13,182 \\
Validation & 12,792 & 1,696 \\
Test & 12,799 & 1,695 \\ \midrule
Simple & 50,244 & 9,189 \\
Complex & 68,031 & 7,392 \\
\bottomrule \end{tabular}
\caption{Statistics of the TabFact dataset.}
\label{table:dataset}
\end{table} 


\begin{table*}[t]
\centering
{
\small
\begin{tabular}{l|p{1.5cm}<{\centering} p{1.5cm}<{\centering} ccc}
\toprule
Model & Val & Test & Test (simple) & Test (complex) & Small Test Set \\ 
\midrule
Human Performance & - & - & - & - & 92.1 \\
\midrule
LPA & 65.2 & 65.0 & 78.4 & 58.5 & 68.6 \\
LogicalFactChecker &71.8& 71.7& 85.4& 65.1& 74.3\\
HeterTFV & 72.5 & 72.3 & 85.9 & 65.7 & 74.2 \\
ProgVGAT& 74.9 & 74.4& 88.3& 67.6& 76.2\\ \midrule
Table-BERT &66.1&65.1&79.1&58.2&68.1 \\
TAPAS \cite{dong2021structural} & - & 76.0 & 89.0 & 69.8 & - \\
TAPAS \cite{eisenschlos2020understanding} &81.0 &81.0& 92.3& 75.6& 83.9 \\
\midrule
ours & \textbf{82.7} & \textbf{82.1} & 93.3 & \textbf{76.7} & 84.3 \\
\ \ -- w/o augmented data & 82.4 & 82.1 & 93.4 & 76.6 & \textbf{84.4} \\
\ \ -- w/o auxiliary task & 81.8 & 81.9 & \textbf{93.6} & 76.3 & 84.1 \\
\bottomrule
\end{tabular}
}
\caption{Performance on the official splits of TabFact in terms of verification accuracy (\%).
Baselines are organized into \textit{logical program-driven} (i.e. LPA, LogicalFactChecker, HeterTFV and ProgVGAT) and \textit{non-logical program-driven} (i.e. Table-BERT and TAPAS). Human performance is reported by \citet{chen2019tabfact}.
}
\label{table:result}
\end{table*} 

\stitle{Baselines}
We compare our system with the following competitive baselines:
\begin{itemize}[leftmargin=1em]
\setlength\itemsep{0.2em}
\item Latent Program Algorithm (LPA) \cite{chen2019tabfact} synthesizes logical programs based on the given statement and table, executes programs to return bool labels, and aggregates the results according to the confidence score of each program assigned by a Transformer-based model. 

\item LogicalFactChecker \cite{zhong2020logicalfactchecker} captures token-level semantic interaction between a statement, a table and a derived program using BERT with graph-based masking. Logical semantics of each program is captured with neural module networks \cite{andreas2016learning}. 



\item HeterTFV \cite{shi2020learn} constructs a heterogeneous graph to incorporate the statement, the table and the program, and applies a heterogeneous graph attention network to capture both linguistic and symbolic information.


\item ProgVGAT \cite{yang2020program} generates a program and verbalize the execution progress as evidences. The system applies a graph attention network \cite{velivckovic2017graph} to capture the execution graph, the table and statement.



\item Table-BERT \cite{chen2019tabfact} applies BERT for NLI taking a statement as the hypothesis and a linearized table as the premise.

\item TAPAS \cite{herzig2020tapas} is a Transformer-based model pre-trained on textual and tabular data. \citet{dong2021structural} and \citet{eisenschlos2020understanding} have formulated table-based fact verification as an NLI task, and applied TAPAS with task-specific intermediate pretraining. The latter one achieves the current SOTA performance on TabFact.

\end{itemize}



\stitle{Model Configurations}
Our system also adopts the officially released TAPAS-Large model, which applies intermediate pre-training and is fine-tuned on TabFact, as our base model\footnote{https://github.com/google-research/tapas}.
Following \citet{eisenschlos2020understanding}, we set the max input length to .
We use  training steps, and optimize the learning objective with an AdamW optimizer \cite{loshchilov2018decoupled} which sets the learning rate to , a batch size of  and a warmup ratio of .
All hyper-parameters are decided according to the validation performance.
For multi-task learning, we set the coefficient between two losses  to .
For data augmentation, we use the uncased BERT-Large model as the MLM.
For computational efficiency, we select the top  predictions for probabilistic data augmentation.



\subsection{Results}\label{sec:result}

\stitle{Overall Performance}
\Cref{table:result} presents the results of different verification models. 
Among the baseline methods, TAPAS with task-specific intermediate pretraining demonstrates the best performance. It implies that explicit logical programs is not a necessity for reasoning between the table and the statement.
We observe that our system outperforms the best baseline with 2.1\% relative improvement on the validation set and 1.4\% relative improvement on the test set in terms of accuracy.
It is noteworthy that, our system applies the same backbone model and pretraining process as the previous best method, so that all the improvements are attributed to the salience-aware learning strategies.
Besides, our system reduces the gap between machine performance and human performance on the small test set to 7.8\%.
These experimental results verify our hypothesis that masked salient token prediction and salience-aware data augmentation are conducive to table-based fact verification.




\begin{table}[h]
\small
\centering
\begin{tabular}{c|l|cc}
\toprule
 & Strategy & Val & Test   \\ \midrule
\multirow{2}{*}{Masking} & Random  & 82.1 & 81.9 \\
& Salient  & \textbf{82.4} & \textbf{82.1} \\ \midrule
\multirow{2}{*}{Augmentation} & Uniform  & 81.5 & 81.3 \\
& Probabilistic  & \textbf{81.8} & \textbf{81.9} \\
\bottomrule
\end{tabular}
\caption{
Ablation results for masking strategy and augmentation strategy.
To avoid co-effects, we conduct experiments on  masking (or augmentation) strategy  without using augmented data (or auxiliary task).}
\label{table:ablation}
\end{table} 


\begin{table*}[t]
\small
\centering
\begin{tabular}{  m{12cm} | m{1.6 cm} m{1 cm} } \toprule
\makecell[c]{Token Salience Estimation} & \multicolumn{2}{c}{Augmentation}
\\ \midrule
{\setlength{\fboxrule}{1pt}
\fcolorbox{white}{orange!5}{The} 
\fcolorbox{white}{orange!10}{file}
\fcolorbox{white}{orange!20}{format} 
\fcolorbox{white}{orange!30}{mobipocket} 
\fcolorbox{red}{orange!2}{comes}
\fcolorbox{white}{orange!5}{with}
\fcolorbox{white}{orange!30}{all}
\fcolorbox{white}{orange!50}{three}
\fcolorbox{blue}{orange!80}{supports}
\fcolorbox{white}{orange!5}{.}
}
&
\makecell[l]{
works \\
worked \\
compatible \\
}
& 
\makecell[l]{
0.615 \\
0.051 \\
0.029
} 
\\  \midrule
{\setlength{\fboxrule}{1pt}
\fcolorbox{white}{orange!5}{The} 
\fcolorbox{red}{orange!2}{player}
\fcolorbox{white}{orange!5}{from} 
\fcolorbox{white}{orange!5}{the} 
\fcolorbox{blue}{orange!80}{Santiago}
\fcolorbox{white}{orange!40}{province}
\fcolorbox{white}{orange!5}{lives}
\fcolorbox{white}{orange!5}{in}
\fcolorbox{white}{orange!5}{the}
\fcolorbox{white}{orange!40}{city} 
\fcolorbox{white}{orange!20}{navarrete}
\fcolorbox{white}{orange!5}{.}
}
&
\makecell[l]{
population \\
people \\
one 
}
& 
\makecell[l]{
0.352\\
0.184 \\
0.035
}
\\ \midrule
{\setlength{\fboxrule}{1pt}
\fcolorbox{white}{orange!78}{Canton} 
\fcolorbox{white}{orange!2}{,}
\fcolorbox{white}{orange!5}{Ohio} 
\fcolorbox{white}{orange!5}{was} 
\fcolorbox{white}{orange!5}{the}
\fcolorbox{white}{orange!5}{location}
\fcolorbox{red}{orange!2}{for}
\fcolorbox{white}{orange!2}{the}
\fcolorbox{white}{orange!5}{event}
\fcolorbox{white}{orange!5}{,} 
\fcolorbox{white}{orange!40}{fightfest}
\fcolorbox{white}{orange!5}{2}
\fcolorbox{white}{orange!5}{,}
\fcolorbox{white}{orange!5}{which}
\fcolorbox{white}{orange!5}{lasted}
\fcolorbox{white}{orange!5}{only}
\fcolorbox{blue}{orange!80}{3}
\fcolorbox{white}{orange!10}{rounds}
\fcolorbox{white}{orange!5}{.}
}
&
\makecell[l]{
of \\
to \\
in \\
}
& 
\makecell[l]{
0.709 \\
0.001 \\
0.001
}
\\ \bottomrule
\end{tabular}
\caption{
Examples of salience estimation and data augmentation. 
Darker background indicates more salience.
\fcolorbox{blue}{white}{Blue} rectangles mark the targeted most salient tokens in masked salient token prediction.
\fcolorbox{red}{white}{Red} rectangles mark the least salient tokens that are to be substituted by the augmentation tokens, for which weights are listed.
}
\label{table:case}
\end{table*}



\stitle{Effect of Masked Salient Token Prediction}
The performance of the base model with masked salient token prediction is marked as ``{w/o augmented data}'' in \Cref{table:result}.
The auxiliary task solely brings along 1.7\% relative improvement on the validation set and 1.4\% relative improvement on the test set.
This demonstrates that the indirect supervision brought by the auxiliary task can directly benefit the main task training.
\Cref{table:ablation} compares salient masking and random masking 
for the auxiliary task.
For fair comparison, we mask one token in each entailed statement for both strategies.
The results show that salient masking reduces error rate on the validation set by relative 1.7\% (and by relative 1.1\% on the test set) in comparison with random masking.
This is not surprising since random masking may mask non-salient tokens which are not decisive for table-text alignment and logical inference.


\stitle{Effect of Salience-aware Data Augmentation}
The performance of the base model with salient-aware data augmentation is marked as ``w/o auxiliary task'' in \Cref{table:result}.
The data augmentation independently brings 1.0\% relative improvement on the validation set and 1.1\% relative improvement on the test set.
The results demonstrate that table-based fact verification requires abundant training data and verify the effectiveness of the proposed data augmentation strategy.
\Cref{table:ablation} compares probabilistic weights and uniform weights.
The results show that probabilistic data augmentation reduces error rate on the validation set by 1.6\% relatively (and by 3.2\% relatively on the test set) in comparison with uniform data augmentation.
This observation is reasonable because the augmented data are not equally confident according to the MLM predictions.
Moreover, the predicted probabilities from the pretrained language model correlate with real-world distribution of English language.


\stitle{Performance on Simple and Complex Instances}
We further compare the performance of baselines and variants of our system on two groups of test instances labeled with different verification difficulties. 
Our system outperforms all the baselines on both simple and complex instances with at least 1.0\% absolute improvement.
Ablation results in \Cref{table:result} also show that the auxiliary task improves the base model more on complex instances while data augmentation improves the base model more on simple instances.
These results are consistent with the features of the two salience-aware learning strategies.
Masked salient token prediction seeks to enhance the model to capture table-text alignment and the underlying logical relations,
so that complex instances requiring more complicated reasoning gain more benefits.
Salience-aware data augmentation seeks to 
augment statements
by simply replacing non-salient tokens.
This strategy increases the training data but does not augment the implicit logical form covered by the dataset so that the improvement on complex instances is not as significant as that on simple instances.












\subsection{Case Study}
\label{sec:case}


We present a case study with three representative examples to illustrate salience estimation and data augmentation in \Cref{table:case}.
The detected salient tokens can be entities and numeric values from the table, tokens indicating relations, and the results of logical operations.
Non-salient tokens can be common nouns, verbs, prepositions and so on.
These tokens are detected as non-salient because they are not closely associated with facts in the given table.
For example, the table in the second example is about the residence of different athletes, so ``\texttt{player}'' in the statement may be substituted to related terms without interfering the verification result.
It is noteworthy that entities consisting of multiple words tend to have relatively small salience scores for some parts.
It may be due to that verification models can identify the corresponding cell by part of the entity.
But it also raises the risk of incorrect verification or polluted data augmentation when modifying a part of a multi-word entity.






















 \section{Conclusion}

In this paper, we proposed a novel system for table-based fact verification.
Our system employs salience-aware learning and introduce complementary supervision signals by leveraging both salience and non-salient tokens from different perspectives. 
The system consists of three key techniques, including probing-based salience estimation, masked salient token prediction and salience-aware data augmentation.
Experiments on the TabFact benchmark show that our system leads to significant improvements over the current SOTA systems.
For future work, we plan to extend salience-aware learning to other NLU tasks, including NLI \cite{bowman2015large, williams2018broad} and Tabular QA \cite{sun2016table,chen2020hybridqa}.
Applying the idea of salience estimation to NLG tasks, such as controlled table-to-text generation \cite{parikh2020totto} and paraphrasing \cite{iyyer2018adversarial, huang2021generating}, is another meaningful direction.  \section*{Ethical Consideration}

This work does not present any direct societal consequence. 
The proposed work seeks to develop a salience-aware learning framework for fact verification using tabular data as evidence. 
We believe this leads to intellectual merits that benefit claim and statement verification for Web corpora, as well as detection of misinformation. 
It potentially also has broad impacts for NLU and NLG tasks where tables serve as a medium of knowledge sources.
The experiments are conducted on a widely-used open benchmark.


The goal of this research topic is to help identify misinformation, which seeks to benefit societal fairness.
While we treat tables as reliable sources of evidences like relevant studies do, we do not hypothesize that the populated information by Web users in tables is not completely free of societal bias. We believe this is a meaningful research direction for further exploration. While not being explicitly studied in this work, the incorporation of salience-aware inference could be a way to control or mitigate societal biases.



\section*{Acknowledgement}

We appreciate the anonymous reviewers for their insightful comments.

This research is supported by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under contract number FA8650-17-C-7715, by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, and by the National Science Foundation of United States Grant IIS 2105329.
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government. 
\bibliographystyle{acl_natbib}
\bibliography{sections/custom}



\end{document} 