\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\usepackage{subfig}

\title{Ikshana : A Theory of Human Scene Understanding Mechanism}


\author{
  Venkata Satya Sai Ajay Daliparthi \\
  Department of Computer Science\\
  Blekinge Insitute of Technology\\\
  Karlskrona, Sweden \\
  \texttt{Veda18@student.bth.se} \\


}

\begin{document}
\maketitle

\begin{abstract}
In recent years, deep neural networks achieved state-of-the-art performance on many computer vision tasks. The two most commonly observed drawbacks of these deep neural networks are: the requirement of a massive amount of labeled data and a vast number of parameters. In this work, we propose a theory named Ikshana, to explain the functioning of the human brain, while humans understand a natural scene/image. We have designed an architecture named IkshanaNet and evaluated on the Cityscapes pixel-level semantic segmentation benchmark, to show how to implement our theory in practice. The results showed that the Ikshana theory could perform with less training data. Also, through some experiments evaluated on the validation set, we showed that the Ikshana theory can significantly reduce the number of parameters of the network. In conclusion, a deep neural network designed by following the Ikshana theory will learn better vector representations of the image, useful for any computer vision task.
\end{abstract}



\section{Introduction}
The human brain can seamlessly perceive several perceptual and semantic information regarding the natural scene/image during a glance. The visual scene information perceived during/after a glance refers to the gist (a summary) of the scene/image. The gist includes all the visual information from the low-level (e.g., colors and contours) to the high-level (e.g., shapes and activation). Due to this reason, some studies \cite{oliva2005gist} suggested that the gist can be investigated at both the perceptual and conceptual level. The structural representation of the image refers to the perceptual gist, and the semantic information of the image refers to the conceptual gist. However, the conceptual gist is more refined and modified than the perceptual gist. Depending on the situation and the environment, the human brain can seamlessly grasp the required information regarding the scene/image, by recognizing the objects and observing their structure. On the other hand, for a computer to do the same is the fundamental goal of the computer vision field. \newline
In recent years, deep learning methods have shown a significant improvement over traditional handcrafted techniques on several computer vision tasks. Though these deep neural networks achieved state-of-the-art performance in many cases, the two major drawbacks of these networks are: the requirement of a massive amount of labeled data and a vast number of parameters. The collection of labeled data is an expensive and time taking process. Further, the increasing number of parameters will make it difficult for the deep neural networks to scale during deployment. Particularly in the fields such as autonomous driving, the computer inside the vehicle should provide efficient real-time results. Also, in the embedded systems, the size of the network plays a crucial role in the performance. Several few-shot learning methods and real-time methods addressed these drawbacks of the deep neural networks.\newline
 Even though the deep neural networks are said to be inspired by the functioning of the human brain, is this how human brain learns to perform any visual task? \textbf{NO}. Because, the human brain does not require a massive amount of labeled data to perform any visual task, and it has the ability to perform with few data samples. However, we cannot observe a similar phenomenon in the case of many deep neural networks. In this work, we argue that there is a disconnection between the current deep learning methods and neuroscience. Several works\cite{rayner1998eye}\cite{oliva2000diagnostic}\cite{henderson2003human}\cite{evans2005perception}\cite{fei2007we} in neuroscience have addressed the fundamental question, i.e.,``how does the human brain perform the visual tasks?" by investigating through conceptual and perceptual gist. They conducted
several experiments and proposed various theories to explain the functioning of the human brain. However, there was no general principle that explains the functioning of the human brain and its mechanisms. Even though there is a general principle, we expect that to be different from human-to-human. \newline
Motivated by the fact that a better learning algorithm will have the ability to perform with few data samples. In this work, we addressed the common drawbacks of the deep neural networks, i.e., the requirement of a vast number of parameters and labeled data, by proposing a theory of human scene understanding mechanism named \textit{Ikshana}. The word Ikshana was derived from the Sanskrit language, which has many synonyms such as the eye, sight, look, etc. The idea is that ``To understand the conceptual gist of a given image, humans look at the image multiple times recurrently, at different scales."
We selected the high-level computer vision task semantic segmentation and the Cityscapes benchmark to show how to implement the Ikshana theory in practice.\newline
\textbf{Semantic segmentation} is the task of assigning a class label to every pixel in the given image, which has many applications in various fields such as medical, autonomous driving, robotic navigation, localization, and scene understanding. We designed a simple deep neural network architecture by following the Ikshana theory named IkshanaNet, which has only 4M parameters. Without any pre-training, we trained the IkshanaNet on the Cityscapes test set in three different ways: IkshanaNet- was trained on the entire training set ( images), IkshanaNet- was trained the half of the training set ( images), and IkshanaNet- was trained on the quarter of the training set ( images). We evaluated these variants on the Cityscapes benchmark and observed that the variants trained on less training data shown considerable performance. We observed only a  IOU percentage points difference after losing half of the training data. During our initial architectural search of the IkshanaNet, the networks designed by following the Ikshana theory, have shown similar phenomena when trained on less training data.
Also, through some experiments evaluated on the validation set, we showed that the Ikshana theory would help in reducing the number of parameters of the deep neural networks.\newline 
In this work, we argue that using image classification networks (such as VGGNet\cite{7486599}, ResNet\cite{he2016deep}, DenseNet\cite{huang2017densely}, and etc.) as a backbone to learn representations from the image is the bottleneck causing the disconnection with neuroscience. Because, the most common architectural pattern observed in many deep neural network architectures was to learn a representation with  filters (say ) from the input image and go deeper on top of that representation () until the network achieves adequate performance. According to our Ikshana theory, this commonly observed architectural pattern was like teaching the computers to learn by looking at the image once. Even though these deep neural network architectures are learning  filters from the input image, we argue that they are not sufficient for many high-level computer vision tasks, where the spatial location of the objects and their relation plays a crucial role in the output.
Instead of that, the Ikshana theory suggests looking at the image multiple times recurrently by learning representations from the input image every time.\newline
The aim of this work is to find a better learning algorithm to perform with a few data samples and parameters. Here, we are neither addressing the semantic segmentation task or competing with state-of-the-art techniques. Hence, we are not comparing our method to any other works.\newline
The Ikshana theory is the main contribution of this work and not the IkshanaNet architecture. The architecture is an example to show how to implement the Ikshana theory in practice. Further investigation is needed to develop a new backbone architecture by following the Ikshana theory,  which will contribute to several computer vision tasks.

\section{Related work}
In \textbf{Neurological} terms, all the low-level and high-level computer vision tasks come under a single term called human scene understanding. 
In human scene understanding, a scene was a view of a real-world environment that contains multiple surfaces and objects, organized in a meaningful way. 
The gist (a summary) refers to the visual information perceived after/ during a glance at scene\cite{oliva2005gist}. Some early works\cite{potter1975meaning} \cite{intraub1981rapid} on conceptual gist explained that a typical scene fixation of  to  was often sufficient to understand the gist of the image. However, many prominent works \cite{rayner1998eye} \cite{schyns1994blobs}\cite{oliva2000diagnostic}\cite{oliva2001modeling}\cite{henderson2003human}\cite{evans2005perception}\cite{fei2007we} in neurosciene investigated the perceptual gist. Several findings have shown that, how the modelling and shape of the scene was done in the human brain. However, in most of the cases, we need the neural networks to understand the semantic information of the given image. So, our theory focuses on the conceptual gist rather than perceptual gist. Some works\cite{oliva2000diagnostic}\cite{oliva2001modeling} empirically showed that a spatial resolution of eight cycles/image and four cycles/image was sufficient to identify the objects in gray-scale and color images, respectively. \newline
\textbf{Neural networks} were there since 's\cite{mcculloch1943logical}\cite{rumelhart1986learning}\cite{rosenblatt1958perceptron}\cite{lin2013network} but some prominent works\cite{deng2009imagenet}\cite{krizhevsky25hinton}\cite{he2016deep} made them popular during recent years. In our work, we used the convolutional neural network (CNN) architecture\cite{lecun1998gradient} to learn representations from the images, which itself was inspired by a work on human receptive fields\cite{hubel1962receptive}. The architecture of the IkshanaNet was inspired by VGG\cite{7486599} and  Densenet\cite{huang2017densely}.\newline
The first prominent work on \textbf{Semantic segmentation} using deep learning was the fully convolutional networks (FCN)\cite{long2015fully}. Later on, many semantic segmentation networks followed the idea of the skip-connections introduced by FCN. The total prominent works on deep learning-based semantic segmentation methods can be roughly classified into five categories.  They are encoder-decoder based methods (DeconvNet\cite{noh2015learning}, SegNet\cite{badrinarayanan2017segnet}, and U-Net\cite{ronneberger2015u}), regional proposal based methods (MaskRCNN\cite{he2017mask}, FPN\cite{lin2017feature}, PANet\cite{liu2018path}, and PSPNet\cite{zhao2017pyramid}), dilated convolutions and increased resolution of feature map methods (DeepLab\cite{chen2017deeplab}, DeepLab V3+\cite{chen2018encoder}, and HRNet\cite{SunXLW19}), context information methods (DANet\cite{fu2019dual}, HANet\cite{choi2020cars}, and OCR\cite{10.1007/978-3-030-58539-6_11}), and boundary refinement methods (Gated-SCNN\cite{takikawa2019gated} and SegFix\cite{yuan2020segfix}). Most of these above mentioned works, achieved state-of-the-art performance at a certain time in the past. However, these state-of-the-art models had complex architectures and requires a huge number of parameters. Due to this reason, most of these methods fail to perform in real-time. Especially, in the fields such as the autonomous driving, where the goal was to provide efficient results in real-time. \newline
\textbf{Real-time semantic segmentation} methods such as  ENet\cite{paszke2016enet}, ICNet\cite{zhao2017pyramid}, ESPNet\cite{mehta2018espnet}, HardNet\cite{chao2019hardnet}, SwiftNet\cite{orsic2019defense} and BiseNet\cite{yu2018bisenet} were introduced to deal with scalability drawbacks of the S-O-T-A models. 
Another common drawback of the deep neural networks was the requirement of a massive amount of labeled data.\newline
\textbf{Few-shot segmentation} methods such as PANet\cite{wang2019panet}, Attention-based\cite{hu2019attention}, Texture-bias\cite{azad2020texture}, etc. were introduced to address the massive labelled data requirement.
Almost in many computer vision tasks, we can observe this similar kind of phenomena, such as the state-of-the-art methods, the real-time methods, and the few-shot learning methods.\newline
To our knowledge,  our work was inspired and related to the above-mentioned works. There might be a chance that some works may be directly/indirectly related to this work and were not mentioned here.




\section{Method}
\subsection{Ikshana (The Eye) Theory}
In her prominent work\cite{potter1975meaning}, professor Mary C. Potter found that an average human can understand the gist of the image in between the time interval of  to . During that time interval, the Ikshana theory approximates the functioning of the human brain.\newline
The Ikshana theory states that \textbf{``To  understand the conceptual gist of a given image, humans look at the image multiple times recurrently, at different scales.''} The word Ikshana was derived from the Sanskrit language, which has many synonyms such as the eye, sight, look, etc. \newline
An example to explain the Ikshana theory is presented in figure \ref{fig:ikshana}, where there is an image () on the left side and the human brain's mechanism on the right side.
According to Ikshana theory, for a human to understand the conceptual gist of the given image the following process is occuring in the human brain: \newline
At a time step (), during the first glance (),  the human brain learns the first representation () from the image and stores that representation in the memory (), as shown in the equation .

At a time step (), during the second glance (), the human brain holds the first representation () in the memory and learns the second representation () from the image. Then the brain stores both the representation () along with () in the memory(), as show in the equation .

At a time step (), during the third glance (), the human brain holds the first and the second representations ( and ) in the memory and learns the third representation () from the image. Then the brain stores the representation () along with ( and ) in the memory(), as show in the equation .  

This kind of recurrent process occurs at () times at a single image scale. Depending upon the given task (), by combing all the information stored in the memory until ()'th time step, human understands the conceptual gist () of the image at a single scale, as shown in the equation . 

This process occurs at different scales () and generates the outputs (, ,,.... ).  By considering all the outputs at different scales, the human brain selects some of those representations and forgets the remaining representations. In this way, the human brain learns() the final output () of the given visual task (), as shown in the equation .

From the equations , , , , and , this is how the Ikshana theory approximates the functioning of the human brain, while human understands the conceptual gist of the image.\newline
The time taken(/number of glances required) by an average human to understand the gist of the image depends upon several factors such as the given task, age, intelligence, memory, etc. However, the number of glances required by a neural network to learn the semantic information from the given image, depends upon the given visual task and performance requirement in the application field.\newline 
\newline
\textbf{Note:} According to the Ikshana theory, the most commonly used architectures for learning representations from the input image, such as VGGNet\cite{7486599}, ResNet\cite{he2016deep}, DenseNet\cite{huang2017densely}, and etc., are like learning () with  filters from the image and then going deep on top of that () representation until the network achieves adequate performance. Instead of that, our theory suggests going deeper by learning  representations from the input image and preceding layers every-time, until the network achieves adequate performance.
\newline
\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{Ikshana.png}
\end{center}
   \caption{Ikshana Theory at Single Scale}
\label{fig:ikshana}
\end{figure}



\subsection{Architecture}

To implement the Ikshana theory in practice, a novel architecture named IkshanaNet was introduced. Humans can look at the image and seamlessly learn various representations \cite{potter1975meaning} regarding the image. However, for a computer to do the same, the convolutional neural network architecture was used to learn representations from the image.
Humans need the spatial resolutions around eight cycles/images to understanding the gist of the given image\cite{oliva2000diagnostic}\cite{oliva2001modeling}. However, for a computer to do the same, three scales are selected in the IkshaNet architecture.
The network size was MB and had M parameters. The entire architecture of the IkshanaNet consists of three basic building blocks; they are () the glance module, () the projection module, and () a x convolutional layer, as shown in figure \ref{fig:IkshanaNet}. \newline
The \textbf{glance module} consists of three x convolutional layers (with different dilation rates), and it was used to learn representations from the given feature map. Even though the number of input filters passed into the glance module varies several times, the output feature map always returns a  filtered feature map. The \textbf{projection module} consists of three x convolutional layers that take in and returns the same number of filters every time. Every convolutional layer in the glance and projection modules was followed by batch normalization and a ReLU activation layer. The \textbf{x convolution layers} were used to reduce the number of filters in a given future map. Except for the last x convolutional layer, that returns the output, every  x convolutional layer in the architecture was followed by batch normalization and a ReLU activation layer. \newline 
The entire architecture of IkshanaNet could be briefly divided into four parts; they are the scale , the scale , the scale , and the output.\newline
\textbf{At scale }, the input image was passed through a glance module with a dilation rate (d), that returns a feature map with  filters. Then the input image was concatenated with the previously learned feature map . \textbf{The concatenation of the input image in the feature map was the crucial step, to make sure that we are learning representations from the input image every time.} Then, the feature map was passed through another glance module with a dilation rate (d), that returns another feature map with  filters. Then the resulting feature map was concatenated with the feature-maps from the preceding layers  and passed through another glance module with dilation rate (d), which takes in  filters and returns  filters.  Again, the resulting feature-map was concatenated with feature-maps from the preceding layers . At this step, the input image was removed from the feature map through tensor slicing.\textbf{This step was crucial to make sure that the input image was removed from the feature map}. Then the resulting feature map will have  filters learned from three glances modules. \textbf{In this way, the network followed the Ikshana theory and had three glances at the input image, multiple times recurrently}. Then that feature map was passed through a projection module. Here, there was a side branch, a x convolutional layer that takes the  filtered feature map as input returns a feature map with  filters, named side one output (). Parallelly, the feature map was passed through an average pooling layer, which reduces the size of the feature map by a factor of two. The resulting feature map was then passed to the scale  part. \newline
\textbf{At scale } part, the input image was down-sampled by a factor of two and concatenated with the feature map from the scale  . The feature map with  filters was then passed through three glance modules with different dilation rates (d=,,), which returns a feature map with  filters . Then the image in the feature map was removed from the feature map and then passed through a projection module. Here, there was another side branch, a x convolutional layer, that takes the  filtered feature map as input returns a feature map with  filters, named side two output (). Parallelly, the feature map was passed through an average pooling layer, which reduces the size of the feature map by a factor of two. From here, the feature map was passed to the scale  part.\newline
\textbf{At scale } part, the input image was down-sampled by a factor of four and concatenated with the feature map from the above layers . The feature map with  filters was then passed through three glance modules with different dilation rates (d=,,), and result in a feature map with  filters . Then the image in the feature map was removed from the feature map and then passed through a projection module. Here, there was a final side branch, a x convolutional layer, that takes the  filtered feature map as input and returns a feature map with  filters, named side three output ().\newline
Finally, the outputs from two scales ( and ) are up-sampled to match with the size of the input image and then concatenated . From, that a feature map with  filters was selected through a x convolutional layer,i.e, the output  of the network. 
\begin{figure}[t]
\begin{center}
   \includegraphics[height = 7.7in,width =7cm]{IkshanaNet.png}
\end{center}
   \caption{The Architecture of IkshanaNet}
\label{fig:IkshanaNet}
\end{figure}
\section{Experiments and Results}
\subsection{Dataset}
The Cityscapes\cite{cordts2016cityscapes} pixel-level semantic labeling dataset was selected as a benchmark to evaluate the IkshanaNet. By using the preparation scripts,  classes in the dataset were converted into  classes. The cityscapes fine-labels dataset was the only dataset used for training, and no other dataset was used for pre-training.
\subsection{Pre-Processing}
The images were resized to half of the original resolution, i.e., 
 x . The mean values of  and standard deviation values of  were used to normalize the image data. These are the pre-processing steps, and no data augmentation steps were involved in this experiment.
\subsection{Hyper-Parameters}
In this experiment, the following hyper-parameters were used while training the models:
\begin{itemize}
    \item \textbf{Criterion:} Cross entropy loss
    \item \textbf{Learning rate:} The learning rate scheduler, ReduceLROnPlateau (mode = min, factor = , patience = , verbose = ) was used with the initial learning rate of .
    \item \textbf{Optimizer:} SGD + Momentum ()
    \item \textbf{Epochs:} 
    \item \textbf{Batch size:} 
\end{itemize}
\textbf{Note:} Different momentum values such as , , , , and  were tuned and found that the IkshanaNet performs better with the momentum value of . This was the only hyper-parameter tuning step involved in this experiment. 
\subsection{Specifications}
\begin{itemize}
    \item \textbf{Framework:} PyTorch 
    \item \textbf{GPU:} NVIDIA Tesla T x  ( GB RAM)
    \item \textbf{CUDA Version:} 
\end{itemize}
\subsection{Independent and Dependent Variables}
\subsubsection{Independent Variables} Independent Variables of this experiment were the three variants of the IkshanaNet, as follows:
\begin{enumerate}
    \item The IkshanaNet- was trained with  training set images, and  validation set images.
    \item The IkshanaNet- was trained with  training set images (half of the original training set), and  validation set images.
    \item The IkshanaNet- was trained with  training set images (the quarter of the original training set), and  validation set images.
\end{enumerate}
\subsubsection{Dependent Variables} Dependent Variables of this experiment were the class-wise IOU score and category-wise IOU scores. All three independent variables were evaluated on  test set images of the Cityscapes pixel-level semantic segmentation benchmark.
\subsection{Results}
\subsubsection{Quantitative results}
The qualitative results of all three independent variables were provided in table . Only average class IOU and category IOU results were present in table 1. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Method & Avg. class IOU & Avg. Category IOU\\
\hline 
IkshanaNet- &  &  \\
\hline
IkshanaNet- &  &  \\
\hline
IkshanaNet- &  & \\
\hline
\end{tabular}
\end{center}
\caption{Quantitative Results}
\end{table}
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Method & Parms & Class IOU & Cat IOU\\
\hline 
IkshanaNet-M & M&  & \\
\hline 
IkshanaNet- & K&  & \\
\hline
IkshanaNet- & K &  &  \\
\hline
IkshanaNet- & K &  & \\
\hline
IkshanaNet- & K &  &  \\
\hline
\end{tabular}
\end{center}
\caption{Val Set Results}
\end{table}

\subsubsection{Qualitative results}
\begin{figure}[t]
\begin{tabular}{cccc}
\textbf{Input Image} & \textbf{IkshanaNet-}& \textbf{IkshanaNet-}& \textbf{IkshanaNet-}\\
& & &\\
\subfloat{\includegraphics[height =0.8in,width =1.5in]{berlin_in.png}}&
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{berlin_1.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{berlin_2.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{berlin_3.png}}\\
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bielefeld_in.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bielefeld_1.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bielefeld_2.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bielefeld_3.png}}\\
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bonn_in.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bonn_1.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bonn_2.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{bonn_3.png}}\\
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{leverkusen_in.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{leverkusen_1.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{leverkusen_2.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{leverkusen_3.png}}\\
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{mainz_in.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{mainz_1.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{mainz_2.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{mainz_3.png}}\\
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{munich_in.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{munich_1.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{munich_2.png}} &
\subfloat{\includegraphics[height =0.8in,width = 1.5in]{munich_3.png}}\\
& & &\\
\end{tabular}
\caption{Quantitative results}
\end{figure}

\section{Discussion}

The following interpretations and observations were made after conducting the experiments:
\begin{enumerate}
\item From table , it was observed that the difference between the IkshanaNet- (trained on half of the training data) and the IkshanaNet- (trained on full training data) was around  IOU percentage points. 
The IkshanaNet- (trained on the quarter part of the training data) achieved  IOU points lesser than the IkshanaNet- and  IOU points lesser than the IkshanaNet-. Even after losing the training data, our method managed to achieve considerable performance. 
\item We observed a similar phenomenon many times during our initial experiments evaluated on the validation set (We provided some of them in the supplementary material, in the folder named Ikshana Theory). The reason behind the occurrence of this phenomenon was the Ikshana theory of human scene understanding mechanism. 
\item Humans can easily perceive and understand a variety of information by having a quick glance at the images. However, for computers, the images are nothing but a bunch of numbers arranged in a matrix.  As of our knowledge, placing multiple CNN layers in the network will perform multiple matrix multiplications on the input image to transform it into the desired output. In this work, we argue that if the input image has lots of useful information regarding the scene, then why not use that information multiple times, rather than using it once. 
\item As of our knowledge, every time the image was passed through a convolutional layer, we were losing some information regarding the image and learning some useful representations from it. Most of the current literature on deep neural networks discussed in such a way that, ``the ability of CNNs to effectively down-sample the input image into a latent vector space was an advantage for low-level tasks such as image classification (where output was just a number representing the class of the image). On another side, it becomes a disadvantage for using CNNs for high-level tasks such as semantic segmentation (where output was of the same size transformed image)''. However, we partially agree with the current literature. Instead, we argue that Professor Yann LeCun did an exceptional work by replicating the human receptive fields for the machines. The CNN architecture might be sufficient for dealing with many computer vision tasks.
\item In semantic segmentation, early works such as FCN addressed the loss of information problem by CNNs through introducing skip-connections from higher-layers to the lower-layers in the network. Later on, many other works on semantic segmentation using deep neural networks followed the idea of skip connections and proposed many encoder-decoder architectures. However, in this work, we are proposing a completely different approach for semantic segmentation and other computer vision tasks. Several empirically proved best-performing concepts in semantic segmentation using deep neural networks, such as skip-connections, Atrous Spatial pyramid pooling (ASPP), Object contextual representations (OCR), Attention Based multi-scale networks, SegFix and etc. can be easily integrated to a network designed by following the Ikshana theory.
\item In this work, we are suggesting that learning representations from the input image every time will help the network to learn better vector representations of the image, instead of the traditional way (of learning a representation with  filters from the input image and going deep on top of that representation until the network achieves adequate performance).
\item During our initial experiments, the networks designed by following the Ikshana theory showed considerable performance with less number of parameters. In table , we presented the results of a few different architectures designed by following our theory, evaluated on the validation set.  Note: In table , the method named IkshanaNet-main was the architecture that we discussed in section . Here, the images were resized to the resolution of  x , and the number of training epochs was . However, all these methods were trained with the same hyper-parameters. (In the supplementary material, we provided the source code and class-wise IOU results for all of those architectures.) 
\item In table , we can observe that a network with 88K parameters shown considerable performance compared to the IkshanaNet-M with M parameters. We expect our theory to help in designing lightweight architectures suitable for embedded and mobile devices, where there was limited computational power available.
\item The universal approximation theorem states that a deep neural network with large enough depth will have the ability to approximate any complex function. However, in real-time methods, it was evident that even small networks were approximating these complex functions. In this work, we suggest going deeper by learning multiple representations from the input image rather than the traditional method.
\item In this work, we are not making any claims, such as the current deep neural networks are not the correct way of learning. Instead, we think that the current deep learning methods were like training a super-intelligent neural network that can look at the image once and perform many computer vision tasks, and even achieved state-of-the-art performance in most cases. However, we are doing that at the cost of millions and even sometimes billions of parameters. Through this work, we suggest letting the network look at the images multiple times recurrently, i.e., as humans do, to scale these deep neural networks to real-world applications.
\end{enumerate}
\section{Validity threats}
\begin{enumerate}
    \item During training, the input images were resized to the resolution of  x , exactly half of the original resolution, which will impact the performance of the model. The less batch size of  might also affect the performance.
    \item The IkshanaNet- and IkshanaNet- were trained on a few training samples but evaluated on the same test set.  The performance of these two methods depends on how well the selected data samples represent the whole training set. So, an exact comparison of these methods to the main method IkshanaNet- may not be valid.
    \item The IkshanaNet architecture might have defects and the exact architecture may not be suitable for other computer vision tasks.
\end{enumerate}

\section{Conclusions and Future work}
\subsection{Conclusions}
In conclusion, we empirically showed that a network designed by following the Ikshana theory would have the ability to perform with a few data samples. We also presented some empirical evidence to show that our theory will help in reducing the number of parameters of the network. The main contribution of this work is the Ikshana theory and not the IkshanaNet. The IkshanaNet architecture is just an example to show how to implement our theory in practice. 
\subsection{Future Work}
Further investigation is required to design a new backbone architecture, by following the Ikshana theory, to learn representations from the images, which will contribute to every computer vision task. Along with that, additional investigation is required to address the following questions:
\begin{enumerate}
    \item To what extent does a network designed by following the Ikshana theory would generalize to other datasets? Is there any improvement compared to the current state-of-the-art methods? 
    \item To what extent does a network designed by following the Ikshana theory would be resistant to adversarial attacks compared to currently available networks?
    \item Some works\cite{oliva2000diagnostic}\cite{oliva2001modeling} in neuroscience suggests that spatial resolutions around eight cycles/image are sufficient for humans to understand the basic gist. In this work, we used three spatial resolutions of the image. What are the different spatial resolutions required by a neural network to capture the semantic information of the given image? Do these required spatial resolutions changes from one task to another task?
\end{enumerate}


\bibliographystyle{unsrt}  
\bibliography{references}  





\end{document}
