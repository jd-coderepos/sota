
\documentclass{article} \usepackage{iclr2024_coNFErence,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}
\newcommand{\smpara}[1]{\left({#1}\right)}
\newcommand{\midpara}[1]{\left[{#1}\right]}
\newcommand{\bigpara}[1]{\left\{{#1}\right\}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\iid}{\overset{\mathrm{iid}}{\sim}}

\usepackage{tikz}
\newcommand{\comp}[2]{\overset{#2}{\underset{{#1}}{\TikCircle}}}
\newcommand{\inner}[2]{\langle#1,#2\rangle}

\newcommand{\grad}[1]{{\nabla_{\bm{#1}}}}
\renewcommand{\div}[1]{{\textup{div}_{\bm{#1}}}}
\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak


\newenvironment{myproof}[2]{\paragraph{\textit{Proof of {#1} {#2}. }}}{\hfill}

\newcommand\TikCircle[1][2.5]{\tikz[baseline=-#1]{\draw[thick](0,0)circle[radius=#1mm];}}
\usepackage{xcolor}



\newcommand{\se}[1]{\textcolor{magenta}{[SE: #1]}}



\newcommand{\jcc}[1]{\textcolor{black}{[JC: #1]}}
\newcommand{\jc}[1]{{\color{black}{{#1}}}}
\def\eqref#1{(\ref{#1})}
\newcommand{\djk}[1]{{\color{black}{{#1}}}}
\def\eqref#1{(\ref{#1})}
%
 
\usepackage{svg}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor, color, colortbl}         \usepackage{tikz}
\usepackage[export]{adjustbox}
\usepackage{mathtools}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{pifont}\usepackage{xcolor}
\usepackage{calc}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage[normalem]{ulem}

\usepackage{mathtools}

\usepackage{subcaption}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{wrapfig}

\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{arydshln}
\usepackage{minitoc}



\newtheorem{theorem}{Theorem}\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\allowdisplaybreaks

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newsavebox{\leftbox}
\newsavebox{\rightbox}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\definecolor{Gray}{gray}{0.9}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\cc}[1]{\cellcolor{gray!#1}}
\hypersetup{colorlinks}

\makeatletter
\def\adl@drawiv#1#2#3{\hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}#2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{\noalign{\vskip\aboverulesep
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip\belowrulesep}}
\makeatother


\title{Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion}



\author{Dongjun Kim\thanks{Equal contribution}~~\thanks{Work done during an internship at SONY AI} ~\&~Chieh-Hsin Lai \\
Sony AI\\
Tokyo, Japan \\
\texttt{dongjoun57@kaist.ac.kr}, \texttt{chieh-hsin.lai@sony.com} \\
\And
Wei-Hsiang Liao~\&~Naoki Murata~\&~Yuhta Takida~\&~Toshimitsu Uesaka~\&~Yutong He \\
Sony AI \\
Tokyo, Japan \\
\AND
Yuki Mitsufuji \\
Sony AI, Sony Group Corporation \\
Tokyo, Japan \\
\AND
Stefano Ermon \\
Stanford University \\
CA, USA \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\iclrfinalcopy \begin{document}


\maketitle
\doparttoc
	\parttoc

\begin{abstract}
Consistency Models (CM)~\citep{song2023consistency}
 accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose \textbf{C}onsistency \textbf{T}rajectory \textbf{M}odel (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID ) and ImageNet at  resolution (FID ). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.
\end{abstract}



\section{Introduction}\label{sec:intro}


\begin{wrapfigure}{r}{0.5\textwidth}
	\vskip -0.4in
	\centering
		\centering		\includegraphics[width=\linewidth]{umbrella_ver2.pdf}
	\caption{A schematic illustration of CTM.}
	\label{fig:umbrella}
 \vskip -0.4in
\end{wrapfigure}

Deep generative models encounter distinct training and sampling challenges. Variational Autoencoder (VAE)~\citep{kingma2013auto} can be trained easily but
 may suffer from posterior collapse, resulting in blurry samples, while Generative Adversarial Network (GAN)~\citep{goodfellow2014generative} generates high-quality samples but faces training instability. Conversely, Diffusion Model (DM)~\citep{sohl2015deep,ho2020denoising, song2020score} addresses these issues by learning the score (i.e., gradient of log-density)~\citep{song2019generative}, which can generate high quality samples.
However, compared to VAE and GAN excelling at fast sampling,  DM involves a gradual denoising process that slows down sampling, requiring numerous model evaluations.

Score-based diffusion models synthesize data by solving the reverse-time (stochastic or deterministic) process corresponding to a prescribed forward process that adds noise to the data~\citep{song2019generative,song2020score}. Although advanced numerical  solvers~\citep{lu2022dpm, zhang2022fast} of Stochastic Differential Equations (SDE) or Ordinary Differential Equations (ODE) substantially reduce the required Number of Function Evaluations (NFE), 
further improvements are challenging 
due to the intrinsic discretization error present in all solvers~\citep{de2021diffusion}. Recent developments in sample efficiency thus focus on directly  estimation of the integral along the sample trajectory, amortizing the computational cost of numerical solvers.
\emph{Distillation models}~\citep{salimans2021progressive} in Figure~\ref{fig:umbrella}, exemplified by the Consistency Model (CM)~\citep{song2023consistency}, presents a promising approach for estimating the integration with a single NFE (Figure~\ref{fig:i_am_fig_1}). 
However, their generation quality does not improve as NFE increase, and there is no straightforward mechanism for balancing computational resources (NFE) with quality.

\begin{figure}[t]
 \vskip -0.05in
    \centering
    \includegraphics[width=0.85\textwidth]{fig_1_ver2.pdf}
    \caption{Training and sampling comparisons of score-based and distillation models with CTM. Score-based models exhibit discretization errors during SDE/ODE solving, while distillation models can accumulate errors in multistep sampling. CTM mitigates these issues with -sampling ().}
    \label{fig:i_am_fig_1}
    \vskip -0.1in
\end{figure}

\begin{wrapfigure}{r}{0.48\textwidth}
	\vskip -0.2in
	\centering
		\centering		\includegraphics[width=\linewidth]{FID_oneshot_v2.pdf}
        \vskip -0.1in
 \caption{SOTA on CIFAR-10. Closeness to the origin indicates better performance.}
	\label{fig:cifar_}
 \vskip -0.1in
\end{wrapfigure}
This paper introduces the \emph{\textbf{C}onsistency \textbf{T}rajectory \textbf{M}odel} (CTM) as a unified framework simultaneously assessing both the integrand (score function) and the integral (sample) of the Probability Flow (PF) ODE, thus bridging score-based and distillation models (Figure \ref{fig:umbrella}). CTM estimates both infinitesimal steps (score function) and long steps (integral over any time horizon) of the PF ODE from any initial condition, providing increased  flexibility at inference time. Its score evaluation capability accommodates a range of score-based sampling algorithms based on  solving differential equations~\citep{song2020score}, expanding its applicability across various domains~\citep{saharia2022palette}. In particular, CTM enables exact likelihood computation, setting it apart from previous distillation models. 
Additionally, its integral approximation capability facilitates the incorporation of distillation sampling methods~\citep{salimans2021progressive,song2023consistency} that involve long ``jumps'' along the solution trajectory. This unique feature enables a novel sampling method called \emph{-sampling,} which alternates forward and backward jumps along the solution trajectory, with  governing the level of stochasticity.


CTM's dual modeling capability for both infinitesimal and long steps of the PF ODE greatly enhances its training flexibility as well. It allows concurrent training with reconstruction loss, denoising diffusion loss, and adversarial loss within a unified framework. Notably, by incorporating CTM's training approach with -sampling, we achieve the new State-Of-The-Art (SOTA) performance in both density estimation and image generation for CIFAR-10~\citep{krizhevsky2009learning} (Figure~\ref{fig:cifar_}) and ImageNet~\citep{russakovsky2015imagenet} at a resolution of  (Table~\ref{tab:ImageNet64_baseline}).


\section{Preliminary}\label{sec:preliminary}

In DM~\citep{sohl2015deep,song2020score}, the encoder structure is formulated using a set of continuous-time random variables defined by a fixed forward diffusion process\footnote{This paper can be extended to VPSDE encoding~\citep{song2020score} with re-scaling~\citep{kim2022refining}.}, 

initialized by the data variable, . A reverse-time process~\citep{anderson1982reverse} from  to  is established 
    ,
where  is the standard Wiener process in reverse-time, and  is the marginal density of  following the forward process. The solution of this reverse-time process aligns with that of the forward-time process marginally (in distribution) when the reverse-time process is  initialized with . The deterministic counterpart of the reverse-time process, called the PF ODE~\cite{song2020score}, is given by 
where  is the probability distribution of the solution of the reverse-time stochastic process from time  to zero, initiated from . Here,  is the denoiser function~\citep{efron2011tweedie}, 
an alternative expression for the score function . For notational simplicity, we omit , a subscript in the expectation of the denoiser, throughout the paper. 

In practice, the denoiser  is approximated using a neural network , obtained by minimizing the Denoising Score Matching (DSM)~\citep{vincent2011connection, song2020score} loss 
, where  is the transition probability from time  to , initiated with . With the approximated denoiser, the empirical PF ODE is given by

Sampling from DM involves solving the PF ODE, equivalent to computing the integral

where  is sampled from a prior distribution  approximating . 
Decoding strategies of DM primarily fall into two categories: \emph{score-based sampling} with time-discretized numerical integral solvers, and \emph{distillation sampling} where a neural network directly estimates the integral.


\paragraph{Score-based Sampling}
Any off-the-shelf ODE solver, denoted as  (with an initial value of  at time  and ending at time ), can be directly applied to solve Eq.~\eqref{eq:int_target}~\citep{song2020score}. For instance, DDIM~\citep{song2020denoising} corresponds to a 1st-order Euler solver, while EDM~\citep{karras2022elucidating} introduces a 2nd-order Heun solver. Despite recent advancements in numerical solvers~\citep{lu2022dpm, zhang2022fast}, further improvements may be challenging due to the inherent discretization error present in all solvers~\citep{de2021diffusion}, ultimately limiting the sample quality obtained with few NFEs.

\paragraph{Distillation Sampling}
Distillation models~\citep{salimans2021progressive,meng2023distillation} successfully amortize the sampling cost by directly estimating the integral of Eq.~\eqref{eq:int_target} with a single neural network evaluation. However, their multistep sampling approach~\citep{song2023consistency} exhibits degrading sample quality with increasing NFE, lacking a clear trade-off between computational budget (NFE) and sample fidelity. Furthermore, multistep sampling is not deterministic, leading to uncontrollable sample variance.
We refer to Appendix~\ref{sec:related_work} for a thorough literature review.

\section{CTM: An Unification of Score-based and Distillation Models}
To address the challenges in both score-based and distillation samplings, we introduce the Consistency Trajectory Model (CTM), which seamlessly integrates both decoding strategies. Consequently, our model is versatile and can perform sampling through either SDE/ODE solving or direct prediction of intermediate points along the PF ODE trajectory.


\subsection{Decoder Parametrization of Consistency Trajectory Models}\label{sec:motivation}

CTM predicts both infinitesimal changes and intermediate points of the PF ODE trajectory. Specifically, we define  as the solution of the PF ODE from initial time  with an initial condition  to final time :

 can access any intermediate point along the trajectory by varying final time . However, with the current expression of , the infinitesimal change needed to recover the denoiser information (the integrand) can only be obtained by evaluating the -derivative at time , . Therefore, we introduce a dedicated expression for  using an auxiliary function  to enable easy access to both the integral via  and the integrand via  with Lemma~\ref{th:unification}. 

\begin{lemma}[Unification of score-based and distillation models]\label{th:unification} Suppose that the score satisfies . 
    The solution, , defined in Eq.~\eqref{eq:oracle_sol} can be expressed as:
    
    Here,  satisfies:
    \begin{itemize}
        \item When ,  is the solution of PF ODE at , initialized at . 
        \item As , . Hence,  can be defined at  by its limit: . \end{itemize}
\end{lemma}

\begin{wrapfigure}{r}{0.3\textwidth}
	\vskip -0.12in
	\centering
	\includegraphics[width=\linewidth]{s_t_plot_ver2.pdf}
	\caption{Learning objectives of Score-based ( line), distillation ( line), and CTM (upper triangle).}
	\label{fig:s_t_plot}
  \vskip -0.5in
\end{wrapfigure}
Indeed, the 's expression in Lemma~\ref{th:unification} is naturally linked to the Taylor approximation to the integral:

for any . 
Here, it is evident that  includes all residual terms in Taylor expansion, which turns to be the discretization error in sampling. The goal of CTM is to approximate this -function using a neural network  and estimate the solution trajectory with the parametrization inspired by Lemma~\ref{th:unification} as follows:

We remark that this parametrization satisfies the initial condition  for free, leading to improved training stability\footnote{Ensuring the initial condition's satisfaction is crucial for stable training. Directly estimating  with a network leads to rapid divergence, causing instability as the network output may deviate arbitrarily from the initial condition.}. Appendix~\ref{sec:parametrization} offers further insights into this parametrization.


\subsection{CTM Training}


To achieve trajectory learning, CTM should match the model prediction to the ground truth  by

for any . We opt to approximate  by solving the empirical PF ODE with a pre-trained score model . Our neural network is then trained to align with \djk{the reconstruction}:


In a scenario with no ODE discretization and  no score approximation errors,  perfectly reconstructs the PF ODE trajectory, and comparing the prediction and reconstruction in Eq.~\eqref{eq:global} leads  at optimal , given sufficient network flexibility. The same conclusion holds by matching the local consistency:

where  is stop-gradient. With the initial condition () satisfied, matching Eq.~\eqref{eq:local} avoids collapsing to the trivial solution (Proposition~\ref{th:ptw_traj_distill} in Appendix~\ref{sec:convergence_analysis}).

\begin{wrapfigure}{r}{0.33\textwidth}
 \vskip -0.2in
	\centering
		\includegraphics[width=\linewidth]{ctm_model_ver2.pdf}
	\caption{\djk{An illustration of CTM prediction and target at time  with an initial value .}}
	\label{fig:ill_models}
  \vskip -0.3in
\end{wrapfigure}
To estimate the entire solution trajectory with higher precision, we introduce \emph{soft matching}, illustrated in Figure~\ref{fig:ill_models}, ensuring consistency between prediction from  and the prediction from  for any :

This soft matching spans two frameworks:
\begin{itemize}
\item As , Eq.~\eqref{eq:global} enforces \emph{global consistency matching}, i.e., a reconstruction loss.
\item As , Eq.~\eqref{eq:local} is \textit{local consistency matching}. Additionally, if , it recovers CM's distillation loss.
\end{itemize}

To quantify the dissimilarity between  and  and enforce Eq.~\eqref{eq:soft}, we could use either  distance in pixel space or in feature space. However, the pixel distance may overemphasize the distance at large  due to the diffusion scale, requiring time-weighting adjustments. Furthermore, feature distance requires a time-conditional feature extractor, which can be expensive to train. Hence, we propose to use a feature distance  in clean data space by comparing

CTM loss is defined as

which leads the model's prediction, at optimum, to match with the empirical PF ODE's solution trajectory, defined by the pre-trained DM (teacher), see Appendix~\ref{appendix:general_theory} (Propositions \ref{th:ptw_distill} and \ref{th:density_match}) for details.

\subsection{Training Consistency Trajectory Models}

Training CTM with Eq.~\eqref{eq:ctm_loss} may empirically lead inaccurate estimation of  when  approaches . This is due to the learning signal of  being scaled with  by Lemma~\ref{th:unification}, and this scale decreasing to zero as  approaches . Consequently, although our parametrization enables the estimation of both the trajectory and its slope, the accuracy of slope (score) estimation may be degraded. To mitigate this problem, we use Lemma~\ref{th:unification}'s conclusion that
 when  and train  with the DSM loss\footnote{We opt for the conventional DSM loss instead of minimizing   with teacher score supervision, as the DSM loss's optimum can recover the true denoiser , whereas teacher score supervision can only reach  as its optimum.}

Empirically, regularizing  with  improves score accuracy, which is especially important in large NFE sampling regimes.


\begin{wrapfigure}{r}{0.48\textwidth}
\vskip -0.12in
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{CTM Training}\label{alg:CTM}
    \begin{algorithmic}[1]
        \Repeat
        \State Sample  from data distribution
        \State Sample 
        \State Sample , , 
        \State Calculate 
        \State Calculate 
        \State Update 
        \State Update 
        \Until {converged}
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}
On the other hand, CTM, distilling from the teacher model, is constrained by the teacher's  performance. This challenge can be mitigated with adversarial training to improve trajectory estimation. The one-step generation of CTM enables us to calculate the adversarial loss efficiently, in the similar way of conventional GAN training: 

where  is a discriminator. This adversarial training allows \emph{the student model (CTM) to beat the teacher model (DM)}. To summarize, CTM allows the integration of  reconstruction-based CTM loss, diffusion loss, and adversarial loss

in a single training framework, by optimizing . Here,  and  are the weighting functions, see Algorithm~\ref{alg:CTM}.

\section{Sampling with  CTM}\label{sec:gamma-sample}


CTM enables score evaluation through , supporting standard score-based sampling with ODE/SDE solvers. In high-dimensional image synthesis, as shown in Figure~\ref{fig:sample_comparison}'s left two columns, CTM performs comparably to EDM using Heun's method as a PF ODE solver.


\begin{wrapfigure}{r}{0.63\textwidth}
	\vskip -0.26in
	\centering
	\includegraphics[width=\linewidth]{Cat_model_comparison_v6.pdf}
 \vskip -0.05in
	\caption{Comparison of score-based models (EDM), distillation models (CM), and CTM with various sampling methods and NFE trained on AFHQ-cat~\citep{choi2020stargan} .}
	\vskip -0.2in
	\label{fig:sample_comparison}
\end{wrapfigure}
CTM additionally enables time traversal along the solution trajectory, allowing for the newly introduced \emph{-sampling} method, refer to Algorithm~\ref{alg:gamma}  and Figure~\ref{fig:gamma_sampling}. Suppose the sampling timesteps are . With , where  is the prior distribution, -sampling denoises  to time  with , and perturb this denoised sample with forward diffusion to the noise level at time . It iterates this back-and-forth traversal until reaching to time .

Our -sampling is a new distillation sampler that unifies previously proposed sampling techniques, including distillation sampling and score-based sampling.


\begin{figure}[t]
	\centering
	\begin{subfigure}{0.31\linewidth}
		\centering
		\includegraphics[width=\linewidth]{gamma_1.pdf}
		\subcaption{ (Fully stochastic)}
	\end{subfigure}
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{gamma_mid.pdf}
		\subcaption{}
	\end{subfigure}	
 	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{gamma_0.pdf}
		\subcaption{ (Deterministic)}
	\end{subfigure}
	\caption{Illustration of -sampling with varying  value. It denoises with the network evaluation and iteratively diffuses the sample in reverse by .}
    \label{fig:gamma_sampling}
\end{figure}

\begin{itemize}
\item Figure~\ref{fig:gamma_sampling}-(a): When , it coincides to the multistep sampling introduced in CM, which is fully stochastic and results in semantic variation when NFE changes, e.g., compare samples of NFE 4 and 40 with the deterministic sample of NFE 1 in the third column of Figure~\ref{fig:sample_comparison}. With the fixed , CTM reproduces CM's samples in the fourth column of Figure~\ref{fig:sample_comparison}.

\item Figure~\ref{fig:gamma_sampling}-(c): When , it becomes the deterministic distillation sampling that estimates the solution of the PF ODE. A key distinction between the -sampling and score-based sampling is that CTM avoids sampling errors by directly estimating Eq.~\eqref{eq:oracle_sol}. \jc{However, score-based samplers like DDIM (1st-order Euler solver) or EDM (2nd-order Heun solver) are susceptible to discretization errors from Taylor approximation, especially with small NFE.} (the leftmost column of Figure~\ref{fig:sample_comparison}). Deterministic nature as  ensures the sample semantic preserved across NFE changes, visualized in the rightmost column of Figure~\ref{fig:sample_comparison}.

\item Figure~\ref{fig:gamma_sampling}-(b): When , it generalizes the EDM's stochastic sampler (Algorithm~\ref{alg:EDM}). Appendix~\ref{sec:var_bound} shows that -sampling's sample variances scale proportionally with .
\end{itemize}


The optimal choice of  depends on practical usage and empirical configuration~\citep{karras2022elucidating,xu2023restart}. Figure~\ref{fig:cat_stroke} demonstrates -sampling in stroke-based generation~\citep{meng2021sdedit}, revealing that the sampler with  leads to significant semantic deviations from the reference stroke, while smaller  values yield closer semantic alignment and maintain high fidelity. In contrast, Figure~\ref{fig:sampling_cifar10} showcases 's impact on generation performance. In Figure~\ref{fig:sampling_cifar10}-(a),  has less influence with small NFE, but the setup with  is the only one that resembles the performance of the Heun's solver as NFE increases. Additionally, CM's multistep sampler () significantly degrades sample quality as NFE increases. This quality deterioration concerning  becomes more pronounced with higher NFEs, shown in Figure~\ref{fig:sampling_cifar10}-(b), potentially attributed to error accumulation during the iterative long ``jumps'' for \djk{denoising}. We explain this phenomenon using a 2-step -sampling example in the following theorem, see Theorem~\ref{th:sampling_agg_error_N_steps} for a generalized result for -steps.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{cat_stroke.pdf}
	\caption{ controls sample variance in stroke-based generation (see Appendix~\ref{subsec:traj_control}).}
	\label{fig:cat_stroke}
\end{figure*}

\begin{wrapfigure}{r}{0.6\textwidth}
\vskip -0.1in
	\centering
	\begin{subfigure}{0.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_by_nfe.pdf}
		\subcaption{FID by NFE}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_by_gamma.pdf}
		\subcaption{Sensitivity to }
	\end{subfigure}	
	\caption{(a) CTM enables score-based sampling and distillation -sampling on CIFAR-10. (b) The FID degrade highlights the importance of trajectory learning.}
	\label{fig:sampling_cifar10}
	\vskip -0.2in
\end{wrapfigure}
\begin{theorem}[(Informal) 2-steps -sampling]\label{th:sampling_agg_error_2_steps}
Let  and . Denote  as the density obtained from the -sampler with the optimal CTM, following the transition sequence , starting from . Then
.
\end{theorem}
When it becomes -steps, -sampling iteratively conducts long jumps from  to  for each step , which aggregates the error to be . \jc{In contrast, such time overlap between jumps does not occur in -sampling}, eliminating the error accumulation, resulting in  error, see Appendix~\ref{sec:connect_sde}. In summary, CTM addresses challenges associated with large NFE in distillation models with  and removes the discretization error in score-based models. 


\section{Experiments}

\begin{table}[t]
\begin{minipage}[t]{.56\linewidth}
	\scriptsize
 \caption{Performance comparisons on CIFAR-10.}
 \resizebox{.95\linewidth}{!}{
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\multirow{2}{*}{Model} & \multirow{2}{*}{NFE} & \multicolumn{2}{c}{Unconditional} & Conditional \\\cmidrule(lr){3-4}\cmidrule{5-5}
  & & FID & NLL & FID \\\midrule
            \multicolumn{5}{l}{\textbf{GAN Models}}\\
            BigGAN~\citep{brock2018large} & 1 & 8.51 & \xmark & - \\
            StyleGAN-Ada~\citep{karras2020training} & 1 & 2.92 & \xmark & 2.42 \\
            StyleGAN-D2D~\citep{kang2021rebooting} & 1 & - & \xmark & 2.26 \\
            StyleGAN-XL~\citep{sauer2022stylegan} & 1 & - & \xmark & 1.85 \\
            \midrule
            \multicolumn{5}{l}{\textbf{Diffusion Models -- Score-based Sampling}}\\
            DDPM~\citep{ho2020denoising} & 1000 & 3.17 & 3.75 & - \\
            \multirow{2}{*}{DDIM~\citep{song2020denoising}} & 100 & 4.16 & - & - \\
            & 10 & 13.36 & - & - \\
            Score SDE~\citep{song2020denoising} & 2000 & 2.20 & 3.45 & - \\
            VDM~\citep{kingma2021variational} & 1000 & 7.41 & \underline{2.49} & - \\
            LSGM~\citep{vahdat2021score} & 138 & 2.10 & 3.43 & - \\
            EDM~\citep{karras2022elucidating} & 35 & 2.01 & 2.56 & 1.82 \\\midrule
            \multicolumn{5}{l}{\textbf{Diffusion Models -- Distillation Sampling}}\\KD~\citep{luhman2021knowledge} & 1 & 9.36 & \xmark & - \\
            DFNO~\citep{zheng2023fast} & 1 & 5.92 & \xmark & - \\
            Rectified Flow~\citep{liu2022flow} & 1 & 4.85 & \xmark & - \\
            PD~\citep{salimans2021progressive} & 1 & 9.12 & \xmark & - \\
            CD (official report)~\citep{song2023consistency} & 1 & 3.55 & \xmark & - \\CD (retrained) & 1 & 10.53 & \xmark & - \\
            CD + GAN~\citep{lu2023cm} & 1 & 2.65 & \xmark & - \\
            \cc{15}CTM~(ours) & \cc{15}1 & \cc{15}\underline{1.98} & \cc{15}\textbf{2.43} & \cc{15}\underline{1.73} \\\cdashlinelr{1-5}
            PD~\citep{salimans2021progressive} & 2 & 4.51 & - & - \\
            CD~\citep{song2023consistency} & 2 & 2.93 & - & - \\
            \cc{15}CTM~(ours) & \cc{15}2 & \cc{15}\textbf{1.87} & \cc{15}\textbf{2.43} & \cc{15}\textbf{1.63} \\
            \bottomrule
	\end{tabular}
 \label{tab:cifar10_baseline}
 }
    \end{minipage}\hfill
    \begin{minipage}[t]{.44\linewidth}
	\caption{Performance comparisons on ImageNet .}
	\label{tab:ImageNet64_baseline}
	\scriptsize
 \resizebox{.95\linewidth}{!}{
	\centering
	\begin{tabular}{lccc}
		\toprule
		Model & NFE & FID & IS \\\midrule
            ADM~\citep{dhariwal2021diffusion} & 250 & 2.07 & - \\
            EDM~\citep{karras2022elucidating} & 79 & 2.44 & 48.88 \\
            BigGAN-deep~\citep{brock2018large} & 1 & 4.06 & - \\
            StyleGAN-XL~\citep{sauer2022stylegan} & 1 & 2.09 & \textbf{82.35} \\\midrule
            \multicolumn{4}{l}{\textbf{Diffusion Models -- Distillation Sampling}}\\PD~\citep{salimans2021progressive} & 1 & 15.39 & - \\
            BOOT~\citep{gu2023boot} & 1 & 16.3 & -  \\
            CD~\citep{song2023consistency} & 1 & 6.20 & 40.08 \\
            \cc{15}CTM (ours) & \cc{15}1 & \cc{15}\underline{2.06} & \cc{15}\underline{69.83} \\\cdashlinelr{1-4}
            PD~\citep{salimans2021progressive} & 2 & 8.95 & - \\
            CD~\citep{song2023consistency} & 2 & 4.70 & - \\
            \cc{15}CTM (ours) & \cc{15}2 & \cc{15}\textbf{1.90} & \cc{15}63.90 \\
\bottomrule
	\end{tabular}}
 \centering
 \includegraphics[width=\linewidth]{fid_is_curve.pdf}
 \vskip -0.15in
 \captionof{figure}{FID-IS curve on ImageNet.}\label{fig:fid_is}
\end{minipage} 
\vskip 0.15in
\begin{minipage}[t]{.99\linewidth}
     \centering
\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{edm_NFE_79_v3.png}
		\subcaption{EDM ( NFE)}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{ctm_NFE_1_v3.png}
		\subcaption{CTM w/o GAN ( NFE)}
	\end{subfigure}	
 	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{ctm_gan_NFE_1_v3.png}
		\subcaption{CTM w/ GAN ( NFE)}
	\end{subfigure}
 \vskip -0.05in
 \captionof{figure}{Samples generated by (a) EDM, (b) CTM without GAN (), and (c) CTM with GAN (). CTM distills knowledge from EDM (teacher) and employs adversarial training for further refining fine-grained details. More generated samples are demonstrated in Appendix~\ref{sec:generated_samples}.}
	\label{fig:sampling_imagenet}
 \end{minipage}
 \vskip 0.1in
 \begin{minipage}[t]{.99\linewidth}
     \centering
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{avg_NFE_1.png}
		\subcaption{Without classifier-rejection sampling (NFE 1)}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{avg_NFE_2.png}
		\subcaption{With classifier-rejection sampling (avg. NFE 2)}
	\end{subfigure}	
 \vskip -0.05in	
  \captionof{figure}{Random samples (Siberian Husky) (d) with and (e) without classifier-free sampling.}
	\label{fig:classifier_rejection}
\end{minipage}
\vskip -0.1in
\end{table}

\subsection{Student (CTM) beats teacher (DM) -- Quantitative Analysis}

We evaluate CTM on CIFAR-10 and ImageNet , using the pre-trained diffusion checkpoints from EDM for CIFAR-10 and CM for ImageNet as the teacher models. We adopt EDM's training configuration for  and employ StyleGAN-XL's~\citep{sauer2022stylegan} discriminator for . During training, we employ adaptive weights  and , inspired by VQGAN~\citep{esser2021taming} to balance DSM and GAN losses with the CTM loss. For both datasets, we utilize the DDPM architecture. For CIFAR-10, we take EDM's implementation; and for ImageNet, CM's implementation is used.
On top of these architectures, we incorporate -information via auxiliary temporal embedding with positional embedding~\citep{vaswani2017attention}, and add this embedding to the -embedding. This training setup (Appendix \ref{sec:implementation}), along with the deterministic sampling (), allows CTM's generation to outperform teacher models with NFE  and achieve SOTA FIDs with NFE . 

\textbf{CIFAR-10 } CTM's NFE  generation excels both EDM and StyleGAN-XL with FID of  on conditional CIFAR-10, and CTM achieves the SOTA FID of  with  NFEs, surpassing all generative models. These results are obtained with the implementation based on the official PyTorch code of CM. However, retraining CM with this official PyTorch code yields FID of  (unconditional), higher than the reported FID of . Additionally, CTM's ability to approximate scores using  enables evaluating Negative Log-Likelihood (NLL)~\citep{song2021maximum,kim2022maximum}, establishing a new SOTA NLL. This improvement can be attributed, in part, to CTM's reconstruction loss when , and improved alignment with the oracle process~\citep{lai2023fp}.

\textbf{ImageNet } CTM's generation surpasses both teacher EDM and StyleGAN-XL with NFE 1, outperforming previous models with no guidance~\citep{dhariwal2021diffusion}, see Figure~\ref{fig:sampling_imagenet} for the comparison of CTM with the teacher model. Notably, all results in Tables~\ref{tab:cifar10_baseline} and \ref{tab:ImageNet64_baseline} are achieved within K-K training iterations, requiring only  of the iterations compared to CM and EDM. 



\textbf{Classifier-Rejection Sampling } CTM's fast sampling enables classifier-rejection sampling. In the evaluation, for each class, we select the top 50 samples out of  samples based on predicted class probability, where  is the rejection ratio. This sampler, combined with NFE  sampling, consumes an average of NFE . In Figure~\ref{fig:fid_is}, CTM, employing cost-effective classifier-rejection sampling, shows a FID-IS trade-off comparable to classifier-guided results~\citep{ho2021classifier} achieved with high NFEs of 250. Additionally, Figure~\ref{fig:classifier_rejection} confirms that samples rejected by the classifier exhibit superior quality and maintain class consistency, in agreement with the findings of \citet{ho2021classifier}. We employ the classifier at resolution of  provided by \citet{dhariwal2021diffusion}.

\subsection{Qualitative Analysis}

\begin{wrapfigure}{r}{0.6\textwidth}
\vskip-0.36in
\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_by_loss_NFE_1_final.pdf}
		\subcaption{NFE }
	\end{subfigure}	
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_by_loss_NFE_18_final.pdf}
		\subcaption{NFE }
	\end{subfigure}	
 \vskip -0.05in
 \caption{Comparison of local, global, and the proposed soft consistency matching.}
	\label{fig:CTM}
 \vskip-0.1in
\end{wrapfigure}




\textbf{CTM Loss} Figure~\ref{fig:CTM} highlights the advantages of employing the proposed soft consistency matching in Eq.~\eqref{eq:soft} during CTM training. It outperforms the local consistency matching (Eq.~\eqref{eq:local}). Additionally, it demonstrates comparable performance to the global consistency matching (Eq.~\ref{eq:global}) with NFE , superior performance with large NFE. Furthermore, soft matching is computationally efficient, enhancing the scalability of CTM.
\begin{wrapfigure}{r}{0.6\textwidth}
\centering
 \vskip -0.35in
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_by_DSM_NFE_1_final.pdf}
		\subcaption{NFE }
	\end{subfigure}	
        \begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_by_DSM_NFE_18_final.pdf}
		\subcaption{NFE }
	\end{subfigure}	
        \vskip -0.05in
	\caption{The effect of DSM loss.}
	\label{fig:DSM}
 \vskip -0.25in
\end{wrapfigure}


 
\textbf{DSM Loss } Figure~\ref{fig:DSM} illustrates two benefits of incorporating  with . It preserves sample quality for small NFE unless DSM scale outweighs CTM. For large NFE sampling, it significantly improves sample quality due to accurate score estimation. Throughout the paper, we maintain  based on insights from Figure~\ref{fig:DSM}, unless otherwise specified.


\begin{wrapfigure}{r}{0.6\textwidth}
\vskip -0.2in
\centering
\begin{subfigure}{0.48\linewidth}
\centering
		\includegraphics[width=\linewidth]{FID_ctm_dsm_gan_NFE_1_final.pdf}
		\subcaption{NFE }
	\end{subfigure}	
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{FID_ctm_dsm_gan_NFE_18_final.pdf}
		\subcaption{NFE }
	\end{subfigure}	
\vskip -0.05in
 \caption{The effect of GAN loss.}
	\label{fig:GAN}
  \vskip -0.2in
 \end{wrapfigure}
\textbf{GAN Loss } Analogous to the DSM loss, Figure~\ref{fig:GAN} illustrates the advantages of incorporating the GAN loss for both small and large NFE sample quality. Figure~\ref{fig:sampling_imagenet} demonstrates that CTM can produce samples resembling those of EDM (teacher), with GAN refining local details. Throughout the paper, we adopt the warm-up strategy for GAN training: deactivate GAN training with  for warm-up iterations and then activate GAN training with , in line with the recommendation from VQGAN~\citep{esser2021taming}. This warm-up strategy is applied by default unless otherwise specified.



\textbf{Training Without Pre-trained DM } Leveraging our score learning capability, we replace the pre-trained score approximation, , with CTM's approximation, , allowing us to obtain the corresponding empirical PF ODE . Consequently, we can construct a pretrained-free target, , to replace  in computing the CTM loss . In contrast to  (Eq.~\eqref{eq:target}),  eliminates the need for pre-trained DMs. When incorporated with DSM and GAN losses, it achieves a NFE  FID of  on unconditional CIFAR-10, a performance on par with pre-trained DMs. We highlight that the CTM loss can be used with or without the pre-trained DM, which is contrastive to CM with the Consistency Training loss for pretrained-free training that uses an ad-hoc noise injection technique.



\section{Conclusion}

CTM, a novel generative model, addresses issues in established models. With a unique training approach accessing intermediate PF ODE solutions, it enables unrestricted time traversal and seamless integration with prior models' training advantages. A universal framework for Consistency and Diffusion Models, CTM excels in both training and sampling. Remarkably, it surpasses its teacher model, achieving SOTA results in FID and likelihood for few-steps diffusion model sampling on CIFAR-10 and ImageNet , highlighting its versatility and process.



\section*{Acknowledgement}
We sincerely acknowledge the support of everyone who made this research possible. Our heartfelt thanks go to Koichi Saito, Woosung Choi, Kin Wai Cheuk, and Yukara Ikemiya for their assistance. 


\clearpage
\newpage









\bibliography{dgm}
\bibliographystyle{iclr2024_coNFErence}
\clearpage
\newpage
\appendix

\tableofcontents
	\newpage
	\parttoc

\section{Related Works}\label{sec:related_work}
\paragraph{Diffusion Models} DMs excel in high-fidelity synthetic image and audio generation~\citep{dhariwal2021diffusion,saharia2022photorealistic,rombach2022high}, as well as in applications like media editing, restoration~\citep{meng2021sdedit,cheuk2023diffroll,kawar2022denoising,saito2023unsupervised,hernandez2023vrdmg,murata2023gibbsddrm}. Recent research aims to enhance DMs in sample quality~\citep{kim2022maximum,kim2022refining}, density estimation~\citep{song2021maximum,lu2022maximum}, and especially, sampling speed~\citep{song2020denoising}. 


\paragraph{Fast Sampling of DMs} The SDE framework underlying DMs~\citep{song2020score} has driven research into various numerical methods for accelerating DM sampling, exemplified by works such as \citep{song2020denoising,zhang2022fast,lu2022dpm}. Notably, \citep{lu2022dpm} reduced the ODE solver steps to as few as -. Other approaches involve learning the solution operator of ODEs~\citep{zheng2023fast}, discovering optimal transport paths for sampling~\citep{liu2022flow}, or employing distillation techniques~\citep{luhman2021knowledge,salimans2021progressive,berthelot2023tract,shao2023catch}. However, previous distillation models may experience slow convergence or extended runtime.
\citet{gu2023boot} introduced a bootstrapping approach for data-free distillation. Furthermore, \citet{song2023consistency} introduced CM which extracts DMs' PF ODE to establish a direct mapping from noise to clean predictions, achieving one-step sampling while maintaining good sample quality. CM has been adapted to enhance the training stability of GANs, as \citep{lu2023cm}. However, it's important to note that their focus does not revolve around achieving sampling acceleration for DMs, nor are the results restricted to simple datasets.


\paragraph{Consistency of DMs} Score-based generative models rely on a differential equation framework, employing neural networks trained on data to model the conversion between data and noise. These networks must satisfy specific consistency requirements due to the mathematical nature of the underlying equation. Early investigations, such as \citep{kim2022soft}, identified discrepancies between learned scores and ground truth scores. Recent developments have introduced various consistency concepts, showing their ability to enhance sample quality~\citep{daras2023consistent,li2023diffusion}, accelerate sampling speed~\citep{song2023consistency}, and improve density estimation in diffusion modeling~\citep{lai2023fp}. Notably, \citet{lai2023equivalence} established the theoretical equivalence of these consistency concepts, suggesting the potential for a unified framework that can empirically leverage their advantages. CTM can be viewed as the first framework which achieves all the desired properties.


\section{Theoretical Insights on CTM}\label{appendix:general_theory}
In this section, we explore several theoretical aspects of CTM, encompassing convergence analysis (Section~\ref{sec:convergence_analysis}), properties of well-trained CTM, variance bounds for -sampling, and a more general form of accumulated errors induced by -sampling (cf. Theorem~\ref{th:sampling_agg_error_2_steps}).

We first introduce and review some notions. Starting at time  with an initial value of  and ending at time , recall that  represents the true solution of the PF ODE, and  is the solution function of the following empirical PF ODE.

Here  denotes the teacher model's weights learned from DSM. Thus,  can be expressed as

where .



\subsection{Convergence Analysis -- Distillation from Teacher Models}\label{sec:convergence_analysis}

\paragraph{Convergence along Trajectory in a Time Discretization Setup.}

CTM's practical implementation follows CM's one, utilizing discrete timesteps  for training. Initially, we assume local consistency matching for simplicity, but this can be extended to soft matching. This transforms the CTM loss in Eq.~\eqref{eq:ctm_loss} to the discrete time counterpart:

where  is a metric, and




In the following theorem, we demonstrate that irrespective of the initial time  and end time , CTM , will eventually converge to its teacher model, .



\begin{proposition}\label{th:ptw_distill} Define .  Assume
that  is uniform Lipschitz in  and that the ODE solver admits local truncation error bounded uniformly by  with . If there is a  so that , then for any  and   


\end{proposition}


Similar argument applies, confirming convergence along the PF ODE trajectory, ensuring Eq.~\eqref{eq:local} with  replacing :

by enforcing the following loss

where





\begin{proposition}\label{th:ptw_traj_distill}  If there is a  so that , then for any  and   


\end{proposition}



\paragraph{Convergence of Densities.} In Proposition~\ref{th:ptw_distill}, we demonstrated point-wise trajectory convergence, from which we infer that CTM may converge to its training target in terms of density. More precisely, in Proposition~\ref{th:density_match}, we establish that if CTM's target  is derived from the teacher model (as defined above), then the data density induced by CTM will converge to that of the teacher model. Specifically, if the target  perfectly approximates the true -function: 

Then the data density generated by CTM will ultimately learn the data distribution .

Simplifying, we use the  for the distance metric  and consider the prior distribution  to be , which is the marginal distribution at time  defined by the diffusion process in Eq.~\eqref{eq:sde_forward}.



\begin{proposition}\label{th:density_match}  Suppose that 
\begin{enumerate}[(i)]
        \item 
    The uniform Lipschitzness of  (and ),
    
    \item The uniform boundedness in  of : there is a  so that
    
\end{enumerate}
If for any , there is a  such that . Let  denote the pushforward distribution of  induced by . Then, as , . Particularly, if the condition in Eq.~\eqref{eq:tar_perfect_match} is satisfied, then  as .
\end{proposition}


\subsection{Non-Intersecting Trajectory of the Optimal CTM}\label{sec:non_intersect}
CTM learns distinct trajectories originating from various initial points  and times . In the following proposition, we demonstrate that the distinct trajectories derived by the optimal CTM, which effectively distills information from its teacher model ( for any ), do not intersect.


\begin{proposition}\label{th:gt_injective}
Suppose that a well-trained  such that  for any , and that  is Lipschitz, i.e., there is a constant 
so that for any  and 


Then for any , the mapping  is bi-Lipschitz. Namely, for any 
    
    This implies that ,  for all . 
\end{proposition}

Specifically, the mapping from an initial value to its corresponding solution trajectory, denoted as , is injective. Conceptually, this ensures that if we use guidance at intermediate times to shift a point to another guided-target trajectory, the guidance will continue to affect the outcome at .

\subsection{Variance Bounds of -sampling}\label{sec:var_bound}

Suppose the sampling timesteps are . In Proposition~\ref{th:gamma_sampling}, we analyze the variance of 

 resulting from -step -sampling, initiated at 

Here, we assume an optimal CTM which precisely distills information from the teacher model  for all , for simplicity.
\begin{proposition}\label{th:gamma_sampling}
We have
    
    where  and  is a Lipschitz constant of .
\end{proposition}

In line with our intuition, CM's multistep sampling () yields a broader range of  compared to , resulting in diverging semantic meaning with increasing sampling NFE. 


\subsection{Accumulated Errors in the General Form of -sampling.}\label{sec:acc_error}

We can extend Theorem~\ref{th:sampling_agg_error_2_steps} for two steps -sampling for the case of multisteps. 


We begin by clarifying the concept of ``density transition by a function''. For a measurable mapping  and a measure  on the measurable space , the notation  denotes the pushforward measure, indicating that if a random vector  follows the distribution , then  follows the distribution .


Given a sampling timestep  . Let  represent the density resulting from N-steps of -sampling initiated at . That is, 

Here,  denotes the sequential composition. We assume an optimal CTM which precisely distills information from the teacher model  for all .



\begin{theorem}[Accumulated errors of N-steps -sampling]\label{th:sampling_agg_error_N_steps}
Let .

Here,  denotes the oracle transition mapping from  to , determined by Eq.~\eqref{eq:sde_forward}. The pushforward density via  is denoted as , with similar notation applied to , where  denotes the transition mapping associated with the optimal CTM trained from Eq.~\eqref{eq:ultimate_loss}.

    
\end{theorem}










\subsection{Transition Densities with the Optimal CTM}
In this section, for simplicity, we assume the optimal CTM,  with a well-learned , which recovers the true -function. We establish that the density propagated by this optimal CTM from any time  to a subsequent time  aligns with the predefined density determined by the fixed forward process. 



We now present the proposition ensuring alignment of the transited density.
\begin{proposition}\label{th:transition}
    Let  be densities defined by the diffusion process Eq.~\eqref{eq:sde_forward}, where . Denote  for any .  Suppose that the score  satisfies that there is a function  so that  and 
    \begin{enumerate}[(i)]
        \item Linear growth: , for all 
        \item Lipschitz: , for all .
    \end{enumerate}
    Then for any  and , .
\end{proposition}

This theorem guarantees that by learning the optimal CTM, which possesses complete trajectory information, we can retrieve all true densities at any time using CTM.








\section{Algorithmic Details}

\subsection{Motivation of Parametrization}\label{sec:parametrization}

Our parametrization of  is affected from the discretized ODE solvers. For instance, the one-step Euler solver has the solution of

The one-step Heun solver is

Again, the solver scales  with  and multiply  to the second term. Therefore, our  is a natural way to \djk{represent} the ODE solution.

For future research, we establish conditions enabling access to both integral and integrand expressions. Consider a continuous real-valued function . We aim to identify necessary conditions on  for the expression of  as:

for a vector-value function  and that  satisfies:
\begin{itemize}
    \item  exists;
    \item it can be expressed algebraically with .
\end{itemize}
Starting with the definition of , we can obtain



Suppose that there is a continuous function  so that 

then 

The second equality follows from the mean value theorem (We omit the continuity argument details for Markov filtrations). Therefore, we obtain the desired property 2). We summarize the necessary conditions on  as:



We now explain the above observation with an example by considering EDM-type parametrization. Consider  and . Then  can be expressed as 

where  is defined as 

Then, we can verify that  satisfies the condition in Eq.~\eqref{eq:cond} and that




The DSM loss with this  becomes

However, empirically, we find that the parametrization of  and  other than the ODE solver-oriented one, i.e.,  and , faces training instability. Therefore, we set  as our default design and estimate -function with the neural network.


\subsection{Characteristics of -sampling}\label{sec:connect_sde}
 
 \textbf{Connection with SDE} When , a single step of -sampling is expressed as:

where . This formulation cannot be interpreted as a differential form~\citep{oksendal2003stochastic} because it look-ahead future information (from  to ) to generate the sample  at time . This suggests that there is no It\^o's SDE that corresponds to our -sampler pathwisely, opening up new possibilities for the development of a new family of diffusion samplers.

\textbf{Connection with EDM's stochastic sampler}\label{sec:edm_stochastic}
We conduct a direct comparison between EDM's stochastic sampler and CTM's -sampling. We denote  as Heun's solver initiated at time  and point  and ending at time . It's worth noting that EDM's sampler inherently experiences discretization errors stemming from the use of Heun's solver, while CTM is immune to such errors.

The primary distinction between EDM's stochastic sampling in Algorithm~\ref{alg:EDM} and CTM's -sampling in Algorithm~\ref{alg:gamma} is the order of the forward (diffuse) and backward (denoise) steps. However, through the iterative process of forward-backward time traveling, these two distinct samplers become indistinguishable. Aside from the order of forward-backward steps, the two algorithms essentially align if we opt to synchronize the CTM's time  to with the EDM's time , respectively, and their s accordingly. 

\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{EDM's sampler}\label{alg:EDM}
    \begin{algorithmic}[1]
    \State Start from 
        \For{ to }
        \State 
        \State Diffuse 
        \State Denoise 
        \EndFor
        \State \textbf{Return}  
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{CTM's -sampling}\label{alg:gamma}
    \begin{algorithmic}[1]
    \State Start from 
        \For{ to }
        \State 
        \State Denoise 
        \State Diffuse 
        \EndFor
        \State \textbf{Return}  
    \end{algorithmic}
\end{algorithm}
\end{minipage}


\begin{algorithm}[H]
    \centering
    \caption{Loss-based Trajectory Optimization}\label{alg:application}
    \begin{algorithmic}[1]
    \State  is given
    \State Diffuse 
        \For{ to }
        \State 
        \State Denoise 
        \For{ to }
        \State Sample 
        \State Apply corrector 
        \EndFor
        \State Sample 
        \State Diffuse 
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Trajectory Control with Guidance}\label{subsec:traj_control}


We could apply -sampling for application tasks, such as image inpainting or colorization, using the (straightforwardly) generalized algorithm suggested in CM. In this section, however, we propose a loss-based trajectory optimization algorithm in Algorithm~\ref{alg:application} for potential application downstream tasks.

Algorithm~\ref{alg:application} uses the time traversal from  to , and apply the loss-embedded corrector~\citep{song2020score} algorithm to explore -manifold. For instance, the loss could be a feature loss between  and . With this corrector-based guidance, we could control the sample variance. This loss-embedded corrector could also be interpreted as sampling from a posterior distribution. For Figure~\ref{fig:cat_stroke}, we choose  with , , and .

\section{Implementation Details}\label{sec:implementation}

\begin{table}[t]
	\caption{Implementation details.}
	\label{tab:implementation}
	\scriptsize
	\centering
	\begin{tabular}{lcccc}
		\toprule
		Hyperparameter & \multicolumn{3}{c}{CIFAR-10} & ImageNet 64x64 \\\cmidrule(lr){2-4}\cmidrule(lr){5-5}
        & \multicolumn{2}{c}{Unconditional} & Conditional & Conditional \\\cmidrule(lr){2-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}
        & Training with  & Training from Scratch & Training with  & Training with  \\\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}
        Learning rate & 0.0004 & 0.0004 & 0.0004 & 0.000008 \\
        Discriminator learning rate & 0.002 & 0.002 & 0.002 & 0.002 \\
         & 0.9999 & 0.999 & 0.999 & 0.999 \\
         & 18 & 18 & 18 & 40 \\
        ODE solver & Heun & Self & Heun & Heun \\
        Max. ODE steps & 17 & 17 & 17 & 20 \\
        EMA decay rate & 0.999 & 0.999 & 0.999 & 0.999 \\
        Training iterations & 100K & 100K & 100K & 30K \\
        Mixed-Precision (FP16) & True & True & True & True \\
        Batch size & 256 & 128 & 512 & 2048 \\
        Number of GPUs & 4 & 4 & 4 & 8 \\
            \bottomrule
	\end{tabular}
\end{table}

\subsection{Training Details}\label{appendix:training_details}

Following \citet{karras2022elucidating}, we utilize the EDM's skip scale and output scale for  modeling as

where  refers to a neural network that takes the same input arguments as .
The advantage of this EDM-style skip and output scaling is that if we copy the teacher model's parameters to the student model's parameters, except student model's -embedding structure,  initialized with  would be close to the teacher denoiser . This good initialization partially explains the fast convergence speed.

We use 4V100 (16G) GPUs for CIFAR-10 experiments and 8A100 (40G) GPUs for ImageNet experiments. We use the warm-up for  hyperparameter. On CIFAR-10, we deactivate GAN training with  until 50k training iterations and activate the generator training with the adversarial loss (added to CTM and DSM losses) by increasing  to one. The minibatch per GPU is 16 in the CTM+DSM training phase, and 11 in the CTM+DSM+GAN training phase. On ImageNet, due to the excessive training budget, we deactivate GAN only for 10k iterations and activate GAN training afterwards. We fix the minibatch to be 11 throughout the CTM+DSM or the CTM+DSM+GAN training in ImageNet.

We follow the training configuration mainly from CM, but for the discriminator training, we follow that of StyleGAN-XL~\citep{sauer2022stylegan}. For  calculation, we use LPIPS~\citep{zhang2018unreasonable} as a feature extractor. We choose  and  from the -discretized timesteps to calculate , following CM. Across the training, we choose the maximum number of ODE steps to prevent a single iteration takes too long time. For CIFAR-10, we choose  and the maximum number of ODE steps to be 17. For ImageNet, we choose  and the maximum number of ODE steps to be 20. We find the tendency that the training performance is improved by the number of ODE steps, so one could possibly improve our ImageNet result by choosing larger maximum ODE steps.

For  calculation, we select  of time sampling from EDM's original scheme of . For the other half time, we first draw sample from  and transform it using . This specific time sampling blocks the neural network to forget the denoiser information for large time. For  calculation, we use two feature extractors to transform GAN input to the feature space: the EfficientNet~\citep{tan2019efficientnet} and DeiT-base~\citep{touvron2021training}. Before obtaining an input's feature, we upscale the image to 224x224 resolution with bilinear interpolation. After transforming to the feature space, we apply the cross-channel mixing and cross-scale mixing to represent the input with abundant and non-overlapping features. The output of the cross-scale mixing is a feature pyramid consisting of four feature maps at different resolutions~\citep{sauer2022stylegan}. In total, we use eight discriminators (four for EfficientNet features and the other four for DeiT-base features) for GAN training.

Following CM, we apply Exponential Moving Average (EMA) to update  by

However, unlike CM, we find that our model bestly works with  or , which largely remedy the subtle instability arise from GAN training. Except for the unconditional CIFAR-10 training with , we set  to be 0.999 as default. Throughout the experiments, we use , , , and .

\subsection{Evaluation Details}\label{appendix:sampling_details}

For likelihood evaluation, we solve the PF ODE, following the practice suggested in \citet{kim2022maximum} with the RK45~\citep{dormand1980family} ODE solver of  and . 

Throughout the paper, we choose  otherwise stated. In particular, for Tables~\ref{tab:cifar10_baseline} and \ref{tab:ImageNet64_baseline}, we report the sample quality metrics based on either the one-step sampling of CM or the  sampling for NFE 2 case. For CIFAR-10, we calculate the FID score based on \citet{karras2022elucidating} statistics. For ImageNet, we compute the metrics following \citet{dhariwal2021diffusion} and their pre-calculated statistics. For the StyleGAN-XL ImageNet result, we recalculated the metrics based on the statistics released by \citet{dhariwal2021diffusion}, using StyleGAN-XL's official checkpoint.

For large-NFE sampling, we follow the EDM's time discretization. Namely, if we draw -NFE samples, we equi-divide  with  points and transform it (say ) to the time scale by . However, we emphasize the time discretization for both training and sampling is a modeler's choice.

\section{Additional Generated Samples}\label{sec:generated_samples}
\begin{figure}[t]\centering
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{image_1.pdf}
		\subcaption{Tench}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{image_2.pdf}
		\subcaption{Tree frog}
	\end{subfigure}	
 	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{image_3.pdf}
		\subcaption{Elephant}
	\end{subfigure}
  	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=\linewidth]{image_4.pdf}
		\subcaption{Kimono}
	\end{subfigure}
	\caption{Uncurated sample comparisons with identical starting points, generated by EDM (FID 2.44) with NFE 79, CTM (FID 2.06) with NFE 1, CTM (FID 1.90) with NFE 2, and CM (FID 6.20) with NFE 1, on (a) tench (class id: 0), (b) tree frog (class id: 31), (c) elephant (class id: 386), and (d) kimono (class id: 614).}
    \label{fig:image_results}
	 \vskip -0.1in
\end{figure}
\newpage
\clearpage
\section{Theoretical Supports and Proofs}\label{sec:proof}

\subsection{Proof of Lemma~\ref{th:unification}}
\begin{myproof}{Lemma}{\ref{th:unification}}
As the score, , is integrable, the Fundamental Theorem of Calculus applies, leading to

\end{myproof}

\subsection{Proof of Theorem~\ref{th:sampling_agg_error_2_steps}}



\begin{myproof}{Theorem}{\ref{th:sampling_agg_error_2_steps}}
Define  as the oracle transition mapping from  to  via the diffusion process Eq.~\eqref{eq:sde_forward}. Let  represent the transition mapping from the optimal CTM, and  represent the transition mapping from the empirical probability flow ODE. 
Since all processes start at point  with initial probability distribution  and , Theorem 2~in \citep{chen2022sampling} and  from Proposition~\ref{th:transition} tell us that for 





Here (a) is obtained from the triangular inequality, (b) and (c) are due to  and  from Proposition~\ref{th:transition}, and (d) comes from Eq.~\eqref{eq:tv_error_1_step}.  
    
    

\end{myproof}





\subsection{Proof of Proposition~\ref{th:ptw_distill}}


\begin{myproof}{Proposition}{\ref{th:ptw_distill}} Consider a LPIPS-like metric, denoted as , determined by a feature extractor  of . That is,  for . For simplicity of notation, we denote  as .
    Since , it implies that for any , , and   


Denote 

Then due to Eq.~\eqref{eq:ctm_zero} and  is an ODE-trajectory function that , we have

Therefore, 


Notice that since , .

So we can obtain via induction that 

\end{myproof}

Indeed, an analogue of Proposition~\ref{th:ptw_distill} holds for time-conditional feature extractors.

Let  be a LPIPS-like metric determined by a time-conditional feature extractor . That is,  for . We can similarly derive







\subsection{Proof of Proposition~\ref{th:density_match}}



\begin{myproof}{Proposition}{\ref{th:density_match}}
    We first prove that for any  and , as ,
    
      We may assume  so that , , and ,  as .
    
Since both  and  are uniform continuous on , together with Proposition~\ref{th:ptw_distill}, we obtain Eq.~\eqref{eq:conti_time_distill} as .

     In particular, Eq.~\eqref{eq:conti_time_distill} implies that when 
    
    
    This implies that , the pushforward distribution of  induced by , converges in distribution to . Note that since  is uniform Lipschitz
    
     is asymptotically uniformly equicontinuous.  Moreover,  is uniform bounded in . Therefore, the converse of Scheff\'e's theorem
    \citep{boos1985converse,sweeting1986converse} implies that  as . Similar argument can be adapted to prove  as  if the regression target  is replaced with .
\end{myproof}



\subsection{Proof of Proposition~\ref{th:gt_injective}}


\begin{lemma}\label{th:injective_sol_op}
    Let  be a function which satisfies the following conditions:
    \begin{enumerate}[(a)]
        \item  is Lipschitz for any : there is a function  so that for any  and 
        
        \item Linear growth in : there is a - integrable function  so that for any  and 
                
    \end{enumerate}
    Consider the following ODE 
    
   Fix a , the solution operator  of Eq.~\eqref{eq:lemma_ode} with an initial condition  is defined as
   
   Here  denotes the solution at time  starting from the initial value .
   Then  is an injective operator. Moreover,  is bi-Lipschitz; that is, for any 
   
   Here .
   In particular, if ,  for all .
\end{lemma}
\begin{myproof}{Lemma}{\ref{th:injective_sol_op}}

    Assumptions (a) and (b) ensure the solution operator in Eq.~\eqref{eq:sol_op} is well-defined by applying Carath\'eodory-type global existence theorem~\citep{reid1971ordinary}. We denote  as .
    We need to prove that for any distinct initial values  and  starting from ,  . Suppose on the contrary that there is an  so that . For , consider 
     and .
    Then both  and  satisfy the following ODE
    
    Thus, the uniqueness theorem of solution to Eq.~\eqref{eq:lemma_ode_reverse} leads to , which means . This contradicts to the assumption. Hence,  is injective. 

    
    Now we show that  is bi-Lipschitz for any . For any ,
    
    By applying Gr\"ownwall's lemma, we obtain 
    
    On the other hand, consider the reverse time ODE of Eq.~\eqref{eq:lemma_ode} by setting , 
    , and , then  satisfies the following equation
    
    Similarly, we define the solution operator to Eq.~\eqref{eq:lemma_ode_reverse_general} as 
       
    Here  denotes the initial value of Eq.~\eqref{eq:lemma_ode_reverse_general} and  is the solution starting from . Due to the Carath\'eodory-type global existence theorem, the operator  is well-defined and 
    
    For simplicity, let  and . Also, denote the solutions starting from initial values  and  as  and  , respectively.
    Therefore, using a similar argument, we obtain
        
    By applying Gr\"ownwall's lemma, we obtain 
            
    Therefore, 
       
\end{myproof}

\begin{myproof}{Proposition}{\ref{th:gt_injective}} With the definition of , we obtain

Here, . Thus, the result follows by applying Lemma~\ref{th:injective_sol_op} to the integral form of .

\end{myproof}






\subsection{Proof of Proposition~\ref{th:gamma_sampling}}




\begin{lemma}\label{th:Lip_var}
    Let  be a random vector on  and  be a bi-Lipschitz mapping with Lipschitz constant ; namely, for any 
    
    Then 
    
\end{lemma}

\begin{myproof}{Lemma}{\ref{th:Lip_var}}
    Let  be an i.i.d. copy of . Then  and  are also independent. Thus,  and .
    
    Since  and  are identically distributed, .
    Thus, by Lipschitzness of 
        
    The final equality follows the same reasoning as in Eq.~\eqref{eq:h_var}. Likewise, we can apply the argument from Eq.~\eqref{eq:h_var_lip} to show that
    
    Therefore, .
\end{myproof}


\begin{myproof}{Proposition}{\ref{th:gamma_sampling}}
For any , since  and  are independent,

Proposition~\ref{th:gt_injective} implies that  is bi-Lipschitz and that for any 

where . Proposition~\ref{th:gamma_sampling} follows immediately from the inequalities \eqref{eq:recursive_var} and \eqref{eq:bi_lip_G}.
\end{myproof}


\subsection{Proof of Proposition~\ref{th:transition}}

\begin{myproof}{Proposition}{\ref{th:transition}}
     is known to satisfy the Fokker-Planck equation~\citep{oksendal2003stochastic} (under some technical regularity conditions). In addition, we can rewrite the Fokker-Planck equation of  as the following equation (see Eq.~(37) in \citep{song2020score})
    
    where . 

    Now consider the continuity equation for  defined by  
    
    Since the score  is of linear growth in  and upper bounded by a summable function in , the vector field  satisfies that 
        
    for any compact set . Here  denotes the Lipschitz constant of  on .

    Thus, Proposition 8.1.8 of \citep{ambrosio2005gradient} implies that for -a.e. , the following reverse time ODE (which is the Eq.~\eqref{eq:sde_forward}) admits a unique solution on 
    
    Moreover, , for . By applying the uniqueness for the continuity equation (Proposition 8.1.7 of \citep{ambrosio2005gradient}) and the uniqueness of Eq.~\eqref{eq:ode_charac_conti}, we have  for . Again, since the uniqueness theorem with the given , we obtain  for any  and .
    
\end{myproof}










\end{document}
