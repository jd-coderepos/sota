\documentclass{article}

\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{hyperref}

\usepackage[accepted]{icml2021}
\icmltitlerunning{Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech}

\begin{document}

\twocolumn[
\icmltitle{Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Vadim Popov}{equal,hw}
\icmlauthor{Ivan Vovk}{equal,hw,hse}
\icmlauthor{Vladimir Gogoryan}{hw,hse}
\icmlauthor{Tasnima Sadekova}{hw}
\icmlauthor{Mikhail Kudinov}{hw}
\end{icmlauthorlist}

\icmlaffiliation{hw}{Huawei Noah's Ark Lab, Moscow, Russia}
\icmlaffiliation{hse}{Higher School of Economics, Moscow, Russia}

\icmlcorrespondingauthor{Vadim Popov}{vadim.popov@huawei.com}
\icmlcorrespondingauthor{Ivan Vovk}{vovk.ivan@huawei.com}

\icmlkeywords{Text-to-Speech, score matching, diffusion probabilistic modelling, SDE}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. The code is publicly available at \url{https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS}.

\end{abstract}

\section{Introduction}
\label{sec:intro}

Deep generative modelling proved to be effective in various machine learning fields, and speech synthesis is no exception. Modern text-to-speech (TTS) systems often consist of two parts designed as deep neural networks: the first part converts the input text into time-frequency domain acoustic features (\textit{feature generator}), and the second one synthesizes raw waveform conditioned on these features (\textit{vocoder}). Introduction of the conventional state-of-the-art autoregressive models such as Tacotron2 \cite{Tacotron2} used for feature generation and WaveNet \cite{WaveNet} used as vocoder marked the beginning of the neural TTS era. Later, other popular generative modelling frameworks such as Generative Adversarial Networks \cite{GANPaper} and Normalizing Flows \cite{NFlows} were used in the design of TTS engines for a parallel generation with comparable quality of the synthesized speech. 


Since the publication of the WaveNet paper \yrcite{WaveNet}, there have been various attempts to propose a parallel non-autoregressive vocoder, which could synthesize high-quality speech. Popular architectures based on Normalizing Flows like Parallel WaveNet \cite{ParallelWaveNet} and WaveGlow \cite{WaveGlow} managed to accelerate inference while keeping synthesis quality at a very high level but demonstrated fast synthesis on GPU devices only. Eventually, parallel GAN-based vocoders such as Parallel WaveGAN \cite{ParallelWaveGAN}, MelGAN \cite{MelGan}, and HiFi-GAN \cite{HiFi-GAN} greatly improved the performance of waveform generation on CPU devices. Furthermore, the latter model is reported to produce speech samples of state-of-the-art quality outperforming WaveNet.

Among feature generators, Tacotron2 \cite{Tacotron2} and Transformer-TTS \cite{TransformerTTS} enabled highly natural speech synthesis. Producing acoustic features frame by frame, they achieve almost perfect mel-spectrogram reconstruction from input text. Nonetheless, 
they often suffer from computational inefficiency and pronunciation issues coming from attention failures.
Addressing these problems, such models as FastSpeech \cite{FastSpeech} and Parallel Tacotron \cite{ParallelTacotron} substantially improved inference speed and pronunciation robustness by utilizing non-autoregressive architectures and building hard monotonic alignments from estimated token lengths. However, in order to learn character duration, they still require pre-computed alignment from the teacher model. Finally, the recently proposed Non-Attentive Tacotron framework \cite{NonAttentiveTacotron} managed to learn durations implicitly by employing the Variational Autoencoder concept.


Glow-TTS feature generator \cite{GlowTTS} based on Normalizing Flows can be considered as one of the most successful attempts to overcome pronunciation and computational latency issues typical for autoregressive solutions. Glow-TTS model made use of Monotonic Alignment Search algorithm (an adoption of Viterbi training \cite{Viterbi} finding the most likely hidden alignment between two sequences) proposed to map the input text to mel-spectrograms efficiently. The alignment learned by Glow-TTS is intentionally designed to avoid some of the pronunciation problems models like Tacotron2 suffer from. Also, in order to enable parallel synthesis, Glow-TTS borrows encoder architecture from Transformer-TTS \cite{TransformerTTS} and decoder architecture from Glow \cite{GlowNF}. Thus, compared with Tacotron2, Glow-TTS achieves much faster inference making fewer alignment mistakes. Besides, in contrast to other parallel TTS solutions such as FastSpeech, Glow-TTS does not require an external aligner to obtain token duration information as Monotonic Alignment Search (MAS) operates in an unsupervised way.



Lately, another family of generative models called Diffusion Probabilistic Models (DPMs) \cite{DiffusionBasic} has started to prove its capability to model complex data distributions such as images \cite{DDPM}, shapes \cite{Shapes}, graphs \cite{Graphs}, handwriting \cite{Handwriting}. 
The basic idea behind DPMs is as follows: we build a forward diffusion process by iteratively destroying original data until we get some simple distribution (usually standard normal), and then we try to build a reverse diffusion parameterized with a neural network so that it follows the trajectories of the reverse-time forward diffusion. Stochastic calculus offers a continuous easy-to-use framework for training DPMs \cite{SDE-main} and, which is perhaps more important, provides a number of flexible inference schemes based on numerical differential equation solvers.

As far as text-to-speech applications are concerned, two vocoders representing the DPM family showed impressive results in raw waveform reconstruction: WaveGrad \cite{WaveGrad} and DiffWave \cite{DiffWave} were shown to reproduce the fine-grained structure of human speech and match strong autoregressive baselines such as WaveNet in terms of synthesis quality while at the same time requiring much fewer sequential operations. However, despite such a success in neural vocoding, no feature generator based on diffusion probabilistic modelling is known so far. 

This paper introduces Grad-TTS, an acoustic feature generator with a score-based decoder using recent diffusion probabilistic modelling insights. In Grad-TTS, MAS-aligned encoder outputs are passed to the decoder that transforms Gaussian noise parameterized by these outputs into a mel-spectrogram. To cope with the task of reconstructing data from Gaussian noise with varying parameters, we write down a generalized version of conventional forward and reverse diffusions. One of the remarkable features of our model is that it provides explicit control of the trade-off between output mel-spectrogram quality and inference speed. In particular, we find that Grad-TTS is capable of generating mel-spectrograms of high quality with only as few as ten iterations of reverse diffusion, which makes it possible to outperform Tacotron2 in terms of speed on GPU devices. Additionally, we show that it is possible to train Grad-TTS as an end-to-end TTS pipeline (i.e., vocoder and feature generator are combined in a single model) by replacing its output domain from mel-spectrogram to raw waveform.

\section{Diffusion probabilistic modelling}
\label{sec:diffusion}

Loosely speaking, a process of the diffusion type is a stochastic process that satisfies a stochastic differential equation (SDE)



where  is the standard Brownian motion,  for some finite time horizon , and coefficients  and  (called \textit{drift} and \textit{diffusion} correspondingly) satisfy certain measurability conditions. A rigorous definition of the diffusion type processes, as well as other notions from stochastic calculus we use in this section, can be found in \cite{SDE-book}.

It is easy to find such a stochastic process that terminal distribution  converges to standard normal  when  for any initial data distribution  ( is  identity matrix and  is data dimensionality). In fact, there are lots of such processes as it follows from the formulae given later in this section. Any process of the diffusion type with such property is called \textit{forward diffusion} and the goal of diffusion probabilistic modelling is to find a \textit{reverse diffusion} such that its trajectories closely follow those of the forward diffusion but in reverse time order. This is, of course, a much harder task than making Gaussian noise out of data, but in many cases it still can be accomplished if we parameterize reverse diffusion with a proper neural network. In this case, generation boils down to sampling random noise from  and then just solving the SDE describing dynamics of the reverse diffusion with any numerical solver (usually a simple first-order Euler-Maruyama scheme \cite{SDE-numerical} is used). If forward and reverse diffusion processes have close trajectories, then the distribution of resulting samples will be very close to that of the data . This approach to generative modelling is summarized in Figure~\ref{fig:diffusion}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[scale=0.6]{diffusion.png}}
\caption{Diffusion probabilistic modelling for mel-spectrograms.}
\label{fig:diffusion}
\end{center}
\vskip -0.2in
\end{figure}

Until recently, score-based and denoising diffusion probabilistic models were formalized in terms of Markov chains \cite{DiffusionBasic, ScoreBasedGeneration, DDPM, ScoreBasedImproved}. A unified approach introduced by Song et al. \yrcite{SDE-main} has demonstrated that these Markov chains actually approximated trajectories of stochastic processes satisfying certain SDEs. In our work, we follow this paper and define our DPM in terms of SDEs rather than Markov chains. As one can see later in Section \ref{sec:grad-tts}, the task we are solving suggests generalizing DPMs described in \cite{SDE-main} in such a way that for infinite time horizon forward diffusion transforms any data distribution into  instead of  for any given mean  and diagonal covariance matrix . So, the rest of this section contains the detailed description of the generalized forward and reverse diffusions we utilize as well as the loss function we optimize to train the reverse diffusion. All corresponding derivations can be found in \hyperref[sec:appendix]{Appendix}.

\subsection{Forward diffusion}
\label{subsec:fwd_diffusion}

First, we need to define a forward diffusion process that transforms any data into Gaussian noise given infinite time horizon . If -dimensional stochastic process  satisfies the following SDE:



for non-negative function , which we will refer to as noise schedule, vector , and diagonal matrix  with positive elements, then its solution (if it exists) is given by 



Note that the exponential of a diagonal matrix is just an element-wise exponential. Let



and 



By properties of It\^o's integral conditional distribution of  given  is Gaussian:



It means that if we consider infinite time horizon then for \textit{any} noise schedule  such that  we have



So, random variable  converges in distribution to  independently of , and it is exactly the property we need: forward diffusion satisfying SDE (\ref{eq:fwd_diffusion}) transforms any data distribution  into Gaussian noise  .

\subsection{Reverse diffusion}
\label{subsec:rev_diffusion}

While in earlier works on DPMs reverse diffusion was trained to approximate the trajectories of forward diffusion, Song et al. \yrcite{SDE-main} proposed to use the result by Anderson \yrcite{SDE-reverse}, who derived an explicit formula for reverse-time dynamics of a wide class of stochastic processes of the diffusion type. In our case, this result leads to the following SDE for the reverse diffusion:



where  is the reverse-time Brownian motion and  is the probability density function of random variable . This SDE is to be solved backwards starting from terminal condition .

Moreover, Song et al. \yrcite{SDE-main} have shown that instead of SDE (\ref{eq:bwd_diffusion_sde}), we can consider an ordinary differential equation



Forward Kolmogorov equations corresponding to (\ref{eq:fwd_diffusion}) and (\ref{eq:bwd_diffusion_ode}) are identical, which means that the evolution of probability density functions of stochastic processes given by (\ref{eq:fwd_diffusion}) and (\ref{eq:bwd_diffusion_ode}) is the same.

Thus, if we have a neural network  that estimates the gradient of the log-density of noisy data , then we can model data distribution  by sampling  from  and numerically solving either (\ref{eq:bwd_diffusion_sde}) or (\ref{eq:bwd_diffusion_ode}) backwards in time.

\subsection{Loss function}
\label{subsec:loss}

Estimating gradients of log-density of noisy data  is often referred to as \textit{score matching}, and in recent papers \cite{ScoreBasedGeneration, ScoreBasedImproved}  loss was used to approximate these gradients with a neural network. So, in our paper, we use the same type of loss.

\begin{figure*}[ht]
\begin{center}
\center{\includegraphics[width=1.0\linewidth]{grad-tts-new.png}}
\end{center}
\caption{Grad-TTS inference scheme.}
\label{fig:main}
\end{figure*}

Due to the formula (\ref{eq:sol_distribution}), we can sample noisy data  given only initial data  without sampling intermediate values . Moreover,  is Gaussian, which means that its log-density has a very simple closed form. If we sample  from  and then put 



in accordance with (\ref{eq:sol_distribution}), then the gradient of log-density of noisy data in this point  is given by



where  is the probability density function of the conditional distribution (\ref{eq:sol_distribution}). Thus, loss function corresponding to estimating the gradient of log-density of data  corrupted with noise accumulated by time  is



where  is sampled from  and  is calculated by formula (\ref{eq:x_t}).

\section{Grad-TTS}
\label{sec:grad-tts}

The acoustic feature generator we propose consists of three modules: encoder, duration predictor, and decoder. In this section, we will describe their architectures as well as training and inference procedures. The general approach is illustrated in Figure~\ref{fig:main}. Grad-TTS has very much in common with Glow-TTS \cite{GlowTTS}, a feature generator based on Normalizing Flows. The key difference lies in the principles the decoder relies on.

\subsection{Inference}
\label{subsec:inference}

An input text sequence  of length  typically consists of characters or phonemes, and we aim at generating mel-spectrogram  where  is the number of acoustic frames. In Grad-TTS, the encoder converts an input text sequence  into a sequence of features  used by the duration predictor to produce hard monotonic alignment  between encoded text sequence  and frame-wise features . The function  is a monotonic surjective mapping between  and , and we put  for any integer . Informally speaking, the duration predictor tells us how many frames each element of text input lasts. Monotonicity and surjectiveness of  guarantee that the text is pronounced in the correct order without skipping any text input. As in all TTS models with duration predictor, it is possible to control synthesized speech tempo by multiplying predicted durations by some factor.

The output sequence  is then passed to the decoder, which is a Diffusion Probabilistic Model. A neural network  with parameters  defines an ordinary differential equation (ODE)



which is solved backwards in time using the first-order Euler scheme. The sequence  is also used to define the terminal condition . Noise schedule  and time horizon  are some pre-defined hyperparameters whose choice mostly depends on the data, while step size  in the Euler scheme is a hyperparameter that can be chosen after Grad-TTS is trained. It expresses the trade-off between the quality of output mel-spectrograms and inference speed.

Reverse diffusion in Grad-TTS evolves according to equation (\ref{eq:decoder}) for the following reasons:

\begin{itemize}
    \item We obtained better results in practice when using dynamics (\ref{eq:bwd_diffusion_ode}) instead of (\ref{eq:bwd_diffusion_sde}): for small values of step size , they performed equally well, while for larger values the former led to much better sounding results.
    \item We chose  to simplify the whole feature generation pipeline.
    \item We used  as an additional input to the neural network . It follows from (\ref{eq:log_density}) that the neural network  essentially tries to predict Gaussian noise added to data  observing only noisy data . So, if for every time  we supply  with an additional knowledge of how the limiting noise  looks like (note that it is different for different text input), then this network can make more accurate predictions of noise at time .
\end{itemize}

We also found it beneficial for the model performance to introduce a temperature hyperparameter  and to sample terminal condition  from  instead of . Tuning  can help to keep the quality of output mel-spectrograms at the same level when using larger values of step size .

\subsection{Training}
\label{subsec:training}

One of Grad-TTS training objectives is to minimize the distance between aligned encoder output  and target mel-spectrogram  because the inference scheme that has just been described suggests to start decoding from random noise . Intuitively, it is clear that decoding is easier if we start from noise, which is already close to the target  in some sense.

If aligned encoder output  is considered to parameterize an input noise the decoder starts from, it is natural to regard encoder output  as a normal distribution , which leads to a negative log-likelihood encoder loss:



where  is a probability density function of . Although other types of losses are also possible, we have chosen  (which actually reduces to Mean Square Error criterion) because of this probabilistic interpretation. In principle, it is even possible to train Grad-TTS without any encoder loss at all and let the diffusion loss described further do all the job of generating realistic mel-spectrograms, but in practice we observed that in the absence of  Grad-TTS failed to learn alignment.

The encoder loss  has to be optimized with respect to both encoder parameters and alignment function . Since it is hard to do a joint optimization, we apply an iterative approach proposed by Kim et al. \yrcite{GlowTTS}. Each iteration of optimization consists of two steps: (i) searching for an optimal alignment  given fixed encoder parameters; (ii) fixing this alignment  and taking one step of stochastic gradient descent to optimize loss function with respect to encoder parameters. We use Monotonic Alignment Search at the first step of this approach. MAS utilizes the concept of dynamic programming to find an optimal (from the point of view of loss function ) monotonic surjective alignment. This algorithm is described in detail in \cite{GlowTTS}.

To estimate the optimal alignment  at inference, Grad-TTS employs the duration predictor network. As in \cite{GlowTTS}, we train the duration predictor  with Mean Square Error (MSE) criterion in logarithmic domain:



where  is an indicator function, ,  and stop gradient operator  is applied to the inputs of the duration predictor to prevent  from affecting encoder parameters.

As for the loss related to the DPM, it is calculated using formulae from Section \ref{sec:diffusion}. As already mentioned, we put , so the distribution of noisy data (\ref{eq:sol_distribution}) simplifies, and its covariance matrix becomes just an identity matrix  multiplied by a scalar



The overall diffusion loss function  is the expectation of weighted losses associated with estimating gradients of log-density of noisy data at different times :



where  stands for target mel-spectrogram  sampled from training data,  is sampled from uniform distribution on ,  -- from  and the formula 



is used to get noisy data  according to the distribution (\ref{eq:sol_distribution}). The above formulae (\ref{eq:loss_diff}) and (\ref{eq:x_t_sampling}) follow from (\ref{eq:loss_t}) and (\ref{eq:x_t}) by substitution . We use losses (\ref{eq:loss_t}) with weights  according to the common heuristics that these weights should be proportional to .

To sum it up, the training procedure consists of the following steps:

\begin{itemize}
    \item Fix the encoder, duration predictor, and decoder parameters and run MAS algorithm to find the alignment  that minimizes .
    \item Fix the alignment  and minimize  with respect to encoder, duration predictor, and decoder parameters.
    \item Repeat the first two steps till convergence.
\end{itemize}

\subsection{Model architecture}
\label{subsection:architecture}

As for the encoder and duration predictor, we use exactly the same architectures as in Glow-TTS, which in its turn borrows the structure of these modules from Transformer-TTS \cite{TransformerTTS} and FastSpeech \cite{FastSpeech} correspondingly. The duration predictor consists of two convolutional layers followed by a projection layer that predicts the logarithm of duration. The encoder is composed of a pre-net,  Transformer blocks with multi-head self-attention, and the final linear projection layer. The pre-net consists of  layers of convolutions followed by a fully-connected layer.

The decoder network  has the same U-Net architecture \cite{UNet} used by Ho et al. \yrcite{DDPM} to generate  images, except that we use twice fewer channels and three feature map resolutions instead of four to reduce model size. In our experiments we use -dimensional mel-spectrograms, so  operates on resolutions ,  and . We zero-pad mel-spectrograms if the number of frames  is not a multiple of . Aligned encoder output  is concatenated with U-Net input  as an additional channel.

\section{Experiments}
\label{sec:exp}

LJSpeech dataset \cite{LJSpeech} containing approximately  hours of English female voice recordings sampled at kHz was used to train the Grad-TTS model. The test set contained around  short audio recordings (duration less than  seconds each). The input text was phonemized before passing to the encoder; as for the output acoustic features, we used conventional -dimensional mel-spectrograms. We tried training both on original and normalized mel-spectrograms and found that the former performed better. Grad-TTS was trained for  iterations on a single GPU (NVIDIA RTX  Ti with GB memory) with mini-batch size . We chose Adam optimizer and set the learning rate to .

\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[scale=0.33]{diff.png}}
\caption{Diffusion loss at training.}
\label{fig:training}
\end{center}
\vskip -0.1in
\end{figure}

We would like to mention several important things about Grad-TTS training:

\begin{itemize}
    \item We chose , ,  and .
    \item As in \cite{GAN-TTS1, GAN-TTS2}, we use random mel-spectrogram segments of fixed length ( seconds in our case) as training targets  to allow for memory-efficient training. However, MAS and the duration predictor still use whole mel-spectrograms.
    \item Although diffusion loss  seems to converge very slowly after the beginning epochs, as shown on Figure~\ref{fig:training}, such long training is essential to get a good model because the neural network  has to learn to estimate gradients accurately for all . Two models with almost equal diffusion losses can produce mel-spectrograms of very different quality: inaccurate predictions for a small subset  may have a small impact on  but be crucial for the output mel-spectrogram quality if ODE solver involves calculating  in at least one point belonging to .
\end{itemize}

Once trained, Grad-TTS enables the trade-off between quality and inference speed due to the ability to vary the number of steps  the decoder takes to solve ODE (\ref{eq:decoder}) at inference. So, we evaluate four models which we denote by Grad-TTS-N where . We use  at synthesis for all four models. As baselines, we take an official implementation of Glow-TTS \cite{GlowTTS}, the model which resembles ours to the most extent among the existing feature generators, FastSpeech \cite{FastSpeech}, and state-of-the-art Tacotron2 \cite{Tacotron2}. Recently proposed HiFi-GAN \cite{HiFi-GAN} is known to provide excellent sound quality, so we use this vocoder with all models we compare.

\subsection{Subjective evaluation}
\label{subsec:subj}
To make subjective evaluation of TTS models, we used the crowdsourcing platform Amazon Mechanical Turk. For Mean Opinion Score (MOS) estimation we synthesized  sentences from the test set with each model. The assessors were asked to estimate the quality of synthesized speech on a nine-point Likert scale, the lowest and the highest scores being  point (``Bad'') and  points (``Excellent'') with a step of  point. To ensure the reliability of the obtained results, only Master assessors were assigned to complete the listening test. Each audio was evaluated by  assessors. A small subset of speech samples used in the test is available at \url{https://grad-tts.github.io/}.

\begin{table}[H]
\caption{Ablation study of proposed generalized diffusion framework. Grad-TTS reconstructing data from  for  reverse diffusion iterations is compared with the baseline Grad-TTS-10 -- the model reconstructing data from  for  iterations.}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
 &Worse, \% &Identical, \% &Better, \% \\ \hline
 & & & \\ \hline
 & & & \\ \hline
 & & & \\ \hline
\end{tabular}
\end{center}
\label{tab:preference_test}
\end{table}

MOS results with  confidence intervals are presented in Table~\ref{tab:main}. It demonstrates that although the quality of the synthesized speech gets better when we use more iterations of the reverse diffusion, the quality gain becomes marginal starting from a certain number of iterations. In particular, there is almost no difference between Grad-TTS-1000 and Grad-TTS-10 in terms of MOS, while the gap between Grad-TTS-10 and Grad-TTS-4 ( was the smallest number of iterations leading to satisfactory quality) is much more significant. As for other feature generators, Grad-TTS-10 is competitive with all compared models, including state-of-the-art Tacotron2. Furthermore, Grad-TTS-1000 achieves almost natural synthesis with MOS being less than that for ground truth recordings by only . We would like to note that the relatively low results of FastSpeech could possibly be explained by the fact that we used its unofficial implementation \url{https://github.com/xcmyz/FastSpeech}.

\begin{table*}[ht]
\caption{Model comparison.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Model &Enc params\footnote{} &Dec params &RTF &Log-likelihood &MOS \\ \hline
Grad-TTS-1000 & \multirow{4}{*}{} & \multirow{4}{*}{} & & \multirow{4}{*}{} & \\ \cline{1-1}\cline{4-4}\cline{6-6}
Grad-TTS-100 & & & & & \\ \cline{1-1}\cline{4-4}\cline{6-6}
Grad-TTS-10 & & & & & \\ \cline{1-1}\cline{4-4}\cline{6-6}
Grad-TTS-4 & & & & & \\ \hline
Glow-TTS & & & & & \\ \hline
FastSpeech & \multicolumn{2}{c}{} \vline & & & \\ \hline
Tacotron2 & \multicolumn{2}{c}{} \vline & & & \\ \hline
Ground Truth & \multicolumn{2}{c}{} \vline & & & \\ \hline
\end{tabular}
\end{center}
\label{tab:main}
\end{table*}

To verify the benefits of the proposed generalized DPM framework we trained the model with the same architecture as Grad-TTS to reconstruct mel-spectrograms from  instead of . The preference test provided in Table \ref{tab:preference_test} shows that Grad-TTS-10 is significantly better ( in sign test) than this model taking ,  and even  iterations of the reverse diffusion. It demonstrates that the model trained to generate from  needs more steps of ODE solver to get high-quality mel-spectrograms than Grad-TTS we propose. We believe this is because the task of reconstructing mel-spectrogram from pure noise  is more difficult than the one of reconstructing it from its noisy copy . One possible objection could be that the model trained with  as terminal distribution can just add  to this noise at the first step of sampling (it is possible because  has  as its input) and then repeat the same steps as our model to generate data from . In this case, it would generate mel-spectrograms of the same quality as our model taking only one step more. However, this argument is wrong, since reverse diffusion removes noise not arbitrarily, but according to the reverse trajectories of the forward diffusion. Since forward diffusion adds noise gradually, reverse diffusion has to remove noise gradually as well, and the first step of the reverse diffusion cannot be adding  to Gaussian noise with zero mean because the last step of the forward diffusion is not a jump from  to zero.

\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[scale=0.33]{qual.png}}
\caption{Typical errors occurrence.}
\label{fig:qualitative}
\end{center}
\vskip -0.1in
\end{figure}

We also made an attempt to estimate what kinds of mistakes are characteristic of certain models. We compared Tacotron2, Glow-TTS, and Grad-TTS-10 as the fastest version of our model with high synthesis quality. Each record was estimated by  assessors. Figure \ref{fig:qualitative} demonstrates the results of the multiple-choice test whose participants had to choose which kinds of errors (if any) they could hear: sonic artifacts like clicking sounds or background noise (``sonic'' in the figure), mispronunciation of words/phonemes (``mispron''), unnatural pauses (``pause''), monotone speech (``monotonic''), robotic voice (``robotic''), wrong word stressing (``stress'') or others. It is clear from the figure that Glow-TTS frequently stresses words in a wrong way, and the sound it produces is perceived as ``robotic'' in around a quarter of cases. These are the major factors that make Glow-TTS performance inferior to that of Grad-TTS and Tacotron2, which in their turn have more or less the same drawbacks in terms of synthesis quality.

\subsection{Objective evaluation}
\label{subsec:obj}
Although DPMs can be shown to maximize weighted variational lower bound \cite{DDPM} on data log-likelihood, they do not explicitly optimize exact data likelihood. In spite of this, Song et al. \yrcite{SDE-main} show that it is still possible to calculate it using the instantaneous change of variables formula \cite{NODE} if we look at DPMs from the ``continuous'' point of view. However, it is necessary to use Hutchinson's trace estimator to make computations feasible, so in Table~\ref{tab:main} log-likelihood for Grad-TTS comes with a  confidence interval.

We randomly chose  sentences from the test set and calculated their average log-likelihood under two probabilistic models we consider -- Glow-TTS and Grad-TTS. Interestingly, Grad-TTS achieves better log-likelihood than Glow-TTS even though the latter has a decoder with x larger capacity and was trained to maximize exact data likelihood. Similar phenomena were observed by Song et al. \yrcite{SDE-main} in the image generation task.

\subsection{Efficiency estimation}
\label{subsec:efficiency}

We assess the efficiency of the proposed model in terms of Real-Time Factor (RTF is how many seconds it takes to generate one second of audio) computed on GPU and the number of parameters. Table~\ref{tab:main} contains efficiency information for all models under comparison.
Additional information regarding absolute inference speed dependency on the input text length is given in Figure~\ref{fig:speed}.

Due to its flexibility at inference, Grad-TTS is capable of real-time synthesis on GPU: if the number of decoder steps is less than , it reaches RTF . Moreover, although it cannot compete with Glow-TTS and FastSpeech in terms of inference speed, it still can be approximately twice faster than Tacotron2 if we use  decoder iterations sufficient for getting high-fidelity mel-spectrograms. Besides, Grad-TTS has around  parameters, thus being significantly smaller than other feature generators we compare.

\begin{figure}[!ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[scale=0.34]{inference_speed_comparison.png}}
\caption{Inference speed comparison. Text length is given in characters.}
\label{fig:speed}
\end{center}
\vskip -0.1in
\end{figure}

\subsection{End-to-end TTS}
\label{subsec:e2e}

The results of our preliminary experiments show that it is also possible to train an end-to-end TTS model as a DPM. In brief, we moved from U-Net to WaveGrad \cite{WaveGrad} in Grad-TTS decoder: the overall architecture resembles WaveGrad conditioned on the aligned encoder output  instead of ground truth mel-spectrograms  as in original WaveGrad. Although synthesized speech quality is fair enough, it cannot compete with the results reported above, so we do not include our end-to-end model in the listening test but provide demo samples at \url{https://grad-tts.github.io/}.
\footnotetext{Encoder and duration predictor parameters are calculated together.}

\section{Future work}
\label{sec:future}

End-to-end speech synthesis results reported above show that it is a promising future research direction for text-to-speech applications. However, there is also much room for investigating general issues regarding DPMs.

In the analysis in Section \ref{sec:diffusion}, we always assume that both forward and reverse diffusion processes exist, i.e., SDEs (\ref{eq:fwd_diffusion}) and (\ref{eq:bwd_diffusion_sde}) have strong solutions. It applies some Lipschitz-type constraints \cite{SDE-book} on noise schedule  and, what is more important, on the neural network . Wasserstein GANs offer an encouraging example of incorporating Lipschitz constraints into neural networks training \cite{WGAN-GP}, suggesting that similar techniques may improve DPMs.

Little attention has been paid so far to the choice of the noise schedule  -- most researchers use a simple linear schedule. Also, it is mostly unclear how to choose weights for losses (\ref{eq:loss_t}) at time  in the global loss function optimally. A thorough investigation of such practical questions is crucial as it can facilitate applying DPMs to new machine learning problems.

\section{Conclusion}
\label{sec:conclusion}

We have presented Grad-TTS, the first acoustic feature generator utilizing the concept of diffusion probabilistic modelling. The main generative engine of Grad-TTS is the diffusion-based decoder that transforms Gaussian noise parameterized with the encoder output into mel-spectrogram while alignment is performed with Monotonic Alignment Search. The model we propose allows to vary the number of decoder steps at inference, thus providing a tool to control the trade-off between inference speed and synthesized speech quality. Despite its iterative decoding, Grad-TTS is capable of real-time synthesis. Moreover, it can generate mel-spectrograms twice faster than Tacotron2 while keeping synthesis quality competitive with common TTS baselines.

\bibliography{example_paper}
\bibliographystyle{icml2021}















\newpage
\onecolumn
\section*{Appendix}
\label{sec:appendix}
We include an appendix with detailed derivations, proofs and additional information. Our proposed diffusion probabilistic framework employs generalized terminal distribution  instead of  as proposed by Song et al. \yrcite{SDE-main}. The derivation for the solution (\ref{eq:solution}) of SDE (\ref{eq:fwd_diffusion}) that transforms the original data distribution to the terminal distribution is described in Appendix \ref{app:solving_forward_diff_sde}. In Appendix \ref{app:derivation_of_cond_dist} we also derive the distribution which the solution (\ref{eq:solution}) for the diffused data  follows. Then, the goal of diffusion probabilistic modelling is to reconstruct the reverse-time trajectories of the forward diffusion process, and Song et al. \yrcite{SDE-main} showed that these dynamics can follow two different differential equations: either SDE (\ref{eq:bwd_diffusion_sde}) proposed by Anderson \yrcite{SDE-reverse} or ODE (\ref{eq:bwd_diffusion_ode}). So, Appendix \ref{app:reverse_dynamics} contains these differential equations for  serving as terminal distribution. They depend on time-dependent gradient field  supposed to be modelled using neural network. In order to train it, we show how to compute the gradient in Appendix \ref{app:score_estimation}.

\renewcommand{\thesubsection}{\Alph{subsection}}

\subsection{Solving forward diffusion SDE}
\label{app:solving_forward_diff_sde}

Forward diffusion SDE is given by 



where  is -dimensional stochastic process,  is the standard -dimensional Brownian motion,  is -dimensional vector,  is  diagonal matrix with positive diagonal elements  and noise schedule  is non-negative function . Consider change of variables . Then we can rewrite forward diffusion SDE as



For every  we have



Exponential of a diagonal matrix is just element-wise exponential, so we can rewrite it in multidimensional form as



or writing this down in terms of :



where  is  identity matrix.



\subsection{Derivation of conditional distribution of }
\label{app:derivation_of_cond_dist}

Let . It is a diagonal matrix and its -th diagonal element  equals . Assume  for each . It\^o's integral  is defined as the limit of integral sums when mesh of partition  tends to zero:



where the first equality in distribution holds due to the properties of Brownian motion and the fact that  are deterministic (implying that  are independent normal random variables with mean  and variance ) and the second equality in distribution follows from L\'evy's continuity theorem (it is easy to check that the sequence of characteristic functions of random variables on the left-hand side converges point-wise to the characteristic function of the random variable on the right-hand side). Then, simple integration gives



It implies that in multidimensional case we have:



and it follows from (\ref{sol}) that







\subsection{Reverse dynamics}
\label{app:reverse_dynamics}
 
The result by Anderson \yrcite{SDE-reverse} implies that if -dimensional process of the diffusion type  satisfies



where  is a function , then its reverse-time dynamics is given by



where  is the probability density function of random variable  and  is the reverse-time standard Brownian motion such that  is independent of its past increments  for . Reverse-time dynamics means that all the integrals associated with reverse-time differentials have  as their lower limit (e.g.  relates to ). Anderson's result is obtained under the assumption that Kolmogorov equations (for probability density functions) associated with all considered processes have unique smooth solutions. On the other hand, Song et al. \yrcite{SDE-main} argued that SDE (\ref{fwd}) has the same forward Kolmogorov equation as the following ODE:



which means that processes following (\ref{fwd}) and (\ref{det}) are equal in distribution if they start from the same initial distribution . In our case  and , so we have two equivalent reverse diffusion dynamics:



and



where both differential equations are to be solved backwards.






\subsection{Score estimation}
\label{app:score_estimation}

If  is known, then (\ref{dist}) implies that



where  is the probability density function of conditional distribution . So, if we sample  by the formula  where , then . In the simplified case when  we have  where . In this case gradient of noisy data log-density reduces to . If , then we have



\end{document}