\documentclass{article}

\usepackage{xspace}
\usepackage[dvipsnames]{xcolor}
\usepackage{pifont}
\newcommand{\cmark}{\textcolor[HTML]{59a14f}{\ding{51}}}\newcommand{\xmark}{\textcolor[HTML]{e15759}{\ding{55}}}

\usepackage{todonotes}
\usepackage[final]{corl_2022} 

\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{enumitem}

\usepackage{wrapfig}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{vcell}

\usepackage[font=scriptsize,labelfont=sl,labelsep=period]{caption}



\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\appsecref}[1]{Appendix~\ref{#1}}

\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\appfigref}[1]{Appendix Figure~\ref{#1}}
\newcommand{\subfig}[1]{\textit{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\apptabref}[1]{Appendix Table~\ref{#1}}
\newcommand{\taskname}[1]{{\small #1}}

\newcommand{\ie}{\textrm{i.e.,}\xspace}
\newcommand{\eg}{\textrm{e.g.,}\xspace}
\newcommand{\etc}{\textrm{etc.}}
\newcommand{\etal}{\textrm{et~al.}}

\DeclareMathOperator*{\argmax}{\arg\!\max}
\newcommand{\inputvl}{}

\let\ph\phantom\newcommand{\transportermodel}{\transporter-only}

\definecolor{mintgreen}{RGB}{202,255,202}
\definecolor{titanwhite}{RGB}{238,238,255}

\usepackage{todonotes}


\newcommand{\transporter}{Transporter}
\newcommand{\transporterabbrev}{TN}
\newcommand{\semantic}{\textbf{semantic}}
\newcommand{\spatial}{\textbf{spatial}}
\newcommand{\observation}{\bm{\gamma}}
\newcommand{\tpick}{\mathcal{T}_\text{pick}}
\newcommand{\tplace}{\mathcal{T}_\text{place}}

\newcommand{\seen}[1]{\textcolor{blue}{#1}}
\newcommand{\unseen}[1]{\textcolor{red}{#1}}


\newcommand{\bcz}{Image-BC~}
\newcommand{\bczcnn}{Image-BC (CNN)~}
\newcommand{\bczvit}{Image-BC (ViT)~}
\newcommand{\unet}{C2FARM-BC~}

\newcommand{\highlight}[1]{\textcolor{black}{#1}}

\newcommand{\model}{\textsc{PerAct}}
\newcommand{\modelfullns}{\textsc{Perceiver-Actor}}
\newcommand{\modelns}{\textsc{PerAct}}
\newcommand{\modelfull}{\textsc{\modelfullns~}}

\newcommand{\blank}{\rule{0.3cm}{0.25mm}~}

\vspace{-1cm}
\title{\modelfullns: \\ A Multi-Task Transformer for Robotic Manipulation}



\vspace{-2cm}
\author{ Mohit Shridhar~\thanks{Work done partly while the author was a part-time intern at NVIDIA.}\hspace{8px} Lucas Manuelli~ \hspace{4px}  Dieter Fox~\\
University of Washington \hspace{6px} NVIDIA\\
\tt\small mshr@cs.washington.edu \hspace{2px} lmanuelli@nvidia.com \hspace{2px} fox@cs.washington.edu\
\mathcal{T}_{\textrm{trans}} = \underset{(x,y,z)}{\argmax} \  \mathcal{Q}_{\textrm{trans}}((x,y,z) \ | \ \mathbf{v}, \mathbf{l} \,), \hspace{1cm}  &
\mathcal{T}_{\textrm{rot}} = \underset{(\psi,\theta,\phi)}{\argmax} \  \mathcal{Q}_{\textrm{rot}}((\psi,\theta,\phi) \ | \ \mathbf{v}, \mathbf{l} \,), \\
\mathcal{T}_{\textrm{open}} = \underset{\omega}{\argmax} \  \mathcal{Q}_{\textrm{open}}(\, \omega \ | \ \mathbf{v}, \mathbf{l} \,), \hspace{1cm}  &
\mathcal{T}_{\textrm{collide}} = \underset{\kappa}{\argmax} \ \mathcal{Q}_{\textrm{collide}}(\, \kappa \ | \ \mathbf{v}, \mathbf{l} \,),

    \mathcal{L}_{\textrm{total}} = - \mathbb{E}_{Y_{\textrm{trans}}}[\textrm{log} \mathcal{V}_{\textrm{trans}}] - \mathbb{E}_{Y_{\textrm{rot}}}[\textrm{log} \mathcal{V}_{\textrm{rot}}] - \mathbb{E}_{Y_{\textrm{open}}}[\textrm{log} \mathcal{V}_{\textrm{open}}] -
    \mathbb{E}_{Y_{\textrm{collide}}}[\textrm{log} \mathcal{V}_{\textrm{collide}}],
-13pt]                                                         \\
\vcell{Method}    & \vcell{10}       & \vcell{100}                                                & \vcell{10}       & \vcell{100}                                                      & \vcell{10}       & \vcell{100}                                                  & \vcell{10}       & \vcell{100}                                                 & \vcell{10}       & \vcell{100}                                           & \vcell{10}       & \vcell{100}                                                         & \vcell{10}       & \vcell{100}                                            & \vcell{10}       & \vcell{100}                                                  & \vcell{10}       & \vcell{100}                                             \0pt] 
\hline \1pt]  
\hline \\
                  & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{screw}\\\texttt{bulb}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{put in}\\\texttt{safe}\end{tabular}}      & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{place}\\\texttt{wine}\end{tabular}} & 
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{put in}\\\texttt{cupboard}\end{tabular}} &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{sort}\\\texttt{shape}\end{tabular}} & 
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{push}\\\texttt{buttons}\end{tabular}}& \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{insert}\\\texttt{peg}\end{tabular}} &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{stack}\\\texttt{cups}\end{tabular}}         & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{place}\\\texttt{cups}\end{tabular}}    \\
        
                  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
                  \cmidrule(lr){14-15} \cmidrule(lr){16-17}
                  \cmidrule(lr){18-19}
                  \-\rowheight]
\printcellbottom  & \printcellbottom & \printcellbottom                                           & \printcellbottom & \printcellbottom                                                 & \printcellbottom & \printcellbottom                                             & \printcellbottom & \printcellbottom                                            & \printcellbottom & \printcellbottom                                      & \printcellbottom & \printcellbottom                                                    & \printcellbottom & \printcellbottom                                       & \printcellbottom & \printcellbottom                                             & \printcellbottom & \printcellbottom                                        \-6pt]
\bczcnn          & 0                    & 0                                                      & 0                    & 4                                                            & 0                    & 0                                                        & 0                    & 0                                                          & 0                    & 0                                                & 4                    & 0                                                        & 0                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\highlight{\bczvit}   & 0                    & 0                                                  & 0                    & 0                                                  & 4                    & 0                                                       & 4                    & 0                                                     & 0                    & 0                                                & 16                   & 0                                                    & 0                    & 0                                                & 0                    & 0                                                 & 0                    & 0                                                   \\
\unet             & 12                   & 8                                                      & 0                    & 12                                                           & \textbf{36}          & 8                                                        & \textbf{4}           & 0                                                          & 8           & 8                                                & \textbf{88}          & \textbf{72}                                              & 0                    & \textbf{4}                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\model~(w/o Lang) & 0                    & \textbf{24}                                            & 8                    & 20                                                           & 8                    & \textbf{20}                                                       & 0                    & 0                                                          & 0                    & 0                                                & 60                   & 68                                                       & 4                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\rowcolor[rgb]{0.9,1.0,0.9}\model~ & \textbf{28}          & \textbf{24}                                            & \textbf{16}          & \textbf{44}                                                  & 20                   & 12                                                       & 0                    & \textbf{16}                                                & \textbf{16}          & \textbf{20}                                      & 56                   & 48                                                       & \textbf{4}           & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \-4pt]
\model~                & 80                                                    & 72                                                    & 56                                                         & 84                                                       & 80                                                  & 68                                                      & 60                                                  & 68                                                   & 36                                                     \\& \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                       & \multicolumn{1}{l}{}                                     & \multicolumn{1}{l}{}                                & \multicolumn{1}{l}{} \-0.6em]
\rowcolor[rgb]{0.9,1.0,0.9} \model~  latents & 84                                                    & 88                                                    & 44                                                         & 68                                                       & 84                                                  & 48                                                      & 48                                                  & 84                                                   & 12                                                     \\
\rowcolor[rgb]{0.9,1.0,0.9} \model~  latents & 84                                                    & 48                                                    & 52                                                         & 84                                                       & 84                                                  & 52                                                      & 32                                                  & 92                                                   & 12                                                     \\
\rowcolor[rgb]{0.9,1.0,0.9} \model~  latents  & 92                                                    & 84                                                    & 48                                                         & 100                                                      & 92                                                  & 32                                                      & 32                                                  & 100                                                  & 20                                                     \\
& \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                       & \multicolumn{1}{l}{}                                     & \multicolumn{1}{l}{}                                & \multicolumn{1}{l}{} \-0.6em]
\rowcolor[rgb]{1.0,1.0,0.90} \model~  patches    & 72                                                    & 48                                                    & 96                                                         & 92                                                       & 76                                                  & 76                                                      & 36                                                  & 96                                                   & 32                                                     \\
\rowcolor[rgb]{1.0,1.0,0.90} \model~  patches   &  68                                                    & 64                                                    & 56                                                         & 52                                                       & 96                                                  & 56                                                      & 36                                                  & 92                                                   & 20                                                     \4pt]
\cline{1-10} \-0.6em]
\rowcolor[rgb]{0.898,1.0,0.953}\model~ w/o Rot Aug  & 20                                                    & 32                                                    & 48                                                         & 8                                                        & 8                                                   & 56                                                      & 8                                                   & 4                                                    & 0                                                      \\& \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                       & \multicolumn{1}{l}{}                                     & \multicolumn{1}{l}{}                                & \multicolumn{1}{l}{} \-0.6em]
\rowcolor[rgb]{0.96,0.96,1} \model~  voxels    & 24                                                    & 48                                                    & 44                                                         & 12                                                       & 4                                                   & 32                                                      & 0                                                   & 4                                                    & 0                                                      \\
\rowcolor[rgb]{0.96,0.96,1} \model~  voxels    & 12                                                    & 20                                                    & 52                                                         & 0                                                        & 0                                                   & 60                                                      & 0                                                   & 0                                                    & 0                                                      \\& \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                  & \multicolumn{1}{l}{}                                       & \multicolumn{1}{l}{}                                     & \multicolumn{1}{l}{}                                & \multicolumn{1}{l}{} \-0.6em]
\rowcolor[rgb]{1.0,1.0,0.90} \model~  patches    & 8                                                     & 48                                                    & 76                                                         & 0                                                        & 12                                                  & 16                                                      & 0                                                   & 0                                                    & 0                                                      \\
\rowcolor[rgb]{1.0,1.0,0.90} \model~  patches    & 12                                                    & 36                                                    & 72                                                         & 12                                                       & 0                                                   & 20                                                      & 0                                                   & 0                                                    & 0                                                      \\
\bottomrule
\end{tabular}
\end{table} 
In \tabref{table:sen_analysis}, we investigate three factors that affect \model's performance: rotation data augmentation, number of Perceiver latents, and voxelization resolution. All multi-task agents were trained with 100 demonstrations per task and evaluated on 25 episodes per task. To briefly summarize these results: (1)  yaw perturbations improve performance on tasks with lots of rotation variations like \texttt{stack blocks}, but also worsen performance on tasks with constrained rotations like \texttt{place wine}. (2) \model~with just  latents is competitive with (and sometimes even better than) the default agent with  latents, which showcases the compression capability of the Perceiver architecture. (3) Coarse grids like  are sufficient for some tasks, but  high-precision tasks like \texttt{sort shape} need higher resolution voxelization. (4) Large patch-sizes reduce memory usage, but they might affect tasks that need sub-patch precision. 




\section{High-Precision Tasks}
\label{app:high_pres}

\begin{wraptable}{r}{0.3\textwidth}
\vspace{-0.89cm}
\centering
\scriptsize
\begin{tabular}{lcc} 
\toprule
              & Multi & Single  \\ 
\midrule
\texttt{place cups}    & 0     & 24     \\
\texttt{stack cups}    & 0     & 32      \\
\texttt{insert peg} & 0     & 16      \\
\bottomrule
\end{tabular}
\label{table:high_pres_res}
\caption{\scriptsize{Success rates (mean \%) of multi-task and single-task \model~agents trained with 100 demos and evaluated on 25 episodes.}}
\end{wraptable}
 In \tabref{table:rlbench}, \model~achieves zero performance on three high-precision tasks: \texttt{place cups}, \texttt{stack cups}, and \texttt{insert peg}. To investigate if multi-task optimization is itself one of the factors affecting performance, we train 3 separate single-task agents for each task. We find that single-task agents are able to achieve non-zero performance, indicating that better multi-task optimization methods might  improve performance on certain tasks. 

\section{Additional Related Work}

In this section, we briefly discuss additional works that were not mentioned in \secref{sec:related_work}.

\highlight{\textbf{Concurrent Work.} Recently, Mandi et al.~\citep{mandi2022effectiveness} found that pre-training and fine-tuning on new tasks is competitive, or even better, than meta-learning approaches for RLBench tasks in multi-task (but single-variation) settings. This pre-training and fine-tuning paradigm might be directly applicable to~\model, where a pre-trained \model ~agent could be quickly adapted to new tasks without the explicit use of meta-learning algorithms.}

\highlight{\textbf{Multi-Task Learning.} In the context of RLBench, Auto-~\citep{liu2022auto_lambda} presents a multi-task optimization framework that goes beyond uniform task weighting from \secref{sec:training}. The method dynamically tunes task weights based on the validation loss. Future works with \model~could replace uniform task weighting with Auto- for better multi-task performance. In the context of Meta-World~\citep{yu2020meta}, Sodhani et al.~\citep{pmlr-v139-sodhani21a} found that language-conditioning leads to performance gains for multi-task RL on 50 task variations.}

\highlight{\textbf{Language-based Planning.} In this paper, we only investigated single-goal settings where the language instruction does not change throughout the episode. However, language-conditioning natural allows for composing several instructions in a sequential manner~\citep{lynch2020grounding}. As such, several prior  works~\citep{ALFWorld20,ahn2022can,zeng2022socratic,huang2022language} have used language as medium for planning high-level actions, which can then be executed with pre-trained low-level skills. Future works could incorporate language-based planning for grounding more abstract goals like \textit{``make dinner''}.}

\textbf{Task and Motion Planning.} In the sub-field of Task and Motion Planning (TAMP)~\citep{kaelbling2013integrated,garrett2021integrated}, Konidaris \etal~\citep{konidaris2018skills} present an action-centric approach to symbolic planning. Given a set of predefined action-skills, an agent interacts with its environment to construct a set of symbols, which can then be used for planning.

\textbf{Voxel Representations.} Voxel-based representations have been used in several domains that specifically benefit from 3D understanding.
Like in object detection~\citep{mao2021voxel,he2022voxel}, object search~\citep{zheng2022towards}, and vision-language grounding~\citep{blukis2022persistent,corona-etal-2022-voxel}, voxel maps have been used to build persistent scene representations~\citep{sitzmann2019deepvoxels}.
In Neural Radiance Fields (NeRFs), voxel feature grids have dramatically reduced training and rendering times~\citep{mueller2022instant,yu_and_fridovichkeil2021plenoxels}. Similarly, other works in robotics have used voxelized representations to embed viewpoint-invariance for driving~\citep{lal2021coconets} and manipulation~\citep{tung20203d}. The use of latent vectors in Perceiver~\citep{jaegle2021perceiver} is broadly related to voxel hashing~\citep{niessner2013real} from computer graphics. Instead of using a location-based hashing function to map voxels to fixed size memory, PerceiverIO uses cross attention to map the input to fixed size latent vectors, which are trained end-to-end. Another major difference is the treatment of unoccupied space. In graphics, unoccupied space does not affect  rendering, but in \model, unoccupied space is where a lot of ``action detections'' happen. Thus the relationship between unoccupied and occupied space, \ie scene, objects, robot, is crucial for learning action representations.

\textbf{Long-Context and Latent-Space Transformers.} Several approaches have been proposed for extending Transformers to longer context lengths~\citep{tay2020efficient}. Latent-space Transformers that use fixed-size latents instead of the full context, are one such approach~\citep{jaegle2021perceiver,goyal2021coordination}. There is no clear winner in terms of trade-offs between speed, memory, and performance. However, latent-space methods have achieved compelling results in object detection~\citep{carion2020end} and slot-attention based object discovery~\citep{locatello2020object}.

\section{Additional Q-Prediction Examples} \label{app:more_qpred}
\figref{fig:qpred_extra} showcases additional -prediction examples from trained \model~agents. Traditional object-centric representations like poses and instance-segmentations struggle to represent piles of beans or tomato vines with high-precision. Whereas action-centric agents like \model~focus on learning perceptual representations of actions, which elevates the need for practitioners to define \textit{what should be an object} (which is a harder problem and often specific to tasks and embodiments). 

\begin{figure*}[h]
    \centering
    \hspace*{-3.1cm}
    \includegraphics[width=1.4\textwidth]{figures/q_pred_extra_v1.pdf}
    \caption{\textbf{Additional Q-Prediction Examples.} Translation -Prediction examples from \model. The top two rows are from simulated tasks without any data augmentation perturbations, and the bottom row is from real-world tasks  with translation and yaw-rotation perturbations.}
    \label{fig:qpred_extra}
\end{figure*}
\newpage
\section{Things that did not work}
\vspace{-0.2cm}
In this section, we describe things we tried, but did not work or caused issues in practice. 

\textbf{Real-world multi-camera setup.} We tried setting up multiple Kinect-2s for real-world multi-view observations, but we could not solve interference issues with multiple Time-of-Flight sensors. Particularly, the depth frames became very noisy and had lots of holes. Future works could try turning the cameras on-and-off in a rapid sequence, or use better Time-of-Flight cameras with minimal interference. 

\textbf{Fourier features for positional embeddings.} Instead of the learned positional embeddings, we also experimented with concatenating Fourier features to the input sequence like in some Perceiver models~\citep{jaegle2021perceiver}. The Fourier features led to substantially worse performance. 

\textbf{Pre-trained vision features.} Following CLIPort~\citep{cliport}, we tried using pre-trained vision features from CLIP~\citep{radfordLearningTransferableVisual2021}, instead of raw RGB values, to bootstrap learning and also to improve generalization to unseen objects. We ran CLIP's ResNet50 on each of the 4 RGB frames, and upsampled features with shared  decoder layers in a UNet fashion. But we found this to be extremely slow, especially since the ResNet50 and decoder layers need to be run on 4 independent RGB frames. With this additional overhead, training multi-task agents would have taken substantially longer than 16 days. Future works could experiment with methods for pre-training the decoder layers on auxiliary tasks, and pre-extracting features for faster training. 

\textbf{Upsampling at multiple self-attention layers.} Inspired by Dense Prediction Transformers (DPT)~\citep{ranftl2021vision}, we tried upsampling features at multiple self-attention layers in the Perceiver Transformer. But this did not work at all; perhaps the latent-space self-attention layers of Perceiver are substantially different to the full-input self-attention layers of ViT~\citep{dosovitskiy2020image} and DPT~\citep{ranftl2021vision}.

\textbf{Extreme rotation augmentation.} In addition to yaw rotation perturbations, we also tried perturbing the pitch and roll. While \model~was still able to learn policies, it took substantially longer to train. It is also unclear if the default latent size of  is appropriate for learning 6-DoF polices with such extreme rotation perturbations. 

\textbf{Using Adam instead of LAMB.} We tried training \model~with the Adam~\citep{kingma2014adam} optimizer instead of LAMB~\citep{you2019large}, but this led to worse performance in both simulated and real-world experiments. 
\vspace{-0.1cm}
\section{Limitations and Risks} \label{app:limitations}
\vspace{-0.1cm}
While \model~is quite capable, it is not without limitations. In the following sections, we discuss some of these limitations and potential risks for real-world deployment. 

\textbf{Sampling-Based Motion Planner.} \model~relies on a sampling-based motion planner to execute discretized actions. This puts \model~at the mercy of randomized planner to reach poses. While this issue did not cause any major problems with the tasks in our experiments, a lot of other tasks are sensitive to the paths taken to reach poses. For instance, pouring water into a cup would require a smooth path for tilting the water container appropriately. This could be addressed in future works by using a combination of learned and sampled motion paths~\citep{james2022coarse}.

\textbf{Dynamic Manipulation.} Another issue with discrete-time discretized actions is that they are not easily applicable to dynamic tasks that require real-time closed-loop maneuvering. This could be addressed with a separate visuo-servoing mechanism that can reach target poses with closed-loop control. Alternatively, instead of predicting just one action, \model~could be extended to predict a sequence of discretized actions. Here, the Transformer-based architecture could be particularly advantageous. Also, instead of just predicting poses, the agent could also be trained to predict other physical parameters like target velocities~\citep{zeng2020tossingbot}.

\textbf{Dexterous Manipulation.} Using discretized actions with N-DoF robots like multi-fingered hands is also non-trivial. Specifically for multi-fingered hands, \model~could be modified to predict finger-tip poses that can be reached with an IK (Inverse Kinematics) solver. But it is unclear how feasible or robust such an approach would be with under-actuated systems like multi-fingered hands.   


\begin{figure*}[!t]
    \centering
    \vspace{-1.2cm}
    \includegraphics[width=0.8\textwidth]{figures/robustness.pdf}
    \caption{\textbf{Perturbation Tests.} Results from a multi-task \model~agent trained on a single drawer and evaluated on several instances perturbed drawers. Each perturbation consists of 25 evaluation episodes, and reported successes are relative to the training drawer.}
    \label{fig:robustness}
    \vspace{-1.5em}
\end{figure*}
\textbf{Generlization to Novel Instances and Objects.} 
\highlight{In \figref{fig:robustness}, we report results from small-scale perturbation experiments on the \texttt{open drawer} task. We observe that changing the shape of the handles does not affect performance. However, handles with randomized textures and colors confuse the agent since it has only seen one type of drawer color and texture during training.
Going beyond this one-shot setting, and training on several instances of drawers might improve generalization performance.}
Although we did not explicitly study generalization to unseen objects, it might be feasible to train \model's action-detector on a broad range of objects and evaluate its ability to handle novel objects, akin to how language-conditioned instance-segmentors and object-detectors are used~\citep{kamath2021mdetr}.
Alternatively, pre-trained vision features from multi-modal encoders like CLIP~\citep{radfordLearningTransferableVisual2021} or R3M~\citep{nair2022r3m} could be used to boostrap learning. 

\textbf{Scope of Language Grounding.} Like with prior work~\citep{cliport}, \model's understanding of verb-noun phrases is closely grounded in demonstrations and tasks. For example, ``cleaning'' in \textit{``clean the beans on the table with a dustpan''} is specifically associated with the action sequence of pushing beans on to a dustpan, and not ``cleaning'' in general, which could be applied to other tasks like cleaning the table with a cloth.

\textbf{Predicting Task Completion.} For both real-world and simulated evaluations, an oracle indicates whether the desired goal has been reached. This oracle could be replaced with a success classifier that can be pre-trained to predict task completion from RGB-D observations. 

\textbf{History and Partial Observability.} \model~relies purely on the current observation to predict the next action. As such, tasks that require history like counting or ordering are not feasible, unless accompanied by a task-completion predictor. Similarly, for tasks involving partial observability \eg~looking through drawers one-by-one for a specific object, \model~does not keep  track of what was seen before. Future works could include observations from previous timesteps, or append Perceiver latents, or train a Recurrent Neural Network to encode latents across timesteps.

\textbf{Data Augmentation with Kinematic Feasibility.} The data augmentation method described in \secref{app:data_aug} does not consider the kinematic feasibility of reaching perturbed actions with the Franka arm. Future works could pre-compute unreachable poses in the discretized action space, and discard any augmentation perturbations that push actions into unreachable zones.  

\textbf{Balanced Datasets.} Since \model~is trained with just a few demonstrations, it occassionally tends to exploit biases in the training data. For instance, \model~might have a tendency to always \textit{``place blue blocks on yellow blocks''} if such an example is over-represented in the training data. Such issues could be potentially fixed by scaling datasets to include more diverse examples of objects and attributes. Additionally, data visualization methods could be used to identify and fix these biases. 

\highlight{\textbf{Multi-Task Optimization.} The uniform  task sampling strategy presented in \secref{sec:training} might sometimes hurt performance. Since all tasks are weighted equally, optimizing for certain tasks with common elements (\eg~moving blocks), might adversarial affect the performance on other dissimilar tasks (\eg~turning taps). Future works, could use dynamic task-weighting methods like Auto-~\citep{liu2022auto_lambda} for better multi-task optimization.}


\textbf{Deployment Risks.} \model~is an end-to-end framework for 6-DoF manipulation. Unlike some methods in Task-and-Motion-Planning  that can sometimes provide theoretical guarantees on task completion, \model~is a purely reactive system whose performance can only be evaluated through empirical means. Also, unlike prior works~\citep{cliport}, we do not use internet pre-trained vision encoders that might contain harmful biases~\citep{birhane2021multimodal,bender2021dangers}. Even so, it is prudent to thoroughly study and mitigate any biases before deployment. As such, for real-world applications, keeping humans in the loop both during training and testing, might help. Usage with unseen objects and observations with people is not recommended for safety critical systems.



\section{Emergent Properties}

In this section, we present some preliminary  findings on the emergent properties of \model.

\begin{wrapfigure}{r}{0.40\textwidth}
  \vspace{-1.3cm}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figures/tracking_v1.png}
  \caption{\textbf{Object Tracker.} Tracking  an unseen hand sanitizer instance.}
  \label{fig:tracking}
  \end{center}
\end{wrapfigure}
\subsection{Object Tracking}
Although \model~was not explicitly trained for 6-DoF object-tracking, our action detection framework can be used to localize objects in cluttered scenes. In this \href{https://peract.github.io/media/results/animations/handsan_tracking_v2.mp4}{video}, we show  an agent that was trained with one hand sanitizer instance on just 5 ``press the handsan'' demos, and then evaluated on tracking an unseen sanitizer instance. \model~does not need to build a complete representation of hand sanitizers, and only has to learn \textit{where to press} them. Our implementation runs at an inference speed of 2.23 FPS (or 0.45 seconds per frame), allowing for near real-time closed-loop behaviors. 

\begin{wrapfigure}{r}{0.40\textwidth}
  \vspace{-2.9cm}
  \begin{center}
    \includegraphics[width=0.31\textwidth]{figures/multimodal_v1.pdf}
  \caption{\textbf{Examples of Multi-Modal Predictions.}}
  \label{fig:multimodal}
  \end{center}
  \vspace{-1cm}
\end{wrapfigure}

\vspace{1cm}
\subsection{Multi-Modal Actions}
\model's problem formulation allows for modeling multi-modal action distributions, \ie scenarios where multiple actions are valid given a specific goal. \figref{fig:multimodal} presents some selected examples of multi-modal action predictions from \model. Since there are several \textit{``yellow blocks''} and \textit{``cups''} to choose from, the -prediction distributions have several modes. In practice, we observe that the agent has a tendency to prefer certain object instances over others (like the front mug in \figref{fig:multimodal}) due to preference biases in the training dataset. We also note that the cross-entropy based training method from \secref{sec:training} is closely related to Energy-Based Models (EBMs)~\citep{lecun2006tutorial,florence2022implicit}. In a way, the cross-entropy loss is \textit{pulling up} expert 6-DoF actions, while \textit{pushing-down} every other action in the discretized action space. At test time, we simply maximize the learned -predictions, instead of minimizing an energy function with optimization. Future works could look into EBM~\citep{florence2022implicit} training and inference methods for better generalization and execution performance.



\end{document}
