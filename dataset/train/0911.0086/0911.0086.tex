
\documentclass{article} \usepackage{fullpage}

\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{color}
\usepackage{enumerate, verbatim}
\usepackage{algorithm,algorithmic} 
\usepackage{amssymb, amsmath, amsthm}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\newcommand{\STAB}{\mathrm{STAB}}
\newcommand{\ent}{H}
\def\marrow{{\marginpar[\hfill]{}}}

\title{Sorting under Partial Information\\
(without the Ellipsoid Algorithm)\footnote{This work was supported by the ``Actions de Recherche Concert\'ees'' (ARC) fund of the ``Communaut\'e fran\c{c}aise de Belgique'', NSERC of Canada, and the Canada Research Chairs Programme. G.J.\ and R.J.\ are Postdoctoral Researchers of the ``Fonds National de la Recherche Scientifique'' (F.R.S.--FNRS). A preliminary version of the work appeared in \cite{SUPI-STOC}.}}

\date{}

\author{Jean Cardinal, Samuel Fiorini, Gwena\"el Joret\footnote{Universit\'e Libre de Bruxelles (ULB), Brussels, Belgium. {E-mail: \tt\small \{jcardin,sfiorini,gjoret\}@ulb.ac.be}.},\\ 
Rapha\"el M. Jungers\footnote{Universit\'e Catholique de Louvain (UCL), Louvain-La-Neuve, Belgium. {E-mail: \tt\small raphael.jungers@uclouvain.be}}, J. Ian Munro\footnote{University of Waterloo, Waterloo, Ontario, Canada. {E-mail: \tt\small imunro@uwaterloo.ca}}
}

\begin{document}

\maketitle

\sloppy

\begin{abstract}
We revisit the well-known problem of sorting under partial information: sort a finite set given the outcomes of comparisons between some pairs of elements. The input is a partially ordered set , and solving the problem amounts to discovering an unknown linear extension of , using pairwise comparisons. The information-theoretic lower bound on the number of comparisons needed in the worst case is , the binary logarithm of the number of linear extensions of . In a breakthrough paper, Jeff Kahn and Jeong Han Kim ({\em J.\ Comput.\ System Sci.\ 51 (3), 390--399, 1995}) showed that there exists a polynomial-time algorithm for the problem achieving this bound up to a constant factor. Their algorithm invokes the ellipsoid algorithm at each iteration for determining the next comparison, making it impractical.

We develop efficient algorithms for sorting under partial information. Like Kahn and Kim, our approach relies on graph entropy. However, our algorithms differ in essential ways from theirs. Rather than resorting to convex programming for computing the entropy, we approximate the entropy, or make sure it is computed only once, in a restricted class of graphs, permitting the use of a simpler algorithm. Specifically, we present:
\begin{enumerate}
\item an  algorithm performing  comparisons;
\item an  algorithm performing at most  comparisons;
\item an  algorithm performing  comparisons.
\end{enumerate}
All our algorithms can be implemented in such a way that their computational bottleneck is confined in a preprocessing phase, while the sorting phase is completed in  time, where  denotes the number of comparisons performed.
\end{abstract}

\newpage
\section{Introduction}

\paragraph*{Problem Definition}

We consider the following problem:\medskip

{\it Let  be a set equipped with an unknown linear order . Given a subset of the relations , determine the complete linear order by queries of the form: ``is ?''.}\medskip

\begin{figure}
\begin{center}
\includegraphics[scale=.5]{example.pdf}
\end{center}
\caption{\label{fig:example}An instance of the problem of sorting under partial information. In this example, we use 4 comparisons (dashed edges). At every step, the Hasse diagram of the currently known partial order is shown.}
\end{figure}

This problem is called {\it Sorting under Partial Information}. We are given the outcomes of a number of comparisons between elements of a linearly ordered set, and we wish to ``complete the sort'' by performing more comparisons. The partially ordered set (poset)  encoding these known outcomes is a partial information that should help reducing the number of comparisons performed. Denoting by  the number of linear extensions of , it is obvious that the number of required comparisons is at least  in the worst case\footnote{Throughout the paper,  denotes the binary logarithm of .}. An example is given in Figure~\ref{fig:example}.

\paragraph*{Previous Results}

The problem was first posed by Fredman~\cite{F76}. He showed that there exists an algorithm that performs  additional comparisons between elements of . However, the number of comparisons performed by Fredman's algorithm is not  when  is sub-linear, and deciding what comparisons should be done takes super-polynomial time. At that time, it remained open whether there existed, on the one hand, an algorithm performing  comparisons, and, on the other hand, an algorithm running in polynomial time.

The first question was answered by Kahn and Saks~\cite{KS84j}. They showed that there always exists a query of the form ``is ?'' such that the fraction of linear extensions in which  is smaller than  lies in the interval . This is a relaxation of the well-known -- conjecture, a conjecture formulated independently by Fredman, Linial, and Stanley, see \cite{L84}. A simpler proof yielding weaker bounds was given by Kahn and Linial~\cite{KL91}. Better bounds were later given by Brightwell, Felsner, and Trotter~\cite{BFT95}, and Brightwell~\cite{B99}. Iteratively choosing such a comparison yields an algorithm that performs  comparisons. However, finding the right comparisons remained intractable.

In 1995, Kahn and Kim published a breakthrough paper~\cite{KK95} in which they describe a polynomial-time algorithm performing  comparisons, thus answering both questions positively. Their key insight is to relate  to the entropy of the incomparability graph of , a quantity that can be computed in polynomial time. Their algorithm, although polynomial, is still far from practical because it uses the ellipsoid algorithm  times to determine the comparisons.

\paragraph*{Contribution}

Our results are summarized in Table \ref{tab:results} below. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Algorithm & Global complexity & Number of comparisons\\
\hline
\hline
\cite{KK95} & & \\
\hline
\hline
{\bf Algorithm 1} &  & \\
\hline
{\bf Algorithm 2} &  & \\
\hline
{\bf Algorithm 3} &  & \\
\hline
\end{tabular}
\end{center}
\label{tab:results}
\caption{We denote by  the time needed for the ellipsoid algorithm to compute the entropy of a poset of order . The original bound given by Kahn and Kim on the number of comparisons performed by their algorithm is . The improved bound given in the table is a byproduct of our results. (The notation  means that the hidden constant may depend on .)
}
\end{table}

We now compare these results to those of Kahn and Kim (denoted: K\&K). In terms of global complexity, each of our algorithms greatly improves over that of K\&K. Furthermore:

\begin{itemize}
\item If  is super-linear in , the number of comparisons of our second algorithm is lower than that of K\&K. By optimizing over , it can be shown that the number of comparisons is actually  in this case, a number of comparisons comparable to that of Fredman's algorithm. 

\item If  is linear or sub-linear in , the number of comparisons of our third algorithm is comparable to that of K\&K, although the constant in front of  is still far from the best constant achieved by a super-polynomial algorithm via balancing pairs \cite{BFT95,B99}.

\item Our algorithms have the following useful property: they compute information that guides the sorting and can then be reused to solve any given instance with the same partial information , in time proportional to the number of comparisons, plus a term linear in .
\end{itemize}

Finally, note that randomized algorithms for sorting under partial information can be derived from random linear extension generation algorithms. The idea here would be to estimate the efficiency of a comparison -- that is, the fraction of linear extensions remaining after some query ``is ?'' is performed -- arbitrarily closely by testing a sufficiently large random sample of linear extensions. However, the running time of such an algorithm would be much higher than the ones we propose here. For instance the recent sampling algorithm from Huber~\cite{H06}, has expected running time , and this sampling step has to be performed a large number of times.

\paragraph*{Outline and Key Ideas}

K\&K showed that graph entropy, as defined by K\"orner~\cite{K73}, is a useful tool in the problem of sorting under partial information. Letting  be the entropy of the incomparability graph of , they showed that . Every comparison performed by their algorithm decreases  by at least some constant. Hence the total number of comparisons is  and thus . Furthermore, their algorithm is polynomial, because the entropy can be computed in polynomial time using convex programming.

Our goal is to obtain practical algorithms, without sacrificing the number of comparisons. Our first key idea is to compute a {\sl greedy chain decomposition\/} of , that is, a partition of  into chains (totally ordered subsets), obtained by iteratively extracting a longest chain. This allows us to get rid of the costly convex programming machinery and enables us to focus only on the relevant part of . In \cite{POP_SICOMP}, we have provided bounds on the amount of information (in terms of entropy) that is lost when we forget the relations of  between two distinct chains of a greedy chain decomposition.

As a warmup, we first describe Algorithm 1, an insertion sort-like algorithm. Then we describe Algorithm 2, a mergesort-like algorithm: find a greedy chain decomposition of , and merge the chains using a simple linear-time merging algorithm. The number of comparisons performed by this algorithm can be shown to be close to , up to an arbitrarily small factor and a term linear in . This is described in Section~\ref{sec:merge}.

As noted above, our mergesort-like algorithm performs better than that of K\&K provided the information theoretic lower bound  is super-linear. The algorithms are comparable (in terms of number of comparisons) if  is linear. If  is sub-linear, we have to use another strategy: instead of forgetting all the relations of  between the chains of a greedy chain decomposition, we keep some of them. Namely, we keep all the relations between the elements of the longest chain and the rest of . When  is small compared to , the longest chain contains a large fraction of the elements. Hence, this less radical strategy keeps most of the information contained in .

Our second key idea is contained in the following algorithm: find a longest chain , use the mergesort-like algorithm on , yielding a chain , and cautiously merge the chains  and  using the current partial information. Thus we reduce the general sorting problem to an easier subproblem known as {\sl merging under partial information}. It is a special case of the problem of sorting under partial information in which  can be covered by exactly two chains, and has been studied by Linial~\cite{L84}. By using an algorithm for merging under partial information performing  comparisons, we obtain an algorithm for the general sorting problem performing  comparisons. This is shown in Section \ref{sec:cautious_merge}.

The problem of merging under partial information is tackled in Section~\ref{sec:MUPIsec}. Linial~\cite{L84} already provided an algorithm for the problem, but we develop an alternative solution. We first show that in this special case, the entropy of the incomparability graph of  can be computed very easily. The computation relies on a structural lemma on the entropy of bipartite graphs by K\"orner and Marton~\cite{KM88}, and on the additional structure exhibited by the incomparability graph of a poset covered by two chains.

Then, we show that given the vertex weights achieving the entropy, there exists a sequence of pairwise chain mergings, each of which decreases  by an amount proportional to the number of comparisons performed. After each merging, the weights on the vertices can be updated efficiently. This yields the desired algorithm for merging under partial information, and thus an algorithm for sorting under partial information performing  comparisons. We refer to it as Algorithm 3. The global complexity of Algorithm 3 is .

The plan of the paper is as follows. Preliminaries on complexity measures, the entropy of a graph, and greedy chain decompositions, are given in Section~\ref{sec:prelim}. In Section~\ref{sec:tight_bound}, we offer new results on the entropy, improving several aspects of K\&K's analysis. Mainly, we prove the tight inequality , whereas K\&K show .

As a first simple example of a near-optimal algorithm for sorting under partial information, we describe our (simple) Algorithm~1 in Section~\ref{sec:insertion}. This algorithm has global complexity  and performs a number of comparisons within a  factor only of the information-theoretic lower bound. 

As mentioned above, the mergesort-like algorithm (Algorithm 2) is given in Section~\ref{sec:merge},
while Sections \ref{sec:cautious_merge} and~\ref{sec:MUPIsec} are devoted to
Algorithm 3 performing  comparisons. In the last section, Section~\ref{sec:reuse}, we explain how that algorithm can be implemented in such a way that all costly computations are done in a {\em preprocessing phase}. As a result, the algorithm can reuse the information computed during that preprocessing phase and solve any other instance with the same partial information , in time proportional to the number of comparisons plus a term linear in .

As a final remark, we report an important observation from an anonymous referee concerning Linial's algorithm for merging under partial information~\cite{L84}. Using dynamic programming, it can be shown that this algorithm can be implemented in a way that would be competitive with our proposition. It would not, however, have a sorting phase that is as efficient.

A related note is that the algorithm for merging under partial information given in the preliminary version~\cite{SUPI-STOC} of this paper is slightly different from the one presented here. The resulting new algorithm for sorting under partial information is simpler and can be implemented so that the sorting phase takes  time, where  is the number of comparisons performed by the algorithm. Achieving the latter property was left as an open problem in~\cite{SUPI-STOC}.

We also include an appendix, in which we discuss the complexities of some important steps used in our algorithms, among which is the construction of a greedy chain decomposition.

\paragraph*{Other Related Works}
In 2004, Yao proved that the information-theoretic lower bound for the problem of sorting under partial information also holds for quantum decision trees, up to a term linear in ~\cite{Yao04}. His analysis also relies on the notion of graph entropy.

In a recent paper, Daskalakis et al.~\cite{DKMRV09} analyze the problem of discovering a partial order using comparisons. In that setting, a comparison can have three outcomes, including one stating that the two elements are incomparable, and the goal is to completely identify the underlying partial order. They propose an algorithm performing a number of comparisons that is within a constant factor of the information-theoretic lower bound for partial orders of a given width.

\section{Preliminaries}
\label{sec:prelim}

We give a number of definitions and basic results, and summarize the contribution of Kahn and Kim~\cite{KK95} to the problem. 

\paragraph*{Complexity Measures}

Consider an algorithm for sorting under partial information. The {\sl query complexity\/} is the number of comparisons between elements of  that are done by the algorithm. The {\sl preprocessing complexity\/} measures the computational work done before the first comparison is performed. The rest of the work is measured by the {\sl sorting complexity}. The {\sl preprocessing phase\/} and {\sl sorting phases\/} are defined similarly. Thus, in the preprocessing phase, we are restricted to only process the input poset. The comparisons are performed during the sorting phase. The {\sl global complexity\/} is simply the sum of the preprocessing and sorting complexities.

Our model of computation is a RAM machine with -size words. The global complexity is measured as the total number of arithmetic and logical operations on words.

\paragraph*{Entropy and Sorting}

We recall that a subset  of vertices of a graph is a {\sl stable set\/} (or {\sl independent set}) if the vertices in  are pairwise nonadjacent. The {\sl stable set polytope\/} of a graph  with vertex set  and order  is the -dimensional polytope

where  is the characteristic vector of the subset , assigning the value  to every vertex in , and  to the others. The {\sl entropy\/} of  is defined as (see~\cite{K73,CKLMS90})

Any point  describes a feasible solution of the convex program defined in the right-hand side of \eqref{def-H}. The {\sl entropy\/} of  is the value of the objective function of that program with respect to , which we denote by .

For any given poset , we consider two graphs: the {\sl comparability graph\/}  and the {\sl incomparability graph\/} . The vertex set of  is the ground set of  and two distinct vertices  and  are adjacent in  whenever they are comparable in . The incomparability graph  is simply the complement of . Following K\&K, we denote by  the entropy of  and by  the entropy of .

Entropy plays an important role in the sorting under partial information problem. 

The first reason is explained by the following result due to K\&K. In particular, it implies . Thus the information theoretic lower bound and the entropy of the incomparability graph of  are tightly related.

\begin{lemma}[\cite{KK95}]
\label{lem:kk}
For any poset  of order ,

where .
\end{lemma}

The second reason is that, while computing  is -complete \cite{BW91}, computing  can be done in polynomial time by solving the convex minimization problem (\ref{def-H}), as we now explain. When , the stable set polytope  has a known description in terms of linear inequalities. Although the number of inequalities is (in most cases) exponential, the corresponding separation problem can be solved efficiently. Hence (\ref{def-H}) can be solved by the ellipsoid algorithm. 
(To be precise, the ellipsoid algorithm will actually {\em approximate}
the optimum of (\ref{def-H}) to any fixed precision, in polynomial time.)

Much of this favorable behaviour is due to the perfection of . We recall that a graph  is {\sl perfect\/} if  holds for every induced subgraph  of , where  and  denote the clique and chromatic numbers of , respectively. If  is perfect, then its complement  is also perfect~\cite{L72}. It is known that the comparability graph  of  is perfect, and therefore so is the incomparability graph  of . The latter statement is known as Dilworth's Theorem. The following basic result is a manifestation of convex programming duality (see for instance \cite{S95} for a proof).

\begin{lemma}
\label{lem:perf}
Assume  is a perfect graph with vertex set  and order , and let  and  be feasible solutions to (\ref{def-H}) for  and , respectively. Then  and  are optimal iff  for all . In particular, .
\end{lemma}

Csisz{\'a}r et al.~\cite{CKLMS90} have characterized perfect graphs as the graphs that ``split graph entropy''. More precisely, they proved that  is perfect if and only if, for {\em every\/} probability distribution  on the vertex set of , the sum of the entropies of  and  with respect to  (see the references for a precise definition of this) equals the (Shannon) entropy of .

The algorithm of Kahn and Kim~\cite{KK95} is based on two main lemmas, Lemma \ref{lem:kk} above and the next lemma. Whenever  and  are incomparable elements of , we denote by  the poset obtained by adding the relation  to the partial order of  and then closing transitively.

\begin{lemma}[\cite{KK95}]
\label{lem:kk2}
In any poset  of order  that is not a chain there are ,  incomparable such that

where .
\end{lemma}

\paragraph*{The Algorithm of K\&K and its Complexity}

Let  denote the ground set of . Given an optimal solution  to (\ref{def-H}) for , K\&K show how to choose a pair ,  as in Lemma \ref{lem:kk2}. Knowing the {\sl primal\/} solution , this choice can be done efficiently (in  time).


Comparing  and  gives a new partial information . The key is that for any outcome, . This is proved by modifying appropriately an optimal {\sl dual\/} solution, that is, an optimal solution  to (\ref{def-H}) for . By Lemma \ref{lem:perf},  for all . Knowing , a new dual solution  can be efficiently constructed (in  time).

To determine the next comparison, the K\&K algorithm needs to compute an optimal solution  to (\ref{def-H}) for . Because the optimality of  is not guaranteed, letting  for  does not work. This explains why their algorithm uses the ellipsoid algorithm before each comparison.

We have shown in \cite{POP_SICOMP} that  can be expressed via a convex minimization problem with  variables and at most  constraints, making possible the use of interior point algorithms for computing  (this alternative formulation is described in Section~\ref{sec:tight_bound}).
Although this makes the K\&K algorithm more practical, this does not make it competitive with our algorithms in terms of running time since it is unlikely that computing  using interior point algorithms can be done in less than  time (plugging in in a straightforward way the number of variables and constraints in complexity bounds for interior point algorithms would yield a  complexity \cite{BV04}).

\paragraph*{Greedy Chain Decompositions}

Suppose we want to approximate the entropy  of a given perfect graph . 
We have shown \cite{POP_SICOMP} that the following greedy heuristic performs very well. First, iteratively remove a maximum stable set in . Denote by , \ldots,  the stable sets extracted from . Second, construct the {\sl greedy point\/}
 
in . The entropy of this point is

Note that this is precisely the entropy of the probability distribution .

\begin{theorem}[\cite{POP_SICOMP}]
\label{thm:greedy}
Let  be a perfect graph on  vertices and let  be an arbitrary greedy point in . 
Then, for every ,

\end{theorem}

In the context of the sorting under partial information problem, we apply the greedy heuristic to . This gives a decomposition of  into chains , \ldots,  that we call a {\sl greedy chain decomposition}. Although the fastest known algorithm for computing a maximum chain in a poset of order  has complexity  (see \cite{Gbook}, Chapter 5), a greedy chain decomposition can be found in  time, see Appendix~\ref{app:greedy}.

\section{A Tight Bound on the Entropy of an Incomparability Graph}
\label{sec:tight_bound}

K\&K conjectured that the value for the constant  in Lemma~\ref{lem:kk} could be improved to . We show that one can actually take , which is best possible, as shown by the poset consisting of two incomparable elements.

\begin{theorem}
\label{th:c1}
For any poset  of order ,

\end{theorem}

Before proving this result, we give an equivalent definition of the entropy of a poset in terms of consistent collections of intervals, that is used crucially in our proof of Theorem~\ref{th:c1}.

We say that a collection of open intervals , each of which is contained in the interval , is {\sl consistent\/} with  if  implies that the interval for  is entirely to the left of the interval for , that is, . We denote  the set of all such collections of intervals.

As is easily seen \cite{POP_SICOMP},  equals the minimum of 

over all vectors  such that there exists a collection of intervals in  where, for each , the interval for  has length . In other words, the following lemma holds.

\begin{lemma}
\label{lem:intH}
Let  be a poset of order  with ground set . Then, we have

\end{lemma}

This new definition of the entropy of a poset has some advantages.

First, it yields a convex program with  variables and at most  constraints for computing the entropy. This shows that the entropy of a poset can be computed with interior point algorithms.

Second, it gives a more intuitive framework to reason about the entropy of a poset. As an illustration we sketch short proofs of two results by Kahn and Kim~\cite{KK95}.

In order to show that , the ``easy part'' of Lemma \ref{lem:kk}, K\&K consider an optimal solution  to \eqref{def-H}. Because  is feasible, it defines a box that is contained in . (The defining property of this box is that it has  and the origin as opposite vertices.) Because  is optimal, it yields a simplex that contains . Thus the box is contained in , which is contained in the simplex. This gives inequalities between the volumes of these polytopes. The volume of the box is  and that of the simplex is . By invoking a beautiful result of Stanley~\cite{S86} relating the volume of  to , and also  (see Lemma \ref{lem:perf}), K\&K derive the desired inequalities. Stanley~\cite{S86} proves that the stable set polytope  and the {\sl order polytope} 
 have the same volume. Because  canonically decomposes into  simplices, of volume  each, one obtains that the volume of , and thus , is precisely .

Now let  denote any optimal collection of intervals consistent with . These intervals define another box of volume , this time contained in . This directly implies , without using Stanley's result. An elegant, elementary proof of the inequality  was given more recently by Brightwell and Tetali (\cite{BT03}, Theorem 5.2).

The second result of Kahn and Kim~\cite{KK95} is an adversarial strategy that forces any algorithm for sorting under partial information to perform a number of queries that is close to the lower bound. We give a short proof of this, namely that any algorithm can be forced to perform  queries. Initially, compute an optimal collection of intervals  consistent with . When faced with the query ``is ?'', answer ``yes'' if and only if  in the current collection of intervals, where  denotes the midpoint of the interval for . If the answer is ``yes'' (and ), replace the interval for  by  and the interval for  by . If the answer is ``no'', replace the interval for  by  and the interval for  by . Since , such an answer guarantees that each comparison decreases  by at most . Therefore, the number of comparisons performed is at least .

We now prove Theorem~\ref{th:c1}.

\begin{proof}[Proof of Theorem~\ref{th:c1}]
The proof is by induction on  and, for  fixed, on the number of incomparabilities in . The result being true for , we assume . Consider an optimal vector  and corresponding collection of open intervals .
Let  be such that  is maximum.

If  is comparable to all elements of , then the induction hypothesis implies


Hence, we may assume that  is incomparable to some element in . Let  be such an element
with  maximum. Clearly, .
In fact, it must be that : Indeed, by our choice of  and ,
we have  for every . Thus, if ,
then one could extend to the right the interval corresponding to 
by an amount of  and still
have a collection of intervals consistent with . However, this new collection
defines a corresponding vector  such that

contradicting the optimality of .

Exchanging  and  if necessary, we may assume that .
By shortening the intervals of  and  in two different ways,
we will define two collections of open intervals
 and .
In the first one, we will have , while for the second  will hold. To this aim, we introduce a few quantities.

Let  (thus, ).
Let

and

The collection  equals
, except that

Similarly,  equals
 with the following two exceptions:



Let  () be the interval order defined by , with  whenever .
Clearly, both  and  extend .

We claim that there exists an index  such that

This is proved below. Assuming that the claim is correct, let  be the vector defined by the collection
of open intervals .
This vector gives an upper bound on the entropy of , namely

Hence,


Using \eqref{eq:index}, \eqref{eq:cost}, and the induction hypothesis on ,
we obtain



In order to prove the claim that there exists an index  such that \eqref{eq:index} holds, we show the following two inequalities:


For proving \eqref{eq:eP1eP2}, it is enough to show
 and .
By definition of  and , we have

and (using )

Hence, . Also,

and

implying . Therefore, \eqref{eq:eP1eP2} holds true.

We proceed and show \eqref{eq:roots}. The left-hand side of \eqref{eq:roots} is a function of , which we denote by  for short. We have
2ex]
\displaystyle
\frac{1}{2\sqrt{\lambda}} + \frac{\sqrt{\lambda}}{2}  & \textrm{ otherwise.}
\end{array}
\right.

f'(\lambda) = \left \{
\begin{array}{ll}
\displaystyle
\frac{1}{4\sqrt{\lambda}} - \frac{1}{2\sqrt{1 - \lambda}}  & \textrm{ if } \lambda \leq \frac{1}{2} \
As the reader will easily check,  is positive over the open interval 
and negative over . Since , we deduce that
 for every , as claimed.
This concludes the proof.
\end{proof}

Finally, we sketch a simple proof of the weaker inequality . We follow the same proof structure as above. Instead of picking  such that  is maximum,  pick  such that  is maximum. Then pick  such that the interval for  contains the midpoint of the interval for . If , then define  by replacing the interval for  by its first quarter and the interval for  by its last quarter. Otherwise, we have . In this case, define  by replacing the interval for  by its last quarter and the interval for  by its first quarter.

\section{Insertion Sort}
\label{sec:insertion}

We first propose an  sorting algorithm with query complexity . It consists of first finding a maximum chain , then iteratively inserting the remaining elements of  in the chain , using binary search, see Algorithm \ref{alg:insertion_sort}. In order to show that its query complexity is , we need two lemmas.

\begin{algorithm}[ht]
\caption{Insertion sort-like algorithm for sorting under partial information} \label{alg:insertion_sort}
\begin{algorithmic}
\STATE \COMMENT{\textit{Phase 1 (preprocessing)}} 
\STATE find a maximum chain  
\STATE \COMMENT{\textit{Phase 2 (sorting)}}
\WHILE{} 
\STATE remove an element of  and insert it in  with a binary search
\ENDWHILE
\RETURN 
\end{algorithmic}
\end{algorithm}

\begin{lemma}
\label{lem:bigchain}
Let  be a poset of order  and let  be a maximum chain in . Then .
\end{lemma}
\begin{proof}
It is well known~(\cite{K86,CFJ07}) that the entropy of a graph on  vertices with stability number  is at least .
The result follows by applying this to .
\end{proof}

\begin{lemma}
\label{lem:redfac}
For all , .
\end{lemma}

Now the number of comparisons performed by the algorithm is at most


This algorithm has the property that we can perform the preprocessing step only once, and sort all instances with the same partial information in time . To achieve this, we store the maximum chain  in a balanced binary search tree in time  and insert each remaining element in time .

\section{Merge Sort}
\label{sec:merge}

In order to improve on the previous algorithm, we use an approach similar to merge sort, see Algorithm \ref{alg:merge_sort}. This algorithm is illustrated in Figure~\ref{fig:merge_sort}.

\begin{algorithm}
\caption{Mergesort-like algorithm for sorting under partial information} \label{alg:merge_sort}
\begin{algorithmic}
\STATE \COMMENT{\textit{Phase 1 (preprocessing)}}
\STATE find a greedy chain decomposition , \ldots,  of 
\STATE 
\STATE \COMMENT{\textit{Phase 2 (sorting)}}
\WHILE{}
\STATE pick the two smallest chains  and  in 
\STATE merge  and  into a chain , in linear time
\STATE remove  and  from , and replace them by 
\ENDWHILE
\RETURN the unique chain in 
\end{algorithmic}
\end{algorithm}

\begin{figure}
\begin{center}
\includegraphics[scale=1]{mergesort.pdf}
\end{center}
\caption{\label{fig:merge_sort}Illustration of Algorithm~\ref{alg:merge_sort}.}
\end{figure}

Let  denote the entropy of the probability distribution , the distribution of the sizes of the chains in the greedy chain decomposition. Our next lemma bounds the query complexity of Algorithm \ref{alg:merge_sort} in terms of .
\begin{lemma}
\label{lem:Huffman}
The query complexity of Algorithm \ref{alg:merge_sort} is at most .
\end{lemma}
\begin{proof}
Phase 2 of Algorithm \ref{alg:merge_sort} is a multiway merge of the chains  extracted from . The two smallest chains are iteratively merged, thereby forming a Huffman tree: this is a known strategy for merging sorted sequences of different lengths (see for instance~\cite{FB72}). Huffman codes have average codeword length within one bit of the entropy~\cite{TC}. Hence the average root-to-leaf distance in the tree with respect to the distribution  is at most .

Merging two chains is done in linear time by iteratively choosing the minimum. Consider an element of the chain . In the worst case, this element is compared at every node of the path from the leaf node corresponding to , to the root of the tree. Every time we compare two elements at one node of the tree, we charge the comparison to the element that is selected. We denote by  the length of the path to the th chain . Summing over all the elements, we get:

proving the lemma.
\end{proof}

The following theorem uses this bound and Theorem~\ref{thm:greedy}.
\begin{theorem}
\label{thm:nearopt}
For any , the query complexity of Algorithm \ref{alg:merge_sort} is at most

\end{theorem}

\begin{proof}
From Lemma \ref{lem:Huffman}, we infer that the query complexity is at most

\end{proof}
We conclude that Algorithm \ref{alg:merge_sort} is an algorithm with query complexity at most , for any . It is shown in Appendix~\ref{app:greedy} that the greedy chain decomposition can be performed in time . This is actually the bottleneck of the algorithm, and the global complexity of Algorithm \ref{alg:merge_sort} is  as well. Hence any improvement on the greedy chain decomposition algorithm would yield an improved sorting algorithm.

Note that again, we can reuse the chain decomposition obtained in the preprocessing phase for sorting any instance with the same partial information in time proportional to the query complexity.

\section{Cautious Merge Sort}
\label{sec:cautious_merge}

The query complexity of Algorithm \ref{alg:merge_sort} is not  because it completely ignores a large part of the partial information. Now, we show that using the partial information for the {\sl last\/} merge suffices to obtain an algorithm with query complexity .

The subproblem at hand is that of {\sl merging under partial information\/}. It is a special case of sorting under partial information, in which the given poset  is covered by two chains  and , that is,  is of width at most . (The {\sl width\/} of a poset  is the maximum size of an antichain of .)

That problem was studied by Linial~\cite{L84}, who proposed an algorithm with query complexity . However, this algorithm requires computing polynomially many  determinants. In Section \ref{sec:MUPIsec}, we obtain an  algorithm for the problem with query complexity at most . 

\begin{theorem}
\label{thm:mupired}
Suppose there exists an algorithm for the problem of merging under partial information with query complexity at most , given as partial information a poset  of order  and width at most . Then there exists an algorithm for the problem of sorting under partial information with query complexity at most .
\end{theorem}
\begin{proof}
Let Algorithm \ref{alg:MUPI} be the hypothesized algorithm for merging under partial information. 
(Such an algorithm will be given in Section \ref{sec:MUPIsec}.) 
Consider the following algorithm, illustrated in Figure~\ref{fig:mupireduction}.

\begin{algorithm}[h!]
\caption{Improved mergesort-like algorithm for sorting under partial information} \label{alg:cautious_merge_sort}
\begin{algorithmic}[1]
\STATE find a maximum chain 
\STATE \label{line:merge_B} apply Algorithm \ref{alg:merge_sort} to the poset , yielding a chain 
\STATE \label{line:call_MUPI} apply Algorithm \ref{alg:MUPI} to the current partial information 
\RETURN the resulting chain
\end{algorithmic}
\end{algorithm}

\begin{figure}
\begin{center}
\includegraphics[scale=1]{mupireduction.pdf}
\end{center}
\caption{\label{fig:mupireduction}Illustration of Algorithm~\ref{alg:cautious_merge_sort}.}
\end{figure}

From Lemma~\ref{lem:bigchain}, we have , and therefore (using Lemma~\ref{lem:redfac}):

Now from Theorem~\ref{thm:nearopt} the number of comparisons in lines \ref{line:merge_B} and \ref{line:call_MUPI} of Algorithm \ref{alg:cautious_merge_sort} is at most
2ex]
\leq & (1+\varepsilon) \log e(P) + \Big( (1+\varepsilon)\big(1 + \ln (1 + \frac{1}{\varepsilon})\big) + \ln 2 \Big)\,nH(\bar P) + c_3 \log e(P) \qquad \text{(from (\ref{eq:rest}))}\2ex]
\leq & (9.09 + c_3) \log e(P) \qquad \text{(taking )}.
\end{array}

H(G) = \sum_{i=1}^k \frac{|A_i| + |B_i|}{n} \, h\left(\frac{|A_i|}{|A_i| + |B_i|}\right).

\label{eq:bip_ratio}
\frac{|A_i|}{|N_{G'}(A_i)|}

x_u := \frac{|A_i|}{|A_i| + |B_i|}\ \ \text{whenever}\ \ u \in A_{i},
\qquad \text{and} \qquad
x_v := \frac{|B_i|}{|A_i| + |B_i|}\ \ \text{whenever}\ \ v \in B_{i}.

\STAB(G) = \{x \in \mathbb{R}^V : x_u + x_v \leq 1\ \forall uv \in E,\ 0 \leq x_v \leq 1\ \forall v \in V\}.

x_{v} &=& 1-x_{u} \quad \text{(because  is tight)}\\
    &>& x_{v'} \quad \text{(because  is not tight)}\\
    &=& 1-x_{u'} \quad \text{(because  is tight)}\\
    &\geq& x_{v} \quad \text{(because  is an edge and  is feasible),}

\label{K:weight}
x_{u} = \frac{|A \cap K|}{|K|}\ \ \text{for all}\ \ u \in A \cap K, 
\qquad\text{and}\qquad
x_{v} = \frac{|B \cap K|}{|K|}\ \ \text{for all} \ \ v \in B \cap K.
\left|x_{v} + \sigma - \frac{|A \cap K|}{|K|}\right|
\xi \mapsto \frac{|A \cap K|}{|K|} \log \xi +
\frac{|B \cap K|}{|K|} \log (1-\xi)

|Y| \left(1 + \left\lfloor \log \frac{|X|}{|Y|} \right\rfloor\right) + \left\lfloor \frac{|X|}{2^{\lfloor \log \frac{|X|}{|Y|} \rfloor}} \right\rfloor - 1.

\left\lfloor \log \frac{|X|}{|Y|} \right\rfloor = \log \frac{|X|}{|Y|} - \xi.

|Y| \Big(1-\xi+2^\xi + \log \frac{|X|}{|Y|}\Big) \leq |Y| \log\frac{4|X|}{|Y|},

x_v + x_w = \frac{|B \cap K|}{|K|} + \frac{|A \cap L|}{|L|}
\geq \frac{|B \cap K|}{|K|} + \frac{|A \cap K|}{|K|} = 1,

x'_{v} + x'_{w} = (1/2 +\varepsilon) + x_{w} \leq (1/2 +\varepsilon) + (1/2 - 2\varepsilon) < 1,

\ent(x') - \ent(x'') \geq 
-\frac {s_{j}}n \log \frac{s_{j}}{s_{j}+t_{j}} -  \frac{s_{j}}{n}
=  \frac {s_{j}}n \log \frac{s_{j}+t_{j}}{s_{j}} - \frac{s_{j}}{n}.

\label{eq:delta_ent_red}
\ent(x') - \ent(x''') \geq 
\frac {s_{j}}n \log \frac{s_{j}+t_{j}}{s_{j}} - \frac{s_{j}}{n}.

\label{eq:delta_r_red}
r_{j} - r_{j+1} \geq s_{j}.

\ent(x') - \ent(x'') \geq 
-\frac {s_{j}}n \log \frac{s_{j}}{s_{j}+t_{j}}
=  \frac {s_{j}}n \log \frac{s_{j}+t_{j}}{s_{j}}.

\label{eq:delta_ent_blue}
\ent(x') - \ent(x''') \geq 
\frac {s_{j}}n \log \frac{s_{j}+t_{j}}{s_{j}}.

\label{eq:delta_r_blue}
r_{j} - r_{j+1} = 0.

\label{eq:delta_phi}
\phi_{j} - \phi_{j+1}
\geq s_{j}\log \frac{s_{j}+t_{j}}{s_{j}}.

\label{eqn:onemerge}
s_{j} \log \frac{s_{j}+t_{j}}{s_{j}} \geq \frac 12 s_{j} \log \frac{4t_{j}}{s_{j}}.

\phi_{1} = \sum_{j=1}^{k} (\phi_{j} - \phi_{j+1})
\geq \sum_{j=1}^{k}  s_{j} \log \frac{s_{j}+t_{j}}{s_{j}}
\geq \sum_{j=1}^{k} \frac 12 s_{j} \log \frac{4t_{j}}{s_{j}} 
\geq \frac{q}{2}.

q \leq 2\phi_{1} = 2n \ent(x) + 2 r_{1}.

q \leq 2n \ent(x) + 2 r_{1} = 2n \ent(x) + 2n \ent(\tilde{x}) \leq 3n \ent(x),

3n\ent (x) \leq 3n\ent(\bar{P}) \leq 6\log e(P) .

\rho := \beta / \alpha

\frac{|X|}{|N_{G'}(X)|} > \frac{\beta}{\alpha}.

The claim follows. By the max-flow min-cut theorem, case (ii) arises if and only if the maximum value of a -- flow in  is strictly smaller than . 

Computing a maximum -- flow in  amounts to computing a maximum -matching in the convex bipartite graph , where the weights of the vertices are defined as  whenever  and  whenever . This can be done in  time by adapting Glover's algorithm for computing a maximum matching in a convex bipartite graph \cite{G67} to the weighted case, and using a heap for storing vertices of . 

Second, once the maximum possible value of the ratio \eqref{eq:bip_ratio} is determined, we seek a maximizer . This amounts to converting the last maximum -- flow computed during the bisection into a minimum -- cut. Because case (i) arises, the value of the flow equals . Hence, all the directed edges incident to  are saturated. If all the directed edges incident to  are also saturated, then  and we may take . Otherwise, we perform a BFS from  in an auxiliarly network obtained from  by deleting all saturated directed edges, reversing all non-saturated edges (in particular, all the edges from  to ) and adding all the directed edges  from  to  that carry a nonzero flow. Because  is -convex and the support of the maximum -- flow in  constructed by Glover's algorithm is of size , we can perform the BFS in  time. We then define  as the vertices of  that cannot be reached from  in the auxiliary network. The lemma follows.
\end{proof}

\end{document}
