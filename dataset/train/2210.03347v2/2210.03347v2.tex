\documentclass{article} \usepackage[accepted]{icml2023}



\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{xspace}
\usepackage{xargs}
\usepackage{stfloats}
\usepackage{marginnote}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{times}
\usepackage{array}
\usepackage{fancyvrb}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{makecell}
\usepackage[section]{placeins}
\usepackage{framed}
\usepackage{tcolorbox,color,verbatim}
\usepackage{graphbox}
\usepackage{alltt}
\definecolor{shadecolor}{rgb}{.9, .9, .9}
\newenvironment{code}{\snugshade\verbatim}{\endverbatim\endsnugshade}
\usepackage{tikz,pgfplots}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.14}
\usetikzlibrary{shapes,arrows, positioning}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\definecolor{g-red}{HTML}{DB4437}
\definecolor{g-blue}{HTML}{4285F4}
\definecolor{g-green}{HTML}{0F9D58}
\definecolor{g-yellow}{HTML}{F4B400}
\definecolor{g-orange}{HTML}{FF9800}
\definecolor{g-grey}{HTML}{9E9E9E}
\newcommand\ourmodel{{\texttt{Pix2Struct}}}
\newcommand\mandarj[1]{[\textcolor{magenta}{MJ: {#1}}]}
\newcommand\kristout[1]{[\textcolor{red}{KT: {#1}}]}
\icmltitlerunning{Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding}

\begin{document}

\twocolumn[
\icmltitle{Pix2Struct: Screenshot Parsing as Pretraining for \\ Visual Language Understanding}




\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kenton Lee}{equal,goog}
\icmlauthor{Mandar Joshi}{equal,goog}
\icmlauthor{Iulia Turc}{succ}
\icmlauthor{Hexiang Hu}{goog}
\icmlauthor{Fangyu Liu}{cam}
\icmlauthor{Julian Eisenschlos}{goog}
\icmlauthor{Urvashi Khandelwal}{goog}
\icmlauthor{Peter Shaw}{goog}
\icmlauthor{Ming-Wei Chang}{goog}
\icmlauthor{Kristina Toutanova}{goog}
\end{icmlauthorlist}
\icmlaffiliation{goog}{Google Research}
\icmlaffiliation{cam}{University of Cambridge}
\icmlaffiliation{succ}{succinctly.ai}
\icmlcorrespondingauthor{Kenton Lee}{kentonl@google.com}
\icmlcorrespondingauthor{Mandar Joshi}{mandarj@google.com}
\icmlkeywords{visually situated language}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} \setcounter{footnote}{1}

\begin{abstract}
Visually-situated language is ubiquitous---sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present \ourmodel, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. \ourmodel~is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, and image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.
\end{abstract}

\section{Introduction}
Research on the interaction between language and vision has traditionally focused on tasks where images and text can be separated into distinct channels, e.g. visual question answering or image captioning. However, \emph{visually-situated language} is a far more pervasive way in which these modalities interact and blend together. For example, documents, tables, infographics, and user interfaces (UIs) are intended to be consumed holistically, without clear boundaries between textual and visual elements (Figure~\ref{fig:tasks}). Comprehensive understanding of this information requires a deep set of skills, including the ability to recognize text, understand language, and incorporate diverse visual context.

Previous work on understanding visually-situated language is scattered. The focus is typically on complex task-specific combinations of available inputs and tools. For example, document-understanding models~\citep{layoutlmv3} rely on external OCR systems, UI-understanding models rely on platform-specific metadata (e.g. Android view hierarchy)~\citep{uibert}, and diagram-understanding models rely on diagram parses~\citep{kembhavi2016diagram}. Domain-specific engineering can be effective for high-resource settings such as documents, where there is an abundance of tools and data available. However, these pipelined models lack sharing of the underlying data, model architectures, and objectives across domains, limiting their general applicability. Moreover, relying on external systems like OCR increases engineering complexity, limits adaptability, and can increase overall computational cost. Recent work on OCR-free, end-to-end document understanding from images~\citep{donut, dessurt} has attempted to remove such task-specific engineering and reliance on external components during inference by learning to decode OCR outputs during pretraining---a significant step towards more general-purpose models. However, the focus on text at the surface level limits the depth of knowledge transferred from unsupervised data. 

\begin{figure*}[!t]
\centering
\begin{subfigure}[b]{0.36\textwidth}
\centering
\tiny
\textbf{Screenshot Parsing Pretraining}
\end{subfigure}
\begin{subfigure}[b]{0.26\textwidth}
\centering
\tiny
\textbf{AI2D}
\end{subfigure}
\begin{subfigure}[b]{0.19\textwidth}
\centering
\tiny
\textbf{Screen2Words}
\end{subfigure}
\begin{subfigure}[b]{0.15\textwidth}
\centering
\tiny
\textbf{DocVQA}
\end{subfigure}
\begin{subfigure}[b]{0.36\textwidth}
\frame{\includegraphics[width=\linewidth, height=3.38cm]{figures/pretraining_example.png}}
\end{subfigure}
\begin{subfigure}[b]{0.26\textwidth}
\frame{\includegraphics[width=\linewidth, height=3.38cm]{figures/ai2d_example.png}}
\end{subfigure}
\begin{subfigure}[b]{0.19\textwidth}
\frame{\includegraphics[width=\linewidth, height=3.38cm]{figures/screen2words_example.png}}
\end{subfigure}
\begin{subfigure}[b]{0.15\textwidth}
\frame{\includegraphics[width=\linewidth, height=3.38cm]{figures/docvqa_example.png}}
\end{subfigure}
\\[-2ex]
\begin{subfigure}[t]{0.01cm}
\end{subfigure}
\begin{subfigure}[t]{0.36\textwidth}
\small
\begin{snugshade*}
\begin{verbatim}
<<Pro>
 <<<$15> </mo>>
  <<20 users included>
   <10 GB of storage> 
   <Priority email support>
   <Help center access>>
  <Get started>>>
\end{verbatim}
\end{snugshade*}
\end{subfigure}
\begin{subfigure}[t]{0.26\textwidth}
\small
\begin{snugshade*}
\begin{verbatim}
carnivore






\end{verbatim}
\end{snugshade*}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\small
\begin{snugshade*}
\begin{verbatim}
list of videos
for weather
reports in
different
locations


\end{verbatim}
\end{snugshade*}
\end{subfigure}
\begin{subfigure}[t]{0.15\textwidth}
\small
\begin{snugshade*}
\begin{verbatim}
Fred LeCrone






\end{verbatim}
\end{snugshade*}
\end{subfigure} \caption{Examples of visually-situated language understanding tasks, including diagram QA (AI2D), app captioning (Screen2Words), and document QA (DocVQA). We also include an example of our proposed pretraining task (screenshot parsing) on the left.~\ourmodel~encodes the pixels from the input image (above) and decodes the output text (below).}
\vspace{-10pt}
\label{fig:tasks}
\end{figure*}

We present~\ourmodel\footnote{For pretrained checkpoints and code, see \url{https://github.com/google-research/pix2struct}.}, a pretrained model that combines the simplicity of purely pixel-level inputs with the generality and scalability provided by self-supervised pretraining from diverse and abundant web data. Specifically, we propose a \emph{screenshot parsing} objective that requires predicting an HTML-based parse from a masked screenshot of a web page. HTML provides clean signals about text, images, and layouts, while the masked inputs encourage joint reasoning about their co-occurrence. With the diversity and complexity of textual and visual elements found on the web, \ourmodel~learns rich representations of the underlying structure of web pages, which we show can effectively transfer to a variety of downstream visual language understanding tasks.

A key ingredient which enables this transfer is processing inputs visually and holistically as they are intended for human readers. We introduce variable-resolution inputs for vision transformers (ViT) that prevent distortion of the original aspect ratio, which can vary greatly across documents, figures, and UIs. During finetuning, we render other inputs (e.g., questions in VQA and bounding boxes in UI tasks) onto the image input for the task. In effect, we consume all our inputs through a single modality, simplifying the modality combination problem in previous work.

We train two variants with 282M and 1.3B parameters, which we refer to as~\ourmodel-Base and~\ourmodel-Large respectively, on 80M screenshots of web pages collected from the URLs in the C4 corpus~\citep{t5}\footnote{We do not use the released text in C4. The web page content and screenshots were crawled directly from the URLs.}. Experiments on four domains and nine tasks show that our finetuned models strongly outperform Donut (ranging from 9 to 53 points), the strongest existing baseline without pipelines. Compared with models with domain-specific pipelines, we lag behind the state of the art in high-resource domains such as documents and natural images but observe significant improvements (ranging from 1 to 44 points) in low-resource domains such as illustrations and UIs. We hope these results encourage the community to continue developing such general-purpose methods and further enable new applications in this currently fragmented intersection of language and vision. 

To summarize, our major contributions are as follows:
\begin{itemize}[leftmargin=20pt,topsep=0pt,itemsep=0pt]
    \item We introduce the area of general-purpose visually-situated language understanding, which consists of diverse tasks but common challenges.
    \item We propose a \emph{screenshot parsing} pretraining objective based on the HTML source of web pages. Our objective is shown to be more effective than prior attempts to enable the elegant pixel-to-text design for general-purpose visually-situated language understanding.
    \item We introduce variable-resolution input representations to ViT and new fine-tuning strategies that seamlessly integrate language and vision inputs by directly rendering any text prompts on top of the input image.
\end{itemize}

\section{Method}
\subsection{Background} 
Prior attempts at pixel-only modeling of visually situated language have largely focused on documents and natural images. For documents, Donut~\citep{donut} and Dessurt~\citep{dessurt} combine pretrained objectives based on surface-level features from synthetic images or predicted OCR outputs. For natural images, recent work---GIT2~\citep{wang2022git} and PaLI~\citep{pali}---focuses on collecting and training on large scale image captioning data that transfers well to datasets with natural images (e.g. TextCaps). 

We aim to provide a single pretrained model that can be finetuned on a wider variety of tasks and domains. The input to our model is an image in the form of raw pixels only, and the output is text in the form of token sequences, similar to Donut. The goal is a visual analog of models like T5~\citep{t5}, where the generality of simple inputs and outputs is combined with the power of pretraining on large unsupervised sources of data. During finetuning, the complexity of adapting to diverse downstream tasks resides only in data preprocessing.

Even without visual context, pixel-only language modeling for text has only recently been attempted~\citep{rust2022language}---perhaps because it requires solving multiple hard sub-problems. First, the ability to read with high fidelity while also building rich high-level representations poses a difficult optimization problem. Second, encoding text-heavy inputs (e.g. long documents) involves processing high-resolution images with variable aspect ratios. State-of-the-art document understanding models~\citep{layoutlmv3} therefore rely on the combination of (possibly noisy) OCR outputs with low resolution images. 

We show the components of \ourmodel~that address these challenges. Section~\ref{sec:architecture} discusses modifications to the transformer inputs to handle variable aspect ratios and resolutions. Section~\ref{sec:pretraining} details our proposed screenshot parsing objective and Section~\ref{sec:curriculum} describes curriculum learning for  more robust transfer learning. Finally, Section~\ref{sec:finetuning} shows how \ourmodel~consumes textual and visual inputs for downstream tasks (e.g. questions and images) in the same space by rendering text inputs onto images. 

\subsection{Architecture}
\label{sec:architecture}

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=1\textwidth, keepaspectratio]{figures/input_rep.pdf}
\end{center}
\caption{Comparison of our variable resolution inputs and the typical fixed resolution input. We illustrate the preprocessing for a target sequence length of 36 patches for both inputs.}
\label{fig:input_rep}
\end{figure*}

\ourmodel~is an image-encoder-text-decoder based on ViT~\citep{vit}. While the bulk of the model is fairly standard, we propose one small but impactful change to the input representation to make~\ourmodel~more robust to various forms of visually-situated language.
Before extracting fixed-size patches, the standard ViT scales the input images to a predefined resolution, which creates two undesirable effects: (1) rescaling the image distorts the true aspect ratio, which can be highly variable for documents, mobile UIs, and figures. (2) transferring these models to downstream tasks with higher resolution is non-trivial~\citep{train-test-resolution,simvlm}, since the model only observes one specific resolution during pretraining.

We instead propose to always scale our input image up or down such that we extract the maximal number of fixed-size patches that fit within the given sequence length (Figure~\ref{fig:input_rep}). In order for the model to handle variable resolutions unambiguously, we use 2-dimensional absolute positional embeddings for the input patches.
Together these changes to the standard ViT inputs provide two major advantages in terms of robustness to: (1) extreme aspect ratios, which is common in the domains that we experiment with, and (2) on-the-fly changes to the sequence length and resolution.

\subsection{Pretraining}
\label{sec:pretraining}
The goal of pretraining is for \ourmodel~to represent the underlying structure of the input image. To that end, we create self-supervised pairs of input images and target text from web pages. For each page in the pretraining corpus, we start by collecting its HTML source and a screenshot using a viewport of 1024 x 1024.

\textbf{Screenshot parsing inputs~\& outputs}~~
The screenshot and HTML are modified to ensure rich and dense learning signal during pretraining. These modifications provide a reasonable trade-off between preserving the semantics of the page and requiring a practical decoder sequence length.

We condense the HTML DOM tree by (1) only keeping nodes with \emph{visible} elements or descendants with visible elements and (2) if a node does not contain visible elements and it only has a single child, replacing the singleton child with any grandchildren to remove chained nesting. In each node, we only use the text, along with filenames and alt-text of images. Much more information could be retained (e.g. element tags, style, titles and URLs) in future work. The decoder sequence length is further reduced by finding the largest linearized subtree that fits within a predefined sequence length. A bounding box indicating the region covered by the chosen subtree is also drawn on the screenshot.

For better context modeling, we introduce a BART-like~\citep{lewis-etal-2020-bart} learning signal by masking 50\% of the text and decoding the entire subtree. The masked regions are randomly sampled spans of text from the chosen subtree where we render masks (Figure~\ref{fig:screenshot_parsing_running}).

\begin{figure*}[t!]
\centering
\begin{minipage}{.18\textwidth}
\frame{\includegraphics[width=\textwidth]{figures/example.png}}
\end{minipage}
\begin{minipage}{.05\textwidth}
\centering
\large
:
\end{minipage}
\begin{minipage}{.18\textwidth}
\frame{\includegraphics[width=\textwidth]{figures/example_masked.png}}
\end{minipage}
\begin{minipage}{.05\textwidth}
\centering
\large
$\rightarrow$
\end{minipage}
\begin{minipage}{.35\textwidth}
\scriptsize
\begin{code}
<<<Python>
  <img_src=py_logo.png img_alt=Python>>
 <<C++>
  <img_src=cpp_logo.png img_alt=C++>>
 <<Java>
  <img_src=java_logo.png img_alt=Java>>
 <Submit>>
\end{code}
\end{minipage}
\caption{Toy illustration of input-output pairs (right) sampled from the original web page (left).}
\label{fig:screenshot_parsing_running}
\end{figure*}
\textbf{Comparison to existing pretraining strategies}~~
Our proposed screenshot parsing seamlessly integrates signals reminiscent of several well-known pretraining strategies:
\begin{itemize}[leftmargin=12pt,itemsep=4pt,topsep=0pt,parsep=0pt,partopsep=0pt]
    \item Recovering the unmasked parts of the parse is similar to OCR, a prerequisite skill for understanding language. OCR pretraining was proposed in Donut which uses synthetic renderings or OCR outputs.
    In Figure~\ref{fig:screenshot_parsing_running}, predicting~\texttt{<C++>} exemplifies this learning signal. 
    \item Recovering the masked parts of the parse is much like masked language modeling~\citep{bert}. A major difference is that the visual context often provides additional powerful cues. In Figure~\ref{fig:screenshot_parsing_running}, predicting~\texttt{<Python>} exemplifies this signal.
    \item Recovering the alt-text from images is a common pretraining strategy for image captioning~\citep{conceptual-captions, wang2022git, pali}. A major difference is that the model is permitted to use the web page as additional context. In Figure~\ref{fig:screenshot_parsing_running}, predicting~\texttt{img\_alt=C++} exemplifies this learning signal.
\end{itemize}
Appendix~\ref{sec:pretraining_ex} contains more details including examples of screenshots paired with their gold and predicted parses.

\subsection{Warming up with a reading curriculum}
\label{sec:curriculum}
While we can directly pretrain \ourmodel~on the screenshot parsing task, we find that doing this naively can result in instability and slow learning. However, if we first expose the model to a short ``warmup'' stage of simply learning to read, we find a strong curriculum learning effect where (1) pretraining is more stable and converges faster, and (2) we observe better finetuning performance, as discussed in Section~\ref{sec:ablations}. We create images of text snippets with random colors and fonts. The model is simply trained to decode the original text (see Appendix~\ref{sec:warmup_example} for examples). This type of curriculum learning was also used in Dessurt~\citep{dessurt} and can also be viewed as a simplified version of Donut's pretraining.

\subsection{Finetuning}
\label{sec:finetuning}
Finetuning ~\ourmodel~is straightforward and largely a matter of preprocessing the downstream data to unambiguously reflect the task in the image inputs and text outputs, analogous to the way T5~\citep{t5} is used for text-based tasks. In this section, we cover the preprocessing strategies for the tasks described in Table~\ref{tab:datasets}. Examples of this preprocessing are shown in Figure~\ref{fig:tasks}.

Captioning is the most straightforward, since the input image and the output text can be directly used (as in TextCaps, Screen2Words). In the case where the focus of the caption is a specific bounding box (as in Widget Captioning), we draw the target bounding box on the image itself.

For visual question answering (as in OCR-VQA, ChartQA, DocVQA, InfographicsVQA), while multimodal models typically reserve a specialized text channel for the question, we opt to instead directly render the question as a header at the top of the original image.~\ourmodel~reads both the question and the image jointly via the visual modality. This strategy is analogous to the common practice of simply concatenating all inputs during finetuning of pretrained text models, first proposed in GPT~\citep{gpt} and has been the default method in NLP since then. Intuitively, this strategy is effective because~\ourmodel~has been pretrained to be sensitive to long-range interactions between various parts of the input image. 
In the case of multiple choice answers (as in AI2D), we also render the choices in the header as part of the question.

The most complex scenario is RefExp, where the task is choosing between UI components that a natural language expression could be referring to. For each candidate, we create a training instance where the input image contains the bounding box and referring expression, and the decoding target is ``true'' or ``false''. We sample five negative candidates per positive candidate during training. During inference, we pick the candidate for which the model generates ``true'' with the highest score.\footnote{or lowest score if something other than ``true'' was generated}
 

\section{Experimental Setup}
\subsection{Benchmarks}
We evaluate \ourmodel~on multiple benchmarks for visually-situated language understanding across four domains: illustrations, user interfaces, natural images, and documents. Since we are the first to aggregate datasets with this scope, we optimized for diversity in domains and in task-format. Evaluation is restricted to standard splits without additional labeled data. 
Table~\ref{tab:datasets} in Appendix~\ref{sec:finetuning_datasets} provides a summary of the datasets with details in Section~\ref{sec:results}.

We use evaluation metrics as defined in the original papers: (a) average normalized Levenshtein similarity (ANLS) for DocVQA and InfographicVQA, (b) exact match (EM) for AI2D, RefExp, and OCR-VQA, (c) relaxed accuracy (RA) for ChartQA, and (d) CIDEr for the generation tasks.

\subsection{Implementation and Baselines}
\textbf{Pretraining}~~We pretrain two model variants: (a) a \emph{base} model with 282M parameters including 12 encoder and 12 decoder layers with a hidden size of 768, and (b) a \emph{large} model with 1.3B parameters including 18 layers with a hidden size of 1536. Both models have the same warmup stage using text rendered from BooksCorpus~\citep{books} lasting 30K steps with a maximum input sequence length of 128 patches. The base model is then pretrained further for 270K steps with the screenshot parsing objective using a batch size of 2048 on 64 Google Cloud TPUs. 
The large model is pretrained for 170K steps with a batch size of 1024 on 128 Google Cloud TPUs. 
Both models use an input sequence length of 2048 patches and are optimized using Adafactor~\citep{shazeer2018adafactor}. The learning rate schedule uses a linear warmup of 1000 steps to 0.01, followed by cosine decay to 0. The decoder sequence length is 128 tokens, and we choose pretraining targets to have at most 1024 characters. As a reference point, the base model reaches ~30 BLEU and the large model reaches ~32 BLEU on the pretraining validation set. 
Details about finetuning can be found in Appendix~\ref{sec:hyperparams}.

\textbf{Baselines}~~
Across all tasks, we found a large number of methods which could serve as baselines. We compare \ourmodel~ against state of the art (SotA) methods in each domain (see Section~\ref{sec:results} for method descriptions). Several methods use model ensembles, multitask with labeled training data from other datasets~\citep{powalski2021going,wang2022git}, or train with validation data~\citep{li2021structurallm}. For fair comparison and ease of experimentation,
we focus on single-model and single-task baselines trained on standard splits. Several (per-task) SotA~\citep{li2021vut,masry-etal-2022-chartqa} use  domain-specific inputs (e.g. view hierarchies for UIs or gold data tables for charts) making it difficult to apply them to other domains. 
For a strong, consistent visual baseline across domains, we finetuned Donut on tasks where a purely visual baseline was unavailable.\footnote{Except RefExp due to the complexity inference.}

\section{Results}
\label{sec:results}
Table~\ref{tab:main_res} compares \ourmodel~with prior work.
\begin{table*}[t]
\setlength{\tabcolsep}{4pt}
\small
\centering
\begin{tabular}{lllccccccccc}
\toprule
\multicolumn{2}{l}{Method} & \makecell[l]{Pretraining} & \makecell[c]{ChartQA} & \makecell[c]{AI2D} & \makecell[c]{OCR\\VQA} & \makecell[c]{Ref\\Exp} & \makecell[c]{Widget\\Cap} & \makecell[c]{Screen2\\Words} & \makecell[c]{Text\\Caps} & \makecell[c]{DocVQA} & \makecell[c]{Info\\VQA} \\
\midrule
\multicolumn{2}{l}{\makecell[l]{State of the art\\~~w/ pipelines}} & - & \makecell[c]{\small{(VTP)}\\45.5} & \makecell[c]{\small{(DQAN)}\\38.5} & \makecell[c]{\small{(LATr)}\\67.5} & \makecell[c]{\small{(UIB)}\\90.8} & \makecell[c]{\small{(VUT)}\\~~97.0} & \makecell[c]{\small{(VUT)}\\~~64.3} & \makecell[c]{\small{(PaLI)}\\\bf{160.4}} & \makecell[c]{\small{(UDOP)}\\\bf{84.7}} & \makecell[c]{\small{(UDOP)}\\\bf{47.4}} \\
\cmidrule{1-12}
\multirow{6}{*}{\rotatebox{90}{Pixel only}} & GIT2 & \makecell[l]{Image captioning} & - & - & 70.3 & - & - & - & 145.0 & - & - \\
& Donut & \makecell[l]{OCR} & 41.8 & 30.8 & 66.0 & - & 127.4 & ~~56.4 & ~~74.4 & 67.5 & 11.6 \\
\cmidrule{2-12}
&\ourmodel\\
&\makecell[l]{~~~~Base} & \makecell[l]{Screenshot parsing} & 56.0 & 40.9 & 69.4 & 92.2 & 133.1 & 107.0 & ~~88.0 & 72.1 & 38.2 \\
&~~~~Large & \makecell[l]{Screenshot parsing} & \bf{58.6} & \bf{42.1} & \bf{71.3} & \bf{94.2} & \bf{136.7} & \bf{109.4} & ~~95.5 & 76.6 & 40.0 \\
\bottomrule
\end{tabular}
\caption{
\ourmodel~outperforms prior visual methods on 8 out of 9 benchmarks with SotA results on 6. While GIT2's image captioning pretraining understandably helps on TextCaps, screenshot parsing transfers to a wider variety of downstream tasks. The individual pipeline SotA methods are described in Section~\ref{sec:results} with full results in Appendix~\ref{sec:full_results}.}
\label{tab:main_res}
\vspace{-10pt}
\end{table*}

\subsection{Illustrations}

\textbf{ChartQA}~\citep{masry-etal-2022-chartqa} is a VQA dataset with questions based on charts, i.e. visual representations of tabular data.\footnote{We evaluate on the task without the gold data table.}. VisionTaPas~\citep{masry-etal-2022-chartqa}, the current SotA, is a pipeline which operates on data tables predicted from the given charts. It consists of (1) a ViT encoder for encoding the chart image, (2) a TaPas encoder for encoding the question and the data table, and (3) a cross-modal encoder. In contrast,~\ourmodel~does not rely on table extractors and uses the chart directly---improving the SotA from 45.5 to 58.6 with the large variant.


\textbf{AI2D}~\citep{kembhavi2016diagram} contains multiple choice questions based on illustrative science diagrams (about geological processes, biological structures etc.). The dataset comes with train and test splits. We set aside 1\% of the train split for validation. The current SotA DQA-NET~\citep{kembhavi2016diagram} focuses on modeling  entity relationships via a pipeline of tools for extracting arrows, blobs, and other visual elements.~\ourmodel-Large outperforms DQA-NET and Donut by 3.6 and 11.27 points respectively without any domain-specific modifications. 

\textbf{OCR-VQA}~\citep{mishra2019ocr} is a VQA dataset on images of book covers. The questions are based on book metadata such as title, author, genre etc. Much of work on OCR-VQA, including the pipeline SotA LATr~\citep{biten2022latr}, uses off-the-shelf OCR. Recent work, GIT2~\citep{wang2022git}, the current SotA, is pretrained on 12.9B image caption pairs.  Their final finetuning stage is preceded by intermediate finetuning on eight VQA datasets including VQAv2~\citep{goyal2017making}, VizWiz-VQA~\citep{chen2022grounding}, and OCR-VQA~\citep{mishra2019ocr} amongst others. Despite not using more labeled training data, we outperform GIT2 by almost 1 point. 

\subsection{UIs}
\textbf{RefExp}~\citep{uibert} Given a natural language referring expression, an app screenshot, and a set of components (via bounding boxes on the screenshot), the goal is to retrieve the component that the expression refers
to. UIBert~\citep{uibert}, the current SotA, is pretrained on a combination of inputs from mobile apps including  screenshots, OCR text, and Android view hierarchies. Our models substantially ourperform UI Bert by 1.4 and 3.4\% absolute, with~\ourmodel-Large setting the new SotA.


\textbf{Widget Captioning}~\citep{li-etal-2020-widget} is an image captioning task where the input is an app screenshot annotated with a single bounding box denoting a widget (e.g. a button or a scroll bar). The caption describes the functionality of the widget (e.g. \emph{find location}).
VUT~\citep{li2021vut}, the current SotA uses a specialized UI encoder combining images, bounding boxes, and view hierarchies. \ourmodel-Large improves the SotA CIDEr from 127.4 to 136.7.

\textbf{Screen2Words}~\citep{screen2words} is an image captioning task where the input is an app screenshot and the caption describes the functionality of the page (see Figure~\ref{fig:tasks} for an example).
~\ourmodel-Large improves the state of the art CIDEr from 64.3 to 109.4.

\subsection{Natural Images}
\textbf{TextCaps}
Recently, GIT2 (5.1B parameters) and PaLI (17B parameters) have advanced the state of the art on TextCaps by pretraining on 10B+ image-caption pairs extracted from the web.
PaLI (CIDEr 135.4) and GIT2 (CIDEr 145) show comparable performance without OCR inputs. PaLI achieves SotA (CIDEr 160.4) performance when finetuned with OCR, indicating that even for large-scale methods, end-to-end pixel-only performance lags behind pipeline SotA. While their image captioning-based pretraining understandably improves TextCaps, previous work~\citep{donut} shows that captioning may not transfer to other domains (e.g. documents). Moreover, screenshot parsing subsumes signals from  captioning (Section~\ref{sec:pretraining}) while using a fraction of the data used for pretraining GIT2 and PaLI. These results suggest that ~\ourmodel~could further benefit from scaling in pretraining data and model size.

\subsection{Documents}

\textbf{DocVQA}~\citep{mathew2021docvqa} is a dataset of questions about scanned documents,\footnote{from the UCSF Industry Documents Library \url{https://www.industrydocuments.ucsf.edu}} including typewritten, printed, handwritten and born-digital text.
\ourmodel-Large outperforms Donut, the previous visual SotA on DocVQA by 9 points. Top-performing single-task methods like 
UDOP~\citep{tang2022unifying} (ANLS 84.7)
typically use three components: (a) an off-the-shelf OCR system, (b) pretrained text and image encoders, and (c) additional pretraining on the IIT-CDIP scanned documents corpus. Despite using purely visual representations and no in-domain pretraining data,~\ourmodel~achieves competitive performance (ANLS 76.6). 

\textbf{InfographicVQA}~\citep{mathew2022infographicvqa} is a dataset of questions about infographics from the web.
A unique challenge of this dataset is its large images with extreme aspect ratios. Donut scales images to a fixed aspect ratio, which we speculate is the cause of its poor performance with an ANLS of 11.6.~\ourmodel-Large sets the state of the art amongst visual models with an ANLS of 40.

For both DocVQA and InfographicVQA, text-only baselines are at or near the state of the art. A T5-based model (T5 + 2D + U) with 2D positional biases~\citep{borchmann2021due} achieves ANLS of 81 on DocVQA and 46.1 on InfographicVQA. This is in part due to the text-heavy nature of the data (especially DocVQA) where visual context plays a lesser role, and the more mature pretrained text-based encoders can do the heavy lifting.

\textbf{Common trends}~~Overall,~\ourmodel~outperforms Donut in all tasks, underscoring the effectiveness of our pretraining.  We also advance the single-task state of the art on six of nine benchmarks across four domains. Scaling up from base to large results in considerable improvements on all tasks despite the base model being trained for 3$\times$ more iterations than the large model. Previous work~\citep{liu2019roberta,t5} has shown that large batch sizes and many training steps contribute greatly to the quality of the pretrained model. Results indicate that further scaling up of \ourmodel~is a promising direction.

\section{Analysis}
\label{sec:ablations}
\begin{table}[!t]
\centering
\small
\setlength{\tabcolsep}{5pt}
\begin{tabular}{l r r r}
\toprule
Pretraining & \makecell[r]{Doc\\VQA} & \makecell[r]{Widget\\Captioning} & \makecell[r]{TextCaps}\\
\midrule\
Full & 67.8 & 137.5 & 84.2~\\
~~~-- Warmup & 56.2 & 128.0 & 71.7~\\
~~~-- Masking & 55.7 & 129.4 & 77.4~\\
~~~-- Screenshot Parsing & 12.2 & 35.1 & 24.2~\\
\bottomrule
\end{tabular}
\caption{
Ablations of pretraining components. Each ablation is a modification with respect to the full model, while keeping the total number of pretraining steps constant.
}
\label{tab:ablations}
\end{table}

\textbf{Ablating pretraining objectives}~~
Table~\ref{tab:ablations} analyzes the importance of each component of our pretraining recipe on DocVQA, Widget Captioning, and TextCaps validation sets. The full pretraining method consists of a warmup reading stage on the BooksCorpus followed by pretraining using the screenshot parsing objective. For these experiments, we use the base variant with a total of 100K steps of pretraining including 30K warmup steps followed by 70K steps of screenshot parsing. The screenshot parsing ablation removes the screenshot parsing stage altogether and uses an extended warmup stage of 100K steps. The warmup ablation skips the warmup stage and directly pretrains from random initialization for 100K steps. The masking ablation uses 30K steps warmup followed by 70K steps of screenshot parsing without masking.\footnote{All models use the same hyperparameters.}

The biggest drop in performance comes from ablating the screenshot parsing stage, effectively reducing the pretraining to reading linear text. Ablating the warmup and masking is nearly equivalent on DocVQA and Widget Captioning while the warmup is slightly more important in TextCaps. Overall, our results seem to indicate that reading and understanding visually-situated language is a complex problem involving skills including recognizing text, understanding language, and incorporating visual context.

\textbf{Ablating variable-resolution inputs}~~
Figure~\ref{fig:aspect_ratio} compares various ways to convert input images into a constant number of patches. This ablation is performed on the warmup stage (Section~\ref{sec:curriculum}), where we measure full sequence accuracy. The `padded' variant maintains the original aspect ratio, but introduces significant padding, which sacrifices the effective resolution. The `stretched' variant, typically used in ViT, introduces no padding but distorts the original image. Our variable-resolution inputs get the best of both worlds by maintaining the original aspect ratio while maximally utilizing the budget specified by the sequence length. Experiments in Appendix~\ref{sec:resolution} show that this benefit leads to more effective learning, even for a task as simple as transcribing text in the input image.

\begin{figure}[!t]
\centering
\begin{tikzpicture}
\begin{axis}[
   width=0.8\columnwidth,
   height=0.5\columnwidth,
   legend cell align=left,
   legend style={at={(1, 0)},anchor=south east,font=\scriptsize, style={row sep=-0.1cm}},
   mark options={mark size=2},
   font=\small,
   xmin=0, xmax=30,
   ymin=0, ymax=75,
   ytick={20, 40, 60},
   xtick={0, 10, 20, 30},
   xticklabels={0, 10k, 20k, 30k},
   ymajorgrids=true,
   xmajorgrids=true,
   xlabel style={yshift=0.5ex,},
   xlabel=Training steps (Warmup stage),
   ylabel style={align=center},
   ylabel=Exact Match (\%),
   ylabel style={yshift=-0.5ex,}]
    \addplot[g-blue, line width=1.2pt] plot coordinates {
(30, 71.6796875)
(29, 71.6796875)
(28, 71.6796875)
(27, 70.60546875)
(26, 71.09375)
(25, 69.921875)
(24, 68.75)
(23, 68.75)
(22, 67.96875)
(21, 66.11328125)
(20, 66.40625)
(19, 64.55078125)
(18, 63.18359375)
(17, 63.671875)
(16, 63.28125)
(15, 61.23046875)
(14, 59.9609375)
(13, 58.7890625)
(12, 56.73828125)
(11, 53.61328125)
(10, 51.85546875)
(9, 48.73046875)
(8, 45.99609375)
(7, 44.140625)
(6, 37.59765625)
(5, 13.28125)
(4, 0.68359375)
(3, 0.09765625)
(2, 0.0)
(1, 0.0)
    };
    \addlegendentry{Variable}
    \addplot[g-red, line width=1.2pt, dash pattern=on \pgflinewidth off 2pt] plot coordinates {
(30, 51.66015625)
(28, 51.171875)
(26, 50.390625)
(25, 50.0)
(24, 49.4140625)
(23, 49.4140625)
(21, 46.97265625)
(19, 42.7734375)
(17, 40.8203125)
(15, 36.71875)
(13, 34.47265625)
(11, 27.9296875)
(9, 20.41015625)
(7, 10.64453125)
(6, 4.78515625)
(5, 0.29296875)
(4, 0.0)
(3, 0.0)
(2, 0.0)
(1, 0.0)
    };
    \addlegendentry{Padded}
    \addplot[black, line width=1.2pt, dash pattern=on 6pt off 6pt] plot coordinates {
(30, 66.2109375)
(28, 66.30859375)
(27, 65.91796875)
(25, 65.625)
(23, 62.890625)
(21, 62.5)
(19, 60.15625)
(17, 58.30078125)
(15, 53.3203125)
(13, 48.6328125)
(12, 49.90234375)
(10, 44.04296875)
(8, 34.5703125)
(6, 17.3828125)
(5, 2.34375)
(4, 0.0)
(3, 0.0)
(2, 0.0)
(1, 0.0)
    };
    \addlegendentry{Stretched}
\end{axis}
\end{tikzpicture} \caption{Our variable-resolution inputs prevent aspect-ratio distortion while minimizing padding.
}
\label{fig:aspect_ratio}
\end{figure}

\section{Discussion}
This section lays out some of the challenges in training general-purpose visual language understanding models, and discuss a road map for future work.

\textbf{Resolution}~~Like Donut, we found that pretraining and finetuning performance are extremely sensitive to the input resolutions.\footnote{See Appendix~\ref{sec:resolution} for a concrete comparison.} The difficulty in using high-resolution images has been a bottleneck for pixel-only models since higher resolutions often lead to longer sequence lengths. This bottleneck has in part been responsible for the dominance of OCR-based pipelines which are able to use lower image resolutions due to a dedicated text encoder.\footnote{OCR pipelines, while noisy, often result in manageable sequence lengths for large-scale text encoders.} However, steady progress with Donut and~\ourmodel~combined with recent progress in long range transformers~\citep{press2021train} provides hope that pixel-only models will bridge the gap with OCR-based pipelines.

\textbf{The visual web}~~As a first attempt towards a general-purpose visual language understanding model, we focused on simplicity both in terms of how we use the HTML source and our choice for the pretraining corpus, C4---a known public corpus used in previous work~\citep{t5} that is significantly smaller and narrower than corpora used to train the largest language models today. However, web data includes even richer multimodal signals such as videos and interactions. We posit that future versions of general-purpose visual language understanding models will benefit from better data curation. This opportunity also comes with a caveat: just like text-based models, we must be careful of harmful content on the web, which multimodal models would also be sensitive to.

\textbf{Generality}~~While we have focused on general pixel-only models, we do acknowledge that using OCR-pipelines or metadata can be appropriate or even necessary in certain domains. 
For NLP, the scaling of pretrained text based models has led to not only simpler model architectures and preprocessing, but also emergent abilities on newer tasks which were hitherto considered far too difficult~\citep{wei2022emergent}. A general-purpose model may also enable broader applications for visual language, e.g. filling in missing accessibility annotations~\citep{zhang2021screen}. 
Finally, given that the overwhelming majority of prior work has leveraged OCR-based features, it seems necessary to advance OCR-free alternatives (as this paper does) in order to enable a clearer longer-term understanding around the proper role for OCR.
The broader objective of this work is to bring pretraining for visually-situated language understanding a step closer to text-based counterparts and pave the way for similar benefits from data and model scaling. 

\section{Related Work}
To the best of our knowledge, no prior work has pretrained and evaluated a visually-situated language understanding model on tasks spanning all four domains of documents, illustrations, user interfaces, and natural images.~\footnote{Some prior approaches have been evaluated on two domains.} We build on prior work primarily focused on a single domain and briefly highlight the similarities as well as the points of departure with respect to such work here.

\textbf{Document understanding}~~State-of-the-art models in this domain are based on a pipeline of an external OCR system and a model that combines images and OCR annotations~\citep{docformer,powalski2021going,layoutlmv2}, \textit{inter alia}. Prominent representatives are LayoutLMv3~\citep{layoutlmv3}, which uses a simplified transformer-based architecture and losses that encourage patch--OCR alignment. TILT~\citep{powalski2021going} pretrains a text decoder and an image + OCR-output encoder followed by intermediate finetuning on multiple QA tasks. \ourmodel~is more closely related to Donut and Dessurt~\citep{dessurt}, both image-to-text models without OCR at inference time; the main difference stems from our more powerful pretraining task from ground truth structures and resolution flexibility enabling transfer to a variety of visual language domains.

\textbf{UI understanding}~~Models in this group have focused solely on the UI domain using pretraining data from mobile and web apps. While some models use image-only inputs~\citep{Liu2018LearningDS, Chen2020UnblindYA}, higher accuracy approaches tend to benefit from often-noisy structures of view hierarchies~\citep{li-etal-2020-mapping} and element annotations, e.g. UIBert~\citep{uibert}, ActionBert~\citep{actionbert}, VUT~\citep{li2021vut}. One exception is concurrent work~\citep{li2023spotlight} which achieves comparable performance with image-only inputs. The screen parsing task~\citep{wu2021screen}, while similar in name, is an amalgamation of pipelines over domain-specific structures that are not intended to produce transferable representations.

\textbf{Natural image understanding}~~Pix2Seq uses the image-to-text architecture for core vision tasks such as object detection and instance segmentation~\citep{chen2022unified,chen2021pix2seq}. Additionally, a variety of model architectures~\citep{singh2019towards,sidorov2019textcaps,wang2020multimodal} and objectives~\citep{yang2021tap} have been proposed for understanding natural images containing short segments of text (e.g. street signs). The predominant source of pretraining data has been image-caption pairs often in conjunction with the output of OCR~\citep{pali,yang2021tap}. GIT2~\citep{wang2022git}, the pixel-only SoTA, learns from 12.9 billion image-caption pairs and is about 4 times larger than \ourmodel--- it outperforms our model significantly on natural images (TextCaps) but underperforms on illustrations (OCR-VQA). PaLI benefits from using a pipeline with OCR, obtaining higher performance on TextCaps. These methods have not been evaluated on more text-dense input domains.

\textbf{Illustrations}~~Models for illustrations have not been fully pretrained on large scale data, perhaps because such data is not readily available. Some components of such models, e.g. T5 and TaPas~\citep{eisenschlos-etal-2020-understanding} used in the VL-T5 and VisionTaPas models of ~\citet{masry-etal-2022-chartqa} or LATr's OCR output encoder~\citep{biten2022latr} have been pretrained on digital-born or OCR-ed documents. Our approach outperforms current SotA models, without relying on other intermediate structures.

\textbf{Models learning from markup structure}~~MarkupLM~\citep{li2021markuplm} and Webformer~\citep{wang2022webformer} learn encoders of HTML from web pages. HTLM~\citep{aghajanyan2021htlm} and  CM3~\citep{aghajanyan2022cm3} are generative models of simplified HTML to enable zero-shot prompting with text and natural images. Im2Tex~\citep{deng2017image} is conceptually the most relevant in showing that a pixel-only parser can be learned from freely-available pairs of markup and renders, but doesn't focus on transferring this signal to wider applications.

\textbf{Datasets}~~We have selected datasets representing challenges in visually-situated language understanding in a variety of domains, but our selection is not aimed to be exhaustive. The DUE benchmark~\citep{borchmann2021due} focuses on a more limited domain of visual document understanding (e.g. excluding natural images and UIs), but integrates a more comprehensive set of tasks within the document understanding domain.

\bibliography{main}
\bibliographystyle{icml2023}

\clearpage
\appendix

\section{Resolution in visually-situated language understanding tasks}
\label{sec:resolution}


\begin{figure*}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
   width=0.6\textwidth,
   height=0.4\textwidth,
   font=\small,
   xmin=32768, xmax=5000000,
   ymin=0, ymax=80,
   xmode=log,
   ytick={20, 40, 60},
   xtick={32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304},
   xticklabels={128, 256, 512, 1024, 2048, 4096, 8192, 16384},
   xlabel=Input sequence length]
\end{axis}
\begin{axis}[
   axis x line*=top,
   width=0.6\textwidth,
   height=0.4\textwidth,
   legend cell align=left,
   legend style={at={(1, 0)},anchor=south east,font=\small},
   mark options={mark size=2},
   font=\small,
   xmin=32768, xmax=5000000,
   ymin=0, ymax=80,
   xmode=log,
   ytick={20, 40, 60},
   xtick={32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304},
   xticklabels={$2^{15}$, $2^{16}$, $2^{17}$, $2^{18}$, $2^{19}$, $2^{20}$, $2^{21}$, $2^{22}$},
   ymajorgrids=true,
   xmajorgrids=true,
   xlabel=Effective number of pixels,
   ylabel style={align=center},
   ylabel=ANLS (\%),
   ylabel style={yshift=-0.5ex,}]
   
    \addplot[mark=o, g-orange, line width=1.2pt] plot coordinates {
(32768, 10.341893196105957)
(65536, 16.964523315429688)
(131072, 32.167327880859375)
(262144, 56.5987663269043)
(524288, 69.2957992553711)
(1048576, 72.01505279541016)
    };
    \addlegendentry{\ourmodel-base}
    
    \addplot[mark=square, g-green, line width=1.2pt] plot coordinates {
(409600, 40.95)
(921600, 52.96)
(1228800, 56.2)
(4915200, 59.56)
    };
    \addlegendentry{Donut}
    \addplot[black, dash pattern=on \pgflinewidth off 5pt, line width=1.2pt] plot coordinates {
(50176, 0)
(50176, 100)
    };
    \addlegendentry{ViT pretraining}
    \addplot[dotted, black, line width=1.2pt] plot coordinates {
(262144, 0)
(262144, 100)
    };
    \addlegendentry{ViT finetuning}
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
   width=0.4\textwidth,
   height=0.4\textwidth,
   mark options={mark size=2},
   font=\small,
   xmin=32768, xmax=1048576,
   ymin=0, ymax=350,
   ytick={50, 100, 150, 200, 250, 300},
   xmode=log,
   xtick={32768, 65536, 131072, 262144, 524288, 1048576},
   xticklabels={128, 256, 512, 1024, 2048, 4096},
   xlabel=Input sequence length]
\end{axis}
\begin{axis}[
   axis x line*=top,
   width=0.4\textwidth,
   height=0.4\textwidth,
   legend cell align=left,
   legend style={at={(1, 1)},anchor=north east,font=\small},
   mark options={mark size=2},
   font=\small,
   xmin=32768, xmax=1048576,
   ymin=0, ymax=350,
   xmode=log,
   ytick={50, 100, 150, 200, 250, 300},
   xtick={32768, 65536, 131072, 262144, 524288, 1048576},
   xticklabels={$2^{15}$, $2^{16}$, $2^{17}$, $2^{18}$, $2^{19}$, $2^{20}$},
   ymajorgrids=true,
   xmajorgrids=true,
   xlabel=Effective number of pixels,
   ylabel style={align=center},
   ylabel=Examples per second,
   ylabel style={yshift=-0.5ex,}]
   
    \addplot[mark=o, g-orange, line width=1.2pt] plot coordinates {
(32768, 254.66)
(131072, 191)
(1048576, 62.18)
    };
    \addlegendentry{\ourmodel-base}
    
    \addplot[mark=x, g-blue, line width=1.2pt] plot coordinates {
(32768, 79.82)
(131072, 68.56)
(1048576, 20.18)
    };
    \addlegendentry{\ourmodel-large}
\end{axis}
\end{tikzpicture} \caption{Overview of the impact of resolution on the DocVQA task. Note that the bottom axis only applies to \ourmodel. \ourmodel~is also the only model that adapts to various resolutions seamlessly, without any retraining or post-hoc parameter creation. (Left) In both Donut and \ourmodel, we show clear benefits from use larger resolutions. (Right) Inference speed measured by auto-regressive decoding (max decoding length of 32 tokens) on the validation set of DocVQA using a v3-8 Cloud TPU.}
\label{fig:resolution}
\end{figure*}

Previous methods rescale input images to fixed resolutions, which can introduce severe aspect ratio distortions for inputs such as webpages and documents. In contrast, we prevent aspect ratio distortion by rescaling input images  up or down such that we extract the maximal number of patches that fit within the given sequence length (Figure~\ref{fig:input_rep}).  

Figure~\ref{fig:resolution} gives an overview of the importance of input resolutions in visually-situated language understanding tasks. Though \ourmodel~is more efficient at making use of the input resolution, both \ourmodel~and Donut require high resolutions to perform well on DocVQA (note the log scale). For example, we only see significantly diminishing returns after about 1M pixels (4096 patches of $16\times16$ pixels for \ourmodel~and $1024\times1024$ for fixed-resolution models). However, ViT models typically pretrain with resolutions of $224\times224$ and finetune with up to $512\times512$. This is a subtle but critical detail that makes using standard ViT out of the box suboptimal.

On the right of Figure~\ref{fig:resolution}, we also present example inference speeds on a v3-8 Cloud TPU when performing inference on DocVQA. At full resolution (4096 sequence length or 1M pixels), the base model processes 62 documents per second, and the large model processes 20 documents per second.

\clearpage
\section{Full Results}
\label{sec:full_results}

\begin{table*}[t]
\setlength{\tabcolsep}{9pt}
\small
\centering
\begin{tabular}{llrrrrrrrrr}
\toprule
\multicolumn{2}{l}{Method} & \makecell[c]{Chart\\QA} & \makecell[c]{AI2D} & \makecell[c]{OCR\\VQA} & \makecell[c]{Ref\\Exp} & \makecell[c]{Widget\\Cap} & \makecell[c]{Screen2\\Words} & \makecell[c]{Text\\Caps} &  \makecell[c]{Doc\\VQA} & \makecell[c]{Info\\VQA}\\
\midrule
\multirow{13}{*}{\rotatebox{90}{Pipelined}}&TILT & - & - & - & - & - & - & - & 87.1\rlap{$^*$} & -~\\
&VUT & - & - & - & - & 94.8 & 64.3 & - & - & -~\\
&TAP & - & - & - & - & - & - & 99.5 & - & -~\\
&LATr & - & - & 67.5 & - & - & - & - & - & -~\\
&PLC & - & - & - & - & 97.0 & - & - & - & -~\\
&\makecell[l]{T5 + 2D + U} & - & - & - & - & - & - & - & 81.0 & 46.1~\\
&RoBERTa & - & - & - & - & - & - & - & 69.5 & -~\\
&LayoutLMv3 & - & - & - & - & - & - & - & 83.4 & -~\\
&DQA-NET & - & 38.5 & - & - & - & - & - & - & -~\\
&UI Bert & - & - & - & 90.8 & - & - & - & - & -~\\
&M4C & - & - & 63.9 & - & - & - & 81 & - & 14.7~\\
&VisionTaPas & 45.5 & - & - & - & - & - & - & - & -~\\
& PaLI & - & - & - & - & - & - & \bf{160.4} & - & -~\\
& UDOP & - & - & - & - & - & - & - & \bf{84.7} & \bf{47.4}~\\
\midrule
\multirow{5}{*}{\rotatebox{90}{Pixel only}} 
&GIT2 & - & - & 70.3\rlap{$^*$} & - & - & - & 145.0 & - & -~\\
&Donut & 41.8 & 30.8 & 66.0 & - & 127.4 & 56.4 & 74.4 & 67.5 & 11.6~\\
\cmidrule{2-11}
&\ourmodel-Base & 56.0 & 40.9 & 69.4 & 92.2 & 133.1 & 107.0 & 88.0 & 72.1 & 38.2~\\
&\ourmodel-Large & \bf{58.6} & \bf{42.1} & \bf{71.3} & \bf{94.2} & \bf{136.7} & \bf{109.4} & 95.5 & 76.6 & 40.0~\\
\bottomrule
\end{tabular}
\caption{Amongst single-task single-model methods,~\ourmodel~achieves state-of-the-art results on 6 out of 9 benchmarks spanning 4 domains. * indicates that the method used additional labeled data from other tasks and are not directly comparable to single task methods. VisionTaPas uses a table extraction tool. DQA-NET uses diagram processing tools for detecting arrows, blobs, etc in addition to standard OCR. UI Bert and VUT use Android view hierarchies. All other non-image methods use standard OCR.}
\label{tab:full_res}
\end{table*}

Table~\ref{tab:full_res} reports full results for pipeline and pixel-only methods. For fair comparison and ease of experimentation, we focus on single-model and single-task baselines trained on standard splits. Several (per-task) SotA~\citep{li2021vut,masry-etal-2022-chartqa} use  domain-specific inputs (e.g. view hierarchies for UIs or gold data tables for charts) making it difficult to apply them to other domains. 

\begin{table*}[b]
\renewcommand{\arraystretch}{0.9}
\centering
\begin{tabular}{l l p{8cm}}
\toprule
Dataset & Domain & Description~\\
\midrule
OCR-VQA & Illustrations & VQA over book covers.~\\
ChartQA & Illustrations & VQA over charts (visualization of tabular data)\\
AI2D & Illustrations & VQA over science diagrams\\
RefExp & UIs &  Detect UI component matching a natural language query~\\
Widget Captioning & UIs & Captioning a UI component on a screen~\\
Screen2Words & UIs & Captioning a UI screen to describe functionality\\
TextCaps  & Natural images & Captioning of natural images containing text\\
DocVQA & Documents & VQA over scanned documents.~\\
InfographicsVQA & Documents & VQA over high-res infographics.\\
\bottomrule
\end{tabular}
\caption{Summary our proposed diverse benchmark for visually-situated language understanding}
\label{tab:datasets}
\end{table*}

\pagebreak
\section{Finetuning Dataset Details}
\label{sec:finetuning_datasets}
Table~\ref{tab:datasets} show the datasets in our benchmark for visually-situated language understanding.


\clearpage
\section{Hyperparameters}
\label{sec:hyperparams}
The base and large models are finetuned with an input sequence length of 4096 and 3072 respectively, except the base model on InfographicVQA which benefits from a longer sequence length of 6144. We cannot use a longer sequence length for the large variant due to TPU/GPU memory constraints. 
We finetune for 5000 or 10000 steps with a batch size of 32, 128, or 256, with hyperparameter tuning and early stopping based on the validation set. Table~\ref{tab:hyperparams} contains hyperparameter values for all tasks.

\begin{table*}[t]
\centering
\begin{tabular}{ l r r r c r r r} 
\toprule
\multirow{3}{*}{Dataset} & \multicolumn{3}{c}{Base} &~~~& \multicolumn{3}{c}{Large}\\
\cmidrule{2-4}\cmidrule{6-8}
& Seq Len & Batch & Steps & & Seq Len & Batch & Steps \\
\midrule
DocVQA & 4096 & 256 & 10000 & & 3072 & 128 & 10000 ~\\
InfographicVQA & 6144 & 64 & 10000 & & 3072 & 128 & 10000 ~\\
AI2D & 4096 & 32 & 5000 & & 3072 & 32 & 5000 ~\\
ChartQA & 4096 & 256 & 10000 & & 3072 & 128 & 10000 ~\\
OCR-VQA & 4096 & 256 & 10000 & & 3072 & 128 & 10000 ~\\
RefExp & 4096 & 256 & 10000 & & 3072 & 128 & 10000 ~\\
Screen2Words & 4096 & 32 & 10000 & & 3072 & 32 & 10000 ~\\
Widget Cap. & 4096 & 256 & 5000 & & 3072 & 128 & 5000 ~\\
TextCaps & 4096 & 256 & 5000 & & 3072 & 128 & 5000 ~\\
\bottomrule
\end{tabular}
\caption{Model hyperparameters}
\label{tab:hyperparams}
\end{table*}

\section{Warmup Stage Data}
\label{sec:warmup_example}
\begin{figure*}[b]
\centering
\begin{minipage}{.48\textwidth}
\frame{\includegraphics[width=\textwidth]{figures/warmup.png}}
\end{minipage}
\begin{minipage}{.05\textwidth}
\centering
\large
$\rightarrow$
\end{minipage}
\begin{minipage}{.44\textwidth}
\small
\begin{code}
The elves, it seemed, were
possessed of some mysterious
power over the arts; without eve
\end{code}
\end{minipage}
\caption{Example of input-output pairs during the warmup stage.}
\label{fig:warmup}
\end{figure*}

For the warmup stage, we create images of text snippets from the BooksCorpus~\cite{books} with random colors (uniformly sampled from all possible RGB values), fonts (uniformly sampled from all possible Google Fonts~\footnote{\url{https://developers.google.com/fonts}}), and font sizes (uniformly sampled from 12pt to 36pt) on a white background. The text snippets are up to 128 bytes long. The width of the images are 640 pixels, and the text is wrapped of it exceeds the width of the image. The height of the image is fit to the content height. The text is unmasked as this stage is intended purely as a learning-to-read task.


Exposing the model to a short “warmup” stage of simply learning to read, results in a strong curriculum learning effect where
(1) pretraining is more stable and converges faster, and (2) we observe better finetuning performance. Figure~\ref{fig:warmup} shows an example of rendered text from the BooksCorpus with its ``parse''.


\section{Pretraining Data}
\label{sec:pretraining_ex}
The pretraining data is constructed from URLs in the C4 corpus. We collect 80M (about one third of the total number of documents) pairs of screenshots paired with their HTML source. The screenshots have a width of 1024 pixels, and the height of the image is fit to the content height.

The figures below show screenshots of our pretraining data along with ground-truth and predicted parses.

\begin{figure*}
\small
\centering
\frame{\includegraphics[width=0.5\textheight]{figures/ex1.png}}
\begin{tcolorbox}
\paragraph{Ground-truth Parse}
\begin{verbatim}
<<<<CrossFit Thunderhawk | Rio Rancho>
   <dedicated to promote healthy kids and teens in Rio Rancho, NM>>
  <<Home> <About> <Schedule> <Media> <Blog> <Contact Us> <Free Class>>>
 <<Drop-ins>
  <Bring your child in for a drop-in to get a WOD in!>>
 <<<If you are visiting from out of town or traveling for club sports,
    make sure your child’s routine is not disrupted. Bring them in for
    a drop in to get a WOD in!>
   <<1-day CrossFit Athlete $15>
    <1-day Competitor $25>>>
  <<Become A Member>
   <We’d love to meet you and show you around.>>>>
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\paragraph{Predicted Parse}
\begin{verbatim}
<<<<img_src=thunderhawk-logo-white img_alt=Thunderhawk Sports & Fitness>
   <Thunderhawk Sports & Fitness>>
  <<Home> <About> <Programs> <Team> <Blog> <Contact Us> <Get Started>>>
 <<<Drop-Ins>
   <Bring your child in for a drop-in to get a workout>>
  <<<If you are visiting from out of town or traveling for club sports,
     make sure your child’s routine is not disrupted. Bring them to our
     drop-in for a full session!> <<1:1 drop-in for
\end{verbatim}
\end{tcolorbox}
\end{figure*}

\begin{figure*}
\small
\centering
\frame{\includegraphics[width=0.5\textheight]{figures/ex4.png}}
\begin{tcolorbox}
\paragraph{Ground-truth Parse}
\begin{verbatim}
<, I tried something Valentine's themed. If you'd like to help
raise money for fighting children's cancer you can follow the link right
above and help out, too. As inspiration for this semi-homemade recipe, 
I looked at the two recipes on the bag of sweet dough, I got an idea and 
today I'm going to share with you how that worked out.
\xa0 I got the bag of Sweet Dough using a coupon for a free product
that was sent to my by Rhodes BakeNServ in exchange for testing out
their products and sharing the results with all of you; no other form of 
compensation was received.>
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\paragraph{Predicted Parse}
\begin{verbatim}
<, I tried something Valentine's themed. If you'd like to help
out, I think you'd go right ahead and do a post. Click on the link right
above and help out, too. As inspiration for this semi-homemade recipe, 
I've shared up two recipes on the bag of sweet dough. I got an idea and 
today I'm going to share with you the second one. 
Thank you for any of the amazing baking ideas plus this free product 
that was sent to my by Rhodes BakeNServ in exchange for testing. 
I'm really excited and sharing this recipe with all of you
\end{verbatim}
\end{tcolorbox}
\end{figure*}

\begin{figure*}
\small
\centering
\frame{\includegraphics[width=0.5\textheight]{figures/ex5.png}}
\begin{tcolorbox}
\paragraph{Ground-truth Parse}
\begin{verbatim}
<<<100<FAST FREE SHIPPING>> 
 <img_alt=Velvet Eyewear> 
 <<<<Fringe Benefits>
    <<Posted by> <Lindsay Sperin> <on> <August 19, 2016>>> 
   <<img_src=img>
    <Fall is undeniably the best season for fashion
     for a multitude of reasons.> 
    <img_src=img>>>
  <<NEWS> 
   <<Polarized vs. UV Protection - What's The Difference?>
    <What's Hot in The Hamptons>>>> 
 <<img_src=en-us img_alt=en> <English>>>
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\paragraph{Predicted Parse}
\begin{verbatim}
<<<10<FAST FREE SHIPPING>> 
 <img_alt=Velvet>
 <<<<Fringe Benefits>
    <<Posted by> <Velvet Fashion> <on> <October 1, 2018>>>
   <<Fall is undeniably the best season for fashion 
     for a multitude of reasons.> 
    <img_alt=Fringe Benefits>>>
  <<Search> 
   <<Polarized vs. UV Protection: Velvet's Best Sunscreen> 
    <The Best Sunblock Sunscreen>>>>>
\end{verbatim}
\end{tcolorbox}
\end{figure*}

\begin{figure*}
\small
\centering
\frame{\includegraphics[height=0.45\textheight]{figures/ex2.png}}
\begin{tcolorbox}
\paragraph{Ground-truth Parse}
\begin{verbatim}
<<Menu>
 <img_src=ftg_webheader>
 <<<Spin-Off Games>
   <<Fairytale Games is a growing universe. Because of this, we have and
   will continue to grow spin-off games that utilize characters,
   storylines, and even poke fun of our games. Keep checking back and
   you just might be surprised at what you see!>
    <<Rumplestiltskin!>
     <Super Fairytale Fighters 2>>
    <<<Share this:>
      <<Twitter> <Facebook>>>
     <Loading...>>>> 
  <<Leave a Reply> 
   <<<Your email address will not be published.> 
     <<Required fields are marked> <*>>>
    <<Comment> <*>>>>>>
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\paragraph{Predicted Parse}
\begin{verbatim}
<<Menu>
 <img_src=cropped-blogheader>
 <<<Fairytale Games>
   <<Fairytale Games is a growing universe. Because of this, we are
     excited to continue to grow spin-off games that utilize characters,
     storylines, and even poke fun of our games. Keep checking back and
     you just might be surprised at what you see!> 
    <<Fairytale Games>
     <Fairytale Games on Steam>>
    <<<Share this:>
      <<Twitter> <Facebook>>>
     <Loading...>>>> 
  <<Leave a Reply> 
   <<<Your email address will not be published.> 
     <<Required fields are marked
\end{verbatim}
\end{tcolorbox}
\end{figure*}

\begin{figure*}
\small
\centering
\frame{\includegraphics[height=0.5\textheight]{figures/ex3.png}}
\begin{tcolorbox}
\paragraph{Ground-truth Parse}
\begin{verbatim}
<<<<Coronavirus Update! We are open and ready to help you.>
   <We are conducting most of our appointments via phone to help prevent 
    the spread of the virus.>>
  <Chapter 13 Coronavirus Update>> 
 <<img_src=Logoo img_alt=Stamps & Stamps Attorneys At Law>
  <img_src=Phone>
  <Contact for a free Initial Consultation> 
  <<Call Us> <(937) 247-6447>> 
  <<Text Us> <(937) 265-6418>>> 
 <<Home> <About> <Articles> <Videos> <Testimonials> <Tax Relief> <News> 
  <Podcasts> <Rate Us> <Contact>> 
 <<We can provide the guidance you need to get through stressful family> 
  <disputes with your rights and interests intact.>> 
 <<<img_src=Bankruptcy img_alt=Bankruptcy Overview>
   <<Bankruptcy> <Overview>>>
  <img_src=Criminal-Defense1
   img_alt=Criminal Defense & Traffic Offenses>>>
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\paragraph{Predicted Parse}
\begin{verbatim}
<<<<Coronavirus Update! We are open and ready to help you.> 
   <We are conducting most of our appointments via phone to help prevent 
    the spread of infection.>> 
  <CLICK HERE FOR MORE INFO>> 
 <<img_src=logo img_alt=Stamps & Stamps Attorneys At Law> 
  <img_src=phone>
  <<<Call Us> <(904) 222-2222>> 
   <<Text Us> <(904) 222-2222>>>>
 <<Home> <About> <Articles> <
\end{verbatim}
\end{tcolorbox}
\end{figure*}



\end{document}