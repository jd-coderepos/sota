\PassOptionsToPackage{square,comma,numbers,sort&compress}{natbib}
\documentclass{article}





\usepackage[final]{neurips_2020}






\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{threeparttable}
\usepackage{tablefootnote}
\usepackage{amssymb}  
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pifont} 
\usepackage{graphicx}
\usepackage{float}

\usepackage{color}
\usepackage{xcolor}

\newcommand{\hzt}[1]{{\color{red}{[hzt: #1]}}}
\newcommand{\yw}[1]{{\color{red}{[yw: #1]}}}
\newcommand{\pz}[1]{{\color{blue}{[#1]}}}






\newcommand{\op}{\mbox{op}}
\newcommand{\Eop}{\operatorname{E}}
\newcommand{\mean}[2][]{\ensuremath{\Eop_{#1}\left\{#2\right\}}}
\newcommand{\meanop}{\operatorname{mean}}
\newcommand{\Mean}[2][]{\ensuremath{\meanop_{#1}\left\{#2\right\}}}
\newcommand{\aveop}{\Eop}
\newcommand{\ave}[2][]{\ensuremath{\aveop_{#1}\left\{#2\right\}}}
\newcommand{\varop}{\operatorname{var}}
\newcommand{\var}[2][]{\ensuremath{\varop_{#1}\left\{#2\right\}}}
\newcommand{\stdevop}{\operatorname{stdev}}
\newcommand{\stdev}[2][]{\ensuremath{\stdevop_{#1}\left\{#2\right\}}}
\newcommand{\covop}{\operatorname{cov}}
\newcommand{\cov}[2][]{\ensuremath{\covop_{#1}\left\{#2\right\}}}
\newcommand{\eff}{\operatorname{eff}}
\newcommand{\Prop}{\operatorname{P}}
\newcommand{\prob}[1]{\ensuremath{\Prop\left[#1\right]}}
\newcommand{\Volume}{\operatorname{vol}}
\newcommand{\volume}[1]{\ensuremath{\Volume\left(#1\right)}}

\newcommand{\erf}{\operatorname{erf}}
\newcommand{\sgnop}{\operatorname{sgn}}
\newcommand{\sgn}[1]{\ensuremath{\sgnop\left(#1\right)}}

\newcommand{\KLop}{\operatorname{D}}
\newcommand{\KL}[2]{\KLop\left(#1\lVert#2\right)} \newcommand{\KLsym}[2]{\KLop\left(#1,#2\right)}   


\newcommand{\inproduct}[2]{\left\langle#1,#2\right\rangle}

\newcommand{\cardop}{\operatorname{card}}
\newcommand{\card}[1]{\ensuremath{\cardop\left(#1\right)}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\vol}{\operatorname{vol}}
\newcommand{\convop}{\operatorname{conv}}
\newcommand{\conv}[1]{\ensuremath{\convop\left(#1\right)}}

\newcommand{\diagop}{\operatorname{diag}}
\newcommand{\diag}[1]{\ensuremath{\diagop\left(#1\right)}}
\newcommand{\cof}{\operatorname{cof}}           \newcommand{\adj}{\operatorname{adj}}           \newcommand{\traceop}{\operatorname{Tr}}
\newcommand{\tr}[1]{\ensuremath{\traceop\left(#1\right)}}
\newcommand{\rankop}{\operatorname{rank}}
\newcommand{\rank}[1]{\ensuremath{\rankop\left(#1\right)}}
\newcommand{\Nullop}{\operatorname{null}}
\newcommand{\Null}[1]{\ensuremath{\Nullop\left(#1\right)}}
\newcommand{\rangeop}{\operatorname{range}}
\newcommand{\range}[1]{\ensuremath{\rangeop\left(#1\right)}}
\newcommand{\Dimop}{\operatorname{dim}}
\newcommand{\Dim}[1]{\ensuremath{\Dimop\left(#1\right)}}
\newcommand{\spanop}{\operatorname{span}}
\newcommand{\spn}[1]{\ensuremath{\spanop\left\{#1\right\}}}
\newcommand{\vectop}{\operatorname{vec}}
\newcommand{\vect}[1]{\ensuremath{\vectop\left(#1\right)}}
\newcommand{\condop}{\operatorname{cond}}
\newcommand{\cond}[1]{\ensuremath{\condop\left(#1\right)}}

\newtheorem{thm}{Theorem}\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{obs}[thm]{Observation}
\newtheorem{defn}{Definition}
\newtheorem{fact}{Fact}[thm]
\newtheorem{assum}{Assumption}
\newtheorem{rmk}[thm]{Remark}



 







\newcommand{\led}[1]{\overset{\text{\ding{#1}}}{\leq}}
\newcommand{\lqq}[1]{\overset{\text{\ding{#1}}}{=}}
\newcommand{\lee}[1]{\overset{\text{\ding{#1}}}{=}}
\newcommand{\ged}[1]{\overset{\text{\ding{#1}}}{\geq}}
\newcommand{\Diag}{\text{Diag}}
\newcommand{\st}{{\text{s.t.}}}
\newcommand{\noise}{\textup{\scriptsize{noise}}}
\newcommand{\thres}{\textup{\scriptsize{thres}}}
\newcommand{\hy}[1]{\textcolor{red}{#1}}
\newcommand{\Rss}[2]{{\mathbb{R}^{#1\times#2}}}
\newcommand{\Rs}[1]{{\mathbb{R}^{#1}}}
\newcommand{\kl}{\textbf{KL}}


\newcommand{\epsi}[1]{{\epsilon_{#1}}}

\newcommand{\wmi}[1]{\bm{w}_{#1}}
\newcommand{\wmii}[1]{\bm{w}^{#1}}
\newcommand{\task}{\mathcal{T}}
\newcommand{\Ti}[1]{T_{#1}}
\newcommand{\T}{T}
\newcommand{\bi}[1]{b_{#1}}
\newcommand{\wms}{\bm{w}^{*}}
\newcommand{\wmsi}[1]{\bm{w}^{*}_{#1}}
\newcommand{\wmssi}[1]{\bm{w}^{*}_{#1,i}}
\newcommand{\wsi}[1]{\widehat{\bm{w}}^{*}_{#1}}
\newcommand{\SSi}[1]{D_{T_{#1}}}
\newcommand{\Ee}{E}
\newcommand{\BRML}{}
\newcommand{\phis}{\widehat{\phi}}

\newcommand{\etai}[1]{\eta_{#1}}
\newcommand{\sigmai}[1]{\sigma_{(#1)}}
\newcommand{\xmi}[1]{\bm{x}_{#1}}
\newcommand{\ymi}[1]{\bm{y}_{#1}}
\newcommand{\zmi}[1]{\bm{z}_{#1}}
\newcommand{\zmbari}[1]{\bar{\bm{z}}_{#1}}
\newcommand{\zmhati}[1]{\hat{\bm{z}}_{#1}}
\newcommand{\zmti}[1]{\widetilde{\bm{z}}_{#1}}
\newcommand{\ami}[1]{\bm{a}_{#1}}
\newcommand{\gmi}[1]{\bm{g}^{#1}}
\newcommand{\gti}[1]{\widetilde{\bm{g}}^{#1}}
\newcommand{\ghi}[1]{\widehat{\bm{g}}^{#1}}
\newcommand{\fii}[1]{f^{#1}}
\newcommand{\xti}[1]{\tilde{\bm{x}}^{#1}}
\newcommand{\vti}[1]{\tilde{\bm{v}}^{#1}}
\newcommand{\zmii}[2]{\bm{z}^{#1}_{#2}}
\newcommand{\zti}[2]{\widetilde{\bm{z}}^{#1}_{#2}}
\newcommand{\xmii}[2]{\bm{x}^{#1}_{#2}}
\newcommand{\dmii}[2]{\bm{\delta}^{#1}_{#2}}
\newcommand{\phii}[2]{\bm{\phi}^{#1}_{#2}}

\newcommand{\ci}[1]{c_{#1}}
\newcommand{\Sb}{\bar{\mathcal{S}}}
\newcommand{\Sbi}[1]{{\bar{\mathcal{S}}}_{#1}}
\newcommand{\Sti}[1]{{\widetilde{\mathcal{S}}}_{#1}}
\newcommand{\Sm}{\mathcal{S}}
\newcommand{\proj}{\mathcal{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\J}{\mathcal{J}}

\newcommand{\0}{\bm{0}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}

\newcommand{\am}{\bm{a}}
\newcommand{\bmm}{\bm{b}}
\newcommand{\dm}{\bm{d}}
\newcommand{\emm}{\bm{e}}
\newcommand{\gm}{\bm{g}}
\newcommand{\wm}{\bm{\theta}}
\newcommand{\xm}{\bm{x}}
\newcommand{\ym}{\bm{y}}
\newcommand{\hmm}{\bm{h}}
\newcommand{\sm}{\bm{s}}
\newcommand{\zm}{\bm{z}}
\newcommand{\um}{\bm{u}}
\newcommand{\vm}{\bm{v}}

\newcommand{\Am}{\bm{A}}
\newcommand{\Bm}{\bm{B}}
\newcommand{\Dm}{D}
\newcommand{\Hm}{\bm{H}}
\newcommand{\Imm}{\bm{I}}
\newcommand{\Gm}{\bm{G}}

\newcommand{\Oc}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\Ocs}[1]{\mathcal{O}\big(#1\big)}
\newcommand{\R}{\mathbb{R}}
\newcommand{\X}{\bm{\mathcal{X}}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\Om}{\mathcal{O}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\EEi}[1]{\mathbb{E}(#1)}
\newcommand{\Pro}{\mathbb{P}}
\newcommand{\PPi}[1]{\mathbb{P}(#1)}


\newcommand{\norm}[1]{\left\|#1\right\|}
 

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand{\E}{\mathbb{E}}
\newcommand{\KLD}{\text{KL}}
\newcommand{\ENT}{\text{H}}
\newcommand{\MLE}{\text{MLE}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\QQz}{q_{\bm{\phi}}}
\newcommand{\PPz}{p}
\bibliographystyle{chicago}

\title{Improving GAN Training with Probability Ratio Clipping and Sample Reweighting}






\author{Yue Wu$^1$,~~ Pan Zhou$^2$,~~ Andrew Gordon Wilson$^3$,~~ Eric P. Xing$^{1,4}$,~~ Zhiting Hu$^{1,5}$ \\
	$^1$Carnegie Mellon University, $^2$Salesforce Research, $^3$New York University\\
	$^4$Petuum Inc., $^5$UC San Diego\\
	{ \texttt \tt wuyueholmes@outlook.com,}\ {\texttt \tt pzhou@salesforce.com,}\ {\texttt\tt aglwilson@gmail.com,}\\ 
	{\texttt \tt epxing@andrew.cmu.edu,}\ {\texttt \tt zhitinghu@gmail.com}
}

\begin{document}

\newtheorem{claim}{Claim}[section]
\newtheorem{prop}{Proposition}[section]
\newenvironment{psketch}{\renewcommand{\proofname}{Proof sketch}\proof}{\endproof}
\maketitle
  


\begin{abstract}
Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) often suffer from inferior performance due to unstable training, especially for text generation. To solve this issue, we propose a new variational GAN training framework which enjoys superior  training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that improves discriminator training by downplaying bad-quality fake samples.  Moreover, our  variational GAN framework  can provably overcome the training issue in many GANs that an optimal discriminator cannot provide any  informative gradient to training generator. 
By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.\footnote{Code available at: \href{https://github.com/Holmeswww/PPOGAN}{github.com/Holmeswww/PPOGAN}}







\end{abstract}

\section{Introduction}
\label{sec:intro}
Generative adversarial networks (GANs)~\citep{goodfellow2014generative} have achieved remarkable success in image and video synthesis~\cite{DCGAN,brock2018large,mathieu2015deep}. However, it is usually hard to train a GAN well, because the training process is commonly unstable, subject to disturbances and even collapses. 
To alleviate this issue, substantial efforts have been paid to improve the training stability from different perspectives, 
e.g., divergence minimization \cite{nowozin2016f,nock2017f}, Wasserstein distance with Lipschitz continuity of the discriminator \cite{WGAN,wgangp,wwgan}, energy-based models \cite{zhao2016energy,berthelot2017began}, to name a few. 

In spite of the above progresses, the instability in training has not been well resolved~\cite{chu2020smoothness}, since it is difficult to well balance the strength of the generator and the discriminator. What is worse, such an instability issue is exacerbated in text generation due to the sequential and discrete nature of text~\cite{fedus2018maskgan,caccia2018language,hu2017toward}. Specifically, the high sensitivity of text generation to noise and the underlying errors caused by sparse discriminator signals in the generated text can often result in destructive updates to both generator and discriminator, enlarging the instability in GANs.  




 


In this work, we develop a novel variational GAN training framework to improve the training stability, which is broadly applicable to GANs of a variety of architectures for image and text generation. This training framework is derived from a variational perspective of GANs~\citep{hu2018deep} and the resulting connections to reinforcement learning (in particular, RL-as-inference)~\cite{abdolmaleki2018maximum,PPO} and other rich literature~\cite{grover2019bias,hu2017unifying,burda2015importance}. 
Our approach consists of two stabilization techniques, namely, probability ratio clipping and sample re-weighting, for stabilizing the generator and  discriminator respectively.
{\bf (1)} Under the variational perspective, the generator update is subject to a KL penalty on the change of the generator distribution. This KL  penalty closely resembles that in the popular Trust-Region Policy Optimization (TRPO)~\cite{TRPO} and its variant, i.e., Proximal Policy Optimization (PPO)~\cite{PPO}. This connection motivates a simple surrogate objective with a clipped probability ratio between the new generator and the old one. 
The probability ratio clipping discourages excessively large generator updates, and has shown to be effective in the context of stabilizing policy optimization~\cite{PPO}. Figure~\ref{fig:intro-stability} (left) shows the intuition about the surrogate objective, where we can observe the objective value decreases with an overly large generator change and thus imposes regularization on the updates.


\begin{figure}[t]
\begin{center}
\centerline{
\includegraphics[width=0.33\columnwidth]{figs/surrogate.pdf}
\includegraphics[width=0.33\columnwidth]{figs/reweightD.pdf}
\includegraphics[width=0.33\columnwidth]{figs/reweightG.pdf}
}
\vspace{-4pt}
\caption{
Illustration of the proposed approach for stabilizing GAN training. Results are from the CIFAR-10 experiment in Sec.\ref{sec:exp:image}.
{\bf Left:} The conventional and surrogate objectives for generator training, as we interpolate between the initial generator parameters $\wm_{old}$ and the updated generator parameters $\wm_{new}$ which we compute after one iteration of training. The $\wm_{new}$ obtains maximal surrogate objective. 
The surrogate objective curve starts decreasing after $x=1$, showing the objective imposes a penalty for having too large of a generator update. In contrast, the conventional objective (for WGAN-GP) keeps increasing with larger generator updates. 
{\bf Middle and right:} Discriminator and generator losses w/ and w/o sample re-weighting. 
WGAN-GP with our re-weighting plugged in shows lower variance in both discriminator and generator losses throughout training. 
}
\label{fig:intro-stability}
\end{center}
\vspace{-6mm}
\end{figure}



{\bf (2)} When updating the discriminator, the new perspective induces an importance sampling mechanism, which effectively re-weights fake samples by their discriminator scores. Since low-quality samples tend to receive smaller weights, the discriminator trained on the re-weighted samples is more likely to maintain stable performance, and in turn provide informative gradients for subsequent generator updates. Figure~\ref{fig:intro-stability} (middle/right) demonstrates the effect of the re-weighting in reducing the variance of both discriminator and generator losses. 


Besides, our variational GAN training framework can provably overcome the training issue~\cite{lipschitz} that an optimal discriminator cannot provide any  informative gradient to training generator. This issue usually occurs in GAN training~\cite{lipschitz}, since the discriminator often converges much faster than the generator. Empirically, we conduct extensive experiments on a wide range of tasks, including text generation, text style transfer, and image generation. Our approach shows significant improvement over state-of-the-art methods, demonstrating its broad applicability and efficacy.




\section{Related Work}\label{sec:related}


\textbf{Wasserstein distance, WGAN, and Lipschitz continuity.} 
The GAN framework~\cite{goodfellow2014generative} features two components: a generator $G_\theta$ that synthesizes samples $\x$ given some noise source $\bm{z}$, namely $\x=G_\theta(\bm{z})$ with $\bm{z}\sim p_z(\bm{z})$, and a discriminator that distinguishes generator’s output and real data, which provides gradient feedback to improve the generator's performance. WGAN~\cite{WGAN} improves the training stability of GANs by minimizing the Wasserstein distance $W(p_r, p_\theta)$ between the generation distribution $p_\theta$ (induced from $G_\theta$) and the real data distribution $p_r$.
Its training loss is formulated as:
\begin{equation}
\min\nolimits_{\bm{\theta}} \max\nolimits_{f \in\mathcal{D}} \EE_{\xm\sim p_r}[f(\x)] - \EE_{\xm\sim p_\theta}[f(\x)],
\label{eq:related:wgan}
\end{equation}
where $\mathcal{D}$ is the set of 1-Lipschitz functions; $f$ acts as the discriminator and is usually implemented by a neural network $f_\phi$.
The original resort to enforce the Lipschitz constraint is through weight clipping \cite{WGAN}. WGAN-GP~\cite{wgangp} later improves it by  replacing it with a gradient penalty on the discriminator. CT-GAN~\cite{wwgan} further imposes the Lipschitz continuity constraint on  the manifold of the real data $\xm \sim p_r$. Our approach is orthogonal to these prior works and can serve as a drop-in replacement to stabilize generator and discriminator in various kinds of GANs, such as WGAN-GP and CT-GAN.






Research on the Lipschitz continuity of GAN discriminators have resulted in the theory of ``informative gradients'' \cite{lipschitz,zhou2018understanding}. Under certain mild conditions, a Lipschitz discriminator can provide informative gradient to the generator in a GAN framework: when $p_\theta$ and $p_r$ are disjoint, the gradient $\nabla f^*(\xm)$ of optimal discriminator $f^*$ w.r.t each sample $\xm\sim p_\theta$ points to a sample $\x^* \sim p_r$, which guarantees that the generation distribution $p_{\theta}$ is moving towards $p_r$. We extend the informative gradient theory to our new case and show theoretical guarantees of our approach.




\textbf{Reinforcement learning as inference.} 
Casting RL as probabilistic inference has a long history of research~\cite{dayan1997using,deisenroth2013survey,rawlik2013stochastic,levine2018reinforcement,abdolmaleki2018maximum}. 
For example, \citet{abdolmaleki2018maximum} introduced maximum a-posteriori policy optimization from a variational perspective. \citet{tan2018connecting} connected the formulation with other paradigms of learning such as maximum likelihood estimation and data augmentation~\citep{hu2019learning}. 
TRPO~\cite{TRPO} is closely related to this line by using a KL divergence regularizer to stabilize standard RL objectives. PPO~\cite{PPO} further proposed a practical clipped surrogate objective that emulates the regularization. Our approach draws on the connections to the research, particularly the variational perspective and PPO, to improve GAN training.






\textbf{Other related work.} 
Importance re-weighting has been adopted in different problems, such as learning knowledge constraints~\cite{hu2018deep}, and improving VAEs~\cite{burda2015importance} and GANs~\citep{hu2017unifying,song2019bridging}. 
We derive from the variational perspective which leads to re-weighting and clipping in the new context of GAN training stabilization. Our approach is orthogonal to and can be combined with other stabilization techniques such as large-batch training \citep{brock2018large} and parameter averaging (EMA) \citep{yaz2018unusual,brock2018large}. 










\section{Improving GAN Training}
\subsection{Motivations}
Our approach is motivated by connecting GAN training with the well-established RL-as-inference methods~\cite{abdolmaleki2018maximum,levine2018reinforcement,tan2018connecting} under a variational perspective. 
The connections enable us to augment GAN training with existing powerful probabilistic inference tools as well as draw inspirations from the rich RL literature for stable training. In particular, the connection to the popular TRPO~\cite{TRPO} and PPO~\cite{PPO} yields the probability ratio clipping in generator training that avoids destructive updates  (Sec.\ref{sec:method:gen}), and the application of importance sampling estimation gives rise to sample re-weighting for adaptive discriminator updates (Sec.\ref{sec:method:dis}). The full training procedure is summarized in Alg.\ref{alg:opt}.

Specifically, as described in Sec.\ref{sec:related}, the conventional WGAN formulation for updating the generator $p_\theta(\x)$ maximizes the expected discriminator score $\E_{p_\theta}[ f_\phi(\x) ]$, where $f_\phi$ is the Lipschitz-continuous discriminator parameterized with $\bm{\phi}$. The objective straightforwardly relates to policy optimization in RL by seeing $p_\theta$ as a policy and $f_\phi$ as a reward function. Thus, inspired by the probabilistic inference formulations of policy optimization~\cite{abdolmaleki2018maximum,hu2018deep,tan2018connecting}, here we transform the conventional objective by introducing a non-parameterized auxiliary distribution $q(\x)$ and defining a new variational objective:
\begin{equation}
\small
\begin{split}
    \mathcal{L}(\bm{\theta}, q) = \E_q[ f_\phi(\x) ] - \KLD\left( q(\x) \| p_\theta(\x) \right),
\end{split}
\label{eq:var-obj}
\end{equation}
where $\KLD$ is the KL divergence.
Intuitively, we are maximizing the expected discriminator score of the auxiliary $q$ (instead of generator $p_\theta$), and meanwhile encouraging the generator to stay close to $q$. We note that \citet{hu2018deep} have also related the above objective to GANs, with the different goal of integrating structured knowledge with deep generative modeling. 

As we shall see in more details shortly, the
new formulation allows us to take advantage of off-the-shelf inference methods, which naturally leads to new components to improve the GAN training. Maximization of the above objective is solved by the expectation maximization (EM) algorithm~\cite{neal1998view} which alternatingly optimizes $q$ at E-step and optimizes $\bm{\theta}$ at M-step. More specifically, at each iteration $t$, given the current status of generator parameters $\bm{\theta}=\bm{\theta}^{(t)}$, the E-step that maximizes $\mathcal{L}(\bm{\theta}^{(t)}, q)$ w.r.t $q$ has a closed-form solution:
\begin{equation}
\small
\begin{split}
q^{(t)}(\x) = \frac{p_{\theta^{(t)}}(\x)\exp\{ f_\phi(\x) \}}{Z_\phi},
\end{split}
\label{eq:q-sol}
\end{equation}
where $Z_\phi = \int_x p_{\theta^{(t)}}(\x)\exp\{ f_\phi(\x) \}$ is a normalization term that  depends on the discriminator parameters $\bm{\phi}$.
We elaborate on the M-step in the following subsections, where we continue to develop the practical procedures for updating the generator and the discriminator, respectively.


\subsection{Generator Training with Probability Ratio Clipping}\label{sec:method:gen}

The M-step optimizes $\mathcal{L}(\bm{\theta}, q^{(t)})$ w.r.t $\bm{\theta}$, which is equivalent to minimizing the KL divergence term in Eq.\eqref{eq:var-obj}. However, since the generator $p_\theta$ in GANs is often an \emph{implicit} distribution that does not permit evaluating likelihood, the above KL term (which involves evaluating the likelihood of samples from $q$) is not applicable. We adopt an approximation, which has also been used in the classical wake-sleep algorithm~\cite{hinton1995wake} and recent work~\cite{hu2018deep}, by minimizing the \emph{reverse} KL divergence as below. With Eq.\eqref{eq:q-sol} plugged in, we have:
\begin{equation}
\small
\begin{split}
\min\nolimits_\theta \KLD\left( p_\theta(\x) \| q^{(t)}(\x) \right) = \min\nolimits_\theta - \E_{p_\theta}\left[ f_\phi(\x) \right] + \KLD\left( p_\theta(\x) \| p_{\theta^{(t)}}(\x) \right).
\end{split}
\label{eq:g-loss}
\end{equation}
As proven in the appendix, this reverse KL approximation does not change the optimization problem in Eq.\eqref{eq:var-obj}. 
The first term on the right-hand side of Eq.\eqref{eq:g-loss} recovers the conventional objective of updating the generator. Of particular interest is the second term, which is a new KL regularizer between the generator $p_\theta$ and its ``old'' state $p_{\theta^{(t)}}$ from the previous iteration. The regularizer discourages the generator from changing too much between updates, which is useful to stabilize the stochastic optimization procedure. The regularization closely resembles to that of TRPO/PPO, where a similar KL regularizer is imposed to prevent uncontrolled policy updates and make policy gradient robust to noises. Sec.\ref{sec:theory} gives  analysis on the KL-regularized generator updates.

In practice, directly optimizing with the KL regularizer can be infeasible due to the same difficulty with the implicit distribution as above. Fortunately, PPO~\cite{PPO} has presented a simplified solution that emulates the regularized updates using a clipped surrogate objective, which is widely-used in RL. We import the solution to our context, leading to the following practical procedure of generator updates.

\textbf{Probability Ratio Clipping.}  
Let $r_t$ denote the probability ratio $r_t(\bm{\theta})=\frac{p_\theta(\x)}{p_{\theta^{(t)}}(\x)}$ which measures the difference between the new and old generator distributions.
For instance, $r_t(\bm{\theta}^{(t)}) = 1$. 
The clipped surrogate objective for updating the generator, as adapted from PPO, is:
\begin{equation}
\small
\begin{split}
\mathcal{L}^{CLIP}(\bm{\theta}) = \E_{p_\theta}\left[ \min\left( r_t(\bm{\theta}) f_\phi(\x),~ r_t^{clip}(\bm{\theta}) f_\phi(\x) \right) \right],
\end{split}
\label{eq:obj-clip}
\end{equation}
where $r_t^{clip}(\bm{\theta}) = \text{clip}\left(r_t(\bm{\theta}), 1-\epsilon, 1+\epsilon\right)$ clips the probability ratio, so that moving $r_t(\bm{\theta})$ outside of the interval $[1-\epsilon, 1+\epsilon]$ is discouraged. Taking the minimum puts a ceiling on the increase of the objective. Thus the generator does not benefit by going far away from the old generator.

Finally, to estimate the probability ratio $r_t(\bm{\theta})$ when $p_\theta$ is implicit, we use an efficient approximation similar to \cite{MLGAN,grover2019bias} by introducing a binary classifier $C$ trained to distinguish real and generated samples. Assuming an optimal classifier $C$ which has $p_\theta(\x) = \frac{1-C(\x)}{C(\x)}p_r(\x)$~\cite{goodfellow2014generative,MLGAN}, we approximate $r_t$ by: 
\begin{equation}
\small
\begin{split}
r_t(\bm{\theta}) = \frac{p_\theta(\x)}{p_{\theta^{(t)}}(\x)} \approx \frac{(1-C(\x))\cdot C^{(t)}(\x)}{(1-C^{(t)}(\x))\cdot C(\x)},
\end{split}
\label{eq:r-approx}
\end{equation}
where $C^{(t)}(\x)$ denotes the classifier at the $t$-th iteration. 
Note that the rightmost expression depends on $\bm{\theta}$ because $\x$ is the output of the generator, i.e., $\x=G_\theta(\bm{z})$.
In practice, during the phase of generator training, we maintain $C$ by fine-tuning it for only one iteration every time after $\bm{\theta}$ is updated (Alg.\ref{alg:opt}). Thus the maintenance of $C$ is cheap. We give more details of the configuration of $C$ in the appendix. In the cases where an explicit generative model is used (e.g., a language model for text generation), the probability ratio $r_t$ can directly be evaluated by definition without the need of $C$, though in our text generation experiments (Sec.\ref{sec:exp:text}) we still used $C$ for approximating $r_t$.




\subsection{Discriminator Training with Sample Re-weighting}\label{sec:method:dis}

We next discuss the training of the discriminator $f_\phi$, where we augment the conventional training with an importance weighting mechanism for adaptive updates. 
Concretely, given the form of the auxiliary distribution solution $q^{(t)}$ in Eq.\eqref{eq:q-sol}, we first draw from the recent energy-based modeling work~\cite{kim2016deep,hu2018deep} and propose to optimize $\bm{\phi}$ by maximizing the data log-likelihood of $q^{(t)}$, $\mathcal{L}(\bm{\phi})=\E_{p_r}[\log q^{(t)}(\x)]$. By taking the gradient, we have: \begin{equation}
\small
\begin{split}
    \nabla_\phi \mathcal{L}(\bm{\phi}) = \nabla_\phi \Big( \E_{p_r}\left[ f_\phi(\x)  \right] - \log Z_\phi \Big) 
    = \E_{p_r}\left[ \nabla_\phi  f_\phi(\x)  \right] -  \E_{q^{(t)}}\left[ \nabla_\phi f_\phi(\x) \right].
\end{split}
\label{eq:d-grad}
\end{equation}
We can observe that the resulting form resembles the conventional one (Eq.\ref{eq:related:wgan}) as we are essentially maximizing $f_\phi$ on real data while minimizing $f_\phi$ on fake samples. An important difference is that here fake samples are drawn from the auxiliary distribution $q^{(t)}$ instead of the generator $p_\theta$. This  difference leads to the new sample re-weighting component as below. Note that, as in WGAN (Sec.\ref{sec:related}), we maintain $f_\phi$ to be from the class of $1$-Lipschitz functions, which is necessary for the convergence analysis in Sec.\ref{sec:theory}. In practice, we can use gradient penalty \cite{wgangp,wwgan} for the Lipschitz continuity.


\textbf{Sample Re-weighting.}  
We use the tool of importance sampling to estimate the expectation under $q^{(t)}$ in Eq.\eqref{eq:d-grad}. Given the multiplicative form of $q^{(t)}$ in Eq.\eqref{eq:q-sol}, similar to~\cite{abdolmaleki2018maximum,hu2018deep,deng2020residual}, we use the generator $p_{\theta^{(t)}}$ as the proposal distribution. This leads to 
\begin{equation}
\small
\begin{split}
\E_{q^{(t)}}\left[ \nabla_\phi f_\phi(\x) \right] = \E_{p_{\theta^{(t)}}}[ \exp\{f_\phi(\x)\} \cdot \nabla_\phi f_\phi(\x) ] \ /\  Z_\phi.
\end{split}
\label{eq:d-weight}
\end{equation}
Note that $Z_\phi$ is the normalization factor defined in Eq.\eqref{eq:q-sol}. Thus, fake samples from the generator are weighted by the exponentiated discriminator score when used to update the discriminator. Intuitively, the mechanism assigns higher weights to samples that can fool the discriminator better, while low-quality samples are downplayed to avoid destructing the discriminator performance. It is worth mentioning that similar importance weighting scheme has been used in~\cite{hu2017unifying,MLGAN} for generator training in GANs, and~\cite{burda2015importance} for improving variational auto-encoders. Our work instead results in a re-weighting scheme in the new context of discriminator training.

The algorithm below summarizes the proposed training procedure for the generator and discriminator.

\begin{algorithm}[!h]
\centering
\caption{\small GAN Training with Probability Ratio Clipping and Sampling Re-weighting}
\label{alg:opt}
\begin{algorithmic}[1]
\STATE Initialize the generator $p_\theta$, the discriminator $f_\phi$, and the auxiliary binary classifier $C$
\FOR {$t \gets 1$ to $T$}
    \FOR {certain number of steps}
	    \STATE Update the discriminator $f_\phi$ with sample re-weighting through Eqs.\eqref{eq:d-grad}-\eqref{eq:d-weight}, and maintain $f_\phi$ to have upper-bounded Lipschitz constant through, e.g., gradient penalty~\cite{wgangp}.
	\ENDFOR
	\FOR {certain number of steps}
	    \STATE Finetune the real/fake binary classifier $C$ (for 1 step)
	    \STATE Estimate probability ratio $r_t(\bm{\theta})$ using $C$ through Eq.\eqref{eq:r-approx}
	    \STATE Update the generator $p_\theta$ with probability ratio clipping through Eq.\eqref{eq:obj-clip}
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis}\label{sec:theory}
To provide theoretical insight on the performance of our method, we prove that our framework holds the same guarantees as WGAN-GP~\citep{wgangp} and Lipschitz GANs~\citep{lipschitz}. Formally, we show that the method is fully compatible with \emph{Proposition 1} in \citep{wgangp} and \emph{Theorem 2} in \citep{lipschitz}, which provides rigorous analysis on GANs with Lipschitz discriminators and concludes 1) informative gradient pushes the generator distribution to the real data distribution and 2) the only Nash-equilibrium is $p_{\theta}=p_{r}$. Note that the theorems do not guarantee distributional convergence of $p_{\theta}$ to $p_r$, same as in \citep{wgangp,lipschitz}.







Our analysis is based on the reverse KL updates for the generator (Eq.\ref{eq:g-loss}), while the probability ratio clipping serves as a practical emulation for the updates. We begin by adapting \emph{Proposition 1} in \citet{wgangp} to our problem: 
\begin{prop} Let $\PPz_r$ and $q$ be two distributions in $X$, a compact metric space. Then, there is a $1$-Lipschitz function $f^*$ which is the optimal solution of $$\max_{\norm{f}_L \leq 1} \EE_{\xm \sim \PPz_r} \left[f(\xm)\right] - \EE_{\xm\sim q} \left[f(\xm)\right]$$
Let $\pi^*$ be the optimal coupling between $\PPz_r$ and $q$, defined as the minimizer of: $W(\PPz_r, q) = \inf_{\pi \in \Pi(\PPz_r,q)} \EE_{(\x,\y)\sim \pi} \left[\norm{\x - \y}\right]$ where $\Pi(\PPz_r, q)$ is the set of joint distributions $\pi(\x, \y)$ whose marginals are $\PPz_r$ and $q$, respectively. Then, if $f^*$ is differentiable, $\pi^*(\x = \y) = 0$, and $\x_\tau = \tau \x + (1-\tau)\y$ with $0 \leq \tau \leq 1$, it holds that 
$\PP_{(\x,\y)\sim\pi^*}\left[\nabla f^*(\x_\tau)=\frac{\y-\x_\tau}{\norm{\y-\x_\tau}}\right] = 1$. \label{wgangpprop}
\end{prop}
Proposition~\ref{wgangpprop} indicates that in presence of an optimal discriminator $f^*$, given any sample $\ym$ drawn from the variational distribution $q$, there exists a sample $\xm$ drawn from real data distribution $\PPz_r$ such that $\nabla_{\xm} f^*(\xm_\tau)\!=\!\frac{\ym-\xm_\tau}{\norm{\ym-\xm_t}}$ for all linear interpolations  $\xm_\tau  =  \tau \xm + (1-\tau)\ym$ with $0 \leq \tau \leq 1$. Therefore, an optimal discriminator $f^*$ can provide informative gradient to update $q$ and push $q$ towards to the real distribution $\PPz_r$. 

By the definition of $q$ with respect to $\PPz_{\theta}$ in Eq.\eqref{eq:q-sol}, the support of $\PPz_{\theta}$ and $q$ are the same; namely, given any $\x\sim \PPz_{\theta}, \y\sim \PPz_r$, we also have $q(\x) \neq 0$. Therefore, for all $\xm \sim \PPz_{\theta}$, $\xm$ is also a valid sample from $q$, the $f^*$ in {Proposition \ref{wgangpprop}} provides informative gradient with respect to $\x_\tau = \tau \x + (1-\tau)\y, \forall \tau\in [0,1]$:
$\PP_{(\x,\y)\sim\pi^*}\left[\nabla f^*(\x_\tau)=\frac{\y-\x_\tau}{\norm{\y-\x_\tau}}\right] = 1$
Therefore, assuming $f^*$ is the optimal discriminator to \eqref{eq:d-grad}, optimizing Eq.\eqref{eq:g-loss} can provide informative gradient that points the generator $p_\theta$ toward $p_r$.
 
 
 
\section{Experiments} \label{sec:results}
We conduct extensive experiments on three unsupervised generation tasks, including image generation, text generation, and text style transfer. The three tasks apply GANs to model different data modalities, namely, image, text, and neural hidden representations, respectively. Our approach consistently offers improvement over the state-of-the-arts on all tasks. See appendix for all experimental details.



\begin{figure}[t]
\begin{minipage}{0.47\textwidth}
\vspace{-0.8em}
\setlength{\tabcolsep}{2.2pt}  
\renewcommand{\arraystretch}{0.85} 
\footnotesize
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    {\bf Method}     & {\bf IS} ($\uparrow$) & {\bf FID} ($\downarrow$) \\
    \cmidrule{1-3}
    Real data & 11.24$\pm$.12 & 7.8\\
\cmidrule{1-3}
WGAN-GP (\citeyear{wgangp}) & 7.86$\pm$.08 & - \\
    CT-GAN (\citeyear{wwgan}) &  8.12$\pm$.12 & - \\
    SN-GANs (\citeyear{SNGAN}) & 8.22$\pm$.05 & 21.7$\pm$.21 \\
    WGAN-ALP (\citeyear{wganalp}) & 8.34$\pm$.06 & 12.96$\pm$.35\\
    SRNGAN (\citeyear{srngan}) & 8.53 $\pm$.04 & 19.83\\
Ours {(re-weighting only)} & 8.45$\pm$.14 & 13.21$\pm$.60 \\
    Ours (full) & \textbf{8.69$\pm$.13} &\textbf{10.70$\pm$.10}\\
    \bottomrule
  \end{tabular}
  \vspace{-5pt}
  \captionof{table}{CIFAR-10 results. 
Our method is run 3 times for  average and  standard deviation. 
} \label{tab:exp:image:IS_FID}
\vskip -0.4in
\end{minipage}
\hfill
\begin{minipage}{0.51\textwidth}
\begin{center}
\centerline{
\includegraphics[width=0.9\columnwidth]{cifar10/combined_pan.png}
}
\vskip -0.1in
\captionof{figure}{Generated samples by WGAN-GP (top-left), CT-GAN (bottom-left), and ours (right). 
}
\label{cifar10_combined}
\end{center}
\end{minipage}
\vspace{-15pt}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.40\columnwidth]{figs/convergence_IS_full.pdf}
\hspace{7pt}
\includegraphics[width=0.41\columnwidth]{figs/reweight_norm.pdf}
\vspace{-2pt}
\caption{{\bf Left:}  Inception score on CIFAR-10 v.s. training batches (including both generator and discriminator batches). The DCGAN~\cite{DCGAN} architecture is used. {\bf Right:} The gradient norms of discriminators on fake samples. 
}
\label{fig:image:converge}
\end{center}
\vspace{-5pt}
\end{figure}


\subsection{Image Generation}\label{sec:exp:image}
We first use the popular CIFAR-10 benchmark for evaluation and in-depth analysis of our approach.






\textbf{Setup.}  
CIFAR-10~\cite{cifar10} contains 50K images of sizes $32\times 32$. Following the setup in CT-GAN~\cite{wwgan}, 
we use a residual architecture to implement both generator and discriminator, and also impose a Lipschitz constraint on the discriminator. For each iteration, we update both generator and discriminator for 5 times. We use Inception Score (IS)~\cite{salimans2016improved} for evaluating generation quality and diversity, and Frechet Inception Distance (FID)~\cite{FID} for capturing model issues, e.g., mode collapse~\cite{xu2018empirical}. 












\textbf{Results.} Table~\ref{tab:exp:image:IS_FID} reports the results on CIFAR-10. 
For the three latest methods, SN-GANs~\cite{SNGAN} introduced spectral normalization to stabilize the discriminator training; WGAN-ALP~\cite{wganalp} developed an explicit Lipschitz penalty; and SRNGAN~\cite{srngan} introduced a weight-normalization scheme for generalization.
Table~\ref{tab:exp:image:IS_FID} shows that our full approach (CT-GAN + discriminator sample re-weighting + generator probability ratio clipping) achieves the best, with both IS and FID significantly surpassing the  baselines. These results accord with the visual results in Figure~\ref{cifar10_combined} where our generated samples show higher visual quality than those of the baselines. Comparison between CT-GAN and our approach with only re-weighting shows significant improvement. By further adding the probability ratio clipping to arrive our full approach, the performance (both IS and FID) is further improved with a large margin. The results demonstrate the effectiveness of the two components in our approach. 






 

Figure~\ref{fig:intro-stability} in Sec.\ref{sec:intro} has shown the effects of the proposed approach in stabilizing the generator and discriminator training. Here we further analyze these two components. Figure~\ref{fig:image:converge} (left) shows the convergence curves of different GAN methods. For a fair comparison, all models use the same DCGAN architecture~\cite{DCGAN}, and both our approach and WGAN-GP~\cite{wgangp} enforce the same discriminator Lipschitz constraint. Following the optimal setup in \citep{wgangp}, the update ratio of both WGAN-GP and our ``re-weighting only'' is 5:1 (i.e., each iteration updates the discriminator for 5 times and the generator for one time). Our full approach and ``clipping only'' use an update ratio of 5:5, because the probability ratio clipping that discourages large generator updates allows us to update the generator more frequently, which is desirable. Note that the x-axis in Figure~\ref{fig:image:converge} accounts for both generator and discriminator batches (i.e., an 5:5 iteration is counted as 10 training batches). Thus, for any given point on the x-axis, all comparison methods used roughly the same amount of computation.
From the curves, one can observe that our full approach surpasses our approach with only sample re-weighting, and they both converge faster and achieve a higher IS score than ``clipping only'', WGAN-GP, and DCGAN. It is interesting to note that ``clipping only'' does not offer a performance improvement over WGAN-GP, though its combination with sample re-weighting (i.e., the full approach) does improve over ``re-weighting only''. This is indeed not unexpected, because clipping and re-weighting are derived from the variational framework (Eq.\ref{eq:var-obj}) in a principled way. Discarding either of the two could lead to improper handling of the variational distribution $q$ and fails to conform to the framework. 

Figure~\ref{fig:image:converge} (right) investigates how the fake sample re-weighting can affect the discriminator training. By injecting re-weighting into WGAN-GP, the gradients on fake samples become more stable with lower variance, which partially explains the better training stability of discriminator in Figure~\ref{fig:intro-stability}.







 
































\begin{table}[t]
\renewcommand{\arraystretch}{0.9} 
\centering
\small
\begin{tabular}{l | llllll | l}
\toprule
Length & MLE  & SeqGAN~\cite{yu2017seqgan} & LeakGAN~\cite{guo2018long} & RelGAN~\cite{nie2018relgan}  & WGAN-GP~\citep{wgangp} & Ours & Real  \\
\cmidrule{1-8}
20   & 9.038 & 8.736   & 7.038   & 6.680 & 6.89  & \textbf{5.67} & 5.750  \\
40   & 10.411 & 10.310  & 7.191   & 6.765 & 6.78 & \textbf{6.14} & 4.071\\
\bottomrule
\end{tabular}
\vspace{1pt}
\caption{Oracle negative log-likelihood scores ($\downarrow$) on synthetic data. 
} \label{tab:txtgan_oracle}
\vspace{-10pt}
\end{table}





\begin{table}[t]
\renewcommand{\arraystretch}{0.9} 
\setlength{\tabcolsep}{4.9pt} 
\small
\centering
\begin{tabular}{@{}r llll | l | l@{}}
\toprule
{\bf Method}        & {\bf BLEU-2} ($\uparrow$) & {\bf BLEU-3} ($\uparrow$) & {\bf BLEU-4} ($\uparrow$) & {\bf BLEU-5} ($\uparrow$) & {\bf NLL$_{gen}$} ($\downarrow$) & {\bf Human} ($\uparrow$)\\
\cmidrule{1-7}
MLE           & 0.768  & 0.473  & 0.240   & 0.126  & 2.382  & - \\
LeakGAN~\cite{guo2018long}       & 0.826  & 0.645  & 0.437  & 0.272  & 2.356  & - \\
RelGAN 100~\cite{nie2018relgan}  & 0.881  & \textbf{0.705}  & \textbf{0.501}  & 0.319  & 2.482  & - \\
RelGAN 1000~\cite{nie2018relgan} & 0.837  & 0.654  & 0.435  & 0.265  & 2.285  & 3.42$\pm$1.23 \\
WGAN-GP~\cite{wgangp}       & 0.872  & 0.636  & 0.379  & 0.220   & \textbf{2.209}  & - \\\cmidrule{1-7}
Ours          & \textbf{0.905}  & 0.692  & 0.470   & \textbf{0.322}  & 2.265 & {\bf 3.59} $\pm$ {\bf 1.12} \\
\bottomrule
\end{tabular}
\vspace{1pt}
\caption{
Results on EMNLP2017 WMT News. BLEU measures text quality and NLL$_{gen}$ evaluates sample diversity. Results of previous text GAN models are from~\cite{nie2018relgan}, where RelGAN (100) and RelGAN (1000) use different hyper-parameter for gumbel-softmax. Our approach uses the same gumbel-softmax hyper-parameter as RelGAN~(1000). 
}
\label{tab:text-emnlp}
\vspace{-10pt}
\end{table}


\subsection{Text Generation}\label{sec:exp:text}
In this section, we evaluate our approach on text generation, a task that is known to be notoriously difficult for GANs due to the discrete and sequential nature of text. 

\textbf{Setup.} We implement our approach based on the RelGAN~\cite{nie2018relgan} architecture, a state-of-the-art GAN model for text generation. Specifically, 
we replace the generator and discriminator objectives in RelGAN with ours. We follow WGAN-GP~\cite{wgangp} and impose discriminator Lipschitz constraint with gradient penalty. 
Same as~\cite{nie2018relgan}, we use  Gumbel-softmax approximation~\cite{jang2016categorical,maddison2016concrete} on the discrete text to enable gradient backpropagation, and the generator is initialized with maximum likelihood (MLE) pre-training. 
Same as previous studies, we evaluate on both synthetic and real text datasets. 


\textbf{Results on Synthetic Data.} 
The synthetic data consists of 10K discrete sequences generated by an oracle-LSTM with fixed parameters~\cite{yu2017seqgan}. This setup facilitates evaluation, as the quality of generated samples can  be directly measured by  the negative log-likelihood (NLL) of the oracle on the samples. We use synthetic data with sequence lengths 20 and 40, respectively. Table~\ref{tab:txtgan_oracle} reports the results. MLE is the baseline with maximum likelihood training, whose output model is used to initialize the generators of GANs. Besides the previous text generation GANs~\cite{yu2017seqgan,guo2018long,nie2018relgan}, we also compare with WGAN-GP which uses the same neural architecture as RelGAN and ours. From Table~\ref{tab:txtgan_oracle}, one can observe that  our approach significantly outperforms all other approaches on both synthetic sets. Our improvement over RelGAN and WGAN-GP demonstrates that our proposed generator and discriminator objectives are more effective than the previous ones. 


\textbf{Results on Real Data.}
We then evaluate our method on the EMNLP2017 WMT News, a large real text data used for text GAN studies~\cite{guo2018long,nie2018relgan}. The dataset consists of 270K/10K training/test sentences with a maximum length of 51 and a vocabulary size of 5,255. To measure the generation quality, we use the popular BLEU-$n$ metric which measures $n$-gram overlap between generated and real text ($n\in\{2,3,4,5\}$). To evaluate the diversity of generation, we use the negative log-likelihood of the generator on the real test set (NLL$_{gen}$)~\cite{guo2018long,nie2018relgan}. From the results in Table~\ref{tab:text-emnlp}, one can see that our approach shows comparable performance with the previous best model RelGAN~(100) in terms of text quality (BLEU), but has better sample diversity.
Our model also achieves much higher BLEU scores than WGAN-GP. 
We perform \emph{human} evaluation, with randomly sampled 50 sentences for RelGAN~(1000) against ours and asked 5 annotators to score each sentence on a scale of 1-5. We use the same questions as designed by \citep{nie2018relgan}. Ours obtained an average human score of $3.59 \pm 1.12$, higher than $3.42 \pm 1.23$ by RelGAN (Fleiss' Kappa score $0.61$ showing substantial inter-rater agreement). 
















\begin{figure*}
\begin{minipage}{0.34\textwidth}
\small
  \centering
  \begin{tabular}{@{}rl@{}}
    \toprule
    {\bf Method} &  {\bf BLEU} \\
    \cmidrule{1-2}
    \citet{zhang2018style} & 24.48 \\
    \citet{tian2018structured} & 24.90 \\
    \citet{subramanian2018multiple} & 31.20 \\
    \citet{tikhonov2019style} & 32.82 \\\cmidrule{1-2}
    Ours & \textbf{33.45$\pm$.95}\\
    \bottomrule
  \end{tabular}
  \captionof{table}{
 BLEU scores between model generations and human-written text on the Yelp data. We run our method for 5 times and report the average and standard deviation.
} \label{tab:styletransfer_tbl2}
\vspace{-20pt}
\end{minipage}
\hfill
\begin{minipage}{0.64\textwidth}
\begin{center}
\centerline{\includegraphics[width=0.85\columnwidth]{style_transfer/style_adj2.pdf}}
\vspace{-10pt}
\captionof{figure}{
Trade-off between style accuracy and content preservation. The {\color{orange} orange} circles denote our results using varying values for an objective weight~\cite{tikhonov2019style} which manages the trade-off. 
}
\label{fig:styletransfer_img}
\end{center}
\end{minipage}
\vspace{-20pt}
\end{figure*}


\subsection{Text Style Transfer}\label{sec:exp:style}
Text style transfer task is gaining increasing attention in NLP~\cite{hu2017toward,shen2017style,yang2018unsupervised}. The task aims at rewriting a sentence to modify its style (e.g., sentiment) while preserving the content. Previous work applies GANs on neural hidden states to learn disentangled representations~\cite{shen2017style,tikhonov2019style}. The task thus can serve as a good benchmark for GANs, as hidden state modeling provides a new modality that differs from image and text modeling as studied above.



\textbf{Setup.} We follow the same experimental setting and use the same model architecture in the latest work~\cite{tikhonov2019style}. In particular, the VAE-based model~\cite{hu2017toward,kingma2013auto} is extended by adding a latent code discriminator which eliminates stylistic information in the latent code. We replace their adversarial objectives with our proposed ones, and impose discriminator Lipschitz constraint with gradient penalty~\cite{wgangp}. 
We test on sentiment transfer, in which the sentiment (positive/negative) is treated as the text style. We use the standard Yelp review dataset,
and the ground truth output text provided by \cite{li2018delete}.


\textbf{Results.} Following the previous work~\cite{tikhonov2019style}, we first report the BLEU score that measures the similarity of the generated samples against the human written text. Table~\ref{tab:styletransfer_tbl2} shows that our approach achieves best performance, improving the state-of-the-art result~\cite{tikhonov2019style} from BLEU $32.82$ to $33.45$.

The second widely used evaluation method is to measure (1) the style accuracy by applying a pre-trained style classifier on generated text, and (2) the content preservation by computing the BLEU score between the generated text and the original input text (BLEU-X). There is often a trade-off between the two metrics. Figure~\ref{fig:styletransfer_img} displays the trade-off by different models. Our results locate on the top-right corner, indicating that our approach achieves the best overall style-content trade-off.


























\section{Conclusion}\label{sec:conclusion}
We have presented a new training framework of GANs derived from a new variational perspective and draws on rich connections with RL-as-inference. This results in probably ratio clipping for generator updates to discourage overly large changes, and fake sample re-weighting for stabilized discriminator updates. Experiments show our approach demonstrates superior training stability and improves over previous best methods on image generation, text generation, and text style transfer. 
The connection between the GAN and RL formalisms can potentially inspire more cross-pollination between the two fertile research fields. We are also interested in extending the formulation to connect more machine learning paradigms~\citep{hu2020learning}, for more systematic understanding, unification, and generalization of diverse learning algorithms.






\section*{Broader Impacts}
This work offers a unique viewpoint on two promising fields with lots of applications and impacts: Generative Adversarial Networks and Reinforcement Learning. The improvement to image generation results may be adapted to speed up photo editing, improve scene rendering, and create more realistic simulation for robot training. Furthermore, the contribution to text generation and text style transfer can be adopted to improve the quality of machine translation, and automated news-summaries.

Nevertheless, GANs can also be applied to faking images of people and jeopardize personal identities (i.e. Deepfake). We hope that future works can counter this issue through deep-fake detection.
\bibliography{citations}













\newpage



\section{Appendix}

\subsection{Proof on the equivalence between Reverse KL Divergence and KL Divergence} \label{pf_kl}
We prove that optimizing $\kl(p_{\wm} || q )$ are equivalent to optimizing $\kl(q || p_{\wm})$.
This provides guarantee for the approximation that leads to \eqref{eq:g-loss}. 

\textbf{Claim:} Under the assumption that $f_\phi$ Lipschitz, $f_\phi$ is bounded because the input $\xm$ is bounded. Let $K$ be the Lipschitz constant of $f_\phi$, and let $c = f_\phi(0)$
\begin{equation}
    |f_\phi(x)-c|\leq K|x-0| = K|x|
\end{equation}
We then show that $\kl(p_{\wm} || q )$ differ $\kl(q || p_{\wm})$ by at most a constant. Since the function $f_{\phi}(\xm)$ is lower and upper-bounded. There exists $a,b$, such that $-a \leq f_{\phi}(\xm) \leq b$ for any $\xm$ bounded.
\begin{equation}\label{asfsafvsgvsdf}
\begin{split}
 & \kl(q || p_{\wm}) - \kl(p_{\wm} || q )  \\
= & \int_{\xm} \left[ q(\xm) \log\left(\frac{q(\xm)}{p_{\wm}(\xm)}\right)- p_{\wm}(\xm) \log\left(\frac{p_{\wm}(\xm)}{q(\xm)}\right) \right] d \xm\\
= & \int_{\xm} \left[ q(\xm) +p_{\wm}(\xm) \right] \log\left(\frac{q(\xm)}{p_{\wm}(\xm)}\right)  d \xm\\
\lee{172} & \int_{\xm}p_{\wm}(\xm)  \left[ 1 +  \frac{\exp{(\alpha f_{\phi}(\xm))}}{Z} \right] \log\left( \frac{\exp{(\alpha f_{\phi}(\xm))}}{Z}\right)  d \xm\\
\led{173} & \alpha(a+b) \int_{\xm}p_{\wm}(\xm)  \left[ 1 +  \frac{\exp{(\alpha f_{\phi}(\xm))}}{Z} \right]    d \xm\\
\lee{174} & 2\alpha(a+b),\\
\end{split}
\end{equation}
where \ding{172} plugs $q^*(\xm) = \frac{p_{\wm}(\xm)\exp{(\alpha f_{\phi}(\xm))}}{Z}$; \ding{173} uses the fact $\log\left( \frac{\exp{(\alpha f_{\phi}(\xm))}}{Z}\right) = \log\left( \frac{\exp{(\alpha f_{\phi}(\xm))}}{\int_{\xm} p_{\wm}(\xm)\exp{(\alpha f_{\phi}(\xm))} d \xm }\right) \leq \log\left( \frac{\exp{(\alpha b)}}{\int_{\xm} p_{\wm}(\xm)\exp{(-\alpha a)} d \xm }\right)= \alpha(a+b)$; \ding{174} uses $\int_{\xm}p_{\wm}(\xm)     \frac{\exp{(\alpha f_{\phi}(\xm))}}{Z} d \xm=1$.
The above claim completes the theoretical guarantee on the reverse-KL approximation in \eqref{eq:g-loss}.

\subsection{Proof on the necessity of Lipschitz constraint on the discriminator} \label{pf_issue}
Although \cite{hu2018deep} shows preliminary connections between PR and GAN, the proposed PR framework does not provide informative gradient to the generator when treated as a GAN loss. 
Following~\cite{zhou2018understanding}, we consider the training problem when the discriminator (i.e. $f_{\phi}(\xm)$ here) is optimal: when discriminator $f^*_{\phi}(\xm)$ is optimal, then the gradient of generator $g(f_{\phi}(\xm))$ is $\nabla_{f^*_{\phi}(\xm)} g(f^*_{\phi}(\xm)) \cdot \nabla_{\xm} f^*_{\phi}(\xm)$ which could be very small due to vanished $\nabla_{\xm} f^*_{\phi}(\xm)$. In this way, it is hard to push the generated data distribution $p_{\wm}$ towards the targeted real distribution $p_r$. This problem also exists in \eqref{eq:d-grad} because
\begin{equation}\begin{split}
f^*_{\phi}(\xm) =  \arg\min_{f_{\phi}(\xm)} \alpha \left[   p_r(\xm) f_{\phi}(\xm)  - q(\xm)    f_{\phi}(\xm) \right].
\end{split}
\end{equation}
So if $p_r$ and $q$ are disjoint, we have
\begin{equation}\begin{split}
f^*_{\phi}(\xm)& =  \arg\min_{f_{\phi}(\xm)} \alpha \left[   p_r(\xm) f_{\phi}(\xm)  - q(\xm)    f_{\phi}(\xm) \right]\\
&= \begin{cases}
                \arg\min_{f_{\phi}(\xm)} p_r(\xm) f_{\phi}(\xm), & \mbox{if } \xm\sim p_r \\
                \arg\min_{f_{\phi}(\xm)} - q(\xm)    f_{\phi}(\xm), & \mbox{if } \xm \sim q.
                 \end{cases}
\end{split}
\end{equation}
Note that for any $\xm\sim p_r$, $f^*_{\phi}(\xm)$ is not related to $q$ and thus its gradient $\nabla f^*_{\phi}(\xm)$ also does not relate to $q$. Similarly, for any $\xm\sim q$, $\nabla f^*_{\phi}(\xm)$ does not provide any information of  $p_r$. Therefore, the proposed loss in \cite{hu2018deep} cannot guarantee informative gradient \cite{zhou2018understanding} that pushes $q$ or $p_{\wm}$ towards to $p_r$. 

\subsection{Experiments: More Details and Results}

\subsubsection{Binary classifier for probability ratio clipping}
For the image generation and text generation, the binary classifier $C$ in Eq.\eqref{eq:r-approx} has the same architecture as the discriminator except an additional Sigmoid activation at the output layer. The binary classifier is trained with real and fake mini-batches alongside the generator, and requires no additional loops. We select the clipping parameter $\epsilon$ from $\{0.2, 0.4\}$, as they are typically used in PPO.

In addition in the task of image generation, we observe similar overall performance between training on raw inputs from the generator/dataset and training on input features from the first residual block of the discriminator ($D$), thus further reducing the computational overhead of the binary classifier.


\subsubsection{Image Generation on CIFAR-10}
We translate the code\footnote{\href{https://github.com/biuyq/CT-GAN}{github.com/biuyq/CT-GAN}} provided by \citet{wwgan} into Pytorch to conduct our experiments. We use the same architecture: a residual architecture for both generator and discriminator, and enforcing Lipschitz constraint on the discriminator in the same way as CT-GAN~\cite{wwgan}. 
During training, we interleave 5 generator iterations with 5 discriminator iterations. We optimize the generator and discriminators with Adam (Generator lr: $5e-5$, Discriminator lr: $1e-4$, betas: $(0.0, 0.9)$). We set the clipping threshold $\epsilon := 0.4$ for the surrogate loss and we linearly anneal the learning rate with respect to the number of training epochs.


\paragraph{Discriminator sample re-weighting stabilizes DCGAN}
We quantitatively evaluate the effect of discriminator re-weighted sampling by comparing DCGAN~\cite{DCGAN} against DCGAN with discriminator re-weighting. Starting from the DCGAN architecture and hyper-parameters, we run 200 random configurations of learning rate, batch size, non-linearity (ReLU/LeakyReLU), and base filter count (32, 64). Results are summarized in Table \ref{tab:wwgan_tbl}. DCGANs trained with re-weighted sampling has significantly less collapse rate, and achieves better overall performance in terms of Inception Score. These results well demonstrate the effectiveness of the proposed discriminator re-weighted sampling mechanism.

\begin{table}[h]
  \vskip - 0.1in
  \centering
  \begin{tabular}{r lll}
    \toprule
    {\bf Method}    &  {\bf Collapse rate} & {\bf Avg IS} & {\bf Best IS}  \\
    \midrule
    DCGAN & 52.4\% & 4.2 & 6.1\\
    DCGAN + Re-weighting & 30.2\% &5.1 & 6.7\\
    \bottomrule
  \end{tabular}
\caption{Outcomes of 200 trials with random configurations. The performance of the models are measured through Inception score. We identify training collapse when the average discriminator loss over 2000 batches is below $1e^{-20}$ or above $1 - 1e^{-20}$. DCGAN re-weighted with our loss has lower collapse rate and higher average performance.}
  \label{tab:wwgan_tbl}
\end{table}



\paragraph{Discriminator re-weighted samples}  
To provide an illustration of how discriminator weights can help the discriminator concentrate on the fake samples of better quality during the training phase, in Figure~\ref{fig:q_weights} we plot the fake samples of a trained ResNet model alongside their corresponding discriminator weights. 
\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{cifar10/sample_from_q.png}}
\vskip -0.1in
\caption{One batch of generated images together with their corresponding softmax discriminator weights. The more photo-realistic images (columns 2, 3, 5, 8) receive higher discriminator weights. In this batch, the generator will be influenced more by gradients from the better-quality samples above.}
\label{fig:q_weights}
\end{center}
\vskip -0.3in
\end{figure}

\paragraph{Clipped surrogate objective}
One unique benefit of the clipped surrogate objective is that it allows our model to obtain an estimate of the effectiveness of the discriminator, which then enables us to follow a curriculum that takes more than one $(n_g)$ generator steps per $(n_c)$ critic steps. In practice, setting $n_g = n_c = 5$ achieves good quality, which also allows us to take $5$ times more generator steps than prior works \cite{WGAN, wgangp, wwgan, SNGAN} with the same number of discriminator iterations.
Table~\ref{tab:exp:image:IS_FID} shows the improvement enabled by applying the surrogate objective.

\paragraph{Generated samples} 
Figure~\ref{appendix:fig:samples} shows more image samples by our model.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=.5\columnwidth]{cifar10/6.jpg}
            \includegraphics[width=.5\columnwidth]{cifar10/2.jpg}}
\centerline{\includegraphics[width=.5\columnwidth]{cifar10/5.jpg}
            \includegraphics[width=.5\columnwidth]{cifar10/8.jpg}}
\caption{More samples from our generator on CIFAR-10}
\label{appendix:fig:samples}
\end{center}
\vskip -0.2in
\end{figure}

\subsubsection{Text Generation}
We build upon the Pytorch implementation\footnote{\href{https://github.com/williamSYSU/TextGAN-PyTorch}{github.com/williamSYSU/TextGAN-PyTorch}} of RelGAN. We use the exact same model architecture as provided in the code, and enforce Lipschitz constraint on the discriminator in the same way as in WGAN-GP~\cite{WGAN}. 

During training, we interleave 5 generator iterations with 5 discriminator iterations. We use Adam optimizer (generator lr: 1e-4, discriminator lr: 3e-4). We set the clipping threshold $\epsilon=0.2$ for the surrogate loss and we linearly anneal the learning rate with respect to the number of training epochs.

\subsubsection{Text Style Transfer}
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=.5\columnwidth]{style_transfer/Style_transfer.png}}
\caption{Model architecture from \cite{tikhonov2019style}, where the style discriminator ($D$) is a structured constraint the generator optimize against. A latent code discriminator ensure the independence between semantic part of the latent representation and the style of the text. \textcolor{blue}{Blue} dashed arrows denote additional independence constraints of latent representation and controlled attribute, see \cite{tikhonov2019style} for the details.}
\label{appendix:fig:style_transfer_arch}
\end{center}
\vskip -0.2in
\end{figure}

We build upon the Texar-TensorFlow~\cite{texar} style-transfer model by \citet{tikhonov2019style}\footnote{\href{https://github.com/VAShibaev/text_style_transfer}{https://github.com/VAShibaev/text\_style\_transfer}}. We use the exact same model architecture and hyper-parameters as provided in the code, and enforce Lipschitz constraint on the discriminator in the same way as WGAN-GP~\cite{WGAN}. In addition, we replace the discriminator $D$ in Figure \ref{appendix:fig:style_transfer_arch}, by our loss with an auxiliary linear style classifier as in \citet{ACGAN}. We did not apply the surrogate loss to approximate the KL divergence, but relied on gradient clipping on the generator.


 

\end{document}