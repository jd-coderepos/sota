\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{filecontents}
\usepackage{float}
\usepackage{hyperref}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}


\title{AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First - Using Relation Extraction to Identify Entities}





\author{
  \begin{tabular}{c}
  Nicholas Popovic \\
  \textnormal{Karlsruhe Institute of Technology (KIT), Germany} \\
  {\tt popovic@kit.edu} 
   \end{tabular}\begin{tabular}{c}
  Walter Laurito\\
  \textnormal{FZI Research Center for Information Technology, Germany} \\
  {\tt laurito@fzi.de} 
   \end{tabular} \
    X_{s} = \max_{a_{i} \in A} (sim(e_{s}, a_{i}))

where  is the set of prototype embeddings which contains an embedding for each entity type and  is the dot product similarity of two vectors.
We select the  spans with the highest values for  as our \textit{candidate mentions}  for relation extraction. 
We compute the \textit{mention loss} as the mean triplet loss \cite{DBLP:conf/cvpr/SchroffKP15} across all prototype embeddings in  and all mentions in .

\subsection{Relation Extraction}

For relation extraction, we use the document-level relation extraction model DL-MNAV \cite{popovic_fsdlre_2022}. 
We use the concatenation of two span representations as a representation for the relation between them \cite{wang_extracting_2019}.
The resulting relation representations are compared to a single relation prototype embedding per relation type, as well as  additional prototypes representing the none-of-the-above class (this follows the MNAV model \cite{sabo_revisiting_2021}).
The relation type corresponding to the prototype resulting in the highest dot product similarity for a relation representation is used as the predicted type.
As loss function for the relation classification we use \textit{adaptive thresholding loss} \cite{zhou_document-level_2020} as it is capable of handling the large imbalance between positive and negative training examples present in document-level relation extraction tasks.\\

Due to quadratic scaling of the pairwise comparisons it is not feasible to perform relation extraction on all possible continuous spans.
We, therefore, perform relation classification on the top  spans\footnote{During training we add annotated spans which are not among the top  spans.} with the highest span scores, meaning that we have to classify a maximum of  relation representations for a given input text.
The computational complexity of the system can, therefore, be adjusted dynamically at inference time by changing , for example to be run on GPUs with smaller memory capacity or on GPUs with higher memory capacity to improve the quality of predictions. \\

As a result of the soft mention detection, it is possible that some of the  spans are overlapping and correspond to the same target (see appendix \ref{sec:appendix_b} for examples).
This means that the relation classifier may output multiple predictions for the same relation instance with slightly different mention spans. 
For predictions in which both the head and tail entity overlap, we therefore output only the prediction with the highest classification score.

\subsection{Entity Type Classification}

Finally, we use a simple mapping to determine the entity type of the spans which participate in the relations predicted by the relation classifier. 
The mapping used can be found in appendix \ref{sec:appendix_a}. For spans classified as "PRIMARY" we additionally change the predicted type to "ORDERED", if they are the head entity of more than one "Direct" relation.

\begin{table*}
\centering
\scalebox{0.7}{
\begin{tabular}{ll|cccc|ccc}
\hline
&&\multicolumn{4}{c|}{Entity Extraction}&\multicolumn{3}{c}{Relation Extraction}\\
\hline
pooling & preprocessing &  strict [\%] &  exact [\%] &  partial [\%] &  type [\%] & precision [\%] & recall [\%] &  [\%] \\
\hline
\multicolumn{9}{c}{Development set}\\
\hline
max & None & \textbf{59.79  0.99} & \textbf{60.19  0.99} & \textbf{69.20  0.68} & \textbf{67.45  1.05} & \textbf{65.86  1.34} & 44.14  0.97 & \textbf{52.86  1.10} \\
mean & None & 58.02  3.67 & 58.49  3.70 & 69.14  2.02 & 66.98  2.31 & 61.27  5.09 & \textbf{44.58  1.56} & 51.61  2.87 \\
max & LaTeX2Text & 58.90  0.79 & 59.30  0.87 & 68.69  1.00 & 66.88  1.09 & 64.54  2.61 & 43.77  1.76 & 51.66  0.77 \\
mean & LaTeX2Text & 54.59  15.07 & 54.99  14.44 & 65.62  11.28 & 64.22  14.22 & 63.89  33.24 & 40.59  15.90 & 49.64  23.63 \\
\hline
\multicolumn{9}{c}{Test set}\\
\hline
max & None & - & - & 37.83  0.85 & 37.88  0.85 & 45.80  5.80 & 20.96  0.08 & 28.66  1.19\\
mean & None & - & - & \textbf{41.21  1.18} & \textbf{41.23  1.19} & 42.25  3.19& \textbf{26.55  1.19} & \textbf{32.28  0.20} \\
max & LaTeX2Text & - & - & 38.33  1.57 & 38.38  1.57 & 46.09  0.77 & 21.64  1.60 & 29.45  1.41 \\
mean & LaTeX2Text & - & - & 34.53  11.02 & 34.64  11.13 & \textbf{47.02  20.70} & 18.20  8.10 & 26.24  11.64 \\
\hline
\end{tabular}
}
\caption{\label{tab:results_test} Entity and relation extraction scores for 4 different models on both the development and the test set. NER metrics strict and exact were not produced by the test set evaluation script on the competition site and the test set is not publicly available at the time of writing.}
\end{table*}

\section{Experimental Setup}






For our language model we use SciBERT \cite{beltagy_scibert_2019}, which is trained on scientific text, via Huggingface's Transformers library \cite{wolf_transformers_2020}. 
For LaTeX preprocessing (see section \ref{sec:preprocessing}) we use Pylatexenc\footnote{\url{https://github.com/phfaist/pylatexenc}}. 
As our optimizer, we use AdamW \cite{loshchilov_decoupled_2019} with learning rates , a linear warmup of 1 epoch followed by a linear decay to zero, for a total of 60 epochs\footnote{The length of one epoch is dictated by the number of training examples, which is 3119.}, a batch size of 4, and apply gradient clipping with a max norm of 1. 
During training, we randomly downsample the amount of candidate spans for soft mention detection to 1000, while ensuring that all labeled spans are included. During training and development set evaluation, we set , the number of spans to perform relation classification on, to 50, as preliminary experiments showed this value to yield a good compromise between model performance and training time. For test set evaluation we increase  to 400.
Training takes approximately 10 hours on a single NVIDIA V100 GPU using mixed precision. 
We perform early stopping based on the micro  score for relation extraction on the development set. 
We train each hyperparameter configuration 3 times using different random seeds and report the median and standard deviation for each metric. 
As a result of the different combinations of preprocessing and mean-/max-pooling, we examine the performance of 4 configurations on the test set.
For our evaluation, we report the micro  scores for NER metrics as used in SemEval-2013 Task 9.1 \cite{segura-bedmar_semeval-2013_2013}\footnote{We use the following implementation: \url{https://github.com/davidsbatista/NER-Evaluation}}.
For relation extraction we report \textit{micro} precision, recall and  scores, unless otherwise indicated.
\section{Results}



\subsection{Overview}

The results of the 4 model configurations on the test set are reported in table \ref{tab:results_test}.
In comparison to the other approaches taking part in SemEval-2022 Task 12, our system ranks in place 3/9 in terms of relation extraction  score.\footnote{Scores for other metrics are not publicly visible on the leaderboard at the time of writing.}

In general, we find that our model produces predictions with significantly higher precision than recall.

\subsection{Impact of Preprocessing}
With respect to the preprocessing procedure, we observe no clear performance impact.
We conclude that SciBERT appears to cope well with LaTeX code and preprocessing, as described in this paper, is not required.
\subsection{Impact of Pooling Procedure}
Regarding the pooling procedures we find that mean pooling tends to cause higher variability in the classification performance of the models.
For the models trained using mean pooling and preprocessing, 1 of 3 models performed significantly worse than the others, causing the large standard deviation in the results.

\subsection{Impact of Domain}

\begin{table}
\centering
\scalebox{0.8}{

\begin{tabular}{lrrrr}
\hline
 & \multicolumn{4}{c}{domain} \\
\hline
 & cs & econ & math & physics \\
\hline
\% of training corpus & 16.63 & 27.08 & 12.82 & 32.08 \\
\hline
relation type &  &  &  &  \\
\hline
Direct & 33.12 & 21.05 & 63.82 & 85.71 \\
Count & - & - & 84.62 & 100.00 \\
Corefer-Symbol & 21.05 & 20.47 & 91.30 & 100.00 \\
Corefer-Description & 3.51 & 0.00 & 76.92 & 96.00 \\
\hline
macro & 19.23 & 13.84 & 79.17 & 95.43 \\
micro & 25.93 & 19.49 & 78.77 & 88.77 \\

\hline
\end{tabular}
}
\caption{\label{tab:domain_re} F1 scores for relation extraction across different domains and relation types on the development set. cs and econ do not contain any instances of "Count".}
\end{table}

In table \ref{tab:domain_re}, we show the relation extraction  scores for a model across the 4 different domains covered by the development set paired with the distribution of training data across domains.
We observe large performance differences depending on the domain with math and physics showing very high macro  scores (79.17\% / 95.43\%) and computer science and economics performing poorly (19.23\% / 13.84\%).
While physics content does represent the majority of training examples, the distribution of domains across training examples does not fully explain the disparity.

\subsection{Impact of }

\begin{filecontents*}{data.csv}
x,p,r,f,re
10,70.1863354037267,14.089775561097257,23.468328141225335,30.389016018306634
20,66.85823754789271,21.758104738154614,32.8316086547507,41.235697940503435
30,66.13162118780096,25.6857855361596,37.00044903457565,48.192219679633865
40,67.08683473389355,29.862842892768082,41.328731665228645,54.46224256292906
50,67.41854636591479,33.54114713216958,44.79600333055787,58.71853546910755
60,66.7433831990794,36.15960099750624,46.90659118479579,62.60869565217392
70,66.81127982646422,38.403990024937656,48.77276326207443,65.81235697940502
80,66.87435098650052,40.14962593516209,50.175301908843004,68.37528604118994
90,67.03629032258065,41.45885286783042,51.23266563944531,70.89244851258582
100,67.19211822660098,42.51870324189526,52.080946926307746,72.35697940503432
110,66.86046511627907,43.01745635910225,52.35204855842185,73.31807780320366
120,66.41294005708849,43.51620947630923,52.58003766478342,74.279176201373
130,66.28895184135978,43.765586034912715,52.722493428464134,74.96567505720823
140,65.85365853658537,43.765586034912715,52.58426966292136,75.74370709382151
150,65.64531104921078,44.077306733167084,52.74151436031331,76.38443935926773
160,65.35288725939506,44.451371571072315,52.91280148423005,77.07093821510298
170,65.24886877828054,44.9501246882793,53.229974160206716,78.03203661327231
180,65.28403967538323,45.137157107231914,53.37265020272761,78.5812356979405
190,65.1978417266187,45.1995012468828,53.387334315169376,79.13043478260869
200,65.12042818911686,45.51122194513716,53.57798165137615,79.58810068649885
210,64.88888888888889,45.51122194513716,53.49945034811285,79.90846681922197
220,64.86246672582077,45.573566084788034,53.533504210911765,80.13729977116705
230,64.80990274093722,45.69825436408978,53.60146252285192,80.64073226544622
240,64.66960352422907,45.760598503740646,53.59620299379336,80.82379862700229
250,64.5048203330412,45.885286783042396,53.6247723132969,81.09839816933638
260,64.27947598253276,45.885286783042396,53.54674427064388,81.32723112128146
270,64.36681222707423,45.947630922693264,53.61949799927246,81.6933638443936
280,64.31679721496954,46.07231920199501,53.686887032328364,82.0137299771167
290,64.14273281114012,45.947630922693264,53.541590991645485,82.10526315789474
300,64.0069384215091,46.00997506234414,53.53645266594123,82.47139588100687
310,63.730569948186535,46.00997506234414,53.43953656770456,82.65446224256293
320,63.706896551724135,46.07231920199501,53.47322720694646,82.83752860411899
330,63.65202411714039,46.07231920199501,53.45388788426762,82.97482837528604
340,63.4020618556701,46.00997506234414,53.32369942196531,83.02059496567506
350,63.130881094952954,46.00997506234414,53.227551388388015,83.15789473684211
360,63.10845431255338,46.07231920199501,53.26126126126126,83.29519450800915
370,63.05460750853242,46.07231920199501,53.24207492795389,83.52402745995423
380,63.00085251491902,46.07231920199501,53.22290241267555,83.7070938215103
390,62.87170773152082,46.13466334164589,53.21826681049983,83.84439359267735
400,62.87170773152082,46.13466334164589,53.21826681049983,83.84439359267735
\end{filecontents*}

\begin{figure}
    \centering
    \scalebox{0.83}{
    \begin{tikzpicture}
    \begin{axis}[legend pos=south east, ylabel=\%, xlabel=]
    \addplot[color=black,smooth, thick, dotted] table [x=x, y=p, col sep=comma] {data.csv};\addlegendentry{RE precision}
    \addplot[color=black,smooth, thick, dashed] table [x=x, y=r, col sep=comma] {data.csv};\addlegendentry{RE recall}
    \addplot[color=black,smooth, thick] table [x=x, y=f, col sep=comma] {data.csv};\addlegendentry{RE }
    \addplot[color=blue,smooth, thick, dashed] table [x=x, y=re, col sep=comma] {data.csv};\addlegendentry{entity recall}
    \end{axis}
    \end{tikzpicture}
    }
    \caption{Plot of the impact of increasing values of  on precision, recall, and F1 scores on the development set.}
    \label{fig:k_rel}
\end{figure}

In figure \ref{fig:k_rel}, we show the change in relation extraction performance across different values for .
We also include in the plot the percentage of entity spans in the top  ranked spans (entity recall).
While the relation extraction performance improves proportional to the entity recall for  the improvement slows down for higher .
We hypothesize that this is due to the limiting of  and the candidate span downsampling during training, which prevents the model from seeing some of the more difficult cases.
In appendix \ref{sec:appendix_b}, we show examples of detected spans.

\subsection{Impact of Tokenization}


\begin{table}
\centering
\scalebox{1}{
\begin{tabular}{lrrr}
\hline
matching & precision & recall &  \\
\hline
strict & 55.85 & 44.01 & 49.23 \\
partial & 62.87 & 46.13 & 53.22 \\
\hline
\end{tabular}
}
\caption{\label{tab:partial_matching} Comparison of strict and partial matching requirements with respect to classification scores on the development set.}
\end{table}


In order to measure the impact of tokenization errors produced by adjusting labels during training, we perform a partial matching of relation labels as follows:
For predicted relation triples which are false positives, we accept them as true positives for an annotated instance if the \textit{intersection-over-union} (IOU) scores of both head and tail entities are greater than 67\% and the predicted relation type matches the label. In table \ref{tab:partial_matching} we show the results of both strict and partial matching for our best model on the development set.
We find that the relaxed requirements for span accuracy result in an increase in the  score of 3.99\%. 
We conclude that tokenization errors, while measurable, do not account for the majority of errors of our model.





\section{Conclusion}



In this paper, we present an end-to-end joint entity and relation extraction approach for linking mathematical symbols to their descriptions in LaTeX documents.
Our model appears to be sensitive to the domain of the input documents, achieving high macro  scores of 95.43\% and 79.17\% for physics and math content, respectively, while achieving macro  scores of only 19.23\% and 13.84\% for computer science and economics related content.
We find that the model's predictions are higher in precision than in recall.
We perform a detailed error analysis and identify cross-domain generalization as the most critical problem to tackle in future work.

\section*{Acknowledgements}


The authors acknowledge support by the state of Baden-WÃ¼rttemberg through bwHPC.

\bibliography{references}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Entity Type Classification Map}
\label{sec:appendix_a}


Table \ref{tab:rules} shows the classification map used for determining entity types based on relations for SemEval-2022 Task 12.

\begin{table}[h]
\centering
\scalebox{0.9}{
\begin{tabular}{lcc}
\hline
\textbf{Relation} & \textbf{head entity} & \textbf{tail entity}\\
\hline
Direct & PRIMARY* & SYMBOL\\
Count & PRIMARY & SYMBOL\\
Corefer-Symbol & SYMBOL & SYMBOL\\
Corefer-Description & PRIMARY & PRIMARY\\
\hline
\end{tabular}
}
\caption{\label{tab:rules}Classification map for entity types based on relations in which the spans participate. *In a postprocessing step, entity types of spans which are the head entity of multiple "Direct" relations are adjusted to "ORDERED".}
\end{table}



\subsection{Examples of Spans Detected for Different Values of }
\label{sec:appendix_b}
Examples of spans detected via soft mention detection are shown in figures \ref{fig:spans_cs}, \ref{fig:spans_econ}, \ref{fig:spans_math}, and \ref{fig:spans_physics}.

\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, angle=0]{images/spans_cs.pdf}
    \caption{An example of spans detected in the domain of computer science. The top row shows ground truth labels in green, while the rows below are spans detected at .}
    \label{fig:spans_cs}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, angle=0]{images/spans_econ.pdf}
    \caption{An example of spans detected in the domain of economics. The top row shows ground truth labels in green, while the rows below are spans detected at .}
    \label{fig:spans_econ}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, angle=0]{images/spans_math.pdf}
    \caption{An example of spans detected in the domain of mathematics. The top row shows ground truth labels in green, while the rows below are spans detected at .}
    \label{fig:spans_math}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth, angle=0]{images/spans_physics.pdf}
    \caption{An example of spans detected in the domain of physics. The top row shows ground truth labels in green, while the rows below are spans detected at .}
    \label{fig:spans_physics}
\end{figure*}




\end{document}
