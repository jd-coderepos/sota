\section{Experiments}\label{sec:experiments}

In this section, we present implementation details, the evaluation of our approach in comparison with other relevant SOTA, and an ablation study focusing on various types of interactions. Finally, we present an analysis of the refinement process and the computational complexity of our model. Our model has the advantage of being highly computationally efficient, lightweight and fast to train (see Sec~\ref{sec:complexity}). 
We experiment on three datasets: MuPoTS-3D~\cite{singleshot}, Panoptic~\cite{panoptic} and NBA2K~\cite{zhu_2020_eccv_nba}. Additionally, we include qualitative results on COCO~\cite{coco}. We also use standard metrics for evaluation such as \textbf{MPJPE}~\cite{ionescu_human3.6m:_2014} --which measures the accuracy of the 3D root-relative pose-- and \textbf{3DPCK}~\cite{mehta_monocular_2016} with a threshold of 15cm, as it is standard in the literature~\cite{guo2021pi,smap_eccv2020,hdnet_2020}. Complementary to 3DPCK (from now on PCK), we use \textbf{AUC} (area under the curve) as a more complete metric. Additionally, \textbf{PCK\textsubscript{abs}} is used to evaluate absolute camera-centered 3D human poses.  






\subsection{Implementation details}
\label{sec:implementation}
We optimize our model parameters using ADAM~\cite{adam} with learning rate of 0.0001 in a single GTX 1080 Ti. To estimate the initial 3D human poses we use~\cite{Moon_2019_ICCV_3DMPPE}. Although, any other initialization approach could be used. Regarding training data, although generally used for this task, we discard the use of MuCo-3DHP. Given its synthetic nature it does not contain real interactions between people. Instead, we use \textbf{MuPoTS-3D} as both train and test set which does contain real interactions. For fair comparison, we resource to k-fold cross-validation dividing the dataset into 10 folds. For evaluating on the \textbf{Panoptic} studio~\cite{panoptic} dataset, we follow the evaluation protocol presented in~\cite{zanfir_cvpr_2018_multiple,zanfir_multipeople_nips18}. Finally, please notice that even though the \textbf{NBA2K dataset} is synthetic, it captures plausible interaction between players as opposed to MuCo-3DHP. For more details regarding data and implementation, please refer to the supplementary material.

























\subsection{Comparison with state-of-the-art methods}


We present a direct comparison with the method closest to ours, PI-Net~\cite{guo2021pi}, and show other SOTA methods that also deal with multi-person 3D pose estimation~\cite{HMOR2020,hdnet_2020,smap_eccv2020,zanfir_multipeople_nips18}.
The quantitative results for \textbf{MuPoTS-3D} dataset are reported in Table~\ref{tab:mupots}. Here, we show results of the initialization method RootNet~\cite{Moon_2019_ICCV_3DMPPE} used by both PI-Net~\cite{guo2021pi} and our model. We present, two rows referencing PI-Net~\cite{guo2021pi}. The first one, shows the results when training the model with MuCo-3DHP~\cite{singleshot} dataset, as reported in their work. For a fair comparison, we fine-tune the model with the MuPoTS-3D~\cite{singleshot} dataset and perform the same cross validation. Note that we train and test with MuPoTS-3D due to the reasons explained in Sec.~\ref{sec:implementation}. As it can be seen, our model shows a 3.0\% improvement over this method when estimating the root-relative pose, 2.2\% for AUC for all people and 2.1\% for only matched people. Also, worthy of notice, we remarkably outperform RootNet~\cite{Moon_2019_ICCV_3DMPPE} in all metrics. Comparing to the rest of the methods, our model outperforms the best PCK metric from HDNet by 3.3\%, the best AUC from HMOR by 2.6\%, and the best PCK\textsubscript{abs} from HMOR~\cite{HMOR2020} by 0.3\% . For qualitative comparisons on this dataset, please refer to Fig.~\ref{fig:qualitative}.




\begin{table}[]
\centering
\caption{\ti{Quantitative comparison on the \textbf{MuPoTS-3D}~\cite{singleshot} dataset}. *Results shown for these methods are merely referential as they are not re-trained with the same data as ours. 
Note that both PI-Net and our method use~\cite{Moon_2019_ICCV_3DMPPE} for initialization.}
  \resizebox{0.9 \textwidth}{!}{\label{tab:mupots}
\begin{tabular}{@{}lcccccclll@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Metric}} &
  \multicolumn{3}{c}{All people} &
  \multicolumn{3}{c}{Matched people} &
  \multicolumn{3}{c}{\multirow{2}{*}{Datasets training}} \\ \cmidrule(lr){2-7}
\multicolumn{1}{c}{} & 3DPCK & 3DPCK abs & AUC rel & 3DPCK & 3DPCK abs & AUC rel & \multicolumn{3}{c}{}                        \\ \midrule
Initial (RootNet~\cite{Moon_2019_ICCV_3DMPPE})    & 81.2  & 31.4      & 39.5    & 82.5  & 32.0      & 40.2    & \multicolumn{3}{l}{MuCo + COCO}             \\
PI-Net~\cite{guo2021pi} (MuCo)        & 82.5  & -         & -       & 83.9  & -         & -       & \multicolumn{3}{l}{MuCo}                    \\
PI-Net~\cite{guo2021pi} (fine-tunned) & 82.8  & -         & 43.9    & 84.3  & -         & 44.7    & \multicolumn{3}{l}{MuPots Cross Validation} \\
\textbf{Ours} &
  \textbf{85.8} &
  \textbf{44.1} &
  \textbf{46.1} &
  \textbf{87.3} &
  \textbf{45.0} &
  \textbf{46.9} &
  \multicolumn{3}{l}{MuPots Cross Validation} \\ \midrule
HMOR~\cite{HMOR2020}*                & 82.0    & 43.8      & 43.5    & -     & -         & -       & \multicolumn{3}{l}{MuCo + COCO +}           \\
HDNet~\cite{hdnet_2020}*               & -     & -         & -       & 83.7  & 35.2      & -       & \multicolumn{3}{l}{MuCo}                    \\
SMAP~\cite{smap_eccv2020}*                & 73.5  & 35.4      & -       & 80.5  & 38.7      & 42.7    & \multicolumn{3}{l}{MuCo + COCO}             \\ \bottomrule
\end{tabular}}
\end{table}






 
The results for the \textbf{CMU Panoptic} dataset are shown in Table~\ref{tab:panoptic}. We evaluate our method under MPJPE after root alignment following previous works~\cite{zanfir_cvpr_2018_multiple,zanfir_multipeople_nips18}. The dataset presents a challenging scenario as the majority of images contain several people at a time in a closed environment, severely affected by occlusion and truncation. Our method successfully reduces the interference of occlusions and truncation and improves by a large amount the initial estimations (fine-tuned RootNet~\cite{Moon_2019_ICCV_3DMPPE}). To show how our model is able to deal with truncation, results of the metric calculated over all joints in the dataset are presented in Table~\ref{tab:panoptic}, including those that are out of the image and are not visible. Most of these non-visible joint constitute cases of either truncation or occlusion. Our method improves over 30 mm. in average over the initial method. Note that we also have a significant improvement (14.1 mm) over the initial method if we account only for visible joints which is the standard practice. With regards to the SOTA in this dataset (HMOR~\cite{HMOR2020}), we outperform the method by an overall of 5.3 mm and have a consistent improvement over all the actions. For qualitative comparisons on the Panoptic~\cite{panoptic} dataset, please refer to Fig.~\ref{fig:qualitative} and the supplementary material. 


\begin{table}
\centering
\caption{\ti{Evaluation on the \textbf{Panoptic}~\cite{panoptic} dataset.} RootNet~\cite{Moon_2019_ICCV_3DMPPE} model was fine-tuned with CMU Panoptic data to provide a better initialization. The reported metric is \textbf{MPJPE} relative to the root joint and results are reported in mm. *The average of~\cite{zanfir_multipeople_nips18} and~\cite{HMOR2020} are recalculated following the standard practice in~\cite{zanfir_cvpr_2018_multiple} and~\cite{smap_eccv2020} (i.e. average over activities) for a more direct comparison.}
\label{tab:panoptic}
  \resizebox{0.6\textwidth}{!}{\begin{tabular}{@{}lccccc@{}}
\toprule
\multicolumn{1}{c}{Method}     & Haggling      & Mafia         & Ultim.        & Pizza         & Mean  \\ \midrule
RootNet~\cite{Moon_2019_ICCV_3DMPPE} (w/ all joints)  & 83.3 & 107.9 & 106.0 & 118.4 & 103.9 \\
Ours (w/ all joints)     & 59.4 & 68.9  & 67.2  & 86.4  & 70.5  \\ \midrule
Zanfir \et~*\cite{zanfir_multipeople_nips18} &72.4	&78.8	&66.8	&94.3	&78.1\\
RootNet~\cite{Moon_2019_ICCV_3DMPPE}  & 52.1 & 65.3  & 58.0  & 80.4  & 63.9  \\
SMAP~\cite{smap_eccv2020}                    & 63.1 & 60.3  & 56.6  & 67.1  & 61.8  \\
HMOR*~\cite{HMOR2020}                     & 50.9 & 50.5  & 50.7  & 68.2  & 55.1  \\
\textbf{Ours} & \textbf{42.0} & \textbf{50.3} & \textbf{47.3} & \textbf{59.4} & \textbf{49.8}     \\\bottomrule
\end{tabular}}
\end{table}



%
 
\begin{table}
\centering
\caption{\ti{Evaluation on the \textbf{NBA2K}~\cite{zhu_2020_eccv_nba} dataset}. We use the \textbf{MPJPE} metric. *This method has been fine-tuned with the same dataset and uses the same initial method (RootNet~\cite{Moon_2019_ICCV_3DMPPE}) for fair comparison.}
\label{tab:nba_results}
  \resizebox{0.9 \textwidth}{!}{\begin{tabular}{@{}lccccclccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{MPJPE {[}mm{]}}                         &  & \multicolumn{5}{c}{MPJPE-PA {[}mm{]}}             \\ \cmidrule(lr){2-6} \cmidrule(l){8-12} 
\multicolumn{1}{c}{}                        & Cory  & Glen  & Oscar & Tomas          & Mean  &  & Cory  & Glen  & Oscar & Tomas & Mean  \\ \midrule
RootNet~\cite{Moon_2019_ICCV_3DMPPE}                                        & 154.3 & 167.7 & 159.3 & 136.2          & 154.1             &  & 115.8 & 137.5 & 122.4 & 103.9 & 119.7             \\ \midrule
Pi-Net* (fine-tunned)~\cite{guo2021pi}                            & 136.6 & 155.3 & 140.2 & \textbf{119.8} & 137.8             &  & 109.7 & 129.2 & 111.5 & 96.2  & 111.5             \\ \midrule
\textbf{Ours} &
  \textbf{130.0} &
  \textbf{142.0} &
  \textbf{134.7} &
  121.7 &
  \textbf{131.9} &
   &
  \textbf{99.6} &
  \textbf{111.5} &
  \textbf{104.4} &
  \textbf{95.8} &
  \textbf{102.7} \\ \bottomrule
\end{tabular}}
\end{table}


%
 
For evaluating on the \textbf{NBA2K dataset}, we use the MPJPE without and with Procrustes Alignment (MPJPE-PA) as shown in Table~\ref{tab:nba_results}. See how both methods that use interaction information from the scene (Pi-Net and ours) are able to improve the results over the initial estimations (RootNet~\cite{Moon_2019_ICCV_3DMPPE}). Furthermore, our method outperforms the baseline (PI-Net~\cite{guo2021pi}) which we fine-tune with this dataset. Moreover, our method shows to be superior than the baseline at capturing interactions and refining the 3D pose of multiple people (see Fig.~\ref{fig:qualitative}). This are particularly interesting results as the dataset contains several people in each scene with high levels of interaction. 














\begin{table}
\centering
\caption{\ti{Importance of different levels of ~\textbf{interaction} in our model.} For each dataset we have 4 different models: the initialization method (RootNet~\cite{Moon_2019_ICCV_3DMPPE}), a no interaction baseline, a baseline with interaction of all ungrouped joints in the scene (scene interaction), and interaction between joints grouped by persons (people interaction). The model that captures interactions between people makes better pose refinements. See text for more details.}
\label{tab:ablation}
  \resizebox{0.6\textwidth}{!}{\begin{tabular}{@{}llcc@{}}
\toprule
\multicolumn{1}{c}{Dataset} & \multicolumn{1}{c}{Metric} & MPJPE {[}mm{]} & MPJPE-PA {[}mm{]} \\ \midrule
\multirow{4}{*}{MuPoTS-3D}    & Initial (RootNet)                  & 134.9          & 93.3          \\
                              & Baseline – no interaction          & 136.4          & 95.1          \\
                              & Baseline – scene interaction       & 134.5          & 96.1          \\
                              & \textbf{Ours – people interaction} & \textbf{104.8} & \textbf{79.7} \\ \midrule
\multirow{4}{*}{CMU Panoptic} & Initial (RootNet)                  & 63.9           & 54.6          \\
                              & Baseline – no interaction          & 57.0           & 47.4          \\
                              & Baseline – scene interaction       & 58.2           & 45.1          \\
                              & \textbf{Ours – people interaction} & \textbf{49.8}  & \textbf{40.5} \\ \bottomrule
\end{tabular}}
  \vspace{-0.3cm}
\end{table} \begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth, trim={0.1cm 0.1cm 0cm 0}, clip = true]{figures/analysis.png}
        \put(-370,-2){{{Input image}}}
        \put(-285,-2){{{People interaction}}}
        \put(-185,-2){{{Scene interaction}}}
        \put(-85,-2){{{No interaction}}}
        

  \vspace{-0.0cm}
  \caption{\small{\textbf{Interaction analysis.} We show the effect of one joint over all other joints in the scene. Joints are grouped by person. We present 17 joints for each person. Each person's number in the matrices corresponds to the number shown in the bounding box in the images. The magnitude of each matrix element represents the maximum displacement in 3D space measured in meters of the joints in each row caused by the corresponding joint in each column.} 
  }
  \label{fig:interactions}
  \vspace{-0.1cm}
\end{figure*}
 
\subsection{Interaction vs. no interaction}
Having shown the effectiveness of our method at refining 3D poses, we continue with a careful analysis of our interaction component. Table~\ref{tab:ablation} shows how the level at which we enforce the interaction to be learned affects the performance in comparison to the initial estimations. We define three different levels of interaction: (1) no interaction, (2) scene interaction, and (3) people interaction. The latter corresponds to our final method. For all the cases, we use the same architecture. However, we change the interaction levels by changing what we input to our method. To eliminate learning interactions ({\em no interaction}), we input each person's pose individually as a unique and different set and not together. In this manner, it is impossible for the model to build an interaction-based embedding. At most, the model remains restricted to capture self-joint interactions. To enforce learning what we refer to as \ti{scene interaction}, we make each joint in the scene a set by itself. Having each joint as set element instead of the whole person's pose, we enforce a representation that can learn interactions between joints but without knowing which joint corresponds to which person. Thus, loosing the sense of person as an "entity". The results from Table~\ref{tab:ablation} show that our method based on people's interaction clearly outperforms other degrees of interaction consistently over both datasets. We report the results on the MuPoTS-3D and Panoptic datasets and use the MPJPE metric only for simplicity. 

Figure~\ref{fig:interactions} gives us insight on what happens with the refinement on each level of interaction. Here, we present two images containing three persons each: high interaction between individuals (top image), low interaction (bottom image). Additionally, for each image we show three matrices, each regarding an interaction type presented above. Each matrix depicts the effect of the pose refinement when perturbing one joint over all of the joints of every person in the scene. This also includes the person whose joint has been perturbed. Each column represents the joint being perturbed and each row represents the affected joints. The perturbation applied to the joints is a displacement in the positive directions of x, y and z in the 3D space by 10 cm. The magnitude in each element in the matrix represents the maximum absolute value of the change in any of the 3D space coordinate direction in meters. With this setup, we can study how one person's joint affects other person's joints as well as its own. Here, we notice some key observations. (1) for the cases in which we enforce to learn interactions (people and scene), the effect of one person's joint over other person's joints (effect of interaction) is higher for the image on the top (high interaction) than for the one in the bottom (lower interaction). (2) in the case of people interaction, it can be clearly seen that one person's joint affects in greater degree the pose of its own body in contrast to other people's body. This is expected, as the model here has the notion of which joints correspond to which person. This does not happen in the scene interaction case. Also, (3) we can confirm that in the case of no-interaction, one person's pose has no effect over others. The reader is referred to the supplemental material for additional examples.






\begin{figure*}[t!]
  \includegraphics[width=\linewidth, trim={0.1cm 0.1cm 0cm 0}, clip = true]{figures/fig_explanation.png}
        \small{\put(-380,-2){{{Input image}}}}
        \small{\small{\put(-292,125){{{Side-view}}}}}
        \small{\small{\put(-290,51){{{Bird-view}}}}}
        \small{\put(-308,-2){{{Initial Estimation}}}}
        \small{\small{\put(-215,125){{{Side-view}}}}}
        \small{\small{\put(-215,51){{{Bird-view}}}}}
        \small{\put(-225,-2){{{Refined Poses}}}}
        \small{\small{\put(-139,125){{{Side-view}}}}}
        \small{\small{\put(-139,51){{{Bird-view}}}}}
        \put(-150,-2){{{Ground Truth}}}
        \put(-80,-2){{{Overlay poses for P\textsubscript{1}}}}
        
  \centering
  \vspace{-0.0cm}
  \caption{\small{\textbf{Effects of the pose refinement.} From left to right: Input image, initial 3D pose estimations, refined poses, ground truth, and detail of the update on person P's joints. For each estimation we include a bird-view so that absolute translation is better appreciated. The last column shows the root-relative pose improvement.} 
  }
  \label{fig:explain}
  \vspace{-0.3cm}
\end{figure*}



%
 \subsection{Effects of the refinement over initial estimations}
We show the effect of our model in refining the initial estimations. Our method can improve both absolute and root-relative poses while more effectively dealing with inter-person occlusions and truncations. This is achieved because our \textit{interaction embedding} enables the model to reason directly in the 3D space, whereas other methods can only reason from 2D image cues. Furthermore, our loss encourages the model to learn a refinement for both the absolute and the root-relative poses. See Fig.~\ref{fig:explain}. The three middle columns depict the initial estimation, the refined poses and the ground truth from a slightly rotated camera view along with a bird-view, respectively. From these views, we can appreciate the interaction between person 1 (P\textsubscript{1}) and person 3 (P\textsubscript{3}). The initial estimation does not take into account this interaction and, therefore, makes the mistake of overlapping the two bodies. Our model yields to more realistic estimations by exploiting these interactions. The rightmost picture shows the initial, refined and ground truth root-relative poses for P\textsubscript{1}. See how our estimations correct the initial joint positions taking them closer to the ground truth (highlighted in red arrows). For example, the hands and ankle joints are closer to the ground truth than the initial estimations. The same happens with the hip joints. Additionally, Fig.~\ref{fig:teaser} shows the input image, the ground truth and the refined poses overlapped with the initial estimations (in transparency) both in a tilted camera view and a bird-view. Here, we can also appreciate an interaction between the people that are about to hug (P\textsubscript{2} and P\textsubscript{3}). Better seen in the bird-view, our refinement locates both persons in a more coherent way, whereas, the initial estimation places them further apart.



\subsection{Qualitative Results}



Qualitative results on the MuPoTS-3D, Panoptic, NBA2K ana COCO datasets are shown in Fig.~\ref{fig:qualitative}. Row 1 fist column shows a case where people are closely interacting with each other (holding hands). Our model corrects the persons poses so their hands are closer together. Row 1 second column shows how our method corrects cases of severe truncation. Row 2 shows results on the NBA2K~\cite{zhu_2020_eccv_nba} dataset and the last row shows as well results on images-in-the-wild from COCO~\cite{coco} dataset. NBA2K~\cite{zhu_2020_eccv_nba} 
presents several interactions in each scene. We show how our method improves over the SOTA~\cite{Moon_2019_ICCV_3DMPPE}, especially in cases were people need to be grouped closely together. Our method captures the interactions between the players and can determine which players should be grouped together. In each case, with dotted red circles, we show either a correctly located group of players (refined poses) or incorrectly placed players (initial poses).



\subsection{Computational complexity}
\label{sec:complexity}
Our model is fast and has very few parameters. Also, we need less data to train to make the refinements in direct comparison to~\cite{guo2021pi} and indirectly compared to~\cite{Moon_2019_ICCV_3DMPPE,HMOR2020,smap_eccv2020}. We use 90K model parameters, need 5.1M FLOPS to make an inference which takes 2ms and we required roughly 15K training samples. In comparison to~\cite{Moon_2019_ICCV_3DMPPE}, which takes 250.7ms to make an inference, using our model as a refinement step constitutes a negligible overhead. For a more comprehensible comparison table, please refer to our supplementary material.







%
