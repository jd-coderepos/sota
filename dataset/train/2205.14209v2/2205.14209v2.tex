\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc} \usepackage{hyperref} \usepackage{url} \usepackage{booktabs} \usepackage{amsfonts} \usepackage{nicefrac} \usepackage{microtype} \usepackage{lipsum}
\usepackage{fancyhdr} \usepackage{graphicx} \usepackage{subfigure}
\usepackage{colortbl}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{appendix}
\usepackage[linesnumbered,ruled]{algorithm2e}

\usepackage{amsmath}
\usepackage{amsthm}

\graphicspath{{media/}} 

\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 






\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\title{StarGraph: \\Knowledge Representation Learning \\based on Incomplete Two-hop Subgraph}

\author{
 Hongzhu Li, Xiangrui Gao\thanks{First author and second author contribute equally to this work}, Linhui Feng, Yafeng Deng, Yuhui Yin\thanks{Corresponding authors} \\
 Qihoo 360 AI Research\\
 \texttt{\{lihongzhu, gaoxiangrui, fenglinghui, dengyafeng, yinyuhui\}@360.cn} \\
}


\begin{document}
\maketitle
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector, ignoring the rich information contained in the neighborhood. We propose a method named StarGraph, which gives a novel way to utilize the neighborhood information for large-scale knowledge graphs to obtain entity representations. An incomplete two-hop neighborhood subgraph for each target node is at first generated, then processed by a modified self-attention network to obtain the entity representation, which is used to replace the entity embedding in conventional methods.
We achieved SOTA performance on ogbl-wikikg2 and got competitive results on fb15k-237. The experimental results proves that StarGraph is efficient in parameters, and the improvement made on ogbl-wikikg2 demonstrates its great effectiveness of representation learning on large-scale knowledge graphs.
The code is now available at \url{https://github.com/hzli-ucas/StarGraph}.
\end{abstract}



\section{Introduction}
A Knowledge Graph (KG) is a directed graph with real-world entities as nodes and relationships between entities as edges. In this graph, each directed edge together with its head and tail entities forms a triple (head entity, relation, tail entity), indicating that the head and tail entities are connected by a relation.

Knowledge graph embedding (KGE), also known as knowledge representation learning (KRL), aims to embed entities and relations into low-dimensional continuous vector spaces to characterize their latent semantic features. A scoring function is defined to measure the plausibility for triples in such spaces, then the embeddings of entities and relations are learned by maximizing the total plausibility of the observed triples. These learned embeddings can be used to implement various tasks such as knowledge graph completion \cite{bordes2013translating,wang2014knowledge}, relationship extraction \cite{riedel2013relation}, entity classification \cite{nickel2011three}, etc.
The plausibility of each triple is calculated on the embeddings of the entities and relations in it, and the embeddings are directly taken out from the embedding tables.
Such a shallow lookup decides that those models are inherently transductive.
Moreover, the rich contextual information contained in the neighboring triples is not taken into account.

Compared with shallow embedding models, methods that are able to encode neighborhood information, usually perform much better across various KG datasets \cite{zhang2018link,zhang2021labeling,wang2019coke}. Any generic graph neural networks could be employed as the encoder.
However, there is a problem adopting these methods to large-scale knowledge graphs, for previous work \cite{nathani2019learning,Wang2020KnowledgeGE} takes the multi-hop subgraph of the node as input. Due to the large number of nodes and edges, multi-hop subgraphs in large-scale graphs can easily exceed the size limitation, and the subgraphs generation and network calculations can both be very time-consuming.

The neighborhood surely contains information for the target node, therefore can be used for learning its representation.
In order to adopt neighborhood neural encoders in large-scale KG, an intuitive idea is to utilize partial neighborhood information instead of the complete multi-hop subgraph.

In this paper, we propose to learn the knowledge representation for each target node based on its incomplete 2-hop neighborhood subgraph. 
Neighbors out of reach within two hops are not as closely related to the target node according to us, so are not taken into consideration. 
An example of a complete 2-hop subgraph is given in Figure~\ref{fig:star_graph}(a), where we can see, even in such a small knowledge graph, the 2-hop subgraph comprises quite a few nodes and edges and seems to contain a lot of redundant information. It is more efficient to construct a proper incomplete subgraph with a few nodes and edges.

We got inspiration from the anchor-based strategy \cite{galkin2022nodepiece}, which selects a small fraction of all the graph nodes as anchors and learn embeddings only for anchors instead of all nodes. In our work, we sample anchors from the 2-hop neighborhood, along with the edges to reach each anchor, to construct the incomplete 2-hop subgraph, which is illustrated in Figure~\ref{fig:star_graph}(b). The incomplete subgraph is not restricted to contain only anchors, we can also sample some general nodes into the subgraph as supplementary information. 
\begin{figure}
 \centering
 \subfigure[complete 2-hop subgraph]{\includegraphics[width=0.45\textwidth]{complete}}
 \subfigure[incomplete 2-hop subgraph]{\includegraphics[width=0.45\textwidth]{incomplete}}
 \caption{Illustration of a subgraph generated by StarGraph. Dots and lines represent nodes and edges in the graph, respectively, with larger dots indicating anchors. The color red indicates the example target node and the sampled subgraph.}
 \label{fig:star_graph}
\end{figure}

In order to reasonably model the subgraph structure and enable sufficient interaction of node embeddings, we adopt a self-attention network to extract the neighborhood information. Taking the characteristics of knowledge graphs into consideration, we modify the attention module to be more efficient and propose a novel way to embed the edges in a graph.

Comparing the subgraphs in Figure~\ref{fig:star_graph}(a) and (b), the trimmed subgraph is much more efficient and is likely to be more effective to describe the target node, especially for large-scale knowledge graphs. And this is demonstrated by our experimental results.
The way a node being represented by the incomplete subgraph is like how to locate a star in the sky, which is to use a few bright stars (anchors) to indicate the location. For the incomplete subgraph in the entire graph looks like a constellation among the stars, we call the proposed method \emph{StarGraph}.

\section{Related Work}
\textbf{Distance-based models} consist a major branch of knowledge graph embedding methods.
TransE \cite{bordes2013translating} embeds relations and entities into the same vector space, and the relation embedding is interpreted as a translation from the embeddings of the head entity to the tail entity. RotatE \cite{sun2018rotate} uses the rotations of vectors to explain various relations, where the inverse relations can be modeled by complex embeddings. PairRE \cite{chao2021pairre} propose to learn two embeddings for each relation, respectively used to map head and tail entity embeddings into the corresponding relation space. TripleRE \cite{yu2021triplere} learns another embedding for each relation on the basis of PairRE, and the extra relation embedding is used as a translation vector between the mapped entity embeddings of head and tail.

\textbf{Contextual information} has been widely used for knowledge graphs to enhance model performance, but in many different ways. CoKE \cite{wang2019coke} treats link prediction as a natural language processing (NLP) task, and trains the NLP model with long entity-relation-sequences taken from the graph, linking entities originally distant from each other more closely. 
GC-OTE \cite{tang2020orthogonal} directly integrates the graph context into the distance scoring function by considering tail/head entities with the same head/tail entity and relation all at once and minimizing the distance between their embeddings.
NodePiece \cite{galkin2022nodepiece} borrows the idea of subword-tokenization from NLP and proposes to use a small number of nodes as anchors, constructs vocabulary for each entity using surrounded anchors and relations, which implicitly includes contextual information, and calculates node representations from this.

\textbf{Graph encoders} can be conventional graph neural networks, e.g, GCN \cite{kipf2017semi} and GraphSAGE \cite{hamilton2017inductive}, or the recently proposed Graph Transformer \cite{dwivedi2021generalization} and Graphormer \cite{ying2021transformers}, whose work are developed on the basis of Transformer \cite{vaswani2017attention}.
The basic unit in Transformer is the self-attention module, which provides a novel way to deal with the inputs. It takes a sequence of feature vectors, (tokens as being called) as inputs and updates each token with all others within the sequence, which is controlled with an attention matrix. It provides an effective way for information exchange and allows the tokens staying unordered, which seems suitable to process the structure of graphs.

The graph attention networks \cite{nathani2019learning} and graph attenuated attention networks \cite{Wang2020KnowledgeGE} base their work on ConvVB \cite{dai2018novel} and CapsE \cite{vu2019capsule}, which are proved invalid by Sun et al. \cite{sun2020a}, thus will not be compared with in this paper.

\section{Methodology}
The StarGraph is a method to obtain entity representations based on incomplete neighborhood information, which includes two stages, generating and encoding  a subgraph for each node. The obtained entity representations, together with the relation embeddings, are used to compute the scores of the triples by a distance-based score function, and optimized with the self-adverse negative sampling loss \cite{sun2018rotate}. We will describe each part in detail below.

\subsection{Subgraph Generation}
To generate the proper incomplete subgraphs, we should start with select part of the nodes to form the anchor set. For ease of description, we denote the anchor set as , all nodes of the total graph as . There are , and . Then for each target node, we pick part of the anchors and nodes within its 2-hop neighborhood to construct the subgraph.

\subsubsection{Anchor Set}
\label{sec:anc_set}
At first, a minority of nodes are selected as anchors. When the number of anchors is fixed and much less than the number of graph nodes, the most intuitive idea is that the anchors should be selected according to the degree of centrality in order to provide common information to as many nodes as possible.
Following this idea, Galkin et al. \cite{galkin2022nodepiece} employs the deterministic anchor selection strategy where 40\% of the total number of anchors are nodes with top Personalized Page Rank \cite{page1998the} scores, 40\% are top degree nodes, but remaining 20\% are selected randomly for they found random anchor selection to be as effective as centrality-based strategies.
We argue that the random part introduce the uncertainty, and anchor sets generated by the same strategy may have different performances in experiments. Besides, there is no need to use two centrality measures, and we only use the degree of nodes as the measure.

If the selected anchors are too concentrated, it may result in that some nodes are surrounded by too many anchors while some other nodes have no anchors around them. To alleviate this problem, we introduce a hyper-parameter \emph{skip-threshold}. When the percentage of anchors in the neighboring nodes of a node exceeds the threshold, that node will not be selected as an anchor. We also need to specify the \emph{anchor-set-size}, i.e., how many nodes are selected as anchors. The process of generating the anchor set can be seen in Algorithm~\ref{alg:ancgene}. 
Detailed discussion on the effect of \emph{skip-threshold} can be found in Appendix~\ref{apd:anchors}, but in the main text the value is fixed at 0.5.

\begin{algorithm}
\caption{Generate the anchor set for the graph}
\label{alg:ancgene}
\DontPrintSemicolon
\SetAlgoLined
\KwIn {graph\_nodes\ ,\ skip\_threshold,\ anchorset\_size}
\KwOut {anchor\_set\ }
\;
\;
\For{}{
  \;
  \;
  \If{}{
    \textbf{continue}\tcp*{many anchors around, skip this node}
  }
  \;
  \If{}{
    \textbf{break}\tcp*{got enough anchors, end the loop}
  }
}
\end{algorithm}

\subsubsection{Anchors Sampling}
\label{sec:anc_smp}
For each node, a fixed number of anchors are sampled from its 2-hop neighbors, together with the paths to reach the anchors, to construct the incomplete subgraph. When the anchors within two hops are more than needed, for example, sampling six anchors for the target node from its 2-hop subgraph given in Figure~\ref{fig:ancsmp}(a), only the first part of the anchors sampled will be retained. The easiest sampling order to think of is breadth-first-search (\emph{BFS}), but it might induce severe imbalance in anchors distribution, as shown in Figure~\ref{fig:ancsmp}(b).
The sampled anchors are concentrated around node\#2, and this sampling strategy fails to associate the target node with some non-anchor neighbors.

We propose a more \emph{balanced} sampling strategy that aims to share more anchors with the non-anchor neighbors, by sampling one-hop anchors of the target node and its every non-anchor neighbor in turn. The detailed steps are described in Algorithm~\ref{alg:ancsmp}, and the sampling results of this strategy are shown in Figure~\ref{fig:ancsmp}(c). Note that anchors with smaller degrees will be selected in preference according to line\#7 and \#13 in Algorithm~\ref{alg:ancsmp}. It is based on the considering that 1) 
less-popular anchors make each target node more recognizable from others; 2) less-popular anchors can be visited more often to alleviate the long-tail distribution problem in the usage of anchors.

\begin{figure}
 \centering
 \subfigure[2-hop subgraph]{\includegraphics[width=0.2\textwidth]{subgraph}}
 \subfigure[BFS]{\includegraphics[width=0.2\textwidth]{BFS}}
 \subfigure[balanced]{\includegraphics[width=0.2\textwidth]{balanced}}
 \caption{Six anchors sampled from the 2-hop subgraph of the target node  adopting different strategies. Dots and lines represent nodes and edges, where colored dots are anchors and solid lines indicate how we reach the anchors. The numbers on dots indicate the sampling order of anchors.}
 \label{fig:ancsmp}
\end{figure}

\begin{algorithm}
\caption{Sample anchors for subgraph of each node}
\label{alg:ancsmp}
\DontPrintSemicolon
\SetAlgoLined
\KwIn {anchor\_set\ ,\ node,\ sample\_size}
\KwOut {subgraph\_anchors}
\textbf{Def }\textbf{ as }\;
\;
\;
\While{}{
  \;
  \If{}{
    \;
    \;
  }
  \For{}{
    \;
    \If{}{
      \;
      \;
    }
  }
}
\end{algorithm}

\subsubsection{Neighbors \& Center}
\label{sec:nbors_center}
\emph{Neighbors} refer to a fixed number of nodes sampled from one-hop neighborhood of the target node, and \emph{center} refers to the target node itself. We use these nodes as additional information to further enrich the description of the target node. Different from anchors, that are picked from the predefined anchor set, neighbors and centers could be any node in the graph. We sample the neighbors in a degree-decreasing order, trying to use them to make a common description for each target node.

\subsection{Subgraph Encoding}
A self-attention network \cite{vaswani2017attention}, whose input is considered as a sequence though, its operation on each element within the sequence is naturally global and undifferentiated. Therefore, we consider it a perfect choice to encode graphs.
The network takes the embeddings of nodes within a generated subgraph as inputs, each node treated as a token. 
Since a self-attention layer has a global receptive field, one layer can adequately mix the information of all nodes. We use a single standard transformer block, i.e. one attention layer followed by two linear layers along with residual connections and layer normalization, to construct the network.
After this, the output node features are aggregated by mean-pooling across the subgraph to get the representation of target node. More discussion on the choice of encoders can be found in Appendix~\ref{apd:path_aggr}.

\subsubsection{Reduced Attention Module}
\label{sec:reduced}
The attention module consumes a lot of computational resources, so we try to explore a more efficient way to implement the attention mechanism. For a standard -heads attention layer, it takes  as input, sets the dimension of each head as , and gets the output  by concatenating the outputs of  heads together. The output of the -th head  is calculated by

where  are model parameters to be learned.

The role of the attention module is to integrate information from all nodes in the subgraph. Ensuring this, we make two modifications to attention-head to reduce the amount of computation.
Firstly, to get a usable attention matrix, the second dimension of  and  is not forced to equals , so setting a smaller dimension  will reduce the computational complexity. Secondly, with an MLP afterward, there is no need to transform the features in the information mixing step. Since , we can directly slice  into  parts with the same shape of  to omit the multiplication operation of  and .
The output of the -th head  of a reduced attention layer is calculated by

where , and  are model parameters to be learned.

Beside the attention layer, the computational complexity of MLP can be reduced by decreasing the intermediate dimension . The default value of hyper-parameter \emph{mlp-ratio} is 4, and a smaller value can be considered.

\subsubsection{Path Information Fusion}
\label{sec:path}
Treating each anchor as a token, the anchor path, i.e., edges the anchor needs to pass to reach the target node, should be considered as the position information. Instead of following the practice of previous work, 
where the position embeddings are directly added onto the token embeddings, we propose to take the characteristics of KGE into consideration. 
Insprired by the score function of TripleRE \cite{yu2021triplere}, 

based on the assumption that

we simplify the conditions and assume that


Substituting ``head, relation, tail'' with ``anchor, path, token'', respectively, we can get token embedding by adding path information to 1-hop or 2-hop anchor as

Note that after incorporating the path information, the token embedding is more like the representation of the target node rather than the original anchor, therefore mean-pooling is a proper way to aggregate information from the tokens.

\subsubsection{Supplementary Nodes}
As described in section~\ref{sec:nbors_center}, a subgraph may contain nodes sampled from , providing additional information for the target node. As with the entire graph, each node keeps some unique information, while only a small group of nodes are anchors, carrying much general information. Denoting the embedding sizes of  and  as  and , we set a large  to improve the information capacity, and a small  to reduce the probability of overfitting. To get the tokens of the same embedding size,  is used as , the hidden size for attention module, and the node embeddings are mapped to dimension  by a linear layer ahead.

Type embedding is added to the token embedding before the attention module, indicating each node within the subgraph to be the type of anchor, neighbor or center.

\subsection{Score Function}
We adopt a score function that is modified upon TripleRE \cite{yu2021triplere}, a recently proposed distance-based method. The formula of TripleREv2 is

where  denotes the element-wise product,  and  correspond to the head and tail entity embeddings respectively,  comprise the relation embedding,  is a constant specified by users, set to 1 in the original work.
We modify equation (\ref{equ:triplere_v2}) by switching  with , so that the coefficient  acts on the relation embedding.
Now the score of triplets  is calculated as

which can be interpreted as a boosting method based on TransE and PairRE. The new form is referred to as \emph{TripleRE'} in the rest of this paper. Although TripleRE' is exactly the same as TripleREv2 when  equals 1, according to our experimental results, TripleRE' usually performs better than TripleREv2 during tuning the hyper-parameter .

\subsection{Optimization}
We utilize the self-adversarial negative sampling loss \cite{sun2018rotate} as objective for training,

where  is a fixed margin and  is the sigmoid function. We randomly sample  negative triples for each positive one, and  is the i-th negative triple.

\section{Experiments}
\subsection{Datasets and Metric}
\textbf{ogbl-wikikg2} \cite{hu2020open} is a large-scale knowledge graph extracted from the Wikidata knowledge base \cite{vrandevcic2014wikidata}, containing 2,500,604 entities, 535 relation types and 17,137,181 triplet edges (head, relation, tail). \textbf{fb15k-237} \cite{toutanova2015observed} is a subset of \emph{fb15k} \cite{bordes2013translating}, that consists of triples from Freebase, with inverse relations removed. It contains 14,505 entities, 237 relation types and 310,079 triplet edges.
\paragraph{Task and Metric} The task is to predict new triplet edges given the training edges. The evaluation metric follows the standard filtered metric widely used in KG. Specifically, each test triplet edges are corrupted by replacing its head or tail with negative entities. The goal is to rank the true head (or tail) entities higher than the negative entities, which is measured by Mean Reciprocal Rank (MRR).

\subsection{Detailed Settings}
We set batch-size to 512, negative-sampling-size to 64, and train the model for 500,000 steps on ogbi-wikikg2 and 100,000 steps on fb15k-237. The learning rate is initialized to 0.0001 unless otherwise stated, and decreased by 0.1 at half the maximum training step. Dropout is adopted after each linear layer, and the drop-ratio is 0.05. AdamW is used as the optimizer in our experiments.

For the generated incomplete subgraphs, anchors are always required. The paths and the supplementary nodes are optional, which are not included in subgraphs unless being specified. The number of anchors and neighbors (if used) sampled for each subgraph is fixed to 20 and 5 respectively, and any shortfall is filled with the [PAD] token. The default values for the embedding dimension are , . 
Hyper-parameter  in equation (\ref{equ:triplere_new}) is tuned and set to 0.1 on ogbi-wikikg2 and 1.0 on fb15k-237, and  in equation (\ref{equ:objective}) is set to 6.0 following PairRE \cite{chao2021pairre}.

\subsection{Ablation Study}
\label{sec:ablation}
\paragraph{Anchors} The strategies for anchor set generation and anchors sampling are proposed in section~\ref{sec:anc_set} and~\ref{sec:anc_smp} respectively, and display the effects of them in Table~\ref{tab:anchors}. Comparison between the first two rows validates the effectiveness of the proposed sampling strategy by a 1.5\% gap. The results of the second and third rows are close to each other, despite the uncertainty in \emph{degree+PPR+rand} does not exist in our generation strategy. Comparing the last two rows, the performance gain shows that a larger anchor set performs better, at least under the condition that .
 For the following experiments, the row marked in gray is used as the default setting with anchors unless otherwise specified.
\begin{table}
 \caption{The results on ogbl-wikikg2 with different anchor sets and anchor-sampling strategies.}
 \centering
 \begin{tabular}{lcc|cc}
 \toprule
 Anchor Set &  & Sampling & Test MRR & Valid MRR \\
 \midrule
 degree+PPR+rand \cite{galkin2022nodepiece} & 20,000 & BFS & 0.6899 & 0.6953 \\
 degree+PPR+rand \cite{galkin2022nodepiece} & 20,000 & balanced & 0.7040 & 0.7104 \\
 \rowcolor{gray!30}degree-skip0.5 & 20,000 & balanced & 0.7042 & 0.7093 \\
 degree-skip0.5 & 80,000 & balanced & 0.7084 & 0.7164 \\
 \bottomrule
 \end{tabular}
 \label{tab:anchors}
\end{table}

\paragraph{Network}
The first two rows in Table~\ref{tab:network} show that, the larger dimension of features (also known as  and \emph{hidden-dim} in this paper) the better result, meantime the higher computational complexity. Comparing the overall performance of network 256-32-4 with its reduced counterpart 256-R32-4, we can see that the reduced attention module proposed in section~\ref{sec:reduced} saves a little training time without degrading the results. With further adaptation of \emph{attn-dim} and \emph{mlp-ratio}, network 512-R8-2 achieves a better trade-off between the results and the consumption of training time.
The row marked in gray is used as the default network for the following experiments, unless otherwise specified.

\begin{table}
 \caption{The results on ogbl-wikikg2 with different networks. The `R' in Network ID indicates the reduced attention module, operating like equation (\ref{equ:reduced}). Train Time refers to the practical time spent for a whole training process, and every experiment is performed on a single A100 GPU.}
 \centering
 \begin{tabular}{l|ccc|ccc}
 \toprule
 Network ID & hidden-dim & attn-dim & mlp-ratio & Train Time & Test MRR & Valid MRR \\
 \midrule
 256-32-4 & 256 & 32 & 4 & 28.7h & 0.7042 & 0.7093 \\
 512-64-4 & 512 & 64 & 4 & 48.7h & 0.7141 & 0.7189 \\
 \midrule
 \rowcolor{gray!30}256-R32-4 & 256 & 32 & 4 & 27.4h & 0.7067 & 0.7085 \\
 512-R8-2 & 512 & 8 & 2 & 33.3h & 0.7129 & 0.7146 \\
 \bottomrule
 \end{tabular}
 \label{tab:network}
\end{table}

\paragraph{Subgraph Components} On the basis of anchors, other types of information are added to subgraphs in turn. The results in Table~\ref{tab:info} show that the addition of both path and supplementary nodes information can make obvious improvement. The subgraphs containing all kinds of information have the best results overall.
\begin{table}
 \caption{The results on ogbl-wikikg2 with subgraphs containing different types of information. The \checkmark indicates the corresponding type of information to be contained.}
 \centering
 \begin{tabular}{cccc|cc}
 \toprule
 anchors & path & center & neighbors & Test MRR & Valid MRR \\
 \midrule
 \checkmark & & & & 0.7067 & 0.7085 \\
 \checkmark & \checkmark & & & 0.7222 & 0.7373 \\
 \checkmark & & \checkmark & & 0.7197 & 0.7235 \\
 \checkmark & & & \checkmark & 0.7248 & 0.7307 \\
 \checkmark & & \checkmark & \checkmark & 0.7248 & 0.7319 \\
\checkmark & \checkmark & \checkmark & \checkmark & 0.7239 & 0.7403 \\
 \bottomrule
 \end{tabular}
 \label{tab:info}
\end{table}



\subsection{Results on ogbl-wikikg2 \& fb15k-237}
We combine the advantaged strategies and modifications in section~\ref{sec:ablation} to conduct experiments, and present some of our best results in Table~\ref{tab:wiki} and~\ref{tab:fb}, with the corresponding experiment settings given in Appendix~\ref{apd:hyper_param}, showing the trade-off between the number of parameters and the results.

Our method achieves state-of-the-art results on ogbl-wikikg2 with a significant 3\%-ish improvement.
We presume that the construction of incomplete multi-hop subgraphs is a summarization and extraction of the basic concepts in a knowledge graph, and much information is already stored in subgraph structures, making the learning task easier and thus leading to better results, especially for large-scale knowledge graphs. 
On fb15k-237, our results are constrained by the distance-based model, for the parameters in StarGraph are optimized through the score function of TripleRE'. So we directly train the embeddings of entities and relations with TripleRE' as the conduct group. StarGraph does not achieve as significant improvement on fb15k-237 as on ogbl-wikikg2 though, the results are still comparable with its conventional KGE counterpart with less parameters, which validates that StarGraph dose work on smaller KGs as well.

As with the number of parameters, the efficiency of StarGraph is more evident on large-scale KGs, as the network itself takes up a certain amount of parameters.
Comparing the \#Params marked in gray, the number of embedding parameters of StarGraph on fb15k-237 is only about 30\% of the control item, but the total number of parameters is about 80\%; the number of of embedding parameters of StarGraph on ogbl-wikikg2 is 1\% of the control item and the total number of parameters is only 1.4\%.
\begin{table}
 \caption{The results on ogbl-wikikg2. Results of related work are taken from the OGB Leaderboard.}
 \centering
 \begin{tabular}{lccrcc}
 \toprule
Model & \#Dims & \#Embs & \#Params & Test MRR & Valid MRR \\
 \midrule
 TransE & 500 & 2.5M & 1,250,569,500 & 0.4256±0.0030 & 0.4272±0.0030 \\
 RotatE & 250 & 2.5M & 1,250,435,750 & 0.4332±0.0025 & 0.4353±0.0028 \\
 PairRE & 200 & 2.5M & 500,334,800 & 0.5208±0.0027 & 0.5423±0.0020 \\
 AutoSF & - & 2.5M & 500,227,800 & 0.5458±0.0052 & 0.5510±0.0063 \\
 ComplEx & 250 & 2.5M & 1,250,569,500 & 0.5027±0.0027 & 0.3759±0.0016 \\
 TripleRE & 200 & 2.5M & 500,763,337 & 0.5794±0.0020 & 0.6045±0.0024 \\
 TripleREv2 & 200 & 2.5M & \cellcolor{gray!30}500,763,337 & 0.6045±0.0017 & 0.6117±0.0008 \\
 ComplEx-RP & 50 & 2.5M & 250,167,400 & 0.6392±0.0045 & 0.6561±0.0070 \\
 AutoSF + NodePiece & - & 20k & 6,860,602 & 0.5703±0.0035 & 0.5806±0.0047 \\
 TripleREv2 + NodePiece & 200 & 20k & 7,289,002 & 0.6582±0.0020 & 0.6616±0.0018 \\
 TripleREv3 + NodePiece & 200 & 20k & 36,421,802 & 0.6866±0.0014 & 0.6955±0.0008 \\
 InterHT + NodePiece & 200 & 20k & 19,215,402 & 0.6779±0.0018 & 0.6893±0.0015 \\
 TranS + NodePiece & 200 & 20k & 19,215,402 & 0.6882±0.0019 & 0.6988±0.0006 \\
 TranS(large) + NodePiece & - & 20k & 38,430,804 & 0.6939±0.0011 & 0.7058±0.0018 \\
\midrule
 \multirow{3}{*}{StarGraph + TripleRE'} & 256 & 20k & \cellcolor{gray!30}7,148,802 & 0.7222 & 0.7373 \\
 & 512 & 80k & 44,819,970 & 0.7263 & 0.7407 \\
 & 512/32 & 20k/2.5M & 93,039,522 & 0.7290 & 0.7327 \\
\bottomrule
 \end{tabular}
 \label{tab:wiki}
\end{table}

\begin{table}
 \caption{The results on fb15k-237. Results of related work are taken from the corresponding papers, despite TransE being provided by Nguyen et al. \cite{dai2018novel}.}
 \centering
 \begin{tabular}{lcrrcc}
 \toprule
Model & \#Dims & \#Embs & \#Params & Test Hit@10 & Test MRR \\
 \midrule
 TransE & 100 & 14,505 & 1,474,200 & 0.465 & 0.294\\
 RotatE & 1000 & 14,505 & 29,484,000 & 0.533 & 0.338\\
 PairRE & 1500 & 14,505 & 22,468,500 & 0.544 & 0.351\\
 GC-OTE & 400 & 14,505 & - & 0.550 & 0.361\\
 CoKE & 256 & 14,505 &  10,190,000 & 0.549 & 0.364\\
 NodePiece + RotatE & 200 & 1,000 &  3,200,000 & 0.425 & 0.258\\
 \midrule
 \multirow{2}{*}{TripleRE'} & 256 & 14,505 & \cellcolor{gray!30} 3,895,296 & 0.5429 & 0.3455\\
 & 1000 & 14,505 & 15,216,000 & 0.5520 & 0.3514\\
 \midrule
 \multirow{2}{*}{StarGraph + TripleRE'} & 512 & 2,304 & \cellcolor{gray!30}3,148,290 & 0.5454 & 0.3426 \\
 & 512 & 4,503 & 4,275,714 & 0.5475 & 0.3459 \\
 \bottomrule
 \end{tabular}
 \label{tab:fb}
\end{table}



\section{Conclusion}
In this paper, we propose StarGraph, a novel method to learn knowledge representations by generating and encoding the incomplete 2-hop subgraph for the node. The experimental results verify that our method is parameters-efficient and can obtain better or comparative results with fewer parameters on different datasets. More importantly, StarGraph achieves significant improvement of results on large-scale knowledge graphs.

Based upon the core idea of incomplete subgraph, we have proposed several strategies and modifications for the implementation. Though each strategy and modification is proved to be effective alone, it is worth further exploration on how to make them work collaboratively to the best effect and whether there are better alternatives.
Additionally, beside the transductive link prediction, StarGraph is also able to perform other related tasks, such as inductive link prediction, node classification, etc. All these are treated as future work.



\bibliographystyle{unsrt} 
\bibliography{template} 

\clearpage
\appendix

\section{Experience on Anchor Set Generation}
\label{apd:anchors}
During the generation of anchor set, the hyper-parameter \emph{skip-threshold} is used to control the sparsity of anchors, the smaller value the sparser anchors; \emph{anchor-set-size} decides the maximum number of anchors, which can also be set with \emph{anchor-ratio}, the proportion of anchors occupying in the graph. Note that when \emph{skip-threshold} is small, skipping too many nodes may cause the actual size of the anchor set  to be smaller than the specified \emph{anchor-set-size}.

The experimental results with different values of \emph{skip-threshold} and \emph{anchor-set-size} are displayed in Table~\ref{tab:anc_wiki} and~\ref{tab:anc_fb}, where we can summarise some experience as follows. When the \emph{skip-threshold} is fixed, the larger , the better results. But when the \emph{skip-threshold} varies, the situation becomes a little tricky.
Comparing the results on ogbl-wikikg2 when \emph{skip-threshold}=20,000, we can draw the conclusion that when the anchor ratio is extremely small, a high \emph{skip-threshold} to keep the best few known nodes as anchors should bring better results. But when the anchor set exceeds a certain size, such as 80,000 on ogbl-wikikg2 and 2,000+ on fb15k-237, a lower \emph{skip-threshold} leads to better results with an equal or even smaller .
Anyway, a proper \emph{skip-threshold} helps to make a better trade-off between the number of parameters and the results.
\begin{table}
\caption{The results on ogbl-wikikg2 with different anchor sets.}
\centering
\begin{tabular}{cccc|cc}
\toprule
skip-threshold & anchor-set-size & anchor-ratio &  & Test MRR & Valid MRR\\
\midrule
1.0 & 20,000 & 0.008 & 20,000 & 70.88 & 71.09\\
0.5 & 20,000 & 0.008  & 20,000 & 70.67 & 70.85\\
0.2 & 20,000 & 0.008  & 20,000 & 70.65 & 70.85\\
\midrule
1.0 & 80,000 & 0.032  & 80,000 & 71.01 & 71.30\\
0.5 & 80,000 & 0.032 & 80,000 & \textbf{71.12} & 71.53\\
0.2 & 80,000 & 0.032 & 79,157 & 71.03 & \textbf{71.66}\\
\bottomrule
\end{tabular}
\label{tab:anc_wiki}
\end{table}

\begin{table}
\caption{The results on fb15k-237 with different anchor sets.}
\centering
\begin{tabular}{cccc|cc}
\toprule
skip-threshold & anchor-set-size & anchor-ratio &  & Test Hit@10 & Valid Hit@10\\
\midrule
1.0 & 1,450 & 0.1 & 1,450 &.53.55 & 54.15 \\
0.5 & 1,450 & 0.1 & 1,450 & 53.97 & 54.58 \\
0.2 & 1,450 & 0.1 & 1,450 & 53.80 & 54.43 \\
\midrule
1.0 & 14,505 & 1.0 & 14,505 & 54.33 & 54.93 \\
0.5 & 14,505 & 1.0 & 4,503 & \textbf{54.75} & 55.06 \\
0.2 & 14,505 & 1.0 & 2,304 & 54.54 & \textbf{55.12} \\
\bottomrule
\end{tabular}
\label{tab:anc_fb}
\end{table}


\section{Experiments with Different Encoders}
\label{apd:path_aggr}
To explore the effectiveness of self-attention network to encode graphs, we conduct the control experiment of different encoders and present the results in Table~\ref{tab:encoders}. MLP consists of two linear layers and a non-linear activation between them, Attention+Mean is the encoder adopted in StarGraph, and Mean omitting the attention module is just a mean-pooling operation performed over all tokens. Note that the case marked in gray is actually the same method as NodePiece, in spite of the \emph{mlp-ratio} here is 4 instead of 2 in their experiments.
\begin{table}
\caption{The Test/Valid MRR on ogbl-wikikg2 with different encoders in different situations,}
\centering
\begin{tabular}{cc|ccc}
\toprule
Sampling & path & MLP & Mean & Attention+Mean \\
\midrule
BFS &  & \cellcolor{gray!30}0.6886/0.6924 & 0.6749/0.6733 & 0.6899/0.6953 \\
balanced &  & 0.7079/0.7152 & 0.6596/0.6587 & 0.7067/0.7085 \\
balanced & \checkmark & 0.7094/0.7247 & 0.7101/0.7250 & 0.7222/0.7373 \\
\bottomrule
\end{tabular}
\label{tab:encoders}
\end{table}

Compare results with different anchors-sampling strategies, the \emph{balanced} sampling proposed by us achieves 1\%+ improvement for both MLP and StarGraph. But when it comes to the introduction of path information, MLP does not process the subgraphs as effectively as StarGraph. By the way, it is interesting to compare the results of Mean with or without the path information. The proposed infusion way of path information, which is developed form the KG distance-based model as mentioned in section~\ref{sec:path}
, are proved to be effective for the graph structure by the 5\%+ improvement.

Overall, the experimental results confirm that the self-attention network cooperates the best with our subgraph-based representation learning.

\section{Implementation and Hyper-parameters}
\label{apd:hyper_param}
The experiment settings for StarGraph in Table~\ref{tab:wiki} is given in Table~\ref{tab:wiki_settings}. For each setting, the best learning rate is chosen from \{5e-5, 1e-4, 2e-4\} according to the experimental results.
\begin{table}
 \caption{Experiment settings for StarGraph on ogbl-wikikg2.}
 \centering
 \begin{tabular}{cccccc|cc}
 \toprule
  & Sampling & Network ID & path & nbors\&center & learning rate & Test MRR & Valid MRR \\
 \midrule
 20,000 & balanced & 256-R32-4 & \checkmark & & 1e-4 & 0.7222 & 0.7373 \\
 80,000 & balanced & 512-R8-2 & \checkmark & & 5e-5 & 0.7263 & 0.7407 \\
 20,000 & balanced & 512-R8-2 & & \checkmark & 2e-4 & 0.7290 & 0.7327 \\
 \bottomrule
 \end{tabular}
 \label{tab:wiki_settings}
\end{table}

For the results in Table~\ref{tab:fb}, model TripleRE' corresponds to the conventional KGE method, the dimension of embeddings is provided as \#Dim, and the best learning rate is 2e-4 in \{1e-4, 2e-4, 5e-4, 1e-3\}. 
The experiment settings for StarGraph is given in Table~\ref{tab:fb_settings}, and the learning rate is chosen from \{1e-4, 2e-4, 5e-4, 1e-3\}.
\begin{table}
 \caption{Experiment settings for StarGraph on fb15k-237.}
 \centering
 \begin{tabular}{cccccc|cc}
 \toprule
  & Sampling & Network ID & path & nbors\&center & learning rate & Test Hit@10 & Test MRR \\
 \midrule
 2,304 & balanced & 512-R8-2 & \checkmark & & 5e-4 & 0.5454 & 0.3426 \\
 4,503 & balanced & 512-R8-2 & \checkmark & & 5e-4 & 0.5475 & 0.3459 \\
 \bottomrule
 \end{tabular}
 \label{tab:fb_settings}
\end{table}

The selection of the feature dimension and the combination of different modifications are in fact restricted, mainly due to the video memory size. 
Note that adopting multiple advantaged strategies and modifications does not always result in the sum of improvements, and the best learning rates for different networks or different sizes of  anchor sets are usually different. It will take a lot of work in the future to explore more of the potential for StarGraph.


\end{document}
