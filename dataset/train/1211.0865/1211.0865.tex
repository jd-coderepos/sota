



\documentclass{LMCS}

\def\doi{9(1:04)2013}
\lmcsheading {\doi}
{1--29}
{}
{}
{Oct.~27, 2011}
{Feb.~27, 2013}
{}
\usepackage{amssymb}
\usepackage{amsmath, amsthm}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{proof}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{pstricks,pst-node}
\usepackage{enumerate,hyperref}

\usetikzlibrary{arrows}

\usetikzlibrary{arrows}

\tikzstyle{node}        =[draw, rounded corners=8, shade,top color=green!10,bottom color=blue!10, minimum size=.5cm, ultra thin]

\tikzstyle{refinement}   =[dotted, above]
\tikzstyle{reduction} =[->]

\newcommand{\tto}[0]{\leftrightarrow}

\newcommand{\subse}{\; \subseteq \;}

\newcommand{\To}[0]{\Rightarrow}
\newcommand{\interp}[1]{[\negthinspace[#1]\negthinspace]}
\newcommand{\annotate}[2][\hat\sigma]{\ensuremath{#1 \star #2}}
\newcommand{\ot}[0]{\leftarrow}
\newenvironment{oneshot}[1]{\@begintheorem{#1}{\unskip}}{\@endtheorem}



\begin{document}

\title[A Rewriting View of Simple Typing]{A Rewriting View of Simple Typing}


\author[A.~Stump]{Aaron~Stump\rsuper a}
\address{{\lsuper{a,c,d}}Computer Science, The University of Iowa}
\email{astump@acm.org, \{garrin-kimmell,roba-elhajomar\}@uiowa.edu}

\author[H.~Zantema]{Hans~Zantema\rsuper b}
\address{{\lsuper b}Department of Computer Science, TU Eindhoven, The Netherlands; and Institute for Computing and Information Sciences, Radboud University, The Netherlands}
\email{h.zantema@tue.nl}

\author[G.~Kimmell]{Garrin~Kimmell\rsuper c}
\address{\vskip-6 pt}

\author[R.~ El Haj Omar]{Ruba~El Haj Omar}
\address{\vskip-6 pt}

\thanks{This work was
    partially supported by the U.S. National Science Foundation, contract
    CCF-0910510, as part of the Trellys project.}


\keywords{Term rewriting, Type safety, Confluence}
\ACMCCS{[{\bf Software and its Engineering}]: Software notations and tools---
  Formal language definitions---Syntax}
\subjclass{D.3.1}


\newtheorem{theorem}[thm]{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{definition}[thm]{Definition}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem*{theorema}{Theorem}


\begin{abstract}
This paper shows how a recently developed view of typing as small-step
abstract reduction, due to Kuan, MacQueen, and Findler, can be used to
recast the development of simple type theory from a rewriting
perspective.  We show how standard meta-theoretic results can be
proved in a completely new way, using the rewriting view of simple
typing.  These meta-theoretic results include standard type
preservation and progress properties for simply typed lambda calculus,
as well as generalized versions where typing is taken to include both
abstract and concrete reduction.  We show how automated analysis tools
developed in the term-rewriting community can be used to help automate
the proofs for this meta-theory.  Finally, we show how to adapt a
standard proof of normalization of simply typed lambda calculus, for
the rewriting approach to typing.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

This paper develops a significant part of the theory of simple types
based on a recently introduced rewriting approach to typing.  The idea
of viewing typing as a small-step abstract reduction relation was
proposed by Kuan, MacQueen, and Findler in 2007, and explored also by
Ellison, \c{S}erb\u{a}nu\c{t}\u{a}, and
Ro\c{s}u~\cite{rosu+10,ellison+08,kuan+07}.  These works sought to use
rewrite systems to specify typing in a finer-grained way than usual
type systems.  Our motivation is more foundational: we seek to prove
standard meta-theoretic properties of type systems directly, based on
the rewriting formulation.  The goal is to develop new methods which
could provide a different perspective on familiar type systems, and
perhaps yield new results for more advanced type systems.

Our focus in this paper is simple type systems, where the central
typing construct is the function type .  We will view such
types as abstractions of functions, and incrementally rewrite
(typable) functions to such function types, using an abstract
small-step reduction relation.  It will be straightforward to prove
the standard property of type safety, based on type preservation and
progress, using this rewriting formulation.  This viewpoint also
allows us to combine the usual concrete reduction relation and our new
abstract reduction relation together, simply by taking their
set-theoretic union.  We will prove that this combined reduction
relation is confluent for typable terms, defined as terms which
reduce, using abstract steps, to a type.  To prove both type preservation 
and confluence we use observations developed in the context of abstract
reduction systems.  We then develop our final main result, which is a
proof of normalization for the simply typed lambda calculus, based on
the rewriting approach.  This proof has several novel features, which
shed new light on the reducibility semantics of types used in standard
proofs of normalization.

This paper expands in several important ways on a previous paper of
Stump, Kimmell, and El Haj Omar, which was presented at RTA 2011~\cite{stump+11}:
\begin{iteMize}{}
\item 
We use the rewriting method to prove type preservation for full
-reduction; the RTA '11 paper showed it only for call-by-value
computation.
\item We prove preservation for a new notion we call generalized
  typing, where concrete and abstract reduction steps can be
  intermixed.  This generalizes the so-called \emph{direct computation
  rules} of the well-known NuPRL system~\cite{allen+06}.
\item We correct an error in the RTA '11 paper, where we claimed that
  type preservation is a corollary of confluence for typable terms.
  In fact, confluence is a straightforward corollary of type preservation.
\item We have shown how a standard proof of normalization for simply
  typable terms is adapted to the rewriting approach to typing.  This
  adaptation reveals an interesting perspective on types as
  abstractions of terms.
\item Due to the amount of new material, we have dropped the treatment
  of several variants of STLC, which are studied in the RTA paper.
\end{iteMize}
As Zantema had a substantial contribution to these extensions, he was
added as an author.  

The remainder of the article is organized as
follows. Section~\ref{sec:rewriting-prelim} provides a brief
introduction to abstract reduction systems as used later in the
paper. Section~\ref{sec:std-presentation} gives a standard
presentation of the simply typed lambda calculus along with the
fundamental meta-theoretic properties. Section~\ref{sec:restlc}
recasts the simply typed lambda calculus static and operational
semantics within the framework of abstract reduction systems.
Section~\ref{sec:ars} gives some abstract reduction theory to be used
in Section~\ref{sec:presstlc} where type preservation and confluence
is proved.  Section~\ref{sec:progtpsafe} then proves progress and type
safety.  Section~\ref{sec:unistc} proves type preservation and
confluence for a system with uniform syntax for types and term.  For
this result, we use automated tools developed in the term-rewriting
community, to verify some of the properties necessary for applying
theorems proved in Section~\ref{sec:ars}.
Section~\ref{sec:genpresstlc} extends these to a generalized notion of
typing, based on the union of the concrete and abstract reduction
relations.  Section~\ref{sec:normstlc} applies a rewriting approach to
prove the normalization of well-typed simply typed lambda calculus
terms. We conclude and identify future directions in
Section~\ref{sec:conclusion}.

\section{Rewriting Preliminaries}
\label{sec:rewriting-prelim}

In this section we collect some basic properties in the setting of
abstract reduction systems.  That is, we consider relations 
being a subset of  for some arbitrary set .

We write  for relation composition, and inductively define
 (the identity) and  for .
As usual, for a relation  we write  for its reverse,
 for its reflexive closure (zero or one times),
 for its transitive closure
(one or more times), and  for its
transitive reflexive closure (zero or more times).  We will also
use standard notation  for the image of set  under relation :

\noindent We can use this notation to denote the set of predecessors
of a set  with respect to  as .  We will also
write  for .

\vspace{3mm}
\noindent A relation  is said to
\begin{iteMize}{}
\item be {\em confluent}
(Church Rosser, ) if ,
\item be {\em locally confluent}
(Weak Church Rosser, ) if ,
\item have the {\em diamond property}
() if ,
\item be {\em deterministic} (det) if .
\item be {\em terminating} if there is no infinite descending chain .
\item be {\em convergent} if it is confluent and terminating.
\end{iteMize}

\noindent We will sometimes also call an element  confluent
iff for all  with  and ,
there exists  with  and .  It is
well-known and easy to see that det.

\ 

\noindent Finally, if  and  are binary relations, below
we will often write  for .

\section{A Standard Presentation of Simple Typing}
\label{sec:std-presentation}

In this section, we summarize a standard presentation of the simply
typed lambda calculus (STLC), including syntax and semantics, and statements
of the basic meta-theoretic properties of type preservation and
progress.  Sections~\ref{sec:restlc} and following will recapitulate
this development in detail, from the rewriting perspective.  Including
some type and term constants, together with reduction rules for them,
is very standard in the study of programming languages and typed
lambda calculus.  One example is Mitchell's treatment of STLC with
additional rules~\cite[Section 4.4.3]{M96}).  For progress, it is
indeed instructive to include reduction rules for some selected
constants.  Otherwise, there are no stuck terms that should be ruled
out by the type system, since in pure STLC, every closed normal form
is a value, namely a -abstraction.  We treat additional rules
representatively (as opposed to parametrically), using constants 
and  below.

\subsection{Syntax and Semantics}
\label{sec:stlcstand}

The syntax for terms, types, and typing contexts is the following,
where , , and  are specific constants, and  ranges over a
countably infinite set of variables:

\noindent We will write \textit{Types} for the set of all types.  We
assume standard additional conventions and notations, such as
 for the capture-avoiding substitution of  for  in
, and  for grafting a term into an evaluation context.
Figure~\ref{fig:stlc} defines a standard type system for STLC.  The
judgments derived by the rules in the figure are of the form
, which can be viewed as deterministically computing
a type  as output, given a term  and a typing context 
as inputs.  In the topmost leftmost rule of the Figure, we use the
notation  to mean that there is a binding  in
.  We assume there is at most one such binding in ,
renaming bound variables as necessary to ensure this.  A standard
small-step reduction semantics, for unrestricted -reduction, is
defined using the rules of Figure~\ref{fig:stlcopsem}.  Following
standard usage, terms of the form  or  are
called redexes.  An example of a concrete reduction is (with redexes
underlined):



\begin{figure}

\caption{Type-computation rules for STLC with selected constants}
\label{fig:stlc}
\end{figure}

\begin{figure}

\caption{Small-step reduction semantics for STLC}
\label{fig:stlcopsem}
\end{figure}

\subsection{Basic Meta-theory}
\label{sec:basicmeta}

The main theorem relating the reduction relation  and typing is
\textbf{type preservation}, which states the following, either for
unrestricted -reduction  or for some restriction of 
(as we will consider below):


\noindent The standard proof method is to proceed by induction on the
structure of the typing derivation, with case analysis on the
reduction derivation (cf. Chapters 8 and 9 of~\cite{pierce02}).  A separate
induction is required to prove a substitution lemma, needed critically
for type preservation for -reduction steps:


\noindent For call-by-value programming languages, one also typically
proves \textbf{progress}, formulated in terms of values:

\noindent Here, the notation  means ; i.e.,  is a normal form.  Normal forms
which are not values are called \emph{stuck} terms.  An example is
.  Combining type preservation and progress allows us to
prove \textbf{type safety}~\cite{wright+94}.  This property states
that the normal forms of closed well-typed terms are values, not stuck
terms, and in our setting can be stated:

\noindent This is proved by induction on the length of the reduction
sequence from  to .  As already noted, without constants (
and  here), this result is not so interesting for STLC, since it
follows already by simpler reasoning: reduction cannot introduce new
free variables, so  must be closed; and it is then easy to prove
that closed normal forms are -abstractions, and hence values
by definition.

\section{Simple Typing as Abstract Reduction}
\label{sec:restlc}

In this section, we see how to view a type-computation (also called
type-synthesis) system for STLC as an abstract operational semantics.
We view function types  as abstract functions from 
to , and allow these to be applied to arguments.  When  is applied to the abstract term , an abstract
-reduction step is possible, simulating concrete
-reduction for any function of type  applied to an
argument of type .  Thus, we will see abstract reduction as truly
an abstraction of the usual reduction, which we thus view, in
contrast, as concrete.

\begin{figure}

\caption{Syntax for STLC using mixed terms}
\label{fig:synrestlc}
\end{figure}

\begin{figure}

\caption{Concrete call-by-value reduction (), concrete full -reduction (), and abstract reduction () for STLC}
\label{fig:restlc}
\end{figure}

To view typing as an abstract form of reduction, we use mixed terms,
defined in Figure~\ref{fig:synrestlc}.  Types like  will
serve as abstractions of -abstractions.  For our development
below, we are going to consider both unrestricted -reduction,
and also call-by-value -reduction, a common restriction
implemented in practical functional programming languages like
\textsc{OCaml}.  Figure~\ref{fig:restlc} gives rules for concrete
call-by-value reduction (), concrete full -reduction
(), and abstract reduction ().  As above, we will refer
to any term of the form displayed in context on the left hand side of
the conclusion of a rule as a redex. We denote the union of these
reduction relations as .  The definition of call-by-value
evaluation contexts  enforces left-to-right evaluation order in a
standard way, while unrestricted evaluation contexts  make
abstract reduction and full -reduction non-deterministic:
reduction is allowed anywhere inside a term.  This is different from
the approach followed by Kuan et al., where abstract and concrete
reduction are both deterministic.  Here is an example of reduction using
the abstract operational semantics:

\noindent The final result is a type , which does not reduce (as
noted below).  Indeed, using the standard typing rules of
Section~\ref{sec:stlcstand}, we can prove that the starting term of
this reduction has that type , in the empty typing context.
Abstract reduction to a type plays the role of typing above.

\begin{lem}
\label{lem:tpnorm}
For all types , we have .
\end{lem}
\begin{proof} This follows by induction on  and inspection of the rules for .
\end{proof}

If we look back at our standard typing rules (Figure~\ref{fig:stlc}),
we can now see them as essentially big-step abstract operational
rules.  Recall that big-step call-by-value operational semantics for
STLC includes this rule (as well as several others which we elide):

\noindent In our setting, big-step call-by-value semantics would be
seen as a concrete big-step reduction, which we might denote
.  The abstract version of this rule, where we abstract
-abstractions by arrow-types, is

\noindent If we drop the typing context from the standard typing rule
for applications (in Figure~\ref{fig:stlc}), we obtain essentially
the same rule.

The standard approach to proving type preservation relates a
small-step concrete operational semantics with a big-step abstract
operational semantics (i.e., the standard typing relation).  We find
it both more elegant, and arguably more informative to relate abstract
and concrete small-step relations, as we will do in Section~\ref{sec:presstlc} below.

\subsection{Rewriting Properties of Abstract Reduction}

In this subsection, we study the properties of abstract reduction from
the perspective of the theory of abstract reduction systems (ARSs).
From this point of view, abstract reduction is very well behaved: it
is a convergent ARS, as the following two theorems show.

\begin{thm}[Termination of Abstract Reduction]
\label{thm:termabstr}
The relation  is terminating.
\end{thm}
\begin{proof} We recursively define a natural-number measure  which can be confirmed to reduce from  to  
whenever :

\end{proof}

\begin{thm}
\label{lem:confl}
The relation  is confluent.
\end{thm}
\begin{proof}
In fact, we will prove  has the diamond property (and hence is
confluent).  Suppose  and .  No critical
overlap is possible between these steps, because none of the redexes
in the -rules of Figure~\ref{fig:restlc} (such as  in
the  rule) can critically overlap another such redex.  If
the positions of the redexes in the terms are parallel, then (as
usual) we can join  and  by applying to each the reduction
required to obtain the other.  Finally, we must consider the case of
non-critical overlap (where the position of one redex in  is a
prefix of the other position).  We can also join  and  in
this case by applying the reduction to  which was used in , because abstract reduction cannot duplicate or delete
an -redex.  The only duplication of any subterm in the abstract
reduction rules of Figure~\ref{fig:restlc} is of the type  in
.  The only deletion possible is of the type 
in .  Since types cannot contain redexes, there is
no duplication or deletion of redexes.  This means that if the
position of the first redex is a prefix of the second (say), then
there is exactly one descendant (see Section 4.2 of~\cite{terese}) of
the second redex in , and this can be reduced in one step to join
 with the reduct of  obtained by reducing the first redex.
So every \textit{aa}-peak can be completed with one joining step on
each side of the diagram.  This gives the diamond property (and thus
confluence for ).
\end{proof}

\subsection{Relation with Standard Typing}

In this subsection, we prove the following theorem, which relates our
notion of typing with the standard one.  The proof begins after the
statement of some simple auxiliary lemmas, whose proofs are routine
and omitted.  The proof of the right-to-left direction of the
implication will take advantage of the fact that abstract reduction is
convergent, as proved in the previous subsection.

\begin{thm}
\label{thm:relatetyp}
For standard terms , a typing judgment  holds iff .
\end{thm}

\begin{lem}
\label{lem:lemma1}
If  , then
.
\end{lem}
\begin{lem}
\label{lem:lemma2}
If  , then
.
\end{lem}
\begin{lem}
\label{lem:lemma3}
If  , then .
\end{lem}

\proof[Proof of Theorem~\ref{thm:relatetyp}, left-to-right]
Suppose .
We will now prove  by
induction on the structure of the typing derivation of .
To simplify the writing of the proof, we will use the
following notation:


\

\noindent \textbf{Base Case:}

\noindent There must be some  such that  and .
So  as required.

\

\noindent\textbf{Base Case:}

\noindent We indeed have , as required.  The case for
 is similar.

\

\noindent \textbf{Case:}

\noindent By the induction hypotheses for the derivations given for
the two premises of this rule, we have:

\noindent Our goal now is to construct the reduction sequence:

\noindent To construct this sequence, it is sufficient to apply transitivity of
 and Lemmas~\ref{lem:lemma1} and~\ref{lem:lemma2}.

\ 

\noindent \textbf{Case:}

\noindent By the induction hypothesis on the premise of this rule,
we have:

\noindent Now we need to show that

\noindent By applying one  step and Lemma~\ref{lem:lemma3}
we get:

\noindent This requires the fact that , which holds because
           since we may rename 
          to avoid this, and because  contains no term variables
          and hence is unaffected by applying .\qed


\proof[Proof of Theorem~\ref{thm:relatetyp}, right-to-left]
Since abstract reduction is convergent (Theorems~\ref{thm:termabstr}
and~\ref{lem:confl}), we may assume that redexes in the reduction
sequence to  are always reduced in leftmost order.  Note that
convergence is sufficient to justify this assumption, as  is a
normal form, and hence any strategy is guaranteed to reduce the
starting term to  in a finite number of steps.  This assumption
will simplify some reasoning below.  We assume
 and prove
 by induction on the number  of
leftmost  steps in the reduction to .

\

\noindent \textbf{Base Case:} there are no  steps.
This means that our term  cannot be reduced

\noindent In this case,  must be a variable (or else substitution could not
result in a type ). So,  for some variable , where
. Then we get:


\noindent \textbf{Step Case:}
there is at least one  step.  We proceed by case splitting on the form of .

\

\noindent\textbf{Case:}

\noindent This case cannot occur, since either
, in which case we cannot have
 for any type ; or else
, and then .  We
cannot have a  step in that case, because types are normal
forms for abstract reduction (Lemma~\ref{lem:tpnorm}).

\noindent\textbf{Case:}

\noindent The only possible step is , and we indeed have
.  The case for  is similar.

\noindent\textbf{Case:}

\noindent In this case, the reduction sequence must be of the following form,
for some mixed term  and type , and some natural numbers  and :

\noindent where

\noindent We are justified in assuming this, because there must be
some first position in the reduction sequence from  to 
where a descendant of  is reduced.  That descendant here is
.  In the reduction sequence prior to that point, we
are assuming (as noted at the start of the proof) that steps occur in
leftmost order, so the  steps come first, and then the 
ones.  Now we can apply the induction hypothesis to (1) and (2), which
each have shorter length than the original reduction sequence.  This
gives us the premises of the following inference, which suffices to
complete this case:


\noindent \textbf{Case:}

\noindent In this case, we may assume the reduction sequence is of the
following form, for some :

\noindent where

\noindent This is because  is itself an abstract
redex, and since we are assuming our reduction is in leftmost,
it must be reduced immediately.  Now we can apply the induction
hypothesis on  and get the
premise of the following inference, which suffices to complete this
case:





\section{Generic Theorems for Preservation and Combined Confluence}
\label{sec:ars}

In this section, we collect some abstract properties for  and
, from which type preservation and confluence of  can
be concluded. In subsequent sections we will instantiate these
theorems with abstract and concrete reduction relations.

For the first theorem, recall that in our setting  computes the
type of a term, or else could reach a stuck term like  which does not correspond to a type.  We want to speak about
reductions that lead to types, so we need to phrase the following
theorem in terms of some set , which we will instantiate later with
a set of types.  In condition (3) of the theorem, we interpose
 to restrict peaks to those objects
which -reduce to an object in .

\begin{thm}
\label{thmtp}
Assume
\begin{enumerate}[\em(1)]
\item  (that is,  is a set of objects in normal form with respect to ).
\item  is confluent.
\item ; that is, for
  every  such that there exists  with
  , and every  and  with  and , there exists a  such that
   and either  or
  .
\item every normal form with respect to 
is also a normal form with respect to .
\end{enumerate}
Then if  and , we have .
\end{thm}

\begin{proof}
Let  and , we have to prove that .  We do this by induction on the number  of steps in . In case  we have .  By (1),  is a normal form with
respect to , which is a normal form with respect to  due
to (4). So  is not possible, and the claim holds
trivially.

For the induction step assume  for which . Applying (3) now yields  such that  and
either  or . In case 
we apply the induction hypothesis on  and conclude
.  In case  we apply
confluence of  (2) by which  and  have a common
-reduct. As  is a normal form with respect to  by
(1), we conclude , concluding the proof.
\end{proof}

\begin{lem}
\label{lem:extendac}
Suppose  and  are binary relations such that
\begin{enumerate}[\em(1)]
\item  is confluent, and
\item .
\end{enumerate}
Then we also have

\end{lem}
\begin{proof}
Assume  and ; we have to find  such that  and . We do this by induction on
. For  we choose . For  write . By (2) an element  exists such that 
and either  or . If  we apply
(1) yielding  satisfying  and  and we
are done. If  then we apply the induction hypothesis
yielding  and .
\end{proof}

\begin{thm}
\label{thmcr}
Let  and  be binary relations (recall from
Section~\ref{sec:rewriting-prelim} that we write  for ). Assume
\begin{enumerate}[\em(1)]
\item  is terminating,
\item  is confluent,
\item , and
\item every normal form with respect to 
is also a normal form with respect to .
\end{enumerate}
Then   is confluent.
\end{thm}

\begin{proof} 
By Lemma~\ref{lem:extendac}, we have:

\noindent Now let  and ; for proving
the theorem we have to prove that  exists satisfying  and . Choose  to be a -normal form of
, which exists due to (1).  Assume ; we will prove
that  by induction on . For  this follows from
. For  let . From
the induction hypothesis we conclude .  Combining (2)
and  yields

So since  we conclude that  exists
satisfying  or , and . Since  is not only a
-normal form, but also a -normal form according to (4), 
we conclude . Hence ,
concluding the  proof of . Applying the same argument on
 we conclude , concluding the proof of the
theorem. 
\end{proof}

\vspace{3mm}

One may wonder whether the requirement of
termination is essential for Theorem \ref{thmcr}. It is: on the set  the
relations  and  satisfy
all requirements of Theorem \ref{thmcr}, while  is not confluent. 

One may wonder whether in Theorem \ref{thmcr} the condition (4) on normal
forms is essential. It is, even if not only  is terminating and
confluent but also , as is shown by the following example of
relations on 10 elements, in which  steps are denoted by 
dashed arrows and  steps are denoted by solid arrows.

\vspace{4mm}

\begin{center}
\begin{pspicture}(0,0)(10,4)\psset{arrows=->,arrowsize=3pt 3}
\rput(1.3,3.2){\circlenode{2}{}}
\rput(0,2){\circlenode{1}{}}
\rput(5,4){\circlenode{7}{}}
\rput(6,2){\circlenode{6}{}}
\rput(7.7,2.5){\circlenode{9}{}}
\rput(2.3,1.5){\circlenode{10}{}}
\rput(4,2){\circlenode{5}{}}
\rput(5,0){\circlenode{8}{}}
\rput(10,2){\circlenode{4}{}}
\rput(8.7,0.8){\circlenode{3}{}}
\ncline{1}{2}
\nccurve[ncurv=1,angleB=180,angleA=90]{1}{7}
\ncline{10}{2}
\ncline{5}{10}
\ncline{7}{5}
\ncline{4}{3}
\nccurve[ncurv=1,angleB=0,angleA=270]{4}{8}
\ncline{8}{6}
\ncline{9}{3}
\ncline{6}{9}

\psset{linestyle=dashed}
\ncline{5}{8}
\ncline{6}{7}
\nccurve[ncurv=1,angleB=90,angleA=0]{7}{4}
\nccurve[ncurv=1,angleB=270,angleA=180]{8}{1}
\ncline{9}{4}
\ncline{10}{1}
\end{pspicture}
\end{center}

\vspace{4mm}

In this example there are two convertible normal forms, so the union is
not confluent, and both  and  are both confluent and terminating;
 is even deterministic. Also condition  of Theorem
\ref{thmcr} is easily checked, even stronger:
.
This example was found using a SAT solver. A direct encoding of the example to be
looked for run out of resources. However, by adding a symmetry requirement, 
was observed on the first example, the SAT solver yielded a satisfying assignment
that could be interpreted as a valid example. The example given
above was obtained from this after removing some redundant arrows.
Independently, Bertram Felgenhauer found an example that could be
simplified to exactly the same example as given here. This remarkable
example was the starting point of developing the tool CARPA by which such
examples can be found fully automatically.

\section{Type Preservation and Combined Confluence for STLC}
\label{sec:presstlc}

We now prove type preservation for full -reduction (the 
relation of Section~\ref{sec:restlc}), based on the rewriting
formulation.  This is in contrast to the results of Kuan et al., who
obtain type preservation for the rewriting approach as a corollary of
type preservation based on a standard big-step notion of typing (and
the relation of that notion of typing with the small-step notion).  

\begin{defi}[Typability]
A mixed term  is called \textbf{typable} 
if  for some type .  
\end{defi}

\noindent If we translate our standard statement of type preservation (at the
beginning of Section~\ref{sec:basicmeta}) so that it uses abstract
reduction instead of the usual typing relation, we have the following.

\begin{thm}[Type Preservation]
\label{thm:presstlc}
Let  be mixed terms and  be a type.
If  and , then .
\end{thm}

\noindent The proof of this theorem is given by applying Theorem
\ref{thmtp}: we need to check its conditions (1), (2), (3) and (4). We
instantiate the set  in condition (1) with the set of types ,
which are normal forms by Lemma~\ref{lem:tpnorm}.  Condition (2)
follows from Lemma \ref{lem:confl}. Condition (4) is immediate from
the definitions of  and : if  applies on a
term , then  either contains  via rule
 by which  applies via , or
 contains  via rule  by which
   applies via . So it remains to check condition
  (3), which follows from the following lemma.

\begin{lem}
\label{lem:beta}
Let  be a typable mixed term and let  be mixed terms
such that  and . Then a mixed term  exists such that  and 
either  or . Furthermore, if the step
from  to  is a call-by-value step, so is the step from 
to .
\end{lem}

\begin{proof}
We distinguish the ways the redexes in  are related.

If the redexes of  and  are parallel,
then  can be chosen such that  and 
(preserving whether or not the -step is call-by-value).

If the redex of  is above the redex of ,
then the  step is either of the type  or
, in which the  acts on the mixed term  as it
occurs in the rule  or . As this  is not
duplicated, we get  such that  and
 (and the step  is not call-by-value).

If the redex of  is below the redex of , then some further case analysis is required.

If there is no overlap, then  can be chosen such that 
 (preserving being call-by-value) and . 

If there is overlap and  is an application of
 or , then  and , and
 can be chosen to be , satisfying
 and .

The remaining case is illustrated by the following picture:

\begin{center}
\begin{tikzpicture}
\draw (0,1.5) node(t) {} ;
\draw (-2.5,0) node(sa) {};
\draw (2.5,0) node(sb) {};

\draw[-latex'] (t) -- node[left=1pt,very near end]{} (sa);
\draw[-latex'] (t) -- node[left=1pt,very near end]{}
                      (sb);

\draw (-2.5,-1.5) node (saa) {};
\draw (0,-3) node (b) {};

\draw[-latex'] (sb) -- node[right=2pt] {\textit{since}\ }
                       node[left=1pt,very near end]{}
                       node[right=1pt,very near end]{*}
                       (b);
\draw[-latex'] (sa) -- node[left=2pt] {\textit{since}\ }
                       node[left=1pt,very near end]{}
                       node[right=1pt,very near end]{*}
                       (saa);
\draw[-latex'] (saa) -- node[left=1pt,very near end]{} (b);

\end{tikzpicture}
\end{center}

The picture already shows that by choosing  we
obtain  and   if we can prove 
. For doing so we use the assumption that  is
typable: there exists a type  such that 
. Since  is a type 
it does not contain a  symbol, so somewhere in this
reduction the  in  should be removed. By
inspecting the rules we see that this can only be done by the rule
 by which  is replaced by .
Next the (invisible) application symbol in 
should be removed. This can only be done by the rule .
This rule is only applicable if first  is rewritten by 
steps to , indeed proving .
\end{proof}

\begin{thm}
\label{prop:confstlch}
The relation  is confluent.
\end{thm}
\begin{proof}
We will apply Theorem \ref{thmcr}.  For this, we need to check
properties (1) to (4) for the particular relations
 and
. Property
(2) follows from Theorem \ref{lem:confl} and the fact that
 is the identity
relation. All peaks must be of the form , due to the composition with
. By Theorem
\ref{lem:confl}, if , then there exists
 such that . Thus, any
 peak  can be
completed with . Likewise, By Theorem \ref{thm:termabstr} 
is terminating, so  is also terminating, proving property (1). Property (3) follows from
Lemma \ref{lem:beta}. So it remains to prove Property (4). This is
immediate from the definitions of  and : if 
applies on a term , then  either contains  via rule
 by which  applies via , or
 contains  via rule  by which
 applies via . 
\end{proof}

\begin{cor}[Confluence of Combined Reduction]
\label{prop:confstlc}
Every typable mixed term is confluent with respect to the reduction
relation .
\end{cor}
\begin{proof} Confluence of the set of typable mixed terms is equivalent
to confluence of  the relation ,
which is easily seen to be equal to

\noindent By Theorem~\ref{prop:confstlch}, the latter relation is confluent.
\end{proof}

A form of typability is essential, since the relation  is not
confluent in general, as Kuan et al. note also in their setting.  For
instance, the non-typable term  has
two distinct normal forms


\section{Progress and Type Safety for STLC}
\label{sec:progtpsafe}

In this section, we complete the basic meta-theory for STLC by proving
progress and type safety theorems for call-by-value reduction (the
 relation of Section~\ref{sec:restlc}).  Lemmas~\ref{lem:cac}
and~\ref{lem:stuck} are stated in a somewhat more general way, so that
we can also use them to show type safety for the generalized form of
typability we will consider in Section~\ref{sec:genpresstlc}.

\subsection{Quasi-Stuck Terms}

We begin by inductively defining the set of \textbf{quasi-stuck} terms
, in Figure~\ref{fig:qs}.  Also, let us call a quasi-stuck term
which is not a value \textbf{stuck}.  The purpose of these definitions
is to generalize a characterization of -normal standard terms to
mixed terms (Lemmas~\ref{lem:qsnf} and~\ref{lem:qsnfb}, proved next),
in such a way that we can show that the set of quasi-stuck terms is
closed under abstract reduction (Lemma~\ref{lem:redqs}, proved below).
This will allow us to prove that typable quasi-stuck terms must be
values (Lemma~\ref{lem:stuck}), from which we easily obtain the
desired main theorems of progress and type safety.

\begin{figure}[b]
\begin{iteMize}{}
\item Mixed values  are in .
\item Terms of the form  or  are in  if .
\item Terms of the form  or  are in  if  and  is neither  nor .
\item Terms of the form  or  are in  if  and  is not a mixed value.
\item Terms of the form  are in  if  and  is not a mixed value.
\end{iteMize}
\caption{Inductive definition of the set  of quasi-stuck terms}
\label{fig:qs}
\end{figure}

\begin{lem}
\label{lem:qsnf}
If  is quasi-stuck, then .
\end{lem}
\begin{proof} The proof is by an easy structural induction on , 
using the definition of quasi-stuck. 
\end{proof}

\begin{lem}
\label{lem:qsnfb}
If standard term  is closed and , then  is
quasi-stuck.
\end{lem}
\begin{proof}
The proof is by structural induction on .  If  is
a (standard) value it is quasi-stuck, and it cannot be a variable
since  is closed.  So suppose it is an application .
Since  is closed,  cannot be a variable.  We consider now
the remaining possibilities.  It could be that  is  and 
is some other -normal form.  Then by the induction hypothesis,
 is quasi-stuck, and  is, too, using the second clause above
in the definition of quasi-stuck terms.  Next, we could have the
situation where  is , and  is any -normal form except
.  Then by the induction hypothesis,  is quasi-stuck, and 
is, too, using the third clause in the definition of quasi-stuck
terms.  Next, we could have that  is a -abstraction, and
 is any -normal form except a standard value.  Then by the
induction hypothesis,  is quasi-stuck, and it cannot be a mixed
value other than a standard value, because  is a standard term.
So  is quasi-stuck, too, using the fourth clause .  Finally, if
 is some application, then by the induction hypothesis,  and
 are both quasi-stuck.  Since  is not a value, the fifth
clause above gives us that  is quasi-stuck. 
\end{proof}

\begin{lem}[Reduction of Quasi-Stuck Terms]
\label{lem:redqs}
If  is quasi-stuck, and , then  is also quasi-stuck.
Furthermore, if  is a mixed value, then so is ; and if  is
not a mixed value, then neither is .
\end{lem}
\begin{proof}
The proof is by structural induction on
.  Suppose  is a mixed value.  Then it is easy to see by
inspection of the reduction rules that  must be, too.  So suppose
 is of the form  or  with .  Then either the
assumed reduction is of the form , or else of the form
 or .  In the former case, the
resulting term is a quasi-stuck non-value.  In the latter, we may
apply the induction hypothesis to conclude that  is quasi-stuck,
and hence  (or ) is a quasi-stuck non-value.

If  is of the form  or , where  and 
is not  or , then either the assumed reduction is of the form
 or else  or .  In the former case, the resulting term is
a quasi-stuck non-value, by the third clause of the definition of
quasi-stuck terms above.  In the latter, if  is not a value, we
again use our induction hypothesis to conclude that  is a
quasi-stuck non-value, and hence not  or .  So  is a
quasi-stuck non-value, too.  If  is a value, then so is , and
reduction cannot turn a value other than  into  or .  So
again,  has the required form to be a quasi-stuck non-value.

Suppose  is of the form  or ,
with  and  not a mixed value.  Then either the assumed
reduction is of the form ; or else of the form  or ; or
else of the form 
or .  In the first two cases, the
resulting term still has the required form to be a quasi-stuck
non-value.  In the third case, we know  is not a value by the
definition of quasi-stuck terms, so we may use our induction
hypothesis to conclude that  is a quasi-stuck non-value, which
is sufficient to conclude that the resulting term is again stuck.

Finally, suppose  is of the form , where  is not a
mixed value.  Then the assumed reduction must be of the form either
 or else , for some
 with , or else some  with .  This is because, by inspection of the reduction rules, 
itself cannot be a redex if  is not a mixed value.  In the former
case, we may apply the induction hypothesis to conclude that 
is a quasi-stuck non-value, and hence so is .  In the latter,
we may apply the induction hypothesis to conclude that  is quasi-stuck,
and hence so is . 
\end{proof}

\begin{lem}
\label{lem:cac}
If  is quasi-stuck (including the case where  is a
closed mixed value), and , then .
\end{lem}
\begin{proof}  The proof is by induction on the length of
the reduction sequence from  to .  If this length is , the
result obviously holds.  So suppose we have .  Since  is quasi-stuck, we have  by
Lemma~\ref{lem:qsnf}.  So it must be the case that .  Since
 is quasi-stuck by Lemma~\ref{lem:redqs}, we may apply our
induction hypothesis to conclude , and hence .
\end{proof}

\begin{lem}
\label{lem:stuck}
Suppose  is a closed quasi-stuck term.  Suppose further that .
Then  is a mixed value.
\end{lem}
\begin{proof} The proof is similar to the previous one,
and proceeds by induction on the length of the reduction sequence from
 to .  If this length is , the result holds, since types are
mixed values.  So suppose we have .  Since
 is quasi-stuck, we have  by Lemma~\ref{lem:qsnf}.  So
it must be the case that .  We now consider cases on the
form of .  If  is a mixed value the result holds.  So suppose it
is a non-value.  Then by Lemma~\ref{lem:redqs},  must also be a
quasi-stuck non-value, and we may apply the induction hypothesis to
derive a contradiction. \end{proof}

\subsection{Concluding Progress and Type Safety}

Armed with the concept of quasi-stuck terms and its associated lemmas,
we can now obtain the main results of this section.

\begin{thm}[Progress]
\label{thm:progstlc}
If standard term  is closed, , and , then
 is a (standard) value.
\end{thm}

\begin{proof} By Lemma~\ref{lem:qsnfb} and the assumption , we know  is quasi-stuck.
Now since our assumption that  implies ,
we can apply Lemma~\ref{lem:stuck} to conclude that  is a mixed
value (and hence a standard value, since  is a standard term).  
\end{proof}

\begin{thm}[Type Safety]
\label{thm:safety}
If standard term  is closed, , and , then  is a standard value.
\end{thm}
\begin{proof} The proof is by induction on the length of the reduction
sequence from  to .  In the base case, we apply
Theorems~\ref{thm:progstlc}, since we have  in that
case.  For the step case, suppose we have .  In this case, we can apply Theorem~\ref{thm:presstlc} to
conclude .  It is easily proved by induction on the
structure of call-by-value evaluation contexts  that if we have
, then  is a standard term .  We may now apply the
induction hypothesis, since we have  and . \end{proof}

\section{Applying Automated Analysis Tools to Type Preservation}
\label{sec:unistc}

In this section, we show how automated tools for analyzing
term-rewriting systems can be applied to automate part of the proof of
type preservation.  We will consider a language, which we call
Uniform-STC, that does not distinguish terms and types syntactically.
Advanced type systems like Pure Type Systems must often rely solely on
the typing rules to distinguish terms and types (and kinds,
superkinds, etc.)~\cite{B92}.  In Uniform-STC, we explore issues that
arise in applying the rewriting approach to more advanced type
systems.  We must now implement kinding (i.e., type checking of types)
as part of the abstract reduction relation.  We adopt a combinatory
formulation so that the abstract reduction relation can be described
by a first-order term-rewriting system.

\begin{figure}[t]
  \centering

  \caption{Uniform-STLC language syntax and evaluation contexts}
  \label{fig:unified-syntax}
\end{figure}


\begin{figure}[t]

  \caption{Concrete and abstract reduction rules}
  \label{fig:uni-rules}
\end{figure}

Figure~\ref{fig:unified-syntax} shows the syntax for the Uniform-STC
language.  There is a single syntactic category  for mixed terms
and types, which include a base type  and simple function types.
 and  are the
usual combinators, indexed by terms which determine their simple
types.  The \textit{kind} construct for terms is used to implement
kinding.  The rules for concrete and abstract reduction are given in
Figure~\ref{fig:uni-rules}.  The concrete rules are just the standard
ones for call-by-value reduction of combinator terms.  For abstraction
reduction, we are using first-order term-rewriting rules (unlike for
previous systems).  



For STLC (Section~\ref{sec:presstlc}), abstract -redexes have the
form .  For Uniform-STC, since there is no syntactic
distinction between terms and types, abstract -redexes take the
form , and we must use kinding to ensure that 
is a type.  This is why the  rule introduces a
\textit{kind}-term.  We also enforce kinding when abstracting simply
typed combinators  and  to their types.  The rules for \textit{kind}-terms
( and ) make sure that the first
term is a type, and then reduce to the second term.

Here, we define typability by value  to mean abstract reduction to
 where  is \emph{kindable}, which we define as
.  This definition avoids the need to
define types syntactically.

Following the methodology embodied in Theorem~\ref{thmtp}, we must
first prove the abstract reduction is confluent.  In fact, it is
convergent, and we can apply analysis tools to determine this, as
shown in the next two theorems.

\begin{thm}
  \label{uni-term}
  The term rewriting system  is terminating.
\end{thm}

\begin{proof}
The automated termination checker \textsc{Aprove} reports that the
rewrite system for  is terminating, using a recursive path
ordering~\cite{aprove}.
\end{proof}

\begin{thm}
\label{uni-confluent}
  The term rewriting system   is confluent.
\end{thm}
\begin{proof}
Abstract reduction for Uniform-STC does not have the diamond property
due to the non-left-linear rule , where there could indeed
be redexes in the expressions matching the repeated variable .
By Theorem~\ref{uni-term}, however, we can apply Newman's Lemma to
conclude confluence from local confluence.  Local confluence follows
because all the -peaks can be joined using either one -step on
either side as for STLC, or else using additional balancing steps if
one of the rules applied is .  

But even easier than this reasoning is applying an automated
confluence checker: the ACP tool immediately reports that the abstract
reduction relation is confluent~\cite{aoto+09}.
\end{proof}

The proofs of Theorems~\ref{uni-term} and~\ref{uni-confluent}
demonstrate how the rewriting approach to typing benefits from recent
advances in analysis tools for term rewriting: we can use termination
and confluence checkers to analyze the abstract reduction relation
 corresponding to typing.  We expect this situation to recur
for more advanced type systems, although some may provide new
challenges for automated analysis tools (we give an example below).

\begin{lem}
\label{uni-complete}
  . 
\end{lem}


\begin{proof}
We distinguish the peaks originating at typable terms . 

If  and  steps are parallel --   -- the peak can be completed
directly .

If the  and  steps overlap, there are two cases,
corresponding to  and
 reduction steps. We show the completion
for  peaks (omitting the 
steps to simplify the presentation); the argument for
 peaks is similar.



\noindent The -steps are justified because the peak term
(shown on line (P)) is typable by composition with .  By confluence of
abstract reduction, this implies that the sources of all the left
steps are also typable.  For each -step, since abstract
reduction cannot drop redexes (as all rules are non-erasing), we argue
as for STLC that a descendant of the appropriate displayed
\textit{kind}-term or application must eventually be contracted, as
otherwise, a stuck descendant of such would remain in the final term.
Kindable terms cannot contain stuck applications or stuck
\textit{kind}-terms, because our abstract reduction rules are
non-erasing.  And contraction of those displayed \textit{kind}-terms
or applications requires the reductions used for the -steps,
which are sufficient to complete the peak.
\end{proof}


\begin{lem}
  \label{uni-normal}
  Every normal form with respect to  is also a normal form with respect to .
\end{lem}

The normal forms of  include ,  where 
and  are -normal forms,  where , and
 where  is not generated by the grammar . By inspection,  and .

\begin{thm}[Type Preservation]
\label{thm:presuni}
Let  be mixed terms and  be a term such that . 
If  and , then .


\begin{proof}
  By application of Theorem~\ref{thmtp}. Condition (1) is satisfied by
  instantiating  by the set  of terms . Condition (2) follows by
  Theorem~\ref{uni-confluent}. Condition (3) by
  Lemma~\ref{uni-complete}, condition (4) by Lemma~\ref{uni-normal}.
\end{proof}
\end{thm}


\begin{thm}
  \label{uni-combined-confluent}
  Every mixed typable term is confluent with respect to the reduction
  relation .

  \begin{proof}
For proving that  is confluent for typable mixed terms we 
need to check properties (1) to (4) of Theorem \ref{thmcr} for the 
particular relations 
and . The composition
of  and  with  serves
to ensure that we are only considering typable terms.

Property (2) follows from Theorem \ref{uni-confluent} and the fact that
 is the identity relation. All 1-step
peaks of must be of the form , due to the
composition with . By Theorem
\ref{uni-confluent}, if , then there exists
 such that . Thus, any
 peak  can be completed with . By Theorem \ref{uni-term}  is terminating,
so  is also
terminating, proving property (1). Property (3) follows from Lemma
\ref{uni-complete}. Property (4) follows from Lemma~\ref{uni-normal}.
  \end{proof}
  
\end{thm}











As an aside, note that a natural
modification of this problem is out of the range of ACP, version 0.20.
Suppose we are trying to group kind-checking terms so that we can
avoid duplicate kind checks for the same term.  For this, we may wish
to permute \textit{kind}-terms, and pull them out of other term
constructs.  The following rules implement this idea, and can be
neither proved confluent nor disproved by ACP, version 0.20.  Just the
first seven rules are also unsolvable by ACP. {\small
\begin{verbatim}
(VAR a b c A B C D)
(RULES
  S(A,B,C) -> kind(A,kind(B,kind(C,
              arrow(arrow(arrow(A,arrow(B,C)),arrow(A,B)),arrow(A,C)))))
  K(A,B) -> kind(A,kind(B,arrow(A,arrow(B,A))))
  app(arrow(A,b),A) -> kind(A,b)
  kind(base,a) -> a
  kind(arrow(A,B),a) -> kind(A, kind(B, a))
  kind(A,kind(A,a)) -> kind(A,a)
  kind(A,kind(B,a)) -> kind(B,kind(A,a))
  app(kind(A,b),c) -> kind(A,app(b,c))
  app(c,kind(A,b)) -> kind(A,app(c,b))
  arrow(kind(A,b),c) -> kind(A,arrow(b,c))
  arrow(c,kind(A,b)) -> kind(A,arrow(c,b))
  kind(kind(a,b),c) -> kind(a,kind(b,c))
)
\end{verbatim}
}

\section{Generalizing Nuprl's Direct Computation Rules}
\label{sec:genpresstlc}

Martin-L\"of's Intuitionistic Type Theory (ITT), as formulated
in~\cite{martinloef+84}, is a system of four judgments presented with
a rigorous but informal semantics.  A typing judgment of the form
 ``means that  has a canonical object of the canonical type
denoted by A as value''~\cite[page 174]{martinloef+84}.  Here,
Martin-L\"of is making use of the concept of a term (of ITT) having a
value, a concept he defines earlier in the paper.  The authors of the
Nuprl system realized that this semantics justifies more permissive
typing rules than allowed by Martin-L\"of's own formal
systems~\cite{constable+86} (see also Section 2.2 of~\cite{allen+06}
for a historical perspective).  In particular, it justifies so-called
\emph{direct computation} rules, which turned out to be useful for
formal development with Nuprl:

\noindent Applying Theorem~\ref{thm:relatetyp}, we can view this rule
from a rewriting perspective.  We will use call-by-value reduction, as
full -reduction would require additional technicalities that
would not be illuminating (we would have to use parallel reduction and
incorporate a proof of confluence of -reduction, in order to
get preservation of generalized typing).

\noindent In this section, we will take the idea of Nuprl's direct
computation rules one step further, by adopting the following
definition.

\begin{defi}[Generalized Typability]
A mixed term  is called \textbf{generalized typable}
if  for some type .
\end{defi}

\noindent This allows us to view (call-by-value versions of) Nuprl's
direct computation rules as embodying a special case of generalized
typability, namely .  We will see in this section
that we can prove type preservation directly for generalized typing,
using the rewriting approach.  Note that generalized typability is not
obviously decidable, since  is not terminating

A simple example of generalized typability is given by the
term .  Note that
the argument term  is not simply typable.  This
term has several -reduction sequences, including the following
one:

\noindent Because this term -reduces to a type, the generalized
type-safety property we will obtain in this section tells us that the
-normal form of this term, if such exists, is a value.  This can,
of course, be confirmed for this case, where the -normal form is
just .  Notice that this example also shows that
 is not confluent, as we can also reduce it to a stuck term
in this way:


\begin{thm}[Generalized Type Preservation for Call-By-Value Reduction]
\label{thm:genpresstlc}
If  and , then .
\end{thm}

\begin{proof}
We cannot conveniently apply Theorem~\ref{thmtp}, because the natural
instantiation would be to take  for the relation  in
the theorem -- but then we would have to prove confluence of
, which does not hold (as shown just above).  So instead we
give a direct proof, by induction on the length of the assumed
-sequence from  to .  The sequence cannot be of length ,
since  cannot be a type (since it -reduces, as no type can).

For the step case: suppose the assumed -reduction is of the form
.  We now consider cases for the form of
overlap of the step  and .  Suppose the
-step is .  If the -step is in ,
that means , where the hole in  is at the same
position as in .  We can just permute these steps, to obtain
 and .  Now the
induction hypothesis can be applied with  (i.e., ) as
the peak term, and  as the term to which it -steps.  

So suppose the -step is in the displayed  of .
Then before the reduction sequence from  to  can perform a
-step, it must first reduce the residual of  to , since
that residual occurs in a -reduction position.  So the reduction
sequence from  to  must look like the following, where the
hole in  and in  are at the same position:

\noindent By performing the -reductions which transformed  to
, we can reduce  to , and then we are done,
since we then have .

We now must consider the case where the -step is .  Again, if the -step is in
, we can permute steps and apply the induction hypothesis.  If
the -step is in  or in , we can also permute the steps,
though if the reduction is in  (say ), we will in
general have , since  need not
appear exactly once in .  Nevertheless, we can still apply the
induction hypothesis with  as the peak term, since we will only
ever produce one -step from  by permuting steps.  Finally,
suppose the -step is .  By similar reasoning as in the previous case, the
-reduction sequence from  to  may
contain -steps transforming  to some , but it cannot
take a -step until it has reduced the displayed  to , with  and .  This is because that displayed term is in -reduction
position and neither a value nor a redex.  We can then duplicate any
-steps taken in  to -reduce  (i.e., ) to
.  This term then -reduces to , and we are
done.
\end{proof}

\begin{thm}[Generalized Progress]
\label{thm:genprogstlc}
If standard term  is closed, , and ,
then  is a (standard) value.
\end{thm}

\begin{proof} As for Theorem~\ref{thm:progstlc}, we obtain
this result by applying Lemmas~\ref{lem:qsnfb} and~\ref{lem:stuck}. 
\end{proof}

\begin{thm}[Generalized Type Safety]
\label{thm:gensafety}
If standard term  is closed, , and , then  is a (standard) value.
\end{thm}
\begin{proof} This is a direct corollary of
Theorems~\ref{thm:genpresstlc} and~\ref{thm:genprogstlc}. \end{proof}

\section{A Rewriting Approach to Normalization for STLC}
\label{sec:normstlc}

In this Section, we will see how the rewriting approach to typing
impacts a standard approach to proving that every typable (closed)
standard term of the simply typed lambda calculus has a -normal
form.  We will work with a slightly different presentation of STLC
than we saw in Section~\ref{sec:restlc}, in particular dispensing with
the term constants  and .  We assume a non-empty set of type
constants .  The syntax we are using in this section is:



\noindent The abstract and concrete reduction relations are then
defined as follows, where we use mixed terms  as contexts
(sometimes using meta-variable  in this case), writing
 to denote the replacement of the unique occurrence of a
special variable  in  by .  



\subsection{Interpretation of Mixed Terms}
\label{sec:interp}

The proof in this section is based on ideas from standard proofs, such
as Girard's proof in the book \emph{Proofs and
  Types}~\cite{girard-proofs-types}.  The technical details evolve
differently, however, since we are using the rewriting approach to
typing.  Similarly to Girard's proof, we are going to define an
interpretation of open types as sets of standard terms.  Here, we need
to generalize this to give interpretations  of mixed
terms , where (as standard)  assigns interpretations to the
free variables of .  The most enlightening observation that will
come from this is Theorem~\ref{thm:abstr} (Abstraction Theorem), which
says that interpretation is monotonic with respect to abstract
reduction: if , then
.  If one views a set as
abstracting its elements, and if one considers a mixed term as a code
for the set of terms which is its interpretation, then the Abstraction
Theorem shows that more abstract codes have more abstract
interpretations.  This is an elegant perspective that arises -- from
the standard Tait-Girard method -- only by taking a small-step view of
typing; existing proofs for normalization in the literature do not
have any theorem which corresponds (in any obvious way) to the
Abstraction Theorem.

So now to begin the development, let WN be the set of standard terms
which are weakly normalizing with respect to  (that is, terms
 such that there exists some  such that ).  Also, if  is any binary relation on standard terms
and  any set of standard terms, we will write  for the
image of  under  (that is, ). 

\

\noindent We first define  to be the set of all sets 
of standard terms satisfying the following conditions:
\begin{enumerate}[(1)]
\item 
\item 
\item 
\end{enumerate}
\noindent The first condition ensures that  and 
imply .  An assumption like this is often made about such
sets of terms.  We will call elements of 
\emph{reducibility sets}.  Much work has been devoted to comparing
different conditions for families of sets in the context of the
interpretation of types (see, e.g.,~\cite{riba07,gallier90}).  Our
focus here is not so much on the specific conditions on the
interpretations of mixed terms, as on how interpretations of terms in
the abstract reduction relation are related.  The conditions we adopt
here are simple and sufficient for weak normalization of closed terms
(cf. also Chapter 12 of~\cite{pierce02}).

We will use  as a meta-variable for \emph{assignments}, which are
functions from \textit{Var} to .  We write  to
mean the function  updated to map variable  to
.  Now for any  and  with
, we define the
interpretation  of  with respect to  in
Figure~\ref{fig:interp}.  To ensure that interpretations of types
satisfy the first property above of reducibility sets, we need
to close under  in the last two clauses of the definition
(in Figure~\ref{fig:interp}).  Since we are proving normalization, we
take the set of normalizing terms as the interpretation of ,
similarly to what is standardly done for atomic types (e.g., in
Girard's proof).

\begin{figure}

\caption{The interpretation of mixed terms}
\label{fig:interp}
\end{figure}

\subsection{Interpretations of Types are Reducibility Sets}

In this section, we prove that for all types  and  with
, we have
.  We will elide this condition
relating  (or instead ) and  below.  We prove the three
properties of reducibility sets given in the previous section.
The properties must be proved in order, as later properties depend on
earlier ones.  The first property is needed in a more general form,
for any mixed term , and not just types .  The second two
properties are only needed for types.  The proofs in this section are
similar to those used for the standard definition of typing, except
that there, they are usually proved by mutual induction.  Here we can
prove them independently, though in sequence, due to the simpler form
of the second property.  While the development in this section is
similar to the usual one, in the next section we will see something
significantly different.

\begin{lem}
\label{lem1}

\end{lem}
\begin{proof} The proof is by structural induction on .  If  is a -abstraction,
or application, the desired property follows by idempotence of
 as an operator on sets of terms.  If  is a variable ,
then the property follows by the same property for , since we
stipulated assignments map variables to elements of .  If
, then we must prove

\noindent But this just amounts to the obvious fact that if  and  is weakly normalizing, then  is also weakly
normalizing.

\

\noindent Finally, suppose  is  for some .  Assume an
arbitrary , and arbitrary  with
.  We must show .  To do
this, by the definition of the interpretation of -terms, it
suffices to consider arbitrary , and show
.  We have  by
the definition of the interpretation of -terms.  Then we get the
desired conclusion by the induction hypothesis on , since
.
\end{proof}

\begin{lem}
\label{lem2}

\end{lem}

\begin{proof} The proof is by structural induction on .  If  is , then the
desired property holds immediately, since  is in
.  So suppose , for some 
and .  We must exhibit some .  By
the induction hypothesis applied to , there exists some
.  Now take  for the
required term , where we assume .  We just have to confirm that .  So assume arbitrary
, and show .  By Lemma~\ref{lem1}, it suffices
to prove , since .  But we are assuming
.
\end{proof}

\begin{lem}
\label{lem3}

\end{lem}

\begin{proof} The proof is again by structural induction on , and is trivial when
 is .  So suppose , and assume arbitrary
.  We must show .
By Lemma~\ref{lem2}, we know there exists some term
.  Then by the definition of the
interpretation of -terms, .  By the
induction hypothesis applied to , we then have
.  But this implies , as
required.
\end{proof}

\begin{cor}

\end{cor}

\noindent The above lemmas have proved that 
satisfies the three properties for membership in .  In
the next section, we will also need the following lemma, whose proof
is routine and omitted:

\begin{lem}[Semantic Substitution]
\label{lem:semsubst}

\end{lem}

\subsection{The Abstraction Theorem}
\label{sec:abstr}

In this section, we prove a remarkable theorem, from which the
normalization property for typable terms will follow as a corollary.
For any mixed terms  and , and any  with
, we have:

\begin{thm}[Abstraction Theorem]
\label{thm:abstr}

\end{thm}
\noindent Note that well-definedness of  in the
statement of the theorem follows from the assumption about  and
the observation that abstract reduction cannot introduce new
variables.

\

\noindent This theorem is remarkable because it reflects the essence of
abstraction: the gathering of different concrete entities under the
same abstract one.  The Abstraction Theorem shows that abstract
reduction is increasing the set of concrete terms which are collected
under a mixed (and so partially abstract) term.  In the next section,
we will see how to conclude normalization from this theorem.

\

\begin{proof}[Proof of Theorem~\ref{thm:abstr}]
It suffices to prove by structural induction on  that for all
 and for all  and  where  is a redex and  its
contractum:

\noindent \textbf{Case:} , where the hole is
in .  The case where the hole is in  is similar, so we omit
it.  To show the required
,
consider arbitrary .  By the definition
of the interpretation of applications, we must have
 and  with
.  Now by the induction hypothesis applied to
 we have:

\noindent This implies .  From
this, we obtain the desired  by the
definition of the interpretation of applications.

\

\noindent \textbf{Case:} , for some
, , and , with the hole in .  Consider an arbitrary
.  By the definition of the
interpretation of -abstractions, this implies that there
exists a term  such that  and for all
, we have
.  We must
show .  By the definition of
the interpretation of -terms and Lemma~\ref{lem1}, it suffices to
prove  for
arbitrary .  Again applying Lemma~\ref{lem1},
we can see it suffices to prove
.  This now
follows by the induction hypothesis applied to context .

\

\noindent \textbf{Case:} .  Now we must distinguish the
two cases for an abstract reduction.

\

\noindent \textbf{Case 1.} Suppose that we have

\noindent We must prove .  So assume
arbitrary , and show
.  To show that, it suffices to consider
arbitrary , and prove
.  By the definition of the
interpretation of -abstractions, we have , for some , with
 for all
.  Since , it suffices
by Lemma~\ref{lem1} just to prove .
This follows from the fact just derived, applying also Lemma~\ref{lem:semsubst}.


\

\noindent \textbf{Case 2.} Suppose that we have

\noindent Assume an arbitrary .  By the
definition of the interpretation of applications, we then have that
there exists  and 
such that .  We must show .
By the definition of the interpretation of -terms, we obtain
.  By Lemma~\ref{lem1}, this suffices
to  establish , since .
\end{proof}

\subsection{Concluding Normalization}

Using the Abstraction Theorem, we can obtain the main result that
typable terms are normalizing.  First, we need this helper lemma
stating that standard terms are in their own interpretations:

\begin{lem}
\label{lem:inown}
Consider an arbitrary standard term  and assignment , as well
as function  from variables to standard terms.  Suppose also
that for all , we have .  Then
we have .
\end{lem}
\begin{proof}
The proof is by structural induction on .  If  is a
variable , then we have  by assumption.  If 
is of the form , then the definition of the
interpretation of mixed terms tells us:

\noindent To show that  is itself a member of the set
on the right-hand side of this equation, it suffices to consider an
arbitrary , and show .  Here we can apply the
induction hypothesis for , with  and
.  The two substitutions still satisfy the
required properties.  Finally, if  is of the form , the
result easily follows from the induction hypothesis applied to 
and also to , and the definition of the interpretation of
applications.
\end{proof}

\begin{thm}[Normalization for Typable Terms]
For all closed standard terms  and types , if , then .
\end{thm}

\proof By Lemma~\ref{lem:inown}, we have
.  Then by iterated application of
Theorem~\ref{thm:abstr}, we know that
.  By
Lemma~\ref{lem3}, .
Putting these facts together, we get this chain of relationships,
which suffices:


\subsection{Summary of The Standard Proof}

\newcommand{\redd}[1]{\textit{Red}_{#1}}
\newcommand{\nxt}[0]{\textit{next}}
\newcommand{\sn}[0]{\textit{SN}}

Here, we summarize Girard's proof of strong normalization, for
purposes of comparison~\cite{girard-proofs-types}.  This proof is
based on the usual judgment  for STLC.  One first
defines an interpretation of types:

\noindent This does not require use of a function  as above
(though the standard proof for System F does).  For this
interpretation of types, one then proves these three properties, by
mutual structural induction on the type  mentioned in all three
properties:

\begin{enumerate}[(1)]
\item .
\item .
\item If  is neutral, then .
\end{enumerate}

\noindent A term is neutral iff it is not a -abstraction.
The third property implies that all the variables are in  for
every .  Finally, one derives the following different theorem in
place of the Abstraction Theorem:

\begin{thm}[Reducibility]
\label{thm:red}
Suppose , and
consider arbitrary , for all .
Then .
\end{thm}

\noindent Now we can obtain as a corollary that 
implies , since  by the first property
above, and a substitution  replacing  by  satisfies the
required condition, since all variables are included in all sets
.

\subsection{Discussion}

The main difference in the rewriting-based development and the
standard one is in deriving the Abstraction Theorem.  The form of the
theorem is completely different from Theorem~\ref{thm:red}.  One nice
technical feature is that for the proof of the Abstraction Theorem, we
did not need to apply a substitution to terms inhabiting
interpretations of types, as we did for Theorem~\ref{thm:red}.  We
still needed to use the idea of such a substitution, but it appeared
only in a simple helper lemma, namely Lemma~\ref{lem:inown}.  This is
an advantage of the rewriting-based version, since the substitution
does not clutter up the proof of the central result.  One disadvantage
of the rewriting-based version is that we needed the function 
and Lemma~\ref{lem:semsubst} -- but this is not such a significant
disadvantage, since those devices are needed when we move to System F
in the standard development anyway.

\section{Conclusion}
\label{sec:conclusion}

We have seen how rewriting techniques can be used to develop the
meta-theory of simple types.  Typing is treated as a small-step
abstract reduction relation, and type safety, based on type
preservation and progress theorems, can be established by analysis of
the interactions between abstract and concrete reduction steps.  A
crucial ingredient of our approach to type preservation, as defined by
Theorem~\ref{thmtp}, is to have a confluent abstract reduction
relation.  For simply typed lambda calculus, this was a trivial
matter, but we saw a more complex example, where applying automated
confluence-checking tools developed in the term-rewriting community
was able to automate this part of the type preservation proof.
Confluence of the combination of abstract and concrete reduction for
typable terms is an easy corollary of type preservation
(Theorem~\ref{thmcr}).  We have also seen how to adapt a standard
proof of normalization for simply typed terms, for the rewriting
approach to typing.  For this proof, mixed terms are interpreted as
sets of standard terms, and the crucial insight is embodied in the
Abstraction Theorem, which shows that those sets are enlarged by
reduction of the corresponding mixed terms.

There are many avenues for future work.  First, the rewriting approach
should be applied to more advanced type systems, including ones with
impredicative polymorphism.  Dependent type systems pose a particular
challenge, because from the point of view of abstract reduction,
-bound variables must play a dual role.  When computing a
dependent function type  from an abstraction , we may need to abstract  to , as for STLC; but we may
also need to leave it unabstracted, since with dependent types,  is
allowed to appear in the range type .  It would also be
interesting to see if there are consequences of the rewriting approach
to typing when applied to proofs via the Curry-Howard isomorphism.
Theorem~\ref{thm:abstr} (Abstraction) shows how the set of proofs in
the meaning of a mixed proof term (part proof and part formula)
increases as the term is abstracted.  Certainly, the present methods
yield the syntactic capability to incrementally transform a proof to
the theorem it proves.  This could already be valuable in practice for
efficient proof checking, for example of large proofs produced by SAT
or SMT solvers (cf.~\cite{stump+12}).

It would be interesting to go further in automating proofs of type
preservation based on the rewriting approach.  While the Programming
Languages community has invested substantial effort in recent years on
computer-checked proofs of properties like type safety for programming
languages (initiated particularly by the POPLmark
Challenge~\cite{poplmark}), there is relatively little work on fully
automatic proofs of type preservation (an example
is~\cite{schurmann+98}).  The rewriting approach could contribute to
filling that gap, since the methods we used above for analyzing
interactions of abstract and concrete steps to prove type preservation
are similar to those used for proving confluence of combined
reduction.

Our longer term goal is to use this approach to design and analyze
type systems for symbolic simulation.  In program verification tools
like \textsc{Pex} and \textsc{KeY}, symbolic simulation is a central
component~\cite{KeyBook2007,Tillmann+2005}.  But these systems do not
seek to prove that their symbolic-simulation algorithms are correct.
Indeed, the authors of the \textsc{KeY} system argue against expending
the effort to do this~\cite{Beckert+06}.  The rewriting approach
promises to make it easier to relate symbolic simulation, viewed as an
abstract reduction relation, with the small-step operational
semantics.

\textbf{Acknowledgments.} We thank the anonymous LMCS reviewers for
their very detailed comments, and a number of technical suggestions
which have greatly improved this paper; and also participants of the
RTA 2011 conference for their helpful feedback and suggestions about
this work.



\begin{thebibliography}{10}

\bibitem{AGM92}
S.~Abramsky, D.~Gabbay, and T.~Maibaum, editors.
\newblock {\em {Handbook of Logic in Computer Science}}.
\newblock Oxford University Press, 1992.

\bibitem{allen+06}
Stuart~F. Allen, Mark Bickford, Robert~L. Constable, Richard Eaton, Christoph
  Kreitz, Lori Lorigo, and E.~Moran.
\newblock {Innovations in computational type theory using Nuprl}.
\newblock {\em J. Applied Logic}, 4(4):428--469, 2006.

\bibitem{aoto+09}
T.~Aoto, J.~Yoshida, and Y.~Toyama.
\newblock {Proving Confluence of Term Rewriting Systems Automatically}.
\newblock In R.~Treinen, editor, {\em Rewriting Techniques and Applications
  (RTA)}, pages 93--102, 2009.

\bibitem{poplmark}
B.~Aydemir, A.~Bohannon, M.~Fairbairn, J.~Foster, B.~Pierce, P.~Sewell,
  D.~Vytiniotis, G.~Washburn, S.~Weirich, and S.~Zdancewic.
\newblock {Mechanized metatheory for the masses: The POPLmark Challenge}.
\newblock In {\em Proceedings of the Eighteenth International Conference on
  Theorem Proving in Higher Order Logics (TPHOLs 2005)}, 2005.

\bibitem{B92}
H.~Barendregt.
\newblock {Lambda Calculi with Types}.
\newblock In S.~Abramsky, D.~Gabbay, and T.~Maibaum, editors, {\em {Handbook of
  Logic in Computer Science}}, volume~2, pages 117--309. Oxford University
  Press, 1992.

\bibitem{KeyBook2007}
B.~Beckert, R.~H\"ahnle, and P.~Schmitt, editors.
\newblock {\em {Verification of Object-Oriented Software: The {KeY} Approach}}.
\newblock LNCS 4334. Springer-Verlag, 2007.

\bibitem{Beckert+06}
B.~Beckert and V.~Klebanov.
\newblock {Must Program Verification Systems and Calculi be Verified?}
\newblock In {\em Proceedings, 3rd International Verification Workshop
  (VERIFY), Workshop at Federated Logic Conferences (FLoC), Seattle, USA},
  pages 34--41, 2006.

\bibitem{constable+86}
Robert~L. Constable, Stuart~F. Allen, S.~F. Allen, H.~M. Bromley, W.~R.
  Cleaveland, J.~F. Cremer, R.~W. Harper, Douglas~J. Howe, T.~B. Knoblock,
  N.~P. Mendler, P.~Panangaden, Scott~F. Smith, James~T. Sasaki, and S.~F.
  Smith.
\newblock {\em {Implementing Mathematics with The Nuprl Proof Development
  System}}.
\newblock Prentice Hall, 1986.

\bibitem{ellison+08}
C.~Ellison, T.~\c{S}erb\u{a}nu\c{t}\u{a}, and G.~Ro\c{s}u.
\newblock {A Rewriting Logic Approach to Type Inference}.
\newblock In A.~Corradini and U.~Montanari, editors, {\em Recent Trends in
  Algebraic Development Techniques (WADT)}, pages 135--151, 2008.

\bibitem{gallier90}
Jean Gallier.
\newblock {\em {On Girard's "Candidats De Reductibilit\'{e}"}}, pages 123--230.
\newblock Academic Press, 1990.

\bibitem{aprove}
J.~Giesl, P.~Schneider-Kamp, and R.~Thiemann.
\newblock {Automatic Termination Proofs in the Dependency Pair Framework}.
\newblock In U.~Furbach and N.~Shankar, editors, {\em Automated Reasoning,
  Third International Joint Conference (IJCAR)}, pages 281--286, 2006.

\bibitem{girard-proofs-types}
J.-Y. Girard, Y.~Lafont, and P.~Taylor.
\newblock {\em {Proofs and Types}}.
\newblock Cambridge University Press, 1989.

\bibitem{rosu+10}
M.~Hills and G.~Rosu.
\newblock {A Rewriting Logic Semantics Approach to Modular Program Analysis}.
\newblock In C.~Lynch, editor, {\em Proceedings of the 21st International
  Conference on Rewriting Techniques and Applications, RTA 2010, July 11-13,
  2010, Edinburgh, Scotland, UK}, pages 151--160, 2010.

\bibitem{kuan+07}
G.~Kuan, D.~MacQueen, and R.~Findler.
\newblock A rewriting semantics for type inference.
\newblock In {\em Proceedings of the 16th European conference on Programming
  (ESOP)}, pages 426--440. Springer-Verlag, 2007.

\bibitem{martinloef+84}
P.~Martin-L{\"o}f and Z.~A. Lozinski.
\newblock {Constructive Mathematics and Computer Programming}.
\newblock {\em Philosophical Transactions of the Royal Society of London.
  Series A, Mathematical and Physical Sciences}, 312(1522):pp. 501--518, 1984.

\bibitem{M96}
J.~Mitchell.
\newblock {\em {Foundations for Programming Languages}}.
\newblock The MIT Press, 1996.

\bibitem{pierce02}
B.~Pierce.
\newblock {\em {Types and Programming Languages}}.
\newblock The MIT Press, 2002.

\bibitem{riba07}
Colin Riba.
\newblock {Strong Normalization as Safe Interaction}.
\newblock In {\em 22nd IEEE Symposium on Logic in Computer Science (LICS 2007),
  10-12 July 2007, Wroclaw, Poland, Proceedings}, pages 13--22. IEEE Computer
  Society, 2007.

\bibitem{schurmann+98}
C.~Sch{\"u}rmann and F.~Pfenning.
\newblock {Automated Theorem Proving in a Simple Meta-Logic for LF}.
\newblock In C.~Kirchner and H.~Kirchner, editors, {\em 15th International
  Conference on Automated Deduction (CADE)}, pages 286--300, 1998.

\bibitem{stump+11}
Aaron Stump, Garrin Kimmell, and Roba El~Haj Omar.
\newblock {Type Preservation as a Confluence Problem}.
\newblock In Manfred Schmidt-Schau{\ss}, editor, {\em Proceedings of the 22nd
  International Conference on Rewriting Techniques and Applications (RTA)},
  volume~10 of {\em LIPIcs}, pages 345--360, 2011.

\bibitem{stump+12}
Aaron Stump, Duckki Oe, Andrew Reynolds, Liana Hadarean, and Cesare Tinelli.
\newblock Smt proof checking using a logical framework.
\newblock {\em Formal Methods in System Design}, pages 1--28.
\newblock available online as of July, 2012.

\bibitem{terese}
TeReSe, editor.
\newblock {\em Term Rewriting Systems}, volume~55 of {\em Cambridge Tracts in
  Theoretical Computer Science}.
\newblock Cambridge University Press, 2003.

\bibitem{Tillmann+2005}
N.~Tillmann and W.~Schulte.
\newblock {Parameterized Unit Tests}.
\newblock {\em SIGSOFT Softw. Eng. Notes}, 30:253--262, 2005.

\bibitem{wright+94}
A.~Wright and M.~Felleisen.
\newblock {A Syntactic Approach to Type Soundness}.
\newblock {\em Information and Computation}, 115(1):38--94, 1994.

\end{thebibliography}


\end{document}
