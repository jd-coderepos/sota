\documentclass[nocopyrightspace,preprint]{sigplanconf}
\usepackage{stmaryrd,bcprulesmhfix,tensor}
\bcprulessavespacetrue
\suppressrulenamesfalse
\usepackage{mathpartir}
\usepackage{upgreek}
\usepackage{proof}
\usepackage[all]{xy}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{xypic}
\usepackage{mathtools}
\bibliographystyle{abbrv}
\usepackage{url,amssymb}
\renewcommand{\phi}{\varphi}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{comment}
\RequirePackage{txfonts}

\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{1ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}
\usepackage{color}

\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkblue}{rgb}{0,0,0.5}

\usepackage{ifpdf}






\usepackage{paralist}

\makeatletter
\newcommand*{\bdiv}{\nonscript\mskip-\medmuskip\mkern5mu\mathbin{\operator@font div}\penalty900\mkern5mu\nonscript\mskip-\medmuskip
}
\makeatother
\newcommand{\keywd}[1]{\mathtt{#1}}
\newcommand{\KPER}{\mathrm{KPER}}
\newcommand{\myread}[1]{!{#1}}
\newcommand{\wcpo}[1]{-cpo}
\newcommand{\wcpos}[1]{-cpos}
\newcommand{\myref}[1]{\keywd{ref}(#1)}
\newcommand{\regm}{{-}}
\newcommand{\sq}[4]{\tensor*[^{#1}_{#2}]{\Diamond}{^{#3}_{#4}}}
\newcommand{\sqsol}{\Diamond}
\newcommand{\intt}[1]{\textit{int}(#1)}
\newcommand{\myeffto}[3]{\xrightarrow[#2]{#1\,\mid\, #3}}
\newcommand{\effto}[1]{\stackrel{#1}{\to}}
\newcommand{\wrong}{\textit{wrong}}
\newcommand{\mnond}{\texttt{?}}
\newcommand{\unitt}[1]{\textit{unit}(#1)}
\newcommand{\sqleq}{\sqsubseteq}
\newcommand{\QPER}{\operatorname{QPER}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\booll}[1]{\textit{bool}(#1)}
\newcommand{\reff}[1]{\textit{loc}(#1)}
\newcommand{\prodd}[1]{\textit{pair}(#1)}
\newcommand{\funn}[1]{\textit{fun}(#1)}


\newcommand{\mylet}{\keywd{let}}
\newcommand{\FV}{\textit{FV}}
\newcommand{\subty}{\mathrel{<:}}
\newcommand{\mmskip}{\keywd{skip}}
\newcommand{\mwhile}{\keywd{while}}
\newcommand{\partfun}{\rightharpoondown}
\newcommand{\mdo}{\keywd{do}}
\newcommand{\mif}{\keywd{if}}
\newcommand{\mthen}{\keywd{then}}
\newcommand{\melse}{\keywd{else}}
\newcommand{\massign}{\keywd{:=}}
\newcommand{\mequals}{\keywd{=}}
\newcommand{\mseq}{\keywd{;}}
\newcommand{\mtrue}{\keywd{true}}
\newcommand{\mfalse}{\keywd{false}}
\newcommand{\mx}[1]{\keywd{#1}}
\newcommand{\und}[1]{\underline{#1}}
\newcommand{\sbra}{[\![}
\newcommand{\sket}{]\!]}
\newcommand{\lift}[1]{\lceil #1 \rceil}
\newcommand{\vars}{\mathbb{V}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\bools}{\mathbb{B}}
\newcommand{\stores}{\mathit{Stores}}
\newcommand{\stacks}{\mathit{Stacks}}
\newcommand{\nats}{\mathbb{N}}
\newcommand{\inttype}{\keywd{int}}
\newcommand{\reftype}{\keywd{ref}}
\newcommand{\myexp}[1]{#1\ \texttt{exp}}
\newcommand{\intexp}{\myexp{\inttype}}
\newcommand{\boolexp}{\myexp{\booltype}}
\newcommand{\comm}{\texttt{com}}
\newcommand{\booltype}{\keywd{bool}}
\newcommand{\unittype}{\keywd{unit}}
\newcommand{\unitval}{\keywd{()}}
\newcommand{\mquote}[1]{\mathtt{#1}}
\newcommand{\bop}{\mathrel{bop}}
\newcommand{\op}{\mathrel{op}}
\newcommand{\lop}{\mathrel{lop}}
\newcommand{\mnot}{\keywd{not}}
\newcommand{\bnfeq}{: =}
\newcommand{\bigtop}{\mathbb{T}}
\newcommand{\bigbot}{\mathbb{F}}
\newcommand{\singleton}[1]{\{#1\}}
\newcommand{\labs}{\mathbb{L}}
\newcommand{\dom}[1]{\mathrm{dom}({#1})}
\newcommand{\cod}[1]{\mathrm{cod}({#1})}
\newtheorem{assumption}{Assumption}


\newcommand{\pause}{\vspace{1.5ex}}
\newcommand{\gap}{\quad\quad}
\newcommand{\biggap}{\quad\quad\quad}
\newcommand{\equalsdef}{\stackrel{\mathit{def}}{=}}

\newcommand{\rom}[1]{{\rm #1}}

\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\lz}[1]{\makebox[1.5in][r]{}}
\newcommand{\rz}[1]{\makebox[3in][l]{}}

\newcommand{\why}[1]{\quad\quad\mbox{\{ #1 \}}}

\newcommand{\centre}[1]{}
\newcommand{\centretran}[1]{}
\newcommand{\idd}{\textit{id}}

\newenvironment{fig}{\begin{minipage}{\textwidth}}{\end{minipage}\pause }
\newcommand{\squelch}[1]{}
\newcommand{\myremark}[1]{\par***** NOTE ******* #1\par}

\newcommand{\ipushval}[1]{\texttt{pushc}\ #1}
\newcommand{\ipushvar}[1]{\texttt{pushvar}\ #1}
\newcommand{\ipop}[1]{\texttt{pop}\ #1}
\newcommand{\iop}[1]{\texttt{binop}_{#1}}
\newcommand{\iif}[1]{\texttt{brtrue}\ #1}
\newcommand{\ihalt}{\texttt{halt}}
\newcommand{\store}{S}
\newcommand{\mstack}{\sigma}
\newcommand{\mconfig}[1]{\langle #1 \rangle}
\newcommand{\mstep}[3]{[#1];\mconfig{#2}\to \mconfig{#3}}
\newcommand{\multistep}[3]{[#1];\mconfig{#2}\to^{*} \mconfig{#3}}
\newcommand{\meval}[3]{[#1];\mconfig{#2}\Downarrow \mconfig{#3}}
\newcommand{\stretchyspace}{\hspace{1cm}\hspace*{\fill}}
\newcommand{\sameline}{\endprooftree\stretchyspace\prooftree}
\newcommand{\nextline}{\ruleend\smallskip \rulestart}
\newcommand{\lind}[1]{\mathcal{L}_{#1}}
\newcommand{\diverges}[1]{\mathcal{D}_{#1}}

\newcommand\orth[2]{\ensuremath{#1\,\bot\, #2}\xspace}

\newcommand{\vfix}[3]{\keywd{rec}\:{#1}\:{#2} = {#3}}
\newcommand{\letin}[2]{\keywd{let}\:{#1}\!=\!{#2}\:\keywd{in}\:}

\newcommand{\letrec}[3]{\keywd{let\ rec}\ {#1\ #2}\!\Leftarrow\!{#3}\ \keywd{in}\ }
\newcommand{\opletrec}[3]{\keywd{let\ rec}}
\newcommand{\lettwoin}[4]{\keywd{let}\:{#1}\!\Leftarrow\!{#2};{#3}\!\Leftarrow\!{#4}\:\keywd{in}\:}
\newcommand{\tlet}[3]{\keywd{let}\  #1 \Leftarrow #2\ \keywd{in}\ #3}
\newcommand{\valof}[1]{\keywd{val}\:{#1}}
\newcommand{\app}[2]{{#1}\:{#2}}
\newcommand{\abs}[2]{\lambda{#1}.\:{#2}}
\newcommand{\tabs}[2]{\Lambda{#1}.{#2}}
\newcommand{\tapp}[2]{{#1}_{#2}} 
\newcommand{\proj}[2]{\pi_{#1}{#2}}
\newcommand{\tinj}[2]{\keywd{in}_{#1}^{#2}}
\newcommand{\inj}[1]{\keywd{in}_{#1}}
\newcommand{\einj}[1]{\keywd{in}_{#1}}
\newcommand{\fold}[2]{\keywd{fold}_{#1}\:{#2}}
\newcommand{\unfold}[1]{\keywd{unfold}\:{#1}}
\newcommand{\alloc}[1]{\keywd{ref}\:{#1}}
\newcommand{\deref}[1]{!{#1}}
\newcommand{\assign}[2]{{#1}:={#2}}
\newcommand{\throw}[1]{\keywd{raise}\:{#1}}
\newcommand{\catch}[3]{{#1}\:\keywd{handle}\:{#2}\Rightarrow{#3}}
\newcommand{\try}[5]{\keywd{try}\:{#1}\Leftarrow{#2}\:\keywd{handle}\:#3 . #4\:\keywd{in}\:{#5}}
\newcommand{\scatch}[2]{#1 \: \keywd{handle}\: #2}
\newcommand{\stry}[4]{\keywd{try}\: #1 \Leftarrow #2 \:\keywd{catch}\:#3\:\keywd{in}\:#4}
\newcommand{\mycase}[1]{\texttt{case}\: #1\:\texttt{of}}
\newcommand{\wrap}[1]{\texttt{wrap}\:{#1}}
\newcommand{\unwrap}[1]{\texttt{unwrap}\:{#1}}
\newcommand{\encap}[1]{\texttt{encap}\:{#1}}
\newcommand{\altloc}{k}
\newcommand{\locset}{L}
\newcommand{\alllocs}{\mathcal{L}}
\newcommand{\locsin}[1]{\mathrm{locs}({#1})}
\newcommand{\rdsin}[1]{\mathrm{rds}({#1})}
\newcommand{\crdsin}[1]{\mathrm{crds}({#1})}
\newcommand{\wrsin}[1]{\mathrm{wrs}({#1})}
\newcommand{\alsin}[1]{\mathrm{als}({#1})}
\newcommand{\select}[2]{{#1}|_{#2}}
\newcommand{\join}{\uplus}
\newcommand{\equalon}[1]{=_{#1}}
\newcommand{\ifz}[1]{\keywd{iszero}\:{#1}}
\newcommand{\myif}[3]{\keywd{if}\ #1\ \keywd{then}\ #2\
  \keywd{else}\ #3}
\newcommand{\myatomic}[1]{\keywd{atomic}(#1)}  
\newcommand{\cas}[3]{\keywd{cas}( #1, #2, #3)}
\newcommand{\myskip}{\keywd{skip}}
\newcommand{\mypar}[2]{#1 \| #2}
\newcommand{\semparallel}{~|~}
\newcommand{\await}[2]{\keywd{await}\ #1\ \keywd{then}\ #2}
\newcommand{\lett}{{\keywd{let}\ }}
\newcommand{\bee}{{\Leftarrow}}
\newcommand{\inn}{{\ \keywd{in}\ }}
\newcommand{\ttype}[2]{T_{#1}\,#2}
\newcommand{\eff}{\varepsilon}
\newcommand{\reads}{\mathrm{rds}}
\newcommand{\rds}{\reads}
\newcommand{\writes}{\mathrm{wrs}}
\newcommand{\wrs}{\writes}
\newcommand{\concs}[1]{\mathrm{cos}(#1)}
\newcommand{\locs}[1]{\mathrm{locs}(#1)}
\newcommand{\locwr}[1]{\mathrm{writes}(#1)}
\newcommand{\locsep}[2]{\mathrm{sep}(#1,#2)}
\newcommand{\allocs}{\mathrm{als}}
\newcommand{\sing}[1]{\{#1\}}
\newcommand{\erase}[1]{U(#1)}
\newcommand{\powerset}{\mathbb{P}}
\newcommand{\rels}[1]{\mathcal{R}_{#1}}
\newcommand{\toprel}[1]{\mathbb{T}_{#1}}
\newcommand{\botrel}[1]{\mathbb{F}_{#1}}
\newcommand{\diagrel}[1]{\Delta_{#1}}
\renewcommand{\topfraction}{.8}
\newcommand{\fota}{A^\circ}
\newcommand{\fotb}{B^\circ}
\newcommand{\readsg}[1]{\texttt{r}_{#1}}
\newcommand{\writesg}[1]{\texttt{w}_{#1}}
\newcommand{\storetyping}{\Sigma}
\newcommand{\obseq}{\approx}
\newcommand{\converges}{\Downarrow}
\newcommand{\mperp}{\bot}
\newcommand{\fullonly}[1]{}
\newcommand{\Rule}[4]{\ensuremath{\makebox[1cm]{\hfill {\rm #1}}
    \,\dfrac{#2}{#3}\,\,{#4}}}
\newcommand{\rrule}[5]{\ensuremath{\makebox[#1]{\hfill {\rm #2}}
    \,\dfrac{#3}{#4}\,\,{#5}}}
\newcommand{\Extend}[2]{{#1}\downharpoonright^{#2}}
\newcommand{\rrulesub}[7]{\ensuremath{\makebox[#1]{\hfill {\rm #2}}_{\makebox[#3]{\hfill {\rm #4}}} \,\dfrac{#5}{#6}\,\,{#7}}}
\newcommand{\Halfrule}[4]{\ensuremath{\makebox[0.5cm]{\hfill {\rm #1}}
    \,\dfrac{#2}{#3}\,\,{#4}}}
\newcommand{\RuleTwo}[4]{\ensuremath{\makebox[2cm]{\hfill {\rm #1}}
    \,\dfrac{#2}{#3}\,\,{#4}}}
\newcommand{\RuleTwofive}[4]{\ensuremath{\makebox[2.5cm]{\hfill {\rm #1}}
    \,\dfrac{#2}{#3}\,\,{#4}}}
\newlength{\lruleZeroname}
\setlength{\lruleZeroname}{0cm}
\newcommand{\RuleZero}[4]{\ensuremath{\makebox[\lruleZeroname]{\hfill {\rm #1}}
    \,\dfrac{#2}{#3}\,\,{#4}}}
\newcommand{\TODO}[1]{{TODO: \emph{#1}}}
\newcommand{\masked}{\ensuremath{\tau}}
\newcommand{\regs}[1]{\mathrm{regs}({#1})}
\newcommand{\domL}[1]{\ensuremath{\mathrm{dom}}(#1)}
\newcommand{\domR}[1]{\ensuremath{\mathrm{dom}'}(#1)}
\newcommand{\dompL}[1]{\ensuremath{\mathrm{priv}}(#1)}
\newcommand{\dompR}[1]{\ensuremath{\mathrm{priv}'}(#1)}
\newcommand{\Parms}{\mathrm{Par}}
\newcommand{\strels}[2]{\mathrm{StRel}({#1},{#2})}
\newcommand{\Env}{\mathrm{Env}}
\newcommand{\CEnv}{\mathrm{Env}_\mathbb{C}}
\newcommand{\PEnv}{\mathrm{Env}_\mathbb{P}}
\newcommand{\fix}{\mathrm{fix}}
\newcommand{\Mcnt}{M_{\mathrm{cnt}}}
\newcommand{\Mmem}{M_{\mathrm{mem}}}
\newcommand{\Mbuf}{M_{\mathrm{buf}}}
\newcommand{\Mconst}{M_{\mathrm{const}}}
\newcommand{\Vsum}{V_{\mathrm{sum}}}
\newcommand{\inv}[1]{\ensuremath{\mathrm{inv}(#1)}}
\newenvironment{myexample}{\vspace{1em}{\bf Example.}}{\qed \vspace{1em}}
\newcommand{\ok}[2]{\ensuremath{{#1} \vdash {#2}\ \mathit{ok}}}
\newcommand{\eq}[1]{\mathrel{\sim_{#1}}}
\renewcommand{\P}{\ensuremath{\mathcal{P}}}
\newcommand{\INT}{\ensuremath{\mathbb{Z}}}
\newcommand{\BOOL}{\ensuremath{\mathbb{B}}}
\newcommand{\UNIT}{\ensuremath{\mathbf{1}}}
\newcommand{\reg}{\ensuremath{\rho}}
\newcommand{\regid}{\ensuremath{\mathsf{r}}}
\newcommand{\Regids}{\ensuremath{\mathit{Regs}}}
\newcommand{\Locs}{\ensuremath{\labs}}
\newcommand{\new}{\ensuremath{\mathit{new}}}
\newcommand{\free}{\ensuremath{\mathit{free}}}
\newcommand{\lookup}{\ensuremath{\mathit{lookup}}}
\newcommand{\update}{\ensuremath{\mathit{update}}}

\newcommand{\Stores}{\ensuremath{\mathbb{H}}}
\newcommand{\sem}[1]{\ensuremath{\llbracket {#1} \rrbracket}}
\newcommand{\semC}[1]{\ensuremath{\llbracket {#1} \rrbracket}}
\newcommand{\semV}[1]{\ensuremath{\llceil {#1} \rrceil
}}
\newcommand{\csem}[1]{\ensuremath{\llbracket {#1}
    \rrbracket_\mathbb{C}}}
\newcommand{\psem}[1]{\ensuremath{\llbracket {#1} \rrbracket_\mathbb{P}}}
\newcommand{\nwrs}{{\ensuremath{\mathit{nwrs}}}}
\newcommand{\nwrL}{{\ensuremath{\mathit{nwrs}}}}
\newcommand{\nwrR}{{\ensuremath{\mathit{nwrs}'}}}
\newcommand{\elEffs}{\mathcal{E}}
\newcommand{\aEff}[1]{\ensuremath{\mathit{al}_{#1}}}
\newcommand{\rEff}[1]{\ensuremath{\mathit{rd}_{#1}}}
\newcommand{\cEff}[1]{\ensuremath{\mathit{co}_{#1}}}
\newcommand{\wEff}[1]{\ensuremath{\mathit{wr}_{#1}}}
\newcommand{\iEff}[1]{\ensuremath{\mathit{id}_{#1}}}
\newcommand{\EffExt}[3]{\ensuremath{\Omega^{#1}_{#2}({#3})}}
\newcommand{\Prels}[3]{\ensuremath{\mathcal{R}^{#1}_{#3}({#2})}}
\newcommand{\Ext}[3]{\ensuremath{\operatorname{Ext}}^{#1}_{#3}({#2})}
\newcommand{\Alloc}{\operatorname{Alloc}}
\newcommand\one{\ensuremath{\mathbf{1}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newcounter{Examplecount}
\setcounter{Examplecount}{0}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newcommand\im{\mathrm{Img}}

\newcommand{\loc}{\mathfrak{l}}
\newcommand{\Loc}{\mathfrak{k}}

\newcommand{\locV}[1]{\ensuremath{\mathfrak{#1}}\xspace}
\newcommand{\locList}{\ensuremath{\mathfrak{list}}\xspace}
\newcommand{\locListOdd}{\ensuremath{\mathfrak{listodd}}\xspace}
\newcommand{\locListEven}{\ensuremath{\mathfrak{listeven}}\xspace}
\newcommand{\locSet}{\ensuremath{\mathfrak{set}}\xspace}
\newcommand{\locInt}{\ensuremath{\mathfrak{int}}\xspace}
\newcommand{\locHi}{\ensuremath{\mathfrak{snd}}\xspace}
\newcommand{\locLo}{\ensuremath{\mathfrak{fst}}\xspace}
\newcommand{\locLazyOne}{\ensuremath{\mathfrak{lazy}(i)}\xspace}

\newcommand{\locMSQ}{\ensuremath{\mathfrak{msq}}\xspace}


\newcommand{\cloc}{\ensuremath{X}\xspace}

\newcommand\R{\ensuremath{\,R\,}\xspace}
\newcommand\E{\ensuremath{\,E\,}\xspace}


\newcommand\w{\ensuremath{\mathsf{w}}\xspace}
\newcommand\wext{\ensuremath{\mathsf{w}_e}\xspace}
\newcommand\q{\ensuremath{\mathsf{q}}\xspace}
\newcommand{\bij}{\ensuremath{\mathbb{B}}\xspace}
\newcommand{\world}{\ensuremath{\mathbf{W}}\xspace}
\newcommand{\worldIso}{\ensuremath{\mathbf{W}_{\mathrm{Iso}}}\xspace}
\newcommand\heap{\ensuremath{\mathsf{h}}\xspace}
\newcommand\hinit{\ensuremath{\mathsf{h}_{\mathit{init}}}\xspace}
\newcommand\h{\heap}
\renewcommand\k{\ensuremath{\mathsf{k}}\xspace}
\newcommand\heapp{\ensuremath{\mathsf{q}}}
\newcommand{\Values}{\mathbb{V}}
\newcommand{\ValuesB}{\mathbb{VB}}
\newcommand{\Comps}{\mathbb{C}}
\newcommand{\Astores}{\mathfrak{S}}
\newcommand{\bigsig}{\ensuremath{\mathbb{S}}\xspace}
\newcommand{\ah}{\ensuremath{\sigma}\xspace}
\newcommand{\taken}[2]{\ensuremath{#1\!\! \! \upharpoonright^#2}\xspace}

\newcommand\val{\ensuremath{\mathsf{a}}\xspace}
\newcommand\vval{\ensuremath{a}\xspace}
\newcommand\fval{\ensuremath{\mathsf{f}}\xspace}
\newcommand\ffval{\ensuremath{f}\xspace}
\newcommand\cval{\ensuremath{\mathsf{c}}\xspace}
\newcommand\ccval{\ensuremath{c}\xspace}

\newcommand\pull[2]{\ensuremath{\underline{#1}_{#2}}\xspace}
\newcommand\push[2]{\ensuremath{\overline{#1}_{#2}}\xspace}
\newcommand\rel{\ensuremath{\mathrm{Rel}}\xspace}

\newcommand\Get[2]{\ensuremath{\mathsf{get}_{#1}(#2)}\xspace}
\newcommand\Put[2]{\ensuremath{\mathsf{put}_{#1}(#2)}\xspace}

\newcommand\lGet{\ensuremath{\mathsf{get}}\xspace}
\newcommand\lPut{\ensuremath{\mathsf{put}}\xspace}


\newcommand\Cscr{\ensuremath{\mathcal{C}}\xspace}
\newcommand\Std{\ensuremath{\mathit{Std}}\xspace}
\newcommand\Rscr{\ensuremath{\mathcal{R}}\xspace}
\newcommand\Pscr{\ensuremath{\mathcal{P}}\xspace}
\newcommand\fst[1]{\ensuremath{\mathsf{fst}(#1)}\xspace}
\newcommand\Eq[1]{\ensuremath{\mathsf{Eq}(#1)}\xspace}
\newcommand{\readSet}[1]{\ensuremath{\mathcal{R}({#1})}}
\newcommand{\writeSet}[1]{\ensuremath{\mathcal{W}({#1})}}
\newcommand{\allocSet}[1]{\ensuremath{\mathcal{A}({#1})}}
\newcommand{\fr}[1]{\ensuremath{\mathsf{Fr}(#1)}\xspace}
\newcommand{\res}[2]{\ensuremath{\mathsf{Res}(#1,#2)}\xspace}
\newcommand{\myety}[4]{{#1}\mathrel{\&}{#2} \mid #3 \mid #4}
\newcommand{\ety}[2]{{#1}\mathrel{\&}{#2}}
\newcommand{\valty}[1]{#1}

\newcommand\inR[2]{\ensuremath{#1:#2}}
\newcommand\indep[1]{\ensuremath{\bot(#1)}}
\newcommand\indepB[2]{\ensuremath{#1 \bot #2}}

\newcommand\hext[2]{\ensuremath{#1 \supseteq #2}}
\newcommand\rloc[3]{\ensuremath{#1 \stackrel{#3}{\sim} #2}}
\newcommand\rrloc[3]{\ensuremath{#1 \stackrel{#3}{=} #2}}
\newcommand\gloc[3]{\ensuremath{#1 \xrightarrow{#3}#2}}

\newcommand\ie{\emph{i.e.}\xspace}
\newcommand\eg{\emph{e.g.}\xspace}
\newcommand\etal{\emph{et al.}\xspace}

\newcommand\mh[1]{\red{MH: #1}}

\newcommand\LLoc{\mathfrak{L}}
\newcommand\RegLoc{\mathfrak{r}}

\newcommand*{\corner}{\mbox{{}}}

\newcommand{\red}[1]{\textcolor{red}{#1} }
\newcommand{\blue}[1]{\textcolor{blue}{#1} }

\newcommand\lTo{\leftarrow}
\newcommand\rTo{\rightarrow}

\newcommand\su[1]{\ensuremath{supp(#1)}\xspace}

\newcommand\perm{\ensuremath{\pi}\xspace}
\newcommand\permSet{\ensuremath{perm}\xspace}

\newcommand{\tup}[1]{(#1)}



\usepackage{paralist}


\makeatletter
\makeatother



\title{Effect-Dependent Transformations for Concurrent Programs}
\authorinfo{Nick Benton}{Microsoft Research, Cambridge, UK}{nick@microsoft.com}
\authorinfo{Martin Hofmann}{LMU, Munich, Germany}{hofmann@ifi.lmu.de}
\authorinfo{Vivek Nigam}{UFPB, Jo\~ao Pessoa, Brazil}{vivek.nigam@gmail.com}

\begin{document}
\maketitle
\begin{abstract}  
We describe a denotational semantics for an abstract effect
system for a higher-order, shared-variable concurrent programming
language. We prove the soundness of a number of general effect-based
program equivalences, including a parallelization equation that
specifies sufficient conditions for replacing sequential composition
with parallel composition. Effect annotations are relative to abstract 
locations specified by contracts rather than physical footprints allowing us 
in particular to show the soundness of
some transformations involving fine-grained concurrent data structures, such as
Michael-Scott queues, that allow concurrent access to different parts
of mutable data structures.

Our semantics is based on refining a trace-based semantics for
first-order programs due to Brookes. By moving from concrete to
abstract locations, and adding type refinements that capture the
possible side-effects of both expressions and their concurrent
environments, we are able to validate many equivalences that do not
hold in an unrefined model.  The meanings of types are expressed using
a game-based logical relation over sets of traces. Two programs 
and  are logically related if one is able to solve a two-player
game: for any trace with result value  in the semantics of 
(challenge) that the player presents, the opponent can present an
(response) equivalent trace in the semantics of  with a logically related result value .

\end{abstract}
\squelch{
\category{F.3.2}{Logic and Meanings of Programs}{Semantics of Programming Languages -- Denotational semantics, Program analysis}
\category{F.3.2}{Logic and Meanings of Programs}{Studies of Program Constructs -- Type structure}
\terms
Languages, Theory
\keywords
Type and effect systems, region analysis, logical relations, parametricity, program transformation
}
\noindent
\section{Introduction}
\label{sec:intro}
Type-and-effect systems refine conventional types with extra
static information capturing a safe upper bound on the possible
side-effects of expression evaluation. Since their introduction by
Gifford and Lucassen \cite{DBLP:conf/lfp/GiffordL86},
effect systems have been used for many purposes, including
region-based memory management \cite{birkedaltoftevejlstrup}, tracking
exceptions \cite{pessauxleroy,bentonbuchlovsky}, communication
behaviour \cite{amtoftnielsons} and atomicity
\cite{flanaganqadeerpldi03} for concurrent programs, and information
flow \cite{brobergsands:flowlocks}. 

A major reason for
tracking effects is to justify program
transformations, most obviously in optimizing compilation \cite{DBLP:conf/icfp/BentonKR98}. For example,
one may remove computations whose results are
unused, \emph{provided} that they are sufficiently pure, or commute two
state-manipulating computations, \emph{provided} that the locations they may
read and write are suitably disjoint. Several groups 
have recently studied the semantics of effect systems, with a focus on
formally justifying such effect-dependent equational reasoning \cite{DBLP:conf/popl/KammarP12,DBLP:conf/aplas/BentonKHB06,DBLP:dblp_conf/popl/Benton0N14,birkedal,DBLP:conf/icfp/ThamsborgB11}. A common approach, which we follow here, is to
interpret effect-refined types using a
logical relation over the (denotational or operational) semantics of
the unrefined (or untyped) language, simultaneously identifying both the
subset of computations that have a particular effect type and a
coarser notion of equivalence (or approximation) on that subset. Such
a semantic approach decouples the meaning of effect-refined types from
particular syntactic rules: one may establish that a term
has a type using various more or less approximate inference systems, or
by detailed semantic reasoning. 

For sequential computations with global state,  denotational
models already provide significant
abstraction. For example, the denotations of \verb|skip| and
\verb|X++;X--| are typically equal, so it is immediate that the
second is semantically pure. More generally, 
the meaning of a
judgement  guarantees that the result of
evaluating  will be of type  with side-effects at most ,
under assumptions  (a `rely' condition), on the behaviour of
's free variables. The possible interaction points
between  and its environment are restricted to initial states and
parameter values, and final states and results, of  itself and its
explicitly-listed free variables. Furthermore, all those interaction
points are visible in the term and are governed by specific
annotations appearing in the typing judgement.

For shared-variable concurrency, there are many more possible
interactions. An expression's environment now also includes anything
that may be running concurrently and, moreover, atomic steps of 
and its concurrent environment may be arbitrarily interleaved, so it
is no longer sufficient to just consider initial and final states. A
priori, this leads to far fewer equations between programs. For
example, \verb|X++;X--| may be distinguished from \verb|skip| by being
run concurrently with a command that reads or writes \verb|X|. But few
programs do anything useful in the presence of unconstrained
interference, so we need ways to describe and control
it. 
Fine-grained, optimistic algorithms, which rely on
custom protocols being followed by multiple threads with concurrent
access to a shared data structure, can significantly
outperform ones based on coarse-grained locking, but are notoriously
challenging to write and verify.

There is a huge literature on shared-variable
concurrency, from type systems ensuring race-freedom of programs with
locks \cite{abadi:typessafelock} to sophisticated semantic models for
reasoning about refinement of fine-grained concurrent datastructures
\cite{dreyer}. This paper explores effect types as a
straightforward, lightweight interface language for modular reasoning
about equivalence and refinement, e.g. for safely transforming
sequential composition into parallelism.  We show how the semantics of
a simple effect system scales smoothly to the concurrent setting,
allowing us to control interference and prove non-trivial
equivalences, extending (somewhat to our surprise) to the correctness
of some fine-grained algorithms.

We build on a trace semantics for concurrent programs, due to Brookes
\cite{brookes96ic}, which explicitly describes possible interference
by the environment. We extend Brookes's semantics to a higher-order
language and then refine it by a semantically-formulated effect system
that separately tracks: (1) the store effects of an expression during
evaluation; (2) the assumed effects of transitions by the environment;
and (3) the overall end-to-end effect.  
Rather than tracking effects at
the level of individual concrete heap cells, we view the heap as a set
of abstract data structures, each of which may span several locations,
or parts of locations \cite{DBLP:dblp_conf/popl/Benton0N14}. Each abstract location has its own notion of
equality, and its own notion of legal mutation. Write effects, for
example, need only be flagged when the equivalence class of an
abstract location may change. Both typing and refinement judgements may be established by a combination of generic type-based rules and semantic reasoning in the model.













\label{examples}
We begin with some motivating examples. 

\paragraph{Equivalence modulo non-interference:}
Our semantics justifies the following equation \emph{at} the  effect type 
:

This says that the two commands are equivalent with return type \texttt{unit}, exhibit the effect , signifying concurrent or `chaotic' access to  along the way, and have an overall end-to-end effect of  plus reading and writing , \emph{provided} that the effect, , of the concurrent environment does not involve .
\nopagebreak

\paragraph{Overlapping References:}
Let , implement a bijection , and consider the following functions:

which multiplex two abstract integer references onto a single concrete one. Note that the write functions,  and , use compare-and-swap,  , to atomically update the value of the reference. 



Our generic rules then say that a program, , that only reads and/or writes one abstract  reference can be commuted, or executed in parallel, with another program, , that only reads and/or writes into a different reference. This lets one use types to, say, justify parallelizing a call to  followed by one to , even though they read and write the same concrete location, which looks like a race.


\paragraph{Version numbers:}
\newcommand{\clocver}{{\cloc}_{\textrm{ver}}}
\newcommand{\clocval}{{\cloc}_{\textrm{val}}}
One can isolate a transaction that reads and then writes a piece of state simply by enclosing the whole thing in . A more concurrent alternative adds a monotonic version number to the data. A transaction then works on a private copy, only committing its changes back (and incrementing the version) if the current version number is the same as that of the original copy. We can define an abstract integer reference  in terms of two concrete ones,  and , governed by a specification that says   may only change when  increases. We define 

Under the assumption that  is a pure function (has effect type  for any ), we can show

at type  for any  not including chaotic access, , to . The environment effect  here \emph{may} include reading and writing , so concurrent calls to  are linearizable. 

\paragraph{Loop Parallelization:}
Our next example is inspired by a loop unrolling optimization~\cite{DBLP:conf/popl/TristanL10}. Assume given a linked list of integers pointed by . Consider the following functions: 
2pt]
\mathsf{map2Par}~f = & \keywd{let \ rec }~\mathsf{applyf2}~n = \\ 
  & \quad \blue{n.ele := f(n.ele) ~ {\underline{~\|~}} ~ {n.next.ele := f(n.next.ele)}}; \\
  & \quad \keywd{if}~n.next.next = null~\keywd{then}~\unitval\\
  & \quad \keywd{else}~\keywd{if}~n.next.next.next = null~\keywd{then} \\ 
  & \qquad \quad n.next.next.ele = f(n.next.next.ele)\\
  & \qquad \keywd{else}~ \mathsf{applyf2}~(n.next.next)\\
  & \keywd{in}~ \keywd{if}~!head = null~\keywd{then}~\unitval \\
  & \quad~ \keywd{else}~\keywd{if}~!head.next = null~ \keywd{then}\\
  & \qquad  !head.next.ele := f(!head.next.ele)\\
  & \quad ~ \keywd{else}~\mathsf{applyf2}~(!head) 
\end{array}

  \begin{array}{ll}
      \mathsf{dequeue}~ () = & \keywd{let\ rec}~ \mathsf{try}~() = \\&
   \quad \keywd{let}~ n_0 = \myread{head} ~\keywd{in}~
   \keywd{if}~\myread{n_0}.next = null~ \keywd{then}~ null~ 
   \\ & \qquad \keywd{else}~ \keywd{let}~ n_1 = \myread{n_0}.next ~\keywd{in}~
   \\ & \quad \qquad \keywd{if}~\cas{\myread{head}}{n_0}{n_1}~\keywd{then}~\myread{n_1}.ele~ \keywd{else}~ \mathsf{try}~() \\&
   \keywd{in}~\mathsf{try}~() \ 
\caption{Enqueue and Dequeue programs for a Michael-Scott Queue at location .}
 \label{fig:deqenq}
 \vspace{-3mm}
 \end{figure}


 \paragraph{Michael-Scott Queue:} The Michael-Scott
 Queue~\cite{michael-scott} (MSQ) is a fine grained concurrent data
 structure, allowing threads to access and modify
 different parts of a queue safely and simultaneously. We present a version like that of Turon et al  \cite{dreyer}, which is an idealized version of the MSQ, without a tail pointer. 
 
 An MSQ maintains a pointer  to a non-empty linked list as depicted in Figure~\ref{fig:MSQ}. The first node, the node containing the element  in the figure, is not an element of the queue, but is a ``sentinel''. Hence the queue in the figure holds . 
 
 The enqueue and dequeue operations are defined in Figure~\ref{fig:deqenq} and illustrated in Figure~\ref{fig:MSQ}.
 Elements are dequeued from the beginning of the linked list, and enqueued at the end, which involves a traversal that is done without locking.
Once the end, , of the linked list is found, the program atomically attempts to insert the new element. This is necessary because other programs may have enqueued elements to the end of the list, meaning that  is no longer the end of the list. 
   
 The dequeue operation should move the  pointer from the current
 sentinel, , to the following element . However, as other
 programs may also be attempting to dequeue an element, we use
 compare-and-swap to atomically update  the  pointer if 
 still points to the same sentinel. Notice that the dequeued elements
 can still reach the sentinel of the queue. (In Figure~\ref{fig:MSQ},
 these are the nodes containing .)  This is
 necessary because there might be other (slower) threads that want to
 enqueue an element and are still searching for the end of the list
 by traversing the portion of the queue that has already been
 dequeued. 


 We prove  that the enqueue and dequeue of Figure~\ref{fig:deqenq} are equivalent to  and , their atomic versions 
which perform all operations in a single step, at a type that allows the environment to be concurrently reading and writing the queue.
So the fine-grained MSQ behaves like a synchronized queue, as might also be implemented using locks.
\section{Syntax}
\label{sec:syntax}
In this section we define the syntax of a metalanguage for concurrent, stateful
computations and higher-order functions. 
Communication between parallel computations is via a shared heap
mapping dynamically allocated locations to structured values, which
include pointers. To keep the model simple, we do not allow functions
to be stored in the heap (no higher-order store).



\paragraph{Memory model}
We assume a countably infinite set  of physical locations
 and a set  of
``R-values'' that can be stored in those references including
integers, booleans, locations, and tuples of R-values, written .  We assume that it is possible to tell of which form a
value is and to retrieve its components in case it is a tuple.  A heap , then, is a \emph{finite map} from  to , written
, specifying that the value stored in location  is . We write  for the domain of  and
write  for the heap that agrees with 
except that it gives the variable  the value . The set
of heaps is denoted by .  We also assume that 
yields a pair  where  is a fresh
location and  is . 


\paragraph{Syntax of expressions}
The syntax of untyped values and computations is:

Here,  ranges over variables,  over R-values, and 
over built-in functions, which include arithmetic, testing
whether a value is an integer, function, pair or reference,
equality on simple values, etc. Each  has a corresponding semantic
partial function , so for example  for integers .

The construct  defines a recursive function with body
 and recursive calls made via ; we use  as
syntactic sugar in the case when 
 is not free in . Next, 
(reading) returns the contents of location , 
(writing) updates location  with value , and 
(allocating) returns a fresh location initialized with . The
metatheory is simplified by using ``let-normal form'', in which the
only elimination for computations is , though we sometimes nest
computations as shorthand for let-expanded versions in examples.
 
The construct  is evaluated by arbitrarily
interleaving evaluation steps of  and 
until each has produced a value, say  and ; the result is then
. Assignment, dereferencing and allocation are atomic, but
evaluation of nested expressions is generally not.
To enforce atomicity, 
 evaluates an arbitrary  in one step, without any environmental interference.
One can then define a (more realistic) compare-and-swap operation
:

this atomically both checks if location  contains  and, if so,
replaces it with  and returns ; otherwise the location is unchanged and the returned value is .


We define the free variables, , of a term, closed terms, and
the substitution  of  for  in , in the usual
way. Locations may occur in terms, but the type system will
constrain their use.






\section{Denotational Model}\label{values}
We now sketch a denotational semantics for our metalanguage based on
Brookes' trace semantics \cite{brookes96ic}. 
Fuller details can be found in a technical report (attached), which in
particular establishes computational adequacy of the model with
respect to a small-step operational semantics using interleaving.

\subsection{Preliminaries}
A \emph{predomain} is an -cpo, \ie, a partial order with
suprema of ascending chains.  A \emph{domain} is a predomain with a
least element, .  Recall that  is
\emph{continuous} if it is monotone  and preserves suprema of chains, \ie, . Any set is a predomain with the discrete order (flat
predomain). If  is a set and  a predomain then any
 is continuous. We denote a partial (continuous)
function from set (predomain)  to set (predomain)  by .
If  are predomains the cartesian product  and the set
of continuous functions  form themselves predomains
(with the obvious componentwise and pointwise orders) and make the
category of predomains cartesian closed. Likewise, the partial continuous functions  between predomains  form a domain. 

If  and  are subsets of predomains  and
 we define  and
 in the usual way. We may
write  for .  

A subset  is \emph{admissible} if whenever  is
an ascending chain in  such that  for all , then
, too. If  is continuous
and  is a domain then one defines 
with . One has,  and
if  is admissible and contains  and  then , too.  An element 
of a predomain  is \emph{compact} if whenever 
then  for some . E.g.\ in the domain of partial
functions from  to  the compact elements are
precisely the finite ones.  A continuous partial function  is a \emph{retract} if  and  hold for all
. In short:  and . If, in
addition,  has a finite image then  is called a \emph{deflation}
\cite{Abramsky94domaintheory}. Note that if  is a retract then 
and if  then . We also note that if  is in the
image of a deflation then  is compact.
 
We define the usual state monad on predomains, by taking . 


\begin{definition}
Let  be a subset of a predomain . Then  is
the least admissible superset of . Concretely,  iff there exists a chain  such that  for all  and .
\end{definition}
\begin{lemma}\label{funad}
If  is continuous;  are
arbitrary subsets and  is admissible then
 implies
.
\end{lemma}
\begin{lemma}\label{func}
Let  be predomains and let  be a chain of retracts on
 such that  is compact for each  and  and  implies  for all .
Then .
\end{lemma}

\subsection{Traces}
A trace models a terminating run of a concurrent computation as
a sequence of pairs of heaps, each representing
pre- and post-state of one or more atomic actions. The semantics of a
program then is a (typically large) set of traces (and
final values), accounting
for all possible environment interactions. 


\begin{definition}[Traces]
A trace is a finite sequence of the form  where for , we have   and . We write  for the set of traces. 
\end{definition}

Let  be a trace. A trace of the form  where  is said to
arise from  by stuttering. A trace of the form  where
 is said to arise from t by mumbling. For example, if
 then  
arises from 
by stuttering. In the case where  the trace 
arises from  by mumbling. A set of traces  is closed under stuttering and
mumbling if whenever  arises from  by stuttering or mumbling and
 then , too. 

Brookes~\cite{brookes96ic} gives a fully-abstract semantics for while-programs
with parallel composition using sets of traces closed under stuttering
and mumbling. We here extend his semantics to higher-order functions
and general recursion.

\begin{definition}[Trace Monad]
\label{def:monad}
Let  be a predomain. Elements of the domain  
  are sets  of pairs  where  is a
 trace and  such that the following properties are satisfied:
\begin{itemize}
\item \textit{[S\&M]}: if  arises from t by stuttering or mumbling and
  then . 
\item \textit{[Down]}: if  and  then
  .
\item \textit{[Sup]}: if  is a chain in  and  for all  then  . 
\end{itemize}
The elements of  are partially ordered by inclusion.  
\end{definition}
\begin{lemma}
If  is a predomain then  is a domain. 
\end{lemma}
An element  of  represents the possible outcomes of a
nondeterministic, interactive computation with final result in
. Thus, if  for 
then there could be  interactions with the
environment with heaps  being ``played'' by the
environment and ``answered'' with heaps  by the
computation. After that, this particular computation ends and  is
the final result value.

For example, the semantics of 
 
contains many traces, including the following, where we write  for
the heap in which  has value :

\begin{tabular}{l}
, \\
,\\
,\\
,\\

\end{tabular}

Axiom [S\&M] is taken from Brookes. It ensures that the semantics does
not distinguish between late and early choice \cite{dreyer} and
related phenomena which are reflected, e.g., in resumption semantics
\cite{plotkin76siam}, but do not affect observational equivalence.
Note that non-termination is modelled by the empty set, so we are
working with an `angelic' notion of equivalence
(`may semantics'
\cite{DBLP:dblp_conf/icalp/NicolaH83}). For example, the semantics of
 
is the same as that of  and contains, for example
 but also (stuttering)
.  Note that it is not possible to
tell from a trace whether an external update of  has happened
before or after the reading of .

Let us also illustrate how traces iron out some intensional
differences that show up when concurrency is modelled using transition
systems or resumptions. Consider the following two programs where
 denotes a nondeterministically chosen boolean value.

Both  and  admit the same traces, namely
 and  and stuttering variants
thereof.  In semantic models based on transition systems or
resumptions and bisimulation, these are distinguished, which
necessitates the use of special mechanisms such as history and
prophecy variables \cite{DBLP:journals/tcs/AbadiL91}, forward-backward
simulation \cite{DBLP:dblp_journals/iandc/LynchV96}, or speculation
\cite{dreyer} in reasoning.


Axioms [Down] and [Sup] are known from the Hoare powerdomain
\cite{plotkin76siam}. Recall that the Hoare
powerdomain  contains the subsets of  which are downclosed
([Down]) and closed under suprema of chains ([Sup]). Such subsets are
also known as Scott-closed sets.  Thus,  is the restriction of
 to the sets closed under stuttering and
mumbling. Axiom [Down] ensures that the ordering is indeed a partial
order and not merely a preorder. 
Additional nondeterministic outcomes that are less
defined than existing ones are not recorded in the semantics.

\begin{definition}
If  then  is the
least subset of  containing , i.e.\  is the closure
of  under [S\& M], [Down], [Sup].
\end{definition}

\begin{definition}
Let  be a predomains.  We define the continuous
functions  and  by:

\end{definition}
\noindent
These endow  with the structure of a strong monad. The continuous function
 is defined by:

If  are traces, we write  to
mean that  can be obtained by interleaving  and  in
some way, i.e.,  is contained in the shuffle of  and .
In order to model parallel composition we introduce the following
helper function

The continuous map  is defined by:

\noindent
Notice that due to mumbling  iff
there exists an element  where  and
. The presence of such an element, however, models an atomic
execution of the computation represented by .


\subsection{Semantic values}
The predomain  of untyped values
is the least solution of the following domain equation: 

That is, values are either R-values, continuous functions from values to computations (), or tuples of values.
We tend to identify the summands of the right hand side with subsets
of  but may use tags like  when
 to avoid ambiguity.


We have families of deflations  and , referred to as canonical deflations, so
that  and  are ascending chains converging to the
identity. The definition is entirely standard and may be found in the
accompanying material. It shows in particular that  and
 are \emph{bifinite} (equivalently SFP) (pre-)domains
\cite{Abramsky94domaintheory} and as such also Scott (pre-)
domains. The presence of these deflations allows us to apply
Lemma~\ref{func} and simplifies reasoning in general.

The semantics of values  and
terms  are given by the
recursive clauses in Figure~\ref{seme}. Environments, , are properly tuples of values; we abuse notation slightly by treating them as maps from variables, , to values, , (and write  for functional update) to avoid mentioning an explicit context in which untyped terms are well-formed.




\begin{figure*}[tph]
\vspace{-10mm}

\caption{Denotational semantics \label{seme}}
\end{figure*}
\section{Abstract Locations}
We build on the concept of abstract locations defined by Benton,
Hofmann, and Nigam \cite{DBLP:dblp_conf/popl/Benton0N14}. These allow
complicated data structures that span several concrete locations,
or only parts of them, to be a regarded as a single ``location'' that
can be written to and read from. Essentially, an abstract location is
given by a partial equivalence relation on heaps modelling well-formedness and 
 equality together with a transitive relation modelling allowed
modifications of the abstract location. Abstract locations then allow
certain commands that modify the physical heap to be treated as 
read-only or even pure if they respect the contracts. Abstract locations are
related to \emph{islands} \cite{DBLP:conf/popl/AhmedDR09} which also
  allow one to specify heap allocated data structures and use
  transition systems for that purpose. An important difference is that
  abstract locations do not require physical footprints in the form of
  sets of concrete locations.

Due to the absence of dynamic allocation at the level of abstract
locations in the present paper, we can slightly simplify the original definition 
\cite{DBLP:dblp_conf/popl/Benton0N14}, dropping those axioms that involve the interaction with dynamic allocation.\footnote{Though our examples do all satisify these axioms, leaving the way open to a future extension with dynamically allocation of abstract locations and concurrency.}
On the other hand, in the presence of concurrency, we need \emph{two}
partial equivalence relations: one that  models semantic
equivalence and well-formedness and a finer one that constrains the
heap modifications that other concurrent computations that are
independent of the given abstract locations are allowed to do
\emph{while} an operation on the abstract location is ongoing, but
temporarily preempted.

\begin{definition}[Concurrent Abstract Location]\label{absloc}
  A \emph{concurrent abstract location}  consists of the following data:

(1) a partial equivalence relation   on
   modeling the ``semantic equivalence'' on the bits of the
  store that  uses.
If  then the same computation started on  and , respectively, will yield related or even equal results.
 
(2) a partial equivalence relation   on
   refining  and modeling the ``strict equivalence'' on the bits of the
  store that  uses. 
If a concurrent computation on  has reached  and is preempted, then another computation may replace  with  where  and then the original computation on  may resume on  without the final result being compromised. 

(3) a transitive  (and reflexive on the support of ) 
 relation  modeling how exactly the
  heap may change upon writing the abstract location and in particular
  what bits of the store such writes leave intact. In other words, if
   then  might arise by writing
  to  in  and all possible writes are specified by
  . We call  the \emph{step relation} of . 

In addition, we require the following 
conditions where  stands for .
\begin{enumerate}
  \item If  then ;
  \item if  then  and .
\end{enumerate}
If  and at the same time
, then we say that  arises from  by a \emph{silent move} in . Our semantic framework will permit silent
moves at all times.
\end{definition}
We now introduce some examples of abstract locations. 


\label{sec:abs-examples}
\paragraph{Single Integer} For our simplest example, consider the following abstract location parametric with respect to concrete location  as follows:

Two heaps are semantically equivalent (w.r.t.\  that
is) if the values stored in  are integers and equal; the step
relation requires all other concrete locations to be unchanged.

We will sometimes abuse notation and write  for . 

\paragraph{Overlapping references}
Let  be a concrete location encoding a pair of integer values using a bijection .
We define the abstract location  as below. We omit  which is similar, but only looks at the second projection, instead of the first. 
2pt]



\rrloc{\h}{\h'}{\locLo(\cloc)} \iff  \rloc{\h}{\h'}{\locLo(\cloc)} \2pt]
  
\end{array}

\hspace{-4mm}
 \begin{array}{ll}
 \rloc{\h}{\h'}{\mathfrak{X}}
  \iff &   \h(\cloc_{Val}) = \h'(\cloc_{Val})\\
\rrloc{\h}{\h'}{\mathfrak{X}} \iff&  \rloc{\h}{\h'}{\mathfrak{X}}\\
    \gloc{\h}{\h_1}{\mathfrak{X}} \iff& \forall \cloc' \notin \{\cloc_{Ver},\cloc_{Val}\}. \h(\cloc') = \h_1(\cloc') ~ \land~\\
    & \inR{\h}{\mathfrak{X}} \land \inR{\h_1}{\mathfrak{X}} \land \h(X_{Ver}) <= \h_1(X_{Ver})~  \land~ \\
    &[\h(X_{Val}) \neq \h_1(X_{Val}) \Rightarrow \h(X_{Ver}) < \h_1(X_{Ver})]
 \end{array}

\hspace{-4mm}
 \begin{array}{ll}
 \rloc{\h}{\h'}{\locListEven(\cloc)}
  \iff &   L(\cloc,\h) \land L(\cloc,\h') \land L(\cloc,\h).len = L(\cloc,\h').len ~\land\ \\ 
  & L(\cloc,\h)[2i] = L(\cloc,\h')[2i]\\ 
  &  \textrm{for } 0 \leq i \leq \lfloor L(\cloc,\h).len / 2 \rfloor
  \\
\rrloc{\h}{\h'}{\locListEven(\cloc)} \iff&  \rloc{\h}{\h'}{\locListEven(\cloc)}\\
    \gloc{\h}{\h_1}{\locListEven (\cloc)} \iff& 
  \inR{\h}{\locListEven (\cloc)} \land \inR{\h_1}{\locListEven (\cloc)}~ \land \\
  & L(\cloc,\h) \land L(\cloc,\h_1) \land \textrm{for } 0 \leq i \leq \lfloor L(\cloc,\h).len / 2 \rfloor\  \\
  & \quad L(\cloc,\h)[2i+1] = L(\cloc,\h_1)[2i+1]~ \land \\ 
  & \quad L(\cloc,\h)[2i].next = L(\cloc,\h_1)[2i].next~\land \\
  & \forall \cloc' \notin L(\cloc,\h).locs. \h(\cloc') = \h_1(\cloc)
 \end{array}

 \begin{array}{ll}
  \h,\cloc\stackrel{\mathit{next}}{\to} \cloc' \iff & \cloc' \textrm{ can be reached 
  from  in } \\ &
  \textrm{by following a chain of next pointers}  
 \end{array}
 
 \hspace{-3mm}
  \begin{array}{l@{\quad}l}
   \h(\cloc).head = \cloc_0 &
   \h(\cloc_i).elem = v_i \textrm{ for }\\
   \h(\cloc_i).next = \cloc_{i+1}  \textrm{ for }&
   \h(\cloc_n).next = null
  \end{array}
 
\textit{fp}(\cloc,\h)=\{\cloc'\mid \cloc\stackrel{\mathit{next}}{\to} \cloc'\vee\cloc'\stackrel{\mathit{next}}{\to} \cloc\}

 \begin{array}{lcl}
  \rloc{\h}{\h'}{\locMSQ(\cloc)} &\iff&  \exists \vec{X}\ \vec{X'}\ \exists \vec{v}.\textit{List}(\cloc,\h,\vec{X},\vec{v}) \land \textit{List}(\cloc,\h',\vec{X'},\vec{v})\1pt]
\gloc{\h}{\h_1}{\locMSQ(\cloc)} &\iff& \inR{\h}{\locMSQ(\cloc)}\wedge
\inR{\h_1}{\locMSQ(\cloc)}\wedge \textit{step}^*(\h,\h_1)\

In all of these examples, the only silent moves are identity moves.  This is not so in the examples from \cite{DBLP:dblp_conf/popl/Benton0N14} which contained  data-structures that would reorganize during lookups and also patterns like late initialisation. 
\subsection{Worlds}
We will group the abstract locations used to describe a program into a
\emph{world}. In this paper we do not model dynamic evolution of
worlds; all abstract locations ever used must be set up upfront. While
allocation of concrete locations may happen to increase a data structure modelled by
an abstract location, e.g.\ in the Michael-Scott Queue example, no new
such datastructures can appear. It is possible, however, to extend our
work in this direction by using (proof-relevant) Kripke logical
relations
\cite{DBLP:dblp_conf/popl/Benton0N14,DBLP:conf/popl/AhmedDR09}.
\begin{definition}[world]
A \emph{world} is a set of abstract locations. 

The relation  (heap  satisfies world ) is defined as the largest relation such that  implies 
\begin{itemize}
\item \inR{\h}{\loc} for all ; 
\item if  and  then 
 holds for all 
 with  and . 
\end{itemize}
\end{definition}
The original account of abstract locations
\cite{DBLP:dblp_conf/popl/Benton0N14} also has a notion of
independence of locations which facilitates reasoning in the presence
of dynamic allocation, and in particular permitted relocation of
abstract locations. Since we are not currently treating dynamic
allocation of abstract locations, we can avoid this notion here.

We remark that if our world  contains two obviously ``dependent'' abstract locations, e.g.\ has  both an integer location and a boolean location
placed at the same physical location, then there will be no heap  such that . 

We assume a fixed \emph{current} world  which may appear in definitions without being notationally reflected. See also Assumption~\ref{assi}.
\section{Effects}
For each abstract location  we have three elementary effects
 (reading from ),  (writing to ),
and  (chaotic or concurrent access). The chaotic access is
similar to writing, but allows writes that are not in sync. For
example,  and  both have
individually the  effect, but  and  are
distinguishable with a context that assumes the
-effect. Thus,  and  are not equal ``at type''
. At type  they are, however, equal, because a
context that copes with this effect may not assume that both produce
equal results.

We use the  effect to tell the environment not to look at
a particular location during a concurrent computation. For example, we
will be able to show that  is equivalent to  ``at
type''  whenever . This means that
the two computations are indistinguishable by environments that do not
read, let alone modify  during the computation and assume regular
read-write access once it is completed.
It would alternatively be possible to replace the -effect
using a special set of private locations akin to the private regions
from \cite{birkedal}.

We use the notation , ,  to
refer to the abstract locations  for which  contains
, , and , respectively. We write
.  We also write
 for  with all read effects removed and each  in  replaced by
.

\begin{definition}
An effect  is well-formed (with respect to the current world) if   and  and . 
An effect specification is a triple  of well-formed effects such that . 
\end{definition}
An effect specification  approximates the
behaviour of a computation  in the following way: the effect
 summarizes side effects that may occur during the execution
of  (corresponding to a guarantee condition in the rely-guarantee
formalism \cite{DBLP:journals/logcom/ColemanJ07}); the effect  summarizes effects of the interacting
environment that  can tolerate while still functioning as expected
(corresponding to a rely condition). Finally, 
summarizes the side effects that may occur between start and completion of . All the effects that the
environment might introduce must be recorded in  because they are
not under ``our'' control and might happen at any time even as the
very last thing before the final result is returned. The effects
flagged in , on the other hand, do not necessarily show up in
, for a computation might be able to clean up those effects
prior to returning the final result. The requirement that  is owed to the fact that all effects should preserve their own precondition, however the precondition of  is agreement on  which is not preserved by . The requirement  reflects the fact that  includes  as a special case. 

Note that if  is a (well-formed) effect, then it
is the case that . We will use this observation to simplify some side
conditions.

In our concrete examples, we abbreviate  by just , in other words, the chaotic effect silently implies the write effect.

Consider the computations  and .  Let  stand for
 and analogously .  Each of the two
computations can be assigned the effect , but they are distinguishable at that effect typing. Under
the looser specification , however, they are indistinguishable, and our semantics is
able to validate this equivalence, see Example~\ref{jife}.

Finally, consider the program  that simply reads a location storing an integer. We can show that this program has type , where the read effect on  is only in the global effects. 
 
\paragraph{Notations.}
For any well-formed effects  we use the notation
 to mean that . Note that this implies in particular
, etc. Intuitively, two
programs exhibiting effects  and , respectively, commute
with each other.  We write  to mean
 for each .  We write
 for the transitive closure of
. Thus,
 allows steps by locations recorded as writing in
 and silent steps by all locations in the current world.

We define the notation  which appears in the parallel congruence rule by

\section{Typing and congruence rules}

Types are given by the grammar 
 
where  ranges over user-specified abstract types. They will typically 
 include reference types such as 
and also types like lists, sets, and even
objects. In  the triple of 
effects  must be an effect specification. 

We use two judgments:
\begin{itemize}
\item  specifying that values  and  have
 type  and that  approximates ,
 \item  specifying that the programs  and  
under the context  have type , with the effect specification  specifying, respectively, the effects during execution, the effects of the interacting environment and the start and completion effects. Moreover,  approximates  at this specification. 
\end{itemize}

We assume an ambient set of \emph{axioms} each having the form
 where  are values in the metalanguage and 
is a type meaning that  and  are claimed to be of type 
and that  approximates . This must then be proved ``manually''
using the semantics rather than using the rules. The

We also define typing judgements  and
 which denote the
special case when  and  can be derived from the rules
from Figure~\ref{tycrule}. We do not formulate explicit typing rules
to save space.

The plan is to justify all the rules semantically using a logical relation (Section~\ref{logrel}) and to then conclude their soundness w.r.t.\ typed observational appoximation and equivalence (Section~\ref{obseq}). 

The parallel composition rule states that two programs  and 
can be composed when their internal effects are not conflicting in the
sense that the internal effects of one program appear as environment
interaction effects of the other program. Note the relationship to the
parallel composition rule of the rely-guarantee formalism
\cite{DBLP:journals/logcom/ColemanJ07}. Also note that the effects of
computations  and  are not required to be independent from
each other as we do in the parallization rule further down.

 The appearance of the
-operation deserves special mention. It might be, for example,
that  modifies  on the way, thus 
but cleans up this modification by eventually restoring the old value
of . This would be reflected by
. In that case, we would
not expect to see  in the end-to-end effect of the
parallel composition and that is precisely what  achieves.

The rules labelled (Sem) make available all kinds of program
transformations that are valid on the level of the \emph{untyped}
denotational semantics, including commuting conversions for let and
if, fixpoint unrolling, and beta and eta equalities. 

Finally, we have several effect-dependent (in)equalities: the
parallelization rule generalises a similar rule from
\cite{birkedal}. The other ones are concurrent version of analogous
rules for sequential computation that have been analysed in previous
work
\cite{DBLP:conf/aplas/BentonKHB06,benton07ppdp,DBLP:conf/icfp/ThamsborgB11,DBLP:dblp_conf/popl/Benton0N14}
and are at the basis of all kinds of compiler optimizations.  The side
conditions on the effects are rather subtle and much less obvious than
those found in a sequential setting. The parallelization rule is similar to the parallel congruence rule in that it requires the participating computations to mutually tolerate each other. This time, however, since the two computations being compared will do rather different things temporarily they must be oblivious against chaotic access, hence the  strengthenings in the premise. 

The reason for the appearance of  in the other rules is similar. The rule for pure lambda hoist seems unusual and will thus be explained in more detail. First, the computation  to be hoisted may indeed have side effects  so long as they are cleaned up by the time  completes and the intervening environment does not notice (modelled by the conditions 
 and final effect ). In the conclusion the transient effect  shows up again, but -ed since it only appears in different sides. Also in the other rules like commuting etc.\ 
it is the case that the familiar side conditions on applicability only affect the end-to-end effects whereas the transient effects are merely required not to interfere with the environment. 



\begin{figure*}[t]
\vspace{-3mm}
 





 


\vspace{-2mm}
\label{tycrule}
\caption{Typing and congruence rules}
\vspace{-4mm}
\end{figure*}
\begin{figure*}










\caption{Effect-dependent transformations\label{eqth}.}
\vspace{-3mm}
\end{figure*}
The following definitions provide the semantics of our effect annotations. 
\begin{definition}[Tiling]
Let . We write  to mean that (i)  and 
(ii)  and (iii)  and   imply .
\end{definition}
Thus, assuming semantic consistency of heaps,  and  evolve to  and  according to the modifying (writing or chaotic) locations in , and if  agree on the reads of  then written locations will either be identicallly modified or left alone.  

If the step relations of all abstract locations commute with each
other then tiling admits an alternative characterisation in terms of
preservation of binary relations \cite{DBLP:conf/aplas/BentonKHB06}. The present more
operational version is inspired by the treatment of effects in
\cite{birkedal}.

\begin{lemma}\label{tillem}
Suppose that , , . 
The following hold whenever well-formed. 
\begin{compactenum}
\item \label{tiltrans}
If  and  then ;  
\item  
\item\label{tilmon} If  then 
\item\label{tilmcon} 
\item \label{tilrd} If  and  then  . (this relies on .)
\item\label{tilwf} Suppose . If  then ; if  then . 
\end{compactenum}
\end{lemma}
\section{Logical Relation}\label{logrel}

\begin{definition}[Specifications] A value 
specification is a relation  such that 
\begin{itemize}
\item if  and  and  then ; 
\item if  and  are chains such that  then , i.e.,  is admissible qua relation; 
\item if  then  for each , i.e.\  is closed under the canonical deflations. 
\end{itemize}
Similarly,  a computation specification is a 
relation  such that 
 and  is admissible qua relation and  is closed under the canonical deflations . 
\end{definition}
The requirement  ensures smooth interaction with the down-closure built into our trace monad. Admissibility is needed for the soundness of recursion and closure under the canonical deflations, finally is needed so that Lemma~\ref{func} can be applied. 
\begin{definition}
If  and 
then 
the relation  is defined by 
 
In particular, for  to hold, both  must be functions (and not elements of base type or tuples). 
\end{definition}
\begin{lemma}
If  and  are specifications so is . 
\end{lemma}
The following is the crucial definition of this paper; it gives a semantic counterpart to observational approximation and, due to its game-theoretic flavour, allows for very intuitive proofs. 
\begin{definition}
\label{defn:crucial}
Let  be a value specification 
and  an effect specification. We
define the relations  and 
 between sets of trace-value pairs, i.e.\ on : 

 if and only if 

We define the relation  as the admissible closure of , i.e.\ . 
\end{definition}
The game-theoretic view of  may be
understood as follows. Given  we can consider a
game between a proponent (who believes )
and an opponent who believes otherwise. The game begins by the
opponent selecting an element  and , the \emph{pilot trace} and a 
start heap  such that 
 to begin a trace in . Then, the proponent answers with a matching heap  so that . If  does not hold, proponent does not need to ensure that writes are in sync. 
The opponent
then plays a heap  so that
. At this point, it is in the proponents interest to make sure that  for otherwise opponent may make ``funny'' moves.

 Then, again, proponent plays a heap
 such that  and so on until,
proponent has played  so that
. After that final heap has been played, it is checked that  holds. If not, proponent loses. If yes, then  proponent must also
play a value  and it is then checked whether or not
 and . If
this is the case or if at any one point in the game the opponent was
unable to move because there exists no appropriate heap then the
proponent has won the game. Otherwise the opponent wins and we have
 iff the proponent has a
winning strategy for that game.

We notice that by Lemma~\ref{tillem}(\ref{tilwf}) well-formedness of
heaps w.r.t.\ the ambient world is a global invariant which allows us
to refrain form explicitly assuming and asserting it in subsequent
proofs and statements.

We now illustrate the game with a few examples. 
\begin{example}
\label{jife}
\normalfont
 Consider the following programs:
 

 \noindent
Let  be the abstract location for a single integer stored at  (see Section~\ref{sec:abs-examples}). Let  be the value specification for the unit type.  

We  show that   under the assumption that , that is, when the environment does not 
read nor write .  This condition is clearly necessary, for  and  can be distinguished by an environment allowed to read or write . 

Let us now prove the claim when . The opponent picks a pilot trace in the semantics of , for example, 

\noindent
where  and  and  and
. The other possible traces are stuttering or
mumbling variants of this one and do not present additional
difficulties. The opponent also chooses a heap  such that
, i.e., .  Now the proponent
will choose to stutter for the time being and thus selects
. Indeed,  holds,
so this is legal. The opponent now presents  such that
. By the assumption on  we know
that  and also
. The proponent now answers with
. It follows that
 and also
. Finally, by
stuttering 
so that proponent wins the game.
\end{example}
\begin{example}
\label{ex:parallel}
\normalfont
  Consider the following programs  and :

   

\noindent
We show , provided  does not read nor modify  and . This equivalence could be deduced syntactically using our parallelization equation shown in Figure~\ref{eqth}. For illustrative purpose, however, we describe its semantic proof using a game. 

The opponent picks a pilot trace in , for example, the trace , where  denotes a heap where  and  store  and , respectively. Notice that in this trace,  is incremented before  and since  does not read nor modify  and , the environment move does not change the values in  nor . We are also given an initial heap   that agrees with the initial heap  on the reads of . Thus,  should be of the form . 

We now play the move . This is a valid move in the game as . The environment moves returning  as it does not read nor modify  and . We can now match the trace above by playing  and returning , winnning the game.
\end{example}
The following is one of the main technical result of our paper and shows that
the computation specifications  can indeed serve as the
basis for a logical relation. We just show here the soundness proof for the parallel congruence rule. The missing proofs appear in the attached Appendix.
\begin{theorem}\label{main}
The following hold whenever well-formed. 
\begin{compactenum}
\item\label{eins} If  then . 
\item\label{einsa}  is a computation specification. 
\item\label{zwei} If  then . 
\item\label{drei} If  then  is in . 
\item\label{vier} Suppose that  is an effect specification where . Suppose that whenever  and  then there exist  such that  and  and . We then have  for any ,  . 
\item\label{fuenf} If 
  and  then . 
\item\label{sechs} If  and  then .
\item\label{acht}   
.  
\end{compactenum}
\end{theorem}
\begin{proof}
Ad \ref{sechs}. Suppose that  and  and let ,
thus  (ignoring  by item
\ref{zwei}) where  and . Let ,
 be corresponding winning strategies.  The idea is to use 
when we are in  and to use  when we are in . Supposing
that  starts with a  fragment we begin by playing according to . Let  be of the form:

composed of pieces of the traces  and . Assume w.l.o.g. that the first piece  is a part of . We are given a initial heap  such that . Since , we can apply strategy  to guide us through the first part of the game, obtaining:

Moreover, we have an environment move which forms the tile . Thus, we have the tile  which can be seen as an environment move for . Therefore, we can use strategy  for the  and continue the game, obtaining the trace piece:

Now, we can return to the  game as the trace above is seen as an environment move for . Alternating these strategies, we get a trace  which is in . Let  be the final values reached at the end. It is clear that  and also  and . 

It remains to assert the stronger statement  . To see this suppose that . 
Since the entire game can be viewed as an instance of the game  vs  with interventions by  vs.\  regarded as environment interactions we have  so that in fact 
 and . The case of  and , interchanged is analogous.
\end{proof}
We assign a value specification  
to each refined type by

We omit the obvious definition of the other basic types and assume
value specifications for user-specified types as given.

\begin{assumption}\label{assi}
We henceforth adopt the following \emph{soundness assumption} which must be established concretely for every concrete instance of our framework. 
\begin{itemize}
\item The initial heap satisfies the current world: . 
\item Each axiom is type sound: whenever  is an axiom then 
  and . 
\item Each axiom is inequationally 
sound: whenever  is an axiom then 
 . 
\end{itemize}
\end{assumption}
\begin{theorem}\label{tysound}
Suppose that  and . Then
 (interpreting a context as a cartesian
product) implies  and
.
\end{theorem}
\begin{proof}
By induction on derivations. Most cases are already subsumed by
Theorem~\ref{main}.
The typing rules regarding functions and recursion follow from the definitions and from the fact that all specifications are admissible.  
\end{proof}
\section{Typed observational approximation}\label{obseq}

\begin{definition}[Observational approximation]
Let  be value expressions where  and . We say that  observationally approximates  at type
 if for all  such that  (``observations'') it
is the case that if  for
 and starting from  then  for some . We write 
in this case. We say that  and  are observationally equivalent
at type , written  if both  and .
\end{definition} 
This means that for every test harness  we build around  and
, no matter how complicated it is and whatever environments it
sets up to run concurrently with  and  it is the case that each
terminating computation of  (in the environment installed by )
can be matched by a terminating computation with the same result by
 in the same environment. It is important, however, that the
environment be well typed, thus will respect the contracts set up by
the type . E.g.\ if  is a functional type expecting, say,
a pure function as argument then, by the typing restriction, the
environment  cannot suddenly feed  and  a side-effecting
function as input.

We remark that observational approximation extends canonically to open
terms by lambda abstracting free variables (and adding a dummy
abstraction in the case of closed terms)
\cite{DBLP:dblp_conf/popl/Benton0N14}.

As usual, the logical relation is sound with respect to typed
observational approximation and thus can be used to deduce nontrivial
observational approximation relations. We state and prove the precise
formulation of this result.

\begin{theorem}
Let  be closed values and suppose that . 
Then . 
\end{theorem}

\begin{proof}
If  then by Thm~\ref{tysound} we have , so 
. 

Let . We have  and
thus in particular
. There must
therefore exist a matching heap  and a value  such that
 and .
\end{proof}
This means that the examples from earlier on give rise to valid transformations in the sense of observational approximation. For instance, for  and  form Example~\ref{jife} we find that 
 at type  whenever  does not appear in . 

\section{Effect-dependent transformations}
We will now establish the semantic soundness of the inequational
theory of effect-dependent program transformations given in
Figure~\ref{eqth}.  It includes concurrent versions of the
effect-dependent equations from \cite{DBLP:conf/aplas/BentonKHB06,DBLP:conf/icfp/ThamsborgB11},
but the side conditions on the environmental interaction are by no
means obvious. We also note that some equations now only hold in one
direction thus become inequations. This is in particular the case for
duplicated computations. Suppose that  is a computation that
nondeterministically chooses a boolean value and let
. Then, even though  does not read
nor write any location we only have , but not
 for  admits the result
 but  does not. Furthermore, due to presence of
nontermination the equations for dead code elimination and pure lambda hoist also hold in one direction only. It might be possible to restore both directions of said equations by introducing special effects for nondeterminism and nontermination; we have not explored this avenue. We concentrate the individual effect-dependent transformations before
summarising the foregoing results in the general soundness Theorem
\ref{eqthm}. 

In many of the equations, co-effects play an important role. For example, in the commuting and parallelization equations, the internal effects  and  in the premises are replaced by   and  in the internal effects of the conclusion. This makes sense intuitively because the computations are run in a different order, so for the internal moves, the locations in  and  can be modified in any way (see Example~\ref{ex:parallel}). However, in the global effect, we can still guarantee the effects  and  because of the -conditions. This intuition appears directly in the soundness proofs.

The following thus constitutes the second main technical
result of our paper. We sketch the soundness proof for parallelization. The detailed proofs appear in the attached Appendix.
\begin{theorem}\label{mainzwei}
The following hold whenever well-formed. 
\begin{itemize}
\item \textbf{Commuting} \label{commusound} If  and 
 and  and  and  then
\begin{small}
 
\end{small}\item\textbf{Duplicated} \label{dupsound} 
If  and  and , then

\item \label{puresound} \textbf{Pure} Let , such that . If  for some \emph{arbitrary} trace  (with ) and  value , then ;

\item\label{deadsound} \textbf{Dead} Suppose that , where   and . Then .





\item\label{parizesound} 
 \textbf{Parallelization} If  and  and  and  and , then 

\end{itemize}
\end{theorem}
\begin{proof}
(Sketch) \textbf{Parallelization.} 

Assume w.l.o.g.\ that the pilot trace takes the form  where  and . Just as in the commuting case we set up two side games  vs.\  on . Unlike, in that case, however, these games are running simultaneously and along with the main game. Moves by the environment in the main game are forwarded to the side game we are currently in, i.e., the one to which the current portion of  being played on belongs. At each change of control, we switch between the two side games making last sequence of moves of the other game into a single environment move. It is here that the resilience against chaotic modification is needed. Once the play is over we then assert the claims about the end-to-end effect  location by location using the definition of tiling. 
\end{proof}

\begin{theorem}
\label{eqthm}
Suppose that  and  and assume that for each 
 axiom  it holds that .
 Then  (interpreting a context as a cartesian product) implies  and 
. 
\end{theorem}
\begin{proof}[Sketch]
In essence the proof is by induction on derivations of inequalities. However, we need to slightly strengthen the induction hypothesis as follows: 

Define 

We now show by induction on derivations that 
 implies  and that    implies 
. 

The various cases now follow from earlier results in a straightforward manner. Namely, we use Theorem~\ref{main} for the congruence rules and 
Theorem~\ref{mainzwei} for the effect-dependent transformations. 

As a representative case we show the case where  and . Inductively, we know 
 and  for some . By Theorem~\ref{tysound}, we also have
 and analogous statements for . We can, therefore, assume, w.l.o.g.\ that  and then use Theorem~\ref{main} (\ref{fuenf}) repeatedly ( times) so as to conclude . 

The rules for dead code and pure lambda hoist rely on 
 the cases ``Dead'' and ``Pure'' of Thm~\ref{mainzwei} in a slightly indirect way. We sketch the argument for pure lambda hoist. The pilot trace begins with a trace belonging to  and yielding a value  for . We can then invoke case ``Pure'' on subsequent occurrences of  in the right hand side. 
\end{proof}

\begin{theorem}
Suppose that  and  and that  where  denotes transitive closure. 
Then . 
\end{theorem}
\begin{proof}
If  then by Thm~\ref{tysound} we have , so 
. 

Let . We have  and
thus in particular
. There must
therefore exist a matching heap  and a value  such that
 and .
\end{proof}











We now return to the examples that we discussed in Section~\ref{examples} and demonstrate how to prove using our denotational semantics the properties that have been discussed informally.

\paragraph{Overlapping References}
With this example, we illustrate the parallelization rule. In
particular, the functions declared in Section~\ref{examples} have the
following type, where  does not read nor write :

\begin{small}

\end{small}

The obvious and analogous typings for  and
 are elided.  We justify this typing semantically
as described in Theorem~\ref{main}. To illustrate how this is done,
consider the function . We show how the game
is played against itself using the typing shown above. We start with a
``pilot trace'', say:


\noindent 
where  denotes a store with  and other components left out for simplicity.
The first step corresponds to our reading of  and in the second step
-- since there was no environment intervention -- we write  into the first
component.

We now start to play: Say that we start at the heap . We answer . If the environment does not change , then we write  to its first component resulting in the following trace, which is possible for .



\noindent
If, however, the environment plays  (a modification of both
components of X has occurred), then we answer . Again,



\noindent
is a possible trace for . It is easy to check that there is a strategy that justifies the typing given above.

\noindent
Now, consider a program, , that only calls , and another program, , that only calls . Since the former functions have disjoint effects to the latter ones,  and  will have effect specifications, respectively, of the form 
 and , 
where . Thus we can use the parallelization rule shown in Figure~\ref{eqth} to conclude that 
the behavior of  is the same as executing these programs sequentially, although they read and write to the same concrete location.

\paragraph{Loop Parallelization}
We show that the function  is equivalent to . It is easy to see that the function  is equivalent to the program , which is the program obtained from  by replacing the underlined parallel operator `' in  by a sequential operator `'. The proof goes simply by unfolding . 

We then proceed by showing  and  are equivalent using our equations and the abstract locations  and  defined above. The piece of code that applies  first, namely , has global effects , while the second application, namely, , has effects . Notice that . Therefore, provided that the environment does not read nor modify the list, we can apply the parallelization equation to justify running  and  parallel is equivalent to running them in sequence.


\paragraph{Michael-Scott Queue}
We now show that the  and  functions described in Section~\ref{examples} for the Michael-Scott Queue have the same behavior as their atomic versions. 
We only show the case for , as the case for  is similar. More precisely, we now justify the axiom

\noindent
where .  That is, they approximate each other
 at a type where the environment is
allowed to operate on the queue as well. We also note that the
converse of the axiom is obvious by stuttering and mumbling.
After consuming a dummy argument  let the resulting pilot trace be
 and  be the
start heap to match. We can now assume that the passages from 
to  are according to the protocol,
i.e.\ . Namely, should this not
be the case we are free to make arbitrary moves and still win the
game by default of the environment player. Therefore, there must exist
 such that in the move  the element  is dequeued
and  holds for .  We can thus match this trace by
a trace in the semantics of  by
stuttering until :



\noindent
where  and  have the same content, but not necessarily the exact same layout. Given the environment's allowed effects it is then clear that also  and  have the same content, but not necessarily the same as  and  because in the meantime other operations on the queue might have succeeded. We then dequeue the corresponding element from  leading to  and continue by stuttering. 



\noindent
It is now clear that this is a matching trace and that  so we are done. 

Notice that the congruence rules now allow us to deduce the equivalence of 
 and 
 
for  being enqueues or dequeues, which effectively amounts to linearizability. 
\section{Discussion}
We have shown how a simple effect system for stateful computation and
its relational semantics, combined with the notion of abstract
locations, scales to a concurrent setting. The resulting type system
provides a natural and useful degree of control over the otherwise
anarchic possibilities for interference in shared variable languages,
as demonstrated by the fact that we can delineate and prove the
conditions for non-trivial contextual equivalences, including
fine-grained data structures.

The primary goal of this line of work is not so much to find reasoning
principles that support the most subtle equivalence arguments for
particular programs, but rather to capture more generic properties of
modules, expressed in terms of abstract locations and relatively
simple effect annotations, that can be exploited by clients (including
optimizing compilers) in external reasoning and transformations. But
there are of course, particularly in view of the fact that we allow
deeper reasoning to be used to establish that expressions can be
assigned particular effect-refined types, very close connections with
other work on richer program logics and models. 

Rely-guarantee reasoning is widely used in program logics for concurrency, including
relational ones \cite{liangfengpopl12}, whilst our abstract locations
are very like the \emph{islands} of Ahmed et al
\cite{DBLP:conf/popl/AhmedDR09}. Recent work of Turon et al
\cite{dreyer} on relational models for fine-grained concurrency
introduces richer abstractions, notably state transition systems
expressing inter-thread protocols that can involve ownership
transfer. These certainly allow the verification of more complex
fine-grained algorithms than can be dealt with in our setting, and it
would be natural to try defining an effect semantics over such a
model. Indeed, one might reasonably hope that effects could provide
something of a `simplifying lens', with refined types capturing things
that would otherwise be extra model structure or more complex
invariants, such that the combination does not lead to further
complexity. The use of Brookes's trace model (also used
by, for example, Turon and Wand \cite{DBLP:conf/popl/TuronW11}) already seems to
bring some simplification compared to transition systems or
resumptions. 

Birkedal et al \cite{birkedal} have also studied relational semantics
for effects in a concurrent language. The language considered there has dynamic allocation via regions and higher-order store, neither of which we have here. On the other hand, their invariants are based on simply-typed concrete locations and thus do not allow to capture effects at the level of whole datastructures as abstract locations do. As a result, the examples in \cite{birkedal} are of a simpler nature than ours. Furthermore, we offer a subtler parallelization rule, distinguish transient and end-to-end effects, and validate other effect-dependent equivalences like commuting, lambda
hoist, deadcode and duplication. Our  use of denotational methods and in
particular the extension of Brookes' trace semantics to higher-order
functions does result in a rather simpler and more intuitive 
definition of the logical relation by
comparison with \cite{birkedal}. While some of the complications are  due to the
dynamic allocation and typed locations, others like the explicit step
counting, the need for effect-instrumented operational semantics, and the separation of
branches in the definition of safety are not. We thus see our work also as a proof-of-concept for
denotational semantics in the realm of higher-order concurrent
programming.


The `RGSim' relation proposed by Liang \etal\ for proving concurrent
refinements under contextual assumptions also has many similarities
with our logical relation~\cite[Def.4]{liangfengpopl12}. The focus of
that work is on proving particular equivalences and refinements,
whereas we encapsulate general patterns of behaviour in a refined type
system and can show the soundness of generic program transformations
relying only on effect types (which combine smoothly with hand proofs
of particular equivalences).

There are many directions for further work. Most importantly, we would
like to add dynamic allocation of abstract locations following
\cite{DBLP:dblp_conf/popl/Benton0N14}. In addition to relieving us
from having to set up all data structures in the initial heap this
would, as we believe, also allow us to model and reason about
lock-based protocols in an elegant way. Other possible extension
include higher-order store and weak concurrency models.

\newpage

\bibliography{bib}

\newpage 



\appendix

\section{Proof of Theorem~\ref{main}}

\begin{proof}
In each case, using Corollary~\ref{funad} and Lemma~\ref{func} (for
case \ref{fuenf}), we can in fact assume w.l.o.g.\ that the assumed
pairs are in  rather than . 

\medskip 

 Ad~\ref{eins}.  Let , i.e.\  where
 .  By down-closure ([Down]) we also have . We can now play the strategy guaranteed by the assumption
  which will yield (depending
 on the opponent's moves) a trace  and a value  such that
  and . Now, since  is a
 specification we get  noting that  is
 idempotent. So, we modify the strategy so as to return 
 rather than  and thus obtain a winning strategy asserting the
 desired conclusion.

Ad~\ref{einsa} This is an easy consequence from \ref{eins}. 

Ad~\ref{zwei} Pick . Since  is
closed under suprema it suffices to show that
 for
  each . Fix such  and pick , thus
  . 


By induction on the closure process we can assume w.l.o.g.\ that 
arises from  by a single mumbling or stuttering step
or that  for some  or else that  where . 

In the former two cases fix a strategy for the original element of
. We will use this strategy to build a new one demonstrating that , hence  as required.  


If  arises by stuttering, so  and  we play
the strategy until  is worked off. If the opponent then produces a
heap  to match  we answer .

\smallskip 

Now  is always true (Lemma~\ref{tillem}) so this is
a legal move. Thereafter, we continue just as in the original
strategy. In the special case where  is empty, we must also show
that  knowing  
where  and  is the matching trace. We have  for otherwise opponent's playing  would have been illegal. Since, by assumption , we can conclude  and then  by Lemma~\ref{tillem}(\ref{tilmon}\&\ref{tiltrans}). 

\smallskip 

If  arises by mumbling then we must have  
and . We play until the
strategy has produced a match  for . So far, the play has
produced a trace  matching , and a state  so that
. Now, we can ask what the original
strategy would produce if we gave it (temporarily assuming opponent's
role) the state  as a match for . Note that this is legal
because . The strategy will then
produce  such that  and our
answer in the play on the new trace against the challenge  will
be this very . Indeed, by composing tiles (Lemma~\ref{tillem}) we
have  as required. Thereafter, the
play continues according to the original strategy.

\smallskip 

For down-closure, we play the strategy against  yielding a
match  where . That same strategy also
wins against  because  since  is a value
specification.
\smallskip 

For closure under [Sup], finally, pick  so that  recalling that . Since we have a winning strategy for , we also have one (by down-closure which was already proved) for  as required. 

\medskip

Ad \ref{drei}. Suppose . By \ref{zwei} which we have just proved we only need to match elements of the form . The opponent plays  where  and we answer with  itself and . This is always a legal move (Lemma~\ref{tillem}) and  , so we win the game. 

\medskip 
 
Ad \ref{vier}.  Again, we only need to match traces of the form  where . In this case, suppose that the opponent plays  where . The assumption gives  such that  and  and . We thus play  and  and indeed  and  hold so this is a winning move. 

Ad \ref{fuenf}. Suppose 
 and . Suppose that  where  and  in  (note that we can ignore the -closure). We need to produce a trace  such that  and  in  and . Assume that:

We are given a heap , such that . We can use the strategy  from  for . We play according to  to work off the -part. This results in a matching trace :

where  and . 
We get . Now, we are given a heap  that is an environment move forming the tile . From the fact that  and Lemma~\ref{tillem}(\ref{tilrd}) we can conclude . 



Thus we can continue our play by using the strategy  from
 which yields a
continuation  of our trace and a final answer . It is then
clear that  so this combination of
strategies does indeed win.



\medskip 

Ad \ref{sechs}. Suppose that  and  and let ,
thus  (ignoring  by item
\ref{zwei}) where  and . Let ,
 be corresponding winning strategies.  The idea is to use 
when we are in  and to use  when we are in . Supposing
that  starts with a  fragment we begin by playing according to . Let  be of the form:

composed of pieces of the traces  and . Assume w.l.o.g. that the first piece  is a part of . We are given a initial heap  such that . Since , we can apply strategy  to guide us through the first part of the game, obtaining:

Moreover, we have an environment move which forms the tile . Thus, we have the tile  which can be seen as an environment move for . Therefore, we can use strategy  for the  and continue the game, obtaining the trace piece:

Now, we can return to the  game as the trace above is seen as an environment move for . Alternating these strategies, we get a trace  which is in . Let  be the final values reached at the end. It is clear that  and also  and . 

It remains to assert the stronger statement  . To see this suppose that . 
Since the entire game can be viewed as an instance of the game  vs  with interventions by  vs.\  regarded as environment interactions we have  so that in fact 
 and . The case of  and , interchanged is analogous.


Ad \ref{acht}. This is direct from the definition of atomic and appealing on the fact that .
\end{proof}

\section{Proof of Theorem~\ref{mainzwei}}

\begin{proof}
\textbf{Commuting.}
By Theorem~\ref{main}(\ref{zwei}) we can assume our pilot trace  to be 
of the form:

where

We make similar use of Theorem~\ref{main}(\ref{zwei}) in the subsequent cases without explicit mention. 

We are also given a heap  such that . Because ,  and  agree on the reads of . Thus we can start a game  vs.\  using  and . We forward all environment's moves from the main game to the side game and use the responses from the side game to answer in the main game. Suppose that the side game leads to the valid -trace 

where  and (1) . 
Notice that in the global game these are legal responses as  for .

We now have an environment move . Since  and , the heaps  and  agree in the reads of . Therefore, we can run a game  vs.\   using  and , obtaining the trace: 

where  and (2) . The reasoning is similar to the use of the previous game.

Thus we have that .

Now, we need to conclude that . This follows from the fact that  and (1) and (2). In particular, from (1) and , we get that  and  agree on the locations in , while from (2), we get that  and  agree on the locations in . This finishes the proof.

\textbf{Duplicated.}
Assume given a trace in :

and a heap  such that . Since  and , we have that  and  agree on the reads of .

We start by simply stuttering:

where  for . Notice that for , we have . So the stuttering moves are valid responses. 

We will now play  vs.\  to construct the missing heap ``??''. We first run a game using  and , where the environment moves are simply stutter moves:

such that  and . Notice that using stuttering environment moves are valid as  for .

Since  and  agree on the reads of  and  and  agree on  from , we can run the game  vs.\  again on  and  with stutter environment moves:

where  and . Thus, . 

We now put  which leads to a valid trace due to repeated mumbling.  Finally, 
 follows
from  and .

\textbf{Pure.}
We start with a trace from , for example  and an arbitrary heap . We now consider the game involving  vs.\  on  and :

We have that  and . By mumbling, . We can reply with  in the main game.

\textbf{Dead.} 
Assume given a trace of the form:

and  such that . We now
initiate a side game  vs.\  on this trace and respond in the
main game by stuttering. Thus, we obtain traces  in the main game and  in the side game.

The main trace is in . The side game tells us that 
  and that  and therefore
. It remains to
show that . This follows from the
fact that  has only reads as  and  agree on all
locations.














\paragraph{Parallelization.}








We start with a trace in . Assume that the trace is of the following form:

where each  is a possibly empty sequence of moves of the form  and

are traces from  and , respectively. We are also given a heap  such that . We also have . We run a side game  vs.\  
using  and , yielding:

 Assume that  and  are, respectively, the first and last moves of this trace. We have  and (1) . Notice that these are legal moves in the global game as we have  tiles for the player moves and  times for the environment moves.

 Now, assume there is an environment move .  Since  and , the heaps  and  agree on the reads of  and  and  also agree on the reads of . (Notice as well that  as  is a valid effect.) Therefore, we can invoke an  game using  and , obtaining the trace:  

Assume that  and  are, respectively, the first and last moves of this trace. We have  and (2) . For the same reasons as above, these are legal moves in the global game.

Therefore . 

We need now to prove that . From (1) and  and , we have that  and  agree on the locations of . Similarly,  and  agree on the locations of . Since there are only  tiles and  and ,  and  agree on the locations of . This finishes the proof.

\end{proof}


\end{document}
