\documentclass[a4paper]{article}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{enumerate}
\usepackage[OT4]{fontenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{authblk}
\setcounter{tocdepth}{3}
\usepackage{epstopdf}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx}
\usepackage{fullpage}
\usepackage{todonotes}
\usepackage[hidelinks]{hyperref}
\usepackage{breakurl}
\usepackage[capitalise]{cleveref}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{microtype}


\ifx\shortversion\empty
	\newenvironment{shortv}{}{}
	\excludecomment{fullv}
    \newcommand{\appref}[1]{\marginpar{\tiny #1\\full version}}
\else
	\newenvironment{fullv}{}{}
	\excludecomment{shortv}
    \newcommand{\appref}[1]{}
\fi

\urldef{\mailsa}\path|a.karczmarz@mimuw.edu.pl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{invariant}[theorem]{Invariant}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\makeop}{\mathtt{make\_string}}
\newcommand{\dropop}{\mathtt{drop\_string}}
\newcommand{\compop}{\mathtt{compare}}
\newcommand{\actop}{\mathtt{activate}}
\newcommand{\dactop}{\mathtt{deactivate}}

\newcommand{\eqop}{\mathtt{equal}}
\newcommand{\concop}{\mathtt{concat}}
\newcommand{\splitop}{\mathtt{split}}
\newcommand{\findop}{\mathtt{find}}
\newcommand{\lcpop}{\mathtt{LCP}}
\newcommand{\layerop}{\mathtt{layer}}
\newcommand{\anchfind}{\mathtt{find}}
\newcommand{\anchinsert}{\mathtt{insert}}
\newcommand{\anchdelete}{\mathtt{delete}}
\newcommand{\symbols}{\mathcal{S}}
\newcommand{\sigs}{\bar{\Sigma}}

\newcommand{\anchfindk}{\mathtt{find\_k}}
\newcommand{\lastsymb}{\mathcal{Z}}
\newcommand{\compress}{\textsc{Compress}}
\newcommand{\depth}{\textsc{Depth}}
\newcommand{\rle}{\textsc{Rle}}
\newcommand{\shrink}[1]{\ensuremath{\textsc{Shrink}_{#1}}}
\newcommand{\cshrink}[1]{\ensuremath{\overline{\textsc{Shrink}_{#1}}}}

\newcommand{\str}{w}
\newcommand{\strb}{y}
\newcommand{\rules}{R}
\newcommand{\emptystring}{\ensuremath{\eps}}

\newcommand{\hs}{h}
\newcommand{\sig}{Sig}
\newcommand{\grammar}{\mathcal{G}}
\newcommand{\mword}{B}
\newcommand{\decomp}{D}

\newcommand{\spair}[3]{\ensuremath{#1 \rightarrow #2#3}}
\newcommand{\spower}[3]{\ensuremath{#1 \rightarrow #2^{#3}}}

\newcommand{\slev}{\textit{level}}
\newcommand{\sanch}{\textit{anch}}
\newcommand{\slength}{\textit{length}}
\newcommand{\stree}{\mathcal{T}}
\newcommand{\sstr}{\textit{str}}
\newcommand{\ustree}{\mathcal{\overline{T}}}
\newcommand{\spar}[1]{\mathrm{par}_{\stree(#1)}}
\newcommand{\spars}{\mathtt{pars}}
\newcommand{\uspar}{\mathrm{par}}
\newcommand{\usleft}{\mathrm{left}}
\newcommand{\usright}{\mathrm{right}}
\newcommand{\uschild}{\mathrm{child}}
\newcommand{\ussig}{\mathrm{sig}}
\newcommand{\stlayer}{\Lambda}

\newcommand{\itroot}{\mathtt{root}}
\newcommand{\itbegin}{\mathtt{begin}}
\newcommand{\itend}{\mathtt{end}}

\newcommand{\coll}{\mathcal{W}}

\newcommand{\itparent}{\mathtt{parent}}
\newcommand{\itchild}{\mathtt{child}}
\newcommand{\itindex}{\mathtt{index}}
\newcommand{\itlevel}{\mathtt{level}}
\newcommand{\itleft}{\mathtt{left}}
\newcommand{\itrepr}{\mathtt{repr}}
\newcommand{\itright}{\mathtt{right}}
\newcommand{\itnil}{\mathtt{nil}}
\newcommand{\itanch}{\mathtt{anchor}}
\newcommand{\itdegree}{\mathtt{degree}}
\newcommand{\itfirst}{\mathtt{First}}
\newcommand{\itlast}{\mathtt{Last}}
\newcommand{\itsig}{\mathtt{sig}}
\newcommand{\itaux}{\mathtt{aux}}
\newcommand{\itrskip}{\mathtt{rskip}}
\newcommand{\itlskip}{\mathtt{lskip}}
\newcommand{\itrext}{\mathtt{rext}}
\newcommand{\itlext}{\mathtt{lext}}

\newcommand{\stpush}{\mathtt{push}}
\newcommand{\stpop}{\mathtt{pop}}
\newcommand{\sttop}{\mathtt{top}}
\newcommand{\stempty}{\mathtt{empty}}
\newcommand{\stcomp}{\mathtt{collapse}}

\newcommand{\poly}{\mathrm{poly}}

\newcommand{\insertop}{\mathtt{insert}}
\newcommand{\deleteop}{\mathtt{delete}}
\newcommand{\connectedop}{\mathtt{connected}}
\newcommand{\updateop}{\mathtt{update}}
\newcommand{\verifyop}{\mathtt{verify}}

\newcommand{\algptr}{P}
\newcommand{\algqtr}{Q}
\newcommand{\algparent}{P'}
\newcommand{\algqarent}{Q'}


\newcommand{\eps}{\varepsilon}
\newcommand{\edot}{{\cdot}}


\pagestyle{plain}

\begin{document}


\begin{titlepage}
\date{}
\title{Optimal Dynamic Strings\thanks{Work done while Paweł Gawrychowski held a post-doctoral position at  Warsaw  Center  of
Mathematics and Computer Science. Piotr Sankowski is supported by the Polish National Science Center, grant no 2014/13/B/ST6/00770.}}

\author[1]{Paweł Gawrychowski}
\author[1]{Adam Karczmarz}
\author[1]{Tomasz Kociumaka}
\author[2]{\\Jakub Łącki}
\author[1]{Piotr Sankowski}

\affil[1]{Institute of Informatics, University of Warsaw, Poland}
\affil[ ]{\texttt{[gawry,a.karczmarz,kociumaka,sank]@mimuw.edu.pl}}
\affil[2]{Sapienza University of Rome, Italy}
\affil[ ]{\texttt{j.lacki@mimuw.edu.pl}}

\maketitle
\thispagestyle{empty}


\begin{abstract}
In this paper we study the fundamental problem of maintaining
a~dynamic collection of strings under the following operations:
\begin{itemize}
\item  -- concatenates two strings,
\item  -- splits a string into two at a given position,
\item  -- finds the lexicographical order (less, equal, greater) between two strings,
\item  -- calculates the longest common prefix of two strings.
\end{itemize}
We present an efficient data structure for this problem, where an update requires
only  worst-case time with high probability, with  being the total length of all strings
in the collection, and a query takes constant worst-case time.
On the lower bound side, we prove that even if the only possible query is checking equality of
two strings, either updates or queries take amortized  time; hence
our implementation is optimal.

Such operations can be used as a basic building block to solve other string problems.
We provide two examples. First, we can augment our data structure to provide
pattern matching queries that may locate occurrences of a specified pattern  in the
strings in our collection in optimal  time, at the expense of increasing update time to .
Second, we show how to maintain a history of an edited text, processing updates in  time,
where  is the number of edits, and how to support pattern matching queries against the whole
history in  time. 

Finally, we note that our data structure can be applied to test dynamic tree isomorphism
and to compare strings generated by dynamic straight-line grammars. 
\end{abstract}
\end{titlepage}
\newpage

\tableofcontents
\newpage

\section{Introduction}
Imagine a set of text files that are being edited. Each edit consists in adding or removing a single character or copy-pasting longer
fragments, possibly between different files. We want to perform fundamental queries on such
files. For example, we might be interested in checking equality of two files, finding their
first mismatch, or pattern matching, that is, locating occurrences of a given pattern in all files.
All these queries are standard operations supported in popular programs, e.g., text editors. However, modern
text editors rise new interesting problems as they often maintain the entire history of edits.
For example, in TextEdit on OS X Lion the save button does not exist anymore.
Instead, the editor stores all versions of the file that can be scrolled through using the so called timeline.
Analogously, in Google Docs one sees a file together with its whole history of edits.
A natural question arises whether we can efficiently support a \emph{find} operation,
that is whether it is possible to efficiently locate all occurrences of a given pattern in all versions of a file.

We develop improved and more efficient tools that can be used as a basic building block in algorithms on dynamic texts.
In particular, we present an optimal data structure for dynamic strings equality.
In this problem we want to maintain a collection of strings that can be concatenated, split, and tested for equality. Our algorithm
requires  worst-case time with high probability\footnote{Our algorithms always return correct answers, and the
randomization only impacts the running time.} for an update, where  is the total length of the strings in the collection,
and compares two strings in  worst-case time, assuming -time arithmetic on
the lengths of the strings.

Our solution is obtained by maintaining a certain context-insensitive representation of the strings,
which is similar but conceptually simpler than what has been used in the previous works~\cite{Alstrup,Mehlhorn}
(which use the same assumptions concerning randomization and arithmetics).
This allows us to obtain better running time in the end, but getting the desired bound requires a deeper insight in comparison to previous results.
While our improvement in the update time is , we
believe that obtaining tight bounds for such a fundamental problem is of major interest.
Thus, we also provide a matching  lower bound for the amortized complexity
of update or query, showing that our solution is indeed the final answer. This lower bound is obtained
by appropriately modifying the proof of the lower bound for dynamic path connectivity~\cite{logarithmic}.

Next, we describe how to support lexicographical comparison and computing the longest
common prefix of two strings in constant worst-case time. All time complexities remain unchanged if we
want to maintain a collection of persistent strings, that is, concatenate and split do not destroy their
arguments. The aforementioned lower bound applies to non-persistent strings, and hence also to persistent
strings. Nevertheless, in the persistent setting  might be exponential with respect to the total input size,
while it is only polynomial in the lower-bound examples.
Thus, the bounds would not meet if we measured the running time as a function of the total input size.
We also show how to apply our date structure to: support pattern matching queries in dynamic string collections, 
support find operation in history of edits, compare strings generated by dynamic straight line grammars, and test dynamic
tree isomorphism. 

\subsection{Our Results}
\paragraph{Dynamic String Equality and Lexicographic Comparison}
The first result on dynamic string equality was given by Sundar and Tarjan~\cite{sundartarjan}.
They achieved amortized  time for an update, where  is
the number of operations executed so far and  is the total length of the strings. This was later improved
to randomized logarithmic time for the special case of repetition-free sequences~\cite{Pugh}.
The first improvement for the general case was due to Mehlhorn et al.~\cite{Mehlhorn}, who
decreased the update time to expected  time. They also provided a deterministic version
working in  time. Finally, much later, the deterministic algorithm of~\cite{Mehlhorn} was
substantially improved by Alstrup et al.~\cite{Alstrup}, who achieved 
update time.\footnote{While not explicitly stated in the paper, this bound holds with high probability, as the algorithm uses a hash table.} In all these solutions, equality of two strings can be checked in worst-case
constant time. We provide the final answer by further improving the update time to  with
high probability and showing that either an update or an equality test requires  
time, even if one allows amortization and randomization (either Las Vegas or Monte Carlo).\begin{shortv}\footnote{This extended abstract aims to present a brief overview of our results. The references to formal definitions, and detailed descriptions included in the full version are put on the margin.}
\end{shortv}

We note that it is very simple to achieve  update time for maintaining a non-persistent
family of strings under concatenate and split operations, if we allow the equality queries
to give an incorrect result with polynomially small probability.
We represent every string by a balanced search tree with characters in the leaves and every
node storing a fingerprint of the sequence represented by its descendant leaves. However, it is not clear how to make
the answers always correct in this approach (even if the time bounds should
only hold in expectation). Furthermore, it seems that both computing the longest common
prefix of two strings of length  and comparing them lexicographically requires  time in this approach.
This is a serious drawback, as the lexicographic comparison is a crucial ingredient in our applications related to pattern matching.

\paragraph{Straight-line Grammars}
A straight-line grammar (also known as a straight-line program) is a context-free grammar
generating exactly one string. Such grammars are a very convenient abstraction of text
compression. For instance, it is known that given a LZ77 representation of a text of length 
consisting of  phrases, we can construct a straight-line grammar of size
~\cite{Charikar,Rytter} that generates the given text. Therefore, we might hope to efficiently process
compressed texts by working with their straight-line grammars.

However, operating on such
grammars is nontrivial. Even the seemingly simple task of checking if two non-terminals derive
the same string is challenging, as the derived strings might have exponential
length in terms of the size of the grammar . Using combinatorial properties of strings,
Hirshfeld et al.~\cite{Hirshfeld} and Plandowski~\cite{Plandowski} independently showed
how to check in polynomial time whether two straight-line grammars describe the same string.
The time complexity of both implementations is rather high (for example, although not stated
explicitly in~\cite{Plandowski}, it seems to be ). This was recently improved
by Jeż to , where  is the length of the described string.

This problem can be easily solved with a data structure for maintaining a family of persistent strings
to obtain the same  complexity for 
preprocessing a straight-line program
for checking if two non-terminals derive the same string (and if not, computing the longest common prefix of the two derived strings) in constant time, assuming
constant-time arithmetic on the lengths of the generated strings.
While matching the best previously known time bound, our solution has the advantage of being dynamic:
we are able to efficiently add new non-terminals to the grammar and ask queries about
already existing non-terminals.
We believe that our results have further consequences for algorithmics of straight-line grammars.
See~\cite{LohreySurvey} for a survey of the area.

\paragraph{Pattern Matching in Dynamic Texts}
Finding all  occurrences of a length  pattern in a static text can be done in  time
using suffix trees, which can be constructed in linear time~\cite{McCreight:1976,SuffixTree}.
Suffix trees can be made partially dynamic by allowing prepending or appending single characters
to the text~\cite{onlinesuffix}. In the fully dynamic setting, where insertions or deletions can
occur in the middle of the text, the problem becomes much more difficult.
However, if we are only interested in inserting or deleting single characters in  time,
queries can be implemented in  time~\cite{Gu:1994},
where  is again the number of operations executed so far.
Ferragina~\cite{Ferragina:1997} has shown how to handle insertions and deletions of blocks of text with the query
time roughly proportional to the number of updates made so far. This was soon improved to support
updates in  time and queries in  time~\cite{DBLP:journals/siamcomp/FerraginaG98}.
The first polylogarithmic-time data structure for this problem was presented by Sahinalp and Vishkin~\cite{Sahinalp:1996}, who achieved  time for an update and optimal
 time for a query. Later, the update time was improved to
 at the expense of increasing query time to
 by Alstrup et al.~\cite{Alstrup}. By building on our structure
for maintaining a collection of dynamic strings, we are able to further improve the update time
to  (we remove the  factor) with optimal query time (we remove the  additive term).
We also extend pattern matching to the persistent setting, in which case the update times are preserved
and the query time becomes .

\paragraph{Pattern Matching in the History of an Edited Text}
We consider a problem of maintaining a text that is being edited.
At each step either a character is added, some block of text is removed or moved to a different location (cut-paste).
We develop a data structure that can be updated in  time with high probability and
supports pattern matching queries.
Such a query locates and reports first  occurrences of a length- pattern in the whole history of a text in chronological
order in  time.
To the best of our knowledge, we are the first to consider this natural problem.

\paragraph{Dynamic Tree Isomorphism}
As shown in~\cite{treei}, a data structure for maintaining a family of sequences can also be used for solving dynamic tree isomorphism problem.
In this problem, a family of trees is maintained and can be updated by adding/removing edges and adding vertices.
Moreover, each two trees can be tested for being isomorphic.
The result of~\cite{treei} can be immediately improved with the data structure of
Alstrup et al.~\cite{Alstrup}, and our result improves it further by a  factor
to decrease the update time to  with high probability, where  is the total number of vertices in all trees, while keeping the query time constant.

\subsection{Related Techniques}
Our structure for dynamic string equality is based on maintaining a hierarchical representation
of the strings, similarly to the previous works~\cite{Mehlhorn,Alstrup}.
In such an approach the representation should be, in a certain sense, locally consistent, meaning that
two equal strings have identical representations and the representations of two strings
can be joined to form the representation of their concatenation at a relatively small cost.
The process of creating such a representation can be imagined as parsing: breaking the string into blocks,
replacing every block by a new character, and repeating the process on the new shorter
string.

Deciding how to partition the string into blocks is very closely related to the
list ranking problem, where we are given a linked list of  processors and every processor wants
to compute its position on the list. This requires resolving contention, that
is, choosing a large independent subset of the processors. A simple approach,
called the random mate algorithm~\cite{TreeContraction,Vishkin}, is to give a random bit
to every processor and select processors having bit  such that their successor has bit .
A more complicated (and slower by a factor of ) deterministic solution, called deterministic
coin tossing, was given by Cole and Vishkin~\cite{DeterministicTossing}. Such symmetry-breaking
method is the gist of all previous solutions for dynamic string equality. Mehlhorn et al.~\cite{Mehlhorn}
used a randomized approach (different than random mate) and then applied deterministic coin tossing
to develop the deterministic version. Then, Alstrup et al.~\cite{Alstrup} further optimized the
deterministic solution.

The strategy of breaking a string into blocks and repeating on the shorter string has been
recently very successfully used by Jeż, who calls it the recompression method~\cite{JezRecompression},
to develop more efficient and simpler algorithms for a number of problems on compressed strings and for solving
equations on words. In particular, a straightforward consequence of his fully compressed pattern
matching algorithm~\cite{JezFully} is an algorithm for checking if two straight-line grammars
of total size  describe the same string of length  in time .
However, he considers only static problems, which allows him to appropriately choose the partition 
by looking at the whole input.

Very recently, Nishimoto et al.~\cite{DBLP:journals/corr/NishimotoIIBT15} further extended
some of the ideas of Alstrup et al.~\cite{Alstrup} to design a space-efficient dynamic index.
They show how to maintain a string in space roughly proportional to its Lempel-Ziv factorization,
while allowing pattern matching queries and inserting/deleting substrings in time roughly proportional
to their lengths.

To obtain an improvement on the
work of Alstrup et al.~\cite{Alstrup}, we take a step back and notice that while partitioning
the strings into blocks is done deterministically, obtaining an efficient implementation
requires hash tables, so the final solution is randomized anyway. This suggests that, possibly, it does
not make much sense to use the more complicated deterministic coin tossing, and applying
the random mate paradigm might result in a faster solution. We show
that this is indeed the case, and the obtained structure is not only faster, but also conceptually
simpler (although an efficient implementation requires a deeper insight and solving a few new
challenges, see \cref{sec:comp} for a detailed discussion).

\subsection{Organization of the Paper}
In \cref{sec:preliminaries} we introduce basic notations.
Then, in \cref{sec:collection} we present the main ideas of our dynamic string collections
and we sketch how they can be used to give a data structure that supports equality tests in  time.
We also discuss differences between our approach and the previous ones (in \cref{sec:comp}).
The details of our implementation are given in the following three sections:
\cref{sec:pointers} describes iterators for traversing parse trees of grammar-compressed strings.
Then, in~\cref{sec:contexti} we prove some combinatorial facts concerning context-insensitive decompositions.
Combined with the results of previous sections, these facts let us handle equality tests and updates on the dynamic string collection in a clean and uniform way
in \cref{sec:adding}.

Next, in~\cref{sec:lb} we provide a lower bound for maintaining a family of non-persistent strings.
Then, in~\cref{sec:om}, we show how to extend the data structure developed in~\cref{sec:collection,sec:pointers,sec:contexti,sec:adding} in order to support lexicographic comparisons and longest common prefix queries.
\cref{sec:anchored} introduces some basic tools that are related to answering pattern-matching queries in the following three sections.
In \cref{sec:pm} we show that the our dynamic string collection can be extended with pattern matching queries against all strings in the data structure.
However, this comes at a cost of making the data structure non-persistent.
In \cref{sec:ppm} we address this issue presenting a persistent variant with an extra additive term in the query time.
Finally, in \cref{sec:timeline} we show how to use our data structures to support pattern matching queries in all versions of an edited text.

We conclude with content deferred from \cref{sec:preliminaries,sec:collection}: 
discussion of technical issues related to the model of computation in \cref{app:model} and a rigorous formalization of our main concepts in \cref{app:formalsymbols}.
\section{Preliminaries}\label{sec:preliminaries}
Let  be a finite set of \emph{letters} that we call an \emph{alphabet}.
We denote by  a set of all finite \emph{strings} over ,
and by  all the non-empty strings.
Our data structure maintains a family of strings over some integer alphabet .
Internally, it also uses strings over a larger (infinite) alphabet .
We say that each element of  is a \emph{symbol}.
In the following we assume that each string is over , but the exact description of the set  will be given later.

Let  be a string (throughout the paper we assume that the string indexing is 1-based).
For  a word  is called a \emph{substring} of .
By  we denote the \emph{occurrence} of  in  at position , called a \emph{fragment} of .
We use shorter notation , , and , to denote , , and , respectively.
Additionally, we slightly abuse notation and use  for  to represent empty fragments. 

We say that a string  is \emph{1-repetition-free} if its every two consecutive symbols are distinct.
The concatenation of two strings  and  is denoted by  or simply .
For a symbol  and ,  denotes the string of length  whose every element is equal to .
We denote by  the reverse string .





To compute the \emph{run-length encoding} of a string , we compress maximal substrings that consist of equal symbols.
Namely, we divide  string into \emph{blocks}, where each block is a maximal substring of the form  (i.e., no two adjacent blocks can be merged).
Then, each block , where , is replaced by a pair .
The blocks with  are kept intact.
For example, the run-length encoding of  is .

We use the word RAM model with multiplication and randomization, assuming that the machine word has  bits.
Some of our algorithms rely on certain numbers fitting in a machine word, which is a~weak
assumption because a word RAM machine with word size  can simulate a word RAM machine
with word size  with constant factor slowdown.
A more detailed discussion of our model of computation is given in~\cref{app:model}.

\section{Overview of Our Result}\label{sec:collection}
In this section we show the main ideas behind the construction of our data structure
for maintaining a family of strings that can be tested for equality.
The full details are then provided in~\cref{sec:pointers,sec:contexti,sec:adding}.
Our data structure maintains a dynamic collection  of non-empty strings.
The collection is initially empty and then can be extended using the following updates:
\begin{itemize}
  \item , for , results in .
   \item , for , results in .
  \item , for  and , results in
    .
\end{itemize}
Each string in  has an integer \emph{handle} assigned by the update which created it.
This way the arguments to  and  operations have constant size.
Moreover, we make sure that if an update creates a string which is already present in , the original handle is returned.
Otherwise, a fresh handle is assigned to the string. We assume that handles are consecutive integers starting from 
to avoid non-deterministic output.
Note that in this setting there is no need for an extra  operation, as it can be implemented by
testing the equality of handles.

The w.h.p. running time can be bounded by  for  and  for  and ,
where  is the total length of strings in ; see~\cref{thm:data_structure} for details.

\subsection{Single String Representation}\label{sec:single_string}
In order to represent a string, we build a straight-line grammar that generates it.
\begin{definition}
A context-free grammar is a \emph{straight-line grammar} (a straight-line program) if for every non-terminal 
there is a single production rule  (where  is a string of symbols),
and each non-terminal can be assigned a level in such a way that if , then the levels of non-terminals in  are smaller than the level of .
\end{definition}
It follows that a straight-line grammar generates a single string.
Let us describe how we build a grammar representing a given string .
This process uses two functions that we now introduce.

We first define a function \rle{} based on the run-length encoding of a string .
To compute , we divide  into maximal blocks consisting of adjacent equal symbols and replace each block of size at least two with a new non-terminal. Multiple blocks corresponding to the same string are replaced by the same non-terminal.
Note that the result of  is 1-repetition-free.

The second function used in building the representation of a string is , where  is a parameter.
This function also takes a string  and produces another string.
If  is 1-repetition-free, then the resulting string is constant factor shorter (in expectation).

Let us now describe the details.
We first define a family of functions  () each of which uniformly at random assigns  or  to every possible symbol.
To compute , we define blocks in the following way.
If there are two adjacent symbols  such that  and , we mark them as a block.
All other symbols form single-symbol blocks.
Then we proceed exactly like in  function.
Each block of length two is replaced by a new non-terminal.
Again, equal blocks are replaced by the same non-terminal.

Note that the blocks that we mark are non-overlapping.
For example, consider  and assume that , , and .
Then the division into blocks is , and .
In a 1-repetition-free string, each two adjacent symbols are different, so they form a block with probability .
Thus, we obtain the following:
\begin{fact}\label{lem:compress}
If  is a 1-repetition-free string, then  for every .
\end{fact}

In order to build a representation of a string , we repeatedly apply  and , until a string of length one is obtained.
We define:

The \emph{depth} of the representation of , denoted by , is the smallest , such that .
We say that the representation contains  \emph{levels} numbered .
Observe that  is actually a random variable, as its value depends on the choice of random bits .
Using \cref{lem:compress}, we obtain the following upper bound on the depth:

\begin{restatable}{lemma}{smalldepth}\label{lem:small_depth}
If  is a string of length  and , then

\end{restatable}
In the proof we use the following theorem.

\begin{theorem}[Theorem 7 in \cite{Lehre:2013}]\label{thm:drift}
Let  be a stochastic process over some state space , where .
Suppose that there exists some , where , such that .
Let  be the first hitting time.
Then  for all .
\end{theorem}

\begin{proof}[Proof of \cref{lem:small_depth}]
Let  be a random variable defined as .
We have that .

By \cref{lem:compress}, .
Clearly, , which implies .
Thus, .
Since every call to  uses different random bits , distinct calls are independent.
Thus,  we get that .
By applying \cref{thm:drift}, for every  we get

Here,  is the smallest index, such that , which means .
Hence, .
The Lemma follows.
\end{proof}

It follows that in a collection  of  strings of length , we have  
for every  with high probability.

Finally, we describe how to build a straight-line grammar generating the string .
Observe that every call to  replaces some substrings by non-terminals.
Each time a string  is replaced by a non-terminal , we add a production  to the grammar.
Once we compute a string  that has length , the only symbol of this string is the start symbol of the grammar.

\subsection{Representing Multiple Strings}\label{sec:persistent_ds}
We would now like to use the scheme from previous section to build a representation of a collection of strings .
Roughly speaking, we build a union  of the straight-line grammars representing individual strings.
Formally,  is not a grammar because it does not have a start symbol, but it becomes a straight-line grammar once
an arbitrary non-terminal is chosen a start symbol.
Still, we abuse the naming slightly and call  a grammar.

\begin{shortv}
We say that two production rules  and  are \emph{equivalent} if .
We shall make sure that  does not contain two distinct equivalent production rules.
Actually, we even ensure that no two symbols of  generate the same string.

A call to  produces a string, in which some substrings (blocks) in  are replaced by single non-terminals.
We say that each such replacement is a \emph{collapse}.
In the process of building the representation of a single string, for each collapse we add production rules to .
We also assure that equal blocks are collapsed to the same non-terminal.

To build a representation of a collection of strings, we use the same approach globally for all strings.
Hence, we have a global  that contains a list of all previously generated production rules.
Whenever some block is collapsed and we are to add a new production rule , we first check if  already contains an equivalent production rule .
If this is the case, the function replaces  with  and does not update .
Otherwise, the new production rule  is added to , and the substring  is replaced by .
For simplicity, throughout this paper we assume that there is a single grammar  that is being built and it can be accessed from any function that we describe.
\end{shortv}

As the collection of strings  is growing over time, we need to extend the scheme from \cref{sec:single_string} to support multiple strings in such a dynamic setting.
Since our construction is randomized, some care needs to be taken to formalize this process properly.
This is done in full details in \cref{app:formalsymbols}.
In this section we provide a more intuitive description.

Our first goal is to assure that if a particular string is generated multiple times by the same instance of the data structure,
its representation is the same each time.
In order to do that, 
we ensure that if a block of symbols  is replaced by a non-terminal during the construction of the representation of some string of , 
a new non-terminal is created only when the block  is encountered for the first time.
To this end, we use a dynamic dictionary (implemented as a hash table) that maps already encountered blocks to the non-terminals.
Combined with our definition of , this suffices to ensure that each non-terminal in the grammar generates a distinct string.

To keep this overview simple, we assume that the non-terminals come from an infinite set .
The string generated by  is denoted by .
The set  is formally defined so that it is independent from the random choices of the  functions,
which leads to some symbols being inconsistent with those choices. 
In particular many symbols may generate the same string, but exactly one of them is consistent. 
Hence, each string  can be assigned a unique symbol  that generates .
Other symbols do not appear in the grammar we construct, i.e., .

We say that  \emph{represents} a string  if .
A grammar  is called \emph{closed} if all symbols appearing on the right-hand side of a production rule
also appear on the left-hand side.
Our data structure maintains a grammar  which satisfies the following invariant:
\begin{invariant}\label{inv:consistency}
 is closed and represents each .
\end{invariant}


Following earlier works~\cite{Alstrup, Mehlhorn}, our data structure operates on signatures which are just integer identifiers of symbols.
Whenever it encounters a symbol  missing a signature (a newly created non-terminal or a terminal encountered for the first time), it assigns a fresh signature.
At the same time it reveals bits  for .\footnote{The data structure never uses bits  for . To be precise, it fails before they ever become necessary.}
Since the bits are uniformly random, this can be simulated by taking a random machine word.

We use  to denote the set of signatures corresponding to the symbols
that are in the grammar .
We distinguish three types of signatures.
We write  for  if  represents a terminal symbol,  to denote that  represents
a non-terminal  which replaced two symbols 
(represented by signatures , correspondingly)
in .
Analogously, we write
 to denote that  represents a non-terminal introduced by  when
 copies of a symbol  were replaced represented by .
We also set .
\begin{shortv}
 if  is a signature created in , when a substring   was collapsed,
and   if  is a signature created in  when a substring  was collapsed.
The \emph{level} of a signature  is the integer , such that  is created when some substring is collapsed in .
Note that this value is uniquely defined for each signature.
\end{shortv}

For each signature  representing a symbol ,
we store a record containing some attributes of the associated symbol.
More precisely, it contains the following information, which can be stored in  space
and generated while we assign a signature to the symbol it represents: (a) The associated production rule ,  (if it represents a non-terminal) or the terminal symbol  if ; (b) The length  of the string generated from ; (c) The level  of  (which is the level , such that  could be created in the call to ); (d) The random bits  for .

In order to ensure that a single symbol has a single signature,
we store three dictionaries which for each  map  to  whenever ,   to  whenever , and  to  whenever .
Thus, from now on we use signatures instead of the corresponding symbols.

Finally, we observe that a signature representing  can be used to identify  represented by .
In particular, this enables us to test for equality in a trivial way.
However, in order to make sure that updates return deterministic values, we use consecutive integers as handles to elements of 
and we explicitly store the mappings between signature and handles.

\subsection{Parse Trees}\label{sec:pt_overview}
We define two trees associated with a signature  that represents the string .
These trees are not stored explicitly, but are very useful in the description of our data structure.
We call the first one of them the \emph{uncompressed parse tree} and denote it by .
When we are not interested in the signature itself, we sometimes call the tree  instead of .

Consider the strings  stacked on top of each other, in such a way that  is at the bottom.
We define  to be a tree that contains a single node for each symbol of each of these strings.
Each symbol of  originates from one or more symbols of  and this defines how the nodes in  are connected: a node representing some symbol is a parent of all nodes representing the symbols it originates from.
The tree  is rooted with the root being the node representing the only symbol in ,
whose signature is .
Moreover, the children of every node are ordered left-to-right in the natural way.
If  is a node in  distinct from the root, we define  to be the parent node of  in .
We denote by  the -th child of  in .

\begin{figure}[ht]
\hspace{20pt}
\includegraphics[scale=0.6]{ustree.pdf}
\hfill
\includegraphics[scale=0.6]{stree.pdf}
\hspace{20pt}
\caption{Let . Left picture shows the tree , whereas the right one depicts .
The signature stored in every node is written inside the circle representing the node.
We have , , , , , , and .
Moreover, , , ,  and .
Note that the parent of the rightmost leaf has level  in , but the parent of the same leaf in  has level .
}
\label{fig:trees}
\end{figure}
 
Observe that  may contain nodes with just one child.
This happens exactly when some symbol forms a single-symbol block in  or .
Hence, for a signature  we also define a \emph{parse tree} , which is obtained from  by dissolving nodes with one child (similarly as before, we sometimes use the notation  to denote the same tree).
Observe that  is indeed a parse tree, as every node corresponds to an application of a production rule from .
In particular, a node  with signature  has exactly two children if  and  children if .
Thus, each internal node of  has at least two children, which implies that  has  nodes, where .
Similarly to the uncompressed parse trees, each tree  is also rooted and the children of each node are ordered.
Moreover, if  is a node in  distinct from the root, we define  to be the parent node of  in .
See \cref{fig:trees} for an example.

Consider a tree  and its node  representing a symbol with signature .
Observe that if we know , we also know the children of , as they are determined by the production rule associated with .
More generally,  uniquely determines the subtree rooted at~.
On the other hand,  in general does not determine the parent of .
This is because there may be multiple nodes with the same signature, even within a single tree.

We show how to deal with the problem of navigation in the parse trees.
Namely, we show that once we fix a tree  or , we may traverse it efficiently, using the information stored in .
We stress that for that purpose we do not need to build the trees  and  explicitly.

We use \emph{pointers} to access the nodes of .
Assume we have a pointer  to a node  that corresponds to the -th symbol of .
As one may expect, given  we may quickly get a pointer to the parent or child of .
More interestingly, we can also get a pointer to the node  () that lies \emph{right} (or \emph{left}) of  in 
in constant time.
The node  is the node that corresponds to the -th symbol of ,
while  corresponds to the -th symbol.
Note that these nodes may not have a common parent with .
For example, consider  in \cref{fig:trees} and denote the level-0 nodes corresponding to the first three letters (b, a and n) by ,  and .
Then, , but also , although .
The pointers let us retrieve some information about the underlying nodes, including their signatures.
A pointer to the leftmost (rightmost) leaf of  can also be efficiently created.
The full interface of parse tree pointers is described in \cref{sec:pointers}.

In order to implement the pointers to  trees, we first introduce
pointers to compressed trees.
The set of allowed operations on these pointers is more limited, but sufficient
to implement the needed pointers to  trees
in a black-box fashion.
Each pointer to a tree  is implemented as a stack that represents the path from the root of  to the pointed node.
To represent this path efficiently, we replace each subpath that repeatedly descends to the leftmost child by a single entry in the stack (and similarly for paths descending to rightmost children).
This idea can be traced back to a work of Gąsieniec et al.~\cite{DBLP:conf/dcc/GasieniecKPS05}, who implemented constant-time
forward iterators on grammar-compressed strings.
By using a fully-persistent stack, we are able to create a pointer to a neighbor node in constant time, without destroying the original pointer.
However, in order to do this efficiently, we need to quickly handle queries about the leftmost (or rightmost) descendant of a node 
at a given depth, intermixed with insertions of new signatures into .
To achieve that, we use a data structure for finding level ancestors in dynamic trees~\cite{Alstrup:2000}.
The details of this construction are provided in~\cref{sec:pointers}.

\subsection{Comparison to Previous Approaches}\label{sec:comp}

Let us compare our construction with the approaches previously used for similar problems in~\cite{Mehlhorn} and~\cite{Alstrup}.
\cite{Mehlhorn} describes two: a deterministic one (also used in~\cite{Alstrup}) and a randomized one.

Our algorithm for  is simpler than the corresponding procedure in~\cite{Mehlhorn}.
In particular, we may determine if there is a block boundary between two symbols, just by inspecting their values.
In the deterministic construction of~\cite{Mehlhorn}, this requires inspecting  surrounding symbols.

However, the use of randomness in our construction poses some serious challenges,
mostly because the size of the uncompressed parse tree  can be  for a string  of length  with non-negligible probability.
Consider, for example, the string . With probability at least  we have 
for , and consequently  contains at least  nodes. Hence, implementing 
in time proportional to  requires more care.

Another problem is that even prepending a single letter  to a string  results in a string  
whose uncompressed parse tree  might differ from  by  nodes (with non-negligible probability).
In the sample string considered above, the strings  and 
differ by a prefix of length  for  with probability .
In the deterministic construction of~\cite{Mehlhorn}, the corresponding strings may only differ by  symbols.
In the randomized construction of~\cite{Mehlhorn}, the difference is of constant size.
As a result, in~\cite{Mehlhorn,Alstrup}, when the string  is added to the data structure, the modified prefixes of (the counterpart of)  can be computed explicitly, which is not feasible in our case. 

To address the first problem, our grammar is based on the compressed parse trees  and we operate on the uncompressed trees 
only using constant-time operations on the pointers. 
In order to keep the implementation of  and  simple despite the second issue,
we additionally introduce the notion of context-insensitive decomposition, which captures the part of  which is preserved in the parse tree of every superstring of  (including ).
A related concept (of \emph{common sequences} or \emph{cores}) appeared in a report by Sahinalp and Vishkin~\cite{SV1995} and was later used in several
papers including a recent a work by Nishimoto et al.~\cite{DBLP:journals/corr/NishimotoIIBT15}.

Consequently, while the definition of  is simpler compared to  the previous constructions,
and the general idea for supporting  and  is similar,
obtaining the desired running time requires deeper insight and more advanced data-structural tools.

\newcommand{\itstring}{\mathit{pref}}
\newcommand{\itsuf}{\mathit{suff}}

\subsection{Context-Insensitive Nodes}\label{sec:context_short}
In this section we introduce the notion of context-insensitive nodes of  to express
the fact that a significant part of  is ``preserved'' in the uncompressed parse trees of superstrings of .
For example, if we concatenate two strings  and  from , it suffices to take care of the part of  
which is not ``present'' in  or . We formalize this intuition as follows.

Consider a node  of , which represents a particular fragment  of .
For every fixed strings  this fragment can be naturally identified with a fragment of the concatenation .
If  contains a node representing that fragment, we say that  is \emph{preserved} in the \emph{extension} .
If  is preserved for every such extension, we call it \emph{context-insensitive}.
A weaker notion of \emph{left} and \emph{right} context-insensitivity is defined to impose a node
to be preserved in  all \emph{left} and all \emph{right extensions},  i.e., extensions with  and , respectively.

The following lemma captures some of the most important properties of context-insensitive nodes.
Its proof can be found in \cref{sec:contexti}, where a slightly stronger result appears as \cref{cor:context_insensitive}.

\begin{lemma}\label{lem:context_insensitive} Let  be a node of .
  If  is right context-insensitive, so are nodes , , and all children of~.
  Symmetrically, if  is left context-insensitive, so are nodes , , and all children of~.
Moreover, if  is both left context-insensitive and right context-insensitive, then it is context-insensitive.
\end{lemma}

We say that a collection  of nodes in  or in  forms a \emph{layer} if every leaf has exactly one ancestor in .
The natural left-to-right order on  lets us treat every layer as a sequence of nodes.
The sequence of their signatures is called the \emph{decomposition} corresponding to the layer.
Note that a single decomposition may correspond to several layers in , but only one of them (the lowest one) does not contain nodes with exactly one child.

If a layer in  is composed of (left/right) context-insensitive nodes,
we also call the corresponding decomposition (left/right) context-insensitive.
The following fact relates context-insensitivity with concatenation of strings and their decompositions.
It is proven in~\cref{sec:contexti}.

\begin{restatable}{fact}{twodecompositions}\label{fact:two-decompositions}
Let  be a right context-insensitive decomposition of  and let  be a left-context insensitive decomposition of .
The concatenation  is a decomposition of . Moreover, if  and  are context-insensitive, then 
is also context-insensitive.
\end{restatable}

The context-insensitive decomposition of a string may need to be linear in its size.
For example, consider a string  for .
We have .
Thus,  contains a root node with  children.
At the same time, the root node is not preserved in the tree .
Thus, the smallest context-insensitive decomposition of  consists of  signatures.

However, as we show, each string  has a context-insensitive decomposition, whose run-length encoding has length , which is  with high probability.
This fact is shown in~\cref{sec:constructing_cid}.
Let us briefly discuss, how to obtain such a decomposition.
For simplicity, we present a construction of a right context-insensitive decomposition.

Consider a tree .
We start at the rightmost leaf  of this tree.
This leaf is clearly context-insensitive (since all leaves are context-insensitive).
Then, we repeatedly move to .
From~\cref{lem:context_insensitive} it follows that in this way we iterate through right context-insensitive nodes of .
Moreover, nodes left of each  are also right context-insensitive.
In order to build a decomposition, we start with an empty sequence  and every time before we move from  to  we prepend  with the sequence of signatures of all children of  that are left of , including the signature of the node .
In each step we move up , so we make  steps in total.
At the same time,  either has at most two children or all its children have equal signatures.
Thus, the run-length encoding of  has length  and, by using tree pointers, can be computed in time that is linear in its size.
This construction can be extended to computing context-insensitive decompositions of a given substring of , which is used in the  operation.

\subsection{Updating the Collection}
In this section we show how to use context-insensitive decompositions in order to add new strings to the collection.
It turns out that the toolbox which we have developed allows us to handle ,  and  operations in a very simple and uniform way.

Consider a  operation.
We first compute (the run-length encodings of) context-insensitive decompositions of  and .
Denote them by  and .
By~\cref{fact:two-decompositions}, their concatenation  is a decomposition of .
Moreover, each signature in  belongs to .
Let  be the layer in  that corresponds to .
In order to ensure that  represents , it suffices to add to  the signatures of all nodes of  that lie above .
This subproblem appears in ,  and  operations and we sketch its efficient solution below.

Consider an algorithm that repeatedly finds a node  of the lowest level that lies above  and substitutes in  the children of  with .
This algorithm clearly iterates through all nodes of  that lie above .
At the same time, it can be implemented to work using just the run-length encoding of the decomposition  corresponding to  instead of  itself.
Moreover, its running time is only linear in the length of the run-length encoding of  and the depth of the resulting tree. 
For details, see~\cref{lem:compressdec}.

Thus, given the run-length encoding of decomposition of  we can use the above algorithm to add to  the signatures of all nodes of  that lie above .
The same procedure can be used to handle a  operation (we compute a context-insensitive decomposition of the prefix and suffix and run the algorithm on each of them) or even a  operation.
In the case of  operation, the sequence of letters of  is a trivial decomposition of , so by using the algorithm, we can add it to the data structure in  time.
Combined with the efficient computation of the run-length encodings of context-insensitive decompositions, this gives an efficient way of handling ,  and  operations.
See~\cref{sec:conclusions} for precise theorem statements.

\section{Navigating through the Parse Trees}\label{sec:pointers}
In this section we describe the notion of \emph{pointers} to trees  and  that has been introduced in~\cref{sec:pt_overview} in detail.
Although in the following sections we mostly make use of the pointers
to  trees, the pointers to compressed parse trees
are essential to obtain an efficient implementation
of the pointers to uncompressed trees.

Recall that a pointer is a handle that can be used to access some node of a tree  or  in constant time.
The pointers for trees  and  are slightly different and we describe them separately.

The pointer to a node of  or  can be created for any string represented by  and, in fact, for any existing signature .
Once created, the pointer points to some node  and cannot be moved.
The information on the parents of  (and, in particular, the root )
is maintained as a part of the internal state of the pointer.
The state can possibly be shared with other pointers.

In the following part of this section, we first describe the pointer interface.
Then, we move to the implementation of pointers to  trees.
Finally, we show that by using the pointers to  trees in a black-box fashion, we may obtain pointers to  trees.

\subsection{The Interface of Tree Pointers}
We now describe the pointer functions we want to support.
Let us begin with pointers to the uncompressed parse trees.
Fix a signature  and
let , where  corresponds to the symbol .
First of all, we have three primitives for creating pointers:
\begin{itemize}
  \item  -- a pointer to the root of .
  \item  () -- a pointer to the leftmost (rightmost) leaf of .
\end{itemize}
We also have six functions for navigating through the tree.
These functions return appropriate pointers or  if the respective nodes do not exist.
Let  be a pointer to .
\begin{itemize}
  \item  -- a pointer to .
  \item  -- a pointer to the -th child of  in .
  \item  () -- a pointer to the node  (, resp.).
  \item  () --
    let  be the nodes to the right (left) of 
    in the -th level in , in the left to right (right to left) order
    and let  be the largest integer such that
    all nodes  correspond to the same signature as .
    If  and  exists, return a pointer to ,
    otherwise return .

\end{itemize}
Note that the nodes  and  might not have a common parent with the node .


Additionally, we can retrieve information about the pointed node  using several functions:
\begin{itemize}
  \item  -- the signature corresponding to , i.e., .
  \item  -- the number of children of .
  \item  -- if , an integer  such that , otherwise .
  \item  -- the level of , i.e., the number of edges on the path from  to the leaf of  (note that this may not be equal to , for example if  has a single child that represents the same signature as ).
  \item  -- a pair of indices  such that  represents .
  \item  () -- let  be the nodes to the right (left) of 
    in the -th level in , in the left to right (right to left) order.
    Return the maximum  such that
    all nodes  correspond to the same signature as .
\end{itemize}

Moreover, the interface provides a way to check if two pointers ,  to
the nodes of the same tree  point to the same node:
 is clearly equivalent to .

The interface of pointers for trees  is more limited.
The functions , , , , ,
, ,  and  are defined
analogously as for uncompressed tree pointers.
The definitions of  and  are slightly different.
In order to define these functions, we introduce the notion of
the \emph{level- layer} of .
The level- layer  (also denoted by )
is a list of nodes  of  such that 
and either  or , ordered from the leftmost to the rightmost nodes.
In other words, we consider a set of nodes of level  in  and then replace each node with its first descendant (including itself) that belongs to .
This allows us to define  and :
\begin{itemize}
  \item  () -- assume .
    Return a pointer to the node to the right (left) of  on , if such node exists.
\end{itemize}
For example, assume that  points to the leftmost leaf in  in \cref{fig:trees}.
Then  is the pointer to the leaf representing the first , but  is the pointer to the node representing the string .

\begin{remark}The functions  and  operating on  trees
(defined above)
are only used for the purpose of implementing the pointers to uncompressed
parse trees.
\end{remark}
\subsection{Pointers to Compressed Parse Trees}
As we later show, the pointers to  trees can be implemented using pointers to  in a black-box fashion.
Thus, we first describe the pointers to trees .

\begin{lemma}\label{lem:ptr}
Assume that the grammar  is growing over time.
We can implement the entire interface of pointers to trees  where

in such a way that all operations take  worst-case time.
The additional time needed to update the shared pointer state is also  per creation of a new signature.
\end{lemma}
Before we embark on proving this lemma, we describe auxiliary
data structures constituting the shared infrastructure for pointer
operations.
Let  and let  be the tree 
with the nodes of levels less than  removed.
Now, define  and  to be the signatures
corresponding to the leftmost and rightmost
leaves of , respectively.
We aim at being able to compute functions  and  in
constant time, subject to insertions of new signatures
into .

\newcommand{\bmask}{\mathtt{b}}
\newcommand{\bcnt}{\mathtt{bcnt}}

\begin{lemma}
  We can maintain a set of signatures  subject
to signature insertions and queries  and
, so that both insertions and queries take constant time.
\end{lemma}

\begin{proof}
For brevity, we only discuss the implementation of , as  can
be implemented analogously.
We form a forest  of rooted trees on the set , such that each
 is a root of some tree in .
Let .
If , then  is a parent of  in  and
if , where , then  is
a parent of .
For each  we also store a -bit mask  with
the -th bit set if some (not necessarily proper) ancestor of  in  has
level equal to .
The mask requires a constant number of machine words.
When a new signature  is introduced, a new leaf has to be added
to .
Denote by  the parent of  in .
The mask  can be computed in  time: it is equal
.
We also store a dictionary  that maps each introduced value 
to the number of bits set in .
Note that since , .

Now, it is easy to see that  is the highest (closest to the root)
ancestor  of  in , such that .
In order to find , we employ the \emph{dynamic level ancestor}
data structure of Alstrup et al.~\cite{Alstrup:2000}.
The data structure allows us to maintain a dynamic forest subject
to leaf insertions and queries for the -th nearest ancestor
of any node.
Both the updates and the queries are supported in worst-case  time.
Thus, we only need to find such number  that  is
the -th nearest ancestor of  in .
Let  be the mask  with the bits numbered from  to 
cleared.
The mask  can be computed in constant time with standard bitwise operations.
Note that  for some ancestor  of 
and as a result  contains the key .
Hence,  is the number of bits set in .
Also,  denotes the number of ancestors of  at levels less than .
Thus  is the number of ancestors
of  at levels not less than .
Consequently, the -th nearest ancestor of  is the needed node .
\end{proof}

Equipped with the functions  an , we now move to the proof
of \cref{lem:ptr}.
\begin{proof}[Proof of \cref{lem:ptr}]
  Let  be such that .
  Recall that we are interested in pointers to  for some fixed
.
A functional stack will prove to be useful in the following description.
For our needs, let the functional stack be a data structure storing
values of type . Functional stack has the following interface:
\begin{itemize}
  \item  -- return an empty stack
  \item  -- return a stack  equal to  with a value  on top.
  \item  -- return a stack  equal to  with the top value removed.
  \item  -- return the top element of the stack .
\end{itemize}
The arguments of the above functions are kept intact.
Such stack can be easily implemented as a functional list \cite{Okasaki:1999}.

Denote by  the root of .
The pointer  to  is implemented as a functional stack containing
some of the ancestors of  in , including  and ,
accompanied with some additional information (a  pointer
is represented by an empty stack).
The stack is ordered by the levels of the ancestors, with 
lying always at the top.
Roughly speaking, we omit the ancestors being the internal nodes
on long subpaths of  descending to the leftmost (rightmost)
child each time. Gąsieniec et al.~\cite{DBLP:conf/dcc/GasieniecKPS05} applied a similar idea
to implement forward iterators on grammar-compressed strings (traversing only the leaves of the parse tree).

More formally, let  be the set of ancestors of  in ,
including .
The stack contains an element per each , where
 ( respectively) is a set of such ancestors  that  simultaneously:
\begin{itemize}
  \item[(1)] contains  in the subtree rooted at its leftmost (rightmost resp.) child,
  \item[(2)] is the leftmost (rightmost) child of .
\end{itemize}
The stack elements come from the set
.
Let for any node  of ,  denote an
integer such that  represents the substring of  starting at .
If , then the stack contains the entry .
If  is the leftmost child of its parent in , then the stack
contains the entry .
Similarly, for  which is the rightmost child of ,
the stack contains .
Otherwise,  is the -th child of some -th power signature, where
, and the stack contains  in this case.
Note that the top element of the stack is always of the form .

Having fixed the internal representation of a pointer , we are
now ready to give the implementation of the -pointer interface.
In order to obtain , we take the first coordinate of the top element of .
The  can be easily read from  and .

Let us now describe how  works.
If  points to the root, then the return value is clearly .
Otherwise, let the second coordinate of the top element be .
We have that .
If , we may simply return .
Otherwise, if ,  returns .
When , we return .

The third ``offset'' coordinate of the stack element  is only
maintained for the purpose of computing the value , which
is equal to .
We show that the offset  of any entry depends
only on ,  and the previous preceding stack element 
(the case when  is the root of  is trivial).
Recall that  is an ancestor of  in .
\begin{itemize}
\item If , then on the path  in 
  we only descend to the leftmost children, so .
\item If , then ,  and
 is the parent of  in .
Hence,  in this case.
\item If , then on the path  in 
  we only descend to the rightmost children, so
  .
\end{itemize}
For the purpose of clarity, we hide the third coordinate of the stack element
in the below discussion, as it can be computed according to the above
rules each time  is called.
Thus, in the following we assume that the stack contains
elements from the set .

The implementation of the primitives ,  and 
is fairly easy:  returns ,
 returns the pointer
,
whereas  returns .

It is beneficial to have an auxiliary
function .
If the stack  contains  as the top element
and  as the next element, where ,
 returns a stack  with  removed, i.e.
.
Now, to execute , we compute the -th child  of 
from  and return ,
where  if ,  if  has exactly
 children and  otherwise.

The  is equal to  if ,
for an integer  and .
If, however, , then the actual parent
of  might not lie on the stack.
Luckily, we can use the  function: if  is the second-to top stack
entry, then the needed parent is .
Thus, we have 
in this case.
The case when  is analogous -- we use 
instead of .

To conclude the proof, we show how to implement  (the implementation
of  is symmetric).
We first find  -- a pointer to the nearest ancestor  of  such that  is not in
the subtree rooted at 's rightmost-child.
To do that, we first skip the ``rightmost path'' of nearest ancestors
of  by computing a pointer  equal to  if  is
not of the form  and  otherwise.
The pointer  can be now computed by calling .
Now we can compute the index  of the child  of  such that
 contains the desired node: if , then , and if , then .
The pointer  to  can be obtained by calling ,
where  is equal to  if  is the rightmost child
of  and  otherwise.
The exact value of the pointer  depends on whether
 has level no more than .
If so, then .
Otherwise, .

To sum up, each operation on pointers take worst-case  time, as they
all consist of executing a constant number of functional stack operations.
\end{proof}

\subsection{Pointers to Compressed Parse Trees}
We now show the implementation of pointers to uncompressed parse trees.

\begin{lemma}\label{lem:uptr}
Assume that the grammar  is growing over time.
We can implement the entire interface of pointers to trees  where

in such a way that all operations take  worst-case time.
The additional worst-case time needed to update the shared pointer state is also  per creation of new signature.
\end{lemma}

\begin{proof}
To implement the pointers to the nodes of a tree  we use pointers to the nodes of  (see \cref{lem:ptr}).
Namely, the pointer  to a tree  is a pair , where  is a pointer to  and  is a level.

Recall that  can be obtained from  by dissolving nodes with one child.
Thus, a pointer  to a node  of  is represented by a pair , where:
\begin{itemize}
\item If ,  points to . Otherwise, it points to the first descendant
  of  in  that is also a node of ,
\item  is the level of  in .
\end{itemize}

Thus,  simply returns .
The operations  and  can be implemented directly, by calling the corresponding operations on .
Observe that the desired nodes are also nodes of .
Moreover, thanks to our representation, the return value of  is .

To run  we consider two cases.
If the parent of  is a node of , then  returns .
Otherwise, it simply returns .
Note that we may detect which of the two cases holds by inspecting the level of .

The implementation of  is similar.
If the node pointed by  is also a node of , we return .
Otherwise,  has to be equal to  and we return .

If  does exist and is a node of , the return value of  is the same as the return value of .
Otherwise, the parent of the node pointed by  either has a single child or does not exist, so  returns .

In the function  we have two cases.
If  is odd, then , as the odd
levels of  do not contain adjacent equal signatures.
The same applies to the case when  points to the root of .
Otherwise, the siblings of  constitute a block of equal signatures
on the level .
Thus, we return .
The implementation of  is analogous.

 returns  and  returns .

Finally,  can be used to implement  ( is symmetric).
If , then .
Also,  holds in a trivial way.
In the remaining case we have  and it follows that 
for .
Thus, we return .
\end{proof}

\begin{remark}
In the following we sometimes use the pointer notation
to describe a particular node of .
For example, when we say ``node '', we actually
mean the node  such that  points to .
\end{remark}


\section{Context-Insensitive Nodes}\label{sec:contexti}
In this section we provide a combinatorial characterization of context-insensitive nodes,
introduced in \cref{sec:context_short}.
Before we proceed, let us state an important property of our way of parsing,
which is also applied in \cref{sec:om} to implement longest common prefix queries.

\begin{lemma}\label{lem:lcpd}
Let ,  and let  be the longest common prefix of  and .
There exists a decomposition  such that  and  is the longest common prefix
of  and .
\end{lemma}
\begin{proof}
Observe that the longest common prefix of  and 
expands to a common prefix of  and . We set  to be this prefix and  to be the corresponding
suffix of . If  we have nothing more to prove and thus suppose .

Note that  starts at the last position of  before which both
 and  place block boundaries.
Recall that  places a block boundary between two positions only based on the characters on these positions.
Thus, since the blocks starting with  are (by definition of ) different in  and ,
in one of these strings, without loss of generality in , this block extends beyond .
Consequently,  is a proper prefix of a block. Observe, however,
that a block may contain more than one distinct character only if its length is exactly two.
Thus,  as claimed.
\end{proof}

Next, let us recall the notions introduced in \cref{sec:context_short}. Here, we provide slightly more formal definitions.
For arbitrary strings , ,  we say that  (which is formally a triple )
is \emph{extension} of . We often identify the extension with the underlying concatenated string ,
remembering, however, that a particular occurrence of  is distinguished.
If  or , we say that  is a \emph{right extension} or \emph{left extension}, respectively.

Let us fix an extension  of . Consider a node  at level  of .
This node represents a particular fragment  which has a naturally corresponding fragment of the extension.
We say that a node  at level  of  is a \emph{counterpart} of  with respect to 
(denoted ) if  and  represents the fragment of  which naturally corresponds to .

\begin{lemma}\label{lem:close}
If a level- node  has a counterpart  with respect to a right extension ,
then  for any integer , , and .
Moreover, if , then .
In particular, the nodes do not exist for  if and only if the respective nodes do not exist for .


Analogously, if  has a counterpart  with respect to a left extension  ,
then  for any integer , , and .
Moreover, if , then .
\end{lemma}

\begin{proof}
Due to symmetry of our notions, we only prove statements concerning the right extension.
First, observe that  simply follows from the construction
of a parse tree and the definition of a counterpart.

Next, let us prove that . If , this is clear since
all leaves of  have natural counterparts in .
Otherwise, we observe that  holds for every .
This lets us apply the inductive assumption
for the nodes  at level  to conclude that .

This relation can be iteratively deduced for all nodes to the left of  and ,
and thus  and  represent symbols of  and  within the longest
common prefix  of these strings.
\cref{lem:lcpd} yields a decomposition  where  is the longest common prefix of 
and  while . Clearly, nodes at level  representing symbols within this longest
common prefix are counterparts.
Thus, whenever  represents a symbol within  and  is its counterpart with respect to ,
we have .

Observe that  means that  represents a symbol within , and this immediately
yields .
Also, both in  and in  nodes representing symbols in  have a common parent, so
 holds irrespective of  and  representing symbols within  or within .
\end{proof}

\begin{lemma}\label{lem:lr}
Let . If  has counterparts with respect to both  and , then 
has a counterpart with respect to .
\end{lemma}
\begin{proof}
We proceed by induction on the level  of . All nodes of level  have natural counterparts with respect to any extension,
so the statement is trivial  for , which lets us assume .
Let  and  be the counterparts of  with respect to  and , respectively.
Also, let  be the children of .
Note that these nodes have counterparts  with respect to  and
 with respect to .
Consequently, by the inductive hypothesis they have counterparts  with respect to .

Observe that  when  is viewed as an extension  of .
Now, \cref{lem:close} gives 
and thus nodes  have a common parent . We shall prove
that  is the counterpart of  with respect to . For this it suffices to prove
that it does not have any children to the left of  or to the right of .
However, since  and  is the rightmost child of ,
 must be the rightmost child of .
Next, observe that  when  is viewed as an extension  of .
Hence, \cref{lem:close} implies that ,
i.e., . As  is the leftmost child of ,  must be the leftmost child of .
\end{proof}

A node  is called \emph{context-insensitive} if it is preserved in any extension of ,
and \emph{left} (\emph{right}) \emph{context-insensitive}, if it is preserved in any left (resp. right) extension.
The following corollary translates the results of \cref{lem:close,lem:lr}
in the language of context-insensitivity. Its weaker version is stated in \cref{sec:context_short} as \cref{lem:context_insensitive}.

\begin{corollary}\label{cor:context_insensitive} Let  be a node of .
\begin{enumerate}[(a)]
\item\label{it:left} If  is right context-insensitive, so are nodes , , and all children of~.
Moreover, if , then  is also right context-insensitive.
\item\label{it:right} If  is left context-insensitive, so are nodes , , and all children of~.
Moreover, if , then  is also left context-insensitive.
\item\label{it:both} If  is both left context-insensitive and right context-insensitive, it is context-insensitive.
\end{enumerate}
\end{corollary}


We say that a collection  of nodes in  or in  forms a \emph{layer} if every leaf has exactly one ancestor in .
Equivalently, a layer is a maximal antichain with respect to the ancestor-descendant relation.
Note that the left-to-right order of  gives a natural linear order on , i.e.,  can be treated as a sequence of nodes.
The sequence of their signatures is called the \emph{decomposition} corresponding to the layer.
Observe a single decomposition may correspond to several layers in , but only one of them does not contain nodes with exactly one child.
In other words, there is a natural bijection between decompositions and layers in .

We call a layer (left/right) context-insensitive if all its elements are (left/right) context-insensitive.

We also extend the notion of context insensitivity to the underlying decomposition.
The decompositions can be seen as sequences of signatures.
For two decompositions ,  we define their concatenation  to be
the concatenation of the underlying lists.
The following fact relates context-insensitivity with concatenation of words and their decompositions.

\twodecompositions*


\begin{proof}
Let  and  be layers in  and  corresponding to  and , respectively.
Note that  can be seen as a right extension  of  and as a left extension  of .
Thus, all nodes in  have counterparts in  and these counterparts clearly form a layer.
Consequently,  is a decomposition of .
To see that  is context-insensitive if  and  are, it suffices to note that any extension 
of  can be associated with extensions  of  and  of .
\end{proof}

\begin{fact}\label{fct:layer}
Let  be adjacent nodes on a layer .
If  and  correspond to the same signature , they are children of the same parent.
\end{fact}
\begin{proof}
  Let  and note that both  and  both belong to ,
i.e., they represent adjacent symbols of .
However,  never places a block boundary between two equal symbols.
Thus,  and  have a common parent at level .
\end{proof}

\section{Adding New Strings to the Collection}\label{sec:adding}

\subsection{Constructing Context-Insensitive Decompositions}\label{sec:constructing_cid}
Recall that the run-length encoding partitions a string into maximal blocks (called \emph{runs}) of equal characters
and represents a block of  copies of symbol  by a pair , often denoted as .
For , we simply use  instead of  or . 
In \cref{sec:single_string}, we defined the  function operating on symbols as a component of our parse scheme.
Below, we use it just as a way of compressing a sequence of signatures.
Formally, the output of the  function on a sequence of items
is another sequence of items,
where each maximal block of  consecutive items  is replaced with a single item .

We store run-length encoded sequences as linked lists. This way we can create a new sequence consisting of  copies
of a given signature  (denoted by ) and concatenate two sequences ,  (we use
the notation ), both in constant time.
Note that concatenation of strings does not directly correspond to concatenation of the underlying lists:
if the last symbol of the first string is equal to the first symbol of the second string, two blocks need to be merged.

Decompositions of strings are sequences of symbols, so we may use  to store them space-efficiently.
As mentioned in \cref{sec:context_short}, this is crucial for context-insensitive decompositions.
Namely, string  turns out to have a context-insensitive decomposition  such that .
Below we give a constructive proof of this result.

\begin{algorithm}[ht]
\begin{algorithmic}[1]
\Function{}{}
\State 
\State 
\State 
\While{ \textbf{and} }
\If{ \textbf{and} }\label{alg:layer:if1}
\State  \Comment{Pointers  and  are set for the purpose of analysis only.}
\State 
\Else
\State 
\State	
\State	
\EndIf
\If{ \textbf{and} }\label{alg:layer:if2}
\State 
\State 
\Else
\State 
\State	
\State	
\EndIf
\EndWhile
\If{}
\State \Return 
\Else
\State \Return 
\EndIf
\EndFunction
\end{algorithmic}
\caption{Compute a context-insensitive decomposition of a string  given by a signature .}
\label{alg:layer}
\end{algorithm}

\begin{lemma}\label{lem:layerimpl}
Given , one can compute the run-length encoding of a context-insensitive decomposition of  in  time.
\end{lemma}

\begin{proof}
The decomposition is constructed using procedure  whose implementation is given as \cref{alg:layer}.
Let us define  as the -th iterate of  (where )
and  as the -th iterate of .
Correctness of \cref{alg:layer} follows from the following claim.

\begin{claim}
Before the -th iteration of the \textbf{while} loop,
 points to a left context-insensitive node at level  and  points to a right context-insensitive node at level .
Moreover  for some  and 
is a context-insensitive decomposition of .
\end{claim}
\begin{proof}
Initially, the invariant is clearly satisfied with  because all leaves are context-insensitive.
Thus, let us argue that a single iteration of the \textbf{while} loop preserves the invariant.
Note that the iteration is performed only when .

Let  and  be the values of  and  before the -th iteration
of the \textbf{while} loop.
Now assume that the loop has been executed for the -th time.
Observe that  and  for some 
and from  it follows that .
Also,  and  are extended so that
 is equal to the decomposition
we got from the invariant. Moreover, observe that  points to the leftmost child of the new value of ,
and  points to the rightmost child of the new~. Consequently, after these values are set, we have  for some 
and  is a decomposition of .

Let us continue by showing that the new values of  and  satisfy the first part of the invariant.
Note that  implies  and  are not set to .
In the \textbf{if} block at line~\ref{alg:layer:if1}, \cref{cor:context_insensitive}(\ref{it:right}) implies
that  indeed points to a left context-insensitive at level .
The same holds for  in the \textbf{else} block.
A symmetric argument shows that  is set to point to a right context-insensitive node at level .

Along with \cref{cor:context_insensitive}(\ref{it:both}) these conditions imply
 that all nodes  for  are context-insensitive and thus the new representation is context-insensitive.
\end{proof}

To conclude the proof of correctness of \cref{alg:layer}, we observe that after leaving the main loop
the algorithm simply constructs the context-insensitive decomposition mentioned in the claim.
Either  points at the root of  or .
If  points at the root or , we only need to add a single
run-length-encoded entry between  and .
Otherwise,  has exactly two children  and  with
different signatures, so we need to add two entries
 and  between  and .
\end{proof}

We conclude with a slightly more general version of \cref{lem:layerimpl}.
\begin{lemma}\label{lem:layerimpl2}
Given  and lengths  such that , one can compute in time 
a run-length-encoded context-insensitive decomposition of .
\end{lemma}
\begin{proof}
The algorithm is the same as the one used for  \cref{lem:layerimpl}.
The only technical modification is that the initial values of  and  need to be set to leaves representing the first and the last position of 
in . In general, we can obtain in  time a pointer to the -th leftmost leaf of .
This is because each signature can be queried for its length. Note that for nodes with more than two children we cannot scan
them in a linear fashion, but instead we need to exploit the fact that these children correspond to fragments of equal length
and use simple arithmetic to directly proceed to the right child.

Finally, we observe that by \cref{lem:close}, the subsequent values of  and  in the implementation on 
are counterparts of the values of  and  in the original implementation on .
\end{proof}

\begin{remark}\label{remark:left-context}
In order to obtain a left context-insensitive decomposition we do not need to maintain the list  in \cref{alg:layer} and it suffices to set  at each iteration. Of course, \cref{lem:layerimpl2} needs to be restricted to left extensions
. A symmetric argument applies to right context-insensitive decompositions.
\end{remark}

\subsection{Supporting Operations}
We now use the results of previous sections to describe the process of updating the collection
when , , and  operations are executed.
Recall that we only need to make sure that the grammar 
represents the strings produced by these updates.
The core of this process is the following algorithm.

\begin{algorithm}
\begin{algorithmic}[1]
\Function{}{}
\While{}
\State  a lowest-level node of  among all proper ancestors of \label{line:ancestor}
\State 
\EndWhile
\EndFunction
\end{algorithmic}
\caption{Iterate through all nodes above a layer }
\label{alg:layers}
\end{algorithm}

\begin{lemma}
\cref{alg:layers} maintains a layer  of .
It terminates and upon termination, the only element of  is the root of .
Moreover, line~\ref{line:ancestor} is run exactly once per every proper ancestor of the initial  in 
\end{lemma}

\begin{proof}
Consider line~\ref{line:ancestor} of \cref{alg:layers}.
Clearly,  and every child of  belongs to .
Recall that every node in , in particular , has at least two children.
Thus, by replacing the fragment of  consisting of all children of  with  we obtain a new layer , such that .
Hence, we eventually obtain a layer consisting of a single node.
From the definition of a layer we have that in every parse tree there is only one single-node layer and its only element is the root of .
The Lemma follows.
\end{proof}

We would like to implement an algorithm that is analogous to \cref{alg:layers}, but operates on decompositions, rather than on layers.
In such an algorithm in every step we replace some fragment  of a decomposition  with a signature  that generates .
We say that we \emph{collapse} a production rule .

In order to describe the new algorithm, for a decomposition  we define the set of \emph{candidate production rules} as follows.
Let  be the run-length encoding of a decomposition .
We define two types of candidate production rules.
First, for every  we have rules .
Moreover, for  we have .
The \emph{level} of a candidate production rule  is the level of the signature .

The following lemma states that collapsing a candidate production rule of minimal level corresponds to executing one iteration of the loop in \cref{alg:layers}.
\begin{lemma}\label{lem:cpr}
Let  be a layer of  and let  be the corresponding decomposition.
Consider a minimum-level candidate production rule  for .
Let  be the nodes of  corresponding to .
These nodes are the only children of a node  which satisfies .
\end{lemma}
\begin{proof}
Let  and let  be a minimum-level node above .
Observe that all children of  belong to  and the sequence of their signatures forms a substring of .
By \cref{fct:layer} this substring is considered while computing candidate productions, and the level of the production
is clearly .
Consequently, all nodes above  have level at least .
In other words, every node of  has an ancestor in .
Since , we have in particular ,
i.e., the corresponding signatures  form a substring of .
We shall prove that they are a single block formed by .
It is easy to see that no block boundary is placed between these symbols.
If  is even, we must have  and  simply cannot form larger blocks.
Thus, it remains to consider odd  when  for a single signature .
We shall prove that the block of symbols  in  is not any longer. By \cref{fct:layer}
such a block would be formed by siblings of . However, for each of them an ancestor must belong to .
Since their proper ancestors coincide with proper ancestors of , these must be the sibling themselves.
This means, however, that a block of symbols  in  corresponding to  was not maximal,
a contradiction.
\end{proof}

\begin{lemma}\label{lem:compressdec}
Let  be a layer in  and  be its corresponding decomposition.
Given a run-length encoding of  of length , we may implement an algorithm that updates  analogously to \cref{alg:layers}.
It runs in  time.
The algorithm fails if .
\end{lemma}

\begin{proof}
By \cref{lem:cpr}, it suffices to repeatedly collapse the candidate production rule of the smallest level.
We maintain the run-length encoding of the decomposition  as a doubly linked list , whose every element corresponds to a signature and its multiplicity.
Moreover, for  we maintain a list  of candidate production rules of level .
The candidate production rules of higher levels are ignored.
With each rule stored in some  we associate pointers to elements of , that are removed when the rule is collapsed.
Observe that collapsing a production rule only affects at most two adjacent elements of , since we maintain a run-length encoding of the decomposition.

Initially, we compute all candidate production rules for the initial decomposition .
Observe that this can be done easily in time proportional to .
Then, we iterate through the levels in increasing order.
When we find a level , such that the list  is nonempty, we collapse the production rules from the list .
Once we do that, we check if there are new candidate production rules and add them to the lists  if necessary.
Whenever we find a candidate production rule  to collapse, we first check if  already contains a signature  associated with an equivalent production rule  for some .
If this is the case, the production rule from  is collapsed.
Otherwise, we collapse the candidate production rule, and add signature  to .
Note that this preserves \cref{inv:consistency}.
Also, this is the only place where we modify .

Note that the candidate production rules are never removed from the lists .
Thus, collapsing one candidate production rule may cause some production rules in lists  () to become obsolete (i.e., they can no longer be collapsed).
Hence, before we collapse a production rule, we check in constant time whether it can still be applied, i.e., the affected signatures still exist in the decomposition.

As we iterate through levels, we collapse minimum-level candidate production rules.
This process continues until the currently maintained decomposition has length .
If this does not happen before reaching level , the algorithm fails.
Otherwise, we obtain a single signature that corresponds to a single-node layer in .
The only string in such a layer is the root of , so the decomposition of length  contains solely the signature of the entire string .
As a result, when the algorithm terminates successfully, the signature of  belongs to .
Hence, the algorithm indeed adds  to .

Regarding the running time, whenever the list  is modified by collapsing a production rule, at most a constant number of new candidate production rules may appear and they can all be computed in constant time, directly from the definition.
Note that to compute the candidate production rule of the second type, for two distinct adjacent signatures  and  in  we compute the smallest index , such that  and .
This can be done in constant time using bit operations, because the random bits  are stored in machine words.
Since we are only interested in candidate production rules of level at most , we have enough random bits to compute them.

Every time we collapse a production rule, the length of the list  decreases.
Thus, this can be done at most  times.
Moreover, we need  time to compute the candidate production rules for the initial decomposition.
We also iterate through  lists .
In total, the algorithm requires  time.
\end{proof}

\begin{corollary}\label{cor:decomposition-to-sig}
Let  be a decomposition of a string .
Assume that for every signature  in  we have .
Then, given a run-length encoding of  of length , we may add  to  in  time.

The algorithm preserves \cref{inv:consistency} and fails if .
\end{corollary}

Using \cref{cor:decomposition-to-sig} we may easily implement ,  and  operations.

\begin{lemma}\label{lem:makeop}
Let  be a string of length .
We can execute  in  time.
This operation fails if .
\end{lemma}

\begin{proof}
Observe that  (treated as a sequence) is a decomposition of .
Thus, we may compute its run-length encoding in  time and then apply \cref{cor:decomposition-to-sig}.
\end{proof}

\begin{lemma}\label{lem:splitop}
Let  be a string represented by  and .
Then, we can execute a  operation in  time.
This operation fails if .
\end{lemma}

\begin{proof}
Using \cref{lem:layerimpl2} we compute the run-length-encoded decompositions of  and  in  time.
Then, it suffices to apply \cref{cor:decomposition-to-sig}.
\end{proof}

\begin{lemma}\label{lem:concop}
Let  and  be two strings represented by .
Then, we can execute a  operation in  time.
The operation fails if .
\end{lemma}

\begin{proof}
We use \cref{lem:layerimpl} to compute the run-length-encoded context-insensitive decompositions of  and  in  time.
By \cref{fact:two-decompositions} the concatenation of these decompositions is a decomposition of .
Note that given run-length encodings of two strings, we may easily obtain the run-length encoding of the concatenation of the strings.
It suffices to concatenate the encodings and possibly merge the last symbol of the first string with the first symbol of the second one.
Once we obtain the run-length encoding of a decomposition of , we may apply \cref{cor:decomposition-to-sig} to complete the proof.

Note that for the proof, we only need a right-context insensitive decomposition of  and a left-context insensitive decomposition of .
Thus, we apply the optimization of \cref{remark:left-context}.
However, it does not affect the asymptotic running time.
\end{proof}

\subsection{Conclusions}\label{sec:conclusions}

The results of this section are summarized in the following theorem.

\begin{theorem}\label{thm:data_structure}
There exists a data structure which maintains a family of strings  and supports  and  in 
time,  in  time, where  is the total length of strings involved in the operation and 
is the total input size of the current and prior updates (linear for each , constant for each  and ).
An update may fail with probability  where  can be set as an arbitrarily large constant.
The data structure assumes that total length of strings in  takes at most  bits in the binary representation.
\end{theorem}

\begin{proof}
Consider an update which creates a string  of length .
By \cref{lem:small_depth}, we have that 
Hence, an update algorithm may fail as soon as it notices that .
The total length of strings in  is at least , and thus .
We extend the machine word to  bits, which results in a constant factor overhead in the running time.

This lets us assume that updates implemented according to \cref{lem:makeop,lem:splitop,lem:concop} do not fail.
By \cref{lem:makeop},  runs in  time, and, by \cref{lem:splitop,lem:concop}, both  and  run in  time.
Since we may terminate the algorithms as soon as  turns out to be larger than ,
we can assume that  runs in  time, while  and  run in  time.
\end{proof}

Note that the failure probability in \cref{thm:data_structure} is constant for the first few updates. 
Thus, when the data structure is directly used as a part of an algorithm, the algorithm could fail with constant probability.
However, as discussed in \cref{app:model}, restarting the data structure upon each failure is a sufficient mean to eliminate failures 
still keeping the original asymptotic running time with high probability with respect to the total size of updates (see~\cref{lem:restart}).
Below, we illustrate this phenomenon in a typical setting of strings with polynomial lengths.

\begin{corollary}\label{cor:persistent}
Suppose that we use the data structure to build  a dynamic collection of  strings, each of length at most .
This can be achieved by a Las Vegas algorithm whose running time with high probability is  where 
is the total length of strings given to  operations and  is the total number of updates (, , and ).
\end{corollary}

\begin{proof}
We use \cref{thm:data_structure} and restart the data structure in case of any failure.
Since the value of  fits in a machine word (as this is the assumption in the word RAM model),
by extending the machine word to  bits, we may ensure that the total length of strings in  (which is )
fits in a machine word.  Moreover, , so , i.e., the running time of  is 
and of  and  is .
We set  in \cref{thm:data_structure} as a sufficiently large constant
so that \cref{lem:restart} ensures that the overall time bound holds with the desired probability.
\end{proof}

\section{Dynamic String Collections: Lower Bounds}\label{sec:lb}

In this section we prove a lower bound for any data structure maintaining a dynamic collection
of strings under operations ,  and  and
answering equality queries . The strings are not persistent, that is, both 
and  destroy their arguments and return new strings. Furthermore,
 only needs to support comparing strings of length one.
In such a setting, if we are maintaining  strings of length at most ,
the amortized complexity of at least one operations among , ,  is
. The lower bound applies to any Monte Carlo structure returning correct answers
w.h.p.

The lower bound is based on a reduction from dynamic connectivity. A well-known
result by P\v{a}tra\c{s}cu and Demaine~\cite{logarithmic} is that any data structure maintaining
an undirected graph under operations ,  and answering queries
 either needs  amortized time for an update or
 amortized time for a query, where  inserts a new edge ,
 deletes an existing edge  and  checks if
 and  are in the same component.
\begin{shortv}
The lower bound is based on a reduction from dynamic connectivity. A well-known
result by P\v{a}tra\c{s}cu and Demaine~\cite{logarithmic} is that any data structure maintaining
an undirected graph under operations ,  and answering queries
 either needs  amortized time for an update or
 amortized time for a query.\end{shortv}
It can be assumed that the graph is
a collection of disjoint paths and the lower bound applies to any Monte Carlo structure returning
correct answers with high probability. For the reduction we represent every path with
a string, so that connectivity can be checked by inspecting one character of an appropriate string.

Because our reduction is not completely black-box,
we start with a brief review of the proof presented in ~\cite{logarithmic}.

\subsection{Lower Bound for Dynamic Connectivity}
To prove a lower bound on dynamic connectivity, we consider the following problem:
maintain  permutations  on
 elements. An update  sets  and a query
 checks whether ,
where .
We assume that  is a power of two and consider sequences
of  pairs of operations, each pair being 
with  and  chosen uniformly at random and  with
 chosen uniformly at random and .
That is, the  is asked to prove a tautology, and must gather enough information
to certify it by probing cells. Then, if the word size is ,
it can be proved that the expected total number of cell probes (and hence also the total time
complexity) must be  through an entropy-based argument. The essence
of this argument is that if we consider  indices 
and execute at least one update  with  for
every , then all partial sums  are independent random
variables uniformly distributed in .

We encode the problem of maintaining such  permutations
as an instance of dynamic connectivity for disjoint paths. For every 
we create a layer of  nodes . Then, for every
 and  we connect  with .
Clearly, the resulting graph is a collection of disjoint paths. Furthermore,
 can be implemented by asking all  queries of the form
.  requires first removing all edges
between nodes from the -th and -th layer and then connecting them
according to . In total, the whole sequence can be processed with  updates
and  queries, and therefore the amortized complexity of ,
 or  is  on some input. The lower bound
immediately applies also to Las Vegas algorithms. It can be also argued that it
applies to Monte Carlo algorithms with error probability  for sufficiently
large , see Section 6.4 of~\cite{logarithmic}.

\subsection{Reduction}
Now we explain how to further encode the instance of dynamic connectivity for disjoint paths
as maintaining a dynamic collection of strings. Intuitively, we represent every path with
a string.
The -th string  describes the path connecting  and .
More precisely, we maintain the invariant that ,
where every  is a separate character. Additionally, for every  we prepare
a one-character string  such that  for .

To implement  we need to check if the -th character of 
is equal to . We split  into two parts  such that , 
and 
and compare  with the previously prepared string . Then,
we merge ,  and  to recover the original  and maintain the invariant.

The difficulty lies in implementing . We need to replace all edges between
the -th and -th layer. We first split every  into two parts  such that
 and . Then we would like to create the new 
by concatenating  and  such that  and 
(in other words, ). However, while we can extract every such ,
we are not able to find the corresponding  for every  by only comparing
strings in our collection (and without augmenting the set of allowed operations).
However, recall that every  is chosen uniformly at random. We take ,
or in other words set  instead of .
It is easy to implement such update in our setting, because after choosing 
uniformly at random we just need to concatenate  and  to form
the new , for every , but the meaning of 
is different now. Nevertheless, the expected total
number of cell probes must still be  by the same argument. That is,
if we consider  indices 
and call  with  for
every , then all  are independent random variables uniformly
distributed in . So the amortized complexity of ,  or
 must be . Again, the lower bound immediately
applies to Las Vegas algorithms and, by the same reasoning as in the lower bound for
dynamic connectivity, to Monte Carlo algorithms with error probability 
for sufficiently large .


\begin{theorem}
\label{thm:polylowerbound}
For any data structure maintaining a dynamic collection of  non-persistent strings of length
 subject to operations , , , and
 queries, correct with high probability, the amortized complexity of
, , or  is .
\end{theorem}

\section{Dynamic String Collections with Order Maintenance}\label{sec:om}

\newcommand{\prevst}{\mathtt{prev}}
\newcommand{\succst}{\mathtt{succ}}
\newcommand{\findst}{\mathtt{find}}
\newcommand{\insertst}{\mathtt{insert}}
\newcommand{\nilst}{\mathtt{nil}}
\newcommand{\sub}{\subseteq}
\newcommand{\sm}{\setminus}
\newcommand{\utrie}{\overline{T}}
\newcommand{\trie}{T}

\newcommand{\tval}{\mathtt{val}}
\newcommand{\tlink}{\mathtt{link}}
\newcommand{\tleaf}{\mathtt{leaf}}
\newcommand{\tsub}{\mathtt{sub}}
\newcommand{\tlca}{\mathtt{lca}}
\newcommand{\troot}{\mathtt{root}}
\newcommand{\tdepth}{\mathtt{depth}}
\newcommand{\tup}{\mathtt{up}}
\newcommand{\tdown}{\mathtt{down}}
\newcommand{\tmake}{\mathtt{make\_explicit}}
\newcommand{\taddleaf}{\mathtt{add\_leaf}}


In this section we describe how to extend the data structure developed in \cref{sec:collection} to support the following two types of queries
in constant time:
\begin{itemize}
  \item , for ,  checks whether ,
    , or .
  \item , for , returns the length of the longest common prefix
    of  and .
\end{itemize}

In order to support these queries, we maintain the strings from  in the lexicographic order.
The overall idea is similar to the one sketched in~\cite{Alstrup}.
However, due to the reasons already described in \cref{sec:comp}, our implementation requires some more insight.

For each non-negative integer , we maintain a \emph{trie}  that contains  for each string  in the data structure
with .
The tree is stored in a compacted form, that is we compress long paths consisting of nodes with exactly one child.
Since an edge of a trie may correspond to a long substring of some , roughly speaking, we store in it a pointer to a tree  that allows us to access the sequence of symbols represented by the edge.
Moreover, we maintain links between corresponding nodes in  and .
In order to update the tries, we develop pointers for traversing them, with similar objectives to that behind the pointers to  trees.

Whenever a string is added to our data structure, we update the tries .
At the same time, we discover the string from  that lies just before the inserted string in the lexicographically sorted list of all strings from .
By combining this with an order-maintenance data structure~\cite{Dietz:1987,Bender:2002}, we are able to answer  queries in constant time.
Moreover, by using a data structure for answering lowest common ancestor queries~\cite{Cole:2005} on the trie , we are able to answer  queries in constant time.
This idea was not used in~\cite{Alstrup}, where  queries are answered in logarithmic time.


Let us now describe the algorithm more formally.
Suppose that a grammar  is maintained so that it satisfies \cref{inv:consistency}
and so that it may only grow throughout the algorithm.
Then, we may maintain a set of strings  so that each string 
is represented by a signature  subject to the following operations:
\begin{itemize}
  \item queries  and  for every ,
  \item update  for    inserting the corresponding string  to .
\end{itemize}
Our solution supports queries in constant time while an update requires  time.
Note that in order to implement the dynamic string collections, we store the order-maintenance component
for our collection  of strings. Consequently, we call  for all strings
constructed with , , and  operations.

\subsection{Simple  Implementation}
As the technical side of our construction is quite complex, we begin with a simple implementation of the  operation,
which runs in  time.
In the efficient implementation of the order-maintenance component, the  operation uses similar ideas
to compute the longest prefix of  which is also a prefix of some string in the collection .
This involves dealing with many technical issues,  and thus it is easier to understand the key combinatorial idea by analyzing the  implementation described below.


The algorithm is based on~\cref{lem:lcpd}.
For a pointer  to some , we denote by  the prefix of  that ends immediately before the symbol represented by the node pointed by 
and by  the corresponding suffix of .


\begin{algorithm}
\begin{algorithmic}[1]
\Function{}{, }

\State 
\State 

\While{} \label{alg:samelevel_beg}
\State 
\EndWhile

\While{}
\State 
\EndWhile \label{alg:samelevel_end}

\For{ \textbf{downto} } \label{alg:lcp:for}
    \If{}\label{alg:lcp:cond}
    	\State \label{alg:lcp:k}
    	\State 
    	\State 
     \EndIf
    \If{ \textbf{or} }
    \State \Return 
   	\EndIf
    \If{}\label{alg:lcp:lcp}
    	\State 
    	\State 
    \EndIf
\EndFor
\State 
\State \Return  \label{line:return}
\EndFunction
\end{algorithmic}
\caption{Pseudocode of }
\label{alg:lcp}
\end{algorithm}

\begin{lemma}\label{lem:lcp}
  Let  and  be two strings represented by  given by the signatures .
Then, we can execute an  operation in  time.
\end{lemma}

\begin{proof}
The algorithm traverses the trees  and  top-down, and at every level  it finds the longest common prefix of  and .
The pseudocode is given as \cref{alg:lcp}.
Let us now discuss the details.

The goal of lines~\ref{alg:samelevel_beg}--\ref{alg:samelevel_end} is to make both pointers point to nodes of the same level,
which is the minimum of  and .
At the beginning of each iteration of the \textbf{for} loop starting in line~\ref{alg:lcp:for}, the following invariants are satisfied.
\begin{enumerate}
  \item ,
  \item , and
  \item the longest common prefix of  and  contains at most one distinct character.
\end{enumerate}
Moreover, we claim that at line~\ref{alg:lcp:lcp} of each iteration, 
is the longest common prefix of  and .



The first invariant is satisfied at the beginning of the first iteration, because the \textbf{while} loops ensures . It is easy to see that each iteration of the \textbf{for} loop preserves it, as it decreases both  and  by one (unless ).
Let us now move to the other two invariants.

Before the first iteration,  and  or 
because  or .
Thus, the invariants are satisfied.

It remains to show that they are preserved in every iteration.
If  at line~\ref{alg:lcp:cond}, then  and 
start with different symbols and thus  is already the longest
common prefix of  and .
Otherwise, the longest common prefix of  and  consists (by the invariant) of exactly
one distinct character, and the prefix can be easily seen to be 
where  and  is computed in line~\ref{alg:lcp:k}.
In the following two lines we shift  and  to move  from the suffix to the prefix.

If  as a result of this operation, then  can be easily seen to be a prefix of ,
and thus  is a prefix of .
Symmetrically, if , then  is a prefix of . In both cases we correctly report the final answer.

Thus, at line~\ref{alg:lcp:lcp}  indeed is the longest common prefix of  and .
In particular, for  this already allows determining the answer as we do after leaving the \textbf{for} loop.
Otherwise, we infer from Lemma~\ref{lem:lcpd} that the longest common prefix  of 
and  can be represented as  where 
and . We move pointers  and  to the leftmost children
so that after this operation . The longest
common prefix of  and  is then  and thus, as claimed, it
consists of at most one distinct character.

Clearly, the running time of the entire function is proportional to the sum of heights of  and .
\end{proof}


\subsection{Tries}
Recall that a trie is a rooted tree whose nodes correspond to prefixes of strings in a given family of strings .
If  is a node, the corresponding prefix is called the \emph{value} of the node and denoted .
The node  whose value is  is called the \emph{locus} of .

The parent-child relation in the trie is defined so that the root is the locus of ,
while the parent~ of a node  is the locus of  with the last character removed.
This character is the \emph{label} of the edge between  and .
If  is a ancestor of , then  denotes the concatenation
of edge labels on the path from  to . Note that  where  is the root.
For two nodes  of a trie, we denote their lowest common ancestor by .

A node  is \emph{branching} if it has at least two children and \emph{terminal} if .
A \emph{compressed trie} is obtained from the underlying trie by dissolving all nodes except
the root, branching nodes, and terminal nodes. Note that this way we compress paths of nodes with single children
and thus the number of remaining nodes becomes bounded by .

In many applications, including this work, \emph{partially compressed} tries are constructed,
where some \emph{extra} nodes are also important and thus they are not dissolved.
In general, we refer to all preserved nodes of the trie as \emph{explicit} (since they are stored explicitly)
and to the dissolved ones as \emph{implicit}.
Edges of a (partially) compressed trie correspond to paths in the underlying tree
and thus their labels are strings in . Typically, only the first character of these labels is stored explicitly
while the others are assumed to be accessible in a problem-specific implicit way.

\subsection{Overview}
Let . For , let 
and let  be the trie of strings  for .
We write  instead of  if the set  is clear from context.
If , then by  we denote the terminal node  of  for which
.

Observe that for each  and each node  there exists a unique node 
such that . We denote it by .
Note that  and that  preserves the ancestor-descendant relation:

\begin{observation}\label{obs:link}
Let  be nodes of  for . If  is a (proper) ancestor of , then 
is a (proper) ancestor of .
\end{observation}

A node  is a \emph{down-node} if it is root, branching, or terminal.
A node  is an \emph{up-node} if  for some down-node
.
We define the (partially) compressed versions  of tries  in which the explicit
nodes are precisely the up-nodes and the down-nodes. We also assume that each down-node 
for  explicitly stores a link to .

Our implementation is going to maintain the tries  subject to   operations.
The actual goal of this is to store  and augment it with auxiliary components helpful in answering the queries.
In short, to handle  queries, we use a data structure of Cole and Hariharan~\cite{Cole:2005} for
lowest common ancestor queries in dynamic trees.
For  queries, we use an order-maintenance data structure~\cite{Dietz:1987,Bender:2002}
applied for the Euler tour of~.



With the aid of these component answering queries is simple, so our main effort lies in updating the data structure upon insertions .
This is achieved in two phases. First, for each level  we determine the \emph{mount} of  in 
which is defined as the lowest node  of  whose value 
is a prefix of . It is easy to see that in ,  is an ancestor of   (improper if  is a prefix of  for some )
and that all internal nodes on the path from  to  do not exist in .
Consequently, after determining the mounts, we simply turn each  into 
by adding a new leaf as a child of the mount (unless the mount is already ).


Since some of the mounts might be implicit in , we introduce a notion of \emph{trie pointers} to traverse the uncompressed tries
, so that a trie pointer  points to a node .  Conceptually, they play a similar role to that of pointers to nodes of uncompressed parse trees; see \cref{sec:pointers}.

Let us describe the interface of trie pointers.
First of all, we have a primitive  to create a new pointer to the root of .
Central parts of the implementation are two functions to navigate the tries starting from a node  pointed by :
\begin{itemize}
  \item  -- returns a pointer to a node  such that 
  and  is largest possible, as well as the optimal .
  \item  -- returns a pointer to .
\end{itemize}

Before we proceed with describing the interface for updating , let us introduce a notion binding trie pointers with pointers to uncompressed parse trees. Let .
Observe that if  is a (proper) prefix of  for some string ,
then  contains a unique node  such that .
We say that a pointer  to  is \emph{associated} with  (or with a trie pointer  pointing to ) if it points to such a node .  If , then
there is no such node  and thus we assume that  is associated with .

We have two ways of modifying the (partially) compressed tries:
\begin{itemize}
  \item \textbf{Internal node insertion.}  -- given a trie pointer  to a node , make  explicit in .
  Returns a reference to .
  \item \textbf{Leaf insertion.}  -- given a trie pointer  to the mount of  in , and an associated pointer  to ,
update  to represent . Assumes the mount is already explicit in . Returns a reference to .
\end{itemize}
Note that these operations may result in  not being equal to  for any particular  (since, e.g., 
has a precisely defined set of explicit nodes.
However, we assume that the trie pointer operations (in particular  and ) are allowed only if 
and (in case of the  operation)  for a particular set . Moreover, we assume
that any modification of  invalidates all pointers to~ except for pointers to proper descendants of the node pointed by .

The following result is proved in \cref{sec:trierepr,sec:triepointers}.
\begin{theorem}\label{thm:trie_pointers}
Tries  can be implemented so that
internal node insertions, leaf insertions, as well as
 and  operations run in worst-case  time, while
 operation works in worst-case  time.
Moreover, any alternating sequence of length  of  and  operations (where the result of one operation is an argument of the subsequent) takes  time.
\end{theorem}
\noindent
In \cref{sec:trie_extra} we take care about the auxiliary components in 
and prove the following result:
\begin{lemma}\label{lem:trie_extra}
Auxiliary components of  can be implemented so that operations  and  take constant time,
while internal node and leaf insertions require  time.
\end{lemma}

\subsection{ Implementation}
In this section we implement the  operation which adds a string  to the family .
This string is given by a signature  representing it.

Recall that the main difficulty lies in determining the mounts  of  in each trie 
for .
The following two results following from \cref{lem:lcpd} provide the required combinatorial background.
\begin{corollary}\label{cor:lcpd}
Let , , ,
and .
Then  is an ancestor of  and .
\end{corollary}
\begin{proof}
  Let , ,
and let  be the longest common prefix of  and .
Note that .
By \cref{lem:lcpd}, we have a factorization 
such that  and  is the longest common prefix of  and .
Observe that the latter implies that  and thus that  is an ancestor of .
Moreover,  which yields the second part of the claim.
\end{proof}

\begin{lemma}\label{lem:inscorr}
For , let  be the mount of  in .
Then  is a descendant of  and .
\end{lemma}
\begin{proof}
Observe that if  is a prefix of , then 
must be a prefix of . Consequently,  is indeed a descendant of .

To prove that , consider tries built for .
If , then all  are root nodes, hence 
and the condition clearly holds.
Otherwise, for some  we have  .
Let .
Note that  is a descendant of  and, by \cref{obs:link},  is consequently a descendant of .
Moreover, \cref{cor:lcpd} implies that 
and thus , as claimed.
\end{proof}

\begin{algorithm}
\begin{algorithmic}[1]
\Function{}{}
		\State 
		\State 
		\State 
	\For{ \textbf{downto} }\label{alg:insert:for}	
		\If{}\label{alg:insert:if}
    \State 
		\State 
	\Else
    \State 
    \State 
	\EndIf
	\If{}
		\State 
		\If{}\; 
		\Else\; 
		\EndIf
	\EndIf
\EndFor
\For{ \textbf{to} }
	\State 
	\State 
	\State 
	\If {}
		\State 
		\State 
	\EndIf
\EndFor
\EndFunction
\end{algorithmic}
\caption{Pseudocode of  implementation.}\label{alg:insert}
\end{algorithm}

We conclude this section with the actual implementation
and the proof of its correctness.
We extend our notation so that 
for  representing .

\begin{lemma}\label{lem:trie_insert}
Let  be a signature representing a string .
\Cref{alg:insert} provides a correct implementation of 
working in  time.
\end{lemma}
\begin{proof}
Let us start with the first part of the algorithm, including the \textbf{for} loop at line~\ref{alg:insert:for}.
Let  be the mount of  in .
We claim that the following conditions are satisfied:
 \begin{enumerate}[(a)]
   \item\label{it:one}  and  point to nodes of ,
   \item\label{it:two}  is a pointer to  associated with  and  points to an ancestor  of  such that .
     \item\label{it:three}  is a pointer to  associated with  and  points to the mount .
 \end{enumerate}

Observe that (\ref{it:one}) is clear from definition of each  and .
The remaining two points are proved inductively.
We have  and , where  is the signature representing .
Consequently, condition (\ref{it:two}) is indeed satisfied for .
Next, observe that the \textbf{if} block at line~\ref{alg:insert:if} makes (\ref{it:two}) imply (\ref{it:three}).
Indeed, observe that  is exactly the longest prefix of  which can be expressed
as  for some descendant  of .
Consequently, pointers  and  satisfy the claimed conditions.
Finally, if  we make  point to  and  as the associated pointer to .
By \cref{lem:inscorr} condition (\ref{it:two}) is satisfied for .
Note that \cref{thm:trie_pointers} implies that this part of the algorithm runs in 
time since we have an alternating sequence of  subsequent  and  operations.

Correctness of the second part of the algorithm follows from the definition of mounts .
Its running time is also  by \cref{thm:trie_pointers,lem:trie_extra}.
\end{proof}

\subsection{Implementation of Tries}\label{sec:trierepr}
In this section we give a detailed description of how the tries  are represented
and what auxiliary data is stored along with them.
We also show how this is maintained subject to  and  updates.

\subsubsection{Trie Structure}

Each edge of  is represented by a tuple consisting of references to the upper endpoint , the lower endpoint ,
as well as a pointer  such that  is the first symbol of  and  is associated with .
Each explicit node  stores a reference to the edge from the parent
and references to all the edges to the children of .
The latter are stored in a dictionary (hash table) indexed by the first character of the edge label.
If  has exactly one child, a reference to the edge to that child is also stored outside of the dictionary
so that the edge can be accessed without knowledge of the first character of its label.

Additionally, each  stores its depth , which is defined as the length
of the (unique) string  such that .
Note that this can be seen as the weighted length of  in which the weight of a symbol  is .
In particular, at level  we have .





\newcommand{\tanc}{\mathtt{anc}}
\newcommand{\tdesc}{\mathtt{desc}}
For a node  we define  () as the nearest ancestor (resp. descendant)
of  which is explicit in . If  is explicit, we have .




\begin{lemma}\label{lem:structure}
The trie structure can be updated in constant time subject to internal node insertion  and leaf insertion .
\end{lemma}
\begin{proof}
First, let us focus on internal node insertion. Let  be the node pointed by .
Let  and let the edge from  to its parent be represented as .
Note that this edge needs to be split into two edges:  and .
We thus update a reference to the parent edge at 
and a reference to the appropriate child edge at .
Recall that the latter is stored in the dictionary under the  key.

Finally, we need to create the representation of .
This is mainly a matter of storing the references to the newly created edges.
The lower one is kept in the dictionary at  under key .
We also store  which can be easily determined from .
Consequently, internal node insertion can indeed be implemented in constant time.

Let us proceed with leaf insertion. If , there is nothing to do since .
Otherwise, we need to create a new node  and a new edge .
References to that edge need to be stored in  and in the dictionary at .
In the latter we insert it under  key. By our assumption,
no such edge has previously existed. The last step is to store .
\end{proof}


\newcommand{\tpath}{\mathtt{path}}
\subsubsection{Navigation on Uniform Paths.}
Let  be a path in  from an explicit node  to its proper explicit descendant .
We say that  is \emph{-uniform} if  for some symbol  and integer .
An -uniform path is \emph{maximal} if it cannot be extended to a longer -uniform path.
Observe that maximal uniform paths are edge-disjoint.
If  is a child of  in  we say that the edge from  to  is -uniform.
Detection of uniform edges is easy:
\begin{fact}\label{fct:unicheck}
Given a representation  of an edge of , we can check in  time if
the edge is uniform.
\end{fact}
\begin{proof}
Note that the edge could only be -uniform for .
Observe that this is the case if and only if

This is due to the fact that  is a prefix of .
\end{proof}


\newcommand{\tlocate}{\mathtt{locate}}
With a path  in  we associate the uncompressed path  in 
with the same endpoints.
For each uniform path  we would like to efficiently locate in  any node of .
More precisely, we consider the following  operation: if  contains
a node  with , compute  and ; otherwise return .
The path  can be specified by any edge belonging to it.

We introduce a function 
to measure hardness of a particular query.

\begin{lemma}\label{lem:st}
For all maximal uniform paths  in  we can maintain data structures which support
 queries in  time if there is a sought node 
and in  time otherwise.
Moreover, the data structures (as well the collection of maximal uniform paths) can be maintained
in constant time subject to internal node insertions and leaf insertions.
\end{lemma}
\begin{proof}
First, observe that the depths of all nodes on  form an arithmetic progression whose difference is .
Thus, it suffices to know the depths  and  of endpoints and the length 
to test if the sought node exists.
Consequently, for each path  we shall store these three quantities. If each uniform edge stores a pointer to the data structure responsible for the path containing that edge, we can make sure  that the path  can indeed by specified by one of the member edges.

To support queries with non- answers, we shall maintain the depths of all nodes of  in two forms:
a finger search tree~\cite{BrodalLMTT03} and a dictionary (hash table) mapping a value to a finger pointing to the corresponding element of the search tree.
This way, we can find  or 
in  time by checking if the dictionary has an entry at  for subsequent integers  starting from .
Once we detect  or , the other can be determined in constant time since finger search tree supports
constant-time iteration.

Our query algorithm performs this procedure but stops once we reach . In this case it falls back to ordinary search
in the finger search tree, which clearly succeeds in  time.
Consequently, the query algorithm runs in  time, as claimed.

Thus, it suffices to describe how to support internal node insertions and leaf insertions.
First, let us focus on internal node insertions. If the newly created node  subdivides a uniform edge,
we shall insert its depth to the data structure of the corresponding path .
Since we know  and , i.e., the predecessor and the successor of  in the finger search tree,
this element insertion takes constant time.

Otherwise, we need to check if the two newly created edge are uniform and, if so, if they extend an already existing uniform paths
or constitute new ones. All this can be easily accomplished using \cref{fct:unicheck}.
A new path  contains two nodes, so its data structure can be easily created in constant time.
The extension of an already existing path, on the other hand, reduces to insertion of a new depth larger or smaller than
all depths already stored. Again, finger search tree supports such insertions in constant time.

Finally, for leaf insertions we create one edge, and if it is uniform, we need to update our data structures.
We proceed exactly as in the second case for internal node insertions, since this edge either extends
an already existing path or constitutes a new one.
\end{proof}


\subsection{Implementation of Operations on Trie Pointers}\label{sec:triepointers}
In this section we implement  and  operations.
Recall that these operations require trees  to be 
for a fixed collection .
In particular, we insist on explicit nodes being precisely the down-nodes and the up-nodes.

\subsubsection{Combinatorial Toolbox}


For a node , let us define  as the set of all strings 
for which  is a descendant of .

\begin{fact}\label{fct:link}
Let  and let  be nodes of  such that 
is a proper descendant of .
The last character of  is not the same as the first character of .
Moreover, 
\end{fact}
\begin{proof}
Let , , 
and . The fact that  is a proper prefix of 
also means that  places a block boundary in  just after .
Since a boundary is never placed between two equal adjacent characters, this implies that the last character of 
is indeed different from the first character of .

The second part of the claim easily follows from 
and  for any integer .
\end{proof}


\begin{lemma}\label{lem:nodown}
Let  for  and let  be the nearest descendant of 
which is a down-node.
The path  has no internal explicit nodes.
\end{lemma}
\begin{proof}
If  is a down-node, then  and thus the claim holds trivially.
Consequently, we shall assume that  is not a down-node.
Let  and .
Let us also define  as the nearest ancestor of  being a down-node,
and .

By \cref{obs:link},  is a proper ancestor of  and  of .
Moreover, by \cref{fct:link}, the last character of  is distinct from the first character of .

For a pair of fixed strings  let us define  and
.
We shall prove that neither  nor  is an internal node of .
Since any down-node is of the former form and any up-node of the latter form, this shall conclude the proof.
Note that (by \cref{cor:lcpd})  is an ancestor of .

First, suppose that . Then, 
is a descendant of  and thus, by \cref{obs:link},  is a descendant of .
Consequently, indeed neither  nor  can be a proper ancestor of .

Thus, one of the strings ,  (without loss of generality ) does not belong to .
Let us fix a string  and define  as well as .
Note that  is a proper ancestor of  and, since the path  contains no internal down-nodes,
an ancestor of . Consequently, by \cref{obs:link},  is an ancestor of .
By \cref{cor:lcpd},  is an ancestor of  and .
Since the last character of  is distinct from the first character of ,
this implies that  is an ancestor of . Since , this yields
that no ancestor of  is an internal node of the  path.
This in particular holds for both  and .
\end{proof}


\begin{lemma}\label{lem:oneup}
Let  be down-nodes in  such that  is a proper ancestor of 
and the path  contains no internal down-nodes.
This path contains at most one internal up-node.
\end{lemma}
\begin{proof}
Suppose that  is a down-node such that 
is an internal up-node on the path .
Let  be strings such that .
Since  is not a down-node,  is a proper ancestor of .
Let . By~\cref{cor:lcpd},
 is a descendant of  and .
Since  is a down-node, this means that  is a descendant of 
and, in particular, .

By \cref{fct:link} applied for  and , the last character of 
(i.e., the last character of )
is distinct from the first character of  (i.e., the first character of ).
Consequently,  is uniquely determined as the highest internal node of 
satisfying .
\end{proof}



\subsubsection{Implementation Details}

A trie pointer  to a node  is implemented as a pair ,
where  if and only if , i.e.  is an explicit node.
Otherwise,  points to a node of  associated with , for some .

\begin{lemma}\label{lem:link}
Given a trie pointer , the pointer 
can be computed in  time.
\end{lemma}
\begin{proof}
Let  be the node represented by  and let .
If  is a down-node, then a reference to  is stored at  and  is explicit, therefore  can be returned as the resulting trie pointer.

If  is an up-node, a pointer  to an associated node of  for some 
can be read from the representation of the edge going from  to its only child.
Thus, we shall assume that  is not explicit and thus .

Let  be the nearest descendant of  which is a down-node.
By \cref{lem:oneup},  or  is the only child of  in .
In either case,  can be determined in constant time and so can be ,
because down-nodes explicitly store their link pointers.

Let  be the sought (possibly implicit) node.
By \cref{lem:nodown},  path contains no explicit internal nodes.
Thus, either  or  is the (explicit) parent of  in .
Note that the latter holds exactly if the parent of  is at depth exactly .
Since nodes of  store their depth and since  can be easily retrieved from ,
this condition can be checked in constant time. We return  in either case.
\end{proof}

\begin{lemma}\label{lem:down}
  Given a trie pointer , a signature , and an integer , the pair 
can be computed in  time where  is the node  pointed by 
and .
\end{lemma}
\begin{proof}
Let  be the node represented by  and let .
First, suppose that  is explicit.
If  does not have a child  with edge  such that , we return .
Otherwise, the dictionary lets us retrieve  and an associated pointer .
First, suppose that the edge is not -uniform (which can be detected using \cref{fct:unicheck}).
In this case  and it is not hard to see that 
and .

If the edge is uniform, we extend it to the maximal -uniform path 
and run the \linebreak
 operation of \cref{lem:st}.
Let  be the lower endpoint of . If  is not a proper descendant
of , this way we locate  and .
The associated pointer can be easily located as
,
where  is the pointer stored in an edge .

Otherwise, we recursively proceed from  trying to follow 
copies of . This time, we are however guaranteed that reaching a child of  is impossible and
thus the previous case immediately yields a result.

Finally, suppose that  is implicit and let .
First, we check if  is an ancestor of .
This happens if  does not exceed .
Since  is stored and  can be retrieved from ,
this condition can be checked in constant time. If the result is positive, we return 
where .
Otherwise, we recursively proceed trying to follow  copies
of  starting from . As before, one of the previous cases are used for ,
so the procedure terminates in at most two steps.
\end{proof}

\begin{lemma}\label{lem:amort}
An alternating sequence of  and  operations (such that the result of a given operation
is the argument of the subsequent one) runs in  time where  is the length of the sequence.
\end{lemma}
\begin{proof}
Let us define two potential functions.
To analyze situation after  (before ) we use
 
 while after  (before )
 

We shall prove that no single operation decreases the potential by more than one
and that the slow, superconstant-time case of  is required only if the potential is
increased from  or  to at least a value proportional to the running time.

Let us analyze the  operation. First, suppose that .
Then,  is implicit and, since ,
the resulting node  is also an implicit ancestor of .
Moreover,

and

i.e.,  as claimed.
Finally, recall that  works in constant time in this case.

Thus, it suffices to prove that the if  in \cref{lem:down} requires superconstant time,
it the running time of  is bounded by the potential growth.
 This is, however, a straightforward consequence of the formulation of \cref{lem:down}.

 Consequently, it suffices to analyze . We shall prove that  for every  and .
 If  is explicit, there is nothing to prove.
 Let  be the nearest descendant of  which is a down-node, and let  be the lowest ancestor of 
 whose parent (in ) is a down-node. Also, let , , and .
 By \cref{lem:nodown} applied to , there is no explicit inner node on the  path.
 Thus, by the second part of \cref{fct:link}, we have
 
 and
 
 Consequently, , as claimed.
\end{proof}

Note that \cref{lem:structure,lem:st,lem:amort} immediately yield \cref{thm:trie_pointers}.

\subsection{Auxiliary Components for Answering Queries}\label{sec:trie_extra}
In this section we give a detailed description of auxiliary data structures stored with 
in order to provide constant-time answers to  and  queries.

First of all, for each  we store a reference to  so that given a handle of 
we can get to the corresponding terminal node in constant time.
Observe that .
Since every explicit node stores its depth, answering  queries reduces to  queries on .
Consequently, we maintain for  a data structure of Cole and Hariharan~\cite{Cole:2005} for LCA queries in dynamic trees.
It allows  time queries and supports constant-time internal node insertions and leaf insertions as one of the several possible update operations.

\newcommand{\tbeg}{\mathtt{beg}}
\newcommand{\tend}{\mathtt{end}}

Supporting  queries is a little bit more challenging. For this, we treat  as an ordered trie (based on the order on )
and we maintain an Euler tour the trie. It is formally a list  in which every explicit node  corresponds to exactly two elements  and . The order on  is such that a description of a subtree rooted at  starts with , ends with  and between these elements contains the description of subtrees of , ordered from left to right.
In this setting,  if and only if  precedes  in .
Hence, in order to answer  queries it suffices to store an order-maintenance data structure~\cite{Dietz:1987,Bender:2002} for .

Such a data structure allows inserting a new element to the list if its predecessor or successor is provided.
Thus, when we add a new node to , we need to determine such a predecessor or successor. For this, in each explicit node of 
we maintain an AVL tree storing references to edges going to the children. The AVL is ordered by the first characters of edge labels.
Hence, whenever we modify the hash table storing these references, we also need to update the AVL tree. This happens  times
for each node insertion, and thus takes  time. Now, for a newly added node 
the predecessor of  can be determined as follows: If  has a preceding sibling , it is .
Otherwise, it is  where  is the parent of . A successor of  can be retrieved analogously.
Determining the preceding or succeeding sibling of  is implemented using the AVL tree of children of 
and takes  time.
Consequently, the component for  allows -time queries and requires  time per update of .

The discussion above proves \cref{lem:trie_extra}.

\subsection{Conclusions}\label{sec:trie_conc}

\begin{theorem}\label{thm:persistent_compare}
The data structure described in \cref{sec:collection} can be augmented to support  and 
queries in worst-case constant time (without failures). The running time of updates , , and  increases by a constant factor only.
\end{theorem}

\begin{proof}
After each update we perform  to insert the newly created string to the order-maintenance components.
Its running time  is dominated by the  term in the update times.
\end{proof}


\section{Pattern Matching in a Set of Anchored Strings}\label{sec:anchored}
In this short section we relate 2D range reporting with pattern matching.
We adapt this rather folklore technique to our setting,
in which strings can be compared in constant time,
but do not have integer identifiers consistent with the lexicographic order.
Alstrup et al.~\cite{Alstrup} used very similar tools.
Subsequent results of Mortensen~\cite{Mortensen:2003}, however, let us slightly improve the update times.

\begin{definition}
  We call a pair  of strings from  an \emph{anchored}
  string.
  An anchored string 
  is called a \emph{substring}
  of another anchored string  if and only if
   is a suffix of  and  is a prefix of .
\end{definition}

Anchored strings can also be seen as strings containing exactly
one special character  -- an anchor.
Thus, a pair  can be seen as a string  and
 is a substring of  if and only if
 is a substring of .

Assume that we have a set  of strings such that for any two elements 
of  we can in constant time find out if  is lexicographically smaller than 
and whether  is a prefix of  (the latter check can be reduced to verifying if ).
Moreover, suppose that  is closed under string reversal, i.e.,  for every .
We wish to maintain a dynamic multiset of anchored strings 
so that we can find
interesting elements of  containing some given anchored string.
More formally, we want to support the operation ,
where , reporting  elements  such that 
is a substring of .
If there is less than  such elements,  reports
all of them.
In order to efficiently distinguish elements of , we use a unique integer key
for each of them. Keys are provided while we perform an insertion and are later used
to identify the element. In particular, they are reported by 
and deletion uses them as an argument.


\begin{theorem}[Anchored index]\label{thm:ai}
  Let  be the size of the multiset .
  There exists a data structure supporting the  operation
in worst-case time  and insertion or deletion
to the set  in worst-case time .
\end{theorem}
\begin{proof}
We reduce our case to the problem of maintaining the set
two-dimensional points on plane, so that
we can report a points within some
orthogonal rectangle.
Mortensen~\cite{Mortensen:2003} proposed a data structure
for the problem of dynamic two-dimensional orthogonal
range reporting with worst-case update time of 
and query time of , where
 is the size of the currently stored set of points.
Note that the data structure only assumes comparisons on the coordinates
with worst-case  time overhead, as opposed to an explicit
mapping of coordinates to integers.

In the reduction we introduce a new letter  larger than all letters in .
We observe that for two strings  we can efficiently lexicographically compare  and  with  and .
This is because the order between  and  is the same as between  and ,
while  is smaller than  whenever  is smaller than  but is not a prefix of .

The reduction of anchored strings to two-dimensional
points works as follows: the element of 
is mapped to a point .
In order to answer , where , we
query the orthogonal range data structure for
 points in a rectangle
.
Clearly,  is a prefix of  if  is both
lexicographically not smaller than  and not greater than .
Similarly,  is a prefix of  if and only if
 is not smaller than  and not greater than .

As the data structure of Mortensen only supports sets of points,
we also maintain a mapping  from integer keys to
points, and for each distinct point  we maintain
a list of keys  mapped to .
We insert a point  into the range reporting structure only
when  after the insertion
and delete  when 
becomes empty.

For each point  reported by the range-reporting structure
we actually report all keys from , which does
not increase the asymptotic reporting time of .
\end{proof}

\section{Pattern Matching in a Non-Persistent Dynamic Collection}\label{sec:pm}
In this section we develop a dynamic index, i.e., a dynamic collection
of strings, which supports pattern matching queries.
As opposed to the data structure of \cref{sec:collection}, the dynamic index is not persistent,
i.e.,  and  destroy their arguments.
Formally, we maintain a multiset  of strings that is initially empty.
The index supports the following updates:
\begin{itemize}
  \item , for , results in .
      \item , for , results in .
   \item , for , results in .
  \item , for  and , results in
    .
\end{itemize}

Again, strings in  are represented by handles so that input to each operation takes constant space.
Since  is a multiset, each string created by an update is assigned a fresh handle.
Hence, as opposed to the data structure of \cref{sec:collection}, equal strings may have different handles.

The data structure supports pattern matching queries for (external) patterns .
More precisely,  for  determines
if  occurs in some string .
Optionally, it also reports all the occurrences as \emph{stubs}, which can be turned to an actual occurrence
(a handle of a particular  plus a position within ) at an extra cost.
We could modify the implementation of the  operation to report, for a given threshold ,
no more than  arbitrary occurrences. However, the choice of the occurrences reported would depend on the internal randomness
of our data structure. Hence, one would need to make sure that future operations do not depend on this choice.


In the description we assume that  is the total input size of the updates made so far.
Since  and  destroy their arguments,  is also an upper bound on the total
length of strings in  (both after and before the current update).
From \cref{lem:small_depth} it follows that a string of length at most  has  with high probability.
Thus, our update algorithms fail as soon as they encounter a string of larger depth.
Since the running time of each update is , each update is successful with high probability.



Our data structure consists of separate components which we use for long (longer than ) and for short patterns (shorter than ).
The data structure for long patterns described in \cref{sec:long} is similar to the persistent data structure of \cref{sec:collection,sec:om}.
Since our approach in this case results in an additive overhead of  in the query, for short patterns it is
more efficient to use a tailor-made solution, which we present in \cref{sec:short}.
By maintaining both data structures and querying the appropriate one, we obtain a running time, which is only \emph{linear} in the length of the queried string.

\newcommand{\lists}{\mathcal{L}}


\subsection{Permanent Pointers to Characters of }\label{sec:lists}
A common ingredient of our solutions for both long and short patterns is a data structure 
which provides pointers to specific characters of strings .
These pointers are preserved in the natural way while  and  operations are performed.
This is actually the reason why  and  destroy their arguments: pointers to characters
are reused.

\begin{lemma}\label{lem:lists}
We can maintain a data structure  providing pointers to characters of strings .
This data structure supports the following updates:  and  in  time,
 in  time, and  in  time.
Moreover, given a handle to  and a position , , in  time it returns
a pointer to . Converting a pointer into a handle and a position, also works in  time.
\end{lemma}
\begin{proof}
We store an AVL tree for each  and a mapping between the handle of strings  and roots of the trees. The -th (in the in-order traversal) node of the tree corresponding to 
represents . After augmenting the trees so that each node stores the size of its subtree, we
can determine the node representing  in  time.

For  and  we simply create or destroy a tree corresponding to .
Operations  and  reduce to splitting and joining AVL trees, which can be done in logarithmic time.
The former also requires finding the -th node, with respect to which we perform the split.

We implement pointers to characters simply as pointers to the corresponding nodes of the AVL tree.
This way determining the handle and the position only requires traversing a path from the pointed node to the root of the tree,
and determining a pointer to the given  reduces to retrieving the -th node of the AVL tree
corresponding to .
\end{proof}

We shall use pointers provided by  to implement occurrence stubs for .
More precisely, a stub is going to be a pointer and a relative position with respect to that pointer.
By \cref{lem:lists}, an occurrence in  represented by such a stub can be identified in  time.

\subsection{Long Patterns}\label{sec:long}

We start with describing the main ideas of our solution. In the description we assume  but we shall actually
use this approach for  only.
Our solution shares some ideas with an index for grammar-compressed strings by Claude and Navarro~\cite{DBLP:journals/fuin/ClaudeN11}
and a dynamic index in compressed space by Nishimoto et al.~\cite{DBLP:journals/corr/NishimotoIIBT15}.

\subsubsection{Combinatorial Background}
Let us consider an occurrence  of a pattern  in a string .
Following the idea of~\cite{DBLP:journals/fuin/ClaudeN11}, we shall associate it with the lowest node  of  which represents a fragment
containing  (called the \emph{hook} of the occurrence) and with its signature .
Note that the considered occurrence of  in  yields an occurrence of  in 
and if  is partitioned according to the production corresponding to , the occurrence
is not contained in any fragment of  created this way.
This yields a non-trivial partition  of the pattern.
More formally, if , then for some
,  is a suffix of ,
for each , 
and  is a prefix of .
Such an occurrence of  in  is called an \emph{anchored occurrence} of  in 
and the anchored string  is called the \emph{main anchor} of the occurrence.
The main idea of queries to our data structure is to identify the hooks of each occurrence.
For this, we determine all symbols  in  which admit an anchored occurrence of .

For each signature  with  let us define an anchored string  as follows:
if , then , and if , then .
\begin{lemma}\label{lem:relanch}
  Let .
A pattern  has an anchored occurrence in  with main anchor 
if and only if  is a substring of . If so, the number of anchored occurrences and their relative positions (within )
can be determined in constant time.
\end{lemma}
\begin{proof}
Note that if  is a substring of , then the corresponding occurrence of  in  is clearly anchored
with main anchor .
Thus, let us suppose that  has an anchored occurrence in  with main anchor  and let  be the partition of  this occurrence yields (, ).
Let us consider two cases.
First, if , then  and  is clearly a substring of .
The starting position is .

Next, suppose that  and let . Note that , ,  is a suffix of , and  is a prefix of . Consequently,  is a substring of  and thus of .
However, there are actually , i.e., ,
occurrences which start at subsequent positions  for .
\end{proof}


\paragraph{Potential Anchors}
\cref{lem:relanch} reduces pattern matching for  to pattern matching of anchored strings  (with )
in a set  for  and . What is important, we do not have to consider all  possible anchored strings that can be obtained by inserting an anchor into .
As stated in the following Lemma, only a few such anchored patterns are necessary.
An analogous observation appeared in a recent paper by Nishimoto et al.~\cite{DBLP:journals/corr/NishimotoIIBT15}.


\begin{lemma}\label{lem:maina}
Consider the run-length encoding  of a context-insensitive decomposition  of a pattern .
If  such that  is a main anchor for an anchored occurrence of  in some , then  or  for some .
\end{lemma}
\begin{proof}
Note that  can be seen as an extension of .
Let  be the root of  and let  be the partition of  induced by children of , and let  be the corresponding children of~.


For  and  let  be the counterpart in  of the node
in  corresponding to the -th copy of  in the decomposition.
Observe that each node  is a descendant of some , and thus the fragment represented by 
is either fully contained in  or in .
Therefore there exists a prefix of  which forms a decomposition of .

Now, consider nodes  and . Observe that  is their common signature, and thus by \cref{fct:layer} they have a common parent in .
Unless this parent is  and  and , the fragment represented in total by  and  is either
fully contained in  or in .
On the other hand, if , then clearly .
Consequently, the only prefixes of  which may form a decomposition of  are  and 
for  .
\end{proof}
\begin{corollary}\label{cor:maina}
Given a pattern , we can in  time compute a set of \emph{potential main anchors} of ,
i.e., a set of  anchored strings  such that , containing (in particular) all main anchors satisfying that condition.
\end{corollary}
\begin{proof}
\cref{lem:layerimpl} lets us compute in  time the run-length encoding of a context-insensitive decomposition of .
For an encoding of size , \cref{lem:maina} gives  potential main anchors.
This leads to   potential main anchors computable in  time.
\end{proof}

\subsubsection{Data Structure}
Having developed all the necessary combinatorial tools, we are ready to describe its components.

In our non-persistent dynamic collection we store more information than in the persistent data structure described in \cref{sec:persistent_ds}.
As a result, updating the data structure is a bit more involved and less efficient, but we are able to support more powerful queries.
We maintain a grammar , which is implemented the same as way as in the data structure of \cref{sec:persistent_ds}.


Moreover, we maintain the component  provided by \cref{lem:lists} as well as an anchored index (see \cref{thm:ai}) that contains anchored strings  for each node .
More precisely, the anchored index contains an entry  whose anchored string is 
and whose key is a pair consisting of a signature  and a pointer provided by  to the first character
of the fragment of  represented by . Note that such keys are distinct.
The anchored index requires that certain strings can be quickly compared lexicographically.
In order to do that, we maintain an order-maintenance component  of~\cref{sec:om} for maintaining the lexicographic order
on some strings of . In particular, we shall require that for  not only  but also its reverse
is represented by .

To sum up, our data structure maintains a dictionary of signatures ,  the component ,
the anchored index, and the order-maintenance component .
\subsubsection{Queries}

Our query algorithm works in several steps. First, we make sure that  contains a signature representing the pattern  and its reverse  (using \cref{lem:makeop} in  time).
Next, we build a context-insensitive decomposition of  in order to determine  potential main anchors  (using \cref{lem:layerimpl,cor:maina}).
For each of the  potential anchors , we make sure that , , , and 
are represented by  (using \cref{lem:splitop}) and add them to .
In total this requires  time with high probability.

We process the potential anchors one by one.
Note that  contains all the strings necessary to query the anchored index (see \cref{thm:ai}) with pattern .
Hence, we make such queries each of which gives a hook  of an occurrence of .
More precisely, we get the signature  and a pointer to the first character represented by .
For each hook  we determine the number of occurrences and generate relative position of at most  occurrences (both using \cref{lem:relanch}).
This way we generate stubs of the occurrences in  time.
Recall that an occurrence represented by a stub can be determined in logarithmic time (using \cref{lem:lists}).
Hence the following result:
\begin{lemma}\label{lem:findop}
Queries  can be implemented in  time where  is the total number of occurrences and  is the total length of strings in the collection.
A stub of an occurrence in  returned by  can be located in  additional time.
The  operation may fail with inverse polynomial probability.
\end{lemma}

\subsubsection{Updates}
We now describe how to maintain our data structure.
For maintaining the dictionary  of signatures we may use exactly the same algorithm as in the persistent string collection.
Let us now move to maintaining the anchored index and the data structure .

\begin{lemma}\label{lem:update_ai}
Given a pointer  to a node  and handles to  and  in ,
we may compute  and insert it to or remove it from the anchored index, making sure that  contains all necessary strings.
This takes  time with high probability.
\end{lemma}
\begin{proof}
First, observe that ,  and its production
rule let us compute the indices of fragments of  equal to
 and  such that .
Consequently, these strings can be added to  via  operations (\cref{lem:splitop}) starting from .
Similarly, their reverses can be extracted from .
Finally, all the constructed strings need to be added using  of the order-preserving component .
All this takes  time with high probability and lets us add  to the anchored index.

Next, observe that the key can also be constructed in  time. This is because  lets us retrieve
the index of first character of the fragment represented by  and the component , by \cref{lem:lists},
returns a pointer to a given position in logarithmic time.

Finally, note that the size of the anchored index is linear with respect to the total length of strings in .
Hence, by \cref{thm:ai}, updating the anchored index requires  time.
\end{proof}

\begin{lemma}\label{lem:makeopn}
For a string , we can execute  and  in  time
with high probability.
\end{lemma}

\begin{proof}
First, let us focus on . We use \cref{lem:makeop} to make sure that  contains  and .
We also update  to enable pointers to characters of .
Finally, we traverse  and apply~\cref{lem:update_ai} to insert  for all nodes  at positive levels.
All this takes  time with high probability.

Implementation of  is even simpler. First, we traverse   and apply~\cref{lem:update_ai} to remove 
for all nodes  at positive levels. Next, we remove  from .
\end{proof}

The key observation to efficient implementation of  and 
is that the entries in the anchored index do not need to be modified for nodes at or below context-insensitive layers.
Thus, we only need to process nodes above it; the following Lemma lets us list them efficiently.
\begin{lemma}\label{lem:list-above}
Let  be a decomposition of  corresponding to a layer  of .
Given a run-length encoding of  of length  we may list pointers to all nodes of  that are (proper) ancestors of  in
 time.
\end{lemma}
\begin{proof}
Roughly speaking, we would like to run a DFS on  that starts in the root node and is trimmed whenever we encounter a node belonging to .
However, we do not have a list of nodes belonging to , but only the run-length encoding of the corresponding decomposition.
Since nodes on any root-to-leaf path have distinct signatures, this is not a major obstacle.
We still run a DFS that visits the children of each node starting from the leftmost one.
We observe that we encounter the first node belonging to  exactly when we first visit a node  with  being the first symbol in . At this moment we backtrack and continue the DFS until we visit a node with signature equal to the second symbol of .

This way, we visit all nodes of  and their ancestors.
As a result, the algorithm is not efficient if the size of  is much bigger than .
Thus, we introduce the following optimization.

Let  be the run-length encoding of decomposition .
Assume that we detect a node belonging to  with signature , which is a child of a node .
By \cref{fct:layer}, we may immediately skip the following  children of .
This can only happen  times during the execution of our algorithm.

As a result we visit a set of nodes of  that forms a tree containing  leaves.
Since every vertex of  has at least two children, in total we visit  nodes.
\end{proof}

\begin{lemma}\label{lem:concsplitop}
We can execute  and  operations in 
time with high probability.
\end{lemma}
\begin{proof}
Let  and suppose we split  into  and , or concatenate  and  into .
First, we make sure that the resulting strings and their reversals are represented by 
(using \cref{lem:splitop} or \cref{lem:concop}).

Next, we construct their context-insensitive decompositions  and  of  and , respectively (using~\cref{lem:layerimpl2}).
By \cref{fact:two-decompositions} their concatenation  forms a decomposition of .
We use \cref{lem:list-above} to list nodes of  above , nodes of  above , and nodes of 
above .

Next, we apply \cref{lem:update_ai} to insert or remove  for all the listed nodes.
First, we process removed nodes, then update the  components, and finally we process the inserted nodes.
Note that for all nodes  below the context-insensitive decompositions the  entries do not need to be updated.
\end{proof}

\subsubsection{Summary}
\begin{theorem}\label{thm:long}
A collection of non-persistent strings can be maintained under operations  and 
working in  time,  and  requiring  time
so that  takes  time, where  is the total
number of occurrences and  is the total input size of updates so far.
Each occurrence returned by  requires additional  time to locate.
All operations may fail with probability  where  is an arbitrarily large constant.
\end{theorem}
\begin{proof}
We implement the operations according to \cref{lem:makeopn,lem:concsplitop,lem:findop}, which results
in the running times provided. Recall that the implementations of these operations fail as soon as
they encounter a string of length up to  whose depth is not . By~\cref{lem:small_depth},
this happens only with probability  if the constant at the  bound is sufficiently large.
\end{proof}
\subsection{Short Patterns}\label{sec:short}

Given a parameter , we design a separate implementation for short patterns, where
both  and  work in  time, and  generates
all  occurrences in  time if .

Intuitively, the main idea is to maintain partitions of all strings in  into blocks with lengths from . If  is such
a partition of a string  with , every pair of adjacent blocks  (or  if there is just one block) is stored in a \emph{generalized suffix tree}.
A generalized suffix tree of a collection of strings
 is a compacted trie storing all strings of the form _ij=1,2,\ldots,|t_i| \ is a unique character. It is known that a generalized suffix tree can be maintained
efficiently under inserting and deleting strings from the collection; see~\cite{dictionary}.
In more detail, it is easy to see that inserting a~string  requires splitting up to  edges of the tree, where
splitting an edge means dividing it into two shorter edges by creating a new explicit node,
where we connect a new leaf. All these edges can be retrieved in  time if every explicit
node representing a string  stores a \emph{suffix link} to the explicit node representing string .
Deletions are processed in a symmetric manner.
In the original implementation~\cite{dictionary}, it was assumed that the size of the alphabet is constant, and
an additional effort was required to avoid making every _iuccO(1)O(|p|)pppps=s_1 s_2 \ldots s_bb=1s_1s_i s_{i+1}i+1=bs_ipups_is_{i-1}s_is_i s_{i+1}s_{i-1}s_is_i s_{i+1}s_is_{i-1}s_{i-1} s_i2^{\ell+1}\leq 4|p|O(|p|+occ)s_i s_{i+1}\lists(\coll)s_i\lists(\coll)w\in \coll\makeop(w)ww_1w_2\ldots w_bw_{i}w_{i+1}\dropop(w)w = w_1 w_2 \ldots w_bw_i w_{i+1}w_1b=1O(|w|)\concop(w,w')|w|,|w'| \geq 2^{\ell}w = w_1 w_2 \ldots w_bw' = w'_1 w'_2 \ldots w'_{b'}w_b w'_1O(2^\ell + \log |ww'|) = O(2^\ell + \log n)w'|w| + |w'| < 2^{\ell+1}ww'ww'ww'ww'w_bw's\in [2^\ell,3\cdot 2^\ell)2^{\ell+1}2^\ells-2^\ell \in [2^\ell,2^{\ell+1})O(2^\ell)www'O(\log |w|)=O(\log n)\splitop(w,k)|w| < 2^{\ell+1}ww[..k]w[k+1..]ww[k]w_iw = w_1 w_2 \ldots w_b|w_1 w_2 \ldots w_i| \geq k|w_1 w_2 \ldots w_{i+1}| > kww_1 w_2 \ldots w_{i-1}w_iw_{i+1} w_{i+2} \ldots w_bw_{i-1} w_ii>1 w_{i} w_{i+1}i<bO(2^\ell)w_i[..k']w_i[k'+1..|w_i|]k' = k-|w_1 w_2 \ldots w_{i-1}|\makeop\concopw[..k]w[(k+1)..]O(2^\ell+\log n)\concop\splitopO(2^\ell + \log n)\makeop(w)\dropop(w)O(|w|)\findop(p)2^{\ell-1} \le |p|< 2^{\ell}O(|p|+occ)occn\findop(p)O(\log n)\concop\splitopO(\log^2 n)\makeop(w)\dropop(w)O(|w|\log n)\findop(p)O(|p|+occ)occn\findop(p)O(\log n)\frac{1}{n^c}cI_\ell\ell1\le 2^{\ell-1} \le \log^2 n\findop|p|\ge \log^2 nO(|p|+occ)|p|\makeop(w)\dropop(w)O(|w|\log n)I_\ellO(|w|)O(|w|\log\log n)\splitop\concopO(\log^2 n)I_\ellO(2^\ell+\log n)O(\log^2n + \log n \log \log n) = O(\log^2 n)nI_\ell\ellI_{\ell+1}nn_{\ell}:=2^{2^{\ell/2}}nn_{\ell-1}=2^{2^{(\ell-1)/2}}I_{\ell+1}O(n_{\ell-1}\cdot (2^\ell+\log n_{\ell-1}))=O(n_{\ell-1}^{\sqrt{2}})=o(n_{\ell}-n_{\ell-1})I_{\ell+1}nn_{\ell-1}n_{\ell}I_{\ell}I_{\ell}I_{\ell+1}\splitop\concop\coll\makeop(\str)\str\in \Sigma^+\coll:=\coll\cup \{\str\}\dropop(\str)\str\in \coll\coll:=\coll\setminus \{\str\}\concop(\str_1,\str_2)\str_1,\str_2\in \coll\coll:=\coll\cup \{\str_1\str_2\}\dropop(w_1)\dropop(w_2)\splitop(\str,k)\str\in \coll1\leq k<|\str|\coll:=\coll\cup \{\str[..k], \str[(k + 1)..]\}\dropop(w)\coll\coll0\findop(p)p\in \Sigma^+pw\in \collpw\in \collpw\grammar\grammar\grammar\grammar\makeop\concop\splitop\coll\grammarw\in \coll\sstr^{-1}(w^R)\in \sigs(\grammar)\coll\grammar\actop(s,s')s,s'\in \sigs(\grammar)\sstr(s)=\sstr(s')^R\coll := \coll\cup \{\sstr(s)\}\dactop(s,s')s,s'\in \sigs(\grammar)\sstr(s)=\sstr(s')^R\coll := \coll\setminus \{\sstr(s)\}\findop(p)p\in \Sigma^+pw\in \coll\actop\dactop\grammar(\coll)\grammar(\coll)\coll\coll\sub \Sigma^+s\sigs(\grammar(\coll))w\in \collv\in \stree(w)\ussig(v)=st\makeop\concop\splitop\actop\dactop\actopO(1+|\grammar(\coll')\setminus \grammar(\coll)|\cdot (\log n + \log t + \log|\grammar(\coll')|))1-\frac{1}{t^c}n=|\sstr(s)|\coll'=\coll\cup\{\sstr(s)\}\dactopO(1+|\grammar(\coll)\setminus \grammar(\coll')|\cdot (\log n + \log t + \log|\grammar(\coll)|))1-\frac{1}{t^c}n=|\sstr(s)|\coll'=\coll \setminus \{\sstr(s)\}\findopO(|p|+\depth(p)(\log |p|+\log t + \log |\grammar(\coll)|))1-\frac{1}{t^c}O(\depth(w))pw\in\collv\in \stree(w)w\in \colls \in \sigs(\grammar(\coll))\itanch(s)\sanch(s)ss\in \sigs(\grammar(\coll))\sstr(s)\in \coll\spars(s)ss'\in \grammar(\coll)ss'\to s^kks'\to s_1ss_1\in \sigs(\grammar(\coll))s'\to ss_2s_2\in \sigs(\grammar(\coll))s'\in \spars(s)s'\spars(s)s'pw\in \collO(|p|+\depth(p))O(\depth(p))O(\log |p|+\log t)O(\log |\grammar(\coll)|)O(|p|+\depth(p)(\log |p|+\log t + \log |\grammar(\coll)))1-\frac{1}{t^c}p_\ell|p_rp=p_\ell p_rs\in \sigs(\grammar(\coll))p\sstr(s)p_\ell|p_rp\sstr(s)s\in \sigs(\grammar(\coll))pw\in \collvp\ussig(v)sw\in \collvs=\ussig(v)O(\depth(w))wO(\depth(w))O(1+\depth(w)-\slev(s))\sstr(s)\in \colls\spars(s)s's'\to s_1s_2ks'\to s^ks\slev(s')\ge \slev(s)+1pspw\in \coll\sigs(\grammar(\coll))\actopw\coll\sstr^{-1}(w)\sigs(\grammar(\coll))\stree(w)\sigs(\grammar(\coll))v\in\stree(w)\ussig(v)v\stree(w)\sigs(\grammar(\coll))v\ussig(\spar{w}(v))\spars(s)vs\sigs(\grammar(\coll))vs\to s_1s_2s\to s_1^k\grammar(\coll)\itanch(s)ww^RO(\log n+\log t + \log|\grammar(\coll)|)\sigs(\grammar(\coll')\sm\grammar(\coll))\sstr^{-1}(w)\sstr^{-1}(w)\grammar(\coll)\sstr^{-1}(w)\coll\dactop\sstr^{-1}(w)\sigs(\grammar(\coll))s\stree(w)v\ussig(v)=s\sigs(\grammar(\coll))s':=\ussig(\spar{w}(v))\spars(s)s'\spars(s)\sstr(s)\in \colls\sigs(\grammar(\coll))vs\to s_1s_2s\to s_1^k\itanch(s)O(\log n+\log t + \log|\grammar(\coll)|)\grammar(\coll)\sm\grammar(\coll')\splitop\concopw_1,w_2\in \Sigma^+w=w_1w_2|\grammar(w)\sm \grammar(\{w_1,w_2\})| = O(\depth(w))|\grammar(w_1)\sm \grammar(w)| = O(\depth(w_1))|\grammar(w_2)\sm \grammar(w)| = O(\depth(w_2))D_1D_2w_1w_2\grammar(D_i) \sub \grammar(w)\cap \grammar(w_i)i=1,2D_i|\rle(D_i)|=O(\min(\depth(w),\depth(w_i))){|\grammar(w)\sm {\grammar(D_1\cdot D_2)}|={O(\depth(w)+|\rle(D_1\cdot D_2)|)}}|\grammar(w_i)\sm \grammar(D_i)|\le O(\depth(w_i)+|\rle(D_i)|)\coll\concop\splitopO((\log n + \log t)(\log N + \log t))\makeop\dropopO(n(\log N + \log t))\findopO(|p|+(\log |p|+\log t)(\log N+\log t) + occ(\log N+\log t))nN\collt\makeop\concop\splitop\dropopO(t^{-c})c\coll\mword\grammarww^Rw\collO(n+\log t)\makeopO(\log n + \log t)\splitop\concop\coll\grammar(\coll)O(N)|\grammar(s)|\le |\sstr(s)|\makeop\dropopO(n(\log n + \log t + \log N))\splitop\concop\grammar(\coll)O(\log n + \log t)\splitop\concopO((\log n + \log t)(\log N + \log t))\findop|\grammar(\coll)|=O(N)\depth(p)\le \log |p|+\log t|p|>N\log |p|\le \log Nn\poly(n)O(N\log n +M\log^2 n)N\makeopM\makeop\concop\splitopO(|p|+\log^2 n + occ\log n)TT_1tT_ii\in [2,t]T_iT_{i-1}ppT_iT_1,T_2,\ldots,T_tTpTT_i1\leq i\leq tpT_iT_jj<iT_1=\epsT_2=aT_3=acT_4=abcT_5=acacT_4T_3T_5T_1,\ldots,T_tTpo_1o_2pTo_iT_{\tau(o_i)}\delta(o_i)id(o_i)T_{\tau(o_i)}[\delta(o_i)..(\delta(o_i)+|p|-1)]o_io_1o_2\tau(o_1)\leq \tau(o_2)id(o_1)\neq id(o_2)o_3p\tau(o_1)<\tau(o_3)<\tau(o_2)id(o_3)=id(o_1)T_1,\ldots,T_tTo(\tau(o),\delta(o))o_1,o_2,\ldots\tau(o_i)\leq \tau(o_{i+1})i2A_tt=1A_1=\emptysett>1A_{t-1}T_tT_{t-1}T_t=XcYT_{t-1}=XYXYcT_tppp_1cp_2p_1\neq\epsX\neq\epsp_1Xcp_2cYp_1|cp_2X|cYp_1p_2X|cYp_1|cp_2p=p_1cp_2p_1=\epsY\neq\epsc|YA_tA_t=A_{t-1}\cup\{X|cY,c|Y\}T_tT_{t-1}T_t=XYT_{t-1}=XBYXYBT_tpXYpp_1p_2p_1p_2p_1Xp_2YA_t=A_{t-1}\cup\{X|Y\}T_tT_{t-1}T_{t-1}=XABYT_t = XBAYA,BX,YA_tXB|AYX|BAYX\ne \epsXBA|YY\ne \eps3|A_t|=O(t)\coll\makeop\splitop\concopA_tO(t\log t)\tau:A_t\to \{1,\ldots,t\}a\in A_j\setminus A_{j-1}\tau(a)=ja=X|YA_{j}\setminus A_{j-1}\delta(a)XYT_jT_j[\delta(a)-|X|..\delta(a)+|Y|-1]=XY\delta(a)aA_jA=A_t\tauT_1,\ldots,T_tn(x,y)kO(\log{n}+k)xvY_vvypY_vpyY_lY_rlrvY_vY_lY_rO(|Y_l|+|Y_r|)O(n\log{n})Y_v[x_1,x_2]\times [y_1,y_2]y_1y_2Y_{\text{root}}O(\log{n})x[x_1,x_2]kkO(\log{n})Y_v\tauY_vY_vO(1)kO(\log{n})O(\log{n})(Y_v,a,b)Y_v[a..b]O(1)k(Y_v,a,b)\tau(Y_v[c])Y_v[c](Y_v,a,c-1)(Y_v,c+1,b)O(\log{n})kO(k\log{n})pp=p_1p_2p_1,p_2\coll|p|\makeop|p|-1\splitop\collaT_{\tau(a)}[\delta(a)-|p_1|..\delta(a)+|p_2|-1]O(|p|)O(|p|)\anchfindpO(|p|\log{t})kO(|p|\log{t}+k\log{t})1c\in\SigmaccT_1,T_2,\ldots,T_tTO(t \log t)TpO(|p|\log t)tO(\log t)T_1,\ldots,T_tAAkA\cap([x_1,x_2]\times [y_1,y_2])Y_vY_{c_1},Y_{c_2},\ldotsc_1,c_2,\ldotsvLLO(\log{\log{n}})insert(x,y)xy\in Lmark(x)x\in Lsucc(x)x\in LL\Gammaa\geq 8v\in\Gammaw(v)vc^v_1,c^v_2,\ldotsv\Gamma\Gammavvh2a^hh\frac{1}{2}a^h\Gammaa\geq 8\Gamma4a\Gammaa/4\Gamman\GammaO(\log_a{n})e\Gammavw(v)>2a^hw(v)\frac{3}{2}a^hvk(v)j(v)\max\left(\sum_{i=0}^{j(v)}w(c^v_i),\sum_{i=j(v)+1}^{k(v)} w(c^v_i)\right)\frac{3}{4}a^h+2a^{h-1}j(v)c^v_ii\leq j(v){c^v_i}',{c^v_i}''vvvv',v''w(v)2a^hvv'v''v'c^v_1,\ldots,c^v_{j(v)}v''c^v_{j(v)+1},\ldots,c^v_{k(v)}O(\log{n})\Gammanv\in \GammaD_vvD_vO(1)D_vO(U(n))D_vO(Q(n))D_v.LD_vD_vD_v\GammaO(U(n)\log n)e\Gammaev\GammaleD_vO(U(n))D_v.LO(1)w(v)>\frac{3}{2}a^lD_{v'}D_{v''}\frac{1}{2}a^lvD_vD_{v'}D_{v''}vv',v''D_{v'}D_{v''}vO(U(n))w(v)=2a^lD_{v'},D_{v''}v'v''D_vinsert(x,y,w,x')y\in D_vwvxyxD_wx'y=\perpxw=\perpxvnext(x,w)prev(x,w)x\in D_vwfirst(x,y)x,\ldots,yD_vv\in\GammainsertnextprevD_vO(\log{\log{n}})firstO(1)w_1,w_2,\ldots,w_kvk\leq 4a=O(1)L_{w_i}w_iinsert(x,y,w,x')xL_{w_i}xL_wL_{w_i}insertnextprevfirst(x,y)D_v\tau(x)insert(x,*,*,*)\mathcal{CT}e_1,\ldots,e_lD_v\mathcal{CT}(e)\mathcal{CT}ee_je_j\tau(e_j)\mathcal{CT}(e_j)e_jj>1j<k\mathcal{CT}(e_j)e_1,\ldots,e_{j-1}e_{j+1},\ldots,e_ke_a,\ldots,e_be_c\mathcal{CT}(e_c)\tmlca\mathcal{CT}(e_a)\mathcal{CT}(e_b)e\tau(e)eD_v\mathcal{CT}e'e''eD_ve'e''\tau(e')=-\infty\tau(e'')=\infty\tau(e')<\tau(e'')\tmlca(\mathcal{CT}(e'),\mathcal{CT}(e''))=\mathcal{CT}(e')\tmlca(\mathcal{CT}(e),\mathcal{CT}(e'))=\mathcal{CT}(e')\tmlca(\mathcal{CT}(e),\mathcal{CT}(e''))=\mathcal{CT}(e'')\mathcal{CT}(e)\mathcal{CT}(e'')\mathcal{CT}(e'')\mathcal{CT}(e')\mathcal{CT}(e^*)\mathcal{CT}(e'')\mathcal{CT}(e)\tau(e)>\tau(e^*)>\tau(e'')>\tau(e')e^*e'e''\mathcal{CT}(e^*)\mathcal{CT}(e)\mathcal{CT}(e')\mathcal{CT}(e'')e'e''\mathcal{CT}(e')\mathcal{CT}(e'')\mathcal{CT}(e)\mathcal{CT}(e^*)\mathcal{CT}(e)\mathcal{CT}(e'')\mathcal{CT}\tmlca\tmlcaO(1)e\mathcal{CT}(e'')\GammaD_vA\GammaAr\GammaAD_rB_yB_yAy(x,y)B_y(x',y')ys_rD_ryO(\log{n})r=v_1,\ldots,v_b(x',y')v_1,v_2,\ldots(x,y)D_{v_i}D_{v_i}D_{v_{i+1}}(x,y)(x,y)v_bO(\log{n}\log{\log{n}})k[x_1,x_2]\times [y_1,y_2]O(\log{n})\Gamma[x_1,x_2]va_v,b_vD_va_v,\ldots,b_vv[y_1,y_2]prevnextvvkO(\log{n}\log{\log{n}}+k\log{n})TO(\log{t}\log{\log{t}})TO(m \log{t}\log\log{t})mO(\log t)O(\log{n})O(\log{n})\mword[0, 2^{\mword})tO(t)xx1-\frac{1}{2n^c}c>0nn^{O(1)}u_1,\ldots,u_mDn_iu_1,\ldots,u_ic>2u_it_i1\le t_i \le n_i^{\sqrt{c}}1-\frac{1}{2n_i^c}DO(\sum_{i=1}^m t_i)1-\frac{1}{n_m^{\omega(\sqrt{c})}}T_i=\sum_{j=1}^i t_jn_i \le T_i\le n_i^{1+\sqrt{c}}N=n_mlT_l \ge \sqrt{N}i\ge la := 1-\sum_{k=1}^\infty \frac{1}{2k^2}=\frac{12-\pi^2}{12}ka^kk=\omega(\log N)\frac{1}{N^{\omega(1)}}T_{l-1}O(\log N)T+O(T_{l-1}\log N)=T+O(\sqrt{N}\log N)=O(T)\sum_{i=l}^m \frac{1}{2n_i^c} \le \frac{m}{2n_l^c}N\ge mn_l^{1+\sqrt{c}} \ge T_l \ge \sqrt{N}O(1)O(1)\symbols\sstr\sstr^{-1}\Sigma\symbols\Sigma \subseteq \symbols(S_1,S_2)\in \symbolsS_1,S_2\in \symbols(S,k)\in \symbolsS\in \symbolsk\in \mathbb{Z}_{\ge 2}S\in \symbolss\in \Sigma^+\sstr(S)\slength(S)=|\sstr(S)|\hs_i(S)S\in \symbolsi\in \mathbb{Z}_{\ge 1}\grammar(w)w\in \Sigma^+\shrink{i}\grammar(\coll)w\in \coll\symbols\grammar(w)w\in \Sigma^+\grammar(\Sigma^+)\slev(S)SS\in \Sigma\slev(S)=0S=(S_1,k)k\in \mathbb{Z}_{\ge 2}\slev(S)=\slev(S_1)+1S=(S_1,S_2)\slev(S)l\max(\slev(S_1), \slev(S_2))\hs_{l/2}(S_1) = 0\hs_{l/2}(S_2) = 1S\grammar(\Sigma^+)S\shrink{\slev(S)}\Sigma^+\grammar(\Sigma^+)S\grammar(\Sigma^+)S\grammar(\sstr(S))\sstr(\cdot)\grammar(\Sigma^+)\Sigma^+l=\slev(S)S\in \grammar(w)ww\sstr(S)\cshrink{l}(w)S0\le i \le l\cshrink{i}(w)\cshrink{i}(\sstr(S))i=00\le i < lii+1\shrink{i+1}\cshrink{i}(w)\cshrink{i}(\sstr(S))w\cshrink{i+1}(w)\shrink{i+1}\cshrink{i}(w)\shrink{i+1}\shrink{i}(\sstr(S))\shrink{l}(\sstr(S))=SS\grammar(\sstr(S))\sstr(S_1)=w=\sstr(S_2)S_1=S_2\grammar(w)\sstr\Sigma^+\grammar(\Sigma^+)\sstr^{-1}\grammarw\sstr^{-1}(w) \in \grammar\grammar\grammar\grammar \subseteq \grammar(\Sigma^+)w\in \coll$.
\end{invariant}


\bibliographystyle{plainurl}
\bibliography{paper}

\end{document}
