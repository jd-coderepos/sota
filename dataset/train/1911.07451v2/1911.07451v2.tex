\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}         \usepackage{mathtools}
\usepackage{makecell, verbatim, sidecap}

\usepackage[margin=4pt,font=small,labelfont=bf,labelsep=endash,tableposition=top]{caption}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}


\renewcommand{\texttt}[1]{}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi

\def\eg{{\it e.g.}\xspace}
\def\ie{{\it i.e.}\xspace}
\def\Real{{\mathbb R}}
\def\real{{\mathbb R}}
\def\R{{\mathbb R}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\renewcommand{\vec}[1]{\ensuremath{\pmb{#1}}}
\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\set}[1]{\ensuremath{\mathscr{#1}}}



\makeatletter
\@tfor\next:=abcdefghijklmnopqrstuvwxyxz\do
{\begingroup\edef\x{\endgroup
		\noexpand\@namedef{v\next}{\noexpand\vec{\next}}}\x}
\@tfor\next:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\do
{\begingroup\edef\x{\endgroup
		\noexpand\@namedef{m\next}{\noexpand\mat{\next}}}\x}
\@tfor\next:=ABCDEFGHIJKLMNOPQRSTUVWXYZ\do
{\begingroup\edef\x{\endgroup
		\noexpand\@namedef{s\next}{\noexpand\set{\next}}}\x}
\makeatother


\def\sG{\mathcal{G}}          



\newcommand{\Transpose}{\ensuremath{{\!\top}} }    \let\T\Transpose


\newcommand{\opt}{\ensuremath{\textsc{Opt}}}

\newcommand{\norm}[1]{\ensuremath{\left\|#1 \right\|}}

\newcommand{\eyes}{\ensuremath{{\mathbf{I}}}}

\newcommand{\1}{{\mathbbm{1}}}           



\begin{document}

\title{DirectPose: Direct End-to-End Multi-Person Pose Estimation}

\author{Zhi Tian, Hao Chen, Chunhua Shen\thanks{The first two authors
equally contributed to this work. Corresponding author: \texttt{chunhua.shen@adelaide.edu.au} }
\
\vo_0, \vo_1, ..., \vo_{K-1}, \vv_0, \vv_1, ..., \vv_{K-1} = \zeta(\mF),

	\vx_t = \phi_t(\vv_t) + \vo_t, \qquad t = 0, 1, ..., K-1.

Note that the coordinates need to be re-scaled by the down-sampling ratio of . We omit the re-scaling operator here for simplicity. Note that all the operations in KPAlign module are differentiable and therefore {\it the whole model can be trained in an end-to-end fashion with standard back-propagation}, which sets our work apart from previous bottom-up or top-down keypoint detection frameworks such as CMU-Pose \cite{cao2017realtime} or Mask R-CNN \cite{he2017mask}. \emph{Being end-to-end trainable also makes the locator
be able to
learn to localize the keypoints without explicit supervision, which is
critically
important to KPAlign.
}


\paragraph{Grouped KPAlign:} The aforementioned KPAlign module is required to sample  feature vectors for  keypoints. This is actually not necessary because some keypoints (\eg, nose, eyes and ears) always populate in a local area. Therefore, we propose to group the keypoints and the keypoints in the same group will use the same feature vector, which reduces the number of sampled feature vectors from  to  and achieves a similar performance, where  is the number of groups.
\paragraph{Using Separate Convolutional Features:} In the KPAlign described before, all of the keypoint groups use the feature maps  as the input. However, we find that the performance can be improved \emph{remarkably} if we use separate feature maps for the  keypoint groups (\ie, using ). In that way, the demand for the information encoded in a single  can be further mitigated. In order to reduce the computational complexity, the number of channels of each  is set as  (\ie, from  to ).
\paragraph{Where to Sample Features?} For the sake of convenience, the sampler in the aforementioned aligner samples features on the input feature maps of the locator, and therefore the predictor and locator take as inputs the same feature maps. However, it is not reasonable as the locator and predictor require different levels of feature maps. The locator predicts the initial but imprecise locations for all the keypoints (or keypoint groups) of an instance and thus requires high-level features with a larger receptive field. In contrast, the predictor needs to make precise predictions but only for the keypoints in a local area because the features have been aligned by the aligner. As a result, the predictor prefers high-resolution low-level features with a smaller receptive field. To this end, we feed lower levels of feature maps into the sampler. Specifically, if a locator uses feature maps  and  is not the finest feature maps, the sampler will take  as the input. If  is already the finest feature maps, the sampler will still sample on it.

\subsection{Regularization from Heatmap Learning}
It is well-known that regression-based tasks are difficult to learn \cite{glorot2010understanding, sun2018integral} and have poor generalization. That is why almost all previous keypoint detection methods \cite{cao2017realtime, he2017mask, wei2016convolutional, sun2019deep, newell2017associative} are based on heatmap prediction, which cast keypoint detection to a pixel-to-pixel prediction task. However, this reformulation makes an end-to-end training infeasible since it involves the \emph{non-differentiable} transformation between the heatmap-based prediction and desired continuous keypoint coordinates. Therefore, an end-to-end keypoint detection framework has to be regression-based.

As a result, we need to seek a way that can make the regression-based task easier to learn and generalize. To this end, given the fact that heatmap-based learning is much easier, we use the heatmap-based prediction task as an auxiliary task.
Thus,
the heatmap-based task can serve as a hint for the regression-based task and thus can regularize the task. In our experiments, the jointly learning significantly boosts the performance of the regression-based task. Note that \emph{the heatmap-based task is only used as an auxiliary loss during training}. It is
removed when testing.

\paragraph{Heatmap Prediction:} As shown in Fig.~\ref{fig:key_point_framework}, the heatmap prediction task takes as input the FPN feature maps  with downsampling ratio being . Afterwards, two  conv layers with channel being  are applied here, which are followed by another  conv layer with output channel being  for the final heatmap prediction, where  is the number of keypoints for each instance. Previous heatmap-based keypoint detection methods \cite{cao2017realtime} generate un-normalized Gaussian distribution centered at each keypoint and thus generate the heatmaps in a per-pixel regression fashion. In contrast, since our framework does not
rely
on the heatmap prediction when testing, we
perform
a per-pixel classification here for simplicity. Note that we make use of multiple binary classifiers (\ie, one-versus-all) and therefore the number of output channels is  instead of .



\paragraph{Ground-truth Heatmaps and Loss Function:} The ground-truth heatmaps are generated as follows. On the heatmaps, if a location is the nearest location to a keypoint with type , the classification label for the location is set as , where . Otherwise, the label is .\footnote{Strictly speaking, the generated ground-truth is a set of binary labels, rather than the conventional real-valued
        heatmap. We slightly abuse the term here.
}
Finally, in order to overcome the imbalance between positive and negative samples, we use focal loss \cite{lin2017focal} as the loss function.

\section{Experiments}
Our experiments are conducted on human keypoint detection task of the large-scale benchmark COCO dataset \cite{lin2014microsoft}. The dataset contains more than  person instances with  annotated keypoints. Following the common practice \cite{cao2017realtime, he2017mask}, we use the COCO  \texttt{trainval35k} split ( images) for training and \texttt{minival} split ( images) as validation for our ablation study. We report our main results on the \texttt{test}-\texttt{dev} split ( images). Unless specified, we only make use of the human keypoint annotations without bounding-boxes. The performance is computed with Average Percision (AP) based on Object Keypoint Similarity (OKS).
\paragraph{Implementation Details:} Unless specified, ResNet-50 \cite{he2016deep} is used as our backbone networks. We use two training schedules. The first is quick and used to train a fast prototype of our models in ablation experiments. Specifically, the models are trained with stochastic gradient descent (SGD) on 8 V100 GPUs for  epochs with a mini-batch of 16 images. For the main results on \texttt{test}-\texttt{dev} split, we use a longer training schedule; the models are trained for  epochs with a mini-batch of 32 images. We set the initial learning rate to  and use a linear schedule  to decay it. Weight decay and momentum are set as 0.0001 and 0.9, respectively. We initialize our backbone networks with the weights pre-trained on ImageNet \cite{deng2009imagenet}. For the newly added layers, we initialize them as in \cite{lin2017focal}. When training, the images are randomly resized and horizontally flipped with probability being , and the images are also randomly cropped into  patches. When testing, we run inference on the whole image and the testing images are resized to have its shorter side being  and their longer side less or equal to . If bounding-box detection is available, NMS is applied to the detected bounding-boxes. Otherwise, we do NMS on the minimum enclosing rectangles of keypoints of the instances. The NMS threshold is set as  for all experiments.
\subsection{Ablation Experiments}
\subsubsection{Baseline: the naive end-to-end framework}
\begin{table}
	\small
	\begin{center}
	\begin{tabular}{ l | c c c c c}
		\hline
		& AP & AP & AP & AP & AP \\
		\hline\hline
		Baseline & 43.4 & 73.8 & 45.1 & 38.9 & 50.9 \\
		w/ KPAlign & 43.0 & 74.2 & 43.9 & 39.0 & 49.6 \\
		w/ KPAlign & \textbf{50.5} & \textbf{77.6} & \textbf{54.9} & \textbf{44.4} & \textbf{60.0} \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Ablation experiments on COCO \texttt{minival} for the proposed KPAlign module. Baseline: the naive keypoint detection framework, as shown in Fig. \ref{fig:naive_framework}. ``w/ KPAlign": using the KPAlign module in the naive framework but disabling the aligner in it.
``w/ KPAlign":
	using the full-featured KPAlign module.}
	\label{table:naive_w_kpalign}
\end{table}
We first experiment with the naive end-to-end keypoint detection framework in Fig.~\ref{fig:naive_framework} by replacing the bounding-box head in FCOS with the keypoint detection head. Moreover, as described before, we use pseudo-boxes to compute the label for each location on FPN feature maps during training.
As shown in Table~\ref{table:naive_w_kpalign}, the naive framework can only obtain low performance ( in AP). As mentioned before, the low performance is due to the misalignment between the features and keypoint predictions. In the following experiments, we will show that our proposed KPAlign can overcome the issue.
\subsubsection{Keypoint alignment (KPAlign) module}
In this section, we equip the above naive framework with our proposed KPAlign module. As shown in Fig.~\ref{fig:key_point_framework}, KPAlign serves as the final prediction layer, which was a standard convolutional layer in the naive framework. As shown in Table~\ref{table:naive_w_kpalign}, KPAlign improves the keypoint detection performance by a large margin (more than  points in AP).
In order to demonstrate that the improvement is indeed due to the retained alignment between the features and keypoint predictions rather than other factors (\eg, slightly more network parameters), we conduct another experiment in which the aligner of KPAlign is disabled. In other words, the offsets predicted by the locator are ignored and thus all the keypoints of an instance are predicted with the same features as in the naive framework. As shown in Table~\ref{table:naive_w_kpalign}, without the aligner, the performance drops dramatically to  in AP, which is nearly the same as the performance of the naive framework. Therefore, it is safe to claim that the improvement is due to the retained alignment.

\subsubsection{Grouped KPAlign}
\begin{table}
	\small
	\begin{center}
	\begin{tabular}{ l | c c c c c}
		\hline
		& AP & AP & AP & AP & AP \\
		\hline\hline
		KPAlign & 50.5 & 77.6 & 54.9 & 44.4 & 60.0 \\
		+ Grouped & 50.6 & 77.5 & 55.4 & 44.3 & 60.2 \\
		+ Sep. features & 51.4 & 78.2 & 55.6 & 45.6 & 60.6  \\
		+ Better sampling & \textbf{52.2} & \textbf{78.3} & \textbf{56.6} & \textbf{46.3} & \textbf{61.7} \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Ablation experiments on COCO \texttt{minival} for the design choices in KPAlign. ``+ Grouped": using Grouped KPAlign, which has slightly better performance than the original but largely reduce the number of sampled feature vectors, thus being faster. ``+ Sep. features": using separate (but slimmer) feature maps for different keypoint groups, which has similar computational complexity but improved performance. ``+ Better sampling": the predictor samples features on finer feature maps (\ie, from  to ).}
	\label{table:better_kp_align}
\end{table}
As described before, it is not necessary to sample  (\ie,  on COCO) feature vectors (one feature vector per keypoint) as some keypoints are always together and thus can be predicted with the same feature vector. In this experiments, we divide the keypoints into 9 groups\footnote{These groups respectively include (nose, left eye, right eye, left ear, right ear), (left shoulder, ), (left elbow, left wrist), (right shoulder, ), (right elbow, right wrist), (left hip, ), (left knee, left ankle), (right hip, ) and (right knee, right ankle).}, which reduces the number of the sampled feature vectors from  to  and makes the module faster. As shown in Table~\ref{table:better_kp_align}, the Grouped KPAlign can achieve slightly better performance than the original KPAlign. Therefore, in the sequel, we will use the Grouped KPAlign for all the following experiments. We also attempted other ways forming the groups but they achieve a similar performance.

\subsubsection{Using separate convolutional features}
As described before, using separate feature maps for these keypoint groups can improve the performance. Here we conduct an experiment for this. As shown in Table~\ref{table:better_kp_align}, using separate feature maps can boost the performance by  in AP (from  to ). Note that the number of channels of these separate feature maps is reduced from  to , and thus the model has similar computational complexity to the original one. As a result, the model using separate feature maps achieves a better trade-off between speed and accuracy.

\subsubsection{Where to sample features in KPAlign?}
In this experiment, the sampler samples on finer feature maps, as described in Sec.~\ref{sec:kpalign}, since the locator requires low-resolution high-level feature maps with a larger receptive field while the predictor prefers high-resolution low-level feature maps. As shown in Table~\ref{table:better_kp_align} (``+ Better Sampling"), using the sampling strategy can improve the performance to . Note that using the sampling strategy does not increase the computational complexity of the model. Moreover, the better sampling strategy improves the AP and AP by  and , respectively, which implies that the sampling strategy can result in more accurate keypoint predictions because the improvement mainly comes from the APs at higher thresholds.
\subsubsection{Regularization from heatmap learning}
\begin{table}
	\small
    \begin{center}
	\begin{tabular}{ l | c c c c c}
		\hline
		& AP & AP & AP & AP & AP \\
		\hline\hline
		Baseline & 52.2 & 78.3 & 56.6 & 46.3 & 61.7 \\
		w/  Heatmap & 57.7 & 82.8 & 63.1 & 51.8 & 66.9 \\
		w/ \space\space Heatmap & 58.0 & 82.5 & 63.3 & 52.7 & 66.6 \\
		\hline
		+ Longer sched. & \textbf{63.1} & \textbf{85.6} & \textbf{68.8} & \textbf{57.7} & \textbf{71.3} \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Ablation experiments on COCO \texttt{minival} for DirectPose with heatmap prediction. Baseline: without the heatmap learning. `` Heatmaps": predicting the heatmaps with downsampling ratio being . `` Heatmaps": predicting the heatmaps with downsampling ratio being  (\ie, using ). ``+ Long sched.": increasing the number of training epochs from  to . As shown in the table, learning with heatmap prediction can largely improve the performance. Moreover, using the design choices of the heatmaps (\eg, the resolution) have a small impact on the final performance, which is one of the advantages of our framework over previous bottom-up methods.} \label{table:e2e_pose_w_heatmaps}
\end{table}
As shown in Table~\ref{table:e2e_pose_w_heatmaps} (``w/  Heatmaps"), by jointly learning the regression-based model with a heatmap prediction task, the performance of the regression-based task can be largely improved from  to . Note that the heatmap prediction is only used during training to provide the multi-task regularization. Moreover, we also conduct experiments with the heatmap prediction with a lower resolution (\ie, ``w/  Heatmaps"). As shown in Table~\ref{table:e2e_pose_w_heatmaps}, even with the low-resolution heatmaps, the model can still yield a similar performance. This suggests that our method is not sensitive to the design choices for the heatmap learning and thus eliminates the heuristic tuning for the heatmap branch. This sets our method apart from previous heatmap-based bottom-up methods, whose performance highly depends on the design of the heatmap branch (\eg, the heatmaps' resolution and etc.).

Moreover, we find that our method is highly under-fitting and previous methods such as \cite{sun2019deep} with heatmaps learning are trained with much more epochs than ours, and therefore we increase the number of epochs from  to . As shown in Table~\ref{table:e2e_pose_w_heatmaps}, this improves the performance by  in AP.
\subsection{Combining with Bounding Box Detection}
\begin{table}
	\small
	\begin{center}
	\begin{tabular}{ c | c c c | c c c}
		\hline
		w/ BBox & AP & AP & AP & AP & AP & AP \\
		\hline\hline
		 & - & - & - & \textbf{63.1} & \textbf{85.6} & \textbf{68.8} \\
		\checkmark & 55.3 & 81.5 & 59.9 & 61.5 & 84.3 & 67.5 \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Our framework with person bounding-box detection on COCO \texttt{minival}. The proposed framework can achieve reasonable person detection results ( in AP). As a reference, the Faster R-CNN person detector in Mask R-CNN \cite{he2017mask} achieves  in AP.}
	\label{table:e2e_pose_w_bbox}
\end{table}
As mentioned before, by simply adding a bounding-box branch, the proposed framework can simultaneously detect bounding boxes and keypoints. Here we confirm it by the experiment. The bounding-box detection is implemented by adding the original box detection head of FCOS to the framework. As shown in Table~\ref{table:e2e_pose_w_bbox}, our framework can achieve a reasonable person detection performance, which is similar to the Faster R-CNN detector in Mask R-CNN ( vs. ). Although Mask R-CNN can also simultaneously detect bounding-boxes and keypoints, we further unify the two tasks into the same methodology.
\subsection{Comparisons with State-of-the-art Methods}
\begin{table}
	\small
	\begin{center}
	\begin{tabular}{ l | c c c c c }
		\hline
		Method & AP & AP & AP & AP & AP \\
		\hline\hline
		\multicolumn{6}{c}{\textbf{Top-down Methods}} \\
		\hline
		Mask R-CNN \cite{he2017mask} & 62.7 & 87.0 & 68.4 & 57.4 & 71.1 \\
		CPN \cite{chen2018cascaded} & 72.1 & 91.4 & 80.0 & 68.7 & 77.2 \\
		RMPE \cite{fang2017rmpe} & 72.3 & 89.2 & 79.1 & 68.0 & 78.6 \\
		CFN \cite{huang2017coarse} & 72.6 & 86.1 & 69.7 & 78.3 & 64.1 \\
		HRNet-W48 \cite{sun2019deep} & \textbf{75.5} & \textbf{92.5} & \textbf{83.3} & \textbf{71.9} & \textbf{81.5} \\
		\hline
		\multicolumn{6}{c}{\textbf{Bottom-up Methods}} \\
		\hline
		CMU-Pose \cite{cao2017realtime} & 61.8 & 84.9 & 67.5 & 57.1 & 68.2 \\
		AE \cite{newell2017associative} & 56.6 & 81.8 & 61.8 & 49.8 & 67.0 \\
		AE & 62.8 & 84.6 & 69.2 & 57.5 & 70.6 \\
		AE & 65.5 & 86.8 & 72.3 & 60.6 & 72.6 \\
		PersonLab \cite{papandreou2018personlab} & 65.5 & 87.1 & 71.4 & 61.3 & 71.5 \\
		PersonLab & \textbf{67.8} & \textbf{89.0} & \textbf{75.4} & \textbf{64.1} & \textbf{75.5} \\
		\hline
		\multicolumn{6}{c}{\textbf{Direct End-to-end Methods}} \\
		\hline
		\textbf{Ours (R-50)} & 62.2 & 86.4 & 68.2 & 56.7 & 69.8 \\
		\textbf{Ours (R-50)} & 63.0 & 86.8 & 69.3 & 59.1 & 69.3 \\
		\textbf{Ours (R-101)} & 63.3 & 86.7 & 69.4 & 57.8 & 71.2 \\
		\textbf{Ours (R-101)} & \textbf{64.8} & \textbf{87.8} & \textbf{71.1} & \textbf{60.4} & \textbf{71.5} \\
		\hline
	\end{tabular}
	\end{center}
	\caption{The performance of our proposed end-to-end framework on COCO \texttt{test}-\texttt{dev} split.  and  respectively denote using refining and multi-scale testing. As shown in the table, the new end-to-end framework achieves competitive or better performance than previous strong baselines (\eg, Mask R-CNN and CMU-Pose).}
	\label{table:comparsions_with_sota}
\end{table}
In this section, we evaluate the proposed end-to-end keypoint detection framework on MS-COCO \texttt{test}-\texttt{dev} split and compare it with previous bottom-up and top-down ones. We make use of the best model in ablation experiments. As shown in Table~\ref{table:comparsions_with_sota}, without any bells and whistles (\eg, multi-scale and flipping testing, the refining in \cite{cao2017realtime, newell2017associative}, and any other tricks), the end-to-end framework achieves  and  in AP on COCO \texttt{test}-\texttt{dev} split, with ResNet-50 and ResNet-101 as the backbone, respectively. With multi-scale testing, our framework can achieve  and  with ResNet-50 and ResNet-101, respectively. Qualitative results will be provided in the supplemental material.

\paragraph{Compared to Bottom-up Methods:} The performance of our ResNet-50 based end-to-end framework is better ( vs. ) than the strong baseline CMU-Pose \cite{cao2017realtime} that uses multi-scale testing and post-processing with CPM \cite{wei2016convolutional}, and filters the results with an object detector. Our framework also achieves much better performance than the bottom-up method AE \cite{newell2017associative} ( vs. ) and is even better than the method with refining. Compared to PersonLab, with the same backbone ResNet-101 and single-scale testing, our proposed framework also has a competitive performance with it ( vs. ). Note that our proposed framework is much simpler than these bottom-up methods, in both training and testing.

\paragraph{Compared to Top-down Methods:} With the same backbone ResNet-50, the proposed method has a similar performance with previous strong baseline Mask R-CNN ( vs. ). Our model is still behind other top-down methods. However, it is worth noting that these methods often employ a separate bounding-box detector to obtain person instances. These instances are then cropped from the original image and a single person pose estimation method is separately applied to each the cropped image to obtain the final results. As noted before, this strategy is slow as it cannot take advantage of the sharing computation mechanism in CNNs. In contrast, our proposed end-to-end framework is much simpler and faster since it directly maps the raw input images to the final instance-aware keypoint detections with a fully convolutional network.
\paragraph{Timing:} The averaged inference time of our model on COCO \texttt{minival} split is 74ms and 87ms per image with ResNet-50 and ResNet-101, respectively, which is slightly faster than Mask R-CNN with the same hardware and backbones (Mask R-CNN takes 78ms per image with ResNet-50). Additionally, the running time of Mask R-CNN depends on the number of the instances while our model, similar to one-stage object detectors, has nearly constant inference time for any number of instances.



\section{More Discussions and Results}
Here
 1) we further compare our proposed DirectPose against the recent SPM method \cite{Nie_2019_ICCV}. 2) We show the visualization results of the proposed KPAlign module. 3) The loss curves of training with or without the proposed heatmap learning are shown as well. 4) We show some final detection results with or without the simultaneous bounding-box detection.



\subsection{Comparison to SPM}

Here we highlight the difference between our proposed DirectPose and SPM \cite{Nie_2019_ICCV}. SPM makes use of Hierarchical Structured Pose Representations (Hierarchical SPR) to avoid learning the long-range displacements between the root and the keypoints, which shares a similar motivation with the proposed KPAlign. However, SPM considers all the key nodes (including the root nodes and intermediate nodes) in the hierarchical SPR as the regression targets, and instance-agnostic heatmaps are used to predict these nodes. This is similar to  OpenPose \cite{cao2017realtime} with the only exception that SPM predicts these nodes instead of the final keypoints, and thus the predicted nodes are also instance-agnostic. As a result, SPM still needs a grouping post-processing to assemble the detected nodes into full-body poses. In contrast, the proposed KPAlign only requires the coordinates of the final keypoints as the supervision, and aligns the features and the predicted keypoints in an unsupervised fashion. Hence, our proposed framework can directly predict the desired instance-aware keypoints, without the need for any form of grouping post-processing.

\subsection{Visualization of KPAlign}
The visualization results of KPAlign are shown in Fig.~\ref{fig:more_visualization_of_kpalign}. As shown in the figure, the proposed KPAlign can make use of the features near the keypoints to predict them. Thus, the feature vectors can avoid encoding the keypoints far from their spatial location, which results in improved performance.

\subsection{Training Losses of using Heatmap Learning}
\begin{figure}[t!]
\begin{center}
  \includegraphics[width=.75\linewidth]{figures/training_losses.pdf}
\end{center}
  \caption{The loss curves of training with or without the heatmap learning. As shown in the figure, with the heatmap learning, the model can achieve a significantly lower loss value and thus much better performance.}
\label{fig:training_losses}
\end{figure}
In order to demonstrate the impact of the heatmap learning, we plot the loss curves of training with or without the heatmap learning in Fig.~\ref{fig:training_losses}. As shown in the figure, the heatmap learning can greatly help the training of the model and make the model achieve a much lower loss value, thus resulting in much better performance.
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{figures/more_kpalign_visulization_results.pdf}
\end{center}
  \caption{Visualization results of KPAlign on MS-COCO \texttt{minival}. The first image in each group shows the outputs of the locator in KPAlign (\ie, the locations where the sampler samples the features used to predict the keypoints). The orange point denotes the original location where the features will be used if KPAlign is not used. The second image shows the final keypoint detection results. As shown in the figure, the proposed KPAlign can make use of the features near the keypoints to predict them. The final image shows that the ground-truth keypoints. Zoom in for a better look.}
\label{fig:more_visualization_of_kpalign}
\end{figure*}

\subsection{Visualization of Keypoint Detections}
We show more visualization results of DirectPose in Fig.~\ref{fig:visualization_r101}. As shown in the figure, the proposed DirectPose can directly detect all the desired instance-aware keypoints without the need for the grouping post-processing or bounding-box detection. The results of the proposed DirectPose with simultaneous bounding-box detection are also shown in Fig.~\ref{fig:visualization_r50_w_bbox}.
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{figures/visualization_r101.jpg}
\end{center}
  \caption{Visualization results of the proposed DirectPose on MS-COCO \texttt{minival}. DirectPose can directly detect a wide range of poses. Note that some small-scale people do not have ground-truth keypoint annotations in the training set of MS-COCO, thus they might be missing when testing.}
\label{fig:visualization_r101}
\end{figure*}
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{figures/visualization_r50_w_bbox.jpg}
\end{center}
  \caption{Visualization results of the proposed DirectPose with the simultaneous bounding-box detection on MS-COCO \texttt{minival}.}
\label{fig:visualization_r50_w_bbox}
\end{figure*}
 



\section{Conclusion}
We have proposed the first direct end-to-end human pose estimation framework, termed DirectPose. Our proposed model is end-to-end trainable and can directly map a raw input image to the desired instance-aware keypoint detections within constant inference time, eliminating the need for the grouping post-processing in bottom-up methods or the bounding-box detection and RoI operations in top-down ones. We also proposed a keypoint alignment (KPAlign) module to overcome the major difficulty that is the lack of the alignment between the convolutional features and the predictions in the end-to-end model, significantly improving the keypoint detection performance. Additionally, we further improve the regression-based task's performance by jointly learning it with a heatmap-based task. Experiments demonstrate that the new end-to-end method can obtain competitive or better performance than previous bottom-up and top-down methods.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
