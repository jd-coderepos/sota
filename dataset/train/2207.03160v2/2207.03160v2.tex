
\clearpage

\subsection{A.3 Proof of Lemma 2}
\label{app_proof_3}

\ 






\noindent\fbox{
  \parbox{0.99\textwidth}{
    \noindent \textbf{Lemma 2}. 
    Assume  satisfies HOP1-2 order preserving:
    
    Then  where  is the mean curvature in the structure space, and  is the mean curvature optimization results of  in the embedding space.
}
}





































































\begin{figure}[t]
  \centering
  \includegraphics[width=4.2in]{fig/FIg-Page-14.pdf}
  \caption{Proof of Lemma 2}
  \label{fig:appendix_lemma2}
\end{figure}

We randomly select a point  in the data set , and then select 3 neighbor nodes , while ensuring   are the HOP2 of .

As shown in Figure \ref{fig:appendix_lemma2},  Let , ,  be distance in structure space,
and let , , . 
Because  and , we have  and . 


We can find a suitable  to make  and .
and let ,,.
Then according to Lemma 1, in the embedding sapce,  and  and.

First, according to Law of cosines\footnote{https://en.wikipedia.org/wiki/Law\_of\_cosines}, we have:



According to discrete Gauss-Bonnet theorem, the discrete curvature of point  is a function of , , . Let's write the three angle separately.















The same as Eq. (\ref{eq:angleijk1}):



Let , and , we directly represent three angles according to the inverse cosine function.




According to Lemma 1,  will increase  and , and decreases .
so   will increase  amd .

















Then, we sum Eq.(\ref{eq:ijk1_d_v1}) Eq.(\ref{eq:ijk2_d_v1}) Eq.(\ref{eq:k1jk2_d_v1}) together.




where 

Let 





Let 



So



So , So 

So


So , and 
because  so , and .








