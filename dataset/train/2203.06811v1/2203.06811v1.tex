\begin{figure*}[t] 
	\centering
    \includegraphics[width=0.99\linewidth]{figure/main_figure_1}
	\vspace{-2mm}
\caption{Overview of the proposed MTDT-Net.
	(a) MTDT-Net consists of an encoder , a style encoder , a domain style transfer network  and a generator . Given a source image, label map , and target images , MTDT-Net aims to produce domain transferred image .
	The other reconstructed images  are auxiliary outputs generated only during the training process.
	(b)  consists of two TAD residual blocks (ResBlock). The TAD module is followed by each convolutional layer, given the channel-wise statistics of target domains .
	(c) TAD transfers the target domain with  by statistics modulation.
	(d) The multi-head discriminator predicts which domain the image is from, as well as determines whether the image is real or fake.
	Note that, for the sake of brevity, we illustrate a single target domain setting, but our model deals with multi-target domain adaptation.
	}
	\label{fig:overview}
	\vspace{-3mm}
\end{figure*}

\section{A Direct Adaptation Strategy (ADAS)}
\label{sec:method}

In this section, we describe our direct adaptation strategy for multi-target domain adaptive semantic segmentation.
We have a labeled source dataset  and  unlabeled target datasets , where  and  are the image and the ground-truth label, respectively.
The goal of our approach is to directly adapt a segmentation network  to multiple target domains without training STDA models.
Our strategy contains two sub-modules: a multi-target domain transfer network (MTDT-Net), and a bi-directional adaptive region selection (BARS).
We describe the details of MTDT-Net and BARS in \secref{sec:MTDT-Net} and \secref{sec:BARS}, respectively.


\subsection{Multi-Target Domain Transfer Network (MTDT-Net)}
\label{sec:MTDT-Net}
The overall pipeline of MTDT-Net is illustrated in \figref{fig:overview}-(a).
The network consists of an encoder , a generator , a style extractor , a domain style transfer network .
To build an image feature space, we adopt a typical autoencoder structure with the encoder  and the generator .
Given the source and target images , the encoder  extracts the individual features  that are later passed through the generator  to reconstruct the original input images  as follows:


We extract the style tensors ,  of the source image through the style encoder , and the content tensor  from the segmentation label only in the source domain through an  convolutional layer  as follows:


We assume that the image features are composed of the scene structure and detail representation, which we call the content feature  and style feature ,  as follows:

where the source image feature  is passed through generator  to obtain the reconstructed input image .
The synthesized images  are auxiliary outputs to be utilized for network training.
The goal of our network is to generate a domain transferred image  using the same generator  as follows:

where  is the domain transferred features, which is composed of the source content  and the -th target domain style features .

To obtain the target domain style tensors, we design a domain style transfer network () which transfers the source style tensors  to the target style tensors  as follows:

where the channel-wise mean  and variance  vectors encode the -th target domain feature statistics computed by the cumulative moving average (CMA) algorithm and Welford's online algorithm \cite{welford1962note} described in \algref{alg:CMA}.
The  in \figref{fig:overview}-(b) consists of two TAD ResBlock built with a series of convolutional layer, our new Target-Adaptive Denormalization (TAD), and ReLU.
TAD is a conditional normalization module that modulates the normalized input with learned scale and bias similar to SPADE\cite{park2019semantic} and RAD\cite{richter2021enhancing} as shown in \figref{fig:overview}-(c).
We pass the standard deviation  and the target mean  through each fully connected (FC) layer and use them as scale and bias as follows:

where  is the instance-normalized \cite{ulyanov2016instance} input to TAD.
For adversarial learning with multiple target domains, we adopt a multi-head discriminator composed of an adversarial discriminator  and a domain classifier  as shown in \figref{fig:overview}-(d). 


Each group of networks  and  is trained by minimizing the following losses,  and , respectively:


\noindent\textbf{Reconstruction Loss} 
We impose L1 loss on the reconstructed images  to build an image feature space: 


\noindent\textbf{Adversarial Loss}
We apply the patchGAN \cite{isola2017image} discriminator  to impose an adversarial loss on the domain transferred images and the corresponding target images:


\begin{algorithm}
\caption{Domain feature statistics extraction}
\label{alg:CMA}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input: }}
 \renewcommand{\algorithmicensure}{\textbf{Update: }}
 \REQUIRE 
 \ENSURE  
 \\ \% \textit{1. Initialization}
  \FOR {} 
  \STATE ,  \textit{ \hfill//}
  \ENDFOR
 \\ \% \textit{2. Online update
  \hfill // is \# of update iterations}
  \FOR {}
\FOR {}
  \STATE 
  \STATE 
  \STATE 
  \STATE   expand  to  
  \IF {}
  \STATE 
  \ELSE
  \STATE 
\STATE
  
  \STATE
  
  \ENDIF
  \ENDFOR
  \ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Domain Classification Loss}
We build the domain classifier  to classify the domain of the input images.
We impose the cross-entropy loss with the target images for  and with the domain transferred images for :

where  is the one-hot encoded class label of the target domain .

\noindent\textbf{Perceptual Loss}
We impose a perceptual loss \cite{johnson2016perceptual} widely used for domain transfer as well as style transfer \cite{dumoulin2016learned, huang2017arbitrary}:

where the set of layers  is the subset of the perceptual network .





\begin{figure}[t] 
	\centering
	\begin{tabular}{c}
    \includegraphics[width=0.99\linewidth]{figure/BARS} 
    \end{tabular}
	\caption{Overview of BARS.
	For each class ,
	BARS extracts the centroids  from the intermediate features  of the segmentation network  with RoI pooling and update them with CMA algorithm.
	Then, BARS measures the similarity of two cases, ``" and ``", and selects the adaptive region.
	\textcircled{m} is a switch that selects the labels for centroid update in \equref{eq:centroid}, either ,  for the first  iterations or ,  after the  iterations. We set m as 300 iterations for our experiments.
	}
\label{fig:BARS}
	\vspace{-3mm}
\end{figure}









\begin{table*}[t]
\newcommand\w{1.0cm}
    \centering
    \normalsize{
    \begin{tabular}{m{1.6cm}|
    l@{\hspace{1mm}}
    |
    >{\centering\arraybackslash}m{1cm}@{\hspace{1mm}}
    |
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}|
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}|
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}}
    \toprule
    & Method & Target & flat & constr. & object & nature & sky & human & vehicle & mIoU & Avg.
    \\
    \hline
     \multirow{6}{*}{G C, I} & \multirow{2}{*}{ADVENT \cite{vu2019advent}} & C &
     93.9 & 80.2 & 26.2 & 79.0 & 80.5 & 52.5 & 78.0 & 70.0 & \multirow{2}{*}{67.4}
     \\
     & & I &
     \textbf{91.8} & 54.5 & 14.4 & 76.8 & 90.3 & 47.5 & 78.3 & 64.8 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{MTKT\cite{saporta2021multi}} & C &
     94.5 & 82.0 & 23.7 & 80.1 & \textbf{84.0} & 51.0 & 77.6 & 70.4 & \multirow{2}{*}{68.2}
     \\
     & & I &
     91.4 & 56.6 & 13.2 & \textbf{77.3} & \textbf{91.4} & 51.4 & \textbf{79.9} & 65.9 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{Ours} & C &
     \textbf{95.1} & \textbf{82.6} & \textbf{39.8} & \textbf{84.6} & 81.2 & \textbf{63.6} & \textbf{80.7} & \textbf{75.4} & \multirow{2}{*}{\textbf{71.2}}
     \\
     & & I &
     90.5 & \textbf{63.0} & \textbf{22.2} & 73.7 & 87.9 & \textbf{54.3} & 76.9 & \textbf{66.9} &
     \\
     
     \clineB{1-12}{2.5}
     \multirow{6}{*}{G C, M} & \multirow{2}{*}{ADVENT\cite{vu2019advent}} & C &
     93.1 & 80.5 & 24.0 & 77.9 & 81.0 & 52.5 & 75.0 & 69.1 & \multirow{2}{*}{68.9}
     \\
     & & M &
     90.0 & 71.3 & 31.1 & 73.0 & 92.6 & 46.6 & 76.6 & 68.7 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{MTKT\cite{saporta2021multi}} & C &
     95.0 & 81.6 & 23.6 & 80.1 & 83.6 & 53.7 & 79.8 & 71.1 & \multirow{2}{*}{70.9}
     \\
     & & M &
     \textbf{90.6} & 73.3 & 31.0 & 75.3 & \textbf{94.5} & 52.2 & \textbf{79.8} & 70.8 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{Ours} & C &
     \textbf{96.4} & \textbf{83.5} & \textbf{35.1} & \textbf{83.8} & \textbf{84.9} & \textbf{62.3} & \textbf{81.3} & \textbf{75.3} & \multirow{2}{*}{\textbf{73.9}}
     \\
     & & M &
     88.6 & \textbf{73.7} & \textbf{41.0} & \textbf{75.4} & 93.4 & \textbf{58.5} & 77.2 & \textbf{72.6} &
     \\
     
     \clineB{1-12}{2.5}
     \multirow{9}{*}{G C, I, M} & \multirow{3}{*}{ADVENT\cite{vu2019advent}} & C &
     93.6 & 80.6 & 26.4 & 78.1 & 81.5 & 51.9 & 76.4 & 69.8 & \multirow{3}{*}{67.8}
     \\
     & & I &
     \textbf{92.0} & 54.6 & 15.7 & 77.2 & 90.5 & 50.8 & 78.6 & 65.6 &\\
     & & M &
     89.2 & 72.4 & 32.4 & 73.0 & 92.7 & 41.6 & 74.9 & 68.0 &
     \\
     \cline{2-12}
     & \multirow{3}{*}{MTKT\cite{saporta2021multi}} & C &
     94.6 & 80.7 & 23.8 & 79.0 & 84.5 & 51.0 & 79.2 & 70.4 & \multirow{3}{*}{69.1}
     \\
     & & I &
     91.7 & \textbf{55.6} & 14.5 & 78.0 & \textbf{92.6} & 49.8 & \textbf{79.4} & 65.9 &
     \\
     & & M &
     \textbf{90.5} & \textbf{73.7} & 32.5 & 75.5 & \textbf{94.3} & 51.2 & \textbf{80.2} & 71.1 &
     \\
     \cline{2-12}
     & \multirow{3}{*}{Ours} & C &
     \textbf{95.8} & \textbf{82.4} & \textbf{38.3} & \textbf{82.4} & \textbf{85.0} & \textbf{60.5} & \textbf{80.2} & \textbf{74.9} & \multirow{3}{*}{\textbf{71.3}}
     \\
     & & I &
     89.9 & 52.7 & \textbf{25.0} & \textbf{78.1} & 92.1 & \textbf{51.0} & 77.9 & \textbf{66.7} &
     \\
     & & M &
     89.2 & 71.5 & \textbf{45.2} & \textbf{75.8} & 92.3 & \textbf{56.1} & 75.4 & \textbf{72.2} &
     \\
     \bottomrule
     
    \end{tabular}
    }
    \caption{Quantitative comparison between our method and state-of-the-art methods on GTA5 (G) to Cityscapes (C), IDD (I), and Mapilary (M) with 7 classes setting. \textbf{Bold}: Best score among all the methods.}
\label{tab:7classes}
\end{table*}

\subsection{Bi-directional Adaptive Region Selection (BARS)}
\label{sec:BARS}

The key idea of BARS is to select the pixels where the feature statistics are consistent, then train a task network  by imposing loss on the selected region as illustrated in~\figref{fig:BARS}.
We apply it in both the domain transferred image and the target image.
We first extract each centroid feature  of class  as follows:

where  is an indicator function,  is the number of pixels of class , and  are the indices of the spatial coordinates.
The feature map  is from the second last layer of the task network .
To extract the centroids, we use the ground-truth label  of the domain transferred image and the pseudo label  of the target image.
For the online learning with the centroids, we also apply the CMA algorithm in \algref{alg:CMA} to the above centroids.
Then, we design the selection mechanism using the following two assumptions:
\begin{itemize}
\item The region with features  far from the target centroid  would disturb the adaptation process.
\item The region with target features  far from the centroids  is likely to be a noisy prediction region.
\end{itemize}
Based on these assumptions, we find the nearest class  for each pixel in the feature map using the L2 distance between features on each pixels and centroid features as follows:

We obtain the filtered labels ,  using the nearest class :

Finally, we train the task network  with the labels using a typical cross-entropy loss :
\setlength\abovedisplayskip{10pt plus 2pt minus 10pt}


