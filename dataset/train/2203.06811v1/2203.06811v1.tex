\begin{figure*}[t] 
	\centering
    \includegraphics[width=0.99\linewidth]{figure/main_figure_1}
	\vspace{-2mm}
\caption{Overview of the proposed MTDT-Net.
	(a) MTDT-Net consists of an encoder $E$, a style encoder $SE$, a domain style transfer network $DST$ and a generator $G$. Given a source image, label map $I_\mathcal{S}, Y_\mathcal{S}$, and target images $I_{\mathcal{T}_k}$, MTDT-Net aims to produce domain transferred image $I_{\mathcal{S}\to \mathcal{T}_k}$.
	The other reconstructed images $I'_\mathcal{S}, I'_{\mathcal{T}_k}, I''_\mathcal{S}$ are auxiliary outputs generated only during the training process.
	(b) $DST$ consists of two TAD residual blocks (ResBlock). The TAD module is followed by each convolutional layer, given the channel-wise statistics of target domains $\mu_{\mathcal{T}_k}, \sigma_{\mathcal{T}_k}$.
	(c) TAD transfers the target domain with $\mu_{\mathcal{T}_k}, \sigma_{\mathcal{T}_k}$ by statistics modulation.
	(d) The multi-head discriminator predicts which domain the image is from, as well as determines whether the image is real or fake.
	Note that, for the sake of brevity, we illustrate a single target domain setting, but our model deals with multi-target domain adaptation.
	}
	\label{fig:overview}
	\vspace{-3mm}
\end{figure*}

\section{A Direct Adaptation Strategy (ADAS)}
\label{sec:method}

In this section, we describe our direct adaptation strategy for multi-target domain adaptive semantic segmentation.
We have a labeled source dataset $\mathcal{S}=\{I_\mathcal{S}, Y_\mathcal{S} \}$ and $N$ unlabeled target datasets $\mathcal{T}_k=\{I_{\mathcal{T}_k}\}, k\in \{1,...,N\}$, where $I$ and $Y$ are the image and the ground-truth label, respectively.
The goal of our approach is to directly adapt a segmentation network $T$ to multiple target domains without training STDA models.
Our strategy contains two sub-modules: a multi-target domain transfer network (MTDT-Net), and a bi-directional adaptive region selection (BARS).
We describe the details of MTDT-Net and BARS in \secref{sec:MTDT-Net} and \secref{sec:BARS}, respectively.


\subsection{Multi-Target Domain Transfer Network (MTDT-Net)}
\label{sec:MTDT-Net}
The overall pipeline of MTDT-Net is illustrated in \figref{fig:overview}-(a).
The network consists of an encoder $E$, a generator $G$, a style extractor $SE$, a domain style transfer network $DST$.
To build an image feature space, we adopt a typical autoencoder structure with the encoder $E$ and the generator $G$.
Given the source and target images $I_\mathcal{S}, I_{\mathcal{T}_1}, ..., I_{\mathcal{T}_N}$, the encoder $E$ extracts the individual features $\mathcal{F}_\mathcal{S}, \mathcal{F}_{\mathcal{T}_1}, ... \mathcal{F}_{\mathcal{T}_N}$ that are later passed through the generator $G$ to reconstruct the original input images $I'_\mathcal{S}, I'_{\mathcal{T}_1}, ..., I'_{\mathcal{T}_N}$ as follows:
\begin{gather}
\begin{split}
\mathcal{F}_{\mathcal{S}} = E(I_\mathcal{S}), ~~ \mathcal{F}_{\mathcal{T}_k} = E(I_{\mathcal{T}_k}), \\
I'_\mathcal{S} = G(\mathcal{F}_\mathcal{S}), ~~ I'_{\mathcal{T}_k} = G(\mathcal{F}_{\mathcal{T}_k}).
\end{split}
\label{eq:direct_recon}
\end{gather}

We extract the style tensors $\gamma_\mathcal{S}$, $\beta_\mathcal{S}$ of the source image through the style encoder $SE$, and the content tensor $\mathcal{C}_\mathcal{S}$ from the segmentation label only in the source domain through an $1\times1$ convolutional layer $\phi(\cdot)$ as follows:
\begin{gather}
\begin{split}
\{\gamma_\mathcal{S}, \beta_\mathcal{S}\} = SE(I_\mathcal{S}),~\mathcal{C_S}=\phi(Y_\mathcal{S}). \end{split}
\label{eq:indirect_recon1}
\end{gather}

We assume that the image features are composed of the scene structure and detail representation, which we call the content feature $\mathcal{C}_\mathcal{S}$ and style feature $\gamma_\mathcal{S}$, $\beta_\mathcal{S}$ as follows:
\begin{gather}
\begin{split}
I''_\mathcal{S} = G(\mathcal{F}'_\mathcal{S}),~\mathcal{F}'_\mathcal{S} = \gamma_\mathcal{S} \mathcal{C_S}  + \beta_\mathcal{S},
\end{split}
\label{eq:indirect_recon}
\end{gather}
where the source image feature $\mathcal{F}'_\mathcal{S}$ is passed through generator $G$ to obtain the reconstructed input image $I''_\mathcal{S}$.
The synthesized images $I'_\mathcal{S}, I'_{\mathcal{T}_k}, I''_\mathcal{S}$ are auxiliary outputs to be utilized for network training.
The goal of our network is to generate a domain transferred image $I_{\mathcal{S}\to{\mathcal{T}_k}}$ using the same generator $G$ as follows:
\begin{gather}
\begin{split}
I_{\mathcal{S}\to{\mathcal{T}_k}} = G(\mathcal{F}_{\mathcal{S}\to{\mathcal{T}_k}}),
~\mathcal{F}_{\mathcal{S}\to{\mathcal{T}_k}} = \gamma_{\mathcal{S}\to{\mathcal{T}_k}} \mathcal{C_S}  + \beta_{\mathcal{S}\to{\mathcal{T}_k}},
\end{split}
\label{eq:transfer}
\end{gather}
where $\mathcal{F}_{\mathcal{S}\to{\mathcal{T}_k}}$ is the domain transferred features, which is composed of the source content $\mathcal{C_S}$ and the $k$-th target domain style features $\gamma_{\mathcal{S}\to{\mathcal{T}_k}}, \beta_{\mathcal{S}\to{\mathcal{T}_k}}$.

To obtain the target domain style tensors, we design a domain style transfer network ($DST$) which transfers the source style tensors $\gamma_\mathcal{S}, \beta_\mathcal{S}$ to the target style tensors $\gamma_{\mathcal{S}\to \mathcal{T}_k}, \beta_{\mathcal{S}\to \mathcal{T}_k}$ as follows:
\begin{gather}
\begin{split}
\gamma_{\mathcal{S}\to{\mathcal{T}_k}}, \beta_{\mathcal{S}\to{\mathcal{T}_k}} = DST(\gamma_\mathcal{S},\beta_\mathcal{S}, \mu_{\mathcal{T}_k}, \sigma_{\mathcal{T}_k}),
\end{split}
\label{eq:DST}
\end{gather}
where the channel-wise mean $\mu_{\mathcal{T}_k}$ and variance $(\sigma_{\mathcal{T}_k})^2$ vectors encode the $k$-th target domain feature statistics computed by the cumulative moving average (CMA) algorithm and Welford's online algorithm \cite{welford1962note} described in \algref{alg:CMA}.
The $DST$ in \figref{fig:overview}-(b) consists of two TAD ResBlock built with a series of convolutional layer, our new Target-Adaptive Denormalization (TAD), and ReLU.
TAD is a conditional normalization module that modulates the normalized input with learned scale and bias similar to SPADE\cite{park2019semantic} and RAD\cite{richter2021enhancing} as shown in \figref{fig:overview}-(c).
We pass the standard deviation $\sigma_{\mathcal{T}_k}$ and the target mean $\mu_{\mathcal{T}_k}$ through each fully connected (FC) layer and use them as scale and bias as follows:
\begin{gather}
\begin{split}
\text{TAD}(\hat{f}_\mathcal{S}, \mu_{\mathcal{T}_k}, \sigma_{\mathcal{T}_k}) = FC(\sigma_{\mathcal{T}_k})\hat{f}_\mathcal{S} + FC(\mu_{\mathcal{T}_k}),
\end{split}
\label{eq:TAD}
\end{gather}
where $\hat{f}_\mathcal{S}$ is the instance-normalized \cite{ulyanov2016instance} input to TAD.
For adversarial learning with multiple target domains, we adopt a multi-head discriminator composed of an adversarial discriminator $D_{adv}=D'_{adv}\circ D_{shared}$ and a domain classifier $D_{cls}=D'_{cls}\circ D_{shared}$ as shown in \figref{fig:overview}-(d). 


Each group of networks $\mathcal{G}=\{ E, SE, DST, G, \phi \}$ and $\mathcal{D}=\{ D_{adv}, D_{cls} \}$ is trained by minimizing the following losses, $\mathcal{L}^{\mathcal{G}}$ and $\mathcal{L}^{\mathcal{D}}$, respectively:
\begin{gather}
\begin{split}
    \mathcal{L}^{\mathcal{D}} = -\mathcal{L}_{adv} + \mathcal{L}_{cls}^{\mathcal{D}},\\
    \mathcal{L}^{\mathcal{G}} = \mathcal{L}_{rec} + \mathcal{L}_{per} + \mathcal{L}_{adv} + \mathcal{L}_{cls}^{\mathcal{G}}.
\end{split}
\label{eq:full_obj}
\end{gather}

\noindent\textbf{Reconstruction Loss} 
We impose L1 loss on the reconstructed images $I'_\mathcal{S}, I'_{\mathcal{T}_k}, I''_\mathcal{S}$ to build an image feature space: 
\begin{gather}
\begin{split}
\mathcal{L}_{rec} = \mathcal{L}_1(I_\mathcal{S}, I'_\mathcal{S}) + \mathcal{L}_1(I_\mathcal{S}, I''_\mathcal{S}) + \sum_{k=1}^N \mathcal{L}_1(I_{\mathcal{T}_k}, I'_{\mathcal{T}_k}).
\end{split}
\label{eq:rec}
\end{gather}

\noindent\textbf{Adversarial Loss}
We apply the patchGAN \cite{isola2017image} discriminator $D_{adv}$ to impose an adversarial loss on the domain transferred images and the corresponding target images:
\begin{gather}
\begin{split}
    \mathcal{L}_{adv}
    & = \sum_{k=1}^{N} \Big( \mathbb{E}_{I_{\mathcal{T}_k}} \left[ \log D_{adv}(I_{\mathcal{T}_k}) \right] \\
    &+ \mathbb{E}_{I_{\mathcal{S}\to {\mathcal{T}_k}}} \left[1 - \log D_{adv}(I_{\mathcal{S}\to {\mathcal{T}_k}}) \right] \Big). \\
\end{split}
\label{eq:adv}
\end{gather}

\begin{algorithm}
\caption{Domain feature statistics extraction}
\label{alg:CMA}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input: }}
 \renewcommand{\algorithmicensure}{\textbf{Update: }}
 \REQUIRE $\mathcal{F}_{\mathcal{T}_k} \in \mathbb{R}^{H\times W\times C}, k\in \{1,...,N \}$
 \ENSURE  $\mu_{\mathcal{T}_k}, (\sigma_{\mathcal{T}_k})^2 \in \mathbb{R}^{C}$
 \\ \% \textit{1. Initialization}
  \FOR {$k=1 ~to~ N$} 
  \STATE $M_{\mathcal{T}_k} = 0$, $S_{\mathcal{T}_k} = 0 $ \textit{ \hfill//$M_{ \mathcal{T}_k}, S_{\mathcal{T}_k} \in \mathbb{R}^{H\times W\times C}$}
  \ENDFOR
 \\ \% \textit{2. Online update
  \hfill //$N_{update}$ is \# of update iterations}
  \FOR {$n = 0 ~to~ {N_{update}}$}
\FOR {$k=1 ~to~ N$}
  \STATE $\mu_{n}^{\mathcal{T}_k} \xleftarrow{} \frac{1}{HW} \sum_{i=0}^{H-1}\sum_{j=0}^{W-1}{M_{\mathcal{T}_k}{(i,j)}}$
  \STATE $M_{\mathcal{T}_k} \xleftarrow{} M_{\mathcal{T}_k} + ({{\mathcal{F}_{\mathcal{T}_k} - M_{\mathcal{T}_k}})/({n+1})}$
  \STATE $\mu_{n+1}^{\mathcal{T}_k} \xleftarrow{} \frac{1}{HW} \sum_{i=0}^{H-1}\sum_{j=0}^{W-1}{M_{\mathcal{T}_k}(i,j)}$
  \STATE ${\Tilde{\mu}}_{n}^{\mathcal{T}_k}, {\Tilde{\mu}}_{n+1}^{\mathcal{T}_k}$ $\leftarrow{}$ expand $\mu_{n}^{\mathcal{T}_k},\mu_{n+1}^{\mathcal{T}_k}$ to $\mathbb{R}^{H\times W\times C}$ 
  \IF {$n = 0$}
  \STATE $S_{\mathcal{T}_k} \xleftarrow{} (\mathcal{F}_{\mathcal{T}_k}-{\Tilde{\mu}}_{n+1}^{\mathcal{T}_k})^2$
  \ELSE
  \STATE $S_{\mathcal{T}_k} \xleftarrow{} S_{\mathcal{T}_k} + (\mathcal{F}_{\mathcal{T}_k}-{\Tilde{\mu}}_{n}^{\mathcal{T}_k})(\mathcal{F}_{\mathcal{T}_k}-{\Tilde{\mu}}_{n+1}^{\mathcal{T}_k})$
\STATE
  $\mu_{\mathcal{T}_k} \xleftarrow{} \mu^{\mathcal{T}_k}_{n+1}$
  \STATE
  $(\sigma_{\mathcal{T}_k})^2 \xleftarrow{} \frac{1}{nHW} \sum_{i=0}^{H-1}\sum_{j=0}^{W-1}{S_{\mathcal{T}_k}(i,j)}$
  \ENDIF
  \ENDFOR
  \ENDFOR
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Domain Classification Loss}
We build the domain classifier $D_{cls}$ to classify the domain of the input images.
We impose the cross-entropy loss with the target images for $\mathcal{D}$ and with the domain transferred images for $\mathcal{G}$:
\begin{gather}
\begin{split}
    \mathcal{L}_{cls}^{\mathcal{D}}
    & = -\sum_{k=1}^{N} t_k\log D_{cls}(I_{\mathcal{T}_k}), \\
    \mathcal{L}_{cls}^{\mathcal{G}}
    & = -\sum_{k=1}^{N} t_k\log D_{cls}(I_{\mathcal{S} \to {\mathcal{T}_k}}),
\end{split}
\label{eq:cls}
\end{gather}
where $t_k \in \mathbb{R}^N$ is the one-hot encoded class label of the target domain $\mathcal{T}_k$.

\noindent\textbf{Perceptual Loss}
We impose a perceptual loss \cite{johnson2016perceptual} widely used for domain transfer as well as style transfer \cite{dumoulin2016learned, huang2017arbitrary}:
\begin{gather}
\begin{split}
    \mathcal{L}_{per} = \sum_{k=1}^{N} \sum_{l \in L} || P_l(I_{\mathcal{S}}) - P_l(I_{\mathcal{S} \to {\mathcal{T}_k}}) ||_2^2,
\end{split}
\label{eq:per}
\end{gather}
where the set of layers $L$ is the subset of the perceptual network $P$.





\begin{figure}[t] 
	\centering
	\begin{tabular}{c}
    \includegraphics[width=0.99\linewidth]{figure/BARS} 
    \end{tabular}
	\caption{Overview of BARS.
	For each class $c\in \{1,...,N_{cls} \}$,
	BARS extracts the centroids $\dot{\mathcal{C}}^{c}_{\mathcal{S}\to{\mathcal{T}_k}}, \dot{\mathcal{C}}^{c}_{\mathcal{T}_k}$ from the intermediate features $\dot{\mathcal{F}}_{\mathcal{S}\to{\mathcal{T}_k}}, \dot{\mathcal{F}}_{\mathcal{T}_k} $ of the segmentation network $T$ with RoI pooling and update them with CMA algorithm.
	Then, BARS measures the similarity of two cases, ``$\dot{\mathcal{F}}_{\mathcal{S}\to{\mathcal{T}_k}}\leftrightarrow \dot{\mathcal{C}}_{\mathcal{T}_k}^c$" and ``$\dot{\mathcal{F}}_{\mathcal{T}_k}\leftrightarrow \dot{\mathcal{C}}_{\mathcal{S}\to{\mathcal{T}_k}}^c$", and selects the adaptive region.
	\textcircled{m} is a switch that selects the labels for centroid update in \equref{eq:centroid}, either $Y_\mathcal{S}$, $\hat{Y}_{\mathcal{T}_k}$ for the first $m$ iterations or $Y^{BARS}_{\mathcal{S}\to{\mathcal{T}_k}}$, $\hat{Y}^{BARS}_{\mathcal{T}_k}$ after the $m$ iterations. We set m as 300 iterations for our experiments.
	}
\label{fig:BARS}
	\vspace{-3mm}
\end{figure}









\begin{table*}[t]
\newcommand\w{1.0cm}
    \centering
    \normalsize{
    \begin{tabular}{m{1.6cm}|
    l@{\hspace{1mm}}
    |
    >{\centering\arraybackslash}m{1cm}@{\hspace{1mm}}
    |
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}|
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}|
    >{\centering\arraybackslash}m{\w}@{\hspace{1mm}}}
    \toprule
    & Method & Target & flat & constr. & object & nature & sky & human & vehicle & mIoU & Avg.
    \\
    \hline
     \multirow{6}{*}{G $\to$C, I} & \multirow{2}{*}{ADVENT \cite{vu2019advent}} & C &
     93.9 & 80.2 & 26.2 & 79.0 & 80.5 & 52.5 & 78.0 & 70.0 & \multirow{2}{*}{67.4}
     \\
     & & I &
     \textbf{91.8} & 54.5 & 14.4 & 76.8 & 90.3 & 47.5 & 78.3 & 64.8 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{MTKT\cite{saporta2021multi}} & C &
     94.5 & 82.0 & 23.7 & 80.1 & \textbf{84.0} & 51.0 & 77.6 & 70.4 & \multirow{2}{*}{68.2}
     \\
     & & I &
     91.4 & 56.6 & 13.2 & \textbf{77.3} & \textbf{91.4} & 51.4 & \textbf{79.9} & 65.9 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{Ours} & C &
     \textbf{95.1} & \textbf{82.6} & \textbf{39.8} & \textbf{84.6} & 81.2 & \textbf{63.6} & \textbf{80.7} & \textbf{75.4} & \multirow{2}{*}{\textbf{71.2}}
     \\
     & & I &
     90.5 & \textbf{63.0} & \textbf{22.2} & 73.7 & 87.9 & \textbf{54.3} & 76.9 & \textbf{66.9} &
     \\
     
     \clineB{1-12}{2.5}
     \multirow{6}{*}{G $\to$C, M} & \multirow{2}{*}{ADVENT\cite{vu2019advent}} & C &
     93.1 & 80.5 & 24.0 & 77.9 & 81.0 & 52.5 & 75.0 & 69.1 & \multirow{2}{*}{68.9}
     \\
     & & M &
     90.0 & 71.3 & 31.1 & 73.0 & 92.6 & 46.6 & 76.6 & 68.7 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{MTKT\cite{saporta2021multi}} & C &
     95.0 & 81.6 & 23.6 & 80.1 & 83.6 & 53.7 & 79.8 & 71.1 & \multirow{2}{*}{70.9}
     \\
     & & M &
     \textbf{90.6} & 73.3 & 31.0 & 75.3 & \textbf{94.5} & 52.2 & \textbf{79.8} & 70.8 &
     \\
     \cline{2-12}
     & \multirow{2}{*}{Ours} & C &
     \textbf{96.4} & \textbf{83.5} & \textbf{35.1} & \textbf{83.8} & \textbf{84.9} & \textbf{62.3} & \textbf{81.3} & \textbf{75.3} & \multirow{2}{*}{\textbf{73.9}}
     \\
     & & M &
     88.6 & \textbf{73.7} & \textbf{41.0} & \textbf{75.4} & 93.4 & \textbf{58.5} & 77.2 & \textbf{72.6} &
     \\
     
     \clineB{1-12}{2.5}
     \multirow{9}{*}{G $\to$C, I, M} & \multirow{3}{*}{ADVENT\cite{vu2019advent}} & C &
     93.6 & 80.6 & 26.4 & 78.1 & 81.5 & 51.9 & 76.4 & 69.8 & \multirow{3}{*}{67.8}
     \\
     & & I &
     \textbf{92.0} & 54.6 & 15.7 & 77.2 & 90.5 & 50.8 & 78.6 & 65.6 &\\
     & & M &
     89.2 & 72.4 & 32.4 & 73.0 & 92.7 & 41.6 & 74.9 & 68.0 &
     \\
     \cline{2-12}
     & \multirow{3}{*}{MTKT\cite{saporta2021multi}} & C &
     94.6 & 80.7 & 23.8 & 79.0 & 84.5 & 51.0 & 79.2 & 70.4 & \multirow{3}{*}{69.1}
     \\
     & & I &
     91.7 & \textbf{55.6} & 14.5 & 78.0 & \textbf{92.6} & 49.8 & \textbf{79.4} & 65.9 &
     \\
     & & M &
     \textbf{90.5} & \textbf{73.7} & 32.5 & 75.5 & \textbf{94.3} & 51.2 & \textbf{80.2} & 71.1 &
     \\
     \cline{2-12}
     & \multirow{3}{*}{Ours} & C &
     \textbf{95.8} & \textbf{82.4} & \textbf{38.3} & \textbf{82.4} & \textbf{85.0} & \textbf{60.5} & \textbf{80.2} & \textbf{74.9} & \multirow{3}{*}{\textbf{71.3}}
     \\
     & & I &
     89.9 & 52.7 & \textbf{25.0} & \textbf{78.1} & 92.1 & \textbf{51.0} & 77.9 & \textbf{66.7} &
     \\
     & & M &
     89.2 & 71.5 & \textbf{45.2} & \textbf{75.8} & 92.3 & \textbf{56.1} & 75.4 & \textbf{72.2} &
     \\
     \bottomrule
     
    \end{tabular}
    }
    \caption{Quantitative comparison between our method and state-of-the-art methods on GTA5 (G) to Cityscapes (C), IDD (I), and Mapilary (M) with 7 classes setting. \textbf{Bold}: Best score among all the methods.}
\label{tab:7classes}
\end{table*}

\subsection{Bi-directional Adaptive Region Selection (BARS)}
\label{sec:BARS}

The key idea of BARS is to select the pixels where the feature statistics are consistent, then train a task network $T$ by imposing loss on the selected region as illustrated in~\figref{fig:BARS}.
We apply it in both the domain transferred image and the target image.
We first extract each centroid feature $\dot{\mathcal{C}}$ of class $c$ as follows:
\begin{gather}
\begin{split}
\dot{\mathcal{C}}_{\mathcal{S}\to \mathcal{T}_k}^{c}
= \frac{1}{N_c}\sum_{i} \sum_{j} 
{\mathbbm{1}(Y_\mathcal{S}(i, j)=c)} \dot{\mathcal{F}}_{\mathcal{S}\to \mathcal{T}_k}(i, j),\\
\dot{\mathcal{C}}_{\mathcal{T}_k}^{c} = \frac{1}{N_c}\sum_{i} \sum_{j} 
{\mathbbm{1}(\hat{Y}_{\mathcal{T}_k}(i, j)=c)} \dot{\mathcal{F}}_{\mathcal{T}_k}(i, j),
\end{split}
\label{eq:centroid}
\end{gather}
where $\mathbbm{1}$ is an indicator function, $N_c$ is the number of pixels of class $c$, and $i, j$ are the indices of the spatial coordinates.
The feature map $\dot{\mathcal{F}}$ is from the second last layer of the task network $T$.
To extract the centroids, we use the ground-truth label $Y_\mathcal{S}$ of the domain transferred image and the pseudo label $\hat{Y}_{\mathcal{T}_k}$ of the target image.
For the online learning with the centroids, we also apply the CMA algorithm in \algref{alg:CMA} to the above centroids.
Then, we design the selection mechanism using the following two assumptions:
\begin{itemize}
\item The region with features $\dot{\mathcal{F}}_{\mathcal{S}\to \mathcal{T}_k}$ far from the target centroid $\dot{\mathcal{C}}_{\mathcal{T}_k}$ would disturb the adaptation process.
\item The region with target features $\dot{\mathcal{F}}_{\mathcal{T}_k}$ far from the centroids $\dot{\mathcal{C}}_{S\to \mathcal{T}_k}$ is likely to be a noisy prediction region.
\end{itemize}
Based on these assumptions, we find the nearest class $\dot{c}$ for each pixel in the feature map using the L2 distance between features on each pixels and centroid features as follows:
\begin{gather}
\begin{split}
\dot{c}_{\mathcal{S}\to \mathcal{T}_k}(i, j) = \argmin_c || \dot{\mathcal{F}}_{\mathcal{S}\to \mathcal{T}_k}(i, j) - \dot{\mathcal{C}}^{c}_{\mathcal{T}_k} ||_2, \\
\dot{c}_{\mathcal{T}_k}(i, j) = \argmin_c || \dot{\mathcal{F}}_{\mathcal{T}_k}(i, j) - \dot{\mathcal{C}}_{\mathcal{S}\to \mathcal{T}_k}^c ||_2.
\end{split}
\end{gather}
We obtain the filtered labels $Y_{\mathcal{S}\to{\mathcal{T}_k}}^{BARS}$, $\hat{Y}_{\mathcal{T}_k}^{BARS}$ using the nearest class $\dot{c}$:
\begin{gather}
\begin{split}
Y_{\mathcal{S}\to{\mathcal{T}_k}}^{BARS}(i, j) = 
\begin{cases}
Y_\mathcal{S}(i, j) & \quad \text{if}~\dot{c}_{\mathcal{S}\to \mathcal{T}_k}(i,j)=Y_\mathcal{S}(i, j),\\
\emptyset & \quad \text{otherwise}\\
\end{cases},
\\
\hat{Y}_{\mathcal{T}_k}^{BARS}(i, j) = 
\begin{cases}
\hat{Y}_{\mathcal{T}_k}(i, j) & \quad \text{if}~\dot{c}_{\mathcal{T}_k}(i,j)=\hat{Y}_{\mathcal{T}_k}(i, j),\\
\emptyset & \quad \text{otherwise}\\
\end{cases}.
\end{split}
\end{gather}
Finally, we train the task network $T$ with the labels using a typical cross-entropy loss $\mathcal{L}_{Task}$:
\setlength\abovedisplayskip{10pt plus 2pt minus 10pt}
\begin{gather}
\begin{split}
\min_{T} \left( \mathcal{L}_{Task} (I_{\mathcal{S} \to \mathcal{T}_k}, 
Y_{\mathcal{S}\to{\mathcal{T}_k}}^{BARS}) + \mathcal{L}_{Task} (I_{\mathcal{T}_k}, \hat{Y}_{\mathcal{T}_k}^{BARS}) \right).
\end{split}
\end{gather}

