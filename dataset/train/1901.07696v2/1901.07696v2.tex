\section{Experimental setup}

\subsection{Research questions}\label{sec:research-question}

We list four research questions that guide the remainder of the paper: 
\textbf{RQ1}: What is the overall performance of PAAG? Does it outperform state-of-the-art baselines?
\textbf{RQ2}: What is the effect of each module in PAAG? Does the discriminator give a useful training signal to the answer generation module?
\textbf{RQ3}: Is PAAG capable to extract useful information from noisy reviews?
\textbf{RQ4}: What is the performance of PAAG at different data domain? 

\subsection{Dataset}
We collect a large-scale dataset from a real-world e-commerce website, including question-answering pairs, reviews, and product attributes.
This dataset is available at \url{https://github.com/gsh199449/productqa}.
On this website, users can post a question about the product.
Most questions are asking for an experience of user who has already bought the product.
In the collected data, each QA pair is associated with the reviews and attributes of the corresponding product.
We remove all QA pairs without any relevant review and split the whole dataset into training and testing set.
In total, our dataset contains cover 469,953 products and 38 product categories.
The average length of question is 9.03 words and ground truth answer is 10.3 words.
The average number of attribute is 9.0 key-value pairs.
There are 78.74\% of training samples have more than 10 relevant reviews and 75.33\% of training samples have more than 5 attributes.

\subsection{Evaluation metrics}

To evaluate our proposed method, we employ BLEU~\cite{Papineni2002BleuAM} to measure the quality of generated sentence by computing overlapping lexical units (\eg unigram, bigram) with the reference sentence.
We also consider three embedding-based metrics~\cite{forgues2014bootstrapping} (including Embedding Average, Embedding Greedy and Embedding Extreme) to evaluate our model, following several recent studies on text generation~\cite{Serban2017AHL, Xu2017NeuralRG, Tao2018Get}. 
These three metrics compute the semantic similarity between the generated and reference answer according to the word embedding.

Since automatic evaluation metrics may not always consistent with human perception~\cite{Stent2005EvaluatingEM}, we use human evaluation in our experiment. 
Three annotators are invited to judge the quality of 100 randomly sampled answer generated by different models. 
These annotators are all well-educated Ph.D. students and they are all native speakers. 
Two of them have the background of NLP/summarization and another annotator does not major in computer science.
We show human annotators a question, several reviews and attributes of the product along with answers generated from each model.

Statistical significance of observed differences between the performance of two runs are tested using a two-tailed paired t-test and is denoted using \dubbelop (or \dubbelneer) for strong significance for $\alpha = 0.01$. 

\subsection{Comparisons} \label{sec:baselines}

In order to prove the effectiveness of each module in PAAG, we conduct some ablation models shown in Table~\ref{tab:ablations}.

To evaluate the performance of our dataset and the proposed framework, we compare our model with the following baselines:
(1) \textbf{S2SA}: Sequence-to-sequence framework~\cite{Sutskever2014SequenceTS} has been proposed for language generation task. 
We use seq2seq framework which is equipped with attention mechanism~\cite{Bahdanau2015Neural} and copy mechanism~\cite{Gu2016IncorporatingCM} as baseline method. 
The input sequence is question and ground truth output sequence is the answer.
(2) \textbf{S2SAR}: We implement a simple method which can incorporate the review information when generating the answer.
Different from the S2SA, we use an RNN to read all the reviews and concatenate the final state of this RNN with encoder final state as the initial state of decoder RNN.
(3) \textbf{SNet}: S-Net~\cite{Tan2018snet} is a two-stage state-of-the-art model which extracts some text spans from multiple documents context and synthesis the answer from those spans.
Due to the difference between our dataset and MS-MARCO~\cite{Nguyen2016MSMA}, our dataset does not have text span label ground truth for training the evidence extraction module.
So we use the predicted extraction probability to do weighted sum the original review word embeddings, and use this representation as extracted evidence to feed into the answer generation module.
(4) \textbf{QS}: We implement the query-based summarization model proposed by Hasselqvist \etal~\cite{Hasselqvist2017QueryBasedAS}. 
Accordingly, we use product reviews as original passage and answer as a summary.
(5) \textbf{BM25}: BM25 is a bag-of-words retrieval function that ranks a set of reviews based on the question terms appearing in each review.
We use the top review of ranking list as the answer.
(6) \textbf{TF-IDF}: Term Frequency-Inverse Document Frequency is a numerical statistic that is intended to reflect how important a question word is to a review.
We use this statistic to model the relevance between review and question and select the most similar review as the answer of question.

\begin{table}[t]
\centering
\caption{Ablation models for comparison.}
\label{tab:ablations}
\begin{tabular}{@{}l@{~}l}
\toprule
Acronym & Gloss \\
\midrule

RAGF &  \multicolumn{1}{p{6.5cm}}{\small \raggedright \textbf{R}eview reader + \textbf{A}ttributes encoder + \textbf{G}ated \textbf{F}usion}\\

RAGFD &  \multicolumn{1}{p{6.5cm}}{\small RAGF + consistency \textbf{D}iscriminator}\\

RAGFWD &  \multicolumn{1}{p{6.5cm}}{\small RAGF + \textbf{W}asserstein consistency \textbf{D}iscriminator}\\

PAAG &  \multicolumn{1}{p{6.5cm}}{\small RAGFWD + Gradient Penalty }\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Implementation details}

Without using pre-trained embeddings, we randomly initialize the network parameters at the beginning of our experiments.
All the RNN networks have 512 hidden units and the dimension of word embedding is 256.
To produce better answers, we use beam search with beam size 4.
Adagrad~\cite{Duchi2010AdaptiveSM} with learning rate 0.1 is used to optimize the parameters and batch size is 64.
We implement our model using TensorFlow~\cite{abadi2016tensorflow} framework and train our model and all baseline models on NVIDIA Tesla P40 GPU.

\section{Experimental Result}

\subsection{Overall performance}

For research question \textbf{RQ1}, to demonstrate the effectiveness of PAAG, we examine the overall performance in term of BLEU, embedding metrics and human evaluation. 
Table~\ref{tab:comp_bleu_baselines} and Table~\ref{tab:comp_emb_baselines} list performances of all comparisons in terms of two automatic evaluation metrics.
Significant differences are with respect to SNet (row with shaded background).
In these experimental results, we see that PAAG achieves a 111\%, 8\% and 62.73\% increment over the state-of-the-art baseline SNet in terms of BLEU, embedding greedy and consistency score, respectively.
In Table~\ref{tab:comp_emb_baselines}, we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth.

For human evaluation, we ask annotators to rate each generated answer according to two aspects: consistency and fluency.
The rating score ranges from 1 to 3, and 3 is the best.
We finally take the average across answers and annotators, as shown in Table~\ref{tab:comp_human_baslines}.
In Table~\ref{tab:comp_human_baslines}, we can see that PAAG outperforms other baseline models in both sentence fluency and consistency with the facts.
We calculate the variance score in Table~\ref{tab:comp_human_baslines}, which shows that annotators agree with each other's judgments in most cases.
Although the BLEU score of S2SAR is lower than the S2SA, the embedding score and human score for S2SAR are higher than S2SA.
Regardless of few word overlapping between generated and ground answer, the human evaluation and results in terms of embedding metrics verify S2SAR outperforms S2SA.
This observation demonstrates the effectiveness of incorporating review in answer generation.

To explore the difficulty of this task, we use a very intuitive method by adding the review information into decoder shown in S2SAR.
Although there is a small increment of S2SAR with respect to S2SA in all metrics, we still find a noticeable gap between S2SAR and PAAG.
This observation demonstrates that PAAG makes better use of review and attribute information than the simple method S2SAR.
In view of the facts extracted from the review and attributes, we examine directly using the most similar review to question as the answer.
More specifically, we evaluate the performance of the top of review ranking list which is ranked by text similarity algorithm such as BM25 and TF-IDF.
From the result of three metrics, the performance of extractive methods is worth than all the generative methods.
It is worth noting that since the answer generated by extractive methods is written by human, it have very high fluency scores.
But these answers may not match the question, so the consistency score is very low.
Consequently, using the most similar review to question as answer is not a better method than generating answers from scratch.

As our task definition and query based text summarization have some similarities in some way, we can see the reviews as original passage and answer as a query based summary.
We also use the query-based text summarization algorithm~\cite{Hasselqvist2017QueryBasedAS} to generate answer.
Similarly, we also employ a reading comprehension method SNet to tackle this task.
Since query-based text summarization and reading comprehension models are not defined to tackle QA task in e-commerce scenario, it can not fully utilize the interactions between question, review, and attributes.
These methods also lack of ability of denoising the reviews.

\newcommand{\cbkgrnd}{\cellcolor{blue!15}}
\newcommand{\phantomtriangle}{\phantom{\dubbelop}}
\begin{table}[t]
\centering
\caption{BLEU scores comparison between baselines.}
\footnotesize
\begin{tabular}{@{}lcc cc c@{}}
\toprule
& BLEU & BLEU1  & BLEU2 & BLEU3 & BLEU4 \\
\midrule
\multicolumn{6}{@{}l}{\emph{Text generation methods}}\\

S2SA & 1.6186\phantom{0} & 15.4754\phantom{0}& 3.1437\phantom{0} & 0.8267\phantom{0}   & 0.1706\phantom{0} \\

S2SAR & 1.7549 & 15.1708 & 3.2156 & 0.9078 & 0.2142 \\
\cbkgrnd SNet & \cbkgrnd 0.9550 & \cbkgrnd 13.7029 & \cbkgrnd 2.5374 & \cbkgrnd 0.4007 & \cbkgrnd 0.0597  \\
QS & 1.6848 & 15.4961 & 2.9508 & 0.8315 & 0.2119 \\

PAAG & \textbf{2.0189}\dubbelop & \textbf{16.2232}\dubbelop  & \textbf{3.5711}\dubbelop & \textbf{1.0290}\dubbelop & \textbf{0.2787}\dubbelop \\
\midrule
\multicolumn{6}{@{}l}{\emph{Sentence extraction methods}}\\
BM25 & 0.4125 & 6.9630 & 0.7097 & 0.1333 & 0.0439 \\
TF-IDF & 0.2548 & 5.5480 & 0.5127 & 0.0779 & 0.0190 \\
\bottomrule
\end{tabular}
 \vspace{-3mm}
\label{tab:comp_bleu_baselines}
\end{table}

\begin{table}[t]
\centering
\footnotesize
\caption{Embedding scores comparison between baselines.}
\begin{tabular}{@{}lcc c@{}}
\toprule
& Average & Greedy  & Extrema \\
\midrule
\multicolumn{4}{@{}l}{\emph{Text generation methods}}\\

S2SA & 0.410013\phantom{0} & 98.653415\phantom{0} & 0.269461\phantom{0} \\

S2SAR & 0.419979\phantom{0} & 99.742679\phantom{0}& 0.278666\phantom{0} \\
\cbkgrnd SNet & \cbkgrnd 0.397162 & \cbkgrnd 95.791356 & \cbkgrnd 0.277781 \\
QS & 0.400291 & 93.255031 & 0.252164 \\

PAAG & \textbf{0.424218}\dubbelop & \textbf{103.912364}\dubbelop & \textbf{0.288321}\dubbelop \\
\midrule
\multicolumn{4}{@{}l}{\emph{Sentence extraction methods}}\\
BM25 & 0.325946 & 76.814465 & 0.172976 \\
TF-IDF & 0.308293 & 85.020442 & 0.155390 \\
\bottomrule
\end{tabular}
 \vspace{-3mm}
\label{tab:comp_emb_baselines}
\end{table}

\begin{table}[t]
\centering
\caption{Consistency and fluency comparison by human evaluation.}
\footnotesize
\begin{tabular}{@{}lcc cc@{}}
\toprule
& \multicolumn{2}{c}{Fluency} & \multicolumn{2}{c}{Consistency} \\ \cline{2-5} 
& mean & variance  & mean & variance \\
\midrule
\multicolumn{5}{@{}l}{\emph{Text generation methods}}\\
S2SA & 2.22 & 0.3 & 1.62 & 0.29 \\
S2SAR & 2.405 & 0.365 & 1.82 & 0.39 \\
\cbkgrnd SNet & \cbkgrnd 1.93 & \cbkgrnd 0.36 & \cbkgrnd 1.355 & \cbkgrnd 0.225 \\
QS & 2.335 & 0.285 & 1.725 & 0.355 \\
PAAG & 2.865\dubbelop & 0.105 & 2.205\dubbelop & 0.445 \\
\midrule
\multicolumn{5}{@{}l}{\emph{Sentence extraction methods}}\\
BM25 & 2.70 & 0.24 & 1.45 & 0.29 \\
TF-IDF & 2.48 & 0.38 & 1.14 & 0.12 \\
\bottomrule
\end{tabular}
 \vspace{-3mm}
\label{tab:comp_human_baslines}
\end{table}

\begin{table}[t]
\centering
\caption{BLEU scores of different ablation models.}
\footnotesize
\begin{tabular}{@{}lcc cc c@{}}
\toprule
& BLEU & BLEU1  & BLEU2 & BLEU3 & BLEU4 \\
\midrule

RAGF & 1.7931 & 15.7213 & 3.3705 & 0.9385 & 0.2079 \\
RAGFD & 1.8597 & 15.9021 & 3.4160 & 0.9409 & 0.2340 \\
RAGFWD & 1.9389 & 16.1755 & \textbf{3.5986} & 0.9865 & 0.2461 \\
PAAG & \textbf{2.0189} & \textbf{16.2232}  & 3.5711 & \textbf{1.0290} & \textbf{0.2787} \\
\bottomrule
\end{tabular}
 \vspace{-3mm}
\label{tab:comp_bleu_ablation}
\end{table}

\begin{figure}[!t]
  \centering
  \subfigure{
    \label{fig:qa-attn-1}
    \includegraphics[width=8.5cm, height=3.3cm]{figs/case1.pdf}}
  \subfigure{
    \label{fig:qa-attn-2}
    \includegraphics[width=8.5cm, height=2.0cm]{figs/case2.pdf}}
    \vspace{-8mm}
    \caption{Visualizations of question-aware review attention map.}
    \vspace{-2mm}
  \label{fig:attention}
\end{figure}

\subsection{Ablation studies}

Next we turn to research question \textbf{RQ2}.
We conduct ablation tests on the usage of adversarial learning method.
The BLEU score of each ablation model is shown in Table~\ref{tab:comp_bleu_ablation}.
In the method RAGFD, we use the vanilla GAN architecture which minimize the divergence.
There is a slight increment from RAGF to RAGFD, which demonstrates the effectiveness of discriminator.
From Table~\ref{tab:comp_bleu_ablation}, we find that RAGFWD achieves a 4.3\% improvement over RAGFD in terms of BLEU, and PAAG outperforms RAGFWD 4.1\% in terms of BLEU.
Accordingly, we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty.
This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture.

\subsection{Denoising ability}

To address \textbf{RQ3}, in this section we provide an analysis of the denoising ability of our model. According to Table~\ref{tab:comp_bleu_baselines} and Table~\ref{tab:comp_bleu_ablation}, we observe RAGF achieves 2.1\% improvement over SASAR, in terms of BLEU.
Such observation demonstrates that question-aware review generation module gives the denoising ability to the model.
To further investigate the effectiveness of extracting facts from reviews, we visualize two question-review attention maps, shown in Figure~\ref{fig:attention}.
Question of the left figure in Figure~\ref{fig:attention} is ``Will the color fade when cleaning?'' and the right is ``Is it convenient to clean''. 
The review of the left figure is ``Good shopping experience. The pants were washed without discoloration and no color difference compared to the picture. It looks good, comfortable and cheap.'' and the right is ``The color looks good and the texture is great. I haven't started it yet, but it's very easy to clean''.
In this figure, we can see that there is a very strong interaction between question word \begin{CJK*}{UTF8}{gbsn}清洗\end{CJK*} (cleaning) and phrase in review \begin{CJK*}{UTF8}{gbsn}清洗会很方便\end{CJK*} (very easy to clean).
Concretely, these figures show that the question-review attention module can capture the salience semantic part in review according to the question.


In the most cases, the higher word overlap between question and review, the more useful the review is.
To prove the ability of review gated fusion module shown in Equation~\ref{rev-gate-softmax}, we use the BM25 algorithm to calculate the similarity between question and each review.
Then we calculate the cosine distance between the salience score produced by review gated fusion module calculated and BM25 similarity score, shown in Figure~\ref{fig:review_gates_simi}.
In order to demonstrate the denoising ability of adversarial learning method, we compare our full model PAAG with the baseline model RAGF, this experiment proves that the usage of WGAN can encourage our model to capture the salience review better.

\begin{figure}[htbp]
\begin{minipage}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figs/review_gates_simi.pdf}
    \vspace{-7mm}
    \caption{Similarity between review gates and BM25 score.}
    \label{fig:review_gates_simi}
\end{minipage}\vspace{-2mm}
    \hfill \begin{minipage}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figs/emb_parameter.pdf}
    \vspace{-7mm}
    \caption{Greedy embedding metric with training epoch.}
    \label{fig:parameter_test}
\end{minipage} 
\vspace{-2mm}
\end{figure}
\vspace{-2mm}

\subsection{Discussions}

\begin{CJK*}{UTF8}{gbsn}
\begin{table}[t]
\centering
\caption{Examples of the generated natural answers by PAAG and other models.}
\scriptsize
\vspace{-5mm}
\begin{tabular}{l|l}
\toprule
\multicolumn{1}{c|}{\multirow{6}{*}{reviews}} & \multicolumn{1}{p{6.5cm}}{衣服 质量 很 好 ， 就 是 我 比较 瘦 ， 这个 S 号 对于 我 来说 还是 比较 肥 。 很 适合 孕妇 穿 。 (The quality of the clothes is very good. Because I am thin, the S size is still quite fat for me. It is suitable for pregnant women to wear.)}     \\ \cline{2-2} 
\multicolumn{1}{c|}{}                         & \multicolumn{1}{p{6.5cm}}{颜色 漂亮 ， 宽松 舒服 ， 就 是 线头 有点 多 ， 竟然 还有 口袋 ， 方便 实用 ！ (The clothes are beautiful in color, comfortable to wear, and the thread is a bit more. This dress has a pocket and it is convenient and practical!)}      \\ \cline{2-2} 
\multicolumn{1}{c|}{}                         & \multicolumn{1}{p{6.5cm}}{不 轻 的 雪纺 连衣裙 ， 很 有 垂坠感 ， 觉得 夏天 春天 都 适合 穿 ， 很 衬肤 色白 呢 ， 胸前 会 有 透 ， 不过 不 影响 穿 着 (This chiffon dress feels heavy when worn. I think summer and spring are suitable for wearing. Putting on this dress will bring out my skin white. But this dress will be transparent on the chest, but it will not affect the wear.)}      \\ \hline
attributes                                    & \multicolumn{1}{p{6.5cm}}{裙长: 中裙||裙型: A 字裙||材质: 涤纶||袖长: 七分 袖||领型: 其它||袖型: 其它||上市时间: 2018 春季||版型: A 型}     \\ \hline
question                                     & \multicolumn{1}{p{6.5cm}}{怀孕 五 个 多 月 能 穿 吗 (I have been pregnant for more than five months, can I wear it?)}   \\ 
\hline
reference                                     & \multicolumn{1}{p{6.5cm}}{能 穿 到 生 都 没 问题 的 (You can wear it until your child is born)}   \\ \hline
S2SA                                          & \multicolumn{1}{p{6.5cm}}{可以 的 ， 我 儿子 三 个 月 就 穿 了 (I can wear it, my son wore it when he was three months old.)}    \\ \hline
RAGF                                          & \multicolumn{1}{p{6.5cm}}{可以 啊 ， 我 的 就 是 五 个 月 (I can wear it, my pregnancy is five months.)}  \\ \hline
PAAG                                          & \multicolumn{1}{p{6.5cm}}{可以 的 ， 我 就是 (I can wear it, I am pregnant.)} \\ 
\bottomrule
\end{tabular}
\vspace{-3mm}
\label{tab:case}
\end{table}
\end{CJK*}

Finally, we address \textbf{RQ4}. Table~\ref{tab:case} shows an example and its corresponding generated answers by different methods.
We observe that S2SA only generates the answer which is fluent, but generated answers are contradictory to the facts.
Due to there is no fact consistency constraint in RAGF, it will also face this problem when generating answers
However, PAAG overcomes this shortcoming by using consistency constraint given by discriminator at training, and then produce the answer which is not only fluent but also consistent with the facts.

\begin{table}[t]
\centering
\caption{Comparison of BLEU scores between different product categories.}
\vspace{-4mm}
\label{tab:comp_domains}
\scriptsize
\begin{tabular}{lllllll}
\toprule
                                           & \multicolumn{2}{c}{PAAG}                                                            & \multicolumn{2}{c}{S2SA}                                                            & \multicolumn{2}{c}{S2SAR}                                                         \\ \cline{2-7} 
                                            & \multicolumn{1}{c}{BLEU1} & \multicolumn{1}{c}{BLEU2}& \multicolumn{1}{c}{BLEU1} & \multicolumn{1}{c}{BLEU2} & \multicolumn{1}{c}{BLEU1} & \multicolumn{1}{c}{BLEU2} \\ \hline
\multicolumn{1}{l|}{Jewelry}                & \textbf{19.53}                   & \multicolumn{1}{l|}{\textbf{6.35}}                     & 17.65                   & \multicolumn{1}{l|}{4.26}                      & 18.74                   & 4.69                 \\
\multicolumn{1}{l|}{Mattress}           & \textbf{18.89}                   & \multicolumn{1}{l|}{4.14}   & 16.35                   & \multicolumn{1}{l|}{3.00}    & 17.52                   & \textbf{5.57}       \\
\multicolumn{1}{l|}{Clothing}               & 18.18                   & \multicolumn{1}{l|}{\textbf{5.17}} & \textbf{18.39}                   & \multicolumn{1}{l|}{4.98}  & 18.36                    & 4.68   \\
\multicolumn{1}{l|}{Kitchenware}            & \textbf{18.00}                   & \multicolumn{1}{l|}{\textbf{4.31}} & 15.23                    & \multicolumn{1}{l|}{3.19} & 17.15                   & 4.09   \\
\multicolumn{1}{l|}{Power and Handtools}     & \textbf{16.34 }                  & \multicolumn{1}{l|}{\textbf{3.98}} & 13.73                   & \multicolumn{1}{l|}{3.20} & 15.60                   & 3.22 \\
\multicolumn{1}{l|}{Skin Care}          & 18.01                   & \multicolumn{1}{l|}{\textbf{4.57}} & 15.39                   & \multicolumn{1}{l|}{3.55}   & \textbf{18.33}                   & 4.40   \\
\multicolumn{1}{l|}{Gardening} & 13.67                   & \multicolumn{1}{l|}{\textbf{2.30}}  & 11.86                   & \multicolumn{1}{l|}{1.52}  & \textbf{15.74}                   & \textbf{2.30}   \\
\multicolumn{1}{l|}{Baby}                   & \textbf{18.22}                   & \multicolumn{1}{l|}{\textbf{4.51}}  & 16.95                   & \multicolumn{1}{l|}{3.71}  & 17.27                   & 3.75 \\
\multicolumn{1}{l|}{Automotive Accessories}       & 17.46                   & \multicolumn{1}{l|}{\textbf{3.43}} & 15.49                    & \multicolumn{1}{l|}{3.14}  & \textbf{17.86}                   & 3.00  \\
\multicolumn{1}{l|}{Gift}                   & \textbf{19.25 }                  & \multicolumn{1}{l|}{3.93}  & 17.23                     & \multicolumn{1}{l|}{3.06}  & 18.39                   & \textbf{4.24} \\
\bottomrule
\end{tabular}
\end{table}

We evaluate performances of PAAG on different categories.
Shown in Table~\ref{tab:comp_domains}, we see that our proposed model beats the other two baselines (S2SA and S2SAR), on majority of product categories in terms of BLEU score.
To prove the significance of the above results, we also do the paired student t-test between our model and baseline methods, the p-value of S2SA is 0.0086 and S2SAR is 0.0100.
From the t-test, we can see that the performance of our model is significantly higher than other baselines.

To investigate the robustness of parameter, we train our model in different parameter size and evaluate them by embedding metric shown in Figure~\ref{fig:parameter_test}.
As the training progresses, the performance of each model is rising.
However, the model with a large number of parameters does not have a great advantage in the final performance of the model with a smaller parameters.

