

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newif\ifaccepted

\acceptedtrue


\ifaccepted
\usepackage[accepted]{icml2023}
\else
\usepackage{icml2023}
\fi




\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{IEEEtrantools}
\usepackage{multirow}
\usepackage{clrscode3e}
\usepackage{changepage}
\usepackage{caption}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\argmax}{argmax}

\makeatletter
\newsavebox\myboxA
\newsavebox\myboxB
\newlength\mylenA

\newcommand*\xoverline[2][0.75]{\sbox{\myboxA}{}\setbox\myboxB\null \ht\myboxB=\ht\myboxA \dp\myboxB=\dp\myboxA \wd\myboxB=#1\wd\myboxA \sbox\myboxB{}\setlength\mylenA{\the\wd\myboxA}\addtolength\mylenA{-\the\wd\myboxB}\ifdim\wd\myboxB<\wd\myboxA \rlap{\hskip 0.5\mylenA\usebox\myboxB}{\usebox\myboxA}\else
        \hskip -0.5\mylenA\rlap{\usebox\myboxA}{\hskip 0.5\mylenA\usebox\myboxB}\fi}
\makeatother

\newcommand{\bigconc}{\mathop{\mathpalette\bigconcinn\relax}}
\newcommand{\bigconcinn}[2]{\vcenter{\hbox{}}}
\newcommand{\bigconcchoose}[1]{\def\bigconcsize{}\ifx#1\displaystyle
    \let\bigconcsize\Big
  \else
    \ifx#1\textstyle
      \let\bigconcsize\big
    \fi
  \fi#1}

\newcommand*{\anddot}{\mathclose{}\nonscript\mskip.5\thinmuskip
\boldsymbol{.}\;\mathopen{}}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Neural Priority Queues for GNNs}

\begin{document}

\twocolumn[
\icmltitle{Neural Priority Queues for Graph Neural Networks (GNNs)}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rishabh Jain}{cambridge}
\icmlauthor{Petar Veličković}{cambridge,deepmind}
\icmlauthor{Pietro Li\`{o}}{cambridge}
\end{icmlauthorlist}

\icmlaffiliation{cambridge}{University of Cambridge, Cambridge, UK}
\icmlaffiliation{deepmind}{Google DeepMind, London, UK}

\icmlcorrespondingauthor{Rishabh Jain}{rj412@cam.ac.uk}

\icmlkeywords{Machine Learning, ICML, GNN, Memory, CLRS, Algorithms, Algorithmic Reasoning, Neural PQ, NPQ, Priority Queues}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning.
Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited
exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues,
a differentiable analogue to algorithmic priority queues, for GNNs.
We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata,
and reason about their use with algorithmic reasoning. This is further demonstrated by empirical
results on the CLRS\nobreakdash-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions,
as empirically shown on a dataset from the Long-Range Graph Benchmark.
\end{abstract}

\section{Introduction}
Algorithms and Deep Learning methods possess very fundamentally different properties. Training deep learning
models to mimic algorithms would allow us to get neural models that show generalisation ability similar to the algorithms,
while retaining the robustness to noise of deep learning systems. This building and training of neural networks to execute
algorithmic computations is referred to as Neural Algorithmic Reasoning \citep{Velickovic-NAR}.

Architectures that align more with the underlying algorithm for the reasoning task, tend to generalize better \citep{Xu-Neural-Networks-Reasoning}.
Previous works have drawn inspiration from the external memory and data structure use of programmes and algorithms, and have found success in improving
the algorithmic reasoning capabilities of recurrent neural networks (RNNs) by extending them with differentiable variants for these memory
and data structures \citep{Graves-NTM,Grefenstette-neural-queue}.

Recently, graph neural networks (GNNs) have found immense success with algorithmic tasks \citep{Chen-GNNs-Substructure-count,Velickovic-CLRS}.
There have been works attempting to augment GNNs with memory, with majority using gates to do so. However, gated memory leads to very limited
persistence. Furthermore, these works have solely focused on dynamic graphs,
and extending these to non-dynamic graphs would involve significant effort. 

In this paper, we propose the extension of the message passing framework of GNNs with external memory modules.
We focus on adding a differentiable analogue to priority queues, as priority queues are a general data structure used by different algorithms and
can be reduced to other data structures like stacks and queues. We name the thus formed framework for differentiable priority queues as `Neural PQs'.
We describe NPQ, an implementation under this framework, and also explore various variants for this.
NPQ shows various properties that were lacking in previous works with GNNs, which we believe enable NPQ to help GNNs with algorithmic reasoning.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{./imgs/introduction/gnn-using-memory-module.png}
    \caption[Proposed extension of message-passing framework with external memory modules]{\textbf{Left:} A GNN processor based on the message passing framework. At each timestep,
             pair-wise messages are formed using the node features.
             These messages are aggregated, and then used to update the node features for the next timestep. \textbf{Right:} The memory module framework we propose.
             The memory module takes the node features and previous memory state, to output the next memory state and messages for the GNN processor.
             These messages are aggregated alongside the traditional node-to-node messages. In this project, we focus on memory modules inspired from priority queues.}
    \label{fig:gnn-using-memory-modules}
\end{figure}

We summarize the contributions of this paper below:
\begin{itemize}
    \item
        We propose the `Neural PQ' framework, an extension of the message-passing GNN framework to allow use of memory modules, with particular inspiration from priority queues.
    \item
        We present and motivate a set of desiderata for memory modules -- (1) Memory-Persistence, (2) Permutation-Equivariance, (3) Reducibility to Priority Queues,
        and (4) No dependence on intermediate supervision. Past works have already expressed some subsets of these as desirables.
    \item
        We propose NPQs, an implementation within the Neural PQ framework, that exhibit all the above mentioned properties.
        This is the first differentiable analogue to priority queues, and the first memory modules for GNNs to exhibit all the above desiderata, to the best of our knowledge.
    \item
        We perform extensive quantitative analysis, via a variety of experiments and find:
        \begin{itemize}
            \item NPQs, when training to reason Dijkstra's shortest path algorithm, close the gap between the baseline test performance and
                  ground truth by over .
            \item
                The various Neural PQs outperform the baseline on 26 out of 30 algorithms from the CLRS-30 dataset \citep{Velickovic-CLRS}.
                The performance gains are not restricted to algorithms that actually use a priority queue.
\item
                Neural PQs also help with long-range reasoning. These help local message-passing networks to capture long-range interaction.
                Thus, the benefits of using the Neural PQs are not limited to algorithmic reasoning, and these can be used on a variety of other tasks.
        \end{itemize}
\end{itemize}

\section{Background}
\label{sec:clrs-background}
\paragraph{CLRS Benchmark \citep{Velickovic-CLRS}} Various prior works have shown the efficiency of GNNs for algorithmic tasks.
However, many of these works tend to be disconnected in terms of the algorithms they target, data processing and evaluation, making direct comparisons difficult.
To take the first steps in solving this issue, \citet{Velickovic-CLRS} propose the CLRS Algorithmic Reasoning Benchmark which consists of
30 algorithms from the `Introduction to Algorithms' textbook by \citet{Cormen-CLRS}. They name this dataset as CLRS-30.

The authors employ the encode-process-decode paradigm \citep{Hamrick-encode-process-decode} and compare different processor networks (which are different GNNs) choices.
Below we provide some more details on this encode-process-decode setup. Since we focus on the CLRS Benchmark for evaluation,
this forms as the baseline architectural structure.

Let us take a graph , with Let  as the one-hop neighbourhood of node .
Let  be the node features for node ,  the edge features for edge
 and  the graph features. The \emph{encode} step involves encoding these inputs using linear
layers ,  and
:
\begin{IEEEeqnarray}{rClrClrCl}
    \textbf{h}_i & = & f_n(\textbf{x}_i) \qquad & \textbf{h}_{ij} & = & f_e(\textbf{e}_{ij}) \qquad & \textbf{h}_g & = & f_g(\textbf{g})
\end{IEEEeqnarray}

These are then used in a processor network during the \emph{process} step.
The previous latent features  are used along with the current node feature 
encoding to get a recurrent encoded input  using a recurrent encoding function . This recurrent cell update is line with the work of
\citet{Velickovic-Neural-Execution-Graphs}. A message from node  to node ,  is computed for each pair of nodes using a message
function . These messages are aggregated using a permutation-invariant aggregation function . Finally, a readout function  transforms the aggregated messages
and node encodings into processed node latent features.
\begin{IEEEeqnarray}{rCl}
    \textbf{z}_i^{(t)} & = & f_A(\textbf{h}_i, \textbf{h}_i^{(t - 1)}) \label{eq:z} \\
    \textbf{m}_{ij} & = & f_m(\textbf{z}_i^{(t)}, \textbf{z}_j^{(t)}, \textbf{h}_{ij}, \textbf{h}_g) \label{eq:m-ij} \\
    \textbf{m}_i & = & \bigoplus_{j \in \mathcal{N}_i} \textbf{m}_{ji} \label{eq:m-i} \\
    \textbf{h}_i^{(t)} & = & f_r(\textbf{z}_i^{(t)}, \textbf{m}_i) \label{eq:nxt-latent}
\end{IEEEeqnarray}

For different processors, ,  and  may differ.

The last \emph{decode} step consists of using relevant decoding functions to get the required prediction. This might be the predicted hints or predicted output.



\section{Related Work}
The ability of RNNs to work in a sequential manner led to their popularity in previous works for reasoning about algorithms, as algorithms tend to be iterative in nature.
Noting that most computer programmes make use of external memory, \citet{Graves-NTM} proposed addition of an external memory module to RNNs, which makes reasoning about
algorithms easier. Subsequent methods have worked upon this idea and have found success with memory modules inspired from different data structures.
This includes Stack-Augmented RNNs by \citep{Joulin-Stack-RNN} and Neural DeQues by \citet{Grefenstette-neural-queue}. A key limitation of all these proposals is that they
are only defined for use by RNNs. Unlike GNNs, RNNs are unable to use the structured information about the algorithms' input spaces.

Early explorations on augmenting GNNs with memory focused on the use of internal memory in the form of gates, such as Gated Graph Sequence Networks
by \citet{Li-GGSN}, and Temporal Graph Networks by \citet{Rossi-TGN}. However, the use of such RNN-like gate mechanisms limits the persistence of the graph/node histories.
Persistent Message Passing (PMP) by \citet{Strathmann-PMP} is a noteworthy GNN that makes use of non-gated persistent external memory, by persisting some of the nodes at each
timestep. However, PMPs cannot be applied to non-dynamic graphs without significant effort. Furthermore, they require intermediate-supervision.

\section{Neural PQ Framework}
Previous works have proposed Neural Stacks, Queues and DeQues \citep{Grefenstette-neural-queue,Joulin-Stack-RNN} that
have a RNN controller. In this project, we propose the use of memory modules, with focus on differentiable PQs (or Neural PQs),
with a GNN model acting as the controller. Furthermore, we propose integration of such memory modules with message-passing
framework by allowing the Neural PQ to send messages to each node. The setup for this is shown in Figure \ref{fig:gnn-using-neural-pq}.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.8\linewidth]{./imgs/implementation/gnn-using-neural-pq.png}
    \caption[Proposed Neural PQ framework]{Neural PQ controlled using a GNN processor. At each timestep, the node features are used to pop values from the priority queue. These values are used
             to form the messages that are sent to the different nodes. The node features and previous state are also used to determine what values to push,
             and update the priority queue. This uses Neural PQ as the memory module in Figure \ref{fig:gnn-using-memory-modules}.}
    \label{fig:gnn-using-neural-pq}
\end{figure}

\subsection{Desiderata}
We form the framework with the following desiderata in mind -- (1) Memory-Persistence, (2) Permutation-Equivariance, (3) Reducibility to Priority Queues,
and (4) No dependence on intermediate supervision. We motivate the need for these below.

\paragraph{Memory-Persistence} Memory-Persistence is necessary to make full use of the extended capacity provided by the external memory modules.
This is especially true for models running over several timestep, with a long temporal interaction, where we would want to access memory added at a much earlier timestep.
Furthermore, memory persistence helps avoid over-smoothing. Node embeddings of GNNs tend to start off varied. As more messages are passed, the embeddings
tend to converge to each-other, thus making the nodes indistinguishable, as the number of layers increases.
Memory-persistence would allow GNNs to remember older states, when the embeddings were more distinguished, and use these to promote more varied embeddings,
despite the depth of the model.

\paragraph{Permutation-Equivariance} A GNN layer is said to be equivariant to permutation of the nodes if and only if any permutation of the node IDs, while maintaining
the overall graph structure, leads to the same permutation of the node features. This reflects one of the most basic symmetries of graph structures, and is necessary
to ensure that isomorphic graphs receive the same representation, up to certain permutations and transformation. This makes permutation-equivariance an essential
property for GNN layers. Thus, we want our Neural PQs to also be equivariant to permutations.

\paragraph{Priority Queue Alignment} Priority queues are a general data structure used by various algorithms. Furthermore, different data structures can be modelled using
priority queues, like stacks and queues are priority queues with the time of insertion as the priority. A memory module that aligns well with priority queues would lead
to the overall model, that uses the memory module, to align with the related algorithms better. Algorithmically aligned models lead to greater generalisation
\citep{Xu-Neural-Networks-Reasoning}. Thus, this is essential for greater algorithmic reasoning.

\paragraph{No intermediate supervision requirement} By not requiring any intermediate supervision, memory modules become easier to apply directly to different
algorithmic tasks. This allows us to even use the Neural PQ for reasoning over algorithms that may not use a priority queue themselves. General Neural PQs can be helpful
with such tasks due to additional properties, like memory-persistence.

\subsection{Framework}
We present the framework for a Neural PQ controlled by a message-passing GNN below. We use the equations for the baseline message-passing GNN as described in
Section \ref{sec:clrs-background}. Let us suppose we have the same setup as the baseline. The encode and decode part remain the same, but now the processor uses
an implementation of the Neural PQ Framework. Let the graph in consideration be .
Let the previous hidden state of the GNN be , and the previous state of the Neural PQ be .

In the Neural PQ framework, we calculate the set of values  to be popped for each node , using a pop function .
Messages are formed from these popped values using a message encoding function .
Each node aggregates these messages along with the traditional node-to-node pairwise messages. Lastly, a push function
 updates the state of the Neural PQ, to obtain the next state . Formally, we can define the following equations for the framework:
\begin{IEEEeqnarray}{rCl}
    \textbf{z}_i^{(t)} & = & f_A(\textbf{h}_i, \textbf{h}_i^{(t - 1)}) \label{eq:z-pq-gen} \\
    \textbf{m}_{ij} & = & f_m(\textbf{z}_i^{(t)}, \textbf{z}_j^{(t)}, \textbf{h}_{ij}, \textbf{h}_g) \label{eq:m-ij-pq-gen} \\
    V_i & = & f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}) \label{eq:V-pq-gen} \\
    M_i & = & f_{M}^{(pq)}(V_i, \textbf{z}_i^{(t)}) \, \cup \, \left\{ \textbf{m}_{ji} \, | \, j \in \mathcal{N}_i \right\} \label{eq:M-pq-gen} \\
    \textbf{m}_i & = & \bigoplus_{\textbf{m} \in M_i} \textbf{m} \label{eq:m-i-pq-gen} \\
    \textbf{h}_i^{(t)} & = & f_r(\textbf{z}_i^{(t)}, \textbf{m}_i) \label{eq:nxt-latent-pq-gen} \\
    \textbf{h}_{pq}^{(t)} & = & f_{push}(\textbf{h}_{pq}^{(t - 1)}, \textbf{z}^{(t)}) \label{eq:nxt-pq-state-gen}
\end{IEEEeqnarray}
where  is a multi-set of all the encoded inputs , i.e. .
 and  depend on which message-passing GNN processor we choose,
while ,  and  depend on the Neural PQ implementation.

Note that, in the above proposed framework, we choose to delay the update of the priority queue due to pop operation until the .
This is done to keep the Neural PQ framework general and to segregate the queue read and update
operations. This also allows us to prove the permutation-equivariance properties of the framework, as discussed below.

Even though the presented framework is inspired from priority queues, we can implement various other data structures, like queues and stacks, by appropriate
 and  definitions. The Neural PQ framework exhibits and promotes various properties from the desiderata.
By design, these do not require any additional supervision. Furthermore, since the push, pop and message encoding
functions only depend on the destination node's features, the multi-set of all node features and the Neural PQ state,
all implementations are also equivariant to permutations of the nodes, under certain assumptions. For a detailed proof, refer to
Appendix \ref{sec:appendix-permutation-equivariance}.

\section{NPQ}
\label{sec:pqv2}
We propose NPQ, an implementation following the fore-mentioned Neural PQ framework that exhibits all the proposed desiderata. We divide the overall definition of NPQ into
4 sub-sections -- (1) State, (2) Pop Function, (3) Message Encoding Function, and (4) Push Function. Taking inspiration from Neural DeQues \citep{Grefenstette-neural-queue},
NPQs consist of continuous push and pop operations.

\subsection{State}

\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.5\linewidth]{./imgs/implementation/pqv2-state.png}
    \caption[NPQ State]{\textbf{State of the NPQ.} It consists of two lists of the same length, representing the values  in the queue and their respective
             strengths . In the above example, the NPQ consists of three elements, ,  and ,
             with strengths ,  and  respectively.}
    \label{fig:pqv2=state}
\end{figure}

The state of the NPQ must hold all the memory values pushed into the queue. Alongside these values, since we define continuous push and pop operations, we also need to keep
track of the strengths/proportions of each queue element still present. We can represent this state  as a tuple of the list of memory values
 and the list of strengths of these memory values
.
The th element of  is , the value of the th element of the priority queue, and the th element
of  is , the strength of the th element of the priority queue.
\begin{IEEEeqnarray}{rCl}
    \textbf{h}_{pq}^{(t)} & = & \langle \textbf{v}^{(t)}, \textbf{s}^{(t)} \rangle \label{eq:state-pqv2}
\end{IEEEeqnarray}
where  is a tuple.
Figure \ref{fig:pqv2=state} shows a sample state for the NPQ.


\subsection{Pop function}
\label{sec:pqv2-pop}

\begin{figure}[tbhp]
    \centering
    \includegraphics[width=\linewidth]{./imgs/implementation/pqv2-pop.png}
    \caption[NPQ Pop Function]{Sample \textbf{pop operation} for a single node  in NPQ. \textbf{Left:} Pop-request  generation. First we compute the attention
             coefficients  using the node features  and the memory values . We also calculate the pop-strength
             . The coefficients and the pop strength are together used to determine the pop fractions to request. NPQ uses Max Popping, which
             only pops the element with the highest attention coefficient, in this case . Thus, we request popping of only this element with
             strength . \textbf{Right:} Using pop requests from all the nodes and the strengths of each value currently in the queue, NPQ
             grants certain fraction  to node  to pop element . We use this granted proportion to determine the value popped.}
    \label{fig:pqv2=pop}
\end{figure}

We propose a continuos pop function, i.e. we pop a fractional proportions of the values in the queue.
This fraction,  for node , is computed as noted in Equation \ref{eq:pqv2-pop-strength}.
This equation is similar to the ones used for Neural DeQues by \citet{Grefenstette-neural-queue}.
\begin{IEEEeqnarray}{rCl}
    s_{pop}^{(i)} & = & \text{ sigmoid} \left( f_s^{(pop)}(\textbf{z}_i^{(t)}) \right) \label{eq:pqv2-pop-strength}
\end{IEEEeqnarray}

We use a request-grant framework to maintain the constraint that no value can be popped more than it is present,
i.e. one cannot pop  of a value  that may be present in the PQ with only a strength of .
Each node  requests to pop fraction  of PQ element . NPQ takes all the
 values into consideration and grants a fraction  of PQ element  to node , which may or may not be the same as the requested .
Equation \ref{eq:pqv2-q} shows the calculation of this granted fraction given the requested fractions  and the PQ element strengths .
\begin{IEEEeqnarray}{rCl}
    q_j^{(i)} & = & \left\{
        \begin{array}{cl}
            p_j^{(i)} & \text{, if } \sum_{k \in \mathcal{V}} p_j^{(k)} \leq \textbf{s}_{j}^{(t - 1)} \\
            \frac{p_j^{(i)}}{\sum_{k \in \mathcal{V}} p_j^{(k)}} \cdot \textbf{s}_{j}^{(t - 1)} & \text{, else}
        \end{array}
    \right. \label{eq:pqv2-q}
\end{IEEEeqnarray}
The main idea behind this equation is that ideally we would want to satisfy each request  for popping. We cannot do that when the sum of pop requests is greater
than the strength with which the element is present in the PQ. In this case, we completely pop the element and return to each node fraction of this value in proportion to
the strength each node requested. It maintains the requirement that  and .
These granted proportions are used to calculate the final value popped.
\begin{IEEEeqnarray}{C}
    \mathbf{v} = \sum_{j \in \mathcal{I}^{(t - 1)}_{pq}} q_j^{(i)} \cdot \textbf{v}_j^{(t - 1)} \\
    f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \langle \textbf{v}^{(t - 1)}, \textbf{s}^{(t - 1)} \rangle) = \left\{
\mathbf{v}
    \right\} \label{eq:pqv2-pop}
\end{IEEEeqnarray}
where  is the set of indices for the NPQ.
Note that since we are only popping a single value from the NPQ, we are returning a single element set.
The requested pop proportions  are calculated using the continuous pop strength value , and attention coefficients ,
denoting the coefficient for the th element of the queue with respect to the th node.
These are calculated using a multi-head additive attention mechanism \citep{Bahdanau-Attention,Vaswani-Attention}.
This is done with inspiration from GATs by \citet{Velickovic-GAT}.
\begin{IEEEeqnarray}{rCl}
    e_{j}^{(i,h)} & = & \text{ LeakyReLU}\left(f_{a_1}^{(h)}(\textbf{z}_i^{(t)}) + f_{a_2}^{(h)}(\textbf{v}^{(t - 1)}_j)\right) \\
    \alpha_{j}^{(i,h)} & = & \text{ softmax}_j\left( e_{j}^{(i,h)} \right) \label{eq:alpha-h-pqv2} \\
    c_{j}^{(i)} & = & \text{ softmax}_j\left( f_{a}([\alpha_{j}^{(i,1)}, \ldots, \alpha_{j}^{(i,h)}, \ldots]) \right) \label{eq:att-coef-pqv2}
\end{IEEEeqnarray}
where  is the attention coefficients for th element of the queue with respect to node  via attention-head ,
and ,  and  are linear layers.

Using these coefficients, we propose two ways of popping elements from the queue -- Max Popping and Weighted Popping. We refer to NPQ using max popping
and weighted popping as NPQ and NPQ, respectively.

\subsubsection*{Max Popping}
The element  of the queue with the highest attention coefficient  is requested to be popped for the node .
\begin{IEEEeqnarray}{rCl}
    k & = & \argmax_{k \in \mathcal{I}^{(t - 1)}_{pq}} c_{k}^{(i)} \\
    p_j^{(i)} & = & s_{pop}^{(i)} \cdot \mathbb{I}_{\{ k \}}\left( j \right)
\end{IEEEeqnarray}
where  is the indicator function for set , i.e.  and .

\subsubsection*{Weighted Popping}
The attention coefficients are treated as soft-weights with which each element in the PQ is requested to be popped.
\begin{IEEEeqnarray}{rCl}
    p_j^{(i)} & = & s_{pop}^{(i)} \cdot c_{j}^{(i)}
\end{IEEEeqnarray}

Figure \ref{fig:pqv2=pop} shows sample pop operation for NPQ.

\subsection{Priority Queue Message Function}
We use a simple message encoding function, where each output is passed through a linear layer .
\begin{IEEEeqnarray}{rCl}
    f_M^{(pq)}(V_i, \textbf{z}_i^{(t)}) & = & \left\{ f_m^{(pq)}(\textbf{v}) \, | \, \textbf{v} \in V_i \right\} \label{eq:message-pqv2}
\end{IEEEeqnarray}

\subsection{Push function}
\label{sec:pqv2-push}

\begin{figure}[tbhp]
    \centering
    \includegraphics[width=\linewidth]{./imgs/implementation/pqv2-push.png}
    \caption[NPQ Push Function]{Sample \textbf{push operation}. The overall push operation can be divided into two main steps. \textbf{Step 1:} In the first step, we
             update the queue strengths to reflect the removal of popped fractions of each element. This is done by summing the granted pop fractions
              from all nodes. The granted pop-fractions  can be calculated from the previous NPQ state and the node embeddings
             . These aggregated pop-grants are then subtracted from the strengths of the respective queue elements. Some elements might
             end up with  strength, and these are then removed from the queue, as shown here with the greyed-out value  here.
             \textbf{Step 2:} The next step is to actually push a value into the queue. The value to be pushed and its strength are determined
             by the node embeddings .}
    \label{fig:pqv2=push}
\end{figure}

As mentioned earlier, the push function is actually the state update function. Here we first delete the popped proportions from the NPQ.
Let  be the previous NPQ state. Then,
we can define the NPQ state with the popped proportions deleted as , which
are calculated as below.
\begin{IEEEeqnarray}{rCl}
    \textbf{s}_i^{\prime} & = & \textbf{s}_{i}^{(t - 1)} - \sum_{k \in \mathcal{V}} q_i^{(k)} \\
    \textbf{s}^{\prime} & = & \text{ nonzero}_i\left( \textbf{s}_i^{\prime} \right) \\
    \textbf{v}^{\prime} & = & \textbf{v}^{(t)}[\text{ arg-nonzero}_i\left( \textbf{s}_i^{\prime} \right)]
\end{IEEEeqnarray}
where  is the proportion NPQ element  granted to be popped for node  as defined in Equation \ref{eq:pqv2-q},
nonzero is sequence of  with all zero  removed,
and similarly, arg-nonzero is the relevant indices of the sequence.

We push a single value  for the whole graph. To determine this value, we pass each node embedding through a linear layer  and sum the formed values
across all the nodes. In line with Neural DeQues by \citet{Grefenstette-neural-queue}, this values is activated using a tanh function to get the final value to be pushed.

The push function is continuous and so requires calculation of the push strength .
This is done in a similar manner to the push values calculation, using a linear layer . We use a logistic sigmoid activation here instead of tanh,
akin to Neural DeQues.
\begin{IEEEeqnarray}{C}
    \textbf{v} = \text{ tanh}\left( \sum_{i \in \mathcal{V}} f_v(\textbf{z}_i^{(t)}) \right) \\
    \mathbf{s} = \text{ sigmoid}\left( \sum_{i \in \mathcal{V}} f_s^{(push)}(\textbf{z}_i^{(t)}) \right)\\
    f_{push}(\langle \textbf{v}^{(t - 1)}, \textbf{s}^{(t - 1)} \rangle, \textbf{z}^{(t)}) =
        \langle \textbf{v}^{\prime} \, || \,  [\textbf{v}], \textbf{s}^{\prime} \, || \,  [\mathbf{s}_{push}] \rangle \, \, \, \label{eq:push-pqv2}
\end{IEEEeqnarray}
Note that in the above equations, we use  and , which are not actually inputs to the  function. This is done mainly to maintain
readability of the functions. These equations can be easily reformulated to only use  and , in order to follow the
general Neural PQ framework. Refer to Appendix \ref{sec:appendix-pqv2-reformulation} for the reformulation.

\subsection{Properties}
Simply by virtue of following the Neural PQ framework, NPQ exhibits two of the desiderata -- Permutation-Equivariance, and no dependence on intermediate supervision.
We do not update or replace the previously stored NPQ elements, but rather persist them as long as possible, and only delete their proportions when we pop them.
This allows NPQ to achieve much greater memory-persistence than done using gated memories.

Lastly, the push and pop operations of the NPQ are defined to be aligned close to the push and pop operations of the traditional
priority queue. In fact, under some assumptions, we can prove that NPQ can be reduced to a traditional priority queue. This can be done by taking the
push and pop functions to be encoding the key-value pairs for the priority queue elements. For a detailed proof, refer to Appendix \ref{sec:pq-alignment}.

Thus, NPQ satisfies the four stated desiderata.

\subsection{Variants}
We also explore some variations on the proposed NPQ. One such variation involves consideration of greater memory-persistence by not deleting the popped elements.
We refer to this variation as NPQ-P.

Notably, NPQ treats popping as a node-wise activity. We can instead treat popping as a graph operation, i.e. each node receives the same set of popped values.
This can be done by either sending all the node-wise popped values to all the nodes, or by popping a single value for all the nodes. We refer to these two variants
as NPQ-SA and NPQ-SV, respectively.

Empirically, we found these latter two variants more useful when combined with the first one. We refer to these combined variations as NPQ-P-SA and NPQ-P-SV, respectively.
For the exact equations for the variants, refer to Appendix \ref{sec:pqv0}-\ref{sec:pqv2-sv}.

\section{Evaluation}
\label{sec:evaluation}
The main hypothesis we test is whether the Neural PQ implementations are useful for algorithmic reasoning by
using the CLRS-30 dataset \citep{Velickovic-CLRS}. To do so, we undertake multiple experiments --
(1) We first focus on a single algorithm, Dijkstra's Shortest Path algorithm, evaluating the performance
of the Neural PQs with a MLP MPNN as the base GNN, comparing them with the MPNN baseline as well as an
MLP MPNN with an oracle priority queue. (2) We also evaluate the performance
of the Neural PQs on rest of the algorithms from the CLRS benchmark. (3) Lastly,
we also test whether the Neural PQs are useful for long-range reasoning, by evaluating their performance on a dataset
from the Long Range Graph Benchmark \citep{Dwivedi-Long-Range-Benchmark}.
Appendix \ref{sec:further-evaluations} shows some more experiments performed.

\subsection{Dijkstra's Algorithm -- MPNN Base}
We train the models on Dijkstra's algorithm from CLRS-30, and test for out-of-distribution (OOD) generalisation, i.e.
the models are trained on smaller input graphs, containing 16 nodes, and tested on larger graphs,
containing 256 nodes. The training data consists of 1000 samples, while the testing and validation data consist of 32 samples each.
We test the models on larger graph sizes than done by
\citet{Velickovic-CLRS} (they use graphs with 64 nodes) to better
test the generalisation ability, and because baseline MPNN model already gets around  test
performance with 64 nodes.

To test the limit of attainable performance from Neural PQs, we test an MPNN with access
to an Oracle PQ, where apart from the standard input features,
we also take information about the values pushed and popped from the priority queue as input.
The Oracle Neural PQ forces the push and pop operation to be determined by the algorithmic PQ.
This information about the actual PQ is used in training, validation as well as testing.

Table \ref{table:dijkstra-oracle-evals} shows the test performance of the different models.
We see that the last model performs much better than the early-stopped model for the baseline and Oracle PQ.
Notably, the last and early-stopped model perform similarly for NPQ. NPQ
outperforms the baseline as well as the Oracle PQ.
In fact, we see that it \textbf{closes the gap} between the test performance of baseline MPNN and true solution,
i.e. 100\% test performance, by over .

\begin{table}[tbh]
\centering
\caption{Test performance (Mean  Standard Deviation) of models with MLP MPNN base on
learning Dijkstra's Algorithm with 256 node graphs, run with 3 different seeds.
The table shows the results for the \emph{Best} validation score model (early-stopped model)
and the \emph{Last} model in training.}
\begin{tabular}{lll}
    \hline
    \centering \bfseries Method & \bfseries Best & \bfseries Last \\
    \hline
    Baseline &  &  \\
    NPQ &  &  \\
    NPQ &  &  \\
    NPQ-SA &  &  \\
    NPQ-SV &  &  \\
    NPQ-P-SA &  &  \\
    NPQ-P-SV &  &  \\
    \hline
    \hline
    Oracle PQ  &  &  \\
    \hline
\end{tabular}
\label{table:dijkstra-oracle-evals}
\end{table}







\subsection{Different Algorithms from CLRS-30}
\label{sec:eval-all-algs}
We train and test five models for each algorithm from CLRS-30 dataset --
`Baseline' (no memory module), NPQ\nobreakdash-P\nobreakdash-SA, NPQ\nobreakdash-P\nobreakdash-SV, NPQ and NPQ.
We train each model on graphs with 16 nodes, and test them on graphs with 128 nodes, and consider only the early-stopped models.

Figure \ref{fig:eval-best-pq-all-algs} shows the comparison for best performing Neural PQ and the baseline MPNN for each algorithm.
We see that for \textbf{26 out of the 30 algorithms, at least one of the Neural PQs outperforms the baseline MPNN}. Interestingly, the optimal Neural PQ version depends
on the algorithm of choice. Notably, the performance gain of using a Neural PQ does not seem to be limited to algorithms that use a traditional priority queue.
This supports our belief that the Neural PQ implementations are quite general, and these can perform various roles, such as acting as a traditional data structure,
or a persistent-memory for accessing past overall graph states.
We provide the table with algorithm-wise performance of each Neural PQ in Appendix \ref{sec:further-evaluations}.
Focussing on NPQ, we found that it outperforms the baseline for  algorithms (more than half of the algorithms).
We see that for  algorithms, it improves the performance or closes the gap to true prediction by at least .
For  algorithms, it improves performance/reduces the gap by at least .

\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{./imgs/evaluation/best-pq-and-baseline-all-algs.png}
    \caption{Evaluation results for best performing Neural PQ and the baseline MPNN model
             for the 30 algorithms from CLRS-30, sorted by the relative improvement in performance.}
    \label{fig:eval-best-pq-all-algs}
\end{figure*}

\subsection{Long-Range Reasoning}
\label{sec:eval-lrgb}
Message-passing based GNNs exchange information between 1-hop neighbours to build node representations at each layer. Past works have shown that such
information propagation leads to over-squashing when the path of information traversal is long, and so such models perform poorly on tasks requiring long-range
interaction \citep{Alon-GNN-over-squashing,Dwivedi-Long-Range-Benchmark}. \citet{Dwivedi-Long-Range-Benchmark} have proposed a collection of
graph learning datasets to form `Long Range Graph Benchmark' (LRGB), each of which arguably require long-range interaction reasoning to achieve strong performance.
In these experiments, we test the performance of using Neural PQs on Peptides-struct dataset from the LRGB benchmark.

Figure \ref{fig:eval-peptides-struct} shows the test MAE results for the different Neural PQs and the baseline.
Notably, all Neural PQs outperform the baseline for GATv2 processor, while only NPQ-P-SV and
NPQ outperform the baseline on the other two processors. The success of NPQ-P-SV and NPQ
means that these Neural PQs are empirically helping the models with long-range reasoning. Notably, we see that Weighted popping seems more useful for long-range reasoning.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=\linewidth]{./imgs/evaluation/peptides-struct.png}
    \caption{Test MAE (Mean  Standard Deviation) of different Neural PQs with different base
             processors on Peptides-struct dataset, run with 3 different seeds. \textbf{Lower} the test MAE, \textbf{better} is the performance.}
    \label{fig:eval-peptides-struct}
\end{figure}

\section{Conclusion and Future Works}
External memory modules have helped traditional RNNs improve their algorithmic reasoning capabilities.
A natural hypothesis would be that external memory modules can also help graph neural networks (GNNs) with algorithmic reasoning.
However, this remains a largely unexplored domain.
In this paper, we proposed Neural PQs, a general framework for adding memory modules to GNNs, with inspirations from traditional priority queues.
We proposed and motivated a desiderata for memory modules, and presented NPQ, an implementation with the Neural PQ framework that exhibited the desiderata.

We empirically show that NPQs indeed help with algorithmic reasoning, and without any extra supervision, matches the performance of the baseline
model that has access to true priority queue operations on Dijkstra's algorithm. The performance gains are not limited to algorithms using priority queues.
Furthermore, we show that the Neural PQs help with capturing long-range interaction, by demonstrating their prowess on the \verb|Peptides-struct| dataset
from the Long-Range Graph Benchmark.

The success of the Neural PQs has a wide effect on the field of representational learning. It opens up a research domain exploring the use of memory modules
with GNNs, especially their interfacing with the message-passing framework. The Neural PQs take crucial steps towards advancing the neural algorithmic
reasoning field. These also hold potential with various other fields and tasks, as seen by their performance on the long-range reasoning task.

We have limited our focus on simple memory module operations. Potential future works could involve exploration of more complicated definitions.
These definitions might be formed by analysing the reasons behind the greater success of Neural PQs on some algorithms as opposed to others.
Neural PQs can also be used for various other graph tasks, and it would be interesting to explore their uses for these.

\ifaccepted
\section*{Acknowledgements}

We thank Adrià Puigdomènech and Karl Tuyls for reviewing the paper prior to the submission.

\fi


\bibliography{workshop_paper}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn

\section{Permutation-Equivariance}
\label{sec:appendix-permutation-equivariance}
Node permutation-equivariance is an essential property shown by majority of the GNNs, as it embodies a key graph symmetry.
A GNN layer is said to be equivariant to permutation of the nodes if and only if any permutation of the node IDs, while maintaining
the overall graph structure, leads to the same permutation of the node features. Let us continue with considering our
graph to be . Let  be a permutation of the node IDs.
For ease, let  be an overloaded permutation operation, affecting the permutation  over all domains .
For example, for the domain of vertices/nodes , we have  for all .

\subsection{MPNN Permutation-Equivariance}
GNNs following the message-passing framework are permutation-equivariant. We consider the recurrent setup of CLRS benchmark here.
This is fairly easy to show. First, we recall the relevant equations from Section \ref{sec:clrs-background} below.
\begin{IEEEeqnarray}{rCl}
   \textbf{z}_i^{(t)} & = & f_A(\textbf{h}_i, \textbf{h}_i^{(t - 1)}) \\
   \textbf{m}_{ij} & = & f_m(\textbf{z}_i^{(t)}, \textbf{z}_j^{(t)}, \textbf{h}_{ij}, \textbf{h}_g) \\
   \textbf{m}_i & = & \bigoplus_{j \in \mathcal{N}_i} \textbf{m}_{ji} \\
   \textbf{h}_i^{(t)} & = & f_r(\textbf{z}_i^{(t)}, \textbf{m}_i)
\end{IEEEeqnarray}
We can consider matrices  and  indexed by the vertices , containing values  and , respectively.
We also have a matrix of edge features  index by edges  with value .
Further, we can define the above operations as a single layer , such that:
\begin{IEEEeqnarray}{rCl}
   \mathbf{H}^{(t)} & = & \mathbf{F}_{mpnn} \left( \mathbf{H}^{(t - 1)} , \mathbf{H}, \mathbf{E} \right)
\end{IEEEeqnarray}
In order to prove that message-passing GNNs are permutation-equivariant, we need to show that:
\begin{IEEEeqnarray}{rCl}
   \mathbf{F}_{mpnn} \left( \rho \left( \mathbf{H}^{(t - 1)} \right) , \rho \left( \mathbf{H} \right), \rho \left( \mathbf{E} \right) \right) & = & \rho \left( \mathbf{H}^{(t)} \right)
\end{IEEEeqnarray}

\subsubsection*{Proof}
We start by noting that by definition of  and permutation,  and  are matrices such that
they have values  and , respectively, for index .
Also,  is indexed by pairs , where , containing value .
Thus, we can define  as below.
\begin{IEEEeqnarray}{rCl}
   \mathbf{H}^{\prime(t)} & = & \mathbf{F}_{mpnn} \left( \rho \left( \mathbf{H}^{(t - 1)} \right) , \rho \left( \mathbf{H} \right), \rho \left( \mathbf{E} \right) \right)
\end{IEEEeqnarray}
where  has value  for index , with  as defined below.
\begin{IEEEeqnarray}{rCl}
   \textbf{z}_i^{\prime(t)} & = & f_A(\textbf{h}_{\rho(i)}, \textbf{h}_{\rho(i)}^{(t - 1)}) \\
   \textbf{m}_{ij}^{\prime} & = & f_m(\textbf{z}_{i}^{\prime(t)}, \textbf{z}_{j}^{\prime(t)}, \textbf{h}_{\rho(i)\rho(j)}, \textbf{h}_g) \label{eq:perm-eq-1} \\
   \textbf{m}_i^{\prime} & = & \bigoplus_{j \in \rho(\mathcal{N}_{\rho(i)})} \textbf{m}_{ji}^{\prime} \label{eq:perm-eq-2} \\
   \textbf{h}_i^{\prime(t)} & = & f_r(\textbf{z}_i^{\prime(t)}, \textbf{m}_i^{\prime}) \label{eq:perm-eq-3}
\end{IEEEeqnarray}
where  is the one-hop neighbourhood on the permuted graph, and can be simply defined as
.
All these equations follow simply from application of the MPNN equations, as noted before, on the permuted matrices.

We start by noting that  is simply the value . Thus, we get the below equation.
\begin{IEEEeqnarray}{rCl}
   \textbf{z}_i^{\prime(t)} & = & \textbf{z}_{\rho(i)}^{(t)} \label{eq:perm-eq-a}
\end{IEEEeqnarray}
Using this in equation \ref{eq:perm-eq-1}, we get:
\begin{IEEEeqnarray}{rCl}
   \textbf{m}_{ij}^{\prime} & = & f_m(\textbf{z}_{\rho(i)}^{(t)}, \textbf{z}_{\rho(j)}^{(t)}, \textbf{h}_{\rho(i)\rho(j)}, \textbf{h}_g) \\
   & = & \textbf{m}_{\rho(i)\rho(j)}
\end{IEEEeqnarray}
Using this in equation \ref{eq:perm-eq-2}, we get:
\begin{IEEEeqnarray}{rCl}
   \textbf{m}_i^{\prime} & = & \bigoplus_{j \in \rho(\mathcal{N}_{\rho(i)})} \textbf{m}_{\rho(j)\rho(i)} \label{eq:perm-eq-b}
\end{IEEEeqnarray}
We also note that, by definition of , we get the following.
\begin{IEEEeqnarray}{C}
   j \in \rho(\mathcal{N}_{\rho(i)}) \iff \rho(j) \in \mathcal{N}_{\rho(i)}
\end{IEEEeqnarray}
Using this in Equation \ref{eq:perm-eq-b}, we get:
\begin{IEEEeqnarray}{rCl}
   \textbf{m}_i^{\prime} & = & \bigoplus_{\rho(j) \in \mathcal{N}_{\rho(i)}} \textbf{m}_{\rho(j)\rho(i)} \\
   & = & \textbf{m}_{\rho(i)}
\end{IEEEeqnarray}
Substituting the above value and value from Equation \ref{eq:perm-eq-a} in Equation \ref{eq:perm-eq-3}:
\begin{IEEEeqnarray}{rCl}
   \textbf{h}_i^{\prime(t)} & = & f_r(\textbf{z}_{\rho(i)}^{(t)}, \textbf{m}_{\rho(i)}) \\
   & = & \textbf{h}_{\rho(i)}^{(t)}
\end{IEEEeqnarray}

But that means that .
\begin{IEEEeqnarray}{rCl}
   \therefore \mathbf{F}_{mpnn} \left( \rho \left( \mathbf{H}^{(t - 1)} \right) , \rho \left( \mathbf{H} \right), \rho \left( \mathbf{E} \right) \right) & = & \rho \left( \mathbf{H}^{(t)} \right)
\end{IEEEeqnarray}

Hence, proved that message-passing GNNs show node-permutation equivariance.

\subsection{Neural PQ Permutation-Equivariance}
In a similar vein, we can show that the memory modules following the Neural PQ framework proposed by me, show node-permutation equivariance.
Below we recall the equations for the Neural PQ framework.
\begin{IEEEeqnarray}{rCl}
   \textbf{z}_i^{(t)} & = & f_A(\textbf{h}_i, \textbf{h}_i^{(t - 1)}) \\
   \textbf{m}_{ij} & = & f_m(\textbf{z}_i^{(t)}, \textbf{z}_j^{(t)}, \textbf{h}_{ij}, \textbf{h}_g) \\
   V_i & = & f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}) \\
   M_i & = & f_{M}^{(pq)}(V_i, \textbf{z}_i^{(t)}) \, \cup \, \left\{ \textbf{m}_{ji} \, | \, j \in \mathcal{N}_i \right\} \\
   \textbf{m}_i & = & \bigoplus_{\textbf{m} \in M_i} \textbf{m} \\
   \textbf{h}_i^{(t)} & = & f_r(\textbf{z}_i^{(t)}, \textbf{m}_i) \\
   \textbf{h}_{pq}^{(t)} & = & f_{push}(\textbf{h}_{pq}^{(t - 1)}, \textbf{z}^{(t)})
\end{IEEEeqnarray}
where  is a multi-set of all the encoded inputs , i.e. .

We can take , and  as defined in the previous section. Then, we can define the overall operations of the Neural PQ as a single
layer , such that:
\begin{IEEEeqnarray}{rCl}
   \mathbf{H}^{(t)}, \textbf{h}_{pq}^{(t)} & = & \mathbf{F}_{npq} \left( \mathbf{H}^{(t - 1)} , \mathbf{H}, \mathbf{E}, \textbf{h}_{pq}^{(t - 1)} \right)
\end{IEEEeqnarray}
In order to prove that modules following the Neural PQ framework are permutation-equivariant, we need to show that:
\begin{IEEEeqnarray}{rCl}
   \mathbf{F}_{npq} \left( \rho \left( \mathbf{H}^{(t - 1)} \right) , \rho \left( \mathbf{H} \right), \rho \left( \mathbf{E} \right), \textbf{h}_{pq}^{(t - 1)} \right)
      & = & \rho \left( \mathbf{H}^{(t)} \right), \textbf{h}_{pq}^{(t)}
\end{IEEEeqnarray}

\subsubsection*{Proof}
The description of ,  and  follow here same as before.
We can define  and  as below.
\begin{IEEEeqnarray}{rCl}
   \mathbf{H}^{\prime(t)}, \textbf{h}_{pq}^{\prime(t)} & = &
      \mathbf{F}_{npq} \left( \rho \left( \mathbf{H}^{(t - 1)} \right) , \rho \left( \mathbf{H} \right), \rho \left( \mathbf{E} \right), \textbf{h}_{pq}^{(t - 1)} \right)
\end{IEEEeqnarray}
Thus,  has value  for index , with  and 
as defined below.
\begin{IEEEeqnarray}{rCl}
   \textbf{z}_i^{\prime(t)} & = & f_A(\textbf{h}_{\rho(i)}, \textbf{h}_{\rho(i)}^{(t - 1)}) \\
   \textbf{m}_{ij}^{\prime} & = & f_m(\textbf{z}_{i}^{\prime(t)}, \textbf{z}_{j}^{\prime(t)}, \textbf{h}_{\rho(i)\rho(j)}, \textbf{h}_g) \\
   V_i^{\prime} & = & f_{pop}(\textbf{z}_i^{\prime(t)}, \textbf{z}^{\prime(t)}, \textbf{h}_{pq}^{(t - 1)}) \label{eq:perm-eq-4} \\
   M_i^{\prime} & = & f_{M}^{(pq)}(V_i^{\prime}, \textbf{z}_i^{\prime(t)}) \, \cup \, \left\{ \textbf{m}_{ji}^{\prime} \, | \, j \in \rho(\mathcal{N}_{\rho(i)}) \right\} \label{eq:perm-eq-5} \\
   \textbf{m}_i^{\prime} & = & \bigoplus_{\textbf{m} \in M_i^{\prime}} \textbf{m} \label{eq:perm-eq-6} \\
   \textbf{h}_i^{\prime(t)} & = & f_r(\textbf{z}_i^{\prime(t)}, \textbf{m}_i^{\prime}) \label{eq:perm-eq-7} \\
   \textbf{h}_{pq}^{\prime(t)} & = & f_{push}(\textbf{h}_{pq}^{(t - 1)}, \textbf{z}^{\prime(t)}) \label{eq:perm-eq-8}
\end{IEEEeqnarray}
where  is a multi-set of all the node embeddings
, i.e. .

The following can be shown in a similar fashion as the previous proof:
\begin{IEEEeqnarray}{rCl}
   \textbf{z}_i^{\prime(t)} & = & \textbf{z}_{\rho(i)}^{(t)} \label{eq:perm-eq-d} \\
   \textbf{m}_{ij}^{\prime} & = & \textbf{m}_{\rho(i)\rho(j)} \label{eq:perm-eq-e} \\
   j \in \rho(\mathcal{N}_{\rho(i)}) & \iff & \rho(j) \in \mathcal{N}_{\rho(i)} \label{eq:perm-eq-f}
\end{IEEEeqnarray}

Using Equation \ref{eq:perm-eq-d} and the definition of , we get:
\begin{IEEEeqnarray}{rCl}
    \textbf{z}^{\prime(t)} & = & \{ \! \{\textbf{z}_{\rho(i)}^{(t)} \, | \, i \in \mathcal{V} \} \! \} \\
    & = & \{ \! \{\textbf{z}_i^{(t)} \, | \, i \in \mathcal{V} \} \! \} \\
    & = & \textbf{z}^{(t)} \label{eq:perm-eq-g}
\end{IEEEeqnarray}
because  and  is a permutation, and so the multi-sets are equal.

Using Equation \ref{eq:perm-eq-d} and Equation \ref{eq:perm-eq-g}, we can update Equation \ref{eq:perm-eq-4} as below.
\begin{IEEEeqnarray}{rCl}
    V_i^{\prime} & = & f_{pop}(\textbf{z}_{\rho(i)}^{(t)}, \textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}) \\
    & = & V_{\rho(i)} \label{eq:perm-eq-h}
\end{IEEEeqnarray}

Substituting Equation \ref{eq:perm-eq-d}, Equation \ref{eq:perm-eq-e} and Equation \ref{eq:perm-eq-h} in Equation \ref{eq:perm-eq-5}, we get:
\begin{IEEEeqnarray}{rCl}
    M_i^{\prime} & = & f_{M}^{(pq)}(V_{\rho(i)}, \textbf{z}_{\rho(i)}^{(t)}) \, \cup \, \left\{ \textbf{m}_{\rho(i)\rho(j)} \, | \, j \in \rho(\mathcal{N}_{\rho(i)}) \right\}
\end{IEEEeqnarray}
Using Equation \ref{eq:perm-eq-f} in the above equation, we get:
\begin{IEEEeqnarray}{rCl}
    M_i^{\prime} & = & f_{M}^{(pq)}(V_{\rho(i)}, \textbf{z}_{\rho(i)}^{(t)}) \, \cup \, \left\{ \textbf{m}_{\rho(i)\rho(j)} \, | \, \rho(j) \in \mathcal{N}_{\rho(i)} \right\} \\
    & = & M_{\rho(i)}
\end{IEEEeqnarray}
Substituting this in Equation \ref{eq:perm-eq-6}, we get:
\begin{IEEEeqnarray}{rCl}
    \textbf{m}_i^{\prime} & = & \bigoplus_{\textbf{m} \in M_{\rho(i)}} \textbf{m} \\
    & = & \textbf{m}_{\rho(i)}
\end{IEEEeqnarray}
Using this and Equation \ref{eq:perm-eq-d} in Equation \ref{eq:perm-eq-7}, we get:
\begin{IEEEeqnarray}{rCl}
    \textbf{h}_i^{\prime(t)} & = & f_r(\textbf{z}_{\rho(i)}^{(t)}, \textbf{m}_{\rho(i)}) \\
    & = & \textbf{h}_{\rho(i)}^{(t)}
\end{IEEEeqnarray}
This means that .

Additionally, substituting the value from Equation \ref{eq:perm-eq-g} in Equation \ref{eq:perm-eq-8}, we get:
\begin{IEEEeqnarray}{rCl}
    \textbf{h}_{pq}^{\prime(t)} & = & f_{push}(\textbf{h}_{pq}^{(t - 1)}, \textbf{z}^{(t)}) \\
    & = & \textbf{h}_{pq}^{(t)}
\end{IEEEeqnarray}

Since,  and , we have:
\begin{IEEEeqnarray}{rCl}
    \mathbf{F}_{npq} \left( \rho \left( \mathbf{H}^{(t - 1)} \right) , \rho \left( \mathbf{H} \right), \rho \left( \mathbf{E} \right), \textbf{h}_{pq}^{(t - 1)} \right)
      & = & \rho \left( \mathbf{H}^{(t)} \right), \textbf{h}_{pq}^{(t)}
\end{IEEEeqnarray}

Hence, proved, that modules following the Neural PQ framework show node-permutation equivariance.

\section{NPQ Reformulation}
\label{sec:appendix-pqv2-reformulation}
In Section \ref{sec:pqv2}, we introduced the push and pop operations for NPQ. However, the equations defined there make use of the granted pop proportions
 (and  as well in the push operation). These are not exactly available to the respective functions as defined in the
Neural PQ framework. However, these are used in Section \ref{sec:pqv2} only to make the equations easier to understand, and they instead can be reformulated
to conform to the Neural PQ framework. We provide the reformulation below.

\subsection{Pop Function}
The calculation of pop request fractions  as defined in Section \ref{sec:pqv2-pop} can be combined into a single function
, which takes ,  and  -- the embedding of node , previous NPQ state and the index of
the queue element we want to calculate the pop request for, and returns the pop request fraction .
\begin{IEEEeqnarray}{rCl}
    p_j^{(i)} & = & \proc{Pop-Request}(\textbf{z}_i^{(t)}, \textbf{h}_{pq}^{(t - 1)}, j)
\end{IEEEeqnarray}

We can calculate the sum of the pop requests as below:
\begin{IEEEeqnarray}{rCl}
    \proc{Tot-Pop-Req}(\textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}, j) & = & \sum_{\mathbf{z}_k \in \textbf{z}^{(t)}} \proc{Pop-Request}(\mathbf{z}_k, \textbf{h}_{pq}^{(t - 1)}, j)
\end{IEEEeqnarray}
By definition of , we have:
\begin{IEEEeqnarray}{rCl}
    \sum_{k \in \mathcal{V}} p_j^{(k)} & = & \sum_{k \in \mathcal{V}} \proc{Pop-Request}(\textbf{z}_k^{(t)}, \textbf{h}_{pq}^{(t - 1)}, j) \\
    & = & \sum_{\mathbf{z}_k \in \textbf{z}^{(t)}} \proc{Pop-Request}(\textbf{z}_k, \textbf{h}_{pq}^{(t - 1)}, j) \\
    & = & \proc{Tot-Pop-Req}(\textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}, j)
\end{IEEEeqnarray}
Using this, we can reformulate the pop proportions  as below:
\begin{IEEEeqnarray}{rCl}
    q_j^{(i)} & = & \left\{
        \begin{array}{cl}
            p_j^{(i)} & \text{, if } \proc{Tot-Pop-Req}(\textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}, j) \leq \textbf{s}_{j}^{(t - 1)} \\
            \frac{p_j^{(i)}}{\proc{Tot-Pop-Req}(\textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}), j} \cdot \textbf{s}_{j}^{(t - 1)} & \text{, else}
        \end{array}
    \right.
\end{IEEEeqnarray}

It is easy to see that this reformulation conforms to the pop function as defined in the Neural PQ framework.

\subsection{Push Function}
We can rewrite the push function as below:
\begin{IEEEeqnarray}{rCl}
    \textbf{s}_i^{\prime} & = & \textbf{s}_{i}^{(t - 1)} - \min\left( \proc{Tot-Pop-Req}(\textbf{z}^{(t)}, \textbf{h}_{pq}^{(t - 1)}, i), \textbf{s}_{i}^{(t - 1)} \right) \\
    \textbf{s}^{\prime} & = & \text{ nonzero}_i\left( \textbf{s}_i^{\prime} \right) \\
    \textbf{v}^{\prime} & = & \textbf{v}^{(t)}[\text{ arg-nonzero}_i\left( \textbf{s}_i^{\prime} \right)] \\
    \textbf{v} & = & \text{ tanh}\left( \sum_{\mathbf{z}_k \in \textbf{z}^{(t)}} f_v(\mathbf{z}_k) \right) \\
    \mathbf{s} & = & \text{ sigmoid}\left( \sum_{\mathbf{z}_k \in \textbf{z}^{(t)}} f_s^{(push)}(\mathbf{z}_k) \right)\\
    f_{push}(\langle \textbf{v}^{(t - 1)}, \textbf{s}^{(t - 1)} \rangle, \textbf{z}^{(t)}) & = &
        \langle \textbf{v}^{\prime} \, || \,  [\textbf{v}], \textbf{s}^{\prime} \, || \,  [\mathbf{s}_{push}] \rangle
\end{IEEEeqnarray}

Again, it is easy to see that the above reformulation conforms to the push function as defined in the Neural PQ framework.

\section{Priority Queue Alignment}
\label{sec:pq-alignment}
Following is the priority queue setup we consider. We will then show that under certain assumptions, we can reduce the NPQ computation to the equations for the priority
queue setup defined.

Let us suppose some algorithm uses a priority queue. We shall take the algorithm to push at most  element and pop at most  element in each timestep.
We take the pushing and popping to be controlled by the overall graph, but under certain assumptions, the reduction can be extended to having these from nodes
instead. Further, we take that the output of the popping is returned to some specific node.
Let  be the set of past un-popped key-value-pair pushes to the priority queue.
Let  be the output to the node .
We further assume that all priority keys and values are unique.
We can represent the operation of a traditional priority queue over a timestep as below,
using  from the previous timestep and by calculating the next  and the outputs .
We take the value returned to be  if no value is returned to the node.
\begin{IEEEeqnarray}{rCl}
    \mathbf{o}_i^{(t)} & = & \left\{
        \begin{array}{ll}
            \nu_{max}^{(t - 1)} & \text{, if we pop this timestep and return to node } i \\
            \mathbf{0} & \text{, else}
        \end{array}
    \right. \\
    P^{\prime(t)} & = & \left\{
        \begin{array}{ll}
            P^{(t - 1)} - (k_{max}^{(t - 1)}, \nu_{max}^{(t - 1)}) & \text{, if we pop this timestep} \\
            P^{(t - 1)} & \text{, else}
        \end{array}
    \right. \\
    P^{(t)} & = & \left\{
        \begin{array}{ll}
            P^{\prime(t)} \cup (k^{(t)}, \nu^{(t)}) & \text{, if we push some key-value pair } (k^{(t)}, \nu^{(t)}) \\
            P^{\prime(t)} & \text{, else}
        \end{array}
    \right.
\end{IEEEeqnarray}
where  such that .

We shall now show that, under certain assumptions, the NPQ operations can be reduced to the above operations. More specifically, we shall show that
the NPQ state  mimics the priority queue state , and the NPQ messages  mimics the returned value .
The main assumptions we make is that the linear layers are capable of expressing the required functions and the intermediate embedding sizes are big enough to not lose
any information. We go into more details about the assumptions as we describe the reduction.

We continue with the graph  setup, with  as the node features.
Since NPQ uses a GNN controller, we assume that we can make all decisions from the node features, i.e. the node features determine whether we want to push and
pop values and if so, what value and key to push, and which node to pop to. Let 
be the previous NPQ state, such that each element of  is an encoding of a unique key-value-pair in , with an element existing for each
key-value pair. Let  be the mapping from NPQ values to the corresponding keys, and  be the mapping from NPQ to the values in .
Let for all , .

\paragraph{Pop Function} We shall now breakdown the pop function, to make the overall computation match the traditional priority queue's. We assume that
we can instantiate  in a manner such that  iff we want to pop a value for node  in timestep , else .
Let us further suppose that the attentional mechanism calculating the coefficient  simply extracts the encoded priority key in
. More specifically, coefficient  is calculated as below in NPQ.
\begin{IEEEeqnarray}{rCl}
    e_{j}^{(i,h)} & = & \text{ LeakyReLU}\left(f_{a_1}^{(h)}(\textbf{z}_i^{(t)}) + f_{a_2}^{(h)}(\textbf{v}^{(t - 1)}_j)\right) \\
    \alpha_{j}^{(i,h)} & = & \text{ softmax}_j\left( e_{j}^{(i,h)} \right) \\
    c_{j}^{(i)} & = & \text{ softmax}_j\left( f_{a}([\alpha_{j}^{(i,1)}, \ldots, \alpha_{j}^{(i,h)}, \ldots]) \right)
\end{IEEEeqnarray}
For simplicity, we can take number of attention heads to be 1. Let  for all . Further,
suppose that .
Also, let us take  to be an identity function. Thus, we get the below equation for .
\begin{IEEEeqnarray}{rCl}
    e_{j}^{(i,h)} & = & \kappa(\textbf{v}^{(t - 1)}_j) \\
    \alpha_{j}^{(i,h)} & = & \text{ softmax}_j\left( \kappa(\textbf{v}^{(t - 1)}_j) \right) \\
    c_{j}^{(i)} & = & \text{ softmax}_j\left( \text{ softmax}_j\left( \kappa(\textbf{v}^{(t - 1)}_j) \right) \right)
\end{IEEEeqnarray}
Since we use Max Popping, we have the pop proportions requested as below.
\begin{IEEEeqnarray}{rCl}
    k & = & \argmax_{k \in \mathcal{I}^{(t - 1)}_{pq}} \text{ softmax}_k\left( \text{ softmax}_k\left( \kappa(\textbf{v}^{(t - 1)}_k) \right) \right) \\
    p_j^{(i)} & = & s_{pop}^{(i)} \cdot \mathbb{I}_{\{ k \}}\left( j \right)
\end{IEEEeqnarray}
We can show easily that , for some set  and values . Thus, we can simplify
the pop proportions.
\begin{IEEEeqnarray}{rCl}
    k & = & \argmax_{k \in \mathcal{I}^{(t - 1)}_{pq}} \kappa(\textbf{v}^{(t - 1)}_k) \\
    p_j^{(i)} & = & s_{pop}^{(i)} \cdot \mathbb{I}_{\{ k \}}\left( j \right)
\end{IEEEeqnarray}
But  is nothing but index of the NPQ element corresponding to .
Thus, we can re-formulate the above equation to use this.
\begin{IEEEeqnarray}{rCl}
    p_j^{(i)} & = & s_{pop}^{(i)} \cdot \mathbb{I}_{\left\{ k_{max}^{(t - 1)} \right\}}\left( \kappa(\textbf{v}^{(t - 1)}_j) \right)
\end{IEEEeqnarray}
Using the assumption about  and rewriting the indicator function, we get the following equation.
\begin{IEEEeqnarray}{rCl}
    p_j^{(i)} & = & \left\{
        \begin{array}{ll}
            1 & \text{, if we pop this timestep, return to node } i \\
            & \text{\, \, \, and } \kappa(\textbf{v}^{(t - 1)}_j) = k_{max}^{(t - 1)} \\
            0 & \text{, else}
        \end{array}
    \right.
\end{IEEEeqnarray}
Since , NPQ will fully grant each pop request. Thus, we have .
NPQ has pop function's output values as defined below.
\begin{IEEEeqnarray}{rCl}
    f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \langle \textbf{v}^{(t - 1)}, \textbf{s}^{(t - 1)} \rangle) & = & \left\{
        \sum_{j \in \mathcal{I}^{(t - 1)}_{pq}} q_j^{(i)} \cdot \textbf{v}_j^{(t - 1)}
    \right\}
\end{IEEEeqnarray}
Using the previous equations, we get the following reduction for .
\begin{IEEEeqnarray}{rCl}
V_i & = & \left\{
        \begin{array}{ll}
            \{ \textbf{v}^{(t - 1)}_j \} & \text{, if we pop this timestep and return to node } i \\
            & \text{\, \, \, where } \kappa(\textbf{v}^{(t - 1)}_j) = k_{max}^{(t - 1)} \\
            \{ \mathbf{0} \} & \text{, else}
        \end{array}
    \right.
\end{IEEEeqnarray}

\paragraph{Message Encoding Function} We assume that NPQ learns a message encoding function  such that 
and  for all . Thus, the reduction for  is as follows.
\begin{IEEEeqnarray}{rCl}
    M_i & = & \left\{
        \begin{array}{ll}
            \nu_{max}^{(t - 1)} & \text{, if we pop this timestep and return to node } i \\
            \mathbf{0} & \text{, else}
        \end{array}
    \right.
\end{IEEEeqnarray}
where we make use of the fact that 
which follows by the definition of ,  and the key-value-pair .

This in fact is the same value as the output value , returned in the traditional priority queue.
Thus, we have shown that NPQ messages mimic the returned output. We only need to now show that the state update can be mimicked as well.

\paragraph{Push Function}
It is straightforward to see, albeit somewhat tedious to show, that the NPQ state 
is such that  and that the correspondence between  and the key-value-pairs in
 is maintained by  and . Thus, we use this directly without proof.

Similar to the push strength, we assume that we can instantiate  in a manner such that  iff we want to push a value in timestep ,
else . We also assume that we can instantiate  such that when we want to push a value, we get  as a unique recoverable encoding
of the key-value-pair that we want to push. That means that we can now define another mapping,  and , such that these are equal
to  and  for the previous elements, and for the new element, these are equal to the newly pushed key-value-pair. This means that the new state NPQ
 mimics the priority queue state .

Hence proved that we can reduce the operations of NPQ to a traditional priority queues, under the noted assumptions.

\section{Greater Memory-Persistence -- NPQ-P}
\label{sec:pqv0}
NPQ defined earlier deletes the popped elements from the queue. In this variation, we explore a more persistent Neural PQ implementation. We refer to this as NPQ-Persistent or NPQ-P.
NPQ-P does not delete popped elements. Here the pop operation acts more like a `seek' operation, i.e. it just returns the element but does not delete it.
We make a further simplification by making the push and pop operations discrete.
Thus, at each timestep, one element is pushed into the queue, and one element (or a weighted combination of elements) per node is read and passed as a message
to each node. Below we note the changes in the components for this implementation, as compared to NPQ. The message encoding function remains the same, but the rest change.

\subsection{State}
Since we do not have a continuous push and pop operation, we do not need to keep track of the strengths of the different values in the queue.
Thus, the state of the priority queue  is simply the list of memory values.
For the sake of consistency with NPQ, we represent the state as a single value tuple.
\begin{IEEEeqnarray}{rCl}
    \textbf{h}_{pq}^{(t)} & = & \langle \textbf{v}^{(t)} \rangle \label{eq:state-pqv0}
\end{IEEEeqnarray}

\subsection{Pop function}
\label{sec:pqv0-pop}
As noted, pop does not delete an element from the queue.
The priority of the elements of the queue is determined using attention coefficients , denoting the coefficient for the th element of the
queue with respect to the th node, as defined in Equation \ref{eq:att-coef-pqv2}.
Since we no longer have push and pop strengths, we no longer need the request-grant framework. We again have two popping strategies -- Max Popping and Weighted Popping,
and related implementations NPQ-P and NPQ-P respectively.

\subsubsection*{Max Popping}
\begin{IEEEeqnarray}{rCl}
    j & = & \argmax_{j \in \mathcal{I}^{(t - 1)}_{pq}} c_{j}^{(i)} \\
    f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \langle \textbf{v}^{(t - 1)} \rangle) & = & \left\{ \textbf{v}^{(t - 1)}_j \right\} \label{eq:pop-max-pqv0}
\end{IEEEeqnarray}

\subsubsection*{Weighted Popping}
\begin{IEEEeqnarray}{rCl}
    f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \langle \textbf{v}^{(t - 1)} \rangle) & = & \left\{
        \sum_{j \in \mathcal{I}^{(t - 1)}_{pq}} c_{j}^{(i)} \cdot \textbf{v}^{(t - 1)}_j
    \right\} \label{eq:pop-weighted-pqv0}
\end{IEEEeqnarray}

\subsection{Push Function}
\label{sec:push-pqv0}
Since we are no longer deleting elements, the push function simply consists of appending new push value to the queue.
\begin{IEEEeqnarray}{rCl}
    \textbf{v} & = & \text{ tanh}\left( \sum_{i \in \mathcal{V}} f_v(\textbf{z}_i^{(t)}) \right) \\
    f_{push}(\langle \textbf{v}^{(t - 1)} \rangle, \textbf{z}^{(t)}) & = & \langle \textbf{v}^{(t - 1)} \, || \,  [\textbf{v}] \rangle \label{eq:push-pqv0}
\end{IEEEeqnarray}

\section{Graph Priority Queue -- NPQ-SA}
In NPQ, each node pops different elements from the queue, and thus receives different messages from the queue. This node-wise treatment of the priority queue
might not be always ideal, and we might want to treat the priority queue messaging to be uniform for the whole graph, i.e. we might want each node to
receive the same values from the Neural PQ. In NPQ Send to All or NPQ-SA, we propose a variant that returns the same set of popped values for each node. This set is simply
the union of all the values that would have been popped from the queue in NPQ for the different nodes. Thus, in NPQ-SA, each node receives
 values from the queue. All the components of the Neural PQ remain the same as in Section \ref{sec:pqv2} except for the pop function, which is as follows.

\subsection{Pop Function}
Most of the function remains the same and uses the attention coefficients and pop fractions  as defined in Section \ref{sec:pqv2-push}.
The changes are as below, where we now first calculate , the set of values popped for node , if we were using the pop function of NPQ.
These are then union-ed to get the values returned for each node.

\begin{IEEEeqnarray}{rCl}
    V^{\prime}_k & = & \left\{
        \sum_{j \in \mathcal{I}^{(t - 1)}_{pq}} q_j^{(k)} \cdot \textbf{v}_j^{(t - 1)}
    \right\} \\
    f_{pop}(\textbf{z}_i^{(t)}, \textbf{z}^{(t)}, \langle \textbf{v}^{(t - 1)} \rangle) & = & \bigcup_{k \in \mathcal{V}} V^{\prime}_k
\end{IEEEeqnarray}


\section{Graph Priority Queue -- NPQ-SV}
\label{sec:pqv2-sv}
NPQ-SA is one way to model a graph controlled priority queue. Another way would be to only pop a single value from the priority queue,
and return this single value to all nodes. We implement this strategy in NPQ Single Value or NPQ-SV.
Again, only the pop function changes here.

\subsection{Pop Function}
NPQ-SV makes two changes to the pop function of NPQ -- (1) all pop strengths are aggregated to get a single value for all the nodes, and (2) all the attention
coefficients are aggregated to get the same values for all the nodes. We calculate this single pop strength  and attention coefficients
 as below.
\begin{IEEEeqnarray}{rCl}
    s_{pop} & = & \text{ sigmoid} \left( \sum_{i \in \mathcal{V}} f_s^{(pop)}(\mathbf{z}_i^{(t)}) \right) \\
    c_j & = & \text{ softmax}_j\left(\sum_{i \in \mathcal{V}} c_j^{(i)}\right)
\end{IEEEeqnarray}
where  is the node-wise attention coefficients, as calculated in Section \ref{sec:pqv2-push}. We now need to just update the pop requests
 to use these values, as done below. Rest of the function remains the same as in NPQ.

\subsubsection*{Max Popping}
\begin{IEEEeqnarray}{rCl}
    k & = & \argmax_{k \in \mathcal{I}^{(t - 1)}_{pq}} c_{k} \\
    p_j^{(i)} & = & s_{pop} \cdot \mathbb{I}_{\{ k \}}\left( j \right)
\end{IEEEeqnarray}

\subsubsection*{Weighted Popping}
\begin{IEEEeqnarray}{rCl}
    p_j^{(i)} & = & s_{pop} \cdot c_{j}
\end{IEEEeqnarray}

\section{Further Evaluations}
\label{sec:further-evaluations}
In Section \ref{sec:evaluation}, we present various experiments we performed. In this section, we provide further details about the experiments and their results,
along with some more experiments.

\subsection{Dijkstra's Algorithm -- Different Base Processors}
We run experiments to test whether the performance improvements seen with the MPNN controlling the Neural PQs are
also seen with the use of different processors controlling the Neural PQs. We continue with Dijkstra's algorithm as the target task, and explore
the use of different base processors -- Deep Sets \citep{Zaheer-Deep-Sets}, GAT \citep{Velickovic-GAT}, GATv2 \citep{Brody-GATv2}, PGN \citep{Velickovic-PGN}
and Triplet MPNN \citep{Ibarz-Triplet-MPNN}, apart from MPNN. We compare the performance of baseline (no memory module),
NPQ-P-SA, NPQ-P-SV, NPQ and NPQ,
each with these different base processors.

Table \ref{table:all-procs-best-large} shows the test performance of the different Neural PQs when used with the above mentioned different base processors/controllers,
on graphs with 256 nodes.
We observe that for each processor, at least one of the Neural PQs outperforms the baseline.
NPQ outperforms the baseline for all processors, except Deep Sets and PGN, for both of which, it gets
a performance very close to the baseline. Thus, the use of Neural PQs does not seem to be limited to the basic MLP MPNN, although
interestingly, for some processor GNNs, like Deep Sets and PGN here, other Neural PQ variants seem to be more useful.

\setlength{\tabcolsep}{4pt}
\begin{table}[tbhp]
\begin{adjustwidth}{-2cm}{-2cm}
\centering
\captionsetup{margin=2cm}
\caption[Evaluation results with different bases on Dijkstra's algorithm on graphs with 256 nodes]{Test performance (Mean  Standard Deviation) of different
         Neural PQs with different base processors on learning Dijkstra's Algorithm,
         run with 3 different seeds. Tested on graphs with  nodes, and with Early-stopped model.}
\small
\begin{tabular}{lccccc}
    \hline
    \bfseries Processor & \bfseries Baseline & \bfseries NPQ-P-SA & \bfseries NPQ-P-SV & \bfseries NPQ & \bfseries NPQ \\
    \hline
    Deep Sets &  &  &  &  &  \\
    GAT &  &  &  &  &  \\
    GATv2 &  &  &  &  &  \\
    MPNN &  &  &  &  &  \\
    PGN &  &  &  &  &  \\
    Triplet MPNN &  &  &  &  &  \\
    \hline
\end{tabular}
\label{table:all-procs-best-large}
\end{adjustwidth}
\end{table}
\setlength{\tabcolsep}{6pt}

\subsection{Different Algorithms from CLRS}
Section \ref{sec:eval-all-algs} talks about our experiment of using the different Neural PQs for all 30 algorithms from CLRS-30.
Table \ref{table:all-algs-best-large} shows the algorithm-wise performance for each model. This also shows the Win/Tie/Loss
counts, which are calculated in the same manner as \citet{Velickovic-CLRS}. Table \ref{table:all-algs-best-large-win-tie-loss}
shows the algorithm-wise win/tie/loss of each model.

\setlength{\tabcolsep}{4pt}
\begin{table}[tbhp]
\begin{adjustwidth}{-2cm}{-2cm}
\centering
\captionsetup{margin=2cm}
\small
\begin{tabular}{lccccc}
    \hline
    \bfseries Algorithm & \bfseries Baseline & \bfseries NPQ-P-SA & \bfseries NPQ-P-SV & \bfseries NPQ & \bfseries NPQ \\
    \hline
    Activity Selector &  &  &  &  &  \\
    Articulation Points &  &  &  &  &  \\
    Bellman Ford &  &  &  &  &  \\
    BFS &  &  &  &  &  \\
    Binary Search &  &  &  &  &  \\
    Bridges &  &  &  &  &  \\
    Bubble Sort &  &  &  &  &  \\
    DAG Shortest Paths &  &  &  &  &  \\
    DFS &  &  &  &  &  \\
    Dijkstra &  &  &  &  &  \\
    Find Max Subarray &  &  &  &  &  \\
    Floyd-Warshall &  &  &  &  &  \\
    Graham Scan &  &  &  &  &  \\
    Heapsort &  &  &  &  &  \\
    Insertion Sort &  &  &  &  &  \\
    Jarvis March &  &  &  &  &  \\
    KMP Matcher &  &  &  &  &  \\
    LCS Length &  &  &  &  &  \\
    Matrix Chain Order &  &  &  &  &  \\
    Minimum &  &  &  &  &  \\
    MST-Kruskal &  &  &  &  &  \\
    MST-Prim &  &  &  &  &  \\
    Na{\"i}ve String Matcher &  &  &  &  &  \\
    Optimal BST &  &  &  &  &  \\
    Quickselect &  &  &  &  &  \\
    Quicksort &  &  &  &  &  \\
    Segments Intersect &  &  &  &  &  \\
    SCC &  &  &  &  &  \\
    Task Scheduling &  &  &  &  &  \\
    Topological Sort &  &  &  &  &  \\
    \hline
    Overall Average &  &  &  &  &  \\    
    Win/Tie/Loss Counts &  &  &  &  &  \\
    \hline
\end{tabular}
\caption[Evaluation results with MPNN base on all 30 algorithms from CLRS-30 benchmark.]{Test performance (Mean  Standard Deviation) of the
         Neural PQs on out-of-distribution test data for all 30 algorithms from CLRS-30,
         run with 3 different seeds.}
\label{table:all-algs-best-large}
\end{adjustwidth}
\end{table}
\setlength{\tabcolsep}{6pt}


\setlength{\tabcolsep}{4pt}
\begin{table}[tbhp]
\begin{adjustwidth}{-2cm}{-2cm}
\centering
\captionsetup{margin=2cm}
\small
\begin{tabular}{lccccc}
    \hline
    \bfseries Algorithm & \bfseries Baseline & \bfseries NPQ-P-SA & \bfseries NPQ-P-SV & \bfseries NPQ & \bfseries NPQ \\
    \hline
    Activity Selector & \emph{T} & \emph{T} & L & L & \emph{T} \\
    Articulation Points & \emph{T} & \emph{T} & \emph{T} & \emph{T} & \emph{T} \\
    Bellman Ford & L & \emph{T} & L & L & \emph{T} \\
    BFS & L & \emph{T} & \emph{T} & \emph{T} & L \\
    Binary Search & \emph{T} & L & \emph{T} & L & \emph{T} \\
    Bridges & \emph{T} & \emph{T} & L & \emph{T} & L \\
    Bubble Sort & \emph{T} & L & L & \emph{T} & L \\
    DAG Shortest Paths & \emph{T} & L & L & L & \emph{T} \\
    DFS & L & \emph{T} & \emph{T} & L & \emph{T} \\
    Dijkstra & L & \emph{T} & \emph{T} & \emph{T} & L \\
    Find Max Subarray & \emph{T} & \emph{T} & L & L & \emph{T} \\
    Floyd-Warshall & L & L & L & L & \textbf{W} \\
    Graham Scan & \textbf{W} & L & L & L & L \\
    Heapsort & \emph{T} & \emph{T} & \emph{T} & \emph{T} & \emph{T} \\
    Insertion Sort & \emph{T} & \emph{T} & \emph{T} & \emph{T} & \emph{T} \\
    Jarvis March & \emph{T} & L & L & L & \emph{T} \\
    KMP Matcher & \emph{T} & L & \emph{T} & L & L \\
    LCS Length & L & \emph{T} & L & L & \emph{T} \\
    Matrix Chain Order & L & L & L & L & \textbf{W} \\
    Minimum & L & L & L & \textbf{W} & L \\
    MST-Kruskal & L & L & L & L & \textbf{W} \\
    MST-Prim & L & \emph{T} & L & L & \emph{T} \\
    Na{\"i}ve String Matcher & \emph{T} & \emph{T} & L & \emph{T} & \emph{T} \\
    Optimal BST & L & L & L & L & \textbf{W} \\
    Quickselect & \emph{T} & \emph{T} & \emph{T} & \emph{T} & \emph{T} \\
    Quicksort & \emph{T} & \emph{T} & \emph{T} & \emph{T} & \emph{T} \\
    Segments Intersect & \emph{T} & \emph{T} & \emph{T} & \emph{T} & \emph{T} \\
    SCC & \emph{T} & \emph{T} & L & \emph{T} & L \\
    Task Scheduling & \emph{T} & \emph{T} & \emph{T} & \emph{T} & L \\
    Topological Sort & L & \emph{T} & \emph{T} & L & L \\
    \hline
    Overall Counts &  &  &  &  &  \\
    \hline
\end{tabular}
\caption[Win/Tie/Loss counts of the Neural PQs and Baseline on all algorithms from the CLRS-30 dataset]{Win/Tie/Loss counts of the Neural PQs and Baseline on out-of-distribution
         test data for all 30 algorithms from CLRS-30, run with 3 different seeds.}
\label{table:all-algs-best-large-win-tie-loss}
\end{adjustwidth}
\end{table}
\setlength{\tabcolsep}{6pt}

\subsection{Long-Range Reasoning}
Table \ref{table:lrgb} shows the evaluation results for the baseline and Neural PQs on the Peptides-Struct dataset from Long-Range Graph Benchmark
\citep{Dwivedi-Long-Range-Benchmark} for different processors, as detailed in Section \ref{sec:eval-lrgb}.

\setlength{\tabcolsep}{4pt}
\begin{table}[tbhp]
\begin{adjustwidth}{-2cm}{-2cm}
\centering
\captionsetup{margin=2cm}
\small
\begin{tabular}{lccccc}
    \hline
    \bfseries Processor & \bfseries Baseline & \bfseries NPQ-P & \bfseries NPQ-P-SV & \bfseries NPQ & \bfseries NPQ \\
    \hline
    GATv2 &  &  &  &  &  \\
    GCN &  &  &  &  &  \\
    GINE &  &  &  &  &  \\
    \hline
\end{tabular}
\caption[Evaluation results on Peptides-Struct data from the Long Range Graph Benchmark]{Test MAE (Mean  Standard Deviation) of different Neural PQs with
         different base processors on Peptides-struct dataset,
         run with 3 different seeds. \textbf{Lower} the test MAE, \textbf{better} is the performance.}
\label{table:lrgb}
\end{adjustwidth}
\end{table}
\setlength{\tabcolsep}{6pt}




\end{document}
