

\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\iffalse \newcommand{\holger}[1]{\noindent}
    \newcommand{\xianzhong}[1]{\noindent}
\else
    \newcommand{\holger}[1]{[{\bf \color{orange} HC: #1}]}
    \newcommand{\xianzhong}[1]{[{\bf \color{magenta} XL: #1}]}
\fi

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{natbib}
\usepackage{hyperref}
\usepackage{url}
\usepackage{svg}
\usepackage{multirow}

\title{Offline Tracking with Object Permanence}

\author{Xianzhong Liu, Holger Caesar\\ Intelligent Vehicles Lab\\
TU Delft\\
\texttt{H.Caesar@tudelft.nl} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

 \iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
To reduce the expensive labor cost for manual labeling autonomous driving datasets, an alternative is to automatically label the datasets using an offline perception system. However, objects might be temporally occluded. Such occlusion scenarios in the datasets are common yet underexplored in offline auto labeling. In this work, we propose an offline tracking model that focuses on occluded object tracks. It leverages the concept of object permanence which means objects continue to exist even if they are not observed anymore. The model contains three parts: a standard online tracker, a re-identification (Re-ID) module that associates tracklets before and after occlusion, and a track completion module that completes the fragmented tracks. The Re-ID module and the track completion module use the vectorized map as one of the inputs to refine the tracking results with occlusion. The model can effectively recover the occluded object trajectories. It achieves state-of-the-art performance in 3D multi-object tracking by significantly improving the original online tracking result, showing its potential to be applied in offline auto labeling as a useful plugin to improve tracking by recovering occlusions. 
\end{abstract}

\vspace{-4mm}

\section{Introduction}
Supervised deep learning-based models have achieved good performance in autonomous driving. However, it usually requires a huge amount of labeled data with high quality to train and tune such data-hungry models. The traditional approach is manual labeling, where all the labels are provided by human annotators. A more effective way is to auto label datasets, where labels can be automatically provided by a trained perception system. Waymo first proposed to auto label data offline to improve the quality of the generated labels~\cite{offboard_labeling}. In online tracking, the location of an object is inferred only from past and present sensor data. Offline multi-object tracking (MOT) is acausal and the position of an object can be inferred from past, present, and future sensor data. A consistent estimate of the scene can thus be optimized globally using the data not limited to only a short moment in the past. Based on global information,~\cite{Auto4d,offboard_labeling, CTRL, Detzero} have developed offline auto labeling pipelines that generate accurate object trajectories in 3D space from LiDAR point cloud sequence data.

 Previous offline labeling methods mainly focused on using the observation sequence to refine the unoccluded trajectories (e.g. location, orientation, and size of the bounding boxes). However, object trajectories and point cloud sequences are sometimes partially missing because of the temporary occlusions on the objects. Severely occluded objects are likely to be missed by detectors and thus cannot be continuously tracked. Furthermore, the trackers in~\cite{Auto4d,offboard_labeling} adopt the death memory strategy to terminate the unmatched tracks, which means a track not matched with any detections for a predefined number of frames will be terminated. Therefore, the identity of the same object may switch after reappearance from occlusion. \cite{Immortaltrackers} proposed to capture object permanence by not terminating any unmatched tracks. It has been applied in~\cite{CTRL, Detzero} for its effectiveness in reducing identity switches (IDS). However, their data association is simply done by using the constant-velocity prediction from a Kalman filter~\cite{kalman_filter} which cannot capture the nonlinear motion under long occlusions. Thus, tracking under occlusion remains a challenge in point cloud-based offline auto labeling. 
Motion prediction models~\cite{laneGCN, gao2020vectornet}, on the other hand, can produce accurate vehicle trajectories over longer horizons based on a semantic map. The lanes on the map serve as a strong prior knowledge to guide the motion of target vehicles and thus can be used to estimate motion under occlusion.


In this work, we propose an offline tracking model containing three modules to tackle the aforementioned problems. The first module is an off-the-shelf online tracker which produces the initial tracking result. Leveraging the idea of object permanence, the second Re-ID module tries to reassociate the terminated tracklets with the possible future candidate tracklets. 
Based on the association result, the last track completion module completes the missing trajectories. Taking the insight from the motion prediction task, we have extracted map information as a prior to enhance the association and track completion modules. Unlike common motion prediction methods with a prediction horizon predefined during training~\cite{PGP, HOME}, our model can use a flexible prediction horizon at inference time to decode the predicted poses in the track completion module, to deal with variable occlusion durations. 
We train and evaluate our modules on the nuScenes dataset~\cite{nuscenes}. Previous offline auto labeling methods are mainly optimized for the precision of the visible bounding boxes, which is a metric for the object detection task. Their performances on MOT metrics are not fully optimized or evaluated. On the other hand, our work focuses on the optimization and evaluation for the MOT metrics (e.g. AMOTA). We observe significant improvements after performing Re-ID over the occlusion scenarios. We then demonstrate the ability of the track completion module to recover the occluded tracks.
To further evaluate our model on a larger amount of occlusion cases, we also introduce artificial pseudo-occlusions by masking the ground truth (GT) tracks.
We finally evaluated the two modules separately on the pseudo-occlusions. 

The contribution of this paper can be summarized as follows:
\vspace{-2mm}
\begin{itemize}
\item We propose an offline tracking model for Re-ID (Sect.~\ref{sec: Re-ID}) and occluded track recovery (Sect.~\ref{sec: track completion}) to track occluded vehicles.
\item We innovatively apply prediction-based methods in offline tracking and use the lane map as a prior to improve the Re-ID (Sect.~\ref{sec: map affinity}) and track completion (Sect.~\ref{sec: track completion}). To deal with variable occlusion duration, we decode trajectories from variable time queries (Sect.~\ref{sec: track completion}).
\item We optimize our method for the MOT task and demonstrate the improvements relative to the original online tracking results on the nuScenes dataset (Sect.~\ref{sec: online tracking eval}), showing the potential of our model to be applied in offline auto labeling as a useful plugin to improve tracking and recover occlusions.
\end{itemize}

\vspace{-2mm}
\section{Related work}
\label{related_work}
\vspace{-1mm}
\subsection{Tracking under occlusion}
\vspace{-1mm}
Estimating a target's motion under occlusion is one of the main challenges in the tracking task.
A common method adopts a constant velocity~\cite{basline_with_AMOTA, Immortaltrackers} or constant acceleration~\cite{CAMOMOT} transition model and uses the Kalman filter~\cite{kalman_filter} to update the tracks when detections are missed due to occlusion. \cite{Centerpoint} directly estimates displacement offset (or velocity) between frames. The unmatched track is updated with a constant velocity model. PermaTrack~\cite{Object_permanence} supervises the occluded motion with pseudo-labels that keep constant velocity. Those methods heavily rely on motion heuristics. Such heuristics work well for short occlusion since objects are likely to keep moving forward with little velocity or acceleration variation. However, they cannot capture the nonlinear motion under long occlusion.
Due to the large deviation error accumulated over time, the prediction of the unmatched track would not be associated with the correct detection. Most tracking models that have finite death memory~\cite{Centerpoint,basline_with_AMOTA} would terminate such unmatched tracks, even if the object is detected again later. Such premature termination would lead to IDS. Immortal Tracker~\cite{Immortaltrackers} effectively reduces such IDS by extending the life of unmatched tracks forever. \cite{batch3dmot} uses a multi-modal Graph Neural Network (GNN) to produce robust association offline. 
Our model does not rely on any handcrafted motion heuristics to predict future motion or to associate.
Given any pair of future and history tracklets, the model outputs the learned affinity scores directly.





\vspace{-2mm}
\subsection{Offline auto labeling from lidar point clouds}
\vspace{-1mm}
Manually annotating lidar point cloud data takes much effort due to the sparsity of the data and the temporal correlation of the sequence. Several works have attempted to tackle this problem by automatically labeling the dataset. Auto4D and 3DAL~\cite{Auto4d,offboard_labeling} fully automate annotation by taking initial tracklets generated from an online tracker (i.e. AB3DMOT~\cite{basline_with_AMOTA}). Then, the point cloud features are extracted globally on the temporal horizon to refine the initial tracks. Given full observation, such refinement utilizes future information to perform global optimization offline. In Auto4d~\cite{Auto4d}, two rounds of refinement are performed to sequentially refine the box size and trajectory. Point cloud features and motion features are used to jointly refine the trajectory. In 3DAL~\cite{offboard_labeling}, static objects and dynamic objects are treated separately. Segmented foreground point cloud feature sequences and bounding box sequence features are used together to refine the box sequences. Though these methods can produce accurate bounding boxes, their data association is still simply done online. The global information available in the offline setting has yet to be utilized in tracking. Such methods are likely to produce IDS and inconsistent tracks under occlusion. CTRL~\cite{CTRL} uses the Immortal Tracker~\cite{Immortaltrackers} to associate fragmented tracks due to long occlusion or missing detection. Then it backtraces the tracks by extrapolation to track the missing detections. Detzero~\cite{Detzero} also uses the Immortal Tracker to perform bidirectional tracking on the time horizon and then ensemble the results with forward and reverse order tracking fusion. As a result, CTRL and Detzero have achieved better results which surpass the human annotation. However, their data association still relies on a Kalman filter with the constant-velocity model.
\vspace{-1mm}
\subsection{Map-based prediction}
\vspace{-1mm}
Semantic maps are widely used as an input in motion prediction methods. As it contains accurate scene context from the surrounding environment, including road lanes, crosswalks, etc. With such information, models can predict how target vehicles navigate in the environment over a long horizon. HOME~\cite{ HOME} rasterizes the map as multiple vector layers with distinct RGB values representing different map elements. Alternatively, VectorNet~\cite{gao2020vectornet} proposes the vectorized approach which represents curves as vectorized polylines. It uses a subgraph network to encode each polyline as a node in a fully connected global interaction graph. It achieves better performance over the CNN baseline and reduces the model size. To capture higher resolution, LaneGCN~\cite{laneGCN} uses polyline segments as map nodes. It models a sparsely connected graph following the map topology. Similarly, PGP~\cite{PGP} constrains the connected edges such that any traversed path through the graph corresponds to a legal route that a vehicle can take. 


\vspace{-1mm}
\section{Method}
\label{headings}
\vspace{-1mm}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Overview.pdf}
    \caption{A brief overview of the offline tracking model. 
    \textbf{(a) Online tracking result}: 
    Each tracklet is represented by a different color (the history tracklet is red). 
    \textbf{(b) Offline Re-ID }: 
    The matched pair of tracklets are red. The unmatched ones are black. 
    \textbf{(c) Recovered trajectory }.
    }
    \label{fig: pipeline}
\end{figure*}
As shown in Fig.~\ref{fig: pipeline}, our model initially takes the detections from a detector as input. Following~\cite{Immortaltrackers}, we also perform non-maxima suppression (NMS) on the detections. We only perform intra-class NMS with an IoU threshold of 0.1. Then it uses an off-the-shelf online tracker to associate detections and generate initial tracklets. Next, the Re-ID module tries to associate the possible future tracklets with the terminated history tracklet. If a pair of tracklets are matched, the track completion module interpolates the gap between them by predicting the location and orientation of the missing box at each timestamp. Both modules extract motion information and lane map information to produce accurate results. The model finally outputs the track with refined new identities and completes the missing segments within the tracks. 


\vspace{-1mm}
\subsection{Re-ID}
\label{sec: Re-ID}
\vspace{-1mm}
Based on the concept of object permanence, we assume any tracklets could potentially be terminated prematurely due to occlusion, and any other tracklets that appeared after the termination could be matched. The Re-ID model has two branches, the motion and the map branch. The motion branch takes tracklet motion as input and outputs affinity scores based on vehicle dynamics. The map branch takes motion and the lane map as inputs and outputs affinity scores based on map connectivity. Thus, for each target history tracklet, the model computes motion affinity and map-based affinity scores for it with all the possible future tracklets. The problem can thus be treated as a \textbf{binary classification task} for each single matching pair. The final score is a weighted sum of the motion and map affinity scores. We perform bipartite matching by greedily associating history tracklets with their future candidate tracklets based on the final scores. The matching pair which has a tracking score lower than a threshold is excluded from the association. The detailed formulation and input features are introduced in Appendix~\ref{sec: Re-ID formulation}.
\vspace{-1mm}
\subsubsection{Motion affinity}
\label{sec: motion affinity}
\vspace{-1mm}

\begin{figure}
    \vspace{-4mm}
    \centering
    \includegraphics[width=0.5\textwidth]{motion_branch.pdf}
    \caption{The network structure of motion affinity branch. The orange branch represents the history tracklet encoder, whereas the blue branch represents the future tracklets encoder. Three possible future candidates correspond to the three outputted motion affinity scores.}
    \label{fig: motion branch}
\end{figure}

To compute the motion affinity, we transform the trajectories from the global frame to the local agent frame, where the origin aligns with the pose of the last observation of the history tracklet. The motion feature of each tracklet contains the location, orientation, time and velocity information at every time step. Given one history tracklet and all its possible future matching candidate tracklets, the model takes the motion features as input and outputs affinity scores in the range of  for all future tracklet candidates. As shown in Fig.~\ref{fig: motion branch}, the motion affinity branch has a simple structure that only contains MLP and GRU. The GRU encoder encodes the history motion and outputs the last hidden state as history encoding. The history encoding is then used as the initial hidden state for the UGRU~\cite{urnn} to encode future motions so that the future encoder is also aware of the history information when encoding the future motion. Unlike the normal U-RNN used in motion prediction for encoding history motion features, our UGRU first does a forward pass and then does a backward pass in reverse order to encode the future motion features. The future motion encodings are concatenated with the history encoding to decode motion affinity scores. 
\vspace{-1mm}
\subsubsection{Map-based affinity}
\vspace{-1mm}
\label{sec: map affinity}
To improve the association accuracy, we further extract information from the lane graph as another branch. The lane centerlines provide information about the direction of traffic flow and the possible path for the vehicles to follow, which can be utilized for occluded motion estimation over a long horizon. We divide the lanes into sections with a maximum length of 20m. Each lane section is encoded as a node in the graph. For each node, the corresponding lane section is discretized to several poses with a resolution of 1m. Each lane pose feature contains the location, orientation, and semantic information of the lane. 

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{map_branch.pdf}
\caption{A brief overview of the map branch. The branch starts with three parallel encoders which encode the future tracklet, lanes and history tracklet respectively. The model then propagates information between tracklets and the lane map by performing attention. Finally, map-based affinity scores are decoded from the tracklet features.}
\label{fig: map branch}
\end{figure*}

As shown in Fig.~\ref{fig: map branch}, the map branch starts by encoding the history and the future tracklets, using the same encoder previously introduced in the motion branch (Sect.~\ref{sec: motion affinity}). The lane pose features are also encoded in parallel using a single-layer MLP. The model then performs agent-to-lane attention to fuse the information from the tracklets to the lanes. The model selectively performs attention such that the tracklet encodings are only aggregated to the surrounding lanes nearby. We adopted the spatial attention layer in~\cite{laneGCN} to aggregate the feature from tracklets to lane poses.
Then, the model aggregates the feature from each sub-node lane pose to get the lane node features. We apply a 2-layer-GRU to aggregate sub-node features. The first GRU layer is bidirectional so that the information can flow in both directions. 
After getting the lane node features carrying the information of the tracklets, the model performs gloabl attention~\cite{Transformer_attention} to propagate information globally in the graph.
Next, the model aggregates the updated lane features back to tracklets by applying a lane-to-agent attention layer~\cite{laneGCN}. Finally, the future and history tracklet encodings updated with global map information are concatenated together. An MLP decoder is applied to the concatenated features to decode map affinity scores.

\vspace{-1mm}
\subsubsection{Association}
\vspace{-1mm}
To match  history tracklets with  future tracklets, we have an affinity score matrices  and  from the two branches, we first filter the matching pairs whose affinity scores are lower than a threshold, then we get the final matching score matrix:

where  is a scalar weight. During experiments, we simply set  to 0.5.
  is the weighted sum of the two affinity matrices. Finally, we solve the bipartite problem by maximizing the overall matching scores. Due to the sparsity of , we simply perform a greedy matching.
\vspace{-1mm}
\subsection{Track completion}
\vspace{-1mm}
\label{sec: track completion}
With the upstream Re-ID results, we have associated multiple tracklet pairs together. Therefore, we now have multiple fragmented tracks which have a missing segment in the middle. The track completion model interpolates the gaps within the tracks. Based on the history and future motion, the model first generates an initial trajectory in between. Then, a refinement head is applied to refine the initial trajectory based on motion and map features. The trajectory completion model learns a \textbf{regression task} as in most motion prediction methods~\cite{laneGCN,PGP}. The detailed formulation is in Appendix~\ref{sec: Track completion formulation}.

\begin{figure*}[htb]
\centering
\includegraphics[width=0.9\textwidth]{track_completion.pdf}
\caption{A brief overview of the track completion model. The model stacks several sub-modules together to sequentially aggregate features and refine the results.  }
\label{fig: track completion}

\end{figure*}

We first transform the future and history tracklets from the global frame to the local agent frame. However, instead of using the last pose of the history tracklet as the origin, we set the origin as the midpoint of the line connecting the two endpoints of the missing segment to reduce the over-dependency on the history tracklet. 
Most prediction models generally have a fixed prediction horizon~\cite{laneGCN,PGP}. But the occlusion horizon in reality is never fixed. We thus use time features as input queries to decode poses at all the target timesteps. Specifically, each time query feature at time  is given as , where  is the total prediction horizon. So that we can have a variable number of time queries corresponding to a variable prediction horizon. 

The overall structure of the track completion model is shown in  Fig.~\ref{fig: track completion}. It encodes time features as queries and iteratively aggregates context information to the query features to refine the generated tracks. The motion encoder first encodes the history and future motion features using two parallel encoders. The UGRU in each encoder takes the final hidden state from the other, hence the information can propagate through the temporal gap. The time features are encoded with a single-layer MLP as initial queries. A cross-attention layer then aggregates the motion information to the time queries. We also add a skip connection to concatenate the attention output with the motion encoding and time features. To improve the result, we added a refinement phase to the initial trajectory. The model encodes lane features and performs lane-to-agent attention~\cite{laneGCN} to fuse the extracted lane information with the generated trajectory information. The trajectory encodings carrying the lane information then go through the final trajectory refinement head which uses a self-attention layer with a skip connection to propagate information within the track. The following bidirectional GRU implicitly smooths the track by aggregating features from each pose to its adjacent poeses. Finally, an MLP decodes the refined trajectory.

\vspace{-1mm}
\subsection{Loss function}
\vspace{-1mm}
The Re-ID model and the track completion model are trained separately. Since the losses are commonly used, we show them in  Appendix~\ref{sec: loss formulas}.

For the Re-ID model, we adopt the focal loss in~\cite{focal_loss} to train the binary classification task. In each sample, we only have one GT future tracklet as a positive match, and there are usually multiple negative pairs. We thus use the focal loss for its capability to deal with class imbalance. 

For the track completion model, we use the smooth  loss as in~\cite{DCMS,laneGCN} to train the coordinates regression head in both the initial trajectory decoder and the final refinement blocks. For yaw angle regression, we first adjust all the GT yaw angles such that their absolute differences with the predicted yaw angles are less than . Then, we apply  loss on the regressed yaw angles. The final regression loss  is a weighted sum of the coordinates regression loss  and the yaw angle loss .The weights are  and .

\vspace{-1mm}
\section{Experimental setup}
\vspace{-1mm}

\subsection{Training setup}
\vspace{-1mm}
\textbf{Training data:}
While~\cite{offboard_labeling,Auto4d} train their methods using online tracker outputs, we train only using the GT, but additionally inject pseudo-occlusions by masking a partial segment of each GT track.
This gives two advantages: 
\textbf{1) More accurate inputs}:
Unlike the imperfect trajectories generated from an online tracker, the trajectories from the available autonomous driving datasets are accurate and contain little noise.
With proper data augmentation, our model can still deal with imperfect data when inferencing with the trajectories produced from an online tracker.
\textbf{2) More training samples}: Since we have longer intact tracks as input, we can generate data with longer occlusions.

\textbf{Pseudo-occlusion:} 
We randomize the length of the pseudo-occlusion for each track during the training of the Re-ID and track completion models. 
The pseudo-occlusion duration is selected such that the history tracklet and all the future tracklets have at least one observation.

\textbf{Data augmentation:} 
To deal with the possible imperfections from the online tracking result during inference, we augment the data during training by randomly rotating the local frame in each sample and adding random noise to the motion inputs. 

\subsection{Evaluation setup} 
\vspace{-1mm}
We train and evaluate our model on the nuScenes dataset~\cite{nuscenes}. 
The standard MOT evaluation on nuScenes could punish the fully occluded predictions since the evaluation code filters out all the GT boxes with no points inside and then linearly interpolates the GT tracks. Therefore, some of the bounding boxes recovered from occlusion could be regarded as false positives (FP) as the GT boxes they are supposed to be matched with are linearly interpolated. We thus train and validate our model on the training split of the nuScenes dataset. The evaluation is separately done on the validation split and the test split so that we can not only benchmark our method in a standard way but also adjust the evaluation procedure for occlusion cases. 
In each of the experiments below, we use one of the following three evaluation setups as indicated.

\textbf{Official nuScenes setup}: Following the official nuScenes MOT evaluation protocol, we filter out all the empty GT boxes without any lidar or radar points inside. We thus focus on the visible GT boxes. Therefore, this evaluation can also be applied to the test split on the official server.

\textbf{All-boxes setup}: To get a comprehensive evaluation result, we do not filter any GT boxes. Therefore, the evaluation takes both the visible and the occluded boxes into account.

\textbf{Pseudo-occlusion setup}: To generate more occlusions with longer duration for evaluation, we take the GT vehicle tracks from the nuScenes prediction validation set and mask a partial segment within each track. Unlike the previous setups where the model takes the online tracking result as input, the model takes the unoccluded GT tracklets as inputs under this setup. 

\section{Experiments}
\vspace{-1mm}
We first evaluate our offline tracking model on the imperfect data generated from the online tracking result. We show the relative improvements it brings to the original online tracking result on MOT metrics. In this setting, the experiments evaluate the potential of the offline tracking model to be applied in offline auto labeling. We use CenterPoint~\cite{Centerpoint} as the initial off-the-shelf detector and tracker. Next, we evaluate our offline tracking model using human-annotated data. We mask the vehicle tracks to create a large amount of pseudo-occlusion cases for evaluation. The human-annotated data have little input noise, which excludes the imperfections from the online tracker so the evaluation focuses solely on the offline tracking model we proposed. Furthermore, the real occlusion cases only take a relatively small portion of the dataset compared to the unoccluded cases and the occlusion duration is typically short. Unlike the real occlusions in the dataset, the generated pseudo-occlusions are abundant and typically have longer durations. We focus our method only on the vehicle classes. The implementation details are in Appendix~\ref{sec: implementation details}.
\vspace{-1mm}
\subsection{Quantitative evaluation with online tracking results}
\label{sec: online tracking eval}
\vspace{-1mm}
\subsubsection{Re-ID model evaluation}
\label{sec: Re-ID evaluation online}
\vspace{-1mm}
In the sole evaluation of the Re-ID model, we adopt the \textbf{official nuScenes setup} which follows the standard evaluation protocol of nuScenes and filters out the empty GT boxes for evaluation. We only use the Re-ID model to reassociate the tracklets before and after occlusions, and we do not use the track completion model to interpolate the trajectories between the gaps. Instead, the nuScenes evaluation code automatically does a simple linear interpolation to fill all the gaps on both the GT tracks and the predicted tracks. For comparison, we select the lidar-based SOTA methods that also use CenterPoint~\cite{Centerpoint} detections from the nuScenes leaderboard. 

\begin{table*}[t]\vspace{-6mm}
    \centering
    \begin{tabular}{c|c|cccc}
    \textbf{Method}              & \textbf{Overall} & \textbf{Car}  & \textbf{Bus}  & \textbf{Truck} & \textbf{Trailer} \\ \hline
    CenterPoint~\cite{Centerpoint}                  & 69.8            & 82.9          & 71.1          & 59.9           & 65.1             \\
    SimpleTrack~\cite{SimpleTrack}                   & 70.0             & 82.3          & 71.5          & 58.7           & 67.3            \\
    UVTR~\cite{UVTR}                   & 70.1             & 83.3          & 67.2          & 58.4           & \textbf{71.6}             \\
    Immortal Tracker~\cite{Immortaltrackers}                   & 70.5             & 83.3          & 71.6          & 59.6           & 67.5             \\
    NEBP~\cite{NEBP}                         & 70.8           & 83.5          & 70.8          & 59.8           & 69               \\
    3DMOTFormer++~\cite{3DMOTFormer}             & 72.3             & 82.1          & 74.9          & 62.6           & 69.6             \\
    ShaSTA~\cite{ShaSTA}                       & 73.1           & 83.8          & 73.3          & \textbf{65}    & 70.4
    \\ \hline
    Offline Re-ID (Motion + Map) & \textbf{73.4}  & \textbf{84.2} & \textbf{75.1} & 64.1           & 70.3            
    \end{tabular}
    \caption{Comparison of AMOTA scores over the SOTA methods using CenterPoint~\cite{Centerpoint} detections on the nuScenes test split (official nuScenes setup). }
    \label{tab: Re-ID AMOTA test }
\end{table*}

\begin{table}
\centering
\begin{tabular}{c|cccc}
\textbf{Method}       & \textbf{AMOTP \text{/ m}}          & \textbf{Recall}        & \textbf{MOTA}          & \textbf{IDS}          \\ \hline
CenterPoint~\cite{Centerpoint}  & 0.596          & 73.5          & 59.4          & 340          \\
SimpleTrack~\cite{SimpleTrack}                   & 0.582             & 73.7          & 58.6                    & 259            \\
UVTR~\cite{UVTR}                   & 0.636             & \textbf{74.6}         & 59.3                  & 381             \\
Immortal Tracker~\cite{Immortaltrackers}    & 0.609 & 74.5          & 59.9 & 155 \\ 
NEBP~\cite{NEBP}                         & 0.598           & 74.1          & \textbf{61.9}                    & \textbf{93}               \\
3DMOTFormer++~\cite{3DMOTFormer}             & 0.542             & 73.0          & 58.6                    & 210            \\
ShaSTA~\cite{ShaSTA}                       & 0.559           & 74.3          & 61.2          & 185    \\ \hline

Offline Re-ID (Motion + Map) & \textbf{0.532}          & 74.2          & 61.3 & 204         
\end{tabular}
\caption{Comparison of MOT metrics over the SOTA methods using CenterPoint~\cite{Centerpoint} detections on the nuScenes test split (official nuScenes setup).}
\label{tab: Re-ID evaluation test }
\end{table}

From the results in Tab.~\ref{tab: Re-ID AMOTA test } and~\ref{tab: Re-ID evaluation test }, our model effectively improves the original online tracking result from CenterPoint~\cite{Centerpoint}. Please also note that the Re-ID model only refines the limited occlusion cases. Therefore, the improvement brought by the Re-ID model is upper bound by the number of occlusions. Yet the result after refinement already outperforms the other SOTA methods on AMOTA and AMOTP, indicating the Re-ID model can accurately reassociate the tracklets before and after occlusions. We thus have demonstrated the potential of our Re-ID model to improve the offline tracking result during auto labeling, especially for occlusion-rich scenes. We show the effects of the map and the motion branches separately and the general improvements brought by our Re-ID model to other SOTA online trackers on the validation split in Appendix ~\ref{sec: Re-ID results val}.




\vspace{-1mm}
\subsubsection{Track completion model evaluation}


\begin{table}
\centering
\label{tab: track completion results online}
\begin{tabular}{c|cc|cc|cc|cc}
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Track Completion}\\ { }\end{tabular}} & \multicolumn{2}{c|}{\textbf{AMOTA}} & \multicolumn{2}{c|}{\textbf{AMOTP \text{/ m}}} & \multicolumn{2}{c|}{\textbf{IDS}} & \multicolumn{2}{c}{\textbf{Recall}} \\ \cline{2-9} 
                                                                             & w/o     & w                & w/o     & w                & w/o    & w               & w/o     & w                \\ \hline
CenterPoint~\cite{Centerpoint}                                                                  & 70.2    & \textbf{72.4}    & 0.634   & \textbf{0.615}   & 254    & \textbf{183}    & 73.7    & \textbf{74.5}    \\
SimpleTrack~\cite{SimpleTrack}                                                              & 70.0    & \textbf{71.0}    & 0.668   & \textbf{0.629}   & 210    & \textbf{170}    & 72.5    & \textbf{72.9}    \\
VoxelNet~\cite{Voxelnet}                                                                   & 69.6    & \textbf{70.6}    & 0.710   & \textbf{0.665}   & 308    & \textbf{230}    & 72.8    & \textbf{72.9}    \\
ShaSTA~\cite{ShaSTA}                                                                   & 72.0    & \textbf{72.6}    & 0.612   & \textbf{0.593}   & 203    & \textbf{174}    & 73.0    & \textbf{75.3}   
\end{tabular}
\caption{Track completion evaluation on the nuScenes validation split (All-boxes setup). w: with Re-ID and track completion. w/o: without Re-ID and track completion. }
\label{tab: Track completion over SOTA }
\end{table}
Based on the Re-ID results, the track completion model recovers the missing trajectories between all the matched tracklet pairs.
Since we want to evaluate the ability of our model to recover the occluded trajectories, we adopt the \textbf{all-boxes setup} and thus do not filter out any GT bounding boxes. A detailed description of the experimental setup is in Appendix~\ref{sec: Track completion evaluation online setup}. The results are shown in Tab.~\ref{tab: Track completion over SOTA }. We have shown the improvements our model brings to multiple SOTA online trackers. We have only tuned our model using the results from CenterPoint tracker~\cite{Centerpoint} and then applied our model to the results from other trackers. The improvements have demonstrated that our model can be used as a general plugin to recover the imperfections caused by occlusions during offline autolabeling. Our model is compatible with any initial tracker, thus it can be easily integrated into the existing offline autolabeling frameworks~\cite{offboard_labeling,Auto4d}. We have also shown the ability of our track completion model to recover the occluded objects in Appendix~\ref{sec: occluded boxes recovery}. 


\vspace{-1mm}
\subsection{Quantitative evaluation with pseudo-occlusions}
\label{sec: pseudo-occlusion eval}
\vspace{-1mm}
\subsubsection{Re-ID model evaluation}
\vspace{-1mm}

\begin{table}
    \centering
    \begin{tabular}{c|c c c c }
     \textbf{Method}  & CVM & Motion & Map & Motion + Map \\
    \hline \textbf{Association accuracy } & 58.7\% & 90.1\% & 89.6\% & \textbf{90.3\%} \\
    \end{tabular}
    \caption{Re-ID evaluation on the nuScenes validation split (Pseudo-occlusion setup). 
    }
    \label{tab: pseudo occlusion Re-ID}
\end{table}
We evaluate the Re-ID model on pseudo-occlusions following the setup in Appendix~\ref{sec: Re-ID evaluation pseudo-occ setup}.  We also include a constant velocity model (CVM) associator as our baseline. The CVM associator takes the last observable position and velocity as inputs and then predicts future trajectories with a constant velocity. The future tracklet which has the shortest distance to the constant velocity prediction will be matched. The association accuracies are listed in Tab.~\ref{tab: pseudo occlusion Re-ID}. 

Given inputs without noise, our model in all settings achieves high association accuracies over 89\% and outperforms the CVM associator by a large margin over . With the perfect motion input, the motion branch has a higher association accuracy over the map branch by . After the combination of the motion and the map branch, the accuracy increases by  compared to the result using only the motion branch. Hence, the two branches are complementary to each other.
\vspace{-1mm}
\subsubsection{Track completion model evaluation}
\vspace{-1mm}
To standardize the evaluation, for each evaluated sample track, we mask 6 seconds of its trajectory and take 2 seconds of history tracklet and future tracklet as inputs. We chose HOME~\cite{HOME} as a baseline and re-implemented it on nuScenes. HOME originally decodes the predicted trajectories from sampled endpoints. To make the comparison fair, we directly give the GT trajectory endpoints to the HOME, so that the model is also aware of the future. 


\begin{table}
    \centering
    \begin{tabular}{c|c c c}
    \textbf{Method} & \textbf{ADE \text{/ m}} & \textbf{Yaw error \text{/ deg}} & \textbf{MR} \\
    \hline HOME~\cite{HOME} & 0.814 & - &  \\
     Motion & 0.705 & 2.38 &  \\
     Motion + map &  &  &  \\
    \end{tabular}
    \caption{Track completion evaluation on the nuScenes validation split (Pseudo-occlusion setup).  The prediction horizon is 6s which is the same as the nuScenes prediction challenge.
    }
    \label{tab:pseudo-occ track_completion}
\end{table}
From the result, the performance is improved on every metric after using the map information for trajectory refinement, indicating the extracted map prior improves the trajectory recovery. Our model also outperforms HOME on both ADE and MR, demonstrating its potential to recover trajectories under long occlusions.  

\vspace{-1mm}
\subsection{Qualitative results}
\vspace{-1mm}
We have visualized several representative samples in the evaluation with the online tracking result and pseudo-occlusions. Due to the page limit, we show them in Appendix~\ref{sec: Qualitative results}.


\vspace{-1mm}
\section{Conclusion}
\vspace{-1mm}
While the previous point cloud-based offline auto labeling methods focused on generating accurate bounding boxes over visible objects, we have proposed a novel offline tracking method focusing on the occlusions on vehicles. The Re-ID module can effectively reduce IDS caused by premature termination under occlusion. Based on the Re-ID result, the track completion model recovers the occluded trajectories. Leveraging the idea from motion prediction, we innovatively extracted information from the lane map and used it as a prior to improve the performance of our models. We have demonstrated the ability of our model on the nuScenes validation split, using both the imperfect online tracking results and the handcrafted pseudo-occlusion data. The offline tracking model improves the original online tracking result and achieves SOTA performance, showing its potential to be applied in auto labeling autonomous driving datasets. It also achieves good performances on the handcrafted pseudo-occlusions and outperforms the baselines by large margins. Future work will aim to unify the Re-ID and track completion modules as an end-to-end model.



\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\clearpage
\newpage

\appendix
\section*{Appendix}
In this appendix, we first present the qualitative results~\ref{sec: Qualitative results}. We also show supplementary experimental results of the Re-ID model~\ref{sec: Re-ID results val} and track completion model~\ref{sec: occluded boxes recovery} on the validation split. Next, give the detailed formulation of the Re-ID and the track completion~\ref{sec: Formulation}. We also give the formulas and the hyper-parameters of the training loss functions~\ref{sec: loss formulas}. Then, we describe the implementation details~\ref{sec: implementation details}. Finally, we give a detailed explanation of the experimental setups~\ref{sec: experimental setup}. 

\section{Qualitative results}
\label{sec: Qualitative results}
\subsection{Joint evaluation with online tracking results}
\begin{figure*}[htb!]
\centering
\includegraphics[width=\textwidth]{figures/Visualization_online_result.png}
\caption{Qualitative results of the offline tracking model. In each sample, GT boxes are plotted as rectangles. Each GT track is represented by a unique color. To separate the output of the model from the GT tracks, we use arrows to represent the output poses from the model instead of rectangles. Red and blue arrows are the history and future tracklets matched by the offline Re-ID model. The recovered trajectories are plotted as purple arrows. The orange dotted lines are the lanes. In the background, the white area is drivable. The gray area in the right plot is the pedestrian crossing. The red cross is the average position of the ego vehicle during the occlusion. }
\label{fig: online_tracking joint}
\end{figure*}
As shown in Fig.~\ref{fig: online_tracking joint}, we applied our offline tracking model to improve the online tracking result by jointly performing Re-ID and track completion. Three samples in the three plots show different occlusion types. The left plot shows a long occlusion with a duration of 8s and a gap distance of 71m. The middle plot shows a relatively short occlusion with a duration of 3s and a gap distance of 25m. The right plot shows an occlusion that happened on a turning vehicle at a crossroad. The occlusion duration in the third sample is 3.5s and the gap distance is 13m. With the three samples, we show the ability of our offline tracking model to deal with long and short occlusions, as well as the non-linear motion under occlusion. Taking the online tracking results and the semantic map as inputs, our offline tracking model first correctly associated together the tracklets belonging to the same GT track. The associated future (blue) and history (red) tracklets in each plot are covered by a single GT track. Then the model recovered the occluded trajectories by interpolating the gaps between the associated tracklet pairs. The recovered trajectories (purple) also aligned well with the corresponding GT tracks.  

\subsection{Re-ID model evaluation on pseudo-occlusion}
\begin{figure*}[htb!]
\centering
\includegraphics[width=1.0\textwidth]{match_visualization_3_.png}
\caption{Qualitative results of the Re-ID model evaluated with pseudo-occlusion. The history tracklets are green. Future tracklets are colored according to their affinity scores. Higher scores are represented in red and lower scores are in blue. The thin dotted lines represent the lanes from the semantic map. The left figure shows the prediction from the motion branch. The middle figure shows the prediction from the map branch, and the right figure shows the GT. }
\label{fig:pseudo-occ Re-ID}
\end{figure*}
As shown in Fig.~\ref{fig:pseudo-occ Re-ID}, a representative case is selected to show the map information extracted from the lane graph can help correct the association. In the left sub-figure, the motion branch tends to assign a high affinity score (red) to the future tracklet candidate above. Though this association fits the motion pattern, it is a false match. After adding the map information, the affinity score of the false future candidate is suppressed (colors become bluer), since it is not on the same lane as the history tracklet. The affinity score of the true future tracklet increases in the map branch (colors become redder), showing that our offline tracking model can utilize the map prior to accurately associating tracklets.

\subsection{Track completion model evaluation on pseudo-occlusion}
\begin{figure*}[htb!]
\centering
\includegraphics[width=\textwidth]{figures/track_completion_visualization_short.png}
\caption{Qualitative results of the track completion model evaluated with pseudo-occlusion. In the plot, 2 pairs of prediction results are shown. In each pair, the left image shows the decoded trajectory using only the motion feature, whereas the right image shows the trajectory after refinement. The history tracklets, future tracklets, predicted trajectories and GT trajectories correspond to the green, red, blue, and pink arrows respectively. The lanes are in orange.}
\label{fig:pseudo-occ track_completion} 
\end{figure*}
As shown in Fig.~\ref{fig:pseudo-occ track_completion}, both the trajectories before and after refinement can predict the yaw angle very well. However, the initial trajectories before refinement are not smooth sometimes. They constantly deviate from the lane and GT trajectory. After refinement, the trajectories are smoother and tend to follow the lane. Less deviation from the GT trajectory is observed. Hence it accords with our previous quantitative evaluation, showing our track completion model is able to leverage the map prior to generating accurate trajectories.


\section{Quantitative Re-ID results on validation split}
\label{sec: Re-ID results val}
To ablate the effect of motion and map branches, we also separately evaluated our model on the nuScenes validation split. The results are shown in Tab.~\ref{tab: Re-ID AMOTA online } and~\ref{tab: Re-ID evaluation online}. The performance of our model generally improves after aggregating with map information. Our model outperforms the baselines on AMOTA, including the offline method 3D-PM-CL-CP which uses camera and lidar data. Please also note that we did not tune our model on the validation split. Also, the refinement is performed only in limited scenarios where occlusion happens. Though the margin between our method and the Immortal Tracker is not significant, we argue this is because the overall AMOTA is a simple macro average over all classes. If the imbalanced box numbers in different classes are taken into account, our model will be more advantageous, since our model can predict the abundant car class (58317 boxes) well, whereas the Immortal Tracker can predict the bus class (2112 boxes) well. Compared with the input result from the CenterPoint tracker~\cite{Centerpoint}, our model reduces IDS by 45\%  and increases AMOTA by 2\% on the vehicle tracks after refinement.


\begin{table*}[htb]
    \centering
    \begin{tabular}{c|c|cccc}
    \textbf{Method}            & \textbf{Overall}       & \textbf{Car}           & \textbf{Bus}           & \textbf{Truck}         & \textbf{Trailer}       \\ \hline
     CenterPoint~\cite{Centerpoint} & 70.9          & 84.3          & 83.0          & 69.1          & 47.1          \\
    Immortal Tracker~\cite{Immortaltrackers}         & 72.8          & 84.0          & \textbf{87.1} & 69.4          & 50.4          \\
    3D-PM-CP~\cite{batch3dmot}          & 71.8 & 84.9          & 83.7          & 68.9 & 49.7 \\
    3D-PM-CL-CP~\cite{batch3dmot}*       & 72.4 & 85.1 & 85.5          & 69.5          & 49.3 \\ \hline
    Offline Re-ID (Motion only)            & 72.7          & 84.9          & 85.4 & 69.7          & \textbf{50.9} \\
    Offline Re-ID (Map only)               & \textbf{72.9} & 85.2          & 85.5          & \textbf{70.0} & 50.8          \\
    Offline Re-ID (Motion + Map)      & \textbf{72.9} & \textbf{85.3} & 85.5          & \textbf{70.0} & 50.8         
    \end{tabular}
    \caption{Comparison of AMOTA scores on the nuScenes validation split (official nuScenes setup). * denotes the use of sensor data for tracking.}
    \label{tab: Re-ID AMOTA online }
\end{table*}

\begin{table*}[htb]
\centering
\begin{tabular}{c|cccc}
\textbf{Method}       & \textbf{AMOTP \text{/ m}}          & \textbf{Recall}        & \textbf{MOTA}          & \textbf{IDS}          \\ \hline
CenterPoint~\cite{Centerpoint}  & 0.623          & 73.0          & 60.5          & 267          \\
Immortal Tracker~\cite{Immortaltrackers}    & \textbf{0.574} & 73.9          & 60.5 & \textbf{109} \\ \hline
Offline Re-ID (Motion only)       & 0.613 & \textbf{74.8} & 62.1          & 168 \\
Offline Re-ID (Map only)         & 0.603 & 74.6 & \textbf{62.2} & 145          \\
Offline Re-ID (Motion + Map) & 0.603          & 74.6          & 62.1 & 147         
\end{tabular}
\caption{Comparison of AMOTA scores on the nuScenes validation split (official nuScenes setup). * denotes the use of sensor data for tracking.}
\label{tab: Re-ID evaluation online}
\end{table*}

\begin{table*}[htb]
\centering
\begin{tabular}{c|cc|cc|cc|cc}
 \multirow{2}{*}{\textbf{Offline Re-ID}} & \multicolumn{2}{c|}{\textbf{AMOTA}} & \multicolumn{2}{c|}{\textbf{AMOTP \text{/ m}}} & \multicolumn{2}{c|}{\textbf{IDS}} & \multicolumn{2}{c}{\textbf{Recall}}   \\ \cline{2-9} 
                               & w/o     & w                & w/o     & w                & w/o          & w            & w/o           & w             \\ \hline
CenterPoint~\cite{Centerpoint}                  & 70.9    & \textbf{72.9}    & 0.623   & \textbf{0.603}   & 267          & \textbf{183} & 73.0          & \textbf{74.6} \\
SimpleTrack~\cite{SimpleTrack}                  & 70.6    & \textbf{71.4}    & 0.637   & \textbf{0.619}   & \textbf{175} & 179          & 71.9          & \textbf{73.0} \\
VoxelNet~\cite{Voxelnet}                    & 70.3    & \textbf{71.1}    & 0.690   & \textbf{0.653}   & 337          & \textbf{245} & \textbf{74.7} & 73.5          \\
ShaSTA~\cite{ShaSTA}                       & 72.7    & \textbf{73.3}    & 0.600   & \textbf{0.583}   & 210          & \textbf{180} & \textbf{74.2} & 72.5         
\end{tabular}
\caption{Improvements brought by our offline Re-ID model over different SOTA online trackers (official nuScenes setup). w: with Re-ID. w/o: without Re-ID.}
\label{tab: Re-ID over SOTA}
\end{table*}
In Tab.~\ref{tab: Re-ID over SOTA}, we also show the improvements our Re-ID model can bring to online trackers. We have only tuned our model using the results from CenterPoint tracker~\cite{Centerpoint}. Still, our model can generally improve the tracking results from other online trackers, which demonstrates the potential of our model to be used as a useful plugin to improve the tracking result in offline auto labeling. 
\vspace{-3mm}
\section{Occluded objects recovery on validation split}
\label{sec: occluded boxes recovery}
In Tab.~\ref{tab: occluded boxes recovery }, we show that the track completion model can effectively recover the occluded trajectories by increasing the number of TP boxes and reducing the number of FN boxes. The number of FP is less than using the Immortal Tracker adopted in~\cite{Detzero, CTRL}.
\vspace{-3mm}
\begin{table*}[h]
\centering
\begin{tabular}{c|cccc}
\textbf{Method}                   & \textbf{AMOTA}         & \textbf{TP}             & \textbf{FP}            & \textbf{FN}             \\ \hline
CenterPoint~\cite{Centerpoint}              & 70.2          & 59332          & \textbf{8197} & 14704          \\
Immortal Tracker~\cite{Immortaltrackers}                & 72.3          & 59271          & 9593          & 14883          \\ \hline 
Offline Track Completion & \textbf{72.4} & \textbf{60675} & 8953          & \textbf{13432}
\end{tabular}
\caption{Our track completion model can effectively recover the occluded objects by increasing TP (All-boxes setup). }
\label{tab: occluded boxes recovery }
\end{table*}

\section{Detailed formulation}
\label{sec: Formulation}
\subsection{Re-ID formulation}
\label{sec: Re-ID formulation}
The upstream online tracker provides a set of tracklets  in the scene. In , history tracklets are the tracklets that end earlier before the last frame of the scene. They are represented as . For the -th history tracklet , it contains history observations of , where  is the last observation of  on the temporal horizon and  the first observation. Each observation contains the location, orientation, size, uncertainty, and velocity. It is defined as:

where  represent the size of the -th bounding box.  is the confidence score, and  the velocities on  directions.

For each history tracklet , it has a set of possible future candidate tracklets for matching: . Each future tracklet  starts after the termination of : 



In practice, we set  where  is the horizon of the death memory of the online tracker since the GT future tracklets reappearing within such period are likely to be associated by the online tracker. Each tracklet has a feature dimension of , where  is the length of the tracklet. The 8 features correspond to , where  are the local BEV coordinates,  the yaw angle,  the time relative to the last observation of the history tracklet and  the velocities on x and y direction.

The model also extracts information from the fully connected lane graph  where each node in  is a lane centerline section.  is the set containing all the edges for each pair of nodes. Each lane node is encoded from a lanelet, which is a section of the lane with a maximal length of 20m. A lanelet is represented as several lane poses. Each lane pose contains the feature of , where  are the BEV coordinates of the lane poses in the local frame,  the yaw angle,  the binary flag indicating whether the lane section ends. Following PGP~\cite{PGP}, we have also included , a 2-D binary vector indicating whether the pose lies on a stop line or crosswalk, to capture both the geometry as well as traffic control elements along the lane. The graph thus has a feature with the dimension of , where  is the number of lane nodes and  is the length of a lanelet. 

Given a history tracklet  and a future tracklet  and lane graph , the model outputs motion affinity scores and map-based affinity scores:


where  and  represent the networks of the motion branch and map branch respectively. 
The final matching score for the association  is a weighted sum of  and . Therefore, we can construct a matching score matrix  for tracklet association, where  is the number of tracklets  in the scene and  the number of history tracklets . 



Each row in  represents a history tracklet and each column represents a future tracklet. Based on the established affinity matrix, bipartite matching is performed for association such that the sum of the matching scores is maximized. 



\max_{X \in \left\{ 0,1 \right\}^{n\times N}} \sum_{i=1}^n \sum_{j=1}^N \boldsymbol{C}_{i, j} X_{i, j}

\text{s.t.} & \sum_{i=1}^n X_{i, j} \leq 1, \forall j  \\
    &\sum_{j=1}^N X_{i, j} \leq 1, \forall i


For tracklet pairs violating the constraint Eq.~\ref{eq:future candidate constraints}, the corresponding elements will not be considered for matching.

\subsection{Track completion formulation}
\label{sec: Track completion formulation}
Following the formulation in Sect.~\ref{sec: Re-ID formulation}, the model receives a history observation sequence , and a matched future observation sequence  , where  is the occlusion horizon of the -th trajectory,  and  the lengths of history and future sequence respectively. The model predicts a motion sequence  in between. Here, we choose to output the  coordinates in the BEV plane, and the yaw angle . 

where  represents the network of the track completion model. We use the same motion features as in the previous Sect.~\ref{sec: Re-ID formulation}, with the exception that we do not use the velocity explicitly. This results in a feature dimension of . Given that the start and end pose of the predicted are already known, the model can implicitly predict the velocity in between. Hence, we discard the velocity features to reduce the possible noise from the imperfect online tracking result. For the semantic map, we also parameterize the lanes in the same way as introduced in the previous Sect.~\ref{sec: Re-ID formulation}. 

\section{Loss functions} 
\label{sec: loss formulas}
We adopt the focal loss~\cite{focal_loss} to train the Re-ID model.


where  is the predicted affinity score, and  is the GT score.  and  are two hyperparameters, which are set to  and  respectively. 

For the track completion model, we use Huber loss (i.e. smooth L1 loss) as in~\cite{DCMS,laneGCN,gu2021densetnt} to train the  regression heads in both the initial trajectory decoder and the final refinement blocks: 

where  are the predicted  coordinates in local agent frame,  the prediction horizon.  and  denotes the  and  norm . For yaw angle regression, we first adjust all the GT yaw angles such that their absolute differences with the predicted yaw angles are less than . Then, we apply  loss on the regressed yaw angles:


The final regression loss  is:

where  and  are the weights. We set  and .

\section{Implementation details}
\label{sec: implementation details}
We train the Re-ID model for 50 epochs on Tesla V-100 GPU using a batch size of 64 with the AdamW~\cite{AdamW} optimizer with an initial learning rate of , which decays by a factor of 0.6 every 10 epochs. The training time is 11.5 hours. We train the track completion model following the same setting except for a different decay factor of 0.5 every 10 epochs. The training time is 4.2 hours. 

Our model is agnostic to the selections of the detector and online tracker. For the evaluation with online tracking results, we use an association threshold of 0.9. If the the motion affinity and map-based affinity scores are both lower than the threshold, the corresponding matching pair would not participate in the association. For the evaluation with pseudo-occlusions, we use the tracks from the nuScenes validation split provided by the nuScenes software devkit for the motion prediction challenge. 
\section{Supplement of experimental setups}
\label{sec: experimental setup}
\subsection{Experimental setup for track completion model evaluation with online tracking results}
\label{sec: Track completion evaluation online setup}
We take the Re-ID result refined by the motion and the map branch as input. Based on the Re-ID result, the track completion model interpolates the gaps and recovers the missing trajectories. If a gap within a track is spatially larger than 3m or temporally longer than 1.8 seconds, it is considered a possible occlusion, and the track completion model predicts the poses for the missing timestamps in between. Otherwise, the evaluation code automatically does a linear interpolation to fill the small gaps. The sizes of the recovered bounding boxes are linearly interpolated from the two bounding boxes in the history tracklet and the future tracklet. 
\subsection{Experimental setup for Re-ID model evaluation with pseudo-occlusions}
\label{sec: Re-ID evaluation pseudo-occ setup}
We mask the GT tracks in the nuScenes validation split to create pseudo-occlusions to evaluate our Re-ID model independently. In every evaluated sample, all future tracks are masked for random durations to create diverse pseudo-occlusions. The durations range from 1.5s to each track's maximal length (12.5s at most) such that each future tracklet has at least one last pose visible. The history tracklet in each sample is also randomly deprecated such that it has at least one pose left as input and is at most 2.5 seconds long. The distribution of pseudo-occlusions is shown in Fig.~\ref{fig: psd_occ_length_distribution}. The number of matching candidate tracklets ranges from 2 to 65, as shown in Fig.~\ref{fig: track_num_distribution}.


\begin{figure}[htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/tracklet_num_distribution.png}
    \caption{The distribution of the number of candidate tracklets in Re-ID model evaluation with pseudo-occlusions. The candidate tracklet number ranges from 2 to 65.}
    \label{fig: track_num_distribution}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/occlusion_duration_distribution.png}
    \caption{The distribution of the pseudo occlusion durations in Re-ID model evaluation. The occlusion duration ranges from 1.5 seconds to 12.5 seconds.}
    \label{fig: psd_occ_length_distribution}
   \end{minipage}
\end{figure}





\end{document}
