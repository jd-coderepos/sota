\documentclass{article} 

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[hyperfootnotes=false]{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsmath,amsfonts,amsthm}       \usepackage{mathtools}      \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{subcaption}
\usepackage{bbm}
\usepackage{multirow}
\usepackage[inline]{enumitem}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage[capitalise]{cleveref}  \usepackage{comment}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[font=small]{caption}

\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\argmin}{\arg\,\min}
\newcommand{\argmax}{\arg\,\max}

\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {1em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Learning to Bootstrap for Combating Label Noise}
\usepackage{authblk}
\author[1]{Yuyin Zhou}
\author[1]{Xianhang Li}
\author[2]{Fengze Liu}
\author[3]{Xuxi Chen}
\author[4]{Lequan Yu}
\author[1]{Cihang Xie}
\author[5]{Matthew P. Lungren}
\author[5]{Lei Xing}

\affil[1]{University of California, Santa Cruz}
\affil[2]{Johns Hopkins University}
\affil[3]{UT Austin}
\affil[4]{The University of Hong Kong}
\affil[5]{Stanford University}

\date{\vspace{-6ex}}

\begin{document}

\maketitle

\begin{abstract}
Deep neural networks are powerful tools for representation learning, but can easily overfit to noisy labels which are prevalent in many real-world scenarios. 
Generally, noisy supervision could stem from variation among labelers, label corruption by adversaries, etc.
To combat such label noises, one popular line of approach is to apply customized weights to the training instances, so that the corrupted examples contribute less to the model learning.
However, such learning mechanisms potentially erase important information about the data distribution and therefore yield suboptimal results.
To leverage useful information from the corrupted instances, an alternative is the bootstrapping loss, which reconstructs new training targets on-the-fly by incorporating the network's own predictions (i.e., pseudo-labels).


In this paper, we propose a more generic learnable loss objective which enables a joint reweighting of instances and labels at once.
Specifically, our method dynamically adjusts the \textit{per-sample importance weight} between the real observed labels and pseudo-labels, where the weights are efficiently determined in a meta process.
Compared to the previous instance reweighting methods, our approach concurrently conducts implicit relabeling, and thereby yield substantial improvements with almost no extra cost.
Extensive experimental results demonstrated the strengths of our approach over existing methods on multiple natural and medical image benchmark datasets, including CIFAR-10, CIFAR-100, ISIC2019 and Clothing 1M.
The code is publicly available at~\url{https://github.com/yuyinzhou/L2B}.

\end{abstract}
\section{Introduction}
Recent advances in deep learning have achieved great success on various computer vision applications, where large-scale clean datasets are available.
However, noisy labels or intentional label corruption by an adversarial rival could easily cause dramatic performance drop~\cite{nettleton2010study}. 
This problem is even more crucial in the medical field, given that the annotation quality requires great expertise.
Therefore, understanding, modeling, and learning with noisy labels has gained great momentum in recent research efforts~\cite{frenay2013classification,natarajan2013learning,han2019deep,li2019learning,liu2020early,jiang2018mentornet,ren2018learning,xue2019robust,li2020dividemix,wang2020learning,zheng2021meta,Yao_2021_CVPR,Zhu_2021_CVPR,wu2021ngc,zhou2021learning}.

Existing methods of learning with noisy labels primarily take a loss correction strategy.
One popular direction is to first estimate the noise corruption matrix and then use it to correct the loss function~\cite{patrini2017making,goldberger2017training}.
However, correctly estimating the noise corruption matrix is usually challenging and often involves assumptions about the noise generation process~\cite{xia2019anchor,liu2015classification,hendrycks2018using}. 
Other research efforts focus on selecting clean samples from the noisy data~\cite{jiang2018mentornet,han2018co,yu2019does,fang2020rethinking} by treating samples with small loss as clean ones~\cite{arpit2017closer}. 
Instead of directly discarding those ``unclean'' examples, an extension of this idea is focusing on assigning learnable weights to each example in the noisy training set~\cite{ren2018learning,shu2019meta}, where noisy samples have low weights. 
However, discarding or attending less to a subset of the training data (e.g., noisy samples) can erase important information about the data distribution.

To fully exploit the corrupted training samples, another direction is to leverage the network predictions (i.e., pseudo-labels~\cite{lee2013pseudo}) to correct or reweight the original labels~\cite{reed2014training,tanaka2018joint}, so that the holistic data distribution information could be preserved during network training.
One representative work is the bootstrapping loss~\cite{reed2014training}, 
which introduces a perceptual consistency term in the learning
objective that assigns a weight to the pseudo-labels to compensate for the erroneous guiding of noisy
samples.
While in this strategy, the weight for the pseudo-labels is manually selected and remains the same for all training samples, which does not prevent fitting the noisy ones and can even lead to low-quality label correction ~\cite{arazo2019unsupervised}. 
To tackle this challenge, Arazo et al.~\cite{arazo2019unsupervised} designed a dynamic bootstrapping strategy to adjusts the label weight by fitting a mixture model. 
Instead of separately reweighting labels or instances, in this paper, we propose a more generic learning strategy to enable a joint instance and label reweighting.
We term our method as \textbf{L}earning to \textbf{B}ootstrap (\textbf{L2B}), where we aim to leverage the learner's own predictions to bootstrap itself up for combating label noise from a meta-learning perspective.


During each training iteration, L2B learns to dynamically re-balance the importance between the real observed labels and pseudo-labels, where the per-sample weights are determined by the validation performance on a separated clean set in a meta network.
Unlike the bootstrapping loss used in~\cite{reed2014training,arazo2019unsupervised,zhang2020distilling} which explicitly conducts relabeling by taking a weighted sum of the pseudo- and the real label, L2B reweights the two losses associated with the pseudo- and the real label instead (where the weights need not be summed as 1). 
In addition, we theoretically prove that our formulation, which reweights different loss terms, can be reduced to the original bootstrapping loss and therefore conducts an implicit relabeling instead. 
By learning these weights in a meta-process, our L2B yields substantial improvement (e.g., \textbf{+8.9\%} improvement on CIFAR-100 with 50\% noise) compared with the instance reweighting baseline with almost no extra cost.
We conduct extensive experiments on public natural image datasets (i.e., CIFAR-10, CIFAR-100, and Clothing 1M) and medical image dataset (i.e., ISIC2019), under different types of simulated noise and real-world noise. Our method outperforms various existing explicit label correction and instance reweighting works, demonstrating the strengths of our approach.

Our main contributions are as follows:
\begin{itemize}
    \item We propose a generic learnable loss objective which enables a joint instance and label reweighting, for combating label noise in deep learning models. 
    
    \item We prove that our new objective is, in fact, a more general form of the bootstrapping loss, and propose L2B to efficiently solve for the weights in a meta-learning framework.
    
    \item  Compared with previous instance re-weighting methods, L2B exploits noisy examples more effectively without discarding them by jointly re-balancing the contribution of real and pseudo labels.
    
    \item We show the theoretical convergence guarantees for L2B, and demonstrate its superior results on natural and medical image recognition tasks under both synthetic and real-world noise.
\end{itemize}


\section{Related Works}
\paragraph{Learning through explicit relabeling.}
To effectively handle noisy supervision, many works propose to directly correct the training labels through estimating the noise transition matrix ~\cite{xia2019anchor,yao2020dual,goldberger2017training,patrini2017making} or modeling noise by graph models or neural networks~\cite{xiao2015learning,vahdat2017toward,veit2017learning,lee2018cleannet}.
Patrini et al.~\cite{patrini2017making} estimate the label corruption matrix to directly correct the loss function.
Hendrycks et al.~\cite{hendrycks2018using} further propose to improve the corruption matrix by using a clean set of data, which then enables training a corrected classifier. However, these methods usually require assumptions about noise modeling.
For instance, Hendrycks et al.~\cite{hendrycks2018using} assume that the noisy label is only dependent on the true label and
independent of the data.
Another line of approaches proposes to leverage the network prediction for explicit relabeling. 
Some methods~\cite{tanaka2018joint,yi2019probabilistic} relabel the samples by directly using pseudo-labels in an iterative manner.
Han~et al. use generated prototypes as pseudo-labels to be more noise tolerant~\cite{han2019deep}. 
Instead of assigning the pseudo-labels as supervision, Reed~et al.~\cite{reed2014training} propose to generate new training targets by a convex combination of the real and pseudo labels.
In a recent study, Ortego et al.~\cite{Ortego_2021_CVPR} directly apply this strategy for classification refinement, and combine it with contrastive learning for training noise-robust models.
However, using a fixed weight for all samples does not prevent fitting the noisy ones could even limit the label correction.
To tackle this challenge, Arazo et al. propose a dynamic bootstrapping strategy, which calculates sample weights
by modeling per-sample loss with a beta mixture model~\cite{arazo2019unsupervised}.
Zhang~et al. further propose to learn this weight in a meta step, and combine semi-supervised learning for furthering the performance~\cite{zhang2020distilling}.



\paragraph{Instance reweighting.}
To reduce the negative effect of corrupted examples, many research efforts have also been dedicated to selecting or reweighting training instances so that noisy samples contribute less to the loss~\cite{jiang2018mentornet,ren2018learning,fang2020rethinking}.
Based on the observation deep neural networks tend to learn simple patterns first before fitting label noise~\cite{arpit2017closer},
many methods treat samples with small loss as clean ones~\cite{jiang2018mentornet,shen2019learning,han2018co,yu2019does,Wei_2020_CVPR}.
Among those methods,
Co-teaching~\cite{han2018co} and Co-teaching$+$~\cite{yu2019does} train two networks where each network selects small-loss samples in a mini-batch to train the other.
Li~et al.~\cite{li2020dividemix} further propose to incorporate semi-supervised learning techniques to better leverage the noisy examples.
Jiang~et al.~\cite{jiang2018mentornet} propose to use curriculum learning to improve convergence and generalization by ordering instances.
Rather than directly selecting clean examples for training, meta-learning-based instance reweighting methods are also gaining momentum recently~\cite{ren2018learning,shu2019meta,xu2021faster}.
In these methods, the example weights and the network parameters are updated in a bi-level optimization to determine
the contribution of each training sample.
This line of approach has also been successfully applied for robust medical image analysis~\cite{xue2019robust,mirikharaji2019learning}.


Different from the aforementioned approaches which separately handle instance reweighting and label reweighting, we propose a new general learning objective to simultaneously adjust the per-sample loss weight while implicitly relabeling the training samples.


\section{Methodology}
\label{sec:method}
\subsection{Preliminary}
Given a set of $N$ training samples, i.e., $\mathcal{D}_{tra} = \{(x_i, y_i)| i = 1,...,N\}$, where $x_i \in \mathbb{R}^{W\times H}$ denotes the $i$-th image and $y_i$ is the observed noisy label. 
In this work, we also assume that there is a small unbiased and clean validation set $\mathcal{D}_{val} = \{(x^v_i, y^v_i)|i= 1,...,M\}$ and $M \ll N$, where the superscript $v$ denotes the validation set. 
Let $\mathcal{F} (:, \theta)$ denote the neural network model parameterized by $\theta$. 
Given an input-target pair $(x, y)$, we consider the loss function of
$\mathcal{L}(\mathcal{F} (x,\theta), y)$ (e.g., cross-entropy loss) to minimize during the training process.
Our goal, in this paper, is to properly utilize the small validation set $\mathcal{D}_{val}$ to guide the model training on $\mathcal{D}_{tra}$, for reducing the negative effects brought by the noisy annotation.

To establish a more robust training procedure, Reed et al. proposed the bootstrapping loss~\cite{reed2014training} to enable the learner to ``disagree'' with the original training label, and effectively re-label the data during the training. Specifically, the training targets will be generated using a convex combination of training labels and predictions of the current model (i.e., pseudo-labels~\cite{lee2013pseudo}), for purifying the training labels. Therefore, for a $L$-class classification problem, the loss function for optimizing $\theta$ can be derived as follows:
\begin{equation}
\label{Eqn:pseudo_gt}
y_i^\textup{pseudo} = \argmax_{l=1,..,L} \mathcal{P} (x_i,\theta),
\end{equation}
\begin{equation}
\label{Eqn:reed_hard_loss}
\theta^* = \argmin_\theta \sum_{i=1}^N \mathcal{L}(\mathcal{F} (x_i,\theta), \beta y_i^\textup{real} + (1 - \beta) y_i^\textup{pseudo}),
\end{equation}
where $\beta$ is used for balancing the weight between the real labels and the pseudo-labels. $\mathcal{P} (x_i,\theta)$ is the model output. $y^\textup{real}$ and $y^\textup{pseudo}$ denote the observed label and the pseudo-label respectively.
However, in this method, $\beta$ is manually selected and fixed for all training samples, which does not prevent fitting the noisy ones and can even lead to low-quality label correction~\cite{arazo2019unsupervised}.
Moreover, we observe that this method is quite sensitive to the selection of the hyper-parameter $\beta$.
For instance, as shown in Figure~\ref{fig:overview}(a), even a similar $\beta$ selection (i.e., $\beta=0.6$/$\beta=0.8$) behaves differently under disparate noise levels, making the selection of $\beta$ even more intractable.
Another limitation lies in that Eq.~\eqref{Eqn:reed_hard_loss} treats all examples as equally important during training, which could easily cause overfitting for biased training data.


\begin{figure}[t]
\begin{center}
\includegraphics[width=\textwidth]{L2B.pdf}
\end{center}
\vspace{-2em}
  \caption{(a) The original bootstrapping loss~\cite{reed2014training} is sensitive to the reweighting hyper-parameter $\beta$. Under different noise levels, the optimal $\beta$ is different (\emph{NF} stands for noise fraction). (b) Schematic description of our Learning to Bootstrap (i.e., \textbf{L2B}) method. The reweighting hyper-parameters are learned in a meta-process.}
\vspace{-1em}
\label{fig:overview}
\end{figure}

\subsection{Learning to Bootstrap through Meta-Learning}
To address these above challenges, in this paper, we aim to learn to bootstrap the model by conducting a joint label reweighting and instance reweighting.
To achieve this, we propose to generate meta-learned weights for guiding our main learning objective: 
\begin{equation}
\label{eq:theta_star}
\begin{aligned}
\theta^*(\boldsymbol{\alpha}, \boldsymbol{\beta})  = \argmin_\theta \sum_{i=1}^N &\alpha_i \mathcal{L}(\mathcal{F} (x_i,\theta), y_i^\textup{real}) \\ + &\beta_i \mathcal{L}(\mathcal{F} (x_i,\theta), y_i^\textup{pseudo}),
\end{aligned}
\end{equation}
with $\{\alpha_i, \beta_i\}_{i=1}^N$ being the balance weights. 
Here we note that this new learning objective can be regarded as a general form of the original bootstrapping loss, as Eq.~\eqref{eq:theta_star} can be reduced to Eq.~\eqref{Eqn:reed_hard_loss} when $\alpha_i + \beta_i = 1$ given that $\mathcal{L}(\cdot)$ is the cross-entropy loss (see details in Appendix A).
By relaxing this constraint such that $\boldsymbol{\alpha}, \boldsymbol{\beta} \ge \boldsymbol{0}$, we can see that the optimization of Eq.~\eqref{eq:theta_star} not only allows the main learner to explore the optimal combination between the two loss terms but also concurrently adjust the contribution of different training samples.
In addition, compared with Eq.~\eqref{Eqn:reed_hard_loss}, the optimization of Eq.~\eqref{eq:theta_star} does not rely on explicitly generating new training targets (i.e., $\beta y_i^\textup{real} + (1 - \beta) y_i^\textup{pseudo}$), but rather conducts implicit relabeling during training by reweighting different loss terms.
We note that the key to L2B is that the sum of $\alpha_i$ and $\beta_i$ need not be 1, which results in \textbf{+8.9\%} improvement on CIFAR-100 with 50\% noise (Section~\ref{sec:performance}).

Note that this form is also similar to self-distillation in~\cite{li2017learning}. But different from~\cite{li2017learning} where the weights are determined by heuristics, our weights $\boldsymbol{\alpha}, \boldsymbol{\beta}$ are meta-learned based on its performance on the validation set $\mathcal{D}_{val}$, that is
\begin{equation}\label{eq:weights_star}
\boldsymbol{\alpha}^*, \boldsymbol{\beta}^* = \argmin_{\boldsymbol{\alpha}, \boldsymbol{\beta} \ge \boldsymbol{0}}\frac{1}{M} \sum_{i=1}^M\mathcal{L}(\mathcal{F} (x_i^v,\theta^*(\boldsymbol{\alpha}, \boldsymbol{\beta})), y_i^v).
\end{equation}
It is necessary to constrain $\alpha_i, \beta_i \ge 0$ for all $i$ to avoid potential unstable training~\cite{ren2018learning}.
Both the meta learner (i.e., Eq.~\eqref{eq:weights_star}) and the main learner (i.e., Eq.~\eqref{eq:theta_star}) are optimized concurrently, which allows the model to maximize the performance on the clean validation set $\mathcal{D}_{val}$ by adjusting the importance weights of the observed and the pseudo-labels in a differentiable manner.


\paragraph{Online Approximation.}
For each step $t$ at training, a mini-batch of training examples $\{(x_i,y_i),1\leq i\leq n\} $ with $n \ll N$ is sampled to estimate a temporary adjustment to the parameters based on the descent direction of the loss function. 
For simplicity, let $f_{i}(\theta)$ denote $\mathcal{L}(\mathcal{F} (x_i,\theta), y_i^\textup{real})$ and $g_{i}(\theta)$ denote $\mathcal{L}(\mathcal{F} (x_i,\theta), y_i^\textup{pseudo})$ in the following sections.
Given any $\boldsymbol{\alpha},\boldsymbol{\beta}$, we use 
\begin{equation}\label{eq:theta_hat}
\hat{\theta}_{t+1} = \theta_t - \lambda \nabla (\sum_{i=1}^n \alpha_i \ f_{i}(\theta)+\beta_i\ g_{i}(\theta))\Big |_{\theta=\theta_t}
\end{equation}
to approach the solution of Eq.~\eqref{eq:theta_star}. Here $\lambda$ is the step size. We then estimate the corresponding optimal $\boldsymbol{\alpha},\boldsymbol{\beta}$ as
\begin{equation}\label{eq:alpha_t}
\boldsymbol{\alpha}_t^*, \boldsymbol{\beta}_t^* = \argmin_{\boldsymbol{\alpha}, \boldsymbol{\beta} \ge \boldsymbol{0}}\frac{1}{M} \sum_{i=1}^M f_i^v(\hat{\theta}_{t+1}).
\end{equation}




However, directly solving for Eq.~\eqref{eq:alpha_t} at every training step requires too much computation cost. To reduce the computational complexity, we apply one step gradient descent of $\boldsymbol{\alpha}_t, \boldsymbol{\beta}_t$ on a mini-batch of validation set $\{(x_i^v,y_i^v),1\leq i \leq m\}$ with $m\leq M$ as an approximation. Specifically,
\begin{equation}\label{eq:alpha_beta}
(\alpha_{t,i},\beta_{t,i})= -\eta\nabla(\sum_{i=1}^m f_i^v(\hat{\theta}_{t+1}))\Big |_{\alpha_i=0,\beta_i=0},
\end{equation}
where $\eta$ is the step size for updating $\boldsymbol{\alpha},\boldsymbol{\beta}$. To ensure that the weights are non-negative, we apply the following rectified function:
\begin{equation}\label{eq:rectified_weights}
\Tilde{\alpha}_{t,i}=\text{max}(\alpha_{t,i},0),\ \Tilde{\beta}_{t,i}=\text{max}(\beta_{t,i},0).
\end{equation}
To stabilize the training process, we also normalize the weights in a single training batch so that they sum up to one: 
\begin{equation}\label{eq:alpha_norm}
\Tilde{\alpha}_{t,i}=\frac{\Tilde{\alpha}_{t,i}}{\sum_i^n \Tilde{\alpha}_{t,i}+\Tilde{\beta}_{t,i}},\ \Tilde{\beta}_{t,i}=\frac{\Tilde{\beta}_{t,i}}{\sum_i^n \Tilde{\alpha}_{t,i}+\Tilde{\beta}_{t,i}}.
\end{equation}
Finally, we estimate $\theta_{t+1}$ based on the updated $\boldsymbol{\alpha}_{t},\boldsymbol{\beta}_{t}$ so that $\theta_{t+1}$ can consider the meta information included in $\boldsymbol{\alpha}_{t},\boldsymbol{\beta}_{t}$:
\begin{equation}\label{eq:theta_t+1}
\theta_{t+1} = \theta_t - \lambda \nabla (\sum_{i=1}^n \alpha_{t,i} \ f_{i}(\theta)+\beta_{t,i}\ g_{i}(\theta))\Big |_{\theta=\theta_t}.
\end{equation}

See Appendix B for detailed calculation of the gradient in Eq.~\eqref{eq:theta_t+1}. 
A schematic description of our Learning to Bootstrap algorithm is illustrated in Figure~\ref{fig:overview}(b) and the overall optimization procedure can be found in Algorithm~\ref{alg:L2B}.


\begin{algorithm}[t!]
\caption{Learning to Bootstrap}
\label{alg:L2B}
\begin{algorithmic}[1]
\REQUIRE $\theta_0$, $\mathcal{D}_{tra}$, $\mathcal{D}_{val}$, $n$, $m$, $L$
\ENSURE $\theta_T$
\FOR{$t=0$ ... $T-1$}
\STATE $\{x_i, y_i\} \gets$ \text{SampleMiniBatch}($\mathcal{D}_{tra}$, $n$)
\STATE $\{x_i^v, y_i^v\} \gets$ \text{SampleMiniBatch}($\mathcal{D}_{val}$, $m$)
\STATE For the $i$-th sample of $\mathcal{D}_{tra}$, compute $y^\textup{pseudo}_i = \argmax_{l=1,..,L} \mathcal{P} (x_i,\theta_t)$ 
\STATE Learnable weights $\boldsymbol{\alpha}$, $\boldsymbol{\beta}$
\STATE Compute training loss $l_f \gets \sum_{i=1}^n \alpha_i f_i(\theta_t) + \beta_i g_i(\theta_t)$
\STATE $\hat{\theta}_{t + 1} \gets \theta_t - \lambda \nabla l_f\Big |_{\theta=\theta_t}$
\STATE Compute validation loss $l_g \gets \frac{1}{m} \sum_{i=1}^m f^v_i(\hat{\theta}_{t + 1})$
\STATE $(\boldsymbol{\alpha}_{t},\boldsymbol{\beta}_{t})\gets-\eta\nabla l_g \Big |_{\boldsymbol{\alpha}=\boldsymbol{0},\boldsymbol{\beta}=\boldsymbol{0}}$
\STATE $\Tilde{\alpha}_{t,i}\gets\text{max}(\alpha_{t,i},0),\ \Tilde{\beta}_{t,i}\gets\text{max}(\beta_{t,i},0)$
\STATE $\Tilde{\alpha}_{t,i}\gets\frac{\Tilde{\alpha}_{t,i}}{\sum_i^n \Tilde{\alpha}_{t,i}+\Tilde{\beta}_{t,i}},\ \Tilde{\beta}_{t,i}\gets\frac{\Tilde{\beta}_{t,i}}{\sum_i^n \Tilde{\alpha}_{t,i}+\Tilde{\beta}_{t,i}}$
\STATE Apply learned weights $\boldsymbol{\alpha}, \boldsymbol{\beta}$ to reweight the training loss as $\hat{l}_f \gets \sum_{i=1}^n \alpha_{t,i} f_i(\theta_t) + \beta_{t,i} g_i(\theta_t)$
\STATE  $\theta_{t + 1} \gets \theta_t - \lambda \nabla \hat{l}_f\Big |_{\theta=\theta_t}$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Convergence Analysis} 
In proposing Eq.~\eqref{eq:theta_star}, we show that with the first-order approximation of $\boldsymbol{\alpha}, \boldsymbol{\beta}$ in Eq.~\eqref{eq:alpha_beta} and some mild assumptions, our method guarantees to convergence to a local minimum point of the validation loss, which yields the best combination of $\boldsymbol{\alpha}, \boldsymbol{\beta}$. 
Details of the proof are provided in Appendix C. 

\begin{theorem}
\label{theorem:convergence}
Suppose that the training loss function $f,g$ have $\sigma$-bounded gradients and the validation loss $f^v$ is Lipschitz smooth with constant L. With a small enough learning rate $\lambda$, the validation loss monotonically decreases for any training batch $B$, namely, 
\begin{equation}\label{eq:Validation_loss_decrease}
G(\theta_{t+1})\leq G(\theta_t),
\end{equation}
where $\theta_{t+1}$ is obtained using Eq.~\eqref{eq:theta_t+1} and $G$ is the validation loss
\begin{equation}\label{eq:Validation_loss}
G(\theta)=\frac{1}{M}\sum_{i=1}^M f_i^v(\theta),
\end{equation}

Furthermore, Eq.~\eqref{eq:Validation_loss_decrease} holds for all possible training batches only when the gradient of validation loss function becomes $0$ at some step $t$, namely, $G(\theta_{t+1})=G(\theta_t)\ \forall B \Leftrightarrow \nabla G(\theta_t)=0$
\end{theorem}



    



\section{Experiments}
\label{sec:experiments}
\subsection{Datasets}

\paragraph{CIFAR-10 \& CIFAR-100.}~\label{sec:data} Both CIFAR-10 and CIFAR-100 contain 50K training images and 10K test images of size 32 Ã— 32.
Following previous works~\cite{tanaka2018joint,kim2019nlnl,li2020dividemix}, we experimented with both \emph{symmetric} and \emph{asymmetric} label noise.
In our method, we used 1,000 clean images in the validation set $\mathcal{D}_{val}$ following~\cite{jiang2018mentornet,ren2018learning,shu2019meta,hendrycks2018using,zheng2021meta}.

\paragraph{ISIC2019.} Following~\cite{xue2019robust}, we also evaluated our algorithm on a medical image dataset, i.e., skin lesion classification data, under different symmetric noise levels. The dataset contains nine different diagnostic categories: Melanoma, Melanocytic nevus, Basal cell carcinoma, Actinic keratosis, Benign keratosis, Dermatofibroma, Vascular lesion and Squamous cell carcinoma.
Our experiments were conducted on the 25,331 dermoscopic images of the 2019 ISIC Challenge\footnote{\url{https://challenge2019.isic-archive.com/data.html}}, where we used 20400 images as the training set $\mathcal{D}_{tra}$, 640 images as the validation set $\mathcal{D}_{val}$, and tested on 4291 images. 


\paragraph{Clothing 1M.} We also evaluate on a large-scale real-world noisy dataset, Clothing 1M~\cite{xiao2015learning}, which has 1 million training images collected from online shopping websites with labels generated
from surrounding texts.
In addition, the Clothing 1M benchmark also provides an official validation set of 14,313  images and a test set of
10,526 images.

   

\subsection{Implementation Details}
For all CIFAR-10 and CIFAR-100 comparison experiments, we used an 18-layer PreActResNet~\cite{he2016identity} as the baseline network following the setups in~\cite{li2020dividemix}, unless otherwise specified.
The model was trained using SGD with a momentum of
0.9, a weight decay of 0.0005, and a batch size of 256 for CIFAR-100 and 512 for CIFAR-10. 
The network was trained from scratch for 300 epochs. We
set the learning rate as 0.15 initially with a cosine annealing decay. Following~\cite{li2020dividemix}, we set the warm up
period as 10 epochs for both CIFAR-10 \& CIFAR-100. 
The optimizer and the learning rate schedule remained the same for both the main and the meta model. 
Gradient clipping is applied to stabilize training.
All experiments were conducted with one V100 GPU, except for the experiments on Clothing 1M which were conducted with one RTX A6000 GPU.

For ISIC2019 experiments, we used ResNet-50 with ImageNet pretrained weights. A batch size of 64 was used for training with an initial learning rate of 0.01. The network was trained for 30 epochs in total with the warmup period as 1 epoch.
All other implementation details remained the same as above.
For Clothing 1M experiments, we used an ImageNet pre-trained 18-layer ResNet~\cite{he2016identity} as our baseline. We finetuned the network with a learning rate of 0.005 for 300 epochs. The model was trained using SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 256. Following~\cite{li2020dividemix}, to ensure the labels (noisy) were balanced, for each epoch, we sampled 250 mini-batches from the training data. 


\begin{table}[t]
        \centering
        \caption{Comparison of different methods in test accuracy (\%) on CIFAR-10 with symmetric noise. \emph{NF} stands for the noise fraction.}
        \vspace{-.5em}
         \resizebox{0.7\linewidth}{!}{
        \begin{tabular}[b]{|l|c|c|c|c|c|}
                \hline
                \multirow{2}{*}{Method} & \multicolumn{4}{c|}{CIFAR-10}  \\
                \cline{2-5}
                 & 20\% \emph{NF}  &30\% \emph{NF}  &40\% \emph{NF}  &50\% \emph{NF}   \\
                 \hline
                Cross-Entropy  &86.9 &84.9&83.3 &81.3   \\
                Bootstrapping~\cite{reed2014training} &85.2 &84.8 &82.9 &79.2    \\
                Distillation~\cite{li2017learning} &88.0 &86.8 &85.5 &80.0   \\
                GLC~\cite{hendrycks2018using}  &91.4 &90.3 &88.5 &86.4  \\
                L2RW~\cite{ren2018learning}  &90.6 &89.0 &86.6 &85.3  \\
                \hline
                \textbf{L2B (Ours)} &\textbf{92.2} &\textbf{90.7} &\textbf{89.9} &\textbf{88.5} \\
                \hline
        \end{tabular}
         }
        \label{tab:cifar10_comparison}
        \vspace{-.5em}
\end{table}





\subsection{Performance Comparisons}
\label{sec:performance}
\paragraph{Experiments on CIFAR-10 \& CIFAR-100 .} We compare our method with different baselines: 1) Bootstrapping, which modifies the training loss by generating new training targets\footnote{We set $\beta=0.8$ in Eq.~\eqref{Eqn:reed_hard_loss} following the default setting in~\cite{reed2014training}.}, 2) Distillation~\cite{li2017learning}, which transfers the knowledge distilled from the small clean dataset; 3) L2RW~\cite{ren2018learning}, which reweights different instances through meta-learning; 4) GLC~\cite{hendrycks2018using}, which uses the trusted clean data  to correct losses; and 5) Cross-Entropy (the standard training) under different levels of symmetric labels noise ranging from $20\%\sim 50\%$.
To ensure a fair comparison, we report the best epoch for all comparison approaches.
All results are summarized in Table~\ref{tab:cifar10_comparison} and Table~\ref{tab:cifar100_comparison}, which shows L2B significantly outperforms all other competing methods by a large margin across all noise fractions.
It is also observed that compared with previous meta-learning-based instance reweighting method L2RW, the performance improvement is substantial especially under larger noise fraction, which suggests the advantages of jointly reweighting different loss terms.
For example, on CIFAR-100, the accuracy improvement of our proposed L2B reaches $7.6\%$ and $8.9\%$ under 40\% and 50\% noise fraction, respectively. 


\begin{table}[t]
        \centering
        \caption{Comparison with different methods in test accuracy (\%) on CIFAR-100 with symmetric noise. \emph{NF} stands for the noise fraction.
        }
        \vspace{-.5em}
        \label{tab:cifar100_comparison}
        \resizebox{0.7\linewidth}{!}{
        \begin{tabular}{|c|c|c|c|c|}
                        \hline
                        \multirow{2}{*}{Method} & \multicolumn{4}{c|}{CIFAR-100} \\
                        \cline{2-5}
                           & 20\% \emph{NF}  &30\% \emph{NF}  &40\% \emph{NF}  &50\% \emph{NF}   \\
                         \hline
                        Cross-Entropy   &59.6 &52.2 &49.2 & 44.4 \\
                        Bootstrapping~\cite{reed2014training}  &61.8 &54.2 &50.2 &45.8  \\
                        Distillation~\cite{li2017learning}   &62.7 &57.3 &53.7 &45.7  \\
                        GLC~\cite{hendrycks2018using}  &68.8 &65.9 &62.1 &57.9 \\
                        L2RW~\cite{ren2018learning}   &67.8 &63.8 &59.7 &55.6  \\
                        \hline
                        \textbf{L2B (Ours)}  &\textbf{71.8} &\textbf{69.5} &\textbf{67.3} &\textbf{64.5}\\
                        \hline
        \end{tabular}
        }
        \vspace{-.5em}
\end{table}

We also test our model with asymmetric noise labels (i.e., 40\% noise fractions) and summarize the testing accuracy in Table~\ref{tab:aymmetric_noise}. 
Among all compared methods, we re-implement L2RW under the same setting and report the performance of all other competitors from previous papers~\cite{kim2019nlnl,Kim_2021_CVPR,li2020dividemix}.
Compared with previous meta-learning-based instance reweighting methods (i.e., LR2W~\cite{ren2018learning}, MW-Net~\cite{shu2019meta}), a dynamic bootstrapping method (M-correction~\cite{arazo2019unsupervised}) and other explicit relabeling methods (i.e., F-correction~\cite{patrini2017making}, Tanaka~et al.~\cite{tanaka2018joint}), our L2B achieves superior performance with 40\% asymmetric noise. Note that we mainly compare with methods which do not use any augmentation techniques, as our L2B does not rely on those. However, we do notice that our method outperforms M-correction which is built on top of mixup augmentation~\cite{zhang2018mixup}, demonstrating the benefits of our joint instance and label reweighting mechanisms for tackling asymmetric noise.


\begin{table}[tbh!]
        \centering
        \footnotesize
        \caption{Comparison with asymmetric noise.}
        \vspace{-.5em}
        \begin{tabular}[b]{|c|c|}
                \hline
                Method   &CIFAR-10   \\
                 \hline
                Cross-Entropy &85.0 \\
                F-correction~\cite{patrini2017making}  &87.2 \\
                M-correction~\cite{arazo2019unsupervised} &87.2     \\
                Chen~et al.~\cite{chen2019understanding} &88.6    \\
                P-correction~\cite{yi2019probabilistic} &88.5    \\
                Tanaka~et al.~\cite{tanaka2018joint} &88.9    \\
                MLNT~\cite{li2019learning} &89.2    \\
                L2RW~\cite{ren2018learning} &89.2    \\
                MW-Net~\cite{shu2019meta} &89.7 \\
                NLNL~\cite{kim2019nlnl} &89.9 \\
                JNPL~\cite{Kim_2021_CVPR} &90.7 \\
                \hline
                \textbf{L2B (Ours)} & \textbf{91.8}    \\
                \hline
        \end{tabular}
        \label{tab:aymmetric_noise}
        \vspace{-.5em}
 \end{table}    



\paragraph{Alleviate potential overfitting to noisy examples.} We also plot the testing accuracy curve under different noise fractions in Figure~\ref{fig:accuracy_comparison}, which shows that our proposed L2B would help preventing potential overfitting to noisy samples compared with standard training.
Meanwhile, compared to simply sample reweighting (L2RW), our L2B introduces pseudo-labels for bootstrapping the learner and is able to converge to a better optimum.


\begin{figure}[tbh!]
        \centering
        \includegraphics[width=0.95\linewidth]{acc.pdf}
        \vspace{-1em}
        \caption{Test set accuracy versus the number of epochs on CIFAR-100 under the noise fraction of 20\%  and 40\%.}
        \vspace{-1em}
        \label{fig:accuracy_comparison}
\end{figure}



\paragraph{Generalization to real-world noisy labels.} 
We test L2B on Clothing 1M~\cite{xiao2015learning}, a large-scale dataset with real-world noisy labels. 
To further illustrate the effectiveness of our approach, we compare with:  1) Tanaka et al.~\cite{tanaka2018joint}, 2) MLNT~\cite{li2019learning}, 3) MW-Net~\cite{shu2019meta}, 4) GLC~\cite{hendrycks2018using}, and 5) MLC~\cite{zheng2021meta} using the same validation and testing splits provided by the benchmark.
The results of all competitors are reported from~\cite{zheng2021meta}.
As shown in Table~\ref{tab:clothing1M_comparison}, our L2B achieves an average performance of 77.5\% accuracy from 3 independent runs with different random seeds, outperforming all competing methods on the Clothing 1M benchmark.


\begin{table*}[h!]
\centering
\caption{Comparison with state-of-the-art methods in test accuracy (\%) under real-world noise on Clothing 1M. Our results are reported from 3 independent runs with different random seeds. All other results are reported from~\cite{zheng2021meta}.}
\label{tab:clothing1M_comparison}
\vspace{-.5em}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method}    &Tanaka  &MLNT &MW-Net &GLC   & MLC & \textbf{L2B}    \\
&~et al.~\cite{tanaka2018joint}
&~\cite{li2019learning}
&~\cite{shu2019meta}
&~\cite{hendrycks2018using}
&~\cite{zheng2021meta}
&\textbf{(Ours)}
\\
 \hline 
Accuracy     &72.2  &73.5 &73.7 &73.7  & 75.8 & $\textbf{77.5}\pm0.2$ \\
\hline
\end{tabular}
}
\vspace{-1em}
\end{table*}

\paragraph{Generalization to medical image analysis.} Table~\ref{tab:ISIC} demonstrates the generalizability of our proposed L2B to medical image analysis~\cite{xue2019robust}. Compared with 1) L2RW~\cite{ren2018learning}, which has also been applied to skin lesion classification/segmentation~\cite{xue2019robust,mirikharaji2019learning} with noisy supervision; 2) Distillation~\cite{li2017learning}; 3)
Mixup~\cite{zhang2018mixup}; and 4) Bootsrapping~\cite{reed2014training}, our method achieves better results under all different noise levels.

\begin{table}[h!]
\footnotesize
\centering
\caption{Comparison with different methods in test accuracy (\%) on ISIC
with symmetric noise.}
\vspace{-1em}
\label{tab:ISIC}
\vspace{1ex}
\begin{tabular}{|c|c|c|c|c|}
\hline
Method  &20\% \emph{NF} &30\% \emph{NF} &40\% \emph{NF}  &50\% \emph{NF}  \\
 \hline
Cross-Entropy   &79.4 &77.5 &75.3 &73.7   \\
Bootstrapping~\cite{reed2014training}  &80.8 &77.7 &75.7  &74.8  \\
Distillation~\cite{li2017learning}  &80.1 &78.8 &76.8  &74.4  \\
Mixup~\cite{zhang2018mixup}  &80.2 &77.9 &76.8   &74.9 \\
L2RW~\cite{ren2018learning}  &80.1  &77.7 &76.3  &74.1   \\
\hline
\textbf{L2B (Ours)}  &\textbf{81.1} &\textbf{80.2} &\textbf{78.6} &\textbf{76.8}   \\
\hline
\end{tabular}
\end{table} 


 

   



    
\paragraph{Stability of experimental results.}
We repeated the experiments 5 times with different random seeds for network initialization and label noise generation, and report mean $\pm$ standard deviation under different experimental settings. As can be seen from Table~\ref{tab:multiple_runs}, with different noise fractions and datasets, the standard deviation among the 5 runs are consistently less that $0.5\%$.



\begin{table}[h!]
          \centering
          \small
           \caption{Multiple-runs experiments under the noise fraction of 20\% \emph{NF} and 40\% \emph{NF}.}
           \vspace{-.5em}
       \resizebox{0.5\linewidth}{!}{
            \begin{tabular}[t]{|c|c|c|}
                    \hline
                   Datasets   &20\% \emph{NF}  &40 \% \emph{NF}  \\
                     \hline
                     CIFAR-10 & 91.99$\pm$ 0.10 &89.39$\pm$  0.22\\
                     \hline
                     CIFAR-100  &71.78 $\pm$ 0.38  &67.23 $\pm$  0.33   \\
                    \hline
                   ISIC 2019   &81.16 $\pm$  0.29 &78.26 $\pm$ 0.45   \\
                    \hline
            \end{tabular}
        \label{tab:multiple_runs}
        \vspace{-.5em}
        }
\end{table}


\subsection{Ablation Study}
\paragraph{On the importance of $\boldsymbol{\alpha}, \boldsymbol{\beta}$.} To understand why our proposed new learning objective can outperform previous meta-learning-based instance reweighting methods, we conduct the following analysis to understand the importance of hyper-parameter $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ in our method. Specifically, we set $\boldsymbol{\alpha} = \boldsymbol{0}$ and $\boldsymbol{\beta} = \boldsymbol{0}$ respectively to investigate the importance of each loss term in Eq.~\eqref{eq:theta_star}.
In addition, we also show how the restriction of $\alpha_i + \beta_i = 1$ (Eq.~\eqref{Eqn:reed_hard_loss}) would deteriorate our model performance as follows. 
\begin{itemize}
   \item $\boldsymbol{\alpha} = \boldsymbol{0}$. As shown in Table~\ref{tab:ablation}, in this case, the performance even decreases compared with the baseline approach. 
   This is due to that when only pseudo-labels are included in the loss computation, the error which occurs in the initial pseudo-label will be reinforced by the network during the following iterations.
   \item $\boldsymbol{\beta} = \boldsymbol{0}$. From Eq.~\eqref{eq:theta_star}, we can see that setting $\boldsymbol{\beta}$ as $\boldsymbol{0}$  is essentially equivalent to the baseline meta-learning-based instance reweighting method L2RW~\cite{ren2018learning}.
   In this case, the performance is largely improved compared to the baseline, but still inferior to our method, which jointly optimizes $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$.
    
    
   \item  $\boldsymbol{\alpha} + \boldsymbol{\beta} = \boldsymbol{1}$. We also investigate whether the restriction of $\boldsymbol{\alpha} + \boldsymbol{\beta} = \boldsymbol{1}$ is required for obtaining optimal weights during the meta-update, as in~\cite{zhang2020distilling}. 
   As shown in Table~\ref{tab:ablation}, L2B ($\boldsymbol{\alpha}, \boldsymbol{\beta} \ge \boldsymbol{0}$) consistently achieves superior results than L2B ($\boldsymbol{\alpha} + \boldsymbol{\beta} = \boldsymbol{1}$) under different noise levels on CIFAR-100. The reason may be the latter is only reweighting different loss terms, whereas the former not only explores the optimal combination between the two loss terms but also jointly adjusts the contribution of different training samples.
\end{itemize}
  

\begin{table}[h!]
        \centering
        \small
        \caption{\textbf{Ablation study under the noise fraction of 20\%  and 40\%}.  L2B ($\boldsymbol{\alpha}, \boldsymbol{\beta} \ge \boldsymbol{0}$) consistently achieves superior results to L2B ($\boldsymbol{\alpha} + \boldsymbol{\beta} = \boldsymbol{1}$) under different noise levels on CIFAR-100.}
        \vspace{-.5em}
       \resizebox{0.45\linewidth}{!}{
            \begin{tabular}[t]{|c|c|c|}
                    \hline
                    Method   &20\% \emph{NF}  &40\% \emph{NF}  \\
                     \hline
                    Cross-Entropy &59.6 &49.2\\
                     \hline
                    $\boldsymbol{\alpha} = \boldsymbol{0}$ &55.7  &47.1   \\
                    $\boldsymbol{\beta} = \boldsymbol{0}$ &63.2  &57.5  \\
                    $\boldsymbol{\alpha} + \boldsymbol{\beta} = \boldsymbol{1}$ &64.8  &59.1  \\
                    \hline
                    $\boldsymbol{\alpha}, \boldsymbol{\beta} \ge \boldsymbol{0}$ & \textbf{71.8}  &\textbf{67.3}  \\
                    \hline
            \end{tabular}
         \label{tab:ablation}
        }
\end{table}


\begin{figure*}[h]
\begin{center}
\includegraphics[width=\textwidth]{example5.png}
\end{center}
\vspace{-1.5em}
  \caption{Examples of $\alpha$ and $\beta$ on CIFAR-10 with asymmetric noise fraction of 20\%. When the estimated  pseudo label is of high-quality, i.e., the pseudo label is different from the noisy label but equal to the clean label, our model will automatically assign a much higher weight to $\beta$ than to $\alpha$ for corrupted training samples. When the pseudo label is equal to the noisy label (i.e., the two loss terms are equal to each other), $\alpha$ and $\beta$ are almost identical. 
  }
\label{fig:example_weight}
\vspace{-.5em}
\end{figure*}

We also demonstrate a set of qualitative examples to illustrate how our proposed L2B benefits from the joint instance and label reweighting paradigm. 
In Figure~\ref{fig:example_weight}, we can see that when the estimated  pseudo label is of high-quality, i.e., the pseudo label is different from the noisy label but equal to the clean label, our model will automatically assign a much higher weight to $\beta$ for corrupted training samples. On the contrary, $\alpha$ can be near zero in this case. This indicates that our L2B algorithm will pay more attention to the pseudo label than the real noisy label when computing the losses.
In addition, we also show several cases where the pseudo label is equal to the noisy label, where we can see that $\alpha$ and $\beta$ are almost identical under this circumstance since the two losses are of the same value. 
Note that the relatively small values of $\alpha$ and $\beta$ are due to that we use a large batch size (i.e., 512) for CIFAR-10 experiments. By normalizing the weights in each training batch (see Eq.~\eqref{eq:alpha_norm}), the value of $\alpha$ and $\beta$ can be on the scale of $10^{-4}$.






\paragraph{Parameter normalization.}
We note that the normalization of $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ is one key component for accelerating the training process.
However, we observe that different normalization methods of $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ behave quite differently for different datasets. 
To further investigate this, we apply the following normalization functions to each $\alpha_i$ and $\beta_i$ on ISIC2019, CIFAR-100 and Clothing 1M: 1) Eq.~\eqref{eq:alpha_norm} as in~\cite{ren2018learning}, 2) Sigmoid function,
\begin{equation}\label{eq:sigmoid_ISIC}
\alpha_{t,i}=\frac{1}{1 + e^{-\alpha_{t,i}}},\ \beta_{t,i}=\frac{1}{1 + e^{-\beta_{t,i}}},
\end{equation}
and 3) Softmax function,
\begin{equation}\label{eq:softmax_ISIC}
\alpha_{t,i}=\frac{e^{\alpha_{t,i}/\tau}}
{\sum_i^n e^{\alpha_{t,i}/\tau}+e^{\beta_{t,i}/\tau}},\ \beta_{t,i}=\frac{e^{\beta_{t,i}/\tau}}{\sum_i^n e^{\alpha_{t,i}/\tau}+e^{\beta_{t,i}/\tau}},
\end{equation}
where $t$ stands for the training iteration and $\tau$ denotes the temperature parameter for scaling the weight distribution. 
$\tau$ is set as 10.0 when using the Softmax function for normalization.
The comparison among these three different normalization methods is summarized in Figure~\ref{fig:normalization_comparison} on ISIC2019 and CIFAR-100 datasets with 40\% symmetric noise. 
We can see that while Eq.~\eqref{eq:alpha_norm} achieves the best result on CIFAR-100, it yields large training instability on the ISIC2019 dataset. Changing the normalization function to Sigmoid and Softmax can make the training procedure much more stable on the ISIC2019 dataset. 


\begin{figure}[h]
\centering
        \includegraphics[width=\linewidth]{normalization.png}
        \vspace{-3em}
        \caption{\textbf{Comparison among different normalization functions (i.e., Eq.~\eqref{eq:alpha_norm} as in ~\cite{ren2018learning}, Sigmoid function and Softmax function)}. (a) Testing accuracy curve with different normalization functions under 40\% symmetric noise label on the ISIC dataset. (b) Testing accuracy curve with different normalization under 40\% symmetric label noise on CIFAR-100.}
        \label{fig:normalization_comparison}
\end{figure}


In addition, we notice that Softmax (e.g., using the Softmax function with a temperature scaling of 10.0 instead of Eq.~\eqref{eq:alpha_norm} to normalize $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$) slightly outperforms Sigmoid for ISIC2019. Future research should address how to develop a unified normalization function which can be generalizable for different learning tasks.


\paragraph{Comparison with meta-learning-based label correction.}
One recent study~\cite{zheng2021meta} has proposed meta label correction to explicitly relabel the noisy examples via a meta-network. Unlike our L2B which conducts implicit relabeling via reweighting different loss components, this method uses a meta-model for explicit label correction. We re-implement the results with the public code by using PreActResNet-18 as the backbone network
and the comparison results on CIFAR-10 are show in Table~\ref{tab:MLC_comparison}. 
We can see that our method consistently outperforms MLC for noise ratios ranging from 10\% to 50\% on CIFAR-10.

\begin{table}[h!]
\footnotesize
\centering
\caption{Comparison with MLC ~\cite{zheng2021meta} in test accuracy (\%) on CIFAR-10 with symmetric noise.}
\vspace{-1em}
\label{tab:MLC_comparison}
  \resizebox{0.78\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Method  &10\% \emph{NF} &20\% \emph{NF} &30\% \emph{NF}  &40\% \emph{NF} &50\% \emph{NF}  \\
 \hline
MLC   &90.1 &90.1 &88.3 &87.2 & 86.0 \\
 \hline
\textbf{L2B (Ours)}  &\textbf{93.6}  &\textbf{92.2} &\textbf{90.7} &\textbf{89.9} &\textbf{88.5}   \\
\hline
\end{tabular}
}
\vspace{-1em}
\end{table} 



\paragraph{Training cost.}
We list our training time on CIFAR-10 compared with the previous meta-learning based instance reweighting methods, i.e., MLNT~\cite{li2019learning}, L2RW~\cite{ren2018learning}, in Table~\ref{tab:training_cost}. The training times are reported based on PreActResNet-18 on a single V100 GPU card. Our method L2B is directly built on top of L2RW, and our method (which applies a joint instance reweighting and label reweighting) incurs almost no extra training cost when using the same architecture and hardware conditions.

\begin{table}[h]
\footnotesize
\centering
\caption{Training time comparison of MLNT~\cite{li2019learning}, L2RW~\cite{ren2018learning} and L2B (Ours). Our L2B incurs almost no additional training cost when using the same architecture and hardware conditions.}
\vspace{-1em}
\label{tab:training_cost}
  \resizebox{0.5\linewidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
MLNT~\cite{li2019learning}	&L2RW~\cite{ren2018learning}	&\textbf{L2B (Ours)}\\
 \hline
8.6 h	&6.5h	&6.5h \\
\hline
\end{tabular}
}
\vspace{-1.2em}
\end{table} 



\subsection{Discussion \& Limitation}
L2B is designed for building robust representation models, which is crucial for many real-world applications in healthcare, vision, etc. For example, high intra- and inter-physician variations are well-known in medical diagnostic tasks, which lead to erroneous labels that could derail our learning algorithms. 
L2B can help rectify such noisy data distributions and prevent potential overfitting during training and thus reduce the risking of medical errors.

This paper specifically focuses on the situation where a small clean validation set is accessible (e.g., Clothing 1M~\cite{xiao2015learning}).
However, we do note that the extra validation dataset is not a requirement in meta-learning as we can use a subset of pseudo-labeled training data as
the validation data~\cite{xu2021faster}. 
How to properly select a high-quality pseudo-labeled set and how it affects the algorithm compared to the external validation set should be investigated in the future.
Future study should also analyze the role of L2B in conjunction with other meta-learning-based instance reweighting methods such as MW-Net~\cite{shu2019meta}, and semi-supervised/self-supervised learning methods~\cite{li2020dividemix,Li_2021_ICCV}, for furthering the performance.


\section{Conclusion}
In this paper, we present L2B, a simple and effective method for training noise-robust models. 
Concretely, we propose a novel and generic learning objective to enable joint reweighting of instances and labels for combating the label noise in deep representation learning. A meta process is employed to dynamically adjust the per-sample importance weight between real observed labels and pseudo-labels.
Our L2B outperforms prior instance reweighting or label reweighting works under both synthetic and real-world noise with almost no extra cost. 






\clearpage
\appendix

\section*{Appendix}

\section{Equivalence of the two learning objectives}
We show that Eq.~\eqref{eq:theta_star} is equivalent with Eq.~\eqref{Eqn:reed_hard_loss} when $\forall i\  \alpha_i+\beta_i=1$. For convenience, we denote $y_i^{\text{real}},y_i^{\text{pseudo}},\mathcal{F}(x_i,\theta)$ using $y_i^r,y_i^p,p_i$ respectively. 
\begin{align}
&\alpha_i\mathcal{L}(p_i,y_i^r)+\beta_i\mathcal{L}(p_i,y_i^p)=\sum_{l=1}^L\alpha_i y_{i,l}^r\log p_{i,l}\\&+\beta_i y_{i,l}^p\log p_{i,l} 
=\sum_{l=1}^L(\alpha_i y_{i,l}^r+\beta_i y_{i,l}^p)\log p_{i,l}
\end{align}
Due to that $\mathcal{L}(\cdot)$ is the cross-entropy loss, we have $\sum_{l=1}^L y_{i,l}^r=\sum_{l=1}^L y_{i,l}^p=1$. Then $\sum_{l=1}^L\alpha_i y_{i,l}^r+\beta_i y_{i,l}^p=\alpha_i+\beta_i$. So if $\alpha_i+\beta_i=1$, we have 
\begin{align}
\sum_{l=1}^L(\alpha_i y_{i,l}^r+\beta_i y_{i,l}^p)&\log p_{i,l}=\mathcal{L}(p_i,\alpha_i y_i^r+\beta_i y_i^p)\\
&=\mathcal{L}(p_i,(1-\beta_i) y_i^r+\beta_i y_i^p)
\end{align}

\section{Gradient used for updating $\theta$}
We derivative the update rule for $\boldsymbol{\alpha},\boldsymbol{\beta}$ in Eq.~\eqref{eq:theta_t+1}. 
\begin{align}
\alpha_{t,i}&=-\eta\frac{\partial}{\partial \alpha_i}(\sum_{j=1}^m f_j^v(\hat{\theta}_{t+1}))\Big |_{\alpha_i=0} \\
&=-\eta\sum_{j=1}^m \nabla f_j^v(\hat{\theta}_{t+1})^T\frac{\partial \hat{\theta}_{t+1}}{\partial \alpha_i}\Big |_{\alpha_i=0} \\ 
&=-\eta\sum_{j=1}^m \nabla f_j^v(\hat{\theta}_{t+1})^T\\
&\frac{\partial( \theta_t - \lambda \nabla (\sum_k \alpha_k \ f_{k}(\theta)+\beta_k\ g_{k}(\theta))\Big |_{\theta=\theta_t}) }{\partial \alpha_i}\Big |_{\alpha_i=0}\\
&=\eta\lambda\sum_{j=1}^m \nabla f_j^v(\theta_t)^T \nabla f_i(\theta_t)
\end{align}

\begin{align}
\beta_{t,i}&=-\eta\frac{\partial}{\partial \beta_i}(\sum_{j=1}^m f_j^v(\hat{\theta}_{t+1}))\Big |_{\beta_i=0} \\
&=-\eta\sum_{j=1}^m \nabla f_j^v(\hat{\theta}_{t+1})^T\frac{\partial \hat{\theta}_{t+1}}{\partial \beta_i}\Big |_{\beta_i=0} \\ 
&=-\eta\sum_{j=1}^m \nabla f_j^v(\hat{\theta}_{t+1})^T \\
&\frac{\partial( \theta_t - \lambda \nabla (\sum_k \alpha_k \ g_{k}(\theta)+\beta_k\ g_{k}(\theta))\Big |_{\theta=\theta_t}) }{\partial \beta_i}\Big |_{\beta_i=0}\\
&=\eta\lambda\sum_{j=1}^m \nabla f_j^v(\theta_t)^T \nabla g_i(\theta_t)
\end{align}

Then $\theta_{t+1}$ can be calculated by Eq.~\eqref{eq:theta_t+1} using the updated $\alpha_{t,i},\beta_{t,i}$. 
\section{Convergence}
This section provides the proof for covergence (Theorem~\ref{theorem:convergence})

\begin{theorem*}
Suppose that the training loss function $f,g$ have $\sigma$-bounded gradients and the validation loss $f^v$ is Lipschitz smooth with constant L. With a small enough learning rate $\lambda$, the validation loss monotonically decreases for any training batch $B$, namely, 
\begin{equation}\label{eq:Validation_loss_decrease_app}
G(\theta_{t+1})\leq G(\theta_t),
\end{equation}
where $\theta_{t+1}$ is obtained using Eq.~\eqref{eq:theta_t+1} and $G$ is the validation loss
\begin{equation}\label{eq:Validation_loss_app}
G(\theta)=\frac{1}{M}\sum_{i=1}^M f_i^v(\theta),
\end{equation}

Furthermore, Eq.~\eqref{eq:Validation_loss_decrease_app} holds for all possible training batches only when the gradient of validation loss function becomes $0$ at some step $t$, namely, $G(\theta_{t+1})=G(\theta_t)\ \forall B \Leftrightarrow \nabla G(\theta_t)=0$
\end{theorem*}

\textit{Proof.} At each training step $t$, we pick a mini-batch $B$ from the union of training and validation data with $|B|=n$. From section B we can derivative $\theta_{t+1}$ as follows: 
\begin{align}
\theta_{t+1}&=\theta_t-\lambda\sum_{i=1}^n(\alpha_{t,i}\nabla f_i(\theta_t)+\beta_{t,i}\nabla g_i(\theta_t)) \\
&=\theta_t-\eta\lambda^2M\sum_{i=1}^n(\nabla G^T\nabla f_i \nabla f_i+\nabla G^T\nabla g_i \nabla g_i)
\end{align}

We omit $\theta_t$ after every function for briefness and set $m$ in section B equals to $M$. Since $G(\theta)$ is Lipschitz-smooth, we have 
\begin{equation}
    G(\theta_{t+1})\leq G(\theta_t)+\nabla G^T \Delta \theta+\frac{L}{2}||\Delta \theta||^2.
\end{equation}


Then we show $\nabla G^T \Delta \theta+\frac{L}{2}||\Delta \theta||^2\leq 0$ with a small enough $\lambda$. Specifically, \begin{equation}
\nabla G^T \Delta \theta = -\eta\lambda^2M\sum_i(\nabla G^T \nabla f_i)^2+(\nabla G^T \nabla g_i)^2.
\end{equation}

Then since $f_i,g_i$ have $\sigma$-bounded gradients, we have 
\begin{align}
\frac{L}{2}||\Delta \theta||^2&\leq \frac{L\eta^2\lambda^4M^2}{2}\sum_i (\nabla G^T \nabla f_i)^2||\nabla f_i||^2 \\
&+(\nabla G^T \nabla g_i)^2||\nabla g_i||^2\\
&\leq \frac{L\eta^2\lambda^4M^2\sigma^2}{2}\sum_i (\nabla G^T \nabla f_i)^2 +(\nabla G^T \nabla g_i)^2
\end{align}

Then if $\lambda^2 < \frac{2}{\eta \sigma^2 ML}$, 
\begin{align}
\nabla G^T \Delta \theta+\frac{L}{2}||\Delta \theta||^2&\leq (\frac{L\eta^2\lambda^4M^2\sigma^2}{2}-\eta\lambda^2M) \\&
\sum_i(\nabla G^T \nabla f_i)^2+(\nabla G^T \nabla g_i)^2\leq 0.
\end{align}

Finally we prove $G(\theta_{t+1})=G(\theta_t)\ \forall B \Leftrightarrow \nabla G(\theta_t)=0$: 
If $\nabla G(\theta_t)=0$, from section B we have $\alpha_{t,i}=\beta_{t,i}=0$, then $\theta_{t+1}=\theta_t$ and thus $G(\theta_{t+1})=G(\theta_t)\ \forall B$. Otherwise, if $\nabla G(\theta_t)\neq 0$, we have 
\begin{equation}
    0<||\nabla G||^2=\nabla G^T \nabla G = \frac{1}{M}\sum_{i=1}^M\nabla G^T \nabla f_i^v,
\end{equation}
which means there exists a $k$ such that $\nabla G^T\nabla f_k^v>0$. So for the mini-batch $B_k$ that contains this example, we have

\begin{align}
    G(\theta_{t+1})-G(\theta_t)&\leq \nabla G^T\Delta\theta+\frac{L}{2}||\Delta\theta||^2 \\
    &\leq (\frac{L\eta^2\lambda^4M^2\sigma^2}{2}-\eta\lambda^2M)\\
    &\sum_{i\in B}(\nabla G^T\nabla f_i)^2+(\nabla G^T\nabla g_i)^2 \\
    &\leq (\frac{L\eta^2\lambda^4M^2\sigma^2}{2}-\eta\lambda^2M) \nabla G^T \nabla f_k^v \\
    &<0.
\end{align}




\bibliographystyle{plain}
{\small
	\bibliography{egbib}
}



\end{document}
