\documentclass{article}


\PassOptionsToPackage{table, usenames, dvipsnames}{xcolor}

\usepackage{pdfpages} \usepackage{pgffor} 

\makeatletter
\AtBeginDocument{\let\LS@rot\@undefined}
\makeatother

\def\supplementfilename{supplementary.pdf}

\pdfximage{\supplementfilename}
\def\numbersupplementpages{\the\pdflastximagepages}

\newif\ifarXiv
\arXivtrue 







\usepackage[nonatbib,preprint]{neurips_data_2023}









\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[table, usenames, dvipsnames]{xcolor}         \usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{media9}
\usepackage{subcaption}
\usepackage{footmisc}
\usepackage{wrapfig}



\usepackage{multirow}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{pifont}\usepackage{balance}
\usepackage{arydshln}  

\usepackage[capbesideposition=outside,capbesidesep=quad]{floatrow}		

\usepackage{xspace} \usepackage{xfrac}


\def\eg{\emph{e.g.}\xspace} \def\Eg{\emph{E.g.}\xspace}
\def\ie{\emph{i.e.}\xspace} \def\Ie{\emph{I.e.}\xspace}
\def\etc{\emph{etc.}\xspace}
\def\wrt{w.r.t.}
\def\etal{\emph{et al.}\xspace}


\newcommand{\cmark}{\textcolor{OliveGreen}{\ding{51}}}\newcommand{\xmark}{\textcolor{red}{\ding{55}}}

\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\Lb}{\pazocal{L}}
\newcommand{\Nb}{\pazocal{N}}
\newcommand{\Sb}{\pazocal{S}}
\newcommand{\fb}{\pazocal{F}}
\newcommand{\Rho}{\mathrm{P}}


\newcommand{\martin}[1]{\textcolor{purple}{\textbf{Martin:} #1}}

\newcommand{\boldparagraph}[1]{\vspace{0.0em}\noindent{\bf #1}}

\colorlet{colorFst}{Green!25}       \colorlet{colorSnd}{SpringGreen!45} \colorlet{colorTrd}{Yellow!30}      \colorlet{colorLow}{darkgray!30}    \newcommand{\fs}{\cellcolor{colorFst}\bf}   \newcommand{\nd}{\cellcolor{colorSnd}}      \newcommand{\rd}{\cellcolor{colorTrd}}      \newcommand{\lo}{\color{colorLow}}          




\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\title{The Drunkard’s Odometry: Estimating Camera Motion in Deforming Scenes}


\author{David Recasens \\
  University of Zaragoza\\  
  \And
  Martin R.~Oswald \\
  ETH Zurich, University of Amsterdam \\
  \And
  Marc Pollefeys \\
  ETH Zurich, Microsoft \\
  \And
  Javier Civera \\
  University of Zaragoza \\  
}

\begin{document}

\maketitle

\begin{abstract}
    Estimating camera motion in deformable scenes poses a complex and open research challenge. 
    Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality.
    We further present a novel deformable odometry method, dubbed the Drunkard’s Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations.
    In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data.
    Dataset and code: \url{https://davidrecasens.github.io/TheDrunkard'sOdometry/}
\end{abstract}




\section{Introduction} \label{sec:intro}
\begin{figure}
  \centering
  \footnotesize
  \setlength{\tabcolsep}{0pt}
  \setlength\extrarowheight{55pt}
  \begin{tabular}{cc}
    \rotatebox[origin=c]{90}{\hspace{22pt}camera view} &
    \multirow{2}{*}[0pt]{
    \includemedia[
      width=0.96\linewidth,    
      activate=pageopen,
      addresource=Images/teaserLow2.mp4,
      flashvars={
        source=Images/teaserLow2.mp4
      }
    ]{\includegraphics{Images/thumbnail_low.jpg}}{VPlayer.swf}
    } \\
    \rotatebox[origin=c]{90}{\hspace{20pt}3rd person view}
  \end{tabular}
  \setlength{\tabcolsep}{35pt}
  \setlength\extrarowheight{0pt}
  \begin{tabular}{cccc}
    ~~~~~Level 0 & Level 1 & Level 2 & Level 3 \-6pt]
    \caption{\textbf{Sample images of the Drunkard's Dataset} with their corresponding depth, optical flow and normal maps. Non-rigid deformations are simulated by smooth deformations of all scene parts.}
    \label{fig:sampled_images}    
\end{figure*}
The Drunkard's Dataset is a publicly available set of  different camera trajectory recordings of  different deforming indoor scenes, where each one has been recorded four times, one for each difficulty level. For each level, we generated over K frames and recorded camera poses, RGB images, depths, optical flow and normal maps at , being the camera poses and the depth in real-world metric scale consistent throughout different scenes. Find sample images in Figure~\ref{fig:sampled_images}.

\begin{table}    
    \centering
    \scriptsize
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{0.8}
    \floatbox[\capbeside]{table}[0.4\columnwidth]{\caption{\textbf{Overview of the four difficulty levels.} Deformation and camera trajectory noises increases with higher levels. At level 0 there is neither deformation nor camera noise, i.e.\ it represents a rigid scene and a smooth motion.} 
    \label{tab:table_drunk_levels}}{
    \begin{tabular}{ccc}
        \toprule
        Difficulty level & Deformations & Trajectory noise \\
        \midrule
        Level 0 & 0  & 0    \\
        \rd Level 1 & \rd Low     & \rd Low    \\
        \nd Level 2 & \nd Medium  & \nd Medium \\
        \fs Level 3 & \fs High    & \fs High   \\     
        \bottomrule
    \end{tabular}}    
\end{table}


We used Blender~\cite{blender} to render the deformations of the  real-world scanned indoor 3D models of the Habitat-Matterport 3D dataset~\cite{ramakrishnan2021habitat}.
We manually designed camera trajectories such that every room in each building is visited once and the camera trajectory ends at the starting point, except for scenes ,  and , in which the camera traverses the building three times, but in each loop visiting the rooms in different order. The Blender files are publicly available, along with the scripts we used for generating them, so that anyone can render in-house versions of the Drunkard's Dataset scenes modifying parameters of deformations, pose trajectory, resolution or camera type among others.

As Table~\ref{tab:table_drunk_levels} details, Level 0 stands for zero deformation and camera noise, resulting in a rigid scene and a smooth camera motion, well suited for rigid SfM/SLAM methods. 
The following levels have an increasing degree of deformation and trajectory noise. Having four levels of difficulty allows to benchmark both rigid and non-rigid methods in a graduated manner.


\section{The Drunkard's Odometry}  \label{sec:drunkSLAM}
Given a pair of RGB-D images ,  and , , our Drunkard's Odometry estimates a dense scene flow  between them, and the relative camera motion  iteratively ( stands for the iterative block step). 
 contains pixel-wise rigid-body transformations that ideally, i.e.\ in absence of noise, maps every 3D point  --corresponding to pixel , back-projected from the pixel with image coordinates  and its sensor depth -- to its ground truth equivalent 3D point  back-projected from the true associated pixel with image coordinates  and sensor depth .  stands for the inverse projection model. As the estimated scene flow  might be affected by noise, we will denote as 
the transformation of  to the local frame of , that in general will not coincide the true corresponding point .  is composed by the rigid transformation coming from the camera motion  and the one coming from the non-rigid surface deformations , so that . If the scene is rigid , and if it is deforming . 

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{Images/Overview_drunk3_.pdf}
    \caption{\textbf{Overview of the Drunkard's Odometry architecture.} 
    }
    \label{fig:overview}
\end{figure*}

Our iterative pose estimation model, shown in Figure~\ref{fig:overview}, is based on the foundations of the 3D flow estimation architecture of RAFT-3D~\cite{teed2021raft}, which do not have pose estimation capabilities at all. Firstly, a pose regression network encodes both color images  and  and outputs a initial pre-estimate for the camera motion . This rigid transformation is used for initialization, so that for any  pixel of camera 1  . Additionally, we encode both images  and  in two feature maps, which we use to build a 4D correlation pyramid between the features of all pixel pairs at four different scales, each scale halving the resolution of the previous one. From  and   at each iteration , we can obtain dense 2D pixel correspondences 

where  stands for the pinhole camera projection model. At the beginning of each iteration, these correspondences are used to sample the correlation features from the fixed 4D correlation pyramid and it is one of the inputs of the update block. The estimated optical flow  coming from these correspondences is also an input for the update block, being . 

The update block takes also as input an inverse depth residual obtained from the difference between the estimated inverse depth map  and the sensor inverse depth , . Values for  are interpolated from the grid defined by .   acts as selector of the third dimension of the 3D point.

A pair of context features that extracts semantic and contextual information from image 1 are also pre-rendered before entering the iterative process using a context encoder network and given to the update block. One context feature map is kept fix during all iterations and another is used as initialization of the hidden state of the General Recurrent Unit (GRU)~\cite{cho2014properties}, which is at the heart of the update block (see the network fine-grained details in the supplementary material). 

At each iteration , the estimates for  and  are mapped to the Lie algebra with a logarithm map to result in a twist field  and  before being given to the update block. The update operator outputs a set of updates for the optical flow , for the twist camera pose  and for the inverse depth , the hidden state of the recurrent network, a mask and a set of rigid-motion embeddings and confidence maps. These last two maps, next to the updated estimate of  and , and  are used by the least-squares optimizer block to update  (see details in \cite{teed2021raft} for this update). Scene flow is updated as .

Internally the network works at  of the original resolution, and the estimated mask at each iteration of the update block is used to perform a convex upsampling to the original resolution of  and intermediate updated 2D flow  by the update block. 

The supervision comes from comparing the estimated optical flow  obtained from   and  with Eq.~\ref{eq:1} and the intermediate pre-estimated  with the ground truth optical flow , the inverse depth error between  and , the relative camera pose error of  and the initial guess pre-estimated by the pose network  against . The total loss results in

with  being the optical flow loss term,  the inverse depth loss term, and  and  the relative camera pose loss terms.  stands for the relative weight of the  loss term and  weights each loop.



\section{Experiments}  \label{sec:experiments}
In this section, we show the evaluation results of our Drunkard's Odometry against several relevant baselines in two non-rigid datasets: our synthetic Drunkard's and the real Hamlyn data~\cite{mountney2010three,stoyanov2005soft,stoyanov2010real,pratt2010dynamic}.

\begin{table*}[!b]    
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0.6pt}
    \newcommand{\groupsep}{\hskip 6pt}
    \begin{tabular}{cll cccc@{\groupsep} cccc@{\groupsep} cccc@{\groupsep} cccc}
        \toprule
        \multicolumn{3}{c}{} & \multicolumn{4}{c}{\textbf{Level 0}} & \multicolumn{4}{c}{\textbf{Level 1}} & \multicolumn{4}{c}{\textbf{Level 2}} & \multicolumn{4}{c}{\textbf{Level 3}} \\        
        \cmidrule(lr){4-7} \cmidrule(lr){8-11} \cmidrule(lr){12-15} \cmidrule(lr){16-19} 
         &  &  & frames & RPE & RPE & ATE & frames & RPE & RPE & ATE & frames & RPE & RPE & ATE & frames & RPE & RPE & ATE \\
\multicolumn{1}{c}{\textbf{Scene}} & \multicolumn{1}{l}{\textbf{Method}} & \multirow{-2}{*}{\textbf{\makecell{Align-\\ment}}} & [\%] & [cm] & [º] & [m] & [\%] & [cm] & [º] & [m] & [\%] & [cm] & [º] & [m] & [\%] & [cm] & [º] & [m] \\            
        \midrule             
         & COLMAP~\cite{schoenberger2016sfm}  &   & 42  &   \nd 0.36  &    \fs 0.10  &   \fs 0.11  &   42  &   \nd 0.85  &   \nd 0.18  &   \fs 0.12  &   42  &   \nd 1.89  &   \nd 0.35  &   \fs 0.81  &   25  &   3.64  &   \nd 0.51  &   \nd 1.14   \\ 
         & DROID-SLAM~\cite{teed2021droid}  &                                             & 100  &   \rd 0.77  &  \nd 0.28  &  2.38  &   100  &  \rd 1.41  &  \rd 0.42  &  \rd 1.79  &   100  &   2.53  &  \rd  0.69  &   \nd 1.26  &   100  &  \rd 3.58  &  \rd 1.01  &   \fs 1.00   \\ 
         & EDaM~\cite{recasens2021endo}   &     & 100  &   1.83  &  \rd 1.21  &  \rd 1.49  &   100  &   1.82  &   1.37  &   1.83  &   100  &  \rd  2.05  &   1.72  &   1.95 &   100  &   \nd 2.66  &   2.27  &   2.01  \\ 
         \multirow{-4}{*}{0}& Drunkard’s Odometry  &    & 100  &   \fs 0.34  &   \fs 0.10  &   \nd 0.67  &   100  &   \fs 0.59  &   \fs 0.16  &   \nd 1.08  &  100  &   \fs 1.14  &   \fs 0.28  &  \rd 1.35 &   100  &   \fs 1.82  &   \fs 0.48  &  \rd 1.74  \\          
        \noalign{\vskip 1mm} 
        \hdashline 
        \noalign{\vskip 1mm}      
         & COLMAP~\cite{schoenberger2016sfm}  &    & 32  &   \fs 0.38  &   \fs 0.083  &   \fs 0.10  &   32  &   \nd 1.2  &   \fs 0.15  &   \fs 0.24  &   32  &   \nd 3.12  &   \nd 0.30  &   \fs 0.90  &   23  &  \rd 8.59  &   \nd 0.71  &   \fs 2.31   \\ 
         & DROID-SLAM~\cite{teed2021droid}   &   &  0  &   -  &   -  &  -  &   0  &   -  &   -  &   -  &   0  &   -  &   -  &   -  &   0  &   -  &   -  &   -   \\ 
         & EDaM~\cite{recasens2021endo}   &   & 100  &  \rd 5.50  &  \rd 2.16  &  \rd 4.85  &  100  &  \rd 5.27  &  \rd 2.27  &  \rd 4.81  &   100  &   \rd 5.39  &  \rd 2.56  &  \rd 4.79 &   100  &   \nd 5.88  & \rd 2.96  &  \rd 4.90  \\ 
         \multirow{-4}{*}{4}& Drunkard’s Odometry  &      & 100  &   \nd 0.60  &   \nd 0.14  &   \nd 1.21  &   100  &   \fs 0.83  &   \nd 0.18  &   \nd 1.39  &   100  &   \fs 1.43  &   \fs 0.28  &   \nd 2.46 &   100  &   \fs 2.26  &   \fs 0.46  &   \nd 4.66  \\ 
        \noalign{\vskip 1mm} 
        \hdashline 
        \noalign{\vskip 1mm}      
         & COLMAP~\cite{schoenberger2016sfm}  &    & 100  &   \fs 0.40  &   \fs 0.08  &   \fs 0.20  &   80  &   \nd 1.12  &   \fs 0.16  &   \fs 0.53  &   100  &   \rd 3.58  &   \nd 0.356  &   \nd 1.38  &   31  &   4.95  &   \fs 0.46  &   \fs 2.45   \\ 
         & DROID-SLAM~\cite{teed2021droid} &    & 100  &   \rd 0.56  &   \rd 0.21  &  \rd 1.25  &   100  &   \rd 1.52  &  \rd 0.39  &  \rd 1.56  &   100  &   \nd 3.16  & \rd 0.67  &  \rd 2.43  &   100  &  \rd 4.69  &  \rd 1.02  & \nd 2.70   \\ 
         & EDaM~\cite{recasens2021endo}   &     & 100  &   3.05  &   1.98  &   2.82  &   100  &   3.13  &   2.11  &   2.73  &   100  &   3.57  &   2.46  &   2.99 &   100  &   \nd 4.12  &   2.98  &   \rd 2.86  \\ 
         \multirow{-4}{*}{5}& Drunkard’s Odometry  &    & 100  &   \nd 0.45  &   \nd 0.13  &   \nd 0.47  &   100  &   \fs 0.74  &   \nd 0.18  &   \nd 0.70  &   100  &   \fs 1.44  &   \fs 0.29  &   \fs 1.24 &   100  &   \fs 2.40  &   \nd 0.49  &   \fs 2.45  \\
        \bottomrule
    \end{tabular}\vspace{-6pt}
    \caption{\textbf{Trajectory errors for Drunkard's test scenes for all difficulty levels.}
    Note that COLMAP is an offline method and is only shown for reference. Our odometry method mostly outperforms the compared online odometry methods.
    Best results are highlighted as \colorbox{colorFst}{\bf first}, \colorbox{colorSnd}{second}, and \colorbox{colorTrd}{third}.}
\label{tab:table_drunk_eval}
\end{table*}



\boldparagraph{Drunkard's Setup.}
We trained our Drunkard's Odometry in all scenes of the Drunkard's Dataset except of the test ones, with around  ratio for training and testing, respectively, of the difficulty level 1 with an input resolution of , batch size of , learning rate of , Adam optimizer~\cite{loshchilov2017decoupled}, weight decay of ,  iterations of the iterative block during training and test, hyperparameters , ,  and , and during  epochs ( days) on a single RTX Nvidia Titan. An ablation study available in the supplementary material was performed to obtain the hyperparameter values. Everything was trained from scratch except for the pose network encoder which was pretrained on ImageNet~\cite{deng2009imagenet}. 


\boldparagraph{Drunkard's Benchmark.}
We put to fight our Drunkard's Odometry in the Drunkard's Dataset benchmark against the gold-standard SfM pipeline COLMAP~\cite{schoenberger2016sfm, schoenberger2016mvs}, that searches for matches across all images in an offline manner using only the RGB channels of our images; the robust and accurate DROID-SLAM~\cite{teed2021droid} that uses a combination of local-online and global-offline bundle adjustment refinements trained in also in virtual environments with optical flow and evaluated using the same RGB-D images as Drunkard's Odometry; and the frame-to-frame tracking designed for non-rigid endoscopic scenes Endo-Depth-and-Motion (EDaM)~\cite{recasens2021endo} that here uses the RGB plus the single-view estimated depth maps using a network trained self-supervisely on monocular images of KITTI~\cite{geiger2013vision}. The non-rigid SfM SD-DefSLAM~\cite{gomez2021sd} was tested but fails in the beginning of the sequences. Complex non-rigid optimization methods such as this one are unstable and tend to fail in difficult sequences with complicated 3D surfaces and abrupt camera trajectories like the ones in our Drunkard's Dataset.

We chose scenes 0, 4 and 5 as the test ones with ,  and  frames respectively. The rest of the scenes were used for training. The trajectories estimated by COLMAP and EDaM are compared against the ground truth after  alignment, as they are up-to-scale. DROID-SLAM and our Drunkard's are aligned to the ground truth with a  transformation, as the RGB-D input allows them to estimate the real scale. The reported metrics are: Relative Position Error (RPE) for translation and rotation, that measures the local accuracy of the estimated trajectory against the ground truth between consecutive frames, and the Absolute Trajectory Error (ATE) for translation that computes the global consistency between both trajectories (see \cite{prokhorov2019measuring} for details).

For each sequence, the percentage of registered frames over the total is shown, a metric in which COLMAP shows poor performance. As a consequence, its trajectory metrics are influenced positively as it excludes frames that are challenging to track and probably would have increased the error. This is beneficial in particular for the ATE, as it takes into account the global consistency rather than frame-to-frame errors like the RPE, and happens earlier in higher deforming scenes. Also note that DROID-SLAM is very GPU-memory demanding, in part because of the final global bundle adjustment and it is not able work with long sequences like scene 4. However, our Drunkard's Odometry and EDaM are more robust, partly due to tracking only between adjacent frames.

Table~\ref{tab:table_drunk_eval} shows our results. Note that our Drunkard's Odometry practically always outperforms DROID-SLAM and EDaM in RPE and ATE, also in rigid scenes, even if our model is trained exclusively in non-rigid scenes and does not use loop closure or full bundle adjustment. 
The gap is significantly larger at higher deformation levels, for which the Drunkard's Odometry errors increase much less. This demonstrates that our method is able to generalize at predicting surface deformations. Only COLMAP is able to outperform our Drunkard's Odometry in ATE. Note, in any case, that COLMAP has a much lower recall, as it only estimates camera motion for a substantially lower percentage of frames. If we focus on RPE, a more fair metric in this case, our Drunkard's Odometry is on par to COLMAP at lower deformation levels, and clearly outperforms it at higher ones. 

 
\boldparagraph{Validation in real endoscopies.}
We used the Hamlyn dataset~\cite{stoyanov2005soft}, that contains intracorporeal endoscopic RGB scenes with weak textures, deformations and reflections. Specifically, we chose scenes 1 and 17 (see Figure~\ref{fig:hamlyn}), which are significant exploratory ones. Most of Hamlyn's videos have very small camera motions, being of no interest for benchmarking odometry methods. We slightly cropped the images to remove black pixels at the borders. Depth data was taken from the public tracking test data of EDaM~\cite{recasens2021endo} which was estimated by a single-view dense depth network trained in a self-supervised manner in all Hamlyn scenes except for the test ones. Note that this depth does not have the same quality as the real ground truth one from the Drunkard's Dataset. 

\begin{figure}[!b]
  \centering
  \newcommand{\imgheight}{3.1cm}
  \subfloat[][Scene 1 frame]{
  \includegraphics[height=\imgheight]{Images/test1.jpg}
    \label{subfig:image1}
    }
  \subfloat[][Scene 17 frame]{
    \includegraphics[height=\imgheight]{Images/test17.jpg}
    \label{subfig:image2}
    }
  \subfloat[][Tracking Results]{
    \scriptsize
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cclc}
        \toprule 
        Scene & \#Frames & Method &  \\        
        \midrule        
         &   &   EDaM~\cite{recasens2021endo} & \nd 0.0044  \\ 
         &   &   DROID-SLAM~\cite{teed2021droid} & \fs 0.0019  \\ 
         &   &   Drunkard’s trained w/o deform. & 0.0055  \\         
        \multirow{-4}{*}{1} & \multirow{-4}{*}{1058}   &  Drunkard’s trained w/ deform. & \rd 0.0045  \\
        \noalign{\vskip 1mm} 
        \hdashline
        \noalign{\vskip 1mm} 
        &   &   EDaM~\cite{recasens2021endo} & \nd 0.0236  \\ 
        &   &   DROID-SLAM~\cite{teed2021droid} & 0.0338  \\ 
        &   &   Drunkard’s trained w/o deform. & \rd 0.0309  \\
        \multirow{-4}{*}{17} & \multirow{-4}{*}{1306}   &   Drunkard’s trained w/ deform. & \fs 0.0214 \\
        \bottomrule
    \end{tabular}\label{subfig:tableHamlyn}
    }
    \vspace{-8pt}
  \caption{\textbf{Sample frames and  results for the Hamlyn test videos.}}
  \label{fig:hamlyn}
\end{figure}
Since the Hamlyn dataset lacks ground truth camera poses we propose a novel ground truth-free trajectory metric to measure the quality of the estimated odometries. 
The key idea is to generate loopy videos by duplicating and reversing a given image sequence and concatenating it to the end of the original sequence.
The generated "palindrome video" is twice as long as the original sequence and any tracking method should ideally loop back to its starting position. We then simply measure the discrepancy between start and end pose and accumulate it over various loop lengths.
We denote the new metric as the Absolute Palindrome Trajectory Error () and define it as

where  is the total number of frames and  the translation Root Mean Squared Error (RMSE) of between the first and last pose of the sub-trajectory loop . This last pose of the loop  is the result of sequentially applying to the starting camera pose --the identity -- the first  estimated relative camera poses  for the scene plus the first  estimated relative camera poses for the same scene but ran backwards .
Then, the  is the module of the translation of this last estimated pose.
We note that the  error is trivially minimized if all transformations are zero, but this case is easy to check.

Table~\ref{subfig:tableHamlyn} shows our  metric in the Hamlyn test scenes for the baselines that are able to track the whole scene (EDaM and DROID-SLAM) and our Drunkard's Odometry trained only in rigid scenes of the Drunkard's Dataset, i.e.\ scenes from difficulty level 0, trained in deformable scenes, specifically from level 1. Before computing the , all the trajectories are scaled with the one given by a  alignment, having as reference the trajectory estimated by EDaM, since it showed a good qualitative tracking performance in Endo-Depth-and-Motion~\cite{recasens2021endo}. 

Figure~\ref{fig:apte_plots} shows how  values evolve as the length of the loops grows. Notice that a longer loop does not always mean higher . This is because there can be some specific problematic frames with a bad pose estimation that induce a drift accumulation in the following ones getting worse  measures. However, this can be reversed if the tracking recovers partially during the way back in the loop. This is especially remarkable for DROID-SLAM in scene 1. It has some very unstable pose estimations, but is able to relocate later. This is probably thanks to its global bundle adjustments that readjusts all poses in the trajectory together and that works great in a scene were the same place is revisited constantly. The single  values shown in the main paper are the average of all the  measures.

\begin{figure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=1.\linewidth]{Images/apte1.pdf}  \-6pt]
  \caption{\footnotesize  plots in Hamlyn's scene 17.}
    \label{fig:apte2}
\end{subfigure}
\vspace{-6pt}
\caption{\textbf{Camera tracking performance on the Hamlyn dataset.}  values along the  different -frames-length-loops in Hamlyn's scenes 1 (a) and 17 (b) for EDaM~\cite{recasens2021endo}, DROID-SLAM~\cite{teed2021droid} and Drunkard's Odometry (ours) with and without having been trained in deformable scenes of the Drunkard's Dataset (i.e.\ trained in level 1 and 0, respectively). The training on deforming scene substantially improves the performance of our method.}
\label{fig:apte_plots}
\end{figure}

The first conclusion we can extract is that our model performs better if it is trained in non-rigid scenes rather than rigid. Again, this shows its capacity to learn deformation patterns and retrieve a more stable camera pose under real-world non-rigid challenging scenes, and even trained in a different domain. In scene 1 the same area is revisited several times, i.e.\ there are many near loop closures always around the same place, which benefits DROID-SLAM that applies local and global bundle adjustment, hence avoiding drift. However, if there are no recurrent near loop closures and the camera is moving sharply as in scene 17, DROID-SLAM loses its advantage and EDaM and, specifically, Drunkard's Odometry outperforms it.
SD-DefSLAM~\cite{gomez2021sd} was also tested here and breaks after a few frames, far from the full lenght of the trajectory, even having originally been built and tested in Hamlyn. In consequence, we could not compute the  since we need the full trajectory ran forwards and backwards.


\boldparagraph{Limitations.}
\label{sec:limitations}
Our Drunkard's Dataset's most clear limitation is that it is synthetic. However, as we argued, we believe that the difficulties for acquiring high-quality data with ground truth annotations in the target application domains motivates their use. Notice that is not possible to record true ground truth optical flow in real-world sequences, as we do not have access to the exact pixel motion information. It is precisely this optical flow availability which unlocks the use of powerful flow-based models trained in synthetic deformable data which generalize well to real non-rigid scenes. Despite the real indoor images of the Drunkard's Dataset may resemble real deformable scenes in texture and shape, such as bouncy castles, moving fabrics or canvas, they are far from the medical application environment. Still, we think that generating medical data with realistic deformations, fluids or textures in large scale is out of reach. As a proof, such data does not exist yet and motivates the use of the Drunkard's data.
Our Drunkard's Odometry has all the limitations inherent to a frame-to-frame tracking method. Drift accumulates very quickly, and even if our sequences are loopy we do not either detect loop closures or correct our trajectories based on them. However, the SLAM literature shows that SLAM methods (e.g., \cite{gao2018ldso}) can be build on top of odometry ones (e.g., \cite{engel2017direct}). 


\section{Conclusions}  \label{sec:conclusions}
Estimating camera motion in deformable scenes is a challenging research problem relatively under-explored in the literature, and for which a lack of clear benchmarks slows down research progress.
In this work, we created the Drunkard's Dataset, a large-scale simulated dataset with perfect ground truth and a wide variety of scenes and deformation levels to train and validate deep neural models.
In addition, we propose the Drunkard's Odometry method for deformable scenes to validate our dataset.
The method minimizes a scene flow loss, but as its main contribution, intrinsically decomposes the estimated twist flow into two components: The majority of motion is aimed to be explained by a rigid-body camera motion, and all remaining motion is explained by scene deformations.
In contrast to most existing works our method does not require a static scene part for estimating a reference coordinate frame which is crucial in fully deforming scenarios like endoscopy.
To also assess odometry estimates in the absence of ground truth data, we further define a novel ground-truth-free metric for trajectory evaluation that measures the cyclic consistency of a tracking algorithm.
Both the dataset and source code for our baseline method will be released upon acceptance.
Our experimental results validate our dataset, illustrates its challenges, and also shows that our Drunkard's Odometry is able to outperform relevant baselines. 


{\small
\boldparagraph{Acknowledgements.}
This work was supported by the EU Comission (EU-H2020 EndoMapper GA 863146), the Spanish Government (PID2021-127685NB-I00 and TED2021-131150BI00), the Aragon Government (DGA-T45 17R/FSE), and a research grant from FIFA.
}

\balance

{\small
\bibliographystyle{ieeetr}
\bibliography{egbib}
}
 
\ifarXiv
    \foreach \x in {1,...,\numbersupplementpages}
    {
        \clearpage
        \includepdf[pages={\x}]{\supplementfilename}
    }
\fi

\end{document}
