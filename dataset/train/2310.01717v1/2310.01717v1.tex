\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{diagbox}
\usepackage{tikz}
\usetikzlibrary{tikzmark,calc}
\usepackage[leftcaption]{sidecap}
\graphicspath{ {./} }
\usepackage{refcount}
\usepackage{wrapfig}
\setlength{\intextsep}{0pt}
\usepackage{tablefootnote}
\newcommand{\footnoteagain}[1]{\hyperref[#1]{\footnotemark[\getrefnumber{#1}]}}
\usepackage{pifont}

\newtheorem{theorem}{Theorem}

\usepackage[round]{natbib}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\urlstyle{same}
\usepackage{color}
\usepackage{bm}
\usepackage{caption}
\usepackage{algorithm}

\title{Ensemble Distillation for Unsupervised Constituency Parsing}

\author{Behzad Shayegh \;\; Yanshuai Cao \;\; Xiaodan Zhu \;\; Jackie C.K. Cheung \;\; Lili Mou
\\
Dept. Computing Science, Alberta Machine Intelligence Institute (Amii), University of Alberta\\
Borealis AI\quad
Dept. Electrical and Computer Engineering, Queen’s University\\
Ingenuity Labs Research Institute, Queen’s University\\
Quebec Artificial Intelligence Institute (MILA), McGill University\quad
Canada CIFAR AI Chair\\
\texttt{\href{mailto:the.shayegh@gmail.com}{the.shayegh@gmail.com}} \quad
\texttt{\href{mailto:yanshuai.cao@borealisai.com}{yanshuai.cao@borealisai.com}}\quad
\texttt{\href{mailto:xiaodan.zhu@queensu.ca}{xiaodan.zhu@queensu.ca}}\\
\texttt{\href{mailto:jcheung@cs.mcgill.ca}{jcheung@cs.mcgill.ca}} \quad
\texttt{\href{mailto:doublepower.mou@gmail.com}{doublepower.mou@gmail.com}}
}

\begin{document}
\maketitle

\begin{abstract}
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance.
To this end, we propose a notion of ``tree averaging,'' based on which we further propose a novel ensemble method for unsupervised parsing.
To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods.
Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.\footnote{Code available at \href{https://github.com/MANGA-UOFA/ED4UCP}{\url{github.com/MANGA-UOFA/ED4UCP}}}
\end{abstract}


\section{Introduction}
\label{sec:introduction}

Constituency parsing is a core task in natural language processing (NLP), which interprets a sentence and induces its constituency tree, a syntactic structure representation that organizes words and phrases into a hierarchy~\citep{constituencyTree}. It has wide applications in various downstream tasks, including semantic role labeling~\citep{mohammadshahi-henderson-2023-syntax} and explainability of AI models~\citep{tenney-etal-2019-bert}.
Traditionally, parsing is accomplished by supervised models trained with linguistically annotated treebanks~\citep{charniak-2000-maximum}, which are expensive to obtain and may not be available for low-resource scenarios.
Also, these supervised parsers often underperform when encountering domain shifts. 
This motivates researchers to explore unsupervised methods as it eliminates the need for annotated data.

To address unsupervised parsing, researchers have proposed various heuristics and indirect supervision signals. \citet{clark-2001-unsupervised} employs context distribution clustering to induce a probabilistic context-free grammar~\citep[PCFG;][]{Booth1969ProbabilisticRO}. \citet{klein-manning-2002-generative} define a joint distribution for sentences and parse structures, the latter learned by expectation--maximization (EM) algorithms. \citet{snyder2009unsupervised} address multilingual unsupervised parsing by aligning potential parse trees in a generative model.


In the deep learning era, unsupervised parsing techniques keep advancing. \citet{cao-etal-2020-unsupervised} utilize linguistic constituency tests~\citep{fromkin2003introduction} as heuristics, evaluating all spans as potential constituents for selection. \citet{li-lu-2023-contextual} modify each span based on linguistic perturbations and observe changes in the contextual representations within a masked language model; according to the level of distortion, they determine how likely the span is a constituent. \citet{maveli-cohen-2022-co} use rules to train two classifiers with local features and contextual features, respectively, which are further refined in a co-training fashion. Another way to obtain the parsing structure in an unsupervised way is to treat it as a latent variable and train it in downstream tasks, such as text classification~\citep{li-etal-2019-imitation}, language modeling~\citep{shen2018ordered,kim-etal-2019-unsupervised}, and sentence reconstruction~\citep{drozdov-etal-2019-unsupervised-latent,kim-etal-2019-compound}. Overall, unsupervised parsing is made feasible by such heuristics and indirect supervisions, and has become a curious research direction in NLP.

In our work, we uncover an intriguing phenomenon of low correlation among different unsupervised parsers, despite their similar overall  scores (the main evaluation metric for parsing), shown in Table~\ref{tab:correlation-table}. This suggests that different approaches capture different aspects of parsing structures, and our insight is that combining these parsers may leverage their different expertise, achieving higher performance for unsupervised parsing.

To this end, we propose an ensemble method for unsupervised parsing. We first introduce a notion of ``tree averaging'' based on the similarity of two constituency trees. Given a few existing unsupervised parsers, referred to as \textit{teachers},\footnote{Our full approach involves training a student model from the ensemble; thus, it is appropriate to use the term \textit{teacher} for an ensemble component.} we then propose to use a CYK-like algorithm~\citep{Kasami1965AnER,YOUNGER1967189,MANACHER1978127,sennrich-2014-cyk} that utilizes dynamic programming to search for the tree that is most similar to all teachers' outputs. In this way, we are able to obtain an ``average'' parse tree, taking advantage of different existing unsupervised parsers.

To improve the inference efficiency, we distill our ensemble knowledge into a student model. In particular, we choose the recurrent neural network grammar~\citep[RNNG;][]{dyer-etal-2016-recurrent} with an unsupervised self-training procedure~\citep[URNNG;][]{kim-etal-2019-unsupervised}, following the common practice in unsupervised parsing~\citep{kim-etal-2019-compound,cao-etal-2020-unsupervised}. Our ensemble-then-distill process is able to mitigate the over-smoothing problem, where the standard cross-entropy loss encourages the student to learn an overly smooth distribution \citep{wen-etal-2023-f}. Such a problem exists in common multi-teacher distilling methods~\citep{wu-etal-2021-one}, and it would be especially severe when the teachers are heterogeneous, signifying the importance of our approach.

We evaluated our ensemble method on the Penn Treebank~\citep[PTB;][]{marcus-etal-1993-building} and SUSANNE~\citep{SusanneCorpus} corpora. Results show that our approach outperforms existing unsupervised parsers by a large margin in terms of  scores, and that it achieves results comparable to the supervised counterpart in the domain-shift setting. Overall, our paper largely bridges the gap between supervised and unsupervised parsing.

In short, the main contributions of this paper include: 1) We reveal an intriguing phenomenon that existing unsupervised parsers have diverse expertise, which may be leveraged by model ensembles; 2) We propose a notion of tree averaging and utilize a CYK-like algorithm that searches for the average tree of existing unsupervised parsers; and 3) We propose an ensemble-then-distill approach to improve inference efficiency and to alleviate the over-smoothing problem in common multi-teacher distilling approaches.

\begin{table}[t]

    \begin{center}
        \resizebox{0.9\linewidth}{!}{
            \begin{tabular}{c c c c}
                \cline{2-2}
                \multicolumn{1}{r|}{Compound PCFG} & \multicolumn{1}{c|}{} & & \\
                \cline{2-3}
                \multicolumn{1}{r|}{DIORA} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \\
                \cline{2-4}
                \multicolumn{1}{r|}{S-DIORA} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\
                \cline{2-4}
                &Compound & DIORA & S-DIORA \\
                &PCFG &  & \\
            \end{tabular}
            \quad
            \begin{tabular}{c c c c}
                \cline{2-2}
                \multicolumn{1}{r|}{DIORA\ding{73}} & \multicolumn{1}{c|}{} & & \\
                \cline{2-3}
                \multicolumn{1}{r|}{DIORA\ding{71}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \\
                \cline{2-4}
                \multicolumn{1}{r|}{DIORA\ding{91}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\
                \cline{2-4}
                & DIORA\ding{73} & DIORA\ding{71} & DIORA\ding{91} \\
            \end{tabular}
        }
    \end{center}\vspace{-3pt}
    \caption{Correlation analysis of unsupervised parsers. Numbers are the  score of one model against another on the Penn Treebank dataset~\citep{marcus-etal-1993-building}. The left table considers three heterogeneous models (Compound PCFG, DIORA, and S-DIORA), whereas the right table considers three runs (\ding{73}, \ding{71}, and \ding{91}) of the same model. All their  scores against the ground truth fall within the range of 59--61, thus providing a controlled experimental setting.}
    \label{tab:correlation-table}
\end{table}

\section{Approach}

\subsection{Unsupervised Constituency Parsing}
\label{subsection:unsupConstPars}

In linguistics, a \textit{constituent} refers to a word or a group of words that function as a single unit in a hierarchical tree structure~\citep{constituencyTree}. In the sentence ``The quick brown fox jumps over the lazy dog,'' for example, the phrase ``the lazy dog'' serves as a noun phrase constituent, whereas ``jumps over the lazy dog'' is a verb phrase constituent. In this study, we address \textit{unsupervised constituency parsing}, where no linguistic annotations are used for training. This reduces human labor and is potentially useful for low-resource languages. Following most previous work in this direction~\citep{cao-etal-2020-unsupervised,maveli-cohen-2022-co,li-lu-2023-contextual}, we focus on \textit{unlabeled, binary} parse trees, in which each constituent has a binary branching and is not labeled with its syntactic role (such as a noun phrase or a verb phrase).

The standard evaluation metric for constituency parsing is the  score, which is the harmonic mean of precision and recall~\citep{shen2017neural,SupervisedCRFConstituencyParsing}:

where  and  are predicted and reference trees, respectively, and  is the set of constituents of a tree .

\subsection{A Notion of Averaging Constituency Trees} 

In our study, we propose an ensemble approach to combine the expertise of existing unsupervised parsers (called \textit{teachers}), as we observe they have low correlation among themselves despite their similar overall  scores (Table~\ref{tab:correlation-table}).

To accomplish ensemble binary constituency parsing, we need to define a notion of tree averaging; i.e., our ensemble output is the average tree that is most similar to all teachers' outputs. Inspired by the evaluation metric, we suggest the average tree should have the highest total  score compared with different teachers. Let  be a sentence and  is the th teacher parser. Given  teachers, we define the average tree to be

where  is all possible unlabeled binary trees on sentence , and  is the th teacher's output. 

It is emphasized that only the trees of the same sentence can be averaged. This simplifies the  score of binary trees, as the denominators for both precision and recall are  for a sentence with  words, i.e., . Thus, Eqn.~(\ref{eq:AvgTree}) can be rewritten as:

Here, we define the  function to be the number of times that a constituent  appears in the teachers’ outputs. In other words, Eqn.~(\ref{eq:SimpleAvgTree}) suggests that the best averaging tree should be the one that hits the teachers' predicted constituents most.

\textbf{Discussion on MBR decoding.}
\label{discussion:SelectiveMBR}
Our work can be seen as minimum Bayes risk (MBR) decoding~\citep{MBRbook}. In general, MBR yields an output that minimizes an \textit{expected error} (called \textit{Bayes risk}), defined according to the task of interest. In our case, the error function can be thought of as , and minimizing such-defined Bayes risk is equivalent to maximizing the total hit counts, as shown by Eqn.~(\ref{eq:SimpleAvgTree}).

However, our MBR approach significantly differs from prior MBR studies in NLP. In fact, MBR has been widely applied to text generation~\citep{kumar-byrne-2004-minimum,10.1162/tacl_a_00491,suzgun-etal-2023-follow}, where a set of candidate output sentences are obtained by sampling or beam search, and the best one is selected based on a given error function, e.g., the dissimilarity against others; such an MBR method is \textit{selective}, meaning that the output can only be selected from a candidate set. On the contrary, our MBR is \textit{generative}, as the sentence's entire parse space  will be considered during the  process in (\ref{eq:SimpleAvgTree}). This allows us to obtain the global lowest-risk tree and consequently achieve higher performance. 
Here, the global search is feasible because the nature of tree structures facilitates efficient exact decoding with dynamic programming, discussed in the next subsection.

\subsection{Our CYK Variant}\label{Sec:OurCYK}

As indicated by Eqn.~(\ref{eq:SimpleAvgTree}), our method searches for the constituency tree with the highest total hit counts of its constituents in the teachers' outputs. We can achieve this by a CYK~\citep{Kasami1965AnER,YOUNGER1967189}-like dynamic programming algorithm, because an optimal constituency parse structure of a \textit{span}---a continuous subsequence of a sentence---is independent of the rest of the sentence.

Given a sentence , we denote by  a span starting from the th word and ending right before the th word. Considering a set of teachers , we define a recursion variable 

which is the best total hit count for this span.\footnote{Note that, in Eqns.~(\ref{eq:H})--(\ref{eq:CTbuilding}),  should not be , because the hit count is based on the teachers' sentence-level parsing.} We also define  to be the corresponding locally best parse structure, unambiguously represented by the set of all constituents in it.

The base cases are  and  for , suggesting that the best parse tree of a single-word span is the word itself, which appears in all teachers' outputs and has a hit count of .

For recursion, we see a span  will be split into two subspans  and  for some split position~, because our work focuses on binary constituency parsing. Given , the total hit count for the span  is the summation over those of the two subspans  and , plus its own hit count. To obtain the best split, we need to vary  from  to  (exclusive), given by

where the hit count is a constant for  and can be omitted in implementation. Then, we have

Eqn.~(\ref{eq:CTbuilding}) essentially groups two sub-parse structures  and  for the span . This can be represented as the union operation on the sets of constituents.

The recursion terminates when we have computed , which is the best parse tree for the sentence , maximizing overall similarity to the teachers' predictions and being our ensemble output.
In Appendix~\ref{apdx:CYKalgo}, we summarize our ensemble procedure in pseudocode and provide an illustration. 

\subsection{Ensemble Distillation}
\label{sec:ensembledistillation}

In our work, we further propose an ensemble distilling approach that trains a \textit{student} parser from an ensemble of teachers. This is motivated by the fact that the ensemble model requires performing inference for all teacher models and may be slow.
Specifically, we choose the recurrent neural network grammar~\citep[RNNG;][]{dyer-etal-2016-recurrent} as the student model, which learns shift--reduce parsing operations~\citep{shiftreduce} along with language modeling using recurrent neural networks.  The choice of RNNG is due to its unsupervised refinement procedure~\citep[URNNG;][]{kim-etal-2019-unsupervised}, which treats syntactic structures as latent variables and uses variational inference to optimize the joint probability of syntax and language modeling, given some unlabeled text. Such a self-training process enables URNNG to significantly boost parsing performance. 

Concretely, we treat the ensemble outputs as pseudo-groundtruth parse trees and use them to train RNNG with cross-entropy loss. Then, we apply URNNG for refinement, following previous work~\citep{kim-etal-2019-compound,cao-etal-2020-unsupervised}.

\textbf{Discussion on union distillation.}
\label{discussion:uniondistill}
An alternative way of distilling knowledge from multiple teachers is to perform cross-entropy training based on the union of the teachers' outputs~\citep{wu-etal-2021-one}, which we call \textit{union distillation}. Specifically, the cross-entropy loss between a target distribution~ and a learned distribution  is , which tends to suffer from  an over-smoothing problem~\citep{WeiDialogGenerateShort,wen2022equal,wen-etal-2023-f}: the machine learning model will predict an overly smooth distribution  to cover the support of ; if otherwise  is zero but  is non-zero for some~, the cross-entropy loss would be infinity. Such an over-smoothing problem is especially severe in our scenario, as will be shown in Section~\ref{ss:analysis}, because our multiple teachers are heterogeneous and have different expertise (Table~\ref{tab:correlation-table}). By contrast, our proposed method is an ensemble-then-distill approach, which first synthesizes a best parse tree by model ensemble and then learns from the single best tree given an input sentence.


\section{Experiments}

\subsection{Datasets}

We evaluated our approach on the widely used the Penn Treebank~\citep[PTB;][]{marcus-etal-1993-building} dataset, following most previous work~\citep{shen2018ordered,kim-etal-2019-compound,cao-etal-2020-unsupervised,maveli-cohen-2022-co,li-lu-2023-contextual}. We adopted the standard split: 39,701 samples in Sections 02--21 for training, 1,690 samples in Section 22 for validation, and 2,412 samples in Section 23 for test. It is emphasized that we did not use linguistically labeled trees in the training set, but took the sentences to train teacher unsupervised parsers and to distill knowledge into the student.

In addition, we used the SUSANNE dataset~\citep{SusanneCorpus} to evaluate model performance in a domain-shift setting. Since it is a small, test-only dataset containing 6,424 samples in total, it is not possible to train unsupervised parsers directly on SUSANNE, which on the other hand provides an ideal opportunity for domain-shift evaluation.

We adopted the standard evaluation metric, the  score of unlabeled constituents, as has been explained in Section~\ref{subsection:unsupConstPars}. We used the same evaluation setup as \citet{kim-etal-2019-compound}, ignoring punctuation and trivial constituents, i.e., single words and the whole sentence. We reported the average of sentence-level  scores over the corpus.

\subsection{Competing Methods}

Our ensemble approach involves the following classic or state-of-the-art unsupervised parsers as our teachers, which are also baselines for comparison.

\begin{compactitem}[\quad]
\item \textbf{Ordered Neurons}~\citep{shen2018ordered}, a neural language model that learns syntactic structures with a gated attention mechanism;

\item \textbf{Neural PCFG}~\citep{kim-etal-2019-compound}, which utilizes neural networks to learn a latent probabilistic context-free grammar;

\item \textbf{Compound PCFG}~\citep{kim-etal-2019-compound}, which improves the Neural PCFG by adding an additional sentence-level latent representation;

\item \textbf{DIORA}~\citep{drozdov-etal-2019-unsupervised-latent}, a deep inside--outside recursive auto-encoder that 
marginalizes latent parse structures during encoder--decoder training;

\item \textbf{S-DIORA}~\citep{drozdov2020diora}, a single-tree encoding for DIORA that only considers the most probable tree during unsupervised training;

\item \textbf{ConTest}~\citep{cao-etal-2020-unsupervised}, which induces parse trees by rules and heuristics inspired by constituency tests~\citep{mccawley1998syntactic}; and

\item \textbf{ContexDistort}~\citep{li-lu-2023-contextual}, which induces parsing structures from pretrained masked language models---in particular, the BERT-base model~\citep{devlin-etal-2019-bert} in our experiments---based on contextual representation changes caused by linguistic perturbations.
\end{compactitem}

To combine multiple teachers, we consider several alternatives:
\begin{compactitem}[\quad]
\item \textbf{Selective MBR}, which selects the lowest-risk constituency tree among a given candidate set (Section~\ref{discussion:SelectiveMBR}). In particular, we consider teaches' outputs as the candidates, and we have     . This differs from our MBR approach, which is generative and performs the  over the entire binary tree space, shown in Eqn.~(\ref{eq:AvgTree}).

\item \textbf{Union distillation}, which trains a student from the union of the teachers' outputs (Section~\ref{sec:ensembledistillation}).
\end{compactitem}

For hyperparameters and other setups of previous methods (all teacher and student models), we used default values mentioned in either papers or codebases. It should be emphasized that our proposed ensemble approach does not have any hyperparameters, thus not requiring any tuning.


\subsection{Main Results}


\textbf{Results on PTB.} Table~\ref{tab:main_results} presents the main results on the PTB dataset, where we performed five runs of replication either by loading original authors' checkpoints or by rerunning released codebases. Our replication results are comparable to those reported in previous papers, inventoried in Appendix~\ref{apdx:ExperimentDetail}, showing that we have successfully established a foundation for our ensemble research.

We first evaluate our ensembles of corresponding runs (Row~12), which is a fair comparison against teacher models (Rows~3--9). Without RNNG/URNNG distillation, our method outperforms the best teacher (Row~8) by  points in terms of  scores, showing that our ensemble approach is highly effective and justifying the proposed notion of tree averaging for unsupervised parsing. 

It is also possible to have an ensemble of the best (or worst) teachers across different runs, as the teacher models are all validated by a labeled development set in previous papers. We observe that the ensemble of the best (or worst) teachers achieves slightly higher (or lower) scores than the ensemble of the teachers in corresponding runs, which is intuitive. However, the gap between the best-teachers ensemble and worst-teachers ensemble is small (Rows~13 vs.~14), showing that our approach is not sensitive to the variance of teachers. Interestingly, the ensemble of the worst teachers still outperforms the best single teacher by a large margin of   points. 

We compare our ensemble approach with selective MBR (Row~11), which selects a minimum-risk tree from the teachers' predictions. As shown, selective MBR outperforms all the teachers too, again verifying the effectiveness of our tree-averaging formulation. However, its performance is worse than our method (Row~12), which can be thought of as generative MBR that searches the entire tree space using a CYK-like algorithm.


\begin{table}[t]
\centering
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{r l | c | c c c}
\toprule
& Method & Mean & Run~1 & +RNNG & +URNNG\\
\midrule
1& Left branching &  & -- & -- & --\\
2& Right branching &  & -- & -- & --\\
\midrule
3& Ordered Neurons~\citep{shen2018ordered} &  &  &  & \\
4& Neural PCFG~\citep{kim-etal-2019-compound} &  &  &  & \\
5& Compound PCFG~\citep{kim-etal-2019-compound} &  &  &  & \\
6& DIORA~\citep{drozdov-etal-2019-unsupervised-latent} &  &  &  & \\
7& S-DIORA~\citep{drozdov2020diora} &  &  &  & \\
8& ConTest~\citep{cao-etal-2020-unsupervised} &  &  &  & \\
9& ContexDistort~\citep{li-lu-2023-contextual} &  &  &  & \\
\midrule
10& Union distillation  & -- & -- &  & \\
11& Selective MBR &  &  &  & \\
12& Our ensemble (corresponding run) &  &  &  & \\
13& Our ensemble (worst teacher across runs) &  & -- &  & \\
14& Our ensemble (best teacher across runs) &  & -- &  & \\
\midrule
15& Oracle &  & -- &  &  \\
\bottomrule
\end{tabular}
}
\vspace{5pt}
\caption{ scores on PTB. Teacher models' results are given by our five runs of replication (detailed in Appendix~\ref{apdx:ExperimentDetail}) for a fair comparison. 
Due to the limit of computing resources, we trained RNNG/URNNG with the first run only. The oracle refers to the highest possible  score of binarized groundtruth trees.
}
\label{tab:main_results}
\end{table}

Then, we evaluate the distillation stage of our approach, which is based on Run~1 of each model. We observe our RNNG and URRNG follow the same trend as in previous work that RNNG may slightly hurt the performance, but its URNNG refinement\footnote{URNNG is traditionally used as a refinement procedure following a noisily trained RNNG~\citep{kim-etal-2019-compound, cao-etal-2020-unsupervised}. If URNNG is trained from scratch, it does not yield meaningful performance and may be even worse than right-branching~\citep{kim-etal-2019-unsupervised}. Thus, we excluded URNNG from our teachers.} yields a performance boost. 
It is also noted that such a boosting effect of URNNG on our approach is less significant than that on previous models, which is reasonable because our ensemble (w/o RNNG or URNNG) has already achieved a high performance. Overall, we achieve an  score of  in Row~14, being a new state of the art of unsupervised parsing and largely bridging the gap between supervised and unsupervised constituency parsing.

We compare our ensemble-then-distill approach with union distillation (Row~10), which trains from the union of the teachers' first-run outputs. As expected in Section~\ref{discussion:uniondistill}, union distillation does not work well; its performance is worse than that of the best teacher (Run~1 of Row~8), suggesting that multiple teachers may confuse the student and hurt the performance. Rather, our approach requires all teachers to negotiate a most agreed tree, thus avoiding confusion during the distilling process. 

\begin{wraptable}{r}{0.45\textwidth}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{r l | c c c}
\toprule
& Method & Run 1 & +RNNG & +URNNG\\
\midrule
1& Left branching &  & -- & --\\
2& Right branching &  & -- & --\\
\midrule
3& Ordered Neurons &  &  & \\
4& Neural PCFG &  &  & \\
5& Compound PCFG &  &  & \\
6& DIORA &  &  & \\
7& S-DIORA &  &  & \\
8& ConTest &  &  & \\
9& ContexDistort &  &  & \\
\midrule
10& Selective MBR &  &  & \\
11& Our ensemble & &  & \\
\midrule
12& PTB-supervised & -- &  & \\
13& SUSANNE oracle &  & -- & --\\
\bottomrule
\end{tabular}
}
\caption{ scores in the domain-shift setting from PTB to SUSANNE. Note that all models were trained on PTB, including RNNGs and URNNGs. Since our approach is highly robust, we only considered the models of the first run on PTB in this experiment.}\vspace{-10pt}
\label{tab:susanne_results}
\end{wraptable}

\textbf{Results on SUSANNE.} Table~\ref{tab:susanne_results} presents parsing performance under a domain shift from PTB to SUSANNE. We directly ran unsupervised PTB-trained models on the test-only SUSANNE corpus without finetuning. This is a realistic experiment to examine the models' performance in an unseen low-resource domain.

We see both selective MBR (Row~10) and our method (Row~11) outperform all teachers (Rows~3--9) in the domain-shift setting, and that our approach outperforms selective MBR by  points. The results are consistent with the PTB experiment.

For the ensemble-distilled RNNG and URNNG (Row~11), the performance drops slightly, probably because the performance of our ensemble approach without RNNG/URNNG is saturating and close to the PTB-supervised model (Row~12), whose RNNG/URNNG distillation also yields slight performance drop.
Nevertheless, our RNNG and URNNG (Row~11) outperform all the baselines in all settings. Moreover, the inference of our student model does not require querying the teachers, and is x faster than the ensemble method (Appendix~\ref{app:efficiency}). Thus, the ensemble-distilled model is useful as it achieves competitive performance and high efficiency.

\subsection{In-Depth Analysis}
\label{ss:analysis}
\begin{figure}[!b]
\begin{center}
\includegraphics[width=\linewidth]{DenoiseVsExp.png}
\end{center}\vspace{-10pt}
\caption{Effect of denoising vs.~utilizing different expertise. Results are the  scores on the PTB test set. The {\color{blue} \textit{italic blue annotation}} is an interpretation of the plot.}
\label{fig:denoisingvsexp}
\end{figure}

\textbf{Denoising vs.~utilizing different expertise}. A curious question raised from the main results is why our ensemble approach yields such a substantial improvement. We have two plausible hypotheses: 1) The ensemble approach merely smooths out the teachers' noise, and 2) The ensemble approach is able to utilize different expertise of heterogeneous teachers.

We conducted the following experiment to verify the above hypotheses. Specifically, we compare two settings: the ensemble of three runs of the same model and the ensemble of three heterogeneous models. We picked the runs and models such that the two settings have similar performance. This sets up a controlled experiment, as the gain obtained by the ensemble of multiple runs suggests a denoising effect, whereas a further gain obtained by the ensemble of heterogeneous models suggests the effect of utilizing different expertise.

We repeated the experiment for seven groups with different choices of models and show results in Figure~\ref{fig:denoisingvsexp}. As seen, the ensemble of different runs always outperforms a single run, showing that the denoising effect does play a role in the ensemble process. Moreover, the ensemble of heterogeneous models consistently leads to a large add-on improvement compared with the ensemble of multiple runs; the results convincingly verify that different unsupervised parsers learn different aspects of the language structures, and that our ensemble approach is able to utilize such different expertise.


\textbf{Over-smoothing in multi-teacher knowledge distillation.} As discussed in Section~\ref{discussion:uniondistill}, union distillation is prone to the over-smoothing problem, where the student learns an overly smooth, wide-spreading distribution. This is especially severe in our setting, as our student learns from multiple heterogeneous teachers. 

\begin{table}[t]
\centering
\resizebox{.5\linewidth}{!}{
\begin{tabular}{lcc}
\toprule
Distillation Approach & Mean Entropy & Std \\
\midrule
Union distillation    & 11.42       & 0.10              \\
Our ensemble distillation & 4.93        & 0.13              \\
Oracle-supervision distillation & 2.26        & 0.13              \\
\bottomrule
\end{tabular}
}
\vspace{5pt}
\caption{The mean and standard deviation (std) of the prediction entropy for distilled RNNGs.}
\label{tab:Entropy}
\end{table}

The over-smoothing problem can be verified by checking the entropy, , of a model's predicted distribution . In Table~\ref{tab:Entropy}, we report the mean and standard deviation of the entropy\footnote{The entropy of each run is averaged over 2,412 samples. The calculation of entropy is based on the codebase of \citet{kim-etal-2019-unsupervised}, available at \url{https://github.com/harvardnlp/urnng}} across five runs. Results clearly show that the union distillation leads to a very smooth distribution (very high entropy), which also explains its low 
\begin{wrapfigure}{r}{0.45\textwidth}
    \includegraphics[width=\linewidth]{IncrementalEnsemble.png}
    \caption{Ensemble performance with different numbers of teachers. The lines are best-performing, average, and worst-performing combinations. These results are averaged over five runs available in the experiments conducted for Table~\ref{tab:main_results}. The gray shades are the best and worst runs.}
    \label{fig:teachercount}\vspace{-.5cm}
\end{wrapfigure}
performance (Table~\ref{tab:main_results}). On the contrary, our ensemble-then-distill approach yields much lower entropy, providing strong evidence of the alleviation of the over-smoothing problem.

\textbf{Analyzing the number of teachers.} In our main experiment (Table~\ref{tab:main_results}), we perform an ensemble of seven popular unsupervised parsers. We would like to analyze the performance of ensemble models with different numbers of teachers,\footnote{We have  combinations, which are tractable because our CYK algorithm is efficient.} and results are shown in Figure~\ref{fig:teachercount}.

We see a consistent trend that more teachers lead to higher performance. Profoundly, the top dashed line suggests that, even if we start with a strong teacher, adding weaker teachers also improves, or at least does not hurt, the performance. Further, the decrease in the width of gray shades (deviations of best and worst runs) suggests that more teachers also lead to lower variance. Overall, this analysis conclusively shows that, with a growing number of teachers, our ensemble approach not only improves performance, but also makes unsupervised parsing more robust.

\textbf{Additional results.} We present supplementary analyses in the appendix. \ref{app:efficiency}: Inference efficiency; \ref{app:breakdown}:~Performance by different constituency types.


\section{Related Work}
\textbf{Unsupervised syntactic structure discovery} carries a long history and has attracted much attention in different ages~\citep{klein2005unsupervised,shen2018ordered,li-lu-2023-contextual}. Its significance lies in the potential to help low-resource domains~\citep{kann-etal-2019-neural} and its important role in cognitive science, such as understanding how children learn language~\citep{Exemplar2Grammar}.
\citet{6137337} show unsupervised constituency parsing methods are not limited to linguistics but can also be used to parse motion-sensor data by treating it as a language. This approach finds an abstraction of motion data and leads to a better understanding of the signal semantics.

Unsupervised syntactic structure discovery can be divided into different tasks: unsupervised constituency parsing, which  organizes the phrases of a sentence in a hierarchical manner~\citep{constituencyTree}; unsupervised dependency parsing~\citep{nivre2010dependency, naseem2010using, han-etal-2020-survey}, which determines the syntactic relation between the words in a sentence; and 
unsupervised chunking, which aims at segmenting a text into groups of syntactically related words in a flattened structure~\citep{deshmukh-etal-2021-unsupervised-chunking, wu2023unsupervised}.

Our work falls in the category of unsupervised constituency parsing. Previous work has proposed various heuristics and indirect supervisions to tackle this task \citep{snyder2009unsupervised,kim-etal-2019-compound,drozdov-etal-2019-unsupervised-latent}, as mentioned in Section~\ref{sec:introduction}.
In our work, we propose to build an ensemble model to utilize the expertise of different unsupervised parsers.

\textbf{Minimum Bayes risk (MBR) decoding} minimizes a Bayes risk (i.e., expected loss) during inference~\citep{MBRbook}. For example, machine translation systems may generate a set of candidate outputs, and define the risk as the dissimilarity between one candidate output and the rest; MBR decoding selects the lowest-risk candidate translation that is most similar to others~\citep{kumar-byrne-2004-minimum,10.1162/tacl_a_00491,suzgun-etal-2023-follow}. Similar approaches are applied to other decoding tasks, such as speech recognition~\citep{inproceedingsspeechrecognition} and text summarization~\citep{suzgun-etal-2023-follow}.

In this work, we develop a novel generative MBR method for constituency parsing that searches the entire binary tree space by an efficient CYK-like dynamic programming, significantly differing from existing MBR approaches that perform selection on a candidate set.

\textbf{Knowledge distillation (KD)} is commonly used to train a small student model from a large teacher model~\citep{sun-etal-2019-patient,jiao-etal-2020-tinybert}. Evidence show that the teacher's predicted probability contains more knowledge than a groundtruth label and can better train the student model~\citep{hinton2015distilling,wen-etal-2023-f}.

Interestingly, KD is originally proposed to train a small model from an ensemble of teachers~\citep{KDoriginal,hinton2015distilling}. They address simple classification tasks and use either voting or average ensembles to train the student. A voting ensemble is similar to MBR, but only works for classification tasks; it cannot be applied to structure prediction (e.g., sequences or trees). An average ensemble takes the average of probabilities; thus, it resembles union distillation, which is the predominant approach for multi-teacher distillation in recent years~\citep{wu-etal-2021-one,yang2020model}.
However, these approaches may suffer from the over-smoothing problem when teachers are heterogeneous (Section~\ref{discussion:uniondistill}).
In our work, we propose a novel MBR-based ensemble method for multi-teacher distillation, which largely alleviates the over-smoothing problem and is able to utilize different teachers' expertise.

\section{Conclusion}
In this work, we reveal an interesting phenomenon that different unsupervised parsers learn different expertise, and we propose a novel ensemble approach by introducing a new notion of ``tree averaging'' to leverage such heterogeneous expertise. Further, we distill the ensemble knowledge into a student model to improve inference efficiency; the proposed ensemble-then-distill approach also addresses the over-smoothing problem in multi-teacher distillation. Overall, our method shows consistent effectiveness with various teacher models and is robust in the domain-shift setting, largely bridging the gap between supervised and unsupervised constituency parsing.

\textbf{Future work} may be considered from both linguistic and machine learning perspectives. The proposed ensemble method largely bridges the gap between supervised and unsupervised parsing of the English language. A future direction is to address unsupervised linguistic structure discovery in low-resource and multilingual settings. Regarding the machine learning aspect, our work demonstrates the importance of addressing the over-smoothing problem in multi-teacher distillation, and we expect our ensemble-and-distill approach can be extended to different data types, such as sequences and graphs, with proper design of data-specific ensemble methods.

\newpage

\bibliographystyle{plainnat}
\bibliography{main}

\newpage

\appendix
\section{Our CYK Variant}
\label{apdx:CYKalgo}

\begin{figure}[!b]
\resizebox{\linewidth}{!}{
\renewcommand{\tabcolsep}{2pt}
    \centering
    \begin{tabular}{|c|c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c|c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c|c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c|c@{}c@{}c@{}c@{}c@{}c@{}c@{}c@{}c|}
    \hline
    & \multicolumn{9}{c|}{Teacher 1} & \multicolumn{9}{c|}{Teacher 2} & \multicolumn{9}{c|}{Teacher 3} & \multicolumn{9}{c|}{Teacher 4}\\
    \hline
    \multirow{5}{*}{\rotatebox[origin=c]{90}{Tree}}
    & &&&&\tikzmarknode{t1w12345}{\ding{71}}&&&&
    & &&&&\tikzmarknode{t2w12345}{\ding{71}}&&&&
    & &&&&\tikzmarknode{t3w12345}{\ding{71}}&&&&
    & &&&&\tikzmarknode{t4w12345}{\ding{71}}&&&&
    \\
    & &&&\tikzmarknode{t1w1234}{\ding{71}}&&\widthof{\hphantom{\ding{71}}}&&&
    & &&&\tikzmarknode{t2w1234}{\hphantom{\ding{71}}}&&\tikzmarknode{t2w2345}{\hphantom{\ding{71}}}&&&
    & &&&\tikzmarknode{t3w1234}{\hphantom{\ding{71}}}&&\tikzmarknode{t3w2345}{\hphantom{\ding{71}}}&&&
    & &&&\tikzmarknode{t4w1234}{\hphantom{\ding{71}}}&&\tikzmarknode{t4w2345}{\ding{71}}&&&
    \\
    & &&\tikzmarknode{t1w123}{\hphantom{\ding{71}}}&&\tikzmarknode{t1w234}{\hphantom{\ding{71}}}&&\tikzmarknode{t1w345}{\hphantom{\ding{71}}}&&
    & &&\tikzmarknode{t2w123}{\hphantom{\ding{71}}}&&\tikzmarknode{t2w234}{\hphantom{\ding{71}}}&&\tikzmarknode{t2w345}{\ding{71}}&&
    & &&\tikzmarknode{t3w123}{\ding{71}}&&\tikzmarknode{t3w234}{\hphantom{\ding{71}}}&&\tikzmarknode{t3w345}{\hphantom{\ding{71}}}&&
    & &&\tikzmarknode{t4w123}{\hphantom{\ding{71}}}&&\tikzmarknode{t4w234}{\ding{71}}&&\tikzmarknode{t4w345}{\hphantom{\ding{71}}}&&
    \\
    & &\tikzmarknode{t1w12}{\ding{71}}&&\tikzmarknode{t1w23}{\hphantom{\ding{71}}}&&\tikzmarknode{t1w34}{\ding{71}}&&\tikzmarknode{t1w45}{\hphantom{\ding{71}}}&
    & &\tikzmarknode{t2w12}{\ding{71}}&&\tikzmarknode{t2w23}{\hphantom{\ding{71}}}&&\tikzmarknode{t2w34}{\ding{71}}&&\tikzmarknode{t2w45}{\hphantom{\ding{71}}}&
    & &\tikzmarknode{t3w12}{\ding{71}}&&\tikzmarknode{t3w23}{\hphantom{\ding{71}}}&&\tikzmarknode{t3w34}{\hphantom{\ding{71}}}&&\tikzmarknode{t3w45}{\ding{71}}&
    & &\tikzmarknode{t4w12}{\hphantom{\ding{71}}}&&\tikzmarknode{t4w23}{\hphantom{\ding{71}}}&&\tikzmarknode{t4w34}{\ding{71}}&&\tikzmarknode{t4w45}{\hphantom{\ding{71}}}&
    \\
    &
    \tikzmarknode{t1w1}{}&&\tikzmarknode{t1w2}{}&&\tikzmarknode{t1w3}{}&&\tikzmarknode{t1w4}{}&&\tikzmarknode{t1w5}{} & 
    \tikzmarknode{t2w1}{}&&\tikzmarknode{t2w2}{}&&\tikzmarknode{t2w3}{}&&\tikzmarknode{t2w4}{}&&\tikzmarknode{t2w5}{} & 
    \tikzmarknode{t3w1}{}&&\tikzmarknode{t3w2}{}&&\tikzmarknode{t3w3}{}&&\tikzmarknode{t3w4}{}&&\tikzmarknode{t3w5}{} & 
    \tikzmarknode{t4w1}{}&&\tikzmarknode{t4w2}{}&&\tikzmarknode{t4w3}{}&&\tikzmarknode{t4w4}{}&&\tikzmarknode{t4w5}{}
    \\\hline
    \multicolumn{0}{c}{}
    \\\cline{0-9}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{}}
    & &&&&4&&&&\\
    & &&&1&&1&&&\\
    & &&1&&1&&1&&\\
    & &3&&0&&3&&1&\\
    & 4&&4&&4&&4&&4\\
    &
    {}&&{}&&{}&&{}&&{}
    \\\cline{0-9}
    \multicolumn{0}{c}{}
    \\\hline
    & \multicolumn{9}{c|}{} & \multicolumn{9}{c|}{} & \multicolumn{9}{c|}{} & \multicolumn{9}{c|}{} \\
    \hline
    \multirow{6}{*}{\rotatebox[origin=c]{90}{}}
    & &&&&&&&& & &&&&&&&& & &&&&&&&& & &&&&&&&&\\
    & &&&&&&&& & &&&&&&&& & &&&&&&&& & &&&&&&&&\\
    & &&&&&&&& & &&&&&&&& & &&&&&&&& & &&&&&&&&\\
    & &\tikzmarknode{l2b1w12}{11}&&&&&&& & &\tikzmarknode{l2b2w12}{11}&&\tikzmarknode{l2b2w23}{08}&&&&& & &\tikzmarknode{l2b3w12}{11}&&\tikzmarknode{l2b3w23}{08}&&\tikzmarknode{l2b3w34}{11}&&& & &\tikzmarknode{l2b4w12}{11}&&\tikzmarknode{l2b4w23}{08}&&\tikzmarknode{l2b4w34}{11}&&\tikzmarknode{l2b4w45}{09}&\\
    & \tikzmarknode{l2b1w1}{4}&&\tikzmarknode{l2b1w2}{4}&&\tikzmarknode{l2b1w3}{4}&&\tikzmarknode{l2b1w4}{4}&&\tikzmarknode{l2b1w5}{4} & \tikzmarknode{l2b2w1}{4}&&\tikzmarknode{l2b2w2}{4}&&\tikzmarknode{l2b2w3}{4}&&\tikzmarknode{l2b2w4}{4}&&\tikzmarknode{l2b2w5}{4} & \tikzmarknode{l2b3w1}{4}&&\tikzmarknode{l2b3w2}{4}&&\tikzmarknode{l2b3w3}{4}&&\tikzmarknode{l2b3w4}{4}&&\tikzmarknode{l2b3w5}{4} & \tikzmarknode{l2b4w1}{4}&&\tikzmarknode{l2b4w2}{4}&&\tikzmarknode{l2b4w3}{4}&&\tikzmarknode{l2b4w4}{4}&&\tikzmarknode{l2b4w5}{4}\\
    &
    {}&&{}&&{}&&{}&&{} & 
    {}&&{}&&{}&&{}&&{} & 
    {}&&{}&&{}&&{}&&{} & 
    {}&&{}&&{}&&{}&&{}\\
    \cline{0-36}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{}}
    & &&&&&&&& & &&&&&&&& & &&&&&&&& & \\
    & &&&&&&&& & &&&&&&&& & &&&&&&&& & \\
    & &&\tikzmarknode{l3b1w123}{16}&&&&&& & &&\tikzmarknode{l3b2w123}{16}&&\tikzmarknode{l3b2w234}{16}&&&& & &&\tikzmarknode{l3b3w123}{16}&&\tikzmarknode{l3b3w234}{16}&&\tikzmarknode{l3b3w345}{16}&& & \\
    & &\tikzmarknode{l3b1w12}{11}&&\tikzmarknode{l3b1w23}{08}&&\tikzmarknode{l3b1w34}{11}&&\tikzmarknode{l3b1w45}{09}& & &\tikzmarknode{l3b2w12}{11}&&\tikzmarknode{l3b2w23}{08}&&\tikzmarknode{l3b2w34}{11}&&\tikzmarknode{l3b2w45}{09}& & &\tikzmarknode{l3b3w12}{11}&&\tikzmarknode{l3b3w23}{08}&&\tikzmarknode{l3b3w34}{11}&&\tikzmarknode{l3b3w45}{09}& & \\
    & \tikzmarknode{l3b1w1}{4}&&\tikzmarknode{l3b1w2}{4}&&\tikzmarknode{l3b1w3}{4}&&\tikzmarknode{l3b1w4}{4}&&\tikzmarknode{l3b1w5}{4} & \tikzmarknode{l3b2w1}{4}&&\tikzmarknode{l3b2w2}{4}&&\tikzmarknode{l3b2w3}{4}&&\tikzmarknode{l3b2w4}{4}&&\tikzmarknode{l3b2w5}{4} & \tikzmarknode{l3b3w1}{4}&&\tikzmarknode{l3b3w2}{4}&&\tikzmarknode{l3b3w3}{4}&&\tikzmarknode{l3b3w4}{4}&&\tikzmarknode{l3b3w5}{4} & \\
    &
    {}&&{}&&{}&&{}&&{} & 
    {}&&{}&&{}&&{}&&{} & 
    {}&&{}&&{}&&{}&&{}\\
    \cline{0-27}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{}}
    & &&&&&&&& & &&&&&&&& & \\
    & &&&\tikzmarknode{l4b1w1234}{23}&&&&& & &&&\tikzmarknode{l4b2w1234}{23}&&\tikzmarknode{l4b2w2345}{21}&&& & \\
    & &&\tikzmarknode{l4b1w123}{16}&&\tikzmarknode{l4b1w234}{16}&&\tikzmarknode{l4b1w345}{16}&& & &&\tikzmarknode{l4b2w123}{16}&&\tikzmarknode{l4b2w234}{16}&&\tikzmarknode{l4b2w345}{16}&& & \\
    & &\tikzmarknode{l4b1w12}{11}&&\tikzmarknode{l4b1w23}{08}&&\tikzmarknode{l4b1w34}{11}&&\tikzmarknode{l4b1w45}{09}& & &\tikzmarknode{l4b2w12}{11}&&\tikzmarknode{l4b2w23}{08}&&\tikzmarknode{l4b2w34}{11}&&\tikzmarknode{l4b2w45}{09}& & \\
    & \tikzmarknode{l4b1w1}{4}&&\tikzmarknode{l4b1w2}{4}&&\tikzmarknode{l4b1w3}{4}&&\tikzmarknode{l4b1w4}{4}&&\tikzmarknode{l4b1w5}{4} & \tikzmarknode{l4b2w1}{4}&&\tikzmarknode{l4b2w2}{4}&&\tikzmarknode{l4b2w3}{4}&&\tikzmarknode{l4b2w4}{4}&&\tikzmarknode{l4b2w5}{4} \\
    &
    {}&&{}&&{}&&{}&&{} & 
    {}&&{}&&{}&&{}&&{}\\
    \cline{0-18}
    \multirow{6}{*}{\rotatebox[origin=c]{90}{}}
    & &&&&\tikzmarknode{l5b1w12345}{31}&&&& \\
    & &&&\tikzmarknode{l5b1w1234}{23}&&\tikzmarknode{l5b1w2345}{21}&&& \\
    & &&\tikzmarknode{l5b1w123}{16}&&\tikzmarknode{l5b1w234}{16}&&\tikzmarknode{l5b1w345}{16}&& \\
    & &\tikzmarknode{l5b1w12}{11}&&\tikzmarknode{l5b1w23}{08}&&\tikzmarknode{l5b1w34}{11}&&\tikzmarknode{l5b1w45}{09}& \\
    & \tikzmarknode{l5b1w1}{4}&&\tikzmarknode{l5b1w2}{4}&&\tikzmarknode{l5b1w3}{4}&&\tikzmarknode{l5b1w4}{4}&&\tikzmarknode{l5b1w5}{4} \\
    &
    {}&&{}&&{}&&{}&&{}\\
    \cline{0-9}
\end{tabular}
    \begin{tikzpicture}[remember picture, overlay]
        \draw[-, black, thick] (t1w12345) -- (t1w1234);
        \draw[-, black, thick] (t1w12345) -- (t1w5);
        \draw[-, black, thick] (t1w1234) -- (t1w12);
        \draw[-, black, thick] (t1w1234) -- (t1w34);
        \draw[-, black, thick] (t1w12) -- (t1w1);
        \draw[-, black, thick] (t1w12) -- (t1w2);
        \draw[-, black, thick] (t1w34) -- (t1w3);
        \draw[-, black, thick] (t1w34) -- (t1w4);
        
        \draw[-, black, thick] (t2w12345) -- (t2w12);
        \draw[-, black, thick] (t2w12345) -- (t2w345);
        \draw[-, black, thick] (t2w12) -- (t2w1);
        \draw[-, black, thick] (t2w12) -- (t2w2);
        \draw[-, black, thick] (t2w345) -- (t2w34);
        \draw[-, black, thick] (t2w345) -- (t2w5);
        \draw[-, black, thick] (t2w34) -- (t2w3);
        \draw[-, black, thick] (t2w34) -- (t2w4);
        
        \draw[-, black, thick] (t3w12345) -- (t3w123);
        \draw[-, black, thick] (t3w12345) -- (t3w45);
        \draw[-, black, thick] (t3w123) -- (t3w12);
        \draw[-, black, thick] (t3w123) -- (t3w3);
        \draw[-, black, thick] (t3w12) -- (t3w1);
        \draw[-, black, thick] (t3w12) -- (t3w2);
        \draw[-, black, thick] (t3w45) -- (t3w4);
        \draw[-, black, thick] (t3w45) -- (t3w5);
        
        \draw[-, black, thick] (t4w12345) -- (t4w1);
        \draw[-, black, thick] (t4w12345) -- (t4w2345);
        \draw[-, black, thick] (t4w2345) -- (t4w234);
        \draw[-, black, thick] (t4w2345) -- (t4w5);
        \draw[-, black, thick] (t4w234) -- (t4w2);
        \draw[-, black, thick] (t4w234) -- (t4w34);
        \draw[-, black, thick] (t4w34) -- (t4w3);
        \draw[-, black, thick] (t4w34) -- (t4w4);
        
        \draw[-, red, thick] (l2b1w12) -- (l2b1w1);
        \draw[-, red, thick] (l2b1w12) -- (l2b1w2);
        
        \draw[-, black!30, thick] (l2b2w12) -- (l2b2w1);
        \draw[-, black!30, thick] (l2b2w12) -- (l2b2w2);
        \draw[-, red, thick] (l2b2w23) -- (l2b2w2);
        \draw[-, red, thick] (l2b2w23) -- (l2b2w3);
        
        \draw[-, black!30, thick] (l2b3w12) -- (l2b3w1);
        \draw[-, black!30, thick] (l2b3w12) -- (l2b3w2);
        \draw[-, black!30, thick] (l2b3w23) -- (l2b3w2);
        \draw[-, black!30, thick] (l2b3w23) -- (l2b3w3);
        \draw[-, red, thick] (l2b3w34) -- (l2b3w3);
        \draw[-, red, thick] (l2b3w34) -- (l2b3w4);
        
        \draw[-, black!30, thick] (l2b4w12) -- (l2b4w1);
        \draw[-, black!30, thick] (l2b4w12) -- (l2b4w2);
        \draw[-, black!30, thick] (l2b4w23) -- (l2b4w2);
        \draw[-, black!30, thick] (l2b4w23) -- (l2b4w3);
        \draw[-, black!30, thick] (l2b4w34) -- (l2b4w3);
        \draw[-, black!30, thick] (l2b4w34) -- (l2b4w4);
        \draw[-, red, thick] (l2b4w45) -- (l2b4w4);
        \draw[-, red, thick] (l2b4w45) -- (l2b4w5);
        
        \draw[-, black!30, thick] (l3b1w12) -- (l3b1w1);
        \draw[-, black!30, thick] (l3b1w12) -- (l3b1w2);
        \draw[-, red, thick] (l3b1w123) -- (l3b1w12);
        \draw[-, red, thick] (l3b1w123) -- (l3b1w3);
        
        \draw[-, black!30, thick] (l3b2w12) -- (l3b2w1);
        \draw[-, black!30, thick] (l3b2w12) -- (l3b2w2);
        \draw[-, black!30, thick] (l3b2w123) -- (l3b2w12);
        \draw[-, black!30, thick] (l3b2w123) -- (l3b2w3);
        \draw[-, black!30, thick] (l3b2w34) -- (l3b2w3);
        \draw[-, black!30, thick] (l3b2w34) -- (l3b2w4);
        \draw[-, red, thick] (l3b2w234) -- (l3b2w2);
        \draw[-, red, thick] (l3b2w234) -- (l3b2w34);
        
        \draw[-, black!30, thick] (l3b3w12) -- (l3b3w1);
        \draw[-, black!30, thick] (l3b3w12) -- (l3b3w2);
        \draw[-, black!30, thick] (l3b3w123) -- (l3b3w12);
        \draw[-, black!30, thick] (l3b3w123) -- (l3b3w3);
        \draw[-, black!30, thick] (l3b3w34) -- (l3b3w3);
        \draw[-, black!30, thick] (l3b3w34) -- (l3b3w4);
        \draw[-, black!30, thick] (l3b3w234) -- (l3b3w2);
        \draw[-, black!30, thick] (l3b3w234) -- (l3b3w34);
        \draw[-, red, thick] (l3b3w345) -- (l3b3w34);
        \draw[-, red, thick] (l3b3w345) -- (l3b3w5);

        \draw[-, black!30, thick] (l4b1w12) -- (l4b1w1);
        \draw[-, black!30, thick] (l4b1w12) -- (l4b1w2);
        \draw[-, black!30, thick] (l4b1w34) -- (l4b1w3);
        \draw[-, black!30, thick] (l4b1w34) -- (l4b1w4);
        \draw[-, red, thick] (l4b1w1234) -- (l4b1w12);
        \draw[-, red, thick] (l4b1w1234) -- (l4b1w34);

        \draw[-, black!30, thick] (l4b2w12) -- (l4b2w1);
        \draw[-, black!30, thick] (l4b2w12) -- (l4b2w2);
        \draw[-, black!30, thick] (l4b2w34) -- (l4b2w3);
        \draw[-, black!30, thick] (l4b2w34) -- (l4b2w4);
        \draw[-, black!30, thick] (l4b2w1234) -- (l4b2w12);
        \draw[-, black!30, thick] (l4b2w1234) -- (l4b2w34);
        \draw[-, black!30, thick] (l4b2w345) -- (l4b2w34);
        \draw[-, black!30, thick] (l4b2w345) -- (l4b2w5);
        \draw[-, red, thick] (l4b2w2345) -- (l4b2w2);
        \draw[-, red, thick] (l4b2w2345) -- (l4b2w345);

        \draw[-, black!30, thick] (l5b1w345) -- (l5b1w34);
        \draw[-, black!30, thick] (l5b1w345) -- (l5b1w5);
        \draw[-, black!30, thick] (l5b1w34) -- (l5b1w3);
        \draw[-, black!30, thick] (l5b1w34) -- (l5b1w4);
        \draw[-, black!30, thick] (l5b1w12) -- (l5b1w1);
        \draw[-, black!30, thick] (l5b1w12) -- (l5b1w2);
        \draw[-, red, thick] (l5b1w12345) -- (l5b1w12);
        \draw[-, red, thick] (l5b1w12345) -- (l5b1w345);
    \end{tikzpicture}
}
    \caption{Step-by-step illustration of our CYK algorithm, showing the dynamic changes in the  along with the construction of the corresponding optimal binary constituency tree.}
    \label{fig:illustration}
\end{figure}
In this appendix, we provide a step-by-step illustration of our CYK-based ensemble algorithm in Section~\ref{Sec:OurCYK}.

Consider four teachers predicting the trees in the first row of Figure~\ref{fig:illustration}. The hit count of each span is shown in the second row. For example, the span  hits  times, namely, Teachers 1--3. 

The initialization of the algorithm is to obtain the total hit count for a single word, which is simply the same as the number of teachers because every word appears intact in every teacher's prediction. The initialization has five cells in a row, and is omitted in the figure to fit the page width.

For recursion, we first consider the constituents of two words, denoted by . A constituent's total hit count, denoted by  in Eqn.~(\ref{eq:H}), inherits those of its children, plus its own hit count. In the cell of , for example, , where  is the hit count of the span , shown before. 

For the next step of recursion, we consider three-word constituents, i.e., . For example, the span  has two possible tree structures  and . The former leads to a total hit count of , whereas the latter leads to . Therefore,  highlighted in red is chosen, with the best hit count .

The process is repeated until we have the best parse tree of the whole sentence, which is  for the 5-word sentence in Figure~\ref{fig:illustration}. We also provide the pseudocode for the process in Algorithm~\ref{algo:CYK}.

\begin{algorithm}[!t]
\caption{Pseudocode for our CYK variant}
\label{algo:CYK}
\begin{algorithmic}[1]
\Function{our\_cyk}{}
    \For{ to }  Base cases
        \State 
        \State 
    \EndFor
    \For{ to } Iterate over different lengths of constituents
        \For{ to }  Iterate over different possible constituents of length 
            \State 
            \State 
        \Statex \hspace{4cm} The {\color{gray}gray} term need not be implemented as it is a constant in 
            \State 
            \State 
        \EndFor
    \EndFor
    \State \Return 
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Supplementary Analyses}
\label{app:additional}

\subsection{Inference Efficiency}\label{app:efficiency}

\begin{wraptable}{r}{0.45\textwidth}
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l@{\qquad} l r r}
        \toprule
        &&\multicolumn{2}{c}{Inference Time (ms)}\\
        & Model & w/ GPU & w/o GPU \\
        \midrule
        \multicolumn{4}{l}{Teachers}\\
        & ON & 35 & 130 \\
        & Neural PCFG & 610 & 630 \\
        & Compound PCFG & 560 & 590 \\
        & DIORA & 30 & 30 \\
        & S-DIORA & 110 & 140 \\
        & ConTest & 4,300 & 59,500 \\
        & ContexDestort & 1,890 & 11,110 \\
        \multicolumn{4}{l}{Our ensemble} \\
        & CYK part & 6 & 6 \\
        & Total & 7,541 & 72,136 \\
        \multicolumn{4}{l}{Student} \\
        & RNNG & 410 & 410 \\
        \bottomrule
    \end{tabular}
}
    \caption{Per-sample inference time (in milliseconds) on the PTB test. }
    \label{tab:time_analysis}\vspace{-3pt}
\end{wraptable}

We propose to distill the ensemble knowledge into a student model to increase the inference efficiency. We conducted an analysis on the inference time of different approaches, where we measured the run time using 28 Intel(R) Core(TM) i9-9940X (@3.30GHz) CPUs with and without one Nvidia RTX Titan GPU. Table~\ref{tab:time_analysis} reports the average time elapsed for performing inference on one sample\footnote{The average time was computed on 100 samples of the PTB test set, due to the slow inference of certain teachers without GPU.} of the PTB test set, ignoring loading models, reading inputs, and writing outputs. 

In the table, the total inference time of our ensemble model is the summation of all the teachers and the CYK algorithm. As expected, an ensemble approach is slow because it has to perform inference for every teacher. However, our CYK-based ensemble algorithm is extremely efficient and its inference time is negligible compared with the teacher models.

The RNNG student model learns the knowledge from the cumbersome ensemble, and is able to perform inference efficiently with an 18x and 175x speedup with and without GPU, respectively. This shows the necessity of having knowledge distillation on top of the ensemble. Overall, RNNG achieves comparable performance to its ensemble teacher (Tables~\ref{tab:main_results} and~\ref{tab:susanne_results}) but drastically speeds up the inference, being a useful model in practice.


\subsection{Performance by Constituency Labels}\label{app:breakdown}


In this work, we see different unsupervised parsers learn different patterns (Table~\ref{tab:correlation-table}), and their expertise can be utilized by an ensemble approach (Section~\ref{ss:analysis}). From the linguistic point of view, we are curious about whether there is a relation between such different expertise and the linguistic constituent labels (e.g., noun phrases and verb phrases). 

With this motivation, we report in Figure~\ref{fig:pertag} the breakdown performance by constituency labels, where the most common five labels---namely, noun phrases, propositional phrases, verb phrases, simple declarative clauses, and subordinating conjunction clauses---are considered, covering 95\% of the cases in the PTB test. Notice that the predicted constituency parse trees are still unlabeled (without tags like noun phrases), whereas the groundtruth constituency labels are used for collecting the statistics. Consequently, only recall scores can be calculated in per-label performance analysis~\citep{drozdov-etal-2019-unsupervised-latent,kim-etal-2019-compound,cao-etal-2020-unsupervised}. 

As seen, existing unsupervised parsers indeed exhibit variations in the performance of different constituency labels. For example, ConTest achieves high performance of prepositional phrases, whereas DIORA works well for clauses (including simple declarative clauses and subordinating conjunction clauses); for noun phrases, most models perform similarly. By contrast, our ensemble model achieves outstanding performance similar to or higher than the best teacher in each category. This provides further evidence that our ensemble model utilizes different teachers' expertise. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{PerTag.png}
\end{center}\vspace{-10pt}
\caption{Performance by constituency labels on the PTB test set. Results are measured by recall, because the predicted parse trees are unlabeled; thus, precision and  scores cannot be computed~\citep{drozdov-etal-2019-unsupervised-latent}. Bars and gray intervals are the mean and standard deviation, respectively, over five runs.}
\label{fig:pertag}
\end{figure}

\section{Inventory of Teacher Models}
\label{apdx:ExperimentDetail}

Our experiments involve seven existing unsupervised parsers as teachers, each of which has five runs either based on authors' checkpoints or by our replication using authors' codebases. We show the details in Table~\ref{tab:detailed_variants}, where we also quote the mean  scores and, if available, max  scores reported in 
 respective papers. Overall, we have achieved similar performance to previous work, which shows the success of our replication and establishes a solid foundation for our ensemble research.

\newpage
\begin{table}[h]
\centering
\resizebox{.83\linewidth}{!}{
\begin{tabular}{l c | l | c}
& Run & Source & \\
\hline
\toprule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Ordered Neurons}} &
\multicolumn{3}{l}{mean , max  reported in~\citet{shen2018ordered}} \\
\cmidrule(){2-4}
&1 & Our replication using the original codebase\tablefootnote{\url{https://github.com/yikangshen/Ordered-Neurons}\label{footnote:ONcodebase}} ()& \\
&2 & Our replication using the original codebase\footnoteagain{footnote:ONcodebase} ()& \\
&3 & Parsed data available in \citet{kim-etal-2019-compound}\tablefootnote{\url{https://github.com/harvardnlp/compound-pcfg}\label{footnote:compoundcodebase}} & \\
&4 & Our replication using the original codebase\footnoteagain{footnote:ONcodebase} ()& \\
&5 & Our replication using the original codebase\footnoteagain{footnote:ONcodebase} ()& \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Neural PCFG}} &
\multicolumn{3}{l}{mean , max  reported in~\citet{kim-etal-2019-compound}} \\
\cmidrule(){2-4}
&1 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
&2 & Parsed data available in the original codebase\footnoteagain{footnote:compoundcodebase} & \\
&3 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
&4 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
&5 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
\hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{Compound PCFG}} &
\multicolumn{3}{l}{mean ,  max  reported in~\citet{kim-etal-2019-compound}} \\
\cmidrule(){2-4}
&1 & Parsed data available in the original codebase\footnoteagain{footnote:compoundcodebase} & \\
&2 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
&3 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
&4 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
&5 & Our replication using the original codebase\footnoteagain{footnote:compoundcodebase} ()& \\
\hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{DIORA}} &
\multicolumn{3}{l}{mean  reported in~\citet{drozdov-etal-2019-unsupervised-latent}} \\
\cmidrule(){2-4}
&1 & The mlp-softmax checkpoint available on the original codebase\tablefootnote{\url{https://github.com/iesl/diora}\label{footnote:dioracodebase}} & \\
&2 & Our replication using the original codebase\footnoteagain{footnote:dioracodebase} ()& \\
&3 & Our replication using the original codebase\footnoteagain{footnote:dioracodebase} ()& \\
&4 & Our replication using the original codebase\footnoteagain{footnote:dioracodebase} ()& \\
&5 & Our replication using the original codebase\footnoteagain{footnote:dioracodebase} ()& \\
\hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{S-DIORA}} &
\multicolumn{3}{l}{mean , max  reported in~\citet{drozdov2020diora}} \\
\cmidrule(){2-4}
&1 & Our replication using the original codebase\tablefootnote{\url{https://github.com/iesl/s-diora}\label{footnote:sdioracodebase}}() & \\
&2 & Our replication using the original codebase\footnoteagain{footnote:sdioracodebase} ()& \\
&3 & Our replication using the original codebase\footnoteagain{footnote:sdioracodebase} ()& \\
&4 & Our replication using the original codebase\footnoteagain{footnote:sdioracodebase} ()& \\
&5 & Our replication using the original codebase\footnoteagain{footnote:sdioracodebase} ()& \\
\hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{ConTest}} &
\multicolumn{3}{l}{mean , max  reported in~\citet{cao-etal-2020-unsupervised}} \\
\cmidrule(){2-4}
&1 & A checkpoint provided by the authors through personal email & \\
&2 & Our replication using the original codebase\tablefootnote{\url{https://github.com/stevenxcao/constituency-test-parser}\label{footnote:ConTestcodebase}} ()& \\
&3 & Parsed data provided by the authors through personal email & \\
&4 & Our replication using the original codebase\footnoteagain{footnote:ConTestcodebase} ()& \\
&5 & Our replication using the original codebase\footnoteagain{footnote:ConTestcodebase} ()& \\
\hline
\multirow{7}{*}{\rotatebox[origin=c]{90}{ContexDistort\tablefootnote{Given a pretrained language model, ContexDistort is a deterministic algorithm. Therefore, we used different layers of the language model as runs to obtain different results.}}} &
\multicolumn{3}{l}{ reported in~\citet{li-lu-2023-contextual}} \\
\cmidrule(){2-4}
&1 & Our replication using the original codebase\tablefootnote{\url{https://github.com/jxjessieli/contextual-distortion-parser}\label{footnote:ContexDistortcodebase}} on 10th layer of ``bert-base-cased'' & \\
&2 & Our replication using the original codebase\footnoteagain{footnote:ContexDistortcodebase} on 12th layer of ``bert-base-cased'' & \\
&3 & Our replication using the original codebase\footnoteagain{footnote:ContexDistortcodebase} on 11th layer of ``bert-base-cased'' & \\
&4 & Our replication using the original codebase\footnoteagain{footnote:ContexDistortcodebase} on 8th layer of ``bert-base-cased'' & \\
&5 & Our replication using the original codebase\footnoteagain{footnote:ContexDistortcodebase} on 9th layer of ``bert-base-cased'' & \\
\hline
\end{tabular}
}
\vspace{5pt}
\caption{ scores are on PTB test for different teachers in different runs. Note that the runs were randomly shuffled for the randomized experiment. }
\label{tab:detailed_variants}
\end{table}

\end{document}
