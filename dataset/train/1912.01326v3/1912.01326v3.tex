\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[ruled]{algorithm2e}
\usepackage{array}
\usepackage{multirow}
\usepackage{color,graphicx}
\usepackage{cases}
\usepackage[export]{adjustbox}
\usepackage[dvipsnames]{xcolor}
\usepackage{textpos}
\usepackage{siunitx}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{7487} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
        
\ifcvprfinal\pagestyle{empty}\fi

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\providecommand{\tabularnewline}{\\}

\begin{document}

\title{A Context-Aware Loss Function for Action Spotting in Soccer Videos}

\author{
Anthony Cioppa*\\
{\small University of Li\`ege}\\
{\tt\small anthony.cioppa@uliege.be}
\and
Adrien Deli\`ege*\\
{\small University of Li\`ege}\\
{\tt\small adrien.deliege@uliege.be}
\and
Silvio Giancola*\\
{\small KAUST}\\
{\tt\small silvio.giancola@kaust.edu.sa}
\and
Bernard Ghanem\\
{\small KAUST}\\
\and
Marc Van Droogenbroeck\\
{\small University of Li\`ege}\\
\and
Rikke Gade\\
{\small Aalborg University}\\
\and
Thomas B. Moeslund\\
{\small Aalborg University}\\
}


\maketitle

\newcommand{\mysection}[1]{\vspace{2pt}\noindent\textbf{#1}}
\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\Figure}[1]{Figure~\ref{fig:#1}}
\newcommand{\Equation}[1]{Equation~\eqref{eq:#1}}
\newcommand{\Equations}[2]{Equations \eqref{eq:#1} and \eqref{eq:#2}}
\newcommand{\Section}[1]{Section~\ref{sec:#1}}
\newcommand{\SoccerNet}{SoccerNet~\cite{Giancola_2018_CVPR_Workshops}\xspace}
\newcommand{\ActivityNet}{ActivityNet~\cite{caba2015activitynet}\xspace}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
\blfootnote{\textbf{(*)} Denotes equal contributions. Code available at \url{https://github.com/cioppaanthony/context-aware-loss}.}


\definecolor{myred}[a=.5]{RGB}{215,25,28} 
\definecolor{myorange}[a=.5]{RGB}{253,174,97}
\definecolor{anthoblue}[a=.5]{RGB}{31,119,180}
\definecolor{anthoorange}[a=.5]{RGB}{255,127,14}
\definecolor{anthogreen}[a=.5]{RGB}{0,150,0}
\definecolor{anthored}[a=.5]{RGB}{150,0,0}
\definecolor{anthobrown}[a=.5]{RGB}{153,76,0}
\definecolor{mygreen}[a=.5]{RGB}{166,217,106} 
\definecolor{mygray}[a=.5]{gray}{0.6}


\newcommand{\whitebox}{\hfill\textcolor{white}{\rule[1mm]{1.8mm}{2.8mm}}\hfill}
\newcommand{\redbox}{\hfill\textcolor{myred}{\rule[1mm]{1.8mm}{2.8mm}}\hfill}
\newcommand{\orangebox}{\hfill\textcolor{myorange}{\rule[1mm]{1.8mm}{2.8mm}}\hfill}
\newcommand{\greenbox}{\hfill\textcolor{mygreen}{\rule[1mm]{1.8mm}{2.8mm}}\hfill}
\newcommand{\graybox}{\hfill\textcolor{mygray}{\rule[1mm]{1.8mm}{2.8mm}}\hfill}

\begin{abstract}
In video understanding, action spotting consists in temporally localizing human-induced events annotated with single timestamps. In this paper, we propose a novel loss function that specifically considers the temporal context naturally present around each action, rather than focusing on the single annotated frame to spot. We benchmark our loss on a large dataset of soccer videos, SoccerNet, and achieve an improvement of   over the baseline. We show the generalization capability of our loss for generic activity proposals and detection on ActivityNet, by spotting the beginning and the end of each activity.  Furthermore, we provide an extended ablation study and display challenging cases for action spotting in soccer videos. Finally, we qualitatively illustrate how our loss induces a precise temporal understanding of actions and show how such semantic knowledge can be used for automatic highlights generation.
    
\end{abstract}

 


\section{Introduction}
\label{sec:Intro}


\begin{figure}[t]
    \centering
\setlength{\fboxsep}{0pt}\colorbox{black}{\begin{minipage}{\linewidth}
            \rule{0mm}{4.8mm}
            \redbox\redbox\redbox\redbox\orangebox
            \orangebox\graybox\graybox\graybox\graybox
            \greenbox\greenbox\greenbox\greenbox\orangebox
            \orangebox\redbox\redbox\redbox\redbox
            \null\\
            \null\hfill
            \includegraphics[width=0.235\textwidth]{imgs/timeline_frame_5.jpeg}
            \includegraphics[width=0.235\textwidth]{imgs/timeline_frame_29.jpeg}
            \includegraphics[width=0.235\textwidth]{imgs/timeline_frame_36.jpeg}
            \includegraphics[width=0.235\textwidth]{imgs/timeline_frame_77.jpeg}
            \null\hfill
            \\label{eq:seg_loss}
    \mathcal{L}^\text{seg} = \frac{1}{C \ N_F}\sum_{i = 1}^{N_F} \sum_{c=1}^C \tilde{L}(p^c(x_i),s^c(x_i))

    \hspace{-0.5mm}\mathcal{L}^\text{as} = \sum_{i=1}^{N_\text{GT}}\sum_{j=1}^{2+C}\alpha_j \left(\textbf{Y}_{i,j}-\hat{\textbf{Y}}^M_{i,j}\right)^2 \hspace{-1mm}+ \beta \hspace{-2mm}\sum_{i=N_\text{GT}+1}^{N_\text{pred}} \hspace{-1mm}\left(\hat{\textbf{Y}}^M_{i,1}\right)^2
    \label{eq:LossActionSpotting}

    \mathcal{L} = \mathcal{L}^\text{as} + \lambda^\text{seg}\ \mathcal{L}^\text{seg}
    \label{eq:LossFinal}





\mysection{Network for action spotting.} 
The architecture of the network is illustrated in \Figure{Network} and further detailed in the \textbf{supplementary material}. We leverage frame feature representations for the videos (\eg ResNet) provided with the dataset, embodied as the output of the frame feature extractor of \Figure{Network}. The temporal CNN of \Figure{Network} is composed of a spatial two-layer MLP, followed by four multi-scale 3D convolutions (\ie across time, features and classes). The temporal CNN outputs a set of  features for each frame organized in  feature vectors (one per class) of size , as in~\cite{Sabour2017Dynamic}. These features are input into a segmentation module, in which we use Batch Normalization~\cite{Ioffe2015BatchNorm} and sigmoid activations. The closeness of the  vectors obtained in this way to a pre-defined vector gives the  segmentation scores output by the segmentation module, as~\cite{Deliege2018HitNet}. The  features obtained previously are concatenated with the  scores and fed to the action spotting module, as shown in Figure~\ref{fig:Network}. It is composed of three successive temporal max-pooling and 3D convolutions, and outputs  vectors of dimension . The first two elements of these vectors are sigmoid-activated, the  last are softmax-activated. The activated vectors are stacked to produce the prediction  of dimension  for the action spotting task.


 \section{Experiments}
\label{sec:Exp}

We evaluate our new context-aware loss function in two scenarios: the action spotting task of \SoccerNet, and activity localization and detection tasks on ActivityNet~\cite{caba2015activitynet}.

\subsection{Experiments on SoccerNet}

\mysection{Data.} Three classes of action are annotated in SoccerNet by Giancola~\etal~\cite{Giancola_2018_CVPR_Workshops}: goals, cards, and substitutions, so  in this case. They identify each action by one annotated frame: the moment the ball crosses the line for \emph{goal}, the moment the referee shows a player a card for \emph{card}, and the moment a new player enters the field for \emph{substitution}. We train our network on the frame features already provided with the dataset. Giancola~\etal first subsampled the raw videos at  fps, then they extracted the features with a backbone network and reduced them by PCA to  features for each frame of the subsampled videos. Three sets of features are provided, each extracted with a particular backbone network: I3D~\cite{Carreira_2017_CVPR}, C3D~\cite{Tran2015ICCV}, and ResNet~\cite{He_2016_CVPR}.


\mysection{Action spotting metric.} 
We measure performances with the action spotting metric introduced in SoccerNet~\cite{Giancola_2018_CVPR_Workshops}. An action spot is defined as positive if its temporal offset from its closest ground truth is less than a given tolerance . The average precision (AP) is estimated based on Precision-Recall curves, then averaged between classes (mAP). An Average-mAP is proposed as the AUC of the mAP over different tolerances  ranging from 5 to 60 seconds.


\mysection{Experimental setup.} We train our network on batches of \emph{chunks}. We define a chunk as a set of  contiguous frame feature vectors. We set  to maintain a high training speed while retaining sufficient contextual information. This size corresponds to a clip of  minutes of raw video. A batch contains chunks extracted from a single raw video. We extract a chunk around each ground-truth action, such that the action is randomly located within the chunk. Then, to balance the batch, we randomly extract  chunks composed of background frames only. An epoch ends when the network has been trained on one batch per training video. At each epoch, new batches are re-computed for each video for data augmentation purposes. Each raw video is time-shift encoded before training. Each new training chunk is encoded with the YOLO-like encoding. 


The number of action spotting predictions generated by the network is set to , as we observed that no chunks of  minutes of raw video contain more than  actions. We train the network during  epochs, with an initial learning rate  linearly decreasing to . We use Adam as the optimizer with default parameters~\cite{Diederick2015Adam}.


For the segmentation loss, we set the margins  and  in \Equations{taumax}{taumin}, following the practice in~\cite{Sabour2017Dynamic}. For the action spotting loss in \Equation{LossActionSpotting}, we set  for , while  is optimized (see below) to find an appropriate weighting for the location components of the predictions. Similarly,  is optimized to find the balance between the loss of the action vectors and the regularization of the remaining predictions. For the final loss in \Equation{LossFinal}, we optimize  to find the balance between the two losses.


\mysection{Hyperparameter optimization.}
For each set of features (I3D, C3D, ResNet), we perform a joint Bayesian optimization~\cite{BayesianOpt} on the number of frame features  extracted per class, on the temporal receptive field  of the network (\ie temporal kernel dimension of the 3D convolutions), and on the parameters . Next, we perform a grid search optimization on the slicing parameters . 

For ResNet, we obtain . For goals (resp. cards, substitutions) we have  (resp. , ),  (resp. , ),  (resp. , ), and  (resp. , ). Given the framerate of 2 fps, those values can be translated to seconds by scaling them down by a factor of 2. The value  corresponds to a temporal receptive field of  seconds on both sides of the central frame in the temporal dimension of the 3D convolutions.



\begin{table}[t]
\begin{centering}
\begin{tabular}{c||c|c|c}
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Frame features}\tabularnewline
 & I3D & C3D & ResNet\tabularnewline\hline\hline
SoccerNet baseline 5s~\cite{Giancola_2018_CVPR_Workshops} & - & - & 34.5\tabularnewline\hline 
SoccerNet baseline 60s~\cite{Giancola_2018_CVPR_Workshops} & - & - & 40.6\tabularnewline\hline 
SoccerNet baseline 20s~\cite{Giancola_2018_CVPR_Workshops} & - & - & 49.7\tabularnewline\hline\hline
Vats \etal~\cite{Vats2019Event_full} & - & - & 57.5\tabularnewline\hline\hline
Ours & 53.6 & 57.7 & \textbf{62.5}\tabularnewline
\end{tabular}
\caption{\textbf{Results on SoccerNet.} Average-mAP (in \%) on the test set of SoccerNet for the action spotting task. We establish a new state-of-the-art performance.}
\label{tab:results}
\par\end{centering}
\end{table}



\mysection{Main results.} 
The performances obtained with the optimized parameters are reported in Table~\ref{tab:results}. As shown, we establish a new state-of-the-art performance on the action spotting task of SoccerNet, outperforming the previous benchmark by a comfortable margin, for all the frame features. ResNet gives the best performance, as also observed in~\cite{Giancola_2018_CVPR_Workshops}. A sensitivity analysis of the parameters  reveals robust performances around the optimal values, indicating that no heavy fine-tuning is required for the context slicing. Also, performances largely decrease as the slicing is strongly reduced, which emphasizes its usefulness. 


\mysection{Ablation study.} 
Since the ResNet features provide the best performance, we use them with their optimized parameters for the following ablation studies. \textbf{(i)} We remove the segmentation module, which is equivalent to setting  in \Equation{LossFinal}. This also removes the context slicing and the margins  and . \textbf{(ii)} We remove the action context slicing such that the ground truth for the segmentation module is the raw binary annotations, \ie all the frames must be classified as background except the action frames. This is equivalent to setting . \textbf{(iii)} We remove the margins that help the network focus on improving its worst segmentation scores, by setting  in \Equations{taumax}{taumin}. \textbf{(iv)} We remove the iterative one-to-one matching between the ground truth  and the predictions  before the action spotting loss, which is equivalent to using  instead of  in \Equation{LossActionSpotting}. The results of the ablation studies are shown in \Table{ablation}.


From an Average-mAP perspective, the auxiliary task of temporal segmentation improves the performance on the action spotting task (from  to ), which is a common observation in multi-task learning~\cite{Zamir_2018_CVPR}. When the segmentation is performed, our temporal context slicing gives a significant boost compared to using the raw binary annotations (from  to ). This observation is in accordance with the sensitivity analysis. It also appears that it is preferable to not use the segmentation at all rather than using the segmentation with the raw binary annotations ( vs ), which further underlines the usefulness of the context slicing. A boost in performance is also observed when we use the margins to help the network focus on improving its worst segmentation scores (from  to ). Eventually, Table~\ref{tab:ablation} shows that it is extremely beneficial to match the predictions of the network with the ground truth before the action spotting loss (from  to ). This makes sense since there is no point in evaluating the network on its ability to order its predictions, which is a hard and unnecessary constraint. The large impact of the matching is also justified by its direct implication in the action spotting task assessed through the Average-mAP.


\begin{table}
    \centering
    \begin{tabular}{c||c|c|c|c||c}
    & Segm. & Slic. & Marg. & Match. & Result\\ \hline \hline 
    (i)   &  &  &  & \checkmark & 58.9\\ \hline 
    (ii)  & \checkmark &  & \checkmark & \checkmark & 57.8\\ \hline 
    (iii) & \checkmark & \checkmark &  & \checkmark & 59.0\\ \hline 
    (iv)  & \checkmark & \checkmark & \checkmark &  & 46.8\\ \hline\hline
    Ours  & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{62.5}\\ 
    \end{tabular}
    \caption{
    \textbf{Ablation study.} 
    We perform ablations by 
    \textbf{(i)} removing the segmentation (), hence the slicing and the margins; 
    \textbf{(ii)} removing the context slicing (); 
    \textbf{(iii)} removing the margins that help the network focus on improving its worst segmentation scores (, ); 
    \textbf{(iv)} removing the matching (using  instead of  in ). Each part evidently contributes to the overall performance.
    }
    \label{tab:ablation}
\end{table}




\mysection{Results through game time.}
In soccer, it makes sense to analyze the performance of our model through game time, since the actions are not uniformly distributed throughout the game. For example, a substitution is more likely to occur during the second half of a game. We consider non-overlapping bins corresponding to  minutes of game time and compute the Average-mAP for each bin. \Figure{perfs-through-time} shows the evolution of this metric through game time.


It appears that actions occurring during the first five minutes of a half-time are substantially more difficult to spot than the others. This may be partially explained by the occurrence of some of these actions at the very beginning of a half-time, for which the temporal receptive field of the network requires the chunk to be temporally padded. Hence, some information may be missing to allow the network to spot those actions. Besides, when substitutions occur during the break, they are annotated as such on the first frame of the second halves of the matches, which makes them practically impossible to spot. In the test set, this happens for  of the matches. None of these substitutions are spotted by our model, which thus degrades the performances during the first minutes of play in the second halves of the matches. However, they merely represent  of all the substitutions, and removing them from the evaluation only boosts our Average-mAP by  (from  to ).


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/performances_through_time_total.pdf}
    \caption{\textbf{Performance as function of game time.} 
    {\color{anthoorange}\textbf{Average-mAP}} spotting performance over the game time with all ground-truth actions of the dataset binned in  minute intervals. 
    It appears that actions around the half-time break are more challenging to spot.
    {\color{anthoblue}\textbf{Number of actions}} for each bin.
    {\color{mygray}\textbf{Our performance ()}}.}
    \label{fig:perfs-through-time}
\end{figure}




\mysection{Results as function of action vicinity.}
We investigate whether actions are harder to spot when they are close to each other. We bin the ground-truth actions based on the distance that separates them from the previous (or next, depending on which is the closest) ground-truth action, regardless of their classes. Then, we compute the Average-mAP for each bin. The results are represented in Figure~\ref{fig:perfs-closeness}. 

We observe that the actions are more difficult to spot when they are close to each other. This could be due to the reduced number of visual cues, such as replays, when an action occurs rapidly after another and thus must be broadcast. Some confusion may also arise because the replays of the first action can still be shown after the second action, \eg a sanctioned foul followed by a converted penalty.
This analysis also shows that the action spotting problem is challenging even when the actions are further apart, as the performances in Figure~\ref{fig:perfs-closeness} eventually plateau.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/performances_proximity.pdf}
    \caption{\textbf{Performance as function of action vicinity.} 
    {\color{anthoorange}\textbf{Average-mAP}} spotting performance per bin of ground-truth actions grouped by distance (in seconds) from their closest  ground-truth action.
    It appears that nearby actions are more challenging to spot.
    {\color{anthoblue}\textbf{Number of actions}} for each bin.
    {\color{mygray}\textbf{Our performance ()}}.}
    \label{fig:perfs-closeness}
\end{figure}





\mysection{Per-class results.}
We perform a per-class analysis in a similar spirit as the Average-mAP metric. For a given class, we fix a tolerance  around each annotated action to determine positive predictions and we aggregate these results in a confusion matrix. An action is considered spotted when its confidence score exceeds some threshold optimized for the  score on the validation set. From the confusion matrix, we compute the precision, recall and  score for that class and for that tolerance . Varying  from  to  seconds provides the evolution of the three metrics as a function of the tolerance. Figure~\ref{fig:goalmetrics} shows these curves for \emph{goals} for our model and for the baseline \cite{Giancola_2018_CVPR_Workshops}. The results for cards and substitutions are provided in \textbf{supplementary material}.


Figure~\ref{fig:goalmetrics} shows that most goals can be efficiently spotted by our model within  seconds around the ground truth ( seconds). We achieve a precision of  for that tolerance. The previous baseline plateaus within  seconds ( seconds) and still has a lower performance. In particular for goals, many visual cues facilitate their spotting, \eg multiple replays, particular camera views, or celebrations from the players and from the public. 


\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/counting_metrics.pdf}\\
    \caption{\textbf{Per-class results (goals).} A prediction of class \emph{goal} is a {\color{anthoblue}\textbf{true positive (TP)}} with tolerance  when it is located at most  seconds from a ground-truth goal. The baseline results are obtained from the best model of~\cite{Giancola_2018_CVPR_Workshops}. Our model spots most goals within  seconds around the ground truth ( seconds).
    }
    \label{fig:goalmetrics}
\end{figure}





\subsection{Experiments on ActivityNet}

In this section, we evaluate our context-aware loss in a more generic task than action spotting in soccer videos. We tackle the \emph{Activity Proposal} and \emph{Activity Detection} tasks of the challenging ActivityNet dataset, for which we use the ResNet features provided with the dataset at  fps. 


\mysection{Setup.} 
We use the current state-of-the-art network, namely BMN \cite{Lin_2019_ICCV}, with the code provided in~\cite{BMNCode}. BMN is equipped with a temporal evaluation module (TEM), which plays a similar role as our temporal segmentation module. We replace the loss associated with the TEM by our novel temporal segmentation loss . The slicing parameters are set identically for all the classes and are optimized with respect to the AUC performance on the validation set by grid search with the constraint . The optimization yields the best results where .



\mysection{Results.} 
The average performances on  runs of our experiment and of the BMN base code~\cite{BMNCode} are reported in Table~\ref{tab:results-ActivityNet}. Our novel temporal segmentation loss improves the performance obtained with BMN~\cite{BMNCode} by  and  for the activity proposal task (AR@100 and AUC) and by  for the activity detection task (Average-mAP). These increases compare with some recent increments, while being obtained just by replacing their TEM loss by our context-aware segmentation loss. The network thus has the same architecture and number of parameters. We conjecture that our loss , through its particular context slicing, helps train the network by modelling the uncertainty surrounding the annotations. Indeed, it has been shown in \cite{alwassel2018diagnosing,sigurdsson2017actions} that a large variability exists among human annotators on which frames to annotate as the beginning and the end of the activities of the dataset. Let us note that in BMN, the TEM loss is somehow adapted around the action frames in order to mitigate the penalization attributed to their neighboring frames. Our work goes one step further, by directly designing a temporal context-aware segmentation loss.


\begin{table}
\begin{centering}
\begin{tabular}{l||c|c|c}
Method & AR@ & AUC & Average-mAP \\\hline 
Lin~\etal~\cite{Lin2017Temporal}      &  &  & \\\hline 
Gao~\etal~\cite{Gao_2018_ECCV}        &  &  & - \\\hline 
BSN~\cite{Lin_2018_ECCV}              &  &  & \\\hline 
P-GCN~\cite{zeng2019graph_full}       & -       & -       & \\\hline 
BMN~\cite{Lin_2019_ICCV}              &  &  & \\\hline\hline
BMN code~\cite{BMNCode}               &  &  & \\\hline
Ours: \cite{BMNCode} +  &  &  & \\
\end{tabular}
\caption{
\textbf{Results on ActivityNet} validation set for the proposal task (AR@100, AUC) and for the detection task (Average-mAP). For our experiments, we report the average values on  runs.}
\label{tab:results-ActivityNet}
\par\end{centering}
\end{table}




 \section{Automatic Highlights Generation for Soccer}
\label{sec:Discussion}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/segmentation_detection_with_arrows.png}
\caption{\textbf{Action spotting and segmentation} for the  half of the ``Remuntada" FCB - PSG. {\color{anthoblue} \textbf{Ground truth actions}}, {\color{anthoorange}\textbf{temporal segmentation curves}}, and {\color{anthogreen}\textbf{spotting results}} are illustrated.  We can identify {\color{anthored}\textbf{unannotated interesting actions}} using our segmentation. }
    \label{fig:predsandsegs}
\end{figure}

Some action spotting and temporal segmentation results are shown in Figure~\ref{fig:predsandsegs}. It appears that some sequences of play have a high segmentation score for some classes but do not lead, quite rightly, to an action spotting. It turns out that these sequences are often related to unannotated actions of supplementary classes that resemble those considered so far, such as \emph{unconverted goal opportunities} and \emph{unsanctioned fouls}. Video clips of the two actions identified in Figure~\ref{fig:predsandsegs} are provided in the \textbf{supplementary material}. 


To quantify the spotting results of goal opportunities, we can only compute the precision metric since these actions are not annotated. We manually inspect each video sequence of the test set where the segmentation score for goals exceeds some threshold  but where no ground-truth goal is present. We decide whether the sequence is a goal opportunity or not by asking two frequent observers of soccer games if they would include it in the highlights of the match. The sequence is a true positive when they both agree to include it and a false positive, otherwise. The precision is then computed for that . By gradually decreasing  from  to , we obtain the precision curve shown in Figure~\ref{fig:PerformanceHighlight}. It appears that  of the sequences with a segmentation score larger than  are considered goal opportunities. 



As a direct by-product, we derive an automatic highlights generator without explicit supervision. We extract a video clip starting  seconds before each spotting of a \emph{goal} or a \emph{card} and ending  seconds after. We proceed likewise for the sequences with a segmentation score  for \emph{goals}. We dismiss substitutions as they rarely appear in highlights. We assemble the clips chronologically to produce the highlights video, provided in \textbf{supplementary material}. Evaluating its quality is subjective, but we found its content to be adequate, even if the montage could be improved. Indeed, only sequences where a goal, a goal opportunity, or a foul occurs are selected. This reinforces the usefulness of the segmentation,  as it provides a direct overview of the proceedings of the match, including proposals for unannotated actions that are interesting for highlights.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/performances_highlight.pdf}
    \caption{\textbf{Precision for goal opportunities}, as a function of the threshold on the segmentation score to exceed for manually inspecting a sequence. For scores larger than , a {\color{anthoorange}\textbf{precision}} of  is achieved, \ie  of the sequences inspected were goal opportunities. {\color{anthoblue}\textbf{Number of sequences}} inspected per threshold.}
    \label{fig:PerformanceHighlight}
\end{figure} \section{Conclusion}
\label{sec:Conclusion}

We tackle the challenging action spotting task of SoccerNet with a novel context-aware loss for segmentation and a YOLO-like loss for the spotting. The former treats the frames according to their time-shift from their closest ground-truth actions. The latter leverages an iterative matching algorithm that alleviates the need for the network to order its predictions. To show generalization capabilities, we also test our context-aware loss on ActivityNet. We improve the state-of-the-art on ActivityNet by  in AR@100,  in AUC, and  in Average-mAP, by only including our context-aware loss without changing the network architecture. We achieve a new state-of-the art on SoccerNet, surpassing by far the previous baseline (from  to  in Average-mAP) and spotting most actions within  seconds around their ground truth. Finally, we leverage the resulting segmentation results to identify unannotated actions such as goal opportunities and derive a highlights generator without specific supervision.


\mysection{Acknowledgments.} This work is supported by the DeepSport project of the Walloon region and the FRIA (Belgium), as well as the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-CRG2017-3405. \clearpage


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/Figure_Network5.png}
    \caption{
    \textbf{Pipeline for action spotting.}
    We propose a network made of
    a \textbf{\color{Orange}frame feature extractor} and a \textbf{\color{Orange} temporal CNN} outputting  class feature vectors per frame,
    a \textbf{\color{NavyBlue}segmentation module} outputting per-class segmentation scores, and 
    a \textbf{\color{Green}spotting module} extracting  values per spotting prediction (\ie the confidence score  for the spotting, its location  and a per-class prediction).
    }
    \label{fig:Network_Supp}
\end{figure*}



\section{Supplementary Material}

A supplementary video summarizing our work is available on 
\url{https://youtu.be/FAeWxs0d4_o}.


\subsection{Notations}
Let us recall the following notations from the paper:
\begin{itemize}
    \item  is the number of classes in the spotting task.
    \item  is the number of frames in the chunk considered.
    \item  is the number of ground-truth actions in the chunk considered.
    \item  is the number of predictions output by the network for the spotting task.
    \item  is the number of features computed for each class, for each frame, before the segmentation module (see Figure~\ref{fig:Network_Supp}).
    \item  is the temporal receptive field of the network (used in the temporal convolutions).
    \item  regroups the spotting predictions of the network, and has  dimension . The first column represents the confidence scores for the spots, the second contains the predicted locations, and the other are per-class classification scores.
    \item  encodes the ground-truth action vectors of the chunk considered, and has dimension . 
    \item  () denotes the context slicing parameters of class .
\end{itemize}

We also use the following notations for the layers of a convolutional neural network:
\begin{itemize}
    \item FC() is a fully connected layer (\eg in a multi-layer perceptron) between any vector to a vector of size . 
    \item ReLU is the rectified linear unit.
    \item Conv() is a convolutional layer with  kernels of dimensions .
\end{itemize}




\subsection{Detailed Network Architecture for SoccerNet}

The architecture of the network used in the paper for the action spotting task of \SoccerNet, as depicted in Figure~\ref{fig:Network_Supp}, is detailed hereafter.

\textbf{\color{Orange}1. 
Frame feature extractor and temporal CNN.}
\SoccerNet provides three frame feature extractors with different backbone architectures (I3D, C3D, and ResNet). 
Each of them respectively extracts , , and  features that are further reduced to  features with a Principal Component Analysis (PCA).
We use the PCA-reduced features provided with the dataset as input of our temporal CNN.


The aim of the temporal CNN is to provide  features for each frame, while mixing temporal information across the frames. It transforms an input of shape  into an output of shape .

First, each frame is input to a -layer MLP to reduce the dimensionality of the feature vectors of each frame. We design its architecture as: FC() - ReLU - FC() - ReLU. We thus obtain a set of  features, which we note .

Then,  is input to a spatio-temporal pyramid, \ie it is input in parallel to each of the following layers of the pyramid:
\begin{itemize}
    \item Conv() - ReLU
    \item Conv() - ReLU
    \item Conv() - ReLU
    \item Conv() - ReLU
\end{itemize}
producing  features for each frame, which are concatenated with  to obtain a set of  features. 

Finally, we feed these features to a Conv() layer, which produces a set of  features, noted .

\textbf{\color{NavyBlue}2. Segmentation module.} This module produces a segmentation score per class for each frame. It transforms  into an output of dimension , through the following steps:
\begin{itemize}
    \item Reshape  to have dimension .
    \item Use a frame-wise Batch Normalization. 
    \item Activate with a sigmoid so that each frame has, for each class, a feature vector .
    \item For each frame, for each class, compute the distance  between  and the center of the unit hypercube , \ie a vector composed of  for its  components. Hence, .
    \item The segmentation score is obtained as , which belongs to . This way, scores close to  for a class (\ie  close to the center of the cube) can be interpreted as indicating that the frame is likely to belong to that class.
\end{itemize}
The segmentation scores  output by the segmentation module thus has dimension  and is assessed through the segmentation loss .


\textbf{\color{Green}3. Spotting module.} The spotting module takes as input  and , and outputs the spotting predictions  of the network. It is composed of the following layers:
\begin{itemize}
    \item ReLU on , then concatenate with . This results in  features.
    \item Temporal max-pooling  with a  stride.
    \item Conv() - ReLU
    \item Temporal max-pooling  with a  stride.
    \item Conv() - ReLU
    \item Temporal max-pooling  with a  stride.
    \item Flatten the resulting features, which yields .
    \item Feed  to a FC() layer, then reshape to  and use sigmoid activation. This produces the confidence scores and the predicted locations for the action spots.
    \item Feed  to a FC() layer, then reshape to  and use softmax activation on each row. This produces the per-class predictions for the action spots.
    \item Concatenate the confidence scores, predicted locations, and per-class predictions to produce the spotting predictions  of shape .
\end{itemize}
Eventually,  is assessed through the action spotting loss .





\subsection{Iterative One-to-One Matching}

The iterative one-to-one matching between the predicted locations  and the ground-truth locations  described in the paper is illustrated in Figure~\ref{fig:onetoonematching}. It is further detailed mathematically in Algorithm~\ref{alg:matching}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/OneToOneMatching.pdf}\\
    \caption{\textbf{Iterative one-to-one matching.} Example of the iterative one-to-one matching. At iteration , each ground-truth location is matched with its closest predicted location (\textbf{{\color{anthogreen}green arrows}}), and vice-versa (\textbf{{\color{anthobrown}brown arrows}}). Locations that match each other are permanently matched (\textbf{\color{gray}gray arrows}), and the process is repeated with the remaining locations at iteration 2. In this case, two iterations suffice to match all the ground-truth locations with a predicted location, as evidenced by the absence of available ground-truth location for iteration 3. 
    }
    \label{fig:onetoonematching}
\end{figure}


\begin{algorithm}
\SetAlgoLined
\KwData{ ground-truth and predicted locations}
\KwResult{Matching couples }
{\bf Algorithm:}\\
\While{}{
    \;
    \For{}{
    \If{}{
        \;
        Save matching couple \;
        Remove  from  and  from \;
        }
    }
}
\caption{Iterative matching between ground-truth and predicted locations.}
\label{alg:matching}
\end{algorithm}








\subsection{Details on the Time-Shift Encoding (TSE)}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/2eventsloss.png}
    \caption{\textbf{Context-aware loss function (close actions).} Representation of our segmentation loss when two actions of the same class are close to each other. The loss is parameterized by the time-shift encoding of the frames and is continuous through time, except at frames annotated as actions. A video clip where we vary the location of the second action is provided with this document (\emph{3dloss.mp4}).}
    \label{fig:two-events-loss}
\end{figure}

The time-shift encoding (TSE) described in the paper is further detailed below. We note  the TSE of frame  related to class .

We denote  (resp. ) the difference between the frame index of  and the frame index of its closest past (resp. future) ground-truth action of class . They constitute the time-shifts of  from its closest past and future ground-truth actions of class , expressed in number of frames (\ie if frames  and  are actions of class , then frame  has  and ). We set  for a frame corresponding to a ground-truth action of class , thus ensuring the relations . 
The TSE  is defined as the time-shift among  related to the action that has the dominant influence on . The rules used to determine which time-shift is selected are the following:
\begin{itemize}
    \item if : keep , because  is located \emph{just after} the past action, which still strongly influences . \item if :  is in the \emph{transition zone} after the past action, whose influence weakens, thus the decision depends on how far away is the future action:
    \begin{itemize}
        \item if : keep , because  is located \emph{far before} the future action, which does not yet influence .
        \item if :
        The future action may be close enough to influence :
        \begin{itemize}
            \item if : keep , because  is closer to the \emph{just after} region of the past action than it is to the \emph{just before} region of the future action, with respect to the size of the transition zones. \item else: keep , because the future action influences  more than the past action.
        \end{itemize}
    \end{itemize}
    \item if : keep , because  is located \emph{far after} the past action, which does not influence  anymore.
\end{itemize}
For completeness, let us recall the following details mentioned in the main paper.
If  is both located \emph{far after} the past action and \emph{far before} the future action, selecting either of the two time-shifts has the same effect in our loss. Furthermore, for the frames located either before the first or after the last annotated action of class , only one time-shift can be computed and is thus set as . Finally, if no action of class  is present in the video, then we set  for all the frames. This induces the same behavior in our loss as if they were all located far before their closest future action.


The TSE is used to shape our novel context-aware loss function for the temporal segmentation module. The cases described above ensure the temporal continuity of the loss, regardless of the proximity between two actions of the same class, excepted at frames annotated as ground-truth actions. This temporal continuity can be visualized in Figure~\ref{fig:two-events-loss}, which shows a representation of  (analogous to Figure~\ref{fig:Loss}) when two actions are close to each other. It is further illustrated in the video clip \emph{3dloss.mp4} provided with this document, where we gradually vary the location of the second action. For each location of the second action, the TSE of all the frames is re-computed, and so is the loss.









\subsection{Extra Analyses}

\mysection{Per-class results.} As for the class \emph{goal} in Figure~\ref{fig:goalmetrics} of the main paper, Figures~\ref{fig:cardmetrics} and~\ref{fig:substitutionmetrics} display the number of TP, FP, FN and the precision, recall and  metrics for the classes \emph{card} and \emph{substitution} as a function of the tolerance  allowed for the localization of the spots. 

Figure~\ref{fig:cardmetrics} shows that most cards can be efficiently spotted by our model within  seconds around the ground truth ( seconds). We achieve a precision of  for that tolerance. The previous baseline plateaus within  seconds ( seconds) and still has a lower performance. 

Figure~\ref{fig:substitutionmetrics} shows that most substitutions can be efficiently spotted by our model within  seconds around the ground truth ( seconds). We achieve a precision of  for that tolerance. The previous baseline reaches a similar performance for that tolerance, and reaches  within  seconds ( seconds) around the ground truth. 

Except for the precision metric for the substitutions with tolerances larger than  seconds, our model outperforms the previous baseline of \SoccerNet. As mentioned in the paper, for goals, many visual cues facilitate their spotting, \eg multiple replays, particular camera views, or celebrations from the players and from the public. Cards and substitutions are more difficult to spot since the moment the referee shows a player a card and the moment a new player enters the field to replace another are rarely replayed (\eg for cards, the foul is replayed, not the sanction). Also, the number of visual cues that allow their identification is reduced, as these actions generally do not lead to celebrations from the players or the public. Besides, cards and substitutions may not be broadcast in full screen, as they are sometimes merely shown from the main camera and are thus barely visible. Finally, substitutions occurring during the half-time are practically impossible to spot, as said in the main paper.

\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/counting_metrics_cards.pdf}\\
    \caption{\textbf{Per-class results (cards).} A prediction of class \emph{card} is a {\color{anthoblue}\textbf{true positive (TP)}} with tolerance  when it is located at most  seconds from a ground-truth card. The baseline results are obtained from the best model of~\cite{Giancola_2018_CVPR_Workshops}. Our model spots most cards within  seconds around the ground truth ( seconds). 
    }
    \label{fig:cardmetrics}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{imgs/counting_metrics_substitutions.pdf}\\
    \caption{\textbf{Per-class results (substitutions).} A prediction of class \emph{substitution} is a {\color{anthoblue}\textbf{true positive (TP)}} with tolerance  when it is located at most  seconds from a ground-truth substitution. The baseline results are obtained from the best model of~\cite{Giancola_2018_CVPR_Workshops}. Our model spots most substitutions within  seconds around the ground truth ( seconds). 
    }
    \label{fig:substitutionmetrics}
\end{figure}









\mysection{Segmentation loss analysis.} We provide a supplementary analysis on the  parameter, which balances the segmentation loss and the action spotting loss in Equation~\ref{eq:LossFinal} of the main paper.
We fix different values of  and train a network for each value. We show the segmentation scores on one game for the \emph{goal} class in Figure~\ref{fig:lambdaAnalysis}. We also display the Average-mAP for the whole test set for the different values of .

It appears that extreme values of  substantially influence both the action spotting performance and the segmentation curves, hence the automatic highlights generation. Small values (\ie ) produce a useless segmentation for spotting the interesting unannotated \emph{goal opportunities}. This is because the loss does not provide a sufficiently strong feedback for the segmentation task as it does not penalize enough the segmentation scores. These values of  also lead to a decrease in the Average-mAP for the action spotting task, as already observed in the ablation study presented in the main paper. Moreover, very large values () penalize too much the unannotated goal opportunities, for which the network is then forced to output very small segmentation scores. Such actions are thus more difficult to retrieve for the production of highlights. These values of  also lead to a large decrease in the Average-mAP for the action spotting task, as the feedback of the segmentation loss overshadows the feedback of the spotting loss. Finally, it seems that for , the spotting performance is high while providing informative segmentation scores on \emph{goal opportunities}. These values lead to the spotting of several \emph{goal opportunities}, shown in Figure~\ref{fig:lambdaAnalysis}, which might be included in the highlights automatically generated for this match by the method described in the main paper.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/lambda_with_frames.png}
    \caption{\textbf{Influence of } on the segmentation and spotting results of the second half of the famous ``Remuntada" match, Barcelona - PSG, for the class \emph{goal}, for different values of . The best Average-mAP for the spotting task is located around , while the best value for spotting unannotated goal opportunities might be around . For this value, several meaningful \emph{goal opportunities} have a high \textbf{\color{orange}segmentation score}: \textbf{(a)} a shot on a goal post, \textbf{(b)} a free kick, \textbf{(c)} lots of dribbles in the rectangle, and \textbf{(d)} a headshot right above the goal.
    }
    \label{fig:lambdaAnalysis}
\end{figure}







\mysection{Comments on improvements on ActivityNet.}
In Table~3, we report the averages over samples of  results for each metric, that we further analyze statistically below for the Av.-mAP.
First, following D'Agostino's normality test, we can reasonably assume that the samples are normally distributed, since we obtain -values  ( for BMN and  for ours respectively). The standard deviations of the samples are  and . Since the difference between the averages is , the normal distributions overlap beyond two standard deviations from their centers, which shows that our improvements are beyond noise domain. Furthermore, Bartlett's test for equal variances gives a -value of  (), which allows us to use Student's -test to check whether the two samples can be assumed to have the same mean or not. We obtain a -value of \num{2.3e-18}, which strongly indicates that our results are significantly different from those of BMN and hence confirm the significant improvement.
For the AR@100 and AUC, similar analyses give final -values of \num{7.4e-3} and \num{9.8e-2}, which corroborates the statistical significance of our improvements.


\subsection{Extra Actions and Highlights Generation}


Figure~\ref{fig:match_189} shows additional action spotting and segmentation results. We can identify actions that are unannotated but display high segmentation scores such as goal opportunities and unsanctioned fouls. A goal opportunity around the  minute can be identified through the segmentation results. Besides, a false positive spot (green star) for a card is predicted around the  minute, further supported by a high segmentation score. A manual inspection reveals that a severe unsanctioned foul occurs at this moment. The automatic highlights generator presented in the main paper would include it in the summary of the match. Even though this foul does not lead to a card for the offender, the content of this sequence corresponds to an interesting action that would be tolerable in a highlights video.





\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/segmentation_detection189.pdf}
    \caption{
    \textbf{Extra action spotting and segmentation results.} These results are obtained on the second half of the match Barcelona - Espanyol in December 2016. {\color{anthoblue} \textbf{Ground truth actions}}, {\color{anthoorange}\textbf{temporal segmentation curves}}, and {\color{anthogreen}\textbf{spotting results (green stars)}} are illustrated. Unannotated actions can be identified and included in the highlights using our segmentation. For example, a goal opportunity occurs around the  minute. A false positive spot for a card is predicted by our network around the  minute. As it corresponds to a severe unsanctioned foul, it is fine for our automatic highlights generator to include it in the summary of the match.
    }
    \label{fig:match_189}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/FP_substitution_1.png}
    \caption{
    \textbf{False positive spot of a substitution} for the second half of the famous ``Remuntada" match, Barcelona - PSG, in March 2017. The LED panel used to announce substitutions is visible on the left, which presumably explains why the network predicted the sequence around this frame as a substitution.
    }
    \label{fig:FP_substitution}
\end{figure} 


Figure~\ref{fig:FP_substitution} shows a frame for which our network provides a high segmentation score and a false positive spot around the  minute (\ie  minute of the match) for \emph{substitutions} in Figure~\ref{fig:predsandsegs} of the main paper. We can see that the LED panel used by the referee to announce substitutions is visible on the frame. This may indicate that the network learns, quite rightly, to associate this panel with substitutions. As a matter of fact, at this moment, even the commentator announces that a substitution is probably imminent.   














 
\end{document}
