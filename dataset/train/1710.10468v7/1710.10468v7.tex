

\documentclass{article}
\usepackage{spconf,amsmath,graphicx,amssymb}
\usepackage[export]{adjustbox}
\usepackage{subfig}
\usepackage{color}
\usepackage{enumitem}
\usepackage{multirow}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\hypersetup{colorlinks=true}

\title{Speaker Diarization with LSTM}

\name{Quan Wang\textsuperscript{1} \quad Carlton Downey\textsuperscript{2} \quad Li Wan\textsuperscript{1} \quad Philip Andrew Mansfield\textsuperscript{1} \quad Ignacio Lopez Moreno\textsuperscript{1}\thanks{More information of this work can be found at: \url{https://google.github.io/speaker-id/publications/LstmDiarization}}}

\address{\textsuperscript{1}Google Inc., USA \qquad \textsuperscript{2}Carnegie Mellon University, USA\
\widetilde{k}=\arg\max_{k \geq 1} | \mathrm{MSCD} ' (k) | .

\widetilde{k}=\arg\max_{1 \leq k \leq n} \frac{\lambda_k}{\lambda_{k+1}} .


\item Let the eigen-vectors corresponding to the largest 
eigen-values be
.
We replace the th segment embedding by the corresponding dimension in these eigen-vectors:
.
Then we use the same K-Means algorithm in Sec. \ref{sec:kmeans} to cluster these
new embeddings, and produce speaker labels.
\end{enumerate}

\subsection{Discussion}
Speech data analysis is an extremely challenging problem domain, and conventional clustering algorithms such as K-Means often perform poorly. This is due to a number of unfortunate properties inherent to speech data, which include:

\begin{enumerate}[label=(\roman*)]
  \item \textbf{Non-Gaussian Distributions}: Speech data are often Non-Gaussion. In this setting, the centroid of a cluster (central to K-Means clustering)
        is not a sufficient representation.
  \item \textbf{Cluster Imbalance}: In speech data, it is often the case that one speaker will speak often, while other speakers will speak rarely. In this setting, K-Means may incorrectly split large clusters into several smaller clusters.
  \item \textbf{Hierarchical Structure}: Speakers fall into various groups according to gender, age, accent, \textit{etc.} This structure is problematic since the difference between a male and a female speaker is much larger than the difference between two female speakers. This makes it difficult for K-Means to distinguish between clusters corresponding to groups, and clusters corresponding to distinct speakers. In practice, this often causes K-Means to incorrectly cluster all embeddings corresponding to male speakers into one cluster, and all embeddings corresponding to female speakers into another.
\end{enumerate}

The problems caused by these properties are not limited to K-Means clustering, but are endemic to most parametric clustering algorithms. Fortunately, these problems can be mitigated by employing a non-parametric connection-based clustering algorithm such as spectral clustering.

\section{Experiments}
\label{sec:exp}

\begin{table*}
\centering
  \caption{DER (\%) on two English-only datasets for different embeddings and clustering
  algorithms.
  }
  \label{tab:der}
  \begin{tabular}{| c | c | c | c | c | c | c | c | c | c |}
    \hline
    \multirow{2}{*}{\bf Embedding} & \multirow{2}{*}{\bf Clustering}
    & \multicolumn{4}{|c|}{\bf CALLHOME American English Eval}
    & \multicolumn{4}{|c|}{\bf NIST RT-03 English CTS Eval} \\ \cline{3-10}
    & & Confusion & FA & Miss & Total & Confusion & FA & Miss & Total \\ \hline
    \multirow{4}{*}{i-vector} & Naive
    & 26.41 & \multirow{4}{*}{2.40} & \multirow{4}{*}{3.55} & 32.36 & 35.35 & \multirow{4}{*}{4.66} & \multirow{4}{*}{2.62} & 42.63 \\
    & Links
    & 25.40 & & & 31.36 & 33.56 & & & 40.48 \\
    & K-Means
    & 22.86 & & & 28.81 & 24.38 & & & 31.66 \\
    & Spectral
    & 14.59 & & & 20.54 & 13.84 & & & 21.12 \\ \hline
    \multirow{4}{*}{d-vector} & Naive
    & 12.41 & \multirow{4}{*}{1.94} & \multirow{4}{*}{4.51} & 18.87 & 18.76 & \multirow{4}{*}{4.09} & \multirow{4}{*}{4.45} & 27.30 \\
    & Links
    & 11.02 & & & 17.47 & 18.56 & & & 27.10 \\
    & K-Means
    & 7.29 & & & 13.75 & 7.80 & & & 16.34 \\
    & Spectral
    & 6.03 & & & 12.48 & 3.76 & & & 12.30 \\ \hline
  \end{tabular}
\end{table*}

\subsection{Models}
We run experiments with all combinations of both i-vector and d-vector models, with the four clustering algorithms discussed in Sec. \ref{sec:clustering}.
Both models are trained on an anonymized collection of voice searches,
which has around 36M utterances and 18K speakers.

The i-vector model is trained using 13 PLP coefficients with delta and delta-delta coefficients.
The GMM-UBM includes 512 Gaussians,
and the total variability matrix includes 100 eigen-vectors.
The final i-vectors are reduced to 50-dimensional using LDA.

The d-vector model is a 3-layer LSTM network with a final linear layer. Each LSTM layer has
768 nodes, with projection \cite{sak2014long} of 256 nodes.

Our Voice Activity Detection (VAD) model is a very small GMM model using the same PLP features
as i-vector. It only has two full covariance Gaussians:
one for speech, and one for non-speech. We found this simple VAD generalizes better
across domains (from queries to telephone) for diarization than
CLDNN \cite{zazo2016feature} VAD models.

\subsection{Datasets}
We report Diarization Error Rates (DER) on three standard public datasets:
(1) CALLHOME American English \cite{canavan1997callhome} (LDC97S42 + LDC97T14);
(2) 2003 NIST Rich Transcription (LDC2007S10),
the English conversational telephone speech (CTS) part;
and (3) 2000 NIST Speaker Recognition Evaluation (LDC2001S97), Disk-8.

The first two datasets are English only, and are relatively smaller. Thus we use these two datasets to compare different algorithms.

The third dataset is used by most diarization papers, and is usually directly referred to as ``CALLHOME" in literature. It contains 500 utterances distributed across six languages: Arabic, English, German, Japanese, Mandarin, and Spanish.

\subsection{Experiment setup}

Our diarization evaluation system is based on the pyannote.metrics library \cite{bredin2017pyannote}.


The CALLHOME American English dataset has a default 20-vs-20 utterances division for Dev-vs-Eval. For NIST RT-03 CTS,
we randomly divide the 72 utterances into 14-vs-58 Dev and Eval sets.
For each diarization system,
we tune the parameters such as Voice Activity Detector (VAD) threshold,
LSTM window size/step (Fig. \ref{fig:inference}), and clustering parameters on the Dev
set, and report the DER on the Eval set.

For NIST RT-03 CTS,
we only report DERs based on those provided un-partitioned evaluation map (UEM) files.
For the other two datasets, as is the standard convention in literature
\cite{castaldo2008stream,shum2013unsupervised,senoussaoui2014study,sell2015diarization,garcia2017speaker,zajic2017speaker},
we tolerate errors less than 250ms in locating segment boundaries. 

As is typical, for each audio file, multiple channels are merged into a single channel
\cite{shum2013unsupervised,sell2015diarization,dimitriadis2017developing},
and we do not process the parts that are before the first
annotation or after the last annotation. Additionally, as is standard in literature, we exclude
overlapped speech (multiple speakers speaking at the same time) from our evaluation.
For offline clustering algorithms, we constrain the system to produce at least 2 speakers.

\subsection{Results}

Our experimental results are shown in Table \ref{tab:der}, \ref{tab:der2} and \ref{tab:der3}.
We report the total DER together with its three components: False Alarm (FA), Miss,
and Confusion. FA and Miss are mostly from Voice Activity Detection errors, and
partly from the aggregation from frame-level i-vectors or window-level d-vectors to segments.
The FA and Miss differences between i-vector and d-vector are due to their different
window sizes/steps and aggregation logics.

In Table \ref{tab:der}, we can see that d-vector based diarization systems significantly
outperform i-vector based systems. For d-vector systems,
the optimal sliding window size and step are 240ms and 120ms, respectively.

We also observe that as expected, offline diarization produces significantly better results than online diarization. Specifically, online diarization predicts the incorrect number of speakers much more frequently than offline diarization.
This problem could potentially be mitigated by the addition of a ``burn-in" stage before entering the online mode.

In Table \ref{tab:der2}, we compare our d-vector + spectral clustering system with others' work on the same dataset.
Though our LSTM model is completely trained on out-of-domain and English-only data, we can still achieve state-of-the-art performance on this multilingual dataset. The performance could potentially be further improved by using in-domain training data and adding a final resegmentation step.

Additionally, in Table \ref{tab:der3}, we followed the same practice in \cite{zajic2017speaker} to evaluate our system
on a subset of 109 utterances from CALLHOME American English that have 2 speakers (called CH-109 in \cite{dimitriadis2017developing}).
Number of speakers is fixed to 2 for this evaluation.

\begin{table}
\centering
  \caption{DER (\%) on NIST SRE 2000 CALLHOME. Since we didn't do resegmentation, we report others' work by listing both with \& without Variational Bayesian (VB) resegmentation \cite{sell2015diarization}. Note that unlike others' work, our model is trained with out-of-domain data (English voice search vs. multilingual telephone speech).
  }
  \label{tab:der2}
  \begin{tabular}{| c | c c c c |}
    \hline
    \bf Method & \bf Confusion & \bf FA & \bf Miss & \bf Total \\ \hline
    d-vector + spectral & 12.0 & 2.2 & 4.6 & 18.8 \\
    Castaldo \textit{et al.} \cite{castaldo2008stream} & 13.7 & --- & ---  & --- \\
    Shum \textit{et al.} \cite{shum2013unsupervised} & 14.5 & --- & --- & --- \\
    Senoussaoui \textit{et al.} \cite{senoussaoui2014study} & 12.1 & --- & --- & --- \\
    Sell \textit{et al.} \cite{sell2015diarization} (+VB) & 13.7 (11.5) & --- & --- & --- \\
    Romero \textit{et al.} \cite{garcia2017speaker} (+VB) & 12.8 (9.9) & --- & --- & --- \\
    \hline
  \end{tabular}
\end{table}

\begin{table}
\centering
  \caption{DER (\%) on CALLHOME American English 2-speaker subset (CH-109).
  }
  \label{tab:der3}
  \begin{tabular}{| c | c c c c |}
    \hline
    \bf Method & \bf Confusion & \bf FA & \bf Miss & \bf Total \\ \hline
    d-vector + spectral & 5.97 & 2.51 & 4.06 & 12.54 \\
    Zaj{\'i}c \textit{et al.} \cite{zajic2017speaker} & 7.84 & --- & --- & --- \\
    \hline
  \end{tabular}
\end{table}


\subsection{Discussion}

Though we listed DER metrics from different papers in Table \ref{tab:der2} and \ref{tab:der3}, we find that it is difficult to fully align these numbers,
an unfortunately common problem in the diarization community.
This is due primarily to the large number of moving parts required for a functional diarization pipeline. For example, different teams
use different Voice Activity Detection marks (not publicly available), different training datasets, and different Dev sets for parameter tuning.

The evaluation protocols and software also differ from
paper to paper. Most teams exclude FA and Miss from their evaluations,
and directly refer to Confusion as their DER.
However, we observed that a poor VAD with high Miss usually filters out
the difficult parts in the speech, and makes the clustering problem much easier.
Some papers like \cite{dimitriadis2017developing} use the non-standard
Speaker Clustering Errors in frame percentage
as their metric, and also exclude FA and Miss from this error.
Additionally, it's unclear how overlapped speech is handled in some papers.

In our experiments, we do our best to ensure the comparisons are as fair as possible, and
avoid tuning parameters on Eval sets.

\section{Conclusions}
\label{sec:conclusions}


In this paper, we built on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combined LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. We conducted experiments on four clustering algorithms combined with both
i-vectors and d-vectors, and reported the performance on three standard public datasets: CALLHOME American English, NIST RT-03 English CTS, and NIST SRE 2000.
In general, we observed that
d-vector based systems achieve significantly lower DER than i-vector based systems.



\section{Acknowledgements}
\label{sec:ack}

We would like to thank Dr. Herv\'{e} Bredin for the continuous support with the
\texttt{pyannote.metrics} library. We would like to thank Dr. Gregory Sell and Prof. Pietro Laface for helping us
understand the evaluation datasets.
We would like to thank Yash Sheth and Richard Rose for the helpful discussions.

\newpage
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
