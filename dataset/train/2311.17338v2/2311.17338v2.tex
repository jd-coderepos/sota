\section{Experiments}
\label{sec:experiments}


\subsection{Experimental Setups}


\textbf{Data for Training}. Considering the quality of the video content, we select the Pexel Videos dataset \footnote{\url{https://huggingface.co/datasets/Corran/pexelvideos}} to serve as the training data. The dataset contains abundant videos with high quality, each averaging 19.5 seconds in duration. Owing to the lack of entity labels within the original data, we apply the processing approach detailed in \cref{subsec: training data process} to enhance the dataset's utility for our purposes. We process 76,303 videos for training and 100 videos for evaluation. In addition, we use the pre-trained model of VidRD~\cite{gu2023reuse}, which is trained on 5.3M video data.


\noindent
\textbf{Evaluation Metrics.}
We evaluate our proposed VideoAssembler using two aspects of metrics:

(i) Metrics for video quality evaluation. Previous works like~\cite{singer2022make, ge2023preserve, blattmann2023align} use two metrics for quantitative evaluation, i.e., \textbf{Fr\'{e}chet Video Distance (FVD)} ~\cite{thomas2019fvd} and Video \textbf{Inception Score (IS) }~\cite{saito2020generate}. FVD is a video quality evaluation metric based on FID~\cite{parmar2022fid}. Following~\cite{singer2022make}, we use a trained I3D model~\cite{carreira2017i3d} for calculating FVD. Following previous works ~\cite{singer2022make, hong2023cogvideo, blattmann2023align}, a trained C3D model~\cite{tran2015leanring} is used for calculating the video version of IS.

(ii) Metrics for identity consistency and video-prompt alignment.
a) We compute the \textbf{DINO score}~\cite{ruiz2023dreambooth} between the generated entity and the given entity to evaluate the entity fidelity. b) Following ~\cite{wu2023tune_a_video}, we calculate the average cosine similarity between all pairs of video frames to evaluate the \textbf{Frame-consistency}. We calculate the average CLIP score between all frames of generated videos and corresponding prompts to evaluate the \textbf{Textual-alignment}.


\subsection{Main Results}

There is no existing work for identity-consistent video generation with reference entities to directly compare with. However, our framework is flexible and allows us to control the number of reference entities for different tasks, such as image-to-video generation with single or multiple frames or video editing with a video. Furthermore, the generative aspect of our VideoAssembler is regulated through textual prompts, thus our comparative analysis primarily encompasses the domains of text-to-video, image-to-video synthesis, and video editing tasks.

\begin{table}[ht]
    \setlength{\tabcolsep}{1.2mm}
    \centering
    \begin{tabular}{lcrr}
       \toprule
       Model & Videos for Training & IS  & FVD  \\
       \midrule
       CogVideo~\citep{hong2023cogvideo} & 5.4M & 25.27 & 701.59 \\
       MagicVideo~\citep{zhou2022magicvideo} & 27.0M & - & 699.00 \\
       LVDM~\citep{he2022lvdm} & 2.0M & - & 641.80 \\
       Video LDM~\citep{blattmann2023align} & 10.7M & 33.45 & 550.61 \\
       Make-A-Video~\citep{singer2022make} & 20.0M & 33.00 & 367.23 \\
       VideoFactory~\citep{wang2023videofactory} & 140.7M & - & 410.00 \\
       PYoCo~\cite{ge2023preserve}  & 22.5M & 47.76 & 355.19 \\
       \midrule
       \rowcolor{gray!20}
       \rowcolor{gray!20}
       I2VGen-XL~\cite{zhang2023i2vgen-xl} & 10.3M & 18.90 & 597.49 \\
       \midrule
       \textbf{VideoAssembler} & \textbf{5.3M+76K} & \textbf{48.01} & \textbf{346.84} \\
       \bottomrule
    \end{tabular}
    \caption{
    Quantitative evaluation results on UCF-101.The second and third rows show text-to-video and image-to-video (in gray) methods. All the videos for evaluation are generated in a zero-shot manner. 
    In comparison with other methods, VideoAssembler achieves the best metrics while using fewer videos for training.
    }
    \label{tab:eval_ucf101}
\end{table}

\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{figure6.pdf}
\caption{Qualitative results and comparison between our VideoAssembler and VideoDreamer ~\cite{chen2023videodreamer}, which also is en entity driven method. Our method demonstrates superior entity fidelity and action control compared to it, as evidenced by (example c). Moreover, our model exhibits finer motion control (examples a, b) and good performance in video editing (example d).}
\label{fig: qualitative results}
\end{figure*}

\noindent
\textbf{Quantitative Evaluation.}
To prove the efficiency of the proposed VideoAssembler, we conduct the generative performance on UCF-101~\cite{soomro2012ucf101}, which has 101 brief class names (10,000 videos for test). The dataset which is commonly employed to assess the generation performance of various methods, such as works~\cite{singer2022make, hong2023cogvideo, wang2023videofactory}. To acquire the necessary reference entity images for our method, we preprocess the UCF-101 video dataset employing the same steps as delineated for our training data preparation. For inference on the UCF-101 dataset, we utilize a singular textual prompt and four reference entities. ~\cref{tab:eval_ucf101} shows the FVD and IS results. It is discernible that our method exhibits good performance when benchmarked against other SOTA approaches. The outcomes further indicate that our method is capable of generating videos with superior performance metrics. Noticed that most of the existing image-to-video methods do not report quantitative results, but only provide visualized examples, such as ~\cite{molad2023dreamix, chen2023videocrafter}.

\begin{table}[ht]
    \setlength{\tabcolsep}{1.5mm}
    \centering
    \begin{tabular}{lcrr}
       \toprule
       Model & Videos for Training & IS  & FVD  \\
       \midrule
       CogVideo~\cite{hong2023cogvideo} & 5.4M & - & 1294 \\
       MagicVideo~\cite{zhou2022magicvideo} & 27.0M & - & 998 \\
       \midrule
       \rowcolor{gray!20}
       \rowcolor{gray!20}
       I2VGen-XL~\cite{zhang2023i2vgen-xl} & 10.3M & 10.52 & 341.72 \\
       \midrule
       \textbf{VideoAssembler} & \textbf{5.3M+76K} & \textbf{15.79} & \textbf{252.0} \\
       \bottomrule
    \end{tabular}
    \caption{Results on MSR-VTT. The second and third rows show text-to-video and image-to-video methods (in gray), respectively.}
    \label{tab:eval_msrvtt}
\end{table}


We also conduct a zero-shot comparison with methods~\cite{hong2023cogvideo,zhou2022magicvideo,zhang2023i2vgen-xl} on the MSR-VTT dataset (2,990 videos). The results of FVD and IS metrics are reported in \cref{tab:eval_msrvtt}, which show that our proposed method surpasses its counterparts in performance metrics. Noticed that our method is 89.72 higher than I2VGen-XL, which is an image-to-video generation work, on the FVD metric. Besides, to demonstrate the fidelity and identity consistency of entities within the videos synthesized by VideoAssembler, we employ the DINO score, Textual-align, and Frame-consistency for evaluative purposes on the UCF-101 and MSR-VTT compared with~\cite{zhang2023i2vgen-xl}. The results are shown in ~\cref{tab:dino_alignment_consistency}, which indicate that the videos we generate have good text alignments, and the continuity between frames is also commendable.



\begin{table}[ht]
    \setlength{\tabcolsep}{1.2mm}
    \centering
    \begin{tabular}{lccc}
       \toprule
       Methods & DINO & Textual-align & Frame-consist \\
       \midrule
       I2VGen-XL~\cite{zhang2023i2vgen-xl} & 44.1 & 21.34 & 89.40 \\
       \textbf{VideoAssembler} & \textbf{50.8} & \textbf{25.41} & \textbf{90.22} \\
       \midrule
       \rowcolor{gray!20}
       I2VGen-XL~\cite{zhang2023i2vgen-xl} & 42.7 & 18.42 & \textbf{89.77} \\
       \rowcolor{gray!20}
       \textbf{VideoAssembler} & \textbf{49.5} & \textbf{21.57} & 84.76 \\
       \bottomrule
    \end{tabular}
    \caption{Results of DINO, textual-alignment, and frame-consistency on UCF-101 and MSR-VTT. The white part is the result of UCF-101, and the gray part is the result of MSR-VTT.}
    \label{tab:dino_alignment_consistency}
\end{table}

\noindent
\textbf{Human Evaluations.} We perform human evaluations of our method, which is conducted by a panel of 34 human raters, over 15 videos with corresponding prompts. We adopt the Likert Scale~\cite{likert1932technique} to evaluate entity fidelity, prompt alignment, and the quality of the generated videos. The range of these three scores is between 1 and 5, which represents from very dissatisfied to very satisfied. We show the results of human evaluations in \cref{tab:human_evaluation}. Because there is no similar work that can be directly compared, we tend to focus more on user satisfaction for the videos we generate when designing user evaluations. Our method achieves high scores on both entity fidelity and text alignment, as well as producing high-quality videos. These results indicate that our method performs well under human evaluation and demonstrates its superiority over existing methods.

\begin{table}[ht]
    \setlength{\tabcolsep}{2.0mm}
    \centering
    \begin{tabular}{lc}
       \toprule
       Scores & VideoAssembler \\
       \midrule
       \textbf{Entity Fidelity} & 4.16  \\
       \textbf{Prompt Alignment} & 4.09  \\
       \textbf{Quality} & 3.72 \\
       \bottomrule
    \end{tabular}
    \caption{Results of human evaluations. The value indicates the degree of user satisfaction with the generated content, which is an absolute measure with a maximum score of 5 at the design stage.}
    \label{tab:human_evaluation}
\end{table}

\noindent
\textbf{Performance of Video Editing.} In addition, we put the whole video into VideoAssembler to achieve video editing. we evaluate the CLIP-Score of text and image on the DAVIS, which is built by~\cite{jay2023loveu} for video editing test. The results compared with~\cite{brooks2023instructpix2pix,qi2023fatezero} are shown in \cref{tab:video editing}.

\begin{table}[ht]
    \setlength{\tabcolsep}{1.5mm}
    \centering
    \begin{tabular}{lccc}
       \toprule
       Methods & Textual-align & Frame-consistency \\
       \midrule
       Framewise IP2P~\cite{brooks2023instructpix2pix} & 25.11 & 86.76 \\
       FateZero~\cite{qi2023fatezero} & 23.81 & \textbf{92.92} \\
       \midrule
       \textbf{VideoAssembler} & \textbf{27.65} & 89.43 \\
       \bottomrule
    \end{tabular}
    \caption{Results of video editing on DAVIS dataset. Noticed that method~\cite{brooks2023instructpix2pix}  can only perform image editing, and for comparison, we repeated the evaluation of its generated video frames.}
    \label{tab:video editing}
\end{table}

\noindent
\textbf{Qualitative Evaluation.} We show the qualitative results and comparisons with VideoDreamer~\cite{chen2023videodreamer} in \cref{fig: qualitative results}. We delineate four distinct configurations for analysis: input with a single entity, input with multiple entities, utilization of the same entity as in VideoDreamer, and the video editing task. In the context of video editing, we utilize content originally synthesized by VideoCrafter1~\cite{chen2023videocrafter} as our foundational material. When contrasted with VideoDreamer, as illustrated in example (c), the superior fidelity with which our method renders the input entity is apparent. 
Examples (a) and (b) demonstrate our model's adaptability to either single or multiple entity inputs, while example (d) showcases its proficient editing capabilities.



\begin{figure*}[ht]
\centering
\includegraphics[width=0.94\linewidth]{figure7.pdf}
\caption{We present the outcomes of VideoAssembler without the REP and EPAF in examples (a, b), respectively. 
Selection of the entity as a dog, accompanied by subsequent text replacement with 'dog' and 'cat' in examples (c, d), and show the model's control ability.
}
\label{fig: ablation}
\end{figure*}

\subsection{Ablation Studies}

\begin{table}[ht]
    \setlength{\tabcolsep}{8.0mm}
    \centering
    \setstretch{0.9}
    \begin{tabular}{lcc}
       \toprule
       Frames &  IS  & FVD  \\
       \midrule
        & 47.02 & 406.69 \\
        & 48.01 & \textbf{346.84} \\
        & \textbf{49.13} & 506.61 \\
       \midrule
       \rowcolor{gray!20}
        & 17.40 & 283.14 \\
       \rowcolor{gray!20}
        & 15.79 & \textbf{252.05} \\
       \rowcolor{gray!20}
        & \textbf{19.10} & 324.17 \\
       \bottomrule
    \end{tabular}
    \caption{Effect of the quantity of the input entity. The white part is the result of UCF-101, and the gray part is the result of MSR-VTT.}
    \label{tab:quantity-effect}
\end{table}

\noindent
\textbf{Effects of Input Entity Quantity.} 
Given that our method accommodates a diverse array of inputs encompassing varying numbers of entities, we devise an experiment to ascertain the impact of varying the number of input frames, denoted as . Our evaluation strategy involves assessing the algorithm with distinct input configurations: singular (1) and multiple (4 and 8). We use FVD and IS on UCF-101 and IS on MSR-VTT to evaluate our method. Analysis of the data in \cref{tab:quantity-effect} reveals that the IS metric achieves optimal performance at , while the FVD metric records its best performance at . Due to the minimal fluctuation of the IS index under different  values, we consider the model to perform best when . This also suggests that our approach requires only a minimal number of entities to accomplish high-quality video generation.

\noindent
\textbf{Effects of the REP Encoder and the EPAF Module.} Within our VideoAssembler framework, the REP encoder principally assimilates detailed appearance to enhance the fidelity of input entities. Concurrently, the EPAF module integrates text-alignment attributes of the entities into the denoising process, enriching the model's capability. 
To assess each part's effectiveness, we evaluate the model separately without the REP encoder and EPAF module.
We show the visualization results in \cref{fig: ablation}. Examples (a) and (b) demonstrate that the inclusion of our REP encoder is crucial for maintaining the fidelity of the input entities. This substantiates that the features introduced into the cross-attention layer are predominantly semantic in nature, akin to the approach employed by Method ~\cite{wang2023videocomposer}. Moreover, it can be observed that the exclusive use of the REP encoder may impinge upon the generative creativity of the model, thereby diminishing its text-alignment capability. Case (b) illustrates this phenomenon, indicating that in instances where foreground content is lacking, the REP encoder fails to reconcile the generated content with its semantic counterpart.

\noindent
\textbf{Effects of Prompts for Identical Entities.} To discern the dominant influence between textual prompts and input entities, given their dual control mechanism, we design an experiment as follows: for a given entity image, we input text descriptions that are either congruent or incongruent with the entity's category. When given an entity image of a dog, we assess the prompts ``dog'' and ``cat''. The corresponding results are illustrated in examples (c, d) of \cref{fig: ablation}. The analysis reveals that while the prompt exerts control over the generated video, the overarching structure of the content is also steered by the entity. 
This affirms that our method can blend the entity in the video and control it with prompts.