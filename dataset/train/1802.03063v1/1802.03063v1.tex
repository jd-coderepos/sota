\documentclass{article} \usepackage{iclr2018_conference,times}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      
\usepackage{graphicx} \usepackage{subfigure} 
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin} \DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator*{\argmax}{argmax} \usepackage{tabularx}
\usepackage{multirow}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\usepackage{lipsum}


\title{Learning Latent Representations \\in Neural Networks for Clustering \\through Pseudo Supervision \\and Graph-based Activity Regularization}


\author{
  Ozsel~Kilinc \\
  Electrical Engineering Department\\
  University of South Florida\\
  Tampa, FL 33620 \\
  \texttt{ozsel@mail.usf.edu} \\
\And
  Ismai~Uysal  \\
  Electrical Engineering Department \\
  University of South Florida \\
  Tampa, FL 33620  \\
  \texttt{iuysal@usf.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}

\maketitle

\begin{abstract}
	
In this paper, we propose a novel unsupervised clustering approach exploiting the hidden information that is indirectly introduced through a pseudo classification objective. Specifically, we randomly assign a pseudo parent-class label to each observation which is then modified by applying the domain specific transformation associated with the assigned label. Generated pseudo observation-label pairs are subsequently used to train a neural network with Auto-clustering Output Layer (ACOL) that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised objective based on Graph-based Activity Regularization (GAR) terms, softmax duplicates of each parent-class are specialized as the hidden information captured through the help of domain specific transformations is propagated during training. Ultimately we obtain a $k$-means friendly latent representation. Furthermore, we demonstrate how the chosen transformation type impacts performance and helps propagate the latent information that is useful in revealing unknown clusters. Our results show state-of-the-art performance for unsupervised clustering tasks on MNIST, SVHN and USPS datasets, with the highest accuracies reported to date in the literature.
\end{abstract}

\section{Introduction}

Clustering, the unsupervised process of grouping similar examples together, is one of the most fundamental challenges in machine learning research and has been studied extensively in different aspects such as feature selection, distance functions, grouping methods, etc. \citep{aggarwal2013}. $k$-means \citep{macqueen1967some} and Gaussian Mixture Models (GMM) \citep{Bishop07} are two well-known conventional clustering algorithms that are applicable to a wide range of problems. Traditionally, these methods are applied to low-level features such as raw data or gradient-orientation histograms (HOG) for images. Therefore, their distance metrics are limited to local relations in the data space and inadequate to represent hidden dependencies in latent spaces. On the other hand, spectral clustering \citep{Luxburg07} is another conventional approach producing more flexible distance metrics than $k$-means and GMM. However, these types of solutions are not scalable to large datasets as they need to compute the full graph Laplacian matrix.

In recent years, researchers have focused on the unsupervised learning of high-level features on which to apply clustering and shown that learning good representations is important for the accuracy and robustness of the clustering task. Deep Embedding Clustering (DEC) \citep{XieGF16} was proposed to simultaneously learn feature representations and cluster assignments using deep neural networks (DNN). In this approach, first DNN parameters are initialized with a layer-wise trained deep autoencoder \citep{VincentLLBM10} and then the initialized DNN is used to obtain the latent representation on which to perform $k$-means clustering for the initialization of cluster centers. This complicated initialization is followed by a challenging optimization process that minimizes the Kullbackâ€“Leibler (KL) divergence between the centroid-based probability distribution and the auxiliary target distribution derived from the soft cluster assignments. Similarly, Joint Unsupervised Learning (JULE) \citep{YangPB16} combines agglomerative clustering with convolutional neural networks (CNN) and formulates them as a recurrent process. Although JULE proposes an end-to-end learning framework, it suffers scalability issues due to its agglomerative clustering. 

Novel deep generative models that can be trained via direct backpropagation have recently been proposed avoiding the difficulties in preexisting generative models such as Restricted Boltzmann Machines (RBM), Deep Belief Networks (DBN) and Deep Boltzmann Machines (DBM) that are trained by MCMC-based algorithms \citep{hinton2006fast, boltzmann}. Among two canonical examples of these models, Variational Autoencoders (VAE) \citep{KingmaW13, RezendeMW14} integrate stochastic latent variables into the conventional autoencoder architecture while Generative Adversarial Networks (GAN) \citep{GoodfellowPMXWOCB14} propose an adversarial training procedure implementing a min-max adversarial game between two neural networks: the discriminator and the generator. Following these advances, researchers have started to study new hybrid models with the goal of performing unsupervised clustering through deep generative models. For example, Variational Deep Embedding (VaDE) \citep{JiangZTTZ17} proposed a clustering framework combining VAE and GMM together. Also, Gaussian Mixture Variational Autoencoder (GMVAE) \citep{DilokthanakulMG16} built upon the semi-supervised model by \citet{KingmaMRW14} to perform unsupervised clustering within the VAE framework with a Gaussian mixture as a prior distribution. GAN-based methods include: Categorical Generative Adversarial Networks (CatGAN) \citep{Springenberg15}, an approach incorporating neural network classifiers with an adversarial generative model, and Adversarial Autoencoder (AAE) \citep{MakhzaniSJG15}, a probabilistic autoencoder variant integrating traditional reconstruction error with adversarial training criterion of GANs. Besides, \citet{premachandran2016} proposes to fuse the disentangled features learned by Information Maximizing Generative Adversarial Networks (InfoGAN), an extension to GANs that uses mutual information to induce representation, with $k$-means clustering.

In this paper, we propose a novel unsupervised clustering approach building upon the previous study on learning of latent annotations in a particular semi-supervised setting where a coarse level of supervision is available for all observations, i.e. parent-class labels, but the model has to learn a fine level of latent annotations, i.e. sub-classes, under each one of these parents. For clarification, assume that we are given a dataset of hand-written digits such as MNIST \citep{lecun1998mnist} where the overall task is the complete categorization of each digit, but the only available supervision is whether a digit is smaller or greater than 5. To study this particular semi-supervised setting on neural networks, \citet{KilincU17ACOL} proposed a novel output layer modification, Auto-clustering Output Layer (ACOL). ACOL allows simultaneous supervised classification (per provided parent-classes) and unsupervised clustering (within each parent) where clustering is performed through Graph-based Activity Regularization (GAR) technique recently proposed in \citet{KilincU17GAR}. More specifically, as ACOL duplicates the softmax nodes at the output layer for each class, GAR allows for competitive learning between these duplicates on a traditional error-correction learning framework. 

To learn latent annotations in a fully unsupervised setup, we substitute the real, yet unavailable, parent-class information with a pseudo one. More specifically, we choose a domain specific transformation to be applied to the observations in a dataset to generate examples for a pseudo parent-class. The transformed dataset constitutes the examples of that pseudo parent-class and every new transformation generates a new one. Regarding the MNIST example for this fully unsupervised setting, now we simply augment the dataset by applying a transformation to examples, e.g. rotating by $90^o$, and label transformed examples as \textit{rotated} and non-transformed examples as \textit{original}. This new augmented dataset is provided to the network as a two-class classification problem with pseudo classes labeled as \textit{original} and \textit{rotated} as visualized in Figure \ref{fig:pseudo_motivation}. While being trained over this pseudo supervision, through ACOL and GAR, the neural network learns the latent representation distinguishing the real digit identities in an unsupervised manner.

The idea of employing an auxiliary task to learn a good data representation has been previously studied for different domains \citep{CollobertWBKKK11, AhmedYXGX08}. Most recent study, Exemplar CNN \citep{DosovitskiyFSRB16}, proposed to use a regularizer enforcing the feature representation to be approximately invariant to the transformations while training the network to discriminate between a set of pseudo parent-classes (``surrogate classes'' with their definition). This approach requires thousands of transformations to obtain a good representation and also it cannot exploit more than 300 examples per ``surrogate class'' severely limiting its scalability. Furthermore, some elementary transformations, such as rotation, have only a minor impact on the performance. In comparison, in our approach, only 8 pseudo parent-classes generated by rotation-based transformations provide a rich latent representation to obtain state-of-the-art unsupervised clustering performance. 

\begin{figure}[t]
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth,trim={0.5cm 0.5cm 0.5cm 0.5cm},clip]{figures/example}}
		\caption{Assume that we are given a dataset of hand-written digits such as MNIST where the overall task is the complete categorization of each digit. Then, we simply augment the dataset by applying a transformation to examples, e.g. rotating by $90^o$, and label each of them either as \textit{original} or as \textit{rotated}. This new augmented dataset is provided to the network as a two-class classification problem. While being trained over this pseudo supervision, through ACOL and GAR, the neural network also learns the latent representation distinguishing the real digit identities in unsupervised an manner.}
		\label{fig:pseudo_motivation}
	\end{center}
	\vskip -0.2 in
\end{figure}

\section{Background}
\subsection{Auto-clustering Output Layer}

Unlike traditional output layer structure, the Auto-clustering Output Layer (ACOL) \citep{KilincU17ACOL} defines more than one softmax node ($k_s$ duplicates) per parent-class. Outputs of $k_s$ duplicated softmax nodes that belong to the same parent are then combined in a subsequent pooling layer for the final prediction. Training is performed in the configuration shown in Figure~\ref{fig:pseudo_clustering} where $n_p$ is the number of parent-classes. This might look like a classifier with redundant softmax nodes. However, duplicated softmax nodes of each parent are specialized using GAR throughout the training in a way that each one of $n=n_pk_s$ softmax nodes represent an individual sub-class of a parent, i.e. annotation.

\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=80mm,trim={1cm 1.1cm 1.8cm 1.2cm},clip]{figures/schema_acol}}
		\caption{Neural network structure with the ACOL. Each softmax node corresponds to an individual sub-class of a parent, i.e. annotation. During feedforward operation of the network, pooling layer calculates final parent-class predictions through sub-class probabilities.}
		\label{fig:pseudo_clustering}
	\end{center}
	\vskip -0.2 in
\end{figure}

In order to mathematically describe this modification, let us consider a neural network with $L-1$ hidden layers where $l$ denotes the individual index for each layer such that $l \in \{0,...,L\}$. Let $\boldsymbol{Y} ^{(l)}$ denote the output of the nodes at layer $l$. $\boldsymbol{Y} ^{(0)}=\boldsymbol{X}$ is the input and $f(\boldsymbol{X})=f^{(L)}(\boldsymbol{X})=\boldsymbol{Y}^{(L)}=\boldsymbol{Y}$ is the output of the entire network. $\boldsymbol{W} ^{(l)}$ and $\textbf{b}^{(l)}$ are the weights and biases of layer $l$, respectively. Then, the feedforward operation of the neural networks can be written as 
\begin{equation}
\label{eq:pseudo_neuralnetwork}
f^{(l)}\big(\boldsymbol{X}\big) = 
\boldsymbol{Y}^{(l)} = 
h^{(l)}\big(\boldsymbol{Y}^{(l-1)}\boldsymbol{W}^{(l)} + \boldsymbol{b}^{(l)}\big)
\end{equation}
where $h^{(l)}$(.) is the activation function applied at layer $l$. 

For ACOL networks, $h^{(L-1)}(.)$ and $h^{(L)}(.)$ respectively correspond to softmax and linear activation functions. Also, $\boldsymbol{W}^{(L)}: = [\boldsymbol{I}_{n_p} \dots \boldsymbol{I}_{n_p}]^T$ and $\boldsymbol{b}^{(L)} := \boldsymbol{0}$ where $\boldsymbol{I}$ denotes the identity matrix as ACOL simply defines constant weights between the augmented softmax layer and the pooling layer to sum up the output probabilities of the softmax nodes belonging to the same parent. Let $\boldsymbol{Z}$ denote the activities at the input of augmented softmax layer such that 
\begin{equation}
\boldsymbol{Z} := \boldsymbol{Y}^{(L-2)}\boldsymbol{W}^{(L-1)} + \boldsymbol{b}^{(L-1)}
\end{equation} 
corresponding to an $m \times n$ matrix where $m$ is the number of examples and $n$ is the total number of all softmax nodes at the augmented softmax layer such that $n=n_pk_s$, where $n_p$ is the number of parent-classes and $k_s$ is the clustering coefficient of ACOL.
Then, the output of the ACOL applied network can be written in terms of $\boldsymbol{Z}$ as 
\begin{equation}
\boldsymbol{Y} =  
\softmax\big(\boldsymbol{Z}\big)\boldsymbol{W}^{(L)}
\end{equation}

\subsection{Graph-based Activity Regularization} 

\citet{KilincU17ACOL} adopted the Graph-based Activity Regularization (GAR) technique \citep{KilincU17GAR} as the unsupervised regularization term to create competition between the duplicated softmax nodes of the augmented softmax layer which ultimately results in specialized but equally-active softmax nodes each representing a latent annotation within a parent.

The GAR technique applies the regularization over the positive part of the activities at the input of softmax nodes such that 
\begin{equation}
\label{eq:pseudo_garpredictions}
g\big(\boldsymbol{X}\big) = 
\boldsymbol{B} := 
\max{\big(\boldsymbol{0}, \boldsymbol{Z}\big)}
\end{equation}
and defines two terms to turn $n \times n$ symmetric matrix $\boldsymbol{N}$, which is defined as  $\boldsymbol{N}:=\boldsymbol{B}^T\boldsymbol{B}$, into the identity matrix. While the \textit{affinity} term penalizes the non-zero off-diagonal entries of $\boldsymbol{N}$, \textit{balance} attempts to equalize diagonal entries. Let $\boldsymbol{v}$ be a $1 \times n$ vector representing the diagonal entries of $\boldsymbol{N}$ such that $\boldsymbol{v}:=[N_{11} \dots N_{nn}]$ and $\boldsymbol{V}$ be defined as $n \times n$ symmetric matrix such that $\boldsymbol{V}:=\boldsymbol{v}^T\boldsymbol{v}$. Then, the \textit{affinity} and \textit{balance} terms can be written as

\begin{minipage}{0.5\linewidth}
	\begin{equation}
	\label{eq:pseudo_affinity}
	\textit{Affinity} = \alpha\big(\boldsymbol{B}\big) :=\frac{\sum\limits_{i \ne j}^n{N_{ij}}}{(n-1)\sum\limits_{i = j}^n{N_{ij}}}
	\end{equation}
\end{minipage}\begin{minipage}{0.5\linewidth}
	\begin{equation}
	\label{eq:pseudo_balance}
	\textit{Balance} = \beta\big(\boldsymbol{B}\big) := \frac{\sum\limits_{i \ne j}^n{V_{ij}}}{(n-1)\sum\limits_{i = j}^n{V_{ij}}}
	\end{equation}
\end{minipage}

which modifies the overall objective function of the training proposed in \citet{KilincU17ACOL} as
\begin{equation}
\label{eq:pseudo_overallobj}
\mathcal{L}\big(f\big(\boldsymbol{X}\big), \boldsymbol{t}\big) + 
\mathcal{U}\big(g\big(\boldsymbol{X}\big)\big) = 
\mathcal{L}\big(\boldsymbol{Y}, \boldsymbol{t}\big) + c_{\alpha}\alpha\big(\boldsymbol{B}\big) + c_{\beta}\big(1-\beta\big(\boldsymbol{B}\big)\big) + c_F||\boldsymbol{B}||^2_F
\end{equation}
where $\mathcal{L}(.)$ is the supervised log loss function, $\boldsymbol{t}=[t_1\dots t_{m}]^T$ is the vector of provided parent-class labels such that $t_i \in \{1,...,n_p\}$ (recall that, in the semi-supervised setting considered in \citet{KilincU17ACOL}, there is a real partial supervision available for all examples, e.g. a digit is smaller or greater than 5), $\mathcal{U}(.)$ is the unsupervised regularization term consisting of \textit{affinity}, \textit{balance} and $||\boldsymbol{B}||_F$ (the Frobenius norm for $\boldsymbol{B}$) that is employed to limit the denominators of both \textit{affinity} and \textit{balance} terms not to diminish their effects and $c_{\alpha}, c_{\beta}, c_F$ are the weighting coefficients. 

GAR has been originally proposed for the classical type of semi-supervised setting where the number of labeled observations is much smaller than the number of unlabeled observations, but all existing classes are equally represented by the available labels even at limited numbers. \citet{KilincU17GAR} have shown that defining the objective of the regularization over the matrix $\boldsymbol{N}$ yields a scalable and efficient graph-based solution and that the entire operation corresponds to propagating the available labels across the graph $\mathcal{G}_\mathcal{M}$ whose edges are specified by the $m \times m$ symmetric matrix $\boldsymbol{M}:=\boldsymbol{B}\boldsymbol{B}^T$ that infers the adjacency of the examples based on the predictions of the neural network. More specifically, it has been shown that as the matrix $\boldsymbol{N}$ turns into the identity matrix, $\mathcal{G}_\mathcal{M}$ becomes a disconnected graph including $n$ disjoint subgraphs each of which is $\nicefrac{m}{n}$-regular. This indicates that the strong adjacencies in the matrix $\boldsymbol{M}$ get stronger, weak ones diminish and each label is propagated to $\nicefrac{m}{n}$ examples through the strong adjacencies. 

On the other hand, in the particular semi-supervised setting considered by \citet{KilincU17ACOL} (i.e. a coarse level of labeling is available for all observations but the model still needs to learn a fine level of latent annotation for each one of them), when applied to an ACOL network, GAR provides that the latent information introduced by the coarse supervision is propagated from the graph $\mathcal{G}_\mathcal{Y}$ (whose edges are specified by $m \times m$ symmetric matrix $\boldsymbol{Y}\boldsymbol{Y}^T$) to its spanning subgraph $\mathcal{G}_\mathcal{M}$ to reveal deeper latent annotations. In other words, although these two graphs are made up of the same vertices ($m$ examples) while propagating the latent information that is captured through supervised adjacency introduced by $\mathcal{G}_\mathcal{Y}$
across $\mathcal{G}_\mathcal{M}$, GAR terms eliminate some of the edges of $\mathcal{G}_\mathcal{Y}$ from $\mathcal{G}_\mathcal{M}$ in a way that $\mathcal{G}_\mathcal{M}$ ultimately becomes a disconnected graph of $n$ disjoint subgraphs each of which now corresponds to a latent annotation.

\section{Proposed Framework}
\subsection{Objective Function}

The unsupervised clustering approach proposed in this paper adopts the same framework introduced in \citet{KilincU17ACOL}. Since the real parent-class labels (a digit is smaller or greater than 5) are unavailable in a fully unsupervised setting, we randomly assign pseudo parent-class labels each of which is associated with a domain specific transformation used to generate the examples of that pseudo parent-class.

In this setting, $n_p$ now corresponds to the number of pseudo parent-classes and $\boldsymbol{\tilde{t}}=[\tilde{t}_1\dots\tilde{t}_m]^T$ is a vector of randomly assigned pseudo parent-class labels which are uniformly distributed across $n_p$ pseudo parent-classes such that $\tilde{t}_i \in \{1,...,n_p\}$. Also, there exists a set of transformations $\mathcal{S}_\mathcal{T}=\{\mathcal{T}_1,...,\mathcal{T}_{n_p}\}$ where transformation $\mathcal{T}_j$ is used to generate the examples of the $j$\textsuperscript{th} pseudo parent-class such that $\boldsymbol{\tilde{x}}_i=\mathcal{T}_j(\boldsymbol{x}_i)$. $\mathcal{S}_\mathcal{T}$ also includes non-transformation $\mathcal{T}_1$ providing $\boldsymbol{\tilde{x}}_i=\mathcal{T}_1(\boldsymbol{x}_i)=\boldsymbol{x}_i$ to ensure that the original observations are introduced to the network during training. $\boldsymbol{\tilde{t}}$ is associated with a vector of transformations $\boldsymbol{T}=[T_1 \dots T_m]^T$ such that $T_i=\mathcal{T}_{\tilde{t}_i}$. 

Let $\odot$ be an element-wise operation defined between the vector of transformations $\boldsymbol{T}$ and the original input $\boldsymbol{X}=[\boldsymbol{x}_1 \dots \boldsymbol{x}_m]^T$ such that
\begin{equation}
\boldsymbol{\tilde{X}} = 
\begin{bmatrix}
\boldsymbol{\tilde{x}}_{1} \\ \boldsymbol{\tilde{x}}_{2} \\ \vdots \\ \boldsymbol{\tilde{x}}_m \\
\end{bmatrix} 
=
\boldsymbol{T} \odot \boldsymbol{X} = 
\begin{bmatrix}
T_{1} \\ T_{2} \\ \vdots \\ T_m \\
\end{bmatrix}
\odot
\begin{bmatrix}
\boldsymbol{x}_{1} \\ \boldsymbol{x}_{2} \\ \vdots \\ \boldsymbol{x}_m \\
\end{bmatrix} 
=
\begin{bmatrix}
T_{1}(\boldsymbol{x}_{1}) \\ T_{2}(\boldsymbol{x}_{2}) \\ \vdots \\ T_{m}(\boldsymbol{x}_m) \\
\end{bmatrix}
=
\begin{bmatrix}
\mathcal{T}_{\tilde{t}_1}(\boldsymbol{x}_{1}) \\ \mathcal{T}_{\tilde{t}_2}(\boldsymbol{x}_{2}) \\ \vdots \\ \mathcal{T}_{\tilde{t}_m}(\boldsymbol{x}_m) \\
\end{bmatrix} 
\end{equation}
where $\boldsymbol{\tilde{X}}$ corresponds to the modified input per randomly assigned pseudo labels $\boldsymbol{\tilde{t}}$. The output of the entire network and the positive part of the augmented softmax layer activities respectively become $\boldsymbol{Y} = f(\boldsymbol{\tilde{X}})$ and $\boldsymbol{B} = g(\boldsymbol{\tilde{X}})$. Then, the objective function defined in (\ref{eq:pseudo_overallobj}) can simply be adopted by substituting the real, yet unavailable, observation-label pair $(\boldsymbol{X}, \boldsymbol{t})$ with a pseudo one $(\boldsymbol{\tilde{X}}, \boldsymbol{\tilde{t}})$ such that 
\begin{equation}
\label{eq:pseudo_updateddobj}
\mathcal{L}\big(f\big(\boldsymbol{\tilde{X}}), \boldsymbol{\tilde{t}}\big) +
\mathcal{U}\big(g\big(\boldsymbol{\tilde{X}}\big)\big) =
\mathcal{L}\big(\boldsymbol{Y}, \boldsymbol{\tilde{t}}\big) + c_{\alpha}\alpha\big(\boldsymbol{B}\big) + c_{\beta}\big(1-\beta\big(\boldsymbol{B}\big)\big) + c_F||\boldsymbol{B}||^2_F
\end{equation}

\subsection{Modified \textit{Affinity} and \textit{Balance} Terms} 

Recall that an $n \times n$ symmetric matrix $\boldsymbol{N}=\boldsymbol{B}^T\boldsymbol{B}$ specifies the edges of the graph between the softmax duplicates and that GAR terms have been proposed to regularize the matrix $\boldsymbol{N}$ in a way that it turns into the identity matrix. While the objective of \textit{affinity}, i.e. penalizing the non-zero off-diagonal entries of $\boldsymbol{N}$, corresponds to assigning an example to only one softmax node with the probability of 1, the objective of \textit{balance}, i.e. equalizing diagonal entries of $\boldsymbol{N}$, corresponds to preventing collapsing onto a subspace of dimension less than $n$. 

Among the off-diagonal entries of $\boldsymbol{N}$ determining the \textit{affinity} cost, for each one of $n$ softmax nodes, there exist $k_s-1$ entries describing its relation with the other duplicates of the same parent-class (let us define them as \textit{intra-parent} entries) and $(n_p-1)k_s$ entries describing its relation with the softmax nodes belonging to other parent-classes (let us define them as \textit{inter-parent} entries). While \textit{inter-parent} entries are explicitly affected by the pseudo classification objective as well as the regularization, \textit{intra-parent} entries do not experience the classification directly. Therefore, the \textit{affinity} cost due to \textit{inter-parent} entries is minimized at a different rate than the \textit{affinity} cost due to \textit{intra-parent} entries. On the other hand, as it is calculated over the diagonal entries of $\boldsymbol{N}$, the \textit{balance} cost does not either experience the pseudo classification objective explicitly. As a result, due to the direct impact of the pseudo classification objective which is observed only on the \textit{affinity} cost, the weighting between the regularization terms actively alters during the training and needs to be re-tuned through the hyperparameters $c_\alpha$ and $c_\beta$. This effect can be observed more clearly as $n_p$, the number of parent-classes, increases.

To ensure a more robust regularization we introduce a modification for the \textit{affinity} and \textit{balance} terms: We discard all \textit{inter-parent} entries of $\boldsymbol{N}$ and represent the remaining ones as a three dimensional tensor $\boldsymbol{\tilde N}$. Thus, $\boldsymbol{\tilde N}$ is a $k_s \times k_s \times n_p$ tensor such that $\boldsymbol{\tilde N}_{:,:,k}$ specifies the relations between $k_s$ softmax duplicates of the $k$\textsuperscript{th} parent-class where $k \in \{1,...,n_p\}$. Also, $\boldsymbol{\tilde V}$  is another $k_s \times k_s \times n_p$ tensor defined as 
\begin{equation}
\boldsymbol{\tilde V}_{:,:,k}=[\tilde N_{1,1,k}\dots \tilde N_{k_s,k_s,k}]^T[\tilde N_{1,1,k}\dots \tilde N_{k_s,k_s,k}]
\end{equation}

Then, the modified \textit{affinity} and \textit{balance }terms can be respectively written as 

\begin{minipage}{0.5\linewidth}
	\begin{equation}
	\label{eq:pseudo_modified_affinity}
	\tilde\alpha\big(\boldsymbol{B}\big) := \frac{1}{n_p}\sum\limits_{k=1}^{n_p}\frac{\sum\limits_{i \ne j}^{k_s}{\tilde N_{ijk}}}{(k_s-1)\sum\limits_{i = j}^{k_s}{\tilde N_{ijk}}}
	\end{equation}
\end{minipage}\begin{minipage}{0.5\linewidth}
	\begin{equation}
	\label{eq:pseudo_modified_balance}
	\tilde\beta\big(\boldsymbol{B}\big) := \frac{1}{n_p}\sum\limits_{k=1}^{n_p}\frac{\sum\limits_{i \ne j}^{k_s}{\tilde V_{ijk}}}{(k_s-1)\sum\limits_{i = j}^{k_s}{\tilde V_{ijk}}}
	\end{equation}
\end{minipage}

and simply correspond to calculating the original terms given in (\ref{eq:pseudo_affinity}), (\ref{eq:pseudo_balance}) on each 2-D $k_s \times k_s \times 1$ slice of $\boldsymbol{\tilde N}$ and $\boldsymbol{\tilde V}$ tensors and then averaging the results for $n_p$ of them.

Replacing these modified terms in (\ref{eq:pseudo_updateddobj}), the overall modified objective function becomes 
\begin{equation}
\label{eq:pseudo_updateddobj_modified}
\mathcal{L}\big(f\big(\boldsymbol{\tilde{X}}), \boldsymbol{\tilde{t}}\big) +
\mathcal{U}\big(g\big(\boldsymbol{\tilde{X}}\big)\big) =
\mathcal{L}\big(\boldsymbol{Y}, \boldsymbol{\tilde{t}}\big) + c_{\alpha}\tilde \alpha\big(\boldsymbol{B}\big) + c_{\beta}\big(1-\tilde \beta\big(\boldsymbol{B}\big)\big) + c_F||\boldsymbol{B}||^2_F
\end{equation}

\subsection{Training and Cluster Assignments}
Network parameters are trained by implementing the stochastic optimization method Adam \citep{KingmaB14} based on the objective given in (\ref{eq:pseudo_updateddobj_modified}). After training, $k$-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer such that
\begin{equation}
\boldsymbol{F} = \boldsymbol{Y}^{(L-2)} = f^{(L-2)}(\boldsymbol{X})
\end{equation}
Recalling that the original examples are already introduced to the network as the examples of first pseudo parent-class through transformation $\mathcal{T}_1$, we obtain the latent space representation only for the original examples to perform $k$-means clustering. 

One might suggest performing $k$-means clustering on the representation observed in the augmented softmax layer ($\boldsymbol{Z}$ or $\softmax(\boldsymbol{Z})$) rather than $\boldsymbol{F}$. Properties and respective clustering performances of these representation spaces are empirically demonstrated in the following sections.  


Algorithm \ref{algo:pseudo_training} below describes the entire training and cluster assignment procedure.  

\begin{algorithm}[t]
	\label{algo:pseudo_training}
	\begin{small}
		\DontPrintSemicolon
		\SetKwFunction{proc}{proc}
		\SetKwInOut{Input}{Input}
		\SetKwInOut{return}{return}
		\SetKwFunction{random}{random}
		\SetKwFunction{kmeans}{kmeans}
		
		\Input{$\boldsymbol{X}=[\boldsymbol{x}_1 \dots \boldsymbol{x}_{m}]^T$, $n_p$, \\ 
			a set of transformations $\mathcal{S}_\mathcal{T}=\{\mathcal{T}_1,...,\mathcal{T}_{n_p}\}$, \\
			batch size $b$, weighing coefficients $c_{\alpha}, c_{\beta}, c_F$, the number of clusters $k$}
		\Repeat{\textnormal{stopping criteria is met}}
		{
			
			$\boldsymbol{\tilde{t}} \longleftarrow \random(n_p)$
			\tcp*{Randomly assign labels across $n_p$ classes}
			$\boldsymbol{T} \longleftarrow  [\mathcal{T}_{\tilde{t}_1},...,\mathcal{T}_{\tilde{t}_m}]$
			\tcp*{Obtain the vector of transformations corresponding to $\boldsymbol{\tilde{t}}$}
			$\boldsymbol{\tilde{X}} \longleftarrow \boldsymbol{T} \odot \boldsymbol{X}$
			\tcp*{Obtain the modified input}
			$ \big\{ (\boldsymbol{\acute{X}}_1, \boldsymbol{\acute{t}}_1),..., (\boldsymbol{\acute{X}}_{\nicefrac{m}{b}}, \boldsymbol{\acute{t}}_{\nicefrac{m}{b}})\big\} \longleftarrow (\boldsymbol{\tilde{X}},\boldsymbol{\tilde{t}})$
			\tcp*{Shuffle and create batch pairs}
			\For{$i \gets 1$ \textbf{to} $\nicefrac{m}{b}$}{
				Take $i$\textsuperscript{th} pair $(\boldsymbol{\acute{X}}_i, \boldsymbol{\acute{t}}_i)$ \\
				Forward propagate for $\boldsymbol{\acute{Y}}_i=f(\boldsymbol{\acute{X}}_i)$ and  
				$\boldsymbol{\acute{B}}_i=g(\boldsymbol{\acute{X}}_i)$ \\
				Take a gradient step for $\mathcal{L}\big(\boldsymbol{\acute{Y}}_i,\boldsymbol{\acute{t}}_i\big) + c_{\alpha}\tilde\alpha\big(\boldsymbol{\acute{B}}_i\big) + c_{\beta}\big(1-\tilde\beta\big(\boldsymbol{\acute{B}}_i\big)\big) + c_F||\boldsymbol{\acute{B}}_i||^2_F$
			}
		}
		$\boldsymbol{F} \longleftarrow f^{(L-2)}(\boldsymbol{X})$
		\tcp*{Obtain latent space representation $\boldsymbol{F}$ for the original examples}
		$\boldsymbol{y} \longleftarrow \kmeans(\boldsymbol{F},k)$
		\tcp*{Assign clusters by performing $k$-means on $\boldsymbol{F}$}
		\return{Cluster assignments $\boldsymbol{y}$}
		\caption{Model training and cluster assignments}
		
	\end{small}
\end{algorithm}
 
\section{Experiments}
\subsection{Experimental Setup and Datasets}
The models have been implemented in Python using Keras \citep{chollet2015keras} and Theano \citep{Theano}. Open source code is available at \hyperref{http://github.com/ozcell/LALNets}{}{}{http://github.com/ozcell/LALNets} that can be used to reproduce the experimental results obtained on three benchmark image datasets, MNIST \citep{lecun1998mnist}, SVHN \citep{svhn} and USPS. Specifications of these datasets are presented in Table \ref{tab:pseudo_datasets}.

\begin{table}[h]\centering
\ra{1.2}
\caption{Datasets used in the experiments.}
\resizebox{\columnwidth}{!} {
\begin{tabular}{@{}lllll@{}}\toprule
		& Data type 				& Number of examples 			& Dimension 				& Number of classes \\
\midrule
MNIST 	&Image: Hand-written digits & Train: 60000, Test: 10000		& $1\times28\times28$		& 10				\\
USPS 	&Image: Hand-written digits & Train: 7291, Test: 2007		& $1\times16\times16$		& 10				\\
SVHN	&Image: Street-view digits 	& Train: 73257, Test: 26032 	& $3\times32\times32$		& 10				\\
\bottomrule
\label{tab:pseudo_datasets}
\end{tabular}
}
\vskip -0.2 in
\end{table}

All experiments have been performed on a 6-layer convolutional neural network (CNN) model whose specifications are given in Table \ref{tab:pseudo_models} where coefficients of GAR terms have been chosen as $k_s=20, c_\alpha=0.1, c_\beta=1$, $c_F=0.000001$. During training, pseudo supervised objective is introduced as an 8 pseudo parent-class classification problem, i.e. $n_p=8$, through the following rotation-based transformations:
\begin{equation}
\label{eq:pseudo_transformations}
\mathcal{T}_i =
\begin{cases}
i=1: & \text{No transformation} \\ 
i=2: & \text{Rotate by $90^o$} \\
i=3: & \text{Rotate by $180^o$} \\
i=4: & \text{Rotate by $270^o$} \\ 
i=5: & \text{Flip horizontally} \\
i=6: & \text{Flip horizontally + Rotate by $90^o$} \\
i=7: & \text{Flip horizontally + Rotate by $180^o$} \\
i=8: & \text{Flip horizontally + Rotate by $270^o$} \\
\end{cases}
\end{equation}
For all experiments, we used a batch size of 400 and each experiment has been repeated 10 times. To ensure that the representation obtained through the proposed approach is well-generalized for never-seen-before data, we train the neural network parameters using only the training set examples of each dataset and obtain the clustering performances using $k$-means with $k=10$ on the latent space representation $\boldsymbol{F}$ of the untransformed test set examples (through $\mathcal{T}_1$).
\begin{table}[h]\centering
\ra{1.2}
\caption{Specifications of the CNN model used in the experiments.}
\resizebox{\columnwidth}{!} {
\begin{tabular}{@{}ll@{}}\toprule
Model name & Specification\\
\midrule
6-layer CNN & 2*(32x3x3) - MP2x2 - Drop(0.2) - 2*(64x3x3) - MP2x2 - Drop(0.3) - FC 2048 - Drop(0.5) - FC 8*20 \\	
\bottomrule
\label{tab:pseudo_models}
\end{tabular}
}
\vskip -0.2 in
\end{table}

\subsection{Quantitative Comparison}
Following \citet{JiangZTTZ17} and \citet{YangPB16}, we evaluate the test performances using unsupervised clustering accuracy given as
\begin{equation}
\label{ACC}
ACC = \max_{\mathfrak{f}\in \mathfrak{F}}\frac{\sum_{i = 1}^{m}{1\{t^*_i = \mathfrak{f}(y_i)\}}}{m}
\end{equation}

where $t^*_i$ is the ground-truth label, $y_i$ is the assigned cluster, and $\mathfrak{F}$ is the set of all possible one-to-one mappings between assignments and labels. Both metrics range between $[0, 1]$ where a larger value indicates more precise clustering results.

Figure \ref{fig:pseudo_img_tsne} presents the t-SNE \citep{maaten2008tsne} visualizations of the latent space $\boldsymbol{F}$ throughout the training for 2000 untransformed test examples from MNIST. Each group corresponds to a cluster (i.e. a digit) under the first pseudo parent-class (i.e. the class of untransformed examples including all ten digits). Color codes denote the ground-truths for the digits. From epoch 1 to epoch 400 of the unsupervised (but pseudo supervised) training, clusters become well-separated and simultaneously the clustering accuracy increases. As clearly observed from this figure, using the pseudo supervision, the neural network also reveals some hidden patterns useful to distinguish the real digit identities and ultimately learns to categorize each one of them. It is also worth noting that a high level of clustering accuracy is achieved relatively quickly (after only 50 epochs) as seen both in the t-SNE and test accuracy plots.
\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth,trim={0.2cm 0.2cm 0.2cm 0.2cm},clip]{figures/mnist_tsne}}
		\caption{t-SNE visualization of the latent space $\boldsymbol{F}$ throughout the training for 2000 untransformed test examples from MNIST. Color codes denote the ground-truths for the digits. Note the separation of clusters from epoch 1 to epoch 400 of the unsupervised (but pseudo supervised) training. For reference, clustering accuracy for the entire test set is also provided. This figure is best viewed in color.}
		\label{fig:pseudo_img_tsne}
	\end{center}
	\vskip -0.2 in
\end{figure}

Table \ref{tab:pseudo_all_acc} summarizes quantitative unsupervised clustering performances observed on three datasets in terms of unsupervised clustering accuracy (ACC). Results of a broad range of recent existing solutions are also presented for comparison. These solutions are grouped according to their approaches to unsupervised clustering. Following the very recent developments in deep generative models, VaDE \citep{JiangZTTZ17} and GMVAE \citep{DilokthanakulMG16} employ variational autoencoders while CatGAN \citep{Springenberg15}, AAE \citep{MakhzaniSJG15} and IMSAT \citep{HuMTMS17} adopt adversarial training. DEC \citep{XieGF16} simultaneously learns feature representations and cluster assignments using DNNs. On the other hand, JULE \citep{YangPB16} combines agglomerative clustering with CNNs. Also, the performances of two conventional approaches, applying $k$-means on raw data space and applying $k$-means on the autoencoder representation, are provided to show a baseline for unsupervised clustering performances. Our approach statistically significantly outperforms all the contemporary methods that reported unsupervised clustering performance on MNIST except IMSAT \citep{HuMTMS17} displaying very competitive performance with our approach, i.e. $98.32\%(\pm 0.08)$ vs. $98.40\%(\pm 0.40)$. However, results obtained on the SVHN dataset, i.e. $76.80\%(\pm 1.30)$ vs. $57.30\%(\pm 3.90)$, show that our approach statistically significantly outperforms IMSAT on this realistic dataset and defines the current state-of-the-art for unsupervised clustering on SVHN. Besides, the USPS dataset provides another basis of comparison between our approach and JULE. 
\begin{table}[h]\centering
\ra{1.2}
\caption{Quantitative unsupervised clustering performance (ACC) on MNIST, USPS and SVHN datasets. Results of a broad range of recent existing solutions are also presented for comparison. The last row demonstrates the benchmark scores of the proposed framework in this article.}
\vskip 0.05in
\begin{tabular}{@{}lrrrr@{}}\toprule
							  	&$k$ & MNIST-\textit{test}  	& USPS-\textit{full}$^\dagger$	& SVHN-\textit{test}	\\
\midrule
VaDE \citep{JiangZTTZ17}		  	&$10$& $94.06\%\hskip 3.35em$ 	& -						& -	\\
GMVAE \citep{DilokthanakulMG16}	&$10$& $82.31\%(\pm 3.75)$		& -						& -	\\
GMVAE \citep{DilokthanakulMG16}	&$16$& $87.82\%(\pm 5.33)$		& -						& -	\\
GMVAE \citep{DilokthanakulMG16}	&$30$& $92.77\%(\pm 1.60)$		& -						& -	\\		
\midrule
CatGAN \citep{Springenberg15}  	&$20$& $90.30\%\hskip 3.35em$ 	& -						& - \\
AAE \citep{MakhzaniSJG15}      	&$16$& $90.45\%(\pm 2.05)$		& -						& - \\
AAE \citep{MakhzaniSJG15}      	&$30$& $95.90\%(\pm 1.13)$		& -						& - \\
IMSAT \citep{HuMTMS17}      	&$10$& $98.40\%(\pm 0.40)$		& -						& $57.30\%(\pm 3.90)$ \\
\midrule
$k$-means \citep{XieGF16}	    &$10$& $53.49\%\hskip 3.35em$	& -						& - \\
AE+$k$-means \citep{XieGF16}	&$10$& $81.84\%\hskip 3.35em$	& -						& - \\
\midrule
DEC \citep{XieGF16}	          	&$10$& $84.30\%\hskip 3.35em$	& -						& $11.9\%(\pm0.40)^{\dagger\dagger}$ \\
\midrule
JULE \citep{YangPB16}	      	&$10$& $96.10\%\hskip 3.35em$	& $95.00\%\hskip 3.35em$& -	\\
\midrule
\textbf{Our approach} 	      	&$10$& $98.32\%(\pm 0.08)$		& $96.51\%(\pm 0.26)$	& $76.80\%(\pm 1.30)$	\\
\bottomrule
\label{tab:pseudo_all_acc}
\end{tabular}
\vskip -0.2 in	
\raggedright \tiny $ ^\dagger$ Only for USPS dataset, following JULE \citep{YangPB16}, we reported unsupervised clustering performance over the full dataset for a fair comparison.\\
\raggedright \tiny $ ^{\dagger\dagger}$ Excerpted from \citep{HuMTMS17}.
\end{table}


\subsection{Representation Properties}

Recall that, for the 6-layer CNN model employed in the experiments, $\boldsymbol{F}=\boldsymbol{Y}^{(L-2)}$ corresponds to the output of the fully-connected layer of 2048 ReLU nodes, $\boldsymbol{Z} = \boldsymbol{F}\boldsymbol{W}^{L-1} + \boldsymbol{b}^{L-1}$ is the input of the augmented softmax layer of 160 nodes, i.e. $n=n_pk_s$, where 8 pseudo parent-classes are represented by 20 softmax duplicates each. 

Figure \ref{fig:pseudo_latent_rep} provides the average value for each dimension of $\boldsymbol{F}$, $\boldsymbol{Z}$ and $\softmax(\boldsymbol{Z})$ observed with respect to untransformed test set examples and the norm of the associated weights. Note that the representation on $\boldsymbol{F}$ is not distributed to the entire space but the weights associated to these unused dimensions do not decay. On the other hand, due to the pseudo supervision task, the output of the augmented softmax layer i.e. $\softmax(\boldsymbol{Z})$, becomes a one-hot encoded representation of which 140 dimensions, i.e. $(n_p-1)k_s$, are inactive for the untransformed examples; however, the representation at its input is distributed to all dimensions. Figure \ref{fig:pseudo_latent_rep} also summarizes how the dimension size of $\boldsymbol{F}$, i.e. the number of ReLU nodes in the fully-connected layer, affects the clustering performance. Decreasing the number of dimensions of $\boldsymbol{F}$ up to a point, i.e. $\approx 1024$, does not significantly affect the clustering accuracy. However, further decrease beyond this point dramatically reduces the performance. 
\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth,trim={0.2cm 0.2cm 0.2cm 0.2cm},clip]{figures/latent_rep}}
		\caption{The average value for each dimension of $\boldsymbol{F}$, $\boldsymbol{Z}$ and $\softmax(\boldsymbol{Z})$ observed with respect to untransformed test set examples and the norm of the associated weights. Note that the representation on $\boldsymbol{F}$ is not distributed to the entire space but the weights associated to these unused dimensions do not decay. On the other hand, due to the pseudo supervision task, the output of the augmented softmax layer i.e. $\softmax(\boldsymbol{Z})$, becomes a one-hot encoded representation of which 140 dimensions are inactive for the untransformed examples; however, the representation at its input is distributed to all dimensions. The last plot shows how the dimension size of $\boldsymbol{F}$ affects the clustering performance. This figure is best viewed in color.}
		\label{fig:pseudo_latent_rep}
	\end{center}
	\vskip -0.2 in
\end{figure}

For comparison, Figure \ref{fig:pseudo_img_tsne_comparison} presents t-SNE visualizations of these latent representations observed with respect to 2000 untransformed test examples from MNIST. One can clearly see that clusters are not well-separated on one-hot encoded $\softmax(\boldsymbol{Z})$; however, separations of the clusters are quite similar and clear on the representation spaces $\boldsymbol{F}$ and $\boldsymbol{Z}$. Hence, one can also obtain similar clustering accuracy, i.e. $=98.16\%\pm(0.14)$, by applying $k$-means on the representation space $\boldsymbol{Z}$.
\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth,trim={0.2cm 0.2cm 0.2cm 0.2cm},clip]{figures/mnist_tsne_comparison}}
		\caption{Comparison of t-SNE visualizations of the latent spaces $\boldsymbol{F}$, $\boldsymbol{Z}$ and $\softmax(\boldsymbol{Z})$ for 2000 test examples from MNIST. Color codes denote the ground-truths for the digits and each label represents the major digit of a cluster. Clusters are not well-separated on one-hot encoded $\softmax(\boldsymbol{Z})$; however, separations of the clusters are quite similar and clear on the representation spaces $\boldsymbol{F}$ and $\boldsymbol{Z}$. This figure is best viewed in color.}
		\label{fig:pseudo_img_tsne_comparison}
	\end{center}
	\vskip -0.2 in
\end{figure}


\subsection{Graph Interpretation of the Latent Information Propagation through GAR}

Recall that GAR terms have been originally proposed to propagate the available labels towards the unlabeled examples in a semi-supervised setting and \citet{KilincU17ACOL} have shown that these terms can also be adopted to propagate the hidden information that is introduced by a coarse level of supervision and which is useful to discover a deeper level of latent annotations. In the fully unsupervised setting considered in this paper, as no real supervision is available, hidden information useful to discover unknown clusters is now captured through the help of domain specific transformations and propagated by GAR terms as well. 

Figure \ref{fig:pseudo_img_graph} visualizes the realization of this propagation using the real predictions obtained on MNIST. Colored circles denote the ground-truths for the vertices, i.e. examples, and gray lines denote the edges, i.e. non-zero weighted connections between the examples representing their similarity. Note that, for vertices in graph $\mathcal{G}_\mathcal{Y}$, there are two different colors indicating true pseudo parent-class labels assigned per the applied transformation (for simplicity, out of 8, only the examples of the first two pseudo parent-classes are used for this illustration), albeit ten different colors indicating the real digit identity for vertices in graph $\mathcal{G}_\mathcal{M}$. Recall that edges of these two graphs, $\mathcal{E}_\mathcal{Y}$ and $\mathcal{E}$, are respectively inferred by matrices $\boldsymbol{Y}\boldsymbol{Y}^T$ and $\boldsymbol{B}\boldsymbol{B}^T$ where $\boldsymbol{B}=\max(0,\boldsymbol{Z})$ and that $\mathcal{G}_\mathcal{M}$ is the spanning subgraph of $\mathcal{G}_\mathcal{Y}$. That is, $\mathcal{G}_\mathcal{M}=(\mathcal{M},\mathcal{E})$ shares the same vertices $\mathcal{M}$ with graph $\mathcal{G}_\mathcal{Y}=(\mathcal{M},\mathcal{E}_\mathcal{Y})$, which is constructed per the pseudo supervision; however, $\mathcal{E}$ is a subset of $\mathcal{E}_\mathcal{Y}$ as some of the edges in graph $\mathcal{G}_\mathcal{Y}$, such as those between the examples of digit 0 and 1, are eliminated in graph $\mathcal{G}_\mathcal{M}$ due to GAR regularization terms. As training continues, pseudo supervision eliminates the edges between the examples of different pseudo parent-classes and turns graph $\mathcal{G}_\mathcal{Y}$ into a disconnected graph of $n_p=8$ disjoint subgraphs (only two of them are illustrated). Simultaneously, GAR terms eliminate the edges between the examples of the same parent-class in graph $\mathcal{G}_\mathcal{M}$ to discover previously unknown clusters. Ultimately, $\mathcal{G}_\mathcal{M}$ becomes disconnected graphs of $\delta$ disjoint subgraphs where $n_p\le \delta \le n_pk_s$ and each disjoint subgraph corresponds to a cluster. 
\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=120mm,trim={0.2cm 2.5cm 0.2cm 0.8cm},clip]{figures/mnist_graph_2}}
		\caption{Visualizations of the graph $\mathcal{G}_\mathcal{Y}$ and its spanning subgraph $\mathcal{G}_\mathcal{M}$ for randomly chosen 500 test examples from MNIST (this selection is performed only for the simplicity of the visualization). Colored circles denote the ground-truths for the vertices, i.e. examples, and gray lines denote the edges, i.e. non-zero weighted connections between the examples representing their similarity. Note that, for vertices in graph $\mathcal{G}_\mathcal{Y}$, there are two different colors indicating true pseudo parent-class labels assigned according to the applied transformation (for simplicity, out of 8, only the examples of first two pseudo parent-classes are used for this illustration), albeit ten different colors indicating the real digit identity for vertices in graph $\mathcal{G}_\mathcal{M}$. As training continues, pseudo supervision eliminates the edges between the examples of different pseudo parent-classes and turns graph $\mathcal{G}_\mathcal{Y}$ into a disconnected graph of $n_p=8$ disjoint subgraphs (only two of them are illustrated). Simultaneously, GAR terms eliminate the edges between the examples of the same parent-class in graph $\mathcal{G}_\mathcal{M}$ to discover previously unknown clusters. Ultimately, $\mathcal{G}_\mathcal{M}$ becomes disconnected graphs of $\delta$ disjoint subgraphs where $n_p\le \delta \le n_pk_s$ and each disjoint subgraph corresponds to a cluster. This figure is best viewed in color.}
	\label{fig:pseudo_img_graph}
	\end{center}
	\vskip -0.2 in
\end{figure}

\subsection{The Impact of the Number of Clusters $k$}

For the quantitative clustering results, we set the number of clusters for the $k$-means to the number of classes assuming a prior knowledge, i.e. $k=10$. To demonstrate the representation power of the proposed approach as an unsupervised clustering model, on MNIST, we deliberately choose different $k$ values for the $k$-means clustering applied on the representation space $\boldsymbol{F}$. For two different $k$ settings i.e. 7 and 20, Figure \ref{fig:pseudo_impact_k} illustrates a few examples of each cluster. One can see that when $k$ is smaller than the actual number of classes, digits with similar appearances are grouped together, such as digits 4 and 9, 5 and 8, 0 and 6. When $k$ is set to a bigger value than the number of classes, some digits are divided into subclasses based on visually identifiable image properties such as digit tilt, roundness, etc. Note the differences between upright and oblique digit 1 as shown in clusters 2 and 20, between two styles of digit 6 as shown in clusters 18 and 19, and between two styles of digit 2 as shown in clusters 7 and 12.
\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=120mm,trim={0.2cm 0.2cm 0.2cm 0.2cm},clip]{figures/impact_of_k_2}}
		\caption{Illustration of a few examples of each cluster for two different $k$ settings i.e. 7 and 20. When $k$ is smaller than the actual number of classes, digits with similar appearances are grouped together, such as digits 4 and 9, 5 and 8, 0 and 6. When $k$ is set to a bigger value than the number of classes, some digits are divided into subclasses based on visually identifiable image properties such as digit tilt, roundness, etc. Note the differences between upright and oblique digit 1 as shown in clusters 2 and 20, between two styles of digit 6 as shown in clusters 18 and 19, and between two styles of digit 2 as shown in clusters 7 and 12.}
		\label{fig:pseudo_impact_k}
	\end{center}
	\vskip -0.2 in
\end{figure}

\subsection{The Impact of Transformations}

As the revealed unknown clusters are directly related with the captured latent information through pseudo parent-classes, choosing the right set of transformations for the clustering task of concern is crucial for the performance. Figure \ref{fig:pseudo_impact_trans} presents t-SNE visualizations of the representation spaces observed when different sets of transformations are adopted.

The first row of Figure \ref{fig:pseudo_impact_trans} illustrates the clustering results when one of four different transformation types, i.e. scaling, shearing, translation and random permutation of the pixels, is applied variably to generate 8 pseudo parent-classes. One can observe some level of grouping with scaling and shearing-based transformations; however, the clusters defined by these groupings do not represent real digit identities (as shown by the colored dots) and may indicate other features of images. On the other hand, translating the images or randomly permuting the pixel positions do not provide any useful knowledge to discover any well-defined clustering. 


The second row of Figure \ref{fig:pseudo_impact_trans} presents the results obtained when rotation-based transformations listed in (\ref{eq:pseudo_transformations}) are adopted. One can easily observe that only two or four pseudo parent-classes generated using rotation-based transformations are sufficient to obtain decent clustering representing the real digit identities. Considering that, for MNIST, the clustering accuracy obtained using all 8 transformations in (\ref{eq:pseudo_transformations}) is $98.32\%(\pm 0.08)$, we have achieved $97.80\%(\pm 0.18)$ accuracy using $\mathcal{S}_\mathcal{T}=\{\mathcal{T}_1,\mathcal{T}_2,\mathcal{T}_3,\mathcal{T}_4\}$, $72.52\%(\pm 6.20)$ accuracy using $\mathcal{S}_\mathcal{T}=\{\mathcal{T}_1,\mathcal{T}_2\}$ and $96.84\%(\pm 0.29)$ accuracy using $\mathcal{S}_\mathcal{T}=\{\mathcal{T}_1,\mathcal{T}_3\}$. Recalling that $\mathcal{T}_2$ and $\mathcal{T}_3$ respectively correspond to rotating the images by 90\textsuperscript{o} and 180\textsuperscript{o}, one can say that comparing the untransformed images with their 180\textsuperscript{o} rotated versions is more effective in terms of capturing the latent information that is useful to distinguish the real digit identities. In fact, $\mathcal{T}_3$ alone is sufficient to achieve state-of-the-art clustering accuracy on MNIST. Adding more rotation-based transformations to $\mathcal{S}_\mathcal{T}$ further improves the clustering performance. To summarize, the type of the transformation generating the pseudo parent-classes is more important than their number and different transformations can reveal different clustering patterns. Therefore, finding the right transformation type for the clustering task of concern is crucial for the proposed approach in this paper and it remains an important research question how to identify the kind of transformation most optimized for the clustering task at hand. 
\begin{figure}[h]
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth,trim={0.2cm 0.2cm 0.2cm 0.2cm},clip]{figures/mnist_tsne_transformations}}
		\caption{t-SNE visualizations of the representation spaces observed when different sets of transformations are adopted. The first row illustrates the clustering results when one of four different transformation types, i.e. scaling, shearing, translation and random permutation of the pixels, is applied variably to generate 8 pseudo parent-classes. The second row presents the results obtained when rotation-based transformations listed in (\ref{eq:pseudo_transformations}) are adopted. To summarize, the type of the transformation generating the pseudo parent-classes is more important than their number and different transformations can reveal different clustering patterns. Therefore, finding the right transformation type for the clustering task of concern is crucial for the proposed approach in this paper.}
		\label{fig:pseudo_impact_trans}
	\end{center}
	\vskip -0.2 in
\end{figure}

\section{Conclusion}

In this paper, we introduced a novel unsupervised clustering approach building upon the previous study on an output layer modification, ACOL, which is proposed to learn latent annotations on neural networks when a partial supervision is provided. To discover unknown clusters in a fully unsupervised setup, we substitute the real, yet unavailable, partial supervision with a pseudo one. More specifically, we randomly assign pseudo parent-class labels each of which is associated with a different domain specific transformation. Each observation is modified by applying the transformation corresponding to the assigned pseudo label. Generated observation-label pairs are used to train an ACOL network that introduces multiple softmax nodes for each pseudo parent-class. Due to the unsupervised regularization based on GAR terms, each softmax duplicate under a parent-class is specialized as the latent information captured by the help of domain specific transformations is propagated throughout the training. Ultimately we obtain a $k$-means friendly latent representation. Furthermore, we demonstrate that the neural network can learn by comparing differently transformed examples and translate that knowledge to reveal unknown clusters. The proposed approach was validated on three image benchmark datasets, MNIST, SVHN and USPS, through t-SNE visualizations and unsupervised clustering accuracy exceeds those reported by well-accepted approaches in the literature. Future work will extend this approach to other domains such as sequential data. We will also explore how to optimize domain specific transformations based on known or otherwise identifiable characteristics of the dataset being considered for clustering.

\bibliography{bibliography}
\bibliographystyle{iclr2018_conference}

\end{document}