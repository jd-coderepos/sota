\documentclass[a4paper]{article}


\usepackage[utf8]{inputenc}
\usepackage[francais,american]{babel}
\usepackage{enumerate}
\usepackage{amsmath,amstext,amsfonts,amssymb,amsthm,xfrac,paralist}
\usepackage{url}
\usepackage{xspace}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array}
\usepackage{verbatim}
\usepackage{times}
\usepackage[ruled,vlined,boxed,commentsnumbered]{algorithm2e}



 \usepackage{tikz}

\usepackage{fancybox}
\usepackage{fp}

\usepackage{graphicx}

\tikzstyle{fancytitle} = [fill=blue!40, text=black, rounded corners,inner sep=4pt] 
\tikzstyle{mybox} = [draw=blue!40, fill=blue!20, very thick, rectangle, rounded corners, inner ysep=10pt, drop shadow]
\tikzstyle{fancytitleR} = [fill=bluegreen, text=black, rounded corners,inner sep=4pt] 
\tikzstyle{myboxTitle} = [draw=bluetheme, fill=bluetheme, very thick, rectangle, rounded corners, inner ysep=10pt, drop shadow]
\tikzstyle{myboxR} = [draw=bluegreen, fill=bluegreen!20, very thick, rectangle, rounded corners, inner ysep=10pt]
 \usepackage{calc}

\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{backgrounds}

\newcommand{\pr}[1]{\ensuremath{p_{#1}}}

\newcommand{\inSched}{
\setcounter{nbProcs}{0}
\FPadd{\cury}{\procspace}{1}
}

\newcommand{\addProc}{
\addtocounter{nbProcs}{1}
\FPsub{\cury}{\cury}{\procspace}
\FPsub{\cury}{\cury}{1}
\draw[draw=none] (-1.5,\cury) rectangle (-0.5,\cury-1) node[midway] {\pr{\thenbProcs}};
\FPeval{\curx}{0}
}

\newcommand{\addProcChain}{
\addtocounter{nbProcs}{1}
\FPsub{\cury}{\cury}{\procspace}
\FPsub{\cury}{\cury}{1}
\FPsub{\cury}{\cury}{\procspace}

\draw[draw=none] (-1.5,\cury) rectangle (-0.5,\cury-1) node[midway] {};
\FPeval{\curx}{0}
}


\newcommand{\addJobSpeed}[4][anchor=center]{ 

\draw[fill=white] (\curx,\cury-1) rectangle () node[#1,midway,very thick] {}; 
\FPdiv{\curz}{#2}{#3}
\FPadd{\curx}{\curx}{\curz}
}

\newcommand{\addLisJob}[4][midway]{ \draw (\curx,\cury-1) rectangle () node[#1] {}; 
\FPsub{\cury}{\cury}{\procspace}
\FPsub{\cury}{\cury}{1}
}

\newcommand{\addLisJobSpeed}[5][]{ \draw (\curx,\cury-1) rectangle () node[#1,midway,very thick] {}; 
\FPsub{\cury}{\cury}{\procspace}
\FPsub{\cury}{\cury}{1}
}

\theoremstyle{plain}
\newtheorem{algo}{Algorithm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}

\theoremstyle{remark}
\newtheorem*{notation}{Notations}
\newtheorem*{beware}{BEWARE}
\newtheorem*{remark}{Remark}


\newcommand{\todo}[1]{{\color{red}\rule[-.1cm]{.4cm}{.4cm}~~{
\color{red}{#1}}}\xspace}
\newcommand{\fw}[1]{{\color{blue}\rule[-.1cm]{.4cm}{.4cm}~~{
\color{blue}{#1}}}\xspace}

\newcommand{\fmax}{\ensuremath{f_{\max}}\xspace}
\newcommand{\fmin}{\ensuremath{f_{\min}}\xspace}
\newcommand{\finf}{\ensuremath{f_{\inf,i}}\xspace}
\newcommand{\fr}{\ensuremath{f_{\texttt{rel}}}\xspace}
\newcommand{\freex}{\ensuremath{f_{\texttt{re-ex}}}\xspace}

\newcommand{\energy}{\textsc{Energy}} \newcommand{\exe}{\ensuremath{\mathcal{E}\!xe}\xspace}
\newcommand{\dff}{\textsc{Dec\-reasing-First-Fit}\xspace}
\newcommand{\platform}{\ensuremath{\mathcal{P}}\xspace} \newcommand{\partition}{2-\textsc{Partition}\xspace}
\newcommand\II{\ensuremath{\mathcal{I}}\xspace}
\newcommand\EE{\ensuremath{\mathcal{E}}\xspace}

\newcommand{\approxchain}{\textsc{Ap\-prox-Chain}}
\newcommand{\xopt}{\textsc{X-Opt}}
\newcommand{\merge}{\textsc{Merge-Lists}}
\newcommand{\add}{\textsc{Add-List}}
\newcommand{\trim}{\textsc{Trim}}
\newcommand{\computeVl}{\textsc{Compute\_}}

\newcommand{\grobt}{\textsc{GRoBT}\xspace}
\newcommand{\approxindep}{\textsc{Approx-Indep}\xspace}

\newcommand{\tricrit}{\textsc{Tri-Crit}\xspace}
\newcommand{\chain}{\textsc{Tri-Crit-Chain}\xspace}
\newcommand{\indep}{\textsc{Tri-Crit-In\-dep}\xspace}

\let\ab\allowbreak



\title{Approximation algorithms for energy, reliability and makespan
  optimization problems}

\author{Guillaume Aupy\\
   LIP, ENS Lyon, France\\
   Guillaume.Aupy@ens-lyon.fr \\
   \and
   Anne Benoit \\
   LIP, ENS Lyon, France \& Institut Universitaire de France, Paris, France \\
   Anne.Benoit@ens-lyon.fr \\
   }

\date{\today}



\begin{document}
\maketitle

\begin{abstract}
  In this paper, we consider the problem of scheduling an application
  on a parallel computational platform. The application is a
  particular task graph, either a linear chain of tasks, or a set of
  independent tasks. The platform is made of identical processors,
  whose speed can be dynamically modified. It is also subject to
  failures: if a processor is slowed down to decrease the energy
  consumption, it has a higher chance to fail. Therefore, the
  scheduling problem requires to re-execute or replicate tasks (i.e.,
  execute twice a same task, either on the same processor, or on two
  distinct processors), in order to increase the reliability. It is a
  tri-criteria problem: the goal is to minimize the energy
  consumption, while enforcing a bound on the total execution time
  (the makespan), and a constraint on the reliability of each task. 

  Our main contribution is to propose approximation algorithms for
  these particular classes of task graphs.  For linear chains, we
  design a fully polynomial time approximation scheme. However, we
  show that there exists no constant factor approximation algorithm
  for independent tasks, unless P=NP, and we are able in this case to
  propose an approximation algorithm with a relaxation on the makespan
  constraint.
\end{abstract}




\section{Introduction}
\label{sec.intro}

Energy-awareness is now recognized as a first-class constraint in the
design of new scheduling algorithms. To help reduce energy
dissipation, current processors from AMD, Intel and Transmetta allow
the speed to be set dynamically, using a dynamic voltage and frequency
scaling technique (DVFS). Indeed, a processor running at speed 
dissipates  watts per unit of time \cite{pow3IPDPS}.  However,
it has been recognized that reducing the speed of a processor has a
negative effect on the reliability of a schedule: if a processor is
slowed down, it has a higher chance to be subject to transient
failures, caused for instance by software errors 
\cite{Zhu04EEM,Degal05SEI}. 

Motivated by the application of speed scaling on large scale machines
\cite{Oliner04}, we consider a tri-criteria problem
energy/reliability/makespan: the goal is to minimize the energy
consumption, while enforcing a bound on the makespan, i.e., the total
execution time, and a constraint on the reliability of each task.  The
application is a particular task graph, either a linear chain of
tasks, or a set of independent tasks. The platform is made of
identical processors, whose speed can be dynamically modified. 

In order to make up for the loss in reliability due to the energy
efficiency, we consider two standard techniques: \emph{re-execution}
consists in re-executing a task twice on a same processor
\cite{Zhu04EEM,Zhu06}, while \emph{replication} consists in executing
a same task on two distinct processors simultaneously
\cite{Assayad11}. We do not consider \emph{checkpointing}, which
consists in ``saving'' the work done at some points, hence reducing
the amount of work lost when a failure occurs
\cite{Melhem03CP,Zhang03CP}.
 
The schedule therefore requires to (i) decide which tasks are
re-executed or replicated; (ii) decide on which processor(s) each task
is executed; (iii) decide at which speed each processor is processing
each task. For a given schedule, we can compute the total execution
time, also called {\em makespan}, and it should not exceed a
prescribed deadline. Each task has a reliability that can be computed
given its execution speed and its eventual replication or
re-execution, and we must enforce that the execution of each task is
reliable enough.  Finally, we aim at minimizing the energy
consumption.  Note that we consider a set of homogeneous processors,
but each processor may run at a different speed; this corresponds to
typical current platforms with DVFS. 


\paragraph{Related work.}
The problem of minimizing the energy consumption without exceeding a
given deadline, using DVFS, has been widely studied, without
accounting for reliability issues.  The problem for a linear chain of
tasks is known to be solvable in polynomial time in this case, see
\cite{aupy12ccpe}. \cite{Alon97} showed that the problem of
scheduling independent tasks can be approximated by a factor
: they exhibit a polynomial time approximation scheme
(PTAS). \cite{RenaudGoudGreedy} studied the performance of greedy
algorithms for the problem of scheduling independent tasks, with the
objective of minimizing the energy consumption, and proposed some
approximation algorithms.

All these work do not account for reliability issues. However,
\cite{Zhu04EEM} showed that reducing the speed of a processor
increases the number of transient failure rates of the system; the
probability of failures increases exponentially, and this probability
cannot be neglected in large-scale computing \cite{Oliner04}.  Few
authors have tackled the tri-criteria problem including reliability,
and to the best of our knowledge, there are no approximation
algorithms for this problem. \cite{Zhu06} initiated the study of this
problem, using re-execution. However, they restrict their study to the
scheduling problem on a single processor, and do not try to find any
approximation ratio on their algorithm. \cite{Assayad11} have
recently proposed an off-line tri-criteria scheduling heuristic (TSH),
which uses replication to minimize the makespan, with a
threshold on the global failure rate and the maximum power
consumption.  TSH is an improved critical-path list sche\-duling
heuristic that takes into account power and reliability before
deciding which task to assign and to replicate onto the next free
processors. However, the complexity of this heuristic is unfortunately
exponential in the number of processors, and the authors did not try
to give an approximation ratio on their heuristic.  Finally,
\cite{rr7757} also study the tri-criteria problem, but from an
heuristic point of view, without trying to ensure any approximation
ratio on their heuristics. Moreover, they do not consider replication
of tasks, but only re-execution as in \cite{Zhu06}.  However, they
present a formal model of the tri-criteria problem, re-used in this
paper.

Finally, there is some related work specific to the problem of
independent tasks, since several approximation algorithms have been
proposed for variants of the problem. One may try to minimize the
 norm, i.e., the quantity , with  processors, where  means that
task~ is assigned to processor~, and  is the weight of
task~ \cite{Alon97}.  Minimizing the power consumption
then amounts to minimize the  norm
\cite{RenaudGoudGreedy}, and the problem of makespan minimization is
equivalent to minimizing  
the  norm, i.e., minimize  \cite{Graham69,Ausiello99}.  These problems
are typical {\em load balancing} problems, in which the load
(computation requirement of the tasks) must be balanced between
processors, according to various criteria.




\paragraph{Main contributions.}  
In this paper, we investigate the tri-criteria problem of minimizing
the energy with a bound on the makespan and a constraint on the
reliability.  First in Section~\ref{sec.fw}, we formally introduce
this tri-criteria scheduling problem, based on the previous models
proposed by \cite{Zhu06} and~\cite{rr7757}.  To the best of our
knowledge, this is the first model including both re-execution and
replication in order to deal with failures. The main contribution of
this paper is then to provide approximation algorithms for some
particular instances of this tri-criteria problem.

For linear chains of tasks, we propose a fully polynomial time
approximation scheme (Section~\ref{sec.lin}). Then in
Section~\ref{sec.indep}, we show that there exists no constant factor
approximation algorithm for the tri-criteria problem with independent
tasks, unless P=NP. We prove that by relaxing the constraint on
the makespan, we are able to give a polynomial time constant factor
approximation algorithm.  To the best of our knowledge, these are the
first approximation algorithms for the tri-criteria problem. 


\section{Framework}
    \label{sec.fw}
Consider an application task graph ,
where  is the set of tasks, ,
and where  is the set of precedence edges between tasks. For , task~ has a weight~, that corresponds to the
computation requirement of the task.   is the
sum of the computation requirements of all tasks.

The goal is to map the task graph onto  identical processors, with
the objective of minimizing the total energy consumption, while
enforcing a bound on the total execution time (makespan), and matching
a reliability constraint. Processors can have arbitrary speeds,
determined by their frequency, that can take any value in the interval
 (dynamic voltage and frequency scaling with continuous
speeds). Higher frequencies, and hence faster speeds, allow for a
faster execution, but they also lead to a much higher (supra-linear)
power consumption.  Moreover, reducing the frequency of a processor
increases the number of transient failures of the system. Therefore,
some tasks are executed once at a speed high enough to satisfy the
reliability constraint, while some other tasks are executed several
times (either on the same processor, or on different processors), at a
lower speed. We detail below the conditions that are enforced on the
corresponding execution speeds. The problem is therefore to decide
which tasks should be executed several times, on which processor, and
at which speed to run each execution of a task, as well as the
schedule, i.e., in which order the tasks are executed on each
processor.  Note that \cite{rr7757} showed that it is always better
to execute a task at a single speed, and therefore we assume in the
following that each execution of a task is done at a single speed.

We now detail the three objective criteria (makespan, reliability,
energy), and then define formally the problem.


\subsection{Makespan}

The makespan of a schedule is its total execution time. The first task
is scheduled at time , so that the make\-span of a schedule is simply
the maximum time at which one of the processors finishes its
computations.  Given a schedule, the makespan should not exceed the
prescribed deadline~. 

Let  be the execution time of a task  of
weight~ at speed~. We assume that the cache size is adapted to
the application, therefore ensuring that the execution time is
linearly related to the frequency \cite{Melhem03CP}: . Note that we consider a worst-case scenario, and the
deadline~ must be matched even in the case where all tasks that are
scheduled to be executed several times fail during their first
executions, hence all execution times for a same task should be
accounted for.



\subsection{Reliability}
    \label{sec.rel}
To define the reliability, we use the failure model of
\cite{Zhu04EEM} and \cite{Zhu06}. 
\emph{Transient} failures are failures caused by software errors for example. 
They invalidate only the execution of
the current task and the processor subject to that failure will be
able to recover and execute the subsequent tasks assigned to it (if
any). In addition, we use the reliability model introduced by \cite{Shatz89}, 
which states that the radiation-induced
transient failures follow a Poisson distribution.  The parameter
 of the Poisson distribution is then ,
where  is the processing speed, the exponent
 is a constant, indicating the sensitivity of failure
rates to dynamic voltage and frequency scaling, and
 is the average failure rate at speed~\fmax.  We
see that reducing the speed for energy saving increases the failure
rate exponentially.  The reliability of a task~ executed once at
speed  is   Because the
failure rate~ is usually very small, of the order of
 per time unit \cite{Assayad11}, or even 
\cite{Baleani03,Izo07}, we can use the first order approximation of
 as 
where  and . 

\medskip
Note that this equation holds if \mbox{}.  With, say, , we need
 to get an accurate approximation with
: the task should execute within 
minutes. In other words, large (computationally demanding) tasks
require reasonably high processing speeds with this model (which makes
full sense in practice).

We want the reliability~ of each task  to be greater than
a given threshold, namely , hence enforcing a local
constraint dependent on the task: .   
If task~ is executed only once at speed~, then the
reliability of~ is . Since the reliability increases
with speed, we must have  to match the reliability
constraint.
If task~ is executed twice (speeds~ and~),
then the execution of~ is successful if and only if one of the
attempts do not fail, so that the reliability of~ is , and this quantity should be
at least equal to . 

We restrict in this work to a maximum of two executions of a same task,
either on the same processor (what we call {\em re-execution}), or on
two distinct processors (what we call {\em replication}).  This is
based on the following observation on the two cases in which a third
execution of a task may be useful. 
\begin{compactenum}
\item The deadline is such that even if all tasks are executed
  twice at the slowest possible speed, the execution time is still
  lower than the deadline. Then, the problem is to decide which task
  should be executed three times, and it is quite similar to the
  problem that we discuss in this paper. 
\item Some tasks are too big to be re-executed while there remains
  some time such that some small tasks can be executed at least three
  times at a speed even slower. In this case, the gain in energy
  consumption is negligible compared to the energy consumption of the
  big tasks at speed~\fr.
\end{compactenum}


Note that if both execution speeds are equal, i.e.,
, then the reliability constraint writes
, and
therefore  In the following,  is the solution to
the equation , and hence task~ can be executed twice at
a speed greater than or equal to  while meeting the reliability
constraint. In practice,  is small enough so that tasks are
usually executed faster than this speed, hence reinforcing the
argument that it is meaningful to restrict to two executions of a same
task. 


\subsection{Energy}


The total energy consumption corresponds to the sum of the energy
consumption of each task. Let  be the energy consumed by
task . For one execution of  at speed~, the
corresponding energy consumption is , which corresponds to the dynamic part of the
classical energy models of the literature \cite{pow3IPDPS,BKP07}.  
Note that we do not take static energy into account, because all 
processors are up and alive during the whole execution.

If task  is executed only once at speed~, then .  Otherwise, if task  is executed twice at speeds
 and , it is natural to add up the energy consumed
during both executions, just as we consider both execution times when
enforcing the deadline on the makespan. Again, this corresponds to the
worst-case execution scenario.  We obtain .  Note that some authors \cite{Zhu06} consider only
the energy spent for the first execution in the case of re-execution,
which seems unfair: re-execution comes at a price both in the makespan
and in the energy consumption.  Finally, the total energy consumed by
the schedule, which we aim at minimizing, is .









    \subsection{Optimization problem}
\label{opt_problem}
Given an application graph  and 
identical processors, \tricrit is the problem of finding a schedule
that specifies which tasks should be executed twice, on which
processor and at which speed each execution of a task should be
processed, such that the total energy consumption~ is minimized,
subject to the deadline~ on the makespan and to the local reliability
constraints  for each .


We focus in this paper on the two following sub-problems that are 
restrictions of \tricrit to special application graphs:
\begin{itemize}
\item \chain: the graph is such that 
\\; 

\item \indep: 
  the graph is such that .
\end{itemize}






\section{Linear chains}
\label{sec.lin}

In this section, we focus on the \chain problem, that was shown to be
NP-hard even on a single processor \cite{rr7757}. We derive an FPTAS
(Fully Polynomial Time Approximation Scheme) to sol\-ve the general
problem with replication and re-execution on ~processors. We start
with some preliminaries in Section~\ref{lin.char} that allow us to
characterize the shape of an optimal solution, and then we detail the
FPTAS algorithm and its proof in Section~\ref{lin.fptas}.


\subsection{Characterization}
\label{lin.char}


First, we note that while \chain is NP-hard even on a single
processor, the problem has polynomial complexity if no replication nor
re-execution can be used. Indeed, each task is executed only once, and
the energy is minimized when all tasks are running at the same speed. 
Note that this result can be found in \cite{aupy12ccpe}. 
\begin{lemma}
  \label{lemma_norel}
  Without replication or re-execution, solving \chain can be done 
  in polynomial time, and each task is
  executed at speed  . 
\end{lemma}
\begin{proof}
  For a linear chain of tasks, all tasks can be mapped on the same
  processor, and scheduled following the dependencies. No task may
  start earlier by using another processor, and all tasks run at the
  same speed. Since there is no replication nor re-execution, each
  task must be executed at least at speed \fr for the reliability
  constraint. If , then the tasks should be executed at speed
   so that the deadline constraint is matched (recall that
  ), hence the result. 
\end{proof}

Next, accounting for replication and re-execution, we characterize the
shape of an optimal solution. For linear chains, it turns out that
with a single processor, only re-execution will be used, while with
more than two processors, there is an optimal solution that do not use
re-execution, but only replication. 
\begin{lemma}[Replication or re-execution]
  \label{chain_reporreex}
  When there is only one processor, it is optimal to only use
  re-execu\-tion to solve \chain.  When there are
  at least two processors, it is optimal to only use replication to
  solve \chain.
\end{lemma}

\begin{proof}
  With one processor, the result is obvious, since replication cannot
  be used. With more than one processor, if re-execution was used on
  task~, for , we can derive a solution with the
  same energy consumption and a smaller execution time by using
  replication instead of re-execution. Indeed, all instances of
  tasks~, for , must finish before  starts its
  execution, and similarly, all instances of tasks~, for ,
  cannot start before both copies of~ has finished its
  execution. Therefore, there are always at least two processors
  available when executing~ for the first time, and the execution
  time is reduced when executing both copies of~ in parallel
  (replication) rather than sequentially (re-execution). 
\end{proof}


We further characterize the shape of an optimal solution by showing
that two copies of a same task can always be executed at the same
speed. 
\begin{lemma}[Speed of the replicas]
  \label{lemma.speed.chain}
  For a linear chain, when a task is executed two times, it is optimal
  to have both replicas executed at the same speed.
\end{lemma}

\begin{proof}
  The proof for re-execution has been done by \cite{rr7757}: by
  convexity of the energy and reliability functions, it is always
  advantageous to execute two times the task at the same speed, even
  if the application is not a linear chain. 

  For replication, this lemma is only true in the case of linear
  chains. Indeed, because of the structure of the chain, as explained
  in the proof of Lemma~\ref{chain_reporreex}, both copies of a task
  have the same constraints on starting and ending time, and hence it
  is better to execute them exactly at the same time. 
\end{proof}



We can further characterize an optimal solution by providing detailed
information about the execution speed of the tasks, depending whether
they are executed only once, re-executed, or replicated. 
\begin{proposition}
  \label{prop_WC_fr}
  If , then in any optimal solution of
  \chain, all tasks that are neither
  re-executed nor replicated are executed at speed~\fr.  Furthermore, 
  let  be the subset of tasks that are either
  re-executed or replicated. Then, these tasks are all executed at the
  same speed \freex, if . 
\end{proposition}

\begin{proof}
  The proof for  (re-execution) can be found in \cite{rr7757}.
  We prove the result for , which corresponds to the case
  with replication and no re-execution (see
  Lemma~\ref{chain_reporreex}).
Note first that since , if no task is
  replicated, we have enough time to execute all tasks at speed~\fr.

  Now, let us consider that task~ is replicated at
  speed~ (recall that both replicas are executed at the same
  speed, see Lemma~\ref{lemma.speed.chain}), and task~ is executed
  only once at speed~. Then, we have
   (reliability constraint on~), and 
   (otherwise, executing  only once at speed~\fr would improve
  both the energy and the execution time while matching the
  reliability constraint).

  If , let us show that we can rather execute~ at
  speed~ and  at a new speed , while keeping the
  same deadline: .  The energy consumption is then .  Moreover, we know that the minimum of the function
  , given that  is a constant (where  and  are the
  unknowns), is obtained for  (see
  Theorem~1 by \cite{aupy12ccpe}).  Therefore, if the optimal speed of
   (i.e., ) is strictly greater than~, then the optimal
  speed for  is , that means that we
  can improve both energy and execution time by executing  only
  once at speed~\fr.  Otherwise, the speed of  is further
  constrained by~\fr, hence the previous inequality () does not hold anymore, and the function is
  minimized for . The value of  can be easily deduced
  from the constraint on the deadline.  This proves that all tasks
  that are not replicated are executed at speed~\fr.

  Let . We now prove that if two
  tasks are replicated at a speed greater than~, then both tasks
  are executed at the same speed. Suppose that  and  are executed twice at speeds . Let .
  Then , and therefore we can execute
  both tasks at speed~ while keeping the same deadline and
  matching the reliability constraints. By convexity, such an
  execution gives a better energy consumption. We can iterate on all
  the tasks that are replicated, hence obtaining the speed at which
  each task will be re-executed, \freex. This concludes the proof.
\end{proof}


Following Proposition~\ref{prop_WC_fr}, we are able to precisely
define~\freex, and give a closed form expression of the energy of a
schedule. 
\begin{corollary}
  \label{cor.energy.chain}
Given a subset  of tasks re-executed or replicated, let 
, and 

Then, if ,  
the optimal energy consumption is 


Note that the energy consumption only depends on~, and therefore
\chain is equivalent in this case
to the problem of finding the optimal set of tasks that have to be
re-executed or replicated.
\end{corollary}

\begin{proof}
  Given a deadline~, the problem is to find the set of tasks
  re-executed (or replicated), and the speed of each task. Thanks to
  Proposition~\ref{prop_WC_fr}, we know that the tasks that are not in
  this set are executed at speed~\fr, and given the set of tasks
  re-executed or replicated, we can easily compute the optimal speed
  to execute each task in order to minimize the energy consumption:
  all tasks are executed at the same speed, and we have
  , with 
  in the case of replication (), and  in the case
  of re-execution (). 
  Hence the corollary.
\end{proof}

\begin{remark}
  Note that if there is a task  such that , then the optimal solution for this set of replicated tasks
  is obtained by executing~ at speed \finf, and by executing all
  the other tasks at a new speed ,
  such that  is exactly met.  We can do this recursively until
  there are no more tasks~ such that . Using the procedure \computeVl() (see
  Algorithm~\ref{algo.computevl}), we can
  compute the optimal energy consumption in a time polynomial
  in~. 

\begin{algorithm}[htb]
\caption{Computing re-execution speeds; tasks in  are
  re-executed.}
\label{algo.computevl}
procedure {\computeVl}()\\
\Begin{
\;
p=1p\geq 2\\
\;
\While{ or }{
\;
\;
p=1p\geq 2
}
\Return{;}
}
\end{algorithm}

Let  be the result of \computeVl(). Then the optimal
energy consumption is
 .



\end{remark}



\begin{corollary}
If , \chain can be solved using an exponential time 
exact algorithm.
\end{corollary}

\begin{proof}
  The algorithm computes for every subset  of tasks the energy
  consumption if all tasks in this subset are re-executed, and it
  chooses one with the minimal energy consumption, that corresponds to
  an optimal solution. It takes exponential time to compute every
  subset~, with . 
\end{proof}



Thanks to Corollary~\ref{cor.energy.chain}, we are also able to identify
problem instances that can be solved in polynomial time. 


\begin{theorem}
  \label{thm.chain}
 \chain can be solved in polynomial time in the
 following cases: 
\begin{enumerate}
    \item  (no re-execution nor replication);
    \item , , where ~is the 
      only positive
      solution to the polynomial , and
      hence , and 
      for ,  (all
      tasks can be re-executed);
\item , , and 
      for ,  (all tasks can
      be replicated).
\end{enumerate}
\end{theorem}

\begin{proof}
  First note that when , the optimal solution is
  to execute each task only once, at speed , since . Indeed, this solution matches both reliability and
  makespan constraints, and it was proven to be the optimal solution
  in Proposition~2 by \cite{aupy12ccpe} (it is easy to see that
  replication or re-execution would only increase the energy
  consumption).

  Let us now consider that . We aim at showing
  that the minimum of the energy function is reached when the total
  weight of the re-executed or replicated tasks is

Then necessarily, when this total weight is greater than~, the
optimal solution is to re-execute or replicate all the tasks. Hence
the theorem. We differentiate the two cases in the following ( or
).

\paragraph{Case 1 (). }
We want to show that the minimum energy is reached when the total
weight of the subset of tasks is exactly . Let  is executed twice in the solution, and let
.

We saw in Corollary~\ref{cor.energy.chain} that the energy consumption
cannot be lower than  where .  Therefore, we want to minimize .

If we differentiate , we can see that the minimum is reached when 
,
 that is, 
 , or 
 
The only positive solution to this equation is , and
therefore the minimum is reached for this value of , and
then . 

When , re-executing each task is the best strategy to
minimize the energy consumption, and that corresponds to the case . The re-execution speed may then be
lower than . Therefore, it may happen that  for some task~. However, even with a tighter deadline, it
would be better to re-execute~ at speed 
rather than to execute it only once at speed~\fr. Therefore, since
, it is optimal to re-execute~, at
the lowest possible speed, i.e.,~\finf. Note that this changes the
value of \freex, and the call to \mbox{\computeVl()} (see
Algorithm~\ref{algo.computevl}) returns tasks that are executed
at~\finf, together with the re-execution speed for all the other tasks.



\paragraph{Case 2 (). }
Similarly, we want to show that, in this case, the minimum energy is
reached when the total weight of the subset of tasks that are
replicated is exactly . Let  is executed
twice in the solution, and let .

We saw in Corollary~\ref{cor.energy.chain} that the energy consumption
cannot be lower than  where .  Therefore, we want to minimize .

If we differentiate , we can see that the minimum is reached when 

that 
is, , or 

The only positive solution to this equation is , and
therefore the minimum is reached for this value of~, and then 
.  

When , replicating each task is the best strategy to 
minimize the energy consumption, and that corresponds to the case 
. Similarly to Case~1, it is easy to see that
each task should be replicated, even if , since \mbox{.} The optimal solution can also be obtained with a
call to \computeVl(). 
\end{proof}


\subsection{FPTAS for \chain}
\label{lin.fptas}

We derive in this section a fully polynomial time approximation scheme
(FPTAS) for \chain, based on the FPTAS for SUBSET-SUM \cite{cormen},
and the results of Section~\ref{lin.char}. Without loss of generality,
we use the term {\em replication} for either re-execution or
replication, since both scenarios have already been clearly
identified. The problem consists in identifying the set of replicated
tasks~, and then the optimal solution can be derived from
Corollary~\ref{cor.energy.chain}; it depends only on the total weight
of these tasks, , denoted in the following
as~.

Note that we do not account in this section for \finf or \fmin for
readability reasons: \finf can usually be neglected because  is supposed to be very small whatever~, and \fmin simply
adds subcases to the proofs (rather than an execution at speed~,
the speed should be ).


\medskip
First we introduce a few preliminary functions in
Algorithm~\ref{algo.prel}, and we exhibit their properties. These are
the basis of the approximation algorithm. 

When , \xopt() returns the optimal value for
the weight~ of the subset of replicated tasks~, i.e.,
the value that minimizes the energy consumption for \chain. 
The optimality comes directly from the proof of Theorem~\ref{thm.chain}. 





Given a value~, which corresponds to , 
\energy() returns the optimal energy consumption when a
subset of tasks~ is replicated. 

Then, the function \trim() trims a sorted list
 in time~, given  and
.  is sorted into non decreasing order. The function
returns a trimmed list, where two consecutive elements differ from at
least a factor , except the last element, that is the
smallest element of~ strictly greater than~. This trimming
procedure is quite similar to that used for SUBSET-SUM \cite{cormen},
except that the latter keeps only elements lower than~. Indeed,
SUBSET-SUM can be expressed as follows: given  strictly positive
integers , and a positive integer~, we wish to
find a subset  of  such that \mbox{}
is as large as possible, but not larger than~.  
 In our case, the optimal solution may be
obtained either by approaching~ by below or by above.   


Finally, the approximation algorithm is \approxchain
(see Algorithm~\ref{algo.prel}),
where \mbox{}, and it returns an energy consumption~ that is
not greater than \mbox{()} times the optimal energy
consumption. 
Note that if , then \add() adds
element~ at the end of list~ (i.e., it returns the list
);  is the list
; and \merge() is merging two sorted
lists (and returns a sorted list).


 
\begin{algorithm}
\caption{Approximation algorithm for \chain.}
\label{algo.prel}
function {\xopt}()\\
\Begin{
\;
\lIf{}{\Return{}\;}
\lElse{\Return{}\;}
}

function {\energy}()\\
\Begin{
\;
\lIf{}{\Return{}\!\!\;}
\lElse{\Return{}\;}
}

function {\trim}()\\
\Begin{
; ; 
;
\;
\For{ \KwTo }{
\If{ ( and ) or }
{;
\;}
}
\Return{;} 
}




function \approxchain()\\
\Begin{
; 
;
\;
    \For{ \KwTo }{
\;
\;
}
Let  be the two largest elements of ;  \\
    \Return{;}
}
\end{algorithm}


We now prove that this approximation scheme is an FPTAS: 

\begin{theorem}
    \label{approx.chain}
    \approxchain\xspace is a fully polynomial time approximation
    scheme for \chain.
\end{theorem}

\begin{proof}
We assume that
\begin{compactitem}
    \item if , then ; 
    \item if , then ; 
\end{compactitem}
otherwise the optimal solution is obtained in polynomial time (see
Theorem~\ref{thm.chain}).  









Let , and 
. 
Note that  is not empty, since . 


\smallskip
First we characterize the solution with the following lemma: 

\begin{lemma}
    \label{lemma.i1i2}
    Suppose .  Then in the solution of \chain, the
    subset of replicated tasks~ is either an element
     such that  is maximum, or an element
     such that  is minimum. 
\end{lemma}

\begin{proof}
Recall first that according to Proposition~\ref{prop_WC_fr}, the
  energy consumption of a linear chain is not dependent on the number
  of tasks replicated, but only on the sum of their weights. 

  Then the lemma is obvious by convexity of the functions, and 
since \xopt\xspace returns the
  optimal value of~, the weight of the replicated tasks.
  Therefore, the closest the weight of the set of replicated tasks is
  to the optimal weight, the better the solution is. Finally, any element in  is a solution (since we have a solution
  for \xopt), and if the minimal element (if it exists) of 
  is not a solution, (\freex too large because of time constraints),
  then no element of  can be a better solution. 
\end{proof}




We are now ready to prove Theorem~\ref{approx.chain}.  
Let , and 
 .  
Thanks to Lemma~\ref{lemma.i1i2}, the optimal set of replicated
tasks~ is such that  or . 
The corresponding energy consumption is (Corollary~\ref{cor.energy.chain}): 



The solution returned by \approxchain\xspace corresponds either to~ or to~, where  and~ are the
two largest elements of the trimmed list. We first prove that at least
one of these two elements, denoted~, is such that  
, where . 




\paragraph{Existence of  such that . }
We differentiate two cases. 

\begin{description}
\item[(a)] If , then  is the value obtained by the FPTAS
  for SUBSET-SUM \cite{cormen} with the approximation
  ratio~, since it is the largest value not greater
  than~, and our algorithm is identical for such values.  Moreover,
  note that ~is the optimal solution of SUBSET-SUM by definition,
  and therefore .  If , the
  value  satisfies the property.  

  If , we prove that the property remains valid, by
  considering the SUBSET-SUM problem with a bound~ instead
  of~. Then, since , we have  by definition
  of~. Moreover, \approxchain\xspace is not removing any element
  of the list greater than~, and therefore all elements between
   and~ are kept, similarly to the FPTAS for SUBSET-SUM. 
  If , then  satisfies the property. Otherwise,
   is the result of the FPTAS for SUBSET-SUM with a bound~,
  whose optimal solution is~, and therefore  is such that 
  ;  satisfies the
  property.  



\item[(b)] If , no elements greater than~ have been
  removed from the lists, and \approxchain\xspace has been identical
  to the FPTAS for SUBSET-SUM. Then,  is the solution, that
  is valid both for SUBSET-SUM applied with the original bound~
  (optimal solution~), and with the modified bound  (optimal
  solution~). Therefore,  and , which concludes the proof. 
\end{description}

We have shown that there always is  (either  or ) such
that .  Next, we show that the
energy  obtained with this value~ is such that
. 


\paragraph{Approximation ratio on the energy:  .} 



Let us consider first that . Then we have 
.  
Re-using the previous inequalities on , we obtain: 
. 
Then, this can be rewritten so that  appears: 



The case  leads to the same inequality; the only difference is in
the energy~, where  is replaced by , and the
same difference holds for  ( is replaced by
). 

Finally, note that with no reliability constraints, each task is
executed only once at speed , and therefore the energy
consumption is at least . Moreover, by
hypothesis,  (for ). Therefore,
 and .


\medskip
We conclude that 



\paragraph{Conclusion. }
The energy consumption returned by \approxchain\xspace, denoted as
, is such that , since we take the minimum
out of the consumption obtained for  or~, and  is
either  or~. Therefore, . 

It is clear that the algorithm is polynomial both in the size of the
instance and in~, given that the trimming
function and \approxchain\xspace have the same complexity as in the
original approximation scheme for SUB\-SET-SUM (see~\cite{cormen}), and
all other operations are polynomial in the problem size (\xopt,
\energy). 
\end{proof}


\section{Independent tasks}
\label{sec.indep}
    
In this section, we focus on the problem of scheduling independent
tasks, \indep. Similarly to \chain, we know that \indep is NP-hard,
even on a single processor. We first prove in
Section~\ref{sec.inapprox} that there exists no constant factor
approximation algorithm for this problem, unless P=NP. We discuss and
characterize solutions to \indep in Section~\ref{char.indep}, while
highlighting the intrinsic difficulty of the problem. The core result
is a constant factor approximation algorithm with a relaxation on the
constraint on the makespan (Section~\ref{algo.indep}).





\subsection{Inapproximability of \indep}
\label{sec.inapprox}

\begin{lemma}
    \label{indep.inapprox}
For all , there does not exist any -approxi\-mation
of \indep, unless . 
\end{lemma}

\begin{proof}
Let us assume that there is a -approxi\-mation algorithm for
\indep. 
We consider an instance  of \partition: given  strictly positive 
integers , does there exist a subset  of  
such that ? Let
.

We build the following instance~ of our problem. We have  
independent tasks~ to be mapped on  processors, and: 
\begin{compactitem}
\item task~ has a weight ;
\item ; 
\item .
\end{compactitem}

\medskip
We use the -approxi\-mation algorithm to solve~, and the
solution of the algorithm  is such that , where  is the optimal solution. 
We consider the two following cases.\\ 
(i) If the -approxi\-mation algorithm returns a solution, then
necessary all tasks are executed exactly once at speed~\fmax, since
 and there are two processors. Moreover,
because of the makespan constraint, the load on each processor is
equal. Let  be the indices of the tasks executed on the first
processor. We have , and
therefore ~is also a solution to~. \\
(ii)~If  the -approxi\-mation algorithm does not return a
solution, then there is no solution to~. Otherwise, if  is a
solution to~, there is a solution to~ such that tasks
of~ are executed on the first processor, and the other tasks are
executed on the second processor. Since ,
the approximation algorithm should have returned a valid solution.  

Therefore, the result of the algorithm for~ allows us to
conclude in polynomial time whether there is a solution to the
instance~ of \partition or not. Since \partition is
NP-complete \cite{GareyJohnson}, the inapproximability result is true
unless P=NP. 
\end{proof}



\subsection{Characterization}
\label{char.indep}

As discussed in Section~\ref{sec.intro}, the problem of scheduling
independent tasks is usually close to a problem of load balancing, and
can be efficiently approximated for various mono-criterion versions of
the problem (minimizing the makespan or the energy, for instance).  
However, the tri-criteria problem turns out to be much harder, and
cannot be approximated, as seen in Section~\ref{sec.inapprox}, even
when reliability is not a constraint. 

Adding reliability further complicates the problem, since we no longer
have the property that on each processor, there is a constant
execution speed for the tasks executed on this processor. Indeed, some
processors may process both tasks that are not replicated (or
re-executed), hence at speed~\fr, and replicated tasks at a slower
speed.  Similarly to Section~\ref{lin.fptas}, we use the term {\em
  replication} for either re-execution or replication; if a task is
replicated, it means it is executed two times, and it appears two
times in the load of processors, be it the same processor or two
distinct processors.



Furthermore, contrary to the \chain problem, we do not always have the
same execution speed for both executions of a task, as in
Lemma~\ref{lemma.speed.chain}:  
\begin{proposition}
\label{prop.indep}
  In an optimal solution of \indep, if a task~ is executed twice:
\begin{compactitem}
    \item if both executions are on the same processor, then both are executed 
at the same speed, lower than ;
\item however, when the two executions of this task are on distinct
  processors, then they are not necessarily executed at the same
  speed.  Furthermore, one of the two speeds can be greater than
  .
\end{compactitem}
Moreover, we have . 
\end{proposition}

\begin{proof}
  We start by proving the properties on the speeds.  
  When both executions occur on the same processor, this property was
  shown by \cite{rr7757}: a single execution at speed~\fr leads to a
  better energy consumption (and a lower execution time). 

  In the case of distinct processors, we give an example in which
  the optimal solution uses different speeds for a replicated task,
  with one speed greater than . Note that one
  of the speeds is necessary lower than ,
  otherwise a solution with only one execution of this task at
  speed~\fr would be better, similarly to the case with re-execution. 

  Consider a problem instance with two processors, ,
  , and three tasks such that , ,
  and .  Because of the time constraints,  and  are
  necessarily executed on two distinct processors, and neither of
  them can be re-executed on its processor.  The problem consists in
  scheduling task~ to minimize the energy consumption. There are
  three possibilities: 
\begin{compactitem}
\item  is executed only once on any of the processors, at
  speed~; 
\item  is executed twice on the same processor; it is executed on
  the same processor than~, hence having an execution time of
  , and therefore both
  executions are done at a speed ; 
\item  is executed once on the same processor than~ at
      a speed , and once on the other processor at
      a speed . 
\end{compactitem} 
It is easy to see that the minimum energy consumption is obtained with
the last solution, and that , hence the result.  

Finally, note that since at least one of the executions of the task
should be at a speed lower than , and since the
deadline is~, in order to match the deadline, the weight of the
replicated task has to be strictly lower than
. 
\end{proof}

Because of this proposition, usual load balancing algorithms are
likely to fail, since processors handling only non-replicated tasks
should have a much higher load, and speeds of replicated tasks may be
very different from one processor to another in the optimal solution. 


















\medskip

We now derive lower bounds on the energy consumption, that will be
useful to design an approximation algorithm in the next section. 



\begin{proposition}[Lower bound without reliability]
 \label{bound.lp}
The optimal solution of \indep cannot have an energy lower than
 .
\end{proposition}

\begin{proof}
Let us consider the problem of minimizing the energy consumption, with
a deadline constraint~, but without accounting for the constraint
on reliability. A lower bound is obtained if the load on each
processor is exactly equal to~, and the speed of each
processor is constant and equal to~. The corresponding
energy consumption is , hence
the bound. 
\end{proof}

However, if the speed  is small compared to~\fr,
the bound is very optimistic since reliability constraints are not
matched at all. Indeed, replication must be used in such a case. 
We investigate bounds that account for replication in the following,
using the optimal solution of the \chain problem. 





\begin{proposition}[Lower bound using linear chains]
    \label{bound.chain.improved}
    For the \indep problem, the optimal solution cannot have an energy
    lower than the optimal solution to the \chain problem on a single
    processor with a deadline~, where the weight of the
    re-executed tasks is lower than .
\end{proposition}

\begin{proof}
  We can transform any solution to the \indep problem into a solution
  to the \chain problem with deadline~ and a single
  processor. Tasks are arbitrarily ordered as a linear chain, and the
  solution uses the same number of executions and the same speed(s)
  for each task. It is easy to see that the \indep problem is more
  constrained, since the deadline on each processor must be enforced. 
  The constraint on the weights of the re-executed tasks comes from
  Proposition~\ref{prop.indep}. Therefore, the solution to the \chain
  problem is a lower bound for \indep. 
\end{proof}

The optimal solution may however be far from this bound, since we do
not know if the tasks that are re-executed on a chain with a long
deadline~ can be executed at the same speed when the deadline
is~. The constraint on the weight of the re-executed tasks allows
us to improve slightly the bound, and this lower bound is the basis of
the approximation algorithm that we design for \indep. 



\subsection{Approximation algorithm for \indep}
\label{algo.indep}



We have seen in Section~\ref{sec.inapprox} that there exists no
constant factor approximation algorithm for \indep, unless P=NP, even
without accounting for the reliability constraint. This is due to the
constraint on the makespan and the maximum speed~\fmax. Therefore, in
order to provide a constant factor approximation algorithm, we relax
the constraint on the makespan and propose an
-approxi\-mation algorithm. The solution~ is
such that , where  is
the optimal solution with the deadline constraint~, and the
makespan of the algorithm  is such that . 

The result of Section~\ref{sec.inapprox} means that for all , there is no -approxi\-mation algorithm for \indep,
unless .  Therefore, we present 
an algorithm that realizes a -approximation, where the minimum relaxation on
the deadline is smaller than~. It is of course
possible to run the algorithm with larger values of~, leading
to a better guarantee on the energy consumption. 




\medskip
{\bf Sketch of the algorithm.}  In the first step of the algorithm, we
schedule each task with a big weight alone on one
processor, with no replication. A task~ is considered as {\em big}
if . This step is done
in polynomial time: we sort the tasks by non-increasing
weights, and then we check whether the current task is such that . If it is the case, we schedule the task alone
on a processor and we let  and . The procedure ends
when the current task is small enough, i.e., all remaining tasks
are such that , with the updated values
of  and~. 



\newcounter{nbProcs}
\newcounter{fra}
\newlength{\wiwi}
\newcounter{vis}
\newcounter{ssecbu}

\begin{figure*}
\centering
\subfloat[Input: six processors and eleven tasks]
{\begin{tikzpicture}
\begin{scope}[xscale=0.67]

\FPeval{\procspace}{.2}
\FPeval{\cury}{0}
\FPeval{\curx}{0}

\coordinate (OFF) at (0,0);
\begin{scope}[scale=.35, every node/.style={scale=0.6},shift=(OFF)]

\inSched
\addProc
\draw[->] (0,0-36*\procspace) -- (16,0-36*\procspace);
\draw[dashed] (15,0) -- (15,0-36*\procspace) ;
\draw[draw=none] (15,0-36*\procspace) node[below] {\ensuremath{D}};
\draw (0,0-35.5*\procspace) -- (0,0-36.5*\procspace) ;
\draw[draw=none] (0,0-36*\procspace) node[below] {\ensuremath{0}};

\addProc
\addProc
\addProc
\addProc
\addProc


\end{scope}

 \coordinate (SH) at (7,0);
 
 \begin{scope}[scale=.35, every node/.style={scale=0.6},shift=(SH)]


\addLisJob{15}{1}{1}
\addLisJob{11.25}{1}{2}
\addLisJob{6}{1}{3}
\addLisJob{6}{1}{4}
\addLisJob{5}{1}{5}
\addLisJob{4}{1}{6}


\end{scope}
 \coordinate (SH) at (14,0);

\begin{scope}[scale=.35, every node/.style={scale=0.6},shift=(SH)]


\addLisJob{4}{1}{7}
\addLisJob{3}{1}{8}
\addLisJob{2}{1}{9}
\addLisJob[below right]{1}{1}{{10}}
\addLisJob[below right]{1}{1}{{11}}

\end{scope}
\end{scope}
\end{tikzpicture}


}\1cm]
\subfloat[Greedy algorithm to schedule the new tasks]
{
\begin{tikzpicture}
\begin{scope}[xscale=0.67]

\FPeval{\procspace}{.4}
\FPeval{\cury}{0}
\FPeval{\curx}{0}

\coordinate (OFF) at (0,0);
\begin{scope}[scale=.35, every node/.style={scale=0.6},shift=(OFF)]

\inSched
\addProc

\draw (0,0-20.5*\procspace) -- (0,0-21.5*\procspace) ;
\draw[draw=none] (0,0-21.5*\procspace) node[below] {\ensuremath{0}};
\draw[dashed,red] (15,0) -- (15,0-21.5*\procspace) ;
\draw[draw=none] (15,0-21.5*\procspace) node[below] {\ensuremath{D}};
\draw[dashed] (25,0) -- (25,0-21.5*\procspace) ;
\draw[draw=none] (25,0-21.5*\procspace) node[below] {\ensuremath{\beta D}};


\addJobSpeed{15}{1}{1}
\addProc
\FPdiv{\speed}{3}{4}
\addJobSpeed{11.25}{\speed}{2}

\addProc
\FPdiv{\speed}{3}{4}
\addJobSpeed{6}{\speed}{3}
\FPdiv{\speed}{5}{12}
\addJobSpeed[above=0.2pt]{2}{\speed}{\ensuremath{9^{(2)}}}
\FPdiv{\speed}{3}{4}
\addJobSpeed[above=0.1]{1}{\speed}{{10}~}

\addProc
\FPdiv{\speed}{3}{4}
\addJobSpeed{6}{\speed}{4}
\FPdiv{\speed}{5}{12}
\addJobSpeed[above=0.2pt]{2}{\speed}{\ensuremath{9^{(1)}}}
\FPdiv{\speed}{3}{4}
\addJobSpeed[above=0.1]{1}{\speed}{{11}}

\addProc
\FPdiv{\speed}{5}{12}
\addJobSpeed[above=0.2pt]{3}{\speed}{\ensuremath{8^{(1)}}}
\FPdiv{\speed}{3}{4}
\addJobSpeed{5}{\speed}{5}


\addProc
\FPdiv{\speed}{5}{12}
\addJobSpeed[above=0.2pt]{3}{\speed}{\ensuremath{8^{(2)}}}

\FPdiv{\speed}{3}{4}
\addJobSpeed{4}{\speed}{6}
\addJobSpeed{4}{\speed}{7}


\end{scope}



\end{scope}
\end{tikzpicture}
}
\caption{-approximation algorithm for independent tasks\label{fig.indep}}
\end{figure*}


\begin{compactitem}


\item If , i.e., the load is {\em large enough}, we do not use replication, but we
  schedule the tasks at
  speed~, using a simple scheduling heuristic, \dff~\cite{Graham69}. Tasks
  are sorted by non increasing weights, and at each time step, we
  schedule the current task on the least loaded processor. Thanks to
  the lower bound of Proposition~\ref{bound.lp}, the energy
  consumption is not greater than the optimal energy consumption, and
  we determine  such that the deadline is
  enforced.  

\item If , the previous bound is not good enough, and therefore we use the
  FPTAS on a linear chain of tasks with deadline~ for \chain (see
  Theorem~\ref{approx.chain}).  The FPTAS is called with
  
  where . Note that it is
  slightly modified so that only tasks of weight
   can be replicated, and that we enforce a
  minimum speed~.  The FPTAS therefore determines which tasks
  should be executed twice, and it fixes all execution speeds.
   
  We then use \dff in order to map the tasks onto the  processors,
  at the speeds determined earlier. The new set of tasks includes both
  executions in case of replication, and tasks are sorted by non
  increasing execution times (since all speeds are fixed). At each
  time step, we schedule the current task on the least loaded
  processor. If some tasks cannot fit in one processor within the
  deadline~, we re-execute them at speed~ on two processors. Thanks to the lower bound of
  Proposition~\ref{bound.chain.improved}, we can bound the energy
  consumption in this case.
\end{compactitem}

\medskip
\noindent We illustrate the algorithm on an example in
Figure~\ref{fig.indep}, where eleven tasks must be mapped on six
processors. For each task, we represent its execution speed as its
height, and its execution time as its width. There are two {\em big}
tasks, of weights  and~, that are each mapped on a distinct
processor. Then, we have  and we call \approxchain\xspace with
deadline~; tasks  and  are replicated. Finally, \dff
greedily maps all instances of the tasks, slightly exceeding the
original bound~, but all tasks fit within the extended deadline.

\medskip
This algorithm leads to the following theorem: 

\begin{theorem}
    \label{thm.indep}
For the problem \indep, there are
-approxi\-mation
algorithms, for all , that run in
polynomial time.
\end{theorem}




Before proving Theorem~\ref{thm.indep}, we give some preliminary
results: we prove below the optimality of the first step of the
algorithm, i.e., the optimal solution would schedule tasks of weight
greater than  alone on a processor:

\begin{proposition}
  \label{prop.grobt}
  In any optimal solution to \indep, each task~ such that  
is executed only once, 
and it is alone on its processor.
\end{proposition}

\begin{proof}
  Let us prove the result by contradiction.  Suppose that there exists
  a task~ such that , and that
  this task is executed on processor~.  Suppose also that there
  is another task~ executed on~, with .
  Necessarily, there exists a processor, say~, whose load is
  smaller than , since the load of~ is strictly
  greater than~.
Consider the energy of the tasks executed on processors 
  and~. Because of the convexity of the energy function, it is
  strictly better to execute task~ on processor~, and then
  ~is executed alone on processor~, at a speed
  .  \end{proof}

\begin{comment}
\begin{proposition}
    \label{prop.sizereex}
For the problem \indep, if a task  is replicated or re-executed, then 
necessarily, .
\end{proposition}
\begin{proof}
  Since at least one of the execution of the task should be at a speed
  lower than  (otherwise we would not have a
  better energy than one execution at \fr), and since the deadline is
  , in order to match the deadline the weight of the task has to be
  strictly lower than . 
\end{proof}
\end{comment}





Next, we prove a lemma that will allow us to tackle the case where the
load is {\em large enough} (), and we obtain a minimum on the
approximation ratio of the deadline~. 

\begin{lemma}
\label{lemma.bigload}
For the problem \indep where each task~ is such that
 ,  scheduling each task only once at speed  with
 the \dff heuristic leads to a make\-span of at most , with
 .
\end{lemma}

Note that we introduce  since the lemma is
also used in the case . Also, since  is
increasing with~ and the bound is computed in fact for a number of
processors smaller than the original one (some processors are
dedicated to {\em big} tasks), the value of  computed with the
total number of processors~ is not smaller and it is possible to
achieve a makespan of at most~. 

\begin{proof}
Let  be the maximal load of the processors after
applying \dff on the weights of the tasks.  Let us find  such
that : this means that within
a time , we can schedule all tasks at speed , and therefore at speed , since the most loaded processor succeeds to be within
the deadline .

Let  be the maximal load of the processors in an
optimal solution, and let  be the last task executed on the
processor with the maximal load  by \dff. We have
either  or . 


\paragraph{} If , we know that
, since \dff is a -approxi\-mation
\cite{Graham69}. 
We want to compare  to  (average load).  We
consider the solution of \dff. At the time when  was scheduled,
all the processors were at least as loaded as the one on which 
was scheduled, and hence we obtain a lower bound on~: .  Furthermore,
 (because
 and ).
Finally, , which
means that , and
.

In this case, with , we can execute all the
tasks at speed
 within the deadline~.


\paragraph{} 
If , it is known that \dff is optimal for the execution time
\cite{Graham69}, i.e., , and we aim
at finding an upper bound on~.  We assume in the
following that tasks are sorted by non increasing weights.

If , then we show that ~is the only task executed on its processor (recall that ~is
the last task executed on the processor with the maximal load by
\dff). Indeed,
there cannot be  tasks of weight not smaller than~,
hence , and  is the first task scheduled on its
processor. Moreover, 
if \dff were to schedule another task on the processor of~,
  then this would mean that the  other processors all have a load
  greater than~, and hence the total load would be 
  greater than~. 
Then, since  and ,
we have  and we can execute each task at speed  within a deadline~. Indeed, the maximal
load is then~, by definition of~. Therefore, the result
holds (with ).

Now suppose that . In that case, if  was the
only task executed on its processor, then we would have
, which is impossible
since .  Therefore, 
is not the only task executed on its processor.  A direct consequence
of this fact is that . Indeed, \dff schedules the 
largest tasks on  distinct processors; since  is the last
task scheduled on its processor, but not the only one, then  is
not among the  first scheduled tasks. Also, there are only two
tasks on the processor executing~, since 
and the tasks scheduled before~ have a weight at least equal
to~. Finally, . 

After scheduling task  on processor  for ,
\dff schedules task  on processor  for , and ~is therefore scheduled on processor~,
together with task , and we have   
.
Note that because the  are sorted, . 
We also have : indeed, when  was
scheduled, the load of the  processors was at least equal to the
load of the processor where  was scheduled. Hence, 
 cannot be greater than~.  Then, since
,  , and finally . 

In order to find an upper bound on~, we provide a
lower bound to~, as a function of~: 


We then have , and we consider two cases. 


If , 
then we have , and finally 

We can conclude that . 


Otherwise,  and ~is a decreasing function of~,
i.e., its minimum is reached when  is maximal, and .
 Hence, . 
Since ,  and 

Finally, since , . 






Overall, if , we have the
bound  Therefore, for , we can
execute all the tasks on the processor of maximal load (and hence all
the tasks) at speed  within the deadline
 in the case . 

\medskip We can now conclude the proof of
Lemma~\ref{lemma.bigload} by saying that for , i.e., , scheduling each task only once at speed  with
 the \dff heuristic leads to a make\-span of at most .
 \end{proof}





We are now ready to prove Theorem~\ref{thm.indep}. 

\begin{proof}[{\bf Proof of Theorem~\ref{thm.indep}}]
First, thanks to Proposition~\ref{prop.grobt}, we know that the first
step of the algorithm takes decisions that are identical to the
optimal solution, and therefore these tasks that are executed once,
alone on their processor, have the same energy consumption than the
optimal solution and the same deadline. We can therefore safely ignore
them in the remaining of the proof, and consider that for each task~, 
. 

\medskip
In the case where , we use the fact that  is a lower bound on the
  energy (Proposition~\ref{bound.lp}).  Each task is executed once
  at speed , and therefore the
  energy consumption is equal to the lower bound . 
  The bound on the deadline is obtained by applying
  Lemma~\ref{lemma.bigload}. 






\medskip
We now focus on the case . 
Therefore, in the following, .  The
algorithm runs the FPTAS on a linear chain of tasks with
deadline~, and  as defined in
Equation~\eqref{def.epsilon}. The FPTAS returns a solution on the
linear chain with an energy consumption~ such that
, where  is
the  optimal energy consumption for \chain
with deadline~ on a single processor. According to
Proposition~\ref{bound.chain.improved}, since the solution for the
linear chain is a lower bound, the optimal solution of \indep is such that 
. 





For each task~, let  be the speed of its
execution returned by the FPTAS for \chain. Note that in case of
re-execution, then both executions occur at the same speed
(Lemma~\ref{lemma.speed.chain}).  We now consider the \indep problem
with the set of tasks~: for each task~,
 and its weight is ; moreover, if  is re-executed,
we add two copies of~ in~.  Then,
 by
definition of the solution of \chain.


Let  be the
relaxation on the deadline that we have from
Lemma~\ref{lemma.bigload}. The goal is to map all the tasks
of~ at speed~\fr within the deadline~, which
amounts at mapping the original tasks at the speeds assigned by the
FPTAS: \begin{itemize}
\item If there are tasks~ such that , we execute them at speed~ alone
on their processor, so that they reach exactly the deadline~. Note that in this case, the energy consumption of the algorithm
becomes greater than , since we execute these tasks
faster than the FPTAS to fit on the processor. 
\item Tasks~ such that  are executed alone on their processor at speed~\fr. 
\item For the remaining tasks and processors, we use \dff as in
  Lemma~\ref{lemma.bigload}. Since the previous tasks take a time of
  at least~ in the solution of the FPTAS, and they are mapped alone
  on a processor, we can safely remove them and apply the lemma. Note
  that the number of processors may now be smaller than~, hence
  leading to a smaller bound~.
\end{itemize}





In the end, all tasks are mapped within the deadline~ (where
 is computed with the original number of processors). There
remains to check the energy consumption of the solution returned by
this algorithm. 

\medskip
If all tasks are such that , 
. \\
According to Equation~\eqref{def.epsilon}, , and therefore 




\medskip
Otherwise, let  be the set of tasks~ such that
. For ,  . Since  (larger tasks have been
processed in the first step of the algorithm), we have
.  This means that  belongs to the set of
the tasks that are re-executed by the FPTAS.  Hence, since we enforced
an additional constraint, we have . The
least energy consumed for this task by any solution to \indep is
therefore obtained when re-execu\-ting task~ on two distinct
processors at speed~, in order to fit within the
deadline~. Task~ appears two times in~, and we let
 be the minimum energy consumption required in the optimal
solution for tasks of~:
.

\smallskip 
The algorithm leads to the same energy consumption as the FPTAS except
for the tasks of~ that are removed from the set~ of
replicated tasks, and that are executed at speed~:
 
Since , we obtain


Furthermore,  since it considers only the
optimal energy consumption of a subset of tasks. We have , and from
Proposition~\ref{prop_WC_fr}, it is easy to see that 
, i.e.,  is smaller than
the energy of every task executed once at speed~\fr. Hence, , 
and since , . Finally, 
. 
Thanks to Equation~\eqref{def.epsilon}, 
 (note that there are at
least two tasks in~, since tasks are duplicated). 

Finally, reporting in the expression of ,  


\medskip
To conclude, we point out that this algorithm is polynomial in the
size of the input and in . 
\end{proof}

We can improve the approximation ratio on the energy for large values
of~. The idea is to avoid the case in which tasks are replicated by the
chain but are not fitting within~ because the speed at which
they are re-executed is too small. To do so, we fix a value
, such that
 for . The variant of the algorithm is
used only when  (after scheduling the big tasks). 
The algorithm decides that the load is large enough when , leading to a
-approximation in this case. In the other
case (), it is possible to
prove that when there are tasks such that
, then necessarily all tasks are
re-executed. Next we apply Theorem~\ref{thm.chain} while fixing values
for the 's, so as to obtain in polynomial time the optimal
solution with new execution speeds, that can all be scheduled
within~ using Lemma~\ref{lemma.bigload}.  Details can be
found in the appendix.


\section{Conclusion}
\label{sec.conclusion}
In this paper, we have designed efficient approximation algorithms for
the tri-criteria energy/reliability/make\-span problem, using
replication and re-execution to increase the reliability, and dynamic
voltage and frequen\-cy scaling to decrease the energy consumption.
Because of the antagonistic relation between processor speeds and
reliability, this tri-criteria problem is much more challenging than
the standard bi-criteria problem, which aims at minimizing the energy
consumption with a bound on the makespan, without accounting for a
constraint on the reliability of tasks.

We have tackled two classes of applications. For linear chains of
tasks, we propose a fully polynomial time approximation scheme.
However, we show that there exists no constant factor approximation
algorithm for independent tasks, unless P=NP, and we are able in this
case to propose an approximation algorithm with a relaxation on the
makespan constraint: with a deadline at most two times larger than the
original one, we can approach the optimal solution for energy
consumption. 

As future work, it may be possible to improve the deadline relaxation
by using a FPTAS to schedule independent tasks \cite{Ausiello99}
rather than \dff \cite{Graham69}.  Also, 
an open problem is to find approximation algorithms for the
tri-criteria problem with an arbitrary graph of tasks. Even though
efficient heuristics have been designed with re-execution of 
tasks (but no replication) by \cite{rr7757}, it is not clear how to
derive approximation ratios from these heuristics. It would be
interesting to 
design efficient algorithms using replication and re-execution for
the general case, and to prove approximation ratios on these
algorithms. A first step would be to tackle fork and fork-join graphs,
inspired by the study on independent tasks. 

\paragraph*{Acknowledgements:}
This work was supported in part by the ANR {\em RESCUE} project. 


\bibliographystyle{abbrv}
\bibliography{biblio} 


\clearpage
\appendix

\section*{Appendix:
  -approximation
  algorithm for \indep}


This algorithm is used only for , and we define: 



Recall that . The value~ is therefore increasing
with~, and for , we have .
 Furthermore,  and . Finally, since , . 



\medskip
\noindent{\bf Modifications to the original algorithm.} \\
The handling of {\em big} tasks is identical. However, we do not use
replication when : 
we schedule tasks at speed  using
\dff. Proposition~\ref{prop.epsilon1} below shows that we obtain the
desired guarantee in this case.  
In the other case (), we apply
the FPTAS with the parameter~. It is now possible to
show that (i) either we can schedule all tasks with the speeds returned by the
FPTAS within the deadline~; (ii) or there is at least one
task that does not fit, but then all tasks are re-executed and we can find
an optimal solution that can be scheduled thanks to
Theorem~\ref{thm.chain}. The correction of this case is proven in
Proposition~\ref{prop.2}. 

\begin{proposition}
 \label{prop.epsilon1}
 For the problem \indep where each task~ is such that
 , if  , then scheduling each
 task only once at speed  with \dff
 is a -approxi\-mation algorithm, with .
\end{proposition}

\begin{proof}
  We use the fact that  is a lower bound on the
  energy (Proposition~\ref{bound.lp}).  If each task is executed once
  at speed , since , then the energy consumption is
  at most at a ratio  of
  the value of the optimal energy consumption. 
  The bound on the deadline is obtained by applying
  Lemma~\ref{lemma.bigload}. 
\end{proof}


\begin{proposition}
\label{prop.2}
For the problem \indep where each task~ is such that
 , if , then there is a -approxi\-mation
 algorithm, with . 
\end{proposition}

\begin{proof}
  Similarly to the original algorithm, we use the FPTAS and we obtain
  a -approxi\-mation algorithm unless there is a task~ such
  that , and hence
  . Since  (larger
  tasks have been processed in the first step of the algorithm), we
  have .  This means that  belongs to the
  set of the tasks that are re-executed by \approxchain.  Hence, since
  we enforced an additional constraint, we have
  .  Finally,






Let  be the total weight of the re-executed tasks
( or  in \approxchain), and let  be the optimal weight to solve \chain with one processor. 
We compute .  By
definition of  (Corollary~\ref{cor.energy.chain}), the optimal
speed at which each re-execution should occur, we have:

where  (Corollary~\ref{cor.energy.chain} 
applied to ). We now express : 

and therefore 
,
and finally , 
that is minimized when  is maximized. Applying the upper bound
on  from Equation~\eqref{eq.fchain}, we obtain:


Since , we have
 , and
. Since
 and , we obtain
, 
and therefore we have 
. This means that each task that can be
re-executed in any solution to \indep is indeed re-executed in the
solution given by \approxchain, since all these tasks have a weight
lower than . Since  is greater
than the total weight of the tasks that can be re-executed, we can use
Theorem~\ref{thm.chain} in the case , on the subset of
tasks~ such that . The other tasks
are executed once at speed~. We define , so that  and we can apply Theorem~\ref{thm.chain}.  Then, in polynomial
time, we have the optimal solution with new execution speeds:
.  Furthermore for each task~,
necessarily 

Note that since , we have , and
. We can
therefore schedule the new tasks~ within the deadline
relaxation using \dff, as a direct consequence of
Lemma~\ref{lemma.bigload}. \end{proof}



We can conclude by stating that thanks to
Propositions~\ref{prop.epsilon1} and~\ref{prop.2}, since
 is in  and  is in
, this algorithm is a
-approximation. 
Indeed,  and therefore
.  


Furthermore, the algorithm is polynomial in the size of the input and
in .

\end{document}
