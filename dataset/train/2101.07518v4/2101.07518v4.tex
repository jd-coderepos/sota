
\section{Experiments}\label{4}
This section evaluates the proposed method. In the following, we first describe the experimental setup, then compare our method with the state-of-the-arts, and finally conduct ablation studies to analyze the effectiveness of individual components.\footnoteonlytext{The authors from the universities in Taiwan completed the experiments on the datasets.}
\vspace{-0.2in}

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{1mm}
\caption{Evaluation results on GoPro test set. The best score in its column is highlighted in bold and the second best is underlined. Symbol  indicates those methods without released code; thus we cite the results from the original papers or evaluate on the released deblurred images. All methods are trained on GoPro training set. Time and Params are measured in millisecond (ms) and million (M).}
\begin{tabular}{l|c|c|c|c|c}\hline\hline
Model & PSNR   & SSIM  & Time  & Params  & GFLOPs  \\\hline
MSCNN~\cite{Nah_2017_CVPR}  & 30.40 & 0.936 & 943 & 12 &  336    \\
SRN~\cite{tao2018srndeblur} & 30.25 & 0.934 & 650 & 7 & 167 \\
DSD~\cite{gao2019dynamic} & 30.96 & 0.942 & 1300 & \underline3 & 471 \\
DeblurGAN-v2~\cite{Kupyn_2019_ICCV} & 29.55 & 0.934 & 42 & 68 & \bf42\\
DMPHN~\cite{Zhang_2019_CVPR}  & 31.36 & 0.947 & 354 & 22 & 235\\
EDSD~\cite{Yuan_2020_CVPR} & 29.81 & 0.934 & \bf10 & \bf1 & -- \\
MTRNN~\cite{MT_2020_ECCV} & 31.13 & 0.944 & 53 & \underline3 & 164 \\
RADN~\cite{RADN_2020_ECCV} & 31.85 & 0.953 & 38 & -- & -- \\
SAPHN~\cite{SAPN2020} & 32.02 & 0.953 & 770 &  -- & --   \\
MIMO-UNet+~\cite{MIMO} & 32.45 & 0.957 & \underline{23} & 16 & \underline{154} \\
MPRNet~\cite{Zamir_2021_CVPR} & \underline{32.66} & \underline{0.959} &  138 & 20  & 760    \\
\noalign{\hrule height 1.0pt}
BANet & 32.54 & 0.957 & \underline{23} & 18  & 264       \\
BANet+ & \bf33.03 & \bf0.961  & 25 & 40 & 588  \\\hline\hline
\end{tabular}
\label{Tab:Quant_eval}
\vspace{-0.1in}
\end{table}

\subsection{Experimental Setup}
We evaluate the BANet on three image deblurring benchmark datasets: 1) GoPro~\cite{Nah_2017_CVPR} that consists of  pairs of blurred and sharp images of resolution , where  pairs are used for training, and the rest for testing, 2) HIDE~\cite{HAdeblur} that contains  pairs of HD images, all for testing, and RealBlur~\cite{rim_2020_ECCV} that consists of  pairs for training and  pairs for testing. 
The RealBlur dataset is further split into two datasets: RealBlur-R collected from raw images and RealBlur-J from JPEG images.
We train our model using Adam optimizer with parameters  and .
We set the initial learning rate to , which then decays to  based on the cosine annealing strategy.
Following~\cite{MT_2020_ECCV, Yuan_2020_CVPR}, we utilize random cropping, flipping, and rotation for data augmentation. 
Lastly, we implement our model with PyTorch library on a computer equipped with Intel Xeon Silver 4210 CPU and NVIDIA 2080ti GPU.


\begin{figure*}[t!]
\centering
\includegraphics[width=2\columnwidth]{Figure/Visualization_Result_GoPro.pdf}
\vspace{-0.1in}
\caption{Qualitative comparisons on GoPro~\cite{Nah_2017_CVPR} test set. The deblurred results listed from left to right are from MTRNN~\cite{MT_2020_ECCV}, DSD~\cite{gao2019dynamic},  DMPHN~\cite{Zhang_2019_CVPR}, MIMO-UNet+\cite{MIMO}, MPRNet~\cite{Zamir_2021_CVPR}, and Ours.}
\label{fig:Visualization_Result_GoPro}
\end{figure*}

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{4.5mm}
\caption{Evaluation results on HIDE dataset. The best score in its column is highlighted in bold and the second best is underlined. Symbol  indicates those methods without released code; thus we cite the results from the original papers or evaluate on the released deblurred images. All methods are trained on GoPro training set. Time is measured in millisecond (ms).}
\begin{tabular}{l|c|c|c}\hline\hline
{Model} & {PSNR }  & {SSIM } & Time  \\\hline
DeblurGAN-v2~\cite{Kupyn_2019_ICCV} & 27.40 & 0.882 & 42 \\
SRN~\cite{tao2018srndeblur} & 28.36 & 0.904 & 424 \\
HAdeblur~\cite{HAdeblur} & 28.87 & 0.903 & -- \\
DSD~\cite{gao2019dynamic} & 29.10 & 0.913 & 1200 \\
DMPHN~\cite{Zhang_2019_CVPR} & 29.10 & 0.918 & 341 \\
MTRNN~\cite{MT_2020_ECCV} & 29.15 & 0.918 & 53 \\
SAPHN~\cite{SAPN2020}  & 29.98 & 0.930 & -- \\
MIMO-UNet+~\cite{MIMO} & 30.00 & 0.930 & 28     \\
MPRNet~\cite{Zamir_2021_CVPR} & \bf30.93 & \bf0.939 & 138 \\
\noalign{\hrule height 1.0pt}
BANet   & 30.16 & 0.930 & \bf23 \\
BANet+ & \underline{30.58} & \underline{0.935} & \underline{25} \\\hline\hline
\end{tabular}
\label{Tab:Quant_eval_HIDE}
\end{table}

\begin{figure*}[t!]
\centering
\includegraphics[width=2\columnwidth]{Figure/Visualization_Result_HIDE.pdf}
\vspace{-0.1in}
\caption{Qualitative comparisons on HIDE~\cite{HAdeblur} dataset. The deblurred results listed from left to right are from MTRNN~\cite{MT_2020_ECCV}, DSD~\cite{gao2019dynamic},  DMPHN~\cite{Zhang_2019_CVPR}, MIMO-UNet+\cite{MIMO}, MPRNet~\cite{Zamir_2021_CVPR}, and Ours.}
\label{fig:Visualization Result_HIDE}
\end{figure*}


\vspace{-0.1in}
\subsection{Experimental Results}
\noindent\textbf{Quantitative Analysis}:
We compare our method with 11 latest approaches, including MSCNN~\cite{Nah_2017_CVPR}, SRN~\cite{tao2018srndeblur}, DSD~\cite{gao2019dynamic}, DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, DMPHN~\cite{Zhang_2019_CVPR},
EDSD~\cite{Yuan_2020_CVPR}, 
MTRNN~\cite{MT_2020_ECCV}, RADN~\cite{RADN_2020_ECCV},
SAPHN~\cite{SAPN2020},
MIMO-UNet+~\cite{MIMO}, and MPRNet~\cite{Zamir_2021_CVPR}, which also handle dynamic deblurring on the GoPro~\cite{Nah_2017_CVPR} test set. 
For HIDE~\cite{HAdeblur}, we choose nine recent deblurring methods, including DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur}, HAdeblur~\cite{HAdeblur}, DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, MTRNN~\cite{MT_2020_ECCV}, SAPHN~\cite{SAPN2020}, MIMO-UNet+~\cite{MIMO}, and MPRNet~\cite{Zamir_2021_CVPR}, according to their availability in released pre-trained weights.
For RealBlur~\cite{rim_2020_ECCV}, we choose four methods that trained on the RealBlur training set, including DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur}, MPRNet~\cite{Zamir_2021_CVPR}, and MIMO-UNet+~\cite{MIMO}.

To better compare with recent approaches, we devise two versions of our model, BANet and BANet+. The only difference between them is the number of channels used in a BAM, and BANet with 128 channels involves 18 million parameters while BANet+ has 40 million parameters with 192 channels.
Table~\ref{Tab:Quant_eval} lists the objective scores (PSNR and SSIM), runtime, parameters, and GFLOPs on the GoPro test set for all the compared methods.
We observe that the self-recurrent models, MSCNN~\cite{Nah_2017_CVPR}, SRN~\cite{tao2018srndeblur}, DSD~\cite{gao2019dynamic}, MTRNN~\cite{MT_2020_ECCV}, SAPHN~\cite{SAPN2020}, and MPRNet~\cite{Zamir_2021_CVPR}, consume longer runtime than the non-recurrent ones,~\ie,~DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, RADN~\cite{RADN_2020_ECCV}, MIMO-UNet+~\cite{MIMO}, and ours.
As reported in Table~\ref{Tab:Quant_eval}, BANet runs faster with fewer parameters and GFLOPs as well as achieves better performance than recurrent-based methods, MSCNN~\cite{Nah_2017_CVPR}, SRN~\cite{tao2018srndeblur}, DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, MTRNN~\cite{MT_2020_ECCV}, and SAPHN~\cite{SAPN2020} and non-recurrent methods, such as DeblurGAN-v2~\cite{Kupyn_2019_ICCV} and RADN~\cite{RADN_2020_ECCV} on the GoPro test set. BANet also performs favorably against an efficient multi-scale model, MIMO-UNet+~\cite{MIMO}, with the same runtime and a comparable model size. 
BANet+ outperforms the best competitor, MPRNet~\cite{Zamir_2021_CVPR}, by 0.37 dB in PSNR with faster runtime (ms) and lower GFLOPs ().
Table~\ref{Tab:Quant_eval_HIDE} shows the quantitative results on HIDE~\cite{HAdeblur}. As can be seen, BANet outperforms all the compared methods except for MPRNet~\cite{Zamir_2021_CVPR} with a faster inference time. BANet+ only works comparably to MPRNet~\cite{Zamir_2021_CVPR} since MPRNet seems to perform favorably on HIDE~\cite{HAdeblur} particularly, but our model runs much faster.
Table~\ref{Tab:Quant_eval_RealBlur} lists the quantitative comparisons on the RealBlur test set, demonstrating that both BANet and BANet+ outperform the compared methods on the RealBlur-J and RealBlur-R test sets.

\begin{figure*}[t!]
\centering
\includegraphics[width=2\columnwidth]{Figure/RealBlur_A_results.pdf}
\vspace{-0.1in}
\caption{Examples of deblurred results obtained using DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur},  MPRNet~\cite{Zamir_2021_CVPR}, MIMO-UNet+~\cite{MIMO}, and Ours on RealBlur~\cite{rim_2020_ECCV} test set.}
\label{fig:RealBlur_A}
\vspace{-0.2in}
\end{figure*}

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{1.5mm}
\caption{Evaluation results on RealBlur test set. All methods are trained on RealBlur training set. Time is measured in millisecond (ms).}
\vspace{-0.1in}
\begin{tabular}{l|cc|cc|c}
\hline\hline
& \multicolumn{2}{c|}{RealBlur-J} & \multicolumn{2}{c|}{RealBlur-R} &\\ Model & PSNR  & SSIM   & PSNR  & SSIM  & Time \\ \hline
DeblurGAN-v2~\cite{Kupyn_2019_ICCV} & 29.69    & 0.870  & 36.44  & 0.935 & 44 \\
SRN~\cite{tao2018srndeblur} & 31.38  & 0.901 & 38.65  & 0.965 & 420 \\
MPRNet~\cite{Zamir_2021_CVPR} & 31.76 & 0.922 & 39.31 & \bf0.972 & 81  \\
MIMO-UNet+~\cite{MIMO} & 31.92 & 0.919 & -- & -- & \underline{23} \\
\noalign{\hrule height 1.0pt}
BANet  & \underline{32.00} & \underline{0.923}  & \underline{39.55}  & \underline{0.971}  &  \bf{22} \\
BANet+  & \bf{32.42}  & \bf0.929 & \bf39.90  & \bf{0.972}  &  24  \\
\hline\hline
\end{tabular}
\label{Tab:Quant_eval_RealBlur}
\end{table}

\noindent\textbf{Qualitative Analysis}:
Fig.~\ref{fig:Visualization_Result_GoPro} and Fig.~\ref{fig:Visualization Result_HIDE} show qualitative comparisons on the GoPro test set and HIDE dataset with previous state-of-the-arts MTRNN~\cite{MT_2020_ECCV}, DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, MIMO-UNet+~\cite{MIMO}, and MPRNet~\cite{Zamir_2021_CVPR}.
As observed in Fig.~\ref{fig:Visualization_Result_GoPro}, MTRNN~\cite{MT_2020_ECCV}, DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, MIMO-UNet+~\cite{MIMO}, and MPRNet~\cite{Zamir_2021_CVPR} do not well recover regions with texts or severe blurs whereas BANet can restore those regions better.
In Fig.~\ref{fig:Visualization Result_HIDE}, MTRNN~\cite{MT_2020_ECCV}, DSD~\cite{gao2019dynamic}, DMPHN~\cite{Zhang_2019_CVPR}, and MIMO-UNet+~\cite{MIMO} do not deblur the striped t-shirt and texts well, while BANet recovers those parts better. 
Fig.~\ref{fig:RealBlur_A} and Fig.~\ref{fig:RealBlur_B} demonstrate some deblurred results using DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur}, MPRNet~\cite{Zamir_2021_CVPR}, MIMO-UNet+~\cite{MIMO}, and ours, on the RealBlur~\cite{rim_2020_ECCV} test set. As can be seen, although all these models can remove blurs, BANet performs favorably on delicate image details.



\begin{figure*}[t!]
\centering
\includegraphics[width=2\columnwidth]{Figure/RealBlur_B_results.pdf}
\vspace{-0.1in}
\caption{Examples of deblurred results obtained using DeblurGAN-v2~\cite{Kupyn_2019_ICCV}, SRN~\cite{tao2018srndeblur},  MPRNet~\cite{Zamir_2021_CVPR}, MIMO-UNet+~\cite{MIMO}, and Ours on RealBlur~\cite{rim_2020_ECCV} test set.}
\label{fig:RealBlur_B}
\vspace{-0.1in}
\end{figure*}

\noindent\textbf{User Study}:
We further conduct a user study to evaluate the subjective quality of deblurred results on real blurred images chosen from the RealBlur-J test set. We compare our method (BANet+) against four methods, including MIMO-UNet+~\cite{MIMO}, MPRNet~\cite{Zamir_2021_CVPR}, SRN~\cite{tao2018srndeblur}, and DeblurGAN-v2~\cite{Kupyn_2019_ICCV}. Note that all the methods are trained on the RealBlur-J training set.

In the study, 34 subjects aged from 21 to 40 years participated in the study without any prior knowledge of the experiment. Their vision is either normal or corrected to be normal. We picked  blurred images with varying scenes for the experiment and obtained the deblurred results using all the compared approaches.
Since each method is compared against BANet with all the chosen blurred images in the experiment, we have  image pairs in total. Each subject is shown all the image pairs, one at a time, and asked which one he/she prefers in terms of visual quality. Each image pair is displayed randomly and placed side by side. Subjects are asked to check images carefully before choosing without a time limit.

Table~\ref{tab:user study} shows the subjective evaluation results, where the values represent the percentage that the deblurring results with our method are preferred to the counterparts with the other compared methods for all the votes collected. It indicates that our method obtains over 95\% preference votes compared to all the compared methods, which again demonstrates that our approach achieves better subjective visual quality.



\begin{table}[t]
\centering
\setlength{\tabcolsep}{1mm}
\caption{Results of User Study. The values represent the percentage that our method was chosen over the other compared methods.}
\begin{tabular}{c|cccc}\hline
 & MIMO-UNet+~\cite{MIMO} & MPRNet~\cite{Zamir_2021_CVPR}  & SRN~\cite{tao2018srndeblur} & DeblurGAN-v2~\cite{Kupyn_2019_ICCV} \\\hline
Ours & 95.6\% & 95.6\%  & 98.5\% &  99.6\% \\\hline 
\end{tabular}
\label{tab:user study}
\vspace{-0.1in}
\end{table}

\subsection{Ablation Study}
In the ablation studies, we do all the experiments on the BANet (18M) version.

\noindent{\bf BAM with Different Components:}
Table~\ref{tab:BAM_ablation} shows an ablation on different component combinations in our Blur-Aware Module (BAM) tested on the GoPro test set. As can be seen, adding a simple attention refinement (AR) mechanism to PDC (Net1~vs.~Net2) can improve PSNR by 0.41 dB, which shows the effectiveness of spatial attention for deblurring.
Using MKSP in PDC (Net1~vs.~Net4) improves PSNR by 0.72 dB, which has much more performance gain compared to using strip pooling (SP)~\cite{hou2020strip} in Net3 or AR in Net2.   
Substituting PDC in Net5 with CPDC (Net6), our proposed version of BAM leads to a further performance gain.
Thanks to its mechanism for locating blur regions based on both global attention and local convolutions, our BAM attains the best performance while achieving fast inference time.    

\begin{table}[t!]
\centering
\setlength{\tabcolsep}{3mm}
\caption{Ablation study on GoPro test set using different component combinations in BAM}
\begin{tabular}{c|cccccc}\hline
Model & PDC & AR & SP &  MKSP & CPDC & PSNR \\\hline
Net1  &  &   &  &  & & 31.39   \\
Net2  &  &  & & & & 31.80  \\
Net3  &  & &  & & & 31.81  \\
Net4  &  & &  &  & & 32.11  \\
Net5  &  &  &  &  & & 32.24  \\
Net6  & &  &  &  &  & 32.54  \\
\hline
\end{tabular}
\label{tab:BAM_ablation}
\end{table}

\noindent{\bf Numbers of Stacked BAMs:}
Using more layers to enlarge the receptive field may improve performance for computer vision or image processing tasks. 
Nevertheless, stacking more layers for deblurring does not guarantee better performance~\cite{SAPN2020} and might consume extra inference time. 
However, using our residual learning-based BAM design, we can stack multiple layers to expand the effective receptive field for better deblurring. 
In Table~\ref{tab:BAM_ablation2}, we show performance comparisons with various numbers of BAMs stacked in our model on the GoPro test set. 
We list four versions: stack-4, stack-8, stack-10, and stack-12, corresponding to 4, 8, 10, and 12 BAMs stacked in BANet. 
Although the quantitative performance improves with the number of  BAMs, the improvement became saturated after 12. 
Therefore, we choose 10 for its excellent balance between efficiency and visual quality.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{4mm}
\caption{Performance comparisons of the stacking number of BAMs in BANet on GoPro test set.}
\begin{tabular}{c|cccc}\hline
BANet & stack-4 & stack-8  & stack-10 & stack-12 \\\hline
PSNR & 31.36 & 32.37  & 32.54 &  \bf32.55 \\\hline 
\end{tabular}
\label{tab:BAM_ablation2}
\end{table}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{3mm}
\caption{Performance comparisons of strip pooling (SP) and MKSP on GoPro test set with PDC.}
\begin{tabular}{c|cccc}\hline
BANet & SP & MKSP & MKSP & MKSP \\\hline
PSNR & 31.81 & 32.03  & \bf32.11 &  32.04 \\\hline 
\end{tabular}
\label{tab:MKSP_ablation}
\end{table}

\noindent{\bf Effectiveness of MKSP and CPDC:}
In Table~\ref{tab:MKSP_ablation}, we investigate the effects of kernel combination of MKSP on the GoPro test set. 
MKSP with five kernel sizes of , , , , and  performs a little worse than
the first four sizes (, , , and ), indicating that adding the kernel size of  would not catch blur features more accurately, thus not helping with
the performance.
In Table~\ref{tab:compared CPDC with pdc}, we verify that CPDC, which uses a single convolution as a fusion bridge, outperforms PDC. 
For a fair comparison, we also compare CPDC against a PDC variant that stacks two PDCs in a series, called PDC, with a similar parameter size, and CPDC still performs better. 

\begin{table}[t]
\centering
\setlength{\tabcolsep}{6mm}
\caption{Ablation study of CPDC (w/o BA) compared to PDC (w/o BA) on GoPro test set.}
\begin{tabular}{cccc}\hline
 & PDC & PDC & CDPC \\\hline
PSNR & 31.39 & 31.78  & \bf32.13  \\ 
Parms (M) & \bf6 & 10  & 10  \\\hline
\end{tabular}
\label{tab:compared CPDC with pdc}
\end{table}

\iffalse
\begin{table}[t!]
\centering
\setlength{\tabcolsep}{3mm}
\caption{}
\begin{tabular}{cccc}\hline
& CPDC & CDPC  & CPDC \\\hline
PSNR & 32.03 & 32.13  &  32.34   \\ 
Parms (M) & 6 & 10  & 14  \\\hline
\end{tabular}
\label{tab:CPDC with different dilated rate}
\end{table}
\fi

\subsection{Blur-aware Attention vs. Self-Attention}
RADN~\cite{RADN_2020_ECCV} utilizes a similar self-attention (SA) mechanism proposed in~\cite{SAGAN_2019_PMLR} for deblurring. 
It helps connect regions with similar blurs to facilitate global access to relevant features across the entire input feature maps. 
However, its high memory usage makes applying it to high-resolution images  infeasible. 
Thus, SA is usually employed in network layers on a smaller scale like in RADN~\cite{RADN_2020_ECCV}, where important blur information would be lost due to down-sampling. 
In contrast, our proposed region-based attention is more suitable for correlating regions with similar blur characteristics. Moreover, it can process high-resolution images thanks to its low memory consumption. 
To further demonstrate our BAâ€™s efficacy, we compare the SA~\cite{SAGAN_2019_PMLR} with BA using our BANet (stack-4) as a backbone network, as shown in Fig.~\ref{fig:compared with sa}(b). 
Due to the high memory demand for SA () to process  images, we adopt our stack-4 model for training. 
When testing the networks, we separate the input image into eight sub-images for both SA and BA to deblur, each equipped with a single 2080ti GPU. Since our BA requiring lower memory usage (, where ) can process the image with the full resolution, we also show its result.
In Table~\ref{tab:SA_vs_BA}, SA and BA represent deblurring an image with its eight sub-images separately, whereas BA for processing the entire image at once. As can be observed, the proposed BA works much more efficiently than SA with a comparable result. When deblurring the entire image at once, BA undoubtedly performs the best.


\begin{figure}[t!]
\centering
\includegraphics[width=1\columnwidth]{Figure/Compared_with_self_attention.pdf}
\caption{Architecture comparisons between (a) our original BAM and (b) BA replaced by SA~\cite{SAGAN_2019_PMLR} in BAM.}
\label{fig:compared with sa}
\end{figure}


\begin{table}[t!]
\centering
\setlength{\tabcolsep}{7mm}
\caption{Performance comparison between BA and SA~\cite{SAGAN_2019_PMLR} using BANet (stack-4) on GoPro test set.  represents deblurring on eight sub-images instead of an entire image.}
\begin{tabular}{cccc}\\\hline
    & SA & BA & BA  \\\hline
    PSNR &  31.11  & 31.09 & \bf31.36    \\
    Time (ms)  & 770 & 16 & \bf12 \\\hline   
\end{tabular}
\label{tab:SA_vs_BA}
\end{table}






