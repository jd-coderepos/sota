

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} \usepackage[accsupp]{axessibility}  

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{color,colortbl}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{lightGreen}{rgb}{.935,.99,.935}
\definecolor{lightBlue}{rgb}{.935,.935,.99}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{10829} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment}
\author{
	Mengke Li \quad Yiu-ming Cheung\thanks{Yiu-ming Cheung is the Corresponding Author.} \quad Yang Lu \vspace{.3em}\\
	Department of Computer Science, Hong Kong Baptist University, Hong Kong\\
	Department of Computer Science and Technology, School of Informatics, Xiamen University, China\\
	{\tt\small \{csmkli, ymc\}@comp.hkbu.edu.hk, luyang@xmu.edu.cn}
}

\maketitle

\begin{abstract}
Long-tailed data is still a big challenge for deep neural networks, even though they have achieved great success on balanced data. We observe that vanilla training on long-tailed data with cross-entropy loss makes the instance-rich head classes severely squeeze the spatial distribution of the tail classes, which leads to difficulty in classifying tail class samples. Furthermore, the original cross-entropy loss can only propagate gradient short-lively because the gradient in softmax form rapidly approaches zero as the logit difference increases. This phenomenon is called softmax saturation. It is unfavorable for training on balanced data, but can be utilized to adjust the validity of the samples in long-tailed data, thereby solving the distorted embedding space of long-tailed problems. To this end, this paper proposes the Gaussian clouded logit adjustment by Gaussian perturbation of different class logits with varied amplitude. We define the amplitude of perturbation as cloud size and set relatively large cloud sizes to tail classes. The large cloud size can reduce the softmax saturation and thereby making tail class samples more active as well as enlarging the embedding space. To alleviate the bias in a classifier, we therefore propose the class-based effective number sampling strategy with classifier re-training. Extensive experiments on benchmark datasets validate the superior performance of the proposed method. Source code is available at \href{https://github.com/Keke921/GCLLoss}{\textcolor{blue}{https://github.com/Keke921/GCLLoss}}.
\end{abstract}

\section{Introduction} \label{sec:intro}
Deep neural networks (DNNs) have been widely utilized in a variety of visual recognition problems~\cite{he2016deep, he2017mask, ren2016faster, Wang2017NormFace} by virtue of the large-scale, high-quality, and annotated datasets. DNNs usually require the training dataset to be artificially balanced and have sufficient samples of each class. Unfortunately, from a practical perspective, object frequency usually follows a power law and typically exhibits a long-tailed distribution. Naive learning on such data is prone to undesirable bias towards the head classes which occupy the majority of the training samples~\cite{zhang2021survey}. Since tail classes have few training samples that cannot cover the real distribution in embedding space, their spatial span is severely compressed by head classes. In addition, a vast number of head class samples generate overwhelming discouraging gradients for tail classes. Thus, the learning of a classifier is biased towards the head classes. As a result, directly training on long-tailed data brings two key problems: 1) the distorted embedding space, and 2) the biased classifier.

\begin{figure}[t]
\centering
 \includegraphics[width=0.46\textwidth]{./Figure/intro_toy6.png}
 \caption{t-SNE visualization of the distorted embedding space. (Color for the best view.) The embeddings are calculated with ResNet-32 on a subset with four classes of CIFAR-10-LT. We randomly select four classes with the training numbers 500, 200, 100, and 50, respectively. The gray areas show the obscure regions between different classes.}
 \label{fig:intro-toy}
 \vspace{-6pt}
\end{figure}


In the literature, most of the recently proposed approaches focus on addressing the second problem only, \ie, the biased classifier. For example,  Menon \etal~\cite{adjustment21} and Hong \etal~\cite{Hong2021CVPR} applied post-adjust strategy to the trained model to calibrate the class boundary. Nevertheless, the distorted embedding cannot be adjusted with the post-hoc calibration, which is not conducive to further improving the model performance. Most recently, the two-stage decoupling methods~\cite{bbn20, decouple20, Kaidi2019, wang2020devil, DisAli21} have been proposed to obtain good embeddings in the first stage and then re-balance the classifier in the second stage. These methods obtain the representation by cross-entropy (CE) loss, which, however, leads to a severely uneven distributed embedding space. We implement a toy experiment to illustrate the distortion of the embedding space as shown in \cref{fig:intro-toy}, where t-SNE~\cite{van2008visualizing} is utilized to visualize the features of a long-tailed subset from CIFAR-10 dataset. We can observe that the tail class occupies a much small spatial span than the head class. This is because the tail class with fewer samples cannot cover the ground truth distribution. Moreover, \cref{fig:intro-toy} also shows that there are obscure regions (\ie, the grey area) between different classes. Softmax saturation~\cite{Chen2017CVPR} is one of the factors of these obscure regions because it leads to insufficient training. These obscure regions have a severe effect on the tail classes but little on the head classes. Since tail class samples clustered around the class boundary aggravate their spatial squeezing, while the head class samples with enough variety can already cover the true distribution.


Softmax saturation refers to the inopportune early gradients vanishing produced by the softmax~\cite{Chen2017CVPR, Zhang2021class}, which weakens the validity of training samples and impedes model training. However, from another perspective, the seemingly harmful softmax saturation has the ability to balance the valid samples of different classes and thus help calibrate the distortion of embedding space. Specifically, we disturb the logit of different classes with different amplitudes. We name the disturbed logit as Gaussian clouded logit (GCL) and the amplitude of the disturbance as cloud size, because we set the disturbance to a Gaussian distribution. The tail classes have few training samples and thus the training samples of them should be more valid. We therefore disturb the logit of tail classes with large relative cloud sizes to reduce the softmax saturation. In this way, tail class samples can provide more gradients without overfitting and thus indirectly affect their embedding space. In addition, a large cloud size of the tail class logit corresponds to the large cloud size on feature in the direction of the class anchor. Therefore, tail classes can have large margins towards the class boundary, so as to alleviate the severe uneven distribution between the head and tail classes. Conversely, the head classes are set to small cloud sizes, so that they can be automatically filtered out during training. Eventually, as shown in ~\cref{fig:intro}, the tail class samples can be pushed more away from the class boundary so as the distortion of the embedding space can be calibrated.

\begin{figure}[t]
\centering
 \includegraphics[width=0.5\textwidth]{./Figure/intro_h.png}
 \caption{An overview of GCL. (Color for the best view.) The tail class logit is assigned to a larger sample cloud size than the head class, which corresponds to a large relative cloud size of the feature in the direction of the tail class anchor. In this way, the distortion of the embedding space can be calibrated well.}
 \label{fig:intro}
 \vspace{-6pt}
\end{figure}

To address the biased classifier, we re-balance the training data with a class-wise sampling strategy. As training with GCL makes the validity of different classes vary, the so-called ``effectiveness''~\cite{cui2019class} of them are different. Existing class-wise balanced sampling strategies will lead to excessive training of tail classes for GCL. We thereby propose the class-based effective number (CBEN) sampling strategy, which is based on sample validity and label frequencies to re-balance the classifier. This simple but effective sampling strategy helps mitigate the classifier bias towards the head classes and further boost the performance of GCL.

Extensive experiments on multiple commonly used long-tailed recognition benchmark datasets demonstrate that the proposed GCL surpasses the recently proposed counterparts. In summary, the key contributions of our work are three-fold:
\begin{itemize}
    \item We propose the GCL adjustment loss function, which utilizes softmax saturation to balance the sample validity of different classes. An evenly distributed embedding can be obtained with the proposed GCL.
    \item We propose a simple but effective class-based effective number (CBEN) sampling strategy for re-balancing the classifier to avoid repeat training of tail classes. This sampling strategy can further boost the performance of GCL.
    \item Extensive experiments on popular long-tailed datasets demonstrate that the proposed method outperforms the state-of-the-art counterparts.
\end{itemize}



\section{Related Works}
\label{sec:related_works}
Long-tailed classification is one of the long-standing research problems in machine learning. Several kinds of approaches have been proposed to address it. This section briefly introduces the most related three regimes, namely loss modification, logit adjustment, and decoupling representation.
\subsection{Loss Modification}
Modifying the loss function through re-weighting is the most natural method. Sample-wise re-weighting methods~\cite{Mengye2018Learning, Tsung2020Focal} attempt to make the model pay more attention to the difficult samples by introducing fine-gained coefficients in the loss for imbalanced learning. For example, focal loss~\cite{Tsung2020Focal} introduces a tunable focusing parameter, which is negatively correlated with the predicted probability of the target class. This focusing parameter helps the model training focus on hard samples and prevents the numerous easy negatives from overwhelming. Class-wise re-weighting methods~\cite{Huang2016CVPR, Salman2018Cost, cui2019class, tan2020Equalization} assign the standard CE loss with category-specific parameters that are inversely proportional to the class frequencies. For example, Tan \etal~\cite{tan2020Equalization} proposed equalization loss, which utilizes a weight term to randomly ignore the discouraging gradients of head class samples. These methods can alleviate the data imbalance to a certain extent. However, the classification difficulty of a sample is not directly related to its corresponding class size. Further, another side effect of assigning higher weights to difficult samples/tail classes is overly focusing on harmful samples (\eg, noisy data or mislabeled data)~\cite{Pang2017Understanding}.

\subsection{Logit Adjustment}
Logit adjustment assigns relatively large margins for tail classes. Most recently, Menon \etal~\cite{adjustment21} have proposed a logit adjustment (LA) method which is consistent with minimizing the balanced error. The logit shifting in LA of different classes is based on label frequencies of training data. Differently, LADE~\cite{Hong2021CVPR} calibrates the logit to the test set using the label distribution of test data, so that the test set can also be imbalanced. Tang~\etal~\cite{De-confound-TDE20} adopted causal intervention to remove the ``bad'' SGD momentum and keep the ``good'' one to avoid the harmful causal effect for tail prediction. DisAlign~\cite{DisAli21} adjusts the logit by calibrating the model prediction to a reference distribution of classes that favors the balanced prediction. These methods well adjust the model logits through post-hoc shifting but without considering calibrating the embedding space. Another type of approach~\cite{Kaidi2019, Cao2020CVPR} addresses long-tailed data by leaving large relative margins for tail classes during training. For example, label-distribution-aware margin (LDAM) loss proposed by Cao~\etal~\cite{Kaidi2019} utilizes Rademacher complexity to theoretically prove that the margin should be inversely proportional to a quarter power of label frequencies. The hard margin on target logit helps make the intro-class samples more compact, but does not truly enlarge the tail class span in embedding space.

\subsection{Decoupling Representation }
Many recent works have focused on improving the long-tailed visual recognition performance by decoupling the representation and classifier.
Most recently, LDAM-DRW~\cite{Kaidi2019} has been proposed, which learns features in the first stage and adopts the deferred re-weighting (DRW) to fine-tune the decision boundary in the second stage. It significantly improves the long-tailed prediction accuracy, but the theoretical explanation of DRW is not clear. After that, Kang~\etal~\cite{decouple20} precisely pointed out that the learning process of representation and classifier can be decoupled into two separate stages. The representation learning is conducted on the original long-tailed data in the first stage and the classifier learning is performed on class-balanced re-sampling data in the second stage. A lot of works \cite{wang2020devil, Wang2020Frustratingly, DisAli21, mislas21} have further refined this strategy. For example, Zhang~\etal~\cite{DisAli21} proposed an adaptive calibration function to calibrate the predicted logits of different classes into a balanced class prior in the second stage. Zhong~\etal~\cite{mislas21} proposed label distribution-based soft label to deal with different degrees of over-confidence for classes and can improve the classifier learning in the second stage. Another alternative direction is proposed by Zhou~\etal~\cite{bbn20}, which splits the network structure into two branches that focus on learning the representation of head and tail classes, respectively. This method incorporates feature mixup~\cite{verma2019manifold} into a cumulative learning strategy and also achieves the state-of-the-art results. Following~\cite{bbn20}, Wang~\etal~\cite{contrastive21} introduced contrastive learning into this bilateral-branch network to further improve the long-tailed classification performance.

\section{Proposed Approach: GCL}\label{method}
The key idea of our proposed GCL is to utilize the softmax saturation to automatically balance the valid samples of head and tail classes. The theoretical motivation and the formulation of the loss function of the proposed approach are presented as follows.

\iffalse
\subsection{Basic Notations}
The notation used throughout the paper is defined in this section.

\textbf{For the dataset}: Suppose  represent a sample  from the training set , where  has totally  classes and  training samples,  represents the image that needs to be classified and  is the ground truth label. For class , the number of training samples is  and .

\textbf{For the backbone}:
 is the feature obtained from the embedding layer with the dimension of .  represents the weight matrix of the classifier, where  represents the anchor vector of class  in the classifier. The predicted logit of class  is represented by , thus, . We use the subscript  to represent the target class, namely,  indicates the target logit and  is the non-target logit.
\fi

\subsection{Motivation}\label{sec:motivation}

\cref{fig:intro-toy} shows that the obscure region among different classes, especially the tail class, is large. One important factor of this obscure region is the softmax saturation in CE loss~\cite{Chen2017CVPR}. Suppose  represents a sample  from the training set  with the total  samples in  classes, and  is the ground truth label. The softmax loss function for the input image  can be written as:

where  represents the predicted logit of class . We use the subscript   to represent the target class. That is,  indicates the target logit and  is the non-target logit.

In backward propagation, the gradients on  is calculated by:

Without loss of generality, we use the binary classification as an example. Supposing  is from class 1, the gradients on  is then calculated by:

\cref{eq:bi_partial} indicates that the gradient of the target class rapidly approaches zero with the increase of the logit difference. Softmax can only slightly separate various classes, and lacks the power to evenly distribute each class in the embedded space. Therefore, there are many overlapping areas among the classes. In particular, under the circumstances of long-tailed classification, the tail class features are not sufficient to cover the real distribution in embedding space. The early gradient vanish caused by softmax saturation exacerbates the squeezing of their embedding space. A straightforward approach is to introduce hard margin~\cite{Deng2019ArcFace, Kaidi2019, Zhang2021class}. However, the hard margin will cause the samples to shrink towards the class anchor and easy to overfit tail classes, which cannot evenly distribute the embedding space well. Fortunately, softmax saturation can help filter out the head class samples and make the tail class samples fully participate in training. In this way, the tail classes can be pushed away from the head classes and indirectly enlarge their embedding space. 

\iffalse
\begin{figure}[t]
\centering
 \includegraphics[width=0.46\textwidth]{./Figure/gradient.png}
 \caption{The gradient on   in binary classification case. As the logit difference increases, the gradient rapidly approaches zero.}
 \label{fig:ce}
 \vspace{-6pt}
\end{figure}
\fi

\subsection{Embedding Space Calibration}\label{sec:sc}
Suppose the features of different class samples satisfy Gaussian distribution. We can obtain a disturbed feature  of the input by Gaussian sampling, which is represented as:

where  is the feature obtained from the embedding layer with the dimension of .  is the disturbance sampled from Gaussian distribution, and the mean vector and covariance matrix are represented by  and  , respectively.  is a parameter that is used to adjust the amplitude of disturbance. In addition,  should be a small number because a large disturbance will mislead the model. This disturbed feature is the input of the classifier. We use  to represent the weight matrix of the classifier, where  represents the anchor vector of class  in the classifier. Then, the corresponding disturbed logit  of class  is calculated by:
-6pt]
  &= \mathbf{w}^T_{j}\mathbf{f} + \mathbf{b}_{j} + \mathbf{w}^T_{j}(\mathbf{\delta\mathbf{E}})\\
  \
As the range of  is enlarged with random Gaussian disturbances, we call it Gaussian clouded logit, and  is the clouded term. Please note that the clouded term has the different degrees of influence on the final predicted results based on different predicted logits. It has a relatively small impact on  when the original logit  is large. On the contrary, it will play a key role for  when  is small. As a result, we need to normalize the effect caused by different predicted logits and maintain the consistency of the influence of the clouded term. Inspired by \cite{Wang2017NormFace, wang2018cosface, Deng2019ArcFace}, we normalize the clouded logits based on cosine distance. In this way, the norm of the feature and the class anchor can be normalized to the fixed numbers. We use  and  to represent these two numbers. The normalized clouded logit is named \textit{clouded cosine logit}, which is calculated by:

-6pt]
  & = \displaystyle s\cdot(\frac{\mathbf{w}^T_{j}\mathbf{f}}{\|\mathbf{w}^T_{j}\|\| \mathbf{f}+\delta\mathbf{E}\|} + \delta\frac{\mathbf{w}^T_{j}\mathbf{E}}{\|\mathbf{w}^T_{j}\|\| \mathbf{f}+\delta\mathbf{E}\|})\\
\end{array},

  \Tilde{z}^{cld}_{j} \approx s\cdot(\frac{\mathbf{w}^T_{j}\mathbf{f}}{\|\mathbf{w}^T_{j}\|\| \mathbf{f}\|} + \frac{\delta}{s_1} I_j\mathbf{E}),
\label{cld_z_tmp}
\begin{array}{cc}
\Tilde{z}^{cld}_{j} & = s\cdot(\Tilde{z}_{j} + \frac{\delta}{s_1}\varepsilon_j)  \\
     \
where  is the cosine distance, and  is the angle between  and .  is the logit cloud size that depends on different classes.

To achieve the two goals mentioned in \cref{sec:motivation}, \ie, 1) encourage tail class samples to participate more in training; 2) enlarge the embedding space for the tail classes, the size of logit cloud should be negatively correlated with the number of training samples. For the most frequent class, the diversity of training samples is sufficient and we set its logit cloud size to zero, while utilizing larger cloud sizes for tail classes. The merits of this large relative cloud size of tail classes are three-fold: 1) reduce the softmax saturation and thereby increase the training degree of tail classes; 2) different values are sampled randomly from the Gaussian cloud so as to avoid overfitting; 3) enlarge the margin of class boundary for tail classes and can calibrate the distortion of the embedding space. We therefore empirically set the cloud size for class  as:

where  is the sample numbers of the most frequent class. We experimentally verify the effectiveness of this cloud size adjustment strategy in \cref{sec:cloud_size} .

The Gaussian clouded logit difference  between the target and non-target classes is:
-6pt]
& = z_y-z_j+\varepsilon ({\delta_y-\delta_j})
\end{array}.
\label{eq:z_cld_i}
\Tilde{z}^{cld}_j=  s\cdot(\Tilde{z}_j  - \delta_j\|\varepsilon\|).
\label{gcl}
  \mathcal{L}_{GCL} = -\frac{1}{N}\sum_i\log \frac{e^{\Tilde{z}^{cld}_{y_i}}}{\sum_j e^{\Tilde{z}^{cld}_j}}.
\label{eq:sp}
  \rho_j= \frac{1-\beta_j}{1-\beta_j^{n_j}}.
\label{eq:beta}
  \beta_j= b \times \frac{\delta_j-\delta_{min}}{\delta_{max}-\delta_{min}}+a,
-3pt]  \subfloat[On test set]{
        \includegraphics[width=0.48\textwidth]{./Figure/test.png}
        \label{fig:toy-test}}
\vspace{-6pt}
\caption{Visualization of the embedding via t-SNE from CIFAR-10-LT with , where backbone network is ResNet-32. (Color for the best view.)}
\label{fig:visualization}
\end{figure}

\begin{table}[t]
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
 \centering  \caption{Ablation experiment of different cloud size adjustment strategies (AS) on CIFAR-10-LT with .} \label{ab_delta}
 \vspace{-6pt}
 \setlength{\tabcolsep}{10pt}
\renewcommand{\arraystretch}{0.75}
  {\begin{tabular}{>{\centering}p{70pt}>{\centering}p{80pt}>{\raggedleft\arraybackslash}p{25pt}}
  \toprule[0.8pt]
  AS &Expression & Acc.(\%)  \\
  \hline
  \hline
  & &  \-6pt]
  pow. diff. (e:1/3)  &  &80.80 \\
  & &  \-6pt]
  log. diff.  &   & \underline{\textbf{82.68}}  \\
  \bottomrule[0.8pt]
 \end{tabular}}
 \vspace{-6pt}
\end{table}

\subsubsection{Cloud Size Adjustment Strategy}\label{sec:cloud_size}
We explored several different cloud size adjustment strategies (AS), which included cosine form (cos.), power difference (pow. diff.) with different exponents (e:1/3 and e:1/4), and logarithmic difference (log. diff.). For a fair comparison, the sampler and re-training strategy were selected as CBEN and cRT, respectively. \cref{ab_delta} shows the results. We choose the log. diff. strategy according to \cref{ab_delta}.


\begin{table}[t]
 \begin{minipage}[t]{0.233\textwidth}
 \centering  \caption{Ablation experiment of different re-sampling strategy on CIFAR-10-LT with .} \label{ab_rs}
 \vspace{-6pt}
 \setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{0.75}
 {\begin{tabular}{c c c}  \toprule[0.8pt]
  Sam.&  RT & Acc.(\%)  \\
  \hline
  \hline
  & &\-6pt]
  CB  &cRT & 82.43 \\
  & &  \-6pt]
  CBEN &cRT & \underline{\textbf{82.68}}  \\
  \bottomrule[0.8pt]
 \end{tabular}}
 \end{minipage}
 \hfill
 \begin{minipage}[t]{0.233\textwidth}
 \centering  \caption{Ablation experiment of different re-training strategies on CIFAR-10-LT with .} \label{ab_rt}
 \vspace{-6pt}
 \setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{0.75}
  {\begin{tabular}{c c c }
  \toprule[0.8pt]
  Sam. &RT & Acc.(\%)  \\
  \hline
  \hline
  & &\-6pt]
  CBEN & LWS   &82.25 \\
  & &  \-6pt]
  CBEN & cRT &\underline{\textbf{82.68}} \\
  \bottomrule[0.8pt]
 \end{tabular}}
 \end{minipage}
 \vspace{-6pt}
\end{table}

\subsubsection{Classifier Re-balance Strategies} \label{sec:sample_strategy}
We compared different strategies of data re-sampling and the classifier re-training to better analyze our proposed method. The re-sampling strategy (sam.) included: instance balance (IB)~\cite{decouple20}, class balance (CB)~\cite{decouple20}, class balance with effective number (EN)~\cite{cui2019class}, and our proposed class-based effective number (CBEN). For a fair comparison, the re-training strategies for all samplers were cRT. \cref{ab_rs} shows the effectiveness of CBEN.
For the selection of classifier re-training strategy, we first trained the backbone without any classifier re-training technology. Then, we fixed the representation and re-balance the classifier with learnable weight scaling (LWS)~\cite{decouple20}, -normalization (-nor.)~\cite{decouple20}, and cRT, respectively. \cref{ab_rt} presents the top-1 accuracy of CIFAR-10-LT with . We can observe that, even without any classifier re-training technique, our approach can still beat most state-of-the-arts including two-stage methods. For example, our GCL without classifier re-training suppresses BBN by . Further, cRT performs the best among the classifier re-training strategies, which improves the top-1 accuracy by .   From \cref{ab_rs} and \cref{ab_rt}, we can observe that IB+cRT degrades model performance, which indicates that training the classifier with IB may lead to classifier overfitting.

\section{Conclusion}
In this paper, we have found that softmax saturation reduces sample validity, which has different effects on head and tail classes. This implies that, from another perspective, softmax saturation can be utilized to automatically adjust the training sample validity of different classes. Subsequently, we have proposed the GCL. The tail class logits are set to relatively large cloud sizes to encourage more tail class samples to participate in training as well as leave large margins, which help obtain evenly distributed embedding space. The effectiveness of different classes is varied via GCL. Then, the simple but effective CBEN sampling strategy incorporated with cRT for classifier balancing has been proposed, which can further boost the model performance. Extensive experiments on various benchmark datasets have demonstrated that the proposed GCL has superior performance compared to the existing state-of-the-art methods.

\paragraph{Acknowledgment}This work was supported in part by NSFC/RGC JRS Grant: N\_HKBU214/21, ORP of Zhejiang Lab: 2021KB0AB03, GRF Grant: 12201321, NSFC Grants: 62002302 and 61672444, NSF of Fujian Province: 2020J01005, HKBU Grants: RC-FNRA-IG/18-19/SCI/03.
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{reference}
}

\end{document}
