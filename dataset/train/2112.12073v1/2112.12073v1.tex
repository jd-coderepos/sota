\documentclass[sigconf]{acmart-me}








\usepackage{booktabs} \usepackage{url}
\usepackage{color}
\usepackage{enumitem}
\usepackage{array}
\usepackage{algorithm,algorithmic}
\hyphenation{Media-Eval}


\setcopyright{rightsretained}



\acmDOI{}

\acmISBN{}

\acmConference[MediaEval'21]{Multimedia Evaluation Workshop}{December 13-15 2021}{Online} 
\acmYear{2021}
\copyrightyear{}

\acmPrice{}


\begin{document}
\title{Two Stream Network for Stroke Detection in Table Tennis}



\author{Anam Zahra, Pierre-Etienne Martin}
\affiliation{CCP Department, Max Planck Institute for Evolutionary Anthropology, D-04103 Leipzig, Germany}
\email{anam_zahra@eva.mpg.de, pierre_etienne_martin@eva.mpg.de}




\renewcommand{\shorttitle}{Sports Video Task}


\begin{abstract}
This paper presents a table tennis stroke detection method from videos. The method relies on a two-stream Convolutional Neural Network processing in parallel the RGB Stream and its computed optical flow. The method has been developed as part of the MediaEval 2021 benchmark for the Sport task. Our contribution did not outperform the provided baseline on the test set but has performed the best among the other participants with regard to the mAP metric.
\end{abstract}

\begin{teaserfigure}
    \includegraphics[width=\linewidth]{imgs/Arch_1_crop.pdf}
    \caption{Pipeline method for stroke detection from videos. Cuboids of RGB and optical flow are fed to the network and classified as stroke or non-stroke. The feature dimension is described as follow: $RGB channels \times temporal \times height \times width$.
    }
    \label{fig:model_fig}
\end{teaserfigure}




\maketitle

\section{Introduction}
\label{sec:intro}

With the advent of Convolutional Neural Networks (CNNs), especially after the success of AlexNet~\cite{krizhevsky2012imagenet}, object detection, localization, and classification from images and videos have greatly progressed~\cite{krizhevsky2012imagenet,girshick2015fast,wang2013action,carreira2017quo}.The development of computer vision methods has motivated broader applications in the academic world. Our team is currently working on egocentric recordings from children in kindergarten and at home. The analysis of these recordings shall give us an automatic overview of their interactions on a daily basis. We hope to link these interactions with their cognitive development and, thereby, better understand early child development. With our participation in the Sports Video Task~\cite{mediaeval/Martin21/task}, in the stroke detection subtask, we hope to perfect our knowledge in event detection and transpose it to our project.
\par
The diversity of applications and visual data in sport, makes sports video analysis attractive for researchers. Automated sport event detection and action classification, especially from low-resolution videos, are helpful for monitoring and training purposes. For example in~\cite{hughes2002use,lees2003science}, the authors automate the performance analysis for the training optimization of players. Similarly, Sports Video task at MediaEval 2021 benchmark aims at improving athlete performance and training experience through the first steps of stroke detection and classification from videos.
\par
Event detection in videos is the first step to many other hot-topics such as video summarizing~\cite{khan2015video}, automated semantic segmentation~\cite{ballan2011event} and action recognition~\cite{dhamsania2016survey,martin2020fine}. These methods may be used to build summary, selecting highlights, and assisting players in training sessions. One way to approach the problem of event detection in sports with balls, can be through ball detection and tracking. Several researchers have tried to get the 2D, and 3D ball trajectories in order to achieve so~\cite{tamaki2013reconstruction,myint2015tracking,myint2016tracking}.
\par
Inspired from~\cite{koch2015siamese,PeICIP19,PeICPR20,voeikov2020ttnet}, this method combines the optical flow and features learned from the RGB stream in order to detect a stroke in table tennis and assess its duration. This implementation is an extension of the baseline code provided by the Sport Task organizers~\cite{mediaeval/Martin21/baseline}.

\section{Approach}
\label{sec:approach}
Initially, we sought to use ball detection and tracking to perform stroke detection. The first implementation used the pretrained model TTNet~\cite{voeikov2020ttnet}. However, the model failed to adapt to the acquisition conditions from TTStroke-21~\cite{PeCBMI:2018}, on which the task is built upon, and no fine-tuning was possible since no ball coordinates are available in the provided annotations. Therefore we decided to train a model from scratch.
\par
In this section, we first present the preparation of the videos and then the model presenting the processed data. Both processes are depicted in Fig.~\ref{fig:model_fig}. Post processing is performed to form a final decision.


\subsection{Data Preparation}

In video content analysis, the motion of objects of interest between frames can be of significant interest in order to understand their evolution in space. As such, we decided to use optical flow as a modality to perform stroke detection. Inspired by~\cite{PeICIP19}, we decided to use DeepFlow method~\cite{deepflow} to compute the optical flow from consecutive frames. The optical flow is computed from frames resized to $320\times128$. This size was initially chosen to keep the ball at least two pixels big, as it has previously been done in~\cite{voeikov2020ttnet}. Both the RGB and optical flow frames are consecutively stacked in a tensor of length 75. As in~\cite{mediaeval/Martin21/baseline}, stroke detection is tackled as a classification problem with two classes: ``Stroke'' and ``Non-stroke''.


\subsection{Model}

As shown in figure~\ref{fig:model_fig}, our Two-Stream model is composed of two branches of the same length. Each branch is a succession of four blocks and each block is composed of a convolutional layer with $3\times3\times3$ filters, followed by a ReLU activation function, and a $2\times2\times2$ pooling layer. The output of each branch is then flattened and fed to a fully connected layer that outputs a feature vector of length $500$. Both feature vectors are then concatenated and fed into a final fully connected layer of length two to predict the ``Stroke'' and ``Non-storke'' classes. One branch takes RGB frames of the video and the other computed optical flow. The model is trained using a stochastic gradient descent method over $250$ epochs with a learning rate of $0.001$, a batch size of $10$, a weight decay of $0.005$, and a Nesterov momentum~\cite{sutskever2013importance} of $0.5$. The negative samples creation and input processing is the same as the baseline~\cite{mediaeval/Martin21/baseline}.

\subsection{Post Processing}

Our model classifies 75 consecutive frames. In order to create stroke segments over the whole video, we classify every 75 frames of the videos, which leads to applying a sliding window without overlap. If two consecutive segments are classified as stroke, the segments are fused to create only one stroke.


\section{Results and Analysis}
\label{sec:res}
The metrics for evaluating the detection performance are described in~\cite{mediaeval/Martin21/task}. Our approach reached a mean Average Precision (mAP) of $0.00124$ and a Global Intersection over Union (G-IoU) of $0.0700$. It falls behind the baseline which reaches respectively $0.0173$ and $0.144$. Our other attempts using early concatenation of the RGB and Optical Flow modalities - meaning an input of size $5\times320\times128$ in one branch model - or training method without shuffling of the data, reached even lesser performance.

\par

Nevertheless, from a classification point of view, and according to the Fig.~\ref{fig:train}, our model learned the stroke features and can perform reasonable results when stroke boundaries are known: $86.4\%$ of accuracy on the validation set after only 60 epochs. Which may indicates that the main failure is coming from the post processing method.

\begin{figure}
 \centering
 \includegraphics[width=.95\linewidth]{imgs/Train_crop.png}
 \caption{Training Process}
 \label{fig:train}
\end{figure}

Indeed, by looking at the stroke distribution across the different sets, see table~\ref{tab:hist}, we may notice how little the inferred stroke ratio is on the test set: 0.57 strokes for 1000 frames, whereas the stroke rate is 1.85 and 2.28 for 1000 frames in the training and validation sets. Furthermore, our post processing was not limited in term of stroke duration, leading to everlasting strokes: 4500 frames - meaning the fusions of 60 consecutive video segments. These
points indicate that our post-processing method can be improved.

\begin{table}
    \caption{Stroke concentration and duration in frame per set.}
    \begin{tabular}{|c|cccc|}
    \hline
    Set  & \# Strokes/1K frames & Mean  & Min  & Max \\
    \hline
    Train & 1.85    & $143.2\pm36.16$   & 52 & 296   \\
    Valid & 2.28    & $134.3\pm26.13$   & 72 & 292   \\
    Test  & 0.57    & $361.0\pm770.7$   & 75 & 4500  \\ 
    \hline
    \end{tabular}
    \label{tab:hist}
\end{table}



\par 

A better separation of the stroke may be reached by defining the event using ball tracking and the ball motion~\cite{calandre2021table}. This was our initial attempt, inspired by~\cite{voeikov2020ttnet}, but the available pretrained model considers a different point of view and was unable to adapt to the \texttt{TTStroke-21} videos point of view.




\section{Conclusion}
\label{sec:conclusion}

The Sports Video Task, and more specifically the stroke detection subtask, has proven to be challenging. Even if our implementation has learned to classify strokes, we were not able to outperform the baseline performance. We have underlined the importance of the post processing step through a stroke concentration and duration analysis. Furthermore, our failure to adapt a pretrained model on similar dataset, but with a different acquisition point of view, stresses the difficulty of the deep trained models to adapt to a change of scene, which is inherent to the fine-grained aspect of the classification subtask. As first time participants, we thought to tackle only one task to ease our submission. However, we now believe that a method tackling both the detection and classification may be the best for solving the Sport Video subtasks.


\bibliographystyle{ACM-Reference-Format}
\def\bibfont{\small} \bibliography{sigproc} 
\end{document}