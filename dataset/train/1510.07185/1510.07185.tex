\documentclass[final,onetabnum,onefignum,onethmnum]{siamltex}  
\usepackage{graphicx}

\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{graphicx}
\usepackage[figure,noend,vlined,algo2e]{algorithm2e}
\usepackage{url}

\renewcommand{\floatpagefraction}{0.1}



\hyphenation{ab-strac-tion}
\hyphenation{ab-strac-tions}
\hyphenation{ac-tion}
\hyphenation{ac-tiv-ity}
\hyphenation{ac-tiv-ities}
\hyphenation{add-ition}
\hyphenation{add-itions}
\hyphenation{add-ition-al}
\hyphenation{add-ition-al-ly}
\hyphenation{after}
\hyphenation{al-go-rithm}
\hyphenation{al-go-rithms}
\hyphenation{al-pha-bet}
\hyphenation{amort-ize}
\hyphenation{amort-ized}
\hyphenation{ana-lyse}
\hyphenation{an-aly-ses}
\hyphenation{an-aly-sis}
\hyphenation{an-tici-pate}
\hyphenation{an-tici-pated}
\hyphenation{an-tici-pa-tion}
\hyphenation{apply}
\hyphenation{are}
\hyphenation{area}
\hyphenation{areas}
\hyphenation{as-sum-ing}
\hyphenation{avail-able}
\hyphenation{avail-abil-ity}
\hyphenation{aver-age}
\hyphenation{author}
\hyphenation{authors}
\hyphenation{aux-il-iary}
\hyphenation{back-track}
\hyphenation{back-track-ing}
\hyphenation{base}
\hyphenation{based}
\hyphenation{battle-ground}
\hyphenation{be-tween}
\hyphenation{bucket}
\hyphenation{buckets}
\hyphenation{cap-acity}
\hyphenation{centre}
\hyphenation{chan-ges}
\hyphenation{char-ac-ter}
\hyphenation{coach-ing}
\hyphenation{com-pari-son}
\hyphenation{com-pari-sons}
\hyphenation{com-pati-bil-ity}
\hyphenation{com-pat-ible}
\hyphenation{com-pen-dium}
\hyphenation{com-pen-diums}
\hyphenation{com-pon-ent}
\hyphenation{com-pon-ents}
\hyphenation{con-se-qu-en-ce}
\hyphenation{con-sider}
\hyphenation{con-sider-ed}
\hyphenation{con-sider-ing}
\hyphenation{cor-res-pond}
\hyphenation{cor-res-pond-ence}
\hyphenation{cor-res-pond-ing}
\hyphenation{cor-res-pond-ing-ly}
\hyphenation{cor-res-ponds}
\hyphenation{cover}
\hyphenation{covers}
\hyphenation{covered}
\hyphenation{credit}
\hyphenation{credits}
\hyphenation{cur-ricu-lum}
\hyphenation{cycle}
\hyphenation{cycles}
\hyphenation{de-vel-op-ment}
\hyphenation{de-liver}
\hyphenation{de-liver-ing}
\hyphenation{des-troy}
\hyphenation{des-troys}
\hyphenation{de-struc-tive}
\hyphenation{de-ter-min-is-tic}
\hyphenation{dic-tion-ary}
\hyphenation{dis-order}
\hyphenation{docu-ment}
\hyphenation{double}
\hyphenation{doubling}
\hyphenation{drop-outs}
\hyphenation{eager}
\hyphenation{edu-ca-tion}
\hyphenation{elem-ent}
\hyphenation{elem-ents}
\hyphenation{entry}
\hyphenation{entries}
\hyphenation{event}
\hyphenation{ex-pect-ed}
\hyphenation{events}
\hyphenation{Eng-lish}
\hyphenation{evalu-ation}
\hyphenation{every-one}
\hyphenation{every}
\hyphenation{exact}
\hyphenation{exact-ly}
\hyphenation{exam-ine}
\hyphenation{exam-in-ation}
\hyphenation{ex-ecu-tive}
\hyphenation{ex-peri-ment}
\hyphenation{ex-peri-men-tal}
\hyphenation{ex-peri-ments}
\hyphenation{ex-po-nen-tial}
\hyphenation{ex-press}
\hyphenation{fa-cil-ity}
\hyphenation{fa-cil-ities}
\hyphenation{fa-cul-ty}
\hyphenation{for-ward}
\hyphenation{flex-ible}
\hyphenation{flexi-bil-ity}
\hyphenation{focus}
\hyphenation{func-tion}
\hyphenation{func-tions}
\hyphenation{geo-met-ric}
\hyphenation{geom-etry}
\hyphenation{gradu-ate}
\hyphenation{gradu-ated}
\hyphenation{gradu-ates}
\hyphenation{gradu-ation}
\hyphenation{greedy}
\hyphenation{hier-arch-ic-al}
\hyphenation{hier-archy}
\hyphenation{high-ly}
\hyphenation{ideal-ly}
\hyphenation{idiom-at-ic}
\hyphenation{im-pli-cit}
\hyphenation{im-pli-cit-ly}
\hyphenation{in-as-much}
\hyphenation{ini-tial-iza-tion}
\hyphenation{in-struc-tion}
\hyphenation{in-struct-or}
\hyphenation{in-struct-ors}
\hyphenation{inter-action}
\hyphenation{inter-nation-al}
\hyphenation{intra-net}
\hyphenation{intro-duce}
\hyphenation{intro-duc-ing}
\hyphenation{intro-duc-tion}
\hyphenation{ir-rele-vant}
\hyphenation{jump-ing}
\hyphenation{Ka-ta-jai-nen}
\hyphenation{know-led-ge}
\hyphenation{lan-guage}
\hyphenation{lan-guages}
\hyphenation{limit-ed}
\hyphenation{length}
\hyphenation{lengths}
\hyphenation{line}
\hyphenation{level}
\hyphenation{levels}
\hyphenation{lines}
\hyphenation{main-tain-ed}
\hyphenation{match-ing}
\hyphenation{matri-ces}
\hyphenation{mat-rix}
\hyphenation{mean-ing}
\hyphenation{mean-ings}
\hyphenation{method}
\hyphenation{methods}
\hyphenation{me-ticu-lous}
\hyphenation{me-ticu-lous-ly}
\hyphenation{me-ticu-lous-ness}
\hyphenation{micro-bench-mark}
\hyphenation{micro-bench-marks}
\hyphenation{micro-scop-ic}
\hyphenation{micro-sec-ond}
\hyphenation{micro-sec-onds}
\hyphenation{mis-pre-dic-tion}
\hyphenation{mode}
\hyphenation{mod-el-ling}
\hyphenation{modi-fi-ca-tion}
\hyphenation{modi-fi-ca-tions}
\hyphenation{modu-lar-ity}
\hyphenation{multi-level}
\hyphenation{navi-ga-tor}
\hyphenation{navi-ga-tors}
\hyphenation{ne-ces-sar-ily}
\hyphenation{ne-ces-sary}
\hyphenation{ob-ject-ives}
\hyphenation{ob-ject-ive}
\hyphenation{one}
\hyphenation{onto}
\hyphenation{op-er-ation}
\hyphenation{op-er-ations}
\hyphenation{order}
\hyphenation{others}
\hyphenation{other-wise}
\hyphenation{over-head}
\hyphenation{par-en-thesis}
\hyphenation{pack-age}
\hyphenation{pack-ages}
\hyphenation{par-en-theses}
\hyphenation{par-en-the-size}
\hyphenation{par-en-the-sized}
\hyphenation{par-en-the-siz-ing}
\hyphenation{people}
\hyphenation{per-form-ance}
\hyphenation{point-ing}
\hyphenation{point-ed}
\hyphenation{point-er}
\hyphenation{pos-sible}
\hyphenation{pos-sibly}
\hyphenation{power}
\hyphenation{power-ful}
\hyphenation{primi-tive}
\hyphenation{prob-lem}
\hyphenation{prob-lem-at-ic}
\hyphenation{prob-lems}
\hyphenation{pro-gram-ming}
\hyphenation{pro-jects}
\hyphenation{pro-po-si-ti-on}
\hyphenation{psych-olo-gist}
\hyphenation{psych-olo-gists}
\hyphenation{Psych-ology}
\hyphenation{psych-ology}
\hyphenation{psy-cho-logic-al}
\hyphenation{quali-fied}
\hyphenation{quali-fies}
\hyphenation{quali-fy}
\hyphenation{quali-fy-ing}
\hyphenation{quick-ly}
\hyphenation{real-iz-able}
\hyphenation{real-iza-tion}
\hyphenation{ref-er-ence}
\hyphenation{ref-er-ences}
\hyphenation{regu-lar}
\hyphenation{rele-vance}
\hyphenation{rele-vant}
\hyphenation{rele-vant-ly}
\hyphenation{re-quire}
\hyphenation{re-quired}
\hyphenation{re-quir-ing}
\hyphenation{re-sult}
\hyphenation{re-sults}
\hyphenation{re-search-ers}
\hyphenation{re-vers-ible}
\hyphenation{sat-el-lite}
\hyphenation{se-quence}
\hyphenation{ser-ious}
\hyphenation{ser-ious-ly}
\hyphenation{simi-lar}
\hyphenation{simi-lar-ly}
\hyphenation{single-ton}
\hyphenation{soft-ware}
\hyphenation{space}
\hyphenation{spa-cing}
\hyphenation{speci-fies}
\hyphenation{speci-fied}
\hyphenation{spe-cify}
\hyphenation{speci-fy-ing}
\hyphenation{stra-tegic}
\hyphenation{stra-tegic-al}
\hyphenation{Strou-strup}
\hyphenation{stu-di-es}
\hyphenation{sym-metry}
\hyphenation{table}
\hyphenation{tables}
\hyphenation{tech-nical}
\hyphenation{tech-niques}
\hyphenation{tek-nis-ka}
\hyphenation{that}
\hyphenation{the}
\hyphenation{them-selves}
\hyphenation{this}
\hyphenation{though}
\hyphenation{time}
\hyphenation{times}
\hyphenation{today}
\hyphenation{total}
\hyphenation{trans-ac-tion}
\hyphenation{trans-ac-tions}
\hyphenation{trees}
\hyphenation{Tu-ring}
\hyphenation{under-gradu-ate}
\hyphenation{under-gradu-ates}
\hyphenation{un-exam-ined}
\hyphenation{un-known}
\hyphenation{until}
\hyphenation{you}
\hyphenation{your}
\hyphenation{value}
\hyphenation{values}
\hyphenation{vio-late}
\hyphenation{vio-la-tion}
\hyphenation{vio-la-tions}
\hyphenation{visit}
\hyphenation{visits}
\hyphenation{where}
\hyphenation{with-in}
\hyphenation{work-shop}
\hyphenation{work-shops}
\hyphenation{wrote}
 
\newcommand{\Size}{\mbox{}}
\newcommand{\Minimum}{\mbox{}}
\newcommand{\Maximum}{\mbox{}}
\newcommand{\Insert}{\mbox{}}
\newcommand{\Extract}{\mbox{}}
\newcommand{\Search}{\mbox{}}
\newcommand{\Successor}{\mbox{}}
\newcommand{\Predecessor}{\mbox{}}

\newcommand{\floors}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceils}[1]{\left\lceil #1 \right\rceil}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\Access}{\mbox{}}
\newcommand{\Rank}{\mbox{}}
\newcommand{\Select}{\mbox{}}

\newcommand{\MinOne}{\mbox{}}
\newcommand{\MinTwo}{\mbox{}}
\newcommand{\MaxPile}{\mbox{}}
\newcommand{\FirstCandidate}{\mbox{}}

\newcommand{\ylatyhjaa}{\mbox{}}
\newcommand{\twodots}{\,.\,.\,}

\begin{document}

\title{Memory-Adjustable Navigation Piles\\{}with Applications to
  Sorting and Convex Hulls\thanks{Parts of this paper have appeared
  in preliminary form in \cite{AEK13} and \cite{DE14}}}

\author{Omar Darwish\thanks{Max-Planck Institute for Informatics, Saarbr\"{u}cken, Germany}
\and
Amr Elmasry\thanks{Department of Computer Engineering and Systems, Alexandria University, Egypt}
\and 
Jyrki Katajainen\thanks{Department of Computer Science, University of Copenhagen, Denmark}
}
 
\maketitle
\pagestyle{plain}
\pagenumbering{arabic}

\begin{abstract} 
We consider space-bounded computations on a random-access machine (RAM)
where the input is given on a read-only random-access medium, the
output is to be produced to a write-only sequential-access medium, and
the available workspace allows random reads and writes but is of
limited capacity.  The length of the input is  elements, the length
of the output is limited by the computation, and the capacity of the
workspace is  bits for some predetermined parameter .
We present a state-of-the-art priority queue---called an adjustable
navigation pile---for this restricted RAM model.  Under some
reasonable assumptions, our priority queue supports \Minimum{} and
\Insert{} in  worst-case time and \Extract{} in  worst-case time for any . (We use  as a
shorthand for .)
We show how to use this data structure to sort  elements and to
compute the convex hull of  points in the two-dimensional Euclidean
space in  worst-case time for any . 
Following a known lower bound for the space-time product of any branching program for finding unique elements, 
both our sorting and convex-hull algorithms are optimal.
The adjustable navigation pile has turned out to be useful when
designing other space-efficient algorithms, and we expect that it will find
its way to yet other applications.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\subsection{Problem Area}  Consider a sequential-access machine (Turing
machine) that has three tapes: input tape, output tape, and work
tape. In space-bounded computations, the input tape is read-only, the
output tape is write-only, and the aim is to limit the amount of space
used in the work tape. In this set-up, the theory of language
recognition and function computation requiring  bits of
working space for an input of size  is well established; people
talk about log-space programs \cite[Section 3.9.3]{Sav08} and classes
of problems that can be solved in log-space \cite[Section
  8.5.3]{Sav08}. Also, in this set-up, trade-offs between space and
time have been extensively studied \cite[Chapter 10]{Sav08}.  Although
one would seldom be forced to rely on a log-space program, it is still
theoretically interesting to know what can be accomplished when only a
logarithmic number of extra bits are available.

In this paper we reconsider the space-time trade-offs on a
random-access machine (RAM).  Analogous with the sequential-access
machine, we have separate storage media for read-only input, write-only
output, and read-write workspace which is of limited capacity.  Now,
however, both the input and workspace allow random access, but the
output is still to be produced sequentially.  Over the years, starting
by a seminal paper of Munro and Paterson \cite{MP80}---where a related
model was used, the space-time trade-offs in this \emph{restricted RAM
  model} have been studied for many problems including: sorting
\cite{Fre87,PR98}, selection \cite{EJKS14,Fre87}, and various
geometric problems \cite{ABBKMRS11,BKLSS15,DNR12,DNR15}.  The practical
motivation for some of the previous work has been the appearance of
special devices, where the size of working space is limited
(e.g.~mobile devices) and where writing is expensive (e.g.~flash
memories).

An algorithm or a data structure is said to be \emph{memory
  adjustable} if it uses  bits of working space for a given
parameter .  Naturally, we expect to use at least a constant number of words, 
so  is a lower bound for the space usage,  being the size
of the machine word in bits. Sorting is one of the few problems for
which the optimal space-time product has been settled: Beame showed
\cite{Bea91} (see also \cite[Theorem 10.13.8]{Sav08}) that
 is a lower bound, and Pagter and Rauhe showed
\cite{PR98} that an  worst-case running time is
achievable for any .
As for the convex-hull problem, Chan and Chen \cite{CC07} gave an
algorithm for computing the convex hull of a planar set of  points,
stored in a read-only array, that runs in  worst-case time for any .

\subsection{Model of Computation} 
We assume that the elements being manipulated are on a read-only array, 
and use  to denote the number of elements stored there.  
Observe that  does \emph{not} need to be known beforehand.  
The output is sent to a separate write-only stream, 
where the output printed cannot be read or rewritten. 

In addition to the input and output media, a limited
random-access workspace is available.  The data on this workspace is
manipulated wordwise as on the word RAM \cite{Hag98}. We assume that
the word size  is at least  bits and that the
processor is able to execute the same arithmetic, logical, and bitwise
operations as those supported by contemporary imperative programming
languages---like C \cite{KR88}.  It is a routine matter \cite[Section
  7.1.3]{Knu11} to store a bit vector of size  such that it
occupies  words and any string of at most  bits can be
accessed in  worst-case time.  That is, the \emph{time
  complexity} of an algorithm is proportional to the number of
primitive operations plus the number of element accesses and element
comparisons performed.
We do \emph{not} assume the availability of any powerful memory-allocation
routines. The workspace is an infinite array (of words), 
and the space used by an algorithm is the prefix of this array. 
Even though this prefix can have some unused zones, the length of
the whole prefix specifies the \emph{space complexity} of the algorithm. 

In our setting, the elements lie in a read-only array and the data
structure only constitutes references to these elements. We assume
that each of the elements appears in the data structure at most once,
and it is the user's responsibility to make sure that this is the
case. Also, all operations are position-based; the position of an
element can be specified by its index.  Since the
positions can be used to distinguish the elements, we implicitly
assume that the elements are distinct.


\subsection{Our Results} 
Let  be a read-only array and let  denote its size.  Consider
a priority queue  storing a subset of the elements in .  We use
 to denote the number of elements in . In the adjustable
set-up, a \emph{(min-)priority queue} is a data structure that
supports the following operations:
\begin{description}
\item[:] Return the index of the minimum element
  in , as long as .
\item[:] Insert   into , for some .
\item[:] Extract  from , for some .
\end{description}
A \emph{max-priority queue}, which is defined to support the operation
 instead of , is obtained from a min-priority
queue by reversing the comparison function used in element
comparisons.
In the non-adjustable set-up, any priority queue---like a binary heap
\cite{Wil64} or a queue of pennants \cite{CMP88} (that both operate
in-place)---could be used to store positions of the elements instead
of the elements themselves.

In the first part of the paper, we improve and simplify the
memory-adjustable priority queue presented by Pagter and Rauhe
\cite{PR98} by introducing a kindred data structure that we call an
\emph{adjustable navigation pile}.  Compared to the navigation piles of
\cite{KV03}, that require  bits, our adjustable variant can
achieve the same asymptotic run-time performance with only 
bits. (Another priority queue that uses  bits in addition
to the input was given in \cite{Elm03}.)  In Table
\ref{table:queues}, we compare the performance of the new data
structure to some of its competitors. Note that the stated bounds are
valid under some reasonable assumptions declared in Section \ref{sec:priority-queues}.

In the second part of the paper, we use the adjustable navigation pile
for sorting.  Our algorithm is simpler and more intuitive than that of
Pagter and Rauhe \cite{PR98}, and we also achieve the optimal
 running time for any .  
The algorithm is priority-queue sort like
heapsort \cite{Wil64}: Insert the  elements one by one into a
priority queue and extract the minimum from that priority queue 
times.

In the third part of the paper, we improve the Chan-Chen bound for the
convex-hull problem by introducing an algorithm that runs in 
 time for any .  To prove this
result, we augment the adjustable navigation pile with extra
information while still using  bits of workspace.

\begin{table}[tb!]
\caption{The performance of adjustable navigation piles and their
  competitors in the restricted RAM model;  is
  the size of the read-only input and  is an asymptotic target for
  the size of workspace in bits where .}
\label{table:queues}
\begin{center}
\begin{small}
\tabcolsep0.75ex
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Reference}\ylatyhjaa & \textbf{Space\strut} & \Minimum{} &
\Insert{} & \Extract{}\\ 
\hline
\cite{CMP88}\ylatyhjaa{} &  & 
 &  & \\
\cite{KV03} &  & 
 &  & \\
\cite{Fre87} &  &  &  & \\
\cite{PR98} &  &
 &  amortized & \\
{}[this paper] &  &
 &  & \\
\hline
\end{tabular}
\end{small}
\end{center}
\end{table}


\subsection{Related Models} The basic feature that distinguishes the
model we use from other related models is the capability of having
random access to the input data. In the context of sequential-access
machines, the input is on a tape that only allows single-pass
algorithms. The so-called \emph{streaming model} still enforces
sequential access, but allows multi-pass algorithms, and the goal is
to minimize the number of passes over the input when the size of the
random-access workspace is limited. Munro and Paterson \cite{MP80}
considered this model. For some problems, the restricted RAM model is
more powerful than the multi-pass streaming model. For example, for
the selection problem the lower bound known for the multi-pass
streaming model \cite{Cha10} can be bypassed in the restricted RAM
model \cite{EJKS14}.

In the \emph{in-place model}, the elements are to be stored at the
beginning of an infinite array, a constant number of additional
variables are allowed, and the elements may be swapped and
overwritten, but not modified, still keeping this compact
representation.  All the problems considered in this
paper---maintaining priority queues (see,
e.g.~\cite{CMP88,EEK15,Wil64}), sorting (see,
e.g.~\cite{EEK15,KPT96,Wil64}), and computing convex hulls
\cite{BIKMMT04}---have been studied in this classical setting.  In the
\emph{restore model}~\cite{CMR14}, the input elements may be
temporarily rearranged, and even modified, during a computation, but
at the end the input must be restored to its original state. Again,
sorting problems have been central in the exploration of the power of
this model (see \cite{CMR14,KP94}). In general, any reversible
algorithm would work well in the restore model, so reversible
computing is distantly related to this study.

\subsection{Bit Vectors with \Rank{} and \Select{} Support}
Given a bit vector  of  bits, of which  are  bits, consider the
following operations:

\begin{description}
\item[:] Return , i.e.~the bit at index  for
  some .
\item[:] Return the number of  bits among the bits
  , for some .
\item[:] Return the index of the th 1 bit, for some , i.e.~if
  , this means that  and . 
\end{description}
The operations  and  are
similarly defined considering the  bits instead of the  bits.

For a bit vector of size , our requirements for an acceptable
solution are that all the operations should run in
 worst-case time, the space used should be  bits,
and the construction of the data structure should take 
worst-case time.  The problem of extending a bit vector with
\Rank{}-\Select{} operations has been addressed in several papers
(see, for example, \cite{Jac89,Mun96,RRR07}).  Most of the known
solutions rely on the idea of dividing the bit vector into blocks,
precomputing \Rank{} and \Select{} values for some specific
positions, and calculating the other values on the fly using the
stored values, some precomputed tables, and bits in the bit vector
itself.  For example, the solution presented in \cite{RRR07} would be
suitable for our purposes; it requires 
bits, and  worst-case time per operation.

\section{Memory-Adjustable Priority Queues}
\label{sec:priority-queues}

\subsection{Assumptions}
In this section two memory-adjustable priority queues are described.
The first structure is a straightforward adaptation of a tournament tree
(also called a selection tree \cite[Section 5.4.1]{Knu73}) for
read-only data. For a parameter , it uses  words of workspace. 
The second structure is an improvement of a navigation pile \cite{KV03} for which 
the workspace is  bits, for , where  is the size of the 
read-only input. Both data structures can perform \Minimum{} and \Insert{} 
in  worst-case time and \Extract{} in  worst-case time.

When describing the data structures, we
make the following assumptions:
\begin{enumerate}
\item  is known beforehand.
\item Insertions and extractions are \emph{monotonic} such that,
  at any given point of time, there is a single element (if any)
  indicating that the elements smaller than or equal to it are outside
  the data structure.  We call such an element the \emph{latest
    output}, and say that an element is \emph{alive} if it is larger
  than the latest output. In particular, an extraction must remove the
  smallest element and an insertion must add an element that is larger
  than the latest output.
\item Insertions are \emph{sequential}---but insertions and
  extractions can be intermixed---such that the elements are inserted
  one by one from consecutive input entries starting from the first
  element stored in the read-only input.
\end{enumerate}
These assumptions are valid when a priority queue is used
for sorting. Actually, in sorting all insertions are executed before
extractions; a restriction that is not mandated by the data structure.
At the end of this section, we show how to get rid of these
assumptions. The first assumption is not critical. But, 
when relaxing the second assumption, the required size of workspace has to increase by  bits.
When relaxing the third assumption, the asymptotic worst-case running time of \Insert{}
will become the same as that required by \Extract{}.

\subsection{Adjustable Tournament Trees} 
For an integer , we use  as a
shorthand for . It suffices that ; even if  was larger, the
operations would not be asymptotically faster. The input array is
divided into  \emph{buckets} 
each containing  elements, except for the last bucket that may contain less.
A complete binary tree is built above these buckets. Each leaf of
this tree
\emph{covers} a single bucket and each branch covers the
buckets of the leaves in the subtree rooted at that branch. We
call the elements within the buckets covered by a node the
\emph{covered range} of this node. Note that the covered range of a
node is a sequence of elements stored in consecutive locations of the
input array. The data stored at each node is an
index specifying the position of the smallest alive element in the
covered range of that node.
 
An \emph{adjustable tournament tree} is an array of  positions (indices). 
To make the connection to our adjustable navigation piles clear, 
we store the indices in this array in breadth-first order as in a binary heap \cite{Wil64}. 
At each level, we start the indexing of the nodes  from
0. For the sake of simplicity, we maintain a \emph{header} that stores
the offsets to the beginning of each level (even though this
information is easy to calculate). For a node number , its
left child has number  at the level below, the right child has number  at the level below, 
and the parent has number  at the level above. When we know the current level and the index
of a node at that level, the information available at the header and the above formulas
are enough to get to a neighbouring node in constant time. In
Figure~\ref{fig:tournament}, we give an illustration of an adjustable tournament tree
for a set of  elements when .

\begin{figure}[tb!]
\begin{center}
\includegraphics[scale=0.41]{figure-tournament.pdf}
\end{center}

\caption{An adjustable tournament tree for a set of  elements
  when . Indices stored at the nodes are visualized as
  pointers. Only the smallest alive element in each bucket is shown.}
\label{fig:tournament}
\end{figure}

To support \Insert{} efficiently, we partition the data structure into
three components: tournament tree, submersion buffer, and
insertion buffer.  The \emph{submersion buffer} is the last full
bucket that is being submerged into the tournament tree. The
\emph{insertion buffer} is the bucket that embraces the new
elements. One or both of these buffers can be empty. The
idea is to insert the elements into a buffer and, when the buffer gets
full, submerge it into the tournament tree.  To make \Minimum{} 
straightforward, we recall the position of the overall
minimum of these three components.

In Figure~\ref{fig:submersion}, we describe in pseudo-code how a
bucket is submerged into the tournament tree at one go. In \Insert{},
the submersion is done incrementally. The process starts from a leaf
corresponding to the given bucket and proceeds in a bottom-up manner
to the root by following the implicit parent pointers.  We call the
path of nodes visited the \emph{updating path}. First, the index at
the leaf is set to point to the minimum of the bucket. Then, every
branch node on the updating path inherits the position of the smaller
of the two elements pointed to via its two children.  At the end the
root stores the index of the overall minimum among all alive
elements. Since, at some point, some buckets may have no alive elements, 
we use an unspecified constant  to indicate that the  
covered range of the node in question has no alive elements.

\begin{algorithm2e}[tb!]
\begin{minipage}{1.1\textwidth}
\SetStartEndCondition{ }{}{}\SetKwInput{Proc}{procedure}
\SetKwInput{Input}{input}
\SetKwInput{Data}{data}  
\SetKwFor{For}{for}{{\rm :}}{}
\SetKwIF{If}{ElseIf}{Else}{if}{{\rm :}}{else if}{else{\rm :}}{}\DontPrintSemicolon
\Proc{}
\Input{{\rm -}: index of the
  beginning of the bucket to be submerged\newline
{\rm -}: index of the minimum alive element
  within this bucket} 
\Data{: number of elements; :
  workspace target rounded to a power of 2\newline
: read-only array of elements\newline : array of indices from \newline : array of offsets, }

\;
\;
\;
\For {} {
  \;
  \;
  \;
  \;
  \If {} {
    \;
  }
  \ElseIf {} {
    \; 
  }
  \ElseIf {} {
    \;  
  }
  \Else {
    \; 
  }
  
}
\end{minipage}
\caption{Submersion when done at one go.}
\label{fig:submersion}
\end{algorithm2e}


\begin{algorithm2e}[tb!]
\SetStartEndCondition{ }{}{}\SetKwInput{Proc}{procedure}
\SetKwInput{Input}{input}
\SetKwInput{Assert}{assert}
\SetKwInput{Data}{data}  
\SetKwIF{If}{ElseIf}{Else}{if}{{\rm :}}{else if}{else{\rm :}}{}\DontPrintSemicolon
\Proc{}
\Input{: index of an element to be inserted into }
\Data{: number of elements; :
  workspace target rounded to a power of 2\newline
: read-only array of elements\newline : array of indices from \newline 
: index of the beginning of the insertion buffer\newline
: number of elements in the
  insertion buffer\newline
: index of the minimum of the
  insertion buffer}

\Assert{}

\;
\If {} {
  \;
}
\If {} {
  \;
}

\If {} {
  \;
  \;
  \;
  \;
  \;
  \;
}
execute  steps of the submersion process, if one is in progress

\caption{Inserting an element into an adjustable tournament tree.}
\label{fig:insert}
\end{algorithm2e}

The pseudo-code of \Insert{} is given in Figure~\ref{fig:insert}.  At
first, the next element from the input array becomes part of the
insertion buffer. If the new element is smaller than the buffer
minimum and/or the overall minimum, the positions of these minima are
updated.  Once the insertion buffer becomes full, the submersion
buffer must have been already submerged into the tournament tree.  At
this point, we treat the insertion buffer as the new submersion buffer
and start a new incremental submersion process that recomputes the
indices at the nodes on the updating path bottom-up, one by one.  As
long as the submersion is not finished, each \Insert{} carries out a
constant amount of the submersion work. Since the work needed to
update this path is , which is  when , the process
terminates before the insertion buffer becomes again full.  
Clearly, \Insert{} takes  worst-case time.

\begin{algorithm2e}[tb!]
\begin{minipage}{1.1\textwidth}
\SetStartEndCondition{ }{}{}\SetKwInput{Proc}{procedure}
\SetKwInput{Input}{input}
\SetKwInput{Assert}{assert}
\SetKwInput{Data}{data}  
\SetKwFor{For}{for}{{\rm :}}{}
\SetKwIF{If}{ElseIf}{Else}{if}{{\rm :}}{else if}{else{\rm :}}{}\DontPrintSemicolon
\Proc{}
\Input{: index of an element to be extracted from }
\Data{: number of elements; :
  workspace target rounded to a power of 2\newline
: read-only array of elements\newline : array of indices from \newline 
: index of the beginning of the insertion buffer\newline
: index of the minimum
of the
  insertion buffer\newline
: index of the beginning of the submersion buffer\newline
: index of the minimum
of the
  submersion buffer}

\;
\;
\;
\For {} {
  \If {} {
       \;
  }
}
\If {} {
  \;
}

\ElseIf {} {
  \;
  \;
\;
}
\Else {

}
\;
\For {} {
  \If {} {
      \;
  }
}
\end{minipage}

\caption{Extracting an element from an adjustable tournament tree.}
\label{fig:extract}
\end{algorithm2e}

In \Extract{}, there are three cases depending on whether the bucket
that contains the element to be extracted is one of the buffers or
is covered by a node of the tournament tree.  However, these cases are quite
similar. For a pseudo-code description, see Figure~\ref{fig:extract}.
To begin with, the latest output is set up to date.  The bucket index
of the given element can be determined by simple calculations.
Because of the monotonicity assumption the smallest alive element 
is to be extracted, so the bucket must be scanned to find
its new minimum.  If the current bucket is the insertion buffer, it is
just enough to update the position of its minimum.  If the current
bucket is the submersion buffer, the submersion process is completed
by recomputing the indices at the nodes on the updating path covering the
submersion buffer. Hereafter the submersion buffer ceases to exist.
If the current bucket is covered by the tournament tree, it is
necessary to recompute the indices at nodes on the updating path covering
the current bucket. At the end, the position of the overall minimum is
to be updated.  It is the scanning of a bucket that makes this
operation expensive: The worst-case running time is ,
which is  when .

\begin{figure}[tb!]
\begin{center}
\centerline{\includegraphics[scale=0.41]{figure-navigation.pdf}}
\end{center}

\vspace*{-0.5cm}
\caption{An adjustable navigation pile corresponding to the adjustable
  tournament tree in \mbox{Figure}~\ref{fig:tournament}. In this snapshot all
  buckets contain one or more alive elements so the bit vector---that
  is not shown---indicating their existence contains just 1 bits.}
\label{fig:pile}
\end{figure}

\subsection{Adjustable Navigation Piles} 
In brief, an \emph{adjustable navigation pile} is a compact
representation of an adjustable tournament tree.  The main differences
are as follows (compare Figure~\ref{fig:tournament} and
Figure~\ref{fig:pile}):
\begin{enumerate}
\item A bit vector of size  is used to indicate
  whether or not the buckets covered by each node contain any alive
  elements. As for the data in the tournament tree, in this bit vector the
  bits of the nodes are stored in breadth-first order. This bit vector
  can be used to emulate the constant .
\item Only branch nodes, i.e.~nodes whose heights are larger than
  0, store some additional information about the position of the
  smallest alive element in the covered range of each of these nodes. In the
  complete binary tree built above the buckets, the number of branch nodes
  is .
\item The navigation information is stored in a bit vector of
  size  (an explanation will follow shortly).  To save space,
  at the bottom of the tree, the position of the smallest alive
  element is only specified approximately. Here the details of our
  construction differ from those used in the navigation piles of
  \cite{KV03} and their precursors \cite{PR98}, although the
  techniques used are similar.
\end{enumerate}

A branch node of height  covers
 buckets. As in a navigation pile \cite{KV03}, due to scarcity of bits, 
the bucket index is \emph{relative} within the covered range. We use  bits to
specify in which bucket the smallest alive element is.  
A significant new ingredient, borrowed from \cite{PR98}, is the concept of a
\emph{quantile}. For a branch node of height
, every covered bucket is divided into  quantiles, and we
store additional  bits to specify in which quantile the smallest
alive element is.  We call this quantile the {\it active quantile} of the node. 
A quantile contains 
elements, except that the last quantile can possibly be smaller.
Thus, we use  bits per node; but if , we
only use  bits (since this is enough to
specify the exact position of the smallest alive element).  To sum up,
since there are  nodes of height  and since at each
node we store  bits, the total number of
navigation bits is bounded by


The navigation bits are stored in a bit vector in breadth-first order.
As before, we maintain a \emph{header} giving the position of the first bit
at each level. The space needed by the header is 
bits. Inside each level, the navigation information is stored compactly
side by side, and the nodes are numbered at each level starting from
.  Since the length of the navigation bits is fixed for all nodes
at the same level, using the height and the index of a node, it is
easy to calculate the positions where the navigation bits of that node
are stored.

Let us now consider how to access the desired quantile for a branch
node. The procedure is summarized in pseudo-code form
in Figure~\ref{fig:quantile}. Let the branch node be at index  within
its own level, and assume that its height is . The first element of
the covered range of this branch node is in position .  The first  bits of the navigation information
give the desired bucket inside the covered range; let the relative
bucket index be , so we have to
go  positions forward. The second  bits
of the navigation information give the desired quantile inside that
bucket; let this quantile be , so we have to proceed another
 positions forward before we
reach the beginning of the desired quantile. Obviously, these
calculations can be carried out in constant time.

\begin{algorithm2e}[tb!]
\SetStartEndCondition{ }{}{}\SetKwInput{Proc}{procedure}
\SetKwInput{Input}{input}
\SetKwInput{Assert}{assert}
\SetKwInput{Data}{data}  
\SetKwFor{For}{for}{{\rm :}}{}
\SetKwIF{If}{ElseIf}{Else}{if}{{\rm :}}{else if}{else{\rm :}}{}\DontPrintSemicolon
\Proc{}
\Input{: index of a branch node at its own level; : height of that node}
\Data{: number of elements; :
  workspace target rounded to a power of 2\newline
 : array of navigation bits\newline : array of offsets}

\;
\; 
\If {} {
  \;
  \;
   \textbf{return} \;
}

\;

\;

\;

\;

\;

\;

\;

   \textbf{return} \;

\caption{Calculating the beginning and size of the active quantile of a node.}
\label{fig:quantile}	
\end{algorithm2e}

The priority-queue operations are implemented in a similar way as
for an adjustable tournament tree. To facilitate constant-time
\Minimum{}, we keep in memory the index of the overall minimum (since
the root of an adjustable navigation pile does not necessarily specify
a single element).  In \Insert{} and \Extract{}, the subtle difference
is dealing with quantiles. When updating the
navigation information for a branch node, at the bottom of an
adjustable navigation pile, we do not have direct access to the
minimum among the alive elements covered. Instead, we have to scan the
elements in the quantiles specified for the sibling nodes of the nodes
along the updating path.  Later on, a quantile is said to be
\emph{active} if it contains the minimum among the alive elements
covered by a node.  After updating the navigation bits of a node ,
we locate its parent node  and its sibling node . The navigation bits of
 are used to locate its active quantile.  This quantile is
scanned, and the minimum of the alive elements is found and compared
with the minimum of the alive elements covered by . From the bucket
index and the position of the smaller of the two elements, the
navigation bits of  are then calculated and accordingly
updated. If the active quantile of  has only one element, the
position of this single element can be stored as such.

The key point is that for a node of height  the size of the active
quantile is at most , so the total work
done in the scans of the quantiles of the siblings along the updating
path is proportional to , which is .  It follows that the
asymptotic efficiency of the priority-queue operations is the same as for an
adjustable tournament tree.

\subsection{Getting Rid of the Assumptions}  So far we have consciously
ignored the fact that the sizes of the buckets depend on , 
and that we might not know this value beforehand. The
standard way of handling this is to rely on global
rebuilding \cite[Chapter V]{Ove83}. We use an estimate  and initially set .
We build two data structures, one for  and another for . The first structure
is used to perform the priority-queue operations, but insertions and
extractions  are mirrored in the second structure (if the extracted element exists there).  
When the structure for  becomes too small, we dismiss the smaller structure
in use, double , and in accordance start building a new structure of size . 
We should speed up the construction of the new structure by inserting up to two
alive elements into it at a time, instead of only one. This guarantees
that the new structure will be ready for use before the first one
is dismissed.  Even though global rebuilding makes the
construction more complicated, the time and space
bounds remain asymptotically the same.


Since we have random-access capability to the read-only input, it is
not necessary that elements are inserted by visiting the input
sequentially, but insertions should still be monotonic. If
this is the case, in connection with each \Insert{}, we have to fix
the information related to the current bucket as in \Extract{}. That
is, we have to find the smallest alive element of the bucket and, if
the inserted element is smaller than the current minimum at that
bucket, update the navigation information on the updating path covering
that bucket. The worst-case cost of \Insert{} then becomes
the same as that of \Extract{}, i.e.~.

In some applications, insertions and extractions may 
not be monotonic. To handle this situation, we have to allocate one bit per array entry
( bits in total), indicating whether the corresponding element is alive or not.  

\section{Sorting}
\label{sec:sorting}

\subsection{The Pagter-Rauhe Algorithm}
Let us turn our attention to sorting. Given  elements in
a read-only array, the task is to print the elements in
non-decreasing order. Assume that the asymptotic workspace target is .
In the basic setting, Pagter and Rauhe \cite{PR98} proved that
the running time of their sorting algorithm is 
using  bits of workspace. Lagging behind the optimal bound for
the space-time product by a logarithmic factor when , they suggested using their memory-adjustable data structure in
Frederickson's adjustable binary heap \cite{Fre87} to handle
subproblems of size  using  bits for each.  In
accordance, by combining the two data structures, they
achieve an optimal  running time for sorting
when .  In our treatment we avoid the
complication of plugging two data structures together.

\subsection{Priority-Queue Sort} 
To sort the elements, we create an empty adjustable navigation
pile, insert the elements into this pile by scanning the read-only
array from beginning to end, and then repeatedly extract the minimum of the
remaining elements from the pile. The pseudo-code in
Figure~\ref{sorting-algorithm} implements this algorithm.

\begin{algorithm2e}[tb!]
\SetStartEndCondition{ }{}{}\SetKwInput{Proc}{procedure}
\SetKwInput{Input}{input}  
\SetKwFor{For}{for}{{\rm :}}{}
\SetKwFor{While}{while}{{\rm :}}{}
\SetKwIF{If}{ElseIf}{Else}{if}{{\rm :}}{else if}{else{\rm :}}{}\DontPrintSemicolon
\Proc{{\rm -}{\rm -}}
\Input{: read-only array of  elements; : workspace target}

{\rm -}\;
\For {} {
  \;
}
\While {} {
  \;
  \;
  \;
}
\caption{Priority-queue sort; the position of an
  element is specified by its index.}
\label{sorting-algorithm}	
\end{algorithm2e}

\subsection{Analysis}  From the bounds derived for the priority queue,
the asymptotic performance can be directly deduced: The worst-case
running time is  and the size of workspace is
 bits, where . It is also easy to count the number of element
comparisons performed during the execution of the algorithm. When
inserting the  elements into the data structure,  element
comparisons are performed. We can assume that after these insertions,
the buffers are submerged into the main structure. In each \Extract{}
we have to find the minimum of a single bucket which requires at most
 element comparisons. In addition, we have to
update a single path in the complete binary tree. At each level, the
minimum below the current node is already known and we have to scan the
quantiles of the sibling nodes. During the path update, we have to perform
at most  element comparisons.
Hence, the total number of element comparisons performed is bounded by , 
which is  since . 

\section{Augmenting the Adjustable Navigation Pile}
\label{aug}

\subsection{Motivation for Augmentation} 
In our algorithm for computing the convex hull of a set of planar points (see Section~\ref{ch}), 
insertions are neither monotonic nor sequential and extractions are not monotonic. 
Nevertheless, we want to keep the algorithm memory adjustable and use  bits of extra
space for .  To limit the size of working space, our
solution is to work with a subset of the input constituting at most
 elements at a time. In more details, the input is processed in a
number of rounds, where in each round we shall have two filter
elements and only elements whose values are between these filters are
to be inserted or extracted from the adjustable navigation pile. We
refer to these  elements as the \emph{candidates}.  Note that
the candidates need not be contiguous in the read-only input array.  
In every round, we use a vector of  bits, one bit per candidate, 
to indicate whether each of these candidates is still alive or has been deleted.
Subsequently, we need to map the indices of the input array to
indices in the range . To be able to do that,
the adjustable navigation pile needs to be augmented with additional
information. We shall refer to the adjustable navigation pile explained in
Section \ref{sec:priority-queues} as the \emph{unaugmented navigation
  pile} to distinguish it from the augmented version to be described
in this section.  Next, we introduce the additional structures used in
the augmentation. Then, we explain how to update and utilize these
structures in the priority-queue operations.  A schematic view
illustrating the components of an augmented navigation pile is given in
Figure~\ref{ANP}.

\begin{figure}[t!]
\centerline{\includegraphics[scale=0.41]{figure-augmentation.pdf}}
\caption{A snapshot of an augmented navigation pile that has 
  and . Only elements within the range of the filters are
  shown. The snapshot is taken considering the given 
  vector. For each node, the navigation bits are displayed in
  left-to-right order as follows: relative bucket index, quantile
  index,  index, and . As
  an example, we can see that node  refers to the candidate  as
  its minimum alive element. Hence, its relative bucket index is ,
  which refers to bucket . Bucket  is partitioned into two
  quantiles in the view of node , where the candidate  lies in
  the second quantile . So, the quantile index for node  is
  .  in  is dedicated to
  bucket . At node ,  is partitioned
  into two parts.  Since the candidate  is 
  , and as this candidate is in the
  first part of , the start.part index of node
   is set to .  The part that contains 
  is referred to as .}	
\label{ANP}
\vspace{-0.5cm}
\end{figure}

\subsection{Additional Structures} 
For the sake of simplicity, assume that the required workspace target  is
a power of 2.  We augment the adjustable navigation pile with the
following data structures:
\begin{itemize}
\item A bit vector  of size  is used to denote
  whether each candidate is currently alive or not. The order of the
  candidates in this vector is identical to their order in the read-only
  array. The  vector is dynamically updated by \Insert{}
  and \Extract{} operations.

\item A bit vector  of size ---corresponding to the same
  elements, in the same order, as ---is
  used to denote whether a candidate is the first, among other
  candidates, of an active quantile or not. We refer to the candidate corresponding to the first entry for a  
  candidate from quantile  as .
	(Note that a candidate may simultaneously be the first candidate of more than one active quantile.) 
	The  vector is also dynamic, as every
  \Insert{} and \Extract{} may change it.  Every bucket will possibly map
  to a part of  and , which obviously has at
  most  entries.  We refer to the part of 
  corresponding to bucket  as .

\item A static bit vector  is used to store the number
  of candidates contained in each bucket. We encode these counts in
  unary, using a  bit to mark the border between every two
  consecutive buckets. Since we are dealing with at most 
  candidates, the vector contains at most  ones; and since we have
  exactly  buckets, it contains  zeros. The 
  vector should efficiently support \Rank{} and \Select{} queries. The
  bit vector and the accompanying rank-select structures thus consume
   bits.  The objective is to efficiently locate the first
  entry of any bucket in  and  using the index of the bucket. 
	Assume the first bucket has index . Let  be the index of the bucket whose first
  entry in  and  is to be located. Then, .\Select is the index of the last  bit
  preceding bucket  in .  It follows that 
  is the number of candidates lying in the buckets , , \dots,
  , which precede the first entry of bucket  in
   and . The size (number of bits)
  of  is calculated as 
	.\Select  .\Select.

\item For every node of height , associated with bucket  and
  quantile ,  is further divided into 
  subparts (or less, if its size is less than ). Another  bits
  will be stored in every node of height  to indicate in which
  subpart  lies. We refer to
  this subpart as . Since the size of
   is at most , the size of
   is bounded from above by , which is the size of a quantile.

\item For every branch node of height , associated with bucket  and
  quantile , an additional  bits
  will be used. These bits encode the number of first candidates
  of other active quantiles that exist in the same subpart
   before , 
  i.e.~the number of ones in 
  preceding the one representing . 
  Let us refer to the value of this number as .  
	The reason we need only  bits to store this
  information for each node of height  is that only quantiles tied
  to nodes (whose heights are less than ) on one and only one path
  from a leaf node to the node associate with  can have their first candidates
  before  in .
\end{itemize}

It directly follows, using simple calculations, that the space
complexity of the augmented navigation pile is still  bits. 

To keep the time complexity for \Insert{} and \Extract{} in , we need to know, in an efficient way, if a given candidate that
belongs to an active quantile is alive or not.  Starting with the
index of a candidate in the read-only array, we want to find, without
altering the time bounds, the index of the corresponding bit in .
Given a node , associated with bucket  and quantile ,
the index of  in
 is to be located. Using the  vector
and the navigation bits of node , we can easily locate
, where the entry we are searching for
lies. We also get the value  from the
navigation bits of node . As previously stated, the size of
 is at most the size of quantile . While
visiting node , we shall be scanning quantile  anyhow. A scan of
 would then not alter the worst-case
asymptotic time complexity. We scan  to find
the th one bit, the index of the entry at which we find this bit is
the index of  in .

We always access the elements of a quantile
sequentially. To locate the corresponding elements in
, we start at the first entry of the quantile in
 as explained above. While
scanning the quantile, we repeatedly check the elements one after
another. If the next element is a candidate (lying in the range of the
filters), we increase the current index to the next entry in
 to correspond to this candidate.  The same procedure
can be applied on buckets. We get the first entry of the bucket
in  using the  vector, and then move
sequentially on the bucket and on , increasing the
 index whenever we encounter the next candidate in the bucket.

\subsection{Operations} 
We next explain how \Insert{} and \Extract{} can be performed in our
augmented navigation pile in   worst-case time per operation.

We first find the bucket in which the element to be inserted or extracted
lies; this can be done with simple calculations in constant time once
we have the array index of the element. Let the index of this bucket
be . Using the  vector, we get the first entry of
this bucket in  as explained earlier, and move
sequentially on bucket  and . We can then get the
indices of the candidates lying in this bucket within
, and consequently know whether each of these
candidates is currently alive or not. After knowing the index of the
element to be inserted or extracted in , we should set
the corresponding bit to 1 or 0 respectively. Also, while scanning the bucket, we
would know if this element is the minimum in the bucket or not. If the
minimum alive element in bucket  has changed due to the current operation,
the following updates need to be done.

As in an unaugmented navigation pile, the information in the nodes along the updating path is to be fixed bottom up. 
The update will work as in the unaugmented navigation pile, where we scan the quantiles associated with the nodes lying on or hanging from the updating path. However, here we also want to know whether each of the candidates in these quantiles is alive or not.
Before accessing a quantile, we get its first entry in
; this can be done as previously described. 
Then, we simultaneously scan both the quantile and the corresponding subpart in .

Next, we show how to update the  vector.  Suppose that
we are to handle a node , associated with quantile , on the updating path, 
knowing that its child has just been handled.  If the index of 
 is different from the index of the first 
candidate of each of the two quantiles associated with the two children
of node , we reset the bit for  
in  to 0.  Note that we do not reset that bit
to 0 if it is the first entry of the quantile of a child of node 
in .  Alternatively, this bit may be temporarily reset
to 0 in the previous step and then again set to 1 within the upcoming
step.  Assume that the child of node  that is associated with the quantile
that has the smaller element of the two children of  associated with quantile
.  Now the quantile associated with node  should be either the
first half or the second half of quantile .  If it is the first
half, then the corresponding entry in  must have been
already set to 1 before.  Else, we move sequentially on quantile 
and  till we reach the first candidate in the second
half of quantile , and set its corresponding entry in  to 1. 
The above procedure is repeated for every node on the updating path.

For the nodes along the updating path, we show next how the additional
bits in our augmented navigation pile will be updated.  Consider a
node  that is associated with bucket  and quantile  after the update.
While handling node , as explained earlier, we are able to know the
entry in  for  as
well as the size of .  Knowing these values,
it is easy to update the bits indicating  in
node . Also, after getting these bits, we loop on
 to count the number of 1 bits in this subpart
preceding the entry for  in
, and store this value for node  in .

It is obvious that the update will be performed on at most  nodes on
the updating path. Also, looping on the quantiles and the
corresponding parts of  for the nodes on the updating
path would sum up to . So the time complexity for the update is in , as claimed.

The following lemma summarizes the functionality of our data structure.


\begin{lemma}
Assume that we are given a read-only array of  elements, and two
filters that enclose at most  of these elements between their
values.  Using  bits of workspace, after spending 
worst-case time on building the augmented structure, \Insert{} and
\Extract{} can be applied to any element of the array whose value is
between the filters in  worst-case time per operation.
\end{lemma}


\section{Convex Hulls}
\label{ch}

\subsection{The Chan-Chen Algorithm}

Consider now the problem of computing the convex hull of a set
of  points in the plane given in a read-only array . Without loss of generality,
we can assume that the points are unique such that no two
points have the same -coordinate or -coordinate. The task is to
print the points on the convex hull in the clockwise order of their
appearance on the hull, starting from some arbitrary point. As is
standard, it is enough to show how to compute the upper hull of the
point set; the lower hull can be computed in a symmetric manner.
We assume the availability of the standard geometric primitive that
tells whether or not there is a right turn on point  when going from
point  to point  via ; we denote this predicate as
.

\begin{algorithm2e}[tb!]
\begin{minipage}{1.0\textwidth}
\SetStartEndCondition{ }{}{}
\SetKwInput{Proc}{procedure}
\SetKwInput{Input}{input}  
\SetKwFor{For}{for}{{\rm :}}{}
\SetKwFor{While}{while}{{\rm :}}{}
\SetKwIF{If}{ElseIf}{Else}{if}{{\rm :}}{else if}{else{\rm :}}{}\DontPrintSemicolon
\Proc{{\rm -}{\rm -}}
\Input{: read-only array of  points; : workspace target}

\;
\For {} {
  \If {-coordinate  -coordinate} {
     \;
  }
}  

\While {} { 

  let  be the vertical slab containing  and the next  points (if
  possible)\;
~~~~on the right of and closest to the wall determined by \; 
  let  be the indices of the  points in
  , sorted by -coordinate\; 
  use Graham's scan to compute the upper hull of these points\;
  let  be the indices of the points
  on this hull ~~\textbf{//} \;
   ~~\textbf{//} right endpoint of the hull edge crossing the right wall of \;
  \For {each point  to the right of the wall determined by } {
    \If { \rm\textbf{and} } {
       \textbf{continue}
    }
    \While {} {
      \;
    }
      \;
  }
  \For {} {
    \;
  }
  \;
}
\end{minipage}

\caption{High-level description of the Chan-Chen algorithm.\label{convex-hull-algorithm}}
\end{algorithm2e}

A high-level description of the algorithm by Chan and Chen \cite{CC07}
is given in Figure~\ref{convex-hull-algorithm}. Our algorithm is
similar but the details are different.  When the working space
of  bits is available, they set the space
parameter  to  and use  indices to recall
the points being processed. The algorithm performs  rounds and handles the  points with the next smallest 
-coordinates in each round; these points form a vertical slab
. In each round, the algorithm starts with a known hull vertex
 and computes the part of the upper hull for the points of
 starting from point  and ending at the left endpoint
of the hull edge crossing the right wall of .

When finding the  points with the next smallest -coordinates
among the remaining points, the algorithm uses space for 
indices and maintains in the first half the  indices of the points with the
smallest -coordinates among the points examined so far. Each time,
the second half is refilled with another  indices from the
unexamined portion, the median of the -coordinates of the 
recorded points is found, and these points are partitioned around this
median. This is repeated until all the points are examined, leaving
the  points within the slab . The upper hull of these
points can then be constructed using any
of the known  convex-hull algorithms \cite[Chapter~3]{PS85}; a natural choice is to use Graham's scan since the rest of
the algorithm follows the same elimination strategy. The Chan-Chen
algorithm eliminates the points that are not on the convex hull by traversing
the tentative hull chain in reverse order, the points with the larger 
-coordinates first. This procedure is done through finding the hull
edge crossing the right wall of  by performing a pass over the
remaining points (those to the right of ). Suppose a point
 from the remaining points is currently being inspected, by
imitating Graham's scan, the point  is tentatively added to the
hull if it is above the tentative hull edge found so far crossing the
right wall of . Also, adding  to the hull might require
removing some points from the chain in Graham-scan fashion. In the description
given in Figure~\ref{convex-hull-algorithm}, we do not add any new points
to the tentative hull chain. We just keep  as the point representing 
the right endpoint of the hull edge crossing the right wall of  and we 
update  accordingly. Hence, points are only removed from the hull chain 
during the elimination process. At the end of the pass, the leftover points on the chain 
are indeed on the convex hull and are accordingly reported.

Finding the next  points with the smallest -coordinates
requires  time, computing the upper hull for  points requires  time, and pruning the tentative hull chain from points not on the
upper hull requires  time. Since there are  rounds, the
algorithm runs in  worst-case time,
which is .

\subsection{Our Algorithm}

Our convex-hull algorithm uses three navigation piles: a
(max-)augmented navigation pile that we call \MaxPile{}, and two
(min-)unaugmented navigation piles that we call \MinOne{} and
\MinTwo{}.  
Without loss of generality, we assume that the orientation (either max or min) 
of the navigation piles is with respect to the -coordinates of the
points. The algorithm works as follows:

\begin{enumerate}

\item
Insert all the  points in both \MinOne{} and \MinTwo{}.
\item
Let  be the index of the point with the minimum -coordinate, and  if the input set is empty.
\item
\textbf{while} :

\begin{enumerate}

\item Extract the minimum  points, one by one, from \MinOne{} (or
  until ), and keep track of the first and last points. 
  These two values will be used as filters for \MaxPile{}; let us call them 
   and . These  points will be the candidates considered in this round, 
  determining the slab .

\item Reinitialize an empty \MaxPile{} using the current candidates and filters, 
  without actually inserting the points. Reinitialize the
   vector by scanning the  points, bucket by
  bucket. Then, build the \Rank{} and \Select{} structures for the
   vector.

\item
\label{critical-piece}
 Construct the upper hull for the current  points. This can be
  done by using a space-efficient implementation of Graham's scan. For
  example, the in-place variant described in \cite{BIKMMT04} could be
  modified to use a bit vector of alive elements, instead of swapping
  the input elements. In this computation, \MaxPile{} and \MinTwo{}
  can be deployed as follows.

\begin{enumerate}
\item Extract the two points with the minimum -coordinates 
from \MinTwo{} and insert both of them into \MaxPile{}.
\item \textbf{repeat}  times (or until ): 
\begin{enumerate}
\item Extract the minimum point from \MinTwo{}; let its index be .
\item \textbf{while}  
  \textbf{and} \textbf{not} (next to maximum of \MaxPile{},  maximum of \MaxPile{}, :\newline \hspace*{1.25em}repeatedly extract the maximum point from \MaxPile{}.
\item Insert the point  into \MaxPile{}.
\end{enumerate}
\end{enumerate}

At this point, the alive points in \MaxPile{} form a tentative upper hull.  

\item The goal here is to eliminate the points that are not really on the hull
among the points in \MaxPile{} forming the tentative hull chain. We do so by 
computing the hull edge crossing the right wall of .

\begin{enumerate}
\item Set  to , which represents the right endpoint of the hull edge crossing the right wall of .
\item \textbf{for} every point with index  whose -coordinate is
  greater than :

\begin{enumerate}
\item If  \textbf{and} (maximum of \MaxPile{}, , , \textbf{continue}.
\item \textbf{while}  \textbf{and} \textbf{not} (next to maximum of \MaxPile{},  maximum of \MaxPile{},
):\\ \hspace*{1.25em}
repeatedly extract the maximum point from \MaxPile{}.\item Set  to .
\end{enumerate}
\end{enumerate}

\item \label{spec} Set  to , then extract and
  neglect points from \MinOne{} and \MinTwo{} until we reach  as
  the current minimum for both.
\item  Extract all alive points from \MaxPile{} and report them as points on the convex hull, but in reverse order. This reversal can be done by inserting the points in an empty min-navigation pile, then extracting them.
\end{enumerate}
\end{enumerate}

By inspecting our algorithm and the algorithm of Chan and Chen, at
high level, the algorithms are quite similar. The main
difference is that we use adjustable navigation piles in order to
recall the points and deal with them. It should also be noted that
the full power of our techniques (augmenting the structure) is only needed 
in step~(\ref{critical-piece}). All other parts could have been handled efficiently without
augmenting the adjustable navigation pile.

\subsection{Analysis}

We need to prove that our convex-hull algorithm achieves a time
complexity of  and a space complexity of
 bits. As the space complexity, we deploy three
navigation piles that are proved to require  bits. As
for the time complexity, step (1) requires  time to build two
unaugmented navigation piles. Obviously, step (2) can
be done in  time. Now, we are going to analyse step (3). There
are  rounds in total. The work done in each round
can be summed up as follows:

\begin{itemize}

\item In step (3a), extracting  points requires 
  time.

\item Step (3b) needs  time to construct the 
  vector and the data structures for answering \Rank{} and \Select{}
  queries.

\item Constructing the upper hull of  points in step (3c) is done in
   time.
Note that  points are extracted from \MinTwo{}. Each of these
points will be inserted and may be extracted later from \MaxPile{}, but
a point can be inserted once and extracted once from \MaxPile{}, for a
total of at most  insertions and  extractions.

\item The time complexity of step (3d) is  too. We need
   to loop on the whole input sequence. Also, we may be
  extracting points from \MaxPile{}. Given that the number of alive
  points in \MaxPile{} is at most , then the extractions need at
  most  time.

\item Step (3e) does not affect the time complexity of the
  algorithm. Here, we extract points from \MinOne{} and
  \MinTwo{}. Since these piles initially contain the  points and no
  more insertions are done into them, throughout the algorithm all
  extractions from these piles require  time.

\item Given that the number of points in \MaxPile{} is at most ,
  step (3f) (including the reversal of the order of the points) 
	will be done in  time.
\end{itemize}

As a conclusion, the total cost of step (\ref{spec}) is . Except for step (\ref{spec}), the worst-case time complexity
of step (3) is  per round. Multiplying this by the
number of rounds , the time complexity of our
convex-hull algorithm is , as claimed.

\section{Concluding Remarks}
\label{sec:remarks}

\subsection{Summary} 

When constructing adjustable navigation piles, four techniques
are important: 1)~implicit links when indexing different types of
objects, 2)~bit packing and unpacking, 3)~buffering and incremental
submersion, and 4)~quantile thinning.  In addition to their connection
to binary heaps \cite{Wil64}, we pointed out the strong connection 
between the navigation piles and tournament trees.  
Conclusively, our data structure is shown to be a useful ingredient in space-efficient algorithms 
for problems that employ incremental sorting within their engine.
In general, we illustrated some conditions for when a succinct
implementation of a priority queue that uses a workspace constituting a sublinear number of bits is possible,
so that algorithm designers would be careful when using the structure.  

On the negative side, in practice, navigation piles are slow
\cite{JK06} for two reasons: 1)~The bit-manipulation machinery is
heavy and index calculations devour clock cycles. 2)~The cache
behaviour is poor because the memory accesses lack locality.
It is expected that the situation is not better for adjustable navigation piles.


Our sorting is a heapsort algorithm \cite{Wil64} that uses
an adjustable navigation pile instead of a binary heap.  For our
convex-hull algorithm we had to augment the data structure with extra
information while still maintaining the same asymptotic memory usage.
In spite of the optimality of the asymptotic running time of our
algorithms, one could criticize the practicality of the model itself, since the
memory-access patterns may not always be friendly to contemporary
computers while it is not allowed to move the elements around.

\subsection{Related Developments}
The credit for the use of quantile thinning should go to Pagter and
Rauhe \cite{PR98}.  However, the way the technique is used in the
adjustable navigation pile leads to a simpler and more elegant data
structure.  Recently, quantile thinning has also been used in a data
structure to answer heavy-hitter queries for a set of points on a line
\cite{EHMN11}.

Buffering and incremental submersion is a general data-structural
transformation that can be used to speed up \Insert{} in priority
queues \cite{AHRT05}.  Recently, this technique has also been used in a
space-efficient manner in weak heaps \cite{EEK13} and in strengthened
lazy heaps \cite{EEK15}.

After introducing the adjustable navigation piles in the conference version of this paper, 
our data structure has also found application in space-efficient graph algorithms
\cite{EHK15} and space-efficient plane-sweep geometric algorithms \cite{EK15}.

\subsection{Other Data Structures for Read-Only Data} 
In our experience, very few data structures can be made memory
adjustable as elegantly as priority queues. A stack is a gratifying
companion \cite{BKLSS15}.  As counterweight, a dictionary must maintain a
permutation of a set of size ; this means that it is difficult to
manage with much less than  bits. However, when the
goal is to cope with about  bits, a bit vector extended with \Rank{}
and \Select{} facilities (for a survey, see~\cite{NP12}) is a
relevant data structure.  Two related constructions are the wavelet
stack used in \cite{EJKS14} and the wavelet tree introduced in
\cite{GGV03} (for a survey, see \cite{Nav12}).

\subsection{Open Problems}

The optimality of our algorithm for computing convex hulls follows
from the fact that the sorting problem reduces to the problem of
computing the convex hull of a planar point set \cite[Section
  3.2]{PS85}. However, in the restricted RAM model, for the following
variants of the problem, the exact space-time trade-offs are still
unknown:
\begin{enumerate}
\item Compute the convex hull of a planar set of points given in
  lexicographic sorted order according to their coordinates.
\item Compute the convex hull of a simple polygon. 
\item Compute the extreme points of a planar set of  \emph{distinct} points,
  i.e.~the points on the convex hull in any order, and express the
  complexity as the function of  and , where  denotes the
  size of the output.
\end{enumerate}
For these problems, the space-time lower bound obtained via the
unique-elements problem \cite[Section 10.13.7]{Sav08} does not hold any more.
For the best known results, we refer to \cite{BKLSS15,CC07} (just be
aware that in these papers the space bounds are expressed in words,
not in bits).


\subsection*{Acknowledgements}

We thank Tetsuo Asano for introducing the restricted RAM model to us
and taking part in the initial discussions on the topic that led to
the post at a conference \cite{AEK13}.

\bibliography{shortstrings,read-only-data}
\bibliographystyle{DIKU}

\end{document}
