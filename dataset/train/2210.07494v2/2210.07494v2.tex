\message{ !name(main.tex)}\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}





\usepackage[final]{neurips_2022}









\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[dvipsnames]{xcolor}
\usepackage{colortbl,booktabs}
\usepackage{environ}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage{diagbox}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm2e}
\RestyleAlgo{ruled}
\NewEnviron{elaboration}{
	\par
	\begin{tikzpicture}
		\node[rectangle,minimum width=0.99\textwidth] (m) {\begin{minipage}{0.99\textwidth}\BODY\end{minipage}};
		\draw[thick] (m.south west) rectangle (m.north east);
	\end{tikzpicture}
}

\PassOptionsToPackage{hyphens}{url}
\hypersetup{colorlinks=true}

\definecolor{Tianlong_color}{rgb}{0.858, 0.188, 0.478}
\newcommand{\TL}[1]{\textcolor{Tianlong_color}{TL: #1}}

\newcommand{\Add}[1]{\textcolor{blue}{#1}}

\def\mE{\mathbb{E}}
\def\mR{\mathbb{R}}
\def\bx{{\bf x}_i}
\def\bX{{\bf X}}
\def\cx{{\bf x}_{(k)}}
\def\tilx{\tilde{\bf x}_i}
\def\tilX{\tilde{\bf X}}
\def\std{std}
\def\bU{{\bf U}}

\newcommand{\bm}[1]{\mathbf{#1}}

\title{A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking}



\author{Keyu Duan\textsuperscript{1}, Zirui Liu\textsuperscript{2}, Peihao Wang\textsuperscript{3}, Wenqing Zheng\textsuperscript{3}, \\ \textbf{Kaixiong Zhou\textsuperscript{2}, Tianlong Chen\textsuperscript{3}, Xia Hu\textsuperscript{2}, Zhangyang  Wang\textsuperscript{3}} \\
\textsuperscript{1}National University of Singapore, \textsuperscript{2}Rice University, \textsuperscript{3}University of Texas at Austin \\
    \small{\texttt{\{k.duan\}@u.nus.edu;}}
	\small{\texttt{\{zl105,Kaixiong.Zhou,xia.hu\}@rice.edu;}}  \\
	\small{\texttt{\{peihaowang,w.zheng,tianlong.chen,atlaswang\}@utexas.edu}}
}

\begin{document}

\message{ !name(main.tex) !offset(-3) }


\maketitle
\vspace{-5mm}
\begin{abstract}
	Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to find the rationale for designing scalable GNNs. To this end, we first systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding \textit{efficiency}, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named \textit{EnGCN}, to address the existing issues. Remarkably, our proposed method has achieved new state-of-the-art (SOTA) performance on large-scale datasets. Our code is available at \url{https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking}.
\end{abstract}

\section{Introduction}
\vspace{-2mm}
The Graph Neural Networks (GNNs) have shown great prosperity in recent years~\citep{kipf2016semi,velickovic2017graph,hamilton2017inductive,xu2018powerful}, and have dominated a variety of applications, including recommender systems~\citep{he2020lightgcn,ying2018graph,zheng2021cold}, social network analysis~\citep{tang2009relational,gao2018large,huang2019graph}, scientific topological structure prediction (e.g. cellular function prediction~\citep{hu2020open,zitnik2017predicting}, molecular structure prediction~\citep{hu2019strategies,you2020graph}, and chemical compound retrieval~\citep{wale2008comparison}), and scalable point cloud segmentation~\citep{li2019deepgcns,wang2019dynamic}, etc. Although the message passing (MP) strategy provides GNNs' superior performance, the nature of evolving massive topological structures prevents MP-based GNNs~\citep{li2020deepergcn,klicpera2018predict,xu2018representation,kipf2016semi,velickovic2017graph,xu2018powerful,gao2019graph, zhou2019multi} from scaling to industrial-grade graph applications. Specifically, as MP requires nodes aggregating information from their neighbors, the integral graph structures are inevitably preserved during forward and backward propagation, thus occupying considerable running memory and time. For example~\citep{ying2018graph}, training a GNN-based recommendation system over 7.5 billion items requires three days on a 16-GPU cluster (384 GB memory in total).

To facilitate understanding, a unified formulation of MP with \(K\) layers is presented as follows:
\begin{equation}\label{equ:message_passing}
	\bm{X}^{(K)} = {\bm{A}}^{(K-1)}\sigma\bigg({\bm{A}}^{(K-2)}\sigma\big(\cdots \sigma({\bm{A}}^{(0)}\bm{X}^{(0)}\bm{W}^{(0)}) \cdots\big)\bm{W}^{(K-2)}\bigg)\bm{W}^{(K-1)},
\end{equation}
where \(\sigma\) is an activation function (e.g. ReLU) and \({\bm{A}}^{(l)}\) is the weighted adjacency matrix at the \(l\)-th layer. As in Equ.~\eqref{equ:message_passing}, the key bottleneck of vanilla MP lies on \({\bm{A}}^{(l)}\bm{X}^{(l)}\). For the memory usage, the entire sparse adjacency matrix is supposed to be stored in one GPU. As the number of nodes grows, it is quite challenging for a single GPU to afford the message passing over the full graph.

Up to now, massive efforts have been made to mitigate the aforementioned issue and scale up GNNs~\citep{chiang2019cluster, zeng2019graphsaint, hamilton2017inductive, chen2018fastgcn, zou2019layer, wu2019simplifying, frasca2020sign, sun2021scalable, zhang2021graph}. Most of them focus on approximating the iterative full-batch MP to reduce the memory consumption for training within one single GPU. It is worth noting that we target at the algorithmic scope and do not extend to scalable infrastructure topics like distributed training with multiple GPUs~\citep{bojchevski2020scaling,md2021distgnn} and quantization~\citep{liu2021exact}. Briefly, the previous works encompass two branches: \textit{Sampling-based} and \textit{Decoupling-based}. Namely, the former methods~\citep{hamilton2017inductive, chen2018fastgcn, zeng2019graphsaint, chiang2019cluster, chen2017stochastic, cong2020minimal, huang2018adaptive} perform \textit{batch training} that utilizes sampled
subgraphs as a small batch to approximate the full-batch MP so that the memory consumption is considerably reduced. The latter follows the principle of performing \textit{propagation} (\(\bm{A}^{(l)}\bm{X}^{(l)}\)) and \textit{prediction} (\(\bm{X}^{(l)}\bm{W}^{(l)}\)) separately, either precomputing the propagation~\citep{wu2019simplifying, frasca2020sign, klicpera2018predict, liu2022neighbor2seq, bojchevski2020scaling} or post-processing with label propagation~\citep{sun2021scalable, huang2020combining}. Despite the prosperity of scalable GNNs, there are still plights under-explored:
we lack a systematic study of the reservoir from the perspective of \textit{effectiveness} and \textit{efficiency}, without which it is unachievable to tell the rationale of the designing philosophy for large-scale graph learning in practice.

To this end, we first establish a consistent benchmark and provide a systematic study for large-scale graph training for both \textit{Sampling-based} methods~(Sec.~\ref{sec:formu_sampling}) and \textit{Decoupling-based} methods~(Sec.~\ref{sec:formu_decoupling}). For each branch, we conduct a thorough investigation of the design strategy and implementation details of typical methods. Then, we carefully examine the sensitive hyperparameters and unify them in one ``sweet spot'' set by a linear greedy hyperparameter (HP) search (Sec.~\ref{sec:benchmarking_effectiveness}), i.e., iteratively searching the optimal value for an HP while fixing the others. For all concerned methods, the performance comparison is conducted on representative datasets of different scales, varying from about $80,000$ nodes to $2,400,000$, including Flickr~\citep{zeng2019graphsaint}, Reddit~\citep{hamilton2017inductive}, and ogbn-products~\citep{hu2020open}. This step is a crucial precondition on our way to the ultimate as the configuration inconsistency significantly prohibits a fair comparison as well as the following analysis. Besides, regarding \textit{efficiency}, we theoretically present the time and space complexities for the various branches, and empirically evaluate them on GPU memory usage, throughput, and convergence (Sec.~\ref{sec:benchmarking_efficiency}). In addition to the benchmark, we further present a new ensembling training manner \textit{EnGCN} (Sec.~\ref{sec:engcn}) to address the existing issues mentioned in our benchmark analysis (Sec.~\ref{sec:pros_and_cons}). Notably, via organically integrating with self-label-enhancement (SLE)~\cite{sun2021scalable}, EnGCN achieves the new state-of-the-art (SOTA) on multiple large-scale datasets.

\vspace{-1mm}
\section{Formulations For Large-scale Graph Training Paradigms}\label{sec:formulation}
\vspace{-2mm}
\subsection{Sampling-based Methods}\label{sec:formu_sampling}
\vspace{-2mm}
Given the formulation of Equ.~\eqref{equ:message_passing}, \textit{sampling-based} paradigm seeks the optimal way to perform batch training, such that each batch will meet the memory constraint of a single GPU for message passing. For completeness, we restate the unified formulation of sampling-based methods as follows:
\begin{equation}
	\bm{X}_{\mathcal{B}_{0}}^{(k)} = \widetilde{\bm{A}}_{\mathcal{B}_{1}}^{(k-1)}\sigma\bigg(\widetilde{\bm{A}}_{\mathcal{B}_{2}}^{(k-2)}\sigma\big(\cdots \sigma(\widetilde{\bm{A}}_{\mathcal{B}_{K}}^{(0)}\bm{X}_{\mathcal{B}_{K}}^{(0)}\bm{W}^{(0)}) \cdots\big)\bm{W}^{(K-2)}\bigg)\bm{W}^{(K-1)},
\end{equation}
where $\mathcal{B}_{l}$ is the set of sampled nodes for the $l$-th layer, and \(\widetilde{\bm{A}}^{(l)}\) is the adjacency matrix for the \(l\)-th layer sampled from the full graph. Given the local view of GNN --- one node's representation is only related to its neighbors --- a straightforward way for unbiased batch training would be \(\mathcal{B}_{l+1}=\mathcal{N}(\mathcal{B}_l)\), where \(\mathcal{N}\) denotes the set of neighbors. \(\mathcal{B}_0\) is randomly sampled according to the uniform distribution. Notably, this batch training style could achieve SOTA performance but also suffers from the ``\textit{neighbor explosion}'' problem, where the time consumption and memory usage grow exponentially with the GNN depth, causing significant memory and time overhead. To mitigate this, a number of \textit{sampling-based} methods were proposed. The key difference among them is how $\{\mathcal{B}_0, \dots, \mathcal{B}_{K-1}, \mathcal{B}_{K}\}$ are sampled. Given a large-scale graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, there are three categories of widely-used sampling strategies:

\vspace{-2mm}
\paragraph{Node-wise Sampling~\cite{hamilton2017inductive}}\label{sec:node_wise_sampling.}
\(\mathcal{B}_{l+1} = \bigcup_{v \in \mathcal{B}_l} \{u \ |\ u \sim Q \cdot \mathbb{P}_{\mathcal{N}(v)}\}\), where \(\mathbb{P}\) is a sampling distribution; \(\mathcal{N}(v)\) is the sampling space, i.e., the \(1\)-hop neighbors of \(v\); and \(Q\) denotes the number of samples. The representative node-wise sampling method is:

\(\star\) \textit{GraphSAGE}~\citep{hamilton2017inductive}: In GraphSAGE, \(\mathbb{P}\) is the uniform distribution.

Compared with the aforementioned \textit{naive batch training}, the node-wise sampling~\citep{hamilton2017inductive} alleviates the "\textit{node explosion}" problem by fixing the number of sampled neighbors \(Q\) for each node. It thus reduces the space complexity from \(D^K\) to \(Q^K\), where \(D\) is the averaged node degree. However, as \(Q\) is not far less than \(D\) in order of magnitude, such mitigation is moderate, which is empirically validated by our empirical results in Sec.~\ref{sec:benchmarking_effectiveness} and Sec.~\ref{sec:benchmarking_efficiency}.

\vspace{-2mm}
\paragraph{Layer-wise Sampling~\cite{chen2018fastgcn,zou2019layer}.}\label{sec:layer_wise_sampling}
\(\mathcal{B}_{l+1} = \{ u \ |\ u \sim Q \cdot \mathbb{P}_{\mathcal{N}(\mathcal{B}_{l})} \}\), where \(\mathcal{N}(\mathcal{B}_l) = \bigcup_{v \in \mathcal{B}_l} \mathcal{N}(v)\) denotes the union of \(1\)-hop neighbors of all nodes in \(\mathcal{B}_l\).  We introduce a couple of layer-wise sampling methods as follows.

\(\star\) \textit{FastGCN}~\citep{chen2018fastgcn}: The sampling distribution \(\mathbb{P}\) is designed regarding the node degree, where the probability for node \(u\) of being sampled is \(p(u) \propto || \hat{\bm{A}}(u,:)||^2\).

\(\star\) \textit{LADIES}~\citep{zou2019layer}: More recently, based on FastGCN, Zou et.al.~\citep{zou2019layer} propose LADIES that extends the sampling space from \(\mathcal{N}(\mathcal{B}_l)\) to \(\mathcal{N}(\mathcal{B}_l) \cup \mathcal{B}_l\) by adding the self-loops.

Notably, Compared with the node-wise sampling, the layer-wise sampling essentially solves the ``\textit{neighbor explosion}'' problem by fixing the number of overall sampled nodes in a layer to \(Q\). However, the layer-wisely induced adjacency matrix is usually sparser than the others, which accounts for its sub-optimal performance in practice.

\vspace{-2mm}
\paragraph{Subgraph-wise Sampling~\cite{chiang2019cluster,zeng2019graphsaint}.}\label{sec:subgraph_wise_sampling}
\(\mathcal{B}_K = \mathcal{B}_{K-1} = \cdots = \mathcal{B}_0 = \{ u \ |\ u \sim Q\cdot \mathbb{P}_{\mathcal{G}} \}.\) In the subgraph-wise sampling, all layers share the same subgraph induced from the entire graph \(\mathcal{G}\) based on a specific sampling strategy \(\mathbb{P}_{\mathcal{G}}\), such that the sampled nodes are confined in the subgraph. Typically, this sampling strategy has two representative works:

\(\star\)~\textit{ClusterGCN}~\citep{chiang2019cluster}: ClusterGCN first partitions the entire graph into clusters based on some graph partition algorithms, e.g. METIS~\citep{karypis1998fast}, and then select several clusters to form a batch.

\(\star\)~\textit{GraphSAINT}~\citep{zeng2019graphsaint}: GraphSAINT samples a subset of nodes based on sampling strategy \(\mathbb{P}_{\mathcal{G}}\) and then induces the corresponding subgraph as a batch. The commonly-used sampling strategies include: \((i)\) node sampler: \(\mathbb{P}(u) = ||\widetilde{\bm{A}}_{:,u}||^2\), \((ii)\) edge sampler: \(\mathbb{P}(u, v) = \frac{1}{deg(u)} + \frac{1}{deg(v)}\), and \((iii)\) random walk sampler. They are illustrated in Appendix~\ref{app:sampling_schemes}.

\vspace{-2mm}
\subsection{Decoupling-based Methods}\label{sec:formu_decoupling}
\vspace{-2mm}
Training GNNs with full-batch message passing at each epoch is not plausible. In this section, we summarize another line of scalable GNNs which decouple the message passing from GPU training to CPUs. Specifically, the message passing is conducted only once at CPUs accompanied by large accessible memory. Depending on the processing order, there are two typical ways to decouple these two operations: $(i)$ \textit{pre-processing} and $(ii)$ \textit{post-processing}.

\vspace{-2mm}
\paragraph{Pre-processing: MP precomputating~\cite{wu2019simplifying,frasca2020sign,sun2021scalable}.} Recalling Equ.~\eqref{equ:message_passing}, without loss of generalization, we assume that $\bm{A}^{(k-1)} = \bm{A}^{(k-2)} = \cdots \bm{A}^{(0)} = \bm{A}$, i.e. the topological structure for the entire graph remains the same during forward propagation, meeting most of the cases. To decouple the two operations, \textit{message passing} ($\bm{A}\bm{X}$) and \textit{feature transformation} ($\bm{X}\bm{W}$), we can first pre-compute the propagated node representations and then train a neural network for the downstream task based on these fused representations:
\begin{align}\label{equ:precomputing}
	\underbrace{\bm{X}^l = \bm{A}^l \bm{X}}_{\text{\textit{precomputing}}},\quad \underbrace{\bar{\bm{X}} = \rho(\bm{X}, \bm{X}^{1}, \cdots, \bm{X}^K), \quad \bm{Y} = f_{\theta}(\bar{\bm{X}})}_{\text{\textit{end-to-end training on a GPU}}},
\end{align}
where $\bm{X}^l$ can be regarded as the node representation aggregating $l$-hop neighborhood information; $K$ is the largest propagation hop; $\rho(\cdot)$ is a function that combines the aggregated features from different hops; and $f_{\theta}(\cdot)$ is a feature mapping function parameterized by $\boldsymbol{\theta}$. We summarize three existing pre-computing schemes as follows.

\(\star\) \textit{SGC}~\cite{wu2019simplifying}: SGC leverages the node representations aggregated with k hops and feeds the resultant features to a full-connected layer. We can formulate this scheme by letting $\rho(\cdot)$ select the last element $\bm{X}^K$ and $f_{\theta}(\cdot)$ be a linear layer with readout activation: $\bm{Y} = \sigma(\bm{X}^K \boldsymbol{\Theta})$.

\noindent
\(\star\) \textit{SIGN}~\cite{frasca2020sign}: SIGN concatenates features from different hops and then fuse them as the final node representation via a linear layer. To be more specific, $\rho(\cdot)$ is defined as $\bar{\bm{X}} = \begin{bmatrix}\bm{X} & \bm{X}^{1} & \cdots & \bm{X}^{K}\end{bmatrix} \boldsymbol{\Omega}$, where \(\boldsymbol{\Omega}\) is a transformation matrix, and $f_{\theta}(\cdot)$ is defined as a linear readout layer $\bm{Y} = \sigma(\bar{\bm{X}} \boldsymbol{\Theta})$.

\noindent
\(\star\) \textit{SAGN}~\cite{sun2021scalable}: SAGN adopts attention mechanism to combine feature representations from $K$ hops: $\bar{\bm{X}} = \sum_{l=1}^K \bm{T}^{l} \bm{X}^l$, where $\bm{T}^{l}$ is a diagonal matrix whose diagonal corresponds to the attention weight for each node of $k$-hop information. The attention weight for the $i$-th node is calculated by $T_i^{k} = \operatorname{softmax}_K(\operatorname{LeakyReLU}(\boldsymbol{u}^T \bm{X}_i + \boldsymbol{v}^T\bm{X}^k_j))$, where the subscripts slices the data matrices along the row.
The feature mapping function is implemented by an MLP block with a skip connection to initial features: $\bm{Y} = \operatorname{MLP}_\theta(\bar{\bm{X}} + \bm{X} \boldsymbol{\Theta}_r)$.


\vspace{-2mm}
\paragraph{Post-processing: Label Propagation.}
The label propagation algorithms~\citep{zhu2005semi, wang2007label, karasuyama2013manifold, gong2016label, liu2018learning, huang2020combining, wang2020unifying} diffuse labels in the graph and make predictions based on the diffused labels. It is a classical family of graph algorithms for \textit{transductive learning}, where the nodes for testing are used in the training procedure. The label propagation can be written in a unified form as follows:
\begin{equation}\label{equ:lp}
	\bm{Y}^{(l)} = \alpha \bm{A}\bm{Y}^{(l-1)} + (1-\alpha) \bm{G}.
\end{equation}
The diffusion procedure iterates the formula above with $l$ for multiple times to guarantee convergence. It requires two sets of inputs: \((i)\) the stack of the \textit{label embeddings} of all nodes, denoted as $\bm{Y}^{(0)}\in \mathbb{R}^{N\times c}$, where \(c\) is the number of classes. In our implementation, the $\bm{Y}^{(0)}$ is the output of a trained MLP model~\cite{huang2020combining}. \((ii)\) the \textit{diffusion embedding}, denoted as $\mathbf{G} \in \mathbb{R}^{N\times c}$ that propagate themselves across the edges in the graph. Depending on how the diffusion embeddings of unlabeled nodes are computed, two types of $\bm{G}$ are summarized as follows:



\(\star\) \textit{Zeros}~\citep{zhu2005semi}:
\(\mathbf{G}_{i,:} =
\begin{cases}
	\hat{\mathbf{Y}}_{i,:}-\alpha\bm{A}\bm{Y}_{i,:}^{(k)}, & i \in \mathcal{T}_{train} \\
	\mathbf{0},                                            & \textit{otherwise}
\end{cases}\), where \(\mathcal{T}_{train}\) denotes the training set and \(\hat{\mathbf{Y}}\) is the stack of true labels. For \textit{zeros}, \(\mathbf{Y}^{(0)}=\mathbf{G}\).

\(\star\) \textit{Residual}~\citep{huang2020combining}:
\(\mathbf{G}_{i,:} =
\begin{cases}
	\hat{\mathbf{Y}}_i, & v_i \in \mathcal{T}_{\textit{train}} \\
	\hat{\mathbf{Z}}_i, & \textit{otherwise}
\end{cases}\), where \(\hat{\mathbf{Z}}=\mathbf{Z}+\hat{\mathbf{E}}\). $\mathbf{Z}$ is the predictions of a trained simple neural network, e.g. MLP, and \(\hat{\mathbf{E}}\) is an residual error matrix, which is optimized iteratively for multiple times by \(\mathbf{E}^{t+1}=(1-\alpha)\mathbf{E}+\alpha \mathbf{A}\mathbf{E}^{(t)}\), where \(\mathbf{E}=\mathbf{Z}-\hat{\mathbf{Y}}\) and \(\mathbf{E}^{(0)}=\mathbf{E}\).

\vspace{-2mm}
\subsection{More Related Works}
\vspace{-2mm}
\textbf{Model-agnostic Tricks.} Besides the training methods as introduced above, there are some model-agnostic tricks that have been empirically confirmed to be effective for boosting large-scale graph training. Although those add-ons cannot be included into our benchmarking analysis, it is of equal importance to introduce them for completeness. Here we briefly introduce two representative ones:

\(\star\) \textit{Self-Label-Enhanced (SLE)}~\cite{sun2021scalable}: SLE includes two individual tricks, \textit{self training} and \textit{label augmentation}. Here we use \(\mathcal{T}\) denoting the training set. For self training, the unlabeled nodes with high confidence (larger than a pre-defined threshold) are added to \(\mathcal{T}\) after a certain number of training epochs. For label augmentation, it trains an additional model \(\Phi(\cdot)\). The forward propagation can be formulated as \(out = \Phi(\hat{\bm{A}}^{k}\bm{Y}_{\mathcal{T}})\). \(out\) is added to the main model to make the final prediction.

\(\star\) \textit{GIANT}~\cite{chien2021node}: In general, node features are usually pre-embed with graph-agnostic language models, such as word2vec~\cite{mikolov2013distributed} and BERT~\cite{devlin2018bert}. Recently, Chien et.al. propose a graph-related node feature extraction framework (GIANT), which embeds the raw texts to numerical features by taking advantage of graph structures, to help boost the performance of GNNs for the downstream tasks.

\textbf{Memory-based GNN Training}. Focus on mitigating the ``\textit{Neighbor Explosion}'' problem of full-batch training as introduced, memory-based GNNs~\cite{fey2021gnnautoscale, ding2021vq} try to save the GPU memory with different techniques while including all neighbor nodes into computing during the message passing. GAS~\cite{fey2021gnnautoscale} incorporates \textit{historical embeddings}~\cite{chen2017stochastic} to provably maintain the expressive power of full-batch GNN. VQ-GNN~\cite{ding2021vq} utilizes \textit{vector quantization} to scale convolutional-based GNN and resemble the performance of full-batch message passing by learning an additional quantized feature matrix and a corresponding low-rank adjacent matrix.

\vspace{-3mm}
\section{Benchmarking Over Effectiveness}\label{sec:benchmarking_effectiveness}
\vspace{-1mm}
\subsection{Implementation Details}
\vspace{-1mm}
We test numerous large-scale graph training methods with a greedy hyperparameter (HP) search to find their \textit{sweet spot} and the best performance for a fair comparison. The search space is defined in Table~\ref{tab:search_space}. The access and statistics of all used datasets are introduced in Appendix~\ref{app:datasets}. Particularly, for label propagation, we select two representative algorithms: Huang et.al.~\citep{huang2020combining}, the \textit{residual} diffusion type, and Zhu et.al.~\citep{zhu2005semi}, the \textit{zeros} type. The number of propagation is the maximum iteration $k$. The aggregation ratio is $\alpha$ as in Equ.~\eqref{equ:lp}, and the number of MLP layers is the number of MLP layers that precedes the label propagation module following Huang et.al.~\citep{huang2020combining}.
\begin{wraptable}{r}{0.55\linewidth}
	\vspace{-3mm}
	\caption{The search space of hyperparameters for benchmarked methods.}
	\label{tab:search_space}
	\centering
	\resizebox{1\linewidth}{!}{
		\begin{tabular}{cl|c}
			\toprule
			Category            & Hyperparameter (Abbr.)   & Candidates                                                                             \\
			\midrule
			\multirow{7}{*}{\shortstack{Sampling \&                                                                                                 \\ Precomputing}} & Learning rate (LR) & \(\{1e-2^*, 1e-3, 1e-4\}\)   \\
			                    & Weight Decay (WD)        & \(\{1e-4^*, 2e-4, 4e-4\}\)                                                             \\
			                    & Dropout Rate (DP)        & \(\{0.1, 0.2^*, 0.5, 0.7\}\)                                                           \\
			                    & Training Epochs (\#E)    & \(\{20, 30, 40, 50^*\}\)                                                               \\
			                    & Hidden Dimension (HD)    & \(128^*, 256, 512\)                                                                    \\
			                    & \# layers (\#L)          & \(\{ 2^*, 4, 6 \}\)                                                                    \\
			                    & Batch size$^a$ (BS)      & \(\{1000^*, 2000, 5000\}\)                                                             \\
			\midrule
			\multirow{6}{*}{LP} & Diffusion Type (DT)      & \{ residual$^*$, zeros \}                                                              \\
			                    & \# Propagations (\#Prop) & \{ 2, 20$^*$, 50 \}                                                                    \\
			                    & Aggregation Ratio (AR)   & \{ 0.5, 0.75$^*$, 0.9, 0.99 \}                                                         \\
			                    & Adj. Norm (Adj.)         & \{ $\bm{D}^{-1}\bm{A}$, $\bm{A}\bm{D}^{-1}$, $\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}$$^*$ \} \\
			                    & Auto Scale (AS)          & \{ \textit{True}$^*$, \textit{False} \}                                                \\
			                    & \# MLP Layers (\#ML)     & \{ 2$^*$, 3, 4 \}                                                                      \\
			\bottomrule
			\multicolumn{3}{l}{$^*$ marks the default value}                                                                                        \\
			\multicolumn{3}{l}{\shortstack[l]{$^a$ we do not search batch size for precomputing based methods since                                 \\ they do not follow a sample-training style.}} \\
		\end{tabular}}
	\vspace{-4mm}
\end{wraptable}
Limited by space, we select five representative approaches that covers all branches as we introduced, including GraphSAGE~\cite{hamilton2017inductive}, LADIES~\cite{zou2019layer}, ClusterGCN~\cite{chiang2019cluster}, SAGN~\cite{sun2021scalable}, and C\&S~\cite{huang2020combining}. We illustrate the selected results in Fig.~\ref{fig:hp_search} and the results of other methods in Fig.~\ref{fig:app_hp_search}. For each subplot, from left to right, each column denotes the search results for one HP. Once one HP was searched, its value will be fixed to the best results for the rest HP searching. Iteratively, we obtain the best performance in the last column. For convenience and clarity, we list the searched optimal hyperparameter settings of all test methods in Table~\ref{tab:seached_hp_for_all_methods}.

\vspace{-2mm}
\subsection{Experimental Observations}
\begin{figure}[!ht]
	\vspace{-4mm}
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/hp_search}
	\end{center}
	\caption{The greedy hyperparameter searching results for selected representative methods. The x-axis denotes the searched HPs, where the abbreviations are consistent with Table~\ref{tab:search_space}.}
	\label{fig:hp_search}
\end{figure}

\textbf{Obs. 1. Sampling-based methods are more sensitive to the hyperparameters related to MP.} According to Fig.~\ref{fig:hp_search}, in comparison with precomputing, all sampling-based methods are non-sensitive to hyperparameters (HPs) that are related to the feature transformation matrices, including weight decay, dropout, and hidden dimension; but particularly sensitive to the MP-related HPs, including the number of layers and batch size. For model depth, sampling-based methods generally achieve the \textit{sweet spots} when the number of layers is confined to shallow and suffers from the \textit{oversmoothing} problem~\cite{li2018deeper, chen2022bag, oono2020graph, zhou2020towards, zhou2021dirichlet} as the GNN models go deeper. However, this issue is moderately mitigated in decoupling-based methods as the model depth does not align with the number of MP hop.

\textbf{Obs. 2. Sampling-based methods' performance is nearly positive-correlated with the training batch size.} According to the results of the last column of all sampling-based methods, the performance of the layer-wise and subgraph-wise sampling methods is roughly proportional to the batch size. Expectedly, the model performance could further increase as the batch size grows till the upper bound of full-batch training because more links can be preserved. Particularly, in our experiment, we set the number of sampled neighbors of \textit{node-wise sampling} to a large threshold such that the performance of GraphSAGE can be regarded as \textit{full-batch training}'s. It can be easily found that the performance of sampling-based methods is inferior to \textit{full-batching training} (GraphSAGE), further proving our conjecture that the missing links by sampling are non-trivial.

\textbf{Obs. 3. Precomputing-based methods generally perform better on larger datasets.} As show in Fig.~\ref{fig:hp_search} and Fig.~\ref{fig:app_hp_search}, C\&S (\textit{label propagation}) outperforms the \textit{full-batch training} (GraphSAGE as introduced in \textit{Obs. 2}) on the largest dataset ogb-products by a large margin of $4.5\%$, although both two branches have on-par performance on smaller datasets. Remarkably, our searched results for GraphSAGE and LP on ogbn-products also reached better performance, compared with the ones on the OGB leaderboard~\footnote{\url{https://ogb.stanford.edu/docs/leader_nodeprop/}}. Noticing that GraphSAGE encounters the out-of-memory (OOM)\footnote{We rerun it on a GPU with larger memory and the accuracy is 80.56\%} runtime error with increasing depth, the observation partially indicates that, limited by model depth and \textit{neighbor explosion} problem, it is possibly not powerful for extremely large-scale graphs to learn expressive representations.

\vspace{-2mm}
\section{Benchmarking Over Efficiency}\label{sec:benchmarking_efficiency}
\vspace{-2mm}
\subsection{Time And Space Complexity}

In this section, we present another benchmark regarding the efficiency of scalable graph training methods. Firstly, we briefly summarize a general complexity analysis in Table~\ref{tab:complexity}. For \textit{sampling-based} methods, we note that the time complexity is for training GNNs by iterating over the whole graph. The time complexity  $\mathcal{O}(L||\bm{A}||_0 D + LND^2)$ consists of two parts. The first part $L||\bm{A}||_0 D$ is from the Sparse-Dense Matrix Multiplication,i.e., $\bm{A}\bm{X}$. The second part $LND^2$ is from the normal Dense-Dense Matrix Multiplication, i.e., $(\bm{A}\bm{X})\bm{W}$. Regarding the space complexity, we need to store the activations of each layer in memory, which has a $\mathcal{O}(bLD)$ space complexity. Note that we ignore the memory usage of model weights and the optimizer here since they are negligible compared to the activations. For \textit{decoupling-based} methods, the training paradigm is simplified as MLPs, and thus the complexity is the same as the traditional mini-batch training. We do not include \textit{label propagation} in our analysis since it can be trained totally on CPUs.

\begin{wraptable}{r}{0.6\linewidth}
	\centering
	\vspace{-6mm}
	\caption{The time and space complexity for training GNNs with sampling-based and decoupling-based methods, where $b$ is the averaged number of nodes in the sampled subgraph and $r$ is the averaged number of neighbors of each node. Here we do not consider the complexity of pre-processing sice it can be done in CPUs.}
	\label{tab:complexity}
	\vspace{0.5mm}
	\resizebox{1\linewidth}{!}{
		\begin{tabular}{l|cc}
			\toprule
			Category                                                                & Time Complexity                        & Space Complexity     \\
			\midrule
			Node-wise Sampling~\citep{hamilton2017inductive}                        & $\mathcal{O}(r^LND^2)$                 & $\mathcal{O}(br^LD)$ \\
			Layer-wise Sampling~\citep{frasca2020sign, zou2019layer}                & $\mathcal{O}(rLND^2)$                  & $\mathcal{O}(brLD)$  \\
			Subgraph-wise Sampling~\citep{chiang2019cluster, zeng2019graphsaint}    & $\mathcal{O}(L||\bm{A}||_0 D + LND^2)$ & $\mathcal{O}(bLD)$   \\
			Precomputing~\citep{wu2019simplifying, frasca2020sign, sun2021scalable} & $\mathcal{O}(LND^2)$                   & $\mathcal{O}(bLD)$   \\
			\bottomrule
		\end{tabular}}
	\vspace{-3mm}
\end{wraptable}

\vspace{-2mm}
\subsection{Throughput And Memory Usage}
\paragraph{Implementation Details}
To fairly benchmark the training speed and memory usage for large-scale graph training methods, we empirically evaluate the throughputs and actual memory for various methods during the training procedure.
Here ``Throughput'' measures how many times can we complete the training steps within a second.
Note that we omit the label propagation methods since it is not trained by \textit{backward propagation}. We provide our implementation details for computing the throughput and memory usage in section~\ref{app:implementation_details_speed_and_mem}. We report the hardware throughput and activation usage in Table~\ref{tab:mem_speed}. We summarize three main observations.

\begin{table}[!ht]
	\centering
	\caption{The memory usage of activations and the hardware throughput (higher is better). The hardware here is an RTX 3090 GPU.}
	\label{tab:mem_speed}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{ccccccc}
			\hline
			\multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}\\\end{tabular}} & \multicolumn{2}{c}{Flickr}                              & \multicolumn{2}{c}{Reddit}                                        & \multicolumn{2}{c}{ogbn-products}                                                                                                                                                                                                                       \\
			\multicolumn{1}{l}{}                                           & \begin{tabular}[c]{@{}c@{}}Act \\Mem. (MB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Throughput\\(iteration/s)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Act\\Mem. (MB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Throughput\\(iteration/s)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Act\\Mem. (MB)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Throughput\\(iteration/s)\end{tabular} \\
			\hline
			GraphSAGE                                                      & 230.63                                                  & 65.96                                                             & 687.21                                                 & 27.62                                                             & 415.94                                                 & 37.69                                                             \\
			ClusterGCN                                                     & 18.45                                                   & 171.46                                                            & 20.84                                                  & 79.91                                                             & 10.62                                                  & 156.01                                                            \\
			GraphSAINT                                                     & 16.51                                                   & 151.77                                                            & 21.25                                                  & 70.68                                                             & 10.95                                                  & 143.51                                                            \\
			FastGCN                                                        & 19.77                                                   & 226.93                                                            & 22.53                                                  & 87.94                                                             & 11.54                                                  & 93.05                                                             \\
			LADIES                                                         & 33.26                                                   & 195.34                                                            & 43.21                                                  & 116.46                                                            & 20.33                                                  & 93.47                                                             \\
			\hline
			SGC                                                            & 0.01                                                    & 115.02                                                            & 0.02                                                   & 89.91                                                             & 0.01                                                   & 267.31                                                            \\
			SIGN                                                           & 16.99                                                   & 96.20                                                             & 16.38                                                  & 75.33                                                             & 16.21                                                  & 208.52                                                            \\
			SAGN                                                           & 72.94                                                   & 55.28                                                             & 72.37                                                  & 43.45                                                             & 71.81                                                  & 80.04                                                             \\
			\hline
			                                                               &                                                         &                                                                   &                                                        &                                                                   &                                                        &
		\end{tabular}}
	\vspace{-5mm}
\end{table}

\textbf{Obs. 4. GraphSAGE is significantly slower and occupies more memory compared to other baselines.} This is partially because of the large neighbor sampling threshold we set and inherently owing to its neighborhood explosion. Namely, to compute the loss for a single node, it requires the neighbors' embeddings at the down-streaming layer recursively. Please refer to Sec.~\ref{sec:node_wise_sampling.} for details.

\textbf{Obs. 5. SGC does not occupy any activation memory.} As shown in Table~\ref{tab:mem_speed}, SGC only occupies about $0.01$ MB actual memory during training. This is because SGC only has one linear layer and the activation is exactly the input feature matrix, which has been stored in memory. Thus, it is not accounted towards the activation memory.

\textbf{Obs. 6. In general, the speed of decoupling-based methods is comparable to sampling-based methods.} Besides the scale of sparse adjacency matrix, the feature set size is also crucial for occupying memory. Although precomputing-based methods avoid storing the graph structures in a GPU, they may take advantage of multi-hop features, where the corresponding memory is multiplied many times.

\vspace{-2mm}
\subsection{Convergence Analysis} \label{sec:convergence_analysis}
\vspace{-2mm}
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/convergence}
	\end{center}
	\vspace{-2mm}
	\caption{The empirical results of convergence for \textit{sampling-based} methods (real line) and \textit{precomputing-based} methods (dash line).}
	\label{fig:convergence_analysis}
	\vspace{-3mm}
\end{figure}

For convergence analysis, we test all benchmarked methods on Flickr, Reddit, and ogbn-products. The training loss and validation accuracy (val\_acc) are shown in Fig.~\ref{fig:convergence_analysis}. Based on the empirical results, we summarize the main observation as follows:

\textbf{Obs. 7. In general, precomputing-based methods have faster and more stable convergence than sampling-based methods.} This is because sampling-based methods usually incur a variance among batches that poses unstable and slow convergence~\cite{cong2020minimal}. However, precomputing-based methods mitigate this by moving message passing from backward propagation to the precomputing stage.

\section{EnGCN: Rethinking Graph Convolutional Networks With Ensembling}\label{sec:engcn}
\vspace{-1mm}
\subsection{An Empirical Summary: Pros And Cons}\label{sec:pros_and_cons}
\vspace{-1mm}

Based on our benchmark results in section~\ref{sec:benchmarking_effectiveness} and \ref{sec:benchmarking_efficiency}, we summarize the advantages (marked as \textit{\textbf{Pros}}) and constraints (marked as \textit{\textbf{Cons}}) for different branches as follows. Besides the summary, we also provide a joint comparison of effectiveness and efficiency for various methods in Appendix~\ref{app:joint_comparison}.

\(\star\) \textit{Sampling-based}: \textbf{(\textit{Pros})} Sampling subgraphs into GPU training allows them taking advantage of numerous graph convolution layers, such as GCN~\cite{kipf2016semi}, GraphSAGE~\cite{hamilton2017inductive}, and GIN~\cite{xu2018powerful}, and this is flexible for them to design specific architecture for different downstream tasks, e.g. node classification and graph classification. \textbf{(\textit{Cons})} As aforementioned, sampling-based methods suffers from \textit{link sparsity}~\cite{zeng2019graphsaint} (section~\ref{sec:benchmarking_effectiveness}) and unstable and low convergence (section~\ref{sec:benchmarking_efficiency}) problems, both of which prevent sampling-based methods from achieving SOTA performance.

\(\star\) \textit{Precomputing-based}: \textbf{(\textit{Pros})} Decoupling \textit{message passing} from GPU training to CPU precomputing allows precomputing-based methods utilize a mixture of feature transformation units (e.g. attention mechanism and MLP) to train in a well-studied manner. This guarantees stable and fast convergence (section~\ref{sec:benchmarking_efficiency}). Particularly, integrating with the add-ons like SLE~\cite{sun2021scalable} and GIANT~\cite{chien2021node}, precomputing-based methods achieve SOTA performance on large-scale \textit{open graph benchmark} (ogb)~\cite{hu2020open} datasets. \textbf{(\textit{Cons})} In general, precomputing-based methods at least occupy a CPU memory space of \(\mathcal{O}(LNd)\), where \(L\) is the number of layers; \(N\) is the number of nodes; and \(d\) is the dimension of input features. In comparison, it is \(L\) times as large as the others, which is not affordable for extremely large-scale graphs. For example, containing about 111 million nodes, the largest ogb dataset, ogbn-papers100M, requires approximately \(57\) Gigabytes (GB) to store the initial feature matrix, given the data type is float and the dimension of features is 128. As the number of layers increases, the required CPU memory space will grow proportionally to an unaffordable number.

\(\star\) \textit{Label Propagation}: \textbf{(\textit{Pros})} As a traditional branch of graph learning algorithm, label propagation is a simple but effective add-on as a post-processing trick nowadays. Because of its mode-agnostic nature, it can be simply attached to the end of any graph representation learning algorithm to boost the final prediction. \textbf{(\textit{Cons})} Label propagation has many additional sensitive hyperparameters as we introduced in Table~\ref{tab:search_space} and is specifically designed for the node classification task.

\vspace{-2mm}
\subsection{Motivation and Related Works}
\vspace{-1mm}
To address the above constraints of \textit{sampling-based} methods and \textit{precomputing-based} methods, let us first recap the full-batch message passing in Equ.~\eqref{equ:message_passing}. We reformulate it into a more general form:
\begin{equation*}
	\bm{X}^{(k)} = \Phi^{(k-1)}\bigg({\bm{A}}\Phi^{(k-2)}\big(\cdots \bm{A}\Phi^{(0)}({\bm{A}}\bm{X}^{(0)})\big)\bigg),
\end{equation*}
where \(\Phi^{(i)}\) denotes the feature mapping model for the \(i\)-th layer. To make the message passing scalable, following the rationale of \(decoupling\), we propose a different training scheme from precomputing: \textit{Instead of end-to-end training, we sequentially train the \(\Phi\)s in a layer-wise manner.} In this way, no precomputing is required and thus the corresponding constraint of CPU memory occupation is essentially mitigated. To elaborate on this, we present the layer-wise training manner:
\begin{equation}
	\underbrace{\bm{X}^{(l)} = \bm{A}\bm{X}^{(l-1)}}_{\textit{Message passing on CPUs}},\quad \underbrace{\bm{Z}^{(l)}=\Phi^{(l)}(\bm{X}^{(l)})}_{\textit{forword propagation}},\quad \underbrace{\nabla\Phi^{(l)}=\nabla\mathcal{L}(\bm{Z}^{(l)},\bm{Y})}_{\textit{backward propagtion}}.
\end{equation}
From layer \(0\) to \(k\), we do message passing once and then train \(\Phi^{(l)}\) in batches for epochs. Finally, one can simply use the output of model \(\Phi\) as the prediction. Besides, from the perspective of ensembling, the models \(\Phi\) can be naturally viewed as a set of weak learners trained on multiple views of the input \(\bm{X}\). As a result, it is compatible to use ensembling to boost the final predictions, such as majority voting. In addition, Based on our empirical results, this training manner is capable of achieving SOTA methods on relatively small datasets without exhaustive finetuning. To further boost the performance, we organically integrate SLE, which \textit{has achieved new SOTA performance on several representative datasets}. We name this model \textit{EnGCN} (Ensembling GCN).

\textbf{Related Works.} Interestingly, the layer-wise training manner and majority voting are naturally consistent with the \textit{boosting} algorithms, where we sequentially train weak learners with instance reweighting and make the final prediction by majority voting. In the scope of graph representation learning, AdaGCN~\cite{sun2019adagcn} first applies adaboosting~\cite{freund1999short, hastie2009multi} to address the \textit{oversmoothing} problem of deep GCNs. Though focusing on different topics, AdaGCN has a similar training scheme as ours. Therefore, we implement a scalable version for it, which is included as a SOTA baseline in our experiment. In addition, AdaClusterGCN~\cite{zheng2021adaboosting} proposed an adaboosting application that ensembles weak learners trained on different clusters.

\vspace{-2mm}
\subsection{Methodology}\label{sec:engcn_methodology}
\vspace{-2mm}
Considering a large-scale graph \(\mathcal{G}=(\bm{A}, \bm{X}, \bm{y})\), where \(\bm{A}\) is the adjacent matrix, \(\bm{X}\) is the node features, and \(\bm{y}\) is the true labels. Respectively, \(\mathcal{T}_{train}\), \(\mathcal{T}_{val}\) and \(\mathcal{T}_{test}\) denotes the training, validation, and test set. Let \(\bm{X}^{(l)}\) and \(\bm{Y}^{(l)}\) denotes the embeddings of node features and labels at the \(l\)-th layer, respectively. We use \(\tilde{\bm{y}}^{(l)}\) and \(\widetilde{\mathcal{T}}_{train}^{(l)}\) denoting the pseudo labels, pseudo training set for self training at layer \(l\).

\textbf{Initialization.} we initialise several important matrices and vectors:
\begin{equation*}
	\bm{X}^{(0)}=\bm{X}, \quad \bm{Y}^{(0)}_{i,:} = \begin{cases}
		\text{one\_hot}(\bm{y}_{i}), & i \in \mathcal{T}_{train} \\
		\bm{0},                      & \text{otherwise}\end{cases}, \quad \widetilde{\mathcal{T}}^{(0)}_{train} = \mathcal{T}_{train}, \quad \tilde{\bm{y}}^{(0)}_{i} = \begin{cases}
		\bm{y}_{i}, & i \in \widetilde{\mathcal{T}}_{train} \\
		\bm{0},     & \text{otherwise}\end{cases}
\end{equation*}

\textbf{Layer-wise Training.}
From \(0\) to \(k\), we follow a layer-wise training manner, where each training stage contains three phases: \textit{pre-processing}, \textit{training}, and \textit{post-processing}. For layer \(l\), the three phases are introduced as follows.

\textit{Pre-processing.} For pre-processing, we precompute \(\bm{X}^{(l)}\) and \(\bm{Y}^{(l)}\) in CPUs as follows:
\begin{equation}
	\bm{X}^{(l)} = \hat{\bm{A}}\bm{X}^{(l-1)}, \quad \bm{Y}^{(l)} = \hat{\bm{A}}\bm{Y}^{(l-1)},
\end{equation}
where \(\tilde{\bm{A}}\) is symmetrically normalized~\cite{kipf2016semi}. Note that pre-processing is skipped when \(l=0\).

\textit{Training.} We solely train two simple models till convergence, which empirically takes dozens of epochs on real-world datasets. The forward propagation is:
\begin{equation}
	\text{out}^{(l)} = \Omega(\bm{X}^{(l)},\bm{Y}^{(l)}) = \Phi(\bm{X}^{(l)}) + \Psi(\bm{Y}^{(l)}),
\end{equation}
where \(\Phi\) and \(\Psi\) are two MLP models that are shared through all layers. Specifically, when \(l=0\), the forward propagation is reduced to \(\text{out}^{(0)}=\Phi(\bm{X}^{(0)})\) where \(\Psi\) is not evolved. This is because the initialized \(\bm{Y}^{(0)}\) contains many zero vectors and will pose the overfitting problem. For backward propagation, we compute the training loss using the pseudo labels \(\tilde{\bm{y}}^{(l)}\) instead of \(\bm{y}^{(l)}\).

\textit{post-processing.} After obtaining the trained models, we save the state of them as \(\Omega^{(l)} = (\Phi^{(l)}, \Psi^{(l)})\) for ensembling. Furthermore, \textit{self training} is used to enhance the training set. Following Sun et.al.~\cite{sun2021scalable}, the pseudo labels and pseudo training masks are updated as follows.
\begin{equation}
	\widetilde{\mathcal{T}}^{(l+1)}_{train} = \widetilde{\mathcal{T}}^{(l)}_{train}\cup \{i\ |\  \underset{c}{max}(\tau(\text{out}^{(l)}_{i}))\ge\alpha\}, \quad
	\tilde{\bm{y}}^{(l+1)}_{i} = \begin{cases}
		\bm{y}_{i}, & i \in \mathcal{T}_{train}                                                \\
		c,          & \text{else if } \underset{c}{max}(\tau(\text{out}^{(l)}_{i})) \ge \alpha \\
	\end{cases},
\end{equation}
where \(\tau\) is the softmax function.

\textbf{Inference With Majority Voting.} After k layers, we have obtained a series of weak learners \(\{ \Omega^{(l)} \ |\ 0 \le l \le k\}\). The final prediction of node \(n\) is made by weighted majority voting~\cite{hastie2009multi}:
\begin{equation}
	\hat{y}_{n} = \underset{c}{argmax}\sum_{l=0}^{k}\bigg(\bm{z}_{n}^{(l)}-\frac{1}{d}\sum_{i=1}^{d}(\bm{z}_{n,i}^{(l)})\bigg),
\end{equation}
where \(\bm{z}^{(l)}=log\_softmax(\text{out}^{(l)}_{n})\).

\subsection{Empirical Analysis}

\textbf{Experiment Settings.} Consistent with our effectiveness benchmark, we test our proposed \textit{EnGCN} on Flickr, Reddit, and ogbn-products. A similar hyperparameter (HP) search was conducted to find its suitable HP setting. The search space is provided in Appendix~\ref{app:hp_engcn}. For the baselines, we directly use all benchmark results from section~\ref{sec:benchmarking_effectiveness}, where the SOTA performance has been achieved.

\textbf{Main Experiment.} As shown in Table~\ref{tab:comparison_results}, \textit{EnGCN} outperforms the \textit{sampling-based} and \textit{decoupling-based} methods on multi-scale datasets. For Flickr and Reddit, EnGCN outperforms the baselines by a large margin. Remarkably, EnGCN has achieved new SOTA performance on ogbn-products, outperforming C\&S by 2.88\% and the SOTA model (GIANT-XRT+SAGN+MCR+C\&S) in the ogb leaderboard by 1.26\%. In addition to the comparison experiment, we also conduct a couple of ablation studies to provide more insights into EnGCN in Appendix~\ref{app:ablation_engcn}.

\begin{table}[!ht]
	\centering
	\caption{The comparison experiment results on Flickr, Reddit, and ogbn-products}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{llccc}
			\toprule
			Category                            & Baselines                               & Flickr                        & Reddit                        & ogbn-products                 \\
			\midrule
			\multirow{5}{*}{Sampling-based}     & GraphSAGE~\cite{hamilton2017inductive}  & 53.63 \(\pm\) 0.13\%          & 96.50 \(\pm\) 0.03\%          & 80.61 \(\pm\) 0.16\%          \\
			                                    & FastGCN~\cite{chen2018fastgcn}          & 50.51 \(\pm\) 0.13\%          & 79.50 \(\pm\) 1.22\%          & 73.46 \(\pm\) 0.20\%          \\
			                                    & LADIES~\cite{zou2019layer}              & 50.51 \(\pm\) 0.13\%          & 86.96 \(\pm\) 0.37\%          & 75.31 \(\pm\) 0.56\%          \\
			                                    & ClusterGCN~\cite{chiang2019cluster}     & 51.20 \(\pm\) 0.13\%          & 95.68 \(\pm\) 0.03\%          & 78.62 \(\pm\) 0.61\%          \\
			                                    & GraphSAINT~\cite{zeng2019graphsaint}    & 51.81 \(\pm\) 0.17\%          & 95.62 \(\pm\) 0.05\%          & 75.36 \(\pm\) 0.34\%          \\
			\midrule
			\multirow{5}{*}{Decoupling-based}   & SGC~\cite{wu2019simplifying}            & 50.35 \(\pm\) 0.05\%          & 93.51 \(\pm\) 0.04\%          & 67.48 \(\pm\) 0.11\%          \\
			                                    & SIGN~\cite{frasca2020sign}              & 51.60 \(\pm\) 0.11\%          & 95.95 \(\pm\) 0.02\%          & 76.85 \(\pm\) 0.56\%          \\
			                                    & SAGN~\cite{sun2021scalable}             & 50.07 \(\pm\) 0.11\%          & 96.48 \(\pm\) 0.03\%          & 81.21 \(\pm\) 0.07\%          \\
			                                    & GAMLP~\cite{zhang2021graph}             & 52.58 \(\pm\) 0.12\%          & 96.73 \(\pm\) 0.03\%          & 83,76 \(\pm\) 0.19\%          \\
			                                    & C\&S~\cite{huang2020combining}          & 51.24 \(\pm\) 0.17\%          & 95.33 \(\pm\) 0.08\%          & 85.11 \(\pm\) 0.07\%          \\
			\midrule
			\multirow{3}{*}{Other SOTA Methods} & AdaGCN~\cite{sun2019adagcn}             & 52.97 \(\pm\) 0.01\%          & 96.05 \(\pm\) 0.00\%          & 76.41 \(\pm\) 0.00\%          \\
			                                    & SAGN+SLE~\cite{sun2021scalable}\(^{*}\) & 54.60 \(\pm\) 0.40\%          & 97.10 \(\pm\) 0.00\%          & 84.28 \(\pm\) 0.14\%          \\
			                                    & \shortstack[l]{GIANT-XRT+                                                                                                               \\SAGN+MCR+C\&S}~\cite{zhang2021improving}\(^{*}\) & - & - & 86.73 \(\pm\) 0.08\% \\
			\midrule
			Ours                                & EnGCN                                   & \textbf{56.43 \(\pm\) 0.21\%} & \textbf{97.14 \(\pm\) 0.03\%} & \textbf{87.99 \(\pm\) 0.04\%} \\
			\bottomrule
			\multicolumn{5}{l}{\small \(*\): the results are from the original papers}
		\end{tabular}}
	\label{tab:comparison_results}
\end{table}

\textbf{The Training Efficiency and Convergence Landscape of EnGCN.}
For EnGCN, since all we need to train is two simple shallow MLPs (Section~\ref{sec:engcn_methodology}), the GPU throughput and memory consumption are expected to be sufficiently efficient. The remained concern is solely about the convergence of EnGCN. Due to the nature of \textit{layer-wise training}, the convergence of EnGCN is more complicated than other end-to-end training methods. In Figure~\ref{fig:engcn_convergence}, we show the convergence landscape of EnGCN and provide several interesting observations as follows.

\ding{182} As shown in Figure~\ref{fig:engcn_convergence}, the train accuracy and validation accuracy generally increase layer-wisely till convergence. Noticeably, though the training accuracy occasionally drops, the validation accuracy still relatively remains positive. \ding{183} At the beginning of each layer, the accuracy changes rapidly, indicating the remarkable distribution difference for various hops. \ding{184} Different datasets are sensitive to different hops. For example, the \(2\)-nd hop is crucial to boost the training and validation accuracy on Flickr, while for Reddit and ogbn-products, \(1\)-hop neighbors are more important.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{figs/engcn_convergence}
	\caption{The convergence landscape of EnGCN. All models are trained with \(4\) layers' features. For each layer-wise phase, we train the model with 70 epochs.}
	\label{fig:engcn_convergence}
	\vspace{-3mm}
\end{figure}


\begin{wrapfigure}{r}{0.48\linewidth}
	\vspace{-3mm}
	\centering
	\includegraphics[width=0.95\linewidth]{figs/cpu_mem.pdf}
	\vspace{-3mm}
	\caption{The allocated CPU memory of EnGCN and selected baselines on Flickr.}
	\vspace{-2mm}
	\label{fig:cpu_mem}
\end{wrapfigure}
\textbf{The CPU memory consumption of EnGCN.}
To confirm the low CPU memory consumption of EnGCN, we provide a comparison experiment and illustrate the results in Figure~\ref{fig:cpu_mem}. The x-axis denotes the models' number of layers while the y-axis records their allocated memories that are reported by ``\textit{aten::empty}'' of PyTorch. As shown in Figure~\ref{fig:cpu_mem}, the precomputing-based methods, SIGN and SAGN, suffer from expensive CPU memory consumption as the model depth increases. For sampling-based methods, since there is no need to pre-store a large number of feature matrices, the memory consumption increases much more smoothly. For EnGCN, as no precomputing is required, the CPU memory consumption is considerably reduced in comparison with SIGN and SAGN, which intuitively validates the CPU memory efficiency of EnGCN.

\vspace{-3mm}
\section{Conclusion}
\vspace{-2mm}
The scalability issue of graph convolutional networks has been a notoriously challenging research problem. In this work, we establish a fair and consistent benchmark for large-scale graph training w.r.t effectiveness and efficiency. We provide a unified formulation for dozens of works and further assess them on the basis of accuracy, memory usage, throughput, and convergence. Furthermore, provided with the comprehensive benchmark results, we rethink the scalability issue of GCNs from the perspective of ensembling and then present an ensembling-based trainer scheme (EnGCN) that solely needs to train a couple of simple MLPs to achieve new SOTA on multi-scale large datasets. We hope our study on benchmarking and rethinking to help lay a solid, practical, and systematic foundation for the scalable GCN community and provide researchers with broader and deeper insights into large-scale graph training.

\clearpage
\bibliographystyle{unsrt}
\bibliography{ref}
\clearpage
\appendix
\renewcommand{\thepage}{A\arabic{page}}
\renewcommand{\thesection}{A\arabic{section}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}


\section{More details of Formulations}
\subsection{Representative Subgraph Sampling Schemes} \label{app:sampling_schemes}
\(\star\) \textbf{Node Sampler}~\cite{chen2018fastgcn,zeng2019graphsaint}: \(\mathbb{P}(u) = ||\widetilde{\bm{A}}_{:,u}||^2\), where all nodes are sampled independently based on the normalized distribution of \(\mathbb{P}\). This sampling strategy is logically equivalent to layer-wise sampling~\citep{chen2018fastgcn}.

\(\star\) \textbf{Edge Sampler}~\cite{zeng2019graphsaint}: \(\mathbb{P}(u, v) = \frac{1}{deg(u)} + \frac{1}{deg(v)}\), where all edges are sampled independently based the edge distribution above. In our implementation, we utilize the sampled nodes (once contained in the sampled edges) to induce the subgraph as input, which should include more edges to help boost the performance.

\(\star\) \textbf{Random Walk Sampler}~\cite{leskovec2006sampling, zeng2019graphsaint}: Here, we first sample a subset of root nodes uniformly, based on which we perform a random walk at a certain length to obtain the subgraph as a batch.

\(\star\) \textbf{Graph Partitioner}~\cite{chiang2019cluster,karypis1998fast}: We first partition the entire graph into clusters with graph clustering algorithms and then select multiple clusters to form a batch.

\section{Additional Experiment Results}
\subsection{Additional Hyperparameter Searching Results}
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{figs/app_hp_search.pdf}
	\end{center}
	\caption{The greedy hyperparameter searching results for other methods.}
	\label{fig:app_hp_search}
\end{figure}
\begin{table}[!ht]
	\caption{The searched optimal hyperparameters for all tested methods}
	\vspace{-3mm}
	\centering
	\resizebox{0.9\textwidth}{!}{
		\begin{tabular}{llccc}
			\toprule
			\multirow{2}{*}{Category}   & \multirow{2}{*}{Methods}                   & \multicolumn{3}{c}{Datasets}                                          \\
			\cmidrule(lr){3-3}  \cmidrule(lr){4-4} \cmidrule(lr){5-5}
			                            &                                            & Flickr                                       & Reddit & ogbn-products \\
			\midrule
			\multirow{10}{*}{Sampling}  & GraphSAGE~\citep{hamilton2017inductive}    & \makecell{LR: 0.0001, WD: 0.0001, DP: 0.5,                            \\ EP: 50, HD: 512, \#L: 4, BS: 1000} & \makecell{LR: 0.0001, WD: 0.0 DP: 0.2, \\ EP: 50, HD: 512, \#L: 4, BS: 1000} & \makecell{LR: 0.001, WD: 0.0 DP: 0.5, \\ EP: 50, HD: 512, \#L: 4, BS: 1000}\\
			\cmidrule(lr){2-5}
			                            & FastGCN~\citep{chen2018fastgcn}            & \makecell{LR: 0.001, WD: 0.0002, DP: 0.1,                             \\ EP: 50, HD: 512, \#L: 2, BS: 5000} & \makecell{LR: 0.01, WD: 0.0 DP: 0.5, \\ EP: 50, HD: 256, \#L: 2, BS: 5000} & \makecell{LR: 0.01, WD: 0.0 DP: 0.2, \\ EP: 50, HD: 256, \#L: 2, BS: 5000}      \\
			\cmidrule(lr){2-5}
			                            & LADIES~\citep{zou2019layer}                & \makecell{LR: 0.001, WD: 0.0002, DP: 0.1,                             \\ EP: 50, HD: 512, \#L: 2, BS: 5000} & \makecell{LR: 0.01, WD: 0.0001 DP: 0.2, \\ EP: 50, HD: 512, \#L: 2, BS: 5000} & \makecell{LR: 0.01, WD: 0.0 DP: 0.2, \\ EP: 30, HD: 256, \#L: 2, BS: 5000}      \\
			\cmidrule(lr){2-5}
			                            & ClusterGCN~\citep{chiang2019cluster}       & \makecell{LR: 0.001, WD: 0.0002, DP: 0.2,                             \\ EP: 30, HD: 256, \#L: 2, BS: 5000} & \makecell{LR: 0.0001, WD: 0.0 DP: 0.5, \\ EP: 50, HD: 256, \#L: 4, BS: 2000} & \makecell{LR: 0.001, WD: 0.0001 DP: 0.2, \\ EP: 40, HD: 128, \#L: 4, BS: 2000}      \\
			\cmidrule(lr){2-5}
			                            & GraphSAINT~\citep{zeng2019graphsaint}      & \makecell{LR: 0.001, WD: 0.0004, DP: 0.2,                             \\ EP: 50, HD: 512, \#L: 4, BS: 5000} & \makecell{LR: 0.01, WD: 0.0002 DP: 0.7, \\ EP: 30, HD: 128, \#L: 2, BS: 5000} & \makecell{LR: 0.01, WD: 0.0 DP: 0.2, \\ EP: 40, HD: 128, \#L: 2, BS: 5000}      \\
			\midrule
			\multirow{8}{*}{Decoupling} & SGC~\citep{wu2019simplifying}              & \makecell{LR: 0.01, WD: 0.0002,                                       \\ EP: 100, \#L:2, DP: 0.5} & \makecell{LR: 0.01, WD: 0.0001, \\ EP: 50, \#L:2, DP: 0.1} & \makecell{LR: 0.001, WD: 0.0001, \\ EP: 500, \#L:8, DP: 0.1} \\
			\cmidrule(lr){2-5}
			                            & SIGN~\citep{frasca2020sign}                & \makecell{LR: 0.001, WD: 0.0002,                                      \\ EP: 100, HD:256, \#L:4, DP: 0.2} & \makecell{LR: 0.01, WD: 0.0002, \\ EP: 50, HD: 512, \#L:8, DP: 0.7} & \makecell{LR: 0.01, WD: 0.0001, \\ EP: 500, HD:256, \#L:4, DP: 0.2} \\
			\cmidrule(lr){2-5}
			                            & SAGN~\citep{sun2021scalable}               & \makecell{LR: 0.01, WD: 0.0001,                                       \\ EP: 20, HD:64, \#L:4, DP: 0.7} & \makecell{LR: 0.001, WD: 0.0002, \\ EP: 50, HD: 256, \#L:2, DP: 0.5} & \makecell{LR: 0.001, WD: 0.0, \\ EP: 500, HD:512, \#L:4, DP: 0.5} \\
			\cmidrule(lr){2-5}
			                            & GAMLP~\citep{zhang2021graph}               & \makecell{LR: 0.001, WD: 0.0002,                                      \\ EP: 20, HD:64, \#L:2, DP: 0.5} & \makecell{LR: 0.001, WD: 0.0001, \\ EP: 30, HD: 128, \#L:6, DP: 0.5} & \makecell{LR: 0.001, WD: 0.0002, \\ EP: 500, HD:768, \#L:8, DP: 0.7} \\
			\cmidrule(lr){2-5}
			                            & LP~\citep{huang2020combining, zhu2005semi} & \makecell{DT: residual, \#Prop: 20, AR: 0.9,                          \\ Adj: \(D^{-1/2}AD^{-1/2}\), AS: True, \#ML:2} & \makecell{DT: residual, \#Prop: 50, AR: 0.9, \\ Adj: \(D^{-1}A\), AS: True, \#ML:2} & \makecell{DT: residual, \#Prop: 20, AR: 0.9, \\ Adj: \(D^{-1}A\), AS: True, \#ML:3} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-3mm}
	\label{tab:seached_hp_for_all_methods}
\end{table}

\subsection{A Joint Comparison of Effectiveness and Efficiency}\label{app:joint_comparison}
To further facilitate a comprehensive understanding of the benchmark results, we provide an illustration in Figure~\ref{fig:joint_comparison} to jointly compare the effectiveness and efficiency of the methods. An empirical summary could be found in Section~\ref{sec:pros_and_cons}.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.98\linewidth]{figs/joint_comparison.pdf}
	\caption{The joint comparison of effectiveness (accuracy) and efficiency (throughput) for sampling-based and precomputing-based methods.}
	\label{fig:joint_comparison}
	\vspace{-3mm}
\end{figure}
\vspace{-3mm}
\subsection{The Hyperparameter Settings for EnGCN}\label{app:hp_engcn}
The searched HPs for EnGCN includes learning rate (0.01, 0.001, 0.0001), weight decay (0, 1e-5, 1e-4), dropout (0.2, 0.5, 0.7), epochs (30, 50, 70), hidden dimension (128, 256, 512), batch size (5000, 10000), batch norm (True, False), self learning threshold (\(\alpha\)=0.8, 0.9, 0.95), and number of layers (4, 5, 8). The searched HPs that produce the reported results on Flickr, Reddit, and ogbn-products are shown in Table~\ref{tab:hp_engcn}.
\begin{table}[!ht]
	\centering
	\caption{The searched optimal hyperparameters for EnGCN on Flickr, Reddit, and ogbn-products}
	\resizebox{0.98\linewidth}{!}{
		\begin{tabular}{lc}
			\toprule
			Datasets      & Searched HPs                                                                                                               \\
			\midrule
			Flickr        & lr 0.0001 weight decay 0.0001 dropout 0.2 epoch 70 hidden dimension 256 number of layers 4 batch size 10000 \(\alpha\) 0.9 \\
			Reddit        & lr 0.001 weight decay 0 dropout 0.2 epochs 70  hidden dimension 512  number of layers 4  batch size 5000 \(\alpha\) 0.95   \\
			ogbn-products & lr 0.01  weight decay 0  dropout 0.2  epochs 70  hidden dimension 512  number of layers 8 batch size 10000  \(\alpha\) 0.8 \\
			\bottomrule
		\end{tabular}}
	\label{tab:hp_engcn}
	\vspace{-3mm}
\end{table}

\vspace{-2mm}
\subsection{Ablation Study for EnGCN}\label{app:ablation_engcn}

\textbf{Ablating Ensembling.} Here we provide an ablation study to confirm the effectiveness of ensembling (inference with majority voting). For ablated models, we directly use the ones after \(l\)-hop training, \(0\le l\le 3\). The experiment results are shown in Table~\ref{tab:ablation_engcn}. Notably, with majority voting, the performance is boosted by a large margin on Flickr and also has noticeable improvement on Reddit and ogbn-products. Besides, we find that the test accuracy of EnGCN after \(l\)-hop training keeps increasing as \(l\) grows. This phenomenon is consistent with the empirical results in AdaGCN~\cite{sun2019adagcn}.

\begin{table}[!ht]
	\centering
	\caption{The test accuracy (\%) for ablated EnGCNs.}
	\label{tab:ablation_engcn}
	\begin{tabular}{lccc}
		\toprule
		Category                       & Flickr                    & Reddit                    & ogbn-products             \\
		\midrule
		EnGCN after \(0\)-hop training & 46.11\(\pm\)0.14          & 72.63\(\pm\)0.09          & 63.42\(\pm\)0.08          \\
		EnGCN after \(1\)-hop training & 46.26\(\pm\)0.17          & 94.17\(\pm\)0.05          & 73.05\(\pm\)0.13          \\
		EnGCN after \(2\)-hop training & 50.00\(\pm\)0.49          & 95.01\(\pm\)0.03          & 75.20\(\pm\)0.06          \\
		EnGCN after \(3\)-hop training & 50.56\(\pm\)0.80          & 94.72\(\pm\)0.04          & \textbf{75.64\(\pm\)0.33}          \\
		EnGCN with majority voting     & \textbf{56.21\(\pm\)0.21} & \textbf{96.66\(\pm\)0.06} & 75.59\(\pm\)0.04 \\
		\bottomrule
	\end{tabular}
\end{table}


\section{Additional Implementation Details}\label{app:add_implementation_details}
\subsection{Access and Statistics of Benchmark Datasets}\label{app:datasets}
All datasets we used could be accessed through the APIs provided py PyTorch Geometric\footnote{\url{https://github.com/pyg-team/pytorch_geometric}}~\cite{fey2019fast}. The statistics of Flickr, Reddit, and ogbn-products are provided as follows.
\begin{table}[!ht]
	\centering
	\caption{The statistics of Flickr, Reddit, and ogbn-products}
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{lccccc}
			\toprule
			Dataset       & Nodes     & Edges      & Classes & splitting (Train/Validation/Test) & Task                       \\
			\midrule
			Flickr        & 89,250    & 899,756    & 7       & 0.50 / 0.25 / 0.25                & Multi-Class Classification \\
			Reddit        & 232,965   & 11,606,919 & 41      & 0.66 / 0.10 / 0.24                & Multi-Class Classification \\
			ogbn-products & 2,449,029 & 61,859,140 & 47      & 0.10 / 0.02 / 0.88                & Multi-Class Classification \\
			\bottomrule
		\end{tabular}}
	\label{tab:my_label}
\end{table}

\vspace{-3mm}
\subsection{Implementation details of testing GPU memory and throughput}\label{app:implementation_details_speed_and_mem}

Here we provide the details of implementation and hyperparameters for the throughput and memory usage experiments.
Regarding the implementation,
we evaluate the hardware throughput based on Chen et.al.~\citep{chen2021actnn}.
For the activation memory, we measure it based on \texttt{torch.cuda.memory\_allocated}.

Regarding the hyperparameter setting in the throughput and memory usage measurement,
we set the hidden dimension to 128 across different models and datasets.
We control the number of nodes whose embedding requires gradients roughly equal to 5,000 across different models and datasets.
Thus, our method is fair in the sense that we control the number of active nodes per batch in the same for different methods.
We note that for graph-wise sampling-based methods (e.g., ClusterGCN, GraphSAINT),  the number of nodes whose embedding requires gradients equals the number of nodes retained in the GPU memory.
However, for other sampling-based methods (e.g., GraphSAGE, FastGCN), they need to gather the neighbor embeddings to update the node embedding in the current batch.
These embeddings of nodes that are outside the current batch do not require gradients.
We also want to clarify that the hyperparameter ``batch\_size'' in our script has a different meaning for different methods.
For example, for precomputing methods, a 5,000 ``batch\_size'' means each mini-batch contains 5,000 input samples (i.e., nodes).
For GraphSAINT, ``batch\_size'' means the number of roots in the random walk sampler.
Thus, the number of nodes in each mini-batch roughly contains ``batch\_size'' $\times$ ``walk\_length''.

\vspace{-4mm}
\section{Intended Use}
\vspace{-2mm}
The license of our repository is MIT license. For more information, please refer to \url{https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking/blob/main/LICENSE}. Our benchmark is for researchers and scientists in graph mining and data science community to propose innovative methods, especially for large-scale graph training. We implement a number of representative scalable GNN models, provide several abstract classes for further inheriting, and define a unified training process for a fair comparison.
In our code base, we implement two abstract classes for \textit{sampling-based} and \textit{precomputing-based} methods based on our unified formulations in Section~\ref{sec:formulation} , respectively. One could build up his/her new sampling-based or precomputing-based GNN models upon the code base by solely overwriting a few specific functions. For detailed usage including installation, reproduction, etc., please refer to our documentation in the repository.
 \end{document}
