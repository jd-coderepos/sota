
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{url}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[T1]{fontenc}
\usepackage{color, colortbl}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=cyan,
    filecolor=magenta,      
    urlcolor=cyan,
    }
\definecolor{Gray}{gray}{0.9}



\title{Class Prototype-based Cleaner for Label Noise Learning}



\author{Jingjia Huang \thanks{Corresponding author.} \\
ByteDance Inc\\
Beijing, China \\
\texttt{huangjingjia@bytedance.com} \\
\And
Yuanqi Chen\\
School of Electronic and Computer Engineering \\
Peking University Shenzhen Graduate School \\
Shenzhen, China \\
\texttt{cyq373@pku.edu.cn @pku.edu.cn} \\
\AND
Jiashi Feng \& Xinglong Wu \\
ByteDance Inc\\
Beijing, China \\
\texttt{\{jshfeng,wuxinglong\}@bytedance.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\JS}[1]{\textcolor{blue}{[JS: #1]}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Semi-supervised learning based methods are current SOTA solutions to the noisy-label learning problem, which rely on learning an unsupervised label cleaner first to divide the training samples into a  labeled set for clean data and an unlabeled set for noise data. 
Typically, the cleaner is   obtained  via fitting a mixture model to  the distribution of  per-sample training losses.  However, the modeling procedure is  \emph{class agnostic} and  assumes the loss distributions of clean and noise samples are the same across different classes. Unfortunately, in practice, such an assumption does not always hold due to the varying learning difficulty of different classes, thus leading to sub-optimal   label noise partition criteria. In this work, we reveal this long-ignored problem and propose a simple yet effective solution, named  \textbf{C}lass \textbf{P}rototype-based label noise \textbf{C}leaner (\textbf{CPC}). Unlike previous works treating all the classes equally, CPC fully considers loss distribution heterogeneity and applies  class-aware modulation to partition the clean and noise data. CPC takes advantage of loss distribution modeling and intra-class consistency regularization in feature space simultaneously  and thus can  better distinguish clean and noise labels. We theoretically justify the effectiveness of our method by explaining it from the Expectation-Maximization (EM) framework. Extensive experiments are conducted on the noisy-label benchmarks CIFAR-10, CIFAR-100, Clothing1M and WebVision. The results show that CPC consistently brings about performance improvement across all benchmarks. Codes and pre-trained models will be released at \url{https://github.com/hjjpku/CPC.git}.
\end{abstract}

\section{Introduction}
Deep Neural Networks (DNNs) have brought about significant progress to the computer vision community over past few years. One key to its success is the availability of large amount of training data with proper annotations. However, label noise is very common in real-world applications. Without proper intervention, DNNs would be easily misled by the label noise and yield  poor performance. 

In order to improve the performance of DNNs when learning with noise labels, various methods have been developed \citep{liu2020early, Li2020DivideMixLW, reed2014training, nishi2021augmentation}. Among them, semi-supervised learning based methods \citep{nishi2021augmentation,Li2020DivideMixLW} achieve the most competitive results. The semi-supervised   learning methods  follow a two-stage   pipeline. They first model the  loss distribution of training samples to construct a noise cleaner based on the ``small-loss prior" \citep{han2020survey}, which says in the early stage of training, samples with smaller cross-entropy losses are more likely to have clean labels. The prior is widely adopted and demonstrated to be highly effective in practice \citep{han2020survey}. Given the noise cleaner, the training samples are divided into a labeled clean set and an unlabeled noise set. Then, semi-supervised learning strategies like MixMatch \citep{berthelot2019mixmatch} are employed to train DNNs on the divided dataset. 

The key to their performance lies in the accuracy of the label-noise cleaner \citep{cordeiro2022longremix}. Usually, a single Gaussian Mixture Model (GMM) \citep{Li2020DivideMixLW} is used  to model the   loss distribution of all the training samples across different categories. However, this modeling procedure is class-agnostic, which assumes a DNN model has the same learning speed to fit the training samples in different categories, thus the same loss value on samples in different categories can reflect the same degree of noise likelihood.

Unfortunately, such assumption does not hold  in practise. 
In Fig.~\ref{fig:motivation}, 
we present the cross-entropy loss distribution of training samples at the end of DNNs warm-up period. We conduct Kolmogorov-Smirnov test \citep{massey1951kolmogorov} to quantify the loss distribution difference between the samples in each class and samples in the whole dataset. The results show that for 54\%  categories in CIFAR-100 under 90\% symmetric noise, the p-value is lower than 0.05\footnote{A p-value  0.05 suggests the probability that the class-wise loss distribution are the same with the global loss distribution is lower than 5\%.} for the hypothesis test that the probability distribution of clean samples in the class is the same with the probability distribution of clean samples in the whole dataset, while the number in the case of noise samples is 53\%. Therefore, the class-agnostic label noise cleaner, which establishes a overly rigid criterion shared by all the classes, would introduce more noise samples to the clean set while reject clean samples, and consequently get the model perform poorly. A straightforward remedy to the problem is to fit distinct GMMs to losses of samples in different classes respectively, yielding a class-aware GMM cleaner. Nevertheless, this class-aware modeling strategy implicitly assumes that label noise is existed in every class. In the case of asymmetric noise \emph{e.g.,} CIFAR10-asym40\%, where samples in parts of classes are clean, such a naive strategy would classify most of hard samples in the clean classes as noise, and results in negative affect on model training.
\begin{figure}[t]
  \centering
  \includegraphics[width=1\columnwidth]{figure/motivation.png}
  \caption{ Loss distribution of samples in CIFAR-100 with 90\% symmetric noise at epoch 30 (left)  and CIFAR-10 with 40\% asymmetric noise at epoch 10 (right), where the curves indicate mean probability density over all the categories while the shadow indicates the 95\% confidence interval. The loss distribution for each class deviates significantly from the average loss distribution.}
   \label{fig:motivation}
   \vspace{-1\baselineskip}
\end{figure}




Considering that images in the same category should share similar visual representations, the similarity between a sample and the cluster center (\emph{e.g.}, class prototype) of its labeled class is helpful for recognizing label noise. In this paper, we propose a simple \textbf{C}lass \textbf{P}rototype-based label noise \textbf{C}leaner (\textbf{CPC}) to apply class-aware modulation to the partitioning of clean and noise data, which takes advantage of intra-class consistency regularization in feature space and loss distribution modeling, simultaneously. CPC learns embedding for each class, \emph{i.e.,} class prototypes, via intra-class consistency regularization, which urges samples in the same class to gather around the corresponding class prototype while pushes samples not belonging to the class away.
Unlike the aforementioned naive class-aware GMM cleaner, CPC apply class-aware modulation to label noise partitioning via representation similarity measuring without assuming that label noise is existed in every class, which is more general for different label noise scenarios. Meanwhile, CPC leverages the ``small-loss prior'' to provide stronger and more robust supervision signals to facilitate the learning of prototypes. 


We plug CPC to the popular DivideMix\citep{Li2020DivideMixLW} framework, which iterates between label noise partitioning and DNNs optimization. With the stronger label noise cleaner in the first stage, DNNs can be trained better in the second stage, which would further improve the learning of class prototypes. We theoretically justify the procedure from Expectation-Maximization algorithm perspective, which guarantees the efficacy of the method. We conduct extensive experiments on multiple noisy-label benchmarks, including CIFAR-10, CIFAR-100, Clothing1M and WebVision. The results clearly show that CPC effectively improves accuracy of label-noise partition, and brings about consistently performance improvement across all noise levels and benchmarks. 

The contribution of our work lie in three folds: (1) We reveal the long-ignored problem of \emph{class-agnostic} loss distribution modeling that widely existed in label noise learning, and propose a simple yet effective solution, named  Class Prototype-based label noise Cleaner (CPC); (2) CPC takes advantage of loss distribution modeling and intra-class consistency regularization in feature space simultaneously, which can  better distinguish clean and noise labels; (3) Extensive experimental results show that our method achieves competitive performance compared to current SOTAs. 


\section{Related Work}
\label{sec:related_work}
Recent advances in robust learning with noisy labels can be roughly divided into three groups.
(a) \textbf{Label correction methods} aim to translate wrong labels into correct ones.
Early studies rely on an auxiliary set with clean samples
for clean label inference \citep{Xiao2015LearningFM,Vahdat2017TowardRA,Li2017LearningFN,Lee2018CleanNetTL}.
Recent efforts focus on performing label correction procedures without supervision regarding clean or noise labels.
\citep{Yi2019ProbabilisticEN,Tanaka2018JointOF} propose to jointly optimize labels during learning model parameters.
\citet{li2020mopro} propose to correct corrupted labels via learning class prototypes and utilize the pseudo-label generated by measuring the similarity between prototypes and samples to train model. \citet{wu2021ngc} and \citet{li2021learning} introduce neighbouring information in feature space  to correct noise label, and propose a graph-based method and a class prototype-based method, respectively. 
(b) \textbf{Sample selection methods}
select potential clean samples for training to eliminate the effect of noise labels on learning the true data distribution.
\citep{Han2018CoteachingRT,Jiang2018MentorNetLD,Jiang2020BeyondSN,Yu2019HowDD} involve training two DNNs simultaneously and focus on the samples that are probably to be correctly labeled.
(c) \textbf{Semi-supervised learning methods} conceal noise labels and treat these samples as unlabeled data \citep{Ding2018AST}. DivideMix \citep{Li2020DivideMixLW} is a typical algorithm among these works, which compromises an unsupervised label noise cleaner that divides the training data to a labeled clean set and an unlabeled noise set, followed by semi-supervised learning that minimize the empirical vicinal risk of the model. Inspired by DivideMix, a series of methods \citep{cordeiro2022longremix,nishi2021augmentation,Cordeiro2021PropMixHS} are proposed, which achieve SOTA performance. However, all these methods rely on  the class-agnostic loss distribution modeling to achieve the label noise cleaner, which hinders the performance of the model. The class-agnostic loss distribution modeling implicitly assumes a DNN model has the same learning speed to memory training samples in different categories. However, in reality, the memorization speed are actually different and will cause the the problem of under learning in hard classes as revealed by \citet{wang2019symmetric}. In this paper, we focuses on another problem, \emph{i.e.,} class agnostic loss distribution modeling problem caused by the issue in the context of label noise cleaner. In our method, we propose the simple yet effective class prototype-based label noise cleaner to solve the problem. Besides, compared to previous prototype-based label noise learning methods \citep{li2020mopro,li2021learning}, our method are different from them in two folds: (1) we utilize prototypes as label noise cleaner to effectively improve the semi-supervised learning based methods; (2) CPC takes advantage of both loss distribution modeling and intra-class consistency regularization in feature space simultaneously which learns better prototypes.

\begin{figure}[t]
\center
  \includegraphics[width=0.9\columnwidth]{figure/pipeline}
\caption{Illustration of the training pipeline in a single epoch. Blue modules are utilized in the first stage, where we update the prototypes in CPC and partition the training data.
   Green modules are utilized in the second stage, where the DNN model is trained based on the partitioned data.}
  \vspace{-1\baselineskip}
   \label{fig:pipeline}
\end{figure}

\section{Preliminary}\label{sec:preliminary}
In label noise learning, given a training set , where  is an image and  is the annotated label over  classes,  the label  could differ from the unknown true label . 
In this paper, we follow the popular label noise learning framework DivideMix \citep{Li2020DivideMixLW}, which first warms up the model for a few epochs by training on all the data using the standard cross-entropy loss, and then trains the model by iterating a two-stage   pipeline. The pipeline comprises an unsupervised label cleaner  to divide training samples into a labeled set for clean data  and an unlabeled set for noise data , followed by a semi-supervised learning stage that trains the model to minimise the empirical vicinal risk (EVR) \citep{zhang2017mixup}:

where  and  indicate MixMatch \citep{berthelot2019mixmatch} augmented clean and noise set.   and  denote the losses for samples in set  and , which are weighted by .   is the softmax output of DNNs, where  is the predicted label. For more details about EVR, please refer to the appendix~\ref{app:evr}. 

In \citet{Li2020DivideMixLW}, the unsupervised label cleaner is operated under the ``small-loss prior", which is widely adopted and demonstrated to be highly effective \citep{han2020survey}.
 The prior assumes that in the early stage of training, samples with smaller cross-entropy losses are more likely to have clean labels. The well known insight behind the ``small-loss prior" is that DNNs tend to learn simple patterns first before fitting label noise \citep{arpit2017closer}. Given a training sample  and the softmax output  of DNNs, where  is the predicted label, the cross-entropy loss  reflects how well the model fits the training sample. 

To achieve the unsupervised label cleaner , a two-component Gaussian Mixture Model (GMM) is employed to fit the loss distribution of all training samples, \emph{i.e.,} , where , and  is a mixing coefficient.
The component with smaller mean represents the distribution of clean samples and the other one is for noise samples.
We use  indicates the data is clean or not. Then,  represents the clean probability of , which is the posterior probability of its loss belonging to the clean component.  The label cleaner is shared by training samples across different classes, which is actually \textit{class-agnostic}.
A hypothesis implicitly accompanying this loss distribution modeling method is ignored by current works, which assumes the loss distributions of clean and noise samples are consistent across different categories.
Unfortunately, as illustrated in Fig.\ref{fig:motivation}, the  hypothesis dose not hold in practise. In this paper, we propose the class prototype-based label noise cleaner which applies class-aware modulation to the partitioning of clean and noise data and improves label noise learning.

\section{Methodology}


\subsection{Overview} \label{overview}
Our method follows the two-stage label noise learning framework DivideMix \citep{Li2020DivideMixLW} and improves the framework with the proposed CPC. CPC comprises class prototypes , where  indicates the prototype of -th class and  is the dimension of prototype embedding.
Our DNN model consists of a CNN backbone, a classifier head and a projection layer. The backbone maps an image input  to a feature vector . The classifier takes  as input and outputs class prediction . The projection layer serves to project the high dimension feature  to a low-dimensional embedding , where . 

As shown in Fig.~\ref{fig:pipeline}, we update the DNN as well as the CPC by iterating a two-stage training pipeline in every epoch. In the first stage, we update CPC as well as the projector in DNN, and utilize the updated CPC to partition label noise. We first calculate the cross-entropy loss of every training sample and fits a GMM to the losses. We utilize the GMM as a label noise cleaner to get a labeled clean set  and a unlabeled noise set . The data partition  and  are utilized to update the prototypes in CPC and parameters in the projector. Note that we cut off the gradient back-propagation from the projector to the CNN backbone. Then, the updated CPC is employed to re-divide the training data into another two set  and . In the second stage, we train DNN model to minimise the EVR in Eq.~(\ref{loss_EVR}) with data partitioned by the cleaner. In the first  epochs, we wait CPC to warm up, and minimise the EVR of DNNs based on training data partitioned by the GMM cleaner. After the -th epoch, the label noise estimation results of CPC, \emph{i.e.,}  and  are employed to train DNNs, while the estimation results of GMM cleaner are only used to update prototypes in CPC. In inference, we utilize DNN classifier for image recognition, directly. In \ref{app:alg}, we further delineate the full framework.

\subsection{Class Prototype-based Label Noise Cleaner}
In order to apply class-aware modulation to the label noise partitioning, we propose to learn an embedding space where samples from the same class are aligned with their class prototypes, and leverage the prototypes to recognize noise labels. The prototypes are typically learnt with intra-class consistency regularization, which urges samples in the same class to align with the corresponding class prototype while keeping samples not belonging to the class away. Previous methods \citep{wang2022pico,li2020mopro} apply the intra-class consistency regularization to prototype learning via  unsupervised contrastive objectives, \emph{e.g.,} prototypical contrastive objective \citep{li2020prototypical}, where the unsupervised training labels are typically determined by the similarity between samples and prototypes. The 
accuracy of the training labels are highly depends on the quality of representation learnt by the CNN encoder, which would be too low to effectively update the prototypes, especially in the early stage of training. In contrast, we empirically find that the GMM cleaner, which is operated under the well evaluated ``small-loss prior'', are not as sensitive as the prototypes to the representation quality, and can provide more robust and accurate training labels. 

Therefore, we propose to take samples in clean set  as positive samples and those in noise set  as negative samples to update prototypes. Specifically, given the feature embedding  of a sample  from , we update prototypes  as well as the parameters of the projector to maximize the score  between  and , and minimize the score between  and  via minimize : 

where  weights the losses between positive pair and negative pairs to avoid under-fitting the positive samples. Given  of a sample  from , we update prototypes  as well as the parameters of the projector to minimize the score  between  and  via minimizing :

At last, for noise samples in  with high classification confidence, the samples are more likely to belong to the class predicted by DNNs, which is potentially valuable to the update of prototypes. Therefore, we collect such training samples  from  taking the averaged classification confidence of samples in  as the threshold. Specifically, given a sample in  with the label predicted by DNNs , the sample is collected into  if   . Then, we update the prototypes and projectors to minimize :

The overall empirical risk  for prototypes and the projector is as follows:

where  is the weight scalar.

 CPC distinguishes a clean sample  with the score  and the threshold . Samples with  are classified as clean, and otherwise as noise.



\subsection{Theoretical Justification on the Efficacy of CPC} \label{sec:em}
We provide theoretical justification on the efficacy of CPC from the perspective of Expectation-Maximization algorithm, which guarantees that though CPC does not follow the classical prototypical contrastive objective, it can  still learn meaningful prototypes and act as an effective cleaner. 

We consider training data with label noise  as the observable data, and  as the latent variable, where  \emph{iff}  is clean (\emph{i.e.,} ). The prototypes  in the cleaner are taken as parameters expected to be updated. Then, the negative log likelihood for  given  is as follows:

where . According to the Bayes theorem and Jensen's inequality , we have

where  is the upper bound of . Typically, we can adopt the EM algorithm to find the prototypes  that minimize the upper bound by iterating:

\textbf{E-step}: Compute a new estimate of  (\emph{i.e.,} clean or noise) according to prototypes  from the last iteration:
    
\textbf{M-step}: Find the prototypes  that minimizes the bound:
    
In our method, in order to introduce the ``small-loss prior'' to provide stronger and more robust supervision signals to the learning of CPC, in the \textbf{E-step}, we estimate the distribution of clean or noise of samples, denoted as , via the GMM cleaner instead of  in Eq.~(\ref{estep}). And consequently, we replace the  in  Eq.~(\ref{mstep}) to  and find the prototype  minimize the bound. Next, we provide the justification that the EM algorithm still work by proving that  can be considered as an approximation to  in our framework.



In our method, , where , 
which is the label predicted by the DNN parameterized by .
As introduced in section~\ref{overview}, in the first stage of each epoch, the CPC's estimation results  are utilized to divide training samples into a labeled set for clean  data  and an unlabeled set for noise data .
Then the parameters of DNNs, which we denote as , are optimized using Eq.~(\ref{loss_EVR}) in the second stage.
There exists an optimal  with respected to , with which the softmax output  of DNNs satisfies:

where  is the cross-entropy loss between the network prediction and the annotated label.
With these loss values,
the subsequent GMM cleaner can easily distinguish samples of  from samples of .
In other words,
under the optimal ,
the estimation of the GMM cleaner would be consistent with the partition of CPC, \emph{i.e.,} . In practice, in each epoch, we takes the  optimized to minimize Eq.~(\ref{loss_EVR}) as an approximation to the optimal  with respect to , and consequently we can get  as an approximation to . Therefore, we can see that with the ``small loss prior'' introduced into the prototype learning, the EM optimization procedure would still work, which guarantees CPC can learn meaningful prototypes and act as an effective cleaner.
In appendix~\ref{app:kl}, we further present more details and empirical results to demonstrate the approximation is hold in practice. 




\section{Experiments}\label{sec:exp}
\subsection{Datasets and Implementation Details}
\textbf{Datasets.} We evaluate our method on the following popular LNL benchmarks. For CIFAR-10 and CIFAR-100 \citep{krizhevsky2009learning}, we experiment with two types of synthetic noise: symmetric and asymmetric, which are injected into the datasets following the standard setup in \citep{Li2020DivideMixLW}. Clothing1M \citep{xiao2015learning} and WebVision1.0 \citep{li2017webvision} are two large-scale real-world label noise benchmarks. Clothing1M contains 1 million images in 14 categories acquired from online shopping websites, which is heavily imbalanced and most of the noise is asymmetric \citep{yi2019probabilistic}. WebVision1.0 contains 2.4 million images crawled from the web using the concepts in ImageNet-ILSVRC12 (ILSVRC12). Following convention, we compare with SOTAs on the first 50 classes of WebVision, as well as the performance after transferring to ILSVRC12.  

\textbf{Implementation details.} We plug the proposed CPC to the DivideMix \citep{Li2020DivideMixLW} framework. For Clothing1M and CIFAR-10 with asymmetric noise, we employ a single class-agnostic GMM for loss-distribution modeling. For other cases, we find that  class-aware GMMs would further improve the performance of CPC. Following DivideMix, we employ ResNet18 \citep{he2016identity} for CIFAR-10 and CIFAR-100, and utilize ImageNet pre-trained ResNet-50 for Clothing1M. Since previous works chose different backbones, \emph{e.g.}, Inception-resnet v2 \citep{szegedy2017inception} and ResNet-50, we adopt the weaker one, \emph{i.e.,} ResNet-50 according to \citep{DBLP:journals/corr/abs-2103-13646}, and train it from scratch for fair comparison. The threshold of CPC  is set  by default for all the datasets except for the extremely imbalanced Clothing1M where it is set to . For CIFAR-10 and CIFAR-100, we train the models for 450 epochs. For the large-scale dataset Clothing1M and WebVision1.0, we train the model for 80 and 100 epochs, respectively. The warm-up periods of prototypes for all the datasets is set to the first 5\% epochs after network warm-up, except in CIFAR-100 with noise ratios larger than 80\% when set to 10\% of total epochs. For the other settings, we simply follow the standard set-up as in DivideMix. For more implementation details, please refer to the appendix \ref{app:conig} and codes in supplementary materials.
\begin{table}[h]
\vspace{-1\baselineskip}
\caption{Comparison with SOTAs on Real-world Benchmarks. Following GJS\citep{englesson2021generalized}, we run our method three times with different random seeds and report the mean and standard deviation of classification accuracy.  indicates methods utilize ResNet50 for WebVision, while others utilize Inception-resnet v2. The best results are indicated with  boldface.}
\label{table-sota-real}
\small
\begin{center}
\begin{tabular}{c|cc|cc|c}
& \multicolumn{2}{c|}{WebVision} & \multicolumn{2}{c|}{WebVision  ILSVRC12} & Clothing1M  \\
 & top1 & top5 & top1 & top5 &  \\ \midrule
ELR+ & 77.78 & 91.64 & 70.29 & 89.76 & 74.8 \\
DivideMix & 77.32 & 91.64 & 75.2 & 90.84 & 74.76 \\
DivideMix &  &  &  &  & 74.76 \\
LongReMix & 78.92 & 92.32 & - & - & 74.38 \\
NGC & 79.16 & 91.84 & 74.44 & 91.04 & - \\
AugDMix & - & - & - & - & 75.11 \\
NCR & \textbf{80.5} & - & - & - & 74.6 \\
GJS &  &  &  &  & - \\ \midrule
Baseline &  &  &  &  &  \\
Ours &  & \textbf{93.46} & \textbf{75.75} & \textbf{93.49} & \textbf{75.40} 
\end{tabular}\end{center}
\vspace{-2\baselineskip}
\end{table}

\subsection{Comparison with State-of-the-art methods}
\textbf{Real-world noise benchmarks.} We evaluate our method on real-world large scale data sets, and compare our method with latest SOTA label noise learning methods, including DivideMix\citep{Li2020DivideMixLW}, LongReMix\citep{cordeiro2022longremix}, NGC\citep{wu2021ngc}, GJS\citep{englesson2021generalized}, ELR+\citep{liu2020early}, AugDMix\citep{nishi2021augmentation} and NCR\citep{huang2021learning}. For WebVision, we measure the top1 and top5 accuracy on WebVision validation set and ImageNet ILSVRC12 validation set. We take ResNet50-based DivideMix \citep{DBLP:journals/corr/abs-2103-13646} as baseline. As shown in Table~\ref{table-sota-real}, our CPC improves top1 and top5 accuracy over baseline model on WebVision by 3.33\% and 2.81\%, respectively. Our method achieves competitive performance on WebVision, and shows stronger transferable capability,  outperforming other competitors on the ILSVRC12 validation set significantly. For Clothing1M, we apply the strong augmentation strategy \citep{nishi2021augmentation} to DivideMix as our baseline, and rerun the method three times. Our method achieves 75.4\% accuracy on this challenging benchmark, outperforming all the other SOTAs. We also notice that though NCR achieves SOTA result on WebVision, it shows moderate performance compared to ELR+, DivideMix and AugDMix on Clothing1M  containing asymmetric noise with imbalanced data distribution. It reveals that our method could be more robust across different label noise scenarios.

\begin{table}[t]
\vspace{-1\baselineskip}
\caption{Comparison with SOTAs on CIFAR-10 and CIFAR-100. Following previous work \citep{ wu2021ngc}, we run our method three times with different random seeds and report the mean and standard deviation of classification accuracy.  indicates our baseline. * indicates semi-supervised learning based label noise learning methods. SOTA results are indicated with  boldface.}
\label{table-sota-cifar}
\small
\begin{center}
\begin{tabular}{c|cccc|c}
& \multicolumn{4}{c|}{CIFAR-10 / CIFAR-100 (Sym)} & CIFAR-10 (Asym) \\
& 20\% & 50\% & 80\% & 90\% & 40\% \\ \midrule
ELR+ & 94.9  / 76.3  & 93.9. / 72.0 & 90.9. / 57.2 & 74.5  / 30.9 & 88.9 \\
NCR & 95.2  / 76.6 & 94.3 / 72.5 & 91.6  / 58.0 & 75.1  / 30.8 & 90.7 \\
ProtoMix & 96.4  / \textbf{80.3} & 95.3 / 76.0 & 93.3  / 61.1 & 77.4  / 33.1 & 92.6 \\
DivideMix* & 96.1  / 77.3 & 94.6  / 74.5 & 93.2  / 60.2 & 76.0  / 31.5 & 93.4 \\
LongReMix* & 96.2  / 77.8 & 95.0  / 75.6 & 93.9  / 62.9 & 82.0  / 33.8 & 94.7 \\
AugDMix*\dagger & 96.3  / 79.5 & 95.6  / 77.2 & 93.6  / 66.4 & 91.9  / 41.2 & 94.6 \\ \midrule
\multirow{2}{*}{NGC} & 95.88 / 79.31 & 94.54 / 75.91 & 91.59 /62.7 & 80.46 / 29.76 & 90.55 \\
  &  /  &  /  &  /  &  /  &   \\
\multirow{2}{*}{GJS} & 95.33 / 75.71 & - & 79.11 / 44.49 & - & 89.65 \\
 &  /  & - &  /  & - &  \\
\multirow{2}{*}{Ours*} & \textbf{96.50} / 80.22 & \textbf{95.64} / \textbf{79.31} & \textbf{94.78} / \textbf{69.56} & \textbf{92.55} / \textbf{54.60} & \multicolumn{1}{c}{\textbf{94.73}} \\
 &  /   &  /    &  /  &  /  &  \end{tabular}
\vspace{-2\baselineskip}
\end{center}
\end{table}

\textbf{Synthetic noise benchmarks.} We evaluate the performance of CPC on CIFAR-10 and CIFAR-100 datasets with symmetric label noise level ranging from 20\% to 90\% and asymmetric noise of rate 40\%. We take AugDMix as the baseline, and compare our method with latest SOTA methods, where DivideMix, LongReMix and Aug-DMix are semi-supervised learning based methods. Following NGC and GJS, we run our method three times with different random seeds and report the mean and standard deviation. For other methods, \emph{e.g.,} ProtoMix \citep{li2021learning}, we report the best results reported in their papers. As shown in Table~\ref{table-sota-cifar}, though with a baseline method as strong as AugDMix, our method brings about performance improvement across all noise levels as well as noise types consistently, and establishes new SOTAs on CIFAR-10 and CIFAR-100. Additionally, we notice that, under asymmetric noise set-up, semi-supervised learning based methods consistently outperform other methods that achieve SOTA results on WebVision benchmark,including NGC, GJS and NCR. The results reveal that semi-supervised learning based method could be more robust to asymmetric noise, while our method achieves SOTA performance among them.


\subsection{Analysis}
\textbf{Is CPC a better label noise cleaner?} We evaluate the performance of label noise cleaner under both symmetric and asymmetric label noise set-ups. For symmetric noise, we use CIFAR-100 with 90\% noise as benchmark to reveal the relationship between CPC and the significant performance improvement under this set-up. For asymmetric noise, we employ the most commonly adopted CIFAR-10-asym40\% as benchmark. The AUC of clean/noise binary classification results of a cleaner is calculated as the evaluation metric. We take the original class-agnostic GMM cleaner (GMM) proposed in DivideMix as baseline, and compare it to our CPC and the aforementioned naive class-aware GMM cleaner (GMM). Furthermore, we also implement another version of CPC that trained based on the class-aware GMM cleaner. To distinguish these two CPC, we denote the regular one trained based on conventional class-agnostic GMM cleaner as CPC, and the other one as CPC. As shown in Figure~\ref{fig:auc}, in both cases, the regular CPC outperforms the baseline GMM as well as GMM, which demonstrates our class prototype-based method is the better label noise cleaner. As for the comparison between GMM and GMM, we find that in the situation of high symmetric noise, though GMM shows better performance in the early stage of training, GMM outperforms it in the second half stage of training. In the case of asymmetric noise, GMM, which tend to classify hard clean samples in clean categories as noise wrongly, consistently underperforms GMM across the whole training period. The results further prove that our class prototype-based method is the better choice for applying class-aware modulation to label noise cleaning, which is more robust across different noise types. Moreover, we find that in the case of asymmetric noise, CPC achieves higher AUC compared to GMM, which shows our method can partially make up for the shortcomings of GMM. In the case of symmetric noise, we find that GMM can further improve the performance of CPC, where CPC achieves the best performance among the four cleaners.




\textbf{How do different label noise cleaners affect label noise learning?} We plug different cleaners to DivideMix framework, and keep all the other training settings the same as described in the implementation details. As shown in Table~\ref{table-ablation}, the final performance of the model is consistent with the performance of the cleaner used. On CIFAR-100 with 90\% symmetric noise, performance improvement bought about by CPC are 7.68\%, while model with CPC outperforms the baseline method by 13.4\%. We also report the comparison results on large-scale WebVision dataset, where the performance of different models show the same trend of change as in CIFAR-100-sym90\%. As for the asymmetric noise situation, \emph{i.e.,} CIFAR-10-asym40\% and Clothing1M, model with CPC, which has superior label noise partitioning capability as shown in Fig.\ref{fig:auc}, achieves best performance while CPC beat GMM in both cases.  The results demonstrate that CPC is helpful to train a better model in label noise learning.
\begin{figure}[t]
\centering
  \includegraphics[width=1\columnwidth]{figure/auc}
    \vspace{-2\baselineskip}
  \caption{ AUC of different label noise cleaners with respect to the training period. Left is the results on CIFAR-100 under high symmetric noise ratio 0.9. Right is the results on CIFAR-10 under medium asymetric noise ratio 0.4.}
   \vspace{-1\baselineskip}
   \label{fig:auc}
\end{figure}

\begin{table}[t]
\caption{The affect of different label noise cleaner to the final classification accuracy. The best results are indicated with  boldface.}
\label{table-ablation}
\small
\begin{center}
\begin{tabular}{c|c c c c}
\multirow{2}{*}{Model}& \multicolumn{1}{c}{CIFAR-100} & {WebVision} &  CIFAR-10 & \multirow{2}{*}{Clothing1M} \\
 & sym-90\% & top1 & asym-40\%  &\\ \midrule
w/ GMM & 41.2 &76.32 & 94.6 & 74.73  \\
w/ GMM & 45.6 & 78.66 & 94.18 & 74.18 \\
w/ CPC & 48.88  & 79.4 & \textbf{94.73} & \textbf{75.4} \\
w/ CPC & \textbf{54.6}  & \textbf{79.63} & 94.29 & 74.36  \end{tabular}\end{center}
\vspace{-2\baselineskip}
\end{table}

\textbf{Is the GMM cleaner beneficial to the learning of prototypes?}
In our method, we propose to leverage the GMM cleaner to facilitate the learning of prototypes via the ``small loss prior''. To validate the effectiveness of our method, we first compare the quality of prototypes learnt in CPC with prototypes learnt in another prototype-based label noise learning method MoPro \citep{li2020mopro}. We take WebVision as benchmark and utilize prototypes to classify test samples via measuring the similarity between samples and prototypes. The results show that, on the first 50 classes of WebVision, our prototype achieves a top1 accuracy of 78.44\%, while MoPro's accuracy is 72.23\%, which demonstrates that our method is able to learn better prototypes.
To further verify the contribution of the GMM cleaner, we remove the GMM cleaner and learn class prototypes in CPC via the typical prototypical contrastive objective as in MoPro. In experiments, we find that without the help of the GMM cleaner, the learnt prototypes generate less accurate data partition that further drawing back the overall training framework for DNNs, which proves the benefits of the GMM cleaner to our method. For more details and discussion, please refer to \ref{app:abl}.





\section{Conclusion}
In this paper, we reveal the long-ignored problem of \emph{class-agnostic} loss distribution modeling that widely existed in label noise learning, and propose a simple yet effective solution, named  Class Prototype-based label noise Cleaner (CPC). CPC takes advantage of loss distribution modeling and intra-class consistency regularization in feature space simultaneously, which can  better distinguish clean and noise labels. We  justify the effectiveness of our method by explaining it from the EM algorithm perspective theoretically and providing extensive empirical proves. The experimental results show that our method achieves competitive performance compared to current SOTAs. 


\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\appendix
\section{Appendix}
\subsection{Empirical Vicinal Risk} \label{app:evr}
We introduce the Empirical Vicinal Risk following \citet{cordeiro2022longremix}. In the semi-supervised learning based label noise learning framework, with the labeled set  and unlabeled set  from a cleaner, the DNNs are trained to minimise the empirical vicinal risk (EVR) \citep{zhang2017mixup}:

where  and  denote the losses for set  and , which are weighted by .  and  indicate MixMatch \citep{berthelot2019mixmatch} augmented clean and noise set:

with

where  is a Dirac mass centered at , , and   (0, +).


\subsection{Other Training Details} \label{app:conig}
\subsubsection{Training configurations}
In our method, we follow most of training set-up of DivideMix\citep{Li2020DivideMixLW}. We present the detailed training configures as follows:
\begin{itemize}
    \item \textbf{CIFAR-10 and CIFAR-100}. For all the experiments on CIFAR, we train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 128. The network is trained for 450 epochs. We set the initial learning rate as 0.02, and reduce it by a factor of 10 after 225 epochs. The warm up period for the DNN is 10 epochs. The weight  is set to  as in DivideMix.
    \item \textbf{Clthing1M}. We train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 32. The model is trained for 80 epochs. The warm up period for the DNN is 1 epoch. The initial learning rate is set as 0.002 and reduced by a factor of 10 after 40 epochs. For each epoch, we sample 1000 mini-batches from the training data. The weight  is set to 0.
    \item \textbf{WebVision}. We train our DNN model as well as class prototypes in CPC via SGD with a momentum of 0.9, a weight decay of 0.001, and a batch size of 32. The model is trained for 100 epochs. The warm up period for the DNN is 1 epoch. The initial learning rate is set as 0.01 and reduced by a factor of 10 after 50 epochs. For each epoch, we sample 1000 mini-batches from the training data. The weight  is set to 0.
\end{itemize}

\subsubsection{Hyper-parameter Study}
In this paper, we mainly follow the tuning procedure as in DivideMix to determine the newly introduced hyper-parameters. First of all, we initialize the hyper-parameters to . Then,  for the large scale real world benchmark Clothing1M and WebVision, the hyper-parameter tuning is done on the validation set of Clothing1M and transferred to WebVision. For CIFAR, a small validation set with clean data is split from training data for hyper-parameter tuning. Due to the diversity of experimental set-ups, it would be an irritating task to tune hyper-parameters for each experimental set-up, respectively. Therefore, we only tune the hyper-parameters under CIFAR-100(sym80\%) and CIFAR-100(sym50\%), and transfer the hyper-parameters obtained  under CIFAR-100(sym80\%) to the noisier set-up \emph{i.e.,} CIFAR-100(sym90\%), and those obtained under CIFAR-100(sym50\%) to the less challenge set-ups \emph{i.e.,} noise ratio lower than 50\%  and all noise ratio on CIFAR-10. 

In practical, when a clean validation set is inaccessible, it would be the difficult to tune the hyper-parameters. To shed some light to the hyper-parameter set-up in these cases, we try to conclude some empirical solutions via studying the variation of performance of CPC with respect to the newly introduced hyper-parameters on different benchmarks. According to experimental results, we find that CPC is robust in the choice of hyperparameters in the range listed in Tab.\ref{tab:hyper}. Generally,  can be a good choice in most cases.

\begin{table}[]
\caption{The variation of performance of CPC with respect to the change of hyper-parameters. The classification accuracy of DNNs is reported. The best results are indicated with  boldface. }\label{tab:hyper}
\resizebox{\columnwidth}{!}{\begin{tabular}{l|llllllllll}
\hline
 & \multirow{2}{*}{baseline} & \multicolumn{3}{l}{CPC Warm-up epochs (e)} & \multicolumn{3}{l}{CPC threshold ()} & \multicolumn{3}{l}{Prototypical loss weight ()} \\
 &  & 5\% & 10\% & 15\% & 0.5 & 0.6 & 0.7 & 0 & 0.5 & 1 \\ \hline
CIFAR-100(sym90\%) & 41.2 & 52.32 & \textbf{54.60} & 53.7 & \textbf{54.60} & 54.33 & 54.05 & \textbf{54.60} & 54.48 & 54.51 \\
WebVision & 76.3 & \textbf{79.63} & 79.32 & 79.04 & \textbf{79.63} & 79.52 & 79.36 & 79.16 & 79.44 & \textbf{79.63} \\
CIFAR-10(asym40) & 94.60 & \textbf{94.73} & 94.68 & 94.59 & \textbf{94.73} & 94.71 & 94.65 & \textbf{94.73} & 94.68 & 94.72 \\
Clothing1M & 74.73 & \textbf{75.40} & 75.04 & 74.89 & 75.08 & 75.15 & \textbf{75.40} & 75.35 & 75.28 & \textbf{75.40}
\end{tabular}}
\end{table}

\subsection{Discussion on the contribution of GMM cleaner to CPC} \label{app:abl}


  In typical prototypical contrastive objective, the unsupervised training labels are determined by similarity between samples and prototypes. Compared to it, we empirically find that GMM cleaner provides more accurate training labels for prototypes, especially in the early stage of training. For example, in CIFAR-10(asym-40\%), the averaged accuracy of training labels from GMM cleaner is 9.7\% higher during the CPC warming up period. 
  
  To evaluate the contribution of GMM cleaner in our framework, we further present ablation study results in Tab.~\ref{tab:abl}. For \emph{CPC w/o GMM Cleaner}, we remove the GMM cleaner and learn class prototypes in CPC with prototypical contrastive objective as in MoPro \citep{li2020mopro}. In experiments, we find that without the help of the GMM cleaner, the learnt prototypes generate less accurate data partition that further drawing back the overall training framework for DNNs as shwon in Tab.~\ref{tab:abl}. The situation is especially severe on the challenging benchmark with more diverse data, \emph{e.g.,} WebVision.  The results demonstrate the benefits of the GMM cleaner in our method. 
  
  To prove the superiority of our method, we also compare the quality of prototypes learnt in our method with prototypes learnt in MoPro \citep{li2020mopro} on the first 50 classes of WebVision.
  To evaluate the quality of prototypes learnt in CPC, we utilize the prototypes to classify test samples via measuring the similarity between samples and prototypes. We implement the experiment with the official code released by the MoPro team. The results show that our prototype achieves a top1 accuracy of 78.44\%, while MoPro's accuracy is 72.23\%. The result demonstrates that our method is able to learn better prototypes.
  
\begin{table}[]
\caption{Ablation study on the contribution of GMM cleaner.The classification accuracy of DNNs is reported. The best results are indicated with  boldface.}\label{tab:abl}
\resizebox{\columnwidth}{!}{\begin{tabular}{l|llll}
method & CIFAR-100(sym90\%) & WebVision & CIFAR-10(asym40\%) & Clothing1M \\ \hline
Baseline & 41.2 & 76.3 & 94.6 & 74.73 \\
CPC w/o GMM Cleaner & 42.9 & 26.8 & 93.92 & 74.09 \\
CPC & \textbf{54.6} & \textbf{79.63} & \textbf{94.73} & \textbf{75.4}
\end{tabular}}
\end{table}

\subsection{Supplementary discussion on the theoretical justification}
\label{app:kl}
\subsubsection{Is  a proper approximation to  in practical?} 
In Section~\ref{sec:em}, we replace the estimation of CPC  in Eq.~(\ref{mstep}) with the estimation of GMM cleaner  and justify  can be considered as an approximate to . To investigate if the approximation holds in practical, we calculate the K-L Divergence as well as classification consistency between  and . As shown in Figure~\ref{fig:kl}, as the training going on, the KLD  between  and  is converged and the classification consistency increases. 
\begin{figure}[t]
\centering
  \includegraphics[width=1\columnwidth]{figure/kl}
    \vspace{-2\baselineskip}
  \caption{ The left figure shows the KLD between  and . The right figure presents the consistency rate between  and . Results are collected from CIFAR-10-aysm40\%. }
\label{fig:kl}
\end{figure}

\subsubsection{Training prototypes with \texttt{} is an approximation to the M-step in EM} \label{app:loss}
As illustrated in Section \ref{sec:em},  in order to introduce the ``small-loss prior'' to provide stronger and more robust supervision signals to the learning of CPC, in the \textbf{E-step}, we estimate the probability distribution of clean or unclean of samples, denoted as , via the GMM cleaner, which is an approximation to the  in Eq.~(\ref{estep}). And consequently, we replace the  in  Eq.~(\ref{mstep}) with  and find the prototype  to minimize the bound, which makes the loss function  in Eq.~(\ref{fullloss}) an approximation to Eq.~(\ref{mstep}). The detailed analysis on the relationship between Eq.~(\ref{fullloss}) and Eq.~(\ref{mstep}) is as follows.

Firstly, we replace the estimation of CPC  in Eq.~(\ref{mstep}) with the estimation of GMM cleaner  which is a justified approximate to :

 In Eq.~(\ref{fullloss}),  is quantified to 1 and 0 by the threshold , which makes it a ``hard" version to  Eq.~(\ref{app-mstep}). Specifically, the first term in Eq.~(\ref{app-mstep}) updates the prototypes  to better align the samples, that classified as clean, with labeled class prototypes. It is equivalent with the effect of Eq.~(\ref{fullloss}) to positive samples, where:
 
 where  is the embedding of sample .
 The second term in Eq.~(\ref{app-mstep}) updates  to prevent the samples, that classified as noise, aligning with labeled class prototypes so as to better recognize the sample as noise (\emph{i.e., }), which is equivalent with the effect of Eq.~(\ref{fullloss}) reducing the probability of negative samples to be recognized as clean:
 


\subsection{Illustration to the overall framework} \label{app:alg}
In this paper, we plug CPC to the popular DivideMix framework. We delineate the overall training framework in Alg.\ref{alg1}.

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{CPC based DivideMix}
	\label{alg1}
	\begin{algorithmic}[1]
		\STATE \textbf{Input}: Dataset , DNNs , , CPC with class prototypes , , clean probability , CPC warm-up period .
		\STATE   WarmUp, WarmUp      \emph{//standard training to warm-up DNNs}
		\WHILE{  MaxEpoch}
	\STATE \emph{// get GMM cleaners by loss distribution modeling and calculate clean/noise probability distribution }
		\STATE GMM
		\STATE GMM
	\STATE \emph{// calculate clean/noise probability distribution via CPC}
		\STATE CPC
		\STATE CPC
		\FOR{r } 
		\STATE \emph{// stage1 begin}
		\STATE 
		\STATE 
		\STATE Get noise labels 
		\STATE Update  based on Eq.\ref{fullloss}
		\STATE \emph{// stage1 end}
		\STATE \emph{// stage2 begin}
	    \IF{}
        \STATE ,  \emph{//use data partition from GMM cleaner to update DNNs during the CPC warm-up period}
        \ELSE
        \STATE 
		\STATE 
	    \ENDIF
	    \STATE Update  based on Eq.\ref{eq:app:evr} as in standard DivideMix
	    \STATE \emph{// stage2 end}
		\ENDFOR
		\STATE 
		\ENDWHILE
		\ENSURE  DNNs ,  
	\end{algorithmic}  
\end{algorithm}

\end{document}
