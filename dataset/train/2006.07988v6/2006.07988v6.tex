
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsfonts,bm}
\usepackage{nicefrac}       \usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[center]{subfigure}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{booktabs}       \usepackage{wrapfig}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\title{Adaptive Universal Generalized PageRank Graph Neural Network}



\author{Eli Chien \& Jianhao Peng\thanks{equal contribution} \\
Department of Electrical and Computer Engineering\\
University of Illinois Urbana-Champaign, USA \\
\texttt{\{ichien3,jianhao2\}@illinois.edu} \\
\AND
Pan Li \\
Department of Computer Science \\
Purdue University, USA \\
\texttt{panli@purdue.edu} \\
\And
Olgica Milenkovic \\
Department of Electrical and Computer Engineering\\
University of Illinois Urbana-Champaign, USA \\
\texttt{milenkov@illinois.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ip}[2]{\left\langle #1, #2 \right \rangle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\pan}[1]{\textcolor{red}{P: #1}}
\newcommand{\eli}[1]{\textcolor{olive}{#1}}
\newcommand{\jianhao}[1]{(Jianhao:\textcolor{blue}{#1})}
\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-of-the-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data. Our implementation is available online.\footnote{https://github.com/jianhao2016/GPRGNN}
\end{abstract}

\section{Introduction}
Graph-centered machine learning has received significant interest in recent years due to the ubiquity of graph-structured data and its importance in solving numerous real-world problems such as semi-supervised node classification and graph classification ~\citep{zhu2005semi,shervashidze2011weisfeiler,lu2011link}. Usually, the data at hand contains two sources of information: Node features and graph topology. As an example, in social networks, nodes represent users that have different combinations of interests and properties captured by their corresponding feature vectors; edges on the other hand document observable friendship and collaboration relations that may or may not depend on the node features. Hence, learning methods that are able to simultaneously and adaptively exploit node features and the graph topology are highly desirable as they make use of their latent connections and thereby improve learning on graphs.

Graph neural networks (GNN) leverage their representational power to provide state-of-the-art performance when addressing the above described application domains. Many GNNs use message passing~\citep{gilmer2017neural,battaglia2018relational} to manipulate node features and graph topology. They are constructed by stacking (graph) neural network layers which essentially propagate and transform node features over the given graph topology. Different types of layers have been proposed and used in practice, including graph convolutional layers (GCN)~\citep{bruna2014spectral,kipf2017semi}, graph attention layers (GAT)~\citep{velickovic2018graph} and many others~\citep{hamilton2017inductive,wijesinghe2019dfnets,graphsaint-iclr20,abu2019mixhop}.


However, most of the existing GNN architectures have two fundamental weaknesses which restrict their learning ability on general graph-structured data. First, most of them seem to be tailor-made to work on \emph{homophilic (associative) graphs}. The homophily principle~\citep{mcpherson2001birds} in the context of node classification asserts that nodes from the same class tend to form edges. Homophily is also a common assumption in graph clustering~\citep{von2007tutorial,tsourakakis2015provably,dau2017latent} and in many GNNs design~\citep{klicpera2018predict}. Methods developed for homophilic graphs are nonuniversal in so far that they fail to properly solve learning problems on \emph{heterophilic (disassortative) graphs}~\citep{pei2019geom,bojchevski2019pagerank,bojchevski2020scaling}. In heterophilic graphs, nodes with distinct labels are more likely to link together (For example, many people tend to preferentially connect with people of the opposite sex in dating graphs, different classes of amino acids are more likely to connect within many protein structures~\citep{zhu2020generalizing} etc). GNNs model the homophily principle by aggregating node features within graph neighborhoods. For this purpose, they use different mechanisms such as averaging in each network layer. Neighborhood aggregation is problematic and significantly more difficult for heterophilic graphs~\citep{jia2020outcome}.

Second, most of the existing GNNs fail to be ``deep enough''. Although in principle an arbitrary number of layers may be stacked, practical models are usually shallow (including - layers) as these architectures are known to achieve better empirical performance than deep networks. A widely accepted explanation for the performance degradation of GNNs with increasing depth is \emph{feature-over-smoothing,} which may be intuitively explained as follows. The process of GNN feature propagating represents a form of random walks on ``feature graphs,'' and under proper conditions, such random walks converge with exponential rate to their stationary points. This essentially levels the expressive power of the features and renders them nondiscriminative. This intuitive reasoning was first described for linear settings in~\citet{li2018deeper} and has been recently studied in~\citet{oono2019graph} for a setting involving nonlinear rectifiers.

We address these two described weaknesses by combining GNNs with Generalized PageRank techniques (GPR) within a new model termed GPR-GNN. The GPR-GNN architecture is designed to first learn the hidden features and then to propagate them via GPR techniques. The focal component of the network is the GPR procedure that associates each step of feature propagation with a \emph{learnable weight.} The weights depend on the contributions of different steps during the information propagation procedure, and they can be both positive and negative. This departures from common nonnegativity assumptions~\citep{klicpera2018predict} allows for the signs of the weights to adapt to the homophily/heterophily structure of the underlying graphs. The amplitudes of the weights trade-off the degree of smoothing of node features and the aggregation power of topological features. These traits do not change with the choice of the initialization procedure and elucidate the process used to combine node features and the graph structure so as to achieve (near)-optimal predictions. In summary, the GPR-GNN method can simultaneously learn the node label patterns of disparate classes of graphs and prevent feature over-smoothing.


The excellent performance of GPR-GNN is demonstrated empirically, on real world datasets, and further supported through a number of theoretical findings. In the latter setting, we show that the GPR procedure relates to general polynomial graph filtering, which can naturally deal with both high and low frequency parts of the graph signals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights~\citep{wu2019simplifying,klicpera2018predict,klicpera2019diffusion} inevitably act as low-pass filters. Thus, they fail to learn the labels of heterophilic graphs. We also establish that GPR-GNN can provably mitigate the feature-over-smoothing issue in an adaptive manner even after large-step propagation (i.e., after a large number of propagation steps). Hence, the method is able to make use of informative large-step propagation.


To test the performance of GPR-GNN on homophilic and heterophilic node label patterns and determine the trade-off between node and topological feature exploration, we first describe the recently proposed \emph{contextual stochastic block model} (cSBM)~\citep{deshpande2018contextual}. The cSBM allows for smoothly controlling the ``informativeness ratio'' between node features and graph topology, where the graph can vary from being highly homophilic to highly heterophilic. We show that GPR-GNN outperforms all other baseline methods for the task of semi-supervised node classification on the cSBM consistently from strong homophily to strong heterophily. We then proceed to show that GPR-GNN offers state-of-the-art performance on node-classification benchmark real-world datasets which contain both homophilic and heterophilic graphs. Due to the space limit, we put all proofs, formal theorem statements, and the conclusion section in the Supplement.


\section{Preliminaries}\label{sec:prelim}

Let  be an undirected graph with nodes  and edges . Let  denote the number of nodes, assumed to belong to one of  classes. The nodes are associated with the node feature matrix  where  denotes the number of features per node. Throughout the paper, we use  to indicate the  row and  to indicate the  column of the matrix , respectively. The symbol  is reserved for the Kronecker delta function. The graph  is described by the adjacency matrix , while  stands for the adjacency matrix for a graph with added self-loops. We let  be the diagonal degree matrix of  and  denote the symmetric normalized adjacency matrix with self-loops.


\begin{figure*}[t]
  \centering
  \subfigure[Illustration of our proposed GPR-GNN model.]{\includegraphics[trim={2.5cm 5cm 11cm 5.5cm},clip,width=0.5\linewidth]{GP-GNN.pdf}}
\subfigure[Cora, ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.24\linewidth]{Cora_gamma.pdf}}
  \subfigure[Texas, ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.24\linewidth]{Texas_gamma.pdf}}
  \caption{(a) Hidden state feature extraction is performed by a neural networks using individual node features propagated via GPR. Note that both the GPR weights  and parameter set  of the neural network are learned simultaneously in an end-to-end fashion (as indicated in red). (b)-(c) The learnt GPR weights of the GPR-GNN on real world datasets. Cora is homophilic while Texas is heterophilic (Here,  stands for the level of homophily defined below). An interesting trend may be observed: For the heterophilic case the weights alternate from positive to negative with dampening amplitudes (more examples are provided in Section~\ref{sec:cSBM_intro}). The shaded region corresponds to a  confidence interval.} \label{fig:GPR-GNN}
  \vspace{-0.4cm}
\end{figure*}

\section{GPR-GNNs: Motivation and Contributions}\label{sec:GPRandGPGNN}
\textbf{Generalized PageRanks. }Generalized PageRank (GPR) methods were first used in the context of unsupervised graph clustering where they showed significant performance improvements over Personalized PageRank~\citep{kloumann2017block,li2019optimizing}. The operational principles of GPRs can be succinctly described as follows. Given a seed node  in some cluster of the graph, a one-dimensional feature vector  is initialized according to . The GPR score is defined as , where the parameters , are referred to as the GPR weights. Clustering of the graph is performed locally by thresholding the GPR score. Certain PangRank methods, such as Personalized PageRank or heat-kernel PageRank~\citep{chung2007heat}, are associated with specific choices of GPR weights~\citep{li2019optimizing}. For an excellent in-depth discussion of PageRank methods, the interested reader is referred to~\citep{gleich2015pagerank}. The work in~\citet{li2019optimizing} recently introduced and theoretically analyzed a special form of GPR termed \emph{Inverse PR} (IPR) and showed that long random walk paths are more beneficial for clustering then previously assumed, provided that the GPR weights are properly selected (Note that IPR was developed for homophilic graphs and optimal GPR weights for heterophilic graphs are not currently known). 

\textbf{Equivalence of the GPR method and polynomial graph filtering. } If we truncate the infinite sum in the definition of GPR at some natural number ,  corresponds to a polynomial graph filter of order . Thus, learning the optimal GPR weights is equivalent to learning the optimal polynomial graph filter. Note that one can approximate any graph filter using a polynomial graph filter~\citep{6494675} and hence the GPR method is able to deal with a large range of different node label patterns. Also, increasing  allows one to better approximate the underlying optimal graph filter. This once again shows that large-step propagation is beneficial.

\textbf{Universality with respect to node label patterns: Homophily versus heterophily. }
In their recent work, \citet{pei2019geom} proposed an index to measure the level of homophily of nodes in a graph v \in Vvv.
Note that  corresponds to strong homophily while  indicates strong heterophily. Figures~\ref{fig:GPR-GNN} (b) and (c) plot the GPR weights learnt by our GPR-GNN method on a homophilic (Cora) and heterophilic (Texas) dataset. The learnt GPR weights from Cora match the behavior of IPR~\citep{li2019optimizing}, which verifies that large-step propagation is indeed of great importance for homophilic graphs. The GPR weights learnt from Texas behave significantly differently from all known PR variants, taking a number of negative values. These differences in weight patterns are observed under random initialization, demonstrating that the weights are actually learned by the network and not forced by specific initialization. Furthermore, the large difference in the GPR weights for these two graph models illustrates the learning power of GPR-GNN and their universal adaptability.


\textbf{The over-smoothing problem.} One of the key components in most GNN models is the graph convolutional layer, described by

where  and  represents the trainable weight matrix for the  layer. The key issue that limits stacking multiple layers is the over-smoothing phenomenon: If one were to remove \text{ReLU} in the above expression,  where each row of  only depends on the degree of the corresponding node, provided that the graph is irreducible and aperiodic. This shows that the model looses discriminative information provided by the node features as the number of layers increases.

\textbf{Mitigating graph heterophily and over-smoothing issues with the GPR-GNN model. }GPR-GNN first extracts hidden state features for each node and then uses GPR to propagate them. The GPR-GNN process can be mathematically described as:

where  represents a neural network with parameter set  that generates the hidden state features . The GPR weights  are trained together with  in an end-to-end fashion.
The GPR-GNN model is easy to interpret: As already pointed out, GPR-GNN has the ability to adaptively control the contribution of each propagation step and adjust it to the node label pattern. Examining the learnt GPR weights also helps with elucidating the properties of the topological information of a graph (i.e., determining the optimal polynomial graph filter), as illustrated in Figure~\ref{fig:GPR-GNN} (b) and (c).

\textbf{Placing GPR-GNNs in the context of related prior work. } Among the methods that differ from repeated stacking of GCN layers, APPNP~\citep{klicpera2018predict} represents one of the state-of-the-art GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC~\citep{wu2019simplifying} are special cases of our model since APPNP fixes ,  while SGC removes all nonlinearities with , respectively. These two weight choices correspond to Personalized PageRank (PPR)~\citep{jeh2003scaling}, which is known to be suboptimal compared to the IPR framework when applied to homophilic node classification~\citep{li2019optimizing}. Fixing the GPR weights makes the model unable to adaptively learn the optimal propagation rules which is of crucial importance: As we will show in Section~\ref{sec:theory}, the fixed PPR weights corresponds to low-pass graph filters which makes them inadequate for learning on heterophilic graphs. The recent work~\citep{klicpera2018predict} showed that fixed PPR weights (APPNP) can also provably resolve the over-smoothing problem. However, the way APPNP prevents over-smoothing is independent on the node label information. In contrast, the escape of GPR-GNN from over-smoothing is guided by the node label information (Theorem~\ref{thm:OS}). A detailed discussion of this phenomena along with illustrative examples is delegated to the Supplement.

Among the GCN-like models, JK-Net~\citep{xu2018representation} exhibits some similarities with GPR-GNN. It also aggregates the outputs of different GCN layers to arrive at the final output. On the other hand, the GCN-Cheby method~\citep{defferrard2016convolutional,kipf2017semi} is related to polynomial graph filtering, where each convolutional layer propagates multiple steps and the graph filter is related to Chebyshev polynomials. In both cases, the depth of the models is limited in practice~\citep{klicpera2018predict} and they are not easy to interpret as our GPR-GNN method. Some prior work also emphasizes adaptively learning the importance of different steps~\citep{abu2018watch,berberidis2018adaptive}. Nevertheless, none of the above works is applicable for semi-supervised learning with GNNs and considers heterophilic graphs.

\section{Theoretical properties of GPR-GNNs}\label{sec:theory}


\textbf{Graph filtering aspects of GPR-GNNs.} As mentioned in Section~\ref{sec:GPRandGPGNN}, the GPR component of the network may be viewed as a polynomial graph filter. Let  be the eigenvalue decomposition of . Then, the corresponding polynomial graph filter equals , where  is applied element-wise and . We established the following result.
\begin{theorem}[Informal]\label{thm:LPF}
    Assume that the graph  is connected. If ,  and  such that , then  is a low-pass graph filter. Also, if  and  is large enough, then  is a high-pass graph filter.
\end{theorem}
By Theorem~\ref{thm:LPF} and from our discussion in Section~\ref{sec:GPRandGPGNN}, we know that both APPNP and SGC will invariably suppress the high frequency components. Thus, they are inadequate for use on heterophilic graphs. In contrast, if one allows  to be negative and learned adaptively the graph filter will pass relevant high frequencies. This is what allows GPR-GNN to perform exceptionally well on heterophilic graphs (see Figures~\ref{fig:GPR-GNN}).

\textbf{GPR-GNN can escape from over-smoothing. }As already emphasized, one crucial innovation of the GPR-GNN method is to make the GPR weights adaptively learnable, which allows GPR-GNN to avoid over-smoothing and trade node and topology feature informativeness. Intuitively, when large-step propagation is not beneficial, it increases the training loss. Hence, the corresponding GPR weights should decay in magnitude. This observation is captured by the following result, whose more formal statement and proof are delegated to the Supplement due to space limitations.
\begin{theorem}[Informal]\label{thm:OS}
    Assume the graph  is connected and the training set contains nodes from each of the classes. Also assume that  is large enough so that the over-smoothing effect occurs for  which dominate the contribution to the final output . Then, the gradients of  and  are identical in sign for all .
\end{theorem}
Theorem~\ref{thm:OS} shows that as long as over-smoothing happens,  will approach  for all  when we use an optimizer such as stochastic gradient descent (SGD) which has a suitable learning rate decay. This reduces the contribution of the corresponding steps  in the final output . When the weights  are small enough so that  no longer dominates the value of the final output , the over-smoothing effect is eliminated.



\section{Results for new cSBM synthetic and real-world datasets}\label{sec:cSBM_intro}

\textbf{Synthetic data. } In order to test the ability of label learning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs~\citep{deshpande2018contextual} to generate synthetic graphs. We consider the case with two equal-size classes. In cSBMs, the node features are Gaussian random vectors, where the mean of the Gaussian depends on the community assignment. The difference of the means is controlled by a parameter , while the difference of the edge densities in the communities and between the communities is controlled by a parameter . Hence  and  capture the ``relative informativeness'' of node features and the graph topology, respectively. Moreover, positive s correspond to homophilic graphs while negative s correspond to heterophilic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in~\cite{deshpande2018contextual}. The results show that, asymptotically, one needs  to ensure a vanishing ratio of the misclassified nodes and the total number of nodes, where  and  as before denotes the dimension of the node feature vector.

Note that given a tolerance value ,  is an arc of an ellipsoid for which  and . To fairly and continuously control the extent of information carried by the node features and graph topology, we introduce a parameter . The setting  indicates that only node features are informative, while  indicates that only the graph topology is informative. Moreover,  corresponds to strongly homophilic graphs while  corresponds to strongly heterophilic graphs. Note that the values  and  convey the same amount of information regarding graph topology. This is due to the fact that . Ideally, GNNs that are able to optimally learn on both homophilic and heterophilic graph should have similar performances for  and . Due to space limitation we refer the interested reader to~\citep{deshpande2018contextual} for a review of all formal theoretical results and only outline the cSBM properties needed for our analysis. Additional information is also available in the Supplement.

Our experimental setup examines the semi-supervised node classification task in the transductive setting. We consider two different choices for the random split into training/validation/test samples, which we call sparse splitting () and dense splitting (), respectively. The sparse splittnig is more similar to the original semi-supervised setting considered in~\citet{kipf2017semi} while the dense setting is considered in~\citet{pei2019geom} for studying heterophilic graphs. We run each experiment  times with multiple random splits and different initializations.

\emph{Methods used for comparisons.} We compare GPR-GNN with  baseline models: MLP, GCN~\citep{kipf2017semi}, GAT~\citep{velickovic2018graph}, JK-Net~\citep{xu2018representation}, GCN-Cheby~\citep{defferrard2016convolutional}, APPNP~\citep{klicpera2018predict}, SGC~\citep{wu2019simplifying}, SAGE~\citep{hamilton2017inductive} and Geom-GCN~\citep{pei2019geom}. For all architectures, we use the corresponding Pytorch Geometric library implementations~\citep{fey2019fast}. For Geom-GCN, we directly use the code provided by the authors\footnote{https://github.com/graphdml-uiuc-jlu/geom-gcn}. We could not test Geom-GCN on cSBM and other datasets not originally tested in the paper due to a preprocessing subroutine that is not publicly available~\citep{pei2019geom}.



\emph{The GPR-GNN model setup and hyperparameter tuning.} We choose random walk path lengths with  and use a -layer (MLP) with  hidden units for the NN component. For the GPR weights, we use different initializations including PPR with ,  or  and the default random initialization in pytorch. Similarly, for APPNP we search the optimal  within . For other hyperparameter tuning, we optimize the learning rate over  and weight decay  for all models. For Geom-GCN, we use the best variants in the original paper for each dataset. Finally, we use GPR-GNN(rand) to describe the results obtained with random initialization of the GPR weights. Further experimental settings are discussed in the Supplement. 


\emph{Results. } We examine the robustness of all baseline methods and GPR-GNN using cSBM-generated data with , which includes graphs across the heterophily/homophily spectrum. The results are summarized in Figure~\ref{fig:cSBM_acc}. For both the sparse and dense setting, GPR-GNN significantly outperforms all other baseline models whenever  (heterophilic graphs). On the other hand, all baseline GNNs can be worse then simple MLP when the graph information is weak (). This shows that existing GNNs cannot apply to arbitrary graphs, while GPR-GNN is clearly more robust. APPNP methods have the worst performance on strongly heterophilic graphs. This is in agreement with the result of Theorem~\ref{thm:LPF} which asserts that APPNP intrinsically acts a low-pass filter and is thus inadequate for strong heterophily settings. JKNet, GCN-Cheby and SAGE are the only three baseline models that are able to learn strongly heterophilic graphs under dense splitting. This is also to be expected since JKNet is the only baseline model that combines results from different steps at the last layer, which is similar to what is done in GPR-GNN. GCN-Cheby uses multiple steps in each layers which allows it to partially adapt to heterophilic settings as each layer is related to a polynomial graph filter of higher order compared to that of GCN. SAGE treats ego-embeddings and embeddings from neighboring nodes differently and does not simply average them out. This allows SAGE to adapt to the heterophilic case since the ego-embeddings prevent nodes from being overwhelmed by information from their neighbors. Nevertheless, JKNet, GCN-Cheby and SAGE are not deep in practice. 
\setlength{\intextsep}{2pt}\setlength{\columnsep}{8pt}\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[trim={0cm 7.5cm 0cm 7.5cm},clip,width=0.99\linewidth]{cSBM_sparse_v2.pdf}\\
    \includegraphics[trim={0cm 7.5cm 0cm 7.5cm},clip,width=0.99\linewidth]{cSBM_dense_v2.pdf}
    \vspace{-0.8cm}
  \caption{Accuracy of tested models on cSBM. Error bars indicate  confidence interval.}
  \label{fig:cSBM_acc}
\end{wrapfigure}
Moreover, JKNet fails to learn under the sparse splitting model while GCN-Cheby and SAGE fail to learn well when the graph information is strong (), again under the sparse splitting model.

Also, we observe that random initialization of our GPR weights only results in slight performance drops under dense splitting. The drop is more evident for sparse splitting setting but our method still outperforms baseline models by a large margin for strongly heterophilic graphs. This is also to be expected as we have less label information in the sparse splitting setting where the implicit bias provided by good GPR initialization is helpful. The implicit bias becomes irrelevant for the dense splitting setting, since the label information is sufficiently rich.

Besides the strong performance of GPR-GNN, the other benefit is its interpretability. In Figure~\ref{fig:cSBM}, we demonstrate the learnt GPR weights by our GPR-GNN on cSBM with random initialization. When the graph is weak homophilic (), the learnt GPR weights are decreasing. This is similar to the PPR weights used in APPNP, despite that the decaying speed is different. When the graph is strong homophilic (), the learnt GPR weights are increasing which is significantly different from the PPR weights. This result matches the recent finding in~\cite{li2019optimizing} and behave similar to IPR proposed by the authors. On the other hand, the learnt GPR weights have zig-zag shape when the graph is heterophilic. This again validates Theorem~\ref{thm:LPF} as GPR weights with alternating signs correspond to a high-pass filter. Interestingly, when  the magnitude of learnt GPR weight is decreasing. This is because the graph information is weak and the node feature information is more important in this case. It makes sense that the learnt GPR weight focus on the first few steps. Hence, we have validated the interpretablity of GPR-GNN. In practice, one can use the learnt GPR weights to better understand the graph structured data at hand. We showcase this benefit in the results of real world benchmark datasets.

\begin{figure*}[t]
  \centering
\subfigure[ \hspace*{1.2em},\newline \hspace*{1.2em}()]{\includegraphics[trim={11cm 5.5cm 11cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_0.25_largefont.pdf}}
\subfigure[ \hspace*{1.2em},\newline \hspace*{1.2em}()]{\includegraphics[trim={11cm 5.5cm 11cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_0.75_largefont.pdf}}
  \subfigure[ \hspace*{1.2em},\newline \hspace*{1.2em}()]{\includegraphics[trim={11cm 5.5cm 11cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_-0.25_largefont.pdf}}
  \subfigure[ \hspace*{1.2em},\newline \hspace*{1.2em}()]{\includegraphics[trim={11cm 5.5cm 11cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_-0.75_largefont.pdf}}
  \vspace{-0.2cm}
  \caption{ Figure (a)-(d) shows the learnt GPR weights by GPR-GNN with random initialization on cSBM, dense split. The shaded region indicates  confidence interval.}\label{fig:cSBM}
  \vspace{-0.6cm}
\end{figure*}

\textbf{Real world benchmark datasets. } We use  homophilic benchmark datasets available from the Pytorch Geometric library, including the citation graphs Cora, CiteSeer, PubMed~\citep{sen2008collective,yang2016revisiting} and the Amazon co-purchase graphs Computers and Photo~\citep{mcauley2015image,shchur2018pitfalls}. We also use  heterophilic benchmark datasets tested in~\citet{pei2019geom}, including Wikipedia graphs Chameleon and Squirrel~\citep{musae}, the Actor co-occurrence graph, and webpage graphs Texas and Cornell from WebKB\footnote{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb}. We summarize the dataset statistics in Table~\ref{tab:dataset_stats}.
\begin{table}[th]
\setlength{\tabcolsep}{4pt}
\centering
\small
\caption{Benchmark dataset properties and statistics.}\label{tab:dataset_stats}
\vspace{0.05in}
\begin{tabular}[t]{c|cccccccccc}
Dataset & Cora & Citeseer & PubMed   & Computers & Photo & Chameleon & Squirrel & Actor & Texas & Cornell\\
\midrule
Classes          & 7        & 6          & 5         & 10        & 8    & 5 & 5 & 5  & 5    &5\\
Features         & 1433     & 3703         & 500      & 767       & 745    &2325   &2089   &932    &1703   &1703\\
Nodes            & 2708     & 3327        & 19717     & 13752     & 7650   &2277   &5201    &7600  &183    &183\\
Edges            & 5278     & 4552       & 44324    & 245861    & 119081 &31371 &198353 &26659  &279    &277\\
         & 0.825   & 0.718     & 0.792    & 0.802    & 0.849 &0.247 & 0.217 &0.215  &0.057  &0.301\\
\end{tabular}
\normalsize
\vspace{-0.3cm}
\end{table}

\emph{Results on real-world datasets. }
\begin{table}[t]
\setlength{\tabcolsep}{2.5pt}
\caption{Results on real world benchmark datasets: Mean accuracy ()   confidence interval. Boldface letters are used to mark the best results while underlined boldface letters indicate results within the given confidence interval of the best result.}
\vspace{0.2cm}
\label{tab:realdata_results}
\scriptsize
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
 &
  Cora &
  Citeseer &
  PubMed &
  Computers &
  Photo &
  Chameleon &
  Actor &
  Squirrel &
  Texas &
  Cornell \\ \midrule
GPRGNN &
  \textbf{79.51\tiny{0.36}} &
  67.63\tiny{0.38} &
  \textbf{85.07\tiny{0.09}} &
  {\ul\textbf{82.90\tiny{0.37}}} &
  \textbf{91.93\tiny{0.26}} &
  \textbf{67.48\tiny{0.40}} &
  \textbf{39.30\tiny{0.27}} &
  \textbf{49.93\tiny{0.53}} &
  \textbf{92.92\tiny{0.61}} &
  {\ul \textbf{91.36\tiny{0.70}}} \\
APPNP &
  {\ul \textbf{79.41\tiny{0.38}}} &
  \textbf{68.59\tiny{0.30}} &
  {\ul \textbf{85.02\tiny{0.09}}} &
  81.99\tiny{0.26} &
  91.11\tiny{0.26} &
  51.91\tiny{0.56} &
  38.86\tiny{0.24} &
  34.77\tiny{0.34} &
  91.18\tiny{0.70} &
  \textbf{91.80\tiny{0.63}} \\
MLP &
  50.34\tiny{0.48} &
  52.88\tiny{0.51} &
  80.57\tiny{0.12} &
  70.48\tiny{0.28} &
  78.69\tiny{0.30} &
  46.72\tiny{0.46} &
  38.58\tiny{0.25} &
  31.28\tiny{0.27} &
  92.26\tiny{0.71} &
  {\ul \textbf{91.36\tiny{0.70}}} \\
SGC &
  70.81\tiny{0.67} &
  58.98\tiny{0.47} &
  82.09\tiny{0.11} &
  76.27\tiny{0.36} &
  83.80\tiny{0.46} &
  63.02\tiny{0.43} &
  29.39\tiny{0.20} &
  43.14\tiny{0.28} &
  55.18\tiny{1.17} &
  47.80\tiny{1.50} \\
GCN &
  75.21\tiny{0.38} &
  67.30\tiny{0.35} &
  84.27\tiny{0.01} &
  82.52\tiny{0.32} &
  90.54\tiny{0.21} &
  60.96\tiny{0.78} &
  30.59\tiny{0.23} &
  45.66\tiny{0.39} &
  75.16\tiny{0.96} &
  66.72\tiny{1.37} \\
GAT &
  76.70\tiny{0.42} &
  67.20\tiny{0.46} &
  83.28\tiny{0.12} &
  81.95\tiny{0.38} &
  90.09\tiny{0.27} &
  63.9\tiny{0.46} &
  35.98\tiny{0.23} &
  42.72\tiny{0.33} &
  78.87\tiny{0.86} &
  76.00\tiny{1.01} \\
SAGE &
  70.89\tiny{0.54} &
  61.52\tiny{0.44} &
  81.30\tiny{0.10} &
  \textbf{83.11\tiny{0.23}} &
  90.51\tiny{0.25} &
  62.15\tiny{0.42} &
  36.37\tiny{0.21} &
  41.26\tiny{0.26} &
  79.03\tiny{1.20} &
  71.41\tiny{1.24} \\
JKNet &
  73.22\tiny{0.64} &
  60.85\tiny{0.76} &
  82.91\tiny{0.11} &
  77.80\tiny{0.97} &
  87.70\tiny{0.70} &
  62.92\tiny{0.49} &
  33.41\tiny{0.25} &
  44.72\tiny{0.48} &
  75.53\tiny{1.16} &
  66.73\tiny{1.73} \\
GCN-Cheby &
  71.39\tiny{0.51} &
  65.67\tiny{0.38} &
  83.83\tiny{0.12} &
  82.41\tiny{0.28} &
  90.09\tiny{0.28} &
  59.96\tiny{0.51} &
  38.02\tiny{0.23} &
  40.67\tiny{0.31} &
  86.08\tiny{0.96} &
  85.33\tiny{1.04} \\
GeomGCN &
  20.37\tiny{1.13} &
  20.30\tiny{0.90} &
  58.20\tiny{1.23} &
  NA &
  NA &
  61.06\tiny{0.49} &
  31.81\tiny{0.24} &
  38.28\tiny{0.27} &
  58.56\tiny{1.77} &
  55.59\tiny{1.59} \\ \bottomrule
\end{tabular}
\normalsize
\vspace{-0.3cm}
\end{table}
We use accuracy (the micro-F1 score) as the evaluation metric along with a  confidence interval. The relevant results are summarized in Table~\ref{tab:realdata_results}. For homophilic datasets, we provide results for sparse splitting which is more aligned with the original setting used in~\citet{kipf2017semi,shchur2018pitfalls}. For the heterophilic datasets, we adopt dense splitting which is used in~\cite{pei2019geom}. 

Table~\ref{tab:realdata_results} shows that, in general, GPR-GNN outperforms all tested methods. On homophilic datasets, GPR-GNN achieves the state-of-the-art performance. On heterophilic datasets, GPR-GNN significantly outperforms all the other baseline models. It is important to point out that there are two different patterns to be observed among the heterophilic datasets. On Chameleon and Squirrel, MLP and APPNP perform worse then other baseline methods such as GCN and JKNet. In contrast, MLP and APPNP outperform the other baseline methods on Actor, Texas and Cornell. We conjecture that this is due to the fact that the graph topology information is strong and weak, respectively. Note that these two patterns match the results of the cSBM experiments for  close to  and , respectively (Figure~\ref{fig:cSBM_acc}). Furthermore, the homophily measure  proposed by~\cite{pei2019geom} cannot characterize such differences in heterophilic datasets. We relegate the more detailed discussion of this topic along with illustrative examples to the Supplement.For fairness, we also repeated the experiment involving GeomGCN on homophilic datasets using a dense split - the observed performance pattern tends to be similar which can be found in Supplement.


\begin{figure*}[t]
  \centering
  \vspace{-0.5cm}
\subfigure[PubMed]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{PubMed_gamma_largefont.pdf}}
  \subfigure[Photo]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{Photo_gamma_largefont.pdf}}
  \subfigure[Actor]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{Actor_gamma_largefont.pdf}}
  \subfigure[Squirrel]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{Squirrel_gamma_largefont.pdf}}\\
  \vspace{-0.2cm}
  \subfigure[Epoch]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_-1_ep0_largefont.pdf}}
  \subfigure[Epoch]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_-1_ep50_largefont.pdf}}
  \subfigure[Epoch]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_-1_ep100_largefont.pdf}}
  \subfigure[Epoch]{\includegraphics[trim={11.5cm 5.5cm 11.5cm 5.5cm},clip,width=0.24\linewidth]{cSBM_gamma_-1_ep150_largefont.pdf}}
  \vspace{-0.2cm}
  \caption{ Figures (a)-(d) show the learned GPR weights of our GPR-GNN method with random initialization on various datasets, for dense splitting. Figures (e)-(f) show the learned weights of our GPR-GNN method with initialization  on cSBM, for dense splitting. The shaded region indicates a  confidence interval.}\label{fig:GPRweights}
  \vspace{-0.4cm}
\end{figure*}

We also examined the learned GPR weights on real datasets in Figure~\ref{fig:GPRweights}. Due to space limitations, a more comprehensive GPR weight analysis for other datasets is deferred to the Supplement. We can see that learned GPR weights are all positive for homophilic datasets (PubMed and Photo). In contrast, some GPR weights learned from heterophilic datasets (Actor and Squirrel) are negative. These results agree with the patterns observed on cSBMs. Interestingly, the learned weight  has the largest magnitude for the Actor dataset. This indicates that most of the information is contained in node features. From Table~\ref{tab:realdata_results} we can also see that MLPs indeed outperforms most baseline GNNs (this is similar to the case of cSBM). On the other hand, GPR weights learned from Squirrel have a zig-zag pattern. This implies that graph topology is more informative for Squirrel compared to Actor. From Table~\ref{tab:realdata_results} we also see that baseline GNNs also outperform MLPs on Squirrel.


\emph{Escaping from over-smoothing and dynamics of learning GPR weights. } To demonstrate the ability of GPR-GNNs to escape from over-smoothing, we choose the initial GPR weights to be . This ensures that over-smoothing effects are present with high probability at the very beginning of the learning process. On cSBM with dense splitting, we find that for  out of  runs, GPR-GNN predicts the same labels for all nodes at epoch , which implies that over-smoothing indeed occurs immediately. The final prediction is  accurate which is much larger than the initial accuracy of  at epoch . Similar results can be observed for other datasets and this verifies our theoretical findings. We plot the dynamics of the learned GPR weights in Figure~\ref{fig:GPRweights}(e)-(h), which shows that the peak at last step is indeed reduced while the GPR weights for other steps are significantly increased in magnitude. More results on the dynamics of learning GPR weights may be found in the Supplement.

\begin{table}[t]
\setlength{\tabcolsep}{2.5pt}
\caption{Efficiency on selected real world benchmark datasets: Average running time per epoch(ms)/average total running time(s). Note that Geom-GCN requires a preprocessing procedure so we do not include it in the table. Complete efficiency table for all benchmark datasets is in Supplementary due to space limit.}
\vspace{0.2cm}
\label{tab:realdata_efficiency}
\scriptsize
\begin{tabular}{@{}cccccccc@{}}
\toprule
 & Cora & Pubmed & Computers & Chameleon & Actor & Squirrel & Texas  \\ \hline
GPRGNN & 17.62ms / 3.74s & 20.19ms / 5.53s & 39.93ms / 11.40s & 16.74ms / 3.40s & 19.31ms / 4.49s & 25.28ms / 5.12s & 17.56ms / 3.55s  \\
APPNP & 17.16ms / 4.00s & 18.47ms / 6.29s & 39.59ms / 20.00s & 17.01ms / 3.44s & 16.32ms / 4.04s & 22.93ms / 4.63s & 15.96ms / 3.24s  \\
MLP & 4.14ms / 0.92s & 5.43ms / 2.86s & 5.33ms / 2.77s & 3.41ms / 0.69s & 4.84ms / 0.98s & 5.19ms / 1.05s & 3.81ms / 1.04s  \\
SGC & 3.31ms / 3.31s & 3.81ms / 3.81s & 4.36ms / 4.36s & 3.13ms / 3.13s & 3.98ms / 1.00s & 4.79ms / 4.79s & 2.86ms / 2.09s  \\
GCN & 9.25ms / 1.97s & 14.11ms / 4.17s & 32.45ms / 16.29s & 13.83ms / 2.79s & 12.39ms / 2.50s & 27.11ms / 5.56s & 10.22ms / 2.06s  \\
GAT & 14.78ms / 3.42s & 21.52ms / 6.70s & 61.45ms / 24.28s & 16.63ms / 3.63s & 18.91ms / 3.86s & 47.46ms / 10.05s & 15.50ms / 3.13s  \\
SAGE & 12.06ms / 2.44s & 28.82ms / 6.32s & 171.36ms / 71.94s & 64.43ms / 13.02s & 27.95ms / 5.65s & 343.47ms / 69.38s & 6.08ms / 1.28s  \\
JKNet & 18.97ms / 4.41s & 24.48ms / 6.61s & 35.02ms / 14.96s & 20.03ms / 5.15s & 23.52ms / 4.75s & 29.89ms / 6.67s & 19.67ms / 4.01s  \\
GCN-cheby & 22.96ms / 4.75s & 45.76ms / 12.02s & 218.82ms / 96.58s & 89.41ms / 18.06s & 43.94ms / 8.88s & 440.55ms / 88.99s & 12.34ms / 3.08s \\
\bottomrule
\normalsize
\end{tabular}
\vspace{-0.7cm}
\end{table}

\emph{Efficiency analysis. } We also examine the computational complexity of GPR-GNNs compared to other baseline models. We report the empirical training time in Table~\ref{tab:realdata_efficiency}. Compared to APPNP, we only need to learn  additional GPR weights for GPR-GNN, and usually  (i.e. we choose  in our experiments). This additional computations are dominated by the computations performed by the neural network module . We can observe from Table~\ref{tab:realdata_efficiency} that indeed GPR-GNN has a running time similar to that of APPNP. It is nevertheless worth pointing out that the authors of~\cite{bojchevski2020scaling} successfully scaled APPNP to operate on large graphs. Whether the same techniques may be used to scale GPR-GNNs is an interesting open question.

\section{Conclusions}
We addressed two fundamental weaknesses of existing GNNs: Failing to act as universal learners by not generalizing to heterophilic graphs and making use of large number of propagation steps. We developed a novel GPR-GNN architecture which combines adaptive generalized PageRank (GPR) scheme with GNNs. We theoretically showed that our method does not only mitigates feature over-smoothing but also works on highly diverse node label patterns. We also tested GPR-GNNs on both homophilic and heterophilic node label patterns, and proposed a novel synthetic benchmark datasets generated by the contextual stochastic block model. Our experiments on real-world benchmark datasets showed clear performance gains of GPR-GNN over the state-of-the-art methods. Moreover, we showed that GPR-GNN has desirable interpretability properties which is of independent interest.


\subsubsection*{Acknowledgments}
The work was supported in part by the NSF Emerging Frontiers of Science of Information Grant 0939370 and the NSF CIF 1618366 Grant. The authors thank Mohamad Bairakdar for helpful comments on an earlier version of the paper.



\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\newpage

\appendix
\section{Appendix}







\subsection{Detailed discussion on preventing over-smoothing. }
As mentioned in Section~\ref{sec:theory}, another method -- APPNP -- can also provably prevents over-smoothing~\citet{klicpera2018predict}. The authors of this study use the fact that the PPR propagation will converge to , where  is independent on the node label information provided in the  training data. Each row of  still depends on  and thus APPNP will not suffer from the over-smoothing effect. However, since  is independent of the label information, it can cause undesired consequences that we discuss in what follows.

\begin{figure*}[ht]
  \centering
  \subfigure[A graph .]{\includegraphics[trim={9cm 4cm 9cm 4cm},clip,width=0.32\linewidth]{CaseNull.pdf}}
  \subfigure[One example of label assignment.]{\includegraphics[trim={9cm 4cm 9cm 4cm},clip,width=0.32\linewidth]{CaseA.pdf}}
  \subfigure[Another example of label assignment.]{\includegraphics[trim={9cm 4cm 9cm 4cm},clip,width=0.32\linewidth]{CaseB.pdf}}
  \vspace{-0.2cm}
  \caption{A simple example demonstrating how GPR-GNN escapes over-smoothing.}\label{fig:discuss_OS}
\end{figure*}

Let us consider a simple example shown in Figure~\ref{fig:discuss_OS} involving a connected and undirected graph  (Figure~\ref{fig:discuss_OS} (a)). Consider two different node label assignments shown in Figure~\ref{fig:discuss_OS} (b) and Figure~\ref{fig:discuss_OS} (c). Obviously, the graph topologies depicted in Figure~\ref{fig:discuss_OS} (b) and (c) are identical and the only difference is the class label assignment. In Figure~\ref{fig:discuss_OS} (b), the graph is homophilic and hence the optimal graph filter should emphasize the low-frequency part of the graph signal. In contrast, in Figure~\ref{fig:discuss_OS} (c), the graph is heterophilic as the graph is bipartite with respect to the labels. Hence, the optimal graph filter should emphasize the high-frequency part of the graph signal. This example illustrates that the optimal graph filter should depend on
both the graph topology and the node label information. Recall that the equivalent graph filter that APPNP uses in the asymptotic regime is  which is independent on the node label information. Also, Theorem~\ref{thm:LPF} established that APPNP intrinsically utilizes a low-pass filter. In contrast, GPR-GNN learns the GPR weights guided by the node label information which allows it to account for both
cases (homophilic and heterophilic) shown.




\subsection{Discussion on the insufficiency of homophily measure }\label{sec:HGbad}
\begin{figure*}[ht]
  \centering
  \subfigure[Case 1.]{\includegraphics[trim={9cm 5.5cm 9cm 4cm},clip,width=0.45\linewidth]{HG_bad_figure1.pdf}}
  \subfigure[Case 2.]{\includegraphics[trim={9cm 4cm 9cm 5.5cm},clip,width=0.45\linewidth]{HG_bad_figure2.pdf}}
  \vspace{-0.2cm}
  \caption{A simple example for explaining the insufficiency of homophily measure .}\label{fig:HGbad}
\end{figure*}
As mentioned in Section~\ref{sec:cSBM_intro}, the homophily measure  is inadequate for characterizing whether a heterophilic graph topology is informative or not. Consider two simple examples depicted in Figure~\ref{fig:HGbad}, where the color of the nodes indicates their label. In case 1, blue and green nodes link to all orange and purple nodes. In case 2, blue nodes only link to orange nodes and green nodes only link to purple nodes. From the definition of  one can see that both cases have , since in both cases nodes do not link to other nodes of the same label. However, it is obvious that the graph topology carries more node label information in case 2 compared to case 1. In fact, for case 1 it is impossible to distinguish blue and green nodes merely from the graph topology (and the same is true of orange and purple nodes). One possible alternative for the homophily measure is the Chernoff-Hellinger divergence~\cite{abbe2017community} of the empirical edge probability matrix ; here  is the empirical probability of an edge with one end node labeled  and the other labeled . The intuition behind our suggestion lies in the fact that the Chernoff-Hellinger divergence characterizes the fundamental limit of SBMs. However, as many practical graph generative processes may significantly differ from SBMs, investigating alternative homophily/heterophily measures is another interesting open problem.

\subsection{Proof of Theorem~\ref{thm:LPF}}
We first state the formal version of Theorem~\ref{thm:LPF}.
\begin{theorem}[Formal version of Theorem~\ref{thm:LPF}]\label{thm:formal_LPF}
 Assume the graph  is connected. Let  be the eigenvalues of .
 If ,  and  such that , then . Also, if  and , then .
\end{theorem}

Note that  implies that after applying the graph filter , the lowest frequency component (correspond to ) further dominates. Recall that in the unfiltered case, we do not multiply with . It can also be viewed as multiplying the identity matrix , where the eigenvalue ratio is . 
Hence  acts like a low pass filter in this case. In contrast,  implies that after applying the graph filter, the lowest frequency component (correspond to ) no longer dominates. This correspond to the high pass filter case.

\begin{proof}
 We start with the low pass filter result. From basic spectral analysis~\citep{von2007tutorial} we know that  and . One can also find the analysis in the proof of our Lemma~\ref{lma:main1} in the Supplement. Then by assumption we know that

Hence, proving Theorem~\ref{thm:formal_LPF} is equivalent to show

This is obvious since  is a polynomial of order  with nonnegative coefficients. It is easy to check that . Combine with the fact that all 's are nonnegative we have

Finally, note that the only possibility that the equality in (a) holds is  since . However, by assumption  and  such that  we know that this is impossible. Hence (a) is a strict inequality . Together we complete the proof for low pass filtering part.

For the high pass filter result, it is not hard to see that

where the last step is due to the fact that  and thus . Thus we have

The strict inequalities (b) is from the fact that . Notably,  happens at the boundary , which corresponds the the bipartite graph. It further shows that the graph filter with respect to the choice  emphasizes high frequency components and thus it is indeed acting as a high pass filter.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:OS}}
We start by introducing some additional notation, lemmas and definition before we proceed to the formal statement of Theorem~\ref{thm:OS}. The label matrix is denoted by , where each row is a one-hot vector. We use  to denote the argmax of the vector : we have  if and only if  (ties are broken evenly), and  otherwise. Let us replace the softmax with softmax, where we let  stand for the softmax with a smooth parameter . Note that for  we recover the standard softmax. With a slight abuse of notation, for the vector  we write  to denote element-wise exponentiation. We use  to denote the standard Euclidean inner product. Also we use  for the cross entropy loss where


\begin{lemma}\label{lma:main1}
 Assume that the nodes in an undirected and connected graph  have one of  labels. Then, for  large enough, we have
 
\end{lemma}
For any  and large enough , if the label prediction is dominated by , all nodes will have a representation proportional to . Hence, we will arrive at the same label for all nodes. This is what we refer to as the over-smoothing phenomenon.

\begin{definition}[The over-smoothing phenomenon]\label{def:oversmoothing}
First, recall that . If over-smoothing occurs in the GPR-GNN for  sufficiently large, we have  for some  if  and  for some  if .
\end{definition}

\begin{lemma}\label{lma:main2}
 Let  be the cross entropy loss and let  be the training set. Under the same assumption as given in Lemma~\ref{lma:main1}, the gradient of  for  large enough is 
\end{lemma}
\begin{lemma}\label{lma:softmax2argmax}
  For any real vector  and  large enough, we have .
\end{lemma}

Now we are ready to state the formal version of Theorem~\ref{thm:OS}.
\begin{theorem}[Formal version of Theorem~\ref{thm:OS}]
  Under the same assumptions as those listed in Lemma~\ref{lma:main1}, if the training set contains nodes from each class, then the GPR-GNN method can always avoid over-smoothing. More specifically, for  large enough we have
 
\end{theorem}

Note that when , \eqref{eq:gamma_p}  when ignoring the  term. The equality is achieved if and only if . This means that over-smoothing results in a prediction that perfectly aligns with the ground truth label in the training set. However, if our training set contains at least one node from each class then the equality can never be attained. Thus, the gradient of  will always be positive when . Similarly when , \eqref{eq:gamma_n}  when ignoring the  term. The equality is achieved if and only if . By the same reason we know that under the assumption on training set the equality can never be attained. Thus, the gradient of  will always be negative when . Finally, it is not hard to check that the gradient is bounded in magnitude. Together we have shown that the gradient of  and  are of the same sign. This directly implies that  will approach to  until we escape from over-smoothing when we use a decreasing learning rate for the optimizer (i.e. SGD).

\begin{proof}
 First, let us assume the over-smoothing takes place and the  for the dominate term. By Definition~\ref{def:oversmoothing}, we know that  for some  and  sufficiently large. By Lemma~\ref{lma:main2} we have

where the last step follows from Definition~\ref{def:oversmoothing}. Next, by Lemma~\ref{lma:softmax2argmax}, we may approximate the softmax by the true argmax for  large enough according to


The first equality is due to the fact that  and . Recall that by Lemma~\ref{lma:main1}, . Since we have a self- loop for each node,  and thus . For the case , the same analysis still valid until~\eqref{eq:stillvalid}. Hence we have

Together we complete the proof.


\end{proof}

\subsection{cSBM details}
The cSBM adds Gaussian random vectors as node features on top of the classical SBM. For simplicity, we assume  equally sized communities with node labels  in . Each node  is associate with a  dimensional Gaussian vector  where  is the number of nodes,  and  has independent standard normal entries. The (undirected) graph in cSBM is described by the adjacency matrix  defined as

Similar to the classical SBM, given the node labels the edges are independent. The symbol  stands for the average degree of the graph. Also, recall that  and  control the information strength carried by the node features and the graph structure respectively.

One reason for using the cSBM to generate synthetic data is that the information-theoretic limit of the model is already characterized in~\cite{deshpande2018contextual}. This result is summarized below.
\begin{theorem}[Informal main result in~\cite{deshpande2018contextual}]Assume that ,  and . Then there exists an estimator  such that  is bounded away from  if and only if .
\end{theorem}

In our experiment, we set   and thus have . We vary  and  along the arc  for some  to ensure that we are in the achievable parameter regime. We also choose  for all our experiment.



\subsection{Proof of Lemma~\ref{lma:main1}}
Note that the proof of Lemma~\ref{lma:main1} reduces to a standard analysis of random walks on graph. We include it for completeness and refer the interested readers to the tutorial~\cite{von2007tutorial}.

We start by showing that the symmetric graph Laplacian

is positive semi-definite. Let  be any real vector of unit norm and , then we have

where the last step follows from the definition of the degree.

Next we show that  is indeed an eigenvalue of  associated with the unit eigenvector  where .

Let  be the all one vector. Then, a direct calculation reveals that

Combining this result with the positive semi-definite property of the Laplacian shows that  is indeed the smallest eigenvalue of  associated with the eigenvector . Moreover, from \eqref{sup:eq2} and the assumption that the graph is connected, it is not hard to see that the multiplicity of the eigenvalue  is exactly 1 (See Proposition 2 and 4 in~\cite{von2007tutorial} for more detail). Finally, from \eqref{sup:eq1} it is obvious that the the largest eigenvalue of  is , which correspond to the eigenvector . Hence all other eigenvalues of   .

Next, we prove that . This can also be shown directly from~\eqref{sup:eq2}. Note that

The inequality follows from an application of the Cauchy-Schwartz inequality. Consequently, the largest eigenvalue of  is bounded by  which means that . Note that equality holds if and only if the underlying graph is bipartite. However, this is impossible in our setting since we have added a self loop to each node. Hence . This means

Hence, for any  we have

Note that this can also be written with the  term as

This completes the proof.

\subsection{Proof of Lemma~\ref{lma:main2}}
Recall that our loss function equals

Then by taking the partial derivative of the loss function with respect to  we have

Next, recall that for GPR-GNN we also have . Plugging this expression into the previous formula and applying the chain rule we obtain

Settin  for large enough , it follows from Lemma~\ref{lma:main1} that

Note that in~\eqref{sup:eq3} and~\eqref{sup:eq4} we used the definition of the soft prediction . This completes the proof.

\subsection{Proof of Lemma~\ref{lma:softmax2argmax}}
Let . Then by the definition of softmax for  we have

Note that  when  and  when . Without loss of generality we assume that there are  maxima in , where  and let  denote the set of indices of those maxima. Then, taking the limit  we have

This implies that for  large enough one has

The above result completes the proof.

\subsection{Additional Experimental details}

\begin{table}[ht]
\centering
\small
\caption{The values of the homophily measure for cSBM datasets.}
\vspace{0.05in}
\begin{tabular}[t]{c|ccccccccc}
 &  &  &    &  &  &  &  &  & \\
\midrule
 &0.039  &0.073   &0.170   &0.328   &0.500   &0.673   &0.829   &0.928   &0.960   \\
\end{tabular}
\normalsize
\end{table}

All experiments are performed on a Linux Machine with  cores, GB of RAM, and a NVIDIA Tesla P100 GPU with GB of GPU memory. For the training set, we ensure that number of nodes from each class is approximately the same an keep the total number of training nodes close to . For the validation set, we randomly sample  of the nodes and place the remaining ones into the test set.

For all baseline models, we directly use the implementation available in the Pytorch Geometric library~\cite{fey2019fast}.We use early stopping  and a maximum number of epochs equal to  for both real benchmark dataset and our cSBM synthetic datasets. All models use the Adam optimizer~\cite{kingma2014adam}. Note that the early stopping criteria is exactly the same as in Pytorch Geometric -- when the epoch is greater than half of the maximum epoch, we check if the current validation loss is lower than the average over the past  epochs. If it is not lower, we stop the training process.

For GCN, we use  GCN layers with  hidden units. For GAT, we use  GAT convolutional layers, where the first layer has  attention heads and each head has  hidden units; the second layer has  attention head and  hidden units. For GCN-Cheby, we use  steps propagation for each layer with  hidden units. Note that the number of equivalent hidden units for each layer is for this case. For JK-Net, we use the GCN-based model with  layers and  hidden units in each layer. As for the layer aggregation part, we use a LSTM with  channels and  layers. For the MLP, we choose a -layer fully connected network with 4 hidden units. For APPNP we use the same -layer MLP with  steps of propagation. Besides the GPR-GNN, we fix the dropout rate for the NN part to be  as APPNP and optimize the dropout rate for the GPR part among . For Geom-GCN, we choose the datasets already tested in the paper were the method was first described~\citep{pei2019geom}. For SGC, we use the default  layers after test among . For SAGE, we use  SAGE convolutional layers with  hidden units.

\textbf{The heterophilic datasets used in~\citep{pei2019geom}. }The graphs Chameleon, Actor, Squirrel, Texas and Cornell in their original form are directed graphs (see the github repository of~\citep{pei2019geom}). Since the usual setting for semi-supervised node classifications involves undirected graph, we transformed the graphs into undirected to test them on all previously described benchmark methods. We keep the input graph directed for Geom-GCN as the method uses a fixed preprocessing scheme that was unfortunately not made public by the authors. Our homophily measure values  in Table~\ref{tab:dataset_stats} are all based on undirected graphs and hence the numbers are different from those reported in~\citep{pei2019geom}.




\subsection{Additional Experimental results}

\begin{table}[ht]
\caption{Results for cSBM, sparse splitting. Bold values indicate the best obtained result and while bold, underlined values indicate results within a  confidence interval with respect to the best result.}
\label{tab:cSBM_sparse}
\vspace{0.2cm}
\scriptsize
\begin{tabular}{@{}cccccccccc@{}}
\toprule
 &
   &
   &
   &
   &
   &
   &
   &
   &
   \\ \midrule
GPRGNN &
  \textbf{97.19\tiny{0.16}} &
  \textbf{95.54\tiny{0.15}} &
  \textbf{81.54\tiny{0.73}} &
  60.65\tiny{0.31} &
  \textbf{62.16\tiny{0.23}} &
  \textbf{68.83\tiny{0.28}} &
  \textbf{89.31\tiny{0.16}} &
  \textbf{96.98\tiny{0.08}} &
  \textbf{96.71\tiny{0.13}} \\
GPRGNN(random) &
  88.39\tiny{3.31} &
  88.54\tiny{3.01} &
  66.91\tiny{2.93} &
  56.35\tiny{0.98} &
  58.09\tiny{0.71} &
  64.01\tiny{1.39} &
  81.93\tiny{1.68} &
  94.59\tiny{0.29} &
  93.69\tiny{1.04} \\
APPNP &
  49.57\tiny{0.11} &
  52.45\tiny{0.27} &
  56.32\tiny{0.40} &
  59.55\tiny{0.48} &
  61.21\tiny{0.23} &
  68.41\tiny{0.30} &
  85.66\tiny{0.22} &
  94.37\tiny{0.09} &
  90.02\tiny{0.16} \\
MLP &
  49.88\tiny{0.10} &
  53.40\tiny{0.34} &
  57.14\tiny{0.41} &
  60.55\tiny{0.41} &
  {\ul \textbf{62.15\tiny{0.33}}} &
  61.26\tiny{0.21} &
  57.91\tiny{0.35} &
  53.36\tiny{0.32} &
  49.92\tiny{0.11} \\
SGC &
  54.41\tiny{0.37} &
  59.74\tiny{0.29} &
  55.57\tiny{0.33} &
  51.84\tiny{0.23} &
  53.95\tiny{0.28} &
  65.65\tiny{0.27} &
  85.51\tiny{0.20} &
  93.99\tiny{0.10} &
  88.50\tiny{0.18} \\
GCN &
  55.24\tiny{0.35} &
  61.04\tiny{0.39} &
  56.40\tiny{0.39} &
  52.23\tiny{0.24} &
  54.43\tiny{0.32} &
  67.23\tiny{0.29} &
  84.56\tiny{0.20} &
  90.19\tiny{0.14} &
  78.67\tiny{0.19} \\
GAT &
  53.97\tiny{0.32} &
  57.18\tiny{0.45} &
  53.39\tiny{0.34} &
  51.23\tiny{0.19} &
  53.26\tiny{0.27} &
  64.45\tiny{0.36} &
  81.94\tiny{0.34} &
  88.45\tiny{0.26} &
  78.06\tiny{0.30} \\
SAGE &
  62.30\tiny{0.50} &
  75.10\tiny{0.50} &
  72.84\tiny{0.44} &
  {\ul \textbf{63.88\tiny{0.37}}} &
  58.62\tiny{0.30} &
  63.55\tiny{0.47} &
  73.50\tiny{0.50} &
  75.26\tiny{0.52} &
  62.61\tiny{0.44} \\
JKNet &
  51.70\tiny{0.39} &
  55.83\tiny{0.75} &
  52.67\tiny{0.51} &
  50.27\tiny{0.15} &
  52.02\tiny{0.35} &
  65.67\tiny{0.44} &
  86.35\tiny{0.19} &
  95.13\tiny{0.09} &
  90.32\tiny{0.17} \\
GCN-Cheby &
  61.44\tiny{0.51} &
  73.91\tiny{0.75} &
  71.96\tiny{0.6} &
  \textbf{63.96\tiny{0.43}} &
  59.70\tiny{0.34} &
  64.00\tiny{0.38} &
  72.34\tiny{0.63} &
  73.56\tiny{0.65} &
  60.88\tiny{0.58} \\ \bottomrule
\end{tabular}
\normalsize
\end{table}

\begin{table}[ht]
\caption{Results for cSBM, dense splitting. Bold values indicate the best results found while bold, underlined values indicate results within a  confidence interval with respect to the best result.}
\label{tab:cSBM_dense}
\vspace{0.2cm}
\scriptsize
\begin{tabular}{@{}cccccccccc@{}}
\toprule
 &
   &
   &
   &
   &
   &
   &
   &
   &
   \\ \midrule
GPRGNN &
  \textbf{98.83\tiny{0.06}} &
  \textbf{98.19\tiny{0.08}} &
  \textbf{94.23\tiny{0.14}} &
  \textbf{86.06\tiny{0.20}} &
  \textbf{82.22\tiny{0.20}} &
  \textbf{86.48\tiny{0.20}} &
  \textbf{94.34\tiny{0.13}} &
  \textbf{98.46\tiny{0.08}} &
  \textbf{98.84\tiny{0.06}} \\
GPRGNN(random) &
  98.75\tiny{0.05} &
  98.08\tiny{0.08} &
  94.22\tiny{0.14} &
  86.06\tiny{0.20} &
  {\ul \textbf{81.57\tiny{0.23}}} &
  {\ul \textbf{86.36\tiny{0.20}}} &
  94.09\tiny{0.14} &
  {\ul \textbf{98.38\tiny{0.08}}} &
  98.77\tiny{0.07} \\
APPNP &
  48.94\tiny{0.29} &
  63.87\tiny{0.29} &
  73.30\tiny{0.26} &
  79.30\tiny{0.20} &
  82.41\tiny{0.23} &
  86.47\tiny{0.18} &
  94.20\tiny{0.14} &
  97.96\tiny{0.10} &
  98.53\tiny{0.08} \\
MLP &
  49.79\tiny{0.29} &
  66.69\tiny{0.27} &
  75.36\tiny{0.26} &
  80.30\tiny{0.24} &
  82.19\tiny{0.24} &
  80.88\tiny{0.22} &
  76.07\tiny{0.24} &
  66.61\tiny{0.25} &
  49.65\tiny{0.29} \\
SGC &
  78.95\tiny{0.23} &
  81.79\tiny{0.24} &
  75.15\tiny{0.25} &
  59.40\tiny{0.28} &
  63.75\tiny{0.26} &
  80.81\tiny{0.22} &
  93.04\tiny{0.15} &
  98.05\tiny{0.08} &
  97.80\tiny{0.09} \\
GCN &
  78.50\tiny{0.28} &
  83.68\tiny{0.22} &
  75.98\tiny{0.25} &
  59.98\tiny{0.25} &
  64.09\tiny{0.26} &
  81.89\tiny{0.19} &
  93.91\tiny{0.12} &
  97.78\tiny{0.08} &
  96.29\tiny{0.11} \\
GAT &
  82.39\tiny{0.41} &
  80.37\tiny{0.22} &
  71.01\tiny{0.26} &
  57.68\tiny{0.29} &
  62.95\tiny{0.28} &
  80.61\tiny{0.24} &
  93.26\tiny{0.14} &
  97.99\tiny{0.08} &
  98.40\tiny{0.09} \\
SAGE &
  91.33\tiny{0.23} &
  95.72\tiny{0.12} &
  93.23\tiny{0.17} &
  84.52\tiny{0.20} &
  78.99\tiny{0.24} &
  84.87\tiny{0.20} &
  92.90\tiny{0.15} &
  95.75\tiny{0.11} &
  91.19\tiny{0.24} \\
JKNet &
  96.11\tiny{0.37} &
  95.33\tiny{0.25} &
  87.98\tiny{0.56} &
  59.61\tiny{0.49} &
  63.28\tiny{0.10} &
  80.23\tiny{0.36} &
  93.28\tiny{0.15} &
  98.33\tiny{0.07} &
  98.22\tiny{0.07} \\
GCN-Cheby &
  90.94\tiny{0.16} &
  94.82\tiny{0.13} &
  91.83\tiny{0.17} &
  85.18\tiny{0.21} &
  80.80\tiny{0.25} &
  85.28\tiny{0.21} &
  92.70\tiny{0.16} &
  95.06\tiny{0.13} &
  90.34\tiny{0.18} \\ \bottomrule
\end{tabular}
\normalsize
\end{table}

\begin{table}[ht]
\centering
\caption{Results on homophilic real-world benchmark datasets tested in~\citep{pei2019geom}, dense splitting: Mean accuracy ()   confidence interval. Boldface values indicate the best results found while boldface, underlined values indicates results within the confidence interval with respect to the best result.}
\label{tab:realdata_results2}
\scriptsize
\begin{tabular}{@{}cccc@{}}
\toprule
          & Cora                  & Citeseer                       & PubMed                \\ \midrule
GPRGNN & \textbf{88.65\tiny{0.28}} & 80.01\tiny{0.28}               & \textbf{89.18\tiny{0.15}}       \\
APPNP  & 88.1\tiny{0.23}           & {\ul \textbf{80.5\tiny{0.26}}} & {\ul \textbf{89.15\tiny{0.13}}} \\
MLP       & 76.44\tiny{0.30}  & 76.25\tiny{0.28}          & 86.43\tiny{0.13} \\
SGC       & 86.58\tiny{0.26}  & 76.23\tiny{0.29}          & 83.52\tiny{0.10} \\
GCN       & 86.87\tiny{0.25} & 79.28\tiny{0.25}          & 86.97\tiny{0.12} \\
GAT       & 87.52\tiny{0.24} & \textbf{80.56\tiny{0.31}} & 86.64\tiny{0.11} \\
SAGE      & 86.58\tiny{0.26} & 78.24\tiny{0.30} & 86.85\tiny{0.11} \\
JKNet     & 86.97\tiny{0.27} & 77.69\tiny{0.35}          & 87.38\tiny{0.13} \\
GCN-Cheby & 86.46\tiny{0.26} & 78.66\tiny{0.26}          & 88.2\tiny{0.09}  \\
GeomGCN   & 85.4\tiny{0.26}  & 76.42\tiny{0.37}          & 88.51\tiny{0.08} \\ \bottomrule
\end{tabular}
\normalsize
\end{table}

\begin{figure*}[ht]
  \centering
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_0.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_0.25.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_0.5.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_0.75.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_1.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_-0.25.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_-0.5.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_-0.75.pdf}}
  \subfigure[\hspace*{1.2em}cSBM, ,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{cSBM_gamma_-1.pdf}}
  \caption{ Figures (a)-(i) show the learned GPR weights by GPR-GNN with random initialization on cSBM, dense splitting. The shaded region indicates a  confidence interval.}\label{fig:GPRweights_fulloncSBM}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \subfigure[Cora,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Cora_gamma.pdf}}
  \subfigure[Citeseer,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Citeseer_gamma.pdf}}
  \subfigure[PubMed,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{PubMed_gamma.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Computers_gamma.pdf}}
  \subfigure[Photo,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Photo_gamma.pdf}}
  \subfigure[Chameleon,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Chameleon_gamma.pdf}}
  \subfigure[Actor,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Actor_gamma.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Squirrel_gamma.pdf}}
  \subfigure[Texas,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Texas_gamma.pdf}}
  \subfigure[Cornell,\newline \hspace*{1.2em}()]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.32\linewidth]{Cornell_gamma.pdf}}
  \caption{ Figures (a)-(j) show the learned GPR weights by GPR-GNN with random initialization on various benchmark datasets, dense splitting. The shaded region indicates a  confidence interval. Note that the learned GPR weights are all positive for every homophilic dataset. There is at least one negative learned GPR weight for every heterophilic dataset.}\label{fig:GPRweights_fullonreal}
\end{figure*}

\begin{table}[ht]
\centering
\caption{Additional experiments illustrating that GPR-GNN escapes over-smoothing. We initialize the GPR weights  as described in Section~\ref{sec:cSBM_intro}. We report the mean accuracy at Epoch  and after training (Final epoch). The over-smoothing ratio indicates how many time out of the  runs that GPR-GNN started with lead to the same label for all nodes. For an illustration of how GPR weights change over different epochs, please check Figure~\ref{fig:GPRweights_OS}.}
\vspace{0.2cm}
\label{tab:OSexp}
\begin{tabular}{@{}cccc@{}}
\toprule
\multicolumn{1}{l}{} & Accuracy at epoch (\%) & Accuracy at the final epoch(\%) & Over-smoothing ratio(\%) \\ \midrule
Cora                 & 12.75      & 88.25          & 84                      \\
Computers            & 9.41       & 85.93          & 89                      \\
Squirrel             & 19.87      & 52.06          & 97                      \\
Texas                & 21.05      & 90.05          & 100                     \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[ht]
  \centering
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep0.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep50.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep100.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep150.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep200.pdf}}\\
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep0.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep50.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep100.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep150.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep200.pdf}}\\
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep0.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep50.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep100.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep150.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep200.pdf}}\\
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep0.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep50.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep100.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep150.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep200.pdf}}\\
  \caption{ Learned GPR weights by GPR-GNN with initialization  (last step) on various benchmark datasets, dense splitting. The shaded region indicates a  confidence interval. Also, please check Table~\ref{tab:OSexp}. Note that the GPR weights  are identical to  in terms of graph filtering.}\label{fig:GPRweights_OS}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep0_rand.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep50_rand.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep100_rand.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep150_rand.pdf}}
  \subfigure[Cora, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Cora_ep200_rand.pdf}}\\
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep0_rand.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep50_rand.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep100_rand.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep150_rand.pdf}}
  \subfigure[Computers,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Computers_ep200_rand.pdf}}\\
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep0_rand.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep50_rand.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep100_rand.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep150_rand.pdf}}
  \subfigure[Squirrel,\newline \hspace*{1.2em} epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Squirrel_ep200_rand.pdf}}\\
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep0_rand.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep50_rand.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep100_rand.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep150_rand.pdf}}
  \subfigure[Texas, epoch ]{\includegraphics[trim={10cm 7cm 10cm 7cm},clip,width=0.19\linewidth]{Texas_ep200_rand.pdf}}\\
  \caption{ The dynamics of learning GPR weights with random initialization on various benchmark datasets, dense splitting. The shaded region indicates a  confidence interval. }\label{fig:GPRweights_fulldym}
\end{figure*}

\begin{table}[ht]
\caption{Efficiency on homophilic real world benchmark datasets: Average running time per epoch(ms)/average total running time(s).}
\small
\begin{tabular}{@{}cccccc@{}}
\toprule
       & Cora            & Citeseer        & Pubmed          & Computers         & Photo           \\ \midrule
GPRGNN & 17.62ms / 3.74s & 19.28ms / 3.89s & 20.19ms / 5.53s & 39.93ms / 11.40s  & 21.61ms / 6.18s   \\
APPNP  & 17.16ms / 4.00s & 15.97ms / 3.26s & 18.47ms / 6.29s & 39.59ms / 20.00s  & 20.10ms / 10.93s  \\
MLP    & 4.14ms / 0.92s  & 5.30ms / 1.13s    & 5.43ms / 2.86s  & 5.33ms / 2.77s    & 4.63ms / 2.72s    \\
SGC    & 3.31ms / 3.31s  & 11.45ms / 2.31s   & 3.81ms / 3.81s  & 4.36ms / 4.36s    & 19.12ms / 8.75s   \\
GCN    & 9.25ms / 1.97s  & 17.46ms / 3.53s   & 14.11ms / 4.17s & 32.45ms / 16.29s  & 32.56ms / 11.33s  \\
GAT    & 14.78ms / 3.42s & 19.94ms / 4.47s   & 21.52ms / 6.70s & 61.45ms / 24.28s  & 24.57ms / 11.61s  \\
SAGE   & 12.06ms / 2.44s & 41.40ms / 8.36s   & 28.82ms / 6.32s & 171.36ms / 71.94s & 108.88ms / 42.18s \\
JKNet  & 18.97ms / 4.41s & 3.99ms / 3.99s    & 24.48ms / 6.61s & 35.02ms / 14.96s  & 3.66ms / 3.66s    \\
GCN-cheby & 22.96ms / 4.75s & 23.16ms / 4.68s & 45.76ms / 12.02s & 218.82ms / 96.58s & 82.38ms / 30.48s \\ \bottomrule
\end{tabular}
\normalsize
\end{table}

\begin{table}[ht]
\caption{Efficiency on heterophilic real world benchmark datasets: Average running time per epoch(ms)/average total running time(s).}
\small
\label{tab:my-table}
\begin{tabular}{@{}cccccc@{}}
\toprule
          & Chameleon      & Squirrel        & Actor         & Texas         & Cornell       \\ \midrule
GPRGNN    & 16.74ms / 3.40s  & 25.28ms / 5.12s   & 19.31ms / 4.49s & 17.56ms / 3.55s & 18.42ms / 3.72s \\
APPNP     & 17.01ms / 3.44s  & 22.93ms / 4.63s   & 16.32ms / 4.04s & 15.96ms / 3.24s & 14.66ms / 3.09s \\
MLP       & 3.41ms / 0.69s   & 5.19ms / 1.05s    & 4.84ms / 0.98s  & 3.81ms / 1.04s  & 3.46ms / 0.89s  \\
SGC       & 13.83ms / 2.79s  & 27.11ms / 5.56s   & 12.39ms / 2.50s & 10.22ms / 2.06s & 10.38ms / 2.10s \\
GCN       & 16.63ms / 3.63s  & 47.46ms / 10.05s  & 18.91ms / 3.86s & 15.50ms / 3.13s & 13.67ms / 2.76s \\
GAT       & 20.03ms / 5.15s  & 29.89ms / 6.67s   & 23.52ms / 4.75s & 19.67ms / 4.01s & 19.35ms / 3.91s \\
SAGE      & 89.41ms / 18.06s & 440.55ms / 88.99s & 43.94ms / 8.88s & 12.34ms / 3.08s & 12.15ms / 2.69s \\
JKNet     & 3.13ms / 3.13s   & 4.79ms / 4.79s    & 3.98ms / 1.00s  & 2.86ms / 2.09s  & 2.81ms / 1.18s  \\
GCN-cheby & 64.43ms / 13.02s & 343.47ms / 69.38s & 27.95ms / 5.65s & 6.08ms / 1.28s  & 6.05ms / 1.44s  \\ \bottomrule
\end{tabular}
\normalsize
\end{table}
\end{document}
