\section{Experiments}
\label{doc:results}

\subsection{Setup}

\mypara{Input parameterization.}
For the input parameterization, we use  subdivisions in the radial direction,  in the elevation direction, and  in the azimuth direction. The dimensionality of the input histogram is thus . We validate these choices via controlled experiments that are reported in the supplement.

Our approach yields a family of features parameterized by dimension. The primary setting of our feature space dimensionality is ; the corresponding feature is referred to as \mbox{CGF-32}. The experimental results will demonstrate that this low dimensionality significantly outperforms prior, much larger descriptors.

On laser scan data, the radius  of the sphere  is set to  of the diameter of each model and  is set to  of the diameter. The value of the search radius is validated via controlled experiments reported in the supplement. The local reference frame at each point is computed using a search radius of  of the diameter.

On data from SceneNN~\cite{Scenenn2016}, which has absolute metric scale, the radius  is set to 1.2 meters, which is approximately  of the diameter of the grid of each fragment, and  is set to 0.1 meters. The local reference frame is computed using a search radius of 0.25 meters.

\mypara{Laser scan data.}
For experiments with laser scan data, we use a number of public-domain 3D models that are commonly used for this purpose. We use three models from the AIM@SHAPE repository (Bimba, Dancing Children, and Chinese Dragon), four models from the Stanford 3D Scanning Repository (Armadillo, Buddha, Bunny, and Stanford Dragon), and the Berkeley Angel~\cite{Kolluri2004}. Four of these models~-- Angel, Bimba, Bunny, and Chinese Dragon~-- were used as the training set, the Dancing Children model was used for validation, and the remaining three models~-- Armadillo, Buddha, and Stanford Dragon~-- were used as the test set.

For each model in the training and validation sets~-- Angel, Bimba, Bunny, Chinese Dragon, and Dancing Children~-- we synthesize depth images from 14 views uniformly distributed along the surface of an enclosing sphere. For each depth image we construct a point cloud that lies on the model. We compute the set of pairs of point clouds  that overlap in world space by at least . Since some of these models do not have absolute scale, we set parameters and measure precision in relation to the diameter of the model. Synthesizing depth images allows us to automatically generate as much training data as we need and provides a controlled training environment in which we can validate our design choices. We found that descriptors trained on such synthetically scanned models successfully generalize to raw laser scans.

For testing we use the original raw laser scans of the models in our test set -- Armadillo, Buddha, and Stanford Dragon. All three models were scanned with a Cyberware 3030 MS scanner. Armadillo has 114 scans, Buddha has 58 scans, and the Stanford Dragon has 71 scans. Using the provided alignments we compute a set of pairs of scans  that overlap in world space by at least .  These models demonstrate the ability of CGF to generalize to new domains, handle symmetric objects, and cope with noise encountered in laser scanned models.

\mypara{SceneNN data.}
For experiments on real indoor scenes, we use SceneNN~\cite{Scenenn2016}, a comprehensive recent dataset of indoor scenes scanned with consumer depth cameras. Starting from the raw SceneNN scans, we create fragments and register them using the pipeline of Choi et al.~\cite{Choi2015}. Each fragment is fused from 100 consecutive frames.

50\% of the scenes are used for training, 25\% for validation, and 25\% as the test set, split randomly. We will publish our train/val/test split so that others can replicate our experiments. Let  be the set of pairs of overlapping fragments in the training scenes. For maximally precise alignment during training, we refined the registration of each pair  using ICP \cite{Besl1992}. We use the implementation of ICP provided in the Point Cloud Library \cite{Holz2015}.




\mypara{Training.}
For each point cloud in the training set (synthetic depth image in the case of laser scan data, scene fragment in the case of SceneNN), we sample 40 triplets per point. Of these 40 triplets, 15 are constructed by sampling negatives from , as described in Section~\ref{sec:training}. The remaining 25 are constructed by sampling negatives from the entire model. The threshold  is set to  of the model's diameter in the case of laser scans and 7.5 cm in the case of SceneNN.



\mypara{Baselines.}
We compare CGF to six well-known local descriptors: Point Feature Histograms (PFH)~\cite{Rusu2008-IROS} (dimensionality 125),  Fast Point Feature Histograms (FPFH)~\cite{Rusu2009} (dimensionality 33), Rotational Projection Statistics (RoPS)~\cite{Guo2013} (dimensionality 135), Signature of Histogram Orientations (SHOT)~\cite{Salti2014} (dimensionality 352), Spin Images (SI)~\cite{JohnsonHebert1999} (dimensionality 153), and Unique Shape Contexts (USC)~\cite{Tombari2010b} (dimensionality 1,980). For RoPS we use the implementation provided by the authors~\cite{Guo2013}. For all other baselines we use the implementations provided in the Point Cloud Library~\cite{Holz2015}.
Each of these existing geometric feature descriptors has several parameters that need to be tuned to ensure good performance. We performed extensive hyperparameter sweeps to ensure that each baseline performed as well as possible in our experiments.

We have also applied Principal Components Analysis (PCA) to embed our input 2,244-dimensional histograms into , using our primary dimensionality . This evaluates the advantage of the presented nonlinear feature embedding over a linear embedding of the same input into the same space.

Additional baselines and controlled experiments are reported in the supplement.



\begin{figure*}[!t]
\centering
\begin{minipage}{0.97\textwidth}
\centering
\begin{tabular}{c c c}
    \includegraphics[width = .43\linewidth]{{laser}.pdf} &
    \includegraphics[width = .43\linewidth]{scenenn.pdf} &
    \includegraphics[trim= 0 -1cm 0 0, width = .11\linewidth]{legend-precision.png}\\
     \small{(a) Precision on laser scan data test set} & \small{(b) Precision on the SceneNN test set} & \\
\end{tabular}
\end{minipage}\hfill
\vspace{1mm}
\caption{(a) The precision of several geometric feature descriptors on laser scan data in the test set. For correspondences provided by CGF-32, 41.4\% are precise to within  of the diameter. Prior feature descriptors are less accurate. (b) Precision of local geometric features on pairs of fragments from the SceneNN test set. CGF-32 yields the highest precision: 50.6\% of the matches computed in the learned feature space are within 10 cm of the ground truth. USC (a \mbox{1,980-dimensional} descriptor) comes in second at 29.8\%.}
\label{fig:precision}
\vspace{-1mm}
\end{figure*}

\begin{figure*}[!t]
\centering
\begin{minipage}{0.97\textwidth}
\centering
\begin{tabular}{c c c}
     \includegraphics[width = .43\linewidth]{laser-timings-1.pdf} &
     \includegraphics[width = .43\linewidth]{scenenn-timings.pdf} &
     \includegraphics[trim=-0 -1.3cm 0 0, width = .1\linewidth]{legend.png}\\
    \small{(a) Query time and precision on laser scan data test set} & \small{(b) Query time and precision on the SceneNN test set} & \\
\end{tabular}
\end{minipage}\hfill
\vspace{1mm}
\caption{(a) The query time and precision of several geometric feature descriptors on laser scan data in the test set. CGF-32 has an average query time of 0.42 ms, 3.9 times faster than the second most accurate feature (SI, 1.62 ms). (b) The query time and precision of local geometric features on pairs of fragments from the SceneNN test set. CGF-32 has an average query time of 0.1 ms, 67 times faster than the second most accurate feature (USC, 6.75 ms). The horizontal axis (time) is on a logarithmic scale.}
\label{fig:timings}
\vspace{-1mm}
\end{figure*}


\mypara{Accuracy measure.}
Let , , and  be defined as in Section \ref{sec:training} and consider an overlapping pair . Given a function  that maps points to geometric features, a set of correspondences between  and  can be found by first computing the sets of geometric features  and . Then we build a -d tree  on the set . For each point , we compute  by performing a nearest neighbor query in . Define

as the set of matches yielded by the feature .

Since  only partially overlaps with , we first discard all correspondences  such that

These points have no ground-truth correspondence in . Let  denote the remaining set of correspondences.

For any distance threshold , we can compute the fraction of matches that are within distance  of the ground truth:

This will be our primary measure for evaluating the accuracy of different features .

\mypara{Timings.}
Average correspondence search times for different descriptors were benchmarked using a single thread on an Intel Xeon E7-8890 2.5 GHz CPU. We use FLANN~\cite{flann2014} to perform nearest-neighbor queries.




\subsection{Laser scan data}
\label{sec:synthetic}

Precision of different features on the test set is shown in Figure~\ref{fig:precision}(a). CGF-32 is much more accurate than existing descriptors. For example,  of the correspondences produced by CGF-32 lie within  of the model's diameter of the true match, whereas the most precise prior feature, SI, yields only  precision at this distance. CGF-32 improves over SI by  in relative precision while being  times more compact.

\begin{figure*}[!t]
\centering
\begin{minipage}{0.97\textwidth}
\centering
\begin{tabular}{c c c c c c c}
    \raisebox{8mm}{\rot{Laser scans}} &
    \includegraphics[width = .165\linewidth]{source.jpeg} &
    \includegraphics[width = .165\linewidth]{target.jpeg} &
    \includegraphics[width = .165\linewidth]{fpfh.jpeg} &
    \includegraphics[width = .165\linewidth]{usc.jpeg} &
    \includegraphics[width = .165\linewidth]{ours.jpeg} &
    \includegraphics[height= 0.2\linewidth]{lucy-bar.pdf}\\
    \raisebox{8mm}{\rot{SceneNN}} &
    \includegraphics[width = .165\linewidth]{render-source-shadowless.jpg} &
    \includegraphics[width = .165\linewidth]{render-target-shadowless.jpg} &
    \includegraphics[width = .165\linewidth]{render-fpfh.jpg} &
    \includegraphics[width = .165\linewidth]{render-usc.jpg} &
    \includegraphics[width = .165\linewidth]{render-ours.jpg} &
    \includegraphics[height = 0.2\linewidth]{bar-scenenn.pdf}\\
    & \small{(a) source} & \small{(b) target} & \small{(c) FPFH} & \small{(d) USC} & \small{(e) CGF-32}\\
\end{tabular}
\end{minipage}\hfill
\vspace{1mm}
\caption{{\bf Top.} (a,b) Two laser scans of the Buddha statue. (c-e) Error magnitudes of matches established across the two scans in different feature spaces. CGF provides broad coverage of the surface with accurate matches. Units are in percentage of the model's diameter: black corresponds to error of 3\% of the diameter or higher.
{\bf Bottom.} (a,b) Two fragments in the SceneNN test set. (c-e) Error magnitudes of correspondences established across these fragments in different feature spaces. Black corresponds to errors of 25 cm or higher. Correspondences established via CGF are more precise on average. Note the thin structure above the large hole in the middle of the fragment, along which all other feature spaces fail to establish good correspondences.
Points shown in grey do not have a ground-truth correspondence on the other point cloud.
}
\label{fig:vis-error}
\vspace{-1mm}
\end{figure*}


\begin{comment}
\begin{figure}[ht]
\vspace{-1em}
\centering
\includegraphics[width=1.0\linewidth]{laser.pdf}
\caption{The precision of several geometric feature descriptors on laser scan data in the test set. For correspondences provided by our feature space, 41.4\% are precise to within  of the diameter. Prior feature descriptors are less accurate.}
\label{fig:laserscan}
\end{figure}
\end{comment}

\mypara{Timings.}
Query times for different features are presented in Figure~\ref{fig:timings}. CGF-32 has an average query time of  ms, which is  times faster than the second most accurate feature (SI, 1.62 ms) and  times faster than USC (31.6 ms). CGF-12 has an average query time of  ms, slightly slower than the fastest feature (FPFH, 0.04 ms) while being more precise than all baselines (33.2\% at  of the diameter).

\begin{comment}
\begin{figure}[h]
\centering
    \includegraphics[width = 1\linewidth]{laser-timings-1.pdf}
\caption{Query time and precision on laser scans.}
\label{fig:timing-lasers}
\end{figure}
\end{comment}
\mypara{Visualization.}
Figure~\ref{fig:vis-error} (top) shows error distributions of correspondences established in different feature spaces over two laser scans of the Buddha statue.


\subsection{SceneNN data}
\label{sec:scenenncorr}

Precision of different feature descriptors on real-world scene fragments from the SceneNN test set is shown in Figure~\ref{fig:precision}(b). 50.6\% of the matches established with CGF-32 are within 10 cm of the true match, much more than USC (29.8\%), RoPS (), PFH (), FPFH (), SHOT (), and SI ().
The baseline constructed by applying PCA to our 2,244-dimensional input parameterization yielded precision of , far lower than the precision of the learned nonlinear embedding into the same space. Note that the second highest performing feature on laser scan data, SI, performed poorest on SceneNN.

\begin{comment}
\begin{figure}[!ht]
\centering
\includegraphics[width=1.0\linewidth]{{scenenn}.pdf}
\caption{Precision of local geometric features on pairs of fragments from the SceneNN test set. CGF-32 yields the highest precision: 50.6\% of the matches computed in the learned feature space are within 10 cm of the ground truth. USC (a \mbox{1,980-dimensional} descriptor) comes in second at 29.8\%.}
\label{fig:scenenn}
\end{figure}
\end{comment}

\mypara{Timings.}
Query times for different features are presented in Figure~\ref{fig:timings}. CGF-32 has an average query time of  ms,  times faster than the second most accurate feature (USC,  ms). CGF-12 has an average query time of  ms, matching the speed of FPFH. In addition to its speed, \mbox{CGF-12} is more precise than all other features, with  of correspondences within 10 cm of the true match.

\begin{comment}
\begin{figure}[h]
\centering
    \includegraphics[width = 1\linewidth]{scenenn-timings.pdf}
\caption{Query time and precision on the SceneNN test set.}
\label{fig:timing-scenenn}
\end{figure}
\end{comment}

\mypara{Visualization.}
Figure~\ref{fig:vis-error} (bottom) shows error distributions of correspondences established in different feature spaces over two fragments in the SceneNN test set.

\subsection{Visualization}
\label{sec:visualization}
To get a qualitative sense of the learned representation, we can visualize the variation of the learned features over the surface of any model. Specifically, we can use PCA to project from the learned feature space into the \mbox{3-dimensional} RGB color space. Given a point set, we can evaluate the learned feature for every point, use the learned linear mapping to obtain the corresponding color, and assign this color to the point. Figure \ref{fig:dc-vis} shows the result of this procedure for two synthesized views of the Dancing Children model. Note that the feature mapping appears stable, coherent, and discriminative. Corresponding points on the two views of the model tend to have similar color. Color varies more rapidly in regions of high-frequency geometric variation and is more stable in regions that are geometrically more uniform.

\begin{figure}[h]
\centering
\begin{tabular}{c c}
    \includegraphics[width = .45\linewidth]{render-1.jpg} &
    \includegraphics[width = .45\linewidth]{render-2.jpg}
\end{tabular}
\vspace{0.5mm}
\caption{Visualization of local features over two views of the Dancing Children model. Features were projected from the learned feature space into the RGB color space.}
\label{fig:dc-vis}
\end{figure}

\begin{table*}[ht!]
\footnotesize
\centering
\begin{tabularx}{.95\linewidth}{|*{1}{@{}>{\setlength{\hsize}{1.4\hsize}}X@{}|}|*{6}{@{}>{\setlength{\hsize}{0.95\hsize}}X@{}|}|*{5}{@{}>{\setlength{\hsize}{0.95\hsize}}X@{}|}}
\hline
& OpenCV \cite{Drost2010} & Super 4PCS \cite{Mellado2014} & PCL \cite{Rusu2009,Holz2015} & FGR \cite{Zhou2016} & CZK \cite{Choi2015} & 3DMatch \cite{Zeng2016} & FGR with CGF-32 & CZK with CGF-32\\\hline
Recall~(\%)& 5.3 & 17.8 & 44.9 & 51.1& 59.2 & 65.1 & 60.7 & \textbf{72.0}\\\hline
Precision~(\%) & 1.6 & 10.4 & 14.0 & 23.2 & 19.6 & \textbf{25.2} & 9.4 & 14.6 \\\hline
\end{tabularx}
\normalsize
\vspace{2mm}
\caption{Evaluation on the Redwood benchmark. Plugging our learned descriptor into a pre-existing registration pipeline (CZK) yields the highest recall reported on the benchmark to date, with no training or fine-tuning on this dataset.}
\label{tab:matching}
\end{table*}


\subsection{Geometric registration}
\label{sec:registration}

We now evaluate the utility of CGF in geometric registration. For this purpose, we use Fast Global Registration (FGR)~\cite{Zhou2016}, a state-of-the-art global registration algorithm that relies on feature matching. Since feature matching is the computational bottleneck of the algorithm, using a compact feature is important. The authors' implementation of FGR uses FPFH~\cite{Zhou2016}. We use the published FGR pipeline as the baseline. To evaluate the utility of the learned CGF descriptor, we simply replace FPFH by CGF in the FGR pipeline.



To evaluate geometric registration accuracy, we follow the evaluation protocol of Choi et al.~\cite{Choi2015}, which was also used by Zhou et al.~\cite{Zhou2016} and Zeng et al.~\cite{Zeng2016} .

\begin{comment}

        POTENTIAL MATERIAL FOR SUPPLEMENT:

        For each overlapping pair of point clouds  in the test set, we construct a set of ground-truth correspondences .  First we align  and  in world space using  and  and then compute the nearest neighbor  for every point in . A correspondence  is included in  if the distance  is less than 1\% of the diameter of the model (or 7.5 cm on SceneNN data).


        We say that an alignment  between a pair  is correct if the RMSE is less than  of the diameter of the model (or 20 cm on real-world data). Registration accuracy is defined to be the fraction of correct alignments over the total number of pairs .

\end{comment}


\mypara{Laser scans.}
We compare the accuracy of FGR when FPFH is used to the accuracy of FGR when CGF-32 is used. On the laser scan test set, FGR with FPFH correctly aligns  of the pairs while FGR with CGF-32 correctly aligns . The average RMSE of FGR with FPFH is  of the diameter, while the average RMSE of FGR with CGF-32 is  of the diameter.

\begin{comment}
    Figure \ref{fig:lucyfgr} shows alignments computed for two pairs from Lucy by FGR using our feature space and FPFH. For the first pair both methods succeed, but the aligment computed using our feature space is tighter, exhibiting less error per point than that computed using FPFH. For the second pair FGR using FPFH fails to compute a correct alignment. Using our feature space, FGR is able to find a tight alignment between the fragments.
    \begin{figure*}[!t]
    \begin{center}
    \begin{minipage}{0.97\textwidth}
    \begin{tabular}{c c c c}
        \includegraphics[width = .22\linewidth]{render-1-source.png} &
        \includegraphics[width = .22\linewidth]{render-1-fpfh.png} &
        \includegraphics[width = .22\linewidth]{render-2-source.png} &
        \includegraphics[width = .22\linewidth]{render-2-fpfh.png}\\
         \small{(a) source 1} & \small{(b) FGR + FPFH } & \small{(c) source 2} & \small{(d) FGR + FPFH} \\
        \includegraphics[width = .22\linewidth]{render-1-target.png} &
        \includegraphics[width = .22\linewidth]{render-1-ours.png} &
        \includegraphics[width = .22\linewidth]{render-2-target.png} &
        \includegraphics[width = .22\linewidth]{render-2-ours.png} \\
        \small{(e) target 1} & \small{(f) FGR + Ours} & \small{(g) target 2} & \small{(h) FGR + Ours}
    \end{tabular}
    \end{minipage}\hfill
    \begin{minipage}{0.03\textwidth}
    \includegraphics{fgr_bar.pdf}
    \end{minipage}
    \end{center}
\caption{For the pair (a) and (e), FGR is able to find a correct alignment using either FPFH (b) or our features (f). However the per point error, visualized as a color from yellow (0\% of diameter) to green (1\% of diameter), is lower for our feature space. For the pair (c) and (g), FGR fails to find an alignment using FPFH (d). Using our features, FGR finds a tight alignment (h).}
    \label{fig:lucyfgr}
    \end{figure*}
\end{comment}

\mypara{SceneNN.}
On the SceneNN test set, FGR with FPFH correctly aligns  of the fragment pairs, while FGR with CGF-32 correctly aligns . The average RMSE of FGR with FPFH is 14.86 cm, while the average RMSE of FGR with CGF-32 is 11.83 cm.

If we focus on the correctly aligned pairs and evaluate the average RMSE only across those, FGR with FPFH yields an RMSE of 4.68 cm and FGR with CGF-32 yields an average RMSE of 4.07 cm. This in effect evaluates the tightness of the alignment produced by global registration. \mbox{CGF-32} provides more precise correspondence pairs, which yield tighter alignment.


\begin{comment}
      \begin{figure*}[!t]
      \centering
      \begin{minipage}{0.97\textwidth}
      \begin{tabular}{c c c c}
          \includegraphics[width = .22\linewidth]{render-source-shadowless.jpg} &
          \includegraphics[width = .22\linewidth]{render-fpfh.jpg} &
          \includegraphics[width = .22\linewidth]{render-shot.jpg} &
          \includegraphics[width = .22\linewidth]{render-usc.jpg}\\
           \small{(a) source} & \small{(b) FPFH } & \small{(c) SHOT} & \small{(d) USC} \\
          \includegraphics[width = .22\linewidth]{render-target-shadowless.jpg} &
          \includegraphics[width = .22\linewidth]{render-pfh.jpg} &
          \includegraphics[width = .22\linewidth]{render-spin.jpg} &
          \includegraphics[width = .22\linewidth]{render-ours.jpg} \\
          \small{(e) target} & \small{(f) PFH} & \small{(g) SI} & \small{(h) Ours}
      \end{tabular}
      \end{minipage}\hfill
      \begin{minipage}{0.03\textwidth}
      \includegraphics{bar.pdf}
      \end{minipage}
      \vspace{1mm}
      \caption{(a,e) Two fragments in the SceneNN test set. (b-d,f-h) Error magnitudes of correspondences established across these fragments in different feature spaces. Black corresponds to errors of 25 cm or higher. Points shown in grey do not have a ground-truth correspondence on the other fragment. Correspondences established in our feature space (h) are more precise on average. Note the thin structure above the large hole in the middle of the fragment, along which all other feature spaces fail to establish good correspondences.}
      \label{fig:scenenn-error}
      \end{figure*}
\end{comment}

\mypara{Cross-dataset generalization: Redwood benchmark.}
We now evaluate on the global registration benchmark of Choi et al.~\cite{Choi2015}. This benchmark has four datasets, each containing tens of scene fragments. Geometric registration is performed on every pair of fragments, with no initialization. For this experiment, we use the feature embedding that was trained on the SceneNN dataset. We did not retrain or fine-tune the descriptor in any way. This demonstrates the learned descriptor's ability to generalize to new datasets, as well as its ability to serve as a drop-in replacement in preexisting pipelines that depend upon discriminative geometric features.

The results are reported in Table~\ref{tab:matching}. We report all the baselines from the evaluation conducted on this dataset by Zhou et al.~\cite{Zhou2016}. We plug CGF-32 into FGR~\cite{Zhou2016} and CZK~\cite{Choi2015}, the existing implementations of which use the FPFH feature. This yields the corresponding ``FGR with CGF-32'' and ``CZK with CGF-32'' conditions.

CGF improves the recall of each method by more than 9 percentage points. With \mbox{CGF-32}, the CZK pipeline achieves a recall of 72\%, by far the highest reported on the benchmark. Note that this is 6.9 percentage points higher than the contemporaneous results of Zeng et al.~\cite{Zeng2016}.



Choi et al.~\cite{Choi2015} defined two evaluation measures: recall and precision. Recall is the primary measure. The importance of recall is driven by two factors. First, the maximal level of precision that can be achieved by pairwise registration methods is low due to symmetric structures and other sources of geometric aliasing. Second, there are known ways to raise precision. Given a set of pairwise alignments, robust optimization of all fragments can prune false positives, retaining a given level of recall but increasing precision dramatically~\cite{Choi2015}.

The effect of robust optimization is demonstrated in Table~\ref{tab:pruning}. Given pairwise alignments produced by CZK with CGF-32 features, robust optimization removes false positives and yields a set of pairwise alignments with  recall and  precision. The accuracy of this final result is limited not by the precision of the input set of pairwise alignments~-- as the results demonstrate, the overall pipeline is robust to low precision~-- but by the level of recall. Similar precision can be achieved by applying the framework of Choi et al.~\cite{Choi2015} to any of the prior works in Table~\ref{tab:matching}.

\begin{table}[h!]
\begin{center}
\footnotesize
\begin{tabularx}{\linewidth}{|*{1}{@{}>{\setlength{\hsize}{1.2\hsize}}X@{}|}|*{4}{@{}>{\setlength{\hsize}{0.95\hsize}}X@{}|}}
\hline
\multirow{2}{*}{ } & \multicolumn{2}{c|}{Before pruning} & \multicolumn{2}{c|}{After pruning}\\\cline{2-5}
& FGR with CGF-32 & CZK with CGF-32 & FGR with CGF-32 & CZK with CGF-32 \\\hline
Recall (\%) & 60.7{} & 72.0{} & 60.7{} & 71.1{}\\\hline
Precision (\%) & 9.4{} & 14.6{} & 86.8{} & 95.1{}\\\hline
\end{tabularx}
\normalsize
\end{center}
\caption{After post-processing with robust global optimization~\cite{Choi2015}, CZK with CGF-32 achieves 71.1\% recall and 95.1\% precision on the Redwood benchmark.}
\label{tab:pruning}
\end{table}



