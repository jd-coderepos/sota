\subsection{Proof of Proposition~\ref{prop:term-upper-critical}} \label{app:term-upper-critical}

In this subsection we prove Proposition~\ref{prop:term-upper-critical}.
Given a finite set~$\Gamma$, we regard the elements of $\Rset^\Gamma$ as vectors.
Given two vectors $\vu, \vv \in \Rset^\Gamma$, we define a scalar product by setting $\vu \thickdot \vv := \sum_{X \in \Gamma} \vu(X) \cdot \vv(X)$.
Further, elements of~$\Rset^{\Gamma \times \Gamma}$ are regarded as matrices, with the usual matrix-vector multiplication.

It will be convenient for the proof to measure the termination time of pBPA starting in an arbitrary initial configuration~$\alpha_0 \in \Gamma\Gamma^*$,
 not just with a single initial symbol $X_0 \in \Gamma$.
To this end we generalize $\termt{X_0}$, $\run(X_0)$, etc.\ to $\termt{\alpha_0}$, $\run(\alpha_0)$, etc.\ in the straightforward way.

It will also be convenient to allow ``pBPA'' that have transition rules with more than two stack symbols on the right-hand side.
We call them {\em relaxed pBPA}.
All concepts associated to a pBPA, e.g., the induced Markov chain, termination time, etc., are defined analogously for relaxed pBPA.

A relaxed pBPA is called {\em strongly connected}, if the DAG of the dependence relation on its stack alphabet consists of a single SCC.

For any $\alpha \in \Gamma^*$, define $\numbsymb{\alpha}$ as the Parikh image of~$\alpha$,
 i.e., the vector of $\Nset^{\Gamma}$ such that $\numbsymb{\alpha}(Y)$ is the number of occurrences of $Y$ in $\alpha$.
Given a relaxed pBPA~$\Delta$, let $A_\Delta \in \Rset^{\Gamma\times\Gamma}$ be the matrix with
 \[
  A_\Delta(X,Y) = \sum_{X \btran{p} \alpha} p \cdot \numbsymb{\alpha}(Y)\,.
 \]
We drop the subscript of~$A_\Delta$ if $\Delta$ is clear from the context.
Intuitively, $A(X,Y)$ is the expected number of $Y$-symbols pushed on the stack when executing a rule with $X$ on the left hand side.
For instance, if $X \btran{1/5} XX$ and $X \btran{4/5} \varepsilon$, then $A(X,X) = 2/5$.
Note that $A$ is nonnegative.
The matrix~$A$ plays a crucial role in the analysis of pPDA and related models (see e.g.~\cite{EY:RMC-SG-equations-JACM})
 and in the theory of branching processes~\cite{Harris:book}.
We have the following lemma:
\begin{lemma} \label{lem:cone-vector}
 Let $\Delta$ be an almost surely terminating, strongly connected pBPA.
 Then there is a positive vector $\vu \in \Rset_+^\Gamma$ such that
  $A \cdot \vu \le \vu$, where $\mathord{\le}$ is meant componentwise.
 All such vectors~$\vu$ satisfy $\frac{\umin}{\umax} \ge \pmin^{|\Gamma|}$,
  where $\pmin$ denotes the least rule probability in~$\Delta$,
  and $\umin$ and $\umax$ denote the least and the greatest component of~$\vu$, respectively.
\end{lemma}
\begin{proof}
  Let $X,Y \in \Gamma$.
  Since $\Delta$ is strongly connected, there is a sequence $X = X_1, X_2, \ldots, X_n = Y$ with $n \ge 1$ such that $X_i$ depends directly on~$X_{i+1}$
   for all $1 \le i \le n-1$.
  A straightforward induction on~$n$ shows that $A^n(X,Y) \ne 0$; i.e., $A$ is {\em irreducible}.
  The assumption that $\Delta$ is almost surely terminating implies that
   the spectral radius of~$A$ is less than or equal to one, see, e.g., Section 8.1 of~\cite{EY:RMC-SG-equations-JACM}.
Perron-Frobenius theory (see, e.g., \cite{book:BermanP}) then implies that there is a positive vector $\vu \in \Rset_+^\Gamma$ such that
   $A \cdot \vu \le \vu$;
   e.g., one can take for~$\vu$ the dominant eigenvector of~$A$.

  Let $A \cdot \vu \le \vu$.
  It remains to show that $\frac{\umin}{\umax} \ge \pmin^{|\Gamma|}$.
  The proof is essentially given in~\cite{EKL10:SICOMP}, we repeat it for convenience.
  W.l.o.g.\ let $\Gamma = \{X_1, \ldots, X_{|\Gamma|}\}$.
  We write $\vu_i$ for $\vu(X_i)$.
  W.l.o.g.\ let $\vu_1 = \umax$ and $\vu_{|\Gamma|} = \umin$.
  Since $\Delta$ is strongly connected, there is a sequence $1 = r_1, r_2, \ldots, r_q = |\Gamma|$ with $q \le |\Gamma|$
   such that $X_{r_j}$ depends on~$X_{r_{j+1}}$ for all $j$.
  We have
   \[
    \frac{\umin}{\umax} = \frac{\vu_{|\Gamma|}}{\vu_1} = \frac{\vu_{r_q}}{\vu_{r_{q-1}}} \cdot \ldots \cdot \frac{\vu_{r_2}}{\vu_{r_1}} \,.
   \]
  By the pigeonhole principle there is $j$ with $2 \le j \le q$ such that
   \begin{equation}
    \frac{\umin}{\umax} \ge \left( \frac{\vu_s}{\vu_t} \right)^{q-1} \ge \left( \frac{\vu_s}{\vu_t} \right)^{|\Gamma|}\quad
      \text{where $s := r_j$ and $t := r_{j-1}$.} \label{eq:cone-vector-pigeonhole}
   \end{equation}
  We have $A \cdot \vu \le \vu$, which implies $A(X_s, X_t) \cdot \vu_t \le \vu_s$ and so $A(X_s, X_t) \le {\vu_s}/{\vu_t}$.
  On the other hand, since $X_s$ depends on~$X_t$, we clearly have $\pmin \le A(X_s, X_t)$.
  Combining those inequalities with~\eqref{eq:cone-vector-pigeonhole} yields $\frac{\umin}{\umax} \ge \left(A(X_s, X_t)\right)^{|\Gamma|} \ge \pmin^{|\Gamma|}$.
\qed
\end{proof}

Given a relaxed pBPA~$\Delta$ and vector $\vu \in \Rset_+^\Gamma$,
 we say that $\Delta$ is {\em $\vu$-progressive}, if $\Delta$ has, for all $X \in \Gamma$,
 a rule $X \btran{} \alpha$ such that $|\vu(X) - \numbsymb{\alpha} \thickdot \vu| \ge \umin/2$.
The following lemma states that, intuitively, any pBPA can be transformed into a $\vu$-progressive relaxed pBPA
 that is at least as fast but no more than ${|\Gamma|}$ times faster.
\begin{lemma} \label{lem:progressive}
 Let $\Delta$ be an almost surely terminating pBPA with stack alphabet~$\Gamma$.
 Let $\pmin$ denote the least rule probability in~$\Delta$, and let $\vu \in \Rset_+^\Gamma$ with $A_\Delta \cdot \vu \le \vu$.
 Then one can construct a $\vu$-progressive, almost surely terminating relaxed pBPA~$\Delta'$
  with stack alphabet~$\Gamma$ such that for all $\alpha_0\in\Gamma^*$ and for all $a \ge 0$
 \[
  \calP'(\mathbf{T}_{\alpha_0} \ge a) \quad \le \quad \calP( \mathbf{T}_{\alpha_0} \ge a ) \quad \le \quad \calP'(\mathbf{T}_{\alpha_0} \ge a / |\Gamma|)\,,
 \]
 where $\calP$ and $\calP'$ are the probability measures associated with $\Delta$ and $\Delta'$, respectively.
 Furthermore, the least rule probability in~$\Delta'$ is at least $\pmin^{|\Gamma|}$, and $A_{\Delta'} \cdot \vu \le \vu$.
 Finally, if $A_\Delta \cdot \vu = \vu$, then $A_{\Delta'} \cdot \vu = \vu$.
\end{lemma}
\begin{proof}
\newcommand{\Con}{\mathit{Con}}
A sequence of transitions $X_1 \btran{} \alpha_1, \ldots,  X_n \btran{} \alpha_n$ is called {\em derivation sequence from $X_1$ to $\alpha_n$},
 if for all $i \in \{2, \ldots, n\}$ the symbol $X_i \in \Gamma$ occurs in $\alpha_{i-1}$.
The {\em word induced} by a derivation sequence $X_1 \btran{} \alpha_1, \ldots,  X_n \btran{} \alpha_n$
 is obtained by taking $\alpha_1$, replacing an occurrence of~$X_2$ by~$\alpha_2$,
 then replacing an occurrence of~$X_3$ by~$\alpha_3$, etc., and finally replacing an occurrence of~$X_n$ by~$\alpha_n$.

Given a pBPA~$\Delta$ and a derivation sequence
 $s = \big( X_1 \btran{p_1} \alpha_1^1 X_2 \alpha_1^2, X_2 \btran{p_2} \alpha_2, \ldots,  X_n \btran{p_n} \alpha_{n} \big)$
 with $X_i \ne X_j$ for all $1 \le i < j \le n$,
we define the {\em contraction} $\Con(s)$ of~$s$, a set of $X_1$-transitions with possibly more than two symbols on the right hand side.
The contraction $\Con(s)$ will include a rule $X_1 \btran{} \gamma$, where $\gamma$ is the word induced by~$s$.
We define~$\Con(s)$ inductively over the length~$n$ of~$s$.
If $n = 1$, then $\Con(s) = \{X_1 \btran{p_1} \alpha_1^1 X_2 \alpha_1^2\}$.
If $n \ge 2$, let $s' = \big( X_2 \btran{p_2} \alpha_{2}, \ldots,  X_n \btran{p_n} \alpha_{n} \big)$ and define
 \begin{equation}
  \delta_2 := \left\{X_2 \btran{} \beta \mid \text{$X_2 \btran{} \beta$ is a rule in~$\Delta$} \right\}
    - \left\{ X_2 \btran{p2} \alpha_2 \right\}
    \cup \Con(s')\,; \label{eq:delta-2}
 \end{equation}
 i.e., $\delta_2$ is the set of $X_2$-transitions in~$\Delta$ with $X_2 \btran{p2} \alpha_2$ replaced by $\Con(s')$.
W.l.o.g.\ assume $\delta_2 = \{ X_2 \btran{q_{1}} \beta_1, \ldots, X_2 \btran{q_{k}} \beta_k \}$.
Then we define
 \[
  \Con(s) := \left\{ X_1 \btran{p_1q_1} \alpha_1^1 \beta_1 \alpha_1^2, \ldots, X_1 \btran{p_1q_k} \alpha_1^1 \beta_k \alpha_1^2 \right\}\,.
 \]
The following properties are easy to show by induction on~$n$:
\begin{itemize}
 \item[(a)]
  $\Con(s)$ contains $X_1 \btran{} \gamma$, where $\gamma$ is the word induced by~$s$.
 \item[(b)]
  The rule probabilities are at least $\pmin^n$.
 \item[(c)]
  Let $\Delta'$ be the relaxed pBPA obtained from~$\Delta$ by replacing $X_1 \btran{p_1} \alpha_1^1 X_2 \alpha_1^2$ with $\Con(s)$.
  Then each path in $M_{\Delta'}$ corresponds in a straightforward way to a path in~$M_\Delta$,
   namely to the path obtained by ``re-expanding'' the contractions.
  The corresponding path in~$M_\Delta$ has the same probability
   and is not shorter but at most $|\Gamma|$ times longer than the one in~$M_{\Delta'}$.
 \item[(d)]
  Let $\Delta'$ be as in~(c).
  Then $A_{\Delta'} \cdot \vu \le \vu$.
  Let us prove that explicitly.
  The induction hypothesis $n=1$ is trivial.
  For the induction step, using the definition for~$\delta_2$ in~\eqref{eq:delta-2} and
   $\delta_2 = \{ X_2 \btran{q_{1}} \beta_1, \ldots, X_2 \btran{q_{k}} \beta_k \}$,
   we know by the induction hypothesis that $\sum_{i=1}^k q_i \cdot \numbsymb{\beta_i} \thickdot \vu  \le \vu(X_2)$.
  This implies
   \begin{align*}
    \sum_{i=1}^k p_1 q_i \cdot \numbsymb{\alpha_1^1\beta_i\alpha_1^2} \thickdot \vu & \le p_1 \cdot \numbsymb{\alpha_1^1 X_2 \alpha_1^2} \thickdot \vu \,,
     \quad \text{and hence} \\
    \quad \left(A_{\Delta'} \cdot \vu\right)(X_1) & \le \left( A_{\Delta} \cdot \vu \right)(X_1) \le \vu(X_1)\,.
   \end{align*}
   Since $A_{\Delta}$ and $A_{\Delta'}$ may differ only in the $X_1$-row, we have $A_{\Delta'} \cdot \vu \le \vu$.
 \item[(e)]
  Let $\Delta'$ be as in (c) and~(d).
  If $A_{\Delta} \cdot \vu = \vu$, then $A_{\Delta'} \cdot \vu = \vu$.
  This follows as in~(d), with the inequality signs replaced by equality.
\end{itemize}

Associate to each symbol $X_1 \in \Gamma$ a shortest derivation sequence
 \[
  c(X_1) = \big( X_1 \btran{} \alpha_1, \ldots,  X_{n-1} \btran{} \alpha_{n-1}, X_n \btran{} \varepsilon \big)
 \]
  from $X_1$ to~$\varepsilon$.
Since $\Delta$ is almost surely terminating, the length of~$c(X_1)$ is at most~$|\Gamma|$ for all $X_1 \in \Gamma$.
Let $X_1 \in \Gamma$,
 and let $\gamma_1$ denote the word induced by $c(X_1)$,
 and let $\gamma_2$ denote the word induced by the derivation sequence $c_2(X_1) := \big( X_1 \btran{} \alpha_1, \ldots,  X_{n-1} \btran{} \alpha_{n-1} \big)$.
We have $\numbsymb{\gamma_2} \thickdot \vu =  \numbsymb{\gamma_1} \thickdot \vu + \vu(X_n) \ge \numbsymb{\gamma_1} \thickdot \vu + \umin$,
 so we can choose $\gamma \in \left\{\gamma_1, \gamma_2\right\}$ such that $|\vu(X_1) - \numbsymb{\gamma} \thickdot \vu| \ge \umin/2$.
Choose $\hat c(X_1) \in \{c(X_1), c_2(X_1)\}$ such that $\hat c(X_1)$ induces~$\gamma$.
(Of course, if $c_2(X_1)$ has length zero, take $\hat c(X_1) = c(X_1)$.)
Note that $(X_1 \btran{} \gamma) \in \Con(\hat c(X_1))$.

The relaxed pBPA~$\Delta'$ from the statement of the lemma is obtained by replacing, for all $X_1 \in \Gamma$,
 the first rule of~$\hat c(X_1)$ with $\Con(\hat c(X_1))$.
The properties (a)--(e) from above imply:
\begin{itemize}
 \item[(a)]
  The relaxed pBPA $\Delta'$ is $\vu$-progressive.
 \item[(b)]
  The rule probabilities are at least $\pmin^{|\Gamma|}$.
 \item[(c)]
  For each finite path $w'$ in~$M_{\Delta'}$ from some $\alpha_0 \in \Gamma^*$ to~$\varepsilon$ there is a
   finite path $w$ in~$M_{\Delta}$ from $\alpha_0$ to~$\varepsilon$ such that $|w'| \le |w| \le |\Gamma| \cdot |w'|$ and $\calP'(w') = \calP(w)$.
  Hence, $\calP'(\mathbf{T}_{\alpha_0} < a / |\Gamma|) \le \calP(\mathbf{T}_{\alpha_0} < a) \le \calP'(\mathbf{T}_{\alpha_0} < a)$ holds for all $a \ge 0$,
   which implies $\calP'(\mathbf{T}_{\alpha_0} \ge a) \le \calP( \mathbf{T}_{\alpha_0} \ge a ) \le \calP'(\mathbf{T}_{\alpha_0} \ge a / |\Gamma|)$.
 \item[(d)]
  We have $A_{\Delta'} \cdot \vu \le \vu$.
 \item[(e)]
  If $A_{\Delta} \cdot \vu = \vu$, then $A_{\Delta'} \cdot \vu = \vu$.
\end{itemize}
This completes the proof of the lemma.
\qed
\end{proof}

\begin{proposition} \label{prop:critical-u-progressive}
 Let $\Delta$ be an almost surely terminating relaxed pBPA with stack alphabet~$\Gamma$.
 Let $\vu \in \Rset_+^\Gamma$ be such that $\umax = 1$ and $A_\Delta \cdot \vu \le \vu$ and $\Delta$ is $\vu$-progressive.
 Let $\pmin$ denote the least rule probability in~$\Delta$.
 Let $C := 17 |\Gamma| / ( \pmin \cdot \umin^2 )$.
 Then for each $k \in \Nset_0$ there is $n_0 \in \Nset$ such that
   \begin{align*}
     \tailprob{\alpha_0}{ n^{2k+2} / (2 |\Gamma|) } \quad & \le\quad C / n
      && \qquad \text{for all $n \ge n_0$ and for all $\alpha_0 \in \Gamma^*$ with $1 \le |\alpha_0| \le n^k$.}
   \end{align*}
\end{proposition}
\begin{proof}
For each $X \in \Gamma$ we define a function $g_X: \Rset \to \Rset$ by setting
 \[
  g_X(\theta) := \sum_{X\btran{p} \alpha} p \cdot \exp(-\theta \cdot (-\vu(X) + \numbsymb{\alpha} \thickdot \vu))\,.
 \]
The following lemma states important properties of~$g_X$.
\begin{lemma} \label{lem:g-properties}
  The following holds for all $X \in \Gamma$:
  \begin{itemize}
    \item[(a)]
     For all $\theta > 0$ we have $1 = g_X(0) < g_X(\theta)$.
    \item[(b)]
     For all $\theta > 0$ we have $0 \le g_X'(0) < g_X'(\theta)$.
    \item[(c)]
     For all $\theta \ge 0$ we have $0 < g_X''(\theta)$. In particular, $g_X''(0) \ge \pmin \cdot \umin^2 / 4$.
  \end{itemize}
\end{lemma}
\begin{proof}[Proof of the lemma]\mbox{}
 \begin{itemize}
  \item[(a)]
   Clearly, $g_X(0) = 1$.
   The inequality $g_X(0) < g_X(\theta)$ follows from (b).
  \item[(b)]
   We have:
   \begin{align*}
    g_X(\theta)
    & = \sum_{X\btran{p} \alpha} p \cdot \exp(-\theta \cdot (-\vu(X) + \numbsymb{\alpha} \thickdot \vu)) \\
    g_X'(\theta)
    & = \sum_{X\btran{p} \alpha} p \cdot (\vu(X) - \numbsymb{\alpha} \thickdot \vu) \cdot \exp(-\theta \cdot (-\vu(X) + \numbsymb{\alpha} \thickdot \vu))
\intertext{Let $A(X)$ denote the $X$-row of~$A$, i.e., the vector $\vv\in \Rset^\Gamma$ such that $\vv(Y) = A(X,Y)$.
   Then $A \cdot \vu \le \vu$ implies
}
g_X'(0)
     & = \sum_{X\btran{p} \alpha} p \cdot (\vu(X) - \numbsymb{\alpha} \thickdot \vu) \\
     & = \vu(X) - \sum_{X\btran{p} \alpha} p \cdot \numbsymb{\alpha} \thickdot \vu = \vu(X) - A(X) \thickdot \vu\\
     & \ge \vu(X) - \vu(X) = 0\,.
\end{align*}
   The inequality $g_X'(0) < g_X'(\theta)$ follows from~(c).
  \item[(c)]
   We have
   \begin{align*}
    g_X''(\theta)
    & = \sum_{X\btran{p} \alpha} p \cdot (\vu(X) - \numbsymb{\alpha} \thickdot \vu)^2 \cdot \exp(-\theta \cdot (-\vu(X) + \numbsymb{\alpha} \thickdot \vu)) > 0\,.
   \end{align*}
   Since $\Delta$ is $\vu$-progressive, there is a rule $X \btran{p} \alpha$ with $|\vu(X) - \numbsymb{\alpha} \thickdot \vu| \ge \umin/2$.
   Hence, for $\theta = 0$ we have $g_X''(0) \ge \pmin \cdot \umin^2 / 4$.
 \end{itemize}
This proves the lemma.
\qed
\end{proof}

Let in the following $\theta > 0$.
Given a run $w\in \run(\alpha_0)$ and $i\geq 0$, we write $\Xs{i}(w)$ for the symbol $X \in \Gamma$ for which $w(i) = X \alpha$.
Define
 \[
  \ms{i}_\theta(w) =
   \begin{cases} \displaystyle
           \exp(-\theta \cdot \numbsymb{w(i)} \thickdot \vu) \cdot \prod_{j=0}^{i-1} \frac{1}{g_{\Xs{j}(w)}(\theta)} & \text{if $i=0$ or $w(i-1) \ne \varepsilon$} \\
           \ms{i-1}_\theta(w)                                                                                  & \text{otherwise} \\
   \end{cases}
 \]

\begin{lemma} \label{lem:exponential-martingale}
$\ms{0}_\theta, \ms{1}_\theta, \ldots$ is a martingale.
\end{lemma}
\begin{proof}[Proof of the lemma]
Let us fix a path $v\in \fpath(\alpha_0)$ of length $i \ge 1$ and let $w$ be an arbitrary run of $\run(v)$.
First assume that $v(i-1)=X\alpha\in \Gamma\Gamma^*$.
Then we have:
\begin{align*}
 & \Ex{\ms{i}_{\theta} \;\middle\vert\; \run(v)} \\
 & = \Ex{\exp(-\theta \cdot \numbsymb{w(i)} \thickdot \vu) \cdot \prod_{j=0}^{i-1} \frac{1}{g_{\Xs{j}(w)}(\theta)} \;\middle\vert\; \run(v)} \\
 & = \sum_{X\btran{p}\alpha} p\cdot \exp\left(-\theta\cdot \left(\numbsymb{w(i-1)} - \vec{1}_X +  \numbsymb{\alpha}\right) \thickdot \vu\right) \cdot \prod_{j=0}^{i-1} \frac{1}{g_{\Xs{j}(w)}(\theta)} \\
 & = \sum_{X\btran{p}\alpha} p\cdot \exp\left(-\theta\cdot \left(\numbsymb{w(i-1)}\thickdot \vu - \vu(X) +  \numbsymb{\alpha}\thickdot \vu\right)\right) \cdot \prod_{j=0}^{i-1} \frac{1}{g_{\Xs{j}(w)}(\theta)} \\
 & = \exp\left(-\theta \cdot \numbsymb{w(i-1)}\thickdot \vu\right)\cdot \sum_{X\btran{p}\alpha} p\cdot \exp\left(-\theta \cdot \left(- \vu(X) +  \numbsymb{\alpha}\thickdot \vu\right)\right) \cdot \prod_{j=0}^{i-1} \frac{1}{g_{\Xs{j}(w)}(\theta)} \\
 & = \exp\left(-\theta \cdot \numbsymb{w(i-1)}\thickdot \vu\right)\cdot g_{\Xs{i-1}(w)}(\theta) \cdot \prod_{j=0}^{i-1} \frac{1}{g_{\Xs{j}(w)}(\theta)} \\
 & = \exp\left(-\theta \cdot \numbsymb{w(i-1)}\thickdot \vu\right)\cdot \prod_{j=0}^{i-2} \frac{1}{g_{\Xs{j}(w)}(\theta)} \\
 & = \ms{i-1}_\theta(w)\,.
\end{align*}
If $v(i-1)=\varepsilon$, then for every $w\in \run(v)$ we have $\ms{i}_\theta(w)=\ms{i-1}_\theta(w)$.
Hence, $\ms{0}_\theta, \ms{1}_\theta, \ldots$ is a martingale.
\qed
\end{proof}

Since $\theta > 0$ and since $g_{\Xs{j}(w)}(\theta) \ge 1$ by Lemma~\ref{lem:g-properties}(a),
 we have $0 \le \ms{i}_\theta(w) \le 1$, so the martingale is bounded.
Since, furthermore, $\mathbf{T}_{\alpha_0}$ (we write only $\termt{}$ in the following) is finite with probability~$1$,
 it follows using Doob's Optional-Stopping Theorem (see Theorem~10.10~(ii) of~\cite{book:Williams})
 that $\ms{0}_\theta = \Ex{\ms{\termt{}}_\theta}$.
Hence we have for each $n \in \Nset$:
\begin{align*}
 & \quad   \exp(-\theta \cdot \umax \cdot n^k) \\
 & \le \exp(-\theta \cdot \vu \thickdot \numbsymb{\alpha_0}) = \ms{0}_\theta \\ & = \Ex{\ms{\termt{}}_\theta} && \text{(by optional-stopping)} \\
 & = \Ex{\exp(-\theta \cdot 0) \cdot \prod_{j=0}^{\termt{}-1} \frac{1}{g_{\Xs{j}}(\theta)}} \\
 & = \Ex{\prod_{j=0}^{\termt{}-1} \frac{1}{g_{\Xs{j}}(\theta)}} \\
 & \le \Ex{\frac{1}{g_X(\theta)^\termt{}}} && \text{(for some $X \in \Gamma$)} \\
 & = \sum_{i=0}^\infty \calP(\termt{} = i) \cdot \frac{1}{g_X(\theta)^i} \\
 & \le \sum_{i=0}^{\left\lceil n^{2k+2} / (2 |\Gamma|) \right\rceil-1} \calP(\termt{} = i) \cdot 1  && \text{(Lemma~\ref{lem:g-properties}~(a))} \\
 & \quad + \sum_{i=\left\lceil n^{2k+2} / (2 |\Gamma|) \right\rceil}^\infty \calP(\termt{} = i) \cdot \frac{1}{g_X(\theta)^{ n^{2k+2} / (2 |\Gamma|) }} \\
 & = 1 - \calP(\termt{} \ge n^{2k+2}/(2 |\Gamma|)) \\
 & \quad \mbox{} + \calP(\termt{} \ge n^{2k+2}/(2 |\Gamma|)) \cdot \frac{1}{g_X(\theta)^{n^{2k+2}/(2 |\Gamma|)}}
\end{align*}
Rearranging the inequality, we obtain
\begin{equation}
 \calP(\termt{} \ge n^{2k+2}/(2 |\Gamma|)) \le \frac{1 - \exp(-\theta \cdot \umax \cdot n^{k})}{1 - g_X(\theta)^{-n^{2k+2}/(2 |\Gamma|)}} \;.
  \label{eq:before-hopital}
\end{equation}
For the following we set $\theta = n^{-(k+1)}$.
We want to give an upper bound for the right hand side of~\eqref{eq:before-hopital}.
To this end we will show:
 \begin{equation}
  \lim_{n\to\infty} \frac{\left(1 - \exp(-n^{-(k+1)} \cdot \umax \cdot n^{k})\right) \cdot n}{1 - g_X(n^{-(k+1)})^{-n^{2(k+1)}/ (2 |\Gamma|)}}
\le \frac{1}{1 - \exp \left( - \pmin \cdot \umin^2 / (16 |\Gamma|) \right) }
  \,. \label{eq:quotient}
 \end{equation}
Combining \eqref{eq:before-hopital} with \eqref{eq:quotient}, we obtain
 \begin{align*}
  \limsup_{n \to \infty} \ n \cdot \calP(\termt{} \ge n^{2k+2}/(2 |\Gamma|))
   & \le \frac{1}{1 - \exp \left( - \pmin \cdot \umin^2 / (16 |\Gamma|) \right) } \\
   & < \frac{1}{1 - \left(1 - \frac{16}{17} \cdot \left( \pmin \cdot \umin^2 / (16 |\Gamma|) \right) \right) } \\
   & = 17 |\Gamma| / ( \pmin \cdot \umin^2 )
   \,,
 \end{align*}
which implies the proposition.

To prove~\eqref{eq:quotient}, we compute limits for the nominator and the denominator separately.
For the nominator, we use l'Hopital's rule to obtain:
 \begin{align*}
  \lim_{n\to\infty} \frac{1 - \exp(-\umax \cdot n^{-1})}{n^{-1}}
   & = \lim_{n\to\infty} \frac{-\umax \cdot n^{-2} \cdot \exp(-\umax \cdot n^{-1})}{-n^{-2}} = \umax = 1\,.
 \end{align*}
For the denominator of~\eqref{eq:quotient} we consider first the following limit:
 \begin{align*}
  & \lim_{n\to\infty} \frac1{2 |\Gamma|} \cdot n^{2(k+1)} \cdot \ln g_X(n^{-(k+1)}) \\
  & = \frac1{2 |\Gamma|} \lim_{n\to\infty} \frac{\ln g_X(n^{-(k+1)})}{n^{-2(k+1)}} \\
  & = \frac1{2 |\Gamma|} \lim_{n\to\infty} \frac{g_X'(n^{-(k+1)}) \cdot \left(-(k+1)\right) \cdot n^{-k-2}}{g_X(n^{-(k+1)}) \cdot \left( -2(k+1) \right) \cdot n^{-2k-3}}
       && \text{(l'Hopital's rule)} \\
  & = \frac1{4 |\Gamma|} \lim_{n\to\infty} \frac{g_X'(n^{-(k+1)})}{n^{-(k+1)}}
       && \text{(by Lemma~\ref{lem:g-properties}~(a))\,.} \\
\intertext{If $g_X'(0) > 0$, then the limit is $+\infty$. Otherwise, by Lemma~\ref{lem:g-properties}~(b), we have $g_X'(0) = 0$ and hence}
  & = \frac1{4 |\Gamma|} \lim_{n\to\infty} \frac{g_X''(n^{-(k+1)}) \cdot \left(-(k+1)\right) \cdot n^{-k-2}}{\left(-(k+1)\right) \cdot n^{-k-2}}
       && \text{(l'Hopital's rule)} \\
  & = \frac1{4 |\Gamma|} g_X''(0) \ge \pmin \cdot \umin^2 / (16 |\Gamma|)  && \text{(by Lemma~\ref{lem:g-properties}~(c))\,.}
 \end{align*}
This proves~\eqref{eq:quotient}
 and thus completes the proof of Proposition~\ref{prop:critical-u-progressive}.
\qed
\end{proof}

The following lemma serves as induction base for the proof of Proposition~\ref{prop:term-upper-critical}.
\begin{lemma} \label{lem:critical-strongly-connected}
  Let $\Delta$ be an almost surely terminating pBPA with stack alphabet~$\Gamma$.
  Assume that all SCCs of~$\Delta$ are bottom SCCs.
  Let $\pmin$ denote the least rule probability in~$\Delta$.
  Let $D := 17 |\Gamma| / \pmin^{3|\Gamma|}$.
Then for each $k \in \Nset_0$ there is $n_0 \in \Nset$ such that
   \begin{align*}
     \tailprob{\alpha_0}{ n^{2k+2} / 2 } \quad & \le\quad D / n
      && \quad \text{for all $n \ge n_0$ and for all $\alpha_0 \in \Gamma^*$ with $1 \le |\alpha_0| \le n^k$.}
   \end{align*}
\end{lemma}
\begin{proof}
Decompose $\Gamma$ into its SCCs, say $\Gamma = \Gamma_1 \cup \cdots \cup \Gamma_s$,
 and let the pBPA~$\Delta_i$ be obtained by restricting~$\Delta$ to the $\Gamma_i$-symbols.
For each $i \in \{1, \ldots, s\}$, Lemma~\ref{lem:cone-vector} gives a vector $\vu_i \in \Rset_+^{\Gamma_i}$.
W.l.o.g.\ we can assume for each~$i$ that the largest component of~$\vu_i$ is equal to~$1$,
 because $\vu_i$ can be multiplied with any positive scalar without changing the properties guaranteed by Lemma~\ref{lem:cone-vector}.
If the vectors~$\vu_i$ are assembled (in the obvious way) to the vector $\vu \in \Rset_+^\Gamma$,
 the assertions of Lemma~\ref{lem:cone-vector} carry over;
 i.e., we have $A_\Delta \cdot \vu \le \vu$ and $\umax = 1$ and $\umin \ge \pmin^{|\Gamma|}$.
Let $\Delta'$ be the $\vu$-progressive relaxed pBPA from Lemma~\ref{lem:progressive},
 and denote by~$\calP'$ and $\pmin'$ its associated probability measure and least rule probability, respectively.
Then we have:
\begin{align*}
 \tailprob{\alpha_0}{ n^{2k+2} / 2 }
 & \le \calP'( \termt{\alpha_0} \ge n^{2k+2} / (2 |\Gamma|) )   && \text{(by Lemma~\ref{lem:progressive})} \\
 & \le 17 |\Gamma| / ( \pmin' \cdot \umin^2 \cdot n )           && \text{(by Proposition~\ref{prop:critical-u-progressive})} \\
 & \le 17 |\Gamma| / ( \pmin' \cdot \pmin^{2|\Gamma|} \cdot n ) && \text{(as argued above)} \\
 & \le 17 |\Gamma| / ( \pmin^{3|\Gamma|} \cdot n )              && \text{(by Lemma~\ref{lem:progressive})\,.}
\end{align*}
\qed
\end{proof}

\newcommand{\GammaH}{\Gamma_{\it high}}
\newcommand{\GammaL}{\Gamma_{\it low}}
\newcommand{\DeltaH}{\Delta_{\it high}}
\newcommand{\DeltaL}{\Delta_{\it low}}
Now we are ready to prove Proposition~\ref{prop:term-upper-critical}, which is restated here.
\begin{qproposition}{\ref{prop:term-upper-critical}}
 \stmtproptermuppercritical
\end{qproposition}
\begin{proof}
 Let $D$ be the $D$ from Lemma~\ref{lem:critical-strongly-connected}.
 We show by induction on~$h$:
 \begin{equation}
   \tailprob{X_0}{n^{2^{h+1}-2}} \le \frac{h D}{n} \quad \text{for almost all $n\in\Nset$.} \label{eq:ct-induction}
 \end{equation}
 Note that \eqref{eq:ct-induction} implies the proposition.
 The case $h=1$ (induction base) is implied by Lemma~\ref{lem:critical-strongly-connected}.
 Let $h \ge 2$.
 Partition $\Gamma$ into $\GammaH \cup \GammaL$ such that $\GammaL$
  contains the variables of the SCCs of depth~$h$ in the DAG of SCCs,
  and $\GammaH$ contains the other variables (in ``higher'' SCCs).
 If $X_0 \in \GammaL$, then we can restrict~$\Delta$ to the variables that are in the same SCC as~$X_0$,
  and Lemma~\ref{lem:critical-strongly-connected} implies~\eqref{eq:ct-induction}.
 So we can assume $X_0 \in \GammaH$.

 Assume for a moment that $\tailprob{X_0}{n^{2^{h+1}-2}}$ holds for a run~$w \in \run(X_0)$;
  i.e., we have:
 \begin{align*}
  n^{2^{h+1}-2} \quad & \le \quad |\{ i \in \Nset_0 \mid w(i) \in \Gamma\Gamma^*\}| \\
   & = \quad |\{ i \in \Nset_0 \mid w(i) \in \GammaH\Gamma^*\}| + |\{ i \in \Nset_0 \mid w(i) \in \GammaL\Gamma^*\}|\,.
 \end{align*}
 It follows that one of the following events is true for~$w$:
 \begin{itemize}
  \item[(a)]
   At least $n^{2^h-2}$ steps in~$w$ have a $\GammaH$-symbol on top of the stack.
   More formally,
    \[
     |\{ i \in \Nset_0 \mid w(i) \in \GammaH\Gamma^*\}| \ge n^{2^h-2}\,.
    \]
  \item[(b)]
   Event~(a) is not true, but at least $n^{2^{h+1}-2} - n^{2^{h}-2}$ steps in~$w$ have a $\GammaL$-symbol on top of the stack.
   More formally,
    \begin{align*}
     |\{ i \in \Nset_0 \mid w(i) \in \GammaH\Gamma^*\}| & < n^{2^h-2} \quad \text{and} \\
     |\{ i \in \Nset_0 \mid w(i) \in \GammaL\Gamma^*\}| & \ge n^{2^{h+1}-2} - n^{2^{h}-2}\,.
    \end{align*}
 \end{itemize}
 In order to give bounds on the probabilities of events (a) and~(b),
  it is convenient to ``reshuffle'' the execution order of runs in the following way:
 Whenever a rule $X \btran{} \alpha$ is executed, we do not replace the $X$-symbol on top of the stack by~$\alpha$,
  but instead we push only the $\GammaH$-symbols in~$\alpha$ on top of the stack, whereas the $\GammaL$-symbols in~$\alpha$
  are added to the {\em bottom} of the stack.
 Since $\Delta$ is a pBPA and thus does not have control states,
  the reshuffling of the execution order does not influence the distribution of the termination time.
 The advantage of this execution order is that each run can be decomposed into two phases:
 \begin{itemize}
  \item[(1)]
   In the first phase, the symbol on the top of the stack is always a $\GammaH$-symbol.
   When rules are executed, $\GammaL$-symbols may be produced, which are added to the bottom of the stack.
  \item[(2)]
   In the second phase, the stack consists of $\GammaL$-symbols exclusively.
   Notice that by definition of $\GammaL$, no new $\GammaH$-symbols can be produced.
 \end{itemize}
 In terms of those phases, the above events (a) and~(b) can be reformulated as follows:
 \begin{itemize}
  \item[(a)]
   The first phase of~$w$ consists of at least $n^{2^h-2}$ steps.
   The probability of this event is equal to
    \[
     \calP_{\DeltaH}(\termt{X_0} \ge n^{2^h-2})\,,
    \]
   where $\DeltaH$ is the pBPA obtained from~$\Delta$ by deleting all $\GammaL$-symbols
   from the right hand sides of the rules and deleting all rules with $\GammaL$-symbols on the left hand side,
   and $\calP_{\DeltaH}$ is its associated probability measure.
  \item[(b)]
   The first phase of~$w$ consists of fewer than $n^{2^h-2}$ steps
    (which implies that at most $n^{2^h-2}$ $\GammaL$-symbols are produced during the first phase),
    and the second phase consists of at least $n^{2^{h+1}-2} - n^{2^{h}-2}$ steps.
   Therefore, the probability of the event~(b) is at most
    \[
     \max \left\{ \calP_{\DeltaL}(\termt{\alpha_0} \ge n^{2^{h+1}-2} - n^{2^h-2})
      \;\middle\vert\; \alpha_0 \in \GammaL^*,\; 1 \le |\alpha_0| \le n^{2^h-2} \right\}\,,
    \]
    where $\DeltaL$ is the pBPA~$\Delta$ restricted to the $\GammaL$-symbols,
     and $\calP_{\DeltaL}$ is its associated probability measure.
   Notice that $n^{2^{h+1}-2} - n^{2^h-2} \ge n^{2^{h+1}-2} / 2$ for large enough~$n$.
   Furthermore, by the definition of~$\GammaL$, the SCCs of~$\DeltaL$ are all bottom SCCs.
   Hence, by Lemma~\ref{lem:critical-strongly-connected}, the above maximum is at most~$D/n$.
 \end{itemize}
 Summing up, we have for almost all $n \in \Nset$:
 \begin{align*}
  \tailprob{X_0}{n^{2^{h+1}-2}}
  & \le \calP(\text{event~(a)}) + \calP(\text{event~(b)}) \\
  & \le \calP_{\DeltaH}(\termt{X_0} \ge n^{2^h-2})   +   D/n     && \text{(as argued above)} \\
  & \le \frac{(h-1)D}{n} + \frac{D}{n} = \frac{hD}{n}           && \text{(by the induction hypothesis).}
 \end{align*}
 This completes the induction proof.
\qed
\end{proof}
