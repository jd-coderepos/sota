\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{xcolor}
\usepackage{color, colortbl}
\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}

\usepackage{tabularx}
\usepackage{enumerate}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{float}

\usepackage[hidelinks,pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,letterpaper=true,linkcolor=linkcolor]{hyperref}

\makeatletter
\newcommand\figcaption{\def\@captype{figure}\caption}
\newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\iccvfinalcopy 

\def\iccvPaperID{1379} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{PointCLIP V2: Prompting CLIP and GPT for Powerful\\3D Open-world Learning}

\author{Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin\\\vspace{0.2cm} Shanghang Zhang, Peng Gao\\
\normalsize{ Equal contribution}\quad   Project leader\quad   Corresponding author\vspace{0.3cm}\\
  City University of Hong Kong \quad \vspace{0.07cm}
  The Chinese University of Hong Kong\\
  Shanghai Artificial Intelligence Laboratory \quad
  Peking University\quad
  Yale University\vspace{0.2cm}\\
\texttt{\{xiangyzhu6-c, boweihe2-c\}@my.cityu.edu.hk},\\
\texttt{\{zhangrenrui, gaopeng\}@pjlab.org.cn},\quad \texttt{shanghang@pku.edu.cn}
}

\maketitle

\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Large-scale pre-trained models have shown promising open-world performance for both vision and language tasks. However, their transferred capacity on 3D point clouds is still limited and only constrained to the classification task. In this paper, we first collaborate \textbf{CLIP} and \textbf{GPT} to be a unified 3D open-world learner, named as \textbf{PointCLIP V2}, which fully unleashes their potential for zero-shot 3D classification, segmentation, and detection. To better align 3D data with the pre-trained language knowledge, PointCLIP V2 contains two key designs.
For the visual end, we prompt CLIP via a shape projection module to generate more realistic depth maps, narrowing the domain gap between projected point clouds with natural images. For the textual end, we prompt the GPT model to generate 3D-specific text as the input of CLIP's textual encoder. 
Without any training in 3D domains, our approach significantly surpasses PointCLIP by \textbf{+42.90\%}, \textbf{+40.44\%}, and \textbf{+28.75\%} accuracy on three datasets for zero-shot 3D classification.
On top of that, V2 can be extended to few-shot 3D classification, zero-shot 3D part segmentation, and 3D object detection in a simple manner, demonstrating our generalization ability for unified 3D open-world learning. 
Code is available at \url{https://github.com/yangyangyang127/PointCLIP_V2}.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The advancement of spatial sensors has stimulated widespread attention in recent years for both academia and industry. To effectively understand point clouds, the major data form in 3D, many related tasks are put forward and gained great progress, including 3D classification~\cite{qi2016pointnet, wu2019pointconv, zhang2023parameter}, segmentation~\cite{qi2017pointnet++, Xiang2021Walk, wang2019dynamic,wu2022eda}, detection~\cite{xu2020squeezesegv3, meng2021towards}, and self-supervised learning~\cite{zhang2022learning,guo2023joint,zhang2022point,fu2022pos}. Importantly, for the complexity and diversity of open-world circumstances, the collected 3D data normally contains a large number of `unseen' objects, namely, not ever defined and trained by the already deployed 3D systems. Given the human-laboring data annotations, how to recognize such 3D shapes of new categories has become a hot-spot issue, which still remains to be fully explored.

\begin{figure}[t!]
\centering
\includegraphics[width=0.45\textwidth]{figs/fig1.pdf}
\vspace{0.1cm}
\caption{\textbf{Zero-shot Performance of PointCLIP V2.} On different 3D datasets, our approach achieves significant accuracy enhancement for zero-shot 3D classification over PointCLIP~\cite{zhang2022pointclip}.}
\label{fig1}
\vspace{-0.2cm}
\end{figure}

Recently, large-scale pre-trained vision and language models, \eg, CLIP~\cite{radford2021learning} and GPT-3~\cite{brown2020language}, have obtained a strong capacity to process data in both modalities. However, limited efforts have focused on their application in the point cloud, and existing work only explores the possibility of CLIP on the 3D classification task, without considering other 3D open-world tasks. PointCLIP~\cite{zhang2022pointclip}, for the first time, indicates that CLIP can be adapted for zero-shot point cloud classification without any 3D training. It projects the `unseen' 3D point cloud sparsely into 2D depth maps, and leverages CLIP's image-text alignment for depth map recognition.  
However, as a preliminary work, the performance of PointCLIP is far from satisfactory as shown in Figure~\ref{fig1}, which cannot be put into actual use. More importantly, PointCLIP only draws support from pre-trained CLIP, without considering the powerful large-scale language model (LLM).
Therefore, we ask the question: \textit{Can we properly unify CLIP and LLM to fully unleash their potentials for unified 3D open-world understanding?}

We observe that PointCLIP mainly suffers from two factors concerning the 2D-3D domain gap. \textbf{(1) Sparse Projection.} PointCLIP simply projects 3D point clouds onto depth maps as sparsely distributed points with depth values (Figure~\ref{fig2}). Though simple, the scatter-style figures are dramatically different from the real-world pre-training images for both appearances and semantics, which severely confuses CLIP's visual encoder. \textbf{(2) Naive Text.} PointCLIP mostly inherits CLIP's 2D text input, ``\texttt{a photo of a {[CLASS]}.}'' and only appends simple 3D-related words, ``\texttt{a depth map}''. As visualized in Figure~\ref{fig3}, the textual features extracted by CLIP can hardly focus on the target object with high similarity scores. Such naive text cannot fully describe 3D shapes and harms the pre-trained language-image alignment. 

In this paper, we integrate the advantage of CLIP and the GPT-3~\cite{brown2020language} model and propose \textbf{PointCLIP V2}, a powerful framework for unified 3D open-world understanding, including zero-shot/few-shot 3D classification, zero-shot part segmentation, and zero-shot 3D object detection.
Without `seeing' any 3D training data, V2 can project point clouds into realistic 2D figures and align them with 3D-aware text, which fully unleashes CLIP's pre-trained knowledge in the 3D domain.

\begin{figure}[t!]
\centering
\includegraphics[width=0.48\textwidth]{figs/fig2.pdf}
\caption{\textbf{Comparison of Visual Projection.} PointCLIP V2 (Bottom) generates more realistic depth maps with denser point distribution and smoother depth values.}
\label{fig2}
\vspace{-0.31cm}
\end{figure}

Firstly, we propose to \textbf{Prompt CLIP with Realistic Projection}, which generates CLIP-preferred images from 3D point clouds. Specifically, we transform the irregular point cloud into grid-based voxels and then apply non-parametric 3D local filtering on top. By this, the projected 3D shapes are composed of denser points with smoother depth values. As shown in Figure~\ref{fig2}, our generated figures are more visually similar to real-world images and can highly unleash the representation capacity of CLIP's pre-trained visual encoder.
Secondly, we \textbf{Prompt GPT with 3D Command} to generate text with rich 3D semantics as the input of CLIP's textual encoder. By feeding heuristic 3D-oriented command into GPT-3, \eg, ``\texttt{Give a caption of a table depth map:}'', we leverage its language-generative knowledge to obtain a series of 3D-specific text, \eg, ``\texttt{A height map of a table with a top and several legs.}''. A group of language commands is customized to prompt GPT-3 to produce diverse text with 3D shape information.
As shown in Figure~\ref{fig3}, the textual features of PointCLIP V2 exert stronger matching properties to the projected maps, largely boosting CLIP's image-text alignment for 3D point clouds.

\begin{figure}[t!]
\centering
\includegraphics[width=0.48\textwidth]{figs/fig3.pdf}
\caption{\textbf{Comparison of Textual Input.} We visualize the similarity score maps of the encoded textual and visual features, where PointCLIP V2 (Bottom) shows better alignment.}
\vspace{-0.3cm}
\label{fig3}
\end{figure}

With our prompting schemes, PointCLIP V2 exhibits superior performance for zero-shot 3D classification, surpassing PointCLIP by , , and  accuracy, respectively on ModelNet10~\cite{wu20153d}, ModelNet40~\cite{wu20153d}, and ScanObjectNN~\cite{uy2019revisiting} datasets. Further, our approach can be adapted for more no-trivial 3D open-world tasks by marginal modifications, such as a learnable 3D smoothing for 3D few-shot classification, a back-projection head for zero-shot segmentation, and a 3D region proposal network for zero-shot detection. This fully indicates the power of V2 for general 3D open-world understanding.

Our contributions are summarized as follows:

\begin{itemize}
    \item We propose PointCLIP V2, a powerful cross-modal learner unifying CLIP and GPT-3 to transfer the pre-trained vision-language knowledge into 3D domains. 
    
    \item We introduce a realistic projection to prompt CLIP and 3D-oriented command to prompt GPT-3 to effectively mitigate the domain gap among 2D, 3D, and language.
    
    \item As the first work for unified 3D open-world learning, our PointCLIP V2 can be further extended for zero-shot part segmentation and 3D object detection.
\end{itemize}


\section{Related Works}
\label{sec:related work}

\paragraph{3D Open-world Learning.}
Traditional methods for 3D open-world learning still require 3D training data as a pre-training stage. The series of work of Cheraghian \etal train zero-shot classifiers on `seen' 3D categories by maximizing inter-class divergence in latent space, and then test on `unseen' ones \cite{cheraghian2019mitigating, cheraghian2019zero, cheraghian2022zero}. Some recent works \cite{michele2021generative,liu2021segmenting,naeem20223d, Lu2022Open} also investigate open-world semantic segmentation and 3D object detection for more complex 3D scenes.
Inspired by CLIP-based adaption methods~\cite{zhang2021tip,gao2021clip,lin2022frozen}, PointCLIP \cite{zhang2022pointclip} achieves zero-shot point cloud recognition without any training on 3D datasets. By transferring the pre-trained CLIP model~\cite{radford2021learning}, the 2D knowledge can be effectively utilized for recognizing 3D data. CLIP2Point \cite{Huang2022CLIP} further improves the adaption performance of CLIP on point clouds by an additional 3D pre-training.
In this paper, we propose PointCLIP V2 and follow the open-world setting of PointCLIP, which is more challenging than previous methods as compared in Figure~\ref{fig:scheme_comparison}.
We require no `seen' 3D training and, for the first time, simultaneously conduct zero-shot 3D part segmentation and object detection, achieving unified 3D open-world understanding.

\begin{figure}[t!]
\centering
\includegraphics[width=0.462\textwidth]{figs/comparison_existing_frame.pdf}
\vspace{0.07cm}
\caption{\textbf{Comparison of Open-world Settings.}
Existing methods still depend on prerequisite 3D training to recognize the `unseen' point clouds. In contrast, we require no training in the 3D domain and directly conduct 3D open-world understanding.}
\label{fig:scheme_comparison}
\vspace{-0.22cm}
\end{figure}

\paragraph{Projection for Point Clouds.}
Concurrent to point-based methods~\cite{qi2016pointnet, qi2017pointnet++,  ma2022rethinking}, projection-based point cloud analysis aims to utilize plentiful 2D networks for 3D domains by projecting point clouds into 2D images \cite{su2015multi, Shi2015DeepPano, yang2019learning, wang2019dominant, wei2020view, ahmed2019epn}. 
Therein, PointCLIP~\cite{zhang2022pointclip} follows SimpleView~\cite{goyal2021revisiting} to conduct perspective transformation as 3D-to-2D projection, which achieves high efficiency but limited classification accuracy. 
Under 3D open-world settings, we are motivated to develop more efficient and realistic projection methods for prompting CLIP on point cloud data. In Table~\ref{table:ablation_projection}, we compare our approach with existing advanced projection methods for latency and accuracy. For a fair comparison, we implement all prior works under the pipeline of our V2, namely, with our GPT prompting approach to fully reveal their effectiveness. As shown, our realistic projection exhibits faster inference speed than other approaches and attains higher zero-shot performance than PointCLIP.

\paragraph{Prompt Learning in Vision.}
Prompt engineering first derives from natural language processing, where a textual template, termed as prompt, is generated to narrow the domain gap between the pre-training pre-text task and downstream scenarios~\cite{liu2021pre,Jiang2020How,wallace2019universal,Jiang2020How}. Inspired by this, CoOp~\cite{zhou2022coop} firstly introduces learnable prompting into 2D vision-language classification, and the follow-up CoCoOp~\cite{zhou2022cocoop} extends it for 2D domain generalization. CuPL~\cite{pratt2022Whatdoes} and CaFo~\cite{zhang2023prompt} leverage GPT-3~\cite{brown2020language} to enhance the downstream performance of CLIP on various 2D datasets. From another perspective, visual prompting methods propose to append input images with learnable visual pixels~\cite{jia2022visual, bahng2022visual, chen2022visual, gan2023decorate} or embeddings~\cite{jia2022visual,guo2023viewrefer,zhang2023personalize}, and improve pre-trained vision backbones without downstream fine-tuning. In this paper, we seek to prompt both CLIP's visual encoder by realistic projection and textual encoder by GPT-3 to improve its zero-shot prediction. 

\paragraph{GPT-3 Model.} The Generative Pre-trained Transformer (GPT) models~\cite{radford2018improving, radford2019language, brown2020language} have achieved a progressive improvement in processing natural languages. Among them, GPT-3 demonstrates a remarkable proficiency in both language comprehension and generation, compared to its predecessors~\cite{radford2018improving, radford2019language, liu2019roberta, yang2019xlnet, raffel2020exploring}. GPT-3 is a large-scale autoregressive language model of 175 billion trainable parameters. Although not open-sourced, some efforts have explored its application to downstream tasks, such as PICa~\cite{yang2022empirical} for visual question-answering, CuPL~\cite{pratt2022Whatdoes} for 2D zero-shot recognition, and CaFo~\cite{zhang2023prompt} for 2D few-shot learning. In this work, we, for the first time, prompt GPT-3~\cite{brown2020language} to boost open-world 3D tasks via 3D-related command.

\begin{table}[t!]
\centering
\begin{adjustbox}{width=0.9\linewidth}
	\begin{tabular}{lccc}
	\toprule
		\makecell*[c]{Method} & Latency & ModelNet40 & ScanObjectNN \\ \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-4}
		Phong Shading~\cite{su2015multi} &  &  & \\
		Height Map~\cite{su2018adeeper} &  &  & \\
		Silhouette Map~\cite{su2018adeeper} &  &  &  \\
		PointCLIP~\cite{zhang2022pointclip} &  &  & \\
		\textbf{PointCLIP V2} &  &  & \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.23cm}
\caption{\textbf{Comparison of Different Projection Methods.}
We report zero-shot classification results (\%) on two datasets~\cite{wu20153d, uy2019revisiting}, and compare the inference latency (ms) by projecting 10-view images from an input point cloud.}
\label{table:ablation_projection}
\vspace{-0.2cm}
\end{table}


\section{Methods}
\label{sec:methods}
The overall framework of PointCLIP V2 is shown in Figure \ref{fig:framework2}. Inheriting from CLIP~\cite{radford2021learning}, our framework consists of two pre-trained visual and textual encoders. To bridge the modal gap, we introduce a realistic projection (Sec.~\ref{method:shape_projection}) from 3D to depth maps, and GPT-generated 3D-specific text (Sec.~\ref{method:3D_prompt}) to align depth maps with languages. PointCLIP V2 can also be extended to various 3D tasks for unified 3D open-world learning (Sec.~\ref{s3.4}).


\subsection{Prompting CLIP with Realistic Projection}
\label{method:shape_projection}

To generate more realistic 2D input from 3D data for CLIP and also achieve time efficiency, we project 3D point clouds into depth maps by four steps: Quantize, Densify, Smooth, and Squeeze, as shown in Figure \ref{fig:shape_projection}.

\begin{figure}[t!]
\centering
\includegraphics[width=0.399\textwidth]{figs/shape_projection.pdf}
\vspace{0.19cm}
\caption{\textbf{Prompting CLIP with Realistic Projection.} We present the projection pipeline for one of the views. The switch selects zero- or few-shot classification with learnable smoothing.}
\label{fig:shape_projection}
\vspace{-0.1cm}
\end{figure}

\vspace{-6pt}
\paragraph{Quantize.} 
For different  views to be projected, we respectively create a zero-initialized 3D grid , where  denote its spatial resolutions and  specially represents the depth dimension vertical to the view plane.
Taking one view as an example, we normalize the 3D coordinates of the input point cloud into  and project a point  into a voxel in the grid by

where the voxels are assigned with different depth values, and  denotes a scale factor to adjust the projected shape size. For multiple points projected into the same voxel, we simply assign the minimum depth value. This is because, from the perspective of the target image plane, the points with a smaller depth value  would occlude the larger ones.
Then, we obtain a 3D grid  containing sparse depth values, most voxels of which are empty due to the sparsity of point clouds.

\vspace{-6pt}
\paragraph{Densify.} 
To tackle such unreal scattering, we densify the grid via a local mini-value pooling operation to guarantee visual continuity. We reassign every voxel in  by the minimum voxel value within a local spatial window. Likewise, compared to the average and max pooling, preserving the minimum depth values accords with the occluded visual appearances on the projected maps. In this way, the originally vacant voxels between the sparse points can be effectively filled with reasonable depth values, while the background voxels still remain empty, which derives dense and solid spatial shape representations.

\label{method:tasks}
\begin{figure}[t!]
\centering
\includegraphics[width=0.409\textwidth]{figs/shape_prompt.pdf}
\vspace{0.15cm}
\caption{\textbf{Prompting GPT with 3D Command.} We feed four types of language command into the pre-trained GPT-3, which generates a series of 3D-specific text for CLIP's textual encoder.}
\label{fig:3d_command}
\vspace{-0.25cm}
\end{figure}

\vspace{-6pt}
\paragraph{Smooth.} 
As the local pooling operation might introduce artifacts on some 3D surfaces, we adopt a non-parametric Gaussian kernel for shape smoothing and noise filtering. With a proper kernel size and variance, the filtering can not only remove the spatial noises caused by densification but also preserve the sharpness of edges and corners in the original 3D shapes. By this, we acquire a more compact and smooth shape represented by the 3D grid.

\begin{figure*}[t!]
\centering
\includegraphics[width=0.99\textwidth]{figs/whole_frame.pdf}
\vspace{0.1cm}
\caption{\textbf{The Unified Framework of PointCLIP V2 for 3D Open-world Learning.}
We first generate high-quality depth maps via a realistic projection to prompt CLIP's~\cite{radford2021learning} visual encoder. Then, we design 3D language command to prompt GPT-3~\cite{brown2020language} for 3D-specific text into CLIP's textual encoder. V2 can also be extended to 3D segmentation and detection by simple modifications.}
\label{fig:framework2}
\vspace{-0.1cm}
\end{figure*}

\vspace{-6pt}
\paragraph{Squeeze.} 
As the final step, we simply squeeze the depth dimension of  to acquire the projected depth map . We extract the minima of every depth channel as the value for each pixel location and repeat it for three times as the RGB intensity. Our grid-based projection can be simply achieved by a minimum pooling along the depth channel of , more friendly for hardware implementation.

\subsection{Prompting GPT with 3D Command}
\label{method:3D_prompt}

To better activate CLIP's textual encoder to align with our depth maps, we aim to utilize 3D-specific description with category-wise shape characteristics as the textual input of CLIP, instead of using the general ``\texttt{a photo of a [CLASS]:}''. Considering the powerful descriptive capacity of LLMs, we leverage GPT-3~\cite{brown2020language} to generate 3D-specific text with sufficient 3D semantics for CLIP's textual encoder as shown in Figure~\ref{fig:3d_command}. Normally, GPT-3 receives a language command and outputs a response via pre-trained knowledge. To fully adapt GPT-3 to 3D domains, we propose the following four series of heuristic command:

\vspace{-6pt}
\paragraph{Caption Generation.} 
Given a descriptive command, GPT-3 synthesizes general captions for the target projected 3D shape, \eg, Input: ``\texttt{Describe a depth map of a {[window]}:}''; GPT-3: ``\texttt{It depicts the {[window]} as a dark pane.}''.

\vspace{-6pt}
\paragraph{Question Answering.} 
GPT-3 produces descriptive answers to the 3D-related question, \eg, Input: ``\texttt{How to describe a depth map of a [table]}?''; GPT-3: ``\texttt{The {[table]} may have a rectangular or circular flat top and legs.}''.

\vspace{-6pt}
\paragraph{Paraphrase Generation.} 
For a depth map description, GPT-3 is expected to generate a synonymous sentence. \eg, Input: ``\texttt{Generate a synonym for the sentence: A grayscale depth map of an inclined {[bed]}.}''; GPT-3: ``\texttt{An monochrome depth map of an oblique {[bed]}.}''.

\vspace{-6pt}
\paragraph{Words to Sentence.} 
Based on a group of keywords, GPT-3 is requested to organize them into a complete sentence and enrich additional shape-related contents, \eg, Input: ``\texttt{Make a sentence using these words: a {[table]}, depth map, smooth.}''; GPT-3: ``\texttt{This smooth depth map shows a {[table]} at the corner.}''. The adjective ``\texttt{smooth}'' here depicts the natural appearance caused by the smoothing operation.

\vspace{0.45cm}
For a -category 3D dataset, we place  category names at the ``\texttt{{[CLASS]}}'' position of each command and feed them into GPT-3, which generates 3D-specific descriptions with rich category-wise semantics. Finally, we integrate the descriptions of each category and regard them as the input for CLIP's textual encoder. 


\begin{table*}[ht!]
\centering
\begin{adjustbox}{width=1.0\linewidth}
	\begin{tabular}{lccccccc}
	\toprule
		Method & 2D Pre-train & 3D Pre-train &ModelNet10 & ModelNet40 &S-OBJ\_ONLY &S-OBJ\_BG &S-PB\_T50\_RS \\
		\cmidrule(lr){1-1}
		\cmidrule(lr){2-3} 
		\cmidrule(lr){4-8} 
	    
	    CLIP2Point~\cite{Huang2022CLIP} &\checkmark &\checkmark &  &  &  & &  \\
	    Cheraghian~\cite{cheraghian2022zero} & - & \checkmark & & - &- &- &-  \\
     \cmidrule(lr){1-8}
     PointCLIP \cite{zhang2022pointclip}\vspace{0.05cm} &\checkmark & - & &   & & &  \\
	    \textbf{PointCLIP V2}\vspace{0.1cm} &\checkmark & - & &  & & &\\
     \textit{Improvement} &&&\textcolor{blue}{}&\textcolor{blue}{}&\textcolor{blue}{}&\textcolor{blue}{}&\textcolor{blue}{}\\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.1cm}
\caption{\textbf{Zero-shot 3D Classification (\%) ModelNet10~\cite{wu20153d}, ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting}}. We report the performance of other methods with their \textbf{\textit{best-performing settings, \eg, visual encoder, projected view number, and textual input}}. ``2D Pre-train'' denotes the pre-training of CLIP on image-language pairs, and ``3D Pre-train'' denotes the training on 3D datasets.}
\label{table:zero_shot_classification}
\vspace{-0.1cm}
\end{table*}

\subsection{Unified Open-world Learning}
\label{s3.4}

By introducing the realistic projection and 3D-specific text, PointCLIP V2 exhibits strong generalization capacity and can be adapted for different 3D open-world tasks.

\paragraph{3D Zero-shot Classification.}
For all  views in the visual branch, we feed the projected depth maps  into CLIP's visual encoder and obtain the multi-view features , where .
For the textual branch, we leverage CLIP's textual encoder to extract the category feature , which serves as the zero-shot classification weights.
Then, the final zero-shot classification  are calculated by aggregating the multi-view alignment between  and , formulated as

where  denotes a hyper-parameter weighing the importance of view . 

\paragraph{3D Few-shot Classification.}
Given a small set of 3D training data, we can modify our smoothing operation of the realistic shape projection to be learnable, as shown in Figure~\ref{fig:shape_projection}. Specifically, as the irregular point clouds have been converted into grid-based voxels, we adopt two 3D convolutional layers after the Gaussian filter. Such learnable modules can summarize the 3D geometric knowledge from the few-shot dataset, and further adapt the 3D shape to be more CLIP-friendly. During training, we freeze the two encoders of CLIP to preserve the pre-trained knowledge and avoid over-fitting on small-scale few-shot data.

\paragraph{3D Zero-shot Part Segmentation.}
Besides shape classification, we propose a zero-shot segmentation pipeline for our framework, which can also work for the existing PointCLIP. Instead of the global features , we adopt CLIP's visual encoder to extract dense features  from , where . Specifically, we output the feature maps from the visual encoder before its final pooling operation and upsample the features into the original depth map size. 
For our 3D-specific text, we utilize GPT-3 to generate the descriptions for different part categories. As an example, for a part category ``\texttt{[PART]}'' within object ``\texttt{[CLASS]}'', we construct the command as ``\texttt{Describe the {[PART]} part of a {[CLASS]} in a depth map:}''.
Then, for view , we conduct dense alignment between each pixel and the textual feature , \ie, segmenting different parts of the shape on multi-view depth maps, formulated as

Each element in  denotes the pixel-wise classification logits. After this, we back-project the logits of different views into the 3D space according to the 2D-3D correspondence. As one view can only depict a partial point cloud due to occlusion, we average the prediction across different views for each point, where we acquire the final part segmentation logits in 3D space. Via the geometric back projection, the segmentation task in 3D can be tackled in a zero-shot manner.

\vspace{-0.2cm}
\paragraph{Zero-shot 3D Object Detection.}
For 3D object detection, we follow the settings of 2D open-world detection~\cite{gu2021open, zhong2022regionclip} to equip our V2 as a zero-shot classification head on top of pre-trained region proposal networks (RPN). 
We first utilize 3DETR~\cite{misra2021end} as the 3D RPN to generate class-agnostic 3D box candidates.
Then, we extract the raw points within each 3D box and feed them into V2 for zero-shot classification. 
By this, the V2-based 3DETR can detect `unseen' objects in a zero-shot manner.


\section{Experiments}
\label{sec:experiments}

In this section, we first illustrate the detailed network configurations of PointCLIP V2, and then present our open-world performance on different 3D tasks.

\subsection{Implementation Details}
\paragraph{CLIP Prompting.} 
We follow PointCLIP~\cite{zhang2022pointclip} to project a point cloud into depth maps of  views. We set the size of grid  as , and the projected depth map is upsampled to . The point cloud is placed at the center of the grid, and the scale factor  is set to  for better visual appearances. The window size of the minimum pooling for densifying is . The kernel size of the Gaussian filter is set as . We randomly sample  points as input and adopt Vision Transformer \cite{dosovitskiy2020image} with patch size  as default, denoted as ViT-B/16. 

\vspace{-0.1cm}
\paragraph{GPT Prompting.} 
We design 50 different 3D language commands, containing  for caption generation,  for question answering,  for paraphrase generation, and  for words-to-sentence. Each command triggers GPT-3 to produce  3D-specific descriptions, and we finally obtain around  descriptions for each command type and  descriptions in total for one category. We use ``text-davinci-002'' GPT-3 engine and set the temperature constant to . The largest length of a 3D-specific description is set to . For the textual encoder, a 12-layer transformer~\cite{vaswani2017attention} is adopted to encode our generated text.

\begin{figure*}[t]
\vspace{5pt}
\begin{minipage}[c]{0.60\textwidth}
\centering
\hspace{-10pt}
\includegraphics[width=4.9cm]{figs/modelnet40.pdf}\hspace{10pt}\includegraphics[width=4.9cm]{figs/scanobjectnn.pdf}
\caption{\textbf{Few-shot 3D Classification on ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting}.} We adopt the PB\_T50\_RS split of ScanObjectNN for comparison.}
\label{fig:few_shot_classification}
\end{minipage}\hspace{10pt}
\begin{minipage}[c]{0.33\textwidth}
\vspace{9pt}
\begin{adjustbox}{width=5.9cm}
	\begin{tabular}{cccc}
	\toprule
	\makecell[c]{Learnable\\ Smooth} & \makecell[c]{View\\ Weighing} & \makecell[c]{GPT\\ Prompting} & \makecell[c]{-shot} \\ \cmidrule(lr){1-4}
	- & - & - & \\
	\checkmark & - & - & \\
	\checkmark & \checkmark & - & \\
	\checkmark & - & \checkmark & \\ 
	\checkmark & \checkmark & \checkmark &  \\ 
	\bottomrule
	\end{tabular}
	\end{adjustbox}
	\vspace{5pt}
    \tabcaption{\textbf{Ablation Study of Few-shot Learning on ModelNet40~\cite{wu20153d}.} We report the -shot classification accuracy (\%). }
\label{table:few_ablation}
\end{minipage}\hspace{17pt}
\end{figure*}

\subsection{Zero-shot Classification}
\label{zero-exp_sec}

\begin{table}[t!]
\centering
\begin{adjustbox}{width=0.89\linewidth}
	\begin{tabular}{ccccc}
	\toprule
		Quantize & Densify & Smooth & Squeeze & Zero-shot \\ \cmidrule(lr){1-5}
		- & Min & \checkmark & - & \\
		\checkmark & - & - & \checkmark & \\
		\checkmark & Min & - & \checkmark &  \\
		\checkmark & - & \checkmark & \checkmark & \\
		\checkmark & Max & \checkmark & \checkmark &  \\
		\checkmark & Avg & \checkmark & \checkmark &  \\
  \checkmark & Min & \checkmark & \checkmark &  \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.2cm}
\caption{\textbf{Ablation Study of Realistic Shape Projection} on ModelNet40~\cite{wu20153d} zero-shot classification (\%). We compare the four steps in our projection module.}
\vspace*{-5pt}
\label{table:shape_projection}
\end{table}

\paragraph{Settings.}
The zero-shot classification performance is evaluated on three widely-used benchmarks: ModelNet10~\cite{wu20153d}, ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting}. Three splits of the ScanObjectNN dataset are investigated: OBJ\_ONLY, OBJ\_BG, and PB\_T50\_RS. Following the zero-shot principle, we directly test the classification performance on the full test set without learning from the training set. We compare existing methods under their best settings. Specifically, ViT-B/16 is adopted for both our model and CLIP2Point \cite{Huang2022CLIP}. For PointCLIP, we utilize ResNet-101~\cite{he2016deep}, ResNet-50~\cite{radford2021learning}, and ViT-B/16, respectively for ModelNet10, ModelNet40, and ScanObjectNN datasets, which is to fully achieve its best performance.

\vspace{-0.2cm}
\paragraph{Main Results.} In Table~\ref{table:zero_shot_classification}, we compare the zero-shot classification performance with existing approaches.
Some models require extra pre-training on 3D point cloud datasets. CLIP2Point trains a depth map encoder on ShapeNet dataset \cite{chang2015shapenet}, and then uses it for a 3D zero-shot classification task. Cheraghian \etal \cite{cheraghian2022zero} directly extracts point cloud features with a 3D encoder. They sample `seen' categories in the dataset to pre-train the model, and validate on the `unseen' categories. In contrast, PointCLIP and our V2 discard any 3D training and can directly test on 3D datasets.
For all three benchmarks, our approach outperforms existing works by significant margins. V2 achieves  and  accuracy on ModelNet10 and ModelNet40, respectively, surpassing PointCLIP by  and . V2 also achieves  on PB\_T50\_RS split of the ScanObjectNN dataset, demonstrating our effectiveness under noisy real-world scenes.

\begin{table}[t!]
\centering
\begin{adjustbox}{width=0.91\linewidth}
	\begin{tabular}{ccccc}
	\toprule
		Caption & Question & Paraphrase & Words & Zero-shot \\ \cmidrule(lr){1-5}
		- & - & - & - & \\
		\checkmark & - & - & - & \\
		\checkmark & \checkmark & - & - & \\
		\checkmark & - & \checkmark & - & \\
		\checkmark & \checkmark & \checkmark & - & \\
		\checkmark & \checkmark & - & \checkmark & \\ 
		\checkmark & \checkmark & \checkmark & \checkmark &  \\ 
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.2cm}
\caption{\textbf{Ablation Study of GPT Prompting} on ModelNet40~\cite{wu20153d} zero-shot classification (\%). We compare four types of language command to generate the 3D-specific text.}
\vspace*{-5pt}
\label{table:ablation_commands}
\end{table}

\begin{table*}[t!]
\centering
\vspace*{-0.3pt}
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{l|c|cccccccccccc}
	\toprule
		& mIoU & Airplane & Bag & Cap & Chair & Earphone & Guitar & Knife & Laptop & Mug & Rocket & Skate & Table  \\
        \cmidrule(lr){1-14} 
        \# Shapes   &    & &  & &  &  &  &  &  &  &  &  &  \\
        \cmidrule(lr){1-14} 
        PointCLIP &  &  &  &  &  &  &  &  &  &  &  &  &  \\
        \textbf{PointCLIP V2} &  &  &  &  &  &  &  &  &  &  &  &  &  \\
        \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.15cm}
\caption{\textbf{Zero-shot Part Segmentation (\%) on ShapeNetPart~\cite{yi2016scalable}.} We implement PointCLIP by our proposed segmentation pipeline.}
\label{table:segment_iou}
\end{table*}
\begin{table*}[t!]
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{c|l|c|cccccccccccc}
	\toprule
		 & Method & Mean & Cabinet & Bed & Chair & Sofa & Table & Door & Window & Counter & Desk & Sink & Bathtub \\
        \cmidrule(lr){1-14} 
        \multirow{2}{*}{AP} & PointCLIP* &  &  &  &  &  &  &  &  &  &  &  &  \\
        & \textbf{PointCLIP V2} &  &  &  &  &  &  &  &  &  &  &  &  \\
        \cmidrule(lr){1-14} 
        \multirow{2}{*}{AP} & PointCLIP &  &  &  &  &  &  &  &  &  &  &  & \\
        & \textbf{PointCLIP V2} &  &  &  &  &  &  &  &  &  &  &  &  \\
        \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.15cm}
\caption{\textbf{Zero-shot 3D Object Detection (\%) on ScanNet V2~\cite{dai2017scannet}.} We implement PointCLIP by our proposed detection pipeline.}
\label{table:zeroshot_detection}
\end{table*}

\vspace{-0.2cm}
\paragraph{Ablation Study.}
In Table \ref{table:shape_projection}, we conduct an ablation study of PointCLIP V2 concerning four steps of the realistic projection module. When we directly project the point cloud into 2D images via orthogonal projection, the zero-shot accuracy performs , reduced by . If the quantizing step is adopted, the densifying and smoothing operation can improve zero-shot performance by  and , respectively, indicating the importance of these two steps. We also compare alternative pooling operations for the densifying step, including maximum, minimum, and average pooling. We observe that the minimum pooling achieves the best performance, which is consistent with the occlusion effect in the real world. In Table \ref{table:ablation_commands}, we show the effect of four command types in the GPT prompting module. Under different command combinations, the zero-shot performance is improved with various degrees. If using all four types, the 3D-specific text improves the zero-shot performance by , indicating the great significance of better language-image alignment.

\subsection{Few-shot Classification}

\paragraph{Settings.} We test -shot classification performance on ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting} datasets, where . We adopt the same 3D-specific text used in the zero-shot task as textual input and jointly train the learnable smoothing (Figure \ref{fig:shape_projection}) and inter-view adapter~\cite{zhang2022pointclip}.
 The 3D convolution layers adopt a  kernel size and are followed by a batch normalization \cite{ioffe2015batch} with a ReLU non-linear activation \cite{nair2010rectified}.

\begin{figure}[t!]
\centering
\includegraphics[width=0.47\textwidth]{figs/segment_example.pdf}
\vspace{0.1cm}
\caption{\textbf{Visualization of Zero-shot Part Segmentation on ShapeNetPart \cite{yi2016scalable}.} Our V2 exhibits better fine-grained segmentation than PointCLIP.}
\label{fig:segment_example}
\end{figure}

\vspace{-0.1cm}
\paragraph{Main Results.} 
In Figure~\ref{fig:few_shot_classification}, we show the few-shot classification results of V2, comparing with PointCLIP and other four representative 3D networks: PointNet~\cite {qi2016pointnet}, PointNet++~\cite{qi2017pointnet++}, SimpleView~\cite{goyal2021revisiting}, and CurveNet~\cite{Xiang2021Walk}. As shown, V2 outperforms all other methods by few-shot training and shows a more significant improvement on -shot classification. V2 surpasses PointCLIP's -shot accuracy by \% on ModelNet40 and \% on ScanObjectNN. In addition, our approach achieves a -shot accuracy of  on ModelNet40 dataset, even approaching the fully supervised PointNet\cite{qi2016pointnet}.

\vspace{-0.1cm}
\paragraph{Ablation Study. }
In Table~\ref{table:few_ablation}, we report the impact of different modules on few-shot V2 with -shot results, including the learnable smoothing, the view weighing following PointCLIP, and 3D-specific text from GPT prompting.
We find that the learnable 3D projection module improves -shot accuracy by  than the fixed one, and adopting 3D-specific text improves accuracy by . 

\subsection{Zero-shot Part Segmentation}

\paragraph{Settings.} 
We evaluate the zero-shot segmentation performance on the ShapeNetPart dataset \cite{yi2016scalable}, which includes  categories and  annotated parts. Following prior methods \cite{qi2017pointnet++,wang2019dynamic,ma2022rethinking}, we sample 2048 points from each point cloud, and test on the official test split. \textit{For comparison, we implement PointCLIP via our proposed zero-shot segmentation pipeline and report the best-performing results.} 

\vspace{-0.1cm}
\paragraph{Main Results.}
We show the mean intersection of union score across instances (mIoU) in Table \ref{table:segment_iou}. Our method surpasses PointCLIP by  for overall mIoU and performs consistently better on different object categories. We also visualize the segmentation results in Figure~\ref{fig:segment_example}, which further demonstrates our effectiveness to capture fine-grained 3D patterns in a zero-shot manner.

\subsection{Zero-shot 3D Object Detection.} 

\paragraph{Settings.}
ScanNet V2 dataset~\cite{dai2017scannet} is utilized to evaluate the detection performance, which contains 18 object categories. We adopt the pre-trained 3DETR-m~\cite{misra2021end} model as the region proposal network and extract 1024 points within each 3D box. We report the zero-shot detection performance on the validation set using mean Average Precision (mAP) at two different IoU thresholds of  and , denoted as AP and AP. \textit{Also, PointCLIP is implemented by our efforts for zero-shot 3D detection and we report the best-performing results.}

\vspace{-0.1cm}
\paragraph{Main Results.}
Table \ref{table:zeroshot_detection} shows our zero-shot 3D detection results compared with PointCLIP. We observe that PointCLIP V2 achieves mAP and mAP of  and , outperforming PointCLIP by  and , respectively. This verifies that V2 is superior to recognize 3D open-world objects in real-world scenes and obtains great potential for general 3D open-world learning.

\vspace{0.1cm}

\subsection{Other Experiments}

\paragraph{Computation Burden.} We have compared the latency of inference in Table~\ref{table:ablation_projection}. Additionally, we compare the computation complexity to PointCLIP~\cite{zhang2022pointclip} and CLIP2Point~\cite{Huang2022CLIP} in Table \ref{tab:computation_burden}. We test the computation overhead of each inference on 1 RTX A6000 with ViT-B/32 backbone. From the table, V2 causes a similar overhead to PointCLIP and achieves superior zero-shot accuracy on ModelNet40. Thus we achieve a better accuracy-efficiency trade-off.


\begin{table}[t!]
\centering
\begin{adjustbox}{width=0.93\linewidth}
	\begin{tabular}{c c c c }
	\toprule
	\multirow{1}{*}{Method} &\multicolumn{1}{c}{PointCLIP} &\multicolumn{1}{c}{CLIP2Point} &\multicolumn{1}{c}{PointCLIP V2}\\
	\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
	GFLOPs &  &  & \\
        Memory (GB) &  &  & \\
        Accuracy (\%) &  &  & \\
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.3cm}
\caption{\textbf{Comparison of Accuracy and Computation Overhead} with other approaches on ModelNet40~\cite{wu20153d}. }
\label{tab:computation_burden}
\end{table}

\paragraph{More Ablations for Zero-shot Classification.}
\label{abzero}
In Table~\ref{table:zero_class_backbone} and \ref{table:zero_class_numpoint}, we additionally investigate 2 factors that influence the zero-shot classification performance: the visual encoder backbone and the number of sampled points. 
\textbf{1) Different Backbones.}
In Table \ref{table:zero_class_backbone}, we examine the results with different backbones on ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting} datasets. We observe that the default ViT-B/16 backbone achieves the best overall performance. 
\textbf{2) Sample Rate of Points.}
Table \ref{table:zero_class_numpoint} presents the effect of different numbers of sampled points. Note that the officially released ModelNet40 dataset contains only  points per point cloud, so we adopt a resampled version of ModelNet40 from \cite{wang2022p2p}, which contains  points per point cloud. We observe improvements when increasing the sampling rate of points. 


\section{Conclusion}
\label{sec:conclusion}

We propose PointCLIP V2, a powerful and unified 3D open-world learner, which surpasses the existing PointCLIP with significant margins. We propose to prompt CLIP with a realistic projection module for producing high-quality depth maps from 3D, and prompt GPT-3 model to generate 3D-specific descriptions. The visual and language representations achieve better alignment via prompting. Besides classification, V2 can generalize to various challenging tasks with promising performance, including 3D few-shot classification, 3D zero-shot part segmentation, and object detection. For future work, we will further explore how to adapt CLIP to wider open-world applications, \eg, outdoor 3D detection and visual grounding.

\begin{table}[t]
\centering
\begin{adjustbox}{width=\linewidth}
	\begin{tabular}{ccccccc}
	\toprule
		Datasets &RN50 & RN101 & ViT-B/32 & ViT-B/16 & RN.4\\
        \cmidrule(lr){1-6}
        \specialrule{0em}{1pt}{1pt}
		ModelNet40 & &  &   & &  \\
        ScanObjectNN & & &  & & \\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{1pt}
\caption{\textbf{Ablation Study on Visual Encoders} for Zero-shot Classification (\%) on ModelNet40~\cite{wu20153d} and ScanObjectNN~\cite{uy2019revisiting}.}
\label{table:zero_class_backbone}
\end{table}

\begin{table}[t!]
\centering
\vspace{0.1cm}
\begin{adjustbox}{width=0.89\linewidth}
	\begin{tabular}{ccccccc}
	\toprule
		Point Number &1024 & 2048 &3072 & 4096 & 8192 \\
        \cmidrule(lr){1-6}
        \specialrule{0em}{1pt}{1pt}
        
		 ModelNet40 &64.22 & 65.28 & 66.17 & 66.45  & \textbf{68.56}\\ 
		 \specialrule{0em}{1pt}{1pt}
        ScanObjectNN &34.91 &36.05  &36.26  &37.27 & \textbf{38.90} \\ 
		 \specialrule{0em}{1pt}{1pt}
	\bottomrule
	\end{tabular}
\end{adjustbox}
\vspace{0.3cm}
\caption{\textbf{Ablation Study on Point Number} for Zero-shot Classification (\%).}
\label{table:zero_class_numpoint}
\end{table}


\paragraph{Acknowledgement.}
This work is partially supported by the National Natural Science Foundation of China (Grant No.62206272), and by the National Key R\&D Program of China (NO.2022ZD0160100).


{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}
