\documentclass{LMCS}

\def\dOi{9(3:19)2013}
\lmcsheading {\dOi}
{1--26}
{}
{}
{Oct.~12, 2012}
{Sep.~17, 2013}
{}

\usepackage{latexsym}

\usepackage{amssymb}

\usepackage{tikz}
\usepackage{hyperref,enumerate}
\usetikzlibrary{shapes.arrows,chains}


\hyphenation{clas-ses}
\hyphenation{call-ed}
\hyphenation{re-pre-sent}
\hyphenation{Kol-mo-gorov}

\def\point{ }
\def\spacy{ }
\def\boxast{\boxplus}
\newcommand{\CalL}{{\mathcal L}}
\newcommand{\range}{{\rm range}}
\newcommand{\bin}{{\rm bin}}

\def\fraq#1#2{\mbox{}}

\theoremstyle{plain}\newtheorem{athm}[thm]{Theorem}
\theoremstyle{plain}\newtheorem{aprop}[thm]{Proposition}
\theoremstyle{plain}\newtheorem{aprob}[thm]{Open Problem}
\theoremstyle{plain}\newtheorem{acor}[thm]{Corollary}
\theoremstyle{plain}\newtheorem{alem}[thm]{Lemma}
\theoremstyle{definition}\newtheorem{adefn}[thm]{Definition}
\theoremstyle{definition}\newtheorem{arem}[thm]{Remark}
\theoremstyle{plain}\newtheorem{aexmp}[thm]{Example}
\theoremstyle{plain}\newtheorem{aclm}[thm]{Claim}
\def\ownproof{\noindent {\bf Proof.}\ }
\def\suchthat{:}
\def\niceqed{~~}
\def\conv{{\rm{conv}}}
\def\sp{\\*\indent}

\begin{document}

\title[Automatic functions, linear time and learning]{Automatic
Functions, Linear Time and Learning\rsuper*}
\author[J.~Case]{John Case\rsuper a}\address{{\lsuper a}Department of Computer and Information Sciences,
University of Dela\-ware, Newark, DE 19716-2586, USA}
\email{case@cis.udel.edu}

\author[S.~Jain]{Sanjay Jain\rsuper b}\address{{\lsuper b}Department of Computer Science, National University
of Singapore, Singapore 117417, Republic of Singapore}
\email{sanjay@comp.nus.edu.sg}
\thanks{{\lsuper b}S.~Jain is supported in part by NUS grants C252-000-087-001 and
R252-000-420-112}

\author[S.~Seah]{Samuel Seah\rsuper c}\address{{\lsuper{c,d}}Department of Mathematics, National
University of Singapore, 10 Lower Kent Ridge Road,
Singapore 119076, Republic of Singapore}
\email{samseah85@yahoo.com and fstephan@comp.nus.edu.sg}

\author[F.~Stephan]{Frank Stephan\rsuper d}
\thanks{{\lsuper d}F.~Stephan is supported in part by NUS grant
R252-000-420-112.}

\begin{abstract}
\noindent
The present work determines the exact nature of {\em linear time computable\/}
notions which characterise automatic functions (those whose graphs are 
recognised by a finite automaton).
The paper also determines which type of
linear time notions permit full learnability for learning in the limit
of automatic classes (families of languages which are
uniformly recognised by a finite automaton). In particular it is shown
that a function is automatic
iff there is a one-tape Turing machine with a left end which computes the
function in linear time where the input before the computation and the
output after the computation both start at the left end. It is known
that learners realised as automatic update functions are restrictive for
learning. In the present work it is shown that one can overcome the problem
by providing work tapes additional to a resource-bounded base tape while
keeping the update-time to be linear in the length of the largest datum
seen so far.
In this model, one additional such work tape provides additional learning
power over the automatic learner model and two additional work tapes
give full learning power. Furthermore, one can also consider additional
queues or additional stacks in place of additional work tapes and for
these devices, one queue or two stacks are sufficient for full learning
power while one stack is insufficient.
\end{abstract}
\titlecomment{{\lsuper*}A preliminary version of this paper was presented at the conference
``Computability in Europe'' \cite{CJS12}.}

\keywords{Automatic structures,
linear time computation,
computational complexity,
inductive inference,
learning power of resource-bounded learners}

\subjclass{F.1, F.1.3, F.4, I.2.6}

\ACMCCS{[{\bf Theory of computation}]: Models of
  computation---Abstract machines; Logic---Verification by model
  checking; Formal languages and automata theory; Theory and
  algorithms for application domains---Machine learning theory}

\maketitle
\vfill
\section{Introduction} \label{sec:introduction}

\noindent
In inductive inference, automatic learners and linear time learners have
played an important role, as both are considered as valid notions to model
severely resource-bounded learners.
On one hand, Pitt \cite{Pi89} observed that recursive learners can be made
to be linear time learners by delaying; on the other hand, when learners
are formalised by using automata updating a memory in each cycle with
an automatic function, the corresponding learners are not as powerful
as non-automatic learners \cite{JLS09} and cannot overcome their weakness
by delaying. The relation between these two models is that automatic learners
are indeed linear time learners \cite{CJLOSS11} but not vice versa.
This motivates to study the connection between linear time and automaticity
on a deeper level.
\sp
It is well known that a finite automaton recognises a regular language
in linear time. One can generalise the notion of automaticity from
sets to relations and functions \cite{Bl99,BG00,Ho76,Ho83,KN95,Ru08} 
and say that
a relation or a function is automatic iff an automaton recognises its graph,
that is, if it reads all inputs and outputs at the same speed and accepts
iff the inputs and outputs are related with each other, see
Section~\ref{se:autofuncchar} for a precise definition using the
notion of convolution. For automatic functions it is not directly
clear that they are in deterministic linear time, as recognising a graph
and {\em computing the output of a string from the input\/}
are two different tasks. Interestingly,
in Section~\ref{se:autofuncchar} below, it is shown that automatic functions
coincide with those computed by linear time one-tape Turing machines which
have the input and output both starting at the left end of the tape.
In other words, a function is automatic iff it is linear-time computable
with respect to the most restrictive variant of this notion;
increasing the number of tapes or not restricting the position of the
output on the tape results in a larger complexity class.
\sp
Section~\ref{se:lintimelearn} is dedicated to the question on how
powerful a linear time notion must be in order to capture full
learning power in inductive inference. For the reader's convenience,
here a short sketch of the underlying learning model is given:
Suppose  is a language.
The learner gets as input a sequence  of strings,
where each string in  appears in the sequence and all the strings
in the sequence are from  (such a sequence is called a text for ). 
As the learner is getting the input
strings, it conjectures a sequence of grammars 
as its hypotheses about what the input language is.
These grammars correspond to some hypothesis space ,
where  is the set of possible indices and every possible learning
task equals to some .
If this sequence of hypotheses converges to an index  for the language 
(that is ),
then one can say that the learner has learnt the input language from
the given text. The learner learns a language  if it learns it from all
texts for . The learner learns a class  of languages if it learns
all languages from .
The above is essentially the model of learning in the limit proposed
by Gold \cite{Go67}. Equivalently, one can consider the learner as
operating in cycles, in -th cycle it gets the datum  and
conjectures the hypothesis . In between the cycles, the learner
may remember its previous inputs/work via some memory. The
complexity of learners can be measured in terms of the complexity of
mapping the old memory and input datum to the new memory and hypotheses.
For automatic learners, one considers the above mapping to be given by
an automatic function.
\sp
In respect to the automatic learners
\cite{CJLOSS11,JLS09,JOPS10}, it has been the practice to study the
learnability of automatic classes (which are the set of all languages
in some automatic family) and furthermore only to permit hypothesis
spaces which are themselves automatic families containing the automatic
class to be learnt. It turned
out that certain automatic families which are learnable by a recursive
learner cannot be learnt by an automatic learner. The main weakness
of an automatic learner is that it fails to memorise all past data.
If one considers learning from fat text in which each datum occurs
infinitely often, then automatic learners have the same learning
power as recursive learners and their long-term memory can even
be restricted to the size of the longest datum seen so far \cite{JLS09},
the so called word size memory limitation.
\sp
Following the results of Section~\ref{se:autofuncchar},
one can simulate automatic learners by a learner using a one-tape Turing
machine which updates the content of the tape in linear time in
each round. In the present work this tape (called base tape)
is restricted in length by the length
of the longest datum seen so far --- as the corresponding word size
memory limitation of automatic learners studied in \cite{JLS09}. 
In each cycle, the
learner reads one datum about the set to be learnt and revises its
memory and conjecture. The question considered is how much extra power 
needs to be added to the learner for achieving full learnability; here
the extra power is formalised by permitting additional work tapes
which do not have length-restrictions; in each learning cycle the learner can,
however, only work on these tapes in time linear in the length of the
longest example seen so far. It can be shown using an archivation
technique, that two additional work tapes can store all the data observed in
a way that any learner can be simulated. When having only one
additional work tape, the current results are partial: using
a super-linear time-bound, one can simulate
any learner for a class consisting entirely of infinite languages;
furthermore, some classes not learnable by an automatic learner
can be learnt using one work tape. When considering additional
stacks in place of work tapes, two stacks are sufficient while one stack
gives some extra learning power beyond that of an automatic learner
but is insufficient to learn all in principle learnable classes.


\section{Automatic Functions and Linear Time} \label{se:autofuncchar}

\noindent
In the following, two concepts will be related to each other:
automatic functions and functions computed by position-faithful
one-tape Turing machines. In the following, a formal definition
of these two concepts is given. Automatic functions and structures
date back to the work of Hodgson \cite{Ho76,Ho83} and are based
on the concept of convolution. A convolution permits to write pairs
and tuples of strings by combining the symbols at the same position
to new symbols.

\begin{adefn}
\rm 
Let  be a finite alphabet.
Let  be a special symbol not in .
The {\em convolution}  
of two strings  and ,
where ,
is defined as follows. Let
.
For ,
if  then let  else let ;
if  then let  else let .
Now, the convolution is

where the symbols of this word are from the alphabet
.
\end{adefn}

\noindent
Note that in the above definition, both  and  can be ,
the empty string.
Similarly one can define the
convolution of a fixed number of strings. Now the convolution permits
to introduce automatic functions and relations.

\begin{adefn}[Hodgson \cite{Ho76,Ho83}]
\rm A function , mapping strings to strings
(possibly over a different alphabet), is said to be 
{\em automatic} iff the set
 is regular.

Similarly, an -ary relation
 is
automatic iff
 is
regular. An -ary function  is automatic iff
 is regular.
\end{adefn}

\noindent
Here a regular set \cite{HMU01} is a set which is recognised by a deterministic
finite automaton. This concept is equivalent to the one of sets
recognised by non-deterministic finite automata. Furthermore,
one can define regular sets inductively:
Every finite set of strings is regular. The concatenation of
two regular languages is regular, where ; similarly, the union, intersection and set difference of
two regular sets is regular. A further construct is the Kleene star,
, of a regular language  where . Note that
 always contains the empty string . 
The above mentioned operations
are all that are needed, that is, a set is regular
iff it can be constructed from finite sets by using the above
mentioned operations in finitely many steps. 
The above inductive definition can be used
to denote regular sets by regular expressions which are written
representations of the above mentioned operations, for example,
 represents the set of all strings over  and

represents the set of all binary strings of even length which
contain at least one .
\sp
The importance of the concept of automatic functions and automatic relations
is that every function or relation,
which is first-order definable from a finite number
of automatic functions and relations, is automatic again and the
corresponding automaton
can be computed effectively from the other automata. This gives the second
nice fact that structures consisting of automatic functions and relations
have a decidable first-order theory \cite{Ho83,KN95}.
\sp
A {\em position-faithful one-tape Turing machine} is a Turing
machine which uses a one-side infinite tape, with the left-end having
a special symbol  which only occurs at this position and cannot
be modified. The input starts from the cell at the right 
of  and is during the computation replaced by the output
which starts from the same cell. The end of input and output is the first
appearance of the symbol  which is the default value of an empty
cell before it is touched by the head of the Turing machine or filled with
the input.

It is assumed that the Turing machine halts when it enters an accepting/final 
state (if ever).
A position-faithful one-tape Turing machine computes a function , if
when started with tape content being   ,
the head initially being at , the Turing machine eventually
reaches an accepting state (and halts), 
with the tape content starting with .
Note that there is no restriction on the output beyond the
first appearance of . Furthermore, a Turing machine can halt
without reaching an accepting state, in which case the computation is
not valid; this possibility is needed when a non-deterministic Turing
machine has to satisfy a time bound on the duration of the computation.
\sp
Though the requirement of ``position-faithfullness'' seems to be a bit
artificial, it turns out that it is a necessary requirement. This is
not surprising, as moving  bits by  cells requires, in the worst case,
proportional to  steps. So sacrificing the requirement
of position-faithfullness clearly increases the class. For example,
the function which outputs the binary symbols between the first and
second occurrence of a digit other than  and  of an input would
become linear time computable by an ordinary one-tape Turing machine
although this function is not linear time
computable by a position-faithful one-tape Turing machine. Such an
additional side-way to move information (which cannot
be done in linear time on one-tape machines) 
has therefore been excluded from the model.
The functions computed by position-faithful one-tape Turing
machines are in a certain sense a small natural class of linear time
computable functions.
\sp
Some examples of automatic functions are those which append to or delete in
a string some characters as long as the number of these characters is bounded
by a constant. For example a function deleting the first 
occurrence (if any)
of  in a string would be automatic; however, a function deleting all
occurrences of  is not automatic. Below is a more comprehensive example
of an automatic function.

\begin{exa} \label{ex:firstlastexchange}
Suppose . Suppose  is a mapping from  to
 such that  interchanges the first and last symbol in ;
.
Then  is automatic and furthermore  can also be computed by a
position-faithful one-tape Turing machine.
\sp
To see the first, note that the union of the set
 and
all sets of the form  with  is a regular set.
Thus 
is a regular set and  is automatic.
\sp
A position-faithful one-tape Turing machine would start on the
starting symbol  and go one step right. In the case
that there is a  in that cell,
the machine halts. Otherwise it memorises in
its state the symbol  there. Then it goes right until it finds
; it then goes one step left. 
The Turing machine then memorises the symbol  at this position
and replaces it by . It then goes left until it finds ,
goes one step right and writes .
\end{exa}

\noindent
That  in the preceding example can be computed in both ways
is not surprising, but indeed a consequence of the main result
of this section which states that the following three
models are equivalent:
\begin{iteMize}{}
\item automatic functions;
\item functions computed in deterministic linear time by a position-faithful
      one-tape Turing machine;
\item functions computed in non-deterministic linear time by a
      position-faithful one-tape Turing machine.
\end{iteMize}
This equivalence is shown in the following two results, where the first
one generalises prior work \cite[Remark 2]{CJLOSS11}.

\begin{thm} \label{th:islinear}
Let  be an automatic function. Then there is a deterministic linear time
one-tape position-faithful Turing machine which computes .
\end{thm}

\proof
The idea of the proof is to simulate the behaviour of a deterministic
finite automaton recognising the graph of . The Turing Machine
goes from the left to the right over the input word and takes note of which
states of the automaton can be reached from the input with only one unique 
possible output.
Once the automaton reaches an accepting state in this simulation
(for input/output pairs), the simulating Turing machine turns back (that
is, it goes from right to left over the tape) 
converting the sequence of inputs and the stored information about 
states as above into that output which produces the unique accepting run 
on the input. Now the formal proof is given.
\sp
Suppose that a deterministic automaton with  states (numbered 
to , where  is the starting state) accepts a word
of the form  iff 
is in the domain of  and ; the automaton rejects
any other sequence. Note that this small modification of the
way the convolution is represented simplifies the proof.
As  depends uniquely on , any string of the form
 
accepted by the automaton satisfies . 
Let  be the transition function for the automaton above
and  be the corresponding extended transition
function~\cite{HMU01}.
\sp
Suppose that the input is .
Let the cell number  be that cell which carries the input  
(with cell  carrying ), that is the -th cell to the
right of ;  is in cell number . 
Note that the Turing Machine described below
does not use the cell number in its computation; 
the numbering is used just for ease of notation.
The simulating Turing machine
uses a larger tape alphabet containing extra symbols from
,
that is, one considers the additional symbols consisting of tuples
of the form , where  
and
. These symbols are written temporarily onto
the tape while processing the word from the left to the right and later
replaced when coming back from the right to the left.
\sp
Intuitively, during the computation while going from left to right, 
for cell number ,
one wishes to replace  by the tuple  where,
for :  iff there is no word
of the form  such that the automaton on input
 reaches the state 
(that is, for no , 
); 
 iff there is exactly one
such word;  iff there are at least two such words. Here
the  and  can also be  (when  is larger than
the length of the relevant string, for example  for ).
\sp
For doing the above, the Turing machine simulating the automaton replaces the 
cell to the right of , that is,
the cell containing , by .
Then, for the -th cell, , 
to the right of ,
with entry  (from the input or
 if ) the Turing machine replaces  by
 under the following conditions,
(where the entry in the cell to the left was  and where  ranges over ): 
\begin{iteMize}{}
\item  is  iff there is exactly one  
  such that  is  and  
  and there is no pair  
   such that  
  is  and ;
\item  is  iff there are at least two pairs  
   such that
   is  and 
  or there is at least one pair  
  such that 
  is  and ;
\item  is  iff for all pairs  
  such that 
   , it holds that 
   is .
\end{iteMize}
Note that the third case applies iff the first two do not apply. The automaton
replaces each symbol in the input as above until it reaches the cell 
where the intended symbol  has
 for some accepting state .
(Note that the accepting states occur in the automaton only if both
the input and output are exhausted by the convention made above.)
If this happens, the Turing machine turns around, memorises
the state , erases this cell (that is, writes )
and goes left. 
\sp
When the Turing machine moves left from the cell number  to the cell 
number  (which contains the entry ),
where the state memorised for the cell number  is , 
then it determines the
unique  such that  and
;
then the Turing machine 
replaces the symbol on cell  by .
Then the automaton keeps the state  in the memory and goes to
the left and repeats this
process until it reaches the cell which has the symbol
 on it. Once the Turing machine reaches there, it
terminates.
\sp
For the verification, note that the output  
(with  appended) satisfies
that the automaton, after reading , is always in a state 
with ,
as the function value  is unique in ;
thus, whenever the automaton ends up in an accepting state  with
 then the input-output-pair
 has
been completely processed and  has been
verified. Therefore, the Turing machine can turn and follow the
unique path, marked by  symbols, backwards in order to reconstruct
the output from the input and the markings. All superfluous symbols
and markings are removed from the tape in this process. 
As the automaton accepts ,
and  depends uniquely on , . 
Hence the runtime
of the Turing machine is bounded by , that is, the
runtime is linear.\niceqed

\begin{rem}
Note that the Turing machine in the above theorem 
makes two passes, one from the origin to
the end of the word (plus maybe constantly many more cells)
and one back. These two passes are needed
for a deterministic Turing machine: Recall
the function  from Example~\ref{ex:firstlastexchange} with

for all non-empty words . When starting at the left
end, the machine has first to proceed to the right end to read the last
symbol before it can come back to the left end in order to write that
symbol into the new first position. Hence the runtime of the one-tape
deterministic Turing machine (for the simulation as in
Theorem~\ref{th:islinear})
cannot be below  for an input . Non-deterministic Turing
machines can, however, perform this task with one pass.
\end{rem}

\noindent
For the converse direction of the equivalence of the two models
of computation, that is, of automatic functions and position-faithful linear
time Turing machines, assume that a function is computed by a position-faithful
non-deterministic one-tape Turing machine in linear time.
For an input , any two non-deterministic
accepting computations have to produce the same output . Furthermore,
the runtime of each computation has to follow the same linear bound
, independent of whether the computation ends up in an
accepting state or a rejecting state.

\begin{thm} \label{th:isautomatic}
Let  be a function computed by a non-deterministic one-tape
position-faithful Turing machine in linear time.
Then  is automatic.
\end{thm}

\proof
The proof is based on crossing-sequence methods, see \cite{He65,He65b}
and \cite[Section VIII.1]{Od99}. The idea is to show that  is automatic
by providing a non-deterministic automaton
which recognises the graph  of the function by going
symbol by symbol over the convolution of input and output and for each
symbol, the automaton guesses, for the Turing Machine on the corresponding
input, the crossing sequence on the right side
and verifies that this crossing-sequence is
compatible with the previously guessed crossing-sequence on the
left side of the symbol plus the local transformation of the respective
input symbol to the output symbol. This is now explained in more detail.
\sp
Without loss of generality one can assume that the position-faithful
Turing machine  computing  starts at 
and returns to that position at the end; a computation accepts only when the
full computation has been accomplished and the automaton has returned to
. By a result of Hartmanis~\cite{Ha68} and
Trakhtenbrot~\cite{Tr64}, there is a constant  such that an accepting
computation visits each cell of the tape at most  times; otherwise
the function  would not be linear time computable. This permits to
represent the computation locally by considering for each visit to a cell --- 
the direction from which the Turing machine  entered the cell, in which 
state it was, what activity it did and in which direction it left the cell. 
Below, the -th cell to the right of  is referred to as cell
number .
The local computation at the cell number 
can be considered as a tuple
, for some ,
where  is the initial symbol at the cell number , and for each ,
 denotes the state the Turing machine  was in
when it visited the cell number  for the -th time,
 is the state the Turing machine  was in when it 
left the cell number  after the -th visit,
 is the direction in which the 
Turing machine  left after the -th visit,
and  is the symbol written in the cell number  
by the Turing machine 
during the -th visit;
 here denotes the total number of visits of the Turing machine 
to the -th cell. Note that the number of possibilities for
the local computation as above is bounded by a constant.
\sp
As an intermediate step one shows that
a non-deterministic finite state automaton can recognise the set
\begin{quote}
  
  the Turing machine  on input 
  does not move beyond cell number .
\end{quote}
This is done by initially guessing the local computation at 
(the -th cell).
Then on each subsequent input  (where  or 
might be ), starting with ,
the automaton (i) guesses the local computation at the -th cell, 
(ii) checks that this guess in (i) is consistent with the local
computation guessed
at cell  (that is, each time the Turing machine  moved
from cell  to cell  or cell  to , the corresponding guessed
leaving/entering states match), (iii) the computation within the cell
is consistent with the
Turing machine 's state table (that is, either each of the entries
 satisfies that Turing machine has transition
from state  on reading input  to
state  writing  in the cell and moving in direction
, where  and  or the Turing machine
does not reach this cell and ), (iv) 
for the last input the automaton also checks that it is of the form
 and that the Turing machine  does not reach
this cell.
\sp
If at the end, all the computation and guesses are consistent then the
automaton accepts.
The automaton thus passes over the full word and
accepts  iff the 
non-deterministic computation transforms 
 into 
.
\sp
It follows that the set 

is regular as well, as it is first-order definable from
 and the prefix relation:  
 does not end with  and
 is a prefix of an element in .
Thus  is automatic.\niceqed

\begin{rem}
One might ask whether the condition on the input and output starting at
the same position is really needed. The answer is ``yes''. Assume by way
of contradiction that it would not be needed and that all functions linear
time computable by a one-tape Turing machine without any restrictions on
output positions are automatic. Then one could consider the free monoid
over . For this monoid, the following function could be computed
from : The output is  if ; the output is 
if such a  does not exist. For this, the machine just compares  with
 and erases ,  with  and erases  and so on,
until it reaches (a) a pair of the form  with 
or (b) a pair of the form  or (c) a pair of the form

or (d) the end of the input.
In cases (a) and (b) the output has to be  and the machine just erases
all remaining input symbols and puts the special symbol  to denote the
special case; in case (c) the value  is just obtained by changing
all remaining input symbols  to  and the Turing machine
terminates. In case (d) the valid output is the empty string and the
Turing machine codes it adequately on the tape. Hence  would be
automatic. But now one could first-order define concatenation  by
letting  be the  for which ; this would give
that the concatenation is automatic, which is known to be 
false. The non-automaticity of the concatenation can be seen
as follows: For each automatic function there
is, by the pumping lemma \cite{HMU01}, a constant  such that
each value is at most  symbols longer than the corresponding input; now the
mapping  fails to satisfy this for any given
constant , for example,  and  are
mapped to .
Hence the condition on the starting-positions cannot be dropped.
\end{rem}

\noindent
One can generalise non-deterministic computation to computation
by alternating Turing machines \cite{CKS81}.
Well known results in this field \cite{CKS81}
are that sets decidable in alternating logarithmic space are equal
to sets decidable in polynomial time and
that alternating polynomial time computations define the class
PSPACE for sets. Therefore it might be interesting
to ask what is the equivalent notion for alternating linear time computation.
The following definition deals with the
alternating computation counterpart of position-faithful linear
time computations.

\begin{adefn} \rm
An alternating position-faithful one-tape Turing machine  has
-states and -states among the Turing machine 
states which permit the machine to guess one bit (which can then
be taken into account in future computation).
It uses, as the name says, exactly one tape which initially
contains , where  is the input string.
At the end of the computation, the output is the string 
between the  and the first .
 is linear time bounded iff there is a constant  such that, for
each input  of length  and each run of , the duration of
the run until  halts is at most  time steps.
Furthermore,  alternatingly computes a function 
iff for each string  on the input there is a unique string  
(which must be equal to ) such that,
for a computation tree  formed by chosing at each -state
the guessed bit appropriately (the -states are still
true branching nodes in this tree ), one has that
each computation path on  ends up in an accepting state 
and each computation produces the same output .
\end{adefn}

\noindent
It is easy to see that every function  computed non-deterministically
by a position-faithful one-tape Turing machine in linear time
is also computed by an alternating position-faithful one-tape Turing
machine in linear time. However, the converse direction is open;
if the answer would be negative, one could use it as the basic definition
of a concept similar to automatic structures which is slightly more
general.

\begin{oprob}
Is every function  computable in alternating linear time
by a position-faithful one-tape Turing machine automatic?
\end{oprob}


\section{Linear Time Learners} \label{se:lintimelearn}

\noindent
The following definition of learning is based on
the Gold's \cite{Go67} notion of learning in the limit. The presentation
differs slightly in order to incorporate memory restrictions and automaticity
as considered in this paper; note that learners without any restrictions
on the way the long term memory is organised can store all past data and
are therefore as powerful as those considered by Gold \cite{Go67}.

Informally, a learning scenario can be described as follows.
Suppose a family  of languages is given (in some effective
form), where  is an index set.
The learner, as input, gets a listing of the elements of some set .
The learner is supposed to figure out, 
in the limit from the listing as above, an index  such that . 
For ease of presentation it is assumed that all languages 
are not empty.
\sp
The listing of elements is formalised as a text.
A {\em text}  for a language  is an infinite sequence,
, containing all elements of  but no
non-element of , in any order with repetitions allowed. 
Let  denote the sequence of first  elements of the text:
.
The basic model of inductive inference \cite{An80,BB75,Go67,JORS99,OSW86}
is that the learner  is given a text 
of all the words in a language , one word per cycle.
At the same time  outputs a sequence  of indices, 
one index in each cycle. Intuitively, each  can be considered
as a conjecture of the learner regarding what the language  is,
based on the data . In general, the indices
conjectured are from some index set  and 
interpreted in a hypothesis space , where
.
\sp
The learner maintains information about past data in
form of some memory, which may change between cycles.
Thus, the learner can be considered as an
\begin{quote}
algorithmic mapping from (old memory, new datum) to
(new memory, new conjecture)
\end{quote}
where the learner has some fixed initial memory.
The learner {\em learns} or {\em identifies} the language , if,
for all possible texts for , there is some 
such that  is an index for  and
 for . The learner learns a class  of
languages if it learns each language in .
\sp
The most basic set of hypothesis spaces are
automatic families of languages. Here, a family of
languages , is {\em automatic} if the index set  and
the set  are both regular. 
Automatic families \cite{JLS09,JOPS10} are the automata-theoretic 
counterpart of indexed families \cite{An80,LZZ08} which were widely used
in inductive inference to represent the class to be learnt.
Note that when  is a hypothesis space for ,
which is an automatic family as well, then there is an automatic function 
mapping the indices from  back to those in , that is, 
for all those  where  equals some . Hence one can
without loss of generality (for learning criteria considered in
this paper) directly use the hypothesis space
 for the class  to be learnt.
\sp
A learner  is called {\em automatic}
if the mapping (old memory, new input word) to 
(new memory, new conjecture) for the learner is automatic, that is,
the set

is regular. In general,  and  are the old and new versions
of the long term memory of the learner. Automatic learners are, roughly
speaking, the most restrictive form of learners which update a long term
memory in each cycle where they process one new datum given to the learner.
\sp
The next definition generalises the notion of automatic learning to
a learner which has a linear or nearly linear time bound for each of
its cycle. This generalisation is natural, due to the correspondence
between automatic function and linear time computable functions given
in the previous section of this paper.

\begin{adefn}
A learner  is a Turing machine which maintains some memory and in each
cycle receives as input one word to be learnt, updates its memory
and then outputs an hypothesis.
The tapes of the Turing machine are all one-sided
infinite and contain 
at the left end. The machine operates in cycles, where in each cycle
it reads one current datum (from a text of the language to be learnt)
and formulates one hypothesis. Furthermore, it has some long term memory
in its tape where the memory in Tape  is always there while the
memories in the additional data structures (Tapes )
is only there when these additional data structures are explicitly
permitted.
\begin{iteMize}{}
\item At the beginning of each cycle,
      Tape  (base tape) contains convolution of the input
      and some information (previous long term memory) which is not 
      longer in length (up to an additive
      constant) than the length of the longest word seen so far.
      The head on Tape  of the Turing machine starts at  at
      the beginning of each cycle.
\item At the end of each cycle,
      Tape  (base tape) has to contain the convolution of
      the new long term memory and the hypothesis which the
      learner is conjecturing.
\item During the execution of a cycle, the learner
      can run in time linear in the current length of Tape 
      and, like a position-faithful one-tape Turing machine,
      replace the convolution of the current datum and old long term
      memory by the convolution of the hypothesis and the new long term
      memory. Furthermore, the memory in Tape  
      has to meet the constraint that
      it is at most (up to an additive constant) the length of the longest 
      datum seen so far (including the current datum), hence there
      is an explicit bound on the length of Tape  in each cycle.
\item Tapes  are normal tapes, whose contents and head positions
      are not modified during change of cycles.  can use these tapes
      for archiving information and doing calculations. There is
      no extra time allowance for the machine to use these tapes, hence
      the machine can only access a small amount (linear in the length
      of Tape ) in each cycle of these tapes.
\item Without loss of generality, one can assume that the length of the
      longest datum seen so far is stored in the memory in Tape .
\end{iteMize}
The learner is said to have  additional work tapes iff
it has in addition to Tape  also the Tapes .
Figure~\ref{figdefnlearner} illustrates a learner with two additional tapes.
\end{adefn}

\begin{figure}[t]
\begin{tikzpicture}[box/.style={draw,rectangle,minimum width=45pt,
  minimum height=45pt},
  start chain=1 going right,
  start chain=2 going right,
  start chain=3 going right,
  node distance= 0.35mm]
    \node [on chain=1] {Base Tape \ \mbox{ }};
    \node [on chain=2, yshift=-1cm] {Work Tape 1};
    \node [on chain=3, yshift=-2cm] {Work Tape 2};
    \node[draw,on chain=1]{\mbox{}};
    \foreach \x in {1,2} {
        \x, \node [draw,on chain=1] {}; }
    \node[draw,on chain=1]{};
    \foreach \x in {1,2} {
        \x, \node [draw,on chain=1] {}; }
    \node[draw,on chain=1]{\mbox{}};
    \node[draw,on chain=2]{\mbox{}};
    \foreach \x in {1,2,...,3} {
        \x, \node [draw,on chain=2] {}; }
    \node[draw,on chain=2]{};
    \foreach \x in {1,2,...,5} {
        \x, \node [draw,on chain=2] {}; }
    \foreach \x in {1,2,...,3} {
        \x, \node [draw,on chain=2] {}; }
    \node[draw,on chain=3]{\mbox{}};
    \foreach \x in {1,2,...,5} {
        \x, \node [draw,on chain=3] {}; }
    \node[draw,on chain=3]{};
    \foreach \x in {1,2,...,4} {
        \x, \node [draw,on chain=3] {}; }
    \foreach \x in {1,2} {
        \x, \node [draw,on chain=3] {}; }
    \node [name=r,on chain=2] {\ldots}; 
    \node [name=r,on chain=3] {\ldots}; 
\end{tikzpicture}
\caption{Learner with two working tapes; the head positions are  and other
   data positions are ; note that data characters can be convoluted
   characters from finitely many alphabets in order to store the convolution
   of several tracks on a tape.} \label{figdefnlearner}
\end{figure}


\noindent
Note that in the definition of Tape , it is explicit that the length of
the hypothesis produced is bounded by the length of the largest
example seen so far plus a constant. This is compatible with learning, as 
for all automatic families, (i) for any finite set  in the family,
the length of the smallest index  for  overshoots the length 
of the longest element of  by at most a constant
and (ii) for any infinite set  in the family
there are words in  which are longer than some index for ;
thus a learner cannot fail just because
the indices of a language  are extremely long compared
to the size of the members of  -- though, of course, there may be other
reasons for a learner not to be successful.
\sp
Note that if
only Tape  is present, the model is equivalent to an automatic learner
with the memory bounded by the size of the longest datum seen so
far (plus a constant) \cite{CJLOSS11,JLS09}. The next examples illustrate
what type of learnable automatic classes exist.

\begin{exa} \label{ex:autolearn}
The following automatic classes are learnable by an automatic learner
with its memory bounded by the length of the longest example seen so
far (plus a constant):
\sp
First, the class of all extensions of an index, that is,
 and 
for all . Here the learner maintains as a memory the longest
common prefix  of all data seen so far and whenever the memory is 
and a new datum  is processed, the learner updates  to the longest
common prefix of both,  and , which is also the next hypothesis.
\sp
Second, the class of all closed intervals in the
lexicographic ordering, that is,  and ; here  denotes that
  is lexicographically before .
The learner maintains as memory
the lexicographically least and greatest elements seen so far, the convolution
of these elements also serves as hypothesis.
\sp
Third, the class of all strings of
length different from the index, that is,  and
. Here the learner archives in
Tape  binary string which is of length  plus the length of
the longest example
seen so far; the -th bit of this string (starting with )
is  iff an example
of length  has been seen so far, and  iff no example
of length  has been seen so far. The conjecture is  for the least
 such that either the -th bit of the memory is  or  is  plus
the length of the memory string.
\end{exa}

\noindent
For any automatic family, , 
the equivalence question for indices is automatic, that is,
the set  is regular.
Thus for the purposes of this paper, 
one can take the hypothesis space to be one-one, 
that is, different indices represent different languages.
In a one-one hypothesis space, the index  of a finite language 
has, up to an additive constant, the same length as the longest word in ;
this follows easily from \cite[Theorem 3.5]{JOPS10}.
This observation is crucial as otherwise the time-constraint on the learner
would prevent the learner from eventually outputting the correct index;
for infinite languages this is not a problem as the language must contain
arbitrarily long words.
\sp
Angluin \cite{An80} gave a characterisation when a class is learnable in 
general. This characterisation, 
adjusted to automatic families, says that a class is learnable
iff, for every , there exists a finite set 
such that there is no  with . All
the automatic families from Example~\ref{ex:autolearn} satisfy this criterion;
however, Gold \cite{Go67} provided a simple example of a non-learnable
class which of course then also fails at Angluin's criterion: One infinite
set plus all of its finite subsets.
\sp
The main question of this
section is which learnable classes can also be learnt by a linear-time learner
with  additional work tapes. For ,
this is in general not possible, as 
automatic learners fail to learn various learnable classes \cite{JLS09},
for example the class of all sets , with the index
 being from , and the class of all
sets .
\sp
Freivalds, Kinber and Smith \cite{FKS95} introduced limitations on the
long term memory into inductive inference; Kinber and Stephan \cite{KS95}
transferred it to the field of language learning. Automatic learners have
similar limitations and are therefore not able to learn all learnable
automatic classes \cite{CJLOSS11,JLS09}.
The usage of additional work tapes for linear time learners permits to
overcome these limitations, the next results specify how many additional
work tapes are needed. 
Recall from above that work tapes are said to be {\em additional\/}
iff they are in addition to
the base tape.

\begin{thm} \label{th:onetape}
Suppose  and consider the automatic family
 over the alphabet  which is defined as follows:
 consists of
(i)  and
(ii)  and 
(iii) ,
for each .
Then, 
does not have an automatic learner but has a linear-time learner using one
additional work tape.
\end{thm}

\proof
An automatic learner cannot memorise all the data from  it sees.
For any automatic learner, one can show, see \cite{JLS09}, that there
are two finite sequences
of words from , one containing  and one not
containing , such that the
automatic learner has the same long term memory after having seen both
sequences.
If one presents to the automatic learner, after these sequences, all
the elements
of , then the automatic learner's limiting behaviour on the
two texts so formed is the same, even though they are texts for two different
languages,  or , in .
Therefore the learner cannot learn the class .
\sp
A linear time learner with one additional work tape (called Tape )
initially conjectures  and uses Tape  to archive 
all the examples seen at the current end of the written part of the tape.
When the learner sees a word of the form , it maintains a copy of
it in the memory part of Tape  and conjectures  as its hypothesis. 
In each subsequent cycle, the
learner scrolls back Tape  by one word and compares the word there
as well as the current input with ; if one of these two is  then
the learner changes its conjecture to , else it keeps its
conjecture as .
In the case that the origin of Tape 1 () is reached, the learner from
then onwards ignores Tape 1 and only compares the incoming input
with .
It is easy to verify that the learner as described above learns~.\niceqed

\begin{thm} \label{th:twotapes}
Every learnable automatic family  has a linear-time learner using
two additional work tapes.
\end{thm}

\proof
Jain, Luo and Stephan \cite{JLS09} showed that for every learnable
automatic family  there is an automatic
learner  using memory bounded in length by the length 
of the longest example seen
so far (plus a constant) which learns the class from every fat text
(a text in which every element of the language appears infinitely often). 
So the main idea is to use the two additional tapes in order to 
simulate and feed the learner  with a fat text. 
The two additional tapes are used to store all the incoming data and
then to feed the learner  with each data item infinitely often.
The words in the tapes are stored using some separator  to separate
the words. Thus,  indicates that the tape
contains the words , ,  and .

The learner  for  using two additional tapes works as follows.
Suppose the previous memory stored in Tape 0 is  (initially the memory
stored on Tape  is the initial memory of )
and the current datum is . Then,  does the following:
\begin{iteMize}{}
\item Compute .
\item Find the last word in Tape , say . Erase this word from Tape .
      In the case that Tape  was already empty, let .
\item Compute .
\item Write  and  at the end of Tape  (using the separator  to
      separate the words).
\item When the beginning of Tape  is reached (), interchange 
      the roles of Tape  and Tape  from the next cycle.
\item The new memory to be stored on Tape  is  and 
      the conjecture is .
\end{iteMize}
It is easy to see that in each cycle, the time spent is
proportional to  and thus linear in the length
of the longest word seen so far (plus a constant); note that 
 are also bounded
by that length (plus a constant). Furthermore, in the simulation of
, each input word to  is given to  infinitely often.
Hence  learns each language from the class .\niceqed

\begin{oprob}
It is unknown whether one can learn every in principal learnable
automatic class using an automatic learner augmented by only one
work tape.
\end{oprob}


\medskip
\noindent
Further investigations deal with the question what happens if one does
not add further work tapes to the learner but uses other methods to
store memory.
Indeed, the organisation in a tape is a bit awkward and using a queue solves
some problems. A queue is a tape where one reads at one end and writes at the
opposite end, both the reading and writing heads are unidirectional and cannot
overtake each other. Tape  satisfies the same constraints as in the model
of additional work tapes and one also has the constraint that in each cycle
only linearly many symbols (measured in the length of the longest datum seen
so far) are stored in the queue and retrieved from it.

\begin{thm} \label{th:queue}
Every learnable automatic family  has a linear-time learner using
one additional queue as a data structure.
\end{thm}

\proof
The learner simulates an automatic learner  for 
using fat text, in a way similar
to that done in Theorem~\ref{th:twotapes}. Let  in the -th step map
 to  for 's memory . 
\sp
For ease of presentation, the contents of Tape  is considered as
consisting of a convolution of 4 items (rather than  items,
as considered in other parts of the paper).
At the beginning of a cycle the linear-time learner  has 
 on Tape 
where  is the current datum,  the archived memory of  and
``'' refers to irrelevant or empty content. In the -th cycle,
the linear-time learner  
scans four times over Tape  from beginning to the end
and each time afterwards returns to the beginning of the tape:
\begin{enumerate}[(1)]
\item Copy  from Tape  to the write-end of the queue;
\item Read a word from the read-end of the queue, call it ,
      and update Tape  to ;
\item Copy  from Tape  to the write-end of the queue;
\item Simulate  on Tape  in order to map  to
       and update Tape  to .
\end{enumerate}
It can easily be verified that this algorithm permits to simulate 
using the data type of a queue and that each cycle takes only time
linear in the length of the longest datum seen so far.
Thus,  learns . \niceqed

\medskip
\noindent
A further data structure investigated is the provision of
additional stacks. Tape  remains a tape in this model and has
still to obey to the resource-bound of not being longer than the longest
word seen so far (plus a constant). Theorems~\ref{th:onetape}
and~\ref{th:twotapes} work
also with one and two stacks, respectively, as the additional work tapes
are actually used like stacks.

\begin{thm}
There is an automatic class which can be learnt
with one additional stack but not by an automatic learner. Furthermore,
every learnable automatic class can be learnt by a learner
using two additional stacks.
\end{thm}

\noindent
Furthermore, the next result shows that in general one stack is not enough;
so one additional stack gives only intermediate learning power while two
or more additional stacks give the full learning power. The class witnessing
the separation contains only finite sets.
\sp
For information on Kolmogorov complexity,
the reader is referred to standard text books \cite{Ca02,DH10,LV08,Ni09}. 
The next paragraphs provide a brief description of the basic concepts.
\sp
Consider a Turing machine  which computes a partial-recursive function from 
 to . The first input to  
is also referred to as a program.
Machine  is universal iff for every further
machine , there is a constant  such that, for every 
in the domain of , there is a , which is at most  symbols longer than
, satisfying . 
Fix a universal machine . Now the conditional
Kolmogorov complexity  is the length of the shortest program
 with ; the plain Kolmogorov complexity 
is . Note that, due to the universality of ,
the values of  can only be improved by a constant 
(independent of )
when changing from one universal machine to another one.
In some cases below,  is the conditional
Kolmogorov complexity when given an -tuple 
where  might vary; such a tuple can be coded up in any way which
permits to identify the parts uniquely, as automaticity is not required,
the coding  would do it.
\sp
If  is a partial-recursive function then there is a constant  with
 for all  in the domain of . In particular,
if one can find a way to describe the strings  in a set 
by binary strings 
such that some algorithm can compute each 
from the corresponding description , then 
for some constant  and all . For this reason, one often
says that  can be described by  bits when the corresponding
 can be chosen to have  bits.

\begin{thm} \label{th:stackminusage}
The class of all  with
 cannot be learnt by a linear-time learner using
one additional stack.
\end{thm}

\proof
Assume that the linear-time learner  using one stack, 
in addition to the base tape,
is given.  In order to find languages not
learnt by , one focuses on  where the parameter  is
large; in addition one considers only  of the form  for some ;
this parameter  and  will play some role in the arguments below.
Note that all data-items in  have the length .
For , let  be a
string of length  such that the Kolmogorov
complexity  is at least  and the first  bits
of each  is the binary bit representation of . Furthermore,
assume that  is a constant so large that the Kolmogorov complexity
of the content of Tape  (which can be assumed to be always 
symbols long, since
all data have length , but which can use more than two alphabet symbols)
is at most  and that the stack can, in each round, pull or push
up to  symbols, where the stack alphabet has at most  symbols
(one can code up a larger alphabet in binary and choose the constant
 sufficiently large to absorb the extra amount of storage).
Hence, in each cycle, what the machine does depends on the content of
Tape  (worth  bits) and on the top  symbols of the stack
(worth  bits). Furthermore, assume that all words of length 
different from  have already been presented to the learner
and let  denote the content of Tape  and  denote the content
of the stack where  are the top  symbols (or less if  is the
empty word). Below one considers the behaviour/configuration of the learner
when it is presented with further inputs and one considers 
as the initial configuration of the learner for this purpose.
Below, the configuration of the learner, at any stage before reading the
next input, is denoted by , where the first argument
is the content of the tape and the second argument is the content
of the stack.
\sp
Intuitively, as the 's are complex, the learner needs to store
them on the stack when it receives them (otherwise, it would lose
information about which 's it has seen). This forces the stack
to grow larger and larger and prevents the learner from accessing
earlier stored data on the stack, thus making the earlier stored
information useless. This allows to show that some language 
is not learnable by .
\sp
Claim~\ref{cl:third} gives a permutation 
 of  such that
 does not touch any, but the top  symbols of , on
input . Claim~\ref{cl:fourth} uses
this claim to show that on some sequence  of 's,  reaches a
configuration , with  and
 being same as  except for the top  symbols removed,
where the learner never touches  on the input 
, and for any future input involving 's never
touches . This, then allows to claim in Claim~\ref{cl:fifth}
that the learner cannot learn some . 
Claims~\ref{cl:first} and~\ref{cl:second} are used in proving the above
claims.
Now the five claims about the configuration of  are proven formally.

\begin{clm} \label{cl:first}
There do not exist two distinct input sequences of words of length
, one containing an  
and one not containing , ending up in the same configuration
 where  has at most length  and  has not been
touched (that is, starting from configuration , the
initial portion 
 of  above 
was never at the top of the stack during the processing of any of the
two sequences).
\end{clm}

\noindent
Assume by way of contradiction that this claim fails, that is, 
there are two such sequences  and . Then one can
bring the learner into the
configuration  by either of the sequences and thereafter feed
the learner with the  with , and then with a string 
of length , different from , forever.
The convergence
behaviour of the learner, in both cases, is the same as the configuration
 is independent of the sequence  or 
by which the learner reached it;
from then onwards the learner receives, in both cases, the same data and 
conjectures the same hypotheses, as in both cases they are based on 
the same data, Tape 0 and stack.
In one case the learner has to learn  and in
the other case the learner has to learn ; thus
the learner can learn at most one of these two sets.
This completes the proof of the claim.

\begin{clm} \label{cl:second}
There is no input sequence 
and no splitting of  into  such that , after reading these
inputs, is in a configuration of the form  with
 and without having pulled and pushed back any
symbols of  and with the conditional Kolmogorov complexity satisfying
.
\end{clm}

\noindent
For a proof of the claim,
assume by way of contradiction that there is such an input sequence 
.
Then there is a partial-recursive
function  such that , given 
, finds a sequence
 such that  whenever
,  for all ,  having
the first  bits being the binary representation of  and
 pulling on these inputs the symbols belonging to  without
touching those of  and ending up in the configuration .
Note that one does not need to know  for this search, hence the
search depends only on the inputs given to  and returns an input
sequence such that its Kolmogorov complexity 
given
 is at 
most that of , that
is, below  (assuming that  is sufficiently large).
It follows that at least one  differs from ;
furthermore, no other  can be equal to 
by the rules that each  encodes  in the
first  bits and equals to  whenever .
However, this would contradict Claim~\ref{cl:first}.
This completes the proof of
Claim~\ref{cl:second}.

\begin{clm} \label{cl:third}
There is a permutation  of
 such that the splitting  with
either  or 
satisfies that  on input 
never touches the symbols in .
\end{clm}

\noindent
Let  be the given splitting of .
If  then  is empty and nothing needs to be proven;
thus assume that .
\sp
Now define 
and inductively for , split  at the
middle into two equal parts  and  
both of length  such that
. Note
that there is a unique permutation
of the form  of

such that

Note that  can be computed from .
Note that  
(for  and ,  being sufficiently large)
for the following reasons:
;
;
.
\sp
By induction one can see that
 for all 
whenever  are sufficiently large; note that  is the more
complex half of  and therefore has by induction hypothesis
at least the complexity 
minus some constant
which can be brought into the form  by
assuming that  is larger than the corresponding constant.
\sp
Furthermore, the values  can be computed from  and
, hence one can represent 
by  and  and . Hence

for any  with .
There are two cases for each  with :
\sp
First, . Then  and,
on input , the learner
can have pulled at most  symbols
from the stack; hence it has neither touched  nor the bottom
 symbols of .
\sp
Second, . Then 

Assuming that  and  are sufficiently large, one obtains

Thus, using Claim~\ref{cl:second} it follows that for all 
 
there are at least  symbols in the stack above  after reading
.
\sp
Hence, using above cases, one can conclude by induction on  that the
symbols in  are not touched while processing the input
.

\begin{clm} \label{cl:fourth}
Split  into  as in Claim~\ref{cl:third}. There is a sequence
of all , perhaps with repetitions, such that after reading this
sequence  is in a configuration , with , 
such that for all further
inputs from ,  does not touch the symbols on the
part of the stack denoted by .
\end{clm}

\noindent
Assuming that this sequence does not exist, one could use the sequence
given in Claim~\ref{cl:third} to remain above  in the stack until all
symbols are passed and then one could feed some sequence of  until
all but at most  symbols above  are used up; that is, one would
be in a configuration of the form  with  and
. Hence one can, given  and
 search a
tuple  such that each  starts with a binary
number of length  representing  and each  has  bits
and there is a sequence of inputs drawn from this tuple on which the
configuration of  with  changes to
 without
touching . The first tuple  of this type
found by searching has Kolmogorov complexity at most
 (obtained by coding the inputs , , ,
,  and
the routine for the search programme) which is less than 
, the lower bound on the 
Kolmogorov complexity of , for sufficiently large .
Therefore some  differs from  and therefore one can reach
the configuration  from  by
either having seen
 or not having seen . It follows from Claim~\ref{cl:first}
that this cannot occur, hence there is some minimal extension 
of  such that  and when reading any sequence of the
data  after having reached the
configuration , it will not touch  in the stack,
that is, all future activity depends only on  and .

\begin{clm} \label{cl:fifth}
 fails to learn some language of the form
 or .
\end{clm}

\noindent
Let  as in
Claim~\ref{cl:fourth}. One can now
show that there is a tuple  with
 and  extending the -bit representation of 
such that  when fed with some input-sequence taken from
 ends up in a
configuration of the form  without touching  and
this configuration is computed from ; as in
Claim~\ref{cl:fourth} one can argue that some .
Now one can feed all the  into  for the
configurations  and , respectively,
for both in the same way and in a loop repeated forever.
In both cases the learner 
either converges to the same index or does not converge,
but in one case the text which  has received is a text for 
and in the other case it is a text for .
Hence  fails to learn at least one of these two sets.\niceqed

\section{Relaxing the Timing Constraints}

\noindent
In this section, it is investigated how the learning power improves
if the severe restrictions on work Tape  or the computation
time are a bit relaxed.
The next result shows that, if one allows a bit more than just linear time,
then one can learn, using one work tape, all learnable
automatic classes of infinite languages. 
The result could even be transferred to families of arbitrary
r.e.\ sets as the simulated learner is an arbitrary recursive learner.
Intuitively, think of  in the following theorem as a slowly growing
function.

\begin{thm} \label{th:onetapesuperlinear}
Assume that  is an automatic family where every 
is infinite and  is a recursive learner which learns this family.
Furthermore, assume that  are recursive functions with the property
that  whenever  so  is some type of inverse
of . Then there is a learner  which learns the above family,
using only one additional work tape,
and satisfies the following constraint: 
if  is the length of the longest example seen so
far, then only the cells number  of Tape  can be non-empty and
the update time of  in the current cycle is .
\end{thm}

\proof
The main idea of the proof is that one constructs a learner which splits
Tape  into four 
tracks for archivation; the learner 
usually uses Track ; in irregular intervals, the learner returns from
its current position to the origin of Tape  and uses Track 
for archiving the examples which come up during this ``return to
the origin'' until it reaches the old data on Tracks  and .
When this happens, the old data found there consist only of words
up to length  (where  is sufficiently small compared to the
current word length ) and the learner can compress the data in
Tracks ,  and  into a list  (to be maintained on
Tape );  will contain, for each word  up to length  occurring
in the input, at most one copy (which gives a corresponding length bound
on the length of ). Once the compression is completed, the
learner returns to the forward mode using the one left over free
track for this purpose. The key idea is to ``space out'' the visits
to the origin such that, for  being the length of the longest datum
seen up to the end of the last visit,  is so much smaller
than the current  that ; this allows
all the data which was archived up to the end
of the previous visit to be compressed into a string of length
up to  and the update of this compressed memory can, in each
round, be done in time .
\sp
The description below gives a more precise description of the
update protocol. As the memory has only to be bounded by the length
of the longest datum seen so far plus some constant, one can assume
without loss of generality that  is at least .
\sp
On Tape , as memory, the learner  archives the convolution of variables
 with the following meaning.
\begin{iteMize}{}
\item  represents in unary the length of the longest word seen so far
and  is an old value of ; initially  and  are  (not ).
\item The variable  is, during the runtime,
only modified by appending symbols at the end and will in the limit
consist of a one-one text of all the words occurring in the language
to be learnt; the words on  are separated by a special character.
For example, 
would represent a beginning of a text consisting of , ,
 and . Furthermore,
each of the words in  would be of length at most .
\item The variable  is the current
configuration of a computation to determine 
(in unary);
this configuration is updated whenever the length and time constraints permit
and the next configuration is shorter than , until the
computation finishes.
\item The variable  is a configuration of , while processing the
initial part  of a text for the input language;
note that this configuration includes the memory of  and the portion
of  it has read.
In each cycle this configuration is updated by one more step of the 
computation,
unless the input  is currently exhausted (that is,  would like
to read a symbol which is not yet there) or the length of the configuration
becomes longer than . 
\item The variable  is the last completed conjecture of
 and updated whenever the configuration  of  contains a new
value to be output.
\end{iteMize}
In each cycle, the learner  would archive the current
input  on the work tape at a position near to the current one (that is, the
input position has to be reached in linear time) and  would furthermore
update the values of  on Tape  ( is
updated only
during some cycles, see below).
\sp
In order to be able to save all required information on the work Tape ,
the tape content is modeled as having four tracks. Usually,
only Track  is used for appending new information at the end of the
tape and Track  is used for making sure that computations of the
variables of Tape  meet the time-bound. 
Tracks  and  are used to store data during cycles when some
special operations are needed to transfer data from Tape  to the memory
 in Tape . Furthermore, initially . 
\sp
When  shows that the computation of
 has terminated, and the observed examples
are so long that  then the learner enters
the phase to do special operations (for next several cycles, as many as needed).
Note that eventually this happens for every value of , as the input
language is infinite (assuming it is from ).
In each cycle during this special phase, 
from its current position at the end of Tape  back
to the origin ,  will transfer/copy all stored words in 
Tape  of length at most
, which are not already in , to .
During this process, the older words stored in Tracks  and  may be 
erased (but not lost, as they have already been copied to , as each
of them are of length at most ). The new input words received during
this phase are copied in Tracks  and  (see below).
Note that a concatenation of
all words up to length  is at most  long (including
separating symbols) and hence  whenever
 consists only of copies of words up to length  appearing in the
language to be learnt and each such word appears at most once in .
\sp
Now, it is described how special operations are done in the special phases,
see also Figure~\ref{figuretrackexplain} for a rough summary of the
handling of old and new data in each cycle.
When going back on Tape ,  will do the following for all words 
 archived in Tracks , ,  starting from the current position up to 
 positions left of the current position (here 
one also considers  that might only partially
overlap with the cells in positions between the current position and
 to the left of the current position; recall that  is the current
input data to the learner): 
if  then  is compared
with all words in  and in the case that it does not coincide with
any archived word in ,  is appended at the end of
; note that
all words archived in the Tracks  and  have at most the length .
For each word , this operation needs time .
Note that  has at most length  and  at most length
, giving an overall bound of
 for the processing of each word .
Furthermore, the concatenation
of all these words archived one after another has length at most ;
so one can conclude that the whole operation needs time
 which is  as
. Furthermore, all  in Tracks 
and  overlapping with the space between the current position in Tape 
and the cell at position  left of the current position
before the start of the cycle are cleared away as these  all have
at most the length . After the clearance,  will be archived in
Track  (where a special symbol outside the alphabet used for the
archivation data is used to fill up blank spaces, if needed)
and the current position moves by  to the left.
This is done until the origin  is reached.
At this point, Track  is empty and
can be used to archive the incoming data in a similar
way while the Turing machine moves back from  to end of used
part of Tape .
When returning to the usual archivation
mode,  is updated to be the current value of  so that all words
archived in Tracks  and  are again having at most length .
From then onwards, one waits until so much data has been observed
such that the computation of  has terminated
and gives a value below (the new value of) .
\sp
One can see from this description 
that, when learning an infinite language,
eventually all words observed will be appended to  and
 will be simulated on the resulting one-one text of the language to
be learnt. Thus,  will eventually stabilise on some index , which will
be taken over as output when the corresponding computation has terminated
and  is larger than . This shows that  follows the simulated
learner  and therefore  learns the class to be learnt.\niceqed

\begin{figure}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|} \hline
Mode     & Usual & Backward Special & Forward Special \\ \hline
Old Data Before & ---   & In Tracks 1, 2 and 3 & In Tracks 1 and 2 \\ \hline
Old Data After  & ---   & Into Base Tape and Track 1 &
  Remains unchanged \\ \hline
New Data        & Into Track 1 & Into Track 2 & Into Track 3 \\ \hline
\end{tabular}
\end{center}
\caption{Handling of data at head position of Tape 1. In backward special
mode, old short data is recorded into 
the base tape and old long data remains in Track 1.}
\label{figuretrackexplain}
\end{figure}

\medskip
\noindent
Pitt's original result \cite{Pi89} on linear time learners did not measure the
time in the size of the largest example seen so far, but in the size
of the overall amount of examples seen so far. So the next two results
deal with the question of the additional learning power provided by
one work tape or one stack when the learner can use a Tape  of
length  and run in time linear in  where  is logarithm of the number
of data seen so far plus the length of the longest example seen so far;
hence  increases, though slowly, when a datum is presented multiply.
\sp
Note that in the proof of Theorem~\ref{th:onetapesuperlinear}, the main reason
to use infinite languages and strings of larger and larger length , was to be
able to transfer all stored data of length  onto .
This can also be done if instead of the length , the unbounded growing number
of examples seen so far is used as a parameter to allow the time needed to
do the transfer (in which case additionally, one can make  a fat text). 
For this, one needs to keep track of some earlier maximal length  and 
number of items  (including counting the multiple copies, in case they 
are there) so that  bounds the overall length of all
examples stored in Tracks  and .
When the number of examples seen so far, , is larger than the current
length of  plus , one
can then start going back, copying new data in Track  until one reaches
the point where the earlier data in Tracks  and  are stored.
At this point one moves all data in Tracks  and  to the end of 
which is stored in Tape  and then
starts moving forward on Tape  again,
copying new data into Track  until one reaches
the end of recorded part of all the tracks. At this point one can consider
Track  and Track  as old recorded data (earlier roles played by Tracks 
and ) and continue recording data in Track  up to the point when
the learner has seen enough examples so as to copy the data in Tracks 
and  to Tape . Continuing in this way, one can copy all data to 
in Tape  eventually and use the data in Tape  to simulate an automatic
learner on fat text by cycling through the examples archived in .
\sp
This allows to show the following result; its
proof is similar to Theorem~\ref{th:onetapesuperlinear} and
the details are omitted.

\begin{thm}
Let  be the logarithm of the number
of data seen so far plus the length of the longest example seen so far
and consider a learner which can store in Tape  information of length 
and can access one additional work tape, with 
update time in each cycle being linear in the corresponding .
Then such a learner can learn every 
learnable automatic family.
\end{thm}

\noindent
The previous and the next result compute the parameter 
of the update time and length of Tape  in the same way. While the previous
result showed that one additional work tape is sufficient for full
learning power
under the corresponding linear time model, the next result shows that one
additional stack is insufficient for full learning power.

\begin{thm}
Let  be the logarithm of the number
of data seen so far plus the length of the longest example seen so far
and consider a learner which can store in Tape  information of length 
and can access one additional stack, with
update time in each cycle being linear in the corresponding .
Then such a learner fails to learn the class 
of all set  where the indices 
range over .
\end{thm}

\proof
Assume by way of contradiction that such a learner  for 
exists. 
\sp
Intuitively, the idea of the proof is that if the learner gets
complex strings (relative to the position), 
then it has to store it in the stack. Thus, if it gets complex strings
in odd positions of the text, and even positions of the text
are filled with simple strings (to form a complete text for some 
target language), then
the learner has to push (codings of) the complex strings on the stack and 
is not able to look at these pushed symbols in later computation.
This allows to construct two such texts for different languages in
the class on which eventually the learner behaves in the same way
(see Claim~\ref{clm-last5}, and then the arguments after this claim);
thus the learner can learn at most one of these two sets.
Claims~\ref{clm-last1} to~\ref{clm-last4} are combinatorial claims 
based on Kolmogorov complexity, needed for proving Claim~\ref{clm-last5}.
Now the formal proof is given.
\sp
Let  denote the binary representation of  using  bits
where, for ,   is the downrounded logarithm of base
, that is, the
maximal integer  with .

\begin{clm}\label{clm-last1}
Suppose  and  are two finite sequences over  such that
,
,
and  has the same Tape  content and stack content 
after processing either  or . Then,  does not learn .
\end{clm}

\noindent
To show the above claim, let 
and .
Let  be a text for . Then,
 has the same convergence behaviour (that is it either diverges or converges
to the same conjecture) on texts  and , which are
texts for  and  respectively. Thus,  fails to learn at least
one of these languages and thus fails to learn .
This completes the proof of the claim.
\sp
For any recursive text  of any language
satisfying  for all ,
define  (using an oracle for the halting problem ) as follows:
Let 
and  be the string  of length  ending
with  which maximises .


\begin{clm}\label{clm-last2}
Let  be a recursive text satisfying  for all .
Then the following statements hold:
\begin{enumerate}[(a)]
\item[(a)]  are pairwise distinct for different ;
\item[(b)] For each , ;
\item[(c)] For each , .
\end{enumerate}
\end{clm}

\noindent
Part (a) follows by definition. Part (b), follows by definition
of  and the fact that .
For part (c) note that
there exists a string  of length , which
ends in , with Kolmogorov complexity
(given )
at least . As  is most complex such string
, part (c) follows.

\begin{clm}\label{clm-last3}
There exists a constant  such that the following holds for .
Suppose  is a recursive text satisfying  for all~.
Then,
.
\end{clm}

\noindent
To see that the claim holds,
note that for some constant , for all ,
,
,
see \cite{LV08}.
Thus, for all large enough ,
 (where the second last inequality follows from
Claim~\ref{clm-last2}(c)).

Let  and  denote the Tape  content and stack content of 
after processing .

\begin{clm}\label{clm-last4}
There exists a constant  such that, for  and  greater than ,
with , the following holds.
Suppose  is a recursive text
satisfying  for all~. Furthermore suppose
that  is computed by a program of Kolmogorov complexity less than 
. Then,


\end{clm}

\noindent
To show that the claim holds, 
suppose  is large enough as required for Claim~\ref{clm-last3}.
Suppose .
Note that by Claim~\ref{clm-last2}~(b), for  with ,
 does not belong to .
Now, given a program for  and ,
one can construct  such that,
for  with ,
\begin{enumerate}[(i)]
\item[(i)]  ends in ,
\item[(ii)] ,
\item[(iii)]  starting with Tape  content  and stack
content , on
input sequence ,
ends in  Tape  content being  and stack content .
\end{enumerate}
Note that  and  can be computed using
. Thus, the expression
\begin{quote}

\end{quote}
is bounded by

for some constant .
However, by Claim~\ref{clm-last3}, 
\begin{quote}
  .
\end{quote}
Thus, for large enough ,
there are some  satisfying ,
 and
. But, then
by Claim~\ref{clm-last1} and Claim~\ref{clm-last2}(a), 
 does not learn .
Hence, .
This proves Claim~\ref{clm-last4}.

\begin{clm}\label{clm-last5}
There exists a constant  such that
for large enough  and  the following holds. 
Suppose  is a recursive text 
satisfying  for all~, and 
is computed by a program of Kolmogorov complexity less than .
Then, while processing ,
\begin{enumerate}[(a)]
\item[(a)] the part of stack consisting of , except for the top 
 symbols, is never removed and
\item[(b)] .
\end{enumerate}
\end{clm}

\noindent
To show the claim, consider .
Now, , and 
thus .
Furthermore, if , for some longest prefix  of , then
the length of the deleted portion of  (that is ),
can be at most , for a constant ;
this can be coded using  bits.
Hence, by Claim~\ref{clm-last4}, for some constant , , for , for some constant .
Thus, for large enough , the length of  above 
is larger than what can be removed from the stack in one cycle.
It follows that, for some constant , 
the machine , on input ,
does not remove symbols from ,
except maybe for up to  symbols from the top.
This proves part (a). Part (b) follows, by using the length of 
above. This completes the proof of Claim~\ref{clm-last5}.
\sp
Using (a) and (b) of the above claim, it follows that
for large enough ,
for each recursive text  of some subset of  and
 having a program shorter than , 
 on  will, when processing subsequent data
from , never remove symbols from  except maybe for the
top  symbols.

Now, given a program for , ,
,
 and the topmost  symbols
of , one can compute a sequence  of length 
such that,
\begin{enumerate}[(i)]
\item[(i)] , for ,
\item[(ii)] for all ,  and
\item[(iii)] for all , 
    ends with  and is of length
   .  
\item[(iv)]  after processing
 has Tape  content  and
the top  symbols of the stack are same as the
top  symbols of .
\end{enumerate}
Now, , is at most 
, for some constant ,
as it was constructed from a description of  and a description of the top
 stack symbols and  symbols of .
On the other hand, 
 by Claim~\ref{clm-last3}.
\sp
Hence, fix a text  of the nonempty strings which repeats each
string infinitely often
and let  be large enough and let  be computed as above.
It follows that  and  
differ for an  with , that is, satisfy
. Let  and
. The strings  do not occur in . Let  and  be least such that  and .
Without loss of generality assume .
Let  and  be obtained from  as follows:
\begin{iteMize}{}
\item If  then 
 else ;
\item If  then
 else .
\end{iteMize}
Furthermore, the index of  has Kolmogorov complexity bounded by
 and the index of  has Kolmogorov complexity bounded
by  up to an additive constant. 
When considering  (and thus  and ) large 
enough, one can absorb this constant into  and 
respectively, and thus
Kolmogorov complexity of  and  are bounded by .
Now  coincides with  below , and
 coincides with  below .

In the various claims above (Claim~\ref{clm-last4}, Claim~\ref{clm-last5}), 
when using complexity of  being , 
only the initial portion of text  of length at most  was used.
Thus, it was enough to have the complexity of some text  coinciding
with  up to first  elements having a complexity below .
Hence,  and  satisfy the requirements needed in the claims.
\sp
Thus, one can conclude that, when  processes text ,
for large enough , the machine  (after having seen the first
 elements of ) does not remove more than 

symbols from the top of the stack .
Furthermore, if one now replaces the first  members of 
by the corresponding members of , then one gets that  on
this new text  has the same convergence behaviour as on
; however, one text is for  while the other one is for
, thus these are texts for two different languages and so 
does not learn at least one of these languages.\niceqed

\section{Conclusion}

\noindent
The starting point of this research is that automatic functions can be
characterised using one-tape Turing machines. More precisely, a function
is automatic iff it is computed by a position-faithful one-tape Turing
machine in linear time. This is the smallest reasonable linear time
complexity class and so the automatic functions turn out to sit at the
bottom of the corresponding hierarchy. An open problem is whether the
corresponding formalisation using alternating linear time position-faithful
one-tape Turing machines also characterises the automatic functions.
\sp
Automatic functions have been investigated in learning theory in order
to model resource-bounded learners. Due to Pitt's delaying trick \cite{Pi89},
unrestricted recursive learners can be bounded heavily in the time
that they use without
losing learning power. However,
automatic learners are not able to learn every learnable class, as their
ability to memorise data is insufficient. Therefore, one might ask
whether one can replace an automatic learner by a linear-time learner
working on a one-tape Turing machine with a tape of length bounded
by the longest datum seen so far plus some additional memory.
\sp
These additional memory devices are not restricted in length, 
though restricted in
the amount of access the learner has per cycle: In each cycle the learner
runs in time linear in the longest example seen so far, updates the
base tape and accesses the additional storage devices only to retrieve
or store a linear number of symbols. It is shown that two additional work
tapes, two additional stacks or one additional queue give full learning
power; furthermore, the learning power of one additional stack is properly
intermediate and the learning power of one additional work tape is better
than no additional work tape. It is an open problem whether there is a
difference in the learning power of one and two additional work tapes.
\sp
For some special cases and slightly superlinear computation time, it
was possible to show that one additional work tape is enough. The methods
of this proof do not generalise to the general case.

\begin{thebibliography}{10}

\bibitem{An80} Dana Angluin.
\newblock Inductive inference of formal languages from positive data.
\newblock {\em Information and Control} 45:117--135, 1980.

\bibitem{BB75} Lenore Blum and Manuel Blum.
\newblock Toward a mathematical theory of inductive inference.
\newblock {\em Information and Control}, 28:125--155, 1975.

\bibitem{Bl99} Achim Blumensath.
\newblock {\em Automatic structures}.
\newblock Diploma thesis, Department of Computer Science, RWTH Aachen, 1999.

\bibitem{BG00} Achim Blumensath and Erich Gr\"adel.
\newblock Automatic structures.
\newblock{\em 15th Annual IEEE Symposium on Logic in Computer
Science}, LICS 2000,
pages 51--62, 2000. 

\bibitem{Ca02} Cristian S.\ Calude.
\newblock {\em Information and Randomness: An Algorithmic Perspective}.
\newblock Second edition, Springer, Heidelberg, 2002.

\bibitem{CJLOSS11} John Case, Sanjay Jain, Trong Dao Le, Yuh Shin Ong, Pavel
  Semukhin and Frank Stephan.
\newblock Automatic learning of subclasses of pattern languages.
\newblock In {\em Information and Comptation}, Volume 218, pages 17--35, 2012.

\bibitem{CJS12} John Case, Sanjay Jain and Frank Stephan.
\newblock Automatic Functions, Linear Time and Learning.
\newblock {\em Computability in Europe: How 
the world computes -- Turing Centenary Coference
and Eighth Conference on Computability in Europe},
(CiE) 2012. Springer LNCS: 7318: 96--106, 2012.

\bibitem{CKS81} Ashok K.\ Chandra, Dexter C.\ Kozen and Larry J.\ Stockmeyer.
\newblock Alternation.
\newblock {\em Journal of the ACM} 28:114--133, 1981.

\bibitem{DH10} Rodney G.\ Downey and Denis R.\ Hirschfeldt.
\newblock {\em Algorithmic Randomness and Complexity.}
\newblock Springer, Heidelberg, 2010.

\bibitem{FKS95} R{\={u}}si{\c{n}}{\v{s}} Freivalds, Efim Kinber and 
Carl~H.~Smith.
\newblock On the impact of forgetting on learning machines.
\newblock {\em Journal of the ACM}, 42:1146--1168, 1995.

\bibitem{Go67} E.~Mark Gold.
\newblock Language identification in the limit.
\newblock\emph{Information and Control} 10:447--474, 1967.

\bibitem{Ha68} Juris Hartmanis.
\newblock Computational complexity of one-tape Turing machine computations.
\newblock {\em Journal of the Association of Computing Machinery}
  15:411--418, 1968.

\bibitem{He65} Fred C.\ Hennie.
\newblock Crossing sequences and off-line Turing machine computations.
\newblock Sixth Annual Symposium on
  {\em Switching Circuit Theory and Logical Design}, pages 168--172, 1965.

\bibitem{He65b} Fred C.\ Hennie.
\newblock One-tape, off-line {Turing} machine computations.
\newblock {\em Information and Control} 8:553--578, 1965.

\bibitem{Ho76} Bernard R. Hodgson.
\newblock {\em Th\'eories d\'ecidables par automate fini}.
\newblock Ph.D.\ thesis, D\'epartement de
math\'ematiques et de statistique, Universit\'e de Montr\'eal, 1976.

\bibitem{Ho83} Bernard R. Hodgson.
\newblock D\'ecidabilit\'e par automate fini.
\newblock {\em Annales des sciences math\'ematiques du Qu\'ebec},
  7(1):39--57, 1983.

\bibitem{HMU01} John E.\ Hopcroft, Rajeev Motwani and Jeffrey D.\ Ullman.
\newblock {\em Introduction to Automata Theory, Languages, and Computation.}
\newblock Second Edition, Addison-Wesley, 2001.

\bibitem{JLS09} Sanjay Jain, Qinglong Luo and Frank Stephan.
\newblock Learnability of automatic classes.
\newblock {\em Language and Automata Theory and Applications}, 4th International
          Conference, LATA 2010.
         Proceedings.
          Springer LNCS 6031:321-332, 2010.

\bibitem{JOPS10} Sanjay Jain, Yuh Shin Ong, Shi Pu and Frank Stephan.
\newblock On automatic families.
\newblock {\em Proceedings of the eleventh Asian Logic Conference}
  in honour of Professor Chong Chitat on his sixtieth birthday,
\newblock pages 94--113, World Scientific, 2012.

\bibitem{JORS99} Sanjay Jain, Daniel N.\ Osherson, James S.\ Royer
and Arun Sharma.
\newblock\emph{Systems That Learn}.
\newblock MIT Press, Second Edition, 1999.

\bibitem{KN95} Bakhadyr Khoussainov and Anil Nerode.
\newblock Automatic presentations of structures.
\newblock {\em Logical and Computational Complexity}, (International
          Workshop LCC 1994). Springer LNCS 960:367--392, 1995.

\bibitem{KS95} Efim Kinber and Frank Stephan.
\newblock Language learning from texts: mind changes, limited memory and
          monotonicity.
\newblock {\em Information and Computation}, 123:224--241, 1995.

\bibitem{LZZ08} Steffen Lange, Thomas Zeugmann and Sandra Zilles.
\newblock Learning indexed families of recursive languages from positive
  data: a survey.
\newblock {\em Theoretical Computer Science}, 397:194--232, 2008.

\bibitem{LV08} Ming Li and Paul Vit\'anyi.
\newblock {\em An Introduction to Kolmogorov Complexity and its Applications.}
\newblock Third Edition. \newblock Springer, 2008.

\bibitem{Ni09} Andr\'e Nies.
\newblock {\em Computability and Randomness}.
\newblock Volume 51 of {\em Oxford Logic Guides}.
\newblock Oxford University Press, Oxford, 2009.

\bibitem{Od99} Piergiorgio Odifreddi.
\newblock {\em Classical Recursion Theory}, Volume II. 
\newblock Studies in Logic and the Foundations of Mathematics, 143.
\newblock Elsevier, 1999.

\bibitem{OSW86} Daniel Osherson, Michael Stob and Scott Weinstein.
\newblock {\em Systems That Learn, An Introduction to Learning Theory
          for Cognitive and Computer Scientists}.
\newblock Bradford --- The MIT Press, Cambridge, Massachusetts, 1986.

\bibitem{Pi89} Lenny Pitt.
\newblock Inductive inference, {DFA}s, and computational complexity.
\newblock {\em Analogical and Inductive Inference, Proceedings of the Second
               International Workshop}, AII 1989.
\newblock Springer LNAI 397:18--44, 1989.

\bibitem{Ru08} Sasha Rubin.
\newblock Automata presenting structures: a survey of the finite string case.
\newblock {\em The Bulletin of Symbolic Logic}, 14:169--209, 2008.

\bibitem{Tr64} Boris A.\ Trakhtenbrot.
\newblock Turing computations with logarithmic delay.
\newblock {\em Algebra i Logika}, 3:33-48, 1964.


\end{thebibliography}

\end{document}
