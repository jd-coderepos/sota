Since this work aims to learn the task of omnidirectional dense depth estimation, and given that - to the authors' knowledge - no other similar work exists, we first review non-learning based methods for geometric scene understanding based on \360 images. We then examine learning based approaches for spherical content and, finally, present recent monocular dense depth estimation methods.

\subsection{Geometric understanding on \360 images}
\label{sec:360geometry}
Similar to pinhole projection model cameras, the same multi-view geometry \cite{hartley2000multiple} principles apply to \360 images. By observing the scene from multiple viewpoints and establishing correspondences between them, the underlying geometrical structure can be estimated. For \360 cameras the conventional binocular or multi-view stereo \cite{furukawa2015multi} problem is reformulated to binocular or multi-view spherical stereo \cite{li2008binocular} respectively, by taking into account the different projection model and after defining the disparity as angular displacements. By estimating the disparity (i.e. depth), complete scenes can be 3D reconstructed from multiple \cite{kim2013scenereconstruction,li2005spherical} or even just two \cite{ma20153d,pathak2016dense} spherical viewpoints. However, all these approaches require multiple \360 images to estimate the scene's geometry. Recently it was shown that \360 videos acquired with a moving camera can be used to 3D reconstruct a scene's geometry via SfM \cite{huang20176} and enable 6 DOF viewing in VR headsets. 

There are also approaches that require only a single image to understand indoors scenes and estimate their layout. PanoContext \cite{zhang2014panocontext}, generates a 3D room layout hypothesis given an indoor \360 image in equirectangular format. With the estimations being bounding boxes, the inferred geometry is only a coarse approximation of the scene. Similar in spirit, the work of Yang et al. \cite{yang2016efficient} generates complete room layouts from panoramic indoor images by combining superpixel information, vanishing points estimation and a geometric context prior under a Manhattan world assumption. However, focusing on room layout estimation, it is unable to recover finer details and structures of the scene. Another similar approach \cite{xu2017pano2cad} addresses the problem of geometric scene understanding from another perspective. Under a maximum a posteriori estimation it unifies semantic, pose and location cues to generate CAD models of the observed scenes. Finally, in \cite{kim2016room} a spherical stereo pair is used to estimate  both the room layout but also object and material attributes. After retrieving the scene's depth by stereo matching and subsequently calculating the normals, the equirectangular image is projected to the faces of a cube that are then fed to a CNN whose \textbf{object} predictions are fused into the \360 image to finally reconstruct the 3D layout. 

\subsection{Learning for \360 images}
\label{sec:360learning}
One of the first approaches to estimate distances purely from omnidirectional input \cite{plagemann2010nonparametric} under a machine learning setting utilized Gaussian processes. Instead of estimating the distance of each pixel, a range value per image column was predicted to  drive robotic navigation. Nowadays, with the establishment of CNNs, there are two straightforward ways to apply current CNN processing pipelines to spherical input. Either directly on a projected (typically equirectangular) image, or by projecting the spherical content to the faces of a cube (cubemap) and running the CNN predictions on them, which are then merged by back-projecting them to the spherical domain. The latter approach was selected by an artistic style transfer work \cite{ruder2017artistic}, where each face was re-styled separately and then the cubemap was re-mapped back to the equirectangular domain. Likewise, in SalNet360 \cite{monroy2017salnet360}, saliency predictions on the cube's faces are refined using their spherical coordinates and then merged back to \360. The former approach, applying a CNN directly to the equirectangular image, was opted for in \cite{zhang2017learning} to increase the dynamic range of outdoor panoramas.

More recently, new techniques for applying CNNs to omnidirectional input were presented. Given the difficulty to model the projection's distortion directly in typical CNNs as well as achieve invariance to the viewpoint's rotation, the alternative pursued by \cite{frossard2017graph} is based on graph-based deep learning. Specifically they model distortion directly into the graph's structure and apply it to a classification task. A novel approach taken in \cite{su2017learning} is to learn appropriate convolution weights for equirectangular projected spherical images by transferring them from an existing network trained on traditional 2D images. This conversion from the 2D to the \360 domain is accomplished by enforcing consistency between the predictions of the 2D projected views and those in the \360 image. Moreover, recent work on convolutions \cite{jeon2017active,dai2017deformable} that in addition to learning their weights also learn their shape, are very well suited for learning the distortion model of spherical images, even though they have only been applied to fisheye lenses up to now \cite{deng2018restricted}. Finally, very recently, Spherical CNNs were proposed in \cite{cohen2017convolutional,cohen2018spherical} that are based in a rotation-equivariant definition of spherical cross-correlation. However these were only demonstrated in classification and single variable regression problems. In addition, they are also applied in the spectral domain while we formulate our network design for the spatial image domain.
   
\subsection{Monocular depth estimation}
\label{sec:monocular}
Depth estimation from monocular input has attracted lots of interest lately. While there are some impressive non learning based approaches \cite{ranftl2016dense,liu2014discrete,karsch2016depth}, they come with their limitations, namely reliance on optical flow and relevance of the training dataset. Still, most recent research has focused on machine learning to address the ill-posed depth estimation problem. Initially, the work of Eigen et al. \cite{eigen2014depth} trained a CNN in a coarse-to-fine scheme using direct depth supervision from RGB-D images. In a subsequent continuation of their work \cite{eigen2015predicting}, they trained a multi-task network that among predicting semantic labels and normals, also estimated a scene's depth. Their results showed that jointly learning the tasks achieved higher performance due to their complementarity. In a recent similar work \cite{ren-cvpr2018}, a multi-task network that among other modalities also estimated depth, was trained using synthetic data and a domain adaptation loss based on adversarial learning, to increase its robustness when running on real scenes. Laina et al. \cite{laina2016deeper} designed a directly supervised fully convolutional residual network (FCRN) with  novel up-projection blocks that achieved impressive results for indoor scenes and was also used in a SLAM pipeline \cite{tateno2017cnnslam}. 

Another body of work focused on applying Conditional Random Fields (CRFs) to the depth estimation problem. Initially, the output of a deep network was refined using a hierarchical CRF \cite{li2015depth}, with Liu et al. \cite{liu2016learning} further exploring the interplay between CNNs and CRFs for depth estimation in their work. Recently, multi-scale CRFs were used and trained in an end-to-end manner along with the CNN \cite{xu2017multi}. Dense depth estimation has also been addressed as a classification problem. Since perfect regression is usually impossible, dense probabilities were estimated in \cite{chakrabarti2016depth} and then optimized to estimate the final depth map. Similarly, in \cite{li2017monocular} and \cite{cao2017estimating} depth values were discretized in bins and densely classified, to be afterwards refined either via a hierarchical fusion scheme or through the use of a CRF respectively. Taking a step further, a regression-classification cascaded network was proposed in \cite{fu2017compromise} where a low spatial resolution depth map was regressed and then refined by a classification branch.

The concurrent works of Garg et al. \cite{garg2016unsupervised} and Godard et al. \cite{monodepth17} showed that unsupervised learning of the depth estimation task is possible. This is accomplished by an intermediate task, view synthesis, and allowed training by only using stereo pair input with known baselines. In a similar fashion, using view synthesis as the main supervisory signal, learning to estimate depth was  also achieved by training with pure video sequences in a completely unsupervised manner \cite{zhou2017unsupervised,wang2017learning,mahjourian2018unsupervised,yang2017unsupervised,yin2018geonet}. Another novel unsupervised depth estimation method relies on aperture supervision \cite{srinivasan2017aperture} by simply acquiring training data in various focus levels. Finally, in \cite{chen2016single} it was shown that a CNN can be trained to estimate depth from monocular input with only relative depth annotations.