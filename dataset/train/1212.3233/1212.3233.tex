





In this section we derive our approximation algorithms for $\JRPD$, showing an
approximation ratio of $e/(e-1)\approx 1.58$, which we then improve to $1.574$.
Both algorithms are based on randomized LP-rounding.



\myparagraph{The LP relaxation.}
For the rest of this section, fix an arbitrary instance
$\calI = (\COST, \costVector,\demands)$ of {\JRPD}.
Let finite set $\universe\subset\rational$ contain the release times and deadlines.
Here is the standard LP relaxation of the problem:

\begin{alignat}{3}
	\textrm{minimize}\quad\quad  \costfn{\mbfx} \;&=\;  
        \textstyle \sum_{t\in\universe} {(\COST\,x_{t} + \sum_{\rho=1}^m{\costof{\rho}\, x^\rho_t})}
											\hspace{-1.3in}&&
	\notag \\
	\textrm{subject to}\quad\quad\quad\quad 
			\textstyle x_t, x^\rho_t \;&\ge 0\;
											&	&\textrm{for all}\; t \in \universe, \rho \in \{1, \ldots, m\}
	\notag
        \\
        \textstyle	x_t  \;&\geq\;  x^\rho_t 
	 										&	&\textrm{for all}\; t \in \universe, \rho \in \{1, \ldots, m\}
	\label{eqn: density bound}
	\\
			 \sum_{t\in \universe\cap[r,d]} {x^\rho_t} \;&\geq 1\;
											&	&\textrm{for all}\; (\rho,r,d) \in \demands.
	\label{eqn: satisfy bound}
\end{alignat}

\myparagraph{The statistic $\statistic \distribution$.}
Let $\distribution$ be a probability distribution on $[0,1]$. As we are about to show,
the approximation ratio of algorithm $\round_\distribution$ (defined below)
and the integrality gap of the LP
are at most $1/\statistic \distribution$, where $\statistic \distribution$ 
is defined by the following so-called {\em tally game} (following \cite{jrp-deadlines-nonner}).
To begin the game, fix any {\em threshold} $\threshold \ge 0$,
then draw a sequence of independent samples $\sample 1,\sample 2,\ldots,\sample h$ 
from $\distribution$, stopping when their sum exceeds $\threshold$,
that is when $\sample 1+\sample 2+\ldots+\sample {h} > \threshold$.
Call $\threshold- (\sample 1+\sample 2+\ldots+\sample {h-1})$ the {\em waste}.
Note that, since the waste is less than $\sample h$, it is in $[0,1)$.
Let $\EWaste \distribution \threshold$ denote the expectation of the waste.
Abusing notation, let $\Exp[\distribution]$ denote the expected value of a~single sample drawn from~$\distribution$.
Then $\statistic \distribution$ is defined by

\begin{equation*}
\statistic \distribution = \min\Big\{\, \Exp[\distribution]\,,\, 
		1-\sup_{\threshold \ge 0} \EWaste \distribution \threshold \,\Big\}.
\end{equation*}

Note that the distribution $\distribution$ that chooses $\half$ with probability 1 has 
$\statistic \distribution = \half$.
The challenge is to simultaneously increase $\E[p]$ and reduce the maximum expected waste.



\myparagraph{A generic randomized-rounding algorithm.}
The upper bound of 1.574 relies on a randomized-rounding algorithm, $\round_\distribution$.
The algorithm is parameterized by an arbitrary probability distribution $\distribution$ on $[0,1]$
and gives a $(1/\statistic \distribution)$-approximation:

\begin{lemma}\label{lemma: round}
For any distribution $\distribution$ on $[0,1]$ and fractional LP solution $\mbfx$,
if $\statistic p > 0$,
then with probability 1, Algorithm~$\round_\distribution(\COST,\costVector,\demands,\mbfx)$ 
returns a feasible schedule.
The expected cost of the schedule is at most $\costfn{\mbfx}/\statistic \distribution$.
\end{lemma}

The main ideas underlying $\round_\distribution$ and its analysis are from \cite{jrp-deadlines-nonner};
the presentation here (\lref[Section]{sec: details}) addresses some technical subtleties.
Subsequent sections (with a minor exception) use \lref[Lemma]{lemma: round} as a black box;
they can be read independently of the proof in \lref[Section]{sec: details}.



\subsection{The details of \texorpdfstring{$\round_\distribution$}{Round(p)} and 
proof of \texorpdfstring{Lemma~\ref{lemma: round}}{Lemma 1}}\label{sec: details}

Fix an arbitrary optimal fractional solution $\mbfx$ of the LP relaxation for instance $\calI$.
As previously discussed, without loss of generality we can assume that
the given universe $\universe$ of release times and deadlines is $[\maxDeadline]$,
where $\maxDeadline$ ($\le 2n$) is the maximum deadline.
We focus on producing a ``rounded'' schedule $S$ for $\calI$
with expected cost at most $1/\statistic p$ times
$\costfn \mbfx = \sum_{j=1}^\maxDeadline \big(\COST\, x_j + \sum_\rho \costof{\rho}\, x_j^\rho\big)$.


\newcommand{\rmo}{r\mbox{--}1}

\myparagraph{Extend to continuous time.}
To start, we recast the problem of rounding $\mbfx$ as a continuous-time problem.
Extend the universe $\universe$ of times
from the discrete set $\universe = [\maxDeadline]$
to the continuous interval $\overline\universe = (0,\maxDeadline]$
and relax each demand period $[r,d]$, 
replacing it with the half-open period $(\rmo,d]$.
Let $\overline\calI$ denote the modified instance.
To find a schedule $S$ for the given instance $\calI$,
we will find a~schedule $\overline S$ for $\overline\calI$,
then take $S = \{ (\lceil t \rceil, R)\suchthat (t,R) \in \overline S\}$.
$S$ clearly has the cost not larger than that of $\overline S$
and is also feasible (because the release times and deadline are integers).

For the algorithm, reinterpret the fractional solution $\mbfx$ 
as a {\em continuous-time} solution $\overline\mbfx$ over universe $\overline\universe$:
as $t\in\overline\universe$ ranges continuously from $0$ to $\maxDeadline$,
the continuous-time solution 
$\overline\mbfx$ ships continuously at the {\em shipping rate} $\overline x_t = x_{\lceil t\rceil}$
and has each retailer $\rho$ join at his {\em take rate} 
$\overline x^\rho_t = x^\rho_{\lceil t\rceil}$.
The example at the top of \lref[Figure]{fig:bar_chart} illustrates $\overline \mbfx$ over time.



\begin{figure}[t]
  \centering
    \includegraphics[height=1in]{bar_chart.pdf}
    \vspace*{-10pt}
    \caption{
      The continuous solution $\overline\mbfx$:
      the universe is $\overline\universe=(0,\maxDeadline]$.
      At each time $t$ in $\overline\universe$,
      the solution ships at rate $\overline x_t= x_{\lceil t\rceil}$
      while each retailer $\rho$ takes at rate $\overline x^\rho_t=x^\rho_{\lceil t \rceil}$.
      During any demand period $(r-1,d]$, the retailer's cumulative take
      is at least 1.
    }
    \label{fig:bar_chart}
\end{figure}



At each time $t$ in $\overline\universe$,
define the {\em total shipped} up to time $t$ to be
$\MEAS{t} = \int_{0}^ t \overline x_t \,dt = \int_0^t x_{\lceil t \rceil}\,dt$.
Define $\rho$'s {\em take} up to time $t$ to be
$\AREA{t} = \int_{0}^ t \overline x^\rho_t \,dt = \int_0^t x^\rho_{\lceil t \rceil}\,dt$.
Over any interval $(t,t']$, define the amount shipped to be $\MEAS{t'}-\MEAS{t}$.
Likewise define $\rho$'s take to be $\AREA{t'}-\AREA{t}$.



\myparagraph{The algorithm.}
$\round_\distribution$ draws samples $(\sample 1, \sample 2, \ldots, \sample \GOTstop)$
i.i.d.~from the distribution $p$, 
stopping when the sum of the samples first exceeds $\MEAS{\maxDeadline}-1$.
It creates orders at the times $\mathbf t = (t_1,t_2,\ldots,t_{\GOTstop})$
such that, for each $i\in[\GOTstop]$,
the continuous-time solution $\overline\mbfx$ 
ships $s_i$ units in interval $(t_{i-1},t_i]$ (interpreting $t_0$ as 0).
By the choice of the number of samples ${\GOTstop}$,
$\overline\mbfx$ ships strictly less than 1 unit in $(t_{\GOTstop},\maxDeadline]$.

After $\round_\distribution$ chooses $\mathbf t$, 
for each retailer $\rho$ independently,
it has $\rho$ join a minimum-size subset of the orders that satisfies $\rho$'s demands.
(It computes this optimal subset using the standard earliest-deadline-first algorithm.)
This determines the schedule $\overline S$ for the continuous-time instance $\overline\calI$.
To get the schedule $S$ for the original instance $\calI$,
the algorithm shifts each order time $t_i$ to its ceiling $\lceil t_i\rceil$.
The algorithm is shown in \lref[Figure]{fig:round}.

We remark that, in practice, modifying the algorithm to round up 
the times as soon as they are chosen might yield lower-cost solutions for some instances.  
That is, take each $t_i$ to be the minimum integer such that the amount shipped over 
interval $(t_{i-1}, t_i]$ is at least $\sample i$ (interpreting $t_0$ as $0$).  
Stop when the amount shipped over $(t_i, U]$ is less than 1.



\newcommand{\StateLong}[1]
{\State\parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1 \strut}}

\begin{figure}[t]
\centering
\begin{minipage}{\textwidth}
\hrulefill
\begin{algorithmic}[1] \setlength{\parindent}{0in}\setlength{\parskip}{1pt}\setlength{\itemsep}{1pt}

    \StateLong{
      Draw samples $(\sample 1,\sample 2, \sample 3, \ldots, \sample \GOTstop)$
      i.i.d.~from $\distribution$, stopping when the sum of the samples
      first exceeds $\MEAS{\maxDeadline}-1$.
      Schedule orders at times in $\mathbf t = (t_1,t_2,\ldots,t_{\GOTstop})$
      such that, for each $i\in[\GOTstop]$,
      each $t_i$ is the minimum such that
      $\overline\mbfx$ ships $\sample i$ units
      in the interval $(t_{i-1},t_i]$
      (interpreting $t_0$ as~0).
      (By the choice of $\GOTstop$,
      $\overline\mbfx$ ships strictly less than 1 unit
      in interval $(t_{\GOTstop},\maxDeadline]$.)} \label{line:t}

    \For{{\bf each} $\rho\in[m]$} 

    \StateLong{
      Use the earliest-deadline-first algorithm to choose a minimum-size subset
      of the orders for~$\rho$ to join to satisfy his demands.
      More explicitly:
    }\label{line:join}

    \While{retailer $\rho$ has any not-yet-satisfied demand $(\rho,r,d)$}
    \label{line:while}

    \State Let $d^*$ be the earliest deadline of such a demand.

    \State
    Have $\rho$ join the order at time $T = \max \{ t_i \in \mathbf t \suchthat t_i \le d^*\}$.
    \label{line:T}

    \EndWhile \label{line:end}

    \EndFor

    \State   Let $\overline S$ denote the resulting schedule for $\overline \calI$.  
    Return $S = \big\{ (\lceil t \rceil, R)\suchthat (t,R)\in \overline S\big\}$.

\end{algorithmic}
\hrulefill
\hrulefill
\caption{ $\round_\distribution(\COST, \costVector, \demands, \mbfx)$
  randomly rounds the continuous-time fractional solution $\overline\mbfx$.
}\label{fig:round}
\end{minipage}
\end{figure}




\begin{proof}(of \lref[Lemma]{lemma: round})
  {\em Correctness and feasibility.}
  We claim first that the number $\GOTstop$ of order times 
  in line~\ref{line:t} has finite expectation; in other words,
  	with probability 1, $\GOTstop$ is finite --- line~\ref{line:t} finishes.
This follows from standard calculation:
    $\GOTstop$
    is the number of samples taken before the sum of
    the samples exceeds $\MEAS{\maxDeadline}-1$.
    Since $\E[\distribution] \ge \statistic p > 0$,
    there exists $\epsilon>0$ such that $\Pr[s \ge \epsilon] \ge \epsilon$.
    Thus, the expected number of samples needed 
    to increase the sum by $\epsilon$ is at most $1/\epsilon$.
    By linearity of expectation,
    the expected number of samples needed to increase 
    the sum by $\MEAS{\maxDeadline}$ is at most
    $\MEAS{\maxDeadline}/\epsilon^2$.
    Thus, $\E[\GOTstop] \le \MEAS{\maxDeadline}/\epsilon^2<\infty$.

  In the argument below (for cost estimation)
	we show  that each iteration of the inner loop on line~\ref{line:while}
  satisfies some not-yet-satisfied demand $(\rho,r,d)$.
  Thus, with probability 1, $\round_\distribution$ terminates.
  By inspection, it does not terminate until all demands are satisfied.
  Thus, with probability 1, $\round_\distribution$ 
  returns a feasible solution.



\emmedparagraph{Cost of the schedule.} 
We use the following basic properties of $\overline\mbfx$.
\begin{enumerate}
\item Over any interval $(t,t'] \subseteq \overline\universe$,
    each retailer $\rho$'s take is at most the amount shipped.
    This follows directly from LP constraint~\eqref{eqn: density bound}.
\item For each demand $(\rho, r, d)$,
    retailer $\rho$'s take over the demand period $(r-1,d]$ is at least 1.
	This holds because the take equals $\sum_{t=r}^d x_t$, 
    which is at least 1 by LP constraint~\eqref{eqn:  satisfy bound}.
    Consequently, the amount shipped over $(r-1,d]$ is also at least 1.
\end{enumerate}

  The given fractional solution $\mbfx$ 
  has warehouse cost $\COST\, \MEAS{\maxDeadline}$
  and retailer cost $\sum_{\rho} \costof{\rho}\, \AREA{\maxDeadline}$.
  (Recall that $\MEAS{\maxDeadline}$ is the amount $\overline\mbfx$ ships up to time $\maxDeadline$ 
  while $\AREA{\maxDeadline}$ is $\rho$'s take up to time $\maxDeadline$.)

  The algorithm's schedule $S$ has warehouse cost $\COST\, \GOTstop$
  and retailer cost $\sum_{\rho} \costof{\rho}\, \LOTstop$,
  where $\GOTstop$ is the number of orders placed in line~\ref{line:t}
  and $\LOTstop$ is the number of those orders joined by $\rho$
  in lines~\ref{line:join}--\ref{line:end}.

  We show $\E[\GOTstop] \le \MEAS{\maxDeadline}/\statistic p$
  and $\E[\LOTstop] \le \AREA{\maxDeadline}/\statistic p$.
  By linearity of expectation, these bounds imply that
  the schedule's expected cost is a most $1/\statistic p$ times the cost of $\mbfx$,
  proving the lemma.

  \smallskip

  First analyze $\E[\GOTstop]$, the expected number of samples until the sum
  of the samples exceeds \mbox{$\MEAS{\maxDeadline}-1$}. Clearly $\GOTstop$ is a
  stopping time.\footnote {That is, for any $i\in\nat$, the event $\GOTstop =
  i$ is determined by the first $i$ samples.} As noted previously, $\GOTstop$
  has finite expectation. So, by Wald's Lemma (see \lref[Appendix]{sec: walds
  lemma}), the expectation of the sum of the first $\GOTstop$ samples
  is not smaller than the expectation
  of $I$ times the expectation of each sample: $\E[\sum_{i=1}^\GOTstop \sample
  i] \geq \E[\GOTstop]\, \E[p]$. The sum is at most $\MEAS{\maxDeadline}$,
  because the sum of the first $\GOTstop-1$ samples is less than
  $\MEAS{\maxDeadline}-1$ and the last sample is at most 1. Thus,
  $\MEAS{\maxDeadline} \ge \E[\GOTstop]\, \E[p]$. Rearranging, $\E[\GOTstop]
  \le \MEAS{\maxDeadline}/\E[p] \le \MEAS{\maxDeadline}/\statistic p$, as
  desired.

  \smallskip

  Next analyze $\E[\LOTstop]$.
  Fix a retailer $\rho\in[m]$.
  Focus on the inner loop, lines~\ref{line:join}--\ref{line:end}.
  For each iteration $j\in[\LOTstop]$,  let $d^*_j$
  and $T_j$ denote, respectively, the value of $d^*$ and $T$
  in iteration $j$.
  The order that $\rho$ joins at time $T_j$
  indeed satisfies that iteration's unmet demand $(\rho, r, d^*_j)$,
  because $\rho$'s take over the period $(r-1,d^*_j]$ is at least 1,
  so the amount shipped over the period is at least 1,
  so, by the choice of $\mathbf t$ in line~\ref{line:t},
  the period has to contain some order time $t_i$,
  and $T_j \in [t_i, d^*_j]$.
  Then, by a standard induction on $j$, after $\rho$ joins the order at time $T_j$,
  all of $\rho$'s demands whose demand periods overlap $(0,T_j]$ are satisfied.
  Hence $\rho$'s order times are strictly increasing:
  $T_1 < T_2 < \cdots < T_{\LOTstop}$.

  Consider any non-final iteration $j$ of the loop.
  Define $\psi_j = (\sample 1, \sample 2, \ldots, \sample {k})$,
  the {\em state at the end of iteration $j$},
  to be the first $k$ samples drawn from $\distribution$,
  where $k=k_j$ is the number of samples
  needed to determine $T_j$ in iteration $j$.
  Explicitly, $k$ is determined by the condition

  \begin{equation}\label{eqn:kjt}
    t_{k-1}\,\le\, d^*_j\, <\, t_{k},
  \end{equation}

\noindent
  which implies $T_j = t_{k-1}$.
  (The sample $\sample {k}$ is included in $\state j$ because,
  for $T_j$ to be the maximum order time less than or equal to $d^*_j$,
  the order time {\em following} $T_j$ must exceed $d^*_j$.)
  When we look at the related warehouse shipments, 
  $t_{k-1} \leq d^*_j < t_k$ implies $\MEAS{t_{k-1}} \leq\, \MEAS{d^*_j} \, 
  \leq\, \MEAS{t_k}$. 
  The last inequality is in fact strict, 
  because the algorithm chooses $t_k$ minimally.
  Since $\overline\mbfx$ ships each $\sample i$ over the interval $(t_{i-1},t_i]$,
  the last relation implies that

  \begin{equation}\label{eqn:kj}
  \sum_{i=1}^{k-1} \sample i\,\le\, \MEAS{d^*_j}\, <\, \sum_{i=1}^{k} \sample i.
  \end{equation}
	
  Define $\rho$'s {\em take during iteration $j$},
  denoted $X_j$, to be $\rho$'s take over the interval $(T_{j-1},T_j]$
  (interpreting $T_0$ as 0).
  To finish the proof,  consider the sum $\sum_{j=1}^{\LOTstop-1} X_j$,
  that is, $\rho$'s take up to the start of the last iteration.

  The sum's upper index~$\LOTstop - 1$ is a stopping time.
  (Indeed, $\state j$ determines 
  which of $\rho$'s demands remain unsatisfied at the start of iteration $j+1$.
  Iteration $j+1$ will be the final iteration $\LOTstop$
  iff those unsatisfied demands can be satisfied by a single order.
  Thus, $\state j$ determines whether $\LOTstop - 1 = j$.)
  Clearly $\LOTstop-1$ has finite expectation.
  (Indeed, $\LOTstop$ is at most the number of demands.)

  We claim that expectation of each term $X_j$ in the sum,
  given the state at the start of iteration~$j$, is at least $\statistic p$:

  \begin{equation}\label{eqn:claim}
    \E[X_{j}~|~\state {j-1}]~\ge~ \statistic p.
  \end{equation}

  Before we prove Claim~\eqref{eqn:claim},
  observe that it implies the desired bound on $\LOTstop$, as follows.
  The upper index $\LOTstop-1$ of the sum $\sum_{j=1}^{\LOTstop-1} X_j$
  is a stopping time with finite expectation,
  and the conditional expectation of each term is at least $\statistic p$,
  so, by Wald's Lemma (see \lref[Appendix]{sec: walds lemma}),
  the expectation of the sum is at least $\E[\LOTstop-1] \statistic p$.
  On the other hand,  the value of the sum never exceeds $\AREA{\maxDeadline}-1$.
  (Indeed,   at the start of the last iteration $j=\LOTstop$, 
  some demand $(\rho, r, d)$ remains unsatisfied,
  and that demand, which has total take at least 1,
  does not overlap $(0,T_{j-1}]$,
  so $\rho$'s take up to time $T_{j-1}$ can be at most 1 less than the total take.)
  Thus, $\E[\LOTstop -1]\statistic p \le \AREA{\maxDeadline}-1$.
  Since $\statistic p \le 1$, this implies the desired bound $\E[\LOTstop] \le \AREA{\maxDeadline}/\statistic p$.

  To finish, we prove Claim~\eqref{eqn:claim}.
  Fix any state $\state {j-1}$ and  let $k=k_{j-1}=|\state {j-1}|$.
  Consider iteration $j$.
  Note that $\state {j-1}$ determines both $T_{j-1}$ and $d^*_j$.
  Call the samples in $\state{j}$ but not in $\state {j-1}$ {\em newly exposed}.
  Crucially, $\state {j-1}$ does not condition the newly exposed samples.
  Let random variable~$h = k_j-k_{j-1}$ be the number of newly exposed samples.
  By Condition~\eqref{eqn:kj}, $h$ is the index such that
  \(
  \sum_{i=1}^{k+h-1} \sample i
  \,\le\, \MEAS{d^*_j}
  \,<\, 
  \sum_{i=1}^{k+h} \sample i.
  \)

  Define $z = \MEAS{d^*_j} - \sum_{i=1}^{k} \sample i$. Then $z\ge 0$ with probability~$1$ and $z$
  is determined by $\state {j-1}$.
  Using $s'_1, s'_2, \ldots, s'_h$ to denote the newly exposed samples (i.e., $s'_\ell = s_{k+\ell}$),
  the condition on $h$ above is equivalent to

  \[
  s'_1 + s'_2 + \cdots + s'_{h-1} \,\le\, z \,<\,   s'_1 + s'_2 + \cdots + s'_{h-1} + s'_h.
  \]

\noindent
  That is, the iteration exposes new samples just until their sum exceeds $z$.
  Upon consideration, this process corresponds to a play of the tally game with threshold $z$,
  in the definition of the statistic $\statistic p$.
  (See \lref[Figure]{fig:tally}.)
\begin{figure}[t]
    \centering
    \includegraphics[height=.93in]{rounding_lemma.pdf}
\caption{The increase in $\rho$'s take in each iteration corresponds to a play of the tally game.}
    \label{fig:tally}
  \end{figure}
Recall from the definition of $\statistic p$
  that the {\em waste} is $w = z-(s'_1+s'_2+\cdots+s'_{h-1})$.

  By inspection, using that $\sum_{i=1}^{k+h-1} \sample i = \MEAS{T_j}$,
  the waste $w$ in this setting equals $\MEAS{d^*_j} - \MEAS{T_j}$,
  so $\rho$'s take $X_{j}$ during the iteration,  that is, $\AREA{T_j} - \AREA{T_{j-1}}$, equals

  \begin{align*}
  [\AREA{d^*_j} - \AREA{T_{j-1}}]
  -
  [\AREA{d^*_j} - \AREA{T_j}]
  ~&\ge~
  1
  -
  [\AREA{d^*_j} - \AREA{T_j}]
		\\
  ~&\ge~
  1 - [\MEAS{d^*_j} - \MEAS{T_j}]
  ~=~ 
  1-w.
  \end{align*}

  The first inequality holds because,
  as observed previously, $\rho$'s take over interval $(T_{j-1},d^*_j]$ is at least 1.
  The next inequality holds because
  $\rho$'s take over $(T_j, d^*_j]$ is at most the amount shipped.

  Recall that, by definition of $\statistic p$,
  the expectation of $(1-w)$ is at least $\statistic \distribution$.
  Thus, the inequality above implies Claim~\eqref{eqn:claim} --- that
  the conditional expectation of each $X_{j}$ is at least $\statistic p$.
\end{proof}

The next utility lemma
says that, in analyzing the expected waste in the tally game,
it is enough to consider thresholds $\threshold$ in $[0,1)$.

\begin{lemma}\label{lemma: 01}
  For any distribution $p$ on $[0,1]$,
  $\sup_{\threshold\ge 0} \EWaste \distribution \threshold  ~=~\sup_{\threshold\in[0,1)} \EWaste \distribution \threshold $.
\end{lemma}

\begin{proof}
  Play the tally game with any threshold $\threshold \geq 1$.  
  Consider the first prefix sum $\sample 1+\sample 2+\cdots+\sample h$
  of the samples, such that the ``slack'' 
  $\threshold - (\sample 1 + \sample 2 + \cdots + \sample h)$ is less than 1.
  Let random variable~$\threshold'$ be this slack.  Note that $\threshold'\in[0,1)$. 
  For any value $u\in[0,1)$,
  the expected waste conditioned on the event ``$\threshold'=u$'' is $\EWaste \distribution u$,
  which is at most $\sup_{y\in[0,1)} \EWaste \distribution y$.
  Thus, for \emph{any} threshold $z\ge 1$, $\EWaste \distribution \threshold $ 
  is at most $\sup_{y\in[0,1)} \EWaste \distribution {y}$. 
\end{proof}




\subsection{Upper bound of \texorpdfstring{$e/(e-1) \approx 1.582$}{e/(e-1) = 1.582}}

Consider the specific probability distribution $\distribution$ on $[0,1]$
with probability density function $\distribution(y) = 1/y$ for $y\in [1/e,1]$
and $\distribution(y) = 0$ elsewhere.

\begin{lemma}\label{lemma: upper bound 1}
  For this distribution $p$, $\statistic \distribution ~\ge~ (e-1)/e ~=~ 1-1/e$.
\end{lemma}

\begin{proof}
  By \lref[Lemma]{lemma: 01},
  $\statistic \distribution$ is the minimum of $\Exp[\distribution]$ 
  and $1 - \sup_{\threshold \in [0,1)} \EWaste \distribution \threshold $.

  By direct calculation,
  $\Exp[\distribution] = \int_{1/e}^1 y\,\distribution(y)\, dy ~=~ \int_{1/e}^1 1\, dy = 1-1/e$.
  Now
  consider playing the tally game with threshold $\threshold$.
  If $\threshold \in[0, 1/e]$, then (since the waste is at most $\threshold$)
  trivially $\EWaste \distribution \threshold  \le \threshold \le 1/e$.
  So, consider any $\threshold\in [1/e,1]$.  Let $\sample 1$ be just the first sample.
  The waste is $\threshold$ if $\sample 1 > \threshold$ and otherwise is at most $\threshold-\sample 1$.
	Thus,  the expected waste is 

	\begin{align*}
		\EWaste \distribution \threshold \;
		\leq \; & \Pr[\sample 1 > \threshold] \cdot\threshold
		 		~+~ \Pr[\sample 1 \le \threshold]\cdot \Exp[\threshold - \sample 1 ~|~ \sample 1 \le \threshold]  \\
		= \; & \threshold ~-~ \Pr[\sample 1 \le \threshold]\cdot \Exp[\sample 1 ~|~ \sample 1 \le \threshold] \\
		= \; & \threshold - \int_{1/e}^\threshold y\, p(y)\, dy
	  ~=~
	  \threshold - \int_{1/e}^\threshold \, dy
	  ~=~ \threshold - (\threshold-1/e)
	  ~=~ 1/e.
	\end{align*}

Since  both $\Exp[\distribution]$ and
$1-\sup_{\threshold \in [0,1)}\EWaste{\distribution}{\threshold}$ are at least $1-1/e$, the lemma follows.
\end{proof}

From \lref[Lemma]{lemma: upper bound 1}~and~\lref[Lemma]{lemma: round},
the approximation ratio of our
algorithm $\round_\distribution$, with distribution $\distribution$ defined above,
is at most $1/\statistic{\distribution} = \frac{e}{e-1} \approx 1.582$.



\subsection{Upper bound of \texorpdfstring{$1.574$}{1.574}}

On close inspection of the proof of \lref[Lemma]{lemma: upper bound 1}, 
it is not hard to see that the estimate for the waste in that proof is likely not tight. 
The reason is that the proof estimates the waste based on just the first sample, 
while, for the distribution being analyzed,
there is non-zero probability that {\em two} samples
are generated before reaching the threshold, further reducing the waste.
To improve the upper bound,
we adjust the probability distribution (and the analysis) accordingly.

Define a probability distribution $\distribution$ on $[0,1]$, having a point mass at $1$, as follows.
Fix $\theta = 0.36455$ (slightly less than $1/e$).
Over the half-open interval $[0,1)$, the probability density function is

\[
\distribution(y) ~=~
\begin{cases}
  0                  
  & \text{for } y \in [0,\theta) 
  \\
  1/y
  & \text{for } y \in [\theta,2\theta) 
  \\
  \frac{1-\ln((y-\theta)/\theta)}{y}
  & \text{for } y \in [2\theta, 1).
\end{cases}
\]

Define the probability of choosing $1$ to be $1-\int_0^1 \distribution(y) \,dy \approx 0.0821824$.
Note that $\distribution(y)\ge 0$ for $y\in [2\theta,1)$ since $\ln((1-\theta)/\theta)\approx 0.55567$,
so $p$ is indeed a probability distribution on $[0,1]$.



\begin{lemma}\label{lemma: upper bound 2}
  The statistic $\statistic \distribution$ for this $\distribution$ is at least $0.63533 > 1/1.574$.
\end{lemma}

\begin{proof}
Recall that $\statistic \distribution = \min \{\Exp[\distribution], 1-\sup_{\threshold\in[0,1)} \EWaste \distribution \threshold\}$.
By calculation, the probability measure~$\mu$ induced by $\distribution$ has
$\mu[1] \approx 0.0821824$ and

\begin{equation*}  
  \mu[0,v) ~=~
  \begin{cases}
    0                 
    & \text{for } v \in [0,\theta) 
    \\
    \ln( v/\theta)
    & \text{for } v \in [\theta,2\theta) 
    \\
    \ln (v/\theta)   ~-~
    		\int_{2\theta}^{v} \frac{\ln((y-\theta)/\theta)}{y} \,dy
    & \text{for } v \in [2\theta, 1).
  \end{cases}
\end{equation*}

\noindent
  The following calculation shows $\Exp[\distribution]> 0.63533$:

  \begin{eqnarray*}
    \Exp[\distribution]&=&\mu[1]+\int_{\theta}^{1}y\distribution(y)dy\\
    &=&\mu[1]+\int_{\theta}^{1}dy-\int_{2\theta}^{1}\ln{\left((y-\theta)/\theta\right)}dy\\
    &=&\mu[1]+(1-\theta)- \left((y-\theta)\ln{\left((y-\theta)/\theta\right)}-y \right) \vertbar_{2\theta}^1\\
    &=&\mu[1]+2-3\theta-  (1-\theta)\ln{\left((1-\theta)/\theta\right)}\\
    &>& 0.0821+2-3\cdot 0.36455-(1-0.36455)\cdot 0.5557\\
    &>&0.63533.
  \end{eqnarray*}

  To finish, we show $\sup_{\threshold\ge 0} \EWaste \distribution \threshold = \theta \le 1 - 0.63533$.
  
  By \lref[Lemma]{lemma: 01}, assume that $\threshold\in [0,1)$.
  In the tally game defining $\EWaste \distribution \threshold$,
  let $s_1$ be the first random sample drawn from $\distribution$.
  If $s_1 > \threshold$, then the waste equals $\threshold$.
  Otherwise, the process continues recursively with $\threshold$ replaced by $z' = \threshold-s_1$.
  This gives the recurrence

\begin{equation*}
\EWaste \distribution \threshold  ~=~ \threshold\,\mu[\threshold,1] 
							+ \int_0^\threshold \EWaste \distribution {\threshold-y} \distribution(y)\,dy
  						~=~ \threshold\,\mu[\threshold,1] 
							+ \int_\theta ^\threshold \EWaste \distribution {\threshold-y}\, \distribution(y)\,dy.
\end{equation*}

\noindent
Break the analysis into three cases, depending on the value of $z$.

\medskip
\noindent
\mycase{1} $\threshold\in [0,\theta)$. In this case $\mu[\threshold,1] = 1$, 
  so $\EWaste \distribution \threshold = \threshold \le \theta$.

\medskip
\noindent
\mycase{2} $\threshold\in [\theta,2\theta)$.
  For $y\in [\theta, \threshold]$, we have $\threshold-y\in[0,\theta]$,
  so, by Case~1, $\EWaste \distribution {\threshold-y} = \threshold-y$.
  Using the recurrence, 

\begin{align*}
	\EWaste \distribution \threshold ~&=~ 
   \threshold \mu[\threshold,1]+\int_{\theta}^\threshold (\threshold-y)\distribution(y)\,dy
\\
   ~&=~ \threshold\left(1-\int_{\theta}^\threshold \distribution(y)dy\right)+\int_{\theta}^\threshold (\threshold-y)\distribution(y)dy
   ~=~ \threshold -\threshold+\theta=\theta.
\end{align*}
 
\medskip
\noindent
\mycase{3} $\threshold\in [2\theta,1)$.
 For $y\in [\theta, \threshold]$, we have $\threshold-y\in [0,2\theta]$,
 so, by Cases~1~and~2 and the recurrence,

\begin{align*}
\EWaste \distribution \threshold
   &~=~ \threshold \mu[\threshold,1]
				+\int_{\theta}^{\threshold-\theta} \theta \distribution(y)dy
				+\int_{\threshold-\theta}^{\threshold} (\threshold-y)\distribution(y)dy
\\
  &~=~ \threshold-\threshold\int_{\theta}^{\threshold-\theta}\distribution(y)dy	
				+\int_{\theta}^{\threshold-\theta} \theta \distribution(y)dy
				-\int_{\threshold-\theta}^{\threshold} y\distribution(y)dy
\\
   &~=~\threshold-(\threshold-\theta)\ln{\frac{\threshold-\theta}{\theta}}
				-\int_{\threshold-\theta}^{\threshold} y\distribution(y)dy
\\
   &~=~ \threshold-(\threshold-\theta)\ln{\frac{\threshold-\theta}{\theta}}
				-\int_{\threshold-\theta}^{\threshold} dy
				+\int_{2\theta}^{\threshold}  \ln{\left((y-\theta)/\theta\right)} dy
\\
   &~=~ (\threshold-\theta)\left(1-\ln{\frac{\threshold-\theta}{\theta}}\right)
			+\int_{2\theta}^{\threshold}  \ln{\left((y-\theta)/\theta\right)} dy
\\
 &~=~(\threshold-\theta)\left(1-\ln{\frac{\threshold-\theta}{\theta}}\right)
				+\left(y-\theta)\cdot (\ln{\left((y-\theta)/\theta\right)}-1\right)\vertbar_{2\theta}^\threshold
\\
   &~=~(\threshold-\theta) 
        \left(
          1-\ln{\frac{\threshold-\theta}{\theta}}
          +\ln{\frac{\threshold-\theta}{\theta}}-1
        \right)
        + \theta 
        =\theta.
\end{align*}

Thus, in all cases, $\EWaste{\distribution}{\threshold}\le\theta$,
completing the proof.
\end{proof}



\begin{theorem}\label{thm: upper bounds}
  $\JRPD$ has a randomized polynomial-time $1.574$-approximation algorithm,
  and the integrality gap of the LP relaxation is at most $1.574$.
\end{theorem}

\begin{proof}
  By \lref[Lemma]{lemma: upper bound 2},
  for any fractional solution $\mbfx$, Algorithm~$\round_p$
  (using the probability distribution $\distribution$ from that lemma)
  returns a feasible schedule of expected cost at most $1.574$ 
  times $\costfn{\mbfx}$.  

  To see that the schedule can be computed in polynomial time,
  note first that the LP relaxation can be solved in polynomial time.
  The optimal solution $\mbfx$ is minimal,
  so each $x_t$ is at most 1, which implies that $\MEAS{\maxDeadline} = \sum_{t} x_t$ 
  is at most the number of demands, $n$.
  In Algorithm~$\round_p$,
  each sample from the distribution $\distribution$ from \lref[Lemma]{lemma: upper bound 2}
  can be drawn in polynomial time.
  Each sample is~$\Omega(1)$, and the sum of the samples 
  is at most $\MEAS{\maxDeadline} \le n$,
  so the number of samples is $O(n)$.
  In the inner loop of the algorithm
  (starting at line~\ref{line:join}),
  for each retailer~$\rho$, the subset of orders joined
  can be computed in time $O(n)$, by amortization, so the total time for
  this step is $O(mn)$, where $m$ is the number of retailers.
\end{proof}


