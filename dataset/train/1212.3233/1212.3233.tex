





In this section we derive our approximation algorithms for , showing an
approximation ratio of , which we then improve to .
Both algorithms are based on randomized LP-rounding.



\myparagraph{The LP relaxation.}
For the rest of this section, fix an arbitrary instance
 of {\JRPD}.
Let finite set  contain the release times and deadlines.
Here is the standard LP relaxation of the problem:



\myparagraph{The statistic .}
Let  be a probability distribution on . As we are about to show,
the approximation ratio of algorithm  (defined below)
and the integrality gap of the LP
are at most , where  
is defined by the following so-called {\em tally game} (following \cite{jrp-deadlines-nonner}).
To begin the game, fix any {\em threshold} ,
then draw a sequence of independent samples  
from , stopping when their sum exceeds ,
that is when .
Call  the {\em waste}.
Note that, since the waste is less than , it is in .
Let  denote the expectation of the waste.
Abusing notation, let  denote the expected value of a~single sample drawn from~.
Then  is defined by



Note that the distribution  that chooses  with probability 1 has 
.
The challenge is to simultaneously increase  and reduce the maximum expected waste.



\myparagraph{A generic randomized-rounding algorithm.}
The upper bound of 1.574 relies on a randomized-rounding algorithm, .
The algorithm is parameterized by an arbitrary probability distribution  on 
and gives a -approximation:

\begin{lemma}\label{lemma: round}
For any distribution  on  and fractional LP solution ,
if ,
then with probability 1, Algorithm~ 
returns a feasible schedule.
The expected cost of the schedule is at most .
\end{lemma}

The main ideas underlying  and its analysis are from \cite{jrp-deadlines-nonner};
the presentation here (\lref[Section]{sec: details}) addresses some technical subtleties.
Subsequent sections (with a minor exception) use \lref[Lemma]{lemma: round} as a black box;
they can be read independently of the proof in \lref[Section]{sec: details}.



\subsection{The details of \texorpdfstring{}{Round(p)} and 
proof of \texorpdfstring{Lemma~\ref{lemma: round}}{Lemma 1}}\label{sec: details}

Fix an arbitrary optimal fractional solution  of the LP relaxation for instance .
As previously discussed, without loss of generality we can assume that
the given universe  of release times and deadlines is ,
where  () is the maximum deadline.
We focus on producing a ``rounded'' schedule  for 
with expected cost at most  times
.


\newcommand{\rmo}{r\mbox{--}1}

\myparagraph{Extend to continuous time.}
To start, we recast the problem of rounding  as a continuous-time problem.
Extend the universe  of times
from the discrete set 
to the continuous interval 
and relax each demand period , 
replacing it with the half-open period .
Let  denote the modified instance.
To find a schedule  for the given instance ,
we will find a~schedule  for ,
then take .
 clearly has the cost not larger than that of 
and is also feasible (because the release times and deadline are integers).

For the algorithm, reinterpret the fractional solution  
as a {\em continuous-time} solution  over universe :
as  ranges continuously from  to ,
the continuous-time solution 
 ships continuously at the {\em shipping rate} 
and has each retailer  join at his {\em take rate} 
.
The example at the top of \lref[Figure]{fig:bar_chart} illustrates  over time.



\begin{figure}[t]
  \centering
    \includegraphics[height=1in]{bar_chart.pdf}
    \vspace*{-10pt}
    \caption{
      The continuous solution :
      the universe is .
      At each time  in ,
      the solution ships at rate 
      while each retailer  takes at rate .
      During any demand period , the retailer's cumulative take
      is at least 1.
    }
    \label{fig:bar_chart}
\end{figure}



At each time  in ,
define the {\em total shipped} up to time  to be
.
Define 's {\em take} up to time  to be
.
Over any interval , define the amount shipped to be .
Likewise define 's take to be .



\myparagraph{The algorithm.}
 draws samples 
i.i.d.~from the distribution , 
stopping when the sum of the samples first exceeds .
It creates orders at the times 
such that, for each ,
the continuous-time solution  
ships  units in interval  (interpreting  as 0).
By the choice of the number of samples ,
 ships strictly less than 1 unit in .

After  chooses , 
for each retailer  independently,
it has  join a minimum-size subset of the orders that satisfies 's demands.
(It computes this optimal subset using the standard earliest-deadline-first algorithm.)
This determines the schedule  for the continuous-time instance .
To get the schedule  for the original instance ,
the algorithm shifts each order time  to its ceiling .
The algorithm is shown in \lref[Figure]{fig:round}.

We remark that, in practice, modifying the algorithm to round up 
the times as soon as they are chosen might yield lower-cost solutions for some instances.  
That is, take each  to be the minimum integer such that the amount shipped over 
interval  is at least  (interpreting  as ).  
Stop when the amount shipped over  is less than 1.



\newcommand{\StateLong}[1]
{\State\parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1 \strut}}

\begin{figure}[t]
\centering
\begin{minipage}{\textwidth}
\hrulefill
\begin{algorithmic}[1] \setlength{\parindent}{0in}\setlength{\parskip}{1pt}\setlength{\itemsep}{1pt}

    \StateLong{
      Draw samples 
      i.i.d.~from , stopping when the sum of the samples
      first exceeds .
      Schedule orders at times in 
      such that, for each ,
      each  is the minimum such that
       ships  units
      in the interval 
      (interpreting  as~0).
      (By the choice of ,
       ships strictly less than 1 unit
      in interval .)} \label{line:t}

    \For{{\bf each} } 

    \StateLong{
      Use the earliest-deadline-first algorithm to choose a minimum-size subset
      of the orders for~ to join to satisfy his demands.
      More explicitly:
    }\label{line:join}

    \While{retailer  has any not-yet-satisfied demand }
    \label{line:while}

    \State Let  be the earliest deadline of such a demand.

    \State
    Have  join the order at time .
    \label{line:T}

    \EndWhile \label{line:end}

    \EndFor

    \State   Let  denote the resulting schedule for .  
    Return .

\end{algorithmic}
\hrulefill
\hrulefill
\caption{ 
  randomly rounds the continuous-time fractional solution .
}\label{fig:round}
\end{minipage}
\end{figure}




\begin{proof}(of \lref[Lemma]{lemma: round})
  {\em Correctness and feasibility.}
  We claim first that the number  of order times 
  in line~\ref{line:t} has finite expectation; in other words,
  	with probability 1,  is finite --- line~\ref{line:t} finishes.
This follows from standard calculation:
    
    is the number of samples taken before the sum of
    the samples exceeds .
    Since ,
    there exists  such that .
    Thus, the expected number of samples needed 
    to increase the sum by  is at most .
    By linearity of expectation,
    the expected number of samples needed to increase 
    the sum by  is at most
    .
    Thus, .

  In the argument below (for cost estimation)
	we show  that each iteration of the inner loop on line~\ref{line:while}
  satisfies some not-yet-satisfied demand .
  Thus, with probability 1,  terminates.
  By inspection, it does not terminate until all demands are satisfied.
  Thus, with probability 1,  
  returns a feasible solution.



\emmedparagraph{Cost of the schedule.} 
We use the following basic properties of .
\begin{enumerate}
\item Over any interval ,
    each retailer 's take is at most the amount shipped.
    This follows directly from LP constraint~\eqref{eqn: density bound}.
\item For each demand ,
    retailer 's take over the demand period  is at least 1.
	This holds because the take equals , 
    which is at least 1 by LP constraint~\eqref{eqn:  satisfy bound}.
    Consequently, the amount shipped over  is also at least 1.
\end{enumerate}

  The given fractional solution  
  has warehouse cost 
  and retailer cost .
  (Recall that  is the amount  ships up to time  
  while  is 's take up to time .)

  The algorithm's schedule  has warehouse cost 
  and retailer cost ,
  where  is the number of orders placed in line~\ref{line:t}
  and  is the number of those orders joined by 
  in lines~\ref{line:join}--\ref{line:end}.

  We show 
  and .
  By linearity of expectation, these bounds imply that
  the schedule's expected cost is a most  times the cost of ,
  proving the lemma.

  \smallskip

  First analyze , the expected number of samples until the sum
  of the samples exceeds \mbox{}. Clearly  is a
  stopping time.\footnote {That is, for any , the event  is determined by the first  samples.} As noted previously, 
  has finite expectation. So, by Wald's Lemma (see \lref[Appendix]{sec: walds
  lemma}), the expectation of the sum of the first  samples
  is not smaller than the expectation
  of  times the expectation of each sample: . The sum is at most ,
  because the sum of the first  samples is less than
   and the last sample is at most 1. Thus,
  . Rearranging, , as
  desired.

  \smallskip

  Next analyze .
  Fix a retailer .
  Focus on the inner loop, lines~\ref{line:join}--\ref{line:end}.
  For each iteration ,  let 
  and  denote, respectively, the value of  and 
  in iteration .
  The order that  joins at time 
  indeed satisfies that iteration's unmet demand ,
  because 's take over the period  is at least 1,
  so the amount shipped over the period is at least 1,
  so, by the choice of  in line~\ref{line:t},
  the period has to contain some order time ,
  and .
  Then, by a standard induction on , after  joins the order at time ,
  all of 's demands whose demand periods overlap  are satisfied.
  Hence 's order times are strictly increasing:
  .

  Consider any non-final iteration  of the loop.
  Define ,
  the {\em state at the end of iteration },
  to be the first  samples drawn from ,
  where  is the number of samples
  needed to determine  in iteration .
  Explicitly,  is determined by the condition

  

\noindent
  which implies .
  (The sample  is included in  because,
  for  to be the maximum order time less than or equal to ,
  the order time {\em following}  must exceed .)
  When we look at the related warehouse shipments, 
   implies . 
  The last inequality is in fact strict, 
  because the algorithm chooses  minimally.
  Since  ships each  over the interval ,
  the last relation implies that

  
	
  Define 's {\em take during iteration },
  denoted , to be 's take over the interval 
  (interpreting  as 0).
  To finish the proof,  consider the sum ,
  that is, 's take up to the start of the last iteration.

  The sum's upper index~ is a stopping time.
  (Indeed,  determines 
  which of 's demands remain unsatisfied at the start of iteration .
  Iteration  will be the final iteration 
  iff those unsatisfied demands can be satisfied by a single order.
  Thus,  determines whether .)
  Clearly  has finite expectation.
  (Indeed,  is at most the number of demands.)

  We claim that expectation of each term  in the sum,
  given the state at the start of iteration~, is at least :

  

  Before we prove Claim~\eqref{eqn:claim},
  observe that it implies the desired bound on , as follows.
  The upper index  of the sum 
  is a stopping time with finite expectation,
  and the conditional expectation of each term is at least ,
  so, by Wald's Lemma (see \lref[Appendix]{sec: walds lemma}),
  the expectation of the sum is at least .
  On the other hand,  the value of the sum never exceeds .
  (Indeed,   at the start of the last iteration , 
  some demand  remains unsatisfied,
  and that demand, which has total take at least 1,
  does not overlap ,
  so 's take up to time  can be at most 1 less than the total take.)
  Thus, .
  Since , this implies the desired bound .

  To finish, we prove Claim~\eqref{eqn:claim}.
  Fix any state  and  let .
  Consider iteration .
  Note that  determines both  and .
  Call the samples in  but not in  {\em newly exposed}.
  Crucially,  does not condition the newly exposed samples.
  Let random variable~ be the number of newly exposed samples.
  By Condition~\eqref{eqn:kj},  is the index such that
  

  Define . Then  with probability~ and 
  is determined by .
  Using  to denote the newly exposed samples (i.e., ),
  the condition on  above is equivalent to

  

\noindent
  That is, the iteration exposes new samples just until their sum exceeds .
  Upon consideration, this process corresponds to a play of the tally game with threshold ,
  in the definition of the statistic .
  (See \lref[Figure]{fig:tally}.)
\begin{figure}[t]
    \centering
    \includegraphics[height=.93in]{rounding_lemma.pdf}
\caption{The increase in 's take in each iteration corresponds to a play of the tally game.}
    \label{fig:tally}
  \end{figure}
Recall from the definition of 
  that the {\em waste} is .

  By inspection, using that ,
  the waste  in this setting equals ,
  so 's take  during the iteration,  that is, , equals

  

  The first inequality holds because,
  as observed previously, 's take over interval  is at least 1.
  The next inequality holds because
  's take over  is at most the amount shipped.

  Recall that, by definition of ,
  the expectation of  is at least .
  Thus, the inequality above implies Claim~\eqref{eqn:claim} --- that
  the conditional expectation of each  is at least .
\end{proof}

The next utility lemma
says that, in analyzing the expected waste in the tally game,
it is enough to consider thresholds  in .

\begin{lemma}\label{lemma: 01}
  For any distribution  on ,
  .
\end{lemma}

\begin{proof}
  Play the tally game with any threshold .  
  Consider the first prefix sum 
  of the samples, such that the ``slack'' 
   is less than 1.
  Let random variable~ be this slack.  Note that . 
  For any value ,
  the expected waste conditioned on the event ``'' is ,
  which is at most .
  Thus, for \emph{any} threshold ,  
  is at most . 
\end{proof}




\subsection{Upper bound of \texorpdfstring{}{e/(e-1) = 1.582}}

Consider the specific probability distribution  on 
with probability density function  for 
and  elsewhere.

\begin{lemma}\label{lemma: upper bound 1}
  For this distribution , .
\end{lemma}

\begin{proof}
  By \lref[Lemma]{lemma: 01},
   is the minimum of  
  and .

  By direct calculation,
  .
  Now
  consider playing the tally game with threshold .
  If , then (since the waste is at most )
  trivially .
  So, consider any .  Let  be just the first sample.
  The waste is  if  and otherwise is at most .
	Thus,  the expected waste is 

	

Since  both  and
 are at least , the lemma follows.
\end{proof}

From \lref[Lemma]{lemma: upper bound 1}~and~\lref[Lemma]{lemma: round},
the approximation ratio of our
algorithm , with distribution  defined above,
is at most .



\subsection{Upper bound of \texorpdfstring{}{1.574}}

On close inspection of the proof of \lref[Lemma]{lemma: upper bound 1}, 
it is not hard to see that the estimate for the waste in that proof is likely not tight. 
The reason is that the proof estimates the waste based on just the first sample, 
while, for the distribution being analyzed,
there is non-zero probability that {\em two} samples
are generated before reaching the threshold, further reducing the waste.
To improve the upper bound,
we adjust the probability distribution (and the analysis) accordingly.

Define a probability distribution  on , having a point mass at , as follows.
Fix  (slightly less than ).
Over the half-open interval , the probability density function is



Define the probability of choosing  to be .
Note that  for  since ,
so  is indeed a probability distribution on .



\begin{lemma}\label{lemma: upper bound 2}
  The statistic  for this  is at least .
\end{lemma}

\begin{proof}
Recall that .
By calculation, the probability measure~ induced by  has
 and



\noindent
  The following calculation shows :

  

  To finish, we show .
  
  By \lref[Lemma]{lemma: 01}, assume that .
  In the tally game defining ,
  let  be the first random sample drawn from .
  If , then the waste equals .
  Otherwise, the process continues recursively with  replaced by .
  This gives the recurrence



\noindent
Break the analysis into three cases, depending on the value of .

\medskip
\noindent
\mycase{1} . In this case , 
  so .

\medskip
\noindent
\mycase{2} .
  For , we have ,
  so, by Case~1, .
  Using the recurrence, 


 
\medskip
\noindent
\mycase{3} .
 For , we have ,
 so, by Cases~1~and~2 and the recurrence,



Thus, in all cases, ,
completing the proof.
\end{proof}



\begin{theorem}\label{thm: upper bounds}
   has a randomized polynomial-time -approximation algorithm,
  and the integrality gap of the LP relaxation is at most .
\end{theorem}

\begin{proof}
  By \lref[Lemma]{lemma: upper bound 2},
  for any fractional solution , Algorithm~
  (using the probability distribution  from that lemma)
  returns a feasible schedule of expected cost at most  
  times .  

  To see that the schedule can be computed in polynomial time,
  note first that the LP relaxation can be solved in polynomial time.
  The optimal solution  is minimal,
  so each  is at most 1, which implies that  
  is at most the number of demands, .
  In Algorithm~,
  each sample from the distribution  from \lref[Lemma]{lemma: upper bound 2}
  can be drawn in polynomial time.
  Each sample is~, and the sum of the samples 
  is at most ,
  so the number of samples is .
  In the inner loop of the algorithm
  (starting at line~\ref{line:join}),
  for each retailer~, the subset of orders joined
  can be computed in time , by amortization, so the total time for
  this step is , where  is the number of retailers.
\end{proof}


