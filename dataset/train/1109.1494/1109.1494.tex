\documentclass[11pt]{article}

\usepackage[paper=a4paper]{geometry}
\usepackage{verbatim}
\usepackage{algpseudocode, algorithm}
\usepackage{amsmath, amsfonts, amsthm}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage[T1]{fontenc}
\usepackage[normalem]{ulem}
\usepackage{braket}












\RequirePackage[l2tabu, orthodox]{nag}

\newcommand{\WARNING}{\text{\textcolor{red}{\textbf{!!!!~WARNING~!!!!}}}}

\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\ep}{\epsilon}

\newcommand{\alphah}{\widehat{\alpha}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\st}{\ensuremath{\text{ st.\@ }}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\N}{{\mathbb{N}}\WARNING} \newcommand{\eqdef}{\;\overset{\textup{def}}{=}\;}

\newcommand{\SigmaP}{\ensuremath{\Sigma_{\textup{P}}}}
\newcommand{\SigmaT}{\ensuremath{\Sigma_{\textup{T}}}}
\newcommand{\local}{\ensuremath{\textsc{LocalPM}}}
\newcommand{\occ}{\ensuremath{\textup{occ}}}
\newcommand{\ham}{\ensuremath{\textup{Ham}}}
\newcommand{\concat}{\|}
\newcommand{\upto}{\ldots}
\newcommand{\wildcard}{\ensuremath{\star}\xspace}
\newcommand{\Fam}{\ensuremath{\mathbb{C}_m}} 


\newcommand{\sExactWild}{\textsc{Shift-Exact\textsuperscript{\wildcard}}\xspace}
\newcommand{\DsExactWild}{\ensuremath{d_{\textup{E}}^+}}

\newcommand{\ssExactWild}{\textsc{ShiftScale-Exact\textsuperscript{\wildcard}}\xspace}
\newcommand{\DssExactWild}{\ensuremath{d_\textup{E}^\wildcard\WARNING}}

\newcommand{\sLtwo}{\textsc{Shift-}\xspace}
\newcommand{\DsLtwo}{\ensuremath{d_2^+\WARNING}}

\newcommand{\sLtwoWild}{\textsc{Shift-}\xspace}
\newcommand{\DsLtwoWild}{\ensuremath{d_2^+}}

\newcommand{\ssLtwo}{\textsc{ShiftScale-}\xspace}
\newcommand{\DssLtwo}{\ensuremath{d_2^1\WARNING}}

\newcommand{\ssLtwoWild}{\textsc{ShiftScale-}\xspace}
\newcommand{\DssLtwoWild}{\ensuremath{d_2^{1}}}

\newcommand{\LpolyWild}{\textsc{Poly--}\xspace}
\newcommand{\DLpolyWild}{\ensuremath{d_2^r}}

\newcommand{\sHam}{\textsc{Shift-Ham}\xspace}
\newcommand{\DsHam}{\ensuremath{d_\textup{H}^+}}

\newcommand{\sHamWild}{\textsc{Shift-Ham\textsuperscript{\wildcard}}\WARNING\xspace}
\newcommand{\DsHamWild}{\ensuremath{d_\textup{H}^+}\WARNING}

\newcommand{\ssHam}{\textsc{ShiftScale-Ham}\xspace}
\newcommand{\DssHam}{\ensuremath{d_\textup{H}^1}}

\newcommand{\ssHamWild}{\textsc{ShiftScale-Ham\textsuperscript{\wildcard}}\WARNING\xspace}
\newcommand{\DssHamWild}{\ensuremath{d_\textup{H}^1}\WARNING}


\newcommand{\skMismatch}{\textsc{Shift--Mismatch}\xspace}
\newcommand{\DskMismatch}{\ensuremath{d_\textup{M}^+}}

\newcommand{\skDecision}{\textsc{Shift--Decision}\xspace}
\newcommand{\DskDecision}{\ensuremath{d_\textup{D}^+}}
\newcommand{\dHam}{\DsHam}

\newcommand{\threeSUM}{\textsc{3Sum}\xspace}
\newcommand{\geombase}{\textsc{GeomBase}\xspace}

\newcommand{\freduce}[1]{\ensuremath{\lll_{#1}\!}}
\newcommand{\fequiv}[1]{\ensuremath{\equiv_{#1}\!}}


\newcommand{\cb}[1]{\makebox[4.05mm][c]{#1}}
\newcommand{\CB}[1]{\framebox[4.05mm][c]{#1}}
\newcommand{\cbS}[1]{\makebox[4.15mm][l]{#1}}
\newcommand{\cbx}[1]{\cb{}}
\newcommand{\cby}[1]{\cb{}}
\newcommand{\cbshrink}{\hspace{-0.5pt}}
\newcommand{\cbz}{\cb{0}}
\newcommand{\CBx}[1]{\CB{}}
\newcommand{\CBy}[1]{\CB{}}
\newcommand{\CBz}{\CB{0}}


\def\il{i_{\sf left}}
\def\ir{i_{\sf right}}
\def\squareforqed{\hbox{\rlap{}}}


\font\tenmsb=msbm10 \font\sevenmsb=msbm7 \font\fivemsb=msbm5

\newfam\msbfam
\textfont\msbfam=\tenmsb \scriptfont\msbfam=\sevenmsb
\scriptscriptfont\msbfam=\fivemsb


\newcommand{\margin}[1]{}







\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{problem}[theorem]{Problem}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}




\usepackage[firstinits,style=alphabetic,maxnames=99]{biblatex} 
\renewcommand*{\multicitedelim}{\addcomma\space} 
\bibliography{longnames.bib, bib-latest.bib}

\renewbibmacro{in:}{\ifthenelse{\ifentrytype{article}\OR\ifentrytype{inproceedings}}{}
        {\printtext{\bibstring{in}\intitlepunct}}
}


\AtEveryBibitem{\ifentrytype{inproceedings}{\clearfield{year}\clearfield{publisher}\clearfield{series}\clearfield{editor}\clearfield{address}}{}
}


\makeatletter
\providecommand\bibstyle@faked{}
\providecommand\bibdata@faked{}
\AtBeginDocument{\immediate\write\@mainaux{\noexpand\bibstyle@faked}\immediate\write\@mainaux{\noexpand\bibdata@faked}}
\makeatother



\title{Pattern Matching under Polynomial Transformation\thanks{~The results in Section~\ref{sec:TIL2}, except for those relating to higher degree polynomials, appeared in preliminary form in ``Self-normalised Distance with Don't Cares'', CPM '07.   Section~\ref{sec:TIk} contains revised and rewritten versions of results from ``Jump-Matching with Errors'', SPIRE '07.}}

\author{\ Ayelet Butman,\thanks{~Department of Computer Science, Holon Institute of Technology, Holon, Israel.}
\ Peter Clifford,\thanks{~Department of Statistics, University of Oxford, U.K.}
\ Rapha\"el Clifford,\thanks{~Department of Computer Science, University of Bristol, U.K.}
\ Markus Jalsenius,\footnotemark[4]~~
\\Noa Lewenstein,\thanks{~Department of Computer Science, Netanya Academic College, Israel.}
\ Benny Porat,\thanks{~Department of Computer Science, Bar-Ilan University, Israel.}
\ Ely Porat\footnotemark[6]
\ and Benjamin Sach\thanks{~Department of Computer Science, University of Warwick, U.K.}~
}


\date{}




\begin{document}

\maketitle

\begin{abstract}
    We consider a class of pattern matching problems where a normalising transformation is applied at every alignment. Normalised pattern matching plays a key role in fields as diverse as image processing and musical information processing where application specific transformations are often applied to the input.  By considering the class of polynomial transformations of the input, we provide  fast algorithms and the first lower bounds for both new and old problems.

    Given a pattern of length  and a longer text of length  where both are assumed to contain integer values only, we first show  time algorithms for pattern matching under linear transformations even when wildcard symbols can occur in the input.  We then show how to extend the technique to polynomial transformations of arbitrary degree.  Next we consider the problem of finding the minimum Hamming distance under polynomial transformation. We show that, for any , there cannot exist an  time algorithm for additive and linear transformations conditional on the hardness of the classic \threeSUM problem. Finally, we consider a version of the Hamming distance problem under additive transformations with a bound  on the maximum distance that need be reported.  We give a deterministic  time solution which we then improve by careful use of randomisation to  time for sufficiently small . Our randomised solution outputs the correct answer at every position with high probability.
\end{abstract}







\section{Introduction}

We consider pattern matching problems where the task is to find the distance between a pattern and every substring of the text of suitable length.  In the class of problems we consider, the values in the pattern can first be transformed so as to minimise this distance. Further, the selection of which transformation to apply, which is possibly distinct for each alignment, forms part of the problem that is to be solved. This class of problems generalises the well known problem of exact matching with wildcards~\cite{CH:2002, Clifford:2007} as well as the set of problems known previously as transposition invariant matching~\cite{MNU:2005}, both of which come from the pattern matching literature. However as we will see, it is considerably broader than both with applications in both image processing and musical information retrieval.


By way of a first motivation for our work, consider a fundamental problem in image processing which is to measure the similarity between a small image segment or template and regions of comparable size within a larger scene. It is well known that the cross-correlation between the
two can be computed efficiently at every position in the larger image
using the fast Fourier transform (FFT).  In practice, images may
differ in a number of ways including being rotated, scaled or affected
by noise.  We consider here the case where the intensity or brightness
of an image occurrence is unknown and where parts of either image
contain \emph{don't care} or \emph{wildcard} pixels, i.e.\ pixels
that are considered to be irrelevant as far as image similarity is
concerned.  As an example, a rectangular image segment may contain a
facial image and the objective is to identify the face in a larger
scene. However, some faces in the larger scene are in shadow and
others in light.  Furthermore, background pixels around the faces may
be considered to be irrelevant for facial recognition and these should
not affect the search algorithm.

In order to overcome the first difficulty of varying intensity within
an image, a standard approach is to compute the \emph{normalised}
distance when comparing a template to part of a larger image.  Thus both template and image are transformed or rescaled
in order to make any matches found more meaningful and to allow comparisons between matches at different positions. Within the image processing literature the accepted method of normalisation is to scale the mean and variance of the template and image segments. We take a slightly different although related approach to normalisation which will allow to us to show a number of natural generalisations.



We start by defining measures of distance between a pattern  and text , where  is a string of length  and  is a string of length , both over the integers.
The squared  or Euclidean distance between the pattern and the text at position  is

In this case, for each , the pattern can be normalised, or fitted as closely as possible to the text, by transforming the input to minimise the distance.


In the case of degree one polynomial transformations, the normalised  distance between the pattern and the text at position  can now be written as

where the minimisation is over rational values of  and . The minimisation is per alignment of  and , hence the values of  and  may (and probably will) differ between the positions .

We also consider the case when the input alphabet is augmented with the special wildcard symbol, denoted~``\wildcard''. A position where either the pattern or text has a wildcard will not contribute to the distance. That is, the minimisation is carried out using the sum of the remaining terms. Details are given in the problem definitions in the next section.



\subsection{Problems and our results}\label{sec:problems}

The words \emph{shift} and \emph{scale} are used to refer to additive and multiplicative transformations of the pattern, respectively. The input to all our problems is a text  of length  and a pattern  of length , and the output is a problem specific distance  between  and  at every position  of the text. To avoid overloading variable names, we give the distance  a unique name for each problem.

\begin{problem}[\sLtwoWild]
    \label{prob:sLtwoWild}
    Normalised  distance under shifts. Wildcards are allowed.

When either  or , the contribution of the pair to the sum  is taken to be zero. The minimisation is carried out using the sum of the remaining terms.
\end{problem}

Next we define the normalised  distance under shifts and scaling, corresponding to a degree one polynomial transformation of the values of the pattern.

\begin{problem}[\ssLtwoWild]
    Normalised  distance under shifts and scaling. Wildcards are allowed.

When either  or , the contribution of the pair to the sum \DssLtwoWild(i) is taken to be zero. The minimisation is carried out using the sum of the remaining terms.
\end{problem}

We show that both \sLtwoWild and \ssLtwoWild can be solved in  time by the use of FFTs of integer vectors. Our results are stated in Theorems~\ref{thm:sLtwoWild} and~\ref{thm:ssLtwoWild}. We assume the RAM model of computation throughout in order to be consistent with previous work on matching with wildcards. Further, our techniques also provide  time solutions (Theorems~\ref{thm:sExactWild} and~\ref{thm:ssExactWild}) to the problems of exact shift matching with wildcards (\sExactWild) and exact shift-scale matching with wildcards (\ssExactWild), formally defined as follows.

\begin{problem}[\sExactWild]
    \label{prob:sExactWild}
    Normalised exact matching under shifts. Wildcards are allowed.

Every position  where either  or  is ignored.
\end{problem}

\begin{problem}[\ssExactWild]
    \label{prob:ssExactWild}
    Normalised exact matching under shifts and scaling. Wildcards are allowed. The problem is defined similarly to \sExactWild, only that we check whether there exist  and  \st  for all positions  (except positions where  or ).
\end{problem}

We will also discuss extensions to pattern transformations under polynomials of higher degree in Section~\ref{sec:TIL2}. In terms of normalised  distance we give the following definition.

\begin{problem}[\LpolyWild]
    \label{prob:LpolyWild}
    Normalised  distance under degree- polynomial transformation. Wildcards are allowed. Let  be a polynomial of degree~ with .

When either  or , the contribution of the pair to the sum \DLpolyWild(i) is taken to be zero. The minimisation is carried out using the sum of the remaining terms.
\end{problem}

Note that the problem \ssLtwoWild is the same problem as \LpolyWild with degree . We will show that \LpolyWild can be solved in  time, where  is the exponent for matrix multiplication (e.g.,  when using the Coppersmith-Winograd algorithm).

The second main topic of our work is on normalised pattern matching problems under the Hamming distance. The Hamming distance is perhaps the most commonly considered measure of distance between strings in the field of pattern matching. We therefore define related normalised versions of our pattern matching problems in a similar way to before.

\begin{problem}[\sHam]
    \label{prob:sHam}
    Normalised Hamming distance under shifts. Wildcards are not allowed.

\end{problem}

\begin{problem}[\ssHam]
    \label{prob:ssHam}
    Normalised Hamming distance under shifts and scaling. Wildcards are not allowed.

\end{problem}



Previously it has been shown that \sHam, sometimes also referred to as transposition invariant matching, can be solved in  time~\cite{MNU:2005}.  It has been tempting to believe that it might be possible to improve this time complexity, particularly as there exist algorithms for standard non-normalised pattern matching under the Hamming distance which take  time~\cite{Abrahamson:1987,Kosaraju:1987}.
We show by reductions from the well known \threeSUM problem that for both shift and shift-scale matching under the Hamming distance there cannot exist an  time algorithm for any  (Theorems~\ref{thm:sHamLower} and~\ref{thm:ssHamLower}).

To circumvent this new conjectured lower bound, we consider as our last problem a shift version of the -mismatch problem. In the -mismatch problem, the Hamming distance is to be reported at every alignment as long as it is at most .  If it is greater than  then the algorithm is only required to report that the Hamming distance is large.  We define the problem as follows.

\begin{problem}[\skMismatch]
    \label{prob:skMismatch}
    Normalised -mismatch under shifts. Wildcards are not allowed.

\end{problem}

We first give a simple deterministic  time solution (Theorem~\ref{thm:detkmis}).   We then consider a decision version of the problem where we output only the locations  where  but not the Hamming distance at those locations. The decision version is defined as follows.

\begin{problem}[\skDecision]
    \label{prob:skDecision}
    Normalised -mismatch decision problem under shifts. Wildcards are not allowed.

\end{problem}

Using randomisation we show how to solve this problem in  time for the case that  (Theorem~\ref{thm:rankmis}). Here  is a constant that can be chosen arbitrarily to fine tune the error probability. Namely, our algorithm outputs the correct answer at every alignment with probability at least .  We therefore succeed in breaking our newly introduced running time barrier provided by the reduction from \threeSUM for a limited range of values of .



\subsection{Related work}

\margin{Transposition inv: Add in transposition invariant previous work}

Combinatorial pattern matching has concerned itself mainly with
strings of symbolic characters where the distance between individual
characters is specified by some convention. For the -mismatch problem, an  time algorithm was given in 1986 that uses constant time lowest common ancestor queries on the suffix tree of the pattern and text in a technique that has subsequently come to be known as `kangaroo hopping'~\cite{LV:1986a}. Almost ~years afterwards, the asymptotic running time was finally improved in~\cite{ALP:2004} to  time by a method based on filtering, the suffix tree (with kangaroo hopping) and FFTs.  In 2002, a deterministic  time solution for exact matching with wildcards was given by Cole and Hariharan~\cite{CH:2002} and further simplified in~\cite{Clifford:2007}.  In the same paper by Cole and Hariharan, an  time algorithm for the exact shift matching problem we consider in Section~\ref{sec:TIL2} was presented. Here  is the largest value in the input.  The approach we take to provide a simpler solution for this problem is similar in spirit to that of~\cite{Clifford:2007}.

There has also been some work in recent years on fast algorithms for distance calculation and approximate matching between numerical strings.  A number of different metrics
have been considered, with for example,  time solutions found for the  distance \cite{Atallah:01,CCI:2005,ALPU:2005}
and less-than matching~\cite{Amir:1995} problems and an  time algorithm for the -bounded version of the
 norm first discussed in \cite{CI:2004a} and then improved in \cite{CCI:2005, LP:2005}.


The most closely related work to ours comes under the heading of transposition invariant matching~\cite{LU:2000}.  The original motivation for this problem was within musical information retrieval where musical search is to be performed invariant of pitch level transposition. The transposition invariant distance between two equal lengthed strings  and  is defined to be , where  is the string obtained from  by adding  to every value and the distance  between strings can be variously defined.  Algorithms for transposition invariant Hamming distance, longest common subsequence (LCS) and Levenshtein (edit) distance amongst others were given in~\cite{MNU:2005} whose time complexities are close to the known upper bounds without transposition.   We show, in Section~\ref{sec:3SUM}, lower bounds for the special case of transposition invariant Hamming distance, which we named \sHam. Normalised pattern matching is also of central interest in the image processing literature where normalisation is typically performed by scaling the mean and standard deviation of the template and each suitably sized image segment to be  and , respectively. An asymptotically fast method for performing normalised cross-correlation for template matching, also using FFTs, was given in~\cite{Lewis:1995}.  The methods we give in Section~\ref{sec:TIL2} have some broad similarity to their approach only in the use of FFTs to provide fast solutions. Due to the differences in the definition of normalisation between our work and theirs, the solutions we give are otherwise quite distinct.




As a general class of problems, pattern matching under polynomial transformation is to the best of our knowledge new.  However, if we allow the degree of the polynomial transformation to increase to , then determining for which alignment the normalised distance equals zero is equivalent to the known problem of function matching.  Function matching has a deterministic  time solution, where  is the size of the pattern alphabet, and a faster randomised algorithm which runs in  time and has failure probability ~\cite{AALP:2006}.  \margin{Higher degree: Add relationship to our higher degree result when it exists.}




\subsection{Basic notation}

For a string  of length , we write  to denote the th character of  such that  (the first index is always zero). The -length substring of  starting at position  is denoted . For two strings  and , the notion  is used to denote the string formed by concatenating  and  in that order. All strings in this paper are over the integer alphabet. Therefore,  denotes the product of the numerical characters  and . If strings  and  are of equal length, we use the notation  for the string with characters . This element-wise arithmetic is used similarly for addition, subtraction, division and power. For example, the th symbol of  is . For a real value , the scalar multiplication  is the string .

The notation  will be used to denote the Hamming distance between equal lengthed strings  and :


Throughout this paper we use  to denote the text and  for the pattern. We use  to denote the length of  and  for the length of .

Our algorithms in Section~\ref{sec:TIL2} make extensive use of FFTs.  An important property of the FFT is that the cross-correlation, defined as

can be calculated accurately and efficiently for all  in  time (see e.g. \cite{Cormen:1990}, Chapter 32). The time complexity is reduced from  to  using a standard splitting trick which partitions the text into  length substrings which overlap each other by  characters. When it is clear from the context we use  as an abbreviation for .

We use ``\wildcard'' for the single character wildcard symbol. Under arithmetics on strings, as defined above, we may think of a wildcard as having the value zero. This value is, however, inconsequential for our purposes, as all expressions in this paper have the property that whenever a wildcard symbol is involved in some arithmetics, it is multiplied by a zero.

We write  to denote the set of integers . We also say that  if and only if  for some constant , i.e  up to log factors.


\subsection{Organisation}

The reminder of the paper is organised as follows. In Section~\ref{sec:TIL2} we discuss normalised pattern distance under  distance (\sLtwoWild and \ssLtwoWild) and the decision variants (\sExactWild and \ssExactWild). We also show how to extend the methods to transformations of higher degree polynomials (\LpolyWild). Then in Section~\ref{sec:3SUM} we give running time lower bounds for \sHam and \ssHam by reduction from the \threeSUM problem.  In Section~\ref{sec:TIk} we introduce our new deterministic and randomised algorithms for \skMismatch and \skDecision. Finally, we conclude in Section~\ref{sec:discussion} and set out some open problems.


\section{Normalised  distance}\label{sec:TIL2}

We give  time solutions for shift and shift-scale versions of the normalised  distance problem with wildcards. We further show that this enables us to solve the exact shift matching and exact shift-scale matching problems in the same time complexity for inputs containing wildcard symbols. Lastly we show how to extend our solutions to normalisation under polynomials of arbitrary degree.

\subsection{Normalised  distance under shifts}

In order to handle wildcards, we define two new strings  and  obtained from  and , respectively, such that  if , and  otherwise. Similarly,  if , and  otherwise. We can now express the shift normalised  distance at position  as

Algorithm~\ref{alg:shiftdistance} shows how to compute  for all positions . Correctness and running time is given in the following theorem.


\begin{algorithm}[t]
    \caption{Solution to \sLtwoWild.
    \label{alg:shiftdistance}}
    \begin{enumerate}
        \item Construct  from  such that  if , and  otherwise. Construct  from  similarly.
        \item Compute the following six cross-correlations:

        \begin{tabular}{l}
             \\
            
        \end{tabular}
        \hfill
        \begin{tabular}{l}
             \\
            
        \end{tabular}
        \hfill
        \begin{tabular}{l}
             \\
            
        \end{tabular}

        \item Return . We have . For positions~ where  we have .
    \end{enumerate}
    \vspace{-8pt}
\end{algorithm}

\begin{theorem}
    \label{thm:sLtwoWild}
    The shift version of the normalised  distance with wildcards problem (\sLtwoWild) can be solved in  time.\end{theorem}
\begin{proof}
    Consider Algorithm~\ref{alg:shiftdistance}. We first analyse the running time. Step~1 requires only single passes over the input. Similarly, , ,  and  can all be calculated in linear time once  and  are known. Using the FFT, the six cross-correlations in Step~2 can be calculated in  time. The final vector of Step~3 is obtained in linear time. Thus,  is the overall time complexity of the algorithm.

    To show correctness we consider the minimum value of

This can be obtained by differentiating with respect to  and obtaining the minimising value. Solving

gives us the value

where  is the minimising value at position~. Substituting  into Equation~(\ref{eq:sLtwoMinimise}), expanding and collecting terms, we obtain the final answer as

where  are the correlations defined in Algorithm~\ref{alg:shiftdistance}.

    Lastly we observe that when  there is a wildcard at every position in the alignment of  and . Here the shift normalised  distance is defined to be~0.
\end{proof}


\subsection{Normalised  distance under shift-scale}

Similarly to the shift version of normalised  distance in the previous section, we can now solve the shift-scale version. The solution is slightly more involved but the running time remains the same. Algorithm~\ref{alg:shift-scaleddistance} sets out the main steps to achieve this and the result is summarised in the following theorem.

\begin{algorithm}[t]
    \caption{Solution to \ssLtwoWild.
    \label{alg:shift-scaleddistance}}
    \begin{enumerate}
        \item Construct  from  such that  if , and  otherwise. Construct  from  similarly.
        \item Compute the following six cross-correlations:

        \begin{tabular}{l}
             \\
            
        \end{tabular}
        \hfill
        \begin{tabular}{l}
             \\
            
        \end{tabular}
        \hfill
        \begin{tabular}{l}
             \\
            
        \end{tabular}

        \item Compute

        

        and compute  and . At positions  where , set . At positions  where  and , set  and .


        \item Return
            .

            We have .
    \end{enumerate}
    \vspace{-8pt}
\end{algorithm}


\begin{theorem}
    \label{thm:ssLtwoWild}
    The shift-scale version of the normalised  distance with wildcards problem (\ssLtwoWild) can be solved in  time.
\end{theorem}
\begin{proof}
    Consider Algorithm~\ref{alg:shift-scaleddistance}. Notice that the same six correlations as in Algorithm~\ref{alg:shiftdistance} have to be calculated. The additional strings in Step~3 require linear time, as well as producing the output in Step~4. Hence the overall running time is .

    Similarly to Equation~(\ref{eq:sLtwoMinimise}) we can express the shift-scale version of the normalised  distance at position  as

By minimising this expression with respect to both  and  we get a system of two simultaneous linear equations

By solving this system and using the definitions of  in Algorithm~\ref{alg:shift-scaleddistance}, we get the minimising values

For some positions , the solution to the system might not be unique. This happens at alignments  for which every position  has a wildcard, hence . Here we avoid illegal division by zero by simply setting both  and  to zero (any value would do). A non-unique solution also occurs at alignments  where all  are identical over every non-wildcard position . This is characterised by . To see this, observe that  by Cauchy-Schwarz inequality. Here we set (arbitrarily)  and therefore obtain the minimising value .

    At Stage~4,  and  contain the minimising values for  and  at every position. We substitute these into Equation~(\ref{eq:ssLtwoMinimise}) and expand. This gives us the expression for .
\end{proof}


\subsection{Exact shift and shift-scale matching with wildcards}

For the exact shift matching problem with wildcards, \sExactWild, a match is said to occur at location  if, for some shift  and  for every position  in the pattern, either  or at least one of  and  is the wildcard symbol. Cole and Hariharan~\cite{CH:2002} introduced a new coding for this problem that maps the string elements into~ for wildcards and complex numbers of modulus~ otherwise.  The FFT is then used to find the (complex) cross-correlation between these coded strings, and finally a shift match is declared at location  if the th element of the modulus of the cross-correlation is equal to .

Our Algorithm~\ref{alg:shiftdistance} provides a straightforward alternative method for shift matching with wildcards.  It has the advantage of only using simple integer codings.  Since Algorithm~\ref{alg:shiftdistance} finds the minimum  distance at location , over all possible shifts, it is only necessary to test whether this distance is zero.  The running time for the test is then  since it is determined by the running time of Algorithm~\ref{alg:shiftdistance}.

\begin{theorem}
    \label{thm:sExactWild}
    The problem of exact shift matching with wildcards (\sExactWild) can be solved in  time.
\end{theorem}

The exact shift-scale matching problem with wildcards, \ssExactWild, can be solved similarly by applying Algorithm~\ref{alg:shift-scaleddistance}.

\begin{theorem}
    \label{thm:ssExactWild}
    The problem of exact shift-scale matching with wildcards (\ssExactWild) can be solved in  time.
\end{theorem}


\subsection{Normalised  distance under higher degree transformations}

We can now consider the problem of computing the normalised  distance under general polynomial transformations. The problem, which we termed \LpolyWild, was defined in Problem~\ref{prob:LpolyWild}. Recall that we let

be a polynomial of degree~.
Similarly to the shift and shift-scale versions of the normalised  distance we consider the minimum value of

By differentiating with respect to each  in turn, giving

we obtain a system of  linear equations in  unknowns for each alignment  of the pattern and text. We need to solve these equations and then substitute the minimising  values back into Equation~(\ref{eq:polyLtwoWild}) as we did in the proof of Theorem~\ref{thm:ssLtwoWild}. This procedure is captured by the following theorem.

\begin{theorem}
    The normalised  distance problem with wildcards under polynomial transformations of degree  (\LpolyWild) can be solved in   time.
\end{theorem}
\begin{proof}
    To compute the coefficients for the first linear equation for  we need to perform  cross-correlations.  However, for each subsequent equation for  we only need to perform a constant number of new cross-correlations.  Therefore the total number of cross-correlations is  to give the coefficients of all the equations, taking  time overall.  The time to solve the systems of  equations in  unknowns is  per alignment , where  is the exponent for matrix multiplication.  This gives  time or  using the algorithm of Coppersmith and Winograd~\cite{CW:1990}.

    Once the equations have been solved, and the minimising values of  calculated, they are then substituted into the expression for  in Equation~(\ref{eq:polyLtwoWild}).   To calculate the final values  we require  cross-correlations to be computed as well as  products of vectors of length .  The overall time complexity is therefore .
\end{proof}

This method is of particular relevance for low degree polynomials, or at least polynomials whose degree is less than the number of distinct values in the pattern.  However, if the degree  is greater than the number of distinct values in the pattern, then there exists a suitable polynomial  for any  mapping we should choose.  This gives us a straightforward  time solution by considering each position of the pattern in the text independently and ignoring any values aligned with wildcards in either the pattern or text. For each such position we need only set  to be the mean of the values in the text that align with a value equal to  in the pattern.


\section{Lower bounds for Hamming distance}\label{sec:3SUM}

In this section we will show that no  time algorithm can exist for neither \sHam or \ssHam conditional on the hardness of the classic \threeSUM problem. One formulation of the \threeSUM problem is given below.

\begin{definition}[\threeSUM]
    \label{def:threeSUM}
    Given a set of  positive integers, determine whether there are three elements  in the set such that .
\end{definition}

The \threeSUM problem can be solved in  time and it is a long standing conjecture that this is essentially the best possible. The problem has been extensively discussed in the literature, where Gajentaan and Overmars~\cite{GO:1995} were the first to introduce the concept of \threeSUM-hardness (see definition below) to show that a wide range of problems in computational geometry are at least as hard as the \threeSUM problem. One example is the \geombase problem, defined below, which we will use in one of our reductions in this section. See~\cite{King:2004} for a survey of problems from computational geometry whose hardness relies on that of \threeSUM.

\begin{definition}[\geombase]
    \label{def:geombase}
    Given a set of  points with integer coordinates on three horizontal lines ,  and , determine whether there exists a non-horizontal line containing three of the points.
\end{definition}

Although an  lower bound for \threeSUM is only conjectured, it has been shown that under certain restricted models of computation,  is a true lower bound (see~\cite{ES:1995,Erickson:1999,Erickson:convex:1999}). Under models that allow more direct manipulation of numbers instead of just real arithmetic, such as the word-RAM model, an almost  factor improvement to the standard  solution has been shown to be possible under the Las Vegas model of randomisation (see~\cite{BDP:2005}). Nevertheless, a \threeSUM-hardness result for a problem is a strong indication that finding an  time solution is going to be a challenging task.

Before we show that \sHam and \ssHam are both \threeSUM-hard, we provide a brief but formal discussion about reductions and define \threeSUM-hardness.


\subsection{\threeSUM reductions}

Following the definitions of~\cite{GO:1995} where \threeSUM-hardness was first introduced, we say that a problem  is \emph{-solvable using a problem } if and only if every instance of  of size  can be solved using a constant number of instances of  of at most  size and  additional time. We denote this as . When  is sufficiently small, lower bounds for  carry over to . A problem  is \emph{\threeSUM-hard} if  and  for some constant . In the definition of \threeSUM-hardness of~\cite{GO:1995}, the requirement was that , however, to scale with more powerful models of computation, we require that . If  and  then we say that  and  are \emph{-equivalent}.
In the following section we will show that  where the instance size of \sHam is a text of length  and a pattern of length .

In the literature there are a variety of definitions of the \threeSUM problem. They differ only slightly in their formulations and are all equivalent. One common definition, used as the ``base problem'' in~\cite{GO:1995}, is formulated as follows. Given a set of  integers, determine whether there are three elements  in the set such that . Without too much work, one can show that this definition is -equivalent with Definition~\ref{def:threeSUM} of \threeSUM above (small modifications of the proof of Theorem~3.1 in~\cite{GO:1995} can be used to prove this). Further, it was shown in~\cite{GO:1995} that \geombase is -equivalent to \threeSUM.


\subsection{\threeSUM-hardness of \sHam}

In this section we show that \sHam is \threeSUM-hard.

\begin{lemma}
    \label{lem:sHamHard}
     where the instance size of \sHam is a text of length  and a pattern of length .
\end{lemma}
\begin{proof}
    Let the set  be an instance of \threeSUM of size . First we sort all elements of  so that  where . Let  and for , let . Thus, . We define the following -length strings over the alphabet .

We now construct an instance of \sHam specified by

The text  has length  and the pattern  has length . First we show that if there are elements  such that  then there is a position  such that the shift-normalised Hamming distance between  and  is at most . We will then show that if no such three elements exist then the shift-normalised Hamming distance between  and every -length substring of  is strictly greater than .

    As an illustrative example, suppose that  contains seven elements and suppose that . Consider the alignment of  and  where  in  is aligned with  in :
\begin{center}
        \footnotesize
        \cbS{T:}\cbz\cbz\cbz\cbz\cbz\cbz\cbz \cbx{1}\cbx{2}\cbx{3}\cbx{4}\cbx{5}\CBx{6}\cbx{7}\cby{1}\cby{2}\cby{3}\cby{4}\cby{5}\cby{6}\cby{7}\cbx{1}\cbx{2}\CBx{3}\cbx{4}\cbx{5}\cbx{6}\cbx{7}\cby{8}\cby{9}\cby{\cbshrink 1\!0}\cby{\cbshrink 1\!1}\cby{\cbshrink 1\!2}\cby{\cbshrink 1\!3}\cby{\cbshrink 1\!4}\\
        \cbS{P:}\cb{~}\cb{~}\cb{~}\cb{~}\cb{~}\cb{~}\cb{~}\cb{~}\cb{~}\cbx{7}\cbx{6}\cbx{5}\CBx{4}\cbx{3}\cbx{2}\cbx{1}\cbz\cbz\cbz\cbz\cbz\cbz\cbz \CBz\cbz\cbz\cbz\cbz\cbz\cbz \cb{~}\cb{~}\cb{~}\cb{~}\cb{~}
    \end{center}
We observe that shifting the pattern by  will induce two matches, marked with the squares above. Thus, the shift-normalised Hamming distance is at most  (in fact, it is exactly ). It should be easy to see how this generalises to any size of  and any three elements  such that . Namely, the alignment in which  is aligned with  has Hamming distance at most  since there must also be a match at the position where 0 aligned with . The construction of  and  ensures that there is always an alignment that captures these matches.

    Now suppose there are no elements  such that . Consider a fixed alignment of  and . We will show that there can be at most one match under any shift. By construction of  and , the zeros in  are all aligned with distinct symbols in . Hence for any shift, at most one of these zeros can be involved in a match. The non-zero symbols of  (i.e.,~the -length prefix of ) appear in strictly decreasing order and are aligned with an -length substring of  whose elements appear in non-decreasing order. Therefore, under any shift, at most one of the non-zero symbols in  can be involved in a match. It remains to show that there is no shift such that both a zero and a non-zero symbol in  are simultaneously involved in a match. First, we observe that if there is a match between a  in  and some  in  then there can be no other match as every non-zero symbol in  is aligned with a value that is less than . Suppose therefore that there is a match between a  in  and some  in  (i.e.,~the shift is ). We need to consider three possible cases: there is also a match that involves some  in  aligned with either (i)~a~ in , (ii)~some  in  or (iii)~some  in . In case~(i) the shift must be negative, hence is not compatible with the shift . In case~(ii) we can see that the shift must be greater that  (the largest elements in the set ), hence is not compatible with the shift . In case~(iii) we have that , which contradicts the assumption that there are no elements  such that . Thus, the shift-normalised Hamming distance is at least  for any alignment of  and .

    Finally, we observe that the most time consuming part of the reduction is the sorting of  which could take  time. This concludes the proof.
\end{proof}

\begin{theorem}
    \label{thm:sHamLower}
    \sHam has no  time algorithm, for any , conditional on the hardness of the \threeSUM problem.
\end{theorem}
\begin{proof}
    Given a \threeSUM instance of size , by Lemma~\ref{lem:sHamHard} we construct a \sHam instance of size  and  in  time. If \sHam has an  time algorithm then \threeSUM can be solved in  time.
\end{proof}


Notice that \sHam has an  time solution~\cite{MNU:2005}. See Section~\ref{sec:unbounded} for details.


\subsection{\threeSUM-hardness of \ssHam}

In this section we show that \ssHam is \threeSUM-hard.

\begin{lemma}
    \label{lem:ssHamHard}
     where the instance size of \sHam is a pattern and text of length  each.
\end{lemma}
\begin{proof}
    We reduce from the \geombase problem which is -equivalent to \threeSUM. Before we describe the reduction we adopt a formulation of the \geombase problem that differs slightly in notation. Instead of insisting on the points being on the horizontal lines ,  and , we assume that the points are on the vertical lines ,  and  and we want to determine whether there is a (non-vertical) line containing three points. Under this formulation, let  be an instance of \geombase that contains the integer points , where every .

    We construct an instance of \ssHam that is specified by the text  and the pattern , both of length . It should now be clear that \ssHam returns the shift-and-scale normalised Hamming distance  (for the only alignment of  and ) if and only if there are two values  and  such that  for three distinct positions , which is equivalent to fitting a line through three points. Note that we minimise  and  over the rationals, and any line going through three points is indeed specified by rational values of  and . Since the reduction takes linear time, we have proved the lemma.
\end{proof}

\begin{theorem}
    \label{thm:ssHamLower}
    \ssHam has no  algorithm, for any , conditional on the hardness of the \threeSUM problem.
\end{theorem}
\begin{proof}
    Given a \threeSUM instance of size , by Lemma~\ref{lem:ssHamHard} we construct a \ssHam instance of size  in  time. If \ssHam has an  algorithm then \threeSUM can be solved in  time.
\end{proof}



\section{Normalised -mismatch under shifts}\label{sec:TIk}

In this section we consider two versions of the normalised -mismatch problem under shifts, defined as Problems~\ref{prob:skMismatch} and~\ref{prob:skDecision} in the introduction. Both problems are parameterised by an integer . In the first problem, \skMismatch, the output is the shift-normalised Hamming distance between  and  at every position for which the distance is  or less. Where the distance is larger than , only  is outputted. Recall from the introduction that the shift-normalised Hamming distance between  and  is denoted  and defined by

In Section~\ref{sec:detkmis} we give a deterministic algorithm that solves \skMismatch in  time.

In Section~\ref{sec:randkmis} we consider the the second version of shift-normalised -mismatch, \skDecision, which unlike the previous problem only indicates with yes or no whether the shift-normalised Hamming distance is  or less. We give a randomised solution to this decision problem with the improved running time . The parameter  is a constant that can be chosen arbitrarily to fine tune the error probability. Namely, the probability that our algorithm outputs the correct answer at every alignment is at least . The errors are one-sided such the algorithm will never miss reporting an alignment for which the shift-normalised Hamming distance is indeed  or less. Our algorithm requires that , hence it is suited to situations where the locations of text substrings similar to the pattern are required but the distances themselves are not needed.


\subsection{The unbounded case}\label{sec:unbounded}

In~\cite{MNU:2005}, M\"{a}kinen, Navarro and Ukkonen gave an  time algorithm for the shift-normalised Hamming distance problem, \sHam, which by definition solves the bounded, -mismatch variant in   time also. We briefly recap their method by way of an introduction. First observe that the maximum number of matches for any alignment is exactly

For each alignment , this value can be obtained by creating an -length array , which we refer to as the \emph{shift array}, defined by

for all . This shift array is then sorted to find the most frequent value, which is the  that minimises . The number of times it occurs is . Computing this requires  time per alignment and hence  time overall. In the next section we will reconsider  and demonstrate that it can be run-length encoded in  runs whenever .


\subsection{A deterministic solution}\label{sec:detkmis}

The deterministic algorithm makes use of the notion of difference strings which were introduced in~\cite{LU:2000} and are defined as follows.

\begin{definition}
    \label{dfn:diffstr}
    Let  be a string of length . The \emph{difference string} of , denoted , is defined by

for all . The length of  is .
\end{definition}

We will also make use of a generalisation of the difference string when we present our randomised algorithm in Section~\ref{sec:randkmis}. The core of our deterministic shift-normalised -mismatch algorithm is the relationship between the number of mismatches between  and  and the value of . We begin in Lemma~\ref{lem:TI-up} below by showing that if  is small then the number of mismatches between the difference strings  and  is also small. In~\cite{MNU:2005} a related result was used to reduce the shift-normalised exact matching problem to the conventional exact matching problem. Specifically, they observed that in the special case that , the implication becomes an equivalence, i.e.,  if and only if . Unfortunately, this is not the case in general.

\begin{lemma}
    \label{lem:TI-up}
    Let  be a pattern and  a text. For all ,

\end{lemma}
\begin{proof}
    Let  be such that  and therefore there exists an  such that for at most  distinct position  we have that . Further, at most  distinct positions  have either  or . This implies that there are at least  distinct positions  such that  and . By rearranging these equations, for any such  we have that  and hence by Definition~\ref{dfn:diffstr}, . As required there are at most  mismatches (recall ).
\end{proof}

Lemma~\ref{lem:TI-up} suggests the following strategy. First we find the leftmost up to  mismatches between  and  at each alignment . By Lemma~\ref{lem:TI-up} we can disregard any alignments with more than  mismatches. Finally we use the locations of these mismatches to infer  at the remaining alignments.

The first step can be done using any -mismatch (strictly -mismatch) algorithm which returns the locations of the mismatches. The well-known `kangaroo' method of~\cite{LV:1986a} achieves this in optimal  time. The method is so named as it uses longest common extensions to `hop' between mismatches in constant time. The discarding phase is trivial and therefore we only focus on computing  from the locations of the (at most ) mismatches between  and , where  is an arbitrary non-discarded alignment.

Recall from Section~\ref{sec:unbounded} the definition of the shift array  in Equation~(\ref{eq:shiftarray}), and recall that the value of  is the number of occurrences of the most frequent entry in . We will now use the locations of the mismatches between  and  to obtain a run-length encoded version of  containing  runs. The key property we require is given in Lemma~\ref{lem:run} which states that a matching substring in  and  corresponds to a run (a substring of equal values) in . This immediately implies that  can be decomposed into at most  runs. Specifically, one run of length~ for each mismatch and an additional run for each stretch between mismatches.

\begin{lemma}
    \label{lem:run}
    If  then  for all .
\end{lemma}
\begin{proof}
    Suppose that . We proceed by induction on . The base case  is tautologically true. For the inductive step, let . By the inductive hypothesis, we have that . As , by Definition~\ref{dfn:diffstr} (and rearranging the equation), we have .
\end{proof}

In Section~\ref{sec:unbounded} we discussed that the value of  equals , which could be found by sorting and scanning  in  time. However, we now have  in run-length encoded form (with  runs), therefore the time taken to find  is reduced to . Over all alignments, this gives  time as desired.

We can now give an overview of our deterministic algorithm for \skMismatch. The steps are described in Algorithm~\ref{alg:deterministic} and the overall running time is given in Theorem~\ref{thm:detkmis} below.

\begin{algorithm}[t]
    \caption{Overview of deterministic solution to \skMismatch.
        \label{alg:deterministic}}
    \begin{enumerate}
        \item Compute the difference strings  and  by scanning  and .
        \item Run a -mismatch algorithm on  and  in order to find all alignments where the number of mismatches is at most . The -mismatch algorithm must also return the locations of the mismatches at any alignment where there are at most  mismatches.
        \item Discard all alignments with more than  mismatches.
        \item For each undiscarded alignment , decompose  into at most  runs (substrings with a common value). The start and end points of the runs are determined by scanning the locations of the  mismatches between  and .
        \item Sort the runs in  by value in order to find the most frequent entry  in . Then output , which is the value .
    \end{enumerate}
    \vspace{-8pt}
\end{algorithm}


\begin{theorem}
    \label{thm:detkmis}
    The shift-normalised -mismatch problem (\skMismatch) can be solved deterministically in  time.
\end{theorem}
\begin{proof}
    The solution is outlined in Algorithm~\ref{alg:deterministic}. Correctness follows directly from the discussion in this section. The time complexity of the five steps is as follows. By inspection of the definition, the difference strings computed in Step~1 require  time. Step~2 uses a -mismatch algorithm as a black box and can be performed in  time by using for example the algorithm in~\cite{LV:1986a}. Step~3 makes a single pass of the output of the -mismatch algorithm in  time. Step~4 constructs a run length encoded version of  for each undiscarded~. This requires scanning the  mismatches at each undiscarded alignment. Therefore Step~4 takes  time. Step~5 scans and sorts each  which takes  time per alignment as  is encoded by  runs. Overall the algorithm requires  time as claimed.
\end{proof}


\subsection{An improved, randomised solution} \label{sec:randkmis}

We now present an improved solution to the shift-normalised -mismatch problem which runs in  time. The improved algorithm is for the case that  and is randomised. The errors are one-sided (false-positives) and it outputs the correct answer at all alignments with probability at least  for any constant . For each position , the algorithm gives a yes/no answer to the question ``is ?''. The algorithm does not output the actual distance . Throughout this section, we use  as shorthand for .

In Section~\ref{sec:detkmis} our deterministic algorithm made use of the locations of mismatches in the difference strings  and . Recall that the difference string  was defined to give the differences between consecutive positions in a string~. That is,  for all~. A key observation was that  if and only if , i.e., the positions of  and  require the same shift  to match. However, there is no reason to consider only consecutive differences. In fact, as we will see, one may consider differences under any arbitrary permutation of the position set. This notion is formalised as follows.

\begin{definition}
    \label{dfn:pdiffstr}
    Let  be a string of length  and  be a permutation. The \emph{permuted difference string} of  under , denoted , is defined by

for all . The length of  is .
\end{definition}

Note that the permuted difference string   has length  in contrast to the difference string  of Definition~\ref{dfn:diffstr} which has length .

The central idea of our improved algorithm is to use the value of  to directly determine whether  at each alignment~. In Definition~\ref{dfn:TI-ktight} below we introduce the notion of a permutation being \emph{-tight} for some . Intuitively,  is -tight for  if we can infer directly from  whether .

\begin{definition}
    \label{dfn:TI-ktight}
    Let  be a permutation,  a pattern and  a text substring. We say that  is \emph{-tight} for  if

\end{definition}

It would of course be highly desirable to find a permutation  which is -tight for all  and any . However, we will see that this is in general not possible. 
We begin by showing that any  has the property that   implies that  for all . To do so we first prove a general lemma which will also be useful later. Lemma~\ref{lem:TI-permup} then gives the desired property and is a generalisation of Lemma~\ref{lem:TI-up} to arbitrary permutations.

\begin{lemma}
    \label{lem:TI-kmisiff}
    Let  be a permutation,  a pattern and  a text substring. For all ,

\end{lemma}
\begin{proof}
    The left-hand side of the arrow is the same as , which by Definition~\ref{dfn:pdiffstr} is equivalent to the right-hand side of the arrow.
\end{proof}


\begin{lemma}
    \label{lem:TI-permup}
    Let  be a permutation,  a pattern and  a text substring.

\end{lemma}
\begin{proof}
    Let  and  be such that . By definition there exists an  such that the set  has size at most . As  is a permutation, there are at most  positions  such that either  or . Therefore, for all (at least)  remaining positions  we have that  and . For each such position , by rearranging the two equations it follows from Lemma~\ref{lem:TI-kmisiff} that . Thus, there are at most  mismatches between  and .
\end{proof}

A logical next step would be to attempt to find a permutation  with the property that  implies that  for all . Unfortunately, Lemma~\ref{lem:notconv} below shows that no such permutation can exist. As Corollary~\ref{cor:notktight} states, this immediately implies that there is no permutation which is -tight for all . Instead we will select our permutation at random and show that we can obtain a permutation that is -tight for a given  with constant probability.


\begin{lemma}
    \label{lem:notconv}
    Let  be any permutation and . There exists a pattern  and text substring  such that

\end{lemma}
\begin{proof}
    We define  to be an -length string of zeros. In order to define the -length string  we first introduce some notation.

    Let . We identify a set of  locations  as follows. Location . For , location  is the smallest position in  that is not any of the preceding locations  or any location that is mapped to or from by any of these preceding locations (under ). Formally,  is the smallest location which is not in the set . Observe that the set  has size at most  (since ), hence such a location always exists.

    We can now define  as follows. For all , let  and . At all other locations , . Observe that by construction, the locations  and  are all distinct. Therefore,  contains exactly  ones and  zeros. As , more than half the locations have , and therefore  is minimised by the shift . Thus, .

    We proceed by showing that the alignment of  and  contains at least  matches. There are  locations  in  such that . Of these locations, at most  have . Therefore, there are at least  locations  such that . As , we have by Lemma~\ref{lem:TI-kmisiff} that  at  locations. Now consider locations  for . By construction,  and therefore  by Lemma~\ref{lem:TI-kmisiff}. This implies a further  matching locations. There are therefore at least at least  matches or at most  mismatches between  and . Since  for all  we have that .
\end{proof}

\begin{corollary}
    \label{cor:notktight}
    Let  be any permutation and . There exists a pattern  and text substring  for which  is not -tight.
\end{corollary}
\begin{proof}
Immediate from Definition~\ref{dfn:TI-ktight} and Lemma~\ref{lem:notconv}.
\end{proof}


\subsubsection{Random permutations} \label{sec:permutations}

We will choose a permutation uniformly at random  from a simple family of permutations. On first inspection, we could have chosen from the family of all permutations. We claim without proof that a permutation chosen uniformly at random from the family of all permutations is -tight for any  with constant probability. However, we must be able to efficiently compute   for all  under our chosen permutation. The key problem being that in general  is not easily obtained from . As  varies,  could change drastically, even when  is only incremented by one. Therefore we must be careful in selecting our family of permutations.

We will use the family of cyclic permutations, denoted  (for patterns of length ), defined as follows.

\begin{definition}
    \label{dfn:cperm}
    The set  contains the  cyclic permutations , where

\end{definition}

We now show in Lemma~\ref{lem:k-tight} that  has the desired property of -tightness when . There is a corner case when  which is easily solved in  time using our deterministic algorithm from Section~\ref{sec:detkmis}. For Lemma~\ref{lem:k-tight} we require that .

\begin{lemma}
    \label{lem:k-tight}
    Let  be a pattern and  a text substring. When  and ,

\end{lemma}
\begin{proof}
    Let kP,T_i. We will show that . Note that . We let  be the minimal number of mismatches between  and , and  be the shift which minimises .

    Assume first that . By Lemma~\ref{lem:TI-permup} and Definition~\ref{dfn:TI-ktight} we have that that every  is -tight for  and therefore . Assume second that that .  We split the proof into three cases:


    First we introduce some notation. There are exactly  positions  where . We call such a position an -match. Similarly, any position with  for some  is called an -match. Positions which are not -matches are called -mismatches. Hence there are  distinct -mismatches. We will refer to  as the position that  is mapped to (by ).
    \medskip

    \noindent
    \textbf{Case 1 ().}
    Let  be an arbitrary -mismatch. Position  is mapped to another  -mismatch in exactly  distinct permutations of . This holds for each of the  distinct -mismatches. Hence there are at most  permutations under which some -mismatch is mapped to another -mismatch. The remaining (at least)  permutations  in  immediately have the following two properties:
\begin{enumerate}
        \setlength{\itemsep}{0pt}
        \item[(i)] if position  is an -mismatch then  is an -match;
        \item[(ii)] if position  is an -mismatch then position  is an -match.
    \end{enumerate}
There are  positions  with property~(i) and another (disjoint)  positions  with property~(ii). That is, for each -mismatch there are two positions  that meet one of the two properties above. By Lemma~\ref{lem:TI-kmisiff}, each such  implies that . Therefore, in each of these  permutations ,  and so each such permutation is -tight for . By the assumption of Case~1, , and the assumptions that  and , we have that . Thus, 
    \medskip

    \noindent
    \textbf{Case 2 ().}
    Let  be an arbitrary set of  distinct -mismatches. For any permutation , let

We define

to be the number of mismatch positions between  and  that are also in  or . We now consider the total number of mismatches between  and  (that are in  or ) summed over all permutations in . Let


    Since  by the assumption of Case~2, there are at least  -matches. A permutation  that maps a position  to an -match creates a mismatch  by Lemma~\ref{lem:TI-kmisiff} (as  is an -mismatch). For a fixed , the number of permutations in  that map  to an -match equals the number of -matches, which is at least . Thus, the set  of  -mismatches contributes at least  to .

    Similarly, any position  which is an -match creates a mismatch  by Lemma~\ref{lem:TI-kmisiff} if it is mapped to an -mismatch in . This occurs under exactly  permutations. Recall that any  which is mapped to a position in  under  belongs to . Therefore, given that there are at least  -matches, the contribution is at least  further distinct mismatches to  .

    Summing up the previous two paragraphs, we have shown that .
    Each permutation  that is not -tight for  has  (since ). Therefore,  is a generous upper bound on the number of mismatches across all permutations which are not -tight. This leaves at least  mismatches among the -tight permutations of . Since , we have that  for any , hence each permutation contributes at most  mismatches to . Therefore there are at least  distinct -tight permutations. Thus, .
    \medskip

    \noindent
    \textbf{Case 3 ().}
    Similarly to Case~2, we consider the total number of mismatches between  and  summed over all permutations in . Let

Since  the number of -mismatches is more than  for all . Fix an arbitrary position  and choose an  such that  is an -match. There are at least  permutations  in  that map position  to an -mismatch. By Lemma~\ref{lem:TI-kmisiff},  for each of these permutations. Hence position  will contribute with at least  to . By considering all  positions , we have that .

    Similarly to the reasoning in Case~2, each permutation  that is not -tight for  has  (since ). Again,  is a generous upper bound on the number of mismatches across all permutations which are not -tight. This leaves at least  mismatches among the -tight permutations of . As certainly , we have that there are at least  distinct -tight permutations for . Therefore,

    where the second inequality follows from  and the last inequality from , both assumptions in the statement of the lemma.
\end{proof}



\subsubsection{The algorithm}

Before describing the randomised algorithm we turn our attention to the problem of finding all positions  such that  under an arbitrary cyclic permutation . We will describe a simple deterministic algorithm that computes  by reduction to the conventional -mismatch problem.

Let  be a fixed but arbitrary permutation (). Recall that . We define

Thus, . We have  and . Now define  and  such that

for all  (except those that take the indices ``out of range''). Observe that

where the first substring has length  and the second substring has length . From these definitions it now follows directly that

Thus, in order to determine which positions  have , we first construct , ,  and , and then we run a standard -mismatch algorithm on the pairs  and  and use the previous formula.

We can now finally give an overview of our randomised algorithm for the \skDecision problem. The steps are described in Algorithm~\ref{alg:random}. The overall running time and proof of correctness is given in Theorem~\ref{thm:rankmis} below. The algorithm makes one-sided errors and outputs a false match (incorrectly reports ) with constant probability per alignment. As we will see in the proof of Theorem~\ref{thm:rankmis}, by running the algorithm a logarithmic number of times drastically reduces the probability of an error occurring at one or more alignments.

\begin{algorithm}[t]
    \caption{Overview of randomised solution to \skDecision.
        \label{alg:random}}
    \begin{enumerate}
        \item Pick a cyclic permutation  uniformly at random.
        \item Construct the strings , ,  and .
        \item Run a -mismatch algorithm on the pairs  and  as a black box.
        \item Using the results from Step~3 and Equation~(\ref{eq:hamconcat}), compute  for all~.
        \item Any alignment  with  is declared to have .
    \end{enumerate}
    \vspace{-8pt}
\end{algorithm}


\begin{theorem}
    \label{thm:rankmis}
    For any choice of constant , \skDecision can be solved randomised in  (deterministic) time when . The algorithm makes only false-positive errors (incorrectly declares the Hamming distance is at most~). With probability at least , the algorithm is correct at every alignment.
\end{theorem}
\begin{proof}
    As discussed in Section~\ref{sec:permutations}, if  then we can use the deterministic algorithm from Section~\ref{sec:detkmis} and achieve time complexity of  and no errors. Therefore, we focus on the case that .

    We first consider correctness. It follows from the discussion above that Algorithm~\ref{alg:random} does indeed determine, for every alignment , whether . We first show that
\begin{enumerate}
        \item[(i)]  when ;
        \item[(ii)] the probability that  when   is at most .
    \end{enumerate}

    By Lemma~\ref{lem:TI-permup} we have that if  then . This proves property~(i). By Definition~\ref{dfn:TI-ktight},  if  for all permutations  that are -tight for . The permutation  is selected uniformly at random from  in Step~1, hence by Lemma~\ref{lem:k-tight} it is -tight for  with probability at least . This proves property~(ii). Note that we can apply Lemma~\ref{lem:k-tight} since we have assumed that  and .

    As Algorithm~\ref{alg:random} only makes false-positive errors, we can amplify the probability of giving correct outputs by repeating the algorithm. We repeat it  times, where  is a constant, and output any alignment which is reported by all repeats. More precisely, let  be some alignment such that . The probability that one run of Algorithm~\ref{alg:random} incorrectly reports position  as a match is at most . Thus, the probability that all runs output  as a match is at most

By the union bound over all positions , the probability of the multi-run algorithm outputting a false match in at least one alignment is at most  as required.

    We now consider the time complexity of Algorithm~\ref{alg:random} (without amplification). Step~1 requires only constant time to pick a permutation at random. Step~2 requires  time by inspection of the definitions. Step~3 makes two calls to a -mismatch algorithm. For both calls the input is a pattern of length  and a text of length . Using the fastest known -mismatch algorithm of Amir et al.~\cite{ALP:2004}, this step takes  time. Steps~4 and~5 require only scanning the output of Step~3 and therefore take  time. This gives a time complexity of  time. However, we repeat the algorithm  times to reduce the error probability, hence  is the total time complexity.
\end{proof}


\section{Discussion}\label{sec:discussion}

We have shown how to derive both new upper and lower bounds for a variety of pattern matching problems under polynomial transformations. In some cases we have improved on known results and in others introduced new problem definitions and solutions.  There remain however a number of open questions. First, we suspect that the true complexity of \LpolyWild is unresolved, particularly for higher polynomial transformations. For example, when  there exists a straightforward  time solution by considering the problem independently at each alignment.  It is also still uncertain if the normalised Hamming distance problem is \threeSUM-hard for polynomials of degree greater than one. For \skDecision, our fast randomised algorithm applies only when . However, our lower bound for the same problem applies to the case where we want to determine if the Hamming distance is at most . This leaves a range of values of  where the complexity is not yet determined.  It is also an interesting question whether our randomised solution can be efficiently modified to output the Hamming distance at each alignment rather than simply a decision about whether it is greater or less than  or indeed if a new fast method can be found for this problem which will allow the presence of wildcards in the input.

\section{Acknowledgements}
MJ and BS are both supported by the EPSRC. The authors are grateful to Philip Bille for very helpful discussions on the topic of \threeSUM and related problems and their application to lower bounds for pattern matching problems.

\printbibliography

\end{document}
