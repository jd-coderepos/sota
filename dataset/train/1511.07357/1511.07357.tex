

\documentclass[12pt]{article}\usepackage[cm]{fullpage}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}\usepackage{bm}\usepackage{paralist}\usepackage{picins}\usepackage{mleftright}\usepackage{stmaryrd}\usepackage{algorithm}\usepackage{algorithmic}\usepackage{esvect}
\usepackage{scalerel}

\usepackage{xspace}\usepackage{ifpdf}\usepackage{graphicx}\usepackage[normalem]{ulem} \usepackage{caption}\usepackage{amsmath}\usepackage{amssymb}\usepackage[amsmath,thmmarks]{ntheorem}\theoremseparator{.}



\usepackage{euscript}
\usepackage{mathrsfs}

\usepackage{verbatim}

\usepackage{bbding}

\usepackage{titlesec}\titlelabel{\thetitle. }

\usepackage{bbm}\usepackage{bbding}


\usepackage{hyperref}\ifx\PDFBlackWhite\undefined \hypersetup{breaklinks,ocgcolorlinks, colorlinks=true,urlcolor=[rgb]{0.45,0.0,0.0},linkcolor=[rgb]{0.0,0.45,0.0},citecolor=[rgb]{0,0,0.45}}\fi

\allowdisplaybreaks

\newcommand{\etal}{\textit{et~al.}\xspace}
\newcommand{\eps}{\Mh{\varepsilon}}



\newlength{\savedparindent}
\newcommand{\SaveIndent}{\setlength{\savedparindent}{\parindent}}
\newcommand{\RestoreIndent}{\setlength{\parindent}{\savedparindent}}

\newcommand{\Term}[1]{\textsf{#1}}
\newcommand{\emphind}[1]{\emph{#1}\index{#1}}
\definecolor{blue25}{rgb}{0, 0, 11}
\newcommand{\emphic}[2]{\textcolor{blue25}{\textbf{\emph{#1}}}\index{#2}}

\ifx\PDFBlackWhite\undefined
\else
\renewcommand{\emphic}[2]{\textbf{\emph{#1}}}
\fi

\newcommand{\emphi}[1]{\emphic{#1}{#1}}
\newcommand{\emphOnly}[1]{\emph{\textcolor{blue25}{\textbf{#1}}}}



\newcommand{\cardin}[1]{\left| {#1} \right|}\newcommand{\ceil}[1]{\left\lceil {#1} \right\rceil}
\newcommand{\pth}[1]{\mleft({#1}\mright)}
\newcommand{\brc}[1]{\left\{ {#1} \right\}}
\newcommand{\setof}[1]{\left\{ {#1} \right\}}
\newcommand{\Set}[2]{\left\{ #1 \;\middle\vert\; #2 \right\}}
\newcommand{\pbrc}[1]{\mleft[ {#1} \mright]}

\newcommand{\Ex}[1]{\mathop{\mathbf{E}}\pbrc{#1}}
\newcommand{\ExCond}[2]{\mathop{\mathbf{E}}\left[ #1 \;\middle\vert\; #2 \right]}\newcommand{\EX}[1]{\mathop{\mathbf{E}}\Big[ #1 \Big]}
\newcommand{\Prob}[1]{\mathop{\mathbf{Pr}}\pbrc{#1}}

\newcommand{\remove}[1]{}



\newtheorem{theorem}{Theorem}[subsection]\newtheorem{lemma}[theorem]{Lemma}\newtheorem{claim}[theorem]{Claim}\newtheorem*{restate*}[theorem]{Restatement of }\newtheorem{corollary}[theorem]{Corollary}

\newtheorem{conjecture}[theorem]{Conjecture}

\newtheorem{observation}[theorem]{Observation}

\newcommand{\myqedsymbol}{\rule{2mm}{2mm}}
\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}


\theoremstyle{remark}\theoremheaderfont{\sf}\theorembodyfont{\upshape}\newtheorem{defn}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\newtheorem{remark}[theorem]{Remark}\newtheorem{example}[theorem]{Example}\newtheorem*{remark:unnumbered}[theorem]{Remark}


\theoremheaderfont{\em}\theorembodyfont{\upshape}\theoremstyle{nonumberplain}\theoremseparator{}\theoremsymbol{\myqedsymbol}\newtheorem{proof}{Proof:}\newtheorem{proofNoColon}{Proof}




\numberwithin{figure}{section}\numberwithin{table}{section}\numberwithin{equation}{section}




\newcommand{\HLinkPageSuffix}[3]{\hyperref[#2]{#1\ref*{#2}#3}}\newcommand{\HLinkSuffix}[3]{\hyperref[#2]{#1\ref*{#2}{#3}}}
\newcommand{\HLinkShort}[2]{\hyperref[#2]{#1\ref*{#2}}}
\newcommand{\HLink}[2]{\hyperref[#2]{#1~\ref*{#2}}}
\newcommand{\HLinkPage}[2]{\hyperref[#2]{#1~\ref*{#2}}}

\newcommand{\eqlab}[1]{\label{equation:#1}}\newcommand{\Eqref}[1]{\HLinkSuffix{Eq.~(}{equation:#1}{)}}
\newcommand{\Eqrefpage}[1]{\HLinkPageSuffix{Eq.~(}{equation:#1}{)}}
\newcommand{\eqrefpar}[1]{\hyperref[equation:#1]{(\ref*{equation:#1})}} 

\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\figref}[1]{\HLink{Figure}{fig:#1}}
\newcommand{\figrefpage}[1]{\HLinkPage{Figure}{fig:#1}}

\newcommand{\seclab}[1]{\label{sec:#1}} \newcommand{\secref}[1]{\HLink{Section}{sec:#1}} \newcommand{\secrefpage}[1]{\HLinkPage{Section}{sec:#1}} 

\newcommand{\corlab}[1]{\label{cor:#1}}
\newcommand{\corref}[1]{\HLink{Corollary}{cor:#1}}\newcommand{\correfshort}[1]{\HLinkShort{C}{cor:#1}}\newcommand{\correfpage}[1]{\HLinkPage{Corollary}{cor:#1}}

\providecommand{\deflab}[1]{\label{def:#1}}
\newcommand{\defref}[1]{\HLink{Definition}{def:#1}}
\newcommand{\defrefpage}[1]{\HLinkPage{Definition}{def:#1}}

\newcommand{\apndlab}[1]{\label{apnd:#1}}
\newcommand{\apndref}[1]{\HLink{Appendix}{apnd:#1}}
\newcommand{\apndrefpage}[1]{\HLinkPage{Appendix}{apnd:#1}}

\newcommand{\exmlab}[1]{\label{example:#1}}
\newcommand{\exmref}[1]{\HLink{Example}{example:#1}}

\newcommand{\alglab}[1]{\label{Algorithm:#1}}\newcommand{\algref}[1]{\HLink{Algorithm}{Algorithm:#1}}

\newcommand{\itemlab}[1]{\label{item:#1}}
\newcommand{\itemref}[1]{\HLinkSuffix{(}{item:#1}{)}}

\newcommand{\assumplab}[1]{\label{assumption:#1}}\newcommand{\assumpref}[1]{\HLink{Assumption}{assumption:#1}}



\newcommand{\lemlab}[1]{\label{lemma:#1}}
\newcommand{\lemref}[1]{\HLink{Lemma}{lemma:#1}}
\newcommand{\lemrefpage}[1]{\HLinkPage{Lemma}{lemma:#1}}
\newcommand{\lemrefshort}[1]{\HLinkShort{L}{lemma:#1}}

\newcommand{\itemrefpage}[1]{\HLinkPageSuffix{(}{item:#1}{)}}

\newcommand{\remlab}[1]{\label{rem:#1}}
\newcommand{\remref}[1]{\HLink{Remark}{rem:#1}}
\newcommand{\remrefpage}[1]{\HLinkPage{Remark}{rem:#1}}
\newcommand{\remrefshort}[1]{\HLinkShort{R}{rem:#1}}

\newcommand{\obslab}[1]{\label{observation:#1}}
\newcommand{\obsref}[1]{\HLink{Observation}{observation:#1}}
\newcommand{\obsrefshort}[1]{\HLinkShort{O}{observation:#1}}
\newcommand{\obsrefpage}[1]{\HLinkPage{Observation}{observation:#1}}

\newcommand{\thmlab}[1]{{\label{theo:#1}}}
\newcommand{\thmref}[1]{\HLink{Theorem}{theo:#1}}
\newcommand{\thmrefpage}[1]{\HLinkPage{Theorem}{theo:#1}}
\newcommand{\thmrefshort}[1]{\HLinkShort{T}{theo:#1}}

\newcommand{\pr}{\Mh{\tau}}

\newcommand{\ETH}{\Term{ETH}\xspace}
\newcommand{\SETH}{\Term{SETH}\xspace}
\newcommand{\TrSAT}{\ProblemC{SAT}\xspace}
\newcommand{\kSAT}{\ProblemC{SAT}\xspace}


\newcommand{\opt}{\mathrm{Opt}}


\providecommand{\Mh}[1]{{#1}}

\renewcommand{\th}{th\xspace}

\newcommand{\ds}{\displaystyle}\renewcommand{\Re}{{\mathbb{R}}}
\newcommand{\reals}{\mathbb{R}} 

\newcommand{\ANN}{\Term{ANN}\xspace}\newcommand{\NN}{\Term{NN}\xspace}

\newcommand{\PntSet}{\ensuremath{\Mh{P}}\xspace}\newcommand{\PntSetA}{\ensuremath{\Mh{Q}}\xspace}

\newcommand{\Hd}{\Mh{\mathbb{H}^d}}\newcommand{\N}{\Mh{\mathcal{N}}}\newcommand{\Family}{\Mh{\mathcal{H}}}

\newcommand{\colY}[2]{\Mh{\mathcal{C}}_{#2}\pth{#1}}

\newcommand{\Ni}[1]{\Mh{\mathsf{s}}\pth{#1}}
\newcommand{\cpi}[1]{\Mh{\mathsf{f}}_{#1}}

\newcommand{\sCi}[1]{\Mh{\gamma}_{#1}}
\newcommand{\nCollX}[1]{\Mh{\Gamma}_{#1}}

\newcommand{\distHC}{\mathrm{d}_H}
\newcommand{\distH}[2]{\mathrm{d}_H\pth{#1, #2}}

\newcommand{\Pclose}{\PntSet_{\leq}}
\newcommand{\Pfar}{\PntSet_{>}}
\newcommand{\constA}{\Mh{\mathsf{c}}}\newcommand{\constB}{\Mh{\mathsf{c}_1}}\newcommand{\ballY}[2]{\Mh{\mathrm{b}}\pth{#1,#2}}


\newcommand{\sortX}[1]{\mathrm{sort}\pth{#1}}\newcommand{\tailY}[2]{\Mh{#1^{}_{\bbslash #2}}}

\newcommand{\cCoord}{\Mh{\alpha}}\newcommand{\cTimes}{\Mh{\beta}}\newcommand{\cDSTimes}{\Mh{\delta}}\newcommand{\nnConst}{\Mh{c}}

\newcommand{\p}{\Mh{\rho}}\newcommand{\Lp}{\Mh{L}_{\p}}

\newcommand{\ts}{\hspace{0.6pt}}
\newcommand{\DA}{\Mh{D}}\newcommand{\DSTimes}{\Mh{L}}


\newcommand{\rr}{\Mh{r}}\newcommand{\mLight}{\Mh{r}}\newcommand{\mLightA}{\Mh{\widehat{r}}}

\newcommand{\RR}{\Mh{R}}

\newcommand{\RRA}{\Mh{{U}}}

\newcommand{\cHeavyVal}{\Mh{17}}\newcommand{\cHeavy}{\Mh{c_{\mathrm{h}}}}

\newcommand{\cLight}{\Mh{c_{\mathrm{l}}}}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\newcommand{\subseq}{\Mh{{\mathpzc{m}}}}

\newcommand{\subseqA}{\Mh{{\mathpzc{n}}}}

\newcommand{\seq}{\bm{\Mh{M}}}\newcommand{\seqc}{{\Mh{M}}}\newcommand{\seqA}{\Mh{{\bm{N}}}}

\newcommand{\pnt}{\Mh{\bm{x}}}\newcommand{\pntc}{\Mh{{x}}}\newcommand{\nnpnt}{\Mh{\bm{n}}}\newcommand{\rmC}[2]{{#1}^{}_{\setminus #2}}


\newcommand{\truncY}[2]{{#1}_{\leq #2}}\newcommand{\bmu}{\bm{\mu}}\newcommand{\bsigma}{\bm{\sigma}}

\newcommand{\pntA}{\Mh{\bm{v}}}\newcommand{\pntAc}{\Mh{{v}}}

\newcommand{\pntB}{\Mh{\bm{z}}}\newcommand{\nnfold}[2]{\mathsf{nn}^{}_{\bbslash #1}\pth{ #2}}

\newcommand{\tTimes}{\Mh{t}}

\newcommand{\query}{\Mh{\bm{q}}}\newcommand{\qc}{\Mh{{q}}}

\newcommand{\threshold}{\Mh{\psi}}

\newcommand{\est}{\mathrm{est}}
\newcommand{\med}{\mathrm{med}}
\newcommand{\dist}{\mathrm{dist}}

\newcommand{\snorm}[2]{\left\| \smash{#2} \right\|_{#1}}
\newcommand{\norm}[2]{\left\| {#2} \right\|_{#1}}
\newcommand{\normB}[1]{\left\| {#1} \right\|}
\newcommand{\normT}[3]{\norm{#1}{#3_{\leq #2}}}\newcommand{\Var}[1]{\mathop{\mathbf{V}}\pbrc{#1}}

\newcommand{\Event}{\Mh{\mathcal{E}}}

\newcommand{\sol}{\mathrm{sol}}

\newcommand{\QTimeY}[2]{\Mh{T_Q}\pth{#1, #2}}

\newcommand{\atgen}{\symbol{'100}}

\newcommand{\SarielThanks}[1]{\thanks{Department of Computer Science; University of Illinois; 201 N. Goodwin Avenue; Urbana, IL, 61801, USA; {\tt \si{sariel}\atgen{}\si{illinois.edu}}; {\tt \url{http://sarielhp.org}.} #1 } 
}

\newcommand{\SepidehThanks}[1]{\thanks{Department of EECS; MIT; 77 Massachusetts Avenue, Cambridge, MA 02139, USA;
      {\tt mahabadi@mit.edu}. #1 
   }}

\newcommand{\si}[1]{#1}\newcommand{\IntRange}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\DistD}[1]{\Mh{\mathcal{D}}_{#1}} 
\newcommand{\DistW}{\Mh{\DistD{\vecW}}}
\newcommand{\DistU}{\Mh{\DistD{\vecU}}}
\newcommand{\DistC}{\Mh{\DistD{\vecC}}}
\newcommand{\DistCX}[1]{\Mh{\DistD{\vecC/#1}}}
\newcommand{\DistCY}[2]{\Mh{\DistD{\vecC/#1}^{#2}}}

\newcommand{\nfrac}[2]{#1/#2}

\newcommand{\tldO}{\scalerel*{\widetilde{O}}{j^2}}\newcommand{\tldOmega}{\scalerel*{\widetilde{\Omega}}{j^2}}

\newcommand{\val}{\Mh{\nu}}\newcommand{\trc}{\Mh{\mathrm{trc}}}\newcommand{\trcZ}[3]{\trc^{}_{#1, #2}\pth{ #3}}

\newcommand{\DistB}[1]{\Mh{\EuScript{B}}_{#1}} 
\newcommand{\DistBY}[2]{\Mh{\DistB{\vecC/#1}^{#2}}}

\newcommand{\NNV}[1]{#1^{\Mh{*}}}\newcommand{\nnq}{\NNV{\query}}

\newcommand{\vTh}{\bm{\threshold}}\newcommand{\vecW}{\Mh{\bm{w}}} 

\newcommand{\vecU}{\Mh{\bm{u}}} 

\newcommand{\uc}{\Mh{u}}

\newcommand{\AdmS}{\Mh{\mathcal{G}}}\newcommand{\keepX}[1]{\Mh{{k}}\pth{#1}}

\newcommand{\wc}{\Mh{w}}

\newcommand{\nnNotation}{\Mh{\mathsf{nn}}}\newcommand{\nnCZ}[3]{\nnNotation^{}_{#1}\pth{#2, #3}}\newcommand{\nnCY}[2]{\nnNotation^{}_{#1}\pth{#2}}\newcommand{\nnCX}[1]{\nnNotation\pth{#1}}

\DefineNamedColor{named}{OliveGreen}{cmyk}{0.64, 0, 0.95, 0.40}
\definecolor{OliveGreen}{cmyk}{0.64, 0, 0.95, 0.40 }

\providecommand{\ProblemC}[1]{\textsf{#1}}

\providecommand{\ComplexityClass}[1]{{{{\textsc{#1}}}}}
\providecommand{\NPHard}{\ComplexityClass{NP-Hard}\xspace}
\providecommand{\NPComplete}{\ComplexityClass{NP-Complete}\xspace}

\newcommand{\PTAS}{\Term{PTAS}\xspace}\newcommand{\LSH}{\Term{LSH}\xspace}

\newcommand{\dnnY}[2]{\Mh{\mathsf{d}_{#1}\pth{#2}}}\newcommand{\dnnZ}[3]{\Mh{\mathsf{d}_{#1}\pth{#2,#3}}}\newcommand{\daZ}[3]{\Mh{\mathsf{d}_{#1}\pth{#2,#3}}}

\newcommand{\CSet}{\Mh{I}}
\newcommand{\cCSet}{\Mh{\overline{I}}}

\newcommand{\DSANN}{\Term{DANN}\xspace}\newcommand{\DS}{\Term{DS}\xspace}

\newcommand{\badCoords}{\Mh{B}}
\newcommand{\goodCoords}{\Mh{G}}\newcommand{\cc}{\Mh{\xi}}\newcommand{\vecC}{\Mh{\bm{\xi}}}\newcommand{\Neg}[1]{\overline{#1}}

\newcommand{\TANN}{\mathcal{T}_{\ANN}}





\IfFileExists{sariel_computer.sty}{\def\sarielComp{1}}{}
\ifx\sarielComp\undefined \newcommand{\SarielComp}[1]{}
\newcommand{\NotSarielComp}[1]{#1}\else
\newcommand{\SarielComp}[1]{#1}\newcommand{\NotSarielComp}[1]{}\fi

\SarielComp{\ifx\colorMath\undefined \else
   \DefineNamedColor{named}{ColorMath}{rgb}{0.5,0.2,0}
       \renewcommand{\Mh}[1]{{\textcolor{ColorMath}{#1}}}\fi
}

\newcommand{\XSays}[2]{{ {\fbox{\tt #1:}
      } \tt \textcolor{red}{#2} \marginpar{\textcolor{red}{#1}}
      {\fbox{\tt
            end}} } }
\newcommand{\sariel}[1]{{\XSays{Sariel}{#1}}}






\title{Proximity in the Age of Distraction:\\Robust Approximate Nearest Neighbor Search}

\author{Sariel Har-Peled\SarielThanks{Work on this paper was partially supported by a NSF AF awards
      CCF-0915984 and CCF-1217462.}\and Sepideh Mahabadi\SepidehThanks{This work was supported in part by NSF grant \si{IIS} -1447476.}}

\begin{document}
\clearpage \maketitle


\begin{abstract}
    We introduce a new variant of the nearest neighbor search problem,
    which allows for some coordinates of the dataset to be arbitrarily
    corrupted or unknown.  Formally, given a dataset of  points
     in high-dimensions, and a
    parameter , the goal is to preprocess the dataset, such that
    given a query point , one can compute quickly a point
    , such that the distance of the query to the
    point  is minimized, when ignoring the ``optimal'' 
    coordinates. Note, that the coordinates being ignored are a
    function of both the query point and the point returned.

    We present a general reduction from this problem to answering \ANN
    queries, which is similar in spirit to \LSH (locality sensitive
    hashing) \cite{im-anntr-98}.  Specifically, we give a sampling
    technique which achieves a bi-criterion approximation for this
    problem. If the distance to the nearest neighbor after ignoring
     coordinates is , the data-structure returns a point that
    is within a distance of  after ignoring 
    coordinates.  We also present other applications and further
    extensions and refinements of the above result.

    The new data-structures are simple and (arguably) elegant, and
    should be practical -- specifically, all bounds are polynomial in
    all relevant parameters (including the dimension of the space, and
    the robustness parameter ).
\end{abstract}




\section{Introduction}


The \emph{nearest neighbor} problem (\NN) is a fundamental geometric
problem which has major applications in many areas such as databases,
computer vision, pattern recognition, information retrieval, and many
others. Given a set  of  points in a -dimensional
space, the goal is to build a data-structure, such that given a query
point , the algorithm can report the closest point in
 to the query . A particularly interesting and
well-studied instance is when the points live in a -dimensional
real vector space .  Efficient exact and approximate algorithms
are known for this problem.  (In the \emph{approximate nearest
   neighbor} (\ANN) problem, the algorithm is allowed to report a
point whose distance to the query is larger by at most a factor of
, than the real distance to the nearest point.)  See
\cite{amnsw-oaann-98, k-tanns-97, him-anntr-12, kor-esann-00,
   h-rvdnl-01, kl-nnsap-04, diim-lshsb-04, cr-orcpl-10, p-ebnns-06,
   ac-annfjl-06, ai-nohaa-08, ainr-blsh-14, ar-oddha-15}, the surveys
\cite{s-fmmds-05}, \cite{i-nnhds-04} and \cite{sdi-nnmlv-06}, and
references therein (of course, this list is by no means exhaustive).
One of the state of the art algorithms for \ANN, based on Locality
Sensitive Hashing, finds the -\ANN with query time
, and preprocessing/space  in ,
where  \cite{im-anntr-98}, where  hides a
constant that is polynomial in . For the  norm, this
improves to  \cite{ai-nohaa-08}.

Despite the ubiquity of nearest neighbor methods, the vast majority of
current algorithms suffer from significant limitations when applied to
data sets with corrupt, noisy, irrelevant or incomplete data.  This is
unfortunate since in the real world, rarely one can acquire data
without some noise embedded in it. This could be because the data is
based on real world measurements, which are inherently noisy, or the
data describe complicated entities and properties that might be
irrelevant for the task at hand.

In this paper, we address this issue by formulating and solving a
variant of the nearest neighbor problem that allows for some data
coordinates to be arbitrarily corrupt.  Given a parameter , the
\emphi{-Robust Nearest Neighbor} for a query point , is a
point  whose distance to the query point is
minimized ignoring ``the optimal'' set of -coordinates (the term
`robust' \ANN is used as an analogy to \emph{Robust PCA}
\cite{clmw-rpca-11}).  That is, the  coordinates are chosen so that
deleting these coordinates, from both  and  minimizes
the distance between them. In other words, the problem is to solve the
\ANN problem in a different space (which is definitely not a metric),
where the distance between any two points is computed ignoring the
worst  coordinates.  To the best of our knowledge, this is the
first paper considering this formulation of the Robust \ANN problem.

This problem has natural applications in various fields such as
computer vision, information retrieval, etc.  In these applications,
the value of some of the coordinates (either in the dataset points or
the query point) might be either corrupted, unknown, or simply
irrelevant.  In computer vision, examples include image de-noising
where some percent of the pixels are corrupted, or image retrieval
under partial occlusion (e.g. see \cite{he-scump-07}), where some part
of the query or the dataset image has been occluded.  In these
applications there exists a perfect match for the query after we
ignore some dimensions.  Also, in medical data and recommender
systems, due to incomplete data \cite{swcsr-mimde-09,
   cfvrs-mdmdi-13, wcnk-shmde-13}, not all the features (coordinates)
are known for all the people/recommendations (points), and moreover,
the set of known values differ for each point.  Hence, the goal is to
find the perfect match for the query ignoring some of those features.

For the binary hypercube, under the Hamming distance, the -robust
nearest neighbor problem is equivalent to the \emph{near neighbor}
problem. The near neighbor problem is the decision version of the \ANN
search, where a radius  is given to the data structure in
advance, and the goal is to report a point that is within distance
 of the query point. Indeed, there exists a point  within
distance  of the query point  if and only if 
coordinates can be ignored such that the distance between  and
 is zero.

\paragraph{Budgeted Version.}

We also consider the weighted generalization of the problem where the
amount of uncertainty varies for each feature. In this model, each
coordinate is assigned a weight  in advance, which
tries to capture the certainty level about the value of the coordinate
( indicates that the value of the coordinate is correct and
 indicates that it can not be trusted). The goal is to ignore a
set of coordinates  of total weight at most , and find
a point , such that the distance of the query to the
point  is minimized ignoring the coordinates in
. Surprisingly, even computing the distance between two
points under this measure is \NPComplete (it is almost an instance of
\ProblemC{Min Knapsack}).


\SaveIndent 



\subsection{Results}
We present the following new results:
\begin{compactenum}[(A)]\item \textsc{Reduction from Robust \ANN to \ANN.}  We present a general reduction from the robust \ANN problem to the
    ``standard'' \ANN problem.  This results in a bi-criterion
    constant factor approximation, with sublinear query time, for the
    -robust nearest neighbor problem.
    
    \begin{compactenum}[(I)]
        \item For {-norm} the result can be stated as follows. If
        there exists a point  whose distance to the query point
         is at most  by ignoring  coordinates, the new
        algorithm would report a point  whose distance to the
        query point is at most , ignoring 
        coordinates. The query algorithm performs  \ANN
        queries in -\ANN data structures, where 
        is a prespecified parameter.

        \item In \secref{sec:lp:case}, we present the above result in
        the somewhat more general settings of the  norm.  The
        algorithm reports a point whose distance is within
         after ignoring
         coordinates while
        performing  of -\ANN queries. 
    \end{compactenum}


    \smallskip \item \textsc{-approximation.}  We modify the new algorithm to report a point whose distance to
    the query point is within  by ignoring
     coordinates while performing
     \ANN queries (specifically,
    -\ANN). For the sake of simplicity of exposition, we
    present this extension only in the  norm. See
    \apndref{eps:approx} for details.

    \smallskip \item \textsc{Budgeted version.}  In \secref{sec:budgeted}, we generalize our algorithm for the
    weighted case of the problem. If there exists a point within
    distance  of the query point by ignoring a set of coordinates
    of weight at most , then our algorithm would report a point
    whose distance to the query is at most  by ignoring a set
    of coordinates of weight at most . Again, for the sake of
    simplicity of exposition, we present this extension only in the
     norm.


    \smallskip \item \textsc{Data sensitive L{S}H queries.} It is a well known phenomenon in proximity search (e.g. see Andoni
    \etal~\cite[Section 4.5]{adiim-lshus-06}), that many
    data-structures perform dramatically better than their theoretical
    analysis. Not only that, but also they find the real nearest
    neighbor early in the search process, and then spend quite a bit
    of time on proving that this point is indeed a good \ANN. It is
    natural then to ask whether one can modify proximity search
    data-structures to take an advantage of such a behavior. That is,
    if the query is easy, the data-structure should answer quickly
    (and maybe even provide the exact nearest neighbor in such a
    case), but if the instance is hard, then the data-structure works
    significantly harder to answer the query.
    
    As an application of our sampling approach, we show a
    data-sensitive algorithm for \LSH for the binary hypercube case
    under the Hamming distance. The new algorithm solves the
    Approximate Near Neighbor problem, in time
    , where  is the smallest
    value with 
    where  is the distance of the query 
    to the \th point , and  is the
    distance being tested.

    We also get that such \LSH queries works quickly on low
    dimensional data, see \remref{l:dim} for details.
\end{compactenum}\smallskip The new algorithms are clean and should be practical.  Moreover, our
results for the -robust \ANN hold for a wide range of the parameter
, from  to .





\subsection{Related Work}
There has been a large body of research focused on adapting widely
used methods for high-dimensional data processing to make them
applicable to corrupt or irrelevant data. For example, Robust PCA
\cite{clmw-rpca-11} is an adaptation of the PCA algorithm that handles
a limited amount of adversarial errors in the input matrix. Although
similar in spirit, those approaches follow a different technical
development than the one in this paper.

Also, similar approaches to robustness has been used in theoretical
works. In the work of Indyk on  sketching \cite{i-sdpge-06}, the
distance between two points  and , is defined to be the median
of . Thus, it is required to compute the
 norm of their distance, but only over the smallest 
coordinates.

Finally, several generalizations of the \ANN problem have been
considered in the literature. Two related generalizations are the
Nearest -flat Search and the Approximate Nearest -flat. In the
former, the dataset is a set of -flats (-dimensional affine
subspace) instead of simple points but the query is still a point (see
\cite{m-anlsh-15} for example). In the latter however, the dataset
consists of a set of points but the query is now a -flat (see for
example \cite{aikn-alnnh-09, mnss-akfnn-14}). We note that our problem
cannot be solved using these variations (at least naively) since the
set of coordinates that are being ignored in our problem are not
specified in advance and varies for each query. This would mean that
 different subspaces are to be considered for each point.
In our settings, both  and  can be quite large, and the new
data-structures have polynomial dependency in both parameters.

\paragraph{Data sensitive \LSH.}

The fast query time for low dimensional data was demonstrated before
for an \LSH scheme \cite[Appendix A]{diim-lshsb-04} (in our case, this
is an easy consequence of our data-structure).  Similarly, optimizing
the parameters of the \LSH construction to the hardness of the
data/queries was done before \cite[Section 4.3.1]{adiim-lshus-06} --
our result however does this on the fly for the query, depending on
the query itself, instead of doing this fine tuning of the whole
data-structure in advance for all queries.

\subsection{Techniques}

By definition of the problem, we cannot directly apply
Johnson-Lindenstrauss lemma to reduce the dimensions (in the 
norm case). Intuitively, dimension reduction has the reverse effect of
what we want -- it spreads the mass of a coordinate ``uniformly'' in
the projection's coordinates -- thus contaminating all projected
coordinates with noise from the ``bad'' coordinates.


The basic idea of our approach is to generate a set of random
projections, such that all of these random projections map far points
to far points (from the query point), and at least one of them
projects a close point to a close point. Thus, doing \ANN queries in
each of these projected point sets, generates a set of candidate
points, one of them is the desired robust \ANN.

Our basic approach is based on a simple sampling scheme, similar to
the Clarkson-Shor technique \cite{cs-arscg-89} and \LSH
\cite{him-anntr-12}.  The projection matrices we use are
\emph{probing} matrices. Every row contains a single non-zero entry,
thus every row copies one original coordinate, and potentially scales
it up by some constant.
  
\paragraph{A sketch of the technique:} 

Consider the case where we allow to drop  coordinates.  For a given
query point , it has a robust nearest neighbor
, such that there is a set  of 
``bad'' coordinates, such that the distance between  and
 is minimum if we ignore the  coordinates of 
(and this is minimum among all such choices).

We generate a projection matrix by picking the \th coordinate to be
present with probability , where  is some
constant, for .  Clearly, the probability that such a
projection matrix avoids picking the  bad coordinates is
. In particular, if
we repeat this process  times, where  is some
constant, then the resulting projection avoids picking any bad
coordinate with probability
. On the
other hand, imagine a ``bad'' point , such that one
has to remove, say,  coordinates before the
distance of the point to the query  is closer than the robust
\NN  (when ignoring only  coordinates). Furthermore, imagine
the case where picking any of these coordinates is fatal -- the value
in each one of these bad coordinates is so large, that choosing any of
these bad coordinates results in this bad point being mapped to a far
away point. Then, the probability that the projection fails to select
any of these bad coordinates is going to be roughly

Namely, somewhat informally, with decent probability all bad points
get mapped to faraway points, and the near point gets mapped to a
nearby point. Thus, with probability roughly
, doing a regular \ANN query on the
projected points, would return the desired \ANN. As such, repeating
this embedding  times, and returning
the best \ANN encountered would return the desired robust \ANN with
high probability.



\paragraph{The good, the bad, and the truncated.}

Ultimately, our technique works by probing the coordinates, trying to
detect the ``hidden'' mass of the distance of a bad point from the
query. The mass of such a distance might be concentrated in few
coordinates (say, a point has  coordinates with huge value in
them, but all other coordinates are equal to the query point) -- such
a point is arguably still relatively good, since ignoring slightly
more than the threshold  coordinates results in a point that is
pretty close by.

On the other hand, a point where one has to ignore a large number of
coordinates (say ) before it becomes reasonably close to the query
point is clearly bad in the sense of robustness. As such, our
data-structure would classify points, where one has to ignore slightly
more than  coordinates to get a small distance, as being close.

To capture this intuition, we want to bound the influence a single
coordinate has on the overall distance between the query and a
point. To this end, if the robust nearest neighbor distance, to
 when ignoring  coordinates, is , then we consider
capping the contribution of every coordinate, in the distance
computation, by a certain value, roughly, . Under this
\emph{truncation}, our data-structure returns a point that is 
away from the query point, where  is the distance to the
-robust \NN point.

Thus, our algorithm can be viewed as a bicriterion approximation
algorithm - it returns a point where one might have to ignore slightly
more coordinates than , but the resulting point is constant
approximation to the nearest-neighbor when ignoring  points.

In particular, a point that is still bad after such an aggressive
truncation, is amenable to the above random probing. By carefully
analyzing the variance of the resulting projections for such points,
we can prove that such points would be rejected by the \ANN
data-structure on the projected points.


\paragraph{Budgeted version.}

To solve the budgeted version of the problem we use a similar
technique to importance sampling. If the weight of a coordinate is
, then in the projection matrix, we sample the \th coordinate
with probability  and scale it by a factor of
. This would ensure that the set of ``bad" coordinates
are not sampled with probability . Again we repeat it
 times to get the desired bounds.


\paragraph{Data sensitive \LSH.}

The idea behind the data-sensitive \LSH, is that \LSH can be
interpreted as an estimator of the local density of the point set. In
particular, if the data set is sparse near the query point, not only
the \LSH data-structure would hit the nearest-neighbor point quickly
(assuming we are working in the right resolution), but furthermore,
the density estimation would tell us that this event happened. As
such, we can do the regular exponential search -- start with an
insensitive \LSH scheme (that is fast), and go on to use more
sensitive \LSH{}s, till the density estimation part tells us that we
are done. Of course, if all fails, the last \LSH data-structure used
is essentially the old \LSH scheme.



\section{Preliminaries}

\subsection{The problem}

\begin{defn}
    \deflab{tail}For a point , let  be a
    permutation of , such that
    
    For a parameter , the \emphi{-tail} of 
    is the point
    
    Note, that given  and , computing
     can be done in  time, by median selection.
\end{defn}

Thus, given two points , their distance (in the
-norm), ignoring the  worst coordinates (which we believe to
be noise), is .  Here, we are
interested in computing the nearest neighbor when one is allowed to
ignore  coordinates. Formally, we have the following:


\begin{defn}
    \deflab{k:fold:n:n}For parameters , , a set of points
    , and a query point ,
    the \emphi{-robust nearest-neighbor} to  in 
    is
    
    which can be computed, naively, in  time.
\end{defn}

\begin{defn}
    \deflab{r:m:c}For a point  and a set of coordinates
    , we define  to
    be a point in  which is obtained from
     by deleting the coordinates that are in .
\end{defn}






\subsubsection{Projecting a point set}





\begin{defn}
    \deflab{set:proj}Consider a sequence  of , not necessarily distinct,
    integers , where
    .  For a point
    , its
    \emphi{projection} by , denoted by , is the
    point .
    Similarly, the \emph{projection} of a point set
     by  is the point set
    .
Given a weight vector  the
    \emphi{weighted projection} by a sequence  of a point
     is
    .

    Note, that can interpret  as matrix with dimensions
    , where the \th row is zero everywhere except
    for having  at the \th coordinate (or  in the
    unweighted case), for . This is a restricted
    type of a projection matrix.
\end{defn}


\begin{defn}
    \deflab{t:splay}Given a probability , a natural way to create a
    projection, as defined above, is to include the \th coordinate,
    for , with probability . Let 
    denote the distribution of such projections.
    
    Given two sequences  and
    , let 
    denote the \emph{concatenated} sequence
    
    Let  denote the distribution resulting from
    concatenating  such independent sequences sampled from
    .  (I.e., we get a random projection matrix, which is
    the result of concatenating  independent projections.)
\end{defn}

Observe that for a point  and a projection
, the projected point  might
be higher dimensional than the original point  as it might
contain repeated coordinates of the original point.

\begin{remark}[{Compressing the projections}]
    \remlab{compress}Consider a projection  that was
    generated by the above process (for either the weighted or
    unweighted case). Note, that since we do not care about the order
    of the projected coordinates, one can encode  by counting
    for each coordinate , how many time it is
    being projected. As such, even if the range dimension of  is
    larger than , one can compute the projection of a point in
     time. One can also compute the distance between two such
    projected points in  times.
\end{remark}






\section{The -robust \ANN under the -norm}
\seclab{sec:lp:case}

In this section, we present an algorithm for approximating the
-robust nearest neighbor under the -norm, where  is some
prespecified fixed constant (say  or ). As usual in such
applications, we approximate the \th power of the -norm,
which is a sum of \th powers of the coordinates.

\subsection{The preprocessing and query algorithms}
\seclab{sec:lp:alg} 

\paragraph{Input.}
The input is a set  of  points in , and a parameter
.  Furthermore, we assume we have access to a data-structure that
answer (regular) -\ANN queries efficiently, where
 is a quality parameter associated with these
data-structures.


\paragraph{Preprocessing.}  Let  be three constants to be specified
shortly, such that .  We set
, , and
. We randomly and independently
pick  sequences

Next, the algorithm computes the point sets
, for , and
preprocesses each one of them for -approximate
nearest-neighbor queries for the -norm (), using
a standard data-structure for \ANN that supports this.  Let 
denote the resulting \ANN data-structure for , for
 (for example we can use the data structure of
Indyk and Motwani \cite{im-anntr-98,him-anntr-12} for the 
cases).

\paragraph{Query.} Given a query point , for ,
the algorithm computes the point , and its
\ANN  in  using the data-structure . Each
computed point  corresponds to an original point
. The algorithm returns the -robust nearest
neighbor to  (under the -norm) among
 via direct calculation.






\subsection{Analysis}

\subsubsection{Points: Truncated, light and heavy}
\begin{defn}
    \deflab{truncate}For a point , and a
    threshold , let
     be
    the \emphi{-truncated} point, where
    , for
    . In words, we max out every coordinate of  by
    the threshold .  As such, the
    \emph{-truncated -norm} of
     is
    
\end{defn}

\begin{defn}
    For parameters  and , a point  is
    \emphi{-light} if
    . Similarly, for a
    parameter , a point is
    \emphi{-heavy} if
    .
\end{defn}

Intuitively a light point can have only a few large coordinates.  The
following lemma shows that being light implies a small tail.

\begin{lemma}\lemlab{lp:light} For a number , if a point  is
    -light, then
    .
\end{lemma}
\begin{proof}
Let , and let  be the number of
    truncated coordinates in . If 
    then
    
    which is a contradiction. As such, all the non-zero coordinates of
     are present in , and
    we have that
    
\end{proof}

\subsubsection{On the probability of a heavy point to be sampled as
   light, and vice versa}


\begin{lemma}
    \lemlab{basic:exp:var} Let  be a point in , and
    consider a random , see
    \defref{t:splay}.  We have that
    , and
    
\end{lemma}
\begin{proof}
    Let  be a random variable that is 
    with probability  and  otherwise. For
    , we have that
    
As for the variance, we have
    
    As such, we have
     \end{proof}

\begin{lemma}\lemlab{lp:t:heavy}Let  be a point in , and let  be a
    number.  We have that
    .
\end{lemma}

\begin{proof}
Consider the -truncated point
    , see \defref{truncate}.  Each
    coordinate of  is smaller than , and thus
    
\end{proof}

\begin{lemma}
    \lemlab{lp:heavy:block}Consider a sequence .  If  is a
    -heavy point and
    , then
    
    where .
\end{lemma}
\begin{proof}
    Consider the -truncated point
    .  Since  is
    -heavy, we have that
    .  Now, setting
    , and using
    \lemref{basic:exp:var}, we have
    
    and
    
    by \lemref{lp:t:heavy}.  Now, we have that
    
    As such, by Chebyshev's inequality, and since
    , if , we
    have
    
\end{proof}


\begin{lemma}
    \lemlab{success}Let  be a prespecified point. The probability that a
    sequence  sampled from  does not sample
    any of the  heaviest coordinates of  is
    
    (for the simplicity of exposition, in the following, we use this
    rougher estimate).
\end{lemma}

\begin{proof}
    Let  be the set of  indices of the coordinates of 
    that are largest (in absolute value). The probability that
     does not contain any of these coordinates is
    , and overall this probability is
    
    Now, we have
    
    since, for any integer , we have
    , and
    , for .
\end{proof}


\begin{lemma}
    \lemlab{lp:light:good}Consider a point  such that
     (see
    \defref{tail}). Conditioned on the event of \lemref{success}, we
    have that
    , where .
\end{lemma}

\begin{proof}
    By \lemref{basic:exp:var} for
    , we have
    .
    The desired probability is , which
    holds by Markov's inequality.
\end{proof}

\begin{lemma}
    \lemlab{lp:heavy:far}Let . If  is a
    -heavy point, then
    
\end{lemma}
\begin{proof}Let  and
    , and for all , let
    . By
    \lemref{lp:heavy:block}, with probability at least half, we have
    that
    
    In particular, let , and
    observe that
    
    Thus, we have that
    
    Now set  and note that
    .  Now, by Hoeffding's inequality, we have
    that
    
\end{proof}

\subsubsection{Putting everything together}

\begin{lemma}
    \lemlab{lp:heavy:tail}
Let  be a parameter.  One can build the
    data-structure described in \secref{sec:lp:alg} with the following
    guarantees.  For a query point , let
     be its -robust nearest neighbor in 
    under the  norm, and let
    . Then, with high
    probability, the query algorithm returns a point
    , such that  is a
    -light.
    The data-structure performs  of
    -\ANN queries under -norm.
\end{lemma}

\begin{proof}
    We start with the painful tedium of binding the parameters.  For
    the bad probability, bounded by \lemref{lp:heavy:far}, to be
    smaller than , we set .  For the good
    probability  of \lemref{success} to be
    larger than , implies
    , thus requiring
    . Namely, we set
    .  Finally, \lemref{lp:heavy:far}
    requires
    
    Let  and
    let .
    
    For a query point , let  be its -robust \NN, and
    let  be the set of  largest coordinates in
    . Let  denote the event of sampling
    a projection  that does not
    contain any of the coordinates of .  By \lemref{success}, with
    probability ,
    the event  happens for the data-structure , for any
    .

    As such, since the number of such data-structures built is   
    
    we have that, by Chernoff inequality, with high probability, that
    there are at least  such data structures, say
    .


    Consider such a data-structure .  The idea is now to ignore
    the coordinates of  all together, and in particular, for a
    point , let  be the
    point where the  coordinates of  are removed (as defined in
    \defref{r:m:c}).  Since by assumption
    ,
    by \lemref{lp:light:good}, with probability at least half, the
    distance of  from  is at most
    .  Since there are
     such data-structures, we know that, with high
    probability, in one of them, say , this holds.  By
    \lemref{lp:heavy:far}, any point  (of ),
    that is -heavy, would be in distance
    at least
    
    in the projection  from the projected .  Since
     is a -\ANN data-structure under the
     norm, we conclude that no such point can be returned,
    because the distance from  to  in this
    data-structure is smaller than .
    Note that since for the reported point , the point
     cannot be
    -heavy, and that the
    coordinates in  can contribute at most
    . We conclude that
    the point  cannot be
    -heavy.  Thus, the
    data-structure returns the desired point with high probability.

    As for the query performance, the data-structure performs
     queries in -\ANN data-structures.
\end{proof}

This lemma would translate to the following theorem using
\lemref{lp:light}.

\subsection{The result}

\begin{theorem}
    \thmlab{l:p}
Let  be a set of  points with the
    underlying distance being the  metric, and ,
    , and  be parameters.  One
    can build a data-structure for answering the -robust \ANN
    queries on , with the following guarantees:
    \begin{compactenum}[\,\,(A)]
        \item Preprocessing time/space is equal to the space/time
        needed to store 
        data-structures for performing -\ANN queries
        under the  metric, for a set of  points in
         dimensions.


        \item The query time is dominated by the time it takes to
        perform -\ANN queries in the  \ANN
        data-structures.

        \item For a query point , the data-structure returns,
        with high probability, a point , such that
        if one ignores 
        
        coordinates, then the  distance between  and
         is at most
        
        where  is the distance of the nearest neighbor to
         when ignoring  coordinates.  (Formally,
         is
        -light.)
    \end{compactenum}
\end{theorem}

\begin{corollary}
    Setting , the algorithm would report a point 
    using -\ANN data-structures, such that if one ignores
     coordinates, the  distance between 
    and  is at most .
    Formally,  is
    -light.
\end{corollary}





\section{Budgeted version}
\seclab{sec:budgeted}
\subsection{Definition of the problem}

In this section, we consider the budgeted version of the problem for
-norm. Here, a coordinate  has a cost  of
ignoring it, and we have a budget of , of picking the coordinates
to ignore (note that since we can safely remove all coordinates of
cost , we can assume that ).  Formally, we have a
vector of \emphi{costs} , where
the \th coordinate, , is the cost of ignoring this
coordinate.  Intuitively, the cost of a coordinate shows how much we
are certain that the value of the coordinate is correct.

The set of \emphi{admissible projections}, is

Given two points , their \emphi{admissible distance} is

where we interpret  as a projection (see \defref{set:proj}).


The problem is to find for a query point  and a set of points
, both in , the \emph{robust nearest-neighbor
   distance} to ; that is,

The point in  realizing this distance is the \emphi{robust
   nearest-neighbor} to , denoted by

The unweighted version can be interpreted as solving the problem for
the case where all the coordinates have uniform cost .


\begin{defn}
    \deflab{good:bad}If  is the nearest-neighbor to  under the above
    measure, then the set of \emph{good} coordinates is
    
    and the set of \emphi{bad} coordinates is
    
\end{defn}



In what follows, we modify the algorithm for the unweighted case and
analyze its performance for the budgeted case. Interestingly, the
problem is significantly harder.

\subsubsection{Hardness and approximation of robust distance for two points}

For two points, computing their distance is a special instance of
\ProblemC{Min-Knapsack}. The problem is \NPHard (which is well known),
as testified by the following lemma.

\begin{lemma}
    \lemlab{2:points}Given two points , and a cost vector
    , computing
     is
    \NPComplete, where  is the set of admissible projections
    for  (see \Eqref{admissible}).
\end{lemma}

\begin{proof}
    This is well known, and we provide the proof for the sake of
    completeness.

    Consider an instance of \ProblemC{Partition} with integer numbers
    . Let , and consider
    the point , and set
    the cost vector to be . Observe that
    . In particular, there is a
    point in robust distance at most  from the origin, with the
    total cost of the omitted coordinates being   the given
    instance of \ProblemC{Partition} has a solution.

    Indeed, consider the set of coordinates  realizing
    .  Let
    , and observe that the
    cost of the omitted coordinates
    
    is at most  (by the definition of the admissible set
    ). In particular, we have 
    and . As such, the minimum possible value of 
    is , and if it is , then , and  and
     realize the desired partition.
\end{proof}



Adapting the standard \PTAS for subset-sum for this problem, readily
gives the following.

\begin{lemma}[\cite{i-aamkp-09}]
    \lemlab{i-aamkp}Given points , and a cost vector
    , one can compute a set
    , such that
    
    The running time of this algorithm is .
\end{lemma}

\subsubsection{Embedding with scaling}

Given a vector  , consider generating a sequence
 of integers , by picking the number
, into the sequence, with probability .  We
interpret this sequence, as in \defref{set:proj}, as a projection,
except that we further scale the \th coordinate, by a factor of
, for . Namely, we project the \th
coordinate with probability , and if so, we scale it up by a
factor of , for  (naturally, coordinates
with  would never be picked, and thus would never be
scaled). Let  denote this distribution of weighted
sequences (maybe a more natural interpolation is that this is a
distribution of projection matrices).

\begin{observation}
    \obslab{norm:1}Let  be a cost vector with non zero entries.
    For any point , and a random
    , we have that
    
\end{observation}



\subsection{Algorithm}

The input is a point set  of  points in .  Let
 be the vector of costs, and
 be three constants to be specified
shortly, such that .  Let
, and .

\paragraph{Preprocessing.}

We use the same algorithm as before.  We sample  sequences

Then we embed the point set using these projections, setting
, for . Next, we
preprocess the point set  for -\ANN queries under the
-norm, and let  be the resulting \ANN data-structure, for
.

\paragraph{Answering a query.} 

Given a query point , the algorithm performs a -\ANN query
for  in , this \ANN corresponds to some original
point , for . The algorithm
then -approximate the distance
, for ,
using the algorithm of \lemref{i-aamkp}, and returns the point
realizing the minimum distance as the desired \ANN. 

\subsection{The analysis}

\begin{lemma}
    \lemlab{budget:success} For a query point , let
    
    be the bad coordinates of  (thus
    ).  Then the probability that
     misses all the coordinates of
     is at least .
\end{lemma}

\begin{proof}
    Let  be the bad coordinates in . We
    have that . As such, the
    probability that  fails to sample
    a coordinate in  is
    
    as , and , for
    .  As such, the probability that a sequence
    
    avoids  is at least
    .
\end{proof}

Let  and let

be the distance from  to its nearest neighbor only considering
the coordinates in .


\begin{lemma}
    \lemlab{e:v:w}For a point , and a random
    , we have that
    
    and
    
\end{lemma}
\begin{proof}
    The claim on the expectation follows readily from
    \obsref{norm:1}. As for the variance, let  be a coordinate that
    has non-zero cost, and let  be a random variable that is
     with probability , and 
    otherwise. We have that
    
    As such for , we have that
    
    and thus the lemma follows.
\end{proof}


As before, we want to avoid giving too much weight to a single
coordinate which might have a huge (noisy) value in it. As such, we
truncate coordinates that are too large. Here, things become somewhat
more subtle, as we have to take into account the probability of a
coordinate to be picked.


\begin{defn}
    For a cost vector , a positive number ,
    and a point , let
    
    be the \emphi{truncated} point, where
    
    for .
\end{defn}


The truncation seems a bit strange on the first look, but note that a
coordinate  that has a cost  approaching , is going to be
truncated to zero by the above. Furthermore, it ensures that a
``heavy'' point would have a relatively small variance in the norm
under the projections we use, as testified by the following easy
lemma.

\begin{lemma}
    \lemlab{stupid}Consider a point , and a random
    . For,
    ,
    consider the random variable .  We
    have that
    
    
    and
    
\end{lemma}
\begin{proof}
    The bound on the expectation is by definition. As for the
    variance, by \lemref{e:v:w} and \Eqref{t:w}, we have
    
    
    Let , and . By Chebyshev's
    inequality, we have that
    
    By the above, we have that
    
\end{proof}

\begin{defn}
    \deflab{light:heavy} For a value , a point  is
    \emphi{-light} (resp.~\emphi{heavy}) if
     (resp.~),
    where .
\end{defn}


\begin{lemma}
    \lemlab{stupid2}Consider a point  that is
    -heavy, for , and a
    random . Then, we have that
    
\end{lemma}

\begin{proof}
    Let .  Let
    , and let  be an indicator
    variable that is one if . We have, by
    \lemref{stupid}, that
    
    as .  By Chernoff inequality, we have that
    
    since .

    In particular, we have that
    
    and as such
    
\end{proof}

\subsection{The result}

\begin{theorem}
    \thmlab{main:budgeted} Let  be a point set in , let 
    be the cost of the coordinates, and let  be a
    parameter. One can build a data-structure, such that given a query
    point , it can report a robust approximate
    nearest-neighbor under the costs of . Formally, if
     is the robust nearest-neighbor (see
    \Eqrefpage{n:n:n}) when one is allowed to drop coordinates of
    total cost , and its distance to this point is
     (see \Eqref{a:dist}), then the
    data-structure returns a point , such that 
    is -light (see
    \defref{light:heavy}). The data-structure has the following
    guarantees:
    \begin{compactenum}[\qquad(A)]
        \item The preprocessing time and space is
        , where  is the
        preprocessing time and space needed to build a single
        data-structure for answering (standard) -\ANN queries in
        the -norm for  points in  dimensions.

        \item The query time is
        , where  is the
        query time of answering -\ANN queries in the above \ANN
        data-structures.
    \end{compactenum}
\end{theorem}

\begin{proof}The proof is similar to \lemref{lp:heavy:tail} in the unweighted
    case. We set , , and
    . By the same arguments as the unweighted case, and
    using \lemref{budget:success}, \lemref{e:v:w}, and Markov's
    inequality, with high probability, there exists a data-structure
     that does not sample any of the bad coordinates
    , and that
    . By \lemref{stupid2} and
    union bound, for all the points  such that
     (see \defref{r:m:c}) is
    -heavy, we have
    . Thus by \lemref{i-aamkp} no such
    point would be retrieved by . Note that since for the
    reported point , we have that
     is -light,
    and that
    
    the point  is -light.  Using
    \lemref{i-aamkp} implies an additional  blowup in the
    computed distance, implying the result.
\end{proof}

\begin{corollary}
    Under the conditions and notations of \thmref{main:budgeted}, for
    the query point  and its returned point , there
    exists a set of coordinates  of cost
    at most  (i.e.,    
    
    such that .
    That is, we can remove a set of coordinates of cost at most 
    such that the distance of the reported point  from the query
     is at most .
\end{corollary}
\begin{proof}
    Let  and by \thmref{main:budgeted} 
    is -light for some constant . Let
     and let  be the set
    of coordinates being truncated (i.e., all  such that
    ). Clearly,
    the weight of the coordinates not being truncated is at most
    .
    Also for the coordinates in the set , we have that
    .  Therefore,
    
    assuming that , and noting that
    .
\end{proof}







\section{Application to data sensitive L{S}H queries}
\seclab{d:s:l}

Given a set of  points  and a radius parameter , in
the approximate near neighbor problem, one has to build a
data-structure, such that for any given query point , if there
exists a point in  that is within distance  of ,
it reports a point from  which is within distance
 of the query point. In what follows we present a data
structure based on our sampling technique whose performance depends on
the relative distances of the query from all the points in the
data-set.

\subsection{Data structure}

\paragraph{Input.}

The input is a set  of  points in the hamming space
, a radius parameter , and an approximation
parameter .

\begin{remark:unnumbered}
    In the spirit of \remref{compress}, one can generate the
    projections in this case directly.  Specifically, for any value of
    , consider a random projection
    
    Two points  \emphi{collide} under , if
    , for . Since we
    care only about collisions (and not distances) for the projected
    points, we only care what subset of the coordinates are being
    copied by this projection. That is, we can interpret this
    projection as being the projection , which can
    be sampled directly from , where .
    As such, computing  and storing it takes 
    time. Furthermore, for a point , computing
     takes  time, for any projection
    .
\end{remark:unnumbered}

\paragraph{Preprocessing.}

For , let

where  is a sufficiently large constant.  Here,  is
the \emph{collision probability function} of two points at distance
 under projection , and  is
the number times one has to repeat an experiment with success
probability  till it succeeds with high probability.
Let .

For , compute a set

of projections.  For each projection , we
compute the set  and store it in a hash table
dedicated to the projection . Thus, given a query point
, the set of points \emph{colliding} with  is
the set

stored as a linked list, with a single entry in the hash table of
.  Given , one can extract, using the hash table, in
 time, the list representing .  More
importantly, in  time, one can retrieve the size of this list;
that is, the number .  For any
, let  denote the constructed data-structure.

\paragraph{Query.}

Given a query point , the algorithm starts with ,
and computes, the number of points colliding with it in
. Formally, this is the number

If , the algorithm increases , and continues
to the next iteration, where  is any constant strictly larger
than .

Otherwise,  and the algorithm extracts from
the  hash tables (for the projections of ) the
lists of these  points, scans them, and returns the closest point
encountered in these lists.

The only remaining situation is when the algorithm had reached the
last data-structure for  without success. The algorithm
then extracts the collision lists as before, and it scans the lists,
stopping as soon as a point of distance  had been
encountered.  In this case, the scanning has to be somewhat more
careful -- the algorithm breaks the set of projections of 
into  blocks , each containing
 projections, see \Eqref{f:N}. The algorithm computes
the total size of the collision lists for each block, separately, and
sort the blocks in increasing order by the number of their
collisions. The algorithm now scans the collision lists of the blocks
in this order, with the same stopping condition as above.

\begin{remark}
    There are various modifications one can do to the above algorithm
    to improve its performance in practice. For example, when the
    algorithm retrieves the length of a collision list, it can also
    retrieve some random element in this list, and compute its
    distance to , and if this distance is smaller than
    , the algorithm can terminate and return this point
    as the desired approximate near-neighbor. However, the advantage
    of the variant presented above, is that there are many scenarios
    where it would return the \emph{exact} nearest-neighbor. See below
    for details.
\end{remark}


\subsection{Analysis}


\subsubsection{Intuition}

The expected number of collisions with the query point , for a single
, is

This quantity can be interpreted as a convolution over the point set.
Observe that as  is a monotonically decreasing function
of  (for a fixed ), we have that
.

The expected number of collisions with , for all the
projections of , is

If we were to be naive, and just scan the lists in the \th level,
the query time would be .  As such, if
, then we are ``happy'' since the query time is
small. Of course, a priori it is not clear whether  (or, more
specifically, ) is small.

Intuitively, the higher the value  is, the stronger the
data-structure ``pushes'' points away from the query point. If we are
lucky, and the nearest neighbor point is close, and the other points
are far, then we would need to push very little, to get  which is
relatively small, and get a fast query time. The standard \LSH
analysis works according to the worst case scenario, where one ends up
in the last layer .


\begin{example}[If the data is locally low dimensional]
    \exmlab{low:dim}The quantity  depends on how the data looks like near
    the query. For example, assume that locally near the query point,
    the point set looks like a uniform low dimensional point
    set. Specifically, assume that the number of points in distance
     from the query is bounded by
    , where  is some small
    constant and  is the distance of the nearest-neighbor to
    . We then have that
    
    By setting , we have
    
    Therefore, the algorithm would stop in expectation after 
    rounds. 

    Namely, if the data near the query point locally behaves like a
    low dimensional uniform set, then the expected query time is going
    to be , where the constant depends on the
    data-dimension .
\end{example}




\subsubsection{Analysis}
\begin{lemma}
    \lemlab{fast}If there exists a point within distance  of the query point,
    then the algorithm would compute, with high probability, a point
     which is within distance  of the query point.
\end{lemma}
\begin{proof}
    Let  be the nearest neighbor to the query
    .  For any data-structure , the probability that
     does not collide with  is at most
    
    Since the algorithm ultimately stops in one of these
    data-structures, and scans all the points colliding with the query
    point, this implies that the algorithm, with high probability,
    returns a point that is in distance .
\end{proof}

\begin{remark}
    An interesting consequence of \lemref{fast} is that if the
    data-structure stops before it arrives to , then it
    returns the \emph{exact} nearest-neighbor -- since the
    data-structure accepts approximation only in the last
    level. Stating it somewhat differently, only if the data-structure
    gets overwhelmed with collisions it returns an approximate answer.
\end{remark}


\begin{remark}
    \remlab{m:large}One can duplicate the coordinates  times, such that the
    original distance  becomes .  In particular, this can
    be simulated on the data-structure directly without effecting the
    performance. As such, in the following, it is safe to assume that
     is a sufficiently large -- say larger than .
\end{remark}

\begin{lemma}
    \lemlab{N:i}For any , we have .
\end{lemma}
\begin{proof}
    We have that
    
    since for any positive integer , we have
    .  As such, since we can
    assume that , we have that
    
    Now, we have
    
\end{proof}


\begin{lemma}
    \lemlab{worst:case}For a query point , the worst case query time is
    , with high probability.
\end{lemma}
\begin{proof}
    The worst query time is realized when the data-structure scans the
    points colliding under the functions of .
    
    We partition  into two point sets:
    \smallskip \begin{compactenum}[\qquad (i)]
        \item The close points are
        , and
        \item the far points are
        .
    \end{compactenum}
    \smallskip Any collision with a point of  during the scan terminates
    the algorithm execution, and is thus a \emph{good} collision. A
    \emph{bad} collision is when the colliding point belongs to
    .

    Let  be the partition of the projections of
     into blocks used by the algorithm.  For any , we
    have
    
    since  and by \lemref{N:i}. Such a block,
    has probability of
     to not have the
    nearest-neighbor to  (i.e., ) in its collision
    lists.  If this event happens, we refer to the block as being
    \emph{useless}.

    For a block , let  be the total size of the collision
    lists of  for the projections of  when ignoring good
    collisions altogether. We have that
    
    since . In particular, the \th block is
    \emph{heavy}, if . The probability for a
    block to be heavy, is , by Markov's inequality.

    In particular, the probability that a block is heavy or useless,
    is at most . As such, with high probability,
    there is a light and useful block. Since the algorithm scans the
    blocks by their collision lists size, it follows that with high
    probability, the algorithm scans only light blocks before it stops
    the scanning, which is caused by getting to a point that belongs
    to . As such, the query time of the algorithm is
    .
\end{proof}

Next, we analyze the data-dependent running time of the algorithm.
\begin{lemma}
    Let , for , where
    .  Let
     be the smallest value such that
    . Then, the
    expected query time of the algorithm is
    .
\end{lemma}
\begin{proof}
    The above condition implies that
    , for any .  By
    \Eqref{collisions}, for , we have that
    
    Thus, by Markov's inequality, with probability at least
    , we have that , and the
    algorithm would terminate in this iteration.  As such, let 
    be an indicator variable that is one of the algorithm reached the
    \th iteration. However, for that to happen, the algorithm has
    to fail in iterations .  As such,
    we have that
    

    The \th iteration of the algorithm, if it happens, takes
     time, and as such, the overall expected running
    time is proportional to
    
    Namely, the expected running time is bounded by
    
    using the bound  from
    \lemref{N:i}, and since .
\end{proof}

\subsection{The result}

\begin{theorem}
    \thmlab{L:S:H:sensitive}Given a set  of  points,
    and parameters  and , one can preprocess the point
    set, in  time and space, such
    that given a query point , one can decide
    (approximately) if , in
     expected time, where  is
    the Hamming distance. Formally, the data-structure returns,
    either: \smallskip \begin{compactitem}
        \item ``'', and the
        data-structure returns a witness , such that
        . This is the result
        returned if .
        
        \smallskip \item ``'', and this
        is the result returned if
        .
    \end{compactitem}
    \smallskip The data-structure is allowed to return either answer if
    .  The query
    returns the correct answer, with high probability.
    
    Furthermore, if the query is ``easy'', the data-structure would
    return the \emph{exact} nearest neighbor. Specifically, if
    , and there exists
    , such that
    ,
    then the data-structure would return the exact nearest-neighbor in
     expected time.
\end{theorem}


\begin{remark}
    \remlab{l:dim}If the data is  dimensional, in the sense of having bounded
    growth (see \exmref{low:dim}), then the above data-structure
    solves approximate \LSH in  time, where the constant
    hidden in the  depends (exponentially) on the data dimension
    .

    This result is known, see Datar \etal~\cite[Appendix
    A]{diim-lshsb-04}. However, our data-structure is more general, as
    it handles this case with no modification, while the
    data-structure of Datar \etal is specialized for this case.
\end{remark}

\begin{remark}
    \remlab{f:tune}Fine tuning the \LSH scheme to the hardness of the given data is
    not a new idea. In particular, Andoni \etal~\cite[Section
    4.3.1]{adiim-lshus-06} suggest fine tuning the \LSH construction
    parameters for the set of queries, to optimize the overall query
    time.

    Contrast this with the new data-structure of
    \thmref{L:S:H:sensitive}, which, conceptually, adapts the
    parameters on the fly during the query process, depending on how
    hard the query is.
\end{remark}







\section{Conclusions}

Ultimately, our data-structure is a prisoner of our underlying
technique of sampling coordinates. Thus, the main challenge is to come
up with a different approach that does not necessarily rely on such an
idea. In particular, our current technique does not work well for
points that are sparse, and have only few non-zero coordinates. We
believe that this problem should provide fertile ground for further
research.

\paragraph{Acknowledgments.}
The authors thank Piotr Indyk for insightful discussions about the
problem and also for the helpful comments on the presentation of this
paper.  The authors also thank \si{Jen} Gong, Stefanie Jegelka, and
Amin Sadeghi for useful discussions on the applications of this
problem.





\newcommand{\etalchar}[1]{}
 \providecommand{\CNFX}[1]{ {\em{\textrm{(#1)}}}}
  \providecommand{\tildegen}{{\protect\raisebox{-0.1cm}{\symbol{'176}\hspace{-0.03cm}}}}
  \providecommand{\SarielWWWPapersAddr}{http://sarielhp.org/p/}
  \providecommand{\SarielWWWPapers}{http://sarielhp.org/p/}
  \providecommand{\urlSarielPaper}[1]{\href{\SarielWWWPapersAddr/#1}{\SarielWWWPapers{}/#1}}
  \providecommand{\Badoiu}{B\u{a}doiu}
  \providecommand{\Barany}{B{\'a}r{\'a}ny}
  \providecommand{\Bronimman}{Br{\"o}nnimann}  \providecommand{\Erdos}{Erd{\H
  o}s}  \providecommand{\Gartner}{G{\"a}rtner}
  \providecommand{\Matousek}{Matou{\v s}ek}
  \providecommand{\Merigot}{M{\'{}e}rigot}
  \providecommand{\Hastad}{H\r{a}stad\xspace}
  \providecommand{\CNFCCCG}{\CNFX{CCCG}}
  \providecommand{\CNFBROADNETS}{\CNFX{BROADNETS}}
  \providecommand{\CNFESA}{\CNFX{ESA}}
  \providecommand{\CNFFSTTCS}{\CNFX{FSTTCS}}
  \providecommand{\CNFIJCAI}{\CNFX{IJCAI}}
  \providecommand{\CNFINFOCOM}{\CNFX{INFOCOM}}
  \providecommand{\CNFIPCO}{\CNFX{IPCO}}
  \providecommand{\CNFISAAC}{\CNFX{ISAAC}}
  \providecommand{\CNFLICS}{\CNFX{LICS}}
  \providecommand{\CNFPODS}{\CNFX{PODS}}
  \providecommand{\CNFSWAT}{\CNFX{SWAT}}
  \providecommand{\CNFWADS}{\CNFX{WADS}}
\begin{thebibliography}{CLMW11}

\bibitem[AC06]{ac-annfjl-06}
N.~Ailon and B.~Chazelle.
\newblock Approximate nearest neighbors and the fast {Johnson-Lindenstrauss}
  transform.
\newblock In {\em Proc. 38th Annu. ACM Sympos. Theory Comput. {\em(STOC)}},
  pages 557--563, 2006.
\newblock \href {http://dx.doi.org/10.1145/1132516.1132597}
  {\path{doi:10.1145/1132516.1132597}}.

\bibitem[ADI{\etalchar{+}}06]{adiim-lshus-06}
A.~Andoni, M.~Datar, N.~Immorlica, P.~Indyk, and V.~S. Mirrokni.
\newblock Locality-sensitive hashing using stable distribution.
\newblock In T.~Darrell, P.~Indyk, and G.~Shakhnarovich, editors, {\em
  Nearest-Neighbor Methods in Learning and Vision: Theory and Practice}, pages
  61--72. MIT Press, 2006.

\bibitem[AI08]{ai-nohaa-08}
A.~Andoni and P.~Indyk.
\newblock Near-optimal hashing algorithms for approximate nearest neighbor in
  high dimensions.
\newblock {\em Commun. ACM}, 51(1):117--122, 2008.
\newblock \href {http://dx.doi.org/10.1145/1327452.1327494}
  {\path{doi:10.1145/1327452.1327494}}.

\bibitem[AIKN09]{aikn-alnnh-09}
A.~Andoni, P.~Indyk, R.~Krauthgamer, and H.~L. Nguyen.
\newblock Approximate line nearest neighbor in high dimensions.
\newblock In {\em Proc. 20th ACM-SIAM Sympos. Discrete Algs. {\em(SODA)}},
  pages 293--301, 2009.
\newblock \href {http://dx.doi.org/10.1137/1.9781611973068.33}
  {\path{doi:10.1137/1.9781611973068.33}}.

\bibitem[AINR14]{ainr-blsh-14}
A.~Andoni, P.~Indyk, H.~L. Nguyen, and I.~Razenshteyn.
\newblock Beyond locality-sensitive hashing.
\newblock In {\em Proc. 25th ACM-SIAM Sympos. Discrete Algs. {\em(SODA)}},
  pages 1018--1028, 2014.
\newblock \href {http://dx.doi.org/10.1137/1.9781611973402.76}
  {\path{doi:10.1137/1.9781611973402.76}}.

\bibitem[AMN{\etalchar{+}}98]{amnsw-oaann-98}
S.~Arya, D.~M. Mount, N.~S. Netanyahu, R.~Silverman, and A.~Y. Wu.
\newblock An optimal algorithm for approximate nearest neighbor searching in
  fixed dimensions.
\newblock {\em J. Assoc. Comput. Mach.}, 45(6):891--923, 1998.
\newblock URL: \url{http://www.cs.umd.edu/~mount/Papers/dist.pdf}, \href
  {http://dx.doi.org/10.1145/293347.293348} {\path{doi:10.1145/293347.293348}}.

\bibitem[AR15]{ar-oddha-15}
A.~Andoni and I.~Razenshteyn.
\newblock Optimal data-dependent hashing for approximate near neighbors.
\newblock In {\em Proc. 47th Annu. ACM Sympos. Theory Comput. {\em(STOC)}},
  pages 793--801, 2015.
\newblock \href {http://dx.doi.org/10.1145/2746539.2746553}
  {\path{doi:10.1145/2746539.2746553}}.

\bibitem[CFV{\etalchar{+}}13]{cfvrs-mdmdi-13}
F.~Cismondi, A.~S. Fialho, S.~M. Vieira, S.~R. Reti, J.~M.~C. Sousa, and S.~N.
  Finkelstein.
\newblock Missing data in medical databases: Impute, delete or classify?
\newblock {\em AI in Medicine}, 58(1):63--72, 2013.
\newblock \href {http://dx.doi.org/10.1016/j.artmed.2013.01.003}
  {\path{doi:10.1016/j.artmed.2013.01.003}}.

\bibitem[CLMW11]{clmw-rpca-11}
E.~J. Cand{\`e}s, X.~Li, Y.~Ma, and J.~Wright.
\newblock Robust principal component analysis?
\newblock {\em J. Assoc. Comput. Mach.}, 58(3):11, 2011.
\newblock \href {http://dx.doi.org/10.1145/1970392.1970395}
  {\path{doi:10.1145/1970392.1970395}}.

\bibitem[CR10]{cr-orcpl-10}
A.~Chakrabarti and O.~Regev.
\newblock An optimal randomized cell probe lower bound for approximate nearest
  neighbor searching.
\newblock {\em SIAM J. Comput.}, 39(5):1919--1940, February 2010.
\newblock \href {http://dx.doi.org/10.1137/080729955}
  {\path{doi:10.1137/080729955}}.

\bibitem[CS89]{cs-arscg-89}
K.~L. Clarkson and P.~W. Shor.
\newblock Applications of random sampling in computational geometry, {II}.
\newblock {\em Discrete Comput. Geom.}, 4:387--421, 1989.
\newblock \href {http://dx.doi.org/10.1007/BF02187740}
  {\path{doi:10.1007/BF02187740}}.

\bibitem[DIIM04]{diim-lshsb-04}
M.~Datar, N.~Immorlica, P.~Indyk, and V.~S. Mirrokni.
\newblock Locality-sensitive hashing scheme based on -stable distributions.
\newblock In {\em Proc. 20th Annu. Sympos. Comput. Geom. {\em(SoCG)}}, pages
  253--262, 2004.
\newblock \href {http://dx.doi.org/10.1145/997817.997857}
  {\path{doi:10.1145/997817.997857}}.

\bibitem[{Har}01]{h-rvdnl-01}
S.~{Har-Peled}.
\newblock A replacement for {Voronoi} diagrams of near linear size.
\newblock In {\em Proc. 42nd Annu. IEEE Sympos. Found. Comput. Sci.
  {\em(FOCS)}}, pages 94--103, 2001.
\newblock URL: \url{http://sarielhp.org/p/01/avoronoi}, \href
  {http://dx.doi.org/10.1109/SFCS.2001.959884}
  {\path{doi:10.1109/SFCS.2001.959884}}.

\bibitem[HE07]{he-scump-07}
J.~Hays and A.~A. Efros.
\newblock Scene completion using millions of photographs.
\newblock {\em Trans. Graphics}, 26(3):4, 2007.
\newblock \href {http://dx.doi.org/10.1145/1276377.1276382}
  {\path{doi:10.1145/1276377.1276382}}.

\bibitem[HIM12]{him-anntr-12}
S.~{Har-Peled}, P.~Indyk, and R.~Motwani.
\newblock Approximate nearest neighbors: {Towards} removing the curse of
  dimensionality.
\newblock {\em Theory Comput.}, 8:321--350, 2012.
\newblock Special issue in honor of Rajeev Motwani.
\newblock \href {http://dx.doi.org/10.4086/toc.2012.v008a014}
  {\path{doi:10.4086/toc.2012.v008a014}}.

\bibitem[IM98]{im-anntr-98}
P.~Indyk and R.~Motwani.
\newblock Approximate nearest neighbors: {Towards} removing the curse of
  dimensionality.
\newblock In {\em Proc. 30th Annu. ACM Sympos. Theory Comput. {\em(STOC)}},
  pages 604--613, 1998.
\newblock \href {http://dx.doi.org/10.1145/276698.276876}
  {\path{doi:10.1145/276698.276876}}.

\bibitem[Ind04]{i-nnhds-04}
P.~Indyk.
\newblock Nearest neighbors in high-dimensional spaces.
\newblock In J.~E. Goodman and J.~O'Rourke, editors, {\em Handbook of Discrete
  and Computational Geometry}, chapter~39, pages 877--892. CRC Press LLC, 2nd
  edition, 2004.
\newblock \href {http://dx.doi.org/10.1201/9781420035315.ch39}
  {\path{doi:10.1201/9781420035315.ch39}}.

\bibitem[Ind06]{i-sdpge-06}
P.~Indyk.
\newblock Stable distributions, pseudorandom generators, embeddings, and data
  stream computation.
\newblock {\em J. Assoc. Comput. Mach.}, 53(3):307--323, 2006.
\newblock \href {http://dx.doi.org/10.1145/1147954.1147955}
  {\path{doi:10.1145/1147954.1147955}}.

\bibitem[Isl09]{i-aamkp-09}
M.~T. Islam.
\newblock Approximation algorithms for minimum knapsack problem.
\newblock Master's thesis, Dept. Math \& Comp. Sci., 2009.
\newblock \url{https://www.uleth.ca/dspace/handle/10133/1304}.

\bibitem[KL04]{kl-nnsap-04}
R.~Krauthgamer and J.~R. Lee.
\newblock Navigating nets: simple algorithms for proximity search.
\newblock In {\em Proc. 15th ACM-SIAM Sympos. Discrete Algs. {\em(SODA)}},
  pages 798--807, Philadelphia, PA, USA, 2004. Society for Industrial and
  Applied Mathematics.

\bibitem[Kle97]{k-tanns-97}
J.~Kleinberg.
\newblock Two algorithms for nearest-neighbor search in high dimensions.
\newblock In {\em Proc. 29th Annu. ACM Sympos. Theory Comput. {\em(STOC)}},
  pages 599--608, 1997.

\bibitem[KOR00]{kor-esann-00}
E.~Kushilevitz, R.~Ostrovsky, and Y.~Rabani.
\newblock Efficient search for approximate nearest neighbor in high dimensional
  spaces.
\newblock {\em SIAM J. Comput.}, 2(30):457--474, 2000.
\newblock URL: \url{http://epubs.siam.org/sam-bin/dbq/article/34717}.

\bibitem[Mah15]{m-anlsh-15}
S.~Mahabadi.
\newblock Approximate nearest line search in high dimensions.
\newblock In {\em Proc. 26th ACM-SIAM Sympos. Discrete Algs. {\em(SODA)}},
  pages 337--354, 2015.

\bibitem[MNSS15]{mnss-akfnn-14}
W.~Mulzer, H.~L. Nguy{\^{e}}n, P.~Seiferth, and Y.~Stein.
\newblock Approximate -flat nearest neighbor search.
\newblock In {\em Proc. 47th Annu. ACM Sympos. Theory Comput. {\em(STOC)}},
  pages 783--792, 2015.
\newblock \href {http://dx.doi.org/10.1145/2746539.2746559}
  {\path{doi:10.1145/2746539.2746559}}.

\bibitem[Pan06]{p-ebnns-06}
R.~Panigrahy.
\newblock Entropy based nearest neighbor search in high dimensions.
\newblock In {\em Proc. 17th ACM-SIAM Sympos. Discrete Algs. {\em(SODA)}},
  pages 1186--1195, 2006.

\bibitem[Sam05]{s-fmmds-05}
H.~Samet.
\newblock {\em Foundations of Multidimensional and Metric Data Structures}.
\newblock The Morgan Kaufmann Series in Computer Graphics and Geometric
  Modeling. Morgan Kaufmann Publishers Inc., 2005.

\bibitem[SDI06]{sdi-nnmlv-06}
G.~Shakhnarovich, T.~Darrell, and P.~Indyk.
\newblock {\em Nearest-Neighbor Methods in Learning and Vision: Theory and
  Practice (Neural Information Processing)}.
\newblock The MIT Press, 2006.

\bibitem[SWC{\etalchar{+}}09]{swcsr-mimde-09}
J.~A.~C. Sterne, I.~R. White, J.~B. Carlin, M.~Spratt, P.~Royston, M.~G.
  Kenward, A.~M. Wood, and J.~R. Carpenter.
\newblock Multiple imputation for missing data in epidemiological and clinical
  research: potential and pitfalls.
\newblock {\em BMJ}, 338:b2393, 2009.
\newblock URL: \url{http://dx.doi.org/10.1136/bmj.b2393}.

\bibitem[WCNK13]{wcnk-shmde-13}
B.~J. Wells, K.~M. Chagin, A.~S. Nowacki, and M.~W. Kattan.
\newblock Strategies for handling missing data in electronic health record
  derived data.
\newblock {\em eGEMs}, 1(3), 2013.
\newblock \href {http://dx.doi.org/10.13063/2327-9214.1035}
  {\path{doi:10.13063/2327-9214.1035}}.

\end{thebibliography}



 



\appendix

\section{For the -case: Trading density for proximity}
\apndlab{eps:approx}

The tail of a point in \lemref{lp:heavy:tail} has to be quite heavy,
for the data-structure to reject it as an \ANN. It is thus natural to
ask if one can do better, that is -- classify a far point as far, even
if the threshold for being far is much smaller (i.e., ultimately a
factor of ).  Maybe surprisingly, this can be done, but it
requires that such a far point would be quite dense, and we show how
to do so here. For the sake of simplicity of exposition the result of
this section is provided only under the  norm.

\smallskip The algorithm is the same as the one presented in
\secref{sec:lp:alg}, except that for the given parameter  we
use -\ANN data-structures. We will specify it more
precisely at the end of this section. Also the total number of \ANN
data-structures is 



\subsection{A tail of two points}

We start by proving a more nuanced version of \lemref{lp:heavy:block}.

\begin{lemma}
    \lemlab{heavy:block:eps}Let  be parameters, and let  be a point
    in  that is -heavy.  Then
    
\end{lemma}


\begin{proof}
    Let  be a parameter and consider the
    -truncated point .
    Since  is -heavy, we have that
    .  Now, we
    have
    
    Now, by Chebyshev's inequality, we have that
    
    by \lemref{lp:t:heavy}. Now by setting ,
    this probability would be at least .
\end{proof}

Similar to the \lemref{lp:light:good} by Markov's inequality we have
the following lemma.
\begin{lemma}
    \lemlab{light:good:eps}Consider a point  such that
    . Conditioned on the event of
    \lemref{success}, we have that
    ,
    where .
\end{lemma}
\begin{lemma}
    \lemlab{heavy:far:eps}Let .  If  is
    -heavy point, then
    ,
    assuming  and
    .
\end{lemma}
\begin{proof}For all , let . By
    \lemref{heavy:block:eps}, with probability at least ,
    we have that
    
    In particular, let . We have
    that
    
Now, by Hoeffding's inequality, we have that
    
\end{proof}
\begin{lemma}
    \lemlab{heavy:tail:eps}
Let  be two parameters. For a query point
    , let  be its -fold
    nearest neighbor in , and let
    . Then, with high
    probability, the algorithm returns a point ,
    such that  is a
    -light, where
    . The data-structure performs
     of -\ANN
    queries.
\end{lemma}

\begin{proof}
    As before, we set  and
    . Also, by conditions of
    \lemref{heavy:far:eps} we have
    .
    Also, let .  Let 
    be the set of  largest coordinates in
    . By similar arguments as in
    \lemref{lp:heavy:tail}, there exists
     data structures say
     such that 
    does not contain any of the coordinates of .

    Since by assumption
    ,
    and by \lemref{light:good:eps}, with probability at least
     the distance of  from  is
    at most . Since there are
     such structures, we know that, with
    high probability, for one of them, say , this holds.  By
    \lemref{heavy:far:eps}, any point  (of )
    that is -heavy, would be in distance at
    least  in the projection 
    from the projected , and since  is a
    -\ANN data-structure under the  norm, we
    conclude that no such point can be returned.  Note that since for
    the reported point , the point  cannot be
    -heavy, and the coordinates in  can
    contribute at most ,
    the point  cannot be -heavy. Thus,
    the data-structure returns the desired point with high
    probability.

    As for the query performance, the data-structure performs
     queries of -\ANN data-structures.
\end{proof}

By \lemref{lp:light}, we get the following corollary.
\begin{corollary}
    Given a query point , let 
    be its -robust nearest neighbor in , and
    . Then, with high
    probability, the algorithm returns a point ,
    such that
    
\end{corollary}














\end{document}
