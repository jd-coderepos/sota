\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{hyperref}
\hypersetup{hidelinks}
\hypersetup{colorlinks}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{ulem}
\usepackage{float}

\begin{document}

\title{Adaptive Split-Fusion Transformer}

\author{Zixuan Su, Hao Zhang, Jingjing Chen,~\IEEEmembership{Member,~IEEE}, Lei Pang, Chong-Wah Ngo,~\IEEEmembership{Member,~IEEE}\\ and Yu-Gang Jiang,~\IEEEmembership{Senior Member,~IEEE}
\thanks{Zixuan Su, Jingjing Chen and Yu-Gang Jiang are with School of Computer Science, Fudan University, Shanghai, China (E-mail: zxsu21@m.fudan.edu.cn, \{chenjingjing,ygj\}@fudan.edu.cn). Hao Zhang and Chong-Wah Ngo are with Singapore Management University, Singapore (E-mail: \{hzhang,cwngo\}@smu.edu.sg). Lei Pang is with City University of Hong Kong, Hong Kong, China (E-mail: cactuslei@gmail.com).}
\thanks{The Corresponding author is Jingjing Chen.}
}





\maketitle

\begin{figure*}
\centering
\includegraphics[height=10cm]{Figures/Figure1.png}
\caption{An overview of the ASF-former. The encoders in a reduction and computation stage are separately denoted as ASR-R/C. Both types include Split Parallelism and Adaptive Fusion parts, except that the ASF-R adopts T2T attention for down-sampling token dimension. As shortcut and Conv ({\color{red}red} line) is incompatible with the T2T attention \cite{yuan2021tokens}, they are removed in the reduction stage. (This figure is best viewed in color)
}
\label{fig:framework}
\end{figure*}

\begin{abstract}
Neural networks for visual content understanding have recently evolved from convolutional ones to transformers. The prior (CNN) relies on small-windowed kernels to capture the regional clues, demonstrating solid local expressiveness. On the contrary, the latter (transformer) establishes long-range global connections between localities for holistic learning. Inspired by this complementary nature, there is a growing interest in designing hybrid models which utilize both techniques. 
Current hybrids merely replace convolutions as simple approximations of linear projection or juxtapose a convolution branch with attention without considering the importance of local/global modeling. To tackle this, we propose a new hybrid named \textit{Adaptive Split-Fusion Transformer} (ASF-former) that treats convolutional and attention branches differently with adaptive weights. Specifically, an ASF-former encoder equally splits feature channels into half to fit dual-path inputs. Then, the outputs of the dual-path are fused with weights calculated from visual 
cues. We also design a compact convolutional path from a concern of efficiency. Extensive experiments on standard benchmarks, such as ImageNet-1K, CIFAR-10, and CIFAR-100, show that our ASF-former outperforms its CNN, transformer, and hybrid counterparts in terms of accuracy (83.9\% on ImageNet-1K), under similar conditions (12.9G MACs/56.7M Params, without large-scale pre-training). The code is available at: \url{https://github.com/szx503045266/ASF-former}.
\end{abstract}

\begin{IEEEkeywords}
Visual understanding, Transformer, CNN, Hybrid, Gating.
\end{IEEEkeywords}


\section{Introduction}

Neural networks for learning visual representations have recently split into directions of conventional convolutional neural networks (i.e., CNN) and emerging transformers. CNN used to be the de-facto standard network and was good at modeling localities. On the contrary, the transformer learns holistic features by building pair-wise relations, thus demonstrating strong global expressiveness. Pilot visual transformers, including ViT \cite{dosovitskiy2021an}, T2T-ViT \cite{yuan2021tokens}, deliberately avoid convolutions and only rely on the self-attention. Though achieving good accuracy, they pay extra computations as a price for bypassing efficient convolution operators. 

Since convolutions and self-attention are complementary when viewed from perspectives like local-global modeling and high-low efficiency, it is natural to study hybrid networks to ensure each part serves its best. Existing hybrids usually combine these two parts in a cascade or parallel manner. For a cascade hybrid, researchers usually re-implement linear projections in vanilla transformers with convolutional approximations. For example, token-embedding \cite{Chen2021VisformerTV,Hassani2021EscapingTB,Mehta2021MobileViTLG,Wu2021CvTIC,Yan2021ConTNetWN,Yang2021FocalSF,Yuan2021IncorporatingCD} and linear-projections \cite{Chen2021VisformerTV,He2021PruningSI,Mehta2021MobileViTLG,Wu2021CvTIC,Yan2021ConTNetWN,Yuan2021IncorporatingCD} in attentions/MLPs are commonly replaced by convolutions. These cascade works share a common principle of minimal change. As for parallel hybrids, an extra convolutional branch is inserted on par with the attention in a dual-branch (or dual-path) manner \cite{Chen2021MobileFormerBM,Pan2021OnTI,Peng2021ConformerLF,Xu2021ViTAEVT}. This strategy enables learning local/global visual contexts independently and is beneficial for analyzing the effectiveness of each path.

However, most current hybrid models treat local and global contexts equally, which conflicts with the real-world scenario that the importance of local/global cues varies according to the image visuals and layer depth. For example, tiny objects prefer local evidence, whereas landscapes bias global views in the recognition process. Besides, layer with different depths also shows their biases in learning local/global contexts, as mentioned in \cite{Pan2021OnTI}.

To tackle this, we propose a novel parallel hybrid named {\textit{Adaptive Split-Fusion Transformer}} (ASF-former), which adopts an adaptive gating strategy to select convolution/attention paths according to global visual cues. Its encoder contains two parts: \textit{Efficient Split Parallelism with HMCB} and \textit{Adaptive Fusion} as shown in Figure~(\ref{fig:framework}).

\textbf{Efficient Split Parallelism with HMCB} differs from the existing parallel hybrid models in two aspects. Firstly, we comprehensively and carefully craft an efficient convolution path named \textit{Half-Residual Mobile Convolutional Branch} (HMCB). Compared with existing counterparts, the HMCB demonstrates stronger local capability with fewer computations. Secondly, we split inherent feature channels of pure transformers into half and separately feed sub-features into the Conv/attention branch. Thereby, we could keep the same feature dimension as the original backbone. With these, the Split Parallelism shares a similar complexity as single-path (convolution or attention) models.


\textbf{Adaptive Fusion} module intakes outputs from convolution and attention branches and weighs them with adaptive scalars. Specifically, visual features from both paths are processed by sequential layers, including global pooling, fully-connected layer, and Sigmoid activation, to generate weighting scalars. These scalars are used to weigh features from each branch (Fig. (\ref{fig:framework})). We also add an extra skip connection to alleviate gradient vanishing in backpropagation. We experimentally verify that the new adaptive fusion could effectively and efficiently select convolution/attention branches according to visual contents. We briefly summarize our contributions below.

\hspace*{\fill} \\

\begin{itemize}
    \item \textit{Efficient Split Parallelism with HMCB}. We introduce a new Half-Residual Mobile Convolutional Branch, which complements the attention feature with high cost-effectiveness. 
    \item \textit{Adaptive Fusion}. We first introduce adaptive fusion to combine outputs from convolution and attention branches. We could adjust the importance of local/global modeling with adaptive weights according to visual contents.
    \item \textit{A new hybrid ASF-former}. We build a new CNN-transformer hybrid with efficient convolution layers (HMCB) and an adaptive fusion strategy. Experiments on standard benchmarks, such as ImageNet-1K and downstream datasets, show that the ASF-former could achieve SOTA performance (83.9\% on ImageNet-1K) under similar conditions (12.9G MACs / 56.7M Params). 
\end{itemize}

\section{Related Work}

\textbf{Convolutional Neural Network.} Since the birth of AlexNet~\cite{Krizhevsky2012ImageNetCW}, convolutional neural network (CNN) was the de-facto standard model on different vision topics, such as image recognition~\cite{Simonyan2015VeryDC,Szegedy2015GoingDW,Szegedy2016RethinkingTI,He2016DeepRL,He2016IdentityMI,Huang2017DenselyCC}, object detection~\cite{Girshick2015FastR,Ren2015FasterRT,Redmon2016YouOL,Liu2016SSDSS,Lin2017FocalLF,Lin2017FeaturePN,Cai2018CascadeRD}, instance/semantic segmentation~\cite{He2017MaskR,Chen2017RethinkingAC,Chen2018DeepLabSI} and video recognition~\cite{Feichtenhofer2016ConvolutionalTN,Qiu2017LearningSR,Carreira2017QuoVA,Zhou2018TemporalRR,Wang2018NonlocalNN,Feichtenhofer2019SlowFastNF,Lin2019TSMTS}.

To apply CNN to edge devices, researchers spare many efforts on balancing complexity  and capacity. For example, MobileNet~\cite{howard2017mobilenets} factorized a standard convolution with depthwise separable convolution. Moreover, MobileNetV2~\cite{Sandler2018MobileNetV2IR} introduced an extra inverted residual design and expanded the usage of depthwise convolution into more layers (e.g., intermedia expansion layer). ShuffleNet~\cite{Zhang2018ShuffleNetAE} combines group convolution, channel shuffle, and depthwise convolution to build an efficient CNN unit. EfficientNet~\cite{Tan2019EfficientNetRM} follows an empirical scaling rule to search and adjust hyperparameters of network structure, such as ``\textit{dimension, depth, width, resolution}'', to achieve a high cost-effective.


Some researchers proposed plug-and-play modules introducing a few extra parameters to recalibrate CNN features. For example, the SE-Net~\cite{Hu2018SqueezeandExcitationN} introduced a new ``Squeeze-and-Excitation'' block on top of a plain 2D-CNN backbone. The block could adaptively adjust feature channels with global contexts. Furthermore, GE-Net~\cite{Hu2018GatherExciteEF} studied more choices of ``Gathering'' contexts information than SE-Net. Hao et al. \cite{hao2022group} extended contextual calibrating for videos and proposed GC-Net that utilizes spatial-temporal contexts. SKNet~\cite{Li2019SelectiveKN} selectively fuse two convolution branches with different kernel sizes to adaptively adjust the receptive field size. ResNeSt~\cite{Zhang2020ResNeStSN} further combined
the channel-wise attention with a multi-path strategy. This design would build cross-feature interactions and benefit learning diverse representations. 

\textbf{Vision Transformer.} Transformer receives extensive interest in various multimedia and vision tasks, such as Video Recognition \cite{alfasly2022effective}, Image Understanding \cite{EAPT, YDTR}, Re-Identification \cite{Li2021ExploitingMP, Zhao2022SpatialET,zhou2022moving}, Visual Inpainting \cite{FT-TDR}, Pose Estimation \cite{Li2022ExploitingTC} tasks since the birth of ViT \cite{dosovitskiy2021an}, which validates the feasibility of replacing CNNs with pure transformers with large-scale pre-training. 

Though achieving impressive accuracy, the ViT \cite{dosovitskiy2021an} suffers from a high computational cost. The cost is caused by densely calculating the pair-wise distance between visual tokens in each attention module. To balance computations and classification accuracy, researchers spare more effort on developing new transformers, including DeiT \cite{Touvron2021TrainingDI}, T2T-ViT \cite{yuan2021tokens}, TNT \cite{tnt}, PVT \cite{wang2021pyramid}, Swin \cite{liu2021Swin} than before. Specifically, DeiT \cite{Touvron2021TrainingDI} adopted convnet as a teacher and trained transformer under a teacher-student strategy. It relies on the distillation token to introduce locality into a transformer, thus lowering the requirement for large-scale training data.
T2T-ViT \cite{yuan2021tokens} focused on shrinking token-length. It designs the T2T module to down-sampling tokens via concatenating features of local neighboring pixels.
TNT~\cite{tnt} altered the granularity of patch dividing by further splitting local patches into sub-patches. The inner attention will be calculated within each patch before outer global attention.
In order to be ported to various downstream tasks, PVT~\cite{wang2021pyramid} introduced a progressive shrinking pyramid structure and a spatial-reduction attention for efficiently learning multi-scale and high-resolution features.
For further parameter and computation efficiency, Swin Transformer \cite{liu2021Swin} utilized shifted window to split the feature map and performed self-attention within each local window. 
These models are pure convolutional-free transformers, thus lacking local capacity and convolution's efficiency strength.


\textbf{Hybrid Transformer.} Attracted by the complementary nature of CNN and Attentions, more and more efforts are devoted to developing hybrid transformers. Existing hybrids can be separated into two groups.
The first is cascade hybrid which minimally modify the original transformer model by re-implementing the token-embedding \cite{Chen2021VisformerTV,Hassani2021EscapingTB,Mehta2021MobileViTLG,Wu2021CvTIC,Yan2021ConTNetWN,Yang2021FocalSF,Yuan2021IncorporatingCD} and the linear projections \cite{Chen2021VisformerTV,He2021PruningSI,Mehta2021MobileViTLG,Wu2021CvTIC,Yan2021ConTNetWN,Yuan2021IncorporatingCD} in Attentions/MLPs with convolution operators.
The second is parallel hybrid which juxtaposes an extra convolutional branch on par with the attention \cite{Chen2021MobileFormerBM,Pan2021OnTI,Peng2021ConformerLF,Xu2021ViTAEVT}. For example, Conformer \cite{Peng2021ConformerLF} designed the Feature Coupling Unit (FCU) for transmitting features from one path to another. For acquiring inductive bias from convolution, ViTAE \cite{Xu2021ViTAEVT} built the parallel structure in each block and designed the pyramid reduction module with dilated convolution. These methods treat convolution and attention paths equally. 
ACmix \cite{Pan2021OnTI} instead set two learnable weights for measuring the importance of two paths, but the weights only vary with network depth, failing to be adjusted according to visual contexts.


\section{Our Method}
An overview of ASF-former is shown in Figure~(\ref{fig:framework}). Similar to \cite{Xu2021ViTAEVT,yuan2021tokens}, it contains a total of  encoders, where / encoders reside in \textit{reduction} or \textit{computation} stages. As in \cite{yuan2021tokens}, the two stages differentiate in whether adopting the T2T for shrinking token-length and T2T attentions for reducing computations. To distinguish, we separately denote encoders in the two stages as the ASF-R and ASF-C. We present a detailed pipeline of ASF-former below.

An image  is first soft-split into patches. Each patch shares an identical shape of  with overlap  and padding . These patches are unfolded into a sequence of tokens , where , and token-length is:

Tokens  go through the two stages, including the reduction and computation stages for representation learning.  

\textbf{Reduction stage} contains  replicated ASF-R + T2T pairs, where the prior and the latter module separately serve for feature learning and down-sampling. Denote tokens from the -th pair as   or  . The token-length  and dimension  would reduce and increase according to the depth , due to the T2T operation, while the ASF-R encoder would decrease the token dimension to . A math process of the -th pair is shown as:

where  and  denotes the ASR-R and T2T modules.

Output  of reduction stage is obtained by linear-projecting  to a fixed -dimensional space.


\textbf{Computation stage} contains  identical ASF-C encoders, without changing token-length. Same as the ViT \cite{dosovitskiy2021an}, an extra [{\tt CLASS}] token  is concatenated with  for an input  of this stage. Notably, the [{\tt CLASS}] part would only be processed by the attention branch.

Denoting the ASF-C with function , the process of the -th encoders is:


The [{\tt CLASS}] token yielded by the last ASF-C encoders will be fed into a fully-connected layer for category prediction:


Since ASF-R/C encoders share most parts, we present them together in Section~\ref{sec:enc}. 

\subsection{An ASF-R/C Encoder}
\label{sec:enc}
The ASF-R \& ASF-C encoders are same in \textit{Split Parallelism}, \textit{Adaptive Fusion} and MLP parts, and differs in the attention part (T2T or vanilla attention).

\textbf{Split Parallelism} equally split a tensor of tokens  for the ASF-R (or  for the ASF-C) into two parts , , along the channel axis. Then, the sub-tensor / is separately fed into convolutional/attention branch for local/global modeling. Notably,  are pre/post-processed with \textit{seq2image} or \textit{image2seq} function \cite{yuan2021tokens} to re-arrange tokens into spatial or sequential form. The process is shown below:

where  and  respectively denote attention and convolution paths, and ,  . Hereby, =64  in the ASF-R (or = in ASF-C). Notably, we carefully craft an efficient convolutional branch named \textit{Half-Residual Mobile Convolutional Branch} and present it in Section~\ref{sec:conv}. 

\textbf{Adaptive Fusion} performs weighted sum on tensors processed by the two paths with adaptive scalars  and . Hereby,  and  are calculated according to visual features from the two paths by Eq.~(\ref{eq:alpha})(\ref{eq:beta}). 

where the  denotes the function for generating weighting scalars. Notably, we generate the  \&  in a \textit{Sigmoid} way. Though this way is theoretically equivalent to a \textit{Softmax} function, it is practically simple in implementation. We describe details and compare different fusion strategies in Section~\ref{sec:adaptive}.

\textbf{Attentions \& MLP} are mostly inherited from the general vision transformer regime, with minor modifications on attention settings. Specifically, the ASF-R/C separately adopt the T2T attention and vanilla attentions. Compared with the vanilla, the T2T attention replaces the multi-head scheme with a single-head one and fixes channels of ``query'', ``key'', ``value'' to , concerning computational efficiency. Since the T2T attention reshapes tokens, the shortcut and Conv  are removed in the ASF-R compared with the ASF-C ({\color{red} red} line in Fig.~(\ref{fig:framework})). Output / of the ASF-R/C encoders is generated as in Eq.~(\ref{eq:outr})(\ref{eq:outc}), where  denotes the MLP with two \textit{fc} layers and a GeLU activation:


\subsection{Half-Residual Mobile Convolutional Branch}
\label{sec:conv}
\begin{figure}
\centering
\includegraphics[height=4.5cm]{Figures/Figure2.png}
\caption{Designs of convolutional branch: (a) PCM from ViTAE transformer\cite{Xu2021ViTAEVT}; (b) Residual bottleneck; (c) our \textbf{HMCB}}
\label{fig:convbranch}
\end{figure}
We study existing CNN branches for hybrid models and craft a new \textit{Half-Residual Mobile Convolutional Branch} (HMCB). The HMCB is more complementary to the attention way than its counterparts while consuming fewer computations. We begin with PCM, the recently proposed CNN-branch in ViTAE hybrid \cite{Xu2021ViTAEVT}.

\textbf{Parallel Convolution Module} (PCM) is shown in Figure~(\ref{fig:convbranch}a), it contains stacked convolutions with  kernels. We re-implement it by replacing its internal \textit{group conv} with conventional \textit{conv}. 
This re-implementation is combined with our Split Parallelism and surpasses the original ViTAE hybrid (Accuracy from 82.0\%  82.2\%), at the expense of extra parameters.

\textbf{Residual Bottleneck} is widely used in various vision tasks (Fig.~(\ref{fig:convbranch}b)). Thus, we want to adopt it as the convolutional branch in the hybrid models. We only implement one block of residual bottleneck here to maintain similar params/MACs as the original PCM.


\textbf{Half-Residual Mobile Convolutional Branch} (HMCB) is modified based on PCM, with the help of efficient \textit{conv} approximations (Fig.~(\ref{fig:convbranch}c)). Inspired by MobileNet \cite{howard2017mobilenets} and MobileNetV2 \cite{Sandler2018MobileNetV2IR}, 
we first factorize each conventional  \textit{conv} into one  \textit{depth-wise conv} followed by one  \textit{conv} and then we add another  \textit{conv} before the first \textit{depth-wise conv}.


These approximations remarkably reduce computations. Even if we replicate the half-residual block three times, the HMCB still contains similar Params / MACs to single Residual bottleneck. Specifically, we implant the shortcut at a different position with the conventional residual bottleneck to be compatible with the repetition and promote the training across channels. We compare the three designs in terms of accuracy, Params, and MACs in Table~\ref{table:convbranch} and observe that our HMCB performs the best under all metrics.

\subsection{Adaptive Fusion and Counterparts}
\label{sec:adaptive}
In this part, we present Adaptive Fusion and two simple counterparts. We begin with a \textit{simple fusion} with fixed weights, then introduce a fusion strategy \textit{context-agnostic} weights, and finally, give the \textit{Adaptive Fusion} with contextually relevant weights.

\begin{figure}
\centering
\includegraphics[height=4.5cm]{Figures/Figure3_1.png}
\caption{Fusion strategies: (a) Simple Fusion; (b) Context-Agnostic Fusion; (c) Adaptive Fusion. The shortcut ({\color{blue}blue}) serves for reducing gradient vanishment}
\label{fig:fusion}
\end{figure}

\textbf{Simple Fusion} directly averages outputs from the two branches with equal importance as in Eq.~(\ref{eq:sf}) and Fig.~(\ref{fig:fusion}a). The fusion itself is parameter-free and effective. Thus, it is preferred in a pilot parallel hybrid, ViTAE \cite{Xu2021ViTAEVT}. 


\textbf{Context-Agnostic Fusion} explicitly learns  \&  on par with training process (Fig.~(\ref{fig:fusion}b)). To avoid a phenomenon that the gradient vanishment deactivates a particular branch when  or  falls into extremely tiny values, we add an extra skip connection ({\color{blue}blue} line) to enforce gradients to be propagated to both ways.


\textbf{Adaptive Fusion} calculates  and  according to visual contexts from both branches, which is inspired by SKNet~\cite{Li2019SelectiveKN}. Its process is shown in Eq.~(\ref{eq:af1})\&(\ref{eq:af2}) and Figure~(\ref{fig:fusion}c).  


Specifically, we expand the function  in Eq. (\ref{eq:alpha}) to be two fully-connected layers (\textit{Linear}), with \textit{BatchNorm} and \textit{GeLU} activations in between. To stabilize the training procedure, we add the extra skip connection from the same concern as the prior fusion method.

We comprehensively compare various fusion strategies in Table~\ref{table:fusion} and observe a significant improvement with our Adaptive Fusion strategy.

\subsection{Pyramid Structure}
\label{pyramid}

\begin{figure*}
\centering
\includegraphics[height=4.5cm]{Figures/Figure5.png}
\caption{The architecture of ASF-former. It adopts a 4-stage pyramid structure and is built based on ASF-C encoders. Without [{\tt CLASS}] token, the final output feature will be entirely fed into the classification layer after global average pooling.}
\label{fig:pyramid}
\end{figure*}

ASF-former is built beyond the structure of T2T-ViT~\cite{yuan2021tokens}, which has a reduction stage for down-sampling and a computation stage for feature encoding under the same feature dimension. However, downstream tasks such as object detection, instance segmentation, and semantic segmentation usually prefer multi-level features from the backbone model to capture objects at different scales. This requirement makes ASF-former not quite suitable for transferring to downstream tasks. To deal with this problem, we follow the pyramid structure of Swin Transformer~\cite{liu2021Swin} and further design a new model with our ASF-C encoder, denoted as ASF-former.

As Figure~(\ref{fig:pyramid}) shows, ASF-former adopts a 4-stage pyramid structure with down-sampling layers before each stage. The original images are tokenized and down-sampled by the ratio of 4 in the first stage, while the other down-sampling layers use a ratio of 2. ASF-former has ,,, ASF-C encoders in each stage respectively and the feature channel dimension  in Stage 1 will be doubled in each following stages. Following Swin~\cite{liu2021Swin}, we use the simple convolutions for image tokenization and down-sampling instead of T2T operation; therefore, we could remove the ASF-R from the ASF-former. Besides, instead of adding a [{\tt CLASS}] token for classification, we perform global average pooling on the output feature of the last stage and directly feed it into a fully-connected layer for category prediction.

We will compare ASF-former and its pyramid version, ASF-former, in Table~\ref{table:sota}. Besides, in Section~\ref{detection} and \ref{segmentation}, we will adopt ASF-former for all downstream tasks and further utilize its stage-wise nature.

\section{Experiments}
We evaluate the ASF-former on standard benchmarks, including ImageNet-1K, CIFAR-10/100, with metrics like Top-1/5 accuracy, model Params, and inferencing MACs. Experimental results validate the efficacy and efficiency of the ASF-former.
\subsection{Datasets}
We conduct ablation and transferability experiments on ImageNet-1K and CIFAR-10/100 downstream datasets.

\textbf{ImageNet-1K} \cite{imagenet_cvpr09} defines 1000 categories with 1.35 million images captured in daily life. On average, each category contains around 1.3k samples. These images are split into training/validation sets with a ratio of 26:1.

\textbf{CIFAR-10/100} \cite{krizhevsky2009learning} respectively contains 10/100 categories on 60k images with a fixed  resolution. In the CIFAR-10, each category includes 6k samples, with 5k/1k samples as training/testing. Whereas, in the CIFAR-100, there are 600 samples per category, with 500/100 for training/testing.

\subsection{Experimental settings}
In this part, we briefly introduce hyperparameters of our ASF-former variants and the training \& inference recipes.

\setlength{\tabcolsep}{8pt}
\begin{table*}
\begin{center}
\caption{Hyperparameters of ASF-former-S \& ASF-former-B .}
\label{table:variant}
\begin{tabular}{l|ccc|ccc|rr}
\hline\noalign{\smallskip}
\multirow{3}*{Model} &\multicolumn{3}{c}{Reduction stage} &\multicolumn{3}{c}{Computation stage} &\multicolumn{2}{c}{Model Size}\\
 \cline{2-9}
 &Depth &Token &MLP &Depth &Token &MLP &Params &MACs\\
 & &dim  &dim & &dim  &dim &(M) &(G)\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
\textbf{ASF-former-S} &2 &64 &64 &14 &384 &1152 &\textbf{19.3} &\textbf{5.5}\\
\textbf{ASF-former-B} &2 &64 &64 &24 &512 &1536 &\textbf{56.7} &\textbf{12.9}\\
\hline
\end{tabular}
\end{center}
\end{table*}
\setlength{\tabcolsep}{1.4pt}

\textbf{ASF-former variants}. By customizing hyperparameters, such as the number of encoders (i.e.,  and ) and dimensions of tokens in different layers, we can flexibly control the complexity of ASF-former at different computation scales. To fairly compare the ASF-former with its counterpart of similar computational costs, we propose a \textit{small} and \textit{big} model, respectively denoted as the ASF-former-S and ASF-former-B in Table~\ref{table:variant}. Besides, we set the same , ,  as the original T2T-ViT model (Eq.~(\ref{patch_number})). As for ASF-former, we also design two variants correspondingly, named ASF-former-S and ASF-former-B. Their details are shown in Table~\ref{table:variant_p}.

\setlength{\tabcolsep}{8pt}
\begin{table}
\begin{center}
\caption{Hyperparameters of ASF-former-S \& ASF-former-B .}
\label{table:variant_p}
\begin{tabular}{l|cc|rr}
\hline\noalign{\smallskip}
\multirow{2}*{Model} &Depth &Dim &Params &MACs\\
 &,,, & &(M) &(G)\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
\textbf{ASF-former-S} &3,4,12,5 &64 &\textbf{21.3} &\textbf{5.4}\\
\textbf{ASF-former-B} &3,4,18,5 &96 &\textbf{58.9} &\textbf{12.1}\\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

\textbf{Training \& Inference}. We fix the training/inference recipe as \cite{yuan2021tokens} for a fair comparison. In the training phase, images are randomly cropped into size  before going through the network. We also adopt data-augmentations, such as MixUp \cite{zhang2018mixup}, CutMix \cite{yun2019cutmix}, Rand-Augment \cite{Cubuk2020RandaugmentPA}, Random-Erasing \cite{zhong2020random} to reduce over-fitting. The Exponential Moving Average (EMA) strategy is further used for training stability. We train 310 epochs using AdamW optimization, with a batch size of 512. The learning rate is initialized with 5e-4 and decreases with the cosine learning schedule. 
In the inference phase, images are first resized to let the short side be 256 and then center-cropped into  before being fed into the network.

\subsection{Ablation study}
\label{sec:abla}
In this part, we study the effectiveness of our proposed convolutional branch HMCB, Split Parallelism, Adaptive Fusion, etc. We test them on top of the \textit{small} ASF-former-S for quick verification. 

\textbf{HMCB} \textit{vs} \textbf{Convolutional Candidates}. We plug the PCM, Residual Bottleneck, and HMCB into the ASF-former. To exclude the influence of the fusion strategy, we employ ``Simple Fusion'' in all three ASF-formers. The comparison is shown in Table~\ref{table:convbranch}.
\setlength{\tabcolsep}{4pt}
\begin{table}[th]
\begin{center}
\caption{Comparison of different convolutional branchs on ImageNet-1K Val.}
\label{table:convbranch}
\begin{tabular}{llcccc}
\hline\noalign{\smallskip}
Conv Branch &Regime &Params (M)&MACs (G) &Top-1 (\%)\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
PCM &ViTAE \cite{Xu2021ViTAEVT}& 23.6 & 5.6 & 82.0 \\
PCM &ASF-former& 32.1 & 9.3 & 82.2 \\
Residual Bottleneck&ASF-former& \textbf{18.3} & \textbf{5.4} & 81.7\\
Our HMCB&ASF-former& 18.8 & 5.5 & \textbf{82.5} \\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

We observe that the HMCB achieves the best accuracy (82.5\%) among all candidates while consuming comparable or fewer computations (Params / MACs) than the Residual Bottleneck or PCM. This validates that the HMCB is more complementary to global attention than the rest at a low computational cost. Moreover, plugging PCM into ASF-former (with simple fusion) performs slightly better than in the original ViTAE, verifying the effectiveness of the Split Parallelism mechanism. We fix the convolutional branch to be an HMCB in the following experiments.

\textbf{Split Parallelism} \textit{vs} \textbf{Single Path}. We further compare the Split Parallelism with Single Path methods. We remove the channel split for the Single Path method and feed the entire input into an Attention-Only or HMCB-Only path. Hereby,  we still adopt ``Simple Fusion'' (Fig.~(\ref{fig:fusion}a)) in this ablation . Notably, the HMCB-only replaces the [{\tt CLASS}] token with an average pooled vector to predict final categories.
\setlength{\tabcolsep}{4pt}
\begin{table}[th]
\begin{center}
\caption{Comparison of Split Parallelism and Sing Path on ImageNet-1K Val.}
\label{table:branch}
\begin{tabular}{lcccc}
\hline\noalign{\smallskip}
Branch &Params (M)&MACs (G)&Top-1 (\%)\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Attention-only & 21.5 & 6.1 & 81.7\\
HMCB-only & 22.7 & \textbf{5.2} & 72.4\\
\textbf{Attention + HMCB}& \textbf{18.8} & 5.5 & \textbf{82.5} \\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

The results are shown in Table~\ref{table:branch}. Our Split Parallelism achieves 82.5\% accuracy, which remarkably outperforms single-path settings (81.7\% for Atten-only and 72.4\% for Conv-only). Thanks to the Split strategy, our parallelism achieves comparable or fewer Parameters \& MACs than single path methods. This also indicates that the HMCB and attention branches are complementary; meanwhile, our Split Parallelism could capture and integrate the information from both branches very well.

\textbf{Adaptive Fusion} \textit{vs} \textbf{Counterparts}. We implement fusion strategies in Section~\ref{sec:adaptive}, including ``Simple Fusion'', ``Context-Agnostic Fusion'' and ``Adaptive Fusion'', on top of the ASF-former. All fusion variants intake outputs from the attention branch and HMCB. We present their comparison in Table~\ref{table:fusion}.
\setlength{\tabcolsep}{4pt}
\begin{table}[th]
\begin{center}
\caption{Comparison of different fusion method on ImageNet validation set.}
\label{table:fusion}
\begin{tabular}{lcccc}
\hline\noalign{\smallskip}
Fusion Method &Params &MACs &Top-1\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Simple Fusion & 18.8 & 5.5 & 82.5 \\
Context-Agnostic Fusion & 18.8 & 5.5 & 82.2 \\
\textbf{Adaptive Fusion} & 19.3 & 5.5 & \textbf{82.7} \\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

We find that our Adaptive Fusion achieves 82.7\% accuracy, which is superior to all the other counterparts under similar parameters and MACs. This indicates the effectiveness of adapting the weights according to visual contents and verifies the different branch preferences of different images. Notably, Context-Agnostic Fusion performs worse than Simple Fusion, showing that the coarsely learning context-agnostic weights would even degrade both branches' capability and training effect.

\textbf{Effectiveness of Shortcut}. We validate the influence of the shortcut ({\color{blue}blue} line in Figure~(\ref{fig:fusion}c)) by removing it from Adaptive Fusion. The comparison is shown in Table~\ref{table:shortcut}.
\setlength{\tabcolsep}{4pt}
\begin{table}[th]
\begin{center}
\caption{Effectiveness of shortcut on ImageNet Val.}
\label{table:shortcut}
\begin{tabular}{lccl}
\hline\noalign{\smallskip}
Fusion Method &Params (M)&MACs (G) &Top-1 (\%)\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
ASF-Former-S & 19.3 & 5.5 & 82.7 \\
~~~~ shortcut & 19.3 & 5.5 & 82.0 ( 0.7) \\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

When discarding the skip connection, we can see that the final accuracy degrades by a large margin (0.7\%) and is even much worse than Simple Fusion in Table~\ref{table:fusion}. This demonstrates the necessity of skip connection when fusing the outputs of two branches and verifies its ability to help the model's training by promoting gradient propagation.

\textbf{Effectiveness of Pyramid Structure}. We train and validate the pyramid version of ASF-former-S \&ASF-former-B, named ASF-former-S \& ASF-former-B, under the same recipe. The results are present in Table~\ref{table:sota}.

The underlined ASF-former-S achieves an excellent accuracy of 83.0\%, which outperforms ASF-former-S by 0.3\% with comparable parameters and MACs. This strongly proves the effectiveness of a multi-level pyramid structure for better feature encoding when designing hybrid transformer models. ASF-former-B achieves comparable results with ASF-former-B while consuming fewer MACs. This further indicates the computation-efficient property of our pyramid model.

\subsection{Comparison with the state-of-the-art}

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{Comparison with different methods on ImageNet validation set.}
\label{table:sota}
\begin{tabular}{l|l|crr|cc}
\hline\noalign{\smallskip}
\multirow{2}*{Type} &\multirow{2}*{Model} &Image &Params &MACs &\multicolumn{2}{c}{ImageNet}\\
 & &Size &(M) &(G) &Top-1 &Top-5\\
\noalign{\smallskip}
\midrule[1pt]
\noalign{\smallskip}
\multirow{3}*{\rotatebox{90}{\makecell[c]{\textit{CNN}\Small)}}} &PVT-S \cite{wang2021pyramid} & 224 & 24.5 & 7.6 & 79.8 & -\\
 &DeiT-S \cite{Touvron2021TrainingDI} & 224 & 22.0 & 4.6 & 79.9 & 95.0\\
&Swin-T \cite{liu2021Swin} & 224 & 28.0 & 4.5 & 81.2 & 95.5\\
&TNT-S \cite{tnt} & 224 & 23.8 & 5.2 & 81.5 & 95.7\\
 &T2T-ViT-14 \cite{yuan2021tokens} & 224 & 21.5 & 5.2 & 81.5 & 95.7\\
 &T2T-ViT-14 \cite{yuan2021tokens} & 224 & 21.5 & 6.1 & 81.7 & -\\
\midrule
\multirow{12}*{\rotatebox{90}{\makecell[c]{\textit{Hybrid}\Big)}}} &ResNet-101 \cite{He2016DeepRL} & 224 & 44.5 & 15.2 & 78.3 & 94.1\\
 &ResNet-152 \cite{He2016DeepRL} & 224 & 60.2 & 22.6 & 78.9 & 94.4\\
&RegNetY-16G \cite{Radosavovic2020DesigningND} & 224 & 84.0 & 16.0 & 82.9 & -\\
&ConvNeXt-B \cite{liu2022convnet} & 224 & 89.0 & 15.4 & 83.8 & -\\
\midrule
\multirow{8}*{\rotatebox{90}{\makecell[c]{\textit{Transformer}\Big)}}}  &Conformer-S \cite{Peng2021ConformerLF} & 224 & 37.7 & 10.6 & 83.4 & -\\
&Swin-ACmix-S \cite{Pan2021OnTI} & 224 & 51.0 & 9.0 & 83.5 & -\\
&Focal-B \cite{Yang2021FocalSF} & 224 & 89.8 & 16.0 & 83.8 & -\\
&\textbf{ASF-former-B} & 224 & 56.7 & 12.9 & \textbf{83.9} & \textbf{96.6}\\
 &\textbf{\uline{ASF-former-B}} & 224 & 58.9 & 12.1 & \textbf{\uline{83.9}} & \textbf{\uline{96.5}}\\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

\begin{figure*}
\centering
\includegraphics[height=5.5cm]{Figures/Figure4_2.png}
\caption{The distribution of weights for HMCB and Attention branch. (a) Weights to the depth of encoder. (b) Weights to categories. (Blue/Orange denotes weights for the Attention/HMCB, this figure is best viewed in color)}
\label{fig:vis}
\end{figure*}

We further compare the ASF-former-S and ASF-former-B with SOTAs of pure CNN, transformer, and CNN-transformer hybrid. We separately present models into \textit{Small} and \textit{Big} parts, considering their computation scales (Params/MACs). We only analyzed the results of ASF-former below for brevity. However, it has to be mentioned that ASF-former performs comparably with or even better than ASF-former and further expands the advantages of our design, as shown in Table~\ref{table:sota}.

\textbf{Compared with CNN SOTAs}, our ASF-former outperforms the strong ConvNeXt regime in terms of accuracy with fewer parameters and comparable computations. For example, the {ASF-former-S} is better than the ConvNeXt-T ({82.7}\% \textit{vs} 82.1\%) in accuracy, with much fewer parameters ({19.3}M \textit{vs} 29.0M) and slightly more computations ({5.5}G \textit{vs} 4.5G). And, the {ASF-former-B} surpasses the ConvNeXt-B ({83.9}\% \textit{vs} 83.8\%) with much less parameters ({56.7}M \textit{vs} 89.0M) and MACs ({12.9}G \textit{vs} 15.4G). 

\textbf{Compared with pure transformer SOTAs}, the ASF-former performs better than the Swin Transformer and T2T-ViT regimes in terms of accuracy, Params, with comparable MACs. Specifically, the ASF-former-S achieves higher accuracy than the Swin-T and  T2T-ViT-14 (82.7\% \textit{vs} 81.2\% \textit{vs} 81.7\%), with fewer parameters (19.3M \textit{vs} 28.0M \textit{vs} 21.5M) and comparable MACs (5.5M \textit{vs} 4.5M \textit{vs} 6.1M). Besides, the ASF-former-B outperforms the Swin-B and T2T-ViT-24 under all metrics: Accuracy (83.9\% \textit{vs} 83.5\% \textit{vs} 82.6\%), Params (56.7M \textit{vs} 88M \textit{vs} 64.1M), MACs (12.9G \textit{vs} 15.4G \textit{vs} 15.0G).

\textbf{Compared with the CNN-transformer hybrid SOTAs}, the ASF-former outperforms those cascade hybrids, such as CvT and Focal Transformer, in terms of accuracy and Params, becoming the first parallel hybrid to beat cascade counterparts. Specifically, at a similar MAC scale, the ASF-former-S shows a better accuracy (82.7\% \textit{vs} 82.5\% \textit{vs} 82.2\%) and fewer params (19.3M \textit{vs} 32.0M \textit{vs} 29.1M) than CvT-21 and Focal-T. Meanwhile, the ASF-former-B is better than Focal-B under all metrics: Accuracy (83.9\% \textit{vs} 83.8\%), Params (56.7M \textit{vs} 89.8M), MACs (12.9G \textit{vs} 16.0G).

Among hybrid transformers, ViTAE, Swin-ACmix, Conformer, and our ASF-former all adopt parallel structure, while the ASF-former demonstrates the best accuracy at a similar computation scale. For example, the ASF-former-S and ASF-former-B separately surpass those best available, i.e., ViTAE-S and Swin-ACmix-S, by an accuracy margin of 82.7\%-82.0\%=0.7\% and 83.9\%-83.5\%=0.4\%. This indicates that our split parallelism with HMCB, cooperating with the adaptive fusion, can efficiently enforce the model to be lightweight and effectively boost performance via integrating parallel features.

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{Comparison with different methods on ImageNet-1k validation set after ImageNet-22k pre-training.}
\label{table:22k}
\begin{tabular}{lcccc}
\hline\noalign{\smallskip}
\multirow{2}*{Model} &Image &Params &MACs &ImageNet\\
 &Size &(M) &(G) &Top-1\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
R-101x3~\cite{Kolesnikov2020BigT}& 384 &388 &204.6 &84.4\\
ViT-B/16~\cite{dosovitskiy2021an} &384 &86 &55.4 &84.0\\
ViT-L/16~\cite{dosovitskiy2021an} &384 &307 &190.7 &\textbf{85.2}\\
Swin-B~\cite{liu2021Swin}&224 &88 &15.4 &\textbf{85.2}\\
\midrule
\textbf{ASF-Former-S} (ours) &224 &\textbf{56.7} &\textbf{12.9} &\textbf{85.2}\\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

\textbf{Compared with SOTAs under ImageNet-22k pre-training}, we also pre-train the model on the large-scale ImageNet-22k dataset and then fine-tune it on ImageNet-1k to further explore the upper bound of ASF-former. The classification results are shown in Table~\ref{table:22k}. Our ASF-former achieves the same Top-1 accuracy (85.2\%) as the strong SOTA of Swin-B but requires much less parameters (56.7M \textit{vs} 88.0M) and MACs (12.9G \textit{vs} 15.4G). These indicate the great capacity of the ASF-former as well as its superior computation efficiency.

\subsection{Distribution of Weights}

We plot the distribution of weights for HMCB and Attention branch with respect to the depth of encoder and categories in Fig.~\ref{fig:vis}(a) and (b). For simplicity, we calculate distributions using ASF-former-S.

Figure~(\ref{fig:vis}a) shows the trend of weights changing with the depth of the encoder. Specifically, the ASF-former-S contains 16 encoders. For each encoder, we calculate the mean weight of the HMCB/Attention way on ImageNet-1K Val. We observe that the domination of HMCB in early encoders gradually changes to the attention when depth becomes deeper. This finding is consistent with prior works \cite{Pan2021OnTI} that shallow layers focus on locality, whereas deep layers prefer globality, which will inspire future model designing.

Figure~(\ref{fig:vis}b) shows distribution weights on 1000 categories. We pick the third encoders as they are the most balanced for the HMCB/Attention (3rd depth in Fig.~(\ref{fig:vis}a)). We sort categories according to the descending (or increment) of HMCB (Attention) weight. We observe that categories prefer locality/globality differently, which is also affected by the depth of the encoder.
Specifically, small objects like “\textit{Ruler}” and “\textit{Can opener}” prefer the locality. Categories like “\textit{Honeycomb}” also produce large weights for the convolution branch for its requirement of capturing the local texture \& edge information. Sceneries or large objects like “\textit{Triumphal arch}”, "\textit{Kimono}" and "\textit{Leonberg}" prefer global information.

\subsection{Transferability to downstream datasets}

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{Transferability to CIFAR-10/100.}
\label{table:generalization}
\begin{tabular}{lcccc}
\hline\noalign{\smallskip}
Model &Params &CIFAR-10 &CIFAR-100\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
ViT-B/16 \cite{dosovitskiy2021an} & 86.5 & 98.1 & 87.1\\
ViT-L/16 \cite{dosovitskiy2021an} & 304.3 & 97.9 & 86.4\\
T2T-ViT-14 \cite{yuan2021tokens} &21.5 &97.5 &88.4 \\
TNT-S \cite{tnt} &23.8 &98.7 &90.1\\
ViTAE-S \cite{Xu2021ViTAEVT} &23.6 &98.8 &90.8 \\
CeiT-S \cite{Yuan2021IncorporatingCD}  &24.2 &\textbf{99.1} &90.8\\
DeiT-B \cite{Touvron2021TrainingDI} & 86.6 & \textbf{99.1} & 90.8\\

\midrule
\textbf{ASF-former-S} &19.3 &98.7 &90.4 \\
\textbf{ASF-former-B} &56.7 &98.8 &\textbf{91.0} \\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

To investigate the transferability of our ASF-former, we further fine-tune the proposed models on CIFAR-10 and CIFAR-100 datasets. The initial learning rate is 0.025 for CIFAR-10 and 0.05 for CIFAR-100.

The validation results are shown in Table~\ref{table:generalization}. Our ASF-former achieves comparable results on CIFAR-10 and the state-of-the-art results on CIFAR-100 under  resolution, showing its superior transferability.

\subsection{Transferability to object detection \& instance segmentation}
\label{detection}

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{Transferability to object detection and instance segmentation task.}
\label{table:detection}
\begin{tabular}{llcccc}
\hline\noalign{\smallskip}
\multirow{2}*{Backbone} &\multirow{2}*{Method} &Lr &Params &Box &Mask\\
& &Schd &(M) &mAP &mAP\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
ResNet-50~\cite{He2016DeepRL}&Mask RCNN&1x&44&38.2&34.7\\
PVT-S~\cite{wang2021pyramid}&Mask RCNN &1x &44 &40.4 &37.8\\
ResT-Base~\cite{Zhang2021ResTAE}&Mask RCNN &1x &50 &41.6 &38.7\\
Twins-PCSVT-S~\cite{Chu2021TwinsRT}&Mask RCNN &1x &44 &42.9 &40.0\\
Twins-PVT-S~\cite{Chu2021TwinsRT}&Mask RCNN &1x &44 &43.4 &40.3\\
RegionViT-S+~\cite{Chen2021RegionViTRA}&Mask RCNN &1x &51 &43.5 &40.4\\
Conformer-S~\cite{Peng2021ConformerLF}&Mask RCNN &1x &58 &43.6 &39.7\\
Swin-T~\cite{liu2021Swin}&Mask RCNN&1x&48&43.7&39.8 \\
DAT-T~\cite{Xia2022CVPR_DAT}&Mask RCNN&1x&48 &44.4 &40.4\\
ViTAE-S~\cite{Xu2021ViTAEVT}&Mask RCNN&1x&\textbf{37}& 44.6&40.2\\
Focal-T~\cite{Yang2021FocalSF}&Mask RCNN&1x&49 &44.8 &41.0\\
\textbf{ASF-Former-S} (ours)&Mask RCNN &1x &38 &\textbf{45.9} &\textbf{41.2}\\
\midrule
ResNet-50~\cite{He2016DeepRL}&Cascade RCNN &1x &82 &41.2 &35.9\\
Swin-T~\cite{liu2021Swin}&Cascade RCNN &1x &86 &48.1 &41.7 \\   
ViTAE-S~\cite{Xu2021ViTAEVT}&Cascade RCNN &1x &\textbf{75} &48.9 &42.0\\
DAT-T~\cite{Xia2022CVPR_DAT}&Cascade RCNN &1x &86 &49.1 &42.5\\
\textbf{ASF-Former-S} (ours)&Cascade RCNN &1x &79 &\textbf{49.7} &\textbf{42.8}\\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

As described in Section~\ref{pyramid}, ASF-former is adjusted from the ASF-former, to make the transformer more compatible with the detection \& segmentation tasks. We test the performance of our ASF-former on standard object detection and instance segmentation benchmarks: MS-COCO 2017~\cite{Lin2014Mscoco} datasets. Specifically, the COCO contains 118k training images and 5k validation images. During experiments, we adopt Mask RCNN and Cascade RCNN as the detection frameworks and follow the same experimental settings as Swin~\cite{liu2021Swin} for a fair comparison. Specifically, multi-scale training and AdamW optimizer are used. The initial learning rate is set to be 0.0001 with a weight decay of 0.05. We use the batch size of 16 and 1x schedule (12 epochs).

The experimental results are shown in Table~\ref{table:detection}. We compare our ASF-former with different kinds of vision backbones, including the most widely-used pure CNN, transformer, and the recent strong hybrid models. Among them, ASF-former achieves the best results on both object detection (Box mAP) and instance segmentation (Mask mAP) tasks under two widely-used detection frameworks with comparable or fewer parameters. It indicates the superior capacity of ASF-former as a backbone for downstream tasks.

\subsection{Transferability to semantic segmentation}
\label{segmentation}

\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
\caption{Transferability to semantic segmentation task.}
\label{table:segmentation}
\begin{tabular}{llcccc}
\hline\noalign{\smallskip}
\multirow{2}*{Backbone} &\multirow{2}*{Method} &Lr &Params &\multirow{2}*{mIoU} &mIoU\\
& &Schd &(M) & &(MS)\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
ResNet-50~\cite{He2016DeepRL}&UPerNet &160K &67 &42.1 &42.9\\
Swin-T~\cite{liu2021Swin}&UPerNet &160K &60 &44.5 &45.8\\
Swin-ACmix-T~\cite{Pan2021OnTI}&UPerNet &160K &60 &45.3 &-\\
DAT-T~\cite{Xia2022CVPR_DAT}&UPerNet &160K &60 &45.5 &46.4\\
ViTAE-S~\cite{Xu2021ViTAEVT}&UPerNet &160K &\textbf{49} &45.4 &\textbf{47.8}\\
DW-T~\cite{Ren2022BeyondFD}&UPerNet &160K &61 &45.7 &46.9\\
Focal-T~\cite{Yang2021FocalSF}&UPerNet &160K &62 &45.8 &47.0\\
ConvNeXt-T~\cite{liu2022convnet}&UPerNet &160K &60 &46.0 &46.7\\
Twins-SVT-S~\cite{Chu2021TwinsRT}&UPerNet &160K &54 &46.2 &47.1\\
Twins-PCPVT-S~\cite{Chu2021TwinsRT}&UPerNet &160K &55 &46.2 &47.5\\
\textbf{ASF-Former-S} (ours)&UPerNet &160K &51 &\textbf{46.7} &47.6\\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}

We further validate our ASF-former as the backbone model for semantic segmentation tasks using the standard ADE20K~\cite{Zhou2019ade20k} dataset. It contains more than 20k images in the training set and 2k images in the validation set. Following Swin, we take the widely-used UPerNet as the framework and use the AdamW optimizer for training. The initial learning rate is set to be 0.00006 with a weight decay of 0.01, and the batch size is 16. We follow the 160K lr schedule (160k iterations) for a fair comparison.

The results are shown in Table~\ref{table:segmentation} which include the standard and multi-scale (MS) testing results. We can see that ASF-former achieves a mIoU of 46.7 in standard testing, which outperforms the other backbone models by a large margin with comparable or fewer parameters. Under a multi-scale augmentation setting, ASF-former gets comparable results to the SOTA methods. These show that our method works well as the backbone in segmentation tasks and proves its excellent transferability to downstream vision tasks.

\section{Conclusion}
In this paper, we propose a novel hybrid transformer called ASF-former. It adopts Split Parallelism, which splits channels into half for two-path inputs. It introduces the novel HMCB that complements the attention and the Adaptive Fusion for feature merging. We experimentally verified that the three mechanisms could a good balance of efficacy and efficiency and achieve SOTA results. We also validate that the role of local/global information varies with respect to visual categories and network depth. Besides, this hybrid design also achieves promising results on downstream tasks, including object detection and segmentation.

\normalem
\bibliographystyle{IEEEtran}
\bibliography{tmm}

\end{document}