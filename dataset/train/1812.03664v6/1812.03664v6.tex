\begin{table}[tbp]    
	\centering
	\tabcolsep 10pt
	\caption{Few-shot classification performance with \textbf{Wide ResNet (WRN)-28-10 backbone} on {\it Mini}ImageNet dataset (mean accuracy$\pm$95\% confidence interval). Our implementation methods are measured over 10,000 test trials.}
	{\small
		\begin{tabular}{@{\;}lcc@{\;}}
			\addlinespace
			\toprule
			Setups $\rightarrow$ & {\bf 1-Shot 5-Way} & {\bf 5-Shot 5-Way} \\    \midrule
			PFA~\cite{Qiao2017Few} & 59.60{\tiny $\pm$ 0.41} & 73.74{\tiny $\pm$ 0.19} \\
			LEO~\cite{Rusu2018Meta} & 61.76{\tiny $\pm$ 0.08} & 77.59{\tiny $\pm$ 0.12} \\
			SimpleShot~\cite{Wang2019Simple}& 63.50{\tiny $\pm$ 0.20} & 80.33{\tiny $\pm$ 0.14}\\
			ProtoNet (Ours)  & 62.60{\tiny $\pm$ 0.20} & 79.97{\tiny $\pm$ 0.14} \\
			\midrule
			{\bf Ours}: {\feat}      &  \bf 65.10 {\tiny $\pm$ 0.20} & \bf 81.11 {\tiny $\pm$ 0.14} \\
			\bottomrule
	\end{tabular}}
	\label{supp-tab:wrn}
\end{table}

\begin{table}[tbp]    
	\centering
	\tabcolsep 10pt
	\caption{Few-shot classification performance with \textbf{Wide ResNet (WRN)-28-10 backbone} on {\it Tiered}ImageNet dataset (mean accuracy$\pm$95\% confidence interval). Our implementation methods are measured over 10,000 test trials.}
	{\small
		\begin{tabular}{@{\;}lcc@{\;}}
			\addlinespace
			\toprule
			Setups $\rightarrow$ & {\bf 1-Shot 5-Way} & {\bf 5-Shot 5-Way} \\    \midrule
			LEO~\cite{Rusu2018Meta}  & 66.33{\tiny $\pm$ 0.05} & 81.44{\tiny $\pm$ 0.09} \\
			SimpleShot~\cite{Wang2019Simple}  & 69.75{\tiny $\pm$ 0.20} & \bf 85.31{\tiny $\pm$ 0.15} \\
			\midrule
			{\bf Ours}: {\feat}      &  \bf 70.41 {\tiny $\pm$ 0.23} &  84.38 {\tiny $\pm$ 0.16} \\
			\bottomrule
	\end{tabular}}
	\label{supp-tab:wrn_tiered}
\end{table}

\begin{table}[tbp]    
	\centering
	\tabcolsep 10pt
	\caption{Few-shot classification performance with ConvNet backbone on CUB dataset (mean accuracy$\pm$95\% confidence interval). Our implementation methods are measured over 10,000 test trials. }
	{\small
		\begin{tabular}{@{\;}lcc@{\;}}
			\addlinespace
			\toprule
			Setups $\rightarrow$ & {\bf 1-Shot 5-Way} & {\bf 5-Shot 5-Way} \\    \midrule
			MatchNet~\cite{VinyalsBLKW16Matching}    & 61.16 {\tiny $\pm$ 0.89} & 72.86 {\tiny $\pm$ 0.70} \\
			MAML~\cite{FinnAL17Model}                & 55.92 {\tiny $\pm$ 0.95} & 72.09 {\tiny $\pm$ 0.76} \\
			ProtoNet~\cite{SnellSZ17Prototypical}    & 51.31 {\tiny $\pm$ 0.91} & 70.77 {\tiny $\pm$ 0.69} \\
			RelationNet~\cite{Flood2017Learning}     & 62.45 {\tiny $\pm$ 0.98} & 76.11 {\tiny $\pm$ 0.69} \\ \midrule
			\multicolumn{3}{@{\;}l@{\;}}{\small \bf Instance Embedding} \\
			MatchNet                    & 67.73 {\tiny $\pm$ 0.23} & 79.00 {\tiny $\pm$ 0.16} \\
			ProtoNet                    & 63.72 {\tiny $\pm$ 0.22} & 81.50 {\tiny $\pm$ 0.15} \\
			\midrule
			\multicolumn{3}{@{\;}l@{\;}}{\small \bf Embedding Adaptation} \\
			{\textsc{bilstm}} & 62.05 {\tiny $\pm$ 0.23} & 73.51 {\tiny $\pm$ 0.19} \\
{\textsc{deepsets}} & 67.22 {\tiny $\pm$ 0.23} & 79.65 {\tiny $\pm$ 0.16} \\
{\textsc{GCN}} & 67.83 {\tiny $\pm$ 0.23} & 80.26 {\tiny $\pm$ 0.15} \\
			\midrule
			{\bf Ours}: {\feat}      & \bf 68.87 {\tiny $\pm$ 0.22} & \bf 82.90 {\tiny $\pm$ 0.15} \\
			\bottomrule
	\end{tabular}}
	\label{supp-tab:cub}
\end{table}
In this section, we will show more experimental results over the {\it Mini}ImageNet/CUB dataset, the ablation studies, and the extended few-shot learning.

\subsection{Main Results}
\label{sec:supp-main_result}
The full results of all methods on the {\it Mini}ImageNet can be found in Table~\ref{supp-tab:miniImageNet}. The results of MAML~\cite{FinnAL17Model} optimized over the pre-trained embedding network are also included. We re-implement the ConvNet backbone of MAML and cite the MAML results over the ResNet backbone from~\cite{Rusu2018Meta}. It is also noteworthy that the {\feat} gets the best performance among all popular methods and baselines.

We also investigate the Wide ResNet (WRN) backbone over {\it Mini}ImageNet, which is also the popular one used in~\cite{Qiao2017Few,Rusu2018Meta}. SimpleShot~\cite{Wang2019Simple} is a recent proposed embedding-based few-shot learning approach that takes full advantage of the pre-trained embeddings. We cite the results of PFA~\cite{Qiao2017Few}, LEO~\cite{Rusu2018Meta}, and SimpleShot~\cite{Wang2019Simple} from their papers.
The results can be found in Table~\ref{supp-tab:wrn}. We re-implement {ProtoNet} and our {\feat} approach with WRN. 
It is notable that in this case, our {\feat} achieves {\em much higher} promising results than the current state-of-the-art approaches. 
Table~\ref{supp-tab:wrn_tiered} shows the classification results with WRN on the {\it Tiered}ImageNet data set, where our {\feat} still keeps its superiority when dealing with 1-shot tasks.

Table~\ref{supp-tab:cub} shows the 5-way 1-shot and 5-shot classification results on the CUB dataset based on the ConvNet backbone. The results on CUB are consistent with the trend on the {\it Mini}ImageNet dataset. Embedding adaptation indeed assists the embedding encoder for the few-shot classification tasks. Facilitated by the set function property, the \textsc{deepsets} works better than the \textsc{bilstm} counterpart. Among all the results, the transformer based {\feat} gets the top tier results. 

\begin{figure}[!t]
	\small
	\centering
	\begin{minipage}[h]{4.1cm}
		\centering \includegraphics[height=3.3cm]{files/Interpolation.pdf}\\
		\mbox{({\it a}) \textbf{Task Interpolation}}
	\end{minipage}
	\begin{minipage}[h]{4.1cm}
		\centering \includegraphics[height=3.3cm]{files/Extrapolation.pdf}\\
		\mbox{({\it b}) \textbf{Task Extrapolation}}
	\end{minipage}
	\caption{\textbf{Interpolation} and \textbf{Extrapolation} of few-shot tasks. We train different embedding adaptation models on {\em 5-shot} 20-way or 5-way classification tasks and evaluate models on unseen tasks with different number of classes (\textit{$N$=\{5, 10, 15, 20\}}). It verifies both the interpolation and extrapolation ability of \feat on a varying number of ways in few-shot classification.}
	\label{supp-fig:changeN}
\end{figure}

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Ablation studies on whether the embedding adaptation improves the discerning quality of the embeddings. After embedding adaptation, \feat improves w.r.t. the before-adaptation embeddings a lot for Few-shot classification.}
	\begin{tabular}{@{\;}ccc@{\;}}
		\addlinespace
		\toprule
		& 1-Shot 5-Way   & 5-Shot 5-Way \\
		\midrule
		Pre-Adapt    & 51.60{\tiny $\pm$ 0.20}  & 70.40{\tiny $\pm$ 0.16} \\
		Post-Adapt  & 55.15{\tiny $\pm$ 0.20}  & 71.61{\tiny $\pm$ 0.16} \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:adapt_ablation}
\end{table}

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Ablation studies on the position to average the same-class embeddings when there are multiple shots per class in {\feat} (tested on the 5-Way tasks with different numbers of shots). ``Pre-Avg'' and ``Post-Avg'' means we get the embedding center for each class before or after the set-to-set transformation, respectively.}
	\begin{tabular}{@{\;}ccc@{\;}}
		\addlinespace
		\toprule
		Setups $\rightarrow$ & Pre-Avg & Post-Avg \\
		\midrule
		5     & 71.61{\tiny $\pm$ 0.16} & 70.70{\tiny $\pm$ 0.16}  \\
		15     & 77.76{\tiny $\pm$ 0.14} & 76.58{\tiny $\pm$ 0.14} \\
		30     & 79.66{\tiny $\pm$ 0.13} & 78.77{\tiny $\pm$ 0.13} \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:shot_change}
\end{table}

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Ablation studies on the number of heads in the Transformer of {\feat} (with number of layers fixes to one).}
	\begin{tabular}{@{\;}ccc@{\;}}
		\addlinespace
		\toprule
		Setups $\rightarrow$ & 1-Shot 5-Way & 5-Shot 5-Way \\
		\midrule
		1     & 55.15{\tiny $\pm$ 0.20} & 71.57{\tiny $\pm$ 0.16}  \\
		2     & 54.91{\tiny $\pm$ 0.20} & 71.44{\tiny $\pm$ 0.16} \\
		4     & 55.05{\tiny $\pm$ 0.20} & 71.63{\tiny $\pm$ 0.16} \\
		8     & 55.22{\tiny $\pm$ 0.20} & 71.39{\tiny $\pm$ 0.16} \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:component_head}
\end{table}

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Ablation studies on the number of layers in the Transformer of {\feat} (with number of heads fixes to one).}
	\begin{tabular}{@{\;}ccc@{\;}}
		\addlinespace
		\toprule
		Setups $\rightarrow$ & 1-Shot 5-Way & 5-Shot 5-Way \\
		\midrule
		1     & 55.15{\tiny $\pm$ 0.20} & 71.57{\tiny $\pm$ 0.16}  \\
		2     & 55.42{\tiny $\pm$ 0.20} & 71.44{\tiny $\pm$ 0.16} \\
		3     & 54.96{\tiny $\pm$ 0.20} & 71.63{\tiny $\pm$ 0.16} \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:component_layer}
\end{table}

\subsection{Ablation Studies}
\label{sec:supp-ablation}
In this section, we perform further analyses for our proposed {\feat} and its ablated variants classifying in the {ProtoNet} manner, on the {\it Mini}ImageNet dataset, using the ConvNet as the backbone network. 

\paragraph{Do the adapted embeddings improve the pre-adapted embeddings?} We report few-shot classification results by using the pre-adapted embeddings of support data (\ie, the embedding before adaptation), against those using adapted embeddings, for constructing classifiers. Table~\ref{supp-tab:adapt_ablation} shows that task-specific embeddings after adaptation improves over task-agnostic embeddings in few-shot classifications.


\paragraph{Can \feat possesses the characteristic of the set function?} We test three set-to-set transformation implementations, namely the \textsc{bilstm}, the \textsc{deepsets}, and the Transformer (\feat), w.r.t. two important properties of the set function, \ie, task interpolation and task extrapolation. In particular, the few-shot learning model is first trained with 5-shot 20-way tasks. Then the learned model is required to evaluate different 5-shot tasks with $N=\{5,10,15,20\}$ (Extrapolation). Similarly, for interpolation, the model is trained with 5-shot 20-way tasks in advance and then evaluated on the previous multi-way tasks. The classification change results can be found in Figure~\ref{supp-fig:changeN} (a) and (b). \textsc{bilstm} cannot deal with the size change of the set, especially in the task extrapolation. In both cases, {\feat} still gets improvements in all configurations of $N$. 


\paragraph{When to average the same-class embeddings?}When there is more than one instance per class, \ie $M > 1$, we average the instances in the same class and use the class center to make predictions as in Eq.~\ref{supp-eq:center}. There are two positions to construct the prototypes in {\feat} --- before the set-to-set transformation (Pre-Avg) and after the set-to-set transformation (Post-Avg). In Pre-Avg, we adapt the embeddings of the centers, and a test instance is predicted based on its distance to the nearest adapted center; while in Post-Avg, the instance embeddings are adapted by the set-to-set function first, and the class centers are computed based on the adapted instance embeddings. We investigate the two choices in Table~\ref{supp-tab:shot_change}, where we fix the number of ways to 5 ($N=5$) and change the number of shots ($M$) among $\{5,15,30\}$. The results demonstrate the Pre-Avg version performs better than the Post-Avg in all cases, which shows a more precise input of the set-to-set function by averaging the instances in the same class leads to better results. So we use the Pre-Avg strategy as a default option in our experiments.

\paragraph{Will deeper and multi-head transformer help?} In our current implementation of the set-to-set transformation function, we make use of a shallow and simple transformer, \ie, one layer and one head (set of projection). From~\cite{VaswaniNIPS17Attention}, the transformer can be equipped with complex components using multiple heads and deeper stacked layers. We evaluate this augmented structure, with the number of attention heads increases to 2, 4, 8, as well as with the number of layers increases to 2 and 3. As in Table~\ref{supp-tab:component_head} and Table~\ref{supp-tab:component_layer}, we empirically observe that more complicated structures do not result in improved performance. We find that with more layers of transformer stacked, the difficulty of optimization increases and it becomes harder to train models until their convergence. Whilst for models with more heads, the models seem to over-fit heavily on the training data, even with the usage of auxiliary loss term (like the contrastive loss in our approach). It might require some careful regularizations to prevent over-fitting, which we leave for future work.

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Ablation studies on effects of the contrastive learning of the set-to-set function on {\feat}.}
	\begin{tabular}{@{\;}lcc@{\;}}
		\toprule
		Setups $\rightarrow$ & \bf 1-Shot 5-Way & \bf 5-Shot 5-Way \\\midrule
		$\lambda = 10$  & 53.92 {\tiny $\pm$ 0.20} & 70.41 {\tiny $\pm$ 0.16} \\
		$\lambda = 1$  & 54.84 {\tiny $\pm$ 0.20} & 71.00 {\tiny $\pm$ 0.16} \\
		$\lambda = 0.1$ & \bf 55.15 {\tiny $\pm$ 0.20} & \bf 71.61 {\tiny $\pm$ 0.16} \\
		$\lambda = 0.01$ & 54.67 {\tiny $\pm$ 0.20} & 71.26 {\tiny $\pm$ 0.16} \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:reg1}
\end{table}

\paragraph{The effectiveness of contrastive loss.} 
Table~\ref{supp-tab:reg1} show the few-shot classification results with different weight values ($\lambda$) of the contrastive loss term for {\feat}. From the results, we can find that the balance of the contrastive term in the learning objective can influence the final results. Empirically, we set $\lambda=0.1$ in our experiments. 

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Ablation studies on the prediction strategy (with cosine similarity or euclidean distance) of {\feat}.}
	\begin{tabular}{@{\;}lcccc@{\;}}
		\addlinespace
		\toprule
		Setups $\rightarrow$ & \multicolumn{2}{c}{\bf 1-Shot 5-Way} & \multicolumn{2}{c}{\bf 5-Shot 5-Way} \\
		\midrule
		Backbone $\rightarrow$ & ConvNet & ResNet & ConvNet  & ResNet \\
		\midrule
		\multicolumn{3}{@{\;}l@{\;}}{\small \bf Cosine Similarity-based Prediction} \\
		{\feat}     & 54.64{\tiny $\pm$ 0.20} & 66.26{\tiny $\pm$ 0.20} & 71.72{\tiny $\pm$ 0.16} & 81.83{\tiny $\pm$ 0.15} \\
\midrule
		\multicolumn{3}{@{\;}l@{\;}}{\small \bf Euclidean Distance-based Prediction} \\
		{\feat}     & 55.15{\tiny $\pm$ 0.20} & 66.78{\tiny $\pm$ 0.20} & 71.61{\tiny $\pm$ 0.16} & 82.05{\tiny $\pm$ 0.14} \\
\bottomrule
	\end{tabular}
	\label{supp-tab:prediction_compare}
\end{table}

\paragraph{The influence of the prediction strategy.} We investigate two embedding-based prediction ways for the few-shot classification, \ie, based on the cosine similarity and the negative euclidean distance to measure the relationship between objects, respectively. We compare these two choices in Table~\ref{supp-tab:prediction_compare}. 
Two strategies in Table~\ref{supp-tab:prediction_compare} only differ in their similarity measures. In other words, with more than one shot per class in the task training set, we average the same class embeddings first, and then make classification by computing the cosine similarity or the negative euclidean distance between a test instance and a class prototype. 
During the optimization, we tune the logits scale temperature for both these methods. 
We find that using the euclidean distance usually requires small temperatures (\eg, $\gamma = \frac{1}{64}$) while a large temperature (\eg, $\gamma=1$) works well with the normalized cosine similarity. The former choice achieves a slightly better performance than the latter one.

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Cross-Domain 1-shot 5-way classification results of the {\feat} approach.}
	\begin{tabular}{@{\;}lccc@{\;}}
		\addlinespace
		\toprule
		& \bf C $\rightarrow$ C & \bf C $\rightarrow$ R & \bf R $\rightarrow$ R\\ 
		\midrule
		Supervised     & 34.38{\tiny $\pm$0.16} & 29.49{\tiny $\pm$0.16} & 37.43{\tiny $\pm$0.16}\\
		{ProtoNet}    & 35.51{\tiny $\pm$0.16} &  29.47{\tiny $\pm$0.16} & 37.24{\tiny $\pm$0.16}\\
		\midrule
		{\feat}      & \bf 36.83{\tiny $\pm$0.17} & \bf 30.89{\tiny $\pm$0.17} & \bf 38.49{\tiny $\pm$0.16}\\
		\bottomrule
	\end{tabular}
	\label{supp-tab:generalization1}
\end{table}

\subsection{Few-Shot Domain Generalization} 
\label{sec:supp-generalization}
We show that \feat learns to adapt \textit{the intrinsic structure of tasks}, and \textbf{generalize across domains}, \ie, predicting test instances even when the visual appearance is changed.

\par\noindent\textbf{Setups.} We train a few-shot learning model in the standard domain and evaluate it with cross-domain tasks, where the $N$-categories are aligned but domains are different. In detail, a model is trained on tasks from the ``Clipart'' domain of OfficeHome dataset~\cite{Venkateswara2017Office}, then the model is required to generalize to both ``Clipart (\textbf{C})'' and ``Real World (\textbf{R})'' instances. In other words, we need to classify complex real images by seeing only a few sketches, or even based on the instances in the ``Real World (\textbf{R})'' domain. 

\par\noindent\textbf{Results.} Table~\ref{supp-tab:generalization1} gives the quantitative results. Here, the ``supervised'' refers to a model trained with standard classification and then is used for the nearest neighbor classifier with its penultimate layer's output feature. We observe that {ProtoNet} can outperform this baseline on tasks when evaluating instances from ``Clipart'' but not ones from ``real world''. However, \feat can improve over ``real world'' few-shot classification even only seeing the support data from ``Clipart''. Besides, when the support set and the test set of the target task are sampled from the same but new domains, \eg, the training and test instances both come from ``real world'', {\feat} also improves the classification accuracy w.r.t. the baseline methods. It verifies the domain generalization ability of the {\feat} approach. 

\begin{table}[tbp]
	\centering
	\small
	\tabcolsep 5pt
	\caption{Results of models for transductive FSL with ConvNet backbone on {\it Mini}ImageNet. We cite the results of Semi-ProtoNet and TPN from~\cite{Ren2018Meta} and~\cite{Qiao2019Transductive} respectively. For TEAM~\cite{Qiao2019Transductive}, the authors do not report the confidence intervals, so we set them to 0.00 in the table. $\feat^\dagger$ and $\feat^\ddagger$ adapt embeddings with the joint set of labeled training and unlabeled test instances, while make prediction via ProtoNet and Semi-ProtoNet respectively. }
	\begin{tabular}{@{\;}lcc@{\;}}
		\addlinespace \toprule
		Setups $\rightarrow$ & \bf 1-Shot 5-Way & \bf 5-Shot 5-Way \\
		\midrule
		\multicolumn{3}{@{\;}l@{\;}}{\bf Standard} \\
		ProtoNet                            & 52.61 {\tiny $\pm$ 0.20} & 71.33 {\tiny $\pm$ 0.16} \\
		{\feat}     & 55.15 {\tiny $\pm$ 0.20} & 71.61 {\tiny $\pm$ 0.16} \\ 
\midrule
		\multicolumn{3}{@{\;}l@{\;}}{\bf Transductive} \\
		Semi-ProtoNet~\cite{Ren2018Meta}    & 50.41 {\tiny $\pm$ 0.31} & 64.39 {\tiny $\pm$ 0.24} \\
		TPN~\cite{Liu2018TPN}               & 55.51 {\tiny $\pm$ 0.84} & 69.86 {\tiny $\pm$ 0.67} \\
		TEAM~\cite{Qiao2019Transductive}    & 56.57 {\tiny $\pm$ 0.00} & 72.04 {\tiny $\pm$ 0.00} \\
		Semi-ProtoNet (Ours)    & 55.50 {\tiny $\pm$ 0.10} & 71.76 {\tiny $\pm$ 0.08} \\
		{\feat}$^\dagger$  &  56.49 {\tiny $\pm$ 0.16} &  72.65 {\tiny $\pm$ 0.20} \\
		{\feat}$^\ddagger$  & \bf 57.04 {\tiny $\pm$ 0.16} & \bf 72.89 {\tiny $\pm$ 0.20} \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:transductive}
\end{table}

\subsection{Additional Discussions on Transductive FSL}
\label{sec:supp-extended}
We list the results of the transductive few-shot classification in Table~\ref{supp-tab:transductive}, where the unlabeled test instances arrive simultaneously, so that the common structure among the unlabeled test instances could be captured.
We compare with three approaches, Semi-ProtoNet~\cite{Ren2018Meta}, TPN~\cite{Liu2018TPN}, and TEAM~\cite{Qiao2019Transductive}. Semi-ProtoNet utilizes the unlabeled instances to facilitate the computation of the class center and makes predictions similar to the prototypical network; TPN meta learns a label propagation way to take the unlabeled instances relationship into consideration; TEAM explores the pairwise constraints in each task, and formulates the embedding adaptation into a semi-definite programming form. We cite the results of Semi-ProtoNet from \cite{Ren2018Meta}, and cite the results of TPN and TEAM from \cite{Qiao2019Transductive}. We also re-implement  Semi-ProtoNet with our pre-trained backbone (the same pre-trained ConvNet weights as the standard few-shot learning setting) for a fair comparison.

In this setting, our model leverages the unlabeled test instances to augment the transformer as discussed in \S~\ref{sec:supp-transformer} and the embedding adaptation takes the relationship of all test instances into consideration. Based on the adapted embedding by the joint set of labeled training instances and unlabeled test instances, we can make predictions with two strategies. First, we still compute the center of the labeled instances, while such adapted embeddings are influenced by the unlabeled instances (we denote this approach as {\feat}$^\dagger$, which works the same way as standard {\feat} except the augmented input of the embedding transformation function); Second, we consider to take advantage of the unlabeled instances and use their adapted embeddings to construct a better class prototype as in Semi-ProtoNet (we denote this approach as {\feat}$^\ddagger$).

By using more unlabeled test instances in the transductive environment, {\feat}$^\dagger$ achieves further performance improvement compared with the standard {\feat}, which verifies the unlabeled instances could assist the embedding adaptation of the labeled ones. With more accurate class center estimation, {\feat}$^\ddagger$ gets a further improvement. The performance gain induced by the transductive \feat is more significant in the one-shot learning setting compared with the five-shot scenario, since the helpfulness of unlabeled instance decreases when there are more labeled instances.

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{Results of generalized {\feat} with ConvNet backbone on {\it Mini}ImageNet. All methods are evaluated on instances composed by \textsc{seen} classes, \textsc{unseen}  classes, and both of them (\textsc{combined}), respectively.}
	\begin{tabular}{@{\;}lccc@{\;}}
		\addlinespace
		\toprule
		Measures $\rightarrow$ & \bf \textsc{seen} & \bf \textsc{unseen} & \bf \textsc{combined}\\ \midrule
		\multicolumn{4}{@{\;}l}{\bf 1-shot learning}\\
		ProtoNet                  & 41.73{\tiny $\pm$0.03} & 48.64{\tiny $\pm$0.20} &  35.69{\tiny $ \pm$0.03} \\
		{{\feat}}       & \bf 43.94{\tiny $\pm$0.03} & \bf 49.72{\tiny $\pm$0.20} & \bf 40.50{\tiny $\pm$0.03} \\
		\midrule
		\multicolumn{4}{@{\;}l}{\bf 5-shot learning}\\
		ProtoNet                  & 41.06{\tiny $\pm$0.03} & 64.94{\tiny $\pm$0.17} & 38.04{\tiny $ \pm$0.02} \\
		{{\feat}}       & \bf 44.94{\tiny $\pm$0.03} & \bf 65.33{\tiny $\pm$0.16} & \bf 41.68{\tiny $\pm$0.03} \\
		\midrule
		Random Chance & 1.56 & 20.00 & 1.45 \\
		\bottomrule
	\end{tabular}
	\label{supp-tab:joint}
\end{table}

\subsection{More Generalized FSL Results}
Here we show the full results of {\feat} in the generalized few-shot learning setting in Table~\ref{supp-tab:joint}, which includes both the 1-shot and 5-shot performance. All methods are evaluated on instances composed by \textsc{seen} classes, \textsc{unseen}  classes, and both of them (\textsc{combined}), respectively. In the 5-shot scenario, the performance improvement mainly comes from the improvement of over the \textsc{Unseen} tasks.

\subsection{Large-Scale Low-Shot Learning}
Similar to the generalized few-shot learning, the large-scale low-shot learning~\cite{HariharanG17Low,Gidaris2018Dynamic,Wang2018Low} considers the few-shot classification ability on both \textsc{seen} and \textsc{unseen} classes on the full ImageNet~\cite{RussakovskyDSKS15ImageNet} dataset. There are in total 389 \textsc{seen} classes and 611 \textsc{unseen} classes~\cite{HariharanG17Low}. We follow the setting (including the splits) of the prior work~\cite{HariharanG17Low} and use features extracted based on the pre-trained ResNet-50~\cite{he2016deep}.
Three evaluation protocols are evaluated, namely the top-5 few-shot accuracy on the \textsc{unseen} classes, on the combined set of both \textsc{seen} and \textsc{unseen} classes, and the calibrated accuracy on weighted by selected set prior on the combined set of both \textsc{seen} and \textsc{unseen} classes. The results are listed in Table~\ref{supp-tab:low_shot}. We observe that {\feat} achieves better results than others, which further validates {\feat}'s superiority in generalized classification setup, a large scale learning setup.

\begin{table}[tbp]
	\small
	\tabcolsep 5pt
	\centering
	\caption{The top-5 low-shot learning accuracy over all classes on the large scale ImageNet~\cite{RussakovskyDSKS15ImageNet} dataset (w/ ResNet-50).}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{@{\;}cccccc@{\;}}
			\addlinespace
			\toprule
			{\sc Unseen} & 1-Shot   & 2-Shot     & 5-Shot     & 10-Shot    & 20-Shot \\
			\midrule
			{ProtoNet}~\cite{SnellSZ17Prototypical}    & 49.6  & 64.0  & 74.4  & 78.1  & 80.0 \\
			PMN~\cite{Wang2018Low}   & 53.3  & 65.2  & 75.9  & 80.1  & 82.6 \\
			\midrule
			{\feat}  & \bf 53.8  & \bf 65.4  & \bf 76.0  & \bf 81.2  & \bf 83.6 \\
			\midrule\midrule
			{All} & 1-Shot   & 2-Shot     & 5-Shot     & 10-Shot    & 20-Shot \\
			\midrule
			{ProtoNet}~\cite{SnellSZ17Prototypical}    & 61.4  & 71.4  & 78.0  & 80.0  & 81.1 \\
			PMN~\cite{Wang2018Low}   & 64.8  & 72.1  & 78.8  & 81.7  & 83.3 \\
			\midrule
			{\feat}  & \bf 65.1  & \bf 72.5  & \bf 79.3  & \bf 82.1  & \bf 83.9 \\
			\midrule\midrule
			{All w/ Prior} & 1-Shot   & 2-Shot     & 5-Shot     & 10-Shot    & 20-Shot \\
			\midrule
			{ProtoNet}~\cite{SnellSZ17Prototypical}    & 62.9  & 70.5  & 77.1  & 79.5  & 80.8 \\
			PMN~\cite{Wang2018Low}   & 63.4  & 70.8  & 77.9  & 80.9  & 82.7 \\
			\midrule
			{\feat}  & \bf 63.8  & \bf 71.2  & \bf 78.1  & \bf 81.3  & \bf 83.4 \\
			\bottomrule
		\end{tabular}
	}
	\label{supp-tab:low_shot}
\end{table}