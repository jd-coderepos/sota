\documentclass[conference]{IEEEtran}

\usepackage{setspace}
\usepackage{color}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfig}
\usepackage[ruled]{algorithm2e}
\usepackage{balance}

\newcommand{\hi}[1]{\colorbox{yellow}{#1}}


\newenvironment{myitemize}
{
    \begin{list}{- \ }{}
        \setlength{\topsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\partopsep}{0pt}
        \setlength{\parsep}{0pt}         
        \setlength{\itemsep}{0pt} 
}
{
    \end{list} 
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{assertion}[theorem]{Assertion}


\newcommand{\prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\E}[1]{\mathbb{E}\left[ #1\right]}
\newcommand{\ang}[1]{\left\langle #1 \right\rangle}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\bigo}[1]{O\left( #1\right)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\bin}[2]{\left(\begin{array}{c} #1 \\ #2 \end{array} \right)}
\newcommand{\comment}[1]{}



\begin{document}

\title{Go-With-The-Winner: Client-Side Server Selection for Content Delivery}

\author{Chang Liu, Ramesh K. Sitaraman, and Don Towsley\\
University of Massachusetts, Amherst\hspace{0.2in}Akamai Technologies Inc.\\
\texttt{\{cliu, ramesh, towsley\}@cs.umass.edu}}



\maketitle

\begin{abstract}
Content delivery networks deliver much of the world's web and video content by deploying a large distributed network of servers. We model and analyze a simple paradigm for client-side server selection that is commonly used in practice  where each user independently measures the performance of a set of candidate servers and selects the one that performs the best. For web (resp., video) delivery, we propose and analyze a simple algorithm where each user randomly chooses two or more candidate servers and selects the server that provided the best hit rate (resp., bit rate). We prove that the algorithm converges quickly to an optimal state where all users receive the best hit rate (resp., bit rate), with high probability. We also show that if each user chose just one random server instead of two,  some users receive a hit rate (resp., bit rate) that tends to zero. We simulate our algorithm and evaluate its performance with varying choices of parameters, system load, and content popularity. 
\end{abstract}

\section{Introduction}
Modern content delivery networks (CDNs) host and deliver a large fraction of  the world's  web content, video content, and application services on behalf of enterprises that include most major web portals, media outlets, social networks, application providers, and news channels \cite{nygren2010akamai}.  CDNs deploy large numbers of servers around the world that can store content and deliver that content to users who request it. When a user requests  a content item, say a web page or a video,  the user is directed to one of  the CDN's servers that can serve the desired content to the user. The goal of a CDN is to maximize the performance as perceived by the user while efficiently managing its server resources.  

A key functionality of a CDN is the {\em server selection} process by which client software running on the user's computer or device, such as media player or a browser, is directed to a suitable server of a CDN \cite{dilley2002globally}.  The desired outcome of the server selection process is that each user is directed to a server that can provide the requested content with good performance. The metrics for performance that are optimized vary by the type of content being accessed. For instance, good performance for a user accessing a web page might mean that the web page downloads quickly. Good performance for a user watching a video might mean that the content is delivered by the server at a sufficiently high bit rate to avoid the video from freezing and rebuffering \cite{KrishnanS12}.

Server selection can be performed in two distinct ways that are not mutually exclusive. {\em Network-side server selection} algorithms monitor the real-time characteristics of the CDN and the Internet. Such algorithms are often complex and measure liveness and load of the CDN's servers, as well as latency, loss, and bandwidth of the communication paths between servers and users. Using this information, the algorithm computes a good ``mapping'' of users to servers, such that each user is assigned a ``proximal'' server capable of serving that user's content \cite{nygren2010akamai}. This mapping is computed periodically and is typically made available to client software running on the user's computer or device using the domain name system (DNS). Specifically, the user's browser or media player looks up the domain name of the content that it wants to download and receives as translation the ip address of the selected server.
 
A complementary approach to network-side server selection that is commonly is used is {\em client-side server selection} where the client software running on the user's computer or device embodies a server selection algorithm. The client software is typically unaware of the global state of the server infrastructure, the Internet, or other users. Rather, the client software typically makes future server selection decisions based on its own historical performance measurements from past server downloads. Client-side server selection can often be implemented as a plug-in within media players,  web browsers, and web download managers \cite{AkamaiDLM}.  
 
 While client-side server selection can be used to select servers within a single CDN, it can also be used in a multi-CDN setting. Large content providers often make the same content available to the user via multiple CDNs. In this case,  the client software running on the user's device tries out the different CDNs and chooses the ``best''  server  from across multiple  CDNs. For instance, NetFlix uses three different CDNs and the media player incorporates a client-side server selection algorithm to choose the ``best'' server (and the corresponding CDN) using performance metrics such as the video bit rates achievable from the various choices  \cite{adhikari2012unreeling}. Note also that in the typical multi-CDN  case, both network-side and client-side server selection are used together, where the former is used to choose the candidate servers from each CDN and the latter is used by the user to pick the ``best'' among all the candidates.  
 
  
 
 \subsection{The Go-With-The-Winner paradigm}
A common and intuitive paradigm that is often used for client-side server selection in practice is what we call  {\em ``Go-With-The-Winner''}  that consist of an initial {\em trial period}  during which each user independently ``tries out'' a set of {\em candidate servers} by requesting content or services from them (cf. Figure~\ref{fig:serverselection}). Subsequently, each user independently {\em decides} on the ``best'' performing server using historical performance information that the user collected for the candidate servers during the trial period. It is commonly implemented in the content delivery context that incorporate choosing a web or video content server from among a cluster of such servers. 

Besides content delivery, the Go-With-The-Winner paradigm is also common for other Internet services, though we do not explicitly study such services in our work.  For instance, BIND, which is the most widely deployed DNS resolver (i.e., DNS client) on the Internet,  tracks performance as a smoothed value of the historical round trip times (called SRTT) from past queries  for a set of candidate name servers. Then BIND chooses a particular name server to query in part based on the computed SRTT values \cite{liu2009dns}. It is also notable that BIND implementations incorporate randomness in the candidate selection process.



\begin{figure}[t]
\centering
\includegraphics[width=0.35\textwidth]{serverselection}
\caption{Client-side Server Selection with the Go-With-The-Winner paradigm. User  makes request to two candidate servers  and . After a trial period of observing the performance provided by the candidate, the user selects the better performing server.}
\label{fig:serverselection}
\end{figure}
The three key characteristics of the Go-With-The-Winner paradigm are as follows.
\begin{enumerate}
\item {\em Distributed control.} Each user makes decisions in a distributed fashion using only knowledge available to it. There is no explicit information about the global state of the servers or other users, beyond what the user can infer from it's own historical experience.
\item {\em Performance feedback only.} There is no explicit feedback from a server to a user who requested service beyond what can be inferred by the performance experienced by the user.
\item {\em Choosing the ``best'' performer.} The selection criteria is based on historical performance measured by the user and consists of selecting the best server according to some performance metric (i.e., go with the winner).
\end{enumerate}
Besides its inherent simplicity and naturalness, the paradigm is sometimes the only feasible and robust solution.  For instance, in many settings, the client software running on  the user's device that performs server selection has no detailed knowledge of the state of the server infrastructure as it is managed and owned by other business entities. In this case, the primary feedback mechanism for the client is its own historical performance measurements.

While client-side server selection is widely implemented, its theoretical foundations are not well understood. A goal of our work is to provide such a foundation in the context of web and video content delivery.  {\em It is not our intention to model a real-life client-side server selection process in its entirety which can involve other adhoc implemention-specific considerations. But rather we abstract an analytical model that we can explore to extract basic principles of the paradigm that are applicable in a broad context.}


\subsection{Our contributions}
 We propose a simple theoretical model for the study of client-side server selection algorithms that use the  Go-With-The-Winner paradigm. Using our model, we answer foundational questions such as how does randomness help in the trial period when selecting candidate servers? How many candidate servers should be selected in the trial phase? How long does it take for users to narrow down their choice and decide on a single server? Under what conditions does the selection algorithm converge to a state where all users have made the correct server choices, i.e., the selected servers provide good performance to their users? Some of our key results that help answer these questions follow.

{\em (1)}  In Section~\ref{sec:maxhitrate}, in the context of web content delivery, we analyze a simple algorithm called  where each user independently selects two or more random servers as candidates and decides on the server that provided the best cache hit rate,. We show that with high probability, the algorithm converges quickly to a state where no cache is overloaded and all users obtain a 100\% hit rate. Further, we show that  two or more random choices of candidate servers are necessary, as just one random choice will result in some users (and some servers) incurring cache hit rates that tend to zero, as the number of users and servers tend to infinity. This work represents the first demonstration of the ``power of two choices''  phenomena in the context of client-side server selection for content delivery,  akin to similar phenomena observed in balls-into-bins games \cite{mitzenmacherRS2001}, load balancing,  circuit-switching algorithms \cite{cole1998randomized}, relay allocation for services like Skype \cite{Nguyen:2008}, and multi-path communication \cite{Peter:2007}. 

{\em (2)}   In Section~\ref{sec:maxbitrate}, in the context of video content delivery, we propose a simple algorithm called  where each user independently selects two or more random servers as candidates and decides on the server that provided the best bit rate for the video stream,  We show that with high probability, the algorithm converges quickly to a state where no server is overloaded and all users obtain the required bit rate for their video to play without freezes. Further, we show that two or more random choices of candidate servers are necessary, as just one random choice will result in some users receiving bit rates that tend to zero, as the number of users and servers tends to infinity. 

{\em (3)} In Section~\ref{sec:empirical}, we go beyond our theoretical model and simulate algorithm  in more complex settings. We establish an inverse relationship between the length of the history used for hit rate computation (denoted by  ) and the failure rate defined as the probability that the system converges to a non-optimal state. We show that as  increases the convergence time increases, but the failure rate decreases. We  also empirically evaluate the impact of the number of choices of candidate servers. We show that two or more random choices are required for all users to receive a  hit rate. Though even if only  70\% of the users  make two choices,  it is sufficient for   of the users to receive a  hit rate. Finally, we show that the convergence time increases with system load. But, convergence time decreases when the exponent of power law distribution that describes content popularity increases. 


\section{Hit Rate Maximization for Web Content}
\label{sec:maxhitrate}
The key measure of web performance is {\em download time} which is the  time taken for a user to download a web object, such as a html page or an embedded image. CDNs enhance web performance by deploying a large number servers in access networks that are ``close'' to the users. Each server has a cache that is capable of storing web objects. When a user requests an object, such as a web page,  the user is directed to a server that can serve the object (cf. Figure~\ref{fig:serverselection}). If the server already has the object in its cache, i.e, the user's request is a  {\em cache hit}, the object is served from the cache to the user. In this case, the user experiences good performance, since the CDN's servers are proximal to the user and the object is downloaded quickly.  However, if the requested object is not in the server's cache, i.e., the user's request is a {\em cache miss}, then the server first fetches it from the origin, places it in its cache, and then serves the object to the user. In the case of a cache miss, the performance experienced by the user is often poor since the origin server is typically far away from the server and the user. In fact, if there is a cache miss, the user would have been better off not using the CDN at all, since downloading the content directly from the content provider's origin would likely have been faster!  Since the size of a server's cache is bounded, cache misses are inevitable.  A key goal of server selection for web content delivery is to jointly orchestrate server assignment and content placement in caches such that the cache hit rate is maximized. While server selection in CDNs is a complex process \cite{nygren2010akamai}, we analytically model the key elements that relate to content placement and cache hit rates,  leaving other factors that impact performance such as server-to-user latency for future work. 


\subsection{Problem Formulation}
Let   be a set of  users who each request an object picked independently from a set  of size  using a power law distribution where the  most popular object in  is picked with a probability  where  is the exponent of the distribution and  is the generalized harmonic number that is the normalizing constant, i.e.,  . Note that power law distributions (aka Zipf distributions) are commonly used to model the popularity of  online content such as web pages, and videos. This family of distributions is parametrized by a Zipf rank exponent   with  representing the extreme case of an uniform distribution and larger values of  representing a greater skew in the popularity. It has been estimated that the popularity of web content can be modeled by a power law distribution with an   in the range from 0.65 to 0.85 \cite{Breslau:1999,Gill:2007,Fricker:2012}.
The user then sticks with that content and makes a sequence of requests to the set of available servers. Relating to the reality, users tend to stay with one website for a while, say reading the news or looking at a friend's posts. Here the \textsl{whole website} is what we considered a content. We model  the sequence of requests generated by each user as a Poisson process with a homogeneous arrival rate .  Note that each request from user  can be sent to one or more servers selected from , where  is the server set chosen by user .

Let   be the set of  servers that are capable of serving content to the users. Each server can cache at most  objects and a cache replacement policy such as LRU is used to evict objects when the cache is full. Given that the download time of a web object is significantly different when the request is a cache hit versus a cache miss, we make the reasonable assumption that the user can reliably infer if its request to download an object from a server resulted in a  cache hit or a cache miss immediately after the download completes.

The objective of client-side server selection is for each user  to independently  select a server  using only the performance feedback obtained on whether each request was a hit or a miss. Let  the hit rate function  denote the probability of user  receiving a hit from server  at time . We define the system-wide performance measure , as the best hit rate obtained by the worst user at time ,
 
a.k.a. the \textsl{minmax hit rate}.  Our goal is to maximize .In the rest of the section, we describe a simple canonical  ``Go-With-The-Winner''  algorithm for server selection and show that it converges quickly to an optimal state, with high probability.

{\em Note:} Our formulation is intentionally simple so that it could model a variety of other situations in web content delivery. For instance, a single server could in fact model a cluster of front-end servers that share a single backend object cache. A single object could in fact model a bucket of objects that cached together as is often done in a CDN context \cite{nygren2010akamai}. 

\subsection{The \textsl{GoWithTheWinner} Algorithm}
\comment{After each user  has picked an content item using the power law distribution described in Equation~\ref{eq:powerlaw},  algorithm \textsl{GoWithTheWinner} described below is executed independently by each user  to select a server that's likely to have the content. In this algorithm, each user locally executes a simple ``Go-With-The-Winner'' strategy of trying out  randomly chosen candidate servers initially.  Then, using the past hit rate over a time window of length  as feedback, each user independently either chooses to continue with all the servers in  or decides on a  single server that provided the best performance.  If multiple servers provided a  hit rate in line 8 of the algorithm, the user decides to use the first one found.} 

After each user  selects a content item and a set of  servers ,  the user executes algorithm \textsl{GoWithTheWinner} to select a server likely to have the content. In this algorithm, each user locally executes a simple ``Go-With-The-Winner'' strategy of trying out  randomly chosen candidate servers initially. For each server , the user keeps track of the most recent request results in a vector  where  is the -th recent request results in a hit from server  and  if otherwise. We call  the sliding window size. Using the hit rates, each user then independently either chooses to continue with all the servers in  or decides on a  single server that provided good performance. If there are multiple servers providing  hit rate, the user decides to use the first one found.



  \LinesNumbered
  \PrintSemicolon
  \SetAlgoLined
  \SetNlSty{textsf}{}{}
  \begin{algorithm}[t]
    \caption{GoWithTheWinner}
Each user  independently chooses a random subset  of candidate servers such that  and does the following.\\
    \For{each }{
      set \;
      }
      \For{each arrival of request}{
        set  to the current time\;
        Request content  from {\em all} servers \;
        \For{each server  }{
        		\;
		\;
        		compute hit rate  \;
         	\If{}{
        			{\em decide} on server  by setting \;
			return\;
        		}
	}
    }
  \end{algorithm}



\subsection{Analysis of Algorithm MaxHitRate}
Here we rigorously analyze the case where  and experimentally explore other variants where  and  are larger than  in Section~\ref{sec:nu>ns} and~\ref{sec:empirical}.  Let    be as defined in (\ref{eq:Ht}). If , we show that with high probability , for all , where . That is,  the algorithm converges quickly with high probability to an optimal state where {\em every} user has decided on a single server that provides a 100\% hit rate,  and {\em every} server has the content requested by its users.

{\em Definitions.} A server   is said to be {\em overbooked} at some time  if users request more than  distinct content items from server , where  is the number of content items a server can hold. Note that a server may have more than  users and not be overbooked, provided the users collectively request a set of  or fewer content items. Also, note that a server that is overbooked at time  is overbooked at every  since the number of users requesting  a server can only remain the same or decrease with time. Finally, a user  is said to be {\em undecided} at time   if    and is said to be {\em decided} if it has settled on a single server to serve its content and . Note that each user starts out undecided at  time zero, then decides on a server at some time   and remains decided in all future time later than . Users calculate the hit rates of each of the available servers based on a history record of the last  requests, where  is called the sliding window size. 

\begin{lemma}\label{lem:overbookhit}
If the sliding window size , the probability that some user  decides on an overbooked server  upon any request arrival is at most  . 
\end{lemma}
\begin{proof}
If user  decides on server   then the current request together with the previous  requests are all hits. Let ,  be Bernoulli random variables, s.t.  if the most recent -th request of  is a hit and  if it is a miss. To prove Lemma \ref{lem:overbookhit} we need to show 

Let  be the time a request for content  from  is generated and appears at server . Let  be the time that the last request for  arrives at . Let  be an indicator variable so that  if the request at  resulted in a hit and  if resulted in a miss. Let  be the set of different content items requested at , where . Let  be the number of users requesting  from . WLOG, let  be the content that  requests.  is an exponential random variable, , where  is the number of users requesting  at server . Let  be an indicator that a request for  arrives at  during time interval .
Thus, random variabe  is the number of requests for different content items that arrive in the time interval. With the server running LRU replacement policy, 

because more than  different requests other than  must have arrived for content  to be swapped out of the server. (\ref{eq:hitrate2arrivals}) shows that  only depends on the arrival of other requests, which means events  are mutually independent. Furthermore\footnote{random variables  if  for all .},

where . Furthermore because , 

where .

Thus, we have 

where  is the probability density function of .

Note that  is the number of users requesting  at server , and is bounded by , with high probability \cite{raab1998balls}.

Now, we can finally prove (\ref{eq:hittau}). Let  be an appropriate constant,\\

which is  when .
\end{proof}

By bounding the time for  requests to arrive at user , we have the following,
\begin{lemma}\label{lem:timefortau}
If user  is not \textsl{decided} with server  at time , then the server is \textsl{overbooked} at time  for  where  is a constant, with high probability.
\end{lemma}

\begin{proof}
Let random variable  be the number of requests from  during time .
A bound on the tail probability of Poisson random variables is developed in \cite{PoissonTailBound} as

where  and .

Based on that we can show there are at lease  requests during  w.h.p. as the following,

as  and . Thus, w.h.p. no less than  requests arrives at . And because user is not decided at time  we know that with high probability, at least one of previous  requests receives a miss, which mean between the previous -th request and the miss, there are  different other requests arrive at the server. Thus server  is \textsl{overbooked} at the time the previous -th request arrives, which with high probability is no earlier than .
\end{proof}

Based on Lemmas \ref{lem:overbookhit} and \ref{lem:timefortau}, we can then establish the following theorem about the performance of Algorithm \textsl{GoWithTheWinner}.

\begin{theorem}
\label{thm:hitrate}
With probability at least ,  the minmax hit rate  for all , provided  and 

 That is,  with high probability, 
 algorithm \textsl{GoWithTheWinner} converges by time  to an optimal state where each user  has decided on a server  that serves it content with a  hit rate. 
\end{theorem}

This is the main result for the performance analysis of the algorithm. Due to space limit, please see appendix for detailed proof of this theorem. 

Are two or more random choices necessary for all users to receive a  hit rate? Analogous to the ``power of two choices''  in the balls-into-bins context \cite{mitzenmacherRS2001}, we show that two or more choices are required for good performance. 
\begin{theorem}
\label{thm:onechoice}
For any fixed constants  and , when algorithm  uses one random choice for each user (), the minmax hit rate  with high probability, i.e.,   tends to zero as  tends to infinity, with high probability.  
\end{theorem}
Please see appendix for the proof.
\comment{
\begin{proof}
From the classical analysis of throwing  balls into  bins \cite{mitzenmacherRS2001}, we know that there exist a subset  such that   and all users in  have chosen a single server , with high probability. Now we show that some user in  must have a small hit rate with high probability. Let  represent the set of all objects accessed by all users in . The probability that  can be upper bounded as follows, where  is an arbitrarily slowly growing function of . The number of ways of picking  objects from a set  of  objects is at most . The probability that a user  in  will pick an object in  can be upper bounded by the probability that a user chooses one of the  most popular objects. Thus the probability that a user in  picks an object in   is at most , where  is the  generalized harmonic number and .Thus, the probability that all users in  pick objects in   is at most .  Therefore, the probability that  is at most

Thus, probability that  is small and hence , with high probability. Since the minmax hit rate  is at most  which is at most ,  tends to zero with high probability.
\end{proof}
}
\subsection{The case when }\label{sec:nu>ns}
Now we analyze the case that there are much more users than the number of servers, which is often the case in reality. 
Let  be the number of users associated with  and  be the maximum over all servers.  
Assuming  so that all users are initiated with only one randomly selected server, we have the following results on the maximum incoming users over all servers  and server capacity  for the system to converge to the optimal state that every user gets hit rate .
\begin{theorem}
\label{lem:nu>ns}
\begin{enumerate}
\item When , with probability ,  the maximum load (number of associated users) over all servers . If , all users have hit rate 1.\\
\item When , with probability , the maximum load over all servers . Thus if , all users get hit rate .
\end{enumerate}
\end{theorem}

Theorem~\ref{lem:nu>ns} implies that when  all the servers have balanced load of , thus we don't need more server selection mechanism for load balancing other than just letting all users randomly choose the server. And in this case, it's not beneficial to let users start with more than  randomly selected servers, because with  the load on all servers are balanced already. Thus, as long as we have feasible server capacity  for  and  for , all the users will have enough resources from the server and have  hit rate by randomly select  server.

The number of content items  here doesn't not affect the result of load balancing. Actually, the result stays the same when . And when the number of content items is much smaller than number of users, , the cache size can become smaller () because the number of distinct requests at each server becomes smaller.

\section{Bit rate Maximization for Video Content}
\label{sec:maxbitrate}
In video streaming, a key performance metric is the {\em bit rate} at which an user can download the video stream. If the server is unable to provide the required bit rate  to the user, the video will freeze frequently resulting in an inferior viewing experience and reduced user engagement \cite{KrishnanS12}.  For simplicity, we model the server's bandwidth capacity that is often the critical bottleneck  resource, while leaving other factors that could  influence video performance such as the server-to-user connection and the server's cache\footnote{Unlike the web, cache hit rate is a less critical determinant of  video performance. Videos are cached in chunks by the server. The next chunk is often prefetched from origin if it is not  in cache, even while the current chunk is being played by the user,  so as to hide the origin-to-server latency.} for future work. 

\subsection{Problem formulation}
The bit rate required to play a stream without freezes is often the encoded  bit rate of the stream. For simplicity, we assume that each user requires a bit rate of 1 unit for playing its video and each server has the capacity to serve  units in aggregate. And we assume each server evenly divides its available bit rate capacity among all users who keeps a streaming connection with it. We make the reasonable assumption that each user can compute the bit rate that it receives from its chosen candidate servers and that this bit rate is used as the performance feedback (cf. Figure~\ref{fig:serverselection}).

Different from the delivering web content, where users make repetitive requests to the same website with Poisson processes, we consider users for video streaming have persistent connection with the server. We use a discrete time model in this case as compared to web content delivery where everything is in continuous time. We assume after each time unit, the users look at the bit rate provided by each of the available servers and then make decisions according to the performance (measured by bit rate).
The goal of each user is to find a server who can provide the required bit rate of 1 unit for viewing the video.

\subsection{Algorithm MaxBitRate}
After each user  has picked a video object  using the power law distribution described in Equation~\ref{eq:powerlaw}, Algorithm  described below is executed independently by each user , in discrete time steps.
\begin{enumerate}
\item Choose a random subset of candidate servers   such that .
\item At each time step , do the following:
\begin{enumerate}
\item Request the video content from all servers .
\item For each server  , compute   bit rate provided by server  to user  in the current time step.
\item If there exists a server  such that , then {\em decide} on server  by setting .
\end{enumerate}
\end{enumerate}
Note that the users are executing  a simple strategy of trying out  randomly chosen servers initially.  Then, using the bit rate received in the current time step as feedback, each user independently narrows it's choice of servers to a single server that provided the required unit bit rate. 
If multiple servers provided the required bit rate,  the user decides to use an arbitrary one. Further, note that a user   downloading from a server  at time   knows  immediately whether or not the server is overloaded, since server  is overloaded iff user  received a bit rate of less than 1 unit from the server, i.e., . This is a point of simplification in relation to the more complex situation for hit rate maximization where any single cache hit is not indicative of a non-overloaded server and a historical average of hit rates over a large enough time window  is required as a probabilistic indicator of server overload. And furthermore, this simplification yields both faster convergence to an optimal state in  steps and a much simpler proof of that convergence.

\subsection{Analysis of Algorithm MaxBitRate}
As before, we rigorously analyze the case where . Let the {\em minmax} bit rate  be the best bit rate obtained by the worst user at time , i.e., 

\begin{theorem}
\label{thm:bitrate}
When , the minmax bit rate converges to  unit, for all ,  within time , with high probability. When  on the other hand, the minmax bit rate  with high probability. In particular, when  and the cache size  is , including the case when  is a fixed constant,   tends to zero as  tends to infinity, with high probability. 
\end{theorem}
Please refer to appendix for the proof.

\section{Empirical Evaluation}
\label{sec:empirical}

\begin{figure*}[t]
  \centering
\begin{minipage}[b]{\textwidth}
  \subfloat[, ]{
    \includegraphics[width=0.3\textwidth]{zipftau_utos1.pdf}
  }
  \subfloat[, ]{
    \includegraphics[width=0.3\textwidth]{zipftau_utos10.pdf}
  }
\subfloat[, ]{
   \includegraphics[width=0.3\textwidth]{zipftau_utos20.pdf}
 }
  \caption{The figures show the percentage of undecided users for a typical power law distribution () with spread  and . Note that the undecided users decrease with time in all cases, but the convergence is faster when we use fewer but larger servers by setting  to be larger.   Also, the smaller values of the look-ahead window  result in faster convergence.}
\label{fig:assign}
\end{minipage}
\vspace{0.2in}
\begin{minipage}[b]{\textwidth}
  \centering
  \subfloat[, ]{
    \includegraphics[width=0.36\textwidth]{zipftau1_perf.pdf}
  }
  \hspace{1.1in}
  \subfloat[, ]{
    \includegraphics[width=0.33\textwidth]{zipftau20_perf.pdf}
 }
\caption{Generally, as  increases, convergence time increases but  failure rate decreases. It is also true for larger servers ( = 20), only the failure has gone to zero for all investigated sliding window size.}
\label{fig:tau}
\end{minipage}

\end{figure*}


\begin{figure*}[t]
  \centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=0.68\textwidth]{utos_perf.pdf}
\caption{As  increases  fewer servers with larger capacity are used and convergence time decreases. The decrease is less pronounced beyond   under this setting (, , ).}
\label{fig:utos}
\end{minipage}
\qquad
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=0.68\textwidth]{zipfsigma_perf.pdf}
\caption{There is a very small incremental benefit in using  instead of  , though higher values of  only increased the convergence time. ()}
\label{fig:sigma}
\end{minipage}
\end{figure*}


\begin{figure*}[t]
  \centering
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{zipfmix.pdf}
\caption{Order statistics of the hit rate of the user population. ()}
\label{fig:mix}
\end{minipage}
\qquad
\begin{minipage}[b]{0.45\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{newdist.pdf}
\caption{Minmax hitrate versus time for different power law distributions.}
\label{fig:dist}
\end{minipage}
\end{figure*}

We empirically study our algorithm  by building a simulator. Each user is implemented as a Poisson arrival sequence with unit rate.  We use  users. To simulate varying numbers of servers, users, and applications, we also varied   and   such that . We also simulate a range of values for the spread , and sliding window size . Each server implements an LRU application replacement policy of size . The applications are requested by users using the power law distribution of Equation~\ref{eq:powerlaw} with  to model realistic content popularity \cite{Breslau:1999} \cite{Gill:2007}. However, we also vary  from  (uniform distribution) to  in some of our simulations. 


The system is said to have {\em converged\/} when all users have decided on a single server from their set of candidate servers. There are two complementary metrics that relate to convergence.  {\em Failure rate} is the probability that the system converged to a non-optimal state where there exists servers that are overbooked, resulting in some users  incurring application misses after convergence occurred. The failure rate is measured by performing the simulation multiple times and assessing the goodness of the converged state. {\em Convergence time} is the time it takes for the system to converge provided that it converged to an optimal state.

\subsection{Speed of convergence}



Figure \ref{fig:assign} shows how the fraction of undecided users decreases over time till it reaches zero, resulting in convergence. Note that users do not decide in the first  steps, since they must wait at least that long to accumulate a window of  application hits. However, once the first  steps complete,  the decrease in the number of undecided users is fast as users discover that at least one of their two randomly chosen candidate servers have less load. The rate of decrease in undecided users slows down again towards the end, as pockets of users who experience cache contention in {\em both} of their server choices require multiple iterations to resolve.

In this simulation, we keep the number of users  but vary the number of servers  to achieve different values for . Note that for a fair comparison, we keep the system-wide load  the same.  Load  is a measure of cache contention in the network and is naturally defined as the ratio of the numbers of users in the system and total serving capacity that is available in the system. That is, .
For all three setting of Figure~\ref{fig:assign}, we keep load . The figure shows that with fewer (but larger) servers ( is larger) the convergence time is faster, because having server capacity in a few larger servers provides a larger application hit rate than having the same capacity in several smaller servers. Similar performance gains are also found in the context of web caching and parallel jobs scheduling \cite{Sparrow}. The convergence times are plotted explicitly in 
Figure~\ref{fig:utos} for a greater range of  user-to-server ratios. As  increases from  to , convergence time decreases. The decrease in convergence times are not significant beyond .

\subsection{Impact of sliding window }



The sliding window  is the number of recent requests used by algorithm  to estimate the hit rate. As shown in Figure ~\ref{fig:tau}, there is a natural tradeoff between convergence time and failure rate. When  increases, the users take longer to converge, as they require a  hit rate in a larger sliding window. However, waiting for a longer period also makes their decisions more robust. That is, a user is  less likely to choose an overbooked server, since an overbooked server is less likely to provide a string of  application hits for large . In our simulations with many smaller caches (), when , users made quick choices based on a smaller sliding window. But, this resulted in the system converging to a non-optimal state 100\% of the time. As  further increases, the failure rate decreased. The value of  is a suitable sweet spot as it  results in the  smallest  convergence time for a zero failure rate. However, for fewer but larger servers (), all selections of window size  (thus the small values like ) yielded a  failure rate, while the convergence time still increases as the window size gets larger.






\subsection{Impact of spread }
As shown in Theorems~\ref{thm:hitrate} and~\ref{thm:onechoice}, a spread of   is required for the system to converge to an optimal solution, while a spread of  is insufficient. As predicted by our analysis, our simulations did not converge to an optimal state with . Figure \ref{fig:sigma} shows the  convergence time as a function of spread, for  . 



As   increases, there are two opposing factors that impact the convergence time. The first factor is that  as  increases, each user has more choices and the user is more likely to find a suitable server with less load. On the other hand, an increase in  also increases the total number of initial requests in the system that equals . Thus, the system starts out in a state where servers have greater average load when  is larger. These opposing forces result in a very small incremental benefit when using  instead of , though the higher values of  showed no benefit as convergence time increases with  increases.

We established the ``power of two random choices'' phenomenon where two or more random server choices yield superior results to having just one. It is intriguing to ask {\em what percentage of users need two choices} to reap the benefits of multiple choices? Consider a mix of users, some with two random choices and others with just one. Let  , , denote the average value of the spread among the users.

\comment{
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{zipfmix.pdf}
\caption{Order statistics of the hit rate of the user population. ()}
\label{fig:mix}
\end{figure}
}

In Figure \ref{fig:mix}, we show different order statistics of the  hit rate as a function of . Specifically, we plot the minimum value, -percentile, - percentile and the median (-percentile) of the hit rates of the users after running the system for a long enough period of 200 time units. As our theory predicts, when , the minimum and all the order statistics converge to , as all users converge to a  hit rate. Further, if we are interested in only the median user, any value of the spread is sufficient to guarantee that  of the users obtain a  hit rate. Perhaps the most interesting phenomena is that if , i.e.,  of the users have two choices and the rest have one choice, the -percentile converges to , i.e., all but  of the users experience a  hit rate. For a higher value of , the -percentile converges to , i.e., all but the  of the users experience a  hit rate. This result shows that our algorithm still provides benefits even if only {\em some} users  have multiple random choices of servers available to them.

\comment{
\subsection{Impact of load}
\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{zipf_heatmap_spread4.pdf}
\caption{Heatmap shows that convergence is faster if the cache size () and/or number of servers () is larger for same number of users (). ()}
\label{fig:heat}
\end{figure}
Now we look at the performance of our algorithm under varying load conditions. Recall load .
Figure \ref{fig:heat} shows convergence time as a heatmap with different
values of cache size  and number of servers . The convergence time varies from lower (blue) to higher (red) values. The figure also shows contours where the load is fixed. The white block in the bottom left corner represents 
high load closer to  where our algorithm did not converge within a reasonable amount of time. It is easy to see that convergence time is faster when either the number of servers or the cache size increases. Figure~\ref{fig:load} shows this trend more explicitly as the convergence timße increases with increasing load.
The reason for this behavior is that as load increases there is more contention for the cache resources requiring more time for the algorithm to find a suitable server for each user.
}

\subsection{Impact of demand distribution}



We now study  how hit rate changes with the exponent  in the power law distribution of Equation~\ref{eq:powerlaw}.  Note that the distribution is uniform when  and is the harmonic distribution when .  As  increases, since the tails fall as a power of ,  the distribution gets more and more skewed towards applications with a smaller rank. In  Figure \ref{fig:dist}, we plot the minmax hitrate over  time for different , where we see that a larger  leads to faster convergence. The reason is that as the popularity distribution gets more skewed, a larger fraction of users can share the same VMs for popular applications, leading to better hit rate and faster convergence. 
Thus, the uniform application popularity distribution ()  is the worst case and the algorithm converges faster for the distributions that tend to occur more commonly in practice. Providing theoretical support for this empirical result by analyzing the convergence time to show faster convergence for larger  is a topic for future work.

To summarize our results from empirical evaluation: We establish an inverse relationship between the length of the history used for performance evaluation (denoted by  ) and the failure rate defined as the probability that the system converges to a non-optimal state. We show that as  increases the convergence time increases, but the failure rate decreases. We  also empirically evaluate the impact of the number of choices of candidate servers. We show that two or more random choices are required for all users to receive a  application hit rate. Though even if only  70\% of the users  make two choices,  it is sufficient for   of the users to receive a  application hit rate. 



\section{Related work}
Server selection algorithms have a rich history of both research and actual implementations over the past two decades. Several server selection algorithms have been proposed and empirically evaluated, including client-side algorithms that use historical performance feedback using probes \cite{dykes2000empirical,crovella1995dynamic}.  Server selection has also been studied in a variety of contexts, such as  the web \cite{crovella1995dynamic,sayal1998selection}, video streaming\cite{torres2011dissecting}, and cloud services\cite{wendell2010donar}. Our work is distinguished from the prior literature in that we theoretically model the ``Go-With-The-Winner'' paradigm that is common to many proposed and implemented client-side server selection algorithms. Our work is the first formal study of the efficacy and convergence of such algorithms.

In terms of analytical techniques, our work is closely related to  prior work on balls-into-bins games where the witness tree technique was first utilized \cite{mitzenmacherRS2001}. Witness trees were subsequently used to analyze load balancing algorithms, and circuit-switching algorithms \cite{cole1998randomized}. However, our setting involves additional complexity requiring novel analysis due to the fact that users can share a single cached copy of an object and the hit rate feedback is only a probabilistic indicator of server overload. Also,  our work shows that the ``power of two random choices'' phenomenon  applies in the context of content delivery, a phenomenon known to hold in other contexts such as  balls-into-bins, load balancing, relay allocation for services like Skype \cite{Nguyen:2008}, and circuit switching in interconnection networks \cite{mitzenmacherRS2001}. 
\comment{
There are also parallels between our work on bit rate maximization for video delivery and the recent work on throughput maximization in multi-path communication \cite{Peter:2007}. Similar in spirit to our work, the work on multi-path communication concludes that coordinated rate control over multiple randomly selected paths provides optimal throughput and is significant better than uncoordinated control where rates are determined independently over each single path. However, our work also evolves techniques for determining the convergence rate that is potentially applicable in these domains.
 }
 
 \section{Conclusion}
 Our work constitutes the first formal study of the simple ``Go-With-The-WInner'' paradigm in the context of web and video content delivery.  For web (resp., video) delivery, we proposed a simple algorithm where each user randomly chooses two or more candidate servers and selects the server that provided the best hit rate (resp., bit rate). We proved that the algorithm converges quickly to an optimal state where all users receive the best hit rate (resp., bit rate) and no server is overloaded, with high probability.  While we make some assumptions to simplify the theoretical analysis, our simulations evaluate a broader setting that incorporates a range of values for  and , varying  content popularity distributions, differing load conditions, and situations where only some users have multiple server choices. Taken together, our work establishes that the simple ``Go-With-The-Winner'' paradigm can provide algorithms that converge quickly to an optimal solution, given a sufficient number of random choices and a sufficiently (but not perfectly) accurate performance feedback.
 
\bibliographystyle{abbrv}
\bibliography{ServerSelectionRefs,new_ref,GoWithTheWinner}
\balance
\newpage
\appendix

\subsection{Detailed Proof of Theorem \ref{thm:hitrate}}
\begin{proof}
For simplicity, we prove the situation where , i.e., each user initially chooses two random candidate servers in step 1 of the algorithm. The case where  is analogous. 
\comment{Further we will assume that a user  requesting application from an overbooked server   at time ,  , has ,  as the probability that this assumption is violated at most  as per Lemma~\ref{lem:overbookhit}.}  Wlog, we also assume   is at most , which includes the interesting case of   equal to a constant.  When the server capacity is larger, i.e., if  ,  there will be no overbooked servers with high probability and the theorem holds trivially. This observation follows from a well-known result that if  balls (i.e., users)  uniformly and randomly select   out of  bins (i.e. servers), then the maximum number of users that select a server is  with high probability, when   is a fixed constant \cite{raab1998balls}. 

In contradiction to the theorem, suppose some user  has not decided on a server  by time . We construct a ``witness tree\footnote{A witness tree is so called as it bears witness to the occurrence of an event such as  a user being undecided.}''  of degree  and depth at least , where  . Each node of the witness tree is a server. Each edge of the witness tree is a user whose two nodes correspond to the two servers chosen by that user.  We show that the existence of an undecided user in time step  is unlikely by enumerating all possible witness trees and showing that the occurrence of any such  witness tree is unlikely.  The proof proceeds in the following three steps.

\noindent{\bf (1) Constructing a witness tree.}  If algorithm  has not converged to the optimal state at time , then there exists a user (say )  and a server  such that , since user  has not yet found a server with a  hit rate. We make server  the root of the witness tree.


We find children for the root  to extend the witness tree as follows. Since , by Lemma \ref{lem:timefortau} we know server  is overbooked at time , i.e., there are at least   users requesting server  for  distinct applications at time . Let  be the users who sent requests to server  at time . Wlog, assume that the users  are ordered in ascending order of their IDs.  By Lemma~\ref{lem:overbookhit}, we know that the probability of a user deciding on an overbooked server is small, i.e., at most . Thus, with high probability, users   are undecided at time  since server  is \textsl{overbooked}. Let  be the other server choice associated with user  (one of the choices is server ).  We extend the witness tree by creating  children for the root , one corresponding to each server .  Note that for each of the servers  we know that , since otherwise user  would have decided on server  in time step . Thus, analogous to how we found children for , we can recursively find  children for each of the servers  and grow the witness tree to an additional level.   

Observe that to add an additional level of the witness tree we went from server  at time  to servers  at time , i.e., we went back in time by an amount of . If we continue the same process, we can construct a witness tree that is a -ary tree of depth . 


\noindent{\bf (2) Pruning the witness tree.}
If the nodes of the witness tree are guaranteed to represent distinct servers, proving our probabilistic bound is relatively easy. The reason is that if the servers are unique then the users that represent edges of the tree are unique as well. Therefore the probabilistic choices that each user makes is independent, making it easy to evaluate the probability of occurrence of the tree. However, it may not be the case that the servers in the witness tree constructed above are unique, leading to dependent choices that are hard to resolve. Thus, we create a {\em pruned witness tree} by removing repeated servers from the  original (unpruned) witness tree. 

We prune the witness tree by visiting the nodes of the witness tree iteratively in breadth-first  search order starting at the root. As we perform breadth-first search (BFS), we remove (i.e., prune) some nodes of the tree and the subtrees rooted  at these nodes. What is left after this process is the pruned witness tree. We start by visiting the root. In each iteration, we visit the next  node  in BFS order that has not been pruned. Let  denote the nodes visited {\it before\/} . If  represents a server that is different from the servers represented by nodes in , we do nothing.
 Otherwise, prune all nodes in the subtree rooted at . Then, mark the edge from  to its parent  as a {\em pruning edge}. (Note that the pruning edges are not part of the pruned witness tree.) The procedure continues until either no more nodes remain to be visited or there are  pruning edges. In the latter case, we apply a final pruning by removing all nodes that are yet to be visited, though this step does not produce any more pruning edges. This process results in a pruned witness and a set of  (say) pruning edges.  

 Note that each pruning edge corresponds to a user who we will call a {\em pruned user}. We now make a pass through the pruning edges to select a set  of unique pruned users. Initially,  is set to . We visit the pruning edges in BFS order and for each pruning edge  we add the user corresponding to  to , if this user is distinct from all users currently in  and if , where  is the total number of pruning edges. We stop adding pruned users to set  when we have exactly   users. Note that since a user who made server choices of  and  can appear at most twice as a pruned edge, once with  in the pruned witness tree and once with  in the pruned witness tree. Thus, we are guaranteed to find  distinct pruned users. 
 
After the pruning process, we are left with a pruned witness tree with nodes representing distinct servers and edges representing distinct users. In addition, we have a set  of  distinct pruned users, where  is the number of pruning edges. 

\noindent{\bf (3)  Bounding the probability of pruned witness trees.} 
We enumerate possible witness trees and bound their probability using the union bound. Observe that since the (unpruned) witness tree is a -ary tree of depth , the number of nodes in the witness tree is 

since  and hence .

{\em Ways of choosing the shape of the pruned witness tree}:
The shape of the pruned witness tree is determined by choosing the  pruning edges of the tree. The number of ways of selecting the  pruning edges is at most  since there are at most  edges in the (unpruned) witness tree. \\
{\em Ways of choosing users and
servers for the nodes and edges of the pruned witness tree}: The enumeration
proceeds by considering the nodes in BFS order.  The number of ways of
choosing the server associated with the root is . Consider the  internal node  of the pruned witness tree
whose server has already been chosen to be . Let  have 
children. There are at most  ways of
choosing distinct servers for each of the  children of
.  Also, since there are at most  users in the system
at any point in time, the number of ways to choose 
distinct users for the
 edges incident on  is also at most
. There are 
ways of pairing the users and the servers.
Further, the probability that a chosen user
chooses server  corresponding to node  and a specific one of
 servers chosen above for 's children is  since each set of two servers is equally likely to be chosen in step 1 of the algorithm. Further, note that each of the  users chose  distinct applications and let the probability of occurrence of this event be . This uniqueness probability has been studied in the context of collision-resistant hashing and it is known \cite{bellare2004hash} that
 is largest when the content popularity distribution is the uniform distribution () and progressively becomes smaller as  increases. In particular, 

Putting it together, the number of ways of choosing a distinct
server for each of the  children of , choosing a distinct
user for each of the  edges incident on , choosing a distinct application for each user, and  multiplying by the appropriate probability is at most 
 
provided .
Let  be the number of internal nodes  in the pruned witness tree
such that . Using the bound in Equation~\ref{eq:expbd}
for only these  nodes, the number of ways of choosing the users
and servers for the nodes and edges respectively of the pruned witness tree
weighted by the probability that these choices occurred is at most 
{\em Ways of choosing the pruned users in }: Recall that there are  distinct pruned users  in . The number of ways of choosing the users in  is at
most , since at any time step there are at most  users in the system to choose from. Note that a pruned user has both of its
server choices in the pruned witness tree. Therefore, the probability that a given user is a pruned user is at most  Thus the number of choices for the
 pruned users in  weighted by the probability
that these pruned users occurred is at most

{\em Bringing it all together}: The probability that
there exists a pruned
witness tree with  pruning edges, and  internal nodes with  children each, is at most

since .
There are two possible cases depending on how the pruning process terminates. If the number of pruning edges, , equals  then the third term of Equation~\ref{eq:bd2} is 

using Equation~\ref{eq:mbound} and assuming that cache size  is at least  a suitably large constant.
Alternately, if the pruning process terminates with fewer than  pruning edges, it must be that at least one of the  subtrees rooted at the children of the root  of the 
(unpruned) witness tree has no pruning edge. Thus, the number of internal nodes  of the pruned witness tree with  children  each is bounded as follows:
 
as . Thus, the second term of Equation~\ref{eq:bd2} is 
 
assuming   but is at most .
Thus, in either case, the bound in
Equation~\ref{eq:bd2} is . Further, since there
are at most
 values for , the total probability of a pruned
witness tree is at most
 which is
. This completes the proof of the theorem. 
\end{proof}




\subsection{Proof of Theorem~\ref{thm:onechoice}}

\begin{proof}
From the classical analysis of throwing  balls into  bins \cite{mitzenmacherRS2001}, we know that there exist a subset  such that   and all users in  have chosen a single server , with high probability. Now we show that some user in  must have a small hit rate with high probability. Let  represent the set of all objects accessed by all users in . The probability that  can be upper bounded as follows, where  is an arbitrarily slowly growing function of . The number of ways of picking  objects from a set  of  objects is at most . The probability that a user  in  will pick an object in  can be upper bounded by the probability that a user chooses one of the  most popular objects. Thus the probability that a user in  picks an object in   is at most , where  is the  generalized harmonic number and .Thus, the probability that all users in  pick objects in   is at most .  Therefore, the probability that  is at most

Thus, probability that  is small and hence , with high probability. Since the minmax hit rate  is at most  which is at most ,  tends to zero with high probability.
\end{proof}





\subsection{Proof of Theorem \ref{lem:nu>ns}}
\begin{proof}
We prove the lemma using Chernoff Bound. \\
Recall that there are  servers and  users. Let  be the binary indicator of user  selects server . Because users selects servers uniformly at random, . Thus, .
\begin{enumerate}
\item [(1)] When , we have . For any , we have for the maximum load over servers ,

which equals to  if . Thus, with high probability, . And if the server capacity , all users will have hit rate .
\item [(2)] When , following the same argument as in (1), for any ,

which equals to . Thus the maximum load on all servers is . And as long as the capacity of servers , all users will get hit rate .
\end{enumerate}
\end{proof}





\subsection{Proof for Theorem \ref{thm:bitrate}}


The proof is similar to that of Theorem~\ref{thm:hitrate} in that we create a witness tree, prune  it, and then show that a pruned witness tree is unlikely. However, Algorithm MaxBitRate differs with Algorithm GoWithTheWinner differs in that it's a synchronous algorithm that all users make requests in synchronization and the algorithm executes in discrete time steps rather than continuous time scale. Thus before the proof, we need the following lemmas to assist the formal proof.

\begin{lemma}
\label{lem:cachemiss}
For any time , if user  receives a application miss from server  at some time ,  then server  is overbooked at time .
\end{lemma}
\begin{proof}
If user  requested a service  from server  at time , it must have also requested  from server  at time .  As soon as the request  for  was processed at time , it was placed server . There must have been  other requests for distinct services that caused the service replacement policy to evict ,  resulting in the application miss at time . Thus, at least  distinct services were requested from server  at time , i.e., server  is overbooked at time .
\end{proof}

To prove convergence, we choose the {\em sliding window size} , for a suitably  large positive constant . Further, consider an initial time interval from time zero to time  that consists of   intervals of length  each, where .  Thus,  . 
\begin{lemma}\label{lem:overbookhit2}
The probability that some user  decides on an overbooked server  at some time , ,  is at most  . 
\end{lemma}
\begin{proof}
Suppose user  decides on an overbooked server  at time . Then, it must be the case that 
. Thus, server  provided a application hit to user  in every time , . Recall that each server serves simultaneous requests by first batching the requests according to the requested applications, i.e., each batch contains requests for the same application, and then serving each batch in random order. Since server  is overbooked at time , it must have been overbooked during all the previous time steps. An overbooked server  has at least  distinct applications being requested, i.e., it has at least  batches of requests. The request made by user  will receive a application miss if the batch in which it belongs to is  or higher in the random ordering. Thus, the probability that user  receives a application miss from the overbooked server  in any time step  is at least . Since  is  only if there is no application miss at any time , , the probability of such an occurrence is at most

since .  Using the union bound and choosing a suitably large constant , the probability that there exists a user  who decides on an overbooked server  at some , ,  is at most

since there are  users, at most  overbooked servers, and  time steps. 
\end{proof}

Now with the two lemmas above, we can prove Theorem~\ref{thm:bitrate} as the following.

\begin{proof}
For simplicity, we prove the situation where , i.e., each user initially chooses two random candidate servers in step 1 of the algorithm. The case where  is analogous. 
\comment{Further we will assume that a user  requesting application from an overbooked server   at time ,  , has ,  as the probability that this assumption is violated at most  as per Lemma~\ref{lem:overbookhit}.}  Wlog, we also assume   is at most , which includes the practically interesting case of   equal to a constant.  When the server capacity is larger, i.e., if  ,  there will be no overbooked servers with high probability and the theorem holds trivially. This observation follows from a well-known result that if  balls (i.e., users)  uniformly and randomly select   out of  bins (i.e. servers), then the maximum number of users that select a server is  with high probability, when   is a fixed constant \cite{raab1998balls}. 

In contradiction to the theorem, suppose some user  (say) has not decided on a server  by time . We construct a ``witness tree\footnote{A witness tree is so called as it bears witness to the occurrence of an event such as  a user being undecided.}''  of degree  and depth at least , where  . Each node of the witness tree is a server. Each edge of the witness tree is a user whose two nodes correspond to the two servers chosen by that user.  We show that the existence of an undecided user in time step  is unlikely by enumerating all possible witness trees and showing that the occurrence of any such  witness tree is unlikely.  The proof proceeds in the following three steps.

\noindent{\bf (1) Constructing a witness tree.}  If algorithm  has not converged to the optimal state at time , then there  exists an user (say )  and a server  such that , since user  has not yet found a server with a  hit rate. We make server  the root of the witness tree.


We find children for the root  to extend the witness tree as follows. Since , there exists a time , , such that user   received a application miss from server .  By Lemma~\ref{lem:cachemiss},  server  was overbooked at time , i.e., there are at least   users requesting server  for  distinct applications at time . Let  be the users who sent requests to server  at time . Wlog, assume that the users  are ordered in 
ascending order of their IDs.  By Lemma~\ref{lem:overbookhit}, we know that the probability of a user deciding on an overbooked server is small, i.e., at most . Thus, with high probability, users   are undecided at time  since they made a request to an overbooked server . Let  be the other server choice associated with user  (one of the choices is server ).  We extend the witness tree by creating  children for the root , one corresponding to each server .  Note that for each of the servers  we know that , since otherwise user  would have decided on server  in time step . Thus, analogous to how we found children for , we can recursively find  children for each of the servers  and grow the witness tree to an additional level.   

Observe that to add an additional level of the witness tree we went from server  at time  to servers  at time , i.e., we went back in time by an amount of . If we continue the same process, we can construct a witness tree that is a -ary tree of depth . 


\noindent{\bf (2) Pruning the witness tree.}
If the nodes of the witness tree are guaranteed to represent distinct servers, proving our probabilistic bound is relatively easy. The reason is that if the servers are unique then the users that represent edges of the tree are unique as well. Therefore the probabilistic choices that each user makes is independent, making it easy to evaluate the probability of occurrence of the tree. However, it may not be the case that the servers in the witness tree constructed above are unique, leading to dependent choices that are hard to resolve. Thus, we create a {\em pruned witness tree} by removing repeated servers from the  original (unpruned) witness tree. 

We prune the witness tree by visiting the nodes of the witness tree iteratively in breadth-first  search order starting at the root. As we perform breadth-first search (BFS), we remove (i.e., prune) some nodes of the tree and the subtrees rooted  at these nodes. What is left after this process is the pruned witness tree. We start by visiting the root. In each iteration, we visit the next  node  in BFS order that has not been pruned. Let  denote the nodes visited {\it before\/} . If  represents a server that is different from the servers represented by nodes in , we do nothing.
 Otherwise, prune all nodes in the subtree rooted at . Then, mark the edge from  to its parent  as a {\em pruning edge}. (Note that the pruning edges are not part of the pruned witness tree.) The procedure continues until either no more nodes remain to be visited or there are  pruning edges. In the latter case, we apply a final pruning by removing all nodes that are yet to be visited, though this step does not produce any more pruning edges. This process results in a pruned witness and a set of  (say) pruning edges.  

 Note that each pruning edge corresponds to a user who we will call a {\em pruned user}. We now make a pass through the pruning edges to select a set  of unique pruned users. Initially,  is set to . We visit the pruning edges in BFS order and for each pruning edge  we add the user corresponding to  to , if this user is distinct from all users currently in  and if , where  is the total number of pruning edges. We stop adding pruned users to set  when we have exactly   users. Note that since a user who made server choices of  and  can appear at most twice as a pruned edge, once with  in the pruned witness tree and once with  in the pruned witness tree. Thus, we are guaranteed to find  distinct pruned users. 
 
After the pruning process, we are left with a pruned witness tree with nodes representing distinct servers and edges representing distinct users. In addition, we have a set  of  distinct pruned users, where  is the number of pruning edges. 

\noindent{\bf (3)  Bounding the probability of pruned witness trees.} 
We enumerate possible witness trees and bound their probability using the union bound. Observe that since the (unpruned) witness tree is a -ary tree of depth , the number of nodes in the witness tree is 

since  and hence .

{\em Ways of choosing the shape of the pruned witness tree.}
The shape of the pruned witness tree is determined by choosing the  pruning edges of the tree. The number of ways of selecting the  pruning edges is at most  since there are at most  edges in the (unpruned) witness tree. 

{\em Ways of choosing users and
servers for the nodes and edges of the pruned witness tree}. The enumeration
proceeds by considering the nodes in BFS order.  The number of ways of
choosing the server associated with the root is . Consider the  internal node  of the pruned witness tree
whose server has already been chosen to be . Let  have 
children. There are at most  ways of
choosing distinct servers for each of the  children of
.  Also, since there are at most  users in the system
at any point in time, the number of ways to choose 
distinct users for the
 edges incident on  is also at most
. There are 
ways of pairing the users and the servers.
Further, the probability that a chosen user
chooses server  corresponding to node  and a specific one of
 servers chosen above for 's children is  since each set of two servers is equally likely to be chosen in step 1 of the algorithm. Further, note that each of the  users chose  distinct applications and let the probability of occurrence of this event be . This uniqueness probability has been studied in the context of collision-resistant hashing and it is known \cite{bellare2004hash} that
 is largest when the content popularity distribution is the uniform distribution () and progressively becomes smaller as  increases. In particular, 

Putting it together, the number of ways of choosing a distinct
server for each of the  children of , choosing a distinct
user for each of the  edges incident on , choosing a distinct application for each user, and  multiplying by the appropriate probability is at most 
 
provided .
Let  be the number of internal nodes  in the pruned witness tree
such that . Using the bound in Equation~\ref{eq:expbd}
for only these  nodes, the number of ways of choosing the users
and servers for the nodes and edges respectively of the pruned witness tree
weighted by the probability that these choices occurred is at most 
{\em Ways of choosing the pruned users in .} Recall that there are  distinct pruned users  in . The number of ways of choosing the users in  is at
most , since at any time step there are at most  users in the system to choose from. Note that a pruned user has both of its
server choices in the pruned witness tree. Therefore, the probability that a given user is a pruned user is at most  Thus the number of choices for the
 pruned users in  weighted by the probability
that these pruned users occurred is at most

{\em Bringing it all together.} The probability that
there exists a pruned
witness tree with  pruning edges, and  internal nodes with  children each, is at most

since .
There are two possible cases depending on how the pruning process terminates. If the number of pruning edges, , equals  then the third term of Equation~\ref{eq:bd2} is 

using Equation~\ref{eq:mbound} and assuming that cache size  is at least  a suitably large constant.
Alternately, if the pruning process terminates with fewer than  pruning edges, it must be that at least one of the  subtrees rooted at the children of the root  of the 
(unpruned) witness tree has no pruning edge. Thus, the number of internal nodes  of the pruned witness tree with  children  each is bounded as follows:
 
as . Thus, the second term of Equation~\ref{eq:bd2} is 
 
assuming   but is at most .
Thus, in either case, the bound in
Equation~\ref{eq:bd2} is . Further, since there
are at most
 values for , the total probability of a pruned
witness tree is at most
 which is
. This completes the proof of the theorem. 
\end{proof}



\end{document}
