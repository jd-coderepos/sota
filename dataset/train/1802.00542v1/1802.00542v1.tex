\documentclass[a4paper, 10pt, conference]{ieeeconf}    
                                                      
\usepackage{FG2018}

\usepackage{times}
\usepackage{epsfig}
\usepackage{relsize}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{gensymb}
\usepackage{tabularx, booktabs}
\newcolumntype{Y}{>{\arraybackslash}X}
\usepackage{pifont}
\usepackage{caption}

\FGfinalcopy 


\def\mbf#1{\mathbf{#1}}
\def\mit#1{\mathit{#1}}
\def\tbf#1{\textbf{#1}}
\def\tif#1{\textit{#1}}
\def\bsf#1{\boldsymbol{#1}}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\minisection}[1]{\vspace{2mm}\noindent{\bf #1}.}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{53}}\newcommand{\thickhat}[1]{\mathbf{\hat{\text{}}}}
\newcommand{\thickbar}[1]{\mathbf{\bar{\text{}}}}
\newcommand{\thicktilde}[1]{\mathbf{\tilde{\text{}}}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\IEEEoverridecommandlockouts                              
                                                         
\overrideIEEEmargins

\def\FGPaperID{172} 

\title{\LARGE \bf
ExpNet: Landmark-Free, Deep, 3D Facial Expressions
}


\author{Feng-Ju Chang\textsuperscript{1}, Anh Tuan Tran\textsuperscript{1}, Tal Hassner\textsuperscript{2,3}, Iacopo Masi\textsuperscript{1}, Ram Nevatia\textsuperscript{1}, Gerard Medioni\textsuperscript{1}\\
\textsuperscript{1}~Institute for Robotics and Intelligent Systems, USC, CA, USA\\
\textsuperscript{2}~Information Sciences Institute, USC, CA, USA\\
\textsuperscript{3}~The Open University of Israel, Israel\\
{\tt\small \{fengjuch,anhttran,iacopoma,nevatia,medioni\}@usc.edu, hassner@openu.ac.il}
}


\begin{document}

\IEEEoverridecommandlockouts\pubid{\makebox[\columnwidth]{978-1-5386-2335-0/18/\\mbf{I}\widehat{\mbf{s}}\boldsymbol{\alpha} \in \mathbb{R}^s\mbf{S}\in \mathbb{R}^{3n\times s}\boldsymbol{\eta}\in \mathbb{R}^{m}\mbf{E}\in \mathbb{R}^{3n\times m}3nn\mbf{I}sms=99m=29\boldsymbol{\alpha}\boldsymbol{\eta}\boldsymbol{\alpha}\boldsymbol{\eta}\boldsymbol{\alpha}\boldsymbol{\Pi}\boldsymbol{\Pi}\mbf{S}^{\prime}\boldsymbol{\alpha}\mbf{S}^{\prime}{\delta}_{\mbf{E}_j}\mbf{p}\boldsymbol{\eta}^{\star}f(\{\mbf{W},\mbf{b}\}, \mbf{I}) \mapsto \boldsymbol{\eta}\{\mbf{W},\mbf{b}\}\ell_2\boldsymbol{\eta}_t\mbf{I}_tf(\{\mbf{W},\mbf{b}\}, \mbf{I}_t)\mbf{I}_t\times 1.25\timesK=5\times\times$.4ex] \hline
Total			& 0.599	& 16.42	  & 0.90	 & 0.97	& 0.78	&  0.6&	0.088							\\ 
\bottomrule
\end{tabular}
}
\caption{
{\em Expression estimation runtime.} Comparing a number of alternative methods to our ExpNet. Landmark based methods require several steps for landmark detection and then expression optimization; whereas deep methods solve for expression in a single step.
}
\label{tab:timings}
\end{table}


\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{figure/qual_ckp_no_3dmm_horiz.png}
\caption{
{\em Qualitative expression estimation on CK+}. 3D head shapes estimated by a deep 3DMM fitting method~\cite{tran16_3dmm_cnn}. Expressions added using a number of baseline methods including our ExpNet. Our method is better able to model subtle expressions than 3DDFA. The top-performing landmark detector, CE-CLM~\cite{zadeh2016deep}, does not perform as well on these images.}
\label{fig:qual}
\vspace{-3mm}
\end{figure*}

\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{figure/ExprNet_Figures_wide_v2.jpg}
\caption{
{\em Qualitative expression estimation on EmotiW-17}. 3D head shapes estimated by a deep 3DMM fitting method~\cite{tran16_3dmm_cnn}. We add expressions using a number of baseline methods comparing them with our ExpNet. Our method and 3DDFA~\cite{zhu2015} show consistent expression fitting across scales. Our method additionally models subtle expressions better than 3DDFA. The top-performing facial landmark detector, CLNF~\cite{baltrusaitis2013constrained}, does not perform as well on these images.}
\label{fig:qual_emotiw17}
\vspace{-3mm}
\end{figure*}



\minisection{Baseline methods} We compare our approach to widely used, state-of-the-art face landmark detectors. These are DLIB~\cite{king2009dlib}, CLNF~\cite{baltrusaitis2013constrained}, OpenFace~\cite{baltruvsaitis2016openface}, CE-CLM~\cite{zadeh2016deep}, RCPR~\cite{burgos2013robust}, and 3DDFA~\cite{zhu2015}. Note that CLNF is the method used to produce our training labels. 
 
\minisection{Results} Fig.~\ref{fig:exp_conf} and~\ref{fig:exp_conf_emotiw17} report the emotion classification confusion matrices on the original CK+ and EmotiW-17 datasets (unscaled) for our method (Fig.~\ref{fig:exp_rec_conf_us} and~\ref{fig:exp_rec_conf_us_emotiw17}), comparing it to the other methods, 3DDFA (Fig.~\ref{fig:exp_rec_conf_3ddfa}) and CE-CLM / CLNF (Fig.~\ref{fig:exp_rec_conf_dclm} /~\ref{fig:exp_rec_conf_clnf_emotiw17}). 

On CK+, our expression coefficients were able to capture well surprise (Su), happy (Ha), and disgust (Di) emotions, all emotions which are well defined by facial expressions. On EmotiW-17, our method performed well on neutral (Ne), happy (Ha), sad (Sa), and angry (An), but less so on disgust (Di), fear (Fe), and surprise (Su). From our observations, these last emotions are visually similar to angry (An), which could explain why they challenged our system. On the whole, however, our representation was noticeably better at capturing all emotion classes than its baselines. 


Fig.~\ref{fig:exp_rec_scale} reports emotion classification performances of all methods on scaled versions of the CK+ (Fig.~\ref{fig:exp_rec_scale}(a)) and EmotiW-17 sets (Fig.~\ref{fig:exp_rec_scale}(b)). These results measure the sensitivity of different methods to the input image resolution: The x-axis reports the downsizing factor, proportional to the original scale. A scale of~1 therefore represents the original image sizes (640x490 for CK+; 720x576 for EmotiW-17), scale of~0.2 implies 128x98 for CK+ and 144x115 for EmotiW-17, and so fourth. 

Results in Fig.~\ref{fig:exp_rec_scale} clearly show our approach to be the most accurate in terms of emotion recognition accuracy. It is additionally far more robust to scale changes compared than the other landmark detection based methods. Note also the  difference in emotion recognition between deep methods---ours and~\cite{zhu2015}---and landmark based approaches.

Importantly, our method outperforms CLNF~\cite{baltrusaitis2013constrained} by a wide margin in all tests. This result is significant, as CLNF was the method used to generate our expression labels in Sec.~\ref{sec:gen_data}. Our improved performance suggests that the network learned to generalize from its training data and thus performed better on a wider range of viewing conditions and challenges.



\minisection{Runtime} Tab.~\ref{tab:timings} reports runtimes for the methods tested. All tests were performed on a machine with an NVIDIA, GeForce GTX TITAN X and an Intel Xeon CPU E5-2640 v3 @ 2.60GHz. The only exception was 3DDFA~\cite{zhu2015}, which required a Windows system and was tested using an Intel Core i7-4820K CPU @ 3.70GHz with 8 CPUs.


We compare landmark based approaches with deep, direct method such as 3DDFA and our ExpNet. ExpNet is at least one order of magnitude faster than any of its alternatives. Note that, landmark based expression fitting methods generally follow a three-step process: (i) facial landmark detection, (ii) head pose estimation, and (iii) expression fitting. Their total processing time is therefore the sum of these steps. Although some landmark detection methods (e.g. DLIB) are extremely efficient (0.009s), they are still required to solve the optimization problem of Eq.~(\ref{eq:expr_fitting}), in order to translate these detections to an expression coefficients estimate. This process is much slower than our proposed method.


As for deep methods for expression estimation, the software package provided by 3DDFA~\cite{zhu2015} does not allow testing on the GPU; in their paper, they report GPU runtime to be 0.076 seconds, which is similar to our runtime, which was measured on a GPU. Other facial landmarks detector based methods, including the code used to solve~Eq. (\ref{eq:expr_fitting}), are all intrinsically implemented on the CPU. Though they may conceivably be expedited significantly by porting them to the GPU, we are unaware of any such implementation.


\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figure/qual_ckp_horiz_no_3dmm_fail.png}
\caption{
{\em Expression estimation failures.} Our method is less able to handle extreme facial expressions. Other methods, by comparison, appear to either exaggerate the expression (3DDFA) or are inconsistent across scales (CE-CLM).
}
\label{fig:qual_fail}
\end{figure}

\subsection{Qualitative Results}\label{sec:qual}
Fig.~\ref{fig:qual} and~\ref{fig:qual_emotiw17} provide qualitative renderings of the 3D expressions estimated on CK+ and EmotiW-17 images. Each result was obtained on the original, input image scale (scale 1) and also at our lowest resolution (scale 0.2). All the results in these figures use the same 3D shape provided by 3DMM-CNN~\cite{tran16_3dmm_cnn}. Additional, mid level facial details can possibly be added using, e.g.,~\cite{tran2017extreme}, but to emphasize expressions, rather than details, we used only course facial shapes. 

These figures visualize expressions estimated with our ExpNet compared with the recent deep method for joint estimation of shape and expression~\cite{zhu2015}, and the top performing landmark detectors CE-CLM~\cite{zadeh2016deep}, and CLNF~\cite{baltrusaitis2013constrained}. For reference, we provide also the shape 3D face  shape~\cite{tran16_3dmm_cnn} estimated before expressions were added.


Our expression estimates appear to be much better at capturing expression nuances: This is clear from the subtle expressions, fear and anger, rendered in Fig.~\ref{fig:qual}. This is consistent with the improvement shown in the confusion matrices in Fig.\ref{fig:exp_conf}. 3DDFA appears inconsistent across the same expression (happy) and tends to either exaggerate the expression or underestimate it. Both CE-CLM and CLNF seem sensitive to input image resolutions: They both estimate different expressions for the same input image offered at different scales.

Finally, Fig.~\ref{fig:qual} demonstrates a weaknesses of our ExpNet to strong intensity expressions such as surprise. 3DDFA, by comparison, produces somewhat over-exaggerated estimates on these images. Although CE-CLM produces visually suitable estimates, its predictions are inconsistent across scales.


\section{Conclusions}\label{sec:conclu}
We present a method for deep, 3D expression modeling and show it to be far more robust than than facial landmark detection methods widely used for this task. Our approach estimates expressions without the use of facial landmarks, suggesting that facial landmark detection methods may be redundant for this task. This conclusion is consistent with recent results demonstrating deep, {\em landmark free} 3D face shape estimation~\cite{chang17fpn} and 6DoF head alignment~\cite{tran16_3dmm_cnn}. The significance of these results is that by avoiding facial landmark detection, we can process face images obtained in extreme viewing condition which can be challenging for landmark detection methods.
\bibliographystyle{ieee}
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{baltrusaitis2013constrained}
T.~Baltrusaitis, P.~Robinson, and L.-P. Morency.
\newblock Constrained local neural fields for robust facial landmark detection
  in the wild.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition Workshops},
  pages 354--361. IEEE, 2013.

\bibitem{baltruvsaitis2016openface}
T.~Baltru{\v{s}}aitis, P.~Robinson, and L.-P. Morency.
\newblock Openface: an open source facial behavior analysis toolkit.
\newblock In {\em Winter Conf. on App. of Comput. Vision}, 2016.

\bibitem{Bas:accvw16}
A.~Bas, W.~A.~P. Smith, T.~Bolkart, and S.~Wuhrer.
\newblock Fitting a {3D} morphable model to edges: {A} comparison between hard
  and soft correspondences.
\newblock {\em arxiv preprint}, abs/1602.01125, 2016.

\bibitem{blanz2002face}
V.~Blanz, S.~Romdhani, and T.~Vetter.
\newblock Face identification across different poses and illuminations with a
  3d morphable model.
\newblock In {\em Int. Conf. on Automatic Face and Gesture Recognition}, pages
  192--197, 2002.

\bibitem{blanz2003face}
V.~Blanz and T.~Vetter.
\newblock Face recognition based on fitting a 3d morphable model.
\newblock {\em Trans. Pattern Anal. Mach. Intell.}, 25(9):1063--1074, 2003.

\bibitem{burgos2013robust}
X.~P. Burgos-Artizzu, P.~Perona, and P.~Doll{\'a}r.
\newblock Robust face landmark estimation under occlusion.
\newblock In {\em Proc. Int. Conf. Comput. Vision}, pages 1513--1520. IEEE,
  2013.

\bibitem{chang17fpn}
F.-J. Chang, A.~Tran, T.~Hassner, I.~Masi, R.~Nevatia, and G.~Medioni.
\newblock {F}ace{P}ose{N}et: Making a case for landmark-free face alignment.
\newblock In {\em Proc. Int. Conf. Comput. Vision Workshops}, 2017.

\bibitem{chu2014}
B.~Chu, S.~Romdhani, and L.~Chen.
\newblock {3D}-aided face recognition robust to expression and pose variations.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, 2014.

\bibitem{dhall2012collecting}
A.~Dhall et~al.
\newblock Collecting large, richly annotated facial-expression databases from
  movies.
\newblock 2012.

\bibitem{dhall2017individual}
A.~Dhall, R.~Goecke, S.~Ghosh, J.~Joshi, J.~Hoey, and T.~Gedeon.
\newblock From individual to group-level emotion recognition: Emotiw 5.0.
\newblock In {\em Proceedings of the 19th ACM International Conference on
  Multimodal Interaction}, pages 524--528. ACM, 2017.

\bibitem{edwards1998face}
G.~J. Edwards, T.~F. Cootes, and C.~J. Taylor.
\newblock Face recognition using active appearance models.
\newblock In {\em European Conf. Comput. Vision}, pages 581--595. Springer,
  1998.

\bibitem{eidinger2013age}
E.~Eidinger, R.~Enbar, and T.~Hassner.
\newblock Age and gender estimation of unfiltered faces.
\newblock {\em Trans. on Inform. Forensics and Security}, 9(12), 2014.

\bibitem{Everingham06a}
M.~Everingham, J.~Sivic, and A.~Zisserman.
\newblock ``{H}ello! {M}y name is... {Buffy}'' -- automatic naming of
  characters in {TV} video.
\newblock In {\em Proc. British Mach. Vision Conf.}, 2006.

\bibitem{fabian2016emotionet}
C.~Fabian Benitez-Quiroz, R.~Srinivasan, and A.~M. Martinez.
\newblock Emotionet: An accurate, real-time algorithm for the automatic
  annotation of a million facial expressions in the wild.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, pages
  5562--5570, 2016.

\bibitem{hartley2003multiple}
R.~Hartley and A.~Zisserman.
\newblock {\em Multiple view geometry in computer vision}.
\newblock Cambridge university press, 2003.

\bibitem{hassner2013viewing}
T.~Hassner.
\newblock Viewing real-world faces in {3D}.
\newblock In {\em Proc. Int. Conf. Comput. Vision}, pages 3607--3614. IEEE,
  2013.
\newblock Available:~\url{www.openu.ac.il/home/hassner/projects/poses}.

\bibitem{hassner2015effective}
T.~Hassner, S.~Harel, E.~Paz, and R.~Enbar.
\newblock Effective face frontalization in unconstrained images.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, 2015.

\bibitem{He_2016_CVPR}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, June 2016.

\bibitem{hu2016face}
G.~Hu, F.~Yan, C.-H. Chan, W.~Deng, W.~Christmas, J.~Kittler, and N.~M.
  Robertson.
\newblock Face recognition using a unified {3D} morphable model.
\newblock In {\em European Conf. Comput. Vision}. Springer, 2016.

\bibitem{LFWTech}
G.~B. Huang, M.~Ramesh, T.~Berg, and E.~Learned-Miller.
\newblock Labeled faces in the wild: A database for studying face recognition
  in unconstrained environments.
\newblock Technical Report 07-49, UMass, Amherst, October 2007.

\bibitem{huber:3dmm}
P.~Huber, G.~Hu, R.~Tena, P.~Mortazavian, W.~Koppen, W.~Christmas, M.~RÃ¤tsch,
  and J.~Kittler.
\newblock A multiresolution {3D} morphable face model and fitting framework.
\newblock In {\em Int. Conf. on Computer Vision Theory and Applications}, 2016.

\bibitem{jeni2015dense}
L.~A. Jeni, J.~F. Cohn, and T.~Kanade.
\newblock Dense {3D} face alignment from {2D} videos in real-time.
\newblock In {\em Int. Conf. on Automatic Face and Gesture Recognition},
  volume~1. IEEE, 2015.

\bibitem{king2009dlib}
D.~E. King.
\newblock Dlib-ml: A machine learning toolkit.
\newblock {\em J. Mach. Learning Research}, 10(Jul):1755--1758, 2009.

\bibitem{Klare_2015_CVPR}
B.~F. Klare, B.~Klein, E.~Taborsky, A.~Blanton, J.~Cheney, K.~Allen,
  P.~Grother, A.~Mah, M.~Burge, and A.~K. Jain.
\newblock Pushing the frontiers of unconstrained face detection and
  recognition: {IARPA} {J}anus {B}enchmark-{A}.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, 2015.

\bibitem{kosti2017emotion}
R.~Kosti, J.~M. Alvarez, A.~Recasens, and A.~Lapedriza.
\newblock Emotion recognition in context.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, 2017.

\bibitem{levi2015emotion}
G.~Levi and T.~Hassner.
\newblock Emotion recognition in the wild via convolutional neural networks and
  mapped binary patterns.
\newblock In {\em Int. Conf. on Multimodal Interaction}, pages 503--510. ACM,
  2015.

\bibitem{lucey2010extended}
P.~Lucey, J.~F. Cohn, T.~Kanade, J.~Saragih, Z.~Ambadar, and I.~Matthews.
\newblock The extended cohn-kanade dataset (ck+): A complete dataset for action
  unit and emotion-specified expression.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition Workshops},
  pages 94--101. IEEE, 2010.

\bibitem{Masi:18:learning}
I.~Masi, F.~J. Chang, J.~Choi, S.~Harel, J.~Kim, K.~Kim, J.~Leksut, S.~Rawls,
  Y.~Wu, T.~Hassner, W.~AbdAlmageed, G.~Medioni, L.~P. Morency, P.~Natarajan,
  and R.~Nevatia.
\newblock Learning pose-aware models for pose-invariant face recognition in the
  wild.
\newblock {\em Trans. Pattern Anal. Mach. Intell.}, PP(99):1--1, 2018.

\bibitem{masi2014pose}
I.~Masi, C.~Ferrari, A.~Del~Bimbo, and G.~Medioni.
\newblock Pose independent face recognition by localizing local binary patterns
  via deformation components.
\newblock In {\em Int. Conf. on Pattern Recognition}, pages 4477--4482, 2014.

\bibitem{masi2017rapid}
I.~Masi, T.~Hassner, A.~T. Tran, and G.~Medioni.
\newblock Rapid synthesis of massive face sets for improved face recognition.
\newblock In {\em Int. Conf. on Automatic Face and Gesture Recognition}, pages
  604--611. IEEE, 2017.

\bibitem{masi16dowe}
I.~Masi, A.~Tran, T.~Hassner, J.~T. Leksut, and G.~Medioni.
\newblock Do {W}e {R}eally {N}eed to {C}ollect {M}illions of {F}aces for
  {E}ffective {F}ace {R}ecognition?
\newblock In {\em European Conf. Comput. Vision}, 2016.
\newblock
  Available~\url{www.openu.ac.il/home/hassner/projects/augmented_faces}.

\bibitem{paysan09basel}
P.~Paysan, R.~Knothe, B.~Amberg, S.~Romhani, and T.~Vetter.
\newblock A {3D} face model for pose and illumination invariant face
  recognition.
\newblock In {\em Int. Conf. on Advanced Video and Signal based Surveillance},
  2009.

\bibitem{richardson2016learning}
E.~Richardson, M.~Sela, R.~Or-El, and R.~Kimmel.
\newblock Learning detailed face reconstruction from a single image.
\newblock {\em arXiv preprint arXiv:1611.05053}, 2016.

\bibitem{romdhani2003efficient}
S.~Romdhani and T.~Vetter.
\newblock Efficient, robust and accurate fitting of a {3D} morphable model.
\newblock In {\em Proc. Int. Conf. Comput. Vision}, 2003.

\bibitem{romdhani2005estimating}
S.~Romdhani and T.~Vetter.
\newblock Estimating {3D} shape and texture using pixel intensity, edges,
  specular highlights, texture constraints and a prior.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, volume~2,
  pages 986--993, 2005.

\bibitem{sagonas2015300}
C.~Sagonas, E.~Antonakos, G.~Tzimiropoulos, S.~Zafeiriou, and M.~Pantic.
\newblock 300 faces in-the-wild challenge: Database and results.
\newblock {\em Image and Vision Computing}, 2015.

\bibitem{tang2008real}
H.~Tang, Y.~Hu, Y.~Fu, M.~Hasegawa-Johnson, and T.~S. Huang.
\newblock Real-time conversion from a single 2d face image to a {3D}
  text-driven emotive audio-visual avatar.
\newblock In {\em Int. Conf. on Multimedia and Expo}, pages 1205--1208. IEEE,
  2008.

\bibitem{tran16_3dmm_cnn}
A.~Tran, T.~Hassner, I.~Masi, and G.~Medioni.
\newblock Regressing robust and discriminative {3D} morphable models with a
  very deep neural network.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, 2017.

\bibitem{tran2017extreme}
A.~T. Tran, T.~Hassner, I.~Masi, E.~Paz, Y.~Nirkin, and G.~Medioni.
\newblock Extreme {3D} face reconstruction: Looking past occlusions.
\newblock {\em arXiv preprint arXiv:1712.05083}, 2017.

\bibitem{wolf:YTF}
L.~Wolf, T.~Hassner, and I.~Maoz.
\newblock Face recognition in unconstrained videos with matched background
  similarity.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, 2011.

\bibitem{wu2017facial}
Y.~Wu, T.~Hassner, K.~Kim, G.~Medioni, and P.~Natarajan.
\newblock Facial landmark detection with tweaked convolutional neural networks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2017.

\bibitem{yang2011expression}
F.~Yang, J.~Wang, E.~Shechtman, L.~Bourdev, and D.~Metaxas.
\newblock Expression flow for {3D}-aware face component transfer.
\newblock {\em ACM Trans. on Graphics}, 30(4):60, 2011.

\bibitem{yang2016multi}
Z.~Yang and R.~Nevatia.
\newblock A multi-scale cascade fully convolutional network face detector.
\newblock In {\em Int. Conf. on Pattern Recognition}, pages 633--638, 2016.

\bibitem{yi2014learning}
D.~Yi, Z.~Lei, S.~Liao, and S.~Z. Li.
\newblock Learning face representation from scratch.
\newblock {\em arXiv preprint arXiv:1411.7923}, 2014.
\newblock
  Available:~\url{http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html}.

\bibitem{yin2008high}
L.~Yin, X.~Chen, Y.~Sun, T.~Worm, and M.~Reale.
\newblock A high-resolution 3d dynamic facial expression database.
\newblock In {\em Automatic Face \& Gesture Recognition, 2008. FG'08. 8th IEEE
  International Conference on}, pages 1--6. IEEE, 2008.

\bibitem{zadeh2016deep}
A.~Zadeh, T.~Baltru{\v{s}}aitis, and L.-P. Morency.
\newblock Convolutional experts constrained local model for facial landmark
  detection.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition Workshops},
  2017.

\bibitem{zafeiriou2016facial}
S.~Zafeiriou, A.~Papaioannou, I.~Kotsia, M.~Nicolaou, and G.~Zhao.
\newblock Facial affect``in-the-wild''.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition Workshops},
  pages 36--47, 2016.

\bibitem{zhang2016gender}
K.~Zhang, L.~Tan, Z.~Li, and Y.~Qiao.
\newblock Gender and smile classification using deep convolutional neural
  networks.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition Workshops},
  pages 34--38, 2016.

\bibitem{zhu2015}
X.~Zhu, Z.~Lei, X.~Liu, H.~Shi, and S.~Li.
\newblock Face alignment across large poses: A {3D} solution.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, Las Vegas,
  NV, June 2016.

\bibitem{zhu2015high}
X.~Zhu, Z.~Lei, J.~Yan, D.~Yi, and S.~Z. Li.
\newblock High-fidelity pose and expression normalization for face recognition
  in the wild.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, pages
  787--796, 2015.

\end{thebibliography}


\end{document}
