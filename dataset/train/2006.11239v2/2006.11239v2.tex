\documentclass{article}

\PassOptionsToPackage{numbers,compress}{natbib}






\usepackage[final]{neurips_2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{mathtools,amssymb}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{pgfplots,pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{array,colortbl}
\usepackage{xcolor}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage[capitalise]{cleveref}
\usepackage{caption}
\usepackage{graphbox}
\usepackage{placeins}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{etoolbox}

\newtoggle{hqfigures}
\togglefalse{hqfigures}  

\newcommand{\vardbtilde}[1]{\tilde{\raisebox{0pt}[0.85\height]{}}}
\newcommand{\defeq}{\coloneqq}
\newcommand{\grad}{\nabla}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Ea}[1]{\E\left[#1\right]}
\newcommand{\Eb}[2]{\E_{#1}\!\left[#2\right]}
\newcommand{\Vara}[1]{\Var\left[#1\right]}
\newcommand{\Varb}[2]{\Var_{#1}\left[#2\right]}
\newcommand{\kl}[2]{D_{\mathrm{KL}}\!\left(#1 ~ \| ~ #2\right)}
\newcommand{\pdata}{{p_\mathrm{data}}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bxh}{\hat{\mathbf{x}}}
\newcommand{\btheta}{{\boldsymbol{\theta}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bepsilon}{{\boldsymbol{\epsilon}}}
\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bnu}{{\boldsymbol{\nu}}}
\newcommand{\bSigma}{{\boldsymbol{\Sigma}}}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\title{Denoising Diffusion Probabilistic Models}

\author{Jonathan Ho \\
  UC Berkeley \\
  \texttt{jonathanho@berkeley.edu} \\
   \And
  Ajay Jain \\
  UC Berkeley \\
  \texttt{ajayj@berkeley.edu} \\
   \And
  Pieter Abbeel \\
  UC Berkeley \\
  \texttt{pabbeel@cs.berkeley.edu}
}

\begin{document}

\begin{figure}[b!]
  \vspace{-1em}
  \centering
  \includegraphics[align=c,width=0.595\textwidth]{images/celebahq256_header_image_4x4.png}\hfill
  \includegraphics[align=c,trim=0cm 5.95cm 12cm 0cm,clip,width=0.395\textwidth]{images/cifar10_eps-fixedlarge-mse_20x20.png}
  \caption{\small{Generated samples on CelebA-HQ  (left) and unconditional CIFAR10 (right)}}
  \label{fig:header_samples}
\end{figure}


\maketitle

\begin{abstract}
We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at \url{https://github.com/hojonathanho/diffusion}.
\end{abstract}

\section{Introduction}

Deep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples~\citep{goodfellow2014generative,karras2018progressive,brock2018large,oord2016pixel,menick2018generating,kalchbrenner2017video,dinh2016density,kingma2018glow,prenger2019waveglow,oord2016wavenet,kalchbrenner2018efficient,kingma2013auto,razavi2019generating}, and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs~\citep{du2019implicit,song2019generative}.

This paper presents progress in diffusion probabilistic models~\citep{sohl2015deep}. A diffusion probabilistic model (which we will call a ``diffusion model'' for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed.
When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.

Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models~(\cref{sec:experiments}).
In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling~(\cref{sec:revproc_dsm_diffusion_connection})~\citep{song2019generative,vincent2011connection}. 
We obtained our best sample quality results using this parameterization~(\cref{sec:loss_ablation}), so we consider this equivalence to be one of our primary contributions.

Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching~\citep{du2019implicit,song2019generative}).
We find that the majority of our models' lossless codelengths  are consumed to describe imperceptible image details~(\cref{sec:coding}). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.


\section{Background}
Diffusion models~\citep{sohl2015deep} are latent variable models of the form , where  are latents of the same dimensionality as the data . The joint distribution  is called the \emph{reverse process}, and it is defined as a Markov chain with learned Gaussian transitions starting at :

What distinguishes diffusion models from other types of latent variable models is that the approximate posterior , called the \emph{forward process} or \emph{diffusion process}, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule :


 
\begin{figure}[t]
\centering
         \includegraphics[width=0.75\textwidth]{\iftoggle{hqfigures}{images_high_quality/pgm_diagram_xarrow.pdf}{images/pgm_diagram_xarrow_small.pdf}}
     \caption{\small{The directed graphical model considered in this work.}}
     \label{fig:pgm}
     \vspace{-1em}
\end{figure}


Training is performed by optimizing the usual variational bound on negative log likelihood:

The forward process variances  can be learned by reparameterization~\citep{kingma2013auto} or held constant as hyperparameters, and
expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in , because both processes have the same functional form when  are small~\citep{sohl2015deep}. 
A notable property of the forward process is that it admits sampling   at an arbitrary timestep  in closed form: using the notation  and , we have

Efficient training is therefore possible by optimizing random terms of  with stochastic gradient descent.
Further improvements come from variance reduction by rewriting ~\labelcref{eq:vb_original} as:

(See~\cref{sec:extended_derivations} for details. The labels on the terms are used in \cref{sec:main}.) \Cref{eq:vb} uses KL divergence to directly compare  against forward process posteriors, which are tractable when conditioned on :

Consequently, all KL divergences in \cref{eq:vb} are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.

\section{Diffusion models and denoising autoencoders}
\label{sec:main}

Diffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances  of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process.
To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching~(\cref{sec:revproc_dsm_diffusion_connection}) that leads to a simplified, weighted variational bound objective for diffusion models~(\cref{sec:simplified_training_objective}). Ultimately, our model design is justified by simplicity and empirical results~(\cref{sec:experiments}). Our discussion is categorized by the terms of~\cref{eq:vb}.


\subsection[Forward process]{Forward process and }
We ignore the fact that the forward process variances  are learnable by reparameterization and instead fix them to constants (see~\cref{sec:experiments} for details).
Thus, in our implementation, the approximate posterior  has no learnable parameters, so  is a constant during training and can be ignored.


\subsection[Reverse process]{Reverse process and  } \label{sec:revproc_dsm_diffusion_connection}
Now we discuss our choices in  for .
First, we set  to untrained time dependent constants. Experimentally, both  and  had similar results. The first choice is optimal for , and the second is optimal for  deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance~\citep{sohl2015deep}.


Second, to represent the mean , we propose a specific parameterization motivated by the following analysis of .
With , we can write:

where  is a constant that does not depend on~. So, we see that the most straightforward parameterization of  is a model that predicts , the forward process posterior mean.
However, we can expand \cref{eq:vb_term_orig} further by reparameterizing \cref{eq:q_marginal_arbitrary_t} as  for  and applying the forward process posterior formula~\labelcref{eq:q_posterior_mean_var}:



\Cref{eq:vb_term_langevin_mu} reveals that  must predict  given . Since  is available as input to the model, we may choose the parameterization

where  is a function approximator intended to predict  from . To sample  is to compute
, where .
The complete sampling procedure, \cref{alg:sampling}, resembles Langevin dynamics with  as a learned gradient of the data density.
Furthermore, with the parameterization~\labelcref{eq:mu_func_approx_langevin}, \cref{eq:vb_term_langevin_mu} simplifies to:

which resembles denoising score matching over multiple noise scales indexed by ~\citep{song2019generative}. As \cref{eq:vb_term_langevin_eps} is equal to (one term of) the variational bound for the Langevin-like reverse process~\labelcref{eq:mu_func_approx_langevin}, we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.

To summarize, we can train the reverse process mean function approximator  to predict , or by modifying its parameterization, we can train it to predict . (There is also the possibility of predicting , but we found this to lead to worse sample quality early in our experiments.) We have shown that the  -prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model's variational bound to an objective that resembles denoising score matching.
Nonetheless, it is just another parameterization of , so we verify its effectiveness in \cref{sec:experiments} in an ablation where we compare predicting  against predicting .


\algrenewcommand\algorithmicindent{0.5em}\begin{figure}[t]
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Training} \label{alg:training}
  \small
  \begin{algorithmic}[1]
    \Repeat
      \State 
      \State 
      \State 
      \State Take gradient descent step on
      \Statex 
    \Until{converged}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Sampling} \label{alg:sampling}
  \small
  \begin{algorithmic}[1]
    \vspace{.04in}
    \State 
    \For{}
      \State  if , else 
      \State 
    \EndFor
    \State \textbf{return} 
    \vspace{.04in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure}

\subsection[Data scaling and reverse process decoder]{Data scaling, reverse process decoder, and }
We assume that image data consists of integers in  scaled linearly to . This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior .
To obtain discrete log likelihoods, we set the last term of the reverse process to an independent discrete decoder derived from the Gaussian :

where  is the data dimensionality and the  superscript indicates extraction of one coordinate.
(It would be straightforward to instead incorporate a more powerful decoder like a conditional autoregressive model, but we leave that to future work.) Similar to the discretized continuous distributions used in VAE decoders and autoregressive models~\citep{kingma2016improved,salimans2017pixelcnn++}, our choice here ensures that the variational bound is a lossless codelength of discrete data, without need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of sampling, we display  noiselessly.

\subsection{Simplified training objective}
\label{sec:simplified_training_objective}


With the reverse process and decoder defined above, the variational bound, consisting of terms derived from \cref{eq:vb_term_langevin_eps,eq:discrete_decoder}, is clearly differentiable with respect to  and is ready to be employed for training. However, we found it beneficial to sample quality (and simpler to implement) to train on the following variant of the variational bound:

where  is uniform between  and . The  case corresponds to  with the integral in the discrete decoder definition \labelcref{eq:discrete_decoder} approximated by the Gaussian probability density function times the bin width, ignoring  and edge effects. The  cases correspond to an unweighted version of \cref{eq:vb_term_langevin_eps}, analogous to the loss weighting used by the NCSN denoising score matching model~\citep{song2019generative}. ( does not appear because the forward process variances  are fixed.)
\Cref{alg:training} displays the complete training procedure with this simplified objective.

Since our simplified objective~\labelcref{eq:training_objective_simple} discards the weighting in \cref{eq:vb_term_langevin_eps}, it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound~\citep{gregor2016towards,higgins2017beta}.
In particular, our diffusion process setup in \cref{sec:experiments} causes the simplified objective to down-weight loss terms corresponding to small . These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger  terms. We will see in our experiments that this reweighting leads to better sample quality.

\section{Experiments}
\label{sec:experiments}

We set  for all experiments so that the number of neural network evaluations needed during sampling matches previous work~\citep{sohl2015deep,song2019generative}. We set the forward process variances to constants increasing linearly from  to . These constants were chosen to be small relative to data scaled to , ensuring that reverse and forward processes have approximately the same functional form while keeping the signal-to-noise ratio at  as small as possible ( bits per dimension in our experiments).

To represent the reverse process, we use a U-Net backbone similar to an unmasked \mbox{PixelCNN++}~\citep{salimans2017pixelcnn++,ronneberger2015unet} with group normalization throughout~\citep{wu2018group}. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding~\citep{vaswani2017attention}. We use self-attention at the  feature map resolution~\citep{wang2018non,vaswani2017attention}. Details are in \cref{sec:experiment_details}.


\subsection{Sample quality}

\begin{figure}
 \begin{minipage}{0.55\textwidth}
\captionof{table}{\small{CIFAR10 results. NLL measured in bits/dim.}} \label{table:cifar10_results}
  \centering \scriptsize
  \vspace{-1em}
  \begin{tabular}{lccc}
    \toprule
    Model & IS & FID & NLL Test (Train) \\
    \midrule
    \textbf{Conditional} \\
    \midrule
    EBM~\citep{du2019implicit} &  &  \\
    JEM~\citep{grathwohl2020your} &  &  \\
    BigGAN~\citep{brock2018large} &  &  \\
    StyleGAN2 + ADA (v1)~\cite{karras2020training} &  &  \\
    \midrule
    \textbf{Unconditional} \\
    \midrule
    Diffusion (original)~\citep{sohl2015deep} & & &  \\
    Gated PixelCNN~\citep{oord2016conditional} &  &  &   \\
    Sparse Transformer~\citep{child2019generating} &   &  &  \\
    PixelIQN~\citep{ostrovski2018autoregressive} &  &  \\
    EBM~\citep{du2019implicit} &  &  \\
    NCSNv2~\citep{song2020improved} & &  \\
    NCSN~\citep{song2019generative} &  &  \\
    SNGAN~\citep{miyato2018spectral} &  &  \\
    SNGAN-DDLS~\citep{che2020your} &  &  \\
    StyleGAN2 + ADA (v1)~\citep{karras2020training} &  &  \\
    Ours (, fixed isotropic ) &  &  &   \\
    \textbf{Ours ()} &  &  &  \\ 
    \bottomrule
  \end{tabular}
 \end{minipage}\hfill
 \begin{minipage}{0.4\textwidth}
\captionof{table}{\small{Unconditional CIFAR10 reverse process parameterization and training objective ablation. Blank entries were unstable to train and generated poor samples with out-of-range scores.}} \label{table:loss_ablation}
  \centering \scriptsize
  \vspace{-1em}
  \begin{tabular}{lcc}
    \toprule
    Objective &  IS & FID  \\
    \midrule
    \textbf{ prediction (baseline)} \\
    \midrule
    , learned diagonal  &  &   \\
    , fixed isotropic  &  &      \\
     & -- & --  \\
    \midrule
    \textbf{ prediction (ours)} \\
    \midrule
    , learned diagonal  &  -- & -- \\
    , fixed isotropic  &    &    \\
     () &  &  \\
    \bottomrule
  \end{tabular}
 \end{minipage}
\vspace{-1em}
\end{figure}

\Cref{table:cifar10_results} shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than most models in the literature, including class conditional models. Our FID score is computed with respect to the training set, as is standard practice; when we compute it with respect to the test set, the score is 5.24, which is still better than many of the training set FID scores in the literature.

We find that training our models on the true variational bound yields better codelengths than training on the simplified objective, as expected, but the latter yields the best sample quality. See \cref{fig:header_samples} for CIFAR10 and CelebA-HQ  samples, \cref{fig:samples_lsun_church} and \cref{fig:samples_lsun_bedroom} for LSUN  samples \cite{yu15lsun}, and \cref{sec:extended_samples} for more.

\begin{figure}
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{\iftoggle{hqfigures}{images_high_quality/lsun_church_layout.pdf}{images/lsun_church_layout.jpg}}
    \caption{\small{LSUN Church samples. FID=}}
    \label{fig:samples_lsun_church}
\end{minipage}\hfill
\begin{minipage}[t]{0.49\linewidth}
    \centering
    \includegraphics[width=\linewidth]{\iftoggle{hqfigures}{images_high_quality/lsun_bedroom_layout_l192_step1556000.pdf}{images/lsun_bedroom_layout_l192_step155600.jpg}}
    \caption{\small{LSUN Bedroom samples. FID=}}
    \label{fig:samples_lsun_bedroom}
\end{minipage}
\vspace{-1em}
\end{figure}

\subsection{Reverse process parameterization and training objective ablation}
\label{sec:loss_ablation}

In \cref{table:loss_ablation}, we show the sample quality effects of reverse process parameterizations and training objectives (\cref{sec:revproc_dsm_diffusion_connection}). We find that the baseline option of predicting  works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to~\cref{eq:training_objective_simple}. We also see that learning reverse process variances (by incorporating a parameterized diagonal  into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances. Predicting , as we proposed, performs approximately as well as predicting  when trained on the variational bound with fixed variances, but much better when trained with our simplified objective.




\subsection{Progressive coding}
\label{sec:coding}

\Cref{table:cifar10_results} also shows the codelengths of our CIFAR10 models. The gap between train and test is at most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based models and indicates that our diffusion model is not overfitting (see~\cref{sec:extended_samples} for nearest neighbor visualizations).
Still, while our lossless codelengths are better than the large estimates reported for energy based models and score matching using annealed importance sampling~\citep{du2019implicit}, they are not competitive with other types of likelihood-based generative models~\citep{child2019generating}.

Since our samples are nonetheless of high quality, we conclude that diffusion models have an inductive bias that makes them excellent lossy compressors. Treating the variational bound terms  as rate and  as distortion, our CIFAR10 model with the highest quality samples has a rate of \textbf{1.78} bits/dim and a distortion of \textbf{1.97} bits/dim, which amounts to a root mean squared error of \text{0.95} on a scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.

\paragraph{Progressive lossy compression} We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of \cref{eq:vb}: see \cref{alg:sending,alg:receiving}, which assume access to a procedure, such as minimal random coding~\citep{harsha2007communication,havasi2018minimal}, that can transmit a sample  using approximately  bits on average for any distributions  and , for which only  is available to the receiver beforehand.
\begin{figure}[t]
\begin{minipage}[t]{0.55\textwidth}
\begin{algorithm}[H]
 \caption{Sending } \label{alg:sending}
 \small
 \begin{algorithmic}[1]
    \State Send  using 
    \For{}
     \State Send  using 
    \EndFor
    \State Send  using 
 \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.44\textwidth}
\begin{algorithm}[H]
 \caption{Receiving} \label{alg:receiving}
 \small
 \begin{algorithmic}[1]
 \vspace{.01in}
    \State Receive  using 
    \For{}
     \State Receive  using 
    \EndFor
    \State \textbf{return} 
    \vspace{.01in}
 \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure}
When applied to , \cref{alg:sending,alg:receiving} transmit  in sequence using a total expected codelength equal to \cref{eq:vb}. The receiver, at any time~, has the partial information  fully available and can progressively estimate:

due to~\cref{eq:q_marginal_arbitrary_t}.
(A stochastic reconstruction  is also valid, but we do not consider it here because it makes distortion more difficult to evaluate.)
\Cref{fig:cifar10_rate_distortion} shows the resulting rate-distortion plot on the CIFAR10 test set. At each time , the distortion is calculated as the root mean squared error , and the rate is calculated as the cumulative number of bits received so far at time . The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions.

\pgfplotsset{small,width=5.1cm,height=4cm,tick label style={font=\tiny},label style={font=\tiny}}
\begin{figure}[ht]
\begin{tikzpicture}
\begin{axis}[
  grid=both,minor tick num=1,
  xlabel=Reverse process steps (),
  ylabel=Distortion (RMSE)
]
\addplot+[only marks,mark size=0.9pt] table[x=timestep,y=rmse_scaled,col sep=comma] {plot_data/cifar10_eps-fixedlarge-mse_step790000.csv};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
  grid=both,minor tick num=1,
  xlabel=Reverse process steps (),
  ylabel=Rate (bits/dim)
]
\addplot+[only marks,mark size=0.9pt] table[x=timestep,y=rate,col sep=comma] {plot_data/cifar10_eps-fixedlarge-mse_step790000.csv};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
  grid=both,minor tick num=1,
  xlabel=Rate (bits/dim),
  ylabel=Distortion (RMSE)
]
\addplot+[only marks,mark size=0.9pt] table[x=rate,y=rmse_scaled,col sep=comma] {plot_data/cifar10_eps-fixedlarge-mse_step790000.csv};
\end{axis}
\end{tikzpicture}
\caption{\small{Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a  scale. See~\cref{table:cifar10_rate_distortion_table} for details.}}
\label{fig:cifar10_rate_distortion}
\vspace{-1em}
\end{figure}


\paragraph{Progressive generation}  We also run a progressive unconditional generation process given by progressive decompression from random bits. In other words, we predict the result of the reverse process, , while sampling from the reverse process using~\cref{alg:sampling}. 
\Cref{fig:cropped_cifar10_progressive_samples,fig:cifar10_progressive_sample_quality} show the resulting sample quality of  over the course of the reverse process.
Large scale image features appear first and details appear last.
\Cref{fig:ext_celeba_stochastic_decoding} shows stochastic predictions  with  frozen for various . When  is small, all but fine details are preserved, and when  is large, only large scale features are preserved. Perhaps these are hints of conceptual compression~\citep{gregor2016towards}.

\begin{figure}[h]
  \centering
  \includegraphics[trim=0cm 19.15cm 0cm 0cm,clip,width=0.9\textwidth]{\iftoggle{hqfigures}{images_high_quality/cifar10_eps-fixedlarge-mse_20_progressive.png}{images/cifar10_eps-fixedlarge-mse_20_progressive.jpg}}
  \caption{\small{Unconditional CIFAR10 progressive generation ( over time, from left to right). Extended samples and sample quality metrics over time in the appendix~(\cref{fig:ext_cifar10_progressive_samples,fig:cifar10_progressive_sample_quality}).}}
  \label{fig:cropped_cifar10_progressive_samples}
  \vspace{-1em}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{\iftoggle{hqfigures}{images_high_quality/stochastic_decoding.pdf}{images/stochastic_decoding.jpg}}
  \caption{\small{When conditioned on the same latent, CelebA-HQ  samples share high-level attributes. Bottom-right quadrants are , and other quadrants are samples from .}}
  \label{fig:ext_celeba_stochastic_decoding}
  \vspace{-1em}
\end{figure}




\paragraph{Connection to autoregressive decoding} Note that the variational bound~\labelcref{eq:vb} can be rewritten as:

(See \cref{sec:extended_derivations} for a derivation.) Now consider setting the diffusion process length  to the dimensionality of the data, defining the forward process so that  places all probability mass on  with the first  coordinates masked out (i.e.  masks out the  coordinate), setting  to place all mass on a blank image, and, for the sake of argument, taking   to be a fully expressive conditional distribution. With these choices, , and minimizing  trains  to copy coordinates  unchanged and to predict the  coordinate given . Thus, training  with this particular diffusion is training an autoregressive model.

We can therefore interpret the Gaussian diffusion model~\labelcref{eq:forwardprocess} as a kind of autoregressive model with a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality~\citep{menick2018generating}, so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use , which is less than the dimension of the  or  images in our experiments. Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.


\subsection{Interpolation}

We can interpolate source images  in latent space using  as a stochastic encoder, , then decoding the linearly interpolated latent  into image space by the reverse process, . In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images, as depicted in \cref{fig:interpolations} (left). We fixed the noise for different values of  so  and  remain the same. \cref{fig:interpolations} (right) shows interpolations and reconstructions of original CelebA-HQ  images (). The reverse process produces high-quality reconstructions, and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger  results in coarser and more varied interpolations, with novel samples at  (Appendix \cref{fig:interpolations_coarse_to_fine}).

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{\iftoggle{hqfigures}{images_high_quality/interp_with_diagram.pdf}{images/interp_with_diagram.jpg}}
    \caption{\small{Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.}}
    \label{fig:interpolations}
    \vspace{-1em}
\end{figure}

\section{Related Work}

While diffusion models might resemble flows~\citep{dinh2014nice,rezende2015variational,dinh2016density,kingma2018glow,chen2018neural,grathwohl2019ffjord,ho2019flow++} and VAEs~\citep{kingma2013auto,rezende2014stochastic,maaloe2019biva}, diffusion models are designed so that  has no parameters and the top-level latent  has nearly zero mutual information with the data .
Our -prediction reverse process parameterization establishes a connection between diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics for sampling~\citep{song2019generative,song2020improved}. Diffusion models, however, admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference (see \cref{sec:extended_related_work} for details).
The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training~\citep{bordes2016learning}, variational walkback~\citep{goyal2017variational}, generative stochastic networks~\citep{alain2016gsns}, and others~\citep{salimans2015markov,song2017nice,levy2018generalizing,nijkamp2019learning,lawson2019energy,wu2020stochastic}.

By the known connection between score matching and energy-based modeling, our work could have implications for other recent work on energy-based models~\citep{xie2016theory,xie2017synthesizing,xie2018learning,gao2018learning,xie2019learning,gao2020flow,du2019implicit,nijkamp2019anatomy,grathwohl2020your,deng2020residual}. Our rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent of how rate-distortion curves can be computed over distortion penalties in one run of annealed importance sampling~\citep{huang2020evaluating}. Our progressive decoding argument can be seen in convolutional DRAW and related models~\citep{gregor2016towards,nichol2020vq} and may also lead to more general designs for subscale orderings or sampling strategies for autoregressive models~\citep{menick2018generating,wiggers2020predictive}.

\section{Conclusion}

We have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.


\section*{Broader Impact}

Our work on diffusion models takes on a similar scope as existing work on other types of deep generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive models, and so forth. Our paper represents progress in making diffusion models a generally useful tool in this family of techniques, so it may serve to amplify any impacts that generative models have had (and will have) on the broader world.

Unfortunately, there are numerous well-known malicious uses of generative models. Sample generation techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were available, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection \cite{wang2019cnngenerated}, but improvements in generative models may make this more difficult.
Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.

On the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.

\begin{ack}
This work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant number DGE-1752814. Google's TensorFlow Research Cloud (TFRC) provided Cloud TPUs.
\end{ack}

\setlength{\bibsep}{5pt}
\bibliographystyle{plainnat}
{\small \bibliography{main.bib}}

\pagebreak
\appendix

\section*{Extra information}

\paragraph{LSUN}

FID scores for LSUN datasets are included in \cref{table:lsun_fid}. Scores marked with  are reported by StyleGAN2 as baselines, and other scores are reported by their respective authors.

\begin{table}[h]
\centering
\caption{FID scores for LSUN  datasets}
\label{table:lsun_fid}
\begin{tabular}{@{}lccc@{}}
\toprule
Model                        & LSUN Bedroom &  LSUN Church & LSUN Cat \\ \midrule
ProgressiveGAN \cite{karras2018progressive} & 8.34 & 6.42 & 37.52 \\
StyleGAN \cite{karras2019style} & \textbf{2.65} & 4.21\rlap{}  & 8.53\rlap{}\\
StyleGAN2 \cite{karras2019analyzing} & - & \textbf{3.86} & \textbf{6.93} \\ \midrule 
Ours () & 6.36 & 7.89 & 19.75 \\
Ours (, large) & 4.90 & - & - \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Progressive compression}
Our lossy compression argument in \cref{sec:coding} is only a proof of concept, because \cref{alg:sending,alg:receiving} depend on a procedure such as minimal random coding~\citep{havasi2018minimal}, which is not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound \labelcref{eq:vb} of \citet{sohl2015deep}, not yet as a practical compression system.

\begin{table}[h]
\centering
\caption{Unconditional CIFAR10 test set rate-distortion values  (accompanies~\cref{fig:cifar10_rate_distortion})}
\label{table:cifar10_rate_distortion_table}
\begin{tabular}{ccc}
\toprule
Reverse process time () & Rate (bits/dim) & Distortion (RMSE ) \\
\midrule
1000 & 1.77581 & 0.95136 \\
900 & 0.11994 & 12.02277 \\
800 & 0.05415 & 18.47482 \\
700 & 0.02866 & 24.43656 \\
600 & 0.01507 & 30.80948 \\
500 & 0.00716 & 38.03236 \\
400 & 0.00282 & 46.12765 \\
300 & 0.00081 & 54.18826 \\
200 & 0.00013 & 60.97170 \\
100 & 0.00000 & 67.60125 \\
\bottomrule
\end{tabular}
\end{table}


\section{Extended derivations}
\label{sec:extended_derivations}

Below is a derivation of \cref{eq:vb}, the reduced variance variational bound for diffusion models. This material is from \citet{sohl2015deep}; we include it here only for completeness.
\begingroup
\allowdisplaybreaks

The following is an alternate version of . It is not tractable to estimate, but it is useful for our discussion in \cref{sec:coding}.

\endgroup


\section{Experimental details}
\label{sec:experiment_details}

Our neural network architecture follows the backbone of PixelCNN++~\citep{salimans2017pixelcnn++}, which is a U-Net~\citep{ronneberger2015unet} based on a Wide ResNet~\citep{zagoruyko2016wide}. We replaced weight normalization~\citep{salimans2016weight} with group normalization~\citep{wu2018group} to make the implementation simpler. Our  models use four feature map resolutions ( to ), and our  models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the  resolution between the convolutional blocks~\citep{chen2018pixelsnail}. Diffusion time  is specified by adding the Transformer sinusoidal position embedding~\citep{vaswani2017attention} into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.

We used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21 steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling a batch of 256 images takes 17 seconds.
Our CelebA-HQ/LSUN (256) models train at 2.2 steps per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.

Apart from an initial choice of hyperparameters early on to make network size fit within memory constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample quality, then transferred the resulting settings over to the other datasets:
\begin{itemize}
    \item We chose the  schedule from a set of constant, linear, and quadratic schedules, all constrained so that . We set  without a sweep, and we chose a linear schedule from  to . 
    \item We set the dropout rate on CIFAR10 to  by sweeping over the values . Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized PixelCNN++~\citep{salimans2017pixelcnn++}. We set dropout rate on the other datasets to zero without sweeping.
    \item We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly. We also used random horizontal flips for all other datasets except LSUN Bedroom.
    \item We tried Adam~\citep{kingma2014adam} and RMSProp early on in our experimentation process and chose the former. We left the hyperparameters to their standard values. We set the learning rate to  without any sweeping, and we lowered it to  for the  images, which seemed unstable to train with the larger learning rate.
    \item We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over these values.
    \item We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over this value.
\end{itemize}

Final experiments were trained once and evaluated throughout training for sample quality. Sample quality scores and log likelihood are reported on the minimum FID value over the course of training. On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI~\citep{salimans2016improved} and TTUR~\citep{heusel2017gans} repositories, respectively. On LSUN, we calculated FID scores on 50000 samples using code from the StyleGAN2~\citep{karras2019analyzing} repository. CIFAR10 and CelebA-HQ were loaded as provided by TensorFlow Datasets (\url{https://www.tensorflow.org/datasets}), and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard from the papers that introduced their usage in a generative modeling context. All details can be found in the source code release.

\section{Discussion on related work}
\label{sec:extended_related_work}

Our model architecture, forward process definition, and prior differ from NCSN~\citep{song2019generative,song2020improved} in subtle but important ways that improve sample quality, and, notably, we directly train our sampler as a latent variable model rather than adding it after training post-hoc. In greater detail:
\begin{enumerate}
  \item We use a U-Net with self-attention; NCSN uses a RefineNet with dilated convolutions.
We condition all layers on  by adding in the Transformer sinusoidal position embedding, rather than only in normalization layers (NCSNv1) or only at the output (v2).
  \item Diffusion models scale down the data with each forward process step (by a  factor) so that variance does not grow when adding noise, thus providing consistently scaled inputs to the neural net reverse process. NCSN omits this scaling factor.
  \item Unlike NCSN, our forward process destroys signal (), ensuring a close match between the prior and aggregate posterior of .
Also unlike NCSN, our  are very small, which ensures that the forward process is reversible by a Markov chain with conditional Gaussians. Both of these factors prevent distribution shift when sampling.
  \item Our Langevin-like sampler has coefficients  (learning rate, noise scale, etc.)
derived rigorously from  in the forward process. Thus, our training procedure directly trains our sampler to match the data distribution after  steps: it trains the sampler as a latent variable model using variational inference. In contrast, NCSN's sampler coefficients are set by hand post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric of their sampler.
\end{enumerate}

\section{Samples}
\label{sec:extended_samples}
\paragraph{Additional samples} \Cref{fig:ext_celebahq256_samples}, \ref{fig:ext_cifar10_samples},
\ref{fig:ext_lsun_church_samples},
\ref{fig:ext_lsun_bedroom_samples_large},
\ref{fig:ext_lsun_bedroom_samples},
and \ref{fig:ext_lsun_cat_samples} show uncurated samples from the diffusion models trained on CelebA-HQ, CIFAR10 and LSUN datasets.

\paragraph{Latent structure and reverse process stochasticity}
During sampling, both the prior  and Langevin dynamics are stochastic. To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA  dataset. \Cref{fig:ext_celeba_stochastic_decoding} shows multiple draws from the reverse process  that share the latent  for . To accomplish this, we run a single reverse chain from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple images. When the chain is split after the prior draw at , the samples differ significantly. However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents like  encode these attributes, despite their imperceptibility.

\paragraph{Coarse-to-fine interpolation} \Cref{fig:interpolations_coarse_to_fine} shows interpolations between a pair of source CelebA  images as we vary the number of diffusion steps prior to latent space interpolation. Increasing the number of diffusion steps destroys more structure in the source images, which the model completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of  diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after  diffusion steps, source information is lost and interpolations are novel samples.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{\iftoggle{hqfigures}{images_high_quality/interp_coarse_to_fine.pdf}{images/interp_coarse_to_fine_jpg_small.pdf}}
    \captionof{figure}{Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.}
    \label{fig:interpolations_coarse_to_fine}
\end{figure}

\pgfplotsset{small,width=6.5cm,height=5cm,tick label style={font=\footnotesize},label style={font=\small}}
\begin{figure}[b] \centering
\begin{tikzpicture}
\begin{axis}[
  grid=both,minor tick num=1,
  xlabel=Reverse process steps (),
  ylabel=Inception Score
]
\addplot+[only marks] table[x=timestep,y=is_mean,col sep=comma] {plot_data/cifar10_sample_quality.csv};
\end{axis}
\end{tikzpicture}
\begin{tikzpicture}
\begin{axis}[
  grid=both,minor tick num=1,
  xlabel=Reverse process steps (),
  ylabel=FID
]
\addplot+[only marks] table[x=timestep,y=fid,col sep=comma] {plot_data/cifar10_sample_quality.csv};
\end{axis}
\end{tikzpicture}
\caption{Unconditional CIFAR10 progressive sampling quality over time}
\label{fig:cifar10_progressive_sample_quality}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\iftoggle{hqfigures}{images_high_quality/celebahq256_extended_samples_jpg_hq.jpg}{images/celebahq256_extended_samples_jpg.jpg}}
  \caption{CelebA-HQ  generated samples}
  \label{fig:ext_celebahq256_samples}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{images/celebahq256_more_nnpixelspace.jpg}
    \caption{Pixel space nearest neighbors}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{images/celebahq256_more_nninception.jpg}
    \caption{Inception feature space nearest neighbors}
  \end{subfigure}
  \caption{CelebA-HQ  nearest neighbors, computed on a  crop surrounding the faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.}
  \label{fig:ext_celebahq256_nearest_neighbors_inception}
\end{figure}



\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/cifar10_eps-fixedlarge-mse_20x20.png}
  \caption{Unconditional CIFAR10 generated samples}
  \label{fig:ext_cifar10_samples}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\iftoggle{hqfigures}{images_high_quality/cifar10_eps-fixedlarge-mse_20_progressive.png}{images/cifar10_eps-fixedlarge-mse_20_progressive.jpg}}
  \caption{Unconditional CIFAR10 progressive generation}
  \label{fig:ext_cifar10_progressive_samples}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{\textwidth}\centering
    \includegraphics[width=0.9\textwidth]{images/cifar10_eps-fixedlarge-mse_20_nn.jpg}
    \caption{Pixel space nearest neighbors}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}\centering
    \includegraphics[width=0.9\textwidth]{images/cifar10_eps-fixedlarge-mse_20_nninception.jpg}
    \caption{Inception feature space nearest neighbors}
  \end{subfigure}
  \caption{Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.}
  \label{fig:ext_cifar10_nearest_neighbors}
\end{figure}




\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\iftoggle{hqfigures}{images_high_quality/lsun_church_samples_10x10_highlight_step1152000.pdf}{images/lsun_church_samples_10x10_highlight_step1152000_small.jpg}}
  \caption{LSUN Church generated samples. FID=}
  \label{fig:ext_lsun_church_samples}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\iftoggle{hqfigures}{images_high_quality/lsun_bedroom_10x10_highlight_l192_step1556000.pdf}{images/lsun_bedroom_10x10_highlight_l192_step155600_small.jpg}}
  \caption{LSUN Bedroom generated samples, large model. FID=}
  \label{fig:ext_lsun_bedroom_samples_large}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\iftoggle{hqfigures}{images_high_quality/lsun_bedroom_10x10_highlight_step2388000.pdf}{images/lsun_bedroom_10x10_highlight_step2388000_smallest.jpg}}
  \caption{LSUN Bedroom generated samples, small model. FID=}
  \label{fig:ext_lsun_bedroom_samples}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{\iftoggle{hqfigures}{images_high_quality/lsun_cat_10x10_highlight_step1761000_fid19point75.pdf}{images/lsun_cat_10x10_highlight_step1761000_fid19point75_smallest.jpg}}
  \caption{LSUN Cat generated samples. FID=}
  \label{fig:ext_lsun_cat_samples}
\end{figure}


\end{document}
