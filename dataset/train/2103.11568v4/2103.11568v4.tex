\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{pipelinev4.png}
    \caption{The system pipeline of our unsupervised person re-ID method. The upper branch depicts the memory initialization stage. The training data features are assigned pseudo labels by clustering, where features of the same color belong to the same cluster. The lower branch represents the model training stage. Query features in iterative mini-batch are used to update the memory cluster representations with a momentum. The ClusterNCE loss computes the contrastive loss between query features and all cluster representations.}
    \label{fig:pipeline}
\end{figure}

\section{Method}
We first introduce our overall approach at a high level in Section~\ref{sec:overview}.
Then, we compare the multiple contrastive learning approaches for person re-ID with our proposed cluster contrast method in Section~\ref{sec:ContrastiveLearning}.
Finally in Section~\ref{sec:ClusterContrast}, we explain the details of momentum update in Cluster Contrast along with its working theory.

\subsection{Overview}
\label{sec:overview}

State-of-the-art unsupervised learning methods~\cite{wang2020unsupervised,ge2020mutual,ge2020self,chen2021ice} solve the unsupervised learning person re-ID problem with contrastive learning. 
Specifically, they build a memory dictionary that contains the features of all training images. 
Each feature is assigned a pseudo ID generated by a clustering algorithm. 
During training, the contrastive loss is minimized to train the network and learn a proper feature embedding that is consistent with the pseudo ID.

We focused on designing a proper contrastive learning method to keep the whole pipeline simple while obtaining better performance.
An overview of our training pipeline is shown in Figure~\ref{fig:pipeline}.
The memory dictionary initialization is illustrated in the upper branch.
We use a standard ResNet50~\cite{he2016deep} as the backbone encoder which is pretrained on ImageNet to extract feature vectors, and has basic discriminability though not optimized for re-ID tasks.
We then apply the DBSCAN~\cite{ester1996density} clustering algorithms to cluster similar features together and assign pseudo labels to them. 
The cluster feature representation is calculated as the mean feature vectors of each cluster. 
The memory dictionary is initialized by these cluster feature representations and their corresponding pseudo labels. 
As shown in the lower branch, during the training stage, we compute the ClusterNCE loss between the query image features and all cluster representations in the dictionary to train the network. 
Meanwhile, the dictionary features are updated with a momentum by the query features.

To facilitate the description of methods, we first introduce the notations used in this paper.
Let  denote the training set with  instances. 
And  denotes the corresponding features obtained from the backbone encoder , described as .
 is a query instance feature extracted by , where the query instance belongs to .

\begin{figure}[!t]
    \subfloat[Multi-label classification loss]{
        \centering
		\includegraphics[height=27mm]{mmclv2.png}
    }
    \hfill
    \subfloat[Instance level InfoNCE loss]{
        \centering
		\includegraphics[height=27mm]{spclv2.png}
    }
    \hfill
    \subfloat[ClusterNCE loss (ours)]{
        \centering
		\includegraphics[height=29mm]{oursv2.png}
    }
	\caption{Comparison of three types of memory-based non-parametric contrastive learning losses for re-ID. Different color features indicate different clusters. (a) computes the loss and updates the memory dictionary both at the instance level~\cite{wang2020unsupervised}. (b) computes the loss at the cluster level but updates the memory dictionary at the instance level~\cite{ge2020self}. (c) is our proposed approach and it computes the loss and updates the memory dictionary both at the cluster level.}
	\label{fig:loss}
\end{figure}

\subsection{Cluster Contrast}
\label{sec:ContrastiveLearning}

In this section, we analyze different contrastive learning methods to motivate our design of Cluster Contrast. As shown in Figure~\ref{fig:loss} (a), the multi-label classification loss computes the loss in the instance level through an instance-wise contrastive loss. It stores all image feature vectors in the memory dictionary and computes multi-class score by comparing each query feature to all of them. The memory dictionary is updated by the query features after each training iteration.




In Figure~\ref{fig:loss} (b), SPCL~\cite{ge2020self} computes the loss at cluster level through a cluster-wise InfoNCE loss. It can be defined as follows:


where  is a temperature hyper-parameter,   are the cluster centroids and  stands for the number of clusters. 
It uses the cluster centroid as the cluster level feature vector to compute the the distances between query instance  and all the clusters.  is the positive cluster feature which  belongs to.
The cluster centroids are calculated by the mean feature vectors of each cluster as:

where  denotes the -th cluster set and  indicates the number of instances per cluster. 
 contains all the feature vectors in the cluster .
But similar to multi-classification loss , it stores all image feature vectors in the memory dictionary. The stored image feature vectors are then updated by corresponding query image feature.

Both Figure~\ref{fig:loss} (a) and Figure~\ref{fig:loss} (b) update the feature vectors at an instance level, resulting in feature inconsistency problem.
As shown in Figure~\ref{fig:clustersize}, the cluster size is unbalancedly distributed.
In every training iteration, in a large cluster only a small fraction of the instance features can be updated due to the batch size limitation, whereas in a small cluster all the instances can be updated.
Thus, the updating process is highly varied, and the contrastive loss computed by comparing all instance features is not consistent with the newest model. In each iteration, the network is constantly updated, which causes inconsistent oscillatory distribution of mini-batches.
In contrast, we design our ClusterNCE loss as shown in Figure~\ref{fig:loss} (c) using the following equation:

where  is the unique representation vector of the -th cluster.
It updates the feature vectors and computes the loss both in the cluster level.

We can see that, our proposed algorithm uses unique feature vectors to represent each cluster category and remains distinct throughout the updating process, which is the most significant difference from the previous contrastive loss approaches.
In the next section, we will discuss in detail how our method consistently updates the cluster representation to maintain the cluster consistency with the help of momentum update.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{clustersizev2.png}
    \caption{The cluster size follows a normal distribution in Market1501 dataset.}
    \label{fig:clustersize}
\end{figure}

\subsection {Momentum Update}
\label{sec:ClusterContrast}
In this section, we present how to initialize and update the cluster level memory in the proposed Cluster Contrast method. The training details are presented in Algorithm~\ref{algorithm}.

\paragraph{Memory Initialization.}
Different from the instance level memory dictionary, we store each cluster's representation  in the memory-based feature dictionary. 
We use the mean feature vectors of each cluster to initialize the cluster representation, that is

Eq.~\ref{eq:ClusterInit_c} is executed when each epoch is initialized. And the clustering algorithm runs in each epoch, so  is changing as the model trains.

\paragraph{Memory Updating.}

During training, following~\cite{hermans2017defense},  person identities and a fixed number  of instances for each person identity were sampled from the training set.
Consequently, we obtain a total number of  query images in the minibatch.
We then momentum update the cluster representation iteratively by the query features in the minibatch by the Eq.~\ref{eq:ClusterUpdate_c} as illustrated in Figure~\ref{fig:loss}:

where  is the query features encoded from k-th cluster images and  is the momentum updating factor.  controls the consistency between the cluster feature and most updated query instance feature. As  close to 0, the cluster feature  is close to the newest query feature. It is worth noting that all cluster representations are stored in the memory dictionary, so we calculate loss by comparing each query instance with all cluster representations in each iteration.






\begin{algorithm}[!t]
    \textbf{Require:} Unlabeled training data \\
    \textbf{Require:} Initialize the backbone encoder  with ImageNet-pretrained ResNet-50 \\
    \textbf{Require:} Temperature  for Eq.~\ref{eq:ClusterNCELoss} \\
    \textbf{Require:} Momentum  for Eq.~\ref{eq:ClusterUpdate_c} \\
    \For{ in }{
        Extract feature vectors  from  by  \\
        Clustering  into  clusters with DBSCAN \\
        Initialize memory dictionary with Eq.~\ref{eq:ClusterInit_c} \\
        \For{ in }{
            Sample  query images from  \\
            Compute ClusterNCE loss with Eq.~\ref{eq:ClusterNCELoss} \\
            Update cluster feature with Eq.~\ref{eq:ClusterUpdate_c} \\
            Update the encoder  by optimizer 
        }
    }
    \caption{Unsupervised learning pipeline with Cluster Contrast}
    \label{algorithm}
\end{algorithm}



