\documentclass[letterpaper,11pt]{article}
\usepackage{url}
\usepackage{amssymb,amsmath,amsfonts,amsthm}
\usepackage{fullpage}
\usepackage[usenames]{color}

\newcommand{\polylog}{{\mathrm{polylog}}}

\DeclareMathSymbol{\qedsymb} {\mathord}{AMSa}{"04}
\newcommand{\qedbox}{\hspace*{0pt}\hfill}
\newcommand{\eps}{\varepsilon}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\cE}{\mathcal{E}}
\newcommand{\cH}{\mathcal{H}}

\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\card}[1]{\abs{#1}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\setst}[2]{\left\{\; #1 \,:\, #2 \;\right\}}        

\newcommand{\prob}[1]{\operatorname{Pr}\left[\,#1\,\right]}
\newcommand{\probg}[2]{\operatorname{Pr}\left[\,#1 \:\mid\:
    #2\,\right]}  \newcommand{\expect}[1]{\operatorname{E}\left[\,#1\,\right]}
\newcommand{\expectg}[2]{\operatorname{E}\left[\,#1 \,\mid\,
    #2\,\right]} \newcommand{\var}[1]{\operatorname{Var}\left[\,#1\,\right]}


\newcommand{\oct}{\quad\quad}                                   \renewcommand{\And}{\text{\normalfont\,~and~\,}}
\newcommand{\set}[1]{\left \{ #1 \right \}}                     \newcommand{\union}{\cup}
\newcommand{\hess}{\mathcal{H}}
\newcommand{\Abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\smallsum}[2]{{\textstyle \sum_{#1}^{#2}}}
\newcommand{\smallfrac}[2]{{\textstyle \frac{#1}{#2}}}

\newcommand{\TLE}{\mathrm{TLE}}

\newcommand{\res}{\mathbf{res}}
\newcommand{\GF}{\mathrm{GF}}
\newcommand{\dlog}{\mathrm{dlog}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Var}{\mathbf{Var}}
\renewcommand{\Pr}{\mathbf{Pr}}
\renewcommand{\mod}{\hbox{ mod }}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\tF}{\tilde{F}}
\newcommand{\tH}{\tilde{H}}
\newcommand{\tO}{\tilde{O}}
\newcommand{\tT}{\tilde{T}}
\newcommand{\tTheta}{\tilde{\Theta}}
\newcommand{\teps}{\tilde{\eps}}

\newcommand{\AlgorithmName}[1]{\label{alg:#1}}
\newcommand{\AppendixName}[1]{\label{app:#1}}
\newcommand{\AssumptionName}[1]{\label{ass:#1}}
\newcommand{\ClaimName}[1]{\label{clm:#1}}
\newcommand{\CorollaryName}[1]{\label{cor:#1}}
\newcommand{\DefinitionName}[1]{\label{def:#1}}
\newcommand{\EquationName}[1]{\label{eq:#1}}
\newcommand{\FactName}[1]{\label{fact:#1}}
\newcommand{\LemmaName}[1]{\label{lem:#1}}
\newcommand{\ObservationName}[1]{\label{obs:#1}}
\newcommand{\RemarkName}[1]{\label{rem:#1}}
\newcommand{\SectionName}[1]{\label{sec:#1}}
\newcommand{\TheoremName}[1]{\label{thm:#1}}
\newcommand{\FigureName}[1]{\label{fig:#1}}

\newcommand{\Algorithm}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\Appendix}[1]{Appendix~\ref{app:#1}}
\newcommand{\Assumption}[1]{Assumption~\ref{ass:#1}}
\newcommand{\Claim}[1]{Claim~\ref{clm:#1}}
\newcommand{\Corollary}[1]{Corollary~\ref{cor:#1}}
\newcommand{\Definition}[1]{Definition~\ref{def:#1}}
\newcommand{\Equation}[1]{Eq.\:\eqref{eq:#1}}
\newcommand{\Fact}[1]{Fact~\ref{fact:#1}}
\newcommand{\Lemma}[1]{Lemma~\ref{lem:#1}}
\newcommand{\Observation}[1]{Observation~\ref{obs:#1}}
\newcommand{\Remark}[1]{Remark~\ref{rem:#1}}
\newcommand{\Section}[1]{Section~\ref{sec:#1}}
\newcommand{\Theorem}[1]{Theorem~\ref{thm:#1}}
\newcommand{\Figure}[1]{Figure~\ref{fig:#1}}

\newtheorem{theorem}{Theorem}\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\proofbelow}{3pt}
\newcommand{\afterproof}{\hfill  \par \vspace{\proofbelow}}
\newcommand{\aftersubproof}{\hfill  \par \vspace{\proofbelow}}
\renewenvironment{proof}{\noindent\textbf{Proof.}\,}{\afterproof}
\newenvironment{subproof}{\noindent\textit{Proof.}\,}{\aftersubproof}
\newenvironment{proofof}[1]{\noindent\textbf{Proof} \,(of #1).\,}{\afterproof}
\newenvironment{proofsketch}{\noindent\textbf{Proof (Sketch).} \,}{\afterproof}

\newcommand{\poly}{\mathop{{\rm poly}}}
\renewcommand{\th}{\ifmmode{^{\textrm{th}}}\else{\textsuperscript{th}\ }\fi}
\newcommand{\newterm}[1]{\textit{#1}}
\newcommand{\defeq}{\,:=\,}
\newcommand{\eqdef}{\mathbin{\stackrel{\rm def}{=}}}
\newcommand{\comment}[1]{}
\newcommand{\viscomment}[1]{\textbf{#1}}
\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO:} \textit{#1}}}
\newcommand{\REN}{R\'{e}nyi\xspace}
\newcommand{\Fres}{F^\mathrm{res}}
\newcommand{\tFres}{\tilde{F}^\mathrm{res}}
\newcommand{\posA}{\vspace{3pt}}
\newcommand{\posB}{\vspace{6pt}}

\begin{document}

\author{Jelani Nelson\footnotemark[2]\oct
  David P. Woodruff\footnotemark[3]}

\date{}

\footnotetext[1]{MIT Computer Science and Artificial Intelligence
  Laboratory. \texttt{minilek@mit.edu}. Supported by a National
  Defense Science and Engineering Graduate (NDSEG) Fellowship. Much of this
  work was done while the author was at the IBM Almaden
  Research Center.}
\footnotetext[2]{IBM Almaden Research Center, 650 Harry Road, San
  Jose, CA, USA. \texttt{dpwoodru@us.ibm.com}.}

\title{A Near-Optimal Algorithm for L1-Difference}

\maketitle

\begin{abstract}
\thispagestyle{empty}
\noindent

We give the first -sketching algorithm for integer vectors which
produces nearly optimal sized sketches in nearly linear time. This
answers the first open problem in the list of open problems 
from the 2006 IITK Workshop on Algorithms for Data Streams. 
Specifically, suppose Alice
receives a vector  and Bob receives
,
and the two parties share randomness.  Each party must output a short 
sketch of their vector such that a
third party can later quickly recover a -approximation to
 with  probability given only the sketches.
We give a sketching algorithm which produces
-bit sketches in 
time, independent of . The
previous best known sketching algorithm for  is due to
[Feigenbaum {\it et al.}, SICOMP 2002], which achieved the optimal
sketch length of
 bits but had a running time of
. Notice that our
running time is near-linear for every , whereas for sufficiently
small values of , the running time of the previous algorithm can
be as large as
quadratic. Like their algorithm, our sketching
procedure also yields a small-space, one-pass streaming algorithm
which works even if the
entries of  are given in arbitrary order. 
\end{abstract}

\section{Introduction}\SectionName{intro}
Space and time-efficient processing of massive databases is a
challenging and important task in applications such as observational
sciences, product marketing, and monitoring large systems. Usually the
data set is distributed across several network devices, each receiving
a portion of the data as a stream. The devices must locally
process their data, producing a small sketch, which can then be
efficiently transmitted to other devices for further
processing. Although much work has focused on producing sketches of
minimal size for various problems, in practice the time efficiency to
produce the sketches is just as important, if not more so, than the
sketch size. 

In the 2006 IITK Workshop on Algorithms for Data Streams, the first
open question posed \cite{IITK} was to find a space- {\it and}
time-efficient algorithm for -difference computation. Formally,
there are two parties, Alice and Bob, who have vectors  and wish to compute sketches 
and , respectively, so that a third party can quickly recover a
value  with . Here, 
denotes the -norm of the vector . The third party should
succeed with probability at least  over the randomness of Alice
and Bob (this probability can be amplified by
repeating the process and taking the median).

The original motivation \cite{FKSV02} for the -difference problem is
Internet-traffic monitoring. As packets travel through
Cisco routers, the NetFlow software \cite{CN98} produces summary
statistics of groups of packets with the same source and destination
IP address. Such a group of packets is known as a {\it flow}. At the
end of a specified time period, a router assembles sets of values , where  is a source-destination pair, and  is the
total number of bytes sent from the source to the destination in time
period . The -difference between such sets assembled during
different time periods or at different routers indicates differences
in traffic patterns.

The ability to produce a short sketch summarizing the set of values
allows a central control and storage facility to later efficiently
approximate the -difference between the sketches that it
receives. The routers producing the sketches cannot
predict which source-destination pairs they will receive, or in which
order. Since the routers can transmit their sketches and updates
to their sketches to the central processing facility in an arbitrarily
interleaved manner, it is essential that the -difference
algorithm support arbitrary permutations of the assembled sets of
values. Because of the huge size of the packet streams, it is also
crucial that the computation time required to produce the sketches
be as small as possible.

The first algorithm for this problem is due to Feigenbaum {\it et al.}
\cite{FKSV02}, and achieves a sketch of size 
bits with each party requiring  processing
time (in word operations). Later, Indyk \cite{Indyk06} generalized this
to the
problem of estimating the -norm of a general data stream with
an arbitrary number of updates to each coordinate. For the
-difference problem, the space of Indyk's method is worse than
that of \cite{FKSV02} by a factor of , while the time
complexity is similar. Recently, in \cite{KNW08} it was shown how to
reduce the space complexity of Indyk's method, thereby matching the
sketch size of \cite{FKSV02}. 
However, the time complexity per stream update 
is . Also in \cite{KNW08}, a space lower
bound of  was shown for the
-difference problem for nearly the full range of interesting
values for , thereby showing that the space complexity of
the algorithms of \cite{FKSV02, KNW08} are optimal.

While the space complexity for -difference is settled, there are
no non-trivial lower bounds for the time complexity. Notice that the
 factor in the processing time can be a severe drawback in
practice, and can make the difference between setting the
approximation quality to  or to . Indeed, in
several previous works (see the references in Muthukrishnan's book
\cite{Muthu}, or in Indyk's course notes \cite{IndykCourse}), the main
goal was to reduce the dependence on  in the space
and/or time complexity. This raises
the question, posed in the IITK workshop, as to whether this
dependence on  can be improved for -difference. As a first step, Cormode and
Ganguly \cite{CG07} show that if one increases the sketch size to
, then it is possible to achieve processing
time , thereby removing the dependence on
. Their algorithm even works for -norm estimation of
general data streams, and not just for -difference.
However, for reasonable values of , the sketch size is
dominated by the  term, which may be prohibitive in
practice, and is sub-optimal.

In this paper we show how to achieve a near-optimal  sketch size, while simultaneously achieving
a near linear  processing time, independent of . Notice our
space is only a
factor of  more than the lower bound. The time for a third
party to recover a -approximation to the -difference,
given the sketches,
is nearly linear in the sketch size.
Furthermore, our
sketching procedure naturally can be implemented as a one-pass
streaming algorithm over an adversarial ordering of the coordinates of
 (this was also true of previous algorithms). 
Thus, up to small
factors, we
resolve the first open question of \cite{IITK}.
We henceforth
describe our sketching procedure as a streaming algorithm.

While in \cite{CG07} and \cite{IITK} it is suggested to use the
techniques of \cite{bgks06} and \cite{IW} for estimating  norms,
, which are themselves based on estimating coordinates of heavy
weight
individually and removing them, we do not follow this
approach. Moreover, the approach of \cite{FKSV02} is to embed
-difference into , then use the AMS sketch \cite{AMS99}
and range-summable hash functions they design
to reduce the processing time. We do not follow this approach either.

Instead, our first idea is to embed the -difference problem into
, the number of non-zero coordinates of the underlying vector 
(in this case ) presented as data stream. Such an
embedding has been used before, for example, in lower bounding the
space complexity of estimating  in a data stream
\cite{IW03}.  Suppose for simplicity  for all . Here the idea is for Alice to treat her input  as a
set of distinct items , while Bob
treats his input  as a set of distinct items . Then the size of the set-difference of these
two sets is . Thus, if Alice inserts all of the set
elements corresponding to her coordinates as insertions into an
-algorithm, while Bob inserts all of his elements as deletions,
the -value
in the resulting stream equals . A recent
space-efficient algorithm for estimating  with deletions
is given in \cite{KNW08}.

The problem with directly reducing to  is that, while the
resulting space complexity is small, the processing time can be as
large as  since we
must insert each set element into the -algorithm. We overcome
this by developing a range-efficient  algorithm, i.e. an
algorithm which allows updates to ranges at a time,
which works for streams coming out of our reduction by exploiting the
structure of ranges we update (all updated ranges
are of length at most  and start at an index of the form ).
We note that range-efficient  algorithms have been
developed before \cite{BKS02,PavanTir07}, but those algorithms do
not allow deletions and thus do not suffice for our purposes.

At a high level, our algorithm works by sub-sampling by powers of 
the universe  arising out of our reduction to . At each
level we keep a data structure of size
 to summarize the items that are
sub-sampled at that level. We also maintain a
data structure on the side to handle the case when  is
small, and we in parallel obtain a constant-factor approximation 
of the -difference using \cite{FKSV02}. At the stream's end, we
give our estimate of the -difference based on the summary data
structure living at the level where the expected number of universe
elements sub-sampled is  (we can
determine this level knowing ).
As is the case in many previous streaming algorithms, the sub-sampling
of the stream can be
implemented using pairwise-independent hash functions. This allows us
to use a subroutine developed by Pavan and Tirthapura
\cite{PavanTir07} for quickly counting the number of universe elements
that
are sub-sampled at each of the  levels. Given these counts,
our summary data structures are such that we can update each one
efficiently.

Our summary data structure at a given level maintains ,
where  is the parity-check matrix of
a linear error-correcting code, and  are the vectors derived
from  by sub-sampling at that level. When promised that
 differ on few coordinates, we can treat  as
a corruption of the encoding of the  codeword then attempt to
decode to recover the ``error'' . The decoding succeeds as long
as the minimum distance of the code is sufficiently high. This idea of
using error-correcting codes to
sketch vectors whose distance is promised to be small is known in the
error-correcting codes literature as {\em
  syndrome decoding} \cite{Wyner74}. Aside from error-correction,
syndrome decoding has also
found uses in cryptography: in the work of
\cite{BBR88,Smith07} to give a two-party protocol
for agreeing on a shared secret key when communicating over a noisy
channel, and in the work of \cite{FIMNSW06} as part of a private
two-party communication protocol for computing the Hamming distance
between two bitstrings.

For efficiency reasons, our implementation of the summary data
structure is mostly inspired by work of Feigenbaum {\it et al.}
\cite{FIMNSW06}. Given that
 differ on at most  coordinates, they use the parity-check
matrix of a Reed-Solomon code of minimum distance .  Decoding
can then be done in time  using
an algorithm of Dodis {\it et al.} \cite{DORS08}. In our
application , and thus this recovery procedure is too slow for our
purposes.  To remedy this, we first hash the indices of 
into  buckets with an
-wise independent hash function, then in each bucket
we keep the product of the difference vector, restricted to the
indices mapped to that bucket, with the parity check matrix.
With constant probability, no bucket receives more than
 indices where  differ.
We can thus use a Reed-Solomon code
with minimum distance only , making the algorithm of
\cite{DORS08} fast enough for our purposes.

We note that our summary data structure in each level is in fact a
-set structure, as defined by Ganguly \cite{Ganguly07}, that can be
used
to return a set of  items undergoing insertions and deletions in a
data stream. While Ganguly's -set structure uses near-optimal space
and has fast update time, it only works in the strict turnstile model
(i.e., it requires that each coordinate of  is non-negative,
in which case the -difference problem has a trivial solution:
maintain an -bit counter). This is due to the algorithm's
reliance on the identity , for which there is no analogue
outside the strict turnstile setting. Using certain modifications
inspired by the Count-Min sketch \cite{cm05} it may be possible
to implement his algorithm in the turnstile model, though the resulting
space and time would be sub-optimal. In a different work, Ganguly
and Majumder
\cite{GM06} design a deterministic -set structure based on
Vandermonde matrices, but the space required of this structure is
sub-optimal.

Our fast sketching algorithm for -difference improves the running
time of an algorithm of Jayram and the second author \cite{jw08} 
by a  factor for
estimating  of a matrix , defined as the
sum of Euclidean lengths of the rows of . As -difference is a basic
primitive, we believe our algorithm is likely to have many further
applications.

\section{Preliminaries}\SectionName{prelim}
All space bounds mentioned throughout this paper are in bits, and all
logarithms are base ,  unless
explicitly stated otherwise. Running times are measured as the number
of standard machine word operations (integer arithmetic, bitwise
operations, and bitshifts). Each machine word is assumed to be
 bits so that we can index each vector and do
arithmetic on vector entries in constant time. Also, for integer ,
 denotes the set .

We now formally define the model in which our sketching procedure
runs. Alice receives , and Bob receives
. Both parties have access to a shared source of
randomness and must, respectively, output bit-strings  and
.
The requirement is that a third party can, given access to only 
and , compute a value  such that  (recall ). The probability is over the randomness shared by Alice and
Bob, and the value  is a parameter given to all
parties. The goal is to minimize the lengths of  and , as
well
as the amount of time Alice and Bob each take to compute them.
Without loss of generality, throughout this
document we assume  for all .  This promise can be
enforced by increasing all coordinates of  by , which
does not alter .  Doing so increases the upper bound on
coordinate entries by a factor of two, but this alters our algorithm's
running time and resulting sketch size
by subconstant factors.

Since we present our sketching algorithm as a streaming algorithm
in \Section{body}, we now introduce some streaming notation.
We consider a vector  that is updated in a
stream as follows. The stream has
exactly  updates . Each update  corresponds to the
action .
For each , there are exactly two
stream updates  with .  If these two stream updates are
, then at most
one of  is negative, and at most one is positive. The
nonnegative update corresponds to adding  to , and the
nonpositive update corresponds to subtracting  from  (recall
we assumed ). We make no restriction on the
possible values for  and . That is, our algorithm
functions correctly even if the stream
presents us with an adversarial permutation of the  coordinates
. At the end of the stream , so our streaming algorithm must
approximate . For Alice and Bob to use our streaming
algorithm for sketching, Alice runs the algorithm with updates
 for each , and Bob separately runs the
algorithm (using the same random bits) with updates . The
sketches they produce are simply the contents of the algorithm's
memory at the end of the stream.  It is a consequence of how our
algorithm works that these sketches can be
efficiently combined by a third party to approximate .

\section{Main Streaming Algorithm}\SectionName{body}
Throughout this section we assume .  Otherwise, we
can compute  exactly by keeping the entire vector in memory
using  space with constant update
time.

\subsection{Handling Small }\SectionName{kset}
We give a subroutine \textsc{TwoLeveLEstimator}, described in
\Figure{two-level}, to
compute  exactly when promised that  for some parameter
. We assume that integers polynomially large in 
fit in a machine word, which will be true in our use of this
subroutine later.

In \Figure{two-level}, we assume we have
already calculated a prime  satisfying

(the choice of  will be justified later), along with a
generator  for the multiplicative group .  We also
precalculate logarithm tables  such that 
and , where  and . 
Here
 is the discrete logarithm of  (i.e. the 
such that ).

The subroutine \textsc{TwoLevelEstimator} makes calls to the following
algorithm given in \cite{DORS08}.

\begin{theorem}[Dodis {\it et al.} {\cite[Lemma
    E.1]{DORS08}}]\TheoremName{dodis}
Let  be prime and  have at most 
non-zero
entries (). Given 
for , there is an algorithm to recover
 which uses
 field operations over
.\afterproof
\end{theorem}

\begin{figure*}
\begin{center}
\fbox{
\parbox{6.375in} {
\underline{Subroutine \textsc{TwoLevelEstimator}}:\\

\texttt{\hspace{.1in} // Compute  exactly when promised
  . The value  is as in \Equation{cdef}.}
\begin{enumerate}
\addtolength{\itemsep}{-1mm}
\item Set  and .
  Pick a random  from a -wise independent hash family 
  and a random  from a
  pairwise independent family.
\item For each 
  maintain  counters  modulo
  , initialized to .
 \item Upon seeing stream update , increment  by
   for .
\item At the stream's end, for each , attempt
  to recover the non-zero entries of an
  -sparse vector 
  satisfying  for
  each  using \Theorem{dodis}.
\item Define  to be
  such that  equals  if , and
  equals  otherwise.\\ Output 
  .
\end{enumerate}
}}
\end{center}
\caption{\textsc{TwoLevelEstimator} subroutine pseudocode}\FigureName{two-level}
\end{figure*}

The proof of correctness of \textsc{TwoLevelEstimator} relies in part
on the following lemma.

\begin{lemma}[Bellare and Rompel {\cite[Lemma
    2.3]{BR94}}]\LemmaName{br94}
Let , , be -wise independent for 
an even integer,
, and .  Then . \afterproof
\end{lemma}

\begin{theorem}\TheoremName{two-level}
Ignoring the space to store the hash functions  and tables
, the algorithm \textsc{TwoLevelEstimator}
uses  bits of space.  The hash functions  and
tables  require an additional  bits.  The time to
process a stream update is .  If , the final
output value of \textsc{TwoLevelEstimator} 
equals  exactly with probability at least .
\end{theorem}
\begin{proof}
Aside from storing ,  the number of counters is
, each of size  bits,
totaling  bits.
The space to store  is ,
and the space to store  is  \cite{CW79}.  The
tables  each have  entries, each
requiring  bits. Processing a stream update
requires evaluating , taking  time and 
time, respectively \cite{CW79}. 

As for update time, each stream token requires updating 
counters (Step 3). Each counter update can be done in constant time
with the help of table lookup since .

We now analyze correctness.  Define .
Note .  For
, define the random variable .  We now define two events.

Let  be the event that 
for all .

Let  be the event that there do not exist distinct
 with both  and .

We first argue that, conditioned on both 
holding, the output of \textsc{TwoLevelEstimator} is
correct. Note  (recall the
definition of  in Step 1 of \Figure{two-level}). If 
occurs,
 for all  and
.  One can then view  as holding
, where  is the frequency
(modulo ) of the unique
element in the set  (or  if that
set is empty).  Conditioned on , every  is
-sparse, so we correctly recover  in Step 4 by
\Theorem{dodis} since .  Note that  is
strictly greater than twice the absolute value of the largest
frequency since , and thus negative frequencies
are strictly above  in , and positive frequencies are
strictly below . Thus, given that the 
are correctly recovered,  correctly recovers the
actual frequencies in Step 5, implying correctness of the final
output.

Now we proceed to lower bound . First
we show  occurs with probability at least .
If we let  indicate , then note the random
variables  are -wise independent and . Also, .  
Noting 

then setting  and applying \Lemma{br94},

since .  A union bound implies .

Now we analyze .  Let  be a
random variable indicating  and define the random
variable . Note
 is simply the event that .  We have

where the expectation on the right side of the first equality is
over the random choice of
, and the probability is over the random choice of .  The
first inequality holds by pairwise independence of .
Conditioned on ,  for all
 so that , implying
 by Markov's Inequality.

In total, we have , and the claim is proven.
\end{proof}

\begin{remark}\RemarkName{hash-stuff}
In Step 1 of \Figure{two-level}, we twice pick a hash function
 from an -wise independent family for
some integers  and  (namely, when picking  and
). However, known
constructions \cite{CW79} have , with  a prime power.  This is
easily circumvented.
When we desire an  with unequal domain size  and range size ,
we can pick a prime  then pick an -wise independent hash function
 and define . The family of such  is still -wise independent, and by
choice of , no range value is more than twice more likely than any
other, which suffices for our application with a slight worsening of
constant factors.
\end{remark}

The following theorem analyzes the pre-processing and post-processing
complexity of \textsc{TwoLevelEstimator}.

\begin{theorem}\TheoremName{tle-preprocess}
Ignoring the time needed to find the prime  in \Remark{hash-stuff},
the pre-processing time of \textsc{TwoLevelEstimator} before
seeing the stream is , and the post-processing time is
.
\end{theorem}
\begin{proof}
We first discuss the pre-processing time.  It is known that the prime
 and generator  for  can be found in time
 (see the proof of Theorem 4 in
\cite{BM84}). Once we have , filling in
 takes  time, which dominates the
pre-processing time.  The time to allocate the
 counters  is just .

The post-processing work is done in Steps 4
and 5 in \Figure{two-level}. For Step 4, there are 
values of , for each of which we run the algorithm of
\Theorem{dodis} with  and , thus
requiring a total of  field
operations over .  Since we precalculate the table , we
can do all  operations in constant time, including
division. In
Step 5 we need to sum the absolute values of  non-zero
entries of  vectors , taking time .
\end{proof}

\begin{remark}
Our subroutine \textsc{TwoLevelEstimator} uses the fact that since
 and  is an integer vector, it must be the case that .  From here, what we develop is a {\em -set structure} as
defined by Ganguly \cite{Ganguly07}, which is a data structure
that allows one to recover the -sparse vector .  In fact, any
-set structure operating in the turnstile model (i.e., where some
 can be negative) would have sufficed in place of
\textsc{TwoLevelEstimator}.  We develop our
particular subroutine since previous approaches were either less
space-efficient or did not work in the turnstile setting
\cite{GM06,Ganguly07}.  We remark that at the cost of an
extra  factor in space, but with the benefit of only
 post-processing time, one can replace
\textsc{TwoLevelEstimator} with an alternative scheme.  Namely, for each
, attempt to perfectly the hash the  coordinates contributing to  mapped to  under  by
pairwise independently hashing into
 counters, succeeding with
constant probability.  Each counter holds frequency sums modulo .
By repeating 
times and taking the maximum sum of counter absolute values over any
of the  trials, we succeed in finding the sum of frequency absolute
values of items mapping to  under  with probability . Thus
by a union bound, we recover  with probability  by
summing up over all .  The
estimate of  can be maintained on the fly during updates to
give  post-processing, and
updates still take only  time.
\end{remark}

\subsection{The Full Algorithm}\SectionName{full-stream}
Our full algorithm requires, in part, a constant factor approximation
to the -difference.  To obtain this, we can use the algorithm of
Feigenbaum {\it et al.} \cite{FKSV02} with  a constant.

\begin{theorem}[Feigenbaum {\it et al.} {\cite[Theorem
    12]{FKSV02}}]\TheoremName{feig}
There is a one-pass streaming algorithm for -approximating
the -difference using 
space with update time , and succeeding with
probability at least .\afterproof
\end{theorem}

\begin{remark}
It is stated in \cite{FKSV02} that the update time
in \Theorem{feig} is ,
where  is the time to do arithmetic over
 (not including division).  Section 2.2 of
\cite{FKSV02} points out that
 na\"{i}vely. In fact, it suffices for the
purposes of their algorithm to work over  for the smallest
 such that , in which case a highly
explicit irreducible polynomial of degree  over 
(namely  \cite[Theorem 1.1.28]{vanLint99})
can be used to perform  arithmetic in time
 in the word RAM model without any additional pre-processing
space or time.
\end{remark}

We also make use of the following algorithm due to Pavan and
Tirthapura \cite{PavanTir07}.

\begin{theorem}[Pavan and Tirthapura {\cite[Theorem
    2]{PavanTir07}}]\TheoremName{pavan}
Let  be integers fitting in a machine word with 
and .  There is an algorithm to
calculate  in
time  using 
space.\afterproof
\end{theorem}

\begin{figure*}
\begin{center}
\fbox{
\parbox{6.375in} {
\underline{Main Algorithm \textsc{L1-Diff}}:
\begin{enumerate}
\addtolength{\itemsep}{-1mm}
\item Set .
\item Pick a random hash function  from a
  pairwise independent family so that  for some
  prime  and .
\item Initialize instantiations  of \textsc{TwoLevelEstimator} with .
  All instantiations share the same prime , generator , hash
  functions
  , and logarithm tables .
\item Upon seeing stream update , let  be the output of
  the algorithm from \Theorem{pavan} with inputs  as in Step 2,
  , ,
  , , and .  Feed the update
   to  for
  . Let  be the output of
  .
\item Run an instantiation  of \textsc{TwoLevelEstimator} in
  parallel with 
  which receives all updates, using the same  of
  Step 2. Let its output be .
\item Run the algorithm of \Theorem{feig} in parallel with error
  parameter  to obtain a value .
\item If , output . Otherwise, output
  .
\end{enumerate}
}}
\end{center}
\caption{\textsc{L1-Diff} pseudocode}\FigureName{fullalg}
\end{figure*}

Our main algorithm, which we call \textsc{L1-Diff}, is described in
\Figure{fullalg}.  Both in \Figure{fullalg} and in the proof of
\Theorem{main-alg},  denotes the function which takes as
input a real number  and outputs  if  is negative, and 
otherwise.

\begin{theorem}\TheoremName{main-alg}
The algorithm \textsc{L1-Diff} has update time  and the space used is
.
Pre-processing requires
 time.
Time  is
needed for post-processing.
The output is  with probability at least .
\end{theorem}
\begin{proof}
The hash function  requires  space.  There are
 instantiations of \textsc{TwoLevelEstimator}
(Steps 2 and 4), each with , taking a total of
 space by
\Theorem{two-level}. The hash functions  and tables 
take  space, also by \Theorem{two-level}
(recall we assume ). Step 6 requires only
 space by \Theorem{feig}, since the algorithm is run
with error parameter .

As for running time, in Step 3 we call the algorithm of
\Theorem{pavan}  times, each time with  and
, thus taking a total of  time. We must also feed the necessary
update to each , each time taking  time by
\Theorem{two-level}.  Updating every  thus takes time
.

In pre-processing we need to pick a prime  in the desired range,
which can be accomplished by picking numbers at random and testing
primality; the expected time is . We also need to
prepare  and all the \textsc{TwoLevelEstimator}
instantiations, which takes 
time by \Theorem{tle-preprocess}, in addition to the 
time required to find an appropriate prime  as described in
\Remark{hash-stuff}. The pre-processing time for Step 6
is  (see Figure 1 of \cite{FKSV02}).

In post-processing we need to recover the estimate  from Step 6,
which takes  time, then recover an estimate from some
\textsc{TwoLevelEstimator} instantiation, so the time is as
claimed. In post-processing, to save time one should not run
Steps 4 and 5 of \textsc{TwoLevelEstimator} in \Figure{two-level}
except at the instantiation whose output is used in Step 7.

Now we analyze correctness. Let  be the event that
. We proceed by a case analysis.

For the first
case, suppose .  Then, 
computes  exactly with probability at least  by
\Theorem{two-level}, and hence overall we output  exactly with
probability at least .

Now, suppose . In analyzing this case, it helps
to view \textsc{L1-Diff} as actually computing
, where we consider an
-dimensional vector  that is being updated as follows: when
receiving an update  in the stream, we conceptually view this
update as being  updates
 to
the vector . Here, the vector  is initialized to
. Note that at the stream's end, .

Let  denote the vector whose th entry, , is 
if  and  otherwise.  That is,  receives
stream updates
only from items fed to . For , let  be a
random variable indicating , and let  so that . Define  so that . Thus, . 
Note that .  Conditioned on
, we have the inequalities

and

By
the choice of  in
Step 7 of \Figure{fullalg}, we thus have, assuming 
occurs,

since .

Let  be the event that .  Applying Chebyshev's inequality,

The second inequality holds since  is pairwise independent and 
is the sum of Bernoulli random variables, implying . The last inequality
holds by choice of .

Let  be the event that  outputs 
correctly. Now, conditioned on
, we have  since .  Thus by \Theorem{two-level},
. Overall,
we compute
 of the entire stream correctly with probability at least

\end{proof}

Our streaming algorithm also gives a sketching procedure. This is
because, as long as Alice and Bob share randomness, they can generate
the same  then separately apply the streaming
algorithm to their vectors . The sketch is then just the state of
the
streaming algorithm's data structures.  Since each stream token causes
only linear updates to counters, a third party
can then take the counters from Bob's sketch and subtract them from
Alice's, then do post-processing to recover the estimation of the
-difference. The running time for Alice and Bob to produce their
sketches is the streaming algorithm's pre-processing time, plus 
times the update time. The time for the third party to obtain an
approximation to  is the time required to combine the
sketches, plus the post-processing time.  We thus
have the following theorem.

\begin{theorem}\TheoremName{sketching}
Sharing  randomness, two parties Alice and Bob,
holding vectors , respectively, can produce
-bit sketches 
such that a third party can recover  to within 
with probability at
least  given only .  Each of Alice and Bob use
time  to produce their sketches. In
 time,
the third party can recover  to within a multiplicative
factor of . \afterproof
\end{theorem}

Note Alice and Bob's running time is
always  since .

\begin{remark}
Though we assume Alice and Bob share randomness, to actually implement
our algorithm in practice this randomness must be communicated at some
point.
We note that while the sketch length guaranteed by \Theorem{sketching}
is  bits, the required amount
of shared randomness is , which for large enough 
is larger than the sketch length.  This is easily fixed
though. Since the required randomness is only polynomially larger than
the space used by the sketching algorithm (which is asymptotically equal
to the sketch length), the two parties can use the Nisan-Zuckerman
pseudorandom
generator \cite{NisanZu96} to stretch a seed whose length is linear
  in the sketch length to a pseudorandom string of length
 which still provides the guarantees of
\Theorem{sketching}.  Alice and Bob then only need to communicate this
random seed.
\end{remark}

\section*{Acknowledgments}
We thank Avinatan Hassidim, Piotr Indyk, Yuval Ishai, and Swastik
Kopparty for useful comments and discussions, and Milan Ru\v{z}i\'{c}
for pointing out a tweak to
an early version of our work which improved our space by a factor of
. We thank Venkat Chandar for pointing out the
reference \cite{Wyner74}, and Silvio Micali for pointing out
\cite{BM84}.

\bibliographystyle{plain}

\bibliography{./allpapers}

\end{document}
