

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{graphicx}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multirow}
\usepackage{rotating}
\DeclareMathOperator*{\argmax}{arg\,max}
\def \hfillx {\hspace*{-\textwidth} \hfill}

\aclfinalcopy \def\aclpaperid{625} 



\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems}

\author{Andrea Madotto\thanks{ These two authors contributed equally.} , Chien-Sheng Wu, Pascale Fung\\
  Human Language Technology Center \\
  Center for Artificial Intelligence Research (CAiRE) \\
  Department of Electronic and Computer Engineering \\
  The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong \\
  {\tt [eeandreamad,cwuak,pascale]@ust.hk} }

\date{}

\begin{document}
\maketitle
\begin{abstract}
End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. 
In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.
\end{abstract}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{Mem2seq.pdf}
\setlength{\abovecaptionskip}{-10pt} 
\caption{The proposed Mem2Seq architecture for task-oriented dialog systems. (a) Memory encoder with 3 hops; (b) Memory decoder over 2 step generation.}
\label{FIG:MODEL}
\end{figure*}

\section{Introduction}

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Point of interest (poi)} & \textbf{Distance} & \textbf{Traffic info} & \textbf{Poi type} & \textbf{Address} \\ \hline
 The Westin & 5 miles           & moderate traffic       & rest stop           & 329 El Camino Real              \\ \hline
Round Table  & 4 miles           & no traffic             & pizza restaurant    & 113 Anton Ct                 \\ \hline
Mandarin Roots & 5 miles           & no traffic             & chinese restaurant  & 271 Springer Street        \\ \hline
Palo Alto Cafe & 4 miles           & moderate traffic       & coffee or tea place & 436 Alger Dr                \\ \hline
Dominos & 6 miles           & heavy traffic          & pizza restaurant    & 776 Arastradero Rd                 \\ \hline
Stanford Express Care & 6 miles           & no traffic             & hospital            & 214 El Camino Real   \\ \hline
Hotel Keen & 2 miles           & heavy traffic          & rest stop           & 578 Arbol Dr                    \\ \hline
\end{tabular}
}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|rl}
\hline
\multirow{6}{*}{\begin{sideways}\textit{1th Turn}\end{sideways}} & \textbf{DRIVER} & Where can I get tea? \\ \cline{2-3} 
 & \textit{Seq2Seq} & I have a away from away would you like the address \\ 
 & \textit{+Attn} & I have a listing for a place that serves tea that is 5 miles away \\ 
 & \textit{Ptr-Unk} & There is a away you like would you like more info \\ \cline{2-3} 
 & \textit{\textbf{Mem2Seq}} & \begin{tabular}[c]{@{}l@{}}The nearest tea shop is Palo Alto Cafe located 4 miles away, \\ would you like directions there?\end{tabular} \\ \cline{2-3} 
 & \textbf{GOLD} & \begin{tabular}[c]{@{}l@{}}Palo Alto Cafe is 4 miles away and serves coffee and tea. \\ Do you want the address?\end{tabular} \\ \hline
\multirow{6}{*}{\begin{sideways}\textit{2th Turn}\end{sideways} } & \textbf{DRIVER} & Yes. \\ \cline{2-3} 
 & \textit{Seq2Seq} & The is at at \\ 
 & \textit{+Attn} & \begin{tabular}[c]{@{}l@{}}The address is 329 El Camino Real and it's 3 miles away \\ and there is no traffic\end{tabular} \\ 
 & \textit{Ptr-Unk} & Palo Alto Cafe is 4 miles away and PAD is no traffic \\ \cline{2-3} 
 & \textit{\textbf{Mem2Seq}} & Palo Alto Cafe is 4 miles away at 436 Alger Drive \\ \cline{2-3} 
 & \textbf{GOLD} & Palo Alto is located at 436 Alger Dr. \\ \hline
\end{tabular}
}
\setlength{\abovecaptionskip}{-3pt} 
\caption{Example of generated responses for the In-Car Assistant on the navigation domain.}
\label{EXAMPL_DIALOG_1}
\end{table}

Task-oriented dialog systems help users to achieve specific goals with natural language such as restaurant reservation and schedule arrangement. Traditionally, they have been built with several pipelined modules: language understanding, dialog management, knowledge query, and language generation~\cite{williams2007partially,hori2009statistical,lee2009example, levin2000stochastic,young2013pomdp}. Moreover, the ability to query external Knowledge Bases (KBs) is essential in task-oriented dialog systems, since the responses are guided not only by the dialog history but also by the query results (e.g. Table~\ref{EXAMPL_DIALOG_1}). However, despite the stability of such pipelined systems via combining domain-specific knowledge and slot-filling techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort.

Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results~\cite{serban2016building,wen2016network,zhao2017generative}. Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism~\cite{gulcehre-EtAl:2016:P16-1,eric-manning:2017:EACLshort} have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities.


However, although the above mentioned approaches were successful, they still suffer from two main problems: 
1) They struggle to effectively incorporate external KB information into the RNN hidden states~\cite{sukhbaatar2015end}, since RNNs are known to be unstable over long sequences.
2) Processing long sequences is very time-consuming, especially when using attention mechanisms.

On the other hand, end-to-end memory networks (MemNNs) are recurrent attention models over a possibly large external memory~\cite{sukhbaatar2015end}. They write external memories into several embedding matrices, and use query vectors to read memories repeatedly. This approach can memorize external KB information and rapidly encode long dialog history. Moreover, the multi-hop mechanism of MemNN has empirically shown to be essential in achieving high performance on reasoning tasks~\cite{bordes2016learning}. Nevertheless, MemNN simply chooses its responses from a predefined candidate pool rather than generating word-by-word. In addition, the memory queries need explicit design rather than being learned, and the copy mechanism is absent.

To address these problems, we present a novel architecture that we call Memory-to-Sequence (Mem2Seq) to learn task-oriented dialogs in an end-to-end manner. In short, our model augments the existing MemNN framework with a sequential generative architecture, using global multi-hop attention mechanisms to copy words directly from dialog history or KBs. We summarize our main contributions as such: 
1) Mem2Seq is the first model to combine multi-hop attention mechanisms with the idea of pointer networks, which allows us to effectively incorporate KB information. 
2) Mem2Seq learns how to generate dynamic queries to control the memory access. In addition, we visualize and interpret the model dynamics among hops for both the memory controller and the attention.
3) Mem2Seq can be trained faster and achieve state-of-the-art results in several task-oriented dialog datasets.

\section{Model Description}
Mem2Seq~\footnote{The code is available at \url{https://github.com/HLTCHKUST/Mem2Seq} } is composed of two components: the MemNN encoder, and the memory decoder as shown in Figure~\ref{FIG:MODEL}.
The MemNN encoder creates a vector representation of the dialog history. Then the memory decoder reads and copies the memory to generate a response. 
We define all the words in the dialog history as a sequence of tokens \}\
{ptr}_i = 
\begin{cases} 
max(z) &\mbox{if } \exists z \ \text{s.t.} \ y_i = u_z \\ 
n+l+1&\mbox{otherwise} 
\end{cases} 
\label{eq1}

  p^k_i = \text{Softmax}((q^k)^T C^k_i),
  \label{eq2}

  o^k = \sum_i p^k_i C^{k+1}_i.
  \label{eq3}

h_t = \text{GRU}(C^1(\hat{y}_{t-1}), h_{t-1});
\label{eq4}

P_{vocab}(\hat{y_t}) = \text{Softmax}(W_1[h_t;o^1]) 
\label{eq5}
, as shown in Equation~\ref{eq1}. Once the sentinel is chosen, our model generates the token from , otherwise, it takes the memory content using the  distribution. Basically, the sentinel token is used as a hard gate to control which distribution to use at each time step. A similar approach has been used in~\cite{merity2016pointer} to control a soft gate in a language modeling task. With this method, the model does not need to learn a gating function separately as in~\citet{gulcehre-EtAl:2016:P16-1}, and is not constrained by a soft gate function as in~\citet{see-liu-manning:2017:Long}. 

\subsection{Memory Content}
We store word-level content  in the memory module. Similar to ~\citet{bordes2016learning}, we add temporal information and speaker information in each token of  to capture the sequential dependencies. For example, ``hello  uBP_{ptr}[1e^{-3},1e^{-4}]K= 1,3, 6[64,512][0.1, 0.4]P_{ptr}P_{vocab}h_ty_tP_{vocab}P_{ptr}$. One can find that in (a) hop 1, there is no clear separation of two different colors but each of which tends to group together. On the other hand, the separation becomes clearer in (b) hop 6 as each color clusters into several groups such as location, cuisine, and number. Our model tends to retrieve more information in the first hop, and points into the memories in the last hop.

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{\textit{}
\begin{tabular}{|c|c|c|c|c|c}
\hline
\textbf{room} & \textbf{time} & \textbf{date} & \textbf{party} & \textbf{event} \\ \hline
conference room 100 & 3pm & Thursday & sales team & conference \\ \hline
 - & 11am & Tuesday & - & tennis activity \\ \hline
- & 7pm & Wednesday & - & dentist appointment \\ \hline
conference room 100 &  1pm & Tuesday & HR & meeting \\ \hline
- & 7pm & Friday & - & doctor appointment \\ \hline
- &  11am & Tuesday & - & lab appointment \\ \hline
\end{tabular}
}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|l}
\hline
\textbf{DRIVER}      & When's my tennis match?                    \\ \hline
\textit{Seq2seq}     & Your lab appointment is on Monday at       \\
\textit{+Att}        & Your lab appointment is on Tuesday at 11am \\
\textit{Ptr-Unk}      & Your tennis match is on PAD at 1pm         \\ \hline
\textit{Mem2Seq H1}  & Your is very welcome                       \\
\textit{Mem2Seq H3}  & Your tennis is on Tuesday at 11am   \\
\textit{Mem2Seq H6}  & Your tennis is Monday at 11am              \\\hline
\textbf{GOLD}        & Your tennis match is Tuesday at 11am      \\  \hline
\end{tabular}}
\setlength{\abovecaptionskip}{-3pt} 
\caption{Example of generated responses for the In-Car Assistant on the scheduling domain.}
\label{EXAMPL_DIALOG_2}
\end{table}

\noindent\textbf{Examples:} Table~\ref{EXAMPL_DIALOG_1} and \ref{EXAMPL_DIALOG_2} show the generated responses of different models in the two test set samples from the In-Car Assistant dataset. We report examples from this dataset since their answers are more human-like and not as structured and repetitive as others. 
Seq2Seq generally cannot produce related information, and sometimes fail in language modeling. Instead, using attention helps with this issue, but it still rarely produces the correct entities. For example, Seq2Seq with attention generated 5 miles in Table~\ref{EXAMPL_DIALOG_1} but the correct one is 4 miles. In addition, Ptr-Unk often cannot copy the correct token from the input, as shown by ``PAD'' in Table~\ref{EXAMPL_DIALOG_1}. On the other hand, Mem2Seq is able to produce the correct responses in this two examples. In particular in the navigation domain, shown in Table~\ref{EXAMPL_DIALOG_1}, Mem2Seq produces a different but still correct utterance. We report further examples from all the domains in the supplementary material. 

\noindent\textbf{Discussions:} Conventional task-oriented dialog systems~\cite{williams2007partially}, which are still widely used in commercial systems, require a multitude of human efforts in system designing and data collection. On the other hand, although end-to-end dialog systems are not perfect yet, they require much less human interference, especially in the dataset construction, as raw conversational text and KB information can be used directly without the need of heavy preprocessing (e.g. NER, dependency parsing). To this extent, Mem2Seq is a simple generative model that is able to incorporate KB information with promising generalization ability. We also discovered that the entity F1 score may be a more comprehensive evaluation metric than per-response accuracy or BLEU score, as humans can normally choose the right entities but have very diversified responses. Indeed, we want to highlight that humans may have a low BLEU score despite their correctness because there may not be a large n-gram overlap between the given response and the expected one. However, this does not imply that there is no correlation between BLEU score and human evaluation. In fact, unlike chat-bots and open domain dialogs where BLEU score does not correlate with human evaluation~\cite{liu-EtAl:2016:EMNLP20163}, in task-oriented dialogs the answers are constrained to particular entities and recurrent patterns. Thus, we believe BLEU score still can be considered as a relevant measure. In future works, several methods could be applied (e.g. Reinforcement Learning~\cite{ranzato2015sequence}, Beam Search~\cite{wiseman-rush:2016:EMNLP2016}) to improve both responses relevance and entity F1 score. However, we preferred to keep our model as simple as possible in order to show that it works well even without advanced training methods.

\section{Related Works}
End-to-end task-oriented dialog systems train a single model directly on text transcripts of dialogs~\cite{wen2016network,serban2016building,williams2017hybrid,zhao2017generative, seo2016query,serban2017hierarchical}. Here, RNNs play an important role due to their ability to create a latent representation, avoiding the need for artificial state labels. End-to-End Memory Networks~\cite{bordes2016learning,sukhbaatar2015end}, and its variants~\cite{perez2016gated,wu2017dstc6,dqmem} have also shown good results in such tasks. In each of these architectures, the output is produced by generating a sequence of tokens, or by selecting a set of predefined utterances.

Sequence-to-sequence (Seq2Seq) models have also been used in task-oriented dialog systems~\cite{zhao2017generative}. These architectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models~\cite{luong-pham-manning:2015:EMNLP,bahdanau2014neural}, Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models~\citet{eric-manning:2017:EACLshort}, were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks~\cite{Dehghani:2017:LAC:3132847.3133010,he-EtAl:2017:Long1}, neural machine translation~\cite{gulcehre-EtAl:2016:P16-1,gu-EtAl:2016:P16-1}, language modeling~\cite{merity2016pointer}, and summarization~\cite{see-liu-manning:2017:Long}. 

Less related to dialog systems, but related to our work, are the memory based decoders and the non-recurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN)~\cite{graves2014neural,graves2016hybrid}. Similarly, memory encoders have been used in neural machine translation~\cite{wang-EtAl:2016:EMNLP20161}, and meta-learning application~\cite{DBLP:journals/corr/KaiserNRB17}. However, Mem2Seq differs from these models as such: it uses multi-hop attention in combination with copy mechanism, whereas other models use a single matrix representation. 2) non-recurrent generative models~\cite{vaswani2017attention}, which only rely on self-attention mechanism, are related to the multi-hop attention mechanism used in MemNN.

\section{Conclusion}
In this work, we present an end-to-end trainable Memory-to-Sequence model for task-oriented dialog systems. Mem2Seq combines the multi-hop attention mechanism in end-to-end memory networks with the idea of pointer networks to incorporate external information. We empirically show our model's ability to produce relevant answers using both the external KB information and the predefined vocabulary, and visualize how the multi-hop attention mechanisms help in learning correlations between memories. Mem2Seq is fast, general, and able to achieve state-of-the-art results in three different datasets.

\section*{Acknowledgments}
This work is partially funded by ITS/319/16FP of Innovation Technology Commission, HKUST 16214415 \& 16248016 of Hong Kong Research Grants Council, and RDC 1718050-0 of
EMOS.AI.


\bibliography{acl2018}
\bibliographystyle{acl_natbib}

\onecolumn

\section{Tables}
\subsection{Time per Epoch}
\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|r|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textit{}} & \textbf{T1} & \textbf{T2} & \textbf{T3} & \textbf{T4} & \textbf{T5} & \textbf{DSTC2} & \textbf{In-Car} \\ \hline
\textit{Seq2Seq}                & 0.7         & 1.22        & 0.85        & 0.15        & 1.57        & 9.36           & 2.00            \\ \hline
\textit{+Attn}                  & 1.18        & 2.38        & 2.2         & 0.32        & 6.04        & 22.07          & 6.18            \\ \hline
\textit{Ptr-Unk}                & 1.28        & 2.5         & 2.21        & 0.38        & 6.10        & 20.39          & 6.48            \\ \hline
\textit{\textbf{Mem2Seq H1}}    & 0.73        & 1.10        & 0.55        & 0.15        & 1.05        & 1.49           & 0.68            \\ \hline
\textit{\textbf{Mem2Seq H3}}    & 0.97        & 1.48        & 0.77        & 0.21        & 1.33        & 2.36           & 1.02            \\ \hline
\textit{\textbf{Mem2Seq H6}}    & 1.23        & 2.14        & 1.11        & 0.33        & 2.20        & 4.22           & 1.43            \\ \hline
\end{tabular}}
\setlength{\abovecaptionskip}{-4pt} 
\caption{Minutes per-epoch. Mem2Seq is faster compared to others especially for longer inputs.}
\label{TB:TIME}
\end{table}

\subsection{Hyper-parameters}
\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|r|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{}  & \textbf{T1} & \textbf{T2} & \textbf{T3} & \textbf{T4} & \textbf{T5} & \textbf{DSTC2} & \textbf{In-Car}      & \textit{lr}             \\ \hline
\textit{Seq2Seq}        & 256 (0.1)   & 256 (0.1)   & 128(0.1)    & 256 (0.1)   & 256 (0.1)   & -              & 512 (0.3)            & \multirow{3}{*}{0.0001} \\ \cline{1-8}
\textit{+Attn}          & 256 (0.1)   & 256 (0.1)   & 256 (0.1)   & 256 (0.1)   & 128 (0.1)   & -              & 512 (0.0)            &                         \\ \cline{1-8}
\textit{Ptr-Unk}        & 256 (0.1)   & 256 (0.1)   & 256 (0.1)   & 256 (0.1)   & 128 (0.1)   & -              & 512 (0.0)            &                         \\ \hline
\textit{Mem2Seq GRU L1} & 256 (0.3)   & 256 (0.2)   & 128 (0.2)   & 256 (0.2)   & 256 (0.1)   & 256 (0.1)      & 256 (0.2)            & \multirow{3}{*}{0.001}  \\ \cline{1-8}
\textit{Mem2Seq GRU L3} & 256 (0.1)    & 256 (0.3)   & 128 (0.2)   & 256 (0.3)    & 128 (0.1)   & 128 (0.2)    & 256 (0.1)            &                         \\ \cline{1-8}
\textit{Mem2Seq GRU L6} & 256 (0.2)    & 128 (0.3)   & 256 (0.1)   & 128 (0.3)    & 256 (0.2)   & 256 (0.1)    & 256 (0.2)			  &                         \\ \hline
\end{tabular}}
\caption{Selected hyper-parameters in each datasets for different hops. The values is the embedding dimension and the GRU hidden size, and the values between parenthesis is the dropout rate. For all the models we used learning rate equal to 0.001, with a decay rate of 0.5. }
\end{table}

\section{Visualization}
\subsection{Multiple Hops Attention}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{MULTIHOP_T3.pdf}
\caption{3-hop memory attention visualization from bAbI dialog task 3. In the generation step 8, the attention at hop 1 is shallow and divided equally over the 5 possible choice, and becomes very sharp at hop 3.}
\label{3hop}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{MULTIHOP_T3_WRONG.pdf}
\caption{3-hop memory attention visualization from bAbI dialog task 3. The model tends to make mistake if the attention at last hop is not sharp.}
\label{3hopWRONG}
\end{figure}

\subsection{Last Hop Attention}
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{VIZ_MEMT1_NEW.pdf}\caption{Last hop memory attention visualization from the bAbI dataset. COR and GEN on the top are the correct response and our generated one.}
\label{MEMATT}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{VIZ_MEMTINCAR.pdf}\caption{Last hop memory attention visualization from the In-Car dataset. COR and GEN on the top are the correct response and our generated one.}
\label{MEMATT}
\end{figure}





\section{InCar Assistant Dataset Examples}



\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the weather domain.}
\label{APPENDINX_1}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{location}                & \textbf{monday}                      & \textbf{tuesday}                  & \textbf{wednesday}                    \\ \hline
grand rapids                     & hot, low of 50F, high of 70F         & raining, low of 60F, high of 80F  & rain, low of 20F, high of 30F         \\ \hline
new york                         & misty, low of 30F, high of 50F       & snow, low of 70F, high of 80F     & cloudy, low of 20F, high of 30F       \\ \hline
boston                           & hail, low of 90F, high of 100F       & overcast, low of 60F, high of 70F & rain, low of 50F, high of 60F         \\ \hline
durham                           & hot, low of 90F, high of 100F        & dry, low of 60F, high of 80F      & misty, low of 60F, high of 80F        \\ \hline
san francisco                    & rain, low of 60F, high of 70F        & cloudy, low of 30F, high of 40F   & overcast, low of 90F, high of 100F    \\ \hline
carson                           & raining, low of 70F, high of 80F     & humid, low of 90F, high of 100F   & frost, low of 40F, high of 60F        \\ \hline
san jose                         & blizzard, low of 40F, high of 50F    & snow, low of 90F, high of 100F    & overcast, low of 60F, high of 80F     \\ \hline
\textbf{thursday}                    & \textbf{friday}                  & \textbf{saturday}                     & \textbf{sunday}                   \\ \hline
clear skies, low of 60F, high of 70F & warm, low of 70F, high of 90F    & foggy, low of 50F, high of 60F        & overcast, low of 50F, high of 60F \\ \hline
rain, low of 80F, high of 100F       & rain, low of 40F, high of 60F    & cloudy, low of 30F, high of 50F       & snow, low of 20F, high of 40F     \\ \hline
dew, low of 20F, high of 30F         & cloudy, low of 90F, high of 100F & overcast, low of 50F, high of 70F     & overcast, low of 80F, high of 90F \\ \hline
misty, low of 90F, high of 100F      & hot, low of 70F, high of 90F     & hail, low of 30F, high of 40F         & rain, low of 60F, high of 80F     \\ \hline
stormy, low of 90F, high of 100F     & cloudy, low of 90F, high of 100F & clear skies, low of 90F, high of 100F & foggy, low of 30F, high of 50F    \\ \hline
dry, low of 50F, high of 60F         & foggy, low of 80F, high of 100F  & overcast, low of 50F, high of 70F     & overcast, low of 60F, high of 70F \\ \hline
rain, low of 30F, high of 50F        & rain, low of 80F, high of 100F   & dry, low of 50F, high of 60F          & dry, low of 90F, high of 100F     \\ \hline
\end{tabular}
}

\begin{tabular}{r|l}
\hline
\textbf{DRIVER}      & Is it gonna snow in Durham during the week?                \\ \hline
\textit{Seq2seq}     & there is no snow in the forecast for this week             \\
\textit{+Att}        & there is no snow in the forecast for durham for durham     \\
\textit{Ptr-Unk}      & yes there is no snow in durham on week                     \\ \hline
\textit{Mem2Seq} & no snow, the forecast does not predict any snow on Durham  \\ \hline
\textbf{GOLD}        & Durham does not have any snow predicted this week          \\ \hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Example of generated answers for the In-Car dataset on the Weather Domain.}
\label{APPENDINX_2}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{monday}                      & \textbf{tuesday}                      & \textbf{friday}                      & \textbf{wednesday}                   \\ \hline
windy, low of 40F, high of 50F       & snow, low of 40F, high of 50F         & frost, low of 30F, high of 40F       & hail, low of 70F, high of 80F        \\ \hline
rain, low of 30F, high of 40F        & foggy, low of 60F, high of 70F        & stormy, low of 50F, high of 60F      & snow, low of 80F, high of 90F        \\ \hline
cloudy, low of 20F, high of 40F      & clear skies, low of 70F, high of 90F  & clear skies, low of 60F, high of 70F & dry, low of 40F, high of 60F         \\ \hline
warm, low of 90F, high of 100F       & windy, low of 50F, high of 60F        & blizzard, low of 20F, high of 30F    & rain, low of 60F, high of 70F        \\ \hline
snow, low of 60F, high of 80F        & clear skies, low of 30F, high of 50F  & clear skies, low of 30F, high of 40F & dry, low of 30F, high of 50F         \\ \hline
windy, low of 50F, high of 60F       & drizzle, low of 80F, high of 100F     & windy, low of 80F, high of 100F      & clear skies, low of 50F, high of 70F \\ \hline
clear skies, low of 60F, high of 70F & rain, low of 90F, high of 100F        & foggy, low of 20F, high of 40F       & hail, low of 20F, high of 30F        \\ \hline
\textbf{thursday}                    & \textbf{sunday}                       & \textbf{location}                    & \textbf{saturday}                    \\ \hline
raining, low of 80F, high of 90F     & foggy, low of 40F, high of 50F        & san francisco                        & snow, low of 40F, high of 50F        \\ \hline
drizzle, low of 50F, high of 60F     & clear skies, low of 90F, high of 100F & manhattan                            & dry, low of 70F, high of 90F         \\ \hline
stormy, low of 40F, high of 50F      & hail, low of 40F, high of 60F         & seattle                              & windy, low of 60F, high of 70F       \\ \hline
overcast, low of 80F, high of 90F    & blizzard, low of 20F, high of 40F     & oakland                              & clear skies, low of 50F, high of 70F \\ \hline
raining, low of 50F, high of 60F     & windy, low of 30F, high of 50F        & exeter                               & overcast, low of 60F, high of 70F    \\ \hline
frost, low of 40F, high of 50F       & drizzle, low of 40F, high of 60F      & menlo park                           & foggy, low of 50F, high of 70F       \\ \hline
dry, low of 20F, high of 30F         & cloudy, low of 20F, high of 40F       & carson                               & windy, low of 80F, high of 100F      \\ \hline
\end{tabular}
}

\begin{tabular}{r|l}
\hline
\textbf{DRIVER}      & Is it going to snow today?                         \\
\textbf{CAR}         & What city do you wanna know if it will snow today? \\
\textbf{DRIVER}      & Please check for Oakland                           \\
\textbf{GOLD}        & Today is gonna be warm in Oakland                  \\ \hline
\textit{Seq2seq}     & It is not snow in today in                         \\
\textit{+att}        & No there will be snow in Oakland today             \\
\textit{Ptr-Unk}      & Today is gonna be PAD in                           \\ \hline
\textit{Mem2Seq} & it will not snow in oakland today                  \\ \hline
\end{tabular}
\end{table}











\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the navigation domain.}
\label{APPENDINX_4}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{poi} & \textbf{distance} & \textbf{traffic\_info} & \textbf{poi\_type} & \textbf{address} \\ \hline
Stanford Express Care & 3 miles & no traffic & hospital & 214 El Camino Real \\ \hline
home & 4 miles & heavy traffic & home & 5671 barringer street \\ \hline
Whole Foods & 2 miles & heavy traffic & grocery store & 819 Alma St \\ \hline
P.F. Changs & 6 miles & moderate traffic & chinese restaurant & 669 El Camino Real \\ \hline
Jing Jing & 5 miles & heavy traffic & chinese restaurant & 113 Arbol Dr \\ \hline
Pizza My Heart & 6 miles & moderate traffic & pizza restaurant & 528 Anton Ct \\ \hline
Cafe Venetia & 4 miles & no traffic & coffee or tea place & 269 Alger Dr \\ \hline
\end{tabular}
}

\begin{tabular}{r|l}
\hline
\textbf{DRIVER} & Find me a tea house \\\hline
\textit{Seq2seq} & We' re 2 miles away from from and there is a road block nearby \\
\textit{+att} & We' re 4 miles away from Cafe Venetia and it s at 269 Alger drive \\
\textit{PTRUNK} & We' re 6 miles away from whole foods and you like the address on screen \\ \hline
\textit{Mem2Seq} & We're 4 miles away from Cafe Venetia and there is no traffic \\ \hline
\textbf{GOLD} & There is Cafe Venetia that's 4 miles away at 269 Alger dr \\ \hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the navigation domain.}
\label{APPENDINX_5}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{distance} & \textbf{traffic\_info} & \textbf{poi\_type} & \textbf{address} & \textbf{poi} \\ \hline
4 miles & heavy traffic & rest stop & 578 Arbol Dr & Hotel Keen \\ \hline
1 miles & moderate traffic & hospital & 899 Ames Ct & Stanford Childrens Health \\ \hline
2 miles & no traffic & grocery store & 452 Arcadia Pl & Safeway \\ \hline
1 miles & moderate traffic & coffee or tea place & 436 Alger Dr & Palo Alto Cafe \\ \hline
1 miles & heavy traffic & grocery store & 819 Alma St & Whole Foods \\ \hline
6 miles & moderate traffic & coffee or tea place & 394 Van Ness Ave & Coupa \\ \hline
6 miles & heavy traffic & coffee or tea place & 145 Amherst St & Teavana \\ \hline
\end{tabular}
}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|l}
\hline
\textbf{DRIVER}      & Take me to a place where i can get some hot tea                                               \\\hline
\textit{Seq2seq}     & There is a away                                                                               \\
\textit{att}         & There is a Pizza Chicago and Teavana                                                          \\
\textit{PTRUNK}      & I have a PAD away away                                                                        \\ \hline
\textit{Mem2Seq} & We' re 6 miles away from Palo Alto Cafe and 6 miles away from Coupa, which one do you prefer? \\ \hline
\textbf{GOLD}        & Coupa is nearby and serves tea  \\ \hline
\end{tabular}}
\end{table}



\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the navigation domain.}
\label{APPENDINX_6}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{distance} & \textbf{traffic\_info} & \textbf{poi\_type} & \textbf{address}   & \textbf{poi}   \\ \hline
3 miles           & no traffic             & chinese restaurant & 593 Arrowhead Way  & Chef Chu's     \\ \hline
1 miles           & no traffic             & chinese restaurant & 669 El Camino Real & P.F. Changs    \\ \hline
5 miles           & road block nearby      & gas station        & 200 Alester Ave    & Valero         \\ \hline
3 miles           & no traffic             & pizza restaurant   & 528 Anton Ct       & Pizza My Heart \\ \hline
4 miles           & heavy traffic          & rest stop          & 753 University Ave & Comfort Inn    \\ \hline
5 miles           & heavy traffic          & chinese restaurant & 842 Arrowhead Way  & Panda Express  \\ \hline
2 miles           & heavy traffic          & pizza restaurant   & 704 El Camino Real & Pizza Hut      \\ \hline
6 miles           & no traffic             & friends house      & 864 Almanor Ln     & jacks house    \\ \hline
\end{tabular}
}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|l}
\hline
\textbf{DRIVER} & Find me the nearest gas station. \\\hline
\textit{Seq2seq} & the nearest parking garage is from at away \\
\textit{+att} & The is one is away away \\
\textit{PTRUNK} & PAD nearest gas station is is is \\ \hline
\textit{Mem2Seq} & The nearest gas station is gas station which is you you \\ \hline
\textbf{GOLD} & The nearest gas station is located 5 miles away. Need more info? \\ \hline
\end{tabular}
}
\end{table}


\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the navigation domain.}
\label{APPENDINX_9}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Poi}             & \textbf{Distance} & \textbf{Traffic\_info} & \textbf{Poi\_type} & \multicolumn{1}{c|}{\textbf{Address}} \\ \hline
The Westin               & 4 miles           & no traffic             & rest stop          & 329 El Camino Real                    \\ \hline
Palo Alto Garage R       & 2 miles           & moderate traffic       & parking garage     & 481 Amaranta Ave                      \\ \hline
Mandarin Roots           & 4 miles           & moderate traffic       & chinese restaurant & 271 Springer Street                   \\ \hline
Sigona Farmers Market    & 4 miles           & no traffic             & grocery store      & 638 Amherst St                        \\ \hline
Whole Foods              & 2 miles           & heavy traffic          & grocery store      & 819 Alma St                           \\ \hline
Stanford Shopping Center & 2 miles           & moderate traffic       & shopping center    & 773 Alger Dr                          \\ \hline
toms house               & 1 miles           & no traffic             & friends house      & 580 Van Ness Ave                      \\ \hline
\end{tabular}}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|l}
\hline
\textbf{DRIVER}        & Give me directions to the closest grocery store                                                                                               \\
\textbf{CAR}           & \begin{tabular}[c]{@{}l@{}}There are Whole Foods 2 miles away and \\ Sigona Farmers Market 4 miles away. Where do we go?\end{tabular}         \\
\textbf{DRIVER}        & I need the one that avoids all heavy traffic                                                                                                  \\ \hline
\textit{Seq2seq}       & he is is is is away at                                                                                                                       \\
\textit{+Att}           & Chef Chu s is located at 593 Arrowhead way                                                                                                    \\
\textit{Ptr-Unk}        & PAD is the is PAD the to                                                                                                                      \\ \hline
\textit{Mem2Seq}   & \begin{tabular}[c]{@{}l@{}}Sigona Farmers Market is the only option i is 4 miles \\ away with no traffic, setting navigation now\end{tabular} \\ \hline
\textbf{GOLD} & \begin{tabular}[c]{@{}l@{}}Sigona Farmers Market is located 4 miles away with no traffic. \\ It's located at 638 Amherst St.\end{tabular}     \\ \hline
\end{tabular}}
\end{table}



\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the scheduling domain.}
\label{APPENDINX_7}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|l}
\hline
\textbf{DRIVER} & Set me a reminder for football activity at 1pm this month on the 11th with marie \\\hline
\textit{Seq2seq} & Okay I am scheduling a reminder with your for at on with at \\
\textit{att} & Okay I am scheduling a football activity with on the 15th at 1pm this month on the 15th \\
\textit{PTRUNK} & Okay setting PAD reminder reminder football activity football activity for this this on the 11th month 1pm \\ \hline
\textit{Mem2Seq} & Setting a reminder for football activity with marie on the 11th at 1pm \\ \hline
\textbf{GOLD} & Reminder for football on the 11th of this month at 1pm with marie is set \\ \hline
\end{tabular}
}

\end{table}


\begin{table}[H]
\centering
\caption{Example of generated responses for the In-Car Assistant on the scheduling domain.}
\label{APPENDINX_8}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{room} & \textbf{agenda} & \textbf{time} & \textbf{date} & \textbf{party} & \textbf{event} \\ \hline
conference room 50 & discuss the merger & 1pm & Wednesday & infrastructure team & conference \\ \hline
- & - & 2pm & Wednesday & - & yoga activity \\ \hline
- & - & 2pm & Wednesday & - & doctor appointment \\ \hline
conference room 50 & go over budget & 7pm & Monday & infrastructure team & meeting \\ \hline
- & - & 5pm & Monday & - & dinner \\ \hline
- & - & 10am & Thursday & - & medicine \\ \hline
\end{tabular}
}
\resizebox{\linewidth}{!}{
\begin{tabular}{r|l}
\hline
\textbf{DRIVER} & Tell me when is my next meeting and who are invited in the meeting and what are the agenda of the next meeting \\\hline
\textit{Seq2seq} & The meeting is the meeting in conference room 100 at 1pm \\
\textit{+att} & Your next meeting is on wednesday at 1pm \\
\textit{PTRUNK} & your PAD meeting PAD the in in the to to the the the the \\ \hline
\textit{Mem2Seq} & Your next meeting is on Monday at 7pm hr is to hr in attendance to go over the budget \\ \hline
\textbf{GOLD} & Your next meeting is on Monday at 7 pm with the infrastructure team to go over budget \\ \hline
\end{tabular}
}
\end{table}

\end{document}
