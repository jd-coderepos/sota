\documentclass{article}
\RequirePackage{textcomp}
\RequirePackage{latexsym}
\RequirePackage{amstext}
\RequirePackage{amsmath}
\RequirePackage{amssymb}
\RequirePackage{amsthm}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage[boxed]{algorithm2e}
\usepackage{mdwlist}
\usepackage{float}
\usepackage{rotating}





\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}









\def\labelitemi{\textbullet}
\def\labelitemii{\textopenbullet}
\def\labelitemiii{\textbullet}
\def\labelitemiv{\textopenbullet}





\def\ep{\varepsilon}

\def\DD{\mathcal{D}}



\def\m{\!-\!}
\def\pl{\!+\!}
\def\eq{\!=\!}





\newcommand{\p}[1]{(#1)}
\newcommand{\pa}[1]{\langle#1\rangle}
\newcommand{\pb}[1]{[#1]}
\newcommand{\pc}[1]{\{#1\}}

\newcommand{\bp}[1]{\left(#1\right)}
\newcommand{\bpa}[1]{\left\langle#1\right\rangle}
\newcommand{\bpb}[1]{\left[#1\right]}
\newcommand{\bpc}[1]{\left\{#1\right\}}

\newcommand{\abs}[1]{|#1|}
\newcommand{\set}[1]{\pc{#1}}
\newcommand{\setc}[2]{\pc{#1 : #2}}
\newcommand{\setcsep}[3]{\pc{#1 #3 #2}}



\newcommand{\expp}[1]{\exp\p{#1}}
\newcommand{\logp}[1]{\log\p{#1}}

\renewcommand{\P}[1]{P\p{#1}}
\newcommand{\E}[1]{E\p{#1}}

\newcommand{\OO}[1]{O\p{#1}}
\newcommand{\OM}[1]{\Omega\p{#1}}
\newcommand{\OT}[1]{\Omega\p{#1}}

\newcommand{\poly}[1]{\text{poly}\p{#1}}





\newcommand{\rank}[2]{R\p{#1,#2}}
\newcommand{\rhat}[2]{\hat{R}\p{#1,#2}}
\newcommand{\rhats}[2]{\hat{R}_S\p{#1,#2}}
\newcommand{\rhatg}[2]{\hat{R}_G\p{#1,#2}}

\newcommand{\eva}[1]{\alpha_{#1}}
\newcommand{\evb}[1]{\beta_{#1}}




\title{A randomized online quantile summary in  words}

\author{
David Felber
\thanks{University of California at Los Angeles. \texttt{dvfelber@cs.ucla.edu}.}
\and
Rafail Ostrovsky
\thanks{University of California at Los Angeles. \texttt{rafail@cs.ucla.edu}.}
}
\date{}



\begin{document}

\maketitle

\begin{abstract}

  A quantile summary is a data structure that approximates to -relative
  error the order statistics of a much larger underlying dataset.

  In this paper we develop a randomized online quantile summary for the cash
  register data input model and comparison data domain model that uses
   words of memory. This improves upon
  the previous best upper bound of 
  by Agarwal et. al. (PODS 2012). Further, by a lower bound of Hung and Ting
  (FAW 2010) no deterministic summary for the comparison model can outperform
  our randomized summary in terms of space complexity. Lastly, our summary has
  the nice property that  words suffice
  to ensure that the success probability is .

\end{abstract}
 
\section{Introduction}
\label{sec:introduction}

A quantile summary  is a fundamental data structure that summarizes an
underlying dataset  of size , in space much less than . Given a query
,  returns a sample  of  such that the rank of  in  is
(probably) approximately . Quantile summaries are used in sensor
networks to aggregate data in an energy-efficient manner and in database query
optimizers to generate query execution plans.

Quantile summaries have been developed for a variety of different models and
metrics. The data input model we consider is the standard online cash register
streaming model, in which a new item is added to the dataset at each new
timestep, and the total number of items is not known until the end. The data
domain model we consider is the comparison model, in which stream items come
from an arbitrary ordered domain (and specifically, not necessarily from the
integers).

Formally, our quantile summary problem is defined over a totally ordered domain
 and by an error parameter . There is a dataset  that is
initially empty. Time occurs in discrete steps. In timestep , stream item
 arrives and is then processed, and then any quantile queries  in
that step are received and processed. To be definite, we pick the first timestep
to be . We write  or  for the -item prefix stream  of . The goal is to maintain at all times  a summary  of the
dataset  that, given any query  in , can return a sample  so that , where
   is the \emph{rank of item  in set }, defined as
  . For randomized summaries, we only require
  that ; that is, 's rank is only probably close to , not
  definitely close. In fact, it will be easier to deal with the rank directly,
  so we define  and use that in what follows.



\subsection{Previous work}

The two most directly relevant pieces of prior work are randomized online
quantile summaries for the cash register/comparison model. Aside from oblivious
sampling algorithms (which require storing  samples) we are
unaware of any other randomized online quantile summaries that work in the
comparison model.

The newer of the two is that of Agarwal, Cormode, Huang, Phillips, Wei, and Yi
\cite{ACHPWY2012} \cite{ACHPWY2013}. Among other results, Agarwal et. al.
develop a randomized online quantile summary for the cash register/comparison
model that uses  words of memory.
This summary has the nice property that any two such summaries can be combined
to form a summary of the combined underlying dataset without loss of accuracy or
increase in size.

The earlier such summary is that of Manku, Rajagopalan, and Lindsay
\cite{MRL1999}, which uses  space. At a
high level, their algorithm downsamples the input stream in a non-uniform way
and feeds the downsampled stream into a deterministic summary, which
periodically adjusts the downsampling rate.

We note here that our algorithm is inspired by the algorithm of Manku et. al.
but has important differences. We defer a discussion of the similarities and
differences to section \ref{sec:discussion} after the presentation of our
algorithm in section \ref{sec:online}.

For the comparison model, the best deterministic online summary to date is the
(GK) summary of Greenwald and Khanna \cite{GK2001}, which uses
 space. This improved upon a deterministic (MRL)
summary of Manku, Rajagopalan, and Lindsay \cite{MRL1998} and a summary implied
by Munro and Paterson \cite{MP1978}, which use 
space.

A more restrictive domain model than the comparison model is the bounded
universe model, in which elements are drawn from the integers . For this model there is a deterministic online summary by Shrivastava,
Buragohain, Agrawal, and Suri \cite{SBAS2004} that uses  space.

Not much exists in the way of lower bounds for this problem. There is a simple
lower bound of  which intuitively comes from the fact that no one
sample can satisfy more than  different rank queries. For the
comparison model, Hung and Ting \cite{HT2010} developed a deterministic
 lower bound. Whether this bound can be
extended to hold for our weaker probabilistic guarantee, and whether our
algorithm can be modified to satisfy the stronger deterministic guarantee, are
both open questions.



\subsection{Our results}

In the next section we describe a simple 
streaming summary that is online except that it requires  to be given up
front and that it is unable to process queries until it has seen a constant
fraction of the input stream. In section \ref{sec:online} we develop this simple
summary into a fully online summary that can answer queries at any point in
time. We close in section \ref{sec:discussion} by examining the similarities and
differences between our summary and previous work and discuss a design approach
for similar streaming problems.
 
\section{A simple streaming summary}
\label{sec:simple}

Before we describe our algorithm we must first describe its two main components
in a bit more detail than was used in the introduction. The two components are
Bernoulli sampling and the GK summary \cite{GK2001}.



\subsection{Bernoulli sampling}

Bernoulli sampling downsamples a stream  of size  to a sample stream 
by choosing to include each next item into  with independent probability
. (As stated this requires knowing the size of  in advance.) At the end
of processing , the expected size of  is , and the expected rank of any
sample  in  is .
In fact, for any times  and partial streams  and , where
 is the sample stream of , we have  and
. To generate an estimate for
 from  we use .
The following theorem bounds the probability that  is very large or that
 is very far from  (for any given time , but not for all times  combined). The proof is
folklore, a simple application of Chernoff bounds.

\begin{theorem}
  \label{thm:bernoulli}
  For all times , .

  Further, for all times  and items , .
\end{theorem}
\begin{proof}
  For the first part,  (since ).

  For the second part,  is
  equal to . The
  Chernoff bound is . Here, , so .
\end{proof}

This means that, given any , if we return the sample  with , then  is likely to be
close to .



\subsection{GK summary}

The GK summary is a deterministic summary that can answer queries to relative
error, over any portion of the received stream. If  is the summary after
inserting the first  items  from stream  into  then, given any ,  can return a sample  so that
. Greenwald and Khanna guarantee in
\cite{GK2001} that  uses  words. We call
this the \emph{GK guarantee}.



\subsection{Our summary}

We combine Bernoulli sampling with the GK summary by downsampling the input data
stream  to a sample stream  and then feeding  into a GK summary . It
looks like this:

\begin{figure}[H]
  \includegraphics{figure1.pdf}
  \centering
  \caption{The big picture.}
  \label{fig:figure1}
\end{figure}

The key reason this gives us a small summary is that we never need to store ;
each time we sample an item into  we immediately feed it into . Therefore,
we only use as much space as  uses. In particular, as long as , we use only  words.

To answer a query  for  we ask  the query  and
return the resulting sample . There is a slight issue in that 
may be larger than ; but if the approximation guarantee holds for the
largest item in  then , so using
 instead will not cause more than  relative
error in the approximation.

The probability that our sample stream  is not too big (uses more than  samples) is at least . If this happens to be the case
then the probability that all of its samples  are good (have
) is at least  by theorem \ref{thm:bernoulli} and the union bound.
Choosing  suffices to guarantee that both
events occur with total probability at least .

Further, if both  events occur then the total error introduced by both
 and  is at most . Suppose that  returns  when
given . This means that  by the GK guarantee. Since both events for
 occur, we also have  (and only  in the case that we don't truncate
 to ). Thus, . Equivalently, .



\subsection{Caveats}

There are two serious issues with this summary. The first is that it requires us
to know the value of  in advance to perform the sampling. Also, as a
byproduct of the sampling, we can only obtain approximation guarantees after we
have seen at least  (or at least some constant fraction) of the items.
This means that while the algorithm is sufficient for approximating order
statistics over streams stored on disk, more is needed to get it to work for
online streaming applications, in which (1) the stream size  is not known in
advance, and (2) queries can be answered approximately at all times 
and not just when .

Adapting the idea of our basic streaming summary to work online constitutes the
next section and the bulk of our contribution.
We start with a high-level overview of our online summary algorithm.
In section \ref{sec:online-alg} we formally define an initial version of our
algorithm whose expected size at any given time is  words.
In section \ref{sec:online-error} we show that our algorithm gurantees that
.
In section \ref{sec:online-spacetime} we discuss the slight modifications
necessary to get a deterministic  space
complexity, and also perform a time complexity analysis.
 
\section{An online summary}
\label{sec:online}

Our algorithm works in \emph{rows}, which are illustrated in appendix
\ref{sec:appendix-online-alg-diagram}. Row  is a summary of the first  stream items. Since we don't know how many items will actually be in the
stream, we can't start all of these rows running at the outset. Therefore, we
start each row  once we have seen  of its total items. However,
since we can't save these items for every row we start, we need to construct an
approximation of this fraction of the stream, which we do by using the summary
of the previous row, and join this approximating stream with the new items that
arrive while the row is live. We then wait until the row has seen a full half of
its items before we permit it to start answering queries; this dilutes the
influence of approximating the  of its input that we couldn't store.

Operation within a row is very much like the operation of our fixed-
streaming summary. We feed the joint approximate prefix + new item stream
through a Bernoulli sampler to get a sample stream, which is then fed into a GK
summary (which is stored). After row  has seen half of its items, its GK
summary becomes the one used to answer quantile queries. When row  has seen
 of \emph{its} total items, row  generates an approximation of those
items from its GK summary and feeds them as a stream into row .

Row  is slightly different in order to bootstrap the algorithm. There is no
join step since there is no previous row to join. Also, row  is active from
the start. Lastly, we get rid of the sampling step so that we can answer queries
over timesteps .

After the first  items, row  is no longer needed, so we can clean up
the space used by its GK summary. Similarly, after the first  items,
row  is no longer needed. The upshot of this is that we never need storage
for more than six rows at a time. Since each GK summary uses  words, the six live GK summaries use only a constant
factor more.

Our error analysis, on the other hand, will require us to look back as many as
 rows to ensure our approximation guarantee. We stress that we
will not need to actually \emph{store} these  rows for our
guarantee to hold; we will only need that they didn't have any bad events (as
will be defined) when they \emph{were} alive.



\subsection{Algorithm description}
\label{sec:online-alg}

Our algorithm works in rows. Each row  has its own copy  of the GK
algorithm that approximates its input to  relative error. For each row
 we define several streams:  is the prefix stream of row ,  is
its suffix stream,  is its prefix stream replacement (generated by the
previous row),  is the joint stream  followed by ,  is its
sample stream, and  is a one-time stream generated from  by querying
it with ranks
,
where .

The prefix stream  for row , importantly, is not
directly received by row . Instead, at the end of timestep , row
 generates  and duplicates each of those  items
 times to get the replacement prefix , which is then
immediately fed into row  before timestep  begins.

Each row can be \emph{live} or not and \emph{active} or not. Row  is live in
timesteps  and row  is live in timesteps . Live rows require space; once a row is no longer live we can
free up the space it used. Row  is active in timesteps  and
row  is active in timesteps . This
definition means that exactly one row  is active in any given timestep
. Any queries that are asked in timestep  are answered by .
Given query , we ask  for  and return the
result.

At each timestep , when item  arrives, it is fed as the next item in the
suffix stream  for each live row .  joined with  defines the
joined input stream . For ,  is downsampled to the sample
stream  by sampling each item independently with probability .
For row , no downsampling is performed, so . Lastly,  is fed
into .

Appendix \ref{sec:appendix-online-alg-diagram} shows the operation of and the
communication between the first six rows. Solid arrows indicate continuous
streams and dashed arrows indicate one-time messages. Appendix
\ref{sec:appendix-online-alg-listing} is a pseudocode listing of the algorithm.



\subsection{Error analysis}
\label{sec:online-error}

Define  and  to be
 followed by  and then . That is,  is just the continuation
of  for the entire length of the input stream.

Fix some time . All of our claims will be relative to time ; that is, if
we write  we mean . Our error analysis proceeds as follows. We
start by proving that  is a good approximation of
 when certain conditions hold for . By induction,
this means that  is a good approximation of 
when the conditions hold for all of , and actually it's
enough for the conditions to hold for just 
to get a good approximation. Having proven this claim, we then prove that the
result  of a query to our summary has  close to
. Lastly, we show that  suffices to ensure that the
conditions hold for  with very high
probability ().

\begin{lemma}
  \label{lem:online-error-indstep}
  Let  be the event that  and let  be the
  event that any of the first  samples  in  has . Say that  is \emph{good}
  if neither  nor  occur (or if ).

  For all  such that , and for all items , if
   is good then .
\end{lemma}
\begin{proof}
  At the end of time  we have , which is each item
   in  duplicated  times. If 
  is good then by theorem \ref{thm:bernoulli} and the GK guarantee we have that
  .

  Fix  so that , where  and
   are defined to be  and  for
  completeness. Fixing  this way implies that . By the above bound on  we also
  have that .

  Putting these two bounds together, and recalling that , we find that . For each time  after , the new item  changes the rank
  of  in both streams  and  by the same additive offset, so
  .
\end{proof}

By applying this lemma inductively we can bound the difference between  and
:

\begin{corollary}
  For all  such that , if all of   are good, then .
\end{corollary}

To ensure that all of these  are good would require  to grow with ,
which would be bad. Happily, it is enough to require only the last  sample summaries to be good, since the other items we disregard
constitute only a small fraction of the total stream.

\begin{corollary}
  \label{cor:online-error-lastgood}
  Let . For all  such that ,
  if all of  are good, then
  .
\end{corollary}
\begin{proof}\let\qed\relax
  By lemma \ref{lem:online-error-indstep} we have . At time , 
  and  share all except possibly the first  items. Thus
  
\end{proof}

We now prove that the if the last several sample streams were good then querying
our summary will give us a good result.

\begin{lemma}
  \label{lem:online-error-querygood}
  Let  and . If all  are good, then querying our
  summary with rank  (= querying the active GK summary  with ) returns  such that .
\end{lemma}
\begin{proof}
  By corollary \ref{cor:online-error-lastgood} we have . By theorem
  \ref{thm:bernoulli} and the GK guarantee, .
\end{proof}

Lastly, we prove that  suffices to ensure that all of
  are good with
probability at least .

\begin{lemma}
  \label{lem:online-error-goodprob}
  Let  and . If  then all of  are good with probability at least .
\end{lemma}
\begin{proof}
  There are at most  of these summary
  streams total. Theorem \ref{thm:bernoulli} and the union bound give us
  \eva{r} and
  \evb{r}.

  Together, S_r. It suffices to choose  to obtain .
\end{proof}



\subsection{Space and time complexity}
\label{sec:online-spacetime}

A minor issue with the algorithm is that, as written in section
\ref{sec:online-alg}, we do not actually have a bound on the worst-case space
complexity of the algorithm; we only have a bound on the space needed at any
given point in time. This issue is due to the fact that there are low
probability events in which  can get arbitrarily large and the fact
that over  items there are a total of  sample streams. The space
complexity of the algorithm is , and to bound this value
with constant probability using the Chernoff bound appears to require that , which is too big.

Fortunately, fixing this problem is simple. Instead of feeding every sample of
 into the GK summary , we only feed each next sample if  has seen
 samples so far. That is, we deterministically restrict  to
receiving only  samples. Lemmas \ref{lem:online-error-indstep} through
\ref{lem:online-error-querygood} condition on the goodness of the sample streams
, which ensures that the  receive at most  samples each, and the
claim of lemma \ref{lem:online-error-goodprob} is independent of the operation
of . Therefore, by restricting each  to receive at most  inputs
we can ensure that the space complexity is deterministically  without breaking our error guarantees.

From a practical perspective, the assumption in the streaming setting is that
new items arrive over the input stream  at a high rate, so both the
worst-case per-item processing time as well as the amortized time to process 
items are important. For our per-item time complexity, the limiting factor is
the duplication step that occurs at the end of each time ,
which makes the worst-case per-item processing time as large as .
Instead, at time  we could generate  and store it in 
words, and then on each arrival  we could
insert both  and also the next item in . By the time 
that we generate , all items in  will have been inserted into .
Thus the worst-case per-item time complexity is , where  is the
worst-case per-item time to query or insert into one of our GK summaries.
Over  items there are at most  insertions into any one GK
summary, so the amortized time over  items in either case is , where  is the amortized
per-item time to query or insert into one of our GK summaries.

The pseudocode listing in appendix \ref{sec:appendix-online-alg-listing}
includes the changes of this section.
 
\section{Discussion}
\label{sec:discussion}

Our starting point is a very natural idea of Manku et. al. \cite{MRL1999} that
due to subtle technical difficulties saw no further application to the quantiles
problem for sixteen years. This key idea is to downsample the input stream and
feed the resulting sample stream into a deterministic summary data structure
(compare our figure \ref{fig:figure1} with figure 1 on page 254 of
\cite{MRL1999}). At a very high level, we are simply replacing their
deterministic  MRL summary \cite{MRL1998} with
the deterministic  GK summary \cite{GK2001}.
However, as evidenced by the fact that fourteen years after the GK summary was
published the state of the art was the randomized  summary of Agarwal et. al. \cite{ACHPWY2012}
\cite{ACHPWY2013}, adapting this idea to the GK summary without superconstant
overhead is nontrivial.

Our implementation of this idea is conceptually different from the
implementation of Manku et. al. in two respects. First, we use the GK algorithm
strictly as a black box, whereas Manku et. al. peek into the internals of their
MRL algorithm, using its algorithm-specific interface (\textsc{New},
\textsc{Collapse}, \textsc{Output}) rather than the more generic interface
(\textsc{Insert}, \textsc{Query}). At an equivalent level, dealing with the GK
algorithm is already unpleasant. Using the generic interface, our implementation
could just as easily replace the GK boxes in the diagram in appendix
\ref{sec:appendix-online-alg-diagram} with MRL boxes; or, for the bounded
universe model, with boxes running the q-digest summary of Shrivastava et. al.
\cite{SBAS2004}.

The second respect in which our algorithm differs critically from that of Manku
et. al. is that we operate on \emph{streams} rather than on stream \emph{items}.
We use this approach in our proof strategy too; the key step in our error
analysis, lemma \ref{lem:online-error-indstep}, is a statement about (what to us
are) static objects, so we can trade out the complexity of dealing with
time-varying data structures for a simple induction.

The approach we developed to reduce a deterministic summary to a randomized
summary was:
\begin{enumerate*}
\item For a fixed , downsample the input stream, feed the resulting sample
  stream into the deterministic summary, and prove a probabilistic bound.
\item Run an infinite number of copies of step 1, for exponentially growing
  values of .
\item Replace a constant fraction prefix of each copy with an approximation
  generated by the previous copy, and prove using step 1 that this approximation
  probably doesn't cause too much error.
\item Use step 3 inductively to prove a probabilistic bound for the entire
  stream.
\end{enumerate*}
We believe (albeit on the basis of this problem and our algorithm alone) that
developing streaming algorithms that operate on streams rather than on stream
items is likely to be a useful design approach for many problems.
 
\bibliographystyle{plain}
\bibliography{quantiles}
 

\appendix

\newpage
\section{Diagram for online algorithm}
\label{sec:appendix-online-alg-diagram}

\begin{figure}[H]
    \includegraphics[angle=90,scale=.91]{figure2.pdf}
    \centering
    \caption{Each row  has its own copy  of the GK algorithm that
      approximates its input to  relative error.  is the prefix
      stream of row ,  is its suffix stream,  is its prefix stream
      replacement (generated by the previous row),  is the joint stream
       followed by ,  is its sample stream, and  is a
      one-time stream generated from  at time  to get the
      replacement prefix .}
    \label{fig:figure2}
\end{figure}
 
\newpage
\section{Pseudocode for online algorithm}
\label{sec:appendix-online-alg-listing}

The differences in the algorithms of sections \ref{sec:online-alg} and
\ref{sec:online-spacetime} are marked.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoNoEnd
  \SetAlgoNoLine
  \LinesNumbered
  \SetKwIF{On}{}{}{on}{do}{}{}{}
  \SetKwIF{Wp}{}{}{with probability}{do}{}{}{}
Initially, allocate space for . Mark row  as live and active.\;
\For{}{
\ForEach{live row }{
      \Wp{}{
        \If{section \ref{sec:online-alg}}{
          Insert  into .\;
        }
        \ElseIf{section \ref{sec:online-spacetime}}{
          Insert  into  if  has seen  insertions.\;
          \If{ and  and  has seen 
            insertions}{
            \Wp{}{
              Also insert item  of  into .\;
            }
          }
        }
      }
    }
    \If{ for some }{
      Allocate space for . Mark row  as live.\;
      \If{section \ref{sec:online-alg}}{
        Query  with  to get .\;
        \For{}{
          \For{}{
            \Wp{}{
              Insert  into .\;
            }
          }
        }
      }
      \ElseIf{section \ref{sec:online-spacetime}}{
        Store , to implicitly define .\;
      }
    }
    \If{ for some }{
      Mark row  as active. Unmark row  as active.\;
    }
    \If{ for some }{
      Unmark row  as live. Free space for .\;
    }
  }
\On{query }{
    Let  be the active row.\;
    Query  for rank . Return the result.\;
  }
  \caption{Procedural listing of the algorithm.}
  \label{fig:figure3}
\end{algorithm}
 
\end{document}
