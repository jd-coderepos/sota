\subsection{Dataset and Metrics}
\noindent\textbf{Dataset.}
For 3D human poes estimation (Sec.~\ref{sec:BenM-Eva}), we report results on MuPoTS-3D~\cite{mehta2018single}, CMU Panoptic~\cite{joo2015panoptic}, 3DPW~\cite{von2018recovering}, 3DPW-OCC~\cite{von2018recovering,zhang2020object}, 3DOH~\cite{zhang2020object} and Human3.6M~\cite{ionescu2013human3} datasets. Inspired by PARE~\cite{kocabas2021pare}, we randomly generate synthetic occlusions on the image of MuPoTS-3D and name it MuPoTS-synthOcc. It is only used for evaluation. For experiments on MuPoTS-3D, MuPoTS-synthOcc, 3DPW, and 3DPW-OCC, we follow SMAP~\cite{zhen2020smap} and train our model on the MuCo-3DHP~\cite{mehta2018single} dataset. In addition, for a fair comparison, we mix the data with COCO2017~\cite{lin2014microsoft} during training and $50\%$ of data in each mini-batch is from it (same as~\cite{mehta2018single,mehta2017vnect,zhen2020smap}). For Panoptic, following~\cite{zanfir2018monocular,zhen2020smap}, we choose cameras 16 and 30, and randomly select 9600 images from four activities (Haggling, Mafia, Ultimatum, Pizza) as test set, and 160k images from other sequences as training set. The synthetic dataset for training the DSED reasoning module is built based on AMASS~\cite{mahmood2019amass}. Results on Human3.6M are reported in Sup. Mat.

For the broader study of the reasoning module and the DSED network (Sec.~\ref{sec:broaderStudy}), we apply our reasoning module to two SOTA methods (PifPaf~\cite{kreiss2019pifpaf} and  HigherHRNet~\cite{cheng2020higherhrnet}) and evaluate them on COCO~\cite{lin2014microsoft} and CrowdPose~\cite{li2019crowdpose}. For a fair comparison, we directly adopt the official implementation and use the same training data. Results on CrowdPose are reported in Sup. Mat.

\noindent\textbf{Metrics.}
For pose estimation, we consider mean per joint position error (MPJPE) in mm and percentage of correct keypoints (PCK) in 3D. Following~\cite{zhen2020smap}, a keypoint is declared correct if the Euclidean distance error is smaller than 150mm. We evaluate absolute pose accuracy (subscript \textit{abs}), relative pose accuracy with root alignment (subscript \textit{rel}), and relative pose accuracy of occluded joints (subscript \textit{occ}). For human mesh reconstruction, we consider MPJPE, Procrustes-aligned mean per joint position error (PA-MPJPE), and per vertex error (PVE).

\begin{table}[t]
\renewcommand\arraystretch{0.9}
    \small 
    \centering
    \caption{\textbf{Comparisons on MuPoTS-3D and MuPoTS-synthOCC.} Results on MuPoTS-synthOcc are generated from the official pre-trained model and code if they are released. The PCK$_{occ}$ evaluates the same joints in both datasets to better analyze the occlusion reasoning ability under different levels of occlusion. Best in \textbf{bold}, second best \underline{underlined}.}
    \label{table:MuPoTS-3D}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccc|ccccc}
    \toprule
    &&\multicolumn{5}{c}{MuPoTS-3D}& \multicolumn{5}{|c}{MuPoTS-synthOCC}\\
    \cmidrule(lr){3-7} \cmidrule(l){8-12}
    &&\multicolumn{3}{c}{Matched people}& \multicolumn{2}{|c}{All people} & \multicolumn{3}{|c}{Matched people}& \multicolumn{2}{|c}{All people} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-7} \cmidrule(l){8-10} \cmidrule(l){11-12} 
    & &PCK$_{abs}$&PCK$_{rel}$&PCK$_{occ}$& \multicolumn{1}{|c}{PCK$_{abs}$}&PCK$_{rel}$ &PCK$_{abs}$&PCK$_{rel}$&PCK$_{occ}$& \multicolumn{1}{|c}{PCK$_{abs}$}&PCK$_{rel}$\\
    \midrule
    \multirow{4}{*}{\shortstack{top\\down}}
    &Lcr-net++~\cite{rogez2019lcr} &-&74.0&-&-&-&-&-&-&-&-\\
    &Moon~\etal~\cite{moon2019camera} & 31.8 & 82.5 & 66.8 & 31.5 & \textbf{81.8} & 26.9 & 74.2 & 57.9 & 15.4 & 45.8\\
    &HMOR~\cite{wang2020hmor} & \textbf{43.8} & 82.0 & - & - & - & - & - & - & -  & - \\
    &HDNet~\cite{lin2020hdnet} & 35.2 & 83.7 & - & - & - & 25.8 & 72.3 & 55.9 & - & - \\
    \midrule
    \multirow{5}{*}{\shortstack{bottom\\up}}
    & ORPM~\cite{mehta2018single} & - & 69.8  & - & - & - & - & - & - & -&-\\
    & XNect~\cite{mehta2019xnect} & - & 75.8 & 57.8 & - & - & - & 69.2  & 56.2 & - & - \\
    & SMAP~\cite{zhen2020smap} & 38.7 & 80.5 & 72.9 & 35.4 & 73.5 & \underline{36.4} & 76.1 & 68.9 & 23.9 & 49.1 \\
&Ours & 38.9 & \underline{84.3} & \underline{74.1} & \underline{35.8} & 76.9 & 36.3 & \underline{80.1} &  \underline{71.0} & \underline{24.3} & \underline{52.6} \\
    &Ours (w/ synthetic) & \underline{39.3} & \textbf{86.5} & \textbf{74.9} & \textbf{36.5} & \underline{79.4} & \textbf{37.9} & \textbf{81.7}  & \textbf{72.1} & \textbf{25.5} & \textbf{53.7} \\
    \bottomrule
    \end{tabular}}
\end{table}

\subsection{Benchmark Evaluation}
\label{sec:BenM-Eva}
\noindent\textbf{MuPoTS-3D and MuPoTS-synthOCC.}
Table \ref{table:MuPoTS-3D} compares our method with previous monocular HPE methods. In terms of the relative pose accuracy on which our work mainly focuses, our method significantly surpasses previous bottom-up methods in both visible and occluded joints estimate. Remarkably, even though we use a very similar detection module and the same grouping method, our method outperforms SMAP, which is also the previous SOTA bottom-up method, by 6 PCK$_{rel}$ and 5.6 PCK$_{rel}$ on matched people of MuPoTS-3D and MuPoTS-synthOCC, respectively. When considering all people, we still surpass previous art by 5.9 PCK$_{rel}$ and 4.6 PCK$_{rel}$ on these two datasets. 

More importantly, although top-down methods have the inherent advantage on accuracy since they can use off-the-shelf human detection method and can simplify the problem to single-person pose estimation, our method still outperforms them by 2.8 PCK$_{rel}$ on MuPoTS-3D when considering matched people, and is comparable to them when considering all people. Meanwhile, depending on the number of people in the image, we are faster than the SOTA top-down methods during inference. Overall, the strong results and the relatively fast inference speed prove the effectiveness and efficiency of our method.

\begin{table}[t]
    \renewcommand\arraystretch{0.9}
    \caption{\textbf{Comparisons on 3DPW, 3DPW-OCC, 3DOH, and Panoptic.} Our method yields clear improvements among all datasets. MPJPE is used.}
    \label{tab:other}
    \begin{subtable}[t]{0.43\textwidth}
        \resizebox{0.95\textwidth}{!}{
        \begin{tabular}{cccc}
        \toprule
        & 3DPW & 3DPW-OCC & 3DOH \\
        \midrule
        Moon~\etal~\cite{moon2019camera} & 98.4 & 104.3 & 89.5 \\
        XNect~\cite{mehta2019xnect} & 118.5 & 124.7 & - \\
        SMAP~\cite{zhen2020smap} & 101.5 & 105.2 & 90.6\\
        Ours & \underline{95.8} & \underline{96.9} &-\\
        Ours (w/ Synth) & \textbf{93.1} & \textbf{94.4} & \textbf{80.9} \\
        \bottomrule
        \end{tabular}}
        \caption{3DPW, 3DPW-OCC, and 3DOH}
    \end{subtable}
    \begin{subtable}[t]{.52\textwidth}
        \resizebox{0.95\textwidth}{!}{
        \begin{tabular}{cccccc}
        \toprule
         & Haggling & Mafia & Ultim. & Pizza & Average\\
        \midrule
        Moon~\etal~\cite{moon2019camera} & 89.6 & 91.3 & 79.6 & 90.1 & 87.6\\
        Zanfir~\etal~\cite{zanfir2018deep} & 72.4 & 78.8& 66.8 & 94.3 & 72.1\\
        SMAP~\cite{zhen2020smap} & 63.1 & 60.3 & 56.6 & 67.1 & 61.8 \\
        Ours &  \textbf{54.7} & \underline{55.2} & \textbf{50.1} & \underline{66.4} & \underline{56.1}  \\
        Ours (w/ Synth) & \underline{55.2} & \textbf{55.0} & \underline{50.4} & \textbf{61.4} & \textbf{55.0}\\
        \bottomrule
        \end{tabular}}
        \caption{CMU Panoptic}
    \end{subtable}
\end{table}

\noindent\textbf{3DPW, 3DPW-OCC, and 3DOH.}
These dataset are designed for multi-person human shape reconstruction. It is unfair to compare the errors between the skeleton-based method with SMPL model-based method due to the different definitions of joints. Table~\ref{tab:other} mainly focuses on skeleton-based methods. Results of baselines are generated from the official model. We use the same scripts to match the predicted person with ground-truth and compute the error based on MPI15 joint definition. Therefore, the relative values are more meaningful.

\subsection{Ablation Study}

\begin{table}[t]
\renewcommand\arraystretch{0.9}
    \small 
    \centering
    \caption{\textbf{Ablation studies on MuPoTS-3D dataset.} Det, Reason, and Ref stand for detection, reasoning, and refinement module, respectively. Hg stands for hourglass model, NL for nonlocal blocks. Det of SMAP uses all joints during training and Det of ours uses visible joints only.}
    \label{table:Abla}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    &PCK$_{rel}\uparrow$&\multicolumn{1}{c}{MPJPE$_{rel}\downarrow$ } &PCK$_{occ}\uparrow$&MPJPE$_{occ}\downarrow$\\
    \midrule
    \multicolumn{5}{c}{\textit{{Effect of each module of SMAP\cite{zhen2020smap}}}}\\
    \multicolumn{5}{c}{\tabdashline}\nextRow
    SMAP Det & 70.9 & 122.1 & 56.8 & 158.4 \\
    SMAP Det + Ref & 80.5 & 103.3 & 72.9 & 122.8 \\
    \midrule
    \multicolumn{5}{c}{\textit{{Effect of each module}}}\\
    \multicolumn{5}{c}{\tabdashline}\nextRow
    Det & 74.87 & 116.36 & 51.48 & 184.31 \\
    Det + Reason & 79.28 & 104.33 & 61.24 & 145.94\\
    Det + Reason + Ref & 86.54 & 87.28 & 74.92 & 118.60\\
    \midrule
\multicolumn{5}{c}{\textit{{Training without occlusion label (OccL)}}}\\
    \multicolumn{5}{c}{\tabdashline}\nextRow
    Det (w/o OccL) & 71.12 & 123.02 & 56.53 & 163.74\\
    Det (w/o OccL) + Reason & 74.76 & 112.77 & 58.70 & 118.61\\
    \midrule
    \multicolumn{5}{c}{\textit{{Deeply Supervised Encoder Distillation}}}\\
    \multicolumn{5}{c}{\tabdashline}\nextRow
    Det + Reason (Hg) & 75.58 & 113.35 & 55.30 & 154.76\\
    Det + Reason (DSED) & 79.28 & 104.33 & 61.24 & 145.94\\
    Det + Reason (DSED + NL) & 79.81 & 102.32 & 61.56 & 143.59\\
    \bottomrule
    \end{tabular}}

\end{table}

\noindent\textbf{Effect of each module.}
Compared with other bottom-up methods, we add the occlusion keypoint reasoning module. First, we validate the efficiency of this module and the continuous improvement of different modules. The results can be found in \textit{Effect of each module} in Table~\ref{table:Abla}.  We achieve a continuous improvement in the accuracy of both visible and occluded joints with different modules. In addition, compared with SMAP that predicts all keypoints at the same time and then uses single-person refinement to complete the missing prediction, the idea of splitting the detection step into detection and reasoning already yields an improvement of 3.97 PCK$_{rel}$ after detection module and 8.38 PCK$_{rel}$ after reasoning module. Note that SMAP and our method use the same three-stacked hourglass architecture and output similar intermediate results.

\noindent\textbf{Training with occlusion label.} 
Next, we evaluate the effect of using occlusion labels. We can find that without using occlusion labels, the network will try to estimate both visible and occluded joints at the same time. It achieves a higher accuracy (56.53 PCK$_{occ}$ \vs 51.48 PCK$_{occ}$) on occluded joint detection but gets a lower accuracy (71.12 PCK$_{rel}$ \vs 74.87 PCK$_{rel}$) on all joints.  Training the detection module with occluded joints can to some degree improve the ability of occluded joint detection, but increasing the training difficulty and adding noises by many unpredictable joints. When the reasoning module is used in this case, the accuracy is enhanced but still lower than the model trained with occlusion labels by 4.52 PCK$_{rel}$ due to the false-positive prediction and the noise on visible keypoint prediction that is hard to be fixed by the following reasoning module.

\noindent\textbf{DSED network.}
Compared with only using the detection model, both the hourglass-based reasoning model and DSED-based reasoning model yield improvement on occluded joints prediction. However, the DSED model achieves a much higher accuracy on occlusion reasoning by 5.94 PCK$_{occ}$. More importantly, the overall accuracy only improves 0.71 PCK$_{rel}$ when using the hourglass model but 4.41 PCK$_{rel}$ with the DSED model. This is mainly due to the false-positive estimates and noises given by the hourglass model (see Fig.~\ref{fig:KP}). 
In addition, using non-local (NL) blocks can also yield an improvement of $0.53$ PCK$_{rel}$ after carefully design, but it also increases the memory and time required for training. We do not add it to our final model but only show the results here.

\begin{figure}
    \centering
    \includegraphics[width=0.59\columnwidth]{image/heatmapInfer.jpg}  
    \caption{\textbf{Keypoint detection and reasoning.} We visualize the right knee here. More results are provided in Sup. Mat. From left to right: input image, (a) detected keypoints, keypoints inferred by (b) DSED, and by (c) hourglass model.}
    \label{fig:KP}
\end{figure}

\begin{table}
    \small 
    \centering
    \caption{\textbf{Broader study on 2D human pose.} We show results in single-scale testing setting. Reason stands for reasoning module, Hg for hourglass model.}
    \label{table:broader}
    \begin{subtable}[t]{0.495\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccccc}
        \toprule
        \textit{COCO val-dev} & $\text{AP}$ & $\text{AP}^{50}$ & $\text{AP}^{75}$ & $\text{AP}^{M}$& $\text{AP}^{L}$ & $\text{AR}$  \\
        \midrule
        PifPaf~\cite{kreiss2019pifpaf} & 67.4 & - & - & - & - & - & \\
        \quad + Reason (Hg) & 67.5 & 86.5 & 73.6 & 62.0 & 75.8 & 70.9\\
        \quad + Reason (DSED) & 69.1 & 87.0 & 75.3 & 64.7 & 76.9 & 75.5\\
        \midrule
        HrHRNet-W48~\cite{cheng2020higherhrnet} & 69.9 & 87.2 & 76.1 & - & - & -  \\
        \quad + Reason (Hg) & 68.2 & 86.7 & 75.9 & 64.3 & 76.6 & 72.1\\
        \quad + Reason (DSED) & 70.8 & 87.9 & 77.0 & 66.0 & 78.3 & 76.6\\
        \bottomrule
        \end{tabular}}
        \caption{COCO val-dev 2017}
    \end{subtable}
    \begin{subtable}[t]{.495\textwidth}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lcccccccccc}
        \toprule
        \textit{COCO test-dev} & $\text{AP}$ & $\text{AP}^{50}$ & $\text{AP}^{75}$ & $\text{AP}^{M}$& $\text{AP}^{L}$ & $\text{AR}$  \\
        \midrule
        PifPaf~\cite{kreiss2019pifpaf} & 66.7 & - & - & 62.4 & 72.9 & - \\
        \quad + Reason (Hg) & 66.9 & 88.1 & 72.9 & 62.3 & 73.1 & 70.4\\
        \quad + Reason (DSED) & 68.0 & 88.7 & 75.2 & 64.1 & 74.7 & 75.6\\
        \midrule
        HrHRNet-W48~\cite{cheng2020higherhrnet} & 68.4 & 88.2 & 75.1 & 64.4 & 74.2 & -  \\
        \quad + Reason (Hg) & 67.2 &  87.4 & 74.6 & 63.0 & 73.8 & 71.8\\
        \quad + Reason (DSED) & 69.5 & 89.0 & 76.6 & 65.2 & 76.2 & 75.7\\
        \bottomrule
        \end{tabular}}
        \caption{COCO test-dev 2017}
    \end{subtable}
\end{table}

\subsection{Broader Study of the Reasoning Module and DSED Network}
\label{sec:broaderStudy}
The previous section discusses the effect of the reasoning module and DSED network in 3D HPE, now we consider a more general task, \ie, 2D human pose estimation. We delete the depth-related structure of the reasoning module and modify the DSED network, then apply them to PifPaf and HigherHRNet. We use the official implementation with small modifications on their network to make it compatible with our 2D reasoning module (see Sup. Mat. for details). The results are reported in Table~\ref{table:broader}. All methods are trained on 2017 COCO training set. We can see that even though the reasoning module is not designed for 2D tasks since the bone length is more accurate when computed in 3D, it still yields stable improvement (up to 1.7 AP) and achieves SOTA performances. We can also find that DSED is crucial for the reasoning module. 

\begin{table}
    \setlength{\abovecaptionskip}{0.05cm}
    \setlength{\belowcaptionskip}{-0.15cm}
    \renewcommand\arraystretch{0.5}
    \renewcommand{\baselinestretch}{0.8}
    \small 
    \centering
    \caption{\textbf{Evaluation of occluded joint detection.} The left half compares the generated occlusion labels and the right half shows the model performance trained on these labels. For the left half, we randomly select 500 from 200k images in MuCo-3DHP and manually annotate them to get ground-truth labels.}
    \label{table:occL}
    \resizebox{0.7\columnwidth}{!}{
    \begin{tabular}{cccc|ccc}
    \toprule
     & Precision & Recall & F1-score & \multicolumn{1}{|c}{PCK$_{abs}$}& PCK$_{rel}$ &PCK$_{occ}$\\
    \midrule
    Cylinder~\cite{cheng2019occlusion} & 0.653 & 0.401 & 0.497 & - & - & -  \\
    HybrIK-based~\cite{li2021hybrik} & 0.800 & 0.602 & 0.687 & 38.4 & 83.1 & 73.0  \\
    ours (SSF-based) & 0.885 & 0.805 & 0.843 & \textbf{39.3} & \textbf{86.5} & \textbf{74.9} \\
    \bottomrule
    \end{tabular}}
\end{table}

\subsection{SSF and Occlusion Label Generation}
\label{sec:exp-ssf}
\noindent\textbf{Occlusion label generation.}
We compare our method with the Cylinder human model and the SMPL-based human mesh fitting method (\ie, Adaptive HybrIK). We implement the Cylinder model and modify it to detect occlusion by other people. For comparison with HybrIK, we directly use the official model to fit the SMPL models and then use the same graphics pipeline in our SSF to generate the occlusion labels. Note that the ground-truth skeleton is provided for all methods. The results can be found in Table~\ref{table:occL}. Our method generates 
more accurate occlusion labels with much higher precision and recall. More importantly, the labels generated by previous methods reduce performance. SSF is essential to generate occlusion labels for explicit occlusion reasoning.

\begin{table}
\renewcommand\arraystretch{0.9}
    \setlength{\abovecaptionskip}{0.05cm}
    \setlength{\belowcaptionskip}{-0.15cm}
    \renewcommand\arraystretch{0.5}
    \renewcommand{\baselinestretch}{0.8}
    \small
    \centering
    \caption{\textbf{Evaluation of human mesh recovery on 3DPW dataset.} }
    \label{table:HSE}
    \resizebox{0.6\columnwidth}{!}{
    \begin{tabular}{cccc}
    \toprule
    & MPJPE & PA-MPJPE & PVE\\
    \midrule
    SPIN~\cite{kolotouros2019learning} & 96.9 & 59.2 & 116.4\\
    ROMP (ResNet-50)~\cite{sun2021monocular} & 91.3 & 54.9 & 108.3 \\
    PARE (ResNet-50)~\cite{kocabas2021pare} & 84.3 & 51.2 & 101.2\\
    Adaptive HybrIK (ResNet-34)~\cite{li2021hybrik} & \underline{80.0} & \textbf{48.8} & \underline{94.5} \\
    Ours (w/o Synth, ResNet-34) & \textbf{79.1} & \underline{49.3} & \textbf{92.3}\\
    \bottomrule
    \end{tabular}}
\end{table}

\noindent\textbf{3D human mesh reconstruction.}
We evaluate our SSF on the 3DPW dataset. We use the proposed HUPOR to generate 3D skeletons, and then use the estimated skeletons and the image as input to estimate SMPL parameters. For a fair comparison, we strictly follow HybrIK~\cite{li2021hybrik} to prepare the training data and use ResNet-34~\cite{he2016deep} as backbone. The results are reported in Table~\ref{table:HSE}. Compared with baselines, SSF can recover body mesh more accurately. Qualitative results on human shape estimation can be found in Sup. Mat.
