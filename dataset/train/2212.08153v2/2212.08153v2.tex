\section{Experiments}
\label{section:experiments}

\subsection{Experiment Setup}
\label{section:experiment_setup}

\paragraph{Pre-training} All models are based on the T5.1.1 architecture \citep{t5}, pre-trained from scratch on C4 \citep{c4} using JAX \citep{jax}, FLAX \citep{flax}, and T5X \citep{t5x}. We employ the standard T5 training recipe except for a modified Adafactor \citep{adafactor} optimizer. Appendix \ref{apppendix:training} describes training in greater detail. 

\paragraph{Downstream evaluation}

We evaluate \modelname on open-domain question-answering datasets Natural Questions \citep{nq}, TriviaQA \citep{triviaqa} and WebQuestions \citep{webquestions}. We report results on the open-domain QA splits from \citet{orqa}. For all datasets, each sample is paired with a set of 100-word Wikipedia passages ranked by DPR \citep{dpr} score. The question is prepended to each retrieved passage, and then truncated to 256 tokens. The experiments in the paper use 40 retrieved passages to balance performance and speed, but our results hold across a wide range of retrieved passages.

\paragraph{Inference setup}

For our main results we choose a setting that we believe is most representative for common use of retrieval-augmented models. We perform inference on a single TPUv4 and report inference time per sample (TPS) as measured by xprof \citep{xprof}. We use a batch size of 64 (or the largest batch size that fits, if smaller) for the main experiments. Figure \ref{fig:flops_train_inference} and \ref{fig:encoder_decoder_inference} use batch size 24 to ensure a like-for-like comparison, as it is the largest batch size that fits for vanilla FiD. All experiments use 40 passages of 256 tokens and output size of 32 tokens. Predictions are generated with greedy decoding as we found beam search did not meaningfully improve performance for considered tasks. Analysis in Section \ref{sec:analysis} investigates how trade-offs change with input and output length, low batch size and different sampling methods.

\subsection{Main results}

Figure \ref{fig:main_result_figure} shows performance as a function of inference time for FiD and \modelname. \modelname strongly outperforms FiD at any inference budget and achieves the same performance with order of magnitude faster speed. The following section investigates how each component of \modelname contributes to its performance. Table \ref{table:test_results} compares \modelname to published results.

\subsection{Components}

\paragraph{Layer-sparse cross-attention}
\begin{table}[ht!]
\centering
\begin{tabular}{lc|ccc}
    \textbf{Model} & \textbf{TPS} & \textbf{NQ} & \textbf{TQA} & \textbf{WebQ} \\
    \toprule
    FiD & 101.8 & 46.5 & 65.8 & 41.83 \\
    FiD-Light & 28.3 & 36.3 & 54.5 & 30.8 \\
    FiD-LSA & 29.5 & 45.8 & 65.3 & 41.0 \\
    \bottomrule
\end{tabular}
\caption{Time per sample (ms) and QA exact match for FiD, FiD-Light, and FiD Base-sized models with layer-sparse cross-attention.}
\label{table:layersparse_vs_light}
\end{table} 
First, Table \ref{table:ablations} shows that layer-sparse cross-attention significantly reduces inference cost with modest performance degradation. Separately, Table \ref{table:layersparse_vs_light} compares the inference speed and performance impact of layer-sparse cross-attention with the token-sparse cross-attention from FiD-Light. Reducing cross-attention layers and inducing encoder output sparsity by the same factor lead to similar speedups, but layer-sparse cross-attention achieves the inference speedup with much lower performance penalty. 

Note that we find a much larger performance degradation from compressing the encoder output in our setting compared to the experiments in \citet{fidlight}. Some exploratory experiments suggest that multi-task training fine-tuning on large amounts of data as done in FiD-Light may ameliorate the performance penalty from compressing encoder output; however even with such training \citet{fidlight} still report significant peformance degradation, in contrast to LSA.

Layer-sparsity over a factor of 6 incurs greater performance penalties. However, as shown in Table \ref{table:cross_attention_prop}, with LSA-6 cross-attention already makes up a small proportion of total decoder inference cost.

\paragraph{Multi-query attention}

Table \ref{table:ablations} shows that multi-query attention achieves a large cost reduction on top of layer-sparse cross-attention with minimal performance degradation, consistent with our analysis and findings from \citet{multiquery}.

\paragraph{Decoder scale}
\begin{figure}
     \centering
        \begin{tikzpicture}[scale=1.0]
            \begin{axis}[
            scale only axis,
            width=0.8\columnwidth,
            ylabel={Exact Match},
            xlabel={Time per sample (ms, log scale)},
            mark=x,
            xmode=log,
            ymajorgrids=true,
            xmajorgrids=true,
            xminorticks=true,
            grid style=dashed,
            legend columns=1,
            legend cell align=left,
            legend pos={north west},
        ]
            \addplot[color=fidocolor,mark=triangle*,mark size=3pt, line width=1] table {
6.00157812  42.37
15.825875 48.17
42.82214062 51.22
};
            \addplot[color=red,mark=*,mark size=2pt, line width=1] table {
                5.28 40.2
14.38 46.51
37.67 49.51


            };
            \node at (axis cs:6.00157812,42.37) [anchor= south, color=darkgrey] {\small{Small-Large}};            
            \node at (axis cs:15.825875,48.17) [anchor= east, color=darkgrey] {\small{Base-XL}};
            \node at (axis cs:42.82214062,51.22) [anchor= east, color=darkgrey] {\small{Large-XXL}};            

            \node at (axis cs:5.28,40.2) [anchor= west, color=darkgrey] {\small{Small}};            
            \node at (axis cs:14.38,46.51) [anchor= north west, color=darkgrey] {\small{Base}};            
            \node at (axis cs:37.67,49.51) [anchor= north, color=darkgrey] {\small{Large}};            
            
             \legend{\modelname, FiD + LSA + MQ}        
            \end{axis}
        \end{tikzpicture}
    \caption{Performance on Natural Questions dev set as a function of inference time for \modelname Small, Base and Large models with and without asymmetric decoder.}
    \label{fig:asymmetric}
\end{figure} \begin{table}[ht!]
\centering
\small
\begin{tabular}{l|ccc}
    \textbf{Model} & \textbf{NQ} & \textbf{TQA} & \textbf{WQ} \\
    \toprule

    REALM \citep{realm} & 40.4 & - & 40.7 \\    
    RAG \citep{rag} & 44.5 & 56.8 & 45.2 \\
    RETRO \citep{retro} & 45.5 & - & - \\
    T5-XXL \citep{t5ssm} & 35.2 & 51.9 & 42.8 \\
    ATLAS \citep{atlas}  & 60.4 & 79.8 & - \\    
    \midrule
    FiD-L \citep{fid}  & 51.4 & 67.6 & - \\
    FiD-L (ours) & 51.5 & 68.2 & 44.3 \\
    \modelname (L-XXL)  & 53.2 & 70.7 & 49.7 \\
    \bottomrule
\end{tabular}
\caption{Comparison of \modelname with published  results on Natural Questions, TriviaQA and WebQuestions test sets. We focus on comparing with FiD as other works enhance performance with improved retrieval (such as ATLAS), which is orthogonal to our contributions.}
\label{table:test_results}
\end{table} 
We can see in Table \ref{table:ablations} that increasing the size of the decoder leads to a significant improvement in performance at the cost of a modest increase in inference time. Figure \ref{fig:asymmetric} provides a visual comparison of the performance-inference profile for \modelname with and without asymmetric decoders and shows that asymmetric large decoders achieve a better trade-off.

\subsection{Other analysis}
\label{sec:analysis}
\paragraph{Varying input and target length}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \caption{TPS by \# of retrievals}
        \hspace{-0.36cm}
        \begin{tikzpicture}[scale=1.0]
            \begin{axis}[
            draw,
scale only axis,
            width=0.87\columnwidth,
            xlabel style = {font=\small},
mark=x,
            ymode=log,
            xtick={20, 60, 100},
y tick label style={/pgf/number format/sci},
ymajorgrids=true,
            xmajorgrids=true,
            xminorticks=true,
            grid style=dashed,
            label style={text height=6.0pt},
ticklabel style = {font=\small,text width=10pt}
            ]
            \addplot[color=fidcolor,mark=square*,mark size=2pt] table {
                10 25
                20 50
                40 102
                60 156
                80 203
                100 256
            };
            \addplot[color=cyan,mark=diamond*,mark size=3pt] table {
                10 8
                20 15
                40 30
                60 45
                80 58
                100 73
            };
            \addplot[color=red,mark=*,mark size=2pt] table {
                10 4 
                20 7
                40 14
                60 21
                80 29
                100 36
            };
            \addplot[color=fidocolor,mark=triangle*,mark size=3pt] table {
                10 5 
                20 9
                40 16
                60 23
                80 30
                100 38
            };
            \end{axis}
\end{tikzpicture}
     \end{subfigure} \hspace{-0.15cm}
     \begin{subfigure}[t]{0.23\textwidth}
        \centering
        \caption{TPS by output length}
        \begin{tikzpicture}[scale=1.0]
            \begin{axis}[
            width=0.87\columnwidth,
            scale only axis,
xlabel style = {font=\small},
mark=x,
            ymode=log,
            ytick={100, 1000},
            xtick={32, 256, 512},
ymajorgrids=true,
            xmajorgrids=true,
            xminorticks=true,
            grid style=dashed,
ticklabel style = {font=\small,text width=10pt},
            label style={text height=4.0pt},
            ]
            \addplot[color=fidcolor,mark=square*,mark size=2pt] table {
                32 102
                64 187
                128 357
                256 684
                512 1404
            };
            \addplot[color=cyan,mark=diamond*,mark size=3pt] table {
                32 30
                64 45
                128 75
                256 139
                512 279
            };
            \addplot[color=red,mark=*,mark size=2pt] table {
                32 14
                64 15
                128 16
                256 18
                512 24
            };
            \addplot[color=fidocolor,mark=triangle*,mark size=3pt] table {
                32 16
                64 18
                128 21
                256 33
                512 56
            };
            \end{axis}
\end{tikzpicture}
    \end{subfigure}
    \hfill
    \begin{tikzpicture}
        \begin{customlegend}[
            legend columns=2,
            legend style={
                align=center,
                column sep=1ex,
            },
            legend entries={FiD, FiD + LSA, FiD + LSA + MQ, \modelname}
        ]
            \addlegendimage{mark=square*,solid,color=fidcolor}
            \addlegendimage{mark=diamond*,solid,color=cyan}
            \addlegendimage{mark=*,solid,color=red}
            \addlegendimage{mark=triangle*,mark size=3pt,solid,color=fidocolor}   
        \end{customlegend}
    \end{tikzpicture}            

    \caption{Time per sample (TPS) as a function of retrieved passages (left) or the number of generated tokens (right) for Base FiD variants and \modelname-Base-XL.}
    \label{fig:speed_vs_length}
\end{figure} 
Our main results use a middle-of-the-road setting for FiD applications with a medium number of retrievals and a relatively short output, reflecting common knowledge-intensive tasks. However, it is interesting to ask how \modelname components affect speed for other settings. Figure \ref{fig:speed_vs_length} shows time per sample as a function of retrieved passages and length of the target output for each step from FiD to \modelname. 

We first note that layer-sparse cross-attention and multi-query attention are critical across all settings. For standard output length, the asymmetric decoder is cheap for any reasonable number of retrieved passages, becoming negligible as a fraction of total inference time as the number of retrievals increases. As output length increases, the cost of the disproportionately large decoder rises, although it only becomes a substantial proportion of inference time for output length of 256-512 and above. For tasks with long outputs, such as summarization, one may want to reduce the level of decoder asymmetry (e.g. Base-Large rather than Base-XL).



\paragraph{Low batch size setting}

For our primary investigation we focus on medium batch sizes (24+). There are two reasons one might care about smaller batch sizes: either because larger batches do not fit in memory or because they lead to excessive latency. The first constraint is not binding for \modelname: due to \modelname's memory efficiency we are able to fit larger batches even for the XL-XXL model, and if necessary model size can be further extended with quantization \citep{glm} and parallelism \citep{palminference}. 

For real-time serving latency can be a constraint, but in those settings it is common practice to use much smaller models which are distilled from larger teacher models \citep{distillsurvey}. The student models can utilize a higher batch size, while the teacher models do not have latency constraints, so \modelname also applies to this use case.

For rare cases where a lower batch size is required layer-sparse and multi-query attention are still important, but cannot fully eliminate the decoder as a bottleneck for inference (Table \ref{table:low_batch_size}). The  term in Equation \ref{eqn:mqopint} dominates, reflecting the fact that the model has to repeatedly load model parameters without spreading the cost over many samples.

Instead of scaling the decoder, it would be more cost-effective to apply more expensive sampling methods, because sampling methods increase the effective batch size. For example, beam search with large beams is nearly free at lower batch sizes.

\begin{table}[ht!]
\centering
\begin{tabular}{lcc}
    \textbf{Model} & \textbf{Total TPS} & \textbf{Decoder TPS} \\
    \toprule
     Vanilla FiD &  135  & 123 \\
     + LSA & 51 & 39  \\
     + MQ & 35 & 23 \\
     + Beam 16 & 35 & 23 \\
     + XL Decoder & 117 & 105 \\
    \bottomrule
\end{tabular}
\caption{Inference time per sample (ms) with batch size 1 for Base FiD with varying \modelname components.}
\label{table:low_batch_size}
\end{table} 
\paragraph{Sampling}

We do not apply beam search for our main experiments as decoder inference time is proportional to beam width for medium batch sizes and beam search does not improve performance on the considered set of tasks. Instead, we find that scaling decoder size provides a more cost-efficient way to add decoder capacity. Table \ref{table:beam} compares the performance vs time trade-offs from beam search and scaling the decoder for Natural Questions, and shows that scaling the decoder is significantly more effective. Beam search may be more important for other tasks, such as tasks with longer outputs.

\begin{table}[ht!]
\centering
\begin{tabular}{lcc}
    \textbf{Model} & \textbf{Decoder TPS} & \textbf{NaturalQ} \\
    \toprule
     FiD with LSA, MQ &  0.6  & 46.3 \\
     + Beam 4 & 2.4 & 46.2 \\
     \modelname & 2.0 & 48.2 \\
    \bottomrule
\end{tabular}
\caption{Decoder inference time (ms) and QA exact match for FiD Base models, comparing the trade-offs of beam search versus scaling decoder size.}
\label{table:beam}
\end{table} 