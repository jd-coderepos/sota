\subsection{Another ADMM}\label{sec:otherorder}
\subsubsection{Algorithm design}
In this part, we provide another ADMM algorithm, which considers different constraints compared with the algorithm we previously proposed, \textit{i.e.}, in X \textbf{update} we consider the rank constraint while in Y \textbf{update} we consider the convex constraint.

We denote $\tilde{f}(X) = \|X-\hat{X}\|_F+\mathcal{I}(X)$, $\tilde{g}(Y) = J(Y)$ and the augmented function with dual variable $U$ as
$$\tilde{L}_{\rho}(X,Y,U) = \tilde{f}(X)+\tilde{g}(Y)+\frac{\rho}{2}\|X-Y+U\|_F^2.$$
Following the ADMM procedure, we will have the following three updates in each iteration.
\begin{itemize}
\item $X$ \textbf{update}:
$$X^{k+1} = \text{argmin}_{X}\tilde{L}_{\rho}(X,Y^{k},U^{k}).$$
\item $Y$ \textbf{update}:
$$Y^{k+1} = \text{argmin}_{Y}\tilde{L}_{\rho}(X^{k+1},Y,U^{k}).$$
\item $U$ \textbf{update}:
$$U^{k+1} = U^k+X^{k+1}-Y^{k+1}.$$
\end{itemize}

\paragraph{The Subproblems}
By similar arguments, the optimization problem in $X$ update is equivalent to
\begin{subequations}
\begin{eqnarray*}\label{prob:ADMM}
\min_{X}&& \|X-\frac{1}{1+\frac{\rho}{2}}\left(\hat{X}+\frac{\rho}{2}(Y^k-U^k)\right)\|_F^2
\nonumber\\
\textrm{s.t.}
& & \text{rank}(X)\leq K.\nonumber
\end{eqnarray*}
\end{subequations}
which is to find a low-rank matrix to approximate $\frac{1}{1+\frac{\rho}{2}}\left(\hat{X}+\frac{\rho}{2}(Y^k-U^k)\right)$ and can be solved by truncated SVD.

The optimization problem in $\mathbf{Y}$ update is equivalent to
\begin{subequations}
\begin{eqnarray*}\label{prob:ADMM_Y}
 \min_{\mathbf{Y}}&& \frac{\rho}{2}\|\mathbf{X}^{k+1}-\mathbf{Y}+\mathbf{U}^k\|_F^2
\nonumber\\
\textrm{s.t.}
& & g(\mathbf{Y})\leq 0.\nonumber
\end{eqnarray*}
\end{subequations}

The problem is convex and it can be viewed as a projection of $\mathbf{X}^{k+1}+\mathbf{U}^k$ onto the convex set $\mathcal{S} = \{\mathbf{Y}|g(\mathbf{Y})\leq 0\}$. It can be solved efficiently or even have closed-form solutions.

\subsubsection{Convergence analysis}
The theoretical analysis is also based on the assumption that the dual variable will converge. With the same argument in Section~\ref{sec:convergence}, the value of primal variable is iteratively updated by the function $Y=\mathcal{H}(X)$, where
$$\mathcal{H}(X) = \underset{\text{rank}(\mathbf{Y})\leq K}{\text{argmin}}\|\mathbf{Y}-\left(\alpha\mathbf{X}+(1-\alpha)\mathbf{D}\right)\|_F^2, \quad \alpha = \frac{\rho}{2+\rho},\footnote{To make function $\mathcal{H}(\mathbf{X})$ well-defined, we assume that the minimizer of the $\mathbf{X}$ update is unique, which means that, in each iteration, $\sigma_K\neq \sigma_{K+1}$ holds for the singular value decomposition of $\left(\alpha\mathbf{X}^k+(1-\alpha)\mathbf{D}\right)$. This assumption is used in the sequel.}$$ \textit{i.e.}, $X^{k+1} = \mathcal{H}(X^k)$ An interpretation of the update is provide in Fig~\ref{fig:X_update}.
\begin{figure}
  \centering
\includegraphics[width=0.5\columnwidth]{X_update.eps}\\
  \caption{An illustration of $\mathbf{X}$ update}\label{fig:X_update}
\end{figure}



If the feasible approximation region is convex, the update rule will converge with $\|\mathbf{X}^{k+2}-\mathbf{X}^{k+1}\|_F\leq \alpha\|\mathbf{X}^{k+1}-\mathbf{X}^{k}\|_F$, but unfortunately, the set $\{X|\text{rank} \leq K\}$ is non-convex and the update is not guaranteed to converge. In the following, we provide some simple necessary conditions for the convergence of the primal variable.
\iffalse
\paragaph{A \textit{trivial} Sufficient Condition}

A trivial sufficient condition for the convergence of $\mathbf{X}^k$ is given in Proposition~\ref{proposition:contractmapping}

\begin{proposition}\label{proposition:contractmapping}
If the function $Y = \mathcal{C}(X)$ is a contractive mapping, then it has a fixed point $\mathcal{X}^*$ and the $\mathcal{X}$ update will converge to $\mathcal{X}^*$.
\end{proposition}

This proposition can be proved by the Banach fixed point theorem. However, this sufficient condition is not so useful because it is difficult to guarantee or even check whether $Y = \mathcal{C}(X)$ is contractive or not. Moreover, the author believes that this condition is hard to satisfy because this function is inherently doing projection onto a non-convex set, which is non-contractive \footnote{On the other hand, if we are doing projection onto a convex set, we will be sure that the function is strictly contractive and the update will converge.}.

\subsubsection{A \textit{useful} Necessary Condition}
\fi
Firstly, if $\mathbf{X}^k$ converges to $\mathbf{X}^*$, then $\mathbf{X}^*$ is a fixed point of $\mathcal{H}(X)$ \footnote{This result depends on the continuity of $\mathbf{H}(X)$ at $\mathbf{X}^*$, which requires a formal proof. We conjecture that $\mathcal{H}(X)$ is continuous at the points where it is well-defined.}, as shown in Proposition~\ref{proposition:continuity}.

\begin{proposition}\label{proposition:continuity}.
For any sequence $\mathbf{X}^k$ converging to $\mathbf{X}^*$ we will have
$$\mathbf{X}^* = \mathcal{H}(\mathbf{X}^*).$$
\end{proposition}



We can see that if $\mathbf{X}^k$ converges to $\mathbf{X}^*$, then $\mathbf{X}^*$ is a fixed point of $Y = \mathcal{H}(X)$. We define a set $\mathcal{F}$ as $$\mathcal{F} = \{X|\mathbf{X} = \mathcal{H}(\mathbf{X}), g(\mathbf{X})\leq 0\},$$
and provide a necessary condition for the convergence of $\mathbf{X}^k$ in Lemma~\ref{lemma:necessarycondition}.

\begin{lemma}\label{lemma:necessarycondition}
Under the condition that the dual variable $\mathbf{U}^k$ converges to $\bar{\mathbf{U}}$, if $\mathbf{X}^k$ converges to $\mathbf{X}^*$, then $\mathbf{X}^*\in\mathcal{F}$.
\end{lemma}


Lemma~\ref{lemma:necessarycondition} is useful in the sense that, with the knowledge of $\mathbf{D}$, we can restrict our attention to $\mathbf{F}$. On one hand, if we observe that the dual variable converges, we do not need to keep updating since the only possible limiting points are in $\mathcal{F}$; on the other hand, if we find $\mathcal{F}$ is empty, then we can say that the primal variable will not converge and there is no need to spend more effort on updating.


\paragraph{the Fixed Points of $\mathcal{H}(X)$}

It turns out that the fixed points of $Y = \mathcal{H}(X)$ are highly related to the singular value decomposition of $\mathbf{D}$, which we will show in this section.

To warm up, we firstly provide one fixed point in Corollary~\ref{corrollary:one_fixed_point}.
\begin{corollary}\label{corrollary:one_fixed_point}
If $\tilde{\mathbf{X}}$ is the optimal solution of $\underset{\text{rank}(X)\leq K}{\min}\|X-D\|_F$ by truncated SVD, then $\tilde{\mathbf{X}}=\mathcal{H}(\tilde{\mathbf{X}})$.
\end{corollary}

\begin{proof}
Let $\mathbf{D} = U\Sigma V^T$, we can obtain that the optimal solution is $\tilde{\mathbf{X}} = U\left[
       \begin{array}{cc}
         \Sigma_{1:K} & \mathbf{0} \\
         \mathbf{0} & 0 \\
       \end{array}
     \right] V^T$. Then,
\begin{align*}
\alpha \tilde{\mathbf{X}}+(1-\alpha)D &= \alpha U\left[
       \begin{array}{cc}
         \Sigma_{1:K} & \mathbf{0} \\
         \mathbf{0} & 0 \\
       \end{array}
     \right] V^T + (1-\alpha)U\Sigma V^T\\
& = U\left[
       \begin{array}{cc}
         \Sigma_{1:K} & \mathbf{0} \\
         \mathbf{0} & (1-\alpha)\Sigma_{K+1:N} \\
       \end{array}
     \right] V^T,
\end{align*}
which is the singular value decomposition of $\alpha \tilde{\mathbf{X}}+(1-\alpha)\mathbf{D}$. Then the value of $\mathcal{H}(\mathbf{X}^*)$ is $ U\left[
       \begin{array}{cc}
         \Sigma_{1:K} & \mathbf{0} \\
         \mathbf{0} & 0 \\
       \end{array}
     \right] V^T$.

The proof is completed.
\end{proof}

We note that even though $\tilde{\mathbf{X}}$ is a fixed point of $\mathbf{Y} = \mathcal{H}(\mathbf{X})$, it may not be a stationary point of $\mathbf{X}$ update because $\tilde{\mathbf{X}}$ may not satisfy $g(\mathbf{X})\leq 0$.


Next, we characterize all possible fixed points of $\mathcal{H}(X)$ in Corollary~\ref{corollary:all_fixed_points}.

\begin{corollary}\label{corollary:all_fixed_points}
Suppose $\mathbf{D}$ admits the singular value decomposition of $U \Sigma V^T$ with $\sigma_1\geq \sigma_2\geq ...\geq \sigma_n\geq 0$, and $\mathcal{I} = \{i_1,i_2,...,i_K\}$ is a subset of $\{1,2,...,n\}$ with size $K$. If for any $i\in \mathcal{I}$ and $j\notin \mathcal{I}$ we can have $\sigma_{i}\geq (1-\alpha)\sigma_{j}$, then $\tilde{\mathbf{X}} = \sum_{i\in\mathcal{I}}\sigma_iu_iv_i^T$ is a fixed point of $Y=\mathcal{H}(\mathbf{X})$. On the other hand, if $\tilde{\mathbf{X}}$ is a fixed point, then there must exists such a subset $\mathcal{I}$, such that $\tilde{\mathbf{X}} = \sum_{i\in\mathcal{I}}\sigma_iu_iv_i^T$.
\end{corollary}
\begin{proof}
Suppose $\tilde{\mathbf{X}}$ is a fixed point of $\mathcal{H}(\mathbf{X})$ and $\alpha \tilde{\mathbf{X}}+(1-\alpha)D$ admits the singular value decomposition of $\mathbf{U}\Sigma \mathbf{V}^T$. Then $\tilde{\mathbf{X}} = \mathbf{U}\left[
                                                                                               \begin{array}{cc}
                                                                                                 \Sigma_{1:K} & 0 \\
                                                                                                 0 & 0 \\
                                                                                               \end{array}
                                                                                             \right]\mathbf{V}^T$.
By substituting into $(1-\alpha)D+\alpha\tilde{\mathbf{X}} = \mathbf{U}\Sigma \mathbf{V}^T$ we can have
$$(1-\alpha)D + \alpha \mathbf{U}\left[\begin{array}{cc}
                                                                                                 \Sigma_{1:K} & 0 \\
                                                                                                 0 & 0 \\
                                                                                               \end{array}
                                                                                             \right]\mathbf{V}^T =  \mathbf{U}\Sigma \mathbf{V}^T.$$
Thus $$D = \mathbf{U}\left[\begin{array}{cc}
                                                                                                 \Sigma_{1:K} & 0 \\
                                                                                                 0 & \frac{\Sigma_{K+1:n}}{1-\alpha} \\
                                                                                               \end{array}
                                                                                             \right]\mathbf{V}^T.$$
However, to have the standard SVD of $D$, we need to permutate the diagonal elements of $\left[\begin{array}{cc}
                                                                                                 \Sigma_K & 0 \\
                                                                                                 0 & \frac{\Sigma_{K+1:n}}{1-\alpha} \\
                                                                                               \end{array}
                                                                                             \right]$ to make them decreasing and the corresponding columns of $\mathbf{U},\mathbf{V}$.

However, the diagonal elements of $\left[\begin{array}{cc}
                                                                                                 \Sigma_{1:K} & 0 \\
                                                                                                 0 & \Sigma_{K+1:n} \\
                                                                                               \end{array}
                                                                                             \right]$ are decreasing, which establishes the results in this corollary.
\end{proof}

We provide some remarks regarding this corollary.
\begin{itemize}
\item By Corollary~\ref{corollary:all_fixed_points}, it is easy to see that Corollary~\ref{corrollary:one_fixed_point} is true.
\item The number of fixed points is upper bounded by $n\choose{K}$ because we need to choose $K$ singular values to form one fixed point of $\mathcal{C}(X)$.
\item A larger value of $\alpha$ (meaning a larger $\rho$) will make the condition $\sigma_{i}\geq (1-\alpha)\sigma_{j}$ more difficult to satisfy and the possible fixed points will be fewer.
\end{itemize}











