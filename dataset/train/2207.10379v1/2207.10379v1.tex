

\documentclass[runningheads]{llncs}
\usepackage[svgnames]{xcolor}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{bbm}
\usepackage[hyperfootnotes=false]{hyperref}
\usepackage{engord}
\usepackage{booktabs}
\usepackage{MnSymbol}
\usepackage[accsupp]{axessibility}  






\usepackage{wrapfig}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  
\usepackage{stmaryrd}


\usepackage{marvosym}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{3101}  

\title{Temporal Saliency Query Network \\ for Efficient Video Recognition}

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Temporal Saliency Query Network for Efficient Video Recognition}



\author{
Boyang Xia\textsuperscript{*}\and
Zhihao Wang\textsuperscript{*}\and
Wenhao Wu\textsuperscript{\Letter}\and \\
Haoran Wang\and Jungong Han \\
} 






\authorrunning{Boyang Xia, Zhihao Wang, Wenhao Wu\textsuperscript{\Letter}, Haoran Wang, Jungong Han}




\institute{Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, China
\and  
University of Chinese Academy of Sciences, Beijing, China
\and 
The University of Sydney, Sydney, Australia
\and
Baidu Inc., Beijing, China
\and Computer Science Department, Aberystwyth University, SY23 3FL, UK
}
\maketitle

\begin{abstract}
Efficient video recognition is a hot-spot research topic with the explosive growth of multimedia data on the Internet and mobile devices. Most existing methods select the salient frames without awareness of the class-specific saliency scores, which neglect the implicit association between the saliency of frames and its belonging category. To alleviate this issue, we devise a novel Temporal Saliency Query (TSQ) mechanism, which introduces class-specific information to provide fine-grained cues for saliency measurement. Specifically, we model the class-specific saliency measuring process as a query-response task. For each category, the common pattern of it is employed as a query and the most salient frames are responded to it. Then, the calculated similarities are adopted as the frame saliency scores. To achieve it, we propose a \textbf{Temporal Saliency Query Network (TSQNet)} that includes two instantiations of the TSQ mechanism based on visual appearance similarities and textual event-object relations. Afterward, cross-modality interactions are imposed to promote the information exchange between them.
Finally, we use the class-specific saliencies of the most confident categories generated by two modalities to perform the selection of salient frames. Extensive experiments demonstrate the effectiveness of our method by achieving state-of-the-art results on ActivityNet, FCVID and Mini-Kinetics datasets. Our project page is at \url{https://lawrencexia2008.github.io/projects/tsqnet}.
\keywords{Video Recognition, Transformer, Temporal Sampling}
\end{abstract}
\blfootnote{*: Co-first authorship. \Letter: Corresponding author.}

\section{Introduction}
In the recent years, video understanding has drawn considerable attention from the community~\cite{mamico,ASCNet,wu2021weakly,bcnet,Wu2022TransferringTK,wang2020symbiotic} for the inexorable increase of video content on the Internet. Much progress has been achieved on the techniques to model complex video events, which can be glimpsed on promising precision on multiple benchmark datasets \cite{kay2017kinetics,ucf101}. However, computational costs grow proportionally to the recognition accuracy. This hinders the deployment of video recognition systems in resource-constraint environments, \emph{e.g.} IoT, self-driving and mobile phone applications. Hence, it is imperative to develop efficient video recognition systems to meet the rising demands of resource-efficient applications.


There are many studies that have been conducted on efficient video recognition.
One set of approaches focus on designing lightweight architectures \cite{r2plus1d,x3d}. 
At the other end of the spectrum are the dynamic inference-based approaches, which typically utilize a lightweight policy network to preview the video events, and allocate computation resources depending on the saliency of frames. 
They implant a policy network (or sampler network) inside the reinforce learning paradigm \cite{marl,adaframe,ada3d}, or adopt attention weight as a proxy of policy under the attention mechanism \cite{listentolook,smart2020}. The sampler networks are optimized under the assumption that the most salient frames/regions contribute most to  the video representation, which 
produces one-size-fits-all, \emph{i.e.,} class-agnostic frame saliency measurements. 


Actually, salient patterns are tightly associated with the category semantics. However, one-size-fits-all saliencies are not sensitive to fine-grained semantics. In particular, the sampler may overestimate the saliency of some frames which seem to be representative, but they actually belong to other categories rather than the real one of the current video. 
By contrast, a human can precisely elect the most informative frames with the aid of prior information about the probable category of the video. Because we can naturally build the logic connection between frame sequences and the common pattern of the predicted category, which can be understood as a query-response manner. For example, in \figref{Fig.head}, one can easily select the 3rd, 6th and 7th frames from the video with the assumption that the video may belongs to \textbf{Tailgate Party}. By contrast, one-sizes-fit-all sampler may also be inclined to 5th frames besides those three frames for it is quite representative for another category, \textit{e.g.} \textbf{Parking Car}.


\begin{figure}[t] \centering \includegraphics[width=0.95\textwidth]{figure/head.pdf} \caption{\textbf{A conceptual overview of the TSQ mechanism.} We cast the saliency estimation task as a query-response task. We ask each category a question: Which frames are the most salient ones for it? As we can see in the above example, we get the answer that frames 3rd, 6th, 7th are most salient for \emph{tailgate party} and the frame 5th is salient for \emph{parking car}. No frame is salient for \emph{playing chess}.}
\label{Fig.head} \end{figure}
Inspired by this observation, in this paper, we cast frame saliency measuring as a querying process, to enable discriminative class-specific saliency measurement.
To this end, we present a novel \textbf{Temporal Saliency Query (TSQ) mechanism}, which can measure saliencies of all semantic categories over frame sequence in parallel, and select the saliency of highly-confident categories as final the result. 
Concretely, we formulate class-specific saliency measuring as a query-response task. The common patterns of the various categories are adopted as \textbf{query}, and frame representations gathered by category-frame similarities are taken as the \textbf{response}. Then, the category-frame similarities can be regarded as frame saliencies. A conceptual overview of the TSQ mechanism is shown in \figref{Fig.head}. Specifically, we use cross attention in Transformer Decoder \cite{transformer} to model many-to-many category-frame similarities in parallel. 
On one hand, we represent the common pattern of a category, namely TSQ embedding, by visual prototypes. And the query process is performed over the visual feature of the frame sequence. On the other hand, to handle large intra-class variations of visual appearance, we measure saliency by textual event-object relations for complementary information. As we know, the objects in videos are closely associated with the category annotation of video. For instance, \textbf{cake, candle and balloon with birthday party}. To model the semantic relationships between object and category, we first employ BERT \cite{bert} to represent the object with word embedding of its name. Taking the product as textual embedding, we construct another textual branch in the TSQ mechanism, where the query process is executed over the embedding sequence of object names. Doing so allows us to exploit prior knowledge from off-the-shelf word representations to supply cross-modal complementary clues to saliency measurement. 


















\textbf{Our contributions are summarized as:}
\textit{First}, we propose a novel Temporal Saliency Query mechanism, to alleviate the lack of class-specific information in saliency measuring for temporal sampling frameworks. 
\textit{Second}, we present an efficient multi-modal salient frame sampler \textbf{T}emporal \textbf{S}aliency \textbf{Q}uery \textbf{Net}work\textbf{ (TSQNet)}, which utilize both visual appearance feature and textual feature obtained by object name embeddings to measure frame saliencies in a unified framework. \textit{Third}, we conduct extensive experiments on three large-scale datasets, \emph{i.e.,} ActivityNet, FCVID and Mini-Kinetics, which show TSQNet significantly outperforms the state-of-the-art approaches on accuracy-efficiency trade-off.













\section{Related Work}
\noindent\textbf{Efficient Video Recognition.} 
Efficient video recognition approaches can be roughly categorized into two directions. The first focus on elaborating new lightweight architectures by decomposing 3D convolution operations into 2D and 1D ones \cite{r2plus1d,s3d,mvf}, channel shifting in 2D CNNs \cite{tsm}, \emph{etc.} 
The others are based on a dynamic inference mechanism~\cite{wu2020dynamic,nsnet,AKnet}, which allocates computation resources on a per-sample basis based on the saliencies of frames. 
Wu \emph{et al.} \cite{marl} utilizes multi-agent reinforce learning to model parallel frame sampling and Lin \emph{et al.} \cite{ocsampler} make one-step decision with holistic view.
Meng \emph{et al.} \cite{arnet} and Wang \emph{et al.}  \cite{adafocus,adafocusv2} focus their attention on spatial redundancy.
Panda \emph{et al.} adaptively decide modalities for video segments. Most of the previous works are mainly based on reinforce learning or attention mechanism, which are optimized with video classification objectives. However, this paradigm makes produced adaptive sampling policy class-agnostic and lacks discrimination power in fine-grained semantics. In contrast, our temporal sampling-based framework enables discriminative class-specific frame saliency measuring and shows that class-specific mechanism combined with visual-textual multi-modal complementary measuring can push the envelope of the trade-off between accuracy and computation cost.  







\noindent\textbf{Transformer in Vision Tasks.} Transformer \cite{transformer} is initially proposed to solve the long-term dependence problem in machine translation.
ViT \cite{vit}, SwinTransformer  \cite{swintransformer} and DVT \cite{dvt} split image to patches as words and bring Transformer Encoder to computer vision classification tasks. Query2label \cite{query2label} apply Transformer Decoder to multi-label classification task.
DETR \cite{detr} explore using Transformer Decoder for object detection task.  
Then Transformer Decoder for segmentation is also developed by MaskFormer  \cite{maskformer}. 
The role of Transformer Encoder in C-Tran \cite{ctran} and TransVG \cite{transvg} is to model relations between different modalities.





\section{Method}
Given a video of  frames  , our goal is to estimate the saliency score of frames  and sample top  frames with the highest saliency score to feed into a recognition network to obtain final video prediction . The overview of our method is shown in \figref{Fig.main1}. 
In this section, we first introduce the Temporal Saliency Query (TSQ) mechanism in \secref{tsq_mechanisim}. Then we elaborate on the framework of our TSQNet, including two instantiations of TSQ mechanism with visual and textual modalities and cross-modality interactions of them in \secref{TSQNet}. Finally, we present the inference procedure of TSQNet in \secref{inference}.





\begin{figure}[t] \centering \includegraphics[width=1.0\textwidth]{figure/decoder.pdf} \caption{\textbf{The overview of the Temporal Saliency Query Networks.} Frame sequence is \textbf{queried} with visual and textual TSQ embeddings of categories in VQM and TQM, then TSQNet \textbf{responded} to the queries by gathering most salient frame representations for each category. And the resultant category-frame similarities are adopted as class-specific saliency measurements for two modalities, which are post-processed and fused for final saliency scores. Top  frames with the highest saliency score are sampled and ingested to an off-the-shelf recognition network for final recognition. 
Cross-modality interaction (``Interaction'') is considered for information exchanging during training. 
The projection layer is used to reduce the dimension of input features. 
} \label{Fig.main1} \end{figure}





\subsection{Temporal Saliency Query Mechanism}\label{tsq_mechanisim}
The goal of Temporal Saliency Query (TSQ) mechanism is to perform frame saliency estimation for all categories simultaneously, which is the shared building block for two branches of visual and textual modalities in TSQNet. To expand generic saliency to class-specific version, we are potentially to ask each category a question: 
which frames are the most similar ones to the common pattern of it?
In this way, we can convert saliency estimation task to query-response task: a learnable embedding initialized with the common pattern of each category is set as the \textbf{query}, and the gathered feature from frame sequence with similarities is the \textbf{response}. Then the similarities between each category and frame sequence can be regarded as the saliency scores. We denote the learnable embedding here as \textbf{TSQ embedding}. In TSQ mechanism, a TSQ layer is proposed to enable the query-response functionality and a class-specific classifier is designed to generate coarce predictions of video category and enable discriminative learning of TSQ embedding for each category at the same time. The details of TSQ mechanism are described below.

\noindent\textbf{TSQ Layer.}
The goal of TSQ layer is to model the many-to-many category-frame similarities simultaneously and enable learning of TSQ embeddings, denoted as , under the video classification objective. To achieve this, TSQ layer is build on an attention structure in Transformer \cite{transformer}:

 is a query matrix, which is obtained by projecting each TSQ embedding  with a parameter matrix : .
 and  are the key and value matrix, which are generated by projecting frame feature sequence  with different parameter matrices : .
Then, for the TSQ embedding of the -th category , the attention weight  is produced in querying process realized by scaled dot product operation. Then the value  are gathered with attention weights  and output as response vector , which is fed to FFN of \cite{transformer}, \emph{i.e.,} sequential linear layers with residual connections. The output of FFN is ingested to a class-specific classifier to generate classification predictions. In addition to functioning as gathering weights,  represent the frame saliency measurements of the -th category for it characterizes the relations between the -th category and all  frames. In TSQ mechanism, the more discriminative  is, the better the response vectors  can represent the semantic information of the video, therefore the video classification objective can effectively optimize the this category-frame relation model.

\noindent\textbf{Class-specific Classifier.}
We denote the output of FFN as  here. The goal of class-specific classifier (``CS Classifier'' in \figref{Fig.main1}) are twofold: (1) project  to a coarse video prediction , (2) enable class-specific learning of TSQ embeddings. In class-specific classifier, instead of directly using projection layer with weight matrix  as , we apply  projection layers with different weight matrices  to each  separately. For the -th category, corresponding element of  is computed as: 
\label{spfc}
where  are the bias parameters (see Appendix for illustrative examples).
This class-specific design endows the response vector of each category with exclusive classifier, which effectively reserves the characteristic of each category and make model converge more easily.  is used for calculating regular cross entropy loss with video labels. Notice here the difference between the coarse video prediction  and the final video prediction :  is used for saliency measuring while  is the final classification result of the recognition network.




\subsection{Temporal Saliency Query Network}\label{TSQNet}
Our TSQNet mainly consists of two modules: a Visual Query module and a Textual Query module, which are instantiations of TSQ mechanism with visual and textual representations, respectively. The Visual Query module query the frame appearance sequence with the visual TSQ embedding of each category, and collect the category-frame similarities for class-specific saliency estimation. Textual Query module measures saliencies by modeling event-object (or action-object) relations on the basis of prior knowledge in off-the-shelf language models. Besides, to exchange information between two TSQ modules, cross-modality interactions are performed synchronously during training, which effective compensate scarce scene information for Textual Query module. 


\noindent\textbf{Visual Query Module (VQM).}
The goal of VQM is to generate class-specific saliency measurement from pure visual perspective, which mainly consists of a video encoder, a TSQ layer and a class-specific classifier. The video encoder is a lightweight CNN or transformer backbone, \emph{e.g.,} MobileNetv2 \cite{mobilenetv2} and Mobile-former \cite{mobileformer}, which extract features from RGB frame sequence  to feature sequence . We further use a 1D convolutional layer to reduce the feature dimension from  to , which we still denote as  for brevity. TSQ layer takes visual TSQ embedding as query, and frame sequence as key and value, to generate saliency measurements  from visual features. Class-specific classifier produce visual video coarse predictions , which is further used in the post-processing procedure of saliencies. Next we describe how we obtain visual TSQ embedding.

Following the definition in \secref{tsq_mechanisim}, visual TSQ embedding  here is a set of learnable embeddings initialized with common appearance patterns of categories. We propose a simple prototype based representation for common appearance patterns here. Prior works \cite{proto_network} find that, most of the samples belonging to the same class cluster around a prototype in feature space formed by non-linear mapping of networks. We assume that category prototypes can represent the common patterns of categories. Following definitions in \cite{proto_network}, we use the averaged features of videos belonging to each category produced by video encoder in the training set, where a video feature is obtained by top-k pooling of frame features (see Appendix \ref{appendix:imp} for details).
A 1D convolutional layer is also used to project  to the same -dimension space with , which is still represented by  hereafter.


\noindent\textbf{Textual Query Module (TQM).}
The goal of TQM is to provide knowledge-aware saliency estimation by mining generic event-object relations in videos with the help of prior knowledge in off-the-shelf language models. As observed by prior works \cite{15000object,smart2020}, the event-object (or action-object) relations are generic in videos. Although this knowledge is typically represented in knowledge graph \cite{image_text_matching}, we exploit it in a much more compact fashion, \emph{i.e.,} pre-trained language models.
It is proved that the semantic relationships between words can be effectively captured in pre-trained word representations, \emph{e.g.,} Word2Vec \cite{word2vec} and BERT \cite{bert}. To model category-frame relations, we first build a object vocabulary , on a pre-defined object list, \emph{e.g.,} ImageNet-1K category list () with word embeddings. Then we introduce a lightweight but precise object recognizer to extract appearing object scores from each frame . The frame-level object embedding based feature can obtained: . Correspondingly, the textual TSQ embedding  is initialized by pre-trained word embeddings of the category name, to align with textual feature sequence in embedding space. Similar to VQM, we add a 1D convolutional layer to  and  to reduce dimensions, which are fed into a TSQ layer and class-specific classifier for textual frame saliency measurements  and textual coarse video prediction .




\noindent\textbf{Cross-modality Interaction.}
Here we seek to enable information exchange between TSQ layers of two modalities during training and provide guidance, \emph{e.g.,} scene knowledge, from VQM to TQM. To achieve this, we design a novel \textit{swap-attention} structure, which gather the feature sequence with attention weights of the other modality in both VQM and TQM, to generate two additional response vectors: 

Then the two response vectors based on visual feature sequence  and  are ingested to subsequent layers and compute loss as  and . The same process conducted on textual features sequence renders  and . 
The swap-attention structure is conducive to TQM in two ways: (1)  help optimize scene-aware category-frame relation model (2)  help optimize scene-aware FFN and classifier. 
We weighted the existing four losses to obtain the final loss function:
\label{loss}


\subsection{Inference of TSQNet.}\label{inference}
During inference, to yield final saliency measurements, we  aggregate the generated frame saliency estimation of high-probability predicted categories for two modalities, respectively, and fuse them for final saliency results. 


\noindent \textbf{Saliency Aggregation.}
Here we only describe saliency aggregation for VQM, which is conducted for TQM with the same way. Intuitively, the higher the probability that a video belongs in -th category, the higher the priority of the -th row of attention weights in final saliency result. Following this intuition, we aggregate class-specific saliency measurements of VQM,  with the coarse video prediction . For the -th frame, the measured saliency of VQM is:
\label{reweight1}
In practive, to filter the noise brought about by the low-confidence categories, we only aggregate saliencies of top-5 categories with highest  to get final saliency measurements.


\noindent\textbf{Multi-modality Saliency Fusion.}
We fuse the saliency measurements of VQM and TQM by taking the union of the top  frames and top  frames. The number of frames used for union in two modules are controlled by pre-defined proportion  and , and the budget of selected frames .

\section{Experiments}
\subsection{Experimental Setup}
\label{exp:setup}
\noindent \textbf{Datasets.}
We evaluate our method on three large-scale datasets: ActivityNet, FCVID and Mini-Kinetics.  
ActivityNet~\cite{caba2015activitynet} contains 200 categories, it has 10024 videos for training and 4926 videos for validation, where the average duration of videos is 117 seconds.
FCVID~\cite{fcvid} includes 91,223 videos which 45,611 for training and 45,612 for validation and divided to 239 classes, where the average duration of the videos is 167 seconds.
Mini-Kinetics is a small version of Kinetics \cite{kay2017kinetics}, it consists of 121k training videos and 10k validation ones from 200 categories. Different from first two benchmarks, the videos in Mini-Kinetics are trimmed, with a average length of 10 seconds.

\noindent \textbf{Evaluation metrics.}
For all datasets above, we apply the official train-val split to experiment our method.
Following the previous work, mean Average Precision (mAP) is used as the main evaluation metric for ActivityNet and FCVID, and Top1 accuracy for Mini-Kinetics. We also evaluate the computation cost with giga floating point operations (GFLOPs). 


\noindent \textbf{Implementation details.}
We adopt MobileNetv2~\cite{mobilenetv2} trained on target datasets as the video encoder in VQM, and Efficientnet-B0~\cite{efficientnet} trained on ImageNet-1K as the object recognizer in TQM, respectively. 
For fair comparisons with previous works, we adopt three backbones in ResNet \cite{resnet} series,  \emph{e.g.,} ResNet-50, 101, 152 for recognition networks. 
For resolution of frame processed by recognition networks, we follow previous works to scale the shorter side of frames to 256 and then center cropped them to  for all datasets. On ActivityNet and FCVID, the resolution of frames processed by VQM is  and one for TQM is  \footnotemark[1].
\footnotetext[1]{Note that the total computation cost of a  frame processed by MobileNetv2 and a  frame processed by EfficientNet-B0 equals to the cost of a  frame processed by MobileNetv2, which is the common setting of previous works \cite{adafocus,smart2020}.}
On Mini-Kinetics, the resolution is 112112 for both VQM and TQM.
\tabref{tab:flops} shows decomposition of computation cost of TSQNet when adopting ResNet-50 as recognition network. Please refer to Appendix for more implementation details.



\begin{comment}
We uniformly pre-sample  frames from a video, and for those videos whose lengths are shorter than , we repeat multiple times to padding it to  frames.
Our frame sampler will select top  most salient frames in , 
 and  can be adjusted to accommodate different budgets for downstream applications. 
We use SGD optimizer with momentum of 0.9 and train model with batch size of 64 for 100 epochs. The learning rate is  , decayed by the factor of 0.1 at the 25, 50, 75 epoch. Loss ratio  and  are both 0.6. Fusion proportion  and  are 0.6 and 0.4, respectively.
\end{comment}

\begin{table*}[t]
\centering
\begin{minipage}[t]{0.45\linewidth}
\caption{Example of FLOPs computation.}
\label{tab:flops}
\centering
\scalebox{0.8}{
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{cccccc}
\toprule
Module & Arch.        & Res.             & FLOPs/F & \#F & FLOPs  \\
\midrule
Vis.Enc. & MBv2     &    188     & 0.220G   & 16  & 3.52G  \\
Obj.Rec.  & EN-B0     &    112     & 0.098G   & 16  & 1.56G  \\
Rec.Net. & RN50       &    224    & 4.109G   & 5   & 20.55G \\
VQM & - & - &  -   & -   & 0.36G  \\
TQM & - & - &  -   & -   & 0.10G  \\
\midrule
Total   & -        &    -    & -       & -   & 26.09G 
\\ \bottomrule
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth}
\caption{Comparisons with simple baselines.}
\label{table:baseline}
\centering
\setlength{\tabcolsep}{4pt}
\scalebox{0.8}{
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{ccc}
\toprule
Method & mAP (\%) & FLOPs \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
Uniform &  70.9 & 195.8G \\
Random &  70.2 & 195.8G  \\
Dense &  71.2 &  930.8G \\
MaxConf &  74.2 & 930.8G \\
MaxConf-L & 71.2 & 54.9G \\
\midrule
Ours &  74.3 & 55.3G \\
\bottomrule
\end{tabular}
}
\end{minipage}
\end{table*} \subsection{Comparison with Simple Baselines}\label{exp:baseline}
We compare our TSQNet with some simple baselines with ResNet-101 without TSN-style training as the recognizer in \tabref{table:baseline}. There are multiple rule based baselines, ``uniform'' and ``random'' stand for uniformly and randomly selecting 10 frames from a video. ``Dense'' means using all frames of a video. For ``MaxConf'', we firstly obtain the maximum confidence among all categories for every frame by applying the model along time axis, then select  frames with highest maximum confidence. We also compare with a simple sampler based baseline, ``MaxConf-L'', which is a lightweight version of ``MaxConf'' within a uniformly pre-sampled  frames, as the same as ``ours''. The  in ``MaxConf-L'' and ``ours'' is 50, and  in ``MaxConf'', ``MaxConf-L'' and ``ours'' is 5.
 Our TSQNet obviously presents the best accuracy with limited FLOPs. In fact, ``MaxConf-L'' is an ablated baseline for our class-specific motivation, which replaces our TSQ mechanism with direct frame-level classification. Comparison with ``MaxConf-L'' confirms the efficacy of our TSQ mechanism.


\subsection{Comparison with State-of-the-arts}
\label{exp:sota}

\subsubsection{Results on AcitivtyNet.}
\begin{wrapfigure}{r}{0.5\textwidth} \centering \includegraphics[width=0.45\textwidth]{table/without_uniform-cropped.pdf} \caption{Comparison of the pure VQM and the whole TSQNet with the state-of-the-art based on ResNet-101 recognition network on ActivityNet.} \label{Fig.anet_res101} \end{wrapfigure}
We compare the proposed method with recent SOTA methods on AcitivtyNet in \tabref{table:aet_sota_res50}: SCSampler~\cite{scsampler19}, AR-Net~\cite{arnet}, AdaMML~\cite{adamml}, VideoIQ~\cite{videoiq}, AdaFcous~\cite{adafocus}, Dynamic-STE~\cite{dynamicSTE} and FrameExit~\cite{frameexit}.
Experimental result shows that our method 
outperforms all existing methods with ResNet50 as the main recognition network. 
Compared with SCSampler~\cite{scsampler19} which is also a temporal sampling approach, our method surpass it by 3.7\% while using 1.6 less computation overhead, which demonstrates the discrimination power of TSQ mechanism in temporal saliency estimation.
Comparing to the state-of-the-art method based on early exiting, FrameExit~\cite{frameexit}, we still outperforms it by 0.5\%, which shows our class-specific sampler can find more discriminative frames than this sequential early exiting framework. For a more fair comparison with above pure visual based methods, we also present the results of the visual variant of TSQNet, \emph{i.e.,} `VQM-only' with comparable computes. Although without text modality, it still surpass the SotA methods, which verify the superiority of our TSM mechanism.




We further compare TSQNet with SOTA approaches in \figref{Fig.anet_res101} based on Res101 backbone. Following previous works \cite{adaframe,liteeval,listentolook,marl}, ResNet-101 without TSN-style training is used as the recognizer, as the same as in \secref{exp:baseline}. We calculate mAP under different budget , which varies from 3 to 10.
It is shown that our method achieves clearly superior efficiency-accuracy trade-off over all methods. And the result of pure VQM illustrates the efficiency of TSQ Mechanism.


\begin{table}[t]
\caption{Comparisons with SOTA efficient video recognition methods with ResNet50 as recognition backbone on AcitivtyNet. 188 and 224 here represent resolutions.}
\label{table:aet_sota_res50}
\centering
\setlength{\tabcolsep}{8pt}
\scalebox{0.85}{
\begin{tabular}{ccccc}
\toprule
Method  & Backbone & mAP(\%)  & FLOPs\\
\midrule
SCSampler~\cite{scsampler19}  & ResNet50 & 72.9 & 42.0G \\
AR-Net~\cite{arnet}  &  ResNet18,34,50      & 73.8 & 33.5G  \\
AdaMML~\cite{adamml}  &  ResNet50 & 73.9 & 94.0G \\
VideoIQ~\cite{videoiq}  & ResNet50      & 74.8 & 28.1G  \\
AdaFocus~\cite{adafocus}  &  ResNet50      & 75.0 & 26.6G  \\
Dynamic-STE~\cite{dynamicSTE} & ResNet18,50 & 75.9 & 30.5G \\
FrameExit~\cite{frameexit} & ResNet50   & 76.1 & 26.1G  \\
\midrule
Ours (VQM-only) &  ResNet50  & 75.7 & 24.3G \\
Ours (VQM-only) &  ResNet50  & 76.5 & 26.1G \\
\textbf{Ours} &  ResNet50  & \textbf{76.6} & 26.1G \\
\bottomrule
\end{tabular}
}
\end{table}
\begin{table}[t]
\caption{Comparisons with SOTA video recognition methods using ResNet-152 and more advanced recognition networks on AcitivtyNet.}
\label{table:anet_sota_res152}
\centering
\setlength{\tabcolsep}{5pt}
\scalebox{0.85}{
\begin{tabular}{ccccc}
\toprule
Method  & Backbone & Pretrain & Accuracy(\%)  & mAP(\%)\\
\midrule
P3D~\cite{p3d} & ResNet-152     & ImageNet   & 75.1 & 78.9 \\
RRA~\cite{rra} & ResNet-152     & ImageNet   & 78.8 & 83.4 \\
MARL~\cite{marl}  & ResNet-152     & ImageNet  & 79.8 & 83.8 \\
\textbf{Ours}   &  ResNet-152     & ImageNet   &  \textbf{80.0}   & \textbf{85.2}    \\ 
\midrule
ListenToLook~\cite{listentolook}     & R(2+1)D-152     & Kinetics   & - & 89.9  \\
MARL~\cite{marl}     & SEResNeXt152     & Kinetics   & - & 90.1  \\
Ours       & Swin-B     & Kinetics   & 84.7   & 91.2\\
\textbf{Ours}       & Swin-L     & Kinetics   & \textbf{88.7}   & \textbf{93.7}\\
\bottomrule
\end{tabular}
}
\end{table}

 
To verify that our TSQNet can collaborate with more backbones, we present experiment results with ResNet-152 and Swin-transformer \cite{swintransformer} family as recognition networks in \tabref{table:anet_sota_res152}. It is shown that our method outperforms all method with the same ResNet-152 backbones, and achieves absolute SOTA precision (88.7 Top-1 accuracy and 93.7 mAP) with Swin-Transformer architecture.








\noindent \textbf{Results on FCVID.}
To verify that performance promotion can be achieved on more untrimmed datasets, we also evaluate our method on FCVID in \tabref{table:fcvid}, which shows that our method outperforms competing methods in terms of accuracy while saving much computation cost. 
Compared with SOTA approach AdaFocus~\cite{adafocus}, which is motivated by selecting salient spatial regions, we achieve higher mAP with less computation, which implies that our discriminative temporal sampler can capture more salient information of videos.




\begin{table*}[t]

\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\caption{Comparison with SOTA efficient video recognition methods on FCVID. TSQNet achieves the best mAP with significant computation savings. `188' and `224' are resolutions.}
\label{table:fcvid}
\renewcommand{\arraystretch}{1.0}
\scalebox{0.9}{
\begin{tabular}{ccc}
\toprule
Methods & mAP(\%) & FLOPs \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
LiteEval~\cite{liteeval}  &  80.0  & 94.3G \\
AdaFrame~\cite{adaframe}  &  80.2 & 75.1G  \\
SCSampler~\cite{scsampler19} & 81.0 & 42.0G  \\
AR-Net~\cite{arnet} & 81.3 & 35.1G  \\
AdaFuse~\cite{adafuse} & 81.6 & 45.0G  \\
SMART~\cite{smart2020} & 82.1 & -  \\
VideoIQ~\cite{videoiq} & 82.7 & 27.0G  \\
AdaFocus~\cite{adafocus} & 83.4 & 26.6G  \\
\midrule
Ours (VQM-only) &  82.9 & \textbf{24.4G} \\
Ours (VQM-only) &  83.3 & 26.2G \\
\textbf{Ours} &  \textbf{83.5} & 26.2G \\
\bottomrule
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth}
\centering
\caption{Comparison with state-of-the-art methods on Mini-Kinetics. TSQNet achieves the best Top-1 accuracy with comparable computation cost with the most efficient methods.}
\label{table:Mini_Kinetics}
\renewcommand{\arraystretch}{1.0}
\scalebox{0.9}{
\begin{tabular}{ccc}
\toprule
Methods & Top-1(\%) & FLOPs \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
LiteEval~\cite{liteeval}  & 61.0 & 99.0G \\
SCSampler~\cite{scsampler19}  & 70.8 & 42.0G \\
AR-Net~\cite{arnet}    & 71.7 & 32.0G \\
AdaFuse~\cite{adafuse} & 72.3 & 23.0G \\
VideoIQ~\cite{videoiq}  & 72.3 & 20.4G \\
Dynamic-STE~\cite{dynamicSTE}  & 72.7 & 18.3G \\
FrameExit~\cite{frameexit}  & 72.8 & 19.7G \\
AdaFocus~\cite{adafocus}  & 72.9 & 38.6G \\
\midrule
Ours (VQM-only) &  72.9 & \textbf{18.1G} \\
\textbf{Ours} &  \textbf{73.2} & 19.7G \\
\bottomrule
\end{tabular}
}
\end{minipage} 
\end{table*} 
\begin{table*}[t]
\centering
\begin{minipage}[t]{0.45\linewidth}
\caption{Effectiveness of Class-specific designs.}
\label{table:spfc}
\setlength{\tabcolsep}{4.0pt}
\centering
\scalebox{0.8}{
\renewcommand{\arraystretch}{1.3}
\setlength{\tabcolsep}{3.0pt}
\begin{tabular}{ccc}
\toprule
Attention & Classifier & mAP(\%) \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
CA             & CA         & 74.0    \\
CS             & CA         & 68.7    \\
CS             & CS         & \textbf{74.7}  \\
\bottomrule
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth}
\centering
\caption{Effectiveness of multi-modality fusion and interactions.}
\label{table:select}
\setlength{\tabcolsep}{3.0pt}
\renewcommand{\arraystretch}{1.0}
\scalebox{0.8}{
\begin{tabular}{ccccc}
\toprule
 &  & TQM & VQM & Ours \\
\noalign{\smallskip}
\midrule
\noalign{\smallskip}
 - & - &  72.0 & 74.6 & 74.9\\
  & - & 72.5 & 74.8  &  75.1 \\
 - &  &  72.7 & 74.6 & 75.1 \\
 &  & \textbf{73.1} & \textbf{74.8} & \textbf{75.3} \\
\bottomrule
\end{tabular}
}

\end{minipage} 
\\ \vskip 2mm
\begin{minipage}[t]{0.45\linewidth}
\caption{Results of different textual feature.}
\label{table:textual}
\centering
\setlength{\tabcolsep}{6.0pt}
\renewcommand{\arraystretch}{1.15}
\scalebox{0.8}{
     \begin{tabular}{ccc}
\toprule
 Method & Usage & mAP(\%) \\
 \midrule
  W2V & Top10 & 71.2 \\
  Glove & Top10 &  72.0 \\
    \midrule
  Bert & All & 71.4 \\
  Bert  & Top10 & \textbf{72.1}  \\
 \bottomrule
 \end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth}
\centering
\caption{Impacts of initialization of TSQ embedding.}
\label{table:init}
\setlength{\tabcolsep}{3.0pt}
\renewcommand{\arraystretch}{1.15}
\scalebox{0.8}{
 \begin{tabular}{ccc}
 \toprule
 Branch & Init & mAP(\%) \\
 \midrule
\multirow{2}{*}{Vis}  & Random  & 73.8\\
 & Prototype &  \textbf{74.7}\\
\midrule
\multirow{2}{*}{Text}  & Random &  71.6\\
& Bert Emb.   & \textbf{72.1}\\
 \bottomrule
 \end{tabular}
}
\end{minipage}

\end{table*}





 
\noindent\textbf{Results on Mini-Kinetics.}
We further test the capability of TSQNet on a short trimmed video dataset \emph{i.e.,} Mini-Kinetics, which is more difficult to sample salient frames.
\tabref{table:Mini_Kinetics} demonstrates that our method achieves superior Top-1 accuracy (\textbf{73.2} \emph{v.s.} 72.9) with 2.0 less FLOPs than the state-of-the-art method \cite{adafocus}.

\noindent\textbf{Practical latency.}
We further conduct experiments of practical efficiency, which shows that our TSQNet significantly surpasses two state-of-the-art methods in inference latency, \emph{i.e.,} FrameExit \cite{frameexit} (9.8 videos/sec \emph{v.s.} \textbf{TSQNet 121.1} videos/sec) and AdaFocus \cite{adafocus} (73.8 videos/sec \emph{v.s.} \textbf{TSQNet 121.1} videos/sec) \footnotemark[1]. See Appendix \ref{appendix:speed} for more details.
\footnotetext[1]{Results are obtained on a NVIDIA 3090 GPU with an Intel Xeon E5-2650 v3 @ 2.30GHz CPU.}




\subsection{Ablation Study}
\label{exp:ablation}


In this section, we inspect different aspects of our proposed TSQNet. All ablations are completed on AcitivtyNet with ResNet-101 as recognition network. 


\noindent \textbf{Effectiveness of Class-specific Designs.}
We investigate the effectiveness of our class-specific designs in TSQ mechanism. 
\tabref{table:spfc} presents the results of class-specific (``CS'') version and class-agnostic (``CA'') version of both the attention structure and the classifier in VQM. 
For attention structure, the class-agnostic version refers to setting the size of visual TSQ embedding set  to 1. Then generated attention weight  is directly used as saliency measurement. For the classifier, the class-agnostic version is to replace existing -projection-layer classifier with a single-projection-layer one as aforementioned in \secref{tsq_mechanisim}. 
It is shown that ``CS CS'' (ours) significantly outperforms ``CA CA'' choice, which confirms the effectiveness of class-specific information in saliency measurements. Besides, ``CS CA'' choice presents an unpromising result, which demonstrates that class-specific classifier is critical for TSQ mechanism to function normally in class-specific setting. See Appendix for illustrative examples of these three settings and detailed explanation of comparison of their performance.


\noindent\textbf{Effectiveness of Multi-modal and Fusion and Interactions.}
To verify the effectiveness of fusion of VQM and TQM and multi-modality interactions, we present experimental results on two individual modalities with different usage of  and  in \tabref{table:select}. 
Without any interactions, fusion of two modules relatively impart improvements on TQM and VQM for 2.9\% and 0.3\% respectively, which verifies that two modules are complementary.  clearly elevate the performance of TQM for better category-frame modelling guided by visual features from VQM. The performance of VQM is also slightly improved by introducing textual-modality attention weights.  significantly improves the performance of TQM for better learning of textual FFN and classifier.
Finally, when both losses in CIM are added, the results of both TQM and VQM branch are further promoted, and performance of overall TSQNet is obviously improved (\textbf{75.3} \emph{v.s.} 74.9). See Appendix \ref{appendix:ablation} for detailed investigations on ratios of two losses. 



\noindent \textbf{Different Textual Feature.}
In \tabref{table:textual}, we try three commonly used word embeddings, \emph{i.e.,} Bert~\cite{bert}, Glove~\cite{glove} and Word2Vec~\cite{word2vec}, as well as two fashions of usage of object scores , \emph{i.e.,} top-10 object categories (``Top10'') and all categories (``All'').
Experimental result shows that the Bert embedding with top-10 object score gain the best result, which verifies that both the quality of word embedding and noise filtering of object category count for textual instantiation of TSQ mechanism.



\noindent\textbf{Impacts of Initialization of TSQ Embedding.}
We further explore the initialization of visual and textual TSQ embeddings in \tabref{table:init}. 
The comparison with random initialization confirms that proposed prototype based visual TSQ embedding in VQM and word embedding based textual embedding in TQM provide meaningful and effective initialization for TSQ embeddings. 


\begin{figure}[!t] \centering \includegraphics[width=1.0\textwidth]{figure/init5.pdf} \caption{\textbf{The visual and textual TSQ embeddings before and after training visualized by t-SNE. }The category embeddings with relevant semantics cluster together after training. See \secref{exp:qualitative} for detailed explanation. 
} \label{q_init} \end{figure}
\begin{figure}[!t] \centering \includegraphics[width=0.9\textwidth]{figure/visualization4.pdf} \caption{\textbf{Qualitative Evaluation of Sampled frames.} We visualized the most salient five frames of uniform and our proposed methods with two samples. The frames with \textcolor{Gold}{golden} border represent the identified salient frames by human intuition, and the frames with mask denote the non-salient ones. } \label{qualitative_example} \end{figure}




\subsection{Qualitative Analysis}\label{exp:qualitative}
We visualize visual and textual TSQ embdding by t-SNE in \figref{q_init}, which shows that our class-specific motivation is highly interpretable in terms of relationships between categories.
We also find some categories sharing similar objects are more closer in text TSQ embeddings than in visual ones. For examples, \textbf{Decorating the Christmas tree} and \textbf{Trimming branches or hedges} share tree or tree-related objects and become closer after training. This may be because TQM measure saliency based on event-object relations, which are more robust against scene variations. 
In \figref{qualitative_example}, we exhibit some qualitative examples of \textbf{Decorating Christmas tree} and \textbf{Golfing} for sampled frames by uniform baseline, TQM, VQM and TSQNet. 
In the case of \textbf{Decorating Christmas tree}, it is shown that TQM and VQM are clearly better than uniform baseline. After fusion, TSQNet can sample further more salient frames.
Another qualitative example \textbf{Golfing} is quite interesting.
VQM captures the action moments of swinging a golf club and scenes of a golf course, while TQM captures the golf balls and a golf cart. 
After fusion, TSQNet select the frames of these object, actions and scenes,
which implies our TQM and VQM can cooperate to build a robust sampler aware of object, scene and action information.



\section{Conclusions}
This paper investigates efficient video recognition by proposing a novel Temporal Saliency Query mechanism and presents an efficient multi-modal salient frame sampler Temporal Saliency Query Network.
Extensive experiments verify the proposed method significantly outperforms the state-of-the-art approaches on accuracy-efficiency trade-off.
Our proposed method is model-agnostic and can be used with various network architectures. And since our salient score is class-specific, we can easily extend our method to multi-label efficient video recognition.

\appendix
\section*{Appendix} \label{appendix}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\renewcommand{\thefigure}{A.\arabic{figure}}


\section{Further Implementation Details}\label{appendix:imp}
Here we provide some implementation details of TSQNet. 
We uniformly pre-sample  frames from a video, and for those videos whose lengths are shorter than , we repeat multiple times to padding it to  frames.
Our frame sampler will select top  most salient frames in , 
 and  can be adjusted to accommodate different budgets for downstream applications. 
We use SGD optimizer with momentum of 0.9 and train model with batch size of 64 for 100 epochs. The learning rate is  , decayed by the factor of 0.1 at the 25, 50, 75 epoch. Loss ratio  and  are both 0.6. Fusion proportion  and  are 0.6 and 0.4, respectively.
We use MobileNetv2 and EfficientNet-B0 as the video encoder in VQM and object recognizer in TQM, respectively. 
For video encoder in TQM, we use the ImageNet pre-trained model and finetuned it on target datasets \textit{e.g.,} ActivityNet, \emph{etc.,} for 10 epochs. And for Object recognizer, we directly use the officially released ImageNet model to extract object score of the ImageNet 1000 classes. We use positional embedding  on frame sequence in transformer decoder to model temporal order information.


\noindent \textbf{Prototype feature generation.}
Here we introduce how we obtain visual prototype based representation for visual TSQ embeddings initialization. 
First we apply a classifier to get the classification results for each frame. 
Then we select the top  percent of frames which can correctly predict the ground truth video category, which are then averaged to obtain the representation of each video. 
Finally, we pool all the video representations of each category to get the prototype representation of each category.
We use  for all experiments in this paper.

\noindent \textbf{Saliency score fusion.}
We describe in detail how to fuse the VQM and TQM saliency scores into the final saliency measurement. Suppose we have the VQM salient scores  and TQM salient scores  of one video. We join the top saliency score frames from two modalities to get final  salient frames. Specifically, the number of selected frames from two modalities are determined by   and , respectively, where .  
For example of selecting 5 frames from 16 frames with  situation, we select top  frames from VQM and top  ones are from TQM. And if there exists duplication, which results in a final result of less than 5 frames, the selection will be deferred in the VQM according to the descending order of  until meeting the 5-frame budget. We use  and  in experiments.

\section{Practical Inference Speed}\label{appendix:speed}
To further verify the practical efficiency of our method, we compare the inference speed with two state-of-the-art methods FrameExit \cite{frameexit} and AdaFocus \cite{adafocus} on ActivityNet. 
FrameExit \cite{frameexit} reduce computation cost by early stopping in temporal sequential prediction.
AdaFocus \cite{adafocus} suppose that the existing methods are spatially redundant, so it only selects salient areas to classify for each frames. We test the speed of two methods by running the official code released by the authors.
We evaluate the inference speed of all methods on a NVIDIA 3090 GPU with Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz CPU. Results in two different settings with batch size of 1 and 32 are reported. 
Note that FrameExit \cite{frameexit} exits from recognition at different time for different videos, so it cannot inference in batch setting, which we only report the latency with batch size = 1 here. Experimental results in \tabref{table:inference_speed} show that our method not only saves much theoretical computation complexity but also achieves the fastest actual inference speed (\textbf{121.1} video/s) on both single-sample and batch setting.
\begin{table}
\begin{center}
\caption{Comparisons of practical inference speed with state-of-the-art methods on ActivityNet.}
\label{table:inference_speed}
\begin{tabular}{ccccc}
\toprule
Method  & mAP (\% ) & FLOPs (G) & \tabincell{c}{Throughput(bs=1) \\ (videos/s)}   & \tabincell{c}{Throughput(bs=32)\videos/s)} \\
\midrule
AdaFocus \cite{adafocus} & 75.0 & 26.6 & 5.5 & 73.8 \\
FrameExit \cite{frameexit} &  76.1 & \textbf{26.1} & 9.8 & - \\
\textbf{Ours} & \textbf{76.5} & \textbf{26.1} & \textbf{17.7} & \textbf{121.1} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\section{Additional Ablation Study}\label{appendix:ablation}
In this section, more ablation experiments are conducted to supplement the main paper. ResNet-101 is utilized for the recognition network as the same as in ablation studies of the main paper.

\subsection{Ablation of class-specific classifier}
\begin{figure}[h] \centering \includegraphics[width=0.7\textwidth]{figure/CSCA.pdf} \caption{Illustrative examples of three combinations between the attention structure and the classifier of TSQNet, {i.e.} ``CS+CA'', ``CA+CA'' and ``CS+CS''.} 

\end{figure}
We show the illustrative examples of the combinations of the attention structure and the class-specific classifier under the situation of 200 class and 1280 feature dimensions in Figure~\ref{Fig.head}. ``CA + CA'' represents the class-agnostic attention structure (with 1 query) combined with the class-agnostic classifier (with a single FC). ``CS + CA'', {i.e.} our TSQNet, the class-specific attention structure (with  queries) combined with the class-agnostic classifier (with a single FC). ``CS + CS'' represents the class-specific attention structure (with  queries) combined with class-agnostic classifier (with  FCs).
It is interesting that the performance of ``CS+CA'' (68.7) is much lower than that of ``CA+CA'' (74.0), which seems like a more naive baseline than ``CS+CA''. When using the class-specific attention structure to obtain feature with shape of , the FC classifier () must have a one-to-one correspondence with each class, \emph{i.e., } ``CS+CS'' (74.7), to achieve good results. If using one  FC, \emph{i.e., } ``CS+CA'', to process all classes with the same parameters, discrimination power are insufficient and accuracy will decrease dramatically, which will be even lower than ``CA+CA''.

\subsection{Ablation study of  and }
First, we explore the appropriate values for  and , \emph{i.e.,} the ratios of  and  in \tabref{table:alpha} and \tabref{table3:beta}, respectively. We first fix  to find the best . As shown in \tabref{table:alpha}, as  increases, the performance of both TQM and TSQNet rises up to a maximum at  and then falls down. The performance of VQM remains unchanged, which demonstrates  mainly benefit TQM in interactions. Then we fix  to explore the impacts of .  As presented in \tabref{table3:beta}, the performance shows similar trend and the best results of TQM, VQM and TSQNet are achieved when  and  both equal to 0.6, which implies   benefits both TQM and VQM in interactions. After , the performance of VQM breaks down, for prohibitively large  hinders the convergence of the VQM. 

\begin{table}
\begin{minipage}[t]{0.47\linewidth}
\begin{center}
\caption{Ablation study of  when fixing .}
\label{table:alpha}
\setlength{\tabcolsep}{3.0pt}
\begin{tabular}{cccc}
\toprule
 & TSQNet& TQM  & VQM\\
\midrule
0.0 &  74.9 & 72.0 &74.6 \\
0.2 &  74.9 & 72.3 & 74.6\\
0.4 &  75.0 & 72.5 & 74.6\\
0.6 &  \textbf{75.1} & \textbf{72.7} & 74.6\\
0.8 &  74.8    & 72.6 & 74.6 \\
1.0 &  74.7   & 72.6 & 74.6 \\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\linewidth}
\begin{center}
\caption{Ablation study of  when fixing .}\label{table3:beta}
\setlength{\tabcolsep}{3.0pt}
\renewcommand{\arraystretch}{1.18}
\begin{tabular}{cccc}
\toprule
 & TSQNet & TQM & VQM \\
\midrule
0.0 &  75.1 & 72.7 & 74.6 \\
0.2 &  75.0 & 72.8 & 74.6 \\
0.4 &  75.1  & 72.8 & 74.7\\
0.6 &  \textbf{75.3}  & \textbf{73.1} & \textbf{74.8}\\
0.8 &  71.2  & 72.5 & 67.5\\
\bottomrule
\end{tabular}
\end{center}
\end{minipage}
\end{table} 
\subsection{Detailed Ablation study for transformer decoder structures}
We further ablate the structure of the standard transformer decoder, \emph{viz.,} self-attention, number of layers and heads.  Typical transformer decoder contains a self-attention layer on the top of query matrix and multiple cross-attention layers with multi-head structure. In TSQNet, we use a quite brief version of transformer decoder, containing a single-head cross-attention layer without self-attention layers, to realize TSQ layer. Next we discuss the effectiveness of this design.  

\noindent \textbf{Impact of Self-attention layer.}
On one hand, self-attention layer on queries make each TSQ embedding interact with each other, which may cause the class-specific information to mix with each other and deviates the class-specific nature of TSQ embeddings. On the other hand, self-attention layers bring in extra computation complexity of  ,  where  is the number of categories.
As shown in \tabref{table:selfattention}, adding self-attention layer presents lower performance, which demonstrates that modelling relations between TSQ embeddings of categories can not produce better saliency measuring results. 


\begin{table}
\centering
\begin{minipage}[t]{0.45\textwidth}
\centering
\caption{Ablation study of the usage of self-attention.}
\label{table:selfattention}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cc}
\toprule
Methods & mAP (\%) \\
\midrule
w/ self-atten &  74.5  \\
w/o self-atten & \textbf{74.7} \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
\centering
\caption{Ablation study of Transformer Decoder layers and heads.}
\label{table:layerhead}
\begin{tabular}{cc}
\toprule
Methods & mAP (\%) \\
\midrule
1 layer 8 head & 73.7 \\
2 layer 1 head & 73.4 \\
1 layer 1 head &  \textbf{74.7}  \\
\bottomrule
\end{tabular}
\end{minipage}
\end{table}


\noindent \textbf{Number of layers and heads.}
In TSQNet, the number of cross-attention layers and heads are both one. We present ablation experiments of more layers with more heads in \tabref{table:layerhead}. It is shown that both the increase of number of layers and heads make the mAP drop. For multiple cross-attention layers, the performance drop may attribute to lower discrepancy between queries in intermediate layers, which makes attention weights lack discrimination power between categories.
For multi-head structure, the worse results may result from attention dimension splitting operation when calculating the similarity between query matrix and key matrix, which produces separate local similarities for multiple groups in feature dimension rather than the holistic similarity of the feature dimension.


\section{Additional Qualitative Analysis}\label{appendix:qualitive_analysis}
\figref{Fig.anet} and \figref{Fig.fcvid} show more qualitative results of TSQNet on ActivityNet and FCVID. For each dataset, we selected six examples, first three of which belongs to the same category and the last three  belongs to different categories. In \figref{Fig.anet}, we can see that our approach samples significantly more salient frames than the uniform baseline. 
Similar in \figref{Fig.fcvid}, uniform baseline selects many irrelevant frames, whereas our method selects more theme-related frames.

\begin{figure}[t] \centering \includegraphics[width=1.0\textwidth]{supp_figs/anet.pdf} \caption{Qualitative Analysis on ActivityNet.} \label{Fig.anet} \end{figure}

\begin{figure}[t] \centering \includegraphics[width=1.0\textwidth]{supp_figs/fcvid.pdf} \caption{Qualitative Analysis on FCVID.} \label{Fig.fcvid} \end{figure}








\section{Additional Visualization of TSQ Embeddings}
In this section, we provide the complete t-SNE visualization for TSQ embeddings of  both the VQM and TQM on ActivityNet to supplement the local zooms visualization in Section 4.5 of the main paper. Specifically, we visualize the start and end states of training for the two modules in two different initialization fashions, \emph{i.e.,} random and proposed initialization, respectively. For VQM, we compare the random initialization with the visual common appearance feature initialization. For TQM, we compare the random initialization with the class name Bert embedding feature initialization.

\noindent \textbf{TSQ embeddings of TQM.}
\figref{Fig.bert_start} shows the visualization of TSQ embeddings of TQM with class name Bert embedding feature initialization \textbf{before} training.
\figref{Fig.bert_best} shows the visualization of TSQ embeddings of TQM with class name Bert embedding feature initialization \textbf{after} training.
\figref{Fig.random_start} shows the visualization of TSQ embeddings of TQM with random initialization \textbf{before} training.
\figref{Fig.random_best} shows the visualization of TSQ embeddings of TQM with random initialization \textbf{after} training.

\noindent \textbf{TSQ embeddings of VQM.}
\figref{Fig.vis_center_start} shows the visualization of TSQ embeddings of VQM with common appearance feature initialization \textbf{before} training.
\figref{Fig.vis_center_best} shows the visualization of TSQ embeddings of VQM with common appearance feature initialization \textbf{after} training.
\figref{Fig.vis_random_start} shows the visualization of TSQ embeddings of VQM with random initialization \textbf{before} training.
\figref{Fig.vis_random_best} shows the visualization of TSQ embeddings of VQM with random initialization \textbf{after} training.


\begin{figure}[ht] \centering \includegraphics[width=0.95\textwidth]{supp_figs/bert_start.pdf} \caption{TSQ embeddings of TQM with class name Bert initialization before training.} \label{Fig.bert_start} \end{figure}
 
\begin{figure}[t] \centering \includegraphics[width=0.95\textwidth]{supp_figs/bert_best.pdf} \caption{TSQ embeddings of TQM with class name Bert initialization after training.} \label{Fig.bert_best} \end{figure}

\begin{figure}[t] \centering \includegraphics[width=0.95\textwidth]{supp_figs/random_start.pdf} \caption{TSQ embeddings of TQM with random  initialization before training.} \label{Fig.random_start} \end{figure}

\begin{figure}[t] \centering \includegraphics[width=0.95\textwidth]{supp_figs/random_best.pdf} \caption{TSQ embeddings of TQM with random  initialization after training.} \label{Fig.random_best} \end{figure}

\begin{figure}[t] \centering \includegraphics[width=0.92\textwidth]{supp_figs/visual_center_start.pdf} \caption{TSQ embeddings of VQM with appearance  initialization before training.} \label{Fig.vis_center_start} \end{figure}

\begin{figure}[t] \centering \includegraphics[width=0.92\textwidth]{supp_figs/visual_center_best.pdf} \caption{TSQ embeddings of VQM with common appearance  initialization after training.} \label{Fig.vis_center_best} \end{figure}


\begin{figure}[t] \centering \includegraphics[width=0.95\textwidth]{supp_figs/visual_random_start.pdf} \caption{TSQ embeddings of VQM with random  initialization before training.} \label{Fig.vis_random_start} \end{figure}


\begin{figure}[t] \centering \includegraphics[width=0.95\textwidth]{supp_figs/visual_random_best.pdf} \caption{TSQ embeddings of VQM with random  initialization after training.} \label{Fig.vis_random_best} \end{figure} 



\clearpage
\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}
