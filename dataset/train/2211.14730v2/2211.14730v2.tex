\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{multirow}
\usepackage{diagbox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{enumitem}

\usepackage{xcolor}
\newcommand{\yuqi}[1]{\textcolor{green}{Yuqi: #1}}

\usepackage{xcolor}
\newcommand{\nam}[1]{\textcolor{blue}{Nam: #1}}

\usepackage{xcolor}
\newcommand{\gift}[1]{\textcolor{red}{Gift: #1}}


\title{
A Time Series is Worth 64 Words:

Long-term Forecasting with Transformers









}




\author{Yuqi Nie$^{1}$\footnotemark[1], \ Nam H. Nguyen$^{2}$ \thanks{Equal contribution.}, \ Phanwadee Sinthong$^{2}$, \ Jayant Kalagnanam$^{2}$ \\
$^{1}$Princeton University $^{2}$IBM Research\\
\texttt{ynie@princeton.edu, nnguyen@us.ibm.com, Gift.Sinthong@ibm.com,} \\
\texttt{jayant@us.ibm.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}





We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy.



\end{abstract}

\section{Introduction}

Forecasting is one of the most important tasks in time series analysis. With the rapid growth of deep learning models, the number of research works has increased significantly on this topic \citep{survey1,survey2,survey3}. Deep models have shown excellent performance not only on forecasting tasks, but also on representation learning where abstract representation can be extracted and transferred to various downstream tasks such as classification and anomaly detection to attain state-of-the-art performance.

Among deep learning models, Transformer has achieved great success on various application fields such as natural language processing (NLP) \citep{nlpsurvey}, computer vision (CV) \citep{cvsurvey}, speech \citep{speechsurvey}, and more recently time series \citep{tssurvey}, benefiting from its attention mechanism which can automatically learn the connections between elements in a sequence, thus becomes ideal for sequential modeling tasks. Informer \citep{informer}, Autoformer \citep{autoformer}, and FEDformer \citep{fedformer} are among the best variants of the Transformer model successfully applying to time series data. Unfortunately, regardless of the complicated design of Transformer-based models, it is shown in the recent paper \citep{dlinear} that a very simple linear model can outperform all of the previous models on a variety of common benchmarks and it challenges the usefulness of Transformer for time series forecasting. In this paper, we attempt to answer this question by proposing a channel-independence patch time series Transformer (PatchTST) model that contains two key designs:

\begin{itemize}[leftmargin= 20 pt,itemsep= 5 pt,topsep = 5 pt]
    \item \textbf{Patching.} Time series forecasting aims to understand the correlation between data in each different time steps. However, a single time step does not have semantic meaning like a word in a sentence, thus extracting local semantic information is essential in analyzing their connections. Most of the previous works only use point-wise input tokens, or just a handcrafted information from series. In contrast, we enhance the locality and capture comprehensive semantic information that is not available in point-level by aggregating time steps into subseries-level patches.
    
    \item \textbf{Channel-independence.} A multivariate time series is a multi-channel signal, and each Transformer input token can be represented by data from either a single channel or multiple channels. Depending on the design of input tokens, different variants of the Transformer architecture have been proposed. Channel-mixing refers to the latter case where the input token takes the vector of all time series features and projects it to the embedding space to mix information. On the other hand, channel-independence means that each input token only contains information from a single channel. This was proven to work well with CNN \citep{multichannel} and linear models \citep{dlinear}, but hasn't been applied to Transformer-based models yet.
\end{itemize}

\begin{table*}[t]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{ccccccc}
			\toprule[1.5pt]
			&\multicolumn{1}{c}{Models} & $L$ & $N$ & patch & method & MSE\\
			\midrule
			&\multirow{5}*{\shortstack{Channel-independent \\ PatchTST}}& 96& 96 &  & & 0.518 \\
			&\multicolumn{1}{c}{}& 380 & 96 & & down-sampled & 0.447 \\
            &\multicolumn{1}{c}{}& 336 & 336  & & & 0.397 \\
            &\multicolumn{1}{c}{}& 336 & 42   &\checkmark & & \uline{0.367} \\ 
            &\multicolumn{1}{c}{}& 336 & 42 &\checkmark & self-supervised  & \textbf{0.349} \\ 
            \midrule
&\multicolumn{1}{c}{Channel-mixing FEDFormer}& 336 & 336 & & & 0.597 \\       &\multicolumn{1}{c}{DLinear}& 336 & 336 & & & 0.410 \\  
			\bottomrule[1.5pt]
		\end{tabular}        
  \qquad
        \begin{tabular}{ccccc}
			\toprule[1.2pt]
			&\multicolumn{4}{c}{Running time (s) with $L=336$}  \\		
			&\multicolumn{1}{c}{Dataset} & w. patch & w.o. patch & Gain \\
			\midrule
			&\multirow{1}*{Traffic}& 464  & 10040 & x 22 \\
			&\multirow{1}*{Electricity}& 300 & 5730 & x 19 \\
            &\multirow{1}*{Weather}& 156  & 680 & x 4 \\
			\bottomrule[1.2pt]		
		\end{tabular}
	}
	\caption{A case study of multivariate time series forecasting on Traffic dataset. The prediction horizon is $96$. Results with different look-back window $L$ and number of input tokens $N$ are reported. The best result is in \textbf{bold} and the second best is \uline{underlined}. Down-sampled means sampling every 4 step and adding the last value. All the results are from supervised training except the best result which uses self-supervised learning.}
	\label{table::channel independece vs mixing}
\end{table*}

We offer a snapshot of our key results in Table \ref{table::channel independece vs mixing} by doing a case study on Traffic dataset, which consists of $862$ time series. Our model has several advantages:

\begin{enumerate}[leftmargin= 25 pt,itemsep= 5 pt,topsep = 0 pt]
    \item Reduction on time and space complexity: The original Transformer has $O(N^2)$ complexity on both time and space, where $N$ is the number of input tokens. Without pre-processing, $N$ will have the same value as input sequence length $L$, which becomes a primary bottleneck of computation time and memory in practice. By applying patching, we can reduce $N$ by a factor of the stride: $N\approx L/S$, thus reducing the complexity quadratically. Table \ref{table::channel independece vs mixing} illustrates the usefulness of patching. By setting patch length $P=16$ and
    stride $S=8$ with $L=336$, the training time is significantly reduced as much as $22$ time on large datasets.

    \item Capability of learning from longer look-back window: Table \ref{table::channel independece vs mixing} shows that by increasing look-back window $L$ from $96$ to $336$, MSE can be reduced from $0.518$ to $0.397$. However, simply extending $L$ comes at the cost of larger memory and computational usage. 
    Since time series often carries heavily temporal redundant information, some previous work tried to ignore parts of data points by using downsampling or a carefully designed sparse connection of attention \citep{logtrans} and the model still yields sufficient information to forecast well. 
    We study the case when $L=380$ and the time series is sampled every $4$ steps with the last point added to sequence, resulting in the number of input tokens being $N=96$. The model achieves better MSE score ($0.447$) than using the data sequence containing the most recent $96$ time steps ($0.518$), indicating that longer look-back window conveys more important information even with the same number of input tokens. This leads us to think of a question: is there a way to avoid throwing values while maintaining a long look-back window? Patching is a good answer to it. It can group local time steps that may contain similar values while at the same time enables the model to reduce the input token length for computational benefit. As evident in Table \ref{table::channel independece vs mixing}, MSE score is further reduced from $0.397$ to $0.367$ with patching when $L=336$.

    
    \item Capability of representation learning: With the emergence of powerful self-supervised learning techniques, sophisticated models with multiple non-linear layers of abstraction are required to capture abstract representation of the data. Simple models like linear ones \citep{dlinear} may not be preferred for that task due to its limited expressibility. With our PatchTST model, we not only confirm that Transformer is actually effective for time series forecasting, but also demonstrate the representation capability that can further enhance the forecasting performance. Our PatchTST has achieved the best MSE ($0.349$) in Table \ref{table::channel independece vs mixing}.
\end{enumerate}

We introduce our approach in more detail and conduct extensive experiments in the following sections to conclusively prove our claims. We not only demonstrate the model effectiveness with supervised forecasting results and ablation studies, but also achieves SOTA self-supervised representation learning and transfer learning performance.



\section{Related Work}
\label{gen_inst}

\textbf{Patch in Transformer-based Models.}  Transformer \citep{transformer} has demonstrated a significant potential on different data modalities. Among all applications, patching is an essential part when local semantic information is important. In NLP, BERT \citep{bert} considers subword-based tokenization \citep{wordpiece} instead of performing character-based tokenization. In CV, Vision Transformer (ViT) \citep{vit} is a milestone work that splits an image into 16$\times$16 patches before feeding into the Transformer model. The following influential works such as BEiT \citep{beit} and masked autoencoders \citep{mae} are all using patches as input. Similarly, in speech researchers are using convolutions to extract information in sub-sequence levels from raw audio input \citep{wav2vec2,hubert}. 

\textbf{Transformer-based Long-term Time Series Forecasting.} There is a large body of work that tries to apply Transformer models to forecast long-term time series in recent years. We here summarize some of them. LogTrans \citep{logtrans} uses convolutional self-attention layers with LogSparse design to capture local information and reduce the space complexity. Informer \citep{informer} proposes a ProbSparse self-attention with distilling techniques to extract the most important keys efficiently. Autoformer \citep{autoformer} borrows the ideas of decomposition and auto-correlation from traditional time series analysis methods. FEDformer \citep{fedformer} uses Fourier enhanced structure to get a linear complexity. Pyraformer \citep{pyraformer} applies pyramidal attention module with inter-scale and intra-scale connections which also get a linear complexity.

Most of these models focus on designing novel mechanisms to reduce the complexity of original attention mechanism, thus achieving better performance on forecasting, especially when the prediction length is long. However, most of the models use point-wise attention, which ignores the importance of patches. LogTrans \citep{logtrans} avoids a point-wise dot product between the key and query, but its value is still based on a single time step. Autoformer \citep{autoformer} uses auto-correlation to get patch level connections, but it is a handcrafted design which doesn't include all the semantic information within a patch. Triformer \citep{triformer} proposes patch attention, but the purpose is to reduce complexity by using a pseudo timestamp as the query within a patch, thus it neither treats a patch as a input unit, nor reveals the semantic importance behind it.

\textbf{Time Series Representation Learning.} Besides supervised learning, self-supervised learning is also an important research topic since it has shown the potential to learn useful representations for downstream tasks. There are many non-Transformer-based models proposed in recent years to learn representations in time series \citep{representation,tnc,btsf,ts2vec}. Meanwhile, Transformer is known to be an ideal candidate towards foundation models \citep{foundation} and learning universal representations. However, although people have made attempts on Transformer-based models like time series Transformer (TST) \citep{tst} and TS-TCC \citep{ts-tcc}, the potential is still not fully realized yet.

\section{Proposed Method}

\begin{figure}[t]
\begin{center}
\includegraphics[scale=0.3]{figures/fig_model.pdf}
\end{center}
\caption{PatchTST architecture. (a) Multivariate time series data is divided into different channels. They share the same Transformer backbone, but the forward processes are independent. (b) Each channel univariate series is passed through instance normalization operator and segmented into patches. These patches are used as Transformer input tokens. (c) Masked self-supervised representation learning with PatchTST where patches are randomly selected and set to zero. The model will reconstruct the masked patches. }
\label{fig::patchTST}
\end{figure}

\subsection{Model Structure}
\label{subsec::multi-channel patch transformer}
We consider the following problem: given a collection of multivariate time series samples with look-back window $L:(\vx_1, ..., \vx_L )$ where each $\vx_t$ at time step $t$ is a vector of dimension $M$, we would like to forecast $T$ future values $(\vx_{L+1}, ..., \vx_{L+T})$. Our PatchTST is illustrated in Figure \ref{fig::patchTST} where the model makes use of the vanilla Transformer encoder as its core architecture. 

\textbf{Forward Process.} We denote a $i$-th univariate series of length $L$ starting at time index $1$ as $\vx^{(i)}_{1:L} = (x_1^{(i)},..., x_L^{(i)})$ where $i=1,...,M$. The input $(\vx_1, ..., \vx_L )$ is split to $M$ univariate series $\vx^{(i)} \in \R^{1 \times L}$, where each of them is fed independently into the Transformer backbone according to our channel-independence setting. Then the Transformer backbone will provide prediction results $\hat{\vx}^{(i)}= (\hat{x}^{(i)}_{L+1},..., \hat{x}^{(i)}_{L+T}) \in \R^{1 \times T}$ accordingly .

\textbf{Patching.} Each input univariate time series $\vx^{(i)}$ is first divided into patches which can be either overlapped or non-overlapped. Denote the patch length as $P$ and the stride - the non overlapping region between two consecutive patches as $S$, then the patching process will generate the a sequence of patches $\vx^{(i)}_p \in \R^{P \times N}$ where $N$ is the number of patches, $N = \lfloor \frac{(L-P)}{S} \rfloor + 2$. Here, we pad $S$ repeated numbers of the last value $x_L^{(i)}\in \R$ to the end of the original sequence before patching. 

With the use of patches, the number of input tokens can reduce from $L$ to approximately $L/S$. This implies the memory usage and computational complexity of the attention map are quadratically decreased by a factor of $S$. Thus constrained on the training time and GPU memory, patch design can allow the model to see the longer historical sequence, which can significantly improve the forecasting performance, as demonstrated in Table \ref{table::channel independece vs mixing}.

\textbf{Transformer Encoder.} We use a vanilla Transformer encoder that maps the observed signals to the latent representations. The patches are mapped to the Transformer latent space of dimension $D$ via a trainable linear projection $\mathbf{W}_p\in \R^{D \times P}$, and a learnable additive position encoding $\mathbf{W}_{\text{pos}}\in \R^{D \times N}$ is applied to monitor the temporal order of patches: $\vx^{(i)}_d = \mathbf{W}_p \vx^{(i)}_p + \mathbf{W}_{\text{pos}}$, where $\vx^{(i)}_d \in \R^{D \times N}$ denote the input that will be fed into Transformer encoder in Figure \ref{fig::patchTST}. Then each head $h=1,...,H$ in multi-head attention will transform them into query matrices $Q^{(i)}_h=(\vx^{(i)}_d)^T\mathbf{W}^Q_h$, key matrices $ K^{(i)}_h=(\vx^{(i)}_d)^T\mathbf{W}^K_h$ and value matrices $V^{(i)}_h=(\vx^{(i)}_d)^T\mathbf{W}^V_h$, where $\mathbf{W}^Q_h,\mathbf{W}^K_h \in \R^{D \times d_k}$ and $\mathbf{W}^V_h \in \R^{D \times D}$. After that a scaled production is used for getting attention output $\mathbf{O}^{(i)}_h\in \R^{D \times N}$:
$$
(\mathbf{O}^{(i)}_h)^T = \text{Attention}(Q^{(i)}_h,K^{(i)}_h,V^{(i)}_h) = \text{Softmax} (\frac{{Q^{(i)}_h}{K^{(i)}_h}^T}{\sqrt{d_k}})V^{(i)}_h
$$

The multi-head attention block also includes BatchNorm \footnote{\citet{tst} has shown that BatchNorm outperforms LayerNorm in time series Transformer.} layers and a feed forward network with residual connections as shown in Figure \ref{fig::patchTST}. Afterwards it generates the representation denoted as $\vz^{(i)} \in \R^{D \times N}$. Finally a flatten layer with linear head is used to obtain the prediction result $\hat{\vx}^{(i)}= (\hat{x}^{(i)}_{L+1},..., \hat{x}^{(i)}_{L+T}) \in \R^{1 \times T}$.

\iffalse
\textbf{Loss Function.} We choose to use the MSE loss to measure the discrepancy between the prediction and the ground truth. The loss in each channel is $\| \hat{\vx}^{(i)}_{L+1:L+T} - \vx^{(i)}_{L+1:L+T} \|_2^2$ which is gathered and averaged over $M$ time series to get the overall objective loss: 
$$
\mathcal{L} = \E_{\vx} \frac{1}{M}\sum_{i=1}^M \| \hat{\vx}^{(i)}_{L+1:L+T} - \vx^{(i)}_{L+1:L+T} \|_2^2.
$$
\fi
\textbf{Loss Function.} We choose to use the MSE loss to measure the discrepancy between the prediction and the ground truth. The loss in each channel is gathered and averaged over $M$ time series to get the overall objective loss: 
$
\mathcal{L} = \E_{\vx} \frac{1}{M}\sum_{i=1}^M \| \hat{\vx}^{(i)}_{L+1:L+T} - \vx^{(i)}_{L+1:L+T} \|_2^2.
$

\textbf{Instance Normalization.} This technique has recently been proposed to help mitigating the distribution shift effect between the training and testing data \citep{instance,revin}. It simply normalizes each time series instance $\vx^{(i)}$ with zero mean and unit standard deviation. In essence, we normalize each $\vx^{(i)}$ before patching and the mean and deviation are added back to the output prediction.   

\subsection{Representation Learning}
\label{subsec::representation learning}

Self-supervised representation learning has become a popular approach to extract high level abstract representation from unlabelled data. In this section, we apply PatchTST to obtain useful representation of the multivariate time series. We will show that the learnt representation can be effectively transferred to forecasting tasks. Among popular methods to learn representation via self-supervise pre-training, masked autoencoder has been applied successfully to NLP \citep{bert} and CV \citep{mae} domains. This technique is conceptually simple: a portion of input sequence is intentionally removed at random and the model is trained to recover the missing contents. 


Masked encoder has been recently employed in time series and delivered notable performance on classification and regression tasks \citep{tst}. The authors proposed to apply the multivariate time series to Transformer, where each input token is a vector $\vx_i$ consisting of time series values at time step $i$-th. Masking is placed randomly within each time series and across different series. However, there are two potential issues with this setting: First, masking is applied at the level of single time steps. The masked values at the current time step can be easily inferred by interpolating with the immediate proceeding or succeeding time values without high level understanding of the entire sequence, which deviates from our goal of learning important abstract representation of the whole signal. \citet{tst} proposed complex randomization strategies to resolve the problem in which groups of time series with different sizes are randomly masked. 





Second, the design of the output layer for forecasting task can be troublesome. Given the representation vectors  $\vz_t \in \R^D$ corresponding to all $L$ time steps, mapping these vectors to the output containing $M$ variables each with prediction horizon $T$ via a linear map requires a parameter matrix $\mathbf{W}$ of dimension $(L \cdot D) \times (M \cdot T)$. This matrix can be particularly oversized if either one or all of these four values are large. This may cause overfitting when the number of downstream training samples is scarce. 

Our proposed PatchTST can naturally overcome the aforementioned issues. As shown in Figure \ref{fig::patchTST}, we use the same Transformer encoder as the supervised settings. The prediction head is removed and a $D \times P$ linear layer is attached. As opposed to supervised model where patches can be overlapped, we divide each input sequence into regular non-overlapping patches. It is for convenience to ensure observed patches do not contain information of the masked ones. We then select a subset of the patch indices uniformly at random and mask the patches according to these selected indices with zero values. The model is trained with MSE loss to reconstruct the masked patches.

We emphasize that each time series will have its own latent representation that are cross-learned via a shared weight mechanism. This design can allow the pre-training data to contain different number of time series than the downstream data, which may not be feasible by other approaches. 


\section{Experiments}

\subsection{Long-term Time series forecasting}
\label{subsection::time series forecasting}

\textbf{Datasets.} We evaluate the performance of our proposed PatchTST on $8$ popular datasets, including Weather, Traffic, Electricity, ILI and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). These datasets have been extensively utilized for benchmarking and publicly available on \citep{autoformer}. The statistics of those datasets are summarized in Table \ref{tab:data}. We would like to highlight several large datasets: Weather, Traffic, and Electricity. They have many more number of time series, thus the results would be more stable and less susceptible to overfitting than other smaller datasets.



\begin{table}[htbp!]
\centering
\scalebox{0.8}{
\begin{tabular}{c|cccccccc}
\toprule  
Datasets & Weather  & Traffic & Electricity & ILI & ETTh1 & ETTh2 & ETTm1 & ETTm2 \\
\midrule  
Features & 21 & 862 & 321 & 7 & 7 & 7 & 7 & 7 \\
Timesteps & 52696 & 17544 & 26304 & 966 & 17420 & 17420 & 69680 & 69680 \\
\bottomrule 
\end{tabular}}
\caption{Statistics of popular datasets for benchmark.}
\label{tab:data}
\end{table}



\textbf{Baselines and Experimental Settings.} We choose the SOTA Transformer-based models, including FEDformer \citep{fedformer}, Autoformer \citep{autoformer}, Informer \citep{informer}, Pyraformer \citep{pyraformer}, LogTrans \citep{logtrans}, and a recent non-Transformer-based model DLinear \citep{dlinear} as our baselines. All of the models are following the same experimental setup with prediction length $T\in \{24, 36, 48, 60\}$ for ILI dataset and $T\in \{96, 192, 336, 720\}$ for other datasets as in the original papers. We collect baseline results from \citet{dlinear} with the default look-back window $L=96$ for Transformer-based models, and $L=336$ for DLinear. But in order to avoid under-estimating the baselines, we also run FEDformer, Autoformer and Informer for six different look-back window $L\in \{24, 48, 96, 192, 336, 720\}$, and always choose the best results to create strong baselines. More details about the baselines could be found in Appendix \ref{append::baseline}. We calculate the MSE and MAE of multivariate time series forecasting as metrics. 

\textbf{Model Variants.} We propose two versions of PatchTST in Table \ref{tab:supervised}. PatchTST/64 implies the number of input patches is 64, which uses the look-back window $L=512$. PatchTST/42 means the number of input patches is 42, which has the default look-back window $L=336$. Both of them use patch length $P=16$ and stride $S=8$. Thus, we could use PatchTST/42 as a fair comparison to DLinear and other Transformer-based models, and PatchTST/64 to explore even better results on larger datasets. More experimental details are provided in Appendix \ref{append:exp}.

\textbf{Results.} Table \ref{tab:supervised} shows the multivariate long-term forecasting results. Overall, our model outperform all baseline methods. Quantitatively, compared with the best results that Transformer-based models can offer, PatchTST/64 achieves an overall $\mathbf{21.0\%}$ reduction on MSE and $\mathbf{16.7\%}$ reduction on MAE, while PatchTST/42 attains an overall $\mathbf{20.2\%}$ reduction on MSE and $\mathbf{16.4\%}$ reduction on MAE. Compared with the DLinear model, PatchTST can still outperform it in general, especially on large datasets (Weather, Traffic, Electricity) and ILI dataset. We also experiment with univariate datasets where the results are provided in Appendix \ref{append:uni}. 


\linespread{1.2}
\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|cc|cc|cc|ccc}
			\cline{2-19}
			&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{PatchTST/64}& \multicolumn{2}{c|}{PatchTST/42}& \multicolumn{2}{c|}{DLinear}& \multicolumn{2}{c|}{FEDformer}& \multicolumn{2}{c|}{Autoformer}& \multicolumn{2}{c|}{Informer}& \multicolumn{2}{c|}{Pyraformer}& \multicolumn{2}{c}{LogTrans}& \\
			\cline{2-19}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & \textbf{0.149} & \textbf{0.198} & \uline{0.152} & \uline{0.199} & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 & 0.896 & 0.556 & 0.458 & 0.490 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.194} & \textbf{0.241} & \uline{0.197} & \uline{0.243} & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434 & 0.622 & 0.624 & 0.658 & 0.589 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.245} & \textbf{0.282} & \uline{0.249} & \uline{0.283} & 0.265 & 0.319 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543  & 0.739 & 0.753 & 0.797 & 0.652 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.314} & \textbf{0.334} & \uline{0.320} & \uline{0.335} & 0.323 & 0.362 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705 & 1.004 & 0.934 & 0.869 & 0.675 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & \textbf{0.360} & \textbf{0.249} & \uline{0.367} & \uline{0.251} & 0.410 & 0.282 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410 & 2.085 & 0.468 & 0.684 & 0.384 \\
            &\multicolumn{1}{c|}{} & 192   & \textbf{0.379} & \textbf{0.256} & \uline{0.385} & \uline{0.259} & 0.423 & 0.287 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435 & 0.867 & 0.467 & 0.685 & 0.390 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.392} & \textbf{0.264} & \uline{0.398} & \uline{0.265} & 0.436 & 0.296 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434 & 0.869 & 0.469 & 0.734 & 0.408 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.432} & \textbf{0.286} & \uline{0.434} & \uline{0.287} & 0.466 & 0.315 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466 & 0.881 & 0.473 & 0.717 & 0.396 \\
            \cline{2-19}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & \textbf{0.129} & \textbf{0.222} & \uline{0.130} & \textbf{0.222} & 0.140 & 0.237 & 0.186 & 0.302 & 0.196 & 0.313 & 0.304 & 0.393 & 0.386 & 0.449 & 0.258 & 0.357 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.147} & \textbf{0.240} & \uline{0.148} & \textbf{0.240} & 0.153 & 0.249 & 0.197 & 0.311 & 0.211 & 0.324 & 0.327 & 0.417 & 0.386 & 0.443 & 0.266 & 0.368 \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.163} & \textbf{0.259} & \uline{0.167} & \uline{0.261} & 0.169 & 0.267 & 0.213 & 0.328 & 0.214 & 0.327 & 0.333 & 0.422 & 0.378 & 0.443 & 0.280 & 0.380 \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.197} & \textbf{0.290} & \uline{0.202} & \uline{0.291} & 0.203 & 0.301 & 0.233 & 0.344 & 0.236 & 0.342 & 0.351 & 0.427 & 0.376 & 0.445 & 0.283 & 0.376 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ILI}}& 24    & \textbf{1.319} & \textbf{0.754} & \uline{1.522} & \uline{0.814} & 2.215 & 1.081 & 2.624 & 1.095 & 2.906 & 1.182 & 4.657 & 1.449  & 1.420 & 2.012 & 4.480 & 1.444 \\
            &\multicolumn{1}{c|}{} & 36    & \uline{1.579} & \uline{0.870} & \textbf{1.430} & \textbf{0.834} & 1.963 & 0.963 & 2.516 & 1.021 & 2.585 & 1.038 & 4.650 & 1.463 & 7.394 & 2.031 & 4.799 & 1.467 \\
            &\multicolumn{1}{c|}{}& 48    & \textbf{1.553} & \textbf{0.815} & \uline{1.673} & \uline{0.854} & 2.130 & 1.024 & 2.505 & 1.041 & 3.024 & 1.145 & 5.004 & 1.542 & 7.551 & 2.057 & 4.800 & 1.468 \\
            &\multicolumn{1}{c|}{}& 60    & \textbf{1.470} & \textbf{0.788} & \uline{1.529} & \uline{0.862} & 2.368 & 1.096 & 2.742 & 1.122 & 2.761 & 1.114 & 5.071 & 1.543 & 7.662 & 2.100 & 5.278 & 1.560 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96    & \textbf{0.370} & \uline{0.400} & \uline{0.375} & \textbf{0.399} & \uline{0.375} & \textbf{0.399} & 0.376 & 0.415 & 0.435 & 0.446 & 0.941 & 0.769 & 0.664 & 0.612 & 0.878 & 0.740 \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.413} & 0.429 & 0.414 & \uline{0.421} & \textbf{0.405} & \textbf{0.416} & 0.423 & 0.446 & 0.456 & 0.457 & 1.007 & 0.786 & 0.790 & 0.681 & 1.037 & 0.824 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.422} & \uline{0.440} & \uline{0.431} & \textbf{0.436} & 0.439 & 0.443 & 0.444 & 0.462 & 0.486 & 0.487 & 1.038 & 0.784 & 0.891 & 0.738 & 1.238 & 0.932 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.447} & \uline{0.468} & \uline{0.449} & \textbf{0.466} & 0.472 & 0.490 & 0.469 & 0.492 & 0.515 & 0.517 & 1.144 & 0.857 & 0.963 & 0.782 & 1.135 & 0.852 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & \textbf{0.274} & \uline{0.337} & \textbf{0.274} & \textbf{0.336} & 0.289 & 0.353 & 0.332 & 0.374 & 0.332 & 0.368 & 1.549 & 0.952 & 0.645 & 0.597 & 2.116 & 1.197 \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.341} & \uline{0.382} & \textbf{0.339} & \textbf{0.379} & 0.383 & 0.418 & 0.407 & 0.446 & 0.426 & 0.434 & 3.792 & 1.542 & 0.788 & 0.683 & 4.315 & 1.635 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.329} & \uline{0.384} & \uline{0.331} & \textbf{0.380} & 0.448 & 0.465 & 0.400 & 0.447 & 0.477 & 0.479 & 4.215 & 1.642 & 0.907 & 0.747 & 1.124 & 1.604 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.379} & \textbf{0.422} & \textbf{0.379} & \textbf{0.422} & 0.605 & 0.551 & 0.412 & 0.469 & 0.453 & 0.490 & 3.656 & 1.619 & 0.963 & 0.783 & 3.188 & 1.540 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & \uline{0.293} & 0.346 & \textbf{0.290} & \textbf{0.342} & 0.299 & \uline{0.343} & 0.326 & 0.390 & 0.510 & 0.492 & 0.626 & 0.560 & 0.543 & 0.510 & 0.600 & 0.546 \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.333} & 0.370 & \textbf{0.332} & \uline{0.369} & 0.335 & \textbf{0.365} & 0.365 & 0.415 & 0.514 & 0.495 & 0.725 & 0.619 & 0.557 & 0.537 & 0.837 & 0.700 \\
            &\multicolumn{1}{c|}{}& 336   & \uline{0.369} & \uline{0.392} & \textbf{0.366} & \uline{0.392} & \uline{0.369} & \textbf{0.386} & 0.392 & 0.425 & 0.510 & 0.492 & 1.005 & 0.741 & 0.754 & 0.655 & 1.124 & 0.832 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.416} & \textbf{0.420} & \uline{0.420} & 0.424 & 0.425 & \uline{0.421} & 0.446 & 0.458 & 0.527 & 0.493 & 1.133 & 0.845 & 0.908 & 0.724 & 1.153 & 0.820 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTm2}} & 96    & \uline{0.166} & \uline{0.256} & \textbf{0.165} & \textbf{0.255} & 0.167 & 0.260 & 0.180 & 0.271 & 0.205 & 0.293 & 0.355 & 0.462& 0.435 & 0.507 & 0.768 & 0.642 \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.223} & \uline{0.296} & \textbf{0.220} & \textbf{0.292} & 0.224 & 0.303 & 0.252 & 0.318 & 0.278 & 0.336 & 0.595 & 0.586 & 0.730 & 0.673 & 0.989 & 0.757 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.274} & \textbf{0.329} & \uline{0.278} & \textbf{0.329} & 0.281 & 0.342 & 0.324 & 0.364 & 0.343 & 0.379 & 1.270 & 0.871 & 1.201 & 0.845 & 1.334 & 0.872 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.362} & \textbf{0.385} & \uline{0.367} & \textbf{0.385} & 0.397 & 0.421 & 0.410 & 0.420 & 0.414 & 0.419 & 3.001 & 1.267 & 3.625 & 1.451 & 3.048 & 1.328 \\
			\cline{2-19}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results with supervised PatchTST. We use prediction lengths $T\in \{24, 36, 48, 60\}$ for ILI dataset and $T\in \{96, 192, 336, 720\}$ for the others. The best results are in \textbf{bold} and the second best are \uline{underlined}.}
	\label{tab:supervised}
\end{table*}
\linespread{1}


\subsection{Representation learning}

In this section, we conduct experiments with masked self-supervised learning where we set the patches to be non-overlapped. Otherwise stated, across all representation learning experiments the input sequence length is chosen to be $512$ and patch size is set to $12$, which results in $42$ patches. We consider high masking ratio where $40 \%$ of the patches are masked with zero values. We first apply self-supervised pre-training on the datasets mentioned in Section \ref{subsection::time series forecasting} for $100$ epochs. Once the pre-trained model on each dataset is available, we perform supervised training to evaluate the learned representation with two options: (a) linear probing and (b) end-to-end fine-tuning. With (a), we only train the model head for $20$ epochs while freezing the rest of the network; With (b), we apply linear probing for $10$ epochs to update the model head and then end-to-end fine-tuning the entire network for $20$ epochs. It was proven that a two-step strategy with linear probing followed by fine-tuning can outperform only doing fine-tuning directly \citep{lp-ft}. We select a few representative results on below, and a full benchmark can be found in Appendix \ref{append:self-sup}.

\textbf{Comparison with Supervised Methods.} Table \ref{table::Fine-tuning performance} compares the performance of PatchTST (with fine-tuning, linear probing, and supervising from scratch) with other supervised method. As shown in the table, on large datasets our pre-training procedure contributes a clear improvement compared to supervised training from scratch. By just fine-tuning the model head (linear probing), the forecasting performance is already comparable with training the entire network from scratch and better than DLinear. The best results are observed with end-to-end fine-tuning. Self-supervised PatchTST significantly outperforms other Transformer-based models on all the datasets. 


\begin{table*}[t]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cccccc|ccccccccc}
			\cline{2-17}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{6}{c|}{PatchTST}&  \multicolumn{2}{c|}{\multirow{2}{*}{DLinear}}& \multicolumn{2}{c|}{\multirow{2}{*}{FEDformer}}& \multicolumn{2}{c|}{\multirow{2}{*}{Autoformer}}& \multicolumn{2}{c}{\multirow{2}{*}{Informer}}& \\
			\cline{4-9}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{Fine-tuning}& \multicolumn{2}{c|}{Lin. Prob.}& \multicolumn{2}{c|}{Sup. }& \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c}{}& \\
			\cline{2-17}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & \textbf{0.144} & \textbf{0.193} & 0.158 & 0.209  & \uline{0.152} & \uline{0.199} & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.190} & \textbf{0.236} & 0.203 & 0.249  & \uline{0.197} & \uline{0.243} & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434  \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.244} & \textbf{0.280} & 0.251 & 0.285  & \uline{0.249} & \uline{0.283} & 0.265 & 0.319 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543   \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.320} & \textbf{0.335} & 0.321 & 0.336  & \textbf{0.320} & \textbf{0.335} & 0.323 & 0.362 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705   \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96 & \textbf{0.352} & \textbf{0.244} & 0.399 & 0.294  & \uline{0.367} & \uline{0.251} & 0.410 & 0.282 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410  \\
            &\multicolumn{1}{c|}{}& 192  & \textbf{0.371} & \textbf{0.253} & 0.412 & 0.298  & \uline{ 0.385} & \uline{0.259} & 0.423 & 0.287 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435  \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.381} & \textbf{0.257} & 0.425 & 0.306 & \uline{0.398}  & \uline{0.265} & 0.436 & 0.296 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434  \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.425} & \textbf{0.282} & 0.460 & 0.323 & \uline{0.434}  & \uline{0.287} & 0.466 & 0.315 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466  \\
            \cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96 & \textbf{0.126} & \textbf{0.221} & 0.138 & 0.237  & \uline{0.130} & \textbf{0.222} & 0.140 & 0.237 & 0.186 & 0.302 & 0.196 & 0.313 & 0.304 & 0.393  \\
			&\multicolumn{1}{c|}{}& 192 & \textbf{0.145} & \textbf{0.238} & 0.156 & 0.252  & \uline{0.148} & \uline{0.240} & 0.153 & 0.249 & 0.197 & 0.311 & 0.211 & 0.324 & 0.327 & 0.417  \\
			&\multicolumn{1}{c|}{}& 336 & \textbf{0.164} & \textbf{0.256} & 0.170 & 0.265  & \uline{0.167} & \uline{0.261} & 0.169 & 0.267 & 0.213 & 0.328 & 0.214 & 0.327 & 0.333 & 0.422  \\
			&\multicolumn{1}{c|}{}& 720 & \textbf{0.193} & \textbf{0.291} & 0.208 & 0.297  & \uline{0.202} & \textbf{0.291} & 0.203 & 0.301 & 0.233 & 0.344 & 0.236 & 0.342 & 0.351 & 0.427 \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results with self-supervised PatchTST. We use prediction lengths $T\in \{96, 192, 336, 720\}$. The best results are in \textbf{bold} and the second best are \uline{underlined}.}
	\label{table::Fine-tuning performance}
\end{table*}



\textbf{Transfer Learning.} We test the capability of transfering the pre-trained model to downstream tasks. In particular, we pre-train the model on Electricity dataset and fine-tune on other datasets. We observe from Table \ref{table::transferring performance} that overall the fine-tuning MSE is lightly worse than pre-training and fine-tuning on the same dataset, which is reasonable. The fine-tuning performance is also worse than supervised training in some cases. However, the forecasting performance is still better than other models. Note that as opposed to supervised PatchTST where the entire model is trained for each prediction horizon, here we only retrain the linear head or the entire model for much fewer epochs, which results in significant computational time reduction.


\begin{table*}[t]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cccccc|ccccccccc}
			\cline{2-17}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{6}{c|}{PatchTST}&  \multicolumn{2}{c|}{\multirow{2}{*}{DLinear}}& \multicolumn{2}{c|}{\multirow{2}{*}{FEDformer}}& \multicolumn{2}{c|}{\multirow{2}{*}{Autoformer}}& \multicolumn{2}{c}{\multirow{2}{*}{Informer}}& \\
			\cline{4-9}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{Fine-tuning}& \multicolumn{2}{c|}{Lin. Prob.}& \multicolumn{2}{c|}{Sup. }& \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c}{}& \\
			\cline{2-17}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & \textbf{0.145} & \textbf{0.195} & 0.163 & 0.216  & \uline{0.152} & \uline{0.199} & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.193} & \textbf{0.243} & 0.205 & 0.252  & \uline{0.197} & \textbf{0.243} & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434  \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.244} & \textbf{0.280} & 0.253 & 0.289  & \uline{0.249} & \uline{0.283} & 0.265 & 0.319 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543   \\
            &\multicolumn{1}{c|}{}& 720 & 0.321 & 0.337 & \textbf{0.320} & \uline{0.336}  & \textbf{0.320} & \textbf{0.335} & 0.323 & 0.362 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705   \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96 & \uline{0.388} & \uline{0.273} & 0.400 & 0.288  & \textbf{0.367} & \textbf{0.251} & 0.410 & 0.282 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410  \\
            &\multicolumn{1}{c|}{}& 192  & \uline{0.400} & \uline{0.277} & 0.412 & 0.293 & \textbf{0.385} & \textbf{0.259} & 0.423 & 0.287 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435  \\
            &\multicolumn{1}{c|}{}& 336   & \uline{0.408} & \uline{0.280} & 0.425 & 0.307 & \textbf{0.398}  & \textbf{0.265} & 0.436 & 0.296 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434  \\
            &\multicolumn{1}{c|}{}& 720   & \uline{0.447} & \uline{0.310} &0.457  & 0.317 & \textbf{0.434}  & \textbf{0.287} & 0.466 & 0.315 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466  \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is transferred to other datasets. The best results are in \textbf{bold} and the second best are \uline{underlined}.}
	\label{table::transferring performance}
\end{table*}


\textbf{Comparison with Other Self-supervised Methods.} We compare our self-supervised model with BTSF \citep{btsf}, TS2Vec \citep{ts2vec}, TNC \citep{tnc}, and TS-TCC \citep{ts-tcc} which are among the state-of-the-art contrastive learning representation methods for time series \footnote{We cite results of TS2Vec from \citep{ts2vec} and \{BTSF,TNC,TS-TCC\} from \citep{btsf}.}. We test the forecasting performance on ETTh1 dataset, where we only apply linear probing after the learned representation is obtained (only fine-tune the last linear layer) to make the comparison fair. Results from Table \ref{table::compare with self-sup} strongly indicates the superior performance of PatchTST, both from pre-training on its own ETTh1 data (self-supervised columns) or pre-training on Traffic (transferred columns).




\linespread{1.1}
\begin{table*}[!t]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|c|cccc|ccccccccc}
		    \cline{2-16}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multirow{2}{*}{IMP.}& \multicolumn{4}{c|}{PatchTST}&  \multicolumn{2}{c|}{\multirow{2}{*}{BTSF}}&\multicolumn{2}{c|}{\multirow{2}{*}{TS2Vec}}&\multicolumn{2}{c|}{\multirow{2}{*}{TNC}}&\multicolumn{2}{c}{\multirow{2}{*}{TS-TCC}}& \\
			\cline{5-8}
			&\multicolumn{2}{c|}{}& &\multicolumn{2}{c|}{Transferred}& \multicolumn{2}{c|}{Self-supervised}& \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \\
			\cline{2-16}
			&\multicolumn{2}{c|}{Metrics}& MSE &MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-16}
			&\multirow{5}*{\rotatebox{90}{ETTh1}} & 24  & 42.3$\%$ &\textbf{0.312} & \textbf{0.362} & \uline{0.322} & \uline{0.369} & 0.541 & 0.519 & 0.599 & 0.534 & 0.632 & 0.596 & 0.653 & 0.610 \\
            &\multicolumn{1}{c|}{}& 48 & 44.7$\%$ &\textbf{0.339} & \textbf{0.378} & \uline{0.354} & \uline{0.385} & 0.613 & 0.524 & 0.629 & 0.555 & 0.705 & 0.688 & 0.720 & 0.693 \\
            &\multicolumn{1}{c|}{}& 168 & 34.5$\%$ &\uline{0.424} & \uline{0.437} & \textbf{0.419} & \textbf{0.424} & 0.640 & 0.532 & 0.755 & 0.636 & 1.097 & 0.993 & 1.129 & 1.044 \\
            &\multicolumn{1}{c|}{}& 336 & 48.5$\%$ & \uline{0.472} & \uline{0.472} & \textbf{0.445} & \textbf{0.446} & 0.864 & 0.689 & 0.907 & 0.717 & 1.454 & 0.919 & 1.492 & 1.076 \\
            &\multicolumn{1}{c|}{}& 720 & 48.8$\%$ & \uline{0.508} & \uline{0.507} & \textbf{0.487} & \textbf{0.478} & 0.993 & 0.712 & 1.048 & 0.790 & 1.604 & 1.118 & 1.603 & 1.206 \\
			\cline{2-16}			
		\end{tabular}
	}
	\caption{Representation learning methods comparison. Column name \textit{transferred} implies pre-training PatchTST on Traffic dataset and transferring the representation to ETTh1, while \textit{self-supervised} implies both pre-training and linear probing on ETTh1. The best and second best results are in \textbf{bold} and \uline{underlined}. IMP. denotes the improvement on best results of PatchTST compared to that of baselines, which is in the range of $34.5\%$ to $48.8\%$ on various prediction lengths.}
	\label{table::compare with self-sup}
\end{table*}
\linespread{1}


\subsection{Ablation study}
\label{sec::ablation}

\textbf{Patching and Channel-independence.} We study the effects of patching and channel-independence in Table \ref{tab:ablation}. We include FEDformer as the SOTA benchmark for Transformer-based model.  By comparing results with and without the design of patching / channel-independence accordingly, one can observe that both of them are important factors in improving the forecasting performance. The motivation of patching is natural; furthermore this technique improves the running time and memory consumption as shown in Table \ref{table::channel independece vs mixing} due to shorter Transformer sequence input. Channel-independence, on the other hand, may not be as intuitive as patching is in terms of the technical advantages. Therefore, we provide an in-depth analysis on the key factors that make channel-independence more preferable in Appendix \ref{append::ci}. More ablation study results are available in Appendix \ref{append:abla}.



\linespread{1.1}
\begin{table*}[!htbp]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|ccc}
		    \cline{2-13}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{8}{c|}{PatchTST}&  \multicolumn{2}{c}{\multirow{2}{*}{FEDformer}}& \\
			\cline{4-11}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{P+CI}& \multicolumn{2}{c|}{CI}& \multicolumn{2}{c|}{P}& \multicolumn{2}{c|}{Original} & \multicolumn{2}{c}{}&\\
			\cline{2-13}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & \textbf{0.152} & \textbf{0.199} & 0.164 & 0.213 & 0.168 & 0.223 & 0.177 & 0.236 & 0.238 & 0.314 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.197} & \textbf{0.243} & 0.205 & 0.250 & 0.213 & 0.262 & 0.221 & 0.270 & 0.275 & 0.329 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.249} & \textbf{0.283} & 0.255 & 0.289 & 0.266 & 0.300 & 0.271 & 0.306 & 0.339 & 0.377 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.320} & \textbf{0.335} & 0.327 & 0.343 & 0.351 & 0.359 & 0.340 & 0.353 & 0.389 & 0.409 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & \textbf{0.367} & \textbf{0.251} & 0.397 & 0.271 & 0.595 & 0.376 & - & - & 0.576 & 0.359 \\
            &\multicolumn{1}{c|}{} & 192   & \textbf{0.385} & \textbf{0.259} & 0.411 & 0.276 & 0.612 & 0.387 & - & - & 0.610 & 0.380 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.398} & \textbf{0.265} & 0.423 & 0.282 & 0.651 & 0.391 & - & - & 0.608 & 0.375 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.434} & \textbf{0.287} & 0.457 & 0.309 & - & - & - & - & 0.621 & 0.375 \\
            \cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & \textbf{0.130} & \textbf{0.222} & 0.136 & 0.231 & 0.196 & 0.307 & 0.205 & 0.318 & 0.186 & 0.302 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.148} & \textbf{0.240} & 0.164 & 0.263 & 0.215 & 0.323 & - & - & 0.197 & 0.311 \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.167} & \textbf{0.261} & 0.168 & 0.262 & 0.228 & 0.338 & - & - & 0.213 & 0.328 \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.202} & \textbf{0.291} & 0.219 & 0.312 & 0.244 & 0.345 & - & - & 0.233 & 0.344 \\
			\cline{2-13}
		\end{tabular}
	}
	\caption{Ablation study of patching and channel-independence in PatchTST. 4 cases are included: (a) both patching and channel-independence are included in model (P+CI); (b) only channel-independence (CI); (c) only patching (P); (d) neither of them is included (Original TST model). PatchTST means supervised PatchTST/42. '-' in table means the model runs out of GPU memory (NVIDIA A40 48GB) even with batch size 1. The best results are in \textbf{bold}. }
	\label{tab:ablation}
\end{table*}
\linespread{1}


\textbf{Varying Look-back Window.} In principle, a longer look-back window increases the receptive field, which will potentially improves the forecasting performance. However, as argued in \citep{dlinear}, this phenomenon hasn't been observed in most of the Transformer-based models. We also demonstrate in Figure \ref{fig::varying L} that in most cases, these Transformer-based baselines have not benefited from longer look-back window $L$, which indicates their ineffectiveness in capturing temporal information. In contrast, our PatchTST consistently reduces the MSE scores as the receptive field increases, which confirms our model's capability to learn from longer look-back window.




\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{figures/sample_merged_results.pdf}
\end{center}
\caption{Forecasting performance (MSE) with varying look-back windows on $3$ large datasets: Electricity, Traffic, and Weather. The look-back windows are selected to be $L=24,48,96,192,336,720$, and the prediction horizons are $T=96, 720$. We use supervised PatchTST/42 and other open-source Transformer-based baselines for this experiment.}
\label{fig::varying L}
\end{figure}

\section{Conclusion and Future Work}

This paper proposes an effective design of Transformer-based models for time series forecasting tasks by introducing two key components: patching and channel-independent structure. Compared to the previous works, it could capture local semantic information and benefit from longer look-back windows. We not only show that our model outperforms other baselines in supervised learning, but also prove its promising capability in self-supervised representation learning and transfer learning.

Our model exhibits the potential to be the based model for future work of Transformer-based forecasting and be a building block for time series foundation models. Patching is simple but proven to be an effective operator that can be transferred easily to other models. Channel-independence, on the other hand, can be further exploited to incorporate the correlation between different channels. It would be an important future step to model the cross-channel dependencies properly.



\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage 
\appendix
\section{Appendix}


\subsection{Experimental Details}
\label{append:exp}

\subsubsection{Datasets}

We use $8$ popular multivariate datasets provided in \citep{autoformer} for forecasting and representation learning. \textit{Weather}\footnote{https://www.bgc-jena.mpg.de/wetter/} dataset collects 21 meteorological indicators in Germany, such as humidity and air temperature. \textit{Traffic}\footnote{https://pems.dot.ca.gov/} dataset records the road occupancy rates from different sensors on San Francisco freeways. \textit{Electricity}\footnote{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014} is a dataset that describes 321 customers' hourly electricity consumption. \textit{ILI}\footnote{https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html} dataset collects the number of patients and influenza-like illness ratio in a weekly frequency. \textit{ETT}\footnote{https://github.com/zhouhaoyi/ETDataset} (Electricity Transformer Temperature) datasets are collected from two different electric transformers labeled with 1 and 2, and each of them contains 2 different resolutions (15 minutes and 1 hour) denoted with m and h. Thus, in total we have 4 ETT datasets: \textit{ETTm1}, \textit{ETTm2}, \textit{ETTh1}, and \textit{ETTh2}. 

There is an additional \textit{Exchange-rate}\footnote{https://github.com/laiguokun/multivariate-time-series-data} dataset mentioned in the original paper, which is the daily exchange-rate of eight different countries. However, financial datasets generally have different properties compared to time series datasets in other fields, for example the predictability. It is well known that if a market is efficient, the best prediction for $x_t$ will be just $x_{t-1}$ \citep{emh}. \citet{exchange} argues that the toughest benchmark for exchange-rate forecasting is a random walk without drift. Also, \citet{dlinear} shows that by simply repeating the last value in the look-back window, the MSE loss on exchange-rate dataset can outperform or be comparable to the best results. Therefore, we are prudent in containing it into our benchmark.

\subsubsection{Details of Baseline Settings}
\label{append::baseline}

The default look-back windows for different baseline models could be different. For Transformer-based models, the default look-back window is $L=96$; and for DLinear, the default look-back window is $L=336$. The reason of this difference is that Transformer-based baselines are easy to overfit when look-back window is long while DLinear tend to underfit. However, this can possibly lead to an under-estimation of the baselines. To address this issue, we re-run FEDformer, Autoformer and Informer by ourselves for six different look-back window $L\in \{24, 48, 96, 192, 336, 720\}$. And for each forecasting task (aka each different prediction length on each dataset), we choose the best one from those six results. Thus it could be a strong baseline.

The ILI dataset is much smaller than the other datasets, so a different set of parameters is applied (the default look-back windows of Transformer-based models and DLinear are $L=36$ and $L=104$ respectively; we run FEDformer, Autoformer and Informer for six different look-back window $L\in \{24, 36, 48, 60, 104, 144\}$ and choose the best results). 

\subsubsection{Baselines from Traditional Models}

Time series has been an ancient field of study, with many traditional models developed, for example the famous ARIMA model \citep{arima}. With the bloom of deep learning community, many new models were proposed for sequence modeling and time series forecasting before Transformer appears, such as LSTM \citep{lstm}, TCN \citep{tcn} and DeepAR \citep{deepar}. However, they are demonstrated to be not as effective as Transformer-based models in long-term forecasting tasks \citep{informer,autoformer}, thus we don't include them in our baselines.

\subsubsection{Model Parameters}

By default, PatchTST contains $3$ encoder layers with head number $H=16$ and dimension of latent space $D=128$. The feed forward network in Transformer encoder block consists of 2 linear layers with GELU \citep{gelu} activation function: one projecting the hidden representation $D=128$ to a new dimension $F=256$, and another layer that project it back to $D=128$. For very small datasets (ILI, ETTh1, ETTh2), a reduced size of parameters is used ($H=4, D=16$ and $F=128$) to mitigate the possible overfitting. Dropout with probability $0.2$ is applied in the encoders for all experiments. The code will be publicly available.

\subsubsection{Implementation Details}
\label{sec::implementation details}

Although PatchTST processes channels in parallel which has to make multiple copies of the Transformer's weights, the computation can be implemented efficiently and does not require any special operator. The batch of samples of $\vx \in \R^{M \times L}$ with size $B \times M \times L$ is passed through the patching operator to generate a $4D$ tensor of size $B \times M \times P \times N$ which represents a batch of $\vx^{(i)}_p \in \R^{P \times N}$ in $M$ series. By reshaping the tensor to form a $3D$ one of size $(B \cdot M) \times P \times N$, this batch can be consumed by any standard Transformer implementation. 

We further argue that our proposed PatchTST contains additional benefits: The components in Transformer backbone module shown in Figure \ref{fig::patchTST} can differ across different input series, for instance the embedding layers and head layers. Note that if the embedding layers are designed differently for each group of time series, the reshaping step will be applied after embedding. Besides, the number of variables in the multivariate time series during the training may not need to match the number of series for testing. This is especially beneficial for self-supervised pre-training where the pre-training data can have different number of variables from the fine-tuning data. 

\subsection{Visualization}

We visualize the long-term forecasting results of supervised PatchTST/42 and other baselines in Figure \ref{fig::visual}. Here, we predict $192$ steps ahead on Weather and Eletricity datasets and $60$ steps ahead on ILI dataset. PatchTST provides the best forecasting both in terms of scale and bias.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.4]{figures/visualization2.png}
\end{center}
\caption{Visualization of $192$-step forecasting on \{Weather, Traffic\} datasets with the look-back window $L=336$ and $60$-step forecasting on ILI dataset with $L=104$. PatchTST (in red) can capture the trend and closest to the ground truth (in blue).}
\label{fig::visual}
\end{figure}

\subsection{Univariate Forecasting}
\label{append:uni}

Table \ref{tab::univariate} summaries the results of univariate forecasting on ETT datasets. There is a target feature "oil temperature" within those datasets, which is the univariate series that we are trying to forecast. We cite the baseline results from \citep{dlinear}.

\begin{table*}[!htbp]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|cc|cc|ccc}
			\cline{2-17}
			&\multicolumn{2}{c|}{Models}& \multicolumn{2}{c|}{PatchTST/64}& \multicolumn{2}{c|}{PatchTST/42}& \multicolumn{2}{c|}{DLinear}& \multicolumn{2}{c|}{FEDformer}& \multicolumn{2}{c|}{Autoformer}& \multicolumn{2}{c|}{Informer}& \multicolumn{2}{c}{LogTrans}& \\
			\cline{2-17}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96    & 0.059 & 0.189 & \textbf{0.055} & \textbf{0.179} & 0.056 & 0.180 & 0.079 & 0.215 & 0.071 & 0.206 & 0.193 & 0.377 & 0.283 & 0.468 \\
            &\multicolumn{1}{c|}{}& 192   & 0.074 & 0.215 & \textbf{0.071} & 0.205 & \textbf{0.071} & \textbf{0.204} & 0.104 & 0.245 & 0.114 & 0.262 & 0.217 & 0.395 & 0.234 & 0.409 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.076} & \textbf{0.220} & 0.081 & 0.225 & 0.098 & 0.244 & 0.119 & 0.270 & 0.107 & 0.258 & 0.202 & 0.381 & 0.386 & 0.546 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.087} & 0.236 & \textbf{0.087} & \textbf{0.232} & 0.189 & 0.359 & 0.142 & 0.299 & 0.126 & 0.283 & 0.183 & 0.355 & 0.475 & 0.629 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & 0.131 & 0.284 & \textbf{0.129} & 0.282 & 0.131 & \textbf{0.279} & 0.128 & 0.271 & 0.153 & 0.306 & 0.213 & 0.373 & 0.217 & 0.379 \\
            &\multicolumn{1}{c|}{}& 192   & 0.171 & 0.329 & \textbf{0.168} & \textbf{0.328} & 0.176 & 0.329 & 0.185 & 0.330 & 0.204 & 0.351 & 0.227 & 0.387 & 0.281 & 0.429 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.171} & \textbf{0.336} & 0.185 & 0.351 & 0.209 & 0.367 & 0.231 & 0.378 & 0.246 & 0.389 & 0.242 & 0.401 & 0.293 & 0.437 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.223} & \textbf{0.380} & 0.224 & 0.383 & 0.276 & 0.426 & 0.278 & 0.420 & 0.268 & 0.409 & 0.291 & 0.439 & 0.218 & 0.387 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & \textbf{0.026} & 0.123 & \textbf{0.026} & \textbf{0.121} & 0.028 & 0.123 & 0.033 & 0.140 & 0.056 & 0.183 & 0.109 & 0.277 & 0.049 & 0.171 \\
            &\multicolumn{1}{c|}{}& 192   & 0.040 & 0.151 & \textbf{0.039} & \textbf{0.150} & 0.045 & 0.156 & 0.058 & 0.186 & 0.081 & 0.216 & 0.151 & 0.310 & 0.157 & 0.317 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.053} & 0.174 & \textbf{0.053} & \textbf{0.173} & 0.061 & 0.182 & 0.084 & 0.231 & 0.076 & 0.218 & 0.427 & 0.591 & 0.289 & 0.459 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.073} & \textbf{0.206} & 0.074 & 0.207 & 0.080 & 0.210 & 0.102 & 0.250 & 0.110 & 0.267 & 0.438 & 0.586 & 0.430 & 0.579 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96    & 0.065 & 0.187 & 0.065 & 0.186 & \textbf{0.063} & \textbf{0.183} & 0.067 & 0.198 & 0.065 & 0.189 & 0.088 & 0.225 & 0.075 & 0.208 \\
            &\multicolumn{1}{c|}{}& 192   & 0.093 & 0.231 & 0.094 & 0.231 & \textbf{0.092} & \textbf{0.227} & 0.102 & 0.245 & 0.118 & 0.256 & 0.132 & 0.283 & 0.129 & 0.275 \\
            &\multicolumn{1}{c|}{}& 336   & 0.121 & 0.266 & 0.120 & 0.265 & \textbf{0.119} & \textbf{0.261} & 0.130 & 0.279 & 0.154 & 0.305 & 0.180 & 0.336 & 0.154 & 0.302 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.172} & 0.322 & 0.171 & 0.322 & 0.175 & \textbf{0.320} & 0.178 & 0.325 & 0.182 & 0.335 & 0.300 & 0.435 & 0.160 & 0.321 \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Univariate long-term forecasting results with supervised PatchTST. ETT datasets are used with prediction lengths $T\in \{96, 192, 336, 720\}$. The best results are in \textbf{bold}.}
	\label{tab::univariate}
\end{table*}

\subsection{More Results on Ablation Study}
\label{append:abla}
\subsubsection{Varying Patch Length}

This experiment studies the effect of patch lengths to the forecasting performance. We fix the look-back window to be $336$ and vary the patch lengths $P = [4, 8, 16, 24, 32, 40]$. The stride length is set the same as patch length, meaning no overlapping between patches. The model is trained to predict $96$ steps. One observation from Figure \ref{fig::varying patch len} is that MSE scores don't vary significantly with different choices of $P$, which indicate the robustness of our model against the patch length hyperparameter. Overall, PatchTST benefits from increased patch length, not only in forecasting performance but also in the computational reduction. The ideal patch length may depend on the dataset, but $P$ between $\{8, 16\}$ seems to be general good numbers.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.4]{figures/vary_patch_v2.pdf}
\end{center}
\caption{MSE scores with varying patch lengths $P = [2,4,8,12,16,24,32,40]$ where the look-back window is $336$ and the prediction length is $96$.}
\label{fig::varying patch len}
\end{figure}


\subsubsection{Varying Look-back Window}

Here we provide a full benchmark of quantitative results in Table \ref{tab::different L} for varying look-back window in supervised PatchTST/42 regarding Figure \ref{fig::varying L} in the main text. Generally speaking, our model gains performance improvement with increasing look-back window, which show the effectiveness of our model in learning information from longer receptive field.

\begin{table*}[!htbp]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|cc|ccc}
			\cline{2-15}
			&\multicolumn{2}{c|}{$L$}& \multicolumn{2}{c|}{24(24)}& \multicolumn{2}{c|}{48(36)}& \multicolumn{2}{c|}{96(48)}& \multicolumn{2}{c|}{192(60)}& \multicolumn{2}{c|}{336(104)}& \multicolumn{2}{c}{720(144)}\\
			\cline{2-15}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & 0.222 & 0.246 & 0.212 & 0.243 & 0.178 & 0.219 & 0.160 & 0.204 & 0.152 & 0.199 & 0.147 & 0.198 \\
            &\multicolumn{1}{c|}{}& 192   & 0.265 & 0.279 & 0.254 & 0.277 & 0.224 & 0.259 & 0.204 & 0.245 & 0.197 & 0.243 & 0.190 & 0.240 \\
            &\multicolumn{1}{c|}{}& 336   & 0.325 & 0.322 & 0.310 & 0.316 & 0.278 & 0.298 & 0.257 & 0.285 & 0.249 & 0.283 & 0.242 & 0.282 \\
            &\multicolumn{1}{c|}{}& 720   & 0.404 & 0.374 & 0.385 & 0.365 & 0.350 & 0.346 & 0.329 & 0.338 & 0.320 & 0.335 & 0.304 & 0.328 \\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & 0.766 & 0.419 & 0.671 & 0.381 & 0.477 & 0.305 & 0.401 & 0.267 & 0.367 & 0.251 & 0.365 & 0.251 \\
            &\multicolumn{1}{c|}{} & 192   & 0.725 & 0.398 & 0.616 & 0.356 & 0.471 & 0.299 & 0.406 & 0.268 & 0.385 & 0.259 & 0.382 & 0.258 \\
            &\multicolumn{1}{c|}{}& 336   & 0.752 & 0.410 & 0.635 & 0.364 & 0.485 & 0.305 & 0.421 & 0.277 & 0.398 & 0.265 & 0.398 & 0.267 \\
            &\multicolumn{1}{c|}{}& 720   & 0.786 & 0.427 & 0.673 & 0.383 & 0.518 & 0.325 & 0.452 & 0.297 & 0.434 & 0.287 & 0.436 & 0.289 \\
            \cline{2-15}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & 0.268 & 0.316 & 0.225 & 0.293 & 0.174 & 0.259 & 0.138 & 0.230 & 0.130 & 0.222 & 0.130 & 0.224 \\
			&\multicolumn{1}{c|}{}& 192   & 0.259 & 0.316 & 0.217 & 0.291 & 0.178 & 0.265 & 0.149 & 0.243 & 0.148 & 0.240 & 0.147 & 0.241 \\
			&\multicolumn{1}{c|}{}& 336   & 0.283 & 0.335 & 0.238 & 0.309 & 0.196 & 0.282 & 0.169 & 0.262 & 0.167 & 0.261 & 0.163 & 0.259 \\
			&\multicolumn{1}{c|}{}& 720   & 0.321 & 0.365 & 0.278 & 0.342 & 0.237 & 0.316 & 0.211 & 0.299 & 0.202 & 0.291 & 0.197 & 0.290 \\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{ILI}}& 24    & 3.062 & 1.118 & 1.610 & 0.803 & 1.281 & 0.704 & 1.300 & 0.700 & 1.522 & 0.814 & 1.470 & 0.793 \\
            &\multicolumn{1}{c|}{}& 36    & 2.732 & 1.071 & 1.262 & 0.731 & 1.251 & 0.752 & 1.367 & 0.776 & 1.430 & 0.834 & 1.518 & 0.856 \\
            &\multicolumn{1}{c|}{}& 48    & 3.059 & 1.117 & 1.991 & 0.845 & 1.901 & 0.879 & 1.690 & 0.812 & 1.673 & 0.854 & 1.834 & 0.921 \\
            &\multicolumn{1}{c|}{}& 60    & 2.610 & 1.057 & 1.702 & 0.829 & 1.611 & 0.844 & 1.526 & 0.795 & 1.529 & 0.862 & 1.656 & 0.885 \\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{ETTh1}} & 96    & 0.464 & 0.445 & 0.410 & 0.417 & 0.393 & 0.408 & 0.382 & 0.401 & 0.375 & 0.399 & 0.376 & 0.408 \\
            &\multicolumn{1}{c|}{}& 192   & 0.521 & 0.474 & 0.469 & 0.448 & 0.445 & 0.434 & 0.428 & 0.425 & 0.414 & 0.421 & 0.413 & 0.431 \\
            &\multicolumn{1}{c|}{}& 336   & 0.570 & 0.498 & 0.516 & 0.469 & 0.484 & 0.451 & 0.451 & 0.436 & 0.431 & 0.436 & 0.445 & 0.454 \\
            &\multicolumn{1}{c|}{}& 720   & 0.575 & 0.522 & 0.509 & 0.487 & 0.480 & 0.471 & 0.452 & 0.459 & 0.449 & 0.466 & 0.458 & 0.477 \\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & 0.333 & 0.362 & 0.307 & 0.348 & 0.294 & 0.343 & 0.285 & 0.340 & 0.274 & 0.336 & 0.279 & 0.341 \\
            &\multicolumn{1}{c|}{}& 192   & 0.422 & 0.409 & 0.397 & 0.399 & 0.377 & 0.393 & 0.356 & 0.386 & 0.339 & 0.379 & 0.349 & 0.386 \\
            &\multicolumn{1}{c|}{}& 336   & 0.442 & 0.432 & 0.412 & 0.420 & 0.381 & 0.409 & 0.350 & 0.395 & 0.331 & 0.380 & 0.375 & 0.409 \\
            &\multicolumn{1}{c|}{}& 720   & 0.462 & 0.453 & 0.434 & 0.441 & 0.412 & 0.433 & 0.395 & 0.427 & 0.379 & 0.422 & 0.394 & 0.434 \\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & 0.624 & 0.481 & 0.424 & 0.403 & 0.321 & 0.360 & 0.291 & 0.340 & 0.290 & 0.342 & 0.295 & 0.348 \\
            &\multicolumn{1}{c|}{}& 192   & 0.671 & 0.507 & 0.468 & 0.429 & 0.362 & 0.384 & 0.328 & 0.365 & 0.332 & 0.369 & 0.334 & 0.373 \\
            &\multicolumn{1}{c|}{}& 336   & 0.714 & 0.533 & 0.501 & 0.453 & 0.392 & 0.402 & 0.365 & 0.389 & 0.366 & 0.392 & 0.361 & 0.393 \\
            &\multicolumn{1}{c|}{}& 720   & 0.744 & 0.554 & 0.553 & 0.484 & 0.450 & 0.435 & 0.422 & 0.423 & 0.420 & 0.424 & 0.416 & 0.419 \\
			\cline{2-15}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96    & 0.212 & 0.290 & 0.189 & 0.272 & 0.178 & 0.260 & 0.169 & 0.254 & 0.165 & 0.255 & 0.162 & 0.254 \\
            &\multicolumn{1}{c|}{}& 192   & 0.282 & 0.334 & 0.260 & 0.317 & 0.249 & 0.307 & 0.230 & 0.294 & 0.220 & 0.292 & 0.216 & 0.293 \\
            &\multicolumn{1}{c|}{}& 336   & 0.354 & 0.376 & 0.328 & 0.359 & 0.313 & 0.346 & 0.280 & 0.329 & 0.278 & 0.329 & 0.269 & 0.329 \\
            &\multicolumn{1}{c|}{}& 720   & 0.458 & 0.433 & 0.429 & 0.415 & 0.400 & 0.398 & 0.378 & 0.386 & 0.367 & 0.385 & 0.350 & 0.380 \\
			\cline{2-15}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results with varying look-back window $L$ in supervised PatchTST/42. }
	\label{tab::different L}
\end{table*}

\subsubsection{Patching and Channel-independence}
\textbf{Implementation Details.} For ablation study on patching and channel independence in section \ref{sec::ablation}, we run different variants of PatchTST: 

\begin{itemize}[leftmargin= 20 pt,itemsep= 5 pt,topsep = 5 pt]
    \item Both patching and channel-independence are included in model (P+CI): this is the full PatchTST model that we have proposed in paper.

    \item Only channel-independence (CI): we simply set both patch length $P$ and stride $S$ to be $1$ to avoid patching and only keep channel-independence.

    \item Only patching (P): referring to the implementation in \ref{sec::implementation details}, instead of reshaping the 4$D$ tensor from $B \times M \times P \times N$ to $(B \cdot M) \times P \times N$, we reshape it to $B \times (M \cdot P) \times N$ for channel-mixing with patching.

    \item Neither patching nor channel-independence is included (Original), which is just the original TST model \cite{tst}. 
\end{itemize}

Note that the default number of maximum epochs for Electricity and Traffic datasets are reduced from 100 to 20 for ablation experiments due to the huge space and time complexity of the original time series Transformer and channel independent model (no patching).


\textbf{Full Benchmark of Ablation Study.}
The full benchmark is shown in Table \ref{tab:ablation (full)}, which is a completed version of Table \ref{tab:ablation} in the main text. We can observe that patching together with channel-independence achieves the best results from the table, especially on larger datasets (Weather, Traffic, and Electricity) where the models are less susceptible to overfitting and thus the results would be more convincing. As one can see the improvement is robust on both patching and channel-independence. It is interesting to see that the improvement on ILI dataset is significant as well.

\linespread{1.1}
\begin{table*}[!htbp]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|ccc}
		    \cline{2-13}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{8}{c|}{PatchTST}&  \multicolumn{2}{c}{\multirow{2}{*}{FEDformer}}& \\
			\cline{4-11}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{P+CI}& \multicolumn{2}{c|}{CI}& \multicolumn{2}{c|}{P}& \multicolumn{2}{c|}{Original} & 
            \multicolumn{2}{c}{}&\\
			\cline{2-13}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & \textbf{0.152} & \textbf{0.199} & 0.164 & 0.213 & 0.168 & 0.223 & 0.177 & 0.236 & 0.238 & 0.314 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.197} & \textbf{0.243} & 0.205 & 0.250 & 0.213 & 0.262 & 0.221 & 0.270 & 0.275 & 0.329 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.249} & \textbf{0.283} & 0.255 & 0.289 & 0.266 & 0.300 & 0.271 & 0.306 & 0.339 & 0.377 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.320} & \textbf{0.335} & 0.327 & 0.343 & 0.351 & 0.359 & 0.340 & 0.353 & 0.389 & 0.409 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & \textbf{0.367} & \textbf{0.251} & 0.397 & 0.271 & 0.595 & 0.376 & - & - & 0.576 & 0.359 \\
            &\multicolumn{1}{c|}{} & 192   & \textbf{0.385} & \textbf{0.259} & 0.411 & 0.276 & 0.612 & 0.387 & - & - & 0.610 & 0.380 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.398} & \textbf{0.265} & 0.423 & 0.282 & 0.651 & 0.391 & - & - & 0.608 & 0.375 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.434} & \textbf{0.287} & 0.457 & 0.309 & - & - & - & - & 0.621 & 0.375 \\
            \cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & \textbf{0.130} & \textbf{0.222} & 0.136 & 0.231 & 0.196 & 0.307 & 0.205 & 0.318 & 0.186 & 0.302 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.148} & \textbf{0.240} & 0.164 & 0.263 & 0.215 & 0.323 & - & - & 0.197 & 0.311 \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.167} & \textbf{0.261} & 0.168 & 0.262 & 0.228 & 0.338 & - & - & 0.213 & 0.328 \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.202} & \textbf{0.291} & 0.219 & 0.312 & 0.244 & 0.345 & - & - & 0.233 & 0.344 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{ILI}}& 24    & \textbf{1.522} & \textbf{0.814} & 2.111 & 1.048 & 2.157 & 0.964 & 2.737 & 1.081 & 2.624 & 1.095 \\
            &\multicolumn{1}{c|}{} & 36    & \textbf{1.430} & \textbf{0.834} & 2.000 & 1.002 & 2.564 & 1.058 & 2.126 & 0.935 & 2.516 & 1.021 \\
            &\multicolumn{1}{c|}{}& 48    & \textbf{1.673} & \textbf{0.854} & 2.167 & 1.029 & 2.348 & 1.022 & 2.178 & 0.971 & 2.505 & 1.041 \\
            &\multicolumn{1}{c|}{}& 60    & \textbf{1.529} & \textbf{0.862} & 2.075 & 1.021 & 2.486 & 1.065 & 2.354 & 1.026 & 2.742 & 1.122  \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96    & 0.375 & 0.399 & \textbf{0.365} & \textbf{0.395} & 0.416 & 0.438 & 0.455 & 0.459 & 0.376 & 0.415 \\
            &\multicolumn{1}{c|}{}& 192   & 0.414 & 0.421 & \textbf{0.403} & \textbf{0.415} & 0.459 & 0.464 & 0.503 & 0.486 & 0.423 & 0.446 \\
            &\multicolumn{1}{c|}{}& 336   & 0.431 & 0.436 & \textbf{0.430} & \textbf{0.433} & 0.484 & 0.480 & 0.514 & 0.503 & 0.444 & 0.462 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.449} & 0.466 & \textbf{0.449} & \textbf{0.454} & 0.500 & 0.494 & 0.531 & 0.520 & 0.469 & 0.492 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & \textbf{0.274} & \textbf{0.336} & 0.277 & 0.337 & 0.334 & 0.388 & 0.348 & 0.394 & 0.332 & 0.374 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.339} & \textbf{0.379} & 0.343 & 0.384 & 0.381 & 0.418 & 0.395 & 0.424 & 0.407 & 0.446 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.331} & \textbf{0.380} & 0.333 & 0.383 & 0.361 & 0.414 & 0.369 & 0.419 & 0.400 & 0.447 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.379} & 0.422 & \textbf{0.379} & \textbf{0.420} & 0.423 & 0.448 & 0.433 & 0.458 & 0.412 & 0.469 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & \textbf{0.290} & \textbf{0.342} & 0.300 & 0.354 & 0.326 & 0.368 & 0.324 & 0.370 & 0.326 & 0.390 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.332} & \textbf{0.369} & 0.333 & 0.374 & 0.391 & 0.405 & 0.373 & 0.398 & 0.365 & 0.415 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.366} & \textbf{0.392} & 0.369 & 0.397 & 0.427 & 0.425 & 0.415 & 0.421 & 0.392 & 0.425 \\
            &\multicolumn{1}{c|}{}& 720   & 0.420 & 0.424 & \textbf{0.413} & \textbf{0.423} & 0.481 & 0.457 & 0.480 & 0.459 & 0.446 & 0.458 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}  & 96    & \textbf{0.165} & \textbf{0.255} & 0.166 & 0.259 & 0.195 & 0.274 & 0.208 & 0.289 & 0.180 & 0.271 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.220} & \textbf{0.292} & 0.223 & 0.295 & 0.259 & 0.314 & 0.265 & 0.328 & 0.252 & 0.318 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.278} & \textbf{0.329} & 0.279 & 0.330 & 0.297 & 0.345 & 0.323 & 0.365 & 0.324 & 0.364 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.367} & \textbf{0.385} & 0.370 & 0.387 & 0.400 & 0.404 & 0.469 & 0.444 & 0.410 & 0.420 \\
			\cline{2-13}
		\end{tabular}
	}
	\caption{Ablation study of patching (P) and channel-independence (CI) in PatchTST/42. A full benchmark regarding Table \ref{tab:ablation}. The best results are in \textbf{bold}. '-' in table means the model runs out of GPU memory (NVIDIA A40 48GB) even with batch size 1.}
	\label{tab:ablation (full)}
\end{table*}
\linespread{1}


\subsubsection{Instance Normalization}

Normalization is a technology used in many time series model to improve the forecasting performance \citep{revin,quatformer,dlinear}. In this experiment, we perform analysis on the effect of the instance normalization in our model. We train two models PatchTST/64 and PatchTST/42 with and without using instance normalization and observe the evaluated scores. As indicated in Table \ref{tab:with and without IN}, instance normalization improves the forecasting performance slightly on two models. However, even without instance normalization operator, PatchTST still outperforms other Transformer methods on most of the datasets. This is to confirm that the improvement mainly comes from patching and channel independence designs.

\linespread{1.2}
\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cc|c||cc|cc|cc|cc||cc|cc|ccc}
			\cline{2-17}
			&\multicolumn{2}{c||}{Models}& \multicolumn{2}{c|}{PatchTST/64 (+in)}& \multicolumn{2}{c|}{PatchTST/64 (-in)}& \multicolumn{2}{c|}{PatchTST/42 (+in)}& \multicolumn{2}{c||}{PatchTST/42 (-in)}& \multicolumn{2}{c|}{FEDformer}& \multicolumn{2}{c|}{Autoformer}& \multicolumn{2}{c}{Informer}& \\
			\cline{2-17}
			&\multicolumn{2}{c||}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & \textbf{0.149} & \textbf{0.198} & 0.161 & 0.219 & \uline{0.152} & \uline{0.199} & 0.156 & 0.210 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.194} & \textbf{0.241} & 0.201 & 0.254 & \uline{0.197} & \uline{0.243} & 0.199 & 0.250 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.245} & \textbf{0.282} & 0.253 & 0.298 & \uline{0.249} & \uline{0.283} & 0.248 & 0.294 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543  &  \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.314} & \textbf{0.334} & 0.323 & 0.357 & \uline{0.320} & \uline{0.335} & 0.313 & 0.342 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705 &  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & \textbf{0.360} & \textbf{0.249} & 0.413 & 0.295 & \uline{0.367} & \uline{0.251} & 0.425 & 0.299 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410  \\
            &\multicolumn{1}{c|}{} & 192   & \textbf{0.379} & \textbf{0.256} & 0.425 & 0.302 & \uline{0.385} & \uline{0.259} & 0.439 & 0.302 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435 & \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.392} & \textbf{0.264} & 0.435 & 0.307 & \uline{0.398} & \uline{0.265} & 0.456 & 0.316 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434 & \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.432} & \textbf{0.286} & 0.473 & 0.321 & \uline{0.434} & \uline{0.287} & 0.488 & 0.333 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466 & \\
            \cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & \textbf{0.129} & \textbf{0.222} & 0.133 & 0.230 & \uline{0.130} & \textbf{0.222} & 0.131 & 0.226 & 0.186 & 0.302 & 0.196 & 0.313 & 0.304 & 0.393 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.147} & \textbf{0.240} & 0.148 & 0.244 & \uline{0.148} & \textbf{0.240} & 0.150 & 0.244 & 0.197 & 0.311 & 0.211 & 0.324 & 0.327 & 0.417 & \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.163} & \textbf{0.259} & 0.164 & 0.262 & \uline{0.167} & \uline{0.261} & 0.168 & 0.267 & 0.213 & 0.328 & 0.214 & 0.327 & 0.333 & 0.422 & \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.197} & \textbf{0.290} & 0.196 & 0.291 & \uline{0.202} & \uline{0.291} & 0.201 & 0.298 & 0.233 & 0.344 & 0.236 & 0.342 & 0.351 & 0.427 & \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ILI}}& 24    & \textbf{1.319} & \textbf{0.754} & 3.563 & 1.317  & \uline{1.522} & \uline{0.814} & 3.489 & 1.345 & 2.624 & 1.095 & 2.906 & 1.182 & 4.657 & 1.449  & \\
            &\multicolumn{1}{c|}{} & 36    & \uline{1.579} & \uline{0.870} & 3.426 & 1.205  & \textbf{1.430} & \textbf{0.834} & 4.629 & 1.550 & 2.516 & 1.021 & 2.585 & 1.038 & 4.650 & 1.463 & \\
            &\multicolumn{1}{c|}{}& 48    & \textbf{1.553} & \textbf{0.815} & 4.309 & 1.449 & \uline{1.673} & \uline{0.854} & 3.746 & 1.383 & 2.505 & 1.041 & 3.024 & 1.145 & 5.004 & 1.542 &  \\
            &\multicolumn{1}{c|}{}& 60    & \textbf{1.470} & \textbf{0.788} & 4.065 & 1.402 & \uline{1.529} & \uline{0.862} & 5.174 & 1.622 & 2.742 & 1.122 & 2.761 & 1.114 & 5.071 & 1.543 &  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96    & \textbf{0.370} & \uline{0.400} & 0.385 & 0.410 & \uline{0.375} & \textbf{0.399} & 0.388 & 0.412 & 0.376 & 0.415 & 0.435 & 0.446 & 0.941 & 0.769 \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.413} & 0.429 & 0.417 & 0.432 & 0.414 & \uline{0.421} & 0.430 & 0.438 & 0.423 & 0.446 & 0.456 & 0.457 & 1.007 & 0.786 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.422} & \uline{0.440} & 0.439 & 0.449 & \uline{0.431} & \textbf{0.436} & 0.454 & 0.458 & 0.444 & 0.462 & 0.486 & 0.487 & 1.038 & 0.784 & \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.447} & \uline{0.468} & 0.478 & 0.494 & \uline{0.449} & \textbf{0.466} & 0.494 & 0.497 & 0.469 & 0.492 & 0.515 & 0.517 & 1.144 & 0.857 & \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & \textbf{0.274} & \uline{0.337} & 0.299 & 0.359 & \textbf{0.274} & \textbf{0.336} & 0.313 & 0.374 & 0.332 & 0.374 & 0.332 & 0.368 & 1.549 & 0.952 \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.341} & \uline{0.382} & 0.354 & 0.404 & \textbf{0.339} & \textbf{0.379} & 0.402 & 0.432 & 0.407 & 0.446 & 0.426 & 0.434 & 3.792 & 1.542 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.329} & \uline{0.384} & 0.374 & 0.420 & \uline{0.331} & \textbf{0.380} & 0.448 & 0.465 & 0.400 & 0.447 & 0.477 & 0.479 & 4.215 & 1.642 & \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.379} & \textbf{0.422} & 0.479 & 0.492 & \textbf{0.379} & \textbf{0.422} & 0.688 & 0.588 & 0.412 & 0.469 & 0.453 & 0.490 & 3.656 & 1.619 & \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & \uline{0.293} & 0.346 & 0.308 & 0.358 & \textbf{0.290} & \textbf{0.342} & 0.308 & 0.358 & 0.326 & 0.390 & 0.510 & 0.492 & 0.626 & 0.560 & \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.333} & 0.370 & 0.335 & 0.375 & \textbf{0.332} & \uline{0.369} & 0.356 & 0.390 & 0.365 & 0.415 & 0.514 & 0.495 & 0.725 & 0.619 & \\
            &\multicolumn{1}{c|}{}& 336   & \uline{0.369} & \uline{0.392} & 0.362 & 0.392 & \textbf{0.366} & \uline{0.392} & 0.389 & 0.411 & 0.392 & 0.425 & 0.510 & 0.492 & 1.005 & 0.741 & \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.416} & \textbf{0.420} & 0.432 & 0.429 & \uline{0.420} & 0.424 & 0.430 & 0.439 & 0.446 & 0.458 & 0.527 & 0.493 & 1.133 & 0.845 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm2}} & 96    & \uline{0.166} & \uline{0.256} & 0.172 & 0.258 & \textbf{0.165} & \textbf{0.255} & 0.167 & 0.257 & 0.180 & 0.271 & 0.205 & 0.293 & 0.355 & 0.462\\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.223} & \uline{0.296} & 0.245 & 0.306 & \textbf{0.220} & \textbf{0.292} & 0.226 & 0.303 & 0.252 & 0.318 & 0.278 & 0.336 & 0.595 & 0.586 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.274} & \textbf{0.329} & 0.306 & 0.346 & \uline{0.278} & \textbf{0.329} & 0.301 & 0.348 & 0.324 & 0.364 & 0.343 & 0.379 & 1.270 & 0.871 &\\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.362} & \textbf{0.385} & 0.391 & 0.404 & \uline{0.367} & \textbf{0.385} & 0.392 & 0.407 & 0.410 & 0.420 & 0.414 & 0.419 & 3.001 & 1.267 \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results of supervised PatchTST with instance normalization (+in) or without instance normalization (-in). The best results are in \textbf{bold} and the second best are \uline{underlined}. Although the models perform slightly better with instance normalization, compared to other Transformer models, the proposed approach achieve significantly better forecasting on most of the datasets even without instance normalization. }
	\label{tab:with and without IN}
\end{table*}
\linespread{1}




\subsection{More Results on Self-supervised Representation Learning}
\label{append:self-sup}

\subsubsection{Full Benchmark of Multivariate Forecasting}

In this section we provide a full benchmark of multivariate forecasting results with self-supervised PatchTST in Table \ref{table::Fine-tuning performance (full)}, which is an extended version of Table \ref{table::Fine-tuning performance}. 

\begin{table*}[!htbp]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cccccc|ccccccccc}
			\cline{2-17}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{6}{c|}{PatchTST}&  \multicolumn{2}{c|}{\multirow{2}{*}{DLinear}}& \multicolumn{2}{c|}{\multirow{2}{*}{FEDformer}}& \multicolumn{2}{c|}{\multirow{2}{*}{Autoformer}}& \multicolumn{2}{c}{\multirow{2}{*}{Informer}}& \\
			\cline{4-9}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{Fine-tuning}& \multicolumn{2}{c|}{Lin. Prob.}& \multicolumn{2}{c|}{Sup. }& \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c}{}& \\
			\cline{2-17}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & \textbf{0.144} & \textbf{0.193} & 0.158 & 0.209  & 0.152 & 0.199 & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.190} & \textbf{0.236} & 0.203 & 0.249  & 0.197 & 0.243 & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434  \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.244} & \textbf{0.280} & 0.251 & 0.285  & 0.249 & 0.283 & 0.265 & 0.319 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543   \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.320} & \textbf{0.335} & 0.321 & 0.336  & \textbf{0.320} & \textbf{0.335} & 0.323 & 0.362 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705   \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96 & \textbf{0.352} & \textbf{0.244} & 0.399 & 0.294  & 0.367 & 0.251 & 0.410 & 0.282 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410  \\
            &\multicolumn{1}{c|}{}& 192  & \textbf{0.371} & \textbf{0.253} & 0.412 & 0.298  & 0.385 & 0.259 & 0.423 & 0.287 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435  \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.381} & \textbf{0.257} & 0.425 & 0.306 & 0.398  & 0.265 & 0.436 & 0.296 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434  \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.425} & \textbf{0.282} & 0.460 & 0.323 & 0.434  & 0.287 & 0.466 & 0.315 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466  \\
            \cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96 & \textbf{0.126} & \textbf{0.221} & 0.138 & 0.237  & 0.130 & \textbf{0.222} & 0.140 & 0.237 & 0.186 & 0.302 & 0.196 & 0.313 & 0.304 & 0.393  \\
			&\multicolumn{1}{c|}{}& 192 & \textbf{0.145} & \textbf{0.238} & 0.156 & 0.252  & 0.148 & 0.240 & 0.153 & 0.249 & 0.197 & 0.311 & 0.211 & 0.324 & 0.327 & 0.417  \\
			&\multicolumn{1}{c|}{}& 336 & \textbf{0.164} & \textbf{0.256} & 0.170 & 0.265  & 0.167 & 0.261 & 0.169 & 0.267 & 0.213 & 0.328 & 0.214 & 0.327 & 0.333 & 0.422  \\
			&\multicolumn{1}{c|}{}& 720 & \textbf{0.193} & \textbf{0.291} & 0.208 & 0.297  & 0.202 & \textbf{0.291} & 0.203 & 0.301 & 0.233 & 0.344 & 0.236 & 0.342 & 0.351 & 0.427 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96  & \textbf{0.366} & \textbf{0.397} & 0.371 & 0.400  & 0.375 & 0.399 & 0.375 & 0.399 & 0.376 & 0.415 & 0.435 & 0.446 & 0.941 & 0.769   \\
            &\multicolumn{1}{c|}{}& 192 & 0.431 & 0.443 & 0.411 & 0.428  & 0.414 & 0.421 & \textbf{0.405} & \textbf{0.416} & 0.423 & 0.446 & 0.456 & 0.457 & 1.007 & 0.786 \\
            &\multicolumn{1}{c|}{}& 336 & 0.450 & 0.456 & 0.445 & 0.446  & \textbf{0.431} & \textbf{0.436} & 0.439 & 0.443 & 0.444 & 0.462 & 0.486 & 0.487 & 1.038 & 0.784   \\
            &\multicolumn{1}{c|}{}& 720 & 0.472 & 0.484 & 0.487 & 0.478  & \textbf{0.449} & \textbf{0.466} & 0.472 & 0.490 & 0.469 & 0.492 & 0.515 & 0.517 & 1.144 & 0.857  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96 & 0.284 & 0.343 & 0.285 & 0.344   & \textbf{0.274} & \textbf{0.336} & 0.289 & 0.353 & 0.332 & 0.374 & 0.332 & 0.368 & 1.549 & 0.952  \\
            &\multicolumn{1}{c|}{}& 192 & 0.355 & 0.387 & 0.356 & 0.387  & \textbf{0.339} & \textbf{0.379} & 0.383 & 0.418 & 0.407 & 0.446 & 0.426 & 0.434 & 3.792 & 1.542  \\
            &\multicolumn{1}{c|}{}& 336 & 0.379 & 0.411 & 0.377 & 0.410  & \textbf{0.331} & \textbf{0.380} & 0.448 & 0.465 & 0.400 & 0.447 & 0.477 & 0.479 & 4.215 & 1.642  \\
            &\multicolumn{1}{c|}{}& 720 & 0.400 & 0.435 & 0.395 & 0.434  & \textbf{0.379} & \textbf{0.422} & 0.605 & 0.551 & 0.412 & 0.469 & 0.453 & 0.490 & 3.656 & 1.619  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96 & \textbf{0.289} & 0.344 & 0.292 & 0.348  & 0.290 & \textbf{0.342} & 0.299 & 0.343 & 0.326 & 0.390 & 0.510 & 0.492 & 0.626 & 0.560 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.323} & 0.368 & 0.329 & 0.369  & 0.332 & 0.369 & 0.335 & \textbf{0.365} & 0.365 & 0.415 & 0.514 & 0.495 & 0.725 & 0.619 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.353} & 0.387 & 0.364 & 0.391  & 0.366 & 0.392 & 0.369 & \textbf{0.386} & 0.392 & 0.425 & 0.510 & 0.492 & 1.005 & 0.741 \\
            &\multicolumn{1}{c|}{}& 720  & \textbf{0.398} & \textbf{0.416} & 0.415 & 0.419 & 0.420 & 0.424 & 0.425 & 0.421 & 0.446 & 0.458 & 0.527 & 0.493 & 1.133 & 0.845 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96  & 0.166 & 0.256 & 0.167 & 0.257  & \textbf{0.165} & \textbf{0.255} & 0.167 & 0.260 & 0.180 & 0.271 & 0.205 & 0.293 & 0.355 & 0.462 \\
            &\multicolumn{1}{c|}{}& 192 & 0.221 & 0.295 & 0.229 & 0.300  & \textbf{0.220} & \textbf{0.292} & 0.224 & 0.303 & 0.252 & 0.318 & 0.278 & 0.336 & 0.595 & 0.586 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.278} & 0.333 & 0.289 & 0.343  & \textbf{0.278} & \textbf{0.329} & 0.281 & 0.342 & 0.324 & 0.364 & 0.343 & 0.379 & 1.270 & 0.871 \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.365} & 0.388 & 0.363 & 0.386  & 0.367 & \textbf{0.385} & 0.397 & 0.421 & 0.410 & 0.420 & 0.414 & 0.419 & 3.001 & 1.267 \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results with self-supervised PatchTST. An full benchmark regarding Table \ref{table::Fine-tuning performance}. The best results are in \textbf{bold}.}
	\label{table::Fine-tuning performance (full)}
\end{table*}

\subsubsection{Full Benchmark of Transfer Learning}

In this section we provide Table \ref{table::transferring performance  (full)} which contains the results of pre-training on Electricity dataset then transferred to other 6 datasets. Except Traffic data, the number of time series employed in the pre-training is much larger than the number of series during fine-tuning. It is a full version with respect to Table \ref{table::transferring performance} in the main text and more cogently proves the capability to do transfer learning using our PatchTST model.

\begin{table*}[!h]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|cccccc|ccccccccc}
			\cline{2-17}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{6}{c|}{PatchTST}&  \multicolumn{2}{c|}{\multirow{2}{*}{DLinear}}& \multicolumn{2}{c|}{\multirow{2}{*}{FEDformer}}& \multicolumn{2}{c|}{\multirow{2}{*}{Autoformer}}& \multicolumn{2}{c}{\multirow{2}{*}{Informer}}& \\
			\cline{4-9}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{Fine-tuning}& \multicolumn{2}{c|}{Lin. Prob.}& \multicolumn{2}{c|}{Sup. }& \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}& \multicolumn{2}{c|}{}& \multicolumn{2}{c}{}& \\
			\cline{2-17}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & \textbf{0.145} & \textbf{0.195} & 0.163 & 0.216  & 0.152 & 0.199 & 0.176 & 0.237 & 0.238 & 0.314 & 0.249 & 0.329 & 0.354 & 0.405 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.193} & \textbf{0.243} & 0.205 & 0.252  & 0.197 & \textbf{0.243} & 0.220 & 0.282 & 0.275 & 0.329 & 0.325 & 0.370 & 0.419 & 0.434  \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.244} & \textbf{0.280} & 0.253 & 0.289  & 0.249 & 0.283 & 0.265 & 0.319 & 0.339 & 0.377 & 0.351 & 0.391 & 0.583 & 0.543   \\
            &\multicolumn{1}{c|}{}& 720 & 0.321 & 0.337 & \textbf{0.320} & 0.336  & \textbf{0.320} & \textbf{0.335} & 0.323 & 0.362 & 0.389 & 0.409 & 0.415 & 0.426 & 0.916 & 0.705   \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96 & 0.388 & 0.273 & 0.400 & 0.288  & \textbf{0.367} & \textbf{0.251} & 0.410 & 0.282 & 0.576 & 0.359 & 0.597 & 0.371 & 0.733 & 0.410  \\
            &\multicolumn{1}{c|}{}& 192  & 0.400 & 0.277 & 0.412 & 0.293 & \textbf{0.385} & \textbf{0.259} & 0.423 & 0.287 & 0.610 & 0.380 & 0.607 & 0.382 & 0.777 & 0.435  \\
            &\multicolumn{1}{c|}{}& 336   & 0.408 & 0.280 & 0.425 & 0.307 & \textbf{0.398}  & \textbf{0.265} & 0.436 & 0.296 & 0.608 & 0.375 & 0.623 & 0.387 & 0.776 & 0.434  \\
            &\multicolumn{1}{c|}{}& 720   & 0.447 & 0.310 &0.457  & 0.317 & \textbf{0.434}  & \textbf{0.287} & 0.466 & 0.315 & 0.621 & 0.375 & 0.639 & 0.395 & 0.827 & 0.466  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96  & \textbf{0.368} & \textbf{0.398} & 0.372 & 0.402  & 0.375 & 0.399 & 0.375 & 0.399 & 0.376 & 0.415 & 0.435 & 0.446 & 0.941 & 0.769   \\
            &\multicolumn{1}{c|}{}& 192 & 0.425 & 0.439 & 0.411 & 0.428  & 0.414 & 0.421 & \textbf{0.405} & \textbf{0.416} & 0.423 & 0.446 & 0.456 & 0.457 & 1.007 & 0.786 \\
            &\multicolumn{1}{c|}{}& 336 & 0.470 & 0.471 & 0.442 & 0.454  & \textbf{0.431} & \textbf{0.436} & 0.439 & 0.443 & 0.444 & 0.462 & 0.486 & 0.487 & 1.038 & 0.784   \\
            &\multicolumn{1}{c|}{}& 720 & 0.472 & 0.484 & 0.497 & 0.501  & \textbf{0.449} & \textbf{0.466} & 0.472 & 0.490 & 0.469 & 0.492 & 0.515 & 0.517 & 1.144 & 0.857  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96 & 0.285 & 0.345 & 0.280 & 0.341   & \textbf{0.274} & \textbf{0.334} & 0.289 & 0.353 & 0.332 & 0.374 & 0.332 & 0.368 & 1.549 & 0.952  \\
            &\multicolumn{1}{c|}{}& 192 & 0.350 & 0.388 & 0.350 & 0.387  & \textbf{0.339} & \textbf{0.379} & 0.383 & 0.418 & 0.407 & 0.446 & 0.426 & 0.434 & 3.792 & 1.542  \\
            &\multicolumn{1}{c|}{}& 336 & 0.378 & 0.410 & 0.373 & 0.410  & \textbf{0.331} & \textbf{0.380} & 0.448 & 0.465 & 0.400 & 0.447 & 0.477 & 0.479 & 4.215 & 1.642  \\
            &\multicolumn{1}{c|}{}& 720 & 0.401 & 0.438 & 0.398 & 0.436  & \textbf{0.379} & \textbf{0.422} & 0.605 & 0.551 & 0.412 & 0.469 & 0.453 & 0.490 & 3.656 & 1.619  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96 & \textbf{0.288} & \textbf{0.345} & 0.291 & 0.346 & 0.290 & 0.342 & 0.299 & 0.343 & 0.326 & 0.390 & 0.510 & 0.492 & 0.626 & 0.560 \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.330} & 0.372 & 0.335 & 0.373  & 0.332 & 0.369 & 0.335 & \textbf{0.365} & 0.365 & 0.415 & 0.514 & 0.495 & 0.725 & 0.619 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.359} & 0.392 & 0.365 & 0.391  & 0.366 & 0.392 & 0.369 & \textbf{0.386} & 0.392 & 0.425 & 0.510 & 0.492 & 1.005 & 0.741 \\
            &\multicolumn{1}{c|}{}& 720  & \textbf{0.406} & \textbf{0.421} & 0.423 & 0.424 & 0.420 & 0.424 & 0.425 & \textbf{0.421} & 0.446 & 0.458 & 0.527 & 0.493 & 1.133 & 0.845 \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96  & \textbf{0.164} & 0.256 & 0.166 & 0.257  & 0.165 & \textbf{0.255} & 0.167 & 0.260 & 0.180 & 0.271 & 0.205 & 0.293 & 0.355 & 0.462 \\
            &\multicolumn{1}{c|}{}& 192 & 0.223 & 0.296 & 0.221 & 0.295  & \textbf{0.220} & \textbf{0.292} & 0.224 & 0.303 & 0.252 & 0.318 & 0.278 & 0.336 & 0.595 & 0.586 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.277} & 0.332 & \textbf{0.277} & 0.332  & 0.278 & \textbf{0.329} & 0.281 & 0.342 & 0.324 & 0.364 & 0.343 & 0.379 & 1.270 & 0.871 \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.365} & 0.387 & 0.368 & 0.389  & 0.367 & \textbf{0.385} & 0.397 & 0.421 & 0.410 & 0.420 & 0.414 & 0.419 & 3.001 & 1.267 \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Transfer learning task: PatchTST is pre-trained on Electricity dataset and the model is transferred to other datasets. A full benchmark regarding Table \ref{table::transferring performance  (full)}. The best results are in \textbf{bold}. }
	\label{table::transferring performance  (full)}
\end{table*}


\subsection{Robustness Analysis}
\subsubsection{Results with Different Random Seeds}

The results reported in the main text and appendix above are run with the fixed random seed 2021. To examine the robustness of our results, we train the supervised PatchTST model with 5 different random seeds: 2019, 2020, 2021, 2022, 2023 and calculate the MSE and MAE scores with each selected seed. The mean and standard derivation of the results are reported in Table \ref{tab::different seeds}. It is clear that the variances are considerably small which indicates the robustness against choice of random seeds of our model. 

We also validate the self-supervised PatchTST model on different runs. We pre-train the model once and fine-tune the model 5 times with different random batch selections. The mean and standard derivation across different runs are also provided in Table \ref{tab::different seeds}. We also observe that the variance is insignificant especially on large datasets while higher variance can be seen on smaller datasets.

\linespread{1.1}

\begin{table*}[!h]
	\centering
	\scalebox{0.7}{
		\begin{tabular}{cc|c|c|c|c|c}
			\cline{2-7}
			&\multicolumn{2}{c|}{$L$}& \multicolumn{2}{c|}{PatchTST/42 supervised} & \multicolumn{2}{c}{PatchTST/42 self-supervised} \\
			\cline{2-7}
			&\multicolumn{2}{c|}{Metric} & MSE & MAE & MSE & MAE \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & 0.15250.0024 & 0.20020.0023 & \textbf{0.14500.0008} & \textbf{0.19370.0010} \\
            &\multicolumn{1}{c|}{}& 192   & 0.19750.0015 & 0.24340.0010 & \textbf{0.18930.0003} & \textbf{0.23640.0006} \\
            &\multicolumn{1}{c|}{}& 336   & 0.24940.0012 & 0.28410.0014 & \textbf{0.24130.0003} & \textbf{0.27740.0005} \\
            &\multicolumn{1}{c|}{}& 720   & 0.31940.0002 & 0.33520.0003 & \textbf{0.31560.0020} & \textbf{0.33160.0016} \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96  & 0.36690.0006 & 0.25040.0007 & \textbf{0.35280.0022} & \textbf{0.24430.0016} \\
            &\multicolumn{1}{c|}{} & 192   & 0.38580.0004 & 0.25860.0004 & \textbf{0.37290.0013} & \textbf{0.25310.0009} \\
            &\multicolumn{1}{c|}{}& 336   & 0.39940.0010 & 0.26720.0016 & \textbf{0.38460.0020} & \textbf{0.25880.0011} \\
            &\multicolumn{1}{c|}{}& 720   & 0.43830.0097 & 0.29130.0104 & \textbf{0.42410.0007} & \textbf{0.28160.0010} \\
            \cline{2-7}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & 0.13040.0006 & 0.22340.0006 & \textbf{0.12560.0002} & \textbf{0.22100.0003} \\
			&\multicolumn{1}{c|}{}& 192   & 0.14820.0002 & 0.24030.0002 & \textbf{0.14510.0002} & \textbf{0.23970.0010} \\
			&\multicolumn{1}{c|}{}& 336   & 0.16590.0006 & 0.25960.0006 & \textbf{0.16240.0010} & \textbf{0.25760.0009} \\
			&\multicolumn{1}{c|}{}& 720   & 0.20190.0006 & 0.29170.0006 & \textbf{0.19900.0002} & \textbf{0.29160.0002}  \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTh1}} & 96    & 0.37520.0008 & 0.39990.0004 & \textbf{0.37000.0035} & \textbf{0.40010.0023} \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.41270.0012} & \textbf{0.42070.0006} & 0.41460.0012 & 0.42870.0013 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.42780.0033} & \textbf{0.43340.0028} & 0.42850.0018 & 0.44020.0017 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.44620.0035} & \textbf{0.46370.0027} & 0.46700.0052 & 0.47680.0033 \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & \textbf{0.27490.0005} & \textbf{0.33630.0006} & 0.28690.0039 & 0.34390.0016  \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.33850.0010} & \textbf{0.37890.0014} & 0.35230.0048 & 0.38550.0027 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.32880.0010} & \textbf{0.38230.0027} & 0.37790.0057 & 0.41120.0030  \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.37840.0010} & \textbf{0.42120.0009} & 0.39930.0054 & 0.43850.0038 \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & 0.28930.0009 & 0.34150.0007 & \textbf{0.28760.0012} & \textbf{0.34270.0011} \\
            &\multicolumn{1}{c|}{}& 192   & 0.33160.0008 & 0.36950.0007 & \textbf{0.32960.0026} & \textbf{0.36880.0016} \\
            &\multicolumn{1}{c|}{}& 336   & 0.36610.0022 & 0.39140.0012 & \textbf{0.35830.0015} & \textbf{0.38790.0016} \\
            &\multicolumn{1}{c|}{}& 720   & 0.42000.0056 & 0.42430.0033 & \textbf{0.40940.0044} & \textbf{0.41930.0013} \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96    & 0.16470.0011 & 0.25380.0010 & \textbf{0.16370.0020} & \textbf{0.25370.0024} \\
            &\multicolumn{1}{c|}{}& 192   & 0.22230.0018 & 0.29360.0014 & \textbf{0.21750.0011} & \textbf{0.29080.0013} \\
            &\multicolumn{1}{c|}{}& 336   & 0.27750.0020 & 0.32970.0010 & \textbf{0.27060.0016} & \textbf{0.32600.0016} \\
            &\multicolumn{1}{c|}{}& 720   & 0.36480.0024 & 0.38330.0010 & \textbf{0.35390.0023} & \textbf{0.37990.0024} \\
			\cline{2-7}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results with different random seeds in supervised and self-supervised PatchTST/42. The best results are in \textbf{bold}. }
	\label{tab::different seeds}
\end{table*}
\linespread{1}


\subsubsection{Results with Different Model Parameters}

To see whether PatchTST is sensitive to the choice of Transformer settings, we perform another experiments with varying model parameters. We vary the number of Transformer layers $L=\{3,4,5\}$ and select the model dimension $D=\{128, 256\}$ while the inner-layer of the feed forward network is $F = 2D$. In total, there are 6 different sets of model hyper-parameters to examine. Figure \ref{fig::robustness} shows the MSE scores of these combinations on different datasets. Except ILI dataset reveals high variance with different hyper-parameter settings, other datasets are robust to the choice of model hyper-parameters. 


\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.46]{figures/robustness.pdf}
\end{center}
\caption{MSE scores with varying model parameters. Each bar indicates the MSE score of a parameter combination. The combinations $(L, D) = (3,128)$, $(3,256)$, $(4,128)$, $(4,256)$, $(5,128)$, $(5,256)$ are orderly labeled from 1 to 6 in the figure. The model is run with supervised PatchTST/42 to forecast $96$ steps. For Traffic and Electricity datasets, we reduce the maximum number of epochs to 50 to save computational time.}
\label{fig::robustness}
\end{figure}


\subsection{Channel-independence Analysis}
\label{append::ci}

Intuitively, channel-mixing models should outperform the channel-independent ones since they have more flexibility to explore the cross-channel information, while with channel-independence the correlation is indirectly learnt via weight sharing. However, this is contrast to what we have observed. In Section \ref{sec::civsmix} we will provide an in-depth analysis on why channel-independence has better forecasting performance than channel mixing, and in Section \ref{sec::ciforothers} we show that channel-independence is a general technique that can be used not only for PatchTST but also for the other models. 


\subsubsection{Channel-independence vs Channel-mixing}
\label{sec::civsmix}

We find 3 key factors that makes channel-independent models more preferable:

\begin{itemize}[leftmargin= 20 pt,itemsep= 5 pt,topsep = 5 pt]
    \item Adaptability: Since each time series is passed through the Transformer separately, it generates its own attention maps. That means different series can learn different attention patterns for their prediction, as shown in Figure \ref{fig::attnmap}. In contrast, with the channel mixing approach, all the series share the same attention patterns, which may be harmful if the underlying multivariate time series carries series of different behaviors. Figure \ref{fig::attnmap} reveals an interesting observation that the prediction of unrelated time series relies on different attention patterns while similar series can produce similar maps (e.g. series 11, 25, and 81 contain similar patterns while they are different from others). We suspect this adaptability is one of the main reasons why PatchTST performs much better forecasting than Informer and other channel-mixing models.

    \item Channel-mixing models may need more training data to match the performance of the channel-independent ones. The flexibility of learning cross-channel correlations could be a double-edged sword, because it may need much more data to learn the information from different channels and different time steps jointly and appropriately, while channel-independent models only focus on learning information along the time axis. We examine this assumption by experiments where we train the models with varying training data size and  and the result is shown on left panel of Figure \ref{fig::weather}.  It is clear that channel-independent models converges faster against the size of training data. As what we have observed in the figure and Table \ref{tab:data}, the size of those widely used time series datasets may not be large enough for channel-mixing models to obtain similar performances in supervised learning.
    
    \item Channel-independent models are less likely to overfit data during training. We record the MSE loss on test data and plot on the right panel of Figure \ref{fig::weather}. Channel-mixing models show overfitting after a few initial epochs, while channel-independent models continue optimizing the loss with more training epochs. The best trained models are determined by validation data, which are approximately the lowest points in the test loss curves. It is clear that the forecasting performance of channel-independent models are better. 
\end{itemize}

Furthermore, we would like to comment on a few additional technical advantages of channel-independence: \textbf{(1)} Possibility of learning spatial correlations across series: Although we haven't focused on this research in our paper, the channel-independence design can be naturally extended to learn cross-channel relationships by using methods like graph neural networks \citep{graph1,graph2}. \textbf{(2)} Multi-task learning where different loss types can be imposed on different time series where the same underlying Transformer model is shared. \textbf{(3)} More robust to noise: If noise is dominant in one or several series, this noise will be projected to other series in the embedding space if we mix channels. Channel independence can mitigate this problem by only retaining the noise in these noisy channels. We can further alleviate the noise by introducing smaller weights to the objective losses that associate with noisy channels.

\begin{figure}[h]
\hspace*{-0.5cm}
\includegraphics[scale=0.8]{figures/attn_forecast_4cols_v4.pdf}
\caption{Attention maps and the forecasting of a few time series from Electricity dataset run with supervised PatchTST/64. Attention map is calculated by averaging the attention matrices over all the heads and across all the layers. For each time series, we show the attention map and the prediction in orange. The blue curves are the actual data. The curves before the back lines are the actual input data. Channel-independence design allow each series to learn its own attention map for forecasting in which the pattern can be more similar for more correlated series and different otherwise.}
\label{fig::attnmap}
\end{figure}

\begin{figure}[htbp!]
\begin{center}
\includegraphics[scale=0.6]{figures/figure_loss.pdf}
\end{center}
\caption{Channel-independence vs channel-mixing on Weather dataset. The base model is PatchTST/42, and the prediction length is $96$. We plot the mean values and error bars with 5 different random seeds: $\{2019,2020,2021,2022,2023\}$. \textbf{Left Panel:} Test loss vs train size. Here, train size denotes the fraction of the training data that is used to learn the model from scratch. Channel-independence contributes to a quicker convergence as more training data is available. \textbf{Right Panel:} Test loss vs epochs. Here, we use full train data and plot the first 20 epochs. Channel-mixing model quickly overfits the data.}
\label{fig::weather}
\end{figure}



\subsubsection{Performance of Channel-independence on Other Models}
\label{sec::ciforothers}

To show that channel-independence is a general technique that can be applied to the other models, we apply it to Informer \citep{informer}, Autoformer \citep{autoformer}, and FEDformer \citep{fedformer}. The results are shown in Table \ref{tab:ciforothers}. The channel-independent technique can improve the forecasting performance of those models generally. Although they are still not able to outperform PatchTST which is based on vanilla attention mechanism, we believe that more performance boost and computational reduction can be obtained with more advanced attention designs incorporating the channel-independence architecture. 


\linespread{1.2}
\begin{table*}[t]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cc|c||cc||cc|cc|cc|cc|cc|ccc}
			\cline{2-17}
			&\multicolumn{2}{c||}{Models}& \multicolumn{2}{c||}{PatchTST/42}& \multicolumn{2}{c|}{Informer}& \multicolumn{2}{c|}{Informer-CI}& \multicolumn{2}{c|}{Autoformer}& \multicolumn{2}{c|}{Autoformer-CI}& \multicolumn{2}{c|}{FEDformer}& \multicolumn{2}{c}{FEDformer-CI}\\
			\cline{2-17}
			&\multicolumn{2}{c||}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & 0.152  & 0.199  & 0.300  & 0.384  & \textbf{0.174 } & \textbf{0.232 } & 0.266  & 0.336  & \textbf{0.227 } & \textbf{0.289 } & 0.217  & 0.296  & \textbf{0.214 } & \textbf{0.278 } \\
            &\multicolumn{1}{c|}{}& 192   & 0.197  & 0.243  & 0.598  & 0.544  & \textbf{0.214 } & \textbf{0.270 } & 0.307  & 0.367  & \textbf{0.269 } & \textbf{0.318 } & 0.276  & 0.336  & \textbf{0.258 } & \textbf{0.322 } \\
            &\multicolumn{1}{c|}{}& 336   & 0.249  & 0.283  & 0.578  & 0.523  & \textbf{0.266 } & \textbf{0.310 } & 0.359  & 0.395  & \textbf{0.315 } & \textbf{0.344 } & 0.339  & 0.380  & \textbf{0.302 } & \textbf{0.336 } \\
            &\multicolumn{1}{c|}{}& 720   & 0.320  & 0.335  & 1.059  & 0.741  & \textbf{0.327 } & \textbf{0.356 } & 0.419  & 0.428  & \textbf{0.384 } & \textbf{0.389 } & 0.403  & 0.428  & \textbf{0.374 } & \textbf{0.369 } \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & 0.367  & 0.251  & 0.719  & \textbf{0.391}  &   \textbf{0.705}    &   0.402    & 0.613  & 0.388  & - & - & 0.587  & 0.366  & - & - \\
            &\multicolumn{1}{c|}{} & 192   & 0.385  & 0.259  & \textbf{0.696}  & \textbf{0.379}  &   0.720    &   0.407    & 0.616  & 0.382  & - & - & 0.604  & 0.373  & - & - \\
            &\multicolumn{1}{c|}{}& 336   & 0.398  & 0.265  & 0.777  & \textbf{0.420}  &    \textbf{0.750}   &    0.421   & 0.622  & 0.337  & - & - & 0.621  & 0.383  & - & - \\
            &\multicolumn{1}{c|}{}& 720   & 0.434  & 0.287  & 0.864  & 0.472  &   -    &   -    & 0.660  & 0.408  & - & - & 0.626  & 0.382  & - & - \\
            \cline{2-17}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & 0.130  & 0.222  & 0.274  & 0.368  & \textbf{0.203}  & \textbf{0.299}  & 0.201  & 0.317  & - & - & 0.193  & 0.308  & - & - \\
			&\multicolumn{1}{c|}{}& 192   & 0.148  & 0.240  & 0.296  & 0.386  & \textbf{0.221}  & \textbf{0.316}  & 0.222  & 0.334  & - & - & 0.201  & 0.315  & - & - \\
			&\multicolumn{1}{c|}{}& 336   & 0.167  & 0.261  & 0.300  & 0.394  & \textbf{0.241}  & \textbf{0.337}  & 0.231  & 0.338  & - & - & 0.214  & 0.329  & - & - \\
			&\multicolumn{1}{c|}{}& 720   & 0.202  & 0.291  & 0.373  & 0.439  &   \textbf{0.314}    &    \textbf{0.391}   & 0.254  & 0.361  & - & - & 0.246  & 0.355  & - & - \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ILI}}& 24    & 1.522  & 0.814  & 5.764  & 1.677  & \textbf{5.514 } & \textbf{1.629 } & \textbf{3.483 } & \textbf{1.287 } & 4.210  & 1.500  & \textbf{3.228 } & \textbf{1.260 } & 3.280  & 1.264  \\
            &\multicolumn{1}{c|}{} & 36    & 1.430  & 0.834  & \textbf{4.755 } & \textbf{1.467 } & 5.515  & 1.628  & 3.103  & \textbf{1.148 } & \textbf{2.809 } & 1.162  & \textbf{2.679 } & \textbf{1.080 } & 2.862  & 1.126  \\
            &\multicolumn{1}{c|}{}& 48    & 1.673  & 0.854  & \textbf{4.763 } & \textbf{1.469 } & 5.263  & 1.574  & \textbf{2.669 } & \textbf{1.085 } & 3.218  & 1.267  & \textbf{2.622 } & \textbf{1.078 } & 2.834  & 1.150  \\
            &\multicolumn{1}{c|}{}& 60    & 1.529  & 0.862  & \textbf{5.264 } & \textbf{1.564 } & 5.330  & 1.602  & \textbf{2.770 } & \textbf{1.125 } & 3.627  & 1.396  & \textbf{2.857 } & \textbf{1.157 } & 3.115  & 1.240  \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh1}} & 96    & 0.375  & 0.399  & 0.865  & 0.713  & \textbf{0.590 } & \textbf{0.517 } & 0.449  & 0.459  & \textbf{0.414 } & \textbf{0.421 } & \textbf{0.376 } & 0.419  & 0.387  & \textbf{0.407 } \\
            &\multicolumn{1}{c|}{}& 192   & 0.414  & 0.421  & 1.008  & 0.792  & \textbf{0.677 } & \textbf{0.566 } & 0.500  & 0.482  & \textbf{0.453 } & \textbf{0.448 } & \textbf{0.420 } & 0.448  & 0.439  & \textbf{0.438 } \\
            &\multicolumn{1}{c|}{}& 336   & 0.431  & 0.436  & 1.107  & 0.809  & \textbf{0.710 } & \textbf{0.600 } & 0.521  & 0.496  & \textbf{0.496 } & \textbf{0.468 } & \textbf{0.459 } & 0.465  & 0.479  & \textbf{0.455 } \\
            &\multicolumn{1}{c|}{} & 720   & 0.449  & 0.466  & 1.181  & 0.865  & \textbf{0.777 } & \textbf{0.660 } & \textbf{0.514 } & \textbf{0.512 } & 0.662  & 0.568  & 0.506  & 0.507  & \textbf{0.485 } & \textbf{0.478 } \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & 0.274  & 0.336  & 3.755  & 1.525  & \textbf{0.390 } & \textbf{0.410 } & 0.358  & 0.397  & \textbf{0.337 } & \textbf{0.373 } & 0.346  & 0.388  & \textbf{0.297 } & \textbf{0.348 } \\
            &\multicolumn{1}{c|}{}& 192   & 0.339  & 0.379  & 5.602  & 1.931  & \textbf{0.456 } & \textbf{0.463 } & 0.456  & 0.452  & \textbf{0.409 } & \textbf{0.419 } & 0.429  & 0.439  & \textbf{0.382 } & \textbf{0.399 } \\
            &\multicolumn{1}{c|}{}& 336   & 0.331  & 0.380  & 4.721  & 1.835  & \textbf{0.523 } & \textbf{0.503 } & 0.482  & 0.486  & \textbf{0.432 } & \textbf{0.443 } & 0.496  & 0.487  & \textbf{0.410 } & \textbf{0.428 } \\
            &\multicolumn{1}{c|}{}& 720   & 0.379  & 0.422  & 3.647  & 1.625  & \textbf{0.843 } & \textbf{0.661 } & 0.515  & 0.511  & \textbf{0.443 } & \textbf{0.463 } & 0.463  & 0.474  & \textbf{0.422 } & \textbf{0.444 } \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & 0.290  & 0.342  & 0.672  & 0.571  & \textbf{0.383 } & \textbf{0.414 } & 0.505  & 0.475  & \textbf{0.455 } & \textbf{0.441 } & \textbf{0.379 } & 0.419  & 0.408  & \textbf{0.413 } \\
            &\multicolumn{1}{c|}{}& 192   & 0.332  & 0.369  & 0.795  & 0.669  & \textbf{0.420 } & \textbf{0.434 } & \textbf{0.553 } & \textbf{0.496 } & 0.598  & 0.512  & \textbf{0.426 } & 0.441  & 0.445  & \textbf{0.432 } \\
            &\multicolumn{1}{c|}{}& 336   & 0.366  & 0.392  & 1.212  & 0.871  & \textbf{0.465 } & \textbf{0.467 } & 0.621  & 0.537  & \textbf{0.566 } & \textbf{0.504 } & \textbf{0.445 } & 0.459  & 0.476  & \textbf{0.452 } \\
            &\multicolumn{1}{c|}{}& 720   & 0.420  & 0.424  & 1.166  & 0.823  & \textbf{0.529 } & \textbf{0.502 } & \textbf{0.671 } & 0.561  & 0.680  & \textbf{0.557 } & 0.543  & 0.490  & \textbf{0.533 } & \textbf{0.481 } \\
			\cline{2-17}
			&\multirow{4}*{\rotatebox{90}{ETTm2}} & 96    & 0.165  & 0.255  & 0.365  & 0.453  & \textbf{0.208 } & \textbf{0.298 } & 0.255  & 0.339  & \textbf{0.218 } & \textbf{0.308 } & 0.203  & 0.287  & \textbf{0.198 } & \textbf{0.284 } \\
            &\multicolumn{1}{c|}{}& 192   & 0.220  & 0.292  & 0.533  & 0.563  & \textbf{0.274 } & \textbf{0.345 } & \textbf{0.281 } & 0.340  & \textbf{0.281 } & \textbf{0.339 } & 0.269  & 0.328  & \textbf{0.259 } & \textbf{0.320 } \\
            &\multicolumn{1}{c|}{}& 336   & 0.278  & 0.329  & 1.363  & 0.887  & \textbf{0.351 } & \textbf{0.394 } & 0.339  & 0.372  & \textbf{0.336 } & \textbf{0.370 } & 0.325  & 0.366  & \textbf{0.315 } & \textbf{0.353 } \\
            &\multicolumn{1}{c|}{}& 720   & 0.367  & 0.385  & 3.379  & 1.338  & \textbf{0.482 } & \textbf{0.474 } & 0.433  & 0.432  & \textbf{0.428 } & \textbf{0.418 } & 0.421  & 0.415  & \textbf{0.412 } & \textbf{0.406 } \\
			\cline{2-17}
		\end{tabular}
	}
	\caption{Channel-independence for other models. CI denote channel-independence. Baselines without CI are cited from \citet{dlinear}. The better results between CI and non-CI versions are in \textbf{bold}. PatchTST/42 is placed on the left for easy reference to other CI-based models. '-' denotes running out of GPU memory even with batch size 1, or exceeding the maximum running time (12 hours).}
	\label{tab:ciforothers}
\end{table*}
\linespread{1}

\end{document}
