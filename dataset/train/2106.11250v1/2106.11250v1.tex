\appendix


\section{Model Architecture}
\label{sec:appen_model}
As described in \sref{sec:backbone}, we use a transformer model on top of the discrete video tokens generated by VQ-VAE. Since transformers have different variants, we here show details of our architecture for clarity.
The design of our method largely follows the practice in BERT~\cite{devlin2019bert}, TimeSformer~\cite{bertasius2021space}, Sparser Transformer~\cite{child2019generating}, ViViT~\cite{arnab2021vivit}, and MoCoV3~\cite{chen2021mocov3}.

\subsection{Backbone Model}

\paragraph{Embedding.} 
Given the video frames , we first use VQ-VAE~\cite{van2017neural,ramesh2021zero} to discrete them into video tokens . 
We next use an embedding layer () that to map these discrete tokens to continuous vectors. 
Since transformer layers are permutation-invariant, we follow~\cite{dosovitskiy2020image,devlin2019bert} to add positional information into the input. 
The positional embedding () is factorized as a sum of the temporal embedding , the height embedding , and the width embedding . 
This factorization reduces the number of trainable parameters to encode positional information, which empirically shows a slightly better result.
Finally, a Layer-Normalization~\cite{ba2016layer} layer is added to get the initial hidden outputs :

where we use the superscript 0 to denote that it is the hidden outputs before the first transformer layer.

\paragraph{Attention Blocks.}
Before introducing the detailed model architecture, we first describe the basic building components: the attention block.
An attention block is built based on the attention operator (i.e., `') with a residual connection.
The attention operator takes a single query vector  and its context  as input.
It first computes the attention score between  and each context vector , then the attention scores are normalized by the .
Lastly, the output is a weighted-sum over all the context vectors (transferred by a `value' matrix ):

To compose the attention block from the previous attention operator, the residual connection and layer normalization (i.e., `') are added.
We follow the original transformer model~\cite{vaswani2017attention} that uses a post-layer-norm layout:

In order to reduce computational cost and memory, we also adapt the attention block suggested in Sparse Transformer~\cite{child2019generating} that takes two sets of context vectors  and  as input.
This special attention block computes attention for the two context-vector sets separately and concatenates their output together.
In our case, suppose  and  are the rows and columns of a square matrix, then it reduces the computation cost of calculating attention scores from  to , where  is the number of rows/columns:

\paragraph{Spatio-Temporal Transformer Layer.} 
The spatio-temporal transformer layer is composed with the previously-introduced attention blocks and an additional MLP block.
The -th layer takes the output of the previous layer  as input and outputs the hidden states .
We separate the attention into two attention blocks: the temporal attention block  and the spatial attention block .
Without loss of generality, we will use  and  to denote the intermediate results from temporal and spatial attention blocks, respectively.
First, the temporal attention block attends to the tokens at the same spatial location but in different frames (i.e., at different timesteps): .
Next, the spatial attention block attends to the tokens in the same frame: .
To reduce the computational cost, we incorporate the sparse attention block~\cite{child2019generating} (detailed in the previous paragraph) that factorizes the attention over height and width: , .
The MLP block has two fully-connected layers with GeLU~\cite{hendrycks2016gaussian} activation in the middle.
Overall, the formula of one spatio-temporal transformer layer is:

\paragraph{\texttt{[CLS]} Token.} 
Following the practice in BERT~\cite{devlin2019bert} design, we add a special \texttt{[CLS]} (abbreviation of `classification') token and take its output as the representation of the whole sequence.
We follow TimeSformer~\cite{bertasius2021space} to compute the its output: the \texttt{[CLS]} token attends over the context separately and then the outputs are averaged. We take the temporal attention layer as an example.
Suppose  is the \texttt{[CLS]} feature vector output by layer , then the temporal attention layer do the following computation:

The other attention blocks process the \texttt{[CLS]} token similarly. 


\subsection{Pre-Training and Fine-Tuning Heads}
\label{sec:appen_heads}
Pre-training or fine-tuning usually requires a few additional modules (i.e., heads) on top of the transformer layers that convert the output features to the desired probabilities or vectors.
We next describe the heads used in our pre-training and fine-tuning process.
\paragraph{Token Head for Mask-then-Predict.} 
We first define the prediction head over the tokens following BERT\cite{devlin2019bert}. 
It first processes the last-layer hidden outputs  using a fully-connected layer (with GELU activation~\cite{hendrycks2016gaussian} and layer normalization~\cite{ba2016layer}):

In our \ourmask method (\sref{sec:mask_task}), we will predict the masked tokens (i.e., the token before masking) from their context. 
We thus further convert this hidden vector into a distribution over the token vocabulary:

The weight  is shared with input word embedding layer  \cite{press2017using, devlin2019bert} while the bias  is trained independently.

\paragraph{Contrastive Learning Head}
Next we discuss the pre-training heads for \ourcl. It is on top of the \texttt{[CLS]} hidden output .  We encode the hidden state with MLP.
We use batch normalization \cite{ioffe2015batch} inside the MLP head following the practice in \cite{chen2021mocov3}.

This  feature is used in computing the contrastive loss as in \sref{sec:contrastive}.

\paragraph{FC Layer for Fine-Tuning.}
When fine-tuning for action classification task, we add a fully-connected (FC) layer to the \texttt{[CLS]} output .
We initialize its weight and bias to zero.

\subsection{Special Tokens} 
Besides the  token types introduced in the vocabulary of the VQ-VAE (see \sref{sec:mask_task}), we add several special tokens into the `vocabulary', namely a \texttt{[CLS]} token is introduced as a stub for the whole-video representation, 
a \texttt{[PAD]} token is used when the actual clip length is less than the model's expected input length. For the \ourmask task, we follow BERT~\cite{devlin2019bert} to replace the masked tokens with a specific \texttt{[MASK]} token.



\section{Pre-Training Details}
\label{sec:appen_pretrain}
\begin{table}
\centering
\caption{\textbf{Induced Masking ratio} w.r.t. to different input resolutions and \#masking blocks. The numbers of blocks/masking ratio for each resolution setting used in our experiments are shown in \textbf{bold}.}
\begin{tabular}{ccc|ccccc}
\toprule
\multicolumn{3}{c|}{Input Resolution} & \multicolumn{5}{c}{\#Masking Blocks}                          \\ \cline{4-8} 
Length          & Frame Size        & Token Map Size & 4    & 5             & 6             & 7             & 8    \\ \hline
5               & 128              & 16                & 11.9 & \textbf{14.5} & 17.0          & 19.4          & 21.7 \\
5               & 256               & 32                & 10.6 & 13.1          & \textbf{15.2} & 17.5          & 19.5 \\
10              & 128               & 16                & 10.4 & 12.8          & \textbf{15.0} & 17.1          & 19.2 \\
10              & 256               & 32                & 9.3  & 11.4          & 13.4          & \textbf{15.4} & 17.2 \\ \bottomrule
\end{tabular}\label{tab:appen_masking_blocks}
\end{table}

\subsection{Masking Blocks}

\label{sec:appen_masking_blocks}
As described in \sref{sec:mask_task}, we mask the tokens by blocks (a cube-shape set of tokens).
To avoid masking all the tokens in the clip, we control the maximum block length for the time domain, height, and width.
For spatial dimensions (i.e., height and width), the maximum length is half of the full length (e.g., the maximum block length will be 16 for a token map of length 32).
For temporal dimension (i.e., the clip length), the maximum length will be 2/3 (round up) of the full length so that it allows long-range modeling.
Under these constraints, we uniformly sample a fixed number of mask blocks and take their union to construct the final mask.
The number of blocks is decided by the induced masking ratio, which depends on the input resolutions.
In \tref{tab:appen_masking_blocks}, we show the induced masking ratio w.r.t. different input resolutions and \#masking blocks.
We take the VQ-VAE~\cite{van2017neural} provided in DALL-E~\cite{ramesh2021zero} that has a compression factor of 8, thus the length of the token map is always 1/8 of the frame size.
For each input resolution, we select the number of blocks (shown in bold in \tref{tab:appen_masking_blocks}) whose induced masking ratio is closet to \% following BERT~\cite{devlin2019bert}.




\subsection{Contrastive Learning Loss}
For completeness, we list the two  losses used in contrastive learning here. The first loss for clip  from  is:

The symmetric loss  for feature of the other clip sample  from  (and its feature ) is:



\begin{table}
\centering
\caption{\textbf{Model Configuration}. The `Small' model is mainly used in the analysis (\sref{sec:analysis}) while `Large-Half' model is mainly used in the results (\sref{sec:results}) for the final large-scale experiments.  `Vocab Size' is the number of token types in our model, defined by the pre-trained VQ-VAE model~\cite{ramesh2021zero}. }
\begin{tabular}{@{}lrrrr@{}}
\toprule
                      & Small (in \sref{sec:analysis}) & \;\;\;\;\;\;\;\;\;\;\;\; Base & Large-Half (in \sref{sec:results}) \\ \midrule
Layers                & 6                   & 12     & 24                            \\
Dimensions            & 512                 & 768    & 1024                          \\
Attention Heads       & 8                   & 12     & 16                            \\
Attention Head Dim    & 64                  & 64     & 32                            \\
MLP Intermediate Size & 2048                & 3072   & 2048                          \\
Vocab Size          & 8192                & 8192   & 8192                          \\
Params                & 29.4M               & 119.7M & 210.1M                        \\ \bottomrule
\end{tabular}
\label{tab:appen_model}
\end{table}


\begin{table}
\centering
\caption{\textbf{Training Hyperparameters}.  `Pre-Train-L' is our final Large model in \sref{sec:results} that takes a large-half model. `Pre-Train-S' is the small pre-training in analysis (\sref{sec:analysis}). `K-400' is Kinetics-400. *The batch size for pre-training is the number of samples in updating the weights. Since we use gradient accumulation, it is not correlated to the number of negative examples in \ourcl.}
\resizebox{\textwidth}{!}{

\begin{tabular}{@{}lcc|ccccc@{}}
\toprule
                   & Pre-Train-S & Pre-Train-L & SSV2        & Diving48       & UCF101       & HMDB51       & K-400       \\ \midrule
\multicolumn{8}{l}{\footnotesize{\textit{Optimization}}} \\
Number of Epochs   & 100                 & 10                   & 22          & 50             & 50           & 50           & 30                 \\
Number of Updates  & 120K                & 12K                  & 29K         & 5.8K           & 3.7K         & 1.4K         & 48K                \\
Learning Rate      & 3e-4                & 1e-3                 & \multicolumn{5}{c}{1e-4 for small/base model, 5e-5 for large-half model} \\
Warm-Up Ratio      & 0.05                & 0.1                  & \multicolumn{5}{c}{0.1}                                                         \\
LR Decay           & \multicolumn{2}{c|}{Linear}                 & \multicolumn{5}{c}{Linear}                                                      \\
Backbone Dropout   & \multicolumn{2}{c|}{0.1}                    & \multicolumn{5}{c}{0.1}                                                         \\
Last FC Dropout    & \multicolumn{2}{c|}{-}                      & \multicolumn{5}{c}{0.0}                                                         \\
Optimizer          & \multicolumn{2}{c|}{AdamW}                  & \multicolumn{5}{c}{AdamW}                                                         \\
Batch Size         & \multicolumn{2}{c|}{1024*}                  & \multicolumn{5}{c}{128}                                                         \\
Weight-Decay       & \multicolumn{2}{c|}{0.05}                   & \multicolumn{5}{c}{0.01}                                                        \\
Adam Beta1         & \multicolumn{2}{c|}{0.9}                    & \multicolumn{5}{c}{0.9}                                                         \\
Adam Beta2         & \multicolumn{2}{c|}{0.98}                   & \multicolumn{5}{c}{0.999}                                                       \\
Adam Epsilon       & \multicolumn{2}{c|}{1e-8}                   & \multicolumn{5}{c}{1e-8}                                                        \\
Grad-Clipping Norm & \multicolumn{2}{c|}{1.0}                    & \multicolumn{5}{c}{1.0}                                                         \\ \midrule
\multicolumn{8}{l}{\footnotesize{\textit{Data Augmentation}}} \\
Color Distortion/Gray-Scale
                   & \multicolumn{2}{c|}{No}                     & \multicolumn{5}{c}{No}                                                      \\
Training Spatial Resize      
                   & \multicolumn{2}{c|}{1 (Frame Size)}                     & \multicolumn{5}{c}{2 (Frame Size, Frame Size * 1.25)}                                                      \\
Training Spatial Crops      
                   & \multicolumn{2}{c|}{1 (Center)}                & \multicolumn{5}{c}{3 (Top-Left, Center, Bottom-Right)}                                                      \\
Training Temporal Crops      
                   & \multicolumn{2}{c|}{2 (Random Uniform)}      & \multicolumn{5}{c}{1 (Random Uniform)}                                                      \\
Inference Spatial Resize      
                   & \multicolumn{2}{c|}{1 (Frame Size)}                     & \multicolumn{5}{c}{1 (Frame Size)}                                                      \\
Inference Temporal Crops     
                   & \multicolumn{2}{c|}{1 (Random Uniform)}      & \multicolumn{5}{c}{10 (Uniform)}                                                      \\ 
Training Spatial Flip      
                   & \multicolumn{2}{c|}{No}                    & \multicolumn{1}{c|}{No} & \multicolumn{4}{c}{Yes}                                                      \\
Inference Spatial Crops     
                   & \multicolumn{2}{c|}{1 (Center)}                & \multicolumn{1}{c|}{1 (Center)} & \multicolumn{4}{c}{3 (Top-Left, Center, Bottom-Right)}                                                      \\
\bottomrule
\end{tabular}
}
\label{tab:appen_hyperparam}
\end{table}


\section{Experiment Details}
\label{sec:appen_experiment}
In this section, we show our model configuration and training hyperparameters in details to support the reproducibility of our experiments.

\paragraph{Model Configuration.} Our model configuration details is shown in \tref{tab:appen_model}. Most analysis results (\sref{sec:analysis}) take `Small'  models and our final results (\sref{sec:results}) take `Large-Half' model. Other models are used in \sref{sec:model_scale_analysis}. The final `Large-Half' model halves the attention head dimension and MLP intermediate size as in \cite{child2019generating}. 
For the pre-training heads, we follow BERT to take the intermediate dimension of the token-head to be the same as the backbone's hidden dimension.
For the CLS head, we take 3 layers in MLP and 4096 intermediate dimensions. 
The output dimension is 256.
We test with different number of layers and hidden dimensions of CLS head and generally find that larger head gives better results (as in \cite{qian2020spatiotemporal, chen2021mocov3}).
This CLS head contributes to about  pre-training computational cost overhead.


\paragraph{Training Hyperparameters.} 
We list the training hyperparameters in \tref{tab:appen_hyperparam}. 
Most of the hyperparameters are inherited from previous works to allow fair comparison and reduce tuning effort. 
For optimizer hyperparameters, we mostly follow the implementation of DALL-E~\cite{ramesh2021zero} and BERT~\cite{devlin2019bert}. 
\ssv follows the epoch number in \cite{feichtenhofer2019slowfast} and \cite{feichtenhofer2021large}.
To reduce the computational cost, we pre-extract the VQ-VAE tokens thus we employ a fixed set of spatial data augmentations.
As listed in the bottom of \tref{tab:appen_hyperparam}, we exclude any color distortion and gray scale augmentation.
We resize the video clip to the desired frame size and center-crop it during pre-training.
For downstream tasks, we resize the video clip to frame size or 1.5 times of the frame size, then crop the clip (with frame-size by frame-size spatial size) from the top-left, center, and bottom-right. 
We apply (horizontal) flip to the raw frames, thus a total of 12 spatial augmentations are extracted (12 = 2 resize  3 crops  2 flip/no-flip).
The only exception is \ssv.
This dataset needs to distinguish left/right motions thus we exclude the flip augmentation and only use the center crop during inference following previous works~\cite{feichtenhofer2019slowfast, feichtenhofer2021large}.
Following previous works~\cite{feichtenhofer2019slowfast}, we increase the training epochs for the non-pre-training models (in \sref{sec:pretrain_analysis}) by 4\x~for small datasets (\diving, \ucf, \hmdb) and 1.5\x~for larger datasets (\ssv, \ksmall).



\begin{table}
\centering
\caption{\textbf{Key statistics of video datasets} used in this paper. \htm is used for pre-training while others are downstream datasets. The number of training/validation examples in \hmdb and \ucf are reported for the train-val split 1.}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}lcccccc@{}}
\toprule
                       & HowTo100M & SSV2   & Diving48 & UCF101 & HMDB51 & Kinetics-400 \\ \midrule
Training               & 1238791   & 168913 & 15027    & 9537   & 3570   & 205418       \\
Validation             & -         & 24777  & 1970     & 3783   & 1530   & 17686        \\
Number of Classes      & -         & 174    & 48       & 101    & 51     & 400          \\
Average Video Duration & 6.5min    & 4s     & 6s       & 7s     & 4s     & 10s          \\ \bottomrule
\end{tabular}}
\label{tab:appen_dataset}
\end{table}

\section{Dataset Details}
\label{sec:appen_dataset}
In \tref{tab:appen_dataset}, we list the key statistics of the datasets used in our paper.
\htm is our pre-training datasets that has long-duration uncurated videos.
The videos are collected from YouTube by searching key phrases thus the scale could be easily increased. 
\ssv and \ksmall are two large downstream datasets, where \ssv focuses more on the actions and \ksmall focuses more on the scenes.
\diving, \ucf, \hmdb are three small datasets.
Different from previous datasets on classifying different action types (thus might be potentially inferred from single frames), \diving studies the three stages (takeoff, flight, and entry) of competitive diving.
Thus achieving good results on \diving requires an understanding of the whole video clip.

\section{Computational Cost}
The pre-training takes around 8.9K V100 GPU hours. 
This computational cost is at the same level as ViT~\cite{dosovitskiy2020image} supervised training (5.5K hours on ImageNet-21K~\cite{russakovsky2015imagenet} and 12.8K on JFT\cite{sun2017revisiting}).
It is also at the same level of supervised training a model on Kinetics-400 dataset (6.4K for SlowFast~\cite{feichtenhofer2019slowfast}, about 5.6K for TimeSformer-L \cite{bertasius2021space}).
For fine-tuning, SSV2, Diving48, UCF101, HMDB51, and Kinetics-400 take 1K, 200, 150, 40, 2K GPU hours, respectively.
For analysis, the pre-training takes about 160 GPU hours.
Besides the final model training, energy is also spent on tuning the model and finding the best configuration.
As shown in \sref{sec:cl_analysis}, our method is more robust to the hyperparameters.


\section{Additional Analysis Results}
\label{sec:appen_analysis}

\begin{table*}
\centering
\caption{Results of \textbf{different attention-module layouts and layer-normalization positions}.  `Speed' is the normalized pre-training speed (i.e., number of samples / GPU / second). Models are pre-trained on \htm for 10 epochs. The result numbers represent \ucf accuracy.  }
\begin{tabular}{@{}ccc|cc@{}}
\toprule
        & Params & Speed & Pre-LayerNorm & Post-LayerNorm \\ \midrule
TxHxW                             & 23.1M       &  12.6         &   -            & 65.9           \\
T,HxW \cite{bertasius2021space}   & 27.1M       &  20.0         &   -            & 69.0          \\
T,H,W                             & 35.8M       &  26.4         & 69.0           & 69.6           \\
T,H|W (ours)                      & 29.4M       &  32.0         & 67.6           & 69.4           \\\bottomrule
\end{tabular}\label{tab:layout}
\end{table*}



\subsection{Model Architecture Comparisons}
\label{sec:appen_arch_analysis}
\paragraph{Attention Layouts.}
We here compare different alternative model architectures in Table~\ref{tab:layout}.
We first experiment with different attention layouts discussed in Sec.~\ref{sec:backbone}.
We consider the sparse attention as proposed in \cite{child2019generating} and the sequential attention blocks as in \cite{bertasius2021space}. 
The `TxHxW' model is the basic attention module that takes the flattened tokens as input (of shape T \x H \x W). 
At each layer, each token attends to all other tokens.
The `T,HxW' model separates the temporal attention and spatial attention (`Divided Space-Time' in \cite{bertasius2021space}). 
The `T,H,W' model processes three attention sequentially (`Axial Attention' in \cite{bertasius2021space}).
The `T, H|W' model is our default model that sequentially conduct temporal attention and spatial attention, where the spatial attention are parallel into the height attention and width attention.
As shown in \tref{tab:layout}, `T,H|W' reaches a balance between speed and accuracy.

\paragraph{Pre-Layer-Normalization vs. Post-Layer-Normalization.}
Besides the architectures listed above, we also consider the pre-layer-norm (used in GPT and ViT) and post-layer-norm (used in BERT) variation. 
We empirically find that post-layer-norm architecture is better for our pre-training tasks as shown in \tref{tab:layout} (comparing the last 2 columns).


\subsection{Noisy Masking for Mask-then-Predict}

Our default masking strategy replaces all masked tokens with a special \texttt{[MASK]} symbol.
We also experiment with BERT's masking strategy that only replaces \% of masked tokens to the MASK symbol. 
For other tokens, \% are randomly sampled from the `vocabulary' and \% are kept the same.
For smaller experiments, the two masking strategies show similar results. 
However, this BERT's noisy masking strategy has lower convergence stability on the larger model pre-training.
The pre-training diverges after about  epochs (out of the  epochs).



\begin{table}[t]
\centering
\caption{
Impact of \textbf{masking ratio}. All models are pre-trained with only \ourmask task. 
\#Blocks is the number of masking blocks.
Default setup is underlined. 
}
\begin{tabular}{@{}c|cc|cc@{}}
\toprule
Strategy & \#Blocks & Ratio & Mask-Accu.                  & UCF101          \\ \midrule
Block            & 4              & 11.9\%        &  17.9                       & 66.8            \\
\underline{Block}            & \underline{5}              & \underline{14.5\%}        &  17.6                       & 68.3           \\
Block            & 6              & 17.0\%        &  17.3                       & 67.3            \\ 
i.i.d.           & -              & 11.9\%        &  25.6                       & 64.5                \\
i.i.d.           & -              & 14.5\%        &  24.3                       & 63.5           \\
i.i.d.           & -              & 17.0\%        &  24.0                       & 64.0           \\ \bottomrule
\end{tabular}\label{tab:appen_masking_ratio}
\end{table}

\subsection{Masking Ratio for Block-Masking and I.I.D. Masking}
We test the effect of different masking ratios.
In the main text, we control the number of blocks for block masking.
In \tref{tab:appen_masking_ratio}, we here also show the results of matched masking ratio for i.i.d. masking for completeness.
Empirically, the result differences among various masking ratios are marginal and the original BERT's \% masking ratio (with roughly  masking blocks) works slightly better.
Thus we always select the number of mask blocks whose induced masking ratio is closest to \%.
For all masking ratios, block masking shows significantly better results than the i.i.d. masking.



\begin{table}[t]
\centering
\caption{ Impact of \textbf{\ourcl loss weight }. Default setup is underlined. }
\begin{tabular}{l|ccc}
\toprule
           & Mask-Accu. & CL-Loss & UCF101 \\ \midrule
0.0                & 17.6      & -       & 68.3  \\
0.5                & 17.5      & 1.07    & 70.2  \\
\underline{1.0}                & 17.2      & 1.06    & 69.4  \\
2.0                & 16.9      & 1.05    & 68.0  \\
         & -         & 1.07    & 57.1  \\
\bottomrule
\end{tabular}\label{tab:loss_ratio}
\end{table}

\subsection{Impact of Contrastive Learning Loss Weight}
In \tref{tab:loss_ratio}, we show the impact of loss weight 
(see Sec.~\ref{sec:objective}).
Since the loss have been calibrated by multiplying the temperature,  shows stable results and  is slightly better. 
Setting  will let the model to focus mostly on \ourcl task and its result is worse than pure \ourmask pre-training (i.e., ).
We also list the pure \ourcl pre-training results here (denoted as  but it excludes the \ourmask loss and set ) for reference.








\section{Visualizations}
\label{sec:appen_visualization}

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\includegraphics[
                width=0.8\columnwidth,
                ]{figs/dataset_vis.pdf}
                \vspace{-6pt}
\caption{
Data samples from temporally-heavy and spatially-heavy datasets.
While temporally-heavy datasets need the temporal information to make decisions, most actions in spatially-heavy datasets could be inferred from single frames.
}
\label{fig:dataset_vis}
\end{center}
\vspace{-10pt}
\end{figure}

\subsection{Temporally-Heavy vs. Spatially-Heavy Datasets}
\label{sec:append_dataset_visualization}
We illustrate the differences between temporally-heavy and spatially-heavy datasets in \fref{fig:dataset_vis}.
We here show equally-distributed frames from the video and the label of the video clip.
Note that we do not cherry-pick the data but aim for showing the nature of each dataset.
Overall, understanding in temporally-heavy datasets needs temporal modeling, whereas the action labels of spatially-heavy datasets could be inferred from a single frame.
To understand the SSV2\cite{goyal2017something} example in \fref{fig:dataset_vis}.(a), the model needs to understand the causality, i.e., the order of the frames decides the action label.
In \fref{fig:dataset_vis}.(b), the competitive diving dataset Diving48~\cite{li2018resound} also requires considering nearly all frames to make the decision.
However, for the spatially-heavy datasets (UCF101~\cite{soomro2012ucf101}, HMDB51~\cite{kuehne2011hmdb}, Kinetics-400~\cite{carreira2017quo}), the action label could be inferred from any single sampled frame. 
These observations result in the pretty high frame-level accuracy (i.e., not modeling temporal interactions) in \sref{sec:results}.



\begin{figure}[t]
\vskip 0.1in
\begin{center}
\includegraphics[
                width=0.95\columnwidth,
                ]{figs/block_mask_new.pdf}
                \vspace{-6pt}
\caption{
Nearest-neighbour reconstruction of  \textit{block masking} and \textit{i.i.d masking}.
We mask tokens at different ratios and reconstruct them by simply copying their spatial or spatio-temporal neighbours.
Even under heavy masking (e.g., 45\% masked), this simple reconstruction strategy still yields a reasonable results for \textit{i.i.d masking}, e.g., we can easily recognize the action `petting cat' from the reconstructed images, especially the one reconstructed from spatio-temporal neighbours. However, this becomes significantly more difficult when using \textit{block masking}.
}
\label{fig:block_vs_iid}
\end{center}
\vspace{-10pt}
\end{figure}



\subsection{Masking Strategy Comparisons} 
\label{sec:append_mask_visualization}
We propose to use block masking (in \sref{sec:mask_task}) since i.i.d. masking  may lead to trivial solutions for the \ourmask task given the strong localities in videos.
We illustrate this point in \fref{fig:block_vs_iid} with a simple copy-paste reconstruction method. 
Specifically, after masking, we first replace the masked tokens with their \textit{nearest visible neighbours} (i.e., the unmasked token that has the shortest distance in spatial or spatio-temporal domain), and then forward the reconstructed tokens to the VQ-VAE decoder to generate the RGB images.
For the default 15\% masking ratio, i.i.d. masking is recoverable while block masking causes striped noisy patterns.\footnote{We highlight the masking region in \fref{fig:block_vs_iid}(b) and show the raw RGB images in other cases.}
We also test with the extreme case of masking 45\% tokens (in \fref{fig:block_vs_iid} (d), (e)).
The block-masked images are hard to reconstruct, however, some objects in reconstructed images from i.i.d. masking are still recognizable.
When comparing images under the same masking strategy, recovered images using spatio-temporal neighbours is better than using only spatial neighbours, especially when comparing the images under 45\% i.i.d. masking (i.e., (d).2 and (e).2 in \fref{fig:block_vs_iid}).
Overall, these results indicate that using i.i.d. masking in \ourmask task has a potential trivial solution by copying the neighbourhood, while block-masking resolves this issue.




\begin{figure}[t]
\vskip 0.1in
\begin{center}
\includegraphics[
                width=0.95\columnwidth,
                ]{figs/reconstruction.pdf}
\caption{
Masked-token model reconstruction for SSV2 and Kinetics-400 datasets. Comparing 1. and 4., our model could redraw temporally-consistent and spatially-plausible patches for the masked regions.
}
\label{fig:reconstruction}
\end{center}
\end{figure}


\subsection{Model Reconstruction}
\label{sec:append_recon_visualization}
Since our model is trained with \ourmask task, it is able to reconstruct masked tokens.
In this section, we showcase the reconstructed video frames by our final model (i.e., 24 layers, 1024 dimensions, 5 clip length, and 256 frame size).
As shown in \fref{fig:reconstruction}, we provide two examples from the SSV2 and Kinetics-400 dataset.
We uniformly sample 5 consecutive frames from the video at 1 frame per second.
We show the original frames in the first rows (\fref{fig:reconstruction}.(a).1, (b).1).
As illustrated in \sref{sec:append_dataset_visualization}, the temporally-heavy SSV2 dataset has object motions between frames while the spatially-heavy Kinetics-400 dataset has almost static frames.
In the second rows, we show the images after VQ-VAE compression.
To do this, we first use VQ-VAE encoder to encode the images, and then use VQ-VAE decoder to reconstruct the images, without any corruptions in between.
We see that there is some information loss caused by the VQ-VAE compression (e.g., the text `comfort' in \fref{fig:reconstruction}.(a).1).  
It potentially contributes to relative lower results on spatially-heavy datasets.
In the third and fourth rows, we illustrate the masked area and the prediction from our model.
As shown in \fref{fig:reconstruction}.(a).4, our model could faithfully redraw the shape and texture of the object.
As shown in \fref{fig:reconstruction}.(b).4, the shape of the cat's head is pretty similar to the original frames while the shading is different (but still consistent in different frames).
