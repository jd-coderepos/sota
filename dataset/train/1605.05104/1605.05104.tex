\documentclass[prodmode,acmtocl]{acmsmall}
\let\orgsetcounter\setcounter 

\usepackage{etex}
\usepackage{makeidx}  
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{palatino}
\usepackage{lmodern}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings, framed}
\usepackage{subfigure}
\usepackage{color}
\usepackage{galois}
\usepackage{amsbsy}
\usepackage{latexsym}
\usepackage{url}
\usepackage{ifthen}
\usepackage{epsfig}
\usepackage{pstricks}
\usepackage{xspace}
\usepackage{listings}
\usepackage[all]{xy}
\usepackage{tikz}
\usepackage{cancel}
\usepackage{fixmath}

\newcommand{\pp}[1]{ ^{#1.}}

\newcommand{\XXX}{\Var}
\newcommand{\MMM}{\mathbb{M}}
\newcommand{\LLL}{\mathbb{L}}
\newcommand{\VV}{\mathsf{V}}
\newcommand{\PP}{\mathsf{P}}
\newcommand{\II}{\mathsf{I}}
\newcommand{\AAA}{\mathsf{A}}
\newcommand{\SSS}{\mathsf{S}}
\newcommand{\XX}{\mathsf{L}}
\newcommand{\ccomp}[1]{\stackrel{\mbox{\scriptsize }}{\rightarrow}}
\newcommand{\ccompa}[1]{\stackrel{\mbox{\scriptsize }}{\rightarrow}}
\newcommand{\wkth}[1]{\mbox{\scriptsize #1}}
\newcommand{\absslcg}{\emph{abstract slicing}}
\newcommand{\absslc}{\emph{abstract slice}}
\newcommand{\stcslcg}{\emph{static slicing}}
\newcommand{\stcslc}{\emph{static slice}}
\newcommand{\absstcslcg}{\emph{abstract static slicing}}
\newcommand{\absstcslc}{\emph{abstract static slice}}
\newcommand{\absdynslcg}{\emph{abstract dynamic slicing}}
\newcommand{\absdynslc}{\emph{abstract dynamic slice}}
\newcommand{\absconslcg}{\emph{abstract conditioned slicing}}
\newcommand{\absconslc}{\emph{abstract conditioned slice}}
\newcommand{\abssstcslcg}{\emph{abstract static simultaneous slicing}}
\newcommand{\abssstcslc}{\emph{abstract static simultaneous slice}}
\newcommand{\dagg}[1]{[\textcolor{red}{#1}]}

\newcommand{\stm}{\mbox{\sl Stm}}
\newcommand{\caX}{\cX}
\newenvironment{nomedef}
{\rm\bfseries}
{}
\newcommand{\defthename}[1]{\begin{nomedef}(#1)\quad\!\!\!\end{nomedef}}

\newcommand{\fxnote}[1]{\marginpar{#1}}

\lstset{
  frame=Ltb,
  framerule=0pt,
  aboveskip=20pt,
  framextopmargin=0pt,
  framexbottommargin=0pt,
  framexleftmargin=0pt,
  framesep=0pt,
  rulesep=0pt,
  stringstyle=\ttfamily,
  showstringspaces = false,
  basicstyle=\small\rm\sffamily,
  keywordstyle=\small\bfseries,
  numbers=left,
  mathescape=true,
  numbersep=-10pt,
  numberstyle=\tiny,
  numberfirstline=true,
  breaklines=true,
  language=Java,
  morekeywords={if,then,else,while,do,return,skip,read} 
}

\lstdefinelanguage{pseudocode}{
  morekeywords={data,do,else,for,to,downto,let,call,foreach,function,procedure,if,return,returns,takes,then,until,while,repeat,read,skip},
  sensitive=false,
  morecomment=[l]{//},
  morestring=[b]'',}
\lstnewenvironment{pseudocode}{
  \lstset{language=pseudocode}
  \lstset{commentstyle=\textit}
  \lstset{frame=tb}
  \lstset{linewidth=120mm}
  \lstset{xleftmargin=3mm}
  \lstset{frameround=ffff}
  \lstset{framextopmargin=1mm}
  \lstset{aboveskip=3mm}
  \lstset{belowskip=4mm}
  \lstset{framexbottommargin=1mm}
  \lstset{framexleftmargin=-5mm}
  \lstset{stringstyle=\underbar}
  \lstset{mathescape=true}
}{}

\def\prog{\ensuremath{P}\xspace}
\def\progq{\ensuremath{Q}\xspace}
\def\progr{\ensuremath{R}\xspace}
\def\progss{\ensuremath{S}\xspace}
\def\progs{\ensuremath{\mathbb{P}}\xspace}

\def\exps{\ensuremath{\mathbb{E}}\xspace}
\def\statements{\ensuremath{\mathbb{S}}\xspace}

\def\state{\ensuremath{\sigma}\xspace}
\def\states{\ensuremath{\Sigma}\xspace}
\def\istates{\ensuremath{\Sigma_\iota}\xspace}

\def\trace{\ensuremath{\tau}\xspace}
\def\traces{\ensuremath{\mathbb{T}}\xspace}
\def\atraces{\ensuremath{\mathbb{T^\rho}}\xspace}

\def\variables{\ensuremath{\mathbb{X}}\xspace}
\def\values{\ensuremath{\mathbb{V}}\xspace}
\def\zvalues{\ensuremath{\mathbb{Z}}\xspace}
\def\rvalues{\ensuremath{\mathbb{R}}\xspace}

\def\memory{\ensuremath{\mu}\xspace}
\def\abmemory{\ensuremath{\mu^\rho}\xspace}
\def\memories{\ensuremath{\mathbb{M}}\xspace}
\def\store{\ensuremath{\varepsilon}\xspace}
\def\astore{\ensuremath{\varepsilon^\rho}\xspace}
\def\stores{\ensuremath{\mathbb{E}}\xspace}
\def\heap{\ensuremath{h}\xspace}
\def\heaps{\ensuremath{\mathbb{H}}\xspace}

\newcommand{\BIND}[2]{#1\!\leftarrow\!#2}

\def\lnums{\ensuremath{\mathbb{L}}\xspace}

\def\crit{\ensuremath{C}\xspace}
\def\crits{\ensuremath{\mathbb{C}}\xspace}

\def\uco{\ensuremath{\rho}\xspace}
\def\ucos{\ensuremath{\mbox{\sl uco}}\xspace}
\def\avalue{\ensuremath{\mbox{{\sc{v}}}}\xspace}
\def\avaluee{\ensuremath{\mbox{{\sc{u}}}}\xspace}
\def\astate{{\sigma^\rho}\xspace}
\newcommand{\astatei}[1]{{\sigma^\rho_{#1}}\xspace}
\def\astates{{\Sigma^\rho}\xspace}
\def\iastates{{\Sigma_\iota^\rho}\xspace}
\def\amemory{\ensuremath{M}\xspace}

\def\KL{\ensuremath{\mathtt{KL}}\xspace}
\def\KLi{\ensuremath{\mathtt{KL}i}\xspace}
\def\IC{\ensuremath{\mathtt{IC}}\xspace}
\def\SIM{\ensuremath{\mathtt{SIM}}\xspace}

\def\ABSMEMop{\upharpoonright^\alpha}
\newcommand{\ABSMEM}[3]{#1 \upharpoonright^\alpha_{#3} #2}
\newcommand{\MEM}[2]{#1 \upharpoonright #2}
\def\Proj{\ensuremath{\mathit{Proj}}\xspace}
\def\UEA{\ensuremath{\mathcal{U}^{\mathcal{A}}}}

\newcommand{\ABSDEQUALSX}[4]{#3\left(#1\right) \neq_{x} #3\left(#2\right)}
\newcommand{\ABSDEQUALSY}[4]{#3\left(#1\right) \neq_{y} #3\left(#2\right)}
\newcommand{\ABSEQUALSPX}[4]{#3\left(#1\right) =_{x} #3\left(#2\right)}
\newcommand{\ABSEQUALSPY}[4]{#3\left(#1\right) =_{y} #3\left(#2\right)}

\def\ASG{\ensuremath{\mathit{ASG}}\xspace}
\def\SA{\ensuremath{\mathit{SA}}\xspace}
\def\EA{\ensuremath{\mathit{EA}}\xspace}
    
\newcommand{\dist}{1cm} 
\newcommand{\dista}{0.6cm} 
\newcommand{\distb}{0.3cm} 
\newcommand{\li}{\ar@{-}} 
\newcommand{\lp}{\ar@{.}} 
\newcommand{\fp}{\ar@{.>}} 
\newcommand{\lab}{0.25cm} 
\newcommand{\lmab}{0.1cm} 

\newcommand{\Eq}{\mathit{Eq}} 
\newcommand{\declassify}{\mbox{declassify}} 
\newcommand{\ie}{i.e.\ } 
\newcommand{\lmap}{\longmapsto}
\newcommand{\coM}{\mbox{co}\cM}
\newcommand{\fIrr}{\mbox{\sl firr}}
\newcommand{\ff}{\dot{f}}
\newcommand{\maxx}{\mbox{\sl max}}
\newcommand{\pbcup}{\ds\dot{\bigcup}}
\newcommand{\pcup}{\dot{\cup}}
\newcommand{\rra}{\rightarrowtail}
\newcommand{\minn}{\mbox{\sl min}}
\newcommand{\Jirr}{\mbox{\sl Jirr}}
\newcommand{\fRid}{\mbox{\sl fRid}}
\newcommand{\const}{\mbox{\emph{const}}}
\def\ltrid{\mbox{\tiny{}}}
\def\ltriu{\mbox{\tiny{}}}
\def\invsc#1{(#1)^\wedge}
\def\invec#1{(#1)^{\ltriu}}
\def\invscd#1{(#1)^\vee}
\def\invecd#1{(#1)^{\ltrid}}

\addtolength{\marginparwidth}{1cm}

\newcounter{todos}
\setcounter{todos}{0}
\newcommand{\todo}[1]{
\addtocounter{todos}{1}
\textbf{*\thetodos*}\marginpar{\fbox{
\textsf{{\small
\begin{minipage}{2cm}
({\tiny \thetodos}):
#1
\end{minipage}
}}}}}

\def\Tp#1#2{\mbox{\raisebox{0ex}[1ex][1ex]{}}}
\def\ccS#1{\mbox{\raisebox{0ex}[1ex][1ex]{}}}
\newcommand{\irule}[2]{\frac{\textstyle\rule[-
1.3ex]{0cm}{3ex}#1}{\textstyle\rule[-.5ex]{0cm}{3ex}#2}}

\def\afrac#1#2{\mbox{}}

\def\post{\mbox{\sl post}}
\def\pre{\mbox{\sl pre}}
\def\veov#1{\stackrel{\curvearrowleft}{#1}\!}
\def\imp{\hbox{}}
\def\lop#1#2#3{\mathrel{\mathop{#1}\limits_{#2}^{#3}}}
\def\bbbp{{\mathbb{P}}} 
\def\bbbI{{{\mathbb{I}}}} 
\def\bbbe{{{\mathbb{E}}}} 
\def\bbbk{{{\mathbb{K}}}} 
\def\bbbz{{\mathbb{Z}}}
\def\bbbn{{\mathbb{N}}}
\def\bbbc{{\mathbb{C}}}
\def\bbbf{{\mathbb{F}}}
\def\bbbX{{\mathbb{X}}}
\newcommand{\nint}{\mathbb{Z}}
\newcommand{\bool}{\mathbb{B}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\raz}{\mathbb{Q}}
\newcommand{\scl}{\mathbb{S}}
\newcommand{\pol}{\mathbb{P}}
\newcommand{\val}{\mathbb{V}}
\newcommand{\typ}{\mathbb{T}}
\newcommand{\tT}{\mathbb{T}}
\newcommand{\dd}{\mathbb{D}}
\newcommand{\ddi}[1]{\mathbb{D}_{\mbox{\tiny }}}
\newcommand{\Rei}[2]{\Re_{\mbox{\tiny ,}}}
\newcommand{\Xx}{{\tt X\:}}
\def\Imp{\mbox{\sc Imp}}
\def\fL{{\mathfrak{L}}}
\def\ef{{\mathcal{F}}}
\def\eg{{\mathcal{G}}}
\def\ed{{\mathcal{D}}}
\newcommand{\all}{\Sigma^{\infty}}
\newcommand{\trans}{\Sigma^{\propto}}
\newcommand*{\disj}[1]   {{\bigcurlyvee_{}^{}\left(#1\right)}}
\newcommand{\NInte}[3]{\scl\mbox{\small }}
\newcommand{\An}{\ \mbox{\sc And}}
\newcommand{\modu}{\ \mbox{\sl mod}\ }
\newcommand{\Mirr}{\mbox{\sl Mirr}}
\newcommand{\Ang}{\mbox{\sl Ang}}
\newcommand{\Dem}{\mbox{\sl Dem}}
\newcommand{\Inf}{\mbox{\sl Inf}}
\newcommand{\lco}{\mbox{\sl lco}}
\newcommand{\id}{\mbox{\sl id}}
\newcommand{\Lab}{\mbox{\sl Lab}}
\newcommand{\Act}{\mbox{\sl Act}}
\newcommand{\Per}{\mbox{\sl Per}}
\newcommand{\run}{\mbox{\sl run}}
\newcommand{\iid}{\mbox{\sl\tiny id}}
\newcommand{\op}{\mbox{\sl op}}
\newcommand{\du}[1]{#1^\delta}
\newcommand{\Var}{\mathbb{X}}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\La}{\Leftarrow}
\newcommand{\Lra}{\Leftrightarrow}
\newcommand{\lra}{\longrightarrow}
\newcommand{\pref}{\preccurlyeq}
\newcommand{\ov}{\overline}
\newcommand{\ovr}{\overrightarrow}
\newcommand{\aden}{\blacktriangleright}
\newcommand{\den}{\cD}
\newcommand{\ds}{\displaystyle}
\newcommand{\taug}[1]{\tau^{\dot{#1}}}
\newcommand{\filt}[2]{{\uparrow\!}_{#1}\left(#2\right)}
\newcommand{\filter}[1]{{\uparrow\!}\left(#1\right)}
\newcommand{\filtc}[2]{{\Uparrow\!}_{#1}(#2)}
\newcommand{\undist}[2]{\Upsilon_{\mbox{\tiny }}^{\mbox{\tiny }}}
\newcommand{\Secr}[2]{\mbox{\sl Secr}_{\mbox{\tiny }}^{#2}}
\def\defi{\mbox{\raisebox{0ex}[1ex][1ex]{}}}

\def\gras#1#2{\mbox{\raisebox{0ex}[1ex][1ex]{}}}

\def\dpower#1#2{\mbox{\raisebox{0ex}[1ex][1ex]{}}}

\newcommand{\ud}{\mbox{\raisebox{0ex}[1ex][1ex]{}}}

\def\fun#1{\mbox{\raisebox{0ex}[1ex][1ex]{}}}
\def\wfun#1{\mbox{\raisebox{0ex}[1ex][1ex]{}}}
  
\def\ok#1{\mbox{\raisebox{0ex}[1ex][1ex]{}}}

\def\smallromani{\renewcommand{\theenumi}{\roman{enumi}}
        \renewcommand{\labelenumi}{(\theenumi)}}

\newcommand{\COMMENT} [1]{}
\newcommand{\cci}[2]{\cS_{\mbox{\tiny }}^{\mbox{\tiny }}}

\def\lfp{\mbox{\sl lfp\/}}
\def\gfp{\mbox{\sl gfp\/}}
\def\Con{\mbox{\sl Con\/}}
\def\etal{{\it et al.\ }}
\def\comp{\mathrel{\hbox{\footnotesize\normalsize}}}
\def\equivdef{
\mathrel{\mathop{\equiv}\limits^{\mbox{\tiny def}}}}

\def\rarr#1{\mbox{\raisebox{0ex}[1ex][1ex]{}}} 

\def\grasse#1#2{[\![#1]\!]^{#2}}
\def\grasstr#1{\langle\!|#1|\!\rangle}
\def\grass#1{\llbracket#1\rrbracket}
\def\grassal#1{\{\!|#1|\!\}}
\def\grassd#1{\llbracket#1\rrbracket}
\def\alphast{\alpha^{\mbox{\small st}}}

\def\defemb#1#2{\expandafter\def\csname #1\endcsname
                              {\relax\ifmmode #2\else\hbox{}\fi}}
 
\def\2c-math#1#2{{\par\medskip\noindent 
                      \par\smallskip
                        \noindent\hspace*{\fill} }
                           \\frac{\mbox{\strut}}
{\mbox{\strut}}\frac{{\mbox{\strut}}}
{\mbox{\strut}}8mm]
      Program \prog & Program \progq & Program \progr &Program \progss
    \end{tabular}
  \end{center}
  \caption{,  and  are, respectively, a
    \emph{slice}, a \emph{semantic slice} and an \emph{abstract
      slice} of .\label{fig:ProgP33}}
\end{figure}

\paragraph{Contributions}

In this paper, we aim at introducing a generalized notion of slicing,
allowing us to weaken the notion of "dependency" (from syntax, to
semantics, to abstract semantics) with respect to what is considered
\emph{relevant} for computing the slice.  Since our generalization is
a semantic one, we start from the unifying framework proposed in
\cite{AForm,TheoFoun}, where different forms of slicing are defined
and compared w.r.t.~their characteristics (static/dynamic,
iteration-count/non-iteration-count, etc.), into a comprehensive
formal framework.  The structure of this framework is based on the
formal definition of the criterion, inducing a semantic equivalence
relation  which uniquely characterizes the set of possible slices
of a program  as the set of all the sub-programs\footnote{The
  framework proposed in \cite{AForm,TheoFoun} is parametric on the
  syntactic relation, but here we only consider the relation of being
  a subprogram.} equivalent to  w.r.t. .  This structure
makes the framework suitable for the introduction and the formal
definition of an abstract form of slicing, since abstraction
corresponds simply to consider a weaker criterion, which implies
weakening the equivalence relation  defining slicing.

Once we have the equivalence relation defining a desired notion of
slicing w.r.t. a given criterion, we show how this corresponds to
fixing the notion of dependency we are interested in (namely, the
notion of dependency determining what has to be considered relevant in
the construction of slicing), and we show how the extension to
semantic dependencies may be used to extend the program dependency
graph-based approach to computing slices \cite{horPR89}.  Finally, we
define a notion of abstract dependencies implying abstract criteria.

We show that this new notion of dependency is not suitable for
computing slices by using Program Dependency Graphs, and propose
algorithms for computing (abstract) dependencies and a systematic
approach to compute backward slices.  Such an approach relies on two
systems of logical rules in order to prove (1) Hoare-style tuples
capturing the effect of executing a statement  on a pair of states
for which some similarity (\emph{agreement}) is required by the
slicing criterion (indeed, this similarity corresponds to the semantic
equivalent relation); and (2) when some properties of the state do not
change (are \emph{preserved}) after  is executed.  The combination
of the results provided by these rule systems allows to decide whether
it is safe to remove a statement from a program without changing the
observation corresponding to the criterion.

Importantly, the rule systems and algorithms provided in Section
\ref{sec:theQuestForAbstractSlices} rely on the knowledge and
manipulation of a ``library'' of abstract properties.  For example, in
order to infer that \CODE{2*x} is always even, the abstract domain
representing the parity of number must be known.  If no abstract
property is known except the identity (which is the most precise
property, and is not really abstract), then the approach boils down to
standard slicing.  Importantly, it becomes clear in this case that
slices on the same variables (properties of them in the abstract case;
exact values in the concrete case) are generally bigger in the
concrete setting (when identity is the only available property) with
respect to the corresponding abstract slicing.  Needless to say, this
does not mean that every algorithm for abstract slicing will perform
better than any algorithm for non-abstract slicing; rather, it
provides a practical insight of how optimal (purely semantic-based)
abstract slices may not include statements which are included in
concrete slices.

Part of this work has been previously published in conference
proceedings \cite{MastroeniZanardini,Zanardini,MastroeniNicolic}.  The
present paper joins these works into a coherent framework, and
contains a number of novel contributions
\begin{itemize}
\item[] We formally prove that abstract slicing in the formal
  framework of \cite{AForm} generalizes concrete forms of slicing.
\item[] We formally define the notion of dependency induced
  by a particular criterion, i.e., by the equivalence relation among
  programs induced, in the formal framework, by the chosen criterion.
\item[] We define and prove how we can approximate this
  (concrete semantic) dependency in order to use it for pruning PDGs
  and computing slicing with the well known PDG-based algorithm for
  slicing \cite{RY88}.
\item[] We discuss why the idea of pruning PDGs is not
  applicable to the abstract notion of dependency motivating the need
  of providing different approaches for computing abstract slices.
\item[] The treatment of non-numerical values when computing
  slices was already considered in \cite{Zanardini}.  However, the
  language under study in the present paper is different in that it is
  closer to standard object-oriented languages.  More concretely, that
  work used complex identifiers  as if they were normal
  variables, thus obtaining that sharing between variables was easier
  to deal with.  However, this came at the cost of increasing the
  number of ``variables'' to be tracked by the analysis.  Moreover,
  examples have been provided to illustrate how properties of the heap
  can be taken into account.
\item[] The \GSYSTEM introduced here is a quite refined
  version of the -system \cite{Zanardini}; rules for
  variable assignment and field update have been changed according to
  the new language (which implies a number of technical issues); there
  is a new rule ; the overall discussion has been
  improved.
\item[] The rule system for proving the preservation of
  properties (the \PSYSTEM) is explicitly introduced here.
\item[] The description of how statements can be erased has
  been improved; an algorithm has been explicitly introduced, which
  labels each program point with agreements according to the \GSYSTEM.
  A thorough discussion and proofs are provided, so that it is
  guaranteed that the conditions for erasing a statement (relying on
  the \GSYSTEM, the \PSYSTEM, and the \CODE{labelSequence} procedure
  for labeling program points with agreements) are sound.
\item[] Recent work on field-sensitive sharing analysis
  \cite{ZanardiniG15sh} is included in the computation of abstract
  slices, which results in improving the precision when data structure
  in the heap overlap.
\end{itemize}



\section{Preliminaries}
\label{sec:preliminaries}
\subsection{The programming language}
\label{sec:theProgrammingLanguage}

The language is a simple imperative language with basic
object-oriented features, whose syntax will be easy to understand for
anyone who is familiar with imperative programming and object
orientation.  The language syntax includes the usual arithmetic
expressions  and access to object fields via ``dot''
selectors.  A statement can be \CODE{skip}, a variable assignment
\CODE{x:=e}, a field update \CODE{x.f:=e}, a conditional or a
\CODE{while} loop.  In addition, there exist special statements (1)
\CODE{read} which reads the value of some variable from the input,
simulating the use of parameters; this kind of statement can only
appear at the beginning of the program; and (2) \CODE{write}, which
can only appear at the end of the program and outputs the current
value of some variables\footnote{As a matter of fact, this kind of
  statement is only included in the language for back-compatibility
  and readability.}.  For simplicity, guards in conditionals and loops
are supposed not to have \emph{side effects}.  We denote by 
the set of all programs.

 is the set of program variables and  denotes the
set of values, which can be either integer or reference values, or the
 constant ();
every variable is supposed to be well-typed (as integer or reference)
at every program point.   denotes the set of \emph{line
  numbers} (program points).  Let , and  be the
statement at program line .  For a given program , we denote
by  the set of all and only the line
numbers corresponding to statements of the program , i.e.,
.  This definition is
necessary since when we look for slicing we erase statements without
changing the numeration of line numbers; for instance, in
Figure~\ref{fig:ProgP33}, we have that , so that
.

A \emph{program state}  is a pair
 where  is the executed program point,  is
the number of times the statement at  has been reached so far,
 is the memory.
A \emph{memory} is a pair  where the \emph{store}
 maps variables to values, and the \emph{heap}
 is a sequence of locations where objects can be stored; a
reference value corresponds to one of such locations.  An
\emph{object}  maps field identifiers to values, in the usual way;
 is the value corresponding to the field  of the object ,
and can be either a number, the location in which another object is
stored, or .  For the sake of simplicity, \emph{classes} are
supposed to be declared somewhere, and field accesses are supposed to
be consistent with class declarations.

Unless ambiguity may arise, a memory (or even an entire program state)
can be represented directly as a store, so that  (resp.,
) will be the value of  in the store contained in
 (resp., in ).  Moreover, a store  can be
represented as , meaning
that  for every , and, again,  (resp., ) can be used instead of
 whenever the
store is the only relevant part of the memory (resp., the state).

A \emph{state trajectory}  is a sequence
of program states through which a program goes during the execution.
State trajectories are actually traces equipped with the
 component.
The state trajectory obtained by executing program  from the
input memory  is denoted .  Moreover,
 will be the set of states in  where the program
point is . Any initial state has , i.e., the set of initial
states is
.

In the following,
 denotes
the program semantics where  returns the set of
state trajectories obtained by executing the program  starting
from any initial state in , i.e.,
.  We abuse notation by denoting in the same way also the
semantics of expressions, namely,
, which is such that
 () returns the evaluation
of  in .  Finally, if , in sake of
simplicity, we still abuse notation by denoting in the same way also
the additive lift of semantics, i.e.,
.

\subsection{Basic Abstract Interpretation}
\label{sec:basicAbstractInterpretation}

This section introduces the lattice of \emph{abstract
  interpretations}~\cite{CC77}.  Let  denote a complete lattice , with ordering
, lub , glb , top and bottom element  and
, respectively.  A \emph{Galois connection} (G.c.) is a pair of
monotone functions  and  such that .  In standard terminology,  and  are, respectively,
the concrete and the abstract domain.  Abstract domains can be
formulated as upper closure operators () \cite{CC77}.  Given an
ordered set  with ordering , a uco on , , is a monotone, idempotent ()
and extensive () map.  Each uco
 is uniquely determined by the set of its fixpoints, which is
its image; i.e., .  When
 for some set , and  then we usually write
 instead of  (and in general for any function,
 instead of ).  If  is a complete lattice, then
 is a complete lattice, where  is the
domain of all the upper closure operators on the lattice ; for
every two ucos ,  if and only if  iff
; and, for every ,  and .  In the following we will denote by
 the most concrete uco on a domain, i.e., , and
by  the most abstract one .  is more
precise than  (i.e.,  is an abstraction of ) iff  in .  The \emph{reduced product} of a
family  is  and is one of
the best-known operations for composing abstract domains.

\begin{example}[Numerical abstract domains]
  Let the concrete domain  be : the \emph{parity}
  abstract domain  in Figure~\ref{dom1} (on the left)
  represents the parity of numbers, and is determined by fix-points  where  and  denote even and
  odd numbers, respectively;  is the empty set, and .  For example,  (all numbers
  are even),  (both numbers are odd), and
   (there are both even and odd numbers).
  The \emph{sign} abstract domain  in Figure~\ref{dom1} (on
  the right) is characterized by fix-points  and tracks the sign of integers (zero, positive,
  negative, etc.).  For example, ,
  , ,
  .  Finally, the \emph{parity-sign} domain
  , which is the reduced product  of 
  and , captures both properties (the parity and the sign),
  and has fix-points , , , ,
  , , , , , , and .
  \begin{figure}[h]
    \begin{center}
      \begin{tikzpicture}
        \node (partop) at (0.2,0) {};
        \node (pareven) at (-0.6,-1.5) {};
        \node (parodd) at (1,-1.5) {};
        \node (parbot) at (0.2,-3) {};
        \draw (partop) -- (pareven);
        \draw (partop) -- (parodd);
        \draw (pareven) -- (parbot);
        \draw (parodd) -- (parbot);
        
        \node (signtop) at (3,0) {};
        \node (signpos) at (2,-1.5) {};
        \node (signneg) at (4,-1.5) {};
        \node (signzero) at (3,-1.5) {};
        \node (signbot) at (3,-3) {};
        \draw (signtop) -- (signpos);
        \draw (signtop) -- (signneg);
        \draw (signtop) -- (signzero);
        \draw (signpos) -- (signbot);
        \draw (signneg) -- (signbot);
        \draw (signzero) -- (signbot);

        \node (parsigntop) at (7.7,0) {};
        \node (parsignpos) at (6.2,-1) {};
        \node (parsigneven) at (7.2,-1) {};
        \node (parsignodd) at (8.2,-1) {};
        \node (parsignneg) at (9.2,-1) {};
        \node (parsignposeven) at (5.4,-2) {};
        \node (parsignposodd) at (6.7,-2.5) {};
        \node (parsignnegeven) at (8.7,-2.5) {};
        \node (parsignnegodd) at (10,-2) {};
        \node (parsignzero) at (7.7,-3) {};
        \node (parsignbot) at (7.7,-4) {};
        \draw (parsigntop) -- (parsignpos);
        \draw (parsigntop) -- (parsignneg);
        \draw (parsigntop) -- (parsigneven);
        \draw (parsigntop) -- (parsignodd);
        \draw (parsignpos) -- (parsignposeven);
        \draw (parsignpos) -- (parsignposodd);
        \draw (parsignneg) -- (parsignnegeven);
        \draw (parsignneg) -- (parsignnegodd);
        \draw (parsigneven) -- (parsignposeven);
        \draw (parsigneven) -- (parsignnegeven);
        \draw (parsigneven) -- (parsignzero);
        \draw (parsignodd) -- (parsignposodd);
        \draw (parsignodd) -- (parsignnegodd);
        \draw (parsignposeven) -- (6,-3.2) -- (parsignbot);
        \draw (parsignnegeven) -- (parsignbot);
        \draw (parsignposodd) -- (parsignbot);
        \draw (parsignnegodd) -- (9.4,-3.2) -- (parsignbot);
        \draw (parsignzero) -- (parsignbot);
      \end{tikzpicture}
    \end{center}
    \caption{The ,  and 
      domains.} \label{dom1}
  \end{figure} 
\end{example}

Formally speaking, the value of a reference variable is either a
location  or .  However, the domains introduced in the
next example classify variables not only with respect to 
itself, but also on the data structure in the heap which is reachable
from .  This point of view is similar to previous work on static
analysis of properties of the heap like \emph{sharing}
\cite{DBLP:conf/sas/SecciS05} or \emph{cyclicity}
\cite{DBLP:conf/vmcai/RossignoliS06,tcs13}.

\begin{example}[Reference abstract domains]
  \label{ex:referenceAbstractDomains}
  Let  be , i.e., the possible values
  of reference variables.  The \emph{nullity} domain 
  classifies values on nullity, and has fix-points  where the concretizations of  and
   are, respectively,  and .
  
  On the other hand, it is possible to define a \emph{cyclicity}
  domain  which classifies variables on whether they point
  to \emph{cyclic} or \emph{acyclic} data structures \cite{tcs13}.  A
  \emph{cycle} in the heap is a path in which the same location is
  reached more than once; a double-linked list (one which can be
  traversed in both directions) is a good example of a cyclic data
  structure.  The fix-points of this domain are , where all acyclic values (including ) are
  abstracted to , and all cyclic values (i.e., locations
  from which a cycle is reachable) are abstracted to .  Both
  domains and their reduced product are depicted in Figure \ref{dom2};
  note that there are values which are both null and cyclic, so that
  their intersection collapses to .
  
  Finally, the identity domain , abstracts two concrete values
  to the same abstract value only if they are equal.  Two references
  are \emph{equal} if (1) their are both null; or (2) they are both
  non-null and the objects stored in the corresponding locations are
  equal, where equality on objects means that all their numeric fields
  must be the same number and all reference fields must be equal
  (w.r.t.~this same notion of equality on references).

  \begin{figure}[h]
    \begin{center}
      \begin{tikzpicture}
        \node (nulltop) at (0.2,0) {};
        \node (nullnull) at (-0.6,-1.5) {};
        \node (nullnonnull) at (1,-1.5) {};
        \node (nullbot) at (0.2,-3) {};
        \draw (nulltop) -- (nullnull);
        \draw (nulltop) -- (nullnonnull);
        \draw (nullnull) -- (nullbot);
        \draw (nullnonnull) -- (nullbot);
        
        \node (cycletop) at (4,0) {};
        \node (cyclecyclic) at (3,-1.5) {};
        \node (cycleacyclic) at (5,-1.5) {};
        \node (cyclebot) at (4,-3) {};
        \draw (cycletop) -- (cyclecyclic);
        \draw (cycletop) -- (cycleacyclic);
        \draw (cyclecyclic) -- (cyclebot);
        \draw (cycleacyclic) -- (cyclebot);

        \node (top) at (9.7,0) {};
        \node (cyclic) at (7.7,-1.2)
              {};
        \node (null) at (9.7,-1.7)
              {};
        \node (acyclic) at (11.7,-1.2)
              {};
        \node (bot) at (9.7,-3) {};
        \draw (top) -- (null);
        \draw (top) -- (cyclic);
        \draw (top) -- (acyclic);
        \draw (null) -- (bot);
        \draw (cyclic) -- (bot);
        \draw (acyclic) -- (bot);
      \end{tikzpicture}
    \end{center}
    \caption{The  and the  domains, and their
      reduced product.} \label{dom2}
  \end{figure} 
\end{example}

Let us consider now  ( lattice and ), namely
 is a -tuple of elements of , and consider
.  In this case, we can distinguish between two kinds
of abstractions:\label{rel-nonrel} \emph{non-relational} and
\emph{relational} abstractions \cite{C01-Dag,CC79}.  The
non-relational or \emph{attribute-independent} one \cite[Example
  6.2.0.2]{CC79} consists in ignoring the possible relationships
between the values of the abstracted inputs.  For instance, if 
is applied to the values of variables  and , then  can be
approximated through projection by a pair of abstractions on the
single variables, analyzing the single variables in isolation.  In
sake of simplicity, without losing generality, consider , i.e.,
.  Formally,  is
non-relational if there exist  such that
, i.e,
. For instance,
let  be the abstract domain depicted in Figure~\ref{dom1}
expressing the parity of integer values; the  non-relational
property of  provides the parity of  and 
independently one from each other, meaning that all the possible
combinations of parity of  and  are possible as results
(
and all combinations where at least one variable is  or ).
Relational abstractions may preserve some of the relationship between
the analyzed values \cite{C01-Dag}. For instance, we could define an
abstraction preserving the fact that  is even () if and only
if  is odd (). It is clear that, in this case, we are more
precise since the only possible analysis results are
, ,  and
.

If , , and , then  is a {\em sound\/}
approximation of  if .   is
known as the {\em best correct approximation\/} (bca) of  in
, which is always sound by construction.  Soundness naturally
implies fix-point soundness, that is,
CCCC. If  then we say that  is a {\em complete\/}
approximation of  \cite{CC79,GRSjacm}.  In this case,
CCCC.

\subsection{Equivalence relations, abstractions and partitions}
\label{Sect:partit}

Closure operators and equivalence relations are related concepts
\cite{CC79}.  Recently, this connection has been further studied in
the field of abstract model checking and language based-security
\cite{RT02,HM05}.  In particular, there exists an isomorphism between
equivalence relations and a subclass of upper closure
operators. Consider a set : for each equivalence relation
 we can define an upper closure operator,
 such that  and .  Conversely, for each
upper closure operator , we are able to define
an equivalence relation  such that
.  It is immediate to prove that
 is an equivalence relation, and this comes from 
being merely a function, not necessarily a closure operator.
 is identified as the most concrete closure  such
that  \cite{HM05}.
It is possible to associate with each upper closure operator the most
concrete closure inducing the same partition on the concrete domain
:


\noindent
Note that, for all ,  is the
(unique) most concrete closure that induces the same equivalence
relation as  ().  The fix-points
of  are called the \emph{partitioning} closures.  Being 
a complete Boolean lattice, an upper closure operator
 is partitioning, i.e., , iff it
is complemented, namely if  \cite{HM05}.
  
\begin{figure}[t]  
  \begin{center}  
    \includegraphics[scale=.25]{partizioni1.jpg}  
    \caption{A partitioning closure.}\label{partizioni}  
  \end{center}  
\end{figure}

\begin{example}  
  Consider the set  and one of its possible partitions
  .  The closure  with fix-points
   induces exactly  as a
  state partition, but the most \emph{concrete} closure that induces
   is
  ,
  which is the closure on the right of Figure \ref{partizioni}.
\end{example}

Given a partitioning upper closure operator , an \emph{atom} is
an element  of  such that there does not exists another
element  with .  For example, the
atoms of  are , , , ,
and .  In partitioning closures, atoms are all the possible
abstractions of singletons: in fact,  will never
give  or  since there is always a more precise abstract
value describing .  In the following,  holds iff
 is an atom of .

\subsection{Abstract semantics}
\label{sec:abstractSemantics}

An abstract program semantics is the abstract counterpart of the
concrete semantics w.r.t.\ an abstract program observation: it is
meant to compute, for each program point, an abstract state which
soundly represents \emph{invariant properties} of variables at that
point.  In general, it is computed by an abstract interpreter
\cite{CC79} collecting the set of all the possible values that each
variable may have in each program point and abstracting this set in
the chosen abstract domain.

Given a concrete program state  and an abstract domain
, an \emph{abstract state}
 is obtained by applying the abstraction  to
the values of variables stored in it.
Namely, , where
 and  is such that
.  For simplicity, we can write
, treating the whole state as a store when
applied to variables.  In the case of a reference variable , the
abstraction  gives information about the data structure
pointed to by  (e.g., if , the cyclicity of the
data structure can be represented).  This explains why the heap is not
represented explicitly in the abstract state: instead, relevant
information about the heap is contained in the abstraction of
variables (see the previous discussion before Example
\ref{ex:referenceAbstractDomains}).

In the following, ordering  on abstract states is variable-wise
comparison between abstract values:
 The greater an
abstract state is, the wider is the set of concrete states it
represents.  Moreover, a \emph{covering} of  is a set of
abstract states  such that .  The set of abstract state trajectories is
, namely an abstract trajectory is the
computation of a program on the set of abstract states. The trace in
 of a program , starting from the abstract memory
 is denoted by .

Formally, the \emph{abstract program semantics}
 is such that
 is the set of the sequences of abstract states computed
starting from the abstract initial states in .  We
also abuse notation by denoting  also the abstract
evaluation of expressions. Namely,
 is such
that . This
definition is correct, since by construction, we have that any
abstract state  corresponds to a set of concrete states,
i.e.,
,
namely, it is the set of all the concrete states having as abstraction
in  precisely , and we abuse notation by denoting with
 also its additive lift.  In other words,
 is the best correct approximation of
 by means of an abstract value in .
In general, in order to compute the abstract semantics of a program on
an abstract domain , we have to equip the domain  with the
abstract versions of all the operators used for defining expressions.
In our language, we should define, for example, the meaning of ,
,  and  on abstract values, i.e., on sets of concrete values.
This is standard in abstract interpretation, and these operations are
defined for all the known numerical abstract domains.  For instance,
the sound approximation of the sum operation on  is the
following:

We can reason similarly for all the other operators.  The use of
 in Section
\ref{section:algorithmicIdeasForCheckingNdep} and later in the paper
is twofold: (1) to infer invariant properties, as in Example
\ref{ex:invariantProperties}; and (2) to evaluate expressions at the
abstract level.

\begin{example}
  \label{ex:invariantProperties}
  Consider the following code fragment:
  \begin{lstlisting}
   i := 10;
   j := 0;
   while (i0) {
     i := i-1;
     j := j+1;
   }
  \end{lstlisting}
    and an abstraction ,
  i.e., the property of interest is the sign of both \ii and \jj.  By
  computing the abstract semantics of this simple program, we can
  observe that inside the loop we lose the sign of \ii since \ii
  starts being positive, but then the  operation makes
  impossible to know statically the sign of \ii (the result may
  be positive, zero or negative starting from \ii positive or zero),
  while we have that \jj always remains positive. Moreover, if the
  loop terminates we can surely say that, at the end, , namely it is negative
  (due to the negation of the while guard). Hence, we are able to
  infer that \ii is negative and \jj is positive after line 6.  This
  means that the final abstract state  is such that
   and  (in
  the following, the extensional notation for  will be , similar to the
  notation for concrete states).
\end{example}



\section{Program Slicing}
Program slicing \cite{weiser} is a program-manipulation technique
which extracts from programs those statements which are relevant to a
particular portion of a computation.  In order to answer the question
about which are the relevant statements, an observer needs
a \emph{window} through which only a part of the computation can be
seen \cite{BinGalla96}.  Usually, what identifies the portion of
interest in the computation is the value of some set of variables at a
certain program point, so that a \emph{program slice} comes to be the
subset (syntactically, in terms of statements) of the original program
which contributes directly or indirectly to the values assumed by some
set of variables at the program point of interest.  The \emph{slicing
criterion} is what specifies the part of the computation which is
relevant to the analysis; in this case, a criterion is a pair
consisting of a set  of variables and a program point (or line
number) .  The following definition \cite{BinGalla96} is a possible
formalization the original idea of program slicing \cite{weiser}, in
the case of a single variable:
\begin{mydefinition}\cite{BinGalla96}
  \label{defSlice}
  For a statement  (at program point ) and a variable , the
  slice  of the program  with respect to the slicing criterion
   is any executable program with the following
  properties:
  \begin{enumerate}
  \item  can be obtained by deleting zero or more statements from
    ;
  \item If  halts on the input , then, each time  is reached
    in , it is also reached in , and the value of  at  is
    the same in  and in .  If  fails to terminate, then 
    may be reached more times in  than in , but  and 
    have the same value for  each time  is executed by .
  \end{enumerate}
\end{mydefinition}

It is worth noting that Reps and Yang \cite{RY88}, in
their \emph{slicing theorem}, provide implicitly a similar definition
of program slicing, but it only considers terminating
computations. The following example provides the intuition of how
slicing works.

\begin{figure}
  \begin{lstlisting}
   int c, nl := 0, nw := 0, nc := 0;
   int in := false;
   while ((c=getchar())!=EOF) {
     nc := nc+1;
     if (c=' ' || c='\n' || c='\t') {
       in := false; }
     elseif (in = false) { 
       in := true;
       nw := nw+1; }
     if (c = '\n') {
       nl := nl+1; }
   }
  \end{lstlisting}
  \caption{Word-count program.}\label{programs}
\end{figure}
  
\begin{figure}
  \hspace{-3mm}  \begin{tabular}{c|c}
    \begin{lstlisting}
   int c, nl := 0;
   
   while ((c=getchar())!=EOF) {
     
     if (c = '\n') {
       nl := nl+1; }
   }
      \end{lstlisting}
      &
      \begin{lstlisting}
   int c, nw := 0;
   int in := false;
   while ((c=getchar())!=EOF) {
     
     if (c=' ' || c='\n' || c='\t') {
       in := false; }
     elseif (in = false) {
       in := true;
       nw := nw+1; }
     
   }
      \end{lstlisting}
  \end{tabular}
  \caption{Slices of the word-count program.}\label{example}
\end{figure}

\begin{example}
  Consider the word-count program \cite{MDT07} given in
  Figure \ref{programs}.  It takes in a block of text and outputs the
  number of lines (\CODE{nl}), words (\CODE{nw}) and characters
  (\CODE{nc}).  Suppose the slicing criterion only cares for the value
  of \CODE{nl} at the end of the program; then a possible slice is on
  the left in Figure~\ref{example}.  On the other hand, if the
  criterion is only interested in \CODE{nw}, then a correct slice is
  on the right.
\end{example}

Starting from the original definition \cite{weiser}, the notion of
slicing has gone through several generalizations and versions, but one
feature is constantly present: the fact that slicing is based on a
notion of \emph{semantic equivalence} that has to hold between a
program and its slices or on a corresponding notion of
\emph{dependency}, determining what we keep in the slice while
preserving the equivalence relation.  What we can observe about
definitions of slicing such as the one given in
Definition~\ref{defSlice} is that they are enough precise for finding
algorithms for soundly computing slicing, such as \cite{RY88}, but not
enough formal to become suitable to generalizations allowing us to
compare different forms of slicing and/or to define new weaker forms
of slicing.
  
In the following, we use the formal framework proposed in \cite{AForm}
where several notions and forms of slicing are modeled and compared.
This is not the only attempt to provide a formal framework for slicing
(see Section~\ref{sec:relatedWork}), but we believe that, due to its
semantic-based approach, it is suitable to include an abstraction
level to slicing, which can be easily compared with all the other
forms of slicing included in the original framework. Hence, in the
following section we don't rewrite a formal framework, but we
re-formalize the slicing criterion in order to allow us to easily
include abstraction simply as a new parameter. A brief introduction of
the formal framework together with some examples showing the
differences between the different forms of slicing introduced in the
following is given in the Appendix.



\subsection{Defining Program Slicing: the formal framework}
\label{section:Background}
In this section, our aim is to define the form of slicing that we can
lift to an abstract level. Namely, we consider the framework in
\cite{AForm,TheoFoun}, which allows us to define abstract slicing
simply by defining an abstract criterion which, independently from the
kind of slicing (static, dynamic, conditional, standard, etc.) allows
us to observe properties instead of concrete values. Since our aim is
to define abstract program slicing as a form of slicing, perfectly
integrated in the proposed hierarchy and where the criterion simply
has one more parameter describing the abstraction, we need to slightly
revise the construction in order to provide a completely unified
notation for the slicing criterion.
Note that, the present paper will only deal with \emph{backward}
slicing, where the interest is on the part of the program which
\emph{affects} the observation associated with the slicing criterion
and not on the part of the program which \emph{is affected} by such an
observation (called instead \emph{forward} slicing \cite{Tip95}).

\subsubsection*{Defining slicing criteria}

The slicing criterion characterizes what we have to observe of the
program in order to decide whether a program is a slice or not of
another program.  In particular, we have to fix which computations
have to be compared, i.e., the inputs and the observations on which
the slice and the program have to agree.

In the seminal Weiser approach, given a set of variables of interest
 and program statement , here referred by the program point
 where  is placed, a slicing criterion was modeled as .  In the following, we will gradually enrich and generalize
this model in order to include several different notions and forms of
slicing.  Weiser's approach is known as \emph{static slicing} since
the equivalence between the original program and the slice has,
implicitly, to hold for every possible input.  On the other hand,
Korel and Laski proposed a new technique called \emph{dynamic slicing}
\cite{KorelLaski} which only considers one particular computation, and
therefore one particular input, so that the dynamic slice only
preserves the (subset of the) meaning of the original program for that
input.
Hence, in order to characterize a slicing criterion including also
dynamic slicing we have to add a parameter describing the set of
initial memories : The criterion is now
, where  for static slicing,
while , with , for dynamic
slicing.  Finally, Canfora \etal proposed \emph{conditioned slicing}
\cite{Conditioned}, which requires that a conditioned slice preserves
the meaning of the original program for a set of inputs satisfying one
particular condition .  Let

be the set of input memories satisfying  \cite{AForm}.
Hence, the slicing criterion still can be modeled as
.

Each type of slicing comes in four forms which differ on what the
program and the slices must agree on, namely on the observable
semantics that has to agree.  In the following, we provide an informal
definition of these forms in order to provide the intuition of what
will be formally defined afterwards:
\begin{description}
\item[Standard] It considers one point in a program with respect to a
  set of variables. In other words, the standard form of slicing only
  tracks one program point.  Semantically, this form of slicing
  consists in comparing the program and the slices in terms of the
  ({\em denotational}) I/O semantics from the program inputs selected
  by the criterion. Namely, for each selected input, the results of
  the criterion variables in the point of observation must be the
  same, independently from the executed statements.
\item[Korel and Laski ()] It is a stronger form where the program
  and the slice must follow identical paths
  \cite{KorelLaski}. Semantically, we could say that the program and
  the slice must have the same ({\em operational}) trace semantics
  w.r.t.\ the statements kept in the slice, starting from the program
  inputs selected by the criterion.  In other words, as before, the
  final value must be the same, but in this case these values must be
  obtained by executing precisely the same statements, i.e., following
  the same execution path.
\item[Iteration count ()] When considering the trace semantics,
  the same program point inside a loop may be visited more than once,
  in the following we call {\em -th iteration} of a program point
   the -th time the program point  is visited. The iteration
  count form of slicing requires that a program and its slice agree
  only at a particular -th iteration of a program point of
  interest.  In this way, when a point of interest is inside a loop,
  we have the possibility to require that the variables must agree
  only at some iterations of the loop and not always.
\item[Korel and Laski iteration count ()] It is the combination
  of the last two forms.
\end{description}
In order to deal with these different forms of slicing, the slicing
criterion must be enriched with additional information.  In
particular, the  form of slicing does not change where to observe
variables, but it does change the observed semantics up to that point.
Hence, we simply have to add a boolean parameter : \true~means
that we are considering a  form and we require that the slice
must agree with the program on the execution of statements that are in
the slice (and obviously also in the original program); on the other
hand, \false~indicates a standard, non- form of slicing.  Hence,
a criterion  comes to be .

The  form, instead, affects the observation: in order to embed
this features in the criterion, the third parameter has to be changed.
Let  be the iterations of the
program point  we are interested in; then, instead of
, in the third parameter of the criterion we should have
.  Therefore,  takes the form
, where .
Note that  represents the fact that we are
interested in all occurrences of , as it happens in the standard
form.

There are also some \emph{simultaneous} () forms of slicing that
consider more than one program point of interest.  In order to deal
with  forms of slicing, we simply extend the definition of a
slicing criterion by considering  as a set instead of a
singleton, namely, .

In the Appendix there are some simple examples showing the main
differences between the several forms of slicing introduced so far.


\section{Abstract Program Slicing}
\label{section:AbstractProgramSlicing}
In this section we define a weaker notion of slicing based on Abstract
Interpretation.  In particular, we generalize the formal framework in
\cite{AForm} in order to include also the abstract versions of
slicing.

Program slicing is used for reducing the size of programs to analyze.
Nevertheless, sometimes this reduction is not sufficient for really
improving an analysis.  Suppose that some variables at some point of
execution do not have a desired property (for example, that they are
different from , or from ); in order to understand where the
error occurred, it would be useful to find those statements which
affect such a property of these variables.  Standard slicing may
return too many statements, making it hard for the programmer to
realize which one caused the error.

\begin{example}
  \label{ex:abstractSlicing}
  Consider the following program , that inserts a new element
  \CODE{elem} at position \CODE{pos} in a single-linked list.  For
  simplicity, let \CODE{pos} never exceed the length of \CODE{list}.
  {\em
    \begin{lstlisting}[firstnumber=34]
   y := null;
   x := list;
   while (pos>0) {
     y := x;
     x := x.next; // by hypothesis, this always succeeds
     pos := pos-1;
   }
   z := new Node(elem);
   z.next := x;
   if (y = null) {
     list := z;
   } else {
     y.next = z;
   }
    \end{lstlisting}
  }
  \noindent Suppose that \CODE{list} is cyclic after line 47, i.e., a
  traversal of the list visits the same node twice.  A close
  inspection of the code reveals that no cycle is created between
  lines 34 and 47: \CODE{list} is cyclic after line 47 if and only if
  it was cyclic before line 34.
  
  In the standard approach, it is possible to set the value of
  \CODE{list} after line 47 as the slicing criterion.  In this case,
  since \CODE{list} can be modified at lines 41--47, at least this
  piece of code must be included in the slice.

  On the other hand, let the cyclicity of \CODE{list} after line 47 be
  the property of interest, represented by  (Example
  \ref{ex:referenceAbstractDomains}).  Since this property of
  \CODE{list} does not change, the entire code can be removed from the
  slice.
\end{example}

\subsection{Defining Abstract Program Slicing}
\label{sec:definingAbstractProgramSlicing}

We introduce \emph{abstract program slicing}, which compares a program
and its abstract slices by considering \emph{properties} instead of
exact values of program variables.  Such properties are represented as
abstract domains, based on the theory of \emph{Abstract
  Interpretation} (Section \ref{sec:basicAbstractInterpretation}).

We first introduce the notion of \emph{abstract slicing criterion},
where the property of interest is also specified.  For the sake of
simplicity, the definition only refers to non- forms (i.e.,
 is a singleton instead of a set of occurrences: ). In order to make abstract the criterion we
have to formalize in it the {\em properties} that we aim at observing
on program variables. In particular we could think of observing
different properties for different variables.  Hence, we define a
criterion abstraction  defined as a tuple of abstract domains,
each one relative to a specific subset of program variables: Let 
be a set of variables of interest in  and

a partition of , the notation  means that each uco  is applied to the set of
variable  (left implicit when it is clear from the context),
meaning that  is precisely the property to observe on
. In the following, we denote by  the property
observed on , formally .
This is the most general representation, accounting also for
relational domains. When ucos will be applied to singletons, the
notation will be simplified ( instead of ).

\begin{example}
  Let , ,  and  be the variables in .  Let  be
  , meaning that the
  interest is on the parity of , the sign of , and the
  (relational) property of \emph{intervals} \cite{CC79} of the value
  .  When abstracting a criterion w.r.t.~, the required
  observation at a program state  is
  
\end{example}

In order to be as general as possible, we consider \emph{relational}
properties of variables (see Section~\ref{sec:preliminaries}), so that
properties are associated with tuples instead of single variables.  In
this case, a property is said to \emph{involve} some set (tuple) of
variables.  Given a memory ,  is the result of
applying  to the values in  of the variables involved
by the abstract domain, and  is the corresponding notion
for tuples of ucos.

\begin{mydefinition}[Abstract criterion]
  \label{def:AbstractCriteria} Let  be a set
  of input memories,  be a set of variables
  of interest;  be a set of
  occurrences of interest;  be a truth value indicating if the
  slicing is in  form.  Moreover, let  be the set of
  variables of interest and , with  a partition of .
  Then, the \emph{abstract slicing criterion} is , 
\end{mydefinition}

Note that, when dealing with non-abstract notions of slicing, we have
that each domains is the identity on each single variable, namely , where .
It is also worth pointing out that, exactly as it happens for
non-abstract forms,  corresponds to static slicing,
and  corresponds to dynamic slicing; in the intermediate
cases, we have conditioned slicing.

\COMMENT{
\begin{mydefinition}[Abstract slicing]
  \label{def:AbstractSlicing} Let  and  be executable
  programs such that  is obtained from  by removing
  zero or more statements, and let , with .  
   is an \emph{abstract slice} of
   with respect to  if (1) for each
  , when the execution of  from input
   reaches a point (or occurrence) in , the execution of
   from  reaches  as well; and (2) for each , each 
  has the same property  both in  and in ,
  i.e.,  where 
  and  are the current memories of  and ,
  respectively.  Moreover, if , then the executions of
   and  have to follow identical paths as explained in
  Definition \ref{def:UnifiedEquivalence}.
\end{mydefinition}}

\subsection{The extended formal framework}
\label{subsection:theFormalFramework}

In this section, we extend a formal framework in which all forms of abstract
slicing can be formally represented.  It is an extension of the
mathematical structure introduced by Binkley.  Following their framework, we represent a
form of abstract slicing by a pair , where
 is the traditional syntactic ordering, and 
is a function mapping abstract slicing criteria to semantic
equivalence relations on programs.  Given two programs  and
, and an abstract slicing criterion , we say that
 is a -\emph{(abstract)-slice} of
 with respect to  iff  and
 (i.e.,  and 
are equivalent w.r.t.~).  Some preliminary notions are
needed to define  in the context of abstract slicing.

An \emph{abstract memory} w.r.t.\ a set of variables of interests
 (partitioned in ) is obtained from a memory
by restricting its domain to the variables of interest, and assigning
to each set  of variables an abstract value determined by the
corresponding abstract property of interest .

\begin{mydefinition}
  \label{def:AbstractStateRestriction} Let 
  be a memory,  be the set of 
  a tuple of sets of variables of interest, and  be the corresponding tuple
  of properties of interest such that  is a
  partition of .  The \emph{abstract restriction} of a memory
   w.r.t.~the state abstraction  is defined as
  .
\end{mydefinition}

\begin{example}
  \label{ex:Var}
  Let  be a set of variables, and
  suppose that the properties of interest are the (relational) sign of the product
   and the parity of  (both defined in
  Section~\ref{sec:preliminaries}).
We slightly abuse notation by denoting as 
  also its extension to pairs  where the sign of their product
  matters: e.g., .  In our formal
  framework,  is defined as
  .  Let
  , , , and
  ; then,  comes to be
  .
\end{example}

The \emph{abstract projection} operator modifies a state trajectory by
removing all those states which do not contain occurrences or points
of interest.  If there is a state that contains an occurrence of
interest, then its memory state is restricted via  to the
variables of interest, and only a property is considered for each
tuple.  In the following, the \emph{abstract projection}
 is formally defined.

\begin{mydefinition}[Abstract Projection]
  \label{def:ProjA}
  Let , and 
  such that  if , 
  otherwise. For any , , , we define a function  as:
  
  The abstract projection  is the extension of
   to sequences:
  
\end{mydefinition}

\noindent
 takes a state from a state trajectory, and returns
either one pair or an empty sequence .  Abstract
projection allows us to define all the semantic equivalence relations
we need for representing the abstract forms of slicing.

\begin{example}
  \label{absSli} Consider the program  in
  Figure~\ref{fig:exsl}.
\begin{figure}
    \begin{center}
      {\small
        \begin{tabular}{c|c}
          \begin{lstlisting}
  read(n);
  i := 1; 
  s := 0; 
  p := 1;
  while (i <= n) {
    s := s+i;
    p := p*i;
    i := i+1; }
  write(i,n,s,p);
          \end{lstlisting} 
          &
          \begin{lstlisting}
  read(n);
  i := 1; 
  s := 0; 
  
  while (i <= n) {
    s := s+i;
    
    i := i+1; }
  write(i,n,s);
          \end{lstlisting} 
          \\
          &\\Program  & Program  
      \end{tabular}}
    \end{center}
    
    \caption{Program  and its slice.}\label{fig:exsl}
  \end{figure}  
  Consider  (meaning that we are considering static slicing),
  ,  (meaning that we check
  the value of variables of interest at each iteration of program
  point ).  Moreover, we consider .  Then in Figure~\ref{absFig} we have the
  corresponding abstract projection (the concrete trace is given in
  the Appendix, Example~\ref{ExTrace}).  In this figure, we depict
  states as set of boxes, the first one contains the number of the
  executed program point (with the iteration counter as apex), while
  the other boxes are the different variables associations. The cross
  on a box means that the projection does not consider that variable
  or state. So for instance, in this example we care only of states
   and , and in particular, in states  we are not
  interested in the values of variables, while in states  we are
  interested in the sign of  and in the parity of  (if we would
  be interested in the value of these variables we would have the
  value instead of their property, as it happens in the examples in
  the Appendix).
  
  \begin{figure}
    \begin{center}
      \includegraphics[scale=.4]{Proj6.pdf}
    \end{center}
    \caption{Abstract trajectory projection for program  in
      Example~\ref{absSli}}
    \label{absFig}
  \end{figure}
\end{example}

\subsection{Abstract Unified Equivalence}
\label{subsection:AbstractUnifiedEquivalence}

The only missing step for completing the formal definition of abstract
slicing in the formal framework is to characterize the functions
mapping abstract slicing criteria to abstract semantic equivalence
relations.

\begin{mydefinition}[Abstract Unifying Equivalence]
  \label{def:AbstractUnifiedEquivalence} Let  and  be
  executable programs, and  be an
  abstract criterion.  Then  is \emph{abstract-equivalent} to
   if and only if, for every , it holds that
  , where
   if .  The
  function  maps each criterion  to a
  corresponding abstract semantic equivalence relation.
\end{mydefinition}

Therefore, a generic form of slicing can be represented as
.  This can be used to formally define both
traditional and abstract forms of slicing in the presented abstract
formal framework, so that the latter comes to be a generalization of
the original formal framework.  The following examples show how it is
possible to use these definitions in order to check whether a program
is an abstract slice of another one.

\begin{figure}
  \begin{center}
    \begin{tabular}{c@{\quad}|@{\quad}c}
      \begin{lstlisting}
   read(n);
   read(s);
   i := 1;
   while (i<=n) {
     s := s+2*i;
     i := i+1;  }
   write(i,n,s);
      \end{lstlisting}
      &
      \begin{lstlisting}
   read(n);
   read(s);
   
   write(n,s);
      \end{lstlisting}
    \end{tabular}
  \end{center}
  \caption{Programs \prog and \progq}\label{fig:PandQ}
\end{figure}

\begin{example}
  Consider the programs  and  in
  Figure~\ref{fig:PandQ}. Let , meaning that we are
  interested in the parity of \ss
  () at the end of
  execution () for all possible inputs
  (), in non- form.  Since , in order to show that  is an abstract static slice
  of  with respect to , we have to show that
   holds.  Let
   for some 
  be an initial memory.  The trajectory of  from 
  contains the following steps of computation:
\begin{center}
\includegraphics[scale=.4]{esep1.pdf}
\end{center}
  Applying  (with  since )
  to  returns only the abstract value of the
  variable \ss at point  (due to ):
  
  \noindent Since we have ,
   returns
  .  The abstract memory
  restricts the domain of  to variables of interest, so we
  consider only the part of  regarding , i.e.,
  .  Hence, we have .  Since the parity of 
  only depends on the parity of , being either  or  even,
  the final result is .  Consider now the
  execution of  from , which corresponds to the
  following state trajectory:
  \begin{center}
\includegraphics[scale=.4]{eseq1.pdf}
\end{center}
  Applying  to 
  gives: 
  Therefore,
   is
  equal to
  .
  As  is an arbitrary input, this equation holds for each
  , so that , and this implies that  is an
  abstract static slice of  w.r.t.~.
\end{example}

\begin{figure}
  \begin{center}
    \begin{tabular}[t]{c@{\quad}|@{\quad}c}
      \begin{lstlisting}
   read(n);
   s := 0;
   i := 1;
   while (i<=n) {
     s := s+i;
     i := i+1; }
   write(i,n,s);
      \end{lstlisting}
      &
      \begin{lstlisting}
   read(n);
   s := 0;
   
   write(n,s);
      \end{lstlisting}
    \end{tabular}
  \end{center}
  \caption{Programs \progr and \progss}\label{fig:RandS}
\end{figure}
\begin{example}
  Consider the programs  and  in
  Figure~\ref{fig:RandS}, and let  be ,
  where ; i.e., we
  are interested in the parity of \ss at the end of the execution for
  all inputs where \nn is a multiple of .  Since
  , in order to show that  is an
  abstract conditioned slice of  w.r.t.~, we have
  to show that 
  holds, namely that they have the same abstract projection.  Let
   be an initial memory, and suppose
  .  The trajectory
   of  from  contains the
  following computations:
   \begin{center}
\includegraphics[scale=.4]{esep2.pdf}
\end{center}
While executing  from  gives the state
  trajectory 
   \begin{center}
\includegraphics[scale=.4]{eseq2.pdf}
\end{center}
  Applying  to both state trajectories we have:
  
  Therefore, we have
   .
  As  is an arbitrary input from , this equation holds
  for each , so that , and this implies that  is an
  abstract conditioned slice of  w.r.t.~. It is
  worth noting, that  is not a {\em static} abstract slice of
   since for all the input values  for
   the parity of the final value of \ss is not necessarily even.
\end{example}



\subsection{Comparing forms of Abstract Slicing}
\label{subsection:Comparing}
This section provides a formal theory allowing us to compare abstract
forms of slicing between themselves, and with non-abstract ones.
First of all, we show under which conditions an abstract semantic
equivalence relation \emph{subsumes} another one; analogously, we show
when the form of (\emph{abstract}) slicing, corresponding to the
former equivalence relation, subsumes the form of (\emph{abstract})
slicing corresponding to the latter one.  Such results are necessary
in order to obtain a precise characterization of the extension of the
\emph{weaker than} relation (whose original definition is recalled in
the Appendix) to the abstract forms of slicing.

\COMMENT{
  \begin{figure}[tbp]
    \centering
    \includegraphics[scale=.65,viewport=3.7in 6.7in 5.5in 8.45in]{SlicingTutto1.pdf}
    \caption{ \label{fig:SlcTto}}
  \end{figure}}

The following lemma shows under which conditions on the slicing
criteria there is a relation of subsumption between two semantic
equivalence relations. In the following, we denote
 the relation "more concrete than" in the
lattice of abstract interpretations between tuples of abstractions.
Formally, let us consider
 defined
on the variables  and
 defined
on the variables , such that ,  and  we have , namely the
variables in common are partitioned in the same way.  Then
 iff . Note that, for all the
variables in , the abstraction
 does not require any particular observation, hence on
these variables surely  is more precise. The following
relation is such that, when both  and  are the identity
on all the variables of interest, then the resulting criterion
relation is the same proposed in \cite{TheoFoun} (see the Appendix for
details) for characterizing the original formal framework.

\begin{lemma}
  \label{the:AbstractUnifiedEquivalenceLemma}
  Let two abstract slicing criteria  and
   be given.  If (1)
  ; (2) ; (3)
  ; (4) ;
  and (5)  (denoted \cA), then 
   \emph{subsumes} , i.e., for
  every  and  such that , 
  implies .
\end{lemma}

\begin{proof}
  First of all, note that, if , namely if we
  are considering concrete criteria, then \cA
  collapses to the concrete relation defined in \cite{AForm} (which is
  the  defined in Equation~\ref{eq:defrelcrit} in the
  Appendix). Hence, in this case, the results holds by
  \cite{AForm}.\\ Suppose  with , namely 
  slice of  w.r.t.\ .  This means that, for
  each  ,
  where  is defined as in
  Definition~\ref{def:AbstractUnifiedEquivalence}.  This means that,
  for each state  in the trajectory
  , whose projection
  \footnote{The notation  means that we are
    projecting a state of the computation of .}  is not empty,
  there exists a state  in
   with the same projection.  Let us
  consider now, . We prove that on
  these states  (and in this case there is a corresponding state in the
  trajectory of ), or it is empty (and in this case also the
  state in  has empty projection).  Recall that
  
  where also  is defined as in
  Definition~\ref{def:AbstractUnifiedEquivalence}.  Note that, since
  we are considering both the criteria on the same pair of programs,
  we have also that  corresponds to saying that
  . At this point
  \begin{itemize}
  \item[] If  then , but , hence . This mean that , which by hypothesis is equal
    to , for a memory . By definition and
    hypothesis, . Namely,
    .  Therefore, in particular,
     we have
    , but by
    hypothesis , hence we also have
     (by properties of
    ucos). But then , namely . Hence
    
  \item[] If  then . If  then , then also
     but then by hypothesis we have
    that there exists a memory  such that
    . But then we also have
    .\\ If , then , but
    then there exists  such that also in  we have
    . But
    then, the same memory, in  keep the program point but
    loses the state observation because , hence .
   \item[] Finally, if , then
     . But
     this means that, even if there exists  such that we
     have the state  in the trajectory of
     , also in this case we have .
  \end{itemize}
\end{proof}

\COMMENT{
\begin{lemma}
  \label{the:AbstractUnifiedEquivalenceLemma}
  \defthename{-lemma} Let  and  be sets of
  initial memory states,  and  be sets of variables,
   and  be sets of occurrences,  and  be
  functions from pairs of sets of line numbers to sets of line
  numbers, and  and  be partial functions from line
  numbers and variables to properties on variables, such that , , ,
   and , where
  , and  is the relation "more
  concrete than" in the lattice of abstract interpretations
  \footnote{Given the abstractions  and  modeled
    as \uco{}s,  iff .}, then .
\end{lemma}}

This lemma tells us how it is possible to find the relationship (in
the sense of subsumption) between two semantic equivalence relations
determined by two abstract slicing criteria.  In the following,
abstract notions of slicing will be denoted by adding an ; e.g.,
 denotes static abstract slicing, whereas 
denotes dynamic abstract slicing.  By using this lemma we can show
that, given a slicing criterion , all the abstract
equivalence relations introduced in
Sec.~\ref{subsection:AbstractUnifiedEquivalence} subsume the
corresponding non-abstract equivalence relations
,  and
.  Furthermore, by using this lemma we can
show that  subsumes
, which in turns subsumes
.

\begin{theorem}
  \cite{TheoFoun} Let  and  be semantic equivalence
  relations such that  subsumes .  Then, for every
   and , we have .
\end{theorem}

\COMMENT{
We define two relations for comparing slicing criterion, 
and .   permits to compare criteria of
abstract forms of slicing among themselves, while 
compares criteria of abstract forms of slicing with criteria of
non-abstract forms of slicing.

\begin{mydefinition}
  \label{def:RelA1}
  The partial order  is defined by the following rule:
   such that , , , 
  
\end{mydefinition}

This rule allows comparing  to
 being , and
 to 
whenever .

\begin{mydefinition}
  \label{def:RelA2}
  The relation  for comparing slicing criteria is defined
  by the following rule: , ,  where
  , , ,
  
\end{mydefinition}

This rules allows to compare  to
,  to
, and  to
.

\COMMENT{
  \begin{theorem} The following relations hold:
    
      \begin{array}{c}
        (\sqsubseteq, \mathcal{AD}) \wkth{\ccompa{1}} (\sqsubseteq,
        \mathcal{AC}) \quad (\sqsubseteq, \mathcal{AC})
        \wkth{\ccompa{1}} (\sqsubseteq, \mathcal{AS})\\ 
        (\sqsubseteq, \mathcal{AS}) \wkth{\ccompa{2}} (\sqsubseteq,
        \mathcal{S}) \quad (\sqsubseteq, \mathcal{AD})
        \wkth{\ccompa{2}} (\sqsubseteq, \mathcal{D}) \quad
        (\sqsubseteq, \mathcal{AC}) \wkth{\ccompa{2}} (\sqsubseteq,
        \mathcal{C})
      \end{array}
  \end{theorem}}}
\begin{figure}[htbp]
 \centering
    \includegraphics[scale=.6,viewport=3in 6.35in 4.5in 9in]{Reticolo2a.pdf}
  \caption{Extended hierarchy. \label{fig:Ret2}}
\end{figure}
Fig.~\ref{fig:Ret2} shows the non-\SIM hierarchy obtained by enriching
the hierarchy in Fig.~\ref{fig:Ret1} with standard forms of
\absstcslcg, \absdynslcg{}, and \absconslcg.  In general, we can
enrich this hierarchy with any abstract form of slicing simply by
using the comparison notions defined above.  Non-abstract forms are
particular cases of abstract forms of slicing, as they can be
instantiated by choosing the identity property, , for each
variable of interest.  Hence, non-abstract forms are the "strongest"
forms, since, for each property , we have .  Moreover, if parameters  are fixed, and
 is made less precise or more abstract (i.e., the information
represented by the property is reduced), then the abstract slicing
form becomes weaker, as suggested by dotted lines in Figure
\ref{fig:Ret2}.



\section{Program Slicing and Dependencies}
\label{subsection:dependencies}
In the previous sections we introduced a formal framework of different
notions of program slicing.  In particular, we observed that a kind of
slicing is a pair: a syntactic preorder and a semantic equivalence
relation \cite{AForm}.  After discussing how the notion of ``to be a
slice of'' can be formally \emph{defined}, the focus will shift to how
to \emph{compute} a slice given a program and a slicing criterion.
Again, among all the possible definitions of slicing, we are
interested in slices obtained by erasing statements from the original
program, i.e., the slice is related to the original program by the
syntactic ordering relation .  Given a slicing criterion,
the idea is keeping all the statements affecting the \emph{semantic
  equivalence relation} defined by the criterion.  In other words, we
should have to \emph{translate} the formal definition into a
characterization of which statements has to be kept in a slice, or
vice versa which statements can be erased, in order to preserve the
semantic equivalence defining the chosen notion of slicing.
Intuitively, we have to keep all the statements \emph{affecting} the
semantics defined by the chosen slicing criterion.

The standard approach for characterizing slices and the corresponding
relation \emph{being slice of} is based on the notion of Program
Dependency Graph \cite{horPR89,RY88}, as described by Binkley and
Gallagher \cite{BinGalla96}.  \emph{Program Dependency Graphs} (PDGs)
can be built out of programs, and describe how data propagate at
runtime.  In program slicing, we could be interested in computing
dependencies on statements:  depends on  if some variables
which are used inside  are defined inside , and definitions
in  reach  through at least one possible execution path.
Also,  depends \emph{implicitly} on an if-statement or a loop if
its execution depends on the boolean guard.
\begin{example}
  \label{example:statementDependency}
  Consider the program in Figure~\ref{pdgfig} and the derived PDG
  (edges which can be obtained by transitivity are omitted).
  \begin{figure}[h]
    \center{ \begin{picture}(0,0)\includegraphics{depGraph.pdf}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(3918,1557)(1651,-1221)
\put(2540,-1185){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{y}{v+1}}}}}}
\put(4593,252){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5277, 47){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5140,-227){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4388,-227){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4388,-637){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4388,-1048){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5140,-1048){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4046,-158){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1925,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{w}{3}}}}}}
\put(1651,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3156,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3430,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{z}{3}}}}}}
\put(1651,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1651,-774){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1651,-637){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1925,-774){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{v}{z+w}}}}}}
\put(3430,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{v}{4}}}}}}
\put(1925,-637){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{w}{z+4}}}}}}
\put(1925,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{z}{1}}}}}}
\put(3156,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2267, 47){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2540, 47){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2267,-1185){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture}}
\caption{PDG example.}  \label{pdgfig}
  \end{figure}
   depends on both  and  (and, by transitivity, )
  since  is not known statically when entering .  On the other
  hand, there is \emph{no} dependency of  on either (i) ,
  since  is not used in ; or (ii) , since  is always
  redefined before .  The dependency of  on  is
  implicit since  does not depend on  nor , but  is
  executed conditionally on .
\end{example}
Formally, a \emph{Program Dependence Graph} \cite{GL91ieee}
 for a program  is a directed graph with nodes
denoting program components and edges denoting \emph{dependencies}
between components.  The nodes of  represent the assignment
statements and control predicates in .  In addition, nodes
include a distinguished node called \emph{Entry}, denoting where the
execution starts.  An edge represents either a \emph{control
  dependency} or a \emph{flow} (\emph{data}) {\em dependency}.
Control dependency edges  are such that (1)  is the
entry node and  represents a component of  that is not
nested within any control predicate; or (2)  represents a control
predicate and  represents a component of  immediately nested
within the control predicate represented by .  Flow dependency
edges  are such that (1)  is a node that defines the
variable  (usually an assignment), (2)  is a node that
\emph{uses} , and (3) control can reach  from  via an
execution path along which there is no intervening re-definition of
.

Unfortunately, there is a clear \emph{gap} between the definition of
slicing given in Definition~\ref{defSlice} and the standard
implementation based on program dependency graphs (PDG)
\cite{horPR89,reps91}.  This happens because slicing and dependencies
are usually defined at \emph{different levels} of approximation.  In
particular, we can note that the slicing definition in the formal
framework defines slicing by requiring the same \emph{behavior}, with
respect to a criterion, between the program and the slice, i.e., we
are specifying what is \emph{relevant} as a \emph{semantic}
requirement.  On the other hand, dependency-based approaches consider
a notion of dependency between statements which corresponds to the
\emph{syntactic} presence of a variable in the definition of another
variable.  In other words, slices are usually defined at the
\emph{semantic} level, while dependencies are defined at the
\emph{syntactic} level.  The idea presented in this paper consists,
first of all, in identifying a notion of \emph{semantic} dependency
corresponding to the slicing definition given above, in order to
characterize the implicit parametricity of the notion of slicing on a
corresponding notion of dependency.  This way, we can precisely
identify the semantic definition of slicing corresponding to a given
dependency-based algorithm, characterizing so far the loss of
precision of a given algorithm w.r.t.~the semantic definition.
\begin{figure}
  \begin{center}
    \includegraphics[scale=.3]{schema.pdf}
  \end{center}
  \caption{Schema of dependency-based slicing notions.}\label{fig:schema}
\end{figure}
In Figure~\ref{fig:schema} we show these relations. In particular,
starting from the criterion, we can define an equivalent notion of
dependency which allows us to identify which variables should be kept
in a slice, affecting the whole program semantics (program dependency
notion).
\begin{mydefinition}[(Semantic) Program dependency]
Let  be a slicing criterion, and  be
a program. The program depends on , denoted
 iff

\end{mydefinition}
This means that the variable  affects the observable semantics of
.

Unfortunately, this characterization is not effective due to
undecidability of the program semantics.
In particular, the amount of traces to compare could be infinite, and
also the traces themselves could be infinite. Hence, we consider a
stronger notion of dependency that looks for {\em local} semantic
dependencies, identifying all the variables affecting at least one
expression used in the program (expression dependency notion), and
this is precisely the {\em semantic} generalization of the syntactic
dependency notion used, for instance, in PDG-based algorithm for
slicing.  In other words we characterize when a variable affects the
semantics of an expression in .

Our idea is to make semantic the standard notion of syntactic
dependency, by substituting the notion of \emph{uses} with the notion
of \emph{depends on} \cite{GiacobazziJM12}.  In order to obtain this
characterization, we have to find which variables might affect the
evaluation of the expression  in the assignment \CODE{z:=e} or
in a control statement guarded by , i.e., which variables belong
to the set  of the variables \emph{relevant} to the
evaluation of .  As already pointed out, standard syntactic
dependency calculi compute  as .
\begin{mydefinition}[(Semantic) Expression dependency]
  \label{def:concreteDependencies}
  Let  be a slicing criterion.  Let , .
    
  The formulation of  can be rewritten as
  
\end{mydefinition}

\begin{proposition}\label{prop:approxdep}
  Let  be a slicing criterion. If 
  then there exists  in  such that
  .
\end{proposition} 

\begin{proof}
  Let us reason by contradiction. If for each  in  we
  have , then all the expressions in
   do not depend on  and therefore, independently from ,
   provides precisely the same results.
\end{proof}

By using this notion of dependency, we can characterize the subset
 containing exactly those
variables which are \emph{semantically} relevant for the evaluation of
.  This way, we obtain a notion of dependency which allows us to
derive more precise slices, i.e., to remove statements that a merely
syntactic analysis would leave.

\begin{example}
  \label{ese1} Consider the program : \vspace{-.3cm}
  \begin{lstlisting}
   x:=;
   y:=; 
   w:=;
   z:=w+y+2(x)-w;
  \end{lstlisting}
  \noindent where ,  and  are expressions.  We
  want to compute the static slice  of  affecting the
  final value of \zz (i.e., the \emph{slicing criterion}  is
  interested in the final value of \zz).  If we consider the
  traditional notion of slicing, then it is clear that we can erase
  line 3 without changing the final result for \zz.  However, in the
  usual syntactic approach, we would have a dependency between \zz and
  \ww, since \ww is used where \zz is defined.  Consequently, the
  slice obtained by applying this form of dependency would leave the
  program unchanged.  On the other hand, if the semantic dependency is
  considered, then the evaluation of
   does not depend
  on the possible variations of \ww, which implies that we are able to
  erase line 3 from the slice.
\end{example}

Next we show how the PDG-based approach to slicing can be modified in
order to cope with semantic slicing.  The PDG approach is based on the
computation of the set of variables \emph{used} in a expression
.  In the following, we wonder if this set can be rewritten by
considering a semantic form of dependency.

Hence, let us define the new notion of \emph{semantic} PDG, where all
the flow dependencies are {\em semantic}, i.e., we substitute the flow
edges defined above with semantic flow dependency edges 
which are such that (1)  is a node that defines the variable 
(usually an assignment), (2)  is a node containing an expression
 such that  (where  is the
criterion with respect with we are computing the slice), and (3)
control can reach  from  via an execution path along which there
is no intervening re-definition of . A (semantic) flow path is a
sequence of (semantic) flow edges.

\begin{proposition}\label{prop:flowdep}
  Let  be a program and  be a slicing criterion. Let
   the PDG with flow dependency edges , and
   be the semantic PDG where the flow dependency edge
  are semantic . If  then .
\end{proposition}

\begin{proof}
  Trivially, since if , then  must
  use the variable .
\end{proof}

In principle, a (backward) slice is composed by all the statements
(i.e., nodes) such that there exists a path from the corresponding
node to the relevant (according to the slicing criterion) use of a
variable of interest (in the criterion) \cite{RY88}.
In other words, we follow backward the (semantic) flow edges from the
nodes identified by the criterion, and we keep all the
nodes/statements we reach.  Hence, the criterion, and therefore the
dependency notion, defines the edges that we can follow for computing
the slice.
 
By using the semantic flow dependency edges, we can draw a new
\emph{semantic} PDG containing less flow edges, i.e., only those
corresponding to semantic dependencies.  At this point, the type of
slicing (either static, dynamic or conditional) characterized by the
criterion decides which nodes can be kept in the PDG.

\begin{theorem}\label{th:slispdg}
  Let  be a slicing criterion. Let
   be a program and  its \emph{semantic PDG},
  i.e., a PDG whose flow edges are .  Let  the
  subprogram of  containing all the statements corresponding to
  nodes such that there exists a semantic flow path in 
  from them to a node in
  . Then  is
  a slice w.r.t.\ the criterion .
\end{theorem}

\begin{proof}
  Note that, the PDG construction is syntactic, and therefore
  independent from the input set , hence any slice computed by
  using the PDG holds for any possible input memory in
  . Moreover, since we simply collects the statements
  potentially affecting the program observation, we cannot decide
  which iteration to observe, for this reason each program point is
  taken in the slice independently from the iteration to observe, for
  this reason the obtained slice will provide the same result for any
  possible iteration, i.e., the set of interesting points to observe
  are
  . Finally,
  by construction we cannot have statements executed in the slice
  which are not executed in the original programs. Hence, the
  criterion enforced by the PDG slice construction is
   (direct
  consequence of the \emph{Slicing theorem} in \cite{RY88}), which for
  each  and
   is a slice also
  w.r.t.\ the criterion
  , by \cite{AForm}
  (Equation~\ref{eq:defrelcrit} in the Appendix). Hence, we have that
  the results is a slice w.r.t.\  \cite{AForm}.
\end{proof}

We have pointed out so far the difference between syntactic and
semantic dependencies: it can be the case that a variable
syntactically appears in an expression without affecting its value.
Actually, one could argue that the case is not very likely to happen:
the possibility to find an assignment like \CODE{x:=y-y} in code
written by a professional software engineer is remote to the very
least.  However, when it comes to abstract dependencies, the picture
is quite different, and we could even say that, in the present work,
(concrete) semantic dependencies have been mainly introduced to
prepare the discussion about their abstract counterpart.  Indeed, it
is much more likely that some variables are not semantically relevant
to an expression if the value of interest is an abstract one, e.g.,
the parity or the sign of a numeric expression, or the nullity of a
pointer.  This justifies the definition of a semantic notion of
dependency at the abstract level.


\section{Abstract Dependencies}
\label{sec:abstractDependencies}
This section discusses the problem of defining and computing abstract
dependencies allowing us to capture the dependency relation between
variables w.r.t.\ a given abstract criterion.  In the previous
section, we formalized this relation in the concrete semantic case;
the following example takes it to the abstract level.

\begin{example}
  Consider the expression ~\CODE{2x+y}: although both
  variables are semantically relevant to the result, only \yy can
  affect its parity, since \CODE{2x} will always be even.  On the
  other hand, note that both variables are relevant to the sign of
  , in spite of the positivity of \CODE{x}.  In fact, given
  a negative value for \yy, a change in the value of \xx can alter the
  sign of the entire expression.
\end{example}

First of all, it is worth noting that the notion of \emph{semantic
  program dependency} can be easily extended to abstract criteria,
simply by changing the projection considered.  In this case we will
write that , meaning that  has
effect on the abstract projection of  determined by
.  Also in the abstract case we inherit the undecidability
of the concrete semantics of ; hence, again, we have to
approximate the semantic program dependency with a local notion of
\emph{abstract semantic expression dependency}.  Unfortunately, when
dealing with abstract criteria , some aspects become more
complicated.

\subsection{Abstract slicing and dependencies}

In the previous section, we defined the concrete semantic dependency
by identifying those variables that do not \emph{interfere} with the
final observation of each expression.  Analogously, in order to define
a general notion of abstract semantic dependency, we need to consider
the \emph{abstract} interference between a property of a variable and
a property of an expression.

The definition below follows the same philosophy as \emph{narrow
  abstract non-interference} \cite{GM04popl,Mastroeni13}, where
abstractions for observing input and output are considered, but these
abstractions are observations of the \emph{concrete} executions.
\begin{mydefinition}[(Abstract) Semantic expression dependency, \emph{Ndep}]
  \label{def:abstractDependencies}\label{def:narrowDependencies}
  Consider the abstractions  and
  , where  is the number of
  variables, i.e.,  is a tuple of properties such that
   is the property on the variable .
    
  \end{mydefinition}

This notion is a generalization of
Definition~\ref{def:concreteDependencies} where we abstract the
observation of the result () and the information that we fix
about all the variables different from \xx ().  Still, this
notion characterizes whether the variation of the value of \xx affects
the abstract evaluation in  of .
 
As an important result, we have
 whenever  and
 induce the same partitions (either on values, or tuples of
values).  This happens because, in
Definition~\ref{def:abstractDependencies}, both abstractions are only
applied to singletons.  In the following, only partitioning domains
will be considered since it is straightforward to note that
 is affected only by
, rather than by  itself.

When dealing with abstractions, and therefore with abstract
computations, some more considerations have to be taken into account.
Consider the program in Example~\ref{ese1}, and consider the 
property (Section \ref{sec:basicAbstractInterpretation}) for all
variables on both input and output .  If we compute the set of
variables on which the parity of  \CODE{w+y+2(x)-w}
depends on, then we can observe that  is still independent from
\ww, but is also independent from any possible variation of \xx.
At a first sight, the parity of \CODE{w+y+2(x)-w} is independent
from \xx just because \CODE{2(x)} is constantly even.  However, it
is not only a matter of constancy: a deeper analysis would note that
we can look simply at the abstract value of \xx only because the
operation involved (the sum) in the evaluation is \emph{complete}
(Section \ref{sec:basicAbstractInterpretation}), i.e., precise,
w.r.t.~the abstract domain considered ().  In particular,
when we deal with abstract domains which are complete for the
considered operations, then it is enough to look at the abstract value
of variables in order to compute dependencies.  Indeed, consider
the  domain (Section \ref{sec:basicAbstractInterpretation}).
In this case, even if the sign of \CODE{2(x)} is constantly
positive, the final sign of \zz might be affected by a concrete
variation of \xx (e.g., consider  and two executions
in which \xx is, respectively, 1 and 5).  Therefore, \xx has to be
considered relevant, although the sign of \CODE{2(x)} (the only
sub-expression containing \xx) is constant.  This can be also derived
by considering the logic of independencies from \cite{AB07} since, by
varying the value of \xx, we can change the sign of .

Unfortunately, the notion of abstract dependencies given in
Definition~\ref{def:abstractDependencies} is not suitable for
weakening the PDG approach, as we have done in the concrete semantic
case.  Let us explain the problem in the following example.
\begin{example}\label{ex:probabs}
  Consider the program \CODE{C; x:=y>0?0:1;}, where \CODE{C}
  is come code fragment and the expression \CODE{b?e1:e2} evaluates to
  \CODE{e1} if \CODE{b} is true, and to \CODE{e2} otherwise.

  \noindent Suppose the criterion requires the observation of the
  parity of  at this program point, i.e., .
  Then it is straightforward to observe that the expression depends on
  , but we would like to be more specific (being in the context of
  abstract slicing), and we can observe that it is the sign of 
  that affects the parity of the expression, and therefore of . At
  this point, in the code \CODE{C} we should look for the variables
  affecting not simply  (as expected in standard slicing
  approaches), but more specifically the {\em sign} of , a
  requirement not considered in the abstract criterion.
\end{example}

This example shows that, if we aim at computing abstract dependencies
without losing too much information, we would need an algorithm able
to keep trace backwards, not only of the different variables that
become of interest (affecting the desired criterion), but also of the
different properties to observe on variables affecting the desired
property of the criterion. This means that, while for the concrete
semantic program dependency we can provide a definition depending only
on the criterion, this is not possible in a more abstract context,
where each flow edge should be defined depending on abstract
properties potentially different from those in the abstract criterion,
and which should be characterized dynamically backward starting from
the criterion. Unfortunately, this is not possible in
Definition~\ref{def:abstractDependencies}, where we always look for
the variation of the value and not of an abstract property of \xx,
hence if we would use this notion for substituting the semantic
dependency in PDGs we would not have so much advantage.

These observations make clear that, if we aim at constructively
characterize abstract slicing by means of the abstract dependency
notion provided in Definition~\ref{def:abstractDependencies}, we need
to build from scratch a systematic approach for characterizing
abstract slicing. Towards this direction, the first step we propose is
a computable approximation of the abstract dependencies of
Definition~\ref{def:abstractDependencies}.



\subsection{A constructive approach to Abstract Dependencies}
\label{sec:constructiveApproachesToAbstractProgramSlicing}
By means of the (uco-dependent) definition of operations on abstract
values, it is possible to automatically obtain (an over-approximation
of) the set of relevant variables.  The starting point is the
\emph{brute-force} approach which uses the abstract version of
concrete operations, and explicitly \emph{goes into} the quantifiers
involved in Definition \ref{def:narrowDependencies}.

\begin{example}
  Consider the program in Example~\ref{ese1} and let .
  In order to decide whether
   holds, the
  brute-force approach considers the abstract evaluation of  in
  all contexts where all variables different from \xx does not change,
  up to , while \xx may change.  In this example, this
  boils down to consider pairs of memories where \yy has the same
  parity (with no information about sign), and we take memories where
  \xx changes value, and see whether the final values of  agree
  on the expression observation .  Suppose 
  (meaning that it is even but the sign is unknown) and suppose
  , then we should have to compute the abstract value
  of  for each possible value for \xx. It is clear that we can
  easily find  and  such that
   even if 
  (for instance  while
   and ).  It is clear that the sign
  of  may depend on the value of \xx since to ``fix'' the
  abstract property of the other variables is not enough to ``fix''
  the final value of  w.r.t.~. On the other hand, the
  parity of  does not depend on \xx, in particular if we fix the
  property  of \yy, for instance to  (but it holds
  also for the other abstract values) then
   Namely, if we fix all the (abstract values
  of the) variables but \xx, then the parity of the result does not
  change, hence the variation of \xx does not affect the parity of the
  expression .
\end{example}

In the following, we introduce an algorithm able to improve the
computational complexity of the brute-force approach, especially on
bigger ucos, and when (1) several variables are involved in
expressions, and (2) a significant part of them is irrelevant.

\subsubsection{Checking \emph{Ndep}}
\label{section:algorithmicIdeasForCheckingNdep}

In the following, we discuss how we can constructively compute narrow
dependencies.  Unfortunately, in static analysis, the concrete
semantics cannot be used directly as it appears in
Definition~\ref{def:narrowDependencies}, hence we need to approximate
this abstract notion. The following definition introduces a stronger
notion of dependency based on a sound abstract semantics
 (Section \ref{sec:abstractSemantics}), which
approximates narrow dependencies.
\begin{mydefinition}[\emph{Atom-dep}]
  \label{def:atomicAbstractDependencies}
  An expression  \emph{atom-depends} on  (written
  ) with respect to
   and 
  ( number of variables) if and only if there exist  such that
  
\end{mydefinition}
Being domains partitioning, the \emph{non-atomicity} requirement  amounts to say that all -abstract
evaluations of , starting from different values for , may not
be abstracted in the same abstract value (this is the crucial issue in
\emph{Ndep}), i.e.,  and  may lead to different
abstract values for . Next results shows that {\em Atom-dep} is an
approximation of {\em Ndep}, since {\em Ndep} implies {\em Atom-dep},
meaning that {\em Atom-dep} may only add false dependencies, but
cannot lose abstract dependencies characterized by {\em Ndep}.

\begin{proposition}\label{theorem:equivalenza}
  Consider the abstractions  and
  , where  is the number of
  variables, i.e.,  is a tuples of properties.  For every
   and , 
  implies .
\end{proposition}

\begin{proof}
  Suppose , i.e.,
  , then we have to prove
  , i.e., there exist
   such that  and . Since
  conditions  and  are the same, we have to prove that
  .
  
  Consider  satisfying  and such
  that ,
  then . At this point, since
   contains two different
  values of , then, by definition, it cannot be an atom of
  .
\end{proof}

Starting from this new approximate notion, our idea is to provide an
algorithm
over-approximating the set of variables relevant for a given
expression  when , namely the
abstraction observed in output is the same that we fix on the input
variables.  The idea is to start from an empty set of not relevant
variables  for an expression , and incrementally increase
this set adding all those variables that \emph{surely} are not
relevant for the expression (obtaining an under-approximation of
abstract dependencies). Finally, the complement of such set is
returned, which is an over-approximation of relevant variables.

In order to check the dependency relation, we aim at checking whether
a change of the values of a variables makes a difference in the
evaluation of the expression.  Dependencies are computed according to
\emph{Atom-dep}, in order to approximate \emph{Ndep}.  In a
brute-force approach, \emph{Atom-dep} would be verified by checking
for each  associating atomic values to variables we have that
 is always the same atom.

\begin{example}
  \label{example:dependenciesOnSign}
  Let  be an expression involving variables ,  and , and
  .  In principle, in order to compute the set of
  -dependencies on , we must compute  on
  every possible atomic value\footnote{Remember that atoms in 
    are , , , , and ;
    since this is a partition of concrete values, we describe all
    concrete inputs by computing  on atoms.} of \xx,
  \yy and \zz, i.e.,  must be computed 
  times.  \yy is \emph{not} relevant to  if, for any abstract
  values , there exists an
  atomic abstract value  such that .
  This amounts to say that changing the value of \yy does not affect
  , since we require the same output atomic evaluation for each
  possible abstract value for \yy. Indeed, if the result is not
  atomic, it means that we have at least two different abstract
  results for different values of \yy. Analogously, for different
  (abstract) values for \yy we have different atomic results, then
  again it means that there exists a variation of \yy affecting the
  abstract evaluation of .
\end{example}

\noindent
However, it is possible to be smarter:

\begin{itemize}
\item[] \emph{Excluding states:} consider dependencies of
   in Example \ref{example:dependenciesOnSign}, computed at
  program point .  Suppose  (used as a tool to
  infer invariant properties, as discussed in Section
  \ref{sec:abstractSemantics}) is able to infer, at point , that
  the abstract state  is such that  correctly approximates the value of variables at
  .  Then, we only need to consider states of the form
  
  as inputs for  (now considered as the abstract
  computation of expressions, according to Definition
  \ref{def:atomicAbstractDependencies}) at .
\item[] \emph{Computing on non-atomic states:} let  and .  In
  this case,
   is implied by the more general result
   since  and  are partitions, respectively, of
   and , and  is monotone:  implies .  This means that results obtained on
   can be used on .
\end{itemize}

In the following, we compute dependencies w.r.t. the abstract state
, which is the abstract state computed at  as an
invariant at that program point. In the worse case, we don't have any
information about the different variables, and therefore 
associates  (of the considered abstraction ) to all the
unknown variables.  At this point, we aim at proving that  is
independent from a set of variable , and in order to prove this
fact we need to prove that the evaluation of  is always the same
atom in , independently from the value of the variables in ,
without proving for all these values. Hence, our idea is to prove this
atomicity, if it holds, by iteratively refining the starting abstract
state . Let us explain the intuition in the following
example.

\begin{example}
  \label{example:funesharppMeaning}
  Let  \CODE{x * x + 1} and .  
  Let also
   follow the usual rules on  and : , , , , , etc.  Suppose to start from a memory such that , then we observe that 
   cannot be proved by using these
  rules, since , which is not atomic, and therefore there may be a dependency. Then, consider the possible refinements w.r.t. \xx in , namely 
  . 
  This is enough to compute
   meaning that any
  variation of \xx provides the same atomic result in .
\end{example}

Given an initial abstract state , we define the set of all
the abstract states that refine its abstract value on the variables in
, while leaving unchanged atomic values of other variables.

This is the set of all the possible abstract states that can be
obtained by restricting abstract values of variables in  (the least
values are the atoms of ) starting from an initial memory
, while all the other variables are atomic\footnote{Namely,
  all the other variables are fixed to the smallest possible abstract
  values in the abstract domain .} and fixed by .
Hence, the idea is that, once that all the variables different from
\xx are specified by atoms, we first compute the expression with
, if the result is atomic, it means that all
the values for \xx provide the same results, and we can conclude that
surely  does not depend on \xx. If the result is not atomic, it
may be because there is a dependency or because the abstraction is
incomplete for the semantics of the expressions, hence, in order not
to be too coarse, we consider the covering of the  in ,
namely we take the elements under the top and we repeat for all these
values. If all the computations provide the same atomic results then
we surely have  independent form \xx, otherwise we continue to
refine the abstract values. If, we reach atomic values for \xx then we
terminate the recursion and we conclude that there may be
dependency. Hence we have to define a recursive predicate computing
this iteration: Given an expression  and an atom , the
\emph{atomicity condition}  holds iff
 gives .

with

where  iff
 or  is a direct
sub-value of  in . Intuitively,
 terminates the iterations either when the
evaluation of the expression is always the same atom independently
from the abstract value of the variables in  (meaning that there is
no dependency from ) or when, for all the possible atomic values
for variables in , the evaluation of the expression is not atomic
or may have different atomic values (meaning that there may be
dependency from variables in ).  Hence, a judgment
 means that an atomic value for  was
obtained without the need of further restricting the variables in ,
when all the other variables are atomic; therefore,  only contains
non-relevant variables.
\COMMENT{
Let the set  denote all the abstract states
\footnote{Comparison on abstract states is
  variable-wise comparison on abstract values.} such that (1) ; and (2)
 holds for every .  Namely,
 is the set of abstract states which are equal to
 on variables belonging to , and yield atomic abstract
values on other variables.  To prove  independent from , we need
to prove  for any , where  is the abstract state
computed at  as an invariant at that program point.  This amounts
to say that any variation in  (up to the abstract value ) does not lead to an observable
variation in , whenever all the other variables are fully specified
as atoms.  Given an expression  and an atom , the
\emph{atomicity condition}  holds iff
 gives , or there exists a covering
(Section \ref{sec:abstractSemantics}) 
of  such that  holds for every
.  Importantly,  implies that
 is an atom, and
the second disjunct helps if, due to some loss of information,
 although . 
According to Definition \ref{def:atomicAbstractDependencies}, in order
to prove the non-relevance of \xx it is enough to have  for every  belonging to
.  
Given a set of variables
, a covering  of  is said to be
an \emph{-covering} if (i) for every , and for every ,
it holds that ; and (ii) for every , and for every ,  is either  or
one of its \emph{direct} sub-values (i.e., some 
such that there is no  with
).

The assertion  holds iff (1) there exists an
abstract value  such that ; or (2)
there is an -covering  of ,
such that .
Intuitively, an -covering is a covering set of ``restrictions'' on
an abstract state, which do not involve variables in .  Clearly,
the condition for the non-relevance of a set  to an expression is
related to the definition of -covering, since 
can be obtained by repeatedly applying to  (and the newly
obtained states) the ``compute an -covering of an abstract state''
operation.  The statement  implies that
 is an atom for every  and, therefore, the non-relevance of the whole set
.
The  algorithm (Figure \ref{fig:NdepAlgorithm}) starts by
trying to prove ; since  is
the only -covering of any , this condition is
only satisfiable if  holds, i.e., if 
depends on no variables.  Otherwise, the set  is decreased
non-deterministically (one element at a time, randomly) until some
judgment  is proved.  A judgment
 means that an atomic value for  was obtained
without the need of restricting  variables; therefore,  only
contains non-relevant variables.
\begin{proposition}[Soundness]
  \label{proposition:correctness}
Let , and let  denote the tuple of , on each variable.  
  If  can be proved, then there is no  such that .
\end{proposition} 

\begin{proof}
  Consider the definition of : if
   holds for some atomic abstract value
  , then there are no dependencies at all.  Otherwise, let
   be one of the states belonging to the -covering of
  ; for every , the concrete value
   is abstracted to the same atom, i.e., there is
  no way to distinguish between two computations.  This means that it
  is possible to change  variables to any value, without changing
  the property of .  Since this is true for every , and
  these states are a covering of , the thesis follows.
  \end{proof}
}

The  algorithm (Figure \ref{fig:NdepAlgorithm}) starts by
trying to prove that  is atomic.  If it
find a restriction of the abstract values of all the variables making
the  evaluation atomic, then  depends on no variables.
Otherwise, the set  is decreased non-deterministically (one element
at a time, randomly) until some judgment  is
proved.
\begin{proposition}[Soundness]
  \label{proposition:correctness}
  Let  be partitioning and
  \footnote{This represent the initial information about ,
    if there is no information then it works mapping all the variables
    in  to } an abstract state. Let  denote the
  tuple of , on each variable.  For all  we have that if
   (namely not ), then
  there is no  such that
  .
\end{proposition} 

\begin{proof}
  Suppose to know that the variables in  may soundly have only the
  values determined by , namely for each  the
  concrete value of  in the computation is contained in
  . This means that, for each , we can check
  dependency only for the values ranging over .  Consider
  , by definition of
   we have that  we have
  , moreover again by
  definition these values are atomic hence
  . At
  this point, in order to prove that there is no dependency, we have
  to show that
  
  holds. But, at this point, by hypothesis since
  , meaning that there exists a
  covering of 
   and
   such that  we have
  , but this implies that
   and ,
  which trivially implies the thesis.
\end{proof}

\noindent
Importantly, the assertions  and
 guarantee  not to hold, even if
 cannot be directly proved.  The final
result of  is , where  is
the union of all sets  such that  can be
proved.  By Proposition \ref{proposition:correctness}, this set is an
over-approximation of relevant variables.
\begin{figure}
  \begin{pseudocode}
   function  { 
      := ; // 
     (,);
     return ; // 
   }
   procedure (,) {
     if() then  := ;
     else foreach () { (,); }
   }
  \end{pseudocode}
  \caption{The  algorithm}
  \label{fig:NdepAlgorithm}
\end{figure}

The  algorithm may deal, in principle, with
\emph{infinite} abstract domains, and in particular with abstract
domains with infinite descending chains, since non-dependency results
can be possibly proved without exploring the entire state-space; in
fact, if  can be proved, then it is not
needed to descend into the (possibly infinite) set of sub-states of
.  This is not possible in the brute-force approach.  It is
also straightforward to add \emph{computational bounds} in order to
stop ``refining'' states if some amount of computational effort has
been reached.

\subsubsection{Dependency erasure in the abstract framework}
\label{section:simplifyingDomainsForEliminatingDependencies}

The problem of computing abstract dependencies can be observed from
another point of view: given  and a set  of variables, we may be
interested in soundly approximating the \emph{most concrete} 
such that  does not
hold. Namely, the most concrete observation guaranteeing the
non-interference of the variables in  on the evaluation of 
\cite{GM04popl,Mastroeni13}.  This can be accomplished by repeatedly
simplifying an initial domain  in order to eliminate abstract
values which are \emph{responsible} for dependencies.  In order to
avoid dependencies on , we should have ,
i.e.,  should hold for any .  If this does not hold for some ,
then  is modified to obtain the atomicity of .

We design a simple algorithm  (Figure
\ref{edep}) which repeatedly checks if there exists  such
that .  Initially, the current state 
is  (remember that  is the abstract state
correctly describing invariant properties of variables at the current
program point ); then, it is progressively specialized to states
belonging to one of its -coverings, until one of the following
holds:
\begin{itemize}
\item[] ; in this case,  is
  precise enough to exclude dependencies on  in , and is
  not further modified;
\item[]  cannot be refined anymore (it is atomic on
  ) but  is non-atomic; in this
  case,  needs to be simplified in order to obtain
  .
\end{itemize}
States are processed by means of a queue; the algorithm stops when all
states have been consumed without any modification to , i.e.,
when no non-atomic  has been found.  As in ,
states are progressively restricted, and computations on
 are avoided if the desired property already
holds for .

\begin{figure}
  \begin{pseudocode}
    := ; // 
   repeat {
      := []; // 
     while (()) {
        := ();
       if () then {
         if () then { // 
           // 
            := ;
            := ;
           // 
            := [];
         } else { // 
           foreach() {
             (,)}}}}
   } until ();
   return ; // 
  \end{pseudocode}
  \caption{The  algorithm.}\label{edep}
\end{figure}

The simplifying operator  is a \emph{domain
  transformer}, and works by removing abstract values in order to
obtain .  Formally,


\noindent
The final  is an approximation of the most precise 
s.t.\  is false:

\begin{theorem}
  \label{theorem:nonDependency}
   makes  not narrow-dependent on
  .  In other words: the final  satisfies
  non-narrow-dependency of  on , that is, for every 
  which is atomic on ,
   is atomic.
\end{theorem}

\begin{proof}
  The algorithm halts if, in processing ,  is not
  changed.  Processing  involves computing
   on sub-states when required, in order to prove
  the atomicity property on every concrete state represented by
   (to this end, we exploit monotonicity of
   on states).  This is precisely obtained if every
  state is removed from the queue before any modification to 
  occurs.
\end{proof}

\noindent
On the practical side, the loss of precision in abstract computations
may lead to remove more abstract values than strictly necessary from
the semantic point of view.  It is important to note that 
works as long as  can be computed on the initial domain (in
this case, no problems arise in subsequent computations, since the
``complexity'' of  can only decrease).  This can possibly happen
even if  is infinite (see the end of Section
\ref{section:algorithmicIdeasForCheckingNdep}).  Moreover, unlike
, there is no reasonable trivial counterpart, since any
brute-force approach would be definitely impractical.

\COMMENT{
\subsection{Pruning CFG for computing slices}
\label{section:VsImpl}

This section describes an algorithmic approach to the computation of
\emph{abstract slices}.  The idea is to define a notion of abstract
state, which observes properties of selected program variables.  Such
states are used for analyzing how properties of the variables of
interest evolve.  In order to perform such an \emph{evolution}
analysis, we construct the \emph{abstract} state graph (), whose
vertices are abstract states, and which models program executions at
some level of abstraction. At this point, we propose a technique for
\emph{pruning} the  in order to remove \emph{all} the statements
which are relevant to the properties of interest.

Two different approaches are considered: \emph{simple} () and
\emph{extended} ().  The difference between them lies in the
definition of abstract states:  adds information about the
relationship between input variables and properties of variables of
interest.  Moreover, the approaches also differ in the way pruning is
performed.  In particular,  pruning consists of several
applications of  pruning (see Fig.~\ref{fig:Schema} for a
graphical representation of both approaches).  Note that  can be
only used to extract abstract static slices: since it does not
consider the relationship between variables of interest and input
variables, it is not precise enough in order to be applied to abstract
conditioned or dynamic slicing

\begin{figure}[tbp]
 \centering
  \includegraphics[scale=.7,viewport=3.5in 7.65in 6.5in 9.1in]{Structure.pdf}
  \caption{ and  \label{fig:Schema}}
\end{figure}
 
\COMMENT{We consider only programs written in a simple imperative
  programming language with assignments, sequential composition,
  conditional statements, and while statements. Let  be such a
  program and let us consider a criterion
  \small\normalsize,
  where  represents the last statement of \footnote{We suppose
    that original program has already been modified by a standard
    static slicer.}, and \small \normalsize is a notion
  introduced previously. Moreover, suppose that each property of
  interest is modeled as a \emph{finite} abstract domain (e.g.,
  \small\normalsize).

  Let us define the notion of \emph{abstract state}. An \emph{abstract
    state} describes abstract properties, \small\normalsize, of variables of interest after execution of a
  particular statement , and can be denoted as
  \small\normalsize. Given a concrete state\footnote{Here we do
    not consider occurrences of points, since they are not of interest
    for standard forms} , the abstraction
  function  associating  with the corresponding
  \emph{abstract state} is defined \small\normalsize,
  where  represents the line number of a statement that
  precedes a statement . Remember that concrete states contain
  line number of the next statement to be executed, and \emph{abstract
    states} require line number of the last executed statement. It is
  worth noting that the actual abstraction is performed on variables
  of interest since \emph{abstract states} contain their abstract
  values. 

  Let \small \normalsize be the set of abstract states. The
   of  for \small \normalsize is a directed graph
  \small\normalsize, where
  \small\normalsize, and for
  each pair V_\alpha, there is
  an edge from  to  if there exist concrete states  and 
  corresponding to abstract states  and  respectively (i.e.,
   and ), and if there exists a
  state trajectory  such that  and  are two adjacent
  states of .\\

  In order to simplify representation of vertices of , we
  consider the reduced product, ,
  of properties of tuples of interest as a unique property of
  variables of interest. For each line number, ,  contains
   vertices, and
  we can represent them as , where ,  represents a fix-point of . For each
  vertex, , we denote  and
  . We introduce a notion of \emph{block}, i.e., a
  set of connected vertices of  that has only one input and only
  one output edge. Let  be a block, then  and 
  denote its predecessor and successor respectively. Note that . Moreover, we introduce a notion of strongly
  connected component (SCC) of , that is a maximal set of its
  vertices, , such that for every pair of vertices 
  there is a path from  to  and a path from  to , i.e.,
  vertices  and  are reachable from each other.}

 pruning is illustrated by the method \CODE{pruningSA} given in
Fig.~\ref{fig:SAAlgo}, and its application is clarified by the
following example.  On the other hand,  pruning is characterized
by the method \CODE{pruningEA}, given in Fig.~\ref{fig:EAAlgo}, and
will be described only informally.

\begin{figure}[h]
  \hspace{1cm}
  \includegraphics[scale=.72,viewport=2.5in 5.35in 5.5in 8.7in]{SAAlgo1.pdf}
  \vspace{-2cm}
  \caption{The  approach \label{fig:SAAlgo}}
\end{figure}

\COMMENT{
  \paragraph*{\textbf{Simple approach pruning}}
  In the following we explain one possible way of \emph{pruning}
  , introduced in~\cite{NikoSAS10}. The idea is to traverse
  , to detect all its blocks and to remove the ones that are not
  relevant for the \emph{abstract} computation of interest. The
  remaining vertices determine the \emph{abstract slice}.\\
  \begin{figure}[h]
    \centering
    \includegraphics[scale=.72,viewport=2.5in 4.7in 5.5in 8.7in]{SAAlgo1.pdf}
    \caption{\label{fig:SAAlgo}}
  \end{figure}
  The \emph{simple approach pruning} is illustrated by the method
  \textbf{\texttt{pruningSA}} given in Fig.~\ref{fig:SAAlgo}. The
  invocation of \textbf{\texttt{findBlocks}} returns the set of all
  blocks of . If it does not provide any block,
  \textbf{\texttt{pruningSA}} returns the current . Otherwise,
  the blocks are analyzed in order to find out if it is possible to
  remove them. Let  be a block and suppose  and
  . It is possible to remove  if variables of
  interest have the same abstract values before and after execution of
  , i.e., if . It means that  does not change
  abstract values of variables of interest, so we invoke the method
  \textbf{\texttt{removeBlock}}, which removes it from , and we
  add the edge from  to . If , we
  cannot remove .\\

  The method \textbf{\texttt{findBlocks}}, which takes an  and
  returns all its blocks, requires some clarifications. It is worth
  noting that a block can be either a single vertex or a set of
  vertices corresponding to the instructions that form a loop. The
  latter one represents a cycle in the . In order to find all the
  blocks of  , we consider its SCCs, which can be both single
  vertices and cycles, and that can be determined by Tarjan's
  algorithm \cite{Tarjan}. Suppose that \small
  \normalsize are SCC of , determined by this algorithm. We
  construct the component graph \small\normalsize, where the set \small \normalsize contains a vertex \small
  \normalsize for each strongly connected component
  \small\normalsize, and there is an edge \small \normalsize if  contains a directed edge
   for some C_i
  and some C_j. We determine SCCs
  of G and construct \small \normalsize by invoking
  . For each
  \small\normalsize, we analyze
  \small \normalsize to understand if \small
  \normalsize has only one input and only one output edge, and in that
  case, its corresponding SCC, \small\normalsize, represents a
  block of  and is added to the resulting list.\\

  \COMMENT{
    The method \textbf{\texttt{findBlocks}} require some
    clarifications. Its invocation finds all the blocks of  that
    are connected (by the output edge) with one of  vertices that
    represent the last statement. The outer loop is iterated 
    times, in order to consider all these vertices. Let
     be on of them. We find all the vertices  such
    that , and for each of them we analyze its
    adjacent vertices. Consider the following iteration process:
    , and ,
    . For each ,
     contains all vertices (different from ) forming a
    path with source and destination in . Let us define sets
     and . For each vertex , there is a sequence of
    edges that takes  to  (traversing only vertices of ), and
    a sequence of edges that takes  do  (traversing only
    vertices of ), i.e., the vertices of  form cycles and each
    of these cycles contains .  contains all the edges of
     whose destination is in . The set  is a block if 
    has only one element (blocks have only one input edge), and if,
    for each vertex in , the set of all its reachable vertices is
    . Otherwise, there would be a vertex  such
    that: . Namely,  would have at
    least two output edges:  and , and it is not a property of
    blocks. If there is no  that satisfies this conditions, the
    empty set is returned.\\}
  The \emph{abstract slice} can be determined by removing from the
  starting program  all the statements which vertices do not appear
  in the pruned . Note that there can be some disconnected
  vertices, and they are not of interest. Let us illustrate this
  approach with an example.\\}
\COMMENT{In the following we explain the first way of \emph{pruning}
   in order to obtain \emph{abstract slices}. Let  be a
  block, i.e., set of connected vertices of , and suppose there
  are two vertices,  and , such that there is an
  edge from  to , and there is no other input edge of
  . Analogously, suppose there is an edge from  to ,
  and there is no other output edge of . The block  can be
  removed if variables of interest have the same abstract values
  before and after execution of , i.e., if . In that
  case, we add the edge from  to . If
  , we cannot remove , since its execution modifies
  abstract values of some variables of interest. We continue pruning
   this way until no further block of  can be
  removed. All the statements which vertices do not appear in the
  pruned  should be removed from the \emph{abstract slice}. It
  is worth noting that the blocks to be removed are detected by a
  \emph{bottom-up} strategy, i.e., we start searching for a block from
  the vertices of  that correspond to the last statement of 
  and we move towards the vertices that correspond to the first
  statement of . It is possible to have some disconnected vertices,
  and they are not of interest.\\}

\begin{example}
  \label{example:4}
  Consider  in Fig.~\ref{fig:Ex4}, and let
  .  In the \emph{abstract states} induced
  by ,  can have two abstract values, depending on its
  parity:  or .  Fig.~\ref{fig:Ex1} shows the  of
   for , whose vertices are identified by an
  overlined number given in the top left corner. Consider only edges
  represented by a solid line.
  \begin{figure}[ht]
    \centering
    \includegraphics[scale=.6,angle=90,viewport=2.35in 4.00in 4.35in 9in]{Ex1.pdf}
    \caption{An \label{fig:Ex1}}
  \end{figure}
  \COMMENT{A concrete value of  is modified by the statement  in
    every iteration, but it is obvious that this modification does not
    affect the parity of , since its concrete value increases of
    . Because of that fact, there is no edge from
     to , and from
     to .}

  Let us consider a block  composed of vertices
  ,  and
  . The only input and output edges of  are
  edges from  and towards 
  respectively. The vertices  and
   assign the same abstract value, ,
  to . It means that we can remove the block  and add an edge
  from  to . Analogously, it is
  possible to remove a block , composed of vertices
  ,  and ,
  and to add an edge from  to
  . In a similar way we can remove vertices
   and  and add the edges from
   to  and from
   to . These edges are
  represented by dotted lines in Fig.~\ref{fig:Ex1}. The remaining
  vertices, , ,
   and \footnote{Vertices
     and  represent the end of
    execution.}, represented in bold in Fig.~\ref{fig:Ex1}, correspond
  to the statements  and , which form the abstract slice
   given in Fig.~\ref{fig:Ex4}.
\end{example}

Abstract slices obtained this way can be larger than they are expected
to be. In the worst case, we do not remove any instruction from the
starting program .

In order to extract abstract conditioned slices\footnote{Recall that
  abstract dynamic slicing is a particular case of abstract
  conditioned slicing.}, we refine \emph{abstract states}.  Such a
refinement allows to construct an abstract state graph 
which contains more information and captures the relationship between
input and properties of variables.   pruning
(Fig.~\ref{fig:EAAlgo}) takes as input , and applies the method
\CODE{pruningSA} (Fig.~\ref{fig:SAAlgo}) to all subgraphs ,
, of .  Each application returns a pruned subgraph
, which allows constructing an abstract slice, called
\emph{partial abstract slice}, containing only statements of 
whose vertices appear in .

\begin{figure}[h]
 \centering
    \includegraphics[scale=.9,viewport=2in 7.9in 4in 8.8in]{EAAlgo1.pdf}
  \caption{ \label{fig:EAAlgo}}
\end{figure}
From the point of view of complexity, we can note that, once we have
the graph, the complexity of both algorithms is linear w.r.t.~the size
of the graph.  Actually, the hardest part is the construction of
s (as a matter of fact, the graph used in our examples are
computed manually).  In order to obtain complete automatization, we
should use some automatic tool for building s.  Yorsh
\etal~\cite{YorshBallSagiv} presented a method for static program
analysis that leverages tests and concrete program executions.  They
introduce \emph{state abstractions} which generalize the set of
program states obtained from concrete executions, and define a notion
of \emph{abstract graphs} which is similar to ours.  Furthermore, they
use a theorem prover to check that the generalized set of concrete
states covers all potential executions and satisfies additional safety
properties, and they use these results to construct an approximation
of their \emph{abstract graphs}.  The relation between
\cite{YorshBallSagiv} and our work should be further analyzed, in
order to use their method for an automatic construction of s.}



\section{The quest for Abstract Slices}
\label{sec:theQuestForAbstractSlices}
This section introduces an algorithm for computing conditioned
abstract slices, based on abstract dependencies and the notion of
\emph{agreement} between states.

As explained in Section \ref{sec:practicalIssuesAndOptimizations},
this way to compute slices relies on \emph{a priori} knowledge of the
properties which will be of interest for the analysis.  In most cases,
the majority of the abstract domains to be taken into account are
quite simple (e.g., nullity).  On the other hand, Section
\ref{sec:constructiveApproachesToAbstractProgramSlicing} presents a
general way to compute abstract dependencies on more complex domains.
Indeed, those algorithms can be used here, and their complexity is
acceptable if small domains are dealt with.

The slices obtained by following this methodology will have the
standard form of backward slicing; \emph{predicates} or
\emph{conditions} on states can be specified, in the style of
conditioned slicing \cite{Conditioned}.  In this case, for a predicate
, the judgment  means that the state
 satisfies .  A predicate  at a certain program
point may include either user-provided or statically inferred
information: for example, after a \CODE{x:=new C()} statement,
judgments like ``\CODE{x} is not null'', ``\CODE{x} is not cyclic'',
``\CODE{x} is not sharing with \CODE{y}'' could be provided depending
of the kind of static analyses available (nullity, sharing, cyclicity,
etc.).  A way to decide which statements have to be included in an
abstract slice consists of two main steps:
\begin{itemize}
\item for each statement , a specific static-analysis algorithm
  provides information about the relevant data \emph{after} that
  statement (below, the \emph{agreement}), according to the slicing
  criterion and the program code;
\item if the execution of  \emph{does not affect} its corresponding
  agreement (i.e., some condition on states which must hold after
  ), then  can be removed from the slice.
\end{itemize}

\begin{example}
  \label{ex:conditionsToSlice}
  Consider the following code fragment, and suppose that the slicing
  criterion is the nullity of \xx at the end:

  \vspace{-5mm}

  \begin{lstlisting}[firstnumber=21]
   ...
   y.f := ;
   x := y;
  \end{lstlisting}

  \noindent
  The field update on \yy (line 22) can be removed from the slice
  because
  \begin{itemize}
  \item the question about the nullity of \xx after line  is
    equivalent to the question about the nullity of \yy after line
    ; and
  \item the field update at line  does not affect the nullity of
    \yy.
  \end{itemize}
\end{example}

\noindent
The rest of this section formalizes how these two main steps are
carried out.

\subsection{The logic for propagating agreements}
\label{section:theLogicForPropagatingAgreements}

This section describes how agreements are defined and propagated via a
system of logical rules: the \GSYSTEM\footnote{A version of this
  system was introduced in previous work \cite{Zanardini} as the
  -system.  However, there are many differences between
  both systems, mainly due to the changes in the language under study
  (for example, variables are taken into account here instead of a
  more involved notion of \emph{pointer expression}).}.  Hoare-style
\emph{triples} \cite{Hoa69} are used for this purpose, in the spirit
of the \emph{weakest precondition calculus} \cite{Dij75}.

\begin{mydefinition}
  \label{def:agreement}
  An \emph{agreement}  is a set of conditions
   where each uco  involves a sequence of
  variables  (most usually, just one variable), and all
  conditions involve mutually disjoint sets of variables.  Two states
   and  are said to \emph{agree} on ,
  written , iff, for every
   in , , where notation is abused by taking
   as the sequence of values of variables 
  in , and  as the application of
   (which could be a relational domain) to the elements of such
  a sequence.

  Agreements are easily found to form a lattice, and a partial order
   can be defined:  iff, for
  every ,  such that
  , then .
  Moreover, an intersection operator is induced by the partial order:
   is the greatest agreement which is less
  than or equal to both.
\end{mydefinition}

In the following,  will be the uco corresponding to the
condition  in , or  if no condition
on  belongs to .  For the sake of simplicity, the
discussion will be limited to domains each involving one single
variable.  In this case, ordering amounts to the following:  if , where  is the
comparison on the \emph{precision} of abstract domains, meaning that
 is \emph{more precise} than .

\begin{example}
  \label{ex:agreement} Let 
  and 
  be two states.  Then, they agree on 
  since
  \begin{itemize}
  \item \nn has the same parity in both states;
  \item \xx is \CODE{null} (therefore, it has ``the same nullity'')
    in both states; and
  \item there is no condition on \ii.
  \end{itemize}
  On the other hand, these states do not agree on  because \ii is odd in  whereas it
  is even in .
\end{example}

In a triple , the pre-condition
 is the weakest agreement on two states before a statement
 such that the agreement specified by the post-condition 
holds after the statement.  Predicates on states can be used, so that
triples are, actually, 4-tuples which only take into account a subset
of the states.  Formally, the 4-tuple (or \emph{augmented triple})
 (where the  predicate
is often omitted) holds if, for every  and ,

\noindent
The rules of the \GSYSTEM are shown in Figure \ref{fig:gRules}.  The
\emph{transformed predicate}  is one which is guaranteed to
hold after a statement , given that  holds before, in the
style of \emph{strongest post-condition calculus} \cite{DS90,BN98}.
For example, if , then the condition  certainly holds after .  The way predicates are transformed
is outside the scope of this paper; however, the cited works
introduced calculi for computing such strongest post-conditions.  In
the absence of such tools,  is always a consistent choice
(precision, but not soundness, may be affected since the set of states
to be considered grows larger, and to prove useful results could
become harder).

The \GSYSTEM is tightly related to narrow non-interference
\cite{GM04popl,GM04CSL}.  These works define a similar system of
rules, the -rules, for assertions
, where  and  are basically the
(tuples of) abstract domains corresponding to, resp.,  and
.  The systems differ in that:
\begin{itemize}
\item the use of pointers requires the rules for assignment to account
  for sharing, while -rules only work on integers;
\item in the present approach, domains are supposed to be
  partitioning, so that there is no need to include explicitly the
   operator (Section \ref{Sect:partit});
\item the \GSYSTEM~does not distinguish between \emph{public} and
  \emph{private} since this notion is not relevant in slicing;
\item the rule for conditional is not included in the
  -system; indeed, this is quite a tricky rule, and, in
  general, expressing a conditional with loops and using the rule
   for loops results in inferring less precise
  assertions;
\item in the -system, predicates  on program
  states are not supported.
\end{itemize}

In the following, each rule of the \GSYSTEM is discussed.
Importantly, this rule system relies on the computation of
\emph{property preservation}.  We rely on a rule system which soundly
computes whether executing a statement affects properties of some
variables: the judgment

\centerline{}
\noindent can only obtained by using those rules if it is possible to
prove that executing  in a state  satisfying the condition
 results in a final state  that is equal to the
initial  with respect to the agreement , i.e.,
.  In other word, the statement is equivalent
to \CODE{skip} with respect to the properties of interest.  Note that,
here, the agreement is not used to compare two states at the same
program point; rather, it takes as input the states before and after
executing a statement.  The rule system for proving property
preservation is explained after introducing the \GSYSTEM (Section
\ref{sec:thePSystem}).

\begin{figure}
  \begin{center}
      
      
      
      
      
      
      
      
  \end{center}
  \caption{The \GSYSTEM}
  \label{fig:gRules}
\end{figure}



\subsubsection{Rule }

This rule makes the relation between property preservation and the
\GSYSTEM more clear.  The triple 
amounts to say that two executions agree \emph{after} , provided
they agree \emph{before} on the same .  On the other hand,
the preservation of  on  means that any state
\emph{before}  agrees on  with the corresponding state
\emph{after} .  Property preservation is a stronger requirement
than the mere propagation  of
agreements, so that this rule is sound.  In fact, if
 and both
 and
 hold, then
 follows,
which is, equivalent, by definition, to
.

\begin{example}
  Let the parity of  be the property of interest.  In this case,
  \IMPASSIGN{x}{x+1} does not preserve the parity of , but two
  initial states agreeing on  lead to final states
  which still agree on it.  Therefore,
  
  holds.  On the other hand, \IMPASSIGN{x}{x+2} also satisfies a
  stronger requirement: that  does not change.  Therefore,
  besides having
  ,
  the judgment
   is also
  true.
\end{example}

\subsubsection{Rules , ,
  , }

The  rule describes \emph{no-op}.  The assertion
holds for every  and  since
.

 is also easy: soundness holds by transitivity
(note also the use of  to propagate conditions of states).

Rule  can be used when nothing else can be proved: it
always holds because execution is deterministic, so that two execution
starting from two states which are equal\footnote{The notion of
  equality on references and objects is recalled in Example
  \ref{ex:referenceAbstractDomains}.} on all variables will end in a
pair of states agreeing on any abstraction.  Note that, as pointed out
in Section \ref{sec:theProgrammingLanguage}, \CODE{read} statements
are supposed only to appear at the beginning of a program; therefore,

Finally, in , remember that  is the
partial order on agreements.

\subsubsection{Rule }

This rule means that, given a statement , any agreement  which satisfies the two
conditions of the above part of the rule is a sound pre-condition for
the post-condition .  The conditions are (1) that, given two
states which agree on , the computed results for the
expression  in both states are abstracted by  to the
same abstract value; and (2) that  is as precise as
 on all variables but .  The first condition is
represented in terms of Definition
\ref{def:atomicAbstractDependencies}, and the superscript 
indicates that only states satisfying  have to be considered.
Such a condition can be easily shown to imply the formula
 Note that, by Proposition~\ref{theorem:equivalenza}, the absence of abstract dependencies
w.r.t.~Definition \ref{def:atomicAbstractDependencies}
(\emph{Atom-dep}) implies the absence of abstract dependencies
w.r.t.~Definition \ref{def:abstractDependencies} (\emph{Ndep}) which,
in turn, implies .  This clarifies the relation between abstract
dependencies and the computation of a slice.  Obviously, the second
condition guarantees that, for all variables which are not updated by
the assignment, the agreement required by  still holds when
 is considered.

\subsubsection{Rule }

This rule accounts for the modification of the data structure pointed
to by a variable by means of a field update.  In the following, that a
variable is \emph{affected} means that the data structure pointed to
by it is updated.  Given a field update on a variable , some other
variables (i.e., the data structures pointed to by them) could be
affected.  There exists a well-known static analysis which tries to
detect which variables point to a data structure which is updated by a
field update on : this analysis is known as \emph{sharing analysis}
\cite{DBLP:conf/sas/SecciS05,ZanardiniG15sh}, and usually comes as
\emph{possible-sharing} analysis, where the set of variables which
\emph{could} be affected by a field update is over-approximated.
Moreover, \emph{aliasing analysis} \cite{Hind2001} can be used in
order to compute the set of variables pointing exactly (and directly)
to the same location as ; in this case, \emph{definite-aliasing}
analysis makes sense, which under-approximates the set of variables
which
\emph{certainly} alias with .  According to the result of these
analyses, reference variables can be partitioned in three categories:
(1) variables which certainly alias with , so that they can be
guaranteed to be updated in their field ; (2) variables which could
share with , so that they could be affected by the update in many
ways; and (3) variables which certainly do not share with , so that
they are unaffected by the update.  Let  be the set of
variables possibly sharing with  before the update, and
 be the set of variables definitely aliasing with .  In
the absence of a definite-aliasing analysis, then  can be
safely taken as .

\begin{example}
  \label{ex:sharingFlavours}
  Consider the following code fragment:
  {\em \begin{lstlisting}[firstnumber=10]
   if (...) then { y.f := x; } else { y.f := z; }
   w := x;
   x.g := ;
  \end{lstlisting}}

  \noindent Suppose that, initially, no variable is sharing with any
  other variable (i.e., there is no overlapping between data
  structures referred by different variables), and that the truth
  value of the boolean guard cannot be determined statically, so that
  both branches of the conditional statement have to be considered as
  possible executions.  In this case, the sharing and aliasing
  information before line 12 is as follows:  Note that every
  variable is aliasing with itself (none of them is \CODE{null}), \ww
  is certainly aliasing with \xx because of line 11, and \yy is
  possibly sharing with \xx (the actual sharing depends on the value
  of the guard).
\end{example}

The  rule comes with three pre-conditions.
Pre-condition  only applies to variables in category (1): those
definitely aliasing with .  Pre-condition  applies to
category (2), while pre-condition  applies to categories
(2) and (3).

\begin{itemize}
\item  requires that updating the field  of the location
  pointed to by  (and all variables definitely aliasing with it)
  leads to an agreement on , provided that the initial
  states agree on .
\item  is similar, but states that the agreements must hold
  for \emph{every} sequence of field selectors \footnote{In
    the following, the notation  (starting
    with a dot is intentional) will be used to represents sequences of
    field selectors.}, possibly the empty sequence.  This is needed
  since, given that some  may share with , it cannot be known
  which fields of  will be updated, and how.  In practice, only the
  sequences of field selectors which are compatible with the class
  hierarchy of the program under study have to be considered, as shown
  in Example \ref{ex:unfeasibleFieldSequences}.
\item  applies to variables that could be unaffected by
  the update, i.e., variables in categories (2) and (3) (note that the
  conditions  in  and 
  in  are not mutually exclusive, so that variables in
  category (2) satisfy both).  The relation between  and
   is clear in this case, as agreement on  must
  entail agreement on .
\end{itemize}

\begin{example}[Infeasible sequences of field selectors]
  \label{ex:unfeasibleFieldSequences}
  In a Java-like language, a sequence like
   is not compatible with the
  following class hierarchy since the class of \CODE{g} is \CODE{D}
  whereas \CODE{f} is declared in \CODE{C}:
  \begin{center}
    \begin{lstlisting}[numbers=none]
   class C { D f; D g; }        class D { D h; }
    \end{lstlisting}
  \end{center}
\end{example}

\begin{example}
  \label{ex:fassignNullity}
  Consider the statement .  Let  and the agreement  after  be .  Let
  also  before  be .  In this
  case, an agreement  which satisfies the judgment
  
  can be the same 
  because
  \begin{itemize}
  \item \zz is unaffected by the update (it belongs to category (3)),
    so that  must be at least as precise as
    ;
  \item category (2) contains no variables; and
  \item category (1) only includes \xx itself, and the nullity of \xx
    is clearly unaffected by the update.
  \end{itemize}
\end{example}

One may think that the universal quantification on field sequences in
pre-condition  results in an unacceptable loss of precision.
Indeed, to require that all possible updates to possibly-sharing
variables preserve the desired agreements seems to be too strict.
However, there are a number of things to be considered:
\begin{itemize}
\item A closer look to the rule shows that there is no easier way to
  account for sharing if traditional sharing analysis is used.
\item Example \ref{ex:fassignNullity} shows that it is still possible
  to get meaningful results on domains working on pointer variables.
\item The state of the art in static analysis of object-oriented
  languages indicates that abstract domains on pointers are likely to
  be quite simple ( being one of them).
\item There is recent work \cite{ZanardiniG15sh} introducing a more
  precise, \emph{field-sensitive} sharing analysis which computes
  \emph{how} variables share: this analysis is able to detect which
  \emph{fields} are or are not involved in paths in the heap
  converging from two variables to a shared location.  In order to
  keep the discussion as simple as possible, the definition of
   given in Figure \ref{fig:gRules} uses
  traditional sharing analysis.  However, the impact of
  field-sensitive sharing analysis is discussed in Section
  \ref{sec:practicalIssuesAndOptimizations}, where a refined version
  of  is given.
\end{itemize}

The domain of cyclicity introduced in Section
\ref{ex:referenceAbstractDomains} represents information about
\emph{data structures} in the heap, not only program variables.  In
this sense, field updates have to be regarded as potentially affecting
the propagation of agreements.

\begin{example}
  \label{ex:cyclicityDomain} Let  be, again, the
  statement \CODE{x.f:=y}, and  be ; i.e., the interest is on the cyclicity of the data structure
  pointed to by  after executing .  Suppose also that  and
   are certainly not sharing before , and that an object whose
  type is compatible with  has two reference fields  and .
  Then, the agreement  is \emph{not} a correct
  precondition for the Hoare tuple to hold, since there can be two
  states  and  (Figure \ref{fig:cyclicity}) such
  that \begin{itemize} \item the data structure corresponding to 
    is acyclic, and equal in both states (therefore, there is an
    agreement on the cyclicity of ); \item the data structure
    corresponding to  is cyclic in both  and ,
    but (a) in  there is only a cycle originating from the
    location bound to ; and (b) in  there is a cycle
    originating from  and another one originating from
    .  \end{itemize} In this case, 
  holds but the resulting final states  and  do
  \emph{not} agree on  since  is acyclic in 
  (the only cycle has been broken) while it is still cyclic in
  .  This behavior is captured by 
  because condition  applied to  itself (which, by
  hypothesis, is the only variable in ) does not hold, so
  that the augmented triple cannot be proven.  On the other hand,  would be a correct
  precondition.

  \begin{figure}
    \begin{center}
      \begin{tikzpicture}
        \node (yv1) at (-2,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (y1) at (-2,-1) {};
        \draw[dotted,->] (yv1) -- (y1);
        \node (xv1) at (0,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (x1) at (0,-1) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (xf1) at (-0.5,-2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (xg1) at
        (0.5,-2) {};
        \draw[dotted,->] (xv1) -- (x1);
        \draw[->] (x1) -- node[left] {} (xf1);
        \draw[->] (x1) -- node[right] {} (xg1);
        \draw[->,dashed] (xf1) .. controls (-1.5,-3.5) and (0.5,-3.5)
        .. (xf1);
        \node at (-1.35,-2.7) {\emph{\scriptsize{(cycle)}}};
        \draw (-2.5,0) rectangle (1,-3.3);
        \node at (1.3,-3.1) {};

        \draw[->,thick] (1.3,-1.65) -- node[above] {\CODE{x.f:=y}} (4.3,-1.65);

        \node (pyv1) at (5,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (py1) at (5,-1) {};
        \draw[dotted,->] (pyv1) -- (py1);
        \node (pxv1) at (7,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (px1) at (7,-1) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (pxg1) at
        (7.5,-2) {};
        \draw[dotted,->] (pxv1) -- (px1);
        \draw[->] (px1) -- node[above] {} (py1);
        \draw[->] (px1) -- node[right] {} (pxg1);
        \draw (4.5,0) rectangle (8,-3.3);
        \node at (8.3,-3.1) {};
      \end{tikzpicture}

      \begin{tikzpicture}
        \node (yv1) at (-2,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (y1) at (-2,-1) {};
        \draw[dotted,->] (yv1) -- (y1);
        \node (xv1) at (0,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (x1) at (0,-1) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (xf1) at (-0.5,-2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (xg1) at
        (0.5,-2) {};
        \draw[dotted,->] (xv1) -- (x1);
        \draw[->] (x1) -- node[left] {} (xf1);
        \draw[->] (x1) -- node[right] {} (xg1);
        \draw[->,dashed] (xf1) .. controls (-1.5,-3.5) and (0.5,-3.5) .. (xf1);
        \draw[->,dashed] (xg1) .. controls (-0.5,-3.5) and (1.5,-3.5) .. (xg1);
        \node at (-1.4,-2.7) {\emph{\scriptsize{(cycles)}}};
        \draw (-2.5,0) rectangle (1,-3.3);
        \node at (1.3,-3.1) {};

        \draw[->,thick] (1.3,-1.65) -- node[above] {\CODE{x.f:=y}} (4.3,-1.65);

        \node (pyv1) at (5,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (py1) at (5,-1) {};
        \draw[dotted,->] (pyv1) -- (py1);
        \node (pxv1) at (7,-0.2) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (px1) at (7,-1) {};
        \node[draw,minimum height=0.4cm,minimum width=0.8cm] (pxg1) at
        (7.5,-2) {};
        \draw[dotted,->] (pxv1) -- (px1);
        \draw[->] (px1) -- node[above] {} (py1);
        \draw[->] (px1) -- node[right] {} (pxg1);
        \draw[->,dashed] (pxg1) .. controls (6.5,-3.5) and (8.5,-3.5) .. (pxg1);
        \draw (4.5,0) rectangle (8,-3.3);
        \node at (8.3,-3.1) {};
      \end{tikzpicture}
    \end{center}
    \caption{How two executions agreeing on  do not agree on  after \CODE{x.f:=y}}
    \label{fig:cyclicity}
  \end{figure}
\end{example}

\subsubsection{Rules  and }

In a conditional
\CODE{if ()}  \CODE{else} 
there are two possibilities.  Rule  states that an
input agreement which induces the output one whichever path is taken
is a sound precondition.  Here, the assertion
 means that

\noindent
where the judgment  means that all
four states agree on .  This rule requires  to
hold on the output state independently from the value of .
Soundness is easy (note that the above assertion implies
 and
).

Note that such a  can always be found (in the worst case, it
assigns the identity upper closure operator  to each variable,
so that two states agree only if they are exactly equal).  However,
sometimes it can be more convenient to exploit information about .
In such cases,  can be applied, which means that the
initial agreement  is strong enough to
verify the final one, provided the same branch is taken in both
executions, as  requires.  In fact,  is built
from , and separates states according to its value:

\noindent
The rule means that, whenever two states agree on the branch to be
executed, and the triples on both branches hold, the whole triple
holds as well.

\begin{lemma}[soundness of ]
  \label{lemma:aIfPPSoundness} If  and  both
  satisfy  and agree on
  , then the corresponding
  output states  and  agree on  under
  the hypotheses of the rule.
\end{lemma}

\begin{proof}
  By hypothesis, the same branch is taken in both cases.  Conditions
   and  are consistent since 
  (respectively, ) can only be executed when  is true
  (respectively, false).  The agreement on  holds in both
  paths, so that the entire assertion is correct.
\end{proof}

Note that the rule to be chosen for the conditional depends on the
precision of the outcome:  can be a good choice if
(1) it can be applied; and (2) the result is ``better'' than the one
obtained by .  The second condition amounts to say
that, given the same final agreement , the initial agreement
obtained by using  is weaker (i.e., it is more likely
that two states agree on it) than the one obtained by using
.

\begin{example}
  \label{ex:ifRules}
  Consider the code fragment

  \vspace{-5mm}

  \begin{lstlisting}[numbers=none]
   if (x>0) { x:=x+1; } else { x:=x-1; }
  \end{lstlisting}
  and let  be the agreement after
  the statement, i.e., the relevant property is the sign of \xx.  The
  rule  is able to compute the same  as the
  input agreement because
  \begin{itemize}
  \item the triple 
    holds since the condition  guarantees that \xx is
    positive, and two states which agree on the sign before the
    increment will still agree after it (if \xx is positive in both
    states, then it will remain positive in both);
  \item similarly, the triple  also holds (if \xx is 0 in both states, then it
    will be negative in both; and if it is negative in both, it will
    remain negative in both);
  \item  is less precise than  (i.e.,  since the latter only separates numbers
    into positive and non-positive), so that the input agreement
     is equal to .
  \end{itemize}
  On the other hand,  is not able to compute the same
  input agreement because the precondition
   of the rule
  does not hold.  In fact, consider two states  and : they
  agree on the sign of \xx, but
   and
   have different sign (the
  first is zero while the second is positive).
\end{example}

\subsubsection{Rule }

The meaning of the rule for loops can be understood by discussing its
soundness: if  is preserved after any iteration of the body,
and the agreement which is preserved by the body guarantees the same
number of iterations in both executions (i.e., it is more precise than
), then such an agreement is preserved through the entire
loop.

\begin{lemma}[soundness of ]
  \label{lemma:aWhileSoundness}
  Let  and  satisfy , and agree on
  .  Then, given bs_w, the result
   holds.
\end{lemma}

\begin{proof}
  Let .  There are two
  cases:
  \begin{itemize}
  \item : in this case, the body is not executed, and the result
    holds trivially;
  \item : in this case, .  By the
    hypothesis of the rule,  and  agree on
    , and  still holds since .
  \end{itemize}
  At every iteration, the hypotheses hold.  Moreover 
  guarantees the same number of iterations in both executions.
  Consequently, for a terminating loop (non-termination is not
  considered),  and  will fall in the first
  case (false guard) after the same number  of iterations.  These
  states are exactly  and , and agree on
   after the loop.
\end{proof}

\begin{theorem}[\GSOUNDNESS]
  \label{theorem:aSoundness}
  Let  be a statement,  be required after ,  be
  a predicate and  be the program point before .  Let also
   be an agreement computed before  by means of the
  \GSYSTEM.  Let  and  be two trajectories, and the
  states  and  satisfy
   and .  Then, the condition
   holds, where .
\end{theorem}

\begin{proof}
  Easy from Lemmas \ref{lemma:aIfPPSoundness} and
  \ref{lemma:aWhileSoundness}, and the discussion explaining each rule
  (especially, ).
\end{proof}

\subsubsection{The \PSYSTEM}
\label{sec:thePSystem}

\begin{figure}
  \begin{center}
    

       
       
          
  \end{center}
  \caption{The \PSYSTEM}
  \label{fig:thePSystem}
\end{figure}


Property preservation can be proved by means of a rule system, the
\PSYSTEM (Figure \ref{fig:thePSystem}).  Most rules are
straightforward or very similar to \GSYSTEM rules, and characterize
when executing a certain statement preserves the properties
represented by an agreement  (i.e., for every variable ,
the property/uco  is preserved).

For example, rule  allows proving that a certain
agreement is preserved when the initial value of  cannot be
distinguished from the value of the expression (i.e., the new value of
), when it comes to the property .  In rule
, a mechanism similar to  is
used: definite aliasing and possible sharing can be used to identify
which variables are affected by the field update.  As for
, an optimization based on field-sensitive
sharing analysis (Section \ref{sec:useOfFieldSensitiveSharing}) can be
introduced, which makes it easier to prove property preservation on
field updates.

Indeed, a number of optimizations can be applied to the \PSYSTEM (for
example, one could think that property preservation on
 does not require the preservation of the same
properties on both statements separately).  However, this rule system
is not the central part of this paper, and Figure \ref{fig:thePSystem}
is just a sensible way to infer property preservation.

\subsection{Agreements and slicing criteria}
\label{section:agreementsAndSlicingCriteria}

This approach to compute abstract slices follows the standard
conditioned, non-iteration-count form of backward slicing.  Therefore,
a slicing criterion  takes the form
 where  is the last
program point, and  is a sequence of ucos assigning a property to
each variable in .  The initial agreement can be easily computed
from the criterion and is such that , where  is the element of
 corresponding to .  This shows that there is a close relation
between this specific kind of slicing criteria and agreements, and in
the following, these concepts will be used somehow interchangeably in
informal parts.  It will be shown that this makes sense, i.e.,
criteria and agreements define tightly related notions.  Next
definition defines the correctness of an abstract slice of this kind,
where the slicing criterion is intentionally confused with an
agreement.

\begin{mydefinition}[Abstract slicing condition]
  \label{def:abstractSlicingCondition}
  Let  be the slice of  with respect to an agreement
  (criterion) .  In order for  to be correct,
   and  must
  agree on  for every initial :
  .
\end{mydefinition}

\subsection{Erasing statements}
\label{sec:erasingStatements}

The main purpose of the \GSYSTEM is to propagate a final agreement
backwards through the program code, in order to have a specific
agreement attached to each statement\footnote{Here, a statement is not
  only a piece of code, but also a position (program point) in the
  program, so that no two statements are equal, even if they are
  syntactically identical.  For the sake of readability, the program
  point is left implicit.}.  This is done as follows: a program can be
seen as a sequence  of  statements,
where each  can be either a simple statement (skip, assignment,
field update) or a compound one (conditional or loop), containing one
(the loop body) or two (the branches of the conditional) sequences of
statements (either simple or compound, recursively).  The way to
derive an agreement for every statement in the program is depicted in
the pseudocode of Figure \ref{fig:agreementPropagation}.  The
procedure \CODE{labelSequence} takes as input
\begin{enumerate}
\item a sequence of statements (in the first call, it is the whole
  program code\footnote{Strictly speaking, the sequence of statements
    is the program without the initial sequence of \CODE{read}
    statements (remember that \CODE{read} statements are basically
    meant to provide the input).};
\item a pair of agreements: (2.a) the first one, , refers
  to the beginning of the sequence, and, in the first call, is such
  that the abstraction on each variable is ; and (2.b) the
  second one, , is the desired final agreement, which
  corresponds to the slicing criterion as discussed in Section
  \ref{section:agreementsAndSlicingCriteria}; and
\item a predicate on states which is supposed to hold at the beginning
  of the sequence.
\end{enumerate}
\CODE{labelSequence} goes backward through the program code inferring,
for each , an agreement  which corresponds to the
program point \emph{after} .   will be the same
, whereas, for each ,  will be
inferred by using the \GSYSTEM: more specifically, it is a (ideally,
the best) precondition such that the tuple
 holds.  Note
that, since the initial  is the identity on all variables,
 trivially
holds for every  (execution is deterministic); however, the
 argument plays an important role when dealing with loop
statements.

Importantly, statements inside compound statements (e.g., assignments
contained in the branch of a conditional) are also labeled with
agreements.  This is done by calling \CODE{labelSequence} recursively.
Note that, in this case, if  and 
are. respectively, the sequences corresponding to the ``then'' and
``else'' branch of a conditional statement , then
\CODE{labelSequence} is called with second argument
; this is so because the state does not
change when control goes from the end of a branch to the statement
immediately after .

The treatment of loops follows closely the definition of
.  In the augmented triple,  appears
before and after the statement; this is consistent with the rule.  The
condition 
guarantees that the
 can be proven
by applying .  Moreover, the recursive call on the
body  has  as its
second argument.

\begin{figure}
  \begin{pseudocode}
    procedure labelSequence(`', , ) {
       = ;
       = ;
      every  is  where ';
      // (remember the transformed predicate )
      for  =  downto  {
        if ( is a conditional) {
          let  be the guard;
          let  and  be its branches;  
          call labelSequence(, , );
          call labelSequence(, , );
           is such that ;
        } else if ( is a loop) {
           is an agreement such that
          -  // (see rule )
          - 
          let  be the guard;
          let  be the loop body;
          call labelSequence(, , );
        } else { // non-compound statement
           is such that ;
        }
      }
    }
  \end{pseudocode}
  \caption{Labeling program code with agreements by using the
    \GSYSTEM}
  \label{fig:agreementPropagation}
\end{figure}
  
\COMMENT{ By applying \CODE{labelSequence} to a program , an
  agreement  is attached to every statement  in .
  By construction, the judgment

  \centerline{}
  holds, where
  \begin{itemize}
  \item  is the final agreement (second parameter of the
    initial call to \CODE{labelSequence});
  \item  is the predicate computed by \CODE{labelSequence} for
    the program point after  (note that the procedure performs this
    kind of computation at line 3);
  \item  is the code following  according to
    Definition \ref{def:sRest}.
  \end{itemize}
  
  \begin{mydefinition}
    \label{def:sRest}
    Given a program  and a statement  contained in it, the
    \emph{rest} of  w.r.t.~ is defined as follows (the second
    argument is in boldface for better readability):
    
  \end{mydefinition}
  
  \begin{proposition}
    \label{prop:erasureSoundness}
    Any two executions agreeing on  after  will finally
    agree on .
  \end{proposition}
  
  \begin{proof}
    The result is easy by construction, the only issue being statements
    contained in loop bodies.  In this case, suppose the loop is a
    statement  of the sequence corresponding to a program 
    (nested loops can be dealt with by structural induction).  Due to
    the way  is computed, it is guaranteed that both
    executions  and  will still enter the same
    number (zero or more) of loop iterations, and the agreement
     holds after each of them.  Therefore, the final
    agreement on  follows easily from the agreement on
     after the loop.
  \end{proof}
  
  \begin{proposition}
    \label{prop:erasureSoundness1}
    Given an agreement  and a program , the tuple
    
    \centerline{}
    \noindent holds for every statement  in , where 
    and  are computed by using \CODE{labelSequence}.
  \end{proposition}
  
  \begin{proof}
    This proof by structural induction relies on the soundness of the
    \GSYSTEM (Theorem \ref{theorem:aSoundness}).  There are three cases
    to be considered: (1) when  is exactly one of the ; (2) when
    it is contained in a branch of a conditional statement; or (3) when
    it is contained in a loop body.
    
    Case (1): the augmented triple
     easily
    holds as it is obtained by the repeated use of the rule
    .
    
    Case (2): suppose  is contained in the ``then'' branch
     of a conditional statement  (the dual case of
    the ``else'' branch is similar).  By inductive hypothesis,
    
    holds, where  is the agreement computed by
    \CODE{labelSequence} for the program point immediately after .
    Also, 
    holds as in case (1).  Then, since  is
    the concatenation of  and
    , the result holds for transitivity.
    
    Case (3): if  is contained in the body  of a loop
    , then
    
    holds by inductive hypothesis, where, according to
    \CODE{labelSequence},  is an agreement satisfying the
    condition .  By
    using the rule ,
    
    also holds, and the rest follows similarly to case (2).
  \end{proof}
  
  \begin{corollary}
    \label{prop:erasureSoundnessCorollary}
    (Under the same hypotheses as Proposition
    \ref{prop:erasureSoundness}) Every time two executions agree on
     after , they finally agree on .
  \end{corollary}
  
  This result, which may seem far from obvious when  is in a loop
  body, comes actually from the way  is chosen at lines
  14-16: the second condition at line 16 ensures that it makes no
  difference if the execution will exit the loop immediately or enter
  another iteration.
}

Now, suppose that the judgment  can
be proved, where  and  are computed by
\CODE{labelSequence}, and  refers to the program point before
.  In this case, let  be the program  where  has
been replaced by , and  be an initial state.
Then, the following holds:  agrees with
 with respect to , provided
that executions terminate (non-termination is not considered).

\begin{proposition}
  \label{prop:soundnessPreservation}
  Given a statement  in  such that
  , the output states obtained by
  executing both  and  on some  agree on the
  agreement  corresponding to the desired slicing
  criterion if the execution terminates.
\end{proposition}

\begin{proof}
  Let  and  be two trajectories coming from
  executing, respectively,  and  from the initial state
  .  Let  and  be the states
  of  and , respectively, when control reaches the
  program point before  (or \CODE{skip}, in the case of )
  for the -th time, and  and  be
  their corresponding states after  (or \CODE{skip}).  If  is
  not contained in any loop, then  can only be , and the proof
  is trivial.  Otherwise, it can be any number up to some  (a
  non-negative number).

  If , then the loop is never executed on the input ,
  and the proof follows trivially.

  Otherwise,  and  are identical
  because both executions went exactly through the same statements; as
  a consequence, they certainly agree on .  On the other
  hand,  and  are in general not
  identical, but they still agree on : in fact, the
  following holds:
  
  Due to how the program is labeled with agreements,
   implies an agreement
  of both executions at the end of the loop body with respect to the
  agreement  labeling that program point.  This also
  means that both executions will still agree on  at
  the beginning of the next iteration, and (again, by construction)
  they will also agree on  when  is reached for the
  second time.  This mechanism can be repeated until  is reached,
  and it is easy to realize that the agreement on  at the
  last iteration implies the final agreement on  (see
  Figure \ref{fig:soundnessPreservation}).

  The crux of this reasoning is that, by construction, the agreements
  labeling each program point imply that, when a statement can be
  removed from the loop body, this means that the original program and
  the slice will execute the loop body the same number of times.  In
  general, this does not mean that every loop will be executed the
  same number of times (some property-preserving loops could be even
  sliced out completely).
\end{proof}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \node (pi) at (0,0) {};
      \node (pip) at (4,0) {};
      \node (lb) at (-5.5,-3) {body of loop };
      \draw (-3.5,-1) -- (-3.7,-1) -- (-3.7,-5) -- (-3.5,-5);
      \node (begin) at (0,-1) {};
      \node (beginp) at (4,-1) {};

      \draw[dotted,->] (pi) -- (begin);
      \draw[dotted,->] (pip) -- (beginp);

      \draw[<->] (begin) -- node[above] { (first time),
        } (beginp);

      \node (end) at (0,-5) {};
      \node (endp) at (4,-5) {};

      \node (s0) at (0,-2.7) {};
      \node (s) at (0,-3) {s};
      \node (s1) at (0,-3.3) {};
      \draw[dotted,->] (begin) -- (s0);
      \draw[dotted,->] (s1) -- (end);
       
      \node (skip0) at (4,-2.7) {};
      \node (skip) at (4,-3) {\CODE{skip}};
      \node (skip1) at (4,-3.3) {};
      \draw[dotted,->] (beginp) -- (skip0);
      \draw[dotted,->] (skip1) -- (endp);
      
      \draw[<->] (s0) -- node[above] {} (skip0);
      \draw[<->] (s1) -- node[below] {} (skip1);
      \draw[<->] (end) -- node[above] {} (endp);

      \draw[<->] (s0) .. controls (-0.7,-2.5) and (-0.7,-3.5) .. node[left]
           {} (s1);
      \draw[<->] (skip0) .. controls (4.7,-2.5) and (4.7,-3.5) .. node[right]
           {, } (skip1);
      
      \draw[dotted,->] (end) .. controls (-2.8,-4.5) and (-2.8,-1.5)
      .. (begin);
      \draw[dotted,->] (endp) .. controls (6.8,-4.5) and (6.8,-1.5) .. (beginp);
    \end{tikzpicture}
  \end{center}
  \caption{Graphical representation of some aspects of Proposition
    \ref{prop:soundnessPreservation}}
  \label{fig:soundnessPreservation}
\end{figure}

This proposition can be used in order to remove all statements for
which such a property-preservation judgment can be proved.  In
general, the slice is computed by replacing all statements  such
that  holds by , or,
equivalently removing all of them from the original code.  It is easy
to observe that this corresponds exactly to the notion of backward
abstract slicing given in Section
\ref{section:AbstractProgramSlicing}.

\begin{example}
  \label{ex:conditionsToSlice}
  Consider the following code, and let the final nullity of \xx be the
  property of interest (corresponding to the agreement  after line 15): {\em
        \begin{lstlisting}[firstnumber=8]
   ...
   n := n*2;       // 
   C x := new C(); // 
   if (n=0) {      //  (final result on this branch always null)
     x := null;    // 
   } else {        //  (final result on this branch never null)
     x := new C(); // 
   }               // 
      \end{lstlisting}
      }
  \noindent
  Each agreement on the right-hand side is the label after the
  statement at the same line.  Both lines  and  can be removed
  from the slice because:
  \begin{itemize}
  \item the final nullity of \xx only depends on the equality of \nn
    to  before line 11, which is captured by the  domain;
  \item line 10 does not affect \nn; and
  \item multiplying a number by 2 preserves the property of being
    equal to .
  \end{itemize}
  Note that, although agreements after lines 11 and 13 are both empty
  (no matters how the state is  will be null after line 12 and
  non-null after line 14), the one after line 10 is not because of
  rule .
\end{example}

\begin{example}
  Consider again the code of Example \ref{ex:abstractSlicing}.
  Starting from the final agreement , the code is annotated as follows:

  {\em
    \begin{lstlisting}[firstnumber=33]
                        // 
   y := null;           // 
   x := list;           // 
   while (pos>0) {      // 
     y := x;            // 
     x := x.next;       // 
     pos := pos-1;
   }                    // 
   z := new Node(elem); // 
   z.next := x;         // 
   if (y = null) {      // 
     list := z;         // 
   } else {             // 
     y.next = z;        // 
   }                    // 
    \end{lstlisting}
  }

  \noindent To prove the necessary tuples, it is important to note
  that \CODE{next} is the only reference field selector in the class
  \CODE{Node}, so that any cycle has to traverse it.  Moreover, to
  prove that the cyclicity of \CODE{list} after line 46 is equivalent
  to the cyclicity of \CODE{list} and \CODE{z} needs non trivial
  reasoning about data structures; concretely, it is necessary to have
  some \emph{reachability analysis} \cite{tcs13}\footnote{Note that
    pair-sharing-based cyclicity analysis
    \cite{DBLP:conf/vmcai/RossignoliS06} is not enough since \CODE{y}
    and \CODE{z} are sharing before line 46.} capable to detect that
  there is no path from \CODE{y} to \CODE{y} (otherwise, a cycle could
  be created by \CODE{y.next = z}).  This information should be
  available as , and allows to say that \CODE{y} and
  \CODE{list} (note that \CODE{list} is affected because it is sharing
  with \CODE{y}) are cyclic after line 46 if and only if \CODE{z} or
  \CODE{x} or \CODE{list} were before that line.  The same happens at
  line 42.
  
  Moreover, the agreement does not change in the loop body at lines
  36--40 because (1) data structures are not modified; (2) \CODE{list}
  is not affected in any way; and (3) the value of \CODE{x} changes,
  but its cyclicity does not (by executing \CODE{x:=x.next}, there is
  no way to make unreachable a cycle which was reachable before, or
  the other way around).  Importantly, this also means that the
  cyclicity of \CODE{list} and \CODE{x} is \emph{preserved} by the
  loop, so that it can be safely removed from the slice (the preserved
  property is the same as the agreement after line 40).
  
  On the other hand, the conditional statement at lines 43--47 cannot
  be removed directly because it is not possible to prove that the
  cyclicity of \CODE{list} is preserved through it (actually, it is
  not).  In order to prove that the whole code between lines 34 and 47
  preserves the cyclicity of \CODE{list}, a kind of \emph{case-based}
  reasoning could be used: (1) the initially acyclicity of \CODE{list}
  implies its final acyclicity; and (2) the initially cyclicity of
  \CODE{list} implies its final cyclicity.  Both these results can be
  proved by standard static-analysis techniques \cite{tcs13}.
\end{example}

Needless to say, slices could be sub-optimal (for example, the
requirement about executing loops the same number of times needs not
be satisfied by any correct slice).  It is not difficult to see that,
if a ``concrete'' slicing criterion would be considered instead of an
abstract one, then agreements would only be allowed to contain
conditions  for a certain set of variables.  The
\CODE{labelSequence} would work exactly the same way, with an
important difference: when trying to compute the precondition of a
tuple , the possible outcome
could, again, contain only conditions .

\begin{example}
  \label{ex:conditionsToSlice1}
  In Example \ref{ex:conditionsToSlice}, suppose the final agreement
  be , corresponding to a concrete slicing
  criterion interested in (the exact value of) .  In this case, the
  agreement after line 10 could only be ,
  since (1)  would not be correct, and (2) no other abstract
  domain can appear in agreements.  This way, line 9 could not be
  sliced out since it does not preserve the  property of .
\end{example}

Semantically, abstract slices are in general smaller than concrete
slices.  This is also the case of
Example \label{ex:conditionsToSlice1}.  Clearly, this does \emph{not}
imply that \emph{every} abstract slicing algorithm would remove more
statements than \emph{every} concrete slicing algorithm.

\subsection{Practical issues and optimizations}
\label{sec:practicalIssuesAndOptimizations}

This section discusses how the analysis can realistically deal with
the computation of abstract dependencies and the propagation of
agreements, and an optimization based on recent work on sharing.

\subsubsection{Agreements and ucos}
\label{sec:agreementsAndUcos}

One of the major challenges of the whole approach is how agreements
are propagated backwards through the code as precisely as possible,
i.e., being able to detect that agreement on some ucos before 
implies agreement on (possibly) other ucos after .

Ideally, given ,  and , the \GSYSTEM should find
the \emph{best}  such that
.  However, it is clearly
unrealistic to imagine that the static analyzer will always be able to
find the best ucos without going into severe scalability (even
decidability) issues: in general, there exist infinite possible
choices for an input agreement satisfying the augmented triple.  In
practice, an implementation of this slicing algorithm will be equipped
with a library of ucos among which the satisfaction of augmented
triples can be checked.  The wider the library, the more precise the
results.  Rule of the \PSYSTEM and the \GSYSTEM can be specialized
with respect to the ucos at hand.

\begin{example}
  \label{ex:conditionsToSlice2}
  Consider this code already presented in Example
  \ref{ex:conditionsToSlice}, where the slicing criterion is the final
  nullity of \xx:
  {\em \begin{lstlisting}[firstnumber=8]
   ...
   n := n*2;
   C x := new C();
   if (n=0) {
     x := null;
   } else {
     x := new C();
   }
  \end{lstlisting}}
  \noindent
  In order to be able to remove lines 9 and 10 from the slice, an
  analyzer has to ``know'' that, after merging the result of both
  branches, the uco  precisely describes the agreement
  before line 11.  In other words, the analyzer must know both
   and  in order to be able to manipulate
  information about them.  Moreover, given a library of ucos, rules
  can be optimized for some recurrent programming patterns like guards
  (\CODE{n=0}) or (\CODE{x=null}), or statements \CODE{m:=0}.
\end{example}

It is clear that to design of an analyzer which is able to deal with
all possible ucos is infeasible.  However, the combination of some
simple numeric or reference domains like the one described in this
paper would already lead to meaningful results.

\subsubsection{Use of Field-Sensitive Sharing}
\label{sec:useOfFieldSensitiveSharing}

As already mentioned, \emph{field-sensitive sharing analysis}
\cite{ZanardiniG15sh} is able to keep track of fields which are
involved in \emph{converging paths} from two variables to a common
location in the heap (the \emph{shared} location).  A
\emph{propositional formula} is attached to each pair of variables and
each program point, and specifies the fields involved in \emph{every}
pair of converging paths reaching a common location.  For example, if
the formula  is attached to a pair of variables 
at a certain program point  (written ), this means
that the analysis was able to detect that, for \emph{every} two paths
 and  in the heap starting from  and ,
respectively, and both ending in the same (shared) location,
\begin{itemize}
\item  certainly does not traverse field \CODE{f}, as dictated
  by  (arrows from top-left to
  bottom-right refer to paths from , i.e., the first variable in
  the pair under study); and
\item  certainly traverses \CODE{g}, as prescribed by
   (arrows from top-right to bottom-left
  refer to paths from ).
\end{itemize}
In presence of such an analysis, two kinds of improvements can be
potentially obtained when analyzing a field update:
\begin{itemize}
\item the number of variables which can be actually affected by an
  update is, in general, reduced since it is possible to guarantee
  that some (traditionally) sharing variables will not be affected;
\item even for variables which are (still) possibly sharing with ,
  the set of field sequences  to be considered can be
  substantially smaller.
\end{itemize}

\begin{example}
  \label{ex:fieldSensitiveSharing}
  Suppose that field-sensitive sharing analysis is able to guarantee
  the following at a program point before the field update
  \CODE{x.f:=e}:
  \begin{itemize}
  \item The formula  correctly describes the sharing between
    \xx and \yy; and
  \item The formula  correctly describes the
    sharing between \xx and \zz.
  \end{itemize}
  According to the traditional notion of sharing, both \yy and \zz may
  share with \xx.  However,
  \begin{itemize}
  \item the assignment \CODE{x.f:=e} provably does \emph{not} affect
    \yy because no path from \xx traversing \CODE{f} will reach a
    location that is also reachable from \yy; and
  \item when considering all the possible field sequences starting
    from \zz, only those containing \CODE{h} have to be considered.
  \end{itemize}
\end{example}

The rule \GRULENAME{fassign} can be refined by using field-sensitive
sharing, as follows.  Let  be the program point before the field
update.
\begin{itemize}
\item in pre-condition , the only sharing variables that
  have to be dealt with are those for which it cannot be proved that
  they are unaffected by the update; this can be done by defining a
  new set  of variables which are possibly sharing with
   in such a way that some path from  to a shared location could
  traverse :
   This means that  is considered as
  potentially affected by the update when the propositional formula
  describing how it shares with  does not entail that paths from
   to shared locations do not traverse .
\item in the same pre-condition , the universal
  quantification on field sequences can be restricted to those
  compatible with field-sensitive information.  More formally, a field
  sequence  has to be considered only if it
  is possible that a path from  traversing exactly those fields
  ends in a shared location, or, equivalently, if the set  is a \emph{model} of .  That such
  a set is a model of  is equivalent to say that the
  field-sensitive information is compatible with the existence of a
  pair of paths  and  such that (1)  starts from
  ; (2)  starts from ; (3)  only traverses ;
  (4)  traverses all and only the fields ;
  and (5) both paths end in the same shared location.  Let
   be the set of fields contained in the field
  sequence .  Then, the above condition can be written as 
   where , and  means
  that  is any proposition  or 
  (for some field ) not included in .
\end{itemize}

The refined \GRULENAME{fassign} rule, called \GRULENAME{fassign2},
comes to be



\subsection{Comparison with related algorithms}
\label{sec:comparisonWithRelatedAlgorithms}

The Tukra Abstract Program Slicing Tool
\cite{DBLP:conf/icsoft/HalderC12} implements the computation of a
Dependence Condition Graph \cite{DBLP:journals/scp/HalderC13} for
performing abstract slicing.  To use a Program Dependence Graph is
somehow alternative to the computation of agreements.  As far as the
author make it possible to understand, Tukra only deals with numerical
values, and it is not clear which properties are supported (i.e.,
which is the ``library'' of ucos mentioned in Section
\ref{sec:agreementsAndUcos}).

Moreover, the authors of that tool point out that their approach is
able to exclude some dependencies that were not ruled out in previous
work introducing abstract dependencies \cite{MastroeniZanardini}.
However, they do not consider that a rule system for computing
agreements (essentially, the \GSYSTEM described in the present paper)
was introduced \cite{Zanardini} before Tukra was developed, and does
not suffer from the limitations they describe.



\section{Related Work}
\label{sec:relatedWork}
\COMMENT{ The standard approach for characterizing slices and the
  corresponding relation \emph{being slice of} is based on the notion
  of program dependency graph \cite{horPR89,reps91}, as described by
  Binkley and Gallagher \cite{BinGalla96}.  \emph{Program Dependency
    Graphs} (PDGs) can be built out of programs, and describe how data
  propagate at runtime.  In program slicing, we could be interested in
  computing dependencies on statements:  depends on  if some
  variables which are used inside  are defined inside , and
  definitions in  reach  through at least one possible
  execution path.  Also,  depends \emph{implicitly} on an
  if-statement or a loop if its execution depends on the boolean
  guard.
\begin{example}
    \label{example:statementDependency}
    Consider the program below and the derived PDG (edges which can be
    obtained by transitivity are omitted):
    \begin{figure}[h]
      \center{ \begin{picture}(0,0)\includegraphics{depGraph.pdf}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(3918,1557)(1651,-1221)
\put(2540,-1185){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{y}{v+1}}}}}}
\put(4593,252){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5277, 47){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5140,-227){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4388,-227){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4388,-637){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4388,-1048){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5140,-1048){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4046,-158){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1925,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{w}{3}}}}}}
\put(1651,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3156,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3430,-364){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{z}{3}}}}}}
\put(1651,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1651,-774){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1651,-637){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1925,-774){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{v}{z+w}}}}}}
\put(3430,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{v}{4}}}}}}
\put(1925,-637){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{w}{z+4}}}}}}
\put(1925,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}\IMPASSIGN{z}{1}}}}}}
\put(3156,-500){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2267, 47){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2540, 47){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2267,-1185){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\familydefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture}}
    \end{figure}
     depends on both  and  (and, by transitivity, )
    since  is not known statically when entering .  On the
    other hand, there is \emph{no} dependency of  on either (i)
    , since  is not used in ; or (ii) , since  is
    always redefined before .  The dependency of  on 
    is implicit since  does not depend on  nor , but  is
    executed conditionally on .
  \end{example}
  There exist several techniques for building and analyzing
  dependency graphs, allowing to study how information propagates
  among statements.  Usually, the basic rules for detecting a
  dependency between  and  are
\begin{itemize}
\item \emph{Control dependency edges:}  represents a control
  predicate and  represents a program component immediately
  nested within ;
\item \emph{Flow dependency edges:}  defines a variable 
  which is used in , i.e., , and there is a path from  to
   where  is not redefined..
\end{itemize}
In principle, a statement  belongs to a slice if the slicing
criterion is interested in some variables  at ,  defines
some , and there is a path in the PDG (i.e., a dependency)
from  to .}

The formal framework referred to in this paper \cite{AForm} is not the
only attempt to provide a unified mathematical framework from program
slicing.  In \cite{WardZedan}, the authors have precisely this aim.
In this work, the authors unify different approaches to program
slicing by defining a particular semantic relation, based on the
weakest precondition semantics, called \emph{semirefinement} such
that, given a program , the possible slices are all the
programs that are semirefinements of .  In this framework,
different forms of slicing are modeled as program transformations.
Hence, a program  is a slice of  if the transformation
of  (corresponding to the particular form of slicing to
compute) is a semirefinement of the same transformation of .
This approach is extremely interesting, but does not really allow to
compare the different forms of slicing, feature that we consider
fundamental for introducing the new abstract forms of slicing as
generalizations of the existing ones.  It may surely deserve further
research to study whether also abstract slicing could be modeled in
this framework.

As far as the relation between slicing and dependencies is concerned,
there are at least two works that are related with our ideas in
different ways.  One of the first works aiming at formalizing a
semantic approach to dependency, leading to a semantic computation of
slicing, is the \emph{information-flow logic} by Amtoft and Banerjee
\cite{AB07}.  This logic allows us to formally derive, by structural
induction, the set of all the \emph{independencies} among variables.
In Figure \ref{AB04-fig}, the original notation proposed by the
authors is used, where  is to be read as ``the current
value of  is independent of the initial value of '', and holds
if, for each pair of \emph{initial} states which agree on all the
variables but , the corresponding \emph{current} states agree on
.  Hence,  stands for sets of independencies, and  is a
set of variables representing the \emph{context}, i.e., (a superset
of) the variables on which at least one test surrounding the
statements depends on.

\begin{figure}[h]
  \begin{center}
    \framebox{ 
    }
  \end{center}
  \caption{A fragment of the independency logic}
  \label{AB04-fig}
\end{figure}

\noindent
In our aim of defining slicing in terms of dependencies, the first
thing we have to observe in this logic is that it always computes
(in)dependencies from the \emph{initial} values of variables. This
makes its use for slicing not so straightforward, since it loses the
\emph{local} dependency between statements.  Consider for example the
program fragment .  At
the end of this program, we know that \zz only depends on the initial
value of \xx, but, by using the logic in Fig.~\ref{AB04-fig}, we lose
the trace of (in)dependencies which, in this case, would involve all
the three assignments.  As a matter of fact, this logic is more
suitable for forward slicing, which is the one considered by the
authors \cite{AB07}, since it fixes the criterion \emph{on the input}.
In the trivial example given above, if we consider as criterion the
input of \xx, then we obtain that all the statements depend on \xx.
Therefore, any slice of the original program contains all statements
\cite{AB07}.
In the logic, more explicitly, this notion of dependency is used for
characterizing the set of independencies holding during the execution
of a program.

Another, more recent, approach to slicing by means of dependencies is
\cite{Danicic11}.  In this work, the authors propose new definitions
of control dependencies: non-termination sensitive and insensitive.
These new semantic notions of dependencies are then used for computing
more precise standard slices.  It could be surely interesting to study
the semantic relation between their notion of dependencies and the
ones we propose in this paper.

Finally, a related algorithm for computing abstract slices has been
already discussed in Section
\ref{sec:comparisonWithRelatedAlgorithms}.  It is necessary to point
out that the agreement-based approach to abstract slicing
\cite{Zanardini} was introduced before the Tukra tool.


\section{Conclusion and Future Work}
The present paper formally defines the notion of abstract program
slicing, a general form of slicing where properties of data are
observed instead of their exact value.  A formal framework is
introduced where the different forms of abstract slicing can be
compared; moreover, traditional, non-abstract forms of slicing are
also included in the framework, allowing to prove that non-abstract
slicing is a special case of abstract slicing where no abstraction on
data is performed.

Algorithms for computing abstract dependencies and program slices are
given.  Future work includes an implementation of this analysis for an
Object-Oriented programming language where properties may refer either
to numerical or reference values (to the best of our knowledge,
existing tools only deal with integer variables).  On the other hand,
we observed that the provided notion of abstract dependency is not
suitable for slicing computation by using PDGs.  We believe that it is
possible to further generalize the notion of abstract dependencies
allowing to characterize a recursive algorithm able to track backwards
both the variables that affect the criterion, and the abstract
properties of these variables affecting the abstract criterion.

Another interesting line of research is to understand how other
approaches to slicing can be extended in order to include abstract
slicing.  As noted before, it would be interesting to study whether it
is possible to model abstract slicing as a program transformation,
allowing us to define also abstract slicing in term of
semirefinement \cite{WardZedan}.  Another, more algorithmic,
interesting approach is the one proposed in \cite{Barros10}, where
weakest precondition and strongest postcondition semantics are
combined in a new more precise algorithm for standard slicing.  It
could be very interesting to understand whether this approach could be
extended in order to cope also with the computation of abstract forms
of slicing.


\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Amtoft and Banerjee}{Amtoft and
  Banerjee}{2007}]{AB07}
{\sc Amtoft, T.} {\sc and} {\sc Banerjee, A.} 2007.
\newblock A logic for information flow analysis with an application to forward
  slicing of simple imperative programs.
\newblock {\em Science of Computer Programming\/}~{\em 64,\/}~1, 3--28.

\bibitem[\protect\citeauthoryear{Barros, da~Cruz, Henriques, and Pinto}{Barros
  et~al\mbox{.}}{2010}]{Barros10}
{\sc Barros, J.~B.}, {\sc da~Cruz, D.}, {\sc Henriques, P.~R.}, {\sc and} {\sc
  Pinto, J.~S.} 2010.
\newblock Assertion-based slicing and slice graphs.
\newblock In {\em Proceedings of the 2010 8th IEEE International Conference on
  Software Engineering and Formal Methods}. SEFM '10. IEEE Computer Society,
  Washington, DC, USA, 93--102.

\bibitem[\protect\citeauthoryear{Bijlsma and Nederpelt}{Bijlsma and
  Nederpelt}{1998}]{BN98}
{\sc Bijlsma, A.} {\sc and} {\sc Nederpelt, R.} 1998.
\newblock Dijkstra-{S}cholten predicate calculus : concepts and misconceptions.
\newblock {\em Acta Informatica\/}~{\em 35,\/}~12, 1007--1036.

\bibitem[\protect\citeauthoryear{Binkley, Danicic, Gyim\'othy, Harman, Kiss,
  and Korel}{Binkley et~al\mbox{.}}{2006a}]{AForm}
{\sc Binkley, D.}, {\sc Danicic, S.}, {\sc Gyim\'othy, T.}, {\sc Harman, M.},
  {\sc Kiss, A.}, {\sc and} {\sc Korel, B.} 2006a.
\newblock A formalisation of the relationship between forms of program slicing.
\newblock {\em Science of Computer Programming\/}~{\em 62,\/}~3, 228--252.

\bibitem[\protect\citeauthoryear{Binkley, Danicic, Gyim\'othy, Harman, Kiss,
  and Korel}{Binkley et~al\mbox{.}}{2006b}]{TheoFoun}
{\sc Binkley, D.}, {\sc Danicic, S.}, {\sc Gyim\'othy, T.}, {\sc Harman, M.},
  {\sc Kiss, A.}, {\sc and} {\sc Korel, B.} 2006b.
\newblock Theoretical foundations of dynamic program slicing.
\newblock {\em Theoretical Computer Science\/}~{\em 360,\/}~1, 23--41.

\bibitem[\protect\citeauthoryear{Binkley and Gallagher}{Binkley and
  Gallagher}{1996}]{BinGalla96}
{\sc Binkley, D.~W.} {\sc and} {\sc Gallagher, K.~B.} 1996.
\newblock Program slicing.
\newblock {\em Advances in Computers\/}~{\em 43}.

\bibitem[\protect\citeauthoryear{Canfora, Cinitile, and {De Lucia}}{Canfora
  et~al\mbox{.}}{1998}]{Conditioned}
{\sc Canfora, G.}, {\sc Cinitile, A.}, {\sc and} {\sc {De Lucia}, A.} 1998.
\newblock Conditioned program slicing.
\newblock {\em Information and Software Technology\/}~{\em 40}, 11--12.

\bibitem[\protect\citeauthoryear{Cimitile, De~Lucia, and Munro}{Cimitile
  et~al\mbox{.}}{1996}]{CDM96}
{\sc Cimitile, A.}, {\sc De~Lucia, A.}, {\sc and} {\sc Munro, M.} 1996.
\newblock A specification driven slicing process for identifying reusable
  functions.
\newblock {\em Journal of Software Maintenance\/}~{\em 8,\/}~3, 145--178.

\bibitem[\protect\citeauthoryear{Cousot}{Cousot}{2001}]{C01-Dag}
{\sc Cousot, P.} 2001.
\newblock Abstract interpretation based formal methods and future challenges.
\newblock In {\em Informatics - 10 Years Back. 10 Years Ahead}. 138--156.

\bibitem[\protect\citeauthoryear{Cousot and Cousot}{Cousot and
  Cousot}{1977}]{CC77}
{\sc Cousot, P.} {\sc and} {\sc Cousot, R.} 1977.
\newblock Abstract interpretation: A unified lattice model for static analysis
  of programs by construction or approximation of fixpoints.
\newblock In {\em Proceedings of ACM Symposium on Principles of Programming
  Languages (POPL)}. ACM Press, New York, 238--252.

\bibitem[\protect\citeauthoryear{Cousot and Cousot}{Cousot and
  Cousot}{1979}]{CC79}
{\sc Cousot, P.} {\sc and} {\sc Cousot, R.} 1979.
\newblock Systematic design of program analysis frameworks.
\newblock In {\em Proceedings of ACM Symposium on Principles of Programming
  Languages (POPL)}. ACM Press, New York, 269--282.

\bibitem[\protect\citeauthoryear{Danicic, Barraclough, Harman, Howroyd, Kiss,
  and Laurence}{Danicic et~al\mbox{.}}{2011}]{Danicic11}
{\sc Danicic, S.}, {\sc Barraclough, R.~W.}, {\sc Harman, M.}, {\sc Howroyd,
  J.~D.}, {\sc Kiss, A.}, {\sc and} {\sc Laurence, M.~R.} 2011.
\newblock A unifying theory of control dependence and its application to
  arbitrary program structures.
\newblock {\em Theor. Comput. Sci.\/}~{\em 412,\/}~49, 6809--6842.

\bibitem[\protect\citeauthoryear{{De Lucia}}{{De Lucia}}{2001}]{DeLucia}
{\sc {De Lucia}, A.} 2001.
\newblock Program slicing: Methods and applications.
\newblock In {\em Proceedings of International Workshop on Source Code Analysis
  and Manipulation (SCAM)}.

\bibitem[\protect\citeauthoryear{Dijkstra}{Dijkstra}{1975}]{Dij75}
{\sc Dijkstra, E.} 1975.
\newblock Guarded commands, nondeterminacy and formal derivation of programs.
\newblock {\em Communications of the ACM\/}~{\em 18,\/}~8, 453--457.

\bibitem[\protect\citeauthoryear{Dijkstra and Scholten}{Dijkstra and
  Scholten}{1990}]{DS90}
{\sc Dijkstra, E.} {\sc and} {\sc Scholten, C.~S.} 1990.
\newblock {\em Predicate {C}alculus and {P}rogram {S}emantics}.
\newblock Springer-Verlag.

\bibitem[\protect\citeauthoryear{Field, Ramalingam, and Tip}{Field
  et~al\mbox{.}}{1995}]{FRT96}
{\sc Field, J.}, {\sc Ramalingam, G.}, {\sc and} {\sc Tip, F.} 1995.
\newblock Parametric program slicing.
\newblock In {\em Proceedings of ACM Symposium on Principles of Programming
  Languages (POPL)}. ACM Press, 379--392.

\bibitem[\protect\citeauthoryear{Gallagher and Lyle}{Gallagher and
  Lyle}{1991}]{GL91ieee}
{\sc Gallagher, K.~B.} {\sc and} {\sc Lyle, J.~R.} 1991.
\newblock Using program slicing in software maintenance.
\newblock {\em IEEE Transactions on Software Engineering\/}~{\em 17,\/}~8,
  751--761.

\bibitem[\protect\citeauthoryear{Genaim and Zanardini}{Genaim and
  Zanardini}{2013}]{tcs13}
{\sc Genaim, S.} {\sc and} {\sc Zanardini, D.} 2013.
\newblock Reachability-based {A}cyclicity {A}nalysis by {A}bstract
  {I}nterpretation.
\newblock {\em Theoretical Computer Science\/}~{\em 474,\/}~0, 60--79.

\bibitem[\protect\citeauthoryear{Giacobazzi, Jones, and Mastroeni}{Giacobazzi
  et~al\mbox{.}}{2012}]{GiacobazziJM12}
{\sc Giacobazzi, R.}, {\sc Jones, N.~D.}, {\sc and} {\sc Mastroeni, I.} 2012.
\newblock Obfuscation by partial evaluation of distorted interpreters.
\newblock In {\em Proceedings of the {ACM} {SIGPLAN} 2012 Workshop on Partial
  Evaluation and Program Manipulation, {PEPM} 2012, Philadelphia, Pennsylvania,
  USA, January 23-24, 2012}. 63--72.

\bibitem[\protect\citeauthoryear{Giacobazzi and Mastroeni}{Giacobazzi and
  Mastroeni}{2004a}]{GM04popl}
{\sc Giacobazzi, R.} {\sc and} {\sc Mastroeni, I.} 2004a.
\newblock Abstract non-interference: Parameterizing non-interference by
  abstract interpretation.
\newblock In {\em Proceedings of ACM Symposium on Principles of Programming
  Languages (POPL)}. ACM Press, 186--197.

\bibitem[\protect\citeauthoryear{Giacobazzi and Mastroeni}{Giacobazzi and
  Mastroeni}{2004b}]{GM04CSL}
{\sc Giacobazzi, R.} {\sc and} {\sc Mastroeni, I.} 2004b.
\newblock Proving abstract non-interference.
\newblock In {\em Annual Conf.\ of the European Association for Computer
  Science Logic (CSL~'04)}, {A.~T. J.~Marcinkowski}, Ed. Vol. 3210.
  Springer-Verlag, Berlin, 280--294.

\bibitem[\protect\citeauthoryear{Giacobazzi, Ranzato, and Scozzari}{Giacobazzi
  et~al\mbox{.}}{2000}]{GRSjacm}
{\sc Giacobazzi, R.}, {\sc Ranzato, F.}, {\sc and} {\sc Scozzari, F.} 2000.
\newblock Making abstract interpretations complete.
\newblock {\em J.\ of the ACM.\/}~{\em 47,\/}~2, 361--416.

\bibitem[\protect\citeauthoryear{Halder and Cortesi}{Halder and
  Cortesi}{2012}]{DBLP:conf/icsoft/HalderC12}
{\sc Halder, R.} {\sc and} {\sc Cortesi, A.} 2012.
\newblock Tukra: {A}n {A}bstract {P}rogram {S}licing {T}ool.
\newblock In {\em Proceedings of International Conference on Software Paradigm
  Trends (ICSOFT)}. 178--183.

\bibitem[\protect\citeauthoryear{Halder and Cortesi}{Halder and
  Cortesi}{2013}]{DBLP:journals/scp/HalderC13}
{\sc Halder, R.} {\sc and} {\sc Cortesi, A.} 2013.
\newblock Abstract program slicing on dependence condition graphs.
\newblock {\em Science of Computer Programming\/}~{\em 78,\/}~9, 1240--1263.

\bibitem[\protect\citeauthoryear{Hind}{Hind}{2001}]{Hind2001}
{\sc Hind, M.} 2001.
\newblock Pointer analysis: Haven't we solved this problem yet?
\newblock In {\em Proceedings of the Workshop on Program Analysis for Software
  Tools and Engineering (PASTE)}. ACM Press, New York, 54--61.

\bibitem[\protect\citeauthoryear{Hoare}{Hoare}{1969}]{Hoa69}
{\sc Hoare, C.} 1969.
\newblock An axiomatic basis for computer programming.
\newblock {\em Communications of the ACM\/}~{\em 12,\/}~10, 576--580.

\bibitem[\protect\citeauthoryear{Horwitz, Prins, and Reps}{Horwitz
  et~al\mbox{.}}{1989}]{horPR89}
{\sc Horwitz, S.}, {\sc Prins, J.}, {\sc and} {\sc Reps, T.} 1989.
\newblock Integrating non-interfering versions of programs.
\newblock {\em ACM Transaction on Programming Languages and Systems\/}~{\em
  11,\/}~3.

\bibitem[\protect\citeauthoryear{Hunt and Mastroeni}{Hunt and
  Mastroeni}{2005}]{HM05}
{\sc Hunt, S.} {\sc and} {\sc Mastroeni, I.} 2005.
\newblock The {P}{E}{R} model of abstract non-interference.
\newblock In {\em Proceedings of Static Analysis Symposium (SAS)}. Lecture
  Notes in Computer Science Series, vol. 3672. Springer-Verlag, 171--185.

\bibitem[\protect\citeauthoryear{Korel and Laski}{Korel and
  Laski}{1988}]{KorelLaski}
{\sc Korel, B.} {\sc and} {\sc Laski, J.} 1988.
\newblock Dynamic program slicing.
\newblock {\em Information Processing Letters\/}~{\em 29,\/}~3, 155--183.

\bibitem[\protect\citeauthoryear{Majumdar, Drape, and Thomborson}{Majumdar
  et~al\mbox{.}}{2007}]{MDT07}
{\sc Majumdar, A.}, {\sc Drape, S.~J.}, {\sc and} {\sc Thomborson, C.~D.} 2007.
\newblock Slicing obfuscations: design, correctness, and evaluation.
\newblock In {\em Proceedings of ACM Workshop on Digital Rights Management
  (DRM)}. ACM, New York, NY, USA, 70--81.

\bibitem[\protect\citeauthoryear{Mastroeni}{Mastroeni}{2013}]{Mastroeni13}
{\sc Mastroeni, I.} 2013.
\newblock Abstract interpretation-based approaches to security - {A} survey on
  abstract non-interference and its challenging applications.
\newblock In {\em Semantics, Abstract Interpretation, and Reasoning about
  Programs: Essays Dedicated to David A. Schmidt on the Occasion of his
  Sixtieth Birthday, Manhattan, Kansas, USA, 19-20th September 2013.} 41--65.

\bibitem[\protect\citeauthoryear{Mastroeni and Nikoli\'c}{Mastroeni and
  Nikoli\'c}{2010}]{MastroeniNicolic}
{\sc Mastroeni, I.} {\sc and} {\sc Nikoli\'c, D.} 2010.
\newblock Abstract {P}rogram {S}licing: {F}rom {T}heory towards an
  {I}mplementation.
\newblock In {\em Proceedings of International Conference on Formal Engineering
  Methods (ICFEM)}. Lecture Notes in Computer Science Series, vol. 6447.
  Springer-Verlag, 452--467.

\bibitem[\protect\citeauthoryear{Mastroeni and Zanardini}{Mastroeni and
  Zanardini}{2008}]{MastroeniZanardini}
{\sc Mastroeni, I.} {\sc and} {\sc Zanardini, D.} 2008.
\newblock Data dependencies and program slicing: From syntax to abstract
  semantics.
\newblock In {\em Proceedings of Symposium on Partial Evaluation and
  Semantics-Based Program Manipulation (PEPM)}. 125--134.

\bibitem[\protect\citeauthoryear{Ranzato and Tapparo}{Ranzato and
  Tapparo}{2002}]{RT02}
{\sc Ranzato, F.} {\sc and} {\sc Tapparo, F.} 2002.
\newblock Making abstract model checking strongly preserving.
\newblock In {\em Proceedings of Static Analysis Symposium (SAS)}. Lecture
  Notes in Computer Science Series, vol. 2477. Springer-Verlag, 411--427.

\bibitem[\protect\citeauthoryear{Reps}{Reps}{1991}]{reps91}
{\sc Reps, T.} 1991.
\newblock Algebraic properties of program integration.
\newblock {\em Science of Computer Programming\/}~{\em 17}, 139--215.

\bibitem[\protect\citeauthoryear{Reps and Yang}{Reps and Yang}{1989}]{RY88}
{\sc Reps, T.} {\sc and} {\sc Yang, W.} 1989.
\newblock The semantics of program slicing and program integration.
\newblock In {\em Proc.\ of the Colloq.\ on Current Issues in Programming
  Languages}, {J.~Diaz} {and} {F.~Orejas}, Eds. Lecture Notes in Computer
  Science Series, vol. 352. Springer-Verlag, Berlin, 360--374.

\bibitem[\protect\citeauthoryear{Rossignoli and Spoto}{Rossignoli and
  Spoto}{2006}]{DBLP:conf/vmcai/RossignoliS06}
{\sc Rossignoli, S.} {\sc and} {\sc Spoto, F.} 2006.
\newblock Detecting non-cyclicity by abstract compilation into boolean
  functions.
\newblock In {\em Proceedings of the International Conference on Verification,
  Model Checking, and Abstract Interpretation (VMCAI)}. Lecture Notes in
  Computer Science Series, vol. 3855. Springer-Verlag, 95--110.

\bibitem[\protect\citeauthoryear{Secci and Spoto}{Secci and
  Spoto}{2005}]{DBLP:conf/sas/SecciS05}
{\sc Secci, S.} {\sc and} {\sc Spoto, F.} 2005.
\newblock Pair-sharing analysis of object-oriented programs.
\newblock In {\em Proceedings of the Interanational Symposium on Static
  Analysis (SAS)}. Springer-Verlag, 320--335.

\bibitem[\protect\citeauthoryear{Tip}{Tip}{1995}]{Tip95}
{\sc Tip, F.} 1995.
\newblock A survey of program slicing techniques.
\newblock {\em Journal of Programming Languages\/}~{\em 3}, 121--181.

\bibitem[\protect\citeauthoryear{Ward and Zedan}{Ward and
  Zedan}{2007}]{WardZedan}
{\sc Ward, M.} {\sc and} {\sc Zedan, H.} 2007.
\newblock Slicing as a program transformation.
\newblock {\em ACM Transactions on Programming Languages and Systems\/}~{\em
  29,\/}~2.

\bibitem[\protect\citeauthoryear{Weiser}{Weiser}{1984}]{weiser}
{\sc Weiser, M.} 1984.
\newblock Program slicing.
\newblock {\em IEEE Trans.\ on Software Engineering\/}~{\em 10,\/}~4, 352--357.

\bibitem[\protect\citeauthoryear{Zanardini}{Zanardini}{2008}]{Zanardini}
{\sc Zanardini, D.} 2008.
\newblock The {S}emantics of {A}bstract {P}rogram {S}licing.
\newblock In {\em Proceeding of Working Conference on Source Code Analysis and
  Manipualtion (SCAM)}.

\bibitem[\protect\citeauthoryear{Zanardini}{Zanardini}{2015}]{ZanardiniG15sh}
{\sc Zanardini, D.} 2015.
\newblock {F}ield-{S}ensitive {S}haring.
\newblock {\em CoRR\/}~{\em abs/1306.6526}.

\end{thebibliography}

\bibliographystyle{acmsmall} 

\section{APPENDIX: THE FORMAL SLICING FRAMEWORK}
In this section, we first provide a better intuition of the differences between the forms of slicing introduced in Section~\ref{section:Background} by means of examples and then we recall the main notions introduced in \cite{AForm,TheoFoun} that has been generalized in this paper in the abstract form.

\subsection*{Different forms of slicing: some examples}
\begin{example}\label{Ex:crit}
  Consider the program on the left in Figure~\ref{Exfig:crit}.
  Suppose that the execution start with an initial value  for
  \CODE{n} (written ).  States are denoted as
  , where  is the program point of the executed statement,  is its
  current iteration (i.e., the statement at  is being executed for the -th time in the loop unrolling),
  and  is the actual memory, represented by a list of
  pairings ). In the picture,  is depicted in the first (fully colored) box, while the memory is depicted in the remaining boxes, one for each variable. A program state that is not executed in a trace is depicted by overwriting a grey cross on each box (program point and variables).
The execution trajectory of the program is the following:
\begin{center}
\includegraphics[scale=.4]{ese3-3-1.pdf}
\end{center}
  
\noindent  
  Consider now the code in the center, whose execution trajectory is the following:
\begin{center}
\includegraphics[scale=.4]{ese3-3-2.pdf}
\end{center}
  We can observe that the semantics of the second program follows
  precisely the same path on the statements which are in both programs
  (the only difference is the execution, in the original program of the statement at point , erased in the "candidate" slice), hence it is a slice according both to the standard and the 
  form, namely w.r.t.\  for both the possible values of .  \\
Suppose now we are interested in an  form of slice, variable \xx at the second iteration of the  program point . In this case, the program on the right is not a dynamic slice, since the value of
  \xx in the original program is , while in the candidate slice it is
  undefined. In other words, this program is not slice of the program on the left w.r.t.\ the criterion  ( may be both true or false). \\ 
  Finally, let us consider the execution of the program on the right in Fugure~\ref{Exfig:crit}:
  \begin{center}
\includegraphics[scale=.4]{ese3-3-3.pdf}
\end{center}
 This last program is a standard dynamic slice since the
  final value of the variable of interest \xx is the same, but it is
  not a slice in the  form, since in this last program the statement at program point  is executed, while in the original program it is not executed. Namely, it is a slice w.r.t.\ the criterion  only for .  
  \begin{figure}
    \begin{tabular}{c|c|c}
      \begin{lstlisting}
  read(n);
  i := 1;
  while (i <= n) do {
    if (i mod 2 = 0) {
      x := 17; }
    else { x := 18; }
    i := i + 1;       
  }
  if (i = 1) {
    x = 17; }
  write(i,n,x);
      \end{lstlisting}
      &
      \begin{lstlisting}
  read(n);
  i := 1;
  while (i <= n) {
    if (i mod 2 = 0) {
      x := 17; }
    
    i := i + 1;       
  }
  if (i = 1) {
    x = 17; }
  write(i,n,x);
      \end{lstlisting}
      &
      \begin{lstlisting}
  read(n);
  i := 1;
    
  if (i = 1) { 
    x = 17; }
  write(i,n,x);
      \end{lstlisting}
    \end{tabular}
    \caption{Programs of Example~\ref{Ex:crit}}\label{Exfig:crit}
  \end{figure}
\end{example}

The following example shows the difference between standard and 
forms of {\em static} slicing.

\begin{example}\label{ex:stkl}
  Consider the program  on the left of Figure~\ref{fig:stkl}.
  Suppose static slicing is considered, i.e., all the possible initial
  memories are taken into account.  Given an input , the state
  trajectory is:
   \begin{center}
\includegraphics[scale=.4]{ese3-4-1.pdf}
\end{center}
  Consider now the code on the right: its state trajectory is
  \begin{center}
\includegraphics[scale=.4]{ese3-4-2.pdf}
\end{center}
  \begin{figure}
    \begin{center}
      \begin{tabular}{c|c}
        \begin{lstlisting}
  read(n);
    i := 1;
    i := 2;
    if (i mod 2 = 0) {
       x := i + n;  }
    if (i mod 2 = 1) {
       x := i + n + 1; }
  write(i,n,x);
        \end{lstlisting}
        &
        \begin{lstlisting}
  read(n);
    i := 1;
    
    if (i mod 2 = 1) {
       x := i + n + 1; }
  write(i,n,x);
        \end{lstlisting}
      \end{tabular}
    \end{center}
    \caption{Programs of Example~\ref{ex:stkl}}\label{fig:stkl}
  \end{figure}

  \noindent Consider the standard form of static slicing
  interested in \xx at program point , i.e., .  Then the program
  on the right is a slice of  w.r.t.\ , since in  the value of \xx
  is  in both cases.  On the other hand, if we consider the 
  form, , then the program is no more a slice
  of  since there is a program point, , which is not
  reached in .
\end{example}

\subsection*{The unified equivalence}

The first step for defining the formal framework is to define an
equivalence relation between programs, determining when a program is a
slice of another.  First, a \emph{restricted memory} is obtained from
a memory by restricting its domain to a set of variables.  More
formally, the restriction of  with respect to a set of
variables  is defined as  such that
 is equal to  if , and
undefined otherwise.  This restriction is used to project the trace
semantics only on those points of interest where we have to check the
correspondence between the original program and the candidate slice.

The \emph{trajectory projection} operator modifies a state trajectory
by removing all those states which do not contain occurrences of
program points which are relevant for the slicing criterion.

\begin{mydefinition}[Trajectory Projection \cite{AForm}]
  \label{def:Proj}
  Let , and  such that  if ,  otherwise. For any , , , we define the function  as:
  
  where  is the empty sequence.  The trace projection 
  is the extension of  to sequences ( is sequence
  concatenation):
  
\end{mydefinition}

\noindent  takes a state from a state trajectory, and returns
either one pair or an empty sequence .  If
 is an occurrence of interest, then it returns
.  This means that, at , we
consider exact values of variables in .  If  is not an
occurrence of interest, but, due
to a  form, the projection has to keep trace of a set  of executed statements (even if the variables in that point are not of interest), then  returns
, meaning that we require
the execution of , but we are not interested in the values of
variables in .

Trajectory projection allows us to define all the semantic equivalence
relations characterizing on what a program and its slices have to agree due to the chosen criterion..  Given two programs  and
, we can say that  is a slice of  if it contains a subset of the original
statements and  is \emph{equivalent} to  with respect
to the semantic equivalence relation induced by chosen the slicing criterion.

\begin{mydefinition}[\cite{AForm}]
  \label{def:UnifiedEquivalence} Let  and  be
  executable programs, and  be a slicing
  criterion.  Let  be the set of program
  points of , and  if \footnote{Note that,
  when , as we suppose in this paper, then .  We provide the general definition since
  the original definition of dynamic slicing \cite{KorelLaski}
  does not require that all the line of  are included in
  ; however, our choice follows the paths taken in the original
  framework \cite{TheoFoun}.}
  ( if ).  Then 
  is \emph{equivalent} to  w.r.t.\  if and only
  if  The function  maps any criterion  to the r
 to the corresponding semantic
  equivalence relation, hence, in this case, we write .
\end{mydefinition}

\begin{example}
  \label{ex:formSl}
  Consider the Program  in the left of
  Figure~\ref{fig:exsl}; let the input for \nn be .
 \noindent
  Suppose we want to compute a non iteration-count  form of
  dynamic slicing, i.e., . Namely, the variables of interest are
  \ii and \ss, which are observed at the program point  each time it is reached, and
   the slice has not to execute statements not executed in the original program.  The program on the right of Figure~\ref{ex:formSl} is a slice w.r.t.\ .
    In Figure~\ref{ExTrace} we have the execution trajectory of the original program (on the top), the execution trajectory of the candidate slice (in the middle) and the (same) projection of the two trajectories due to the chosen criterion (on the bottom).
  \begin{figure}
    \begin{center}
      \includegraphics[scale=.4]{Proj.pdf}
    \end{center}
    \caption{Execution trace of  in Example~\ref{ex:formSl} and of the static slice and of their projection.}\label{ExTrace}
  \end{figure}
\end{example}

The formal framework proposed in \cite{AForm,TheoFoun} represents
different forms of slicing by means of a  pair: a
\emph{syntactic preorder}, and a \emph{function from slicing criteria
  to semantic equivalences}.  The \emph{preorder} fixes a syntactic
relation between the program and its slices.  In traditional slicing,
 slices are obtained from the
original program by removing zero or more statements.  This preorder
is called \emph{traditional syntactic ordering}, simply denoted by
, and it is defined as follows: 
The second component  fixes the semantic constraints that a
subprogram has to respect in order to be a slice of the original
program.  As we have seen before, the equivalence relation is uniquely
determined by the chosen slicing criterion determining also a specific form of slicing.
This way, Binkley \etal are able to characterize eight forms of
non- slicing, and twelve forms of  slicing.

Finally, this framework is used to formally compare the different notions of
slicing.  First of all, it is defined a binary relation on slicing criteria
 \cite{AForm}: Let  and 

At this point, we say that a form of
slicing  is \emph{weaker than}  w.r.t. \small \normalsize iff , slicing criteria such that ,
and , if  is a slice of 
w.r.t.\ , then  is a
slice of  w.r.t.\  as
well. In this case we say that  subsumes .
\begin{figure}[tbp]
  \centering
  \includegraphics[scale=.8,viewport=4in 6in 5.6in 9.2in]{Reticolo1.pdf}
  \caption{Given two forms  and , both (non-).,  is
    weaker than  if  is connected to  by a solid line and it
    is below . If  is non-. and  is ., then 
    is weaker than  if  is connected to  by a dotted line and
    it is to the left of .\label{fig:Ret1}}
\end{figure}
Following this definition, Binkley et al.~show that all forms of
slicing introduced in~\cite{AForm} are comparable in the way shown in
Figure~\ref{fig:Ret1}, where the symbols , , ,
,  and  represent static, conditioned,
dynamic, static , conditioned  and dynamic  types of
slicing, respectively.  Subscripts ,  and  represent
,  and  forms of slicing, respectively; the absence of
subscripts denotes the standard forms of slicing.  In
Figure~\ref{fig:Ret1} we explicitly provide both the hierarchy
concerning  and non- forms of conditioned slicing
constructed in~\cite{AForm}.


\label{appendix}

\end{document}
