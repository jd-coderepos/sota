\documentclass{article}











\usepackage[final,nonatbib]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{mathtools,amsmath}
\usepackage{multirow}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}

\title{Domain Generalization by Learning and Removing Domain-specific Features}





\author{Yu Ding \\
  University of Wollongong\\
  \texttt{yd624@uowmail.edu.au} \\
  \And
  Lei Wang \\
  University of Wollongong\\
  \texttt{leiw@uow.edu.au} \\
  \And
  Bin Liang \\
  University of Technology Sydney\\
  \texttt{Bin.Liang@uts.edu.au} \\
  \And
  Shuming Liang \\
  University of Technology Sydney\\
  \texttt{Shuming.Liang@uts.edu.au} \\
  \And
  Yang Wang \\
  University of Technology Sydney\\
  \texttt{Yang.Wang@uts.edu.au} \\
  \And
  Fang Chen \\
  University of Technology Sydney\\
  \texttt{Fang.Chen@uts.edu.au} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Deep Neural Networks (DNNs) suffer from domain shift when the test dataset follows a distribution different from the training dataset. Domain generalization aims to tackle this issue by learning a model that can generalize to unseen domains. In this paper, we propose a new approach that aims to explicitly remove domain-specific features for domain generalization. Following this approach, we propose a novel framework called Learning and Removing Domain-specific features for Generalization (LRDG) that learns a domain-invariant model by tactically removing domain-specific features from the input images. Specifically, we design a classifier to effectively learn the domain-specific features for each source domain, respectively. We then develop an encoder-decoder network to map each input image into a new image space where the learned domain-specific features are removed. With the images output by the encoder-decoder network, another classifier is designed to learn the domain-invariant features to conduct image classification. Extensive experiments demonstrate that our framework achieves superior performance compared with state-of-the-art methods. Code is available at \url{https://github.com/yulearningg/LRDG}.
\end{abstract}


\section{Introduction}

Deep Neural Networks (DNNs) have achieved great performance in computer vision tasks \cite{krizhevsky2017imagenet}. However, the performance would drop if the test dataset follows a distribution different from the training dataset. This issue is also known as domain shift \cite{torralba2011unbiased}. Recent research has found that DNNs tend to learn decision rules differently from humans \cite{geirhos2018imagenet,hermann2020origins,geirhos2020shortcut}. For example, in ImageNet-based \cite{russakovsky2015imagenet} image classification tasks, Convolutional Neural Networks (CNNs) tend to learn local textures to discriminate objects, while we humans could use the knowledge of global object shapes as cues. The features learned by the DNNs may only belong to specific domains and are not generalized for other domains. For example, in real-world photos, objects belonging to the same category have similar textures, but in sketches \cite{li2017deeper}, objects are only drawn by lines and contain no texture information. For a CNN that uses textures to discriminate objects in the photos, poor performance can be expected when it is applied to the sketches. This situation calls for DNNs that can learn features invariant across domains instead of learning features that are domain-specific.

In this paper, we focus on the research topic of domain generalization and follow the multiple source domain generalization setting in the literature. Its goal is to train a model that can perform well on unseen domains. In this setting, we can access multiple labeled source domains and one or more unlabeled target domains. All the source and target domains share the same label space. During the training process, the source domains are available but the target domains are unseen. The target domains are only provided in the test phase.

One typical approach to domain generalization is to learn domain-invariant representations across domains \cite{ghifary2015domain,li2018deep,wang2019learningproject,arjovsky2019invariant,chattopadhyay2020learning,du2020learning,zhao2020domain,mahajan2021domain,rame2022fishr}. This approach is based on the assumption that each domain has its domain-specific features and that all domains share domain-invariant features. For example, textures are domain-specific features for the photos but shapes are domain-invariant features for both photos and sketches. Previous works propose methods that seek to distill the domain-invariant features. Although demonstrating promising performance, these methods do not clearly inform the deep neural networks that the domain-specific features shall be effectively removed. Instead, it is only hoped that they would be removed through achieving the final goal of learning the domain-invariant features. The lack of this clear guidance to the network may affect its learning efficacy. In this paper, we propose a new approach that aims to explicitly remove the domain-specific features in order to achieve domain generalization. As indicated above, CNNs tend to learn the domain-specific features rather than the domain-invariant features for classification. To prevent this from taking place, we actively remove the domain-specific features and guide the CNNs to learn the domain-invariant features for classification. Following this approach, we propose a novel framework: Learning and Removing Domain-specific features for Generalization (LRDG).

Our framework consists of domain-specific classifiers, an encoder-decoder network, and a domain-invariant classifier. The training process of our framework includes two steps. In the first step, each domain-specific classifier is designed to effectively learn the domain-specific features from one source domain. Specifically, a domain-specific classifier is designed to discriminate the images across different classes within one particular source domain. At the same time, this classifier is required to be unable to discriminate the images across different classes within any other source domain. Each source domain therefore corresponds to one domain-specific classifier under this design. In the second step, the encoder-decoder network maps the input images into a new image space where the domain-specific features learned above are to be removed from the input images by utilizing the domain-specific classifiers. Different from the first step, each domain-specific classifier here is unable to discriminate the mapped images across different classes within the corresponding source domain. The mapped images are expected to contain much fewer domain-specific features compared with the original input images. The domain-invariant classifier is then appended to the encoder-decoder network and trained with the mapped images. By this design, the encoder-decoder network actively removes the domain-specific features and the domain-invariant classifier will be better guided to learn the domain-invariant features. Once trained, the encoder-decoder network and the domain-invariant classifier will be used for the classification of the unseen target domains. 

It is worth noting that our framework is different from the data augmentation based methods for domain generalization \cite{xu2020robust,nam2021reducing,zhou2020domain,borlino2021rethinking}. Our framework aims to remove the domain-specific features from the input images while the data augmentation based methods generate various images with novel domain-specific features. Besides, our framework just maps the input images into a new image space and does not augment them to enlarge the training dataset.

We demonstrate the effectiveness of our framework with experiments on three benchmarks in domain generalization. Our framework consistently achieves state-of-the-art performance. We also experimentally verify that our framework effectively reduces the distribution difference among the source and target domains according to the generalization risk bound in the literature \cite{albuquerque2019generalizing}.

\section{Proposed framework}

Assuming that we are given  source domains  which follow different distributions. For each domain (dataset),  where  is the number of samples in , and  is the data-label pair for the th sample in the th domain. Following the literature, we assume that all source and target domains share the same label space. The goal of domain generalization is to use these source domains  to learn a model for the unseen target domain . 

Our work is inspired by recent work \cite{minderer2020automatic}, where it uses a "lens" network (i.e. image-to-image translation network) to remove "shortcuts" (low-level visual features that a CNN can quickly learn, such as watermarks and color aberrations) from input images in a self-supervised learning task. Differently, our work focuses on removing the domain-specific features from the input images for the domain generalization task. We use an encoder-decoder network similar to the "lens" network, but we design a different method to leverage the encoder-decoder network to remove the domain-specific features. In this section, we illustrate our framework in detail. We also provide theoretical analysis for our framework. Fig. \ref{fig:framework} gives an overview of the entire framework.

\begin{figure}[t]
	\begin{center}
	\includegraphics[width=0.98\linewidth]{images/framework_dg_final.pdf}
	\end{center}
	\caption{An overview of the proposed framework LRDG. (a) The domain-specific classifier  is trained with the classification loss  on the source domain  and the uncertainty loss  on the remaining source domains. After training, the weights of all the domain-specific classifiers are frozen. (b) The encoder-decoder network  is trained with the reconstruction loss  and the uncertainty loss  through the domain-specific classifiers. Meanwhile, the domain-invariant classifier  is trained with the classification loss  on the mapped images. (c) In the testing phase, the encoder-decoder network  and the domain-invariant classifier  are used for classification on the target domain .}
	\label{fig:framework}
\end{figure}

\subsection{Learning domain-specific features}

Our framework starts by training  individual domain-specific classifiers  in which the classifier  is designed to only use the domain-specific features from the source domain  to discriminate images. The domain-specific classifiers  should not use the domain-invariant features as cues. In other words,  is expected to be able to effectively discriminate images across different classes within  but it should be difficult for  to discriminate images across different classes within any other domains. Domains excluding  are used to maximize the classification uncertainty or adversarially increase the difficulty of classification for . The classification performance of  on the domains excluding  should be similar to a random guess.

Specifically, the classifier  is trained by minimizing a classification loss  on , and maximizing an uncertainty loss  on the remaining domains , where  denotes the parameters of the classifier .  and  are the classification loss function and the uncertainty loss function, respectively. We use the cross-entropy loss as the classification loss. For the uncertainty loss, since we aim to make the prediction similar to a random guess, we use entropy loss,where  is the number of classes and  denotes the probability of  belonging to class .
Least likely loss \cite{minderer2020automatic} is an alternative to the entropy loss. The classifier first predicts an image and obtains the probabilities of all the classes. The class with the lowest probability is called the least likely class. This image is assigned with a label of this class. Then we train the classifier to predict the least likely class. The least likely loss is However, experiments show that the entropy loss can better achieve the classification randomness than the least likely loss, so we use the entropy loss as the default uncertainty loss.

After training, we freeze the parameters  of these domain-specific classifiers  and use these classifiers to learn domain-invariant features.

\subsection{Removing domain-specific features}

To remove the domain-specific features learned by the domain-specific classifiers, we utilize an encoder-decoder network  that maps the images into a new image space . The output images are fed into the domain-specific classifiers  and a new domain-invariant classifier . 

Unlike the training of the domain-specific classifier  where the source domain  is used for minimizing the classification loss, on the contrary, the source domain  in this step is used to maximize the uncertainty loss , The parameters  of  are frozen and the parameters  of the encoder-decoder network  are trained. Maximizing the uncertainty loss forces the output image  to contain fewer domain-specific features than the input images. In doing so, the encoder-decoder network can remove the domain-specific features in the input images  and retain domain-invariant features in the output images . 

To maintain the overall similarity between the input and output images, we add a reconstruction loss  for the encoder-decoder network, where  is the reconstruction loss function. We use pixel-wise  loss as the default reconstruction loss for its simplicity and reasonably good performance. Other reconstruction losses could also be employed, such as pixel-wise  loss and perceptual losses \cite{johnson2016perceptual}. Detailed discussion is available in the supplementary material.

We then train the domain-invariant classifier  by minimizing the classification loss  on the output images of all the source domains, where  are the parameters of the domain-invariant classifier . This classification loss  also updates the encoder-decoder network to prevent the encoder-decoder network from losing the domain-invariant features due to the uncertainty loss. The uncertainty loss also has the potential to remove the domain-invariant features if it is difficult to separate the domain-specific features from the domain-invariant features.

Overall, when training the domain-specific classifiers we optimize and when learning the domain-invariant features, we optimize where ,  and  are hyperparameters that control the relative weight of these losses. 

For convenience, we denote the encoder-decoder network  and the domain-invariant classifier  as a domain-invariant model. In the testing phase, the domain-invariant model is used for classification on the target domain . 

\subsection{Explanation of LRDG with respect to existing theory}
\label{theory}

We first introduce the generalization risk bound for domain generalization \cite{albuquerque2019generalizing} and then further explain the effectiveness of our framework with respect to this.

Theoretically, the corresponding task for a domain is defined as a deterministic true labeling function , where .  and  are the input space and the label space, respectively.
We denote the space of the candidate hypothesis as , where a hypothesis . The risk of the hypothesis  on a domain  is defined as where  measures the difference between the hypothesis and the true labeling function.

Following \cite{albuquerque2019generalizing}, for the source domains , we define the convex hull  of the source domains as a set of mixture source distributions: . We also define  as the closest domain to the target domain .  is given by , where  is -divergence \cite{kifer2004detecting} that quantifies the distribution difference of two domains.
We use the following generalization risk bound \cite{albuquerque2019generalizing} for the target domain .

\begin{theorem}[Generalization risk bound \cite{albuquerque2019generalizing}] 
\label{theorem}
Given the previous setting, the following inequality holds for the risk ,  for any domain , where ,   and  is the minimum sum of the risks achieved by some  on  and .  measures the distribution difference between the source domains and the target domain.  is the maximum pairwise -divergence among source domains.
\end{theorem}

Theorem \ref{theorem} shows that the upper bound for the target domain depends on  and .
We show that our framework could lower the value of this generalization risk bound for a given domain generalization task. Recall that our encoder-decoder network maps the input images into a new image space. We denote the mapped source domains as  and the mapped target domain as . With the domain-specific classifiers, many domain-specific features are removed from the source domains and the features of the mapped source domains tend to be more domain-invariant. As a result, the mapped source domains  would have smaller distribution difference than the raw source domains, i.e. , indicating that  in Eq. \ref{risk} would probably be reduced. After removing the domain-specific features for each source domain, the mapped target domain  would be closer to the mapped source domains, so our framework could also be likely to reduce  in Eq. \ref{risk}. Concerning Theorem \ref{theorem}, these changes provide a principled explanation and warrant to the effectiveness of the proposed framework. We will demonstrate these changes in the experiment section (Sec. \ref{divergence}). 

\section{Experiments}

We evaluate our framework on three benchmark datasets and compare the performance with previous methods. After that, we study the domain divergence among the source and target domains.

\subsection{Datasets and settings}
\label{datasetting}

\textbf{Datasets.} 
We evaluate our framework on three object recognition datasets for domain generalization. PACS \cite{li2017deeper} contains four domains: Photo (\textbf{P}), Art Painting (\textbf{A}), Cartoon (\textbf{C}) and Sketch (\textbf{S}) with each domain covering seven categories including dog, elephant, giraffe, guitar, horse, house, and person. VLCS \cite{torralba2011unbiased} also has four domains: PASCAL VOC 2007 (\textbf{V}), LabelMe (\textbf{L}), Caltech (\textbf{C}) and Sun (\textbf{S}). The images belong to five categories of bird, chair, car, dog, and person. Office-Home \cite{venkateswara2017deep} has images from  categories over four domains including Art (\textbf{A}), Clipart (\textbf{C}), Product (\textbf{P}), and Real-World (\textbf{R}). For each dataset, following the literature, the experimental protocol is to consider three domains as the source domains and the remaining one as the target domain.

\textbf{Networks and loss functions.}
We use U-net \cite{ronneberger2015u} for the encoder-decoder network. Following the standard setting in the domain generalization literature \cite{dou2019domain,zhao2020domain,huangRSC2020}, we use AlexNet \cite{krizhevsky2017imagenet}, ResNet18 \cite{he2016deep} and ResNet50 \cite{he2016deep} as backbones for the domain-specific classifiers and the domain-invariant classifier. We use AlexNet for PACS and VLCS, ResNet18 for PACS and Office-Home, and ResNet50 for PACS. AlexNet and ResNet are pre-trained by ImageNet \cite{russakovsky2015imagenet} for all the experiments. We use the standard cross-entropy loss as the classification loss . For the uncertainty loss , we choose the entropy loss. For the reconstruction loss , we utilize the pixel-wise  loss. A detailed analysis of the loss functions is available in the supplementary material. 

\textbf{Training setting.}
The encoder-decoder network, the domain-specific classifiers, and the domain-invariant classifier are all optimized with Stochastic Gradient Descent. The source datasets are split into a training set and a validation set. The learning rate is decided by the validation set. We set  for all the experiments. We give equal weight to the classification loss and the uncertainty loss for training the domain-specific classifiers. For  and , we follow the literature \cite{dou2019domain,balaji2018metareg} and directly use the leave-one-domain-out cross-validation to select their values.

\textbf{Methods for comparison.}
We compare our framework with previous domain generalization works  including domain-invariant based methods \cite{li2018deep,wang2019learning,chattopadhyay2020learning,zhao2020domain,du2020learning,mahajan2021domain,rame2022fishr,bui2021exploiting} and other state-of-the-art methods \cite{d2018domain,balaji2018metareg,carlucci2019domain,li2019episodic,dou2019domain,zhou2020domain,nam2021reducing,huangRSC2020,borlino2021rethinking,zhang2021deep,cha2021swad} including data augmentation based methods \cite{nam2021reducing,zhou2020domain,borlino2021rethinking}, meta-learning based methods \cite{balaji2018metareg,li2019episodic,dou2019domain}, etc. 
The baseline is defined as the method of empirical risk minimization (ERM). It trains a classifier by minimizing the classification loss on all source domains.

\setlength{\tabcolsep}{4.8pt}
\begin{table}[ht]
\caption{Comparison with existing methods on PACS.}
\begin{center}
\scriptsize
\begin{tabular}{c|cccccccccc|c}
\hline
\multicolumn{12}{c}{AlexNet}\\
\hline
\multirow{2}{*}{Target} & Baseline & CIDDG & JiGen  & Epi-FCR & MASF & PAR & DMG & ER  & MetaVIB & RSC & LRDG\\
&  & \cite{li2018deep} & \cite{carlucci2019domain} & \cite{li2019episodic} & \cite{dou2019domain} & \cite{wang2019learning} & \cite{chattopadhyay2020learning} & \cite{zhao2020domain} & \cite{du2020learning} & \cite{huangRSC2020} & (ours) \\
\hline
\textbf{A} & 61.13 & 62.70 & 67.63 & 64.70 & 70.35 & 68.70 & 64.65 & 71.34 & 71.94 & 71.62 & \textbf{72.01}\\
\textbf{C} & 68.77 & 69.73 & 71.71 & 72.30 & 72.46 & 70.50 & 69.88 & 70.29 & 73.17 & \textbf{75.11} & 72.97\\
\textbf{P} & 87.96 & 78.65 & 89.00 & 86.10 & 90.68 & 90.40 & 87.31 & 89.92 & \textbf{91.93} & 90.88 & 89.50 \\
\textbf{S} & 58.63 & 64.45 & 65.18 & 65.00 & 67.33 & 64.60 & 71.42 & 71.15 & 65.94 & 66.62 & \textbf{74.86}\\
Avg. & 69.12 & 68.88 & 73.38 & 72.00 & 75.21 & 73.54 & 73.32 & 75.67 & 75.74 & 76.05 & \textbf{77.33}\\
\hline
\hline
\multicolumn{12}{c}{ResNet18}\\
\hline
\multirow{2}{*}{Target} & Baseline  & Epi-FCR & MASF & DMG & ER & MixStyle & SagNet & Stylized & StableNet & RSC & LRDG\\
&  & \cite{li2019episodic} & \cite{dou2019domain} & \cite{chattopadhyay2020learning} & \cite{zhao2020domain} & \cite{zhou2020domain} & \cite{nam2021reducing} & \cite{borlino2021rethinking} & \cite{zhang2021deep} & \cite{huangRSC2020} & (ours) \\
\hline
\textbf{A} & 77.95 & 82.10 & 80.29 & 76.90 & 80.70 & \textbf{84.10} & 83.58 & 82.73 & 81.74 & 83.43 & 81.88\\
\textbf{C} & 74.24 & 77.00 & 77.17 & \textbf{80.38} & 76.40 & 78.80 & 77.66 & 77.97 & 79.91 & 80.31 & 80.20\\
\textbf{P} & 95.89 & 93.90 & 94.99 & 93.35 & \textbf{96.65} & 96.10 & 95.47 & 94.95 & 96.53 & 95.99 & 95.21\\
\textbf{S} & 70.11 & 73.00 & 71.69 & 75.21 & 71.77 & 75.90 & 76.30 & 81.61 & 80.50 & 80.85 & \textbf{84.65}\\
Avg. & 79.54 & 81.50 & 81.03 & 81.46 & 81.38 & 83.70 & 83.25 & 84.32 & 84.69 & 85.15 & \textbf{85.48}\\
\hline
\hline
\multicolumn{12}{c}{ResNet50}\\
\hline
\multirow{2}{*}{Target} & Baseline & Metareg & DSON & DMG & ER & RSC & MatchDG & SWAD & Fishr & mDSDI & LRDG\\
&  & \cite{balaji2018metareg} & \cite{seo2020learning} & \cite{chattopadhyay2020learning} & \cite{zhao2020domain} & \cite{huangRSC2020} &   \cite{mahajan2021domain} & \cite{cha2021swad} & \cite{rame2022fishr} & \cite{bui2021exploiting} & (ours) \\
\hline
\textbf{A} & 82.89 & 87.20 & 87.04 & 82.57 & 87.51 & 87.89 & 85.61 & \textbf{89.30} & 88.40 & 87.70 & 86.57 \\
\textbf{C} & 80.49 & 79.20 & 80.62 & 78.11 & 79.31 & 82.16 & 82.12 & 83.40 & 78.70 & 80.40 & \textbf{85.78} \\
\textbf{P} & 95.01 & 97.60 & 95.99 & 94.49 & \textbf{98.25} & 97.92 & 97.94 & 97.30 & 97.00 & 98.10 & 95.57 \\
\textbf{S} & 72.29 & 70.30 & 82.90 & 78.32 & 76.30 & 83.35 & 78.76 & 82.50 & 77.80 & 78.40 & \textbf{86.59} \\
Avg. & 82.67 & 83.60 & 86.64 & 83.37 & 85.34 & 87.83 & 86.11 & 88.10 & 85.50 & 86.20 & \textbf{88.63} \\
\hline
\hline
\end{tabular}
\end{center}
\label{tab:pacs}
\end{table}

\subsection{Main results}
\label{mainresult}

PACS contains four domains of Art painting, Cartoon, Photo, and Sketch. These datasets have large domain gaps. The classification results of the previous methods and our framework are shown in Table \ref{tab:pacs}. Averagely, our framework consistently achieves the best performance in all three backbones compared with previous works. Especially on Sketch, the accuracy of our framework is averagely  better than the previous SOTA methods, showing superior performance. Our framework also obtains the best performance on Art painting in AlexNet and maintains the highest accuracy on Cartoon in ResNet50 (ours:  vs. SOTA: ). This indicates that removing the domain-specific features from the input images is an effective approach for domain generalization. We can also study whether the domain-specific features would benefit or hurt the performance on the unseen target domain by comparing with mDSDI \cite{bui2021exploiting}, as mDSDI uses the domain-specific features in addition to the domain-invariant features for domain generalization. We can see that our method significantly outperforms mDSDI on Cartoon and Sketch, and achieves a higher average classification performance than mDSDI in ResNet50. Meanwhile, mDSDI obtains better classification results than ours on Art and Photo. This shows that although Art and Photo may contain similar domain-specific features and these features would benefit each other, these domain-specific features would not benefit or even hurt Cartoon and Sketch. 

VLCS also contains four domains. Table \ref{tab:vlcs} shows the classification accuracy of the domain generalization methods using the AlexNet backbone. It can be seen that our framework obtains comparable performance to the best-performing methods, and outperforms the prior approaches on LabelMe and Sun. For Office-Home, ResNet18 is used as the backbone. The classification performance is shown in Table \ref{tab:officehome}. Our framework outperforms the previous methods and achieves the best average performance. Besides, our framework obtains the best performance on Art. These experimental results demonstrate that removing the domain-specific features can significantly improve the generalization performance.

\setlength{\tabcolsep}{7pt}
\begin{table}[ht]
\caption{Comparison with existing methods on VLCS using AlexNet backbone.}
\begin{center}
\scriptsize
\begin{tabular}{c|ccccccccc|c}
\hline
\multirow{2}{*}{Target} & Baseline & CIDDG & JiGen  & Epi-FCR & MASF & ER & MetaVIB & Stylized & RSC & LRDG\\
&  & \cite{li2018deep} & \cite{carlucci2019domain} & \cite{li2019episodic} & \cite{dou2019domain} & \cite{zhao2020domain} & \cite{du2020learning} & \cite{borlino2021rethinking} & \cite{huangRSC2020} & (ours) \\
\hline
\textbf{V} & 66.27 & 64.38 & 70.62 & 67.10 & 69.14 & 73.24 & 70.28 & 68.18 & \textbf{73.93} & 68.95 \\
\textbf{L} & 61.81 & 63.06 & 60.90 & 64.30 & 64.90 & 58.26 & 62.66 & 60.77 & 61.86 & \textbf{65.53} \\
\textbf{C} & 96.17 & 88.83 & 96.93 & 94.10 & 94.78 & 96.92 & 97.37 & 96.86 & \textbf{97.61} & 96.85 \\
\textbf{S} & 63.78 & 62.10 & 64.30 & 65.90 & 67.64 & 69.10 & 67.85 & 63.42 & 68.32 & \textbf{69.27} \\
Avg. & 72.01 & 69.59 & 73.19 & 72.90 & 74.11 & 74.38 & 74.54 & 72.31 & \textbf{75.43} & 75.15 \\
\hline
\end{tabular}
\end{center}
\label{tab:vlcs}
\end{table}

\setlength{\tabcolsep}{8pt}
\begin{table}[ht]
\caption{Comparison with existing methods on Office-Home using ResNet18 backbones.}
\begin{center}
\scriptsize
\begin{tabular}{c|cccccccc|c}
\hline
\multirow{2}{*}{Target} & Baseline & D-SAMs & JiGen & DSON & MixStyle & SagNet & Stylized & RSC & LRDG\\
&  & \cite{d2018domain} & \cite{carlucci2019domain} & \cite{seo2020learning} & \cite{zhou2020domain} & \cite{nam2021reducing} & \cite{borlino2021rethinking} & \cite{huangRSC2020} & (ours) \\
\hline
\textbf{A} & 52.23 & 58.03 & 53.04 & 59.37 & 58.70 & 60.20 & 58.71 & 58.42 & \textbf{61.73}\\
\textbf{C} & 46.20 & 44.37 & 47.51 & 45.70 & \textbf{53.40} & 45.38 &  52.33 & 47.90 & 52.43\\
\textbf{P} & 70.14 & 69.22 & 71.47 & 71.84 & \textbf{74.20} & 70.42 &  72.95 & 71.63 & 72.96\\
\textbf{R} & 73.07 & 71.45 & 72.79 & 74.68 & \textbf{75.90} & 73.38 &  75.00 & 74.54 & 75.89\\
Avg. & 60.41 & 60.77 & 61.20 & 62.90 & 65.50 & 62.34 & 64.75 & 63.12 & \textbf{65.75}\\
\hline
\end{tabular}
\end{center}
\label{tab:officehome}
\end{table}

\subsection{Domain divergence}
\label{divergence}

In this section, we investigate the distribution difference among the source domains and the target domain to demonstrate that our framework can effectively reduce domain divergence.

\subsubsection{Source domain divergence}
\label{divergence_src}

To investigate the distribution difference among the source domains, we compute the -divergence. Following the works of \cite{ben2007analysis,ben2010theory}, we can approximate the -divergence by a learning algorithm to discriminate between pairwise source domains. For example, with source domains  and , we label the samples of  by , and the samples of  by . We then train a classifier (e.g. linear SVM) to discriminate between these two domains. Given a test error  of this classifier, Proxy A-distance (PAD) is defined as , which can approximate the -divergence.

We follow the method from \cite{glorot2011domain,chen2012marginalized,ajakan2014domain} to compute the PAD. For a pair of source domains, we combine these domains and construct a new dataset. This dataset is randomly split into two subsets of equal size. One subset is used for training and the other one is used for test. We train a collection of linear SVMs (with different values of regularization parameters) on the training set and compute the errors  of all the SVMs on the test dataset. The lowest error  is used to compute the PAD.

\begin{figure*}[ht]
\centering
\subfloat[]{\includegraphics[width=0.47\linewidth]{images/px1_tmp.pdf}\label{fig:pad}}
\hfil
\subfloat[]{\includegraphics[width=0.45\linewidth]{images/px_tar1.pdf}\label{fig:pad_tar}}
\caption{Proxy A-distance (PAD) on PACS. \textit{x} axis: PAD computed upon the baseline model; \textit{y} axis: PAD computed upon our framework. \textbf{(a)} PAD of pairwise source domains. \textit{Blue dots}: PAD for Art, Cartoon, and Photo (Sketch as target domain). \textit{Orange dots}: PAD for Cartoon, Photo, and Sketch (Art as target domain). \textit{Red dots}: PAD for Art, Photo, and Sketch (Cartoon as target domain). \textit{Green dots}: PAD for Art, Cartoon, and Sketch (Photo as target domain). \textbf{(b)} PAD of pairwise source-target domains. For a target domain (e.g. Art), the corresponding source domain is the closest mixture source domain (e.g. mixture of Cartoon, Photo, and Sketch) to the target domain. The PAD is computed on the mixture source domain and the target domain.}
\label{fig:pads}
\end{figure*}
Fig. \ref{fig:pad} compares the PAD of the raw source domains and the mapped source domains. The experiments are conducted on PACS with the AlexNet backbone. For the raw source domains, we extract features from the baseline model (i.e. the last pooling layer of AlexNet) to train the linear SVMs, while for the mapped source domains, we use features from our domain-invariant classifier. In the figure, each dot represents a pair of source domains (e.g. Art and Photo). It has two values: the PAD of the source domain pair obtained upon the baseline model (\textit{x} axis) and the PAD of the same pair computed upon our framework (\textit{y} axis). All the dots are below the diagonal meaning that the PAD values of the mapped pairwise source domains are lower than the raw pairwise source domains. With our framework, the mapped source domains become harder to be distinguished, indicating that removing the domain-specific features reduces the distribution difference among the source domains. This also proves that  in the generalization risk bound (Eq. \ref{risk}) would be reduced by our framework.

\subsubsection{Source-Target domain divergence}

We also investigate the distribution difference between the source domains and the target domain. Specifically, we measure the domain divergence between the target domain and the closest mixture source domain  to the target domain.
To obtain this mixture source domain, as defined in Sec. \ref{theory}, we need to find  for each source domain , so that , where  and . Because  can be any real value in the interval of , traversing all values to find the desired  is impossible. Therefore, We limit the values of  to the set of  (11 values in total), and find the setting of  that can obtain . 
\setlength{\tabcolsep}{6pt}
\begin{wraptable}{r}{7.6cm}
\caption{The settings of  for the closest mixture source domains to the target domains on PACS.}
\begin{center}
\footnotesize
\begin{tabular}{l|l|l|c}
\hline
\multicolumn{3}{c|}{Source} & Target\\
\hline
:  & :  & :  &  \\
\hline
\hline
Cartoon:  & Photo:  & Sketch:  & Art \\
\hline
Art:  & Photo:  & Sketch:  & Cartoon \\
\hline
Art:  & Cartoon:  & Sketch:  & Photo \\
\hline
Art:  & Cartoon:  & Photo:  & Sketch \\
\hline
\end{tabular}
\end{center}
\label{tab:pi}
\end{wraptable}
We traverse all possible settings of  and obtain all possible mixture source domains . For each setting of , we random sample  samples from each source domain  and concatenate all these samples into a mixture source dataset.  is the number of samples in the target domain. By this design, each mixture source domain has an equal number of samples to the target domain. Similar to Sec. \ref{divergence_src}, we also train classifiers (i.e. linear SVMs) to discriminate between each mixture source domain and the corresponding target domain. The linear SVMs are trained on image features extracted from the baseline model. We then use the test error to compute the PAD between each mixture source dataset and the target dataset. The mixture source domain with the lowest PAD is the closest mixture source domain  to the target domain. The detailed settings of  for the closest mixture source domains to the corresponding target domains on PACS are listed in Table \ref{tab:pi}. For convenience, we denote the closest mixture source domain  and the target domain  together as a source-target domain pair.

Fig. \ref{fig:pad_tar} shows the PAD of the raw source-target domains and the mapped source-target domains. Similar to Sec. \ref{divergence_src}, for the raw source-target domains, we extract the image features from the baseline model to train the linear SVMs. To compute the PAD of the mapped source-target domains, we extracted the image features from our domain-invariant classifier to train the linear SVMs. In the figure, each dot represents a source-target domain pair (e.g. \{Cartoon, Photo, Sketch\}, Art). We can see that all the dots are below the diagonal. The PAD values of the mapped source-target pairs are lower than the raw source-target pairs. This indicates that the distribution difference between the source domains and the target domain is reduced by our framework.  in the generalization risk bound (Eq. \ref{risk}) would be lowered. Removing the domain-specific features from the source domains can also reduce the distribution difference between the source domains and the target domain. 

In summary, our framework can reduce the distribution difference not only among the source domains but also between the source domains and the target domain. This also demonstrates that our framework could effectively lower the value of the generalization risk bound by reducing  and .

\section{Related work}

Domain generalization is a challenging task that requires models to be well performed on unseen domains.
One common approach is to learn domain-invariant features among the source domains. Previous methods aim to distill the domain-invariant features, but they do not clearly inform the DNNs that the domain-specific features shall be effectively removed. Muandet et al. \cite{muandet2013domain} propose to reduce the domain dissimilarity by a kernel-based method. Ghifary et al. \cite{ghifary2015domain} reduce dataset bias by extracting features that are shared among the source domains with a multi-task autoencoder network. Li et al. \cite{li2018domain} utilize Maximum Mean Discrepancy (MMD) on adversarial autoencoders to align the distributions across source domains. Li et al. \cite{li2018deep} design an end-to-end conditional invariant deep neural network that minimizes the discrepancy of conditional distributions across domains. Arjovsky et al. \cite{arjovsky2019invariant} develop Invariant Risk Minimization (IRM) that uses a causal mechanism to obtain the optimal invariant classifier upon the representation space. Chattopadhyay et al. \cite{chattopadhyay2020learning} propose to learn domain-specific binary masks to balance the domain-invariant and domain-specific features for the prediction of unseen target domains. Zhao et al. \cite{zhao2020domain} propose an entropy regularization method to learn the domain-invariant conditional distributions by using a classification loss and a domain adversarial loss. Du et al. \cite{du2020learning} develop a probabilistic meta-learning method that learns domain-invariant representations with meta variational information bottleneck principle derived from variational bounds of mutual information. Mahajan et al. \cite{mahajan2021domain} assume that domains are generated by mixing causal and non-causal features and that the same object from different domains should have similar representations. Based on this, they propose a new method called MatchDG to build a domain-invariant classifier by matching similar inputs. Rame et al. \cite{rame2022fishr} match the gradients among the source domains to minimize domain invariance.  Unlike the above works, Bui et al. \cite{bui2021exploiting} assume that, besides the domain-invariant features, some domain-specific features also provide useful information for the target domain. However, this cannot always be guaranteed since the target domain is unseen. For example, the backgrounds in the domain Photo may benefit the domain Art, but they would not benefit or even hurt the domain Sketch. Our framework follows the common assumption that the domain-invariant features are generalized across domains, regardless of the effect of the domain-specific features \cite{li2018deep,arjovsky2019invariant,zhao2020domain}.

Recent papers demonstrate that CNNs tend to classify objects based on features from superficial local textures and backgrounds, while humans rely on global object shapes for classification \cite{jo2017measuring,geirhos2018imagenet}. To address this issue, some methods aim to capture the global object shapes from the images. These methods are proposed based on the assumption that the local textures and backgrounds are the domain-specific features, and the global object shapes are the domain-invariant features. Wang et al. \cite{wang2019learningproject} extract semantic representations by penalizing features extracted with gray-level co-occurrence matrix (GLCM) which are sensitive to texture. Wang et al. \cite{wang2019learning} penalize the earlier layers of CNNs from learning local representations and make the CNNs rely on the global representations for classification. Although addressing the superficial local features is a promising approach, the superficial local features may be one kind of domain-specific features and other forms of domain-specific features may also exist. Compared with these methods, our framework is proposed to address the more general domain-specific features rather than the superficial local features. 

\section{Conclusion}
\label{conclusion}

In this work, we propose a new approach that aims to explicitly remove domain-specific features for domain generalization. To this end, we develop a novel domain generalization framework that learns the domain-invariant features by actively removing the domain-specific features from the input images. We also experimentally verify the reduced domain divergence among the source domains and the target domain brought by our approach. Experiments show that our framework achieves strong performance on various datasets compared with existing domain generalization methods.

Despite the advantages of our framework, it has some potential limitations to be further addressed. We need to train the same number of domain-specific classifiers as the source domains. When there are more source domains, more computational resources will be required to train the domain-specific classifiers. This may be addressed by designing a novel domain-specific classifier that can learn the domain-specific features of multiple source domains simultaneously. Another limitation of our framework is that it cannot remove the domain-specific features of the unseen target domain. These domain-specific features should also be removed since they would negatively affect the classification performance. For example, our framework performs slightly worse than the baseline when Photo is the target domain (as shown in Table \ref{tab:pacs}). This may be because Photo contains rich domain-specific features compared with the source domains, and our framework would make incorrect predictions due to these domain-specific features. Besides, this result also shows that domain-specific knowledge is useful for Photo. As the target domain is not available during training, how to remove the domain-specific features from the target domain and whether to retain the domain-specific features of the source domains will be challenging issues to be addressed. One possible future work may be to remove the domain-specific features in a latent feature space. To achieve this, the framework may need to be adjusted, including the domain-specific classifiers and the domain-invariant classifier. The encoder-decoder network incurs extra computational overhead, but performing on a latent space may have the benefit that we may no longer need the encoder-decoder network and the overall framework can be computationally more efficient.

\section*{Acknowledgment}

Yu Ding was supported by CSIRO Data61 PhD Scholarship and the University of Wollongong International Postgraduate Tuition Award. This research was undertaken with the assistance of resources and services from the National Computational Infrastructure (NCI) and the CSIRO Accelerator Cluster-Bracewell.

\bibliographystyle{plain}
\bibliography{neurips_2022_arxiv}

\clearpage




\end{document}