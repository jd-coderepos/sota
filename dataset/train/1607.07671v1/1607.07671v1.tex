\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

\mypartop{Datasets.} We evaluate our method on two challenging datasets:
SIFT Flow~\cite{liu11pami} and PASCAL Context~\cite{mottaghi14cvpr}.
SIFT Flow contains 33
classes in 2688 images.  The dataset is known for its extreme class imbalance~\cite{liu11pami,farabet13pami,eigen15iccv}.
We use the provided fixed split into 2488 training images and 200 test images.

PASCAL Context provides complete pixel-level annotations for both things and stuff classes in the popular PASCAL VOC
2010~\cite{everingham15ijcv} dataset. It contains 4998 training and 5105 validation images.  As there
is no dedicated test set available, we use the validation images exclusively for testing.  We use
the 59 classes plus background commonly used in the literature~\cite{dai15cvpr,dai15iccv,long15cvpr,zheng15iccv}.

\mypar{Evaluation measures.}
Semantic segmentation methods typically measure global accuracy and class-average accuracy. Global accuracy is the percentage of correctly labeled pixels in the dataset. But since class frequencies typically follow a power-law
distribution, it is mostly influenced by a few common classes.
Class-average accuracy instead takes all classes into account equally and it is generally considered a better measure.
It first computes the accuracy for each class separately, and then averages over classes. Both measures are standard for SIFT Flow.
The most common evaluation measure on PASCAL Context is mean Intersection-over-Union~(IOU)~\cite{everingham15ijcv}.
For each class one divides the number of pixels of the intersection of the predicted and ground truth class by their union.
Then the average is taken over classes.

\mypar{Network.}
We use the state-of-the-art classification network VGG-16 \cite{simonyan15iclr}
pre-trained for image classification on ILSVRC 2012~\cite{russakovsky15ijcv}. We use the layers up to CONV5,
discarding all higher layers, as the basis of our network.
We then append a free-form ROI pooling layer~(Section~\ref{sec:freeform}),
a region-to-pixel layer, a softmax layer and pixel-level loss~(Section~\ref{sec:endtoend}, Fig.~\ref{fig:cnn-architectures}c).
To include local context, we combine region and entire bounding box using separate weights~(Section~\ref{sec:freeform}).

\mypar{Regions.}
We use Selective Search~\cite{uijlings13ijcv}, which delivers three sets of region proposals, one per color space~(RGB, HSV, LAB).
During training we change the set of region proposals in each mini-batch to have a more diverse set of proposals without the additional overhead of having three times as many regions.
We use region proposals with a minimum size of 100 pixels for SIFT Flow, and 400 pixels for PASCAL Context.
This results in an average of 370 proposals for SIFT Flow and 150 proposals for PASCAL Context, for each of the three color spaces.
Additionally we use all ground truth regions at training time.
This is especially important for very small objects that are not tightly covered by region proposals.

\mypar{Training.}
The network is trained using Stochastic Gradient Descent~(SGD) with momentum.
For 20 epochs we use a learning rate of 1e-3, followed by 10 epochs using learning rate 1e-4.
All other SGD hyperparameters are taken from Fast R-CNN~\cite{girshick15iccv}.
We use either an inverse-class frequency weighted loss~(referred to as {\em balanced} below) or a natural frequency weighted
loss~({\em unbalanced}).

\subsection{Main results}

\mypartop{SIFT Flow.}
We compare our method to other works on SIFT Flow test in Table~\ref{tab:experiments-siftflow}.
We first compare in the balanced setting, which takes rare classes into
account.
Hence we train our model for the loss described in Section~\ref{sec:rareclasses} and compare to methods using class-average accuracy.
We achieve $64.0\%$, which substantially outperforms the previous state-of-the-art, including the fully convolutional method~\cite{eigen15iccv} by $+8.3\%$ and the region-based method~\cite{caesar15bmvc} by $+8.4\%$.

We also compare in the unbalanced setting using global accuracy, which mainly measures performance on common classes.
Hence we train our model for the loss in Eq.~\eqref{eq:loss-advanced}.
This yields a competitive $84.3\%$ global accuracy, outperforming most previous methods, and coming close to the state-of-the-art~\cite{eigen15iccv} ($86.8\%$).

\begin{table}[t]
\centering
\small
\adjustbox{valign=t}{
\begin{minipage}[t]{.47\linewidth}
\raggedright
\begin{tabular}{L{2.1cm}l C{1cm} C{1cm} C{1cm}}
Method				&	Year	&	Class Acc.	&   Global Acc.	\\
\hline
\hline
Byeon~\cite{byeon15cvpr}	&	2015	&	22.6		&	68.7	\\
Gould~\cite{gould14eccv}	&	2014	&	25.7		&	78.4	\\
Tighe~\cite{tighe10eccv}	&	2010	&	29.4		&	76.9	\\
Pinheiro~\cite{pinheiro14icml}	&	2014	&	30.0		&	76.5	\\
Gatta~\cite{gatta14cvprw}	&	2014	&	32.1		&	78.7	\\
Singh~\cite{singh13cvpr}	&	2013	&	33.8		&	79.2	\\
Shuai~\cite{shuai15cvpr}	&	2015	&	39.7		&	80.1	\\
Tighe~\cite{tighe13cvpr}	&	2013	&	41.1		&	78.6	\\
Keke\c{c}~\cite{kekec14bmvc}	&	2014	&	45.8		&	70.4	\\
\\
\end{tabular}
\end{minipage}}
\hfill
\adjustbox{valign=t}{
\begin{minipage}[t]{.47\linewidth}
\raggedleft
\begin{tabular}{L{2.1cm} C{1cm} C{1cm} C{1cm}}
Method				&	Year	&	Class Acc.	&   Global Acc.	\\
\hline
\hline
Sharma~\cite{sharma14nips}	&	2014	&	48.0		&	79.6	\\
Yang~\cite{yang14cvpr}		&	2014	&	48.7		&	79.8	\\
George~\cite{george15cvpr}	&	2015	&	50.1		&	81.7	\\
Farabet~\cite{farabet13pami}	&	2013	&	50.8		&	78.5	\\
Long~\cite{long15cvpr}		&	2015	&	51.7		& \textbf{85.2}	\\
Sharma~\cite{sharma15cvpr}	&	2015	&	52.8		&	80.9	\\
Caesar~\cite{caesar15bmvc}	&	2015	&	55.6		&	-	\\
Eigen~\cite{eigen15iccv}	&	2015	&	55.7		& \textbf{86.8}	\\
Ours				&	2016	&  \textbf{64.0}	& \textbf{84.3}	\\
\\
\end{tabular}
\end{minipage}}

\caption{
\textit{
Evaluation on SIFT Flow test.
We show results for our model trained for either a balanced or an unbalanced loss.
We also show results of previous works, where we report the maximum result for each metric if multiple results are given.
}
}
\label{tab:experiments-siftflow}
\end{table}

\mypar{PASCAL Context.}
We also evaluate our method on the recent PASCAL Context dataset~\cite{mottaghi14cvpr}.
In Table~\ref{tab:experiments-pascalcontext} we show the results using either a balanced or an unbalanced loss.
Our balanced model achieves $49.9\%$ class-average accuracy, outperforming the only work that reports results for that measure~\cite{long15cvpr} by $+3.4\%$.
Our unbalanced model achieves competitive results on global accuracy~($62.4\%$) and reasonable results on mean IOU~($32.5\%$).

\begin{table}[t]
\centering
\begin{tabular}{l C{1.4cm} C{1.6cm} C{1.8cm} C{1.6cm}}
Method						&	Year	&	Class Acc.	&	Global Acc.	&	Mean IOU	\\
\hline
\hline
O2P~\cite{carreira12eccv} 			&	2012	&	-		&	-		&	18.1		\\
Dai et al.~\cite{dai15cvpr}			&	2015	&	-		&	-		&	34.4		\\
Long et al.~\cite{long15cvpr}			&	2015	&	46.5		&	\textbf{65.9}	&	35.1		\\
Dai et al.~\cite{dai15iccv}			&	2015	&	-		&	-		&	35.7		\\
Zheng et al.~\cite{zheng15iccv}			&	2015	&	-		&	-		&	\textbf{39.3}	\\
\hline
Dai et al. (add. boxes)~\cite{dai15iccv}	&	2015	&	-		&	-		&	40.5		\\ \hline 
Ours						&	2016	&	\textbf{49.9}	&	62.4		&	32.5		\\
\\
\end{tabular}
\caption{
\textit{
Evaluation on PASCAL Context validation.
We show results using a balanced and an unbalanced version of our method, as well as the current
state-of-the art, where we always report the maximum result for each metric.
O2P results are from the errata of Mottaghi et al.~\cite{mottaghi14cvpr}.
Dai et al.~\cite{dai15iccv} train using additional bounding box annotations.
}
}
\label{tab:experiments-pascalcontext}
\end{table}

\mypar{Qualitative analysis.}

Fig.~\ref{fig:examples-siftflow-balanced} and~\ref{fig:examples-pascalcontext} show example
labelings generated by our method on SIFT Flow test and PASCAL Context validation.
Notice how our method accurately adheres to object boundaries, such as buildings~(Fig.~\ref{fig:examples-siftflow-balanced}e,~\ref{fig:examples-siftflow-balanced}h), birds~(Fig.~\ref{fig:examples-pascalcontext}a,~\ref{fig:examples-pascalcontext}c) and boat~(Fig.~\ref{fig:examples-pascalcontext}i).
This is one of the advantages of using a region-based approach.
Furthermore, our method correctly identifies small objects like pole (Fig.~\ref{fig:examples-siftflow-balanced}a) and the streetlight~(Fig.~\ref{fig:examples-siftflow-balanced}d).
This is facilitated by our method's ability to adaptively select the scale on which to do recognition.
Finally, notice that our method sometimes even correctly labels parts of the image missing in the ground truth, such as fence~(Fig.~\ref{fig:examples-siftflow-balanced}d) and cat whiskers~(Fig.~\ref{fig:examples-pascalcontext}d).

\begin{figure}
\begin{minipage}{\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{images/Examples-SIFTFlow-balanced-best.pdf}
\caption{
\textit{
Example labelings on SIFT Flow test.
We show an image, the ground truth labeling and the output of our balanced model.
}
}
\label{fig:examples-siftflow-balanced}
\end{minipage}

\begin{minipage}{\linewidth}
\centering
\includegraphics[width=1.0\textwidth]{images/Examples-PASCALContext60-unbalanced-best.pdf}
\caption{
\textit{
Example labelings on PASCAL Context validation.
We show an image, the ground truth labeling and the output of our unbalanced model.
}
}
\label{fig:examples-pascalcontext}
\end{minipage}
\end{figure}

\subsection{Extra analysis}
\label{sec:extra-analysis}

\mypar{Accuracy at object boundaries.}
Following~\cite{kohli09ijcv,Krahenbuhl11nips},
we evaluate the performance on image pixels that are within 4 pixels of a ground truth object boundary.
We compare our method to the MatConvNet~\cite{vedaldi15mm} reimplementation
of Fully Convolutional Networks (FCN)~\cite{long15cvpr} in Table~\ref{tab:experiments-objectboundaries}.
On SIFT Flow test, FCN-16s obtains 37.9\% class-average accuracy on boundaries, while our method gets to 57.3\%.
When evaluated on all pixels in the image, FCN-16s brings 49.3\%, vs 64.0\% by our method.
Hence, our method is +19.4\% better on boundaries and +14.7\% on complete images.
Analogously, on PASCAL Context we get +4.9\% on boundaries and +1.8\% on complete images.
Since our improvements are consistently larger on object boundaries, we conclude that our method is especially good at capturing them, compared to the basic FCN architecture (Fig.~\ref{fig:cnn-architectures}a).

\begin{table}[t]
\adjustbox{valign=t}{
\begin{minipage}[t]{.46\linewidth}
\raggedright
\begin{tabular}{L{1.4cm} C{2.0cm} C{1.9cm}}
		& Boundaries	&	Full image	\\
\hline
\hline
FCN-16s		& 	\;\;37.9	&	\;\;49.3	\\
Ours		& 	\;\;57.3	&	\;\;64.0	\\
\emph{difference} &	\emph{+19.4}	&	\emph{+14.7}	\\
\hline
FCN-16s		&	\;\;34.0	&	\;\;48.1	\\
Ours		&	\;\;38.9	&	\;\;49.9	\\
\emph{difference} &	\emph{\,+4.9}	&	\emph{\,+1.8}	\\
\\
\end{tabular}

\end{minipage}}
\hfill
\adjustbox{valign=t}{
\begin{minipage}[t]{.50\linewidth}
\raggedleft
\begin{tabular}{L{2.0cm} C{2.4cm} C{1.5cm}}
    ROI pooling	& 		&	Class Acc.	\\
\hline
\hline
bounding box	& 			&	62.3		\\
region		& 			&	62.8		\\
region + box 	& tied weights		&	63.4		\\
region + box	& separate weights	&	64.0		\\
\hline
bounding box 	& purely rect. 	& 	59.3 		\\
\\
\end{tabular}

\end{minipage}}

\adjustbox{valign=t}{
\begin{minipage}[t]{.46\linewidth}
\raggedright
\caption{
\textit{
Class-average accuracy at object boundaries on SIFT Flow test~(top) and PASCAL Context validation~(bottom).
Improvements on boundaries are consistently larger than on full images.
}
}
\label{tab:experiments-objectboundaries}
\end{minipage}}
\hfill
\adjustbox{valign=t}{
\begin{minipage}[t]{.50\linewidth}
\raggedleft
\caption{
\textit{
Results on SIFT Flow test using free-form pooling, bounding box pooling or both.
We also report results when regions are rectangular even in the region-to-pixel layer (purely rectangular).
}
}
\label{tab:experiments-regioncontext}
\end{minipage}}
\end{table}

\mypar{End-to-end training.}
Our region-to-pixel layer enables end-to-end training of region-based semantic segmentation models.
We analyze how this end-to-end training influences performance, by comparing the baseline model
(Fig.~\ref{fig:cnn-architectures}b) to our model (Fig.~\ref{fig:cnn-architectures}c). To isolate the effect of end-to-end training, in
both models we perform ROI pooling on the bounding box only.
Hence all components of the two models are identical, apart from the region-to-pixel layer and the loss they are trained for.
On SIFT Flow test the baseline model achieves a global accuracy of $60.9\%$, compared to our $83.7\%$. 
We conclude that end-to-end training yields considerable accuracy gains over the baseline architecture in Fig.~\ref{fig:cnn-architectures}b.

\mypar{Softmax before max.}
Our application of the max before the softmax (Eq.~\ref{eq:model-advanced}) enables us to recognize each object at its appropriate scale~(Sec.~\ref{sec:endtoend}).
However, using the softmax before the max (Eq.~\ref{eq:model-simple}) yields an alternative model.
Interestingly, on SIFT Flow test our proposed order outperforms the alternative by $+8.7\%$ class-average accuracy.

\mypar{Importance of multi-scale regions.}
We argue that overlapping, multi-scale regions are important to unleash the full potential of region-based methods.
To show this, we train and test our model with non-overlapping regions~\cite{felzenszwalb04ijcv}.
This yields $60.0\%$ class-average accuracy on SIFT Flow test,
which is below the results when using multi-scale overlapping regions ($64.0\%$ class-average accuracy).

\mypar{Free-form versus bounding box representations.}

We analyze the influence of the different representations resulting from different ROI pooling
methods (Sec.~\ref{sec:freeform}). Keeping all else constant, we compare
(I) free-form ROI pooling, (II) bounding box ROI pooling, (III) their combination with tied weights
and (IV) their combination with separate weights. Results are shown in
Table~\ref{tab:experiments-regioncontext}.

Free-form representations perform $+0.5\%$ better than bounding box representations, demonstrating that
focusing accurately on the object is better.
Their combination does even better, yielding another $+0.6\%$ gain with tied weights (same number of model parameters) and $+0.6\%$ with separate weights. Hence both representations are complementary and best treated separately.

In all above experiments the region-to-pixel layer operates on free-form regions.
To verify the importance of the free-form regions themselves, we perform an extra experiment using purely rectangular regions (both in the region-to-pixel layer and during ROI pooling).
This lowers class-average accuracy by -4.7\%, demonstrating the value of free-form regions.