Any nontrivial physical, hardware or software system has a dashboard continually observing the system variables, and updating various measurements.  In such applications, data arrives over time, and we need to continually output the result of some analysis $f$ 
for each time instant $j$ on all data seen thus far.  This challenges privacy of analysis because the same function is computed on several deltas of the data and the collection of these function values can potentially leak information.
Recently, the notion of differential privacy was adopted to address this challenge~\cite{dwork-continual,chan2010private}, and we extend that study.


~\cite{dwork-continual,chan2010private} identified the problem of computing the running sum of a series of $0/1$ updates as an important technical primitive, formulated differential privacy of computing these running sums, and presented upper and lower bounds on accuracy of $\eps$-differentially private algorithms for computing running sums. They showed that an additive accuracy of $O(\frac{1}{\eps} \log^{2}  T)$ with constant probability is possible for the running sums problem, and that $\Omega(\log T)$ additive error was necessary to answer privately all running sum queries for all time steps $j \in [1, T]$. 

\medskip
\noindent
{\bf Power of Running Sums.} The sums problem 
captures many analyses by  applying a suitable predicate  to the data items that map them to $0/1$. 
For example, at time $j$, say data item $D_j=(u_j,m_j)$ is the user ID $u_j$ and the name of the movie $m_j$ watched in an online service by $u_j$ at that time. A natural predicate is ${\cal P}_m(D_j)=1$ if $m_j=m$ and $0$ otherwise; the running sum with this predicate counts the number of user IDs that watched a particular film $m$. Another natural predicate is 
${\cal P}_u(D_j)=1$ if $u_j=u$ and $0$ otherwise; this running sum counts the number of movies watched by a user $u$.  The predicates can be different for different items. E.g., 
${\cal P}_{j,u}(D_j)=1$ if $u_j=u$ and $j \in [9,17]$ will filter movies watched by user $u$ during business hours $9$ AM to $5$ PM.  Even more generally, ${\cal P}$ may be 
a machine learning based classification routine such as whether a
click by any user from a certain IP address on an Internet ad is a
spam or not, and the running sum will count the total number of spam clicks from the given IP address. \hfill$\blacksquare$

\medskip
Our point of departure from prior work is that in reality, monitoring applications emphasize recent data more than data long past.  For example, monitoring applications typically consider a  ``window''' of continual observations such as, last $T$ time units, or last $W$ updates.  More generally, they discount items based on how far they are in the past, and analyze decayed data.
The commonly useful decay models  are exponential and polynomial 
decays~\cite{datar2002maintaining,cohen2003maintaining}. 

\junk{
and algorithms for various data stream decay  models~\cite{cohen2003maintaining}. See the papers on data streaming over sliding windows~\cite{datar2002maintaining} for motivation and applications of the window model and~\cite{cohen2003maintaining} for motivation of the exponential decay model as well as discussion of the usefulness and motivation of considering a polynomial decay model, as well as applications for both models.
}

\medskip
\noindent
{\bf Our results.}
Motivated by this, we consider differential privacy of continual observations over windows and decayed data.  
\junk{
\begin{packed_item}
\item \noindent
{\em What is the desired accuracy of analyses?} We wish accuracy of analysis not with respect to the entire stream of updates thus far, but only with respect to the window of our interest. More generally, we wish our analyses to be accurate with respect to the decayed sums, and not running 
sums without discounting the past.

\item \noindent
{\em What should be the privacy guarantee?} This is tricky.  The immediate instinct is to define 
differential privacy using functions over the window, that is, for a function $f$ over the data items $D_{j-W+1},\ldots,D_j$, require that we compute a differentially private approximation $\tilde{f}$. 
Unfortunately this is not sufficient. For every such $\tilde{f}$, there exists  a $\tilde{f}^*$ which satisfies the notion of differential privacy over the window, but completely compromises all the prior 
data.  For example, one can prove that such an $\tilde{f}^*$  is given
by $\tilde{f}^*(.)=\tilde{f}.D_1D_2\cdots D_{j-W}$. 
Therefore, we have to require differential privacy explicitly over all prior data. This argument holds for other decay models as well. 
\end{packed_item}

\junk{
To sum, we still need differential privacy over the entire history of data, but simultaneously require accuracy over the window or decayed data, which is the challenge.  
Drawing an analogy with the data streams literature, the window stream model lies between increment-only updates  where all data seen thus far is considered, and fully dynamic updates where updates are comprised of arbitrary inserts and deletes, since a shifting window may be thought of as adding a data item on  the ``right'' and deleting a specific item, the one on the  ``left''. It is known in streaming that certain problems that cannot be solved with fully dynamic data can be solved on window streams~\cite{datar2002maintaining}. A similar issue arises with differential privacy, too: do the window or decayed functions, which are non-monotonic in a specific way, lie between the monotonic functions studied thus far (like running sums and the general transformation proposed in~\cite{dwork-continual}) for which differentially private and accurate solutions are possible, and arbitrary non-monotonic functions where such solutions are not known?  So, in addition to the applications perspective,  differentially private analysis of a window of continual observations  is interesting from a technical perspective as well.
}

To sum, we still need differential privacy over the entire history of data, but simultaneously require accuracy over the window or decayed data.  
We study the predicate sums problem from this perspective.
}
At each time step $i$ the algorithm receives a bit $x_i$; at each time
step $j$, the algorithm is required to report an approximation
$\hat{F}(x_1, \ldots, x_{j})$ to a function $F(x_1, \ldots, x_{j})$
and be $\eps$-differentially private over the entire data seen thus far. We use the notion of $(\delta,
\gamma)$-utility, satisfied by algorithms that at any time step $j$
output a value $\hat{F}(x_1, \ldots, x_j)$ which is within $\delta$
absolute error from $F(x_1, \ldots, x_j)$ with probability
$1-\gamma$. Below we summarize our results for sufficiently small $\gamma$ (results for larger $\gamma$ can be found in the body of the paper):
\begin{itemize}
  \setlength{\itemsep}{0.5pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
\item ({\em Window Sum})
  The \emph{window sum} problem with window
  size $W$ requires estimating $F_w(j, W) = \sum_{i = j - W +
    1}^j{x_i}$ for each $j$.  Further, the whole sequence $F_w$ of
  outputs, for all $j$, should be $\eps$-differentially private.
  
  \smallskip
We present an algorithm that achieves $(\delta, \gamma)$-utility for
$\delta = O(\frac{1}{\eps}\log W \log \frac{1}{\gamma})$ (in the regime
$\log W \geq \log \frac{1}{\gamma}$ ). 
While a window sum can be reduced to computing the
difference of two running sums, existing running sum
algorithms~\cite{dwork-continual,chan2010private} achieve error
$\delta = \Theta(\frac{1}{\eps}\log T\log \frac{1}{\gamma})$, which can be much
larger than the range $W$ of $F_w$, and therefore, as bad as the
trivial algorithm that outputs a fixed value independently of the input.

\smallskip
We also present a lower bound of $\Omega(\min\{W/2, \frac{1}{\eps}\log
\frac{1}{\gamma}\})$. Note that the dependence on the error probability
$\gamma$ is optimal. The $W/2$ term in the lower bound is unavoidable,
as the trivial algorithm which outputs $W/2$ at every time step
achieves additive approximation $W/2$ and is perfectly private. This
lower bound generalizes a previous lower for the running sum
problem~\cite{dwork-continual}. 

\smallskip
\item ({\em Exponential Decay}) The \emph{exponential decay sum}
  problem is to estimate $ F_e(j, \alpha) = \sum_{i = 1}^j{x_i
    \alpha^{j-i}}$ accurately, while the whole sequence $F_e$ of
  outputs, for all $j$, should be $\eps$-differentially private.

\smallskip
  We present an algorithm that achieves $(\delta, \gamma)$-utility
  with   $\delta = O(\frac{1}{\eps}\log \frac{\alpha}{1-\alpha} \log
  \frac{1}{\gamma})$.  We also  present a lower bound of
  $\Omega\left(\min\left\{\frac{\alpha}{1-\alpha}, \frac{\log
        (1/\gamma)}{\eps}\right\}\right)$. Once again, the dependence
  on the error probability $\gamma$ is optimal. Unlike $F_w$, $F_e$ at each time step
  depends on the entire sequence of updates; nevertheless, our
  algorithm achieves bounded error, polylogarithmic in the range of $F_e$.

\smallskip
\item ({\em Polynomial Decay}) The \emph{polynomial decay sum} problem
  is to estimate $ F_p(j, c) = \sum_{i = 1}^j{\frac{x_i}{(j-i+1)^c}}$
  accurately, while the whole sequence $F_p$ of outputs, for all $j$,
  should be $\eps$-differentially private.

\smallskip
  We present an algorithm that for each $j$ returns $(1\pm\beta) F_p(j,c) \pm
  \left(\frac{1}{c\beta^2}\log\frac{1}{1-\beta}\right)\log
  \frac{1}{\gamma}$ with probability $1-\gamma$. We also present a lower
  bound of $\Omega\left(1 - \frac{\eps^{c-1}}{\log^{c-1}
      (1/\gamma)}\right)$ against {\em purely} additive error. Polynomial decay presents a greater challenge than window sums or exponential decay since there is no direct way to combine a polynomial decay sum over an
interval $[a, b]$ and 
$[b, c]$ into a polynomial decay sum over $[a, c]$. We develop a general technique that works on a large class of decay sum functions (including polynomial decay) and reduces the problem of estimating the decay sum to keeping multiple window sums in parallel. The technique results in a bi-criteria approximation, because of which our lower and upper bounds are incomparable for this problem. 
      
\end{itemize}

In comparison with the simple randomized response strategy~\cite{rr} (i.e.~with probability $1/2 - \eps/2$ change update $x_i$ to $1-x_i$ and keep exact statistics of the changed input), our algorithms achieve exponentially smaller additive error: randomized response leads to estimators with standard deviation proportional to the energy of the decay function, while our estimators have standard deviation polylogarithmic in the energy. Technically, 
\begin{packed_item}
\item
Our algorithms keep dyadic tree data structures as is natural and also used in~\cite{dwork-continual,chan2010private} and elsewhere. However, in order to provide estimates with error polylogarithmic in the range of the decay function, we need to treat the dyadic tree data structure in non-uniform manner: either adding different noise at different nodes, or weighing the contribution of an update to different nodes differently, which is our technical contribution.
\item
We derive all our lower bounds from a common framework, that is inspired by work on differentially private combinatorial optimization. This extends prior work in two ways:  they apply to decay sum
problems that have not been considered before, and they apply against the
weaker $(\delta, \gamma)$-utility  guarantee  (rather than requiring
that all queries are accurate, as in~\cite{dwork-continual}. 
\end{packed_item}





\junk{
Differential privacy under continual updatesof data stream is an exciting research
direction, and the study of other decayed functions will be of great
interest. In particular, user-level privacy and sketches over windows
are  interesting problems for future study.
}

