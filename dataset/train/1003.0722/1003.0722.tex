\documentclass[11pt]{article}


\usepackage{latexsym,amsmath,amssymb}
\usepackage{epsfig}
\usepackage{tabls}
\usepackage{times,mathptmx}
\usepackage{tabularx}
\usepackage{xspace}
\usepackage{algorithm, algorithmic}
\usepackage{graphicx}

\usepackage{typearea}
\paperwidth 8.5in \paperheight 11in
\typearea{15}




\newcommand{\ignore}[1]{}



\ignore{

\makeatletter
\addtolength{\textheight}{20pt}
 \addtolength{\footskip}{-20pt}
\makeatother


}





\newtheorem{thm}{Theorem}
\newtheorem{theorem}[thm]{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{claim}[thm]{Claim}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}



\def\tab{\hspace{5mm}}
\def\proof{\noindent {\bf Proof:}\hspace{2mm}}
\def\bsq{\qquad}

\def\sse{\subseteq}
\def\lb{{\sf LB}\xspace}
\def\opt{{\sf Opt}\xspace}
\def\lat{{\sf Lat}\xspace}
\def\tsp{{\sf Tsp}\xspace}

 

\def\is{\mathcal{I}}
\def\js{\ensuremath{\mathcal{J}}}
\def\a{\mathcal{A}}
\def\ds{\mathcal{D}}
\def\bs{\mathcal{B}}
\def\p{\mathcal{P}}
\def\f{\mathcal{F}}

\def\spl{{\sf SplitALG}\xspace}
\def\unspl{{\sf UnsplitALG}\xspace}
\def\iso{\ensuremath{{\sf IsoAlg}}\xspace}
\def\palg{\ensuremath{{\sf Partition}}\xspace}
\def\palgl{\ensuremath{{\sf PartnLat}}\xspace}
\def\res{\tilde{Q}}
\def\resu{\tilde{U}}
\def\odt{{\sf ODT}\xspace}

\newcommand{\profit}{\phi}


\def\latency{{\sf latency}}
\def\length{{\sf length}}
\def\E{\mathbf{E}}

\def\isoprob{\ensuremath{{\sf Isolation}}\xspace}
\def\isotime{\ensuremath{{\sf IsoTime}}}
\def\isoopt{{\isotime}^*}

\def\stsp{\ensuremath{{\sf AdapTSP}}\xspace}
\def\strp{\ensuremath{{\sf AdapTRP}}\xspace}
\def\svrp{\ensuremath{{\sf AdapVRP}}\xspace}
\def\lpgst{\ensuremath{{\sf LPGST}}\xspace}

\def\gso{\ensuremath{{\sf GSO}}\xspace}
\def\lgs{{\sf LGST}\xspace}
\def\mgs{{\sf GST}\xspace}
\def\fq{\ensuremath{{\sf FnQ}}\xspace}
\def\cnf{\ensuremath{{\sf CNF}}\xspace}
\def\dnf{\ensuremath{{\sf DNF}}\xspace}
\def\cov{{\sf Cov}\xspace}
\def\qc{\ensuremath{{\sf QueryCost}}\xspace}
\def\sbc{\ensuremath{{\sf StocCover}}\xspace}

\def\ot{{\sf OptTree}\xspace}
\def\gt{{\sf Greedy}\xspace}

\def\var{{\sf sset}\xspace}

\def\dtp{optimal decision tree problem\xspace}

\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\ts{\textstyle}
\newcommand{\poly}{\operatorname{poly}}



\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newcounter{myLISTctr}
\newcommand{\initOneLiners}{\setlength{\itemsep}{0pt}
    \setlength{\parsep }{0pt}
    \setlength{\topsep }{0pt}
}
\newenvironment{OneLiners}[1][\ensuremath{\bullet}]
    {\begin{list}
        {#1}
        {\initOneLiners}}
    {\end{list}}

\newenvironment{proofof}[1]{

\bigskip\noindent{\bf Proof of {#1}:}}
{\hfill


}

\newenvironment{pf}{

\noindent{\bf Proof:}} {\hfill


}

\usepackage{url}
\makeatletter
\def\url@leostyle{\@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
\urlstyle{leo}




\title{Approximation Algorithms for Optimal Decision Trees  \\ and Adaptive
  TSP Problems}
\author{
Anupam Gupta\thanks{Computer Science Department, Carnegie Mellon
    University.}  \and Viswanath Nagarajan\thanks{ Industrial and Operations Engineering Department, University of Michigan.} \and  R. Ravi\thanks{Tepper School of Business, Carnegie Mellon University.}}
\date{}





\begin{document}


\maketitle
\begin{abstract}
We consider the problem of constructing {\em optimal decision trees}: 
  given a collection of tests which can disambiguate between a set of
   possible diseases, each test having a cost, and the a-priori
  likelihood of any particular disease, what is a
  good adaptive strategy to perform these tests to minimize the expected
  cost to identify the disease?
This problem has been studied in several works, with -approximations known in the special cases when either
costs or probabilities are uniform. In this paper, we settle the approximability of the general problem by giving a tight -approximation
algorithm.

We also consider a substantial generalization, the
{\em adaptive traveling salesman problem}. Given an underlying metric space, a random subset  of vertices is drawn
from a known distribution, but  is initially unknown---we get information about whether any vertex is in 
only when it is visited. What is a good adaptive strategy to visit all vertices in the random subset  while
minimizing the expected distance traveled? This problem has applications in routing message ferries in ad-hoc networks,
and also models switching costs between tests in the optimal decision tree problem. We give a poly-logarithmic
approximation algorithm for adaptive TSP, which is nearly best possible due to a connection to the well-known group Steiner tree problem. Finally, we consider the related {\em adaptive traveling repairman
problem}, where the goal is to compute an adaptive tour minimizing the expected sum of arrival times of vertices in the random subset ; we obtain a poly-logarithmic approximation algorithm for this problem as well.
\end{abstract}





\section{Introduction}

Consider the following two adaptive covering optimization problems:
\begin{itemize}

\item \emph{Adaptive TSP under stochastic demands} (\stsp). A traveling  salesperson is given a metric space  and distinct subsets  such that  appears with probability   (and ). She needs to serve requests at a random
subset  of locations drawn from this distribution. However, she does not know the identity of the
  random subset: she can only visit locations, at which time she finds
  out whether or not that location is part of the subset . What adaptive
  strategy should she use to minimize the expected time to serve all
  requests in the random  set ?

\item \emph{Optimal Decision Trees.} Given a set of  diseases, there
  are  binary tests that can be used to disambiguate between these
  diseases.  If the cost of performing test  is ,
  and we are given the likelihoods  that a typical
  patient has the disease , what (adaptive) strategy should the
  doctor use for the tests to minimize the expected cost to identify the
  disease?   
  \end{itemize}

It can be shown that the \dtp is a special case of the adaptive TSP problem: a formal reduction is given in
Section~\ref{sec:odt}.  In both these problems we want to devise adaptive strategies, which take into account
the revealed information in the queries so far (e.g., locations already visited, or tests already done) to determine
the future course of action. Such an {\em adaptive solution} corresponds naturally to a decision tree, where nodes encode the current ``state'' of the solution and branches represent observed random outcomes: see Definition~\ref{def:dt} for a formal definition.  A simpler class of solutions, that have been useful in some other adaptive optimization problems, eg.~\cite{DGV08,GuhaM09,BGLMNR12}, are {\em non-adaptive solutions}, which are specified by just an ordered list of actions. However there are instances for both the above problems where the optimal adaptive solution costs much less than the optimal non-adaptive solution.  
Hence it is
essential that we find good  adaptive solutions.

The \emph{\dtp} has long been studied, its NP-hardness was shown by Hyafil and Rivest in 1976~\cite{rivest-hyafil}  and
many references and applications can be found in~\cite{N11}. 
There have been a large number of papers providing algorithms for this problem~\cite{garey-graham,loveland,kmb,dasgupta,AH12,CPRAM11,N11,gb09}. The best results yield approximation ratios of  and , where  is the minimum non-zero probability and  (resp. ) is the maximum (resp. minimum) cost. In the special cases when the likelihoods  or the costs
 are all polynomially bounded in , these imply an -approximation
algorithm. However, there are instances (when probabilities and costs are exponential) demonstrating an  approximation guarantee for all previous algorithms. On the hardness side, an  hardness of approximation (assuming ) is known for the \dtp~\cite{CPRAM11}. 
While the existence of an -approximation algorithm for the general \dtp has been posed as an open question,
it has not been answered prior to this work.
 


Optimal decision tree is also a basic problem in {\em average-case active learning}~\cite{dasgupta,N11,gb09}. In this
application, there is a set of  data points, each of which is associated with a  or  label. The labels are
initially unknown. A classifier is a partition of the data points into  and  labels. The true classifier  is
the partition corresponding to the actual data labels. The learner knows beforehand, a ``hypothesis class'' 
consisting of  classifiers; it is assumed that the true classifier . Furthermore, in the average case
model, there is a known distribution  of  over . The learner wants to determine  by querying labels
at various points. There is a cost  associated with querying the label of each data point . An active learning
strategy involves adaptively querying labels of data points until  is identified. The goal is to compute a
strategy that minimizes the expectation (over ) of the cost of all queried points. This is precisely the \dtp,
with points being tests and classifiers corresponding to diseases.


\medskip
Apart from being a natural adaptive routing problem, \stsp has many applications in the setting of message ferrying in
ad-hoc networks~\cite{ZA03,SRJB03,ZAZ04,ZAZ05,HLS10}. We cite two examples below:
\begin{itemize}
\item {\em Data collection in sparse sensor networks (see eg.~\cite{SRJB03}).} A collection of sensors is spread over a large geographic area, and
one needs to periodically gather sensor data at a base station. Due to the power and cost overheads of setting up a
communication network between the sensors, the data collection is instead performed by a mobile device (the message
ferry) that travels in this space from/to the base station. On any given day, there is a known distribution 
of the subset  of sensors that contain new information: this might be derived from historical data or domain
experts. The routing problem for the ferry then involves computing a tour (originating from the base station) that
visits all sensors in , at the minimum expected cost.

\item {\em Disaster management (see eg.~\cite{ZAZ04}).} Consider a post-disaster situation, in which usual
communication networks have broken down. In this case, vehicles can be used in order
to visit locations and assess the damage. Given a distribution of the set of affected locations, the goal here is to
route a vehicle that visits all affected locations as quickly as possible  in expectation.
\end{itemize}
In both these applications, due to the absence of a direct communication network, the information at any location is
obtained only when it is visited: this is precisely the \stsp problem.







\subsection{Our Results and Techniques}
In this paper, we settle the approximability of the \dtp:
\begin{theorem}
  \label{thm:main1}
  There is an -approximation algorithm for the \dtp with arbitrary test
  costs and arbitrary probabilities, where  is the number of diseases. The problem admits the same
  approximation ratio even when the tests have non-binary outcomes.
\end{theorem}
In fact, this result arises as a special case of the following theorem:
\begin{theorem}
  \label{thm:main2}
  There is an -approximation algorithm for
  the adaptive Traveling Salesman Problem, where  is the number of vertices and  the number of scenarios in the
  demand distribution.
\end{theorem}



To solve the \stsp problem, we first solve the ``isolation problem'', which seeks to identify which of the 
scenarios has materialized. Once we know the scenario we can visit its vertices using any constant-factor approximation algorithm for TSP. 
The high-level idea behind our algorithm for the isolation problem is this---suppose each vertex lies in at most half the scenarios; then if we
visit one vertex in each of the  scenarios using a short tour, which is an instance of the {\em group Steiner tree} problem\footnote{In the group Steiner tree problem~\cite{gkr} the input is a metric  with root  and groups  of vertices; the goal is to compute a minimum length tour originating from  that visits at least one vertex of each group.}, we'd
notice at least one of these vertices to have a demand; this would reduce the number of possible scenarios by at least  and we can recursively run the algorithm on the remaining scenarios.
This is an over-simplified view, and there are many details to handle: we need not visit all
scenarios---visiting all but one allows us to infer the last one by exclusion; the expectation in the objective
function means we need to solve a \emph{minimum-sum} version of group Steiner tree; not all vertices need lie in less
than half the scenarios.  Another major issue is that we do not want our performance to depend on the magnitude of the
probabilities, as some of them may be exponentially small.  
Finally, we need to charge our cost directly against the optimal decision tree. 
All these issues can indeed be resolved to obtain Theorem~\ref{thm:main2}.




The algorithm for the isolation problem involves an interesting combination of ideas from the group Steiner~\cite{gkr,ccgg} and minimum
latency TSP~\cite{bccprs,cgrt,fhr} problems---it uses a greedy approach that is greedy with respect to two different
criteria, namely the probability measure and the number of scenarios. This idea is formalized in our algorithm for 
the {\em partial latency} group Steiner (\lpgst) problem, which is a key subroutine for \isoprob. While this \lpgst
problem is harder to approximate than the standard group Steiner tree (see Section~\ref{sec:prelim}), for which  is the best approximation ratio, we show  that it admits a better
 {\em bicriteria} approximation algorithm. Moreover, even this bicriteria approximation
guarantee for \lpgst suffices to obtain an -approximation algorithm for \isoprob. 




We also show that both \stsp and the isolation problem are  hard to approximate even on tree metrics;
our results are essentially best possible on such metrics, and we lose an extra logarithmic factor to go to general
metrics, as in the group Steiner tree problem. Moreover, any improvement to the result in Theorem~\ref{thm:main2} would lead to a similar improvement for the group Steiner tree problem~\cite{gkr,hk03,cp05} which is a long-standing open question. 

For the \dtp, we show that we can use a variant of minimum-sum set cover~\cite{FLT04} which is the special case of \lpgst on star-metrics. This avoids an  loss in the approximation guarantee, and hence gives us an -approximation algorithm which is best possible~\cite{CPRAM11}. Although this  variant of min-sum set cover is -hard to approximate (it
generalizes set cover as shown in Section~\ref{sec:prelim}), we again give a constant factor bicriteria approximation algorithm, which leads to the -approximation for optimal decision tree. Our result further reinforces the close connection between the min-sum set
cover problem and the \dtp that was first noticed by~\cite{CPRAM11}.

Finally, we consider the related adaptive traveling repairman problem (\strp), which has the same input as \stsp, but
the objective is to minimize the expected sum of arrival times at vertices in the materialized demand set. In this setting, we
cannot first isolate the scenario and then visit all its nodes, since a long isolation tour may negatively impact the
arrival times. So \strp (unlike \stsp) cannot be reduced to the isolation problem. However, we show that our techniques for \stsp
are robust, and can be used to obtain:
\begin{theorem}
  \label{thm:main3}
  There is an -approximation algorithm for
  the adaptive Traveling Repairman Problem, where  is the number of vertices and  the number of scenarios in the
  demand distribution.
\end{theorem}

\medskip
\noindent {\bf Paper Outline:} The results on the isolation problem appear in Section~\ref{sec:iso}. We obtain the improved approximation algorithm for
optimal decision tree in Section~\ref{sec:odt}. The algorithm for the adaptive traveling salesman problem is in Section~\ref{sec:stsp}; Appendix~\ref{app:hardness} contains a nearly matching  hardness of approximation result. Finally, Section~\ref{sec:strp} is on the adaptive traveling repairman problem.



\subsection{Other Related Work}
The \dtp has been studied earlier by many authors, with algorithms and hardness results being shown
by~\cite{garey-graham,rivest-hyafil,loveland,kmb,AH12,dasgupta,CPRAM11,cprs09,gb09}. As mentioned above, the algorithms
in these papers gave -approximation ratios only when the probabilities or costs (or both) are
polynomially-bounded. The early papers on optimal decision tree considered tests with only binary outcomes.
More recently,~\cite{CPRAM11} studied the generalization with  outcomes per test, and gave an -approximation under uniform costs. Subsequently,~\cite{cprs09} improved this bound to ,
again under uniform costs. Later, \cite{gb09} gave an algorithm under arbitrary costs and
probabilities, achieving an approximation ratio of  or . This is the previous best approximation guarantee; see also Table~1
in~\cite{gb09} for a summary of these results. We note that in terms of the number  of diseases, the previous best approximation guarantee is only . On the other hand, there is an  hardness of approximation for the
\dtp~\cite{CPRAM11}. Our -approximation algorithm for arbitrary costs and probabilities solves an open problem from
these papers. A crucial aspect of this algorithm is that it is   non greedy. All previous results were based on
variants of a greedy algorithm.

There are many results on adaptive optimization dealing with covering problems. E.g., \cite{goemansv06} considered the adaptive set-cover problem; they
gave an -approximation when sets may be chosen multiple times, and an -approximation when each set may
be chosen at most once. The latter approximation ratio was improved in~\cite{msw07} to , and subsequently to the
best-possible -approximation ratio by~\cite{lpry08}, also using a greedy algorithm.  In recent
work~\cite{GK11} generalized adaptive set-cover to a setting termed `adaptive submodularity', and gave many
applications.  In all these problems, the adaptivity-gap  (ratio between optimal adaptive and non-adaptive solutions) is large, as is the case for the problems considered in this paper, and so the solutions
need to be inherently adaptive.



The \stsp problem is related to universal TSP~\cite{jlnrs,ghr} and {\em
  a priori} TSP~\cite{jaillet,ss08,st08} only in spirit---in both the
universal and \emph{a priori} TSP problems, we seek a master tour which is shortcut once the demand set is known, and
the goal is to minimize the worst-case or expected length of the shortcut tour. The crucial difference is that the
demand subset is revealed {\em in toto} in these two problems, leaving no possibility of adaptivity---this is in
contrast to the slow revelation of the demand subset that occurs in \stsp.



\section{Preliminaries}\label{sec:prelim}
We work with a  finite metric  that is given by a set  of  vertices and distance function . 
As usual, we assume that the distance function is symmetric and satisfies the triangle inequality. For any
integer , we let .


\begin{definition}[-tour]
Given a metric  and vertex , an \emph{-tour} is any sequence  of vertices that  begins and ends at . The length of such an -tour is , the total length of all edges in the tour. 
\end{definition}


Throughout this paper, we deal with demand distributions over vertex-subsets that are specified explicitly. A demand
distribution  is specified by  distinct subsets  having associated probabilities
 such that . This means that the realized subset  of demand-vertices will
always be one of , where  with probability  (for all ). We also refer to the
subsets  as {\em scenarios}. The following definition captures adaptive strategies.

\smallskip
\begin{definition}[Decision Tree]
  \label{def:dt}
  A decision tree  in metric  is a
  rooted binary tree where each non-leaf node of  is labeled with a vertex ,
    and its two children  and  correspond to the
    subtrees taken if there \underline{is} demand at  or if there is \underline{no} demand at .
Thus given any realized demand , a unique path  is followed in  from the root down to a leaf.
\end{definition}
Depending on the problem under consideration, there are additional constraints on decision tree  and the
\emph{expected cost} of  is also suitably defined.  There is a (problem-specific) cost  associated with each
scenario  that depends on path , and the expected cost of  (under distribution ) is then
. For example in \stsp, cost  corresponds to the length of path . 


\smallskip

Since we deal with explicitly specified demand distributions , all decision trees we consider will have size polynomial in  (support size of ) and  (number of vertices). 





\paragraph{Adaptive Traveling Salesman}
This problem consists of a metric  with root  and a demand distribution  over subsets of
vertices. The  information on whether or not there is demand at a vertex  is
obtained only when that vertex  is visited. The objective is to find an adaptive strategy that minimizes the expected
time to visit all vertices of the realized scenario drawn from .

We assume that the distribution  is specified {\em explicitly} with a support-size of .
This allows us to model demand distributions that are arbitrarily correlated across vertices. We note however that the
running time and performance of our algorithm will depend on the support size. The most general setting would be to consider black-box access to the distribution : however, as shown
in~\cite{Vish-thesis}, in this setting there is no -approximation algorithm for \stsp  
that uses a polynomial number of samples from the distribution.
 One could also consider \stsp under independent demand distributions. In this case there is a trivial constant-factor approximation algorithm, that visits all vertices having non-zero probability along an approximately minimum TSP tour; note that any feasible solution must visit all vertices with non-zero probability as otherwise (due to the independence assumption) there would be a positive probability of not satisfying a demand. 
 
\begin{definition}[Adaptive TSP]\label{def:stsp} The input is a metric , root  and demand distribution 
given by  distinct subsets  with probabilities
 (where ). The goal in \stsp 
is to compute a decision tree  in metric  such that:
  \begin{itemize}
  \item the root of  is labeled with the root vertex , and
  \item for each scenario , the path  followed on input 
   contains \underline{all} vertices in .
  \end{itemize}
  The objective function is to minimize the expected tour length
  , where  is
  the length of the tour that starts at , visits the vertices on path
   in that order, and returns to .
\end{definition}



\paragraph{Isolation Problem}
This is closely related to \stsp. The input is  the same as \stsp, but the goal
is just to identify the unique scenario that 
has materialized, and not to visit all the vertices in the realized scenario.
\smallskip
\begin{definition}[Isolation Problem]\label{def:iso}
Given metric , root  and demand distribution , the goal in \isoprob is to compute a decision tree  in
metric  such that:
  \begin{itemize}
  \item the root of  is labeled with the root vertex , and
  \item for each scenario , the path  followed on input  ends at a \underline{distinct} leaf-node of .
    \end{itemize}
The objective  is to
  minimize the expected tour length , where  is the length of the -tour that visits the vertices on path  in that order, and returns to .
\end{definition}
\smallskip

The only difference between \isoprob and \stsp is that the tree path  in \isoprob need not contain
all vertices of , and the paths for different scenarios must  end at distinct leaf-nodes. In Section~\ref{sec:stsp} we show that  any approximation algorithm for \isoprob leads to an approximation algorithm for \stsp. So we focus on designing algorithms for \isoprob. 




\paragraph{Optimal Decision Tree} This problem involves identifying a random disease from a set of possible diseases using binary tests.  

\smallskip
\begin{definition}[Optimal Decision Tree]\label{def:odt}
The input is a set of  diseases with 
probabilities  that sum to one, and a collection  of  binary tests with
 costs . There is exactly one realized disease: each disease  occurs with probability . Each test  returns a positive outcome for subset  of  diseases and returns a negative outcome for the rest . The goal in \odt is to compute a decision tree  where each internal node is  labeled by a test and has two children corresponding to  positive/negative test outcomes, such that for each  the path  followed under disease  ends at a distinct leaf node of . The objective is to minimize the expected cost  where  is the sum of test-costs along path .
\end{definition}
\smallskip

Notice that the optimal decision tree problem is exactly \isoprob on a weighted star metric. Indeed, given an instance of \odt, consider a metric  induced by a weighted star with  center  and  leaves corresponding to the tests. For each , we set . The demand scenarios are as
  follows: for each  scenario  has demands .
  It is easy to see that this \isoprob instance corresponds
  exactly to the optimal decision tree instance. See  Section~\ref{sec:odt} for an example. So any algorithm for  \isoprob on star-metrics can be used to solve \odt as well. 





\paragraph{Useful Deterministic Problems} Recall that the group Steiner tree problem~\cite{gkr,hk03} consists of a metric , root  and  groups of 
vertices , and the goal is to find an -tour of minimum length that contains at least one vertex from each group .  Our algorithms for the above stochastic problems rely on solving some variants of group Steiner tree. 

\begin{definition}[Group Steiner Orienteering]\label{def:gso}
The input is a metric , root ,  groups of 
vertices  with associated profits  and a length bound . The goal in \gso is
to compute an -tour of length at most  that maximizes the total profit of covered groups. A group 
is covered if any vertex from  is visited by the tour. 
\end{definition}
\smallskip
An algorithm for \gso is said to be a  -bicriteria approximation algorithm if on any instance of the problem, it finds an -tour of length at most  that has profit at least  times the optimal (which has length at most ). 

\smallskip

\begin{definition}[Partial Latency Group Steiner]\label{def:lpgs} The input is 
a metric ,  groups of vertices  with associated weights
, root  and a target . The goal in \lpgst is to compute an -tour   
that covers at least  groups and minimizes the weighted sum of arrival times over all groups. The \emph{arrival  time} of group  is the length of the shortest prefix of tour    
that contains an -vertex; if the group is not covered, its arrival time is set to be the entire tour-length. The \lpgst objective  is termed {\em latency}, i.e. 
 
\end{definition}
\smallskip
An algorithm for \lpgst is said to be a  -bicriteria approximation algorithm if on any instance of the problem, it finds an -tour that covers at least  groups and has latency at most  times the optimal (which  covers at least  groups). The reason we focus on a bicriteria approximation for \lpgst is that it is harder to approximate than the group Steiner tree problem (see below) and we can obtain  a better bicriteria guarantee for \lpgst. 

To see that \lpgst is at least as hard to approximate as the group Steiner tree problem, consider an arbitrary instance of group Steiner tree with metric , root  and  groups  . Construct an instance of \lpgst as follows. The vertices are  where  is a new vertex. Let . The distances in metric  are:  if  and  if . 
There are  groups with  for  and . The target . The weights are  for  and . Since the distance from  to  is very large, no approximately optimal \lpgst solution will visit . So any such \lpgst solution covers {\em all} the groups  and has latency equal to the length of the solution (as group  has weight one  and all others have weight zero).  This reduction also shows that \lpgst on weighted star-metrics (which is used in the \odt algorithm) is at least as hard to approximate as set cover: this is because when  metric  is a star-metric with center , so is the  new metric .\footnote{Recall that group Steiner tree on star-metrics is equivalent to the set cover problem.}   






\section{Approximation Algorithm for the Isolation Problem} \label{sec:iso}



Recall that an instance of \isoprob is specified by a metric , a root vertex , and  scenarios
 with associated probability values . The main result of this section is:
\begin{theorem}\label{thm:isolation}
If there is a -bicriteria approximation algorithm for group Steiner orienteering then there is an -approximation algorithm for the isolation problem.
\end{theorem}

We prove this in two steps. First, in Subsection~\ref{subsec:iso-alg} we show that a -bicriteria approximation algorithm for \lpgst can be used to obtain an -approximation algorithm for \isoprob. Then, in Subsection~\ref{subsec:grp-lat} we show that any -bicriteria approximation algorithm for \gso leads to an -bicriteria approximation algorithm for \lpgst. 

\paragraph{Note on reading this section:} While the results of this section apply to the isolation problem on general metrics, 
readers interested in just the optimal decision tree problem need to only consider weighted star metrics (as discussed after Definition~\ref{def:odt}). In the \odt case, we have the following simplifications (1) a tour   is simply a sequence of tests, (2) the tour length is the sum of test costs in the  sequence, and (3) concatenating tours corresponds to concatenating test sequences. 

\subsection{Algorithm for \isoprob using \lpgst} \label{subsec:iso-alg}
Recall the definition of \isoprob and \lpgst from Section~\ref{sec:prelim}.  Here we will prove:
\begin{theorem}\label{thm:lpg-to-iso}
If there is a -bicriteria approximation algorithm for \lpgst then there is an -approximation algorithm for \isoprob.
\end{theorem}






We first give a high-level description of our algorithm. The algorithm uses an iterative 
approach and maintains a
candidate set of scenarios that contains the realized scenario. In each iteration, the algorithm eliminates a constant fraction of scenarios from the candidate set. So the number of iterations will be bounded by . In each iteration we solve a suitable instance of \lpgst in order to refine the candidate set of scenarios. 


\paragraph{Single iteration of \isoprob algorithm} As mentioned above, we use \lpgst in each iteration of 
the \isoprob algorithm- we now describe how this is done.
 At the start of each iteration, our algorithm maintains a candidate set  of scenarios that contains the realized scenario.
The probabilities associated with the scenarios  are not the original s but their conditional probabilities . The algorithm \palg (given as Algorithm~\ref{alg:palg}) uses \lpgst to compute an -tour  such that after
observing the demands on , the number of scenarios consistent with these observations is guaranteed to be  a constant factor smaller than . 


To get some intuition for this algorithm, consider the simplistic case when there is a vertex  located near the root  such that  of the scenarios in  contain it. Then just 
visiting vertex  would reduce the number candidate scenarios by , irrespective of the observation at ,
giving us the desired notion of progress. However, each vertex may give a very unbalanced partition of : so we may have to visit multiple vertices before ensuring that the number of candidate scenarios reduces by a constant factor. Moreover, some vertices may
be  too expensive to visit from : so we need to carefully take the metric into account in choosing the set of vertices to visit. Addressing these issues is precisely where the \lpgst problem comes in.


\begin{algorithm}
  \caption{Algorithm }
  \label{alg:palg}
  \begin{algorithmic}[1]
    \STATE \label{step:palg0} \textbf{let} . For each ,
    define , and
    {\small }

    \STATE \label{step:palg-flip}
    \textbf{for each} , set .

    \STATE\label{step:palg1} \textbf{run} the -bicriteria approximation algorithm for \lpgst on the instance 
  with metric , root , groups
     with weights , and target
    . \\
    ~~~~~\textbf{let}  be the -tour
    returned.

    \STATE \label{step:palg2} \textbf{let}  be the
    partition of  where
    {\small }
    \STATE \textbf{return} tour  and the partition
    .
  \end{algorithmic}
\end{algorithm}





Note that the information at any vertex  corresponds to a bi-partition  of the scenario set
, with scenarios  having demand at  and scenarios  having no demand at . So either the
presence of demand or the absence of demand reduces the number of candidate scenarios by half (and represents progress).
To better handle this asymmetry, Step~\ref{step:palg0} associates vertex  with subset  which is the smaller of
; this corresponds to the set of scenarios under which just the observation at  suffices to 
reduce the number of candidate scenarios below  (and represents progress). In Steps~\ref{step:palg-flip} and~\ref{step:palg1}, we view vertex  as covering the
scenarios .



\paragraph{The overall algorithm for \isoprob} Here we describe how the different iterations are combined to solve \isoprob. 
The final algorithm \iso (given as Algorithm~\ref{alg:isoalg}) is described in a recursive manner where each ``iteration'' is a new call to \iso. As mentioned earlier, at the start of each iteration, the algorithm maintains a
candidate set  of scenarios such that the realized scenario lies in .  Upon observing demands along the tour
produced by algorithm \palg, 
 a new set  containing the realized scenario is 
identified such that the number of candidate scenarios reduces by a constant factor (specifically ).
Then \iso recurses on scenarios , which corresponds to the next iteration.  After  such iterations the realized scenario would be correctly
identified.

\begin{algorithm}
  \caption{Algorithm }
  \label{alg:isoalg}
  \begin{algorithmic}[1]
    \STATE If , return this unique scenario as realized.

    \STATE \label{step:iso2} \textbf{run}  \\
    ~~~~~\textbf{let}  be the -tour and
      be the partition of~ returned.

    \STATE \textbf{let}  \textbf{for all} .

    \STATE \label{step:iso3} \textbf{traverse} tour  and return
    directly to  after visiting the first (if any) vertex 
    (for ) that determines that the realized scenario is
    in . If there is no such vertex until the end of
    the tour , then set .

    \STATE  \textbf{run}  to isolate the
    realized scenario within the subset .
  \end{algorithmic}
\end{algorithm}

Note that the adaptive Algorithm~\iso implicitly defines a decision tree too: indeed, we create a path , and hang the subtrees created in the recursive call on each instance  from the respective node . See also Figure~\ref{fig:single-phase}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.8]{single-phase.pdf}
 \end{center}
\caption{\label{fig:single-phase} Example of decision tree in  single iteration using tour .}
\end{figure}






\paragraph{Analysis} The rest of this subsection analyzes \iso and proves  Theorem~\ref{thm:lpg-to-iso}.  
We first provide an  outline of the proof. It is easy to show that \iso correctly identifies the realized
scenario after  iterations: this is shown formally in Claim~\ref{cl:alg4}. We relate the objective values of 
the \lpgst and \isoprob instances in two steps: Claim~\ref{cl:alg1} shows that \lpgst has a smaller optimal value than \isoprob,  
and Claim~\ref{cl:alg2} shows that any approximate \lpgst solution can be used to construct a {\em partial} \isoprob solution incurring the same cost (in expectation). Since different iterations of \iso deal with different sub-instances of \isoprob, we need to relate the optimal cost of these sub-instances to that of the original instance: this is done in Claim~\ref{cl:alg3}.




Recall that the original instance of \isoprob is defined on metric , root  and set  of scenarios with probabilities . \iso works with many sub-instances of the isolation problem. Such an instance  is specified by a subset  which implicitly defines (conditional) probabilities  for all . In other words,  involves identifying the realized scenario {\em conditioned on} it being in set  (the metric and root remain the same as the original instance). Let  denote the optimal value of any instance \js.



\begin{claim}\label{cl:alg1}
  For any instance , the
  optimal value of the \lpgst instance considered in
  Step~\ref{step:palg1} of algorithm  is at most
  .
   \end{claim}
\begin{pf}
  Let  be an optimal decision tree corresponding to  \isoprob
  instance , and hence . Note that by
  definition of the sets , any internal node in
   labeled vertex  has its two children  and 
  corresponding to the realized scenario being in  and  (respectively); and by
  definition of , nodes  and 
  correspond  to the realized scenario being in  and  (now not necessarily in that order).

We now define an -tour  based on a specific root-leaf path in .   Consider the root-leaf path that at any 
node labeled , moves  to the child  or  that corresponds to
  , until it reaches a leaf-node . Let  denote the sequence of vertices in this root-leaf path, and define -tour   
   .  
  Since  is a feasible decision tree for the isolation
  instance,  there is at most one scenario  such that the path  traced in  under demands  ends at leaf-node . In other words,  every scenario  gives rise to a
  root-leaf path  that diverges from the root- path.  By our definition of the root- path,  the scenarios that diverge  from it are precisely , and so 
  .
  
  Next, we show that  is a feasible  solution to the
  \lpgst instance in Step~\ref{step:palg1}. By definition of the groups  (Step~\ref{step:palg-flip} of Algorithm~\ref{alg:palg}), it follows that tour  covers  groups . So the number of groups covered is at least , and  is a feasible \lpgst solution. 
  
Finally, we bound the \lpgst objective value of  in terms of the isolation cost . To reduce notation let  below. 
The arrival times in tour  are: 

Fix any . For any scenario ,  the path  traced in  contains the prefix labeled  of the root- path; so . Moreover, for scenario  which is the only scenario not in ,  we have . Now by~\eqref{eq:lpgs-obj}, . 
\end{pf}

\medskip\noindent If we use a -bicriteria approximation algorithm for \lpgst,
we get the following claim:
\begin{claim}
  \label{cl:alg4}
  For any instance , the
 latency of tour  returned by Algorithm \palg is at most
  .
Furthermore, the resulting partition  has each  for each , when .
\end{claim}
\begin{pf} By Claim~\ref{cl:alg1}, the optimal value of the \lpgst instance in Step~\ref{step:palg1} of algorithm \palg is at
most ; now the -bicriteria approximation guarantee 
 implies that the latency of the solution tour  is at most  times that. This proves the first part
of the claim.

Consider  the tour returned by the \lpgst algorithm in
Step~\ref{step:palg1} of algorithm \palg; and  the resulting partition. 
The -bicriteria approximation guarantee implies  that the number of groups covered by  is   (when ). By definition of the sets , it holds that  for
all . Since all but the last part  is a  subset of some , it holds that  for
.  Moreover, the set  has size . This proves the second part of the claim.
\end{pf}


Of course, we don't really care about the latency of the tour \emph{per se}, we care about the expected cost incurred in
isolating the realized scenario. But the two are related (by their very construction), as the following claim
formalizes:

\begin{claim}\label{cl:alg2}
  At the end of Step~\ref{step:iso3} of , the realized scenario lies in . The expected
  distance traversed in this step is at most .
\end{claim}
\begin{pf}
  Consider the tour  returned by
  the \palg algorithm. Recall that visiting any vertex  reveals whether the
  scenario lies in , or in . In step~\ref{step:iso3} of algorithm \iso,
  we traverse  and one of the following happens:
\begin{itemize}
   \item . Tour returns directly to  from the first vertex  (for ) such that the realized scenario lies in ; here . Since the scenario did not lie in any earlier  for , the definition of 
  gives us that the realized scenario is indeed in .
  \item . Tour  is completely traversed and we return to . In this case, the
  realized scenario does not lie in any of , and  it is inferred to be in the complement set ,
  which is  by definition.
\end{itemize}
Hence for  as defined in  Step~\ref{step:iso3} of , it follows that
 contains the realized scenario; this proves the first part of the claim (and correctness of the algorithm).

For each , let  denote the arrival time of group  in tour ; recall that this is the
length of the shortest prefix of  until it visits an -vertex, and is set to the entire tour length if 
does not cover . The construction of partition  from  implies that
  
and hence .

To bound the expected distance traversed, note the probability that  the traversal returns to  from vertex 
(for )  is exactly ; with the remaining  probability the
entire tour  is traversed. Now, using symmetry and triangle-inequality of the distance function , we have  for all . Hence the  expected length traversed is at most:
 
  which is exactly .
Finally, by Claim~\ref{cl:alg4}, this is at most .
\end{pf}



Now, the following simple claim captures the ``sub-additivity'' of .
\begin{claim}\label{cl:alg3}
  For any instance  and any partition   of ,
  
  where  for all .
\end{claim}
\begin{pf}
Let  denote the optimal decision tree for the instance
  . For each ,
  consider instance ; a feasible decision tree for instance 
  is obtained by taking the decision tree  and considering only paths to
  the leaf-nodes labeled by . Note that this is a feasible
  solution since  isolates all scenarios .
  Moreover, the expected cost of such a  decision tree for  is ; recall that  denotes the tour traced
    by  under scenario . Hence .  Summing over all
  parts , we get
  
  where the penultimate equality uses the fact that  is
  a partition of .
\end{pf}


Given the above claims, we can bound the overall expected cost of the algorithm.
\begin{claim}
  \label{cl:alg4}
  The expected length of the  decision tree  given by  is at most:
  
\end{claim}
\begin{pf}
We prove this by induction on . The base case of  is  trivial, since zero length is traversed. Now
consider . Let instance . For each , consider the
 instance , where . Note that  for all  by Claim~\ref{cl:alg4} (as ). By the inductive hypothesis, for any , the
  expected length of  is at most , since .

  By Claim~\ref{cl:alg2}, the expected length traversed in
  Step~\ref{step:iso3} of  is at most . The probability of recursing on  is exactly
   for each . So, 
    
where the third inequality uses Claim~\ref{cl:alg3}.
\end{pf}

Claim~\ref{cl:alg4} implies that our algorithm achieves  an -approximation 
  for \isoprob. This  completes the proof of Theorem~\ref{thm:lpg-to-iso}.






\subsection{Algorithm for \lpgst using \gso}
\label{subsec:grp-lat}

Recall the definitions of \lpgst and \gso from Section~\ref{sec:prelim}. Here we will prove:
\begin{theorem}\label{thm:lpgs}
If there is a -bicriteria approximation algorithm for \gso then there is an -bicriteria approximation algorithm for \lpgst.
\end{theorem}






We now describe the  algorithm for \lpgst in Theorem~\ref{thm:lpgs}.  Consider any instance of \lpgst with metric
, root ,  groups of vertices  having weights ,  and
target . Let  be an optimal tour for the given instance of \lpgst: let  denote the
latency and  the length of . We assume (without loss of generality) that the minimum non-zero distance in the metric is one. Let parameter .
 Algorithm~\ref{alg:lpgst} is the
approximation algorithm for \lpgst.
The ``guess'' in the first step means the following. We run the algorithm for all choices of  and return the
solution having minimum latency {\em amongst} those that cover at least  groups. Since , the number of choices for  is at most , and so the algorithm runs in polynomial time. 

\begin{algorithm}[ht]
  \caption{Algorithm for \lpgst}
  \label{alg:lpgst}
  \begin{algorithmic}[1]
    \STATE \label{step:grpSt:0} \textbf{guess} an integer  such that .

    \STATE \textbf{mark} all groups as {\em uncovered}.

    \FOR{}

    \STATE \label{step:grpSt:1} \textbf{run} the -bicriteria approximation algorithm for \gso  on the instance with
    groups , root , length bound , and profits:
    

    \STATE \textbf{let}  denote the -tour obtained above.

    \STATE \textbf{mark} all groups visited by  as {\em
      covered}.

    \ENDFOR

    \STATE \label{step:grpSt:3'} \textbf{construct} tour , the concatenation of all the above
    -tours. 
    
    \STATE \label{step:grpSt:3} {\em Extend  if necessary to ensure that  (this is only needed for the analysis).}

    \STATE \label{step:grpSt:2} \textbf{run} the -bicriteria approximation algorithm for \gso  on the instance with groups , root ,
    length bound , and {\em unit profit} for each group, i.e.  for all . 
    
    \STATE {\bf let}  denote the
  -tour obtained above.

    \STATE \textbf{output} tour  as solution to the \lpgst instance.
  \end{algorithmic}
\end{algorithm}




\paragraph{Analysis} 
In order to prove Theorem~\ref{thm:lpgs}, we will show that the algorithm's tour covers at least  groups and has latency .


\begin{claim}\label{cl:lat-gst-1}
  The tour  in Step~\ref{step:grpSt:3} has length
   and latency .
\end{claim}
\begin{pf}
Due to the -bicriteria approximation guarantee of the \gso algorithm used in Step~\ref{step:grpSt:1}, the length of each -tour  is at most . So the length of   in Step~\ref{step:grpSt:3'} is at most .
Moreover, the increase in Step~\ref{step:grpSt:3}
  ensures that . Thus the length of   in Step~\ref{step:grpSt:3'} is , which proves the first part of the claim. 
  
 
 
  The following proof for bounding the latency is based on techniques from
  the {\em minimum latency TSP}~\cite{cgrt,fhr}. Recall the optimal solution 
  to the \lpgst instance, where . For each , let  denote the total weight of
  groups visited in  by time ; note that 
  equals the total weight of the groups covered by .
  Similarly, for each , let  denote the total weight of
  groups visited in , i.e. by iteration 
  of the algorithm. Set , and  the
  total weight of all groups. We have: 
{\small   }
  The last inequality uses the bound  from above.
  
The latency of the optimal tour  is


  Consider any iteration  of the algorithm in
  Step~\ref{step:grpSt:1}. Note that the optimal value of the \gso
  instance solved in this iteration is at least : the
   length prefix of tour  corresponds to a feasible
  solution to this \gso instance with profit at least . The \gso algorithm implies
  that the profit obtained in , i.e.  , i.e. . Using this,
  
This implies 
since . This completes the proof.
\end{pf}

\begin{claim}\label{cl:lat-gst-2}
  The tour  in Step~\ref{step:grpSt:2} covers at least
   groups and has length .
\end{claim}
\begin{pf}
  Since we know that the optimal tour  has length at most
   and covers at least  groups, it is a feasible solution to
  the \gso instance defined in Step~\ref{step:grpSt:2}. So the \gso algorithm 
  ensures that the tour  has length at most  and profit (i.e. number of groups) at least . 
\end{pf}

\begin{lemma}\label{th:grp-lat}
  Tour  covers at least  groups
  and has latency .
\end{lemma}
\begin{pf}
  Since  visits all the vertices in ,
  Claim~\ref{cl:lat-gst-2} implies that  covers at least
   groups. For each group , let  denote
  its \emph{arrival time} under the tour  after 
  Step~\ref{step:grpSt:3}---recall that the arrival time  for
  any group  that is not covered by  is set to the length of
  the tour . Claim~\ref{cl:lat-gst-1} implies that the latency
  of tour , .
  Observe that for each group  that is {\em covered} in , its arrival
  time under tour  remains . For any
  group  {\em not covered} in , its arrival time under 
is  (due to Step~\ref{step:grpSt:3}), and its arrival time under  is . Hence, the arrival time under  of each group  is , i.e., at most a constant factor more
  than its arrival time in . Now using Claim~\ref{cl:lat-gst-1}
  completes the proof.
\end{pf}

Finally, Lemma~\ref{th:grp-lat} directly implies Theorem~\ref{thm:lpgs}.




\medskip
\noindent {\bf Remark:}  The above approach also leads to an 
 approximation algorithm for the {\em minimum latency group Steiner}
problem, which is the special case of \lpgst when the target . 
\smallskip

\begin{definition}[Minimum Latency Group Steiner]\label{def:lgst} The input is 
a metric ,  groups of vertices  with associated non-negative weights
 and root . The goal in \lgs is to compute an -tour 
 that covers all groups with positive weight and minimizes the weighted sum of arrival times of the groups.  
The \emph{arrival  time} of group  is the length of the shortest prefix of the tour  
that contains a vertex from .
\end{definition}
\smallskip
Note that the objective here is to minimize the sum
of weighted arrival times where  every group has to be visited. 
 The algorithm for latency group Steiner is in fact simpler than Algorithm~\ref{alg:lpgst}: we do not need the ``guess''  (Step~\ref{step:grpSt:0}) and we just repeat Step~\ref{step:grpSt:1} until {\em all} groups are covered (instead of stopping after 
iterations). A proof identical to that in Claim~\ref{cl:lat-gst-1} gives:
\begin{corollary}\label{cor:lgs} 
If there is a -bicriteria approximation algorithm for \gso then there is an -approximation algorithm for the latency group Steiner problem.
\end{corollary}

Combined with the -bicriteria approximation algorithm for \gso (see Section~\ref{subsec:gso}) we obtain an -approximation algorithm for \lgs.  It is  shown in~\cite{Vish-thesis} that any -approximation algorithm for \lgs can be used to obtain an -approximation algorithm for group Steiner tree. Thus improving this
-approximation algorithm for latency group Steiner would also improve the best known bound for the standard group
Steiner tree problem.







\section{Optimal Decision Tree Problem}
\label{sec:odt}

Recall that  the \emph{\dtp} consists of  a set of diseases with their probabilities (where exactly one disease occurs) 
and a set of binary tests with costs, and the goal is to identify the realized disease at minimum expected cost. In this section we prove Theorem~\ref{thm:main1}.


As noted in Section~\ref{sec:prelim} the \dtp (Definition~\ref{def:odt})  is a special
  case of \isoprob (Definition~\ref{def:iso}). We recall the reduction for convenience. Given an instance of \odt, consider a metric  induced by a weighted star with
  center  and  leaves corresponding to the tests. For each , we set . The demand scenarios are as
  follows: for each  scenario  has demands .
  It is easy to see that this \isoprob instance corresponds
  exactly to the optimal decision tree instance. Figure~\ref{fig:odt-redn} gives an example.   



 
 
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.68]{odt-redn.pdf}
\end{center}
\caption{\label{fig:odt-redn} Reducing optimal decision tree to Isolation: binary tests (top), multiway tests
(bottom).}
\end{figure}



The main observation here is the following:  

\begin{theorem}
  \label{th:gso-stars} There is a -approximation algorithm for the group Steiner
  orienteering problem on weighted star metrics.
\end{theorem}
\begin{pf}
Consider an instance of \gso (Definition~\ref{def:gso}) on  weighted star-metric
   with center  (which is also the root in \gso) and leaves ,  groups  with profits , and length bound .
  If for each , we define set  of
  cost , then solving the \gso instance is the same
  as computing a collection  of the sets with  that maximizes . But the latter problem is precisely an instance of maximizing a monotone submodular
  function over a knapsack constraint (), for which a -approximation algorithm is
  known~\cite{s04}.
\end{pf}

\medskip Combining this result with Theorem~\ref{thm:isolation}, we obtain an  approximation algorithm for \isoprob on weighted star-metrics and hence \odt. This
proves the first part of Theorem~\ref{thm:main1}.


\paragraph{Multiway tests.}  
Our algorithm
can be easily extended to the generalization of \odt where tests have multiway (instead of binary) outcomes.  In this setting (when each test has at
most  outcomes), any test  induces a partition  of  into  parts (some of them may be empty), and performing test 
determines which part the realized disease lies in. Note that this problem is also a special case of
\isoprob. As before, consider a metric  induced by a weighted star with center  and  leaves corresponding
to the tests. For each , we set . Additionally, for each , introduce  copies
of test-vertex , labeled , at zero distance from each other. The demand scenarios are defined
naturally: for each , scenario  has demands . See also an example in
Figure~\ref{fig:odt-redn}. Clearly this \isoprob instance is equivalent to the (multiway) decision tree instance.
Since the resulting metric is still a weighted star (we only made vertex copies), Theorem~\ref{th:gso-stars} along with Theorem~\ref{thm:isolation} implies an -approximation for the multiway decision tree
problem. This proves the second part of Theorem~\ref{thm:main1}.





\section{Adaptive Traveling Salesman Problem}\label{sec:stsp}


Recall that the adaptive TSP (Definition~\ref{def:stsp}) 
consists of a metric  with root  and demand distribution , and the goal  is to visit all demand vertices (drawn from ) using an -tour of minimum  expected cost.   We first show the following simple fact relating this problem to the isolation problem.


\begin{lemma}
  \label{lem:reduce-to-isol}
If there is an -approximation algorithm for \isoprob then there is an
  -approximation algorithm for \stsp.
\end{lemma}
\begin{pf}
We first claim that any feasible solution  to \stsp is also feasible for \isoprob. For this it suffices to show that
the paths  for any two scenarios  with . Suppose (for a contradiction) that paths
 for some . By feasibility of  for \stsp, path  contains all vertices in
. Since , there is some vertex in ; let  (the other case is identical). Consider the point where  is at a node labeled : then path
 must take the  child, whereas path  must take the  child. This contradicts the assumption
. Thus any solution to \stsp is also feasible for \isoprob; moreover the expected cost remains the
same. Hence the optimal value of \isoprob is at most that of \stsp.


Now, using any -approximation algorithm for \isoprob, we obtain a decision tree  that isolates the realized scenario and
has expected cost ,  where  denotes the optimal value of the \stsp instance. This suggests the
following feasible solution for \stsp:
\begin{enumerate}
 \item Implement  to determine the realized scenario , and return to .
 \item Traverse a -approximate TSP tour~\cite{c} on vertices .
\end{enumerate}
From the preceding argument, the expected length in the first phase is at most . The expected length
in the second phase is at most , where  denotes the minimum length
of a TSP tour on . Note that  is a lower bound on the optimal \stsp
value. So we obtain a solution that has expected cost at most , as claimed.
\end{pf}

\medskip

Therefore, it suffices to obtain an approximation algorithm for \isoprob. In the next subsection we obtain a -bicriteria approximation  algorithm for \gso, which combined with Theorem~\ref{thm:isolation} and Lemma~\ref{lem:reduce-to-isol} yields an -approximation algorithm for both \isoprob and \stsp.  This would prove Theorem~\ref{thm:main2}.  

\subsection{Algorithm for Group Steiner Orienteering} \label{subsec:gso}
Recall the \gso problem (Definition~\ref{def:gso}). 
Here  we obtain a bicriteria approximation algorithm for \gso.
\begin{theorem}
  \label{thm:gso}
  There is a -bicriteria approximation algorithm for
  \gso, where  is the number of vertices in the metric. That is, the algorithm's tour has length  and has profit at least  times the optimal profit of a length  tour.
\end{theorem}


This algorithm is based on a 
greedy framework that is used in many maximum-coverage problems: the solution is constructed iteratively where each iteration adds an -tour that maximizes the ratio of profit to length. In order to find an -tour (approximately) maximizing the profit to length ratio, we use a slight modification of an existing algorithm~\cite{ccgg}; see Theorem~\ref{thm:gso-ratio} below.  The final \gso algorithm is then given as Algorithm~\ref{alg:gsoalg}.

\begin{theorem}  \label{thm:gso-ratio}
There is a polynomial time algorithm that given any instance of \gso, outputs an -tour  having profit-to-length ratio . Here   and  denote the profit and length (respectively) of tour , \opt is the optimal value of the \gso instance,  is the length bound in \gso and  where  is the number of vertices in the metric.
\end{theorem}
\begin{pf} This result essentially follows from~\cite{ccgg}, but requires some modifications  which we present here for completeness. 
We first preprocess the metric to only include
  vertices within distance  from the root : note that since the
  optimal \gso tour cannot visit any excluded vertex, the optimal profit
  remains unchanged by this. To reduce notation, we refer to this restricted vertex-set also as  and let .
  We denote the set of all edges in the metric by .
We assume (without loss of generality) that every group is covered by some vertex in ; otherwise the group can be dropped from the \gso instance. By averaging, there  is some vertex  covering groups of total profit at least . If  then the -tour that just visits vertex  has profit-to-length ratio at least  and is output as the desired tour . Below we assume that . 

\def\lpgo{\ensuremath{\mathsf{LP}_\gso}\xspace}

We use the following linear programming relaxation \lpgo for \gso:
      
It is easy to see that this a valid relaxation of \gso: any feasible \gso solution corresponds to a feasible solution above where the  variables are  valued. So the optimal value . The algorithm is given as Algorithm~\ref{alg:densityGSO} and uses the following known results: Theorem~\ref{thm:ccgg} shows how to round fractional solutions to \lpgo on tree metrics and Theorem~\ref{thm:frt} shows how to transform an \lpgo solution on general metrics to one on a tree. 
\begin{theorem}[\cite{ccgg}]\label{thm:ccgg}
There is a polynomial time algorithm that given any fractional solution  to \lpgo on a tree metric where all variables are integral multiples of , finds a subtree  containing  such that . Here   and  denote the profit and length (respectively) of subtree .
\end{theorem}


\begin{theorem}[\cite{FRT03}]\label{thm:frt} There is a polynomial time algorithm that given any metric  with edges  and capacity function , computes a spanning tree  in this metric such that , where

\end{theorem}



\begin{algorithm}[!h]
  \caption{Algorithm for \gso maximizing profit-to-length ratio.}
\label{alg:densityGSO}
\begin{algorithmic}[1]
    \STATE {\bf solve} the linear program \lpgo to obtain solution .
    \STATE {\bf run} the algorithm from Theorem~\ref{thm:frt} on metric  with edge-capacities  to obtain a spanning tree  with ``new capacities''  on edges of . 
 \STATE \label{step:gso-round} {\bf round} down each  to an integral multiple of .
    \STATE \label{step:gso-flow} {\bf for} each group , let  be
      the maximum flow from  to group  under  
      capacities .
      
    \STATE \label{step:gso-density} {\bf run} the algorithm from Theorem~\ref{thm:ccgg} using variables  and  to obtain subtree . 
    \STATE {\bf output} an Euler tour  of the subtree .
  \end{algorithmic}
\end{algorithm}

By definition of the new edge-capacities  on edges of  (see Theorem~\ref{thm:frt}) it is clear that the capacity of each cut under  is at least as much as under ; i.e.  for all .
For each group , since  capacities  support  units of flow from  to , it follows that the new capacities  on tree  also support such a flow. So  is a feasible solution to \lpgo on tree  with budget .  
In order to apply the rounding algorithm from~\cite{ccgg} for \gso on trees, we need to ensure the technical condition (see Theorem~\ref{thm:ccgg}) that every variable is an integral multiple of  for some . This is the reason behind modifying capacities  in Step~\ref{step:gso-round}. Note that this step reduces the capacity  of each edge  by at most   . Since any cut in tree  has at most  edges,
  the capacity of any cut decreases by at most  after
  Step~\ref{step:gso-round}; and by the max-flow min-cut theorem,
  the maximum flow value for group  is  for each  (in
  Step~\ref{step:gso-flow}). Furthermore, since all edge capacities are
  integer multiples of , so are all the flow values 
  s.  
  So  is a feasible solution to \lpgo on tree  (with budget ) that satisfies the condition required in Theorem~\ref{thm:ccgg}, with .  
Also note  that this rounding down does not change the fractional profits much,  since
  
where the second last inequality follows from  (by the preprocessing). Now applying Theorem~\ref{thm:ccgg} implies that subtree  satisfies:

Finally, since we output an Euler tour of , the theorem follows.
\end{pf}
\medskip

\noindent
{\bf Remark:} A simpler approach in Theorem~\ref{thm:gso-ratio} might have been  to use the randomized algorithm from~\cite{gkr} rather than the deterministic algorithm (Theorem~\ref{thm:ccgg}) from~\cite{ccgg}. This however does not work directly since \cite{gkr} only yields a random solution  with  expected length  and   expected profit . While this does guarantee the existence of a solution with length-to-profit ratio at most , it may not find such a solution with reasonable (inverse polynomial) probability. 

\paragraph{Algorithm}
The \gso algorithm first preprocesses the metric to only include  vertices within distance  from the root : note that the optimal profit remains unchanged by this.   The algorithm then follows a standard greedy approach (see
  eg. Garg~\cite{garg0}), and is given as Algorithm~\ref{alg:gsoalg}. 
  
\begin{algorithm}[!h]
  \caption{Algorithm for \gso.}
\label{alg:gsoalg}
\begin{algorithmic}[1]
    \STATE {\bf initialize} -tour  and mark  all groups as uncovered.
    \WHILE{length of  does not exceed }
    \STATE {\bf set} residual profits:
      
    \STATE\label{step:gso-ratio} {\bf run} the algorithm from Theorem~\ref{thm:gso-ratio} on the \gso instance with profits 
     to obtain -tour .
    
    
   \STATE \label{step:gso-case1} {\bf if}  then
      .
    \STATE \label{step:gso-case2} {\bf if}  then:
\begin{itemize}
\item[(i)] partition tour  into at most  paths,
      each of length at most ;
    \item[(ii)]  let  denote the path containing maximum profit;
    \item[(iii)]  let  be the -tour obtained by connecting both end-vertices of path  to .
    \item[(iv)] set .
    \end{itemize}
    \STATE {\bf set} . Mark all groups visited in  as
      covered.
  \ENDWHILE
  \STATE {\bf output} the -tour .
  \end{algorithmic}
\end{algorithm}

  \paragraph{Analysis}
  Let  denote the optimal profit of the given \gso instance.
 In the following, let
   which comes from Theorem~\ref{thm:gso-ratio}.
  We  prove that Algorithm~\ref{alg:gsoalg} achieves a  bicriteria
  approximation guarantee, i.e. solution  has profit at least  and length . 
  
  By the description of the algorithm, we iterate as long
  as the total length of edges in  is at most  . Note that the increase in length of  in any iteration is at most
   since every vertex is at distance at
  most  from . So the final length . This proves
  the bound on the length. 
  
  It now
  suffices to show that the final subgraph  gets profit at least
  . At any iteration, let  denote the
  profit of the current solution , and  its length. Since  upon
  termination, it suffices to show the following invariant 
  over the iterations of the algorithm:
  

At the start of the algorithm, inequality~\eqref{eq:gso-greedy} holds trivially
  since  for . Consider any iteration where
   at the beginning: otherwise~\eqref{eq:gso-greedy} trivially holds for the next iteration. The
  invariant now ensures that  and hence we proceed
  further with the iteration. Moreover, in Step~\ref{step:gso-ratio} the optimal value
  of the ``residual'' \gso instance with profits  is  (by considering the optimal tour for the \gso instance with profits ). By Theorem~\ref{thm:gso-ratio}, the -tour  satisfies .
  

We finish by handling the two possible cases
(Steps~\ref{step:gso-case1} and~\ref{step:gso-case2}).
  \begin{itemize}
  \item If , then .
  \item If , then  is partitioned into at most  paths of length  each. The path  of best profit has ; so .
  \end{itemize}
In either case -tour  satisfies  inequality~\eqref{eq:gso-greedy}, and since  at the end of  the
iteration, the invariant holds for next iteration as well. This completes the proof of Theorem~\ref{thm:gso}.





\section{Adaptive Traveling Repairman}
\label{sec:strp}

In this section we consider the adaptive traveling repairman problem (\strp), where given a demand distribution, the goal is to
find an adaptive strategy that minimizes the expected {\em sum  of arrival times} at demand vertices. As in adaptive TSP, we assume that the demand distribution  is specified explicitly in terms of its support. 


\smallskip 
\begin{definition}[Adaptive Traveling Repairman]
The input is a metric , root  and demand distribution  given by  distinct subsets  with  probabilities  (which sum to one). The goal in \strp is to compute a decision tree  in
metric  such that:
  \begin{itemize}
  \item the root of  is labeled with the root vertex , and
  \item for each scenario , the path  followed on input  contains \underline{all} vertices in .
  \end{itemize}
  The objective function is to minimize the expected latency
  , where  is the sum of arrival times at vertices  along path .
\end{definition}

\smallskip

We obtain an -approximation algorithm for \strp (Theorem~\ref{thm:main3}). The high-level approach here is similar to that for \stsp, but there are some important differences. Unlike \stsp, we can not directly reduce \strp to the isolation problem: so there is no analogue of Lemma~\ref{lem:reduce-to-isol} here.  The following example illustrates this.

\smallskip

\begin{example}
Consider an instance of \strp 
on a star-metric with center  and leaves . Edges  have unit length for each , and edge  has length . There
are  scenarios: scenario  occurs with  probability; and for each , scenario
 occurs with  probability. The optimal \isoprob value for this instance is  and any reasonable solution clearly will not visit vertex :  it appears in all scenarios and hence provides no information. So if we first follow such an \isoprob solution, the
arrival time for  is ; 
since  occurs with  probability, the
resulting expected latency is . However, the \strp solution that first visits , and then vertices
 has expected latency . 
\end{example}
\smallskip

On the other hand, one can not ignore
the ``isolation aspect'' in \strp either.

\smallskip\begin{example}
Consider another instance of \strp 
on a star-metric with center  and leaves . For each , edge  has unit length and edge  has length . There
are  scenarios: for each , scenario  occurs with  probability. The optimal values for both \strp and \isoprob are . Moreover, any reasonable \strp solution will involve first isolating the realized scenario (by visiting vertices s).
\end{example}

\smallskip



Hence, the algorithm needs to interleave the two goals of isolating scenarios and visiting high-probability vertices. This will become clear in the construction of the  latency group Steiner  instances used by our algorithm (Step~\ref{step:trpalg1} in Algorithm~\ref{alg:trppalg}).





\paragraph{Algorithm Outline} Although we can not reduce \strp to \isoprob, we are still able to use ideas from the \isoprob algorithm. The \strp algorithm also follows an iterative approach and maintains a candidate set   containing the realized scenario. 
We also associate conditional probabilities  for each scenario . In each iteration, the algorithm eliminates a constant fraction of scenarios from : so the number of iterations will be . Each iteration involves solving  an instance of the 
{\em latency group Steiner} (\lgs) problem: recall Definition~\ref{def:lgst} and the -approximation algorithm for \lgs (Corollary~\ref{cor:lgs}). The construction of this \lgs instance is the main point of difference from the \isoprob algorithm. Moreover, we will show that the {\em expected latency} incurred in each iteration is . Adding up the latency over all iterations, would yield an -approximation algorithm for \strp.
 
 
 
\paragraph{Using \lgs to partition scenarios } In each iteration, the algorithm formulates an \lgs instance and computes an -tour  using Corollary~\ref{cor:lgs}. The details are in Algorithm~\ref{alg:trppalg} below. An important property of this tour  is that the number of  candidate scenarios after observing demands on  will be at most  (see Claim~\ref{cl:trp-partn}).

Given a candidate set  of scenarios, it will be convenient to partition the vertices into two parts:  consists of vertices which occur in more than half the scenarios, and  consists of vertices occurring in at most half the scenarios. In the \lgs instance (Step~\ref{step:trpalg1} below), we 
introduce  groups (with suitable weights) corresponding to each scenario .






\begin{algorithm}[h!]
  \caption{}
  \label{alg:trppalg}
  \begin{algorithmic}[1]
    \STATE\label{step:trppalg0} {\bf define}   for each .

    \STATE {\bf let} 
     , ,    and {\small }


   \STATE\label{step:trpalg1} {\bf define} instance  of \lgs (Definition~\ref{def:lgst}) 
    on metric , root  and the following groups:\\
   for each scenario ,
   \begin{itemize}
   \item[-] the \emph{main} group  
     of scenario   has weight 
     and vertices .

   \item[-] for each , group 
     has weight  and vertices .
   \end{itemize}



   \STATE\label{step:trppalg3} \textbf{run} the \lgs algorithm (from Corollary~\ref{cor:lgs}) on instance .   \\
   ~~~~~\textbf{let}  be the -tour  returned.

    \STATE \label{step:trppalg4} \textbf{let}  be the
    partition of  where
    {\small }

   \STATE \textbf{return} tour  and   partition
   .
 \end{algorithmic}
\end{algorithm}


\begin{claim}\label{cl:trp-partn}
When , partition  returned by \palgl satisfies .
\end{claim}
\begin{pf}
For each , we have  and so . We now show that  which would
prove the claim. Let  denote the vertices visited in the tour  output by \palgl.
Consider any : we will show that it is unique. By definition of , we have . By the definition of group  and sets s, this means that  is {\em not covered} by .
Since  is a feasible solution to , 's weight must be zero, i.e. . Thus we have
. Furthermore, if  for any  then , which implies
; so . Note that each  (for )
must be covered by , since s have weight . Also since  is not covered by , we must
have  for all . Thus we have , and combined with the earlier
observation, . This determines  uniquely, and so .
\end{pf}



\paragraph{Final \strp algorithm and analysis}
Given the above partitioning scheme, Algorithm~\ref{alg:trpalg} describes the overall \strp algorithm in a recursive manner. 




\begin{algorithm}[!h]
  \caption{}
  \label{alg:trpalg}
  \begin{algorithmic}[1]
    \STATE If , visit the vertices in this scenario using the -approximation algorithm~\cite{fhr}
    for deterministic traveling repairman, and quit.

    \STATE \label{step:trp2} \textbf{run}  \\
    ~~~~~\textbf{let}  be the
    -tour and  be the partition of~ returned.

    \STATE \textbf{let}  \textbf{for all} .

    \STATE \label{step:trp3} \textbf{traverse} tour  and return
    directly to  after visiting the first vertex 
    (for ) that determines that the realized scenario is
    in .
    
    \STATE {\bf update} the scenarios in  by removing vertices visited in  until , i.e. 

    \STATE  \textbf{run}  to recursively cover the
    realized scenario within .
  \end{algorithmic}
\end{algorithm}


The analysis for this algorithm is similar to that for the isolation problem (Section \ref{subsec:iso-alg})
and we follow the same outline.  For any sub-instance  of \strp, let  denote its optimal value.
Just as in the isolation case (Claim~\ref{cl:alg3}), it can be easily seen that the latency objective function is also
sub-additive.
\begin{claim}\label{cl:trp-subadd}
  For any sub-instance  and any partition   of ,
  
  where  for all .
\end{claim}


The next property we show is that the optimal cost of the \lgs instance  considered in
Steps~\eqref{step:trpalg1}-\eqref{step:trppalg3} of Algorithm~\ref{alg:trppalg} is not too high.


\begin{lemma} \label{lem:latbound}
  For any instance  of \strp, the
  optimal value of the latency group Steiner instance   in
  Step~\ref{step:trppalg3} of Algorithm  is at most
  .
\end{lemma}

\begin{pf}
Let  be an optimal decision tree for the given  \strp instance . Note that any internal node of , labeled
, has two children corresponding to the realized scenario being in  ({\em yes} child) or  ({\em
no} child). Now consider the root-leaf path in  (and corresponding tour  in the metric) which starts at ,
and at any internal node , moves on to the \emph{no} child if , and moves to  the \emph{yes} child if . We claim that this tour is a feasible solution to , the latency group Steiner instance .

To see why, first consider any scenario  that branched off from path   in decision-tree ; let  be
the vertex where the tree path of scenario  branched off from .  If  then by the way we defined
, it follows the ``no'' child of , and so  .  On the other hand, if , then it
must be that  (again from the way  was defined). In  either  case, , and hence visiting  covers {\em all} groups, associated with scenario , i.e.  and . Thus  covers all groups of all the scenarios that branched off it in .

Note that there is exactly one scenario (say ) that does not branch off ; scenario  traverses
 in .
  Since  is a feasible
  solution for \strp,  must visit every vertex in .
  Therefore  covers all the groups associated with
  scenario : clearly  are covered;  is also
  covered unless  (however in that case group  has zero weight and does not need to be covered- see Definition~\ref{def:lgst}). Thus  is a
  feasible solution to .

We now bound the latency cost of tour  for instance . In path , let  (for each
) denote the coverage time for group ,
  and  (for  and ) the coverage time for group .
  The next claim shows that the latency of  for instance  is at most .


\begin{claim} The expected cost of , , which is exactly the latency of tour  for the latency group
Steiner instance .
\end{claim}
\begin{pf}
Fix any ; let  denote the shortest prefix of  containing a vertex from . Note that by
definition,  has length . We will lower bound separately the contributions of  and
 to the cost of .

As all but the last vertex in  are from , by definition of ,
the path  traced in the decision-tree  when scenario  is realized, agrees with this prefix . Moreover,
no vertex of  is visited before the end of . So under scenario , the total arrival time for vertices  is at least . Hence  contributes  at least  towards .

Now consider some vertex ; let  denote the shortest prefix of  containing a
-vertex. Note that  has length , and it is a prefix of  since . As observed earlier, the path traced in decision tree  under scenario   contains : so vertex
 is visited (under scenario ) only after tracing path . So the contribution of  (under scenario
) to  is at least , i.e. the contribution of  is at least 
\end{pf}


Thus we have demonstrated a feasible solution to  of  latency at most .
\end{pf}

It remains to bound the expected additional latency incurred in Step~\ref{step:trp3} of Algorithm~\ref{alg:trpalg} when
a random scenario is realized. Below we assume a  approximation algorithm for latency group Steiner tree (from Corollary~\ref{cor:lgs}).


\begin{lemma}\label{lem:alg3}
  At the end of Step~\ref{step:trp3} of , the realized scenario lies in . 
    The expected increase in latency due to  this step is at most .
\end{lemma}
\begin{pf}
The proof that the realized scenario always lies in the  determined in Step~\ref{step:trp3} is identical to that
in Claim~\ref{cl:alg2} of the \isoprob algorithm, and is omitted. We now bound the expected latency incurred. In the solution  
to the latency group Steiner instance , define  as the coverage time for group , ; and  as the coverage time for group ,  and .

Let  denote the realized scenario. Suppose that  in Step~\ref{step:trp3}. Then by definition of the
parts s, we have  and . So the length along  until  equals . Moreover the total
length spent in this step is at most , to travel till  and then return to  (this uses the symmetry and triangle-inequality properties of the metric). So the latency
of any -vertex increases by at most this amount. Furthermore we claim that the latency of any  increases by
at most : this is clearly true if ; on the other hand if 
then  is visited before  and so it only incurs latency . So the increase in latency of  is
at most .

If  then by the proof of Claim~\ref{cl:trp-partn} the realized scenario  satisfies: , group  is not visited by  (so  is undefined), and all of  is visited by . In this case the total
latency of  is  which is clearly at most ; note that  here.

Thus the expected latency incurred in Step~\ref{step:trp3} is at most  which is twice the latency of  for the latency group
Steiner instance . Finally, since  is a -approximate solution to  and using
Lemma~\ref{lem:latbound}, we obtain the claim.
\end{pf}

Finally, combining Claim~\ref{cl:trp-partn}, Lemma~\ref{lem:alg3} and Claim~\ref{cl:trp-subadd}, 
by a proof identical to that of Theorem~\ref{thm:isolation}, it follows that the final \strp solution has cost . This completes the proof of 
Theorem~\ref{thm:main3}. 
\medskip

We note that for the \strp problem on metrics induced by a tree, our algorithm achieves an  approximation ratio (the guarantees in Theorem~\ref{thm:gso} and Corollary~\ref{cor:lgs} improve by a logarithmic  factor on tree metrics). There is also an -hardness of
approximation the \strp problem on tree metrics~\cite{Vish-thesis}. So there is still a logarithmic gap between the best upper and lower bounds for the \strp problem on tree metrics. In going from tree metrics to general, we lose another logarithmic  factor in the approximation ratio. 






\section{Concluding Remarks}

In this paper, we studied the problem of constructing optimal decision
trees;  this widely studied problem was previously known to admit logarithmic
approximation algorithms for the case of uniform costs or uniform
probabilities. The greedy algorithms used in these cases do not extend
to the case of non-uniform costs and probabilities, and
we gave a new algorithm that seeks to be greedy with respect to two
different criteria; our -approximation is
asymptotically optimal. We then considered a generalization to the adaptive traveling salesman
problem, and obtained an
-approximation algorithm for this adaptive TSP problem. We also showed
that any asymptotic improvement on this result would imply an improved
approximation algorithm for the group Steiner tree problem, which is a long-standing
open problem. Finally, we gave an -approximation algorithm for the adaptive
traveling repairman problem--- closing the 
 gap
between the known upper and lower bounds in this case remains an 
interesting open problem.



\section*{Acknowledgments.}
A preliminary version appeared in the proceedings of the International Colloquium on Automata, Languages and Programming (ICALP), 2010. We thank Ravishankar Krishnaswamy for many useful conversations; the results on the adaptive traveling repairman problem were obtained in joint discussions, and we thank him for permission to include the results here. We also thank the MOR referees for helpful suggestions that improved the presentation of the paper. 
A. Gupta's research was supported in part by NSF awards CCF-0448095 and CCF-0729022, and an Alfred P.~Sloan Fellowship. R. Ravi's  research was supported in part by NSF grant
  CCF-0728841.


\bibliographystyle{alpha} \bibliography{stsp} 


\appendix


\section{Hardness of Approximation for \stsp}
\label{app:hardness}

We show that \stsp is at least as hard to approximate as group Steiner tree.
\begin{theorem}\label{th:iso-hard}
If there is an -approximation algorithm for \stsp then there is an  -approximation algorithm for group Steiner tree.
Hence \stsp is  hard to approximate even on tree metrics.
\end{theorem}
\begin{pf}
This reduction is similar to the reduction~\cite{CPRAM11} from set-cover to the \dtp; we give a proof in
context of \stsp for completeness.

  Consider an arbitrary instance of group Steiner tree on metric 
  with root  and groups ; let  denote its
  optimal value. Assume without loss of generality that  for
  all , and the minimum non-zero distance in  is one. We construct an instance of \stsp as follows. Let
   where  is a new vertex (representing a copy of ), and define metric  on
   as:
  

There are  scenarios in the \stsp instance:  for ,
  and , with probabilities
  
Above  is some large value. The root in the \stsp instance remains . Let 
denote the optimal value time of this instance. We will show that  which would
prove the  theorem.

\noindent {\bf (A) .} Consider the optimal solution to the \stsp
instance; let  denote the -tour traversed by this decision tree under scenario . We now argue that
 is a feasible solution to the group Steiner  tree instance, i.e., . Suppose for a
contradiction that  does not visit any -vertex for some . Then observe that the -tour
traversed by this decision tree under scenario  is also , since the  decision tree can not distinguish scenarios
 and  (the only way to do this is by visiting some -vertex). However this violates the requirement
that the tour (namely ) under scenario  must visit all vertices . Finally, we have
 as required. 

\noindent {\bf (B) .} Let  denote an optimal
  -tour for the given \mgs instance, so . Consider the
  following solution for \stsp:
  \begin{enumerate}
  \item Traverse -tour  to determine whether or not  is
    the realized scenario.
  \item If no demands observed on  (i.e. scenario  is realized), visit vertex  and stop.
  \item If some demand observed on  (i.e. one of scenarios  is realized), then visit {\em all} vertices in  along an
    arbitrary -tour and stop.
   \end{enumerate}
It is clear that this  decision tree is feasible for the \stsp instance.  For any , let  denote the
-tour traversed under scenario  in the above \stsp  decision tree. We have
  , and
   for all . Thus the resulting \stsp objective is
  at most:
  
  Thus we have the desired reduction.
\end{pf}



\end{document}
