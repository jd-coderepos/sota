\documentclass[12pt]{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage{fullpage}
\usepackage{url}

\newtheorem{lemma}{Lemma}
\newtheorem{theo}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{fact}{Fact}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}

\newcommand{\ex}[1]{\bigskip \noindent {\bfseries #1.}}
\newcommand{\exx}[1]{\bigskip \noindent {(\bfseries #1)}}

\newcommand{\E}[1]{\mathbf{E}\left[#1\right]}
\newcommand{\Ep}[1]{\mathbf{E}#1}
\newcommand{\EE}[2]{\mathbf{E}_{#1}[#2]}
\newcommand{\V}[1]{\textrm{Var}[#1]}
\newcommand{\val}[1]{\textrm{value}(#1)}
\newcommand{\ro}[1]{\textrm{root}(#1)}
\newcommand{\Mf}[2]{M(#1,#2)}
\newcommand{\EMf}[2]{\mathbf{E}M(#1,#2)}

\newcommand{\remove}[1]{}



\newcommand{\prelim}[0]{2}
\newcommand{\pendantFirst}[0]{1}
\newcommand{\tightSampling}[0]{10}
\newcommand{\ESrecCDT}[0]{2}
\newcommand{\mainLemmaSparse}[0]{3}




\begin{document}


\title{The Query-commit Problem}
\author{Marco Molinaro\thanks{Tepper School of Business 
Carnegie Mellon University, Pittsburgh, PA
15213, USA, \mbox{molinaro@cmu.edu}} \and R. Ravi \thanks{Tepper School of Business 
Carnegie Mellon University, Pittsburgh, PA
15213, USA, \mbox{ravi@cmu.edu}}}
\date{}		

\maketitle

\begin{abstract}
	In the \emph{query-commit problem} we are given a graph where edges have distinct probabilities of existing. It is possible to query the edges of the graph, and if the queried edge exists then its endpoints are irrevocably matched. The goal is to find a querying strategy which maximizes the expected size of the matching obtained. This stochastic matching setup is motivated by applications in kidney exchanges and online dating.
		
		In this paper we address the query-commit problem from both theoretical and experimental perspectives. First, we show that a simple class of edges can be queried without compromising the optimality of the strategy. This property is then used to obtain in polynomial time an optimal querying strategy when the input graph is sparse. Next we turn our attentions to the kidney exchange application, focusing on instances modeled over real data from existing exchange programs. We prove that, as the number of nodes grows, almost every instance admits a strategy which matches almost all nodes. This result supports the intuition that more exchanges are possible on a larger pool of patient/donors and gives theoretical justification for unifying the existing exchange programs. Finally, we evaluate experimentally different querying strategies over kidney exchange instances. We show that even very simple heuristics perform fairly well, being within 1.5\% of an optimal \emph{clairvoyant} strategy, that knows in advance the edges in the graph. In such a time-sensitive application, this result motivates the use of \emph{committing} strategies.
\end{abstract}


	
	\section{Introduction}
	
			The theory of matchings is among one of the most developed parts of graph theory and combinatorics \cite{lovasz}. Matchings can used in a variety of situations, ranging from allocation of workers to workplaces to exchange of kidney among living donors \cite{roth1}. However, the uncertainty present in most applications is not captured by standard models. In order to address this limitation we consider a stochastic variant of matchings.
			
	Before presenting the query-commit problem we describe an application in kidney exchanges which motivates our model. Unfortunately current patients who require a kidney transplant far outnumber the available organs. In the United States alone, more than 84,000 patients were waiting for a kidney in 2010 and 4,268 people in such situation died in 2008 \cite{unos}. However, a distinguished characteristic pertaining to kidney transplants is that they can be carried using the organ of a living donor, usually a relative of the patient. Such operations have great potential to alleviate the long waiting times for transplants. 
	
	One major issue is that many operations cannot be executed due to incompatibility between a patient and its donor. In order to overcome this, it is important to consider 2-way exchanges: Suppose patient  and its willing donor  are not compatible and the same holds for  and ; however, it is still possible that both patients can receive the required organ via transplants from  to  and from  to . This situation can be modeled using a \emph{compatibility graph}, where each node represents a patient/donor pair and edges represent the cross-compatibility between such pairs. The set of transplants that maximize the number of organs received is then given by a maximum matching in this graph \cite{roth1, roth2}. 
	
	However, such a model does not take into account uncertainty in the compatibility graph. In practice, preliminary tests such as blood-type and antigen screening are used to determine only the likelihood of cross-compatibility between pairs. Final compatibility can only be determined using a time-consuming test called \emph{crossmatching}, which involves combining samples of the recipients' and donors' blood to check their reactivity. Furthermore, such a test must be performed close to the surgery date, since even the administration of certain drugs may affect compatibility \cite{crossmatching}. That is, the transplant should be executed as soon as it is detected that two patient/donor pairs are determined to be cross-compatible.
	
	The kidney exchange application motivates the \emph{query-commit problem}, which can be described briefly as follows. We are given a weighted graph  where the weight  indicates the probability of existence of edge . In each time step we can query an edge  of  and one of the following happen: with probability  (corresponding to the event that  actually exists) its endpoints are irrevocably matched and removed from the graph; with probability  (corresponding to the event that  does not exist)  is removed from the graph. Notice that at the end of this procedure we obtain a matching in , dependent on both the choices of the queries and the randomness of the edges' existence. In the query-commit problem our goal is to obtain a query strategy that maximizes the expected cardinality of the matching obtained. 
		
		\paragraph{Our results.} 	In this paper we address the query-commit problem from both theoretical and experimental perspectives. First, we show that a simple class of edges can be queried without compromising the optimality of the strategy. This result can be used to simplify the decision making process by reducing the search space. In order to illustrate this, we show that employing this property we can obtain in polynomial time an optimal querying strategy when the input graph is sparse. 
		
		Then we turn our attentions to the kidney exchange application, more specifically on instances for the query-commit problem modeled over real data from existing kidney exchange programs. In this context we are able to prove the following result: as the number of nodes grows, almost every such graph admits a strategy which matches almost all nodes. This result support the intuition that more exchanges are possible on a larger pool of patient/donors \cite{david, segev}. More importantly, it shows the potential gains of merging current kidney exchange programs into a nationwide bank.
		
		Finally, we propose and evaluate experimentally different querying strategies, again focusing on the kidney exchange application. We show that even very simple heuristics perform fairly well. Surprisingly, the best among these strategies are on average within 1.5\% of an optimal \emph{clairvoyant} or \emph{non-committing} strategy that knows in advance the edges in the graph. This indicates that the \emph{committing} constraint is not too stringent in this application. Thus, in such time-sensitive application, this result motivates the use of \emph{committing} strategies.
				
		
		\paragraph{Related work.} \cite{chen} recently introduced a generalization of the query-commit problem which contains the extra constraint that a strategy cannot query too many edges incident on the same node. In addition to kidney exchange, the authors also point out the usefulness of this model in the context of online dating. Their main result is that a simple greedy querying strategy is a within a factor of  from an optimal strategy. \cite{mestre} use a combination of two strategies in order to obtain an improved approximation factor of . \cite{viswanath} consider a further extension where edges have values and the goal is to find a strategy that maximize the expected value of the matching obtained. Using an LP-based approach they are able to obtain a strategy which is within a constant factor from an optimal strategy.
		
		We note, however, that in the case of the query-commit problem (i.e. when there is no constraint on the number of edges incident to a node that a strategy can query) every strategy which does not stop before querying all permissible edges is a -approximation \cite{chen}. This follows from two easy facts: (i) for every outcome of the randomness from the edges, such a strategy obtains a maximal matching and (ii) every maximal matching is within a factor of  from a maximum matching. 
		
		The query-commit problem is similar in nature to other stochastic optimization problems with irrevocable decisions, such as stochastic knapsack \cite{goemansKnapsack} and stochastic packing integer programs \cite{goemansIP}. In both \cite{goemansKnapsack, goemansIP} the authors present approximation algorithms as well as bounds on the benefit of using an adaptive strategy versus a non-adaptive one. 
		
		Different forms of incorporating uncertainty have been also studied \cite{SP} and in particular stochastic versions of classical combinatorial problems have been considered in the literature \cite{covering,paths}. Matching also has its variants which handle uncertainty via a 2-stage model \cite{katriel} or in an online fashion \cite{onlinePD, onlineMatching, goel, KVV, unreliable, vazirani}. The latter line of research has been largely motivated by the increasing importance of Internet advertisement.  
		
		The kidney exchange problem, in its deterministic form, has received a great deal of attention in the past few years \cite{david, roth3, roth1, roth2, segev}. In the previous section we argued that 2-way exchanges can increase the number of organs transplanted, but of course larger chains of exchanges can offer even bigger improvements. Unfortunately, considering larger exchanges makes the problem of finding optimal transplant assignments much harder computationally even if all the edges are know in advance \cite{david}. Nonetheless, \cite{david} present integer programming based algorithms which are able to solve large instances of the problem, on scenarios with up to 10,000 patient/donor pairs. The authors point out, however, the importance of considering other models which take into account the uncertainty in the compatibility graph. Finally, \cite{sandholm,unverDynamic,zenios} address the dynamic aspect of exchange banks, where the pool of patients and donors evolve over time. 
		
	The remainder of the paper is organized as follows. In Section \ref{prelim}	we present a more formal definition of the query-commit problem as well as multiple ways of seeing the process in which matchings are obtained from strategies. Then we prove the important structural property (Lemma \ref{pendantFirst}) which is used to obtain in polynomial time optimal strategies for sparse graphs (Section \ref{theoretical}). Moving to the kidney exchange application, we describe in Section \ref{theoKidney} a model for generating realistic compatibility graphs and prove that, as the number of nodes grows, most of these instances admit strategies which match almost all nodes. In Section \ref{experimental} we address the issue of estimating the value of a strategy as well as computing upper bounds on the optimal solution, and conclude by presenting experimental evaluation of querying strategies. As a final remark, the proofs of all lemmas which are not presented in the text are available in the appendix. 
	
	\section{Preliminaries} \label{prelim}

	We use  and  to denote respectively the set of nodes and edges of a given undirected graph , and use  and  to denote their cardinalities. In addition we use  to denote the cardinality of the maximum matching in . We define the neighborhood of a node  as the set  and the (edge) neighborhood of an edge  as the set . Notice an edge is included in its own neighborhood. We ignore isolated nodes in all subsequent graphs. 
	
	  When  is a rooted tree,  denotes its root and  is the subtree of  which contains  and all of its descendants. When  is a binary tree, we use  and  to denote respectively the left and right children of a node ; we also call  and  respectively the \emph{left subtree} and \emph{right subtree} of node . The \emph{height} of a rooted tree is the length (in number of nodes) of the longest path between the root and a leaf.
	
	 Throughout the paper we will be interested in weighted graphs  where  associates nonzero weights to the edges of ; we refer to them simply as weighted graphs. A \emph{scenario} or \emph{realization}  of , denoted by , is a subgraph of  obtained by including each edge  independently with probability . Note there can be up to  possible realizations in .

		Now we describe, somewhat informally, the dynamics of querying strategies. Consider a weighted graph , a scenario  and a querying strategy . We start with an empty matching and  makes its first query for an edge  of . If  then  is added to the current matching and we obtain the residual graph (i.e. the set of permissible edges) . If  then  is not added to the matching and the residual graph is . At this point  queries any other edge of ; usually we focus on the case that the new edge belongs to , since edges outside  cannot be added to the matching. The process then continues in the same fashion. We remark that  is oblivious to the scenario  and only uses information from previous queries in order to decide its next query.
				
	  In order to make this process more precise we use decision trees to represent querying strategies. In our context, a \emph{decision tree}  is a binary tree with the following properties: (i) each internal node  corresponds to a query for an edge ; (ii) all nodes in the right subtree of  correspond to queries for edges in ; (iii) all nodes in the left subtree of  correspond to queries for edges in .
It is also useful to associate to each node  the residual graph  in the following recursive way: ,  and . Figure \ref{fig:exDT} presents an example of a decision tree.
	  
	A decision tree  can be interpreted as a querying strategy as follows: First query the edge  associated to the root of ; if this query is successful then add  to the current matching and proceed querying using the right subtree of , otherwise just proceed querying using the left subtree of . Hence, the execution of  over a scenario  induces a path of nodes  in , where  is the sequence of edges queried by  and  the sequence of residual graphs. 
	
	Notice that, since  only queries permissible edges, the matching obtained is exactly the set of queried edges which belong to ; this matching is denoted by . After unpacking previous definitions, we can also write the random matching  recursively as follows (where  and  to simplify the notation):  with probability  and  with probability . Thus, the expected size of  is given by

	where we use  instead of  to simplify the notation. 
			
	We remark that every strategy can be represented by a decision tree, so we use  to denote a decision tree corresponding to a strategy  and use both terms interchangeably.
	
	Making use of the above definitions, we can formally state the query-commit problem: given a weighted graph  with , we want to find a decision tree  for  that maximizes . The value of an optimal solution is denoted by .
		
	We are interested in finding computationally efficient strategies for the query-commit problem. Since decision trees may be already exponentially larger then the input graph, our measure of complexity must allow implicitly defined strategies. We say that a strategy is \emph{polynomial-time computable} if the time used to decide the query in each step is bounded by a polynomial on the description of the input graph; this includes any preprocessing time (i.e. time to construct a decision tree). In Section \ref{optSparse} we present structures similar to decision trees that are useful to describe time-efficient strategies.
		
\begin{figure}
	\centering
		\includegraphics[scale=0.60]{exDT.eps} \hspace{90pt}
		\includegraphics[scale=0.60]{exDT2.eps}
	\caption{(a) Input graph  with edges labeled from 1 to 4. (b) Example of decision tree  for , with labels corresponding to  for each internal node  of . Let  denote the father of , so that ;  is the subgraph of  consisting of edges  and . The bold path in  is the path obtained by executing  over the scenario  which consists of edges ,  and . Moreover, .}
	\label{fig:exDT}
\end{figure}



	\section{General theoretical results} \label{theoretical}
	
	We say that an edge of a graph is \emph{pendant} if at least one of its endpoints has degree 1. As a start for our theoretical results, we show that pendant edges can be queried first without compromising the optimality of the strategy. This observation will be fundamental for the development of the polynomial-time computable algorithm for sparse graphs and is also used in the heuristics tested in the experimental section. 
		
	\begin{lemma} \label{pendantFirst}
		Consider a weighted graph  and let  be a pendant edge in . Then there is an optimal strategy whose first query is . 
	\end{lemma}
		

	In order to illustrate the relevance of pendant edges we mention the following result.
	
	\begin{lemma}
		Suppose  is a weighted forest and let  be a strategy that always queries a pendant edge in the residual graph. Then for every , .
	\end{lemma}


	\subsection{Optimal strategies for sparse graphs} \label{optSparse}
	
	We say that a graph  is -sparse if . In this section we exhibit a polynomial-time optimal strategy for -sparse graphs when  is constant. We focus on connected graphs but the result can be extended by considering separately the connected components of the graph. 
	
	So let  be a connected -sparse graph and we further assume (for now) that  does not have any pendant edges; the rationale for the latter is that from Lemma \ref{pendantFirst} we can always start querying pendent edges until we reach a residual graph which has none.
	
	\paragraph{Contracted decision trees.} First, we need to introduce the concept of a \emph{contracted decision tree} (CDT), which generalizes the decision trees introduced in Section \ref{prelim}.

	 Given a strategy  for a weighted graph  we use  to denote the set of possible residual graphs after the execution of , that is, . Then a \emph{contracted decision tree}  for  is a rooted tree where every node  is associated to a residual graph  and every internal node  is associated to a query strategy  for  satisfying the following: (i) ; (ii) every internal node  of  has exactly  children  and ; (iii) if  is an ancestor of  in  then . Figure \ref{fig:exCDT} presents an example of a contracted decision tree. 
	 
	\begin{figure}
	\centering
		\includegraphics[scale=0.60]{exCDT0.eps} \hspace{45pt}
		\includegraphics[scale=0.60]{exCDT.eps} \hspace{45pt}
		\includegraphics[scale=0.60]{exCDT2.eps}
	\caption{(a) Input graph  with edges labeled from 1 to 4. (b) A decision tree  for the subgraph of  induced by edges 1 and 2. The set  consists of the graphs  and , where  is the subgraph of  induced by edges 3 and 4,  is the empty graph and  is induced by edge 3. (c) A CDT  for  where . The graphs  for the children of  are, respectively from left to right, ,  and . The decision tree corresponding to  is  in Figure \ref{fig:exDT}.b. Conversely,  can be obtained by contracting  in .}
	\label{fig:exCDT}
\end{figure}
	 
	 A few remarks are in place. First, condition (iii) is not really fundamental in the definition, although it avoids trivial cases in future proofs. More importantly, notice that a decision tree is simply a CDT where strategy  queries a single edge of . Also, a CDT can be seen as decision tree where some of its subtrees were contracted into a single node and, conversely, we can obtain a decision tree from a contracted decision by expanding the partial strategies 's into decision trees.
	
	A CDT  can be interpreted as a strategy in a similar way as in decision trees: start with an empty matching at the root  and query according to , which gives a particular residual graph  depending on the current scenario ; add  to the current matching and proceed querying using the subtree , where  is the child of  with . The expected size of the matching obtained by a CDT  can be written in an recursive expression similar to \eqref{ESrec}:

	where  is the probability with respect to the scenarios of  that the residual graph is  after employing  to .
	
	\paragraph{Decomposition of  and filtering of the strategy space.} Now we turn again to the problem of finding an optimal strategy for . Let  be the set of nodes of  which have degree at least 3. Since the nodes in  have degree at most 2, all of its connected components are either paths of cycles; moreover, we claim that all of them are actually paths. By means of contradiction suppose a connected component of  is a cycle . Since  is connected, it must contain an edge from a node  to a node in . However, this implies that  has degree at least 3 in  and hence , contradicting the fact  is a component of .	
	
	In light of the previous claim it is useful to think about the structure of  is terms of  and the paths  that are the connected components of . Notice that the edges of  which have an endpoint in  are not present in this decomposition; however, there are at most  of these edges, which allows us to ignore them for most part of the discussion of the algorithm. To see this upper bound on the number of such edges first notice that ; this holds because  has no pendant edges and hence all of its nodes have degree at least two, so summing over all degrees in  we get	 Then if  is the number of edges with some endpoint in  we again add all degrees of  to obtain  obtaining the desired bound.
	
	This result also leads to a bound on the number of paths 's. To see this, consider a path  and let  and  be its endpoints. The assumption that  does not have pendant edges implies that there are two distinct edges with one endpoint in  and the other in . Since there are at most  of these edges in  and since the 's are disjoint, we have that there are at most  paths 's. 
	
	Exploiting the structure of  highlighted in the previous observations, we can construct in polynomial time a CDT which gives an optimal querying strategy. We briefly sketch the argument used to obtain this result. The main observation is that after querying an edge  in  we always obtain a residual graph where some edges in  are now pendant. Then we can use Lemma \ref{pendantFirst} to keep querying pendant edges, which leads to a residual graph that does not contain any edges of . These observations imply that in order to obtain an optimal strategy for  we essentially only need to decide which edge to query first in each , as well as an ordering among these edge and the edges with endpoint in ; all the rest of the strategy follows from querying pendant edges. Moreover, using the fact that there are at most  edges with endpoint in  and at most  paths 's, we can enumerate all these possibilities in time  and obtain the desired result.
	
	Now we formalize these ideas. Consider a subgraph  of  and consider one of the paths 's  given by . For an edge  we define  as the strategy which queries edges  in this order and then queries  in this order, always ignoring edges which do not belong to . Essentially  is querying  first and then edges in  which becomes pendant. Notice that there are actually two strategies satisfying the above properties, depending on the orientation of the path ; so we fix an arbitrary orientation for the paths 's in order to avoid ambiguities. 
	
	It follows directly from the definition of  that for any residual graph  we have . More specifically, the set of residual graphs  can only contain the following graphs: , ,  and . For instance suppose nodes  and  both belong to ; then  is the residual graph of the scenario  iff  is the endpoint of edge in  and  is not. 
	
	Now the next lemma makes formal the assertion that after querying the first edge of  we can just proceed by querying pendent edges.
	
	\begin{lemma} \label{mainLemmaSparse}
		Let  be a subgraph of . Let  be an edge in  and suppose that there is an optimal strategy for  that queries  first. Then there is an optimal CDT for  whose root is associated to the strategy .
	\end{lemma}		

		Now let  be the family of all CDT's for  and its subgraphs with the following properties. A CDT  for a subgraph  of  belongs to  if: (i)  has height at most ; (ii) each node  of  is either associated to a strategy which consists of querying one edge incident to  or it is associated to a strategy  for some  and some . The usefulness of this definition comes from the fact that we can focus only on this family of CDT's.
		
	  \begin{lemma} \label{Fopt}
			There is an optimal CDT for  which belongs to .
		\end{lemma}
		
		This lemma is formally proved in the Online Supplement, but the structure of the proof is the following. First we apply Lemma \ref{mainLemmaSparse} repeatedly to show the existence of an optimal CDT satisfying property (ii) of . To complete the proof, we make use of the fact that there are at most  edges incident to  and at most  paths 's in  to we show that there is one such optimal CDT in  satisfying the height requirement (i). 
		
		The main point in restricting to CDT's in  is that there is only a polynomial number of them, as stated in the next lemma.
		
		\begin{lemma} 
			The family  has at most  CDT's. \label{sizeF}
		\end{lemma}
		
		\paragraph{Computing the value of a tree in .} In light of the previous section, we only need find the best among the (polynomially many) CDT's in  in order to obtain an optimal strategy for . However, we still need to be able to efficiently calculate the expected size of the matching obtained by each such tree. In this section we show that this can be done recursively employing equation \eqref{ESrecCDT}. 
		
		Consider a CDT  and let  be a node in . Assume that we have already calculated  for all proper descendants  of . To calculate  we consider two different cases depending of . 
		
		\medskip \noindent \emph{Case 1:  only queries an edge  incident to .} As mentioned previously, . Since  is the probability that  belongs to the realization of , we have that  and the probability that  is the residual graph is also . Therefore, if  and  are the children of  associated respectively to the residual graphs  and , then equation \eqref{ESrecCDT} reduces to:

		After inductively obtaining all the terms in right hand side of the previous expression,  can be computed directly.
		
		\medskip \noindent \emph{Case 2:  equals to .} We proceed in the same way as in Case 1, calculating the terms of the right hand side of \eqref{ESrecCDT}. However, these computations are not as straightforward as in the previous case. Given a random matching , let  denote the event that node  is matched in  and let  be the complementary event. The following lemma is the main tool used during this section and can be obtained via dynamic programming.
		
		\begin{lemma} \label{valueSPi}
			Consider a path  and a subgraph  of . Also consider an edge  and define . Then there is a procedure which runs in time polynomial in the size of  and computes the values , ,  and .
		\end{lemma}
		
		The previous lemma directly gives that  can be computed in polynomial time, so we only need to compute the probabilities that  is the residual graph after employing  to . For that, let  be such that . Recall that  can only contain graphs from the list: , ,  and . It is easy to see that we can write the probability of obtaining each residual graph in  using the probabilities in Lemma \ref{valueSPi}. For instance, the probability of obtaining  is exactly 

		
		Therefore, Lemma \ref{valueSPi} implies that there is an efficient algorithm to compute all terms in the right hand side of equation \eqref{ESrecCDT}, which gives an efficient way to compute  as in Case 1. 
		
	\paragraph{Putting everything together.} Consider a connected -sparse graph , possibly containing pendant edges. We define a strategy  for  which proceeds in two steps. First,  queries pendant edges until none exists. At this point we have a residual graph  with no pendant edges. In the second step it queries according to an optimal strategy  for . Applying Lemma \ref{pendantFirst} repeatedly, and using the fact that  is optimal, we get that  is an optimal strategy for .
	
	In order to prove that  is polynomial-time computable we only need to show that  is polynomial-time computable. To do so, we need the fact that every connected component of  is -sparse, which follows from successive applications of the following easy lemma.
	
	\begin{lemma} \label{delSparse}
		Let  be a connected -sparse graph. Then for every edge , the connected components of  and  are -sparse.
	\end{lemma}

	Let  be the connected component of . Since  is -sparse, we can use the tools from previous sections to find an optimal strategies  for the : we enumerate at most  CDT's for  and calculate the value of each of them using the procedure outlined in the previous subsection; then letting  be the strategy among those which has largest value we get that  is optimal for  (cf. Lemma \ref{Fopt}). Then an optimal strategy  for  is obtained by querying according to strategy , then  and so on. Notice that the total time needed to compute the 's is bounded by , which is polynomial in  for constant . Thus,  is polynomial-time computable and we obtain the desired result. 
	
		\begin{theo}		
			Let  be a connected weighted graph satisfying  for a fixed . Then there is polynomial-time computable optimal strategy for .
		\end{theo}


	\section{Theoretical results for kidney exchanges} \label{theoKidney}

		Now we focus on the kidney exchange application for the query-commit problem. In this context, weighted graphs are interpreted as weighted compatibility graphs: each node represents a pair patient/donor and the weight of an edge represents the likelihood of cross-compatibility between its endpoints. Our main result in this section is to show that the majority of compatibility graphs admits a simple querying strategy that matches essentially all of its nodes. In light of this result, a strong motivation for creating a large unified bank of patient and donors is obtained.
		
		\subsection{Generating weighted compatibility graphs} \label{kidneyModel}
		
		In order to make the previous claim formal we need to introduce a distribution of compatibility graphs.	In the context of deterministic kidney exchanges, \cite{saidman} introduced a process to randomly generate \emph{unweighted} compatibility graphs. This process is modeled over data maintained by the United Network for Organ Sharing in order to produce realistic instances and several works have considered slight variations of it \cite{david,roth3,roth1,saidman,segev}. Two physiological attributes are considered to determine the incompatibility of a patient and a donor. The first is their ABO blood type, where a patient is blood-type incompatible with a donor if their blood-type pair is one of the following: O/A, O/B, O/AB, A/B, A/AB, B/A and B/AB. The second factor is tissue-type incompatibility and represented by PRA (percent reactive antibody) levels. We now briefly describe the process from \cite{saidman} which generates unweighted graphs and then we mention the slight modification that we use to generate weighted graphs. 
		
		A pair patient/donor is characterized by 5 quantities: the ABO blood type of the patient, the ABO blood type of the donor, the indication if the patient is the wife of the donor, the PRA level of the patient and the indication if the patient is compatible with the donor. A random pair patient/donor is obtained by assigning independently a value for the first four quantities and then picking the compatibility depending on these values. The distribution of these values is described in detail in \cite{saidman} and we only highlight one key property:
		
		\begin{fact}\label{prTypeInc}
			For every pair of blood types , the probability that a random pair patient/donor has blood type  and is incompatible is nonzero.
		\end{fact}
		
		In order to generate an unweighted compatibility graph on  vertices we first sample random pairs patient/donor until obtaining  incompatible pairs; each pair is added as a vertex to the graph. Then for every pair  of nodes in the graph, the probability  of cross-compatibility between them is defined based on their physiological characteristics. A key property is that for any two pairs patient/donor  and  the quantity  is nonzero if and only if their blood types are cross-compatible (i.e. the patient of  is blood-type compatible with the donor of  and the patient of  is blood-type compatible with the donor of ). More specifically, there is a constant  independent of  such that 

	To conclude the construction of the graph, for each pair of nodes  a coin is flipped independently and with probability  an edge is added between  and . 
		
		Finally, the modification of the above procedure to generate weighted compatibility graphs consists of changing the last step: if  then the edge  is added to the graph and  becomes its weight.
		
		\subsection{An almost optimal strategy}

		In this section we present a simple querying strategy that achieves  for almost all weighted compatibility graphs  generated by the above procedure. To obtain such strategy we decompose the graph into cliques and complete bipartite graphs based on blood-type compatibility. Then we obtain good strategies for these structured subgraphs and finally compose them into a strategy for the original graph.
		
		Let  be the distribution of weighted compatibility graphs on  nodes generated by the procedure from the previous section. For a graph  in  we use  to denote the subset of vertices corresponding to the patient/donor pairs which have blood type . The lower case version  is used to denote the cardinality of . 
		
		Consider a random graph  and a node  of it. The first observation is that the probability that  has patient/donor blood type  is equal to  where  is a random pair patient/donor. Using the definition of conditional probability and Fact \ref{prTypeInc} we get that this probability is nonzero. Since we have finitely many blood types, this means that there is a constant  independent of  and  such that 

		This property, together with the symmetry of blood types of patients and donors, gives the following fact.
		
		\begin{fact} \label{sizeBloodTypes}
			The following properties hold for every  (where the expectation is taken with respect to the distribution ):
			\begin{enumerate}
				\item 
				\item 
			\end{enumerate}
		\end{fact}
		
		In order to describe and analyze the proposed querying strategy, we first focus on the set of graphs in  which have a typical number of nodes associated to each pair of blood types. That is, for  we consider , which is defined as the set of all graphs  in  which satisfy  We first show how to obtain good strategies for graphs in  and then argue that most of the graphs in  are in this family. 
		
		Fix  and consider a graph  in . Let  be the induced subgraph . Clearly these subgraphs partition the nodes of . Moreover, every two nodes in  are blood-type cross-compatible, since a patient is blood-type compatible with a donor with the same blood type. Therefore, the construction of  (and more specifically the properties of ) implies that  is a complete bipartite graph if  and a complete graph if . The fact that  and part 2 of Fact \ref{sizeBloodTypes} additionally give the following: if  there is complete bipartite subgraph  of  with equally sized vertex classes and with . 
		
		The motivation for partitioning  is that there are very simple strategies that work well in complete (bipartite) graphs, as shown in the next two lemmas. 
		
		\begin{lemma} \label{Sbipartite}
			Let  be a weighted complete bipartite graph with  vertices in each vertex class. Then for every  there is a strategy  such that , where . 
		\end{lemma}		
	
	The analogous lemma when  is a clique can be proved by applying the previous lemma to a complete bipartite subgraph of  with vertex classes containing  vertices. 
	
		\begin{lemma} \label{Scomplete}
			Let  be a weighted complete graph with  vertices. Then for every  there is a strategy  such that , where .  
		\end{lemma}

		Let  be the strategy given by Lemma \ref{Sbipartite} for  and  be the strategy given by Lemma \ref{Scomplete} for . Since the graphs 's and 's are disjoint, we can apply the strategies 's and 's sequentially and obtain a strategy  for  such that  where the factor  in the right hand side appears because the graph  is counted twice. Defining  and employing the bounds from Lemmas \ref{Sbipartite} and \ref{Scomplete}, we have that for every 

	
		Since , Fact \ref{sizeBloodTypes} gives that  and . This gives the following asymptotic bound on the quality of strategy  (assuming  large enough):

		where  is satisfies \eqref{LBprob}. Since  it follows that  and thus  matches almost all nodes of the typical graph  for sufficiently large . 
		
		Now we argue that most graphs in  belong to the family . That is, we want to lower bound the probability that a random graph  has values 's concentrated around the expectation. 
		
		Consider a random graph . Since the blood type of each node of  is chosen independently,  is  distributed according to a binomial process of  trials. Therefore, using Hoeffding's inequality \cite{BoucheronLB03} we get that the probability that  lies outside the interval  is at most , where  is the constant in \eqref{LPBloodType}. Since there are only 4 blood types, we can employ the union bound to estimating the probability that every  is close to its expected value. With this we obtain that the probability that the generated compatibility graph belongs to  is at least , which goes to 1 exponentially fast with respect to .
		
		Combining the fact that there is a good strategy for graphs in  with the fact that most graphs of  belongs to  gives the desired result. 
		
		\begin{theo} \label{thmKidney}
			For any  there is an  such that the following holds for every . Consider a random compatibility graph . Then with probability  over the distribution of  there is a polynomial-time computable strategy  which achieves . 
		\end{theo}


	\section{Computational results} \label{experimental}
		
		In this section we present an experimental evaluation of the performance of simple querying strategies. During our tests, we decided to focus on the application of the query-commit problem to kidney exchanges and therefore all weighted graphs used in the tests were generated randomly according to the procedure described in Section \ref{kidneyModel}. The results show that practical heuristics perform surprisingly well, \emph{even when compared to optimal non-committting strategies}. As a preparation to our experimental results, we first address the issue of estimating the value of a strategy and estimating an upper bound on .
		
		\subsection{Estimating the value of a strategy} \label{value}
		
		 Section \ref{optSparse} already indicated that it is not a trivial task to calculate this value exactly. Given a weighted graph  and a strategy , the direct way of calculating  involves finding  for every scenario . However, there is often an exponential (in ) number of such scenarios. When  is given as a decision tree  we can compute  using the recurrence \eqref{ESrec}; but again this procedure takes time proportional to the number of nodes in , which can be exponential in  and is therefore also impractical. 
		
		 Despite previous studies on the query-commit and related problems \cite{chen,goemansKnapsack, goemansIP}, there is currently no efficient algorithm to compute the expected value of a strategy and most results rely on an estimation of this value by sampling a subset of the scenarios. Our goal in this section is to address how well we can estimate  by sampling from  and establish the accuracy as function of the number of samples. 

		Given a weighted graph  on  nodes and a strategy , the natural way to obtain an unbiased estimation of  is to sample independently  scenarios  and take  as the estimate. Clearly . Moreover, since  for all  we also have that . In order to show that  is concentrated around the expectation we can simply employ Hoeffding's inequality to obtain that for every 

	According to this expression, we need approximately  samples in order to obtain a 95\% confidence interval equal to . 
	
	Notice that the previous bound does not use much of the structure of the matchings. In particular, \eqref{conc1} relies solely on the fact that the size of the matchings lie in . However, this simple concentration estimate is essentially best possible. This holds because we can construct a graph and a strategy  for it which obtains a small matching half of the time and a large matching half of the time. The variance on the size of the matching obtained by  is then essentially as large as possible when compared to any random variable taking values in ; in such case, Hoeffding's inequality is rather tight. More formally, we have that following lemma.
	
	\begin{lemma} \label{tightSampling}
		There is a graph  on  nodes and a strategy  for querying  such that

		for all .
	\end{lemma}
			
	\subsection{Upper bound on } \label{secUB}
		
		Clearly the size of a maximum cardinality matching in  is an upper bound on , since the maximum matching in any realization of  has size at most .
However, this bound can be made arbitrarily weak by considering small edge weights, e.g.  is a set of disjoint edges, each with weight , so  but . 
		
		Notice that actually  can be upper bounded by the expected size of the maximum matching over the realizations of ; that is, for  we have . This bound is tighter than  and, as Section \ref{exp} supports, is oftentimes very close to . An important remark is that  is a valid upper even for the \emph{non-commit} or \emph{clairvoyant} version of the problem, where the strategy can first find out exactly the edges in the realization and then decide which matching to take. Again we encounter the issue of calculating or at least estimating . 
		
		\paragraph{Estimating .} Clearly the sample average estimator used in the previous section can also be used to estimate  and the Hoeffding-based bound still holds. However, we can get substantially better concentration results by bounding the variance of the estimate more carefully. We make use of this tighter concentration to reduce the computational effort to estimate the upper bound on .
		
		Again consider a weighted graph  on  nodes and  edges and consider  independent scenarios . We take  as the estimation of , since clearly . Our goal now is to bound the variance of , which will then be used to provide tighter concentration results for . For that we need to introduce the concept of self-bounding functions. 
		
		A nonnegative function  is \emph{self-bounding} if there exist functions  such that for all  the following hold:

		and


		The following lemma motivates the definition of self-bounding functions.
		
		\begin{lemma}[\cite{BoucheronLB03}] \label{selfBoundingVar}
			Suppose  is a measurable self-bounding function. Let  be independent random variables with support on  and let . Then 
		\end{lemma}

		The connection between the previous lemma and our goal of estimating  comes from the fact that  can be seen as  independent indicator random variables for the edges of  and  can be see as a self-bounding function. To make this precise let  be the edges of  and let  be independent Bernoulli random variables with . Also, for an indicator vector  of the edges of , let  be the size of the maximum matching in the subgraph of  induced by  (i.e. which contains the edge  iff ). It is easy to see that  and the next lemma asserts that  is self-bounding.
		
		\begin{lemma} \label{selfBounding}
			The function  is self-bounding.
		\end{lemma}
		
		Since , Lemma \ref{selfBoundingVar} gives the desired bound . Now that we have a handle on this variance, we can evoke Bernstein's inequality \cite{BoucheronLB03} to bound the concentration of  as follows:

	
	When compared to \eqref{conc1}, the lack of an extra term  in the exponent makes \eqref{boundEmu} a much stronger bound.
						 		

	\subsection{Comparison of heuristics} \label{exp}
		
		In this section we present experimental results comparing simple querying strategies over random weighted compatibility graphs. All strategies use to some extent an optimization based on Lemma \ref{pendantFirst}, that is, they query pendant edges first if one exists. The heuristics considered in the experiments are described next.
		
		\begin{description}
			\item[Maximum probability.] This strategy first queries pendant edges in the residual graph. In case none exists, it queries the edge with highest weight. 
			
			\item[Minimum probability.] Similar to the previous strategy but edges are queried by decreasing weights.
			
			\item[Minimum degree.] This strategy first queries pendant edges in the residual graph. In case none exists, it queries the edge which has minimum degree in the residual graph, where the degree of an edge is defined as the sum of the degrees of its endpoints.
			
			\item[Minimum average degree.] The average degree of an edge  is defined as the sum of the weights of edges incident to  plus the sum of the weights of edges incident to . This strategy first queries pendant edges in the residual graph. In case none exists, it queries the edge which has minimum average degree in the residual graph.  
			
			\item[Batch successive matching.] First, this strategy queries pendant edges in the residual graph. After no more pendant edges exist, it finds a maximum cardinality matching in the residual graph. Then it queries all the edges in this matching in an arbitrary order. After all these edges are queried the process is repeated.
			
			\item[Batch successive weighted matching.] Similar to the previous strategy, but now in each round it computes a maximum \emph{weighted} matching with edge weights . 
			
			\item[Successive weighted matching .] First, this strategy queries pendant edges in the residual graph. After no more pendant edges exist, it finds a maximum weighted matching (with edge weights ) in the residual graph. Then it queries \emph{one} arbitrary edge in this matching. The process is then repeated.
			
			\item[Successive weighted matching .] Similar to the previous strategy, but the edge weights used in the maximum weighted matchings are simply .
	\end{description}
	
		The instances used in the experiments consist of random weighted compatibility graphs on 100 nodes, generated as described in Section \ref{kidneyModel}. Simulating exchange pools with 100 patient/donor pairs is optimistic but not unrealistic, as pointed out in \cite{roth3}. We also carried experiments in graphs with fewer than 100 nodes, but these graphs did not seem to be large enough to discriminate the querying strategies. 
		
		In order to estimate the value of each strategy we employed the sample average estimation discussed in Section \ref{value}. For each execution, we used 38,000 samples in order to obtain a good estimate; according to inequality \eqref{conc1}, with  probability, the estimate is within  of the actual expected matching obtained by the strategy. In order to obtain an estimated upper bound on  we used the sample average of  as described in Section \ref{secUB}. The number of samples was chosen to obtain an estimate which is within  of  with probability . 
		
		The results of the experiments are presented in Table \ref{tab:heu}. The first eight columns correspond to the strategies in the same order as they were described and the last column presents the upper bound  on . In each row, except the last one, we present the estimated value of the strategies on a given instance. The last row of the table indicates the average value of each strategy over all instances. 		
		
\begin{table}
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		maxP & minP & minDeg & minAvgDeg & batchSM & batchWSM & SWMq & SWMp &  \\
		\hline
			
		24.91 & 26.52 & 26.83 & {\bf 28.21} & 25.81 & 26.92 & 27.68 & 26.65 & 28.71 \\
		22.43 & 23.25 & 24.07 & {\bf 24.99} & 22.69 & 23.69 & 24.41 & 23.93 & 25.51 \\
	  18.64 & 19.03 & 19.11 & {\bf 19.53} & 18.87 & 19.24 & 19.36 & 19.17 & 19.82 \\
		16.36 & 15.57 & 16.62 & {\bf 16.79} & 16.49 & 16.56 & 16.67 & 16.75 & 16.85 \\
		19.51 & 21.36 & 20.69 & {\bf 22.22} & 20.06 & 21.06 & 21.86 & 20.17 & 22.56 \\
		22.74 & 23.20 & 23.53 & {\bf 25.14} & 23.20 & 24.00 & 24.77 & 24.02 & 25.64 \\
		24.16 & 24.38 & 25.02 & {\bf 26.44} & 24.54 & 25.33 & 26.08 & 25.28 & 26.82 \\
		21.36 & 22.07 & 23.18 & {\bf 24.30} & 22.41 & 23.15 & 23.90 & 23.68 & 24.66 \\
		25.39 & 26.06 & 27.20 & {\bf 27.98} & 26.11 & 26.79 & 27.47 & 26.63 & 28.43 \\
		22.87 & 21.56 & 23.61 & {\bf 24.06} & 23.06 & 23.47 & 23.81 & 23.60 & 24.43 \\
		\hline
		21.83 & 22.30 & 22.98 & {\bf 23.97} & 22.32 & 23.02 & 23.60 & 22.99 & 24.34 \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Comparison of different strategies. Each row corresponds to an instance, except the last one which reports the average value of each the strategy over all instances.}
	\label{tab:heu}
\end{table}
		
	Table \ref{tab:heu} shows that all these simple strategies perform very well. Surprisingly, these heuristics are actually close to optimal \emph{clairvoyant} strategies, since  is a valid upper bound for the \emph{non-commit} version of this problem as well. These results indicate that, for the kidney exchange application, the commit requirement in the formulation of the problem is not too restrictive, in that good solutions are still obtainable under this constraint. Notice that strategy {\bf minAvgDeg} in particular outperforms all others in \emph{every} instance of the test set. Moreover, its value stays always relatively close to the upper bound , being on average within .
		

		A caveat to these results is that the confidence interval of  for the value of the strategies does not allow complete discrimination among the best performing heuristics. The confidence intervals obtained from \eqref{conc1}, due to its generality, seems to be much looser than the actual bounds on the estimates. This is reinforced by Figure \ref{fig:conv}, which displays the sample average as a function of the number of samples; notice that with 2,500 samples the estimate already starts oscillating closely around the reported value of 28.21. We remark that a similar convergence profile holds for the other strategies tested.
		
		
\begin{figure}
	\centering
		\includegraphics[scale=0.45]{conv.eps}
	\caption{Sample average estimates of the value of {\bf minAvgDeg} on the first instance of the test set as a function of the number of samples. The horizontal axis indicates the number of samples divided by 100.}
	\label{fig:conv}
\end{figure}

		\section{Conclusions and future work} \label{conclusion}
		
		In this paper we considered the query-commit problem, a model for matchings which incorporates uncertainty on the edges of the graph in a way that is suitable for time-sensitive applications. By using the fact that some edges of the graph can be queried without compromising optimality, we show how to obtain an optimal querying strategy for sparse graphs in polynomial time. However, the dependency on the sparsity of the graph is doubly exponential. An interesting open question is to improve this running time, which may indirectly reveal other important properties of optimal strategies. On a similar note, another open question is to prove lower bounds on the computational complexity needed for finding optimal strategies in general graphs. 
	
		In Section \ref{experimental} we evaluated querying strategies over instances of the kidney exchange application and showed that even simple heuristics perform surprisingly well, even when compared to non-committing strategies. An important open question is designing a procedure which is able to provide even better strategies, possibly by starting with a heuristic and successively improving it (e.g. in a local search fashion). However, two hindrances for such procedures are the lack of an algorithm to compute the value of a strategy and the difficulty in representing strategies. As noted previously, the size of a decision tree may be exponential in the size of the input graph. 
	
		Another possibility is to extend the current model to even more realistic setups. For instance, one could consider correlated uncertainty on the edges. In the context of kidney exchanges, the uncertainty on the PRA level of a node introduces correlated uncertainty on all edges incident to it. In addition, recent works in deterministic kidney matchings have considered not only 2-way exchanges but also longer chains of exchanges \cite{david, roth3}, yielding additional transplants. A direction for future research is to study a suitable modification of the query-commit problem which can model uncertainty on longer exchanges. 
		
		Finally, Theorem \ref{thmKidney} indicates the potential of large kidney exchange programs. It would be of great value to obtain a more precise assessment of this potential and to address the logistic problems associated to nationwide transplant programs. 
		
	\paragraph{Acknowledgments.} We thank Tuomas Sandholm, Willem-Jan van Hoeve and David Abraham for helpful discussions.
			

\bibliography{query_commit}
\bibliographystyle{plain}


		
	\appendix

	\section{Proofs for Section 3}
	
	\subsection{Proof of Lemma \pendantFirst}
	
		Let  be an optimal strategy for  and consider the strategy  which first queries  and then proceeds exactly as . We clarify what happens in the following situation: the realization  contains both  and another edge  incident to , and  contains . Clearly  adds  to the matching in the first step and cannot add  while simulating ; in this case,  still probes  whenever  does so and adapts accordingly, although  is not added to the matching. 
		
		We claim that  is optimal, which proves the lemma. The definition of  leads to the following observations. If  does not belong to the realization  then . On the other hand, if  belongs to  then: (i) the th edge queried by  is exactly the th edge queried by  (the minus 1 comes from the fact that  queries  before simulating ) and (ii) every edge added by  to the matching  is also added to the matching , the only possible exception being a single edge  of  incident to ; since  is pendant,  contains at most one edge incident to . In the worst case  contains the edges in  and we still have . 
		
		Together, these observations imply that  and hence  is optimal.
	

	\subsection{Proof of Lemma 2}
	


Let  be the path of  induced by its execution over the scenario . By definition of ,  is pendant in the graph .
		
		By means of contradiction suppose that  is not a maximum matching in , namely there is a matching  in  such that . Then from Berge's Lemma \cite{lovasz} there must be an augmenting path  in  with respect to  and .
		
		Let  be an edge of  queried by . As mentioned in Section \prelim, , and since  we have that  belongs to . Then, since  is augmenting, there are two edges  which are incident to  and belong to . But since  is a pendant edge of , it must be that either  or  does not belong to  and without loss of generality we assume the former. The construction of  implies that there is an edge  with  such that  and  is incident to . 
		
		Since , we know that  also belongs to . Since  is a matching, it follows that  is not incident to an internal node of  and hence it must be incident to the endpoint of  which is also an endpoint of . However, this contradicts the fact that  is an augmenting path, which completes the proof of the lemma.




	\subsection{Proof of Lemma 3} 

		For each  let  be an optimal decision tree for . Consider the natural CDT  for  which has  and the children of  are the trees 's. We claim that  is optimal for . To argue that, let  be the decision tree corresponding to . Using the correspondence between CDT's and decision trees, it suffices to prove that  is an optimal decision tree for . 		
		
		We prove that  is an optimal decision tree for , for every node ; this is done by reverse induction on the depth of  in . The fact that the trees  are optimal removes the necessity of a separate base case, so consider a node  and assume that  is optimal for  and  is optimal for . By the definition of  we have that  is pendant in . Therefore, Lemma \pendantFirst asserts that there is an optimal decision tree for  whose root queries edge . Then it is easy to see that the optimality of  and  implies that actually  is one such optimal decision tree for , which concludes the inductive step and the proof of the lemma.


	\subsection{Proof of Lemma 4}
	
		First let us relax the definition of  by defining  as follows. A CDT  for a subgraph  of  belongs to  if each node  of  is either associated to a strategy which consists of querying one edge incident to  or it is associated to a strategy  for some  and some . Notice  is the set of CDT's in  which have height at most .
		
		\begin{claim}
			There is an optimal CDT for  which belongs to .
		\end{claim}
		
		\begin{proof}
			We prove that for each subgraph  of  there is an optimal CDT for  in . We proceed by induction on the number of edges of the subgraph, with trivial base case for subgraphs with no edges. 
			
			Consider a subgraph  of  with at least one edge and let  be an optimal querying strategy for it. Suppose that the first edge  queried by  is incident to . For each  let  be an optimal CDT for , the existence of which is given by the inductive hypothesis. Then define the decision tree  for  as follows: its root queries edge  and the subtrees of  are the trees . Using the recursive equation for  (equation (\ESrecCDT) in the main paper) it is easy to see that the optimality of the trees  implies that  and hence  is optimal. Moreover,  clearly belongs to , which concludes the inductive step in this case.
			
			Now suppose that  is not incident to ; this implies that  belongs to a path . Let  be an optimal CDT for  whose root is associated to , whose existence is guaranteed by Lemma \mainLemmaSparse. Again, for each  let  be an optimal CDT for . Now we construct the CDT  from  by replacing the subtrees of  by the trees . As in the previous case, the optimality of the trees  implies that  is optimal and also we have that . This concludes the inductive step and the proof of the lemma.
		\end{proof}

		\begin{proof}[Proof of Lemma 4]
			Let  be an optimal CDT for  with the minimum number of nodes. We claim that  has height at most .
			
			By means of contradiction suppose not and consider a path  from  to one of its leaves which has more than  internal nodes. Since there are at most  edges incident to  and at most  paths 's in , this means that either: (i) two nodes in  query the same edge incident to  or (ii) two nodes  in  are associated to two strategies  and , where both  and  belong to the same path . Case (i) is forbidden by the definition of a CDT, so we consider case (ii). 
			
			Without loss of generality assume that  is closer to the root of  than . Notice that by the definition of  we have that . Now we use the fact that  removes all edges in , that is, for every  we have . But the fact that  is a descendant of  implies that  is a subgraph of a residual graph in  and hence . This contradicts a previous observation that , which implies that  has height at most  and concludes the proof of the lemma.  
		\end{proof}
		

	\subsection{Proof of Lemma 5}
	
			Consider a tree ; we claim that each node in  has at most 4 children. Equivalently, if  is a node in  we want to upper bound . If  is associated to a strategy  then as noted previously we have that . Now if  is associated to a strategy that only queries one edge  incident to  then, as in standard decision trees, . It follows that the outdegree of each tree in  is at most 4.
		
		Since a tree in  has height at most , the previous degree bound imply that each such tree has at most  nodes. Now notice that each node has one of  possible strategies, because the strategies 's allowed in  are uniquely determined by the choice of an edge. These observations imply that there are at most  trees in .
		



	\subsection{Proof of Lemma 6}
	
	Let  and let  denote the strategy which queries edges  in order of the indices from  to . That is, if  then  queries the edges  and if  then it queries edges , always ignoring edges outside . To simplify the notation define the random matching  and without ambiguity we use  to also denote the random variable  corresponding to its cardinality. 
	
	First we prove that we can compute in polynomial time the following quantities corresponding to , for all : ,  and ; then we show how to use this information to compute the values required by the lemma. 
	
	We proceed in a dynamic programming fashion. First, it is trivial to compute the quantities associated to  and now we want to compute the quantities associated  assuming that those for the 's with  have already been computed. It is not difficult to see that  Moreover, the analogous expression with complementary conditionings also holds:  Finally, we also have . All these expressions can be easily computed using the information about  and  available by the dynamic programming hypothesis, concluding the proof of our claim. 
	
	By suitably relabeling the edges , the above result implies that we can also compute ,  and  for . A final remark is that using the law of total expectation we can also compute  and .
	
	Now let  be such that . We show how to compute the desired quantities by the lemma: , ,  and . It is useful to think of  roughly as the strategy which queries  first then queries according to  and the according to , for suitable  and .	To make this more formal we need to split into a few cases.
	
	\paragraph{Case 1: .} Notice that since  and  is the first edge queried by , the event  is the same as the event that the edge  exists (both which happen with probability ). In addition, since by hypothesis  is a path with more than one node, we have  and thus no edge in the set  intersects an edge in the set . This guarantees that the outcomes of, say, strategies  and  are independent. 
	
	These observations give the following equations:
	
	
	Also using the fact that , and hence  is not incident to either  or , we obtain that: 
	
	
	Using the additional independence remark made previously, we also have that 
		
	
	Therefore, using the fact that  and the laws of total probability and total expectations, we can compute  and  in polynomial time using the information about the 's. 
	
	\paragraph{Case 2:  or .} The equations for the expectations are the same as in Subcase 1.1. Now if  then 

		
	Applying a similar reasoning to the case  we obtain

	
	Finally, if  then  Therefore, we can again compute  and  in polynomial time using the information about the 's. 
	
	\bigskip Since we can calculate the probabilities associated to the 's in polynomial time and then according to Cases 1 and 2 use this information to calculate directly the values , ,  and , this concludes the proof of the lemma.


	\subsection{Proof of Lemma 7}

		First consider  and let  and  be its connected components (if  has only one component then we set  to be the empty graph). Since  and , we have that . Since each  is connected,  for all  and thus  for all . This shows that all components of  are -sparse. 
		
		Now consider  and let  be its connected components. Since  is connected it must contain a distinct edge connecting  to each . Thus, . Using the fact that , we obtain  Again using the fact that each  is connected, we get  for all  and hence  for all , which shows that all components of  are -sparse.
		

\section{Proofs for Section 4}

	\subsection{Proof of Lemma 8}
	
			Let  and  be the vertex classes of  and for any node  let  denote the set of edges incident to . Consider the strategy  that queries all edges in  (in an arbitrary order), then queries all edges in  and so on. 
			
			We want to upper bound the probability that a node  is unmatched in . In order to achieve this, consider the execution of  right before it starts querying edges in  and let  denote the random matching of  obtained by  at this point. Suppose  is unmatched in , which implies that no edge  could be added to the matching. If  could not be added to the matching then either  does not belong to the realization of  or  is already matched in . Thus, conditioning on the fact that all nodes in  are unmatched in  we get that 
 
			
			Since  does not depend on the existence of any edge , we can use the independence of the edges in  to obtain  for all  such that . Furthermore, notice that in every scenario  leaves at least  nodes from  unmatched, since it can match at most  nodes of . Then inequality \eqref{bipartiteEq} reduces to the desired bound . 
			
			Therefore the probability that  is matched in  is at least . Since  is bipartite,  is equal to the expected number of nodes of  which are matched, so by linearity of expectation . But then for any  we can bound the last summation as follows:

		which gives the desired bound of .	


\section{Proofs for Section 5}

	\subsection{Proof of Lemma \tightSampling}
			
		Consider the graph  depictured in Figure \ref{fig:tightSampling}. It consists of  disjoint paths and every edge has probability 1, except edge  which has probability . Consider the following adaptive strategy : it first queries  and, in case it belongs to the realization of ,  queries edges  sequentially in an arbitrary order; in case  does not belong to the realization,  queries edges   and  sequentially in an arbitrary order. Notice that in the first case  obtains a matching of size , whereas in the second it obtains a matching of size . 

\begin{figure}[htbp]
	\centering
		\includegraphics[scale=0.75]{tight.eps}
	\caption{Graph for proof of Lemma \tightSampling.}
	\label{fig:tightSampling}
\end{figure}
			
		Recall that  and that . From the previous paragraph we know that , where  denotes a binomial random variable with  trials and success probability . Therefore, . Moreover, since , we get that

		However, for every  we can lower bound  by  \cite{matousekVondrak}. By our hypothesis on  we have  and hence we can employ this bound on the last displayed inequality, obtaining:

		
		Since  is symmetric around its expected value , the same upper bound holds for its lower tail:

		The lemma then follows by combining the bounds on upper and lower the tails.


	\subsection{Proof of Lemma 12}
	
			
			Fix  and define the function  as the size of the maximum matching in the subgraph of  which contains edge  iff ; we remark that this subgraph does not contain the edge . It is easy to see that for every   
			
			Now let  be a maximum matching in the subgraph of  induced by  and recall that . Notice that if  then it must be the case that  belongs to . Then we can charge the difference between  and the 's to the edges in :

	which shows that  is self-bounding.




	\section{Concentration inequalities}
	
		For completeness we present two inequalities used to bound large deviations of sums of random variables \cite{BoucheronLB03}.
	
		\begin{lemma}[Hoeffding's inequality] \label{hoeffding}
		Let  be independent random variables with . Let . Then

	\end{lemma}

	\begin{lemma}[Bernstein's inequality] \label{bernIneq}
		Let  be independent random variables with equal variance . Let . Then

	\end{lemma}

		
		
		
\end{document}
