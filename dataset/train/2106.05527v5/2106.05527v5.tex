

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}       \usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{pifont}\usepackage{enumitem}
\usepackage{nicefrac}       \usepackage{color, colortbl}

\usepackage{subcaption}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{wrapfig}

\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newenvironment{roster}
{\begin{enumerate}[font=\upshape,label=(\alph*)]}
	{\end{enumerate}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\definecolor{Gray}{gray}{0.9}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\newcolumntype{f}{>{\columncolor{Gray}}l}
\makeatletter
\def\smallunderbrace#1{\mathop{\vtop{\m@th\ialign{##\crcr
   \crcr
   \noalign{\kern3\p@\nointerlineskip}\tiny\upbracefill\crcr\noalign{\kern3\p@}}}}\limits}
\makeatother
\makeatletter
\def\smalloverbrace#1{\mathop{\vtop{\m@th\ialign{##\crcr
   \crcr
   \noalign{\kern3\p@\nointerlineskip}\tiny\upbracefill\crcr\noalign{\kern3\p@}}}}\limits}
\makeatother
\allowdisplaybreaks

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Soft Truncation: A Universal Training Technique of Score-based Diffusion Model}

\begin{document}

\twocolumn[
\icmltitle{Soft Truncation: A Universal Training Technique of \\Score-based Diffusion Model for High Precision Score Estimation}







\begin{icmlauthorlist}
	\icmlauthor{Dongjun Kim}{kaist}
	\icmlauthor{Seungjae Shin}{kaist}
	\icmlauthor{Kyungwoo Song}{uos}
\icmlauthor{Wanmo Kang}{kaist}
	\icmlauthor{Il-Chul Moon}{kaist,summary_ai}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{KAIST, South Korea}
\icmlaffiliation{uos}{University of Seoul, South Korea}
\icmlaffiliation{summary_ai}{Summary.AI}

\icmlcorrespondingauthor{Dongjun Kim}{dongjoun57@kaist.ac.kr}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Recent advances in diffusion models bring state-of-the-art performance on image generation tasks. However, empirical results from previous research in diffusion models imply an inverse correlation between density estimation and sample generation performances. This paper investigates with sufficient empirical evidence that such inverse correlation happens because density estimation is significantly contributed by small diffusion time, whereas sample generation mainly depends on large diffusion time. However, training a score network well across the entire diffusion time is demanding because the loss scale is significantly imbalanced at each diffusion time. For successful training, therefore, we introduce Soft Truncation, a universally applicable training technique for diffusion models, that softens the fixed and static truncation hyperparameter into a random variable. In experiments, Soft Truncation achieves state-of-the-art performance on CIFAR-10, CelebA, CelebA-HQ , and STL-10 datasets.
\end{abstract}

\section{Introduction}

Recent advances in generative models enable the creation of highly realistic images. One direction of such modeling is \textit{likelihood-free models} \citep{karras2019style} based on minimax training. The other direction is \textit{likelihood-based models}, including VAE \cite{vahdat2020nvae}, autoregressive models \citep{parmar2018image}, and flow models \citep{grcic2021densely}. Diffusion models \citep{ho2020denoising} are one of the most successful \textit{likelihood-based models}, where the reverse diffusion models the generative process. The success of diffusion models achieves state-of-the-art performance in image generation \citep{dhariwal2021diffusion}.

Previously, a model with the emphasis on Fr\'echet Inception Distance (FID), such as DDPM \cite{ho2020denoising} and ADM \cite{dhariwal2021diffusion}, trains the score network with the variance weighting; whereas a model with the emphasis on Negative Log-Likelihood (NLL), such as ScoreFlow \cite{song2021maximum} and VDM \cite{kingma2021variational}, trains the score network with the likelihood weighting. Such models, however, have the trade-off between NLL and FID: models with the emphasis on FID perform poorly on NLL, and vice versa. Instead of widely investigating the trade-off, they limit their work by separately training the score network on FID-favorable and NLL-favorable settings. This paper introduces Soft Truncation that significantly resolves the trade-off, with the NLL-favorable setting as the default training configuration. Soft Truncation reports a comparable FID against FID-favorable diffusion models while keeping NLL at the equivalent level of NLL-favorable models.

For that, we observe that the truncation hyperparameter is a significant hyperparameter that determines the overall scale of NLL and FID. This hyperparameter, , is the smallest diffusion time to estimate the score function, and the score function beneath  is not estimated. A model with small enough  favors NLL at the sacrifice on FID, and a model with relatively large  is preferable to FID but has poor NLL. Therefore, we introduce Soft Truncation, which softens the fixed and static truncation hyperparameter () into a random variable () that randomly selects its smallest diffusion time at every optimization step. In every mini-batch update, we sample a new smallest diffusion time, , randomly, and the batch optimization endeavors to estimate the score function only on , rather than , by ignoring beneath . As  varies by mini-batch updates, the score network successfully estimates the score function on the entire range of diffusion time on , which brings an improved FID. 

There are two interesting properties of Soft Truncation. First, though Soft Truncation is nothing to do with the weighting function in its algorithmic design, surprisingly, Soft Truncation turns out to be equivalent to a diffusion model with a \textit{general weight} in the expectation sense (Eq. \eqref{eq:general_weight}). The random variable of  determines the weight function (Theorem \ref{thm:1}), and this gives a partial reason why Soft Truncation is successful in FID as much as the FID-favorable training (Table \ref{tab:ablation_weighting_function}), even though Soft Truncation only considers the truncation threshold in its implementation (Section \ref{sec:softrunc}). Second, once  is sampled in a mini-batch optimization, Soft Truncation optimizes the log-likelihood \textit{perturbed} by  (Lemma \ref{lemma:1}). Thus, Soft Truncation could be framed by Maximum Perturbed Likelihood Estimation (MPLE), a generalized concept of MLE that is specifically defined only in diffusion models (Section \ref{sec:MPLE}). 

\section{Preliminary}\label{sec:preliminary}

Throughout this paper, we focus on continuous-time diffusion models \cite{song2020score}. A continuous diffusion model slowly and systematically perturbs a data random variable, , into a noise variable, , as time flows. The diffusion mechanism is represented as a Stochastic Differential Equation (SDE), written by

where  is a standard Wiener process. The drift () and the diffusion () terms are fixed, so the data variable is diffused in a fixed manner. We denote  as the solution of the given SDE of Eq. \eqref{eq:forward_sde}, and we omit the subscript and superscript to denote , if no confusion is arised.

The theory of stochastic calculus indicates that there exists a corresponding reverse SDE given by

where the solution of this reverse SDE exactly coincides to the solution of the forward SDE of Eq. \eqref{eq:forward_sde}. Here,  is the backward time differential;  is a standard Wiener process flowing backward in time \cite{anderson1982reverse}; and  is the probability distribution of . Henceforth, we represent  as the solution of SDEs of Eqs. \eqref{eq:forward_sde} and \eqref{eq:reverse_sde}.

The diffusion model's objective is to \textit{learn} the stochastic process, , as a parametrized stochastic process, . A diffusion model builds the parametrized stochastic process as a solution of a generative SDE,

We construct the parametrized stochastic process by solving the generative SDE of Eq. \eqref{eq:generative_sde} backward in time with a starting variable of , where  is an noise distribution. Throughout the paper, we denote  as the probability distribution of .

A diffusion model learns the generative stochastic process by minimizing the score loss \cite{song2021maximum} of

where  is a weighting function that counts the contribution of each diffusion time on the loss function. This score loss is infeasible to optimize because the data score, , is intractable in general. Fortunately,  is known to be equivalent to the (continuous) denoising NCSN loss \cite{song2020score, song2019generative},

up to a constant that is irrelevant to -optimization.

Two important SDEs are known to attain analytic transition probabilities, : Variance Exploding SDE (VESDE) and Variance Preserving SDE (VPSDE) \cite{song2020score}. First, VESDE assumes  and . With such specific forms of  and , the transition probability of VESDE turns out to follow a Gaussian distribution of  with  and . Similarly, VPSDE takes  and , where ; and its transition probability falls into a Gaussian distribution of  with  and .

\begin{figure*}[t]
\centering
	\begin{subfigure}{0.32\linewidth}
	\centering
		\includegraphics[width=\linewidth]{differential_of_nelbo_cifar10_ver3.pdf}
		\subcaption{Integrand by Time}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
	\centering
		\includegraphics[width=\linewidth]{nelbo_cifar10_ver4.pdf}
		\subcaption{Variational Bound Truncated at }
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\includegraphics[width=\linewidth]{nll_nelbo_log_cifar10_ver4.pdf}
		\subcaption{Test Performance by Log-Time}
	\end{subfigure}
	\caption{The contribution of diffusion time on the variational bound experimented on CIFAR-10 with DDPM++ (VP, NLL) \cite{song2021maximum}. (a) The integrand of the variational bound is extremely imbalanced on . (b) The truncated variational bound only changes near . (c) The truncation hyperparameter () is a significant factor for performances.}
	\label{fig:nelbo}
\end{figure*}

Recently, \citet{kim2022maximum} categorize VESDE and VPSDE as a family of linear diffusions that has the SDE of

where  and  are generic -functions. Under the linear diffusions, we derive the transition probability to follow a Gaussian distribution  for certain  and  depending on  and , respectively (see Eq. \eqref{eq:appendix_transition_probability} of Appendix \ref{sec:transition_probability}). We emphasize that the suggested Soft Truncation is applicable for any SDE of Eq. \eqref{eq:forward_sde}, but we limit our focus to the family of linear SDEs of Eq. \eqref{eq:forward_linear_sde}, particularly VESDE and VPSDE among linear SDEs, to maintain the simplicity. With such a Gaussian transtion probability, the denoising NCSN loss with a linear SDE is equivalent to

if , where  is a random perturbation, and  is the neural network that predicts . This is the (continuous) DDPM loss \cite{song2020score}, and the equivalence of the two losses provides a unified view of NCSN and DDPM. Hence, NCSN and DDPM are exchangeable for each other, and we take the NCSN loss as a default form of a diffusion loss throughout the paper.

The NCSN loss training is connected to the likelihood training in \citet{song2021maximum} by

when the weighting function is the square of the diffusion term as , called the likelihood weighting. 

\section{Training and Evaluation of Diffusion Models in Practice}\label{sec:practice}

\subsection{The Need of Truncation}\label{sec:diverging}

In the family of linear SDEs, the gradient of the log transition probability satisfies , where  is given to  with . The denominator of  converges to zero as , which leads  to diverge as , as illustrated in Figure \ref{fig:nelbo}-(a), see Appendix \ref{sec:score_fail} for details. Therefore, the Monte-Carlo estimation of the NCSN loss is under high variance, which prevents stable training of the score network. In practice, therefore, previous research truncates the diffusion time range to , with a positive truncation hyperparameter, .

\subsection{Variational Bound With Positive Truncation}

For the analysis for density estimation in Section \ref{sec:universal}, this section derives the variational bound of the log-likelihood when a diffusion model has a positive truncation because Inequality \eqref{eq:variational_bound_of_song} holds only with zero truncation (). Lemma \ref{lemma:1} provides a generalization of Inequality \eqref{eq:variational_bound_of_song}, proved by applying the data processing inequality \cite{gerchinovitz2020fano} and the Girsanov theorem \cite{pavon1991free,vargas2021solving,song2021maximum}.
\begin{lemma}\label{lemma:1}
For any ,

holds, where , up to a constant, see Eq. \eqref{eq:appendix_denoising_loss}. 
\end{lemma}

Lemma \ref{lemma:1} is a generalization of Inequality \eqref{eq:variational_bound_of_song} in that Inequality \eqref{eq:perturbed_nelbo} collapses to Inequality \eqref{eq:variational_bound_of_song} under the zero truncation: . If the time range is truncated to  for , then from the variational inference, the log-likelihood becomes 

where

with  being the probability distribution of  given  and the score estimation with  at . For any , we apply Lemma \ref{lemma:1} to the right-hand-side of Inequality \eqref{eq:VI} to obtain the variational bound of the log-likelihood as 


\begin{figure}[t]
\vskip -0.2in
\centering
\includegraphics[width=\linewidth]{thumbnail_ver3.pdf}
\caption{The truncation time is key to enhance the microscopic sample quality.}
\label{fig:truncation_variance}
\end{figure}

\subsection{A Universal Phenomenon in Diffusion Training: Extremely Imbalanced Loss}\label{sec:universal}

To avoid the diverging issue introduced in Section \ref{sec:diverging}, previous works in VPSDE \cite{song2021maximum, vahdat2021score} modify the loss by truncating the integration on  with a fixed hyperparameter  so that the score network does not estimate the score function on . Analogously, previous works in VESDE \cite{song2020score, chen2021likelihood} approximate  to truncate the minimum variance of the transition probability to be . Truncating diffusion time at  in VPSDE is equivalent to truncating diffusion variance () in VESDE, so these two truncations on VE/VP SDEs have the identical effect on bounding the diffusion loss. Henceforth, this paper discusses the argument of truncating diffusion time (VPSDE) and diffusion variance (VESDE) exchangeably. 

Figure \ref{fig:nelbo} illustrates the significance of truncation in the training of diffusion models. With the truncation of strictly positive , Figure \ref{fig:nelbo}-(a) shows that the integrand of  in the Bits-Per-Dimension (BPD) scale is still extremely imbalanced. It turns out that such extreme imbalance appears to be a universal phenomenon in training a diffusion model, and this phenomenon lasts from the beginning to the end of training.

\begin{figure}[t]
	\centering
		\includegraphics[width=\linewidth]{denoising_two_ver2.pdf}
	\caption{Illustration of the generative process trained on CelebA-HQ  with NCSN++ (VE) \cite{song2020score}. The score precision on large diffusion time is key to construct the realistic overall sample quality.}
	\label{fig:large_diffusion_time}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.65\linewidth]{monte_carlo_norm_ver2.pdf}
\caption{Norm of reverse drift of generative process, trained on CIFAR-10 with DDPM++ (VP, FID) \cite{song2020score}.}
\label{fig:monte_carlo_norm}
\end{figure}

Figure \ref{fig:nelbo}-(b) with the green line presents the variational bound of the log-likelihood (right-hand-side of Inequality \eqref{eq:nelbo_st}) on the -axis, and it indicates that the variational bound is sharply decreasing near the small diffusion time. Therefore, if  is insufficiently small, the variational bound is not tight to the log-likelihood, and a diffusion model fails at MLE training. In addition, Figure \ref{fig:truncation_variance} indicates that insufficiently small  (or ) would also harm the microscopic sample quality. From these observations,  becomes a significant hyperparameter that needs to be selected carefully.

\subsection{Effect of Truncation on Model Evaluation}

Figure \ref{fig:nelbo}-(c) reports test performances on density estimation. Figure \ref{fig:nelbo}-(c) illustrates that both Negative Evidence Lower Bound (NELBO) and NLL monotonically decrease by lowering  because NELBO is largely contributed by small diffusion time at test time as well as training time. Therefore, it could be a common strategy to reduce  as much as possible to reduce test NELBO/NLL. 

\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{regenerated.pdf}
		\caption{Regenerated samples synthesized by solving the probability flow ODE on  backwards with the initial point of  for , trained on CelebA with DDPM++ (VP, FID) \cite{song2020score}.}
		\label{fig:regenerated}
		\vskip -0.1in
\end{figure}

\begin{wraptable}{r}{0.22\textwidth}
\vskip -0.13in
\centering
	\caption{Ablation on .}
	\label{tab:cifar10_ablation_ncsn}
	\tiny
	\begin{tabular}{ccc}
		\toprule
		\multirow{2}{*}{} & \multicolumn{2}{c}{CIFAR-10}\\
		& NLL () & FID-10k () \\\midrule
		 & 4.95 & 6.95 \\
		 & 3.04 & 7.04 \\
		 & 2.99 & 8.17 \\
		 & 2.97 & 8.29 \\
		\bottomrule
	\end{tabular}
\end{wraptable}
On the contrary, there is a counter effect on FID for . Table \ref{tab:cifar10_ablation_ncsn}, trained on CIFAR-10 \cite{krizhevsky2009learning} with NCSN++ \cite{song2020score}, presents that FID is worsened as we take smaller hyperparameter  for the training. It is the range of small diffusion time that significantly contributes to the variational bound in the blue line of Figure \ref{fig:nelbo}-(b), so the score network with a small truncation hyperparameter,  or , remains unoptimized on large diffusion time. In the lens of Figure \ref{fig:truncation_variance}, therefore, the inconsistent result of Table \ref{tab:cifar10_ablation_ncsn} is attributed to the inaccurate score on large diffusion time.

\begin{wraptable}{r}{0.26\textwidth}
\vskip -0.1in
\centering
	\caption{FID-10k scores.}
	\label{tab:cifar10_ablation_ncsn_truncation}
	\tiny
	\begin{tabular}{llll}
		\toprule
		 &  &  &  \\\midrule
		 & 6.84 & 8.04 & 8.29 \\
		\bottomrule
	\end{tabular}
\end{wraptable}
We design an experiment to validate the above argument in Table \ref{tab:cifar10_ablation_ncsn_truncation}. This experiment utilizes two types of score networks: 1) three alternative networks (As) with diverse  trained in Table \ref{tab:cifar10_ablation_ncsn} experiment; 2) a network (B) with  (the last row of Table \ref{tab:cifar10_ablation_ncsn}). With these score networks, we denoise the noises by either one of the first-typed As from  to a common and fixed , and we use B to further denoise from  to . This further denoising step with model B enables us to compare the score accuracy on large diffusion time for models with diverse truncation hyperparameters in a fair resolution setting. Table \ref{tab:cifar10_ablation_ncsn_truncation} presents that the model with  has the best FID, implying that the training with too small truncation would harm the sample fidelity.

\begin{figure*}[t]
\centering
	\begin{subfigure}{0.32\linewidth}
	\includegraphics[width=\linewidth]{monte_carlo_loss.pdf}
	\subcaption{Monte-Carlo Loss}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
	\includegraphics[width=\linewidth]{monte_carlo_loss_st.pdf}
	\subcaption{Soft Truncation}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
	\includegraphics[width=\linewidth]{importance_distribution.pdf}
	\subcaption{Importance Distribution}
	\end{subfigure}
	\caption{The experimental result trained on CIFAR-10 with DDPM++ (VP, NLL) \cite{song2021maximum}. (a) The Monte-Carlo loss for each diffusion time, . (b) The Monte-Carlo loss for each diffusion time on variaous truncation time. (c) The importance distribution for various truncation distributions.}
	\label{fig:monte_carlo_loss}
\end{figure*}

Specifically, Figure \ref{fig:monte_carlo_norm} shows the Euclidean norm of , where each dot represents for a Monte-Carlo sample from . Here,  is in the reverse drift term of the generative process, . Figure \ref{fig:monte_carlo_norm} illustrates that it is the large diffusion time that dominates the sampling process. Therefore, a precise score network on large diffusion time is particularly important in sample generation. 

The imprecise score mainly affects the global sample context, as the denoising on small diffusion time only crafts the image in its microscopic details, illustrated in Figures \ref{fig:large_diffusion_time} and \ref{fig:regenerated}. Figure \ref{fig:large_diffusion_time} shows how the global fidelity is damaged: a man synthesized in the second row has unrealistic curly hair on his forehead, constructed on the large diffusion time. Figure \ref{fig:regenerated} deepens the importance of learning a good score estimation on large diffusion time. It shows the regenerated samples by solving the generative process time reversely, starting from  \cite{meng2021sdedit}.


\section{Soft Truncation: A Training Technique for a Diffusion Model}

As in Section \ref{sec:practice}, the choice of  is crucial for training and evaluation, but it is computationally infeasible to search for the optimal . Therefore, we introduce a training technique that predominantly mediates the need for -search by softening the fixed truncation hyperparameter into a truncation random variable so that the truncation time varies in every optimization step. Our approach successfully trains the score network on large diffusion time without sacrificing NLL. We explain the Monte-Carlo estimation of the variational bound in Section \ref{sec:monte_carlo}, which is the common practice of previous research but explained to emphasize how simple (though effective) Soft Truncation is, and we subsequently introduce Soft Truncation in Section \ref{sec:softrunc}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{importance_sampling_ver2.pdf}
\caption{Quartile of importance weighted Monte-Carlo time of VPSDE. Red dots represent Q1/Q2/Q3/Q4 quantiles when truncated at . About  and  of Monte-Carlo time are located in  and , respectively. Green dots represent Q0-Q5 quantiles when truncated at . Importance weighted Monte-Carlo time with  is distributed much more balanced compared to the truncation at .}
\label{fig:importance_sampling}
\end{figure*}

\subsection{Monte-Carlo Estimation of Truncated Variational Bound with Importance Sampling}\label{sec:monte_carlo}

In this section, we fix a truncation hyperparameter to be . For every batch , the Monte-Carlo estimation of the variational bound in Inequality \eqref{eq:perturbed_nelbo} is , up to a constant irrelevant to , where  with  and  be the corresponding Monte-Carlo samples from  and , respectively. Note that this Monte-Carlo estimation is tractably computed from the analytic form of the transition probability as  under linear SDEs.

Previous works \cite{song2021maximum,huang2021variational} apply the importance sampling with the importance distribution of , where . It is well known \cite{goodfellow2016deep} that the Monte-Carlo variance of  is minimum if the importance distribution is  with , but sampling of Monte-Carlo diffusion time from  at every training iteration would incur  slower training speed, at least, because the importance sampling requires the score evaluation. Therefore, previous research approximates  by , and  becomes the approximate importance weight. This approximation, at the expense of bias, is cheap because the closed-form of the inverse Cumulative Distribution Function (CDF) is known. Unless we train the variance directly as in \citet{kingma2021variational}, we believe  is the maximally efficient sampler as long as the training speed matters. The importance weighted Monte-Carlo estimation becomes

where  is the Monte-Carlo sample from the importance distribution, i.e., . 

The importance sampling is advantageous in both NLL and FID \cite{song2021maximum} over the uniform sampling, as the importance sampling significantly reduces the estimation variance. Figure \ref{fig:monte_carlo_loss}-(a) illustrates the sample-by-sample loss, and the importance sampling significantly mitigates the loss scale by diffusion time compared to the scale in Figure \ref{fig:nelbo}-(a). However, the importance distribution satisfies  as  in Figure \ref{fig:monte_carlo_loss}-(c) blue line, and most of the importance weighted Monte-Carlo time is concentrated at  in Figure \ref{fig:importance_sampling}. Hence, the use of the importance sampling has a trade-off between the reduced variance (Figure \ref{fig:monte_carlo_loss}-(a)) versus the over-sampled diffusion time near  (Figure \ref{fig:importance_sampling}). Regardless of whether to use the importance sampling or not, therefore, the inaccurate score estimation on large diffusion time appears sampling-strategic-independently, and solving this pre-matured score estimation becomes a nontrivial task. 

Instead of the likelihood weighting, previous works \cite{ho2020denoising, nichol2021improved, dhariwal2021diffusion} train the denoising score loss with the variance weighting, . With this weighting, the importance distribution becomes the uniform distribution, , so it significantly alleviates the trade-off of using the likelihood weighting. However, the variance weighting favors FID at the sacrifice in NLL because the loss is no longer the variational bound of the log-likelihood. In contrast, the training with the likelihood weighting is leaning towards NLL than FID, so Soft Truncation is for the \textit{balanced} NLL and FID, using the likelihood weighting.

\subsection{Soft Truncation}\label{sec:softrunc}

Soft Truncation releases the truncation hyperparameter from a static variable to a random variable with a probability distribution of . In every mini-batch update, Soft Truncation optimizes the diffusion model with  in Eq. \eqref{eq:iw} for a sampled . In other words, for every batch , Soft Truncation optimizes the Monte-Carlo loss

with  sampled from the importance distribution of , where . 

Soft Truncation resolves the oversampling issue of diffusion time near , meaning that Monte-Carlo time is not concentrated on  anymore. Figure \ref{fig:importance_sampling} illustrates the quantiles of importance weighted Monte-Carlo time with Soft Truncation under  and . The score network is trained more equally on diffusion time when , and as a consequence, the loss imbalance issue in each training step is also alleviated as in Figure \ref{fig:monte_carlo_loss}-(b) with purple dots. This limited range of  provides a chance to learn a score network more balanced on diffusion time. As  is softened, such truncation level will vary by mini-batch updates: see the loss scales change by blue, green, red, and purple dots according to various s in Figure \ref{fig:monte_carlo_loss}-(b). Eventually, the softened  will provide a fair chance to learn the score network from small as well as large diffusion time.

\subsection{Soft Truncation Equals to A Diffusion Model With A General Weight}\label{sec:soft_truncation_general_weight}

In the original diffusion model, the loss estimation, , is just a batch-wise approximation of a population loss, . However, the target population loss of Soft Truncation, , is depending on a random variable , so the target population loss itself becomes a random variable. Therefore, we derive the \textit{expected} Soft Truncation loss to reveal the connection to the original diffusion model:

up to a constant, where , by exchanging the orders of the integrations. Therefore, we conclude that Soft Truncation reduces to a diffusion model with a general weight of , see Appendix \ref{sec:general_weight}:


\subsection{Soft Truncation is Maximum Perturbed Likelihood Estimation}\label{sec:MPLE}

As explained in Section \ref{sec:soft_truncation_general_weight}, Soft Truncation is a diffusion model with a general weight, in the expected sense. Reversely, this section analyzes a diffusion model with a general weight in view of Soft Truncation. Suppose we have a general weight . Theorem \ref{thm:1} implies that this general weighted diffusion loss, , is the variational bound of the perturbed KL divergence expected by . Theorem \ref{thm:1} collapses to Lemma \ref{lemma:1} if  for any \footnote{If , the probability satisfies , which is a probability distribution of one mass at .}. See Appendix \ref{sec:proof} for the detailed statement and proof.
\begin{theorem}\label{thm:1}
Suppose  is a nondecreasing and nonnegative absolutely continuous function on  and zero on . For the probability defined by

where ; up to a constant, the variational bound of the general weighted diffusion loss becomes

\end{theorem}
The meaning of Soft Truncation becomes clearer in view of Theorem \ref{thm:1}. Instead of training the general weighted diffusion loss, , we optimize the \textit{truncated} variational bound, . This truncated loss upper bounds the \textit{perturbed} KL divergence,  by Lemma \ref{lemma:1}, and Figure \ref{fig:nelbo}-(c) indicates that the Inequality \eqref{eq:perturbed_nelbo} is nearly tight. Therefore, Soft Truncation could be interpreted as the Maximum Perturbed Likelihood Estimation (MPLE), where the perturbation level is a random variable. Soft Truncation is not MLE training because the Inequality \ref{eq:nelbo_st} is not tight as demonstrated in Figure \ref{fig:nelbo}-(b) unless  is sufficiently small. 

Old wisdom is to minimize the loss variance if available for stable training. However, some optimization methods in the deep learning era (e.g., stochastic gradient descent) deliberately add noises to a loss function that eventually helps escape from a local optimum. Soft Truncation is categorized in such optimization methods that \textit{inflate} the loss variance by intentionally imposing auxiliary randomness on loss estimation. This randomness is represented by the outmost expectation of , which controls the diffusion time range batch-wisely. Additionally, the loss with a sampled  is the proxy of the perturbed KL divergence by , so the auxiliary randomness on loss estimation is theoretically tamed, meaning that it is not a random perturbation.

\subsection{Choice of Truncation Probability Distribution}

We parametrize the probability distribution of  by 

where  with sufficiently small enough truncation hyperparameter. Note that it is still beneficial to remain  strictly positive because a batch update with  would drift the score network away from the optimal point. Figure \ref{fig:monte_carlo_loss}-(c) illustrates the importance distribution of  for varying . From the definition of Eq. \eqref{eq:prior_example},  as , and this limiting delta distribution corresponds to the original diffusion model with the likelihood weighting. Figure \ref{fig:monte_carlo_loss}-(c) shows that the importance distribution of  with finite  interpolates the likelihood weighting and the variance weighting. 

With the current simple form, we experimentally find that the sweet spot is  in VPSDE and  in VESDE with the emphasis on the sample quality. For VPSDE, the importance distribution in Figure \ref{fig:monte_carlo_loss}-(c) is nearly equal to that of the variance weighting if , so Soft Truncation with  improves the sample fidelity, while maintaining low NLL. On the other hand, if  is too small, no  will be sampled near , so it hurts both sample generation and density estimation. We leave further study on searching for the optimal distribution of  as future work. 

\begin{table}[t]
\begin{minipage}[c]{0.5\textwidth}
\centering
	\begin{subfigure}{0.7\linewidth}
		\includegraphics[width=\linewidth]{FID_by_iteration.pdf}
	\end{subfigure}
	\vskip -0.1in
	\captionof{figure}{Soft Truncation improves FID on CelebA trained with UNCSN++ (RVE).}
	\label{fig:st_training}
	\end{minipage}

\begin{minipage}[c]{0.5\textwidth}
\centering
\vskip 0.1in
	\caption{Ablation study of Soft Truncation for various weightings on CIFAR-10 and ImageNet32 with DDPM++ (VP).}
	\label{tab:ablation_weighting_function}
	\vskip -0.05in
	\tiny
	\begin{tabular}{llcccc}
		\toprule
		& \multirow{2}{*}{Loss} & \multirow{2}{*}{\shortstack{Soft\\Truncation}} & \multirow{2}{*}{NLL} & \multirow{2}{*}{NELBO} & FID \\
		&&&&& ODE \\\midrule
		\multirow{4}{*}[-2pt]{CIFAR-10} &  & \xmark & 3.03 & 3.13 & 6.70 \\
		&  & \xmark & 3.21 & 3.34 & 3.90 \\
		&  & \xmark & 3.06 & 3.18 & 6.11 \\
		&  & \cmark & \textbf{3.01} & \textbf{3.08} & 3.96 \\
		&  & \cmark & 3.03 & 3.13 & \textbf{3.45} \\\midrule
		\multirow{4}{*}[-2pt]{ImageNet32} &  & \xmark & 3.92 & 3.94 & 12.68 \\
		&  & \xmark & 3.95 & 4.00 & 9.22 \\
		&  & \xmark & 3.93 & 3.97 & 11.89 \\
		&  & \cmark & \textbf{3.90} & \textbf{3.91} & \textbf{8.42} \\
		\bottomrule
	\end{tabular}
\end{minipage}

\begin{minipage}[c]{0.5\textwidth}
\vskip 0.1in
\centering
	\caption{Ablation study of Soft Truncation for various model architectures and diffusion SDEs on CelebA.}
	\label{tab:ablation_architecture_sde}
	\vskip -0.05in
	\tiny
	\begin{tabular}{lllcccc}
		\toprule
		\multirow{2}{*}{SDE} & \multirow{2}{*}{Model} & \multirow{2}{*}{Loss} & \multirow{2}{*}{NLL} & \multirow{2}{*}{NELBO} & \multicolumn{2}{c}{FID} \\
		&&&&& PC & ODE \\\midrule
		\multirow{2}{*}{VE} & \multirow{2}{*}{NCSN++} &  & 3.41 & 3.42 & 3.95 & -\\
		& &  & 3.44 & 3.44 & 2.68 & -\\\midrule
		\multirow{2}{*}{RVE} & \multirow{2}{*}{UNCSN++} &  & 2.01 & \textbf{2.01} & 3.36 & -\\
		& &  & \textbf{1.97} & 2.02 & \textbf{1.92} & -\\\midrule
		\multirow{8}{*}[-9pt]{VP} & \multirow{2}{*}{DDPM++} &   & 2.14 & 2.21 & 3.03 & 2.32 \\
		& &  & 2.17 & 2.29 & 2.88 & \textbf{1.90}\\\cmidrule(lr){2-3}
		& \multirow{2}{*}{UDDPM++} &  & 2.11 & 2.20 & 3.23 & 4.72\\
		& &  & 2.16 & 2.28 & 2.22 & 1.94\\\cmidrule(lr){2-3}
		& \multirow{2}{*}{DDPM++} &  & 2.00 & 2.09 & 5.31 & 3.95\\
		& &  & 2.00 & 2.11 & 4.50 & 2.90\\\cmidrule(lr){2-3}
		& \multirow{2}{*}{UDDPM++} &  & 1.98 & 2.12 & 4.65 & 3.98\\
		& &  & 2.00 & 2.10 & 4.45 & 2.97\\
		\bottomrule
	\end{tabular}
	\end{minipage}
		\vskip -0.1in
\end{table}

\begin{table}[t]
\begin{minipage}[c]{0.5\textwidth}
\centering
	\caption{Ablation study of Soft Truncation for various  on CIFAR-10 with DDPM++ (VP).}
	\label{tab:ablation_epsilon}
	\vskip -0.05in
	\tiny
	\begin{tabular}{lcccc}
		\toprule
		Loss &  & NLL & NELBO & FID (ODE) \\\midrule
		\multirow{4}{*}{} &  & 4.64 & 4.69 & 38.82 \\
		&  & 3.51 & 3.52 & 6.21 \\
		&  & 3.05 & 3.08 & 6.33 \\
		&  & 3.03 & 3.13 & 6.70 \\\midrule
		\multirow{4}{*}{} &  & 4.65 & 4.69 & 39.83 \\
		&  & 3.51 & 3.52 & 5.14 \\
		&  & 3.05 & 3.08 & 4.16 \\
		&  & \textbf{3.01} & \textbf{3.08} & \textbf{3.96} \\
		\bottomrule
	\end{tabular}
	\end{minipage}

\begin{minipage}[c]{0.5\textwidth}
\vskip 0.1in
\centering
\caption{Ablation study of Soft Truncation for various  on CIFAR-10 trained with DDPM++ (VP).}
\label{tab:ablation_prior}
	\vskip -0.05in
\tiny
\begin{tabular}{lccc}
	\toprule
	Loss & NLL & NELBO & FID (ODE) \\\midrule
	 & 3.24 & 3.39 & 6.27 \\
	 & 3.03 & \textbf{3.05} & 3.61 \\
	 & 3.03 & 3.13 & \textbf{3.45} \\
	 & \textbf{3.01} & 3.08 & 3.96 \\
	 & 3.02 & 3.09 & 3.98 \\
	 & 3.03 & 3.09 & 3.98 \\
	 & \textbf{3.01} & 3.10 & 6.31 \\
	 & 3.02 & 3.09 & 6.54 \\\midrule
	 & \multirow{2}{*}{\textbf{3.01}} & \multirow{2}{*}{3.09} & \multirow{2}{*}{6.70} \\
	 &&&\\
	\bottomrule
\end{tabular}
\end{minipage}

\begin{minipage}[c]{0.5\textwidth}
\vskip 0.1in
\centering
\caption{Ablation study of Soft Truncation for CIFAR-10 trained with DDPM++ when a diffusion is combined with a normalizing flow in INDM \cite{kim2022maximum}.}
\label{tab:ablation_indm}
	\vskip -0.05in
\tiny
\begin{tabular}{lccc}
	\toprule
	Loss & NLL & NELBO & FID (ODE) \\\midrule
	INDM (VP, NLL) & \textbf{2.98} & \textbf{2.98} & 6.01 \\
	INDM (VP, FID) & 3.17 & 3.23 & \textbf{3.61} \\
	INDM (VP, NLL) + ST & 3.01 & 3.02 & 3.88 \\
	\bottomrule
\end{tabular}
\end{minipage}
\end{table}

\begin{table*}
\centering
	\caption{Performance comparisons on benchmark datasets. The boldfaced numbers present the best performance, and the underlined numbers present the second-best performance. We report NLL of DDPM++ on CIFAR-10, ImageNet32, and CelebA with the variational dequantization \cite{song2021maximum} to compare with the baselines in a fair setting.}
	\label{tab:performances}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{lccccccccccc}
			\toprule
			\multirow{3}{*}{Model} & \multicolumn{3}{c}{CIFAR10} & \multicolumn{3}{c}{ImageNet32} & \multicolumn{2}{c}{CelebA} & CelebA-HQ & \multicolumn{2}{c}{STL-10} \\
			& \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{2}{c}{} &  & \multicolumn{2}{c}{} \\
			& NLL () & FID () & IS () & NLL & FID & IS & NLL & FID & FID & FID & IS \\\midrule
			\multicolumn{12}{l}{\textbf{Likelihood-free Models}}\\
			StyleGAN2-ADA+Tuning \citep{karras2020training} & - & 2.92 & \underline{10.02} & - & - & - & - & - & - & - & - \\
			Styleformer \citep{park2021styleformer} & - & 2.82 & 9.94 & - & - & - & - & 3.66 & - & \underline{15.17} & \underline{11.01} \\
			\multicolumn{12}{l}{\textbf{Likelihood-based Models}}\\
			ARDM-Upscale 4 \citep{hoogeboom2021autoregressive} & \textbf{2.64} & - & - & - & - & - & - & - & - & - & - \\
			VDM \citep{kingma2021variational} & \underline{2.65} & 7.41 & - & \underline{3.72} & - & - & - & - & - & - & - \\
			LSGM (FID) \citep{vahdat2021score} & 3.43 & \textbf{2.10} & - & - & - & - & - & - & - & - & - \\
			NCSN++ cont. (deep, VE) \citep{song2020score} & 3.45 & \underline{2.20} & 9.89 & - & - & - & 2.39 & 3.95 & \underline{7.23} & - & - \\
			DDPM++ cont. (deep, sub-VP) \citep{song2020score} & 2.99 & 2.41 & 9.57 & - & - & - & - & - & - & - & - \\
			DenseFlow-74-10 \citep{grcic2021densely} & 2.98 & 34.90 & - & \textbf{3.63} & - & - & 1.99 & - & - & - & - \\
			ScoreFlow (VP, FID) \citep{song2021maximum} & 3.04 & 3.98 & - & 3.84 & \textbf{8.34} & - & - & - & - & - & - \\
			Efficient-VDVAE \citep{hazami2022efficient} & 2.87 & - & - & - & - & - & \textbf{1.83} & - & - & - & - \\
			PNDM \citep{liu2022pseudo} & - & 3.26 & - & - & - & - & - & 2.71 & - & - & - \\
			ScoreFlow (deep, sub-VP, NLL) \citep{song2021maximum} & 2.81 & 5.40 & - & 3.76 & 10.18 & - & - & - & - & - & - \\
			Improved DDPM () \citep{nichol2021improved} & 3.37 & 2.90 & - & - & - & - & - & - & - & - & - \\\midrule
			UNCSN++ (RVE) + ST & 3.04 & 2.33 & \textbf{10.11} & - & - & - & 1.97 & \underline{1.92} & \textbf{7.16} & \textbf{7.71} & \textbf{13.43} \\
			DDPM++ (VP, FID) + ST & 2.91 & 2.47 & 9.78 & - & - & - & 2.10 & \textbf{1.90} & - & - & - \\
			DDPM++ (VP, NLL) + ST & 2.88 & 3.45 & 9.19 & 3.85 & \underline{8.42} & \textbf{11.82} & \underline{1.96} & 2.90 & - & - & - \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
\end{table*}

\section{Experiments}

This section empirically studies our suggestions on benchmark datasets, including CIFAR-10 \citep{krizhevsky2009learning}, ImageNet  \cite{van2016pixel}, STL-10 \citep{coates2011analysis}\footnote{We downsize the dataset from  to  following \citet{jiang2021transgan, park2021styleformer}.} CelebA \citep{liu2015deep}  and CelebA-HQ \citep{karras2018progressive} .

Soft Truncation is a universal training technique indepedent to model architectures and diffusion strategies. In the experiments, we test Soft Truncation on various architectures, including vanilla NCSN++, DDPM++, Unbounded NCSN++ (UNCSN++), and Unbounded DDPM++ (UDDPM++). Also, Soft Truncation is applied to various diffusion SDEs, such as VESDE, VPSDE, and Reverse VESDE (RVESDE). Although we use continuous SDEs for the diffusion strategies, Soft Truncation with the discrete model, such as DDPM \cite{ho2020denoising}, is a straightforward application of continuous models. Appendix \ref{sec:implementation_details} enumerates the specifications of score architectures and SDEs.

From Figure \ref{fig:nelbo}-(c), a sweet spot of the hard threshold is , in which NLL/NELBO are no longer improved under this threshold. As the diffusion model has no information on , we comply \citet{kim2022maximum} to use Inequality \eqref{eq:VI} for NLL computation and Inequality \eqref{eq:nelbo_st} for NELBO computation. Following \citet{kim2022maximum}, we compute , rather than . It is the common practice of continuous diffusion models \cite{song2020score, song2021maximum, dockhorn2021score} to report their performances with , but \citet{kim2022maximum} show that  differs to  by 0.05 in BPD scale when , which is quite significant. We use the uniform dequantization \cite{theis2016note} as default, otherwise noted. For sample generation, we use either of Predictor-Corrector (PC) sampler or Ordinary Differential Equation (ODE) sampler \cite{song2020score}. We denote  as the vanilla training with -weighting, and  as the training by Soft Truncation with the truncation probability of . We additionally denote  for updating the network by the variance weighted loss per batch-wise update. We release our code at \url{https://github.com/Kim-Dongjun/Soft-Truncation}.

\textbf{FID by Iteration} Figure \ref{fig:st_training} illustrates the FID score \cite{heusel2017gans} in -axis by training steps in -axis. Figure \ref{fig:st_training} shows that Soft Truncation beats the vanilla training after 150k of training iterations.

\textbf{Ablation Studies} Tables \ref{tab:ablation_weighting_function}, \ref{tab:ablation_architecture_sde}, \ref{tab:ablation_epsilon}, and \ref{tab:ablation_prior} show ablation studies on various weighting functions, model architectures, SDEs, s, and probability distributions of , respectively. See Appendix \ref{sec:full_tables}. Table \ref{tab:ablation_weighting_function} shows that Soft Truncation beats or equals to the vanilla training in all performances. We highlight that Soft Truncation with  outperforms the FID-favorable model with the variance weighting with respect to FID on both CIFAR-10 and ImageNet32.

Not only comparing with the pre-existing weighting functions, such as  or , Table \ref{tab:ablation_weighting_function} additionally reports the experimental result of a general weighting function of . From Eq. \eqref{eq:general_weight}, Soft Truncation with  and the vanilla training with  coincide in their loss functions in average, i.e., . Thus, when comparing the paired experiments, Soft Truncation could be considered as an alternative way of estimating the same loss, and Table \ref{tab:ablation_weighting_function} implies that Soft Truncation gives better optimization than the vanilla method. This strongly implies that Soft Truncation could be a default training method for a general weighted denoising diffusion loss.

Table \ref{tab:ablation_architecture_sde} provides two implications. First, Soft Truncation particularly boosts FID while maintaining density estimation performances under the variation of score networks and diffusion strategies. Second, Table \ref{tab:ablation_architecture_sde} shows that Soft Truncation is effective on CelebA even when we apply Soft Truncation on the variance weighting, i.e., , but we find that this does not hold on CIFAR-10 and ImageNet32. We leave it as a future work on this extent.

Table \ref{tab:ablation_epsilon} shows a contrastive trend of the vanilla training and Soft Truncation. The inverse correlation appears between NLL and FID in the vanilla training, but Soft Truncation monotonically reduces both NLL and FID by . This implies that Soft Truncation significantly reduces the effort of the  search. Table \ref{tab:ablation_prior} studies the effect of the probability distribution of  in VPSDE. It shows that Soft Truncation significantly improves FID upon the experiment of  on the range of . Finally, Table \ref{tab:ablation_indm} shows that Soft Truncation also works with a nonlinear forward SDE \cite{kim2022maximum}, so the scope of Soft Truncation is not limited to a family of linear SDEs.

\textbf{Quantitative Comparison to SOTA} Table \ref{tab:performances} compares Soft Truncation (ST) against the current best generative models. It shows that Soft Truncation achieves the state-of-the-art sample generation performances on CIFAR-10, CelebA, CelebA-HQ, and STL-10, while keeping NLL intact. In particular, we have experimented thoroughly on the CelebA dataset, and we find that Soft Truncation largely exceeds the previous best FID scores by far. In FID, Soft Truncation with DDPM++ performs 1.90, which exceeds the previous best FID of 2.92 by DDGM. Also, Soft Truncation significantly improves FID on STL-10.

\section{Conclusion}

This paper proposes a generally applicable training method for diffusion models. The suggested training method, Soft Truncation, is motivated from the observation that the density estimation is mostly counted on small diffusion time, while the sample generation is mostly constructed on large diffusion time. However, small diffusion time dominates the Monte-Carlo estimation of the loss function, so this imbalance contribution prevents accurate score learning on large diffusion time. Soft Truncation softens the truncation level at each mini-batch update, and this simple modification is connected to the general weighted diffusion loss and the concept of Maximum Perturbed Likelihood Estimation.

\section*{Acknowledgements}

This research was supported by AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and ICT(2022-0-00077). We thank Jaeyoung Byeon and Daehan Park for their fruitful mathematical advice, and Byeonghu Na for his support of the experiments.

\nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2022}


\newpage
\appendix
\onecolumn

\section{Derivation}

\subsection{Transition Probability for Linear SDEs}\label{sec:transition_probability}

\citet{kim2022maximum} has classified linear SDEs as

where  and  are real-valued functions. VESDE has  and , where  and  are the minimum/maximum perturbation variances, respectively. It has the transition probability of

where  and . VPSDE has  and  with the transition probability of

where  and .

Analogous to VE/VP SDEs, the transition probability of the generic linear SDE of Eq. \eqref{eq:appendix_linear_SDE} is a Gaussian distribution of , where its mean and covariance functions are characterized as a system of ODEs of

with initial conditions to be  and .

Eq. \eqref{eq:mean} has its solution by

If we multiply  to Eq. \eqref{eq:covariance}, then Eq. \eqref{eq:covariance} equals to

If we impose  to Eq. \eqref{eq:appendix_variance}, then the constant  satisfies , and the variance formula becomes

To sum up, the family of linear SDEs of  gets the transition probability to be


\subsection{Diverging Denoising Loss}\label{sec:score_fail}

The gradient of the log transition probability, , is diverging at , where . Below Lemma \ref{lemma:2} indicates that  for any continuous score function, . This leads that the denoising score loss diverges as  as illustrated in Figure \ref{fig:nelbo}-(a).

\begin{lemma}\label{lemma:2}
		Let \mathbf{s}. Suppose a continuous vector field  defined on a subset  of a compact manifold  (i.e., ) is unbounded, then there exists no  such that  a.e. on .
	\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma:2}]
		Since  is an open subset of a compact manifold ,  for all . Also, if ,  is bounded. Hence, the local Lipschitzness of  implies that there exists a positive  such that  for any  and . Therefore, for any , there exists  such that  for all  and , which leads no  that satisfies  a.e. on  as .
	\end{proof}

\subsection{General Weighted Diffusion Loss}\label{sec:general_weight}
The denoising score loss is

for any . For an appropriate class of function ,

holds by changing the order of integration. Therefore, we get

where

If , then we have



\section{Theorems and Proofs}\label{sec:proof}

\begingroup
\renewcommand\thelemma{1}
\begin{lemma}\label{lemma:1}
For any ,

\end{lemma}
\endgroup

\begin{proof}
Suppose  is the path measure of the forward SDE, and  is the path measure of the generative SDE. The restricted measure is defined by , where  if  and  is a measurable set in  otherwise. The restricted measure of  is defined analogously. Then, by the data processing inequality, we get

Now, from the chain rule of KL divergences, we have

From the Girsanov theorem and the Martingale property, we get

and combining Eq. \eqref{eq:appendix_data}, \eqref{eq:appendix_chain} and \eqref{eq:appendix_girsanov}, we have

Now, from

we can transform  into , Eq. \eqref{eq:appendix_truncated_song} is equivalent to

Now, directly applying Theorem 4 of \citet{song2021maximum}, the entropy of  becomes

Therefore, from Eq. \eqref{eq:appendix_truncated} and \eqref{eq:appendix_theorem4_song}, we get

\end{proof}

\begingroup
	\renewcommand\thetheorem{1}
\begin{theorem}Suppose  is a weighting function of the NCSN loss. If  is a nondecreasing and nonnegative absolutely continuous function on  and zero on , then

\end{theorem}
\endgroup

\begin{proof}
We prove the theorm by using

By plugging  in Eq. \eqref{eq:exchange}, we have


Also, plugging  into Eq. \eqref{eq:exchange}, we have


Using Eq. \eqref{eq:appendix_derivation} and \eqref{eq:appendix_plugging}, we get

Then, applying Lemma \ref{lemma:1} to Eq. \eqref{eq:appendix_final_} yields the desired result.
\end{proof}

\begin{corollary}Suppose  is a weighting function of the NCSN loss. If  is a nondecreasing and nonnegative continuous function on  and zero on , then

\end{corollary}

\begin{remark}
A direct extension of the proof indicates that Theorem \ref{thm:1} still holds when  has finite jump on .
\end{remark}
\begin{remark}
The weight of  is the normalizing constant of the unnormalized truncation probability, .
\end{remark}

\begin{proof}
By plugging  in Eq. \eqref{eq:exchange} and using Lemma \ref{lemma:1}, we have

\end{proof}

\section{Additional Score Architectures and SDEs}

\subsection{Additional Score Architectures: Unbounded Parametrization}

From the released code of \citet{song2020score}, the NCSN++ network is modeled by , where the second argument is  instead of . Experiments with  or  were not as good as the parametrization of , and we analyze this experimental results from Lemma \ref{lemma:2} and Proposition \ref{prop:1}.

\begin{proposition}\label{prop:1}
		Let \mathbf{s}. Suppose a continuous vector field  defined on a -dimensional open subset  of a compact manifold  is unbounded, and the projection of  on each axis is locally integrable. Then, there exists  such that  a.e. on .
	\end{proposition}

The gradient of the log transition probability diverges at  theoretically (Section \ref{sec:score_fail}) and empirically (Figure \ref{fig:uddpm}-(a)). Here, in high-dimensional space,  with  is either zero or infinity. Thus, the data score is nearly identical to the gradient of the log transition probability, , and the observation of Figure \ref{fig:uddpm}-(a) is valid for the exact data score, as well.

Although Lemma \ref{lemma:2} is based on , the identical result also holds for the parametrization of , so it indicates that both  and  cannot estimate the data score as . On the other hand, Proposition \ref{prop:1} implies that there exists a score function that estimates the unbounded data score asymptotically, and Proposition \ref{prop:1} explains the reason why the parametrization of \citet{song2020score}, i.e., , is successful on score estimation.

On top of that, we introduce another parametrization that particularly focuses on the score estimation near . We name Unbounded NCSN++ (UNCSN++) as the network of  with  and Unbounded DDPM++ (UDDPM++) as the network of  with .

	In UNCSN++,  and  are the hyperparameters. By acknowledging the parametrization of , we choose  as . Also, to satisfy the continuously differentiability of , two hyperparameters  and  satisfy a system of equations with degree 2, so  and  are fully determined with this system of equations.
	
\begin{figure*}[t]
\centering
	\begin{subfigure}{0.32\linewidth}
	\includegraphics[width=\linewidth]{unbounded_data_score.pdf}
	\subcaption{Approximate data score diverges.}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.32\linewidth}
	\includegraphics[width=\linewidth]{uddpm_time_cdf_ver2.pdf}
	\subcaption{Cumulative density function of  and .}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.32\linewidth}
	\includegraphics[width=\linewidth]{vesde_geometric_ver2.pdf}
	\subcaption{VESDE violates geometric progression.}
	\end{subfigure}
	\caption{(a) The approximate data score, , diverges as . (b) Comparison of DDPM++ and UDDPM++ in terms of the cumulative density function of the second input. (c) Comparison of VESDE and RVESDE in terms of .}
	\label{fig:uddpm}
\end{figure*}
The choice of such  for UDDPM++ is expected to enhance the score estimation near  because the input of  is distributed uniformly when we draw samples from the importance weight. Concretely, when the sampling distribution on the diffusion time is given by , the -distribution from the importance sampling becomes , which is depicted in Figure \ref{fig:uddpm}-(b).

	
	
	
	
	\begin{proof}[Proof of Proposition \ref{prop:1}]
		Let  be a standard mollifier function. If , then  converges to  a.e. on  as  (Theorem 7-(ii) of Appendix C in \cite{evans1998partial}). Therefore, if we define  on the domain of  and  elsewhere, then  a.e. on  as .
		
		Now, to show that  is locally Lipschitz, let  be a compact subset of . From , if there exists  such that  and  for all  and , then  is Lipschitz on . 
		
		First, since  is infinitely differentiable on its domain (Theorem 7-(i) of Appendix C in \cite{evans1998partial}) and , there exists  such that . Second, the mollifier satisfies the uniform convergence on any compact subset of  (Theorem 7-(iii) of Appendix C in \cite{evans1998partial}), which leads that  for some . Therefore,  becomes an element of .
	\end{proof}	

\subsection{Additional SDE: Reciprocal VESDE}

VESDE assumes . Then, the variance of the transition probability  becomes  if the diffusion starts from  with the initial condition of . VESDE was originally introduced in \citet{song2020improved} in order to satisfy the geometric property for its smooth transition of the distributional shift. Mathematically, the variance is geometric if  is a constant, but VESDE losses the geometric property as illustrated in Figure \ref{fig:uddpm}-(c).

To attain the geometric property in VESDE, VESDE approximates the variance to be  by omitting 1 from . However, this approximation leads that  is not converging to  in distribution because  as . Indeed, a bit stronger claim is possible:
\begin{proposition}\label{prop:2}
		There is no SDE that has the stochastic process , defined by a transition probability , as the solution.
	\end{proposition}
	Proposition \ref{prop:2} indicates that if we approximate the variance by , then the reverse diffusion process cannot be modeled by a generative process. 

Rigorously, however, if the diffusion process starts from , rather than , then the variance of the transition probability becomes , which is exactly the variance . Therefore, VESDE can be considered as a diffusion process starting from .

From this point of view, we introduce a SDE that satisfies the geometric progression property starting from . We name a new SDE as the Reciprocal VE SDE (RVESDE). RVESDE has the identical form of SDE, , with

Then, the transition probability of RVESDE becomes

As illustrated in Figure \ref{fig:uddpm}-(c), RVESDE attains the geometric property at the expense of having reciprocated time, . Also, RVESDE satisfies  and . The existence and uniqueness of solution for RVESDE is guaranteed by Theorem 5.2.1 in \cite{oksendal2013stochastic}. 

\section{Implementation Details}\label{sec:implementation_details}

\subsection{Experimental Details}

	\textbf{Training} Throughout the experiments, we train our model with a learning rate of 0.0002, warmup of 5000 iterations, and gradient clipping by 1. For UNCSN++, we take , and for NCSN++, we take . On ImageNet32 training of the likelihood weighting and the variance weighting without Soft Truncation, we take , following the setting of \citet{song2021maximum}. Otherwise, we take . For other hyperparameters, we run our experiments according to \citet{song2020score,song2021maximum}.
	
	On datasets of resolution , we use the batch size of 128, which consumes about 48Gb GPU memory. On STL-10 with resolution , we use the batch size of 192, and on datasets of resolution , we experiment with 128 batch size. The batch size for the datasets of resolution  is 40, which takes nearly 120Gb of GPU memory. On the dataset of  resolution, we use the batch size of 16, which takes around 120Gb of GPU memory. We use five NVIDIA RTX-3090 GPU machines to train the model exceeding 48Gb, and we use a pair of NVIDIA RTX-3090 GPU machines to train the model that consumes less than 48Gb. 
	
	\textbf{Evaluation} We apply the EMA with rate of 0.999 on NCSN++/UNCSN++ and 0.9999 on DDPM++/UDDPM++. For the density estimation, we obtain the NLL performance by the Instantaneous Change of Variable \citep{song2020score, chen2018neural}. We choose  to integrate the instantaneous change-of-variable of the probability flow as default, even for the ImageNet32 dataset. In spite that \citet{song2020score, song2021maximum} integrates the change-of-variable formula with the starting variable to be , Table 5 of \citet{kim2022maximum} analyzes that there are significant difference between starting from  and , if  is not small enough. Therefore, we follow \citet{kim2022maximum} to compute . However, to compare with the baseline models, we also evaluate the way \citet{song2020score, song2021maximum} and \citet{vahdat2021score} compute NLL. We denote the way of \citet{kim2022maximum} as \textit{after correction} and \citet{song2021maximum} as \textit{before correction}, throughout the appendix. We dequantize the data variable by the uniform dequantization \cite{ho2019flow++} for both \text{after}-and-\text{before corrections}. In the main paper, we only report the \textit{after correction} performances.
		
	For the sampling, we apply the Predictor-Corrector (PC) algorithm introduced in \citet{song2020score}. We set the signal-to-noise ratio as 0.16 on  datasets, 0.17 on  and  datasets, 0.075 on 256256 sized datasets, and 0.15 on . On datasets less than  resolution, we iterate 1,000 steps for the PC sampler, while we apply 2,000 steps on the other high-dimensional datasets. Throughout the experiments for VESDE, we use the reverse diffusion \citep{song2020score} for the predictor algorithm and the annealed Langevin dynamics \citep{welling2011bayesian} for the corrector algorithm. For VPSDE, we use the Euler-Maruyama for the predictor algorithm, and we do not use any corrector algorithm.
	
	We compute the FID score \citep{song2020score} based on the modified Inception V1 network\footnote{\url{https://tfhub.dev/tensorflow/tfgan/eval/inception/1}} using the tensorflow-gan package for CIFAR-10 dataset, and we use the clean-FID \citep{parmar2021buggy} based on the Inception V3 network \citep{szegedy2016rethinking} for the remaining datasets. We note that FID computed by \cite{parmar2021buggy} reports a higher FID score compared to the original FID calculation\footnote{See \url{https://github.com/GaParmar/clean-fid} for the detailed experimental results.}.
	
\section{Additional Experimental Results}\label{sec:additional}
		
		\subsection{Ablation Study on Reconstruction Term}\label{sec:reconstruction_training}
	
	\begin{table}[t]
\centering
\caption{Ablation study of Soft Truncation with/without the reconstruction term when training on CIFAR-10 trained with DDPM++ (VP).}
\label{tab:ablation_reconstruction_appendix}
	\vskip -0.05in
\tiny
\begin{tabular}{lcc|cc|cc|c}
	\toprule
	\multirow{6}{*}[-2pt]{Loss} & \multirow{6}{*}[-2pt]{\shortstack{Soft\\Truncation}} & \multirow{6}{*}[-2pt]{\shortstack{Reconstruction\\Term for\\Training}} & \multicolumn{2}{c|}{NLL} & \multicolumn{2}{c|}{NELBO} & FID \\\cmidrule(lr){4-8}
	&&& \multirow{5}{*}{\shortstack{\after correction)}} & \multirow{5}{*}{\shortstack{\with\\residual)}} & \multicolumn{1}{c}{\multirow{5}{*}{ODE}} \\
	&&&&&&&\\
	&&&&&&&\\
	&&&&&&&\\
	&&&&&&&\\\midrule
	 & \xmark & \multicolumn{1}{c}{\xmark} & \multicolumn{1}{c}{2.97} & \multicolumn{1}{c}{3.03} & \multicolumn{1}{c}{3.11} & \multicolumn{1}{c}{3.13} & 6.70\\\cmidrule(lr){1-3}
	 & \xmark & \multicolumn{1}{c}{\cmark} & \multicolumn{1}{c}{3.01} & \multicolumn{1}{c}{2.99} & \multicolumn{1}{c}{3.07} & \multicolumn{1}{c}{3.09} & 6.93 \\\cmidrule(lr){1-3}
	 & \multirow{2}{*}{\cmark} & \multicolumn{1}{c}{\multirow{2}{*}{\xmark}} & \multicolumn{1}{c}{\multirow{2}{*}{2.98}} & \multicolumn{1}{c}{\multirow{2}{*}{3.01}} & \multicolumn{1}{c}{\multirow{2}{*}{3.08}} & \multicolumn{1}{c}{\multirow{2}{*}{3.08}} & \multirow{2}{*}{\textbf{3.96}} \\
	 &&\multicolumn{1}{c}{} &&\multicolumn{1}{c}{}&&\multicolumn{1}{c}{}&\\\cmidrule(lr){1-3}
	 & \cmark & \multicolumn{1}{c}{\cmark} & \multicolumn{1}{c}{\textbf{2.95}} & \multicolumn{1}{c}{\textbf{2.98}} & \multicolumn{1}{c}{\textbf{3.04}} & \multicolumn{1}{c}{\textbf{3.04}} & 4.23 \\
	\bottomrule
\end{tabular}
\end{table}
	
		Table \ref{tab:ablation_reconstruction_appendix} presents that the training with the reconstruction term outperforms the training without the reconstruction term on NLL/NELBO with the sacrifice on sample generation. If  is fixed as , then the bound
		
		is tight enough to estimate the negative log-likelihood. However, if  is a subject of random variable, then the bound is not tight to the negative log-likelihood, as evidenced in Figure \ref{fig:nelbo}-(b). On the other hand, if we do not count the reconstruction, then the bound becomes
		
		up to a constant, and this bound becomes tight regardless of , which is evidenced in Figure \ref{fig:nelbo}-(c). This is why we call Soft Truncation as Maximum Perturbed Likelihood Estimation (MPLE).
			
		
		
\begin{table}[t]
\vskip 0.1in
\centering
	\caption{Ablation study of Soft Truncation for various weightings on CIFAR-10 and ImageNet32 with DDPM++ (VP).}
	\label{tab:ablation_weighting_function_appendix}
	\vskip -0.05in
	\tiny
	\begin{tabular}{llcccccc}
		\toprule
		\multirow{3}{*}[-2pt]{Dataset} & \multirow{3}{*}[-2pt]{Loss} & \multirow{3}{*}[-2pt]{\shortstack{Soft\\Truncation}} & \multicolumn{2}{c}{NLL} & \multicolumn{2}{c}{NELBO} & FID \\\cmidrule(lr){4-8}
		&&& \multirow{2}{*}{\shortstack{after\\correction}} & \multirow{2}{*}{\shortstack{before\\correction}} & \multirow{2}{*}{\shortstack{with\\residual}} & \multirow{2}{*}{\shortstack{without\\residual}} & \multirow{2}{*}{ODE} \\
		&&&&&&&\\\midrule
		\multirow{4}{*}[-2pt]{CIFAR-10} &  & \xmark & 3.03 & \textbf{2.97} & 3.13 & 3.11 & 6.70 \\
		&  & \xmark & 3.21 & 3.16 & 3.34 & 3.32 & \textbf{3.90} \\
		&  & \xmark & 3.06 & 3.02 & 3.18 & 3.14 & 6.11 \\
		&  & \cmark & \textbf{3.01} & 2.98 & \textbf{3.08} & \textbf{3.08} & 3.96 \\\midrule
		\multirow{5}{*}[-2pt]{ImageNet32} &  & \xmark & 3.92 & 3.90 & 3.94 & 3.95 & 12.68 \\
		&  & \xmark & 3.95 & 3.96 & 4.00 & 4.01 & 9.22 \\
		&  & \xmark & 3.93 & 3.92 & 3.97 & 3.98 & 11.89 \\
		&  & \cmark & \textbf{3.90} & \textbf{3.87} & 3.92 & 3.92 & 9.52 \\
		&  & \cmark & \textbf{3.90} & 3.88 & \textbf{3.91} & \textbf{3.91} & \textbf{8.42} \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[t]
\vskip 0.1in
\centering
	\caption{Ablation study of Soft Truncation for various model architectures and diffusion SDEs on CelebA.}
	\label{tab:ablation_architecture_sde_appendix}
	\vskip -0.05in
	\tiny
	\begin{tabular}{lllcccccc}
		\toprule
		\multirow{3}{*}[-2pt]{SDE} & \multirow{3}{*}[-2pt]{Model} & \multirow{3}{*}[-2pt]{Loss} & \multicolumn{2}{c}{NLL} & \multicolumn{2}{c}{NELBO} & \multicolumn{2}{c}{FID} \\\cmidrule(lr){4-9}
		& && \multirow{2}{*}{\shortstack{after\\correction}} & \multirow{2}{*}{\shortstack{before\\correction}} & \multirow{2}{*}{\shortstack{with\\residual}} & \multirow{2}{*}{\shortstack{without\\residual}} & \multirow{2}{*}{PC} & \multirow{2}{*}{ODE} \\
		&&&&&&&&\\\midrule
		\multirow{2}{*}{VE} & \multirow{2}{*}{NCSN++} &  & 3.41 & 2.37 & 3.42 & 3.96 & 3.95 & -\\
		& &  & 3.44 & 2.42 & 3.44 & 3.97 & 2.68 & -\\\midrule
		\multirow{2}{*}{RVE} & \multirow{2}{*}{UNCSN++} &  & 2.01 & 1.96 & \textbf{2.01} & 2.17 & 3.36 & -\\
		& &  & \textbf{1.97} & \textbf{1.91} & 2.02 & 2.18 & \textbf{1.92} & -\\\midrule
		\multirow{8}{*}[-9pt]{VP} & \multirow{2}{*}{DDPM++} &  & 2.14 & 2.07 & 2.21 & 2.22 & 3.03 & 2.32 \\
		& &  & 2.17 & 2.08 & 2.29 & 2.26 & 2.88 & \textbf{1.90}\\\cmidrule(lr){2-3}
		& \multirow{2}{*}{UDDPM++} &  & 2.11 & 2.07 & 2.20 & 2.21 & 3.23 & 4.72\\
		& &  & 2.16 & 2.08 & 2.28 & 2.25 & 2.22 & 1.94\\\cmidrule(lr){2-3}
		& \multirow{2}{*}{DDPM++} &  & 2.00 & 1.93 & 2.09 & \textbf{2.09} & 5.31 & 3.95\\
		& &  & 2.00 & 1.94 & 2.11 & 2.11 & 4.50 & 2.90\\\cmidrule(lr){2-3}
		& \multirow{2}{*}{UDDPM++} &  & 1.98 & 1.95 & 2.12 & 2.15 & 4.65 & 3.98\\
		& &  & 2.00 & 1.94 & 2.10 & 2.10 & 4.45 & 2.97\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[t]
\centering
	\caption{Ablation study of Soft Truncation for various  (equivalently, ) on CIFAR-10 with UNCSN++ (RVE).}
	\label{tab:ablation_epsilon_appendix}
	\vskip -0.05in
	\tiny
	\begin{tabular}{lcccccc}
		\toprule
		\multirow{3}{*}[-2pt]{Loss} & \multirow{3}{*}[-2pt]{} & \multicolumn{2}{c}{NLL} & \multicolumn{2}{c}{NELBO} & FID \\\cmidrule(lr){3-7}
		&& \multirow{2}{*}{\shortstack{after\\correction}} & \multirow{2}{*}{\shortstack{before\\correction}} & \multirow{2}{*}{\shortstack{with\\residual}} & \multirow{2}{*}{\shortstack{without\\residual}} & \multirow{2}{*}{ODE} \\
		&&&&&&\\\midrule
		\multirow{4}{*}{} &  & 4.64 & 4.02 & 4.69 & 5.20 & 38.82 \\
		&  & 3.51 & 3.20 & 3.52 & 3.90 & 6.21 \\
		&  & 3.05 & 2.98 & 3.08 & 3.24 & 6.33 \\
		&  & 3.03 & \textbf{2.97} & 3.13 & 3.11 & 6.70 \\\midrule
		\multirow{4}{*}{} &  & 4.65 & 4.03 & 4.69 & 5.20 & 39.83 \\
		&  & 3.51 & 3.21 & 3.52 & 3.88 & 5.14 \\
		&  & 3.05 & 2.98 & 3.08 & 3.24 & 4.16 \\
		&  & \textbf{3.01} & 2.98 & \textbf{3.08} & \textbf{3.08} & \textbf{3.96} \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[t]
\vskip 0.1in
\centering
\caption{Ablation study of Soft Truncation for various  on CIFAR-10 trained with DDPM++ (VP).}
\label{tab:ablation_prior_appendix}
	\vskip -0.05in
\tiny
\begin{tabular}{lccccc}
	\toprule
	\multirow{3}{*}[-2pt]{Loss} & \multicolumn{2}{c}{NLL} & \multicolumn{2}{c}{NELBO} & FID \\\cmidrule(lr){2-6}
	& \multirow{2}{*}{\shortstack{after\\correction}} & \multirow{2}{*}{\shortstack{before\\correction}} & \multirow{2}{*}{\shortstack{with\\residual}} & \multirow{2}{*}{\shortstack{without\\residual}} & \multirow{2}{*}{ODE} \\
	&&&&&\\\midrule
	 & 3.24 & 3.16 & 3.39 & 3.34 & 6.27 \\
	 & 3.03 & 3.00 & \textbf{3.05} & \textbf{3.05} & 3.61 \\
	 & 3.03 & 2.99 & 3.13 & 3.13 & \textbf{3.45} \\
	 & \textbf{3.01} & 2.98 & 3.08 & 3.08 & 3.96 \\
	 & 3.02 & 2.99 & 3.09 & 3.10 & 3.98 \\
	 & 3.03 & 2.99 & 3.09 & 3.09 & 3.98 \\
	 & \textbf{3.01} & 2.97 & 3.10 & 3.09 & 6.31 \\
	 & 3.02 & 2.96 & 3.09 & 3.09 & 6.54 \\\midrule
	 & \multirow{2}{*}{\textbf{3.01}} & \multirow{2}{*}{\textbf{2.95}} & \multirow{2}{*}{3.09} & \multirow{2}{*}{3.07} & \multirow{2}{*}{6.70} \\
	 &&&&&\\
	\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\vskip 0.1in
\centering
\caption{Ablation study of Soft Truncation for CIFAR-10 trained with DDPM++ when a diffusion is combined with a normalizing flow \cite{kim2022maximum}. We use .}
\label{tab:ablation_indm_appendix}
	\vskip -0.05in
\tiny
\begin{tabular}{lcccccc}
	\toprule
	\multirow{3}{*}[-2pt]{Loss} & \multicolumn{2}{c}{NLL} & \multicolumn{2}{c}{NELBO} & FID \\\cmidrule(lr){2-6}
	& \multirow{2}{*}{\shortstack{after\\correction}} & \multirow{2}{*}{\shortstack{before\\correction}} & \multirow{2}{*}{\shortstack{with\\residual}} & \multirow{2}{*}{\shortstack{without\\residual}} & \multirow{2}{*}{ODE} \\
	&&&&&\\\midrule
	 & 2.97 & 2.94 & 2.97 & 2.96 & 6.06 \\
	 & 3.17 & 3.11 & 3.23 & 3.18 & 3.61 \\
	 & 3.01 & 2.98 & 3.02 & 3.01 & 3.89 \\
	\bottomrule
\end{tabular}
\end{table}
		
		\subsection{Full Tables}\label{sec:full_tables}
		
		Tables \ref{tab:ablation_weighting_function_appendix}, \ref{tab:ablation_architecture_sde_appendix}, \ref{tab:ablation_epsilon_appendix}, \ref{tab:ablation_prior_appendix}, and \ref{tab:ablation_indm_appendix} present the full list of performances for Soft Truncation.
		
		\subsection{Generated Samples}
		
		Figure \ref{fig:denoising} shows how images are created from the trained model, and Figures from \ref{fig:cifar10} to \ref{fig:celebahq256} present non-cherry picked generated samples of the trained model.
		
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{denoising.pdf}
		\caption{Image generation by denoising via predictor-corrector sampler.}
		\label{fig:denoising}
	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{cifar10.png}
		\caption{Random samples of CIFAR-10.}
		\label{fig:cifar10}
	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{celeba_samples.png}
		\caption{Random samples on CelebA.}
		\label{fig:celeba}
	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{bedroom.jpg}
		\caption{Random samples on LSUN Bedroom.}
		\label{fig:bedroom}
	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{church.jpg}
		\caption{Random samples on LSUN Church.}
		\label{fig:church}
	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{ffhq256.jpg}
		\caption{Random samples on FFHQ 256.}
		\label{fig:ffhq256}
	\end{figure}
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{celebahq256.jpg}
		\caption{Random samples on CelebA-HQ 256.}
		\label{fig:celebahq256}
	\end{figure}




\end{document}
