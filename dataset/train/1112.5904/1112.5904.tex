\documentclass[11pt,a4paper]{article}

\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{cite}
\usepackage{psfrag}
\usepackage{ifdraft}
\usepackage{color}
\usepackage{paralist}
\usepackage{graphicx}
\graphicspath{{figures/}} 

\usepackage[colorlinks,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}


\newcommand{\rdots}{\mathinner{\mkern1mu\raise1pt\hbox{.}\mkern2mu\raise4pt\hbox{.}\mkern2mu\raise7pt\vbox{\kern7pt\hbox{.}}\mkern1mu}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\histogram}{mountain}
\newcommand{\Histogram}{Mountain}
\newcommand{\pslg}{PSLG}
\newcommand{\Pslg}{PSLG}

\newcommand{\eqdef}{:=}
\newcommand{\etal}{et~al.}

\newcommand{\marrow}{\marginpar[\hfill]{}}
\newcommand{\niceremark}[4]
   {\textcolor{#4}{\textsc{#1 #2:} \marrow\textsf{#3}}}



\newcommand{\tetsuo}[2][says]{\niceremark{Tetsuo}{#1}{#2}{magenta}}
\newcommand{\kevin}[2][says]{\niceremark{Kevin}{#1}{#2}{blue}}
\newcommand{\maike}[2][says]{\niceremark{Maike}{#1}{#2}{blue}}
\newcommand{\mati}[2][says]{\niceremark{Mati}{#1}{#2}{blue}}
\newcommand{\wolfgang}[2][says]{\niceremark{Wolfgang}{#1}{#2}{red}}
\newcommand{\gunter}[2][says]{\niceremark{G\"unter}{#1}{#2}{magenta}}
\newcommand{\andre}[2][says]{\niceremark{Andr\'e}{#1}{#2}{red}}




\begin{document}

\title{Memory-Constrained Algorithms for Simple Polygons\thanks{This work was initiated at the Dagstuhl Workshop on
Memory-Constrained Algorithms and Applications,
 November 21--23, 2011. We are deeply grateful
to the organizers as well as the participants of the workshop for
helpful discussions during the meeting.
The results were presented
at the
28th European Workshop on Computational Geometry (EuroCG'12), in Assisi, Italy, March 2012~\cite{abbkmrs-mcasp-12}.}}

\author{
Tetsuo Asano\thanks{JAIST, Japan, \texttt{t-asano@jaist.ac.jp}} \and
Kevin Buchin\thanks{TU Eindhoven, Netherlands, \{\texttt{k.a.buchin, m.e.buchin}\}\texttt{@tue.nl } M. Buchin is supported by the Netherlands Organisation for Scientific Research
(NWO) under project no. 612.001.106. } \and
Maike Buchin\footnotemark[3] \and
Matias Korman\thanks{UPC, Barcelona. {\tt{matias.korman@upc.edu}}. With the support of the Secretary for Universities and
Research of the Ministry of Economy and Knowledge of the Government of Catalonia and the European Union.} \and
Wolfgang Mulzer\thanks{Freie Universit\"at Berlin,
Germany, \{\texttt{mulzer, rote}\}\texttt{@inf.fu-berlin.de } } \and
G\"unter Rote\footnotemark[5] \and
Andr\'e Schulz\thanks{WWU M\"unster, Germany, \texttt{andre.schulz@uni-muenster.de} }
}


\maketitle

\begin{abstract}
A constant-work-space algorithm has read-only access
to an input array and may use only  additional
words of  bits, where  is the input size.
We show how to triangulate a
plane straight-line graph with  vertices in  time and constant work-space.
We also consider the problem of preprocessing a simple
polygon  for shortest path queries, where  is given by
the ordered sequence of its  vertices.
For this, we relax the space constraint to allow ~words of work-space.
After quadratic preprocessing, the shortest path
between any two points inside  can be found in  time.
\end{abstract}


\section{Introduction}
\label{sec:intro}

In algorithm development and computer technology, we observe
two opposing trends: on the one hand, there are vast amounts
of computational resources at our fingertips.
Alas, this often leads to bloated software that is written without regard to
resources and efficiency.
On the other hand, we see a proliferation of specialized tiny  devices
that have a limited supply of memory
or power. Software that is oblivious to space
resources is not suitable for such a setting.
Moreover, even if a small device features a fairly large memory, it may still
be preferable to limit the number of write operations.
For one, writing to flash memory is
slow and costly, and it also reduces the lifetime of the memory.
Furthermore, if the input is stored on a removable medium, write-access
may be limited for technical or security reasons.

With this situation in mind, it makes sense to focus on
algorithms that need only a limited amount
of work-space, while the input resides in read-only memory.
In this paper, we will develop such algorithms for geometric
problems in planar polygons or, more generally, plane straight-line graphs
(PSLGs).

In particular, we consider two fundamental problems from computational
geometry~\cite{deBergChvKrOv08}:
first, we are given a \pslg\  with  vertices, and
we would like to find a \emph{triangulation} for , i.e., a
\pslg\ with vertex set  that contains all the edges in  and
to which no edge can be added without violating planarity.
We show how to find such a triangulation in  time
with  words of work-space (Section~\ref{sec:polygon}).
Since our model does not allow storing the output, our algorithm outputs
the triangles of the triangulation one after another.

Then,  we apply this result in order to construct
a \emph{memory-adjustable} data structure for shortest path queries
in simple polygons (Section~\ref{sec:memadj}).
Given a simple polygon  with  vertices and a parameter
, we  build a data structure for  that
requires  words of storage and that lets us output the
edges of a shortest path between any two points inside  in
 time using  work-space.
The preprocessing time is .

\paragraph{Model Assumptions.}\label{sec:def}

The input to our algorithms is either a simple polygon  or a \pslg\

with  vertices, stored in a read-only data structure.\footnote{A
\emph{plane straight-line graph} (PSLG) consists of a planar point set 
(vertices) and a set  of non-crossing line segments
 with endpoints in  (edges). By planarity, we have .}
In case of a \pslg, we assume that  is given in a way
that allows us to enumerate all edges of  in  time and to find the
incident vertices of a given edge in constant time (a standard adjacency
list representation will do).
In case of a polygon, we require that the vertices of  are stored according
to their counterclockwise order along the boundary, so
that we can obtain the (clockwise and counterclockwise) neighbor
of any given vertex in constant time.
We also assume that is takes constant time to access the -
and -coordinates of any  vertex
and to perform basic geometric operations, such as determining
whether a point lies above or below a given line.

Storage is counted in terms of \emph{cells} or \emph{words}.
As usual, a word is assumed to be large enough to contain either
an input item (such as a point coordinate)
or a pointer into the input structure (of  bits).
Thus, in order to convert our storage bounds into
bits, we have to multiply them by a factor of .
In addition to the input, which can only be read, the algorithm has
 words of \emph{work-space} at its disposal for reading and writing.
Here,  is a parameter of the model and can range between  and .
We will consider both the case where  is a fixed constant
and the case where  can be chosen by the user. Since there is no way
to store the result, we use an additional operation \texttt{output} in order
to generate output data. We require that every feature
of the desired structure is \texttt{output} exactly once.

For simplicity, we will make the usual general position assumption:
no three input vertices are on a line and no two input vertices have
the same -coordinate.


\paragraph{Related Work.}
Given the many applications of memory-constrained algorithms, a
significant amount of research has been devoted to them,
even as early as in the 1980s~\cite{MunroPa80}.
One of the most studied problems in this setting is that of
selection in an unsorted array with elements from a totally ordered
universe~\cite{MunroRa96,MunroPa80,Frederickson87,RamanRa98,Chan10}.

In computational complexity theory, the constant-work-space model
is represented by the
complexity class LOGSPACE~\cite{AroraBa09}. There are several algorithmic
results for this class, most prominently Reingold's celebrated method for
finding a path between two vertices in an undirected
graph~\cite{Reingold08}. However, complexity theorists typically do not
try to optimize the running time of their constant-work-space algorithms,
whereas one of our objectives is to solve a given problem as quickly as
possible under the memory constraint.


There are other models that allow only read-access to the input, such as
the \emph{streaming} model~\cite{Muthukrishnan05} or the
\emph{multi-pass} model~\cite{ChanCh07}.
In these models, the input can be read only a bounded number of times
in sequential order, whereas we allow the input to be accessed in any order and
as often as necessary.
Other memory-constrained models are \emph{succinct}
data structures~\cite{JacoSucc89} and \emph{in-place}
algorithms~\cite{BronnimannIaKaMoMoTo04,BronnimannChCh04,ChanCh10,BronnimannCh06}.
The aim of succinct data structures is to use the minimum number of
bits to represent a given input. Although this kind of approach
significantly reduces the memory requirement,
in many cases  bits are still necessary.
For in-place algorithms, we also assume that only  cells of work-space
are available. However, in this model we are  allowed to reorder and sometimes
rewrite the input data. This makes it possible to encode our data
structures through appropriate permutations of the input and often
to achieve the best possible running time. Several classic geometric problems,
such as convex hull computation or nearest-neighbor search, have
been considered in this
model~\cite{BronnimannIaKaMoMoTo04,BronnimannChCh04,ChanCh10,BronnimannCh06}.
Note that the improved running times in the in-place model come at
the expense of requiring more powerful operations from the computational
environment, making the results less widely applicable.
Moreover, in most of
these cases the input values cannot be recovered after the algorithm
is executed.

A classic algorithm from computational geometry that fits into
our model is the gift-wrapping method
(also known as Jarvis' march):
given  points in the plane, we can report the  points
on the convex hull in  time using  cells of
work-space~\cite{s-chc-04}.
More recently, Asano \etal~\cite{AsanoMuRoWa11} initiated
the systematic study of constant-work-space algorithms
in a geometric context.  They
describe algorithms to compute well-known geometric structures (such as the
Delaunay triangulation, the Voronoi diagram, and the Euclidean
minimum spanning tree) using  cells of work-space.
They also show how to obtain a triangulation of a planar point
set and how to find the edges of the shortest path between any two points
inside a simple polygon. These algorithms use constant work-space
and run in quadratic time.
We emphasize once again
that since the output may have linear size, it is not stored,
but reported piece by piece.
Recently, Barba \etal~\cite{BarKorLanSil11}
gave a constant-work-space algorithm
for computing the visibility
region of a point inside a simple polygon.

Although we know of no previous method to triangulate a given
simple polygon  with sublinear work-space, it is known how
to find an \emph{ear} of  in linear time and constant
space~\cite{ElGindyEvTo93} (an ear is a triangle inside 
defined by a single line segment between two vertices of ).
However, due to the limited work-space, there seems to be no easy way
to extend this method in order to obtain a complete triangulation of .


\section{Preliminaries}
\label{sec:primitive}

We start by describing two simple operations and a basic spatial
decomposition technique that will be useful in designing our algorithms

\paragraph{Angular and Translational Sweep.} Let  be a point in the
plane, and let  be a ray with initial point  such that the line
segment  does not intersect any edge of the input \pslg\ .
In  an  \emph{angular sweep}, we move  along , and we would like
to determine the first vertex of  that is hit by the line segment
.
Note that the angular sweep is very similar to what is commonly called
a \emph{gift wrapping query}.
A \emph{translational} sweep is similar, but now we have a second ray
 with initial point , and the segment  is
swept vertically,  while maintaining the endpoints on  and . Figure~\ref{fig:sweeps} depicts
an example of both sweep types.
In both cases, the first vertex of  that is hit by  can be found in
 time: we only need to check for each vertex whether it lies
in the swept area and to maintain the point of smallest angle or of smallest
horizontal distance, respectively.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{cp{8mm}c}
\includegraphics[scale=.80]{angsweep} &&
\includegraphics[scale=.80]{transweep} \\
(a) & &(b)
\end{tabular}
\caption{Illustration of an angular sweep (a) and a translational sweep (b).}
\label{fig:sweeps}
\end{center}
\end{figure}




\paragraph{Vertical Decomposition.}
The \emph{vertical
decomposition} (or \emph{trapezoidation}) of  is
obtained by shooting vertical rays upward and downward from each vertex
of  until they hit another edge of  (or extend to
infinity)~\cite{deBergChvKrOv08}.
This gives a decomposition of the plane into trapezoidal cells.\footnote{Some
of these cells may be unbounded or degenerate into triangles.}
In general position, there are at most two vertices of  on the
boundary of each cell.
(If  represents a simple polygon~, we may be interested
only in the trapezoids interior to .)

In our model,
the trapezoids
incident to a given vertex 
can be determined easily in 
time per trapezoid~\cite{AsanoMuRoWa11,AsanoMuWa11}: first, we enumerate
the edges of  to find the first edge hit by the upward
and the downward vertical ray from  in  time. Then,
we enumerate all edges incident to  in circular order, including the
upward and the downward vertical ray. For each pair of consecutive edges, we perform an appropriate
translational sweep to find the trapezoid that is bounded by them. This takes  time per
edge.



\section{Triangulating a \Histogram{}}
\label{sec:hist}

In the next section, we will present an algorithm for
triangulating an arbitrary \pslg. First, however,
we need to explain how to handle inputs of a special kind.
The algorithm from this section will serve as an important building
block for the general case.

Let  be a simple polygon with vertices
, in circular order.
We call  a  \emph{monotone mountain}\footnote{Also known as
\emph{unimonotone polygon}~\cite{FournierMo84}.}
(or \emph{\histogram} for short) if the -coordinates of
 increase monotonically.
The edge  is called the \emph{base} of .
The \emph{shortest path} between two points  and  in  is
the shortest polygonal chain with endpoints  and  that does not
cross the boundary of .
We define the \emph{shortest path tree} SPT as the union of all
shortest paths from  to the other vertices of , see
Figure~\ref{fig:SPT}. SPT is a rooted tree with root ,
and it has the following properties:
\begin{proposition}
\label{prop:spt}
Any two adjacent edges of SPT form a left turn \textup(wrt.\ \textup);
i.e., SPT bends only ``upwards''.
Let  be an interior face of the \pslg\ formed by SPT
and . Then (i)  is bounded by the shortest paths
from  to two consecutive vertices 
  and ;\footnote{This holds in any simple polygon.}
  (ii)  is a
pseudotriangle\footnote{A pseudotriangle is a polygon with triangular convex hull.}, bounded from below by
 an SPT edge , from the right by an edge  of ,
 and from above by a concave chain
 of SPT edges \textup(as seen from inside\textup);
 and (iii)
  can be triangulated uniquely by connecting
  the rightmost vertex  with the
  reflex vertices on the upper
  boundary.\qed
\end{proposition}

\begin{figure}
\begin{center}
\includegraphics{histogramSPT}
\end{center}
\caption{The shortest path tree SPT from  to all other vertices
(in purple). Additional triangulation edges generated in our algorithm
are dashed. The figure illustrates a forward step (right shaded area)
and a backward step (middle area).}
\label{fig:SPT}
\end{figure}


The idea now is to generate the triangulation during a depth-first traversal of
the edges of SPT, starting
from the base edge  and visiting the children
of each vertex in \emph{counterclockwise} order.
This traversal can be interpreted geometrically as an Euler tour
of the plane graph SPT.
Since there is no space for a stack, this tour must be performed
in a ``stateless'' manner, using angular sweeps to determine the
next edge to be traversed.
We call a vertex \emph{finished} if it has been visited by
the tour and will not be visited again. Otherwise, the vertex is
\emph{unfinished}. In an Eulerian traversal of SPT, the vertices
of  become finished in order from right to left.

Our algorithm maintains two edges: (i) the current edge of the tour ,
with  lying to the right of ; and
(ii) the edge  of  such that
 are the finished vertices of the tour.
Observe that we can use
  to distinguish between vertices that
are finished and those that are not.
In each step we distinguish three different cases,
and we accordingly perform a step as follows.

\textbf{Case 1:}
 \emph{ is not incident to }.
We perform a \emph{forward} step
into the subtree rooted at .

\textbf{Case 2:} \emph{,
but  is a chord of }.
We perform a \emph{sideways} step to the
next edge out of 
that follows  in counterclockwise order.

\textbf{Case 3:} \emph{, and  is the edge  of }. We do a \emph{backward} step and
return to the parent of .


\begin{figure}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=.29\textwidth]{forward} &
\includegraphics[width=.29\textwidth]{side} &
\includegraphics[width=.29\textwidth]{back}
\\
forward step & sideways step & backward step
\end{tabular}
\end{center}
\caption{The three cases for the Eulerian traversal of SPT. The triangles that are reported while performing a step are shaded. Edges in the triangulation that are not contained in SPT are drawn dashed.}
\label{fig:unimonotone-traversal}
\end{figure}


\noindent
We start the algorithm with
 a sideways step from
 (as an exception to the above rules).
The algorithm continues until all vertices are finished and it
tries to make a backward step from .
The details of the three steps are straightforward.
In each step, we determine the
values ,  and  for the next step, and we output some triangles
of the triangulation (see Fig.~\ref{fig:unimonotone-traversal}; see also
Fig.~\ref{fig:SPT} for the forward and the backward step).

\textbf{Forward step.}
Let  be the intersection of the line  with the
edge . We perform a counterclockwise angular sweep of the segment
 around the vertex , letting  move along
 (By Proposition~\ref{prop:spt}, SPT makes only
upward bends, so the line  intersects  and
the segment  lies inside ).
Let  be the first vertex hit by the sweep
(note that  might be ).
Since  is the first child of ,
we update , leave  and
output the triangle  (by Proposition~\ref{prop:spt}(iii)).

\textbf{Sideways step.}
Since  is now finished, we
proceed to the previous edge .
We make a counterclockwise angular sweep of the segment ,
around , letting  mode along .
Let  be the first vertex that is hit (again,  might be .
We set  and output the
triangle   (by Proposition~\ref{prop:spt}(iii)).

\textbf{Backward step.}
Since  is now finished, we
proceed to the previous edge .
Let  be the intersection of the line 
with the base edge . As before, Proposition~\ref{prop:spt}
ensures that  exists.
We do a clockwise angular sweep of the segment  around
, keeping  on the base edge.
Call the first vertex that is hit .
We set .


Each edge of SPT is visited at most twice, so there are  steps.
Each step involves one angular sweep and some
additional processing that takes constant time. Thus, we get

\begin{theorem}
\label{thm:histo_triang}
Let  be a \histogram{} with  vertices. There is an algorithm
that does one counterclockwise scan of 
and outputs all triangles in a triangulation of .
The algorithm needs  angular sweeps and
additional processing time  as well as constant work-space.
\end{theorem}
In particular, we have shown the following.
\begin{corol}
Let  be an explicitly given \histogram{}\ with  vertices.
Then we can output all the triangles in a triangulation of 
in  time with constant work-space.
\end{corol}

\paragraph{Remark.}
Our algorithm produces the same triangulation as the classic
algorithm for triangulating monotone polygons~\cite{GareyJoPrTa78}.
This algorithm processes the
vertices from left to right, and it
maintains a stack that represents
the lower convex hull of the vertices  encountered
so far.
This lower convex hull is also the shortest path from  to .
 When processing the next vertex ,
 the vertices that disappear from the hull are popped from the stack
 and appropriate triangles between the popped vertices and
 the new vertex are generated.

The classic algorithm also performs an implicit depth-first traversal of SPT,
but in contrast to our approach,
the children
of a vertex are visited in \emph{clockwise} order.
Even though we can modify this algorithm for constant work-space,
it does not perform a single scan of the vertices of , a property that
will be crucial in the next section.


\section{Triangulating a \Pslg}
\label{sec:polygon}

We now describe how to triangulate
a \pslg\  with  vertices in 
time and with constant work-space.
Our algorithm only needs  scans over the edges of
, and we make no assumptions about the scanning order.

In the usual model of computation, it is well known that  can
be triangulated using the \emph{vertical decomposition}
(cf. Chazelle and Incerpi~\cite{ChazelleIn84}).
Since our algorithm follows the same strategy, we briefly review
how this works: first, we compute
the edges of the convex hull of  and add them to . Then
we find the vertical decomposition of the resulting graph,
and insert an edge between any two non-adjacent vertices of
 that are contained in the same trapezoid
(see Fig.~\ref{fig:histo_decomp}).
Consider the resulting graph  that contains the convex hull edges
and the newly inserted edges.
All  interior faces of  are \histogram s:
since every vertex (except for the left- and the rightmost ones)
has at least one edge incident to either side,
the faces must be monotone polygons.
Suppose there is a face  with vertices on both the upper and the lower
boundary. Then there would be
a trapezoid with a vertex at the upper and at the lower
boundary, and we would have inserted an edge between those vertices,
so  cannot exist. Now, we can easily triangulate each face of the
resulting graph with an algorithm for triangulating \histogram s.

In our setting, we cannot explicitly compute the decomposition of .
Instead, we enumerate all edges of  and of the convex hull of
. Note that convex hull edges can be found
in linear time per edge through Jarvis march~\cite{s-chc-04}.
For each such edge , we check whether  is the  base of a
\histogram{}.
This is the case if and only if
 is incident to more than one trapezoid above it or more than
one trapezoid below  it.
As described in Section~\ref{sec:primitive}, this can
be checked in linear time.
(An inserted chord is never the base of a \histogram.)
Once the base of a \histogram{}  is at hand, we would like to
use Theorem~\ref{thm:histo_triang} to triangulate it.
For this, we need  to enumerate the
vertices of  in counterclockwise order and to perform
angular sweeps. The former can be done by enumerating the trapezoids
that are incident on one side of , and takes  time
per trapezoid (see Section~\ref{sec:primitive}).
The latter can be done in  time by enumerating the vertices of , because
the vertices outside  cannot affect the angular scan.
Thus, by Theorem~\ref{thm:histo_triang}, it takes  time to triangulate
, where  is the number of vertices in .
Since the total size of
all \histogram s is , we get the following result.

\begin{theorem}
Given a \pslg\  with  vertices,
we can output all triangles in a triangulation of  in 
time with constant work-space.
\end{theorem}

\begin{figure}
\begin{center}
\includegraphics[scale=0.55]{example4}
\end{center}
\caption{Decomposition of a polygon into \histogram s.}
\label{fig:histo_decomp}
\end{figure}


\section{A Memory-Adjustable Data Structure for Shortest Paths}\label{sec:memadj}

Let  be some set of  objects.
In general, the purpose of a \emph{data structure}  for
 is to support certain \emph{queries} on
 efficiently.
Ideally,  has linear size,
and the query algorithm searches within  with only  cells of
additional work-space.
In the classic setting, the whole set  is contained in the
data structure,
so the storage must be at least as large as the input.


We take a different approach: recall that our input
cannot be modified. Thus, our strategy is to
preprocess the data and to store some additional information
in a data structure of size  (for some parameter ).
The objective is to design an algorithm that uses this additional
information in a way that supports efficient query processing.
Ideally, we would like to have a trade-off between the amount of additional
storage and the running time of the algorithm. This has been done
successfully for many other classic problems such as selection and
sorting~\cite{MunroRa96,RamanRa98}.


Naturally, the most important quality measure for any such algorithm is the
query time. However, the preprocessing
time for constructing the data structure should also be taken into account. In
this section, we describe a data structure for computing the shortest path
between any two points inside a simple polygon  with  vertices.
It is known that this takes  time with constant
work-space~\cite{AsanoMuRoWa11}  and  time  with
linear work-space~\cite{GuiHers89}.

\iffalse
\wolfgang{Added a new definition by Tetsuo to the SVN. Not integrated yet.}
Definition of a memory adjustable data structure

\begin{itemize}
\item Input data of  bits are stored in a
read-only array.
\item The size of a data structure is bounded by 
bits for a parameter .
\item An algorithm for constructing the data structure can
use work space of  bits.
\item Query algorithms for answering fixed types of queries
on the input set can use work space of  bits.
\end{itemize}

A memory adjustable data structure should be evaluated by
\begin{itemize}
\item time complexity of the construction algorithm, and
\item time complexities of the query algorithms.
\end{itemize}
\fi

We describe how to construct a data structure that requires 
words of storage and that can find the shortest paths between any two
points in  in time . The preprocessing time is , and
the query algorithm is allowed to use  additional words of work-space.
Note that for  the query time is , while for
 the query time is . Thus, we achieve a smooth trade-off between
the known results.


\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.72]{slab7}}
\caption{A decomposition of a simple polygon into
 disjoint subpolygons using  chords and its
associated tree decomposition.}
\label{fig:slab7}
\end{figure}

\subsection{Preprocessing}
During the preprocessing phase, we use our allotted space to
compute  chords that partition  into  subpolygons with
 vertices each, see Figure~\ref{fig:slab7}.
In the following, these chords will be called
\emph{cut edges}. Our data structure stores the subpolygons
together with a tree that represents the adjacencies between them.
Each subpolygon is represented by the corresponding cut edges and segments of
the boundary of , in counterclockwise order. The boundary segments
can be described with  storage cells each, by using
the index of the first and the last edge. Thus, the total
space for the data structure is .



We use the following well-known observation~\cite{Chazelle82},
which is based on the fact that
any triangulation of a simple polygon dualizes to a tree with
 nodes and
maximum degree .
 \begin{proposition}
\label{prop:cut}
   Let  be a simple polygon with  vertices.
   Any triangulation of  has a chord that splits  into
   two subpolygons with at most
    vertices each.
\qed
 \end{proposition}

A chord as in Proposition~\ref{prop:cut} is called a \emph{balanced cut}.

\begin{theorem}\label{thm:preprocess}
Let  be a simple polygon with  vertices. For any ,
there exists a set of  chords that are pairwise non-intersecting and that
partition  into  subpolygons with  vertices each.
The chords can be found in  time using constant work-space.
\end{theorem}
\begin{proof}
We set .
If , we simply output a triangulation of .
Otherwise, we use Proposition~\ref{prop:cut} iteratively to
split  into smaller and smaller subpolygons.
In each round, we scan over all pieces computed so far,
and for every piece with more than  vertices we find a balanced
cut.
In the end, each piece has between  and  vertices, so
we obtain  subpolygons of size .

Let  be a subpolygon with  vertices. We can find a balanced cut for 
in time : triangulate  using Theorem \ref{thm:histo_triang}.
Whenever the algorithm outputs a new chord, we compute the size
of the two pieces it cuts, and we remember the one with
the most balanced cut. Note that the size of each piece can
be computed in  time: the  subpolygon  is represented as
a sequence of at most  cut edges or segments of the boundary of ,
and for each such segment we can determine the relevant size in
constant time.

Consider the -th round, and  suppose we have  pieces of size
, where .
By Proposition~\ref{prop:cut},
we have  for all .
Thus,  the running time for round  is proportional to

Summing over all rounds, we get a total running time of , as claimed.
\end{proof}

\paragraph{Remark.}
Guibas and Hershberger~\cite{GuiHers89} showed that
if linear work-space is available, the preprocessing time can be reduced
to . For completeness, we briefly sketch their method.
First, we triangulate  with Chazelle's
algorithm~\cite{Chazelle91}.
Then, we find the cut edges by greedily pruning the tree  that
corresponds to the dual graph of the triangulation.
Set .
Every vertex of  has a weight, initialized to .
In each round, we scan over the leaves of . We remove those
leaves whose weight is between  and 
and declare the corresponding edges to be cut edges. Then, we
delete the remaining leaves and add their weight to their parents.

In the end, the remaining part may have less than  vertices. If so,
we remove one cut edge to merge this part with an adjacent one.
Since each vertex is visited once, the whole procedure takes linear time.


\subsection{Query Algorithm}
We now describe the query algorithm. Given two points ,
we would like to find the shortest path  between them,
while taking advantage of the precomputed polygon decomposition. The main idea
is to compute a shortest path for each subpolygon with the constant-work space
algorithm of Asano~\etal~\cite{AsanoMuRoWa11},
and to concatenate the resulting paths. However, additional steps are
necessary to deal with edges of  that cross several subpolygons,
so the algorithm gets a bit more involved.

First, let us quickly review the algorithm of
Asano~\etal~\cite{AsanoMuRoWa11} (referred to as AMRW from now on).
The AMRW-algorithm stores a triple  of points.
The point  is a vertex of , while  and  lie on
the boundary of  (not necessarily vertices). The triple maintains the invariant
that all vertices of  up to 
have been reported. The line segments  and  cut off a
subpolygon  that contains the target .
In each step, the algorithm shoots a ray into  that originates at  and
that lies inside the \emph{visibility cone} determined by  and .
The direction of the ray is chosen according to a case distinction
whose details we omit. Let  be the point where this ray hits
the boundary of . The line segment  divides
 into two parts, and by determining which part
contains the target , we can find a new
triple . This triple either yields a new vertex of ,
or it makes  smaller.
AMRW show that in either case the ray shooting operation can be
charged to a vertex of  in a way where every vertex is charged at most twice.
Thus, since each step takes linear time, the total
running time is bounded by . Please refer to
the original article for further details~\cite{AsanoMuRoWa11}.

Our algorithm uses a similar strategy: it also maintains a triple 
of points that fulfills the same invariant, and in each step it shoots a ray
to determine the triple for the next step. As long as the triple and the
ray are contained in a simple subpolygon of the decomposition, we can just
use the previous method without change while achieving the desired speedup.
We call this the \emph{standard situation}. However, if the points of the
triple are not contained in the same polygon, or if the ray crosses a cut edge
we need to take additional measures in order to quickly update the triple.
In this case, the algorithm switches to a different mode, the
\emph{long-jump situation}. We now describe the details.

\paragraph{Initialization.}
We start by locating the subpolygons
,  that contain  and . For this, we shoot upward
vertical rays from  and from , and we find the first edge (or cut edge)
 and  of  that is hit. This takes linear time.
Then we determine the subpolygons
 and  that contain  and . If the edge is a cut edge, this
is immediate. If not, we go through the description of the subpolygons, and
for each edge sequence, we determine in
constant time whether it contains  or  by comparing indices.
This requires  time, since the total boundary of all  subpolygons
has  pieces.



If , we
apply the constant-work-space method within  and are done. Otherwise,
we take the tree that represents the polygon partition, and we find the path
between  and . Every edge on that path
corresponds to a cut edge that must be crossed by ,
in the same order. For any subpolygon  that is traversed by ,
we define the \emph{entrance} of  as the cut edge
through which  enters  and the \emph{exit} of  as
the cut edge through which  leaves .
The remaining cut edges
are not be crossed by , so
we treat them as obstacles.

We initialize the triple  as in AMRW.
Initially, all of ,  and  are contained in the same subpolygon
of , and we call this polygon .
Next, the algorithm enters the \emph{standard situation}.




\paragraph{Standard Situation.}
In the standard situation, both endpoints  and  are contained
in the same polygon . The vertex  does not necessarily lie
in , but since the AMRW-algorithm works with the subpolygon
 that is cut off by the segments  and , we only need to
deal with the vertices of  and , see Figure~\ref{fig:standard}.

\begin{figure}[htb]
 \centerline{\includegraphics[scale=1]{standard}}
\caption{The standard situation.}
 \label{fig:standard}
\end{figure}


We apply the AMRW-strategy almost without change:
we shoot a ray  that partitions 
into two. If  does not hit the exit of , we can use the same
rules as in AMRW to update the triple  (we take the location of the
exit as a proxy for the target point ).

The only problem arises when  hits the exit.
In this case we must first complete the ray shooting operation:
we extend  into the adjacent subpolygon. If it again hits the
exit chord of this subpolygon, we continue into the third subpolygon, and so on,
until  hits a point  in the boundary of  (or a cut edge that is not
traversed by ). The ray  splits the wedge defined by 
into two parts, and we take the part that contains the target.
The running time is  times the number of subpolygons that are visited.
Then we switch to the \emph{long-jump situation}.
\paragraph{Long-jump situation.}
In general, the invariant of this situation is as follows:
we  maintain a current start vertex  and two
shortest paths, SP from  to a point 
and SP from  to a point .
The shortest paths form a \emph{funnel}  with apex ,
and we maintain the invariant that  lies inside the polygon 
that is cut off by , so  must go into . Notice that all
funnel vertices are reflex, i.e., they have
interior angle larger than .
Let  denote some ray from  into .
For the exposition, we assume that  extends from
 to the right,  goes in the direction of the positive -axis,
SP forms the upper boundary, and SP forms the lower boundary, see
Figure~\ref{fig:funnel-1}.

In general,  and  may lie in different subpolygons 
and . If so, we  assume w.l.o.g.
that  is more advanced than 
(this can be determined in constant time, since we know from the initialization
which sub-polygons must be traversed in which order). Then, our first goal is to
 incrementally extend the shortest path SP to the
lower endpoint of the entrance of  (Procedure \emph{Catch-up}).

If , we shoot a ray  and extend one of the
SP edges (Procedure \emph{Extend}). We will proceed in different
ways depending on which side of the ray  the target 
lies (recall that  is the exit of ).

\begin{figure}[htb]
\centerline{\includegraphics[scale=0.8]{funnel-1}}
\caption{Procedure \emph{Catch-up}: Growing the shorter side of the funnel}
\label{fig:funnel-1}
\end{figure}

\begin{figure}[htb]
\centerline{\includegraphics[scale=0.8]{funnel-2}}
\caption{Procedure \emph{Extend}. The ray  splits the funnel into two funnels.}
\label{funnel-2}
\end{figure}


We have to implement the procedures carefully so that we do not exceed the
storage bounds.  Therefore, we do not store the whole funnel boundaries
SP and SP explicitly, but we store of each SP only the first edge,
the two last edges, and the  SP edges that cross some cut edge.
We will now give the details of the two procedures.

\noindent
\textbf{Procedure \emph{Catch-up}.}
We have computed the shortest path SP to some point
 in the subpolygon
, and our goal is to proceed towards
the lower endpoint of the exit of .
For this, we perform a clockwise sweep of a ray whose initial point is 
and whose original direction follows the last edge of SP, until we
find the first vertex of the lower boundary of  that is hit,
see Figure~\ref{fig:funnel-1}.

Two cases can happen depending on where the new vertex  lies. If it
makes a right turn with the last edge of SP, we have found
one more edge of the funnel. Thus, we add edge  to the funnel, and
continue. If  makes a left turn, we must remove vertices from 
(so as to satisfy the invariant of the funnel). Those vertices are
removed from  towards ,
and also, after reaching , from the beginning of SP (in the
latter case we output the removed vertices as shortest path vertices).
Finally, we add a new edge to SP.

Since we do not explicitly store SP and SP, when removing a SP vertex,
we may have to look for the
predecessor (or successor) edges by angular sweeps.
Suppose  is an edge of SP whose predecessor we want to determine.
If the predecessor of  crosses the entrance of the current subpolygon,
we can find it in constant time (since it was explicitly stored).
Otherwise, let  be the initial point of .
We perform a counter-clockwise angular sweep of the ray with
start vertex  and whose initial direction is the direction of ,
until we hit the first vertex that comes before 
on the lower boundary of the relevant subpolygon.  This takes
 time for each predecessor or successor that we need to find.
The procedure for finding successor edges is symmetrical.

That is, regardless of where  lies, we can a new edge to the funnel. Once this edge is found, we check where our target lies, and act using the AMRW-strategy. We can repeat this operation until we eventually find the lower end of the exit of  (for example, after  {\em Catch-up} has been executed in Figure~\ref{fig:funnel-1}). In this case, we advance  to the next subpolygon, and we iterate until we reach the entrance of .

\noindent
\textbf{Procedure \emph{Extend}.}
We have now two funnel endpoints in the same subpolygon .
If both SP and SP have only one edge, we can switch back to
the standard situation, letting the triple  represent the funnel.

Thus, suppose w.l.o.g. that SP has more than one edge. We take
the next-to-last edge  of SP and shoot a ray in the direction of
 into , see Figure~\ref{funnel-2}. Call this ray  and call its origin
vertex .
If  does not hit the exit of ,
the procedure is easy:
we determine, in  time, on which side of  the target lies.
If  lies above  we pop the last edge of the funnel, as in
Procedure \emph{Catch-up}, and proceed.
If  lies below  then
we report all vertices along SP up to  (using the same method as
in \emph{Catch-up}).
Then we take the triple that consists of , the endpoint of  and
, and we switch back to the standard situation (with ).

Finally, we consider the case that  goes through the exit,
as shown in Figure~\ref{funnel-2}.
In this case, we extend  until it hits the boundary of ,
in some subpolygon .
(This takes time
proportional to  times the number of subpolygons that are
traversed from  to .)
Next, we determine on which side of  the target lies.
If  lies above ,  forms the new last edge of SP,
 is advanced to
, and we continue with Procedure \emph{Catch-up}.
If the target lies below , we report all vertices on SP
up to , and we take a funnel that consists
only of  and the last edge of SP.
We advance  to
 and continue with Procedure \emph{Catch-up}.


\paragraph{Runtime Analysis.}

By the analysis of AMRW, in the standard situation we spend
 time per subpolygon,
for a total of .

During the long-jump situation, there are  operations of
adding or removing a vertex of the funnel: each edge is removed at most once,
and even though not all vertices of the funnel are polygon vertices, it still holds
that each edge of  contains at most a constant number of funnel vertices.

Each funnel operation incurs an overhead of   for an
angular sweep or a ray shooting operation.
The only exception occurs when a ray goes through  subpolygons,
. But in this case the more advanced end of the funnel will
make progress by crossing at least  cut edges. Thus, the running
time  for these cases cannot exceed
 in total.
We have thus obtained the following theorem.

\begin{theorem}
\label{path-with-space}
  Let  be a simple polygon of  vertices and  be a parameter
  between  and~. We can build a shortest-path-data structure for 
  of size  in  time and  work-space (or in  time and space).
  With this data structure, we can compute the shortest path
  within  between any two points in  using
   time and  work-space.
\qed
\end{theorem}

\section{Open Problems}

Obvious topics for future research are improvements of
the results. For example, it would be interesting obtain a time-space trade-off
for triangulating a \pslg. Furthermore, Theorem~\ref{thm:preprocess} describes
how to find a good cut edge for a simple polygon by essentially triangulating
the whole polygon and giving the most balanced cut. A natural question is if we
can obtain a balanced cut in subquadratic time. The size of the smaller part should
be at least a constant fraction of the whole, but it need not be  as in
Proposition~\ref{prop:cut}.  Moreover, the
cut would not necessarily have to be a diagonal connecting two
vertices; any straight (for example, vertical) segment partitioning
the polygon would be fine.

Can the work-space for the last result on finding a shortest path
between  and  (Theorem~\ref{path-with-space}) be reduced to less
than , maybe even constant?  There are two components of our
query algorithm that need  space:
(i)~after locating the subpolygons of  and  in  time, we
 identify and store the sequence of subpolygons traversed by
the path, i.e., we find the path between two vertices in the
tree of subpolygons.
(ii)~we have to process and update the funnels.

There is a chance to reduce the complexity of part~(i): by the
techniques of \cite[Theorem~2]{AsanoMuWa11}, one can walk through the
sequence of subpolygons from  to  in the right order in constant
space and  time.

\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}
