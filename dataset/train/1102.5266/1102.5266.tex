\documentclass{amsart}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\disc}{Disc}
\DeclareMathOperator{\Real}{Re}
\DeclareMathOperator{\Imag}{Im}

\allowdisplaybreaks

\newcommand{\EVAL}{\texttt{SqFreeEVAL}}
\newcommand{\EVALTYPE}{\texttt{EVAL}-type}
\newcommand{\GEN}{\texttt{Bisection}}
\DeclareMathOperator{\HM}{HM}

\begin{document}

\title{\EVAL: An (almost) optimal real-root isolation algorithm}

\author{Michael A. Burr}
\address{Fordham University, 441 East Fordham Road, Bronx, NY 10458, USA}
\email{mburr1@fordham.edu}

\author{Felix Krahmer}
\address{Hausdorff Center for Mathematics, Universit\"at Bonn, Endenicher Allee 60, 53115 Bonn, Germany}
\email{felix.krahmer@hcm.uni-bonn.de}

\begin{abstract}
Let  be a univariate polynomial with real coefficients, .  Subdivision algorithms based on algebraic techniques (e.g., Sturm or Descartes methods) are widely used for isolating the real roots of  in a given interval. In this paper, we consider a simple subdivision algorithm whose primitives are purely numerical (e.g., function evaluation).  The complexity of this algorithm is adaptive because the algorithm makes decisions based on local data.  The complexity analysis of adaptive algorithms (and this algorithm in particular) is a new challenge for computer science. In this paper, we compute the size of the subdivision tree for the \EVAL\ algorithm.

The \EVAL\ algorithm is an evaluation-based numerical algorithm which is well-known in several communities.  The algorithm itself is simple, but prior attempts to compute its complexity have proven to be quite technical and have yielded sub-optimal results.  Our main result is a simple  bound on the size of the subdivision tree for the \EVAL\ algorithm on the benchmark problem of isolating all real roots of an integer polynomial  of degree  and whose coefficients can be written with at most  bits.

Our proof uses two amortization-based techniques: First, we use the algebraic amortization technique of the standard Mahler-Davenport root bounds to interpret the integral in terms of  and .  Second, we use a continuous amortization technique based on an integral to bound the size of the subdivision tree.  This paper is the first to use the novel analysis technique of continuous amortization to derive state of the art complexity bounds.

\noindent{\em Key words}: Continuous Amortization, Adaptive Analysis, Subdivision Algorithm, Integral Analysis, Amortization, Root Isolation.
\end{abstract}

\maketitle

\section{Introduction}
In this paper, we show that the size of the subdivision tree for the simple, evaluation-based, numerical algorithm \EVAL\ has size  for the benchmark problem of isolating all of the real roots of an integer polynomial of degree  whose coefficients can be represented by at most  bits.  Under the mild assumption that , this complexity simplifies to the optimal size of .  The optimality and simplicity of the \EVAL\ algorithm imply that it may be a useful algorithm in practical settings.  The bound on the size of the subdivision tree is achieved via a straight-forward and elementary argument.  The two main techniques which are used in the computation are algebraic amortization, in the form of Mahler-Davenport bounds, and continuous amortization, in the form of an integral technique as presented in \citep{Burr-Krahmer-Yap:integral:09}.

\subsection{\EVALTYPE\ algorithms}
The \EVAL\ algorithm which we study in this paper is a specific example of what we call an \EVALTYPE\ algorithm.  These algorithms are so named because they are based on function evaluation: \EVALTYPE\ algorithms take, as input, functions which allow some subset of the following two predicates: First, these functions and their derivatives can be evaluated at a countable dense subset of their domain.  In this paper, the domain will be the real numbers and the countable dense subset will be the dyadic integers.  Second, these functions and their derivatives can be approximated on intervals in such a way that the approximation converges as the input intervals converge to a point.  In this paper, the approximation is derived from interval arithmetic on a Taylor sequence.  The simplest and most well-known example of an \EVALTYPE\ algorithm is Lorensen and Cline's marching cube algorithm \citep{marchingcube1987}.

\EVALTYPE\ algorithms are typically studied because of their simplicity and generality.  These algorithms are fairly general because their inputs can be extended to more general analytic functions.  In particular, many analytic functions have interval arithmetic available to them, and, therefore, it is possible to approximate these functions on intervals.  In addition, with the limited predicates available to \EVALTYPE\ algorithms, most of the techniques which are used in these algorithms are analytically based (as opposed to algebraically based).  These algorithms are simple because, in many cases, \EVALTYPE\ algorithms are based on simple recursive bisection algorithms.  Such algorithms iteratively subdivide an initial domain until each set in the resulting partition of the initial domain satisfies a (usually simple) terminal condition.  Bisection algorithms are common in computer graphics \citep{SurveySubdivision} as well as in computational science and engineering applications \citep{domaindecomposition}.  Bisection algorithms are of particular interest because they are adaptive; they perform more bisections near difficult features and fewer bisections elsewhere.  However, this adaptivity makes the complexity analysis of such algorithms more difficult because the subdivision tree may have a few deep paths while the remainder of the tree remains modest in size.

\EVALTYPE\ algorithms have been studied in the univariate case in \citep{Henrici:search:70,Yakoubsohn:bisection:05,sagraloff-yap:ceval:09,burr-sharma-yap:eval:09,Burr-Krahmer-Yap:integral:09}, in the bivariate and trivariate cases in \citep{marchingcube1987,snyder:interval:92,plantinga-vegter:isotopic:04,plantinga:thesis:06,linyap2009,burr+3:subdiv2:10}, and in the multivariate case in \citep{galehouse:thesis}.  All of these algorithms are devoted to approximating algebraic (and in some cases analytic varieties) in the real or complex settings.  The algorithms in \citep{burr-sharma-yap:eval:09,Burr-Krahmer-Yap:integral:09} are designed to find all real roots of a polynomial or analytic function while the algorithms in \citep{Henrici:search:70,Yakoubsohn:bisection:05,sagraloff-yap:ceval:09} are designed to find the complex roots of a polynomial or analytic function (note that \citep{Henrici:search:70} is only designed to find a single root of a polynomial).  Each of these algorithms is very closely related to the \EVAL\ algorithm considered in this paper; the main differences are in the setting, in the type of subdivisions performed, and in various preprocessing steps.  We give a more detailed account of these algorithms in the next section.  The two-dimensional \EVALTYPE\ algorithm \citep{plantinga-vegter:isotopic:04,plantinga:thesis:06} was presented for approximating smooth and bounded varieties.  It was extended to singular and unbounded varieties in \citep{burr+3:subdiv2:10}; in addition, the tests performed by the algorithm were improved in \citep{linyap2009}.

\subsection{The \EVAL\ algorithm}
There are many bisection algorithms for finding roots, see Section \ref{Intro:RootsFinding} for references, but among such algorithms, the \EVAL\ algorithm is one of the simplest and most widely applicable, see \citep{burr-sharma-yap:eval:09}.  There are two distinct paths in the literature which arrive at algorithms similar to the \EVAL\ algorithm: one path proceeds through the consideration of magnitudes of derivatives and the other path proceeds via interval arithmetic.

We begin by discussing the history from the magnitudes of derivatives perspective.  In \citep{Henrici:search:70}, the author presents an algorithm for finding a single complex root of a polynomial.  The test  from the paper is essentially used here.  In \citep{Yakoubsohn:bisection:05}, the test is developed into a bisection algorithm and to find all complex roots of entire functions, not just polynomials.  In the paper, however, the test from \citep{Henrici:search:70} is used only as a one-sided test; therefore, the algorithm can only exclude regions from containing roots and does not confirm that roots exist in the final regions.  There, the algorithm was termed a {\em bisection-exclusion} algorithm to reflect this drawback.  Finally, in \citep{sagraloff-yap:ceval:09}, the algorithm from \citep{Yakoubsohn:bisection:05} was adapted to polynomials in order to confirm that roots exist in the final regions; there, the authors studied both an algorithm for finding complex roots as well as one for finding real roots.  The \EVAL\ algorithm is a natural restriction of these complex root-finding algorithms to the real line.

On the other hand, from the interval arithmetic community, a bisection algorithm using interval methods was suggested in \citep{moore:bk,mitchell:robust-ray:90}.  In these papers, any interval function can be used; if the standard centered form for polynomials is used, see \citep{ratschek-rokne:range:bk}, then the exclusion conditions are identical (when  and  are square free) to those for the \EVAL\ algorithm.

In this paper, we study the \EVAL\ algorithm on the standard benchmark problem of finding all of the real roots of a polynomial.  We show that, in this case, the subdivision tree has the favorable size of  which simplifies to the optimal size of  under the mild assumption that .  Since this algorithm uses only local data to find roots, it is an adaptive algorithm and may be more efficient than the standard exact algorithms in certain cases, see \citep{burr-sharma-yap:eval:09}.  In addition, the \EVAL\ algorithm can handle analytic varieties, see \citep{burr-sharma-yap:eval:09}, which extends its reach beyond that of more standard exact algorithms which require sophisticated algebraic primitives and are specialized to polynomials.  These advantages of the \EVAL\ algorithm imply that it may be more practical than other standard root isolation algorithms in practice.

\subsection{Previous complexity results}

The computational complexity of \EVALTYPE\ algorithms has proven to be quite a challenging problem because the algorithms are adaptive and the analytic primitives do not carry much information about the global structure (unlike algebraic information).  Here, we survey the complexity analyses of the precursors to the \EVAL\ algorithm.  In most situations, the complexity is computed in terms of the size of the subdivision tree of the specific \EVALTYPE\ algorithm (this is almost equivalent to counting the number of tests performed by the algorithm). There have been two main techniques to find the size of the subdivision tree: by finding the width of the subdivision tree at various subdivision levels or by finding the local depth of the subdivision tree.

In \citep{Henrici:search:70}, the author is searching for only a single root, and, therefore, retains a single disk containing a root at each stage of the algorithm.  Many tests are performed in the algorithm, however, because at each stage of the algorithm, tests are performed on a covering of the previously retained disk. The final stopping criterion for this algorithm is based on a precision  which is chosen {\em a priori} by the user.  When the worst-case root separation bound for a polynomial is used, the complexity of the subdivision tree becomes .

In \citep{Yakoubsohn:bisection:05}, the author is searching for all of the complex roots of an analytic function.  In the computation, a bound on the width of the tree is computed to bound the number of subdivisions performed.  Since this algorithm only excludes regions and lacks an inclusion test, it is possible that the final output regions do not contain roots or do not separate roots.  The final stopping criterion for this algorithm is based on a precision  which is chosen {\em a priori} by the user. When the worst-case root separation bound for a polynomial is used, the complexity of the subdivision tree becomes either  or  after  steps of the Graeffe iteration.

In \citep{Burr-Krahmer-Yap:integral:09}, we search for all of the real roots of a polynomial.  Here, the computation is based on the depth of the tree over each point of the initial interval.  In the paper, we introduced the idea of continuous amortization via an integral and showed how to use it to bound the size of the subdivision tree.  In particular, we proved a complexity bound of  for the subdivision tree.

In \citep{sagraloff-yap:ceval:09}, the authors present algorithms to find all of the real or all of the complex roots of a polynomial.  In the computation, a bound on the width of the subdivision tree is used to compute the number of subdivisions performed.  The authors show that the complexity of the subdivision tree is  in the real case and  in the complex case.  In addition, the authors show that the bit complexity of both of these algorithms is  where the  means that logarithmic factors in  and  have been suppressed.

Each of the analyses in \citep{Yakoubsohn:bisection:05,Burr-Krahmer-Yap:integral:09,sagraloff-yap:ceval:09} are quite technical, complicated, and require several constants to be defined whose use becomes justified only after the completion of the complexity analysis.  In contrast, the computation in this paper is quite simple and provides the better bound of .  It should be noted that although this is the best bound known, it does not directly replace the bounds presented in these papers because some are in the different setting of the complex plane and others use different preprocessing steps.  In the case where the polynomial and its derivative are both square free and we are searching for the real roots, all of these algorithms are identical and our bound on the subdivision tree is the best.

\subsection{Algebraic and continuous amortization}
In this paper, we use amortization in two forms: algebraic and continuous.  Algebraic amortization originated with Davenport \citep{davenport:85} where the individual root separation bounds are replaced by a product of root separations.  This bound was then studied in \citep{du-sharma-yap:sturm:07,eigenwillig-sharma-yap:descartes:06} where it was generalized to other root separation products including complex roots.  This technique has proven useful to compute the complexity of the subdivision tree for many other root isolation techniques, see Section \ref{Intro:RootsFinding}.  We introduced continuous amortization in \citep{Burr-Krahmer-Yap:integral:09} to bound the size of the subdivision tree of an \EVALTYPE\ algorithm.  In this paper, we show that continuous amortization can be used to significantly simplify complexity calculations.

In continuous amortization, we use a complexity charge  whose domain is the input region, and, for each  in the input region,  is a lower bound on the size of any leaf interval containing .  Then  is related to the depth of the subdivision tree for an interval which contains .  In \citep{Burr-Krahmer-Yap:integral:09}, we used continuous amortization to compute the size of a subdivision tree for an \EVALTYPE\ algorithm.  In this paper, we greatly simplify the computation and provide a complexity bound for the \EVAL\ algorithm.

We call the function  mentioned in the previous paragraph a {\em stopping function} for the algorithm.  Similar functions also appeared in \citep{Henrici:search:70} where they were called {\em inner} and {\em outer convergence} functions.  In \citep{Yakoubsohn:bisection:05} such functions were also termed {\em exclusion functions}.  In both cases, these stopping functions were used to compute the complexity of the algorithm, but they were not used in a continuous amortization computation.

\subsection{Other root isolation algorithms}\label{Intro:RootsFinding}
There is an extensive amount of literature on the complexity of root isolation, see \citep{pan:history-progress:97,Pan1996} for surveys of the previous literature, which we will not attempt to cover here.  Most algorithms are compared by their performance on the benchmark problem of finding all real roots of a polynomial of degree  and whose coefficients can be represented by at most  bits.  For this problem, the bit-complexity of  for complex roots
was first achieved by Sch\"onhage \citep{schonhage:fundamental}.  In many algorithms, the size of the subdivision tree is smaller than this bound because, for each node in the subdivision tree, additional calculations must be performed.  Davenport \citep{davenport:85} proved that the the subdivision tree for the Sturm method is , see \citep{reischert:subresultant:97,lickteig-roy:sequences:01,du-sharma-yap:sturm:07}.  More recently, it has been shown in \citep{eigenwillig-sharma-yap:descartes:06} that the Descartes method also achieves this bound, see \citep{collins-akritas:76,eigenwillig-sharma-yap:descartes:06,krandick-mehlhorn:06,collins-johnson-krandick:cad:02}.  These methods are optimal under the weak assumption that .  In addition, related exact techniques using continued fractions were shown to have a tree size of  when an ideal root bound is used and  when a more practical bound is used \citep{sharma:2008}. In the algebraic computing community, the Descartes method appears to be one of the more practical algorithms, see \citep{collins-johnson-krandick:cad:02,johnson:root-isolation:98,rouillier-zimmermann:roots:04,mrr:bernstein:05,rouillier-zimmermann:roots:04}.  In this paper, we show that the subdivision tree for the \EVAL\ algorithm also achieves this bound; therefore, the \EVAL\ algorithm should also be considered on equal footing with the other more well-known root finding algorithms via the Sturm or Descartes methods.  The \EVAL\ algorithm may, in addition, be considered practical because its computations are numerical and hence easy to implement and its subdivision tree has a favorable size.

\subsection{Organization of this paper}
In Section \ref{Sec:EVAL}, we introduce the \EVAL\ algorithm and discuss the main condition we will use for an interval to be \EVAL\ terminal.  In Section \ref{Sec:Integral}, we review the use of stopping functions to bound the size of the subdivision from \citep{Burr-Krahmer-Yap:integral:09} and create a stopping function for the \EVAL\ algorithm. In Section \ref{Sec:Analysis}, we compute the size of the \EVAL\ algorithm's subdivision tree using continuous amortization via the stopping function technique and achieve the main result of this paper, the  bound on the size of the subdivision tree for the \EVAL\ algorithm.  Finally, we conclude in Section \ref{Conc}.

The authors would like to thank the following people for many useful discussions: Benjamin Galehouse, Michael Sagraloff, and Chee Yap.

\section{The \EVAL\ algorithm}\label{Sec:EVAL}
Given an interval  with integer endpoints and a polynomial  with integer coefficients, i.e., , the \EVAL\ algorithm returns a collection of intervals which cover and isolate the real roots of  in , i.e., every root appears in an output interval and each output interval contains exactly one root (ignoring multiplicities).  In the \EVAL\ algorithm, if the interval  is output, then  contains exactly one root of  and if  is output, then  is a root of .  The \EVAL\ algorithm maintains a (finite) partition  of the interval , i.e., a finite collection of intervals whose interiors are disjoint and whose union is .  The \EVAL\ algorithm iteratively bisects the elements of  until the intervals of the partition  are each small enough to pass the \EVAL\ termination conditions (see Section \ref{Subsec:EVAL}).  Of interest to us is the size  of the partition, i.e., the number of intervals in .

We begin with some terminology: For an interval  the {\em width} of  is  and the {\em midpoint} of  is .  Also, to {\em bisect} an element of the partition  means to replace the interval  by the two subintervals  and .  Note that this implies that  is one more than the number of bisections done by the \EVAL\ algorithm, i.e., the size of the subdivision tree. All of the calculations done by the \EVAL\ algorithm will be performed on the dyadic integers  so that all of the standard operations are exact.  This prevents well-known implementation errors from arising in practice.
\subsection{Statement of the \EVAL\ algorithm}\label{Subsec:EVAL}
In the \EVAL\ algorithm, we first replace  by its square free component, which we briefly call .  Then, we replace  by its square free and relatively prime to  component, i.e., we first take the square free component of  and then take the portion of this polynomial which is relatively prime to .  We briefly call this .  Note that  and , and, moreover, the roots of  are separated by roots of  by Rolle's theorem.  In the case where  is square free, the zeros of  partition  into monotonic regions; in the case where  is not square free, the zeros of  no longer have this property, but they still partition the roots of  (and hence the roots of ).  Throughout the remainder of this paper, except for a brief note in Section \ref{subsec:EVALbounds}, we use these square free substitutions for  and  without mention.  The bounds on the subdivision tree, however, will be in terms of the data for original the  and not for any replacements.

The \EVAL\ algorithm creates a partition of  and determines which intervals in the partition contain roots.  Initially, the partition of  is , the trivial partition.\.15cm]
\hspace*{20pt} \textnormal{()}\qquad or \\
\hspace*{20pt} \textnormal{()}\qquad\.15cm]
For each interval  where \textnormal{} holds and , output 
}}\\

The termination proof for the \EVAL\ algorithm is very similar to the corresponding statement in \citep{Burr-Krahmer-Yap:integral:09,sagraloff-yap:ceval:09}.  The correctness proof is slightly different from the corresponding proofs for other \EVALTYPE\ algorithms.  The correctness follows from the Taylor polynomial centered at : if one of the conditions holds, then it follows that  (for condition ) or  (for condition ) is never zero in  since the inequalities are equivalent to a reverse triangle inequality on the Taylor polynomial.  The first condition implies that  has no zeros in .  The second condition implies that  has at most one zero in  since roots of  separate zeros of  (even though  might not be monotonic due to the replacements above).

\subsection{\EVAL\ terminal intervals}\label{sec:evaltermint}
In this section, we provide a sufficient condition for the \EVAL\ algorithm to terminate without subdividing on a given interval, i.e., for the interval to be \EVAL\ terminal.

\begin{definition}
For any polynomial  of degree , define  to be the multiset of the roots of . In addition, define the function  to be the sum of the reciprocals of the distances from its argument to the roots of :

Note that this function can be represented in a simple form using the harmonic mean .  Then, one has

where  is the set of distances from  to the roots of .
\end{definition}
 and  will be our main objects of study.  We begin with the following lemma which connects  and  with conditions  and , respectively:
\begin{lemma}\label{lem:Sigma}
The following inequality holds for :

The proof is a straight-forward computation.  See the proof of \citep[Lemma 6.2]{Burr-Krahmer-Yap:integral:09} or \citep[Section 5.2]{sagraloff-yap:ceval:09} for details.
\end{lemma}

We use this lemma to show that a simple upper bound on the width of an interval will ensure that the conditions in the \EVAL\ algorithm hold. For example, in condition , divide both sides of the inequality by  and apply Lemma \ref{lem:Sigma} to derive the following inequality:

If , then the sum on the RHS is bounded above by a geometric series with , and, therefore, the sum is bounded by 1.  This implies that condition  holds.  Therefore, the condition  is sufficient to ensure that  is \EVAL\ terminal.  Similarly, if , then  is \EVAL\ terminal by condition .

\section{Stopping functions}\label{Sec:Integral}
In this section, we show how stopping functions can be used to compute the size of the subdivision tree of the \EVAL\ algorithm.  The construction in Section \ref{sec:BasicProperties} was originally presented in \citep{Burr-Krahmer-Yap:integral:09}, but we include it here for completeness and because the construction in Section \ref{sec:stopeval} requires a detailed understanding of the method.

\subsection{Basic properties}\label{sec:BasicProperties}
The use of stopping functions promises to be an important tool for bounding the complexity of subdivision algorithms.  Most of the numerical algorithms appearing in the introduction may benefit from this type of analysis; more algorithms of this type are mentioned in the Conclusion, Section \ref{Conc}.  We begin by formulating an abstract algorithm called the \GEN\ algorithm, which is intended to be the prototype of these types of algorithms in one dimension.  The notion of stopping functions and the \GEN\ algorithm both easily generalize to higher dimensions.

Fix a predicate  (i.e., a Boolean function) on intervals with the following property: if  and  is true, then  is also true.  The \GEN\ algorithm is the following algorithm: given an interval , the algorithm maintains a partition  of .  Initially, let the partition be the trivial partition  and let  be the final partition.\.15cm]
\hspace*{40pt}  is true
}}\\

A {\em stopping function} for the \GEN\ algorithm with predicate  is a real-valued function  with the following property: if, for a given interval , there exists a point  such that , then  is true.  The following theorem, which also appears as \citep[Theorem 3.5]{Burr-Krahmer-Yap:integral:09}, bounds the number of subdivisions performed by the \GEN\ algorithm.

\begin{theorem}\cite[Theorem 3.5]{Burr-Krahmer-Yap:integral:09}\label{thm:integral}
Let  be a stopping function for the \GEN\ algorithm, then

If the \GEN\ algorithm does not terminate, then the integral is infinite.
\begin{proof}
If , then the bound is immediate.  If , then an examination of the \GEN\ algorithm shows that for  there is a lower bound on  since the \GEN\ did not terminate at the parent of :

In addition, , and it, therefore, suffices to show that for every , .  Let  be such that  is maximal in .  Then

In the case when the \GEN\ algorithm does not terminate, we can look at the partition  at any moment in time.  The above argument shows that  is still bounded by the integral .  Since  can be chosen to be arbitrarily large, this shows that the integral is unbounded.
\end{proof}
\end{theorem}

\subsection{A stopping function for \EVAL}\label{sec:stopeval}
The next goal is to transform the inequality  into a stopping function.  Currently, it is not a stopping function because the function on the RHS is not for an arbitrary point of , but for a specific point, the midpoint.  We begin to turn this into a stopping function via the following lemma:

\begin{lemma}\label{lem:HM}
Let  with  and  such that  and  for all .  Then

\begin{proof}
For Inequality (\ref{Eq:HM1}), we expand each of the harmonic means and get the following equivalent inequality:

Noting that all of the denominators are positive, clearing fractions gives that this inequality is equivalent to the following inequality:

This inequality is easily justified by combining similar terms on the RHS to obtain a sum with general term

which is a term that appears on the LHS.  Since the remaining terms on the LHS are positive, this proves the first inequality.

For Inequality (\ref{Eq:HM2}), we expand the harmonic mean to get the following equivalent inequality:

Once again, the denominator is positive, so by clearing fractions we have that this inequality is equivalent to the following inequality:

Since all of the terms on the RHS are positive and include the term on the LHS, this proves the second inequality.
\end{proof}
\end{lemma}

Let , then  is a stopping function for EVAL: Let  be an interval such that  contains  and let  be the midpoint of ; then, .  Assume now that .  Then, inequality (\ref{Eq:HM2}) in Lemma \ref{lem:HM} implies that  for all .  This setup implies the following inequalities:

The second inequality follows from Lemma \ref{lem:HM} and the fact that the terms of the harmonic mean  are all positive (because of the bound on  above).  The remaining inequalities follow from the monotonicity of the harmonic mean.  The last inequality also uses that  by the triangle inequality.  When combined with the observations from Section \ref{sec:evaltermint}, this implies that  is \EVAL\ terminal.  Similarly, let  where  is the corresponding function for , then  is also a stopping function for the \EVAL\ algorithm.  Finally, let , then  is an everywhere positive stopping function for the \EVAL\ algorithm.

\section{Size of the subdivision tree of the \EVAL\ algorithm for the benchmark problem}\label{Sec:Analysis}
In this section, we prove that the size of the subdivision tree of the \EVAL\ algorithm is  where  is the number of bits needed to write the coefficients of .  In this case, the absolute value of all the roots is bounded by  \citep{yap:algebra:bk} (this bound comes from the original , not from the square free substitution).  Hence, we can assume wlog that .  By Theorem \ref{thm:integral}, the complexity of the \EVAL\ algorithm is bounded by .  The crossover points of  are difficult to determine, however, so we replace this integral by a slightly larger one which is easier to evaluate: For any , let  be the set of roots in  which are closest to .  Similarly, for , let  be the set of  such that no other root in  is closer to  than .  Note that  iff  and that two of the 's are either disjoint (except for endpoints) or coincide (in the case of complex conjugates).  Therefore, these 's determine a partition of .  Also, let  be the set of endpoints of the 's; then, for all points , one has  or  because  and  do not share roots.  We define another function :

Note that although  might not correspond to the crossover points of , pointwise,  since  is a maximum of the terms which can occur in .  This implies the following inequalities:

For the second inequality let , then  is either closest to a root of  or a root of . If  is closest to a root of , then  and .  In this case, the sum to the right of the inequality includes all of the roots in  as well as some roots in .  Thus, at least all of the terms of  appear on the RHS of the inequality.  The case where  is closest to a root of  is similar.  This implies the inequality because the set of points for which this inequality may fail is a measure zero subset of .

\subsection{Evaluating the integrals}
Consider the shape of each of the regions where we integrate: since all of the integrals are of the form , we evaluate a general integral of this form where  and  lie on the same side of .
\begin{itemize}
\item In the case where  is real:

These logarithms will be bounded in the next section.
\item In the case where  is not real:

This is now bounded via the relationship between  and .  If , then:

If , then the computation is similar, and the integral is bounded above by .  These logarithms will also be bounded in the next section.
\end{itemize}

\subsection{Finishing the bound on the \EVAL\ algorithm}\label{subsec:EVALbounds}
In this section, we use the computation from the previous section to prove the main result of this paper.  To do this, we consider the roots  with two different cases depending on if  is real or not.
\begin{itemize}
\item If  is real; then  and let .  Then, the term corresponding to  in the RHS of Inequality (\ref{eq:evaluated}) consists of .  Note that integrals may be zero which happens when  or .  Then, using the bounds derived in the preceding section on these integrals, it follows that they are bounded by:
    
    The positive terms are bounded by  (the leading term is ) and for the negative terms, note that  and  are points which are equidistant from  and another root of , e.g.,  is equidistant from  and  where  is the interval immediately to the left of .  Then,  is bounded below by the logarithm of half the distance from  to .
\item If  is not real, then the term corresponding  in the RHS of Inequality (\ref{eq:evaluated}) consists of  which is bounded above by .  By splitting this integral at , the integral is equal to . Using the bounds derived in the preceding section on these integrals, it follows that these integrals are bounded by:
    
    The positive terms are bounded by  (the leading term is ) and for the negative terms, note that , which is the logarithm of half the distance between  and .
\end{itemize}
Combining all of the 's which appear in the integrals results in a bound of  (the leading term is ).  The sum of the logarithmic distances between roots are bounded simultaneously via the standard Mahler-Davenport lower bound on distances between roots, see \citep{davenport:85,du-sharma-yap:sturm:07,eigenwillig-sharma-yap:descartes:06}.  To do this, we construct a directed graph whose nodes are the roots in  and whose edges represent the logarithms which must be calculated.  In this graph, the edges satisfy the conditions of the Mahler-Davenport bound and are chosen so that the in-degree of any node is at most 2.  For each pair of complex roots, , connect them with two directed edges, one from  to , the other in the opposite direction.  On the other hand, if  is real, then let  be a root where  lies immediately to the right of  and  be a root where  lies immediately to the left of  (provided  is not the rightmost or leftmost interval in the partition, respectively).  Those of  or  which are real are connected to  so that the arrow points in the direction of decreasing absolute value.  If  is not real, then connect  to either  or , whichever has {\em positive} imaginary component.  On the other hand, if  is not real, then connect  to either  or , whichever has {\em negative} imaginary component.  Again, these edges are directed so that the arrow points in the direction of decreasing absolute value.  By inspection, we find that the maximum in-degree of this directed graph is .  The Mahler-Davenport bound can then be applied twice to find the result.  The bound implies that the sum of the negative logarithmic distances between the roots appearing in this construction is bounded above by:

The discriminant will be an integer and therefore the discriminant term is bounded above by 1.  The Mahler measure of  is bounded in terms of the -norms of the coefficients of the original  and : .  Therefore, this portion is bounded by  (the leading term is bounded ).  Thus, the complexity of the \EVAL\ algorithm is  (the leading term is bounded ).

If  or  was replaced by a square free version, we used the original  and  because the square free versions of  and  divide the original functions, and, therefore, the Mahler measure of the product of the square free versions is bounded above by .  In fact, the -norms of the coefficients of the original functions are often smaller than the -norms of the square free versions.
\section{Conclusion}\label{Conc}
In this paper, we provided a complexity analysis of the \EVAL\ algorithm and showed it to be optimal under the weak assumption that .  To accomplish this, we used the novel technique of continuous amortization through stopping functions.  The simplicity of this argument exhibits the utility of this technique: the proof of the next closest complexity bound for an \EVALTYPE\ algorithm in \citep{sagraloff-yap:ceval:09} is significantly more complex.

The \EVAL\ algorithm is very easy to implement \citep{moore:bk,mitchell:robust-ray:90,plantinga-vegter:isotopic:04,plantinga:thesis:06,kamath:subdivision:10} and it now joins the Sturm and Descartes methods by having a subdivision tree which grows at the rate .  It, therefore, may become more prevalent in practical situations because it has several desirable properties.  This also answers a question raised in \citep{Henrici:search:70} concerning the good behavior of this technique.

In addition, the continuous amortization technique can be used to bound the number of subdivisions over any interval, and, therefore, may find many more applications for different types of questions about subdivision algorithms.  For example, in many practical applications, the question is to find the roots in a given domain, not just for the benchmark domain; continuous amortization may provide a comparison of different algorithms in these situations.

We close with some continuing research and questions:
\begin{itemize}
\item The algorithm for finding complex roots appearing in \citep{sagraloff-yap:ceval:09} is very similar to the \EVAL\ algorithm. We are currently preparing a simplification of their work using the results from this paper.
\item There are many bisection algorithms where continuous amortization may be useful, see, for example, \citep{Henrici:search:70,Yakoubsohn:bisection:05,sagraloff-yap:ceval:09, plantinga-vegter:isotopic:04,plantinga:thesis:06,snyder:interval:92, galehouse:thesis,burr+3:subdiv2:10,eigenwillig-sharma-yap:descartes:06, du-sharma-yap:sturm:07,linyap2009}.  We plan on extending our techniques to these cases.  In particular, stopping functions which are appropriate for the two dimensional cases treated in \citep{plantinga-vegter:isotopic:04,plantinga:thesis:06,galehouse:thesis} would be very useful because current techniques have not been fruitful in establishing complexity bounds of these algorithms.
\item If  was not square free, then the test for condition  in Algorithm~2.1 is based on the square free part of , not the original function.  The \EVAL\ algorithm, however, will continue to terminate and be correct even when this substitution does not occur, i.e., when the original  is used.  For this reason, it is likely that the above substitution is extraneous.  For example, in the simplest cases where  is not square free and the integral in Inequality (\ref{eq:evaluated}) can be calculated by hand, the result is .
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{Subdivision}

\end{document}
