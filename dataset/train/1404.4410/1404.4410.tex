\documentclass[runningheds]{llncs}

\usepackage{amsfonts}
\usepackage{amsbsy,amssymb,amsmath}
\usepackage{todonotes}
\usepackage{latexsym}
\usepackage{url}
\usepackage{listings}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{color}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,calc,shapes.geometric}
\usepackage{caption}
\usepackage{subcaption}


\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\myRightarrow}{\; \Rightarrow \;}
\newcommand{\fn}[1]{\mathtt{#1}} \newcommand{\nroot}{\fn{root}}
\newcommand{\abs}{\fn{abs}}
\newcommand{\minm}{\fn{min}}



\pdfpageattr {/Group << /S /Transparency /I true /CS /DeviceRGB>>}

\hypersetup{
    colorlinks=true,
    citecolor=blue,
    linkcolor=blue,
    urlcolor=blue
}

\tikzset{
>=stealth',
module/.style={
           rectangle,
           draw=black, very thick,
           text width=7em,
           minimum height=2em,
           text centered},
interface/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}


\begin{document}


\title{A heuristic prover for real inequalities}



\author{Jeremy Avigad\inst{1} \and Robert Y. Lewis\inst{1} \and Cody Roux\inst{2}}


\institute{Carnegie Mellon University, Pittsburgh, PA 15213, USA \and Draper Laboratories, Cambridge, MA 02139, USA}


\maketitle


\begin{abstract}
We describe a general method for verifying inequalities between real-valued expressions, especially the kinds of straightforward inferences that arise in interactive theorem proving. In contrast to approaches that aim to be complete with respect to a particular language or class of formulas, our method establishes claims that require heterogeneous forms of reasoning, relying on a Nelson-Oppen-style architecture in which special-purpose modules collaborate and share information. The framework is thus modular and extensible. A prototype implementation shows that the method works well on a variety of examples, and complements techniques that are used by contemporary interactive provers.
\end{abstract}


\section{Introduction}
\label{section:introduction}

Comparing measurements is fundamental to the sciences, and so it is not surprising that ordering, bounding, and optimizing real-valued expressions is central to mathematics. A host of computational methods have been developed to support such reasoning, using symbolic or numeric methods, or both. For example, there are well-developed methods of determining the satisfiability or unsatisfiability of linear inequalities \cite{pugh:92} \cite{schrijver:86}, polynomial inequalities \cite{basu:et:al:03}, nonlinear inequalities involving functions that can be approximated numerically \cite{gao:et:al:12} \cite{moore:et:al:09}, and inequalities involving convex functions \cite{boyd:vandenberghe:04}. The ``satisfiability modulo theories'' framework \cite{barrett:et:al:08} \cite{nelson:oppen:79} provides one way of integrating such methods with ordinary logical reasoning and proof search; integration with resolution theorem proving methods has also been explored \cite{akbarpour:paulson:08} \cite{prevosto:waldmann:06}. Interactive theorem provers like Isabelle \cite{nipkow:et:al:02} and HOL Light \cite{harrison:07c} now incorporate various such methods, either constructing correctness proofs along the way, or reconstructing them from appropriate certificates. (For a small sample, see \cite{blanchette:et:al:11} \cite{chaieb:nipkow:08} \cite{harrison:07b} \cite{mclaughlin:harrison:05}.)
Such systems provide powerful tools to support interactive theorem proving. But, frustratingly, they often fail when it comes to fairly routine calculations, leaving users to carry out explicit calculations painstakingly by hand. Consider, for example, the following valid implication:

The inference is not contained in linear arithmetic or even the theory of real-closed fields. The inference is tight, so symbolic or numeric approximations to the exponential function are of no use. Backchaining using monotonicity properties of addition, multiplication, and exponentiation might suggest reducing the goal to subgoals  and , but this introduces some unsettling nondeterminism. After all, one could just as well reduce the goal to
\begin{itemize}
\item  and , or
\item  and , or even
\item  and .
\end{itemize}
And yet, the inference is entirely straightforward. With the hypothesis  in mind, you probably noticed right away that the terms  and  can be compared; similarly, the comparison between  and  leads to comparisons between  and , then  and , and so on. 

The method we propose is based on such heuristically guided forward reasoning, using properties of addition, multiplication, and the function symbols involved. As is common for resolution theorem proving, we try to establish the theorem above by negating the conclusion and deriving a contradiction. We then proceed as follows:
\begin{itemize}
 \item Put all terms involved into a canonical normal form. This enables us to recognize terms that are the same up to a scalar multiple, and up to associativity and commutativity of addition and multiplication.
 \item Iteratively call specialized modules to learn new comparisons between subterms, and add these new comparisons to a common ``blackboard'' structure, which can be accessed by all modules.
\end{itemize}
The theorem is verified when any given module derives a contradiction using this common information. The procedure fails when none of the modules can learn anything new. We will see in Section~\ref{section:examples} that the method is far from complete, and may not even terminate. On the other hand, it is flexible and extensible, and easily verifies a number of inferences that are not obtained using more principled methods. As a result, it provides a useful complement to more conventional approaches.

We have designed and implemented modules to learn comparisons from the additive and multiplicative structure of terms, a module to instantiate axioms involving arbitrary functions symbols, and special-purpose modules for common functions like min, max, absolute value, exp, and log. The additive and multiplicative modules have two different implementations, with different characteristic strengths and weaknesses. The first uses a natural but naive Fourier-Motzkin elimination, and the second uses more refined geometric techniques. Our prototype implementation, written in Python, is available online:
\begin{quote}
 \url{https://github.com/avigad/polya}
\end{quote}
We have named the system ``Polya,'' after George P\'olya, in recognition of his work on inequalities as well as his thoughtful studies of heuristic methods in mathematics (e.g.~\cite{hardy:littlewood:88} \cite{polya:04}).

The general idea of deriving inequalities by putting terms in a normal form and combining specialized modules is found in Avigad and Friedman \cite{avigad:friedman:06}, which examines what happens when the additive and multiplicative fragments of real arithmetic are combined. This is analogous to the situation handled by SMT solvers, with the added twist that the languages in question share inequality symbols and multiplication by constant coefficients in addition to the equality symbol. Avigad and Friedman show that the universal fragment remains decidable even if both theories include multiplication by rational constants, while the full first-order theory is undecidable. The former decidability result, however, is entirely impractical, for reasons discussed there. Rather, it is the general framework for combining decision procedures and the use of canonical normal forms that we make use of here.

The outline of the paper is as follows. In Section~\ref{section:framework}, we describe the general blackboard architecture which is the shared interface for the different modules, and the canonical form for terms. In Section~\ref{section:fourier:motzkin}, we describe the implementation of the additive and multiplicative modules based on the Fourier-Motzkin algorithm, whereas in Section~\ref{section:geometric} we describe the implementation based on existing tools from discrete geometry. In Section~\ref{section:functions}, we describe a module that instantiates general axioms, and in Section~\ref{section:other:modules} we describe more specialized modules that contribute information to the blackboard. In Section~\ref{section:examples}, we provide a number of examples that help characterize the method's strengths and weaknesses. Finally, in Section~\ref{section:conclusions}, we discuss some of the many ways that the method can be extended, as well as ways in which the implementation may be improved.

This paper is a revised and expanded version of the conference paper \cite{avigad:lewis:roux:14}. The extensions described in this paper, chiefly the additional modules described in Section~\ref{section:other:modules}, are due to Avigad and Lewis. More detailed descriptions of some of the representations and algorithms can be found in Lewis' MS thesis \cite{lewis:14}.


\section{The Framework}
\label{section:framework}

\subsection{Terms and Canonical Forms}
\label{subsection:terms}

We wish to consider terms, such as , that are built up from variables and rational constants using addition, multiplication, integer powers, and function application. To account for the associativity of addition and multiplication, we view sums and products as multi-arity rather than binary operations. We account for  commutativity by imposing an arbitrary ordering on terms, and ordering the arguments accordingly.

Importantly, we would also like to easily identify the relationship between terms  and  where , for a nonzero rational constant . For example, we would like to keep track of the fact that  is twice . Towards that end, we distinguish between ``terms'' and ``scaled terms'': a scaled term is just an expression of the form , where  is a term and  is a rational constant. We refer to ``scaled terms'' as ``s-terms'' for brevity.

\begin{definition}
  We define the set of \emph{terms}  and \emph{
    s-terms}  by mutual recursion:

Here  ranges over a set of \emph{variables},  ranges over a set of \emph{function symbols}, , and .
\end{definition}
Thus we view  as an s-term of the form , where  is the product ,  is a sum of three s-terms, and  is the result of applying  to the single s-term . 

Note that there is an ambiguity, in that we can also view the coefficient  as the s-term . This ambiguity will be eliminated when we define a notion of \emph{normal form} for terms. The notion extends to s-terms: an s-term is in normal form when it is of the form , where  is a term in normal form. (In the special case where , we require  to be the term .) We also refer to terms in normal form as \emph{canonical}, and similarly for s-terms.

To define the notion of normal form for terms, we fix an ordering  on variables and function symbols, and extend that to an ordering on terms and s-terms. For example, we can arbitrarily set the term  to be minimal in the ordering, then variables, then products, then sums, and finally function applications, recursively using lexicographic ordering on the list of arguments (and the function symbol) within the latter three categories. The set of terms in normal form is then defined inductively as follows:
\begin{itemize}
 \item  are terms in normal form.
 \item  is in normal form provided , each  is in normal form, and .
 \item  is in normal form provided each  is in normal form, and .
 \item  is in normal form if each  is.
\end{itemize}
The details are spelled out in Avigad and Friedman \cite{avigad:friedman:06}. That paper provides an explicit first-order theory, , expressing commutativity and associativity of addition and multiplication, distributivity of constants over sums, and so on, such that the following two properties hold:
\begin{enumerate}
 \item For every term , there is a unique s-term  in canonical form, such that  proves .
 \item Two terms  and  have the same canonical normal form if and only if  proves .
\end{enumerate}
The results can be straightforwardly extended to terms with arbitrary integer exponents. For example, the term  is expressed canonically as , where the constant in the additive term  has been factored so that the result is in normal form. 

The semantics we have chosen for expressions  when  is negative or zero is that such an expression is assumed to denote a real number, but in case  is  we make no further assumptions about the value of . Thus, for example, we do not combine exponents when putting  into canonical form, though  is reduced to . We leave it to the multiplicative module to deal with negative exponents appropriately when the base is known to be nonzero.

The two clauses above provide an axiomatic characterization of what it means for terms to have the same canonical form. As discussed in Section~\ref{section:conclusions}, extending the reach of our methods requires extending the notion of a canonical form to include additional common operations. 


\subsection{The Blackboard}
\label{subsection:blackboard}

We now turn to the blackboard architecture, which allows modules to share information in a common language. To the addition module, multiplication is a black box; thus it can only make sense of additive information in the shared pool of knowledge. Conversely, the multiplication module cannot make sense of addition. But both modules can make sense of information in the form , where  and  are subterms occurring in the problem. The blackboard enables modules to communicate facts of this shape. 

When the user asserts a comparison  to the blackboard,  is first put in canonical form, and names  are introduced for each subterm. It is convenient to assume that  denotes the canonical term . Given the example in the last section, the method could go on to define

In that case,  represents . Any subterm common to more than one term is represented by the same name. Separating terms in this way ensures that each module can focus on only those definitions that are meaningful to it, and otherwise treat subterms as uninterpreted constants.

Now any comparison  between canonical s-terms, where  denotes any of , or , translates to a comparison , where  and  name canonical terms. But this, in turn, can always be expressed in one of the following ways:
\begin{itemize}
 \item  or , or
 \item , where  and . 
\end{itemize}
The blackboard therefore maintains the following data:
\begin{itemize}
 \item a defining equation for each , and
 \item comparisons between named terms, as above.
\end{itemize}
Note that this means that, \emph{a priori}, modules can only look for and report comparisons between terms that have been ``declared'' to the blackboard. This is a central feature of our method: the search is deliberately constrained to focus on a small number of terms of interest. The architecture is flexible enough, however, that modules can heuristically expand that list of terms at any point in the search. For example, our addition and multiplication modules do not consider distributivity of multiplication over addition, beyond multiplication of rational scalars. But if a term  appears in the problem, a module could heuristically add the identity , adding names for the new terms as needed.

\begin{figure}[h]
\centering
\setlength\fboxsep{3pt}
\setlength\fboxrule{2pt}
{\begin{tikzpicture}[auto, scale=0.5, every node/.style={scale=.75}, node distance=2]
    \node (bb) at (0,2) [rectangle, draw] {\parbox[c][2cm][c]{3cm}{\centering {\bf Blackboard} \\ {\footnotesize Stores definitions and comparisons}} };\node (lin) [above left=of bb, rectangle, draw] {\parbox[c][1.5cm][c]{4cm}{\centering {\bf Additive Module} \\ {\footnotesize Derives comparisons using additive definitions}}};
  \node (nonlin) [above right=of bb, rectangle, draw] {\parbox[c][1.5cm][c]{4cm}{\centering {\bf Multiplicative Module} \\ {\footnotesize Derives comparisons using multiplicative definitions}}};
  \node (ax) [below=of bb, rectangle, draw] {\parbox[c][1.5cm][c]{5cm}{\centering {\bf Axiom Instantiation Module} \\ {\footnotesize Derives comparisons using universal axioms}}};
  \node (exp) [below left=of bb, rectangle, draw] {\parbox[c][1.5cm][c]{4cm}{\centering {\bf Exp/Log Module} \\ {\footnotesize Derives comparisons and axioms involving exp and log}}};
  \node (min) [right=of bb, rectangle, draw] {\parbox[c][2cm][c]{3cm}{\centering {\bf Min/Max Module} \\ {\footnotesize Derives comparisons involving min and max}}};
  \node (cc) [left=of bb, rectangle, draw] {\parbox[c][2cm][c]{3cm}{\centering {\bf Congruence Closure Module} \\ {\footnotesize Enforces proper interpretation of functions}}};
  \node (abs) [below right=of bb, rectangle, draw] {\parbox[c][1.5cm][c]{4cm}{\centering {\bf Absolute Value Module} \\ {\footnotesize Derives comparisons and axioms involving abs}}};
  \node (root) [above=of bb, rectangle, draw] {\parbox[c][1.5cm][c]{5cm}{\centering {\bf th Root Module} \\ {\footnotesize Derives comparisons and axioms about fractional exponents}}};
  \draw[->] (bb) [bend left=20] to node {} (lin);
  \draw[->] (bb) [bend left=20] to node {} (nonlin);
  \draw[->] (bb) [bend left=20] to node {} (ax);
  \draw[->] (bb) [bend left=20] to node {} (root);
  \draw[->] (bb) [bend left=20] to node {} (exp);
  \draw[->] (bb) [bend left=5] to node {} (min);
  \draw[->] (bb) [bend left=5] to node {} (cc);
  \draw[->] (bb) [bend left=20] to node {} (abs);
  \draw[->] (lin) [bend left=20] to node {} (bb);
  \draw[->] (nonlin) [bend left=20] to node {} (bb);
  \draw[->] (ax) to [bend left=20] node {} (bb);
  \draw[->] (root) to [bend left=20] node {} (bb);
  \draw[->] (exp) to [bend left=20] node {} (bb);
  \draw[->] (min) to [bend left=5] node {} (bb);
  \draw[->] (cc) to [bend left=5] node {} (bb);
  \draw[->] (abs) to [bend left=20] node {} (bb);
  
  
  \draw[->] (abs) to  node {} (ax);
  \draw[->] (exp) to node {} (ax);
  \draw[-|] (root) to node {} (bb);
  \draw[|->] (bb) to node {} (ax);
\end{tikzpicture}}\caption{The computational structure.}
\label{fig:bbarch}
\end{figure}



To verify an implication, the user asserts the hypotheses to the blackboard, together with the negation of the conclusion. Individual modules then take turns learning new comparisons from the data, and asserting them to the blackboard as well, until a contradiction is obtained, or no further conclusions can be drawn. The setup is illustrated by Figure \ref{fig:bbarch}. Notice that this is essentially the Nelson-Oppen architecture \cite{barrett:et:al:08} \cite{nelson:oppen:79}, in which (disjoint) theories communicate by means of a shared logical symbol, typically equality. Here, the shared language is instead assumed to contain the list of comparisons , and multiplication by rational constants.

Now suppose a module asserts an inequality like  to the blackboard. It is the task of the central blackboard module to check whether the assertion provides new information, and, if so, to update its database accordingly. The task is not entirely straightforward: for example, the blackboard may already contain the inequality , but absent sign information on  or , this does not imply , nor does the converse hold. However, if the blackboard includes the inequalities  and , the new assertion is redundant. If, instead, the blackboard includes the inequalities  and , the new inequality should replace the second of these. A moment's reflection shows that at most two such inequalities need to be stored for each pair  and  (geometrically, each represents a half-plane through the origin), but comparisons between  or  and  should be counted among these.

There are additional subtleties: a weak inequality such as  paired with a disequality  results in a strong inequality; a pair of weak inequalities  and  should be replaced by an equality; and, conversely, a new equality can subsume previously known inequalities. The interactions, while not conceptually difficult, are intricate, and care is needed to get the details right.

Below, we will sometimes refer to the terms  as the ``problem terms,'' that is, the terms that are registered with the blackboard as objects of comparison.

\subsection{An alternative representation of comparisons}
\label{subsection:alternative:representation}

For each pair of problem terms  and , we have noted that the blackboard stores the strongest comparison(s) known to hold between them. Sometimes another representation of this information is useful: we can ask for the range of  such that  is known to hold, and the values of  for which the inequality is strict, as well as the dual questions with  replaced by . A moment's reflection shows that if  and , then  for every value  between  and . Thus, the range of coefficients  for which such a comparison is known form a closed interval, of one of the forms , , , or . Moreover, the comparison can be weak or strict at each finite endpoint, as well as weak or strict in the interior. These possibilities are not entirely independent: for example, if the comparison is strict at either endpoint, it will be strict in the interior.

The blackboard's methods are capable of returning such a representation of the comparisons that hold between  and , in both the  and  directions. More detail can be found in \cite{lewis:14}. This representation is currently used by the minimum module, described in Section~\ref{subsection:minimum}. 



\section{Fourier-Motzkin}
\label{section:fourier:motzkin}

The Fourier-Motzkin algorithm \cite{schrijver:86} is a quantifier-elimination procedure for the theory of the structure , that is, the real numbers as an additive ordered group. Nothing changes essentially if we add to the language of that theory the constant  and scalar multiplication by , for each rational . Here we see that the method can be used to infer comparisons between variables from additive data, and that this can be transported to the multiplicative setting as well.

\subsection{The Fourier-Motzkin Additive Module}
\label{subsection:fm:additive}

The Fourier-Motzkin additive module begins with the comparisons  stored in the blackboard, where  is one of  (disequalities are not used). It also makes use of comparisons , and all definitions  in which the right-hand side is a sum. The goal is to learn new comparisons of the form  or . The idea is simple: to learn comparisons between  and , we need only eliminate all the other variables. 
For example, suppose, after substituting equations, we have the following three inequalities:

Eliminating  from the first two equations we obtain , from which we can conclude . Eliminating  from the last two equations we obtain , from which we can conclude . More generally, eliminating all the variables other than  and  gives the projection of the convex region determined by the constraints onto the  plane, which determines the strongest comparisons for  and  that are implied by the data.

Constants can be represented using the special variable , which
can be treated as any other variable. Thus eliminating all variables except for  and  yields all comparisons between  and a constant.





The additive module simply carries out the elimination for each pair
, . In general, Fourier-Motzkin elimination can
require doubly-exponential time in the number of variables.  With a bit of cleverness, one can use previous eliminations to save some work, but for a problem with  subterms, one is still left with -many instances of Fourier-Motzkin with up to  variables in each. 
It is interesting to note that for the examples described in
Section~\ref{section:examples}, the algorithm performs reasonably
well. In Section~\ref{section:geometric}, however, we describe a more
efficient approach.


\subsection{The Fourier-Motzkin Multiplicative Module}
\label{subsection:fm:multiplicative}

The Fourier-Motzkin multiplication module works analogously: given comparisons  or  and definitions of the form , the module aims to learn comparisons of the first two forms. The use of Fourier-Motzkin here is based on the observation that the structure  is isomorphic to the structure  under the map . With some translation, the usual procedure works to eliminate variables in the multiplicative setting as well. In the multiplicative setting, however, several new issues arise.

First, the multiplicative module only makes use of terms  which are known to be strictly positive or strictly negative. The multiplicative module thus executes a preprocessing stage which tries to infer new sign information from the available data. For example, given the definition  and the sign information  and , one can infer  and assert this comparison to the blackboard. The processing phase also infers straightforward inequalities that hold even when sign information is not available; for example, it infers  whenever  and  are known, even if the signs of  and  are not known. This preprocessing somewhat compensates for the module's need for sign information. However, it is not robust; the more systematic way to accommodate this constraint requires case splitting on the signs of variables. Polya is able to do this in limited settings; see the discussion in Section~\ref{section:conclusions}.



Second, the inequalities that are handled by the multiplicative module
are different from those handled by the additive module, in that terms can have a rational coefficient. For example, we may have an inequality ; here, the multiplicative constant  would correspond to an additive term of  in the additive procedure. This difference makes it difficult to share code between the additive and multiplicative modules, since it prevents the logarithmic transformation from being carried out explicitly. But these rational coefficients are easy to handle in the multiplicative module. 

Finally, the multiplicative elimination may produce information that
cannot be asserted directly to the blackboard, such as a comparison
 or . In that case, we have to pay
careful attention to the signs of  and  and their relation
to  to determine which facts of the form  can be inferred. We compute exact roots of rational numbers when possible, so a comparison  translates to  when  and  are known to be positive. As a last resort, faced with a comparison like , we use a rational approximation of  to try to salvage useful information.

\section{Geometric Methods}
\label{section:geometric}

Although the Fourier-Motzkin modules perform reasonably well on small problems, they are unlikely to scale well. The problem is that many of the inequalities that are produced when a single variable is eliminated are redundant, or subsumed by the others. Thus, by the end of the elimination, the algorithm may be left with hundreds or thousands of comparisons of the form , for different values of . Some optimizations are possible, such as using simplex based methods (e.g.~\cite{dutertre:de:moura:06}) to filter out some of the redundancies. In this section, however, we show how methods of computational geometry can be used to address the problem more directly. On many problems in our test suite, performance is roughly the same. But on some problems of moderate complexity (e.g. example \ref{eq:8i} in Section \ref{section:examples}) we have found our implementation of the geometric approach to be much faster than the Fourier-Motzkin approach. The two methods begin to differ noticeably when the number of problem terms is between 15 and 20. 

\subsection{The Geometric Additive Module}
\label{subsection:additive:geometric}
Geometric methods provide an alternative perspective on the task of
eliminating variables. For real variables  and constants , a linear inequality  determines a half-space in ; when , as in the
homogenized inequalities in our current problem, the defining
hyperplane of the half-space contains the origin. A set of 
homogeneous inequalities determines an unbounded pyramidal polyhedron
in  with vertex at the origin, called a ``polyhedral cone.'' (Equalities, represented as -dimensional hyperplanes, simply reduce the dimension of the polyhedron.) The points inside this polyhedron represent solutions to the inequalities. The problem of determining the strongest comparisons between  and  then reduces to finding extremal ratios of the -th and -th coordinates of points inside the polyhedron.

We use the following well-known theorem of computational geometry (see \cite[Section 1.1]{ziegler:95}):
\begin{theorem}
 A set  is a finite intersection of closed homogeneous linear halfspaces (an \emph{-polyhedron}) if and only if it is a finitely generated conical combination of vectors (a \emph{-polyhedron}).
\end{theorem}

A description of a -polyhedron is said to be a
\emph{-representation} of the polyhedron, and similarly
for -polyhedrons; there are a number of effective methods to convert between representations. 







\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
  \centering
\resizebox{.8\linewidth}{.8\linewidth}{\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\ifx\svgwidth\undefined \setlength{\unitlength}{595.2bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,1)\put(0,0){\includegraphics[width=\unitlength]{3d_poly.pdf}}\put(0.91850593,0.13695526){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{}}}\put(0.30772496,0.84932089){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{}}}\put(0.01061121,0.26542163){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{}}}\end{picture}\endgroup  }
\caption{A polyhedral cone in , defined by three half-spaces}\label{fig:geo:projection:a}
\end{subfigure}\hspace{.05\textwidth}
\begin{subfigure}{.45\textwidth}
  \centering
\resizebox{.8\linewidth}{.8\linewidth}{\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\ifx\svgwidth\undefined \setlength{\unitlength}{595.2bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,1)\put(0,0){\includegraphics[width=\unitlength]{2d_proj.pdf}}\put(0.91781871,0.51748813){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{}}}\put(0.51843319,0.94470046){\color[rgb]{0,0,0}\makebox(0,0)[lb]{\smash{}}}\end{picture}\endgroup  }
\caption{Projected to the  plane, the polyhedron implies  and }
  \label{fig:geo:projection:b}
\end{subfigure}
\caption{Variable elimination by geometric projection}
\label{fig:geo:projection}
\end{figure}

The comparisons and additive equalities stored in the central
blackboard essentially describe an -representation of a
polyhedron. After constructing the corresponding
-representation, it is easy to pick out the implied
comparisons as follows. For every pair of variables  and ,
project the set of vertices to the  plane by setting all the other coordinates to . If there is anything to be learned, all (nonzero) vertices must fall in the same halfplane; find the two outermost points (as in Figure \ref{fig:geo:projection:b}) and compute their slopes to the origin. These slopes determine the coefficients  in two comparisons , and the relative position of the two vertices determine the inequality symbols in place of .



We chose to use Avis' \emph{lrs} implementation of the reverse-search
algorithm \cite{avis:00} to carry out the geometric computations.
Vertex enumeration algorithms typically assume convexity of the
polyhedron: that is, all inequalities are taken to be weak. As it is
essential for us to distinguish between  and , we use a trick
taken from Dutertre and de Moura \cite[Section
5]{dutertre:de:moura:06}. Namely, given a set of strict inequalities , we introduce a new
variable  with constraints  and , and generate the corresponding polyhedron. The  hyperplane is assumed to be infeasible.
If, in the vertex representation, every vertex has a zero -coordinate, then the inequalities are only satisfiable when , which implies that the system with strict inequalities is unsatisfiable. Otherwise, a comparison  is strict if and only if every vertex on the hyperplane  has a zero  coordinate, and weak otherwise.




\subsection{The Geometric Multiplicative Module}
\label{subsection:multiplicative:geometric}

As with the Fourier-Motzkin method, multiplicative comparisons  can be handled in a similar manner, by restricting to terms with known sign information and taking logarithms. Once again, there is a crucial difference from the additive setting: taking the logarithm of a comparison  with , one is left with an irrational constant , and the standard computational methods for vertex enumerations cannot perform exact computations with these terms.

To handle this situation we introduce new variables to represent the logarithms of the prime numbers occurring in these constant terms. Let  represent the prime factors of all constant coefficients in such a problem, and for each , let  be a variable representing . We can then rewrite each  as . Taking logarithms of all such inequalities produces a set of additive inequalities in  variables. In practice, the factorization problems are small and do not create a bottleneck for our algorithm.

In order to find the strongest comparisons between  and , we can no longer project to the  plane, but instead must look at the  hyperplane. The simple arithmetical comparisons to find the two strongest comparisons are no longer applicable; we face the harder problem of converting the vertex representation of a polyhedron to a half-space representation. This problem is dual to the conversion in the opposite direction, and the same computational packages are equipped to solve it. Experimentally, we have found Fukuda's \emph{cdd} implementation of Motzkin's double description method \cite{fukuda:prodon:96} to be faster than \emph{lrs} for this procedure.







\section{The Axiom Module}
\label{section:functions}

The inferences captured by the addition and multiplication modules constitute a fragment of the theory of real-closed fields, roughly, that theory ``minus'' the distributivity of multiplication over addition \cite{avigad:friedman:06}. Recall, however, that we have also included arbitrary function symbols in the language.
An advantage to our framework is that we do not have to treat function
terms as uninterpreted constants; rather, we can seamlessly add
modules that (partially) interpret these symbols and learn relevant inequalities
concerning them.

To start with, a user may wish to add axioms asserting that a particular function  is nonnegative, monotone, or convex. For example, the following axiom expresses that  is nondecreasing:

Given such an axiom, Polya's axiom module searches for useful
instantiations during the course of a search, and may thus learn useful information.

Specifically, given a list of universal axioms in variables , the instantiation module searches for relevant assignments , where each  is a constant and each  is a subterm in the given problem. Each axiom is then instantiated with these assignments, and added to the central blackboard as a set of disjunctive clauses. As the search progresses, elements of these clauses are refuted; if only one remains, it is added to the blackboard, as a new piece of information available to all the modules.

The task is a variant of the classic matching problem, but there are at least three aspects of our framework that present complications. First, given that we consider terms with a rational scalar multiplicative constant, the algorithm has to determine those values. So, in the example above,  and  can be instantiated to an s-term  for any , when such an instantiation provides useful information. Second, we need to take into account the associativity and commutativity of operations like addition and multiplication, so, for example, a term  can be unified with a term  found in the blackboard in multiple ways. Finally, although the framework is built around the idea of restricting attention to subterms occurring in the original problem, at times it is useful to consider new terms. For example, given the axiom

it is clearly a good idea to instantiate  and  to  and
, respectively, whenever , , and 
all appear in the blackboard, even if the term  does not.

In short, we wish to avoid difficult calculations of rational constants, expensive matching up to associativity and commutativity (see e.g.~\cite{contejean:04}), and unrestrained creation of new terms, while at the same time making use of potentially useful instantiations.
The solution we adopted is to use function terms to trigger and
constrain the matching process, an idea commonly used by SMT solvers \cite{demoura:bjorner:07} \cite{nelson:oppen:79}. Given a universal axiom ,  is first converted into clausal
normal form, and each clause  is treated separately. We take the \emph{trigger set} of  to be the set of all functional subterms contained in . A straightforward unification procedure finds all assignments that map each trigger element to a (constant multiple of a) problem term, and these assignments are used to instantiate the full clause . The instantiated clause is asserted to the central blackboard, which checks for satisfied and falsified literals.

For  a term containing unification variables  and  an assignment mapping , the problem of matching  to a problem term  is nontrivial: the matching must be done modulo equalities stored in the blackboard. For example, if , , and , then given the assignment , the term  should be matched to . We thus combine a standard unification algorithm, which suggests candidate assignments to the variables occurring in an axiom, with Gaussian elimination over additive and multiplicative equations, to find the relevant matching substitutions.


















\section{Additional Modules}
\label{section:other:modules}

In addition to the axiom module, which interprets user-defined functions, Polya incorporates a number of modules which interpret built-in functions. Some of these modules simply assert axioms to the axiom module, and allow it to handle instantiation. Other modules derive information that is too specialized to be handled by the generic axiom module. These modules therefore assert identities, comparisons, and clauses the the blackboard based on special features of the functions and terms they are designed to handle. 

\subsection{The Congruence Closure Module}
Polya's blackboard does not enforce that a function must have the same output given equal inputs. This property is known as \emph{congruence closure}. The well-known union-find data structure and its variations (e.g.\ \cite{demoura:bjorner:07} \cite{Moskal2008}) provides an efficient way to maintain these equalities in a database. This maintenance is not a bottleneck for our algorithm, though, and a more naive approach works well. Polya runs a congruence closure module that searches for pairs of problem terms with the same function symbol and arity. If the Blackboard implies that each corresponding pair of arguments are equal, the module asserts that the terms are themselves equal. The runtime of this module is negligible compared to that of the arithmetical modules, so implementing a more structured method is not a priority.

\subsection{The th Root Module}
Handling terms such as  can be difficult, as this expression is undefined when . Canonization must avoid unsound reductions such as simplifying  to . On the other hand, when  is known, additional reductions and inferences can be carried out. The canonizer interprets terms  as , where  is, in turn, interpreted as a function term , where  is a positive integer constant. Reasoning with these functions can largely be handled by the axiom instantiation module, such that for even , inferences about  will be made only if  is known to be positive.

The th root module guarantees that the proper axioms for a given problem have been added to the Blackboard. The module finds a list of  such that  appears as some problem term, and axiomatizes the behavior of  appropriately for each . If  is even, the axioms

are added to the instantiation module. If  is odd, the axiom

is added. These conditional identities provide a sound way of reasoning with fractional exponents.

\subsection{The Exponential and Logarithm Module}
\label{subsection:exponential}
Without computing any exact or approximate values, we can describe the exponential function  as a positive, strictly increasing function defined on all of . The module which interprets this function adds axioms asserting these properties to the axiom instantiation module.

Additionally, the exponential function satisfies the identities 

for scalar . These cannot be axiomatized in a way that the instantiation module will recognize, so the exponential module searches for terms of the appropriate forms and adds equalities as appropriate. Note that this operation is potentially expensive, in that it adds extra terms and multiplicative identities to the blackboard.

The natural logarithm function  has axioms and identities dual to the exponential. Since  is only defined on the positive reals, these axioms are defined conditionally. That is, the module asserts the axiom

to the instantiation module, and only adds identities when the arguments are known to be positive.

\subsection{The Minimum Module}
\label{subsection:minimum}

The minimum function  is interpreted in the standard way: it returns the value of one of its arguments  such that  for all . Terms involving  are canonized in a manner similar to the way that sums are canonized: the arguments are listed according to the underlying term order, and a scalar is brought outside the function so that the first argument is an s-term with coefficient 1. Because the minimum function does not have a fixed arity, it cannot be handled by the general axiom module.

For each problem term  of the form  in the blackboard , the minimum module asserts that  for . As it is useful to find as much sign information as possible for the multiplicative module, the minimum module also checks for  if  for all ; if so, it asserts that  as well.

The module must also account for the fact that  is the \emph{smallest} number less than or equal to all of its arguments. If for some constant  and problem term  we have  for all , then we also know that . The minimum module uses the Blackboard's methods for finding implied coefficient ranges  to find, for each problem term , an interval  for which  implies  holds for all . If such an interval exists, the module determines whether the inequalities are strict at the endpoints, and asserts the relevant information to the blackboard.

\subsection{The Absolute Value Module}
\label{subsection:absolute}
Polya has a specialized module for interpreting the absolute value function. The absolute value of an s-term is canonized by bringing the coefficient outside the absolute value, so that the argument is an s-term with coefficient 1. Basic properties of  are handled by asserting the following axioms to the axiom module:



The axiom module cannot handle the triangle inequality in full generality, and so the absolute value module handles this task on its own. Specifically, the module adds comparisons of the forms

Adding these comparisons indiscriminately would necessitate creating new problem terms  for each argument not already present in the blackboard, which is not likely to be fruitful. The absolute value module takes a more subtle approach, only learning these comparisons if for each , either  is already a problem term, or the sign of  is known (in which case  is replaced with  as appropriate). This approach does not seem to miss any inferences that the indiscriminate approach would capture, since the comparisons learned will only be useful if something is known about each absolute value. The procedure, however, puts additional stress on the modules for arithmetic: for example, the presence of a term  can result in four additional linear inequalities with three terms common to all of them.

\subsection{The Built-in Functions Module}
Sometimes, only minimal information about a function is needed to complete a proof; for instance, it may suffice to know that  or . Polya's built-in functions module will add simple axioms like this for a variety of common functions. Of course, one could create new modules to interpret any of these functions individually, and add more information than the basic properties used here. The goal of this module is to expand Polya's breadth more so than its depth.

The built-in functions module currently axiomatizes  and  as bounded between  and ,  as equal to , and  as bounded between  (strictly) and  (weakly).

\section{Examples}
\label{section:examples}

The current distribution of Polya includes a number of examples that are designed to illustrate the method's strengths, as well as some of its weaknesses. For comparison, we verified a number of these examples in Isabelle, trying to use Isabelle's automated tools as much as possible. These include ``auto,'' an internal tableau theorem prover which also invokes a simplifier and arithmetic reasoning methods, and Sledgehammer  \cite{meng:paulson:09} \cite{blanchette:et:al:11}, which heuristically selects a body of facts from the local context and background library, and exports it to various provers. We also sent some of the inferences directly to the SMT solver Z3 \cite{demoura:bjorner:08}. We report on these results below. We also tried a number of these problems with MetiTarski \cite{akbarpour:paulson:08} and ACL2 \cite{acl2}, which are discussed in Section~\ref{section:conclusions}.


\subsection{Successes}
\label{subsection:successes}

To start with, Polya handles inferences involving linear real inequalities, which are verified automatically by many interactive theorem proving systems. It can also handle purely multiplicative inequalities such as

which are not often handled automatically. It can solve problems that combine the two, like these:

It also handles inferences that combine such reasoning with axiomatic properties of functions, such as: 
 
Isabelle's auto and Sledgehammer fail on all of these but (\ref{eq:4}) and (\ref{eq:5}), which are proved by resolution theorem provers. Sledgehammer can verify more complicated variants of (\ref{eq:4}) and (\ref{eq:5}) by sending them to Z3, but fails on only slightly altered examples, such as:

Z3 gets most of these when called directly, but also fails on (\ref{eq:7}) and (\ref{eq:8}). Moreover, when handling nonlinear equations, Z3 ``flattens'' polynomials, which makes a problem like (\ref{eq:3p5}) extremely difficult. It takes Z3 a couple of minutes when the exponents  and  in that problem are replaced by  and , respectively. Polya verifies all of these problems in a fraction of a second, and is insensitive to the exponents in (\ref{eq:3p5}). It is also unfazed if any of the variables above are replaced by more complex terms.

Polya has built-in knowledge about functions such as , , , , , , and . It verifies examples like these:

\noindent It can also handle examples that combine such functions, such as these:

Z3 fails on (\ref{eq:8a}), (\ref{eq:8b}), (\ref{eq:8d}), and (\ref{eq:8i}), even when the relevant properties of , , etc.\ are given as axioms. Given the right properties, however, it succeeds on the others. Similarly, Isabelle's auto tactic does well on problems that can combine rules for common functions with linear arithmetic; it solves (\ref{eq:8cc}), (\ref{eq:8d}), (\ref{eq:8f}), and (\ref{eq:8h}), with the additional information that it should case split on the sign of the terms inside the absolute value on (\ref{eq:8d}). It fails when  is replaced by  in (\ref{eq:8h}). Sledgehammer verified (\ref{eq:8c}) using the resolution theorem prover Vampire, but neither auto nor Sledgehammer solves the others.

Polya succeeds examples such as

mentioned in the introduction. Sledgehammer verifies this using resolution, and slightly more complicated examples by calling Z3 with the monotonicity of . Sledgehammer restricts Z3 to linear arithmetic so that it can reconstruct proofs in Isabelle, so to verify (\ref{eq:9}) it provides Z3 with the monotonicity of the power function as well. When called directly on this problem with this same information, however, Z3 resorts to nonlinear mode, and fails.

Sledgehammer fails on an example that arose in connection with a formalization of the Prime Number Theorem, discussed in \cite{avigad:et:al:07}:

Z3 verifies it when called directly. Sledgehammer also fails on these \cite{avigad:friedman:06}:

Z3 gets (\ref{eq:11}) but not (\ref{eq:12}). Neither Sledgehammer nor Z3 get these:

Polya verifies all of the above easily.

The following problem was once raised on the Isabelle mailing list:

This inference is verified by Z3 as well as Sledgehammer, but both fail when  and  in the conclusion are replaced by  and , respectively. Polya is insensitive to the exponent.

Let us consider two examples that have come up in recent Isabelle formalizations \cite{avigad:holzl:serafin:unp}. Billingsley  \cite[page 334]{billingsley:95} shows that if  is any function from a measure space to the real numbers, the set of continuity points of  is Borel. Formalizing the proof involved verifying the following inequality:

Sledgehammer and Z3 fail on this, while Polya verifies it easily. 

The second example involves the construction of a sequence  in an interval  with the property that for every , . The proof required showing that  approaches  from the right, in the sense that for every ,  for  sufficiently large. A little calculation shows that  is sufficient. We can implicitly restrict the domain of  to the integers by considering only arguments ; thus the required inference is

Sledgehammer and Z3 do not capture this inference, and the Isabelle formalization was tedious. Polya verifies it immediately.

When restricted to problems involving linear arithmetic and axioms for function symbols, the behavior of Z3 and Polya is similar, although Z3 is much more efficient. As the examples above show, Polya's advantages show up in problems that combine multiplicative properties with either linear arithmetic or axioms involving function symbols. In addition, adding certain axioms to Z3 can cause unexpected interactions: the axioms  and  jointly cause Z3 to fail, even on problems that do not involve any absolute values.

For the kinds of problems described in this section, time constraints are not a serious issue. Polya solves a test suite of 81 problems, including the ones discussed here, in about 8.5 seconds on an ordinary desktop (with an Intel i7-3770 4 core CPU at 3.4 GHz), using the polytope packages, the full set of modules, and a set of standard axioms. As noted in Sections~\ref{subsection:exponential} and \ref{subsection:absolute}, however, the exponential and absolute value modules put additional stress on the arithmetic modules. Problem~(\ref{eq:8i}) comes close to the limit of what the Fourier-Motzkin procedures can handle, and Polya takes more than a minute on that problem using those procedures. If we eliminate that problem and two similar ones from the test suite, Polya solves the remainder with the Fourier-Motzkin procedures in about 13.5 seconds. Moreover, instructing Polya not to solve Problem (\ref{eq:9}) without using the exponential module reduces the total to less than 11 seconds. Under the same conditions, Polya solves the test suite in 6 seconds using the polytope packages. We expect that various optimizations and improvements are possible.

In a prior version of this paper \cite{avigad:lewis:roux:14}, our test suite included only 51 problems that could be solved without invoking any of the modules described in Section~\ref{section:other:modules} other than the congruence closure module. Polya solved these in about 2 seconds on an ordinary desktop using the polytope packages, and in about 5.5 seconds using Fourier-Motzkin. 

Test files for Isabelle, Z3, MetiTarski, and ACL2, as well as more precise benchmark results, can be found in the distribution.

\subsection{Performance on KeYmaera Examples}
\label{subsection:keymaerea}

KeYmaera is a verification tool for hybrid systems that combines automated deduction, real-algebraic methods, and computer algebra \cite{platzer:08} \cite{platzer:09}. Among other applications, it has been used to verify control systems for transportation systems. The current version of KeYmaera uses Z3 and Mathematica as a backend for solving the algebraic problems it generates. These algebraic problems are often well-suited for Polya's approach. 

We obtained a collection of 4442 problems generated by KeYmaera. With a 3 second timeout and case splitting disabled, Polya was able to verify the unsatisfiability of 4252 (96\%) in about six minutes. (With case splitting enabled, Polya solves an additional 15 problems, but runs for about ten minutes.) While we were unable to obtain direct comparisons, the experimental results in \cite{platzer:09} report a similar percentage of examples solved by the best available methods.

\subsection{Shortcomings}
\label{subsection:shortcomings}

Of course, Polya fails on wide classes of problems where other methods succeed. It is much less efficient than the best linear solvers, for example, and should not be expected to scale to large industrial problems. 

Polya has other shortcomings. Recall that the multiplicative module only takes advantage of equations where the signs of all terms are known. When called directly, the module fails to make the trivial inference

The preprocessing step described in Section \ref{subsection:fm:multiplicative} enables Polya to prove this, but this preprocessing is not robust, and minor adjustments cause Polya to fail:


The problem just described is easily solved by case splitting on the signs of  and . This is an instance of a general heuristic: it is often useful to split on the signs of problem terms involved in multiplicative terms, when the signs of these terms are not known. There are other situations where a case split can help when Polya is stuck. For example, one can split on comparisons in a binary minimum or maximum: Polya proves  from either  or , but not outright. Similarly, it is generally useful to split on the sign of an absolute value. We have implemented a mechanism whereby modules can suggest useful case splits for the system to try, as the system carries out nested splits to a user-defined maximum depth. The current implementation is naive and inefficient, however, and needs to be improved (see the discussion in the next section).

Polya's strength comes from the fact that rules and axioms are limited to a small list of ``terms of interest'' stored in the blackboard, allowing modules to contribute information in a flexible way while avoiding combinatorial explosion. But this results in a kind of ``tunnel vision,'' causing Polya to miss inferences that require passage through auxiliary terms. For example, Polya fails to validate

because it does not consider the intermediate term . If this term is added to the blackboard, Polya easily infers , and then . Users can specify such additional terms to consider when posing a problem. Generally speaking, however, it is not an easy task to determine automatically what terms should heuristically be added to the blackboard, and when.

Another shortcoming, in contrast to methods which begin by flattening polynomials, is that Polya does not, \emph{a priori}, make use of distributivity at all, beyond the distributivity of multiplication by a rational constant over addition. Of course, it is by ignoring distributivity that we make the problem modular and tractable; this ignorance is a basic feature of the system, in some sense. However, this leads to some unfortunate consequences. Any reasonable theorem prover for the theory of real-closed fields can easily establish

which can also be obtained simply by writing the left-hand side as . But, as pointed out by Avigad and Friedman \cite{avigad:friedman:06}, the method implemented by Polya is, in fact, nonterminating on this example.


\section{Conclusions and Future Work}
\label{section:conclusions}

One advantage of the method described here is that it should not be difficult to generate proof certificates that can be verified independently and used to construct formal derivations within client theorem provers. In fact, with only minor modifications to the code, we have implemented rudimentary proof tracing, taking variable eliminations in the Fourier Motzkin modules as primitive proof steps. For procedures using real-closed fields, this is much more difficult; see \cite{mclaughlin:harrison:05} \cite{harrison:07b}.

We tried a number of our test problems in MetiTarski \cite{akbarpour:paulson:08}, which combines resolution theorem proving with procedures for real-closed fields as well as symbolic approximations to transcendental functions. We found that MetiTarski does well on problems in the language of real-closed fields, but not with axioms for interpreted functions, nor with the examples with . An interesting heuristic method, implemented in ACL2, is described in \cite{hunt:et:al:03}. That method is considerably different from ours; for example, it flattens polynomial terms. Working with ACL2 involves importing ``books'' that not only define the concepts in a given domain, but also configure ACL2's automation to adopt suitable proof strategies. We experimented with ACL2 on some of the problems in our test suite, with what we took to be a reasonable set of imports. (We are grateful to Grant Passmore for guidance here.) In this context, ACL2 solved 21 out of the 39 of our benchmark problems that involve only arithmetic. When it came to problems involving extra function symbols, we found that ACL2 was sensitive to the amount of background information provided; it did well with individual properties, such as the property that a function is monotone, but fared less well with large batteries of facts about log and exp. It seems likely, however, that ACL2 can be made to perform better on our benchmarks with a more finely tuned default. It also seems likely that Polya would benefit by incorporating some of ACL2's heuristics. (The preliminary tests described in this paragraph can be found in the Polya repository.)

We envision numerous extensions to our method. One possibility is to implement more efficient case splitting and conflict-driven clause learning (CDCL) search, as do contemporary SMT solvers. For example, recall that the multiplicative routines only work insofar as the signs of subterms are known. It is often advantageous, therefore, to split on the signs on subterms. The current implementation of Polya can do so naively, but contemporary mechanisms for backtracking assumptions are vastly more efficient. Similarly, making the addition and multiplication modules incremental would streamline this as well.

There are many ways our implementation could be optimized, and, of course, we would gain efficiency by moving from Python to a compiled language like C++. We find it encouraging, however, that even our unoptimized prototype performs well on interesting examples. It seems to us to be more important, therefore, to explore extensions of these methods, and try to capture wider classes of inequalities. This includes reasoning with powers and logarithms to an arbitrary base; reasoning about the integers as a subset of the reals; reasoning about common functions, such as trigonometric functions; and heuristically allowing other natural moves in the search, such as flattening or factoring polynomials, when helpful. We would also like to handle second-order operators like integrals and sums, and interact better with external theorem proving methods.

We emphasize again that this method is not designed to replace conventional methods for proving linear and nonlinear inequalities, which are typically much more powerful and efficient in their intended domains of application. Rather, our method is intended to complement these, capturing natural but heterogeneous patterns of reasoning that would otherwise fall through the cracks. What makes the method so promising is that it is open-ended and extensible. Additional experimentation is needed to determine how well the method scales and where the hard limitations lie. 

\medskip

\noindent {\em Acknowledgment.} We are grateful to Leonardo de Moura and the anonymous referees for helpful corrections, information, and suggestions.



\def\cprime{}
\begin{thebibliography}{10}

\bibitem{akbarpour:paulson:08}
B. Akbarpour and L. Paulson.
\newblock {MetiTarski: An Automatic Prover for the Elementary Functions}.
\newblock In S. Autexier et al., editors, {\em {AISC/MKM/Calculemus 2008}}, pages 217--231. Springer, 2008.

\bibitem{avigad:et:al:07}
J. Avigad, K. Donnelly, D. Gray,  P. Raff.
\newblock {A formally verified proof of the prime number theorem}.
\newblock {\em ACM Trans. Comput. Logic}, 9(1):2, 2007.



\bibitem{avigad:friedman:06}
J. Avigad and H. Friedman.
\newblock {Combining decision procedures for the reals}.
\newblock {\em Log. Methods Comput. Sci.}, 2(4):4:4, 42, 2006.

\bibitem{avigad:holzl:serafin:unp}
J. Avigad, J. H\"olzl, and L. Serafin.
\newblock A formally verified proof of the Central Limit Theorem.
\newblock In preparation.

\bibitem{avigad:lewis:roux:14}
J. Avigad, R.~Y.~ Lewis, and C. Roux.
\newblock A heuristic prover for real inequalities.
\newblock  In Gerwin Klein and Ruben Gamboa, eds., {\em Interactive Theorem Proving 2014}, Springer, Heidelberg, 61--76, 2014. 

\bibitem{avis:00}
D. Avis.
\newblock Living with lrs.
\newblock In {\em Discrete and computational geometry ({T}okyo, 1998)}, pages 47--56. Springer, 
  2000.

\bibitem{barrett:et:al:08}
C. Barrett, R. Sebastiani, S. A. Seshia and C. Tinelli.
\newblock Satisﬁability modulo theories.
\newblock In A. Biere et al., eds., {\em Handbook of Satisﬁability}, 825--885. IOS Press, 2008.

\bibitem{basu:et:al:03}
S. Basu, R. Pollack,  M. Roy.
\newblock {\em {Algorithms in real algebraic geometry}}.
\newblock Springer, 2003.

\bibitem{billingsley:95}
P. Billingsley.
\newblock {\em Probability and measure}.
\newblock John Wiley \& Sons Inc., 3rd edition, 1995.

\bibitem{blanchette:et:al:11}
J.~Blanchette, S.~B\"{o}hme,  L.~Paulson.
\newblock {Extending Sledgehammer with SMT solvers}.
\newblock {\em Automated Deduction--CADE-23}, pages 116--130, 2011.

\bibitem{boyd:vandenberghe:04}
S.\! Boyd, L.\! Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge University Press, 2004.

\bibitem{chaieb:nipkow:08}
A. Chaieb and T. Nipkow.
\newblock Proof synthesis and reflection for linear arithmetic.
\newblock {\em J. Autom. Reasoning}, 41(1):33--59, 2008.

\bibitem{contejean:04}
E. Contejean.
\newblock A Certified AC Matching Algorithm.
\newblock In van Oostrom, ed., {\em Rewriting Techniques and Applications}, pages 70--84. Springer, 2004.

\bibitem{demoura:bjorner:07}
L.~de Moura, N.~Bj{\o}rner.
\newblock Efficient E-Matching for SMT Solvers. 
\newblock In {\em CADE}, 183--198, 2007.

\bibitem{demoura:bjorner:08}
L.~de~Moura, N.~Bj{\o}rner.
\newblock {Z3: An Efficient SMT Solver}.
\newblock In {\em {TACAS}}, 337--340, 2008.

\bibitem{dutertre:de:moura:06}
B. Dutertre and L. de~Moura.
\newblock A fast linear-arithmetic solver for DPLL(T).
\newblock In T. Ball and R. Jones, editors, {\em CAV 2006}, pages 81--94. 
Springer, 2006.

\bibitem{fukuda:prodon:96}
K. Fukuda and A. Prodon.
\newblock Double description method revisited.
\newblock In {\em Combinatorics and computer science ({B}rest, 1995)}, pages 91--111. Springer,
  1996.

\bibitem{garling:07}
D.~J.~H. Garling.
\newblock {\em {Inequalities: a journey into linear analysis}}.
\newblock Cambridge University Press, Cambridge, 2007.

\bibitem{gao:et:al:12}
S.~Gao, J.~Avigad, E.~M.~Clarke.
\newblock Delta-complete decision procedures for satisfiability over the reals.
\newblock In B.~Gramlich et al., eds., {\em IJCAR}, 286--300, 2012. 

\bibitem{hardy:littlewood:88}
G.~H. Hardy, J.~E. Littlewood,  G.~P\'{o}lya.
\newblock {\em {Inequalities}}.
\newblock Cambridge University Press, Cambridge, 1988.
\newblock Reprint of the 1952 edition.

\bibitem{harrison:07c}
J. Harrison.
\newblock {{HOL} light: a tutorial introduction}.
\newblock In M. Srivas and A. Camilleri, editors, {\em {FMCAD}}, pages 265--269, Springer, 1996.

\bibitem{harrison:07b}
J. Harrison.
\newblock {Verifying Nonlinear Real Formulas Via Sums of Squares}.
\newblock In K. Schneider and J. Brandt, editors, {\em {TPHOLs}}, pages 102--118. Springer,
  2007.

\bibitem{hunt:et:al:03}
W.~A. Hunt, R.~B. Krug, J.~Moore.
\newblock Linear and nonlinear arithmetic in ACL2.
\newblock In D. Geist and E. Tronci, eds., {Correct Hardware Design and Verification Methods},
pages 319--333. Springer, 2003.
    
\bibitem{jones:kerrigan:maciejowski:04}
C.~N. Jones, E.~C. Kerrigan,  J.~M. Maciejowski.
\newblock Equality set projection: A new algorithm for the projection of
  polytopes in halfspace representation.
\newblock Technical report, Department of Engineering, University of Cambridge,
  March 2004.
  
\bibitem{acl2}
M.~Kaufmann, P.~Manolios, and J.~Strother Moore.
\newblock \emph{Computer-aided reasoning: an approach}, Kluwer, 2000.

\bibitem{lewis:14}
R.~Y.~Lewis.
\newblock \emph{Polya: a heuristic procedure for reasoning with real inequalities}.
\newblock M.S.~thesis, Department of Philosophy, Carnegie Mellon University, 2014.

\bibitem{mclaughlin:harrison:05}
S. McLaughlin and J. Harrison.
\newblock {A proof producing decision procedure for real arithmetic}.
\newblock In R. Nieuwenhuis, editor, {\em {Automated Deduction -- CADE-20}}, pages 295--314, Springer, 2005.


\bibitem{meng:paulson:09}
J. Meng and L. Paulson.
\newblock Lightweight relevance filtering for machine-generated resolution
  problems.
\newblock {\em J. Applied Logic}, 7(1):41--57, 2009.

\bibitem{moore:et:al:09}
R. Moore, R. Kearfott,  M. Cloud.
\newblock {\em Introduction to interval analysis}.
\newblock Society for Industrial and Applied Mathematics (SIAM), 2009.


\bibitem{moses:71}
J. Moses.
\newblock Algebraic simplification: A guide for the perplexed.
\newblock {\em Communications of the ACM}, 14:527--537, 1971.

\bibitem{Moskal2008}
M. Moskal, J. {\L}opusa\'nski, J. Kiniry.
\newblock {E-matching for fun and profit}.
\newblock{\em Proceedings of the 5th International Workshop on SMT}, 2008.

\bibitem{nelson:oppen:79}
G. Nelson and D. Oppen.
\newblock {Simplification by cooperating decision procedures}.
\newblock {\em ACM Transactions of Programming Languages and Systems},
  1:245--257, 1979.

\bibitem{nipkow:et:al:02}
T. Nipkow, L. Paulson,  M. Wenzel.
\newblock {\em {Isabelle/{HOL}. A proof assistant for higher-order logic}}.
\newblock Springer, 2002.

\bibitem{platzer:08}
A. Platzer and J. Quesel.
\newblock {KeYmaera: A Hybrid Theorem Prover for Hybrid Systems.}
\newblock In Armando, Baumgartner, Dowek, eds., {\em IJCAR 2008}, Springer,
  Heidelberg, 171-178, 2008. 
  
\bibitem{platzer:09}
A. Platzer, J. Quesel, P. Rummer.
\newblock{Real World Verification}.
\newblock In Schmidt, ed., {\em CADE 2009}, Sprinter, Heidelberg, 485-501, 2009.

\bibitem{polya:04}
G.~Polya.
\newblock {\em How to solve it}.
\newblock Princeton University Press, Princeton, NJ,
  1945.

\bibitem{prevosto:waldmann:06}
V. Prevosto and U. Waldmann.
\newblock {SPASS+{T}}.
\newblock In G. Sutcliffe et al., editors, {\em
  {ESCoR: Empirically Successful Computerized Reasoning 2006}}, 
  pages 18--33. CEUR Workshop Proceedings, 2006.
  
\bibitem{pugh:92}
W. Pugh.
\newblock The omega test: a fast and practical integer programming algorithm
  for dependence analysis.
\newblock {\em Communications of the ACM}, 8:4--13, 1992.

\bibitem{schrijver:86}
A. Schrijver.
\newblock {\em Theory of linear and integer programming}.
\newblock John Wiley \& Sons, 1986.



\bibitem{ziegler:95}
G. Ziegler.
\newblock {\em Lectures on polytopes}.
\newblock Springer, 1995.

\end{thebibliography}

\end{document}
