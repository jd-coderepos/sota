\documentclass{bmvc2k}



\title{Challenging the Universal Representation of Deep Models for 3D Point Cloud Registration}

\addauthor{David Bojanić}{david.bojanic@fer.hr}{1}
\addauthor{Kristijan Bartol}{kristijan.bartol@fer.hr}{1}
\addauthor{Josep Forest}{ josep.forest@udg.edu}{2}
\addauthor{Stefan Gumhold}{stefan.gumhold@tu-dresden.de}{3}
\addauthor{Tomislav Petković}{tomislav.petkovic.jr@.hr}{1}
\addauthor{Tomislav Pribanić}{tomislav.pribanic@.hr}{1}

\addinstitution{
 Faculty of Electrical Engineering and Computing, University of Zagreb \\
 Zagreb, Croatia
}
\addinstitution{
 Computer Vision and Robotics Research Institute (VICOROB) \\
 University of Girona, Girona, Spain
}
\addinstitution{
TU Dresden, Dresden, Germany
}




\runninghead{Bojanić \etal}{Challenging Universal Representations}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}



\usepackage{bbm}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{eucal}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\usepackage{color, colortbl}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\begin{document}

\maketitle

\begin{abstract}
Learning universal representations across different applications domain is an open research problem. In fact, finding universal architecture within the same application but across different types of datasets is still unsolved problem too, especially in applications involving processing 3D point clouds. In this work we experimentally test several state-of-the-art learning-based methods for 3D point cloud registration against the proposed non-learning baseline registration method. The proposed method either outperforms or achieves comparable results w.r.t. learning based methods. In addition, we propose a dataset on which learning based methods have a hard time to generalize.  Our proposed method and dataset, along with the provided experiments, can be used in further research in studying effective solutions for universal representations. Our source code is available at: \url{github.com/DavidBoja/greedy-grid-search}.
\end{abstract}




\section{Introduction}

3D point cloud registration is the problem of finding an optimal rotation and translation that aligns two overlapping 3D point clouds. 
It arises as a subtask in many different computer vision applications such as: 3D reconstruction \cite{3Dreconstruction_reference1,3Dreconstruction_reference2,3Dreconstruction_reference3}, object recognition and categorization \cite{SHOT,RoPS,object_recognition_reference}, shape retrieval \cite{shape_retrieval_reference}, robot navigation \cite{robot_navigation_reference1} and is still a very researched area \cite{comprehensive-survey,3d-registration-review}.

The most recent advances have been inspired by the successes of deep learning, i.e., by the development of novel architectures and layers convenient for point cloud processing, such as PointNet \cite{pointnet} and 3D convolutions \cite{3d-convolutional-network-for-human-action, c3d-3d-convolutional-networks}. Most of the learning-based approaches first extract point cloud features \cite{dcp-learning-representation-for-registration, prnet-self-supervised--for-partial-to-partial-registration, robust-point-cloud-registration-deep-graph-matching, pcam-product-of-cross-attention-matrices}, and then either apply RANSAC for matching \cite{3dmatch, predator, d3feat-joint-learning-of-dense-features} or learn the whole registration pipeline end-to-end \cite{pointnetlk-robust-and-efficient-registration-using-pointnet, pointnetlk-revisited, omnet-learning-overlapping-mask}. Current state-of-the-art methods \cite{predator, geometric-transformer-for-fast-registration, REGTR, SpinNet, DIP, YOHO, d3feat-joint-learning-of-dense-features} achieve remarkable performance on public benchmarks \cite{3dmatch, KITTI, ETH, modelnet40}, even on very difficult examples with an overlap smaller than 30\% percent \cite{predator}. 



The main limitation of the state-of-the-art methods, which is typical for deep-learning based methods \cite{exploring-generalization-in-deep-learning, generalization-in-deep-learning}, is that model performance drops on datasets that differ from the training data. Several recent methods \cite{SpinNet, DIP, geometric-transformer-for-fast-registration, PointDSC} address the generalizability issue and demonstrate significant performance retention between 3DMatch \cite{3dmatch}, KITTI \cite{KITTI}, and ETH \cite{ETH} datasets. To further test the generalization of the existing learning-based methods, we propose a novel FAUST-partial evaluation benchmark. The benchmark consists of overlapping partial views (>60\%), based on the FAUST dataset \cite{FAUST} of 3D human scans, and is substantially different from other public benchmarks. We analyze the performance of the best methods under greater scrutiny by using common 3D registration performance metrics and several distance and angle thresholds.

The aim of this paper is to propose a simple, straightforward 3D registration method and use it as a baseline for comparison with the state-of-the-art, particularly to evaluate their generalization performance. We show competitive performance on KITTI and ETH datasets, when we compare against the models pretrained on 3DMatch. When we compare on a substantially different, FAUST-partial benchmark, we outperform state-of-the-art. This result suggests that the learning-based methods, although remarkable on many public datasets, are still not robust enough to be applied on any 3D data. On the other hand, our baseline method performs consistently, regardless of the data distribution. 

The proposed method is based on the step-wise (grid) search over the possible rotations and translations. 
The point clouds are firstly voxelized. Then, the best transformation candidate is selected as the solution that has the maximum cross-correlation between the two voxel volumes; thus, we call our method \textit{greedy} grid search.

In summary, we:
\begin{itemize}
    \item Evaluate the generalization performance of the state-of-the-art methods under a common set of 3D registration metrics; 
    
    \item Generate a specialized benchmark, called FAUST-partial, based on 3D human scans, which further challenges the generalization of learning-based methods.

    \item Propose a novel 3D registration baseline which selects the transformation candidate based on the maximum cross-correlation between the voxelized point clouds;
    
    \item Demonstrate comparable performance to the state-of-the-art 3D registration methods on public benchmarks and outperform them on FAUST-partial.
\end{itemize}

\section{Related Work}

\textbf{Classical registration.} The most popular classical registration method is the iterative closest point (ICP) algorithm. The algorithm selects a subset of points based on a criteria, calculates the optimal transformation between the clouds using SVD, and iterates until convergence. The original implementations used point-to-point \cite{point2point-icp} and point-to-plane \cite{point2plane-icp} distances for finding the solution, but many other strategies have been proposed \cite{aa-icp, go-icp, trimmed-icp, iterative-global-similarity-points}. GO-ICP \cite{go-icp} proposes a branch-and-bound scheme and proves the global optimality of the algorithm. The 4-point congruent sets (4PCS) algorithm \cite{4pcs} and its variants \cite{v4pcs, generalized-4pcs} are based on the idea that there exist sets of four coplanar points whose alignment corresponds to the alignment of the point clouds. To select the correspondences, RANSAC is used, and ICP is applied for refinement.

\textbf{Handcrafted features.} Methods based on handcrafted features first extract correspondences between the point clouds and then find the transformation using RANSAC. Similar to the image keypoint-based methods such as SIFT \cite{sift}, 3D feature-based methods focus on keypoint detection \cite{a-concise-and-provably-informative-heat-diffusion, harris-3d, learning-a-descriptor-specific-keypoint-detector} and their distinctive description \cite{narf-3d-range-features, fast-point-feature-histograms, unique-shape-context-for-3d-data-description, intrinsic-shape-signatures, using-spin-images-for-object-recognition, recognizing-objects-in-range-data, unique-signatures-of-shistograms, 3d-free-form-object-recognition, keypoint-based-4-pcs}. A few recent methods \cite{cofinet-reliable-coarse-to-fine-correspondences, geometric-transformer-for-fast-registration} retrieve correspondences without keypoint detection by considering all possible matches.

\textbf{Feature learning.} Instead of handcrafting distinctive features, keypoint detection and description can be learned. 3DMatch \cite{3dmatch} transforms patches into volumetric voxel grids of truncated distance function (TDF) values
and processes them through a 3D convolutional network \cite{c3d-3d-convolutional-networks, 3d-convolutional-network-for-human-action} to output local descriptors. Followed by 3DMatch and the popularity of deep learning, many other works propose to learn keypoint detection \cite{d3feat-joint-learning-of-dense-features, usip-unsupervised-stable-interest-point-detection, 3dfeat-net-weakly-supervised} and description \cite{fully-convolutional-geometric-features, ppfnet-global-context-local-features, learning-compact-geometric-features, predator}. Most of these works are learned by optimizing some version of contrastive loss \cite{contrastive-loss1, contrastive-loss2} between the descriptors of matching and non-matching points, and then by applying RANSAC to select the final correspondences.

\textbf{End-to-end registration learning.} There are many recent approaches that learn not only feature description, but also the subsequent matching step, thus learning end-to-end. The first group of these methods \cite{robust-point-cloud-registration-deep-graph-matching, prnet-self-supervised--for-partial-to-partial-registration, dcp-learning-representation-for-registration, rpm-net-robust-matching-using-learned-features, sticky-pillars, iterative-distance-aware-similarity-matrix, pcam-product-of-cross-attention-matrices, deep-gmr}, pioneered by the deep closest point \cite{dcp-learning-representation-for-registration}, follow the ICP idea by iteratively establishing soft correspondences and then applying weighted SVD to obtain the transformations. The remaining group of methods \cite{pointnetlk-robust-and-efficient-registration-using-pointnet, pointnetlk-revisited, feature-metric-registration-without-correspondences, omnet-learning-overlapping-mask, pcrnet}, represented by PointNetLK \cite{pointnetlk-robust-and-efficient-registration-using-pointnet} use PointNet architecture \cite{pointnet} or similar global description strategy to regress the transformation based on the global feature vectors.

\textbf{Generalization to other datasets.} Several recent methods \cite{SpinNet, DIP, PointDSC} attempt to generalize to datasets other than training. All of these methods demonstrate significant performance retention on novel datasets, for example, when evaluating 3DMatch-pretrained models on KITTI \cite{SpinNet, PointDSC}. On the other hand, the results reported on the ETH dataset only show that the computed descriptors have a high recall \cite{SpinNet,d3feat-joint-learning-of-dense-features,DIP} (using the feature-matching-recall measurement), but never actually evaluate the 3D registration.

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=\linewidth]{images/pipeline-ver3.png}
\end{center}
   \caption{The proposed pipeline. The method is divided into 3 steps: \textit{pre-processing}, \textit{processing} and \textit{post-processing}. Each step follows the previous one. The \textit{pre-processing} step prepares the initial data and outputs  voxelized source volumes and one target volume. The \textit{processing} step performs the 3D cross-correlation over each source volume and the target volume. The cross-correlation volumes  are heatmaps that should indicate higher (indicated in yellow on the volumes) or lower (indicated with purple on the volumes) matching between the source  and target  volumes at the corresponding voxels. White spaces are present because the cross-correlations values are clipped so only the upper range is visible. Finally, the \textit{post-processing} step finds the solution from the output volumes by finding the maximal cross-correlation from all the given volumes.}
\label{fig:pipeline}
\end{figure*}

\section{Method description} \label{sec:method-description}

Let  be the \textit{source} point cloud and  the \textit{target} point cloud. The goal of 3D registration is to find the rigid homogeneous transformation  that best aligns  to . The rigid transformation  is composed of a rotation component  and a translation component .

The method pipeline is summarized in Fig. \ref{fig:pipeline}. We divide our method in 3 key steps: \textit{pre-processing}, \textit{processing} and \textit{post-processing}. In this Section we present the general pipeline. The final estimation of the rigid transformation is provided in Eq. \ref{eq:summarized_pipeline2}.


\textbf{R and t parametrization.} To find the correct rotation and translation, we perform a grid search over the rotation and translation space. The rotation space is sampled using 3 Euler angles  that rotate the source point cloud  around the  axes respectively. In a typical 3D scanning use case, two scanned point clouds that need to be registered should be fairly close in the rotation space since the scanning process is limited by a required overlap region. 
We uniformly sample each angle from the range  with an angle step of  and create a Cartesian product of all the possible combinations resulting in  triplets. Finally, we convert these to rotation matrices . Note that this step is only computed once, prior to registration.
The translation space is inherently sampled by the voxelization process of the given point clouds. The possible translations hence correspond to the centers of the source point cloud voxels and are therefore dependent on the voxelization resolution. More details are provided in the following  sections.

\textbf{Pre-processing.} \label{sec:pre-processing} First, we center and rotate the source point cloud  around the origin using the precomputed rotation matrices , obtaining . Next, we make all the point clouds coordinates positive by translating their minimal bounding box point into the origin. This step is only done to facilitate the voxelization process and can be completely omitted. We then voxelize each source  and target  point clouds with a voxel resolution of  cm. 
Instead of having a 3D grid with ones and zeros, we set a value of  (positive voxel) for the filled voxels (voxels where a point from the point cloud is present) and a value of  (negative voxel) for the empty ones (voxels where there are no points present from the point cloud). 
This results in  source volumes  and one target volume . 

\textbf{Processing.} \label{sec:processing} For each source volume , we perform a 3D cross-correlation with the target volume . Essentially, the central voxel of the target volume is translated over each voxel of the source volume where the cross-correlation can be computed by multiplying the overlying voxel values of the two volumes and summing them together. This results in  cross-correlation volumes  with the same 3 dimensions as the source volume. The volumes can be thought of as discrete heatmaps where higher values should represent higher degrees of matching between the voxelized point clouds.
Prior to the cross-correlation, each source volume is padded in order for the target volume to \textit{slide} all over the source volume. We mark with  the padding applied to each source volume , where the values represent the number of voxels padded to the left, right, top, bottom, front and back of the volume, respectively. 
We make use of the Fourier domain to accelerate the computation of the cross-correlation. Both volumes are first transformed into the Fourier space using the FFT algorithm \cite{fft}, after which the cross-correlation simplifies to a matrix multiplication \cite{fft-book}. The output is then transformed back with an inverse FFT. 

\textbf{Post-processing.} \label{sec:post-processing} We finally estimate the rotation matrix  that aligns (rotation-wise)  to  using one of the  precomputed rotation matrices . We select the matrix  that corresponds to  with the maximal cross-correlation value from the  volumes. More concretely, we use the

index to find the estimated rotation matrix .
To estimate the translation we find the voxel with the maximal cross-correlation value from . Then, we translate the central voxel of the target volume to the just found voxel of the  volume. Since the  volume corresponds to the source  volume, we essentially translate the central voxel of the target volume to the voxel of the source volume with the maximal cross-correlation. More concretely, we find the index of the voxel with the maximal cross-correlation value with

Then, to translate the central voxel of the target volume to it, we use the translation:

where each value is multiplied by the voxel resolution  to transform from voxel indices to euclidean coordinates. The central voxel of the target volume can be computed as:

where  are the number of voxels of  along the  dimensions. Intuitively, the central voxel along a dimension is the middle voxel if the number of voxels is odd, and one on the left of the "middle point" if it's odd.


Following all of the steps above, the rigid registration can be summarized as:

where  indicates that the left and right part are aligned. The  translation moves the center of mass of the respective point cloud into the origin and  moves the minimal bounding box point into the origin. More concretely:



where the  notation indicates the row-wise and column-wise indexing, and  indicates the minimal element of an array.

Since the final rigid transformation needs to align  to , Eq. \ref{eq:summarized_pipeline1} equation is further refined as:


Therefore, the final rotation and translation estimations are:



\textbf{Refinement.} Since the rotation and translation spaces are discretized, the initial alignment is only a rough estimate. The upper bounds on the rotation and translation errors are  degrees and  centimeters if the ground truth solution is located in the estimated discretized locations. For an angle step of  and cm the upper bound errors would be  and cm. Hence, the rough initial alignment should provide a very good initialization for a fine registration algorithm. 
We use generalized ICP \cite{generalized-icp} to refine the initial solution since it provided the best results.





\section{Experiments} \label{sec:experiments}
We evaluate the generalization capabilities of state-of-the-art methods trained on 3DMatch \cite{3dmatch} and compare them to our baseline. We use two established benchmarking datasets ETH \cite{ETH} and KITTI \cite{KITTI} and create a novel FAUST-partial benchmark based on the FAUST dataset \cite{FAUST}. These datasets test the generalization abilities in terms of different sensor modalities (RGB-D, laser scanner), different environments (indoor, outdoor), resolution (mm to cm) and completely different structure (from indoor objects to humans). 

\textbf{Metrics.} Following \cite{SpinNet,d3feat-joint-learning-of-dense-features,pcam-product-of-cross-attention-matrices,geometric-transformer-for-fast-registration,DGR} we evaluate the results using the Relative Rotation Error (RRE), the Relative Translation Error (RTE) and the Registration Recall (RR) measures. 
The Relative Rotation Error measures the relative angle in degrees between the ground-truth  and estimated  rotation matrices: 

The Relative Translation Error measures the distance from the ground-truth  and estimated  translation vectors:


The Registration Recall measures the fraction of successfully registered pairs of point clouds. 
A registration is deemed successful (or a true positive in terms of the recall measure) if its RRE and RTE are below predefined thresholds  and :


where  is the set of all the point cloud registration pairs  in the dataset,  is an indicator function and  indicate the  and  for registration pairs . The final RRE and RTE measurements are averaged only over the successfully registered pairs  obtained from the RR.

\textbf{Parameters.} To fully define the baseline, we need to set the parameters of the euler angle range , angle step , the positive voxel value  and the negative voxel value . The parameters  and  then determine . The padding  is determined for each source volume  so that the volume stays the same dimension after the cross-correlation. The only parameter we vary for each dataset is the voxel resolution  since the datasets vary greatly in their dimensions ranging from volumes of  for KITTI to  for FAUST-partial.

\textbf{KITTI.} The KITTI dataset \cite{KITTI} is comprised of 11 sequences of outdoor driving scenarios obtained by a LiDAR scanner. Compared to 3DMatch, the fragments are much larger, have lower resolution and a different structure. Following common practice \cite{SpinNet,d3feat-joint-learning-of-dense-features,geometric-transformer-for-fast-registration,predator,fully-convolutional-geometric-features,predator}, we test our baseline on scenes 8 to 10 using pairs which are at least 10m away from each other. The ground truth transformation matrices are refined using ICP and the evaluation thresholds are set to  and m. We set cm.

As can be seen from Table \ref{tab:KITTI-results}, most state-of-the-art methods tend to generalize well onto the KITTI dataset. The fragments from the dataset are gravity aligned, which is reflected in lower RRE errors since most of the ground truth rotation comes from rotating around one ax. The fragments are also much bigger than those from 3DMatch ( on average in volume compared to m in 3DMatch) which is reflected in higher RTE errors.
Surprisingly, DIP shows poor recall performance with only  aligned pairs. The KITTI dataset is much noisier than 3DMatch which might be affecting the local reference frame (LRF) alignment in DIP. 
The baseline achieves comparable results and only lags behind the best recall result from GeoTransformer by less than  percentage point (pp). However, it compensates by achieving the lowest RRE and RTE measurements.


\begin{table}[h!]
\scriptsize
\begin{center}
\begin{tabular}{l|c|c|c}
\hline
Method      & RR(\%) & RRE() & RTE (cm) \\
\hline
FCGF  \cite{fully-convolutional-geometric-features}      & 24.19 & 1.61  & 27.10    \\
D3Feat-rand \cite{d3feat-joint-learning-of-dense-features} & 18.47 & 1.58  & 37.80    \\
D3Feat-pred \cite{d3feat-joint-learning-of-dense-features}& 36.76 & 1.44  & 31.60    \\
SpinNet   \cite{SpinNet}  & 81.44 & 0.98  & 15.60    \\
DIP       & 51.71  & 1.02   & 13.43  \\
GeoTransformer \cite{geometric-transformer-for-fast-registration} &   & 0.54 &  154.27 \\
FCGF+PointDSC \cite{PointDSC} & 66.13 & 0.88   & 107.71      \\
FCGF+YOHO-O \cite{YOHO} & 81.44  & 1.99  & 54.25      \\
FCGF+YOHO-C \cite{YOHO} & 82.16  & 1.38   &  39.30     \\
\hline
Baseline  & 90.27 &  &  \\
\hline
\end{tabular}
\caption{Results on the KITTI dataset. All the methods are trained on the 3DMatch dataset. Results marked with  are taken from \cite{SpinNet}.}
\label{tab:KITTI-results}
\end{center}
\end{table}
 

\textbf{ETH.} The ETH dataset \cite{ETH} consists of 4 scenes mostly comprised of outdoor vegetation. Compared to 3DMatch, the fragments are larger, have lower resolution and have more complex geometries. Following common practice \cite{the-perfect-match, DIP, SpinNet} we use only point clouds with an overlap greater than . We set the thresholds to  and cm and the voxel resolution of the baseline to cm. The stricter threshold for the RTE reflects the fact that the fragments from ETH are much smaller in volume than those from KITTI.
As can be seen in Table \ref{tab:ETH-results}, the deep learning methods generalize somewhat less than on the KITTI dataset. GeoTransformer and PointDSC achieve low recall results which can be explained by the high RTE error, also visible on the KITTI dataset. Hence, with a stricter RTE threshold, the recall drops significantly. The best performance is achieved by YOHO, followed by the baseline with  recall percentage points difference. However, the baseline achieves, again, the lowest RRE and RTE measures between all the methods.

\begin{table}[h!]
\scriptsize
\begin{center}
\begin{tabular}{l|c|c|c}
\hline
Method      & RR(\%) & RRE() & RTE (cm) \\
\hline
SpinNet \cite{SpinNet} & 73.07 & 1.205 & 5.352    \\
DIP \cite{DIP} & 62.41 & 1.940 & 14.716    \\
GeoTransformer \cite{geometric-transformer-for-fast-registration} & 4.91 & 0.710  &  21.162    \\
FCGF+PointDSC \cite{PointDSC} &  2.81  &  0.573 &  23.426    \\
FCGF+YOHO-O \cite{YOHO} & 79.94 &  2.167  &  16.112    \\
FCGF+YOHO-C \cite{YOHO} &  & 1.950  &  16.176    \\
\hline
Baseline    &   78.40    &     &   \\
\hline
\end{tabular}
\caption{Results on ETH dataset. All the methods are trained on the 3DMatch dataset.}
\label{tab:ETH-results}
\end{center}
\end{table}
 


\textbf{FAUST-partial.} To further test the generalization capabilities of a 3D registration method, we create a new FAUST-partial benchmark based on the FAUST \cite{FAUST} dataset. The state-of-the-art methods that train on the 3DMatch dataset, test their generalization capabilities \cite{SpinNet,DIP,geometric-transformer-for-fast-registration,pcam-product-of-cross-attention-matrices,PointDSC,the-perfect-match} on the KITTI \cite{KITTI} and ETH \cite{ETH} datasets that tend to be comprised of similar objects. We argue that because of that reason, using proper data augmentation whilst learning on 3DMatch, a method can increase its robustness and generalization on noisier and perturbed fragments from other datasets. However, when encountered with completely unseen data (such as 3D human scans), the methods have difficulty generalizing, even when having more than  overlap.

To create a 3D registration benchmark, we use the FAUST training dataset comprised of  human body scans. We divide each scan into multiple overlapping fragments that need to be aligned. The steps are illustrated in Fig.~\ref{fig:faust-partial}. To create the fragments, we begin by moving each scan so the -plane acts as the floor. We do this by moving the minimal bounding box point to the origin. Next, we create a regular icosahaedron centered at the center of mass of each scan. A regular icosahaedron contains  points that lie on a unit sphere around its center, each equidistant from its neighbors. We scale the icosahaedron to a m radius sphere so each scan fits inside it. These points are used as the viewpoints for creating the partial views (fragments), discarding the ones located below the -plane (below the floor). For each viewpoint, we use the hidden point removal \cite{hidden_point_removal} algorithm to create a partial point cloud. 
Next, for each two pairs of viewpoints  we sample a rotation using 3 Euler angles from the range  and a random translation from the range . We rotate and translate the partial point cloud obtained from viewpoint  to finally get a benchmark registration pair. For the final benchmark we use every pair of viewpoints that have an overlap bigger than , resulting in  registration pairs.

\begin{figure*}
\begin{center}

\includegraphics[width=\linewidth]{images/FAUST-partial-ver3.png}
\end{center} 
   \caption{FAUST-partial dataset generation. For a given scan from the FAUST \cite{FAUST} dataset, we translate its minimal bounding box point to the origin. Next, we surround the scan with a regular icosahaedron. Each point of the icosahaedron acts as a viewpoint used to create a partial scan using the hidden point removal algorithm \cite{hidden_point_removal}. For two partial scans with an overalp bigger than , we use a random rotation and translation to obtain a registration pair for the FAUST-partial benchmark.}
\label{fig:faust-partial}
\end{figure*}

For evaluating the baseline we set the voxel resolution to cm and use the thresholds  and cm. The strict threshold for RTE reflects the fact that fragments from FAUST-partial are much smaller in volume than all the other datasets. As can be seen from Table \ref{tab:FAUST-partial-results}, deep learning methods are incapable of generalizing onto datasets that differ considerably from the 3DMatch-like datasets on which they were trained on. Additionally, the FAUST-partial registration pairs have considerably bigger overlaps  than the 3DMatch benchmark. However, the best recall of   from the deep learning methods is achieved by GeoTransformer. Contrary, the baseline performs consistently, achieving a remarkable  recall and the lowest RRE and RTE measures. 


\begin{table}[t]
\scriptsize
\begin{center}
\begin{tabular}{p{3cm}|
>{\centering\arraybackslash}p{0.65cm} >{\centering\arraybackslash}p{0.7cm} >{\centering\arraybackslash}p{1.0cm}} \hline
Method      & RR(\%)   & RRE()  & RTE(cm)\\
\hline
FPFH-8M \cite{fast-point-feature-histograms} & 9.51 & 4.347  & 1.900  \\
SpinNet \cite{SpinNet} & 42.46 & 3.105 & 1.670  \\
GeoTransformer \cite{geometric-transformer-for-fast-registration} & 56.15 & 2.423  & 1.581  \\
FCGF+PointDSC \cite{PointDSC}  & 47.85  & 3.354  & 1.793 \\
FCGF+YOHO-O \cite{YOHO} & 18.91 & 4.489  & 1.852 \\
FCGF+YOHO-C \cite{YOHO} & 29.18 & 3.653  & 1.668  \\
DIP \cite{DIP} & 54.81 & 4.058  & 2.052  \\
\hline
Baseline    &  &   &   \\
\hline
\end{tabular}
\end{center}
 \caption{Results on FAUST-partial dataset. All the methods are trained on the 3DMatch dataset. FPFH-8M is registered with RANSAC using 8 million iterations.}
 \label{tab:FAUST-partial-results}
\end{table}
 



\section{Conclusion}
The proposed classical approach provides a good 3D registration baseline. The method is simple but effective, which is demonstrated on the public benchmarks. Compared to the generalization performance of the state-of-the-art methods, the baseline is on-par. On the newly proposed, FAUST-partial benchmark, the competing methods are struggling to retain the results, or perform significantly worse, even though the generated overlap between the cloud pairs is reasonably high. Contrary to the deep learning methods, the baseline is simple and explainable and serves for detailed analyses. The effects of different strategies are clear and intuitive and provide insights into the registration process. Therefore, on the search of finding universal representations, designing a deep model mimicking the proposed baseline method is an interesting future avenue to pursue. \\
\textbf{Acknowledgement} This work has been supported by the Croatian Science Foundation under the project IP-2018-01-8118.


\bibliography{egbib}
\end{document}
