\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2018}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{soul}
\usepackage{xargs}  
\usepackage{array}\usepackage{subcaption}
\usepackage[pdftex,dvipsnames]{xcolor}  \usepackage{tabularx}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{K}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{amsmath}

\title{Integrating Recurrence Dynamics for Speech Emotion Recognition}
\name{Efthymios Tzinis, 
      Georgios Paraskevopoulos,
      Christos Baziotis, 
      Alexandros Potamianos}
\address{
  School of Electrical \& Computer Engineering,
       National Technical University of Athens, Greece \\
  Behavioral Signal Technologies, Los Angeles, CA, USA\\
  Both authors contributed equally to this work}
  
\email{etzinis@gmail.com, geopar@central.ntua.gr, cbaziotis@mail.ntua.gr, potam@central.ntua.gr}

\begin{document}

\maketitle

\begin{abstract}
We investigate the performance of features that can capture nonlinear recurrence dynamics embedded in the speech signal for the task of Speech Emotion Recognition (SER). Reconstruction of the phase space of each speech frame and the computation of its respective Recurrence Plot (RP) reveals complex structures which can be measured by performing Recurrence Quantification Analysis (RQA). These measures are aggregated by using statistical functionals over segment and utterance periods. We report SER results for the proposed feature set on three databases using different classification methods.  When fusing the proposed features with traditional feature sets, e.g., \cite{schuller2010interspeech}, we show an improvement in unweighted accuracy of up to 5.7\% and 10.7\% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks, respectively, over the baseline \cite{schuller2010interspeech}. Following a segment-based approach we demonstrate state-of-the-art performance on IEMOCAP using a Bidirectional Recurrent Neural Network. 


\end{abstract}
\noindent\textbf{Index Terms}: speech emotion recognition, recurrence quantification analysis, nonlinear dynamics, recurrence plots

\section{Introduction}
Automatic Speech Emotion Recognition (SER) is key for building intelligent human-machine interfaces that can adapt to the affective state of the user, especially in cases like call centers where no other information modality is available \cite{shridialogsystems}. 

Extracting features capable of capturing the emotional state of the speaker is a challenging task for SER. Prosodic, spectral and voice quality Low Level Descriptors (LLDs), extracted from speech frames, have been extensively used for SER \cite{el2011survey}. Proposed SER approaches mainly differ on the aggregation and temporal modeling of the input sequence of LLDs. 
In utterance-based approaches, statistical functionals are applied over all LLD values of the included frames \cite{schuller2010interspeech}. 
These utterance-level statistical representations have been successfully used for SER using Support Vector Machines (SVMs) \cite{shri2011SVM}, Convolutional Neural Networks (CNNs) \cite{emily2017regionalsaliency} and Deep Belief Networks (DBNs) in a multi-task learning setup \cite{xia2017multi}. 
Moreover, segment-based approaches have showcased that computation of statistical functionals over LLDs in appropriate timescales yields a significant performance improvement for SER systems \cite{schuller2006timing}, \cite{tzinis2017segment}. Specifically, in \cite{tzinis2017segment} statistical representations are extracted from overlapping segments, each one corresponding to a couple of words. The resulting sequence of segments representations is fed as input to a Long Short Time Memory (LSTM) unit for SER classification.

Direct SER approaches are usually based on raw LLDs extracted from emotional utterances. CNNs \cite{fayek2017stateoftheart} and Bidirectional-LSTMs (BLSTMs) \cite{ghosh2016representation} over spectrogram representations reported state-of-the-art performances on Interactive Emotional Dyadic Motion Capture (IEMOCAP) database \cite{IEMOCAP}. LSTMs with attention mechanisms have also been proposed in order to accommodate an active selection of the most emotionally salient frames \cite{shri2016attention}, \cite{mirsamadi2017attention}. To this end, Sparse Auto-Encoders (SAE) for learning salient features from  spectrograms of emotional utterances have also been studied \cite{mao2014autoencoder}.      

Despite the great progress that has been made in SER, the aforementioned LLDs are extracted under the assumption of a linear source-filter model of speech generation. However, vocal fold oscillations and vocal tract fluid dynamics often exhibit highly nonlinear dynamical properties which might not be aptly captured by conventional LLDs  \cite{herzel1993bifurcations}. Nonlinear analysis of a speech signal through the reconstruction of its corresponding Phase Space (PS) lies in embedding the signal in a higher dimensional space where its dynamics are unfolded \cite{pitsikalis2009analysis}. Recurrent patterns of these orbits are indicative attributes of system's behavior and can be analyzed using Recurrence Plots (RPs) \cite{RPsfirst1987}. Recurrence Quantification Analysis (RQA) provides complexity measures for an RP which are capable of identifying a system's transitions between chaotic and order regimes \cite{marwan2007recurrenceAll}. A variety of nonlinear features like: Teager Energy Operator \cite{TEOsun2011}, modulation features from instantaneous amplitude and phase \cite{traliarismaragos} as well as geometrical measures from PS orbits \cite{MeasuresonRPS2015speech} have been reported to yield significant improvement on SER when combined with conventional feature sets. However, RQA analysis has not yet been employed for SER. In \cite{SER_RQA_2016exploring} RQA measures have been shown to be statistically significant for the discrimination of emotions but an actual SER experimental setup is missing.

In this paper, we extract RQA measures from speech-frames and evaluate them for SER. We test the efficacy of the proposed RQA feature set under both utterance and segment-based approaches by calculating statistical functionals over the respective time lengths. SVMs and Logistic Regression (LR) classifiers are used for the utterance-based approach as well as an Attention-BLSTM (A-BLSTM) for the respective segment-based approach. The performance of the proposed RQA feature set, as well as the fusion of the RQA features with conventional feature sets \cite{schuller2010interspeech},  is reported on three databases and compared with state-of-the-art results for Speaker-Dependent (SD), Speaker-Independent (SI) and Leave One Session Out (LOSO) SER experiments. 




\section{Feature Extraction}
\label{sect:FeatureExtraction}

\subsection{Baseline Feature Set (IS10 Set)}
\label{sect:IS10featureset}
We use the IS10 feature set \cite{schuller2010interspeech}, in which  features are extracted corresponding to statistical functionals applied on various LLDs. The extraction is performed for both segment and utterance based approaches using the openSMILE toolkit \cite{opensmile}. 

\subsection{Proposed Nonlinear Feature Set (RQA Set)}
\label{sect:RQAFeatureSet}
The RQA feature set for a given speech segment or utterance is extracted as described next. First, we break the given speech signal into frames and for each one we reconstruct its PS as shown in Section~\ref{sect: Phase Space Reconstruction}. For each PS orbit, its respective RP is computed as explained in Section~\ref{sect: Recurrence Plot Computation}. In order to quantify the complex structures of the RP, a list of RQA measures (described in Section~\ref{sect: RQA measures}) is extracted; resulting in a -dimensional representation of the input speech frame. Representations for speech-segments and utterances containing multiple frames are obtained by applying a set of  statistical functionals (listed in Section~\ref{sect: RQA stats}) over  -dimensional frame-attributes and their deltas. Thus, a -dimensional feature vector is obtained.   



\subsubsection{Phase Space Reconstruction}
\label{sect: Phase Space Reconstruction}
Given a speech frame with  samples  we reconstruct its corresponding PS trajectory by computing  time-delayed versions of the original speech frame by multiples of time lag  and creating the vectors lying in  as shown next:    

where  is the embedding dimension of the reconstructed PS and  is the time lag. If the embedding theorem holds and the aforementioned parameters are set appropriately, then the orbit defined by the points  would truthfully preserve invariant quantities of the true underlying dynamics which are assumed to be unknown \cite{TakensTheorem}. In accordance with \cite{pitsikalis2009analysis}, parameters  and  for each speech frame are estimated individually by using Average Mutual Information (AMI) \cite{AMI} and False Nearest Neighbors (FNN) \cite{FalseNearestNeighbors}, respectively.

\subsubsection{Recurrence Plot}
\label{sect: Recurrence Plot Computation}
Given a PS trajectory  we analyze the recurrence properties of these states by calculating the pairwise distances and thresholding these values in order to compute the corresponding RP \cite{RPsfirst1987}. RPs are binary square matrices and are defined element-wise as shown next:

where  is the Heaviside step function,  is the thresholding value,  is the norm used to define the distance between trajectory points (for ,  or  we compute Manhattan, Euclidean or Supremum norm, respectively). Thus, matrix  consists of ones in areas where the states of the orbit are close and zero elsewhere. The measure of proximity is defined by threshold  for which multiple selection criteria have been studied \cite{schinkel2008eselection}. We consider three criteria depending on: 1) a fixed ad-hoc threshold value, 2) a fixed Recurrence Rate (RR) as defined in Table \ref{t:RQAMeasures} (e.g., For  we set  according to a fixed probability of the pairwise distances of PS's points ), and 3) a fixed ratio of the standard deviation  of points , e.g.,  \cite{stdEselection}. For fixed values of  and  we denote as  the respective entry of the RP matrix for simplicity of notation. \newline
An -length diagonal line (of ones) is defined by: 

An -length vertical line is described by:  

An -length white vertical line (of zeros) is defined as:  

We also denote with ,  and  the histogram distributions of lengths of diagonal, vertical and white vertical lines, respectively. Hence, the total number of these lines are correspondingly ,  and , where ,  and  define the minimum lengths for each type of line \cite{marwan2007recurrenceAll}. 

Emerging small-scale structures based on lines of ones or zeros reflect the dynamic behavior of the system. For instance, diagonal lines indicate both similar evolution of states for different parts of PS's orbit and  deterministic chaotic dynamics of the system \cite{marwan2007recurrenceAll}. This is also depicted in Figure \ref{fig:RPS-RPvisualization}. 

\begin{figure}[ht]
        \centering
        \begin{subfigure}[ht]{\linewidth}
           \begin{subfigure}[h]{0.32\linewidth}
              \includegraphics[width=\linewidth, height=2.5cm]{RPS_ee.png}
              \caption{}
              \label{fig:RPS}
          \end{subfigure}
          \begin{subfigure}[h]{0.32\linewidth}
              \includegraphics[width=\linewidth]{ee_RP.png}
              \caption{}
              \label{fig:RP}
          \end{subfigure}
          \begin{subfigure}[h]{0.32\linewidth}
              \includegraphics[width=\linewidth]{Lorenz96_RP.png}
              \caption{}
              \label{fig:RPLorenz}
          \end{subfigure}
        \end{subfigure}
        \caption{(a) Reconstructed PS (, ) and (b) RP (, Manhattan norm) of ms frame corresponding to vowel /e/. (c) RP of Lorenz96 system displaying chaotic behavior \cite{Lorenz96}}
        \label{fig:RPS-RPvisualization}
        
\end{figure}



\subsubsection{Recurrence Quantification Analysis (RQA)}
\label{sect: RQA measures}
For each  RP we extract  RQA measures using the pyunicorn framework \cite{pyunicorn}. Following the notation established in Section~\ref{sect: Recurrence Plot Computation} we provide an overview of these measures in Table \ref{t:RQAMeasures}; they are comprehensively studied in \cite{marwan2007recurrenceAll}, \cite{RQAExtended2005}. 

\begin{table}[ht]
	\caption{Recurrence Quantification Analysis Measures}
    \label{t:RQAMeasures}
	\centering
    \renewcommand{\arraystretch}{1.6}


\begin{tabular}{|c|c|}
\hline
        \textbf{Name} & \textbf{Formulation} \\
        \hline
    	Recurrence Rate  &  \\
        Determinism &  \\
        Max Diagonal Length & 
          \\     
        Average Diagonal Length & 
          \\ 
        Diagonal Entropy &  \\
        Laminarity &  \\
        Max Vertical Length & 
          \\     
        Trapping Time & 
          \\ 
        Vertical Entropy &  \\
        Max White Vertical Length & 
          \\     
        Average White Vertical Length & 
          \\ 
        White Vertical Entropy &  \\
        \hline
	\end{tabular}
\end{table}

\subsubsection{Statistical Functionals}
\label{sect: RQA stats}
After the extraction of frame-wise features and their associated deltas we apply  statistical functionals: min, max, mean, median, variance, skewness, kurtosis, range,  , , , , , ,  percentile and  quartile ranges.     

\subsection{Fused Feature Set (RQA + IS10 Set)}
\label{sect:FusedFeatureSet}
For any emotional speech segment or utterance we extract both feature sets IS10 and RQA as described previously and concatenate them. The final feature vector has  dimensions. 

\section{Classification Methods}
\label{sect:ClassificationMethods}
We investigate both  utterance-based and  segment-based   SER  as outlined below: \vspace*{1mm} 

\noindent\textbf{Utterance-based method:} For each utterance we obtain its statistical representation by extracting the corresponding feature set as described in Section~\ref{sect:FeatureExtraction}. For emotion classification we employ an SVM with Radial Base Function (RBF) kernel and one-versus-rest LR classifier. Cost coefficient  lies in the interval  for both SVM and LR models which is the only hyper-parameter to be tuned. Both models are implemented using the scikit-learn framework \cite{scikit-learn}. \vspace*{1mm}

\noindent\textbf{Segment-based method:} We break each utterance into segments of  s length and  s stride in accordance with \cite{tzinis2017segment}. For each speech segment we extract the feature sets described in Section~\ref{sect:FeatureExtraction} and as a result each utterance is now represented by a sequence of statistical vectors corresponding to different time steps. This sequence is fed as an input to a Long Short Time Memory (LSTM) unit for  emotion classification. SER can be formulated as a many-to-one sequence learning where the expected output of each sequence of segment features is an emotional label derived from the activations of the last hidden layer \cite{shri2016attention}. We employ an A-BLSTM architecture \cite{mirsamadi2017attention} where the decision for the emotional label is derived from a weighted aggregation of all timesteps. We implement this architecture in pytorch \cite{pytorch}. In addition, the grid space of hyper-parameters consists of: number of layers , number of hidden nodes , input noise , dropout rate  and learning rate .    




\section{Experiments and Results}

The following databases are used in our experiments: \vspace*{1mm}

\noindent\textbf{SAVEE:} Surrey Audio-Visual Expressed Emotion (SAVEE) Database \cite{SAVEE} is composed of emotional speech voiced by  male actors. SAVEE includes  utterances ( utterances per actor) of  emotions i.e.,  anger,  disgust,  fear,  happiness,  sadness,  surprise and  neutral.\vspace*{1mm}

\noindent\textbf{Emo-DB:} Berlin Database of Emotional Speech (Emo-DB) \cite{EmoDB} contains  emotional sentences in German, voiced by  actors ( male and  female). Specifically,  emotions are included i.e.,  anger,  disgust,  fear,  joy,  sadness,  boredom and  neutral.\vspace*{1mm}

\noindent\textbf{IEMOCAP:} IEMOCAP database \cite{IEMOCAP} contains  hours of video data of scripted and improvised dialog recorded from  actors. Utterances are organized in  sessions of dyadic interactions between  actors. For our experiments we consider  utterances including 4 emotions (1103 angry, 1636 happy, 1708 neutral and 1084 sad), where we merge excitement and happiness class into the latter one \cite{emily2017regionalsaliency}, \cite{xia2017multi}, \cite{fayek2017stateoftheart}, \cite{ghosh2016representation}. \vspace*{1mm}

We evaluate our proposed feature set under three different SER tasks described next. We also compare our results with the most relevant experimental setups reported in the literature. For all tasks, we report: Weighted Accuracy (WA) which is the percentage of correct classification decisions and Unweighted Accuracy (UA) which is calculated as the average of recall percentage for each emotional class. 

After an extensive study of the RQA configuration parameters described in Section~\ref{sect: Recurrence Plot Computation}, we conclude that best results on SER tasks are obtained using a frame duration of  ms for extracting RPs. In addition, the best performing parameters for the RP configuration seem to be a Manhattan norm with a threshold setting depending on a fixed recurrence rate lying in .      


\subsection{Speaker Dependent (SD)}
\label{sect:SDExperiments}
We evaluate RQA features on SAVEE and Emo-DB following the utterance-based approach described in Section~\ref{sect:ClassificationMethods}. In this setup we apply per-speaker -normalization (PS-N) and split randomly utterances in train and test sets. Accuracies using -fold cross-validation are summarized on Table~\ref{t:sd} for the best performing classifier hyper-parameter values.

The fused set achieves significant performance improvement over the baseline IS10 feature set for both datasets. On SAVEE, WA is improved by  ()  and UA by  ().  We also achieve an improvement of  () and  () for WA and UA, respectively on Emo-DB. The feature set used in \cite{sun2017ensemble} is extracted over cepstral, spectral and prosodic LLDs similar to the ones used in IS10 \cite{schuller2010interspeech}. Noticeably, they achieve similar performance to ours when we use only IS10 but our fused set with LR outperforms on both Emo-DB ( in UA and  in WA) and SAVEE ( in UA and  in WA). The proposed combination of features and LR also surpasses a Convolutional SAE approach \cite{mao2014autoencoder} in terms of WA by  on Emo-DB and  on SAVEE. Presumably, RQA measures contain information closely related to speaker-specific emotional dynamics not captured by conventional features.

\begin{table}[h]
	\caption{SD results on SAVEE and Emo-DB. (ESR) Ensemble Softmax Regression}
    \label{t:sd}
	\centering


\begin{tabular}{cccccc}
\hline
    	\multirow{2}{*}{Features} & \multirow{2}{*}{Model}  & \multicolumn{2}{c}{SAVEE} & \multicolumn{2}{c}{Emo-DB} \\
&     & WA  & UA   & WA & UA  \\ 
		\hline
\multirow{2}{*}{IS10}     & SVM & 77.1 & 74.5 & 88.4 & 87.2\\ 
                                  & LR  & 74.4 & 71.8 & 87.4 & 86.3 \\ 
\hline
		\multirow{2}{*}{RQA}      & SVM & 66.0 & 63.0 & 81.8 & 80.4\\ 
                                  & LR  & 64.4 & 61.1 & 81.9 & 79.9 \\ 
\hline
        \multirow{2}{*}{\shortstack{RQA+IS10}} & SVM & 77.3 & 75.5 & 90.1 & 88.9 \\ 
                                  & LR  & \textbf{80.2} & \textbf{77.9} & \textbf{93.3} & \textbf{92.9} \\
\hline
        \cite{mao2014autoencoder} Spectrogram & SAE & 75.4 & {\raggedleft-}  & 88.3 & {\raggedleft-}   \\
        \cite{sun2017ensemble} LLDs Stats &  ESR & 76.3 & 73.4 & 88.7 & 87.9 \\	
        
        \hline
        
	\end{tabular}
\end{table}





\subsection{Speaker Independent (SI)}
\label{sect:SIExperiments}
Again, we follow the utterance-based approach described in Section~\ref{sect:ClassificationMethods} on both SAVEE and Emo-DB datasets but we do not make any assumptions for the identity of the user during training. We use leave-one-speaker-out cross validation, where one speaker is kept for testing and the rest for training. The mean and standard deviation are calculated only on training data and used for -normalization on all data. From now on we refer to this normalization as Per Fold-Normalization (PF-N). Table~\ref{t:si} presents accuracies averaged over all folds for the best performing classifier hyper-parameter values.

In comparison with the baseline IS10 feature set, the fused feature set obtains an absolute improvement of  and  on SAVEE as well as  and  on Emo-DB in terms of WA and UA, respectively. Furthermore, our fused set achieves higher performance on SAVEE ( in WA and  in UA) and slightly lower in Emo-DB compared to \cite{sun2017ensemble}. In \cite{sun2015weightedHuMoments} Weighted Spectral Features based on Hu Moments (WSFHM) are fused with IS10 on utterance-level which is similar to our approach. In direct comparison using the same model (SVM) we surpass the reported performance in terms of WA by  and  on SAVEE and Emo-DB, respectively. In addition, both RQA and IS10 sets achieve quite low performance on SAVEE. However, their combination yields an impressive performance improvement of  () in WA and  () in UA over IS10 when we use LR. Our results suggest that RQA measures preserve invariant aspects of nonlinear dynamics occurring in emotional speech and are shared across different speakers.        

\begin{table}[h]
	\caption{SI results on SAVEE and Emo-DB. (ESR) Ensemble Softmax Regression}
    \label{t:si}
	\centering


\begin{tabular}{cccccc}
\hline
    	\multirow{2}{*}{Features} & \multirow{2}{*}{Model}  & \multicolumn{2}{c}{SAVEE} & \multicolumn{2}{c}{Emo-DB} \\
&     & WA & UA   & WA & UA \\ 
\hline
        \multirow{2}{*}{IS10}     & SVM & 47.5 & 45.6 & 79.7 & 74.3\\ 
                                  & LR  & 48.5 & 43.1 & 76.1 & 71.9 \\ 
\hline
		\multirow{2}{*}{RQA}      & SVM & 45.6 & 41.1 & 70.9 & 64.2\\ 
                                  & LR  & 47.7 & 42.3 & 71.1 & 67.1 \\ 
\hline
        \multirow{2}{*}{\shortstack{RQA+IS10}} & SVM & 52.5 & 50.6 & 82.1 & 76.9 \\ 
                                               & LR  & \textbf{54.0} & \textbf{53.8} & 80.1 & 77.5 \\
\hline
	    \cite{sun2017ensemble} LLDs Stats &  ESR &
	51.5 & 49.3 & \textbf{82.4} & \textbf{78.7} \\
    \cite{sun2015weightedHuMoments} WSFHM+IS10 & SVM & 50.0 & {\raggedleft-} & 81.7 &  {\raggedleft-} \\
    \hline
	\end{tabular}
\end{table}





\subsection{Leave One Session Out (LOSO)}
In this task, we assume that the test-speaker identity is unknown but we are able to train our model considering other speakers who are recorded in similar conditions. We evaluate on both utterance and segment-based methods (described in Section~\ref{sect:ClassificationMethods}) on IEMOCAP. Given our assumption, we treat each of the  sessions as a speaker group \cite{IEMOCAP}. We use LOSO in order to create train and test folds. In each fold, we use  sessions for training and the remaining  for testing. For the testing session we use one speaker as testing set and the other for tuning the hyper-parameters of our models. We repeat the evaluation by reversing the roles of the two speakers. 
In the final assessment, we report the average performance obtained in terms of WA and UA obtained from all speakers \cite{emily2017regionalsaliency}, \cite{xia2017multi}, \cite{ghosh2016representation}. In order to be easily comparable with the literature we follow three different normalization schemes. We use the aforementioned PS-N and PF-N schemes as well as Global -normalization (G-N). In G-N we calculate the global mean and standard deviation from all the available samples in the dataset and perform -normalization over them. Results on IEMOCAP for the three different normalization schemes are demonstrated on Table \ref{t:iemo}.

A consistent performance improvement is shown for all combinations of normalization techniques and employed models when the fused set is used instead of IS10. Specifically, for SVM the fused set yields a relative improvement varying from  to  in WA and from  to  in UA under all normalization strategies. The same applies for LR (in WA from  to  and in UA from  to  ) as well as for A-BLSTM (in WA from  to  and in UA from  to ). In accordance with our intuition \cite{tzinis2017segment}, a segment-based approach using A-BLSTM surpasses all utterance-based ones in WA from  to  and in UA from  to  for all normalization schemes, when the fused set is used. 

In \cite{emily2017regionalsaliency} low level Mel Filterbank (MFB) features are fed directly to a CNN. In \cite{ghosh2016representation} a stacked autoencoder is used to extract feature representations from spectrograms of glottal flow signals and then a BLSTM is used for classification. We surpass both reported results by  in UA for \cite{emily2017regionalsaliency} and by a margin of  in WA and  in UA for \cite{ghosh2016representation}, respectively even with simple models. 
Compared to a multi-task DBN trained for both discrete emotion classification and for valence-activation in \cite{xia2017multi}, we report  higher WA and  higher UA. 
We also report  higher UA and  lower WA compared to CNNs over spectrograms \cite{fayek2017stateoftheart}. We assume that this inconsistency in performance metrics occurs because a slightly different experimental setup is followed where the final session is excluded from testing \cite{fayek2017stateoftheart}.   




\begin{table}[h]
	\caption{LOSO results on IEMOCAP. (GFS): Glottal Flow Spectrogram, (SP): Spectrogram.}
    \label{t:iemo}
	\centering


\begin{tabular}{ccK{0.33cm}K{0.33cm}K{0.33cm}K{0.33cm}K{0.33cm}K{0.33cm}}

\hline
    	\multirow{2}{*}{Features} & 
        \multirow{2}{*}{Model}  & 
        \multicolumn{2}{c}{PS-N} & 
        \multicolumn{2}{c}{PF-N} &
        \multicolumn{2}{c}{G-N} \\
		&     & WA & UA & WA & UA & WA & UA\\ 
        \hline
        \multirow{3}{*}{IS10}     
        & SVM & 58.3 & 60.9 & 58.9 & 60.1 & 59.2 & 60.5 \\
        & LR & 57.5 & 61.2 & 54.6 & 57.9 & 53.5 & 57.5 \\
        & A-BLSTM & 62.0 & 65.1 & 62.6 & 65.0 & 62.8 & 65.0 \\
        \hline
		\multirow{3}{*}{RQA}      
        & SVM & 52.9 & 54.6 & 53.1 & 53.8 & 53.1 & 53.7\\
        & LR & 52.2 & 54.8 & 52.6 & 54.0 & 52.8 & 54.3 \\
		& A-BLSTM  & 55.6 & 59.3 & 56.6 & 58.3 & 56.7 & 58.7 \\
		\hline
RQA
        & SVM & 59.3 & 61.8 & 59.2 & 60.4 & 59.5 & 60.7 \\
         +& LR & 58.3 & 62.0 & 55.6 & 58.7 & 54.5 & 58.7 \\
        IS10& A-BLSTM & \textbf{62.7} & \textbf{65.8} & \textbf{63.0} & \textbf{65.2} & 62.9 & \textbf{65.5} \\
        
        \hline 
        \cite{emily2017regionalsaliency} MFB & CNN
& {\raggedleft-} & 61.8 & {\raggedleft-} & {\raggedleft-} & {\raggedleft-} & {\raggedleft-} \\
\cite{xia2017multi} IS10 & DBN & {\raggedleft-} & {\raggedleft-} & {\raggedleft-} & {\raggedleft-} & 60.9 & 62.4 \\
\cite{fayek2017stateoftheart} SP & CNN & {\raggedleft-} & {\raggedleft-} & {\raggedleft-} & {\raggedleft-} & \textbf{64.8} & 60.9 \\
        \cite{ghosh2016representation} GFS & BLSTM & {\raggedleft-} & {\raggedleft-} & 50.5 & 51.9 & {\raggedleft-} & {\raggedleft-} \\
       
        
        \hline

        
        
	\end{tabular}
\end{table}

\section{Conclusions}
We investigated the usage of nonlinear RQA measures extracted from RPs for SER. The effectiveness of these features has been tested under both utterance-based and segment-based approaches across three emotion databases. The fusion of nonlinear and conventional feature sets yields significant performance improvement over traditional feature sets for all SER tasks; the performance improvement is especially large when speaker identity is unknown. The fused data set improves on the state-of-the-art for SER under most testing conditions, classification methods and datasets. Recurrence analysis of speech signals is a promising direction for SER research. In the future, we plan to automatically extract features from RPs using convolutional autoencoders in order to substitute RQA measures. 





















































\section{Acknowledgements}
This work has been partially supported by the BabyRobot project supported by EU H2020 (grant \#687831). Special thanks to Nikolaos Athanasiou and Nikolaos Ellinas for their contributions on the experimental environment setup.


\bibliographystyle{IEEEtran}

\bibliography{mybib}



\end{document}