\section{Experiments}


We evaluate our proposed approach for weakly-supervised 3D body pose learning and compare it with the state-of-the-art methods.
Additional training and implementation details can be found in Appendix~\ref{sec:implementation_details}.


\subsection{Datasets}
We use two large-scale datasets, Human3.6M~\cite{h36m_pami} and MPII-INF-3DHP~\cite{mono20173dhp} for evaluation. For weakly-supervised training, we also use the MannequinChallenge dataset~\cite{li2019mannequin} 
and 
MPII Human Pose dataset~\cite{andriluka14cvpr}. The details of each dataset are as follows.

\noindent\textbf{Human3.6M (H36M)}~\cite{h36m_pami} 
provides images of actors performing a variety of actions from four views. 
We follow the standard protocol and use five subjects (S1, S5, S6, S7, S8) for training and test on two subjects (S9 and S11).

\noindent\textbf{MPII-INF-3DH (3DHP)}~\cite{mono20173dhp} provides ground-truth 3D poses obtained using markerless motion-capture system.
Following the standard protocol~\cite{mono20173dhp}, we use five chest height cameras for training. The test-set consists of six sequences with actors performing a variety activities. 

\noindent\textbf{MannequinChallenge Dataset (MQC) }~\cite{li2019mannequin} provides in-the-wild videos of people in static poses while a hand-held camera pans around the scene. The videos do not come with any ground-truth annotations, however, the data is very adequate for our proposed weakly-supervised approach using multi-view consistency. The dataset consists of three splits for training, validation and testing. In this paper, we use  videos from training and validation set as proposed by~\cite{li2019mannequin}, but in practice one could download an immense amount of such videos from YouTube (\#MannequinChallenge). We will show in our experiments that using these in-the-wild videos during training yields better generalization,
in particular, when there is a significant domain gap between the training and testing set. Since the videos can have multiple people inside each frame, they have to be associated across frames to obtain the required multi-view data. To this end, we adopt the pose based tracking approach of~\cite{FlowTrack} and generate person tracklets from each video. For pose estimation, we use a HRNet-w32~\cite{SunXLW19} model pretrained on MPII Pose dataset~\cite{andriluka14cvpr}. In order to avoid training on noisy data, we discard significantly occluded or truncated people. We do this by  discarding all poses that have more than half of the estimated body joints with confidence score lower than a threshold . We also discard poses in which neck or pelvis joints have confidence lower than  since both joints are important for  reconstruction using~\eqref{eqt:normalization_constraint}. Finally, we discard all tracklets with the length lower than 5 frames. This gives us 11,413 multi-view tracklets with 241k images in total. The minimum and maximum length of the tracklets is 5 and 140 frames, respectively. 

\noindent\textbf{MPII Pose Dataset (MPII)~\cite{andriluka14cvpr}} provides 2D pose annotations for 28k in-the-wild images. 

\subsection{Evaluation Metrics}
For evaluation on H36M, we follow the standard protocols and use MPJPE (Mean Per Joint Position Error), N-MPJPE (Normalized-MPJPE) and  P-MPJPE (Procrustes-aligned MPJPE) for evaluations. MPJPE measures the mean euclidean error between the ground-truth and estimated location of 3D joints after root alignment. While NMPJPE~\cite{rohdin2018multiview} also aligns the scale of the predictions with ground-truths, PMPJPE aligns both the scale and rotations using Procrustes analysis. For evaluation on 3DHP dataset, we follow~\cite{mono20173dhp} and also report PCK (Percentage of Correct
Keypoints) and Normalized-PCK as defined in~\cite{rohdin2018multiview}. PCK measures the percentage of predicted body joints that lie within the radius of 150mm from their ground-truths. 3DHP evaluation protocol uses 14 joints for evaluation excluding the pelvis joint which is used for alignment of the poses.  

\subsection{Ablation Studies}

\begin{table}[t]
\centering
\small
\begin{tabularx}{1\columnwidth}{X|ccc|c|c}
\toprule
 \multirow{3}{3cm}{Method} &  \multicolumn{3}{c|}{Supervision} & \multicolumn{2}{c}{Error}  \\
            & ~~~2D~~~  & 3D    & MV    &  2D  px  & 3D mm \\
\midrule
\midrule
FS          & H+M     & H     & -     &        5.9  & 55.5  \\ 
\midrule
WS +      & H+M     & -     & H     &       6.1    & 57.2\\ 
WS          & H+M     & -     & H     &       6.1    & 59.3   \\ 
\midrule
2d-only     & M         & -     & -     &       8.9        & - \\ 
WS +     & M         & -     & H     &       8.3        & 62.3 \\ 
WS          & M         & -     & H              &       8.4        & 69.1 \\ 
WS                  & M         & -     & I    &      9.0    & 106.2\\ 
WS                  & M         & -     & I+Q    &      9.1    & 93.6 \\ 
WS                   & M         & -    & H+I+Q  &       8.4    & 67.4 \\ 
WS +     & M         & -    & H+I+Q    &       8.4    & 60.3\\ 
\bottomrule
\end{tabularx}
\vspace{1mm}
\caption{\textbf{Ablative study:} We provide results when different levels of supervision are used to train the proposed weakly-supervised method. FS: Fully-Supervised, WS: Weakly-Supervised, MV: Multi-View, H: H36M, M: MPII, I: 3DHP, Q: MQC. No 3D supervision is used for all experiments except FS. \vspace{-3mm}}
\label{tab:ablation}

\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[trim={0 0cm 0 0cm},clip,width=\textwidth]{images/impact_of_mqc_compressed.pdf}
    \vspace{-5mm}
    \caption{Impact of using MQC dataset. We run the trained models on the tracks taken from MQC dataset and align the estimated 3D poses using~\eqref{eqt:transformation_matrix}. Since people in MQC dataset do not move, the aligned poses should be very similar. Adding MQC dataset for training \textbf{(right)} yields
more consistent 3D pose estimates as compared to when only H36M is used \textbf{(left)} for multi-view consistency loss. Note that our proposed approach can also fix the errors in 2D pose estimates in the unlabeled multi-view data. \vspace{-3mm}}
    \label{fig:impact_of_mqc}
\end{figure*}


Tab.~\ref{tab:ablation} evaluates the impact of different levels of supervision for training with the proposed approach. We use  H36M for evaluation. We start with a fully-supervised setting (FS) which uses 2D supervision from H36M and MPII (2D=H+M) datasets and 3D pose supervision from H36M (3D=H). No multi-view (MV) data is used in this case. The fully-supervised model yields a MPJPE of 5.9px  and 55.5mm for 2D and 3D pose estimation, respectively. We then remove the 3D supervision and instead train the network using the proposed approach for weakly-supervised learning (WS+). The MV data is taken from H36M (MV=H). For this experiment, we assume that the 2D pose annotations for MV data are available (2D=H+M) and the camera extrinsics  are known. This setting is essentially similar to fully-supervised case since the 2D poses from different views can be triangulated using the known . Training the network under this setting, however, serves as a sanity check that the proposed weakly-supervised approach works as intended which can be confirmed by the obtained 3D pose error of 57.2mm.  If  is unknown (WS) and is obtained from estimated 3D poses using~\eqref{eqt:transformation_matrix}, the error increases slightly to 59.3mm. 

All of the aforementioned settings assume that the MV data is annotated with 2D poses which is infeasible to collect in large numbers. Therefore, we have designed the proposed method to work with MV data without even 2D annotations. Next, we remove the 2D supervision from MV data and only use MPII dataset for 2D supervision (2D=M). For reference, we also report the error of a 2D-only model trained on MPII dataset which yields a 2D pose error of 8.9px. 
Training without 2D pose annotations for MV data with and without ground-truth  yields errors of 62.3mm (WS+) and 69.1mm (WS), respectively, as compared to 57.2mm and 59.3mm when the 2D pose annotations are available. While using ground-truth  always yields better performance, for the sake of easier applicability, in the rest of this paper, we assume it to be unknown unless specified otherwise. It is also interesting to note that the 2D pose error decreases from 8.9px to 8.3px when the multi-view consistency loss~\eqref{eqt:loss_mc} is used. Some qualitative examples of improvements in 2D poses can be seen in~Fig.~\ref{fig:impact_of_mqc}.  

We also evaluate the case when the training data is recorded in different settings than testing data. For this, we use 3DHP for training (MV=I) and test on H36M. Since the images of 3DHP are very different from H36M, it leads to a very high error of 106.2mm. Adding the generated training data from MQC dataset (MV=I+Q) significantly reduces the error to 93.6mm which demonstrates the effectiveness of in-the-wild data from MQC. Combining all three datasets (MV=H+I+Q) reduces the error further to 67.4mm as compared to 69.1mm when only H36M dataset was used for training. We also provide the results when ground-truth  is known (WS+) for H36M and 3DHP datasets (MV=H+I+Q) which shows a similar behaviour and decreases the error from 62.3mm to 60.3mm. 

In our experiments, we found that training only on MQC dataset is not sufficient for convergence and it has to be combined with another dataset which provides multi-view data from more distant viewing angles. This is likely because most videos in MQC dataset do not capture same person from very different viewing angles, whereas datasets such as H36M and 3DHP provide images from cameras with sufficiently large baselines. 


\subsection{Comparison with the State-of-the-Art}

\begin{table}[t]
\scriptsize
\centering
\begin{tabularx}{1\columnwidth}{X|ccc}
\toprule
\bf Methods & \bf MPJPE\  & \bf NMPJPE\  & \bf PMPJPE\  \\
\midrule
\midrule
\multicolumn{4}{c}{Fully-Supervised Methods}\\
\midrule
Rogez \etal \cite{rogez2017lcr} (CVPR'17) & 87.7 & -  & 71.6  \\
Habibie \etal \cite{habibie2019wild} (ICCV'19) & - & 65.7 & - \\
Rhodin \etal \cite{rohdin2018multiview} (CVPR'18)& 66.8 & 63.3 & 51.6  \\
Zhou \etal \cite{zhou2017weakly} (ICCV'17) & 64.9 & - & - \\
Martinez \etal \cite{martinez2017simple} (ICCV'17) & 62.9 & - & 47.7  \\
Sun \etal~\cite{sun2017compositional} (ICCV'17) \bf* & 59.6 & - & - \\ 
Yang \etal~\cite{yang20183d} (CVPR'18)    & 58.6 & - & - \\
Pavlakos \etal \cite{pavlakos2018ordinal} (CVPR'18)& 56.2 & - & -  \\
Sun \etal~\cite{sun18integeral} (ECCV'18)\bf*  & \bf 49.6 & - & {40.6} \\ 
Kocabas \etal \cite{kocabas2019epipolar} (CVPR'19)\bf* & 51.8 & 51.6 & 45.0  \\
Ours - H - baseline & 55.5 &51.4   &41.5 \\
Ours\textbf{*} - H - baseline & 50.2 & \bf 49.9   & \bf 36.9 \\ 
Ours - H+I+Q & 56.1 & 52.7   & 45.9 \\
\midrule
\midrule
\multicolumn{4}{c}{Semi-Supervised Methods - only Subject-1 is used for training }\\
\midrule
Rohdin \etal \cite{rohdin2018geometry} (ECCV'18)  & 131.7 & 122.6 & 98.2 \\
Pavlakos \etal \cite{pavlakos2019texture} (ICCV'19) & 110.7 & 97.6 & 74.5 \\
Li \etal \cite{li2019boosting} (ICCV'19)          & 88.8  & 80.1  & 66.5 \\                     
Rhodin \etal \cite{rohdin2018multiview} (CVPR'18)  & n/a & 80.1  & 65.1 \\
Kocabas \etal \cite{kocabas2019epipolar} (CVPR'19) & n/a & 67.0 & 60.2 \\
Ours - H & 62.8 & 59.6   &51.4\\ 
Ours - H+I+Q & \bf 59.7 & \bf 56.2   & \bf 50.6 \\
\midrule
\midrule
\multicolumn{4}{c}{Weakly-Supervised Methods - no 3D supervision}\\
\midrule
Pavlakos \etal~\cite{pavlakos2017harvesting} (CVPR'17) & 118.4 & - & - \\
Kanzawa \etal \cite{hmrKanazawa18} (CVPR'18) & 106.8 & - & 67.5 \\
Wandt \etal~\cite{wandt2019repnet} (CVPR'19) & 89.9 & - & - \\
Tome \etal \cite{tome2017lifting} (CVPR'17) & 88.4 & - & -  \\   
Kocabas \etal \cite{kocabas2019epipolar}  (CVPR'19) & n/a & 77.75 & 70.67 \\ 
Chen \etal \cite{chen2019unsupervised} (CVPR'19) & - & - & 68.0 \\
Drover \etal \cite{drove2018can3d} (ECCV-W'18) & - & - & 64.6  \\
Kolotouros \etal \cite{kolotouros2019spin} (ICCV'19) & - & - & 62.0 \\
Wang \etal \cite{wang2019nrsfm} (ICCV'19) & 83.0 & - & 57.5 \\
Ours - H & 69.1 &66.3   &55.9 \\
Ours - H+I+Q & \bf 67.4 & \bf 64.5   & \bf 54.5  \\
\bottomrule
\end{tabularx}
\hspace{5mm}

\caption{ Comparison with the state-of-the-art on H36M dataset. \textbf{*}use ground-truth depth of the root keypoint during inference.\vspace{-3mm}}
\label{table:sota_h36m}
\end{table} 


Tab.~\ref{table:sota_h36m} compares the performance of our proposed method with the state-of-the-art on H36M dataset. We group all approaches in three categories; fully-supervised, semi-supervised, and weakly-supervised, and compare the performance of our method under each category.  While fully-supervised methods use complete training set of H36M for 3D supervision, semi-supervised methods use 3D supervision from only one subject (S1) and use other subjects (S5,  S6, S7,  S9) for weak supervision. Weakly-supervised methods do not use any 3D supervision. Some methods also use ground-truth information during inference~\cite{sun2017compositional,sun18integeral,kocabas2019epipolar}. For a fair comparison with those, we also report our performance under the same settings. It is important to note that, many approaches such as~\cite{rohdin2018geometry,rohdin2018multiview,martinez2017simple,rogez2017lcr, yang20183d,chen2019unsupervised,drove2018can3d} estimate root-relative 3D pose. Our approach, on the other hand, estimates absolute 3D poses. While our fully-supervised baseline (Ours-H-baseline) performs better or on-par with the state-of-the-art fully-supervised methods, our proposed approach for weakly-supervised learning   significantly outperforms other methods under both semi- and weakly-supervised categories. 

For a fair comparison with other methods, we report results of our method under two settings: i) using H36M and MPII dataset for training (Ours-H), and ii) with multi-view data from 3DHP and MQC as additional weak supervision (Ours-H+I+Q). In the fully-supervised case, using additional weak supervision slightly worsens the performance (55.5mm vs 56.1mm) which is not surprising on a dataset like H36M which is heavily biased to indoor data and have training and testing images recorded with a same background. Whereas, our approach, in particular the data from MQC, is devised for in-the wild generalization. The importance of additional multi-view data, however, can be seen evidently in the semi-/weakly-supervised settings where it decreases the error from 62.8mm to 59.7mm and from 69.1mm to 67.4mm, respectively. 


\begin{table}[t]
\scriptsize
\centering
\begin{tabularx}{1\columnwidth}{X|sstt}
\toprule
 \multirow{1}{3cm}{\bf Methods}             &  \textbf{\footnotesize MPJPE}    & \textbf{\footnotesize NMPJPE} &  \textbf{\footnotesize PCK}    &  \textbf{\footnotesize NPCK} \\
\midrule
\midrule
\multicolumn{5}{c}{Fully-Supervised Methods}\\
\midrule
Mehta \etal~\cite{mehta2017vnect}           & -                 & -                  & 76.6              & - \\
Rohdin \etal~\cite{rohdin2018multiview}     & n/a               & 101.5              & n/a               & 78.8 \\
Kocabas \etal~\cite{kocabas2019epipolar}*   & 109.0             & 106.4              & 77.5              & 78.1 \\
Ours                                        &  \bf110.8 & \bf98.9 & \bf80.2 &  \bf82.3 \\
Ours*                                       &  \bf99.2  & \bf97.2 & \bf83.0 &  \bf83.3 \\
\midrule
\midrule
\multicolumn{5}{c}{Semi-Supervised Methods}\\
\midrule
Rhodin \etal \cite{rohdin2018multiview}     & n/a               & 121.8             & n/a               & 72.7 \\
Kocabas \etal \cite{kocabas2019epipolar}    & n/a               & 119.9             & n/a               & 73.5  \\
Ours                                        & \bf 113.8         & \bf 102.2         & \bf 79.1          & \bf 81.5 \\
\midrule
\midrule
\multicolumn{5}{c}{Weakly-Supervised Methods}\\
\midrule
Kanazwa~\etal~\cite{hmrKanazawa18}          & 169.5             & -                 & 59.6              & - \\
Kolotouros \etal \cite{kolotouros2019spin} & 124.8              & -                 & 66.8              & - \\
Ours                                        & \bf 122.4         & \bf 110.1         & \bf 76.5          & \bf 79.4 \\
\midrule
Kocabas~\textit{et al.}~\cite{kocabas2019epipolar}\textbf{*} +  & 126.8 & 125.7   & 64.7              & 71.9 \\
Ours\textbf{*} +                &  \bf 109.3        & \bf 107.2         & \bf 79.5 & \bf 80.0 \\
\bottomrule
\end{tabularx}
\caption{Comparison with the state-of-the-art on 3DHP dataset. \textbf{*}use ground-truth 3D location of the root joint during inference.\vspace{-3mm}} 
\label{table:sota_3dhp}
\end{table} 


Compared to the state-of-the-art method~\cite{kocabas2019epipolar} that also uses multi-view information for weak supervision, our method performs significantly better even though the fully-supervised baselines of both approaches perform similar. This demonstrates the effectiveness of our end-to-end training approach and proposed loss functions that are robust to errors in 2D poses. 
While our weakly-supervised approach does not outperform fully-supervised methods, it performs on-par with many recent fully-supervised approaches. 




Tab.~\ref{table:sota_3dhp} compares the performance of our proposed approach with the state-of-the-art on 3DHP dataset. We use our models trained with \textit{Ours-H+I+Q} setting, as described above. We do not use any 3D pose supervision from 3DHP dataset and instead use the same models used for evaluation on H36M dataset. Our proposed approach outperforms all existing methods with large margins under all three categories which also demonstrates the cross dataset generalization of our proposed method. 


Some qualitative results of the proposed approach can be seen in the supplementary material.


