\documentclass[journal]{IEEEtran}

\usepackage[backend=bibtex,style=ieee,minbibnames=1,maxbibnames=4,mincitenames=1,maxcitenames=2]{biblatex}
\addbibresource{References/References.bib}

\usepackage[table,xcdraw]{xcolor}
\usepackage{float}
\usepackage{array}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{siunitx}
\usepackage[psamsfonts]{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{amsxtra}
\usepackage{stmaryrd}
\usepackage{accents}
\usepackage{bm}
\usepackage{upgreek}
\usepackage{etoolbox}
\usepackage[shortcuts]{extdash}
\usepackage{scalerel}
\usepackage{microtype}
\usepackage[keeplastbox]{flushend}
\usepackage{soul}
\usepackage{color}
\usepackage{listings}
\usepackage[scaled=0.8]{beramono} 
\usepackage[T1]{fontenc}
\usepackage{xurl}
\usepackage[breaklinks=true,hidelinks]{hyperref}
\hypersetup{
    colorlinks   = true,
    urlcolor     = blue,
    linkcolor    = blue,
    citecolor    = blue
}
\usepackage{pdfrender}
 
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\renewcommand{\arraystretch}{1.2} 
\newcommand*{\boldcheckmark}{\textpdfrender{
        TextRenderingMode=FillStroke,
        LineWidth=.5pt, 
    }{\checkmark}}
\definecolor{codegray}{gray}{0.9}
\begingroup\lccode`\|=`\\
\lowercase{\endgroup\def\removebs#1{\if#1|\else#1\fi}}
\newcommand{\detokenizebackslash}[1]{\expandafter\removebs\string#1}
\makeatletter
\def\@code {\futurelet\@nexttoken\@codeaux}
\def\@codeaux {\ifx\@nexttoken\egroup\else
    \ifx\@nexttoken\bgroup
    \expandafter\expandafter\expandafter\@codea\else
    \expandafter\expandafter\expandafter\@codeb\fi\fi}
\def\@codea #1{{#1}\@code}
\def\@codeb #1{\ifcat\@nexttoken a\penalty\hyphenpenalty \codehook
    #1\else \codehook{#1}\linebreak[2]\fi\@code}
\newcommand{\highlighttt}[1]{{\fontfamily{txtt}\selectfont
        \@highlighttt #1.}}
\newcommand\code{\bgroup\ttfamily\selectfont
    \afterassignment\@code\let\next= }
\makeatother
\newcommand{\myhighlightmethod}[1]{\fboxsep0pt\colorbox{codegray}{\strut#1}}
\newcommand{\codehook}[1]{\myhighlightmethod{\detokenizebackslash{#1}}}
\lstnewenvironment{codeblock}{\lstset{backgroundcolor=\color{codegray},
        frame=single,
        framerule=0pt,
        basicstyle=\ttfamily,
        mathescape=true,
        linewidth=0.49\textwidth,
        columns=fullflexible}}{}
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
\DeclareMathAccent{\wideparen}{0}{mathx}{"75}
\makeatletter
\let\save@mathaccent\mathaccent
\newcommand*\if@single[3]{\setbox0\hbox{}\setbox2\hbox{}\ifdim\ht0=\ht2 #3\else #2\fi
}
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
\newcommand*\widebar[1]{\@ifnextchar^{{\wide@bar{#1}{0}}}{\wide@bar{#1}{1}}}
\newcommand*\wide@bar[2]{\if@single{#1}{\wide@bar@{#1}{#2}{1}}{\wide@bar@{#1}{#2}{2}}}
\newcommand*\wide@bar@[3]{\begingroup
\def\mathaccent##1##2{\let\mathaccent\save@mathaccent
\if#32 \let\macc@nucleus\first@char \fi
\setbox\z@\hbox{}\setbox\tw@\hbox{}\dimen@\wd\tw@
\advance\dimen@-\wd\z@
\divide\dimen@ 3
\@tempdima\wd\tw@
\advance\@tempdima-\scriptspace
\divide\@tempdima 10
\advance\dimen@-\@tempdima
\ifdim\dimen@>\z@ \dimen@0pt\fi
\rel@kern{0.6}\kern-\dimen@
\if#31
	\overline{\rel@kern{-0.6}\kern\dimen@\macc@nucleus\rel@kern{0.4}\kern\dimen@}\advance\dimen@0.4\dimexpr\macc@kerna
	\let\final@kern#2\ifdim\dimen@<\z@ \let\final@kern1\fi
	\if\final@kern1 \kern-\dimen@\fi
\else
	\overline{\rel@kern{-0.6}\kern\dimen@#1}\fi
}\macc@depth\@ne
\let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
\mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}\macc@set@skewchar\relax
\let\mathaccentV\macc@nested@a
\if#31
	\macc@nested@a\relax111{#1}\else
	\def\gobble@till@marker##1\endmarker{}\futurelet\first@char\gobble@till@marker#1\endmarker
	\ifcat\noexpand\first@char A\else
		\def\first@char{}\fi
	\macc@nested@a\relax111{\first@char}\fi
\endgroup
}
\makeatother
 
\newcommand{\norm}[1]{\big\lVert{#1}\big\rVert} 

\newcommand{\DOWNLOADURL}{https://github.com/facebookresearch/EasyComDataset}



\begin{document}

\title{\huge EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments}

\author{\url{\DOWNLOADURL}\vskip1.0em\IEEEauthorblockN{Jacob~Donley\IEEEauthorrefmark{1},
		Vladimir~Tourbabin\IEEEauthorrefmark{1},
		Jung-Suk~Lee\IEEEauthorrefmark{1},
		Mark~Broyles\IEEEauthorrefmark{1},
	  \\Hao~Jiang\IEEEauthorrefmark{1},
		Jie~Shen\IEEEauthorrefmark{2},
		Maja~Pantic\IEEEauthorrefmark{2},
		Vamsi~Krishna~Ithapu\IEEEauthorrefmark{1},
		Ravish~Mehra\IEEEauthorrefmark{1}}\vskip0.5em\IEEEauthorblockA{\IEEEauthorrefmark{1}Facebook Reality Labs Research, USA.}\qquad \IEEEauthorblockA{\IEEEauthorrefmark{2}Facebook AI Applied Research, UK.}\thanks{Please send correspondence to: \href{mailto:EasyComDataset@fb.com}{EasyComDataset@fb.com}}}


\maketitle


\begin{abstract}
	
    Augmented Reality (AR) as a platform has the potential to facilitate the reduction of the \emph{cocktail party effect}.
    Future AR headsets could potentially leverage information from an array of sensors spanning many different modalities.
    Training and testing signal processing and machine learning algorithms on tasks such as beam-forming and speech enhancement require high quality representative data.
    To the best of the author's knowledge, as of publication there are no available datasets that contain synchronized egocentric multi-channel audio and video with dynamic movement and conversations in a noisy environment.
    In this work, we describe, evaluate and release a dataset that contains over 5 hours of multi-modal data useful for training and testing algorithms for the application of improving conversations for an AR glasses wearer.
    We provide speech intelligibility, quality and signal-to-noise ratio improvement results for a baseline method and show improvements across all tested metrics.
    The dataset we are releasing contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head bounding boxes, target of speech and source identification labels.
    We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.
    
\end{abstract}

\begin{IEEEkeywords}
    augmented reality (AR), cocktail party, beam-forming, speech enhancement, egocentric, audio, video, dataset
\end{IEEEkeywords}
 
\IEEEpeerreviewmaketitle

\section{Introduction} \label{sec:Introduction}

\IEEEPARstart{T}{he} ability to communicate in noisy environments is a challenge for many people and has been a popular research topic for many decades.
The difficulty that arises is often referred to as the \emph{cocktail party effect}~\cite{cherry_experiments_1953,bronkhorst_cocktail_2000}.
As our world moves towards a new era of computing in augmented reality (AR) and virtual reality (VR) there are many benefits to be had from an all day wearable computing device, including the potential to eliminate the cocktail party effect once and for all.
The biggest obstacle to developing systems to overcome this effect is a lack of realistic data that reflects the specific scenario and provides multi-modal information.
Machine learning and signal processing algorithms can benefit from leveraging high quality multi-sensor data,
however, until now, there has not been a single dataset that provides all relevant sensor data from an AR headset in an environment that induces the cocktail party effect.

A critically differentiating aspect of an AR headset-based dataset is its egocentric nature and the associated dynamic movement.
There are few egocentric datasets publicly available that also provide data useful for solving the cocktail party effect.
The Epic Kitchens dataset~\cite{Damen2018EPICKITCHENS,damen2020rescaling}, originally released in 2018 and an extended version released in 2020, contains egocentric video and single channel audio recordings of activities that took place in kitchens.
The environments that were recorded do not contain noise or conversations that are useful for solving the cocktail party problem.
The COSINE speech dataset~\cite{stupakov2009cosine}, released in 2009, contains egocentric audio recordings with seven microphones and in naturally noisy environments.
However, the dataset does not contain other sensor modalities, such as video, nor does it contain annotated labels or data from a head mounted device.
The EgoCom dataset~\cite{northcutt2020egocom}, released in 2020, contains egocentric video and two channel audio recordings of conversations from pairs of glasses.
However, the dataset does not contain acoustic noise necessary for the problem, there is no clock synchronization between devices and the multi-channel microphone data is lossy compressed.
The Amazon Dinner Party Corpus (DiPCo)~\cite{vansegbroeck2019dipco}, released in 2019, contains 39 channels of audio as well as annotations to specifically help address the cocktail party problem in the form of a dinner party.
The CHiME-5~\cite{barker2018fifth} and CHiME-6~\cite{watanabe2020chime6} challenge datasets, released in 2018 and re-run in 2019, respectively, contain 32 audio channels of 4 participants having a dinner party in private homes.
The DiPCo, CHiME-5 and CHiME-6 datasets, however, do not contain any video or rigid wearable microphone array recordings.
The lack of egocentric perspective limits the use of the datasets and does not facilitate solving the cocktail party problem for users of wearable devices.


\setlength{\dashlinedash}{0.2pt}
\setlength{\dashlinegap}{1.5pt}
\setlength{\arrayrulewidth}{0.2pt}

\begin{table*}[htb!]
    \begin{center}
        \caption{Types of data included in existing datasets and EasyCom.} \label{tbl:dataset_comparison_datatypes}
        \begin{tabular}
            {@{}  l  c c c c c c c c   @{}}        
            \toprule
            \multirow{2}{*}{Name}                       &
            \multirow{2}{*}{Year}                       &
            \multirow{2}{*}{\makecell{\# Mics\
& \min\sum_{(i,j)}(c_{i,j}-t)x_{i,j}\\
s.t. & \sum_{i}x_{i,j}\leq1,\sum_{j}x_{i,j}\leq1\\
& x_{i,j}\mbox{ is binary }\forall i,j.

c_{i,j}=\norm{{\bf p}_{j}-H({\bf p}_{i})}_2+\alpha d({\bf b}_{j},{\bf b}_{i}),

\underset{\mathbf{h}(\omega)} {\min} \quad \mathbf{h}^{\mathrm{H}}(\omega) \mathbf{R}(\omega) \mathbf{h}(\omega) \text{,}
\qquad \text{s.t. }
\mathbf{h}^{\mathrm{H}}(\omega) \mathbf{d}(\omega) = g

\mathbf{h}_{\mathrm{maxDI}}(\omega) =  \frac{g^{\ast} \mathbf{R}^{-1}(\omega) \mathbf{d}(\omega)}{\mathbf{d}^H(\omega)\mathbf{R}^{-1}(\omega)\mathbf{d}(\omega)}

\mathbf{R}(\omega) = \frac{1}{4\pi} \int_{\phi} \int_{\theta} \mathbf{d}(\phi, \theta, \omega) \mathbf{d}^{\mathrm{H}}(\phi, \theta, \omega) sin(\theta)d\theta d\phi
\label{eq:R_original}

\mathbf{\tilde{R}}(\omega) = \frac{1}{N_{ATF}}\sum_{n} \sum_{m} \mathbf{d}_{ATF}(\phi_{n}, \theta_{m}, \omega)\mathbf{d}^{\mathrm{H}}_{ATF}(\phi_{n}, \theta_{m}, \omega)

where  is the total number of ATFs in the set.  and  are the azimuthal and the inclination angles associated with a set of discrete points sampling the sphere for which an ATF vector  is measured.

As both the desired participant and the AR glasses wearer are moving constantly relative to one another,  is assumed to change over time. Hence the estimate of  needs to be updated over time as opposed to  whose value approximates a weakly time-varying value. We use the output of the OptiTrack tracking system as the estimated positions of the desired participant and the AR glasses
wearer. We then calculate the relative angle of the desired participant from the AR glasses wearer. Once the relative angle is estimated,  that is closest to the relative angle is chosen from the set of ATFs and used as the estimate of .

The processing of the baseline method is conducted on a frame by frame basis. At every frame, the beamformer coefficient vector, , is updated and frames of multi-channel audio samples are filtered by  in a weighted overlap-add (WOLA) procedure.

 

\section{Baseline Results and Discussion} \label{sec:Results_and_Discussion}

\begin{table*}
	\centering
	\caption{\label{tab:resultsN} Scores for the reference microphone signal and the baseline method output.}
	\begin{tabular}
		{@{} l c c c c c c c c c c c c @{}}        
		\toprule
									     & \multirow{2}{*}{Test Case}                     
									     & \multirow{2}{*}{\makecell{SNR\\\cite{brookes1997voicebox}}}
									     & \multirow{2}{*}{\makecell{SegSNR\\\cite{brookes1997voicebox}}}
									     & \multirow{2}{*}{\makecell{SDR\\\cite{vincent2006performance}}}
									     & \multirow{2}{*}{\makecell{SI-SDR\\\cite{roux2019sdr}}}
									     & \multirow{2}{*}{\makecell{STOI\\\cite{taal2011algorithm}}}
									     & \multirow{2}{*}{\makecell{ESTOI\\\cite{jensen2016algorithm}}}
									     & \multirow{2}{*}{\makecell{HASPI\\\cite{kates2021haspiv2}}}
									     & \multirow{2}{*}{\makecell{SIIB\\\cite{vankuyk2018instrumental}}}
									     & \multirow{2}{*}{\makecell{PESQ\\\cite{ITU_PESQ_2003}}}
									     & \multirow{2}{*}{\makecell{HASQI\\\cite{kates2014hasqiv2}}}
									     & \multirow{2}{*}{\makecell{ViSQOL\\\cite{chinen2020visqol}}} \\
									     \\
		\midrule
		Reference Mic                    & \multirow{2}{*}{Noise}                         &      &      &      &      &      &      &      &      &      &      &     \\
		Baseline Method                  &                                                & & & & & & & & & & &\\
        \midrule
	    Reference Mic                    & \multirow{2}{*}{\makecell{Noise +\\Interferer}}&      &      &      &      &      &      &      &      & &      &     \\
	    Baseline Method                  &                                                & & & & & & & & &      & &\\
		\bottomrule
	\end{tabular}
\end{table*}


To analyze the baseline method performance, we consider signal-to-noise ratios, speech intelligibility and speech quality.
We use 11 intrusive instrumental objective metrics as measures of the performance. 
Three metrics are related to speech quality, four metrics are related to speech intelligibility and four are related to SNR.

We use SNR as one of the metrics~\cite{brookes1997voicebox}, which is well established in the literature. We define the SNR as the ratio of the desired target source signal to all other sounds, where we consider all other sounds as undesired noise.
Another metric used is segmental SNR (SegSNR), which is similar to SNR but the mean SNR is only computed over segments where the target source signal is active~\cite{brookes1997voicebox}.
The Signal to Distortion Ratio (SDR) metric is also used and follows a similar definition to SNR but is computed using the implementation described in~\cite{vincent2006performance}.
The Scale-Invariant SDR (SI-SDR) is the last of the SNR related metrics used~\cite{roux2019sdr}. The SI-SDR is a variant of SDR that was designed to address some assumptions made in the SDR implementation from~\cite{vincent2006performance}.

Speech intelligibility was investigated using the Short-Time Objective Intelligibility (STOI) metric, which has high correlation with time-frequency weighted noisy speech and is computed on short-time segments.
A more recent version of STOI, known as Extended STOI (ESTOI) was also used in the evaluation. ESTOI has been reported to, additionally, accurately predict intelligibility when highly modulated noise is present.
The Hearing-Aid Speech Perception Index (HASPI) version 2 was another intelligibility metric used~\cite{kates2021haspiv2}. HASPI is a metric that estimates intelligibility using a model of the auditory periphery and is valid for normal-hearing and hearing-impaired listeners.
We compute HASQI assuming all participants in the dataset have normal hearing and are speaking at levels of approximately \SI{71}{\decibel}~.
The last of the intelligibility metrics used is Speech Intelligibility In Bits (SIIB)~\cite{vankuyk2018instrumental}, which estimates the amount of information shared between a talker and a listener in bits.
SIIB has been show to have higher correlation to intelligibility than STOI, ESTOI and HASPI version 1~\cite{kates2021haspiv1} across many different datasets~\cite{vankuyk2018evaluation}.

For speech quality estimation we use the Perceptual Evaluation of Speech Quality (PESQ) metric~\cite{ITU_PESQ_2003}, which is widely available and commonly used for speech quality evaluations, although it was originally designed for telephony applications.
The Hearing-Aid Speech Quality Index (HASQI) version 2 was also used to evaluate speech quality and, like HASPI, is based on a model of the auditory periphery, taking into account the effects of hearing loss. 
We compute HASQI with the same assumptions we make with HASPI. 
Finally, the last metric used in the evaluation is the Virtual Speech Quality Objective Listener (ViSQOL) version 3 metric~\cite{chinen2020visqol}, which has been shown to have higher correlation to speech quality than PESQ on several datasets~\cite{hines2015visqol}.

The metric scores are obtained on the EasyCom dataset for all possible target cases (each participant is marked as the desired participant at least once) and the average scores are shown in Table~\ref{tab:resultsN}.
`Reference Mic' and `Baseline Method' denote the case where the unprocessed reference microphone signal is used as the degraded signal for each of the metrics and the case where the baseline method output is used as the degraded signal input for each of the metrics, respectively.
Two subsets of signals are evaluated as test cases for all metrics, namely `Noise' and `Noise + Interferer'.
For the `Noise' test case, the desired participantâ€™s portions of the signals (as determined by the VA labels of the dataset) are used as inputs for the metrics only when there is no competing talker present, and noise is always present.
For the `Noise + Interferer' test case, portions of the signal when the desired participant is active are used regardless of whether there is a competing talker present or not, and noise is always present.
We ignore cases where the participant wearing the AR glasses is actively talking.
In all evaluations, a minimally-noisy clean speech reference signal is used, which is the close microphone signals that are positioned next to the participant's mouths. 
These signals are not identical to the true clean speech component in the noisy speech signals captured by the AR glasses microphones and so the reported metric values may not be meaningful on their own.

Table~\ref{tab:resultsN} shows the results where we can see that in all test cases the baseline method increases all metrics over the `Reference Mic', except one.
The only metric that does not result in an improvement over the `Reference Mic' is PESQ and occurs for the `Noise + Interferer' test case, where the PESQ value for the `Reference Mic' and `Baseline Method' are  and , respectively.
This is likely due to the presence of interfering speech in the reference signal that is provided to the metric for the specific test case.
In Table~\ref{tab:resultsN} we see a \SIrange{1.2}{3.7}{\decibel} of average improvement in the SNR-based metrics.
A \SIrange{0.05}{0.11}{} improvement is made on average in most intelligibility-based metrics, except SIIB, which uses units of bits and shows \SIrange{32}{36}{bits} of improvement. We disregarded the units while making relative comparisons.
Of the improvements that are made in the quality-based metrics, we see a range of average improvement of \SIrange{0.03}{0.10}{}.

The challenging conditions of the dataset make it difficult to improve over the reference microphone signal, however, there are many other types of signals that could be leveraged to outdo the baseline method's results as reported here.
We call on the research community to improve on these results using the dataset for the tasks outlined in this paper.

 

\section{Conclusions} \label{sec:Conclusions}
In this work we have discussed and shown that existing datasets do not provide sufficient data for solving the cocktail party problem from a multi-modal and egocentric point of view, which could be common place in head mounted AR devices.
Existing egocentric datasets are missing either the visual modality, pose information and/or noisy environments.

We have closed the gap in the literature and the gap in available datasets by releasing a dataset with more than five hours of synchronized multi-modal egocentric noisy recordings of natural conversations.
To facilitate accelerated research in the area we have open-sourced the high quality dataset with various annotated labels in the different modalities.


We have proposed a benchmarking task for the dataset and have analyzed a baseline method to address the task.
The baseline method leverages acoustic and positional information to enhance targeted speech in real-time.
Other included modalities could also be leveraged when addressing the benchmarking task.
Our baseline method results in substantial and consistent improvements in speech quality, intelligibility and signal-to-noise ratio -based metrics whilst not distorting the target speech.
Our method can also instantaneously switch between target speech sources.

We challenge the research community to outperform our baseline using the benchmarking task on the provided dataset to help solve the cocktail party problem for AR.


 

\appendices

\section{Post-Processing} \label{appx:PostProcessing}

After the recording of the raw data, several post-processing steps took place to improve the quality of the dataset.
The post-process reduced the size of the dataset, aligned clean speech reference signals and facilitated more accurate annotations.

\subsection{Video Compression} \label{sec:video_compression}
The storage size of the raw uncompressed videos, at several terabytes, is substantially larger than that of the visually lossless (lossy) compressed version, at approximately 42 gigabytes.
The video compression was performed using FFMPEG~\cite{}.
The videos were encoded from uncompressed MPEG in an AVI container to an MPEG4 file format.
The MPEG4 video was compressed using the x264 library with settings using the `veryslow' preset and a Constant Rate Factor (CRF) of 17.

The adaptive B-frame decision method was optimal with 8 B-frames between I and P.
The direction motion vector prediction was automatic and integer pixel motion estimation method was an uneven multi-hexagon search.
The maximum motion vector search range was 24.
All partitions of the P, B and I -macroblocks were considered.
The number of reference frames was 16.
The sub-pixel motion estimation and mode decision was with quantization parameter and trellis RD quantization, enabled on all mode decisions.
There were 60 frames for the frame-type lookahead.

Two channels of the recorded audio were embedded in the compressed video.
The binaural microphone audio from channel five (left) and channel six (right) of the microphone array was embedded.


\subsection{Audio Alignment} \label{sec:audio_alignment}
The AR glasses microphone array and headset microphones were sampled with two different hardware clocks.
Each frame of \SI{2400}{samples} of audio was timestamped using a central clock.
The headset microphone recordings were first coarsely aligned with the time stamps.
The alignment was then further refined using the absolute peak of a generalized cross-correlation with phase transform (GCC-PHAT)~\cite{1162830} using the headset microphone signals and a reference microphone on the AR glasses.


\section{Calibration Data} \label{appx:CalibrationData}
To facilitate effective use of the data across different modalities, calibration data is provided.
The calibration data allows for efficient use of the acoustic array, linking the acoustic and visual modalities, reducing optical distortions and refining tracking estimates.
None of the provided calibration data that is described in the following is a depiction of any future product.

\subsection{Acoustic Array Transfer Functions}
In order to effectively compute spatial enhancement filters or beamforming filters, the transfer function from each microphone to a point in far-field space is required.
This set of transfer functions is commonly called the steering vector, array manifold or array transfer functions (ATFs), the latter is the term used in this work.
The ATFs included in this dataset were measured on a head and torso simulator in an anechoic chamber for a discrete set of positions on a sphere.


\subsection{Spatial Acoustic-Optical Alignment}
The acoustic array has a spatial response and, hence, the ability to localize sound sources in space.
The camera spatially samples photons from the surrounding space and can be correlated with the acoustic space if both spaces are aligned.
In order to align these two modalities, the AR glasses were placed on a head and torso acoustic simulator in a specialized acoustic setup.
The discrete acoustic source locations in azimuth and elevation were marked in the camera field of view and labeled as the corresponding image pixel coordinates.



\subsection{Optical Camera Intrinsics}
The camera lens path introduces typical lens distortions, which change the way the flat image sensor maps to world coordinates.
To compensate for this lens warping, the camera intrinsics and lens de-warp filter parameters were measured.
Images of a standard checkerboard were taken with the camera and then used to produce the de-warping filters.

\subsection{Tracking Marker Locations}
The OptiTrack tracking system used multiple passive infra-red reflective markers to track the participants in the scene.
Multiple tracking markers were used per pair of glasses, five for the AR glasses recording the scene and four for the other glasses.
The resulting tracked position included in the dataset is the center of mass of the tracked marker locations.
In order to re-align the tracked positions to a specific marker, the relative positions of the markers and the center of mass are provided in the dataset.

 

\section*{Acknowledgments} \label{sec:Acknowledgments}

We would like to thank the research assistant for their excellent work helping collect this dataset (whose name is anonymous for privacy reasons).
We would also like to thank everyone who gave valuable feedback on the dataset.
Lastly, we extend our thanks to all participants involved in the data collection.

 

\renewcommand*{\bibfont}{\footnotesize}
\setlength\bibitemsep{0.2em}
\printbibliography


\end{document}
