\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setup}

\begin{table}[t]
  \centering
  \caption{Summary of the datasets.}
  \label{tab:dataset}
  \scalebox{0.95}
  {\begin{tabular}{llrrr}
    \shline
    \textbf{Dataset} & \textbf{Split}      & \# \textbf{Samples} & \# \textbf{Classes} & \textbf{Image Size} \\
    \midrule
    CIFAR-10                    & Train+Test & 60,000 & 10   & 3232 \\
    CIFAR-20                    & Train+Test & 60,000  & 20  & 3232 \\
    STL-10                      & Train+Test & 13,000 & 10   & 9696 \\
    ImageNet-10                 & Train      & 13,000 & 10   & 9696  \\
    ImageNet-Dogs               & Train      & 19,500   & 15   & 9696  \\
    Tiny-ImageNet               & Train      & 100,000  & 200   & 224224  \\
    ImageNet-1k                    & Train      & 1,281,167                   & 1,000                        & 224224  \\
    \shline
    \end{tabular}
  }
\end{table}

\subsubsection{Datasets}
We conducted experiments on seven benchmark datasets, including \textbf{CIFAR-10}~\cite{krizhevsky2009learning}, \textbf{CIFAR-20}~\cite{krizhevsky2009learning}, \textbf{STL-10}~\cite{coates2011analysis}, \textbf{ImageNet-10}~\cite{chang2017deep}, \textbf{ImageNet-Dogs}~\cite{chang2017deep}, \textbf{Tiny-ImageNet}~\cite{le2015tiny}, and \textbf{ImageNet-1k}~\cite{deng2009imagenet}, which are summarized in Table~\ref{tab:dataset}. We note that CIFAR-20 contains 20 superclasses of CIFAR-100. STL-10 includes extra unlabeled images. ImageNet-10, ImageNet-Dogs, and Tiny-ImageNet are the widely-used subsets of ImageNet-1k~\cite{deng2009imagenet}, containing 10, 15, 200 classes, respectively. This paper follows the experimental settings widely used in deep clustering work~\cite{chang2017deep,wu2019deep,ji2019invariant,tsai2020mice,tao2021clustering}, including the image size, backbone, and train-test split. For image size, we use  for CIFAR-10 and CIFAR-20,  for STL-10, ImageNet-10, and ImageNet-Dogs, and  for Tiny-ImageNet and ImageNet-1k. For train-test split, we use the whole datasets including training and testing set for CIFAR-10 and CIFAR-20 while both labeled and unlabeled data are employed for STL-10.

\subsubsection{Backbones}
We use ResNet-34~\cite{he2016deep} as the backbone for fair comparisons to report the main results on moderate-scale benchmark datasets. We use ResNet-18~\cite{he2016deep} on Tiny-ImageNet and ResNet-50~\cite{he2016deep} on ImageNet-1k, following the literature. Unless noted otherwise, we use ResNet-18 for the rest of the experiments. Since the image sizes of CIFAR-10 and CIFAR-100 are relatively small, following~\cite{chen2020simple}, we replace the first convolution layer of kernel size  and stride 2 with a convolution layer of kernel size  and stride 1, and remove the first max-pooling layer for all experiments on CIFAR-10 and CIFAR-100.

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{training_vis_clustersx.pdf}
  \caption{
  Detailed comparison between BYOL~\cite{grill2020bootstrap} and \methodname on CIFAR-10 in terms of (a) standard deviation~(STD) of -normalized features to evaluate the uniformity, (b) cluster imbalance ratio computed by  to show how balanced the clusters are,  (c) cluster statistics, or the sorted number of samples in each cluster for the model at 1000-th epoch, and (d) normalized mutual information~(NMI) between the clustering results and ground-truth labels.
  }
  \label{fig:training_vis_clusters}
\end{figure*}

\subsubsection{Implementation details}
\label{sec:implementation_details}
We train all methods with 1,000 epochs, strictly following the literature~\cite{tsai2020mice,tao2021clustering}, and adopt the stochastic gradient descent (SGD) optimizer and the cosine decay learning rate schedule with 50 epochs for learning rate warmup. The base learning rate for MoCo v2~\cite{chen2020improved}, BYOL~\cite{grill2020bootstrap}, and \methodname were 0.05, scaled linearly with the batch size (LearningRate = 0.05BatchSize/256). Note that the learning rates for predictor networks of BYOL and \methodname are  as the learning rate of feature extractor. It is relatively important to achieve satisfactory performance, as discussed in~\cite{grill2020bootstrap,chen2021exploring}. 

For other hyperparameters of \methodname, the temperature~,  for prototypical scattering loss, and  for positive sampling were set as , , and , respectively. The mini-batch size was 512 for MoCo and 256 for the remaining methods. \methodname was trained on 4 NVIDIA V100 GPUs. 

Regarding CC~\cite{li2021contrastive} and PCL~\cite{li2020prototypical}, we tried our best to reproduce their results for fair comparisons. For CC, we used their official code. For PCL, under the fair conditions of MoCo, we set the loss weight of ProtoNCE to  and the number of clusters to  following the suggestions of authors, which we found can achieve the best results. We integrated CC and PCL into BYOL by adding their losses without changing other settings.

\subsubsection{Configurations of SSL frameworks}
We adopt the same data augmentations as SimCLR~\cite{chen2020simple}, including ResizedCrop, ColorJitter, Grayscale, and HorizontalFlip. We have removed GaussianBlur since we only used a small image size for all datasets. We also strictly follow the settings of BYOL~\cite{grill2020bootstrap}. Specifically, despite the standard ResNet backbones, the projection and predictor networks have the architectures of FC-BN-ReLU-FC, where the projection dimension and hidden size were 256 and 4096 for both two networks, respectively.
For fair comparisons, we have also set the projection dimension of MoCo v2 as 256. We have used symmetric loss for all methods,
\ie, swapping two data augmentations to compute twice loss. For the momentum hyperparameter , we set it to 0.996 for both BYOL and \methodname same as~\cite{grill2020bootstrap} and 0.99 for MoCo v2. For MoCo v2, the queue size, temperature for InfoNCE loss, weight decay were 4,096, 1.0, and , respectively. We have not employed SyncBN in \methodname. We note that SyncBN would introduce much additional computation cost. Instead, we adopt the shufflingBN in MoCo to avoid the trivial solution of non-contrastive learning. 

\subsection{Empirical Justification}
\label{sec:justification}

We provide an empirical justification on how the proposed \methodname improves representation learning for deep clustering from the aspects of PSL and PSA.

\subsubsection{The role of PSL} 
The non-uniform representations produced by non-contrastive representation learning would lead to the collapse of downstream clustering where most samples are assigned to few clusters.
We emphasize that it is desirable to avoid the trivial solution for deep clustering. For example, CC~\cite{li2021contrastive} and etc~\cite{niu2020gatcluster,zhong2021graph} have usually employed an entropy term in loss function to regularize the model equally assigning the images into different clusters.
This paper mainly investigates this phenomenon of non-contrastive representation learning for deep clustering, and the theoretical analysis can be found in~\cite{zhang2022does,tian2021understanding}.

Our \methodname can encourage cluster uniformity for representations via \lossname.
Here, we use the representative non-contrastive learning method, Bootstrap Your Own Latent (BYOL), as the representation learning for deep clustering. We performed spherical -means on the learned representations for the clustering task with 10 different initializations. Following~\cite{chen2021exploring}, we use the standard deviation (STD) of -normalized representations to measure the uniformity. Ideally, if the -normalized representations are uniformly distributed on a unit hypersphere, we have , where  and  are the -normalized version and the dimension of the feature representation . To justify the effectiveness of \methodname, we conducted the following experiments in terms of the uniformity of the representations, the collapse of clustering, and the clustering performance.

First, we visualize the uniformity of representations in Fig.~\ref{fig:training_vis_clusters}(a). Taking a look at STDs during the training stage, \methodname produces higher STDs, while BYOL performs unstable with the STDs gradually decreasing.
Note that a higher standard deviation close to  indicates more uniform representations. Our \methodname yields more uniform representations than BYOL. Most importantly, the uniformity of \methodname is rather stable during training.  

Second, we further visualize the cluster imbalance to measure the potential collapse during clustering. More specifically, we compute the cluster imbalance ratio between the cluster with least samples and the cluster with most samples, , where  is the number of samples in -th cluster.  A higher value indicates more balanced clusters. In addition, we also show the cluster statistics or the sorted number of samples in each cluster for the model at 1000-th epoch. The results of cluster imbalance during training and the cluster statistics at the final epoch are shown in Fig.~\ref{fig:training_vis_clusters}(b) and (c).   Fig.~\ref{fig:training_vis_clusters}(b) shows that the -means clustering process of \methodname produces more balanced clusters with a higher cluster imbalance ratio. On the contrary, the clusters of BYOL are highly imbalanced, which is consistent with  decreasing STDs.
Moreover, Fig.~\ref{fig:training_vis_clusters}(c) shows that the cluster statistics for \methodname are approximately and equally assigned to different clusters, compared to almost long-tailed assignments of BYOL.
The more balanced clusters validate that \methodname can alleviate the collapse of k-means clustering over BYOL.

Finally, Fig.~\ref{fig:training_vis_clusters}(d) shows the clustering performance comparison between the proposed \methodname and BYOL, which is measured by the normalized mutual information (NMI) between clustering results and ground-truth labels. We can see that \methodname produces higher and more stable NMIs than BYOL. In line with the analysis above, we conclude that directly applying BYOL to deep clustering, although avoiding class collision issue, suffers from the collapse of -means clustering due to the non-uniform representations. In contrast, \methodname with PSL yields more uniform representations and well-clustered samples.

\subsubsection{The role of PSA} 

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{just_psax.pdf}
  \caption{
    Visualization of sampled neighbors for \methodname on CIFAR-10: (a) The odds of sampled neighbors that have preserved their original semantic classes under different  during training; and (b) The input images, the 1-nearest-neighbors~(1-NN) of input images, and three sample neighbors at 100th epoch.
  }
  \label{fig:just_psa}
\end{figure}

\begin{table*}[t]
  \centering
  \caption{Clustering results (\%) of various methods on five benchmark datasets. The best and second best results are shown in bold and underline, respectively. We split different methods according to different training paradigms. The works most related to our method are IDFD and PCL that improve the representations for clustering.
  }
  \label{tab:results}
  \begin{threeparttable}
  \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}l*{19}{c}}
    \shline
    \multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{CIFAR-10}} & \multicolumn{3}{c}{\textbf{CIFAR-20}} &\multicolumn{3}{c}{\textbf{STL-10}} & \multicolumn{3}{c}{\textbf{ImageNet-10}} & \multicolumn{3}{c}{\textbf{ImageNet-Dogs}}\\
    \cmidrule{2-16}
     & NMI & ACC & ARI & NMI & ACC & ARI & NMI & ACC & ARI & NMI & ACC & ARI & NMI & ACC & ARI\\
    \midrule
    IIC~\cite{ji2019invariant} & 51.3 & 61.7 & 41.1    & -    & 25.7 & -        & 43.1 & 49.9 & 29.5  & -    & -    & -          & -    & -    & -     \\
    DCCM~\cite{wu2019deep} & 49.6 & 62.3 & 40.8 & 28.5 & 32.7 & 17.3 & 37.6 & 48.2 & 26.2 & 60.8 & 71.0 & 55.5 & 32.1 & 38.3 & 18.2    \\

    PICA~\cite{huang2020deep} & 56.1 & 64.5 & 46.7    & 29.6 & 32.2 & 15.9     & -    & -    & -     & 78.2 & 85.0 & 73.3      & 33.6 & 32.4 & 17.9         \\
    \midrule
    SCAN~\cite{van2020scan} &  79.7 & 88.3 & 77.2 & 48.6 & 50.7 & 33.3 & 69.8 & 80.9 & 64.6 & - & - & - & - & - & -\\
    NMM~\cite{Dang_2021_CVPR} & 74.8 & 84.3 & 70.9 & 48.4 & 47.7 & 31.6 & 69.4 & 80.8 & 65.0 & - & - & - & - & - & -\\
    
    \midrule
    
    CC~\cite{li2021contrastive} &  70.5 & 79.0 & 63.7 & 43.1 & 42.9 & 26.6 & \underline{76.4} & 85.0 & 72.6 & 85.9 & 89.3 & 82.2 & 44.5 & 42.9 & 27.4\\
    MiCE~\cite{tsai2020mice} & 73.7 & 83.5 & 69.8 & 43.6 & 44.0 & 28.0 & 63.5 & 75.2 & 57.5 & - & - & - & 42.3 & 43.9 & 28.6\\
    GCC~\cite{zhong2021graph} & 76.4 & 85.6 & 72.8 & 47.2 & 47.2 & 30.5 & 68.4 & 78.8 & 63.1 & 84.2 & 90.1 & 82.2 & 49.0 & 52.6 & 36.2\\
    TCL~\cite{li2022twin} & \underline{81.9} & 88.7 & 78.0 & 52.9 & 53.1 & 35.7 & \textbf{79.9} & \textbf{86.8} & \textbf{75.7} & 87.5 & 89.5 & 83.7 & 62.3 & 64.4 & 51.6 \\
    TCC~\cite{shen2021you} & 79.0 & \underline{90.6} & 73.3 & 47.9 & 49.1 & 31.2 & 73.2 & 81.4 & 68.9 & 84.8 & 89.7 & 82.5 & 55.4 & 59.5 & 41.7 \\

    \midrule

    MoCo~\cite{he2020momentum} & 66.9 & 77.6 & 60.8 & 39.0 & 39.7 & 24.2 & 61.5 & 72.8 & 52.4 & - & - & - & 34.7 & 33.8 & 19.7\\
    SimSiam~\cite{chen2020simple} & 78.6 & 85.6 & 73.6 & 52.2 & 48.5 & 32.7 & 65.9 & 71.6 & 57.2 & 83.1 & 92.1 & 83.3 & 58.3 & 67.4 & 50.1 \\
    BYOL~\cite{grill2020bootstrap} & 81.7 & {89.4} & \underline{79.0} & \underline{55.9} & \underline{56.9} & \underline{39.3} & 71.3  & {82.5}  & 65.7  & 86.6 & 93.9 & 87.2 & \underline{63.5} & \underline{69.4} & \underline{54.8}\\

    \midrule
    IDFD~\cite{tao2021clustering} & 71.1 & 81.5 & 66.3 & 42.6 & 42.5 & 26.4 & 64.3 & 75.6 & 57.5 & \textbf{89.8} & \underline{95.4} & \underline{90.1} & 54.6 & 59.1 & 41.3\\
    PCL~\cite{li2020prototypical} & 80.2 & 87.4 & 76.6 & 52.8 & 52.6 & 36.3 & 71.8 & 41.0 & 67.0 & 84.1 & 90.7 & 82.2 & 44.0 & 41.2 & 29.9 \\

    \methodname~(\textbf{ours}) & \textbf{88.6} & \textbf{94.3} & \textbf{88.4} & \textbf{60.6} & \textbf{61.4} & \textbf{45.1} & 75.8 & \underline{86.7} & \underline{73.7} & \underline{89.6} & \textbf{95.6} & \textbf{90.6} & \textbf{69.2} & \textbf{74.5} & \textbf{62.7} \\
  \shline
\end{tabular*}
  \begin{tablenotes}
  \item[1] CC and TCL use a large image size  of  for all datasets.
  
\end{tablenotes}
\end{threeparttable}
 
\end{table*}

PSA assumes that the sampled neighbors are truly positive examples with respect to another view, \ie, they belong to the same semantic classes. To validate this assumption, we investigate the behavior of PSA during training by checking whether the semantic classes of input examples have been changed. 

First, we perform -NN classification to predict the classes of both inputs and their sampled neighbors from testing set, and then use the proportion of sampled neighbors that have preserved original classes as the preservation rate. We run the experiments 10 times with different  for PSA. As shown in Fig.~\ref{fig:just_psa}(a), even at the early training stage, the sampled neighbors well preserve original classes for . The preservation rate drops as expected for  since PSA may sample the instances from another cluster for large . 

Second, we showcase some randomly-selected input images whose nearest neighbors have been changed during the sampling procedure, Specifically, Fig.~\ref{fig:just_psa}(b) visualizes the input images, their nearest neighbors, and the sampled neighbors. We note that we replaced the images of sampled neighbors with their nearest neighbors since it is too hard to reconstruct the image from embedding space. Obviously, the sampled neighbors belong to the same class as the input images even at the early stage of training (\ie 100th epoch).

In summary, the results suggest that the sampled neighbors could be truly positive examples both quantitatively and qualitatively so that PSA can improve the within-cluster compactness.



\subsection{Main Results}



In this section, we evaluate \methodname with previous state-of-the-art clustering methods on various benchmark datasets. We divide these methods into 5 types: i) methods without using contrastive learning; ii) multi-stage methods requiring step-by-step pretraining or finetuning; iii) methods directly outputting the cluster assignments; iv) methods learning general representations, \eg, MoCo; and v) methods improving representation learning for clustering. We strictly follow the experimental settings of previous works~\cite{tsai2020mice,tao2021clustering} for fair comparisons. We reproduced PCL~\cite{li2020prototypical}, SimSiam~\cite{chen2020simple}, and BYOL~\cite{grill2020bootstrap} under the same conditions, and directly use the their learned representations for -means clustering.

In terms of qualitative results of clustering, we visualize the learned representations by t-SNE~\cite{van2008visualizing} for four different training epochs throughout the training process in supplemental Fig.~\ref{fig:clustering_quality} and the outlier points produced by the model at 1000-th epoch on CIFAR-10 in supplemental Fig.~\ref{fig:outlier_points_vis}.
For fair comparisons, supplemental Table~\ref{tab:fair_results_with_imagesize_and_split} presents the results excluding the testing set for CIFAR-10/20 and using a larger image size for ImageNet-10/Dogs.

\subsubsection{Results on moderate-scale datasets}
The comparisons on five moderate-scale datasets are reported in Table~\ref{tab:results}. The results of MoCo are referred from~\cite{tsai2020mice}. For fair  comparison, we excluded SPICE~\cite{niu2021spice} in Table~\ref{tab:results} since it requires multiple pre-training stages. 
\methodname achieves significant performance improvement on all benchmark datasets, demonstrating the superiority of \methodname for deep clustering to capture the semantic class information.

\begin{table}[t]
  \centering
  \caption{
      Clustering results (\%) on Tiny-ImageNet. We trained \methodname using ResNet-18. 
  }
  \label{tab:tiny_imagenet}
  \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}lrrr}
      \shline
      \multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Tiny-ImageNet}} \\
      \cmidrule{2-4}
        & NMI       & ACC       & ARI       \\
      \midrule
      DCCM~\cite{wu2019deep}   & 22.4      & 10.8      & 3.8       \\
      PICA~\cite{huang2020deep}   & 27.7      & 9.8      & 4.0       \\
      CC~\cite{li2021contrastive}   & 34.0      & 14.0      & 7.1       \\
      GCC~\cite{zhong2021graph}     & 34.7      & 13.8      & 7.5       \\
      MoCo~\cite{he2020momentum} & 34.2 & 16.0 & 8.0 \\
      PCL~\cite{li2020prototypical} & 35.0 & 15.9 & 8.7 \\
      SimSiam~\cite{chen2020simple} & 35.1 & \underline{20.3} & 9.4 \\
      BYOL~\cite{grill2020bootstrap} & \underline{36.5} & 19.9 & \underline{10.0}\\
      \methodname~(\textbf{ours})     & \textbf{40.5}      & \textbf{25.6}      & \textbf{14.3}      \\
      \shline
  \end{tabular*}
\end{table}
\begin{table}[t]
  \centering
  \caption{Clustering results (\%) on ImageNet-1k using ResNet-50.}\label{tab:result_imagenet}
  \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}lr}
  \shline
  \textbf{Method}      & \textbf{AMI}   \\
  \midrule
  DeepCluster~\cite{caron2018deep}  & 28.1 \\
  MoCo~\cite{he2020momentum}        & 28.5 \\
  PCL~\cite{li2020prototypical}     & \underline{41.0} \\
  \methodname~(\textbf{ours})     & \textbf{52.5} \\
  \shline
  \end{tabular*}
\end{table}


\begin{table}[t]
  \centering
  \caption{
    Linear evaluation on ImageNet-1k dataset. We report the Top-1 classification accuracy (\%) by training a linear classifier; the results are adopted from corresponding papers. The upper group uses more fair conditions, \eg, backbone and training epoch.
    }
  \label{tab:imagenet_linear_classifier}
  \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}llrrr}
  \shline
  \multirow{3}{*}{\textbf{Method}} & \multirow{3}{*}{\textbf{Backbone}} & \multicolumn{2}{c}{\textbf{Pre-training}}   & \multirow{3}{*}{\textbf{ACC}}     \\
  \cmidrule{3-4}
    &    & Batch size& Epochs    &      \\
  \midrule
  Jigsaw~\cite{noroozi2016unsupervised}   & AlexNet   & 256  & -    & 34.6 \\
  Rotation~\cite{gidaris2018unsupervised} & AlexNet   & 128  & 100  & 38.7 \\
  DeepCluster~\cite{caron2018deep}             & AlexNet   & 256  & 500  & 41.0 \\
  InstDisc~\cite{wu2018unsupervised} & ResNet-50 & 256  & 200  & 54.0 \\
  LocalAgg~\cite{zhuang2019local} & ResNet-50 & 128  & 200  & 60.2 \\
  CMC~\cite{tian2020contrastive} & ResNet-50 & -    & 200  & 66.2 \\
  SimCLR~\cite{chen2020simple}   & ResNet-50 & 256  & 200  & 64.3 \\
  MoCo~\cite{he2020momentum} & ResNet-50 & 256  & 200  & 60.6 \\
  MoCo v2~\cite{chen2020improved}   & ResNet-50  & 256 & 200  & 67.5 \\
  PCL~\cite{li2020prototypical}    & ResNet-50   & 256   & 200  & 67.6 \\
  IFND~\cite{chen2021incremental}     & ResNet-50  & 256  & 200  & 69.7 \\
  BYOL~\cite{albrecht2020}     & ResNet-50  & 4096  & 200  & \underline{70.6} \\
  SimSiam~\cite{chen2021exploring}     & ResNet-50  & 256  & 200  & 70.0 \\
  ProPos (\textbf{ours})           & ResNet-50 & 256  & 200  & \textbf{72.2}    \\
  \midrule
  CPC~\cite{oord2018representation} & ResNet-101& 512  & -    & 48.7 \\
  SeLa~\cite{asano2019self} & ResNet-50 & 1024 & 400  & 61.5 \\
  PIRL~\cite{misra2020self} & ResNet-50 & 1024 & 800  & 63.6 \\
  SimCLR~\cite{chen2020simple}   & ResNet-50 & 4096 & 1000 & 69.3 \\
  BYOL~\cite{albrecht2020} & ResNet-50 & 4096 & 1000 & 74.3 \\
  SwAV~\cite{caron2020unsupervised} & ResNet-50 & 4096 & 800  & 75.3 \\
  \shline
  \end{tabular*}
\end{table}

\begin{table*}[t]
  \centering
  \caption{
    Ablation studies~(NMI/ACC/ARI) for different self-supervised learning frameworks, positive sampling alignment (PSA), and prototype scattering loss (\lossname) for \methodname. The best and second best results are shown in bold and underline, respectively.
  }
  \label{tab:ablation_study}

  \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}lcccccc}
  \shline
  \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{CIFAR-10}} & \multicolumn{3}{c}{\textbf{CIFAR-20}} \\
  \cmidrule{2-7}
    & NMI & ACC & ARI & NMI & ACC & ARI \\ \midrule
  CC~\cite{li2021contrastive}  & 66.1\std{0.3} & 74.6\std{0.3} & 58.3\std{0.4} & 46.4\std{0.3} & 45.0\std{0.1} & 29.5\std{0.2} \\
  CC +  \lossname & 74.3\std{0.4} & 83.4\std{0.5} & 69.6\std{1.0} & 48.3\std{0.2} & 49.1\std{0.2} & 32.2\std{0.4} \\
  PCL~\cite{li2020prototypical} & 77.6\std{0.1} & 85.5\std{0.1} &73.4\std{0.0} & 50.0\std{0.3} &48.6\std{0.7} &32.7\std{0.4} \\
  BYOL~\cite{grill2020bootstrap} & 79.4\std{1.7} & 87.8\std{1.7} & 76.6\std{2.8} & 55.5\std{0.6} & 53.9\std{1.6} & 37.6\std{0.9} \\
  BYOL + CC & 76.6\std{3.1} & 86.3\std{2.7} & 73.8\std{4.7} & 51.0\std{2.0} & 48.9\std{3.0} & 33.3\std{2.9} \\
  BYOL + PCL & 74.4\std{2.3} & 85.3\std{0.9} & 71.4\std{1.4} & 49.7\std{0.7} & 46.9\std{0.7} & 27.8\std{1.5} \\
  \midrule
  \methodname (\textbf{ours}) & \underline{85.1}\std{0.5} & \underline{91.6}\std{0.4} & \underline{83.5}\std{0.7} & \textbf{58.2}\std{0.3} & \textbf{57.8}\std{0.2} & \textbf{42.3}\std{0.3} \\
  \methodname w/o \lossname & 79.4\std{0.9} & 87.9\std{0.5} & 76.4\std{1.1} & 57.0\std{0.0} & 55.0\std{0.6} & 39.8\std{1.1} \\
  \methodname w/o PSA & 83.4\std{1.2} & 90.3\std{0.9} & 81.1\std{1.7} & 56.6\std{0.4} & 55.1\std{0.5} & 40.7\std{1.0} \\
  \methodname w/o PSL-uniformity & 79.6\std{0.7} & 87.8\std{1.5} & 76.5\std{2.1} & 56.7\std{0.3} & 56.6\std{1.4} & 39.7\std{1.1} \\
  \methodname w/o PSL-alignment & \textbf{85.3}\std{0.2} & \textbf{92.1}\std{0.1} & \textbf{84.4}\std{0.3} & \underline{57.2}\std{0.3} & \underline{57.3}\std{0.6} & \underline{41.7}\std{0.5} \\
  \shline
  \end{tabular*}
\end{table*}

On the ImageNet-10, our \methodname achieves competitive performance as compared to IDFD~\cite{tao2021clustering} since this dataset is relatively small with only 13K images, which cannot arise discriminative differences for current state-of-the-art methods. On the ImageNet-Dogs, a fine-grained dataset containing different species of dogs from the ImageNet dataset, there are almost 20\% improvements over previous state-of-the-art work. The contrastive-based methods cannot handle this kind of dataset due to severe class collision issue that pushes away the instances from the same class. Meanwhile, IDFD can deal with this problem to some degree thanks to the feature decorrelation along with the instance discrimination.
Furthermore, BYOL and SimSiam can achieve significant improvements, which suggests a great potential for non-contrastive representation learning for deep clustering \emph{without} suffering class collision issue. However, our \methodname has introduced further substantial improvements for deep clustering by addressing existing issues. Although CC~\cite{li2021contrastive} and TCL~\cite{li2022twin} achieve the better NMIs on STL-10, we highlight that they use a large image size of  for all datasets, which is not fair to ours.

\subsubsection{Results on large-scale datasets}


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.96\linewidth]{clustering_quality_ablation.pdf}
  \caption{
    Visualization of feature representations learned by different representation learning frameworks and our proposed \methodname on CIFAR-10 with t-SNE. Zoom in for better view.
  }
  \label{fig:clustering_quality_ablation}
\end{figure*}

To validate the effectiveness of our method on large-scale datasets with large number of classes, we evaluate it on Tiny-ImageNet and ImageNet-1k, which contains 200 and 1000 classes, respectively. The results are reported in Tables~\ref{tab:tiny_imagenet} and~\ref{tab:result_imagenet}.
We note that we exclude the methods that their authors did not report results on the corresponding datasets from Tables~\ref{tab:tiny_imagenet} and~\ref{tab:result_imagenet}.
For ImageNet-1k, we strictly follow the settings in~\cite{li2020prototypical} and employed Adjusted Mutual Information~(AMI) to evaluate the performance, the results in Table~\ref{tab:result_imagenet} are referred from~\cite{li2020prototypical}, and we trained a ResNet-50 for 200 epochs same as~\cite{li2020prototypical}. The results show the strong generalization ability of \methodname on complex datasets with a large number of clusters.

To further show the effectiveness of \methodname for downstream classification task, we conducted the linear evaluation on ImageNet-1k dataset, and provide the comparisons with the recent state-of-the-art methods in Table~\ref{tab:imagenet_linear_classifier}. Following the same settings in~\cite{chen2021exploring}, we have trained the linear classifier for 90 epochs with a batch size of 4,096, an initial learning rate of 1.6, cosine learning rate decay, and the SGD optimizer of momentum 0.9 and weight decay 0. Under fair conditions, \methodname outperforms other competitors by a clear margin.

\subsection{Ablation Study}
\label{sec:ablation_study}
Here, we perform detailed ablation studies with both quantitative and qualitative comparisons to provide more insights into why \methodname performs well for deep clustering.



\subsubsection{Quantitative ablation study} 

The quantitative results are shown in Table~\ref{tab:ablation_study}.

\noindent\textbf{Ablation study of PSL and PSA.}\quad
\methodname w/o PSA improves the baseline results by a large margin while \methodname w/o \lossname achieves marginal improvements, which indicates that \lossname is the key to boosting the clustering performance. However, PSA plays an important role in two parts. First, simply using the PSA can stable and further improve the performance than the baseline BYOL~(only instance alignment loss), especially when the number of semantic classes increases for CIFAR-20. Second, PSA can make \methodname better for clustering when \lossname is used in conjunction with the PSA to pull together the neighbor examples. This is because the well-seperated clusters by \lossname can further ensure that PSA samples the positive neighbors that are in the same semantic classes than the one without \lossname.
On the other hand, \lossname only considers inter-cluster distance, and cannot benefit within-cluster compactness. Therefore, the combination of the positive sampling and \lossname achieves the best clustering results, where \lossname aligns the cluster centers between two augmented views and maximizes the inter-cluster distance, and PSA improves the within-cluster compactness.

To further explore the effect of \lossname, following Eq.~(\ref{eq:decoupled_proto_contr}) we split \lossname into alignment and uniformity terms denoted as \lossname-alignment and \lossname-uniformity in Table~\ref{tab:ablation_study}. It is clear that the performance gain from the alignment term is marginal while the gain from the uniformity term is significant. For only alignment term, we compute the loss after predictor network instead of feature extractor, otherwise, representation collapse will turn out. This indicates that uniformity is more important than alignment which can scatter the prototypes to encourage the uniform representations to address the issues in Sec.~\ref{sec:justification}. However, the alignment term is essential to stabilize the training process, as demonstrated in the results for CIFAR-20 with more clusters.

\noindent\textbf{Ablation study of self-supervised learning framework.}\quad
To alleviate the bias of self-supervised learning framework, we conduct experiments in two folds. First, we integrate CC~\cite{li2021contrastive} and PCL~\cite{li2020prototypical} into BYOL; the results in Table~\ref{tab:ablation_study} show that both of them compromise clustering performance and become unstable. This is because CC contrasts the cluster probability not helpful for representation learning, and PCL would collapse without negative examples; see supplemental Fig.~\ref{fig:pcl_moco_and_byol} for detailed analysis.
We emphasize that BYOL has addressed the class collision issue. Instead, this paper proposes the \methodname with \lossname to scatter the prototypes by maximizing inter-cluster distance and positive sampling to improve within-cluster compactness.
Second, we replace the cluster head of CC with our \lossname on the representations while keeping other hyper-parameters unchanged. Although class collision issue remains, the significant improvements over CC on both datasets suggest that 1) \lossname over representation prototypes is better than the one over cluster probabilities, and 2) \lossname can be generalized to other self-supervised learning frameworks.

\subsubsection{Qualitative ablation study} 
Fig.~\ref{fig:clustering_quality_ablation} visualizes the distribution of representations learned from BYOL, \methodname w/o \lossname, \methodname w/o PSA, and our \methodname.
\methodname w/o PSA leverages \lossname to discriminate different clusters by maximizing the inter-cluster distance, and produces more uniform representations.
Although \methodname w/o \lossname with only PSA achieves marginal improvements and does not produce a significant difference than BYOL, the positive sampling can further improve the within-cluster compactness with only \lossname via the sampling-based instance alignment loss to pull together the neighbor samples.

\subsubsection{Effect of predefined number of clusters}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{overclustering.pdf}
  \caption{
    The effect of the predefined number of clusters  on CIFAR-10/20 datasets.
  }
  \label{fig:overclustering}
\end{figure}

In the above experiments, the number of clusters is predefined as the number of ground-truth classes, which cannot be identified in the practical scenarios.
To this end, we conduct experiments on CIFAR-10 and CIFAR-20 with different number of clusters, \ie, . We reported NMIs following~\cite{caron2018deep} in Fig.~\ref{fig:overclustering}. We note that the predefined  of \methodname during the training of \methodname is the same as the  in -means clustering process for evaluation. To further investigate the influences of , we also reported the results of vanilla BYOL during -means clustering.

The results in Fig.~\ref{fig:overclustering} demonstrate that BYOL and \methodname have the same behavior on these two datasets.
For over-clustering cases~( is larger than the true number of classes), the trends on these two datasets are opposite.
Specifically, over-clustering leads to a performance drop for CIFAR-10, but it leads to an increase for CIFAR-20. However, our \methodname can still produce large improvements over BYOL with the same predefined number of clusters.
The opposite results are due to the significant difference between these two datasets. Although having the same number of samples, CIFAR-10 has 10 distinct classes while CIFAR-20 has, in fact, 100 classes but uses 20 super-classes instead. Same trends are also consistently reported in~\cite{caron2018deep}.
In other cases, if the representations are well aligned within the same semantic clusters, the over-clustering would try to destroy the structures of the clusters and push the semantically similar examples away, which certainly compromises the clustering performance.
For under-clustering cases, the clustering performance has been significantly harmed for both two datasets and two methods. In supplemental Fig.~\ref{fig:underclustering_vis}, we have visualized the learned representations for under-clustering, which shows that the examples from the same semantic classes can be still clustered together.

\subsection{Hyperparameter Analysis}
\label{sec:hyperparameter_analysis}
To investigate the effect of different hyperparameters, we conduct extensive experiments on the CIFAR-10/20 datasets. For the projection dimension, backbone, and data augmentation of SSL, we adopted the BYOL as the baseline method. The results are reported in Fig.~\ref{fig:hyperparameter_analysis}.
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{hyperparameter_analysisx.pdf}
  \caption{
    Effects of different hyperparameters in \methodname.
  }
  \label{fig:hyperparameter_analysis}
\end{figure*}
\begin{table*}[t]
    \centering
    \caption{The training time~(hours) in the settings of 1,000 epochs, 4 V100 GPUs, and ResNet-50.}
    \label{tab:training_time}
    {
    \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}lcccccc}
    \shline
    \textbf{Method} & \multicolumn{1}{c}{\textbf{CIFAR-10}} & \multicolumn{1}{c}{\textbf{CIFAR-20}} & \multicolumn{1}{c}{\textbf{STL-10}} & \multicolumn{1}{c}{\textbf{ImageNet-10}} & \multicolumn{1}{c}{\textbf{ImageNet-dogs}} & \multicolumn{1}{c}{\textbf{Tiny-ImageNet}} \\
    \midrule
    BYOL~\cite{grill2020bootstrap} & 9.0 & 9.0 & 14.7 & 1.7 & 2.6 & 13.0 \\
    \methodname~()   & 10.9(+1.9) & 11.0(+2.0) & 15.8(+1.1) & 2.7(+1.0)  & 3.7(+1.1) & 15.7(+2.7)  \\
    \shline
    \end{tabular*}
    }
\end{table*}

\subsubsection{Frequency of performing -means clustering} 
\methodname performs -means clustering for every  epoch. Here, we study how different  influences the clustering performance. The results in Fig.~\ref{fig:hyperparameter_analysis}(a) demonstrate \methodname is robust to large  and the cluster pseudo-labels, which means it is not necessary to perform clustering for every epoch so that the computation cost can be significantly reduced. In summary, we suggest that  can be set to  by considering the datasets and computation resources.

\subsubsection{The hyperparameter  in PSA} 
The hyperparameter  in PSA controls the degree of positive sampling. Taking a look at  in Fig.~\ref{fig:hyperparameter_analysis}(b), although introducing the positive sampling into BYOL causes a slight drop on CIFAR-10, the clustering performance becomes more stable as evidenced by the standard deviation. This is because the neighbors of one sample are regarded as positive examples. Besides, the performance for CIFAR-20 has increased over baseline with the standard deviation reduced. These results indicate that positive sampling can improve the stability of performance. However, when  is too large, the performance becomes unstable and drops a lot. It is not surprising since during positive sampling with large , the instances from other clusters could be sampled and regarded as positive examples. Therefore, we suggest setting   to a small value, saying .



\subsubsection{The hyperparameter  for \lossname} 
The hyperparameter  controls the importantance of \lossname. The results in Fig.~\ref{fig:hyperparameter_analysis}(c) suggest that \methodname is robust to different choices on CIFAR-20. However, the higher  leads to instability on CIFAR-10. The possible reason is that CIFAR-20 is more diverse and has more semantic classes than CIFAR-10~(100 versus 10). Therefore, we suggest that  can be set to , which has demonstrated superior performance on both two datasets.

\subsubsection{Projection dimension} 
The projection dimension describes the embedding space of SSL. The results in Fig.~\ref{fig:hyperparameter_analysis}(d) shows that \methodname achieves consistent and significant performance improvement over baseline regardless of different projection dimension.

\begin{table}[t]
    \centering
    \caption{
        Clustering results (\%) on the subsets of ImageNet. 
    }
    \label{tab:scan_imagenet_subset}
    \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}lcccccc}
        \shline
        \textbf{ImageNet} & \multicolumn{2}{c}{\textbf{50 Classes}} & \multicolumn{2}{c}{\textbf{100 Classes}} & \multicolumn{2}{c}{\textbf{200 Classes}} \\
        \cmidrule{2-7}
        Method   & NMI            & ARI           & NMI            & ARI            & NMI            & ARI            \\
        \midrule
        \tabincell{l}{-means w/ \\ pre-trained MoCo}  & 77.5           & 57.9          & 76.1           & 50.8           & 75.5           & 43.2           \\
        \tabincell{l}{SCAN~\cite{van2020scan} after\\ clustering
step}     & 80.5           & 63.5          & 78.7           & 54.4           & 75.7           & 44.1           \\
        \tabincell{l}{SCAN~\cite{van2020scan} after\\ self-labeling
step}     & \underline{82.2} & \underline{66.1} & \underline{80.8} & \underline{57.6} & \underline{77.2} & \underline{47.0} \\
        \methodname~(\textbf{ours})   & \textbf{82.8}  & \textbf{69.1} & \textbf{83.5}  & \textbf{63.5}  & \textbf{80.6}  & \textbf{53.8}  \\
        \shline
    \end{tabular*}
\end{table}
\subsubsection{ResNet backbone}\quad 
Fig.~\ref{fig:hyperparameter_analysis}(e) shows that with the deeper ResNet networks, \methodname achieves significant improvements with small standard deviations for clustering, demonstrating its superior stability and performance against the baseline method.

\begin{table*}[t]
    \centering
    \caption{
        Clustering results (\%) on long-tailed datasets of different self-supervised learning frameworks and our proposed \methodname. 
    }
    \label{tab:long_tailed_datasets}
    \begin{tabular*}{1\linewidth}{@{\extracolsep{\fill}}l*{6}{c}}
    \shline
    \multirow{3}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{CIFAR-10-LT}}  & \multicolumn{3}{c}{\textbf{CIFAR-20-LT}} \\
    \cmidrule{2-7}
     & \multicolumn{1}{c}{NMI} & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{ARI} & \multicolumn{1}{c}{NMI} & \multicolumn{1}{c}{ACC}      & \multicolumn{1}{c}{ARI} \\
    \midrule
    MoCo v2~\cite{he2020momentum} & 46.7\std{0.1} & 33.4\std{0.3} & 27.7\std{0.0} & 31.2\std{0.3} & 28.2\std{0.2} & 16.1\std{0.3} \\
    BYOL~\cite{grill2020bootstrap} & 51.6\std{1.0} & 41.3\std{0.4} & 30.8\std{0.4} & 41.9\std{0.4} & 34.6\std{0.5} & 22.3\std{1.0} \\ \hline
    \methodname w/o \lossname & \underline{53.1}\std{0.7} & \underline{42.7}\std{0.4} & \underline{31.6}\std{0.8} & \underline{43.4}\std{0.8} & \underline{35.1}\std{0.6} & \underline{24.0}\std{0.1} \\
    \methodname (\textbf{ours}) & \textbf{55.3}\std{0.4} & \textbf{43.9}\std{0.1} & \textbf{36.3}\std{0.3} & \textbf{44.6}\std{0.2} & \textbf{39.0}\std{0.7} & \textbf{27.3}\std{0.2} \\
    \shline
    \end{tabular*}
\end{table*}

\subsubsection{Data augmentation} 
Data augmentation is important for self-supervised learning. Fig.~\ref{fig:hyperparameter_analysis}(f) shows that the performance drops for both BYOL and \methodname when removing some data augmentations. However, the results suggest that \methodname still performs more stable and is robust to data augmentations.

\subsection{Computational Cost}\label{sec:computational_cost}

The main additional computational cost of \methodname is the -means clustering procedure. We have implemented the -means algorithm with -means++~\cite{arthur2006k} initialization using PyTorch to utilize the GPU and accelerate the clustering process. We performed -means clustering with 10 different initialization in cosine distance. Table~\ref{tab:training_time} summarizes the training time of \methodname and BYOL on different datasets. \methodname does not introduce much additional computational cost.
Besides, as suggested in the results of Fig.~\ref{fig:hyperparameter_analysis}(a), \methodname is robust to the different , so there is no need to perform -means for every epoch. Therefore, the training time can be further reduced for \methodname. The computational cost of the \lossname is also small since we build the cluster centers within mini-batch, saying that \methodname does not need additional memory to store the cluster centers. Consequently, considering the promising performance improvements, the additional computational cost is relatively affordable.

\subsection{Subsets of ImageNet}

In addition, we also reported the clustering results on ImageNet subsets like SCAN~\cite{van2020scan} in Table~\ref{tab:scan_imagenet_subset}.
We strictly follow the settings in~\cite{van2020scan}: we have adopted the same 50, 100, and 200 classes from ImageNet, clustered on the training set, and tested on the validation set. We have used the same experimental settings as the other benchmarked datasets and trained \methodname with ResNet-50 for 300 epochs. We note that SCAN has used the pre-trained model of MoCo trained on the full ImageNet for 800 epochs. The results are directly referred from their published paper including -means with pre-trained MoCo, SCAN after the clustering step, and SCAN after the self-labeling step. With much fewer training epochs and training data, \methodname still produces better performance by a clear margin, demonstrating the superiority of \methodname.

\subsection{Long-tail Dataset}
We also conducted additional experiments in Table~\ref{tab:long_tailed_datasets} to demonstrate the ability of \methodname handling the long-tailed datasets.
We built the long-tailed version of CIFAR-10 and CIFAR-20, termed CIFAR-10-LT and CIFAR-20-LT using the codes of~\cite{tang2020long}, which follows~\cite{zhou2020bbn,cao2019learning}. Specifically, they were built upon the training datasets under the control of data imbalance ratio . Consequently, the samples in the long-tailed datasets are almost all in the minority classes~(head), versus few samples in other classes~(tail). MoCo v2 cannot handle this problem well due to the class collision issue, as a result, the samples in the head will be pushed away and the ones in the tail will be mixed together. BYOL and \methodname do not need negative examples so that they outperform MoCo v2 by a large margin. By introducing 
PSA and \lossname, we can further boost the clustering performance of BYOL.

\subsection{Boosting Performance with Memory Queue}
To better represent the prototype of one class, the mini-batch should contain a sufficient number of samples. However, this would significantly increase the requirement of GPU memory when one dataset has a large number of clusters. Here, we highlight that although we use a mini-batch size of , we find that \methodname generalizes well on the datasets such as Tiny-ImageNet with 200 classes (about 1 sample per class in a mini-batch) and ImageNet-1k with 1,000 classes (about 0.25 sample per class in a mini-batch).

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{memory_queue.pdf}
  \caption{
    The performance of \methodname using a memory queue with different sizes on Tiny-ImageNet.
    We repeated each run for 3 times and reported the mean and std values.
    }
  \label{fig:memory_queue}
\end{figure}
To compute class prototypes accurately with a small mini-batch size, we propose to employ a memory queue for updating prototypes. In the vanilla \lossname, we use the representations within mini-batch  to estimate the prototypes in Eqs.~\eqref{eq:compute_centers_1} and~\eqref{eq:compute_centers_2}.
With a memory queue  used in~\cite{he2020momentum} to store the representations from the momentum-updated encoder, we can update the prototypes with samples in both mini-batch and memory queue, .

To show the effectiveness of the memory queue, we conduct the experiments on Tiny-ImageNet as it is challenging enough with 200 classes, and the corresponding results are shown in Fig.~\ref{fig:memory_queue}; we trained the model with the same settings as detailed in Section~\ref{sec:implementation_details} except for 200 epochs.
As shown in Fig.~\ref{fig:memory_queue}, the performance can be further improved with more samples used to compute the prototypes.
However, the performance could drop when the size of memory queue becomes too large, saying 4,096 in Fig.~\ref{fig:memory_queue}. The reason is that when the memory queue is too large, the representations enqueued at early iterations may be far away from the true ones due to the longer update of the encoder~\cite{he2020momentum}.
Moreover, using the memory queue brings more computational costs and introduces an additional hyperparameter---the size of memory queue.
Therefore, \methodname works well when the mini-batch size is small compared to the number of classes, and its performance can be further improved with a memory queue.