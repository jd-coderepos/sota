\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx} \usepackage{float} \usepackage{subfigure} \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{booktabs}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}



\title{A Densely Connected Criss-Cross Attention Network for Document-level Relation Extraction}

\author {
Liang Zhang\textsuperscript{\rm 1} 
    Yidong Cheng \textsuperscript{\rm 1}\\
    \textsuperscript{\rm 1} Department of Artificial Intelligence, School of Informatics, Xiamen University\\
    \texttt{ \{lzhang,ydcheng\}@stu.xmu.edu.cn} \\
}






\begin{document}
\maketitle

\begin{abstract}
Document-level relation extraction (RE) aims to identify relations between two entities in a given document. Compared with its sentence-level counterpart, document-level RE requires complex reasoning. Previous research normally completed reasoning through information propagation on the mention-level or entity-level document-graph, but rarely considered reasoning at the entity-pair-level.
In this paper, we propose a novel model, called \textbf{Dense}ly Connected \textbf{C}riss-\textbf{C}ross Attention \textbf{Net}work (\textbf{Dense-CCNet}), for document-level RE, which can complete logical reasoning at the entity-pair-level. Specifically, the Dense-CCNet performs entity-pair-level logical reasoning through the Criss-Cross Attention (\textbf{CCA}), which can collect contextual information in horizontal and vertical directions on the entity-pair matrix to enhance the corresponding entity-pair representation. 
In addition, we densely connect multiple layers of the CCA to simultaneously capture the features of single-hop and multi-hop logical reasoning.
We evaluate our Dense-CCNet model on three public document-level RE datasets, DocRED, CDR, and GDA. Experimental results demonstrate that our model achieves state-of-the-art performance on these three datasets. 
\end{abstract}

\section{Introduction}
Relation extraction (RE) aims to identify relationships between two entities from raw texts. It is of great importance to many real-world applications such as knowledge base construction, question answering, and biomedical text analysis \cite{c:101}. Most of the existing work focuses on sentence-level RE, which predicts the relationship between entities in a single sentence \cite{c:102,c:103}. 
However, large amounts of relationships are expressed by multiple sentences in real life \cite{c:104}. According to the statistics of the DocRED \cite{c:104} dataset which is obtained from Wikipedia documents, at least 40.7\% of relations can only be extracted from multiple sentences. 
Therefore, researches on document-level RE models that can extract relational facts across sentences have gained increasing attention recently.

\begin{figure}[t]
\centering
\includegraphics[width=0.95 \columnwidth]{figure1.png} \caption{An example comes from the DocRED dataset, which shows that triples with overlapping entities provide important information for reasoning the complex inter-sentential relations. (a) is a document, in which different colors represent different entities. (b) lists some intra-sentential triplets, which can be easily identified. (c) shows a triple whose relationship requires logical reasoning techniques to be recognized. The arrows between (b) and (c) indicate the correlation among triples.}
\label{fig1}
\end{figure}

Compared with sentence-level RE, the main challenge is that many relations in document-level RE could only be extracted through the technique of reasoning.
Since these relationship facts are not explicitly expressed in the document, the model must captures the correlation between the relationships to infer these relationships.
Therefore, capturing the relevance of  the relationships is essential to improve the reasoning ability of document-level RE models.
Figure~\ref{fig1} shows an example from the DocRED dataset. The Figure 1b lists the intra-sentential triplets, such as (\textit{Altomonte, date of birth, 24 February 1694}), (\textit{Altomonte, father, Martino Altomonte}), and (\textit{Altomonte, country of citizenship, Austrian}), which could be easily recognized since two related entities appear in the same sentence. 
However, it is non-trivial to predict the inter-sentential relations between \textit{Martino Altomonte} and \textit{Austrian} because the document does not explicitly express the relationship between them. 
In fact, the model needs to firstly capture the correlation among (\textit{Altomonte, father, Martino Altomonte}), (\textit{Altomonte, country of citizenship, Austrian}), and (\textit{Martino Altomonte, country of citizenship, Austrian}) and use logical reasoning techniques to identify this complex relationship, as shown in Figure 1c.

To extract these complex relationships, most current approaches constructed a document-level graph based on heuristics, structured attention, or dependency structures \cite{c:105,c:106,c:107,c:108}, and then perform inference with graph convolutional network (GCN) \cite{c:109,c:110} on the graph. Meanwhile, considering the transformer architecture can implicitly model long-distance dependencies, some studies \cite{c:111,c:112} directly applied pre-trained models rather than explicit graph reasoning \cite{c:113}. 
These methods captures the correlation between relationships through the information transfer between tokens, mentions or entities, which can be indirect and inefficient.

In this paper, we use the information transfer between the entity-pairs to capture the correlation between relationships more efficiently and directly.
Moreover, as it can be seen in Figure 1, only (\textit{Altomonte, father, Martino Altomonte}) and (\textit{Altomonte, country of citizenship, Austrian}) triples, rather than the other triples, provide important information to infer the relations between \textit{Martino Altomonte} and \textit{Austrian}. 
Inspired by this phenomenon, we guess that the interaction between the triples with overlapping entities is a reasonable way of entity-pair-level reasoning.

Therefore, we propose a novel Dense-CCNet model by integrating the Criss-Cross Attention (\textbf{CCA}) \cite{c:116} into the densely connected framework \cite{c:121}. 
The CCNet model \cite{c:116} is an advanced semantic segmentation model recently proposed in the field of computer vision, which captures global context information from full-image through the CCA (as shown in Figure~\ref{fig2}). 
The CCA applied to the entity-pair matrix can realize the interaction between entity-pairs with overlapping entities, which can complete the logical reasoning of the entity-pair-level. 
To fully capture the features of single-hop and multi-hop reasoning, we stack the multi-layer modules CCA modules by the densely connected framework.
The lower layers in Dense-CCNet can capture local interdependence among entity-pairs and complete single-hop logical reasoning, while the upper layers can capture global interdependence among entity-pairs and complete multi-hop logical reasoning.

Since the CCA can only complete the reasoning mode of \textbf{AB}, we expand the field (which single-layer CCA can pay attention to) to cover a wider range of reasoning modes, such as  \textbf{AB},  \textbf{AB}, and \textbf{AB}. 
In addition, we found that more than 90\% of the entity pairs are irrelevant (that is, there is no relationship between two entities) in the document, and these entity pairs may limit the model’s reasoning ability. To reduce the influence of unrelated entity-pairs, we use two techniques:
\textbf{(1) Clustering loss}: The clustering loss separates the related entity-pairs (that is, there is relationship between two entities) from the unrelated entity-pairs in the representation space.
\textbf{(2) Attention bias}: We add a bias term to a bias term to the attention score of the CCA, which makes the CCA pay more attention to related entity pairs.




In summary, our main contributions are as follows:
\begin{itemize}
\item We introduce the Dense-CCNet module that can more directly and effectively model the correlation between relationships through the entity-pair-level reasoning.
\item We introduce four methods to further improve the reasoning ability of the CCA: Dense connection, Expanding the Field of Attention, Clustering loss, and Attention bias.
\item Experimental results on three public document-level RE datasets shows that our Dense-CCNet model can achieve state-of-the-art performance.
\end{itemize}








\section{Methodology}
In this section, we elaborate on our Dense-CCNet mode. Our entire model (as shown in Figure~\ref{fig2}) is mainly composed of three parts: Encoder module (Sec. \ref{sec2.1}), Dense-CCNet module (Sec. \ref{sec2.2}), and Classifier module (Sec. \ref{sec2.3}). 

\begin{figure*}[t]
\centering
\includegraphics[width=0.9 \textwidth]{figure3-10.png} \caption{The overall architecture of our Dense-CCNet-based document-level RE model. Firstly, the BERT model encodes the input document to obtain the context embedding of each words, and then we obtains the representations of the entities (,) through a pooling operation. Secondly, the relation features () of all entity pairs are calculated through the Relation Feature Calculation module, which is used to construct the original entity-pair matrix (). Thirdly, the Dense-CCNet module transforms  into a context-enhanced entity-pair matrix (). Finally, the context-enhanced relation features () of the entity pairs (, ), the subject entity embedding (), and object entity embedding () are concatenated and inputted to the classifier to predict the relationship.}
\label{fig2}
\end{figure*}







\subsection{Encoder Module}
\label{sec2.1}
We first treat the document  as a sequence of words, i.e. , where  is the total number of words in the document.
Then, we insert special symbols  and  to mark the start and end positions of the mention respectively, where  is the entity id of the mention. 
It is an improved version of entity marker technology \cite{c:112,c:113,c:118,c:103} by introducing entity id information which can help align the information of different mentions from the same entity. 
Finally, we leverage the pre-trained language model as an encoder to convert documents  into a sequence of contextual embeddings as follows:

We take the embedding of  at the start of mentions  as the mention embedding . Then, we leverage logsumexp pooling \cite{c:120}, a smooth version of max pooling, to obtain the embedding  of entity  which contains  mentions :


After obtaining the embedding of all entities in the document, we construct an \textbf{Entity-Pair Matrix}  through the Relation Feature Calculation module, where  refers to the number of entities and  is the dimension of the relation feature vector. 
The  item in  represents the relation feature vector between the entity  and the entity , which is calculated as follows:

where  is subject entity embedding,  is object entity embedding,  is document embedding, and  is entity-pair-aware context feature,  refers to a feed-forward neural network,  is the learnable weight matrix.


We use the embedding of the document start token “[CLS]” as the document embedding , which can help aggregate cross-sentence information and provide document-aware representation.

The entity-pair-aware context feature  represents the contextual information in the document that the entity  and the entity  pay attention to together. 
The  is formulated as follows:

where  is the attention score of the entity  paying attention to the i-th token  in the document.





\subsection{Dense-CCNet Module}
\label{sec2.2}
In this part, we introduce the Dense-CCNet module in detail.
As shown in Figure~\ref{fig2}, the Dense-CCNet module consists of densely connected  identical layers that are composed of two sub-modules: the \textbf{Criss-Cross Attention (CCA) module} and the \textbf{Transition module}.

We followed the CCNet model \cite{c:116} for the CCA module.
Each entity pair in the entity-pair matrix can pay attention to the relation feature of other entity pairs in horizontal and vertical directions through the CCA module.
The CCA module can be formulated as follows:

where  is the attention score of the  paying attention to the .
Therefore, the CCA module can complete entity-pair-level one-hop reasoning on the entity-pair matrix, and it is possible to complete multi-hop reasoning by stacking multiple layers of the CCA module.

However, simply using Recurrent Criss-Cross Attention (\textbf{RCCA}) \cite{c:116} to complete the logical reasoning of the entity-pair level may have several problems:
\textbf{(1)} The RCCA only focuses on the high-level multi-hop inference feature and ignores the low-level single-hop inference feature which is also very important for document-level RE.
\textbf{(2)} The CCA module can only model the reasoning mode of \textbf{AB}, but cannot model the reasoning mode of \textbf{AB},  \textbf{AB}, and \textbf{AB}.
\textbf{(3)} Since most of the entity pairs are irrelevant in the document, the entity-pair matrix  contains a lot of noise which may affect the reasoning ability of the model.  
Therefore, distinguishing related entity-pairs from unrelated entity-pairs and strengthening the interaction of the relationship feature vectors of related entity-pairs is the key to improving The reasoning ability of the model.
To solve these problems, we have introduced the following methods:

\noindent
\textbf{Dense Connection}: Since dense connections can reuse the features of low-level networks, we stack multiple layers of the CCA modules through the densely connected framework to solve the problem \textbf{(1)} .
In addition, the dense connection can also reduce noise propagation to a certain extent.

\noindent
\textbf{Expanding the Field of Attention}: 
To allow the CCA module to model more inference modes, we modify the CCA module as follows: 
The modified CCA module can cover a wider range of reasoning modes including: \textbf{AB}, \textbf{AB},  \textbf{AB}, and \textbf{AB}.

\noindent
\textbf{Clustering Loss}:
We design a clustering loss function that separates the related entity pairs and the unrelated entity pairs in the feature space to reduce the influence of unrelated entity pairs on the inference process. 
Clustering Loss is formulated as follows:

where  is the set of the irrelevant entity pairs,  is the set of the related entity pairs,  is the average vector of the feature vectors of the entity pairs in the ,  is the average vector of the feature vectors of the entity pairs in the ,  is the feature vector of the i-th entity pair.

\noindent
\textbf{Attention Bias}:
To make the CCA more focused on the related entity pairs, we added a bias to the attention score of the CCA:

where,  is a bias term of the entity pair , which reflects the confidence that the entity pair  is a related entity pair.
 is predicted and trained through a feed-forward neural network:

Where  is a cross-entropy loss function, and  is the 0-1 label of the entity pair.

The Transition module controls the dimensions of the new features generated by each layer of the Dense-CCNet model, which reduces the computational complexity of the model.








\subsection{Classification Module}
\label{sec2.3}
We use the Dense-CCNet to convert the original entity-pair matrix  into a new context-enhanced entity-pair matrix .
Given an entity pair , we first concatenate the two entity embedding (, ) and new relation feature , then we obtain the distribution of relationship via a bilinear function. Formally, we have:


For the loss function, we use adaptive-thresholding loss \cite{c:112}, which learns an adaptive threshold for each entity pair. 
The loss function is broken down into two parts as shown below:

where  is an introduced class to separate positive classes and negative classes: positive classes would have higher probabilities than , and negative classes would have lower probabilities than ,  and  are the positive classes set and negative classes set in document D  respectively.

Finally, our total loss function is defined as follows:



\section{Experiments}

\subsection{Datasets}
We evaluate our Dense-CCNet model on three public document-level RE datasets. 
The statistics of the datasets could be found in Appendix~\ref{appendix-a}.
\begin{itemize}
\item \textbf{DocRED} \cite{c:104}:
The DocRED is a large-scale crowdsourced dataset for document-level RE, which was constructed from Wikipedia and Wikidata. 
The DocRED contains 3053 documents for training, 1000 for validating, and 1000 for the test. It involves 97 types of target relations in total, and each document approximately contains 26 entities on average. 
\item \textbf{CDR} \cite{c:122}:
The CDR is a biomedical dataset is constructed by using the PubMed abstracts, which aims to predict the binary interactions between Chemical and Disease concepts. The CDR contains only one relationship and consists of 1500 human-annotated documents in total. 
The CDR are equally split into training, development, and test sets.
\item \textbf{GDA} \cite{c:123}:
Similar to the CDR, the GDA is also a dataset in the biomedical domain, but is constructed by distant supervision from the MEDLINE abstracts. 
The GDA contains 29192 documents as the training set and 1000 as the test set. Since there is no development set, we follow \cite{c:107} to divide the training set into two parts according to the ratio of 8:2 and use them as training set and development set respectively.
\end{itemize}


\begin{table*}[]
\centering
\begin{tabular}{lcccc}
\toprule
Model                & \multicolumn{2}{c}{Dev}   & \multicolumn{2}{c}{Test} \\ 
                     & Ign       &           & Ign      &           \\ [2pt] \toprule
GEDA-\cite{c:130}        & 54.52       & 56.16       & 53.71      & 55.74       \\
LSR- \cite{c:106}        & 52.43       & 59          & 56.97      & 59.05       \\
GLRE-\cite{c:108}        & -           & -           & 55.4       & 57.4        \\
HeterGSAN-\cite{c:134}  & 58.13       & 60.18       & 57.12      & 59.45       \\ 
GAIN-\cite{c:105}       & 59.14       & 61.22       & 59         & 61.24       \\
[2pt] \toprule
\cite{c:135}             & -           & 54.16       & -          & 53.2        \\
BERT-\cite{c:135}          & -           & 54.42       & -          & 53.92       \\
HIN-\cite{c:111}         & 54.29       & 56.31       & 53.7       & 55.6        \\
Coref\cite{c:146}        & 55.32       & 57.51       & 54.54      & 56.96       \\
ATLOP-\cite{c:112}       & 59.22       & 61.09       & 59.31      & 61.3        \\
DocuNet-\cite{c:113}     & 59.86       & 61.83       & 59.93      & 61.86       \\ [2pt] \toprule
Dense-CCNet- & \textbf{60.72(0.12)} & \textbf{62.74(0.15)} & \textbf{60.46} & \textbf{62.55} \\ \bottomrule


\end{tabular}
\caption{\label{tab1} Results (\%) on the development and test set of the DocRED. We follow ATLOP \cite{c:112} and DocuNet \cite{c:113} for the scores of all baseline models. The results on the test set are obtained by submitting to the official Codalab.
}
\end{table*}


\subsection{Experimental Settings}
Our model was implemented based on PyTorch. We used cased BERT-base \cite{c:125} as the encoder on DocRED and SciBERT-base \cite{c:127} on CDR and GDA.
We set the number of layers of Dense-CCNet to 3.
Our model is optimized with AdamW \cite{c:128} with a linear warmup \cite{c:129} for the first 6\% steps followed by a linear decay to 0. 
All hyper-parameters are tuned on the development set, some of which are listed in Appendix~\ref{appendix-b}.


\subsection{Results on the DocRED Dataset}
The experimental results of our model on DocRED are shown in Table~\ref{tab1}. We followed \cite{c:104} and used  and Ign as the evaluation metrics to evaluate the overall performance of the model. Ign denotes the  score excluding the relational facts that are shared by the training and dev/test sets. We compare the Dense-CCNet model with the following two types of models on the DocRED dataset :
\begin{itemize}
    \item \textbf{Graph-based Models}: these models first construct the document-graph from the document, and then perform inferences through GCN \cite{c:110} on the graph. We include GEDA \cite{c:130}, LSR \cite{c:106}, GLRE \cite{c:108}, GAIN \cite{c:105}, and HeterGSAN \cite{c:134} for comparison.
    
    \item \textbf{Transformer-based Models}: These models directly use the pre-trained language models for document-level RE without graph structures. we compared  \cite{c:135}, BERT- \cite{c:135}, HIN- \cite{c:111},  \cite{c:146}, Coref\cite{c:146}, and ATLOP- \cite{c:112} with our model.
\end{itemize}
In addition, we also consider the DocuNet \cite{c:113} model in the comparison, which formulates document-level RE as a semantic segmentation problem. 


As shown in Table~\ref{tab1}, our Dense-CCNet model achieved \textbf{62.74}\%  and \textbf{62.55}\%  in the training set and test set, which outperforms the state-of-the-art model with \textbf{0.91}\%  and \textbf{0.69}\%  respectively.
Compared with the GAIN model that is the state-of-the-art model of graph-based methods, our model exceeds it by \textbf{1.52}\%  on the dev set and \textbf{1.31}\%  on the test set.
This proves that the logical reasoning on the entity-pairs level is more effective than previous methods on mentions or entities level.


\subsection{Results on the Biomedical Datasets}


On the CDR and GDA data sets, we compared BRAN \cite{c:137}, EoG \cite{c:107}, LSR \cite{c:106}, DHG \cite{c:139}, GLRE \cite{c:108}, ATLOP \cite{c:112}, and DocuNet \cite{c:113} with our model.
The experimental results on two biomedical datasets are shown in Table~\ref{tab2}.

Our Dense-CCNet- model obtained \textbf{77.06(0.71)}\%  and \textbf{86.44(0.25)}\%  on two data sets respectively, which is also the new state-of-the-art result.
The Dense-CCNet-  improved the  score by \textbf{0.76}\% and \textbf{1.14}\% on CDR and GDA compared with DocuNet-. 
These results demonstrate the strong applicability and generality of our approach in the biomedical field.

 







\begin{table}[]
\centering
\begin{tabular}{p{4.7cm}cc} 
\toprule
Model               & CDR          & GDA         \\ [2pt] \toprule
BRAN \cite{c:137}                & 62.1         & -           \\
EoG \cite{c:107}                & 63.6         & 81.5        \\
LSR \cite{c:106}                & 64.8         & 82.2        \\
DHG \cite{c:139}                & 65.9         & 83.1        \\
GLRE \cite{c:108}                & 68.5         & -           \\
 \cite{c:127}        & 65.1         & 82.5        \\
ATLOP-\cite{c:112}   & 69.4         & 83.9        \\
DocuNet-\cite{c:113} & 76.3         & 85.3        \\  [2pt] \toprule
Dense-CCNet-         & \textbf{77.06} & \textbf{86.44}    \\
\bottomrule
\end{tabular}
\caption{\label{tab2} F1 scores (\%) on test sets of the CDR and the GDA.}
\end{table}



\subsection{Ablation Study}
We conducted an ablation experiment to validate the effectiveness of different components of our Dense-CCNet model on the development set of the DocRED dataset.
The results are listed in Table~\ref{tab3}, where 
\textbf{w/o Dense Connection} replaces the Densely connected Criss-Cross Attention with the Recurrent Criss-Cross Attention (RCCA),
\textbf{w/o Expanding Attention} uses standard Criss-Cross Attention and does not extend the field of attention, 
\textbf{w/o Clustering Loss} and \textbf{w/o Attention Bias} respectively removes the Clustering Loss and the Attention Bias from our model.

From Table~\ref{tab3}, we can observe that the w/o Dense leads to a drop of \textbf{1.62\%} , which shows that the features of low-level inference are very helpful for relation extraction and  the features of high-level inference may contain noises. 
The w/o Expanding Attention caused a performance drop of \textbf{0.83\%} , which indicates that document-level RE may include multiple inference modes and our model can effectively expand the reasoning mode of the Criss-Cross Attention through the Expanding the Field of Attention technology.

The w/o Clustering Loss module and w/o Attention Bias module led to performance degradation of \textbf{0.72\%}  and \textbf{1.14\%}  points respectively, which reflects that reducing noise (irrelevant entity-pairs) may be the key to further improving entity-pair level inference. 
We guess that the most ideal entity-pair-level reasoning method may be to only propagate information between related entity pairs.

In addition, we also introduced the ablation study of the number of layers of the Dense-CCNet, and the experimental results are shown in Table~\ref{tab4}.
When the number of layers increases from 2 to 3, our model can capture more multi-hop inference features, so the performance of the model is improved by \textbf{1.3\%} .
However, when the number of layers is increased to 4, the performance drops slightly by \textbf{0.37\%}  points.
The possible reasons is that the noise has a greater impact on the high-level feature or the model falls into over-fitting.























\subsection{Case Study}
We followed GAIN \cite{c:105} to select the same example and conduct a case study to further illustrate that our Dense-CCNet model can effectively capture the interdependence among entity-pairs and perform entity-pair-level logical reasoning compared with the baseline.

The experimental results are shown in Figure~\ref{fig3}. Figure 3c demonstrates that our model has better logical reasoning ability than the baseline.
Figure 3a shows that the entity pair (\textit{``Without Me", May 26, 2002}) has more attention to the entity pairs (\textit{``Without Me", The Eminem Show}) and (\textit{The Eminem Show, May 26,2002}),  which indicates that our model could capture the correlation these among entity-pairs.

\begin{table}[]
\centering
\setlength{\tabcolsep}{2.8mm}{
\begin{tabular}{lcc}
\toprule
Model                                   & \multicolumn{2}{c}{Dev}   \\ 
                                        & Ign      &      \\ [2pt] \toprule
Dense-CCNet-BERT                     & \textbf{60.72}   & \textbf{62.74}         \\ 
w/o Dense Connection                       & 59.23       & 61.12       \\
w/o Expanding Attention                      & 59.82       & 61.91       \\
w/o Clustering Loss                      & 59.97       & 62.02        \\
w/o Attention Bias                & 60.65       & 61.60        \\   \bottomrule
\end{tabular}}
\caption{\label{tab3} Ablation study of the Dense-CCNet on the development set of the DocRED. We turn off different components of the model one at a time.
}
\end{table}

\begin{table}[]
\centering
\setlength{\tabcolsep}{5.5mm}{
\begin{tabular}{lcc}
\toprule
Layer-number                           & \multicolumn{2}{c}{Dev}   \\ 
                                        & Ign      &      \\ [2pt] \toprule
2-Layer                           & 59.41       & 61.44       \\
3-Layer                          & \textbf{60.72} & \textbf{62.74}       \\
4-Layer                           & 60.30       & 62.27       \\ \bottomrule


\end{tabular}}
\caption{\label{tab4} Performance of the Dense-CCNet with the different numbers of layers on the development set of the DocRE.}
\end{table}



\begin{figure*}[t]
\centering
\includegraphics[width=2.0 \columnwidth]{figure4-8.png} \caption{Case study of our Dense-CCNet mode and the baseline model. (c) shows that our model has better logical reasoning ability than the baseline. (a) visualize the attention scores of entity pairs (\textit{``Without Me", May 26, 2002}) paying attention to other entity pairs, which shows that our model can effectively capture the correlation among entity-pairs.}
\label{fig3}
\end{figure*}


\section{Related Work}
\textbf{Sentence-level RE}: 
Early research on RE focused on sentence-level RE, which predicts the relationship between two entities in a single sentence.
Many approaches \cite{c:140,c:141,c:143,c:144,c:145,c:146,c:147,c:148,c:149,c:150} have been proven to effectively solve this problem.
Since many relational facts in real applications can only be recognized across sentences, sentence-level RE face an inevitable restriction in practice.

\noindent \textbf{Document-level RE}: 
To solve the limitations of sentence-level RE in reality, a lot of recent work gradually shift their attention to document-level RE.
Since graph neural network(GNN) can effectively model long-distance dependence and complete logical reasoning, Many methods based on document-graphs are widely used for document-level RE.
Specifically, they first constructed a graph structure from the document, and then applied the GCN \cite{c:110,c:121} to the graph to complete logical reasoning.
The graph-based method was first introduced by \cite{c:152} and has recently been extended by many works \cite{c:107,c:130,c:139,c:151,c:108,c:106,c:105,c:134}.
\cite{c:130} proposed the Graph Enhanced Dual Attention network (GEDA) model and used it to characterize the complex interaction between sentences and potential relation instances.
\cite{c:105} propose Graph Aggregation-and-Inference Network (GAIN) model.
GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document and then constructs an entity-level graph (EG), finally uses the path reasoning mechanism to infer relations between entities on EG.
\cite{c:106} proposed a novel LSR model, which constructs a latent document-level graph and completes logical reasoning on the graph.

In addition, due to the pre-trained language model based on the transformer architecture can implicitly model long-distance dependence and complete logical reasoning, some studies \cite{c:111,c:112,c:135} directly apply pre-trained model without introducing document graphs.
\cite{c:112} proposed an ATLOP model that consists of two parts: adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems.
Recently, the state-of-the-ar model, DocuNet \cite{c:113},  formulates document-level RE as semantic segmentation task and capture  global information among relational triples through the U-shaped segmentation module \cite{c:115}.



However, none of the models completes the logical reasoning for document-level RE through the information propagation between the entity-pairs.
Our Dense-CCNet model can capture the correlation among entity-pairs and complete the entity-pair-level reasoning by integrating the CCA \cite{c:116} into the dense connection framework \cite{c:121}.


\section{Conclusion and Future Work}
In this work, we propose a novel Dense-CCNet model by integrating the Criss-Cross Attention into the densely connected framework. 
Dense-CCNet model can complete entity-pairs-level logical reasoning and model the correlation between entity pairs.
Experiments on three public document-level RE datasets demonstrate that our Dense-CCNet model achieved better results than the existing state-of-the-art model.
In the future, we will try to use our model for other inter-sentence or document-level tasks, such as cross-sentence collective event detection.


\bibliography{anthology,custom,acl_latex.bbl}
\bibliographystyle{acl_natbib}
\begin{table*}[]
\centering
\setlength{\tabcolsep}{7.0mm}{
\begin{tabular}{lccc}
\toprule
Dataset                     & DocRED     &CDR       &GDA  \\  [2pt] \toprule
Train                     &3053	    &500	    &23353         \\ 
Dev                       &1000	    &500	    &5839       \\
Test                      &1000	    &500	    &1000       \\
Relations                 &97	    &2	    &2        \\
Entities per Doc        &19.5	    &7.6	    &5.4        \\
Mentions per Doc        &26.2	    &19.2	    &18.5        \\ 
Entities per Sent        &3.58	    &2.48	    &2.28        \\     \bottomrule

\end{tabular}}
\caption{\label{tab5} Summary of DocRED, CDR and GDA datasets.
}
\end{table*}

\begin{table*}[]
\centering
\setlength{\tabcolsep}{7.0mm}{
\begin{tabular}{lccc}
\toprule
Hyperparam                     & DocRED     &CDR       &GDA  \\ 
                            & BERT     &SciBERT       &SciBERT  \\  [2pt] \toprule
Batch size                    &8	    &16	    &16         \\ 
Epoch                       &100	    &20	    &5 \\
lr for encoder                 &2e-5	    &1e-5	    &1e-5        \\
lr for other parts       &1e-4	    &5e-5	    &5e-5         \\
\{, \}                 &\{1, 0.5\}	    &\{1, 0.5\}	    &\{1, 0.5\}        \\
\{, , \}      &\{1, 1, 1\}	   &\{1, 1, 1\}    &\{1, 1, 1\}        \\\bottomrule

\end{tabular}}
\caption{\label{tab6} Hyper-parameters Setting.}
\end{table*}

\appendix
\newpage
\section{Datasets}
\label{appendix-a}
Table~\ref{tab5} details the statistics of the three document-level relational extraction datasets, DocRED, CDR, and GDA. These statis-tics further demonstrate the complexity of entity structure in document-level relation extraction tasks.


\section{Hyper-parameters Setting}
\label{appendix-b}
Table~\ref{tab6} details our hyper-parameters setting. All of our hyperparameters were tuned on the development set.

\end{document}
