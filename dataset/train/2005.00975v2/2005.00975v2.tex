

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm, algorithmic, letltxmacro}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{tikz-dependency}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{booktabs}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy \def\aclpaperid{395} 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Efficient Second-Order TreeCRF for Neural Dependency Parsing}

\author{
    Yu Zhang,
    Zhenghua Li\thanks{Corresponding author},
    Min Zhang \\
    Institute of Artificial Intelligence, School of Computer Science and Technology, \\
    Soochow University, Suzhou, China \\
    {yzhang.cs@outlook.com, \{zhli13,minzhang\}@suda.edu.cn}
}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
\label{section:abstract}
In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation.
As the most popular graph-based dependency parser
due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss.
This paper for the first time presents a second-order TreeCRF extension to the biaffine parser.
For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF.
To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.
Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data.
We release our code at \url{https://github.com/yzhangcs/crfpar}.
\end{abstract} \section{Introduction}
\label{section:introduction}

As a fundamental task in NLP, dependency parsing has attracted a lot of research interest due to its simplicity and multilingual applicability in capturing both syntactic and semantic information \cite{nivre-lrec2016-UD1.0}.
Given an input sentence ,
a dependency tree, as depicted in Figure~\ref{fig:dep-tree-example}, is defined as ,
where  is a dependency from the head word  to the modifier word  with the relation label .
Between two mainstream approaches, this work focuses on the graph-based paradigm (vs. transition-based).


\begin{figure}[tb]
\begin{center}
\begin{dependency}\begin{deptext}[column sep=.16cm] \
\label{equation:input}
\mathbf{e}_i=\mathrm{emb}({w_i}) \oplus \mathrm{CharLSTM}(w_i)

\label{mlp-arc}
\mathbf{r}_i^{h}; \mathbf{r}_i^{m} =\mathrm{MLP}^{h/m} \left( \mathbf{h}_i \right)
 \label{equation:biaffine}
s(i,j) =  \left[
\begin{array}{c}
  \mathbf{r}_{j}^{m}    \\
    1
\end{array}
\right]^\mathrm{T}
\mathbf{W}^\textit{biaffine}  \mathbf{r}_{i}^{h}
 \label{equation:biaffine-loss}
\mathit{L}(i,j) = -\log{\frac{e^{s(i,j)}}{\sum_{0 \le k \le n} e^{s(k,j)}}}

\label{equation:map-decoding}
{\boldsymbol{y}}^* = \arg\max_{\boldsymbol{y}} \left[ s(\boldsymbol{x},\boldsymbol{y}) \equiv
\sum_{i \rightarrow j \in \boldsymbol{y}}{s(i,j)} \right]
\label{eq:score-definition-2o}
s(\boldsymbol{x}, \boldsymbol{y}) = \sum_{i\rightarrow j \in \boldsymbol{y}}s(i,j) + \sum_{
i\rightarrow \{k,j\} \in \boldsymbol{y} } s(i,k,j)
\label{equation:prob-labeled}
\begin{split}
& p(\boldsymbol{y}\mid\boldsymbol{x})  = \frac{e^{s(\boldsymbol{x},\boldsymbol{y})}}{Z(\boldsymbol{x}) \equiv \sum_{\boldsymbol{y'} \in \mathcal{Y}(\boldsymbol{x})} {e^{s(\boldsymbol{x},\boldsymbol{y'})}}}
\end{split}
\label{equation:training-loss-treecrf}
\begin{split}
\mathit{L}(\boldsymbol{x},\boldsymbol{y}) &= -\log p(\boldsymbol{y}\mid\boldsymbol{x})  \\
&= - s(\boldsymbol{x}, \boldsymbol{y}) + \log Z(\boldsymbol{x})
\end{split}

\label{mlp-sib}
\mathbf{r}_i^{h'}; \mathbf{r}_i^{s}; \mathbf{r}_i^{m'} =\mathrm{MLP}^{h'/s/m'} \left( \mathbf{h}_i \right)
 \label{equation:triaffine}
s(i,k,j) =
\left[
\begin{array}{c}
\mathbf{r}_{k}^{s} \\
1
\end{array}
\right]^\mathrm{T}
{\mathbf{r}_{i}^{h'}}^\mathrm{T}
\mathbf{W}^\textit{triaffine}
\left[
\begin{array}{c}
\mathbf{r}_{j}^{m'} \\
1
\end{array}
\right]

\label{equation:partial-derivative}
\begin{split}
\frac{\partial \log Z}{\partial \mathrm{s}(i, j)} &= \sum_{\boldsymbol{y}:(i,j) \in \boldsymbol{{y}}} p(\boldsymbol{y}\mid\boldsymbol{x}) = p(i \rightarrow j\mid\boldsymbol{x})
\end{split}

\label{equation:training-loss-treecrf-partial}
\begin{split}
\mathit{L}(\boldsymbol{x}, {\boldsymbol{y}^p}) &= -\log \sum\limits_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{x}); \boldsymbol{y} \supseteq {\boldsymbol{y}^p}} p(\boldsymbol{y}\mid\boldsymbol{x})  \\
&= - \log \frac{Z(\boldsymbol{x}, {\boldsymbol{y}^p}) \equiv \sum\limits_{\boldsymbol{y} \in \mathcal{Y}(\boldsymbol{x}); \boldsymbol{y} \supseteq \boldsymbol{y}^p} e^{s(\boldsymbol{x},\boldsymbol{y})}}{Z(\boldsymbol{x})}
\end{split}
2pt]
\hline
\3pt]
\textsc{Loc}             & 95.82          & 93.99          & 96.08          & 94.47 \\
\textsc{Crf} w/o MBR     & 95.74          & 93.96          & 96.04          & 94.34 \\
\textsc{Crf}             & 95.76          & 93.99          & 96.02          & 94.33 \\
\textsc{Crf2o} w/o MBR   & \textbf{95.92} & \textbf{94.16} & \textbf{96.14} & \textbf{94.49} \\
\textsc{Crf2o}           & 95.90          & 94.12          & 96.11          & 94.46 \-8pt]
\multicolumn{5}{c}{CoNLL09} \\
Biaffine17 & - & - & 88.90 & 85.38 \\
Li19 & 88.68 & 85.47 & 88.77 & 85.58 \2pt]
\hline
\2pt]
\hline
\2pt]
\hline
\
\begin{split}
& {\boldsymbol{y}}^* = \arg\max_{\boldsymbol{y}} \left[\sum_{i \rightarrow j \in \boldsymbol{y}}{p(i \rightarrow j|\boldsymbol{x})} \right]
\end{split}
1pt]
\hline
\1pt]
 &         90.44  &         91.11  &         91.04                   &         80.21                   &         86.86                   &         90.67                   &         87.99  &         91.19                    &         88.24                    &         90.35                   &         86.24                    &         93.01                    &         88.95 \\
\textsc{Loc}                  &         90.45  &         91.14  &         90.97                   &         80.02                   &         86.83                   &         90.56                   &         87.76  &         91.14                    &         87.72                    &         90.74                   &         86.20                    &         93.01                    &         88.88 \\
\textsc{Crf}                  &         90.73  &         91.25  &         91.01                   & \textbf{80.56}\rlap{} &         86.92                   &         90.81\rlap{}  & \textbf{88.16} &         91.64\rlap{}   &         88.10                    &         90.85                   &         86.50                    &         93.17\rlap{}   &         89.14\rlap{} \\
\textsc{Crf2o}                & \textbf{90.77} & \textbf{91.29} & \textbf{91.54}\rlap{} &         80.46                   & \textbf{87.32}\rlap{} & \textbf{90.86}\rlap{} &         87.96  & \textbf{91.91}\rlap{} & \textbf{88.62}\rlap{} & \textbf{91.02}\rlap{} & \textbf{86.90}\rlap{} & \textbf{93.33}\rlap{} & \textbf{89.33}\rlap{} \1pt]
Ji19           & 88.28 & 89.90 & 89.85 & 77.09 & 81.16 & 88.93 & 83.73 & 88.91 & 84.82 & 86.33 & 84.44 & 86.62 & 85.83 \\
\textsc{Crf2o} & \textbf{89.72} & \textbf{91.27} & \textbf{90.94}  & \textbf{78.26} & \textbf{82.88} & \textbf{90.79} & \textbf{86.33} & \textbf{91.02} & \textbf{87.92} & \textbf{90.17} & \textbf{85.71} & \textbf{92.49} & \textbf{88.13} \\
\hline
\1pt]
   &         90.56  &         91.03  &         91.98  &         81.59  & 86.83 &         90.64  & 88.23 &         91.67  &         88.20  &         90.63  & 86.51 & 93.03 &         89.23 \\
\textsc{Loc}                    &         90.57  &         91.10  &         91.85  &         81.68  & 86.54 &         90.47  & 88.40 &         91.53  &         88.18  &         90.65  & 86.31 & 92.91 &         89.19 \\
\textsc{Crf}   &         90.52  & \textbf{91.19} &         92.02  &         81.43  &         86.88\rlap{}  &         90.76\rlap{}  &         88.75  &         91.76  &         88.08  & \textbf{90.79} & 86.54 & 93.16\rlap{} &         89.32\rlap{} \\
\textsc{Crf2o} & \textbf{90.76} &         91.12  & \textbf{92.15}\rlap{} & \textbf{81.94} & \textbf{86.93}\rlap{} & \textbf{90.81}\rlap{} &         \textbf{88.83}\rlap{}  & \textbf{92.34}\rlap{} & \textbf{88.21}\rlap{} & 90.78 & \textbf{86.62} & \textbf{93.22}\rlap{} & \textbf{89.48}\rlap{} \\
\multicolumn{14}{c}{using gold POS tags} \1pt]
Zhang19        & 90.15 & 91.39 & 91.10 & 83.39 & 88.52 & 90.84 & 88.59 & 92.49 & 88.37 & 92.82 & 84.89 & 93.11 & 89.85 \\
\textsc{Crf2o} & \textbf{91.32} & \textbf{92.57} & \textbf{92.66} & \textbf{84.56} & \textbf{88.98} & \textbf{91.88} & \textbf{89.83} & \textbf{92.94} & \textbf{89.85} & \textbf{93.26} & \textbf{87.39} & \textbf{93.86} & \textbf{90.76} \\
\bottomrule
\end{tabular}
\caption{LAS on UD2.2 and UD2.3 test datasets.
Again,  and  means significance level at  and  respectively against the \textsc{Loc} parser. }
\label{table:ud2.3-test}
\end{table*}


 
Beyond the dependency-wise accuracy (UAS/LAS), we
would like to evaluate the models regarding performance at sub-tree and full-tree levels.
Table~\ref{table:dev-test-subtree} shows the results. We skip the partially labeled NLPCC19 data.
UCM means unlabeled complete matching rate, i.e., the percent of sentences obtaining whole correct skeletal trees, while
LCM further requires that all labels are also correct.


For SIB, we evaluate the model regarding unlabeled adjacent-sibling subtrees (system outputs vs. gold-standard references). According to Equation~\ref{eq:score-definition-2o},
 is an adjacent-sibling subtree, if and only if  and  are both children of  at the same side, and there are no other children of  between them.
Given two trees, we can collect all adjacent-sibling subtrees and compose two sets of triples.
Then we evaluate the P/R/F values.
Please note that it is impossible to evaluate SIB for partially annotated references.

We can clearly see that by modeling adjacent-sibling subtree scores,
the SIB performance obtains larger improvement than both \textsc{Crf} and \textsc{Loc},
and this further contributes to the large improvement on full-tree matching rates (UCM/LCM).





\paragraph{Capability to learn from partial trees.}

To better understand why \textsc{Crf2o} performs very well on partially annotated NLPCC19,
we design more comparative experiments by retaining either a proportion of random training sentences (full trees) or a proportion of random dependencies for each sentence (partial trees).
Figure~\ref{fig:part-gap} shows the results.


We can see that the performance gap is quite steady
when we gradually reduce the number of training sentences.
In contrast, the gap clearly becomes larger when each training sentence has less annotated dependencies.
This shows that \textsc{Crf2o} is superior to the basic \textsc{Loc} in
utilizing partial annotated data for model training.








 \subsection{Results on Universal Dependencies}

Table~\ref{table:ud2.3-test}
compares different models on UD datasets, which contain a lot of non-projective trees.
We adopt the pseudo-projective approach \cite{nivre-nilsson-2005-pseudo} for handling the ubiquitous non-projective trees of most languages. Basically, the idea is to transform non-projective trees into projective ones using more complex labels for post-processing recovery.






We can see that for the basic local parsers,
the direct non-projective  and the pseudo-projective \textsc{Loc}
achieve very similar performance.

More importantly, both \textsc{Crf} and \textsc{Crf2o} produce consistent improvements over the baseline in many languages.
On both UD2.2 and UD2.3, Our proposed \textsc{Crf2o} model achieves the highest accuracy for 10 languages among 12, and obtains significant improvement in more than 7 languages.
Overall, the averaged improvement is 0.45 and 0.29 on UD2.2 and UD2.3 respectively, which is also significant at .

On average, our \textsc{Crf2o} parser outperforms \citet{ji-etal-2019-graph} by 2.30 on UD2.2 raw texts following CoNLL-2018 shared task setting, and \citet{zhang-etal-2019-empirical} by 0.91 on UD2.3 data with gold POS tags.
It is noteworthy that the German (de) result is kindly provided by Tao Ji after rerunning their parser with predicted XPOS tags, since their reported result in \citet{ji-etal-2019-graph} accidentally used gold-standard sentence segmentation, tokenization, and XPOS tags.
Our \textsc{Crf2o} parser achieves an average LAS of 87.64 using their XPOS tags.









%
 
 \section{Related Works}
\label{section:relwork}



Batchification has been widely used in linear-chain CRF, but is rather complicated for tree structures.
\citet{eisner-2016-inside} presents a theoretical proof on the equivalence of outside and back-propagation for constituent tree parsing, and also briefly discusses other formalisms such as dependency grammar.
Unfortunately, we were unaware of Eisner's great work until we were surveying the literature for paper writing.
As an empirical study, we believe this work is valuable and makes it practical to deploy TreeCRF models in real-life systems.

\citet{falenska-kuhn-2019-non} present a nice analytical work on  dependency parsing, similar to \citet{gaddy-etal-2018-whats} on constituency parsing.
By extending the first-order graph-based parser of \citet{kiperwasser-goldberg-2016-simple}
into second-order, they try to find out
how much structural context is implicitly captured by the BiLSTM encoder.
They concatenate three BiLSTM output vectors () for scoring adjacent-sibling subtrees,
and adopt max-margin loss and the second-order Eisner decoding algorithm \cite{mcdonald-pereira-2006-online}.
Based on their negative results and analysis, they draw the conclusion that high-order modeling is redundant
because BiLSTM can implicitly and effectively encode enough structural context.
They also present a nice survey on the relationship between RNNs and syntax.
In this work, we use a much stronger basic parser and observe more significant UAS/LAS improvement than theirs.
Particularly, we present an in-depth analysis showing that explicitly high-order modeling
certainly helps the parsing model and thus is complementary to the BiLSTM encoder.



\citet{ji-etal-2019-graph} employ graph neural networks to incorporate high-order structural information into the biaffine parser implicitly.
They add a three-layer graph attention network (GAT) component \cite{velickovic2018graph} between the MLP and Biaffine layers.
The first GAT layer takes  and  from MLPs as inputs and produces new representation  and  by aggregating neighboring nodes. Similarly, the second GAT layer operates on  and , and produces  and .
In this way, a node gradually collects multi-hop  high-order information as global evidence for scoring single dependencies.
They follow the original local head-selection training loss.
In contrast, this work adopts global TreeCRF loss and explicitly incorporates high-order scores into the biaffine parser.


\citet{zhang-etal-2019-empirical} investigate the usefulness of structural training for the first-order biaffine parser.
They compare the performance of local head-selection loss, global max-margin loss, and TreeCRF loss on multilingual datasets.
They show that TreeCRF loss is overall slightly superior to max-margin loss, and LAS improvement from  structural learning is modest but significant for some languages.
They also show that structural learning (especially TreeCRF) substantially improves sentence-level complete matching rate, which is consistent with our findings.
Moreover, they explicitly compute the inside and outside algorithms on CPUs via Cython programming.
In contrast, this work proposes an efficient second-order TreeCRF extension to the biaffine parser,
and presents much more in-depth analysis to show the effect of both structural learning and high-order modeling.
































%
 \section{Conclusions}
\label{section:conclusions}


This paper for the first time presents second-order TreeCRF for neural dependency parsing using triaffine for explicitly scoring second-order subtrees.
We propose to batchify the inside algorithm to accommodate GPUs.
We also empirically verify that the complex outside algorithm can be implicitly performed via efficient back-propagation, which naturally produces gradients and
marginal probabilities.
We conduct experiments and detailed analysis on 27 datasets from 13 languages, and  find that structural learning and high-order modeling can further enhance  the state-of-the-art biaffine parser in various aspects: 1) better convergence behavior; 2) higher performance on sub- and full-tree levels; 3)
 better utilization of partially annotated data.















 
\section*{Acknowledgments}

The authors would like to thank: 1) the anonymous reviewers for the helpful comments, 2) Wenliang Chen for helpful discussions on high-order neural dependency parsing, 3) Tao Ji for kindly sharing the data and giving beneficial suggestions for the experiments on CoNLL18 datasets, 4)
Wei Jiang, Yahui Liu, Haoping Yang, Houquan Zhou and Mingyue Zhou for their help in paper writing and polishing.
This work was supported by National Natural Science Foundation of China (Grant No. 61876116, 61525205, 61936010) and a Project Funded by the Priority Academic Program Development (PAPD) of Jiangsu Higher Education Institutions.

\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\appendix
\section{More on Efficiency}
\label{section:decoding-efficiency}
\paragraph{Training speed.}
During training, we greedily find the 1-best head for each word without tree constraints.
Therefore, the processing speed is faster than the evaluation phase. Specifically, for \textsc{Loc}, \textsc{Crf} and \textsc{Crf2o}, the average one-iteration training time is about 1min, 2.5min and 3.5min on PTB.
In other words, the parser consumes about 700/300/200 sentences per second.

\paragraph{MST decoding.}
As \citet{dozat-etal-2017-stanfords} pointed out, they adopted an ad-hoc approximate algorithm which does not guarantee to produce the highest-scoring tree, rather than the ChuLiu/Edmonds algorithm for MST decoding.
The time complexity of the ChuLiu/Edmonds algorithm is  under the optimized implementation of \citet{tarjan1977finding}.
Please see the discussion of \citet{mcdonald-etal-2005-non} for details.


For , we directly borrow the MST decoding approach in the original parser of \citet{Timothy-d17-biaffine}.  achieves 94.43 LAS on PTB-test (inferior to 94.47 of \textsc{Loc}, see Table~\ref{table:dev-test}), and its parsing speed is over 1000 sentences per second.

\paragraph{Faster decoding strategy.}
Inspired by the idea of ChuLiu/Edmonds algorithm, we can further improve the efficiency of the CRF parsing models by avoiding the Eisner decoding for some sentences.
The idea is that if by greedily assigning a local max-scoring head word to each word, we can already obtain a legal projective tree, then we omit the decoding process for the sentence.
We can judge whether an output is a legal tree (single root and no cycles) using the Tarjan algorithm in  time complexity.
Further, we can judge whether a tree is a projective tree also in a straightforward way very efficiently.
In fact, we find that more than 99\% sentences directly obtain legal projective trees on PTB-test by such greedy assignment on marginal probabilities first.
We only need to run the decoding algorithm for the left sentences.


\end{document}
