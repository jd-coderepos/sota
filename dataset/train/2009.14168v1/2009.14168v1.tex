\documentclass{article}









\usepackage[final,nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}  \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{subcaption}
\usepackage{graphicx,here,comment}
\usepackage{bm}
\usepackage{cite}
\usepackage{wrapfig}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{mathtools,xparse,amsmath}


\newcommand{\R}{\mathbb{R}}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\mA}{\mathcal{A}}
\newcommand{\mB}{\mathcal{B}}
\newcommand{\mC}{\mathcal{C}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mJ}{\mathcal{J}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\mP}{\mathcal{P}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\mX}{\mathcal{X}}
\newcommand{\mY}{\mathcal{Y}}
\newcommand{\mZ}{\mathcal{Z}}
\newcommand{\mR}{\mathcal{R}}
\newcommand{\mT}{\mathcal{T}}
\newcommand{\mL}{\mathcal{L}}
\newcommand{\mE}{\mathcal{E}}
\newcommand{\mN}{\mathcal{N}}


\newcommand{\dE}{\mathds{E}}

\newcommand{\Ep}{\mathrm{E}}
\renewcommand{\Pr}{\mathrm{P}}
\newcommand{\Tr}{\mathrm{T}}
\newcommand{\Cr}{\mathrm{C}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\Inj}{\mathrm{Inj}}
\newcommand{\Dr}{\mathrm{D}}
\newcommand{\Sr}{\mathrm{S}}
\newcommand{\mfM}{\mathfrak{M}}
\newcommand{\mfE}{\mathfrak{E}}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\newcommand{\pr}{\mbox{Pr}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{condition}{Condition}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}






\title{Self-Supervised Few-Shot Learning on Point Clouds}



\author{Charu Sharma and Manohar Kaul \\
  Department of Computer Science \& Engineering\\
  Indian Institute of Technology Hyderabad, India\\
\texttt{charusharma1991@gmail.com, mkaul@iith.ac.in} \\
}

\begin{document}

\maketitle

\begin{abstract}
\if 0	
The recent proliferation of a wide range of applications such as robotics, shape synthesis, self-driving cars, etc., has further fueled research in point cloud processing. Prior work on point clouds which can directly handle the raw point clouds in their deep networks perform supervised learning tasks such as classification and segmentation. Supervised method creates the problem of annotating large amount of point clouds. The need of large amount of labeled point clouds has given momentum to researchers to develop methods from unlabeled data. In this paper, we propose self-supervised few-shot learning method on raw point clouds which not only uses unlabeled point cloud data (self-supervised), but also few samples to train the network (few-shot learning). This resolves the problem of generating large amount of labeled point clouds. For our self-supervised method, we construct cover-tree to cover the set of points in a point cloud with variable-sized balls and generate self-supervised labels for two tasks, classification and regression. Our neural network architecture is trained in a few-shot setting and can be treated as a feature extractor post self-supervised training, to extract point cloud representations. Our method can be used further for any raw point cloud processing network for supervised tasks as it is independent of any specific architecture for raw point clouds. 

The increased availability of massive point clouds coupled with a wide variety of applications such as robotics, shape synthesis, and self-driving cars that utilize point cloud data representations has attracted increased attention from both industry and academia. 
Recently, deep neural networks operating on labeled point clouds have shown promising results on supervised learning tasks like classification and segmentation. However, supervised learning leads to the cumbersome task of annotating the point clouds. To combat this problem, we propose two novel self-supervised pre-training tasks that encode a hierarchical partitioning of the point clouds with balls of varying radii in a cover-tree, accompanied by surrogate class labels generated from the ball partitioning, at several scales, imposed on the data by our cover-tree. Furthermore, our self-supervised learning network is restricted to pre-train on the support set (comprising of scarce training examples) used to train the downstream network in a few-shot learning (FSL) setting. Finally, the fully-trained self-supervised network's point embeddings are used to initialize the downstream task's network. We present a comprehensive empirical evaluation of our method in both downstream classification and segmentation tasks and show that supervised methods pre-trained with our self-supervised learning method significantly improves the accuracy of the state-of-the-art methods. Additionally, our method also outperforms previous unsupervised methods in downstream classification tasks.
\fi
The increased availability of massive point clouds coupled with their utility in a wide variety of applications such as robotics, shape synthesis, and self-driving cars has attracted increased attention from both industry and academia. Recently, deep neural networks operating on labeled point clouds have shown promising results on supervised learning tasks like classification and segmentation. However, supervised learning leads to the cumbersome task of annotating the point clouds. To combat this problem, we propose two novel self-supervised pre-training tasks that encode a hierarchical partitioning of the point clouds using a cover-tree, where point cloud subsets lie within balls of varying radii at each level of the cover-tree. Furthermore, our self-supervised learning network is restricted to pre-train on the support set (comprising of scarce training examples) used to train the downstream network in a few-shot learning (FSL) setting. Finally, the fully-trained self-supervised network's point embeddings are input to the downstream task's network. We present a comprehensive empirical evaluation of our method on both downstream classification and segmentation tasks and show that supervised methods pre-trained with our self-supervised learning method significantly improve the accuracy of state-of-the-art methods. Additionally, our method also outperforms previous unsupervised methods in downstream
classification tasks.

\end{abstract}

\section{Introduction}
Point clouds find utility in a wide range of applications from a diverse set of domains such as indoor navigation~\cite{zhu2017target}, self-driving vehicles~\cite{liang2018deep}, robotics~\cite{rusu2008towards}, shape synthesis and modeling~\cite{golovinskiy2009shape}, to name a few. These applications require reliable $3$D geometric features extracted from point clouds to detect objects or parts of objects. Rather than follow the traditional route of generating features from edge and corner detection methods~\cite{guo20143d,lu2014recognizing} or creating hand-crafted features based on domain knowledge and/or certain statistical properties~\cite{rusu2008aligning,rusu2009fast}, recent methods focus on learning \emph{generalized representations} which provide semantic features using labeled and unlabeled point cloud datasets.

\if 0
The intrinsic and elementary representation of shapes in 3D space can be presented by geometric point sets called point clouds. These are produced by LiDAR scanners in a 3D scanning process. 

 Instead of computing traditional geometric features such as edges, corners, faces~\cite{guo20143d}~\cite{lu2014recognizing}, hand-crafted features or certain statistical properties~\cite{rusu2008aligning}~\cite{rusu2009fast}, the recent advances are based on learning generalized representations which provide semantic features using labeled and unlabeled point cloud datasets. 
\fi
 
 As point clouds are \emph{irregular} and \emph{unordered}, there exist two broad categories of methods that learn representations from point clouds. 
 First, methods that convert \emph{raw point clouds} (irregular domain) to \emph{volumetric voxel-grid representations} (regular domain), in order to use traditional convolutional architectures that learn from data belonging to regular domains (e.g., images and voxels). 
 Second, methods that try to learn representations directly from raw unordered point clouds~\cite{dgcnn,qi2017pointnet,qi2017pointnet++,li2018pointcnn}. 
 However, both these class of methods suffer from drawbacks. Namely, converting raw point clouds to voxels incurs a lot of additional memory and introduces unwanted \emph{quantization artifacts} ~\cite{huang1998accurate}, while supervised learning techniques on both voxelized and raw point clouds suffer from the burden of manually annotating massive point cloud data.



To address these problems, research in developing \emph{unsupervised} and \emph{self-supervised} learning methods, relevant to a range of diverse domains, has recently gained a lot of traction. The state-of-the-art unsupervised or self-supervised methods on point clouds are mainly based on \emph{generative adversarial networks} (GANs)~\cite{wu2016learning,achlioptas2018learning,han2019view} or auto-encoders~\cite{yang2018foldingnet,zhao20193d,insafutdinov2018unsupervised,sharma2016vconv}. These methods work with raw point clouds or require either voxelization or 2D images of point clouds. 
And, some of the unsupervised networks depend on computing a \emph{reconstruction error} using different similarity metrics such as the \emph{Chamfer distance} and \emph{Earth Mover's Distance} (EMD)~\cite{yang2018foldingnet}.
Such metrics are computationally inefficient and require significant variability in the data for better results~\cite{achlioptas2018learning}~\cite{yang2018foldingnet}.
\begin{wrapfigure}{r}{0.33\linewidth}
	\centering
    \begin{subfigure}{.33\textwidth}
		\centering
		\includegraphics[clip, trim=1.2cm 1.0cm 1.2cm 1.0cm,width=0.92\linewidth]{ball/table_2ball_a5_1.png}
\label{fig:ball5}
    \end{subfigure}\\
    \vspace{1mm}
    \begin{subfigure}{.33\textwidth}
		\centering
		\includegraphics[clip, trim=1.0cm 0.8cm 1.0cm 0.8cm,width=0.92\linewidth]{ball/table_3ball_a5_die_air_1.png}
\label{fig:ball6}
	\end{subfigure}\caption{Coarse-grained (top) and finer-grained ball cover (bottom) of point cloud of \emph{Table}.}
	\vskip -0.15in
	\label{fig:balls2}
\end{wrapfigure}

Motivated by the aforementioned observations, we propose a self-supervised pre-training method to improve  downstream learning tasks on point clouds in the \emph{few-shot learning} (FSL) scenario.
Given very limited training examples in the FSL setting, the goal of our self-supervised learning method is to boost sample complexity by designing supervised learning tasks using surrogate class labels extracted from smaller subsets of our point cloud. We begin by hierarchically organizing our point cloud data into \emph{balls of varying radii} of a cover-tree $\mT$~\cite{beygelzimer2006cover}. More precisely, the radius of the balls covering the point cloud reduces as the depth of the tree is traversed.
The cover-tree $\mT$ allows us to capture a \emph{non-parametric representation} of the point cloud at multiple scales and helps decompose complex patterns in the point cloud to smaller and simpler ones. 
The cover-tree method has the added benefit of being able to deal more adaptively and effectively to sparse point clouds with highly variable point densities.
We propose two novel self-supervised tasks based on the point cloud decomposition imposed by $\mT$.
Namely, we generate surrogate class labels based on the real-valued distance between each generated pair of balls on the same level of $\mT$, to form a regression task. Similarly, for parent and child ball pairs that 
span consecutive levels of $\mT$, we generate an integer label that is based on the quadrant of the parent ball in which the child ball's center lies. This, then allows us to design a classification task to predict the quadrant label. The rationale behind such a setup is to learn global \emph{inter-ball} spatial relations between balls at the same level of $\mT$ via the regression task and also learn local \emph{intra-ball} spatial relations between parent and child balls, spanning consecutive levels, via the classification task. 
Our learning setup has the added advantage that these pretext tasks are learned at multiple levels of $\mT$, thus learning point cloud representations at \emph{multiple scales} and \emph{levels of detail}. 
We argue that the combination of hierarchically decomposing the point clouds into balls of varying radii and the subsequent encoding of this hierarchy in our self-supervised learning tasks allows the pre-training to learn meaningful and robust representations for a wide variety of downstream tasks, especially in the restricted FSL setting.




\textbf{Our contributions:} 
To the best of our knowledge, we are the first to introduce a \emph{self-supervised} pre-training method for learning tasks on point clouds in the FSL setup.
(i) In an attempt to break away from existing raw point cloud or their voxelized representations, we propose the use of a metric space indexing data structure called a cover-tree, to represent a point cloud in a hierarchical, multi-scale, and non-parametric fashion (as shown in Figure~\ref{fig:balls2}).
(ii) We generate surrogate class labels, that encode the hierarchical decomposition imposed by our cover-tree, from point cloud subsets in order to pose two novel self-supervised pretext tasks.
(iii) We propose a novel neural network architecture for our self-supervised learning task that independently branches out for each learning task and whose combined losses are back propagated to our feature extractor during training to generate improved point embeddings for downstream tasks.  
(iv) Finally, we conduct extensive experiments to evaluate our self-supervised few-shot learning model on classification and segmentation tasks on both dense and sparse real-world datasets. 
Our empirical study reveals that downstream task's networks pre-trained with our self-supervised network's point embeddings significantly outperform state-of-the-art methods (both supervised and unsupervised) in the FSL setup.


   










\if 0
\section{Related Work}
\label{related}
 Most of the earlier networks in point cloud processing transform point cloud to a voxel grid before performing any task~\cite{wu2016learning}, consider several views of a shape~\cite{su2015multi} or use 2D images for point cloud representation learning~\cite{insafutdinov2018unsupervised}. On the contrary, several other variants focus on processing raw point clouds in euclidean space directly by maintaining permutation invariance and learn its local and global shape properties~\cite{dgcnn}~\cite{qi2017pointnet}~\cite{qi2017pointnet++}~\cite{li2018pointcnn}. Recent popular approaches includes supervised, unsupervised and self-supervised methods. DGCNN~\cite{dgcnn} is connected to two types of neural networks i.e., PointNet and graph neural networks where it captures both local and global structure using graph convolution in which graph is dynamically computed after every layer in the network. PointNet~\cite{qi2017pointnet} is a special case of DGCNN with $k$-nearest neighbors being $1$. It encodes only global shape information and lacks in capturing local information. PointNet++~\cite{qi2017pointnet++} incorporates local information. It first constructs a geometric graph on a point cloud and then apply graph coarsening operation in each layer. The aggregation operations used here is max. In PointCNN~\cite{li2018pointcnn}, element-wise product and sum operations of the convolution operator are applied on the $\chi$-transformed features. It gives comparable results with DGCNN. There are unsupervised methods for point clouds which are further subdivided into GANs~\cite{achlioptas2018learning}~\cite{han2019view} or autoencoders-based neural networks~\cite{yang2018foldingnet}. Many of these methods need either voxelization or 2D images of point clouds to work with. The other unsupervised networks which work with raw point clouds are dependent on reconstruction error to be compared using different similarity metrics such as Chamfer distance, Earth Mover's Distance, etc. These metrics are computationally inefficient and requires variability in the data for better results~\cite{achlioptas2018learning}. To avoid these similarity metrics, self-supervised learning provides an independent method for training using self-supervised labels. In this regard, VoxelSSL~\cite{sauder2019self} was introduced which builds the idea of placing randomly displaced image patches to their correct position into point clouds and reconstruct the parts of randomly rearranged point clouds. Existing methods focus on using large-scale point cloud dataset for classification and segmentation tasks and do not include wide variety of datasets. Considering current challenges, we focus on learning point cloud embedding in few-shot setting along with self-supervised learning as pre-training.
\fi

\section{Our Method}
\label{ourmethod}
Our method proposes a cover-tree based hierarchical self-supervised learning approach in which we generate variable-sized subsets (as balls of varying radii per level in the cover-tree) along with self-determined class 
labels to perform self-supervised learning to improve the sample complexity of the scarce training examples available to us in the \emph{few-shot learning} (FSL) setting on point clouds. 
We begin by introducing the preliminaries and notations required for FSL setup on point clouds used in the rest of the paper (Section~\ref{subsec:prelim}). 
We explain our preprocessing tasks by describing the cover-tree structure and how it indexes the points in a point cloud, followed by describing the generation of labels for self-supervised learning (Subsection~\ref{ssec:preproc}). Our self-supervised tasks are explained in Subsection~\ref{ssec:selfsup}. We then describe the details of our model architecture for the self-supervised learning network followed by classification and 
segmentation network (Subsection~\ref{ssec:arch}).

\subsection{Preliminaries and Notation}
\label{subsec:prelim}

Let a \emph{point cloud} be denoted by a set $P=\{ x_1, \cdots, x_n \}$, where $x_i \in \mathds{R}^d$, for $i=1,\cdots,n$. 
Typically, $d$ is set to $3$ to represent $3$D points, but $d$ can exceed $3$ to include extra information like color, surface normals, etc. Then a \emph{labeled point cloud} is represented by an ordered pair $(P,y)$, where 
$P$ is a point cloud in a collection of point clouds $\mathscr{P}$ and $y$ is an integer class label that takes values in the set $Y= \{1, \cdots, K\}$. 
\paragraph{Few-Shot Learning on Point Clouds} 
We follow a few-shot learning setting similar to Garcia et. al.~\cite{garcia2018}. 
We randomly sample $K'$ classes from $Y$, where $K' \leq K$, followed by randomly sampling $m$ labeled point clouds from each of the $K'$ classes. Thus, we have a total of $mK'$ labeled point cloud examples which forms our \emph{support set} $S$ and is used while training. We also generate a \emph{query set} $Q$ of unseen examples for testing that is disjoint from the support set $S$, by picking an example from each one of the $K'$ classes. 
This setting is referred to as $m$-shot, $K'$-way learning. 


\subsection{Our Training}
\label{subsec:training}
\subsubsection{Preprocessing}
\label{ssec:preproc}
To aid self-supervised learning\footnote{We strictly adhere to self-supervised learning on the support set $S$ that is later used for FSL.}, our method uses the cover-tree~\cite{beygelzimer2006cover} , to define a hierarchical data partitioning of the points in a point cloud. 
To begin with, the \emph{expansion constant} $\kappa$ of a dataset is defined as the smallest value such that every ball in the point cloud $P$ can be covered by $\kappa$ balls of radius $1/ \epsilon$, where $\epsilon$ is referred to as the \emph{base of the expansion constant}.
\paragraph{Properties of a cover-tree~\cite{beygelzimer2006cover}} Given a set of points in a point cloud $P$, the cover-tree $\mathcal{T}$ is a leveled tree where each level is associated with an integer label $i$, which decreases as the tree is descended starting from the root. 
Let $B[c, \epsilon^i ] = \{ p \in P \mid \lVert p - c \rVert_2 \leq \epsilon^i \}$ denote a closed $l_2$-ball centered at point $c$ of radius $\epsilon^i$ at level $i$ in the cover-tree $\mT$.
At the $i$-th level of $\mT$ (except the root), we create a non-disjoint union of closed balls of radius $\epsilon^i$ 
called a \emph{covering} that contains all the points in a point cloud $P$. Given a covering at level $i$ of $\mT$, each covering ball's \emph{center} is stored as a \emph{node} at level $i$ of $\mT$\footnote{The terms \emph{center} and \emph{node} are used interchangeably in the context of a cover-tree.}.
We denote the set of centers / nodes at level $i$ by $\mC_i$. 
\if 0
The cover-tree $\mathcal{T}$ obeys the following three invariant properties at all levels:
\begin{enumerate}
	\setlength{\parskip}{0cm}
	\setlength{\itemsep}{0cm}
 	
 	\item \textbf{(Nesting)} $\mathcal{C}_{i} \subset \mathcal{C}_{i-1}$. Once a center $c \in P$ is in a node in $\mathcal{C}_{i}$, then it also appears in all its successor nodes.
 	\item \textbf{(Covering)} For every center $c' \in \mathcal{C}_{i-1}$, there exists a center $c \in \mathcal{C}_{i}$ where $c'$ lies inside $B[c, \epsilon^i ]$, and exactly one such $c$ is a parent of $c'$.
 	\item \textbf{(Separation)} For all centers $c_{1},c_{2} \in \mathcal{C}_{i}$, $c_{1}$ lies outside 
 	$B[c_2, \epsilon^i ]$ and $c_{2}$ lies outside $B[c_1, \epsilon^i ]$.
\end{enumerate}
\fi

\begin{remark}
As we descend the tree's levels, the radius of the balls reduce and hence we start to see \emph{tighter} coverings that faithfully represent the underlying distribution of $P$'s points in a non-parametric fashion. Figure~\ref{fig:balls2} shows an example of two levels of coverings.
\end{remark}


\paragraph{Self-Supervised Label Generation}
Here, we describe the way we generate surrogate labels for two self-supervised tasks that we describe later.
Recall that $\mC_i$ and $\mC_{i-1}$ denote the set of centers / nodes at levels $i$ and $i-1$ in $\mT$, respectively.
Let $c_{i,j}$ denote the center of the $j$-th ball in $C_i$. 
Additionally, let $\#Ch(c)$ be the total number of child nodes of center $c$ and $Ch_k(c)$ be the $k$-th child node of center $c$.
With these definitions, we describe the two label generation tasks as follows.

\underline{Task 1:} We form a set of all possible pairs of the centers in $C_i$, except self-pairs. This set of pairs is given by $\mS^{(i)} = \{  (c_{i,j}, c_{i,j'}  ) \mid 1 \leq j,j' \leq |C_i|, j \neq j'  \}$. 
For each pair $(c_{i,j}, c_{i,j'}  ) \in \mS^{(i)} $, we assign a real-valued class label which is just 
$\lVert c_{i,j} -  c_{i,j'} \rVert_2$ (i.e., the $l_2$-norm distance between the two centers in the pair belonging to the same level $i$ in $\mT$). Such pairs are generated for all levels, except the root node.

\underline{Task 2:} We form a set of pairs between each parent node in $C_i$ with their respective child nodes in 
$C_{i-1}$, for all levels except the leaf nodes in $\mT$. The set of such pairs for balls in levels $i$ and $i-1$ 
is given by 
$\mS^{(i,i-1)} = \{  (c_{i,j}, Ch_k(c_{i,j})  ) \mid  1 \leq j \leq |C_i|, 1 \leq k \leq \#Ch(c_{i,j}) \}$. 
For each parent-child center pair $(p,c) \in \mS^{(i,i-1)}$, we assign an integer class label from $\{1,\cdots,4\}$ (representing \emph{quadrants} of a ball in $\mathds{R}^d$) depending on which quadrant of the ball centered on $p$ that the child center $c$ lies in.


\subsubsection{Self-Supervised Learning Tasks}
\label{ssec:selfsup}
After outlining the label generation tasks, the self-supervised learning tasks are simply posed as:
(i) Task 1: A \emph{regression task} $R$, which is trained on pairs of balls from set $\mS^{(i)}$ along with their 
real-valued distance labels to then learn and predict the $l_2$-norm distance between ball pairs from $C_i$ and 
(ii) Task 2: A \emph{classification task} $C$, which is trained on pairs of balls from set $\mS^{(i,i-1)}$ and their corresponding quadrant integer labels to then infer the quadrant labels given pairs from levels $C_i$ and $C_{i-1}$, where the pairs are restricted to valid parent-child pairs in $\mT$. 


\subsubsection{Network Architecture}
\label{ssec:arch}
\begin{figure}[tbp]
	\centering
\includegraphics[width=0.97\linewidth,height=5.2cm]{PC_SSL5}
	\caption{\textbf{Self-Supervised Deep Neural Network Architecture:} The architecture is used for pre-training using self-supervised labels for two independent tasks. \textbf{Feature Extractor:} The base network used for tasks during self-supervised learning can be used to extract point embeddings for further supervised training in a few-shot setting. Classification task $C$ and regression task $R$ are being trained in parallel using ball pairs taken from cover-tree $\mathcal{T}$ (described in Section~\ref{ourmethod}). Here, $(B_a,B_b)$ and $(B_x,B_y)$ represent ball pair vectors. Classification or segmentation network is pre-trained with this model architecture.}
	\label{fig:arch}
\end{figure}
\if 0
Our neural network architecture used for two self-supervised learning tasks and classification is shown in Fig.~\ref{fig:arch}. Depending on the evaluation task in the end, i.e., classification or segmentation, corresponding neural network can be used for further supervised training in place of classification network in the Fig.~\ref{fig:arch}. We first train our self-supervised network in a few-shot setting using both the tasks with associated ball pairs and self-supervised labels generated in section~\ref{ourmethod}. Then in next step, we provide labeled point clouds $(\mathcal{P}_{B},\mathcal{Y}_{B})$ and $(\mathcal{P}_{N},\mathcal{Y}_{N})$, for training and testing for evaluation during classification in a few-shot setup. Note that, we use few-shot learning setup during both self-supervised and classification network phase.
\fi
Our neural network architecture (Figure~\ref{fig:arch}) consists of two main components: (a) the self-supervised learning network and (b) the downstream task as either a classification or segmentation network. Final point cloud embeddings from the trained self-supervised network are used to initialize the downstream task's network in FSL setting. We now explain each component in detail.

\paragraph{Self-Supervised Network}
\if 0
Our self-supervised network visualized in Fig.~\ref{fig:arch} is composed of three key modules, i.e., a base neural network, and two separate neural networks for our self-supervised learning methods, i.e., a classification network to perform task $C$ and a regression network to perform task $R$. In base network, we first transform the input point clouds by scaling the vectors to unit norm. Then, we use three convolution layers with shared fully connected layers $(32,64,128)$. This module is also marked as feature extractor in Fig.~\ref{fig:arch} which is used during the classification network to initialize the point cloud data after self-supervised training. In the next step, point vectors are used to get the ball vectors using indexing between ball identifiers and points. Ball vector is defined as the centroid of all points present in a ball. These ball vectors are then passed on to the both self-supervised networks performing tasks $C$ and $R$. Both the networks use three separate convolution layers with shared fully connected layers $(64,128,256)$. Then, we concatenate ball vectors to get a $256+256=512$ dimensional vector which represents each ball pair, after which one fully connected layer is used to transform the global feature. For classification network, we finally predict one of the four quadrant numbers as a label for each ball pair. Since this is a classifier, we used cross entropy loss to train the network. For regression network, the last layer predicts the scalar value for each ball pair as $L_{2}$-norm distance. The loss function used here is mean square loss. We used losses from both the classification and regression networks for back-propagation during training. \fi
Our self-supervised network starts with a \emph{feature extractor} that first normalizes the input point clouds in support set $S$, followed by passing them through three MLP layers with shared fully connected layers $(32,64,128)$ to arrive at $128$-dimensional point vectors. For each ball $B$ in $\mT$ (in input space), we construct a corresponding ball in feature space, by grouping in similar fashion, the point embeddings that represent the points in $B$. Then, a feature space ball is represented by a \emph{ball vector}, which is the centroid of the point embeddings belonging to the ball. These ball vectors are then fed to two branches, one for each self-supervised task, i.e. $C$ and $R$, where both branches transform the ball vectors via three separate MLP layers with shared fully connected layers $(64,128,256)$. The ball pairs corresponding to each pretext task are represented by concatenating each ball's vector to get a $256+256=512$ dimensional vector. Finally, $C$ trains to classify the quadrant class label, while the regression task $R$ trains to predict the $l_2$-norm distance between balls in the ball pair. Note that losses from both tasks $C$ and $R$ are back-propagated to the feature extractor during training.

\paragraph{Classification/ Segmentation}
\if 0
The proposed self-supervised method is independent of any specific neural network for further supervised training for classification or segmentation task. The self-supervised network can be pre-trained with self-supervised data and label described in section~\ref{ourmethod} and then, any neural network which is capable of performing classification, segmentation or any task related to unordered set of point clouds such as PointNet, DGCNN, etc., can be used for supervised training for evaluation. To train classification network, we initialize point clouds taken from the feature extractor of the self-supervised network trained in previous step. For our experiments, we use DGCNN and PointNet as a classification and segmentation network and compared against another self-supervised learning method VoxelSSL~\cite{sauder2019self} with DGCNN and DGCNN~\cite{dgcnn} with random initialization.
\fi
At no point is the network jointly trained on both the self-supervised pre-training task and the downstream task, 
therefore any neural network (e.g., PointNet~\cite{qi2017pointnet} and DGCNN~\cite{dgcnn}) capable of performing downstream tasks like classification or segmentation on point clouds can be initialized during training with the point embeddings outputted from our fully-trained self-supervised network's feature extractor. 
\begin{figure}[tbp]
	\centering
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.95\linewidth]{heatmap/org_airplane1_heatmap2.png}
		\caption{}
		\label{fig:sub_1}
	\end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.95\linewidth]{heatmap/dgcnn_airplane1_heatmap2.png}
		\caption{}
		\label{fig:sub_2}
    \end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.95\linewidth]{heatmap/voxel_airplane1_heatmap3.png}
		\caption{}
		\label{fig:sub3}
	\end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.95\linewidth]{heatmap/our_airplane1_heatmap2.png}
		\caption{}
		\label{fig:sub4}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.95\linewidth]{heatmap/org_table26_heatmap3.png}
		\caption{}
		\label{fig:sub5}
    \end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.95\linewidth]{heatmap/dgcnn_table26_heatmap3.png}
		\caption{}
		\label{fig:sub6}
    \end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.95\linewidth]{heatmap/voxel_table26_heatmap3.png}
		\caption{}
		\label{fig:sub7}
	\end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.95\linewidth]{heatmap/our_table26_heatmap3.png}
		\caption{}
		\label{fig:sub8}
    \end{subfigure}
\if 0
    	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.95\linewidth]{heatmap/org_lamp_heatmap.png}
		\caption{}
		\label{fig:sub9}
	\end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.95\linewidth]{heatmap/dgcnn_lamp_heatmap.png}
		\caption{}
		\label{fig:sub10}
    \end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.95\linewidth]{heatmap/voxel_lamp_heatmap1.png}
		\caption{}
		\label{fig:sub11}
	\end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.95\linewidth]{heatmap/our_lamp_heatmap.png}
		\caption{}
		\label{fig:sub12}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.95\linewidth]{heatmap/org_airplane3_heatmap2.png}
		\caption{}
		\label{fig:sub13}
    \end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.95\linewidth]{heatmap/dgcnn_airplane3_heatmap2.png}
		\caption{}
		\label{fig:sub14}
    \end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.95\linewidth]{heatmap/voxel_airplane3_heatmap3.png}
		\caption{}
		\label{fig:sub15}
	\end{subfigure}\begin{subfigure}{.24\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.95\linewidth]{heatmap/our_airplane3_heatmap2.png}
		\caption{}
		\label{fig:sub16}
    \end{subfigure}
\fi
	\caption{\emph{Learned feature spaces} are visualized as the distance from the red point to the rest of the points (yellow: near, dark green: far) for \emph{Aeroplane} and \emph{Table} in (a, e) input 3D space. (b, f) feature space for DGCNN with random initialization. (c, g) feature space for DGCNN network pre-trained with VoxelSSL. And, (d, h) feature space of DGCNN pre-trained with our self-supervised model.}
	\label{fig:heatmap}
	\vspace{-1.5em}
\end{figure}

We study the structure of the point cloud feature spaces compared to their input spaces via a 
heat map visualization as shown in Figure~\ref{fig:heatmap}.
As this is a FSL setup, the learning is not as pronounced as in a setting with training on abundant point cloud examples. Nevertheless, we observe that points in the original input space located on \emph{semantically similar structures} of objects (e.g., on the wings of an aeroplane, on the legs of a table, etc.), despite initially having a large separation distance between them in the input space, eventually end up very close to one another in the final feature space produced by our method. 
In Figure~\ref{fig:sub_1}, points located on the same wing as the red point are marked yellow (close), while points on the other wing are marked green (far) in the original input space. But, in Figure~\ref{fig:sub4} points on both wings (shown in yellow), being semantically similar portions, end up closer in the final feature space of our method. A similar phenomena can be observed in Figure~\ref{fig:heatmap} (second row), when we chose an extreme red point on the left corner of the table top. In contrast, the feature spaces of \emph{DGCNN with random initialization} (Figures~\ref{fig:sub_2},~\ref{fig:sub6}) and \emph{DGCNN pre-trained with VoxelSSL} (Figures~\ref{fig:sub3},~\ref{fig:sub7}) do not exhibit such a lucid grouping of semantically similar parts. 

\section{Experiments}
\label{others}
We conduct exhaustive experiments to study the efficacy of the point embeddings generated by our self-supervised pre-training method. We achieve this by studying the effects of initializing downstream networks with our pre-trained point embeddings and measuring their performance in the FSL setting.  
For self-supervised and FSL experiments, we pick two real-world datasets (ModelNet40~\cite{wu20153d} and Sydney\footnote{\href{http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml}{Sydney Urban Objects Dataset}}) for \emph{3D shape classification} and 
 for our segmentation related experiments, we conduct \emph{part segmentation} on ShapeNet~\cite{yi2016scalable} and \emph{semantic segmentation} on \emph{Stanford Large-Scale 3D Indoor Spaces} (S3DIS)~\cite{armeni20163d}.

 ModelNet40 is a standard point cloud classification dataset used by state-of-the-art methods, consisting of $40$ common object categories containing a total of $12,311$ models with $1024$ points per model. 
 Additionally, we picked Sydney as it is widely considered a hard dataset to classify on due to its sparsity. Sydney has $374$ models from $10$ classes with $100$ points in each model. 
 ShapeNet contains $16,881$ 3D object point clouds from $16$ object categories which are annotated by $50$ part categories. S3DIS dataset contains 3D point clouds of $272$ rooms from $6$ areas in which each point is assigned to one of the $13$ semantic categories. 
 
 All our experiments follow the $m$-shot, $K'$-way setting. Here, $K'$ classes are randomly sampled from the dataset and for each class we sample $m$ random samples for support set $S$ to train the network. For query set $Q$, unseen samples are picked from each of the $K'$ classes.


\subsection{3D Object Classification}
\begin{table}[tbp]
	\caption{Classification results (accuracy \%) on ModelNet40 and Sydney for few-shot learning setup. Bold represents the best result and underlined represents the second best. }
	\label{Classification}
	\centering
	\tiny
	\setlength{\tabcolsep}{2.5pt}
	\begin{tabular}{lllllllll}
		\toprule
		Method & \multicolumn{4}{c}{ModelNet40} & \multicolumn{4}{c}{Sydney}\\
		\cmidrule(r){2-9}
		& \multicolumn{2}{c}{\textbf{5-way}} & \multicolumn{2}{c}{\textbf{10-way}}& \multicolumn{2}{c}{\textbf{5-way}}& \multicolumn{2}{c}{\textbf{10-way}}\\
		\cmidrule(r){2-9}
		& \textbf{10-shot} & \textbf{20-shot} & \textbf{10-shot} & \textbf{20-shot} & \textbf{10-shot} & \textbf{20-shot} & \textbf{10-shot} & \textbf{20-shot}\\
		\midrule
		
3D-GAN &  55.80$\pm$10.68\% &  65.80$\pm$9.90\% & 40.25$\pm$6.49\% & 48.35$\pm$5.59\% &  54.20$\pm$4.57\% & 58.80$\pm$5.75\% & 36.0$\pm$6.20\%&45.25$\pm$7.86\%\\
		Latent-GAN & 41.60$\pm$16.91\%&  46.20$\pm$19.68\% & 32.90$\pm$9.16\% & 25.45$\pm$9.90\% &  64.50$\pm$6.59\% & 79.80$\pm$3.37\% &50.45$\pm$2.97\% &62.50$\pm$5.07\% \\
		PointCapsNet & 42.30$\pm$17.37\% &  53.0$\pm$18.72\% &38.0$\pm$14.30\% & 27.15$\pm$14.86\%  &  59.44$\pm$6.34\% &  70.50$\pm$4.84\% &44.10$\pm$1.95\%&60.25$\pm$4.87\%\\
		FoldingNet & 33.40$\pm$13.11\% &  35.80$\pm$18.19\% & 18.55$\pm$6.49\% & 15.44$\pm$6.82\% & 58.90$\pm$5.59\% &  71.20$\pm$5.96\% &42.60$\pm$3.41\%&63.45$\pm$3.90\%\\
		PointNet++ & 38.53$\pm$15.98\% & 42.39$\pm$14.18\%&  23.05$\pm$6.97\% & 18.80$\pm$5.41\% & \underline{79.89$\pm$6.76\%} & \underline{84.99 $\pm$5.25\%}  & 55.35$\pm$2.23\% & 63.35$\pm$2.83\% \\
		PointCNN & \textbf{65.41$\pm$8.92\%} &  \underline{68.64$\pm$7.0\%} & 46.60$\pm$4.84\% &49.95$\pm$7.22\% & 75.83$\pm$7.69\% & 83.43$\pm$4.37\%&\underline{56.27$\pm$2.44\%}&73.05$\pm$4.10\% \\
		PointNet & 51.97$\pm$12.17\% & 57.81$\pm$15.45\% & 46.60$\pm$13.54\% & 35.20$\pm$15.25\% & 74.16$\pm$7.27\% & 82.18$\pm$5.06\% & 51.35$\pm$1.28\% & 58.30$\pm$2.64\% \\
		DGCNN & 31.60$\pm$8.97\%& 40.80$\pm$14.60\%& 19.85$\pm$6.45\%& 16.85$\pm$4.83\%& 58.30$\pm$6.58\%& 76.70$\pm$7.47\%& 48.05$\pm$8.20\%&\underline{76.10$\pm$3.57\%}\\
		Voxel+DGCNN & 34.30$\pm$4.10\% & 42.20$\pm$11.04\%& 26.05$\pm$7.46\%& 29.90$\pm$8.24\%& 52.50$\pm$6.62\%& 79.60$\pm$5.98\%&52.65$\pm$3.34\%&69.10$\pm$2.60\%\\
		\midrule
		
		Our+PointNet & \underline{63.2$\pm$10.72\%} & \textbf{68.90$\pm$9.41\%} &\textbf{49.15$\pm$6.09\%} & \underline{50.10$\pm$5.0\%} &76.50$\pm$6.31\% &83.70$\pm$3.97\% & 55.45$\pm$2.27\% & 64.0$\pm$2.36\% \\
		Our+DGCNN & 60.0$\pm$8.87\% &  65.70$\pm$8.37\% &  \underline{48.50$\pm$5.63\%}& \textbf{53.0$\pm$4.08\%} &\textbf{86.20$\pm$4.44\%} &\textbf{90.90$\pm$2.54\%} & \textbf{66.15$\pm$2.83\%} & \textbf{81.50$\pm$2.25\%}\\
		\bottomrule
	\end{tabular}
\end{table}
We show classification results with $K' \in \{5,10\}$, and $m \in \{10,20\}$ few-shot settings for support set $S$ during training on both the datasets. For testing, we pick $20$ unseen samples from each of the $K'$ classes as a query set $Q$. 
Table~\ref{Classification} shows the FSL classification results on ModelNet40 and Sydney datasets for: i) unsupervised methods, ii) supervised methods, and iii) supervised methods pre-trained with self-supervised learning methods. 
All methods were trained and tested in the FSL setting.
For the unsupervised methods (3DGAN, Latent-GAN, PointCapsNet, and FoldingNet), we train their network and assess the quality of their final embeddings on a classification task using linear SVM as a classifier. The linear SVM classification results are outlined in the first four rows of Table~\ref{Classification}. Supervised methods (PointNet++, PointCNN, PointNet, and DGCNN) are trained with random weight initializations on the raw point clouds. Voxel+DGCNN indicates the DGCNN classifier initialized with weights from VoxelSSL~\cite{sauder2019self} pre-training, while Our+PointNet and Our+DGCNN are the PointNet and DGCNN classifiers, initialized with our self-supervised method's base network point embeddings, respectively.  

From the results, we observe that our method outperforms all the other methods in almost all the few-shot settings on both \emph{dense} (ModelNet40 with $1024$ points) and \emph{sparse} (Sydney with $100$ points) datasets. Note that existing point cloud classification methods train the network on ShapeNet, which is a large scale dataset with $2048$ points in each point cloud and $16,881$ shapes in total, before testing their model on ModelNet40. We do not use ShapeNet for our classification results, rather we train on the limited support set $S$ and test on the query set $Q$,
corresponding to each dataset. 


\paragraph{Choice of Base of the Expansion Constant ($\epsilon$)}
\begin{figure}[tbp]
\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.75\linewidth]{ModelNet40plot_new}
		\label{fig:sub1}
	\end{subfigure}\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.75\linewidth]{Sydneyplot_new1}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Comparison between 
		\emph{base of the expansion constant $\epsilon$} (on $x$-axis) vs. \emph{accuracy} ($y$-axis at left) and vs. \emph{Silhouette coefficient} ($y$-axis at right) for ModelNet40 and Sydney datasets.}
	\label{fig:density}
\end{figure}
Recall that the \emph{base of the expansion constant $\epsilon$} controls the radius $\epsilon^i$ of a ball at level $i$ in the cover-tree $\mT$. Varying $\epsilon$ allows us to study the effect of choosing tightly-packed (for smaller $\epsilon$) versus loosely-packed (for larger $\epsilon$) ball coverings. 
On one extreme, picking large balls results in coverings with massive overlaps that fail to properly generate unique surrogate labels for our self-supervised tasks, while on the other extreme, picking tiny balls results in too many balls with insufficient points in each ball to learn from.
Therefore, it is important to choose the optimal $\epsilon$ via a grid-search and cross-validation.
Figure~\ref{fig:density} studies the effect of varying $\epsilon$ on \emph{classification accuracy} and the \emph{Silhouette coefficient\footnote{The silhouette coefficient is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The higher, the better.} of our point cloud embeddings}. DGCNN is pre-trained with our self-supervised network in a FSL setup with $K'=10$ and $m \in \{10,20\}$ for support set $S$ and $20$ unseen samples from each of the $K'$ classes in the query set $Q$.
We observe that $\epsilon=2.2$ results in the best accuracy and cluster separation of point embeddings (measure via the Silhouette coefficient) for ModelNet40 and Sydney.
\if 0
 This naturally leads to the trend that ball density with higher accuracy should have higher Silhouette coefficient. We perform this experiment on both dense (ModelNet40) and sparse (Sydney) datasets. We begin this experiment by choosing different values of base $\epsilon \in \{1.5,1.7,2.0,2.2,2.5\}$ of radius in a cover-tree. Then, we pick ball pairs from their corresponding cover-trees. Classification network (DGCNN) is pre-trained with our self-supervised network in a few-shot setup with $K'=10$ and $m \in \{10,20\}$ for support set $S$ and $20$ unseen samples from each of the $K'$ classes for query set $Q$. From Fig~\ref{fig:density}, we observe that the packing of balls with $\epsilon=2.2$ leads to better results in majority of the cases. This experiment clearly shows the need of picking the right value of radius since larger radius can pack more points in a point cloud with more density and less number of balls but leads to more overlapping of balls and lacks in capturing the local structure of a point cloud. On the other hand, smaller radius will pack small portion of a point cloud with more number of balls and low density. The insufficient number of points in a ball will not be able to learn  semantically similar parts in a 3D object properly.
\fi


\begin{table}[tbp]
	\caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
	\label{part}
\tiny
	\setlength{\tabcolsep}{3.0pt}
	\begin{tabular}{llllllllllllllllll}
		\toprule
		
		&Mean&Aero&Bag&Cap&Car&Chair&Earphone&Guitar&Knife&Lamp&Laptop&Motor&Mug&Pistol&Rocket&Skate&Table\\
		
		\cmidrule(r){2-18}
		& \multicolumn{17}{c}{\textbf{10-way 10-shot}}\\
		\cmidrule(r){2-18}
PointNet &27.23 &17.53 &38.13 &\underline{33.9} &9.93 &27.60 &35.84 &11.26 &29.57 &\textbf{42.63} &27.88 &14.0 &31.2 &19.33 &20.5 &34.67 &\underline{41.78}\\ 
		PointNet++ &26.43 &16.31 &33.08 &32.05 &7.16 &30.83 &34.0 &12.22 &29.81 &39.09 &28.73 &12.66 &30.33 &17.8 &21.44 &\underline{35.76} &41.66\\
		PointCNN &25.75 &17.3 &34.52 &31.0 &4.84 &25.79 &33.85 &11.88 &32.82 &30.63 &32.85 &9.33 &\underline{38.98} &21.77 &20.69 &29.4 &36.35\\
		DGCNN &25.58 &17.7 &33.28 &31.08 &6.47 &27.81 &34.42 &11.11 &30.41 &\underline{42.31} &26.52 &13.17 &26.72 &17.74 &24.08 &26.40 &40.09\\ 
		
		VoxelSSL &25.33 &13.51 &32.42 &26.9 &8.72 &28.0 &\underline{39.13} &8.66 &29.08 &38.28 &27.38 &12.7 &30.65 &18.8 &24.02 &27.09 &39.98\\
		\midrule
		Our+PointNet &\textbf{36.81} &\underline{24.59} &\textbf{53.68} &\textbf{42.5} &\underline{14.15} &\underline{42.33} &\textbf{44.75} &\underline{26.92} &\underline{51.16} &38.4 &\underline{39.27} &\underline{15.82} &\textbf{43.66} &\underline{31.03} &\underline{31.58} &\textbf{39.56} &\textbf{49.58} \\
		Our+DGCNN  &\underline{34.68} &\textbf{31.55} &\underline{44.25} &31.61 &\textbf{15.7} &\textbf{49.67} &36.43 &\textbf{29.93} &\textbf{58.65} &25.5 &\textbf{44.43} &\textbf{17.53} &32.69 &\textbf{37.19} &\textbf{32.11} &30.49 &37.22\\ 
		
		\bottomrule
	\end{tabular}
\end{table}
\subsection{Ablation Study}
\begin{wraptable}{r}{0.6\linewidth}
	\caption{Ablation study (accuracy \%) on ModelNet40 and Sydney datasets for DGCNN with random init (without $C$/ $R$) and pre-trained with our self-supervised tasks $C$ and $R$.}
	\label{Ablation}
	\centering
	\tiny
	\setlength{\tabcolsep}{4.5pt}
	\begin{tabular}{lllll}
		\toprule
		Method & \multicolumn{2}{c}{ModelNet40}  &  \multicolumn{2}{c}{Sydney}\\
		\cmidrule(r){2-5}
		& \textbf{10-way 10-shot} & \textbf{10-way 20-shot}  & \textbf{10-way 10-shot} & \textbf{10-way 20-shot}\\
		\midrule
		
Without $C$\&$R$ & 19.85$\pm$6.45\% & 16.85$\pm$4.83\% & 48.05$\pm$8.20\%  & 76.10$\pm$3.57\% \\
		With only $C$ & 46.5$\pm$6.08\% & 52.45$\pm$5.51\% &  60.15$\pm$3.59\% & 76.80$\pm$1.91\%\\
		With only $R$ & 47.0$\pm$6.41\% & 50.70$\pm$4.99\% & 62.75$\pm$2.83\% & 80.60$\pm$2.25\%\\
		With $C+R$& 48.50$\pm$5.63\% & 53.0$\pm$4.08\% & 66.15$\pm$2.32\% & 81.50$\pm$2.59\%\\
		\bottomrule
	\end{tabular}
\end{wraptable}
Table~\ref{Ablation} shows the results of our ablation study on two proposed self-supervised pretext tasks to study the contribution of each task individually and in unison. 
For support set $S$, $K'=10$ and $m \in \{10,20\}$ are fixed. 
The results clearly indicate a marked improvement when the pretext tasks are performed together. 
Although, we observe that learning with just the regression task (With only R) experiences better performance boosts compared to just the classification task (With only C). We attribute this phenomenon to the regression task's freedom to learn features globally across all ball pairs that belong to a single level of the cover-tree $\mT$ (inter-ball learning), irregardless of the parent-child relationships, while the classification task is constrained to only local learning of child ball placements within a single parent ball (intra-ball learning), thus restricting its ability to learn more global features.

\subsection{Part Segmentation}
Our model is extended to perform part segmentation which is a fine-grained 3D recognition task. The task is to predict class labels for each point in a point cloud. We perform this task on ShapeNet which has $2048$ points in each point cloud. We set $K'=10$ and $m=10$ for $S$. We follow the same architecture of part segmentation as mentioned in ~\cite{dgcnn} for evaluation and provide point embeddings from the feature extractor of our trained self-supervised model.

We evaluate part segmentation for each object class by following a similar scheme to PointNet and DGCNN using the \emph{Intersection-over-Union} (IoU) metric. We compute IoU for each object over all parts occurring in that object and the mean IoU (mIoU) for each of the $16$ categories separately, by averaging over all the objects in that category. For each object category evaluation, we consider that shape category for query set $Q$ for testing and pick $K'$ classes from the remaining set of classes to obey the FSL setup. Our results are shown for each category in Table~\ref{part} for $10$-way, $10$-shot learning. Our method is compared against existing supervised networks i.e., PointNet, PointNet++, PointCNN, DGCNN and the latest self-supervised method VoxelSSL followed by DGCNN. We observe that DGCNN and PointNet pre-trained with our model outperform baselines in overall IoU and in majority of the categories of ShapeNet dataset in Table~\ref{part}. In Figure~\ref{fig:partseg}, we compare our results visually with DGCNN (random initialization) and DGCNN pre-trained with VoxelSSL. From Figure~\ref{fig:partseg}, we can see that segmentation shown in our method (Fig.~\ref{fig:seg3},~\ref{fig:seg6}) have far better results than VoxelSSL (Fig.~\ref{fig:seg2},~\ref{fig:seg5}) and DGCNN with random initialization (Fig.~\ref{fig:seg1},~\ref{fig:seg4}). For example, the guitar is clearly segmented into three parts in Fig.~\ref{fig:seg6} and the \emph{headband} is clearly separated from the \emph{ear pads} of the headset in Fig.~\ref{fig:seg3}. 
\begin{figure}[tbp]
	\centering
	\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[clip, trim=3cm 2cm 3cm 2cm,height=16mm,width=0.95\linewidth]{part_seg/dgcnn_earphone_seg.png}
		\caption{}
		\label{fig:seg1}
	\end{subfigure}\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[clip, trim=3cm 2cm 3cm 2cm,height=16mm,width=0.95\linewidth]{part_seg/voxel_earphone_seg.png}
		\caption{}
		\label{fig:seg2}
	\end{subfigure}\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[clip, trim=3cm 2cm 3cm 2cm,height=16mm,width=0.95\linewidth]{part_seg/our_earphone_seg.png}
		\caption{}
		\label{fig:seg3}
	\end{subfigure}\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[clip, trim=3cm 0cm 3cm 0cm,height=16mm,width=0.95\linewidth]{part_seg/dgcnn_guitar_seg.png}
		\caption{}
		\label{fig:seg4}
	\end{subfigure}\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[clip, trim=3cm 0cm 3cm 0cm,height=16mm,width=0.95\linewidth]{part_seg/voxel_guitar_seg.png}
		\caption{}
		\label{fig:seg5}
	\end{subfigure}\begin{subfigure}{.16\textwidth}
		\centering
		\includegraphics[clip, trim=3cm 0cm 3cm 0cm,height=16mm,width=0.95\linewidth]{part_seg/our_guitar_seg.png}
		\caption{}
		\label{fig:seg6}
	\end{subfigure}
	
	\caption{Part segmentation results are visualized for Shapenet dataset for \emph{Earphone} and \emph{Guitar}. For both the objects, (a, d) show DGCNN output, (b, e) represent DGCNN pre-trained with VoxelSSL and (c, f) show DGCNN pre-trained with our self-supervised method.}
	\label{fig:partseg}
\end{figure}
\begin{table}[tbp]
	\caption{Semantic Segmentation results (mIoU \% and accuracy \% on points) on S3DIS dataset in $5$-way $10$-shot setting. Bold represents the best result and underlined represents the second best.}
	\label{semantic}
	\centering
	\scriptsize
	\begin{tabular}{lllllll}
		\toprule
		&  \multicolumn{2}{c}{Random Init} & \multicolumn{2}{c}{VoxelSSL (pre-training)} & \multicolumn{2}{c}{Ours (pre-training)}\\
		\cmidrule(r){2-7}
		& \multicolumn{6}{c}{\textbf{5-way 10-shot}}\\
		\cmidrule(r){2-7}
		Test Area & mIoU \% & Acc \% & mIoU \% & Acc \% & mIoU \% & Acc \% \\
		\midrule
		Area 1 & 61.07 $\pm$ 5.51 \% & 68.24 $\pm$ 6.58  \%& 61.26 $\pm$ 3.06 \% & 57.20 $\pm$ 6.38  \%& \textbf{61.64 $\pm$ 3.11\%} & \textbf{68.71 $\pm$ 6.54 \%}\\
		Area 2 & 55.94 $\pm$ 4.48 \% & 61.52 $\pm$ 4.36  \%& \textbf{57.73 $\pm$ 3.65 \%} & 60.45 $\pm$ 2.80  \%& \underline{56.43 $\pm$ 6.20 \%}& \textbf{64.67 $\pm$ 4.07  \%}\\
		Area 3 & 62.48 $\pm$ 3.48 \% & 66.02 $\pm$ 5.68 \%& 64.45 $\pm$ 3.34 \% & 65.17 $\pm$ 5.34 \%& \textbf{64.87 $\pm$ 6.43 \%}& \textbf{67.07 $\pm$ 7.25 \%}\\
		Area 4 & 60.89 $\pm$ 9.30 \% & 66.68 $\pm$ 9.09 \%& 62.35 $\pm$ 6.17 \% & 65.87 $\pm$ 5.96  \%& \textbf{62.90 $\pm$ 7.14 \%}& \textbf{71.60 $\pm$ 3.92 \%}\\
		Area 5 & 64.27 $\pm$ 4.79 \% & 71.76 $\pm$ 5.84 \%& \textbf{68.06 $\pm$ 2.77 \%} & 73.03 $\pm$ 7.39  \%& \underline{66.36 $\pm$ 2.86 \%}& \textbf{74.28 $\pm$ 3.18 \%}\\
		Area 6 & 63.48 $\pm$ 4.88 \% & \textbf{70.27 $\pm$ 4.33 \%}& 60.65 $\pm$ 3.22 \% & 65.82 $\pm$ 4.90  \%& \textbf{63.52 $\pm$ 4.54 \%}& \underline{66.88 $\pm$ 4.85 \%}\\
		Mean & 61.36 $\pm$ 5.41 \% & 67.41 $\pm$ 5.98 \%& 62.42 $\pm$ 3.70 \% & 64.59 $\pm$ 5.46 \%& \textbf{62.63 $\pm$ 5.05 \%}&  \textbf{68.87 $\pm$ 4.97 \%}\\
		\bottomrule
	\end{tabular}
\end{table}


\subsection{Semantic Segmentation}
In addition to part segmentation, we also demonstrate the efficacy of our point embeddings, via a semantic scene segmentation task. We extend our model to evaluate on S3DIS by following the same setting as~\cite{qi2017pointnet}, i.e., each room is split into blocks of area $1m \times 1m$ containing $4096$ sampled points and each point is represented by a $9$D vector consisting of $3$D coordinates, RGB color values and normalized spatial coordinates. For FSL, we set $K'=5$ and $m=10$ over $6$ areas where the segmentation network is pre-trained on all the areas except one area (our support set $S$) at a time and we perform testing on the remaining area (our query set $Q$). We use the same metric mIoU\% from part segmentation for each area and per-point classification accuracy as an evaluation criteria. The results from each area are averaged to get the mean accuracy and mIoU. Table~\ref{semantic} shows the results for DGCNN model with random initialization, pre-trained with VoxelSSL, and our self-supervised method. From the results, we observe that pre-training using our method improves mIoU and classification accuracy in majority of the cases with a maximum margin of nearly $3\%$ mIoU (Area $6$) and $11\%$ accuracy (Area $1$) over VoxelSSL pre-training and nearly $2\%$ mIoU (Areas $3$, $4$ and $5$) and $5\%$ accuracy (Area $4$) over DGCNN (random initialization).



\section{Conclusion}
In this work, we proposed to represent point clouds as a sequence of progressively finer \emph{coverings}, which get finer as we descend the cover-tree $\mT$. 
We then generated surrogate class labels from point cloud subsets, stored in balls of $\mT$, that encoded the hierarchical structure imposed by $\mT$ on the point clouds. These generated class-labels were then used in two novel pretext supervised training tasks to learn better point embeddings. 
From an empirical standpoint, we found our pre-training method substantially improved the performance of several state-of-the-art methods for downstream tasks in a FSL setup.
An interesting future direction can involve pretext tasks that capture the geometry and spectral information in point clouds.
\clearpage
\medskip


\section*{Broader Impact}
This study deals with self-supervised learning that improves sample complexity of point clouds which in turn aids better learning in a few-shot learning (FSL) setup; allowing for learning on very limited labeled point cloud examples.

Such a concept has far reaching positive consequences in industry and academia. For example, self-driving vehicles would now be able to train faster with scarcer point cloud samples for fewer objects and still detect or localize objects in 3D space efficiently, thus avoiding accidents caused by rare unseen events.
Our method can also positively impact shape-based recognition like segmentation (semantically tagging the objects) for cities with limited number of samples for training, which can then be useful for numerous applications such as city planning, virtual tourism, and cultural heritage documentation, to name a few. Similar benefits can be imagined in the biomedical domain where training with our method might help identify and learn from rare organ disorders, when organs are represented as 3D point clouds. Our study has the potential to adversely impact people employed via crowd-sourcing sites like Amazon Mechanical Turk (MTurk), who manually review and annotate data.


\bibliographystyle{unsrt}
\bibliography{neurips_2020}
\clearpage



\appendix
\section{Additional Experimental Results}
\begin{figure}[tbp]
	\centering
	\begin{subfigure}{.85\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth,height=7.5cm]{ball/aero_1ball_a5.png}
		\caption{(A1)}
		\label{fig:ball1}
	\end{subfigure}
	\begin{subfigure}{.85\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1.5cm 0.5cm 1.5cm,width=0.9\linewidth]{ball/aero_2ball_a5.png}
		\caption{(A2)}
		\label{fig:ball2}
    \end{subfigure}
    \begin{subfigure}{.85\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 3cm 1cm 4cm,width=0.9\linewidth]{ball/aero_4ball_a502.png}
		\caption{(A3)}
		\label{fig:ball3}
	\end{subfigure}
	
	\caption{Ball coverings of point cloud of object \emph{Aeroplane} is visualized using cover-tree $\mathcal{T}$. Here, balls are taken from cover-tree to cover parts of the point cloud at different levels ($i$, $(i-1)$ and $(i-2)$) as the tree is descended for (A1, A2, A3), respectively.}
	\label{fig:balls}
\end{figure}


\paragraph{Visualization of ball covers}
The cover-tree approach of using the balls to group the points in a point cloud is visualized in Figure~\ref{fig:balls}. The visualization shows the process of considering balls shown as transparent spheres at different scales with different densities in a cover-tree. Fig~\ref{fig:ball1} represents the top level (root) of cover-tree which covers the point cloud in a single ball i.e., at level $i$. Fig~\ref{fig:ball2} and Fig~\ref{fig:ball3} shows the balls at lower level with smaller radiuses as the tree is descended. Thus, we learn local features using balls at various levels with different packing densities.



\subsection{3D Object Classification}
\if 0
\paragraph{Baselines}
Our baselines are divided into three categories: self-supervised~\cite{sauder2019self}, unsupervised~\cite{yang2018foldingnet}~\cite{achlioptas2018learning}~\cite{han2019view} and supervised~\cite{dgcnn}~\cite{qi2017pointnet}~\cite{qi2017pointnet++}~\cite{li2018pointcnn} learning methods for point cloud classification and segmentation experiments.  We implemented a basic architecture of ~\cite{sauder2019self} which is our main baseline with their default parameters mentioned in the paper due to unavailability of their code. We use available codes for all the other unsupervised and supervised state-of-the-art methods and evaluate them under few-shot learning setting. ~\cite{sauder2019self} introduces a self-supervised learning task to train a neural network to reconstruct point clouds whose parts are randomly rearranged using voxel ID as a label for each point. FoldingNet~\cite{yang2018foldingnet} is an auto-encoder based method where a graph based encoder captures local structure on top of PointNet and a folding based decoder deforms 2D grid onto 3D surface. LatentGAN~\cite{achlioptas2018learning} operates on raw point clouds which improved GAN trained in fixed latent space of autoencoders (AE) and gaussian mixture model (GMM). 3D-GAN~\cite{wu2016learning} uses voxels to represent point cloud and generates 3D objects from a probabilistic space in volumetric convolutional network and generative adversarial network. PoinCapsNet~\cite{zhao20193d} is also an autoencoder based unsupervised learning method which uses dynamic routing scheme and 2D latent space while considering sparsity of point clouds. Among supervised methods, DGCNN\cite{dgcnn} captures global as well as local structure using graph convolution being applied to $k$-nearest neighbors of each point. The graph is dynamically computed after each layer. PointNet~\cite{qi2017pointnet} is a special case of DGCNN with $k=1$ and encodes only global shape information whereas PointNet++\cite{qi2017pointnet++} tries to capture local structure by applying PointNet in a local manner. PointCNN~\cite{li2018pointcnn} introduces $\chi$-conv operator with hierarchical convolution to learn permutation invariance. 
\fi
\paragraph{Training}
This section provides the implementation details of our proposed self-supervised network. For each point cloud, we first scale it to a unit cube and then built a cover-tree with the base of the expansion constant $\epsilon=2.0$ for all the classification and segmentation experiments. To generate self-supervised labels, we consider upto $3$ levels of cover-tree and form possible ball pairs for both the self-supervised tasks $R$ and $C$. To train our self-supervised network, point clouds pass through our feature extractor which consists of three MLP layers $(32, 64, 128)$ and a shared fully connected layer. Similarly, for both the tasks, we use three MLP layers $(64, 128, 256)$ and shared fully connected layers in two separate branches. Dropout with keep probability of $0.5$ is used in fully connected layers. All the layers include batch normalization and LeakyReLu with slope $0.2$. We train our model with Adam optimizer with an initial learning rate of $0.001$ for $200$ epochs and batch size $8$. For downstream classification and segmentation tasks, we chose default parameters of DGCNN and PointNet for their training. We consider default parameters of all the baselines mentioned in their papers to train their respective networks.

\paragraph{Effect of Point Cloud Density}
We investigate the robustness of our self-supervised method using point cloud density experiment on ModelNet40 dataset with $1024$ points in original. We randomly pick input points during supervised training with $K'=5$ and $m=20$ for support set $S$ and $20$ unseen samples from each of the $K'$ classes as a query set $Q$ during testing. The results are shown in Figure~\ref{fig:sparse} in which we start with picking $128$ points and go upto $1024$ points for DGCNN, DGCNN pre-trained with VoxelNet and DGCNN and PointNet pre-trained with our self-supervised method. Figure~\ref{fig:sparse} shows that even with very less number of points i.e., $128$, $256$, $512$ etc. points, our method achieves comparable results and still outperform the DGCNN with random init and pre-trained with VoxelNet.
\begin{figure}[h]
\centering
	\includegraphics[width=.6\textwidth]{ModelNet10_sparse1}
	\caption{Results with randomly picked points in a point cloud on ModelNet40 dataset in a $5$-way $20$-shot setting.}
	\label{fig:sparse}
\end{figure}



\paragraph{T-SNE Visualization}
To verify the classification results, Fig~\ref{fig:tsne} show T-SNE visualization of point cloud embeddings in feature space of two datasets (Sydney and ModelNet40) with $10$ classes for DGCNN as classification network with random initialization, pre-trained with VoxelNet and our self-supervised network in a few-shot setup. We observe that DGCNN pre-trained with our method shows decent separation for both the datasets as compared to pre-training with VoxelNet which is a self-supervised method and our main baseline. We also show better separation than DGCNN for Sydney dataset and nearly comparable separation to DGCNN with random initialization for ModelNet40 dataset in a few-shot learning setup.

\begin{figure}
	\centering
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{tsne/voxel_sydney10_tsne_100_1last.png}
		\caption{}
		\label{fig:tsne1}
	\end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{tsne/dgcnn_sydney10_tsne_100_1last.png}
		\caption{}
		\label{fig:tsne2}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{tsne/sydney10_tsne_100_1last.png}
		\caption{}
		\label{fig:tsne3}
	\end{subfigure}

	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{tsne/voxel_ModelNet10_tsne_50_1last.png}
		\caption{}
		\label{fig:tsne4}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{tsne/dgcnn_ModelNet10_tsne_50_1last.png}
		\caption{}
		\label{fig:tsne5}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.95\linewidth]{tsne/ModelNet10_tsne_50_1last.png}
		\caption{}
		\label{fig:tsne6}
	\end{subfigure}\caption{T-SNE visualization of point cloud classification for DGCNN network pre-trained with VoxelNet ((a), (d)), with random initialization ((b), (e)) and pre-trained with our self-supervised method ((c), (f)) in a few-shot setup for Sydney (first row) and ModelNet40 (second row) datasets. }
	\label{fig:tsne}
\end{figure}

\begin{figure}[h]
	\centering
    	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.9\linewidth]{heatmap/org_airplane3_heatmap2.png}
		\caption*{(A1)}
		\label{fig:sub13}
    \end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.9\linewidth]{heatmap/dgcnn_airplane3_heatmap2.png}
		\caption*{(A2)}
		\label{fig:sub14}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.9\linewidth]{heatmap/voxel_airplane3_heatmap3.png}
		\caption*{(A3)}
		\label{fig:sub15}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 2cm,width=0.9\linewidth]{heatmap/our_airplane3_heatmap2.png}
		\caption*{(A4)}
		\label{fig:sub16}
    \end{subfigure}
    	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/org_lamp_heatmap.png}
		\caption*{(L1)}
		\label{fig:sub9}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/dgcnn_lamp_heatmap.png}
		\caption*{(L2)}
		\label{fig:sub10}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/voxel_lamp_heatmap1.png}
		\caption*{(L3)}
		\label{fig:sub11}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/our_lamp_heatmap.png}
		\caption*{(L4)}
		\label{fig:sub12}
	\end{subfigure}
\caption{\textbf{Learned feature spaces} are visualized as a distance between the red point to the rest of the points (yellow: near, dark green: far) for (A)\emph{eroplane} and (L)\emph{amp}. \textbf{A1, L1:} input $\mathbb{R}^{3}$ space. \textbf{A2, L2:} DGCNN with random initialization. \textbf{A3, L3:} DGCNN network pre-trained with VoxelNet. And, \textbf{A4, L4:} DGCNN pre-trained with our self-supervised model.}
	\label{fig:heatmap_supp1}
\end{figure}
\begin{figure}[h]
	\centering
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.9\linewidth]{heatmap/org_laptop21_heatmap.png}
		\caption*{(L1)}
		\label{fig:heat1}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.9\linewidth]{heatmap/dgcnn_laptop21_heatmap.png}
		\caption*{(L2)}
		\label{fig:heat2}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.9\linewidth]{heatmap/voxel_laptop21_heatmap1.png}
		\caption*{(L3)}
		\label{fig:heat3}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 1cm 0.5cm 2cm,width=0.9\linewidth]{heatmap/our_laptop21_heatmap.png}
		\caption*{(L4)}
		\label{fig:heat4}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/org_table_heatmap.png}
		\caption*{(T1)}
		\label{fig:heat5}
    \end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/dgcnn_table_heatmap.png}
		\caption*{(T2)}
		\label{fig:heat6}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/voxel_table_heatmap.png}
		\caption*{(T3)}
		\label{fig:heat7}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/our_table_heatmap.png}
		\caption*{(T4)}
		\label{fig:heat8}
    \end{subfigure}
	\caption{\textbf{Learned feature spaces} are visualized as a distance between the red point to the rest of the points (yellow: near, dark green: far) for (L)\emph{aptop} and (T)\emph{able}. \textbf{L1, T1:} input $\mathbb{R}^{3}$ space. \textbf{L2, T2:} DGCNN with random initialization. \textbf{L3, T3:} DGCNN network pre-trained with VoxelNet. And, \textbf{L4, T4:} DGCNN pre-trained with our self-supervised model.}
	\label{fig:heatmap_supp2}
\end{figure}    

\begin{figure}[h]
	\centering    
    	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/org_earphone_heatmap.png}
		\caption*{(E1)}
		\label{fig:heat9}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/dgcnn_earphone_heatmap.png}
		\caption*{(E2)}
		\label{fig:heat10}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/voxel_earphone_heatmap1.png}
		\caption*{(E3)}
		\label{fig:heat11}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=0.5cm 0cm 0.5cm 0cm,width=0.9\linewidth]{heatmap/our_earphone_heatmap.png}
		\caption*{(E4)}
		\label{fig:heat12}
	\end{subfigure}
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/org_guitar_heatmap.png}
		\caption*{(G1)}
		\label{fig:heat13}
    \end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/dgcnn_guitar_heatmap.png}
		\caption*{(G2)}
		\label{fig:heat14}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/voxel_guitar_heatmap.png}
		\caption*{(G3)}
		\label{fig:heat15}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 0cm 1cm 0cm,width=0.9\linewidth]{heatmap/our_guitar_heatmap.png}
		\caption*{(G4)}
		\label{fig:heat16}
    \end{subfigure}
\caption{\textbf{Learned feature spaces} are visualized as a distance between the red point to the rest of the points (yellow: near, dark green: far) for (E)\emph{arphone} and (G)\emph{uitar}. \textbf{E1, G1:} input $\mathbb{R}^{3}$ space. \textbf{E2, G2:} DGCNN with random initialization. \textbf{E3, G3:} DGCNN network pre-trained with VoxelNet. And, \textbf{E4, G4:} DGCNN pre-trained with our self-supervised model.}
	\label{fig:heatmap_supp3}
\end{figure} 
\begin{figure}[h]
	\centering    
    	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/org_motor_heatmap.png}
		\caption*{(M1)}
		\label{fig:heat17}
    \end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/dgcnn_motor_heatmap.png}
		\caption*{(M2)}
		\label{fig:heat18}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/voxel_motor_heatmap.png}
		\caption*{(M3)}
		\label{fig:heat19}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/our_motor_heatmap.png}
		\caption*{(M4)}
		\label{fig:heat20}
    \end{subfigure}
	    	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/org_motorbike22_heatmap.png}
		\caption*{(M5)}
		\label{fig:sub21}
    \end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/dgcnn_motorbike22_heatmap.png}
		\caption*{(M6)}
		\label{fig:sub22}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/voxel_motorbike22_heatmap1.png}
		\caption*{(M7)}
		\label{fig:sub23}
	\end{subfigure}\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[clip, trim=1cm 2cm 1cm 2cm,width=0.9\linewidth]{heatmap/our_motorbike22_heatmap.png}
		\caption*{(M8)}
		\label{fig:sub24}
    \end{subfigure}
\caption{\textbf{Learned feature spaces} are visualized as a distance between the red point to the rest of the points (yellow: near, dark green: far) for (M)\emph{otorbikes}. \textbf{M1, M5:} input $\mathbb{R}^{3}$ space. \textbf{M2, M6:} DGCNN with random initialization. \textbf{M3, M7:} DGCNN network pre-trained with VoxelNet. And, \textbf{M4, M8:} DGCNN pre-trained with our self-supervised model.}
	\label{fig:heatmap_supp4}
\end{figure}
\paragraph{Heatmap Visualization}
We visualize the distance in original and final feature space as a heatmap in Fig.~\ref{fig:heatmap_supp1}, ~\ref{fig:heatmap_supp2}, ~\ref{fig:heatmap_supp3} and ~\ref{fig:heatmap_supp4}. It shows the distance between red point to all the other points (from yellow to dark green) in original 3D space in first row first column and final feature space for DGCNN with random init (first row second column), pre-trained with VoxelNet (second row first column) and our method (second row second column). Since this is a few-shot setup, the learning is not as good as it happens in a setting with all the point clouds but we observe that the parts of aeroplane in first and second rows of Fig.~\ref{fig:heatmap_supp1} such as both the wings are at the same distance and main body is far away from wings for our method whereas it differs for other methods. Similarly, in third and fourth row of Fig.~\ref{fig:heatmap_supp2} in table, the red point is on one of the legs of the table and all the other legs of the table are close to the red point in feature space (yellow) whereas table top is far away from the red point in feature space (dark green) which is not the case with DGCNN with random initialization and DGCNN pre-trained with VoxelNet.


\subsection{Part Segmentation}
We extend our model to perform part segmentation on ShapeNet dataset with $2048$ points in each point cloud. We evaluate part segmentation with $K'=\{5,10\}$ and $m=\{5,10,20\}$ for support set $S$ and pick $20$ samples for each of the $K'$ classes for query set $Q$. Table~\ref{part1}, ~\ref{part2}, ~\ref{part3}, ~\ref{part4}, ~\ref{part5} and ~\ref{part6} show mean IoU (mIoU) for each of the 16 categories separately and their mean over all the categories. From Table~\ref{part1}, ~\ref{part2}, ~\ref{part3}, ~\ref{part4}, we observe that DGCNN and PointNet pre-trained with our model outperform baselines in overall IoU and in majority of the categories while Table~\ref{part5} and ~\ref{part6} shows either the best or the second best for our method in most of the cases. Our results for $10$-way $10$-shot setup is shown in main paper. Along with mIoU results, we also visualize part segmentation results for DGCNN with random init, pre-trained with VoxelNet and our method in Figure~\ref{fig:partseg1} and ~\ref{fig:partseg2} for various object categories. We can see from the figures that segmentation shown in our method have far better results than VoxelNet and DGCNN with random initialization. However, in some cases we observe comparable results for both DGCNN with random init and pre-trained with our method. For example, the objects like Laptop and Bag have almost similar segmented parts for both DGCNN with random init and pre-trained with our method. On the other hand, our method produces properly segmented parts for more complex objects such as Car, Guitar, etc., as compared to the DGCNN with random initialization.
\begin{table}[h]
  \caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
  \label{part1}
  \centering
  \normalsize
  \setlength{\tabcolsep}{4.5pt}
  \begin{tabular}{llllllllll}
    \toprule
    
    &Mean&Aero&Bag&Cap&Car&Chair&Earphone&Guitar&Knife\\

    \cmidrule(r){2-10}
    & \multicolumn{9}{c}{\textbf{5-way 5-shot}}\\
    \cmidrule(r){2-10}
    PointNet & 26.35 & 18.31 & 28.80 & 30.12 & 6.94 &28.60 &\underline{36.06} &11.27 &30.31 \\ 
    PointNet++ &23.57 &15.01 &32.49 &31.18 &5.83 &24.68 &34.51 &11.53 &27.26 \\
    PointCNN &24.5 &17.45 &27.93 &31.42 &6.25 &25.42 &29.86 &7.84 &32.65 \\
    DGCNN & 30.48 & \underline{21.36}&\underline{38.58}&\underline{34.80}&12.25&40.58&32.5&13.45&43.75\\
    VoxelNet & 22.51 &17.01 &30.41 &31.38 &8.55 &21.78 &27.49 &8.88 &27.01 \\
    \midrule
    Our+PointNet &\textbf{33.67} &20.02&\textbf{40.18} &\textbf{39.97}&\underline{12.64} &\underline{41.16} &\textbf{37.82} &\textbf{21.86} &\textbf{59.29} \\
    Our+DGCNN & \underline{30.78} & \textbf{26.82}&34.09&33.53&\textbf{16.05}&\textbf{48.46}&30.9&\underline{19.39}&\underline{47.67}\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
  \label{part2}
  \centering
  \normalsize
  \setlength{\tabcolsep}{4.5pt}
  \begin{tabular}{llllllllll}
    \toprule
    
    &Mean &Lamp&Laptop&Motor&Mug&Pistol&Rocket&Skate&Table\\

    \cmidrule(r){2-10}
    & \multicolumn{9}{c}{\textbf{5-way 5-shot}}\\
    \cmidrule(r){2-10}
    PointNet & 26.35 & \textbf{38.32} &28.70 &11.25 &27.33 &17.65 &\textbf{39.04} &29.85 &39.04\\ 
    PointNet++ &23.57 &28.97 &27.3 &9.76 &27.06 &18.68 &24.16 &26.68 &32.07\\
    PointCNN &24.5 &29.88 &32.13 &14.2 &29.06 &20.14 &16.01 &\underline{30.11} &\underline{41.66}\\
    DGCNN & 30.48 & 28.3&\textbf{56.62}&10.87&30.82&\underline{31.58}&25.96&29.36&36.83\\
    VoxelNet & 22.51 &27.42 &25.0 &11.33 &7.76 &17.83 &18.71 &24.22 &34.95\\
    \midrule
    Our+PointNet &\textbf{33.67} &\underline{33.79} &\underline{45.73} &\underline{14.55} &\textbf{35.92} &29.14 &\underline{27.85} &\textbf{33.82} &\textbf{45.05} \\
    Our+DGCNN & \underline{30.78} & 29.25&34.04&\textbf{16.48}&\underline{34.95}&\textbf{34.94}&26.63&28.84&30.43\\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[h]
  \caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
  \label{part3}
  \centering
  \normalsize
  \setlength{\tabcolsep}{4.5pt}
  \begin{tabular}{llllllllll}
    \toprule
    
    &Mean&Aero&Bag&Cap&Car&Chair&Earphone&Guitar&Knife\\

    \cmidrule(r){2-10}
    

    & \multicolumn{9}{c}{\textbf{5-way 10-shot}}\\
    \cmidrule(r){2-10}
    PointNet &25.9 &15.02 &34.37 &32.96 &8.40 &27.76 &34.15 &9.32 &28.2 \\
    PointNet++ &27.48 &\underline{31.49} &32.16 &27.41 &9.06 &\underline{45.42} &21.8 &11.15 &29.33 \\
    PointCNN &24.39 &18.48 &33.46 &29.21 &4.27 &22.51 &31.3 &10.73 &27.69 \\
    DGCNN &24.5 &17.95 &30.57 &27.03 &7.56 &28.35 &\underline{34.8} &9.53 &27.65\\ 
    
    VoxelNet &24.9 &17.42 &30.2 &32.85 &6.24 &30.77 &\textbf{34.87} &7.62 &25.97 \\
    \midrule
    Our+PointNet &\textbf{34.67} &23.55 &\textbf{47.84} &\textbf{49.28} &\underline{12.85} &38.99 &32.26 &\textbf{31.16} &\textbf{51.69} \\
    Our+DGCNN  &\underline{32.44} &\textbf{35.75} &\underline{37.69} &\underline{34.94} &\textbf{16.94} &\textbf{46.74} &33.88 &\underline{15.75} &\underline{49.95} \\ 
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[h]
  \caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
  \label{part4}
  \centering
  \normalsize
  \setlength{\tabcolsep}{4.5pt}
  \begin{tabular}{llllllllll}
    \toprule
    &Mean&Lamp&Laptop&Motor&Mug&Pistol&Rocket&Skate&Table\\
    \cmidrule(r){2-10}
    & \multicolumn{9}{c}{\textbf{5-way 10-shot}}\\
    \cmidrule(r){2-10}
    PointNet &25.9 &37.48 &27.43 &14.76 &33.67 &20.83 &18.56 &\underline{33.89} &37.54\\
    PointNet++ &27.48 &\underline{40.27} &29.01 &\underline{19.26} &\textbf{42.29} &18.02 &\underline{27.55} &27.05 &28.41\\
    PointCNN &24.39 &30.16 &\underline{31.25} &10.43 &37.87 &24.11 &16.52 &30.29 &31.99\\
    DGCNN &24.5 &39.1 &23.04 &12.49 &32.67 &15.94 &21.34 &25.32 &38.74\\ 
    
    VoxelNet &24.9 &\textbf{40.82} &23.31 &13.01 &24.87 &19.42 &24.02 &27.09 &39.98\\
    \midrule
    Our+PointNet &\textbf{34.67} &32.91 &29.94 &16.82 &\underline{42.2} &\textbf{34.84} &\textbf{28.14} &30.42 &\textbf{51.9} \\
    Our+DGCNN  &\underline{32.44} &30.91 &\textbf{36.45} &\textbf{20.37} &31.5 &\underline{25.71} &23.53 &\textbf{37.85} &\underline{41.05}\\ 
    
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
  \label{part5}
  \centering
  \normalsize
  \setlength{\tabcolsep}{4.5pt}
  \begin{tabular}{llllllllll}
    \toprule
    
    &Mean&Aero&Bag&Cap&Car&Chair&Earphone&Guitar&Knife\\

    \cmidrule(r){2-10}
    
    & \multicolumn{9}{c}{\textbf{10-way 20-shot}}\\
    \cmidrule(r){2-10}
    PointNet &27.4 &18.07 &37.94 &32.85 &8.72 &29.85 &34.02 &10.6 &29.18\\ 
    PointNet++ &\textbf{39.15} &31.95 &39.63 &\textbf{53.84} &\textbf{22.02} &\underline{46.89} &33.66 &\underline{26.9} &53.78\\
    PointCNN &27.26 &18.48 &37.33 &33.98 &4.45 &24.3 &33.54 &10.22 &32.99\\
    DGCNN & \underline{37.34}&\underline{37.13} &42.05 &\underline{50.45} &\underline{17.2} &\textbf{50.2} &\textbf{45.3} &\textbf{28.0} &\underline{59.77} \\ 
    VoxelNet &26.29 &17.4 &31.36 &30.2 &8.21 &29.53 &38.84 &8.52 &27.04\\
    \midrule
    Our+PointNet &36.85 &22.77 &\textbf{52.41} &44.77 &16.86 &41.1 &40.35 &21.55 &53.61 \\
    Our+DGCNN &36.91 &\textbf{37.72}&\underline{47.34}&39.37&11.36&44.34&\underline{40.88}&25.77&\textbf{61.71}\\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[h]
  \caption{Part Segmentation results (mIoU \% on points) on ShapeNet dataset. Bold represents the best result and underlined represents the second best.}
  \label{part6}
  \centering
  \normalsize
  \setlength{\tabcolsep}{4.5pt}
  \begin{tabular}{llllllllll}
    \toprule
    &Mean&Lamp&Laptop&Motor&Mug&Pistol&Rocket&Skate&Table\\
    \cmidrule(r){2-10}
    & \multicolumn{9}{c}{\textbf{10-way 20-shot}}\\
    \cmidrule(r){2-10}
    PointNet &27.4 &\underline{43.23} &28.56 &14.8 &34.02 &17.6 &19.29 &35.79 &43.87\\ 
    PointNet++ &\textbf{39.15} &29.18 &48.08 &\underline{19.29} &\textbf{42.99} &\textbf{52.39} &\textbf{43.8} &\textbf{40.39} &41.68\\
    PointCNN &27.26 &35.18 &31.83 &12.14 &\underline{42.54}&22.8 &21.9 &29.1&45.25\\
    DGCNN & \underline{37.34}&35.57 &25.22 &\textbf{20.69} &40.4 &37.21 &26.93 &35.7 &45.54\\ 
    VoxelNet &26.29 &\textbf{44.92} &26.79 &11.56 &26.91 &17.39 &21.13 &35.9 &44.95\\
    \midrule
    Our+PointNet &36.85 &38.15 &\underline{54.01} &19.16 &34.46 &34.67 &27.54 &\underline{39.14} &\textbf{49.12} \\
    Our+DGCNN &36.91 &36.94&\textbf{56.24}&15.82 &34.48 &\underline{41.94} &\underline{29.44} &29.50 &\underline{46.20} \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{figure}
	\centering
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_table_seg.png}
		\caption*{(T1)}
		\label{fig:seg7}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_table_seg.png}
		\caption*{(T2)}
		\label{fig:seg8}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_table_seg.png}
		\caption*{(T3)}
		\label{fig:seg9}
	\end{subfigure}
		\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_car1_seg.png}
		\caption*{(C1)}
		\label{fig:seg10}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_car1_seg.png}
		\caption*{(C2)}
		\label{fig:seg11}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_car1_seg.png}
		\caption*{(C3)}
		\label{fig:seg12}
	\end{subfigure}
			\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_car_seg.png}
		\caption*{(C4)}
		\label{fig:seg13}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_car_seg.png}
		\caption*{(C5)}
		\label{fig:seg14}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_car_seg.png}
		\caption*{(C6)}
		\label{fig:seg15}
	\end{subfigure}
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_table1_seg.png}
		\caption*{(T4)}
		\label{fig:seg16}
	\end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_table1_seg.png}
		\caption*{(T5)}
		\label{fig:seg17}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_table1_seg.png}
		\caption*{(T6)}
		\label{fig:seg18}
	\end{subfigure}
	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_bag_seg.png}
		\caption*{(B1)}
		\label{fig:seg19}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_bag_seg.png}
		\caption*{(B2)}
		\label{fig:seg20}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_bag_seg.png}
		\caption*{(B3)}
		\label{fig:seg21}
	\end{subfigure}\caption{Part segmentation results are visualized for ShapeNet dataset for (T)\emph{ables}, (C)\emph{ars} and (B)\emph{ag} . For each row, first column (T1, C1, C4, T4, B1) shows DGCNN output, second column (T2, C2, C5, T5, B2) represents DGCNN pre-trained with VoxelNet and third column (T3, C3, C6, T6, B3) shows DGCNN pre-trained with our self-supervised method.}
	\label{fig:partseg2}
\end{figure}
\begin{figure}
    \centering

	\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_guitar1_seg.png}
		\caption*{(G1)}
		\label{fig:seg22}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_guitar1_seg.png}
		\caption*{(G2)}
		\label{fig:seg23}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_guitar1_seg.png}
		\caption*{(G3)}
		\label{fig:seg24}
	\end{subfigure}
		\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_pistol_seg.png}
		\caption*{(P1)}
		\label{fig:seg25}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_pistol_seg.png}
		\caption*{(P2)}
		\label{fig:seg26}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_pistol_seg.png}
		\caption*{(P3)}
		\label{fig:seg27}
	\end{subfigure}
			\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/dgcnn_laptop_seg.png}
		\caption*{(L1)}
		\label{fig:seg28}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/voxel_laptop_seg.png}
		\caption*{(L2)}
		\label{fig:seg29}
    \end{subfigure}\begin{subfigure}{.32\textwidth}
		\centering
		\includegraphics[width=0.92\linewidth]{part_seg/our_laptop_seg.png}
		\caption*{(L3)}
		\label{fig:seg30}
	\end{subfigure}\caption{Part segmentation results are visualized for ShapeNet dataset for (G)\emph{uitar}, (P)\emph{istol} and (L)\emph{aptop}. For each row, first column (G1, P1, L1) shows DGCNN output, second column (G2, P2, L2) represents DGCNN pre-trained with VoxelNet and third column (G3, P3, L3) shows DGCNN pre-trained with our self-supervised method.}
	\label{fig:partseg1}
\end{figure}
\end{document}
