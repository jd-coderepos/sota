

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\alglinelabel}{\addtocounter{ALC@line}{-1}\refstepcounter{ALC@line}\label }



\usepackage[accepted]{icml2023}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{xcolor, color, colortbl}         

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{pifont}\usepackage{enumitem}


\usepackage{subcaption}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{wrapfig}

\usepackage{threeparttable}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{arydshln}
\usepackage{stfloats}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{minitoc}
\input{insbox}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}


\newenvironment{roster}
{\begin{enumerate}[font=\upshape,label=(\alph*)]}
	{\end{enumerate}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\definecolor{Gray}{gray}{0.9}
\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\cc}[1]{\cellcolor{gray!#1}}
\newcolumntype{g}{>{\columncolor{Gray}}c}
\newcolumntype{f}{>{\columncolor{Gray}}l}
\makeatletter
\def\smallunderbrace#1{\mathop{\vtop{\m@th\ialign{##\crcr
				\crcr
				\noalign{\kern3\p@\nointerlineskip}\tiny\upbracefill\crcr\noalign{\kern3\p@}}}}\limits}
\makeatother
\makeatletter
\allowdisplaybreaks
\newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\def\smalloverbrace#1{\mathop{\vtop{\m@th\ialign{##\crcr
				\crcr
				\noalign{\kern3\p@\nointerlineskip}\tiny\upbracefill\crcr\noalign{\kern3\p@}}}}\limits}
\def\adl@drawiv#1#2#3{\hskip.5\tabcolsep
	\xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}#2\z@ plus1fil minus1fil\relax
	\hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{\noalign{\vskip\aboverulesep
		\global\let\@dashdrawstore\adl@draw
		\global\let\adl@draw\adl@drawiv}
	\cdashline{#1}
	\noalign{\global\let\adl@draw\@dashdrawstore
		\vskip\belowrulesep}}				
\newcommand{\wrapfill}{\par\ifnum\value{WF@wrappedlines}>0
	\addtocounter{WF@wrappedlines}{-1}\null\vspace{\arabic{WF@wrappedlines}\baselineskip}\WFclear
	\fi}
\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models}

\begin{document}

\twocolumn[
\icmltitle{Refining Generative Process with Discriminator Guidance\\in Score-based Diffusion Models}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dongjun Kim}{equal,kaist}
\icmlauthor{Yeongmin Kim}{equal,kaist}
\icmlauthor{Se Jung Kwon}{comp}
\icmlauthor{Wanmo Kang}{kaist}
\icmlauthor{Il-Chul Moon}{kaist,comp2}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{KAIST, South Korea}
\icmlaffiliation{comp}{NAVER Cloud}
\icmlaffiliation{comp2}{Summary.AI}


\icmlcorrespondingauthor{Dongjun Kim}{dongjoun57@kaist.ac.kr}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{figure*}[hb]
\centering
	\includegraphics[width=0.95\linewidth]{comparison_thumbnail_v5.pdf}
	\caption{Comparison of the denoising processes. Discriminator Guidance adjusts the score function by estimating the gap  between the predicted model score and the true data score. As a result, the sample generated using Discriminator Guidance is indistinguishable from real data according to the discriminator.}
	\label{fig:thumbnail}
\end{figure*}

\begin{abstract}
	The proposed method, \textbf{Discriminator Guidance}, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at \url{https://github.com/alsdudrla10/DG}.
\end{abstract}

\section{Introduction}

The diffusion model has recently been highlighted for its success in image generation \cite{dhariwal2021diffusion, ho2022cascaded, karras2022elucidating, song2020score}, video generation \cite{singer2022make, ho2022video, voleti2022mcvd}, and text-to-image generation \cite{rombach2022high, ramesh2022hierarchical, saharia2022photorealistic}. The State-Of-The-Art (SOTA) models perform human-level generation, but there is still much more room to be investigated for a deep understanding on diffusion models.	

\begin{figure*}[t]
	\centering
	\begin{subfigure}{0.29\linewidth}
		\centering
		\includegraphics[width=\linewidth]{ADM_samples_v2.pdf}
		\subcaption{ADM \cite{dhariwal2021diffusion}}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.29\linewidth}
		\centering
		\includegraphics[width=\linewidth]{ADM-G_samples_v2.pdf}
		\subcaption{ADM-G \cite{dhariwal2021diffusion}}
	\end{subfigure}		
	\hfil
	\begin{subfigure}{0.29\linewidth}
		\centering
		\includegraphics[width=\linewidth]{ADM-G++_samples_v3.pdf}
		\subcaption{ADM-G++ (Ours)}
	\end{subfigure}
	\caption{Samples from ImageNet 256x256 on (a) ADM, (b) ADM with Classifier Guidance. Classifier Guidance generates high-fidelity but mode degenerated samples. (c) Classifier Guidance combined with Discriminator Guidance improves both sample quality and intra-class diversity. See Appendix \ref{sec:uncurated} for uncurated samples of SOTA models.}
	\label{fig:ImageNet256}
	\vskip -0.1in
\end{figure*}

The generative model community widely uses well-trained score models \cite{dhariwal2021diffusion, rombach2022high} in downstream tasks \cite{meng2021sdedit, kawar2022denoising, su2022dual, kim2022unsupervised}. This is partially because training a new score model from scratch can be computationally expensive. However, as the demand for reusing pre-trained models increases, there are only a few research  efforts that focus on improving sample quality with a pre-trained score model.

To avoid issues such as overfitting \cite{nichol2021improved} or memorization \cite{carlini2023extracting} that may arise from further score training (Figure \ref{fig:further_score_training}), our approach keeps the pre-trained score model fixed and introduces a new component that provides a supervision during sample generation. Specifically, we propose using a discriminator as an auxiliary degree of freedom to the pre-trained model. This discriminator classifies real and generated data at all noise scales, providing direct feedback to the sample denoising process, indicating whether the sample path is realistic or not. We achieve this by adding a correction term to the model score, constructed by the discriminator, which steers the sample path towards more realistic regions (Figure \ref{fig:thumbnail}). This term is designed to adjust the model score to match the data score at the optimal discriminator (Theorem \ref{thm:1}), allowing our approach to find a realistic sample path by adjusting the model score. In experiments, we achieve new SOTA performances on image datasets such as CIFAR-10, CelebA/FFHQ 64x64, and ImageNet 256x256. As discriminator training is a minimization problem that is stable and fast to converge (Figure \ref{fig:discriminator_training}), such a significant gain can be achieved with a cheap budget (Table \ref{tab:budget}). We summarize the contributions as follows.


\vspace{-2mm}
\begin{itemize}\setlength\itemsep{0.2em}
	\item[\checkmark] We propose a new generative process, \textbf{Discriminator Guidance}, with an adjusted score of a given \textit{pre-trained} score model.
	\item[\checkmark] We show that the discriminator-guided samples are \textit{closer} to the real-world data than the non-guided samples, theoretically and empirically.
\end{itemize}
\vspace{-2mm}

\section{Preliminary}

Suppose  be the data distribution and  be the model distribution. Likelihood-based latent variable models optimize their parameters by minimizing the upper bound of the KL divergence , given by

where  are  latent variables;  is an inference distribution with marginal density ; and  is a generative distribution with marginal density , where  is an easy-to-sample prior distribution for generation purpose.

Denoising Diffusion Probabilistic Models (DDPM) \cite{ho2020denoising} perturb the data variable  step-by-step to construct  by adding iterative Gaussian noises, leading  to be a non-parametrized fixed inference distribution with . Most \cite{okhotin2023star} of diffusion models assume a Markov chain for the generative process so to satisfy , and this modeling choice enables to optimize the surrogate objective  in a tractable way.

The continuous-time counterpart \cite{song2020score} of DDPM describes the diffusion process in the language of stochastic differential equations (SDE) by

with  now being a continuum of the diffusion index in , and  and  being the drift and the volatility coefficients, respectively. We describe our model under the continuous-time framework mainly for notational simplicity. Our model is applicable to both discrete- and continuous-time settings.

Under the continuous-time framework, the forward-time diffusion process of Eq. \eqref{eq:forward_sde} has a unique reverse-time diffusion process \cite{anderson1982reverse}

where  and  are the infinitesimal reverse-time and the reverse-time Brownian motion, respectively. Subsequently, the continuous-time generative process becomes

where the estimation target of the score network  is the actual data score . Here,  is the diffused probability density of the data distribution following the forward-time diffusion process in Eq. \eqref{eq:forward_sde}.

The continuous-time model trains the score network with the denoising score matching loss \cite{song2019generative}

where  is the temporal weight and  is the transition probability from  to . This denoising score objective coincides to the joint KL divergence  if  \cite{chen2016relation, song2021maximum}. Also, under different weighting functions, this objective could be equivalently interpreted as the noise matching loss  \cite{ho2020denoising} or the data reconstruction loss  \cite{kingma2021variational}.

There are various approaches to enhance the precision of score training. For instance, \citet{kim2022soft, kingma2023understanding, hang2023efficient} have proposed updating the score network using Maximum Perturbed Likelihood Estimation to improve large-time denoising accuracy. Conversely, \citet{lai2022regularizing, daras2023consistent} have studied the invariant characteristics of the data diffusion process and recommended adding an extra regularization term to the denoising score loss to meet these invariant properties. Our work, on the other hand, aims to refine the fixed model score with noise contrastive estimation, which is distinct from prior attempts to improve score accuracy.


\begin{algorithm}[t]
	\centering
	\caption{Discriminator Training}\label{alg:discriminator}
	\begin{algorithmic}[1]
		\STATE Construct  from the real-world
		\STATE Construct  by sampling from 
		\WHILE{converged}
		\STATE Sample  from the real dataset 
		\STATE Sample  from the sample dataset 
		\STATE Sample  from 
		\STATE Diffuse  for , 
		\STATE Calculate 
		\STATE Update 
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\section{Refining Generative Process with Discriminator Guidance}\label{sec:methodology}

\subsection{Correction of Pre-trained Model Score}\label{sec:error}

After score training, we synthesize samples with the time-reversal generative process

where  represents the score network after the convergence. This generative process could differ from the reverse-time data process if the local optimum  deviates from the global optimum . We show in Theorem \ref{thm:1} that the generative process of Eq. \eqref{eq:generative_process} coincides with the data process of Eq. \eqref{eq:reverse_sde} if we adjust the model score. We call this gap by the \textit{correction term}, which is \textit{nonzero} as long as .
\begin{theorem}\label{thm:1}
	Suppose  be the solution of the time-reversal generative process of Eq. \eqref{eq:generative_process}. Let  and  be the marginal densities (at ) of the forward-time SDE  starting from  and , respectively. If , where  is the prior distribution, and the log-likelihood  equals its evidence lower bound , then the reverse-time SDE
	
	coincides with a diffusion process with adjusted score,
	
	for .
\end{theorem}

\subsection{Discriminator Guidance}\label{sec:adjusted_generative_process}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{EDM_by_discriminator_epoch.pdf}
	\vskip -0.1in
	\caption{Discriminator guidance refines FID on CIFAR-10.}
	\label{fig:discriminator_training}
\end{figure}

The correction term  is intractable in general because the density-ratio  is inaccessible. Therefore, we estimate this density-ratio by training a discriminator at all noise level . For discriminator training, we first draw fake samples from the generative process of Eq. \eqref{eq:generative_process} as many as data instances. Then we classify the real and fake data using the noise-embedded Binary Cross Entropy (BCE)

where  is the temporal weight, see Algorithm \ref{alg:discriminator} and Appendix \ref{sec:forward_instead_of_generative} for details.

As the correction term is represented by

in terms of the optimal discriminator  of , we estimate the correction term  with a neural discriminator  by

With the above tractable correction estimate, we define the \textbf{Discriminator Guidance} (DG) by

Figure \ref{fig:discriminator_training} shows that the discriminator indeed improves sample quality with a quick convergence. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{loss_type3_v6.pdf}
	\caption{Schematic illustration of the analysis in Section \ref{sec:theory}. The gain increases as discriminator learns.}
	\label{fig:loss_visualization}
\end{figure}
\subsection{Theoretical Analysis}\label{sec:theory}
Although we introduced Discriminator Guidance in the context of differential equations, this section examines the approach from the perspective of statistical divergence between the data and the sample distributions. Specifically, we define  as the discriminator-guided sample distribution of Eq. \eqref{eq:dg}. The central question becomes

We answer the question in Theorem \ref{thm:2}. 
\begin{theorem}\label{thm:2}
	If the assumptions of Theorem \ref{thm:1} hold, then
	
	where  is the score error
	
	and  is the discriminator-adjusted score error
	
\end{theorem}

\begin{table}[t]
	\caption{Discriminator-adjusted score error  and corresponding Gain.}
	\label{tab:gain}
	\scriptsize
	\centering
		\begin{tabular}{lcc}
			\toprule
			Discriminator &  & Gain \\\midrule
			Blind  &  & 0 \\
			Optimal  & 0 &  (Maximum)\\\cdashlinelr{1-3}
			Untrained  &  &  \\
			Trained  &  &  \\
			\bottomrule
		\end{tabular}
\end{table}
To measure the effect of discriminator training, we use Theorem \ref{thm:2} to compute the gain by subtracting two KLs,

where  represents the difference between the score error and the discriminator-adjusted score error. Note that while Theorem \ref{thm:2} does not guarantee that the gain is strictly positive, it is initialized near zero and gradually increases throughout discriminator training, as summarized in Table \ref{tab:gain}. Specifically, when the discriminator is completely blind (), there is no signal from the discriminator gradient, and the discriminator-adjusted score error  equals the score error . Therefore, the gain is approximately zero when the discriminator is untrained () as shown in Figure \ref{fig:discriminator_training}. On the other hand, at the optimal discriminator , the neural correction  matches the target correction  and satisfies , allowing Gain to be maximized as discriminator parameters are updated. See Figure \ref{fig:loss_visualization} for a schematic visualization.

In other words, we can interpret that Discriminator Guidance introduces an additional axial degree of freedom  that reparametrizes the score error  into a discriminator-adjusted score error . As a result, the score error  is no longer optimized with the denoising score loss , but the reparametrized error  can be further optimized with an alternative loss  of Eq. \eqref{eq:discriminator_loss}.

\subsection{Optimality Analysis}\label{sec:mode_coverage}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{fid_recall_imagenet_256.pdf}
	\caption{FID and Recall trade-off on DiT-XL-G++.}
	\vskip -0.1in
	\label{fig:precision_recall}
\end{figure}

Let's take a closer look at the score component. Our discriminator training is stable because we keep a pre-trained score model fixed during training, unlike unstable GAN training. Hence, after the discriminator has reached its optimal point, the resulting adjusted model score is given by

Therefore, the sample distribution  balances data distribution and non-guided distribution. The argument also holds for the conditional case, leading DG as a controller for the intra-class diversity. Figure \ref{fig:precision_recall} experiments on ImageNet, demonstrating that there is a sweet spot of DG weight regarding both quality (FID) and diversity (recall).

\subsection{Connection with Classifier Guidance}

Classifier Guidance (CG) \cite{dhariwal2021diffusion} is a milestone technique to guide a sample with a pre-trained classifier . The classifier-guided generative process is . This is equivalent to sampling from the joint distribution of  because

where  is the oracle classifier at . Classifier Guidance provides supervision information on a sample path, evaluating whether the sample is correctly classified by the class label , or not. However, using Classifier Guidance may lead to mode collapse as it maximizes the classifier probability . In contrast, Discriminator Guidance offers enhanced mode coverage, as elaborated in Section \ref{sec:mode_coverage}, by providing distinctive supervision information on whether a sample path is realistic or not.

\begin{algorithm}[t]
	\centering
	\caption{Sampling with Guidance Techniques}\label{alg:sampler}
	\begin{algorithmic}[1]
		\STATE Sample 
		\FOR{ to }
		\STATE Sample  and 
		\STATE  \cite{karras2022elucidating}
		\STATE 
		\STATE 
		\STATE  (Eq. \eqref{eq:tau})
		\STATE 
		\STATE 
		\STATE 
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

As sampling from the joint distribution of  requires accurate score estimation, Discriminator Guidance and Classifier Guidance can be combined for a synergistic effect. We suggest the combination of guidance techniques by

\begin{wraptable}{r}{0.289\textwidth}
	\vskip -0.17in
	\centering
	\caption{Algorithm \ref{alg:sampler} includes DDPM, DDIM, and EDM samplers.}
	\label{tab:sampler}
	\tiny
	\centering
	\begin{tabular}{lcccc}
		\toprule
		Sampler &  &  &  &  \\\midrule
		DDPM & 0 & 1 & 0 & 0 \\
		DDIM & 0 & 0 & 0 & 0 \\
		EDM & 0 & 0 & 0 & 0 \\
		ADM-G & 0 & 1 & 0 & 0 \\\cdashlinelr{1-5}
		\cc{15}EDM-G++ & \cc{15}0 & \cc{15}0 & \cc{15}0 & \cc{15}0\\
		\cc{15}ADM-G++ & \cc{15}0 & \cc{15}1 & \cc{15}0 & \cc{15}0 \\
		\bottomrule
	\end{tabular}
	\vskip -0.1in
\end{wraptable}
where  and  are the time-dependent weights, respectively. The two pieces of information could ideally guide the sample toward the common likely region of classifier and discriminator in a complementary way. 

Algorithm \ref{alg:sampler} describes the full details of our sampling procedure for Eq. \eqref{eq:dg_cg}. The algorithm reduces the samplers of DDPM \cite{ho2020denoising, dhariwal2021diffusion}, DDIM \cite{song2020denoising}, and EDM \cite{karras2022elucidating} with corresponding hyperparameters in Table \ref{tab:sampler}. Our sampler is denoted as the postfix with \textbf{G++} upon a basic sampler by a prefix. See Appendix \ref{sec:sampling} for detailed sampling procedure.

\section{Related Works}

A line of research merges diffusion models with GAN models. \citet{zheng2022truncated, lyu2022accelerating} synthesize the diffused data  with a GAN generator (by putting  as generator's input), and denoise  to  with a diffusion model. \citet{xiao2021tackling} exchange thousands of denoising steps with a small number of sequential conditional GAN generators. \citet{wang2022diffusion} utilize the diffusion concept to train GAN. On the contrary, \citet{jolicoeur2020adversarial} train the diffusion model with an adversarial loss. The diffusion model and GAN in previous works, however, do not interplay with each other in generation process after their training. In contrast, the discriminator in DG intervenes in generation process, directly. Another difference to previous research is that we train our discriminator without any generator, so discriminator training is stable.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{2d_v11.pdf}
	\caption{Comparison of the true data score, synthetic score, and adjusted score in a toy 2-dimensional case. We assume  as data distribution and  as hypothetially synthesized generative distribution with an incorrect score . We solve the probability-flow ODE \cite{song2020score} to visualize samples from each score function.}
	\vskip -0.1in
	\label{fig:2d_two_modal}
\end{figure}

Other previous works use the rejection sampler or MCMC to draw samples from a reweighted model distribution. \citet{azadi2018discriminator, che2020your} utilize the rejection sampler to adjust the generator implicit distribution with the discriminator under the likelihood-ratio trick \cite{gutmann2010noise}. Similarly, \citet{turner2019metropolis} make use of likelihood-ratio trick to sample with MCMC. Specifically, \citet{aneja2021contrastive} and \citet{bauer2019resampled} introduce the importance and rejection samplings from the aggregate posterior in VAE, respectively. In diffusion context, Discriminator Guidance maximizes  in the same spirit of gradient ascent, and it could be interpreted as reweighting the model distribution  with an importance weight of . Consequently, DG does not require sample rejection and is becoming scalable to high-dimensions.

\section{Experiments}

\subsection{A Toy 2-dimensional Case}

\begin{table}[t]
	\caption{Performance on CIFAR-10.}
	\vskip -0.05in
	\label{tab:cifar10}
	\tiny
	\centering
	\begin{threeparttable}
		\begin{tabular}{l@{\hskip 0.2cm}c@{\hskip 0.2cm}c@{\hskip 0.3cm}c@{\hskip 0.2cm}c@{\hskip 0.cm}c}
			\toprule
			\multirow{2}{*}{Model} & \multirow{2}{*}{\shortstack{Diffusion\\Space}} & \multirow{2}{*}{NFE} & \multicolumn{2}{@{\hskip 0.1cm}c@{\hskip 0.8cm}}{Unconditional} & \multicolumn{1}{@{\hskip -0.3cm}c}{Conditional} \\
			& & & NLL & FID & FID \\\midrule
			VDM \cite{kingma2021variational} & Data & 1000 & \textbf{2.49} & 7.41 & - \\
			DDPM \cite{ho2020denoising} & Data & 1000 & 3.75 & 3.17 & - \\
			iDDPM \cite{nichol2021improved} & Data & 1000 & 3.37 & 2.90 & - \\
			Soft Truncation \cite{kim2022soft} & Data & 2000 & 2.91 & 2.47 & - \\
			INDM \cite{kim2022maximum} & Latent & 2000 & 3.09 & 2.28 & - \\
			CLD-SGM \cite{dockhorn2021score} & Data & 312 & 3.31 & 2.25 & - \\
			NCSN++ \cite{song2020score} & Data & 2000 & 3.45 & 2.20 & - \\
			LSGM \cite{vahdat2021score} & Latent & 138 & 3.43 & 2.10 & - \\
			NCSN++-G \cite{chao2022denoising} & Data & 2000 & - & - & 2.25 \\
			EDM\tnote{\textdagger}  (random seed) & Data & 39 & 2.60 & 2.03 & 1.82 \\
			EDM (reported, manual seed) & Data & \textbf{35} & 2.60 & 1.97 & 1.79 \\\cdashlinelr{1-6}
			\cc{15}LSGM-G++ & \cc{15}Latent & \cc{15}138 & \cc{15}3.42 & \cc{15}1.94 & \cc{15}- \\
			\cc{15}EDM-G++ & \cc{15}Data & \cc{15}\textbf{35} & \cc{15}2.55 & \cc{15}\textbf{1.77} & \cc{15}\textbf{1.64} \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}
			\item[\textdagger] We recalculate FID of EDM \cite{karras2022elucidating} under a random seed for a fair comparison with previous research. We report our performances under the random seed by default.
		\end{tablenotes}
	\end{threeparttable}
	\vskip -0.1in
\end{table}

\begin{table}[t]
	\caption{Performance on CelebA/FFHQ 64x64.}
	\vskip -0.05in
	\label{tab:human-face}
	\scriptsize
	\centering
	\begin{tabular}{lccc}
		\toprule
		Model & NFE & CelebA & FFHQ\\\midrule
		DDPM++ \cite{song2020score} & 131 & 2.32 & - \\
		Soft Truncation \cite{kim2022soft} & 131 & 1.90 & - \\
		Soft Diffusion \cite{daras2022soft} & 300 & 1.85 & - \\
		INDM \cite{kim2022maximum} & 132 & 1.75 & - \\
		Diffusion StyleGAN2 \cite{wang2022diffusion} & \textbf{1} & 1.69 & - \\
		EDM \cite{karras2022elucidating} & 79 & - & 2.39 \\\cdashlinelr{1-4}
		\cc{15}Soft Truncation-G++ & \cc{15}131 & \cc{15}\textbf{1.34} & \cc{15}- \\
		\cc{15}EDM-G++ & \cc{15}71 & \cc{15}- & \cc{15}\textbf{1.98} \\
		\bottomrule
	\end{tabular}
	\vskip -0.1in
\end{table}

Figure \ref{fig:2d_two_modal} shows the experimental result of a tractable 2-dimensional toy case. We train a 4-layered MLP discriminator with 256 neurons until convergence, and we hypothesize an incorrect score function  (for a synthetic generative distribution ) that misfits to the data score. If there is no guidance, the incorrect score  will generate samples from a wrong distribution as in Figure \ref{fig:2d_two_modal}. On the contrary,  successfully guides  to , see Figures \ref{fig:2d_two_gaussian} and \ref{fig:2d_standard_gaussian} for additional visualization. 

\subsection{Image Generation}

We experiment on CIFAR-10, CelebA/FFHQ 64x64, and ImageNet 256x256. We use the pre-trained networks on CIFAR-10 and FFHQ from \citet{karras2022elucidating, vahdat2021score}, CelebA from \citet{kim2022soft} and ImageNet from \citet{dhariwal2021diffusion, peebles2022scalable}. 

\textbf{Discriminator Network.} We use the encoder of U-Net structure\footnote{We tested MLP, ResNet18, and a transformer, but the U-Net performed the best for Discriminator Guidance.} as our discriminator network. For diffusion models on data space, we attach two noise-embedded U-Net encoders: the pre-trained ADM classifier \cite{dhariwal2021diffusion} and an auxiliary (shallow) U-Net encoder. We put  to the ADM classifier, and we extract the latent  of  from the last pooling layer of the pre-trained classifier. Then, we put  to the auxiliary U-Net encoder and predict real/fake by its output. We freeze the ADM classifier, and we only fine-tune shallow U-Net encoder as default. Not to mention that fine-tuning save the training cost, fine-tuning performs better or equivalent to training the entire architecture \cite{kato2021non}. For LSGM-G++, we train the U-Net encoder from scratch. For DiT-XL-G++, we train the latent classifier with the same architecture of the ADM classifier except the input dimension, and fine-tuning the shallow U-Net encoder for discriminator. We train a class-conditional discriminator for the class-conditional generation. See Table \ref{tab:configurations} for detailed training configuration.

\begin{table}[t]
	\caption{Performance on ImageNet 256x256.}
	\vskip -0.05in
	\label{tab:ImageNet256}
	\tiny
	\centering
	\begin{tabular}{l@{\hskip 0.05cm}c@{\hskip 0.09cm}c@{\hskip 0.14cm}c@{\hskip 0.13cm}c@{\hskip 0.20cm}c@{\hskip 0.14cm}c@{\hskip 0.14cm}c}
		\toprule
		\multirow{2}{*}{Model} & \multirow{2}{*}{\shortstack{Diffusion\\Space}} & \multirow{2}{*}{FID} & \multirow{2}{*}{sFID} & \multirow{2}{*}{IS} & \multirow{2}{*}{Prec} & \multirow{2}{*}{Rec} & \multirow{2}{*}{F1} \\
		&&&&&&&\\\midrule
		\multicolumn{2}{l}{Validation Data} & 1.68 & 3.67 & 232.21 & 0.75 & 0.66 & 0.70 \\\midrule
		ADM \cite{dhariwal2021diffusion} & Data & 10.94 & 6.02 & 100.98 & 0.69 & 0.63 & 0.66 \\
		DiT-XL/2 \cite{peebles2022scalable} & Latent & 9.62 & 6.85 & 121.50 & 0.67 & 0.67 & 0.67 \\
		ADM-G \cite{dhariwal2021diffusion} & Data & 4.59 & 5.25 & 186.70 & 0.82 & 0.52 & 0.64 \\
		RIN \cite{jabri2022scalable} & Data & 4.51 & - & 161.00 & - & - & - \\
		LDM-4-G \cite{rombach2022high} & Latent & 3.60 & - & 247.67 & \textbf{0.87} & 0.48 & 0.62 \\
		RIN + schedule \cite{chen2023importance} & Data & 3.52 & - & 186.20 & - & - & - \\
		simple diffusion \cite{hoogeboom2023simple} & Data & 2.77 & - & 211.80 & - & - & - \\
		DiT-XL/2-G \cite{peebles2022scalable} & Latent & 2.27 & 4.60 & 278.24 & 0.83 & 0.57 & 0.68 \\\cdashlinelr{1-8}
		\cc{15}ADM-G++ & \cc{15}Data & \cc{15}3.18 & \cc{15}\textbf{4.53} & \cc{15}255.74 & \cc{15}0.84 & \cc{15}0.53 & \cc{15}0.66 \\
		\cc{15}DiT-XL/2-G++ & \cc{15}Latent & \cc{15}\textbf{1.83} & \cc{15}5.16 & \cc{15}\textbf{281.53} & \cc{15}0.78 & \cc{15}\textbf{0.64} & \cc{15}\textbf{0.70} \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[t]
	\caption{Component-wise computational budget.}
	\vskip -0.05in
	\label{tab:budget}
	\scriptsize
	\centering
	\begin{tabular}{lccc}
		\toprule
		\multirow{2}{*}{\shortstack[l]{Dataset (Model)}} & \multirow{2}{*}{\shortstack{Score\\Training}} & \multirow{2}{*}{\shortstack{Sample\\Generation}} & \multirow{2}{*}{\shortstack{Discriminator\\Training}} \\
		&&&\\\midrule
		\multirow{2}{*}{CIFAR-10 (EDM)} & \multirow{2}{*}{\shortstack{200M \1hr)}} & \multirow{2}{*}{\shortstack{1M \
&\mathbb{E}[\mathbf{x}_{0}\vert\mathbf{x}_{t}=\mathbf{y}+\sigma(t)\bm{\epsilon}]\\
&\quad\quad\quad=\mathbf{y}+\sigma(t)\bm{\epsilon}+\sigma^{2}(t)\nabla\log{p_{r}^{t}\big(\mathbf{y}+\sigma(t)\bm{\epsilon}\big)}.

\mathbb{E}_{\bm{\epsilon}}\big[\mathbb{E}[\mathbf{x}_{0}\vert\mathbf{x}_{t}=\mathbf{y}+\sigma(t)\bm{\epsilon}]\big]\approx\mathbf{y}.

	D_{h}(r_{\bm{\theta}_{\infty}}\Vert r_{\bm{\phi}})&=\int \lambda(t) \mathbb{E}_{p_{\bm{\theta}_{\infty}}^{t}(\mathbf{x}_{t})}\big[h(r_{\bm{\theta}_{\infty}}^{t}(\mathbf{x}_{t}))-h(r_{\bm{\phi}}^{t}(\mathbf{x}_{t}))\\
	&\quad-\partial h(r_{\bm{\phi}}^{t}(\mathbf{x}_{t}))(r_{\bm{\theta}_{\infty}}^{t}(\mathbf{x}_{t})-r_{\bm{\phi}}^{t}(\mathbf{x}_{t}))\big]\diff t,
	
D_{f}(p_{r}(\mathbf{x}_{0})\Vert p_{\bm{\theta}}(\mathbf{x}_{0}))=D_{f}(p_{r}^{T}(\mathbf{x}_{T})\Vert \pi(\mathbf{x}_{T}))+E_{\bm{\theta},\bm{\phi}}^{f},

	\diff\mathbf{x}_{t}=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\nabla\log{p_{r}^{t}(\mathbf{x}_{t})}\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},
	
	\diff\mathbf{x}_{t}&=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)(\mathbf{s}_{\bm{\theta}}+\mathbf{c}_{\bm{\theta}})(\mathbf{x}_{t},t)\big]\diff\bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},
	
	D_{KL}(p_{r}\Vert p_{\bm{\theta}})=\frac{1}{2}\int_{0}^{T}g^{2}(t)\mathbb{E}\big[\Vert\mathbf{s}_{\bm{\theta}}(\mathbf{x}_{t},t)-\nabla\log{p_{r}^{t}(\mathbf{x}_{t})}\Vert_{2}^{2}\big]\diff t+D_{KL}(p_{r}^{T}\Vert\pi)
	
	D_{KL}(p_{r}\Vert p_{\bm{\theta}})=\frac{1}{2}\int_{0}^{T}g^{2}(t)\mathbb{E}\big[\Vert\mathbf{s}_{\bm{\theta}}(\mathbf{x}_{t},t)-\nabla\log{p_{r}^{t}(\mathbf{x}_{t})}\Vert_{2}^{2}\big]\diff t+D_{KL}(p_{r}^{T}\Vert\pi).
	\label{eq:generative_process_appendix}
	\begin{split}
	\diff\mathbf{x}_{t}&=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\mathbf{s}_{\bm{\theta}}(\mathbf{x}_{t},t)\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}\\
	&=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\nabla\log{q_{t}(\mathbf{x}_{t})}\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},
	\end{split}
	
	&D_{KL}(p_{r}\Vert p_{\bm{\theta}})=D_{KL}(p_{r}^{T}\Vert \pi)+E_{\bm{\theta}},\\
	&D_{KL}(p_{r}\Vert p_{\bm{\theta},\bm{\phi}})\le D_{KL}(p_{r}^{T}\Vert\pi)+E_{\bm{\theta},\bm{\phi}},
	
	&E_{\bm{\theta}}=\frac{1}{2}\int_{0}^{T}g^{2}(t)\mathbb{E}_{p_{r}^{t}}\big[\Vert\nabla\log{p_{r}^{t}}-\nabla\log{p_{\bm{\theta}}^{t}}\Vert_{2}^{2}\big],\\
	&E_{\bm{\theta},\bm{\phi}}=\frac{1}{2}\int_{0}^{T}g^{2}(t)\mathbb{E}_{p_{r}^{t}}\big[\Vert \mathbf{c}_{\bm{\theta}}-\mathbf{c}_{\bm{\phi}}\Vert_{2}^{2}\big]\diff t.
	
	\diff\mathbf{x}_{t}&=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)(\mathbf{s}_{\bm{\theta}}+\mathbf{c}_{\bm{\phi}})\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}\\	
	&=\Big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\Big(\mathbf{s}_{\bm{\theta}}+\nabla\log{\frac{d_{\bm{\phi}}}{1-d_{\bm{\phi}}}}\Big)\Big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}\\
	&=\Big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\Big(\nabla\log{p_{g}^{t}}+\nabla\log{\frac{d_{\bm{\phi}}}{1-d_{\bm{\phi}}}}\Big)\Big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}\\
	&=\bigg[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\Big(\nabla\log{p_{g}^{t}}+\nabla\log{\frac{d_{\bm{\phi}_{*}}}{1-d_{\bm{\phi}_{*}}}}-\nabla\log{\frac{d_{\bm{\phi}_{*}}}{1-d_{\bm{\phi}_{*}}}}+\nabla\log{\frac{d_{\bm{\phi}}}{1-d_{\bm{\phi}}}}\Big)\bigg]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}\\
	&=\bigg[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\Big(\nabla\log{p_{g}^{t}}+\nabla\log{\frac{p_{r}^{t}}{p_{g}^{t}}}-\nabla\log{\frac{d_{\bm{\phi}_{*}}}{1-d_{\bm{\phi}_{*}}}}+\nabla\log{\frac{d_{\bm{\phi}}}{1-d_{\bm{\phi}}}}\Big)\bigg]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}\\
	&=\bigg[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\Big(\nabla\log{p_{r}^{t}}-\nabla\log{\frac{d_{\bm{\phi}_{*}}}{1-d_{\bm{\phi}_{*}}}}+\nabla\log{\frac{d_{\bm{\phi}}}{1-d_{\bm{\phi}}}}\Big)\bigg]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t}.
	
	D_{KL}(p_{r}\Vert p_{\bm{\theta},\bm{\phi}})\le D_{KL}(p_{r}^{T}\Vert \pi)+\frac{1}{2}\int_{0}^{T}g^{2}(t)\mathbb{E}_{p_{r}^{t}}\bigg[\Big\Vert\nabla\log{\frac{d_{\bm{\phi}_{*}}}{1-d_{\bm{\phi}_{*}}}}-\nabla\log{\frac{d_{\bm{\phi}}}{1-d_{\bm{\phi}}}}\Big\Vert_{2}^{2}\bigg]\diff t.
	
\mathbb{E}_{\mathbf{x}_{0}\sim p_{r}}[-\log{p_{g}(\mathbf{x}_{0})}]\le\mathbb{E}_{\mathbf{x}_{\tau}\sim p_{r}^{\tau}}[-\log{p_{\bm{\theta},\tau}(\mathbf{x}_{\tau})}]+R_{\tau}(\bm{\theta}),

\frac{\diff}{\diff t}\log{p_{\bm{\theta},t}(\mathbf{x}_{t})}=-\text{tr}\bigg(\nabla\Big[\mathbf{f}(\mathbf{x}_{t},t)-\frac{1}{2}g^{2}(t)\mathbf{x}_{\bm{\theta}}(\mathbf{x}_{t},t)\Big]\bigg)

\diff\mathbf{x}_{t}=\bigg[\mathbf{f}(\mathbf{x}_{t},t)-\frac{1}{2}g^{2}(t)\mathbf{s}_{\bm{\theta}}(\mathbf{x}_{t},t)\bigg]\diff t.

\mathcal{L}([0,T])\le \mathcal{L}([\tau,T])+R_{\tau}(\bm{\theta}),

\mathcal{L}([\tau,T])&=\frac{1}{2}\int_{\tau}^{T}\mathbb{E}\Big[g^{2}(t)\Vert \nabla\log{p_{0t}(\mathbf{x}_{t}\vert\mathbf{x}_{0})}-\mathbf{s}_{\bm{\theta}}(\mathbf{x}_{t},t)\Vert_{2}^{2}-g^{2}(t)\Vert\nabla\log{p_{0t}(\mathbf{x}_{t}\vert\mathbf{x}_{0})}\Vert_{2}^{2}-2\text{div}(\mathbf{f})\Big]\diff t-\mathbb{E}\big[\log{\pi(\mathbf{x}_{T})}\big].

\mathcal{F}(t)&=\int \frac{g^{2}(t)}{\sigma^{2}(t)}\diff t\\
&=\int 2\rho\frac{\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho}}{\sigma_{min}^{1/\rho}+t(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}\diff t\\
&=2\rho\log{(\sigma_{min}^{1/\rho}+t(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho}))},

Z&=\int_{\tau}^{T}\frac{g^{2}(t)}{\sigma^{2}(t)}\diff t\\
&=\mathcal{F}(T)-\mathcal{F}(\tau)\\
&=2\rho\log{\frac{\sigma_{min}^{1/\rho}+T(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}}.

&t=F^{-1}(u)\\
&\iff u=F(t)=\frac{1}{Z}\int_{\tau}^{t}\frac{g^{2}(s)}{\sigma^{2}(s)}\diff s=\frac{1}{Z}\big(\mathcal{F}(t)-\mathcal{F}(\tau)\big)\\
&\iff u\log{\frac{\sigma_{min}^{1/\rho}+T(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}}=\log{\frac{\sigma_{min}^{1/\rho}+t(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}}\\
&\iff \bigg(\frac{\sigma_{min}^{1/\rho}+T(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}\bigg)^{u}=\frac{\sigma_{min}^{1/\rho}+t(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}\\
&\iff \big(\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})\big)\bigg(\frac{\sigma_{min}^{1/\rho}+T(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}\bigg)^{u}=\sigma_{min}^{1/\rho}+t(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})\\
&\iff t=\bigg(\big(\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})\big)\bigg(\frac{\sigma_{min}^{1/\rho}+T(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}{\sigma_{min}^{1/\rho}+\tau(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho})}\bigg)^{u}-\sigma_{min}^{1/\rho}\bigg)\bigg/(\sigma_{max}^{1/\rho}-\sigma_{min}^{1/\rho}).

D_{h}(r^{*}\Vert r_{\bm{\phi}})&=\int p_{de}(\mathbf{x})B_{h}(r^{*}(\mathbf{x})\Vert r_{\bm{\phi}}(\mathbf{x}))\diff\mathbf{x}\\
&=\int p_{de}(\mathbf{x})\Big(h\big(r^{*}(\mathbf{x})\big)-h\big(r_{\bm{\phi}}(\mathbf{x})\big)-\partial h\big(r_{\bm{\phi}}(\mathbf{x})\big)\big(r^{*}(\mathbf{x})-r_{\bm{\phi}}(\mathbf{x})\big)\Big)\diff\mathbf{x},

D_{h}(r^{*}\Vert r_{\bm{\phi}})=\int p_{de}(\mathbf{x})\Big[\partial h\big(r_{\bm{\phi}}(\mathbf{x})\big)r_{\bm{\phi}}(\mathbf{x})-h\big(r_{\bm{\phi}}(\mathbf{x})\big)\Big]\diff\mathbf{x}-\int p_{nu}(\mathbf{x})\Big[\partial h\big(r_{\bm{\phi}}(\mathbf{x})\big)\Big]\diff\mathbf{x}.

BD_{f_{LSIF}}(r^{*}\Vert r_{\bm{\phi}})&=\frac{1}{2}\int p_{de}(\mathbf{x})\big(r^{*}(\mathbf{x})-r_{\bm{\phi}}(\mathbf{x})\big)^{2}\diff \mathbf{x}\\
&=\frac{1}{2}\int p_{de}(\mathbf{x})r_{\bm{\phi}}^{2}(\mathbf{x})\diff\mathbf{x}-\int p_{nu}(\mathbf{x})r_{\bm{\phi}}(\mathbf{x})\diff\mathbf{x},

D_{KL}(p_{nu}\Vert p_{nu,\bm{\phi}})\text{ subject to }\int p_{nu,\bm{\phi}}(\mathbf{x})\diff\mathbf{x}=1,

BD_{f_{UKL}}(r^{*}\Vert r_{\bm{\phi}})=\int p_{de}(\mathbf{x})r_{\bm{\phi}}(\mathbf{x})\diff\mathbf{x}-\int p_{nu}(\mathbf{x})\log{r_{\bm{\phi}}(\mathbf{x})}\diff\mathbf{x}.

\int\lambda(t)D_{h}\bigg(\frac{p_{r}^{t}(\cdot)}{p_{\bm{\theta}}^{t}(\cdot)}\Big\Vert \frac{1-d_{\bm{\phi}}(\cdot,t)}{d_{\bm{\phi}}(\cdot,t)}\bigg)\diff t,

\diff\mathbf{y}_{t}=[\mathbf{f}(\mathbf{y}_{t},t)-g^{2}(t)\mathbf{s}_{\bm{\theta}}(\mathbf{y}_{t},t)]\diff\bar{t}+g(t)\diff\mathbf{\bar{w}}_{t},

\diff\mathbf{y}_{t}=\bigg[\mathbf{f}(\mathbf{y}_{t},t)-g^{2}(t)\Big(\mathbf{s}_{\bm{\theta}}(\mathbf{y}_{t},t)+\nabla\log{\frac{p_{\mathcal{T}}^{t}(\mathbf{y}_{t})}{\alpha p_{\mathcal{S}}^{t}(\mathbf{y}_{t})+(1-\alpha)p_{\bm{\theta}}^{t}(\mathbf{y}_{t})}}\Big)\bigg]\diff\bar{t}+g(t)\diff\mathbf{\bar{w}}_{t}.

\diff\mathbf{y}_{t}\approx\bigg[\mathbf{f}(\mathbf{y}_{t},t)-g^{2}(t)\Big(\mathbf{s}_{\bm{\theta}}(\mathbf{y}_{t},t)+\nabla\log{\frac{p_{\mathcal{T}}^{t}(\mathbf{y}_{t})}{p_{\mathcal{S}}^{t}(\mathbf{y}_{t})}}\Big)\bigg]\diff\bar{t}+g(t)\diff\mathbf{\bar{w}}_{t},

\diff\mathbf{y}_{t}\approx\bigg[\mathbf{f}(\mathbf{y}_{t},t)-g^{2}(t)\Big(\mathbf{s}_{\bm{\theta}}(\mathbf{y}_{t},t)+\nabla\log{\frac{p_{\mathcal{T}}^{t}(\mathbf{y}_{t})}{p_{\bm{\theta}}^{t}(\mathbf{y}_{t})}}\Big)\bigg]\diff\bar{t}+g(t)\diff\mathbf{\bar{w}}_{t},

\frac{\diff\mathbf{x}_{t}}{\diff t}=\mathbf{f}(\mathbf{x}_{t},t)-\frac{1}{2}g^{2}(t)(\mathbf{s}_{\bm{\theta}_{\infty}}+w_{t}^{DG}\mathbf{c}_{\bm{\phi}_{\infty}})(\mathbf{x}_{t},t)

\frac{\diff\mathbf{x}_{t}}{\diff t}=\mathbf{f}(\mathbf{x}_{t},t)-\frac{1}{2}g^{2}(t)\mathbf{s}_{\bm{\theta}_{\infty}}(\mathbf{x}_{t},t)

\mathbf{\tilde{x}}_{t-\Delta t}=\mathbf{x}_{t}-\Delta t\Big[f(\mathbf{x}_{t},t)-\frac{1}{2}g^{2}(t)(\mathbf{s}_{\bm{\theta}_{\infty}}+w_{t,1^{\text{st}}}^{DG}\mathbf{c}_{\bm{\phi}_{\infty}})(\mathbf{x}_{t},t)\Big].

\mathbf{x}_{t-\Delta t}&=\mathbf{x}_{t}-\Delta t\bigg[\frac{1}{2}\Big(\mathbf{f}(\mathbf{x}_{t},t)-\frac{1}{2}g^{2}(t)(\mathbf{s}_{\bm{\theta}_{\infty}}+w_{t,1^{\text{st}}}^{DG}\mathbf{c}_{\bm{\phi}_{\infty}})(\mathbf{x}_{t},t)\Big)\\
&\quad\quad\quad\quad\quad+\frac{1}{2}\Big(\mathbf{f}(\mathbf{\tilde{x}}_{t-\Delta t},t-\Delta t)-\frac{1}{2}g^{2}(t-\Delta t)(\mathbf{s}_{\bm{\theta}_{\infty}}+w_{t,2^{\text{nd}}}^{DG}\mathbf{c}_{\bm{\phi}_{\infty}})(\mathbf{\tilde{x}}_{t-\Delta t},t-\Delta t)\Big)\bigg]\\
&=\mathbf{x}_{t}-\Delta t\bigg[\frac{1}{2}\Big(\mathbf{f}(\mathbf{x}_{t},t)+\mathbf{f}(\mathbf{\tilde{x}}_{t-\Delta t},t-\Delta t)\Big)-\frac{1}{4}\Big(g^{2}(t)\mathbf{s}_{\bm{\theta}_{\infty}}(\mathbf{x}_{t},t)+\mathbf{s}_{\bm{\theta}_{\infty}}(\mathbf{\tilde{x}}_{t-\Delta t},t-\Delta t)\Big)\\
&\quad\quad\quad\quad\quad-\frac{1}{4}\Big(w_{t,1^{\text{st}}}^{DG}g^{2}(t)\mathbf{c}_{\bm{\phi}_{\infty}}(\mathbf{x}_{t},t)+w_{t,1^{\text{nd}}}^{DG}g^{2}(t-\Delta t)\mathbf{c}_{\bm{\phi}_{\infty}}(\mathbf{\tilde{x}}_{t-\Delta t},t-\Delta t)\bigg].
\label{eq:tau}
\begin{split}
&\tau=\frac{-\beta_{min}+\sqrt{\beta_{min}^{2}+2(\beta_{max}-\beta_{min})\log{1+\sigma_{WVE}^{2}(t)}}}{\beta_{max}-\beta_{min}},\\
&\nu_{\tau(t)}=e^{-\frac{1}{2}\int_{0}^{\tau(t)}\beta(s)\diff s}.
\end{split}

\diff\mathbf{x}_{t}=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)(\mathbf{s}_{\bm{\theta}_{\infty}}+\mathbf{c}_{\bm{\phi}_{\infty}})(\mathbf{x}_{t},t)\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},

\diff\mathbf{x}_{t}=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\mathbf{s}_{\bm{\theta}_{\infty}}(\mathbf{x}_{t},t)\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},

\diff\mathbf{x}_{t}=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)\mathbf{s}_{\bm{\theta}_{\infty}}(\mathbf{x}_{t},t)\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},

\diff\mathbf{x}_{t}=\big[\mathbf{f}(\mathbf{x}_{t},t)-g^{2}(t)(\mathbf{s}_{\bm{\theta}_{\infty}}+\mathbf{c}_{\bm{\phi}_{\infty}})(\mathbf{x}_{t},t)\big]\diff \bar{t}+g(t)\diff\bar{\mathbf{w}}_{t},

on the small range . Contrastive to the minimum scale ablation study, the larger the scale DG applied, the better the performance. This means that the actual effect of DG lies in constructing the global shape of the generation, rather than denoising fine-details. In the community of diffusion models, there are only a few works that systematically divide the context generation ability and fine-detail capturing ability of diffusion models. Figure \ref{fig:scale_ablation} clarify that DG effectively adjusts the context generation ability of diffusion models, rather than cleansing the fine-dust in images.

\begin{wrapfigure}{R}{0.4\textwidth}
	\vskip -0.2in
	\centering
	\includegraphics[width=\linewidth]{FFHQ_v5.pdf}
	\caption{NFE ablation on FFHQ.}
	\vskip -0.3in
	\label{fig:FFHQ}
\end{wrapfigure}
\textbf{Ablation of NFE on FFHQ} Figure \ref{fig:FFHQ} illustrates FID by NFE after discriminator training. For visualization purpose, we select the best hyperparameters to experiment with, except NFE. Similar to NFE ablation on CIFAR-10, the discriminator guidance keeps enhancing FID throughout NFEs. 

\subsection{Uncurated Samples}\label{sec:uncurated}

Figures \ref{fig:ImageNet256_recall_90}, \ref{fig:ImageNet256_recall_130}, \ref{fig:ImageNet256_recall_281}, \ref{fig:ImageNet256_recall_323}, \ref{fig:ImageNet256_recall_386}, \ref{fig:ImageNet256_recall_417}, \ref{fig:ImageNet256_recall_562} compare ADM-G++ (cfg=0.10) with the vanilla ADM to illustrate how sample fidelity is improved while keeping the sample diversity. ADM-G++ (cfg=0.10) performs FID of 4.45 and recall of 0.60, and ADM performs FID of 10.94 and recall of 0.63.

Figures \ref{fig:ImageNet256_FID_90}, \ref{fig:ImageNet256_FID_130}, \ref{fig:ImageNet256_FID_281}, \ref{fig:ImageNet256_FID_323}, \ref{fig:ImageNet256_FID_386}, \ref{fig:ImageNet256_FID_417}, \ref{fig:ImageNet256_FID_562} compare ADM-G++ (cfg=0.75) with ADM-G (cfg=1.50). These figures show the discriminator guidance is effective in high-dimensional dataset. 

Figures \ref{fig:unconditional_CIFAR10_LSGM-G++}, \ref{fig:unconditional_CIFAR10_EDM-G++}, \ref{fig:conditional_CIFAR10_EDM-G++}, \ref{fig:celeba}, and \ref{fig:ffhq} show uncurated samples from unconditional CIFAR-10 with LSGM-G++, unconditional CIFAR-10 with EDM-G++, conditional CIFAR-10 with EDM-G++, unconditional CelebA with Soft Truncation-G++, and unconditional FFHQ with EDM-G++.

Figure \ref{fig:I2I_uncurated} shows the uncurated samples from I2I translation task.

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_90_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_90_v4.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from lorikeet class (90) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_90}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_130_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_130_v4.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from flamingo class (130) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_130}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_281_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_281_v4.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from tabby cat class (281) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_281}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_323_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_323_v4.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from monarch butterfly class (323) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_323}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_386_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_386_v3.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from african elephant class (386) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_386}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_417_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_417_v3.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from ballon class (417) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_417}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm_uncurated_samples_562_v3.jpg}
		\subcaption{ADM (FID 10.94 recall 0.63)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_recall__uncurated_samples_562_v3.jpg}
		\subcaption{ADM-G++ (FID 4.45 recall 0.60)}
	\end{subfigure}
	\caption{Uncurated random samples from fountain class (562) (a) ADM with poor FID (10.94) and good recall (0.63), (b) ADM-G++ with good FID (4.45) and good recall (0.60).}
	\label{fig:ImageNet256_recall_562}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_90_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_90_v4.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from lorikeet class (90) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_90}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_130_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_130_v3.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from flamingo class (130) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_130}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_281_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_281_v4.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from tabby cat class (281) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_281}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_323_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_323_v3.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from monarch butterfly class (323) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_323}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_386_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_386_v4.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from african elephant class (386) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_386}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_417_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_417_v4.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from ballon class (417) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_417}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G_uncurated_samples_562_v3.jpg}
		\subcaption{ADM-G (FID 4.59 recall 0.52)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{adm-G++_FID__uncurated_samples_562_v3.jpg}
		\subcaption{ADM-G++ (FID 3.18 recall 0.53)}
	\end{subfigure}
	\caption{Uncurated random samples from fountain class (562) (a) ADM-G with good FID (4.59) and poor recall (0.52), (b) ADM-G++ with SOTA FID (3.18) and moderate recall (0.53).}
	\label{fig:ImageNet256_FID_562}
\end{figure}

\begin{figure*}[t]
	\centering
		\includegraphics[width=\linewidth]{uncurated_samples_unconditional_CIFAR10_LSGM-G++_v2.jpg}
	\caption{Uncurated random samples from LSGM-G++ on unconditional CIFAR10 (FID: 1.94).}
	\label{fig:unconditional_CIFAR10_LSGM-G++}
\end{figure*}

\begin{figure*}[t]
	\centering
		\includegraphics[width=\linewidth]{uncurated_samples_unconditional_CIFAR10_EDM-G++_v2.jpg}
	\caption{Uncurated random samples from EDM-G++ on unconditional CIFAR10 (FID: 1.77).}
	\label{fig:unconditional_CIFAR10_EDM-G++}
\end{figure*}

\begin{figure*}[t]
	\centering
		\includegraphics[width=\linewidth]{uncurated_samples_conditional_CIFAR10_EDM-G++_v2.jpg}
	\caption{Uncurated random samples from EDM-G++ on conditional CIFAR10 (FID: 1.64).}
	\label{fig:conditional_CIFAR10_EDM-G++}
\end{figure*}

\begin{figure*}[t]
	\centering
		\includegraphics[width=\linewidth]{uncurated_samples_CelebA_Soft_Truncation-G++_v2.jpg}
	\caption{Uncurated random samples from Soft Truncation-G++ on unconditional CelebA (FID: 1.34).}
	\label{fig:celeba}
\end{figure*}

\begin{figure*}[t]
	\centering
		\includegraphics[width=\linewidth]{uncurated_samples_FFHQ_EDM-G++_v2.jpg}
	\caption{Uncurated random samples from EDM-G++ on unconditional FFHQ (FID: 1.98).}
	\label{fig:ffhq}
\end{figure*}

\begin{figure*}[t]
	\centering
	\begin{subfigure}{0.29\linewidth}
		\centering
		\includegraphics[width=\linewidth]{I2I_real.jpg}
		\subcaption{Cat (Source)}
	\end{subfigure}
	\hfil
	\begin{subfigure}{0.29\linewidth}
		\centering
		\includegraphics[width=\linewidth]{I2I_sdedit.jpg}
		\subcaption{SDEdit (FID: 74.02)}
	\end{subfigure}		
	\hfil
	\begin{subfigure}{0.29\linewidth}
		\centering
		\includegraphics[width=\linewidth]{I2I_sdedit_dg.jpg}
		\subcaption{SDEdit + DG (FID: 61.92)}
	\end{subfigure}
	\caption{Uncurated random translated samples from (a) source cat, (b) SDEdit (FID: 74.02, L2: 49.22, PSNR: 19.21, SSIM: 0.42), and (c) SDEdit + DG with  (FID: 61.92, L2: 50.62, PSNR: 18.94, SSIM: 0.41).}
	\label{fig:I2I_uncurated}
\end{figure*}




\end{document}
