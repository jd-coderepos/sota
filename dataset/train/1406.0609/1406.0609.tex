\documentclass[10pt,conference]{IEEEtran}


\usepackage{times}
\usepackage{amssymb}
 \usepackage{amsmath,bbm}
 \usepackage[linesnumbered,boxed]{algorithm2e}
 \usepackage{amsthm}
\usepackage{graphicx}
      \usepackage{epstopdf}
\usepackage{cite}
\usepackage[colorlinks=true,
            citecolor=red,
            linkcolor=blue,
           anchorcolor=green,
            urlcolor=red]{hyperref}

\newtheorem*{Lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\sfa}{\mathsf{a}}
\newcommand{\sfb}{\mathsf{b}}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}



\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Optimization for Speculative Execution of Multiple Jobs in a MapReduce-like Cluster}

\author{Huanle XU, Wing Cheong LAU \\ Department of Information Engineering, The Chinese University of Hong Kong\\\{xh112, wclau\}@ie.cuhk.edu.hk \\
}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
Nowadays, a computing cluster in a typical data center can easily consist of hundreds of thousands of commodity servers, making component/ machine failures the norm rather than exception. A parallel processing
job can be delayed substantially as long as one of its many tasks is being assigned to a failing machine.
To tackle this so-called straggler problem, most parallel processing frameworks such as MapReduce have adopted various strategies under which the system may speculatively launch additional copies of the same task if its progress is abnormally slow or simply because extra idling resource is available.  In this paper, we focus on the design  of  speculative execution schemes for a parallel processing cluster under different loading conditions.
For the lightly loaded case, we analyze and propose two optimization-based schemes, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing the job utility and the Straggler Detection Algorithm (SDA) which minimizes the overall resource consumption of a job. We also derive the workload threshold under which SCA or SDA should be used for speculative execution. Our simulation results show both SCA and SDA can reduce the job \textit{flowtime} by nearly  comparing to the speculative execution strategy of Microsoft Mantri.
For the heavily loaded case, we propose the Enhanced Speculative Execution (ESE) algorithm which is an extension of the Microsoft Mantri scheme. We show that the ESE algorithm can beat the Mantri baseline scheme by  in terms of job \textit{flowtime} while consuming the same amount of resource.





\end{abstract}


\begin{keywords}
Job scheduling, speculative execution, cloning, straggler detection, optimization
\end{keywords}




\section{Introduction}
\label{Introduction}
Empirical performance studies of large-scale computing clusters have indicated that the completion time of a job {\cite{Outliers}}
is often significantly and unnecessarily prolonged by one or a few so-called ``straggler'' (or outlier) tasks, i.e. tasks which are unfortunately assigned to either a failing or overloaded computing node within a cluster.
As such, recent parallel processing frameworks such as the MapReduce system or its many variants have adopted various preventive or reactive straggler-handling strategies under which the system may automatically launch extra (backup) copies of a task on alternative machines in a judicious manner. Unfortunately, most of the existing speculative execution schemes are based on simple heuristics. In particular, there are two main classes of speculative execution strategies, namely, the Cloning {\cite{Cloning}} approach and Straggler-Detection-based one {\cite{Outliers}}, {\cite{Smart_Speculative}}, {\cite{mapreduce:google}}, {\cite{Dryad}}, {\cite{ESAMR}}, {\cite{Performance}}. Under the Cloning approach, extra copies of a task are scheduled in parallel with the initial task as long as the resource consumption of the task is expected to be low and there is system resource available. For the Straggler-Detection-based approach, the progress of each task is monitored by the system and backup copies are launched when a straggler is detected.
As one may expect, the cloning-based strategy is only suitable for a lightly loaded cluster as it launches the clones in a greedy, indiscriminately fashion. On the other hand,  the straggler-detection based strategy is applicable to both lightly-loaded and heavily-loaded regimes but at the expense of extra system instrumentation and performance overhead. The situation is particularly challenging when the progress of a large number of tasks have to be tracked.

In this paper, we take a more systematic, optimization-based approach for the design and analysis of speculative execution schemes. In particular, we have made the following technical contributions:
\begin{itemize}
\item After reviewing related work in Section \ref{related_work}, we introduce the system model in Section \ref{system_model} and derive the cut-off workload threshold between the
lightly-loaded and heavily-loaded operating regimes of a computing cluster. Based on this workload threshold,
the applicability of a speculative execution strategy can be analyzed for different operating regimes.

\item In Section \ref{SCA_design}, we introduce a generalized cloning-based framework which can jointly optimize the job utility with resource consumption when the cluster is lightly loaded. We also present a specific Smart Cloning Algorithm (SCA) based on this framework.

\item In Section \ref{SDA_design}, we consider the optimal online-scheduling framework and design the Straggler Detection Algorithm (SDA)  which launches an optimal number of extra copies for a straggler task in an on-demand basis, i.e. only after the straggler has been detected. In particular, we show that SDA can minimize overall resource consumption of the arriving jobs in a lightly loaded cluster.

\item In Section \ref{ESE_design}, we propose the Enhanced Speculative Execution (ESE) algorithm for a heavily loaded cluster by extending the speculative execution strategy of Microsoft Mantri~\cite{Outliers}. We demonstrate
that ESE can improve the job completion time of Mantri while consuming the same amount of resource. We also summarize our findings and conclude the paper in Section \ref{conclusions}.

\end{itemize}

\section{Related work}
\label{related_work}
Several speculative execution strategies have been proposed in the literature for the MapReduce system and its variants or derivatives. The initial Google MapReduce system only begins to launch backup tasks  when a job is close to completion. It has been shown that speculative execution can decrease the job execution time by 44\% {\cite{mapreduce:google}}.
The speculative execution strategies in the initial versions of  Hadoop {\cite{hadoop}} and Microsoft Dryad {\cite{Dryad}} closely follow that of the Google MapReduce system. However, the research group from Berkeley presents a new strategy called LATE (\emph{Longest Approximate Time to End}) {\cite{Performance}} in the Hadoop-0.21 implementation. It monitors the progress rate of each task and estimates their remaining time. Tasks with progress rate below certain threshold (\textit{slowTaskTherehold}) are chosen as backup candidates and the one with the longest remaining time is given the highest priority.  The system also imposes a limit on the maximum  number of backup tasks in the cluster\textit{speculativeCap}. Microsoft Mantri {\cite{Outliers}} proposes a new speculative execution strategy for Dryad in which the system estimates  the remaining time to finish, , for each task and predicts the required execution time of a relaunched copy of the task, . Once a computing node becomes available, the Mantri system makes a decision on whether to launch a backup task based on the statistics of  and . Specifically, a duplicate is scheduled if  is satisfied where
 the default value of . Hence, Mantri schedules a duplicate only if the total resource consumption is expected to decrease. Mantri also may terminate a task which shows an excessively large remaining-time-to-finish.

To accurately and promptly identify stragglers, {\cite{Smart_Speculative}} proposes a Smart Speculative Execution strategy and {\cite{ESAMR}} presents an Enhanced Self-Adaptive MapReduce Scheduling Algorithm respectively. The main ideas of {\cite{Smart_Speculative}} include: i) use exponentially weighted moving average to predict process speed and compute the remaining time of a task and  ii) determine which task to backup based on the load of a cluster using a cost-benefit model. Recently, {\cite{Cloning}} proposes  to mitigate the straggler problem by cloning every small job and avoid the extra delay caused by the straggler monitoring/ detection process.
When the majority of the jobs in the system are small, the cloned copies only consume a small amount of additional resource.


\section{System Model}
\label{system_model}
Assume a set of jobs   arriving at a computing cluster at a rate of  jobs per unit time. Different jobs may run different applications and a particular job  which arrives at the cluster at time  consists of  tasks. This cluster has  computing nodes (machine) and each computing node can only hold one task at any time. For simplicity, we assume this cluster is homogeneous in the sense that all the nodes are identical. Further, we assume that the execution time (i.e. the time between the task is launched and the task is finished) of each task of   without any speculative execution follows the same distribution, i.e.,  for . We assume the execution time distribution information can be estimated for each job according to prior trace data of the application it runs and the size of the input data to be processed. Upon arrival, each job joins a queue in the master-node of the cluster, waiting to be scheduled for execution according to some priorities to be determined in the following sections.

Here, we define the job \textit{flowtime} which is an important metric we capture as below.
\begin{definition}
The \textit{flowtime} of a job  is , where  and  denote the finish (completion) time and arrive time respectively.
\end{definition}


If a task  runs  units of time on a computing node, then it consumes  units of resource on this node where  is a constant number. We ignore the resource consumption of an idle machine. For ease of description, we often interchange the two notations in this paper, namely, machine and computing node.

\subsection{Speculative execution under different operating regimes}
The cloning-based strategy for speculative execution schedules extra copies of a task in parallel with the initial task as long as the resource consumption of the task is expected to be low and there is system resource available. Only the result of one which finishes first among all the copies is used for the subsequent computation. Cloning does not incur any monitoring overhead. Nevertheless, cloning consumes a large amount of resource and can easily block the scheduling of subsequent jobs when the cluster workload is heavy.


On the other hand, the Straggler-Detection-based approach makes a speculative copy for the task after a straggler is detected. In this approach, the scheduler needs to monitor the progress for each task.  However, the monitoring incurs extra system instrumentation and performance overhead as discussed in {\cite{Monitoring}}. The situation is particularly challenging when the progress of a large number of tasks have to be tracked.  To make things even worse, it's always difficult to detect a straggler for small jobs as they usually complete their work in a very short period {\cite{Cloning}}.

As one may expect, the cloning-based strategy is only suitable for a lightly loaded cluster as it launches the clones in a greedy, indiscriminately fashion. On the other hand,  the straggler-detection based strategy is applicable to both lightly-loaded and heavily-loaded regimes. Hence, there exists a cutoff threshold to separate the cluster workload into these two operating regimes. When the workload is below this threshold, the cloning-based strategy can obtain a good performance in terms of job \textit{flowtime}.  Conversely, when the workload exceeds this threshold, only the Straggler-Detection-based strategy can help to improve the cluster performance.

\subsection{Deriving the cutoff threshold for different regimes}
In this subsection, we derive the cutoff workload threshold  which allows us to separate our subsequent analysis into the lightly loaded vs. heavily loaded regimes.  We assume that the random variables  and  are independent from each other for all  and . To simplify the analysis, we focus on the task delay in the cluster instead of job delay. Denote by  the task arrival rate to the whole cluster. Thus, . We approximate the task arrival as a Poisson process with rate . Further, denote by  the average task arrival rate to each single machine which is given by .

We model the task service process of each computing node (machine) as a  queue. Applying the result of {\cite{queueing_theory}}, we get the average delay of each task without speculative execution made in the following equation

where  is the average duration of all the tasks in the system without speculative execution implemented.

We proceed to derive the expression of the task delay for the cloning-based strategy. Different from {\cite{Cloning}} where the cloning is done for small jobs only, here we consider a more general cloning scheme in which the small jobs are not distinguished from the big ones. In this scheme, each task should at least make two copies. Otherwise, the task which does not have any extra copy may delay the completion of the entire job.

We illustrate an example in which  (for all ) follows the Pareto Distribution as follows: 
Assume  () copies are launched for a particular task . Then the expected duration for task  is
.
Thus, . This also gives a lower-bound of  the performance improvement of cloning regardless of the number of extra copies to be made for each task.

Denote by  the average number of copies each task makes. Hence, . Further define  and  as the average task duration and equivalent task arrival rate to each machine respectively after the cloning is made.

The first constraint for cloning is that it must not overload the system, i.e. the long-term system utilization of the cluster should be less than 1. Thus, the following inequality holds:

 By considering the constraint  in Eq.(\ref{block}), and the fact that , we have:

\begin{Theorem}
The condition  is necessary to guarantee that the cloning does not overload the system.
\label{threshold}
\end{Theorem}
\begin{proof}
Refer to the technical report \cite{speculative-multi-job}.
\end{proof}
However, the efficiency of cloning is not guaranteed by Theorem \ref{threshold}. An efficient cloning strategy should have a smaller task delay than a strategy which does not make speculative execution. This argument must also hold when each task has only two copies. Denote by  the average task delay when each task has two copies. For convenience, we define . Thus,

and

Combine (\ref{delay}), (\ref{delay_clone}) and (\ref{block_condition}), we can derive the upper bound
  for . Hence, the cutoff threshold is determined by the following equation:

In the following sections, we continue to introduce the cloning-based strategy and Straggler-Detection-based approaches under two different workload regimes. 


\section{Optimal Cloning in the lightly loaded regime}
\label{SCA_design}
In the lightly loaded cluster, i.e., , we first apply the generalized cloning-based scheme to improve the job performance.

We consider that time is slotted and the scheduling decisions are made at the beginning of  each time slot. Assume  consisting of  tasks which are from the set  and  is scheduled at time slot . Denote by  the scheduling time of job . Hence,  and  satisfy the following constraints:


Each task of  can maintain different number of duplicates as the tasks in the same job may be scheduled at different time slots depending on server availability. Denote by  the number of copies made for task . Further let  define the duration of the th clone for task . We assume  follows the same distribution as  and  all the   are i.i.d random variables for .  Define  as the duration of task  and  as the \textit{flowtime} of job  respectively.  Then the following two equations hold:


Equation (\ref{duration_task}) states that as soon as one copy of task  finishes, the task completes. Equation (\ref{duration_job}) describes the job \textit{flowtime}.

We define a utility for each job which is a function of job \textit{flowtime} and the number of tasks it maintains. The formulation (P1) is as follows:


In this formulation, the utility of job ,  is a strictly concave  and differentiable function of   and . Our objective is to maximize the total utility of all the jobs in the cluster while keeping a low resource consumption level. The first constraint states that the total number of  tasks including all task copies at any time slot is no more than  and the second constraint states that each individual task can at most maintain  copies in the cluster.

\subsection{Solving P1 through approximation}
\label{p1_solve}
P1 is an online stochastic optimization problem and the scheduling decisions should be made without knowing the information of future jobs. In Equation (\ref{duration_job}), the tasks of the same job can be scheduled in different time slots. Hence, it is not easy to express the job \textit{flowtime} in terms of the distribution function and thus makes P1 difficult to solve.

Due to the fact that the cluster is lightly loaded, there is a large room for making clones for all the jobs most of the time. Thus, we solve another optimization problem (P2) as a relaxation for P1 when system resource is available. In P2, all the tasks of the same job are scheduled together and maintain the same number of copies. In this way, we can simplify the modeling of the job \textit{flowtime}.

Define  as the job set which contains all unscheduled jobs at time slot . Assume . If there is enough idling servers to schedule the jobs in the cluster at the beginning of time slot , i.e.,  where  is number of available machines, we solve P2 to determine the number of copies for each task in  as below:


Here,  is the number of duplicates assigned to each task in job . Solving P2 is much easier and it only depends on the current information. Define  as the cumulative distribution function of  and we have:

Let  and the distribution function of  is given by:

Further,  and we get

Similarly,
, which yields:


\begin{lemma}
 is a convex function of  when provided  is a convex function of t.
\end{lemma}

\begin{proof}
Refer to \cite{speculative-multi-job}.
\end{proof}
\vspace{.5em}
In the same way,  is also a convex function of  which decreases as  increases.

In traditional scheduling problems, minimizing the job \textit{flowtime} is a common objective. Thus, we consider to minimize the summation of job \textit{flowtime} and resource consumption as a special example, i.e., 

Observe that the first two constraints in P2 are linear. Hence, we can adopt the convex optimization technique to solve P2. The Lagrangian Dual problem of P2 is given by:



where , ,  are nonnegative multipliers. Applying the result of convex optimization, we conclude that there is no duality gap between P2 and D.

We adopt the gradient projection algorithm to get the optimal solution of D.
Define the vector  and the algorithm is outlined below:
\begin{itemize}
\item Initialize  for all , , , .
\item ;
\item ;
\item ;
\item ;
\item if , the gradient algorithm terminates.
\end{itemize}
where  denote the values of the corresponding parameters
during the th iteration of the algorithm.

Next, we proceed to prove the convergence of this algorithm by adopting the method of Lyapunov stability theory {\cite{Control}}. We first define the following Lyapunov function:
\begin{small}

\end{small}
where ,   and  is the optimal solution of D. It can be shown that  is positive definite. (Refer to \cite{speculative-multi-job} for the details of the proof.)

Denote by  the derivative of  with respect to time. With the following lemma, we get .
\begin{lemma}
The subgradient of  at  is given by:

Similarly, the subgradient of  and ,  are:

respectively where  minimizes .
\end{lemma}

\begin{proof}
Refer to \cite{speculative-multi-job} for details.
\end{proof}

\begin{Theorem}
\label{Theorem 1}
When the step size  are positive, the above gradient projection algorithm can converge to the global optimal.
\end{Theorem}

\begin{proof}
First, the trajectories of V converge to the set  according to the Lasalle's Principle {\cite{Control}}.  Then, based on the proof of  in \cite{speculative-multi-job}, for all ,  which is the optimal value. Thus, all the elements in  must be global optimal solutions. Hence, we conclude that the gradient projection algorithm converges to the global optimal.
\end{proof}
Not only that the algorithm is guaranteed to converge to the global optimal, we can further tune its parameters to speed up the convergence in an actual implementation.
Fig.~\ref{converge} depicts the results of a matlab-based simulation experiment to demonstrate the fast convergence rate of the gradient projection algorithm.  In this experiment, we assume that, in a particular time slot , there are 4 jobs waiting to be scheduled, i.e., .  The cluster has 100 available machines and the number of tasks for each job are 10, 20, 5 and 10 respectively. Assume  to follow the Pareto Distribution where  for  and  and the number of copies for each task is given by . We tune the parameters  to be 0.2, 0.3, 0.4 respectively. As shown in Fig.~\ref{converge}, this algorithm can converge very fast to the optimal solution.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{iteration-eps-converted-to.pdf}
\caption{The convergency performance of the gradient projection algorithm. The number of duplicates for the tasks in each job iterates and can converge to the optimal value.  represents the number of duplicates for each task in job  where .}
\label{converge}
\vspace{-0.2 cm}
\end{figure}

\subsection{The design of the Smart Cloning Algorithm (SCA)}
Following the analysis in subsection \ref{p1_solve}, there exists a case where there is no space for cloning the jobs at the beginning of a particular time slot. In this scenario, it does not make sense to solve P2. Instead,  we adopt a smallest remaining workload first scheme. It is well known in scheduling literature that the  Shortest Remaining Processing Time (SRPT) scheduler is optimal for overall flowtime on a single machine where there is one task per job. The SRPT-based approach has been adopted widely for the scheduling in a parallel system as presented in {\cite{Fast_Completion}}{\cite{Joint_Phase}}, {\cite{Flow_Shops}} {\cite{Delay_Tails}} {\cite{Data_Locality}} {\cite{Degree_Locality}}, {\cite{Schedulers}}. Based on P2 and SRPT, we propose the Smart Cloning Algorithm (SCA) below.

SCA consists of  two separate parts. At the beginning of each time slot, we first schedule the remaining tasks of the unfinished jobs and then check whether the condition  is satisfied. If the condition is satisfied, we solve  P2 to determine the number of clones for each task. Otherwise, i.e. ,  we sort  according to the increasing order of the workload in each  and schedule the jobs based on this order and only one copy of each task is created.
Notice that the resultant workload is the product of  and . The corresponding pseudo-code is given in Algorithm \ref{SCA_code} as below.


\IncMargin{1em}
\begin{algorithm}
\label{SCA_code}
\caption{Smart Cloning Algorithm}
\Indm
\KwIn{The jobs in the cluster associated with their running status at time slot ;}
\KwOut{Scheduling decisions for time slot .}
\Indp
schedule the unassigned tasks of the running jobs in the cluster with the fewest remaining first\;
update  and \;

\If{ }
{
return;
}

\eIf{}
{
solve P2 and assign duplicates of the tasks in  based on the optimization result;
}
{
\For{Job  in }
{
assign only one copy for each task of \;
update \;

\If{}
{
return;
}
}
}

return;
\end{algorithm}
\DecMargin{1em}
\vspace{-0.3em}

\subsection{Performance evaluation for SCA}
We run a Matlab-based simulation to evaluate the performance of SCA. In the simulation, the jobs arrive at the computing cluster following a Poisson process with rate . There are  machines in the cluster. The number of tasks in each job is uniformly distributed from 1 to 100. Within a job, the duration of each task follows a common Pareto distribution with a heavy tail order of 2. The expected task duration for different jobs follows a uniform distribution of 1 to 4 units of time.
The resource consumption parameter  is set to be 0.01.
The simulation is run for 1500 units of time and repeated using 3 different seeds.

We use the speculative execution strategy of Microsoft Mantri as the baseline and compare it with the proposed SCA. We use job \textit{flowtime} and job resource consumption as the performance metrics for comparison. Fig.~\ref{SDA} depicts the cumulative density function of job \textit{flowtime} and resource consumption for nearly 27000 jobs where the solid line represents the results of SCA.
Observe from the figure that the average job \textit{flowtime} reduces by  in our algorithm compared to the baseline. It is worthnoting that, under SCA, more than  () of jobs can finish within 6 (9) time units respectively.  As a comparison, about   () of jobs can finish within 17 (25) time units  for the Mantri algorithm. However, our algorithm consumes more resource than the Mantri one. It indicates that  of jobs consume less than 1.5 units resource in the Mantri baseline while  of jobs consume less than 2 units resource in SCA. Recall that the objective of Smart Cloning Algorithm is to maximize the difference of job utility and resource consumption. To further make a fair comparison between these two schemes, we  consider the job utility minus total resource consumption as an additional performance metric. Our simulation results show that SCA can beat the baseline considerably in terms of this metric.


\section{Design of optimal Straggler-Detection-based scheme for the lightly loaded regime}
\label{SDA_design}
As analyzed in Section \ref{system_model}, both the cloning and straggler detection approach can improve the system performance in the lightly loaded regime. However, it is still unknown which one has a better performance. In this section, we formulate a different model to design speculative executions based on individual-task-progress monitoring. Different from the previous Straggler-Detection-based approaches which only duplicate one copy at most for each straggler, this model can automatically determines the optimal number of duplicates when a straggler is detected.

We use the same notations as in Section \ref{SCA_design} except for .  Based on the monitoring result, the scheduler only begins to make speculative copies for task  if a straggler is detected, i.e.  when the progress of  substantially falls behind others. To be more specific,
a task is declared to be a straggler if its estimated remaining time to finish (  ) is
greater than  where  is the expected execution time of a new copy. In this model, different tasks in the same job can maintain different number of duplicates based on their task progress. We define  as the number of copies running for task  at time .
Under this setup,   is a parameter to be optimized for all . Intuitively, if  is too small, a lot of running tasks will be characterized as stragglers. This can incur a large number of duplicates and consume a lot of resource in the cluster. On the other hand, if  is set to be too large, many jobs will be delayed due to a small number of  slow-progressings tasks  as the speculative copies of tasks are not launched.


To model the monitoring progress, we assume that the scheduler can detect the straggler for task  after it completes a portion of the work which is . Here,  is a constant number related to job . Thus,  needs to satisfy the following two constraints:
 


 Further define  and we assume all the  are i.i.d random variables for . In this model, the duplication of a particular task is made only once. Thus, the following equation holds:

In Section \ref{SCA_design}, we derive the distribution function of job \textit{flowtime} by letting all the tasks of jobs being launched simultaneously. However, it is impossible to schedule the jobs in the same manner for the detection based model as stragglers can only be detected after the tasks have run for some time. In this way, it is difficult to simplify the expression for the job \textit{flowtime}. As such,  we choose to only optimize the total resource consumption of the job which yields the following formulation (P3):

Notice that P3 is a stochastic programming problem and we need to find the optimal solutions for  and .

\subsection{Solving P3 through decomposition}
The objective in this formulation can be decoupled as the summation of the expected resource consumption for each individual task. However, the first constraint makes this problem difficult to solve as it cannot be decoupled. We first relax this constraint and minimize the expected resource consumption for an individual task. The constraint will be taken into account later on when we design the actual scheduling algorithm.

Define  if  and  if .  Let . It can be readily shown that

Denote by  the event that a straggler is detected for task  and  the event that  respectively. Then, we have

Further, denote by    the event that . Thus,


Define , then

Combine Equality (\ref{all}), (\ref{condition1}), (\ref{condition2})

For a fixed , the optimal value of  is a function of  and is determined by the following equation:

And the optimal value of  is determined by the following equation:

For the detailed steps, please refer to \cite{speculative-multi-job}.
Define . We conclude that, if  follows the pareto distribution, then  is an increasing function of  when  for all  where  is the heavy-tail order. Moreover, we can choose an appropriate  such that . For the detailed proof, please refer to the technical report. Based on this conclusion, we derive the optimal solution of  for P3 in the following theorem:
\begin{Theorem}
\label{Theorem 2}
The optimal value for  is 2 once a straggler is detected under the Pareto heavy-tail distribution. Moreover, the optimal value for  does not depend on  or   but  the heavy-tail order of the Pareto distribution.
\label{optimal_c_i_j}
\end{Theorem}
\begin{proof}
Refer to \cite{speculative-multi-job}.
\end{proof}


\subsection{The Design of  the Straggler Detection Algorithm (SDA) based on P3}
Based on the solution of P3, we propose the \textit{Straggler Detection Algorithm} (SDA) to optimize the resource consumption for stragglers. SDA monitors the progress for each task running in the cluster. When a new job  arrives, the optimal value for  and  are computed based on Equation (\ref{optimal_c}), (\ref{optimal_sigma}).

SDA consists of three scheduling levels. In the  first level, if the task  is suffering from a straggler, the duplicates of  are assigned to alternative machines. The policy is that for the straggler  , the scheduler duplicates  new copies of it on other machines if resource is available and the machine is randomly chosen from any available ones. If the number of idle machine is less than , then all of these idle machines will be assigned a duplicate of . In the second level, the remaining tasks of the jobs which have begun but not finished yet are scheduled at the beginning of each time slot. The job with the smallest remaining workload has the highest priority. In the third level, the jobs have not begun their work yet will be scheduled in the cluster and among those which has the smallest workload will be scheduled first. Similarly, denote by  the set of unscheduled jobs at time slot  and it's sorted in the same way as SCA. The scheduler assigns only one copy for each task in the newly scheduled job from .


\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{SC_SD-eps-converted-to.pdf}
\caption{The comparison between our proposed SCA and SDA, the Microsoft Mantri's speculative execution algorithm is adopted as the baseline.  Panel a shows the cmf of Job \textit{flowtime} and Panel b shows the cmf of total job resource consumption. }
\label{SDA}
\vspace{-0.2 cm}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Com_sig-eps-converted-to.pdf}
\caption{The comparison between different  in the SDA algorithm. It indicates that when , both the job \textit{flowtime} and resource consumption achieves the optimal.}
\label{comparison}
\vspace{-0.2 cm}
\end{figure}

\subsection{Performance evaluation for SDA}
We run another simulation to evaluate the performance of SDA. The simulation parameters are completely the same as SCA. For , we apply Theorem \ref{optimal_c_i_j} and Equation (\ref{optimal_sigma}) to obtain the optimal value for  which is . The comparison between SDA and the baseline algorithm is shown in Fig.~\ref{SDA}. It shows that both the job \textit{flowtime} and resource consumption of SDA perform better than the baseline though the objective of SDA is to optimize the resource consumption only. Fig.~\ref{comparison} shows the comparison result for different . It indicates that when , both the job \textit{flowtime} and resource consumption achieve the best performance. Moreover, when  decreases below , the total resource consumption increases. The job \textit{flowtime} increases when  increases above  because the speculative copy is made late and cannot help much for reducing the job \textit{flowtime}. Compared to SDA, SCA performs better in terms of \textit{flowtime} for small jobs. However, the average job \textit{flowtime} of SCA is roughly the same while SCA consumes more resource. It is unfair to conclude that the SDA outperforms SCA. The reason is that SDA monitors the progress of each task and only makes the duplicates for the stragglers. However, SCA doesn't depend on the task progress and makes the cloning in a greedy fashion. Due to this reason, SCA tends to consume more resource than SDA. To determine which algorithm is better in real systems, the monitoring cost should be taken into account.

\section{Design of Straggler-Detection-based algorithm for the heavily loaded regime}
\label{ESE_design}
In the heavily loaded cluster, i.e., , the cloning-based scheme cannot be applied and only the Straggler-Detection-based approach is efficient.  However, the SDA algorithm implemented in lightly loaded cluster launches the duplicates immediately when a straggler is detected. In the heavily loaded cluster, the resource is intensive and the scheduler should wait for the available machines. As a result, on-demand scheduling for stragglers is not always possible. In the literature, Microsoft Mantri {\cite{Outliers}} chooses to schedule a speculative copy if  is satisfied when there are available machines. We extend this scheme and propose the \textit{Enhanced Speculative Execution} (ESE) algorithm. The scheduling decision is made in each time slot and we also leave some space for cloning the small jobs. Usually the small job represents for interactive applications like query and have very strict latency requirement. When the workload is heavy, the probability of making speculative copies for the tasks of small jobs is low in Mantri's scheduling algorithm. Cloning for small jobs only incurs a small amount of resource consumption while contributing a lot to the job \textit{flowtime}.

\subsection{The Enhanced Speculative Execution algorithm}
The ESE algorithm also includes three scheduling levels. Explicitly, at the beginning of time slot ,  the scheduler estimates the remaining time of each running task and puts the tasks whose remaining time satisfy a particular condition into the backup candidate set . Define  as the remaining time of task  at the beginning of time slot . Then,

All the tasks in  are sorted according to the decreasing order of   and the scheduler assigns  a duplicate of each task  in  based on this order.  In the same way as P3, we also need to find an appropriate value for  that the expected resource consumption for a single task is minimized.


The scheduler then assigns the remaining tasks of the jobs which have already been scheduled  but have not quitted the cluster yet. Denote by  the unfinished job set at time slot  and the jobs are sorted based on the remaining workloads. Upon scheduling, the jobs which have smaller remaining workload are given the higher priorities.


 is updated after the above scheduling. If there are still available machines and some jobs waiting to be scheduled, the scheduler then tries to do the cloning for small jobs and assign the number of copies which can maximize the difference between the job utility and total resource consumption. For big jobs, no cloning will be made. More precisely, denote by  the job set which contains all the jobs have not been scheduled yet in the cluster. The jobs in  are sorted according to the increasing order of workloads. For small job   which satisfies  and  in , the optimal number of copies cloned for task in  is determined by the following equation.

The parameters  and  can be tuned based on the cluster workload. Here, , ,  and  are the same as Section \ref{SCA_design} and  is equal to  for all .

The corresponding pseudocode is given in Algorithm \ref{ESE_code} in the below.

\IncMargin{1em}
\begin{algorithm}
\label{ESE_code}
\caption{Enhanced Speculative Execution Algorithm}
\Indm
\KwIn{The jobs in the cluster associated with their running status at time slot ;}
\KwOut{Scheduling decisions for time slot .}
\Indp
Count , the number of idle machines at time slot  and update , , .

\For{the task  in  in }
{
Assign a duplicate of   on a random idle machine\;
 -= 1\;

\If{ }
{
return;
}
}
\For{the job  in }
{
Assign the unscheduled tasks of  on idle machines\;
update \;

\If{ }
{
return;
}
}
\For{the job  in }
{
\eIf{ \& }
{
Compute  base on Equation (\ref{clone_small_job})\;
Assign  duplicates for each task in \;
update \;
}
{
Assign one duplicate for each task in \;
update \;
}
\If{ }
{
return;
}
}
return;
\end{algorithm}
\vspace{-0.3em}
\DecMargin{1em}


\subsection{Approximate Analysis for  in ESE}
 has a great impact on the system performance. In the following analysis,  we aim to find an appropriate   in the ESE algorithm through minimizing the expected resource consumption for one particular task.

Define the random variable  as the resource that task  consumes in the cluster. For convenience, we take  and  denote by  the event that . Then the expectation of   can be expressed in the following equation.

Moreover, we have


We give the following definition which helps to derive the expression of .
\begin{definition}
The \textit{asktime} of a running task  is the earliest time that the scheduler checks whether it should duplicate a new copy for task  on available machines or not.
\end{definition}

Assume the interval of each time slot is short enough, then the \textit{asktime} of  can be treated as uniformly distributed in the interval [0, ] due to the cluster is heavily loaded. At this \textit{asktime}, the scheduler assigns a new copy for  if  is satisfied. Hence,

In Equation (\ref{expected3}),  is given by:

Combining Equation (\ref{expected1}),(\ref{expected2}),(\ref{expected3}),(\ref{expected4}),  can be determined and it's a function of . We obtain the optimal value of  that minimizes the expected resource consumption through letting the derivative of  be 0, .

We illustrate the picture of   in Fig.~\ref{sigma2} under different  for the pareto distribution.   is the heavy-tail order and  is the optimal value. It indicates that when  is close to 1.7 where  is 2,  achieves the minimum value. We also compare the different optimal value for  when the heavy-tail order changes. As shown in Fig.~\ref{sigma2},   increases along with . Moreover, for all ,  is very close to 2.0.

\begin{figure}
\centering
\includegraphics[height=15em,width=0.5\textwidth]{sigma2-eps-converted-to.pdf}
\caption{The illustration of  under different  for pareto distribution when  .}
\label{sigma2}
\vspace{-0.3 cm}
\end{figure}

\subsection{Performance Evaluation for ESE}
In this subsection, we first conduct one  simulation to  show the impact of  and the heavy-tail order on the cluster performance  to demonstrate the fitness of our approximate analysis. Then  we continue to compare the performance of ESE algorithm and Mantri's speculative execution strategy.

\vspace{0.2em}
\subsubsection{The impact of  and }
In this simulation, there is only one job which consists of  tasks and the cluster has  computing nodes. The expected task duration is 1 and the other parameters are as same as the simulation for SCA. For each  we run 50 simulations and take the average. We adopt the naive scheme in which speculative execution is not implemented as a comparison to the ESE algorithm. The simulation results are illustrated in Fig.~\ref{single}. It shows that when  is close to  for , both the total amount of resource consumption and job \textit{flowtime} achieve the minimum. Next, we keep the same expected task duration and take  to show the impact of heavy-tail order on the optimal value of .  The result matches well with the theoretical analysis in Fig.~\ref{sigma2}. It also indicates that as  increases, the performance improvement of the ESE algorithm tends to be inconspicuous for a single job case.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{u1-eps-converted-to.pdf}
\caption{Simulation result for a single job. Panel a shows the comparison of resource consumption under different  between ESE algorithm and the naive method without backup. The blue line represents the ESE algorithm, the dark green line describes the result of the approximate analysis while the pink red line represents the method without backup. Panel b shows the comparison of job \textit{flowtime} time. }
\label{single}
\vspace{-0.3 cm}
\end{figure}


\subsubsection{The performance of ESE Algorithm}
In this simulation, there are  machines in the cluster. The simulation parameters are the the same as the SCA and SDA algorithms except for the job arrival rate. Here, we adopt two arrival rates which are 30 and 40 jobs per unit time. We follow the approximate analysis and choose  to be 1.7.  and  are set to be 0.1 and 1 respectively. Similarly, we use the Microsoft Mantri's algorithm as the baseline to compare it with the ESE algorithm. The results are illustrated in Fig.~\ref{ESE_40}. As shown, when the job arrival rate is 40,  of jobs can finish within 10 units time in the ESE algorithm while  of jobs can finish only within 18 units time in the baseline. Further, we observe that the average job \textit{flowtime} reduces by  in the ESE algorithm compared to the baseline. However, the resource consumption for these two algorithms are roughly the same. This is because cloning for small jobs can incur an extra amount of resource consumption. When the job arrival rate  is 30, the job resource consumption for ESE algorithm reduces a lot compared to the baseline. We also apply the SCA and SDA algorithms to the heavily loaded cluster but the performance turns out to be poor. As the cluster is heavily loaded, the cloning in SCA easily blocks the scheduling of newly arriving jobs. Due to resource is intensive, there is a little chance to make on-demand scheduling for stragglers in SDA and thus can not help to improve the performance.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{ESE-eps-converted-to.pdf}
\caption{The comparison between our proposed ESE Algorithm and the Microsoft Mantri's speculative execution algorithm.  The job arrival rate is 40. Panel a shows the cmf of Job \textit{flowtime} and Panel b shows the cmf of total job resource consumption.}
\label{ESE_40}
\vspace{-0.3 cm}
\end{figure}
\section{Conclusions}
\label{conclusions}
In this paper, we address the speculative execution issue in the parallel computing cluster and focus on two metrics which are job \textit{flowtime} and resource consumption. We utilize the distribution information of task duration and build a theoretical framework for making speculative copies. We categorize the cluster into lightly loaded and heavily loaded cases and derive the cutoff threshold for these two operating regimes. Moreover, we propose two different strategies when the cluster is lightly loaded and design the ESE algorithm while the cluster is heavily loaded. This is the first work so far to adopt the optimization-based approach for speculative execution in a multiple-job cluster. In the future work, we will consider to make speculative execution for the cluster where there exists task dependency in a job. Like the MapReduce framework, any reduce task can only begin after the map tasks finish within a job.

\bibliographystyle{abbrv}
\bibliography{Multi-job-scheduling}

\end{document}
