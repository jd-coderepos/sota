\documentclass[10pt,conference]{IEEEtran}


\usepackage{times}
\usepackage{amssymb}
 \usepackage{amsmath,bbm}
 \usepackage[linesnumbered,boxed]{algorithm2e}
 \usepackage{amsthm}
\usepackage{graphicx}
      \usepackage{epstopdf}
\usepackage{cite}
\usepackage[colorlinks=true,
            citecolor=red,
            linkcolor=blue,
           anchorcolor=green,
            urlcolor=red]{hyperref}

\newtheorem*{Lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\sfa}{\mathsf{a}}
\newcommand{\sfb}{\mathsf{b}}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}



\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Optimization for Speculative Execution of Multiple Jobs in a MapReduce-like Cluster}

\author{Huanle XU, Wing Cheong LAU \\ Department of Information Engineering, The Chinese University of Hong Kong\\\{xh112, wclau\}@ie.cuhk.edu.hk \\
}

\maketitle

\vspace{-0.5cm}

\begin{abstract}
Nowadays, a computing cluster in a typical data center can easily consist of hundreds of thousands of commodity servers, making component/ machine failures the norm rather than exception. A parallel processing
job can be delayed substantially as long as one of its many tasks is being assigned to a failing machine.
To tackle this so-called straggler problem, most parallel processing frameworks such as MapReduce have adopted various strategies under which the system may speculatively launch additional copies of the same task if its progress is abnormally slow or simply because extra idling resource is available.  In this paper, we focus on the design  of  speculative execution schemes for a parallel processing cluster under different loading conditions.
For the lightly loaded case, we analyze and propose two optimization-based schemes, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing the job utility and the Straggler Detection Algorithm (SDA) which minimizes the overall resource consumption of a job. We also derive the workload threshold under which SCA or SDA should be used for speculative execution. Our simulation results show both SCA and SDA can reduce the job \textit{flowtime} by nearly $60\%$ comparing to the speculative execution strategy of Microsoft Mantri.
For the heavily loaded case, we propose the Enhanced Speculative Execution (ESE) algorithm which is an extension of the Microsoft Mantri scheme. We show that the ESE algorithm can beat the Mantri baseline scheme by $18\%$ in terms of job \textit{flowtime} while consuming the same amount of resource.





\end{abstract}


\begin{keywords}
Job scheduling, speculative execution, cloning, straggler detection, optimization
\end{keywords}




\section{Introduction}
\label{Introduction}
Empirical performance studies of large-scale computing clusters have indicated that the completion time of a job {\cite{Outliers}}
is often significantly and unnecessarily prolonged by one or a few so-called ``straggler'' (or outlier) tasks, i.e. tasks which are unfortunately assigned to either a failing or overloaded computing node within a cluster.
As such, recent parallel processing frameworks such as the MapReduce system or its many variants have adopted various preventive or reactive straggler-handling strategies under which the system may automatically launch extra (backup) copies of a task on alternative machines in a judicious manner. Unfortunately, most of the existing speculative execution schemes are based on simple heuristics. In particular, there are two main classes of speculative execution strategies, namely, the Cloning {\cite{Cloning}} approach and Straggler-Detection-based one {\cite{Outliers}}, {\cite{Smart_Speculative}}, {\cite{mapreduce:google}}, {\cite{Dryad}}, {\cite{ESAMR}}, {\cite{Performance}}. Under the Cloning approach, extra copies of a task are scheduled in parallel with the initial task as long as the resource consumption of the task is expected to be low and there is system resource available. For the Straggler-Detection-based approach, the progress of each task is monitored by the system and backup copies are launched when a straggler is detected.
As one may expect, the cloning-based strategy is only suitable for a lightly loaded cluster as it launches the clones in a greedy, indiscriminately fashion. On the other hand,  the straggler-detection based strategy is applicable to both lightly-loaded and heavily-loaded regimes but at the expense of extra system instrumentation and performance overhead. The situation is particularly challenging when the progress of a large number of tasks have to be tracked.

In this paper, we take a more systematic, optimization-based approach for the design and analysis of speculative execution schemes. In particular, we have made the following technical contributions:
\begin{itemize}
\item After reviewing related work in Section \ref{related_work}, we introduce the system model in Section \ref{system_model} and derive the cut-off workload threshold between the
lightly-loaded and heavily-loaded operating regimes of a computing cluster. Based on this workload threshold,
the applicability of a speculative execution strategy can be analyzed for different operating regimes.

\item In Section \ref{SCA_design}, we introduce a generalized cloning-based framework which can jointly optimize the job utility with resource consumption when the cluster is lightly loaded. We also present a specific Smart Cloning Algorithm (SCA) based on this framework.

\item In Section \ref{SDA_design}, we consider the optimal online-scheduling framework and design the Straggler Detection Algorithm (SDA)  which launches an optimal number of extra copies for a straggler task in an on-demand basis, i.e. only after the straggler has been detected. In particular, we show that SDA can minimize overall resource consumption of the arriving jobs in a lightly loaded cluster.

\item In Section \ref{ESE_design}, we propose the Enhanced Speculative Execution (ESE) algorithm for a heavily loaded cluster by extending the speculative execution strategy of Microsoft Mantri~\cite{Outliers}. We demonstrate
that ESE can improve the job completion time of Mantri while consuming the same amount of resource. We also summarize our findings and conclude the paper in Section \ref{conclusions}.

\end{itemize}

\section{Related work}
\label{related_work}
Several speculative execution strategies have been proposed in the literature for the MapReduce system and its variants or derivatives. The initial Google MapReduce system only begins to launch backup tasks  when a job is close to completion. It has been shown that speculative execution can decrease the job execution time by 44\% {\cite{mapreduce:google}}.
The speculative execution strategies in the initial versions of  Hadoop {\cite{hadoop}} and Microsoft Dryad {\cite{Dryad}} closely follow that of the Google MapReduce system. However, the research group from Berkeley presents a new strategy called LATE (\emph{Longest Approximate Time to End}) {\cite{Performance}} in the Hadoop-0.21 implementation. It monitors the progress rate of each task and estimates their remaining time. Tasks with progress rate below certain threshold (\textit{slowTaskTherehold}) are chosen as backup candidates and the one with the longest remaining time is given the highest priority.  The system also imposes a limit on the maximum  number of backup tasks in the cluster\textit{speculativeCap}. Microsoft Mantri {\cite{Outliers}} proposes a new speculative execution strategy for Dryad in which the system estimates  the remaining time to finish, $t_{rem}$, for each task and predicts the required execution time of a relaunched copy of the task, $t_{new}$. Once a computing node becomes available, the Mantri system makes a decision on whether to launch a backup task based on the statistics of $t_{rem}$ and $t_{new}$. Specifically, a duplicate is scheduled if $\mathbb{P}(t_{rem}>2*t_{new}) > \delta$ is satisfied where
 the default value of $\delta = .25$. Hence, Mantri schedules a duplicate only if the total resource consumption is expected to decrease. Mantri also may terminate a task which shows an excessively large remaining-time-to-finish.

To accurately and promptly identify stragglers, {\cite{Smart_Speculative}} proposes a Smart Speculative Execution strategy and {\cite{ESAMR}} presents an Enhanced Self-Adaptive MapReduce Scheduling Algorithm respectively. The main ideas of {\cite{Smart_Speculative}} include: i) use exponentially weighted moving average to predict process speed and compute the remaining time of a task and  ii) determine which task to backup based on the load of a cluster using a cost-benefit model. Recently, {\cite{Cloning}} proposes  to mitigate the straggler problem by cloning every small job and avoid the extra delay caused by the straggler monitoring/ detection process.
When the majority of the jobs in the system are small, the cloned copies only consume a small amount of additional resource.


\section{System Model}
\label{system_model}
Assume a set of jobs  $J = \{J_1, J_2, \cdots\}$ arriving at a computing cluster at a rate of $\lambda$ jobs per unit time. Different jobs may run different applications and a particular job $J_i$ which arrives at the cluster at time $a_i$ consists of $m_i$ tasks. This cluster has $M$ computing nodes (machine) and each computing node can only hold one task at any time. For simplicity, we assume this cluster is homogeneous in the sense that all the nodes are identical. Further, we assume that the execution time (i.e. the time between the task is launched and the task is finished) of each task of $J_i$  without any speculative execution follows the same distribution, i.e., $x^i_j \thicksim F_i(t) = Pr(x^i_j < t)$ for $1 \leq  j \leq m_i$. We assume the execution time distribution information can be estimated for each job according to prior trace data of the application it runs and the size of the input data to be processed. Upon arrival, each job joins a queue in the master-node of the cluster, waiting to be scheduled for execution according to some priorities to be determined in the following sections.

Here, we define the job \textit{flowtime} which is an important metric we capture as below.
\begin{definition}
The \textit{flowtime} of a job $J$ is $flow(J_i)=f(J_i)-a(J_i)$, where $f(J_i)$ and $a(J_i)$ denote the finish (completion) time and arrive time respectively.
\end{definition}


If a task $j$ runs $t_j$ units of time on a computing node, then it consumes $\gamma*t_j$ units of resource on this node where $\gamma$ is a constant number. We ignore the resource consumption of an idle machine. For ease of description, we often interchange the two notations in this paper, namely, machine and computing node.

\subsection{Speculative execution under different operating regimes}
The cloning-based strategy for speculative execution schedules extra copies of a task in parallel with the initial task as long as the resource consumption of the task is expected to be low and there is system resource available. Only the result of one which finishes first among all the copies is used for the subsequent computation. Cloning does not incur any monitoring overhead. Nevertheless, cloning consumes a large amount of resource and can easily block the scheduling of subsequent jobs when the cluster workload is heavy.


On the other hand, the Straggler-Detection-based approach makes a speculative copy for the task after a straggler is detected. In this approach, the scheduler needs to monitor the progress for each task.  However, the monitoring incurs extra system instrumentation and performance overhead as discussed in {\cite{Monitoring}}. The situation is particularly challenging when the progress of a large number of tasks have to be tracked.  To make things even worse, it's always difficult to detect a straggler for small jobs as they usually complete their work in a very short period {\cite{Cloning}}.

As one may expect, the cloning-based strategy is only suitable for a lightly loaded cluster as it launches the clones in a greedy, indiscriminately fashion. On the other hand,  the straggler-detection based strategy is applicable to both lightly-loaded and heavily-loaded regimes. Hence, there exists a cutoff threshold to separate the cluster workload into these two operating regimes. When the workload is below this threshold, the cloning-based strategy can obtain a good performance in terms of job \textit{flowtime}.  Conversely, when the workload exceeds this threshold, only the Straggler-Detection-based strategy can help to improve the cluster performance.

\subsection{Deriving the cutoff threshold for different regimes}
In this subsection, we derive the cutoff workload threshold $\lambda^U$ which allows us to separate our subsequent analysis into the lightly loaded vs. heavily loaded regimes.  We assume that the random variables $m_i$ and $x^i_j$ are independent from each other for all $i$ and $j$. To simplify the analysis, we focus on the task delay in the cluster instead of job delay. Denote by $\lambda_t$ the task arrival rate to the whole cluster. Thus, $\lambda_t = \mathbbm{E}[m_i]\cdot \lambda$. We approximate the task arrival as a Poisson process with rate $\lambda_t$. Further, denote by $\lambda^m_t$ the average task arrival rate to each single machine which is given by $\lambda^m_t = \frac{\lambda_t}{M}$.

We model the task service process of each computing node (machine) as a $M/G/1$ queue. Applying the result of {\cite{queueing_theory}}, we get the average delay of each task without speculative execution made in the following equation
\begin{equation}
W_t = \frac{\lambda^m_t \mathbbm{E}[s^2]}{2(1-\lambda^m_t \mathbbm{E}[s])} + \mathbbm{E}[s]
\label{delay}
\end{equation}
where $\mathbbm{E}[s]$ is the average duration of all the tasks in the system without speculative execution implemented.

We proceed to derive the expression of the task delay for the cloning-based strategy. Different from {\cite{Cloning}} where the cloning is done for small jobs only, here we consider a more general cloning scheme in which the small jobs are not distinguished from the big ones. In this scheme, each task should at least make two copies. Otherwise, the task which does not have any extra copy may delay the completion of the entire job.

We illustrate an example in which $F_i(t)$ (for all $i$) follows the Pareto Distribution as follows: $$F_i(t) = \left\{\begin{array}{cc}
1-(\frac{\mu}{t})^{\alpha} & for \ t \geq \mu\\
0 & otherwise
\end{array}\right.$$
Assume $r$ ($\geq 2$) copies are launched for a particular task $\upsilon$. Then the expected duration for task $\upsilon$ is
$\mathbbm{E}[s^{'}] = \frac{\mu r\alpha}{r\alpha - 1}$.
Thus, $\frac{\mathbbm{E}[s^{'}]}{\mathbbm{E}[s]} = \frac{\alpha-1}{\alpha-1/r} > \frac{\alpha-1}{\alpha}$. This also gives a lower-bound of  the performance improvement of cloning regardless of the number of extra copies to be made for each task.

Denote by $\mathbbm{E}[r]$ the average number of copies each task makes. Hence, $\mathbbm{E}[r] \geq 2$. Further define $\mathbbm{E}[s^{c}]$ and $\lambda_t^c$ as the average task duration and equivalent task arrival rate to each machine respectively after the cloning is made.

The first constraint for cloning is that it must not overload the system, i.e. the long-term system utilization of the cluster should be less than 1. Thus, the following inequality holds:
\begin{equation}
\lambda_t^c \cdot \mathbbm{E}[s^{c}] < 1
\label{block}
\end{equation}
 By considering the constraint  in Eq.(\ref{block}), and the fact that $\lambda_t^c = \mathbbm{E}[r] \cdot \lambda^m_t$, we have:

\begin{Theorem}
The condition $\lambda \cdot \mathbbm{E}[m_i] \mathbbm{E}[s] \cdot \frac{4(\alpha-1)}{2\alpha - 1} < M$ is necessary to guarantee that the cloning does not overload the system.
\label{threshold}
\end{Theorem}
\begin{proof}
Refer to the technical report \cite{speculative-multi-job}.
\end{proof}
However, the efficiency of cloning is not guaranteed by Theorem \ref{threshold}. An efficient cloning strategy should have a smaller task delay than a strategy which does not make speculative execution. This argument must also hold when each task has only two copies. Denote by $W_t^c$ the average task delay when each task has two copies. For convenience, we define $\omega = \frac{\lambda \cdot \mathbbm{E}[m_i] \mathbbm{E}[s]}{M}$. Thus,
\begin{equation}
W_t^c = \mathbbm{E}[s]\cdot \frac{\omega \cdot \frac{(\alpha - 1)(1-4\alpha^2+4\alpha)}{\alpha(2\alpha - 1)} + 2(\alpha - 1)}{2\alpha - 1 - 4\omega (\alpha - 1)}
\label{delay_clone}
\end{equation}
and
\begin{equation}
W_t^c < W_t
\label{block_condition}
\end{equation}
Combine (\ref{delay}), (\ref{delay_clone}) and (\ref{block_condition}), we can derive the upper bound
$\omega^U$  for $\omega$. Hence, the cutoff threshold is determined by the following equation:
\begin{equation}
\lambda^U = \frac{\omega^U M}{\mathbbm{E}[m_i] \mathbbm{E}[s]}
\end{equation}
In the following sections, we continue to introduce the cloning-based strategy and Straggler-Detection-based approaches under two different workload regimes. $~$


\section{Optimal Cloning in the lightly loaded regime}
\label{SCA_design}
In the lightly loaded cluster, i.e., $\lambda < \lambda^U$, we first apply the generalized cloning-based scheme to improve the job performance.

We consider that time is slotted and the scheduling decisions are made at the beginning of  each time slot. Assume $J_i \in J$ consisting of $m_i$ tasks which are from the set $\Phi_i = \{\delta^i_1,\delta^i_2,\cdots,\delta^i_{m_i}\}$ and $\delta^i_j$ is scheduled at time slot $w^i_j$. Denote by $w_i$ the scheduling time of job $i$. Hence, $w^i_j$ and $w_i$ satisfy the following constraints:
\begin{equation}
w^i_j \in \{0,1,2\cdots,\} \ \ and \ \ w^i_j \geq a_i   \quad  \forall i; \ j
\label{schedule_time}
\end{equation}
\begin{equation}
w_i = \min\{w^i_1,w^i_2,\cdots,w^i_{m_i}\}   \quad  \forall i
\end{equation}
Each task of $\Phi_i$ can maintain different number of duplicates as the tasks in the same job may be scheduled at different time slots depending on server availability. Denote by $c^i_j$ the number of copies made for task $\delta^i_j$. Further let $t^i_{j,k}$ define the duration of the $k$th clone for task $\delta^i_j$. We assume $t^i_{j,k}$ follows the same distribution as $x^i_j$ and  all the $t^i_{j,k}$  are i.i.d random variables for $1\leq k \leq c^i_j$.  Define $t_j^i$ as the duration of task $\delta^i_j$ and $t_i$ as the \textit{flowtime} of job $J_i$ respectively.  Then the following two equations hold:
\begin{equation}
t^i_j =  \min\{t^i_{j,1}, t^i_{j,2}, \cdots, t^i_{j,c_i}\} \qquad \forall i; 1 \leq j \leq m_i
\label{duration_task}
\end{equation}
\begin{equation}
t_i =  \max\{t^i_{1} + w^i_1, t^i_{2} + w^i_2, \cdots, t^i_{m_i} + w^i_{m_i}\}  - a_i\qquad   \forall i
\label{duration_job}
\end{equation}
Equation (\ref{duration_task}) states that as soon as one copy of task $\delta^i_j$ finishes, the task completes. Equation (\ref{duration_job}) describes the job \textit{flowtime}.

We define a utility for each job which is a function of job \textit{flowtime} and the number of tasks it maintains. The formulation (P1) is as follows:
\begin{eqnarray*}
\max_{c^i_j,w^i_j}  &  &  \sum_{i = 1} U\left( \mathbbm{E}[t_i],m_i\right) - \gamma\cdot \sum_{i=1} \sum_{j=1}^{m_i}c^i_j\cdot \mathbbm{E}[t^i_j]\\
 s.t. &  & \sum_{w_i \leq l}\sum_{w^i_j + t^i_j > l}  c^i_j \leq M \quad \forall l \\
  &  &  1 \leq c^i_j \leq r \quad \forall  i;  1 \leq j \leq m_i\\
  & & (\ref{schedule_time}), (\ref{duration_task}), (\ref{duration_job})
\end{eqnarray*}

In this formulation, the utility of job $J_i$, $U\left( \mathbbm{E}[t^i],m_i\right)$ is a strictly concave  and differentiable function of $ \mathbbm{E}[t^i]$  and $m_i$. Our objective is to maximize the total utility of all the jobs in the cluster while keeping a low resource consumption level. The first constraint states that the total number of  tasks including all task copies at any time slot is no more than $M$ and the second constraint states that each individual task can at most maintain $r$ copies in the cluster.

\subsection{Solving P1 through approximation}
\label{p1_solve}
P1 is an online stochastic optimization problem and the scheduling decisions should be made without knowing the information of future jobs. In Equation (\ref{duration_job}), the tasks of the same job can be scheduled in different time slots. Hence, it is not easy to express the job \textit{flowtime} in terms of the distribution function and thus makes P1 difficult to solve.

Due to the fact that the cluster is lightly loaded, there is a large room for making clones for all the jobs most of the time. Thus, we solve another optimization problem (P2) as a relaxation for P1 when system resource is available. In P2, all the tasks of the same job are scheduled together and maintain the same number of copies. In this way, we can simplify the modeling of the job \textit{flowtime}.

Define $\chi(l)$ as the job set which contains all unscheduled jobs at time slot $l$. Assume $\chi(l) =  \{J_{l_1}, J_{l_2}, \cdots\}$. If there is enough idling servers to schedule the jobs in the cluster at the beginning of time slot $l$, i.e., $\sum_{i}m_{l_i} < N(l)$ where $N(l)$ is number of available machines, we solve P2 to determine the number of copies for each task in $J_{l_i}$ as below:
\begin{eqnarray*}
\max_{c_{l_i}}  &  &  \sum_{i = 1} U\left( \mathbbm{E}[t_{l_i}],m_{l_i}\right) - \gamma\cdot \sum_{i=1} \sum_{j=1}^{m_{l_i}}c_{l_i}\cdot \mathbbm{E}[t^{l_i}_j]\\
 s.t. &  & \sum_{i}m_{l_i}\cdot c_{l_i} \leq N(l) \\
  &  &  1 \leq c_{l_i} \leq r \quad \forall  i\\
  & & t^{l_i}_j =  \min\{t^{l_i}_{j,1}, t^{l_i}_{j,2}, \cdots, t^{l_i}_{j,c_{l_i}}\} \qquad \forall i; 1 \leq j \leq m_{l_i} \\
  & & t_{l_i} =  \max\{t^{l_i}_{1}, t^{l_i}_{2}, \cdots, t^{l_i}_{m_i}\}  + l - a_{l_i}\qquad   \forall i
\end{eqnarray*}

Here, $c_{l_i}$ is the number of duplicates assigned to each task in job $J_{l_i}$. Solving P2 is much easier and it only depends on the current information. Define $H^{l_i}_j(t)$ as the cumulative distribution function of $t^{l_i}_{j}$ and we have:
\begin{equation}
H^{l_i}_j(t) = 1-(1-F_{l_i}(t))^{c_{l_i}}
\end{equation}
Let $d_{l_i} \triangleq  \max\{t^{l_i}_{1}, t^{l_i}_{2}, \cdots, t^{l_i}_{m_i}\}$ and the distribution function of $d_{l_i}$ is given by:
\begin{equation}
W_{l_i}(t) = Pr(d_{l_i} < t) = \prod_{j = 1}^{m_{l_i}} H^{l_i}_j(t) = (1-(1-F_{l_i}(t))^{c_{l_i}})^{m_{l_i}}
\end{equation}
Further, $ \mathbbm{E}[t_{l_i}] =  \mathbbm{E}[d_{l_i}] + l - a_i$ and we get
\begin{equation}
\mathbbm{E}[d_{l_i}] = \int_{0}^{\infty}t\cdot d(W_{l_i}(t)) = \int_{0}^{\infty}(1-W_{l_i}(t))dt.
\end{equation}
Similarly,
$\mathbbm{E}[t^{l_i}_j] = \int_{0}^{\infty}(1-H^{l_i}_j(t))dt$, which yields:
\begin{equation}
 \sum_{j=1}^{m_{l_i}}c_{l_i}\cdot \mathbbm{E}[t^{l_i}_j] = m_{l_i}c_{l_i}\int_{0}^{\infty}(1-F_i(t))^{c_{l_i}}dt
\end{equation}

\begin{lemma}
$c_{l_i} \int_{0}^{\infty}(1-F_i(t))^{c_{l_i}}dt$ is a convex function of $c_{l_i}$ when provided $\ln{(1-F_i(t))}$ is a convex function of t.
\end{lemma}

\begin{proof}
Refer to \cite{speculative-multi-job}.
\end{proof}
\vspace{.5em}
In the same way, $\mathbbm{E}[t_{l_i}] $ is also a convex function of $c_{l_i}$ which decreases as $c_{l_i}$ increases.

In traditional scheduling problems, minimizing the job \textit{flowtime} is a common objective. Thus, we consider to minimize the summation of job \textit{flowtime} and resource consumption as a special example, i.e., $$U\left( \mathbbm{E}[t_{l_i}],m_{l_i}\right) = - \mathbbm{E}[t_{l_i}].$$

Observe that the first two constraints in P2 are linear. Hence, we can adopt the convex optimization technique to solve P2. The Lagrangian Dual problem of P2 is given by:
\begin{equation}
 \min_{\nu,\xi_{l_i},h_{l_i}}{D(\nu,\xi_{l_i},h_{l_i})}
\end{equation}
\begin{equation}
{D(\nu,\xi_{l_i},h_{l_i})}  = \max_{c_{l_i}} f(\nu,\xi_{l_i},h_{l_i},c_{l_i})
\end{equation}
\begin{equation}
\begin{split}
 f(\nu,\xi_{l_i},h_{l_i},c_{l_i}) &= - \sum_{i = 1}  \mathbbm{E}[t_{l_i}] - \gamma\cdot \sum_{i=1}  {m_{l_i} c_{l_i}} \cdot \mathbbm{E}[t^{l_i}_j]\\ & - \nu (\sum_{i=1}m_{l_i}c_{l_i}-N(l)) - \sum_{i}\xi_{l_i} (c_{l_i} - r)\\ & - \sum_{i=1}h_{l_i}\cdot(1 - c_{l_i})
\end{split}
\end{equation}
where $\nu$, $\xi_{l_i}$, $h_{l_i}$ are nonnegative multipliers. Applying the result of convex optimization, we conclude that there is no duality gap between P2 and D.

We adopt the gradient projection algorithm to get the optimal solution of D.
Define the vector $c(l) = (c_{l_1},c_{l_2},\cdots)$ and the algorithm is outlined below:
\begin{itemize}
\item Initialize $c_{l_i}^0 = 1$ for all $i$, $\nu^0 = 0.1$, $\xi_{l_i}^0 = 0.1$, $h_{l_i}^0 = 0.1$.
\item $c_{l_i}^{k+1} = \arg \max_{c_{l_i}} f(\nu^k,\xi_{l_i}^k,h_{l_i}^k,c_{l_i})$;
\item $v^{k+1} = v^k +\eta_1[\sum_{i=1}m_{l_i}c_{l_i}^{k+1}-N(l)]^+_{v^k}$;
\item $\xi_{l_i}^{k+1} = \xi_{l_i}^{k}  + \eta_2[c_{l_i}^{k+1} - r]^+_{\xi_{l_i}^{k}}$;
\item $h_{l_i}^{k+1} = h_{l_i}^{k} + \eta_3[1-c_{l_i}^{k+1}]^+_{h_{l_i}^{k}}$;
\item if $|c^{k+1}(l) - c^{k+1}(l)| < \epsilon$, the gradient algorithm terminates.
\end{itemize}
where $\{c,v,\xi,h\}_{*}^{k}$ denote the values of the corresponding parameters
during the $k$th iteration of the algorithm.

Next, we proceed to prove the convergence of this algorithm by adopting the method of Lyapunov stability theory {\cite{Control}}. We first define the following Lyapunov function:
\begin{small}
\begin{equation}
\begin{split}
V(\nu,\xi_{l_i},h_{l_i}) &= \int_{\nu^*}^{\nu}\frac{(\xi - \nu^*)}{\eta_1}d \xi + \sum_{i}\int_{\xi_{l_i}^*}^{\xi_{l_i}}\frac{(\xi - \xi_{l_i}^*)}{\eta_2}d \xi\\ + & \sum_{i}\int_{h_{l_i}^*}^{h_{l_i}}\frac{(\xi - h_{l_i}^*)}{\eta_3}d \xi
\end{split}
\end{equation}
\end{small}
where $\nu^*$,  $\xi_{l_i}^*$ and $h_{l_i}^*$ is the optimal solution of D. It can be shown that $V(\nu,\xi_{l_i},h_{l_i})$ is positive definite. (Refer to \cite{speculative-multi-job} for the details of the proof.)

Denote by $\dot{V}$ the derivative of $V(\nu,\xi_{l_i},h_{l_i})$ with respect to time. With the following lemma, we get $\dot{V} \leq 0$.
\begin{lemma}
The subgradient of $D(\nu,\xi_{l_i},h_{l_i})$ at $\nu$ is given by:
$$\frac{\partial D}{\partial \nu} = N(l) - \sum_{i=1}m_{l_i}\tilde{c}_{l_i}$$
Similarly, the subgradient of $D(\nu,\xi_{l_i},h_{l_i})$ and $\xi_{l_i}$, $h_{l_i}$ are:
$$
\frac{\partial D}{\partial \xi_{l_i}} = r - \tilde{c}_{l_i}; \quad \frac{\partial D}{\partial h_{l_i}} = \tilde{c}_{l_i} - 1
$$
respectively where $\tilde{c}_{l_i}$ minimizes $ f(\nu,\xi_{l_i},h_{l_i},c_{l_i})$.
\end{lemma}

\begin{proof}
Refer to \cite{speculative-multi-job} for details.
\end{proof}

\begin{Theorem}
\label{Theorem 1}
When the step size $\eta_1, \eta_2,\eta_3$ are positive, the above gradient projection algorithm can converge to the global optimal.
\end{Theorem}

\begin{proof}
First, the trajectories of V converge to the set $ S = \{(\nu,\xi_{l_i},h_{l_i})|\dot{V}(\nu,\xi_{l_i},h_{l_i}) = 0\}$ according to the Lasalle's Principle {\cite{Control}}.  Then, based on the proof of $\dot{V} \leq 0$ in \cite{speculative-multi-job}, for all $(\nu,\xi_{l_i},h_{l_i}) \in S$, $D(\nu,\xi_{l_i},h_{l_i}) = D^*$ which is the optimal value. Thus, all the elements in $S$ must be global optimal solutions. Hence, we conclude that the gradient projection algorithm converges to the global optimal.
\end{proof}
Not only that the algorithm is guaranteed to converge to the global optimal, we can further tune its parameters to speed up the convergence in an actual implementation.
Fig.~\ref{converge} depicts the results of a matlab-based simulation experiment to demonstrate the fast convergence rate of the gradient projection algorithm.  In this experiment, we assume that, in a particular time slot $l$, there are 4 jobs waiting to be scheduled, i.e., $\chi(l) =  \{J_{l_1}, J_{l_2}, J_{l_3}, J_{l_4}\}$.  The cluster has 100 available machines and the number of tasks for each job are 10, 20, 5 and 10 respectively. Assume $F_i(t)$ to follow the Pareto Distribution where $F_i(t) = 1-(\frac{\mu_i}{t})^{2}$ for $t \geq \mu_i$ and $\mu_1 = 1, \mu_2 = 2, \mu_3 = 1, \mu_4 = 2$ and the number of copies for each task is given by $r = 8$. We tune the parameters $\eta_1, \eta_2,\eta_3$ to be 0.2, 0.3, 0.4 respectively. As shown in Fig.~\ref{converge}, this algorithm can converge very fast to the optimal solution.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{iteration-eps-converted-to.pdf}
\caption{The convergency performance of the gradient projection algorithm. The number of duplicates for the tasks in each job iterates and can converge to the optimal value. $c_{l_i}$ represents the number of duplicates for each task in job $l_i$ where $i = 1,2,3,4$.}
\label{converge}
\vspace{-0.2 cm}
\end{figure}

\subsection{The design of the Smart Cloning Algorithm (SCA)}
Following the analysis in subsection \ref{p1_solve}, there exists a case where there is no space for cloning the jobs at the beginning of a particular time slot. In this scenario, it does not make sense to solve P2. Instead,  we adopt a smallest remaining workload first scheme. It is well known in scheduling literature that the  Shortest Remaining Processing Time (SRPT) scheduler is optimal for overall flowtime on a single machine where there is one task per job. The SRPT-based approach has been adopted widely for the scheduling in a parallel system as presented in {\cite{Fast_Completion}}{\cite{Joint_Phase}}, {\cite{Flow_Shops}} {\cite{Delay_Tails}} {\cite{Data_Locality}} {\cite{Degree_Locality}}, {\cite{Schedulers}}. Based on P2 and SRPT, we propose the Smart Cloning Algorithm (SCA) below.

SCA consists of  two separate parts. At the beginning of each time slot, we first schedule the remaining tasks of the unfinished jobs and then check whether the condition $\sum_{i}m_{l_i} <  N(l)$ is satisfied. If the condition is satisfied, we solve  P2 to determine the number of clones for each task. Otherwise, i.e. $\sum_{i}m_{l_i} \geq  N(l)$,  we sort $\chi(l)$ according to the increasing order of the workload in each $J_{l_i}$ and schedule the jobs based on this order and only one copy of each task is created.
Notice that the resultant workload is the product of $m_{l_i}$ and $\mathbbm{E}[x^i_j]$. The corresponding pseudo-code is given in Algorithm \ref{SCA_code} as below.


\IncMargin{1em}
\begin{algorithm}
\label{SCA_code}
\caption{Smart Cloning Algorithm}
\Indm
\KwIn{The jobs in the cluster associated with their running status at time slot $l$;}
\KwOut{Scheduling decisions for time slot $l$.}
\Indp
schedule the unassigned tasks of the running jobs in the cluster with the fewest remaining first\;
update $N(l)$ and $\chi(l)$\;

\If{$N(l) == 0$ }
{
return;
}

\eIf{$\sum_{i}m_{l_i} < N(l)$}
{
solve P2 and assign duplicates of the tasks in $J_{l_i}$ based on the optimization result;
}
{
\For{Job $J_{l_i}$ in $\chi(l)$}
{
assign only one copy for each task of $J_{l_i}$\;
update $N(l)$\;

\If{$N(l) == 0$}
{
return;
}
}
}

return;
\end{algorithm}
\DecMargin{1em}
\vspace{-0.3em}

\subsection{Performance evaluation for SCA}
We run a Matlab-based simulation to evaluate the performance of SCA. In the simulation, the jobs arrive at the computing cluster following a Poisson process with rate $\lambda = 6$. There are $M = 3000$ machines in the cluster. The number of tasks in each job is uniformly distributed from 1 to 100. Within a job, the duration of each task follows a common Pareto distribution with a heavy tail order of 2. The expected task duration for different jobs follows a uniform distribution of 1 to 4 units of time.
The resource consumption parameter $\gamma$ is set to be 0.01.
The simulation is run for 1500 units of time and repeated using 3 different seeds.

We use the speculative execution strategy of Microsoft Mantri as the baseline and compare it with the proposed SCA. We use job \textit{flowtime} and job resource consumption as the performance metrics for comparison. Fig.~\ref{SDA} depicts the cumulative density function of job \textit{flowtime} and resource consumption for nearly 27000 jobs where the solid line represents the results of SCA.
Observe from the figure that the average job \textit{flowtime} reduces by $60\%$ in our algorithm compared to the baseline. It is worthnoting that, under SCA, more than $80\%$ ($90\%$) of jobs can finish within 6 (9) time units respectively.  As a comparison, about $80\%$  ($90\%$) of jobs can finish within 17 (25) time units  for the Mantri algorithm. However, our algorithm consumes more resource than the Mantri one. It indicates that $80 \%$ of jobs consume less than 1.5 units resource in the Mantri baseline while $80\%$ of jobs consume less than 2 units resource in SCA. Recall that the objective of Smart Cloning Algorithm is to maximize the difference of job utility and resource consumption. To further make a fair comparison between these two schemes, we  consider the job utility minus total resource consumption as an additional performance metric. Our simulation results show that SCA can beat the baseline considerably in terms of this metric.


\section{Design of optimal Straggler-Detection-based scheme for the lightly loaded regime}
\label{SDA_design}
As analyzed in Section \ref{system_model}, both the cloning and straggler detection approach can improve the system performance in the lightly loaded regime. However, it is still unknown which one has a better performance. In this section, we formulate a different model to design speculative executions based on individual-task-progress monitoring. Different from the previous Straggler-Detection-based approaches which only duplicate one copy at most for each straggler, this model can automatically determines the optimal number of duplicates when a straggler is detected.

We use the same notations as in Section \ref{SCA_design} except for $c_i$.  Based on the monitoring result, the scheduler only begins to make speculative copies for task $\delta^i_j$ if a straggler is detected, i.e.  when the progress of $\delta^i_j$ substantially falls behind others. To be more specific,
a task is declared to be a straggler if its estimated remaining time to finish ( $t_{rem}$ ) is
greater than $\sigma_i \cdot \mathbbm{E} [t_{new}]$ where $t_{new}$ is the expected execution time of a new copy. In this model, different tasks in the same job can maintain different number of duplicates based on their task progress. We define $c^i_j(t)$ as the number of copies running for task $\delta^i_j$ at time $t \geq 0$.
Under this setup,  $\sigma_i$ is a parameter to be optimized for all $i$. Intuitively, if $\sigma_i$ is too small, a lot of running tasks will be characterized as stragglers. This can incur a large number of duplicates and consume a lot of resource in the cluster. On the other hand, if $\sigma_i$ is set to be too large, many jobs will be delayed due to a small number of  slow-progressings tasks  as the speculative copies of tasks are not launched.


To model the monitoring progress, we assume that the scheduler can detect the straggler for task $\delta^i_j$ after it completes a portion of the work which is $s_i$. Here, $s_i$ is a constant number related to job $J_i$. Thus, $c^i_j(t)$ needs to satisfy the following two constraints:
 \begin{equation}
 \label{p2_1}
c^i_j(t) = 1 \quad if \  w^i_j \leq t \leq  s_i \cdot t^i_{j,1} + w^i_j
\end{equation}
\begin{equation}
 \label{p2_2}
c^i_j(t) = 1 \quad if \ (1 - s_i) \cdot t^i_{j,1} < \sigma_i \cdot \mathbbm{E}[x^i_j] \  \& \ w^i_j \leq t \leq  t^i_{j,1} + w^i_j
\end{equation}

 Further define $c^i_j = \max_{t}\{c^i_j(t)\}$ and we assume all the $t^{i}_{j,k}$ are i.i.d random variables for $1\leq k \leq c^i_{j}$. In this model, the duplication of a particular task is made only once. Thus, the following equation holds:
\begin{equation}
 \label{p2_3}
 t^i_j = \min \{ (1 - s_i) t^i_{j,1},  t^i_{j,2}, \dots, t^i_{j,c^i_j} \} + s_i t^i_{j,1} \quad \forall i
\end{equation}
In Section \ref{SCA_design}, we derive the distribution function of job \textit{flowtime} by letting all the tasks of jobs being launched simultaneously. However, it is impossible to schedule the jobs in the same manner for the detection based model as stragglers can only be detected after the tasks have run for some time. In this way, it is difficult to simplify the expression for the job \textit{flowtime}. As such,  we choose to only optimize the total resource consumption of the job which yields the following formulation (P3):
\begin{eqnarray*}
\min_{c^i_j(t),\sigma_i}  &  &   \mathbbm{E} \left[ \sum_{i = 1}  \sum_{j=1}^{m_i} \gamma \cdot c^i_j \cdot t^i_j - \sum_{i = 1}  \sum_{j=1}^{m_i} \gamma  \cdot (c^i_j - 1) \cdot s_i t^i_{j,1} \right]\\
s.t.  &  & \sum_{i = 1} \sum_{j=1}^{m_i}c^i_j(t) \leq M \quad \forall t\\
  &  & c^i_j = \max_{t}\{c^i_j(t)\} \quad \forall i; j \\
  &  & 1 \leq c^i_j \leq r \quad \forall i; j\\
  &  & (\ref{p2_1}), (\ref{p2_2}), (\ref{p2_3})
\end{eqnarray*}
Notice that P3 is a stochastic programming problem and we need to find the optimal solutions for $c^i_j(t)$ and $\sigma_i$.

\subsection{Solving P3 through decomposition}
The objective in this formulation can be decoupled as the summation of the expected resource consumption for each individual task. However, the first constraint makes this problem difficult to solve as it cannot be decoupled. We first relax this constraint and minimize the expected resource consumption for an individual task. The constraint will be taken into account later on when we design the actual scheduling algorithm.

Define $y^i_j = min\{t^i_{j,2},t^i_{j,3},\dots,t^i_{j,c^i_j}\}$ if $c^i_j \geq 2$ and $y^i_j = \infty$ if $c^i_j = 1$.  Let $d^i_j \triangleq min\{(1 - s_i) t^i_{j,1}, y^i_j\}$. It can be readily shown that
\begin{equation}
\mathbbm{E} \left[ \gamma \cdot c^i_j t^i_j   - \gamma \cdot (c^i_j - 1)  s_i t^i_{j,1} \right] = \gamma \cdot  \mathbbm{E} \left[  c^i_j  d^i_j + s_i  t^i_{j,1} \right]
\end{equation}
Denote by $\pi^i_j$ the event that a straggler is detected for task $\delta^i_j$ and $\vartheta^i_j$ the event that $y^i_j \leq  \sigma_i  \mathbbm{E}[x^i_j]$ respectively. Then, we have
\begin{equation}
\begin{split}
 \mathbbm{E} \left[ c^i_j  d^i_j + s_i  t^i_{j,1}\right] &=\mathbbm{E} \left[ c^i_j  d^i_j   +s_i t^i_{j,1} \left|  \pi^i_j \right.\right]  \cdot Pr \left (\pi^i_j \right)  \\
 & + \mathbbm{E} \left[ c^i_j  d^i_j + s_i  t^i_{j,1}  \left|  \overline{\pi^i_j} \right.\right]  \cdot Pr \left (\overline{\pi^i_j} \right)
 \end{split}
\end{equation}
Further, denote by  $\varrho^i_j$  the event that $d^i_j >  \sigma_i  \mathbbm{E}[x^i_j]$. Thus,
\begin{equation}
 \mathbbm{E} \left[c^i_j  d^i_j  \left|  \pi^i_j \right.\right] = \mathbbm{E} \left[c^i_j  y^i_j  \left|  \vartheta^i_j \right.\right] \cdot Pr(\vartheta^i_j) + \mathbbm{E} \left[c^i_j  d^i_j  \left|  \varrho^i_j \right.\right] \cdot Pr(\overline{\vartheta^i_j})
 \label{all}
\end{equation}
\begin{equation}
  \mathbbm{E} \left[c^i_j  y^i_j  \left|  \vartheta^i_j \right.\right] \cdot Pr(\vartheta^i_j) = c^i_j \int_{0}^{\sigma_i \mathbbm{E}[x^i_j]} t d(1-(1-F_i(t))^{c^i_j - 1})
  \label{condition1}
\end{equation}
Define $G_i(t) = (1-F_i(t))^{c^i_j - 1}(1-F_i(\frac{t}{1-s_i}))$, then
\begin{equation}
 \mathbbm{E} \left[c^i_j  d^i_j  \left|  \varrho^i_j \right.\right] \cdot Pr(\overline{\vartheta^i_j}) =\frac{ c^i_j}{1-F_i(\frac{\sigma_i \mathbbm{E}[x^i_j]}{1-s_i})} \int_{\sigma_i \mathbbm{E}[x^i_j]}^{\infty} t d(1-G_i(t))
 \label{condition2}
\end{equation}
Combine Equality (\ref{all}), (\ref{condition1}), (\ref{condition2})
\begin{equation}
\begin{split}
\mathbbm{E} \left[c^i_j  d^i_j  \left|  \pi^i_j \right.\right] &= c^i_j\int_{0}^{\sigma_i \mathbbm{E}[x^i_j]}(1-F_i(t))^{c^i_j-1}dt\\ & + \frac{ c^i_j}{1-F_i(\frac{\sigma_i \mathbbm{E}[x^i_j]}{1-s_i})} \int_{\sigma_i \mathbbm{E}[x^i_j]}^{\infty}(1-G_i(t))dt
\end{split}
\end{equation}
For a fixed $\sigma_i$, the optimal value of $c^i_j$ is a function of $\sigma_i$ and is determined by the following equation:
\begin{equation}
(c^i_j)^* = \rho_i(\sigma_i) = \arg \min_{c^i_j}\mathbbm{E} \left[c^i_j  d^i_j  \left|  \pi^i_j \right.\right]
\label{optimal_c}
\end{equation}
And the optimal value of $\sigma_i$ is determined by the following equation:
\begin{equation}
\sigma_i^* = \arg{\min_{\sigma_i}\mathbbm{E} \left[  c^i_j  d^i_j + s_i  t^i_{j,1} \left| c^i_j =  \rho_i(\sigma_i) \right. \right]}
\label{optimal_sigma}
\end{equation}
For the detailed steps, please refer to \cite{speculative-multi-job}.
Define $\tau_i(c^i_j,\sigma_i) = \mathbbm{E} \left[c^i_j  d^i_j  \left|  \pi^i_j \right.\right]$. We conclude that, if $F_i(t)$ follows the pareto distribution, then $\tau_i(c^i_j,\sigma_i)$ is an increasing function of $c^i_j$ when $c^i_j \in [2,\infty)$ for all $\alpha_i, \sigma_i > 1$ where $\alpha_i$ is the heavy-tail order. Moreover, we can choose an appropriate $\sigma_i$ such that $\tau_i(2,\sigma_i) < \tau_i(1,\sigma_i)$. For the detailed proof, please refer to the technical report. Based on this conclusion, we derive the optimal solution of $c_i(t)$ for P3 in the following theorem:
\begin{Theorem}
\label{Theorem 2}
The optimal value for $c^i_j(t)$ is 2 once a straggler is detected under the Pareto heavy-tail distribution. Moreover, the optimal value for $\sigma_i$ does not depend on $\mathbbm{E}[x^i_j]$ or $s_i$  but  the heavy-tail order of the Pareto distribution.
\label{optimal_c_i_j}
\end{Theorem}
\begin{proof}
Refer to \cite{speculative-multi-job}.
\end{proof}


\subsection{The Design of  the Straggler Detection Algorithm (SDA) based on P3}
Based on the solution of P3, we propose the \textit{Straggler Detection Algorithm} (SDA) to optimize the resource consumption for stragglers. SDA monitors the progress for each task running in the cluster. When a new job $J_i$ arrives, the optimal value for $c^i_j$ and $\sigma_i$ are computed based on Equation (\ref{optimal_c}), (\ref{optimal_sigma}).

SDA consists of three scheduling levels. In the  first level, if the task $\delta^i_j$ is suffering from a straggler, the duplicates of $\delta^i_j$ are assigned to alternative machines. The policy is that for the straggler  $\delta^i_j$, the scheduler duplicates $c^i_j - 1$ new copies of it on other machines if resource is available and the machine is randomly chosen from any available ones. If the number of idle machine is less than $c^i_j - 1$, then all of these idle machines will be assigned a duplicate of $\delta^i_j$. In the second level, the remaining tasks of the jobs which have begun but not finished yet are scheduled at the beginning of each time slot. The job with the smallest remaining workload has the highest priority. In the third level, the jobs have not begun their work yet will be scheduled in the cluster and among those which has the smallest workload will be scheduled first. Similarly, denote by $\chi(l) =  \{J_{l_1}, J_{l_2}, \cdots\}$ the set of unscheduled jobs at time slot $l$ and it's sorted in the same way as SCA. The scheduler assigns only one copy for each task in the newly scheduled job from $\chi(l)$.


\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{SC_SD-eps-converted-to.pdf}
\caption{The comparison between our proposed SCA and SDA, the Microsoft Mantri's speculative execution algorithm is adopted as the baseline.  Panel a shows the cmf of Job \textit{flowtime} and Panel b shows the cmf of total job resource consumption. }
\label{SDA}
\vspace{-0.2 cm}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Com_sig-eps-converted-to.pdf}
\caption{The comparison between different $\sigma_i$ in the SDA algorithm. It indicates that when $\alpha = 1.707$, both the job \textit{flowtime} and resource consumption achieves the optimal.}
\label{comparison}
\vspace{-0.2 cm}
\end{figure}

\subsection{Performance evaluation for SDA}
We run another simulation to evaluate the performance of SDA. The simulation parameters are completely the same as SCA. For $\alpha = 2$, we apply Theorem \ref{optimal_c_i_j} and Equation (\ref{optimal_sigma}) to obtain the optimal value for $\sigma_i$ which is $1 + \sqrt{2}/2$. The comparison between SDA and the baseline algorithm is shown in Fig.~\ref{SDA}. It shows that both the job \textit{flowtime} and resource consumption of SDA perform better than the baseline though the objective of SDA is to optimize the resource consumption only. Fig.~\ref{comparison} shows the comparison result for different $\sigma_i$. It indicates that when $\sigma_i = 1 + \sqrt{2}/2$, both the job \textit{flowtime} and resource consumption achieve the best performance. Moreover, when $\sigma_i$ decreases below $1 + \sqrt{2}/2$, the total resource consumption increases. The job \textit{flowtime} increases when $\sigma_i$ increases above $1 + \sqrt{2}/2$ because the speculative copy is made late and cannot help much for reducing the job \textit{flowtime}. Compared to SDA, SCA performs better in terms of \textit{flowtime} for small jobs. However, the average job \textit{flowtime} of SCA is roughly the same while SCA consumes more resource. It is unfair to conclude that the SDA outperforms SCA. The reason is that SDA monitors the progress of each task and only makes the duplicates for the stragglers. However, SCA doesn't depend on the task progress and makes the cloning in a greedy fashion. Due to this reason, SCA tends to consume more resource than SDA. To determine which algorithm is better in real systems, the monitoring cost should be taken into account.

\section{Design of Straggler-Detection-based algorithm for the heavily loaded regime}
\label{ESE_design}
In the heavily loaded cluster, i.e., $\lambda \geq \lambda^U$, the cloning-based scheme cannot be applied and only the Straggler-Detection-based approach is efficient.  However, the SDA algorithm implemented in lightly loaded cluster launches the duplicates immediately when a straggler is detected. In the heavily loaded cluster, the resource is intensive and the scheduler should wait for the available machines. As a result, on-demand scheduling for stragglers is not always possible. In the literature, Microsoft Mantri {\cite{Outliers}} chooses to schedule a speculative copy if $\mathbb{P}(t_{rem}>2*t_{new}) > \delta$ is satisfied when there are available machines. We extend this scheme and propose the \textit{Enhanced Speculative Execution} (ESE) algorithm. The scheduling decision is made in each time slot and we also leave some space for cloning the small jobs. Usually the small job represents for interactive applications like query and have very strict latency requirement. When the workload is heavy, the probability of making speculative copies for the tasks of small jobs is low in Mantri's scheduling algorithm. Cloning for small jobs only incurs a small amount of resource consumption while contributing a lot to the job \textit{flowtime}.

\subsection{The Enhanced Speculative Execution algorithm}
The ESE algorithm also includes three scheduling levels. Explicitly, at the beginning of time slot $l$,  the scheduler estimates the remaining time of each running task and puts the tasks whose remaining time satisfy a particular condition into the backup candidate set $D(l)$. Define $t^i_{j,rem}(l)$ as the remaining time of task $\delta^i_j$ at the beginning of time slot $l$. Then,
$$D(l) = \{\delta^i_j:  c^i_j(l) = 1  \ \& \ t^i_{j,rem}(l)> \sigma_i \cdot  \mathbbm{E}[x^i_j] \}$$
All the tasks in $D(l)$ are sorted according to the decreasing order of  $t^i_{j,rem}(l)$ and the scheduler assigns  a duplicate of each task  in $D(l)$ based on this order.  In the same way as P3, we also need to find an appropriate value for $\sigma_i$ that the expected resource consumption for a single task is minimized.


The scheduler then assigns the remaining tasks of the jobs which have already been scheduled  but have not quitted the cluster yet. Denote by $R(l)$ the unfinished job set at time slot $l$ and the jobs are sorted based on the remaining workloads. Upon scheduling, the jobs which have smaller remaining workload are given the higher priorities.


$N(l)$ is updated after the above scheduling. If there are still available machines and some jobs waiting to be scheduled, the scheduler then tries to do the cloning for small jobs and assign the number of copies which can maximize the difference between the job utility and total resource consumption. For big jobs, no cloning will be made. More precisely, denote by $\chi(l) = \{J_{l_1}, J_{l_2}, \cdots\}$ the job set which contains all the jobs have not been scheduled yet in the cluster. The jobs in $\chi(l)$ are sorted according to the increasing order of workloads. For small job $J_{l_i}$  which satisfies $m_{l_i} <\eta \cdot  \frac{N(l)}{|\chi(l)|}$ and $\mathbbm{E}[x^{l_i}_j] < \xi $ in $\chi(l)$, the optimal number of copies cloned for task in $J_{l_i}$ is determined by the following equation.
\begin{equation}
c^*_{l_i} =  \arg \max_{c_{l_i}}U\left( \mathbbm{E}[t_{l_i}],m_{l_i}\right) - \gamma \cdot  \sum_{j=1}^{m_{l_i}}c_{l_i}\cdot \mathbbm{E}[t^{l_i}_j]
\label{clone_small_job}
\end{equation}
The parameters $\eta$ and $\xi$ can be tuned based on the cluster workload. Here, $c_{l_i}$, $t_{l_i}$, $m_{l_i}$ and $t^{l_i}_j$ are the same as Section \ref{SCA_design} and $w^{j}_{m_{l_i}}$ is equal to $l$ for all $j$.

The corresponding pseudocode is given in Algorithm \ref{ESE_code} in the below.

\IncMargin{1em}
\begin{algorithm}
\label{ESE_code}
\caption{Enhanced Speculative Execution Algorithm}
\Indm
\KwIn{The jobs in the cluster associated with their running status at time slot $l$;}
\KwOut{Scheduling decisions for time slot $l$.}
\Indp
Count $N(l)$, the number of idle machines at time slot $l$ and update $D(l)$, $R(l)$, $\chi(l)$.

\For{the task $\delta^i_j$ in  in $D(l)$}
{
Assign a duplicate of  $\delta^i_j$ on a random idle machine\;
$N(l)$ -= 1\;

\If{$N(l) == 0$ }
{
return;
}
}
\For{the job $J_i$ in $R(l)$}
{
Assign the unscheduled tasks of $J_i$ on idle machines\;
update $N(l)$\;

\If{$N(l) == 0$ }
{
return;
}
}
\For{the job $J_{l_i}$ in $\chi(l)$}
{
\eIf{$m_{l_i} <\eta \cdot  \frac{N(l)}{|\chi(l)|}$ \& $\mathbbm{E}[x^{l_i}_j] < \xi$}
{
Compute $(c_{l_i})^*$ base on Equation (\ref{clone_small_job})\;
Assign $(c_{l_i})^*$ duplicates for each task in $J_{l_i}$\;
update $N(l)$\;
}
{
Assign one duplicate for each task in $J_{l_i}$\;
update $N(l)$\;
}
\If{$N(l) == 0$ }
{
return;
}
}
return;
\end{algorithm}
\vspace{-0.3em}
\DecMargin{1em}


\subsection{Approximate Analysis for $\sigma_i$ in ESE}
$\sigma_i$ has a great impact on the system performance. In the following analysis,  we aim to find an appropriate $\sigma_i$  in the ESE algorithm through minimizing the expected resource consumption for one particular task.

Define the random variable $R^i_j$ as the resource that task $\delta^i_j$ consumes in the cluster. For convenience, we take $\gamma = 1$ and  denote by $\theta^i_j$ the event that $x^i_j > \sigma_i \mathbbm{E}[x^i_j]$. Then the expectation of $R^i_j$  can be expressed in the following equation.
\begin{equation}
 \mathbbm{E}[R^i_j] =\mathbbm{E} \left[ R^i_j \left|
  \theta^i_j \right. \right] Pr ( \theta^i_j ) + \mathbbm{E} \left[ R^i_j \left|
 \overline{\theta^i_j} \right. \right] Pr ( \overline{\theta^i_j} )
 \label{expected1}
\end{equation}
Moreover, we have
\begin{equation}
\mathbbm{E} \left[ R^i_j \left|
 \overline{\theta^i_j} \right. \right] Pr ( \overline{\theta^i_j} ) = \int_{0}^{\sigma_i \mathbbm{E}[x^i_j]}td(F_i(t))
  \label{expected2}
\end{equation}

We give the following definition which helps to derive the expression of $\mathbbm{E}[R^i_j]$.
\begin{definition}
The \textit{asktime} of a running task $i$ is the earliest time that the scheduler checks whether it should duplicate a new copy for task $i$ on available machines or not.
\end{definition}

Assume the interval of each time slot is short enough, then the \textit{asktime} of $\delta^i_j$ can be treated as uniformly distributed in the interval [0, $x^i_j$] due to the cluster is heavily loaded. At this \textit{asktime}, the scheduler assigns a new copy for $\delta^i_j$ if $t_{rem} > \sigma_i \mathbbm{E} [t_{new}]$ is satisfied. Hence,
\begin{equation}
\begin{split}
\mathbbm{E} \left[ R^i_j \left|
 {\theta^i_j} \right. \right] Pr ( {\theta^i_j} ) &= \int_{\sigma_i \mathbbm{E}[x^i_j]}^{\infty}d(F_i(t))[\int_{0}^{t-\sigma_i\mathbbm{E}[x^i_j]}\frac{1}{t}(x+\\ & 2\mathbbm{E}[\min\{t-x,t_{new}\}])dx + \sigma_i\mathbbm{E}[x^i_j]]
 \end{split}
  \label{expected3}
\end{equation}
In Equation (\ref{expected3}), $\mathbbm{E}[min\{t-x,t_{new}\}]$ is given by:
\begin{equation}
\mathbbm{E}[min\{t-x,t_{new}\}] = \int_{0}^{t-x}wd(F_i(w)) + (t-x)(1-F_i(t-x))
 \label{expected4}
\end{equation}
Combining Equation (\ref{expected1}),(\ref{expected2}),(\ref{expected3}),(\ref{expected4}), $\mathbbm{E}[R^i_j]$ can be determined and it's a function of $\sigma_i$. We obtain the optimal value of $\sigma_i$ that minimizes the expected resource consumption through letting the derivative of $\mathbbm{E}[R^i_j]$ be 0, .

We illustrate the picture of $\mathbbm{E}[R^i_j]$  in Fig.~\ref{sigma2} under different $\sigma_i$ for the pareto distribution.  $\alpha$ is the heavy-tail order and $\sigma_i^*$ is the optimal value. It indicates that when $\sigma_i$ is close to 1.7 where $\alpha$ is 2, $\mathbbm{E}[R^i_j]$ achieves the minimum value. We also compare the different optimal value for $\sigma_i$ when the heavy-tail order changes. As shown in Fig.~\ref{sigma2},  $\sigma_i^*$ increases along with $\alpha$. Moreover, for all $\alpha \geq 3$, $\sigma_i^*$ is very close to 2.0.

\begin{figure}
\centering
\includegraphics[height=15em,width=0.5\textwidth]{sigma2-eps-converted-to.pdf}
\caption{The illustration of ${E}[R^i_j]/{E}[x^i_j]$ under different $\sigma_i$ for pareto distribution when $\alpha = 2, 3, 4, 5$ .}
\label{sigma2}
\vspace{-0.3 cm}
\end{figure}

\subsection{Performance Evaluation for ESE}
In this subsection, we first conduct one  simulation to  show the impact of $\sigma_i$ and the heavy-tail order on the cluster performance  to demonstrate the fitness of our approximate analysis. Then  we continue to compare the performance of ESE algorithm and Mantri's speculative execution strategy.

\vspace{0.2em}
\subsubsection{The impact of $\sigma_i$ and $\alpha$}
In this simulation, there is only one job which consists of $10000$ tasks and the cluster has $M = 100$ computing nodes. The expected task duration is 1 and the other parameters are as same as the simulation for SCA. For each $\sigma_i \in (0,6)$ we run 50 simulations and take the average. We adopt the naive scheme in which speculative execution is not implemented as a comparison to the ESE algorithm. The simulation results are illustrated in Fig.~\ref{single}. It shows that when $\sigma_i$ is close to $1.7$ for $\alpha = 2$, both the total amount of resource consumption and job \textit{flowtime} achieve the minimum. Next, we keep the same expected task duration and take $\alpha = 2, 3, 4$ to show the impact of heavy-tail order on the optimal value of $\sigma_i$.  The result matches well with the theoretical analysis in Fig.~\ref{sigma2}. It also indicates that as $\alpha$ increases, the performance improvement of the ESE algorithm tends to be inconspicuous for a single job case.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{u1-eps-converted-to.pdf}
\caption{Simulation result for a single job. Panel a shows the comparison of resource consumption under different $\sigma_i$ between ESE algorithm and the naive method without backup. The blue line represents the ESE algorithm, the dark green line describes the result of the approximate analysis while the pink red line represents the method without backup. Panel b shows the comparison of job \textit{flowtime} time. }
\label{single}
\vspace{-0.3 cm}
\end{figure}


\subsubsection{The performance of ESE Algorithm}
In this simulation, there are $M  = 3000$ machines in the cluster. The simulation parameters are the the same as the SCA and SDA algorithms except for the job arrival rate. Here, we adopt two arrival rates which are 30 and 40 jobs per unit time. We follow the approximate analysis and choose $\sigma_i$ to be 1.7. $\eta$ and $\xi$ are set to be 0.1 and 1 respectively. Similarly, we use the Microsoft Mantri's algorithm as the baseline to compare it with the ESE algorithm. The results are illustrated in Fig.~\ref{ESE_40}. As shown, when the job arrival rate is 40, $80\%$ of jobs can finish within 10 units time in the ESE algorithm while $80\%$ of jobs can finish only within 18 units time in the baseline. Further, we observe that the average job \textit{flowtime} reduces by $18\%$ in the ESE algorithm compared to the baseline. However, the resource consumption for these two algorithms are roughly the same. This is because cloning for small jobs can incur an extra amount of resource consumption. When the job arrival rate $\lambda$ is 30, the job resource consumption for ESE algorithm reduces a lot compared to the baseline. We also apply the SCA and SDA algorithms to the heavily loaded cluster but the performance turns out to be poor. As the cluster is heavily loaded, the cloning in SCA easily blocks the scheduling of newly arriving jobs. Due to resource is intensive, there is a little chance to make on-demand scheduling for stragglers in SDA and thus can not help to improve the performance.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{ESE-eps-converted-to.pdf}
\caption{The comparison between our proposed ESE Algorithm and the Microsoft Mantri's speculative execution algorithm.  The job arrival rate is 40. Panel a shows the cmf of Job \textit{flowtime} and Panel b shows the cmf of total job resource consumption.}
\label{ESE_40}
\vspace{-0.3 cm}
\end{figure}
\section{Conclusions}
\label{conclusions}
In this paper, we address the speculative execution issue in the parallel computing cluster and focus on two metrics which are job \textit{flowtime} and resource consumption. We utilize the distribution information of task duration and build a theoretical framework for making speculative copies. We categorize the cluster into lightly loaded and heavily loaded cases and derive the cutoff threshold for these two operating regimes. Moreover, we propose two different strategies when the cluster is lightly loaded and design the ESE algorithm while the cluster is heavily loaded. This is the first work so far to adopt the optimization-based approach for speculative execution in a multiple-job cluster. In the future work, we will consider to make speculative execution for the cluster where there exists task dependency in a job. Like the MapReduce framework, any reduce task can only begin after the map tasks finish within a job.

\bibliographystyle{abbrv}
\bibliography{Multi-job-scheduling}

\end{document}
