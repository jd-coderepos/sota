\section{Experiments}


\heng{give qualitative analysis with examples instead of just numbers and visualization}

\subsection{Dataset}
We first evaluate our model on the Automatic Content Extraction (ACE 2005) dataset,\fnurl{}{https://catalog.ldc.upenn.edu/LDC2006T06} which provides document-level entity, relation, and event annotations.
We use this dataset because it has gold-standard event annotations, which is convenient to evaluate the impact of incorporating events into KG embeddings.
We perform automatic entity linking~\cite{blink} to link the entities to Wikidata (dumped in August 2019), and merge the entities with the same Wikidata entries into one single node.
We use the most recent event-event temporal relation extraction system~\cite{temporal2021} to obtain the event-event temporal relations. 


However, gold-standard event annotations for real-world KGs are usually expensive or even intractable.
To demonstrate that our proposed model can also be applied to KGs without gold-standard event annotations, we choose another large-scale dataset, WikiEvents~\cite{docie2021}, which contains news articles from Wikipedia references about complex events.
We use the most recent cross-document event extraction~\cite{oneie,docie2021}, coreference resolution~\cite{Tuan2021} and temporal tracking system~\cite{resin} to automatically extract events from the news articles.

The statistics of the extracted KGs from the two datasets are shown in Table~\ref{tab:dataset}.

\begin{table}[htbp]
	\centering
    \small
	\begin{tabular}{c|cccc}
		\toprule[1pt] 
		Dataset & Entities &  Rels &  Events &  Args \\
		\midrule[1pt]
		ACE-2005 & 7,376 & 7,441 & 3,071 & 5,587  \\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
        WikiEvents & 104,942 & 151,253 & 39,092 & 112,972    \\

		\midrule[1pt]
	\end{tabular}
	\normalsize
	\caption{The statistics of the extracted knowledge graphs from the two datasets, where \emph{Rels} and \emph{Args} denote the number of entity-entity relations and event argument links respectively.}
	\label{tab:dataset}
\end{table}


\subsection{Evaluation Tasks}
We evaluate our trained knowledge graph embeddings on three typical evaluation tasks: \emph{knowledge graph completion}, \emph{entity classification}, and \emph{relation classification}.
\paragraph{Knowledge Graph Completion}
KG completion~\cite{transe} is a typical task for evaluating knowledge graph embeddings, which aims to predict the tail entity  given a pair of head entity  and relation . 
Since the number of entities in KG is usually large, we use the ranking based metrics~\cite{transe}: mean ranking (MR), mean reciprocal ranking (MRR), and Hits@k to evaluate the KG completion performance.
For WikiEvents dataset with extremely large number of entities, it is not scalable to evaluate among all entities, therefore, we randomly selected a fixed number of negative samples and calculate the rankings of the correct tail entity.
\paragraph{Entity Classificiation}
The entity classification task aims to classify each entity node into a pre-defined entity type category.
We divide all entities in the KG into 80\% training, 10\% validation, and 10\% testing examples.
After we obtain our trained KG embedding, we adopt a two-layer feed-forward neural network with \emph{ReLU} activation on the hidden layer on the top of our model to fine-tune for entity classification task on the training entities, and select the best model on the validation set, then report the entity classification accuracy on the testing entities. 
There are totally 45 fine-grained entity types in \emph{ACE-2005} dataset and 9 types in \emph{WikiEvents}.
\paragraph{Relation Classification}
The relation classification task aims to identify the relation type between each given pair of entities.
Similar to entity classification, we also divide all the entity-entity relations into training, validation, and testing sets and use a two-layer feed-forward neural network for relation classification, and use the accuracy as the evaluation metric.
There are totally 18 relation types in \emph{ACE-2005} dataset and 9 types in \emph{WikiEvents}.


\subsection{Baselines and Implementation Details}
\paragraph{Baselines}
We first adopt the most typical previous models \textbf{\emph{TransE}}~\cite{transe} and \textbf{\emph{CompGCN}}~\cite{compgcn} as the representatives of translation based and graph neural network based KG embedding models respectively.
In both of these two methods, we only use the initial KG and the entity-entity relations without any events, and the entity node embeddings are randomly initialized before training.
To show the influence of incorporating the pretrained language model BERT~\cite{bert}, in \textbf{\emph{CompGCN-BERT}}, we use the \emph{bert-base-uncased} model checkpoint\fnurl{}{https://huggingface.co/bert-base-uncased} to initialize entity node embeddings before conducting message passing using CompGCN.
For our event-enhanced models, \textbf{\emph{EventKE}} represents our proposed event-enhanced knowledge graph embedding model.
In addition, to show the effects of each individual part of our model, we also introduce two model variants for ablation study.
\textbf{\emph{EventKE-w/o-templink}} denotes the model without information aggregation on event-event temporal links. 
In \textbf{\emph{EventKE-w/o-events}}, instead of using annotated events or event extraction results, we randomly initialize all events (including the event triggers, event types, and event arguments) and the use such ``random events'' to train the model.
Note that the number of model parameters in \textbf{\emph{EventKE-w/o-events}} and \textbf{\emph{EventKE}} are exactly the same, we want to evaluate whether the performance gains come from the valuable event information, or merely come from the larger capacity of the whole model.

\paragraph{Implementation Details}
We train the KG embedding models for a maximum of  epochs and apply an early stopping strategy of  epochs (if the validation loss is not lower than the previous best for  consecutive epochs, we will stop training the model).
We train the models on NVIDIA Tesla V100 GPUs using the Adam~\cite{adam} optimizer with a learning rate of .
The average runtime is about 3 hours for the \emph{ACE-2005} dataset and 5 days for the large KG in the \emph{WikiEvents} dataset.
The detailed hyper-parameter choices are shown in Appendix A.

\subsection{Results}


\begin{table*}[h]
	\centering
	\vspace{5pt}
	\small
	\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule[1.5pt] 
		Dataset & \multicolumn{4}{|c|}{\textbf{\emph{ACE-2005}}} & \multicolumn{4}{|c}{\textbf{\emph{WikiEvents}}} \\
		\specialrule{0em}{1pt}{1pt}
		\cline{1-9}
		\specialrule{0em}{1pt}{1pt}
		Metrics  & MRR &  MR  & Hits@10 & Hits@20
		& MRR &  MR  & Hits@10 & Hits@20\\
		\midrule[1pt] 
		TransE~\cite{transe} & 0.095 & 112  & 0.181 & 0.223 & 0.067 & 213  & 0.111 & 0.153\\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
		CompGCN~\cite{compgcn} & 0.102 & 88.8  & 0.201 & 0.256 & 0.074 & 186  & 0.134 & 0.171\\
		CompGCN-BERT & 0.178 & 79.5  & 0.322 & 0.419 & 0.149 & 168  & 0.210 & 0.259  \\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}		
		\textbf{EventKE} (Ours) & \textbf{0.203} & \textbf{67.5}  & \textbf{0.378} & \textbf{0.480} & \textbf{0.168} & \textbf{147}  & \textbf{0.257} & \textbf{0.322} \\
		- w/o temporal links & 0.196 & 78.5  & 0.362 & 0.465 & 0.166 & 149  & 0.249 & 0.312  \\
		- w/o events & 0.181 & 78.9 & 0.111  & 0.408 & 0.146 & 168  & 0.210 & 0.242 \\

		
		\midrule[1pt]
		


	\end{tabular}
		\caption{Knowledge graph completion performance. We set the number of negative samples as .}
	\label{tab:kg_completion}
	\normalsize
	\vspace{-0.45cm}
\end{table*}

\begin{table}[tbp]
	\centering
\small
	\begin{tabular}{c|cc|cc}
		\toprule[1pt] 
		Dataset & \multicolumn{2}{|c|}{\textbf{\emph{ACE-2005}}} & \multicolumn{2}{|c}{\textbf{\emph{WikiEvents}}} \\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
		 Model & Ents &  Rels  & Ents & Rels\\
		\midrule[1pt]
		 TransE & 70.25 & 56.32 &79.63  & 72.29\\
		CompGCN & 71.02 & 55.99 & 80.04 & 73.06\\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
		BERT-Only & 71.13 & 58.64 & 82.11  & 76.01\\
		CompGCN-BERT & 73.92 & 60.81 & 82.56 & 76.58\\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
		\textbf{EventKE} (Ours) & \textbf{77.47} & \textbf{62.60} & \textbf{85.61} & 77.94 \\
		
		- w/o temporal links & 75.81 & 62.20 & 84.32  & \textbf{78.11}\\
		- w/o events & 73.32 & 61.06 & 82.20 & 76.29 \\
		\midrule[1pt]
	\end{tabular}
	\normalsize
	\caption{Accuracy (\%) of entity and relation typing.}
	\vspace{-16pt}
	\label{table:entity_relation}
\end{table}

Table~\ref{tab:kg_completion} and Table~\ref{table:entity_relation} show the performance on knowledge graph completion, entity classification, and relation classification, respectively.
In general, our model significantly outperforms all baseline competitors.
By incorporating event information (compared with \emph{CompGCN-BERT}), our model can achieve 13.4\% average relative gain on KG completion MRR, and 3.5\% and 1.6\% absolute gain on entity and relation classification accuracy.

\paragraph{Effects of incorporating events.}
The evaluation results demonstrate that incorporating event information can greatly improve the quality of entity and relation embeddings.
From the results shown in Table~\ref{tab:kg_completion} and Table~\ref{table:entity_relation}, we can also find that event-event temporal links can also show effectiveness especially on the task of KG completion and entity classification.
We can also see that the performance of \emph{EventKE-w/o-event} is evidently lower than \emph{EventKE} on all tasks, even if it has the same number of model parameters with \emph{EventKE}.
Such results demonstrate that the performance gains come from the event information instead of larger model capacity.

\paragraph{Effects of using BERT.}
From the results, we can also find that \emph{CompGCN-BERT} performs much better that \emph{CompGCN}, which demonstrates that initializing the entity and event node embeddings using pertrained language models can significantly improve the performance.
This is probably because the BERT embeddings contain sentence-level contextual information of entity mentions and event triggers, which are not captured in pretraining in the random initialization setting.

\subsection{Qualitative Analysis}

\begin{table*}[t]
	\centering
	\vspace{5pt}
	\small
	\centering
	\begin{tabular}{m{6cm}|c|c|c}
\toprule[1.5pt] 
		Sentence with events & Event Structure & Task Input & Ranking Change\\
		\specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
		Well, when they do finally enter {\color{brown}{Baghdad}}, U.S. and coalition troops could face urban {\color{blue}{combat}} with the {\color{brown}{Republican Guard}}, some of whom may have withdrawn into the city.  &\begin{minipage}{0.16\textwidth}

      \includegraphics[width=24mm, height=18mm]{figures/event_eg1.pdf}

    \end{minipage} & \begin{minipage}{0.15\textwidth}

      \includegraphics[width=26mm, height=20mm]{figures/input1.pdf}

    \end{minipage} & \begin{minipage}{0.15\textwidth}

      \includegraphics[width=26mm, height=20mm]{figures/rank1.pdf}

    \end{minipage} \\
        \specialrule{0em}{1pt}{1pt}
		\hline
		\specialrule{0em}{1pt}{1pt}
		{\color{brown}{Israel}} {\color{blue}{retaliated}} on {\color{brown}{Hamas}}, namely {\color{brown}{Al-Rantissi}}, it missed him and killed civilians.  &\begin{minipage}{0.15\textwidth}

      \includegraphics[width=26mm, height=22mm]{figures/event_eg2.pdf}

    \end{minipage} & \begin{minipage}{0.16\textwidth}

      \includegraphics[width=28mm, height=20mm]{figures/input2.pdf}

    \end{minipage} & \begin{minipage}{0.15\textwidth}

      \includegraphics[width=26mm, height=18mm]{figures/rank2.pdf}

    \end{minipage} \\
    
		
\midrule[1pt]
	\end{tabular}
		\caption{Qualitative examples from the validation set demonstrating how events can help on KG completion.}
	\label{tab:qualitative}
	\normalsize
	\vspace{-0.45cm}
\end{table*}
To further inspect how events help better understand the knowledge graph, we analyze the output results from the validation set of KG completion task, and show two typical examples in Table~\ref{tab:qualitative}.
Given an input head entity and a relation, we show the rankings of the correct tail entity before and after incorporating event information in the last column of Table~\ref{tab:qualitative}.

In the first example, the model is required to predict where the \emph{Republican Guard} is \emph{located in}, however, the entity-entity relations (in the training set) from the knowledge graph do not contain such information, which makes the model output a low ranking for the correct entity \emph{Baghdad}.
After incorporating event information, the entities \emph{Republican Guard} and \emph{Baghdad} are connected through a \emph{Conflict:Attack} event node, which indicates that \emph{Republican Guard} is currently located in \emph{Baghdad} to participate this attacking event.
As a result, the number of hops between these two entities is greatly reduced, and the model can successfully predict the correct tail entity based on such event information.

Similarly, in the second example, the model is asked to predict the \emph{membership} of \emph{Al-Rantissi}.
We can see that there is also a \emph{Conflict:Attack} event indicating that \emph{Isarel} targets to retaliate on \emph{Hamas} and \emph{Al-Rantissi}.
Although such an attack event can not directly indicate that \emph{Al-Rantissi} is a member of \emph{Hamas}, it can inform the model that \emph{Al-Rantissi} and \emph{Hamas} are strongly related since they have both been targeted in one single attack event.
Therefore, the ranking of \emph{Hamas} is greatly increased from  to , which contributes to the overall performance gain of KG completion.






