\section{Experiments} \label{sec:exp}
\subsection{Experimental Setup}


\paragraph{Source Datasets}

We select the two most widely used \ourtask{} datasets SNLI~\citep{snli} and MNLI~\citep{mnli} as our original datasets.
Prior work~\citep{GururanganSLSBS18,PoliakNHRD18,hans} found various annotation artifacts in them, hence they serve as good use cases for constructing debiased datasets.

\paragraph{Evaluation Datasets}
For the hypothesis-only bias, we use the challenge sets SNLI-hard~\citep{GururanganSLSBS18} and MNLI-hard~\citep{mnli}, which were produced by filtering the test set with a hypothesis-only model~(\cref{sec:nli-hard}).
For syntactic biases, we follow previous work and use HANS~\citep{hans} for evaluation~(\cref{sec:hans-results}).
In addition, we evaluate on the adversarial test benchmark introduced by~\citet{model-agnostic-debias}~(\cref{sec:nli-adv-test}).
This benchmark covers a wide range of adversarial attacks, which will give a more complete picture of what spurious correlations the debiasing methods tackle. 



\paragraph{Generating Debiased Datasets}
We conduct debiased data generation for SNLI and MNLI \emph{separately}.
For SNLI, we use the proposed method described in \cref{sec:ul-debias} to train a generator $G^*_{\text{SNLI}}$.
Then we randomly sample a large number of instances from the generator to construct $\mathcal{D}_{G^*_{\text{SNLI}}}$.
The samples are filtered with a strong NLI model $M$ trained on SNLI to obtain  $\hat{\mathcal{D}}_{G^*_{\text{SNLI}}}$.
Finally, different options (\cref{sec:z-augment}) can be adopted to merge the synthetic data with the original data $\mathcal{D}_{\text{SNLI}}$  to construct debiased versions of SNLI.
The same procedure is used to produce debiased datasets for MNLI, by simply replacing the original dataset with MNLI.
We choose GPT-2 large and Roberta-large as the pretrained language models for $G^*$ and $M$ respectively.\footnote{On one A100 GPU, training the generator takes around 24 hours and generating the samples takes roughly 35 hours for each dataset.}
The size of the constructed debiased datasets are listed in~\cref{tab:data-size}.


\begin{table}[bt]
\begin{center}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{lrr}
      \toprule
      {\bf Options} & {\bf $\mathcal{D}_0=\mathcal{D}_{\text{SNLI}}$  } & {\bf $\mathcal{D}_0=\mathcal{D}_{\text{MNLI}}$ } \\
      \midrule
      \textbf{Original} $\mathcal{D}_0$  & 549,367 &  382,702 \\
      \textbf{Z-Aug} $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_0)$ &  1,142,475 & 744,326 \\
      \textbf{Par-Z} $\mathcal{Z}(\mathcal{D}_0) \cup \mathcal{Z}(\hat{\mathcal{D}}_{G^*})$ & 933,085 & 740,811 \\
      \textbf{Seq-Z} $\mathcal{Z}( \hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_0))$ & 927,906 & 744,200 \\
      \bottomrule
    \end{tabular}
}
\caption{Data size of the constructed debiased datasets for SNLI and MNLI.} \label{tab:data-size}
\end{center}
\end{table}






\paragraph{NLI Model Training}

Since our method directly debiases the training data itself, we keep the model and training objective fixed and only replace the training data with our generated debiased datasets.
For comparability with previous work~\citep{karimi-mahabadi-etal-2020-end,utama-etal-2020-mind,sanh2021learning}, we train BERT-base~\citep{bert} on our debiased datasets.
The NLI models are trained with ordinary \emph{cross-entropy} classification loss,
and the training hyperparameters are listed in~\cref{appendix:hparams}.
We run our experiments five times and report the average and standard deviation of the scores.\footnote{With the exception of our PoE experiments which single run, as hyperparameter tuning for PoE is costlier.}
We also conduct statistical significance testing using a 2-tailed t-test at 95\% confidence level.


\paragraph{State-of-the-art Debiasing Models}
We compare our method with the following three state-of-the-art debiasing models on each of our evaluation datasets.
\textbf{Product-of-Experts}~\citep{he-etal-2019-unlearn,karimi-mahabadi-etal-2020-end} ensembles a bias-only model's prediction $b_i$ with the main model's $p_i$ using $p'_i=softmax(\log p_i + \log b_i)$.
This ensembling enforces that the main model focuses on the samples that the bias-only model does not predict well.
\textbf{Learned-Mixin}~\citep{clark-etal-2019-dont} is a variant of PoE that introduces a learnable weight for the bias-only model's prediction.
\textbf{Regularized-conf}~\citep{utama-etal-2020-mind} uses confidence regularisation to retain the in-distribution performance while conducting model debiasing.




\paragraph{Combining PoE with Our Debiased Datasets}
Our approach changes the training data distribution instead of the model's training objective, and hence is orthogonal to prior work method-wise.
We also report the results of combining PoE with our proposed method, simply by training a PoE model on our debiased datasets.
We adapt the PoE implementation by~\citet{karimi-mahabadi-etal-2020-end}, and we follow their approach to conduct hyperparameter tuning for PoE.\footnote{\url{https://github.com/rabeehk/robust-nli}}
The hyperparameters of the PoE models are reported in \cref{tab:poe-hparams} of \cref{appendix:hparams}.


\begin{table}[bt]
\begin{center}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{lcl}
      \toprule
      {\bf Method (model w/ data)} & {\bf SNLI} & {\bf SNLI-hard} \\
      \midrule
      \multicolumn{3}{l}{\bf{Prior debiasing strategies trained on SNLI}} \\
      {AdvCls~\citep{belinkov-etal-2019-dont}}$*$ & 83.56 & 66.27\\
      {Ens. AdvCls~\citep{stacey-etal-2020-avoiding}}$*$ & 84.09 & 67.42  \\
      {DFL~\citep{karimi-mahabadi-etal-2020-end}}$*$ & 89.57 & \textbf{83.01} \\
      {PoE~\citep{karimi-mahabadi-etal-2020-end}}$*$ & 90.11 & 82.15  \\
      \midrule
      BERT-base w/ $\mathcal{D}_{\text{SNLI}}$ \emph{baseline} & 90.45 & 80.34$_{\pm 0.46}$ \\
      \multicolumn{3}{l}{\bf{Models trained on our debiased datasets}} \\
      BERT-base w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{SNLI}})$ & \textbf{90.67} & \underline{81.78}$_{\pm 0.53}$ \\
      BERT-base w/ Par-Z $\mathcal{Z}(\mathcal{D}_{\text{SNLI}}) \cup \mathcal{Z}(\hat{\mathcal{D}}_{G^*})$ & 88.11 & \underline{82.81}$_{\pm 0.37}$ \\
      BERT-base w/ Seq-Z $\mathcal{Z}( \hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_{\text{SNLI}}))$ & 88.08 & \underline{\textbf{82.82}}$_{\pm 0.15}$ \\
      \midrule
      \multicolumn{3}{l}{\bf{Combining PoE with our debiased datasets}} \\
      BERT-base + PoE w/ $\mathcal{D}_{\text{SNLI}}$ & 90.25 & 82.92 \\
      BERT-base + PoE w/ Seq-Z $\mathcal{Z}( \hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_{\text{SNLI}}))$ & 87.65 & \textbf{84.48} \\
      \bottomrule
    \end{tabular}
}
\caption{Accuracy on SNLI and SNLI-hard. $*$ are reported results and underscore indicates statistical significance against the baseline.
Training on our debiased SNLI datasets significantly boosts the performance on SNLI-hard compared to the baseline, and it improves further when combined with PoE.} \label{tab:snli-results}
\end{center}
\end{table}


\begin{table*}[bt]
\begin{center}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllll}
      \toprule
      {\bf Method (model w/ data)} &  \multicolumn{2}{c}{\bf{MNLI-m}} &  \multicolumn{2}{c}{\bf{MNLI-mm}} &\multicolumn{2}{c}{\bf{MNLI-m hard}}  &  \multicolumn{2}{c}{\bf{MNLI-mm hard}} \\
       &  \multicolumn{1}{c}{dev} & \multicolumn{1}{c}{test} &  \multicolumn{1}{c}{dev} & \multicolumn{1}{c}{test} &  \multicolumn{1}{c}{dev} & \multicolumn{1}{c}{test} &  \multicolumn{1}{c}{dev} & \multicolumn{1}{c}{test} \\
      \midrule
      \multicolumn{9}{l}{\bf{Prior debiasing strategies trained on MNLI}} \\
      {PoE~\citep{karimi-mahabadi-etal-2020-end}}$*$ & 84.58 & 84.11 & 84.85 & 83.47 & 78.02 & 76.81 & 79.23 & 76.83 \\
      {Learned-Mixin~\citep{clark-etal-2019-dont}}$*$ & 80.5 & 79.5 & 81.2 & 80.4 & - & 79.2 & - & 78.2 \\
      {Regularized-conf~\citep{utama-etal-2020-mind}}$*$ & 84.6 & 84.1 & 85.0 & 84.2 & - & 78.3 & - & 77.3 \\
      {BERT-base Main PoE+CE~\citep{sanh2021learning}}$*$ & 83.32 & - & 83.54 & - & - & 77.63 & - & 76.39 \\
      \midrule
      BERT-base w/ $\mathcal{D}_{\text{MNLI}}$ \textit{baseline} & 83.87 & 84.11 & 84.22 & 83.51 & 76.39$_{\pm 0.64}$ & 75.88 & 77.75$_{\pm 0.45}$ & 75.75 \\
      \multicolumn{9}{l}{\bf{Models trained on our debiased datasets}} \\
      BERT-base w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{MNLI}})$ & \textbf{84.72} & \textbf{85.12} &\textbf{85.14} & \textbf{84.09} & \underline{\textbf{78.95}}$_{\pm 0.76}$ & 78.60 & \underline{\textbf{80.29}}$_{\pm 0.54}$ & \textbf{78.51} \\
      BERT-base w/ Par-Z $\mathcal{Z}(\mathcal{D}_{\text{MNLI}}) \cup \mathcal{Z}(\hat{\mathcal{D}}_{G^*})$  & 82.48 & 83.27 & 82.95 & 82.95 & \underline{78.88}$_{\pm 0.80}$ & \textbf{79.19} & \underline{80.02}$_{\pm 0.62}$ & 78.49 \\
      BERT-base w/ Seq-Z $\mathcal{Z}( \hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_{\text{MNLI}}))$  & 82.55 & 83.41 & 82.70 & 83.17 & \underline{78.88}$_{\pm 0.83}$ & \textbf{79.19} & \underline{79.65}$_{\pm 0.44}$ & 78.44 \\
      \midrule
      \multicolumn{9}{l}{\bf{Combining PoE with our debiased dataset}} \\
      BERT-base + PoE w/ $\mathcal{D}_{\text{MNLI}}$ & 84.39 & 84.69 & 84.25 & 83.75 & 78.37 & 77.54 & 79.45 & 78.33 \\
      BERT-base + PoE w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{MNLI}})$ & \textbf{85.22} & \textbf{85.38} & \textbf{85.72}& \textbf{84.53} & \textbf{80.49} & \textbf{80.03} & \textbf{81.52} & \textbf{79.28} \\
      \bottomrule
    \end{tabular}
}
\caption{Accuracy on MNLI-matched (MNLI-m), MNLI-mismatched (MNLI-mm), MNLI-matched hard, and MNLI-mismatched hard. $*$ are reported results and underscore indicates statistical significance against the baseline.
Training on our debiased MNLI datasets significantly boosts the performance on MNLI-matched hard and MNLI-mismatched hard. When combined with PoE, our method improves further and outperforms previous methods.
} \label{tab:mnli-results}
\end{center}
\end{table*}



\subsection{Hypothesis-only Bias in NLI} \label{sec:nli-hard}

\citet{GururanganSLSBS18} found that, on SNLI and MNLI, a model that only has access to the hypothesis can perform surprisingly well, which indicates that the datasets contain hypothesis-only bias.
To alleviate this problem, SNLI-hard and MNLI-hard~\citep{GururanganSLSBS18} subsets were constructed by filtering the test set with a hypothesis-only model and only accepting those that the hypothesis-only model predicts incorrectly.
We examine whether our method successfully mitigates the hypothesis-only bias in NLI, by evaluating the models trained with our debiased datasets on SNLI-hard and MNLI-hard.


\begin{table}[!bt]
\begin{center}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{ll}
      \toprule
      {\bf Method}  & {\bf HANS} \\
      \midrule
      \midrule
      \multicolumn{2}{l}{\bf{Methods trained on SNLI}} \\
      \midrule
      {BERT-base Attention~\citep{joe-explain}}$*$ & 58.42 \\
      {Roberta-large w/ AFLite~\citep{AFLite}}$*$ & 59.6 \\
{Roberta-base w/ TAILOR~\citep{ross2021tailor}}$*$ & 70.5 \\
      \hline \hline
      \multicolumn{2}{l}{\bf{Methods trained on MNLI}} \\
      \midrule
      {Learned-Mixin~\citep{clark-etal-2019-dont}}$*$ &  64.00 \\
      {Learned-Mixin+H~\citep{clark-etal-2019-dont}}$*$ &  66.15 \\
      {PoE~\citep{karimi-mahabadi-etal-2020-end}}$*$ &  66.31$_{\pm 0.6}$ \\
      {DFL~\citep{karimi-mahabadi-etal-2020-end}}$*$ &  69.26$_{\pm 0.2}$ \\
      {PoE+CE~\citep{sanh2021learning}}$*$  & 67.9 \\
      {Regularized-conf~\citep{utama-etal-2020-mind}}$*$ &  69.1$_{\pm 1.2}$ \\
      {E2E Self-debias~\citep{ghaddar-etal-2021-end}}$*$ & \textbf{71.2}$_{\pm 0.2}$ \\
      \hline \hline
      \multicolumn{2}{l}{\bf{Models trained on our debiased datasets}} \\
      \midrule
      Roberta-base w/ $\mathcal{D}_{\text{SNLI}}$ & 65.32$_{\pm 2.22}$\\
      Roberta-base w/ Seq-Z $\mathcal{Z}( \hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_{\text{SNLI}}))$ & \textbf{66.87}$_{\pm 1.47}$ \\
      \midrule
      BERT-base w/ $\mathcal{D}_{\text{MNLI}}$ \emph{baseline} & 54.36$_{\pm 2.56}$ \\
      BERT-base w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{MNLI}})$ & \underline{62.57}$_{\pm 5.91}$  \\
      BERT-base w/ Par-Z $\mathcal{Z}(\mathcal{D}_{\text{MNLI}}) \cup \mathcal{Z}(\hat{\mathcal{D}}_{G^*})$ & \underline{65.11}$_{\pm 5.62}$  \\
      BERT-base w/ Seq-Z $\mathcal{Z}( \hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_{\text{MNLI}}))$ & \underline{\textbf{67.69}}$_{\pm 3.53}$   \\
      \midrule
      BERT-base + PoE w/ $\mathcal{D}_{\text{MNLI}}$ (baseline) & 63.40 \\
      BERT-base + PoE w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{MNLI}})$ & \textbf{68.75} \\
      \midrule
      Roberta-large w/ $\mathcal{D}_{\text{MNLI}}$ & 75.74$_{\pm 2.82}$ \\
      Roberta-large w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{MNLI}})$ & \textbf{78.65}$_{\pm 2.26}$  \\
      \bottomrule
    \end{tabular}
}
\caption{Results on HANS~\citep{hans}. $*$ are reported results and underscore indicates statistical significance against the baseline.
BERT-base trained on our debiased MNLI datasets performs significantly better than the one trained on the original MNLI, and it improves further when combined with PoE. Roberta-large also benefits from training on our debiased dataset.
} \label{tab:hans-results}
\end{center}
\end{table}



\paragraph{Results on SNLI-hard}
\cref{tab:snli-results} shows the results of our method on SNLI and SNLI-hard. 
The results show that, compared to training on SNLI, training with our debiased datasets significantly improves the performance on SNLI-hard. 
The debiased dataset produced by Seq-Z achieves a 2.48\% gain in accuracy on SNLI-hard compared to the SNLI baseline, 
whereas Z-Aug improves both SNLI and SNLI-hard accuracy.


\paragraph{Results on MNLI-hard}
\cref{tab:mnli-results} shows the results of our method on MNLI-matched (MNLI-m) and MNLI-mismatched (MNLI-mm), and their corresponding hard sets.
We use the development sets of MNLI-hard reconstructed by~\citep{karimi-mahabadi-etal-2020-end} to develop our methods. 
To comply with the submission limit of MNLI leaderboard system,
we select the best checkpoint among the five runs using the development set, and report its test set performance in~\cref{tab:mnli-results}.


The results show that BERT-base models trained on our debiased MNLI datasets outperform the models trained on the original MNLI by a large margin on the MNLI-hard sets.
In particular, the Z-Aug version of the debiased datasets gives a 2.72\% and 2.76\% gain in accuracy on MNLI-m hard and MNLI-mm hard respectively, and outperforms the previous state-of-the-art on MNLI-m, MNLI-mm, and MNLI-mm hard.



\paragraph{Combining PoE with Our Debiased Datasets}

We investigate the combination of our method and PoE, to see if the two orthogonal techniques can work together to achieve better performance.
Since hyperparameter tuning of PoE is costly, we choose the best version of the debiased dataset~(Seq-Z for SNLI and Z-Aug for MNLI) using the development set accuracy, and train PoE with it.
The results are listed in the last rows of \cref{tab:snli-results} and \cref{tab:mnli-results}.
We can find that, on both SNLI and MNLI, combining PoE with our debiased dataset yields further improvements on SNLI-hard, MNLI-m hard, and MNLI-mm hard, outperforming previous state-of-the-art results on all three datasets.




\begin{table*}[!bt]
\begin{center}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lllllllll}
      \toprule
&  \textbf{PI-CD} & \textbf{PI-SP} & \textbf{IS-SD} & \textbf{IS-CS} & \textbf{LI-LI} & \textbf{LI-TS} & \textbf{ST} & \textbf{Avg.} \\
      \midrule
      \multicolumn{9}{l}{\bf{Data-augmentation heuristics proposed by~\citet{model-agnostic-debias}}} \\
      {Text Swap}$*$ & 71.7 & 72.8 & 63.5 & 67.4 & 86.3 & \textbf{86.8} & 66.5 & 73.6 \\
      {Sub (synonym)}$*$ & 69.8 & 72.0 & 62.4 & 65.8 & 85.2 & 82.8 & 64.3 & 71.8 \\
      {Sub (MLM)}$*$ & 71.0 & 72.8 & 64.4 & 65.9 & 85.6 & 83.3 & 64.9 & 72.6 \\
      {Paraphrase}$*$ & 72.1 & 74.6 & 66.5 & 66.4 & 85.7 & 83.1 & 64.8 & 73.3 \\
      \midrule
      BERT-base w/ $\mathcal{D}_{\text{MNLI}}$ \textit{baseline} & 70.3$_{\pm 0.5}$ & 73.7$_{\pm 1.4}$ & 53.5$_{\pm 2.3}$ & 64.8$_{\pm 1.4}$ & 85.5$_{\pm 0.9}$ & 81.6$_{\pm 1.4}$ & 69.2$_{\pm 0.8}$ & 71.2$_{\pm 0.8}$ \\
      \multicolumn{9}{l}{\bf{Models trained on our debiased datasets}} \\
      BERT-base w/ Z-Aug $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{D}_{\text{MNLI}})$  & \underline{\textbf{73.1}}$_{\pm 0.9}$ & \underline{76.1}$_{\pm 1.2}$ & \underline{61.8}$_{\pm 6.1}$ & \underline{69.1}$_{\pm 1.3}$ & \underline{86.9}$_{\pm 0.6}$ & 83.1$_{\pm 0.9}$ & \textbf{70.1}$_{\pm 0.5}$ & \underline{74.3}$_{\pm 1.3}$ \\
      BERT-base w/ Par-Z $\mathcal{Z}(\mathcal{D}_{\text{MNLI}}) \cup \mathcal{Z}(\hat{\mathcal{D}}_{G^*})$  & \underline{72.0}$_{\pm 0.9}$ & \underline{\textbf{78.7}}$_{\pm 1.2}$ & \underline{64.5}$_{\pm 5.8}$ & \underline{70.7}$_{\pm 1.7}$ & \underline{88.5}$_{\pm 0.7}$ & 82.6$_{\pm 0.3}$ & 69.6$_{\pm 1.0}$ & \underline{75.2}$_{\pm 1.4}$ \\
      BERT-base w/ Seq-Z $\mathcal{Z}(\hat{\mathcal{D}}_{G^*} | \mathcal{Z}(\mathcal{D}_{\text{MNLI}}))$ & \underline{71.7}$_{\pm 0.9}$ & \underline{77.8}$_{\pm 1.2}$ & \underline{\textbf{66.9}}$_{\pm 3.7}$ & \underline{\textbf{71.1}}$_{\pm 0.7}$ & \underline{\textbf{89.1}}$_{\pm 1.0}$ & 82.3$_{\pm 0.9}$ & 69.3$_{\pm 0.8}$ & \underline{\textbf{75.4}}$_{\pm 0.8}$ \\
\bottomrule
    \end{tabular}
}
\caption{Results on the NLI adversarial test benchmark~\citep{model-agnostic-debias}. We compare with the data augmentation techniques investigated by~\citet{model-agnostic-debias}. $*$ are reported results and underscore indicates statistical significance against the baseline.
Training on our debiased MNLI datasets significantly improves the performance on majority of the categories (PI-CD, PI-SP, IS-SD, IS-CS, LI-LI) and on average.
} \label{tab:adv-attack-results}
\end{center}
\end{table*}




\subsection{Syntactic Bias in NLI} \label{sec:hans-results}


\citet{hans} show that NLI models trained on MNLI can exploit syntactic heuristics present in the data, such as lexical overlap, subsequence, and constituent features.
They introduce HANS, an evaluation dataset that contains examples where the syntactic heuristics fail.
To test whether our method mitigates the syntactic biases in NLI, we evaluate models trained on our debiased datasets on HANS.
If our debiased dataset contains less syntactic bias than the original dataset, the model would not exploit the syntactic heuristics and thus perform better on HANS.
Due to the high variance of the scores on HANS, we run five times for each experiment (except PoE), and report the average and standard deviation of the scores.


\paragraph{Results on HANS}
\cref{tab:hans-results} shows the results on HANS.
The results are categorised into three sections according to the training data: SNLI, MNLI, and our debiased datasets.
The results of models trained on our debiased MNLI datasets show strong improvements: compared to the original MNLI, our debiased MNLI datasets obtain up to a 13.33\% gain in HANS accuracy.
Our Seq-Z variant achieves 67.69\% accuracy, which is comparable with strong PoE baseline~\citep{karimi-mahabadi-etal-2020-end, sanh2021learning}. 
Our method also further improves PoE models: the BERT-base PoE model trained on our Z-Aug MNLI outperforms the one trained on MNLI by 5.3\%.
Additionally, training Roberta-large~\citep{roberta} on our debiased dataset introduces 2.9 points accuracy gain on HANS, indicating that the performance gain by our debiased dataset can generalise to larger and stronger models (more on this in \cref{sec:larger-models}). 


\subsection{Adversarial Tests for Combating Distinct Biases in NLI} \label{sec:nli-adv-test}

\citet{model-agnostic-debias} find that debiasing methods often tie to one particular known bias and it is nontrivial to mitigate multiple NLI biases at the same time.
They introduce a suite of test datasets for NLI models that targets various aspects of robustness, including partial input heuristics (PI), logical inference ability (LI), and stress test (ST).\footnote{Details of the subcategories are described in~\cref{appendix:adv-test-desc}.}
Several data augmentation strategies were investigated by~\citet{model-agnostic-debias}:
\begin{inparaenum}[1)]
\item text swap: swapping the premise and hypothesis in the original data;
\item word substitution: replacing words in the hypothesis with synonyms or generations from a masked language model;
\item paraphrase: using back translation to paraphrase the hypothesis.
\end{inparaenum}

We compare our approach with their data-augmentation heuristics, and the results are shown in~\cref{tab:adv-attack-results}.
Comparing with the MNLI baseline, our debiased MNLI datasets lead to better performance across all categories, which indicates that our method successfully mitigates various distinct biases simultaneously.
All three variants of our debiased datasets outperform the data augmentation heuristics by~\citet{liu2021toward},
which demonstrates the efficacy of our method when compared against manually designed heuristics.

\subsection{Generalisation to Larger Pretrained Language Models} \label{sec:larger-models}

Since our method mitigates the spurious correlations in the dataset, not the model, our approach is model-agnostic and has the potential to benefit larger future models.
To test this hypothesis, we train stronger and more modern models than BERT with our debiased datasets, and see if it can still improve the performance.
More specifically, we choose Roberta-base, Roberta-large~\citep{roberta}, and Albert-xxlarge~\citep{albert}, train them with Seq-Z SNLI and Z-Aug MNLI.

The results in \cref{tab:stronger-models} show that: 
\begin{inparaenum}[1)]
\item these larger models achieve better generalisation performance than BERT-base, which agrees with \citet{generalise-nli,combat-hype-caution};
\item training on our debiased datasets can still improve the performance of these models, yielding an average 2.30\%, 1.23\%, 1.13\% gain for Roberta-base, Roberta-large and Albert-xxlarge respectively.
\end{inparaenum}
This indicates that our method generalises to larger pretrained language models and could potentially enhance future models.


\begin{table}[hbt]
\begin{center}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{llllc}
      \toprule
       & \textbf{Test data} & \textbf{Original} & \textbf{Debiased} & \textbf{$\Delta$} \\
      \midrule
      \multirow{ 5}{*}{\rotatebox[origin=c]{90}{Roberta-base}} & SNLI-hard & 82.02$_{\pm 0.24}$ & \underline{\textbf{83.71}}$_{\pm0.31}$ & 1.69 \\
      & MNLI-m hard & 81.74$_{\pm0.44}$ & \underline{\textbf{83.14}}$_{\pm0.25}$ & 1.40 \\
      & MNLI-mm hard &  81.93$_{\pm0.30}$ & \underline{\textbf{83.12}}$_{\pm0.24}$ & 1.19 \\
      & HANS & 71.17$_{\pm2.95}$ & \underline{\textbf{76.15}}$_{\pm1.52}$ & 4.98 \\
      & Adv.Test avg & 77.63$_{\pm0.49}$ & \underline{\textbf{79.89}}$_{\pm0.38}$ & 2.26 \\
      \midrule
      \multirow{ 5}{*}{\rotatebox[origin=c]{90}{Roberta-large}} & SNLI-hard & 83.61$_{\pm0.31}$ & \underline{\textbf{85.09}}$_{\pm0.32}$ & 1.48 \\
      & MNLI-m hard & 85.44$_{\pm0.62}$ &  \textbf{85.69}$_{\pm0.24}$ &  0.25\\
      & MNLI-mm hard &  85.37$_{\pm0.63}$ & \textbf{85.94}$_{\pm0.21}$  & 0.57 \\
      & HANS & 75.74$_{\pm2.82}$ & \textbf{78.65}$_{\pm2.26}$ & 2.91 \\
      & Adv.Test avg & 80.92$_{\pm0.46}$ & \underline{\textbf{81.86}}$_{\pm0.31}$ & 0.94 \\
      \midrule
      \multirow{ 5}{*}{\rotatebox[origin=c]{90}{Albert-xxlarge}} & SNLI-hard & 83.59& \textbf{84.82} & 1.23 \\
      & MNLI-m hard & \textbf{86.42} & 86.40 & -0.02 \\
      & MNLI-mm hard &  86.38 & \textbf{86.82 }& 0.44 \\
      & HANS & 76.32 & \textbf{79.05} & 2.73 \\
      & Adv.Test avg & 81.91 & \textbf{83.18} & 1.27 \\
      \bottomrule
    \end{tabular}
}
\caption{Performance gain when training larger models with our debiased datasets. Underscore indicates statistical significance against the baseline that is trained on the original datasets. For evaluation on SNLI-hard, the models are trained with SNLI or our debiased Seq-Z SNLI; for other evaluation datasets, the models are trained with MNLI or our debiased Z-Aug MNLI. Albert-xxlarge is experimented with one run due to its higher training cost.} \label{tab:stronger-models}
\end{center}
\end{table}

