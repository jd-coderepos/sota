\documentclass[preprint,review,12pt]{elsarticle}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[figuresright]{rotating}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{mathrsfs}
\usepackage{newtxmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{soul}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{float}
\usepackage{comment}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\ie}{\textit{i.e. }}
\newcommand{\eg}{\textit{e.g. }}
\urlstyle{same}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{setspace} \doublespacing


\journal{Pattern Recognition}
\begin{document}
\begin{frontmatter}

\title{TransferDoc: A Self-Supervised Transferable Document Representation Learning Model Unifying Vision and Language}

\author[inst1]{Souhail Bakkali\corref{cor1}}
\cortext[cor1]{Corresponding author at: L3i, La Rochelle University, La Rochelle, France}
\ead{souhail.bakkali@univ-lr.fr}
\author[inst4]{Sanket Biswas}
\ead{sbiswas@cvc.uab.cat}
\author[inst2]{Zuheng Ming}
\ead{zuheng.ming@univ-lr.fr}
\author[inst1]{Mickael Coustaty}
\ead{mickael.coustaty@univ-lr.fr}
\author[inst3]{Marçal Rusiñol}
\ead{marcal@allread.ai}
\author[inst4]{Oriol Ramos Terrades}
\ead{oriolrt@cvc.uab.cat}
\author[inst4]{Josep Lladós}
\ead{josep@cvc.uab.cat}
\affiliation[inst1]{organization={L3i, La Rochelle University},
            city={La Rochelle},
            country={France}}
\affiliation[inst2]{organization={L2TI, Université Sorbonne Paris Nord},
            city={Paris},
            country={France}}
\affiliation[inst3]{organization={AllRead MLT},
            city={Barcelona},
            country={Spain}}
\affiliation[inst4]{organization={CVC, Universitat Autonoma de Barcelona},
            city={Barcelona},
            country={Spain}}

\begin{abstract}
The field of visual document understanding has witnessed a rapid growth in emerging challenges and powerful multi-modal strategies. However, they rely on an extensive amount of document data to learn their pretext objectives in a ``pre-train-then-fine-tune'' paradigm and thus, suffer a significant performance drop in real-world online industrial settings. One major reason is the over-reliance on OCR engines to extract local positional information within a document page. Therefore, this hinders the model's generalizability, flexibility and robustness due to the lack of capturing global information within a document image. We introduce TransferDoc, a cross-modal transformer-based architecture pre-trained in a self-supervised fashion using three novel pretext objectives. TransferDoc learns richer semantic concepts by unifying language and visual representations, which enables the production of more transferable models. Besides, two novel downstream tasks have been introduced for a ``closer-to-real'' industrial evaluation scenario where TransferDoc outperforms other state-of-the-art approaches.
\end{abstract}

\begin{keyword}

Multimodal Document Representation Learning \sep Document Image Classification \sep Content-based Document Retrieval \sep Few-Shot Document Image Classification \sep Contrastive Learning \sep Self-Attention \sep Transformers

\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}
\begin{figure}[t]
\centering
\begin{subfigure}{.90\textwidth}
  \includegraphics[width=\linewidth]{figures/figure_5.01.png} \quad
  \caption{Overview of the Few-Shot Document Classification process}
  \label{fig:fewshot_document_classification}
\end{subfigure}\hfill
\begin{subfigure}{.90\textwidth}
  \includegraphics[width=\linewidth]{figures/figure_5.02.png} \quad
  \caption{Overview of the Content-based Document Retrieval process}
  \label{fig:document_retrieval}
\end{subfigure}
\caption{\textbf{Task Overview:} We propose a new modification to the "pre-train-then-fine-tune" paradigm for a "closer-to-real" industrial scenario with offline and online stages.}
\label{fig:industrial_data_paradox}
\end{figure}



Document AI~\cite{cui2021document} strives towards automating and accelerating the process of reading, understanding, classifying and extracting information from documents which can be of any kind (\eg digitally born, scanned and/or web-pages). With the expanding traffic in the information management workflows for different industrial sectors (fin-Tech, legal-Tech, insur-Tech, etc.), there is a critical need for information extraction and document retrieval frameworks to be evaluated in an online setting which could reflect a ``closer-to-the-real'' performance of a visual document understanding (VDU) model. Existing state-of-the-art VDU frameworks~\cite{appalaraju2021docformer, Unifieddoc, huang2022layoutlmv3, li2022dit, li2021selfdoc, powalski2021going, xu2020layoutlmv2} have performed immensely well in document representation learning with the conventional ``pre-train-then-fine-tune'' paradigm for document image understanding tasks like classification~\cite{harley2015evaluation}, key information extraction~\cite{jaume2019funsd} and visual question answering~\cite{mathew2021docvqa}. However, when it comes to deploying such models in real-world business cases in the Document AI industry, there is a significant drop in model performance and efficiency. The primary causes that contribute to this issue are: (1) Most of the self-supervised learning-based (SSL) approaches like DocFormer~\cite{appalaraju2021docformer} and LayoutLMv3~\cite{huang2022layoutlmv3} develop their pre-training on millions of document samples. In addition, they rely heavily on OCR engines to learn local spatial information (\ie words along with their 2D and/or 1D positional encoding) to boost their performance for downstream VDU tasks; (2) Other SSL-based approaches like SelfDoc~\cite{li2021selfdoc} and UDoc~\cite{Unifieddoc} have used fewer pre-trained samples and utilized the visual features at a region-level (\ie cropped semantic object regions like tables, figures, etc.) with pre-trained object detector in their cross-modal encoded page representation, which makes it computationally heavy; (3) The offline document collections on which the above models are pre-trained are not representative enough of the online set of documents incoming from a new class category; (4) Multi-modal VDU models require vision-language sample pairs to be fed to the pre-trained embedding network in the online stage to perform the desired downstream task. Nevertheless, handling large volumes of new incoming documents in the online stage requires knowledge transfer from already analyzed offline sources where the model is flexible enough to have both uni-modal (\ie vision-only or language-only) and multi-modal understanding of a document image to save both memory and computational cost.





   





\textcolor{black}{Therefore, instead of limiting VDU frameworks to learn word-level and/or sentence-level local features with their given relative 2D and/or 1D position encoding of the words, we introduce in this paper TransferDoc: a page-level pre-training framework which learns the global information and the overall relationships between vision and language modalities guided by the model itself. TransferDoc learns the cross-modal interactions in a more informative and complete way with less possible errors which occur while using the relative position encoding as visual features (\eg the infographics in advertisement documents).} Moreover, our proposed model utilises a plain vision transformer~\cite{dosovitskiy2020image} for visual feature extraction, eliminating the need for a dense and expensive object detector. In addition, we propose three novel pre-training tasks in the VDU domain of which two are cross-modal objectives: \textit{learning-to-mine (L2M)} and \textit{learning-to-unify (L2U)}, while we propose a \textit{learning-to-reorganize (L2R)} pretext task to re-structure the representation space for each modality. Details are provided in Section\ref{sec:method}.





For a more challenging evaluation scenario suited to address the challenging industrial online setting, our work introduces two novel downstream tasks for VDU domain. The first task introduces an effective Few-Shot Document Image Classification, 
with the aim to learn how to adapt the pre-trained embedding model according to the task change, which is a "closer-to-real" industrial setting, especially in the online phase. As for the second task on Content-based Document Image Retrieval, we aim towards evaluating the representation learning capability of our model in both uni-modal~\cite{harley2015evaluation} and cross-modal document retrieval. 
Therefore, leveraging information from language and vision cues in an integrated fashion is crucial for developing an ideal system which proposes a diversity of ways in which document data could be used. These new evaluation downstream task settings have been illustrated in \figurename~\ref{fig:industrial_data_paradox}.




































%
 
\section{Related Work}
\label{sec:related}
\subsection{Self-Supervised Pre-Training}
Self-supervised methods~\cite{chen2020simple, misra2020self} in the computer vision literature primarily focus on designing suitable pretext tasks to leverage high-level semantic and compact representations from a large-scaled unlabeled corpus during the pre-training phase. These representations are then later utilized in solving downstream application tasks like classification~\cite{dosovitskiy2020image, touvron2021training} and object detection~\cite{carion2020end, zhu2020deformable}. In this context, pre-trained natural language processing (NLP) models like BERT~\cite{devlin2018bert} and RoBERTa~\cite{liu2019roberta} have also shown immense potential in generating contextualized representations from unlabeled text corpus by using masked language modelling (MLM) and next sentence prediction (NSP) pretext tasks. Vision models later incorporated a BERT-like pre-training inspired approach for images~\cite{bao2021beit, he2022masked} to capture the relationship between patches to achieve state-of-the-art performance in Self-supervised learning-based image recognition benchmarks. Alternatively, contrastive-based SSL techniques~\cite{chen2020simple, he2020momentum} were also used extensively to learn a global metric space from unlabeled image samples, which could benefit training large models without over-fitting. However, such approaches could suffer from a loss in generalizability during pre-training when the augmented image batches have similar statistics. To overcome this drawback, Dwibedi \etal~\cite{dwibedi2021little} proposed to sample the nearest neighbors from the dataset in the learnt latent space and treat them as positives. This introduces more semantic variations in the representation space by providing a support set of embeddings and preventing an over-reliance on data augmentation for such approaches. Therefore, our work has drawn inspiration from this pre-training objective to be applied in our SSL-based document understanding framework. 

\subsection{Cross-Modal Document Understanding}
The eventual success of BERT~\cite{devlin2018bert} to achieve state-of-the-art results for several downstream NLP tasks (sequence classification, named entity recognition, question answering etc.) also paved the way for its usage in the field of vision-language research by learning cross-modal encoder representations~\cite{lu2019vilbert, tan2019lxmert} using both modalities. Most of these approaches like CLIP~\cite{radford2021learning} and ALBEF~\cite{li2021align} employ a transformer-based multimodal encoder to jointly model visual (region-based image features) and textual (paired captions) tokens. Since these tokens are unaligned, they apply a pairwise contrastive objective function to align these representations before fusing them with a cross-attention mechanism~\cite{vaswani2017attention}. This enables a more grounded vision and language representation learning for the multimodal encoder.  

The current state-of-the-art methods in multi-modal document pre-training~\cite{xu2020layoutlmv2,huang2022layoutlmv3, appalaraju2021docformer} focus on combining a sequence of 2D positional embeddings from a page-level OCR as an input along with the visual and textual information, concatenating all of them using a multimodal transformer framework as in~\cite{powalski2021going, biten2022latr}. This helps to induce some relative spatial bias which could provide a heavy boost in learning local representations on a large-scale pre-training corpus. Nevertheless, these approaches fail to learn multimodal interactions in a more industrial-used setting when there is a limitation of available pre-training data. Recent works like SelfDoc~\cite{li2021selfdoc} utilize region proposals of semantic object layouts (tables, figures, text regions etc.) of a page using pre-trained Faster-RCNN~\cite{faster2015towards} and apply an OCR on top of them to learn a global unified page representation with fewer available pre-training data. To learn further contextualized visual representations, UDoc~\cite{Unifieddoc} improved the framework by introducing a contrastive learning objective for the visual features quantified in the latent space. Our work uses a similar industrial use-case setting as in ~\cite{li2021selfdoc, Unifieddoc}. 
In contrast to~\cite{li2021selfdoc,Unifieddoc}, we introduce a cross-modal contrastive learning objective inspired by ~\cite{radford2021learning, li2021align} to align the vision and language representations from the publicly available RVL-CDIP industry document benchmark~\cite{harley2015evaluation} through a multimodal cross-attention encoder, which enables more robust and generalized representation learning for document understanding systems. Previous attempts to leverage cross-modal document representation approaches include learning of joint embedding spaces for information extraction~\cite{zhang2020trie} and classification~\cite{bakkali2023vlcdoc, Dauphinee2019Modular, bakkali2020cross, bakkali2020visual}. Yet, these approaches were developed in a supervised setting. This work focuses towards vision-language feature learning during pre-training for unlabeled document images and then fine-tunes them for classification in both conventional and few-shot learning~\cite{snell2017prototypical} scenarios. Also, a content-based document retrieval setting has been introduced to further showcase the flexibility of our framework as in~\cite{gordo2013large, harley2015evaluation}.













 
\section{Method}
\label{sec:method}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/figure_22.png}
  \caption{Overview of the proposed TransferDoc framework with the designed pretext learning objectives.}
  \label{fig:figure_5.1}
\end{figure*}
\textcolor{black}{In this section, we present the cornerstones of our approach. First, we show that instead of learning single instance positives (\ie the instance discrimination task), cross-modal nearest-neighbors are capable of learning better features that are invariant to the intra-class variability encountered in document images. To facilitate cross-modal representation learning, we propose to pre-train multimodal transformers with unified vision-language objectives. Our model learns more diverse positive pairs and thus, better uni-modal representations before fusion using nearest-neighbour contrastive learning (\ie learning-to-mine (L2M)). Moreover, it learns an alignment objective which predicts whether a pair of vision and language is matched (positive) or not matched (negative) after leveraging the visual-textual features into a multimodal attention encoder module (\ie learning-to-unify (L2U)). Second, we show how mining multimodal nearest-neighbors from the pretext task can be used as a prior into a learnable approach designed for semantic clustering. Our proposed approach integrates the pre-trained vision-language features and learns a novel objective which aims to classify each vision-language pair and their neighbors together (\ie learning-to-reorganize (L2R)).}

\subsection{Model Architecture}


\subsubsection{Vision Encoder}
Let  be the document image which we feed to a ViT-B/16 transformer encoder~\cite{dosovitskiy2020image}. We reshape it into a sequence of flattened  patches , where  is the resolution of the document image,  is the number of channels,  is the resolution of each document patch, and  is the resulting number of patches, which serve as the input sequence length for the transformer encoder. The transformer encoder expects a flattened sequence as input of  dimension as the hidden embedding size. The resulting visual embeddings are then represented as .

\subsubsection{Language Encoder}
Let  be the OCRed text within a document image extracted with Tesseract OCR\footnote{\url{https://github.com/tesseract-ocr/tesseract}}. The input sequences are fed into the pre-trained RoBERTa  encoder~\cite{liu2019roberta}. The resulting textual embeddings are then represented as . To pre-process the text input, we tokenize the plain text  using RoBERTa tokenizer to get . Each input sequence is expected to start with a  token, and should end with a  token. The  is then represented as: , where  is the maximum sequence length. For each document, if , the input sequence is truncated so that it fits the desired length. For sequences that are shorter than , they are padded with a special  token until they are  long, and we ignore the  tokens during self-attention computation.

\subsubsection{Cross-Modal Attention Encoder}
The cross-modal attention encoder (CMAE) module captures intrinsic patterns by modeling both the inter-modality and the intra-modality relationships for image regions and text sequences. Specifically, the CMAE module is a transformer-based architecture as in~\cite{vaswani2017attention}, following the same design of a basic module in BERT~\cite{devlin2018bert}, with the parameters of vision-language modalities shared. It consists of a multi-head self-attention sub-layer, and a position-wise feed-forward sub-layer . Meanwhile, residual connections followed by the layer normalization  are also applied around each of the two sub-layers. In the multi-head self-attention sub-layer, the attention is calculated  times, making it to be multi-headed. This is achieved by projecting the queries , keys , and values   times by using different learnable linear projections. Let ,  be the sets of intermediate visual and textual features at the -th layer of the vision and language modalities respectively, where , and . To accomplish the cross-modal interactions between the vision modality  and the language modality , the cross-attention functions are then defined as:

This way, we emphasize the agreement between the visual regions and the semantic meaning of text sequences. Further, the outputs of each vision-language cross-attention module are subsequently fed into the vision-language self-attention module to intensify the inner-modality information by establishing inner-interactions and exploring similar patterns as in ~\cite{li2021selfdoc}. We then have:

Similarly, the  can be formulated by exchanging  with  in Equation~\ref{eq:equation_3}.

\subsection{Pre-training Objectives}


\noindent\textbf{Objective I: Learning-to-Mine (L2M).} The proposed learning objective (L2M) aims to force samples from language and vision that are semantically related to be closer according to the computed nearest-neighbors of each modality. As in SimCLR~\cite{chen2020simple}, a projection head is implemented on top of the vision and language backbones to map the visual-textual representations into a vector representation so that the two training schemes do not interfere with each other. The projection head is implemented as a non-linear multiple-layer perceptron (MLP) with one hidden layer, as it is more suitable for contrastive learning~\cite{chen2020simple}. Then,  normalization is applied to the visual-textual embeddings so that the inner product between features can be used as distance measurements. For the intra-modal objective, it is computed as:

where  computes similarity scores between sample pairs, 
 is a scalar temperature hyper-parameter, and M is the mini-batch size. 
Also, we encourage cross-modality learning by introducing two interactive terms in the inter-modal objective defined as:

The first term in Equation~\ref{eq:equation_5} computes the similarity score between the nearest neighbors of the given document image  with the corresponding text sample . Similarly, the second term computes the similarity score between the nearest neighbors of the given text sample  and its corresponding visual sample pair .
\text{NN}() denotes the nearest neighbour vision operator defined as:

\noindent\textbf{Objective II: Learning-to-Unify (L2U)}. The principle of this objective is to predict whether a pair of document image and its corresponding text is unified (positive) or negative (not unified). We compute the pairwise dot-product similarity between each language sequence  and document image  in the mini-batch as the predictions. The target similarity between the language sequence  and the document image  is computed as the average of the dot-product similarity between  and  and the dot-product similarity between  and . Then, the cross-entropy loss function is computed between the targets and the predictions. Given a mini-batch with  document images and sequence samples, for each document image , the vision-language pairs are constructed as , where  means that  is a unified pair, while  indicates the non-unified ones: 

where  is the probability of matching  to . 
In the L2U scenario, the unifying loss is usually computed in two directions as in~\cite{chen2019closer, wang2018learning, liu2017learning}. The  loss requires the unified text to be closer to the document image than non-unified ones, and in verse the  unify loss constrains the related text to rank before unrelated ones. Therefore, the L2U loss can be written as:

\noindent\textbf{Objective III: Learning-to-reorganize (L2R)}.
This objective aims to leverage the pretext features learnt across L2M and L2U objectives as a prior for clustering both the document images and their corresponding text sequences. We motivated that a pretext task from representation learning can be used to obtain richer semantic links between vision and language modalities~\cite{van2020scan}. 
Specifically, for every document image  and its corresponding text sequences , we mine their  nearest neighbors in the embedding space . Let , and  be the sets of the neighboring samples of ,  in the mini-batch  respectively. We aim to learn a clustering function  with weights  that classifies a sample document image , and a sample text sequence  and their mined neighbors   together. The function  terminates in a softmax function to perform a soft assignment over the vision clusters  and language clusters  with  and . The probabilities of sample pairs ,  being assigned to clusters ,  are denoted as  and  respectively. We then learn the weights of  by minimizing the following objectives for the vision modality:

The first term in the Equation~\ref{eq:equation_10} forces  to make sure that neighbors have the same clustering assignment. Thus, making consistent predictions for a sample document image  and its neighboring samples . 
In order to avoid  from assigning all document samples to a single cluster, we include the second term in the Equation~\ref{eq:equation_10}, which is an entropy loss assigned to the clusters, to make sure that the cluster distribution  is roughly uniformed. Similarly to the vision modality, the language modality loss  is computed likewise.
In general, the number of clusters is unknown. However, similar to prior works~\cite{li2021selfdoc}, we choose  and  equal to the number of ground-truth clusters for the purpose of evaluation. 
\section{Experiments and results}
\label{sec:results}
\begin{table*}[t]
\centering
\begin{center}
    \resizebox{\linewidth}{!} {\begin{tabular}{l|ccc|cc|cc|ccc|ccc}
    \hline
    Setting & \multicolumn{3}{c}{Pre-train Task} & \multicolumn{2}{c}{Meta-Learning} & \multicolumn{2}{c}{Head} & \multicolumn{6}{c}{Modality} \\
    & L2M & L2U & L2R & Meta-train & Meta-test & MLP & CMAE & \multicolumn{3}{c}{Vision} & \multicolumn{3}{c}{Language}\\
    
    \hline
    
    & & & & & & & & \multicolumn{3}{c}{\textbf{5-way/15-Query}} & \multicolumn{3}{c}{\textbf{5-way/15-Query}} \\
    & & & & & & & & \textbf{1-shot} & \textbf{5-shot} & \textbf{20-shot} & \textbf{1-shot} & \textbf{5-shot} & \textbf{20-shot} \\
    \cline{9-14}
    
    \multirow{4}{*}{S1} 
    & \cmark & \xmark & \xmark
    & \xmark & \cmark
    & \xmark & \xmark 
    & 34.66  0.66 & 44.70  0.64 & 51.15  0.66  
    & 32.49  0.63 & 41.73  0.58 & 48.84  0.54 \\

    & \cmark & \xmark & \xmark
    & \xmark & \cmark
    & \cmark & \xmark 
    & 41.33  0.71 & 61.55  0.65 & 75.05  0.49
    & 38.02  0.68 & 54.87  0.66 & 68.92  0.55 \\
    
    & \cmark & \xmark & \xmark
    & \cmark &\cmark
    & \xmark &\xmark 
    & 43.81  0.71 & 63.63  0.63 & 76.46  0.51 
    & 38.91  0.63 & 57.40  0.61 & 72.57  0.54 \\

    & \cmark & \xmark & \xmark
    & \cmark &\cmark
    & \cmark &\xmark & 
    \textbf{53.51}  \textbf{0.80} & \textbf{74.48}  \textbf{0.67} & \textbf{82.86}  \textbf{0.51}  
    & \textbf{38.14}  \textbf{0.61} & \textbf{57.04}  \textbf{0.61} & \textbf{72.20}  \textbf{0.56} \\
    
    \hline
    
    \multirow{2}{*}{S2} 
    & \cmark & \cmark & \xmark
    & \xmark & \cmark
    & \xmark & \cmark & 54.89  0.83 & 74.58  0.62 & 82.91  0.49
    & 67.23  0.96 & 77.82  0.41 & 78.87  0.35 \\

    & \cmark & \cmark & \xmark
    & \cmark & \cmark
    & \xmark & \cmark & \textbf{67.23}  \textbf{0.96} & \textbf{77.82}  \textbf{0.41} & \textbf{78.87}  \textbf{0.35}
    & \textbf{67.01}  \textbf{0.94} & \textbf{77.53}  \textbf{0.42} & \textbf{78.86}  \textbf{0.35} \\
    
    \hline
    
    \multirow{2}{*}{S3} 
    & \cmark & \cmark & \cmark
    & \xmark & \cmark
    & \xmark & \cmark & 79.08  0.88 & 89.10  0.39 & 89.96  0.37
    & 75.45  0.94 & 86.79  0.41 & 88.45  0.38 \\

    & \cmark & \cmark & \cmark
    & \cmark & \cmark
    & \xmark & \cmark  & \textbf{80.63}  \textbf{0.64} & \textbf{89.36}  \textbf{0.49} & \textbf{90.34}  \textbf{0.38}
    & \textbf{79.77}  \textbf{0.61} & \textbf{89.54}  \textbf{0.56} & \textbf{90.33}  \textbf{0.38} \\
    
   \hline
   
    \end{tabular}}
\end{center}
\caption{Ablation study on the Few-shot Image Classification task. All accuracy results are averaged over  test episodes and are reported with  confidence intervals.}
\label{tab:table_5.2}
\end{table*}  
\subsection{Pre-Training}
In the pre-training phase, we use the training set of the RVL-CDIP document benchmark dataset to learn multimodal representations. TransferDoc is initialized from the pre-trained weights of the pre-trained vision and language backbones. For the multimodal attention encoder, the weights are randomly initialized. We pre-train TransferDoc using AdamW~\cite{loshchilov2017decoupled} optimizer with a batch size of  for  steps. We use a weight decay of , . The learning-rate is warmed-up to  in the first  iterations, and decayed to  following a linear decay schedule. The temperature parameter  is set to , and the size of the queue used for L2M is set to . Note that we didn't use any type of data augmentation during pre-training, and we kept the OCRed text as is without any type of post-processing. Also, performance evaluations were conducted on the test set of the RVL-CDIP dataset.

\subsection{Fine-Tuning on Cross-Modal Tasks}
\noindent\textbf{Task I: Few-Shot Document Image Classification.}
To conduct the few-shot document image classification task, we use the pre-trained embedding network from stage one (\ie pre-training), and then apply meta-learning with an episodic manner. The task is illustrated as a K-way C-shot problem. Given C labelled samples for each unseen class, the model should fast adapt to them to classify novel classes. The entire test set can be presented by , where  is the total number of classes in , and  are samples from the test set with label . For a specific K-way C-shot meta-task ,  denotes class labels randomly chosen from dataset . Samples from these classes are randomly chosen to form a Support set and a Query set: (a) the support set for task  is denoted as , which contains C  K samples (\ie K-way  C-shot); (b) the query set is  where  is the number of samples selected for meta-learning. During the meta-learning stage, the proposed model is trained to learn an embedding function to map all input images and text samples from the same class to a mean vector  in a description space as a class descriptor for each class. For class , it is represented by the centroid of embedding features of test samples, obtained by:

where  is the embedding function initialized by the pretext task,  are the test samples labelled with class . As a metric learning-based method, we employ a distance function  and produce a distribution over all classes given a query sample  from the query set :

Euclidean distance is chosen as distance function . As shown in Equation~\ref{eq:equation_4.3.2.2}, the distribution is based on a softmax over the distance between the embedding of the samples (in the query set) and the class descriptors. The loss in the meta-learning stage can then read:



\begin{table*}[t]
\centering
\begin{center}
    \resizebox{\linewidth}{!} {\begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}
        \hline
        Setting & \multicolumn{3}{c}{Pre-train Task} & \multicolumn{6}{c}{Uni-Modal Retrieval} & \multicolumn{6}{c}{Cross-Modal Retrieval} \\
         & L2M & L2U & L2R & \multicolumn{3}{c}{Vision  Vision} & \multicolumn{3}{c}{Language  Language} &  \multicolumn{3}{c}{Vision  Language} &  \multicolumn{3}{c}{Language  Vision}\\
        
        \hline
        
        & & & & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} \\

        \cline{5-16}
        
        S1
        & \cmark & \xmark & \xmark
        & 78.85 & 91.21 & 94.23 & 74.52 & 90.00 & 93.67
        & 5.37 & 14.60 & 21.20 & 4.73 & 13.30 & 19.29 \\
        \hline
        S2
        & \cmark & \cmark & \xmark
        & 80.63 & 92.06 & 94.83 & 75.15 & 90.39 & 93.96 
        & 73.05 & 89.48 & 93.41 & 70.11 & 86.05 & 91.34 \\
        \hline
        S3
        & \cmark & \cmark & \cmark
        & \textbf{82.85} & \textbf{93.15} & \textbf{95.49} & \textbf{79.00} & \textbf{92.07} & \textbf{95.03} 
        & \textbf{75.28} & \textbf{90.07} & \textbf{93.58} & \textbf{73.74} & \textbf{88.00} & \textbf{92.29} \\

        \hline

    \end{tabular}}
\end{center}
\caption{R@K Quantitative evaluation results of uni-modal and cross-modal content-based retrieval on RVL-CDIP test set.}
\label{tab:table_5.4}
\end{table*}   
\begin{table}[t]
\begin{minipage}{0.56\linewidth}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccc}
    \hline
    Method & \multicolumn{3}{c}{Multi-Modal Modality} \\
    \hline
    & \multicolumn{3}{c}{\textbf{5-way/15-Query}} \\
    & \textbf{1-shot} & \textbf{5-shot} & \textbf{20-shot} \\
    \cline{2-4}
    LayoutLMv3~\cite{huang2022layoutlmv3}
    & 26.92  0.56 & 33.08  0.62 & 38.17  0.57 \\
    \hline
    \textbf{TransferDoc}  
    & \textbf{80.20}  \textbf{0.23} & \textbf{89.47}  \textbf{0.53} & \textbf{90.35}  \textbf{0.39} \\
    \hline
    \end{tabular}}
\caption{A comparison with LayoutLMv3 pre-trained model on the Few-Shot Document Image Classification task.}
\label{tab:table_5.3}
\end{minipage}\hfill
\begin{minipage}{0.40\linewidth}
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccc}
    \hline
     Method & \multicolumn{3}{c}{Multi-Modal Retrieval} \\
   \hline
    & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} \\
    \cline{2-4}
    LayoutLMv3~\cite{huang2022layoutlmv3} & 32.62 & 60.29 & 73.95 \\
    \hline
    \textbf{TransferDoc} & \textbf{80.93} & \textbf{92.62} & \textbf{95.27} \\
    \hline
    \end{tabular}}
\caption{Quantitative evaluation results of uni-modal and cross-modal Content-based retrieval in terms of R@K.}
\label{tab:table_5.5}
\end{minipage}
\end{table}
\noindent\textbf{How effective are the designed pretext objectives?}
As detailed in Table~\ref{tab:table_5.2}, we perform an ablation study on the few-shot image classification task under three settings S1, S2 and S3. Each setting is performed w/wo our proposed pretext objectives (\ie L2M, L2M+L2U and L2M+L2U+L2R), w/wo meta-training, w/wo the MLP, and w/ the CMAE. For each setting, the best-performing method is highlighted in bold. The results are averaged over 600 experiments as in~\cite{chen2019closer, ravi2016optimization}. 
We conduct experiments on the most common setting in the conventional few-shot image classification task: 1-shot, 5-shot, and 20-shot (\ie 1 or 5 or 20 labeled instances are available from each novel class). We use the pre-trained TransferDoc as the embedding network, and perform 5-way classification for only novel classes. During meta-training, we split document samples onto 9 classes for fine-tuning, and 7 classes for testing. Note that we sample only 600 samples from each class as in~\cite{dhillon2019baseline}. The results show that the two-step pre-training approach (\ie S3) improves semantic representation learning, and thus improves the overall results of both uni-modal vision and language modalities compared to a one-step only pre-training approach (\ie S1 and S2). Also, with the use of the CMAE, the model learns better information by unifying the vision-language sample pairs. Furthermore, we observe that the performance of our proposed method significantly increases when receiving more samples as input (\ie 20-shot) w/wo meta-training. 

\noindent\textbf{Does the amount of pre-training data matters?} In this part, we conduct experiments on the multi-modal setting given multi-modal inputs in both offline and online stages to make the results a fair comparison with the LayoutLMv3~\cite{huang2022layoutlmv3} baseline. We built our implementation using the released pre-trained model and performed inference (\ie online stage) on the few-shot document image classification task. The results in Table~\ref{tab:table_5.2} illustrate that TransferDoc outperforms the open-sourced available LayoutLMv3 model by a huge margin on the 1-shot, 5-shot and 20-shot classification settings. This shows that TransferDoc is able to fulfill the \textit{objective of a decent transferable VDU model for an industrial online setting breaking the myth of pre-training millions of document samples}.  



\noindent\textbf{Task II: Content-based Document Image Retrieval.}
To conduct the Content-based Document Retrieval task, we evaluate the generalizability of TransferDoc on both uni-modal and cross-modal retrieval tasks on each setting to answer the research question of \textit{the utility of cross-modal representations encoded by the pre-trained TransferDoc to solve query tasks in both uni-modal and cross-modal retrieval settings}. The problem formulation of content-based document retrieval is defined as follows: In the first phase, which corresponds to the indexing phase, we extract the vision and language backbones and generate the embeddings for all document images -in the dataset in which TransferDoc has been already pre-trained on- using the target modality only. In the second phase, which corresponds to the retrieval phase, we process the query modality using the pre-trained TransferDoc model without activating (\ie with frozen backbones) the network of the target modality (\ie which can be either vision or language). 
As a performance measure of the ranking of the retrieved results, we use the standard evaluation metric in content-based retrieval (\ie Recall@K (R@K)). We calculate the R@K on a different number of samples to retrieve (\ie 1, 5 and 10 samples). 

\noindent\textbf{How good is TransferDoc in terms of model efficiency and flexibility?} As reported in Table~\ref{tab:table_5.4}, we conduct experiments on content-based document retrieval in different settings Si, S2 and S3. We observe that in the uni-modal retrieval scenario for setting S1, TransferDoc achieves decent performance in retrieving top-K relevant information which belongs to the same document category as the input query. 
However, in cross-modal retrieval, the performance drops R@K score significantly for both visn  lang and lang  visn tasks. This drop of performance is mainly due to the fact that at a high level, TransferDoc did not identify any agreement between vision-language sample pairs (\eg the font size or character style for an image region is not confirmed by the semantic meaning of the corresponding language features). This means that those features were not amplified. This brings us to add the CMAE module in setting S2 to better capture the cross-modal interactions between vision and language modalities at a higher level with a unified cross-modal learning objective (\ie L2U), with the aim to overcome the problem of the first L2M pretext task when performed on cross-modal retrieval. As seen in Table~\ref{tab:table_5.4} that unifying vision-language sample pairs do not only improve the uni-modal results, but also cross-modal ones with nearly  for the image modality, and nearly  for the text modality, given all R@K scores. 
\begin{figure*}[t!]
\centering
\begin{subfigure}{\textwidth}
  {
  \includegraphics[width=\linewidth]{figures/AA.png}}\quad
  \caption{Vision to vision content-based document retrieval}
  \label{fig:vision_to_vision_retrieval}
\end{subfigure}
\hfill
\begin{subfigure}{\textwidth}
  {
  \includegraphics[width=\linewidth]{figures/BB.png}}\quad
  \caption{Language to language content-based document retrieval}
  \label{fig:language_to_language_retrieval}
\end{subfigure}
\caption{\textbf{Representative uni-modal retrieval samples of the pre-trained TransferDoc model.} Zoom in for better visualization. The first column represents the input example query randomly selected from the test set of RVL-CDIP, and the top-5 retrievals are shown in following columns in order. Red and green borders are used to depict the incorrect and correct classes of retrieved documents respectively. For each task setting (\ie Vision  Vision and Language  Language, the same input example query is used.}
\label{fig:figure_3.3}
\end{figure*}
This justifies the importance of unifying high-level visual and textual features in a cross-modal fashion. Lastly, with the last setting S3, we aim to learn richer semantic concepts by adding the L2R on top to improve the representation learning for vision and language modalities. As illustrated in Table~\ref{tab:table_5.4}, the best R@K scores have been achieved with all the three pre-training objectives combined (\ie L2M+L2U+L2R) for both uni-modal and cross-modal retrieval tasks. Moreover, we conduct experiments on the multi-modal setting for a fair comparison with the LayoutLMv3~\cite{huang2022layoutlmv3} baseline, given multi-modal inputs as query. The results in Table~\ref{tab:table_5.5} illustrate that TransferDoc outperforms the LayoutLMv3 model by a massive margin on the top-k retrieval scores. It is important to notice that The results that have been achieved are quite similar in all configurations (\ie uni-modal, cross-modal and multi-modal). Hence, \textit{TransferDoc enables to produce more transferable and semantically rich embeddings}.
We further show some qualitative samples of different retrieval scenarios given a challenging example from the "specification" and "report" categories as the input query in \figurename s~\ref{fig:figure_3.3},\ref{fig:figure_3.4}. We observe both good retrieval cases and some failure ones for both uni-modal and cross-modal retrieval settings. The layout of a "specification" document includes big structured tables which are also found in other classes like "form" or "questionnaire" or "publication". As well, the "report" category includes mostly text-only information having "memo" and "budget" document-like structures defined as the problem of inter-class variability which is challenging in the case of cross-modal retrieval scenarios, where there are more failure cases. However, we depict some interesting retrieval cases as in "advertisement", "news\_article", "publication", and "resume" queries in the \figurename s~\ref{fig:figure_3.3},\ref{fig:figure_3.4}.
\begin{figure*}[t!]
\centering
\begin{subfigure}{\textwidth}
  {
  \includegraphics[width=\linewidth]{figures/AB.png}}\quad
  \caption{Vision to language content-based document retrieval}
  \label{fig:vision_to_language_retrieval}
 \vspace*{8pt}
\end{subfigure}
\hfill
\begin{subfigure}{\textwidth}
  {
  \includegraphics[width=\linewidth]{figures/BA.png}}\quad
  \caption{Language to vision content-based document retrieval}
  \label{fig:language_to_vision_retrieval}
\end{subfigure}
\caption{\textbf{Representative cross-modal retrieval samples of the pre-trained TransferDoc model.} Zoom in for better visualization. The first column represents the input example query randomly selected from the test set of RVL-CDIP, and the top-5 retrievals are shown in following columns in order. Red and green borders are used to depict the incorrect and correct classes of retrieved documents respectively. For each task setting (\ie Vision  Language and Language  Vision, the same input example query is used.}
\label{fig:figure_3.4}
\end{figure*}


















\begin{table}[t]
\centering
\begin{center}
\resizebox{\columnwidth}{!} {{\begin{tabular}{@{}lrrr@{}}
    \hline
        Method & \# Pre-train Data & Accuracy (\%) & \#Params\\
    \hline
    \textit{Vision-only methods}\\
    \hline
\textbf{TransferDoc (V)}   & \textbf{320k} & \textbf{92.04} & \textbf{87M}   \\
        DiT~\cite{li2022dit}                       & 42M & 92.11 & 87M    \\

    \hline
    \textit{Text-only / (Text + Layout) methods}\\
    \hline
        BERT~\cite{devlin2018bert}                 & - & 89.81 & 110M      \\
        RoBERTa~\cite{liu2019roberta}              & - & 90.06 & 125M      \\
        LayoutLM~\cite{xu2020layoutlm}             & 11M & 91.78 & 113M  \\
        \textbf{TransferDoc (T)}   & \textbf{320k} & \textbf{93.13} & \textbf{125M}   \\
        LiLT~\cite{wang2022lilt}                   & 11M & 95.68 & 113M  \\
    \hline
    \textit{Vision + Text methods}\\
    \hline
\textbf{TransferDoc (V+T)}   & \textbf{320k} & \textbf{93.18} & \textbf{221M}   \\
    \hline
    \textit{Vision + Text + Layout methods}\\
    \hline
        SelfDoc~\cite{li2021selfdoc}                        & 320k & 92.81 &  -         \\
        LayoutLM~\cite{xu2020layoutlm}             & 11M & 94.42 & 160M       \\
        UDoc~\cite{gu2022unified}                           & 11M & 95.05 & 272M       \\
        TILT~\cite{powalski2021going}              & 1M & 95.25 & 230M       \\
        LayoutLMv2~\cite{xu2020layoutlmv2}         & 11M & 95.25 & 200M       \\
        LayoutLMv3~\cite{huang2022layoutlmv3}      & 11M & 95.44 & 133M       \\
        DocFormer~\cite{appalaraju2021docformer}   & 5M & 96.17 & 183M       \\
    \hline
    \end{tabular}}}
\end{center}
\caption{Top-1 accuracy (\%) comparison results of different VDU methods pre-trained in an SSL-based fashion. V, T, and L denote Vision, Text, and Layout modalities.}
\label{tab:table_5.1}
\end{table}
\noindent\textbf{Task III: Document Image Classification.}
The document image classification task aims to predict the category of visually rich document images. The fine-tuning process takes  iterations with a batch size of  and a learning rate of . We report in Table~\ref{tab:table_5.1} the classification performance on the test set of the RVL-CDIP dataset, where the metric used is the top-1 accuracy. The model achieves compelling performance in both uni-modal and multi-modal settings, achieving the best performance on the (V+T) setting with an accuracy of . TransferDoc enables to reduce the gap with related VDU works pre-trained on extensive amount of pre-training document data with the use of three modalities (\ie V+T+L) in both offline and online stages.













 
\section{Conclusion}
\label{sec:conclusion}
In this work, we approach the document representation learning problem by addressing the issue of VDU models relying on 2D word position embeddings and the extensive amount of pre-training document collections. We explore an industrial online setting where new classes appear regularly and large annotated dataset can not be used. To deal with these constraints, we proposed a more generalizable and flexible VDU model named TransferDoc which effectively incorporate cross-modal representations from language and vision. It exhibits superior performance over state-of-the-art models in both uni-modal and cross-modal tasks. Further research in this representation learning framework could be highly beneficial for the DocumentAI community.








 

\section{Acknowledgements}
This work has been co-funded by the French National Research Agency (ANR), and partially supported by the Spanish projects RTI2018-095645-B-C21, the Catalan project 2017-SGR-1783  and the CERCA Program / Generalitat de Catalunya.

\bibliographystyle{elsarticle-num}
\bibliography{cas-refs} 
\end{document}
