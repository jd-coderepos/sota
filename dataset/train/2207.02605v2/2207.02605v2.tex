\section{Experiments}
\label{sec:experiments}
In the section, we first introduce the adopted SemanticKITTI ~\citep{behley2019semantickitti} and nuScenes~\citep{caesar2020nuscenes} datasets and the mean IoU and accuracy metric for point cloud segmentation. We then provide the implementation details of GFNet, including the network architectures and training settings. After that, we perform extensive experiments to demonstrate the effectiveness of GFM and analyze the influences of different hyper-parameters in GFNet. Lastly, we compare the proposed GFNet with recent state-of-the-art point/projection-based methods to show our superiority.






\subsection{Datasets and Evaluation Metrics}
\label{sec:dataset}
\textbf{SemanticKITTI}~\citep{behley2019semantickitti}, derived from the KITTI Vision Benchmark~\citep{geiger2012we}, provides dense point-wise annotations for semantic segmentation task. The dataset presents 19 challenging classes and contains 43551 lidar scans from 22 sequences collected with a Velodyne HDL-64E lidar, where each scan contains approximately 130k points. Following~\cite{behley2019semantickitti,milioto2019rangenet++}, these 22 sequences are divided into 3 sets, \ie, training set (00 to 10 except 08 with 19130 scans), validation set (08 with 4071 scans) and testing set (11 to 21 with 20351 scans). We perform extensive experiments on the validation set to analyze the proposed method, and also report performance on the test set by submitting the result to the official test server. 

\textbf{nuScenes}~\citep{caesar2020nuscenes} is a  large-scale autonomous driving dataset, containing 1000 driving scenes of 20 second length in Boston and Singapore. Specifically, all driving scenes are officially divided into training (850 scenes) and validation set (150 scenes). By merging similar classes and removing rare classes, point cloud semantic segmentation task uses 16 classes, including 10 foreground and 6 background classes. We use the official test server to report the final performance on test set.

\textbf{Evaluation Metrics.}
Following \cite{behley2019semantickitti}, we use mean intersection-over-union (mIoU) over all classes as the evaluation metric. Mathematically, the mIoU can be defined as: 

where , , and  represent the numbers of true positive, false positive, and false negative predictions for the given class , respectively, and  is the number of classes. For a comprehensive comparison, we also report the accuracy among all samples, which can be formulated as:


\subsection{Implementation Details}
\label{sec:imple}
For SemanticKITTI, we use two branches to learn representations from RV/BEV in an end-to-end trainable way, where each branch follows an encoder-decoder architecture with a ResNet-34~\citep{he2016deep} as the backbone. The ASPP module~\citep{chen2017rethinking} is also used between the encoder and the decoder. The proposed geometric flow module (GFM) is incorporated into each upsampling layer. Note that the elements of  fed into GFM are scaled linearly according to the current flowing feature resolution. For RV branch, point clouds are first projected to a range image with the resolution , which is sequentially upsampled bilinearly to  where  is a scale factor. During training, a horizontal  random crop of RV image, \ie, , is used as data augmentation. On the other hand, we adopt polar partition~\citep{zhang2020polarnet} for BEV, and use a polar grid size of  to cover a space of  relative to the lidar sensor. The grid first goes through a mini PointNet~\citep{qi2017pointnet} to obtain the maximum feature activations along the  axis, leading to a reduced resolution  for BEV branch. We employ a SGD optimizer with momentum  and the weight decay . We use the cosine learning rate schedule~\citep{loshchilov2016sgdr} with warmup at the first epoch to 0.1. The backbone network is initialized using the pretrained weights from ImageNet~\citep{deng2009imagenet}. By default, we use  as the loss weight for Eq.\ref{eq:total_loss}. We train the proposed GFNet for 150 epochs using the batch size 16 on four NVIDIA A100-SXM4-40GB GPUs with AMD EPYC 7742 64-Core Processor.

For nuScenes, we adopt ~\cite{milioto2019rangenet++} to project point clouds to a RV image with the resolution  which is then upsampled bilinearly to  where  in our experiments. Besides, a polar grid size of  is used to cover a relative space of  for BEV branch. We train the model for total 400 epoch with batch size 56 using 8 NVIDIA A100-SXM4-40GB GPUs under AMD EPYC 7742 64-Core Processor. We adopt cosine learning rate schedule~\citep{loshchilov2016sgdr} with warmup at the first 10 epoch to 0.2. Other settings are kept the same with SemanticKITTI.



\subsection{Effectiveness of GFM}

In this part, we show the effectiveness of the proposed geometric flow module (GFM) as well as its influences on each single branch. As shown in Figure~\ref{fig:pipeline}, we denote the results from  and  as \textit{RV-Flow} and \textit{BEV-Flow}, respectively, in regard to the information flow between RV and BEV brought by GFM. The predictions from  (obtained by applying KPConv on the concatenation of  and ) are actually our final results, termed as \textit{GFNet}. Note that the above results are evaluated using  for Eq.\ref{eq:total_loss}. In addition, we train also each single branch separately without GFM modules, i.e., using  and  for \textit{RV-Single} and \textit{BEV-Single}, respectively.

\begin{table*}[b]
\centering
\caption{Quantitative comparisons in terms of mIoU to demonstrate the effectiveness of GFM on the validation set of SemanticKITTI.} 
\vspace{1mm}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccccccccccccccccc|c}
\hline
Method  
&\rotatebox{90}{car}		&\rotatebox{90}{bicycle}		&\rotatebox{90}{motorcycle}		&\rotatebox{90}{truck}			&\rotatebox{90}{other-vehicle}
&\rotatebox{90}{person} 	&\rotatebox{90}{bicyclist}		&\rotatebox{90}{motorcyclist} 	&\rotatebox{90}{road}			&\rotatebox{90}{parking} 	
&\rotatebox{90}{sidewalk}   &\rotatebox{90}{other-ground} 	&\rotatebox{90}{building} 		&\rotatebox{90}{fence} 			&\rotatebox{90}{vegetation} 
&\rotatebox{90}{trunk} 		&\rotatebox{90}{terrain} 		&\rotatebox{90}{pole} 			&\rotatebox{90}{traffic-sign}	&\rotatebox{90}{\textbf{mIoU}}    
\\ 
\hline
\textit{RV-Single} &
93.7 & 48.7 & 57.7 & 32.4 & 40.5 & 69.2 & 79.9 & 0.0 & 95.9 & 53.4 & 83.9 & 0.1 & 89.2 & 59.0 & 87.8 & 66.1 & 75.3 & 64.0 & 45.2 & 60.1  \\
\textit{RV-Flow} &
93.8 & 45.0 & 58.8 & 69.9 & 31.6 & 63.6 & 73.8 & 0.0 & 95.6 & 52.9 & 83.6 & 0.3 & 90.3 & 62.1 & 88.0 & 64.3 & 75.8 & 63.2 & 47.4 & \textbf{61.1} \\

\hline
\textit{BEV-Single} &
93.6 & 29.9 & 42.4 & 64.8 & 26.8 & 48.1 & 74.0 & 0.0 & 94.0 & 45.9 & 80.7 & 1.4 & 89.2 & 46.5 & 86.9 & 61.4 & 74.9 & 56.8 & 41.6 & 55.7 \\
\textit{BEV-Flow } &
93.7 & 43.7 & 61.2 & 74.0 & 31.0 & 61.6 & 80.6 & 0.0 & 95.3 & 53.1 & 82.8 & 0.2 & 90.8 & 61.4 & 88.0 & 63.1 & 75.6 & 58.9 & 43.1 & \textbf{61.0} \\
\hline
\textit{GFNet} &
94.2 & 49.7 & 63.2 & 74.9 & 32.1 & 69.3 & 83.2 & 0.0 & 95.7 & 53.8 & 83.8 & 0.2 & 91.2 & 62.9 & 88.5 & 66.1 & 76.2 & 64.1 & 48.3 & \textbf{63.0} \\

\hline
\end{tabular}
}
\label{tab:gfm}
\end{table*}

We compare the performances of \textit{RV/BEV-Single} and \textit{BEV/BEV-Flow} in Table~\ref{tab:gfm}. Specifically, we find that both RV and BEV branches have been improved by a clear margin when incorporating with the proposed GFM module, e.g.,  for BEV. Intuitively, RV is good at those vertically-extended objects like \textit{motorcycle} and \textit{person}, while BEV is sensitive to the classes with large and discriminative spatial size on the x-y plane. For example, \textit{RV-Single} only achieves  on \textit{truck} while \textit{BEV-Single} obtains , which is also illustrated by the first row of Figure~\ref{fig:vis_help} where RV predicts \textit{truck} as a mixture of \textit{truck, car} and \textit{other-vehicle}, but BEV acts much well. This is partially because \textit{truck} is more discriminative on x-y plane (captured by BEV) than vertical direction (captured by RV) compared to \textit{car, other-vehicle}. With the information flow from BEV to RV using GFM, \textit{RV-Flow} significantly boosts the performance from  to . A similar phenomenon can be observed in the second row of Figure~\ref{fig:vis_help}, where BEV misclassifies \textit{bicyclist} as \textit{trunk}, since both of them are vertically-extended and also very close to each other, while RV predicts precisely. With the help of RV, \textit{BEV-Flow} dramatically improves the performance from  to . When further applying KPConv on the concatenation of \textit{RV/BEV-Flow}, the proposed \textit{GFNet} achieves the best performance . These results demonstrate that the proposed GFM can effectively propagate complementary information between RV and BEV to boost the performance of each other, as well as the final performance.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/vis_help}
    \caption{Visualization of RV and BEV. The view with the cyan contour helps the one with red. By incorporating both RV and BEV, our GFNet makes more accurate predictions. 
    }
    \label{fig:vis_help}
\end{figure*}






\subsection{Ablation Studies}
\label{sec:ablation}

\begin{table}[!htb]
    \caption{Ablation studies of attention in GFM, loss weight coefficient  and scale factor  on the SemanticKITTI val set.}
    \begin{subtable}{.5\linewidth}
      \centering
      \vspace{-8mm}
        \caption{Attention in GFM}
       \begin{tabular}{p{1.6cm}<{\centering} p{1.6cm}<{\centering} p{1.6cm}<{\centering}}
\hline
\multicolumn{2}{c}{attention} & \multirow{2}{*}{mIoU} \\ \cline{1-2}
sigmoid         & softmax        &                           \\ \hline
                &                & 62.0                      \\ 
    &                & 62.9                      \\ 
                &    & 63.0                      \\ \hline
\end{tabular}
\label{tab:attention}
    \end{subtable}\begin{subtable}{.5\linewidth}
      \centering
        \caption{ and  under }
        \begin{tabular}{p{0.8cm}<{\centering} | p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering}}
\hline
cfg &  &  &  &  &  &   & mIoU \\
\hline
 & \textcolor{black}{} &  &  &  &  & 3 & 61.7 \\
 & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} &  &  & 3 & 61.8 \\
 & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & 3 & 62.4 \\
 & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & 3 & 63.0 \\
 & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & 2 & 61.7 \\
 & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & \textcolor{black}{} & 4 & 63.2   \\
\hline
\end{tabular}
\label{tab:loss_w}
\end{subtable} 
\label{tab:ablation}
\end{table}

In this subsection, we first explore the impacts of attention mechanism in GFM, the loss weights  defined in Eq.\ref{eq:total_loss}; and the scale factor  introduced in Sec.~\ref{sec:imple}. In the default setting, we use the softmax attention with  and .

As shown in Table~\ref{tab:attention}, without attention mechanism (\ie, no  and  in Figure~\ref{fig:gfm}), the performance  is obviously inferior to the counterparts  and , indicating that the attention operation helps to focus on the strengths instead of weaknesses of source view when fusing it into target view. If not otherwise stated, we use the softmax attention in our experiments.
We evaluate the influences of  in Table~\ref{tab:loss_w}, where , e.g., we have  the configuration . Specifically, when comparing the configurations  to  and , we see that that additional supervisions on dense 2D and each branch RV/BEV 3D predictions further improve model performance. When comparing  and , a large weight on 3D prediction  brings a better result. Therefore, if not otherwise stated, we adopt  for remaining experiments. The scale factor  in Sec.~\ref{sec:imple} indicates the resolution of RV image, e.g., when , we have  and  for training and testing, respectively. When comparing  and  in Table~\ref{tab:loss_w}, we find that a higher resolution significantly improves model performance, from  to , further enlarging  from 3 to 4 only brings a slightly better performance. For a better speed-accuracy tradeoff, we use  in the default setting.




\begin{table}[bt]
\centering
\caption{Comparisons under mIoU, Accuracy and Frame Per Second (FPS) on SemanticKITTI test set. Note that the results of methods with  are obtained from RangeNet++~\citep{milioto2019rangenet++}. From top to down, the methods are grouped into point-based, projection-based and multi-view fusion models.}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccccccccccccccccc|ccc}
\hline
Method  
&\rotatebox{90}{car}		&\rotatebox{90}{bicycle}		&\rotatebox{90}{motorcycle}		&\rotatebox{90}{truck}			&\rotatebox{90}{other-vehicle}
&\rotatebox{90}{person} 	&\rotatebox{90}{bicyclist}		&\rotatebox{90}{motorcyclist} 	&\rotatebox{90}{road}			&\rotatebox{90}{parking} 	
&\rotatebox{90}{sidewalk}   &\rotatebox{90}{other-ground} 	&\rotatebox{90}{building} 		&\rotatebox{90}{fence} 			&\rotatebox{90}{vegetation} 
&\rotatebox{90}{trunk} 		&\rotatebox{90}{terrain} 		&\rotatebox{90}{pole} 			&\rotatebox{90}{traffic-sign}	&\rotatebox{90}{\textbf{mIoU}}   &\rotatebox{90}{\textbf{Accuracy}}
&\rotatebox{90}{\textbf{FPS}}
\\ 
\hline   
PointNet~\citep{qi2017pointnet}& 46.3 & 1.3  & 0.3  &  0.1 & 0.8  & 0.2  & 0.2 & 0.0  &   61.6  & 15.8 & 35.7 & 1.4 & 41.4 & 12.9 & 31.0& 4.6& 17.6& 2.4 & 3.7 & 14.6 & - & 2 \\
PointNet++ \citep{qi2017pointnet++} & & & & & & & & & & & & & & & & & & &  &  & - & 0.1\\
TangentConv  \citep{tatarchenko2018tangent}& & & & & & & & & & & & & & & & & & &  &  & - & 0.3\\
PointASNL \citep{yan2020pointasnl} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & 70.6 &  &  &  & - & - \\
RandLa-Net~\citep{hu2020randla} & 
94.2 & 26.0 &  25.8 & 40.1 & 38.9 & 49.2 & 48.2 &7.2 & 90.7 & 60.3 & 73.7 & 20.4 & 86.9 & 56.3 & 81.4 & 61.3 & 66.8 & 49.2 & 47.7 & 53.9 & 88.8 & 22 \\ 

KPConv~\citep{thomas2019kpconv} &
96.0 & 30.2 & 42.5 & 33.4 & 44.3 & 61.5 & 61.6 & 11.8 & 88.8 & 61.3 & 72.7 & 31.6 & 90.5 & 64.2 & 84.8 & 69.2 & 69.1 & 56.4 & 47.4 & 58.8 & 90.3 & -\\
\hline
SqueezeSeg \citep{wu2018squeezeseg} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & - & 55\\
SqueezeSegV2 \citep{wu2019squeezesegv2} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  & - & 50\\


RangeNet++~\citep{milioto2019rangenet++} &
91.4 & 25.7 & 34.4 & 25.7 & 23.0 & 38.3 & 38.8 & 4.8 & 91.8 & 65.0 & 75.2 & 27.8 & 87.4 & 58.6 & 80.5 & 55.1 & 64.6 & 47.9 & 55.9 & 52.2 & 89.0 & 12 \\

PolarNet~\citep{zhang2020polarnet} & 93.8 & 40.3  & 30.1  & 22.9  & 28.5  & 43.2  & 40.2 &  5.6 & 90.8  & 61.7 & 74.4 & 21.7 & 90.0 & 61.3 & 84.0 & 65.5 & 67.8 & 51.8  & 57.5 & 54.3 & 90.0 & 16\\

3D-MiniNet-KNN~\citep{alonso20203d} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &   &  & 89.7 & 28\\

SqueezeSegV3~\citep{xu2020squeezesegv3} &   
92.5 & 38.7 & 36.5 & 29.6 & 33.0 & 45.6 & 46.2 & 20.1 & 91.7 & 63.4  & 74.8 & 26.4 & 89.0 & 59.4 & 82.0 & 58.7 & 65.4 & 49.6 & 58.9 & 55.9 & 89.5 & 6 \\

SalsaNext~\citep{cortinhal2020salsanext} &
91.9 & 48.3 & 38.6 & 38.9 & 31.9 & 60.2 & 59.0 & 19.4 & 91.7 & 63.7 & 75.8 & 29.1 & 90.2 & 64.2 & 81.8 & 63.6 & 66.5 & 54.3 & 62.1 & 59.5 & 90.0 & 24\\

\hline
MVLidarNet~\citep{chen2020mvlidarnet} &
87.1 & 34.9 & 32.9 & 23.7 & 24.9 & 44.5 & 44.3 & 23.1 & 90.3 & 56.7 & 73.0 & 19.1 & 85.6 & 53.0 & 80.9 & 59.4 & 63.9 & 49.9 & 51.1 & 52.5 & 88.0 & 92\\
MPF~\citep{alnaggar2021multi} & 
93.4 & 30.2  & 38.3 & 26.1 & 28.5  & 48.1 & 46.1  & 18.1 & 90.6 & 62.3  & 74.5 & 30.6   & 88.5 & 59.7 & 83.5 & 59.7 & 69.2 & 49.7 & 58.1 & 55.5 & - & 21\\
TORNADONet~\citep{gerdzhev2021tornado} &
94.2 & 55.7 & 48.1 & 40.0 & 38.2 & 63.6 & 60.1 & 34.9 & 89.7 & 66.3 & 74.5 & 28.7 & 91.3 & 65.6 & 85.6 & 67.0 & 71.5 & 58.0 & 65.9 & 63.1 & 90.7 & 4 \\
AMVNet~\citep{liong2020amvnet} &
96.2 & 59.9 & 54.2 & 48.8 & 45.7 & 71.0 & 65.7 & 11.0 & 90.1 & 71.0 & 75.8 & 32.4 & 92.4 & 69.1 & 85.6 & 71.7 & 69.6 & 62.7 & 67.2 & 65.3 & 91.3 & -\\
 
\rowcolor{Gray} GFNet (ours) &
96.0 & 53.2 & 48.3 & 31.7 & 47.3 & 62.8 & 57.3 & 44.7 & 93.6 & 72.5 & 80.8 & 31.2 & 94.0 & 73.9 & 85.2 & 71.1 & 69.3 & 61.8 & 68.0 & 65.4 & 92.4 & 10\\
\hline
\end{tabular}
}
\label{tab:quanresults}
\end{table}

\subsection{Comparison with Recent State-of-the-Arts}

\textbf{SemanticKITTI.} For fair comparison with recent  methods, we follow the same setting in~\cite{behley2019semantickitti,kochanov2020kprnet}, \ie, both training and validation splits are used for training when evaluating on the test server. As shown in Table~\ref{tab:quanresults}, GFNet achieves the new state-of-the-art performance  mIoU, significantly surpassing point-based methods (\eg,  for KPConv~\citep{thomas2019kpconv}) and single view models (\eg,  for SalsaNext~\citep{cortinhal2020salsanext}). For multi-view approaches~\citep{alnaggar2021multi,chen2020mvlidarnet,gerdzhev2021tornado,liong2020amvnet}, GFNet outperforms recent methods~\cite{alnaggar2021multi,chen2020mvlidarnet,gerdzhev2021tornado} by a large margin. Comparing with AMVNet~\cite{liong2020amvnet}, GFNet clearly outperforms it on the point-wise accuracy, i.e.,  \textit{vs.} . The superior performance of GFNet shows the effectiveness of bidirectionally aligning and propagating geometric information between RV/BEV.
Notably, AMVNet requires to train models for RV/BEV branch as well as their post-processing point head separately, while GFNet is end-to-end trainable. 
Additionally, since the acquisition frequency of the Velodyne HDL-64E LiDAR sensor (used by SemanticKITTI) is 10 Hz, GFNet can thus run in real-time, i.e., 10 FPS.

\textbf{nuScenes.} To evaluate the generalizability of GFNet, we also report the performance on the testset in Table~\ref{tab:nuscenes_test} by submitting results to the test server. Similarly, GFNet achieves superior mIoU performance , which remarkably outperforms PolarNet~\citep{zhang2020polarnet} and tights AMVNet~\citep{liong2020amvnet}. However, our result  under Frequency Weighted IoU (FW IoU) beats  from AMVNet~\citep{liong2020amvnet}, which is consistent with the accuracy comparison on SemanticKITTI. It also reveals that GFNet performs much better on frequent classes while somewhat struggles on those rare/small classes. Despite the good performance of GFNet, it is also interesting to further improve  GFNet by addressing rare/small classes from the perspectives of data sampling/augmentation and loss function.

\begin{table}[t]
\centering
\caption{Comparisons on nuScenes (the test set) under mIoU and Frequency Weighted IoU (or FW IoU).}
\label{tab:nuscenes_test}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccccccccccccc|cc}
\hline
Method  
&\rotatebox{90}{barrier}		&\rotatebox{90}{bicycle}		&\rotatebox{90}{bus}		&\rotatebox{90}{car}			&\rotatebox{90}{const-vehicle}
&\rotatebox{90}{motorcycle} 	&\rotatebox{90}{pedestrian}		&\rotatebox{90}{traffic-cone} 	&\rotatebox{90}{trailer}			&\rotatebox{90}{truck} 	
&\rotatebox{90}{dri-surface}   &\rotatebox{90}{other-flat} 	&\rotatebox{90}{sidewalk} 		&\rotatebox{90}{terrain} 			&\rotatebox{90}{manmade} 
&\rotatebox{90}{vegetation} 		&\rotatebox{90}{\textbf{mIoU}}   &\rotatebox{90}{\textbf{FW IoU}}  
\\ 
\hline
PolarNet~\citep{zhang2020polarnet} &  72.2 & 16.8  & 77.0   & 86.5  & 51.1  & 69.7  & 64.8 &   54.1 &  69.7 & 63.4 & 96.6  & 67.1 & 77.7 & 72.1& 87.1& 84.4& 69.4 &  87.4 \\

AMVNet~\citep{liong2020amvnet}  & 79.8 & 32.4  & 82.2 & 86.4  & 62.5  & 81.9  & 75.3 & 72.3  & 83.5  & 65.1 & 97.4 & 67.0  & 78.8 & 74.6 & 90.8 & 87.9 & 76.1 & 89.5 \\
 
\rowcolor{Gray} GFNet (ours) &
81.1 & 31.6 & 76.0 & 90.5 & 60.2 & 80.7 & 75.3 & 71.8 & 82.5 & 65.1 & 97.8 & 67.0 & 80.4 & 76.2 & 91.8 & 88.9 & 76.1 & 90.4 \\
\hline
\end{tabular}
}
\end{table}









