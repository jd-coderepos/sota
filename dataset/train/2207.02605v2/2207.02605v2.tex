\section{Experiments}
\label{sec:experiments}
In the section, we first introduce the adopted SemanticKITTI ~\citep{behley2019semantickitti} and nuScenes~\citep{caesar2020nuscenes} datasets and the mean IoU and accuracy metric for point cloud segmentation. We then provide the implementation details of GFNet, including the network architectures and training settings. After that, we perform extensive experiments to demonstrate the effectiveness of GFM and analyze the influences of different hyper-parameters in GFNet. Lastly, we compare the proposed GFNet with recent state-of-the-art point/projection-based methods to show our superiority.






\subsection{Datasets and Evaluation Metrics}
\label{sec:dataset}
\textbf{SemanticKITTI}~\citep{behley2019semantickitti}, derived from the KITTI Vision Benchmark~\citep{geiger2012we}, provides dense point-wise annotations for semantic segmentation task. The dataset presents 19 challenging classes and contains 43551 lidar scans from 22 sequences collected with a Velodyne HDL-64E lidar, where each scan contains approximately 130k points. Following~\cite{behley2019semantickitti,milioto2019rangenet++}, these 22 sequences are divided into 3 sets, \ie, training set (00 to 10 except 08 with 19130 scans), validation set (08 with 4071 scans) and testing set (11 to 21 with 20351 scans). We perform extensive experiments on the validation set to analyze the proposed method, and also report performance on the test set by submitting the result to the official test server. 

\textbf{nuScenes}~\citep{caesar2020nuscenes} is a  large-scale autonomous driving dataset, containing 1000 driving scenes of 20 second length in Boston and Singapore. Specifically, all driving scenes are officially divided into training (850 scenes) and validation set (150 scenes). By merging similar classes and removing rare classes, point cloud semantic segmentation task uses 16 classes, including 10 foreground and 6 background classes. We use the official test server to report the final performance on test set.

\textbf{Evaluation Metrics.}
Following \cite{behley2019semantickitti}, we use mean intersection-over-union (mIoU) over all classes as the evaluation metric. Mathematically, the mIoU can be defined as: 
\begin{equation}
mIoU=\frac{1}{C}\sum_{c=1}^C\frac{TP_c}{TP_c+FP_c+FN_c},
\label{mIoU}
\end{equation}
where $TP_c$, $FP_c$, and $FN_c$ represent the numbers of true positive, false positive, and false negative predictions for the given class $c$, respectively, and $C$ is the number of classes. For a comprehensive comparison, we also report the accuracy among all samples, which can be formulated as:
\begin{equation}
Accuracy=\frac{TP+TN}{TP+FP+FN+TN}.
\label{acc}
\end{equation}

\subsection{Implementation Details}
\label{sec:imple}
For SemanticKITTI, we use two branches to learn representations from RV/BEV in an end-to-end trainable way, where each branch follows an encoder-decoder architecture with a ResNet-34~\citep{he2016deep} as the backbone. The ASPP module~\citep{chen2017rethinking} is also used between the encoder and the decoder. The proposed geometric flow module (GFM) is incorporated into each upsampling layer. Note that the elements of $\mathbf{T}_{R\rightarrow B}, \mathbf{T}_{B\rightarrow R}$ fed into GFM are scaled linearly according to the current flowing feature resolution. For RV branch, point clouds are first projected to a range image with the resolution $[64, 2048]$, which is sequentially upsampled bilinearly to $[64\times2S, 2048\times S]$ where $S$ is a scale factor. During training, a horizontal $1/4$ random crop of RV image, \ie, $[128S, 512S]$, is used as data augmentation. On the other hand, we adopt polar partition~\citep{zhang2020polarnet} for BEV, and use a polar grid size of $[480, 360, 32]$ to cover a space of $[radius: (3m, 50m), z: (-3m,1.5m)]$ relative to the lidar sensor. The grid first goes through a mini PointNet~\citep{qi2017pointnet} to obtain the maximum feature activations along the $z$ axis, leading to a reduced resolution $[480, 360]$ for BEV branch. We employ a SGD optimizer with momentum $0.9$ and the weight decay $1e-4$. We use the cosine learning rate schedule~\citep{loshchilov2016sgdr} with warmup at the first epoch to 0.1. The backbone network is initialized using the pretrained weights from ImageNet~\citep{deng2009imagenet}. By default, we use $\lambda=[2.0,2.0,2.0,1.0,1.0]$ as the loss weight for Eq.\ref{eq:total_loss}. We train the proposed GFNet for 150 epochs using the batch size 16 on four NVIDIA A100-SXM4-40GB GPUs with AMD EPYC 7742 64-Core Processor.

For nuScenes, we adopt ~\cite{milioto2019rangenet++} to project point clouds to a RV image with the resolution $[32, 1024]$ which is then upsampled bilinearly to $[32\times3S, 1024\times S]$ where $S=4$ in our experiments. Besides, a polar grid size of $[480, 360, 32]$ is used to cover a relative space of $[radius: (0m, 50m), z: (-5m, 3m)]$ for BEV branch. We train the model for total 400 epoch with batch size 56 using 8 NVIDIA A100-SXM4-40GB GPUs under AMD EPYC 7742 64-Core Processor. We adopt cosine learning rate schedule~\citep{loshchilov2016sgdr} with warmup at the first 10 epoch to 0.2. Other settings are kept the same with SemanticKITTI.



\subsection{Effectiveness of GFM}

In this part, we show the effectiveness of the proposed geometric flow module (GFM) as well as its influences on each single branch. As shown in Figure~\ref{fig:pipeline}, we denote the results from $\mathbf{F_r}$ and $\mathbf{F_b}$ as \textit{RV-Flow} and \textit{BEV-Flow}, respectively, in regard to the information flow between RV and BEV brought by GFM. The predictions from $\mathbf{F_f}$ (obtained by applying KPConv on the concatenation of $\mathbf{F_r}$ and $\mathbf{F_b}$) are actually our final results, termed as \textit{GFNet}. Note that the above results are evaluated using $\lambda=[2,2,2,1,1]$ for Eq.\ref{eq:total_loss}. In addition, we train also each single branch separately without GFM modules, i.e., using $\lambda=[0,2,0,1,0]$ and $\lambda=[0,0,2,0,1]$ for \textit{RV-Single} and \textit{BEV-Single}, respectively.

\begin{table*}[b]
\centering
\caption{Quantitative comparisons in terms of mIoU to demonstrate the effectiveness of GFM on the validation set of SemanticKITTI.} 
\vspace{1mm}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccccccccccccccccc|c}
\hline
Method  
&\rotatebox{90}{car}		&\rotatebox{90}{bicycle}		&\rotatebox{90}{motorcycle}		&\rotatebox{90}{truck}			&\rotatebox{90}{other-vehicle}
&\rotatebox{90}{person} 	&\rotatebox{90}{bicyclist}		&\rotatebox{90}{motorcyclist} 	&\rotatebox{90}{road}			&\rotatebox{90}{parking} 	
&\rotatebox{90}{sidewalk}   &\rotatebox{90}{other-ground} 	&\rotatebox{90}{building} 		&\rotatebox{90}{fence} 			&\rotatebox{90}{vegetation} 
&\rotatebox{90}{trunk} 		&\rotatebox{90}{terrain} 		&\rotatebox{90}{pole} 			&\rotatebox{90}{traffic-sign}	&\rotatebox{90}{\textbf{mIoU}}    
\\ 
\hline
\textit{RV-Single} &
93.7 & 48.7 & 57.7 & 32.4 & 40.5 & 69.2 & 79.9 & 0.0 & 95.9 & 53.4 & 83.9 & 0.1 & 89.2 & 59.0 & 87.8 & 66.1 & 75.3 & 64.0 & 45.2 & 60.1  \\
\textit{RV-Flow} &
93.8 & 45.0 & 58.8 & 69.9 & 31.6 & 63.6 & 73.8 & 0.0 & 95.6 & 52.9 & 83.6 & 0.3 & 90.3 & 62.1 & 88.0 & 64.3 & 75.8 & 63.2 & 47.4 & \textbf{61.1} \\

\hline
\textit{BEV-Single} &
93.6 & 29.9 & 42.4 & 64.8 & 26.8 & 48.1 & 74.0 & 0.0 & 94.0 & 45.9 & 80.7 & 1.4 & 89.2 & 46.5 & 86.9 & 61.4 & 74.9 & 56.8 & 41.6 & 55.7 \\
\textit{BEV-Flow } &
93.7 & 43.7 & 61.2 & 74.0 & 31.0 & 61.6 & 80.6 & 0.0 & 95.3 & 53.1 & 82.8 & 0.2 & 90.8 & 61.4 & 88.0 & 63.1 & 75.6 & 58.9 & 43.1 & \textbf{61.0} \\
\hline
\textit{GFNet} &
94.2 & 49.7 & 63.2 & 74.9 & 32.1 & 69.3 & 83.2 & 0.0 & 95.7 & 53.8 & 83.8 & 0.2 & 91.2 & 62.9 & 88.5 & 66.1 & 76.2 & 64.1 & 48.3 & \textbf{63.0} \\

\hline
\end{tabular}
}
\label{tab:gfm}
\end{table*}

We compare the performances of \textit{RV/BEV-Single} and \textit{BEV/BEV-Flow} in Table~\ref{tab:gfm}. Specifically, we find that both RV and BEV branches have been improved by a clear margin when incorporating with the proposed GFM module, e.g., $55.7\% \rightarrow 61.0\%$ for BEV. Intuitively, RV is good at those vertically-extended objects like \textit{motorcycle} and \textit{person}, while BEV is sensitive to the classes with large and discriminative spatial size on the x-y plane. For example, \textit{RV-Single} only achieves $32.4\%$ on \textit{truck} while \textit{BEV-Single} obtains $64.8\%$, which is also illustrated by the first row of Figure~\ref{fig:vis_help} where RV predicts \textit{truck} as a mixture of \textit{truck, car} and \textit{other-vehicle}, but BEV acts much well. This is partially because \textit{truck} is more discriminative on x-y plane (captured by BEV) than vertical direction (captured by RV) compared to \textit{car, other-vehicle}. With the information flow from BEV to RV using GFM, \textit{RV-Flow} significantly boosts the performance from $32.4\%$ to $69.9\%$. A similar phenomenon can be observed in the second row of Figure~\ref{fig:vis_help}, where BEV misclassifies \textit{bicyclist} as \textit{trunk}, since both of them are vertically-extended and also very close to each other, while RV predicts precisely. With the help of RV, \textit{BEV-Flow} dramatically improves the performance from $55.7\%$ to $61.0\%$. When further applying KPConv on the concatenation of \textit{RV/BEV-Flow}, the proposed \textit{GFNet} achieves the best performance $63.0\%$. These results demonstrate that the proposed GFM can effectively propagate complementary information between RV and BEV to boost the performance of each other, as well as the final performance.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/vis_help}
    \caption{Visualization of RV and BEV. The view with the cyan contour helps the one with red. By incorporating both RV and BEV, our GFNet makes more accurate predictions. 
    }
    \label{fig:vis_help}
\end{figure*}






\subsection{Ablation Studies}
\label{sec:ablation}

\begin{table}[!htb]
    \caption{Ablation studies of attention in GFM, loss weight coefficient $\lambda$ and scale factor $S$ on the SemanticKITTI val set.}
    \begin{subtable}{.5\linewidth}
      \centering
      \vspace{-8mm}
        \caption{Attention in GFM}
       \begin{tabular}{p{1.6cm}<{\centering} p{1.6cm}<{\centering} p{1.6cm}<{\centering}}
\hline
\multicolumn{2}{c}{attention} & \multirow{2}{*}{mIoU} \\ \cline{1-2}
sigmoid         & softmax        &                           \\ \hline
                &                & 62.0                      \\ 
$\checkmark$    &                & 62.9                      \\ 
                & $\checkmark$   & 63.0                      \\ \hline
\end{tabular}
\label{tab:attention}
    \end{subtable}\begin{subtable}{.5\linewidth}
      \centering
        \caption{$\lambda$ and $S$ under $\bigtriangleup=1, \bigtriangledown=2$}
        \begin{tabular}{p{0.8cm}<{\centering} | p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{0.4cm}<{\centering} p{1cm}<{\centering} p{1cm}<{\centering}}
\hline
cfg & $\alpha$ & $\beta$ & $\gamma$ & $\rho$ & $\sigma$ & $S$  & mIoU \\
\hline
$a$ & \textcolor{black}{$\bigtriangleup$} &  &  &  &  & 3 & 61.7 \\
$b$ & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} &  &  & 3 & 61.8 \\
$c$ & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & 3 & 62.4 \\
$d$ & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & 3 & 63.0 \\
$e$ & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & 2 & 61.7 \\
$f$ & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangledown$} & \textcolor{black}{$\bigtriangleup$} & \textcolor{black}{$\bigtriangleup$} & 4 & 63.2   \\
\hline
\end{tabular}
\label{tab:loss_w}
\end{subtable} 
\label{tab:ablation}
\end{table}

In this subsection, we first explore the impacts of attention mechanism in GFM, the loss weights $\lambda$ defined in Eq.\ref{eq:total_loss}; and the scale factor $S$ introduced in Sec.~\ref{sec:imple}. In the default setting, we use the softmax attention with $\lambda=[2,2,2,1,1]$ and $S=3$.

As shown in Table~\ref{tab:attention}, without attention mechanism (\ie, no $\theta(\cdot)$ and $\otimes$ in Figure~\ref{fig:gfm}), the performance $62.0\%$ is obviously inferior to the counterparts $62.9\%$ and $63.0\%$, indicating that the attention operation helps to focus on the strengths instead of weaknesses of source view when fusing it into target view. If not otherwise stated, we use the softmax attention in our experiments.
We evaluate the influences of $\lambda \doteq[\alpha, \beta, \gamma,\rho, \sigma]$ in Table~\ref{tab:loss_w}, where $\bigtriangleup=1, \bigtriangledown=2$, e.g., we have $\lambda \doteq[\alpha, \beta, \gamma,\rho, \sigma]=[2.0,2.0,2.0,1.0,1.0]$ the configuration $d$. Specifically, when comparing the configurations $a$ to $b$ and $c$, we see that that additional supervisions on dense 2D and each branch RV/BEV 3D predictions further improve model performance. When comparing $c$ and $d$, a large weight on 3D prediction  brings a better result. Therefore, if not otherwise stated, we adopt $\lambda=[2.0,2.0,2.0,1.0,1.0]$ for remaining experiments. The scale factor $S$ in Sec.~\ref{sec:imple} indicates the resolution of RV image, e.g., when $S=3$, we have $[128S, 512S]=[384, 1536]$ and $[128S, 2048S]=[383, 6144]$ for training and testing, respectively. When comparing $d$ and $e$ in Table~\ref{tab:loss_w}, we find that a higher resolution significantly improves model performance, from $61.7\%$ to $63.0\%$, further enlarging $S$ from 3 to 4 only brings a slightly better performance. For a better speed-accuracy tradeoff, we use $S=3$ in the default setting.




\begin{table}[bt]
\centering
\caption{Comparisons under mIoU, Accuracy and Frame Per Second (FPS) on SemanticKITTI test set. Note that the results of methods with $^*$ are obtained from RangeNet++~\citep{milioto2019rangenet++}. From top to down, the methods are grouped into point-based, projection-based and multi-view fusion models.}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccccccccccccccccc|ccc}
\hline
Method  
&\rotatebox{90}{car}		&\rotatebox{90}{bicycle}		&\rotatebox{90}{motorcycle}		&\rotatebox{90}{truck}			&\rotatebox{90}{other-vehicle}
&\rotatebox{90}{person} 	&\rotatebox{90}{bicyclist}		&\rotatebox{90}{motorcyclist} 	&\rotatebox{90}{road}			&\rotatebox{90}{parking} 	
&\rotatebox{90}{sidewalk}   &\rotatebox{90}{other-ground} 	&\rotatebox{90}{building} 		&\rotatebox{90}{fence} 			&\rotatebox{90}{vegetation} 
&\rotatebox{90}{trunk} 		&\rotatebox{90}{terrain} 		&\rotatebox{90}{pole} 			&\rotatebox{90}{traffic-sign}	&\rotatebox{90}{\textbf{mIoU}}   &\rotatebox{90}{\textbf{Accuracy}}
&\rotatebox{90}{\textbf{FPS}}
\\ 
\hline   
PointNet$^*$~\citep{qi2017pointnet}& 46.3 & 1.3  & 0.3  &  0.1 & 0.8  & 0.2  & 0.2 & 0.0  &   61.6  & 15.8 & 35.7 & 1.4 & 41.4 & 12.9 & 31.0& 4.6& 17.6& 2.4 & 3.7 & 14.6 & - & 2 \\
PointNet++$^*$ \citep{qi2017pointnet++} & $53.7$& $1.9$& $0.2$& $0.9$& $0.2$& $0.9$& $1.0$& $0.0$& $72.0$& $18.7$& $41.8$& $5.6$& $62.3$& $16.9$& $46.5$& $13.8$& $30.0$& $6.0$& $8.9$ & $20.1$ & - & 0.1\\
TangentConv$^*$  \citep{tatarchenko2018tangent}& $86.8$& $1.3$& $12.7$& $11.6$& $10.2$& $17.1$& $20.2$& $0.5$& $82.9$& $15.2$& $61.7$& $9.0$& $82.8$& $44.2$& $75.5$& $42.5$& $55.5$& $30.2$& $22.2$ & $35.9$ & - & 0.3\\
PointASNL \citep{yan2020pointasnl} & $87.9$ & $0$ & $25.1$ & $39.0$ & $29.2$ & $34.2$ & $57.6$ & $0$ & $87.4$ & $24.3$ & $74.3$ & $1.8$ & $83.1$ & $43.9$ & $84.1$ & $52.2$ & 70.6 & $57.8$ & $36.9$ & $46.8$ & - & - \\
RandLa-Net~\citep{hu2020randla} & 
94.2 & 26.0 &  25.8 & 40.1 & 38.9 & 49.2 & 48.2 &7.2 & 90.7 & 60.3 & 73.7 & 20.4 & 86.9 & 56.3 & 81.4 & 61.3 & 66.8 & 49.2 & 47.7 & 53.9 & 88.8 & 22 \\ 

KPConv~\citep{thomas2019kpconv} &
96.0 & 30.2 & 42.5 & 33.4 & 44.3 & 61.5 & 61.6 & 11.8 & 88.8 & 61.3 & 72.7 & 31.6 & 90.5 & 64.2 & 84.8 & 69.2 & 69.1 & 56.4 & 47.4 & 58.8 & 90.3 & -\\
\hline
SqueezeSeg$^*$ \citep{wu2018squeezeseg} & $68.3$ & $18.1$ & $5.1$ & $4.1$ & $4.8$ & $16.5$ & $17.3$ & $1.2$ & $84.9$ & $28.4$ & $54.7$ & $4.6$ & $61.5$ & $29.2$ & $59.6$ & $25.5$ & $54.7$ & $11.2$ & $36.3$ & $30.8$ & - & 55\\
SqueezeSegV2$^*$ \citep{wu2019squeezesegv2} & $81.8$ & $18.5$ & $17.9$ & $13.4$ & $14.0$ & $20.1$ & $25.1$ & $3.9$ & $88.6$ & $45.8$ & $67.6$ & $17.7$ & $73.7$ & $41.1$ & $71.8$ & $35.8$ & $60.2$ & $20.2$ & $36.3$ & $39.7$ & - & 50\\


RangeNet++~\citep{milioto2019rangenet++} &
91.4 & 25.7 & 34.4 & 25.7 & 23.0 & 38.3 & 38.8 & 4.8 & 91.8 & 65.0 & 75.2 & 27.8 & 87.4 & 58.6 & 80.5 & 55.1 & 64.6 & 47.9 & 55.9 & 52.2 & 89.0 & 12 \\

PolarNet~\citep{zhang2020polarnet} & 93.8 & 40.3  & 30.1  & 22.9  & 28.5  & 43.2  & 40.2 &  5.6 & 90.8  & 61.7 & 74.4 & 21.7 & 90.0 & 61.3 & 84.0 & 65.5 & 67.8 & 51.8  & 57.5 & 54.3 & 90.0 & 16\\

3D-MiniNet-KNN~\citep{alonso20203d} & $90.5$ & $42.3$ & $42.1$ & $28.5$ & $29.4$ & $47.8$ & $44.1$ & $14.5$ & $91.6$ & $64.2$ & $74.5$ & $25.4$ & $89.4$ & $60.8$ & $82.8$ & $60.8$ & $66.7$ & $48.0$ & $56.6$  & $55.8$ & 89.7 & 28\\

SqueezeSegV3~\citep{xu2020squeezesegv3} &   
92.5 & 38.7 & 36.5 & 29.6 & 33.0 & 45.6 & 46.2 & 20.1 & 91.7 & 63.4  & 74.8 & 26.4 & 89.0 & 59.4 & 82.0 & 58.7 & 65.4 & 49.6 & 58.9 & 55.9 & 89.5 & 6 \\

SalsaNext~\citep{cortinhal2020salsanext} &
91.9 & 48.3 & 38.6 & 38.9 & 31.9 & 60.2 & 59.0 & 19.4 & 91.7 & 63.7 & 75.8 & 29.1 & 90.2 & 64.2 & 81.8 & 63.6 & 66.5 & 54.3 & 62.1 & 59.5 & 90.0 & 24\\

\hline
MVLidarNet~\citep{chen2020mvlidarnet} &
87.1 & 34.9 & 32.9 & 23.7 & 24.9 & 44.5 & 44.3 & 23.1 & 90.3 & 56.7 & 73.0 & 19.1 & 85.6 & 53.0 & 80.9 & 59.4 & 63.9 & 49.9 & 51.1 & 52.5 & 88.0 & 92\\
MPF~\citep{alnaggar2021multi} & 
93.4 & 30.2  & 38.3 & 26.1 & 28.5  & 48.1 & 46.1  & 18.1 & 90.6 & 62.3  & 74.5 & 30.6   & 88.5 & 59.7 & 83.5 & 59.7 & 69.2 & 49.7 & 58.1 & 55.5 & - & 21\\
TORNADONet~\citep{gerdzhev2021tornado} &
94.2 & 55.7 & 48.1 & 40.0 & 38.2 & 63.6 & 60.1 & 34.9 & 89.7 & 66.3 & 74.5 & 28.7 & 91.3 & 65.6 & 85.6 & 67.0 & 71.5 & 58.0 & 65.9 & 63.1 & 90.7 & 4 \\
AMVNet~\citep{liong2020amvnet} &
96.2 & 59.9 & 54.2 & 48.8 & 45.7 & 71.0 & 65.7 & 11.0 & 90.1 & 71.0 & 75.8 & 32.4 & 92.4 & 69.1 & 85.6 & 71.7 & 69.6 & 62.7 & 67.2 & 65.3 & 91.3 & -\\
 
\rowcolor{Gray} GFNet (ours) &
96.0 & 53.2 & 48.3 & 31.7 & 47.3 & 62.8 & 57.3 & 44.7 & 93.6 & 72.5 & 80.8 & 31.2 & 94.0 & 73.9 & 85.2 & 71.1 & 69.3 & 61.8 & 68.0 & 65.4 & 92.4 & 10\\
\hline
\end{tabular}
}
\label{tab:quanresults}
\end{table}

\subsection{Comparison with Recent State-of-the-Arts}

\textbf{SemanticKITTI.} For fair comparison with recent  methods, we follow the same setting in~\cite{behley2019semantickitti,kochanov2020kprnet}, \ie, both training and validation splits are used for training when evaluating on the test server. As shown in Table~\ref{tab:quanresults}, GFNet achieves the new state-of-the-art performance $65.4\%$ mIoU, significantly surpassing point-based methods (\eg, $58.8\%$ for KPConv~\citep{thomas2019kpconv}) and single view models (\eg, $59.5\%$ for SalsaNext~\citep{cortinhal2020salsanext}). For multi-view approaches~\citep{alnaggar2021multi,chen2020mvlidarnet,gerdzhev2021tornado,liong2020amvnet}, GFNet outperforms recent methods~\cite{alnaggar2021multi,chen2020mvlidarnet,gerdzhev2021tornado} by a large margin. Comparing with AMVNet~\cite{liong2020amvnet}, GFNet clearly outperforms it on the point-wise accuracy, i.e., $92.4\%$ \textit{vs.} $91.3\%$. The superior performance of GFNet shows the effectiveness of bidirectionally aligning and propagating geometric information between RV/BEV.
Notably, AMVNet requires to train models for RV/BEV branch as well as their post-processing point head separately, while GFNet is end-to-end trainable. 
Additionally, since the acquisition frequency of the Velodyne HDL-64E LiDAR sensor (used by SemanticKITTI) is 10 Hz, GFNet can thus run in real-time, i.e., 10 FPS.

\textbf{nuScenes.} To evaluate the generalizability of GFNet, we also report the performance on the testset in Table~\ref{tab:nuscenes_test} by submitting results to the test server. Similarly, GFNet achieves superior mIoU performance $76.1\%$, which remarkably outperforms PolarNet~\citep{zhang2020polarnet} and tights AMVNet~\citep{liong2020amvnet}. However, our result $90.4\%$ under Frequency Weighted IoU (FW IoU) beats $89.5\%$ from AMVNet~\citep{liong2020amvnet}, which is consistent with the accuracy comparison on SemanticKITTI. It also reveals that GFNet performs much better on frequent classes while somewhat struggles on those rare/small classes. Despite the good performance of GFNet, it is also interesting to further improve  GFNet by addressing rare/small classes from the perspectives of data sampling/augmentation and loss function.

\begin{table}[t]
\centering
\caption{Comparisons on nuScenes (the test set) under mIoU and Frequency Weighted IoU (or FW IoU).}
\label{tab:nuscenes_test}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccccccccccccccc|cc}
\hline
Method  
&\rotatebox{90}{barrier}		&\rotatebox{90}{bicycle}		&\rotatebox{90}{bus}		&\rotatebox{90}{car}			&\rotatebox{90}{const-vehicle}
&\rotatebox{90}{motorcycle} 	&\rotatebox{90}{pedestrian}		&\rotatebox{90}{traffic-cone} 	&\rotatebox{90}{trailer}			&\rotatebox{90}{truck} 	
&\rotatebox{90}{dri-surface}   &\rotatebox{90}{other-flat} 	&\rotatebox{90}{sidewalk} 		&\rotatebox{90}{terrain} 			&\rotatebox{90}{manmade} 
&\rotatebox{90}{vegetation} 		&\rotatebox{90}{\textbf{mIoU}}   &\rotatebox{90}{\textbf{FW IoU}}  
\\ 
\hline
PolarNet~\citep{zhang2020polarnet} &  72.2 & 16.8  & 77.0   & 86.5  & 51.1  & 69.7  & 64.8 &   54.1 &  69.7 & 63.4 & 96.6  & 67.1 & 77.7 & 72.1& 87.1& 84.4& 69.4 &  87.4 \\

AMVNet~\citep{liong2020amvnet}  & 79.8 & 32.4  & 82.2 & 86.4  & 62.5  & 81.9  & 75.3 & 72.3  & 83.5  & 65.1 & 97.4 & 67.0  & 78.8 & 74.6 & 90.8 & 87.9 & 76.1 & 89.5 \\
 
\rowcolor{Gray} GFNet (ours) &
81.1 & 31.6 & 76.0 & 90.5 & 60.2 & 80.7 & 75.3 & 71.8 & 82.5 & 65.1 & 97.8 & 67.0 & 80.4 & 76.2 & 91.8 & 88.9 & 76.1 & 90.4 \\
\hline
\end{tabular}
}
\end{table}









