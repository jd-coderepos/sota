\begin{table*}[t]
	\centering
	\caption{\small 3D object detection results on the KITTI validation set. We report \APBEV ~/ \AP (in \%) of the \textbf{car} category, corresponding to average precision of the bird's-eye view and 3D object box detection. Mono stands for monocular. Our methods with \emph{pseudo-LiDAR} estimated by \PSMNetpd~\cite{chang2018pyramid} (stereo) or \DORN~\cite{fu2018deep} (monocular) are in {\color{blue} blue}. Methods with LiDAR are in {\color{gray} gray}. Best viewed in color.} \label{tbMain}
	\begin{tabular}{=l|+c|+c|+c|+c|+c|+c|+c}
		&  & \multicolumn{3}{c|}{IoU = 0.5} & \multicolumn{3}{c}{IoU = 0.7} \\ \cline{3-8}
		\multicolumn{1}{c|}{Detection algorithm} & Input signal & Easy & Moderate & Hard & Easy & Moderate & Hard \\ \hline
		\Mono~\cite{chen2016monocular} & Mono & 30.5 / 25.2 & 22.4 / 18.2 & 19.2 / 15.5  & 5.2 / 2.5 & 5.2 / 2.3  & 4.1 / 2.3  \\
		\MLFmono~\cite{xu2018multi} & Mono & 55.0 / 47.9 & 36.7 / 29.5 & 31.3 / 26.4 & 22.0 / 10.5 & 13.6 / 5.7 & 11.6 / 5.4\\
		\rowstyle{\color{blue}}
		\AVODC & Mono & 61.2 / 57.0 & 45.4 / 42.8 & 38.3 / 36.3 & 33.7 / 19.5 & 24.6 / 17.2 & 20.1 / 16.2\\
		\rowstyle{\color{blue}}
		\Frustum & Mono & 70.8 / 66.3 & 49.4 / 42.3 & 42.7 / 38.5 & 40.6 / 28.2 & 26.3 / 18.5 & 22.9 / 16.4\\ \hline
		\DOP~\cite{chen20153d} & Stereo & 55.0 / 46.0 & 41.3 / 34.6 & 34.6 / 30.1 & 12.6 / 6.6 & 9.5 / 5.1 & 7.6 / 4.1 \\ 
		\MLFstereo~\cite{xu2018multi} & Stereo & - & 53.7 / 47.4 & - & - & 19.5 / 9.8 & - \\
		\rowstyle{\color{blue}}
		\AVODC & Stereo & 89.0 / 88.5 & 77.5 / 76.4 & 68.7 / 61.2&  74.9 / 61.9 &  56.8 / 45.3 & 49.0 / 39.0 \\
		\rowstyle{\color{blue}}
		\rowstyle{\color{blue}}
		\Frustum & Stereo & 89.8 / 89.5 & 77.6 / 75.5 &  68.2 / 66.3 &  72.8 / 59.4& 51.8 / 39.8 & 44.0 / 33.5 \\ \hline
		\rowstyle{\color{gray}}
		\AVODGT~\cite{ku2018joint} & LiDAR + Mono & 90.5 / 90.5 & 89.4 / 89.2 & 88.5 / 88.2 & 89.4 / 82.8 & 86.5 / 73.5 & 79.3 / 67.1 \\
		\rowstyle{\color{gray}}
		\Frustum~\cite{qi2018frustum} & LiDAR + Mono & 96.2 / 96.1 & 89.7 /  89.3 & 86.8 / 86.2 &  88.1 / 82.6 & 82.2 / 68.8 & 74.0 / 62.0 \\
		\hline
	\end{tabular}
\end{table*}

\subsection{Experimental results}
\label{exp_result}


We summarize the main results in Table~\ref{tbMain}. We organize methods according to the input signals for performing detection. Our stereo approaches based on pseudo-LiDAR significantly outperform all image-based alternatives by a large margin. At IoU = 0.7 (moderate) --- the metric used to rank algorithms on the KITTI leaderboard --- we achieve \emph{double} the performance of the previous state of the art. We also observe that pseudo-LiDAR is applicable and highly beneficial to two 3D object detection algorithms with very different architectures, suggesting its wide compatibility.

One interesting comparison is between approaches using pseudo-LiDAR with monocular depth (\DORN) and stereo depth (\PSMNetpd). While \DORN has been trained with almost ten times more images than \PSMNetpd (and some of them overlap with the validation data), the results with \PSMNetpd dominate. This suggests that stereo-based detection is a promising direction to move in, especially considering the increasing affordability of stereo cameras.

In the following section, we discuss key observations and conduct a series of experiments to analyze the performance gain through pseudo-LiDAR with stereo disparity.

\begin{table}
	\centering
	\tabcolsep 3pt
	\caption{\small Comparison between frontal and \emph{pseudo-LiDAR} representations. \AVOD projects the pseudo-LiDAR representation into the bird-eye's view (BEV). We report \APBEV ~/ \AP (in \%) of the \textbf{moderate car} category at IoU = 0.7. The best result of each column is in bold font. The results indicate strongly  that the data representation is the key  contributor to the accuracy gap. } \label{tbAVOD}
	\begin{tabular}{l|l|c|c}
		\multicolumn{1}{c|}{Detection} & \multicolumn{1}{c|}{Disparity} & Representation & \APBEV~/ \AP \\ \hline
		\MLF~\cite{xu2018multi} & \DispNet & Frontal & 19.5 / 9.8 \\
		\color{blue}\AVODGT &\color{blue} \DispNetS &\color{blue} Pseudo-LiDAR & \color{blue}36.3 / 27.0 \\
		\color{blue}\AVODGT &\color{blue} \DispNetC &\color{blue} Pseudo-LiDAR & \color{blue}36.5 / 26.2 \\ \hline
		\AVODGT & \PSMNetpd & Frontal & 11.9 / 6.6 \\
		\color{blue}\AVODGT &\color{blue} \PSMNetpd &\color{blue} Pseudo-LiDAR & \color{blue}\textbf{56.8} / \textbf{45.3} \\
		\hline
	\end{tabular}
\end{table} 

\paragraph{Impact of data representation.}
When comparing our results using \DispNetS or \DispNetC to \MLFstereo~\cite{xu2018multi} (which also uses \DispNet as the underlying stereo engine), we observe a large performance gap (see Table.~\ref{tbAVOD}). Specifically, at IoU,  we outperform \MLFstereo by at least 16\% on \APBEV and 16\% on \AP. The later is equivalent to a 160\% relative improvement. 
We attribute this improvement to the way in which we represent the resulting depth information. We note that both our approach and \MLFstereo~\cite{xu2018multi} first back-project pixel depths into 3D point coordinates. \MLFstereo construes the 3D coordinates of each pixel as additional feature maps in the frontal view. These maps are then concatenated with RGB channels as the input to a modified 2D object detection pipeline based on Faster-RCNN~\cite{ren2015faster}. As we point out earlier, this has two problems. Firstly, distant objects become smaller, and detecting small objects is a known hard problem~\cite{lin2017feature}.
Secondly, while performing local computations like convolutions or ROI pooling along height and width of an image makes sense to 2D object detection, it will operate on 2D pixel neighborhoods with pixels that are far apart in 3D, making the precise localization of 3D objects much harder (cf.~Fig.~\ref{fig:depth_conv}).

By contrast, our approach treats these coordinates as pseudo-LiDAR signals and applies PointNet~\cite{qi2017pointnet} (in \Frustum) or use a convolutional network on the BEV projection (in \AVOD).
This introduces invariance to depth, since far-away objects are no longer smaller.
Furthermore, convolutions and pooling operations in these representations put together points that are physically nearby.
 
To further control for other differences between \MLFstereo and our method we ablate our approach to use the same frontal depth representation used by \MLFstereo. 
\AVOD fuses information of the frontal images with BEV LiDAR features. We modify the algorithm, following \cite{chen20183d,xu2018multi}, to generate five frontal-view feature maps, including 3D pixel locations, disparity, and Euclidean distance to the camera. We concatenate them with the RGB channels while disregarding the BEV branch in \AVOD, making it fully dependent on the frontal-view branch. (We make no additional architecture changes.) The results in Table~\ref{tbAVOD} reveal a staggering gap between frontal and pseudo-LiDAR results. We found that the frontal approach  struggles with inferring object depth, even when the five extra maps have provided sufficient 3D information.
Again, this might be because 2d convolutions put together pixels from far away depths, making accurate localization difficult. This experiment suggests that the chief source of the accuracy improvement is indeed the \emph{pseudo-LiDAR} representation.

\begin{table}
	\centering
	\caption{\small Comparison of different combinations of stereo disparity and 3D object detection algorithms, using \emph{pseudo-LiDAR}. We report \APBEV ~/ \AP (in \%) of the \textbf{moderate car} category at IoU = 0.7. The best result of each column is in bold font.} \label{tbDisp}
	\begin{tabular}{l|c|c}
		\multicolumn{1}{c|}{}  & \multicolumn{2}{c}{Detection algorithm} \\ \cline{2-3}
		\multicolumn{1}{c|}{Disparity} & \AVODGT & \Frustum \\ \hline
		\color{blue}\DispNetS &\color{blue} 36.3 / 27.0  &\color{blue} 31.9 / 23.5 \\
		\color{blue}\DispNetC  &\color{blue} 36.5 / 26.2 &\color{blue} 37.4 / 29.2\\
		\color{blue}\PSMNet  &\color{blue} 39.2 / 27.4 &\color{blue} 33.7 / 26.7 \\ \hline
		\color{blue}\PSMNetpd  &\color{blue} \textbf{56.8} / \textbf{45.3} &\color{blue} \textbf{51.8} / \textbf{39.8} \\
		\hline
	\end{tabular}
\end{table}

\paragraph{Impact of stereo disparity estimation accuracy.}

We compare \PSMNet~\cite{chang2018pyramid} and \DispNet~\cite{mayer2016large} on pseudo-LiDAR-based detection accuracies. On the leaderboard of KITTI stereo 2015, \PSMNet achieves 1.86\% disparity error, which far outperforms the error of 4.32\% by \DispNetC.

As shown in Table~\ref{tbDisp}, the accuracy of disparity estimation does not necessarily correlate with the accuracy of object detection. \Frustum with \DispNetC even outperforms \Frustum with \PSMNet.
This is likely due to two reasons. First, the disparity accuracy may not reflect the depth accuracy: the same disparity error (on a pixel) can lead to drastically different depth errors dependent on the pixel's true depth, according to Eq.~(\ref{eq_disp_depth}). Second, different detection algorithms process the 3D points differently: \AVOD quantizes points into voxels, while \Frustum directly processes them and may be vulnerable to noise. 

By far the most accurate detection results are obtained by \PSMNetpd, which we trained from scratch on our own KITTI training set.  These results seem to suggest that significant further improvements may be possible through  end-to-end training  of the whole pipeline. 

We provide results using \SPSstereo~\cite{yamaguchi2014efficient} and further analysis on depth estimation in the Supplementary Material.

\paragraph{Comparison to LiDAR information.}
Our approach significantly improves stereo-based detection accuracies. 
A key remaining question is, how close the pseudo-LiDAR detection results are to those based on real LiDAR signal. 
In Table~\ref{tbMain}, we further compare to \AVOD and \Frustum when actual LiDAR signal is available. For fair comparison, we retrain both models. 
For the easy cases with IoU , our stereo-based approach performs very well, only slightly worse than the corresponding LiDAR-based version. However, as the instances become harder (e.g., for cars that are far away), the performance gaps resurfaces --- although  not nearly as pronounced as without pseudo-LiDAR. 
We also see a larger gap when moving to IoU .
These results are not surprising, since stereo algorithms are known to have larger depth errors for far-away objects, and a stricter metric requires higher depth precision. Both observations emphasize the need for accurate depth estimation, especially for far-away distances, to bridge the gap further.  A key limitation of our results may be the low resolution of the 0.4 MegaPixel images, which  cause far away objects to only consist of a few pixels. 

\begin{table}
\centering
\tabcolsep 5pt
\caption{\small 3D object detection on the \textbf{pedestrian} and \textbf{cyclist} categories on the validation set. We report \APBEV~/ \AP at IoU = 0.5 (the standard metric) and compare \Frustum with \emph{pseudo-LiDAR} estimated by \PSMNetpd (in {\color{blue}blue}) and LiDAR (in {\color{gray} gray}). } \label{tbPedestrian}
\begin{tabular}{=c|+c|+c|+c}
Input signal & Easy & Moderate & Hard \\ \hline
\multicolumn{4}{c}{Pedestrian} \\ \hline
\rowstyle{\color{blue}}
Stereo & 41.3 / 33.8 & 34.9 / 27.4 & 30.1 / 24.0\\
\rowstyle{\color{gray}}
LiDAR + Mono & 69.7 / 64.7 & 60.6 / 56.5 & 53.4 / 49.9 \\ \hline
\multicolumn{4}{c}{Cyclist} \\ \hline
\rowstyle{\color{blue}}
Stereo & 47.6 / 41.3 & 29.9 / 25.2 & 27.0 / 24.9\\
\rowstyle{\color{gray}}
LiDAR + Mono & 70.3 / 66.6 & 55.0 / 50.9 & 52.0 / 46.6\\
\hline
\end{tabular}
\end{table}

\begin{figure*}[t]
	\centerline{\includegraphics[width=\linewidth]{figures/figure4}}
	\caption{\textbf{Qualitative comparison.} We compare \AVOD with LiDAR, pseudo-LiDAR, and frontal-view (stereo). Ground-truth boxes are in {\color{red}{red}}, predicted boxes in {\color{green}{green}}; the observer in the pseudo-LiDAR plots (bottom row) is on the very left side looking to the right.  The frontal-view approach (\emph{right}) even  miscalculates the depths of nearby objects and misses far-away objects entirely. Best viewed in color.}
	\label{fig:qualitative}
\end{figure*}

\paragraph{Pedestrian and cyclist detection.}
We also present results on 3D pedestrian and cyclist detection.
These are much more challenging tasks than car detection due to the small sizes of the objects, even given LiDAR signals. At an IoU threshold of 0.5, both \APBEV and \AP of pedestrians and cyclists are much lower than that of cars at IoU 0.7~\cite{qi2018frustum}. We also notice that none of the prior work on image-based methods report  results in this category.

Table~\ref{tbPedestrian} shows our results with \Frustum and compares to those with LiDAR, on the validation set. Compared to the car category (cf.~Table~\ref{tbMain}), the performance gap is significant. We also observe a similar trend that the gap becomes larger when moving to the hard cases. Nevertheless, our approach has set a solid starting point for image-based pedestrian and cyclist detection for future work.

\subsection{Results on the test set}
We report our results on the car category on the test set in Table~\ref{tbTest}. We see a similar gap between pseudo-LiDAR and LiDAR as on the validation set, suggesting that our approach does not simply over-fit to the ``validation data.'' \emph{We also note that, at the time we submit the paper, we are at the first place among all the image-based algorithms on the KITTI leaderboard.} Details and results on the pedestrian and cyclist categories are in the Supplementary Material.

\begin{table}
	\centering
	\tabcolsep 5pt
	\caption{\small 3D object detection results on the \textbf{car} category on the \emph{test} set. We compare \emph{pseudo-LiDAR} with \PSMNetpd (in {\color{blue}blue}) and LiDAR (in {\color{gray} gray}). We report \APBEV~/ \AP at IoU = 0.7.
	: Results on the KITTI leaderboard.} \label{tbTest}
	\begin{tabular}{=c|+c|+c|+c}
		Input signal & Easy & Moderate & Hard \\ \hline
		\multicolumn{4}{c}{\AVOD} \\ \hline
		\rowstyle{\color{blue}}
		Stereo &  66.8 / 55.4 &  47.2 / 37.2 & 40.3 / 31.4\\
		\rowstyle{\color{gray}}
		LiDAR + Mono  &  88.5 / 81.9 &  83.8 / 71.9 & 77.9 / 66.4\\
		\hline
		\multicolumn{4}{c}{\Frustum} \\ \hline
		\rowstyle{\color{blue}}
		Stereo &  55.0 / 39.7 &  38.7 / 26.7 & 32.9 / 22.3\\
		\rowstyle{\color{gray}}
		LiDAR + Mono  &  88.7 / 81.2 &  84.0 / 70.4 & 75.3 / 62.2\\
		\hline
	\end{tabular}
\end{table}

\subsection{Visualization} We further visualize the prediction results on validation images in Fig.~\ref{fig:qualitative}. We compare LiDAR (left), stereo pseudo-LiDAR (middle),
and frontal stereo (right). We used \PSMNetpd to obtain the stereo depth maps. 
LiDAR and pseudo-LiDAR lead to highly accurate predictions, especially for the nearby objects.  However, pseudo-LiDAR fails to detect far-away objects precisely due to inaccurate depth estimates.
On the other hand, the frontal-view-based approach makes extremely inaccurate predictions, even for nearby objects. This corroborates the quantitative results we observed in Table~\ref{tbAVOD}. We provide additional qualitative results and failure cases in the Supplementary Material.
