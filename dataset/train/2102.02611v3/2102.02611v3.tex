
\documentclass{article} 

\usepackage[dvipsnames]{xcolor}    

\usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak













 
\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amssymb}
\usepackage{multicol}
\usepackage{sidecap}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{floatrow}
\usepackage{adjustbox}
\usepackage{xfrac}

\usepackage{enumitem}
\usepackage{multirow}

\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

\usepackage{wrapfig}
\usepackage{chngcntr}

\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{MnSymbol}\usepackage{wasysym}\usepackage{multirow}
\usepackage{xspace}
\usepackage[capitalise]{cleveref}
\creflabelformat{equation}{#2#1#3}
\newcommand{\creflastconjunction}{,\nobreakspace}
\newcommand{\crefpairconjunction}{,\nobreakspace}
\def\arrvline{\hfil\kern\arraycolsep\vline\kern-\arraycolsep\hfilneg}
\usepackage[cal=dutchcal,
 calscaled=1,
scr=euler]{mathalfa}
\newcommand{\ie}[0]{\emph{i.e.},~}
\newcommand{\eg}[0]{\emph{e.g.},~}
\newcommand{\aka}[0]{a.k.a.~}
\newcommand{\etal}{\emph{et al.~}}
\newcommand{\etc}{\emph{etc.~}}
\newcommand{\wrt}{w.r.t.~}
\def\old#1{{\color{gray}{{old:\ #1\ }}}}
\newcommand{\defeq}{\ensuremath{\doteq}}

\usepackage{dsfont}
\newcommand{\kw}[1]{{\small\textsc{\MakeLowercase{#1}}}}
\newcommand{\mat}[1]{\ensuremath{{\mathbf{\MakeUppercase{{#1}}}}}}
\renewcommand{\vec}[1]{\ensuremath{\mathbf{\MakeLowercase{{#1}}}}}
\newcommand{\set}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\gr}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\prm}[1]{\ensuremath{^{#1}}}
\newcommand{\ly}[1]{\ensuremath{^{(#1)}}}
\newcommand{\grn}[2]{\ensuremath{\gr{#1}\prm{#2}}}
\newcommand{\tuple}[1]{\ensuremath{\langle{#1} \rangle}}
\newcommand{\Reals}{\mathds{R}}
\renewcommand{\Re}{\gr{R}}
\newcommand{\Nat}{\set{N}}
\newcommand{\eye}{\mat{I}}
\newcommand{\ones}{\vec{1}}
\newcommand{\Vs}{\set{V}}
\newcommand{\Ws}{\set{W}}
\newcommand{\Ps}{\set{P}}
\newcommand{\Qs}{\set{Q}}
\newcommand{\Sg}{\gr{S}}
\newcommand{\Ig}{\gr{I}}
\newcommand{\Cg}{\gr{C}}
\newcommand{\Gg}{\gr{G}\xspace}
\renewcommand{\gg}{\gr{g}}
\newcommand{\gi}{\gr{i}}
\newcommand{\gj}{\gr{j}}
\newcommand{\gh}{\gr{h}}
\newcommand{\Hg}{\gr{H}}
\newcommand{\hg}{\gr{h}}
\newcommand{\Kg}{\gr{K}}
\newcommand{\Ng}{\gr{N}}
\newcommand{\Bg}{\gr{B}}
\newcommand{\bg}{\gr{b}}
\newcommand{\kg}{\gr{k}}
\newcommand{\Ug}{\gr{U}}
\newcommand{\ug}{\gr{u}}
\newcommand{\Wm}{\mat{W}}
\newcommand{\Xm}{\mat{X}}
\newcommand{\Ym}{\mat{Y}}
\newcommand{\Am}{\mat{A}}
\newcommand{\Gm}{\mat{G}}
\newcommand{\Vm}{\mat{V}}
\newcommand{\Um}{\mat{U}}
\newcommand{\Pm}{\mat{P}}

\newcommand{\xv}{\vec{x}}
\newcommand{\yv}{\vec{y}}
\newcommand{\hv}{\vec{h}}
\newcommand{\ov}{\vec{o}}
\newcommand{\bv}{\vec{b}}
\newcommand{\wv}{\vec{w}}
\newcommand{\W}{\Wm}
\newcommand{\X}{\Xm}
\newcommand{\Y}{\Ym}
\newcommand{\A}{\Am}
\newcommand{\x}{\xv}
\newcommand{\Ns}{\set{N}}
\newcommand{\Ms}{\set{M}}
\newcommand{\As}{\set{N}}
\newcommand{\Bs}{\set{M}}

\def\ct{\mathrm{c}}
\def\Ct{\mathrm{C}}
\def\Lt{\mathrm{L}}
\def\Nt{\mathrm{N}}
\def\Kt{\mathrm{K}}
\def\Xt{\mathrm{X}}

\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sR{{\mathbb{R}}}
\def\sZ{{\mathbb{Z}}}

\def\bpsi{{\boldsymbol{\psi}}}

\newcommand{\eu}{\mathrm{e}\mkern1mu}
\newcommand{\du}{\mathrm{d}\mkern1mu}
\newcommand{\iu}{{i\mkern1mu}}
\newcommand{\ltwo}{\boldsymbol{{\rm L}^{2}}}


\font\btt=rm-lmtk10
\newcommand{\mlp}{{\btt MLP}}
\newcommand{\mlppsi}{\mlp}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mathematicablue}{rgb}{0.11, 0.25, 0.467}
\hypersetup{colorlinks,citecolor={mydarkblue},urlcolor={mydarkblue}, linkcolor={red}} 

\usepackage[ruled,vlined]{algorithm2e}



\title{CKConv: Continuous Kernel Convolution For \centerline{Sequential Data}}



\author{\centerline{David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn}\\
    \centerline{\hspace{0.5mm}Vrije Universiteit Amsterdam \quad \hspace{0.5mm}University of Amsterdam}\\
  \centerline{The Netherlands}\\
  \centerline{\texttt{\{d.w.romeroguzman, a.kuzina\}@vu.nl}}\\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
   Conventional neural architectures for sequential data present important limitations. Recurrent neural networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional neural networks cannot handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that these problems can be solved by formulating the convolutional kernels of CNNs as continuous functions. The resulting \textit{Continuous Kernel Convolution} (CKConv) handles arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that \textit{Continuous Kernel Convolutional Networks} (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a faster and simpler manner.
\end{abstract}

\vspace{-3mm}
\section{Introduction}
\vspace{-1mm}
Recurrent Neural Networks (RNNs) have long governed tasks with sequential data \citep{rumelhart1985learning, hochreiter1997long, chung2014empirical}. Their main ingredient are \textit{recurrent units}: network components with a recurrence formulation which grants RNNs the ability to be unrolled for arbitrarily many steps and handle sequences of arbitrary size. In practice, however, the effective \textit{memory horizon} of RNNs, i.e., the number of steps the network retains information from, has proven to be surprisingly small, most notably due to the \textit{vanishing gradients problem} \citep{hochreiter1991untersuchungen, bengio1994learning}. Interstingly, it is the very recurrent nature of RNNs that allows them to be unrolled for arbitrarily many steps which is responsible for vanishing gradients \citep{pascanu2013difficulty}. This, in turn, hinders learning from the far past and induces a~small~effective~memory~horizon.

Convolutional Neural Networks (CNNs) \citep{lecun1998gradient} have proven a strong alternative to recurrent architectures as long as relevant input dependencies fall within their memory horizon, e.g., \citet{conneau2016very, oord2016wavenet, dai2017very, dauphin2017language, bai2018empirical}. CNNs avoid the training instability and vanishing / exploding gradients characteristic of RNNs by avoiding \textit{back-propagation through time} \citep{werbos1990backpropagation} altogether.
However, these architectures model convolutional kernels as a sequence of independent weights. As a result, their memory horizon must be defined \textit{a-priori}, and larger memory horizons induce a proportional growth of the model size.

In this work, we provide a solution to these limitations. We propose to view a convolutional kernel as a continuous function parameterized by a small neural network instead of a sequence of independent weights. The resulting \textit{Continuous Kernel Convolution} (CKConv) enjoys the following properties:
\begin{itemize}[topsep=0pt, leftmargin=*]
    \item CKConvs can define arbitrarily large memory horizons within a single operation. Consequently, \textit{Continuous Kernel Convolutional Neural Networks} (CKCNNs) detach their memory horizon from \emph{(i)} the depth of the network, \emph{(ii)} the dilation factors used, and \emph{(iii)} the size of the network. 
    \item CKConvs do not rely on any form of recurrence. As a result, CKCNNs (\textit{i}) can be trained in parallel, and (\textit{ii}) do not suffer from vanishing / exploding gradients or small effective memory horizons.
    \item Continuous convolutional kernels can be evaluated at arbitrary positions. Consequently, CKConvs and CKCNNs can be readily used on irregularly sampled data, and data at different resolutions.
\end{itemize}
We observe that continuous kernel parameterizations previously used to handle irregular data \textit{locally}, e.g., \citet{schutt2017schnet, wu2019pointconv}, are not adequate to model long-term dependencies. This is due to the inability of their kernels to model long spatial complex functions (Sec.~\ref{sec:ckconvkernel}). Contrarily, CKConvs perfectly describe long complex non-linear, non-smooth functions by parameterizing their kernels as SIRENs \citep{sitzmann2020implicit}: implicit neural representations with  nonlinearities. Shallow CKCNNs match or outperform state-of-the-art approaches on several tasks comprising stress tests, continuous, discrete and irregular data, as well as resolution changes. To the best of our knowledge, we are first to observe the potential of continuous convolutional kernels to model long-term dependencies, and to provide an useful parameterization to this end.
\begin{figure}
    \centering
    \hfill
         \begin{subfigure}[b]{0.115\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/mlp_graphic.png}
\end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.75\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/ckconv_3.png}
\end{subfigure}
     \hfill
    \vspace{-2mm}
    \caption{Continuous Kernel Convolution (CKConv). CKConv views a convolutional kernel as a vector-valued continuous function  parameterized by a small neural network \mlppsi. \mlppsi\ receives a time-step and outputs the value of the convolutional kernel at that position. We sample convolutional kernels by passing a set of relative positions  to \mlppsi, and perform convolution\break with the sampled kernel next. Since \mlppsi\ is a continuous function, CKConvs can (\textit{i}) construct arbitrarily large kernels, (\textit{ii}) generate kernels at different resolutions, and (\textit{iii}) handle irregular data.\vspace{-2.5mm}}
    \label{fig:ckconv}
\end{figure}
\vspace{-2mm}
\section{Related Work}\label{sec:related_work}
\vspace{-2mm}
\textbf{Continuous kernel formulation.} Continuous formulations for convolutional kernels were introduced to handle irregularly sampled 3D data \textit{locally} \citep{schutt2017schnet, simonovsky2017dynamic, wang2018deep, wu2019pointconv}. As discrete convolutions learn independent weights for specific relative positions, they cannot handle irregularly sampled data effectively. Following work focuses on point-cloud applications \citep{fuchs2020se, hu2020randla, shi2019points, thomas2018tensor}. Other approaches include Monte Carlo approximations of continuous operations \citep{finzi2020generalizing}.
Our work proposes a new broad flavor of applications for which continuous kernels are advantageous.

\textbf{Implicit neural representations.} Implicit neural representations construct continuous data representations by encoding the input in the weights of a neural network \citep{mescheder2019occupancy, park2019deepsdf, sitzmann2020implicit}. This leads to numerous advantages over conventional (discrete) data representations, e.g., memory efficiency, analytic differentiability, with interesting properties for several applications, e.g., generative modelling \citep{dupont2021generative, schwarz2020graf}.

Since we model convolutional kernels as continuous functions and parameterize them via neural networks, our approach can be understood as \textit{implicitly representing the convolutional kernels of a conventional CNN}.
Different is the fact that these convolutional kernels are not known a-priori, but learned as a part of the optimization task of the CNN. Making the connection between implicit neural representations and continuous kernel formulations explicitly brings substantial insights for the construction of these kernels. In particular, it motivates the use of  nonlinearities \citep{sitzmann2020implicit} to parameterize them, which leads to significant improvements over the , , and  nonlinearities used so far for this purpose (Sec.~\ref{sec:ckconvkernel}).

\vspace{-2mm}
\section{The Convolution and Common Kernel Parameterizations}\label{sec:convolution}
\vspace{-2mm}
\textbf{Notation.}  denotes the set . Bold capital and lowercase letters depict vectors and matrices, e.g., , , sub-indices index vectors, e.g., , parentheses index time, e.g.,  is the value of  at time-step , and calligraphic letters depict sequences, e.g., . 


\textbf{Centered and causal convolutions.} 
Let  and  be a vector valued signal and kernel on , such that  and . The convolution is defined as:

In practice, the input signal  is gathered via some sampling procedure. Resultantly, the convolution is effectively performed between the sampled input signal described as a sequence of finite length  and a convolutional kernel  described the same way:

Values  falling outside of  are \textit{padded} by a constant value often defined as zero (Fig.~\ref{fig:centered_conv}).

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.26\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/normalconv.png}
         \caption{}
         \label{fig:centered_conv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.26\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/causalconv.png}
         \caption{}
         \label{fig:causal_conv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/dilated_kernel.png}
         \caption{}
         \label{fig:dilated_conv}
     \end{subfigure}
     \vspace{-2mm}
     \caption{Discrete centered, causal, and dilated causal convolutions.
     \vspace{-3.5mm}}
        \label{fig:causal_vs_centered}
\end{figure}
The convolutional kernel is commonly \textit{centered} around the point of calculation . For sequence modeling this can be undesirable as future input values  are considered during the operation. This is solved by providing a \textit{causal formulation to the convolution}: a formulation in which the convolution at time-step  only depends on input values at time-steps  (Fig.~\ref{fig:causal_conv}):

In practice, causal convolutions are easily implemented via asymmetrical padding. In this work, we consider causal convolutions as default. Nevertheless, our analyses are also valid for centered ones.

\textbf{Discrete convolutional kernels.}
By a large margin, most convolutional kernels  in literature are parameterized as a finite sequence of  independent learnable weights  (Fig.~\ref{fig:causal_vs_centered}). As these weights are independent of one another,  must be kept small to keep the parameter count of the model tractable. Hence, the kernel size is often much smaller than the input length: . 
This parameterization presents important limitations:
\begin{itemize}[topsep=0pt, leftmargin=*]
\item The memory horizon  must be defined a priori.
\item Since , this parameterization implicitly assumes that the convolution  at position  \emph{only depends on input values at positions up to  steps in the past}. Consequently, no functions depending on inputs  for  can be modeled.
\item The most general selection of  is given by a \textit{global memory horizon}: . Unfortunately, as discrete convolutional kernels are modeled as a sequence of independent weights, this incurs an extreme growth of the model size and rapidly becomes statistically unfeasible.
\end{itemize}
\textbf{Dilated convolutional kernels.}
To alleviate these limitations, previous works propose to interleave kernel weights with zeros in order to cover larger memory horizons without additional weights (Fig.~\ref{fig:dilated_conv}). This formulation alleviates some of the previous limitations, but introduces additional ones:
\begin{itemize}[topsep=0pt, leftmargin=*]
\item Dilated kernels are unable to model dependencies of input values falling in the interleaved regions.\item Several authors use dilated convolutions with varying dilation factors as a function of depth, e.g., \citep{bai2018empirical, dai2017very, oord2016wavenet, romero2020wavelet}. By carefully selecting layer-wise dilation factors, one can assure that some kernel hits each input within the memory horizon of the network. However, due to the extreme sparsity of the formulation, it is difficult to estimate the effective amount of processing applied to the input. In addition, this layout ties together (\textit{i}) the memory horizon, (\textit{ii}) the depth, and (\textit{iii}) the layer-wise dilation factors of the network, which effectively constraints the flexibility of the neural architecture design.
\end{itemize}
In contrast to the (dilated) discrete convolutions presented in this section, our proposed formulation allows handling arbitrarily long sequences with arbitrarily large, dense memory horizons in a single layer and under a fixed parameter budget.
\vspace{-2mm}
\section{Continuous Kernel Convolution}\label{sec:continuous_kernel_conv}
\vspace{-2mm}
In this section, we introduce our approach. First, we define it formally, analyze its properties, illustrate its connection to recurrent units, and elaborate on the functional family they can describe. Next, we discuss concrete parameterizations of continuous convolutional kernels, illustrate their connection to implicit neural representations, and show that our final kernels are able to fit complex functions.
\vspace{-2mm}
\subsection{Formulation and Properties}\label{sec:overview}
\vspace{-2mm}
\textbf{Arbitrarily large convolutional kernels.} We formulate the convolutional kernel  as a continuous vector-valued function parameterized by a small neural network \mlp (Fig.~\ref{fig:ckconv}, left).\break \mlp receives a relative position  and outputs the value of the convolutional kernel at that position . As a result, an arbitrarily large convolutional kernel  can be constructed by providing an equally large sequence of relative positions  to \mlp.
For , the size of the resulting kernel is equal to that of the input sequence , and thus it is able to model (global) long-term dependencies.
The \textit{Continuous Kernel Convolution} (CKConv) is given by:

\textbf{Irregularly sampled data.} CKConvs are able to handle irregularly-sampled and partially observed data. To this end, it is sufficient to sample \mlp at positions for which the input signal is known and perform the convolution operation with the sampled kernel. For very non-uniformly sampled inputs, an inverse density function over the samples can be incorporated in order to provide an unbiased estimation of the convolution response (see Appx. \ref{appx:irregularly_sampled}, \citet{wu2019pointconv} for details). 


\textbf{Data at different resolutions.} CKConvs can also process data at different resolutions. Consider the convolution  between an input signal  and a continuous convolutional kernel  sampled at a sampling rate . Now, if the convolution receives the same input signal sampled at a different sampling rate , it is sufficient to sample the convolutional kernel at the sampling rate  in order to perform an \enquote{equivalent} operation:~. As shown in Appx.~\ref{appx:diff_sampling_rates}, it holds that:

That is, convolutions calculated at different resolutions  and  are approximately equal up to a factor given by the resolution change. As a result, CKCNNs \emph{(i)} can be trained in datasets with data at varying resolutions, and \emph{(ii)} can be deployed at resolutions other than those seen during training.

We note that the previous features are hardly attainable by regular architectures, with an exception being RNNs with continuous-time interpretations, e.g., \citet{gu2020hippo,kidger2020neural}.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/RNN_vanishing.png}
         \caption{Recurrent unit ()}
         \label{fig:vanishing_rnn}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/RNN_exploding.png}
         \caption{Recurrent unit (()}
         \label{fig:exploding_rnn}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/exploding_CNN.png}
         \caption{Discrete kernel}
         \label{fig:exploding_conv}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/exploding_CKCNN.png}
         \caption{Continuous kernel}
         \label{fig:exploding_ckconv}
     \end{subfigure}
     \vspace{-2.8mm}
        \caption{Functional family of recurrent units, discrete convolutions and CKConvs. For max. eigenvalues of , , recurrent units are restricted to exponentially decreasing () or increasing () functions  (Figs.~\ref{fig:vanishing_rnn},~\ref{fig:exploding_rnn}). Discrete convolutions can describe arbitrary functions within their memory horizon but are zero otherwise (Fig.~\ref{fig:exploding_conv}). Conversely, CKConvs define arbitrary long memory horizons, and thus are able to describe arbitrary functions upon the entire input sequence (Fig.~\ref{fig:exploding_ckconv}).
        \vspace{-3.5mm}}
        \label{fig:vanishing_exploding_analysis}
\end{figure}
\textbf{(Linear)~recurrent~units~are~continuous~kernel~convolutions.}~Consider~a~recurrent~unit~of~the~form:
0 \jot]
 \tilde{\yv}(\tau) &= \mathrm{softmax}( \Vm \hv (\tau) ),

\setlength{\abovedisplayskip}{3.5pt}
\setlength{\belowdisplayskip}{3.5pt}
     (x * \psi)(t) = \int_{\sR} s(\tau) x(\tau) \psi(t - \tau) \, \du \tau, \label{eq:irreg_sampled}

\setlength{\abovedisplayskip}{-2pt}
\setlength{\belowdisplayskip}{3.5pt}
    \int \hspace{-1mm} f(\tau) \, \du \tau= \int \hspace{-1mm} \frac{f(\tau)}{p(\tau)} p(\tau) \, \du \tau \approx \sum_{i}\frac{f(\tau_i)}{p(\tau_i)}, \ \text{for}\  \tau_{i} \sim p(\tau). 

\setlength{\abovedisplayskip}{3.5pt}
\setlength{\belowdisplayskip}{3.5pt}
    (\xv *\boldsymbol{\psi})_\mathrm{sr_2}(t) \approx \frac{\mathrm{sr_2}}{\mathrm{sr_1}}(\xv *\boldsymbol{\psi})_\mathrm{sr_1}(t).

\setlength{\abovedisplayskip}{3.5pt}
\setlength{\belowdisplayskip}{3.5pt}
    (x * \psi)(t) = \int_{\sR} x(\tau) \psi(t - \tau) \, \du \tau,

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
    \int f(\tau) \, \du \tau \approx \sum_{i = 1}^{\Nt_\mathrm{sr}} f(\tau_{\mathrm{sr}, i}) \Delta_{\mathrm{sr}},

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
   \int_{\sR} x(\tau) \psi\big(t - \tau) \,  \du \tau  & \approx \sum_{i = 1}^{\Nt_{\mathrm{sr}_{1}}} x(\tau_{\mathrm{sr}_{1}, i}) \psi\big(t - \tau_{\mathrm{sr}_{1}, i}) \Delta_{\mathrm{sr}_{1}}  \nonumber \
As a result, we have that both approximations are approximately equal to the continuous integral at positions  defined on both discrete grids. By equating both approximations, we obtain that: 
-0.5\jot]
    \underbrace{\sum_{i = 1}^{\Nt_{\mathrm{sr_2}}} \hspace{-1mm} x(\tau_{\mathrm{sr_2}, i}) \psi\big(t - \tau_{\mathrm{sr_2}, i})}_{(x * \psi)_{\mathrm{sr_2}}(t)} \tfrac{1}{\mathrm{sr_2}} & \approx \underbrace{\sum_{i = 1}^{\Nt_{\mathrm{sr}_{1}}} \hspace{-1mm} x(\tau_{\mathrm{sr_1}, i}) \psi\big(t - \tau_{\mathrm{sr_1}, i})}_{(x * \psi)_{\mathrm{sr_1}}(t)} \tfrac{1}{\mathrm{sr_1}} \nonumber \
which concludes the proof. 
\vspace{-1mm}
\subsection{Linear Recurrent Units Are CKConvs}\label{appx:linrecunitsasckconvs}
\vspace{-1mm}
Interesting insights can be obtained by drawing connections between convolutions and recurrent units. In particular, we can show that linear recurrent units are equal to a CKConv with a particular family of convolutional kernels: exponential functions. Besides providing a generalization to recurrent units, this equality provides a fresh and intuitive view to the analysis of vanishing and exploding gradients.

\textbf{Recurrent unit.} Given an input sequence , a recurrent unit is constructed as:
0 \jot]
 \tilde{\yv}(\tau) &= \mathrm{softmax}( \Vm \hv (\tau) ),

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{1pt}
    \hv(t) = \Wm^{t+1} \hv(-1) + \sum_{\tau = 0}^{t} \Wm^{\tau}\Um \xv(t - \tau).\label{eq:unroll_t_steps}

     &\ \xv = [\xv(0) , \xv(1) , ... , \xv(t - 1), \xv(t) ] \label{eq:correspondence1} \-1.5\jot]
       \hv(t)& =  \sum_{\tau=0}^{t}\xv(\tau) \boldsymbol{\psi}(t-\tau)  = \sum_{\tau=0}^{t}\xv(t - \tau) \boldsymbol{\psi}(\tau) . \label{eq:correspondence3}      
1 \rightarrow 32 \rightarrow 32 \rightarrow \Nt_{\mathrm{out}} \times \Nt_{\mathrm{in}},\Big[G\big( - \tfrac{\mathrm{sr}_\text{test}}{\mathrm{sr}_\text{train}} \big) , G\big(-\tfrac{\mathrm{sr}_\text{test}}{\mathrm{sr}_\text{train}}+1\big), ... , G\big(0\big), ... , G\big(\tfrac{\mathrm{sr}_\text{test}}{\mathrm{sr}_\text{train}}-1\big), G\big(\tfrac{\mathrm{sr}_\text{test}}{\mathrm{sr}_\text{train}}\big)\Big]

\vspace{-2mm}
Note that blurring is only used when the test sampling rate is higher than the train sampling rate, as opposed to the normalization factor  discussed in Eq.~\ref{eq:diff_sampling_rates}, Appx.~\ref{appx:diff_sampling_rates}, which is applied whenever the sampling rates differ.

\subsection{Hyperparameters and Experimental Details}\label{appx:hyperparams}
In this section, we provide further specifications of the hyperparameter configurations with with our models are trained. An overview of these hyperparameters is provided in Tab.~\ref{tab:hyperparams}.

\begin{table*}[t!]
\caption{Hyperparameter specifications of the best performing CKCNN models.}
\label{tab:hyperparams}
\begin{center}
\vskip -3mm
\begin{small}
\scalebox{0.72}{
\begin{adjustbox}{center}
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{\sc{Params.}} & \multirow{2}{*}{\sc{Copy Memory}} & \multirow{2}{*}{\sc{Adding Problem}} & {\sc{sMNIST}} & {\sc{pMNIST}} & {\sc{sCIFAR10}} & \multirow{2}{*}{\sc{CT}} & \multirow{2.25}{*}{\sc{SC}} & \multirow{2}{*}{\sc{SC\_raw}}  & \multirow{2}{*}{\sc{PTB}}   \\
 & & & Small / Big & Small / Big & Small / Big \\
\midrule
Epochs & See Appx.~\ref{appx:hyperparams} &  See Appx.~\ref{appx:hyperparams} & 200 & 200 & 200 & 200 & 200 & 300 & 200\\
Batch Size & 32 & 32 & 64 & 64 & 64 & 32 & 64 & 32 & 24\\
Optimizer & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam & Adam\\
Learning Rate & 5e-4 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.002\\
\# Blocks & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2\\
Hidden Size & 10 & 25 & 30 / 100  & 30 / 100  & 30 / 100 & 30 & 30 & 30 & 128\\
 & See Appx.~\ref{appx:hyperparams} & See Appx.~\ref{appx:hyperparams} & 31.09 / 30.5 & 43.46 / 42.16  &  25.67 & 21.45 & 30.90 & 39.45 & 25.78\\
Dropout & -  & - & 0.1 / 0.2 & - & 0.2 / 0.3  & 0.1  & 0.2 & - & - \\
Input Dropout & -  & - & 0.1 / 0.2 & 0.1 / 0.2 & 0.0 / 0.0 & -  & - & - & 0.1\\
Embedding Dropout & - & - & - & - & - & - & - & - & 0.1 \\
Weight Dropout & -  & - & - & - & - / 0.1 & - & - & -& - \\
Weight Decay & -  & - & - & - & - / 1e-4 & - & - & 1e-4 & 1e-6\\
Scheduler & -  & - & Plateau & Plateau & Plateau & Plateau & Plateau & Plateau & Plateau \\
Patience & - & - & 20 & 20 & 20 & 20  & 15 & 20 & 5\\
Scheduler Decay & - & - & 5 & 5 & 5 & 5  & 5  & 5 & 5\\
\midrule
Model Size & 15.52{\sc{k}} & 70.59{\sc{k}} & 98.29{\sc{k}} / 1.03{\sc{m}} & 98.29{\sc{k}} / 1.03{\sc{m}} & 100.04{\sc{k}} / 1.04{\sc{m}} & 100.67{\sc{k}} & 118.24{\sc{k}} &  98.29{\sc{k}}& 1.8{\sc{m}}\\ 
\bottomrule
\multicolumn{9}{l}{ Hyperparameter values for the classification and varying sampling rate tasks. For hyperparameters w.r.t. irregularly-sampled data please see Tab.~\ref{tab:hyperparams_irreg}. }
\end{tabular}
\end{adjustbox}
}
\end{small}
\end{center}
\vspace{-4.0mm}

\end{table*}
 \textbf{Copy Memory.} We set the number of channels of our CKCNN as to roughly match the number of parameters of the GRU and TCN networks of \citet{bai2018empirical}. This is obtained with 10 hidden channels at every layer. We observe that the time to convergence grew proportional to the length of the sequence considered. Whereas for sequences of length 100 convergence was shown after as few as 10 epochs, for sequences of length 6000 approximately 250 epochs were required. The maximum number of epochs is set to 50, 50, 100, 200 and 300 for sequences of size 100, 200, 1000, 3000 and 6000. We observe that different values of  are optimal for different sequence lengths. The optimal  values found are 19.20, 34.71, 68.69, 43.65 and 69.97 for the corresponding sequence lengths.
 
\textbf{Adding Problem.} We set the number of channels of our CKCNN as to roughly match the number of parameters of the GRU and TCN networks of \citet{bai2018empirical}. This is obtained with 25 hidden channels at every layer. Similarly to the Copy Memory task, we observe that the time to convergence grew proportional to the length of the sequence considered. Interestingly, this task was much easier to solve for our models, with convergence for sequences of length 6000 observed after 38 epochs. The maximum number of epochs is set to 20, 20, 30, 50 and 50 for sequences of size 100, 200, 1000, 3000 and 6000. We observe that different values of  are optimal for different sequence lengths. The optimal  values found are 14.55, 18.19, 2.03, 2.23 and 4.3 for the corresponding sequence lengths. 

\textbf{sMNIST, pMNIST and sCIFAR10.} We construct two models of different sizes for these datasets: CKCNN and CKCNN-Big. The first is constructed to obtain a parameter count close to 100{\sc{k}}. The second model, is constructed to obtain a parameter count close to 1{\sc{m}}. The parameters utilized for these datasets are summarized in Tab.~\ref{tab:hyperparams}. Despite our efforts, we observed that our models heavily overfitted sCIFAR10. Combinations of weight decay, dropout and weight dropout were not enough to counteract overfitting. 

\textbf{CT, SC and SC\_raw.} The parameters utilized for classification on these datasets are summarized in Tab.~\ref{tab:hyperparams}. For hyperparameters regarding experiments with irregularly-sampled data please refer to Tab.~\ref{tab:hyperparams_irreg}. Any non-specified parameter value in Tab.~\ref{tab:hyperparams_irreg} can be safely consider to be the one listed for corresponding dataset in Tab.~\ref{tab:hyperparams}.

\textbf{PennTreeBank} For a character-level language modeling on PTB dataset we use hyperparameters specified in Tab.~\ref{tab:hyperparams}. We use embedding of size 100 following the TCN model from \cite{bai2018empirical}.

\begin{table}[t]
\centering
\caption{Hyperparameter values for experiments on irregularly sampled data. Non-listed parameters correspond to those in Tab.~\ref{tab:hyperparams}.}
\label{tab:hyperparams_irreg}
\begin{center}
\vskip -3mm
\begin{small}
\scalebox{0.69}{
\begin{tabular}{lccccccc}
\toprule
\multirow{2.5}{*}{\sc{Params.}}  & \multirow{2.5}{*}{\sc{PhysioNet}} & \multicolumn{3}{c}{\sc{CT}} & \multicolumn{3}{c}{\sc{SC\_raw}}  \\
\cmidrule{3-8}
 & & \sc{(30\%)} & \sc{(50\%)} & \sc{(70\%)} & \sc{(30\%)} & \sc{(50\%)} & \sc{(70\%)} \\
\midrule
 & 4.38 & 17.24 & 12.00 & 4.24 & 35.66 & 31.70 & 25.29\\
Dropout & 0.0 & 0.2  & 0.2 & 0.0 & 0.1 & 0 & 0\\
Weight Decay & 0.0 & 0.0 & 1e-4 & 0.0 & 1e-4 & 1e-4 & 1e-4 \\ 
Batch Size & 1024 \\
\midrule
Model Size & 175.71{\sc{k}} & \multicolumn{3}{c}{101.75{\sc{k}}} & \multicolumn{3}{c}{99.34{\sc{k}}}  \\ 
\bottomrule
\end{tabular}}
\end{small}
\end{center}
\vspace{-5.0mm}
\end{table}

 \begin{figure}
     \centering
     \includegraphics[width=0.95\textwidth]{images/high_res.png}
     \vspace{-2mm}
     \caption{High-frequency components in  continuous kernels. We observe that continuous kernels parameterized by Sine networks often contain frequency components of frequency higher than the resolution of the grid used during training. Here, for instance, the kernel looks smooth on the training grid. However, several high-frequency components appear when sampled on a finer grid. Though this may be a problematic phenomenon, we believe that, if tuned properly, these high-frequency components can prove advantageous to model fine details in tasks such as super-resolution and compression cheaply.
     \vspace{-4mm}}
     \label{fig:high_freq}
 \end{figure}












\end{document}
