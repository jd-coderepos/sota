\section{Experiments}

\paragraph{Baseline} We compare our method with Stargan-v2~\cite{starganv}, the state-of-the-art for image-to-image generation on CelebA-HQ and AFHQ.

\paragraph{Datasets} We evaluate our method on the CelebA-HQ~\cite{ProGanCelebA} and AFHQ~\cite{starganv} datasets. However, for CelebA-HQ we do not separate the two domains into female and male, since both domains are close to each other. Also, we are not using any extra information (e.g. facial attributes of CelebA-HQ). As for AFHQ, we train our network on each domain separately, since the amount of information shared between these is much lower. Average pooling is used as downscaling operator to generate the LR images, as in \cite{Lag}.

\paragraph{Evaluation metrics} Baseline results are evaluated according to the metrics of image-to-image translation used in \cite{MUNIT, DRIT, msgan}. Specifically, diversity and visual quality of samples produced by different methods are evaluated both with the Fr√©chet inception distance (FID)~\cite{ttur} and the learned perceptual image patch similarity (LPIPS)~\cite{LPIPS}. Since the FID score entwine diversity and fidelity in a single metric~\cite{divCov,precRecal}, we also experiment with the density and coverage metrics proposed in \cite{divCov}.


\subsection{Training Setup}

For our experiments, we fixed the LR image resolution to  and experimented with  and  for the HR image resolution---we ablate the effect of LR image resolution in sec.~\ref{sec:ablation}. 
We train our networks with Adam~\cite{adam} and TTUR~\cite{ttur}, with a learning rate of  for the generator and  for the discriminator. We also used  regularization~\cite{Greg} with , with a batch size of 8. Spectral normalization~\cite{spectralNorm} was used in all the layers of both  and . In eq.~\ref{Rx}, we use  to push the downscaled version of the generated image to be as close as possible to the LR target. We set  when trained on , and to  for .


    \begin{figure*}
    \begin{center}
    
    \includegraphics[width=0.9\linewidth]{fig/diagnearestallv5.pdf}
    \end{center}
       \caption{Qualitative reference-guided image synthesis results on CelebA-HQ. Our method takes the HR source images (top row), and translates them according to the LR target (left column). We also add the real HR target (not seen by the network) for visual comparison. \textbf{See supplementary material for more results}.}
    \label{fig:lots}
    \end{figure*}

\begin{figure*}
    \begin{center}
    \hspace{-1cm}
    \includegraphics[width=0.88\linewidth]{fig/c5.pdf}
    \end{center}
       \caption{Comparison between our method and reference guided Stargan-v2~\cite{starganv} on CelebA-HQ. For both methods, we use the same HR image as source (left column). For the reference image, our method uses the LR images (top row) while Stargan-v2~\cite{starganv} uses the corresponding HR (bottom row).}
    \label{fig:comparison}
    \end{figure*}



\begin{figure}
    \begin{center}
\includegraphics[width=\linewidth]{fig/pulseNN.pdf}
    \end{center}
       \caption{Comparison between our method and PULSE~\cite{pulse} on CelebA-HQ. While both methods use the same LR image (top row), ours also leverages the reference images (left column) to guide the generation process. We also show the corresponding HR (bottom row) for reference only---neither method sees this HR image.}
    \label{fig:pulse}
    \end{figure}
\subsection{Qualitative Evaluation}

Fig.~\ref{fig:comparison} compares images obtained with our framework with those obtained with Stargan-v2~\cite{starganv} using reference-guided synthesis on CelebA-HQ. Since our method focuses on generating images that downscale to the given LR image, the generator learns to merge the high frequency information presents in the source image with the low frequency information of the LR target, while preserving the identity of the person and other distinctive features. Differently from traditional i2i methods that only change the style of the source image while preserving its content, our method adapts the source image to the pose of the LR target. More qualitative samples obtained with our technique are shown in fig.~\ref{fig:lots}, where the first row of HR images are used as source images and the first column is the LR target. We also display the real HR target to show that our model is capable of generating diverse images that are different from the target.


Fig.~\ref{fig:afhq} displays generated samples on AFHQ. Visually, we notice that our model is capable of merging most of the high frequency information coming from HR source image with low frequency information present in the LR target. The degree of this transfer depends on how much information is shared between the domain.  

\subsection{Quantitative Evaluation}
\label{sec:results}
\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
HR image res. & \multicolumn{2}{c}{} & \multicolumn{2}{c}{}  \\
Metric     & FID   & LPIPS           & FID        & LPIPS     \\ 
\midrule
MUNIT~\cite{MUNIT}          & --            & --              & 107.1           & 0.176      \\
DRIT~\cite{DRIT}            & --            & --              & 53.3           & 0.311      \\
MSGAN~\cite{msgan}          & --            & --              & 39.6            & 0.312      \\
Stargan-v2~\cite{starganv}  & 19.58         & 0.22            & \textbf{23.8}   & \textbf{0.38} \\
Ours                        &\textbf{15.52} & \textbf{0.34}   & 25.89           & 0.329       \\
\bottomrule

\end{tabular}}
\caption{Quantitative comparison on the CelebA dataset, comparing our method to other reference-guided i2i methods~\cite{starganv,MUNIT, DRIT, msgan}. We follow same procedure as in Stargan-v2~\cite{starganv}, but for our method we sample ten HR images for each LR image.}
\label{tab:metrics_fid}
\vspace{-0.5cm}
\end{table}

In table~\ref{tab:metrics_fid}, we report FID and LPIPS scores on the results obtained on CelebA-HQ, using two different resolutions,  and . Results with the  resolution show a significantly lower FID of our method compared to \cite{MUNIT, DRIT, msgan}, while being similar to Stargan-v2~\cite{starganv}. We notice a better FID score with the lower  resolution, being then significantly better than Stargan-v2. This is due to the fact that the task is harder with higher scale factor since we need to hallucinate more detailed textural information missing from the LR target.

For a deeper insight on the differences between our method and Stargan-v2, we used the density and coverage metrics of \cite{divCov}. The density measures the overlap between the real data and generated samples, while the coverage measures their diversity, by measuring the ratio of real samples that are covered by the fake samples~\cite{divCov}. Following \cite{divCov}, we used the feature space embedding of both the real and fakes images with a pretrained VGG16~\cite{vgg} on ImageNet. The density metric is then obtained from the -nearest neighbours (with  as in \cite{divCov}) on the 4096 features obtained from the VGG network's second fully connected layer. Diversity results reported in table~\ref{tab:diversity} show higher density values for our method, meaning that their samples are closer to the real data distribution than Stargan-v2. Higher coverage measures are also obtained for our method, meaning a better coverage of the data distribution modes. This is noticeable for the  resolution, since the coverage is close to maximum value (the domain is ), being almost 10\% higher than Stargan-v2 while this gap doubles when the HR value is increased to . This indicates that our model produces more realistic results by exploiting HR information from the source image, while being more diverse by staying faithful to the LR target image.

We also report quantitative results on AFHQ~\cite{starganv} in table~\ref{tab:afhq_metrics}, where we train our model on each domain separately given the higher differences of the domains distributions. Indeed, we found that our method excels on domains where images are structurally similar and share information, such as the ``cat'' domain. However, the ``dog'' and ``wild'' domains show a wider variety of races and species, meaning less information shared between images from the same domain. This reduces the amount of shared information that can be transferred from the HR source, forcing to hallucinate more details out of LR targets. This is confirmed by the lower LPIPS results obtained by our approach compared to Stargan-v2, while keeping similar FID scores.

\subsection{Ablation}
\label{sec:ablation}

Table~\ref{tab:resolution} illustrates the impact of LR resolution on CelebA-HQ. As the LR target resolution increases, the model exploits the information in the LR target more and more over the information provided by the HR source image. This is confirmed by sustained decrease the LPIPS score when the resolution increases from  to , while maintaining similar FID score. The effect of color in the LR target images is also ablated in the supp. material.


\subsection{Comparison to Super-Resolution}
\label{sec:superres}

We now compare our method to PULSE~\cite{pulse}, which super-resolves a face image by optimizing on the latent space of a pretrained StyleGAN model. Fig.~\ref{fig:pulse} presents results that illustrate similarities and differences between PULSE and our method. Both methods generate images that are realistic and faithful to the given LR image. 
From a super-resolution standpoint, our method would be considered reference-guided---but as opposed to image synthesis which is guided by the LR image, the super-resolution reference is another HR face image. We find this provides a certain amount of control over the diversity of the results, which is not possible with PULSE. Our generated samples are therefore much closer to the HR reference image.



\begin{table}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lcccc}
\toprule
HR image res. & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\
Metric    & Density & Coverage & Density & Coverage \\ 
\midrule    
Stargan-v2 & 1.63    & 0.89     & 1.67   & 0.69     \\
Ours       & \textbf{1.97}    & \textbf{0.98}     & \textbf{1.87}   & \textbf{0.92}     \\ 
\bottomrule
\end{tabular}}
\caption{Diversity and coverage metrics~\cite{divCov} comparison on CelebA-HQ with different HR resolutions.}
\label{tab:diversity}
\end{table}



\begin{table}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lcccccc}
\toprule
Category & \multicolumn{2}{c}{Cats}     & \multicolumn{2}{c}{Dogs}             & \multicolumn{2}{c}{Wild} \\ 
Metric & FID  & LPIPS     & FID   & LPIPS     & FID         & LPIPS       \\ 
\midrule
Stargan-v2  & 25.2 & \textbf{0.42} & \textbf{56.5} & 0.34 & \textbf{19.87} & \textbf{0.46} \\
Ours        & \textbf{22.8} & 0.40      & 67.3 & \textbf{0.47}      & 20.61       & 0.23        \\ 
\bottomrule
\end{tabular}}
\caption{Quantitative comparison on the AFHQ dataset. We compare our method to the reference-guided i2i methods Stargan-v2~\cite{starganv} where the target and the source image are from the same domain.}
\label{tab:afhq_metrics}
\vspace{-0.5cm}
\end{table}




\begin{figure}
\begin{center}

\includegraphics[width=0.98\linewidth]{fig//afhq4.pdf}
\end{center}
   \caption{Qualitative results on the AFHQ dataset. Our method takes the HR images, and translate them to the corresponding HR subspace of the given LR target images.}
\label{fig:afhq}

\end{figure}


\begin{table}[]
\centering
\begin{tabular}{lcccc}
\toprule
LR target res. &  &         &        &        \\ 
\midrule
FID        & 15.13             & 15.34    & 19.45    & 13.55    \\
LPIPS        & 0.30           & 0.34     & 0.14     & 0.08     \\ 
\bottomrule
\end{tabular}\caption{Effect of the LR resolution on the FID and LPIPS metrics for the CelebA-HQ dataset.}
\label{tab:resolution}
\vspace{-0.2cm}
\end{table}



