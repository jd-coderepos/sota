\section{Experiments}
\label{sec:exper}

\subsection{Datasets and Evaluation Protocols}
\label{sec:data}

\noindent\textbf{HMDB-51}~\cite{kuehne2011hmdb} has 6766 internet videos/ 51 classes; each video has 20--1000 frames. We report the mean accuracy across three splits.

\noindent\textbf{YUP++}~\cite{yuppp}  has 20 scene classes of  video textures, 60 videos per class. Splits contain scenes captured by the static or moving camera. We use standard splits (1/9 dataset for training) for evaluation.

\noindent\textbf{MPII Cooking Activities}~\cite{rohrbach2012database} contains high-resolution videos of people cooking dishes. 
The 64  activities from 3748 clips include coarse actions \eg, \emph{opening refrigerator}, and fine-grained actions \eg, \emph{peel}, \emph{slice}, \emph{cut apart}. We use the mean Average Precision (mAP) over 7-fold cross validation. For human-centric protocol \cite{anoop_generalized, anoop_rankpool_nonlin}, we use the faster RCNN \cite{faster-rcnn} to crop video around human subjects.

\noindent\textbf{Charades}~\cite{sigurdsson2016hollywood} consist of of 9848 videos of daily indoors activities, 66500 clip annotations and 157 classes.

\noindent\textbf{EPIC-Kitchens}~\cite{Damen_2018_ECCV} is a multi-class egocentric dataset with ~28K training videos associated with 352 noun and 125 verb classes. The dataset consists of 39,594 segments in 432 videos. 
We follow protocol~\cite{Baradel_2018_ECCV}. We evaluate our model on validation, standard seen (S1: 8047 videos), and unseen (S2: 2929 videos) test sets. 


\subsection{Evaluations}
\label{sec:evals}
Below, we show the effectiveness of our method. For smaller datasets, we use the I3D backbone. For  large Charades and EPIC-Kitchens, we additionally investigate  AssembleNet and AssembleNet++ backbones.    Firstly, we evaluate various design components.


\vspace{0.05cm}
\noindent{\textbf{Ground-truth ODF+SVM.}} 
Firstly, we evaluate our ODF on SVM given the HMDB-51 dataset. We set  for Eq. \eqref{eq:moment1} and compare various detector backbones and pooling strategies. Table \ref{tab:det1234} shows that all detectors perform similarly with ({\em det3}) being slightly better than other methods. Moreover, max-pooling on ODFs from all four detectors is marginally better than the average-pooling. However, only the weighted mean ({\em all+wei}) according to Eq. \eqref{eq:wei} outperforms ({\em det3}) which highlights the need for the robust aggregation of ODFs. Similarly, when we combine pre-trained DEEP-HAL with all detectors, the weighted mean ({\em DEEP-HAL+all+wei}) performs best. Table \ref{tab:yup-pool} shows the similar trend on YUP++. We trained SVM only on videos for which at least one  detection occurred, thus a  accuracy is much lower than the main results reported on the full pipeline. Figure \ref{fig:beta} shows that   has a positive impact on reweighting.




\begin{table}[t]\vspace{-0.3cm}
\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ l c c c c }
\toprule
 & {\em sp1} & {\em sp2} & {\em sp3} & mean acc. \\
\hline
{\em det1} &  &  &  & \\
{\em det1} &  &  &  & \\
{\em det3} &  &  &  & \\
{\em det4} &  &  &  & \\
\hdashline
{\em all+avg} &  &  &  & \\
{\em all+max} &  &  &  & \\
{\em all+wei} &  &  &  & \\
\hdashline
{\em DEEP-HAL+all+avg} &  &  &  & \\
{\em DEEP-HAL+all+max} &  &  &  & \\
{\em DEEP-HAL+all+wei} &  &  &  & \\
\bottomrule
\end{tabular}
}
\caption{Evaluations of ODF on HMDB-51. ({\em top}) We evaluate backbones such as ({\em det1}) Inception V2, ({\em det2}) Inception ResNet V2, ({\em det3}) ResNet101 and ({\em det4}) NASNet. ({\em middle}) The average-pooled, max-pooled and the weighted mean combination of all detectors are given by ({\em all+avg}), ({\em all+max}) and ({\em all+wei}). ({\em bottom}) Pre-trained DEEP-HAL combined with all four detectors by the average-pooling, max-pooling and the weighted mean.
}
\vspace{-0.3cm}
\label{tab:det1234}
\end{table}
\begin{table}[t]\vspace{-0.3cm}
\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ l c c c }
\toprule
 & {\em avg} & {\em max} & {\em wei}  \\
\hline
{\em all}          &  &  &  \\
{\em DEEP-HAL+all} &  &  &  \\
\bottomrule
\end{tabular}
}
\caption{Pooling on YUP++. Results for the average-pooled ({\em avg}), max-pooled ({\em max}) and the weighted mean  ({\em wei}) of all detectors ({\em all}) \vs pre-trained DEEP-HAL combined with all detectors by the average-pooling, max-pooling and the weighted mean.
}
\vspace{-0.5cm}
\label{tab:yup-pool}
\end{table}


\begin{figure}[b]\centering \vspace{-0.1cm}
\begin{subfigure}[b]{0.495\linewidth}
\includegraphics[trim=0 0 0 0, clip=true,width=0.99\linewidth]{images/beta-h.pdf}\vspace{-0.2cm}
\caption{\label{fig:beta-h}}
\vspace{-0.2cm}
\end{subfigure}
\begin{subfigure}[b]{0.495\linewidth}
\includegraphics[trim=0 0 0 0, clip=true,width=0.99\linewidth]{images/beta-y.pdf}\vspace{-0.2cm}
\caption{\label{fig:beta-y}}
\vspace{-0.2cm}
\end{subfigure}
\caption{The impact of  in the weighted mean on the classification results. Figure \ref{fig:beta-h} shows results for HMDB-51 on ({\em top}) four detectors combined+SVM and ({\em bottom}) DEEP-HAL with four detectors combined+SVM. Figure \ref{fig:beta-y} shows results for YUP++.}
\vspace{-0.3cm}
\label{fig:beta}
\end{figure}








\vspace{0.05cm}
\noindent{\textbf{Ground-truth SDF.}} The SDF on HMDB-51 and YUP++ yielded  and  accuracy. This is expected as SDFs do not capture a discriminative information per se but they locate salient spatial and temporal regions to focus the main network on them.


\begin{figure}[b]\centering \vspace{-0.1cm}
\begin{subfigure}[b]{0.495\linewidth}
\includegraphics[trim=0 0 0 0, clip=true,width=0.99\linewidth]{images/odf-h.pdf}\vspace{-0.2cm}
\caption{\label{fig:odf-h}}
\vspace{-0.2cm}
\end{subfigure}
\begin{subfigure}[b]{0.495\linewidth}
\includegraphics[trim=0 0 0 0, clip=true,width=0.99\linewidth]{images/odf-y.pdf}\vspace{-0.2cm}
\caption{\label{fig:odf-y}}
\vspace{-0.2cm}
\end{subfigure}
\caption{ODF eval. on SVM on four detectors (the weighted mean). Fig. \ref{fig:odf-h} and \ref{fig:odf-y}  show results on HMDB-51 and YUP++.  and  correspond to the  entries in Eq. \eqref{eq:moment1}. 
}
\label{fig:odf}
\end{figure}

\vspace{0.05cm}
\noindent{\textbf{Multi-moment descr.}} Figure \ref{fig:odf} shows that the concat. of the mean and three eigenvectors according to Eq. \eqref{eq:moment1} yields good results but adding further vectors deteriorates the performance. Adding skewness and kurtosis ( and ) further improves results, while adding eigenvalues has a limited impact. 

\vspace{0.05cm}
\noindent{\textbf{HMDB-51.}} Table \ref{tab:hmdb51f} shows several  DEEP-HAL variants, which all hallucinate BoW/FV/OFF. DEEP-HAL with  our reweighting mechanism.  ({\em DEEP-HAL+W}) outperforms the original DEEP-HAL denoted as ({\em HAF/BoW/FV hal.}) \cite{Wang_2019_ICCV} by . DEEP-HAL with our ODF and SDF descriptors  ({\em DEEP-HAL+ODF}) and ({\em DEEP-HAL+SDF}) outperform ({\em HAF/BoW/FV hal.}) by  and , resp. This shows that both ODF and SDF are effective. Combining DEEP-HAL, ODF and SDF  outperform DEEP-HAL by  demonstrating the complementary nature of ODF and SDF. Utilizing our weighting mechanism with  DEEP-HAL, ODF and SDF denoted as ({\em DEEP-HAL+W+ODF+SDF})  outperform ({\em HAF/BoW/FV hal.}) by . Finally, DEEP-HAL with weighting, and ODF and SDF with RBF feature maps from Eq. \eqref{eq:gauss_lin2a} outperform ({\em HAF/BoW/FV hal.}) by .

\vspace{0.05cm}
\noindent{\textbf{YUP++.}} Table \ref{tab:yupf} shows that ODF is better than SDF, that is ({\em DEEP-HAL+ODF}) and ({\em DEEP-HAL+SDF}) outperform ({\em HAF/BoW/FV hal.}) by  and , resp. This is expected as YUP++ contains dynamic scenes without objects/specific saliency regions correlating with class concepts. However, a combination of detectors/saliency ({\em DEEP-HAL+SDF}) plus weighting ({\em DEEP-HAL+W+ODF+SDF}) plus the RBF  maps ({\em DEEP-HAL+W+G+ODF+SDF}) outperform ({\em HAF/BoW/FV hal.}) by ,  and  accuracy, resp.

\vspace{0.05cm}
\noindent{\textbf{MPII.}} Table \ref{tab:mpiif} shows a  mAP gain over ({\em HAF/BoW/FV hal.}) due to detectors capturing the human interaction with objects. 



\begin{table}[t]\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ l c c c c }
\toprule
 & {\em sp1} & {\em sp2} & {\em sp3} & mean acc. \\
\hline
{\em DEEP-HAL+W} &  &  &  & \\
{\em DEEP-HAL+ODF} &  &  &  & \\
{\em DEEP-HAL+SDF} &  &  &  & \\
{\em DEEP-HAL+ODF+SDF} &  &  &  & \\
{\em DEEP-HAL+W+ODF+SDF} &  &  &  & \\
{\em DEEP-HAL+W+G+ODF+SDF}  &  &  &  & \\
\midrule
\end{tabular}
}
\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\fontsize{9}{9}\selectfont
\centering
\begin{tabular}{ c c }
\kern-0.5em ADL+I3D  \cite{anoop_advers} &  Full-FT I3D  \cite{i3d_net}\\
\kern-0.5em EvaNet (Ensemble)  \cite{Piergiovanni_2019_ICCV} & PA3D + I3D  \cite{Yan_2019_CVPR}\\
 HAF/BoW/FV exact  \cite{Wang_2019_ICCV} & HAF/BoW/FV hal.  \cite{Wang_2019_ICCV}\\
\bottomrule
\end{tabular}
}
\caption{Evaluations of ({\em top}) our methods and ({\em bottom}) comparisons to the state of the art on  HMDB-51.}
\vspace{-0.3cm}
\label{tab:hmdb51f}
\end{table}

\begin{table}[t]\hspace{-0.45cm}
\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ l c c c c c }
\toprule
 & \multirow{2}{*}{\em static} & \multirow{2}{*}{\em dynamic} & \multirow{2}{*}{\em mixed} & mean & mean \\
&                &             &             & {\fontsize{8}{9}\selectfont stat/dyn}  & all \\
\hline
{\em DEEP-HAL+ODF}    				&  &  &  &  &  \\
{\em DEEP-HAL+SDF}    				&  &  &  &  &  \\
 {\fontsize{8}{9}\selectfont {\em DEEP-HAL+SDF+ODF}}  &  &  &  &  &  \\
 {\fontsize{8}{9}\selectfont {\em DEEP-HAL+W+SDF+ODF}}  &  &  &  &  &  \\
 {\fontsize{8}{9}\selectfont {\em DEEP-HAL+W+G+SDF+ODF}}  &  &  &  &  &  \\
\midrule
T-ResNet \cite{yuppp} &  & \% & \% &  & \\
ADL I3D \cite{anoop_advers} &  & \% & - &  & -\\
HAF/BoW/FV hal. \cite{Wang_2019_ICCV} &  & \% & \% &  & \%\\
MSOE-two-stream \cite{Hadji_2018_ECCV} &  & \% & \% &  & \%\\
\bottomrule
\end{tabular}
}
\caption{Evaluations of ({\em top}) our methods and ({\em bottom}) comparisons to the state of the art on YUP++.}
\vspace{-0.3cm}
\label{tab:yupf}
\end{table}


\begin{table}[!tb]\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ l c c c c c c c c }
\toprule
 & {\em sp1} & {\em sp2} & {\em sp3} & {\em sp4} & {\em sp5} & {\em sp6} & {\em sp7} & mAP \\
\hline
{\fontsize{7.5}{9}\selectfont {\em DEEP-HAL+W+ODF+SDF}}      		 &  &  &  &  &  &  &  & \\
{\fontsize{7.5}{9}\selectfont {\em DEEP-HAL+W+G+ODF+SDF}}     &  &  &  &  &  &  &  & \\
\midrule
\end{tabular}
}
\parbox{.99\linewidth}{
\centering
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\fontsize{8}{9}\selectfont
\begin{tabular}{ c c }
\kern-0.5em KRP-FS+IDT  \cite{anoop_rankpool_nonlin} & GRP+IDT  \cite{anoop_generalized}\kern-0.5em\\
\kern-0.5em I3D+BoW/OFF MTL  \cite{Wang_2019_ICCV} & HAF/BoW/OFF hal.  \cite{Wang_2019_ICCV}\\
\bottomrule
\end{tabular}
}
\caption{Evaluations of ({\em top}) our methods and ({\em bottom}) comparisons to the state of the art on MPII.}
\vspace{-0.3cm}
\label{tab:mpiif}
\end{table}

\vspace{0.05cm}
\noindent{\textbf{Charades.}} Table \ref{tab:charades} (top) presents relative gains of our hallucination pipeline ({\em DEEP-HAL}) with weighted mean pooling ({\em W}) and the RBF maps ({\em G}) denoted as ({\em DEEP-HAL+W+G}). We evaluate Object Detection Features ({\em ODF}) and Saliency Detection Features ({\em SDF}) with 512 dim. sketching ({\em SK512})  and note that ({\em DEEP-HAL+W+G+ODF (SK512)}) outperforms ({\em DEEP-HAL+W+G+SDF (SK512)}), and both methods outperform the baseline ({\em HAF/BoW/FV hal.}) \cite{Wang_2019_ICCV}. 

Table \ref{tab:charades} (bottom) shows that combining ODF and SDF into ({\em DEEP-HAL+W+G+SDF+ODF (SK512)}) yields  mAP which constitutes on a  gain over the baseline({\em HAF/BoW/FV hal.}) \cite{Wang_2019_ICCV}. This demonstrates that ODF and SDF are highly complementary. Applying a larger sketch ({\em DEEP-HAL+W+G+ODF+SDF (SK1024)}) yields  mAP which matches the use  ({\em DEEP-HAL+W+G+ODF+SDF (exact)}) that denotes a late fusion by concatenation of ODF and SDF with the stream resulting from DEEP-HAL fed into PredNet. Note that ({\em exact}) indicates that ODF and SDF are not hallucinated at the test time but they are computed. the results matching between ({\em DEEP-HAL+W+G+ODF+SDF (SK1024)}) and  ({\em DEEP-HAL+W+G+ODF+SDF (exact)}) show that we can hallucinate ODF and SDF at the test time while regaining the full performance. We save computational time and hallucinate the detection and saliency features which boost results on Charades by  over the baseline. 

Table \ref{tab:charades2} shows that our idea applied to AssembleNet and AssembleNet++ yields state of the art \eg, we outperform these two networks by \textbf{4.5}\% and \textbf{5.6}\% mAP, respectively. We note that our detectors do not need to be computed at all at the test time. 

In contrast, the best currently reported papers such as SlowFast networks \cite{slowfast} and AssembleNet \cite{assemblenet} achieve 45.2\% and 51.6\% on Charades. 
As SlowFast networks and AssembleNet backbones can be used in place of I3D in our experimental setup,  our approach is `orthogonal' to these latest developments  which focus on heavy mining for combinations of neural blocks/dataflow between them to obtain an `optimal' pipeline. 
We achieve similar results with a simple approach based on self-supervised learning. Our pipeline is lightweight by comparison (no need for computations of the optical flow, or detections or segmentation masks at test time).

\noindent{\textbf{ImageNet (global score) \vs object detectors.}} 
Various scores from the object and saliency detectors which we use cannot be plugged directly into the DEEP-HAL due to the varying number of objects detected and the varying number of frames, thus we propose and use ODF and SDF descriptors. We also note that using a simplified variant of ODF which stacks up ImageNet scores per frame into a matrix (no detectors) to which we apply our multi-moment descriptor yielded  worse results on Charades than our DEEP-HAL+ODF (detectors-based approach) which yields  mAP. This is expected as ImageNet is trained in a multi-class setting (one object per image) while detectors let us model robustly distributions of object classes and locations per frame.

\vspace{0.05cm}
\noindent{\textbf{EPIC-Kitchens.}}Table~\ref{epic-kitchens} shows the experimental results. I3D and AssembleNet/AssembleNet++ learn human-like semantic features due to ODF/SDF, and there is no evidence a backbone can discover these without a guidance. By comparing MPII (3748 clips) with large EPIC-Kitchens (39594 clips) (both about cooking), SDF+ODF boost MPII from 81.8 to 84.8\%, and SDF+ODF boost EPIC-Kitchens from 32.51\% (DEEP-HAL) to 35.88\% (on seen classes protocol), and from 22.33\% (DEEP-HAL) to 27.32\% (on unseen classes protocol). The boost is ~\textbf{3}\% on both MPII and EPIC-Kitchens (nearly 10 more clips than MPII). 


\newcommand{\fsnine}[0]{\fontsize{9}{9}\selectfont}
\newcommand{\fsninee}[0]{\fontsize{9}{9}\selectfont}
\begin{table}[t]\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ c c c }
\toprule
\fsninee HAF/BoW/FV     				& \fsninee {\em DEEP-HAL+}         & \fsninee {\em DEEP-HAL+}     \\
\fsninee hal. \cite{Wang_2019_ICCV}			&  \fsninee {\em W+G+ODF (SK512)}         & \fsninee {\em W+G+SDF (SK512)}  \\
\hline
\fsnine 43.1 & \fsnine 47.22 & \fsnine 45.30   \\
\midrule
\end{tabular}
}
\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ c c c }
\fsninee {\em DEEP-HAL+W+G+}      						& \fsninee {\em DEEP-HAL+W+G+}         				   & \fsninee {\em DEEP-HAL+W+G+}   \\
\fsninee {\em ODF+SDF (SK512)}			& \fsninee {\em ODF+SDF (SK1024)}       & \fsninee {\em ODF+SDF (exact)} \\
\hline
\fsnine 49.06 & \fsnine {\bf 50.14} & \fsnine {\bf 50.16} \\
\bottomrule
\end{tabular}
}
\caption{Evaluations of our methods on  Charades (I3D backbone).}
\label{tab:charades}
\end{table}






\begin{table}[t]\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{c c c c }
\toprule
\multicolumn{4}{c}{{\em AssembleNet++ 50} (Kinetics-400 pre-training)}\\
\fsninee baseline &
\fsninee {\em ODF+SDF (SK512)}			& \fsninee {\em ODF+SDF (SK1024)}       & \fsninee {\em ODF+SDF (exact)} \\
\hline
\fsnine 53.8 & \fsnine 55.81 & \fsnine {\bf 56.94} & \fsnine {\bf 57.30} \\
\midrule
\end{tabular}
}

\parbox{.99\linewidth}{
\setlength{\tabcolsep}{0.12em}
\renewcommand{\arraystretch}{0.70}
\centering
\begin{tabular}{ c c c c }




\multicolumn{4}{c}{{\em AssembleNet++ 50} (without pre-training)}\\
\fsninee baseline &
\fsninee {\em ODF+SDF (SK512)}			& \fsninee {\em ODF+SDF (SK1024)}       & \fsninee {\em ODF+SDF (exact)} \\

\hline
\fsnine 56.7 & \fsnine 60.71 & \fsnine \textbf{61.98} & \fsnine \textbf{62.29} \\
\bottomrule
\end{tabular}
}
\caption{Evaluations of our methods on the Charades dataset (AssembleNet and AssembleNet++ backbones). Note that we do not use segmentation masks for AssembleNet and AssembleNet++, thus baseline results reported by us are slightly lower compared to authors' results of 55.0\% and 59.8\% mAP, respectively.}
\label{tab:charades2}
\end{table}

\begin{table}[!ht]
\begin{center}
\resizebox{0.80\textwidth}{!}{\begin{tabular}{ l c  c c  c c  c }
\toprule
& \multicolumn{2}{c}{Verbs} & \multicolumn{2}{c}{Nouns} & \multicolumn{2}{c}{Actions}\\
\cline{1-7}
& top-1 & top-5 & top-1 & top-5 & top-1 & top-5\\
\hline
& \multicolumn{6}{c}{\bf Validation}\\
LFB Max~\cite{Wu_2019_CVPR} & 52.6 & 81.2 & 31.8 & 56.8 & 22.8 & 41.1\\
WeakLargeScale~\cite{Ghadiyaram_2019_CVPR} & 58.4 & 84.1 & 36.9 & 60.3 & 26.1 & 42.7\\
\hdashline
DEEP-HAL+ODF+SDF(SK1024) & 55.4 & 82.9 & 33.3 & 55.1 & 21.5 & 39.7\\
AssembleNet++ ODF+SDF(SK512) & 57.2 & 84.6 & 34.8 & 56.4 & 23.2 & 41.3\\
AssembleNet++ ODF+SDF(SK1024) & 58.7 & 85.6 & 36.0 & 57.3 & {\bf 24.7} & {\bf 43.0} \\
\hdashline
AssembleNet++ ODF+SDF(exact) & 60.0 & 86.7 & 37.1 & 59.2 & {\bf 25.2} & {\bf 43.4} \\

\midrule
& \multicolumn{6}{c}{\bf Test s1 (seen)}\\
TSN Fusion~\cite{Damen_2018_ECCV} & 48.2 & 84.1 & 36.7 & 62.3 & 20.5 & 39.8\\
LFB Max~\cite{Wu_2019_CVPR} & 60.0 & 88.4 & 45.0 & 71.8 & 32.7 & 55.3\\
WeakLargeScale~\cite{Ghadiyaram_2019_CVPR} & 65.2 & 87.4 & 45.1 & 67.8 & 34.5 & 53.8\\
\hdashline
DEEP-HAL+ODF+SDF(SK1024) & 62.2 & 85.0 & 46.1 & 69.3 & 32.5 & 53.6\\
AssembleNet++ ODF+SDF(SK1024) & 65.0 & 87.8 & 48.8 & 72.5 & {\bf 35.0} & {\bf 56.1}\\
\hdashline
AssembleNet++ ODF+SDF(exact) & 66.2 & 88.5 & 49.3 & 72.8 & {\bf 35.8} & {\bf 56.8}\\
\midrule
& \multicolumn{6}{c}{\bf Test s2 (unseen)}\\



TSN Fusion~\cite{Damen_2018_ECCV} & 39.4 & 74.3 & 22.7 & 45.7 & 10.9 & 25.3\\
LFB Max~\cite{Wu_2019_CVPR} & 50.9 & 77.6 & 31.5 & 57.8 & 21.2 & 39.4\\
WeakLargeScale~\cite{Ghadiyaram_2019_CVPR} & 57.3 & 81.1 & 35.7 & 58.7 & 25.6 & 42.7\\
\hdashline
DEEP-HAL+ODF+SDF(SK1024) & 55.3 & 79.1 & 32.6 & 55.4 & 22.3 & 39.2\\
AssembleNet++ ODF+SDF(SK1024) & 58.3 & 82.1 & 35.2 & 58.2 & {\bf 25.9} & {\bf 42.9}\\
\hdashline
AssembleNet++ ODF+SDF(exact) & 59.0 & 83.3 & 35.7 & 59.0 & {\bf 27.3} & {\bf 44.0}\\


\bottomrule
\end{tabular}}
\end{center}
\caption{Experimental results on the EPIC-Kitchens.
}
\label{epic-kitchens}
\vspace{-0.3cm}
\end{table}
