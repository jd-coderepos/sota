\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  


\usepackage{latexsym}
\usepackage{soul}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathptmx}
\usepackage{nicefrac}
\usepackage{booktabs} \usepackage{makecell} \usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage[font=small]{subcaption}
\usepackage[font=small]{caption}
\usepackage{xargs} \DeclareMathOperator*{\argtopK}{arg\,max_K} \newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{proof}{Proof}

\setcounter{secnumdepth}{2}


\usepackage{color}
\usepackage[textsize=scriptsize]{todonotes}
\definecolor{blue}{RGB}{0, 93, 170}			\definecolor{darkgreen}{RGB}{0, 102, 0}
\newcommand{\kartik}[1]{\todo[color=green!40]{Kartik says: #1}}
\newcommand{\pavan}[1]{\todo[color=darkgreen!60]{Pavan says: #1}}
\newcommand{\ignore}[1]{}
\setlength{\marginparwidth}{1.3cm}









\title{Interactive Question Answering in Multimodal Environments\\ Using External Knowledge as Context}



\author{Anonymous Authors
}

\begin{document}

\maketitle


\begin{abstract}

In this paper, we present the first true use of knowledge-based embeddings from open knowledge sources in order to improve the performance of a reinforcement learning agent. Specifically, we consider the case of a simple reinforcement learning agent 





\end{abstract}


\section{Introduction}
\label{sec:intro}

Question-Answering (QA) and associated tasks have become an extremely important topic of research in the NLP and AI communities in the recent past. The kinds of questions that are handled have gone from simple factoid-based queries to more complex questions that cannot be answered purely by text-based retrieval methods. Such complex questions~\cite{BoPaMiYu18} often require some kind of reasoning, and the retrieval of relevant external knowledge to support that reasoning process. The common thread inherent in the solutions to such tasks is that mere text-based techniques cannot achieve or beat human-level performance, and that NLP systems must instead learn how to utilize additional knowledge from external sources such as knowledge bases (KBs) and knowledge graphs (KGs) to improve their performance on such tasks.


























\section{Related Work}
We will categorize the related work into three primar
\begin{enumerate}
\item Reinforcement Learning
\item \textbf{Question Answering}:\\ 
Question answering (QA) is a long-standing research topic that has been addressed from several different perspectives.
Recently, the rise of contextual language models, like BERT \cite{devlin2019bert}, GPT-2 \cite{radford2019language}, ELMo \cite{peters2018deep} and XLNet \cite{yang2019xlnet}, has fueled the design of many state-of-the-art approaches to question answering over both structured and unstructured data. As an example, a model which employed XLNet has been employed with successful results on the Stanford Question Answering Dataset (SQuAD) \cite{rajpurkar2016squad}.
More recently, A Lite BERT (ALBERT) \cite{lan2019albert} has been shown to yield even better performance on the same dataset, achieving state-of-the-art results.
Recent research is also focusing on the problem of conversational QA, that is a task that closely resembles the way humans acquire information within conversational turns. More precisely, this kind of task allows gathering information by means of a sequence of interconnected questions and answers, thereby giving rise to many challenges, like dealing with coreferences and ellipses. In order to encourage research in this area, many datasets have been recently released, like CoQA \cite{reddy2019coqa} and QuAC \cite{choi2018quac}.
A strongly related problem is given by goal-oriented dialogues and question answering on knowledge bases, where memory-augmented neural networks \cite{weston2014memory,graves2016hybrid} have been employed with promising results \cite{eric2017key,bordes2017learning}.

Recently, a dataset for the task of Complex Sequential Question Answering (CSQA) has been introduced to combine the two problems of
\begin{enumerate*}[label={(\roman*)},font={\itshape}]
\item answering complex questions which could possibly require reasoning over a large KG with several millions of entities, and 
\item learning to handle a conversation from sequences of linked question-answer pairs \cite{saha2018complex}. 
\end{enumerate*}
The authors also show that the use of a complex system which combines hierarchical auto-encoders with key-value memory networks is not sufficient to achieve a good performance.
Results on the same dataset have been slightly improved by \cite{guo2018dialog}, using a semantic-parsing approach that maps natural language utterances into a logical form. However, such logical forms are derived from a grammar that does not cover all the questions in the dataset, and the formal meaning representations used to train the system are derived by relying on the assumption that a logical form is correct if it can be executed to get the expected answer. 

\item External Knowledge
\end{enumerate}


\section{Environment}

Describe briefly here the TW game/environment, as well as the specific setup that we use and design choices, etc.

Text World games can be seen as a Partially Observed Markov Decision Process (POMDP) since or agent never has access to the true underlying world state. We represent the environment as a \textbf{7 tuple( ??)} comprising of the set of environment states, conditional transitional probabilities between the states , the list of commands, observations, observational conditional probabilities, reward function and the discount factor respectively.\cite{cote18textworld}


\section{External Context}

Describe here exactly what kinds of external contexts the agent can have access to. These contexts include:

\begin{enumerate}
    \item No context: Simple agent
    \item Historical
    \item Knowledge? What kinds of knowledge?
    \item All other contexts possible
\end{enumerate}



\section{Agents}

Describe here the agents that are implemented in this work, including but not limited to:

\subsection{Simple Agent}

We consider a simple learning agent to select an appropriate action from the list of available actions and the observed state to achieve the goal described by our environment.

In order to efficiently represent the state space and action space, we consider both random and word embeddings for representations. 

\begin{enumerate}
    \item Model specific details such as GRU, attention, etc
\end{enumerate}


\subsection{Modified Agent}

The modified agent uses historical contextual information to reduce the search space. The historical context includes a list of actions that achieved positive rewards in the past. This helps the learning agent to focus on the sub-goals related to entities specific to the historical context. As in the state and action representations, we use word embeddings to represent this historical context.



\subsection{Knowledgeable Agent}
Knowledgeable agent utilizes some form of external knowledge to reduce complexity of action space. We built on top of the modified agent where the historical contextual information is used along with the knowledge graph such as \textit{Conceptnet} to achieve this goal. The knowledge embeddings are generated from the historical contexts and the knowledge graph.
There can be multiple kinds of knowledgeable agents based on the type of knowledge available. In this paper, we consider two different types of knowledge embeddings for our agents: \textit{Numberbatch} and \textit{Complex}. 
\hl{description of the embedding goes here}
\section{Experiments}

Introduction of knowledge in the form of word embedding.
\begin{figure}[htp]
    \centering
    \includegraphics[width=9cm]{img/example_game_randembvsw2v_comparison.png}
    \caption{Comparison on using embeddings}
    \label{fig:embed}
\end{figure}

\subsection{Quantitative Results}
Bumping up number of experimental runs beyond just  to see if the averages are significant.

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.5]{img/scores_runs_results}
    \includegraphics[scale=0.5]{img/moves_runs_results}
    \caption{Comparison of agents with and without Knowledge graphs}
    \label{fig:scores}
\end{figure*}





\begin{table*}[h]
\centering
\begin{tabular}{lllll} 
\toprule
                     & Simple Agent & Modified Agent & Knowledgeable Agent 1 & Knowledgeable Agent 2   \\ 
\hline
Reward       &  &   &  & \\
No. of Steps &  &   &  & \\
\bottomrule
\end{tabular}
\caption{Comparison of learning agents with and without access to the knowledge graph (mean and standard error averaged over 200 runs) at train time}
\end{table*}




\begin{table*}[h]
\centering
\begin{tabular}{lllll} 
\toprule
                     & Simple Agent & Modified Agent & Knowledgeable Agent 1 & Knowledgeable Agent 2   \\ 
\hline
Average Reward       &  &   &  & \\
Average No. of Steps &  &   &  & \\
\bottomrule
\end{tabular}
\caption{Comparison of learning agents with and without access to the knowledge graph (mean and standard error averaged over 200 runs) at test time.}
\end{table*}


\subsection{Qualitative Analysis}

Need to rely on qualitative results if the quantitatve results above don't really show any interesting trends.


\section{Discussion}

This is where we discuss the results, and the implications of this work for other future work. Can include discussion on RL4KG and KG4RL (both directions). 


\section{Conclusion \& Future Work}

In this work ...

{
\bibliographystyle{aaai}
\bibliography{iqa}
}


\end{document}
