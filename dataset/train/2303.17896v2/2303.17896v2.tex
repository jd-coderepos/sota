\documentclass{bmvc2k}
\pdfoutput=1
\PassOptionsToPackage{table,x11names}{xcolor}
\PassOptionsToPackage{numbers,compress}{natbib}

\usepackage{caption}
\usepackage{subfigure}

\usepackage{booktabs} 
\usepackage{graphicx}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    

\usepackage{hyperref}            
\usepackage{amsthm}
\usepackage{nicefrac}    
\usepackage{multirow}
\usepackage{makecell}
\usepackage{etoolbox}
\usepackage{enumitem}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage{graphics}
\usepackage{amsmath,amssymb,mathtools,bm}
\usepackage{physics} 
\usepackage{siunitx} 
\sisetup{group-separator = {,}}
\usepackage{placeins}
\usepackage[outline]{contour}
\usepackage{cleveref}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{url}              
\usepackage{amsfonts}     
\usepackage{nicefrac} 
\usepackage{microtype}     
\usepackage{adjustbox}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{array, booktabs, boldline} \usepackage{float}

\usepackage{wrapfig,lipsum}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{lemma*}{Lemma}


\title{ \hspace{0.25cm}  Exploring the Limits of Deep Image \\ \hspace{0.25cm}  Clustering using Pretrained Models}

\def\thefootnote{*}\footnotetext{Equal contribution.}

\addauthor{Nikolas Adaloglou\thefootnote}{adaloglo@hhu.de}{1}
\addauthor{Felix Michels\thefootnote}{ felix.michels@hhu.de}{1}
\addauthor{Hamza Kalisch}{  hakal104@hhu.de}{1}
\addauthor{Markus Kollmann}{ markus.kollmann@hhu.de}{1}



\addinstitution{
 Heinrich Heine University \\
 DÃ¼sseldorf, Germany
}

\runninghead{Adaloglou et al.}{Exploring the Limits of Deep Image Clustering}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}


\maketitle



\begin{abstract}
We present a general methodology that learns to classify images without labels by leveraging pretrained feature extractors. Our approach involves self-distillation training of clustering heads, based on the fact that nearest neighbours in the pretrained feature space are likely to share the same label. We propose a novel objective that learns associations between image features by introducing a variant of pointwise mutual information together with instance weighting. We demonstrate that the proposed objective is able to attenuate the effect of false positive pairs while efficiently exploiting the structure in the pretrained feature space. As a result, we improve the clustering accuracy over -means on  different pretrained models by \% and \% on ImageNet and CIFAR100, respectively. Finally, using self-supervised vision transformers we achieve a clustering accuracy of \% on ImageNet. The code is available at \url{https://github.com/HHU-MMBS/TEMI-official-BMVC2023}.
\end{abstract}

\section{Introduction}
\label{intro}
Given a plethora of publicly available pretrained vision models, we ask the following questions: a) how well-structured is the feature space of pretrained architectures with respect to label-related information, and b) how to best adapt this structure to unsupervised tasks. To answer these questions, we focus on unsupervised image classification, also known as image clustering. Image clustering is the task of assigning a semantic label to an image, given an a priori finite set of classes. Ultimately, image clustering consists of simultaneously learning the relevant representations and the cluster assignments. 



To begin addressing the aforementioned questions, we present the key challenges regarding image clustering. First, given that we can roughly estimate the number of ground-truth labels, the underlying distribution among classes is hard to infer from the data, which is typically assumed to be uniform. Second, the images should ideally be classified consistently (images of the same class are grouped together) and confidently (one-hot prediction probability).
\newpage
Consistency can be achieved by either learning features that are invariant under transformations of the same image (e.g.\ cropping, colour jitter, etc.) or invariant w.r.t.\ to substitution by other images that belong to the same semantic class.
Consequently, clustering methods are generally prone to degenerate solutions \cite{sscn}. In other words, samples tend to collapse into a single cluster, or the prediction probability spreads out uniformly.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\textwidth]{figures/fig1.pdf}
\end{center}
\caption{
\textbf{Clustering accuracies on ImageNet (left) and CIFAR100 (right) across 17 pretrained models.} Supervised and self-supervised models (MSN, MoCoV3, DINO) were pretrained on ImageNet. R50 stands for ResNet50 \cite{resnet}, C for ConvNext \cite{convnext}, and V for Vision Transformer \cite{vit}. Small (S), Base (B), and Large (L) indicate the size of the models. The vertical distance of each data point to the diagonal (dashed line) shows the improvement of our method (TEMI) over -means. Best viewed in color.} 
\label{fig:main}
\end{figure*}

 
It is well-established that representation learning plays a critical role in image clustering \cite{chang2017deep}, which is achieved with self-supervised learning \cite{simclr, moco,zbontar2021barlow,byol}. Recent studies have demonstrated that self-supervised features are typically more transferable to new tasks than features from supervised learning \cite{ericsson2021ssltranfer}. The frequently used joint-embedding architectures \cite{byol,dino} are by design invariant to strong image transformations that preserve label-related information. That renders these architectures as promising candidates for image clustering \cite{scan}, which has not been thoroughly explored at scale \cite{tsp}. Even though self-supervision \cite{ericsson2021ssltranfer} and vision transformers (ViTs) \cite{naseer2021intriguing} have been separately established for representation learning, limited research has been conducted to study self-supervised ViTs or vision-language models (i.e.\ CLIP \cite{radford2021clip}).

How to adapt a pretrained model for image clustering is non-trivial. For instance, it is well known that -means is sub-optimal, as it often leads to imbalanced clusters \cite{scan} since it is primarily suitable for evenly scattered data samples around their centroids \cite{yang2017towards}. On the other hand, deep image clustering methods normally rely on pairs by mining the nearest neighbours (NN) based on their feature similarity \cite{dwibedi2021little,huang2019and}. Still, images that are close in the feature space do not always share the same semantic class \cite{scan} and, therefore, must be considered as noisy pairs. 

In this paper, a two-stage method that extends the existing multi-stage clustering approaches is proposed. In contrast to \cite{scan}, where features are learned from scratch for each dataset, we show that multi-stage clustering approaches can leverage pretrained models trained on larger-scale datasets (Fig.\ 1) and focus on learning the cluster assignments. To this end, a self-distillation clustering framework is introduced using a novel objective based on pointwise mutual information and instance weighting. Second, a comprehensive experimental study across models and datasets is conducted. Therein, we report an average gain of \% and \% in clustering accuracy compared to -means on ImageNet and CIFAR100 across  pretrained models, as illustrated in \Cref{fig:main}. Overall, we show that ViTs capture the most transferable label-related features. Finally, we find that self-supervised ViTs \cite{msn} achieve state-of-the-art results (\% clustering accuracy) on ImageNet without using the ground-truth labels or external data.


\section{Related Work}
\label{related}
\noindent\textbf{Single-stage Deep Image Clustering Methods.} Deep image clustering approaches can be roughly divided into single and multi-stage methods. The majority of single-stage methods alternate between learning the features and the clusters, i.e.\ in an expectation-maximization manner. For instance, in DAC, \cite{chang2017deep} formulate a binary pairwise-classification task, where at each iteration, pairs are selected based on their feature similarity. Next, the computed pairs are used to train a convolutional neural network (CNN). In the same direction, in DeepCluster \cite{deepcluster}, the authors alternate between clustering the features of a CNN with -means \cite{lloyd1982kmeans} and using the obtained cluster assignments as pseudo-labels to optimize the parameters of the CNN. 

Later on, in \cite{sela}, the authors demonstrate that DeepCluster is prone to degenerate solutions that are avoided via particular hyperparameter choices. To that end, the authors design a multi-step pseudo-label extraction framework called SeLa. The latter iteratively estimates the pseudo-label assignment matrix under the equipartition constraint. In PCL \cite{pcl}, the authors formulate clustering as learning the cluster centroids with -means in parallel with optimizing the network via contrastive learning \cite{simclr}. To overcome the class collision of the negative pairs, \cite{propos} extend PCL in a proximal framework called ProPos. ProPos only maximizes the distance between the cluster centroids with contrastive learning while mining NN for neighbouring sample alignment \cite{byol}. However, most of the existing approaches still rely on -means for estimating the clusters (pseudo-labels).

Several single-stage approaches exist, which aim to learn the feature representations and clusters jointly. Single-step methods are known to be sensitive to weight initialization \cite{dang2021nearest}. In this direction, DCMM is developed \cite{wu2019deep} to progressively mine NN in the feature space as well as high-confident samples. Another single-stage end-to-end example is IIC, wherein \cite{ji2019invariant} derives a mutual information-based objective for paired data to train a CNN. Nevertheless, the aforementioned approaches only consider stochastic transforms of the same image to obtain a pair. They are hence limited to solely learning invariances w.r.t.\ image augmentations, which cannot cover the variability of a given class \cite{dwibedi2021little}. More recently, \cite{sscn} presented a single-stage end-to-end method, called SSCN, that employs a variant of the cross-entropy loss.

\noindent\textbf{Multi-stage Deep Image Clustering Methods}.
Multi-stage methods initially design a pretext task in order to learn semantically meaningful features, such as denoising autoencoders \cite{xie2016unsupervised}. A major breakthrough in deep image clustering is established by the adoption of contrastive learning \cite{simclr,moco}. For instance, \cite{scan} decouples image clustering into three distinct steps, starting with contrastive learning. Subsequently, the authors train a head to cluster the mined NN from the extracted features. Lastly, they use the pseudo-labels from the confidently assigned samples to fine-tune the whole architecture. A similar approach, called NNM \cite{dang2021nearest}, aims to mine NN from the batch and dataset features. 

Recently, \cite{tsp} leverage self-supervised pretrained ViTs \cite{dino} and train a clustering head, which is closer to our method. Nevertheless, their approach (TSP) heavily relies on -means for the weight initialization phase. Surprisingly, very few image clustering approaches \cite{scan,sscn,sela} have been successfully applied on ImageNet \cite{deng2009imagenet}. Besides, most methods report results only with Resnet50 \cite{resnet}, while superior architectures for image recognition remain unexplored \cite{convnext,vit}.

\section{Proposed Method}
\label{method}

\newcommand{\pmi}{\operatorname{pmi}}
\newcommand{\pmib}{\pmi^\beta}
\newcommand{\bpmi}{\pmi^\beta}
\newcommand{\KL}[2]{\operatorname{KL}(#1 \parallel #2)}
\newcommand{\MI}[2]{\operatorname{I}(#1;#2)}
\newcommand{\Entropy}{\operatorname{H}}
\newcommand{\Expected}[2]{\operatorname{\mathbb{E}}_{#1} [#2]}
\newcommand{\pthet}{q}
\newcommand{\popt}{\pthet^*}
\newcommand{\pstud}{q_s}
\newcommand{\pteach}{q_t}
\newcommand{\probEma}{\operatorname{EMA}}

\subsection{Classification Model}
\label{subsec:mi-objective}
Our aim is to learn a probabilistic classifier from pairs of examples that share label-related information. 
We assume that the data distribution, , is the result of a generative process,  and , with  the prior probability that an example belongs to a class . Consequently, the joint distribution,  that a pair of examples, , belongs to the same class is given by 
We introduce a parametrized probabilistic classifier, , that distributes examples  among classes, with class occupancy given by . Using Bayes' theorem , the joint distribution, , can be approximated by 

To estimate the association between  and  we introduce the pointwise mutual information,  \cite{church1990word}, defined by

 
\begin{theorem}
\label{thm:optimal}
    If (i) each example  belongs to one and only one cluster under the generative model , (ii) the joint distribution  is known, and (iii)  is a probabilistic classifier defined by
    
    then  is equal to the optimal probabilistic classifier, , up to a permutation of cluster indices.
\end{theorem}
The proof can be found in the supplementary material. \Cref{thm:optimal} states that under condition (i) the knowledge of pairs of examples belonging to the same class suffices to establish an objective for an optimal classification model.


\subsection{Self-distillation Clustering Framework} 
The starting point is a pretrained feature extractor (backbone)  that assigns each example  in the dataset  a feature vector . We mine the  nearest neighbours (-NN) of  in the feature space by computing the cosine similarity between  and the feature vectors of all other images in . We denote the set of -NN for x by . During training, we randomly sample  from  along with  from , to generate image pairs that share label information with high probability.

We introduce two clustering heads, a \emph{student head}, , and a \emph{teacher head}, , that share the same architecture but differ w.r.t.\ their parameters, , and . Each head consists of a three-layer fully connected feed-forward network. The image pairs  are fed to the shared backbone and subsequently, in the two heads,  and . The head outputs are converted to probabilistic classifiers,  and , using a temperature-scaled softmax function, which for the student's head is given by

where  is the temperature hyperparameter. Unlike previous self-distillation frameworks \cite{dino}, we use the same temperature  for both heads. We approximate the pointwise mutual information by 

and estimate  by an exponential moving average (EMA) over batches using the teacher's head

with  the batch size and  a momentum parameter. In practice, we symmetrize \Cref{eq:objective} to compute the loss function

Note that only the parameters  of the student's head are updated using backpropagation. The parameters of the teacher's head, , are updated by an exponential moving average for the student parameters, , over past update steps \cite{dino,byol}. As a result,  represents a sufficiently stable target distribution for the student head. In contrast to other self-distillation frameworks \cite{dino}, no complicated adaptation of softmax temperatures over training is required. Following previous work \cite{scan}, we employ an ensemble of  independent clustering heads in training (\cref{fig:diagram}), which alleviates the variability stemming from random initialization. For the evaluation, we use the teacher head with the lowest training loss. 



\subsection{Balancing class utilization}
For a dataset  that has been generated using balanced classes, , we expect that , as a consequence of the optimization process. However, in practice, we observe that classes are typically far from uniformly utilized. We suspect that our self-distillation learning framework leads to over-confident class predictions for a fraction of classes in the early training phase. 

To limit the effect of aligning the cluster predictions and take into account the individual cluster probability of the mini-batches , we introduce a hyperparameter  in \Cref{eq:objective} to avoid collapsing all sample pairs in a single cluster. Without affecting the optimal solution, we rewrite \Cref{eq:objective} as





where  is the head index. 

\subsubsection{Intuition for .} We now provide an explanation of how the above equation addresses the discussed challenges of image clustering. \Cref{eq:objective-pmi} consists of two parts inside the  sum: the numerator  encourages the class assignment of a positive pair to align (consistency) and is maximal when this assignment is one-hot. The denominator  promotes a uniform cluster distribution by up-weighing the summand corresponding to classes with low probability ( is low). In effect,  balances these two effects by reducing the influence of the numerator, and thus degenerative solutions are avoided. 

Note that for , the loss corresponds to the Bhattacharyya distance \cite{bhattacharyya1946measure} if . The Bhattacharyya distance can be minimal even if  is far from one-hot. Moreover, if utilization of all classes is not required -- for example, as in the case of overclustering -- we set . Crucially,  is bounded, and we empirically found that the value of  consistently avoids degenerative solutions across the majority of datasets, as opposed to existing clustering methods \cite{scan,sscn}. In addition, we propose an experimental strategy to choose  without access to the ground-truth labels, as explained in \Cref{sec:experimental-results}. The symmetrized loss from \Cref{eq:objective-pmi} is defined as  in analogy to \Cref{eq:symmetrized-loss}.  


\subsection{Teacher-guided Instance Weighting}
\label{subsec:weight}
As discussed in \Cref{intro}, the mined -NN in the feature space of  tend to be noisy. For this reason, we introduce an instance weighting term for each head  given by

Intuitively,  acts as a guidance term that assigns a higher weight to true positive pairs compared to false positive ones. Importantly,  relies only on the predictions of the teacher. The rationale behind this is that model averaging over training iterations tends to produce more accurate predictions \cite{tarvainen2017mean,polyak1992acceleration}. We call this setup teacher-weighted pointwise mutual information (WPMI). The final objective for each separate head  is given by  


\subsubsection{TEMI: Teacher Ensemble-weighted pointwise Mutual Information}
To compensate for the noisy NN pairs based on the feature space of , we further propose to aggregate information from multiple heads, in contrast to previous works \cite{scan} that employ independent heads. For this purpose, we compute a scalar for each image pair using the mean weight across the heads, which is conceptually similar to model ensembling. We thus call this loss TEMI (teacher ensemble-weighted pointwise mutual information) defined by


\section{Experimental evaluation}
\label{experimental}

\subsection{Datasets, Metrics and Implementation Details}
The proposed method (TEMI) is evaluated on five common benchmark datasets, namely CIFAR10, CIFAR20, CIFAR100 \cite{cifar}, STL10 \cite{stl10}, and ImageNet \cite{deng2009imagenet}. CIFAR10, CIFAR20 and CIFAR100 contain  training images, STL10 contains  training samples, and ImageNet has \num{1281167} training samples. CIFAR20 has the same training data as CIFAR100 with  superclasses derived from the ground-truth labels. We resize all images to . The training set is used during the optimization phase, while the evaluations are carried out on the validation set. Additional information can be found in the supplementary material.

To quantify the clustering performance, we report the clustering accuracy (ACC) and the adjusted random index (ARI). To estimate the accuracy, the one-to-one mapping between cluster predictions and ground-truth labels is computed by the Hungarian matching algorithm \cite{kuhn1955hungarian}. For our overclustering experiments, we report the adjusted mutual information (AMI). Finally, we establish two baselines: a) -means and b) the SCAN clustering loss within our self-distillation framework. For a fair comparison with existing methods, we assume to know in advance the number of ground-truth labels. Concerning the hyperparameters, we set  and  for clustering, while we set  for overclustering. We use -NN on ImageNet and -NN for the remaining datasets. We used the AdamW optimizer \cite{adamw} for  epochs with a batch size of , with a learning rate of , weight decay of  and report results at the end of training. Unlike previous methods \cite{scan}, we found that augmentations (RandAugment \cite{cubuk2020randaugment}, and the ones from \cite{simclr}) were not improving the clustering metrics when training with -NN pairs. Hence, we precomputed the feature representations, which enables training the clustering heads on a single GPU with GB of VRAM within  hours.   




\begin{figure}[htbp]
\begin{minipage}{.5\textwidth}
     \includegraphics[width=\textwidth]{figures/beta_acc_entropy.pdf}
\caption{\textbf{Effect of  on the validation accuracy and on the entropy of  and  on CIFAR100}. The values are computed using TEMI DINO ViT-B/16. The dashed horizontal line illustrates the maximal possible entropy, i.e. . A high entropy of  indicates that the clusters are almost uniformly utilized, while a low entropy of  indicates highly confident predictions (one-hot).}
\label{fig:beta_ent}
\end{minipage}
\hfill
\begin{minipage}{.45\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{figures/overview_fig.pdf}
\end{center}
\caption{\textbf{An overview of the proposed self-distillation clustering framework.} The nearest neighbors are mined in the feature space of . EMA refers to the exponential moving average over the parameters.}
\label{fig:diagram}
\end{minipage}
\end{figure}



     
\subsection{Experimental Results}
\label{sec:experimental-results}
We first present a strategy to choose . As depicted in \Cref{fig:beta_ent}, an accurate model,  should be able to maintain a high entropy , while maintaining its discriminative power. To quantify the latter, we use the conditional entropy . The lower the value of , the more discriminative the predictions. The extreme case  corresponds to a one-hot distribution. Thus, we propose to pick the lowest value of  such that  remains sufficiently low. We experimentally found  to work consistently well across models and datasets. 
 
 
As shown in \Cref{table:losses}, an average accuracy gain of \% over -means is found for CIFAR100 and ImageNet, even with the plain PMI setup. Introducing multiple heads in PMI further improves the obtained results by an average gain of \%. Critically, for our best setup (TEMI), we observe an average gain of \% and \% compared to -means and the SCAN clustering loss, respectively. Note that even  heads were sufficient to get similar performance, specifically less
than 1\% accuracy deterioration compared to 50 heads.

\begin{wraptable}{O}{.55\textwidth}

 \begin{center}
  \begin{tabular}{lccc}
    \toprule
    Method  & Heads & CIFAR100 & ImageNet  \\
    \midrule
    \textit{k}-means & - & 56.99 & 52.26  \\
    SCAN & 50 & 62.60.94 & 55.60.15 \\
    \hline
    PMI & 1 & 61.60.41 & 57.50.22  \\
    WMI & 1 & 63.41.89 & 56.50.41 \\
    \hline
    PMI & 50 & 63.10.56 & 57.70.06  \\
    WPMI & 50 & 65.61.04 & 57.00.38 \\
    TEMI & 50 & \textbf{67.11.30} & \textbf{58.40.22}  \\
    
    \bottomrule
  \end{tabular}
  \end{center}
    \caption{\textbf{Ablation study for the TEMI objective}. All the experiments were conducted with , and DINO ViT-B/16 as the backbone model. The clustering accuracy is reported in \%.}
    \label{table:losses}
\end{wraptable} 


To study the applicability of our method, we then applied our best setup (TEMI) to various publicly available pretrained models, as shown in Fig \ref{fig:main}. Therein, we report an average accuracy gain of \% and \% compared to -means on ImageNet and CIFAR100 across  different pretrained models. More specifically, TEMI MSN ViT-L/14 and TEMI DINO ViT-B/16 are the best-performing self-supervised methods on ImageNet (\% ACC) and CIFAR100 (\% ACC). Moreover, CLIP-based backbones have the highest ACC increase over -means when trained with TEMI, precisely \% on ImageNet and \% on CIFAR100.

\begin{table}[htbp]
\begin{minipage}{.45\textwidth}
  \begin{tabular}{lccc}
    \toprule
    Method  & Arch.   & ACC &   ARI \\
    \hline
    SeLa \cite{sela} & Resnet50  & 30.5 &  16.2  \\
    SCAN \cite{scan} & Resnet50    & 39.9 & 27.5  \\
    SSCN \cite{sscn} & Resnet50    & 41.1  & 29.5 \\
    \midrule
     \multicolumn{4}{l}{\textit{Our method}}\\	
    TEMI DINO  & Resnet50    & 45.2  & 31.3 \\
    TEMI DINO & ViT-B/16     & 58.4  &  45.9 \\
    TEMI MSN   & ViT-L/16  & \textbf{61.6}  &  \textbf{48.4} \\
    \bottomrule
  \end{tabular}
  \newline \newline
\caption{\textbf{Clustering results for the ImageNet validation set, without using additional data.} Evaluation metrics include clustering accuracy (ACC), and adjusted random index (ARI) in \%. All our models are pretrained on ImageNet.}
\label{tab:in1k}
\end{minipage}
\hfill
\begin{minipage}{.45\textwidth}

\begin{tabular}{lc}
  \toprule
  Method      & AMI (\%)   \\
  \midrule
  DeepCluster~\cite{deepcluster}  & 28.1 \\
  MoCo~\cite{moco}        & 28.5 \\
  PCL~\cite{pcl}     & 41.0 \\
  ProPos~\cite{propos}     & 52.5 \\
  \midrule
  TEMI DINO Resnet50 & 51.80.1 \\
  TEMI DINO ViT-B/16 & \textbf{59.90.2} \\
  TEMI MSN ViT-L/16 & 58.80.5 \\
  \bottomrule
  \end{tabular}
  \newline \newline
  \caption{\textbf{Overclustering results on the ImageNet validation set.} The adjusted mutual information (AMI) score for 25K clusters is reported, as in \cite{pcl}. For all experiments, we set .}
  \label{tab:result_overclustering_in1k}
\end{minipage}
\end{table}
 Concerning the supervised pretrained models in Fig. \ref{fig:main}, we demonstrate that ConvNext-L outperforms ViT-L on ImageNet, precisely by \% on ACC with TEMI. However, the supervised ViT-L surpasses ConvNext-L by a large margin of \% in ACC when benchmarked on CIFAR100 with TEMI. Among the architectures investigated, large ViTs learn the most transferable label-related features, even without supervised fine-tuning. This finding is consistent with \cite{naseer2021intriguing}. 




\textbf{Clustering and overclustering results on ImageNet.} Regarding ImageNet, we compare various self-supervised architectures that were trained without any external data, as depicted in \Cref{tab:in1k}. Using the same architecture (Resnet50) as current state-of-the-art models (SSCN \cite{sscn}), TEMI achieves an improvement of \% in ACC. With MSN ViT-L/16 as the backbone, we push the state-of-the-art ACC on ImageNet to \%, resulting in a substantial gain of \% compared to SSCN. The obtained results strongly indicate that first learning the augmentation-invariant features and then focusing on learning the invariances w.r.t.\ images that belong to the same class is an effective strategy for image clustering. Incentivized by the above observation, we investigate the overclustering performance in \Cref{tab:result_overclustering_in1k} by adopting the setup from \cite{pcl}. More concretely, we use K clusters and set  as in ProPos. We almost match the performance of ProPos \cite{propos} with TEMI DINO Resnet50 without tuning the number of clusters or any other hyperparameter while reaching a considerable gain of \% in AMI with TEMI DINO ViT-B/16.


\textbf{Small-scale benchmarks.} In \Cref{tabel:small-datasets-sota}, the transfer performance on three small-scale datasets is evaluated. TEMI DINO ViT-B backbone has inarguably the best transfer performance, outperforming the ACC of ProPos by 4.6\% and TSP by 2.9\% on average. It is worth pointing out that TSP \cite{tsp} uses the same pretrained model, and it is thus a fair comparison. Ultimately, we notice a large accuracy gap between clustering methods and probing in CIFAR20, which suggests that the superclass structure cannot be inferred from the visual input. For instance, clocks, lamps, and telephones are grouped into household electrical devices.

\begin{table}[htbp]
\begin{minipage}{.5\textwidth}

\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{ l c c c c   }
			\toprule
			{Methods} & {CIFAR10}     &  {CIFAR20}      & {STL10} \\\hline
NNM \cite{dang2021nearest} & 84.3 & 47.7  & 80.8 \\  
PCL \cite{pcl} &  87.4 & 52.6  & 41.0 \\ SCAN \cite{scan} &  88.3  & 50.7 &  80.9  \\
SPICE \cite{niu2022spice} &  92.6  & 53.8  & 93.8 \\
ProPos \cite{propos} & 94.3 & 61.4 & 86.7 \\ TSP \cite{tsp}  & 94.0 & 55.6 & 97.9  \\ \hline
TEMI &  \textbf{94.5} &  \textbf{63.2}  &  \textbf{98.5} &  \\ 
\hline 
  \multicolumn{4}{l}{\textit{supervised baseline}}\\
  \multicolumn{1}{l}{Probing } & 96.8  & 89.5 & 99.2 \\ 
			\bottomrule
		\end{tabular}
\end{adjustbox}
\newline \newline
\caption{\textbf{Clustering accuracy on small datasets.} Methods with  use DINO ViT-B pretrained on ImageNet, while  indicates methods that include the validation split during training.}
\label{tabel:small-datasets-sota}


\end{minipage}
\hfill
\begin{minipage}{.45\textwidth}





\begin{center}
\begin{tabular}{lcc}
\toprule
Loss &  Val.  & Train  \\
 &   ACC &  ACC \\
\midrule
PMI   & \textbf{84.10.36}  & \textbf{98.60.38} \\
TEMI & 82.60.67  & 96.50.88 \\
\hline
 \multicolumn{3}{l}{\textit{supervised baseline}}\\
Probing & 85.3  & 99.3 \\
\bottomrule
\end{tabular} 
\end{center}

 \caption{\textbf{Clustering accuracies on CIFAR100 when training only with the true positive NN pairs using TEMI DINO ViT-B/16.}}
 \label{table:true-pairs}

  
\end{minipage}
\end{table}
 
\textbf{Analysis on noise (false positives) from the NN of .} As shown in Tab.\ 5, when keeping only the true positive neighbours from 50-NN, we increased the performance from    using TEMI DINO ViT-B on CIFAR100. We also show that the head weighting term of TEMI in Eq.\ 10 is not needed, highlighting that TEMI is designed for the noisy pairs obtained from -NN. As a reference, DINO ViT-Base has 72\% true positive pairs in 20-NN and 66\% in the mined 50-NN on CIFAR100.


\subsection{Discussion}
\label{discussion}
\noindent\textbf{How expressive can an image classifier be by only training with NN pairs?} We examine the training accuracy in \Cref{table:true-pairs}, by training with the true positive pairs from the computed -NN. The 98.6\% training accuracy on CIFAR100 with TEMI DINO ViT-B/16 indicates that it is possible to train a powerful unsupervised image classifier by only relying on pairs. In fact, we observe that we almost match the supervised linear probing accuracy on CIFAR100 (84.1\% vs 85.3\%). Still, we identify cases where the human-annotated label is ambiguous and cannot be determined solely by the visual signal (supplementary material).

\noindent\textbf{What is the impact of the instance weighting term?} 
After training, we examined the actual value of the instance weighting term . To this end, we computed the mean weights for true positives and false positives sampled from -NN within the CIFAR100 validation set, which take the values  and , respectively. Furthermore,  has a negative impact when only true positive pairs are considered during training (\Cref{table:true-pairs}). This is an expected behavior, as a fraction of true positive pairs will be down-weighted by  due to low feature similarity (i.e. digital and analog clocks).

\noindent\textbf{How discriminative are the cluster assignments of TEMI?}
Besides Fig.\ 2, we quantify the discriminative power of TEMI by computing the mean and median maximum softmax probability (MSP \cite{hendrycks2016msp}). We calculate a mean and median MSP of 88.5\% and 98.9\% on CIFAR100 and 85.3\% and 99.2\% on ImageNet. The computed results verify that the introduced framework results in discriminative predictions.


\noindent{\textbf{Joint learning of encoder and cluster head.}} When jointly training the pretrained backbone with the already trained head, we observed a performance increase only when the pretraining dataset was different from the downstream dataset (67.1\%  70.9\% ACC on CIFAR100 using the ImageNet-pretrained DINO ViT-B/16). We hypothesize this enables learning features specific to the training distribution. Still, the structure in the latent space of the pretrained model is required for TEMI to determine the -NN.

\section{Conclusion}
\label{conclusion}
In this paper, a novel and general self-distillation framework for image clustering was proposed that can achieve competitive results almost out of the box. In addition, a new objective based on pointwise mutual information was presented. After studying the performance of  pretrained models, it was shown that TEMI can be used with any pretraining with significant improvements over -means. Finally, new state-of-the-art results were achieved on ImageNet both for clustering and overclustering, leveraging self-supervised ViTs. To conclude, future works are encouraged to explore the connection between image clustering and representation learning in greater depth.
\bibliography{bibl}

\clearpage{}\appendix



\section{Further discussion points}

\subsection{How to choose  for a new dataset?}
Here, we provide a more detailed explanation of Fig. 3 (in the main text) on how to pick  without access to ground-truth data. First, the motivation behind  is to avoid the imbalanced growth of clusters during training. The closer  is to , the more balanced the clusters (clusters contain a similar number of examples). The reason is that the loss contribution to assign each training sample a single class is reduced for smaller . However, for , each sample occupies all clusters with equal probability. Consequently, we have to impose , but  should be sufficiently close to . We take for  the value when the conditional entropy,  (Fig. 3 in the main text, green line), is starting to become constantly low. We experimentally found  to work consistently well across models and datasets. An exception is CIFAR20, where we used  since superclasses are conceptually a form of under-clustering. 


\subsection{Fine-tuning the pretrained backbone with TEMI}
Given a pretrained backbone network, fine-tuning the backbone simultaneously with training randomly initialized heads gave bad results. However, fine-tuning the backbone simultaneously with fine-tuning the already trained head with TEMI yielded superior performance but only when the pretraining dataset was different from the downstream dataset, e.g.  for CIFAR100 using DINO ViT-B/16 pretrained on ImageNet as the backbone model.

\subsection{Additional computational complexity from multiple heads}
In theory, the computational time complexity of TEMI by adding multiple heads is linear, given a sequential implementation. In practice, due to GPU-related optimizations, it's much faster. In fact, training on a single Nvidia A100 GPU takes only 4 GB of memory with  heads on CIFAR100. Training takes just about 45 minutes because we precompute the feature representations while training with just one head takes about 5 minutes.

\begin{figure*}[h]
    \begin{center}
    \subfigure[CIFAR100 using TEMI DINO ViT-B/16]{
    \includegraphics[width=0.47\columnwidth]{figures/hist_assignment_CIFAR100.pdf}
    }\subfigure[ImageNet using TEMI MSN ViT-L/16]{
    \includegraphics[width=0.47\columnwidth]{figures/hist_assignment_IN1K.pdf}
    }
    \newline
    \caption{\textbf{Histogram of cluster assignments on different datasets}. The horizontal red line illustrates the ideal histogram, where all clusters would be uniformly utilized. We also compute the KL divergence between the predictions and the uniform distribution on CIFAR100 and ImageNet, which is  and , respectively. The predictions would be uniform in the extreme case where the KL divergence is 0.}
    \label{fig:pk_histogram}
     \end{center}
\end{figure*} 
\subsection{How discriminative are the resulted cluster assignments of TEMI?}
Besides Fig. 2 in the main text, we quantify the discriminative power of TEMI by computing the mean and median maximum softmax probability (MSP \cite{hendrycks2016msp}). We calculate a mean and median MSP of 88.5\% and 98.9\% on CIFAR100 and 85.3\% and 99.2\% on ImageNet. The computed results verify that the introduced framework results in discriminative predictions.



\subsection{Are multiple heads necessary?}
The idea of using multiple heads is inspired by previous works, such as SCAN \cite{scan} and SSCN \cite{sscn}. The proposed PMI objective does not require multiple heads by design. As shown in Table 3, we experimentally observed an initial gain of 0.8\% by adding independent heads (PMI and WMI setup with 50 heads). Importantly, one of our core novelties lies in the combination of the teacher predictions from multiple heads, Eq. (10), in the main text, which provides superior results compared to having independent heads (Table 3 in the main text).  Overall, we find the reported performances saturate quickly with more heads and are already close to the maximum for 16 heads on CIFAR100. Based on our first results on CIFAR100, we fixed the number of heads to 50 for all models and datasets.


\subsection{Contrastive versus non-contrastive self-supervised pretraining for image clustering.}
The performance gap between contrastive (MoCoV3 ViT-B) and non-contrastive (DINO ViT-B) backbones likely originates from the homogeneous distribution of examples in feature space as part of the contrastive learning objective, which likely attenuates the necessary structure in feature space for image clustering \cite{wang2020understanding,propos}.

\begin{table*}
	
 \begin{center}
 \begin{adjustbox}{width=1\columnwidth}
		\begin{tabular}{ l c c c c c c c c c c c }
			\toprule
			\multicolumn{1}{c}{Datasets} & \multicolumn{3}{c}{CIFAR10} &      & \multicolumn{3}{c}{CIFAR20} &      & \multicolumn{3}{c}{STL10} \\
			\cmidrule{2-4}\cmidrule{6-8}\cmidrule{10-12}    \multicolumn{1}{c}{Methods} & NMI(\%)  & ACC(\%) & ARI(\%) &      & NMI(\%) & ACC(\%) & ARI(\%) &      & NMI(\%) &ACC(\%)& ARI(\%)\\
\hline
DAC (\citeauthor{chang2017deep}) & 39.6 & 52.2 & 30.6 &      & 18.5 & 23.8 & 8.8  &      & 36.6 & 47   & 25.7 \\ 
DCCM (\citeauthor{wu2019deep}) & 49.6 & 62.3 & 40.8 &      & 28.5 & 32.7 & 17.3 &      & 37.6 & 48.2 & 26.2 \\
PICA (\citeauthor{huang2020deep}) & 59.1 & 69.6 & 51.2 &      & 31   & 33.7 & 17.1 &      & 61.1 & 71.3 & 53.1 \\  
NNM (\citeauthor{dang2021nearest}) & 74.8 & 84.3 & 70.9 &      & 48.4 & 47.7 & 31.6 &      & 69.4 & 80.8 & 65 \\  
PCL (\citeauthor{pcl}) &  80.2 & 87.4 & 76.6        &      & 52.8 & 52.6 & 36.3       &  & 71.8 & 41.0 & 67.0  \\ SCAN (\citeauthor{scan}) & 79.7 &  88.3 & 77.2 &      & 48.6 & 50.7 & 33.3 &      & 69.8 & 80.9 & 64.6 \\
SPICE (\citeauthor{niu2022spice}) & 86.5 &  92.6 & 85.2 &      & 56.7 & 53.8 & 38.7 &      & 87.2 & 93.8 & 87.0 \\
ProPos (\citeauthor{propos})  &  88.6 & 94.3 & 88.4   &      & 60.6 & 61.4 & 45.1   &    & 75.8 & 86.7  & 73.7  \\ TSP (\citeauthor{tsp})  & 88.0 & 94.0 & 87.5 &      &61.4 & 55.6 & 43.3 &      & 95.8 & 97.9 & 95.6 \\ \hline

    
	 TEMI DINO ViT-B/16 & \textbf{88.60.05} &  \textbf{94.50.03} &  \textbf{88.50.08} &      & \textbf{65.40.45} &  \textbf{63.20.38} &  \textbf{48.90.21} &      & \textbf{96.50.13} &  \textbf{98.50.04} &  \textbf{96.80.09} \\

     TEMI MSN ViT-L/16 & 82.90.16 &  90.00.14 &  80.70.22 &      & 59.80.04 &  57.80.42 &  42.50.08 &      & 93.61.10 &  96.70.89 &  93.01.74 \\
     
\hline 
   \multicolumn{12}{l}{\textit{(natural language) supervised pretraining}}\\

TEMI CLIP ViT-L/14 &  92.60.13 &  96.90.07 &  93.20.15 &      &  64.50.12 &  61.81.47 &  46.81.17 &      & 96.40.79 &  97.40.69 &  94.91.26 \\

TEMI Sup. ViT-L/16 & 91.80.65 &  96.00.53 &  91.61.02 &      & 65.00.89 &  58.40.98 &  45.41.41 &      & 82.72.94 &  84.62.37 &  73.92.77 \\
		\hline \hline
   \multicolumn{12}{l}{\textit{supervised baselines}}\\
  
  \multicolumn{1}{l}{Probing DINO ViT-B/16 }   & 92.5 & 96.8 & 93.1 &     & 82.4 & 89.5 & 79.5 &      & 97.8 & 99.2 & 98.2 \\ 

       \multicolumn{1}{l}{Probing MSN ViT-L/16} & 91.5 & 96.4 & 92.3 &   & 80.7    & 88.2 & 77.0 & & 96.8 &   98.8   & 97.4 \\

     \multicolumn{1}{l}{Probing CLIP ViT-L/14}   & 95.1 & 98.1 & 95.8 &     & 85.7 & 91.7 & 83.6 &      & 99.2 & 99.7 & 99.4 \\ 
     
  \multicolumn{1}{l}{Probing Sup. ViT-L/16}   & 91.5 & 96.5 & 92.4 &     & 83.7 & 
90.8 & 81.7 &      & 98.0 & 99.3 & 98.4 \\ 
			\bottomrule
		\end{tabular}\end{adjustbox}	
  \end{center}
	\caption{\textbf{Clustering performance metrics on small-scale benchmark datasets, evaluated on their validation splits.} Probing means training a linear layer on top of the pretrained backbone in a supervised manner. We only highlight the best self-supervised pretrained model as the new state-of-the-art. We clarify that methods with  use models pretrained on external data, while  indicates methods that include additional dataset splits during training (i.e. validation data).}
    \label{tabel:small-datasets-sota}
\end{table*}
 \begin{table}[h]
\begin{center}
  
		\begin{tabular}{ l c c c }
			\toprule
   \multicolumn{1}{c}{Methods} & NMI(\%)  & ACC(\%) & ARI(\%) \\
\hline    
	TEMI DINO ViT-B/16 &  \textbf{76.90.45} &  \textbf{67.11.30} &  \textbf{53.31.02}   \\    	
    TEMI MSN ViT-L/16    &73.00.20 &  61.40.16 &  47.40.42  \\
\hline 
   \multicolumn{4}{l}{\textit{(natural language) supervised pretraining}}\\

TEMI CLIP ViT-L/14 &  79.90.23 &  73.70.92 &  61.20.75       \\
TEMI Sup. ViT-L/16  &85.20.34 &  81.80.73 &  70.60.89  \\
		\hline \hline
   \multicolumn{4}{l}{\textit{supervised baselines}}\\
  \multicolumn{1}{l}{Probing DINO ViT-B/16 }      & 85.7 & 85.3 & 73.6  \\ 

    \multicolumn{1}{l}{Probing MSN ViT-L/16}   & 84.6 & 84.4 & 71.9  \\

    \multicolumn{1}{l}{Probing CLIP ViT-L/14}        & 87.4 & 87.1 & 76.5  \\ 
     
   \multicolumn{1}{l}{Probing Sup. ViT-L/16}      & 86.0 & 
86.3 & 75.0     \\ 
			\bottomrule
		\end{tabular}\end{center}	
	\caption{\textbf{Clustering performance metrics on on the CIFAR100 dataset.} All methods use models pretrained on external data.}
    \label{tabel:cifar100-only}
\end{table}
 
\subsection{A Note on CIFAR100 VS CIFAR20}
We observe that previous works have established the CIFAR20 as a clustering benchmark. However, we believe that the CIFAR20 superclasses are not an ideal benchmark for image clustering. In the reported results in \cref{tabel:small-datasets-sota}, one can easily notice that all models perform worse in CIFAR20 than in CIFAR100 (\cref{tabel:cifar100-only}). Examples that justify the performance gap include a) clocks, computer keyboards, lamps, telephones, and televisions are grouped into household electrical devices, b) bridges, castles, houses, roads, and skyscrapers are grouped into large man-made outdoor things, and c) bears, leopards, lions, and wolfs are grouped into carnivores. These examples illustrate that the superclasses are not separable from the pixel information alone. To this end, we would like to encourage future works to adopt CIFAR100 as a benchmark for image clustering, as shown in \cref{tabel:cifar100-only}. 



\begin{table*}
\begin{adjustbox}{width=0.95\columnwidth,center}
		\begin{tabular}{ l c c c c c c c c c c c }
			\toprule
			\multicolumn{1}{l}{Datasets} & \multicolumn{3}{c}{ImageNet 50} &      & \multicolumn{3}{c}{ImageNet 100} &      & \multicolumn{3}{c}{ImageNet 200} \\
			\cmidrule{2-4}\cmidrule{6-8}\cmidrule{10-12}    \multicolumn{1}{l}{Methods} & NMI(\%)  & ACC(\%) & ARI(\%) &      & NMI(\%) & ACC(\%) & ARI(\%) &      & NMI(\%) &ACC(\%)& ARI(\%)\\
				\hline
				
			SCAN (Resnet50) & 82.2 & 76.8 & 66.1 &      & 80.8 & 68.9 & 57.6 &      & 77.2 & 58.1 &	47.0 \\
  
  Propos (Resnet50)  & 82.8 & - & 69.1 &      &  83.5 & - & 63.5 &      & 80.6 & - & 53.8 \\

	\hline
TEMI DINO ViT-B/16 & 86.100.54 &  80.011.26 &  70.931.24 &      & 85.650.30 &  75.051.11 &  65.451.11 &      & 85.200.21 &  73.120.72 &  62.130.59 \\

TEMI MSN ViT-L/16 & \textbf{88.140.55} &  \textbf{84.871.16} &  \textbf{76.461.17} &      & \textbf{88.530.56} &  \textbf{82.860.73} &  \textbf{74.081.20} &      & \textbf{86.650.32} &  \textbf{77.960.71} &  \textbf{66.700.71} \\

\hline 
   \multicolumn{12}{l}{\textit{(natural language) supervised pretraining}}\\


TEMI CLIP ViT-L/14 & 92.320.38 &  88.270.53 &  82.780.94 &      & 90.060.53 &  83.431.98 &  75.811.36 &      & 88.390.16 &  77.760.37 &  69.410.23 \\

TEMI Sup. ViT-L/16 & 95.750.60 &  95.121.61 &  91.401.88 &      & 94.950.21 &  92.500.23 &  87.950.31 &      & 93.940.02 &  90.370.14 &  84.050.09 \\
	
  \hline
  \hline
     \multicolumn{12}{l}{\textit{supervised baselines}}\\
  
  \multicolumn{1}{l}{Probing DINO ViT-B/16 }  & 95.10 & 95.76 & 91.64 &     & 93.29 & 92.74 & 86.30 &      & 91.64 & 89.48 & 80.61 \\

  
  \multicolumn{1}{l}{Probing MSN ViT-L/16} & 94.21 & 94.92 & 90.03 &     & 93.00 & 92.42 & 85.74 &      & 91.36 & 89.02 & 79.88 \\
 
 \multicolumn{1}{l}{Probing CLIP ViT-L/14} 
   & 98.72 & 98.96 & 97.88 &     & 96.61 & 
96.16 & 92.73 &      & 95.09 & 93.57 & 88.00 \\

 
 \multicolumn{1}{l}{Probing Sup. ViT-L/16 }  & 97.77 & 98.12 & 96.21 &     & 96.13 & 95.76 & 91.90 &      & 95.07 & 93.60 & 88.02 \\
			\bottomrule
   \vspace{0.5cm}
   \end{tabular}
	\end{adjustbox}	
  \caption{\textbf{Clustering performances on ImageNet subsets.} All subsets were evaluated on their respective validation splits, as detailed in \Cref{tab:datasets-info}.}
    \label{tabel:imagenet-subsets}
\end{table*}

 


\clearpage
\section{Proof of Theorem 1}
\label{proofs}


We will first show that 


is bounded and leads to the correct joint distribution.

\begin{lemma}
\label{lemma:expected_pmi}
The mutual information

is an upper bound for the expected pointwise mutual information. In particular,

where  is the Kullback--Leibler divergence.  
\end{lemma}

\begin{proof}

\end{proof}

For the proof of Theorem 1, we now assume that each example  belongs to exactly one cluster  and need to show that the model  maximizing the objective

is equal to  up to a permutation of the clusters.
\begin{proof}
Since  is one-hot by assumption, let us denote the class to which an image  belongs as , so that we have

where the Iverson bracket  is  if  and  otherwise.
We denote the prediction of the classifier by , with

An equivalent formulation of the theorem then is:
 is one-hot for every  and  if and only if .

Using the same factorization we used to formulate the pointwise mutual information in Equation (2) we obtain



\Cref{lemma:expected_pmi} already states that the objective is maximized if and only if 
and therefore


If , we have

Since , this implies
 and therefore
.
Furthermore, from the pigeonhole principle it follows that  for  which both implies that  is one-hot as well as  if , therefore concluding the proof.
\end{proof} 


\section{Additional implementation details.}
To enforce reproducibility, the means and standard deviations are reported for all our experiments and metrics, computed over  independent runs with different seeds. For a fair comparison with SCAN, we tune its entropy regularization hyperparameter, , based on a grid search and use the value . Crucially, we found that some pretrained models (i.e.\ MSN) produce unnormalized features. For that reason, we standardize the features of all models before feeding them to the clustering heads. For the linear probing experiments, we trained a linear layer using the Adam \cite{adam} optimizer with a learning rate of  and weight decay of . 







\section{Randomly sampled images from TEMI cluster assignments}
\enlargethispage{1\baselineskip}
\label{appendix:random-image-samples}
\begin{figure}[H]
\onecolumn\includegraphics[width=\textwidth]{figures/cluster_imgs_supp.pdf}
\vspace{0.3cm}
\caption{\textbf{Randomly sampled images from the ImageNet dataset that are assigned in the same cluster using the TEMI MSN ViT-L/16 model}. The ground-truth label is indicated in the text under the image. The images in each row are assigned to the same cluster. The first four columns correspond to correctly classified images, while the last four are examples of misclassified images.}
\end{figure}
\clearpage
\begin{figure*}[h!]
\includegraphics[width=\textwidth]{figures/cluster_imgs_supp2.pdf}
\vspace{0.3cm}
\caption{\textbf{More randomly sampled images from the ImageNet dataset that are assigned in the same cluster.}}
\end{figure*}



\begin{table}[t]
\begin{center}
\begin{tabular}{lrrrc}
\toprule
\textbf{Dataset} & \textbf{Classes} & \textbf{Train images} & \textbf{Val images} & \textbf{Size}\\ 
\midrule
CIFAR10 & 10 & 50,000 & 10,000 & 32  32\\
CIFAR100 & 100 & 50,000 & 10,000 & 32  32\\
CIFAR20 & 20 & 50,000 & 10,000 & 32  32\\
STL10 & 10 & 5,000 & 8,000 & 96  96\\
ImageNet-50 & 50 & 64,274 & 2,500 & 224  224\\
ImageNet-100 & 100 & 128,545 & 5,000 & 224  224\\
ImageNet-200 & 200 & 256,558 & 10,000 & 224  224\\
ImageNet & 1000 & 1,281,167 & 50,000 & 224  224\\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{An overview of the number of classes and the number of samples on the considered datasets.} The train set is used for training, while the validation split is used to compute the clustering performance metrics. The selected classes on the ImageNet \cite{deng2009imagenet} subsets (ImageNet-50, ImageNet-100, and ImageNet-200) can be found in SCAN \cite{scan}.}
\label{tab:datasets-info}
\end{table} \begin{table}[b]
\begin{center}
\label{tab:pretrain-hp} 
    \begin{tabular}{cc}
\toprule
config & value \\
\hline
optimizer & AdamW \\
base learning rate &  \\
weight decay &  \\
optimizer momentum &  \\
batch size & 512, 1024 (ImageNet)  \\
learning rate schedule & constant \\
softmax temperature  & 0.1 \\
 & 0.6, 0.55 (CIFAR20) \\
cluster heads & 50 \\
warmup epochs & 20, 10 ImageNet \\
training epochs & 200, 800 (STL10) \\
teacher momentum & 0.996 \\
augmentation & None \\
\bottomrule
\end{tabular}\end{center}
\caption{\textbf{Hyperparameters for training the clustering heads.}}
\end{table} \begin{table}[b]
\label{tab:finetune-hp} 
\begin{center}
\begin{tabular}{cc}
\toprule
config & value \\
\hline
optimizer & Adam \\
learning rate &  \\
weight decay &  \\
optimizer momentum &  \\
batch size & 256 \\
learning rate schedule & cosine decay \\
training epochs & 100 \\
augmentation & None \\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Hyperparameters for linear probing.}}
\end{table} 

\begin{table}[h]
\begin{center}
\begin{tabular}{lcccc}
			\toprule
			\multicolumn{1}{c}{Datasets} & \multicolumn{2}{c}{ImageNet} &     \multicolumn{2}{c}{CIFAR100}  \\
			\cmidrule{1-2} \cmidrule{3-5}    
   \multicolumn{1}{c}{Methods} &  TEMI  & \textit{k}-means  &       TEMI  & \textit{k}-means  \\
				\hline
		\multicolumn{5}{l}{\textit{self-supervised methods}}\\		
      MAE ViT-B/16    & 9.090.05 & 4.93 & 7.780.10 & 7.11 \\
    MAE ViT-L/16    & 27.810.13 & 12.45 & 19.560.17 & 12.05 \\
    MAE ViT-H/16    & 22.340.11 & 10.18 & 17.640.19 & 11.31 \\
 \hline
     MOCOv3 ViT-S/16 & 16.730.19 & 12.23 & 16.580.16 & 13.63 \\
    MOCOv3 ViT-B/16 & 54.100.08 & 47.64 & 63.510.53 & 49.94 \\
 \hline
	DINO Resnet50   & 45.200.23  & 32.07 & 45.340.41 & 34.21  \\
	DINO ViT-S/16   & 56.840.25 & 51.84  & 61.690.75 &  50.17 \\
	DINO ViT-B/16   & 58.080.26 & 52.26  & \textbf{67.111.30} & \textbf{57.01} \\
 \hline

    MSN ViT-S/16    & 58.530.39 & 55.58  & 63.060.89 & 49.96 \\
    MSN ViT-B/16    & 60.820.06 & 57.56  & 65.571.23 & 50.60 \\
    MSN ViT-L/16    & \textbf{61.560.28} & \textbf{58.08}  & 61.400.15 & 54.08 \\
 \hline
  \hline
  \multicolumn{5}{l}{\textit{natural language supervised methods}}\\
    CLIP Resnet50   & 45.930.11 & 34.41 & 34.060.72 & 25.96  \\ 
	CLIP ViT-B/16   & 56.680.24 & 45.86 & 60.740.79 & 45.84 \\ 
	CLIP ViT-L/14   & \textbf{63.990.38} & \textbf{54.12} & \textbf{73.700.92} & \textbf{54.55} \\
 \hline
  \hline
  \multicolumn{5}{l}{\textit{supervised methods}}\\	
   Resnet50   & 72.600.18 & 65.69 & 49.770.43 & 40.28 \\
    ConvNext S      & 77.670.41 & 71.85 & 57.310.20 & 43.19\\
   ConvNext B      & 78.230.12 & 73.67 & 58.310.76 & 43.20 \\
    ConvNext L      & \textbf{79.770.20} & \textbf{76.98} & 59.430.24 & 47.94 \\
   ViT-S/16   & 64.720.14 & 60.32 & 60.600.97 & 50.65 \\ 
	 ViT-B/16   & 69.230.27 & 64.48 & 63.360.43 & 51.72 \\
	ViT-L/16   & 77.120.21 & 74.91 & \textbf{81.770.73} & \textbf{70.06} \\

			\bottomrule
		\end{tabular}\label{tab:addlabel}\label{table:sota-comparison}
	\newline \newline \caption{\textbf{Benchmarking various models with the introduced objective versus \textit{k}-means.} We report the clustering accuracy (ACC) in \%}
\end{center}
\end{table}
	 \clearpage{}
\end{document}