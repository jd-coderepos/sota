

\section{Experiments}

In this section, we start by introducing some key implementation details, followed by the description of the datasets as well as the evaluation metrics. Then we present some quantitative comparisons between the propose AIC-Net and some other existing works. Furthermore, qualitative comparisons are given through visualization. Finally, comprehensive ablation studies are performed to inspect some critical aspects of AIC-Net.



\subsection{Implementation Details}
\vspace{-0.1cm}
\noindent
In our AIC-Net, we stack three AIC modules for each branch in the multi-stage feature aggregation part, and two AIC modules are adopted to fuse these features. All the AIC modules used are the bottleneck version as shown in Fig.~\ref{fig:AIC_bottleneck}. For the three AIC modules in feature aggregation, the bottleneck layer is used to decrease the dimensionality of the features from  to . For the AIC modules in feature fusion part, the dimensionalities of features before and after the bottleneck layer are  and .  
Unless stated otherwise, we use three candidate kernels with kernel size  for each dimension of all AIC modules. More details about the network structure can be found in the supplements. 

Our model is trained by using SGD with a momentum of 0.9 and a weight decay of . The initial learning rate is set to be 0.01, which decays by a factor of 10 every 15 epochs. The batch size is 4. We implement our model using PyTorch. All the experiments are conducted on a PC with 4 NVIDIA RTX2080TI GPUs. 


\begin{table*}[t]
\begin{center}
\scalebox{0.88}
{
\begin{tabular} {l |c c c|c c c c c c c c c c c|c} \hline
 &  \multicolumn{3}{c|}{scene completion} & \multicolumn{12}{c}{semantic scene completion} \\ 
\hline
Methods  & prec. & recall & IoU & \cellcolor{rgb1}ceil. & \cellcolor{rgb2}floor & \cellcolor{rgb3}wall & \cellcolor{rgb4}win. & \cellcolor{rgb5}chair & \cellcolor{rgb6}bed & \cellcolor{rgb7}sofa & \cellcolor{rgb8}table & \cellcolor{rgb9}tvs & \cellcolor{rgb10}furn. & \cellcolor{rgb11}objs. & avg. \\ 
\hline
\hline
AIC-Net, = & 88.2 & {\bfseries90.3} & {\bfseries 80.5} & {\bfseries53.0}  & 91.2 & {\bfseries 57.2} & 20.2 & {\bfseries 44.6} & 58.4 & 56.2 & {\bfseries 36.2}  & {\bfseries9.7} & {\bfseries 47.1} & 30.4 & {\bfseries 45.8} 
\\

\hline
AIC-Net, = & {\bfseries88.3} & 89.5 & 79.9 & 51.0 & 91.3 & 56.8 & 18.6 &  41.3 & {\bfseries58.6} & {\bfseries 59.4} & 34.6  & 4.8 & 46.7 & {\bfseries 30.9} & 44.9  \\
AIC-Net, = & 86.3 & {\bfseries 90.3} & 79.1 & 50.7 & {\bfseries 91.7} &54.5 &{\bfseries 21.2} & 38.0 &  55.5& 57.1 &33.2&  7.9& 44.9& 29.4 & 44.0 \\

AIC-Net, = & 87.8 & 88.2 & 78.4 & 49.6 & 91.3 & 55.3 & 15.7 & 38.7 & {\bfseries 58.6} & 52.8 & 30.9 &  0. &  43.9 & 30.2 & 42.5 \\
\hline


\end{tabular}
}  \caption{The performance of AIC-Net under different kernel sets. We use the same kernel set  for each dimension. Results are reported on NYUCAD~\cite{zheng2013beyond} dataset.}
\vspace{-0.3cm}
\label{tab:kernels}
\end{center}
\end{table*}
 
\begin{table*}[t]
\begin{center}
\scalebox{0.85}
{
\begin{tabular} {l |c c c|c c c c c c c c c c c|c} \hline
 &  \multicolumn{3}{c|}{scene completion} & \multicolumn{12}{c}{semantic scene completion} \\ 
\hline
Methods  & prec. & recall & IoU & \cellcolor{rgb1}ceil. & \cellcolor{rgb2}floor & \cellcolor{rgb3}wall & \cellcolor{rgb4}win. & \cellcolor{rgb5}chair & \cellcolor{rgb6}bed & \cellcolor{rgb7}sofa & \cellcolor{rgb8}table & \cellcolor{rgb9}tvs & \cellcolor{rgb10}furn. & \cellcolor{rgb11}objs. & avg. \\ 
\hline
\hline
NYU  &&&&&&&&&& \\
\hline

AIC-Net-noMFs & {\bfseries71.4} & 79.0 & {\bfseries59.9} & 22.3 & {\bfseries90.8} & 32.0  & 14.4 & 14.5 & 47.5 & 41.3 & 12.6 & 16.8 & 32.8 & 12.7 & 30.7
\\
AIC-Net  &62.4&{\bfseries91.8}&59.2& {\bfseries23.2}& {\bfseries90.8} & {\bfseries32.3} & {\bfseries14.8} & {\bfseries18.2} & {\bfseries51.1} &{\bfseries44.8} &{\bfseries15.2} &{\bfseries22.4} &{\bfseries38.3} &{\bfseries15.7}&{\bfseries33.3} 
\\

\hline
\hline
NYUCAD  &&&&&&&&&& \\
\hline


AIC-Net-noMF & 87.2 & {\bfseries90.3} & 79.6 & 51.1  & {\bfseries91.7}  & 57.0   & 18.5  & 39.3  & 51.4  & 51.8  & 30.7   & 1.3  & 45.0   & 30.1 & 42.5
\\
AIC-Net & {\bfseries88.2} & {\bfseries90.3} & {\bfseries 80.5} & {\bfseries53.0}  & 91.2 & {\bfseries 57.2} & {\bfseries20.2} & {\bfseries 44.6} & {\bfseries58.4} & {\bfseries56.2} & {\bfseries 36.2}  & {\bfseries9.7} & {\bfseries 47.1} & {\bfseries 30.4} & {\bfseries 45.8} 
\\
\hline

\end{tabular}
}  \caption{The importance of the modulation factors. AIC-Net-noMFs denotes we set all the modulation factors to be 1. Results are reported on the NYU~\cite{silberman2012indoor} and NYUCAD~\cite{zheng2013beyond} datasets.}
\vspace{-0.6cm}
\label{tab:ModulationFactor}
\end{center}
\end{table*}
 
\noindent
\textbf{Datasets.}
We evaluate the proposed AIC-Net on two SSC datasets. One dataset is the NYU-Depth-V2~\cite{silberman2012indoor}, which is also known as the NYU dataset. The NYU dataset consists of 1,449 depth scenes captured by a Kinect sensor. Following SSCNet~\cite{song2017_SSCNet}, we use the 3D annotations provided by~\cite{rock2015completing} for semantic scene completion task. 
The second dataset is the NYUCAD dataset~\cite{firman2016NYUCAD}. This dataset uses the depth maps generated from the projections of the 3D annotations to reduce the misalignment of depths and the annotations and thus can provide higher-quality depth maps.


\noindent
\textbf{Evaluation metrics.}
For semantic scene completion, we measure the intersection over union (IoU) between the predicted voxel labels and ground-truth labels for all object classes. 
Overall performance is also given by computing the average IoU over all classes. For scene completion, all voxels are to be categorized into either empty or occupied. A voxel is counted as occupied if it belongs to any of the semantic classes. For scene completion, apart from IoU, precision and recall are also reported. Note that the IoU for semantic scene completion is commonly accepted as a more important metric in the SSC task.  








\subsection{Comparison with the State-of-the-Art}
\vspace{-0.1cm}
We compare our AIC-Net with the state-of-the-art methods on NYU and NYUCAD. The results are reported in Table~\ref{tab:NYU} and Table~\ref{tab:NYUCAD}, respectively. In Table~\ref{tab:NYU}, we can see that for the semantic scene completion our method significantly outperforms other methods in overall accuracy.
The proposed AIC-Net achieves 2.9\% better than the cutting-edge approach DDRNet~\cite{li2019rgbd} in terms of the average IoU.
For scene completion, our method is slightly outperformed by DDRNet~\cite{li2019rgbd}. The scene completion task requires to predict the volumetric occupancy, which is class-agnostic. Since our AIC-Net aims at modeling the object variation voxel-wisely, its advantage will fade in the binary completion task.
In Table~\ref{tab:NYUCAD}, our AIC-Net achieves the best semantic segmentation performance as well, and our average IoU outperforms the second-best approach by . For scene completion, our method also observes superior performance, although the advantage is not as significant. 
Among the comparing methods, SSCNet~\cite{song2017_SSCNet} is built using standard 3D convolution. The inferior performance lies twofold. First, the fixed receptive field is not ideal for addressing object variations. Second, 3D convolution is resource demanding, which can limit the depth of the 3D network and consequently sacrifices the modeling capability.

Another interesting observation from these two tables is that our AIC-Net tends to obtain better performance on some categories that have more severe shape variations, \eg \emph{chair}, \emph{table}, \emph{objects}. 




\subsection{Qualitative Results}
\vspace{-0.1cm}
In Fig.~\ref{fig:viz_NYUCAD}, we show some visualization results to evaluate the effectiveness of our AIC-Net qualitatively.
Generally, we can see that the proposed AIC-Net can handle diverse objects with various shapes and thus give more accurate semantic predictions and shape completion than SSCNet~\cite{song2017_SSCNet} and DDRNet~\cite{li2019rgbd}.   
Some challenging examples include ``chairs'' and ``tables'' in Row 1, Row 3, and Row 5, which require a model to adaptively adjust the receptive field voxel-wisely. For example, for some more delicate parts like ``legs'', a smaller receptive field can be more beneficial. 
It shows that our AIC-Net can identify such objects more clearly.
While for some other objects like ``windows'' in Row 5 and Row 7, it expects to see the larger context. Both SSCNet and DDRNet fail in this case, but our method still successfully identifies them from other surrounding distractors. 
The ``bed'' in Row 2, the ``wall'' in Row 6, and the ``sofa'' in Row 4 also demonstrate the superiority of our approach.
In Row 8, the ``objects'' marked by the red dashed rectangle are in a messy environment. Our AIC-Net is less vulnerable to the influence of surrounding objects and more accurately distinguishes the categories and shapes of these ``objects''.


\begin{table*}[t]
\begin{center}
\scalebox{0.88}
{
\begin{tabular} {l |c c c|c c c c c c c c c c c|c} \hline
 &  \multicolumn{3}{c|}{scene completion} & \multicolumn{12}{c}{semantic scene completion} \\ 
\hline
method  & prec. & recall & IoU & \cellcolor{rgb1}ceil. & \cellcolor{rgb2}floor & \cellcolor{rgb3}wall & \cellcolor{rgb4}win. & \cellcolor{rgb5}chair & \cellcolor{rgb6}bed & \cellcolor{rgb7}sofa & \cellcolor{rgb8}table & \cellcolor{rgb9}tvs & \cellcolor{rgb10}furn. & \cellcolor{rgb11}objs. & avg. \\ 
\hline
DDRNet-DDR-ASPP~\cite{li2019rgbd}   &  88.7 & 88.5 & 79.4 & 54.1 & 91.5 & 56.4 & 14.9 & 37.0 & 55.7 & 51.0 &  28.8 & 9.2 & 44.1 &  27.8 & 42.8 \\

DDRNet-AIC-ASPP & 87.9 & 89.1 & 79.4 & 48.0 & 90.9 & 56.1 & 20.1 & 41.6 & 56.6 & 55.0 & 33.1 & 12.6 & 45.3 & 29.0 & 44.4 \\

DDRNet-DDR-AIC &88.0 & 89.6 & 79.7 & 49.0 & 91.4 & 57.6 & 19.7 & 40.5 & 52.3 & 52.9 & 32.5 & 6.1 & 44.6 & 30.7 & 43.4\\

DDRNet-AIC-AIC & 87.5 & 89.3 & 79.1 & 51.7 & 91.5 & 56.4 &   16.5 &   44.1 &  56.3 &  56.4 &  35.4 &  12.3 &  46.1 & 30.4 & 45.2 \\

\hline

\end{tabular}
}  \caption{AIC module as plug-and-play modules. The components of DDRNet~\cite{li2019rgbd} are replaced by the AIC modules. Results are reported on NYUCAD~\cite{zheng2013beyond} dataset.}
\vspace{-0.6cm}
\label{tab:AIC}
\end{center}
\end{table*}
 \begin{table}[t]
\begin{center}
\scalebox{0.8}
{
\begin{tabular} {l|c|c|c|c} 
\hline
Methods 								& Params/k 	& FLOPs/G 	& SC-IoU	& SSC-IoU \\ \hline
SSCNet~\cite{song2017_SSCNet} 			& 930.0  	& 163.8     & 73.2		& 40.0 \\   
DDRNet~\cite{li2019rgbd} 			& 195.0  	& 27.2 	& 79.4	& 42.8 \\ 
\hline









3D conv, =(3, 3, 3) & 440.1 & 61.0 & - & - \\ 

3D conv, =(5, 5, 5) & 1443.6 & 191.1 & - & - \\ 

3D conv, =(7, 7, 7) & 3675.9 & 480.4 & - & - \\ 

\hline

AIC-Net, =  & 628.7 & 85.5 & 79.1 & 45.2 \\ 

AIC-Net, =  & 847.0 & 113.7 & 80.5 & 45.8\\ 

AIC-Net, =  & 716.0 & 96.77 & 79.9 & 44.9 \\


\hline
\end{tabular}
}
\caption{Params, FLOPs and Performance of our approach compared with other methods. 3D conv,  denotes we replace our AIC module with a 3D convolution unit with 3D kernel . AIC-Net denotes a AIC-Net with one AIC module in feature fusion part, while by default we use two AIC modules.}
\vspace{-0.6cm}
\label{tab:params}
\end{center}
\end{table}
 \subsection{Ablation Study}
\vspace{-0.1cm}
In this section, we dive into the AIC-Net to investigate its key aspects in detail. Specifically, we try to answer the following questions. 1). Is it beneficial to use multiple candidate kernels along each dimension of the AIC module? 2). Is the performance improvement simply coming from multiple kernels? 3). Will that work if the AIC module is used as a plug-and-play module? 4). The trade-off between SSC performance and cost.  






\vspace{-0.3cm}
\paragraph{The effectiveness of using multiple kernels}
In our AIC module, we use multiple candidate kernels in each dimension , and use the learned modulation factors to choose proper kernels along each of these dimensions. 
Since we expect our AIC-Net to be able to deal with objects of varying shapes, the kernels in AIC should be sufficiently distinct.
In our experiments, we set the kernel set to be  across all three dimensions. 
The first question needs to be clarified is that will it be enough to use only the maximum kernel, \ie 7 in our network? 
Then, are three kernels better than two? 
From the results of Table~\ref{tab:kernels}, we can see, either two kernels  or three kernels  can outperform kernel . Since the maximum receptive field for all these three options is , the results demonstrate the benefits of using multiple kernels. At the same time, three kernels outperform two kernels by about  because it renders more flexibility in modeling the context.

\vspace{-0.3cm}
\paragraph{Is it necessary to use modulation factors?}
In the above paragraph, we show the benefit of using multiple kernels along each dimension. However, another question arises that is the improvement simply coming from multiple kernels? In other words, is that necessary to learn modulation factors to adaptively select the kernels voxel-wisely? From Table~\ref{tab:ModulationFactor}, we can see when we discard the modulation factors in AIC modules, the performance of AIC-Net observes obvious degradation on both NYU and NYUCAD datasets. These results show that the superior performance of AIC-Net relies on modeling the dimensional anisotropy property by adaptively selecting proper kernels along each dimension.
To further inspect the anisotropic nature of the learned kernels, we observed the statistical values of the modulation factors and found that: 
1.)~the selected kernel sizes are basically consistent with the object sizes; 
2.)~the modulation values for different voxels vary a lot within one scene; 
3.)~the modulation values among the three separable dimensions have significant variation. This indicates the learned ``3D receptive field'' are anisotropic and adaptive. 


\vspace{-0.3cm}
\paragraph{AIC module used as a plug-and-play module}
Due to its ability to model the anisotropic context, our AIC module is expected to be able to benefit other networks when it is used as a plug-and-play module. To validate this, we choose the DDRNet~\cite{li2019rgbd} as the test-bed, and use the AIC module to replace its building blocks, DDR and ASPP. DDR block models 3D convolution in a lightweight manner with the fixed receptive field. ASPP is a feature fusion scheme commonly used in semantic segmentation to take advantage of the multi-scale context.
Table~\ref{tab:AIC} shows the comparison. 
When we use AIC to replace the DDR module in DDRNet~\cite{li2019rgbd}, the SSC-IoU is improved by . When we replace ASPP by our AIC module, we still observe a  improvement in semantic segmentation. Finally, when we replace both DDR and ASPP by AIC, the result can be further boosted. 



\vspace{-0.3cm}
\paragraph{Trade-off in performance and cost}
Since we decompose the 3D convolution into three consecutive 1D convolutions, the model parameters and computation grow linearly with the number of candidate kernels in each dimension. While for standard 3D convolution, the parameters and computation will have cubic growth. Table~\ref{tab:params} presents some comparisons in terms of both efficiency and accuracy. For the \emph{3D conv, } in the table, it means we use this particular 3D convolution to replace our AIC module. As can be seen, when the 3D kernel size is , it will result in 3 times of parameters and FLOPs comparing to our AIC-Net. When the kernel size is increased to , the parameter and computation scale will be  times more than ours. DDRNet is a lightweight structure, which consumes the least parameters and has the lowest computation complexity, but it observes a glaring performance gap comparing to our method. Thus, our AIC-Net achieves a better trade-off between performance and cost. 












































%
