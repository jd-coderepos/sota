

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment} 
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

\usepackage{url}
\usepackage{graphicx,wrapfig}
\usepackage{booktabs,multirow}
\usepackage{colortbl}
\definecolor{hui}{gray}{0.3}
\definecolor{ggray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{ggray}}c}
\usepackage[ruled]{algorithm2e}
\usepackage[pagebackref=true,colorlinks,bookmarks=false]{hyperref}

\begin{document}
\pagestyle{headings}
	\mainmatter
	\def\ECCVSubNumber{1294}  \title{A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation\thanks{J. Feng was partially supported by NUS ECRA FY17 P08, AISG-100E2019-035, and MOE Tier 2 MOE2017-T2-2-151. The authors also thank Quanhong Fu for her help to improve the technical writing aspect of this paper.}}
	
\begin{comment}
    \titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
    \authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
    \author{Anonymous ECCV submission}
    \institute{Paper ID \ECCVSubNumber}
	\end{comment}


\titlerunning{A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation}
\author{Jian Liang\inst{1}\orcidID{0000-0003-3890-1894} \and Yunbo Wang\inst{2}\orcidID{0000-0002-6215-8888} \and Dapeng Hu\inst{1} \and Ran He\inst{3}\orcidID{0000-0002-3807-991X} \and Jiashi Feng\inst{1}\orcidID{0000-0001-6843-0064}}
\authorrunning{Liang et al.}
\institute{Department of ECE, National University of Singapore (NUS)\\
	\email{liangjian92@gmail.com},
	\quad\email{dapeng.hu@u.nus.edu},
	\quad\email{elefjia@nus.edu.sg}\\
	\and
	Peking University
	\quad\email{wangyunbo09@gmail.com}
	\and
	Institute of Automation, Chinese Academy of Sciences
	\quad\email{rhe@nlpr.ia.ac.cn}
	}
\maketitle

\begin{abstract}
This work addresses the unsupervised domain adaptation problem, especially in the case of class labels in the target domain being only a subset of those in the source domain. 
Such a partial transfer setting is realistic but challenging and existing methods always suffer from two key problems, negative transfer and uncertainty propagation. 
In this paper, we build on domain adversarial learning and propose a novel domain adaptation method BAUS with two new techniques termed Balanced Adversarial Alignment (BAA) and Adaptive Uncertainty Suppression (AUS), respectively.
On one hand, negative transfer results in misclassification of target samples to the classes only present in the source domain.
To address this issue, BAA pursues the balance between label distributions across domains in a fairly simple manner. 
Specifically, it randomly leverages a few source samples to augment the smaller target domain during domain alignment so that classes in different domains are symmetric. 
On the other hand, a source sample would be denoted as uncertain if there is an incorrect class that has a relatively high prediction score, and
such uncertainty easily propagates to unlabeled target data around it during alignment, which severely deteriorates adaptation performance. 
Thus we present AUS that emphasizes uncertain samples and exploits an adaptive weighted complement entropy objective to encourage incorrect classes to have uniform and low prediction scores.
Experimental results on multiple benchmarks demonstrate our BAUS surpasses state-of-the-arts for partial domain adaptation tasks. 
Code is available at \url{https://github.com/tim-learn/BA3US}.
\keywords{Partial Transfer Learning; Domain Adaptation; Adversarial Alignment; Uncertainty Propagation; Object Recognition.}
\end{abstract}


\section{Introduction}
Over the past two decades, many research efforts have been devoted to unsupervised domain adaptation (UDA), which aims to leverage labeled source domain data to learn to classify unlabeled target domain data.
Typically, existing UDA methods minimize the discrepancy between two domains by matching their statistical distribution moments \cite{tzeng2014deep,sun2016deep,zellinger2016central,koniusz2017domain} or by domain adversarial learning~\cite{tzeng2015simultaneous,ganin2016domain,tzeng2017adversarial,long2018conditional}. 
Once the domain shift is mitigated, source classifiers can be easily transferred to the target domain even with no labeled target domain data available. 
However, both UDA strategies always assume that different domains share the same label space. 
Such an assumption may not hold in practice and target domain labels may be only a subset of source domain labels.
This introduces an unsupervised partial domain adaptation (PDA) problem that receives increasing research attention recently \cite{cao2018partialb,zhang2018importance,cao2018partial,matsuura2018twins}. 


The PDA problem is challenging since source-only classes may occur in the target domain during distribution alignment, which is well-known as class mismatch that potentially causes \textit{negative transfer}. 
Several previous PDA approaches \cite{cao2018partialb,cao2018partial} mitigate negative transfer by jointly filtering out source-only classes and promote positive transfer by matching the data distribution in the shared classes.
Samples from source-only classes are expected to have lower weights in the adaptation module such that the marginal distributions of two domains can be aligned well.
However, it is rather risky to rule out the source-only classes, especially when the estimation of label distribution in the target domain is inaccurate.


To match two non-identical label spaces, we view the PDA problem from a new perspective and propose to augment the target label space to be the same as the source label space.
Specifically, we develop a simple balanced alignment solution, termed Balanced Adversarial Alignment (BAA), that borrows fewer and fewer samples from the source domain to the target domain within an iterative adversarial learning framework.
We expect that the augmented target domain looks much more similar to the source domain w.r.t. the label distribution, and the challenging PDA problem can be transformed to a well-studied UDA task.
To focus on the originally shared classes, we propose to filter out the source-only classes via a class-level weighting scheme meanwhile, making the large UDA task more compact.


Besides, existing domain adaptation methods always employ a conventional cross-entropy loss to merely promote the prediction score of ground-truth classes but neglect to suppress those of incorrect classes, which may result in a new problem termed \textit{uncertainty propagation}.
Intuitively, if incorrect classes have relatively high prediction scores for source data, some wrong classes would possibly have the largest prediction scores for the aligned target data around them.
This problem is quite critical but has been always ignored in the domain adaptation field.	
To circumvent the issue, we develop an uncertainty suppression solution termed Adaptive Uncertainty Suppression (AUS) that exploits complement entropy \cite{chen2019complement} in the labeled source domain to prohibit possibly high prediction scores from incorrect classes.
Specifically, we emphasize more the uncertain samples corresponding to smaller cross-entropy loss (confidence) and propose a confidence-weighted complement entropy objective in addition to the primary cross-entropy objective.



Generally, our baseline is built on the seminal domain adversarial networks \cite{ganin2015unsupervised,ganin2016domain} and exploits conditional entropy minimization and an entropy-aware weight strategy~\cite{long2018conditional}.
In this paper, we equip the baseline model with two proposed techniques mentioned above and finally formulate a unified framework BAUS which well addresses the negative transfer and uncertainty propagation problems in partial domain adaptation.
We also empirically discover that the uncertainty suppression technique works well for vanilla closed-set domain adaptation.


To sum up, we make the following contributions. To our best knowledge, this is the first work that tackles partial domain adaptation by augmenting the target domain and transforming it into a UDA-like problem. 
The proposed balanced augmentation technique is fairly simple and works very well for PDA tasks.
Besides, we address an overlooked issue in this field named uncertainty propagation by designing an adaptive weighted complement entropy for the source domain.
Extensive results demonstrate that our approach yields new state-of-the-art results on several visual benchmark datasets, including Office31~\cite{saenko2010adapting}, Office-Home~\cite{venkateswara2017deep}, and ImageNet-Caltech~\cite{cao2019learning}.


\section{Related Work}
The past two decades have witnessed remarkable progress in domain adaptation. Interested readers can refer to \cite{csurka2017domain,zhang2019transfer,kouw2019review} for taxonomy and survey. 


\textbf{Unsupervised Domain Adaptation (UDA).} 
Compared with its supervised counterpart, UDA is more practical and challenging since no labeled data in the target domain are available. 
Recently, deep convolutional neural networks have achieved great success for visual recognition tasks, and we focus on deep UDA methods in this work. They can be categorized into three main groups.
The first group aims to minimize the domain discrepancy by matching different statistic moments like maximum mean discrepancy (MMD) \cite{tzeng2014deep,long2015learning,long2017deep,liang2019aggregating} and higher-order moment matching \cite{sun2016deep,zellinger2016central,koniusz2017domain}.
The second group that is widely used introduces a domain discriminator and exploits the idea of adversarial learning \cite{goodfellow2014generative} to encourage domain confusion so that the discriminator can not decide which domain the data come from. 
Some typical examples are \cite{tzeng2015simultaneous,ganin2016domain,tzeng2017adversarial,bousmalis2017unsupervised}.
A third group is a reconstruction-based approach, assuming the reconstruction of both source and target domain samples to be important and helpful. Among them,~\cite{ghifary2016deep,zhu2017unpaired} utilize encoder-decoder reconstruction and adversarial reconstruction, respectively. 
Despite their success for vanilla UDA, they are easily stuck by negative transfer for PDA due to the mismatched marginal label distributions. 


\textbf{Partial Domain Adaptation (PDA).}
In reality, PDA can be considered as a special case of imbalanced domain adaptation, where the target label distribution is quite dissimilar to that of the source domain.
Until recent years,~\cite{ming2015unsupervised} first introduces an imbalanced scenario where label numbers of the source and target domains are not the same, which draws the attention of many researchers~\cite{tsai2016domain,cao2018partialb,cao2018partial,zhang2018importance,matsuura2018twins,cao2019learning,hu2019multi}.
Different from shallow methods \cite{ming2015unsupervised,tsai2016domain}, deep methods \cite{cao2018partialb,cao2018partial,zhang2018importance,matsuura2018twins} are mainly based on the domain adversarial learning framework and achieve promising recognition accuracy.
Selective adversarial network (SAN)~\cite{cao2018partialb} exploits a multi-discriminator domain adversarial network and tries to select source-only classes by imposing different localized weights on different discriminators.
Importance weighted adversarial nets (IWAN)~\cite{zhang2018importance} apply only one domain discriminator and weigh each source sample with the probability of being a target sample.
Partial adversarial domain adaptation (PADA)~\cite{cao2018partial} estimates the target label distribution and then feeds the class-wise weights to both the source discriminator and the domain discriminator, while two weighted inconsistency-reduced networks (TWINs)~\cite{matsuura2018twins} leverage two independent networks to estimate the target label distribution and minimize the domain difference measured by the classifiers' inconsistency on the target samples. 
Deep Residual Correction Network (DRCN) \cite{li2020deep} proposes a weighted class-wise matching strategy to explicitly align target data with the most relevant source subclasses.
Recently, Example Transfer Network (ETN)~\cite{cao2019learning} jointly learns domain-invariant representations across domains and a progressive weighting scheme to quantify the transferability of source examples, which achieves state-of-the-art results on several benchmark datasets.
Generally, all the PDA methods above attempt to filter out the large source domain to match the small target domain. Comparatively, our method tries to augment the small target domain to match the source domain from a different perspective.



\textbf{Data Synthesis and Augmentation.}
Recently, synthesis and augmentation techniques like CycleGAN \cite{zhu2017unpaired} and \emph{mixup} \cite{zhang2018mixup} are favored by UDA and semi-supervised learning methods for improving performance.
For example, \cite{hoffman2018cycada} directly exploits CycleGAN to generate target-like images from source samples to narrow the domain shift for adaptive semantic segmentation.
\cite{mao2019virtual,wang2019semi} extend \emph{mixup} to domain adaptation and generate pseudo training samples via interpolating between certain source samples and uncertain target samples.
To some degree, target augmentation in this paper is like a special case of \emph{mixup} where the mixup coefficient is always binary. 
However, the motivations are totally different. Our method considers neither the interpolated semantic label nor the interpolated domain label for a classification loss.

	
\section{Proposed Method}
We elaborate on the proposed framework for partial domain adaptation (PDA) in this section.
First, we give definitions and notations. 
We follow the protocol of unsupervised PDA where we have a labeled source domain dataset  and an unlabeled target domain dataset  during the training stage.
These two domains have different feature distributions:  due to the domain shift.
Notably, different from vanilla UDA, the target labels are a subset of the source labels for PDA: , and  denotes the total number of classes in .



We aim to learn a deep neural network  that consists of two components: . Here  denotes the feature extractor and  denotes a class predictor.
Since we target at learning domain-invariant features, the prediction function is assumed identical, i.e., .
For simplicity, we also share the feature extractor  for different domains.
We introduce an adversarial classifier  to mitigate the distribution discrepancy across domains as explained later.


\subsection{Domain Adversarial Learning Revisited}
Generative adversarial network (GAN) \cite{goodfellow2014generative} learns two competing components: the discriminator  and the generator  which play a minimax two-player game, where  tries to fool  by generating examples that are as realistic as possible and  tries to classify the generated samples as fake.
Inspired by the idea of GAN, domain adversarial neural networks (DANN) \cite{ganin2015unsupervised,ganin2016domain} develops a two-player game for UDA, where one player  (i.e., domain discriminator) tries to distinguish the source domain datum from that of the target domain and the other player  (i.e., feature extractor) is trained to confuse the domain discriminator .
Generally, the minimax game of DANN is formulated as

where  represents the softmax cross-entropy loss,  denotes the source classifier,  represents the domain-shared feature extractor, and  is a hyper-parameter to trade-off the source risk and domain adversary.
Different from a label flipping step in GAN, a gradient reversal layer (GRL) is further defined in \cite{ganin2015unsupervised} to optimize the objective in Eq.~(\ref{dann}). 
Due to its simplicity and effectiveness, the idea of domain adversarial learning has been adopted in many previous domain adaptation works \cite{liu2016coupled,tzeng2017adversarial,long2018conditional}.
	
	
As shown in Eq.~(\ref{dann}), each sample from both source and target domains is equally involved in the adversarial loss , which seems not reasonable.
If we only have samples distributed in the margin of the classifier (called `hard' samples) from different domains and pursue domain alignment via them, it may perform badly for those `easy' samples across domains.
As such, we expect those `hard' samples and `easy' samples to own lower and higher weights during domain adversarial alignment.
Specifically, we quantify the difficulty via the entropy criterion  and adopt the same weighting strategy as \cite{long2018conditional} to impose an entropy-aware weight  on each sample,



Obviously, if there is no domain shift, UDA (including PDA) degenerates to a typical semi-supervised learning problem.
On one hand, we aim to mitigate the domain shift; on the other hand, this motivates us to adopt the popular entropy minimization principle \cite{grandvalet2005semi} in semi-supervised learning to the PDA task.
It is desirable that all the unlabeled target samples have highly-confident predictions. This is encouraged via the following conditional entropy term,

Inspired by the observation in \cite{cao2018partialb} that redundant information is not beneficial for adaptation, we adopt the following optimization objective of Entropy-regularized DANN (\textbf{E-DANN}) as an initial model of our method,

where ,  denotes the normalized estimated class-level weight vector via the target domain, and  are two empirical trade-off parameters.
	
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth, trim=10 20 0 35,clip]{./src/framework.pdf}
	\caption{Architecture of our domain adaptation method. There are three modules: a shared feature extractor , a classifier  and a domain discriminator . Different from domain adversarial learning \cite{ganin2015unsupervised}, it contains two extra components with marked red border, i.e., balanced augmentation and weighted complement entropy.}
	\label{fig:framework}
\end{figure*}
	
\subsection{Balanced Adversarial Alignment (BAA)}
\label{sec:baa}
The joint distribution shift is the actual root to negative transfer \cite{wang2019characterizing}. 
For example, the marginal label distributions are not symmetric in PDA, and thus source-only classes are prone to be matched with target classes, resulting in a negative transfer problem. 
Generally, a class-level source re-weighting scheme sounds a natural choice for PDA, since it is expected to filter out source-only classes and promote positive transfer between the shared classes across domains.
Previous methods \cite{cao2018partial,matsuura2018twins} resort to target predictions to generate class-level weights and effectively avoid negative transfer to some degree.
Ideally, PDA with a weighting scheme behaves like a small UDA problem, but it heavily relies on accurate target predictions to calculate suitable weights.
	

In contrast, we propose an extremely simple scheme as shown in Fig.~\ref{fig:framework} for the challenging distribution alignment in PDA.
Intuitively, we pursue the balance between different label distributions across domains with quite an opposite solution, i.e., augmenting the target domain using original source samples instead of weighting the source domain.
This idea looks weird but is actually reasonable because we readily turn the PDA task into a large UDA-like task and the negative transfer effects caused by source-only classes can be well alleviated. 
The detailed formulation of balanced augmentation (alignment) is shown below,

Specifically, we adopt a progressive strategy for the target augmentation scheme. We borrow the source samples with different ratios, and the ratio  gradually decreases to 0 as the number of iterations increases.
This is because the learned feature representations in early iterations are not quite transferable and we need more source samples to avoid class mismatch.
When the learned features are desirably discriminative and transferable, the estimation of class-level weights becomes more accurate, making the augmentation trivial.
	
	
Note that we borrow original samples from the source domain rather than exploiting a generative model like CycleGAN~\cite{zhu2017unpaired} to synthesize target-like source images. 
The reason is that obtaining data-dependent translation models between such heterogeneous domains is quite time-consuming and we find translation does not even improve the adaptation results.


\subsection{Adaptive Uncertainty Suppression (AUS)}
	\label{sec:aus}
	\begin{wrapfigure}{r}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth, trim=180 280 95 80,clip]{./src/WCE.pdf}
		\caption{Mitigating the effects of uncertainty propagation from source. [\textcolor{blue}{blue}: source, \textcolor{red}{red}: target, \textcolor{hui}{gray}: adversarial alignment.]}
		\label{fig:wce}
	\end{wrapfigure}
Previous DA methods focus on strengthening the feature transferability by developing various domain alignment strategies, but they mostly ignore the feature discriminability and simply use the conventional cross-entropy loss in the labeled source domain to learn the features.
In that case, even though the domain shift is mitigated, the classifier may perform worse on target data.
This is because source classes are not equally separated from each other, which may lead to the propagation of the confusion (uncertainty) to target predictions and thus confusing features in the target domain, which is termed as the uncertainty propagation problem.
Take a 3-way classification problem as an example. 
There is no class overlap in the source domain, but class 1 and 2 may be close to each other.
A toy example of how class 1 and class 2 behave is shown in Fig.~\ref{fig:wce} where a few samples lie close to the decision boundary between class 1 and class 2.
During domain alignment, some unlabeled target data are enforced to match these source data, which would be easily misclassified. 
However, such a critical problem has always been overlooked in prior literature. 
Looking back at the cross-entropy loss , it only exploits the information from the ground-truth class while ignoring that from other incorrect classes.
For example, the source output \textbf{[0.6, 0.3, 0.1]} is more uncertain than \textbf{[0.6, 0.2, 0.2]}, but both have the same cross-entropy loss.


Though several previous methods \cite{kumar2018co,shu2018dirt} incorporate virtual adversarial training \cite{miyato2018virtual} in the classification term to implicitly increase the margin between different classes, more computation complexity is required and several parameters need to be tuned.
Inspired by \cite{chen2019complement}, we exploit a complement entropy that expects uniform and low prediction scores for incorrect classes for labeled source samples.
To accurately suppress uncertainty, we further place more emphasis on the uncertain samples that own smaller cross-entropy loss (confidence) and propose a confidence-weighted complement entropy objective below,

where  is a hyper-parameter and  is the index of ground-truth class~in~.
Different from the complementary training strategy in \cite{chen2019complement}, we exploit the adaptive weighted complement entropy  as a regularizer, which is more efficient.
We also assign a class-level weight for each sample in Eq.~(\ref{eq:src_coe}) like that in .
	

\subsection{Unified Minimax Optimization Problem of BAUS}
Finally, we integrate all the terms in Eqs.~(\ref{edann}, \ref{aug}, \ref{eq:src_coe}) on both source and target samples to avoid negative transfer and uncertainty prorogation and derive a unified framework for PDA. The overall min-max objective is formulated as

where  is another trade-off hyper-parameter to balance the complement entropy and the cross-entropy term.
Again,  is the normalized estimated class-level weight vector.
To optimize the objective above, we follow~\cite{ganin2015unsupervised} to introduce a gradient reversal layer and adopt the same progressive strategy for parameter , i.e., increasing  from 0 to 1 as the number of iterations grows.


Our method is closely related to DANN~\cite{ganin2015unsupervised}, sharing similar formalism of the domain adaptation theory \cite{ben2010theory} that the expected target risk  on the target examples is bounded by the source risk  and other two terms below, 
	
where  is the ideal joint hypothesis for the combined risk.
DANN further discovers the second term -distance can be upper bounded by error of the domain adversarial classifier.
As we do not have labels of the target domain, we expect the entropy minimization term on the target domain to help reduce the last term.
Besides, the proposed complement entropy objective alleviates uncertainty propagation to make the weight estimation more accurate, and thus our method would turn PDA into a small UDA task.
In this way, we can expect that our method minimizes the empirical target risk .
The detailed optimization is summarized in Algorithm \ref{alg}.


\section{Experiments}
\subsection{Setup}
\textbf{Datasets.} \textbf{Office31} dataset \cite{saenko2010adapting} includes images of 31 object classes from three different domains, i.e., Amazon, DSLR, and Webcam. We follow the standard protocol used in \cite{cao2018partial} and pick up images of 10 categories shared by Office31 and Caltech256~\cite{griffin2007caltech} as target domains. \textbf{Office-Home} dataset  \cite{venkateswara2017deep}  consists of 4 different domains with each containing 65 kinds of everyday objects, i.e., Artistic, Clipart, Product images, and Real World images. Likewise, we follow \cite{cao2018partial} to select the first 25 categories (in alphabetic order) in each domain as a partial target domain. 
\textbf{ImageNet-Caltech} is a large-scale object recognition dataset that consists of two subsets, ImageNet-1K~\cite{russakovsky2015imagenet} and Caltech256~\cite{griffin2007caltech}.
Here we use images from the public validation set of ImageNet-1K for the target domain. 
In reality, each source domain contains 1,000 and 256 classes, and each target domain contains 84 classes.
	
\textbf{Baseline methods.} We utilize all the source and target samples and report the average classification accuracy and standard deviation over 3 random trials. \textbf{A}  \textbf{B} means \textbf{A} is the source domain and \textbf{B} is the partial target domain.
For comprehensive comparison, we provide the recognition results of our methods including E-DANN, Ours (w/ BAA) and Ours (BAUS) on each dataset, and compare them with some popular UDA methods \cite{tzeng2017adversarial,long2018conditional} and existing PDA methods, including SAN~\cite{cao2018partialb}, IWAN~\cite{zhang2018importance}, PADA~\cite{cao2018partial}, SSPDA~\cite{bucci2019tackling}, MWPDA~\cite{hu2019multi}, DRCN~\cite{li2020deep}, ETN~\cite{cao2019learning}, and SAFN~\cite{xu2019unsupervised}.
	
	
\textbf{Implementation details.} If not specified, all the methods adopt \textbf{ResNet-50}~\cite{he2016deep} as backbone.
We fine-tune the pre-trained ImageNet model in \textbf{PyTorch} using a {NVIDIA Titan X} (12 GB memory). 
The adversarial layer and classifier layer are trained through back-propagation, and the learning rate of the classifier layer is 10 times that of lower layers.
We adopt mini-batch SGD with momentum of 0.9 and the learning rate annealing strategy as \cite{ganin2016domain,long2018conditional}: the learning rate is adjusted by , where  denotes the training progress changing from 0 to 1, and  = 0.01,  = 10,  0.75 as suggested by \cite{long2018conditional}. 
Based on the number of classes and balancing analysis in \cite{chen2019complement}, we use  for Office31 and ImageNet-Caltech, and  for Office-Home.
Besides, we set the number of intervals  to 10, and  is set to 200, 500, and 4,000 for Office31, Office-Home, and ImageNet-Caltech, respectively.
Note that our method does not use the ten-crop technique \cite{cao2018partial,long2018conditional} at the evaluation phase for better performance.

	\setlength{\tabcolsep}{1.0pt}
	\begin{table*}[htbp]
		\centering
		\scriptsize
		\caption{Accuracy (\%) on \textbf{Office-Home} dataset for \emph{partial domain adaptation} via ResNet-50~\cite{he2016deep}. The best in \textbf{\color{red}{bold red}}; the second best in \textit{\color{blue}{italic blue}}.}
		\resizebox{1.0\textwidth}{!}{\to\to\to\to\to\to\to\to\to\to\to\to\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm^3\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm}
		\label{tab:oc}\end{table*}

\subsection{Quantitative Results for Partial Domain Adaptation}
The results on three object recognition datasets including \textbf{Office-Home}, \textbf{Office31} and \textbf{ImageNet-Caltech} for PDA are shown in Table~\ref{tab:oc} and \ref{tab:office}, with some baseline results directly reported from ETN~\cite{cao2019learning} with the same protocol.
Obviously, BAUS achieves the best or second-best results on 10 out of 12 transfer tasks on the \textbf{Office-Home} dataset, and Ours (w/ BAA) and SAFN obtain the second and third best results, respectively.
Regarding the average accuracy, BAUS advances the state-of-the-art result on \textbf{Office-Home} in SAFN~\cite{xu2019unsupervised} by 5.78\%, from 71.83\% to 75.98\%.
For two specific tasks \emph{Cl}\emph{Ar} and \emph{Pr}\emph{Rw}, BAUS merely performs slightly worse than the best method.
Besides, a state-of-the-art UDA approach CDAN+E~\cite{long2018conditional} performs worse than ResNet-50, which implies the difficulty of the partial transfer task.
Further compared with UDA methods, even PDA methods like IWAN~\cite{zhang2018importance} and PADA~\cite{cao2018partial} do not work well, which again indicates the partial transfer is quite challenging.


On the small-scale \textbf{Office31} dataset, BAUS again obtains the best average accuracy and performs the best in 3 out of 6 transfer tasks. 
For transfer tasks from a large source domain \emph{A} to small target domains (\emph{D}, \emph{W}), BAUS remarkably outperforms other PDA methods.
BAUS performs slightly worse for the \emph{D}  \emph{A} task because the target domain \emph{D} is very small, making the proposed target augmentation and adaptive uncertainty suppression techniques inefficient. 
On the large-scale \textbf{ImageNet-Caltech} dataset, BAUS performs the best for both transfer tasks and still holds the best average accuracy with significant improvements.
Moreover, UDA methods do not always perform better than ResNet-50, which implies they may suffer from the negative transfer problem. 
	
	
Besides the ResNet-50 backbone network, we further investigate the effectiveness of BAUS with another backbone network VGG-16~\cite{simonyan2014very} on \textbf{Office31} and compare it with state-of-the-art methods in Table~\ref{tab:vgg}.
It can be clearly observed that BAUS achieves the best or second-best results for all the tasks, significantly advancing the average accuracy from 93.88\% to 95.84\%.
Compared with the results in Table~\ref{tab:office}, we find BAUS (97.81\%95.84\%) is also robust than ETN (96.73\%93.88\%) w.r.t. the change of the backbone network.

	\setlength{\tabcolsep}{1.0pt}
	\begin{table*}[htbp]
		\centering
		\scriptsize
		\caption{Accuracy (\%) on \textbf{Office31} and \textbf{ImageNet-Caltech} for \emph{partial domain adaptation} via ResNet-50~\cite{he2016deep}. The best in \textbf{\color{red}{bold red}}; the second best in \textit{\color{blue}{italic blue}}.}
		\resizebox{1.0\textwidth}{!}{\to\to\to\to\to\to\to\to_{\pm1.12}_{\pm1.09}_{\pm0.95}_{\pm0.85}_{\pm0.86}_{\pm0.74}_{\pm0.78}_{\pm0.74}_{\pm0.17}_{\pm0.17}_{\pm0.14}_{\pm0.23}_{\pm0.13}_{\pm0.12}_{\pm0.45}_{\pm0.41}_{\pm0.90}_{\pm1.20}_{\pm0.07}_{\pm0.00}_{\pm0.00}_{\pm0.00}_{\pm0.07}_{\pm0.13}_{\pm0.36}_{\pm0.37}_{\pm0.29}_{\pm0.32}_{\pm0.25}_{\pm0.24}_{\pm0.40}_{\pm0.46}_{\pm0.28}_{\pm0.45}_{\pm0.36}_{\pm0.52}_{\pm0.44}_{\pm0.12}_{\pm0.36}_{\pm0.42}_{\pm0.37}_{\pm0.31}_{\pm0.29}_{\pm0.45}_{\pm0.33}_{\pm0.00}_{\pm0.36}_{\pm0.44}_{\pm0.22}_{\pm0.20}_{\pm0.27}_{\pm0.00}_{\pm0.24}_{\pm0.00}_{\pm0.24}_{\pm0.44}_{\pm0.00}_{\pm0.00}_{\pm0.05}_{\pm0.00}_{\pm0.05}_{\pm0.00}_{\pm0.81}_{\pm0.25}_{\pm0.00}_{\pm0.00}_{\pm0.09}_{\pm0.00}_{\pm0.00}_{\pm0.00}_{\pm0.49}_{\pm0.08}^3_{\pm0.00}_{\pm0.28}_{\pm0.05}_{\pm0.00}_{\pm0.08}_{\pm0.00}_{\pm0.15}_{\pm0.28}}
		\label{tab:office}\end{table*}

	\setlength{\tabcolsep}{3.0pt}
	\begin{table*}[htbp]
		\centering
		\scriptsize
		\caption{Accuracy (\%) on \textbf{Office31} for \emph{partial domain adaptation} via VGG-16~\cite{simonyan2014very}. The best in \textbf{\color{red}{bold red}}; the second best in \textit{\color{blue}{italic blue}}.}
		\resizebox{0.92\textwidth}{!}{\to\to\to\to\to\to\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm^3\pm\pm\pm\pm\pm\pm}
		\label{tab:vgg}
	\end{table*}

\textbf{Ablation study.}
As shown in Tables~\ref{tab:oc} and \ref{tab:office}, we provide the results of E-DANN and Ours (w/ BAA) along with BAUS on three datasets.
Ours (w/ BAA) extends E-DANN by using the proposed balanced adversarial alignment technique in Sec.~\ref{sec:baa} instead, while Ours (BAUS) extends Ours (w/ BAA) by considering the proposed adaptive complement entropy objective in Eq.~(\ref{eq:src_coe}).
Firstly, the baseline method E-DANN always performs well for partial domain adaptation since it removes the irrelevant classes in the source classification term like \cite{cao2018partial}.
Secondly, results on all three datasets demonstrate that Ours (BAUS) performs better than Ours (w/ BAA) and Ours (w/ BAA) performs better than E-DANN, which verify the effectiveness of two proposed techniques in Sec.~\ref{sec:baa} and Sec.~\ref{sec:aus}.
	
\subsection{Quantitative Results for Closed-set Domain Adaptation} 
This section further investigates the effectiveness of the proposed uncertainty suppression technique for vanilla closed-set domain adaptation.
Here we consider integrating them with DANN and CDAN~\cite{long2018conditional} respectively, and compare our methods with state-of-the-art UDA approaches including \cite{wang2019transferable,xu2019unsupervised} on the most-favored \textbf{Office-Home} dataset.
As shown in Table~\ref{tab:uda}, the proposed method BAUS built on CDAN obtains the best average accuracy and ranks the top two in 9 out of 12 different transfer tasks.
It is obvious that the adaptive uncertainty suppression technique works well for closed-set domain adaptation, advancing the average accuracy from 67.6\% to 68.7\% and from 68.0\% to 69.2\%.
In fact, we also study the effectiveness of balanced adversarial alignment but find it hardly improve the performance since the label distributions in UDA have already been symmetric.
	
	\setlength{\tabcolsep}{1.0pt}
	\begin{table*}[htbp]
		\centering
		\small
		\caption{Accuracy (\%) on \textbf{Office-Home} dataset for \emph{vanilla unsupervised domain adaptation} via ResNet-50~\cite{he2016deep}. Methods utilize augmentation during evaluation.}
		\resizebox{0.92\textwidth}{!}{\to\to\to\to\to\to\to\to\to\to\to\to^*^*^*^3^3}
		\label{tab:uda}\end{table*}

\subsection{Qualitative Results for Partial Domain Adaptation} 
We study our methods with different numbers of target classes in Fig.~\ref{fig:par}(a).
The performance decreases when the number is larger than 15, and BAUS always obtains the best results.
As expected, the proposed augmentation technique becomes important when the number of target classes is small.
	

\textbf{Weight visualization.}
As stated before, the weighting scheme plays an important role in PDA.  
Thus we investigate the quality of the estimated class-level weight  in Algorithm~\ref{alg}.
As shown in Fig.~\ref{fig:par}(b-c), we plot the estimated weights of BAUS for the two specific tasks.
Since the \emph{Cl} domain and \emph{Ar} domain are quite different, making the estimation challenging. This is also evidenced in other PDA methods in Table~\ref{tab:oc} since the accuracy is around 55\%. 
For the relatively easy task AD, the weight estimation seems accurate, resulting in high classification accuracy.
	
	\begin{figure}[h]
		\centering
		\scriptsize
		\renewcommand\arraystretch{1.0}
		\begin{tabular}{ccc}
			\includegraphics[width=0.32\linewidth, trim=65 200 75 235,clip]{src/results/k.pdf} & 
			\includegraphics[width=0.32\linewidth, trim=65 200 75 235,clip]{src/results/weight_ac.pdf} &
			\includegraphics[width=0.32\linewidth, trim=65 200 75 235,clip]{src/results/weight_ad.pdf}\\
			(a) Accuracy of ArCl & (b) estimated weight of ArCl & (c) estimated weight of AD
		\end{tabular}
		\caption{(a) Accuracy with varying number of target classes. (b-c) Estimated class-level weights. Yellow bins denote ground-truth classes. Best viewed in color.}
		\label{fig:par}
	\end{figure}
	
\textbf{Parameter Sensitivity.} We study the sensitivity of parameters  and  of BAUS on the \textbf{Office-Home} and \textbf{Office31} datasets.
The mean accuracy is reported in Table~\ref{tab:p1} and Table~\ref{tab:p2}, respectively.
Note that the complement entropy~\cite{chen2019complement} can be considered as a special case of the proposed adaptive one where .
The results indicate using an adaptive objective is much better, and the performance is relatively stable.
Regarding the parameter , the accuracy around 1.0 and 5.0 is also stable, implying our method is not sensitive.
	
	\setlength{\tabcolsep}{3.0pt}
	\begin{table}[h]
		\centering
		\begin{minipage}{0.48\linewidth}
			\centering
			\caption{Sensitivity of parameter .}
			\resizebox{0.95\textwidth}{!}{}
			\label{tab:p1}
		\end{minipage}
		\begin{minipage}{0.48\linewidth}
			\centering
			\caption{Sensitivity of parameter .}
			\resizebox{0.82\textwidth}{!}{}
			\label{tab:p2}
		\end{minipage}
	\end{table}
	
\textbf{Convergence performance.}
As shown in Fig.~\ref{fig:convergence}, we study the convergence performance of the proposed methods for ArCl and AD.
Obviously, the `source only' method works worse without the domain alignment module, and E-DANN performs much better than it and quickly converges after 1,000 iterations.
Besides, both BAUS and Ours (w/ BAA) obtain similar promising results, and BAUS performs slightly better since it further considers the adaptive complement entropy to diminish the uncertainty in source predictions.
	
	\begin{figure}[h]
		\centering
		\scriptsize
		\setlength\tabcolsep{0mm}
		\renewcommand\arraystretch{1.0}
		\begin{tabular}{cc}
			\includegraphics[width=0.4\linewidth, trim=65 200 75 235,clip]{src/results/ac.pdf} & 
			\includegraphics[width=0.4\linewidth, trim=65 200 75 235,clip]{src/results/ad.pdf}\\
			(a) Ar  Cl (Office-Home) & (b) A  D (Office31)
		\end{tabular}
		\caption{Convergence analysis of proposed methods on two different transfer tasks. Accuracy (\%) is given w.r.t. number of iterations. Best viewed in color.}
		\label{fig:convergence}
	\end{figure}
	
	\begin{figure}[!h]
		\centering
		\footnotesize
		\setlength\tabcolsep{0mm}
		\renewcommand\arraystretch{0.1}
		\begin{tabular}{cccc}
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ad0.pdf} &
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ad1.pdf} & 
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ad2.pdf} &
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ad3.pdf}\\
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ac0.pdf} &
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ac1.pdf} & 
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ac2.pdf} &
			\includegraphics[width=0.24\linewidth, trim=35 30 40 35,clip]{src/results/ac3.pdf}\\
			~\\
			(a) Source only & (b) E-DANN & (c) Ours (w/ BAA) & (d) Ours (BAUS) \\
		\end{tabular}
		\caption{t-SNE visualizations for two transfer tasks AD (upper row) and ArCl (bottom row). {\color{blue}Blue: source data}; {\color{red}red: target data}.}
		\label{fig:tsne}
	\end{figure}
	
\textbf{Feature visualization.}
We plot in Fig.~\ref{fig:tsne} the t-SNE embeddings \cite{maaten2008visualizing} of the features learned by `source only', E-DANN and BAUS for two different transfer tasks.
It is easy to discover that the features of target data are rather confusing in `no adaptation' while the balanced alignment module helps mitigate the domain gap and the adaptive uncertainty suppression module helps increase the discrimination of the features.
	
\section{Conclusion}
We develop a novel adversarial learning-based method BAUS for partial domain adaptation, which well addresses two key problems, negative transfer and uncertainty propagation.
To tackle the asymmetric label distributions, BAUS  offers a very simple solution by augmenting the target domain with samples from the source domain.
Then, it uncovers an overlooked issue in the field termed uncertainty propagation and designs an adaptive complement entropy objective to well suppress the uncertainty in source predictions.
Empirical results show it also works well for vanilla closed-set domain adaptation. 
Further experiments have validated that BAUS improves existing methods with substantial gains, establishing new state-of-the-art. 

\begin{algorithm}
	\footnotesize
	\renewcommand\baselinestretch{0.4}\selectfont
	\SetAlgoLined
	\textbf{Input}: Labeled source domain , unlabeled target domain \;
	\textbf{Parameters}: Total training iterations , updating interval , batch size , , , , , \; 
	Initialize the model parameters \;
	Initialize the class-level weight vector , \;
	\For{i = 1 to }{
		Obtain  samples from  and , respectively\;
		Obtain  random samples from \;Update  by optimizing Eq.~(\ref{ours}) and gradient reversaral layer\;
		\If{i \%  == 0}{
			update the class-level weight vector \; \textcolor{gray}{\% note that this step is ignored in closed-set domain adaptation}\\
			calcuate  in Eq.~(\ref{tar_ent}) for model selection\;
			   (1 - /)\;
		}
	}
	\textbf{Output}: Target outputs corresponding to the minimal value of .
	\caption{Pseudo code of our method termed BAUS.}
	\label{alg}
\end{algorithm}



\bibliographystyle{splncs04}
\bibliography{egbib}
	
\end{document}
