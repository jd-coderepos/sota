

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  

\IEEEoverridecommandlockouts                              \overrideIEEEmargins



\newcommand{\E}{\textrm{E}}
\newcommand{\V}{\textrm{Var}}


\newcommand{\dt}{\textnormal{d}t}
\newcommand{\dtau}{\textnormal{d}\tau}
\newcommand{\deta}{\textnormal{d}\eta}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\Upr}{{\mathop{\mathrm{Upr}}}}
\newcommand{\Sgn}{{\mathop{\mathrm{Sgn}}}}
\newcommand{\h}{\mathop{\mathrm{H}}}
\newcommand{\prox}{{\mathop{\mathrm{prox}}}}
\newcommand{\abs}{\mathop{\mathrm{abs}}}
\newcommand{\T}{^{\mathop{\mathrm{T}}}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\diag}{{\mathop{\mathrm{diag}}}}


\newcommand{\val}{\mbox{\boldmath }}
\newcommand{\vbe}{\mbox{\boldmath }}
\newcommand{\vga}{\mbox{\boldmath }}
\newcommand{\vde}{\mbox{\boldmath }}
\newcommand{\vep}{\mbox{\boldmath }}
\newcommand{\vze}{\mbox{\boldmath }}
\newcommand{\vet}{\mbox{\boldmath }}
\newcommand{\vth}{\mbox{\boldmath }}
\newcommand{\vio}{\mbox{\boldmath }}
\newcommand{\vka}{\mbox{\boldmath }}
\newcommand{\vla}{\mbox{\boldmath }}
\newcommand{\vmu}{\mbox{\boldmath }}
\newcommand{\vnu}{\mbox{\boldmath }}
\newcommand{\vxi}{\mbox{\boldmath }}
\newcommand{\vpi}{\mbox{\boldmath }}
\newcommand{\vrh}{\mbox{\boldmath }}
\newcommand{\vsi}{\mbox{\boldmath }}
\newcommand{\vta}{\mbox{\boldmath }}
\newcommand{\vup}{\mbox{\boldmath }}
\newcommand{\vph}{\mbox{\boldmath }}
\newcommand{\vch}{\mbox{\boldmath }}
\newcommand{\vps}{\mbox{\boldmath }}
\newcommand{\vom}{\mbox{\boldmath }}

\newcommand{\vvep}{\mbox{\boldmath }}
\newcommand{\vvth}{\mbox{\boldmath }}
\newcommand{\vvrh}{\mbox{\boldmath }}
\newcommand{\vvpi}{\mbox{\boldmath }}
\newcommand{\vvsi}{\mbox{\boldmath }}
\newcommand{\vvph}{\mbox{\boldmath }}

\newcommand{\vGa}{\mathbf \Gamma}
\newcommand{\vDe}{\mathbf \Delta}
\newcommand{\vTh}{\mathbf \Theta}
\newcommand{\vLa}{\mathbf \Lambda}
\newcommand{\vXi}{\mathbf \Xi}
\newcommand{\vPi}{\mathbf \Pi}
\newcommand{\vSi}{\mathbf \Sigma}
\newcommand{\vUp}{\mathbf \Upsilon}
\newcommand{\vPh}{\mathbf \Phi}
\newcommand{\vPs}{\mathbf \Psi}
\newcommand{\vOm}{\mathbf \Omega}
\newcommand{\vUpsilon}{\mathbf \Upsilon}


\newcommand{\iGa}{\varGamma}
\newcommand{\iDe}{\varDelta}
\newcommand{\iTh}{\varTheta}
\newcommand{\iLa}{\varLambda}
\newcommand{\iXi}{\varXi}
\newcommand{\iPi}{\varPi}
\newcommand{\iSi}{\varSigma}
\newcommand{\iUp}{\varUpsilon}
\newcommand{\iPh}{\varPhi}
\newcommand{\iPs}{\varPsi}
\newcommand{\iOm}{\varOmega}

\newcommand{\va}{\mathbf a}
\newcommand{\vb}{\mathbf b}
\newcommand{\vc}{\mathbf c}
\newcommand{\vd}{\mathbf d}
\newcommand{\ve}{\mathbf e}
\newcommand{\vf}{\mathbf f}
\newcommand{\vg}{\mathbf g}
\newcommand{\vh}{\mathbf h}
\newcommand{\vi}{\mathbf i}
\newcommand{\vj}{\mathbf j}
\newcommand{\vk}{\mathbf k}
\newcommand{\vl}{\mathbf l}
\newcommand{\vm}{\mathbf m}
\newcommand{\vn}{\mathbf n}
\newcommand{\vo}{\mathbf o}
\newcommand{\vp}{\mathbf p}
\newcommand{\vq}{\mathbf q}
\newcommand{\vr}{\mathbf r}
\newcommand{\vs}{\mathbf s}
\newcommand{\vt}{\mathbf t}
\newcommand{\vu}{\mathbf u}
\newcommand{\vv}{\mathbf v}
\newcommand{\vw}{\mathbf w}
\newcommand{\vx}{\mathbf x}
\newcommand{\vy}{\mathbf y}
\newcommand{\vz}{\mathbf z}
\newcommand{\eins}{\mathbf 1}

\newcommand{\vdx}{\delta \mathbf x}
\newcommand{\vdu}{\delta \mathbf u}

\newcommand{\vA}{\mathbf A}
\newcommand{\vB}{\mathbf B}
\newcommand{\vC}{\mathbf C}
\newcommand{\vD}{\mathbf D}
\newcommand{\vE}{\mathbf E}
\newcommand{\vF}{\mathbf F}
\newcommand{\vG}{\mathbf G}
\newcommand{\vH}{\mathbf H}
\newcommand{\vI}{\mathbf I}
\newcommand{\vJ}{\mathbf J}
\newcommand{\vK}{\mathbf K}
\newcommand{\vL}{\mathbf L}
\newcommand{\vM}{\mathbf M}
\newcommand{\vN}{\mathbf N}
\newcommand{\vO}{\mathbf O}
\newcommand{\vP}{\mathbf P}
\newcommand{\vQ}{\mathbf Q}
\newcommand{\vR}{\mathbf R}
\newcommand{\vS}{\mathbf S}
\newcommand{\vT}{\mathbf T}
\newcommand{\vU}{\mathbf U}
\newcommand{\vV}{\mathbf V}
\newcommand{\vW}{\mathbf W}
\newcommand{\vX}{\mathbf X}
\newcommand{\vY}{\mathbf Y}
\newcommand{\vZ}{\mathbf Z}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\frA}{\mathfrak{A}}
\newcommand{\frB}{\mathfrak{B}}
\newcommand{\frC}{\mathfrak{C}}
\newcommand{\frD}{\mathfrak{D}}
\newcommand{\frE}{\mathfrak{E}}
\newcommand{\frF}{\mathfrak{F}}
\newcommand{\frG}{\mathfrak{G}}
\newcommand{\frH}{\mathfrak{H}}
\newcommand{\frI}{\mathfrak{I}}
\newcommand{\frJ}{\mathfrak{J}}
\newcommand{\frK}{\mathfrak{K}}
\newcommand{\frL}{\mathfrak{L}}
\newcommand{\frM}{\mathfrak{M}}
\newcommand{\frN}{\mathfrak{N}}
\newcommand{\frO}{\mathfrak{O}}
\newcommand{\frP}{\mathfrak{P}}
\newcommand{\frQ}{\mathfrak{Q}}
\newcommand{\frR}{\mathfrak{R}}
\newcommand{\frS}{\mathfrak{S}}
\newcommand{\frT}{\mathfrak{T}}
\newcommand{\frU}{\mathfrak{U}}
\newcommand{\frV}{\mathfrak{V}}
\newcommand{\frW}{\mathfrak{W}}
\newcommand{\frX}{\mathfrak{X}}
\newcommand{\frY}{\mathfrak{Y}}
\newcommand{\frZ}{\mathfrak{Z}}

\newcommand{\fra}{\mathfrak{a}}
\newcommand{\frb}{\mathfrak{b}}
\newcommand{\frc}{\mathfrak{c}}
\newcommand{\frd}{\mathfrak{d}}
\newcommand{\fre}{\mathfrak{e}}
\newcommand{\frf}{\mathfrak{f}}
\newcommand{\frg}{\mathfrak{g}}
\newcommand{\frh}{\mathfrak{h}}
\newcommand{\fri}{\mathfrak{i}}
\newcommand{\frj}{\mathfrak{j}}
\newcommand{\frk}{\mathfrak{k}}
\newcommand{\frl}{\mathfrak{l}}
\newcommand{\frm}{\mathfrak{m}}
\newcommand{\frn}{\mathfrak{n}}
\newcommand{\fro}{\mathfrak{o}}
\newcommand{\frp}{\mathfrak{p}}
\newcommand{\frr}{\mathfrak{r}}
\newcommand{\frs}{\mathfrak{s}}
\newcommand{\frt}{\mathfrak{t}}
\newcommand{\fru}{\mathfrak{u}}
\newcommand{\frv}{\mathfrak{v}}
\newcommand{\frw}{\mathfrak{w}}
\newcommand{\frx}{\mathfrak{x}}
\newcommand{\fry}{\mathfrak{y}}
\newcommand{\frz}{\mathfrak{z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} 
 
\usepackage{graphics} \usepackage{epsfig} \usepackage{mathptmx} \usepackage{times} \usepackage{amsmath} \usepackage{amssymb}  \usepackage{algorithm,algorithmic}

\title{\LARGE \bf
Risk Sensitive, Nonlinear Optimal Control: Iterative Linear Exponential-Quadratic Optimal Control with Gaussian Noise }



\author{Farbod Farshidian and Jonas Buchli\thanks{F. Farshidian and J. Buchli are with the
Agile \& Dexterous Robotics Lab at the Institute of Robotics and Intelligent
Systems, ETH Z\"urich, Switzerland. \{farbodf, buchlij\}@ethz.ch}}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}

  In this contribution, we derive ILEG, an iterative algorithm to find risk
  sensitive solutions to nonlinear, stochastic optimal control problems. The
  algorithm is based on a linear quadratic approximation of an exponential risk
  sensitive nonlinear control problem. ILEG allows to find risk sensitive
  policies and thus generalizes previous algorithms to solve nonlinear optimal
  control based on iterative linear-quadratic methods. Depending on the setting
  of the parameter controlling the risk sensitivity, two different strategies on
  how to cope with the risk emerge. For positive-value parameters, the control
  policy uses high feedback gains whereas for negative-value parameters, it uses a
  robust feedforward control strategy (a robust plan) with low gains. These
  results are illustrated with a simple example.  This note should be considered
  as a preliminary report.

\end{abstract}

\section{Introduction}
The advent of cheap and fast processors and the increasing application of 
complex embedded systems, like robots, has made computational methods for
controller design very appealing. Optimal control theory provides a set of
tools to establish this connection between numerical computation and
control. Among the wealth of numerical methods proposed in the optimal
control framework, the Sequential, Linear, Quadratic (SLQ) algorithms are of a
significant importance because of their computational efficiency. The main idea
behind SLQ is to approximate the original nonlinear optimal control problem by a
series of local Linear-Quadratic (LQ) problems. Based on the solutions
of this local problems we
can iteratively improve the solution to the original nonlinear problem.
Algorithms with this spirit are reported in \cite{mayne66, dunn89, sideris05,todorov05}.

One of the main drawbacks of standard SLQ formulation is that the resulting controller for a
stochastic problem with additive process noise is identical to the one which
is obtained by neglecting noise. In other words, the derived SLQ controller is
independent of the process noise statistics. This is known as the certainty
equivalence principle and it stems from the fact that the cost function
in an LQ problem only considers the mean of the given performance index.

In order to deal with this issue, it is necessary to include higher order
statistics of the performance index into the cost function. However a na\"ive
implementation of this idea only increases the nonlinearity of the problem.
One interesting approach to incorporate the higher order statistics is proposed
by Jacobson \cite{jacobson73}. In his risk sensitive control scheme which uses the
expectation of the exponential transformation of the performance index, he showed
that the optimal controller is sensitive to the noise statistics. More
importantly the computational difficulty of calculating this risk sensitive
controller is the same as the original LQ problem for the expectation of the
performance index.

In this paper, we will use the SLQ idea to sequentially approximate the nonlinear
problem with local LQ subproblems. However, instead of the conventional approach,
we will use the risk sensitive method to design the local controllers for the LQ
subproblems. Therefore, the proposed algorithm in this paper will iteratively
approximate the nonlinear problem with a risk sensitive LQ problem. The rest of
this paper is organized as follows; First, we show the relationship between the
solution of the risk sensitive optimal control problem to the one with the
conventional cost function. Then we will derive the theory behind our algorithm.
Finally, we will illustrate its performance on a continuous cliff world problem.


\section{Motivation}
Consider the following general stochastic nonlinear optimal control problem.

where  is a Brownian motion with zero mean and covariance  and
the cost function is defined as

 is the performance index which is in general a random variable
and a functional of the control policy, .  represents the
expectation with respect to this random variable.

This general optimal control problem does not have an analytical
solution, except for a few special cases. One of these cases is a linear system
with a quadratic cost function. However, as uncertainty equivalence principle
states the solution to this LQ problem does not consider the
stochastic characteristic of the problem, i.e. the designed control policy is
indifferent to the stochasticity of the problem. The reason is that the LQ
problem only considers the mean of the cost and ignores the higher order
momenta. A possible solution could be to add a measure of variance to the
regular cost function. Unfortunately, the resulting problem is not anymore an LQ
problem and there is no efficient algorithm to find the solution.

Following the idea of incorporating higher order momenta of the cost
function, we can consider the following family of exponential
functions:
 
where  is a real valued parameter. 

\textbf{\textit{Corollary 1}}: The logarithm of the cost function in Equation \eqref{eq:general_exp_cost_funtion} can be expanded as

where  and  are the variance and the skewness of  (the cost of the optimal policy) respectively. \\
\textit{Proof}: see Appendix A.

Corollary 1 shows that by using the exponential cost function family, we can
incorporate the momenta of higher orders of the original cost function
momentum in the optimal control problem. Fortunately, like for the LQ problem, we can find an
analytical solution for the optimal control problem with linear dynamics and a
cost function defined as the exponential of a quadratic cost. In this paper we
will investigate this class of optimal control problems in more detail. We will
call this problem the \emph{Linear (linear dynamics), Exponential-quadratic
(Exponential-quadratic cost) problem with Gaussian process noise} or in short
``LEG'' optimal control problem.

In the next section, we will devise a dynamic programming approach to find the
optimal controller for the general problem with exponential cost. Furthermore, we
will show that this family of problems includes the common (i.e. with respect to
the mean) optimal control problem as a special case for a specific choice of a
parameter.

\section{Problem formulation}
First, assume a general optimal control problem with the following exponential
cost function:

where  is defined as

 is a general nonlinear function and the state trajectories are
generated through the stochastic system defined by Equation \eqref{eq:wiener_process}.

\textbf{\textit{Theorem 1}}: The solution to the optimal control problem defined
in Equations \eqref{eq:wiener_process} and \eqref{eq:cost_funtion} is

where  is the solution to the following partial differential equation (PDE)

with boundary condition  (in the interest
of compact notation, we dropped the functionality with respect to  and ). \\
\textit{Proof}: see Appendix B.

We call the PDE in Equation \eqref{eq:nonlinear_utility_pde} the \emph{extended
Hamilton-Jacobi-Bellman Equation} or in short \emph{extended HJB Equation}.
This equation forms the basis for deriving our algorithm which iteratively
approximates a general nonlinear exponential optimal control problem by LEG
optimal control  in order to approximate the solution in an efficient manner.
Before continuing to the next section, we will take a look at the relationship
between the exponential and the common optimal control problem.

\textbf{\textit{Note}}: If  approaches zero, the optimal control
policy in Equation \eqref{eq:optimal_control} and the value function in Equation
\eqref{eq:nonlinear_utility_pde} are the solution to the common optimal control
problem with the following cost function.

\textit{Proof}: This can be easily verified by putting  and comparing
it with the common HJB equation.

This shows that the exponential optimal control problem converts to the regular
optimal control problem for  equal to zero.


\section{Iterative Linear Exponential-quadratic Optimal Control under Gaussian Process Noise: ILeg} \label{sec:continuous_ILEQG}
ILEG (\textbf{I}terative, \textbf{L}inear, \textbf{E}xponential-quadratic optimal
control under \textbf{G}aussian process noise) is an iterative optimization
method for solving the optimal control problem for a general nonlinear system
with an exponential cost function which is affected by Gaussian process noise.
ILEG designs locally-optimal feedback control for nonlinear, stochastic,
continuous-time systems. Given an initial, feasible sequence of control inputs,
we iteratively obtain a local linear approximation of the system dynamics and a
exponential-quadratic approximation of the cost function, and then incrementally
improve the control law, until we converge to a local minimum. In that sense it
is closely related to previous approaches to solve nonlinear optimal control
algorithms with iterative LQ methods \cite{sideris05,todorov05} with the key
difference that ILEG is risk sensitive and generalizes previous algorithms.

Lets assume we are in iteration  of the algorithm and  and
 are respectively the state and control input trajectories generated
through implementation the latest optimized controller. Then we approximate
system dynamics with a time varying linear system along these trajectories and
the cost function with the exponential quadratic function as follows

and the quadratic approximation of the cost function

with
 
where  and  and , , , , , and  are the coefficients of the Taylor expansion of the cost function over the nominal trajectory.

\textbf{\textit{Theorem 2}}: The solution to the optimal control problem defined in Equations\! (\ref{eq:dynamics_linear_approximation}-\ref{eq:cost_quadratic_approximation_detail_2}) exists if  is positive semidefinite for all \textit{t} and the solution can be found as follows

with the final values , , and . The optimal control is

\textit{Proof}: see Appendix C.


\section{Summary of the ILEG Algorithm}
Algorithm \ref{alg:ileg} summarizes the ILEG algorithm described in the previous
section. This algorithm assumes the system dynamics and the exponential cost
function as given. It also requires to define a parameter named . As we
stated in the Theorem 2, the matrix expression  should be always positive semi-define which
imposes an upper bound over . In the next section we will discuss the
effect of this parameter in more details.

In each iteration of this algorithm, we need to forward integrate the noise-free
system dynamics using the latest update of the controller. Then we approximate
the system dynamics and the cost function along the forward-integrated trajectories. The
algorithm use a linear approximation for the system dynamics and an
exponential-quadratic approximation for the cost function.

In the next step, we solve the approximated LEG problem using the results
from Theorem 2. This solution gives us an update to the optimal control policy.
Finally we should iterate this process until a termination condition is fulfilled.
\begin{algorithm}[tpb]
\caption{ILEG Algorithm}
\label{alg:ileg}
\begin{algorithmic}
\scriptsize 
	\STATE \textbf{Given}
	\STATE - System dynamics in Equation \eqref{eq:wiener_process} 
	\STATE - Cost function in Equation \eqref{eq:cost_funtion}
	\STATE - Choose  in the allowed range
	\STATE \textbf{Initialization}
	\STATE - Initialize the controller with a stable control law, 
	\REPEAT
	\STATE - Forward integrate the system dynamics:
	\STATE 
	\STATE - Compute the linear approximation of the system dynamics along the nominal trajectory , Equations\! (\ref{eq:A}-\ref{eq:C})
	\STATE - Compute the quadratic approximation of the cost function along the nominal trajectory , Equations\! (\ref{eq:cost_quadratic_approximation_detail_1}-\ref{eq:cost_quadratic_approximation_detail_2}) 
	\STATE - Solve the final value differential Equations\! (\ref{eq:riccati_Sm}-\ref{eq:riccati_s})
	\STATE - Update the control law: 
	\UNTIL{a termination condition is matched}
\end{algorithmic}
\end{algorithm} 

\section{Numerical Example}
In this section, we will show some preliminary results of the ILEG
implementation on a continuous cliff world problem. In this problem a point mass
(1kg) should be navigated from one corner of a rectangle area to the other while
at the border of the area there is a cliff (Figure \ref{fig:cliff_world}). The
mass point motion is influenced by a Brownian motion on both the X and the Y
directions. However the noise standard deviation (SD) in the Y directions is 10 times higher which increases the chances of falling off the cliff. The goal of
this problem is to design a controller which can navigate the point mass form the
start point to the goal point with minimum control effort without falling.
\begin{figure} [bpb]
\centering
\includegraphics[width=0.5\textwidth]{CliffWorldScheme.pdf}
\caption{A continuous cliff world. S and G indicate the start and goal
  position respectively. Moving through the white region induces low
  cost, while ``falling'' over the cliff induces very high cost.}
\label{fig:cliff_world}
\end{figure}

In order to formulate this problem as an optimal control problem as defined by 
Equations \eqref{eq:wiener_process} and \eqref{eq:cost_funtion}, we should
replace the hard constraint of the cliff by a soft constraint which penalizes the
distance of the mass from the cliff. Thus, we define the following cost function
for this problem
 
Equation \eqref{eq:phi_cliff_world} is the terminal cost at  which puts a
high penalty for deviating from the goal state, , at time 3[sec]. It
also penalizes the point mass speed at the final time. Therefore, the final cost
encourages the point mass to reach and stop at the goal state within 3 seconds.
In Equation \eqref{eq:l_cliff_world}, the first term is a penalty term for
falling off the cliff. Finally, the last two terms add cost for the exerted
control forces in each motion directions. Notice that since the noise in Y
direction has higher standard deviation, we penalize the controller less for the
effort to confront the noise.

Although the point mass in this problem has linear system dynamics, the defined
cost function is nonlinear. Consequently the optimal control problem defined by
this cost function is nonlinear. We use ILEG on this problem to find the optimal
policy. The algorithm converges after few iterations. The resulting control
policy shows different characteristics depending on the chosen parameter 
. In general,  has an upper limit beyond which the designed
policy will be unstable. In this cliff world problem, this limit is 50. Here, we
implemented the ILEG algorithm for 5 different choices of .

Figure \ref{fig:gains_y_direction} demonstrates the changes of the feedback gains
over time in the Y direction. As expected, by decreasing  from
 to  the absolute value of the gains decreases
monotonically. For , the value of  which the controller does
not take the stochasticity of the problem into account (it is the solution to the
non exponential cost function). As Figure \ref{fig:gains_y_direction} shows by
increasing  to positive values the controller uses higher gains to reduce
the variance of the generated motions. However, if we decrease  to
negative values, the controller uses lower gains and therefore the motion
generated under this controller will have higher variations. In order to
compensate for these higher variations which can cause the point mass to fall off
the cliff, whereas the -negative controller will choose a more
conservative path. In other words, to deal with the uncertainty, the controller
prefers a safer plan over a stiffer controller. Figure \ref{fig:sigma_comparison}
illustrates this for three different  values.

\begin{figure} [tpb]
\centering
\includegraphics[width=0.5\textwidth]{gains_y_direction.pdf}
\caption{Y direction controller gains for 5 different  values, namely 45, 35, 0, -45, and -100. The left plot illustrates the changes of the proportional gains in the course of time and the right plot shows the derivative gains.}
\label{fig:gains_y_direction}
\end{figure}

As in Figure \ref{fig:sigma_comparison}, the positive  takes a shorter
path than the negative one while the negative  chooses a safer path. In
this figure the shaded error--bands are a measure of the path variations under the
system noise. We see that, since the negative  has lower gains, it has a
wider error--band than the positive one.

\section{CONCLUSIONS AND FUTURE WORK}
In this preliminary work, we have introduced an iterative optimal control
algorithm named as ILEG. ILEG iteratively approximates the system dynamics and
the cost function by a linear system and an exponential-quadratic cost
respectively. Then it efficiently solves the approximated LEG subproblems. We
showed that the advantages of using exponential cost function instead of a
regular one is that the higher order momenta of the performance index are also
considered during the optimization.
  
An interesting aspect of the ILEG algorithm is that it introduces an algorithmic
parameter which can control the behavior of the optimal control. By setting this
parameter to zero, ILEG basically reduces to the well-known SLQ. However by
setting this parameter to a positive or a negative value, we can obtain two
different types of policies. For the positive-value parameter the control policy
mostly relies on the error feedback signal, using high gains ('stiff controls')
while in the negative-value parameter the control policy contains a robust plan
(forward controls), using lower gains.

\subsection{Future Work}
This work is currently in its early stage. The effect of the  parameter
should be studied through more analytical methods rather than a numerical
example. Furthermore, even though Algorithm \ref{alg:ileg} imposes an upper bound
on , it is not totally clear that this is the only restriction over
. Questions like the stability of the designed controller under different
values of  should be also addressed. Last but not least, the proposed
algorithm should be implemented on more practical examples.

\section{ACKNOWLEDGMENTS}
This research has been supported in part by a Swiss National Science
Foundation Professorship Award to Jonas Buchli, the NCCR Robotics and a
Max-Planck ETH Center for Learning Systems Ph.D. fellowship to Farbod Farshidian.




\bibliographystyle{bibliography/IEEEtran}
\bibliography{bibliography/IEEEabrv,bibliography/references}





\onecolumn

\begin{figure*} [tpb]
\centering
\includegraphics[width=0.7\textwidth]{SigmaComparison.pdf}
\caption{The traversed path of the point mass using controllers with 3 different  values, namely 45, 0, and -100. The shaded area is 15 percent SD of the trajectories}
\label{fig:sigma_comparison}
\end{figure*}


\section{Appendix A}
\textbf{\textit{Corollary 1}}: The cost function in Equation \eqref{eq:general_exp_cost_funtion} can be expanded as

where  and  are the variance and the skewness of
 respectively.

\textbf{\textit{Proof}}: Assuming that the cost associated with one execution of
the optimal policy is . We can show  as

Therefore we will have

 is the cumulant
generating function of the random variable . By writing the Taylor
series expansion of , we will have

where  is the \textit{i}th cumulant of . Using the fact
that the first three cumulants are mean, variance, and skewness will conclude the
proof.

\section{Appendix B} 
\textbf{\textit{Theorem 1}}: The solution to the optimal control problem defined
in Equations \eqref{eq:wiener_process} and \eqref{eq:cost_funtion} is

where  is the solution to the following partial differential equation (PDE)

with boundary condition  (to make the equation
shorter, we have dropped the functionality with respect to  and ).

\textbf{\textit{{Proof}}}: 
In order to solve this optimal control problem, we chose a dynamic programming
approach. First consider a discrete time problem with the system dynamics
described by Equation \eqref{app_eq:discrete_dynamics}

where  is a Gaussian random process with zero mean and covariance
. The discrete cost function is
also defined as the following

It can be easily shown that the solution to this discrete optimal control problem
can be obtained using Equation \eqref{app_eq:extended_bellman}.

This equation is called the extended Bellman equation. 

In order to find the solution of the continuous time optimal control problem
defined in Equations \eqref{eq:wiener_process} and \eqref{eq:cost_funtion}, we
should find the equivalent dynamic programming formula. This can be achieved by
discretizing the continuous time equation and using the extended Bellman equation
to find the optimality equation. Then by the use of the Ito lemma, we can derive the
following optimality equation called extended HJB equation.

Using the exponential transformation  in \eqref{app_eq:extended_hjb} we get

Substituting these equations in the extended HJB equation (for the simplicity we
will drop all of the subscripts)

by further simplification we get

If we assume that the cost function is quadratic with respect to control input
 as

the optimal control input will be

and the HJB equation will be



\section{Appendix C}
\textbf{\textit{Theorem 2}}: 
The solution to the optimal control problem defined in Equations\!
(\ref{eq:dynamics_linear_approximation}-\ref{eq:cost_quadratic_approximation_detail_2}) exists if  is positive semidefinite for all the \textit{t}s and the solution can be found as it follows

with the final values , , and . The optimal control is


\textbf{\textit{Proof}}: 
The approximate optimal control problem defined in Equations\! (\ref{eq:dynamics_linear_approximation}-\ref{eq:cost_quadratic_approximation_detail_2}) can be solved by the use of Equation \eqref{app_eq:nonlinear_utility_pde}. We will make the following Ansatz for  to solve PDE
 

Then we will have 

we can rearrange the above equation as



By equating the coefficient of , we will have the following equations.

with final values

and the optimal control



These equations will have solutions if  is positive semidefinite for all \textit{t}. We
can further simplify these equations by regrouping them as





\end{document}
