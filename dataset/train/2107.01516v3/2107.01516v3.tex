\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts, dsfont, booktabs}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp, hyperref}
\usepackage{xcolor}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=magenta,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
    
    
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\begin{document}



\title{Introducing Self-Attention to Target Attentive Graph Neural Networks
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Sai Mitheran}
\IEEEauthorblockA{\textit{National Institute of Technology,}\\
\textit{Tiruchirappalli}\\
saimitheran06@gmail.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Abhinav Java}
\IEEEauthorblockA{\textit{Delhi Technological University} \\
java.abhinav99@gmail.com}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Surya Kant Sahu}
\IEEEauthorblockA{\textit{The Learning Machines} \\
surya.oju@pm.me}
\and
\IEEEauthorblockN{4\textsuperscript{th} Arshad Shaikh}
\IEEEauthorblockA{\textit{BYJU's} \\
arshaikh5775@gmail.com}
}

\maketitle

\begin{abstract}
Session-based recommendation systems suggest relevant items to users by modeling user behavior and preferences using short-term anonymous sessions. Existing methods leverage Graph Neural Networks (GNNs) that propagate and aggregate information from neighboring nodes i.e., local message passing. Such graph-based architectures have representational limits, as a single sub-graph is susceptible to overfit the sequential dependencies instead of accounting for complex transitions between items in different sessions. We propose a new technique that leverages a Transformer in combination with a target attentive GNN. This allows richer representations to be learnt, which translates to empirical performance gains in comparison to a vanilla target attentive GNN. Our experimental results and ablation show that our proposed method is competitive with the existing methods on real-world benchmark datasets, improving on graph-based hypotheses. Code is available at \url{https://github.com/The-Learning-Machines/SBR}.
\end{abstract}

\begin{IEEEkeywords}
Session-based recommendation, Graph neural networks, Transformers
\end{IEEEkeywords}

\section{Introduction}
Traditional recommendation systems use user-item interactions across multiple sessions to model user preferences. However, in session-based recommendation (SBR) systems, the users are anonymous; hence inter-session data cannot be used. The goal here is to predict the items with which the user is likely to interact, given previous item interactions within a single session. In general, the number of user-item interactions is limited, and as a result, modeling user intent is a challenging task. Nevertheless, session-based recommendation is gaining momentum due to increasing privacy concerns. Recent advancements in session-based recommendation systems have focused on modeling the user-item interaction as a directed graph and hence leverages graph-based architectures and related multi-level feature extraction techniques. However, these methods are disposed to representational limits \cite{garg2020generalization} as a sub-graph tends to overfit the sequential dependencies, while the essence of extracting the representations that connote complex transitions between multi-session items are lost.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Intro/Intro_SBRDiagram.png}
    \caption{Session-based Recommendation Systems}
    \label{task}
\end{figure}

In this work we present TAGNN++, incorporating Transformers as universal function approximators to enhance capturing complex transitions that address the limitations of GNNs in learning rich representations. We model item interactions using a GNN and both global and local user interaction with a Transformer. We show that our model is competitive with the existing state-of-the-art techniques on the Diginetica benchmark and on the Yoochoose benchmark. We also explore Adaptive Gradient Clipping \cite{brock2021high} for Transformer-based architectures specific to our task and present an ablation study to analyze the performance of the model.



\section{Related works}



\subsection{Deep Learning based SBRs}




GNNs offer an intuitive approach to session-based recommendation since each session can be mapped into a graph's chain. Each node of the graph represents an item, and each edge represents an interaction. The natural compatibility between data modeled in such a manner and GNNs allow this method to perform well and was introduced in \cite{Wu:2019ke}. Despite the convenient representation of sessions offered by GNNs, it lacks the ability to model long-range dependencies and intricate interactions as substantiated in Graph-Contextualised Self-Attention model \cite{ijcai2019-547}. Qiu \textit{et al.} \cite{Rethinking} proposed using a weighted graph attention layer for focusing on the essential parts of the item embeddings. Extending the idea of using attention with GNNs, Yu \textit{et al.}\cite{yu2020tagnn} proposed using both item embeddings with a GNN and target embedding with an attention mechanism to achieve significant performance gain. However, a GNN applied on a single sub-graph is susceptible to overfit on sequential dependencies instead of accounting for complex transitions between items in different sessions. Using a dual-channel GNN capable of complex item transition modeling addresses this issue \cite{dgtn}. 









\subsection{Use of Adaptive Gradient Clipping (AGC)}

Clipping the gradient is a commonly used approach to improving gradient descent \cite{Zhang2020WhyGC}, but manual selection of the clipping threshold increases the number of hyperparameters. However, the hyperparameter for gradient clipping needs to be tuned carefully, as it is susceptible to the loss function and architecture \cite{9231926}. The chosen threshold is vital as if it is set too high, then the gradient norm will always be smaller than that, and clipping is never applied. If too low, then the network's step size may be too small and cause convergence issues leading to unstable learning. We can clip gradients based on the unit-wise ratio of gradient norms to parameter norms \cite{brock2021high}. This helps ensure stable training across different batch sizes, allowing larger learning rates to ensure quick convergence, and is effective to tackle poorly conditioned loss landscapes. By incorporating this, we verify that performance is marginally increased for Transformer architectures specific to our task.



\section{Method}

\subsection{Problem Formulation and Preliminaries}
The SBR problem contains three types of entities : Users, Items, and User-item interactions. Here, we formally introduce the SBR problem and associated notations. Let  be the set of  sessions and  be the set of  unique items in the dataset. Let  be the item sequence within a session , where  is the item clicked by a user at time step . The goal is to estimate the parameter set  such that  is the maximum-likelihood estimator of , where .

The proposed TAGNN++ leverages two key components, namely - \emph{Graph Neural Networks and Self-Attention}. We use the same background strategy for constructing the session graph from the given data as proposed in \cite{yu2020tagnn} and \cite{Wu:2019ke}. Each session is stored in memory as an adjacency matrix representing a directed graph. In this graph , the  node in the node set  represents an item such that . Each edge connecting nodes  and  denotes subsequent item selections by the user in the given session  such that  \cite{yu2020tagnn}. Lastly, each node has an incoming weight and an outgoing weight associated to it. We effectively capture item transitions and connections using a gated GNN \cite{li2015gated}, by learning both item and session embeddings as in \cite{yu2020tagnn}.



\subsection{Proposed method - TAGNN++}
We observe that a simple attention model is unable to capture both local and global context \cite{NIPS2017_3f5ee243}, hence indicating the need for a more robust way to represent sequences. Furthermore, GNNs build a representation of data by message passing or neighborhood aggregation, using only local information. Garg \textit{et al.} show that even simple graph structures are indistinguishable by GNNs relying only on local information, making it hard for a GNN to compute several graph properties \cite{garg2020generalization}. Additionally, they perform a thorough study of generalization bounds for message passing, which accentuates the need for better function approximators.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Architectures/TAGNN++.png}
    \caption{Proposed Architecture : TAGNN++}
    \label{arch}
\end{figure}





To that end, we propose a multi-headed Transformer-based design for target-aware predictions. The proposed design enables us to leverage both GNNs and powerful attention models; in essence, model item interaction in the form of a graph and a user's long and short-term interaction through the Transformer. We hypothesize that our design is a \textit{superior function approximator} for the task of next-item prediction in session-based recommendation, providing empirical evidence and a carefully crafted ablative study. Recently, Transformers have been successfully used in Natural Language Processing and Computer Vision by parallelizing the self-attention \cite{NIPS2017_3f5ee243} mechanism over multiple heads achieving state of the art results. \cite{yun2019Transformers} also gives a compact support based proof of why Transformers have high representation capacity and are \textit{universal approximators of permutation equivariant sequence to sequence functions}. 

At the  layer the following update takes place in a single head:



where  is the hidden state of the  item in sequence  at layer  such that . , ,  are Queries, Keys and Values respectively, and  denotes the softmax operation. , ,  are learnable parameters that are updated in parallel as opposed to sequentially, and  is a hyperparameter indicating the dimension of the linear layer in the Transformer. In our approach, we allow the Transformer to concatenate several representations or transformations of the interaction between the hidden states of the target and sequence. This is done using multiple such attention heads and the concatenation of the outputs before normalizing them across the layer, before being passed on to a Feed-forward module. At the feature level, it is possible that the values in the attention matrix cover a large domain, which is not desirable since the network finds it hard to learn the optimal parameter across different scales quickly. This is where Layer Normalization comes into play by normalizing across the feature space. 


Finally, the token-wise Feed-forward Layer (FFL) transforms the normalized context vector to the output sequence. The composition of these FFLs implicitly implement a scalar quantization map such that each input is mapped to the output \cite{yun2019Transformers}. We stress that the proposed augmentations to the TAGNN architecture make it more robust to different kinds of data streams. The various operations in the Transformer that include Self-attention, Layer Norm, and the Feed-forward Layer with skip connections play a vital role in enabling Transformers to be universal approximators of sequence-to-sequence functions.



\begin{table}[htb]
\centering
\caption{Comparison of several baseline methods with the proposed method.}
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{\textbf{Method}}                                            & \multicolumn{2}{c}{\textbf{Yoochoose 1/64}} & \multicolumn{2}{c}{\textbf{Diginetica}} \\ \midrule
                                                                        & HR@20            & MRR@20           & HR@20          & MRR@20         \\ \midrule


GRU4REC  \cite{hidasi2015session}                                                               & 60.64            & 22.89            & 29.45          & 8.33           \\
NARM     \cite{li2017neural}                                                               & 68.32            & 28.63            & 49.70          & 16.17          \\
RepeatNet  \cite{repeatnet}                                                             & 70.71            & 31.03            & 47.79          & 17.66          \\
CSRM    \cite{CSRM}                                                                & 71.45            & 30.36            & 50.55          & 16.38          \\
SR-GNN  \cite{Wu:2019ke}                                                                & 70.87            & 30.94            & 50.73          & 17.59          \\
GC-SAN \cite{ijcai2019-547}                                                                 & 70.66            & 30.04            & 51.70          & 17.61          \\
TAGNN \cite{yu2020tagnn}                                                                & 71.02            & 31.12            & 51.31          & 18.03          \\
LESSR  \cite{chen2020handling}                                                                 & 70.64            & 30.97            & 51.71          & \textbf{18.15}          \\  \midrule


\textbf{\begin{tabular}[c]{@{}l@{}}TAGNN++\end{tabular}} & \textbf{71.91} & \textbf{31.57} & \textbf{51.86} & 17.93 \\ \bottomrule 
\end{tabular}
\label{tabcomparison}
\end{table}


\section{Setup and Experimental Results}





\textbf{\textit{Evaluation Metrics}}: We use two metrics from previous studies, i.e., Hit Rate@N, and MRR@N, where N . Hit Rate calculates the number of "hits" in an N-sized list of ranked items, where a "hit" refers to something that the user has clicked on, purchased, or "saved for later", based on context. Mean reciprocal rank (MRR) is used to judge a system where the order/positions of the retrieved items are important.

\textbf{\textit{Datasets}}: For testing our hypothesis, we employ two widely used real-world datasets, Yoochoose\footnotemark[2] and Diginetica\footnotemark[1]. Table \ref{dataset} provides information on the dataset contents. We use only the recent 1/64 fraction of the Yoochoose dataset, denoted as Yoochoose 1/64, which comprises various sessions that specify the clicks by a given user. It is a collection of records in a file containing a Session ID, Timestamp, Item ID, and Category. Diginetica contains anonymized search and browsing logs, product data, anonymized transactions, and an extensive collection of product images.


\textbf{\textit{Preprocessing}}: For simplicity, we apply the same preprocessing as \cite{yu2020tagnn}, \cite{Wu:2019ke}. We drop all unit length sessions and remove items that appear less than five times for both datasets as same as previous studies. For generating training and test sets, sessions of last days are used as the test set for Yoochoose 1/64, and sessions of last weeks as the test set for Diginetica. For an existing session, we generate a series of input session sequences and corresponding labels. We filter out items from the test set which do not appear in the training set.

\textbf{\textit{Hyperparameters}}: We retain most hyperparameter settings from previous baselines to display the advantage of learning better representations using Transformers. We keep  of our datasets for validation and use a batch size of , for  epochs. We use the Adam \cite{da2014method} optimizer with an initial learning rate of , with momentum parameters  = ,  = , and decay it by a factor of  every  epochs. Additionally, we set the  penalty to . 
For Multi-Head Attention in the Transformer, we set the number of heads as  and dropout of , with the embedding dimension as  for Yoochoose 1/64 and Diginetica respectively.



\textbf{\textit{Baselines}}: We compare our method with baseline GNN and Attention-based methods for session-based recommendation, as shown in Table \ref{tabcomparison}. The RNN-based methods \cite{hidasi2015session} \cite{li2017neural} \cite{CSRM}, and further \cite{repeatnet} that takes repeat consumption into account were then outperformed by graph-based methods \cite{Wu:2019ke}, those involving the notion of attention \cite{ijcai2019-547}  \cite{yu2020tagnn}.

\begin{table}[htb]
\centering
\caption{Ablation Study}
\label{ablation}
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{\textbf{Architecture}}                 & \multicolumn{2}{c}{\textbf{Yoochoose 1/64}} & \multicolumn{2}{c}{\textbf{Diginetica}} \\ \midrule
     & \multicolumn{1}{c}{HR@20} & MRR@20  & HR@20   & MRR@20  \\ \midrule
\textbf{TAGNN++}    &                                        \textbf{71.91} & 31.57 & \textbf{51.86} & \textbf{17.93}   \\ 
- AGC & 71.80                    & 31.41 & 51.57 & 17.65 \\ 
- GNN & 71.75           & \textbf{31.62} & 51.48 & 17.59  \\ 
- PE  & 71.69                      & 31.48   & 51.64   & 17.66   \\ 
- Transformer & 71.03                      & 30.69   & 51.42   & 17.84 \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[htb]
\centering
\caption{Dataset Stats}
\begin{tabular}{llclll}
\toprule
\multicolumn{1}{c}{\textbf{Dataset}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Clicks\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}} Train\\ Sessions\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Test \\ Sessions\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Total \\ Items\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}l@{}}Avg. \\ Length\end{tabular}} \\ \midrule
Diginetica &
  982961 &
  719470 &
  68977 &
  43074 &
  5.13 \\ \hline
Yoochoose 1/64 &
  557248 &
  \multicolumn{1}{l}{375043} &
  55898 &
  16339 &
  6.11 \\ \bottomrule
\end{tabular}
\label{dataset}
\end{table}


      \textbf{\textit{Inferences}}: We can infer from the values of HR@20 and MRR@20 that our method is competitive with the previous state-of-the-art on the given benchmarks, whilst outperforming the graph-based TAGNN model \cite{yu2020tagnn}. Hence, it is evident that Deep learning-based methods are more capable of learning a better user-item interaction representation by \emph{capturing complex data distributions and transitions} in the dataset. Incorporating Transformers to overcome the representational limitations in a GNN improves the performance of our recommendation system. The improvement observed verifies that learning better representations as proposed is helpful to model complex patterns, as shown in Fig. ~\ref{fig:eval_sbr}. To verify the efficacy of our architecture, we also perform an ablation study as shown in Table \ref{ablation}. It can be observed that the performance is only marginally affected by the removal of secondary techniques such as AGC (Adaptive Gradient Clipping) and PE (Positional Embedding). Further, the removal of the Transformer decreases the overall performance, verifying that GNNs fail at learning accurate representations in some cases.


Fig. ~\ref{fig:eval_sbr} indicates better performance on Yoochoose 1/64 than Diginetica, due to a greater average session length in the former - as more historical data would enhance the Transformer’s attention mechanism to leverage global context. Both datasets consist of short, medium, and long sessions \cite{Wang2019ASO}. When the user wishes to build a session-based recommender consisting of long sessions (Yoochoose 1/64) with better rank/order as the main objective (tweets, webpages, music), we show that our model outperforms existing methods by a considerable margin. For short/medium length sessions (Diginetica), where both order and the number of desired items in the top N of the ranked list are important (shopping items, movies/videos), our model is competitive with the existing methods. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/Eval/Eval_Plot_Vertical.png}
    \caption{Scatter plot comparison of models on the evaluation metrics}
    \label{fig:eval_sbr}
\end{figure}

\section{Conclusion}
This paper proposes an alternative method for learning richer representations in session-based recommendation models, to overcome the limitations posed by message-passing GNNs. We perform intelligent feature extraction using Transformers with target attentive GNNs. We leverage Multi-head Attention to capture both local and global context. Our method is competitive with the previous state-of-the-art on the aforementioned benchmarks. We show that our method is suitable for rank-based retrieval in long sessions (Yoochoose), and also establish competitive performance for short/medium length sessions (Diginetica). Motivating research in this direction would enable making informative and practical choices in streaming, business operations, and E-Commerce.


\bibliographystyle{IEEEtran}
\bibliography{sample-base}

\end{document}
