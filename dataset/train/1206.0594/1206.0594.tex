\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage{fullpage}
\usepackage{graphicx}

\newcommand{\SVD}{\operatorname{SVD}}
\newcommand{\R}{{\mathbb R}}
\renewcommand{\S}{{\mathbb S}}
\newcommand{\E}{{\mathbb E}}
\newcommand{\N}{{\mathcal N}}
\newcommand{\eps}{\varepsilon}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\FI}{Frequent-Items}
\newcommand{\FD}{Frequent-Directions}
\newcommand{\myfigurewidth}{0.7}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}

\title{Simple and Deterministic Matrix Sketching}
\author{Edo Liberty\thanks{Yahoo! Research}}
\date{\nonumber}

\begin{document}
\maketitle

\begin{abstract}
We adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. 
The algorithm receives the rows of a large matrix  one after the other in a streaming fashion. 
It maintains a sketch matrix  such that for any unit vector 

Sketch updates per row in  require  operations in the worst case.
A slight modification of the algorithm allows for an amortized update time of  operations per row. 
The presented algorithm stands out in that it is: deterministic, simple to implement, and elementary to prove.
It also experimentally produces more accurate sketches than widely used approaches while still being computationally competitive.
\end{abstract}

\section{Introduction}
Efficiently obtaining compact approximations, or sketches, of large matrices is a very common and important problem.
It is essential for obtaining approximate matrix Singular Value Decompositions (SVD) or low rank approximations of large matrices.
It is used in large scale data mining, Latent Semantic Indexing (LSI), -means clustering, Principal Component Analysis (PCA), Linear regression, solving large linear systems, matrix preconditioning, and many other important and commonly performed tasks.

Moreover, modern large data sets are often viewed as large matrices.
For example, textual data in the bag-of-words model is stored such that each row in the data matrix 
corresponds to one document and non zero entries correspond to words in the documents.
Such matrices are often extremely large and distributed across many machines.
Sketching methods, therefore, are designed to be pass efficient which means the data is read at most a constant number of times.
If only one pass is required the computational model is also referred to as the streaming model. 
The streaming model is especially attractive since the analysis can be performed immediately when the data is collected.
In that case its storage can be eliminated altogether. 


There are three main matrix sketching approaches which are presented here in an arbitrary order.
The first generates a sparser version of the matrix. 
Sparser matrices are stored more efficiently and can be multiplied by other matrices 
faster \cite{AroraHazanKale2006}\cite{AchlioptasMcsherry2007}.
The second approach is to randomly combine matrix rows  \cite{PapadimitriouTRV1998}\cite{Vempala2004}\cite{Sarlos06}\cite{tygert07PNAS}.
These methods rely on properties of random low dimensional subspaces and strong concentration of measure phenomena.
The third is to find a small subset of matrix rows (or columns) which approximate the entire matrix. 
This problem is known as the Column Subset Selection Problem and has been thoroughly investigated 
\cite{FriezeKannanVempala1998}\cite{Drineas03passefficient}\cite{BoutsidisMahoneyDrineas2009}\cite{DeshpandeV06}\cite{DrineasMohoneyMuthukrishnan2011}\cite{BoutsidisDrineasMagdon2011}. Recent results obtain algorithms with almost matching lower bounds \cite{DeshpandeV06}\cite{BoutsidisDrineasMagdon2011}\cite{ClarksonWoodruff2009}.
Alas, It is not immediately clear how to compare these methods' results to ours since their objectives are different.
They aim to recover a low rank matrix whose column space contains most of space spanned by the matrix top  singular vectors.
Moreover, most of the above algorithms require several passes over the input matrix.
A simple streaming solution to the Column Subset Selection problem is obtained by sampling columns (rows in this case) from the input matrix.
The rows are sampled with probability proportional to their squared norm.
Despite this algorithm's apparent simplicity, providing tight bounds for its performance required over a decade of research \cite{FriezeKannanVempala1998}\cite{AhlswedeW02}\cite{DrineasKannan2003}\cite{RudelsonVershyninMatrixSampling2007}\cite{VershyninMatrixChernoffBounds}\cite{Oliviera2010}\cite{DrineasMohoneyMuthukrishnan2011}. 

This manuscript proposes a forth approach which draws on the matrix sketching problem's similarity to the item frequency estimation problem.
In what follows, we shortly describe the item frequency approximation problem and a well known solution for it
which our proposed algorithm will be based on. 


In the item frequency approximation problem there is a universe 
of  items  and a stream  of item appearances.
The frequency of an item is the number of times it appears in the stream.
It is trivial to produce these counts using  space simply by keeping a counter for each item.
Our goal is to use  space and produce approximate 
frequencies  such that  for all  and prescribed precision .


This problem received an incredibly simple and beautiful solution by \cite{Misra1982}.
This solution was later and independently rediscovered 
by \cite{DemaineLopezAlejandroMunro2002} and \cite{Karp03} which also improved its update complexity.
The algorithm simulates the process of repeatedly `deleting' form the 
stream  appearances of {\it different} items until this is no longer possible. 
In other words, until there are less than  unique items left. 
This trimmed stream is stored concisely in  space.
The claim is that if item  appears in the final trimmed stream  times 
than  is a good approximation for its true frequency  (even if ).
This is because  where  is the number of times items were deleted (item of type  is deleted at most once in each deletion batch). 
Moreover, we delete  items in every batch and at most  items can be deleted altogether. Thus,  or  which completes the proof.
The reader is referred to \cite{Karp03} for an efficient streaming implementation.
From this point on we refer to this algorithm as \FI. 






Let us now describe the item frequency problem as a matrix sketching problem. 
Let  be a stream of indicator vectors  instead of discrete elements.
Each row in  is  (the 'th standard basis vector) if .
The frequency  can be expressed as . 
A good approximation matrix  would be one such that  is a good approximation to .
Replacing  we get that the condition 
is equivalent to . 
From the above, it is clear that for `item indicator' matrices a sketch  can be
obtained by the \FI~algorithm.

In this paper we suggest \FD~which is an extension of \FI~to general matrices. 
Given any matrix  a sketch  is produced such that:

The intuition behind \FD~is surpassingly similar to the one above.
In the same way that \FI~periodically deletes  different element, \FD~periodically removes from its sketch  orthogonal vectors.
This means that the Frobenius norm of the trimmed sketch matrix reduces by a factor  faster than its projection on any single direction.
Since the final sketch's Frobenius norm is non negative, we are guarantied that no direction in space is reduced by `too much'. 
This intuition exact below.
As a remark, when presented with and `item indicator' matrix \FD~exactly mimics \FI.













\section{The algorithm}\label{thealg}

\begin{algorithm} 
\caption{\FD}
\label{alg}
\begin{algorithmic}
\STATE {\bf Input:}  
\STATE 
\STATE  all zeros matrix   
\FOR{}
	\STATE   \hfill \#  denotes the 'th row of 
	\STATE   \hfill \# ,  \;
	\STATE \hfill \# ,  \;
	\STATE   \hfill \# Only for proof notation
	\STATE  \hfill \#  the least singular value of 
	\STATE  \hfill \#  is an  identity matrix
	\STATE  \hfill \# Here  contains zeros since 
\ENDFOR
\STATE {\bf Return:}  
\end{algorithmic}
\end{algorithm}

\begin{claim}
Let  be the result of applying Algorithm~\ref{alg} to matrix  with prescribed precision parameter  then:
 
\noindent Or alternatively:

\end{claim}

\begin{proof}
We begin by obtaining the value of  by computing the Frobenius norm of .
Let ,  and  be the values of ,  and  after the main loop in the algorithm is executed  times.
For example,  is an all zeros matrix and  is the returned sketch matrix.

The reason that  is because  is, up to a unitary left rotation, a matrix which contains both  and . Remember that the last row of , which  replaced, contains only zero values.
We gain that  which we use shortly.
Now, let us compute the value of  for a vector . 

The first transition is due to the fact that .
The second transition is correct because .
Substituting that ,  and  completes the claim.
\end{proof}

\subsection{Running time}\label{run}
Let  stand for the number of operations required to obtain the Singular Value Decomposition of an  by  matrix.
The worst case update time of \FD is  which is also  operations per incoming vector. 
This is because the execution of the main loop is dominated by computating  the sketch .

However, there is no reason to compute the  of  in each and every iteration.
In Algorithm \ref{alg}, consider replacing the statement 
 with 
for some  and assume for simplicity that  is integer.
In every computation of the  the algorithm nullifies  rows of the sketch.
Therefore, the next  rows it receives can be places in zero valued rows.
Consequently, the  of the sketch is computed only once every  input rows.
This gives a total running time of  which is amortized  per row.
The proof above carries over almost without a change. The resulting approximation is slightly weakened though.
The modified algorithm only guaranties that .


\begin{remark}
From a theoretical stand point  can be reduced using fast matrix multiplication.
Let  denote the smallest scalar such that multiplying two  matrices requires  operations.
To compute the  of  we first use fast matrix multiplication to compute  in time .
Then we compute  which requires  operations.
Finally, we compute the right singular vectors of  by  which, using fast matrix multiplication, requires . This reduces the theoretical computation time of the  to . 
Note that  \cite{CohnKSU05}.
Although computing the  in this manner is numerically unstable and generally recommended against, 
in this case it might be beneficial.
Due to the algorithm's relatively weak approximation guaranty the accuracy loss incurred by squaring the matrix condition number might not be meaningful.
Moreover, there is no need for the  procedure to converge to machine precision. 

\end{remark}

\subsection{Parallelization and sketching sketches}
A convenient property of this sketching technique is that it allows for combining sketches.
In other words, let  and  denote two halves of a larger matrix . 
Also, let  and  be the sketches computed by the above technique for  and  respectively.
Now let the final sketch, , be the sketch of a matrix   which contains both  and .
It still holds that .
To see this we compute  for a test vector .

Here we use the fact that  for  which is a consequence of the derivation above.
This property is especially useful when the matrix (or data) is distributed across many machines which is often the case in modern large scale data.

\subsection{Connection to matrix low rank approximation}
Low rank approximation of matrices is a well studied problem.
The goal is to obtain a small matrix  containing  columns 
which contains in its columns space a matrix  of rank  such that .
Here,  is the best rank  approximation of  and  is either  (spectral norm) or  (Frobenius norm). 
It is difficult to compare our algorithm to this line of work since the types of bounds sought are qualitatively different.
We remark, however, that it is possible to use \FD~to produce a low rank approximation result.\\
\noindent {\bf Lamma~ from \cite{DrineasKannan2003} (modified)}.
Let  denote the projection matrix on the left  singular vectors of  corresponding to its largest singular values.
Then the following holds  where  is the 'th
singular value of .

Therefore, if   we have that 
which is a  approximation to the optimal solution.
Letting the sketch  maintain  ensures that this is the case.
Since  this is asymptotically inferior to the space requirement of \cite{BoutsidisDrineasMagdon2011}. That said, if  this is also optimal due to \cite{ClarksonWoodruff2009}.  


\section{Experiments}\label{experiments}

We compere \FD~to five different techniques. 
The first two constitute a brute force and a na\"ive baseline. 
The other three are common algorithms which are commonly used in practice.
Namely: sampling, hashing, and random projection. 
These produce sketch matrices  such that . 
The tested methods are limited in storage to an  sketch matrix  and additional auxiliary variables in  space.
This is with the exception of the brute force algorithm.
For a given input matrix  we compare the methods' computational efficiency and resulting sketch accuracy.
The computational efficiency is taken as the time required to produce  from the stream of 's rows. 
The accuracy of a sketch matrix  is measured by .

\noindent{\bf Brute Force:} the brute force approach produces the optimal rank  approximation of .
It explicitly computes the matrix  by aggregating the outer products of the rows of . 
The final `sketch' consists of the top  right singular vectors and 
values (square rooted) of  which are obtained by computing its .
The update time per row in  is  and space requirement is .

\noindent{\bf Na\"ive:} upon receiving a row in  the na\"ive method does nothing. The sketch it returns is an all zeros  by  matrix.
This baseline is important for two reasons. 
First, it can actually be more accurate than random methods due to under sampling scaling issues.
Second, although it does not perform any computation is does incur computation overheads and I/O exactly like the other methods.
It is therefore an important benchmark in both accuracy and running time measurements.

\noindent {\bf Sampling:} each row in the sketch matrix  is chosen i.i.d. from  and rescaled.
More accurately, each row  takes the value  with probability .
The space it requires is  in the worst case but it can be much lower if the chosen rows are sparse.
Here this is implemented as  independent reservoir samplers, each sampling one row according to the distribution.
The update running time is therefore,  per row in .

\noindent {\bf Hashing:} The matrix  is generated by adding or subtracting the rows of  from random rows of .
More accurately,  is initialized to be an  by  all zeros matrix. 
Then, when processing  we perform .
Here  and  are perfect hash functions. 
There is no harm in assuming such functions exist since complete randomness is na\"ively possible without dominating either space or running time.
This method is often used in practice by the machine learning community and is referred to as `feature hashing' or `hashing trick'  \cite{WeinbergerDLSA2009}.

\noindent {\bf Random Projection:} 
The matrix  is equivalent to the matrix  where  is an  by  matrix such that  uniformly. 
Since  is a random projection matrix \cite{Achlioptas01}  contains the  columns of  randomly projected from dimension  to dimension . 
This is easily computed in a streaming fashion while requiring at most  space and  operation per row updated.
For proofs of correctness and usage see \cite{PapadimitriouTRV1998}\cite{Vempala2004}\cite{Sarlos06}\cite{tygert07PNAS}.

\noindent {\bf \FD:} This indicates the modified algorithm described in Section~\ref{run} with . 
So, while it requires a sketch matrix of size  it night actually return a sketch of rank .
Moreover, it only guaranties that .
The benefit, however, is that its amortized running time is  per row.



The generated input matrices  contains  dimensional signal and  dimension noise. 
More accurately .
The signal coefficients matrix  is such that  i.i.d.
The diagonal matrix  is  which gives linearly diminishing signal singular values. 
The signal row space matrix  contains a random  dimensional subspace in , for clarity, .
The matrix  is exactly rank  and constitutes the signal we wish to recover. 
The matrix  contributes additive Gaussian noise . 
Due to \cite{Vershynin08prodOfRandMatrices}, the spectral norms of  and  are expected to be the same up to some constant.
Experimentally, this constant is close to .
Therefore, when the signal to noise ratio  is close to (or less than)  we cannot expect to approximate  since the noise dominates the signal.
On the other hand, when  the spectral norm is dominated by the signal which is therefore recoverable.
As a remark, note that the Frobenius norm of  is dominated by the noise for .

The values used in the experiments are , , , , .
Each method produced a sketch for each matrix, , which is generated according to every parameter combination.
Each resulting sketch  was measured for accuracy which is defined as .
The running time for producing each sketch by the different methods was also measured.
The entire experiment was repeated  times and the reported results are median values of these independent executions.
The experiments were conducted on a FreeBSD machine with 50GB RAM, and 12MB cache using a single Intel(R) Xeon(R) X5650 CPU.  
Example results are plotted and explained in Figures~\ref{diff_two_vs_ell}, \ref{running_time} and  \ref{diff_two_vs_snr}.

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\myfigurewidth\textwidth]{diff_two_vs_ell.pdf}
\caption{Accuracy vs. sketch size. The -axis indicates the accuracy of the sketches.
If a method returns a sketch matrix  the accuracy is measured by .
The size of the sketch is fixed for all methods and is .
The value of  is indicated on the -axis.
The form of the input matrix is explained in Section~\ref{experiments}. 
Here the signal dimension is  and the signal to noise ratio is .
There are a few interesting observations here. First, all three random techniques actually perform worse than
na\"ive for small sketch sizes. This is not the case with \FD. 
Second, the three random techniques perform equally well. This might be a result of the chosen input.
Nevertheless, practitioners should consider these methods as comparable alternatives.
The sketching technique performs significantly better than all three.
The curve indicated by ``\FD~Bound" plots the accuracy guarantied by \FD. 
While the worst case bound for \FD~is consistently better than the three competing techniques
it is still far from being tight for \FD~for large sketch sizes. 
Notice that \FD~reaches its best result already at .
This is because the signal dimension is  and \FD is implemented with parameter  (see Section~\ref{run}).
}
\label{diff_two_vs_ell}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\myfigurewidth\textwidth]{running_time.pdf}
\caption{Running time in seconds vs. sketch size. 
Each method produces a sketch matrix  of size  for a dense  matrix. 
The value of  is indicated by the -axis. 
The total amount of computation time required to produce the sketch is indicated on the -axis in seconds.
The brute force method computes the complete  of  and therefore its running time is independent of .
Note that Hashing is almost as fast as the Na\"ive method and independent of  which is expected.
The rest of the methods exhibit a linear dependence on  which is also expected.
Surprisingly though, sketching is more computationally efficient than random projections although both require  operations per input row.
It is important to stress that the implementations above are not very well optimized. 
Different implementations might lead to slightly different results.
}
\label{running_time}
\end{center}
\end{figure}


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\myfigurewidth\textwidth]{diff_two_vs_snr.pdf}
\caption{Accuracy vs. signal to noise ratio. Here, all the sketches are of the same size .
The sketches accuracies are measured as before and indicated on the -axis. 
The -axis gives the value of  the signal to noise ratio.
Here the signal dimension is  and thus the brute force approach, 
which gives the best rank  approximation of , should theoretically recover  almost exactly.
Note, however, that when  the best rank  approximation of the matrix is quite poor.
That means that spectrum of  is dominated by the noise. 
As  grows, the noise diminishes and the rows of  become more concentrated around the  dimensional signal row space.
Note that all the sketching techniques' results improve when the noise diminishes, as expected.  
}
\label{diff_two_vs_snr}
\end{center}
\end{figure}


\section{Discussion}
This paper draws upon a surprising similarity between two problems, the item frequency approximation problem and the matrix sketching problem.
It seems that, in general, solutions to the first can be modified to solve the second but incur 
an additional factor of  in both running time and space requirement. 
This is true, for example, about sampling.
It is also the case for the memory footprint of \FI~which is  while for \FD~it is . 
But, the update time of \FI~is  and that of \FD~is . 
It is natural to seek a modified algorithm which exhibits an  update time.
Another question is whether more advanced algorithms for fining frequent items in streams could also be carried over.
A good candidate is the {\it Count Sketch} algorithm \cite{Charikar2002}. 
Alas, it depends on item hashing in a way which does not naturally translate to the matrix sketching domain.


\vspace{.5cm}
\noindent {\bf Acknowledgments:} The author truly thanks Petros Drineas, Jelani Nelson, Nir Ailon, Zohar Karnin, and Yoel Shkolnisky for very helpful discussions and pointers.
 
\pagebreak
\bibliographystyle{unsrt}
\bibliography{simpleMatrixSketching}

\end{document}