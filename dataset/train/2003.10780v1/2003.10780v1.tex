\section{Experiments}\label{sExps}

\paragraph{Datasets.}
We evaluate and ablate our approach on six datasets of various scales, ranging from the manually created long-tailed CIFAR-10 and CIFAR-100~\cite{CBLoss},  ImageNet-LT, and Places-LT~\cite{OLTR}, to the naturally long-tailed iNaturalist 2017~\cite{inaturalist2017} and 2018~\cite{inaturalist}. Following~\cite{CBLoss}, \cut{Ranking all classes of a dataset by their sizes non-increasingly,} we define the \emph{imbalance factor (IF)} of a dataset as the class size of the first head class divided by the size of the last tail class.

\begin{table*}
\centering
\small
\caption {\label{tab:datasets} Overview of the six datasets used in our experiments. (IF stands for the imbalance factor)}
\vspace{2pt}
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
Dataset & \# Classes & IF & \# Train.\ img.\ & Tail class size & Head class size  & \# Val.\ img.\  & \# Test img.\\ \hline
CIFAR-LT-10\cut{~\cite{Krizhevsky-CIFAR,CBLoss}} & 10 & 1.0--200.0 & 50,000--11,203 & 500--25  & 5,000 & -- & 10,000\\ \hline
CIFAR-LT-100\cut{~\cite{Krizhevsky-CIFAR,CBLoss}} & 100 & 1.0--200.0 & 50,000--9,502  & 500--2 & 500 & -- & 10,000 \\ \hline
iNat 2017\cut{~\cite{inaturalist2017}} & 5,089 & 435.4 & 579,184 & 9 & 3,919 & 95,986 & -- \\ \hline
iNat 2018\cut{~\cite{inaturalist}} & 8,142 & 500.0 & 437,513 & 2 &  1,000 & 24,426 & -- \\ \hline
ImageNet-LT\cut{~\cite{ImageNet,OLTR}} & 1,000 & 256.0 & 115,846 & 5 & 1,280 & 20,000 & 50,000 \\ \hline
Places-LT\cut{~\cite{Places2,OLTR}} & 365 & 996.0 & 62,500 & 5 & 4,980 & 7,300 & 36,500 \\ \hline
\end{tabular}
\vspace{-10pt}
\end{table*}

\vspace{-5pt}
\begin{description} \setlength\itemsep{-1pt}

\item[Long-Tailed CIFAR (CIFAR-LT):] The original CIFAR-10 (CIFAR-100) dataset contains 50,000 training images and 10,000 test images of size 32x32 uniformly falling into 10 (100) classes~\cite{Krizhevsky-CIFAR}. Cui et al.~\cite{CBLoss} created long-tailed versions by randomly removing training examples. In particular, the number of examples dropped from the -th class is , where  is the original number of training examples in the class and . By varying , we arrive at six training sets, respectively, with the imbalance factors (IFs) of 200, 100, 50, 20, 10, and 1, where IF= corresponds to the original datasets. We do not change the test sets, which are balanced. We randomly select ten training images per class as our development set .

\item \textbf{ImageNet-LT}: In spirit similar to the long-tailed CIFAR datasets, Liu et al.~\cite{OLTR} introduced a long-tailed version of ImageNet-2012~\cite{ImageNet} called ImageNet-LT. It is created by firstly sampling the class sizes from a Pareto distribution with the power value , followed by sampling the corresponding number of images for each class. The resultant dataset has 115.8K training images in 1,000 classes, and its imbalance factor is 1280/5. The authors have also provided a validation set with 20 images per class, from which we sample ten images to construct our development set . The original balanced ImageNet-2012 validation set is used as the test set (50 images per class). 

\item \textbf{Places-LT}: Liu et al.~\cite{OLTR} have also created a Places-LT dataset by sampling from Places-2~\cite{Places2} using the same strategy as above. It contains 62.5K training images from 365 classes with an imbalance factor 4980/5. This large imbalance factor indicates that it is more challenging than ImageNet-LT. Places-LT has 20 (100) validation (test) images per class. Our development set  contains ten images per class randomly selected from the validation set.


\item \textbf{iNaturalist (iNat) 2017 and 2018}: The iNat 2017~\cite{inaturalist2017} and 2018~\cite{inaturalist} are real-world fine-grained visual recognition datasets that naturally exhibit long-tailed class distributions. iNat 2017 (2018) consists of 579,184 (435,713) training images in 5,089 (8,142) classes, and its imbalance factor is 3919/9 (1000/2). We use the official validation sets to test our approach. We select five (two) images per class from the training set of iNat 2017 (2018) for the development set.
\vspace{-5pt}
\end{description}

Table~\ref{tab:datasets} gives an overview of the six datasets used in the following experiments. 

\vspace{-10pt}
\paragraph{Evaluation Metrics.}
As the test sets are all balanced, we simply use the top- error as the evaluation metric. We report results for .




\subsection{Object recognition with CIFAR-LT} \label{sec-exp-cifar}
We run both comparison experiments and ablation studies with CIFAR-LT-10 and CIFAR-LT-100. We use ResNet-32~\cite{Resnet} in the experiments.

\vspace{-10pt}
\paragraph{Competing methods.}
We compare our approach to the following competing ones.

\vspace{-10pt}
\begin{itemize}   \setlength\itemsep{-3pt}

\item \textbf{Cross-entropy training.} This is the baseline that trains ResNet-32 using the vanilla cross-entropy loss.

\item \textbf{Class-balanced loss~\cite{CBLoss}.}
It weighs the conventional losses by class-wise weights, which are estimated based on effective numbers. We apply this class-balanced weighting to three different losses: cross-entropy, the focal loss~\cite{Focalloss}, and the recently proposed label-distribution-aware margin loss~\cite{LDAM}.

\item \textbf{Focal loss~\cite{Focalloss}.} The focal loss can be understood as a smooth version of hard example mining. It does not directly tackle the long-tailed recognition problem. However, it can penalize the examples of tail classes more than those of the head classes if the network is biased toward the head classes during training. 


\item \textbf{Label-distribution-aware margin loss~\cite{LDAM}.} It dynamically tunes the margins between classes according to their degrees of dominance in the training set. 

\item \textbf{Class-balanced fine-tuning~\cite{Cui-Finetune}.} The main idea is to first train the neural network with the whole imbalanced training set and then fine-tune it on a balanced subset of the training set. 

\item \textbf{Learning to re-weight (L2RW)~\cite{L2RW}.} It weighs training examples by meta-learning. Please see Section~\ref{sec-vs-L2RW} for more discussions about L2RW and our approach.

\item \textbf{Meta-weight net~\cite{MWN}.} Similarly to L2RW, it also weighs examples by a meta-learning method except that it regresses the weights by a multilayer perceptron.

\vspace{-10pt}
\end{itemize}

\vspace{-10pt}
\paragraph{Implementation details.}
For the first two baselines, we use the code of \cite{CBLoss} to set the learning rates and other hyperparameters. We train the L2RW model using an initial learning rate of 1e-3. We decay the learning rate by 0.01 at the 160th and 180th epochs. For our approach, we use an initial learning rate of 0.1 and then also decay the learning rate at the 160th and 180th epochs by 0.01. The batch size is 100 for all experiments. We train all models on a single GPU using the stochastic gradient descent with momentum.

\begin{table*}
\centering
\caption {\label{tab:CIFAR-10} Test top-1 errors (\%) of ResNet-32 on CIFAR-LT-10 under different imbalance settings. * indicates results reported in ~\cite{MWN}.}
\vspace{2pt}
\resizebox{0.75\textwidth}{!}{\begin{tabular}{l|c|l|l|l|l|l}
\hline

\hline

\hline
Imbalance factor & 200 & \multicolumn{1}{c|}{100} & \multicolumn{1}{c|}{50} & \multicolumn{1}{c|}{20} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c}{1} \\ \hline
Cross-entropy training & 34.32 & 29.64 & 25.19 & 17.77 & 13.61 & 7.53/7.11* \\ \hline
Class-balanced cross-entropy loss~\cite{CBLoss} & 31.11 & 27.63 & 21.95 & 15.64 &13.23 & 7.53/7.11* \\ \hline

\begin{tabular}[c]{@{}l@{}}Class-balanced fine-tuning~\cite{Cui-Finetune}\\ Class-balanced fine-tuning~\cite{Cui-Finetune}*\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.76\\ 33.92\end{tabular} & \begin{tabular}[c]{@{}l@{}}28.66\\ 28.67\end{tabular} & \begin{tabular}[c]{@{}l@{}}22.56\\ 22.58\end{tabular} &
\begin{tabular}[c]{@{}l@{}}16.78\\ 13.73\end{tabular} & \begin{tabular}[c]{@{}l@{}}16.83\\ 13.58\end{tabular} &
\begin{tabular}[c]{@{}l@{}}7.08\\ \textbf{6.77}\end{tabular} \\ \hline

\begin{tabular}[c]{@{}l@{}}L2RW~\cite{L2RW}\\ L2RW~\cite{L2RW}*\end{tabular} & \begin{tabular}[c]{@{}c@{}}33.75\\ 33.49\end{tabular} & \begin{tabular}[c]{@{}l@{}}27.77\\ 25.84\end{tabular} & \begin{tabular}[c]{@{}l@{}}23.55\\ 21.07\end{tabular} &  \begin{tabular}[c]{@{}l@{}}18.65\\ 16.90\end{tabular} & \begin{tabular}[c]{@{}l@{}}17.88\\ 14.81\end{tabular} & \begin{tabular}[c]{@{}l@{}}11.60\\ 10.75\end{tabular} \\ \hline


Meta-weight net~\cite{MWN} & 32.8 & 26.43 & 20.9 & 15.55 & 12.45 & 7.19  \\ \hline



{\bf Ours with cross-entropy loss} & \multicolumn{1}{l|}{\bf 29.34} & {\bf 23.59} & {\bf 19.49} & \textbf{13.54} & \textbf{11.15}  & 7.21 \\ \hline 
 






\hline 

\hline

Focal loss~\cite{Focalloss} & 34.71  & 29.62 & 23.29
 & 17.24 & 13.34 & 6.97 \\ \hline

Class-balanced focal Loss~\cite{CBLoss} & 31.85  & 25.43 & 20.78 & 16.22 & 12.52 & 6.97 \\ \hline

{\bf Ours with focal Loss} & {\bf 25.57} & {\bf 21.1} & \textbf{17.12}  & {\bf 13.9} & {\bf 11.63} & 7.19 \\ \hline

\hline 

\hline




LDAM loss~\cite{LDAM} (results reported in paper) & - & 26.65 & - & - & 13.04 & 11.37 \\ \hline


LDAM-DRW~\cite{LDAM} (results reported in paper) & - & 22.97 & - & - & \textbf{11.84}  & - \\ \hline


{\bf Ours with LDAM loss} & \textbf{22.77} & \textbf{20.0} & {\bf 17.77}  & {\bf 15.63} & 12.6 & {\bf 10.29} \\ \hline

 
\end{tabular}
}\vspace{-5pt}
\end{table*}





\begin{table*}
\centering
\caption {\label{tab:CIFAR-100} Test top-1 errors (\%) of ResNet-32 on CIFAR-LT-100 under different imbalance settings. * indicates results reported in ~\cite{MWN}.}
\vspace{2pt}

\resizebox{0.75\textwidth}{!}{\begin{tabular}{l|c|l|l|l|l|l}
\hline

\hline

\hline
Imbalance factor & 200 & \multicolumn{1}{c|}{100} & \multicolumn{1}{c|}{50} & \multicolumn{1}{c|}{20} & \multicolumn{1}{c|}{10} & \multicolumn{1}{c}{1} \\ \hline
Cross-entropy training & 65.16  & 61.68 & 56.15
 &48.86 &44.29 & 29.50 \\ \hline
Class-balanced cross-entropy loss~\cite{CBLoss} & 64.30 &61.44  & 55.45 &48.47  & 42.88 & 29.50 \\ \hline

\begin{tabular}[c]{@{}l@{}}Class-balanced fine-tuning~\cite{Cui-Finetune}\\ Class-balanced fine-tuning~\cite{Cui-Finetune}*\end{tabular} & 
\begin{tabular}[c]{@{}c@{}}61.34\\61.78\end{tabular} & \begin{tabular}[c]{@{}l@{}}58.5\\58.17 \end{tabular} & \begin{tabular}[c]{@{}l@{}}53.78\\53.60 \end{tabular} &
\begin{tabular}[c]{@{}l@{}}47.70\\47.89 \end{tabular} &
\begin{tabular}[c]{@{}l@{}}42.43\\42.56 \end{tabular} & \begin{tabular}[c]{@{}l@{}}29.37\\29.28 \end{tabular} \\ \hline

\begin{tabular}[c]{@{}l@{}}L2RW~\cite{L2RW}\\ L2RW~\cite{L2RW}*\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.00\\66.62 \end{tabular} & \begin{tabular}[c]{@{}l@{}}61.10\\59.77 \end{tabular} & \begin{tabular}[c]{@{}l@{}}56.83\\55.56 \end{tabular} &
\begin{tabular}[c]{@{}l@{}}49.25\\48.36 \end{tabular}& 
\begin{tabular}[c]{@{}l@{}}47.88\\46.27 \end{tabular}&
\begin{tabular}[c]{@{}l@{}}36.42\\35.89 \end{tabular} \\ \hline

Meta-weight net~\cite{MWN} & 63.38 & 58.39  & 54.34  &46.96  & 41.09 & 29.9 \\ \hline



{\bf Ours with cross-entropy loss} & \multicolumn{1}{l|}{\bf 60.69} & {\bf 56.65} & { \bf 51.47} & \textbf{44.38} & \textbf{40.42} & \textbf{28.14}
\\ \hline

\hline 

\hline


Focal Loss~\cite{Focalloss} & 64.38  & 61.59  & 55.68
 & 48.05 & 44.22 &  \textbf{28.85} \\ \hline

Class-balanced focal Loss~\cite{CBLoss} & 63.77 & 60.40  & 54.79 & 47.41 & 42.01 & \textbf{28.85} \\ \hline

{\bf Ours with focal loss} & \textbf{60.66} & \textbf{55.3} & \textbf{49.92}  & \textbf{44.27} & \textbf{40.41} & 29.15 \\ \hline

\hline

\hline 



LDAM Loss~\cite{LDAM} (results reported in paper) &  - &  60.40  & -  & -  &  43.09 & - \\ \hline


LDAM-DRW~\cite{LDAM} (results reported in paper)& - & 57.96  & - & - & \textbf{41.29}  & - \\ \hline


{\bf Ours with LDAM loss} & {\bf 60.47} & \textbf{55.92} & \textbf{50.84} & \textbf{47.62} & 42.0 & - \\ \hline


\end{tabular}
}\vspace{-10pt}
\end{table*}

\vspace{-10pt}
\paragraph{Results.}
Table~\ref{tab:CIFAR-10} shows the classification errors of ResNet-32 on the long-tailed CIFAR-10 with different imbalance factors. We group the competing methods into three sessions according to which basic loss they use (cross-entropy, focal~\cite{Focalloss}, or LDAM~\cite{LDAM}). We test our approach with all three losses. We can see that our method outperforms the competing ones in each session by notable margins. Although the focal loss and the LDAM loss already have the capacity of mitigating the long-tailed issue, respectively, by penalizing hard examples and by distribution-aware margins, our method can further boost their performances. In general, the advantages of our approach over existing ones become more significant as the imbalance factor increases. When the dataset is balanced (the last column), our approach does not hurt the performance of vanilla losses compared to L2RW. We can draw about the same observations as above for the long-tailed CIFAR-100 from Table~\ref{tab:CIFAR-100}.




\begin{figure*}
    \centering
    \subfigure{
    \includegraphics[width=0.3\textwidth]{plots/cm/CM_cnt_200_im_newest.pdf}
    }
    \centering
    \subfigure{
    \includegraphics[width=0.3\textwidth]{plots/cm/L2RW_cm_im_200_new.pdf}
    }
    \centering
    \subfigure{
    \includegraphics[width=0.3\textwidth]{plots/cm/Our_cifar10_im_200_new_v1.pdf}
    }
    \caption{Confusion matrices by the cross-entropy training, L2RW, and our method on CIFAR-LT-10 (the imbalance factor is 200).}
    \label{fig:confusion_matrices}
    \vspace{-5pt}
\end{figure*}

\vspace{-10pt}
\paragraph{Where does our approach work?} Figure~\ref{fig:confusion_matrices} presents three confusion matrices respectively by the models of the cross-entropy training, L2RW, and our method on CIFAR-LT-10. The imbalance factor is 200. Compared with the cross-entropy model, L2RW  improves the accuracies on the tail classes and yet sacrifices the accuracies for the head classes.  In contrast, ours maintains about the same performance as the cross-entropy model on the head classes and meanwhile significantly improves the accuracies for the last five tail classes.



\begin{figure}
   \centering
    \begin{tabular}{ll}
    \includegraphics[width=0.23\textwidth]{plots/analysis/eps_weights_im_100_new_v1.pdf}
    &
    \includegraphics[width=0.23\textwidth]{plots/analysis/eps_weights_im_10_new_v1.pdf}
    \end{tabular}
    \vspace{-7pt}
    \caption{Mean conditional weights  within each class vs.\ training epochs on CIFAR-LT-10 (left: IF = 100; right: IF = 10).}
    \label{fig:eps_analysis}
\end{figure}


\vspace{-10pt}
\paragraph{What are the learned conditional weights?} We are interested in examining the conditional weights  for each class throughout the training. For a visualization purpose, we average them within each class. Figure~\ref{fig:eps_analysis} demonstrates how they change over the last 20 epochs for the 1st, 4th, 7th, and 10th classes of CIFAR-LT-10. The two panels correspond to the imbalance factors of 100 and 10, respectively. Interestingly, the learned conditional weights of the tail classes are more prominent than those of the head classes in most epochs. Moreover, the conditional weights of the two head classes (the 1st and 4th) are even below 0 at certain epochs. Such results verify our intuition that the scarce training examples of the tail classes deserve more attention in training to make the neural network perform in a balanced fashion at the test phase. 



\begin{table}
\centering
\caption {\label{tab:iNat-LT} Classification errors on iNat 2017 and 2018. (*results reported in paper. CE=cross-entropy, CB=class-balanced)}
\vspace{2pt}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{l|c|c||c|c}
\hline

\hline

\hline
Dataset & \multicolumn{2}{c||}{iNat 2017} & \multicolumn{2}{c}{\scriptsize iNat 2018} \\ \hline
 Method & Top-1 & Top-3/5 & \scriptsize{Top-1} & {\scriptsize Top-3/5}  \\ \hline
CE & 43.49 & 26.60/21.00 & \scriptsize{36.20} & \scriptsize{19.40/15.85}   \\ \hline
CB CE~\cite{CBLoss} & 42.59
 & 25.92/20.60 & \scriptsize{34.69}  & \scriptsize{19.22/15.83} \\ \hline
 
 
{\bf Ours, CE} & {\bf 40.62} & \textbf{23.70/18.40} & \scriptsize{\bf 32.45} & \scriptsize{\textbf{18.02/13.83}}  \\ \hline

\hline

\hline

CB focal~\cite{CBLoss}* & 41.92 & --/20.92 
& \scriptsize{38.88}   & \scriptsize{--/18.97} \\ \hline 




LDAM~\cite{LDAM}* & -- & -- &  \scriptsize{35.42}  & \scriptsize{--/16.48} \\ \hline 


LDAM-drw* & -- & --
&  \scriptsize{32.00}  & \scriptsize{--/14.82} \\ \hline 






cRT~\cite{kang2019decoupling}* & -- & --
&  \scriptsize{34.8}  & \scriptsize{--} \\ \hline 
cRT+epochs* & -- & --
&  \scriptsize{32.4}  & \scriptsize{--} \\ \hline 




\end{tabular}
}\vspace{-12pt}
\end{table}



\begin{table}
\centering
\caption {\label{tab:Ablation_CIFAR-10} Ablation study of our approach by using the cross-entropy loss on CIFAR-LT-10. The results are test top-1 errors\%.}
\vspace{2pt}
\resizebox{0.45\textwidth}{!}
{\begin{tabular}{l|c|l|l}
\hline


Imbalance factor & 100 & \multicolumn{1}{c|}{50} & \multicolumn{1}{c}{20} \\ \hline











L2RW~\cite{L2RW} & 27.77  & 23.55  & 18.65 \\ \hline
 
L2RW, pre-training & 25.96  & 22.04  & 15.67 \\ \hline

L2RW, pre-training, init.\ by  & 26.26  & 22.50  & 17.44 \\ \hline

L2RW, pre-training,  & 24.54  & 20.47  & 14.38 \\ \hline

Ours  & \multicolumn{1}{l|}{\bf 23.59} & {\bf 19.49} & {\bf 13.54} \\ \hline

\hline

\hline

Ours updating  & 25.42  & 20.13 & 15.62  \\ \hline

Class-balanced~\cite{CBLoss} & 27.63  & 21.95 & 15.64 \\ \hline
\end{tabular}
}\vspace{-15pt}
\end{table}



\vspace{-10pt}
\paragraph{Ablation study: ours vs.\ L2RW.}
Our overall algorithm differs from L2RW mainly in four ways: 1) pre-training the network, 2) initializing the weights by \textit{a priori} knowledge, 3) two-component weights, and estimating the class-wise components by a separate algorithm~\cite{CBLoss}, and 4) no clipping or normalization of the weights. Table~\ref{tab:Ablation_CIFAR-10} examines these components by applying them one after another to L2RW. First, pre-training the neural network boosts the performance of the vanilla L2RW. Second, if we initialize the sample weights by our class-wise weights , the errors increase a little probably because the clipping and normalization steps in L2RW require more careful initialization to the sample weights. Third, if we replace the sample weights by our two-component weights, we can bring the performance of L2RW closer to ours. Finally, after we remove the clipping and normalization, we arrive at our algorithm, which gives rise to the best results among all variations. 

\vspace{-10pt}
\paragraph{Ablation study: the two-component weights.}
By Table~\ref{tab:Ablation_CIFAR-10}, we also highlight the importance of the two-component weights  motivated from our domain adaptation point of view to the long-tailed visual recognition. First of all, they benefit L2RW (comparing ``L2RW, pre-training, '' with ``L2RW, pre-training'' in Table~\ref{tab:Ablation_CIFAR-10}). Besides, they are also vital for our approach. If we drop the class-wise weights, our results would be about the same as L2RW with pre-training. If we drop the conditional weights and meta-learn the class-wise weights (cf.\ ``Ours updating ''), the errors become larger than our original algorithm. Nonetheless, the results are better than the class-balanced training (cf.\ last row in Table~\ref{tab:Ablation_CIFAR-10}), implying that the learned class-wise weights give rise to better models than the effective-number-based class-wise weights~\cite{CBLoss}. 






\subsection{Object recognition with iNat 2017 and 2018}
We use ResNet-50~\cite{Resnet} as the backbone network for the iNat 2017 and 2018 datasets. The networks are pre-trained on ImageNet for iNat 2017 and on ImageNet plus iNat 2017 for iNat 2018.  We experiment with the mini-batch size of 64 and the learning rate of 0.01. We train all the models using the stochastic gradient descent with momentum. For the meta-learning stage of our approach, we switch to a small learning rate, 0.001. 


Table~\ref{tab:iNat-LT} shows the results of our two-component weighting applied to the cross-entropy loss. We shrink the text size for iNat 2018 to signify that we advocate experiments with iNat \textit{2017} instead  because there are only three validation/test images per class in iNat 2018 (cf.\  Table~\ref{tab:datasets}). Our approach boosts the cross-entropy training by about 2\% more than the class-balanced weighting~\cite{CBLoss} does. As we have reported similar effects for the focal loss and the LDAM loss on CIFAR-LT with extensive experiments, we do not run them on the large-scale iNat datasets to save computation costs. Nonetheless, we include the results reported in the literature of the focal loss, LDAM loss, and a classifier re-training method~\cite{kang2019decoupling}, which was published after we submitted the work to CVPR 2020.



\begin{table}
\centering

\caption {\label{tab:largescale-LT} Classification errors on ImageNet-LT and Places-LT. (*reported in paper. CE=cross-entropy, CB=class-balanced)}
\vspace{2pt}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{l|c|c||c|c}
\hline
Dataset & \multicolumn{2}{c||}{ImageNet-LT} & \multicolumn{2}{c}{Places-LT} \\ \hline
Method & Top-1 & Top-3/5 & Top-1 & Top-3/5 \\ \hline
CE & 74.74 &  61.35/52.12 & 73.00 & 52.05/41.44   \\ \hline
CB CE~\cite{CBLoss} & 73.41 & 59.22/50.49  & 71.14  & 51.58/41.96 \\ \hline
{\bf Ours, CE} & {\bf 70.10} & {\bf 53.29/45.18}
 & {\bf 69.20} & \textbf{47.95/38.00} \\ \hline
 













 
\end{tabular}
}\vspace{-5pt}
\end{table}




\subsection{Experiments with ImageNet-LT and Places-LT}
Following Liu et al.'s experiment setup~\cite{OLTR}, we employ ResNet-32 and ResNet-152 for the experiments on ImageNet-LT and Places-LT, respectively. For ImageNet-LT, we adopt an initial learning rate of 0.1 and decay it by 0.1 after every 35 epochs. For Places-LT, the initial learning rate is 0.01 and is decayed by 0.1 every 10 epochs. For our own approach, we switch from the cross-entropy training to the meta-learning stage when the first decay of the learning rate happens. The mini-batch size is 64, and the optimizer is stochastic gradient descent with momentum. 

\vspace{-10pt}
\paragraph{Results.}
Table~\ref{tab:largescale-LT} shows that the class-balanced training improves the vanilla cross-entropy results, and our two-component weighting further boosts the results. We expect the same observation with the focal and LDAM losses. Finally, we find another improvement by updating the classification layers only in the meta-learning stage. We arrive at 62.90\% top-1 error (39.86/29.87\% top-3/5 error) on Places-LT, which is on par with 64.1\% by OLTR~\cite{OLTR} or 63.3\% by cRT~\cite{kang2019decoupling}, while noting that our two-component weighting can be conveniently applied to both OLTR and cRT. 



















































