





\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[preprint]{cvpr}      \usepackage{graphicx}











\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{adjustbox}

\usepackage{bbding}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{bbm}
\usepackage{courier}
\usepackage{caption}
\usepackage{float}

\newcommand{\leizhang}[1]{\textcolor{red}{[leizhang: #1]}}






\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{multicol}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{5306} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation}

\author{\textbf{Feng Li\thanks{Equal contribution.}\hspace{.4mm} \thanks{This work was done when Feng Li and Hao Zhang were interns at IDEA. }\hspace{1.5mm}, ~Hao Zhang, ~Huaizhe Xu, ~Shilong Liu},\\ \textbf{~Lei Zhang\thanks{Corresponding author.}\hspace{1.mm}, ~Lionel M. Ni, ~Heung-Yeung Shum} \\
The Hong Kong University of Science and Technology. \\
Dept. of CST., BNRist Center, Institute for AI, Tsinghua University. \\
International Digital Economy Academy (IDEA). \\
The Hong Kong University of Science and Technology (Guangzhou).\\
\texttt{\{fliay,hzhangcx,hxubr\}@connect.ust.hk} \\
\texttt{\{liusl20\}@mails.tsinghua.edu.cn} \\
\texttt{\{leizhang\}@idea.edu.cn} \\
\texttt{\{ni,hshum\}@ust.hk} \\
}
\maketitle


\begin{abstract}
In this paper we present Mask DINO, a unified object detection and segmentation framework. 
Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). 
It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary masks. 
Some key components in DINO are extended for segmentation through a shared architecture and training process. 
Mask DINO is simple, efficient, and scalable, and it can benefit from joint large-scale detection and segmentation datasets. 
Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. 
Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K) among models under one billion parameters. 
Code is available at \url{https://github.com/IDEA-Research/MaskDINO}.

\end{abstract}


\section{Introduction}
Object detection and image segmentation are fundamental tasks in computer vision. Both tasks are concerned with localizing objects of interest in an image but have different levels of focus. Object detection is to localize objects of interest and predict their bounding boxes and category labels, whereas image segmentation focuses on pixel-level grouping of different semantics. Moreover, image segmentation encompasses various tasks including instance segmentation, panoptic segmentation, and semantic segmentation with respect to different semantics, e.g., instance or category membership, foreground or background category. 

Remarkable progress has been achieved by classical convolution-based algorithms developed for these tasks with specialized architectures, such as Faster RCNN~\cite{ren2015faster} for object detection, Mask RCNN~\cite{he2017mask} for instance segmentation, and FCN~\cite{long2015fully} for semantic segmentation. Although these methods are conceptually simple and effective, they are tailored for specialized tasks and lack the generalization ability to address other tasks. The ambition to bridge different tasks gives rise to more advanced methods like HTC~\cite{chen2019hybrid} for object detection and instance segmentation and Panoptic FPN~\cite{kirillov2019panoptic}, K-net~\cite{zhang2021k} for instance, panoptic, and semantic segmentation. 
Task unification not only helps simplify algorithm development but also brings in performance improvement in multiple tasks. 



Recently, DETR-like \cite{carion2020end} models developed based on Transformers~\cite{vaswani2017attention} have achieved inspiring progress on many detection and segmentation tasks.
As an end-to-end object detector, DETR adopts a set-prediction objective and eliminates hand-crafted modules such as anchor design and non-maximum suppression.
Although DETR addresses both the object detection and panoptic segmentation tasks, its segmentation performance is still inferior to classical segmentation models. To improve the detection and segmentation performance of Transformer-based models, researchers have developed specialized models for object detection ~\cite{zhu2020deformable,liu2022dab, li2022dn, zhang2022dino}, image segmentation~\cite{zhang2021k,cheng2021maskformer, cheng2021mask2former}, instance segmentation~\cite{fang2021instances}, panoptic segmentation~\cite{qin2022pyramid}, and semantic segmentation~\cite{jain2021semask}.

Among the efforts to improve object detection, DINO~\cite{zhang2022dino} takes advantage of the dynamic anchor box formulation from DAB-DETR~\cite{liu2022dab} and query denoising training from DN-DETR~\cite{li2022dn}, and further 
achieves the SOTA result on the COCO object detection leaderboard for the first time as a DETR-like model. Similarly, for improving image segmentation, MaskFormer~\cite{cheng2021maskformer} and Mask2Former~\cite{cheng2021mask2former} propose to unify different image segmentation tasks using query-based Transformer architectures to perform mask classification. Such methods have achieved remarkable performance improvement on multiple segmentation tasks.

However, in Transformer-based models, the best-performing detection and segmentation models are still not unified, which 
prevents task and data cooperation between detection and segmentation tasks.
As an evidence, in CNN-based models, Mask-R-CNN~\cite{he2017mask} and HTC~\cite{chen2019hybrid} are still widely acknowledged as unified models that achieve mutual cooperation between detection and segmentation to achieve superior performance than specialized models.
Though we believe detection and segmentation can help each other in a unified architecture in Transformer-based models, the results of simply using DINO for segmentation and using Mask2Former for detection indicate that they can not do other tasks well, as shown in Table \ref{tab:mask2former_head} and \ref{tab:dino_head}. Moreover, trivial multi-task training can even hurt the performance of the original tasks. It naturally leads to two questions: 1) \emph{why cannot detection and segmentation tasks help each other in Transformer-based models?} and 2) \emph{is it possible to develop a unified architecture 
to replace specialized ones?}

To address these problems, we propose Mask DINO, which extends DINO with a mask prediction branch in parallel with DINO's box prediction branch. Inspired by other unified models~\cite{wang2021max,cheng2021maskformer,cheng2021mask2former} for image segmentation, we reuse content query embeddings from DINO to perform mask classification for all segmentation tasks on a high-resolution pixel embedding map (1/4 of the input image resolution) obtained from the backbone and Transformer encoder features. 
The mask branch predicts binary masks by simply dot-producting each content query embedding with the pixel embedding map. 
As DINO is a detection model for region-level regression, it is not designed for pixel-level alignment.
To better align features between detection and segmentation, we also propose three key components to boost the segmentation performance.
First, we propose a unified and enhanced query selection. It utilizes encoder dense prior by predicting masks from the top-ranked tokens to initialize mask queries as anchors. In addition, we observe that pixel-level segmentation is easier to learn in the early stage and propose to use initial masks to enhance boxes, which achieves task cooperation. Second, we propose a unified denoising training for masks to accelerate segmentation training. 
Third, we use a hybrid bipartite matching for more accurate and consistent matching from ground truth to both boxes and masks.

Mask DINO is conceptually simple and easy to implement under the DINO framework. 
To summarize, our contributions are three-fold.
    \textbf{1)} We develop a unified Transformer-based framework for both object detection and segmentation. As the framework is extended from DINO, by adding a mask prediction branch, it naturally inherits most algorithm improvements in DINO including anchor box-guided cross attention, query selection, denoising training, and even a better representation pre-trained on a large-scale detection dataset.
    \textbf{2)} We demonstrate that detection and segmentation can help each other through a shared architecture design and training method. Especially, detection can significantly help segmentation tasks, even for segmenting background "stuff" categories.
    Under the same setting with a ResNet-50 backbone, Mask DINO outperforms all existing models compared to DINO ( AP on COCO detection) and Mask2Former ( AP,  PQ, and  mIoU on COCO instance, COCO panoptic, and ADE20K semantic segmentation).
    \textbf{3)} We also show that, via a unified framework, segmentation can benefit from detection pre-training on a large-scale detection dataset. After detection pre-training on the Objects365~\cite{shao2019objects365} dataset with a SwinL~\cite{liu2021swin} backbone, Mask DINO significantly improves all segmentation tasks and achieves the best results on instance (\textbf{54.5} AP on COCO),  panoptic (\textbf{59.4} PQ on COCO), and semantic (\textbf{60.8} mIoU on ADE20K) segmentation among models under one billion parameters. 
\section{Related Work}
\noindent\textbf{Detection:} 
Mainstream detection algorithms have been dominated by convolutional neural network-based frameworks, until recently Transformer-based detectors~\cite{carion2020end,liu2022dab,li2022dn,zhang2022dino} achieve great progress. DETR~\cite{carion2020end} is the first end-to-end and query-based Transformer object detector, which adopts a set-prediction objective with bipartite matching. DAB-DETR~\cite{liu2022dab} improves DETR by formulating queries as D anchor boxes and refining predictions layer by layer. DN-DETR~\cite{li2022dn} introduces a denoising training method to accelerate convergence. 
Based on DAB-DETR and DN-DETR, DINO~\cite{zhang2022dino} proposes several new improvements on denoising and anchor refinement and achieves new SOTA results on COCO detection. Despite the inspiring progress, DETR-like detection models are not competitive for segmentation. Vanilla DETR incorporates a segmentation head in its architecture. However, its segmentation performance is inferior to specialized segmentation models and only shows the feasibility of DETR-like detection models to deal with detection and segmentation simultaneously.
\\\textbf{Segmentation:}
Segmentation mainly includes instance, semantic, and panoptic segmentation. 
Instance segmentation is to predict a mask and its corresponding category for each object instance. Semantic segmentation requires to classify each pixel including the background into different semantic categories. Panoptic segmentation~\cite{kirillov2019panoptic} unifies the instance and semantic segmentation tasks and predicts a mask for each object instance or background segment. In the past few years, researchers have developed specialized architectures for the three tasks. For example, Mask-RCNN~\cite{he2017mask} and HTC~\cite{chen2019hybrid} can only deal with instance segmentation because they predict the mask of each instance based on its box prediction. FCN~\cite{long2015fully} and U-Net~\cite{ronneberger2015u} can only perform semantic segmentation since they predict one segmentation map based on pixel-wise classification. Although models for panoptic segmentation~\cite{kirillov2019pafpn,xiong2019upsnet} unifies the above two tasks, they are usually inferior to specialized instance and semantic segmentation models. Until recently, some  image segmentation models~\cite{zhang2021k,cheng2021maskformer, cheng2021mask2former} are developed to unify the three tasks with a universal architecture. For instance, Mask2Former~\cite{cheng2021mask2former} improves MaskFormer~\cite{cheng2021maskformer} by introducing masked-attention to Transformer.
Mask2Former has a similar architecture as DETR to probe image features with learnable queries but differs in using a different segmentation branch and some specialized designs for mask prediction. However, while Mask2Former shows a great success in unifying all segmentation tasks, it leaves object detection untouched and our empirical study shows that its specialized architecture design is not suitable for predicting boxes.
\\\textbf{Unified Methods:}
As both object detection and segmentation are concerned with localizing objects, they naturally share common model architectures and visual representations. A unified framework not only helps simplify the algorithm development effort, but also allows to use both detection and segmentation data to improve representation learning. There have been several previous works to unify segmentation and detection tasks, e.g., Mask RCNN~\cite{he2017mask}, HTC~\cite{chen2019hybrid}, and DETR~\cite{carion2020end}. Mask RCNN extends Faster RCNN and pools image features from Region Of Interest (ROI) proposed by RPN.
HTC further proposes an interleaved way of predicting boxes and masks. 
However, these two models can only perform instance segmentation. DETR predicts boxes and masks together in an end-to-end manner. However, its segmentation performance largely lags behind other models. According to Table~\ref{tab:dino_head}, adding DETR's segmentation head to DINO results in inferior instance segmentation results.
How to attain mutual assistance between segmentation and detection 
has long been an important problem to solve.

\section{Mask DINO}
Mask DINO is an extension of DINO~\cite{zhang2022dino}. On top of content query embeddings, DINO has two branches for box prediction and label prediction. The boxes are dynamically updated and used to guide the deformable attention in each Transformer decoder. Mask DINO adds another branch for mask prediction and minimally extends several key components in detection to fit segmentation tasks. To better understand Mask DINO, we start by briefly reviewing DINO and then introduce Mask DINO.
\subsection{Preliminaries: DINO}
DINO is a typical DETR-like model, which is composed of a backbone, a Transformer encoder, and a Transformer decoder. The framework is shown in Fig.~\ref{fig:framework} (the blue-shaded part without red lines). Following DAB-DETR~\cite{liu2022dab}, DINO formulates each positional query in DETR as a 4D anchor box, which is dynamically updated through each decoder layer. Note that DINO uses multi-scale features with deformable attention~\cite{zhu2020deformable}. Therefore, the updated anchor boxes are also used to constrain deformable attention in a sparse and soft way. Following DN-DETR~\cite{li2022dn}, DINO adopts denoising training and further develops contrastive denoising to accelerate training convergence. Moreover, DINO proposes a mixed query selection scheme to initialize positional queries in the decoder and a look-forward-twice method to improve box gradient back-propagation.
\begin{table*}[t]
\begin{adjustbox}{width=0.99\textwidth,center}
\begin{minipage}[t]{0.55\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{c|ccc}
    \toprule
    Model & Box AP & Mask AP  \\
        \midrule
        Mask2Former &  &      \\
        \midrule
        Mask2Former + detection head&&\\
        \midrule
     DINO     &    &   \\
    
    \bottomrule
\end{tabular}
\caption{Simply adding a detection head to Mask2Former results in low detection performance.  indicates the boxes are generated from the predicted masks. The generated boxes from Mask2Former are also inferior to DINO (-4.5 AP). The models are trained for 50 epochs.
}
\label{tab:mask2former_head}
\end{minipage}\hspace{3mm}

\begin{minipage}[t]{0.65\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{c|ccc}
    \toprule
    Model & Box AP & Mask AP  \\
        \midrule
        DINO + Mask2Former segmentation head&&\\
        \midrule
     \begin{tabular}[c]{@{}c@{}}DINO + DETR segmentation head \
    m = q_{c} \otimes \mathcal{M}(\mathcal{T}(C_{b})+\mathcal{F}(C_{e})),
mean)\end{tabular} &\begin{tabular}[c]{@{}c@{}}mIoU\reported)\end{tabular}  \\
        \midrule
        Mask2Former~\cite{cheng2021mask2former} &160k& & &  &  \\
          Mask DINO (ours)    & 160k& &\fontsize{7.0pt}{\baselineskip}\selectfont{(+1.6)} &\textbf{48.7}\fontsize{7.0pt}{\baselineskip}\selectfont{(+2.2)}&\textbf{48.7}\fontsize{7.0pt}{\baselineskip}\selectfont{(+1.6)}\\  \bottomrule
    \end{tabular}
    \centering
    \captionsetup{font={large}}
    \caption{{Results for Mask DINO and Mask2Former with 100 queries using a ResNet-50 backbone on ADE20K \texttt{val}. 
We found the performance variance on this dataset is high and run three times to report both the mean and highest results for both models}.
    }
    \label{tab:sematic_ade}
\end{minipage}\hspace{10mm}
\begin{minipage}[t]{0.7\textwidth}
\makeatletter\def\@captype{table}
\centering
\large
\begin{tabular}{c|c|cccc}
    \toprule
        Model  & Iterations &\begin{tabular}[c]{@{}c@{}}mIoU\high)\end{tabular}&\begin{tabular}[c]{@{}c@{}}mIoU\reported)\end{tabular}  \\
        \midrule
        Mask2Former~\cite{cheng2021mask2former} &90k& & &   \\
          Mask DINO (ours)   & 90k &\textbf{79.8}\fontsize{7.0pt}{\baselineskip}\selectfont{(+1.1)} &\textbf{80.0}\fontsize{7.0pt}{\baselineskip}\selectfont{(+1.0)}&\textbf{80.0}\fontsize{7.0pt}{\baselineskip}\selectfont{(+0.6)}  \\
    \bottomrule
    \end{tabular}
    \centering
\captionsetup{font={large}}
    \caption{Results for Mask DINO and Mask2Former with 100 queries using a ResNet-50 backbone on Cityscapes \texttt{val}. 
We found the performance variance on this dataset is high and run three times to report both the mean and highest results for both models.
    }
    \label{tab:sematic_c}
\end{minipage}
\end{adjustbox}
\end{table*}
 \begin{table*}[t]
    \centering
    \small
        \setlength\tabcolsep{2pt}
        \footnotesize
            \renewcommand{\arraystretch}{1.3}
\begin{adjustbox}{width=0.9\textwidth,center}
    \begin{tabular}{c|c|c|c|c|cc}
\toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Params} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{\makecell{Backbone Pre-training \\ Dataset }} &  \multirow{2}{*}{\makecell{Detection Pre-training \\ Dataset }}   & \multicolumn{2}{c}{{ val}}  \\
        & & &  && \scriptsize w/o TTA& \scriptsize w/ TTA  \\
        \midrule
        \multicolumn{5}{c}{{\textbf{Instance segmentation on COCO}}}&\multicolumn{2}{c}{AP} \\
        \midrule
Mask2Former~\cite{cheng2021mask2former} & M & SwinL & IN-22K-14M &     &  &   \\
        Soft Teacher~\cite{xu2021end} & M & SwinL & IN-22K-14M & O365     & &   \\
        SwinV2-G-HTC++~\cite{liu2021swinv2} & B & SwinV2-G & IN-22K-ext-70M~\cite{liu2021swinv2} & O365    &  &  \\
        \hline
        MasK DINO(Ours) & {M} & SwinL & IN-22K-14M &  &     &   \\
MasK DINO(Ours) & {M} & SwinL & IN-22K-14M & O365 &    \fontsize{7.0pt}{\baselineskip}\selectfont{(+1.1)} &   \\
        
        \midrule
        \multicolumn{5}{c}{{\textbf{Panoptic segmentation on COCO}}}&\multicolumn{2}{c}{PQ} \\


        Panoptic SegFormer~\cite{li2021panoptic}& M & SwinL & IN-22K-14M &      & &   \\
         Mask2Former~\cite{cheng2021mask2former} & M & SwinL & IN-22K-14M &      &  &   \\
\hline
        MasK DINO (ours) & {M} & SwinL & IN-22K-14M &  &    \fontsize{7.0pt}{\baselineskip}\selectfont{(+0.6)} &   \\
MasK DINO (ours) & {M} & SwinL & IN-22K-14M & O365 &    \fontsize{7.0pt}{\baselineskip}\selectfont{(+1.6)} &   \\
        \midrule
        \multicolumn{5}{c}{{\textbf{Semantic segmentation on ADE20K}}}&\multicolumn{2}{c}{mIoU} \\
        \midrule
Mask2Former~\cite{cheng2021mask2former} & M & SwinL & IN-22K-14M  &      &  &   \\
SeMask-L MSFaPN-Mask2Former~\cite{jain2021semask} & M & SwinL-FaPN & IN-22K-14M &      & &   \\
        SwinV2-G-UperNet~\cite{liu2021swinv2} & B & SwinV2-G & IN-22K-ext-70M~\cite{liu2021swinv2} &  &    & \\
        \hline
        MasK DINO (ours) & \textbf{M} & SwinL & IN-22K-14M &  &     &   \\
        MasK DINO (ours) & \textbf{M} & SwinL & IN-22K-14M & O365 &     & \fontsize{7.0pt}{\baselineskip}\selectfont{(+0.9)}  \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\caption{Comparison of the SOTA models on three segmentation tasks. Mask DINO outperforms all existing models. "TTA" means test-time-augmentation.
``O365''  denotes the  Objects365~\cite{shao2019objects365} dataset. 
    }
    \label{tab:sota}
    \vspace{-.3cm}
\end{table*}



%
 \section{Experiments}
We conduct extensive experiments and compare with several specialized models for four popular tasks including object detection, instance, panoptic, and semantic segmentation on COCO~\cite{lin2015microsoft}, ADE20K~\cite{zhou2017scene}, and Cityscapes~\cite{cordts2016cityscapes}. For all experiments, we use batch size 16 and A100 GPUs with 40GB memory. We use a ResNet-50~\cite{he2015deep} and a SwinL~\cite{liu2021swin} backbone for our main results and SOTA model.  Under ResNet-50, we use  A100 GPUs for all tasks without extra data.
The implementation details are in Appendix \ref{sec:vis}.
\subsection{Main Results}\label{sec:main}
\noindent\textbf{Instance segmentation and object detection.} In Table ~\ref{tab:instance}, we compare Mask DINO with other instance segmentation and object detection models. Mask DINO outperforms both the specialized models such as Mask2Former~\cite{cheng2021mask2former} and DINO~\cite{zhang2022dino} and hybrid models such as HTC~\cite{chen2019hybrid} under the same setting. Especially, the instance segmentation results surpass the strong baseline Mask2Former by a large margin (+ AP and + AP)  on the 12-epoch and 50-epoch settings. Moreover, Mask DINO significantly improves the convergence speed, outperforming Mask2Former with less than half training epochs ( AP in 24 epochs). 
In addition, after using mask-enhanced box initialization, our detection performance has been significantly improved (+1.2 AP), which even outperforms DINO by \textbf{0.8} AP. These results indicate that task unification is beneficial. Without bells and whistles, we achieve the best detection and instance segmentation performance among DETR-like model with a SwinL backbone without extra data.
\begin{table*}[t]
\begin{adjustbox}{width=1.0\textwidth,center}
\begin{minipage}[t]{0.55\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{c|ccc}
    \toprule
    test layer\# & Mask DINO & Mask2Former  \\
        \midrule
        layer 0 & \fontsize{7.0pt}{\baselineskip}\selectfont{(+38.5)} &      \\
        \midrule
        layer 3&&\\
     layer 6     &    &   \\
     layer 9  &      &   \\
    
    \bottomrule
\end{tabular}
\caption{Effectiveness of our query selection for mask initialization. We evaluate the instance segmentation performance from different decoder layers in the same model after training for 50 epochs. 
}
\label{tab:dec_test}
\end{minipage}\hspace{3mm}
\begin{minipage}[t]{0.38\textwidth}
\makeatletter\def\@captype{table}
\centering
\vspace{-41pt}
\begin{tabular}{c|cc|cccc}
    \toprule
    \multirow{2}{*}{layer\#}&\multicolumn{2}{c|}{w/o ME} & \multicolumn{2}{c}{w ME} \\
          &Box&  Mask &Box&  Mask \\
        \midrule
    layer 0    &   39.6   & \underline{25.6} &39.8&\textbf{41.2}\fontsize{7.0pt}{\baselineskip}\selectfont{(+15.6)} \\
     layer 9& 46.0  & \underline{50.5} & 46.3&\textbf{51.7}\fontsize{7.0pt}{\baselineskip}\selectfont{(+1.2)}   \\
\bottomrule
\end{tabular}
\caption{Comparsion of our model with and without Mask-enhanced anchor box initialization (ME). ME enhances anchor box initialization and improves final detection performance. Trained for 50 epochs.
}
\label{tab:maskenhance}
\end{minipage}\hspace{6mm}
\begin{minipage}[t]{0.42\textwidth}
\makeatletter\def\@captype{table}
\centering
\vspace{-40.5pt}
\begin{tabular}{c|ccc}
    \toprule
   Feature scale & box AP & {mask AP}  \\
        \midrule
    single scale(1/8)     &      &   \\
     3 scales  &  &    \\
    4 scales &  &      \\
    \bottomrule
\end{tabular}
\caption{Comparison of multi-scale features for Transformer decoder under the 50-epoch setting. Both detection and segmentation benefit from more feature scales.}
\label{tab:feature_scale}
\end{minipage}\hspace{3mm}

\end{adjustbox}
\vspace{-.3cm}
\end{table*}
 \begin{table*}[t]
\begin{adjustbox}{width=1.0\textwidth,center}
\begin{minipage}[t]{0.46\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{cc|cc}
    \toprule
    \multicolumn{2}{c|}{Tasks} & \multirow{2}{*}{Box AP} & \multirow{2}{*}{Mask AP} \\
          Box&  Mask & & \\
        \midrule
    \checkmark     &      &  & \\
     & \checkmark  &  &    \\
    \checkmark     & \checkmark & \fontsize{7.0pt}{\baselineskip}\selectfont{(+0.4)} & \fontsize{7.0pt}{\baselineskip}\selectfont{(+2.7)}     \\
    \bottomrule
\end{tabular}
\caption{Task comparison under the 50-epoch setting. We train the same Mask DINO with different tasks and validate that box and mask can achieve mutual cooperation.}
\label{tab:taskhelp}
\end{minipage}\hspace{6mm}
\begin{minipage}[t]{0.48\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{c|ccc}
    \toprule
   Decoder layer\# & Box AP & {Mask AP}  \\
        \midrule
        3&&\\
     6     &    &   \\
     9  &      &   \\
    12 &  &      \\
    
    \bottomrule
\end{tabular}
\caption{Decoder layer number comparison under the 12-epoch setting. Mask DINO benefits from more decoders, while DINO's performance will decrease with 9 decoders.}
\label{tab:dec_layer}
\end{minipage}\hspace{6mm}

\begin{minipage}[t]{0.48\textwidth}
\makeatletter\def\@captype{table}
\centering
\vspace{-45pt}
\begin{tabular}{cc|cc}
    \toprule
    \multicolumn{2}{c}{Matching} & \multirow{2}{*}{Box AP} & \multirow{2}{*}{Mask AP} \\
          Box&  Mask & & \\
        \midrule
    \checkmark     &      &  &  \\
     & \checkmark  &  &     \\
    \checkmark & \checkmark     &  &       \\
    \bottomrule
\end{tabular}
\caption{Matching method comparison under the 12-epoch setting. We train both tasks together but use different matching methods to verify the effectiveness of hybrid matching.}
\label{tab:match}
\end{minipage}\hspace{6mm}
\end{adjustbox}
\vspace{-.4cm}
\end{table*}

 \begin{table*}[!t]
\begin{adjustbox}{width=1.0\textwidth,center}
\begin{minipage}[t]{0.8\textwidth}
\makeatletter\def\@captype{table}
\centering
\centering
\vspace{-37.005pt}
\begin{tabular}{c|c|ccccc}
    \toprule
    &Epochs& PQ & PQ& PQ&Box AP&Mask AP \\
        \midrule
        w/o decouple&12&&&&&\\
     w/ decouple    &12 &\fontsize{7.0pt}{\baselineskip}\selectfont{(+1.1)}    & & & &   \\
     \midrule
     w/o decouple  &50&    & &&&  \\
    w/ decouple &50 &\textbf{53.0}\fontsize{7.0pt}{\baselineskip}\selectfont{(+0.3)} &\textbf{59.1} &  &  & \textbf{44.3}     \\
    \bottomrule
\end{tabular}
\caption{Effectiveness of decoupled box prediction for panoptic segmentation under the 12-epoch and 50-epoch settings.}
\label{tab:decouple}
\end{minipage}

\begin{minipage}[t]{0.65\textwidth}
\makeatletter\def\@captype{table}
\centering
    \begin{tabular}{l|ccc}
        \toprule
          & Box AP & Mask AP   \\
        \midrule
         Mask DINO (ours) & \textbf{45.7}&\textbf{41.4} \\
         \midrule
 Mask-enhanced anchor box initialization  & 44.5\fontsize{7.0pt}{\baselineskip}\selectfont{(-1.2)}&41.4    \\
         \quad Unified query selection for masks  & 43.6\fontsize{7.0pt}{\baselineskip}\selectfont{(-2.1)} &40.3\fontsize{7.0pt}{\baselineskip}\selectfont{(-1.1)}   \\
         Unified denoising for masks & 44.4\fontsize{7.0pt}{\baselineskip}\selectfont{(-1.3)}&40.3 \fontsize{7.0pt}{\baselineskip}\selectfont{(-1.1)}    \\
         Hybrid matching & 44.9\fontsize{7.0pt}{\baselineskip}\selectfont{(-0.8)} & 40.5 \fontsize{7.0pt}{\baselineskip}\selectfont{(-0.9)}   \\
        \midrule
         remove all the above & 41.7 \fontsize{7.0pt}{\baselineskip}\selectfont{(-4.0)} & 38.5 \fontsize{7.0pt}{\baselineskip}\selectfont{(-2.7)}   \\
        \bottomrule
    \end{tabular}
    \caption{Effectiveness of the proposed components under the 12-epoch setting. 
}
    \label{tab:ablation}
    \end{minipage}
    \end{adjustbox}
    \vspace{-.5cm}
\end{table*}

 \noindent\textbf{Panoptic segmentation.} We compare Mask DINO with other models in Table~\ref{tab:panoptic}. Mask DINO outperforms all previous best models on both the -epoch and -epoch settings by  PQ and  PQ, respectively. This indicates Mask DINO has the advantages of both faster convergence and superior performance. One interesting observation is that we outperform Mask2Former~\cite{cheng2021mask2former} in terms of both  and . However, instead of using dense and hard-constrained masked attention, we predict boxes and then use them in deformable attention to extract query features. Therefore, our box-oriented deformable attention also works well with "stuff" categories,  which makes our unified model simple and efficient. 
In addition, we improve the mask AP by  to  AP, which is  higher than the specialized instance segmentation model Mask2Fomer ( AP).
\\\textbf{Semantic segmentation.} In Table ~\ref{tab:sematic_ade} and \ref{tab:sematic_c}, we show the performance of semantic segmentation with a ResNet-50 backbone. We use  queries for these small datasets. We outperform Mask2Former on both ADE20K and Cityscapes by  and  mIoU on the reported performance.


\subsection{Comparison with SOTA Models}
In Table ~\ref{tab:sota}, we compare Mask DINO with SOTA models on three image segmentation tasks to show its scalability. We use the SwinL~\cite{liu2021swin} backbone and pre-train DINO on the Objects365~\cite{shao2019objects365} detection dataset. Even without using extra data, we outperform Mask2Former on all three tasks, especially on instance segmentation (\textbf{+2.5 AP}). As Mask DINO is an extension of DINO, the pre-trained DINO model can be used to fine-tune Mask DINO for segmentation tasks. After fine-tuning Mask DINO on the corresponding tasks, we achieve the best results on instance ( AP), panoptic (  PQ), and semantic ( mIoU) segmentation among model under one billion parameters. Compared to SwinV2-G~\cite{liu2021swinv2}, we significantly reduce the model size to 1/15 and backbone pre-training dataset to 1/5. Our detection pre-training also significantly helps all segmentation tasks including panoptic and semantic with "stuff" categories. However, previous specialized segmentation models such as Mask2Former can not use detection datasets and adding a detection head to it results in poor performance as shown in Table \ref{tab:mask2former_head}, which severely limits the data scalability. By unifying four tasks in one model, we only need to pre-train one model on a large-scale dataset and finetune on all tasks for 10 to 20 epochs (Mask2Former needs 100 epochs), which is more computationally efficient and simpler in model design.

\subsection{Ablation Studies}
We conduct ablation studies using a ResNet-50 backbone to analyze Mask DINO on COCO \texttt{val2017}. Unless otherwise stated, our experiments are based on object detection and instance segmentation without \emph{Mask-enhanced anchor box initialization}.
\\\textbf{Query selection.} Table ~\ref{tab:dec_test} shows the results of our query selection for instance segmentation, where we additionally provide the performance of different decoder layers in one single model. Mask2Former also predicts the masks of learnable queries as initial region proposals. However, their performance lags behind Mask DINO by a large margin (). With our effective query selection scheme, the mask performance achieves  AP without using the decoder. In addition, our mask performance at layer six is already comparable to the final results with 9 layers. In Table \ref{tab:maskenhance}, we show that in query selection the predicted box is inferior to mask, which indicates segmentation is easier to learn in the initial stage. Therefore, our proposed mask-enhanced box initialization enhances boxes with masks in query selection to provide better anchor boxes (+15.6 AP) for the decoder, which results in \textbf{+1.2} AP improvement in the final detection performance. 
\\\textbf{Feature scales.}
Mask2Former~\cite{cheng2021mask2former} shows that concatenating multi-scale features as input to Transformer decoder layers does not improve the segmentation performance. However, in Table \ref{tab:feature_scale}, Mask DINO shows that using more feature scales in the decoder consistently improves the performance.
\\\textbf{Object detection and segmentation help each other.} To validate task cooperation in Mask DINO, we use the same model but train different tasks and report the 12 epoch and 50 epoch results. As shown in Table ~\ref{tab:taskhelp}, only training one task will lead to a performance drop. Although only training object detection results in faster convergence in the early stage for box prediction, the final  performance is still inferior to training both tasks together.
\\\textbf{Decoder layer number.}
In DINO, increasing the decoder layer number to nine will decrease the performance of box. In Table \ref{tab:dec_layer}, the result indicates that increasing the number of decoder layers will contribute to both detection and segmentation in Mask DINO. We hypothesize that the multi-task training become more complex and require more decoders to learn the needed mapping function.
\\\textbf{Matching.}
In Table~\ref{tab:match}, we show that only using boxes or masks to perform bipartite matching is not optimal in Mask DINO. A unified matching objective makes the optimization more consistent.
\\\textbf{Decoupled box prediction.}
In Table~\ref{tab:decouple}, we show the effectiveness of our decoupled box prediction for panoptic segmentation. This decoupled design of "thing" and "stuff" accelerates training in the early stage (12-epoch setting) and improves the final performance (50-epoch setting).
\\\textbf{Effectiveness of the algorithm components.} In Table \ref{tab:ablation}, we remove each algorithm component at a time and show that each component contributes to the final performance. 
In addition, after removing all the proposed components, both detection and segmentation performance drop by a large margin. This result indicates that if we trivially add detection and segmentation tasks in one DETR-based model, the features are not aligned for detection and segmentation tasks to achieve mutual cooperation.


We also present visualization analysis in Appendix \ref{sec:vis}.


\section{Conclusion}
In this paper, we have presented Mask DINO as a unified Transformer-based framework for both object detection and image segmentation. Conceptually, Mask DINO is a natural extension of DINO from detection to segmentation with minimal modifications on some key components.
Mask DINO outperforms previous specialized models and achieves the best results on all three segmentation tasks (instance, panoptic, and semantic) among models under one billion parameters. {Moreover, Mask DINO shows that detection and segmentation can help each other in query-based models.} In particular, Mask DINO enables semantic and panoptic segmentation to benefit from a better visual representation pre-trained on a large-scale detection dataset. We hope Mask DINO can provide insights for enabling task cooperation and data cooperation towards designing a universal model for more vision tasks.
\\\textbf{Limitations: }Different segmentation tasks fail to achieve mutual assistance in Mask DINO in COCO panoptic segmentation. 
For example, in COCO panoptic segmentation, the mask AP still lags behind the model only trained with instances. 
In addition, under the large-scale setting, we have not achieved a new SOTA detection performance as the segmentation head requires additional GPU memory. To accommodate this memory limitation, for the large-scale setting, we have to use smaller image size and less number of queries compared with DINO, which impacts the final performance of object detection. In the future, we will further optimize the implementation to develop a more universal and efficient model to promote task cooperation.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\appendix
\newpage
\clearpage


\setcounter{page}{1}
\section{Visualization analysis}\label{sec:vis}
\begin{figure*}[htbp]
\centering
\begin{minipage}[h]{0.12\textwidth}
\includegraphics[width=0.75in]{resources/images/tgt.png}
\subcaption{}
\end{minipage}\begin{minipage}[h]{0.12\textwidth}
\includegraphics[width=0.75in]{resources/images/0_ori.png}
\subcaption{}
\end{minipage}\begin{minipage}[h]{0.12\linewidth}
\includegraphics[width=0.75in]{resources/images/0_m2f.png}
\subcaption{}
\end{minipage}
\begin{minipage}[h]{0.12\linewidth}
\includegraphics[width=0.75in]{resources/images/86_2_1_fail.png}
\subcaption{}
\end{minipage}
\begin{minipage}[h]{0.12\linewidth}
\includegraphics[width=0.75in]{resources/images/86_2_1.png}
\subcaption{}
\end{minipage}
\begin{minipage}[h]{0.15\linewidth}
\includegraphics[width=1.05in]{resources/images/8.png}
\subcaption{}
\end{minipage}
\begin{minipage}[h]{0.15\linewidth}
\includegraphics[width=1.05in]{resources/images/8_1.png}
\subcaption{}
\end{minipage}

\caption{(a) The green transparent region is the ground truth mask for the girl. (b)(c) The predicted masks of the -th decoder layer in Mask2Former and Mask DINO, respectively. Note that we attain the predicted masks by first choosing the query which is finally assigned to the ground truth mask in the last decoder layer. Then we visualize the predicted mask of this query by performing dot production with the pixel embedding map. (d)(e) The outputs of the -st layer in Mask2Former and Mask DINO. The red masks are predicted masks and the green box is the predicted box by Mask DINO. The blue points are sampled points by deformable attention. Since the -th layer of Mask2Former usually outputs unfavorable masks, we avoid using its -th layer here. (f)(g) show that Mask DINO can predict correct sampled points, boxes, and masks for background stuffs.}
\label{fig:analyze_2st}
\end{figure*}
 There has been a trend to unify detection and segmentation tasks using convolution-based models, which not only simplifies model design but also promotes mutual cooperation between detection and segmentation. There are mainly three motivations for us to propose Mask DINO. \textbf{First}, DINO~\cite{zhang2022dino} has achieved SOTA results on object detection. Previous works such as Mask RCNN~\cite{he2017mask}, HTC~\cite{chen2019hybrid}, and DETR~\cite{carion2020end} have shown that a detection model can be extended to do segmentation and help design better segmentation models. 
\textbf{Second}, detection is a relatively easier task than instance segmentation. As shown in Table   \ref{tab:instance} (and other previous studies), Box AP is usually  AP higher than mask AP. Therefore, box prediction can guide attention to focus on more meaningful regions and extract better features for mask prediction. \textbf{Third}, the new improvements in DINO and other DETR-like models~\cite{zhu2020deformable,li2022dn} such as query selection and deformable attention can also help segmentation tasks. For example, Mask2Former adopts learnable decoder queries, which cannot take advantage of the position information in the selected top  features from the encoder to guide mask predictions. Fig.   \ref{fig:analyze_2st}(a)(b)(c) show that the output of Mask2Former in the -th decoder layer is far away from the GT mask while Mask DINO outputs much better masks as region proposals. Mask2Former also adopts specialized masked attention to guide the model to attend to regions of interest. However, masked attention is a hard constraint which ignores features outside a provided mask and may overlook important information for following decoder layers. In addition, deformable attention is also a better substitute for its high efficiency allowing attention to be applied to multi-scale features without too much computational overhead. Fig.     \ref{fig:analyze_2st}(d)(e) show a predicted mask of Mask2Former in its -st decoder layer and the corresponding output of Mask DINO. The prediction of Mask2Former only covers less than half of the GT mask, which means that the attention can not see the whole instance in the next decoder layer. 
Moreover, a box can also guide deformable attention to a proper region for background stuff, as shown in Fig.   \ref{fig:analyze_2st}(f)(g).

\section{Implementation details}\label{sec:impl}
\textbf{The code is available in the supplementary materials}. We also provide some detailed descriptions of our implementation here.
\subsection{General settings}\label{sec:generalimpl}
\textbf{Dataset and metrics: }We evaluate Mask DINO on two challenging datasets: COCO 2017~\cite{lin2015microsoft} for object detection, instance segmentation, and panoptic segmentation; ADE20K~\cite{zhou2017scene} for semantic segmentation. They both have "thing" and "stuff" categories, therefore we follow the common practice to evaluate object detection and instance segmentation on the "thing" categories and evaluate panoptic and semantic segmentation on the union of the "thing" and "stuff" categories. Unless otherwise stated, all results are trained on the \texttt{train} split and evaluated on the \texttt{validation} split. For object detection and instance segmentation, the results are evaluated with the standard average precision (AP) and mask AP~\cite{lin2015microsoft} result. For panoptic segmentation, we evaluate the results with the panoptic quality (PQ) metric~\cite{kirillov2019panoptic}. We also report  (AP on the "thing" categories) and  (AP on the "stuff" categories). For semantic segmentation, the results are evaluated with the mean Intersection-over-Union (mIoU) metric~\cite{everingham2015pascal}.
\begin{table*}[h]
    \centering
    \small
        \setlength\tabcolsep{2pt}
        \footnotesize
            \renewcommand{\arraystretch}{1.3}
\begin{adjustbox}{width=0.8\textwidth,center}
    \begin{tabular}{c|c|c|c|c|cc}
\toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Params} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{\makecell{Backbone Pre-training \\ Dataset }} &  \multirow{2}{*}{\makecell{Detection Pre-training \\ Dataset }}   & \multicolumn{2}{c}{{ test}}  \\
        & & &  && \scriptsize w/o TTA& \scriptsize w/ TTA  \\
        \midrule
        \multicolumn{5}{c}{{\textbf{Instance segmentation on COCO}}}&\multicolumn{2}{c}{AP} \\
        \midrule
        Mask2Former~\cite{cheng2021mask2former} & M & SwinL & IN-22K-14M &     &  &   \\
        Soft Teacher~\cite{xu2021end} & M & SwinL & IN-22K-14M & O365     &- &   \\
        SwinV2-G-HTC++~\cite{liu2021swinv2} & B & SwinV2-G & IN-22K-ext-70M~\cite{liu2021swinv2} & O365    & - &  \\
        \hline
        MasK DINO(Ours) & {M} & SwinL & IN-22K-14M & O365 &     &   \\
        \midrule
        \multicolumn{5}{c}{{\textbf{Panoptic segmentation on COCO}}}&\multicolumn{2}{c}{PQ} \\
        \midrule


        Panoptic SegFormer~\cite{li2021panoptic}& M & SwinL & IN-22K-14M &      & &   \\
         Mask2Former~\cite{cheng2021mask2former} & M & SwinL & IN-22K-14M &      &  &   \\

        \hline
        MasK DINO (ours) & {M} & SwinL & IN-22K-14M & O365 &    &   \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\caption{Comparison of SOTA models on COCO \texttt{test-dev}. Mask DINO outperforms all existing models. "TTA" means test-time-augmentation.
    ``O365''  denotes the  Objects365~\cite{shao2019objects365} dataset. 
    }
    \label{tab:testsota}
    \vspace{-.7cm}
\end{table*}
\\\textbf{Backbone: }We report results with two public backbones: ResNet-50~\cite{he2015deep} and SwinL~\cite{liu2021swin}. To achieve SOTA performance using a large model with the SwinL backbone, we use Objects365~\cite{shao2019objects365} to pre-train an object detection model and then fine-tune the model on the corresponding datasets for all tasks. Though we only pre-train for object detection, our model generalizes well to improve the performance of all segmentation tasks.
\\\textbf{Loss function: }As we train detection and segmentation tasks jointly, there are totally three kinds of losses, including classification loss , box loss , and mask loss . Among them, box loss (L1 loss  and GIOU loss~\cite{rezatofighi2019generalized} ) and classification loss (focal loss~\cite{lin2018focal}) are the same as DINO~\cite{zhang2022dino}. For mask loss, we adopt cross-entropy  and dice loss . We also follow \cite{kirillov2020pointrend,cheng2021pointly,cheng2021mask2former} to use point loss in mask loss for efficiency. Therefore, the total loss is a linear combination of three kinds of losses: , where we set , and .
\\\textbf{Basic hyper-parameters: }Mask DINO has the same architecture as DINO~\cite{zhang2022dino}, which is composed of a backbone, a Transformer encoder, and a Transformer decoder. Compared to DINO, we increase the number of decoder layers from six to nine and use  queries. We follow Mask-RCNN~\cite{he2017mask} and Mask2Former~\cite{cheng2021mask2former} to setup the training and inference settings for segmentation tasks. We use batch size  and train 50 epoch for COCO segmentation tasks (instance and panoptic), 160K iteration for ADE20K semantic segmentation, and  iterations for Cityscapes semantic segmentation.
We set the initial learning rate (lr) as  and adopt a simple lr scheduler, which drops lr by multiplying 0.1 at the 11-th epoch for the 12-epoch setting and the 20-th epoch for the 24-epoch setting. For the other segmentation settings, we drop the lr at 0.9 and 0.95 fractions of the total number of training steps by multiplying 0.1. Under the ResNet-50 backbone, we use  A100 GPUs each with 40GB memory for all tasks. We report the frames-per-second (fps) tested on the same A100 NVIDIA GPU for Mask2Former and Mask DINO by taking the average computing time with batch size 1 on the entire validation set.
\\\textbf{Augmentations and Multi-scale setting: }We use the same training augmentations as in Mask2Former~\cite{cheng2021mask2former}, where the major difference from DINO~\cite{zhang2022dino} on COCO is that we use large-scale jittering (LSJ) augmentation~\cite{du2021simple,ghiasi2021simple} and a fixed size crop to , which also works well for detection tasks. We use the same multi-scale setting as in DINO~\cite{zhang2022dino} to use 4 scales in ResNet-50-based models and 5 scales in SwinL-based models.
\subsection{Denoising training}\label{sec:denoise}
Following DN-DETR~\cite{li2022dn}, we train the model to reconstruct the ground-truth objects given the noised ones. These noised objects will be concatenated with the original decoder queries during training, but will be removed during inference. We add noise to both the bounding box and labels, which will serve as positional embedding and content embedding input to decoder queries. As a box can be viewed as a noised version of a segmentation mask, our unified denoising training will reconstruct the masks given the noised boxes, which improves segmentation training. 
\\\textbf{Label noise: } For label noise, we use \emph{label flip}, which randomly flips a ground-truth label into another possible label in the dataset with probability . After adding noise, all the labels will go through a label embedding to construct high-dimensional vectors, which will be the content queries of the decoder.  is set to 0.2 in our model. 
\\\textbf{Box noise: }
A box can be formulated as , which is also the positional query of DINO~\cite{zhang2022dino}. We add two kinds of noise to the box including \emph{center shifting} and \emph{box scaling}. 
For center shifting, we sample a random perturbation  to the box center. The sampled noise is constrained to  and , where  is a hyperparameter to control the maximum shifting. For box scaling, the width and height of the box are randomly scaled to  of the original ones, where  is also a hyperparameter to control the scaling. In our model, we set .


\section{Large models setting}
For large models with the SwinL backbone, we follow the same setting of DINO~\cite{zhang2022dino} to pre-train a model on the Objects365~\cite{shao2019objects365} dataset for object detection. Then we finetune the pre-trained model on COCO instance and panoptic segmentation for 24 epochs and on ADE20K semantic segmentation for 160k iterations. For training settings on instance and panoptic segmentation on COCO,  we use  larger scale () and  A100 GPUs. For training settings on ADE20K semantic, we use  more queries () and  A100 GPUs. We also use Exponential Moving Average (EMA) in this setting, which helps in ADE20K semantic segmentation.
\section{SOTA Results on COCO test-dev}
We show the COCO test-dev results in Table \ref{tab:testsota}.
\end{document}