Learning to track arbitrary objects can be addressed using similarity learning.
We propose to learn a function $f(z, x)$ that compares an exemplar image~$z$ to a candidate image~$x$ of the same size and returns a high score if the two images depict the same object and a low score otherwise.
To find the position of the object in a new image, we can then exhaustively test all possible locations and choose the candidate with the maximum similarity to the past appearance of the object.
In experiments, we will simply use the initial appearance of the object as the exemplar.
The function $f$ will be learnt from a dataset of videos with labelled object trajectories.

Given their widespread success in computer vision~\cite{razavian2014cnn,parkhi2015deep,dosovitskiy2015flownet,krizhevsky2012imagenet}, we will use a deep conv-net as the function~$f$.
Similarity learning with deep conv-nets is typically addressed using Siamese architectures~\cite{bromley1993signature,taigman2014deepface,zagoruyko2015learning}.
Siamese networks apply an identical transformation $\varphi$ to both inputs and then combine their representations using another function $g$ according to $f(z, x) = g(\varphi(z), \varphi(x))$.
When the function $g$ is a simple distance or similarity metric, the function $\varphi$ can be considered an embedding.
Deep Siamese conv-nets have previously been applied to tasks such as face verification~\cite{taigman2014deepface,schroff2015facenet,parkhi2015deep}, keypoint descriptor learning~\cite{zagoruyko2015learning,simo2015discriminative} and one-shot character recognition~\cite{koch2015siamese}.

\subsection{Fully-convolutional Siamese architecture}

We propose a Siamese architecture which is \emph{fully-convolutional} with respect to the candidate image~$x$.
We say that a function is fully-convolutional if it commutes with translation.
To give a more precise definition, introducing $L_{\tau}$ to denote the translation operator $(L_{\tau} x)[u] = x[u-\tau]$, a function $h$ that maps signals to signals is fully-convolutional with integer stride $k$ if
\begin{equation}\label{eq:commutes-translation}
h(L_{k \tau} x) = L_{\tau} h(x)
\end{equation}
for any translation~$\tau$.
(When $x$ is a finite signal, this only need hold for the valid region of the output.)

\begin{figure}[t]
\centering
\includegraphics[scale=.9]{art/conv-explicit.pdf}
\caption{Fully-convolutional Siamese architecture.
Our architecture is fully-convolutional with respect to the search image~$x$.
The output is a scalar-valued score map whose dimension depends on the size of the search image.
This enables the similarity function to be computed for all translated sub-windows within the search image in one evaluation.
In this example, the red and blue pixels in the score map contain the similarities for the corresponding sub-windows.
Best viewed in colour.
}
\label{fig:fully-conv-siamese}
\end{figure}

The advantage of a fully-convolutional network is that, instead of a candidate image of the same size, we can provide as input to the network a much larger \emph{search} image and it will compute the similarity at all translated sub-windows on a dense grid in a single evaluation.
To achieve this, we use a convolutional embedding function $\varphi$ and combine the resulting feature maps using a cross-correlation layer
\begin{equation}\label{eq:cross-correlation}
f(z, x) = \varphi(z) * \varphi(x) + b \, \mathbbm{1},
\end{equation}
where $b \, \mathbbm{1}$ denotes a signal which takes value $b \in \R$ in every location.
The output of this network is not a single score but rather a score map defined on a finite grid~$\mathcal{D} \subset \mathbb{Z}^{2}$ as illustrated in Figure~\ref{fig:fully-conv-siamese}.
Note that the output of the embedding function is a feature map with spatial support as opposed to a plain vector.
The same technique has been applied in contemporary work on stereo matching~\cite{luo2016efficient}.

During tracking, we use a search image centred at the previous position of the target.
The position of the maximum score relative to the centre of the score map, multiplied by the stride of the network, gives the displacement of the target from frame to frame.
Multiple scales are searched in a single forward-pass by assembling a mini-batch of scaled images.

Combining feature maps using cross-correlation and evaluating the network once on the larger search image is mathematically equivalent to combining feature maps using the inner product and evaluating the network on each translated sub-window independently.
However, the cross-correlation layer provides an incredibly simple method to implement this operation efficiently within the framework of existing conv-net libraries.
While this is clearly useful during testing, it can also be exploited during training.


\subsection{Training with large search images\label{sec:training}}

We employ a discriminative approach, training the network on positive and negative pairs and adopting the logistic loss
\begin{equation}
\ell(y, v) = \log(1 + \exp(-y v))
\end{equation}
where $v$ is the real-valued score of a single exemplar-candidate pair and $y \in \{+1, -1\}$ is its ground-truth label.
We exploit the fully-convolutional nature of our network during training by using pairs that comprise an exemplar image and a larger search image.
This will produce a map of scores $v : \mathcal{D} \to \R$, effectively generating many examples per pair.
We define the loss of a score map to be the mean of the individual losses
\begin{equation}\label{eq:global-loss}
L(y, v) = \frac{1}{|\mathcal{D}|} \sum_{u \in \mathcal{D}} \ell(y[u], v[u]) \enspace ,
\end{equation}
requiring a true label $y[u] \in \{+1, -1\}$ for each position $u \in \mathcal{D}$ in the score map.
The parameters of the conv-net $\theta$ are obtained by applying Stochastic Gradient Descent (SGD) to the problem
\begin{equation}
\arg\min_{\theta} \underset{(z, x, y)}{\mathbb{E}} L(y, f(z, x; \theta)) \enspace .
\label{eq:training}
\end{equation}

\begin{figure}[t]
\centering


\begin{tabular}{ccc} 

\includegraphics[width=18mm]{art/pairs/ILSVRC2015_train_00005015/000030.00.crop.z-edit.jpg}
& \includegraphics[width=18mm]{art/pairs/ILSVRC2015_train_00014023/000000.00.crop.z-edit.jpg}
& \includegraphics[width=18mm]{art/pairs/ILSVRC2015_train_00010001/000010.00.crop.z-edit.jpg}
\\
\includegraphics[width=36mm]{art/pairs/ILSVRC2015_train_00005015/000060.00.crop.x-edit.jpg}
& \includegraphics[width=36mm]{art/pairs/ILSVRC2015_train_00014023/000030.00.crop.x-edit.jpg}
& \includegraphics[width=36mm]{art/pairs/ILSVRC2015_train_00010001/000040.00.crop.x-edit.jpg}

\end{tabular}
 \caption{
Training pairs extracted from the same video: exemplar image and corresponding search image from same video.
When a sub-window extends beyond the extent of the image, the missing portions are filled with the mean RGB value.
}
\label{fig:pos-pairs}
\end{figure}

Pairs are obtained from a dataset of annotated videos by extracting exemplar and search images that are centred on the target, as shown in Figure~\ref{fig:pos-pairs}.
The images are extracted from two frames of a video that both contain the object and are at most $T$ frames apart.
The class of the object is ignored during training.
The scale of the object within each image is normalized without corrupting the aspect ratio of the image.
The elements of the score map are considered to belong to a positive example if they are within radius $R$ of the centre (accounting for the stride~$k$ of the network)
\begin{equation}
y[u] = \begin{cases} +1 & \text{if } k \|u - c\| \le R \\ -1 & \text{otherwise} \enspace . \end{cases}
\end{equation}
The losses of the positive and negative examples in the score map are weighted to eliminate class imbalance.

Since our network is fully-convolutional, there is no risk that it learns a bias for the sub-window at the centre.
We believe that it is effective to consider search images centred on the target because it is likely that the most difficult sub-windows, and those which have the most influence on the performance of the tracker, are those adjacent to the target.




Note that since the network is symmetric $f(z, x) = f(x, z)$, it is in fact also fully-convolutional in the exemplar.
While this allows us to use different size exemplar images for different objects in theory, we assume uniform sizes because it simplifies the mini-batch implementation.
However, this assumption could be relaxed in the future.

\subsection{ImageNet Video for tracking}

The 2015 edition of ImageNet Large Scale Visual Recognition Challenge~\cite{ILSVRC15} (ILSVRC) introduced the ImageNet Video dataset as part of the new \emph{object detection from video} challenge.
Participants are required to classify and locate objects from 30 different classes of animals and vehicles.
Training and validation sets together contain almost 4500 videos, with a total of more than one million annotated frames.
This number is particularly impressive if compared to the number of labelled sequences in VOT~\cite{kristan2015visual}, ALOV~\cite{smeulders2014visual} and OTB~\cite{WuLimYang13}, which together total less than 500 videos.
We believe that this dataset should be of extreme interest to the tracking community not only for its vast size, but also because it depicts scenes and objects different to those found in the canonical tracking benchmarks.
For this reason, it can safely be used to train a deep model for tracking without over-fitting to the domain of videos used in these benchmarks.



\subsection{Practical considerations}
\label{sec:considerations}



\subsubsection{Dataset curation}
During training, we adopt exemplar images that are $127 \times 127$ and search images that are $255 \times 255$ pixels.
Images are scaled such that the bounding box, plus an added margin for context, has a fixed area.
More precisely, if the tight bounding box has size $(w, h)$ and the context margin is $p$, then the scale factor~$s$ is chosen such that the area of the scaled rectangle is equal to a constant
\begin{equation}
s (w + 2 p) \times s (h + 2 p) = A \enspace .
\end{equation}
We use the area of the exemplar images $A = 127^2$ and set the amount of context to be half of the mean dimension $p = (w+h) / 4$.
Exemplar and search images for every frame are extracted offline to avoid image resizing during training.
In a preliminary version of this work, we adopted a few heuristics to limit the number of frames from which to extract the training data.
For the experiments of this paper, instead, we have used \emph{all} 4417 videos of ImageNet Video, which account for more than 2 million labelled bounding boxes.

\subsubsection{Network architecture}
The architecture that we adopt for the embedding function~$\varphi$ resembles the convolutional stage of the network of Krizhevsky et al.~\cite{krizhevsky2012imagenet}.
The dimensions of the parameters and activations are given in Table~\ref{tab:architecture}.
Max-pooling is employed after the first two convolutional layers.
ReLU non-linearities follow every convolutional layer except for conv5, the final layer.
During training, batch normalization~\cite{ioffe2015batch} is inserted immediately after every linear layer.
The stride of the final representation is eight.
An important aspect of the design is that no padding is introduced within the network.
Although this is common practice in image classification, it violates the fully-convolutional property of eq.~\ref{eq:commutes-translation}.

\begin{table}[t]
\centering
\caption{Architecture of convolutional embedding function, which is similar to the convolutional stage of the network of Krizhevsky et al.~\cite{krizhevsky2012imagenet}.
The channel map property describes the number of output and input channels of each convolutional layer.
}
\label{tab:architecture}
{\begin{tabular}{L{8ex} C{10ex} C{14ex} C{8ex} C{16ex} C{16ex} L{8ex}} \hline
& & & & \multicolumn{3}{c}{Activation size} \\
Layer & Support & Chan.\ map & Stride & for exemplar & for search & \multicolumn{1}{c}{chans.} \\ \hline
& & & & $127 \times 127$ & $255 \times 255$ & $\times 3$ \\
conv1 & $11 \times 11$ & $96 \times 3$    & 2 & $59 \times 59$ & $123 \times 123$ & $\times 96$ \\
pool1 & $3 \times 3$   &                  & 2 & $29 \times 29$ & $61 \times 61$   & $\times 96$ \\
conv2 & $5 \times 5$   & $256 \times 48$  & 1 & $25 \times 25$ & $57 \times 57$   & $\times 256$ \\
pool2 & $3 \times 3$   &                  & 2 & $12 \times 12$ & $28 \times 28$   & $\times 256$ \\
conv3 & $3 \times 3$   & $384 \times 256$ & 1 & $10 \times 10$ & $26 \times 26$   & $\times 192$ \\
conv4 & $3 \times 3$   & $384 \times 192$ & 1 & $8 \times 8$   & $24 \times 24$   & $\times 192$ \\
conv5 & $3 \times 3$   & $256 \times 192$ & 1 & $6 \times 6$   & $22 \times 22$   & $\times 128$ \\
\hline
\end{tabular}
}
\end{table}

\subsubsection{Tracking algorithm}
Since our purpose is to prove the efficacy of our fully-convolutional Siamese network and its generalization capability when trained on ImageNet Video, we use an extremely simplistic algorithm to perform tracking.
Unlike more sophisticated trackers, we do not update a model or maintain a memory of past appearances, we do not incorporate additional cues such as optical flow or colour histograms, and we do not refine our prediction with bounding box regression.
Yet, despite its simplicity, the tracking algorithm achieves surprisingly good results when equipped with our offline-learnt similarity metric.
Online, we do incorporate some elementary temporal constraints: we only search for the object within a region of approximately four times its previous size, and a cosine window is added to the score map to penalize large displacements.
Tracking through scale space is achieved by processing several scaled versions of the search image.
Any change in scale is penalized and updates of the current scale are damped.
