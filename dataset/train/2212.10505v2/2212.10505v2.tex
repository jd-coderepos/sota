\section{Experiment}\label{sec:exp}



We introduce the experimental setup in \Cref{sec:setup} and then the results in \Cref{sec:main_results} including both plot-to-table translation and downstream QA tasks.


\subsection{Experimental Setup}\label{sec:setup}

\paragraph{Training and inference.} \model{} is trained for 10k steps with a maximum sequence length of 512. The other hyperparameters are identical to \textsc{MatCha} pretraining as introduced in \citet{liu2022matcha}. In \model{} inference we set temperature to $0$ (so the output is deterministic). For LLM prompting, in all cases we use temperature of $0.4$.


\paragraph{Datasets and metrics.} We evaluate on two chart/plot question answering benchmarks ChartQA \citep{masry-etal-2022-chartqa} and PlotQA \citep{methani2020plotqa}. ChartQA contains two sets: augmented (aug.) and human where the augmented set is synthetically generated and the human set is human written. Human written queries usually are more diverse and complex, requiring more reasoning while synthetic questions are usually highly templatic. PlotQA is purely synthetic. It contains v1 \& v2 sets where v1 is mostly extractive questions and v2 focuses more on numerical reasoning.  
Both \texttt{RNSS} and \texttt{RMS}$_{\text{F1}}$ are used for evaluating plot-to-table translation (though we have argued that \texttt{RMS}$_{\text{F1}}$ is the more informative metric). Following \citet{masry-etal-2022-chartqa,methani2020plotqa}, exact match accuracy with 5\% tolerance on numerical error is used to report all QA numbers.


We list data statistics of plot-to-table training in \Cref{tab:deplot_stats}.  Note that the plot-table pairs are only from ChartQA and PlotQA training sets (not their validation/test sets). The statistics of PlotQA and ChartQA test data are listed in \Cref{tab:stats}. Note that we are also using plot-table pairs from the PlotQA test set for evaluating the plot-to-table task (plot-table pairs from v1 and v2 are identical).

\begin{table}[ht]
\centering
  \scalebox{0.9}{
    \begin{tabular}{llcc}
    \toprule
 Component & Rate & Size  \\
    \midrule

synthetic (by us) & 33.3\% & 270K \\
ChartQA &  33.3\% & 22K \\
PlotQA & 33.3\% & 224K \\

\bottomrule
\end{tabular}
}
\caption{Data statistics for the training data of plot-to-table task.}
\label{tab:deplot_stats}
\end{table}


\paragraph{Hardware.} We train and evaluate our models using 64 GCP-TPUv3. The training of \model{} can be completed in roughly 5 hours.

\paragraph{Parameters.} \model{} has 282M parameters. FlanPaLM has 540B parameters. Codex and GPT3 have roughly 175B parameters.




\begin{table}[t!]
    \centering
  \scalebox{0.9}{
    \begin{tabular}{lcc}
    \toprule
 Dataset & \# Tables & \# QA Pairs  \\
    \midrule
ChartQA (Human) & 625 & 1,250 \\
ChartQA (Machine) & 987 & 1,250 \\
PlotQA (v1) & 33K & 1.2M \\
PlotQA (v2) & 33K & 4.3M \\
\bottomrule
\end{tabular}
}
\caption{Dataset statistics of the test sets for ChartQA and PlotQA.}
\label{tab:stats}
\end{table}


\subsection{Main Results}\label{sec:main_results}

\paragraph{Plot-to-table translation.} We evaluate plot-to-table conversion against an OCR and keypoint detection based system proposed by \citet{luo2021chartocr} called ChartOCR. This system also relies on multiple hand-crafted rules that depend on the type of chart. We also compare against two PaLI models~\cite{chen2022pali} (of different input resolutions) finetuned with the same plot-to-table corpus as \model. Finally, we compare with the \textsc{MatCha} base model off-the-shelf. The results are shown in \Cref{tab:plot_to_table}.

\begin{table}[ht]
    \centering
  \scalebox{0.88}{
    \begin{tabular}{lcc}
        \toprule
        Model$\downarrow$, Metric$\rightarrow$ & \texttt{RNSS} & \texttt{RMS}$_{\text{F1}}$ \\
        \midrule
        ChartOCR \citep{luo2021chartocr} & 81.0 & 60.1 \\
\midrule
        PaLI-17B (res. 224) + plot-to-table & 77.2 & 24.8 \\
PaLI-17B (res. 588) + plot-to-table  & 90.5 & 74.9 \\
        \textsc{MatCha} \citep{liu2022matcha} & 95.4 & 92.3 \\
        \midrule
        \model{} & \textbf{97.1} & \textbf{94.2} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Benchmarking plot-to-table conversion accuracy on the PlotQA dataset (all individual plots in PlotQA test sets). Both a pipeline-bsed based method (ChartOCR) and end-to-end methods (PaLI-17B and \textsc{MatCha}) are used as baselines. \texttt{RMS}$_{\text{F1}}$ can capture the shortcomings of baselines such as ChartOCR with much greater sensitivity.}
    \label{tab:plot_to_table}
\end{table}

On both metrics, \model{} outperforms the baseline ChartOCR by very significant margins. The gap is especially large on \texttt{RMS}$_{\text{F1}}$ since ChartOCR might suffice to extract numbers from the plot but can struggle to organize the extracted numbers into a structured table with the correct row and column labels. When compared against PaLI and \textsc{MatCha}, \model{} is also better, suggesting that a visual-language-specific architecture/initialization and task-specific finetuning can both boost plot-to-table accuracy. It is also worth noting that PaLI-17B (res. 588) performs much better than the 224-resolution variant, indicating that high input resolution is a key ingredient for chart information extraction.

\begin{table*}[!ht]
    \centering
  \scalebox{0.88}{
    \begin{tabular}{llcccccccccccc}
    \toprule
& & \multicolumn{3}{c}{ChartQA} & & \multicolumn{3}{c}{PlotQA} \\ \cline{3-5}      \cline{7-9}   & Model$\downarrow$ &  aug. \colorbox{blue!30}{$\ $} & human \colorbox{red!30}{$\ $} & avg. & & v1 \colorbox{blue!30}{$\ $} & v2 \colorbox{blue!30}{$\ $} & avg.\\  \midrule
& human ceiling & - & - & - & & - & - & 80.5 \\
\midrule
  \multirow{8}{*}{\rotatebox[origin=c]{90}{Fully-supervised}} & CRCT &  - & - & - & & 76.9 & 34.4 & 55.7 \\& VL-T5-OCR  & - & - & 41.6 & & 75.9 & 56.0 & 66.0 \\& T5-OCR & - & - & 41.0 & & 72.6 & 56.2 & 64.4 \\& VisionTapas-OCR & - & - & 45.5 & & 65.3 & 42.5 & 53.9 \\& PaLI-17B (res. 224) & 6.2  & 12.6 & 9.4 & & 56.9 & 13.1 & 35.0 \\ & PaLI-17B (res. 588) & 64.9 & 30.4 & 47.6 & & 64.5 & 15.2 & 39.8\\ & Pix2Struct & 81.6 & 30.5 & 56.0 & & 73.2 & 71.9 & 72.5\\ & \oldmodel{}  & 90.2 & 38.2 & 64.2 & & \textbf{92.3} & \textbf{90.7} & \textbf{91.5}\\ \midrule 
\multirow{4}{*}{\rotatebox[origin=c]{90}{One-shot (OURS)}}
 & \model{}+GPT3 CoT  & 37.3 & 36.5 & 36.9 &    & 31.9 & 51.3 & 41.6 \\ & \model{}+GPT3 SC  & 42.6 & 41.9 & 42.3 &    & 35.0 & 51.6 & 43.3 \\ & \model{}+FlanPaLM CoT & 76.7 & 57.8 & 67.3 &    & 51.3 & 44.9 & 48.1 \\ & \model{}+FlanPaLM SC & 78.8 & 62.2 & 70.5 &   & 57.8 & 50.1 & 53.9 \\ & \model{}+Codex PoT SC & \textbf{91.8} & 61.6 & 76.7 &   & 58.8 & 69.8 & 64.3 \\
& \model{}+FlanPaLM+Codex PoT SC & 91.0 & \textbf{67.6} & \textbf{79.3} &   & 62.2 & 71.0 & 66.6 \\
\bottomrule 
\end{tabular}
}



\caption{Main experimental results on two plot/chart QA benchmarks ChartQA \& PlotQA. ``\colorbox{red!30}{$\ $}'' denotes human-written queries while ``\colorbox{blue!30}{$\ $}'' denotes synthetic queries.
Detailed introduction of the baselines can be found in \Cref{sec:baselines}. The last six rows show \model+LLM results -- the only one-shot setup. CoT denotes chain-of-thought prompting, SC denotes self-consistency, PoT denotes program-of-thought prompting. The best results are achieved for ChartQA by majority voting across joint 10 CoT and 10 PoT predictions.}
\label{tab:main}
\end{table*}


\paragraph{Downstream tasks.} We list the main results on ChartQA \citep{masry-etal-2022-chartqa} and PlotQA \citep{methani2020plotqa}
in \Cref{tab:main}.
We evaluate different \model+LLM setups. We evaluate chain-of-thoughts (CoT)~\citep{wei2022cot} prompts for GPT-3~\cite{brown2020language} (\texttt{text-davinci-003}) and FlanPaLM~\cite{chung2022scaling} (540B). In addition, we use self-consistency (SC)~\cite{wang2022selfconsistency} across 10 predictions. Finally, we use program-of-thoughts (PoT)~\cite{chen2022program} to prompt Codex~\cite{chen2021codex} (\texttt{code-davinci-002}) to generate python snippets that can be subsequently executed by an interpreter to extract an answer.\footnote{We also evaluated PaLM and FlanPaLM for code generation but found Codex to be more likely to write correct code instead of do the reasoning in comment blocks.} Since some reasoning operations are better done by plain language (like computing an argmax) and some by code snippets (like floating point arithmetic), we find optimal results by doing self-consistency across both CoT and PoT predictions.

\model+LLM performs especially strong on the ChartQA human set (denoted with ``\colorbox{red!30}{$\ $}'') which contains complex human written queries. Compared with prior SOTA \textsc{MatCha}, \model+LLM when combined with FlanPaLM and Codex and Self-Consistency (SC) achieves an improvement of 29.4\% (38.2\%$\rightarrow$67.6\%). This is also the best setup for PlotQA. On the heavily synthetic queries from PlotQA v1 and v2 (denoted with ``\colorbox{blue!30}{$\ $}''), \model+LLM models underperform the end-to-end SOTA \textsc{MatCha}.





In summary, \model+LLM significantly outperforms finetuned SOTA on human-written chart QA queries and overall underperforms finetuned SOTA on synthetic QA queries.
We believe it is especially important to achieve good performance on the human set as it is much more diverse and reflects the real-world challenges. The results suggest \model+LLM's strong capability in solving novel human queries unseen in demonstration.
It is also worth emphasizing again that \model+LLM requires much less supervision than the finetuned SOTA methods (one shot vs. tens of thousands of training examples). We will discuss why \model+LLM underperforms on PlotQA in error analysis (\Cref{sec:case_study}). 

Besides one-shot learning, we also experimented with zero- and few-shot inference. We found the models generally fail without demonstration and few-shot has similar performance as one-shot. After the submission of this paper, we experimented with RLHF-ed LLMs such as ChatGPT\footnote{\href{https://openai.com/blog/chatgpt}{openai.com/blog/chatgpt}}, GPT-4 \citep{OpenAI2023GPT4TR}, and Bard\footnote{\href{https://bard.google.com/}{bard.google.com}}, finding that such aligned conversational models are capable of processing the \model-generated tables in a zero-shot manner. This can potentially further improve \model+LLM's performance on academic benchmarks by large margins.





\section{Analyses and Discussions}\label{sec:analyses}

In this section, we first conduct case studies and error analysis in \Cref{sec:case_study} to better understand \model{}' wins and losses against end-to-end methods. Then in~\Cref{sec:ood} we study the performance of \model{} when exposed to out-of-distribution web charts and plots.

\subsection{Case Study and Error Analysis}\label{sec:case_study}

To more concretely demonstrate the strengths and weaknesses of \model+LLM, we present two case studies for the downstream task ChartQA. We compare \model+FlanPaLM using either PoT or CoT.

First, in \Cref{tab:case_study_m1} we show an example demonstrating the benefit of using LLM and prompting techniques for stronger numerical reasoning. While the finetuned SOTA \textsc{MatCha} wrongly predicts the answer, \model+FlanPaLM (using either CoT or PoT) produces accurately the answer.


\begin{table}[ht!]
  \centering
  \small
  \begin{tabular}{c}
    \begin{minipage}{.46\textwidth}
      \frame{\includegraphics[width=\linewidth, height=56mm]{figs/deplot_case_study_m1.png}}
    \end{minipage} 
    \\ \\
    \begin{minipage}[t]{.46\textwidth}
    \textit{\textit{Question:} What is the difference between the sum shares of Indonesia and Ireland and the share of Mauritania?} \\ \\
    \textsc{MatCha}: \textcolor{red}{\textbf{0.07}} \\ \\
    \model+FlanPaLM CoT: Let's find the sum of shares of Indonesia and Ireland: 2.88+2.33=5.21. Let's subtract the share of Mauritania: 5.21-4.15=1.06. The answer is  \textcolor[rgb]{0.4,0.8,0}{\textbf{1.06}}.\\ \\
    \model+Codex PoT:\begin{lstlisting}[language=Python]
indonesia = 2.88
ireland = 2.33
mauritania = 4.15
ans=(indonesia+ireland)-mauritania
    \end{lstlisting}
Compiler output: \textcolor[rgb]{0.4,0.8,0}{\textbf{1.06}}
    \end{minipage}
  \end{tabular}
  \caption{\model{}+LLM shows much stronger numerical reasoning skills than the finetuned \textsc{MatCha}.}\label{tab:case_study_m1}
\end{table}

Second, we show an example where the \model+LLM framework fails in \Cref{tab:error}. The LLMs are unable to accurately identify the ``highest value of the gray bar'' since they do not have information about the color of bars. In \Cref{tab:error}, though \model+FlanPaLM correctly predicted ``Yes'', it is correct for the wrong reason -- FlanPaLM randomly chose the highest value in light blue bars which also happens to be smaller than the average of ``identity theft''. This is a typical failure mode where the query refers to a visual attribute but such attribute is lost in plot-to-table translation. In future work, we plan to develop a table encoding scheme that also considers visual attributes to avoid such errors.

\begin{table}[ht!]
  \centering
  \small
  \begin{tabular}{c}
    \begin{minipage}{.46\textwidth}
      \frame{\includegraphics[width=\linewidth, height=76mm]{figs/deplot_error_analysis_right4wrong_reason.png}}
    \end{minipage} 
    \\ \\
    \begin{minipage}[t]{.46\textwidth}
    \textit{\textit{Question:} Is the average of all the bars in ``identity theft'' greater than the highest value of the gray bar?} \\ \\
  \textsc{MatCha}: \textcolor{red}{\textbf{No}} \\ \\
  \model+FlanPaLM CoT: Let's find the average of all the bars in "Identity theft". The sum of all the bars is 66+17+16 = 99. The average is 99/3 = 33. The highest value of the gray bar is 26. 33 is greater than 26. The answer is \textcolor[rgb]{0.4,0.8,0}{\textbf{Yes}}. \\ \\
\model+Codex PoT:
\begin{lstlisting}[language=Python]
#Identity theft corresponds to row 5
#Numbers on row 5 are [66, 17, 16]
#Highest value of the gray bar is 79
ans = 66 > 79
\end{lstlisting}
Compiler output: \textcolor{red}{\textbf{No}} 
\end{minipage}
  \end{tabular}
  \caption{\model+LLM struggles with queries related to visual attributes such as color (which is lost in plot-to-table translation).}\label{tab:error}
\end{table}






While \model+LLM has surpassed finetuned SOTA on ChartQA, we notice that the picture on PlotQA is different -- \model{} underperforms finetuned SOTA \textsc{MatCha} by a large margin (66.6\% vs. 91.5\%). Through error analysis, we observe that there are two major reasons. First, synthetic queries are highly templatic and covers only restricted types of questions. Models finetuned with thousands of examples can learn to solve such templatic questions, even better than humans do (human ceiling on PlotQA is just 80.5\% compared with \textsc{MatCha} performance of 91.5\%). However, \model+LLMs only learn from one-shot in-context example and thus cannot exploit such bias encoded in the training set. 
The second reason is the loss of information in plot-to-table translation. Synthetic queries are usually highly extractive and include questions asking visual attributes such as color, shape, or direction of objects in a plot. When plots are converted to tables, such information is lost. We plan to also decode visual attributes in future work when training the plot-to-table model.

More successful and failure case analyses are available in \Cref{sec:appendix_more_cases}.

\subsection{Out-of-distribution Analysis}\label{sec:ood}

One limitation of our evaluation setup is that the kind and style of charts that are part of \model{}'s training corpus are in the same domain as those in the evaluation sets from ChartQA and PlotQA.
This raises the question of whether \model{} will generalize to charts sourced from different websites or built using completely different tools.
However, few public resources exist containing both charts and their associated tables. 

In order to estimate the out-of-distribution capabilities of \model{} we annotate 10 charts from the recently released TaTa dataset~\cite{gehrmann2022tata}, sourced from \url{dhsprogram.com}. We skip choropleth maps since none have been seen during training. We find \model{} obtains an average 78\% $\texttt{RMS}_{\text{F1}}$ score in reconstructing the underlying tables. 
We observed two limitations in \model{} which we outline below and can attributed to the nature of the training datasets used. 
First the model could get distracted by adjacent text, such as references to external sources, and it benefited from cropping the chart in advance.
Secondly, \model{} struggled to understand labels or values linked to their corresponding bar/pie section by an arrow.
We will address these issues in future work by making the synthetic data creation pipeline more robust.