\vspace{-0.1cm}

This section demonstrates the effectiveness of our pretrained \model{} model and compares our method to the state of the art.
We first outline our experimental setup in Section~\ref{sec:setup}.
We then present ablation studies in Section~\ref{sec:ablation}.
The comparison to the state of the art in dense video captioning, video paragraph captioning and video clip captioning is presented in Section~\ref{sec:sota}. 
Next, we present results in a new few-shot dense video captioning setting in Section~\ref{sec:fewshot}. 
Finally, we show qualitative results in Section~\ref{sec:qualitative}. 

\subsection{Experimental setup}\label{sec:setup}
\vspace{-0.1cm}


\noindent \textbf{Datasets.} 
For pretraining, following prior work showing the benefits of pretraining on a diverse and large dataset~\cite{zellers2021merlot}, we use the \textbf{YT-Temporal-1B} dataset~\cite{zellers2022merlot}, which includes 18 million narrated videos collected from YouTube.
We evaluate \model{} on three downstream dense video captioning datasets: YouCook2~\cite{youcook2}, ViTT~\cite{huang2020multimodal} and ActivityNet Captions~\cite{krishna2017dense}.
\textbf{YouCook2} has 2K untrimmed videos of cooking procedures.
On average, each video lasts 320s and is annotated with 7.7 temporally-localized sentences.
\textbf{ViTT} consists of 8K untrimmed instructional videos.
On average, each video lasts 250s and is annotated with 7.1 temporally-localized short tags.
\textbf{ActivityNet Captions} contains 20k untrimmed videos of various human activities.
On average, each video lasts 120s and is annotated with 3.7 temporally-localized sentences. 
For video clip captioning, we use two standard benchmarks, \textbf{MSR-VTT}~\cite{xu16msrvtt} and \textbf{MSVD}~\cite{chen2011collecting}.
For all datasets, we follow the standard splits for training, validation and testing.
Note that we only use videos available on YouTube at the time of the work, resulting in 10 to 20\% less videos than in the original datasets.

\noindent \textbf{Implementation details.} 
We extract video frames at 1FPS, and subsample or pad the sequence of frames to  frames where we set .
The text encoder and decoder sequence are truncated or padded to  tokens.
Our model has 314M trainable parameters.
We use the Adam optimizer~\cite{kingma15adam}.
We pretrain our model for 200,000 iterations with a batch size of 512 videos split on 64 TPU v4 chips, which lasts a day.
We sum both pretraining objectives with equal weighting to get our final pretraining loss.
More details are included in Appendix Section~\ref{sec:adddetails}.

\begin{table}[t]
\centering
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{6pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ccc|ccc|ccc}
\toprule
& \multicolumn{2}{c|}{Pretraining input} & \multicolumn{3}{c|}{YouCook2} & \multicolumn{3}{c}{ActivityNet} \\ & Untrimmed & Time tokens & \small{S} & \small{C} & \small{F1} & \small{S} & \small{C} & \small{F1} \\
\midrule
1. & \multicolumn{2}{c|}{\textit{No pretraining}}
& 4.0 & 18.0 & 18.1 
& 5.4 & 18.8 & 49.2 \\ 
2. & \xmark & \xmark
& 5.5 & 27.8 & 20.5
& 5.5 & 26.5 & 52.1 \\ 
3. & \cmark & \xmark
& 6.7 & 35.0 & 23.3 
& 5.6 & 27.4 & 52.2 \\ 
4. & \cmark & \cmark
& \textbf{7.9} & \textbf{47.1} & \textbf{27.3} 
& \textbf{5.8} & \textbf{30.1} & \textbf{52.4} \\ 
\bottomrule
\end{tabular}}
\vspace{-0.3cm}
\caption{\small \textbf{Ablation showing the impact of using untrimmed videos and adding time tokens during pretraining.} When we use untrimmed video-speech inputs, time information from transcribed speech sentence boundaries is integrated via time tokens.}
\vspace{-0.9cm}
\label{table:pretraining}
\end{center}
\end{table}

\noindent \textbf{Evaluation metrics.} 
For captioning, we use
CIDEr~\cite{vedantam2015cider} (C) and METEOR~\cite{banerjee2005meteor} (M).
For dense video captioning, we follow the commonly used evaluation tool~\cite{krishna2017dense} which calculates matched pairs between generated events and the ground truth across IoU thresholds of \{0.3, 0.5, 0.7, 0.9\}, and compute captioning metrics over the matched pairs.
However, these metrics do not take into account the story of the video.
Therefore we also use SODA\_c~\cite{fujita2020soda} (S) for an overall dense video captioning evaluation.
To further isolate the evaluation of event localization, we report the average precision and average recall across IoU thresholds of \{0.3, 0.5, 0.7, 0.9\} and their harmonic mean, the F1 Score.

\subsection{Ablation studies}\label{sec:ablation}
\vspace{-0.1cm}

The default \model{} model predicts both text and time tokens, uses both visual frames and transcribed speech as input, builds on the T5-Base language model, and is pretrained on untrimmed videos from YT-Temporal-1B with both the generative and denoising losses.
Below we ablate the importance of each of these factors on the downstream dense video captioning performance by reporting results on YouCook2 and ActivityNet Captions validation sets. 

\noindent \textbf{Pretraining on untrimmed narrated videos by exploiting transcribed speech sentence boundaries.} 
In Table~\ref{table:pretraining}, we evaluate the effectiveness of our pretraining task formulation that uses untrimmed videos and integrates sentence boundaries of transcribed speech via time tokens.
In contrast, most video clip captioning pretraining methods~\cite{huang2020multimodal, luo2020univilm, seo2022end} use short, trimmed, video-speech segments for pretraining.
We adapt this strategy in our model and find that it indeed yields significant performance improvements over the baseline that uses no video-text pretraining (row 2 vs row 1).
However, larger improvements are obtained by using untrimmed video-speech inputs (row 3 vs row 2).
Moreover, using time tokens to integrate time information from transcribed speech drastically improves performance (row 4 vs row 3).
This shows the benefits of exploiting sentence boundaries of transcribed speech via time tokens and of using untrimmed videos during pretraining.
In Appendix Section~\ref{sec:ablation2}, we show additional ablations to quantify how the performance improves by pretraining on longer narrated videos that contain more speech sentences.

\begin{table}[t]
\centering
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{3pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ccc|cc|ccc|ccc}
\toprule
& \multicolumn{2}{c|}{Finetuning Input} & \multicolumn{2}{c|}{Pretraining loss} & \multicolumn{3}{c|}{YouCook2} & \multicolumn{3}{c}{ActivityNet} \\ 
& Visual & Speech & Generative & Denoising & \small{S} & \small{C} & \small{F1} & \small{S} & \small{C} & \small{F1} \\
\midrule
1. & \cmark & \xmark & \multicolumn{2}{c|}{\textit{No pretraining}}
& 3.0 & 15.6 & 15.4 
& 5.4 & 14.2 & 46.5 \\
2. & \cmark & \cmark & \multicolumn{2}{c|}{\textit{No pretraining}}
& 4.0 & 18.0 & 18.1 
& 5.4 & 18.8 & 49.2 \\ 
3. & \cmark & \xmark & \cmark & \xmark
& 5.7 & 25.3 & 23.5 
& \textbf{5.9} & \textbf{30.2} & 51.8 \\
4. & \cmark & \cmark & \cmark & \xmark
& 2.5 & 10.3 & 15.9 
& 4.8 & 17.0 & 48.8 \\
5. & \cmark & \cmark & \cmark & \cmark
& \textbf{7.9} & \textbf{47.1} & \textbf{27.3} 
& 5.8 & 30.1 & \textbf{52.4} \\ 
\bottomrule
\end{tabular}}
\vspace{-0.3cm}
\caption{\small \textbf{Effect of input modalities and pretraining losses.}}
\vspace{-0.7cm}
\label{table:modalities}
\end{center}
\end{table}

\begin{table}[t]
\centering
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{3pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ccc|ccc|ccc}
\toprule
& \multirow{2}{*}{Captioning} & \multirow{2}{*}{Pretraining}
& \multicolumn{3}{c|}{YouCook2}
& \multicolumn{3}{c}{ActivityNet} \\
& & & \small{Recall} & \small{Precision} &
\small{F1} & \small{Recall} & \small{Precision} &
\small{F1} \\
\midrule
1. & \xmark & \xmark
& 17.8 & 19.4 & 17.7 & 47.3 & 57.9 & 52.0 \\ 
2. & \cmark & \xmark
& 17.2 & 20.6 & 18.1 & 42.5 & \textbf{64.1} & 49.2 \\ 
3. & \xmark & \cmark
& 25.7 & 21.4 & 22.8 & 52.5 & 53.0 & 51.1 \\ 
4. & \cmark & \cmark
& \textbf{27.9} & \textbf{27.8} & \textbf{27.3} & \textbf{52.7} & 53.9 & \textbf{52.4} \\ 
\bottomrule
\end{tabular}}
\vspace{-0.3cm}
\caption{\small \textbf{Effect of joint captioning and localization on the localization performance.}
The variant that does not caption corresponds to a localization-only variant that only predicts time tokens.}
\vspace{-0.7cm}
\label{table:inter}
\end{center}
\end{table}

\begin{table}[t]
\centering
\vspace{-0pt}
\begin{center}
\setlength\tabcolsep{4pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{cc|cc|ccc|ccc}
\toprule
& \multirow{2}{*}{\makecell{\small{Language} \\ \small{Model}}} & \multicolumn{2}{c|}{Pretraining} & \multicolumn{3}{c|}{YouCook2} & \multicolumn{3}{c}{ActivityNet} \\
& & \# Videos & Dataset & \small{S} & \small{C} & \small{F1} 
& \small{S} & \small{C} & \small{F1} \\
\midrule
1. & T5-Small & 15M & YTT
& 6.1 & 31.1 & 24.3 
& 5.5 & 26.5 & 52.2 \\
2. & T5-Base &  & 
& 4.0 & 18.0 & 18.1
& 5.4 & 18.8 & 49.2 \\ 
3. & T5-Base & 15K & YTT
& 6.3 & 35.0 & 24.4 
& 5.1 & 24.4 & 49.9 \\ 
4. & T5-Base & 150K & YTT
& 7.3 & 40.1 & 26.7
& 5.4 & 27.2 & 51.3 \\ 
5. & T5-Base & 1M5 & YTT
& 7.8 & 45.5 & 26.8
& 5.6 & 28.7 & 52.2 \\ 
6. & T5-Base & 1M & HTM
& \textbf{8.3} & \textbf{48.3} & 26.6 
& \textbf{5.8} & 28.8 & \textbf{53.1} \\ 
7. & T5-Base & 15M & YTT
& 7.9 & 47.1 & \textbf{27.3} 
& \textbf{5.8} & \textbf{30.1} & 52.4 \\ 
\bottomrule
\end{tabular}}
\vspace{-0.3cm}
\caption{\small\textbf{Effect of language model size and pretraining data.} HTM: HowTo100M~\cite{miech19howto100m}, YTT: YT-Temporal-1B~\cite{zellers2022merlot}.} 
\vspace{-0.9cm}
\label{table:scale}
\end{center}
\end{table}

\noindent \textbf{Input modalities and pretraining objectives.} 
In Table~\ref{table:modalities}, we analyze the importance of input modalities and pretraining tasks on the downstream dense video captioning performance.
The model with visual inputs only (no transcribed speech as input) benefits significantly from pretraining with the generative objective (row 3 vs row 1).
This shows the effectiveness of using the transcribed speech as a proxy annotation for dense video captioning pretraining.
However, this model is pretrained with visual inputs only and its performance largely drops when it is finetuned with both visual and transcribed speech inputs (row 4 vs row 3).
With both modalities, adding the denoising loss strongly benefits our model (row 5 vs rows 4 and 2).
We conclude that the denoising objective benefits multi-modal reasoning.

\noindent \textbf{Effect of captioning on localization.}
In Table~\ref{table:inter}, we compare the event localization performance of our model with a localization-only variant that only predicts event boundaries.
We find that the model that jointly predicts event boundaries and captions localizes better and benefits more from pretraining than the localization-only baseline (row 4 vs row 3), which demonstrates the importance of contextualizing the noisy timestamps of the transcribed speech with the speech semantic content during pretraining.

\noindent \textbf{Model size and pretraining data.}
In Table~\ref{table:scale}, we show that the language model size has a great importance on the performance, as the model with T5-Base outperforms its variant with T5-Small (row 7 vs row 1).
We also evaluate the importance of the size of the pretraining dataset of narrated videos by constructing subsets such that larger subsets include the smaller ones.
We find that scaling up the size of the pretraining dataset is beneficial, and that our pretraining method yields important benefits when only using 150K narrated videos for pretraining (row 4).
We further show that our pretraining method generalizes well to the HowTo100M dataset~\cite{miech19howto100m}.
The model pretrained on HowTo100M (row 6) actually achieves best results on YouCook2, as these datasets are from a similar domain.
Finally, we ablate the importance of the size and pretraining of the visual backbone in Appendix Section~\ref{sec:ablation2}.


\begin{table}[t]
\begin{center}
\setlength\tabcolsep{4pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{l|c|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone}
& \multicolumn{3}{c|}{YouCook2 (val)}
& \multicolumn{3}{c|}{ViTT (test)}
& \multicolumn{3}{c}{ActivityNet (val)} \\
& & \small{S} & \small{C} & \small{M}
& \small{S} & \small{C} & \small{M} 
& \small{S} & \small{C} & \small{M} \\
\midrule
MT~\cite{zhou2018end} & TSN
& --- & 6.1 & 3.2
& --- & --- & ---
& --- & 9.3 & 5.0 \\
ECHR~\cite{wang2020event} & C3D
& --- & --- & 3.8
& --- & --- & ---
& 3.2 & 14.7 & 7.2 \\
PDVC~\cite{wang2021end} & TSN
& 4.4 & 22.7 & 4.7
& --- & --- & ---
& 5.4 & 29.0 & 8.0 \\
PDVC~\cite{wang2021end} & CLIP
& 4.9 & 28.9 & 5.7
& --- & --- & ---
& \textbf{6.0} & 29.3 & 7.6 \\
UEDVC~\cite{zhang2022unifying} & TSN
& --- & --- & ---
& --- & --- & ---
& 5.5 & --- & --- \\
E2ESG~\cite{zhu2022end} & C3D
& --- & 25.0* & 3.5
& --- & 25.0 & 8.1
& --- & --- & ---- \\
\model{} (Ours) & CLIP
& \textbf{7.9} & \textbf{47.1} & \textbf{9.3}
& \textbf{13.5} & \textbf{43.5} & \textbf{8.5}
& 5.8 & \textbf{30.1} & \textbf{8.5} \\
\bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small Comparison to the state of the art for dense video captioning. * Results provided by the authors.  Results of our experiments using the official codebase.}
\label{table:sotac}
\vspace{-0.7cm}
\end{center}
\end{table}


\begin{table}[t]
\begin{center}
\setlength\tabcolsep{5pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{l|c|cc|cc|cc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone}
& \multicolumn{2}{c|}{YouCook2 (val)} 
& \multicolumn{2}{c|}{ViTT (test)}
& \multicolumn{2}{c}{ActivityNet (val)} \\
& & \small{Recall} & \small{Precision}
& \small{Recall} & \small{Precision}
& \small{Recall} & \small{Precision} \\
\midrule
PDVC~\cite{wang2021end} & TSN
& --- & ---
& --- & ---
& 55.4 & 58.1 \\
PDVC~\cite{wang2021end} & CLIP
& --- & ---
& --- & ---
& 53.2 & 54.7 \\
UEDVC~\cite{zhang2022unifying} & TSN
& --- & ---
& --- & ---
& \textbf{59.0} & \textbf{60.3} \\
E2ESG~\cite{zhu2022end} & C3D
& 20.7* & 20.6*
& 32.2* & 32.1*
& --- & --- \\
\model{} (Ours) & CLIP
& \textbf{27.9} & \textbf{27.8}
& \textbf{42.6} & \textbf{46.2}
& 52.7 & 53.9 \\
\bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small Comparison to the state of the art for event localization. * Results provided by the authors.  Results of our experiments using the official codebase.}  
\label{table:sotal}
\end{center}
\vspace{-0.9cm}
\end{table}

\subsection{Comparison to the state of the art}\label{sec:sota}
\noindent \textbf{Dense video captioning.} 
In Table~\ref{table:sotac}, we compare our approach to state-of-the-art dense video captioning methods using cross-entropy training~\footnote{We do not include methods directly optimizing the test metric~\cite{deng2021sketch, mun2019streamlined}.} on the YouCook2, ViTT and ActivityNet Captions datasets.
\model{} sets new state of the art on all three datasets.
In particular, our method improves the CIDEr metric by 18.2 and 0.8 points on YouCook2 and ActivityNet Captions over PDVC.
Our method also outperforms E2ESG~\cite{zhu2022end} which uses in-domain text-only pretraining on Wikihow.
These results demonstrate the strong dense event captioning ability of our pretrained \model{} model.

\begin{figure*}[t]
\centering
\includegraphics[clip, trim=0mm 9cm 0mm 0mm, width=1\linewidth]{figures/cvpr23_qualitative2}
\vspace{-0.9cm}
\caption{\small Example of dense event captioning predictions of \model{} on ActivityNet Captions validation set, compared with ground-truth.
}
\vspace{-0.6cm}
\label{fig:qualitative}
\end{figure*}

\noindent \textbf{Event localization.} 
In Table~\ref{table:sotal}, we evaluate the event localization performance of our dense video captioning model in comparison with prior work.
On both YouCook2 and ViTT, \model{} outperforms prior work~\cite{zhu2022end} tackling dense video captioning as a single sequence generation task.
However, our model underperforms compared to PDVC~\cite{wang2021end} and UEDVC~\cite{wang2021end} on ActivityNet Captions.
We emphasize that our approach integrates less prior knowledge about temporal localization than both these approaches, which include task specific components such as event counters~\cite{wang2021end} or separately train a model for the localization subtask~\cite{zhang2022unifying}.


\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|c|cc|cc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} 
& \multicolumn{2}{c|}{YouCook2 (val)}
& \multicolumn{2}{c}{ActivityNet (val-ae)} \\
& & \small{C} & \small{M} & \small{C} & \small{M} \\
\midrule
\textit{With GT Proposals} & & & \\
VTransformer~\cite{zhou2018end} & V (ResNet-200) + F & 32.3 & 15.7 & 22.2 & 15.6 \\
Transformer-XL~\cite{dai2019transformer} & V (ResNet-200) + F & 26.4 & 14.8 & 21.7 & 15.1 \\
MART~\cite{lei2020mart} & V (ResNet-200) + F & 35.7 & 15.9 & 23.4 & 15.7 \\
GVDSup~\cite{zhou2019grounded} & V (ResNet-101) + F + O & --- & --- & 22.9 & 16.4 \\
AdvInf~\cite{park2019adversarial} & V (ResNet-101) + F + O & --- & --- & 21.0 & 16.6 \\
PDVC~\cite{wang2021end} & V + F (TSN)
& --- & --- & 27.3 & 15.9 \\
\hline
\textit{With Learnt Proposals} & & & \\
MFT~\cite{xiong2018move} & V + F (TSN)
& --- & --- & 19.1 & 14.7 \\
PDVC~\cite{wang2021end} & V + F (TSN)
& --- & --- & 20.5 & 15.8 \\
PDVC~\cite{wang2021end} & V (CLIP)
& --- & --- & 23.6 & 15.9 \\
\model{} (Ours) & V (CLIP)
& \textbf{50.1} & \textbf{24.0} & \textbf{28.0} & \textbf{17.0} \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.6cm}
\caption{\small Comparison to the SoTA for video paragraph captioning.
 Results of our experiments using the official codebase. V/F/O refers to visual/flow/object features.}  
\vspace{-0.4cm}
\label{table:sotapara}
\end{table}


\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\makecell{\small{Trained} \\ \small{Parameters}}} & \multirow{2}{*}{\makecell{\small{Pretraining} \\ \small{Data}}}
& \multicolumn{2}{c|}{MSR-VTT (test)}
& \multicolumn{2}{c}{MSVD (test)} \\
& & & \small{C} & \small{M}
& \small{C} & \small{M} \\
\midrule
ORG-TRL~\cite{zhang2020object} & --- &  & 50.9 & 28.8 & 95.2 & 36.4 \\
SwinBERT~\cite{lin2022swinbert} & 229M &  & 53.8 & 29.9
& 120.6 & 41.3 \\
\model{} (Ours) & 314M & 
& 57.2 & 30.0
& 120.3 & 41.4 \\
\hline
MV-GPT~\cite{seo2022end} & 354M & HowTo100M
& 60.0 & 29.9
& --- & --- \\
\model{} (Ours) & 314M & HowTo100M
& 61.5 & 30.4 
& 140.6 & 44.5 \\
\hline
\model{} (Ours) & 314M & YT-Temporal-1B
& \textbf{64.6} & \textbf{30.8}
& \textbf{146.2} & \textbf{45.3} \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.6cm}
\caption{\small Comparison to the SoTA for video clip captioning.
* indicates results re-evaluated by the same evaluation toolkit.
}
\vspace{-0.4cm}
\label{table:sotaclip}
\end{table}

\noindent \textbf{Video paragraph captioning.} 
In Table~\ref{table:sotapara}, we compare our approach to state-of-the-art video paragraph captioning methods on the YouCook2 and ActivityNet Captions datasets.
\model{} outperforms all prior methods on both datasets, including the ones using ground-truth event boundary proposals at inference time~\cite{dai2019transformer, lei2020mart, zhou2018end, zhou2019grounded, wang2021end, park2019adversarial}, showing strong video paragraph captioning ability.

\noindent \textbf{Video clip captioning.} 
In Table~\ref{table:sotaclip}, we compare our approach to state-of-the-art video clip captioning methods on the MSR-VTT and MSVD datasets.
\model{} improves over prior methods in their respective pretraining data setting while using a comparable number of trained parameters.
We conclude that our pretrained \model{} model generalizes well to the standard video clip captioning setting.

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{6pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lc|ccc|ccc|ccc}
\toprule
& \multirow{2}{*}{Data}
& \multicolumn{3}{c|}{YouCook2}
& \multicolumn{3}{c|}{ViTT}
& \multicolumn{3}{c}{ActivityNet} \\
& & \small{S} & \small{C} & \small{M}
& \small{S} & \small{C} & \small{M} 
& \small{S} & \small{C} & \small{M} \\
\midrule
1. & 1\%
& 2.4 & 10.1 & 3.3
& 2.0 & 7.4 & 1.9
& 2.2 & 6.2 & 3.2 \\
2. & 10\%
& 3.8 & 18.4 & 5.2
& 10.7 & 28.6 & 6.0
& 4.3 & 20.0 & 6.1 \\
3. & 50\%
& 6.2 & 32.1 & 7.6
& 12.5 & 38.8 & 7.8
& 5.4 & 27.5 & 7.8 \\
4. & 100\%
& \textbf{7.9} & \textbf{47.1} & \textbf{9.3}
& \textbf{13.5} & \textbf{43.5} & \textbf{8.5}
& \textbf{5.8} & \textbf{30.1} & \textbf{8.5} \\
\bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\caption{\small \textbf{Few-shot dense event captioning}, by finetuning \model{} using a small fraction of the downstream training dataset.}  
\label{table:fewshot}
\end{center}
\vspace{-1cm}
\end{table}

\subsection{Few-shot dense video captioning}\label{sec:fewshot}
To further evaluate the generalization capabilities of our pretrained \model{} model, we propose a new few-shot dense video captioning setting where we finetune \model{} using only a fraction of the downstream training dataset.
From Table~\ref{table:fewshot}, we observe important improvements when using 10\% compared to 1\% of training data (row 2 vs 1).
In Appendix Section~\ref{sec:fewshot2} we further show that pretraining is essential in this few-shot setting.

\vspace{-0.1cm}
\subsection{Qualitative examples}\label{sec:qualitative}
In Figure~\ref{fig:qualitative}, we show an example of dense event captioning predictions from \model{}.
This example shows that our model can predict meaningful event boundaries and captions, and that the predicted captions and boundaries differ considerably from the transcribed speech input (showing the importance of the visual tokens in the input).
More examples are provided in Appendix Section~\ref{sec:addquali}.