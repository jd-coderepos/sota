

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bbm}
\usepackage{marvosym}
\usepackage[accsupp]{axessibility}  


\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\newcommand{\squishlist}{
 \begin{list}{}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{1pt}
     \setlength{\topsep}{1pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{
  \end{list}  }

\def\cvprPaperID{9804} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Class-Aware Contrastive Semi-Supervised Learning}

\renewcommand{\thempfootnote}{\arabic{mpfootnote}}
\author{Fan Yang\thanks{Equal contribution.} \hspace{0.3cm} 
Kai Wu\footnotemark[\value{footnote}] \hspace{0.3cm} 
Shuyi Zhang \hspace{0.3cm} 
Guannan Jiang \hspace{0.3cm}
Yong Liu \hspace{0.3cm} \\
Feng Zheng\thanks{Corresponding author.} \hspace{0.3cm}
Wei Zhang \hspace{0.3cm}
Chengjie Wang\footnotemark[\value{footnote}] \hspace{0.3cm}
Long Zeng\\
Tencent Youtu Lab \hspace{0.3cm} Tsinghua University \hspace{0.3cm} Southern University of Science and Technolog \hspace{0.3cm} CATL\\
{\tt\small fan-yang20@mails.tsinghua.edu.cn, \{lloydwu, shuyizhang, choasliu, jasoncjwang\}@tencent.com} \\
{\tt\small zfeng02@gmail.com, \{jianggn, zhangwei\}@catl.com, zenglong@sz.tsinghua.edu.cn}
}
\maketitle

\begin{abstract}
Pseudo-label-based semi-supervised learning (SSL) has achieved great success on raw data utilization. However, its training procedure suffers from confirmation bias due to the noise contained in self-generated artificial labels. Moreover, the model's judgment becomes noisier in real-world applications with extensive out-of-distribution data. To address this issue, we propose a general method named Class-aware Contrastive Semi-Supervised Learning (CCSSL), which is a drop-in helper to improve the pseudo-label quality and enhance the model's robustness in the real-world setting. Rather than treating real-world data as a union set, our method separately handles reliable in-distribution data with class-wise clustering for blending into downstream tasks and noisy out-of-distribution data with image-wise contrastive for better generalization. Furthermore, by applying target re-weighting, we successfully emphasize clean label learning and simultaneously reduce noisy label learning. Despite its simplicity, our proposed CCSSL has significant performance improvements over the state-of-the-art SSL methods on the standard datasets CIFAR100~\cite{krizhevsky2009learning} and STL10~\cite{coates2011analysis}. On the real-world dataset Semi-iNat 2021~\cite{su2021semi_iNat}, we improve FixMatch~\cite{sohn2020fixmatch} by 9.80\% and CoMatch~\cite{li2021comatch} by 3.18\%. Code is available  https://github.com/TencentYoutuResearch/Classification-SemiCLS.

\end{abstract}

\section{Introduction}

\begin{figure}
\centering
\begin{minipage}[b]{1.0\linewidth}
    \centering
    \subfloat[][Real-World Data With In-Distribution and Out-of-Distribution Data]{\label{fig1_a}\includegraphics[width=1.0\linewidth]{jpgs/fig1_a.png}}
\end{minipage} 

\medskip
\begin{minipage}[b]{1.0\linewidth}
    \centering
    \subfloat[justification=centering][Pseudo-Label-Based SSL]{\label{fig1_b}\includegraphics[width=0.5\linewidth]{jpgs/fig1_b.png}} 
    \subfloat[justification=centering][Class-Aware Contrastive SSL (ours)]{\label{fig1_c}\includegraphics[width=0.5\linewidth]{jpgs/fig1_c.png}} 
\end{minipage}


\caption{Intuition graph for class-aware contrastive semi-supervised learning.~\subref{fig1_a} represents real-world unlabeled data containing in-distribution and out-of-distribution data (unbalanced distribution or unknown classes). Unlike pseudo-label-based semi-supervised learning in~\subref{fig1_b}, which wrongly clusters noisy out-of-distribution data, CCSSL in~\subref{fig1_c} reduces noise by image-level contrastive learning on out-of-distribution data while maintaining the class-aware clustering ability for in-distribution data.}
\label{fig:intuitive}
\end{figure}

\label{sec:intro}
Raw data utilization is becoming a research focal point for its high accessibility and high affordability. The definition of raw data is the union set of in-distribution data (known classes and balanced distribution) and out-of-distribution data~\cite{xu2021stable} (unknown classes or unbalanced distribution), as shown in \cref{fig:intuitive}. For in-distribution datasets, semi-supervised learning (SSL) has achieved excellent performance with the help of pseudo labeling~\cite{sohn2020fixmatch}~\cite{berthelot2019mixmatch}~\cite{berthelot2019remixmatch}~\cite{hu2021simple}. The primary process for the pseudo-label based SSL is iterative training: 1) creating self-generated pseudo labels on raw data, 2) training on pseudo labels, 3) repeating 1 and 2. The underlying assumption for training on pseudo labels is that the distribution of the labeled data is close to the unlabeled, and the unlabeled dataset does not contain any novel categories. This assumption often does not hold in the real-world applications with extensive out-of-distribution data, which contains unbalanced distribution or unknown classes. SSL's training and labeling loop collapses in the real-world because of the enormous noise introduced by pseudo labeling on out-of-distribution data. The confirmation bias~\cite{arazo2020pseudo} induced by noisy pseudo labels deteriorates SSL performance by a large margin~\cite{su2021realistic}.

SSL has used many techniques to alleviate the confirmation bias and achieved state-of-the-art results on standard in-distribution benchmarks, like CIFAR~\cite{krizhevsky2009learning} and STL10~\cite{coates2011analysis}. Some methods achieve this goal by using the model's self-correcting capability. In~\cite{sohn2020fixmatch}~\cite{berthelot2019mixmatch}~\cite{berthelot2019remixmatch}~\cite{sohn2020fixmatch}, a high confidence threshold is used to filter incorrectly pseudo-labeled data. ~\cite{zhou2021instant}~\cite{liu2021unbiased}~\cite{tang2021humble} use a self-ema teacher to generate pseudo labels with the assumption that the weighted averaged model produces more stable and less noisy predictions. Some methods~\cite{taherkhani2021self}~\cite{wang2021data} try to narrow the distribution gap between raw data and labeled data by predictions' uncertainty. However, justifying a model's prediction by its output without introducing other information still suffers from confirmation bias~\cite{nassar2021labels}. Especially on the real-world data with unbalanced distribution or unknown classes, the effect of a model's self-correcting is facing a huge challenge. By explicitly introducing contrastive information, our CCSSL alleviates the confirmation bias in the feature space and shows great de-noise ability on the real-world data.

Another trend is using prior or posterior information to co-rectify a model's predictions. Both~\cite{he2020momentum}~\cite{chen2020simple} use a pre-trained model as a universal prior information. In~\cite{nassar2021labels}, by knowledge embedding graph, semantic information is used to regularize feature learning. Although the above methods are proved helpful, fixed prior is hard to be blended into downstream tasks, resulting in inferior performance in practice.~\cite{li2021comatch} utilizes posterior information by constructing a prediction graph and introducing contrastive learning from self-supervised learning (Self-SL). However, directly combining image-level feature repulsion interferes with SSL's class-clustering ability. To solve this conflict, in our proposed CCSSL, a class-aware contrastive module is specifically explored for reliable in-distribution data clustering and noisy out-of-distribution data contrasting to better integrate with downstream classification tasks. 

To this end, we propose a class-aware contrastive semi-supervised learning (CCSSL) method. CCSSL consists of a semi-supervised module and a class-aware contrastive module. Any end-to-end pseudo-label-based SSL can replace the semi-supervised module to benefit from the class-aware contrastive module's confirmation bias alleviation ability. Unlike self-correcting methods that try to alleviate noise by model's own output, CCSSL regularizes the training process by introducing information in the feature space. As shown in the \cref{fig:intuitive}, CCSSL constructs the feature space with high dimensional vectors from two strong augmented views. Rather than directly combining contrastive learning, CCSSL uses class-aware clustering on in-distribution data to maintain SSL's clustering ability and image-level contrasting on out-of-distribution data for noise alleviation. Furthermore, to reduce the implicit noise introduced from the class-aware clustering, we incorporate a target re-weighting module that emphasizes learning on high probable in-distribution samples and reduces the effect of uncertain noisy samples. We found that with the help of the re-weighting module, the prediction confidence of learned models is robust enough for roughly discriminating in-distribution and out-of-distribution data. 

Our contributions can be described in three folds:
\squishlist
\item We propose a novel SSL learning method CCSSL, which takes advantage of both SSL's effective learning and Self-SL's noise alleviation ability by class-wise clustering on in-distribution data and image-wise contrasting on out-of-distribution data.  

\item CCSSL is a drop-in helper that can improve upon any end-to-end pseudo-label-based SSL method for confirmation bias alleviation and faster convergence. CCSSL's noise alleviation capability makes SSL methods more practical in the real-world setting.  

\item We thoroughly analyzed the effect of CCSSL with various state-of-the-art SSL methods on both in-distribution data and real-world data. By simply combining CCSSL, current state-of-the-art SSL methods can be further improved.
\squishend

\begin{figure*}
    \centering
    \small
      \includegraphics[width=\textwidth, height=\linewidth, keepaspectratio]{jpgs/method.png}
    \caption{Framework of our proposed CCSSL. Given a batch of unlabeled images, the weakly augmented views will go through a semi-supervised module that can be borrowed from any pseudo-label-based SSL to generate model predictions. With pseudo labels, we make a supervised contrastive matrix with only class-level information. Then, the class-aware contrastive matrix is formed by image-level contrasting on out-of-distribution data to reduce confirmation bias. By applying a re-weighting module, we focus learning on clean data and get the final target matrix. Besides, feature affinity matrix is made by two strong augmented views. Class-aware contrastive module for unlabeled data is formulated by minimizing cross-entropy between the affinity matrix and the target matrix.}
    \label{fig:method}
\end{figure*}

\section{Related Work}
\label{sec:related}
In this section, we describe the recent trends of deep learning based SSL and contrastive based Self-SL. Then we analyze specific methods aiming at using on the real-world setting. 

\subsection{Semi-Supervised Learning}
Deep learning based SSL mainly consists of pseudo-labeling and consistency regularization. Pseudo-label-based methods utilize unlabeled data by training on self-generated predictions. The self-training process validates the model itself and can also be named as self-confirming~\cite{dictionary2002merriam}. However, using pseudo labels suffers confirmation bias~\cite{arazo2020pseudo} because it is easy to overfit incorrect predictions during training. Both ~\cite{sohn2020fixmatch}~\cite{berthelot2019mixmatch} use high confidence predictions to filter noisy data. ~\cite{iscen2019label} makes a detour by label propagation to spread labeled data distribution on unlabeled data. Consistency based methods~\cite{jeong2019consistency}~\cite{tang2021humble}~\cite{sohn2020fixmatch} aim to produce consistent predictions on different image views based on the manifold assumption that different views of the same image should lie on the same point in the high dimensional space. ~\cite{sohn2020fixmatch}~\cite{tarvainen2017mean} create different views of an image by strong augmentations to simulate image perturbations of an object. Lately, pseudo-label-based consistency training~\cite{sohn2020fixmatch}~\cite{berthelot2019mixmatch}~\cite{berthelot2019remixmatch}, which use a weak augmentation for creating pseudo labels and a strong augmentation for consistency training, has dominated and achieved excellent performance gain. However, the research on the real-world setting with extensive out-of-distribution data has been less studied. With evaluating in-distribution data and real-world data, we show CCSSL's ability of alleviating confirmation bias in practical.  

\subsection{Self-Supervised Learning}
Self-SL has a long story for learning a universal representation without supervision. ~\cite{hinton2002training}~\cite{hyvarinen2005estimation} build the base for contrastive learning by proposing and refining the noise contrastive estimation (NCE). Based on NCE, ~\cite{oord2018representation} proposes infoNCE which inspires learning representations to discriminate different views of a sample among other samples. Moco~\cite{he2020momentum} and simCLR~\cite{chen2020simple} train a siamses network~\cite{chen2021exploring} to learn image-wise feature by pulling similar (positive) samples and pushing apart different (negative) samples~\cite{chen2021empirical}. Inspired by contrastive learning, we formulate positive pairs and negative pairs in CCSSL to regularize feature training. However, directly combining contrastive learning ignores the internal conflict between image-level feature repulsion and SSL's class-level feature clustering. By incorporating class-level information, CCSSL can seamlessly blend into downstream tasks.  

\subsection{Real-World Unlabeled Data Learning}
Real-world unlabeled data usually contains out-of-distribution data with unbalanced class distribution or unknown classes. Current pseudo-label-based SSL suffers from confirmation bias by incorrect pseudo-labeling on similar unknown classes or blurry images from out-of-distribution data, resulting in worse performance than supervised baseline~\cite{su2021realistic}. Open set classification is a subfield of real-world unlabeled data learning focusing on novel categories.~\cite{guo2020safe} keeps tracking the effect of the supervised learning model to prevent performance hazards. By incorporating deep-construction-based representation learning,~\cite{yoshihashi2019classification} learns to distinguish unknowns from knowns. Open set classification methods are proven to be effective on handwriting datasets or self-constructed datasets but seldom generalize on real-world data.~\cite{su2021realistic} conducts a systematic evaluation of several recently proposed SSL techniques on a real-world setting by two challenging datasets and shows that recent SSL's performance pales on real-world datasets in comparison to in-distribution standard datasets. Therefore, this paper further explores the real-world setting and narrows the gap between research and real-world applications by confirmation bias alleviation for out-of-distribution data. 
\section{Method}

\label{sec:method}
\subsection{Problem Definition}
In the setting of a semi-supervised classification task, let  be a batch of labeled data , where  is the th image and  represents its one-hot label, and  be a batch of unlabeled data , where  is a hyperparameter that determines the relative ratio of   and . In the real world, unlabeled data  contains in-distribution data  and out-of-distribution data ,  which makes pseudo labels more susceptible to confirmation bias~\cite{nassar2021labels}.

\subsection{Framework}
Our proposed CCSSL consists of a replaceable semi-supervised module and a class-aware contrastive module, as in ~\cref{fig:method}. Our method focuses on introducing class-aware contrastive information to alleviate confirmation bias for semi-supervised module especially in real-world setting. Any pseudo-label-based SSL can be used as the semi-supervised module to enjoy the benefit. The notations are as follows:

\textbf{Data augmentation} provides different views of a single image. For a labeled image , a weak augmentation is used for supervised learning. For an unlabeled image , we apply a weak augmentation  and two strong augmentations . The  and one  are sent to semi-supervise module, while all  are used for class-aware contrastive learning.
        
\textbf{Encoder}  is used to extract representation  for a given input .


\textbf{Semi-Supervised module} can be replaced by any pseudo-label based semi-supervised learning method. The module generates pseudo labels during training and outputs the final prediction at inference time by a classification head . We use FixMatch~\cite{sohn2020fixmatch} in the method for simplicity.
    
\textbf{Class-Aware contrastive module} constructs an embedding space by mapping the high-dimensional representation  to a low-dimensional embedding  through the 
    projection head . We propose the class-aware contrastive learning in the embedding space to improve the robustness for out-of-distribution data and compatibleness with downstream tasks. 
    
\subsection{Semi-Supervised Module} \label{method:semi-sup-module}
Following the general form of pseudo-label-based SSL, the module contains a supervised loss  and an unsupervised loss .  is the same as~\cite{sohn2020fixmatch}.   relies on pseudo labels  from the model's prediction  on the weak view of image . Only pseudo labels with high confidence , where , will be retained for consistency training. H means cross entropy:



However, the manual set threshold  is difficult to guarantee the accuracy of pseudo labels , especially on real-world setting with out-of-distribution data. The phenomenon of overfitting on self-generated labels is called confirmation bias~\cite{arazo2020pseudo}, and it can hardly be alleviated by the model itself during training. Therefore, we propose a novel class-aware contrastive learning method, which introduces unbiased information in the feature space. 

\subsection{Class-Aware Contrastive Module} \label{method:ClassAwareContrastive}

In \cref{fig:method}, we seamlessly integrate clustering and contrasting in the feature space and apply re-weighting for attentive training. Given unlabeled images in a batch, we separate images into two parts based on the likelihood of in-distribution and out-of-distribution, in which  is used as the threshold for separation from the model's prediction confidence.  For highly possible in-distribution data , we apply class-aware clustering in the feature space to seamlessly blend into downstream tasks. For out-of-distribution data , we follow contrastive learning mechanism~\cite{chen2020simple} by taking a image's views as positive samples and others as negative. Then we apply foreground probability re-weighting for each sample to focus learning on high confidence clean data. Next, we will show how to formulate the method step by step. 

\textbf{Contrastive learning} aims to learn a universal prior information for downstream tasks. As in~\cite{he2020momentum}~\cite{chen2020simple}, we randomly sample a minibatch of  images and each image   has two strong augmentations  which are used to extract features  through encoder . Then,  is mapped to obtain a square normalized low-dimensional embedding  by the projection head. Finally, we get an embedding's affinity matrix  by the dot product of embeddings  with a temperature factor , as follow:  

Then, a constrastive matrix  for loss calculation with  is formulated as:


The self-supervised contrastive loss InfoNCE~\cite{oord2018representation} takes the following format:


where  is from the other  of the same image as . The ideal solution for contrastive learning is to push each image far from others and pull same image views closer in the feature space. The property is effective to alleviate noise on the out-of-distribution data. However, it is not compatible with the classification task which requires to cluster images at the class level. To solve the conflict between contrastive learning and downstream tasks, we formulate the class-aware contrastive module to take advantages of both Self-SL's generalizability and SSL's efficient training.

\textbf{Class-Aware Contrastive learning} considers class-aware information for in-distribution data while alleviating noise for out-of-distribution data. Inspired by~\cite{khosla2020supervised}, we consider the embeddings  from the same category as positive pairs instead of the same image with the help of the pseudo-labels  from the semi-supervised module to form supervised contrastive matrix  in \cref{fig:method}.  However, class-wise clustering by pseudo-labels is unsafe to use because a model's prediction may contain a large amount of noises especially for out-of-distribution data, as explained in \cref{sec:intro}. Therefore, we introduce contrastive learning for noise regularization. We use  as the threshold for clustering and contrasting in the feature space, and then  is tranformed to class-aware contrastive matrix  as shown in \cref{fig:method}.  For model's prediction , we assume the images have a high probability to be in-distribution data  and should be pulled closer with the same class. For , our formula degenerates to contrastive learning where only  of the same image are positive pairs. Each element  in  has the following format: 

where  and  are indices of z for augmented sample.  denotes the pseudo label confidence score of the th augmented views. Although class-aware contrastive is theoretically functional, directly using   to judge the cleanness of data is problematic since images just above the  might be out-of-distribution data too. To solve this issue, we propose a foreground re-weighting module to reduce the noise effect on out-of-distribution data. 

\textbf{Foreground Re-weighting} is used to further emphasize training on in-distribution data with high confidence and weaken the potential bias brought from out-of-distribution data. We formulate the final target matrix  by re-weight  with the foreground probability from model's prediction on . Simply by multiplying confidence  of two samples, we get a re-weighting factor , where  and   represent the indices of row and column in , respectively. Each element  of  is defined as follows:



The loss of the class-aware contrastive module  is formulated by minimizing the cross-entropy between  in \cref{eq:dotproduct} and . The loss of  can be defined as:


where  denotes cross entropy and  represents indices of the views from other images of the same class with confidence .  denotes its cardinality and  represents all positive pairs.  as below:



where  is the embedding of  rather than  from the same image. We calcualte the final total loss \cref{eq:finalloss} using a weighted sum of supervised loss , semi-supervised loss  and class-aware contrastive loss .  and  are from \cref{method:semi-sup-module}.  and  are the weights for semi-supervised loss and class-aware contrastive loss, respectively.


\subsection{Applications}

CCSSL method can be used with any pseudo-label-based SSL to alleviate confirmation bias in the training process. Applying CCSSL to SSL methods is simple. We applied the CCSSL to the mainstream SSL methods, like MixMatch\cite{berthelot2019mixmatch}, FixMatch\cite{sohn2020fixmatch} and CoMatch\cite{li2021comatch}. MixMatch and FixMatch are based on pseudo-labels and have a strong augmentation  for unlabeled data learning. We create another  for class-aware contrastive module, which trains parallelly with the original network. CoMatch also incorporates contrastive learning, but no class-wise clustering is involved. Since CoMatch already has two , we directly use them for class-aware contrastive module.  Our experiments in \cref{sec:experiments} show CCSSL not only improves the performance for SSL methods but also speeds up the training process. 



\begin{table*}[t]
\centering
\small

\begin{tabular}{c|ccc|ccc|c}
\toprule
    \multirow{2}{*}{Method} &
    \multicolumn{3}{c|}{CIFAR100} & 
    \multicolumn{3}{c|}{CIFAR10} & STL10 \\
\cline{2-8}
 & 400 & 2500 & 10000 & 40 & 250 & 4000 &  \\
\midrule
 Mixmatch~\cite{berthelot2019mixmatch} & 32.39±1.32 & 60.06±0.37 & 71.69±0.33 & 52.46±11.5 & 88.95±0.86 & 93.58±0.10 & 38.02±8.29 \\
 ReMixMatch~\cite{berthelot2019remixmatch}& 55.72±2.06 & 72.57±0.31 & 76.97±0.56 & 80.90±9.64 & 94.56±0.05 & 95.28±0.13 & - \\
 SSWPL~\cite{taherkhani2021self} & - & 73.48±0.45 & 79.12±0.85 & - & - & - & - \\
LaplaceNet~\cite{sellars2021laplacenet} & - & 68.36±0.02 & 73.40±0.23 & - & - & 95.35±0.07 & - \\
 CoMatch~\cite{li2021comatch} & 58.11±2.34 & 71.63±0.35 & 79.14±0.36 & \textbf{93.09±1.39} & \textbf{95.09±0.33} & 95.44±0.20 & 79.80±0.38 \\
 FixMatch~\cite{li2021comatch} & 51.15±1.75 & 71.71±0.11 & 77.40±0.12 & 86.19±3.37 & 94.93±0.65 & \textbf{95.74±0.05} & 65.38±0.42 \\
 \hline
 \textbf{CCSSL(FixMatch)} & \textbf{61.19±1.65} & \textbf{75.7±0.63} & \textbf{80.68±0.16} & 90.83±2.78 & 94.86±0.55 & 95.54±0.20 & \textbf{80.01±1.39} \\
\bottomrule
\end{tabular}
\caption{Top-1 Accuracy for in-distribution datasets including CIFAR100, CIFAR10, and STL10. On high noise-level datasets CIFAR100 and STL10, we achieve the best performance by simply adding CCSSL to Fixmatch. On the easier dataset CIFAR10 with less noise, CCSSL only provides marginal performance gain. '-' means not self-implemented.}
\label{tab:in-distribution-data}
\end{table*}



\section{Experiments} \label{sec:experiments}
In this section, we first evaluate the effectiveness of CCSSL on in-distribution datasets, which may also contain blur out-of-distribution cases. Then, we show CCSSL's ability of confirmations bias alleviation on real-world data. Last, a qualitative comparison is presented. 

\begin{table}
\centering
\begin{tabular}{c|cc|cc}
\toprule
    \multirow{3}{*}{Method} &
    \multicolumn{4}{c}{Semi-iNat 2021} \\
& \multicolumn{2}{c|}{From Scratch} & 
    \multicolumn{2}{C{2cm}}{From MoCo Pretrain} \\
\cline{2-5}
& Top1 & Top5 & Top1 & Top5 \\
\midrule
Supervised & 19.09 & 35.85 & 34.96 & 57.11 \\
\hline
MixMatch~\cite{berthelot2019mixmatch} & 16.89 & 30.83 & - & - \\
\hspace{0.5cm}\textbf{+CCSSL} & \textbf{19.65} & \textbf{35.09} & -  & - \\
\hline
FixMatch~\cite{sohn2020fixmatch} & 21.41 & 37.65 & 40.3 & 60.05 \\
\hspace{0.5cm}\textbf{+CCSSL} & \textbf{31.21} & \textbf{52.25} & \textbf{41.28} & \textbf{64.3} \\
\hline
CoMatch~\cite{li2021comatch} & 20.94 & 38.96 & 38.94 & 61.85 \\
\hspace{0.5cm}\textbf{+CCSSL} & \textbf{24.12} & \textbf{43.23} & \textbf{39.85} & \textbf{63.68} \\
\bottomrule
\end{tabular}
\caption{Accuracy for real-world dataset - Semi-iNat 2021. We evaluated two scenarios: random starting with serious noise and pre-trained starting with less noise. In the noisy random starting scenario, CCSSL achieves significant performance gain by noise alleviation.  In the noiseless pretrained starting scenario, CCSSL achieves marginal improvement because noise has been reduced by the pretrained model in the beginning.}
\label{tab:out-class-data}
\end{table}

\subsection{Datasets}
We assume that the harder the dataset, the higher noise it has, so we conduct experiments on the following different noise level datasets.  


\textbf{CIFAR10}~\cite{krizhevsky2009learning} is a relatively easy-level in-distribution dataset consisting of 60K 32x32 colour images in 10 classes, with 6K images per class. 
There are 50K training images and 10K test images. We conduct three different experiments on 40, 250, 4000 random selected images in the class balanced way, following~\cite{sohn2020fixmatch}, as in \cref{tab:in-distribution-data}.  

\textbf{STL10}~\cite{coates2011analysis} is a relatively medium-level in-distribution dataset comprises 10 classes. The training set has 5K images (10 predefined folds), 100k unlabeled images containing similar distribution with training images and 800 test images per class. All images are 96x96 pixels. We conduct experiments on all folds following~\cite{sohn2020fixmatch}, as shown in \cref{tab:in-distribution-data}

\textbf{CIFAR100}~\cite{krizhevsky2009learning} is a relatively hard-level in-distribution dataset comprises 100 classes. The training set contains 50K images, while test contains 10K images. All images have a fixed resolution of 32x32. We conduct three different experiments on 400, 2500, 10000 random selected images in a class-balanced way, following~\cite{sohn2020fixmatch}, as shown in \cref{tab:in-distribution-data}. 

\textbf{Semi-iNat 2021}~\cite{su2021semi_iNat} is a real-world dataset designed to expose some of the challenges encountered in a realistic setting, such as significant class imbalance, domain mismatch between the labeled and unlabeled data, and large unknown classes. The labeled training and val part each has 9721 and 4050 images. The unlabeled training part has 313248 images. Experiments in \cref{tab:out-class-data}

\begin{table}[t]
\centering
\small
\setlength\tabcolsep{2pt}

\begin{tabular}{p{0.18\columnwidth}<{\centering}p{0.18\columnwidth}<{\centering}p{0.18\columnwidth}<{\centering}p{0.18\columnwidth}<{\centering}p{0.18\columnwidth}<{\centering}}

\toprule
    {Origin} & {FixMatch} & {CCSSL} & {CoMatch} &{CCSSL}\\
    & & (FixMatch) & & (CoMatch) \\
\midrule
    \centering
    \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1.0\columnwidth]{jpgs/o_1.jpg}}
	\end{minipage} &
	 \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/fm_1.jpg}}
	\end{minipage} &
	 \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/ofm_1.jpg}}
	\end{minipage} &
	 \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/cm_1.jpg}}
	\end{minipage} &
	\begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/ocm_1.jpg}}
	\end{minipage} \\
    \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/o_2.jpg}}
	\end{minipage} &
	 \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/fm_2.jpg}}
	\end{minipage} &
	 \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/ofm_2.jpg}}
	\end{minipage} &
	 \begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/cm_2.jpg}}
	\end{minipage} &
	\begin{minipage}[b]{0.18\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=1\columnwidth]{jpgs/ocm_2.jpg}}
	\end{minipage} \\
	
\bottomrule
\end{tabular}

\caption{Qualitative comparison on FixMatch and Comatch with/without CCSSL by Grad-CAM\cite{Grad-CAM}. With CCSSL's de-noise effect, an SSL method focuses more on the foreground objects. CCSSL methods find all ants for the ant image and capture the correct position on the leaf image.}
\label{tab:visulation}
\end{table}

\subsection{In-distribution Data Evaluation}
For CIFAR10, we set  with  due to low noise level of the dataset and no out-of-distribution data. For high noise level STL10 and CIFAR100, we set  with . We train 512 epochs for all experiments because of fast convergence speed. For other hyperparameters, optimizer and learning rate schedule, our setting is the same as FixMatch~\cite{sohn2020fixmatch}. We compare CCSSL with  MixMatch~\cite{berthelot2019mixmatch}, ReMixMatch~\cite{berthelot2019remixmatch}, FixMatch~\cite{sohn2020fixmatch}, SSWPL~\cite{taherkhani2021self}, LaplaceNet~\cite{sellars2021laplacenet} and Comatch~\cite{li2021comatch}. Since the last three SSL methods can be viewed as an extension of FixMatch, we directly apply CCSSL on FixMatch for a fair comparison. 

In \cref{tab:in-distribution-data}, We find that the performance of CCSSL is higher when the task is noisy and more confirmation bias involved. On CIFAR100 and STL10, the noisiest in-distribution datasets we use, we achieve the best performance only by adding CCSSL on FixMatch. CCSSL is also on par with state-of-the-art methods on CIFAR10. On CIFAR100, we achieve +10.04\%/ +3.99\%/ +3.26\% over origin FixMatch and +3.08\%/ +2.22\%/ +1.54\% over the best of all compared SSL methods for 400, 2500, and 10000 labels respectively. The performance gain decline can be explained by the confirmation bias level decrease with more training data. For the medium hard STL10, we achieve +14.62\% over origin FixMatch and only +0.2\% over the best of all compared SSL methods. CCSSL's performance improvement decreases on the easier dataset with fewer categories. On CIFAR10, we achieve +4.64/ -0.07\%/ -0.2\% over FixMatch and -2.26\%/ -0.23\%/ -0.2\% over the best of all compared SSL methods. With the decrease of noise level, our CCSSL's performance improvement pales compared with other variations of SSL methods tuned for in-distribution datasets but is still helpful for FixMatch. Another thing that needs to be noticed is that when the noise level is the least (CIFAR10 with 4000 labels), FixMatch achieves the best performance by the simplest structure - which means that when the data is clear and enough, the structure of SSL becomes less critical. 

\subsection{Realistic Evaluation} \label{exp:real}
On Semi-iNat 2021, we simulate two situations with different noise levels - (high: training from scratch, low: training from pre-trained model), as shown in \cref{tab:out-class-data}. The results show that CCSSL can improve SSL in both situations but more noise more improvement. 

For Semi-iNat 2021, we use an image size of 224x224, resnet50 backbone, projection head with a dimension of 64 like in~\cite{he2020momentum}, and training on 4 V100 with batchsize 64 for labeled data and batchsize 448 with  for unlabeled data. For FixMatch~\cite{sohn2020fixmatch}, pseudo label thresh  achieves the best results, but 0.6 when combining with our method. For MixMatch~\cite{berthelot2019mixmatch} and CoMatch~\cite{li2021comatch}, we directly follow their papers' setting. We set   with  due to high noise level on out-of-distribution data. When training from Self-SL pre-trained models, we freeze the first three blocks of resnet50 because of the forgetting problem. Other settings are the same as~\cite{su2021realistic}. 


In \cref{tab:out-class-data} training from scratch, state-of-the-art SSL methods only have on-par performance with the supervised baseline. Compared to the performance on in-distribution datasets, the noise in pseudo-labels harms SSL methods by a large margin. With class-aware contrastive module, we are able to alleviate confirmation bias on out-of-distribution data while maintaining downstream tasks' clustering ability. We improve state-of-the-art SSL by a large margin when using with CCSSL (+2.7\%/ +9.8\%/ +3.1\% on MixMatch, FixMatch and CoMatch separately). The experiments show that CCSSL can be a big helper for pseudo-label-based SSL to reduce the negative impact from noisy predictions when training from scratch on the real-world dataset.


In \cref{tab:out-class-data}, training from MoCo Semi-iNat pre-trained model simulates the situation where SSL starts with less noise~\cite{caron2021emerging}. CCSSL only improves +1.05\%/ +0.91\%/ on FixMatch and CoMatch. The result shows that when a model starts with less noise, our proposed CCSSL can also reduce noise introduced in the training process. 

\subsection{Qualitative Evaluation}
In \cref{tab:visulation}, we show the qualitative results on FixMatch and CoMatch with/without CCSSL. With CCSSL's de-noise effect, an SSL method focuses more on the foreground objects. CCSSL methods find all ants for the ant image and capture the correct position on the leaf image.  We choose FixMatch and CoMatch for visualization because the two are the best on Semi-iNat among SSL methods we tested.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{jpgs/worwithout_ccssl.png}
  \caption{Convergence speed and performance difference on MixMatch and FixMatch with/without CCSSL. As shown in the figure, CCSSL methods achieves better performance with much fewer training epochs.}
  \label{fig:worwithout_ccssl}
\end{figure}

\subsection{Convergence Speed}
In \cref{fig:worwithout_ccssl}, we study the convergence speed using CCSSL on MixMatch and FixMatch. We observe that CCSSL methods achieve the best performance with much fewer training iterations. This phenomenon is the same when training on clean pseudo-labels~\cite{li2021comatch} because of the noise alleviation effect of CCSSL. More specifically, better pseudo labels in the early phase helps better guide the learning process and translate to faster convergence.


\section{Ablation}
\subsection{Contrastive \& Class-Aware \& Re-weighting}
\begin{table}
\centering
\small
\begin{tabular}{C{1cm}|C{1.5cm}|C{1cm}|C{1.3cm}|C{1.3cm}}
\toprule
contrastive & re-weighting & class-aware & Semi-iNat & CIFAR100\\
\midrule
 &  &  & 21.41 & 72.54\\
\checkmark &  &  & 27.73 & 72.37\\
\checkmark & \checkmark &  & 29.48 & 72.67\\
\checkmark &  & \checkmark & 30.49 & 75.89 \\
\checkmark & \checkmark & \checkmark & \textbf{31.21} & \textbf{75.96} \\
\bottomrule
\end{tabular}
\caption{Accuracy for different combinations of contrastive learning, class-aware and re-weighting.}
\label{tab:ablation-for-module}
\end{table}



In ~\cref{tab:ablation-for-module}, we investigate three main techniques of CCSSL, including contrastive learning, class-aware, and re-weighting, based on FixMatch. We observe that all components are helpful. However, directly using contrastive learning deteriorates the performance on in-distribution dataset CIFAR100 while largely benefiting the real-world dataset Semi-iNat 2021 with extensive out-of-distribution data. CCSSL found the equilibrium point for contrastive and clustering by class-aware contrastive and re-weighting, and improved FixMatch by a large margin on both in-distribution and real-world datasets.

\subsection{Ratio of Unlabeled Data}

\begin{table}
\centering
\small
\begin{tabular}{c|c|c}
\toprule
Ratio & \multicolumn{2}{c}{Semi-iNat}  \\
() &  &  \\
\midrule
2 & 23.01 & 21.93 \\
4 & 23.88 & 25.19 \\
5 & \textbf{25.33} & 26.81 \\
6 & 22.32 & 24.47 \\
7 & 23.14 & \textbf{27.46} \\
\bottomrule
\end{tabular}
\caption{Different ratios of unlabeled data on Semi-iNat with different degrees of class-aware (). When  (no contrastive involved), smaller ratios achieves better results because of noise in unlabeled data. After setting  to regularize noise, larger ratio achieves better results.}
\label{tab:ablation-for-mu}
\end{table}
As shown in \cref{tab:ablation-for-mu}, we experiment on different values of , which defines the ratio of labeled and unlabeled data in a batch. We observe that when only using class-aware clustering without contrastive regularization (),  CCSSL achieves the best result with a small ratio of unlabeled data (). However, after we increase the effect of contrastive learning by changing  (sample conficence  0.9 will do contrastive learning), the larger ratio has better performance (). Less noisy dataset like CIFAR100 also benefits from large unlabeled data ratio~\cite{nassar2021labels}. This phenomenon can be explained by the higher noise amount introduced from self-generated labels with larger ratio of unlabeled data. With CCSSL's noise alleviation, SSL methods achieve the best performance with a larger unlabeled ratio.


\subsection{Threshold for Class-Aware and Contrastive}
\begin{table}
\centering
\small
\begin{tabular}{c|c|c}
\toprule
 & Semi-iNat & CIFAR100@l2500 \\
\midrule
0 & 23.88 & 75.59 \\
0.4 & 23.38 & \textbf{75.96} \\
0.6 & 24.49 & 75.76 \\
0.7 & 24.64 & 75.9 \\
0.8 & 23.63 & 75.7 \\
0.9 & \textbf{25.21} & 75.79 \\
\bottomrule
\end{tabular}
\caption{Experiments on different class-aware thresholds. Our algorithm is robust with varying   from 0.6 to 0.9. Semi-iNat has a larger noise level and needs larger  which proves CCSSL's ability for noise alleviation.}
\label{tab:ablation-for-push}
\end{table}

We found that our algorithm is robust with varying the threshold  from 0.6 to 0.9. However, the optimal threshold for class-aware and contrastive learning is inconsistent on different noise level datasets. For high noise level Semi-iNat with unbalanced distribution and unknown classes,  achieves the best performance. For low noise level CIFAR100,  achieves the best performance. The result proves CCSSL's noise-reducing ability by needing to regularize more on noisy datasets. 

\section{Limitations}
Potential harmful training fluctuation does exist in our experiments: performance on Semi-iNat fluctuates more than on CIFAR100 in the \cref{fig:worwithout_ccssl}. The main reason lies in the lower quality of pseudo-labels for high noise-level data. Increasing the weight for the class-aware  contrastive module or training with different seeds to average the effect of fluctuation can alleviate the problem. During our experiments, we fixed the weight and seed for a fair comparison. 

\section{Conclusion}
We proposed CCSSL, a general confirmation bias alleviation method for pseudo-label-based SSL methods. CCSSL reduces noise by constructing another feature space instead of using a model's output. By class-aware contrastive module, we apply image-level contrastive on out-of-distribution data for noise alleviation and class-level clustering on in-distribution data for blending into downstream tasks.  We have conducted extensive experiments over different noise level datasets, including in-distribution datasets and a real-world dataset,  to demonstrate the effectiveness of CCSSL. With simply replacing the semi-supervised module, we improve the state-of-the-art SSL~\cite{sohn2020fixmatch}~\cite{berthelot2019mixmatch}~\cite{li2021comatch} by a large margin.
With the effort above, CCSSL is proved to be helpful in making SSL more practical in the real world. 


\textbf{Potential negative societal impact} CCSSL is a fundamental technology that can generate robust semi-supervised classification models in the real world. Maliciously using a classification model to negatively impact society is possible. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
